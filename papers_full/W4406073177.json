{
  "title": "Unveiling the power of language models in chemical research question answering",
  "url": "https://openalex.org/W4406073177",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2109115853",
      "name": "xiuying chen",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2296627531",
      "name": "Tairan Wang",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4367222584",
      "name": "Taicheng Guo",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A4289600595",
      "name": "Kehan Guo",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A3013334361",
      "name": "Juexiao Zhou",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2104319677",
      "name": "Haoyang Li",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2060919619",
      "name": "Zirui Song",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2103988188",
      "name": "Xin Gao",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2125330504",
      "name": "Xiangliang Zhang",
      "affiliations": [
        "University of Notre Dame",
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109115853",
      "name": "xiuying chen",
      "affiliations": [
        "King Abdulaziz University",
        "King Abdullah University of Science and Technology",
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2296627531",
      "name": "Tairan Wang",
      "affiliations": [
        "King Abdullah University of Science and Technology",
        "King Abdulaziz University"
      ]
    },
    {
      "id": "https://openalex.org/A4367222584",
      "name": "Taicheng Guo",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A4289600595",
      "name": "Kehan Guo",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A3013334361",
      "name": "Juexiao Zhou",
      "affiliations": [
        "King Abdulaziz University",
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2104319677",
      "name": "Haoyang Li",
      "affiliations": [
        "King Abdullah University of Science and Technology",
        "King Abdulaziz University"
      ]
    },
    {
      "id": "https://openalex.org/A2060919619",
      "name": "Zirui Song",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2103988188",
      "name": "Xin Gao",
      "affiliations": [
        "King Abdullah University of Science and Technology",
        "King Abdulaziz University"
      ]
    },
    {
      "id": "https://openalex.org/A2125330504",
      "name": "Xiangliang Zhang",
      "affiliations": [
        "University of Notre Dame",
        "King Abdulaziz University",
        "King Abdullah University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2766317792",
    "https://openalex.org/W4312634749",
    "https://openalex.org/W4284674228",
    "https://openalex.org/W3034654340",
    "https://openalex.org/W4372334257",
    "https://openalex.org/W3005769002",
    "https://openalex.org/W4400921803",
    "https://openalex.org/W4399807068",
    "https://openalex.org/W2181951075",
    "https://openalex.org/W4393902605",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W3099977667",
    "https://openalex.org/W4284664200",
    "https://openalex.org/W4284704915",
    "https://openalex.org/W4384644376",
    "https://openalex.org/W4361001766",
    "https://openalex.org/W4391561379",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W3208268490",
    "https://openalex.org/W4317757475",
    "https://openalex.org/W4401864577",
    "https://openalex.org/W4396723768",
    "https://openalex.org/W4396801893",
    "https://openalex.org/W3128121521",
    "https://openalex.org/W3190966924",
    "https://openalex.org/W4378942305",
    "https://openalex.org/W4296605665",
    "https://openalex.org/W2440599146",
    "https://openalex.org/W4245294418",
    "https://openalex.org/W2003458432",
    "https://openalex.org/W2963691377",
    "https://openalex.org/W4318348348",
    "https://openalex.org/W4280634279",
    "https://openalex.org/W2989700832",
    "https://openalex.org/W3175818566",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2921861056",
    "https://openalex.org/W4385571374",
    "https://openalex.org/W3001197829",
    "https://openalex.org/W4387397186",
    "https://openalex.org/W3035081642",
    "https://openalex.org/W3042611018",
    "https://openalex.org/W3167251133",
    "https://openalex.org/W2033178790",
    "https://openalex.org/W3099944244",
    "https://openalex.org/W2782639305"
  ],
  "abstract": null,
  "full_text": "communicationschemistry Article\nhttps://doi.org/10.1038/s42004-024-01394-x\nUnveiling the power of language models in\nchemical research question answering\nCheck for updates\nXiuying Chen 1,2,4 , Tairan Wang 2,4,T a i c h e n gG u o3, Kehan Guo3, Juexiao Zhou 2, Haoyang Li 2,\nZirui Song1, Xin Gao 2 & Xiangliang Zhang 2,3\nWhile the abilities of language models are thoroughly evaluated in areas like general domains and\nbiomedicine, academic chemistry remains less explored. Chemical QA tools also play a crucial role in\nboth education and research by effectively translating complex chemical information into an\nunderstandable format. Addressing this gap, we introduce ScholarChemQA, a large-scale QA dataset\nconstructed from chemical papers. Speciﬁcally, the questions are from paper titles with a question\nmark, and the multi-choice answers are reasoned out based on the corresponding abstracts. This\ndataset reﬂects typical real-world challenges, including an imbalanced data distribution and a\nsubstantial amount of unlabeled data that can be potentially useful. Correspondingly, we introduce a\nChemMatch model, speciﬁcally designed to effectively answer chemical questions by fully leveraging\nour collected data. Experiments show that Large Language Models (LLMs) still have signiﬁcant room\nfor improvement in theﬁeld of chemistry. Moreover, ChemMatch signiﬁcantly outperforms recent\nsimilar-scale baselines:https://github.com/iriscxy/chemmatch.\nQuestion Answering (QA) models have emerged as crucial tools for\nacquiring knowledge and evaluating domain-speciﬁc abilities. For example,\nQA models are designed to provide precise answers to a wide range of\nqueries, thus assisting in the dissemination of information and the\nenhancement of learning processes\n1– 5. Correspondingly, to examine and\nevaluate the accuracy of the given answers6– 9, propose different QA datasets\nto rank the abilities of various language models andﬁnd that these models\nhave ﬂaws in different ways. Question answering is often framed as a\njudgment task. For example, in the BoolQ task10, where a user poses ques-\ntions about a document, the QA model responds with either“yes” or“no,” as\nillustrated in Fig.1a. This type of judgment can be challenging even in\ngeneral contexts. For instance10, found that answering natural questions is\nsurprisingly difﬁcult, as they frequently require a deep understanding of\ncontext, nuances, and speciﬁc details within the document. In scientiﬁc\ndomains, judgment tasks are also a common format. In studies such as11,12,\nt h et a s ki n v o l v e se i t h e rs u p p o r t i n go rr e f u t i n gas c i e n t iﬁc claim. QA tasks in\nscholarlyﬁelds are particularly demanding, as scientiﬁc papers often contain\nspecialized terminology that can be challenging to understand even for\nresearchers\n13– 15. A number of domain-speciﬁc QA datasets are proposed in\nthe biomedical domain16– 18.F o re x a m p l e19, proposes a multi-choice bio-\nmedical QA dataset collected from PubMed papers, and20 collects a\nmultiple-choice dataset to classify which disease the patient has.21 proposes\nLiteratureQA, a QA corpus consisting of papers in the computer science\ndomain with human-engineered questions.\nHowever, the domain of chemical QA has not been explored as\nextensively as other scientiﬁc ﬁelds, such as biology19,22. Recent inter-\ndisciplinary research efforts have increasingly employed language models as\ntools in chemistry23– 26. In the meantime, chemical QA systems provide\nquick, accurate access to essential chemical information, aiding in the\nresolution of complex problems, understanding reactions, and the devel-\nopment of new materials, thus supporting innovation and informed\ndecision-making in chemistry-relatedﬁelds\n27– 31.F o re x a m p l e32, proposes\nmultimodal multiple-choice questions on different science topics, along\nwith annotations of their answers, corresponding lectures, and explanations.\nThe dataset most closely related to our work is KGQA28,a ss h o w ni nF i g .1b.\nKGQA is based on a chemical knowledge graph and relies on a template-\nbased approach to generate QA pairs. However, this approach lacks the\ndiversity found in real-world language and tends to focus primarily on\nfoundational chemical concepts rather than complex, practical research\nquestions. Additionally, KGQA depends heavily on a human-constructed\nknowledge graph, limiting its adaptability and scope for broader research\napplications.\nIn contrast to previous work, this study introduces achemical research\nQA benchmark to evaluate and improve the chemical QA capabilities of\nlanguage models by leveraging the large-scale scholarly chemical papers that\nare readily available. Each year, there are over 500,000 new publications in\nthe ﬁeld of chemistry, as reported by the Web of Science, making it an\nexcellent resource to start with. The QA pairs in these papers originate from\n1Mohamed bin Zayed University of Artiﬁcial Intelligence, Abu Dhabi, UAE.2King Abdullah University of Science and Technology, Jeddah, Saudi Arabia.3University\nof Notre Dame, Notre Dame, IN, USA.4These authors contributed equally: Xiuying Chen, Tairan Wang.e-mail: xiuying.chen@mbzuai.ac.ae\nCommunications Chemistry|             (2025) 8:4 1\n1234567890():,;\n1234567890():,;\nresearch-investigated problems rather than being artiﬁcially created for\nevaluation, thus holding greater relevance and applicability to practical\nscenarios in theﬁeld of chemistry. Concretely, in this paper, we propose\nScholarChemQA, a chemical QA dataset for answering research questions\nwith multi-choice betweenyes, no,a n dmaybe. Firstly, we collected over a\nmillion titles and abstracts related to chemistry from academic platforms.\nThrough a rigorous selection process, we curated 40k QA pairs where each\ntitle, framed as a question, can be answered using the aforementioned\noptions. Out of these, 1k pairs were hand-labeled for training, validation,\nand testing, with yes/no/maybe constituting 65.8%, 21.2%, and 13.0%,\nrespectively. The‘yes’ and ‘no’ labels indicate if the abstract’s experiments\nsupport or refute the conclusion, and the‘maybe’label serves as a nuanced\ni n d i c a t o rf o ra m b i g u o u so rm i x e de v i d e nce situations. Besides, to enrich our\ndataset, we converted an additional 4k titles from statement format into yes/\nno questions. An example case from our dataset is shown in Fig.1c. To\ncorrectly answer the question, the model should have a foundational\nunderstanding of the behavior of a two-dimensional hole gas, the principles\nof GaAs quantum wells, and the concept of phase separation. Semantic\nreasoning skills are also indispensable to interpret the‘coexistence of two\nphases’as the concurrent existence of the mentioned metallic and insulating\nphases. The beneﬁt so fo u rd a t a s e t sa r em u l t i - f a c e t e d .F i r s t l y ,i ti sac h e m i c a l\nQA dataset for research purposes, encompassing a wide range of topics from\nbasic concepts to complex chemical processes. Secondly, it requires complex\nreasoning and in-depth semantic analysis to deduce the answer. Thirdly,\nScholarChemQA sets a new benchmar kf o rA Ii nr e a l - w o r l d ,a c a d e m i c\ncontexts, enhancing AI-driven exploration and discovery in chemistry.\nFor experiments, we ﬁrst evaluate the performance of LLMs on\nScholarChemQA. Results show that even the advanced GPT-3.5 model\nachieves only 54% accuracy, highlighting the difﬁculties faced by LLMs in\nunderstanding research papersﬁlled with complex terminology. Recog-\nnizing the need for improvement and more accessible resources, we aim to\nuse our collected chemical QA dataset to develop a smaller, more precise\nmodel. Theﬁrst challenge here is that the dataset exhibits an imbalanced\nattribute, where just 13% of cases belong to the‘maybe’minority class. This\nis a commonly observed characteristic in real-world datasets, as noted in\nprevious studies\n33. This imbalance becomes more pronounced when\nincluding the automatically annotated yes/no set. The second challenge\ninvolves the incorporation of a substantial amount of unlabeled data. Hence,\nin this paper, we introduce ChemMatch, a chemical question-answering\nmodel withlabel rebalance, pseudo label calibration,a n ddual augmentation\nto address the above challenges. Generally, our ChemMatch follows the\nsemi-supervised paradigm, generatingpseudo-labels for unlabeled data and\ntraining the model to predict these labels using augmented data. Weﬁrst\naddress the issue of imbalanced label distribution byre-weighting the\ninstance-wise lossbased on the inverse frequency of each class. Thepseudo\nlabel calibrationseeks to align pseudo-label estimates with a desired ground\ntruth distribution. To alter unlabeled samples for creating diverse aug-\nmentations, we propose a SoftMix operation that generates bothquestion-\nand context-side augmentation, not in the input space, but in their repre-\nsentation space. Our experimental results demonstrate that our proposed\nmodel signiﬁcantly outperforms modelso fas i m i l a rs c a l ea n dL L M ,\nmarking a step forward in domain-speciﬁc QA model development.\nOur main contributions can be summarized as follows:\n We collected ScholarChemQA, a chemical QA dataset for answering\nresearch questions. This benchmark can be used to evaluate the\nchemistry domain capabilities of AI models.\n We assess recent LLMs including Llama2-70B, GPT-3.5, and GPT-4\non ScholarChemQA, revealing their limitations in comprehending\nchemical research papers and delivering precise answers.\n We propose an open-source, and computationally efﬁcient model\nChemMatch. ChemMatch signiﬁcantly outperforms the advanced\nG P T - 3 . 5a n dG P T - 4m o d e l s ,p r o v i ding a valuable tool for acquiring\nchemical-related knowledge.\nScholarChemQA Dataset\nIn this section, we introduce our data collection process and some key\nattributes of our collected data.\nData collection\nData sources. To compile a comprehensive collection of chemical\npapers, we utilized multiple academic publishing sources including\nElsevier and Springer. The overall process is illustrated in Fig.2a. Firstly,\nby employing a combination of publisher APIs for databases such as\nScopus, ScienceDirect, Springer Nature, Cross-Ref, and Lens\n34, we col-\nlected approximately 10 million abstracts and titles centered on\nchemistry-related studies from 2000 to 2023. Then, we speci ﬁcally\nselected papers that have question marks in their titles to build the QA\ndataset. This is because we can automatically obtain natural scholarly\nquestions, and the corresponding answer is usually found within the\nabstract or the main content of the paper. By focusing on papers with\nquestion marks in their titles, we aim to capture a diverse set of research\nquestions that are directly relevant to the ﬁeld of chemistry. This\napproach allows us to construct a dataset that is rich in domain-speciﬁc\nquestions and answers, providing a valuable resource for training and\nevaluating question-answering models in the scientiﬁc domain. In this\nwork, we employ a multi-choice setting, where the questions are\nanswered with ‘yes’, ‘no’,o r ‘maybe’. This approach simpliﬁes the\nresponse format and allows for a more straightforward evaluation of the\nquestion-answering model’s performance. By restricting the answers to\nthese three options, we can focus on the model’s ability to understand and\nFig. 1 | Comparison of different QA datasets in different domains.Comparison of\n(a) general domain QA datasetBoolQ,( b) chemical domain datasetKGQA, and (c)\nour ScholarChemQA dataset. Our dataset is sourced from chemical research papers,\nin contrast to previous chemical datasets, which were artiﬁcially constructed. Our\ndataset contains text rich in domain-speciﬁc information, making it highly suitable\nfor evaluation.\nhttps://doi.org/10.1038/s42004-024-01394-x Article\nCommunications Chemistry|             (2025) 8:4 2\ncategorize the information presented in the text, making it easier to assess\nits accuracy and reliability in a controlled setting.\nNote that not all questions can beanswered with a yes/no/maybe\nresponse. To handle this, we followed a rule-based approach, where\nwe excluded questions that start with interrogative words (e.g., wh-words)\nor involve selecting from multiple entities. During our initial investigation,\nwe found that approximately 10% of the abstracts contained a\nconclusion subsection that could be considered as the response to the\nassociated question. To enhance the challenge of reasoning, we excluded this\nsection from our context. Finally, we obtained 40k cases that cover various\ntopics, and the distribution of papers from various sources is shown\nin Fig.2b.\nExpert annotation and quality control. Since the original dataset lacked\nanswer labels for the question titles, we conducted an expert annotation\nprocess to collect a labeled dataset. The annotation criterion was as fol-\nlows: We choose to annotate a question with‘yes’when the experiments\nand results of the paper substantiate it. Conversely, we use‘no’when they\ncontradict or refute the statement. A‘maybe’ is annotated in two sce-\nnarios: (1) when the paper outlines conditions in which the answer could\nbe both true and false, or (2) when multiple interventions, observations,\netc., are inquired about, and the answer holds true for some but not all of\nthem. It is crucial to recognize that these answers are not universal truths,\nbut rather depend on the speciﬁc context provided in the research paper.\nWe employed four PhD annotators, each with a background in chem-\nistry, to individually label 525 instances, yielding two annotations for\neach case and a labeled dataset consisting of 1050 instances. One anno-\ntator had access to the conclusion part, reducing the need for extensive\nreasoning, while the other annotator was not provided with the conclu-\nsion part, requiring deeper reasoning from the available context. This\nseparation process ensured both annotation and dataset quality. When\nthere was disagreement in the labeling, a third annotator facilitated dis-\ncussions to achieve consensus among the two initial annotators. The\ninitial labeling yielded a Kappa score of 0.62, indicating substantial\nagreement, and theﬁnal discussion phase ensured an overall high quality\nof the data. The statistics are shown in Table1.\nFor the human-annotated cases, thetrain, validation, and test sets\nconsist of 500, 50, and 500 samples, respectively. Next, we collect additional\nautomatically annotated training cases.\nAutomatic annotation. To further enrich our dataset, we used a simple\nheuristic to collect noisily-labeled instances. We began by selecting\npapers with statement titles that followed speciﬁc Part-Of-Speech (POS)\ntagging structures (NP-(VBP/VBZ)) based on the Stanford POS tagging\nscheme\n35. We then transformed the statement titles into questions by\nemploying a simple method, which involved inserting copulas like‘is’or\nauxiliary verbs such as‘does’ at the beginning of the sentence. We also\nensured that the transformed sentences were coherent, making necessary\nadjustments like adding question marks. The yes/no answer was then\ndetermined based on whether the verb (VB) in the sentence was negated.\nFor example, the title ‘Current fossil fuel infrastructure does not\nyet commit us to 1.5\n∘C warming’ is changed to ‘Does the current\nfossil fuel infrastructure commit us to 1.5∘C warming?’with answer‘No’.\nIn cases where the complex titles involve commas or colons, we relied\non GPT-4 to automatically convert them into appropriate question for-\nmats. In a random sampling of 200 rewritten questions evaluated by\nGPT-4 and human forﬂuency, all questions were classiﬁed as coherent\nand ﬂuent.\nCharacteristics\nIn the collected papers obtained fromLens, the meta-information associated\nwith them provides subject information. Figure2b presents the topic dis-\ntribution of these papers. They cover a wide range of topics, including\nbiochemistry, theoretical chemistry,catalysis, environmental chemistry,\nand material chemistry.\nTo delve further into the QA attributes, we performed a human ana-\nlysis on a random sample of 200 examples, where we categorized the\nquestions into three main aspects and classiﬁed the difﬁculty into back-\nground knowledge-required and knowledge-free categories. The three main\naspects are: chemical interaction (questions about how chemicals interact or\nreact), chemical theory (questions related to fundamental chemistry\nTable 1 | ScholarChemQA statistics\nStatistic Human\nAnnotated\nAutomatically\nAnnotated\nUnlabeled\nSize 1.05k 4k 40k\nProp. of yes (%) 65.8% 80.0% –\nProp. of no (%) 21.2% 20.0% –\nProp. of\nmaybe (%)\n13.0% ––\nAvg. question\nlength\n13.87 14.14 14.20\nAvg. context\nlength\n176.01 175.15 178.41\nFig. 2 | Overview of ScholarChemQA dataset and analysis. aIllustration of data\ncrawling process.b Topic distribution of ScholarChemQA.c Proportional rela-\ntionships between corresponding question types and reasoning types. Different\nquestion types correspond to different reasoning types, showcasing the diversity of\nour dataset. 71.5% of the questions require chemical knowledge for answering,\nshowing the difﬁculty of our chemical question-answering tasks.\nhttps://doi.org/10.1038/s42004-024-01394-x Article\nCommunications Chemistry|             (2025) 8:4 3\ntheories or principles), and chemical attributes (questions focusing on\ninherent properties of speciﬁc chemicals), with the majority falling under\nthe category of chemical attributes. Examples of these are provided in\nTable 2. Regarding the type of reasoning required, around 71.5% of the\nquestions require chemical knowledge for answering. The remaining\nquestions can be addressed through semantic reasoning. For instance, the\ncontext“the metal center is really capable of back-donation to the carbene\"\nprovides the answer to the question“Back-Donation in High-Valent d\n0\nMetal Complexes: Does It Exist?\" Examples can be found in Table2.T o\nbetter illustrate the correspondence between different reasoning types and\nquestion types, we present a Sankey diagram depicted in Fig.2c. It can be\nseen that different question types correspond to different reasoning types,\nshowcasing the diversity of our dataset.\nMethods\nIn this section, weﬁrst deﬁne the task of chemical QA, then describe our\nChemMatch model in detail.\nProblem Formulation\nThe task of building ChemMatch can be formulated as aC-class classiﬁ-\ncation problem in a semi-supervised learning setting. There are labeled\ninstances, denoted as q\ns; cs; ys/C8/C9\n, and unlabeled instances, denoted as\nqu; cu/C8/C9\n,w h e r eq/C3 ; c/C3 2 Rd are thed-dimensional question and context\nrepresentation, andys is the one-hot ground-truth label. To answer a within-\ncontext question x ={ q, c}, ChemMatch makes prediction y0 as\npðy0jxÞ2 RC. The overview for building ChemMatch is illustrated in Fig.3.\nThe loss function to minimize isL ¼ Ls þ Lu.H e r eLs is thesupervised\ncross-entropy loss(H):\nLs ¼ H ys; y0/C0/C1\n: ð1Þ\nThe unsupervised consistency lossLu is deﬁned by adopting a pseudo-\nlabeling approach with consistency restriction:\nLu ¼ H ^p; ^y\n/C0/C1\n; ð2Þ\nTable 2 | Summary of ScholarChemQA question types\nQuestion Type % Example Questions\nChemical Interaction 21.5 Is the polarization of the C =C bond imperative forbifunctional outer-sphere C=C hydrogenation?\nDo ﬁnal-state interactionsobscure short-range correlation effects in quasielasticAðe; e0pÞ scattering?\nChemical Theory 35.0 Does the Oxidation of Zirconium obey Wagner’s Theory?\nDeciphering mechanism of aggregation-induced emission (AIE): Is E-Zisomerisation involved in an AIE process?\nChemical Attribute 43.5 Catalytic amyloids: Is misfolding folding?\nIs the solubility product constant? Introductory experiment in solubility equilibrium\nReasoning Type % Example Question & Context Snippet\nSemantic Reasoning 28.5 Question: Can the supersymmetric ω parameter be generated dynamicallywithout a light singlet?\nContext: It is generally assumed that the dynamical generation of the Higgs mass parameter of the superpotential,ω, implies the\nexistence of a light singlet at or below the supersymmetry breaking scale,M\nSUSY. We present a counter-example in whichthe sunglet\nﬁeld can receive an arbitrarily heavy mass(e.g., of the order of the Planck scale,MP ≈ 1019 GeV). In this example, a non-zero value ofμ\nis generated through soft supersymmetry breaking parameters and is thus naturally of the order ofMSUSY.\nKnowledge Reasoning 71.5 Question: The metallic resistance of a dilute two-dimensional hole gas in a GaAs quantum well:two-phase separationat ﬁnite\ntemperature?\nContext: We have studied the magnetotransport properties of a high mobility two-dimensional hole gas (2DHG) system in a 10nm GaAs\nquantum well with densities in range of 0.7− 1.6*1010c m−2 on the metallic side of the zero-ﬁeld ‘metal-insulator transition’. In a parallel\nﬁeld well aboveBc that suppresses the metallic conductivity, the 2DHG exhibits a conductivitygðTÞ/C25 0:3ðe2=hÞ ln T reminiscent of weak\nlocalization. The experiments are consistent withthe coexistence of two phases in our system: a metallic phase and a weakly insulating\nFermi liquid phase having a percolation threshold close toBc.\nHighlighted texts are matched key phrases between types and examples.\nFig. 3 | Training framework of ChemMatch.ChemMatch is trained using both\nlabeled and unlabeled data. In the supervised training phase, label rebalancing is\napplied to adjust the loss regarding class infrequency. In the unsupervised phase,\npseudo-labels are generated through pseudo-label calibration. The learning from\nunlabeled data is through the enforcement of consistency between the pseudo-labels\nand the predictions of instances augmented using SoftMix.\nhttps://doi.org/10.1038/s42004-024-01394-x Article\nCommunications Chemistry|             (2025) 8:4 4\nwhere^p is the pseudo-label generated for unlabeled input (see Equation (5)).\n^y is obtained bypy jΩ xuðÞ\n/C0/C1\n,w h e r eΩ xuðÞ represents the prediction based\non augmented variations of the question and the context (see section 3.4).\nThe general objective is to ensure that the predicted label of the\ncorresponding augmented case alignswith the pseudo-labels. In this way,\nthe vast amount of unlabeled cases is leveraged as well to optimize the\nprediction ofy.\nThe minimization of both supervised loss (L\ns) and unsupervised loss\n(Lu) is hindered by the imbalanced distribution of classes iny. Speciﬁcally,\nthe‘maybe’class is signiﬁcantly underrepresented compared to the‘yes’and\n‘no’ classes. This imbalance is further aggravated when combined with an\nautomatically annotated dataset that only includes‘yes’ or ‘no’ labels. To\naddress these challenges, we implement a strategy of‘label rebalance’during\nthe supervised training phase and‘pseudo-label calibration’ during the\nsemi-supervised learning process, which are explained in detail below.\nLabel rebalance\nFrom Table1, it is evident that in the human-annotated dataset, the‘yes’\nclass constitutes 65.8%, while the least represented class accounts for only\n13%. Moreover, if we combine the automatically annotated dataset into\ntraining, the imbalance problem becomes even more severe, since the\nautomatic datasets are constructed based only on‘yes’ and ‘no’ classes.\nTherefore, addressing the generalization issue for the less frequent classes is\ncrucial.\nInspired by Ref.36, we integrate the principle of label rebalance into the\ntraditional cross-entropy loss. Intuitively, we increase the loss weight of the\nless frequent class. This adaptationis advantageous for minority classes,\npushing them to have broader margins and achieving higher accuracy. Let’s\nconsider a sample labeledy\ns\ni which represents a class withny training\ninstances. The modiﬁed label-rebalanced softmax cross-entropy loss is\ndeﬁned as:\nLbs ¼ 1 /C0 β\n1 /C0 βny\nH ys\ni ; y0\ni\n/C0/C1\n: ð3Þ\nHere, aβ value of 0 indicates that there’s no re-weighting applied. Asβ\napproaches 1, it signiﬁes re-weighting based on the inverse of the class\nfrequency. The hyperparameterβ and the effective sample numberny allow\na smooth adjustment of the class-balanced factor, ranging from no re-\nweighting to re-weighting by inverse class frequency.\nPseudo-label calibration\nPseudo-labels are often generated by trained models for unlabeled data37,38.\nBy incorporating pseudo-labeled data, the model can leverage a wealth of\nunlabeled data, enhancing its generalization capabilities and improving\nprediction accuracy. To ensure the high quality of pseudo-labels, we cali-\nbrate their distribution so that it aligns with the distribution of the actual\nground truth labels.\nThe ﬁrst operation ismultiplication of the predicted and ground truth\ndistributions: This step enhances the parts of the predicted distribution that\nmatch the ground truth distribution. Ifa certain class has a high probability\nin both the predicted and ground truthdistributions, its probability will\nfurther increase after multiplication. Conversely, if a class has a high\nprobability in the predicted distribution but a low probability in the ground\ntruth distribution, its probability will decrease after multiplication. Let_p 2\nR\nC be the prediction of the pseudo-label of one unlabeled instance. Weﬁrst\nmultiply it with/C22y 2 RC, which is the distribution of ground truth labels\nfrom annotated datays.\nThen,division by the past average distribution:T h i ss t e pa i m st or e d u c e\nthe bias in the predicted distribution caused by the accumulation of his-\ntorical data. If a certain class has appeared frequently in the past average\ndistribution, its probability will be correspondingly reduced in the new\nprediction to avoid the excessive inﬂuence of past data on the current\nprediction. To calibrate each_p, we estimate its distribution in one batch\npðyÞ2 RC, e.g., by taking the average of the model’s predictions on unla-\nb e l e de x a m p l e so v e rt h el a s t1 2 8b a t c h e s .\nThe above process can be summarized as: To adjust the predicted\npseudo-labels to better reﬂect the true likelihood of each class, we apply the\nfollowing pseudo-label calibration operationwith pointwise multiplication\nand division:\n~p ¼ Normalizeð _p × /C22y=pðyÞÞ; ð4Þ\nwhere Normalize(a)= a/∑jaj. Together, these two steps ensure that the\npseudo-labels are both accurate (by aligning with the ground truth\ndistribution) and consistent (by forming a valid probability distribution),\nthereby improving the model’s ability to learn from unlabeled data.\nAdditionally, since ground truth labels typically adopt hard (1-hot)\nencoding, we further modify the calibrated pseudo-labels by applying a\nsharpening function:\n^p\ni ¼ ~p\n1\nT\ni =\nXC\nj¼1\n~p\n1\nT\nj ; ð5Þ\nwhereT is a hyperparameter. AsT approaches 0, the output will approach a\none-hot distribution. A reduction inT steers the model towards generating\npredictions with diminished entropy. Finally, we use^p as the pseudo label in\nEquation (2) and proceed as usual with other processing.\nSoftMix augmentation\nTo utilize the abundance of available unlabeled data and enhance the\nlearning process, the concept of data augmentation has been extensively\nadopted in semi-supervised learning\n39. The key idea is to create data variants,\nmake predictions, and compare them with pseudo labels to guide model\ntraining. As introduced in the semi-supervised learning framework in\nSection 3.1, augmenting unlabeled cases is necessary to formulate a con-\nsistency loss. Most of the existing augmentation methods are ininput space.\nF o re x a m p l e ,a u g m e n t a t i o no ni m a g e si n c l u d e sr o t a t i o n ,c r o p p i n g ,a n d\nﬂipping, and text-domain augmentations include back translation\n37 and\nsynonym substitution40. However, studies41,42 suggest that interpolations in\nhidden layerscan capture more advanced information, enhancing semantic\ndiversity and providing additional training signals. For example, enhancing\ndiversity in latent spaces can improvethe robustness of text generation\nmodels43. Inspired by these insights, we introduce the SoftMix augmentation\noperation, designed to increase diversity and strengthen robustness by latent\nspace augmentations.\nAs our QA paradigm consists of the question and the input document,\nwe naturally have two kinds of augmentation results by using back trans-\nlation to translate these two parts respectively. Back translation means\ntranslating text from the source language to a target language and then\ntranslating it back to the original language. This process helps generate\nlinguistically diverse versions of the question and document, which\ncan improve the model’s robustness and ability to handle varied phrasings\nin QA tasks. We refer to the input with a back-translated question as\n‘question-augmented’,a n dt h es a m eg o e sf o r‘context-augmented’.L e tx\na be\nthe question-augmented input representation, andxb be the answer-\naugmented representation. Amongxa, xb and the original inputxu,o n ec a n\nbe randomly selected to act as a source of perturbation to modify the other\ninputs. For instance, ifx\na is selected for perturbation, bothxu and xb are\nmodiﬁed as:\nx0/C3 ¼ λx/C3 þ 1 /C0 λðÞ xa; ð6Þ\nλ /C24 Betaðα; αÞ; ð7Þ\nwherex0/C3 represents the new training input derived fromx* (xu andxb in this\nexample case), and α is a hyperparameter for Beta distribution. The\nrepresentations of two augmented cases are separately mixed with their own\nhttps://doi.org/10.1038/s42004-024-01394-x Article\nCommunications Chemistry|             (2025) 8:4 5\noriginal representation to produce new training inputs with the same\ntraining target, i.e., the pseudo label. The whole process is illustrated in Fig.3.\nNote that our SoftMix operation is different from the previous\nRemixMatch operation in Ref.39. In their method, they perform a weighted\nsum of multiple input hidden states and output states to form new training\nsamples. In contrast, in our work, we establish a mixture operation only in\nthe input space with representationsthat have similar semantic meanings,\nkeeping the target the same. This maintains a balance between diversity in\nthe latent space and the fundamental generative capability without inter-\nference. In the experiments section 4, we will show that our method sig-\nniﬁcantly outperforms RemixMatch.\nThe newly generated training inputs share the same prediction\nobjective, i.e., the pseudo label. Therefore, their predictions are compared\nagainst the pseudo label ofx\nu, leading to the calculation of the consistency\nloss in Equation (2). Formally, given our augmented and mixed batches, the\nstandard consistency loss in Equation (2)i sc h a n g e dt o :\nLm ¼\nX\n/C32f a;b;ug\nH ^p; Ωðx0/C3 Þ\n/C0/C1\n: ð8Þ\nWe additionally utilizexa, comprising a sole augmented rendition of the\nquestion and its predicted labels, excluding the application of SoftMix. This\nnot only offers a subtle enhancement in performance but also contributes to\nheightened stability:\nLc ¼ H ^p; ΩðxaÞðÞ : ð9Þ\nThe ChemMatch model is optimized byLbs þ Lm þ Lc.\nResults\nBaselines\nWe ﬁrst compare ChemMatch with a basicSupervised baseline model,\nwhich is trained by using only the human-annotated dataset. In addition, we\ncompare ChemMatch with a biomedical baseline PubMedQA19 that\nleverages labeled data to produce static pseudo-labels for the unlabeled\nsamples, which are subsequently utilized to train the classiﬁcation model.\nPubMedQA is a multi-phaseﬁnetuning process, while our model follows\nend-to-end fashion with our uniquesoftmix and rebalance operations.\nWe also compare with strong semi-supervised baselines:\nFixMatch\n44 is a classic semi-supervised baseline that uses pseudo-\nlabeling on a weakly augmented version of the data and then enforces\nconsistency between these pseudo-labels and the predictions on a strongly-\naugmented version of the same data. The pseudo-label is only retained if the\nmodel produces a high-conﬁdence prediction.\nFreeMatch\n38 adjusts the conﬁdence threshold of pseudo labels in a self-\nadaptive manner according to the model’s learning status.\nSoftMatch37 derives a truncated Gaussian function to weight pseudo\nsamples based on their conﬁdence, which can be viewed as a soft version of\nthe conﬁdence threshold.\nOur model differs from the above approaches by leveraging all pseudo\nlabels and aim to enhance their accuracy.\nRemixMatch39 introduces a remix operation in latent space that\ncombines multiple cases to create new learning inputs and targets. This\napproach fundamentally differs from our SoftMix operation, which mixes\ninformation within a single case while maintaining the same target.\nWe also include open-source LLM baselines, such as Llama2-70B\n45,\nGPT-3.5, and GPT-4.\nDatasets\nOur ChemMatch model, though originally designed to address the imbal-\nance phenomenon prevalent in scholarly papers, is applicable to a variety of\nother contexts where similar imbalances occur. Several imbalanced text\nclassiﬁcation benchmark datasets have been developed reﬂecting these\nscenarios. To evaluate our model’s effectiveness beyond the specialized\nchemical question answering dataset, we tested it on established benchmark\nclassiﬁcation datasets\n46. The AG News dataset, extracted from AG’sc o r p u s\nof news articles on the web, utilizes thefour largest classes from this corpus.\nThe Yahoo Answers dataset is a topic classiﬁcation dataset featuring ques-\ntions and best answers from Yahoo!’s ten largest categories, including the\nquestion title, content, and best answer. The Yelp-5 dataset originates from\nthe Yelp Dataset Challenge in 2015. We adopt the task of predicting the\nnumber of stars given by the user. Lastly, the Amazon-5 dataset from the\nStanford Network Analysis Project comprises Amazon reviews. The data\nused for classiﬁcation includes both the review title and its content.\nEvaluation metrics\nEach experiment is repeatedﬁve timeswith different data splits following\nprevious works19,47, and we report the average test accuracy and weighted\nF1 scores. Accuracy reﬂects the proportion of accurate predictions among\nall instances, yet it overlooks the precision of individual classes. On the\ncontrary, weighted-F1 computes metrics for each label and determines their\naverage, considering the number of true instances for each label in the\nweighting process.\nExperimental results\nOutperforming similar-scale models. In Table3, we present the per-\nformance metrics of recent baselines and our model across diverse dataset\nsettings. The imbalance ratioγ represents how many times larger the size\nof the biggest class is compared to the smallest one, andγ ranges from 5\n(Setting 1) to 48 (Setting 4). A few observations can be made from\nthe table.\nFirstly, semi-supervised baselines surpass the naive supervised\nbaselines in most scenarios, which shows the necessity of learning\nfrom unlabeled cases. Secondly, it is valuable to have a larger pool of\nTable 3 | Performance of different models on datasets of various labeled imbalance ratioγ\nModel Setting 1 (500/40k, γ = 5) Setting 2 (2k/20k, γ = 23) Setting 3 (2k/40k, γ = 23) Setting 4 (4k/40k, γ = 48)\nAccuracy F1 Accuracy F1 Accuracy F1 Accuracy F1\nSupervised 66.84 66.71 69.80 68.57 69.80 68.57 70.62 68.59\nPubMedQA 67.56 67.30 71.20 69.37 72.12 69.45 72.30 67.72\nFixMatch 67.64 64.74 71.40 69.46 72.34 69.14 72.98 68.96\nSoftMatch 70.16 67.38 71.53 69.71 72.24 69.75 73.54 68.99\nFreeMatch 69.56 66.42 72.14 70.23 72.60 69.72 72.68 68.13\nChemMatch 71.36 68.55 73.12 70.84 73.84 70.93 74.28 71.06\n- Improvement (%) +2.59% +3.20% +1.36% +0.87% +1.71% +1.74% +2.20% +4.30%\nThe numbers in the bracket are the number of supervised and unsupervised cases in training set, respectively. Numbers inbold denote signiﬁcant improvements over the FreeMatch baseline, as determined\nby a two-tailed paired t-test with a p-value < 0.05. This notation is consistently used throughout the tables. The improvement percentage is comparedto the overall best baseline, FreeMatch.\nhttps://doi.org/10.1038/s42004-024-01394-x Article\nCommunications Chemistry|             (2025) 8:4 6\nunsupervised data and supervised data. For instance, comparing Setting 2\nand Setting 3, even though the supervised count remains the same,\nthere is an improvement or consistent performance when more unsu-\npervised data is added. Thirdly, the ChemMatch model consistently\noutperforms other models across all conﬁgurations. While Accuracy\nprovides a measure of overall performance, the F1 score additionally\ncaptures the equilibrium of accuracy across various classes. Our model\nexcels in both these metrics, thus highlighting its resilience across\ndiverse data distributions and emphasizing its effective utilization of both\nsupervised and unsupervised data.\nPerformance on general-domain datasets. The scenario of imbalanced semi-\nsupervised learning is commonly observed in real-world settings\n48– 51.T o\nverify the generalizability of our model, we further evaluated our model on\nfour benchmark datasets. To simulate an imbalanced setting, we set the\nimbalance ratioγ of 5 for labeled data and 150 for unlabeled data, a common\nsetting in image classiﬁcation\n37. For example, for AG News dataset in setting\n1, the case numbers across four categories are [40, 23, 13, 8]. In setting 2, the\nnumber distribution is [200, 116, 68, 40]. The results are shown in Table4,\nwhere our model outperforms most of the other baselines across different\nsettings. For instance, our model achieves 87.38% accuracy with 200 labeled\ninstances, outperforming FreeMatch’s 86.53%. These results demonstrate\nthe generalization and robustness of our ChemMatch model in handling\nimbalances in different domains and settings.\nComparison with large language models. We compared our model\nwith Llama2-70B, Meditron-70b 52,G P T - 3 . 5 ,G P T - 4a c r o s s2 0 0\nsampled cases, where the chain-o f-thought prompt is in the Sup-\nplementary Note 1. The accuracy and F1 results are shown in Fig.4.\nOur model surpasses the three baseline models probably because it is\ntrained explicitly on the chemical corpus, hence, it’s enriched with\ncorresponding knowledge. The advantages of our model become\nmore evident when considering th e large size and great computa-\ntional source of LLMs. Additionally, Meditron-70b fails to provide\nanswers to the questions and instead simply repeats them, as\nobserved in related queries at https://github.com/epfLLM/meditron/\nissues/13, demonstrating that further training is required for\nthis model.\nPrompt discussion. We employed various strategies when testing LLMs,\nincluding chain-of-thoughts and few-shot learning, in designing the\nprompts. However, we observed that neither strategy substantially\nenhanced performance.For few-shot learning, the limited improvement\nmay be attributed to the dissimilarity in content among the test\nquestions, indicating a necessity for more targeted selection of in-context\nlearning cases. We assume the reasoni st h a tt h el i m i t a t i o n so fL L M si n\nchemical QA task are more related to a deﬁcit in domain-speciﬁc scientiﬁc\nknowledge rather than the thinking strategy. This insight directs us towards\nstrengthening LLMs’ domain-speciﬁc scientiﬁc knowledge in chemical\nQA tasks.\nCase study. We also give an error analysis on the output of LLMs in Fig.5.\nGenerally, we observed that both GPT-3.5 and GPT-4 often provide\nambiguous ‘maybe’ answers, even when the input clearly warrants a deﬁ-\nnitive‘yes’or‘no’response, For instance, the conclusion is initially presented\nin the input as‘the proton can be encapsulated’. The subsequent details then\ndelve into the speciﬁc conditions under which the proton can or cannot be\nencapsulated, creating confusion for the LLMs, as evidenced by the outlined\nreasons. These examples highlight the inconsistency between the LLM’s\nreasoning process and its ﬁnal conclusions, which points to further\nimprovement.\nDISCUSSION\nAblation study\nIn Table5, we assess the contributions of ChemMatch’s main components\nin four Settings. Take setting 4 as anexample, the full ChemMatch model,\nwith an accuracy of 74.28% and F1 score of 71.06%, outperforms its variants.\nExcluding label rebalancing resultsin reduced performance, with accuracy\nand F1 scores dropping to 73.96% and 70.32%, respectively. The perfor-\nmance drops even further when pseudo-label calibration is removed. These\nﬁndings underscore the importance of class balancing. Finally, the lack of\nthe SoftMix component (whenL\nm is absent and onlyLc is utilized) hurts\nperformance in both metrics, underscoring the beneﬁts of augmenting\ndiversity. The comparative performance is consistent throughout different\ndatasets, which demonstrates the robust effectiveness of our proposed\nmodules.\nTable 4 | Accuracy (%) performance of baselines and our ChemMatch on four classiﬁcation benchmark datasets. withγ = 5 for\nlabeled data andγ = 150 for unlabeled data\nModel AG News Amazon Yahoo Yelp\n# Labels 40 200 250 1000 500 2000 250 1000\nPubMedQA 82.63 84.97 50.37 53.32 66.63 67.20 54.70 57.78\nFixMatch 82.68 86.20 50.59 54.68 67.37 67.37 54.07 57.33\nSoftMatch 83.51 85.91 50.39 54.54 66.59 68.33 54.62 56.40\nFreeMatch 84.34 86.53 51.32 54.32 66.03 68.28 53.46 55.81\nChemMatch 85.51 87.38 52.10 55.49 68.52 68.20 55.68 57.54\nThe # Labels indicate the count of the most populous category. Numbers inbold denote signiﬁcant improvements over the FreeMatch baseline, as determined by a two-tailed paired t-test with a p-value\n< 0.05.\nFig. 4 | The accuracy (%) and F1 scores (%) of our model and LLMs on the\nScholarChemQA dataset.It can be seen that our ChemMatch outperforms other\nbaselines, signiﬁcantly surpassing Llama2 and GPT-3.5.\nhttps://doi.org/10.1038/s42004-024-01394-x Article\nCommunications Chemistry|             (2025) 8:4 7\nEffectiveness of label rebalance. We next analyze the three compo-\nnents in detail, starting with a simple numerical study. Theﬁrst com-\nponent is label rebalancing. We examine the inﬂuence of the size ofβ on\nthe class-based weight, as shown in Equation (3). Whenβ = 0, it corre-\nsponds to no re-weighting, and asβ approaches 1, it corresponds to re-\nweighting by inverse class frequency. The proposed concept of the\neffective number of samples enables us to use the hyperparameterβ to\nsmoothly adjust the class-balanced term between no re-weighting and re-\nweighting by inverse class frequency. In Fig.6b, we demonstrate that the\nclass-balanced term always improves the performance of the original loss,\nand larger values ofβ yield more signiﬁcant performance gains.\nEffectiveness of Pseudo-label calibration. In Section 3.3, we introduce\ntwo steps to align the prediction distribution for unlabeled cases with the\nground truth distribution. Herein, we present the histogram distribution\nof the ground truth labels, the predictions from the baseline FixMatch,\nand our model ChemMatch in Fig.7. It can be observed that FixMatch\nhas signiﬁcantly fewer predictions for the minority class, while our\nChemMatch produces a prediction distribution similar to the\nground truth.\nWe also calculate the KL divergence\n53 between predictions and ground\ntruth labels. The KL loss between FixMatch and the labels is 0.0467, while it\nis 0.0028 for ChemMatch. This indicates that the distribution of Pred2 is\ncloser to the target distribution compared to Pred1, as reﬂected by the lower\nKL divergence value. This demonstrates that our techniques for pseudo-\nlabel calibration are effective, and the predicted labels are of good quality,\nclosely resembling the ground truth labels. This provides a favorable con-\ndition for semi-supervised learning.\nEffectiveness of SoftMix operation. Apart from the numerical study,\nwe conduct visualizations to provide a more intuitive understanding of\nthe proposed structure. Firstly, we demonstrate the effectiveness of the\nSoftMix operation through t-SNE projection. In Supplementary Fig. S1,\nwe project the original labeled text, the augmented text using back\ntranslation, and the hidden vector obtained by the SoftMix operation,\nrespectively. It can be seen that the augmented text is close to the original\ntext, which means that the back translation operation brings limited\ndiversity to the training corpus. Then, for the gray nodes projected by the\nhidden cases generated by the SoftMix operation, they are farther away\nfrom both the original document and the augmentation in the input\nspace, and are more dispersed in the latent space. This indicates that the\nSoftMix operation can generate more diverse and informative repre-\nsentations compared with the input space augmentation, potentially\nleading to improved model performance.\nFig. 5 | Error analysis.Supporting fact for the answer is highlighted.\nTable 5 | Ablation study of ChemMatch\nModel Setting 1 Setting 2 Setting 3 Setting 4\nAccuracy F1 Accuracy F1 Accuracy F1 Accuracy F1\nChemMatch 71.36 68.55 73.12 70.84 73.84 70.93 74.28 71.06\nw/o Label Rebalance 70.76 67.79 72.75 70.13 73.56 70.28 73.96 70.32\nw/o Pseudo-label Calibration 70.43 66.84 72.18 69.02 72.78 68.90 73.27 69.02\nw/o SoftMix 70.55 67.10 72.17 69.25 72.94 69.19 73.36 69.29\nw/ RemixMatch 64.86 64.34 66.61 65.87 66.85 66.86 67.79 66.30\nNumbers inbold denote signiﬁcant improvements over the w/o Label Rebalance, as determined by a two-tailed paired t-test with ap-value < 0.05.\nhttps://doi.org/10.1038/s42004-024-01394-x Article\nCommunications Chemistry|             (2025) 8:4 8\nComparison with RemixMatch baseline. We also tried an alternative\nremix operation proposed by Ref.39. It introduces a remix operation in\nlatent space that combines multiple cases to create new learning inputs\nand targets. This approach differs from our SoftMix operation, which\nmixes information within a single case while maintaining the same target.\nAs shown in Table5, with the remixmatch component the model per-\nforms poorly on textual tasks. This observation aligns with the previous\nﬁndings at: https://github.com/microsoft/Semi-supervised-learning/\nblob/main/results/usb_nlp.csv. A plausible explanation is that, in the\ntext domain, the semantic vector representations cannot be mixed\nsimilarly to the computer vision domain, which can cause confusion in\nthe training process and interfere with performance.\nComparison with PubMedQA baseline. PubMedQA is a model pro-\nposed by PubMedQA19. One signiﬁcant difference is that it focuses on the\nbiomedical domain, whereas the performance of language models in the\nchemistry domain remains largely unexplored. In terms of data collec-\ntion, our work encompasses papers from various sources such as Elsevier\nand Springer, which are diverse and cover multiple topics. Regarding\nmodel design, PubMedQA undergoes a multi-phase training process\nusing either ground truth labels or pseudo labels. In contrast, our model\nfollows an end-to-end approach, where it is trained simultaneously with\nboth labeled and pseudo labels. We also introduce a module that includes\na softmix augmentation operation to more effectively utilize imbalanced\ndatasets, which signiﬁcantly outperforms PubMedQA.\nConclusion and broader impacts\nIn this study, we introduce the a large-scale chemical question-answering\ndataset, gathered from academic sources. Given the inherent imbalance of\nthe data attributes, we further present ChemMatch, a question-answering\nmodel speciﬁcally adapted for imbalanced semi-supervised learning. This\nmodel introduces label-rebalance andpseudo-calibration operations to\naddress the imbalance issue. Experimental results show that ChemMatch\nsurpasses recent classiﬁcation baselines and LLMs. Our dataset holds the\npotential for additional scientiﬁc investigation. For instance, it can be used\nfor testing domain-speciﬁc language models on their understanding of\ncomplex chemical concepts. It can also examine chemical research infor-\nmation retrieval systems, particularly in matching questions with corre-\nsponding documents.\nData availability\nThe code and data sample is publicly available athttps://github.com/iriscxy/\nchemmatch. Our dataset is drawn from various academic platforms, each\nhaving distinct data protection policies. A signiﬁcant portion of our dataset\nis sourced from the lens.org34 website, a platform that actively promotes the\ndistribution and sharing of data. As per the guidelines detailed athttps://\nabout.lens.org/policies/#attribution, we are allowed to release the dataset\nwith the Lens ID. The extensive scale of 26,000 cases holds signiﬁcant\npotential and is expected to provide considerable beneﬁts to the community.\nAs for data sourced from other sites likeElsevier, which maintains strict data\nusage policies at https://www.elsevier.com/about/policies/copyright/\npermissions, we release the DOI of theﬁles within our dataset alongside\nour data collection code. This approach enables users to recollect our data\ncollection steps, but always within the constraints set by the original data\nproviders’permissions.\nCode availability\nThe code and data sample is publicly available athttps://github.com/iriscxy/\nchemmatch.\nReceived: 13 September 2024; Accepted: 12 December 2024;\nReferences\n1. Hu, S., Zou, L., Yu, J. X., Wang, H. & Zhao, D. Answering natural\nlanguage questions by subgraph matching over knowledge graphs.\nIEEE Trans. Knowl. Data Eng.30, 824– 837 (2017).\n2. Lan, Y. et al. Complex knowledge base question answering: A survey.\nIEEE Trans. Knowl. Data Eng.35, 11196– 11215 (2022).\nFig. 7 | Distribution of ground truth labels, predictions from FixMatch and our\nChemMatch. FixMatch has signiﬁcantly fewer predictions for the minority class,\nwhile our ChemMatch produces a prediction distribution similar to the ground truth.\nFig. 6 | Impact of the class-balanced term on training and accuracy. aVisualization of the proposed class-balanced termð1 /C0 βÞ=ð1 /C0 βny Þ, whereny is the number of\nsamples in the ground-truth class.b Accuracy rate when trained with and without the class-balanced term. The larger theβ is, the larger the improvement is.\nhttps://doi.org/10.1038/s42004-024-01394-x Article\nCommunications Chemistry|             (2025) 8:4 9\n3. Christmann, P., Saha Roy, R. & Weikum, G. Conversational question\nanswering on heterogeneous sources. InProceeding of International\nConference on Research on Development in Information Retrieval(2022).\n4. Qu, C. et al. Open-retrieval conversational question answering. In\nProceeding of International Conference on Research on Development\nin Information Retrieval(2020).\n5. Auer, S. et al. The sciqa scientiﬁc question answering benchmark for\nscholarly knowledge.Sci. Rep.13, 7240 (2023).\n6. Zheng, S., Li, Y., Chen, S., Xu, J. & Yang, Y. Predicting drug– protein\ninteraction using quasi-visual question answering system.Nat. Mach.\nIntell. 2, 134– 140 (2020).\n7. Jin, Q. et al. Hiddenﬂaws behind expert-level accuracy of multimodal\ngpt-4 vision in medicine.ArXiv (2024).\n8. Maharjan, J. et al. Openmedlm: prompt engineering can out-perform\nﬁne-tuning in medical question-answering with open-source large\nlanguage models.Sci. Rep.14, 14156 (2024).\n9. Mahbub, M. et al. Question-answering system extracts information on\ninjection drug use from clinical notes.Commun. Med.4, 61 (2024).\n10. Clark, C. et al. Boolq: Exploring the surprising difﬁculty of natural yes/\nno questions. InProceeding of of North American Chapter of the\nAssociation for Computational Linguistics(2019).\n11. Wadden, D. et al. Fact orﬁction: Verifying scientiﬁc claims. In\nProceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 7534– 7550 (2020).\n12. Wang, L. L. Using machine learning to verify scientiﬁc claims (2023).\n13. Ghoshal, A. et al. Quaser: Question answering with scalable extractive\nrationalization. InProceeding of International Conference on\nResearch on Development in Information Retrieval(2022).\n14. Garcia-Silva, A. et al. Spaceqa: Answering questions about the design\nof space missions and space craft concepts. InProceeding of\nInternational Conference on Research on Development in Information\nRetrieval (2022).\n15. Peretz, G., Arraf, M. & Radinsky, K. What if: Generating code to answer\nsimulation questions in chemistry texts. InProceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development\nin Information Retrieval, 1335– 1344 (2023).\n16. G o l d s m i t h ,E .J . ,M e n d i r a t t a ,S . ,A k e l l a ,R .&D a h l g r e n ,K .N a t u r a ll a n g u a g e\nquery in the biochemistry and molecular biology domains based on\ncognition search™. Summit Transl. Bioinforma.2009, 32 (2009).\n17. Krithara, A., Nentidis, A., Bougiatiotis, K. & Paliouras, G. Bioasq-qa: A\nmanually curated corpus for biomedical question answering.Sci. Data\n10, 170 (2023).\n18. Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A. & Smit, B.\nLeveraging large language models for predictive chemistry.Nat.\nMach. Intell.6, 161– 169 (2024).\n19. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. & Lu, X. Pubmedqa: A dataset\nfor biomedical research question answering. InProceeding of\nEmpirical Methods in Natural Language Processing(2019).\n20. Jin, D. et al. What disease does this patient have? a large-scale open\ndomain question answering dataset from medical exams.Appl. Sci.\n11, 6421 (2021).\n21. Wang, H., Zhou, L., Zhang, W. & Wang, X. Literatureqa: A qestion\nanswering corpus with graph knowledge on academic literature. In\nProceeding of CIKM(2021).\n22. Laurent, J. M. et al. Lab-bench: Measuring capabilities of language\nmodels for biology research.arXiv preprint arXiv:2407.10362(2024).\n23. Pan, J. Large language model for molecular chemistry.Nat. Comput.\nSci. 3,5 – 5 (2023).\n24. Tibo, A., He, J., Janet, J. P., Nittinger, E. & Engkvist, O. Exhaustive\nlocal chemical space exploration using a transformer model.Nat.\nCommun. 15, 7315 (2024).\n25. M. Bran, A. et al. Augmenting large language models with chemistry\ntools. Nature Machine Intelligence1– 11 (2024).\n26. Oniani, D. et al. Emerging opportunities of using large language\nmodels for translation between drug molecules and indications.Sci.\nRep. 14, 10738 (2024).\n27. Wei, Z. et al. Chemistryqa: A complex question answering dataset\nfrom chemistry (2020).\n28. Zhou, X., Nurkowski, D., Mosbach, S., Akroyd, J. & Kraft, M. Question\nanswering system for chemistry.J. Chem. Inf. Model.61, 3868– 3880\n(2021).\n29. Mirza, A. et al. Are large language models superhuman chemists?\narXiv preprint arXiv:2404.01475(2024).\n30. M. Bran, A. et al. Augmenting large language models with chemistry\ntools. Nat. Mach. Intell.6, 525– 535 (2024).\n31. Guo, T. et al. What can large language models do in chemistry? a\ncomprehensive benchmark on eight tasks.Adv. Neural Inf. Process\nSyst. 36, 59662– 59688 (2023).\n32. Lu, P. et al. Learn to explain: Multimodal reasoning via thought chains\nfor science question answering.Adv. Neural Inf. Process. Syst.35,\n2507– 2521 (2022).\n33. Huang, C., Li, Y., Loy, C. C. & Tang, X. Learning deep representation\nfor imbalanced classiﬁcation. InProceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 5375– 5384 (2016).\n34. Jefferson, O. A. et al. Mapping the global inﬂuence of published\nresearch on industry and innovation.Nat. Biotechnol.36,3 1– 39\n(2018).\n35. Toutanvoa, K. & Manning, C. D. Enriching the knowledge sources\nused in a maximum entropy part-of-speech tagger. InProceeding of\nEmpirical Methods in Natural Language Processing\n(2000).\n36. Cui, Y., Jia, M., Lin, T.-Y., Song, Y. & Belongie, S. Class-balanced loss\nbased on effective number of samples. InProceeding of International\nConference on Computer Vision and Pattern Recognition(2019).\n37. Chen, H. et al. Softmatch: Addressing the quantity-quality tradeoff in\nsemi-supervised learning. InProceeding of International Conference\non Learning Representations(2023).\n38. Wang, Y. et al. Freematch: Self-adaptive thresholding for semi-\nsupervised learning.Proceeding of International Conference on\nLearning Representations(2023).\n39. Berthelot, D. et al. Remixmatch: Semi-supervised learning with\ndistribution alignment and augmentation anchoring.Proceeding of\nInternational Conference on Learning Representations(2020).\n40. Gan, Y. et al. Towards robustness of text-to-sql models against\nsynonym substitution. InProceeding of Association for\nComputational Linguistics, 2505– 2515 (2021).\n41. Zeiler, M. D. & Fergus, R. Visualizing and understanding convolutional\nnetworks. InProceeding of ECCV(2014).\n42. Verma, V. et al. Manifold mixup: Better representations by\ninterpolating hidden states. InProceeding of International Conference\non Machine Learning(2019).\n43. Chen, X. et al. Improving the robustness of summarization systems\nwith dual augmentation.Proceeding of Association for Computational\nLinguistics (2023).\n44. Sohn, K. et al. Fixmatch: Simplifying semi-supervised learning with\nconsistency and conﬁdence. Proc. Neural Inf. Process. Syst.33,\n596– 608 (2020).\n45. Touvron, H. et al. Llama 2: Open foundation andﬁne-tuned chat\nmodels. arXiv preprint arXiv:2307.09288(2023).\n46. Zhang, X., Zhao, J. & LeCun, Y. Character-level convolutional\nnetworks for text classiﬁcation. Proceeding of Neural Information\nProcessing Systems(2015).\n47. Lin, M. et al. Improving model fairness in image-based computer-\naided diagnosis.Nat. Commun.14, 6261 (2023).\n48. Tzaban, H. et al. Product bundle identiﬁcation using semi-supervised\nlearning. InProceeding of International Conference on Research on\nDevelopment in Information Retrieval(2020).\nhttps://doi.org/10.1038/s42004-024-01394-x Article\nCommunications Chemistry|             (2025) 8:4 10\n49. Kim, J. et al. Distribution aligning reﬁnery of pseudo-label for\nimbalanced semi-supervised learning.Proc. Neural Inf. Process. Syst.\n33, 146567– 14579 (2020).\n50. Lee, H., Shin, S. & Kim, H. Abc: Auxiliary balanced classiﬁer for class-\nimbalanced semi-supervised learning.Proc. Neural Inf. Process. Syst.\n34, 7082– 7094 (2021).\n51. Wei, C., Sohn, K., Mellina, C., Yuille, A. & Yang, F. Crest: A class-\nrebalancing self-training framework for imbalanced semi-supervised\nlearning. InProceeding of International Conference on Computer\nVision and Pattern Recognition(2021).\n52. Chen, Z. et al. Meditron-70b: Scaling medical pretraining for large\nlanguage models.arXiv preprint arXiv:2311.16079(2023).\n53. Hershey, J. R. & Olsen, P. A. Approximating the Kullback Leibler\ndivergence between Gaussian mixture models. In2007 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing-ICASSP’07, vol. 4, IV– 317 (IEEE, 2007).\nAuthor contributions\nXiuying Chen and Tairan Wang conceived the ideas, conducted the\nexperiments, and co-wrote the paper. Taicheng Guo, Kehan Guo, Juexiao\nZhou, and Haoyang Li collected and labeled the dataset. Zirui Song per-\nformed additional experiments during the rebuttal phase. Xin Gao and\nXiangliang Zhang contributed to reﬁning and polishing the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s42004-024-01394-x\n.\nCorrespondenceand requests for materials should be addressed to\nXiuying Chen.\nPeer review information Communications Chemistry thanks Sutanay\nChoudhury, Qingyun Wang, and the other, anonymous, reviewers for their\ncontribution to the peer review of this work.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s42004-024-01394-x Article\nCommunications Chemistry|             (2025) 8:4 11",
  "topic": "Biomedicine",
  "concepts": [
    {
      "name": "Biomedicine",
      "score": 0.7014048099517822
    },
    {
      "name": "Computer science",
      "score": 0.6903485059738159
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.56753009557724
    },
    {
      "name": "Question answering",
      "score": 0.5573378801345825
    },
    {
      "name": "Data science",
      "score": 0.5509886741638184
    },
    {
      "name": "Field (mathematics)",
      "score": 0.5387338995933533
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5154936909675598
    },
    {
      "name": "Natural language processing",
      "score": 0.342536985874176
    },
    {
      "name": "Information retrieval",
      "score": 0.34198006987571716
    },
    {
      "name": "Bioinformatics",
      "score": 0.09716299176216125
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I71920554",
      "name": "King Abdullah University of Science and Technology",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I107639228",
      "name": "University of Notre Dame",
      "country": "US"
    }
  ]
}