{
  "title": "TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction",
  "url": "https://openalex.org/W4389520460",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2130562836",
      "name": "Junyi Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114209507",
      "name": "Liangzhi Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166240248",
      "name": "Tong Xiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104884824",
      "name": "Bowen Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109275201",
      "name": "Yiming Qian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385573970",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4401726216",
    "https://openalex.org/W4321009772",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4389518913",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W2136189984",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4366735603",
    "https://openalex.org/W4376122773",
    "https://openalex.org/W4378465262",
    "https://openalex.org/W2972801466",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W3210968241",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W4385570365",
    "https://openalex.org/W4378498632",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4312091849",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W3148437589",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2144211451",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4379469942",
    "https://openalex.org/W4383468911",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4388778348",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4229005866"
  ],
  "abstract": "Since ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge obtained by retrieval augmentation. One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs. To mitigate this, we propose a token compression scheme that includes two methods: summarization compression and semantic compression. The first method applies a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths and reduce token size by doing summarization. The second method further compresses the token size by removing words with lower impact on the semantic. In order to adequately evaluate the effectiveness of the proposed methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB) focusing on food recommendation for women around pregnancy period or infants. Our summarization compression can reduce 65% of the retrieval token size with further 0.3% improvement on the accuracy; semantic compression provides a more flexible way to trade-off the token size with performance, for which we can reduce the token size by 20% with only 1.6% of accuracy drop.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9796–9810\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTCRA-LLM: Token Compression Retrieval Augmented Large Language\nModel for Inference Cost Reduction\nJunyi Liu\n , Liangzhi Li\n , Tong Xiang\n , Bowen Wang\n ,\nYiming Qian\nMeetyou AI Lab,\n Xiamen Key Laboratory of Women’s Internet Health Management,\nOsaka University,\n Agency for Science, Technology and Research (A*STAR)\n{liujunyi, liliangzhi, xiangtong}@xiaoyouzi.com,\nbowen.wang@is.ids.osaka-u.ac.jp, qiany@ihpc.a-star.edu.sg\nAbstract\nSince ChatGPT released its API for public\nuse, the number of applications built on top\nof commercial large language models (LLMs)\nincrease exponentially. One popular usage of\nsuch models is leveraging its in-context learn-\ning ability and generating responses given user\nqueries leveraging knowledge obtained by re-\ntrieval augmentation. One problem of deploy-\ning commercial retrieval-augmented LLMs is\nthe cost due to the additionally retrieved con-\ntext that largely increases the input token size\nof the LLMs. To mitigate this, we propose a\ntoken compression scheme that includes two\nmethods: summarization compressionand se-\nmantic compression. The first method applies\na T5-based model that is fine-tuned by datasets\ngenerated using self-instruct containing sam-\nples with varying lengths and reduce token size\nby doing summarization. The second method\nfurther compresses the token size by remov-\ning words with lower impact on the semantic.\nIn order to adequately evaluate the effective-\nness of the proposed methods, we propose and\nutilize a dataset called Food-Recommendation\nDB (FRDB) focusing on food recommendation\nfor women around pregnancy period or infants.\nOur summarization compression can reduce\n65% of the retrieval token size with further\n0.3% improvement on the accuracy; semantic\ncompression provides a more flexible way to\ntrade-off the token size with performance, for\nwhich we can reduce the token size by 20%\nwith only 1.6% of accuracy drop.\n1 Introduction\nWith the increase in computing power and accumu-\nlation of enormous text data, large language mod-\nels (LLMs) such as ChatGPT (OpenAI, 2023b) and\nGPT-4 (OpenAI, 2023a) have shown impressive\nperformance in dialogue-based question-answering\n(QA), allowing them to interact with users fluently.\nIn open-domain QA where the models are engaged\nCorresponding author.\nin casual conversations with users, LLMs exhibit\nastonishing performance by leveraging strong in-\ncontext learning ability. However LLMs may pro-\nduce vague responses or incorrect answers in cer-\ntain specialized domains, owing to the absence\nof relevant knowledge or a restricted scope of\ninformation acquired during the training stage,\nwhich might potentially result in untruthful answers\nand even cause physical damages to users (Xiang\net al., 2023). For QA in such domains, retrieval-\naugmented generation (RAG) (Lewis et al., 2020),\nwhere the system retrieves external knowledge be-\nforehand and then utilizes LLMs to generate an-\nswers leveraging retrieved knowledge, can greatly\nreduce the hallucinations generated (Shi et al.,\n2023; Shuster et al., 2021).\nMany current commercial LLMs are black-box\nmodels, where the model architectures and the\nweight information are not disclosed. These LLMs\nown superior text comprehension abilities, yet in\nmany cases they can only output desired answers\nthrough complicated prompt engineering. On the\nother hand, deploying open-source LLMs to local\nservers is resource-intensive, in contrast to deploy-\ning smaller models such as T5 (Raffel et al., 2020).\nSome commercial LLMs like GPT-3.5-turbo (Ope-\nnAI, 2023c) and GPT-4 offer access through API\ncalls; however, these models charge users based\non the size of input and output 1. For individuals\nor companies looking to create their own services\nusing LLMs through API calls, utilizing commer-\ncial ones can be resource-consuming if requests are\nmade frequently. Therefore, it is necessary to mini-\nmize the number of input tokens while maintaining\noptimal performance during the API calls.\nIn this work, we propose a token compres-\n1As of May 5th, 2023, GPT-4, capable of processing up to\n8k tokens, charges $0.03 per thousand input tokens and $0.06\nper thousand output tokens; GPT-4-32K which can process\n32k tokens, charges $0.06 per thousand input tokens and $0.12\nper thousand output tokens. GPT-3.5-turbo charges $0.002\nper thousand tokens.\n9796\nQuestion LLM\nRetriever\n Context Answer\nQuestion LLMAnswer\nQuestion LLM\nTop\n-\nk \nRetriever\n Context Answer\nNSP\nToken \nCompressor\nLarge LanguageModel\nRetrieval Augmented Large LanguageModel\nToken CompressionRetrieval Augmented Large LanguageModelknowledge database\nknowledge database\nFigure 1: Illustration of different ways to utilize LLMs for QA. Top: directly using LLM. Middle: using a\nretrieval-augmented LLM. Bottom: using retrieval-augmented LLM with our proposed token compression methods.\nsion scheme specifically designed for the retrieval-\naugmented LLMs (shown in Figure 1), namely,\nToken Compression Retrieval Augmented Large\nLanguage Model (TCRA-LLM). Our proposed\nscheme can reduce up to 65% of the token size with\nadditional 0.3% improvement on accuracy when\ndoing QA on our proposed dataset called Food-\nRecommendation DB (FRDB). We propose two\napproaches to reduce the token size of the LLMs’\ninput: summarization compressionand semantic\ncompression. For summarization compression, we\nleverage self-instruct (Wang et al., 2022) scheme\nto build multiple summarization datasets with vary-\ning lengths to fine-tune the mT5 model (Xue\net al., 2020). The samples from the summarization\ndatasets are generated by GPT-3.5-turbo (OpenAI,\n2023c), which is instructed to shorten the sum-\nmary of the input sentences in an iterative manner.\nThe semantic compression approach is based on a\nsimple yet effective intuition, that removing seman-\ntically less important words in a sentence won’t\ndrastically change its semantic. Here, we deploy\na multi-lingual sentence-transformer (Reimers and\nGurevych, 2020) to encode sentences into embed-\ndings where the distances between original and\nperturbed embeddings are used to measure the se-\nmantic deviation from the original meaning. Larger\nsemantic deviation indicates that the correspond-\ning word owns more important semantic in the\nsentence. We conduct an iterative process that mea-\nsures the semantic importance of each word in the\nsentence and remove less important words.\nIn conclusion, our work has the following three\ncontributions:\n1. We construct a food recommendation QA\ndataset (Section 3) which contains domain\nknowledge that general LLMs might not have.\nThis dataset serves the purpose of evaluating\nretrieval-augmented LLMs.\n2. We propose a multi-level self-instruct scheme\n(Section 4.2) to build summarization datasets\nof different lengths.\n3. We propose two token compression methods\n(Section 4.2), both of which can reduce the\nnumber of input tokens during the API calls of\nretrieval-augmented commercial LLMs while\nmaintaining optimal performance.\n2 Related Work\nLLMs such as GPT-3 (Brown et al., 2020),\nPALM (Chowdhery et al., 2022), OPT (Zhang\net al., 2022), Bloom (Scao et al., 2022), and\nLLaMA (Touvron et al., 2023) are trained on mas-\nsive amounts of data and have demonstrated pow-\nerful comprehension capabilities. These models\nhave been deployed in a breadth of tasks and\nachieve promising results (Zhang et al., 2023;\nAshok and Lipton, 2023; Lu et al., 2023; Wang\net al., 2023; Xiang et al., 2023). One major bar-\nrier that prevents more people from participat-\ning in commercial deployment of the LLMs is\n9797\ntheir training and hosting costs. A way to re-\nduce such costs is through training smaller domain-\nspecific models such as BioMedLM (Bolton et al.,\n2022), BloombergGPT (Wu et al., 2023), and\nLawGPT (Nguyen, 2023). Such domain-specific\ntraining enables smaller LLMs to be applied to cer-\ntain fields but still requires huge investment. For\ninstance, BloombergGPT is trained on 512 40GB\nA100 GPUs with the total budget being approxi-\nmately $2.7 million (Sheikh, 2023).\nAlternatively, LLMs can be used without fine-\ntuning through retrieval augmentation leveraging\nexternal data sources, where the retrieved data is\nused as supplementary information to help LLMs\nimprove logical reasoning and language genera-\ntion (Thorne et al., 2021; Izacard et al., 2022). Pre-\nvious experiments (Ram et al., 2023) show that\nadditional information can be beneficial for LLMs\nacross different model sizes. Retrieval augmenta-\ntion eliminates the cost of tuning an in-house LLM\non new data, and can be easily integrated with com-\nmercial LLM services such as ChatGPT (OpenAI,\n2023b) from OpenAI or Bard (Pichai, 2023) from\nGoogle. Many studies have shown, applying re-\ntrieval augmentation to the commercial LLMs such\nas ChatGPT allow the models to gain knowledge\nin specific domains such as natural science and\nmedicine (Soong et al., 2023; Inaba et al., 2023)\nwhich is not revealed during their training and re-\ntrieval augmentation can be further improved by\napplying more sophisticated retrievers (Shi et al.,\n2023). However, commercial LLMs all have a lim-\nitation on input lengths which put an upper ceiling\non the amount of information that can be fed into\na LLM. Later models such as GPT-4 has looser\nrestriction but the inference cost increases dras-\ntically in comparison with other models. Some\nprevious work applies template-based prompt op-\ntimization (Santra et al., 2023), which select re-\ntrieved context (Mallen et al., 2022) in an adaptive\nmanner, or uses cascading of LLMs with different\nsizes (Chen et al., 2023) to reduce the inference\ncosts. Our proposed method has no conflict with\nthese methods and can be used with them simulta-\nneously.\n3 FRDB\nWe build a Food Recommendation Dataset in Chi-\nnese called FRDB, for recommending foods that\nare safe to consume for women before/during/after\ntheir pregnancy as well as infants. It contains two\nFood type Count ↓\nEntrée 31\nVegetables 31\nSeafood 22\nSweets 22\nMedicine/Health supplement 20\nFruit 17\nGrains 16\nSoft drink 13\nCondiment 10\nMeat/Eggs 10\nSoybeans/Dried fruit 6\nDairy products 2\nTotal 200\nTable 1: Statistics of food types in FRDB.\nparts: multiple-choice (MC) QA pairs and a knowl-\nedge database. The QA pairs contain 1,000 sam-\nples that cover 200 types of food. The categories\nof foods are shown in Table 1.\nThe possible answers to the question falls into\nthree choices based on the increasing degree of rec-\nommendations ranging from 1 (avoid) to 3 (highly\nrecommend). Each type of food has five recom-\nmendation rating corresponding to five groups:\npre-pregnancy, pregnancy, postpartum, lactation,\nand infant. Additionally, we build a knowledge\ndatabase that contains 7,588 entries; details of the\nentries are shown in Table 2. The distribution of\nsentence length in the knowledge database is shown\nin Figure 2.\nMean Max Min Std.\n# of words 88 248 12 27\nTable 2: Statistics of the entries FRDB knowledge\ndatabase. Std. stands for standard deviation.\nAll the information has been verified by the\nhealth-domain professionals. During the verifica-\ntion, we remove the text that is ambiguous to the\nhuman annotators. Two samples of knowledge are\nshown in Table 3. Sample questions are available\nin the Appendix A.\n4 Method\nTypically, a retrieval-augmented LLM consists of\nthree components (shown in Figure 1), a knowledge\ndatabase, a retriever, and the LLM. The knowledge\n9798\nHigh quality knowledge Ambiguous knowledge\nConsuming mushrooms after childbirth is benefi-\ncial for postpartum recovery, constipation relief,\nand promoting lactation due to their rich B vi-\ntamins, protein, and amino acids. A moderate\namount of intake based on the recovery status is\nrecommended.\nPostpartum mothers are safe to consume a moderate amount of cake.\nCakes are easier to digest and absorb for postpartum mothers with\nweaker gastrointestinal systems. However, cakes have relatively\nsmaller nutritional diversity and they should be consumed together\nwith vegetables, fruits, and meats to make the nutrition more balanced.\nTable 3: Examples from the knowledge database. The ambiguous ones are excluded from our dataset.\n0 50 100 150 200 250\nLength of sentence\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nProbability\nFigure 2: The distribution of sentence length in FRDB\nknowledge database.\ndatabase contains all available domain-specific\nknowledge. The retriever applies the question as a\nquery to search for the relevant information from\nthe knowledge database. The retrieved informa-\ntion is then formulated as the context packaged\ntogether with questions as a prompt for LLM to\ngenerate an answer. Our proposed methods are able\nto compress the retrieved information and formu-\nlate shorter context but maintain the effectiveness\nof retrieval augmentation. In this section, we go\nthrough the pipeline of a retrieval-augmented LLM\nsystem for QA and introduce our proposed token\ncompression methods.\n4.1 Information Retrieval\nGenerally, the first step for LLM’s retrieval aug-\nmentation is knowledge retrieval. Given an user\nquery x, the retriever extracts k pieces of in-\nformation from the knowledge database D =\n{d1,d2,··· ,dm}that are most likely to be rele-\nvant to x. There are two mainstream retrieval meth-\nods: dense retrieval (Karpukhin et al., 2020; Ni\net al., 2021) and sparse retrieval (Robertson et al.,\n2009). Dense retrieval first encodes queries and\ndocuments into dense embeddings (Huang et al.,\n2013; Yi et al., 2019) using pre-trained neural en-\ncoders and then finds a query’s nearest neighbors\nin the embedding space using a relevancy measure\nsuch as cosine similarity (Yu et al., 2021). Sparse\nretrieval, on the other hand, maps queries and docu-\nments into a high-dimensional space with methods\nsuch as TF-IDF (Sparck Jones, 1972; Jones, 1973)\nand the most relevant documents are returned to\nthe user as the answer. Typical example of sparse\nretrieval is BM25 (Robertson et al., 1995).\nHere we evaluate both dense and sparse retrieval\nmethods. For dense retrieval, we follow a similar\nprocess from Huang et al. (2013): we first encode\nthe text using the GPT-embedding (OpenAI, 2022)\nprovided by OpenAI, then deploy vector database\nFAISS Index (Johnson et al., 2019) to store the\nembeddings, enabling faster manipulation on them.\nFor sparse retrieval, we deploy BM25 (Robertson\net al., 1995) which is considered the standard way.\n4.1.1 Next Sentence Prediction\nThe retrieved top-kresults Dk = {dk\n1,dk\n2,··· ,dk\nk}\n(dk\ni are the i-th retrieved elements from the original\nset Dwhere 1 ≤i ≤k) are deemed as the most\nrelevant ranked by the retrieval method. Using only\nthe top-1 result as the context is not always reliable,\nbut using more retrieved results will consume more\nspace in the input tokens, leading to higher costs;\ntherefore, to improve reliability without incurring\nadditional costs, we propose to use next-sentence\nprediction (NSP) as a second-stage ranking method.\nIt is based on an intuitive assumption that the re-\ntrieved information is more likely to be predicted\nas the next sentence of the question if it is more\nrelated to the question. The implementation of this\napproach is based on the pre-trained NSP module\nfrom BERT (Devlin et al., 2018); the selected sen-\ntence s with maximum probability from NSP is\nselected as the best result (See Equation 1).\ns= dk\ni\ni= argmax\ni\n{\np(x,dk\n1),··· ,p(x,dk\nk)\n}\n(1)\n9799\nWe conduct experiments to evaluate the im-\npact of including NSP into the retrieval-augmented\nLLM. Here we use OpenAI’s GPT-3.5-turbo as the\nbase LLM and evaluate it on the FRDB dataset\nusing the top-1 retrieval results as the context. The\nresult is shown in Table 4. There is a minor perfor-\nmance gain using NSP with both GPT-embedding\nand BM25 and thus we keep this NSP module in\nall our later experiments.\nMethod Acc. (%)\nEmbedding 89.1\nEmbedding +NSP 90.2\nBM25 83.4\nBM25+NSP 84.9\nTable 4: Performance comparison of retrieval methods\nwith and without NSP. The retrieved sentences are di-\nrectly used as context and fed into GPT-3.5-turbo. Acc.\nstands for accuracy.\nFrom the experiment, we also see that the com-\nbination of dense retrieval with the NSP approach\nobtain the highest accuracy. We tune the value of\nk by searching it from 1 to 10 and perform cor-\nresponding evaluation on the FRDB dataset. The\nexperiment results are shown in Figure 3. We find\nthat k= 3is the optimal choice and we will adhere\nto this value in all our subsequent experiments.\n1 2 3 4 5 6 7 8 9 10\nTop K\n86\n87\n88\n89\n90\n91Accuracy (%)\nFigure 3: Evaluation of the retrieval performance using\nGPT-embedding and NSP with different choices of k.\n4.2 Token Compression\nThe retrieval text is usually long and easily con-\nsume a huge amount of space from the input tokens\nduring the API calls while using commercial LLMs.\nIn order to mitigate this, we propose two methods\nto compress the retrieved text. The first one is\nthe summarization compression which produces\nshorten the original text by rephrasing. The second\nmethod is semantic compression which we perturb\nthe original sentence and rank the impact of the se-\nmantic change from each word in the sentence. The\nwords with lower semantic impact on the sentence\nare removed.\n4.2.1 Summarization Compression\nSummarization models like the mT5 model (Xue\net al., 2020) have been widely used in many ap-\nplications to shorten the input text, but they could\nnot output summary with arbitrary length due to\nthe constraint of its training data. To solve this,\nwe propose to build a summarization model that is\nable to output summary with various lengths.\nTo build such a model, we leverage the power of\nself-instruct (Wang et al., 2022) where we use GPT-\n3.5-turbo to generate training datasets. The proce-\ndure of the data generation is shown in Figure 4.\nFirst, we start with a text xfrom the dataset, then\npack it with additional prompt instruction as we\nillustrated in Figure 4 and send it to GPT-3.5-turbo\nto generate a summary. If the length of the sum-\nmary meets requirements, the procedure is ended;\notherwise, a follow-up prompt will instruct GPT-\n3.5-turbo to further shorten the summary to the\ndesired length. By doing this, we build a collection\nof training datasets with different summary length.\nWe build three datasets that are 30%, 50%, and\n70% of their original length. Each dataset is used to\nfine-tune one summary model independently. We\nrandomly extract from FRDB and generate 400, 50,\nand 50 samples for training, validation, and testing\nrespectively. Training on the generated datasets not\nonly enables the model to produce summaries of\nthe desired length, but also familiarizes the model\nwith domain-specific information by doing further\ndomain adaptation (Gururangan et al., 2020).\n4.2.2 Semantic Compression\nWe propose another compression method based on\nperturbations of the original sentence and rank-\ning the impact of the semantic importance for\neach word in the sentence where words with\nless importance will be removed. We deploy a\nmulti-lingual sentence-transformer (Reimers and\nGurevych, 2020) to encode a sentence into embed-\nding χ0. Then we iteratively remove one word\nin the sentence and obtain an updated embedding\nχi where i is the index of the word in the sen-\ntence and nis the number of words in the sentence.\nWe have a new set Lthat tracks the Euclidean\ndistance between the original and perturbed em-\nbedding L= {L2(χ0,χ1),...,L 2(χ0,χn)}. We\ndenote pj as the value of the j-th percentile in L.\nThe Lj is the new subset that removed the bottom\n9800\nSummarize the following text, the length of the \nsummary result is 30%/50%/70%\n of the original text, keep the \nfirst sentence, and directly output your answer:\nPrompt instruction\n Original input\nGPT-3.5-turbo\nSummary result\nIs the length \nof summary result \nmeets the \nrequirement?\nYes\nThe length you \ngenerated is \nlong/short, please \nregenerate a \nshorter/longer \nsummary:\nNo\nLoop input\nFinal result\nPrompt instruction\nFigure 4: Self-instruct training data generation.\nj-th percentile elements:\nLj = {ω∈L,ω >pj} (2)\nThe words corresponding to the elements in set\nLj are extracted as the context for the LLM.\n5 Evaluation\n5.1 Experiment Setup\nWe conduct studies on the FRDB dataset. The sum-\nmarization compression is based on the pre-trained\nmT5-multilingual-XLSum (Xue et al., 2020). The\nmaximum input and output length is set to 512,\nthe learning rate is 2e-5, the number of epochs\nis set to 10, the batch size is 2, and the rest of\nthe settings follow the default settings from the\nmodels. The training of the mT5 models are con-\nducted on the server that contains an AMD EPYC\n7763 CPU, 256GB RAM, and NVIDIA 4090 GPU\nwith 24GB memory. Following the method de-\nscribed in section 4.2.1, three shortened versions\nof the summarization dataset with 30%, 50%, and\n70% of its original length are generated. Each\nversion is used to fine-tune an mT5 summariza-\ntion model. The pre-trained multi-lingual sentence-\ntransformer (Reimers and Gurevych, 2020) is used\nas an embedding encoder for semantic compres-\nsion. Three compressed sentences in 30%, 50%,\nand 70% of their original length are generated. The\nGPT-3.5-turbo (OpenAI, 2023c) is used for pro-\ncessing prompts generated by our methods.\n5.2 Token Compression Results\n40 50 60 70 80\nPercentage of orignal length (%)\n50\n60\n70\n80\n90\n100Accuracy (%)\nGPT-3.5-turbo with summarization fine-tuned\nGPT-3.5-turbo with semantic compression\nGPT-3.5-turbo with random deletion\nGPT-3.5-turbo with summarization no fine-tuned\nGPT-3.5-turbo without retrieval\nGPT-3.5-turbo with retrieval without token compression\nFigure 5: Performance comparison of token compres-\nsion methods. The horizontal dashed lines represent\naccuracy from the methods that do not output variable\ntoken lengths.\nWe conduct experiments on FRDB which con-\ntains 1,000 maternity/infant food related domain-\nspecific multiple-choice questions, each of which\nonly contains one correct answer. Three sentence-\ncompression methods are evaluated in our experi-\nments: 1) random deletion which randomly deletes\nwords from a sentence, 2) summarization compres-\nsion, and 3) semantic compression.\nThe experiment results are shown in Figure 5.\nTo construct the baseline, we evaluate the GPT-3.5-\nturbo performance in two configurations: without\nretrieval and with retrieval but without token com-\npression. We observe from the results that, once\nadditional information is fed into the model as con-\ntext, the accuracy immediately improves, from 51%\nto 90.2%. This shows the benefit that retrieving do-\nmain information brings to the LLM.\nNext, we compare using the original pre-trained\nmT5 based model with using the version where\nthe model is fine-tuned on the datasets generated\nby self-instruction. With fine-tuning, the accuracy\nimproves dramatically from 60% to 90.6%. The\nsummarization model without fine-tuning output\nhas an average length of 15 words, compare with\n88, 46, and 21 words for our 70%, 50%, and 30%\nlength dataset fine-tuned model respectively. It\nshows that the summarization model might remove\ncritical information by mistake if the input text\n9801\nis on a topic that the summarization model is not\nfamiliar with. A small fine-tuning set (400 sam-\nples) is enough to help the summarization model\nadapting to the new domain.\nThe second compression method we propose\nis semantic compression. It has better flexibility\nto generate variable lengths of tokens but delivers\nlower performance than the summarization com-\npression method. The baseline for our experiment\nis random deletion where we randomly delete a\ncertain percentage of words. This random deletion\nconsistently scores lower performance than both of\nour proposed algorithms.\n5.3 Cost Reduction Comparison\nOur proposed summarization compression method\nis tested on a server with one NVIDIA 4090 GPU\n(24GB), 32GB RAM and Intel i7 CPU. The run-\ntime for such a system is on average 0.383s per sam-\nple. A similar server on the AWS, i.e., g5.2xlarge,\nhas an A10G GPU, 32GB RAM, and 8-core vCPU\nand the hourly price for such a system is $0.485. In\none hour, such a system can process approximately\n9,400 summarizations, so the cost per summariza-\ntion is $5.16e-5. Assume the system utilization rate\nis only 50%, it means that the cost per summariza-\ntion is $1.0319e-04.\nThe GPT-3.5-turbo itself does not have enough\nknowledge to precisely answer the question from\nour dataset (51% accuracy if without additional\ncontext in the prompt). Thus, both the common\nretrieval-augmented GPT-3.5-turbo and our sys-\ntem (retrieval-augmented GPT-3.5-turbo with addi-\ntional token compression) require dense retrieval\nto reach acceptable performance, and the retrieval\ncost, which is $0.0001 per 1,000 query tokens, is\nthe same for both systems. Since the original ques-\ntions are typically short, we can assume that the\naverage length of them is about 128 tokens, which\ntranslates into $1.2500e-05 per question for the\nretrieval. Assuming at full size the input has 512 to-\nkens and the output has 64 tokens, the total cost for\nthe common retrieval-augmented GPT-3.5-turbo\n(for both retrieval and QA using API calls) is about\n$8.9050e-04 per question. In comparison, our algo-\nrithm can compress the retrieved context of GPT-\n3.5-turbo to 35% of its original length, which trans-\nlates into an averagely 50% of reduction during the\nAPI calls. Thus, The cost using our token compres-\nsion system is around $6.1869e-04 per question. It\nreduces the overall costs by 30%.\n5.4 Information Entropy vs. Accuracy\nIn the next experiment, we investigate the impact of\nthe information entropy on the accuracy of different\ntoken compression methods. We measure the word\nfrequency from the FRDB dataset and use it as a\nprobabilistic model to calculate the information en-\ntropy of each word. The mean word information\nentropy is calculated on each sentence to normal-\nize the entropy. The results for the three token\ncompression methods is shown in Figure 6. The\nrandom deletion method removes words randomly\nwhich leads to the average information entropy for\ndifferent sentence lengths approximately the same.\nOn the other hand, the semantic compression algo-\nrithm removes the words that have less semantic\nmeaning. Our experiment shows that, the average\ninformation entropy goes lower as sentences be-\ncome shorter, indicating that the sentence becomes\nless compressible. Additionally, the average word\ninformation entropy is positively correlated with\nthe accuracy when semantic compression is used,\nshowing that higher information will benefit the\nmodel performance. On the contrary, the summa-\nrization compression shows distinct phenomenon.\nInstead of naively removing words, the summariza-\ntion compression compresses the original sentences\ninto different lengths by rephrasing sentences. By\ndoing this, the shortened sentences obtain lower\naverage information entropy but the accuracy stays\nat a similar level in comparison with the original\nsentences. The lower average information entropy\nindicates that sentences become more condensed\nbut the semantic of the sentences stays approxi-\nmately the same.\n5.4 5.6 5.8 6 6.2 6.4 6.6 6.8\nAverage information entropy\n65\n70\n75\n80\n85\n90\n95Accuracy (%)GPT-3.5-turbo with summarization fine-tuned\nGPT-3.5-turbo with semantic compression\nGPT-3.5-turbo with random deletion\nFigure 6: Impact of average information entropy on\naccuracy.\nNext, we investigate the impact of cosine similar-\nity between original and compressed sentences on\naccuracy and we find a positive correlation between\naccuracy and cosine similarity value. It indicates\n9802\ncloser to the original semantic meaning would pro-\nduce better accuracy.\n0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1\nAverage cosine similarity between orginal and compressed sentence\n65\n70\n75\n80\n85\n90\n95Accuracy (%)\nGPT-3.5-turbo with summarization fine-tuned\nGPT-3.5-turbo with semantic compression\nGPT-3.5-turbo with random deletion\nFigure 7: Impact of cosine similarity between original\nand compressed sentence on accuracy.\n5.5 Summarization Statistics\nOur summarization model is built based on the\npre-trained mT5 model and fine-tuned on our self-\ninstruct generated dataset. We generate three\ndatasets which are 30%, 50%, and 70% of the\nlength compared to their original text. Three differ-\nent models are fine-tuned independently. Figure 8\nshows the distribution of sentence length from our\nthree fine-tuned models. At 70% compression, the\nsummary text shifts from an average of 88 words\nto 46 words for 50% length and 21 words for 30%\nlength.\n50 100 150\nLength of sentence\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nProbability\nOriginal sentences\nSummarize to 70% length\nSummarize to 50% length\nSummarize to 30% length\nFigure 8: Distribution of sentence lengths with different\nsummarization lengths.\n5.6 Compression Rate vs. Entropy\nWe conduct a study on the compression rate of our\nmT5 model fine-tuned with the 30% length dataset.\nOur input consists of 1) the original length of the\nsentence, 2) average word entropy, and 3) accu-\nmulated word entropy. We deploy a simple linear\nregression algorithm to predict the compression\nrate. The result is shown in Fig. 9. We find that,\nthere is a positive correlation (0.31) between our\nselected input and compression rate with an RMSE\nof 11.4% and R-squared of 9.6%. This indicates the\ncompression rate of each sentence can be estimated\nprior to the summarization process.\n5 6 7 8 9 10\nAdjusted input features\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\nCompression rate Adjusted data\nFit: y=0.0663692*x\n95% conf. bounds\nFigure 9: Visualization of multi-variable linear regres-\nsion on predicting compression rate.\n5.7 Ablation Study\nFrom our experiments, we find the summarization\ncompression method delivers the best performance.\nHere we compare different retrieval methods and\ninvestigate what the optimal settings are. Four con-\nfigurations are evaluated: embedding only, BM25\nonly, embedding first then BM25, and BM25 first\nthen embedding. The first two configurations are\nstraightforward; in the third configuration, we ap-\nply the embedding-based method to extract the top-\nqresults, then apply BM25 to extract top-kresults\nfrom the qresults where q≥k. In the fourth config-\nuration, we reverse the order where we first extract\nthe top-qresults using BM25 and then extract the\ntop-kresults from the qresults using embedding-\nbased methods. We set k = 3based on previous\nexperiments. The evaluation results are shown in\nTable 5. We find the straightforward dense retrieval\napproach achieves the best performance.\n6 Conclusion\nIn this paper, we propose two methods that re-\nduce the token size for retrieval-augmented LLM.\nAdditionally, we propose a food recommenda-\ntion dataset contains domain-specific knowledge to\nbenchmark the performance of retrieval-augmented\nLLM performance on the GPT-3.5-turbo. We care-\nfully select a subset that focuses on 200 types of\nfood recommendations for maternity and infant\npeople. Without retrieval augmentation, the com-\nmercial GPT-3.5-turbo model is only able to get\n9803\nMethod Top- q Acc. (%)\nEmbedding n/a 90.9\nEmbedding+BM25 10 89.7\nEmbedding+BM25 100 88.0\nBM25 n/a 74.7\nBM25+Embedding 10 89.3\nBM25+Embedding 100 89.8\nTable 5: Evaluation of different retrieval algorithm con-\nfigurations. The top- q column indicates the m value\nused in the search. Acc. stands for accuracy.\n51% percent of the question right, compared to\n90.2% with retrieved context. We use this 90.2% as\nour goal and compare the performance of different\ntoken compression algorithms. Our summarization-\nbased approach achieves the best performance,\nreaching 90.5% of accuracy with 65% token re-\nduction on the retrieved context. It indicates that\nthe summarized text maintains a similar level of\ncritical information but with a significantly shorter\nlength, showing promising ability of our proposed\nmethod. The semantic compression method can\nfurther remove the words that have lower seman-\ntic meaning and provides a more flexible way to\ntrade-off the length of sentences with accuracy.\nLimitations\nThe goal of our model is to reduce the token size\nfor retrieval-augmented LLMs. The compression\nrate is determined based on the methods’ ability to\ncondense sentences while preserving as much of\ntheir essential information as possible. If the sen-\ntences are already short and compacted in meaning,\nthe compression rate won’t be able to be low if we\nwant to maintain most of the critical information\nwithin the sentence. Our algorithm is designed for\nlarge commercial LLMs and smaller open-source\nLLMs may not experience similar levels of perfor-\nmance gain.\nEthics Statement\nOur proposed methods are designed solely for the\ngoal of reducing cost while maintaining the perfor-\nmance using commercial LLMs through API calls.\nThe algorithms are not designed for activities that\nare in violation of human rights or with military\npurposes. The data we collected for our proposed\ndataset does not contain any privacy information.\nReferences\nDhananjay Ashok and Zachary C Lipton. 2023. Prompt-\nNER: Prompting For Named Entity Recognition.\narXiv preprint arXiv:2305.15444.\nE Bolton, D Hall, M Yasunaga, T Lee, C Manning, and\nP Liang. 2022. Stanford crfm introduces pubmedgpt\n2.7 b.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nLingjiao Chen, Matei Zaharia, and James Zou. 2023.\nFrugalGPT: How to Use Large Language Models\nWhile Reducing Cost and Improving Performance.\narXiv preprint arXiv:2305.05176.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342–8360, Online. Association for Computational\nLinguistics.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for web search using\nclickthrough data. In Proceedings of the 22nd ACM\ninternational conference on Information & Knowl-\nedge Management, pages 2333–2338.\nTatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and\nSadao Kurohashi. 2023. MultiTool-CoT: GPT-3 Can\nUse Multiple External Tools with Chain of Thought\nPrompting. arXiv preprint arXiv:2305.16896.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data, 7(3):535–547.\n9804\nKaren Sparck Jones. 1973. Index term weighting. In-\nformation storage and retrieval, 9(11):619–633.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nGuang Lu, Sylvia B Larcher, and Tu Tran. 2023. Hy-\nbrid Long Document Summarization using C2F-FAR\nand ChatGPT: A Practical Study. arXiv preprint\narXiv:2306.01169.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. When Not to Trust Language Models: In-\nvestigating Effectiveness and Limitations of Paramet-\nric and Non-Parametric Memories. arXiv preprint\narXiv:2212.10511.\nHa-Thanh Nguyen. 2023. A Brief Report on LawGPT\n1.0: A Virtual Legal Assistant Based on GPT-3.\narXiv preprint arXiv:2302.05729.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hernández Ábrego, Ji Ma, Vincent Y Zhao,\nYi Luan, Keith B Hall, Ming-Wei Chang, et al.\n2021. Large dual encoders are generalizable retriev-\ners. arXiv preprint arXiv:2112.07899.\nOpenAI. 2022. New and improved embedding model.\nOpenAI. 2023a. GPT-4 Technical Report. CoRR,\nabs/2303.08774.\nOpenAI. 2023b. Introducing ChatGPT.\nOpenAI. 2023c. Introducing ChatGPT and Whisper\nAPIs.\nSundar Pichai. 2023. An important next step on our AI\njourney.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nNils Reimers and Iryna Gurevych. 2020. Making\nMonolingual Sentence Embeddings Multilingual us-\ning Knowledge Distillation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4512–4525,\nOnline. Association for Computational Linguistics.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at TREC-3. Nist Special Publication Sp,\n109:109.\nBishal Santra, Sakya Basak, Abhinandan De, Manish\nGupta, and Pawan Goyal. 2023. Frugal Prompting for\nDialog Models. arXiv preprint arXiv:2305.14919.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nJamiel Sheikh. 2023. Bloomberg Uses Its Vast Data To\nCreate New Finance AI.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval Augmentation\nReduces Hallucination in Conversation. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 3784–\n3803. Association for Computational Linguistics.\nDavid Soong, Sriram Sridhar, Han Si, Jan-Samuel\nWagner, Ana Caroline Costa Sá, Christina Y Yu,\nKubra Karagoz, Meijian Guan, Hisham Hamadeh,\nand Brandon W Higgs. 2023. Improving accu-\nracy of GPT-3/4 results on biomedical data using a\nretrieval-augmented language model. arXiv preprint\narXiv:2305.17116.\nKaren Sparck Jones. 1972. A statistical interpretation\nof term specificity and its application in retrieval.\nJournal of documentation, 28(1):11–21.\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fab-\nrizio Silvestri, Sebastian Riedel, and Alon Y . Levy.\n2021. From Natural Language Processing to Neural\nDatabases. Proc. VLDB Endow., 14(6):1033–1039.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\n9805\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nShuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang,\nFei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang.\n2023. Gpt-ner: Named entity recognition via large\nlanguage models. arXiv preprint arXiv:2304.10428.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-Instruct: Aligning Lan-\nguage Model with Self Generated Instructions. arXiv\npreprint arXiv:2212.10560.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\nbadur, David Rosenberg, and Gideon Mann. 2023.\nBloomberggpt: A large language model for finance.\narXiv preprint arXiv:2303.17564.\nTong Xiang, Liangzhi Li, Wangyue Li, Mingbai Bai,\nLu Wei, Bowen Wang, and Noa Garcia. 2023.\nCARE-MI: chinese benchmark for misinformation\nevaluation in maternity and infant care. CoRR,\nabs/2307.01458.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2020. mT5: A massively multilingual\npre-trained text-to-text transformer. arXiv preprint\narXiv:2010.11934.\nXinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan\nCheng, Lukasz Heldt, Aditee Kumthekar, Zhe Zhao,\nLi Wei, and Ed Chi. 2019. Sampling-bias-corrected\nneural modeling for large corpus item recommenda-\ntions. In Proceedings of the 13th ACM Conference\non Recommender Systems, pages 269–277.\nHongChien Yu, Chenyan Xiong, and Jamie Callan. 2021.\nImproving query representations for dense retrieval\nwith pseudo relevance feedback. In CIKM ’21: The\n30th ACM International Conference on Information\nand Knowledge Management, Virtual Event, Queens-\nland, Australia, November 1 - 5, 2021, pages 3592–\n3596. ACM.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nWenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan,\nand Lidong Bing. 2023. Sentiment Analysis in the\nEra of Large Language Models: A Reality Check.\narXiv preprint arXiv:2305.15005.\n9806\nAppendix\nA Example question for FRDB dataset\nIs it suitable for an infant to consume/have hot\nchocolate? (1) Recommend, (2) Neutral, (3) Avoid\n1. Food is beneficial to the current stage, and\nexcessive consumption will not cause physical\nabnormalities.\n2. Advocate a healthy lifestyle and advise users\nto intake relatively limited amount of foods,\nincluding foods with excessive salt, oil, sugar,\nand so on.\n3. There is evidence that it will cause harm after\neating; Authoritative literature points out that\neating is forbidden at a certain stage.\nB Sentence compression examples\nExamples are shown in Table 6.\nC QA examples\nExamples of QA are shown in Table 7.\nD Knowledge examples\nExamples of knowledge utilized for answering the\nquestions are shown in Table 8.\n9807\nDataset Original Compressed\nFRDB\nBabies are strictly prohibited from consuming\ndonkey meat buns. The reason is that don-\nkey meat buns usually contain a large amount\nof spices and seasonings, with high salt con-\ntent, which will increase the metabolic burden\non the baby’s kidneys. Furthermore, infants\nand young children are in a critical period for\ndeveloping taste preferences, and frequently\nconsuming donkey meat buns may affect the\nformation of their taste buds. It is not recom-\nmended to feed babies donkey meat buns.\nIn the first-instance procedure of\na public prosecution case, if the\nparticipants or observers disrupt\nthe courtroom order, the presid-\ning judge shall handle the situa-\ntion according to the law. Unau-\nthorized recording, videotaping,\nphotographing, or disseminating\ntrial proceedings through mail,\nblog, or microblog may lead to\nthe temporary seizure of storage\nmedia or relevant equipment.\nTable 6: Token compression examples. Both original and compressed sentences are translated into English from\nChinese.\n9808\nQuestions Questions (English translation) Answer\n对于’根据“产妇少吃鹅肝。\n原因是：产后妈妈 适量食\n用鹅肝有利于促进伤口的愈\n合，弥补生产失血和重建肝\n脏铁储备，提高乳汁质量。\n还能提高妈妈 的免疫力，\n具有抗氧化、缓解衰老的作\n用。建议产后妈妈可适量食\n用。”产妇食用/饮用鹅肝’，\n下列哪个选项最适合用于描\n述上述行为？选项：(1) 推\n荐，(2) 中立，(3) 避免\nAccording to \"Maternity should eat less foie\ngras. The reason is: moderate consump-\ntion of foie gras after childbirth can promote\nwound healing, make up for blood loss during\nchildbirth, rebuild liver iron reserves, improve\nbreast milk quality. It can also improve the\nimmunity of mothers, and has an antioxidant\nand anti-aging effect. It is recommended that\npostpartum mothers can consume it in moder-\nation.\" Which of the following options is most\nsuitable to describe the behavior of postpartum\nmothers consuming/eating foie gras? Options:\n(1) Recommend, (2) Neutral, (3) Avoid\nNeutral\n对于’根据“备孕女性可以\n吃咖喱。原因是：咖喱属于\n混合调制的香料，能调节肠\n胃蠕动，提高食欲，其辛辣\n程度根据配料而变，过于辛\n辣的咖喱对胃有一定的刺激\n性，备孕期女性可以根据自\n己的口味喜好选择合适辣度\n的咖喱。”备孕女性食用/饮\n用咖喱’，下列哪个选项最\n适合用于描述上述行为？选\n项：(1) 推荐，(2) 中立，(3)\n避免\nRegarding \"Females preparing for pregnancy\ncan eat curry. The reason is that curry is a\nmixed seasoning that can regulate intestinal\nmovement, increase appetite, and its spiciness\nvaries according to the ingredients. Curry that\nis too spicy can stimulate the stomach to some\nextent. Females preparing for pregnancy can\nchoose curry with appropriate spiciness based\non their own taste preferences.\" Which of the\nfollowing options is most suitable to describe\nthe behavior of females preparing for preg-\nnancy consuming/eating curry? Options: (1)\nRecommend, (2) Neutral, (3) Avoid\nRecommend\n对于’根据“6月大的宝宝少吃\n玉米汁。原因是：玉米汁富\n含维生素、矿物质和碳水化\n合物，可为宝宝提供能量，\n促进其生长发育，且吸收率\n较高。6月龄以后的宝宝 可\n少量食用，注意鲜榨玉米汁\n不要添加糖，以防摄入过多\n的糖，不利于宝宝的口腔健\n康。”6月龄的宝宝 食用/饮\n用玉米汁’，下列哪个选项最\n适合用于描述上述行为？选\n项：(1) 推荐，(2) 中立，(3)\n避免\nRegarding \"Babies at the age of 6 months\nshould consume less corn juice. The reason\nis that corn juice is rich in vitamins, minerals\nand carbohydrates, which can provide energy\nfor babies, promote their growth and develop-\nment, and has a high absorption rate. Babies\nover 6 months old can consume it in modera-\ntion, but be mindful that freshly squeezed corn\njuice should not contain added sugar to pre-\nvent excessive intake of sugar, which could\nbe detrimental to baby’s oral health.\" Which\nof the following options is most suitable to\ndescribe the behavior of a 6-month-old baby\nconsuming/drinking corn juice? Options: (1)\nRecommend, (2) Neutral, (3) Avoid\nNeutral\nTable 7: QA examples. For all examples, the answers are chosen from 3 options, (1) Recommend, (2) Neutral, and\n(3) Avoid.\n9809\nKnowledge Knowledge (English translation)\n备孕女性可以吃土豆泥。原\n因是：备孕人群可以食用，\n不过土豆泥升糖较快，建议\n一次不要吃太多。\nFemales preparing for pregnancy can eat mashed pota-\ntoes. The reason is that it is safe for this group to con-\nsume mashed potatoes. However, mashed potatoes have\na high glycemic index, so it is recommended not to eat\ntoo much at once.\n宝宝 不能吃生鱼片。原因\n是：鱼肉中含有大量的不饱\n和脂肪酸（尤其是DHA），\n有助于促进宝宝 大脑及视\n力发育。但生鱼片如果处\n理不当，容易感染病菌和寄\n生虫。不建议给宝宝吃生鱼\n片。\nBabies should not eat raw fish slices. The reason is\nthat fish contains a large amount of unsaturated fatty\nacids (especially DHA), which can help promote the\ndevelopment of the baby’s brain and vision. However, if\nraw fish slices are not properly processed, they can easily\nbecome contaminated with bacteria and parasites, posing\nhealth risks to babies. Therefore, it is not recommended\nto feed raw fish slices to babies.\n孕妇可以吃罗非鱼。原因\n是：罗非鱼中含有非常丰富\n的不饱和脂肪酸、蛋白质和\n多种氨基酸，容易被人体消\n化吸收。能为孕妈提供多种\n营养物质，帮助胎儿骨骼生\n长和神经系统发育。\nPregnant women can eat tilapia. The reason is that tilapia\nis rich in unsaturated fatty acids, protein and various\namino acids, which are easily digested and absorbed by\nthe human body. It can provide pregnant women with\nvarious nutrients to help promote fetal bone growth and\ndevelopment of the nervous system.\nTable 8: Examples of knowledge that are utilized for answering the questions.\n9810",
  "topic": "Security token",
  "concepts": [
    {
      "name": "Security token",
      "score": 0.8442372679710388
    },
    {
      "name": "Automatic summarization",
      "score": 0.8276629447937012
    },
    {
      "name": "Computer science",
      "score": 0.8094390034675598
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5543441772460938
    },
    {
      "name": "Information retrieval",
      "score": 0.4634985625743866
    },
    {
      "name": "Language model",
      "score": 0.4590117335319519
    },
    {
      "name": "Inference",
      "score": 0.4307924211025238
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3697736859321594
    },
    {
      "name": "Data mining",
      "score": 0.3523164689540863
    },
    {
      "name": "Computer network",
      "score": 0.16138723492622375
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I115228651",
      "name": "Agency for Science, Technology and Research",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210088309",
      "name": "Star Technology and Research (United States)",
      "country": "US"
    }
  ],
  "cited_by": 16
}