{
  "title": "Integration of persistent Laplacian and pre-trained transformer for protein solubility changes upon mutation",
  "url": "https://openalex.org/W4388092799",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5053368625",
      "name": "JunJie Wee",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A5089262981",
      "name": "Jiahui Chen",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": "https://openalex.org/A5084610901",
      "name": "Kelin Xia",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5114375564",
      "name": "Guo‐Wei Wei",
      "affiliations": [
        "Michigan State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4321369634",
    "https://openalex.org/W2092387745",
    "https://openalex.org/W2103083136",
    "https://openalex.org/W2139762959",
    "https://openalex.org/W2288875731",
    "https://openalex.org/W2613688952",
    "https://openalex.org/W2411303265",
    "https://openalex.org/W6780007203",
    "https://openalex.org/W2027839322",
    "https://openalex.org/W2765285487",
    "https://openalex.org/W3183890486",
    "https://openalex.org/W1999528235",
    "https://openalex.org/W2124511088",
    "https://openalex.org/W2481594651",
    "https://openalex.org/W2741292700",
    "https://openalex.org/W2733638363",
    "https://openalex.org/W3005822506",
    "https://openalex.org/W3044722686",
    "https://openalex.org/W2963883198",
    "https://openalex.org/W2984249386",
    "https://openalex.org/W3001282080",
    "https://openalex.org/W3033534833",
    "https://openalex.org/W3082042165",
    "https://openalex.org/W3157078379",
    "https://openalex.org/W4210311437",
    "https://openalex.org/W4382502307",
    "https://openalex.org/W4308112120",
    "https://openalex.org/W6763868836",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6798272402",
    "https://openalex.org/W4386634093",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W6791955017",
    "https://openalex.org/W2107432340",
    "https://openalex.org/W2074719974",
    "https://openalex.org/W2972342264",
    "https://openalex.org/W6853925458",
    "https://openalex.org/W2002165972",
    "https://openalex.org/W2025796118",
    "https://openalex.org/W2002243081",
    "https://openalex.org/W4243631322",
    "https://openalex.org/W4281790889",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2047399603",
    "https://openalex.org/W2569129743",
    "https://openalex.org/W2170802700",
    "https://openalex.org/W2152997175",
    "https://openalex.org/W2146341019",
    "https://openalex.org/W1596947964",
    "https://openalex.org/W6601173283",
    "https://openalex.org/W4311921897",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1496383623",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W3037516419",
    "https://openalex.org/W3179485843",
    "https://openalex.org/W579361209",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W2494835618",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W4383186622",
    "https://openalex.org/W28435248",
    "https://openalex.org/W4234609530",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W3140579943",
    "https://openalex.org/W1526753291",
    "https://openalex.org/W2909727437",
    "https://openalex.org/W4240467879"
  ],
  "abstract": null,
  "full_text": "Integration of persistent Laplacian and pre-trained\ntransformer for protein solubility changes upon\nmutation\nJunJie Wee1, Jiahui Chen2, Kelin Xia3,† and Guo-Wei Wei1,4,5,∗\n1Department of Mathematics, Michigan State University, East Lansing, MI 48824, USA\n2Department of Mathematical Sciences, University of Arkansas, Fayetteville, AR 72701, USA\n3Division of Mathematical Sciences, School of Physical and Mathematical Sciences\nNanyang Technological University, Singapore 637371\n4Department of Biochemistry and Molecular Biology,\nMichigan State University, East Lansing, MI 48824, USA\n5Department of Electrical and Computer Engineering,\nMichigan State University, East Lansing, MI 48824, USA\n† Address correspondences to Kelin Xia. E-mail: xiakelin@ntu.edu.sg\n∗ Address correspondences to Guo-Wei Wei. E-mail: weig@msu.edu\nAbstract\nProtein mutations can significantly influence protein solubility, which results in altered\nprotein functions and leads to various diseases. Despite of tremendous effort, machine\nlearning prediction of protein solubility changes upon mutation remains a challenging task\nas indicated by the poor scores of normalized Correct Prediction Ratio (CPR). Part of the\nchallenge stems from the fact that there is no three-dimensional (3D) structures for the\nwild-type and mutant proteins. This work integrates persistent Laplacians and pre-trained\nTransformer for the task. The Transformer, pretrained with hunderds of millions of protein\nsequences, embeds wild-type and mutant sequences, while persistent Laplacians track the\ntopological invariant change and homotopic shape evolution induced by mutations in 3D\nprotein structures, which are rendered from AlphaFold2. The resulting machine learning\nmodel was trained on an extensive data set labeled with three solubility types. Our model\noutperforms all existing predictive methods and improves the state-of-the-art up to 15%.\nKeywords: Mutation, Protein solubility, Persistent Laplacian, Transformer, AlphaFold2.\n1\narXiv:2310.18760v2  [q-bio.BM]  2 Nov 2023\nContents\n1 Introduction 3\n2 Results 62.1 Overview of TopLapGBT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.2 Performance of TopLapGBT on PON-Sol2 dataset . . . . . . . . . . . . . . . 8\n2.3 Performance of TopLapGBT on independent test set . . . . . . . . . . . . . . . 10\n3 Discussion 103.1 Performance analysis based on different mutation regions . . . . . . . . . . . . 12\n3.2 Performance analysis based on different mutation types . . . . . . . . . . . . . 13\n3.3 Feature analysis based on Residue-Similarity plots . . . . . . . . . . . . . . . 14\n4 Conclusion 16\n5 Materials and Methods 175.1 Persistent Laplacian characterization of proteins . . . . . . . . . . . . . . . . . 18\n5.2 Persistent Laplacian descriptors . . . . . . . . . . . . . . . . . . . . . . . . . 23\n5.3 Persistent Homology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n5.4 Transformer Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275.5 Performance Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n6 Software and resources 29\n2\n1 Introduction\nGenetic mutations alter the genome sequence, leading to changes in the corresponding amino\nacid sequence of a protein. These alternations have far-reaching implications on the protein’s\nstructure, function, and stability, affecting attributes such as folding stability, binding affinity,\nand solubility. The consequences of protein mutations have been extensively studied in diverse\nfields such as evolutionary biology, cancer biology, immunology, directed evolution, and protein\nengineering [1]. Understanding the impact of genetic mutations on protein solubility is crucial\nin various fields, including protein engineering, drug discovery, and biotechnology. Accurately\nanalyzing and predicting the impact of mutations on protein solubility is therefore crucial in\nmany fields, facilitating the engineering of proteins with desirable functions. There are nu-\nmerous intricately interconnected factors impacting protein solubility, ranging from amino acid\nsequence arrangement, post-translational modifications, protein-protein interactions, to envi-\nronmental conditions, such as solvent type, ion type and concentration, the presence of small\nmolecules, temperature, etc. Unfortunately, the existing data set does not contain sufficient\ninformation. This complexity poses significant challenges for the accurate prediction and mod-\neling of protein solubility, often requiring multifaceted computational approaches for reliable\noutcomes.\nComputational predictions serve as a valuable complement to experimental mutagenesis\nanalysis of protein stability changes upon mutation. Such computational approaches offer sev-\neral advantages, including being economical, efficient, and provide a viable alternative to labor-\nintensive site-directed mutagenesis experiments [2]. As a result, the development of accurate\nand reliable computational techniques for mutational impact prediction could substantially en-\nhance the throughput and accessibility of research in protein engineering and drug discovery.\nOver the years, a variety of computational methods have been developed to explore the ef-\n3\nfects of mutations on protein solubility, including but not limited to CamSol [3], OptSolMut [4],\nPON-Sol [5], SODA [6], Solubis [7], and others as summarized in a recent review [8]. Cam-\nSol employs an algorithm to construct a residue-specific solubility profile, although no explicit\nmethod has been made publicly available. OptSolMut is trained on 137 samples, each featur-\ning single or multiple mutations affecting solubility or aggregation. PON-Sol utilizes a random\nforest model trained on a dataset of 406 single amino acid substitutions labeled as solubility-\nincreasing, solubility-decreasing, or exhibiting no change in solubility. SODA, which is based\non the PON-Sol data, specifically targets samples with decreasing solubility [6]. Solubis is an\noptimization tool that increases protein solubility and integrates interaction analysis from FoldX\n[2], aggregation prediction from TANGO [9], and structural analysis from Y ASARA [10]. Re-\ncently, PON-Sol2 [11] extended the original PON-Sol dataset and employed a gradient boosting\nalgorithm for sequence-based predictions. Despite of intensive effort, the current prediction ac-\ncuracy in terms of normalized Correct Prediction Ratio (CPR) remains very low, calling for\ninnovative strategies.\nTopological data analysis (TDA) is a relatively new approach for data science. Its main tech-\nnique is persistent homology [12, 13]. The essential idea of persistent homology is to construct\na multiscale analysis of data in terms of topological invariants. The resulting changes of topo-\nlogical invariants over scales can be used to characterize the intricate structures of data, leading\nto an unusually powerful approach in describing protein structure, flexibility, and folding [14].\nPersistent homology was integrated with machine learning for the classification of proteins in\n2015 [15], which was one the first integrations of TDA and machine learning, and the predic-\ntions of mutation-induced protein stability changes [16, 17] and protein-protein binding free\nenergy changes [18, 19]. One of the major achievements of TDA is its winning of D3R Grand\nChallenges, an annual worldwide competition series in computer-aided drug design [20, 21]. A\nnearly comprehensive summary of the early success of TDA in biological science was given in\n4\na review [22].\nHowever, persistent homology only tracks the changes in topological invariants and cannot\ncapture homotopic shape evolution of data over scales or induced by mutations. To overcome\nthis limitation, Wei and coworkers introduced persistent combinatorial Laplacians, also called\npersistent spectral graphs, on point clouds [23] and evolutionary de Rham-Hodge method on\nmanifolds [24] in 2019. The essence of these methods is the persistent topological Laplacians\n(PTLs) either on point clouds or on manifolds. PTLs not only fully capture the topological\ninvariants in its harmonic spectra as those given by persistent homology, but also capture the\nhomotopic shape evolution of data during the multiscale analysis or a mutation process. PTLs\nwere applied to the predictions of protein flexibility [25] and protein-ligand binding free ener-\ngies [26], protein–protein interactions[27, 28], and protein engineering [1]. The most remark-\nable accomplishment by persistent Laplacian is its accurate forecasting of emerging dominant\nSARS-CoV-2 variants BA.4 and BA.5 about two months in advance [29].\nHowever, TDA approaches depend on the biomolcular structures, which may not be avail-\nable. In fact, many proteins involved in the present study do not have 3D structures. In recent\nyears, advanced natural language processing (NLP) models, including Transformers and long\nshort-term memory (LSTM), have been widely implemented across various domains to extract\nprotein sequence information. For example, Tasks Assessing Protein Embedding (TAPE) intro-\nduced three different architectures, namely transformer, dilated residual network (ResNet), and\nLSTM [30]. Additionally, LSTM-based models like Bepler [31] and UniRep [32] have been\ndeveloped. Additionally, large-scale protein transformer models trained on extensive datasets\ncomprising hundreds of millions of sequences have made significant advancements in the field.\nThese models, including Evolutionary Scale Modeling (ESM) [33] and ProtTrans [34, 35], have\nexhibited exceptional performance in capturing a variety of protein properties. ESM, for in-\nstance, allows for fine-tuning based on either downstream task data or local multiple sequence\n5\nalignments [36]. In the present work, we leverage the pre-trained ESM transformer model to\nextract crucial information from protein sequences.\nIn this work, we will integrate transformer-based sequence embedding and persistent topo-\nlogical Laplacians to predict protein solubility changes upon mutation. While sequence-based\nmodels can be applied without 3D structural information, the PTL-based features require high-\nquality structures. We generate 3D structures of wild type proteins from AlphaFold2 [37] to\nfacilitate topological embedding. By combining both embedding approaches, they naturally\ncomplement each other in classifying protein solubility changes upon mutation. These em-\nbeddings are fed into an ensemble classifier, gradient boosted trees (GBT), to build a ma-\nchine learning model, called TopLapGBT. We validate TopLapGBT on the classification of\nprotein solubility changes upon mutation. We demonstrate that this integrated machine learning\nmodel gives rise to a substantial improvement as compared to existing state-of-the-art models.\nResidue-Similarity plots are also applied to assess how well the TopLapGBT model classify\nthree solubility labels.\n2 Results\n2.1 Overview of TopLapGBT\nTopLapGBT integrates both structure-based and sequence-based features, derived from protein\nstructures and sequences respectively, into a unified model. Our architecture comprises three\ndistinct embedding modules: persistent Laplacian-based embeddings, sequence-based embed-\ndings, and auxiliary feature embeddings, all of which feed into an ensemble classifier as de-\npicted in Figure 1.\nIn the persistent Laplacian-based feature embedding module, we employ persistent Lapla-\ncian techniques to generate features that encapsulate the structural attributes of proteins both\npre- and post-mutation. This approach is particularly effective in capturing the structural alter-\n6\nFigure 1: The illustration of the workflow for TopLapGBT. Protein sequences are first prepro-\ncessed by AlphaFold 2 to generate wild type protein structures. Mutant proteins are generated\nfrom the Jackal software [38]. The structure-based features from persistent Laplacian, auxiliary\nand sequence-based features are then concatenated to form a long feature input for gradient\nboosting tree to classify the protein solubility changes upon mutation. The predicted labels are\nalso analyzed on Residue-Similarity (R-S) plots.\nations induced by mutations within the localized neighborhoods of the mutation sites. Mathe-\nmatically, the persistent Laplacian builds a sequence of simplicial complexes through a filtra-\ntion process, thereby characterizing atom-atom interactions across multiple scales (details in\nthe Methods section). In the sequence-based feature embedding module, a pre-trained trans-\nformer model generates latent feature vectors extracted from protein sequences. Specifically,\nthe transformer model used here is a 650M-parameter protein language model, trained on a\ncorpus of 250M protein sequences spanning multiple organisms [39]. Finally, the auxiliary fea-\nture embedding module incorporates a variety of attributes such as surface area, partial charge,\npKa shifts, solvation free energy, and secondary structural information, synthesized from both\n7\nprotein sequences and structures. These three distinct sets of feature embeddings are subse-\nquently concatenated to produce a comprehensive feature vector. This vector is then fed into a\ngradient-boosting tree classifier to categorize the mutation-induced samples.\n2.2 Performance of TopLapGBT on PON-Sol2 dataset\nIn our study, we utilize the dataset employed by PON-Sol2 as detailed in [11]. The dataset is\ncomprised of 6,328 mutation samples, originating from 77 distinct proteins. These samples are\ncategorized into three labels: decrease in solubility, increase in solubility, and no change in sol-\nubility. Specifically, the dataset contains 3,136 samples demonstrating a decrease in solubility,\n1,026 samples showing an increase, and 2,166 samples with no change. Notably, the dataset\nexhibits a class imbalance, with a ratio of 1 : 0 .69 : 0 .34, indicating a bias towards samples\nthat exhibit a decrease in solubility. To assess the performance of our model, we initially carry\nout a random 10-fold cross-validation on the dataset. Subsequently, an independent blind test\nprediction is executed to provide further validation of the model’s efficacy.\nIn Table 1, we present a comparative analysis of the performance of existing classifiers\nby PON-Sol [5] and PON-Sol2 [11] against our proposed model, TopLapGBT, using 10-fold\ncross-validation. It should be noted that PON-Sol2 incorporates feature selection techniques\nsuch as recursive feature elimination (RFE). To provide a robust assessment of TopLapGBT’s\nperformance, we conduct 10 repeated runs, and the mean values of these runs are reported to\naccount for any randomness in the model’s output.\nPerformance evaluation of our model, TopLapGBT, is conducted using a range of metrics,\nincluding Positive Predictive Value (PPV), Negative Predictive Value (NPV), Sensitivity, Speci-\nficity, Correct Prediction Ratio (CPR), and Generalized Squared Correlation (GC 2). PPV and\nNPV quantify the proportions of correct positive and negative predictions for each solubility\nclass, respectively. Given that we are dealing with a K-class problem with three distinct solu-\n8\nTable 1: Comparison of performance metrics between TopLapGBT and both single layer and\ndouble layer classifiers of PON-Sol2 in the 10-fold crossvalidation. The negative solubility\nsamples are denoted as ”-” whereas the positive solubility change samples are denoted as ”+”.\nThe samples with no solubility change are denoted as ”N”. Performance metrics include the\npositive predicted values (PPV), negative predicted values (NPV), sensitivity, specificity, cor-\nrect prediction ratio (CPR) and generalised correlation (GC 2). PPV refers to the proportions\nof positive predictions for each solubility class while NPV refers to the proportions of negative\npredictions for each solubility class. CPR calculates the percentage of correctly classified sam-\nples while GC2 measures the correlation coefficient of the classification. All normalized metrics\nare also reported. For each metric, the first value is without normalization while the second one\nis with normalization.\nPerformance\nMetric\nModel\nPON-Sol2 [11] TopGBT TopLapGBT\nSingle Three-Class ClassifierTwo-Layer Three-Class Classifier- -\nAll Features 30 Features\nSelected by RFEAll Features 34 Features\nSelected by RFE - -\nPPV\n-\nN\n+\n0.842/0.742\n0.657/0.536\n0.563/0.730\n0.835/0.729\n0.658/0.543\n0.586/0.752\n0.875/0.793\n0.635/0.521\n0.520/0.696\n0.869/0.781\n0.647/0.534\n0.538/0.714\n0.868/0.785\n0.686/0.554\n0.646/0.797\n0.873/0.797\n0.681/0.557\n0.627/0.779\nNPV\n-\nN\n+\n0.913/0.954\n0.841/0.824\n0.877/0.737\n0.901/0.947\n0.847/0.832\n0.877/0.738\n0.893/0.942\n0.847/0.829\n0.877/0.736\n0.891/0.941\n0.855/0.838\n0.878/0.739\n0.932/0.965\n0.864/0.849\n0.886/0.749\n0.931/0.964\n0.858/0.842\n0.888/0.757\nSensitivity\n-\nN\n+\n0.919/0.919\n0.701/0.701\n0.329/0.329\n0.906/0.906\n0.717/0.717\n0.326/0.326\n0.892/0.892\n0.724/0.724\n0.336/0.336\n0.891/0.891\n0.738/0.738\n0.340/0.340\n0.937/0.937\n0.752/0.752\n0.359/0.359\n0.934/0.934\n0.735/0.735\n0.395/0.395\nSpecificity\n-\nN\n+\n0.831/0.839\n0.812/0.697\n0.948/0.938\n0.825/0.831\n0.807/0.697\n0.954/0.947\n0.875/0.883\n0.785/0.667\n0.938/0.927\n0.868/0.874\n0.792/0.678\n0.941/0.932\n0.860/0.872\n0.821/0.697\n0.962/0.954\n0.867/0.881\n0.823/0.707\n0.953/0.944\nCPR 0.747/0.650 0.746/0.650 0.743/0.651 0.747/0.656 0.780/0.6820.792/0.688\nGC2 0.317/0.298 0.309/0.289 0.322/0.313 0.323/0.312 0.371/0.3540.376/0.361\nbility classes, CPR and GC2 are particularly relevant for providing a holistic view of the model’s\nperformance [40]. Specifically, CPR measures the overall accuracy of the model, while GC 2\nquantifies the correlation coefficient of the classification, ranging from 0 to 1. Larger values for\nthese metrics denote better performance. Importantly, due to the class imbalance in the number\nof mutation samples across the categories, all performance metrics are normalized to ensure\na robust and reliable evaluation of the model’s efficacy (further details are elaborated in the\nMethods section).\nThe proposed model, TopLapGBT, demonstrates significant performance gains over exist-\n9\ning featurization methods in PON-Sol2 across all evaluation metrics [11]. Specifically, nor-\nmalized CPR and GC2 scores of TopLapGBT stand at 0.688 and 0.361, marking improvements\nof 4.88% and 15.71% over PON-Sol2, respectively. These gains underscore the merit of in-\ncorporating both structure-based and sequence-based features into the model. To elucidate the\ncontribution of Persistent Laplacian (PL)-based features, we also present a comparative analysis\nwith our TopGBT model in Table 1. The TopGBT model utilizes persistent homology-based\nembeddings alongside auxiliary and pre-trained transformer features. While TopGBT still out-\nperforms all existing PON-Sol2 models, the incorporation of PL-based features in TopLapGBT\nleads to an incremental improvement of 1% and 2% in CPR and GC2 metrics, respectively. This\nvalidates our approach of leveraging Persistent Laplacian to comprehensively capture both the\ntopological and homotopic nuances in the evolution of protein structures.\n2.3 Performance of TopLapGBT on independent test set\nTo robustly assess the performance of TopLapGBT, we subjected it to an independent test using\nthe same dataset employed by PON-Sol2 [11]. In this validation, TopLapGBT consistently out-\nperformed all five existing models, as evidenced in Table 2. Specifically, TopLapGBT registers\na normalized CPR of 0.564 and a normalized GC2 of 0.185, surpassing PON-Sol2 by 3.49% and\n17.83%, respectively. Relative to TopGBT, the inclusion of PL-based features in TopLapGBT\nyielded incremental gains in both CPR and GC2 metrics, thereby further substantiating the util-\nity of Persistent Laplacian in capturing the homotopic shape evolution within protein structures.\n3 Discussion\nThe performance of machine learning models generally relies on the nature of the input fea-\ntures. In our model, the PL-based features depend on one main element which is the quality\nof the protein structures from AlphaFold 2 (AF2). The quality of AF2 structures are crucial\n10\nTable 2: Performance of TopLapGBT with existing state-of-the-art models on the independent\nblind test classification. The negative solubility samples are denoted as ”-” whereas the posi-\ntive solubility change samples are denoted as ”+”. The samples with no solubility change are\ndenoted as ”N”. Performance metrics include the positive predicted values (PPV), negative\npredicted values (NPV), sensitivity, specificity, correct prediction ratio (CPR) and generalised\ncorrelation (GC2). PPV refers to the proportions of positive predictions for each solubility class\nwhile NPV refers to the proportions of negative predictions for each solubility class. CPR\ncalculates the percentage of correctly classified samples while GC 2 measures the correlation\ncoefficient of the classification. All normalized metrics are also reported. For each metric, the\nfirst value is without normalization while the second one is with normalization.Performance\nMetric\nIndependent Test\nPON-Sol [5]SODA SODA(5 as\nThreshold)\nSODA(10 as\nThreshold)\nSODA(17 as\nThreshold)PON-Sol2 [11]TopGBTTopLapGBT\nPPV\n-\nN\n+\n0.593/0.428\n0.427/0.385\n0.151/0.373\n0.427/0.258\nNaN/NaN\n0.080/0.229\n0.606/0.428\n0.425/0.365\n0.047/0.149\n0.673/0.468\n0.397/0.357\n0.060/0.184\n0.742/0.585\n0.383/0.350\n0.098/0.284\n0.804/0.643\n0.600/0.475\n0.233/0.472\n0.781/0.649\n0.617/0.462\n0.524/0.761\n0.789/0.645\n0.624/0.475\n0.476/0.718\nNPV\n-\nN\n+\n0.514/0.691\n0.685/0.700\n0.881/0.693\n0.373/0.537\n0.642/0.667\n0.832/0.605\n0.508/0.684\n0.761/0.739\n0.848/0.633\n0.502/0.677\n0.797/0.782\n0.858/0.649\n0.501/0.677\n0.797/0.782\n0.858/0.649\n0.794/0.887\n0.804/0.793\n0.879/0.684\n0.843/0.920\n0.816/0.795\n0.881/0.692\n0.842/0.918\n0.826/0.809\n0.880/0.688\nSensitivity\n-\nN\n+\n0.263/0.263\n0.456/0.456\n0.448/0.448\n0.488/0.488\n0.000/0.000\n0.253/0.253\n0.195/0.195\n0.759/0.759\n0.069/0.069\n0.098/0.098\n0.886/0.886\n0.057/0.057\n0.068/0.068\n0.954/0.954\n0.046/0.046\n0.802/0.802\n0.671/0.671\n0.161/0.161\n0.867/0.867\n0.692/0.692\n0.126/0.126\n0.864/0.864\n0.713/0.713\n0.115/0.115\nSpecificity\n-\nN\n+\n0.812/0.824\n0.659/0.636\n0.617/0.623\n0.318/0.297\n1.000/1.000\n0.558/0.573\n0.867/0.869\n0.426/0.340\n0.786/0.802\n0.951/0.944\n0.249/0.204\n0.863/0.872\n0.975/0.976\n0.144/0.116\n0.936/0.942\n0.796/0.777\n0.751/0.630\n0.920/0.910\n0.747/0.765\n0.760/0.597\n0.983/0.980\n0.759/0.763\n0.760/0.606\n0.981/0.977\nCPR 0.356/0.3890.282/0.2470.381/0.3410.375/0.3470.382/0.3560.671/0.5450.707/0.5620.711/0.564\nGC2 0.010/0.011NaN/NaN0.041/0.0450.022/0.0220.016/0.0160.181/0.1570.205/0.1840.206/0.185\nin determining the performance of TopLapGBT. Recently, AF2 structures have been reported\nto achieve comparable performance to nuclear magnetic resonance (NMR) structures while\nensemble methods can be used to enhance the performance by combining multiple NMR struc-\ntures [1]. This allows AF2 structures to serve as a practical substitute for experimental structural\ndata. Although AF2 structures are not as reliable as X-ray structures, the fusion of sequence-\nbased pre-trained transformer features and PL-based features provides robust featurization even\nfor low quality AF2 structural data. PL elucidates the precise mutation geometry and topol-\nogy, while sequence-based pre-trained transformer features capture evolutionary patterns from\nan extensive sequence library. This synergy holds significance and can be applied to a diverse\nrange of other challenges in the field of biomolecular research. For the rest of this section, we\nanalyze the model’s performance based on the region of the mutations and the type of muta-\n11\ntions. We also discuss the performance of different feature types using the Residue-Similarity\nplots.\n3.1 Performance analysis based on different mutation regions\nTo delve deeper into the model’s performance, we categorize mutation samples based on their\nstructural regions: interior and surface, as depicted in Figure 2 pre- and post-mutations. These\nregions are defined by their relative accessible solvent area (rASA), using a cutoff value c.\nA residue at the mutation site is classified as buried or interior if its rASA falls below this\ncutoff. While the discrete nature of c initially raised concerns, given that amino acids have a\ncontinuous exposure profile, empirical analyses on databases from organisms like Escherichia\ncoli, Saccharomyces cerevisiae, and Homo sapiens have shown that an optimal rASA cutoff of\napproximately 25% is effective for distinguishing between surface and interior residues [41]. In\nour analysis, we apply this framework to identify surface and interior residues in the solubility\ndataset. We observe that some mutation sites undergo a regional transition, moving from one\nstructural domain to another, consequent to the mutation.\nTo gain nuanced insights into TopLapGBT’s performance, we segment the results accord-\ning to the mutation’s structural location within the protein. We present these segmentations as\nheatmap plots that delineate both mutation regions and amino acid types. Structural regions are\ndefined based on relative accessible surface area (rASA) [41]. By categorizing residues as either\ninterior or surface, we can examine the influence of continuous amino acid exposure on solubil-\nity change classification post-mutation. Figure 2(b) displays accuracy scores for four types of\nmutations: interior-interior, interior-surface, surface-interior, and surface-surface. TopLapGBT\nattains an average accuracy score of 0.770 across these categories. Extended data in Figure\nS1 further breaks down accuracy scores for all 20 distinct amino acids within each region-pair,\nrevealing variations in residue-residue pair performance.\n12\nFigure 2: (a) The definitions of the structural regions on the protein label 213133708 with mu-\ntation ID: I283W. For both wild type and mutant type, amino acids in the proteins are classified\nunder surface or interior regions based on the rASA of the residue. The residue ID 283 of\nprotein label 213133708 was mutated from isoleucine (interior region) to trytophan (surface re-\ngion). Structures are plotted with the software Illustrate[42]. (b) A comparison of performance\nof TopLapGBT among different mutation region types. The y-axis represents the region type\nfor the original residue and the x-axis represents the region type for the mutated residue. The\nnumbers indicated in each cell corresponds to the number of mutation samples in each region-\nregion mutation pair. The accuracy scores (CPR) for both interior-interior and interior-surface\nare 0.813 and 0.812 while the accuracy score for both surface-interior and surface-surface are\n0.725 and 0.730.\n3.2 Performance analysis based on different mutation types\nSwitching focus to mutation types, our model’s capability in classifying solubility changes also\nmerits exploration across the 20 distinct amino acid types in the dataset. In addition to this,\nwe subgroup amino acids as charged, polar, hydrophobic, or special case. Table S1 enumer-\nates the sample counts for each mutation group pair. Figure 3(a) displays accuracy scores for\neach mutation group pair, while Figure 3(b) shows scores for each amino acid pair. Notably,\nthe special-charged and special-polar groups register the highest accuracy, whereas the polar-\nhydrophobic and polar-special groups underperform. One plausible reason could be the inherent\ncomplexity in accurately classifying mutations with non-negative solubility changes. It’s worth\nnoting that PON-Sol2 employed a two-layer classifier to improve classification [11]. Our results\n13\nindicate that TopLapGBT surpasses the performance of this two-layer system.\nFigure 3: A comparison of 10-fold cross validation accuracy scores (CPR) for (a) different\nmutation groups and (b) its associated amino acid types. The y-axis labels the residue type\nof the original protein, whereas the x-axis labels the residue type of the mutant. The squares\ncolored in black in (b) have zero mutation samples. For a reverse mutation, the labels are taken\nwith reverse solubility change unless the change is zero.\n3.3 Feature analysis based on Residue-Similarity plots\nThe Residue Similarity Index (RSI) serves as a potent metric for evaluating the efficacy of\ndimensionality reduction in both clustering and classification contexts [43]. RSI has proven\nits value in generating classification accuracy scores that align well with supervised methods\nin single-cell typing. When applied to our solubility change dataset, Residue-Similarity (R-S)\nplots can be constructed to scrutinize how the Residue Index (RI) and Similarity Index (SI) may\nindicate the quality of cluster separation.\nFigure 4 juxtaposes the R-S plots derived from TopLapGBT against those from various\nfeature sets utilized in model training. Across all visualizations, samples manifest a range of\nclassification outcomes—both correct and incorrect—for each true label. However, a notewor-\nthy observation is that Transformer-pretrain and persistent Laplacian-based features demon-\n14\nstrate superior clustering attributes compared to auxiliary features. The high RI and SI scores\nfor auxiliary features cause these data points to cluster near the upper regions of their respec-\ntive sections. Despite this, the integrative use of all three feature types in TopLapGBT results\nin appreciable clustering performance, corroborated by the CPR metrics obtained in 10-fold\ncross-validation. To solidify the rationale behind adopting robust supervised classifiers like\nTopLapGBT, we contrast the R-S plots with UMAP visualizations (shown in Figure S2). It\nbecomes evident that UMAP plots fail to form clusters that are as distinct as those observed in\nR-S plots, thereby reinforcing the need for a specialized approach to classify mutation samples\neffectively.\nThe impetus for utilizing structure-based features stems from the multifaceted relationship\nthat exists among protein sequence, structure, and solubility. Factors such as hydrophobic-\nity, charge distribution, and intermolecular interactions contribute to the complexity of protein\nsolubility. Traditional prediction methods, which often rely on empirical rules or rudimen-\ntary descriptors, fall short in capturing this intricate molecular interplay. By employing ad-\nvanced mathematical techniques like persistent Laplacian (PL) coupled with machine learning\nalgorithms, we can decipher the complex patterns and relationships embedded within protein\nsequences and structures. Persistent Laplacian, in particular, provides a robust mathematical\nrepresentation that captures both the topological and homotopic evolution of protein structures.\nFurthermore, machine learning models rooted in advanced mathematics offer several advan-\ntages for classifying changes in protein solubility. These models are well-suited for handling\nhigh-dimensional and complex data sets, such as those involving protein sequences and struc-\ntures. They are also capable of learning non-linear relationships and capturing nuanced de-\npendencies that are often overlooked by traditional linear models. Importantly, these advanced\nmodels can adeptly manage class-imbalanced datasets, which are commonly encountered in\nprotein solubility studies.\n15\nFigure 4: The comparison of R-S plots between the different types of features used in\nTopLapGBT model. The y-axis represents the residue score, whereas the x-axis represents\nthe similarity score. RS scores were computed for the testing set, and all 10-folds were visual-\nized. Each section corresponds to one of the 3 true solubility labels, and the sample’s color and\nmarker correspond to the predicted label from TopLapGBT.\n4 Conclusion\nIn the multifaceted quest to understand mutation-induced solubility changes, various scientific\ndomains including quantum mechanics, molecular mechanics, biochemistry, biophysics, and\nmolecular biology have made significant contributions. Despite these concerted efforts, state-\n16\nof-art models have limitations, as evidenced by their normalized CPR value of 0.656 even after\nemploying feature selection methods. Persistent homology (PH) has emerged as a powerful tool\nfor capturing the complexity of biomolecular structures and has achieved noteworthy success\nin drug discovery applications. However, its inability to capture the nuances of homotopic\nshape evolution, crucial for delineating molecular interactions in proteins, presents a critical\nshortcoming.\nOur study introduces TopLapGBT, a novel model that integrates persistent Laplacian (PL)\nfeatures with pretrained transformer features, thereby bridging the gap in capturing both topol-\nogy and homotopic shape evolution. This innovative fusion leads to significant advancements\nin classification performance. Specifically, TopLapGBT achieves normalized CPR and GC 2\nscores of 0.688 and 0.361, respectively, marking improvements of 4.88% and 15.71% over the\nstate-of-the-art PON-Sol2. These findings are further corroborated by an independent blind test,\nwhere TopLapGBT continues to outperform existing models.\nIn summary, our proposed TopLapGBT model not only achieves superior performance over\nexisting state-of-the-art methods but also introduces a more nuanced approach for the classifi-\ncation of protein solubility changes upon mutation. These results underscore the transformative\npotential of integrating geometric and topological features with machine learning in advancing\nthe field of molecular biology.\n5 Materials and Methods\nIn this section, we endeavor to elucidate key mathematical and computational foundations that\nare instrumental for the work presented in this study. Specifically, we delve into spectral graph\ntheory, simplicial complex, and persistent Laplacian methods, highlighting their significance\nin capturing topological and spectral properties essential for the characterization of proteins.\nAdditionally, we discuss machine learning and deep learning paradigms, focusing on their role\n17\nin processing, analyzing, and interpreting these complex features, especially within the confines\nof test datasets and validation settings.\n5.1 Persistent Laplacian characterization of proteins\nSimplicial complex A simplicial complex is made up of a set of simplices and generalises\nbeyond graph networks at higher dimensions [44, 45, 46, 47]. Every simplex is a finite set of\nvertices which can be interpreted as the atoms in a protein structure. Essentially, simplices can\nbe a point (0-simplex), an edge (1-simplex), a triangle (2-simplex), a tetrahedron (3-simplex),\nor in higher dimensions, a p-simplex. In other words, a k-simplex σk = {v0, v1, · ··, vk} is the\nconvex hull formed by k + 1 affinely independent points v0, v1, · ··, vk as follows,\nσk =\n\u001a\nλ0v0 + λ1v1 + λ2v2 + · ··+ λkvk|\nkX\ni=0\nλi = 1; ∀i, 0 ≤ λi ≤ 1\n\u001b\nA geometric simplicial complex K is a finite set of geometric simplexes that satisfy two\nessential conditions. First, any face of a simplex from K is also in K. Second, the intersection\nof any two simplexes in K is either empty or shares faces. Commonly used methods to con-\nstruct simplicial complexes are ˇCech complex, Vietoris-Rips complex, Alpha complex, Clique\ncomplex, Cubic complex, and Morse complex [44, 45, 46, 47].\nChain Group A k-th chain groupCk is a free Abelian group generated by orientedk-simplices\nσk. A boundary operator ∂k : Ck → Ck−1 defined on an oriented k-simplex σk can be written\nas\n∂kσk =\nkX\ni=0\n(−1)i[v0, v1, v2, · ··, ˆvi, · ··, vk],\nwhere [v0, v1, v2, · ··, ˆvi, · ··, vk] is an oriented (k − 1)-simplex, which is constructed by the\nall the vertices except vi, i.e., removing vi from the simplex. The boundary operator satisfies\n∂k−1∂k = 0.\n18\nThe adjoint of ∂k, which is\n∂∗\nk : Ck−1 → Ck,\nsatisfies the inner product relation ⟨∂k(f), g⟩ = ⟨f, ∂∗\nk(g)⟩, for every f ∈ Ck, g ∈ Ck−1. This\nwill be used in the combinatorial Laplacian.\nCombinatorial Laplacian For the k-boundary operator ∂k : Ck → Ck−1 in K, define Bk to\nbe an m × n matrix representation of the boundary operator under the standard bases {σk\ni }n\ni=1\nand {σk−1\nj }m\nj=1 of Ck and Ck−1. Similarly, the matrix representation of∂∗\nk is the transpose matrix\nB⊤\nk , with respect to the same ordered bases of the boundary operator ∂k.\nMore specifically, let m and n be the number of (k − 1)-simplices and p-simplices respec-\ntively in a simplicial complexK. The m×n boundary matrix Bk has entries defined as follows:\nBk(i, j) =\n\n\n\n1, if σk−1\ni < σk\nj , σk−1\ni ∼ σk\nj .\n−1, if σk−1\ni < σk\nj , σk−1\ni ≁ σk\nj .\n0, if σk−1\ni ≮ σk\nj .\nwhere 1 ≤ i ≤ m and 1 ≤ j ≤ n. Here, σk−1\ni < σk\nj represents the i-th (k − 1)-simplex σk−1\ni\nis a face of j-th k-simplex σk\nj and σk−1\ni ∼ σk\nj indicates the coefficient of σk−1\ni in ∂k(σk\nj ) is 1.\nLikewise, σk−1\ni ≮ σk\nj means that σk−1\ni is not a face of σk\nj and σk−1\ni ≁ σk\nj indicates that the\ncoefficient of σk−1\ni in ∂k(σk\nj ) is −1.\nThen the k-combinatorial Laplacian or the topological Laplacian is a linear operator ∆k :\nCk(K) → Ck(K)\n∆k := ∂k+1∂∗\nk+1 + ∂∗\nk∂k. (1)\nThe k-combinatorial Laplacian exhibits an n × n matrix representation Lk and is given by\nLk = Bk+1B⊤\nk+1 + B⊤\nk Bk. (2)\nIn the case k = 0, then L0 = B1B⊤\n1 since ∂0 is a zero map.\n19\nThe number of rows in Bk represents the number of (k −1)-simplices in K and the number\nof columns refers to the number of k-simplices in K. Furthermore, the upper k-combinatorial\nLaplacian matrix is LU\nk = Bk+1B⊤\nk+1 and the lower k-combinatorial Laplacian matrix is LL\nk =\nB⊤\nk Bk. Recall that since ∂0 is a zero map, hence L0(K) = B1B⊤\n1 with B0 being a zero\nmatrix and K being an oriented simplicial complex of dimension 1. In fact, the0-combinatorial\nLaplacian matrix L0(K) is actually the graph Laplacian in spectral graph theory.\nThe above graph Laplacian matrices can be explicitly described in terms of the simplex\nrelations. More precisely, L0 can be described as\nL0(i, j) =\n\n\n\nd(σ0\ni ), if i = j\n−1, if i ̸= j and σ0\ni ⌢ σ0\nj\n0, if i ̸= j and σ0\ni ̸⌢ σ0\nj ,\nwhich is equivalent to the graph Laplacian. Furthermore, when k >0, Lk can be expressed as\nLk(i, j) =\n\n\n\nd(σk\ni ) + k + 1, if i = j\n1, if i ̸= j, σk\ni ̸⌢ σk\nj , σk\ni ⌣ σk\nj and σk\ni ∼ σk\nj\n−1, if i ̸= j, σk\ni ̸⌢ σk\nj , σk\ni ⌣ σk\nj and σk\ni ̸∼ σk\nj\n0, if i ̸= j, and either σk\ni ⌢ σk\nj or σk\ni ̸⌣ σk\nj .\nHere, we denote σk−1\nj ∼ σk\ni if they have the same orientation, i.e. similarly oriented. Fur-\nthermore, we say that two k-simplices σk\ni and σk\nj are upper adjacent (resp. lower adjacent)\nneighbors, denoted as σk\ni ⌢ σk\nj (resp. σk\ni ⌣ σk\nj ), if they are both faces of a common (k + 1)-\nsimplex (resp. they both share a common (k − 1)-simplex as their face). In addition, if the\norientations of their common lower simplex are the same, it is called similar common lower\nsimplex (σk\ni ⌣ σk\nj and σk\ni ∼ σk\nj ). On the other hand, if the orientations are different, it is called\ndissimilar common lower simplex (σk\ni ⌣ σk\nj and σk\ni ≁ σk\nj ). The (upper) degree of a k-simplex\nσk\ni , denoted as d(σk\ni ), is the number of (k + 1)-simplices, of which σk\ni is a face.\nThe eigenvalues of combinatorial Laplacian matrices are independent of the choice of the\norientation [48]. Furthermore, the multiplicity of zero eigenvalues, i.e. the total number of\n20\nzero eigenvalues, of Lk corresponds to the kth Betti number βk, according to the combinatorial\nHodge theorem [49]. The kth Betti numbers are topological invariants that describe the k-\ndimensional holes in a simplicial complex. In particular, β0, β1 and β2 represents the numbers\nof independent components, rings and cavities, respectively.\nPersistent Laplacian Persistent Laplacian (PL) were first introduced by integrating graph\nLaplacian and multiscale filtration [25]. Analyzing the spectra of k-combinatorial Laplacian\nmatrix allows both topological and geometric information (i.e. connectivity and robustness of\nsimple graphs) to be obtained. However, this method is genuinely free of metrics or coordinates,\nwhich induced too little topological and geometric information that can be used to describe a\nsingle configuration.\nTherefore, PL was extended to simplicial complexes. This allows a sequence of simplicial\ncomplexes from a filtration process to generate persistent Laplacian which is largely inspired by\npersistent homology and in earlier works in multiscale graphs. For the rest of this section, we\nintroduce mainly on the construction of PL. First, a k-combinatorial Laplacian matrix is sym-\nmetric and positive semi-definite. Therefore, its eigenvalues are all real and non-negative. The\nmultiplicity of zero spectra (also called harmonic spectra) reveals the topological information,\nand the geometric information will be preserved in the non-harmonic spectra.\nA key concept of PL is the filtration process. Essentially, an ever-increasing filtration value\nf is used to generate a series of topological spaces, which are represented by a nested sequence\nof multiscale simplicial complexes. Naturally, PL generates a sequence of simplicial complexes\ninduced by varying a filtration parameter [25]. For an oriented simplicial complex K, its filtra-\ntion is a nested sequence of simplicial complexes (Kt)m\nt=0 of K\n∅ = K0 ⊆ K1 ⊆ ·· · ⊆Km = K.\n21\nThis nested sequence of simplicial complexes induces a family of chain complexes\n(\n· ··\n∂t\nk+2\n⇋\n∂t∗\nk+2\nCt\nk+1\n∂t\nk+1\n⇋\n∂t∗\nk+1\nCt\nk\n∂t\nk\n⇋\n∂t∗\nk\n· ··\n∂t\n1\n⇋\n∂t∗\n1\nCt\n0\n∂t\n0\n⇋\n∂t∗\n0\n∅\n)\nt∈R+\n. (3)\nwhere Ct\nk = Ck(Kt) is the chain group for the subcomplex Kt, and its k-boundary operator is\n∂t\nk : Ck(Kt) → Ck−1(Kt). In the case k <0, then Ck(Kt) is an empty set and ∂t\nk is a zero map.\nFor 0 < k <dim(Kt), the boundary operator\n∂t\nk(σk) =\nkX\ni=0\n(−1)iσk−1\ni , σ k ∈ Kt, (4)\nwith σk = [v0, · ··, vk] being the k-simplex, and σk−1\ni = [v0, · ··, ˆvi, · ··, vk] being the (k − 1)-\nsimplex for which its vertex vi is removed. Similarly, the adjoint of ∂t\nk is the operator ∂t∗\nk :\nCk−1(Kt) → Ck(Kt). The topological and spectral characteristics can then be studied from\nLk(Kt) by varying the filtration parameter and diagonalizing the k-combinatorial Laplacian\nmatrix. The multiplicity of the zero spectra of Lt\nk is the persistent Betti number βt\nk, which\nrepresents the number of k-dimensional holes in Kt. In other words,\nβt\nk = dim(Lt\nk) − rank(Lt\nk) = nullity(Lt\nk) = # of harmonic spectra of Lt\nk. (5)\nIn particular, βt\n0 represents the number of connected components in Kt, βt\n1 counts the num-\nber of one-dimensional cycles in Kt and βt\n2 reveals the number of two-dimensional voids inKt.\nIn addition, the spectra of Lt\nk can be written in the following ascending order\nSpectra(Lt\nk) = {(λ1)t\nk, (λ2)t\nk, · ··, (λn)t\nk}, (6)\nwhere Lt\nk here is an n × n matrix. The p-persistent k-combinatorial Laplacian can be extended\nbased on the boundary operator as well. Further details can be found in [25].\nIn order to illustrate the difference between PL and PH, Figure 5 describes a point cloud,\nbasic simplices, a filtration process and the comparison between persistent Laplacian and per-\nsistent homology barcodes of 13 points. The filtration process in Figure 5(c) shows the different\n22\nstages of a Rips filtration process for the 13 points. Figure 5(d) shows the persistent homology\nbarcodes (in blue) and persistent non-harmonic spectra (in red). It can be seen that the non-\nharmonic spectra provides the additional homotopic shape evolution that is missing in persistent\nhomology in the later part of the filtration process.\n5.2 Persistent Laplacian descriptors\nIn order to capture the mutation-induced solubility change, we apply the persistent Laplacian\n(PL) to characterize the interactions between the mutation site and the rest of the protein.\nTo describe these interactions, we first propose the interactive PL with the distance function\nDI(Ai, Aj) describing the distance between two atoms Ai and Aj defined as\nDI(Ai, Aj) =\n(\n∞, if Loc(Ai) = Loc(Aj),\nDE(Ai, Aj), otherwise. (7)\nwhere DE(·, ·) is the Euclidean distance between the two atoms and Loc(·) refers to the atom’s\nlocation which is either in the mutation site or in the rest of the protein. Here, we construct\ntwo types of simplicial complexes in our PL computation, such as Vietoris-Rips complex (VC)\nand Alpha complex (AC). Both complexes are used to characterize the first order interactions\nand higher order patterns respectively. To capture and characterize different types of atom-\natom interactions, we generate the PL based on different atom subsets by selecting one type of\natom in the mutation site and one other atom type in the rest of the protein. Different types of\natom-atom interactions characterize the different interactions in proteins. For example, inter-\nactions generated from carbon atoms are associated with hydrophobic interactions. Similarly,\ninteractions between nitrogen and oxygen atoms correlate to hydrophilic interactions and/or hy-\ndrogen bonds. Both types of interactions are illustrated in Figure 6. Interactive PLs have the\ncapability to unveil additional details about bonding interactions and offer a fresh and distinct\nrepresentation of molecular interactions in proteins.\n23\nβ0\nβ1\nλ0\nλ1\nFigure 5: The illustration of (a) point cloud, (b) basic simplices and (c) the filtration process\nand (d) Comparison between PH barcodes [50, 12] and the non-harmonic spectra of persistent\nLaplacians (PLs) [25] from the filtration process in (c). The x-axis represents the filtration\nparameter f. By discretising the filtration region into equal-sized bins and adding all the Betti\nbars together, the topological invariants are summarized into persistent Betti numbers that acts\na topological descriptor extracted from protein structures. Persistent Laplacians (PLs) [25] for\nthirteen points. The first non-zero eigenvalues of dimension 0, λ0(r), and dimension 1, λ1(r),\nof PLs are depicted in red. The harmonic spectra of PLs return all the topological invariants of\nPH, whereas the non-harmonic spectra of PLs capture the additional homotopic shape evolution\nof PLs during the filtration that are neglected by PH.\n24\nThe set of persistent spectra from each persistent Laplacian computation consists of V DI\nγ,α,β\nand V DE\nγ,α,β where γ ∈ {M, W} refers to the mutant protein or the wild type protein, α ∈\n{C, N, O} is the atom type chosen in the rest of the protein and β ∈ {C, N, O} is the atom\ntype chosen in the mutation site. V DI\nγ,α,β applies the distance DI-based filtration to generate\n0-dimensional Laplacian using the Vietoris-Rips complex and V DE\nγ,α,β applies the Euclidean dis-\ntance DE-based filtration to generate 1 and 2-dimensional Laplacian using the alpha complex.\nIn total, there are 54 sets of persistent spectra. The persistent spectra from PL contains both\nharmonic and non-harmonic spectra that are capable of revealing the molecular mechanism of\nprotein solubility.\nβ0\nβ0\nβ1\nλ0\nλ0\nλ1\nFigure 6: An illustration of interactive PL showing hydrophillic interactions based on DI-based\nfiltration (left) and hydrophobic interactions based on DE-based filtration (right) at a mutation\nsite. (a) Hydrophillic interactions between the nitrogen atoms (red) and oxygen atoms (yellow).\n(b) Hydrophobic interactions between the carbon atoms (cyan) and oxygen atoms (dark blue).\nThe hexagon ring is colored in red and the triangle is colored in yellow. (c) The PH barcodes\nand PL for dimension 0 of the hydrophillic interactions in (a). (d) The PH barcodes and PL for\ndimension 0 and dimension 1 of the hydrophobic interactions in (b). The Betti-1 bar is due to\nthe red hexagon ring in (b).\nFor zero dimensions, we consider both the harmonic spectra and non-harmonic spectra in-\n25\nformation for each persistent Laplacian. Filtration using Rips complex with DI distance is\nused. The 0-dimensional PL features are generated from 0 ˚A to 6 ˚A with 0.5 ˚A gridsize. For the\nnon-harmonic spectra information, we count the number of non-harmonic spectra and calcu-\nlate seven statistical values of non-harmonic spectra such as sum, minimum, maximum, mean,\nstandard deviation, variance and the sum of eigenvalues squared. This generates eight statistical\nvalues for each of the nine atomic pairs. Therefore, the dimension of 0-dimensional PL features\nfor a protein is 72. In total, the 0-dimensional PL-based feature size after concatenating features\nat different dimensions for wild type and mutant is 1872.\nFor one or two dimensions, we perform the filtration using Alpha complex with the DE\ndistance. The limited number of atoms in the local protein structure can create only a few\nhigh-dimensional simplexes, resulting in minimal alterations in shape. As a result, it suffice to\nconsider features from only harmonic spectra of persistent Laplacians by coding the topologi-\ncal invariants for the high-dimensional interactions. Using GUDHI[51], the persistence of the\nharmonic spectra can be represented by persistent barcodes. The topological feature vectors\nare generated by computing the statistics of bar lengths, births and deaths. Bars shorter than\n0.1 ˚A are excluded as they do not exhibit any clear physical meaning. The remaining bars are\nthen used for computing the statistics: (1) sum, maximum and mean for lengths of bars; (2)\nminimum and maximum for the birth values of bars; (3) minimum and maximum for the death\nvalues of bars. Each set of point clouds leads to a seven-dimensional vector. These features\nare calculated on nine single atomic pairs and one heavy atom pair. The dimension of one-\nand two-dimensional PL feature vectors for a protein is 140. In total, the higher-dimensional\nPL-based feature size after concatenating features at different dimensions for wild type, mutant\nand their difference is 420.\n26\n5.3 Persistent Homology\nPersistent homology is part of the harmonic spectra of PL. The homology groups in PH illustrate\nthe persistence of topological invariants, hence providing the harmonic spectral information in\nPL. The site- and element-specific PH features are generated in a similar way as compared to\nPL. Similar to PL, filtration construction is also employed to PH. For the zero dimension, the\nfiltration parameter can be discretized into several equally spaced bins, namely [0, 0.5], (0.5, 1],\n· ··, (5.5, 6] ˚A. The death value of the bars are summed in each bin resulting in 12×18 features.\nFor each bin, we count the numbers of persistent bars, resulting in a nine-dimensional vector\nfor each point cloud. Similarly, this is performed for each of the nine single atomic pairs. Hence,\nthe dimension of PH features for a protein is 216. For one or two dimensions, the identical\nfeaturization from the statistics of persistent bars in PH is used. The PH embedding combines\nfeatures at different dimensions as described above and concatenated for wild type, mutant and\ntheir difference, resulting in a 648-dimensional vector.\n5.4 Transformer Features\nRecently, we have seen significant advancements in modelling protein properties using large-\nscale protein transformer models trained on hundreds of millions of sequences. These models,\nlike ESM [33] (evolutionary scale modeling) and ProtTrans[34, 35], have demonstrated impres-\nsive performance. Moreover, hybrid fine-tuning approaches that leverage both local and global\nevolutionary data have proven to enhance these models even further. For instance, eUniRep\nis an improved LSTM-based UniRep model achieved through fine-tuning with knowledge ex-\ntracted from local multiple sequence alignments (MSAs). Additionally, the ESM model can\nbe fine-tuned using either downstream task data or local MSAs. In our research, we employed\nthe ESM-1b transformer, a model that falls under the transformer architecture. This particular\nvariant was trained on a dataset of 250 million sequences using a masked filling procedure and\n27\nboasts an architecture comprising 34 layers with a whopping 650 million parameters. The ESM\ntransformer’s primary role in our work was to generate sequence embeddings. At each layer of\nthe ESM model, it encoded a sequence of length L into a matrix sized at 1,280 ×L, excluding\nthe start and terminal tokens. For our study, we utilized the sequence representation derived\nfrom the final (34th) layer and computed the average along the sequence length axis, resulting\nin a 1,280-component vector.\n5.5 Performance Metrics\nPPV and NPV assesses the true positive and true negative proportion of the predicted results for\neach solubility class. PPV and NPV are computed based on TP, TN, FP and FN which represents\nthe true positive, true negative, false positive and false negative values for each solubility class.\nFor each solubility class, PPV and NPV can be computed by:\nPPV = TP\nTP + FP. (8)\nNPV = TN\nTN + FN. (9)\nFurthermore, specificity and sensitivity can be computed by the following:\nSpecificity = TN\nTN + FP. (10)\nSensitivity = TP\nTP + FN. (11)\nThe correct prediction ratio (CPR) and generalized squared correlation (GC2) are used to evaulate\nthe overall performance of TopLapGBT. CPR and GC2 can be computed as\nCPR = 1\nN\nX\ni\nzii, and (12)\nGC2 = 1\nN(K − 1)\nX\nij\n(zij − eij)2\neij\n, (13)\n28\nwhere K is the number of classes and N is the number of samples. Here, zij represents the\nnumber of samples of class i to class j. Let xi = P\nj zij be the number of inputs from class\ni, and yj = P\ni zij be the number of inputs predicted to class j. Then the expected number of\nsamples in (i, j)-th entry of the multiclass confusion matrix is\neij = xiyj\nN .\nSince the mutational samples across the three solubility classes are imbalanced, we normalized\nthe values to provide more reliable calculation of performance metrics.\n6 Software and resources\nProtein sequences are first preprocessed by AlphaFold 2 to generate wild type protein struc-\ntures. In particular, 3D protein structures are generated from protein sequences using Colab-\nFold [52]. Mutant proteins are generated from the Jackal software[38]. All TopLapGBT mod-\nels are built using the sklearn machine learning library [53]. The hyperparameters for all the\nTopLapGBT are: n estimators = 20000, learning rate = 0.05, max depth = 7, subsample=0.4,\nmin sample split = 3 and max features = sqrt. The PQR files, which contains the partial charge\ninformation of the proteins, are generated from the PDB2PQR software [54]. The PQR files for\nboth the wild type proteins are generated with AMBER force field. The solvation energy and\nsurface area information are calculated from the in-house online software package ESES [55]\nand MIBPB [56]. The pKa values are computed from the PROPKA software package [57]. The\nposition-specific-scoring matrices (PSSM) are computed from the BLAST+ software [58] using\nthe nr database. The secondary structure features and torsion angle sequence-based information\nare calculated from SPIDER [59]. The persistent Laplacian descriptors for both VR complexes\nand alpha complexes are calculated using the GUDHI software library [60]. All computational\nwork in support of this research was performed using the resources from the National Super\n29\nComputing Centre of Singapore (NSCC).\nCode and Data Availability\nThe 3D protein structures and the TopLapGBT code can be found in\nhttps://github.com/ExpectozJJ/TopLapGBT. The source code for the R-S plot can be found at\nhttps://github.com/hozumiyu/RSI.\nSupporting Information\nSupporting Information is available for supplementary tables, figures, and methods.\nAcknowledgments\nThis work was supported in part by NIH grants R01GM126189, R01AI164266, and R01AI146210,\nNSF grants DMS-2052983, DMS-1761320, and IIS-1900473, NASA grant 80NSSC21M0023,\nMSU Foundation, Bristol-Myers Squibb 65109, and Pfizer. It was supported in part by Nanyang\nTechnological University Startup Grant M4081842.110, Singapore Ministry of Education Aca-\ndemic Research fund Tier 1 RG109/19 and Tier 2 MOE-T2EP20120-0013, MOE-T2EP20220-\n0010, and MOE-T2EP20221-0003.\nReferences\n[1] Y . Qiu and G.-W. Wei, “Persistent spectral theory-guided protein engineering,” Nature\nComputational Science, vol. 3, no. 2, pp. 149–163, 2023.\n30\n[2] R. Guerois, J. E. Nielsen, and L. Serrano, “Predicting changes in the stability of proteins\nand protein complexes: a study of more than 1000 mutations,” Journal of molecular biol-\nogy, vol. 320, no. 2, pp. 369–387, 2002.\n[3] P. Sormanni, F. A. Aprile, and M. Vendruscolo, “The CamSol method of rational design of\nprotein mutants with enhanced solubility,” Journal of molecular biology, vol. 427, no. 2,\npp. 478–490, 2015.\n[4] Y . Tian, C. Deutsch, and B. Krishnamoorthy, “Scoring function to predict solubility mu-\ntagenesis,” Algorithms for Molecular Biology, vol. 5, no. 1, pp. 1–11, 2010.\n[5] Y . Yang, A. Niroula, B. Shen, and M. Vihinen, “PON-Sol: Prediction of effects of amino\nacid substitutions on protein solubility,” Bioinformatics, vol. 32, no. 13, pp. 2032–2034,\n2016.\n[6] L. Paladin, D. Piovesan, and S. C. Tosatto, “SODA: Prediction of protein solubility from\ndisorder and aggregation propensity,”Nucleic acids research, vol. 45, no. W1, pp. W236–\nW240, 2017.\n[7] J. Van Durme, G. De Baets, R. Van Der Kant, M. Ramakers, A. Ganesan, H. Wilkinson,\nR. Gallardo, F. Rousseau, and J. Schymkowitz, “Solubis: a webserver to reduce protein\naggregation through mutation,” Protein Engineering, Design and Selection, vol. 29, no. 8,\npp. 285–289, 2016.\n[8] M. Vihinen, “Solubility of proteins,” ADMET and DMPK , vol. 8, no. 4, pp. 391–399,\n2020.\n[9] A.-M. Fernandez-Escamilla, F. Rousseau, J. Schymkowitz, and L. Serrano, “Prediction of\nsequence-dependent and mutational effects on the aggregation of peptides and proteins,”\nNature Biotechnology, vol. 22, no. 10, pp. 1302–1306, 2004.\n31\n[10] H. Land and M. S. Humble, “Y ASARA: A tool to obtain structural guidance in biocatalytic\ninvestigations,”Protein engineering: methods and protocols, pp. 43–67, 2018.\n[11] Y . Yang, L. Zeng, and M. Vihinen, “PON-Sol2: Prediction of effects of variants on protein\nsolubility,”International Journal of Molecular Sciences, vol. 22, no. 15, p. 8027, 2021.\n[12] H. Edelsbrunner and J. L. Harer, Computational Topology: An Introduction . American\nMathematical Society, 2022.\n[13] A. Zomorodian and G. Carlsson, “Computing Persistent Homology,” in Proceedings of\nthe twentieth annual symposium on Computational geometry, pp. 347–356, 2004.\n[14] K. Xia and G.-W. Wei, “Persistent homology analysis of protein structure, flexibility, and\nfolding,” International journal for numerical methods in biomedical engineering, vol. 30,\nno. 8, pp. 814–844, 2014.\n[15] Z. Cang, L. Mu, K. Wu, K. Opron, K. Xia, and G.-W. Wei, “A topological approach for\nprotein classification,” Computational and Mathematical Biophysics, vol. 3, no. 1, 2015.\n[16] Z. Cang and G.-W. Wei, “Topologynet: Topology based deep convolutional and multi-\ntask neural networks for biomolecular property predictions,”PLoS computational biology,\nvol. 13, no. 7, p. e1005690, 2017.\n[17] Z. X. Cang and G. W. Wei, “Integration of element specific persistent homology and ma-\nchine learning for protein-ligand binding affinity prediction,” International journal for\nnumerical methods in biomedical engineering, vol. 34, no. 2, p. e2914, 2018.\n[18] M. Wang, Z. Cang, and G.-W. Wei, “A topology-based network tree for the prediction\nof protein–protein binding affinity changes following mutation,” Nature Machine Intelli-\ngence, vol. 2, no. 2, pp. 116–123, 2020.\n32\n[19] J. Chen, R. Wang, M. Wang, and G.-W. Wei, “Mutations strengthened SARS-CoV-2 in-\nfectivity,”Journal of molecular biology, vol. 432, no. 19, pp. 5212–5226, 2020.\n[20] D. D. Nguyen, Z. Cang, K. Wu, M. Wang, Y . Cao, and G.-W. Wei, “Mathematical deep\nlearning for pose and binding affinity prediction and ranking in D3R grand challenges,”\nJournal of computer-aided molecular design, vol. 33, pp. 71–82, 2019.\n[21] D. D. Nguyen, K. Gao, M. Wang, and G.-W. Wei, “MathDL: mathematical deep learning\nfor d3r grand challenge 4,”Journal of computer-aided molecular design, vol. 34, pp. 131–\n147, 2020.\n[22] D. D. Nguyen, Z. Cang, and G.-W. Wei, “A review of mathematical representations of\nbiomolecular data,” Physical Chemistry Chemical Physics, vol. 22, no. 8, pp. 4343–4367,\n2020.\n[23] R. Wang, D. D. Nguyen, and G.-W. Wei, “Persistent spectral graph,” arXiv preprint\narXiv:1912.04135, 2019.\n[24] J. Chen, R. Zhao, Y . Tong, and G.-W. Wei, “Evolutionary de Rham-Hodge method,”arXiv\npreprint arXiv:1912.12388, 2019.\n[25] R. Wang, D. D. Nguyen, and G.-W. Wei, “Persistent spectral graph,”International Journal\nfor Numerical Methods in Biomedical Engineering, p. e3376, 2020.\n[26] Z. Meng and K. Xia, “Persistent spectral–based machine learning (PerSpect ML) for\nprotein-ligand binding affinity prediction,” Science Advances, vol. 7, no. 19, p. eabc5329,\n2021.\n[27] J. Wee and K. Xia, “Persistent spectral based ensemble learning (PerSpect-EL) for\nprotein–protein binding affinity prediction,”Briefings in Bioinformatics, p. bbac024, 2022.\n33\n[28] J. Bi, J. Wee, X. Liu, C. Qu, G. Wang, and K. Xia, “Multiscale Topological Indices for\nthe Quantitative Prediction of SARS CoV-2 Binding Affinity Change upon Mutations,”\nJournal of Chemical Information and Modeling, vol. 63, no. 13, pp. 4216–4227, 2023.\n[29] J. Chen, Y . Qiu, R. Wang, and G.-W. Wei, “Persistent Laplacian projected Omicron BA.4\nand BA.5 to become new dominating variants,” Computers in Biology and Medicine ,\nvol. 151, p. 106262, 2022.\n[30] R. Rao, N. Bhattacharya, N. Thomas, Y . Duan, P. Chen, J. Canny, P. Abbeel, and Y . Song,\n“Evaluating protein transfer learning with TAPE,” Advances in neural information pro-\ncessing systems, vol. 32, 2019.\n[31] T. Bepler and B. Berger, “Learning protein sequence embeddings using information from\nstructure,” in International Conference on Learning Representations, 2018.\n[32] E. C. Alley, G. Khimulya, S. Biswas, M. AlQuraishi, and G. M. Church, “Unified rational\nprotein engineering with sequence-based deep representation learning,” Nature methods,\nvol. 16, no. 12, pp. 1315–1322, 2019.\n[33] A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo, M. Ott, C. L. Zitnick, J. Ma,\net al., “Biological structure and function emerge from scaling unsupervised learning to 250\nmillion protein sequences,” Proceedings of the National Academy of Sciences , vol. 118,\nno. 15, p. e2016239118, 2021.\n[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\nI. Polosukhin, “Attention is all you need,” Advances in Neural Information Processing\nSystems, vol. 30, 2017.\n34\n[35] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidi-\nrectional Transformers for Language Understanding,” in North American Chapter of the\nAssociation for Computational Linguistics, 2019.\n[36] J. Meier, R. Rao, R. Verkuil, J. Liu, T. Sercu, and A. Rives, “Language models enable\nzero-shot prediction of the effects of mutations on protein function,” inAdvances in Neural\nInformation Processing Systems (M. Ranzato, A. Beygelzimer, Y . Dauphin, P. Liang, and\nJ. W. Vaughan, eds.), vol. 34, pp. 29287–29303, Curran Associates, Inc., 2021.\n[37] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasu-\nvunakool, R. Bates, A. ˇZ´ıdek, A. Potapenko, et al., “Highly accurate protein structure\nprediction with alphafold,” Nature, vol. 596, no. 7873, pp. 583–589, 2021.\n[38] J. Z. Xiang and B. Honig, “Jackal: A protein structure modeling package,” Columbia\nUniversity and Howard Hughes Medical Institute, New York, 2002.\n[39] R. M. Rao, J. Liu, R. Verkuil, J. Meier, J. Canny, P. Abbeel, T. Sercu, and A. Rives, “MSA\nTransformer,” in International Conference on Machine Learning, pp. 8844–8856, PMLR,\n2021.\n[40] P. Baldi, S. Brunak, Y . Chauvin, C. A. Andersen, and H. Nielsen, “Assessing the accuracy\nof prediction algorithms for classification: An Overview,” Bioinformatics, vol. 16, no. 5,\npp. 412–424, 2000.\n[41] E. D. Levy, “A simple definition of structural regions in proteins and its use in analyzing\ninterface evolution,”Journal of molecular biology, vol. 403, no. 4, pp. 660–670, 2010.\n[42] D. S. Goodsell, L. Autin, and A. J. Olson, “Illustrate: Software for biomolecular illustra-\ntion,” Structure, vol. 27, no. 11, pp. 1716–1720, 2019.\n35\n[43] Y . Hozumi, K. A. Tanemura, and G.-W. Wei, “Preprocessing of Single Cell RNA Sequenc-\ning Data Using Correlated Clustering and Projection,” Journal of Chemical Information\nand Modeling, 2023.\n[44] J. R. Munkres, Elements of algebraic topology. CRC Press, 2018.\n[45] A. J. Zomorodian, Topology for computing, vol. 16. Cambridge university press, 2005.\n[46] H. Edelsbrunner and J. Harer, Computational topology: an introduction. American Math-\nematical Soc., 2010.\n[47] K. Mischaikow and V . Nanda, “Morse theory for filtrations and efficient computation of\npersistent homology,”Discrete and Computational Geometry, vol. 50, no. 2, pp. 330–353,\n2013.\n[48] D. Horak and J. Jost, “Spectra of combinatorial Laplace operators on simplicial com-\nplexes,” Advances in Mathematics, vol. 244, pp. 303–336, 2013.\n[49] B. Eckmann, “Harmonische funktionen und randwertaufgaben in einem komplex,” Com-\nmentarii Mathematici Helvetici, vol. 17, no. 1, pp. 240–255, 1944.\n[50] A. Zomorodian, “Topological data analysis,” Advances in applied and computational\ntopology, vol. 70, pp. 1–39, 2012.\n[51] T. G. Project, GUDHI User and Reference Manual. GUDHI Editorial Board, 2015.\n[52] M. Mirdita, K. Sch ¨utze, Y . Moriwaki, L. Heo, S. Ovchinnikov, and M. Steinegger, “Colab-\nFold: Making protein folding accessible to all,” Nature methods, vol. 19, no. 6, pp. 679–\n682, 2022.\n36\n[53] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blon-\ndel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,\nM. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine Learning in Python,”\nJournal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011.\n[54] T. J. Dolinsky, J. E. Nielsen, J. A. McCammon, and N. A. Baker, “PDB2PQR: An auto-\nmated pipeline for the setup of Poisson–Boltzmann electrostatics calculations,” Nucleic\nacids research, vol. 32, no. suppl 2, pp. W665–W667, 2004.\n[55] B. Liu, B. Wang, R. Zhao, Y . Tong, and G.-W. Wei, “ESES: Software for eulerian solvent\nexcluded surface,” 2017.\n[56] D. Chen, Z. Chen, C. Chen, W. Geng, and G.-W. Wei, “MIBPB: A software package for\nelectrostatic analysis,” Journal of computational chemistry , vol. 32, no. 4, pp. 756–770,\n2011.\n[57] H. Li, A. D. Robertson, and J. H. Jensen, “Very fast empirical prediction and rationaliza-\ntion of protein pKa values,” Proteins: Structure, Function, and Bioinformatics , vol. 61,\nno. 4, pp. 704–721, 2005.\n[58] M. Johnson, I. Zaretskaya, Y . Raytselis, Y . Merezhuk, S. McGinnis, and T. L. Madden,\n“NCBI BLAST: A better web interface,” Nucleic acids research , vol. 36, no. suppl 2,\npp. W5–W9, 2008.\n[59] R. Heffernan, K. Paliwal, J. Lyons, A. Dehzangi, A. Sharma, J. Wang, A. Sattar, Y . Yang,\nand Y . Zhou, “Improving prediction of secondary structure, local backbone angles and\nsolvent accessible surface area of proteins by iterative deep learning,” Scientific reports,\nvol. 5, no. 1, p. 11476, 2015.\n37\n[60] C. Maria, J.-D. Boissonnat, M. Glisse, and M. Yvinec, “The GUDHI library: Simplicial\ncomplexes and persistent homology,” in Mathematical Software–ICMS 2014: 4th Inter-\nnational Congress, Seoul, South Korea, August 5-9, 2014. Proceedings 4 , pp. 167–174,\nSpringer, 2014.\n38",
  "topic": "Mutant",
  "concepts": [
    {
      "name": "Mutant",
      "score": 0.6437895894050598
    },
    {
      "name": "Solubility",
      "score": 0.619856059551239
    },
    {
      "name": "Transformer",
      "score": 0.5771769881248474
    },
    {
      "name": "Computer science",
      "score": 0.564482569694519
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4878282845020294
    },
    {
      "name": "Mutation",
      "score": 0.43328163027763367
    },
    {
      "name": "Computational biology",
      "score": 0.397811621427536
    },
    {
      "name": "Machine learning",
      "score": 0.39326852560043335
    },
    {
      "name": "Biology",
      "score": 0.3353763818740845
    },
    {
      "name": "Genetics",
      "score": 0.24649208784103394
    },
    {
      "name": "Chemistry",
      "score": 0.23162680864334106
    },
    {
      "name": "Gene",
      "score": 0.18582206964492798
    },
    {
      "name": "Engineering",
      "score": 0.18186917901039124
    },
    {
      "name": "Voltage",
      "score": 0.09248965978622437
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}