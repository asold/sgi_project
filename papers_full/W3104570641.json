{
    "title": "Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models",
    "url": "https://openalex.org/W3104570641",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A3021949767",
            "name": "Isabel Papadimitriou",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2089131864",
            "name": "Dan Jurafsky",
            "affiliations": [
                "Stanford University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2998696444",
        "https://openalex.org/W2963088995",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2970854433",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W2963737810",
        "https://openalex.org/W2970862333",
        "https://openalex.org/W3103536442",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W3015449890",
        "https://openalex.org/W2899024931",
        "https://openalex.org/W2048176942",
        "https://openalex.org/W3111372685",
        "https://openalex.org/W2023723978",
        "https://openalex.org/W2983086530",
        "https://openalex.org/W2962832505",
        "https://openalex.org/W2964204621",
        "https://openalex.org/W2972889706",
        "https://openalex.org/W2970350231",
        "https://openalex.org/W2126793110",
        "https://openalex.org/W1951216520",
        "https://openalex.org/W2165545766",
        "https://openalex.org/W331019419",
        "https://openalex.org/W3014415613",
        "https://openalex.org/W2739967986",
        "https://openalex.org/W2963751529",
        "https://openalex.org/W2963400886",
        "https://openalex.org/W2973122905",
        "https://openalex.org/W4299838440",
        "https://openalex.org/W3104235057",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W2950547518",
        "https://openalex.org/W3035305735"
    ],
    "abstract": "We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.",
    "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6829–6839,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n6829\nLearning Music Helps You Read: Using Transfer to Study Linguistic\nStructure in Language Models\nIsabel Papadimitriou\nStanford University\nisabelvp@stanford.edu\nDan Jurafsky\nStanford University\njurafsky@stanford.edu\nAbstract\nWe propose transfer learning as a method for\nanalyzing the encoding of grammatical struc-\nture in neural language models. We train\nLSTMs on non-linguistic data and evaluate\ntheir performance on natural language to as-\nsess which kinds of data induce generalizable\nstructural features that LSTMs can use for nat-\nural language. We ﬁnd that training on non-\nlinguistic data with latent structure (MIDI mu-\nsic or Java code) improves test performance on\nnatural language, despite no overlap in surface\nform or vocabulary. To pinpoint the kinds of\nabstract structure that models may be encod-\ning to lead to this improvement, we run simi-\nlar experiments with two artiﬁcial parentheses\nlanguages: one which has a hierarchical recur-\nsive structure, and a control which has paired\ntokens but no recursion. Surprisingly, training\na model on either of these artiﬁcial languages\nleads the same substantial gains when testing\non natural language. Further experiments on\ntransfer between natural languages controlling\nfor vocabulary overlap show that zero-shot per-\nformance on a test language is highly corre-\nlated with typological syntactic similarity to\nthe training language, suggesting that represen-\ntations induced by pre-training correspond to\nthe cross-linguistic syntactic properties. Our\nresults provide insights into the ways that neu-\nral models represent abstract syntactic struc-\nture, and also about the kind of structural in-\nductive biases which allow for natural lan-\nguage acquisition. 1\n1 Introduction\nUnderstanding how neural language models learn\nand represent syntactic structure is an important an-\nalytic question for NLP. Recent work has directly\nprobed the internal activations of models (Conneau\n1We release code to construct the corpora and run\nour experiments at https://github.com/toizzy/\ntilt-transfer\nRandom Uniform\nRandom Zipf\nMusicCode\nNesting Parens\nFlat ParensJapaneseEnglish\nPortuguese\nPretraining Language (L1)\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\nPpl when tested on Spanish (L2) \n (Lower is better)\nFigure 1: We ﬁnd that LSTM LMs can utilize vari-\nous types of non-linguistic structure to help learn to\nmodel human language, and that nested hierarchical\nstructure does not lead to more expressive encodings\nthan ﬂat, head-dependency pair structure. We also\nﬁnd that LSTM LMs learn representations that corre-\nlate with typological syntactic feature distance, allow-\ning them to transfer more effectively from languages\nwhich are grammatically similar.\net al., 2018a; Dalvi et al., 2019; Hewitt and Man-\nning, 2019; Clark et al., 2019), or fed them curated\ninputs that depend on complex syntax (Linzen et al.,\n2016; Gulordava et al., 2018; Talmor et al., 2019;\nMcCoy et al., 2020), in order to uncover latent\nsyntactic awareness.\nWe propose a different approach: we measure\nthe structural awareness of a language model by\nstudying how much this structure acts as an induc-\ntive bias to improve learning when we transfer from\none language or symbolic system to another.\nWe train LSTM models on data with varying de-\ngrees of language-like structure (music, Java code,\nnested symbols), and then evaluate their perfor-\nmance on natural language. Before evaluation, we\nfreeze the LSTM parameters and ﬁne-tune the word\nembeddings on the evaluation language. This lets\nus see if the training data induces language-like\n6830\nLSTMLin Lin\nLSTMLin Lin\nLSTMLin Lin\nLSTMLin Lin\nLSTMLin Lin\nLSTMLin Lin\nLSTM\nLSTM\nLSTM\n①\nTrain on L1\n③\nFinetune word \nembeddings on L2\nSpanish\nSpanish\nSpanish\nSpanish\nTest\nMusic\nParen- \ntheses\nJapan-\nese\n②\nFreeze LSTM \nparameters\n④\nTest on L2\nFigure 2: Diagram illustrating our training procedure: k models are trained on k L1 languages, and then their\nLSTM weights are frozen while their linear layers are ﬁnetuned on a common L2 language (in our case, we always\nuse Spanish as the L2). We can then compare their performance on the common L2.\nstructure in the recurrent parameters of LSTMs—\ndespite removing vocabulary-level confounders.\nBy assessing if representations are useful across\nlanguages, we examine the generalizable represen-\ntations of grammar that LSTMs encode. We call\nthis new method the Test for Inductive Bias via\nLanguage Model Transfer (TILT).\nFirstly, we examine the transfer of abstract struc-\ntural features from languages that are very different\non the surface from human language. We ﬁnd that\npretraining an LSTM on music data2 or Java code\ngreatly improves transfer to human language over\npretraining on structureless random baseline data.\nTo test if the gain in performance is due to the\nLSTM utilizing the recursive nature of music and\ncode, we train models on an artiﬁcial language with\nrecursion (hierarchically nested symbols) and ob-\nserve that they also perform well when evaluated\non human language. However, we also surprisingly\nﬁnd that recursion is a sufﬁcient, but not necessary\ncondition for generalizable, language-like grammar\ninduction. We observe similar gains when pretrain-\ning on a language of matching pairs that do not\nnest hierarchically, showcasing the importance of\nnon-hierarchical head-dependent-type relations in\nLSTM language processing.\nLastly, in transfer experiments between different\nhuman languages, we ﬁnd that transfer is better\nbetween languages that are syntactically typolog-\nically similar, even with no vocabulary overlap.\nThis suggests that models have the ability to form\n2We use the MAESTRO music dataset, which utilizes an\nexact symbolic representation of music (like a music score)\nthat is sequentialized for sequence modelling\nrepresentations of typologically sensible properties\nrather than relying on ad-hoc or non-natural repre-\nsentations. For this result we draw on recent inter-\nlingual work such as Artetxe et al. (2020), Ponti\net al. (2019), and Conneau et al. (2018b), extending\nit to use typological distance to turn these observa-\ntions into quantitative probes.\nThe TILT method allows us to ask a complemen-\ntary set of questions to those answered by current\nanalysis methods. TILTs demonstrate the abstract\nstructural notions that LSTMs can learn, rather than\nprobing for the manifestation of a particular known\nstructure, as in most current methods. By exam-\nining the pretraining structures that give LSTMs\na better ability to model language, we also con-\ntribute to the more general cognitive question of\nwhat structural inductive biases a learner needs to\nbe able to easily acquire human language.\n2 Architecture and Training\nOur methodology consists of training LSTM lan-\nguage models on k different ﬁrst languages (L1s)\nwhich include natural languages, artiﬁcial lan-\nguages, and non-linguistic symbol systems, and\ntesting the performance of these models on a com-\nmon second (L2) language. In our case, we used\nSpanish as the common L2. Before testing on the\nL2 test set, we ﬁne-tune the linear embedding layer\nof the models on the L2 training set, while keeping\nthe LSTM weights frozen. This aligns the vocabu-\nlary of each model to the new language, but does\nnot let it learn any structural information about\nthe L2 language. Though word embeddings do\ncontain some grammatical information like part of\n6831\nspeech, they do not contain information about how\nto connect tokens to each other – that information\nis only captured in the LSTM. Figure 2 illustrates\nour training process. 3\nWe vary the L1 languages and maintain a com-\nmon L2 (instead of the other way around) in order\nto have a common basis for comparison: all of the\nmodels are tested on the same L2 test set, and there-\nfore we can compare the perplexity scores. We run\nn = 5 trials of every experiment with different\nrandom seeds. Any high-resource human language\nwould have provided a good common L2, and Span-\nish works well for our human languages experi-\nments due to the fact that many higher-resource\nlanguages fall on a smooth gradation of typological\ndistance from it (see Table 1).\nWe use the AWD-LM model (Merity et al., 2018)\nwith the default parameters of 3 LSTM layers, 300-\ndimensional word embeddings, a hidden size of\n1,150 per layer, dropout of 0.65 for the word em-\nbedding matrices and dropout of 0.3 for the LSTM\nparameters. We used SGD and trained to conver-\ngence, starting the learning rate at the default of 30\nand reducing it at loss plateau 5 times.\nMuch of the work on multilingual transfer learn-\ning has speculated that successes in the ﬁeld may\nbe due to vocabulary overlap (see for example Wu\nand Dredze (2019)). Since our work focuses mostly\non syntax, we wanted to remove this possibility. As\nsuch, we shufﬂe each word-to-index mapping to\nuse disjoint vocabularies for all languages: the En-\nglish word “Chile” and the Spanish word “Chile”\nwould map to different integers. This addresses the\nconfound of vocabulary overlap, as all language\npairs have zero words in common from the point\nof view of the model.\nSince the vocabularies are totally separated be-\ntween languages, we align the vocabularies for all\nL1-L2 pairs by ﬁnetuning the word embeddings\nof all the pretrained models on the Spanish (L2)\ntraining data, keeping the LSTM weights frozen.\nBy doing this, we remove the confound that would\narise should one language’s vocabulary randomly\nhappen to be more aligned with Spanish than an-\nother’s. These controls ensure that lexical features,\nwhether they be shared vocabulary or alignment of\nrandomly aligned indices, do not interfere with the\nexperimental results which are meant to compare\nhigher-level syntactic awareness.\n3All pretraining jobs took less than 2 days to run on one\nGPU, all ﬁnetuning jobs took less than 1 day to run on one\nGPU.\n3 Experiment 1: Random Baselines\nWe run our method on a random baseline L1: a cor-\npus where words are sampled uniformly at random.\nThis gives us a baseline for how much information\nwe gain ﬁnetuning the word embeddings to the L2,\nwhen there has not been any structurally biasing\ninput to the LSTM from the L1.\nWe also examine the importance of vocabulary\ndistribution by training on a random corpus that\nis sampled from a Zipﬁan distribution. Human\nlanguages are surprisingly consistent in sharing a\nroughly Zipﬁan vocabulary distribution, and we\ntest how pretraining on this distribution affects the\nability to model human language. 4\n3.1 Data\nOur random corpora are sampled from the Span-\nish vocabulary, since Spanish is the common L2\nlanguage across all experiments. Words are sam-\npled uniformly for the Uniform Random corpus,\nand drawn from the empirical Spanish unigram dis-\ntribution (as calculated from our Spanish training\ncorpus) for the Zipﬁan Random corpus. Illustrative\nexamples from all of our corpora can be found in\nFigure 3. The random corpora are controlled to\n100 million tokens in length.\n3.2 Results\nWhen tested on Spanish, the average perplexity is\n513.66 for models trained on the Random Uniform\ncorpus and 493.15 for those trained on the Random\nZipﬁan corpus, as shown in Figure 4. These per-\nplexity values are both smaller than the vocabulary\nsize, which indicates that the word embedding ﬁne-\ntuning captures information about the test language\neven when the LSTM has not been trained on any\nuseful data.\nThe models trained on the Zipﬁan Random cor-\npus are signiﬁcantly better than those trained on the\nUniform corpus (p <<0.05, Welch’st-test over\nn = 5 trials). However, even though training on\na Zipﬁan corpus provides gains when compared\nto training on uniformly random data, in absolute\nterms performance is very low. This indicates that,\nwithout higher-level language-like features, there is\nvery little that an LSTM can extract from properties\nof the vocabulary distribution alone.\n4See Piantadosi (2014) for a review of cognitive, commu-\nnication and memory-based theories seeking to explain the\nubiquity of power law distributions in language.\n6832\nRandom\nThe random corpora are sampled randomly from the Spanish\nvocabulary. There is no underlying structure of any kind that\nlinks words with each other. All words are equally likely to\nbe sampled in the Uniform corpus, while common words are\nmore likely in the Zipﬁan corpus.\nUniform: marroqu´ın jemer pertenecer\nosasuna formaron citoesqueleto\nrelativismo\nZipf: en con conocidas y en los victoriano\ncomo trabajar⟨unk⟩ monte * en juegos d´ıas\nen el\nMusic\nThe music data is encoded from classical piano performances\naccording to the MAESTRO standard. Music is structured on\nmany levels. The red arrow in the example illustrates how, on\na small timescale, each note is linked to its corresponding note\nwhen a motif is repeated but modulated down a whole-step.\nCode\nif (coordFactor == 1.0f)\nreturn sumExpl\nelse {\nresult = sum * coordFactor\n}\nThe code corpus is composed of Java code. The above snippet\ndemonstrates some kinds of structure that are present in code:\nbrackets are linked to their pairs, elsestatements are linked\nto an if statement, and coreference of variable names is\nunambiguous.\nParentheses\nOur artiﬁcial corpora consist of pairs of matching integers. In\nthe Nesting Parentheses corpus, integer pairs nest\nhierarchically and so the arcs do not cross. In the Flat\nParentheses corpus, each integer pair is placed independently\nof all the others, and so the arcs can cross multiple times.\n(There is a one-to-one mapping between Spanish words and\nintegers and so these integers are sampled from the same\nSpanish vocabulary distribution as the Random Zipﬁan\ncorpus. We visualize these corpora here with integers and the\nRandom corpora with words for simplicity).\nNesting:\n0 29 29 0 0 5 5 0 1016 1016 9 8 8 28 28 9\nFlat:\n21 13 21 6294 13 6294 5 5471 5 32 32 5471\nFigure 3: Examples illustrating the content of our non-linguistic corpora for Experiments 1-3. All examples are\ntaken from the corpora.\nThe Zipﬁan Random baseline is controlled for\nvocabulary distribution: if an experiment yields\nbetter results than the Zipﬁan Random baseline, we\ncannot attribute its success only to lexical-level sim-\nilarity to the L2. Therefore, models that are more\nsuccessful than the Zipﬁan baseline at transfer to\nhuman language would have useful, generalizable\nsyntactic information about the structures that link\ntokens.\n4 Experiment 2: Non-linguistic structure\nIn this experiment, we test the performance of\nLSTMs on Spanish when they have been trained\non music and on code data. While music data es-\npecially is very different from human language on\nthe surface level, we know that music and code\nboth contain syntactic elements that are similar to\nhuman language.5 By comparing performance to\nour random baselines, we ask: can LSTMs encode\n5See for example Lerdahl and Jackendoff (1996) for gram-\nmatical structure in music.\nthe abstract structural features that these corpora\nshare with natural language in a generalizable way\nthat’s usable to model human language?\n4.1 Data\nFor our music data we use the MAESTRO dataset\nof Hawthorne et al. (2018). The MAESTRO dataset\nembeds MIDI ﬁles of many parallel notes into a\nlinear format suitable for sequence modelling, with-\nout losing musical information. The ﬁnal corpus\nhas a vocabulary of 310 tokens, and encodes over\n172 hours of classical piano performances. 6\nFor programming code data, we used the Habeas\ncorpus released by Movshovitz-Attias and Cohen\n(2013), of tokenized and labelled Java code. We\ntook out every token that was labelled as a com-\nment so as to not contaminate the code corpus with\nnatural language. 7\n6The MAESTRO dataset is available at https://\nmagenta.tensorflow.org/datasets/maestro\n7The Habeas corpus is available at\nhttps://github.com/habeascorpus/\n6833\nRandom Uniform\nRandom Zipf\nMusic Code\nNesting Parens\nFlat Parens\nPretraining Language (L1)\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\nZero-shot ppl on Spanish (L2) \n (Lower is better)\nFigure 4: Results of Experiments 1 through 3, train-\ning on non-linguistic corpora. Error bars on all bars in-\ndicate a 95% t-test conﬁdence interval over 5 restarts\nwith different random seeds. All structured data is\nmuch better to train on than random data, including\nmusic which has a totally divergent vocabulary surface\nform from the rest. The two parentheses corpora result\nin equivalent perplexities, even though one has a hierar-\nchical underlying structure and the other does not.\nThe music corpus is 23 million tokens in length\nand the code corpus is 9.5 million. We cannot ef-\nfectively control the lengths of these corpora to be\nthe same as all of the others, since there is no con-\ntrolled notion of what one token means in terms of\ninformation. However, we only compare these re-\nsults to the random baseline, which we have trained\non 100 million tokens – if the LSTMs trained on\nthese corpora are under-speciﬁed compared to the\nbaseline, this would only strengthen our results.\n4.2 Results\nOur results show that language models pretrained\non music are far better at modelling Spanish than\nthose pretrained on random data. As shown in\nﬁgure 4, LSTMs trained on music data have an av-\nerage performance of 256.15 ppl on Spanish, com-\npared with 493.15 when training on the Zipﬁan\nrandom corpus. This discrepancy suggests that the\nmodel, when training on music, creates represen-\ntations of the relationships between tokens which\nare generalizable and can apply to Spanish.\nThe music corpus is markedly different from the\nSpanish corpus by most measures. Most saliently,\nMAESTRO uses a vocabulary of just 310 tokens\nto encode various aspects of music like volume\nand note co-occurrence. 8 This is in contrast to\nhabeascorpus-data-withComments\n8For consistency, the model still has a word embedding\nthe Zipﬁan Random corpus, which has the same\nsurface-level vocabulary and distribution as Span-\nish, yet models trained on it perform on average\n237 ppl worse compared to those trained on the\nmusic corpus. Since the surface forms between\nmusic and language are so different, the difference\nin performance cannot be based on surface-level\nheuristics, and our results suggest the presence\nof generalizable, structurally-informed representa-\ntions in LSTM language models.\nWe also show that models trained on Java code\ncan transfer this knowledge to a human L2 bet-\nter than the random baseline. Syntactic properties\nof code such as recursion are similar to natural\nlanguage, though code is constructed to be unam-\nbiguously parsed and lacks a lot of the subtlety\nand ambiguity that characterizes natural language.\nModels trained on code have an average perplexity\nof 139.10 on the Spanish test set. The large discrep-\nancy between this performance and the baseline\nindicates that LSTMs trained on code capture the\nsyntactic commonalities between code and natural\nlanguage in a manner that is usable for modelling\nnatural language.\nOur results on non-linguistic data suggest that\nLSTMs trained on structured data extract repre-\nsentations which can be used to model human lan-\nguages. The non-linguistic nature of these data\nsuggests that it is something structural about the\nmusic and Java code that is helping in the zero-shot\ntask. However, there is a multitude of structural\ninterpretations of music, and it is not clear what\nkinds of structure the LSTM encodes from music.\nIn the next experiment, we create simple artiﬁcial\ncorpora with known underlying structures in order\nto test how the LMs can represent and utilize these\nstructures.\n5 Experiment 3: Recursive Structure\nIn this experiment, we isolate and assess possible\nstructural features of music and code that may ex-\nplain the results of Experiment 2. The most widely-\nknown structural hypothesis is the claim of Hauser\net al. (2002) that the narrow language faculty in\nhumans (the inductive bias in the mind/brain that\nallows humans to acquire and develop language)\ncan be reduced to just recursion. Given the promi-\nnence of such theories, it is natural to ask: is it the\nmatrix of 50,000 rows, but during training only ever sees\nwords 1-310, meaning that much of the word embedding space\nhas never been seen by the LSTM part of the model.\n6834\nunderlying recursive nature of music and code data\nthat causes the gains that we observe in Experiment\n2?\nTo test this possibility, we create a simple re-\ncursive corpus: a Nesting Parentheses corpus of\nhierarchically nesting matching symbols, and run\nthe same experimental setup as we did for Experi-\nments 1 and 2 9. We ﬁnd that plain recursion, even\nwhen the corpus has no other structural subtleties,\nis indeed a sufﬁcient condition for inducing the\nkinds of structural transfer we observed in Experi-\nment 2.\nRecursion is a sufﬁcient quality, but is it the only\nexplanation for our results? We also create a con-\ntrol corpus: a Flat Parentheses corpus, which has\nsimilar pairs of matching parentheses, but which\ndo not nest hierarchically and projectively (the dif-\nference between the two corpora is visually illus-\ntrated in Figure 3). We surprisingly ﬁnd that this\nnon-recursive corpus induces the same amount of\nstructural transfer as the recursive nesting parenthe-\nses, which emphasizes the importance of pairing,\nhead-dependency type structure in the linguistic\nstructural embeddings of LSTMs.\n5.1 Data\nThe vocabulary for these corpora are the integers\n0-50,000, where each number is a parenthesis to-\nken, and that token “closes” when the same integer\nappears a second time. We draw the opening tokens\nfrom the empirical Spanish unigram distribution\n(mapping each Spanish word to an integer), mean-\ning that these corpora have a similar vocabulary\ndistribution, albeit a much simpler non-linguistic\nstructure, to the L2. Both of the corpora are 100\nmillion tokens long, like the random and the natural\nlanguage corpora.\nWe create the Nesting Parentheses corpus by fol-\nlowing a simple stack-based grammar. At timestep\nt, we ﬂip a coin to decide whether to open a new\nparenthesis (with probability 0.4) or close the top\nparenthesis on the stack (with probability 0.6).10 If\nwe are opening a new parenthesis, we sample an in-\nteger xopen from the Spanish unigram distribution,\nwrite the integer xopen at the corpus position t, and\npush xopen onto the stack of open parentheses. If\n9Though these corpora do not strictly use parentheses to-\nkens, we refer to both of these as parentheses corpora, drawing\nour metaphor from the wide variety of studies such as Karpa-\nthy et al. (2016) examining nested parentheses.\n10P(open) has to be strictly less than 0.5, or else the tree\ndepth is expected to grow inﬁnitely.\nwe are closing a parenthesis, we pop the top integer\nfrom the stack, xclose, and write xclose at corpus\nposition t.\nThe Flat Parentheses corpus is made up of pairs\nof parentheses that do not nest. At timestep t, we\nsample an integer x from the empirical Spanish\nunigram distribution, and a distance d from the\nempirical distribution of dependency lengths (cal-\nculated from the Spanish Universal Dependencies\ntreebank (McDonald et al., 2013)). Then, we write\nx at position t and at position t + d. This creates\npairs of matching parentheses which are not in-\nﬂuenced by any other token in determining when\nthey close. Note that this corpus is very similar to\nthe Random Zipf corpus, except that each sampled\ntoken is placed twice instead of once.\n5.2 Results\nLSTMs trained on both parentheses corpora are\nable to model human language far better than mod-\nels trained on the random corpora, indicating that\nthe isolated forms of grammar-like structure in\nthese corpora are useful for modelling human lan-\nguage. Surprisingly, performance is the same for a\nmodel pretrained on the Nesting Parentheses and\nthe Flat Parentheses corpus. This suggests that\nit is not necessarily hierarchical encodings which\nLSTMs use to model human language, and that\nother forms of structure such as ﬂat head-head de-\npendencies may be just as important (de Marneffe\nand Nivre, 2019).\nThe Nesting Parentheses corpus exhibits hierar-\nchical structure while not having any of the irregu-\nlarities and subtleties of human language or music.\nDespite the simplicity of the grammar, our results\nindicate that the presence of this hierarchical struc-\nture is very helpful for an LSTM attempting to\nmodel Spanish. Our models trained on the Nesting\nParentheses corpus have an average perplexity of\n170.98 when tested on the Spanish corpus. This\nis 322 perplexity points better than the baseline\nmodels trained on the Zipf Random corpus, which\nhas the same vocabulary distribution (Figure 4).\nModels trained on the Flat Parentheses cor-\npus are equally effective when tested on Spanish,\nachieving an average perplexity of 170.03. These\nresults are surprising, especially given that the Flat\nParentheses corpus is so similar to the Random\nZipf corpus – the only difference being that inte-\ngers are placed in pairs not one by one – and yet\nperforms better by an average of 323 perplexity\n6835\nLanguage WALS-syntax distance\nfrom Spanish (out of a\nmax of 49 features)\nSpanish (es) 0\nItalian (it) 0\nPortuguese (pt) 3\nEnglish (en) 4\nRomanian (ro) 5\nRussian (ru) 9\nGerman (de) 10\nFinnish (ﬁ) 13\nBasque (eu) 15\nKorean (ko) 18\nTurkish (tr) 23\nJapanese (ja) 23\nTable 1: W ALS-syntax distance between Spanish and\nL1s\npoints. This suggests that representing relation-\nships between pairs of tokens is a key element that\nmakes syntactic representations of language suc-\ncessful in LSTMs.\nThe Flat Parentheses corpus has structure in that\neach token is placed in relation to one other token,\nbut just one other token. To model this successfully\na model would have to have some ability to look\nback at previous tokens and determine which ones\nwould likely have their match appear next. Our\nresults suggest that this kind of ability is just as\nuseful as potentially being able to model a simple\nstack-based grammar.\n6 Experiment 4: Human Languages\nTo further analyze what kinds of generalizable\nstructure LSTMs can infer, we run experiments\nin transferring zero-shot between human languages.\nWe ask: can LSTMs infer and use ﬁne-grained\nsyntactic similarities between typologically simi-\nlar languages? Previous work (Zoph et al., 2016;\nArtetxe et al., 2020) indicates that transfer is more\nsuccessful between related languages. We control\nfor vocabulary overlap, and use typological syntac-\ntic difference as a quantitative probe to ask: are\nﬁne-grained syntactic similarities encoded in gen-\neralizable, transferrable ways? To answer this ques-\ntion, we investigate the extent to which ﬁne-grained\ndifferences in syntactic structure cause different\nzero-shot transfer results.\n6.1 Data\nWe created our language corpora from Wikipedia,\nwhich offers both wide language variation as well\nas a generally consistent tone and subject domain.\nWe used the gensim wikicorpus library to strip\n0 5 10 15 20\nL1 WALS-syntax distance from Spanish\n50\n60\n70\n80\n90\n100\n110\n120\n130\nPpl when tested on Spanish (L2) \n (Lower is better)\nes\nit\npt\nenro\nrude\nfi eu\nko tr\nja\nIndo-European Languages Non Indo-European\nFigure 5: Results of Experiment 4. Transfer is better\nbetween typologically similar languages, even when\nvocabularies are disjoint. Perplexity on Spanish test\ndata plotted against the W ALS-syntax distance of each\nmodel’s L1 to Spanish. The relationship is almost lin-\near for Indo-European languages, and then reaches a\nceiling. Error bars show 95% CIs for n = 5 trials\nwith different random seeds. These results demonstrate\nhow LSTMs can transfer knowledge more easily to lan-\nguages that share structural features with the L1, and\nthat this correlation is robust to multiple trials. The\norange line represents the oracle perplexity of train-\ning all parameters to convergence on the L2 train data.\nRomance languages are in red, other Indo-European\nlanguages are in purple, and non-Indo-European lan-\nguages are blue.\nWikipedia formatting, and the stanfordnlp Python\nlibrary (Qi et al., 2018) to tokenize the corpus. We\nrun experiments on data from 12 human languages,\nall of which have Wikipedias of over 100,000 arti-\ncles: Spanish, Portuguese, Italian, Romanian, En-\nglish, Russian, German, Finnish, Basque, Korean,\nTurkish and Japanese. All of the training corpora\nare 100 million tokens in length. 11\nFor our typological data, we use the World At-\nlas of Linguistic Structure, using the features that\nrelate to syntax (WALS-syntax features). Exam-\nples of syntactic features in WALS include ques-\ntions such as does a language have Subject-Verb-\nObject order, or does a degree word (like “very”)\ncome before or after the adjective. We accessed\nthe WALS data using the lang2vec package (Lit-\ntell et al., 2017). The quantity we are interested in\nextracting from the WALS data is thetypological\ndistance between the L2 (Spanish) and all of the\n11The code for recreating our corpora from Wikipedia\ndumps is available at https://github.com/toizzy/\nwiki-corpus-creator\n6836\nL1 languages mentioned above. Not every feature\nis reported for every language, so we calculate the\nWALS distance by taking into account only the\n49 (syntactic) features that are reported for all our\nchosen languages and count the number of entries\nthat are different (see Table 1). Since they are only\nbased on 49 features, these distances do not provide\na perfectly accurate distance metric. Though we\ncannot use it for ﬁne-grained analysis, correlation\nwith this distance metric would imply correlation\nwith syntactic distance.\n6.2 Results\nOur experiments present a strong correlation be-\ntween the ability to transfer from an L1 language\nto Spanish and the W ALS-syntax distance between\nthose two languages, as shown in Figure 5(a). In\nthe case of Indo-European languages the relation-\nship is largely linear with a Pearson R2 coefﬁcient\nof 0.83. For languages not in the Indo-European\nlanguage family, transfer performance appears to\nreach a noisy ceiling, and Pearson’s R2 = 0.78\nwhen taking into account all languages.12\nOur previous experiments show that LSTMs can\nencode and generalize structural features from data\nthat is structured, both in recursive and in non-\nhierarchical fashion. This experiment provides a\nmore ﬁne-grained analysis using using natural lan-\nguage to show that the syntax induced by LSTMs\nis generalizable to other languages in a typologi-\ncally sensible fashion, even when we do not let the\nmodel take advantage of vocabulary overlap. How-\never, after a certain threshold, the model is unable\nto take advantage of ﬁne-grained similarities and\nperformance on distant languages reaches a ceiling.\nIt should be noted that all of the models trained\non natural language, even the most distant, per-\nform far better than non-linguistic data, indicating\nthat LSTMs are able to extract universal syntactic\ninformation from all natural language L1s that is\napplicable to Spanish.\n7 Discussion\nIn this work we propose the Test for Inductive bias\nvia Language model Transfer (TILT), a novel an-\nalytic method for neural language models which\ntests the ability of a model to generalize and use\n12We veriﬁed that our results also stand when calculating\ncorrelation coefﬁcients using log perplexity, which yielded\nsimilar values: R2 of 0.79 and 0.73 for Indo-European and all\nlanguages respectively.\nstructural knowledge. We pretrain LSTMs on struc-\ntured data, and then use the frozen LSTM weights\nto model human language. In doing so, we treat the\nfrozen LSTM weights as the only structural faculty\navailable to a human language model, and assess if\nthe induced structure is general enough to be used\nto model human language.\nOur experiments are cross-lingual and cross-\nmodal in nature, not searching for representations\nof high-level features in one language, but for rep-\nresentations that encode general ideas of structure.\nWhile the majority of past work analyzing the struc-\ntural abilities of neural models looks at a model’s\ntreatment of structural features that are realized\nin speciﬁc input sentences, our method compares\nthe encoding and transfer of general grammatical\nfeatures of different languages. By using TILTs,\nwe do not have to identify a structural feature of\ninterest and investigate if it is being encoded, but\ninstead asses if generalizable abstract structures\nare encoded in one language by examining if they\ncan be used to model human language. Our work\nthus avoids known issues that have been pointed out\nwith analytic methods like probing (V oita and Titov,\n2020; Pimentel et al., 2020; Hewitt and Liang,\n2019).\nWe run experiments on natural languages, arti-\nﬁcial languages, and non-linguistic corpora. Our\nnon-linguistic and artiﬁcial language experiments\nsuggest three facets of the structural encoding abil-\nity of LSTM LMs. First, that vocabulary distribu-\ntion has a very minor effect for modelling human\nlanguage compared to structural similarity. Second,\nthat models can encode useful language modelling\ninformation from the latent structure inherent in\nnon-linguistic structured data, even if the surface\nforms are vastly differing. Last, that encodings\nderived from hierarchically structured tokens are\nequally useful for modelling human language as\nthose derived from texts made up of pairs of to-\nkens that are linked but non-hierarchical. Run-\nning experiments on a range of human languages,\nwe conclude that the internal linguistic representa-\ntion of LSTM LMs allows them to take advantage\nof structural similarities between languages even\nwhen unaided by lexical overlap.\nOur results on the parentheses corpora do not\nnecessarily provide proof that the LSTMs trained\non the Nesting Parentheses corpus aren’t encoding\nand utilizing hierarchical structure. In fact, previ-\nous research shows that LSTMs are able to suc-\n6837\ncessfully model stack-based hierarchical languages\n(Suzgun et al., 2019b; Yu et al., 2019; Suzgun et al.,\n2019a). What our results do indicate is that, in\norder for LSTMs to model human language, being\nable to model hierarchical structure is similar in\nutility to having access to a non-hierarchical ability\nto “look back” at one relevant dependency. These\nresults shine light on the importance of consider-\ning other types of structural awareness that may be\nused by neural natural language models, even if\nthose same models also demonstrate the ability to\nmodel pure hierarchical structure.\nOur method could be used to test many other\nhypotheses regarding neural language models, by\nchoosing a discerning set of pretraining languages.\nA ﬁrst step in future work would be to test if the\nresults of this paper hold on Transformer architec-\ntures, or if instead Transformers result in differ-\nent patterns of structural encoding transfer. Future\nwork expanding on our results could focus on ab-\nlating speciﬁc structural features by creating hypo-\nthetical languages that differ in single grammatical\nfeatures from the L2, in the style of Galactic Depen-\ndencies (Wang and Eisner, 2016), and testing the\neffect of structured data that’s completely unrelated\nto language, such as images.\nOur results also contribute to the long-running\nnature-nurture debate in language acquisition:\nwhether the success of neural models implies that\nunbiased learners can learn natural languages with\nenough data, or whether human abilities to acquire\nlanguage given sparse stimulus implies a strong\ninnate human learning bias (Linzen and Baroni,\n2020). The results of our parentheses experiments\nsuggest that simple structural head-dependent bias,\nwhich need not be hierarchical, goes a long way\ntoward making language acquisition possible for\nneural networks, highlighting the possibility of a\nless central role for recursion in language learning\nfor both humans and machines.\nAcknowledgements\nWe thank Urvashi Khandelwal, Kawin Ethayarajh,\nKyle Mahowald, Chris Donahue, Yiwei Luo, Alex\nTamkin and Kevin Clark for helpful discussions and\ncomments on drafts, and our anonymous reviewers\nfor their feedback. This work was supported by an\nNSF Graduate Research Fellowship for IP and a\nSAIL-Toyota Research Award. Toyota Research In-\nstitute (“TRI”) provided funds to assist the authors\nwith their research but this article solely reﬂects\nthe opinions and conclusions of its authors and not\nTRI or any other Toyota entity.\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018a. What\nyou can cram into a single vector: Probing sen-\ntence embeddings for linguistic properties. Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers).\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018b. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan\nBelinkov, Anthony Bau, and James Glass. 2019.\nWhat is one grain of sand in the desert? analyzing\nindividual neurons in deep NLP models. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 33, pages 6309–6317.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long Papers).\nMarc D. Hauser, Noam Chomsky, and W. Tecumseh\nFitch. 2002. The faculty of language: What is\nit, who has it, and how did it evolve? Science,\n298(5598):1569–1579.\nCurtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian\nSimon, Cheng-Zhi Anna Huang, Sander Dieleman,\nErich Elsen, Jesse Engel, and Douglas Eck. 2018.\nEnabling factorized piano music modeling and gen-\neration with the maestro dataset.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\n6838\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2016.\nVisualizing and understanding recurrent networks.\nFred Lerdahl and Ray S Jackendoff. 1996. A genera-\ntive theory of tonal music. MIT press.\nTal Linzen and Marco Baroni. 2020. Syntactic struc-\nture from deep learning. Annual Review of Linguis-\ntics, 7.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nPatrick Littell, David R Mortensen, Ke Lin, Katherine\nKairis, Carlisle Turner, and Lori Levin. 2017. Uriel\nand lang2vec: Representing languages as typologi-\ncal, geographical, and phylogenetic vectors. In Pro-\nceedings of the 15th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Volume 2, Short Papers , volume 2, pages\n8–14.\nMarie-Catherine de Marneffe and Joakim Nivre. 2019.\nDependency grammar. Annual Review of Linguis-\ntics, 5:197–218.\nR. Thomas McCoy, Robert Frank, and Tal Linzen.\n2020. Does syntax need to grow on trees? sources of\nhierarchical inductive bias in sequence-to-sequence\nnetworks. Transactions of the Association for Com-\nputational Linguistics, 8:125–140.\nRyan McDonald, Joakim Nivre, Yvonne Quirmbach-\nBrundage, Yoav Goldberg, Dipanjan Das, Kuzman\nGanchev, Keith Hall, Slav Petrov, Hao Zhang, Os-\ncar T ¨ackstr¨om, et al. 2013. Universal dependency\nannotation for multilingual parsing. In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 92–97.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models. In International Conference on\nLearning Representations.\nDana Movshovitz-Attias and William Cohen. 2013.\nNatural language models for predicting program-\nming comments. In Proceedings of the 51st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 35–40.\nSteven T Piantadosi. 2014. Zipf’s word frequency\nlaw in natural language: A critical review and fu-\nture directions. Psychonomic bulletin & review ,\n21(5):1112–1130.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-theoretic probing for linguistic\nstructure.\nEdoardo Maria Ponti, Ivan Vuli ´c, Ryan Cotterell, Roi\nReichart, and Anna Korhonen. 2019. Towards zero-\nshot language modeling. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2900–2910, Hong Kong,\nChina. Association for Computational Linguistics.\nPeng Qi, Timothy Dozat, Yuhao Zhang, and Christo-\npher D. Manning. 2018. Universal dependency pars-\ning from scratch. In Proceedings of the CoNLL 2018\nShared Task: Multilingual Parsing from Raw Text\nto Universal Dependencies , pages 160–170, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nMirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov,\nand Stuart M. Shieber. 2019a. LSTM networks can\nperform dynamic counting. In Proceedings of the\nWorkshop on Deep Learning and Formal Languages:\nBuilding Bridges, volume abs/1906.03648, Florence.\nAssociation for Computational Linguistics.\nMirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov,\nand Stuart M. Shieber. 2019b. Memory-augmented\nrecurrent neural networks can learn generalized\nDyck languages.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2019. oLMpics – on what lan-\nguage model pre-training captures.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\narXiv preprint arXiv:2003.12298.\nDingquan Wang and Jason Eisner. 2016. The Galactic\nDependencies treebanks: Getting more data by syn-\nthesizing new languages. Transactions of the Asso-\nciation for Computational Linguistics, 4:491–505.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nbert. Proceedings of the 2019 Conference on Empir-\nical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP).\nXiang Yu, Ngoc Thang Vu, and Jonas Kuhn. 2019.\nLearning the dyck language with attention-based\nseq2seq models. In ACL 2019.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proceedings of the\n6839\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1568–1575, Austin,\nTexas. Association for Computational Linguistics.\nAppendix: Numerical results of\nexperiments\nFor every experiment we ran ﬁve trials with differ-\nent random seeds. We list the means and standard\ndeviations for each L1 below:\nL1 Language Mean TILT Ppl Std. Dev\nRandom Uniform 513.66 1.01\nRandom Zipf 493.15 2.97\nMusic 256.15 2.65\nCode 139.11 1.24\nNesting Parens 170.98 1.02\nFlat Parens 170.30 1.48\nBasque 108.57 4.93\nEnglish 85.30 3.40\nFinnish 110.92 3.84\nGerman 102.42 0.51\nItalian 67.21 2.10\nJapanese 108.48 0.81\nKorean 118.23 1.23\nPortoguese 61.25 0.21\nRomanian 85.14 6.26\nRussian 100.56 4.74\nSpanish 52.33 0.21\nTurkish 118.45 0.85"
}