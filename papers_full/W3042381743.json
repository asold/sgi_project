{
  "title": "Mono vs Multilingual Transformer-based Models: a Comparison across Several Language Tasks",
  "url": "https://openalex.org/W3042381743",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Feijo, Diego de Vargas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4301144736",
      "name": "Moreira, Viviane Pereira",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2948384082",
    "https://openalex.org/W2475024326",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2888042761",
    "https://openalex.org/W2161249804",
    "https://openalex.org/W3008198684",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2587282796",
    "https://openalex.org/W2795227599",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "BERT (Bidirectional Encoder Representations from Transformers) and ALBERT (A Lite BERT) are methods for pre-training language models which can later be fine-tuned for a variety of Natural Language Understanding tasks. These methods have been applied to a number of such tasks (mostly in English), achieving results that outperform the state-of-the-art. In this paper, our contribution is twofold. First, we make available our trained BERT and Albert model for Portuguese. Second, we compare our monolingual and the standard multilingual models using experiments in semantic textual similarity, recognizing textual entailment, textual category classification, sentiment analysis, offensive comment detection, and fake news detection, to assess the effectiveness of the generated language representations. The results suggest that both monolingual and multilingual models are able to achieve state-of-the-art and the advantage of training a single language model, if any, is small.",
  "full_text": "Mono vs Multilingual Transformer-based Models:\na Comparison across Several Language Tasks\nDiego de Vargas Feijo, Viviane Pereira Moreira\nInstitute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brazil\n{dvfeijo, viviane}@inf.ufrgs.br\nAbstract\nBERT (Bidirectional Encoder Representations\nfrom Transformers) and ALBERT (A Lite\nBERT) are methods for pre-training language\nmodels which can later be ﬁne-tuned for a vari-\nety of Natural Language Understanding tasks.\nThese methods have been applied to a number\nof such tasks (mostly in English), achieving re-\nsults that outperform the state-of-the-art. In\nthis paper, our contribution is twofold. First,\nwe make available our trained BERT and Al-\nbert model for Portuguese. Second, we com-\npare our monolingual and the standard mul-\ntilingual models using experiments in seman-\ntic textual similarity, recognizing textual en-\ntailment, textual category classiﬁcation, sen-\ntiment analysis, offensive comment detection,\nand fake news detection, to assess the effective-\nness of the generated language representations.\nThe results suggest that both monolingual and\nmultilingual models are able to achieve state-\nof-the-art and the advantage of training a sin-\ngle language model, if any, is small.\n1 Introduction\nText embeddings are powerful tools as they are able\nto encompass the context, syntactic, and semantics\nof the texts they represent (Mikolov et al., 2013;\nBojanowski et al., 2017; Pennington et al., 2014;\nPeters et al., 2018). Their use has been popularized\nin the last few years, to a great extent, motivated\nby the free availability of pre-trained models for\na wide number of natural languages. These pre-\ntrained language representations can be used as\nthe features that compose the ﬁrst layer of a task-\nspeciﬁc model. Alternatively, one could ﬁne-tune a\ngeneric model to be applied on a speciﬁc task.\nMore recently, Bidirectional Encoder Repre-\nsentations from Transformers (BERT) (Devlin\net al., 2018) and subsequently A Lite BERT (AL-\nBERT) (Lan et al., 2019) were proposed as task-\nindependent architectures. Both BERT and AL-\nBERT report achieving state-of-the-art results on\nseveral natural language processing (NLP) tasks,\nincluding next sentence prediction, recognizing tex-\ntual entailment, sentiment analysis, question an-\nswering, and named-entity recognition. The au-\nthors of both models released trained monolingual\nmodels in English and Chinese. However, for the\nother languages, only BERT released a multilin-\ngual model that was trained on Wikipedias for 104\nlanguages (Pires et al., 2019). Portuguese is among\nthe languages that compose the multilingual model.\nHowever, it is not clear how the multilingual model\nperforms in single language tasks compared to a\nmodel trained exclusively for that language.\nWe believe that a comparison between the sin-\ngle and multilingual models applied to a variety of\nsingle language tasks would help researchers de-\ncide whether it is worth to invest training a model\non a target language or a multilingual model can\nbe enough. Also, we make our trained models in\nPortuguese available for researchers so they could\nuse these models without needing to master the\ncomplexity that the training involve.\nIn this paper, we present BertPT and AlbertPT,\npre-trained models in Portuguese which are freely\navailable at https://github.com/diego-feijo/\nbertpt/. Both BertPT and AlbertPT were trained\nusing corpora coming from different domains and\nstyles (Wikipedia, news articles, movie subtitles,\nresearch abstracts, and European Parliament ses-\nsions) to assure a wide coverage of the language.\nWe evaluated our pre-trained models on seven\nnatural language understanding tasks. Our perfor-\nmance was compared to baselines published in the\nliterature and to the multilingual BERT model. The\nresults show that BertPT and AlbertPT are able to\noutperform the baselines in most cases. In relation\nto the multilingual BERT model, the results of the\nare within a 5% proportional differences in most\ntests.\narXiv:2007.09757v1  [cs.CL]  19 Jul 2020\n2 Related Work\nVector representation of words, known as word em-\nbeddings, brought a major advance in NLP. The\nfeatures represented by those vectors may help al-\ngorithms achieve better performance in a series of\ntasks. This happens because those vectors are able\nto capture several characteristics of the language,\nallowing algorithms to focus on solving a speciﬁc\ntask. (Mikolov et al., 2013) presented several exam-\nples of the potential of these representations which\nallow clustering by meaning and vector operations.\nDespite the ability of these vectors to represent\nseveral aspects of the language, their ﬁxed repre-\nsentation does not offer ﬂexibility when the mean-\ning of the words needs to be understood within a\ncontext. Such a context-independent representa-\ntion works like a dictionary with a ﬁxed meaning\npreviously determined. This issue was studied by\n(Peters et al., 2018) that proposed ELMo (Embed-\ndings from Language Models). In that approach,\nthe embeddings are a function of all internal layers\nof a bidirectional language model.\nThe use of these representations may help tackle\nthe problem of little training data. This problem\nhappens as the algorithm has to learn both how\nto solve one speciﬁc task and how to represent\nseveral aspects of the language. If a learning algo-\nrithm could already understand some behavior of\nthe language, then the speciﬁc task to be learned\nwould become easier. The technique of using the\npreviously learned knowledge and applying it to an-\nother model is known as Transfer Learning. There\nare two common approaches for transfer learning:\nfeature extraction and ﬁne-tuning. Feature extrac-\ntion happens when the learned representations from\na previous network are used to extract meaning-\nful features from new samples. Fine-tuning (Rad-\nford et al., 2018; Devlin et al., 2018; Lan et al.,\n2019) adopts a similar approach. It takes advan-\ntage of previously trained weights, but the model is\nslightly modiﬁed. Typically, the last few layers of\nthe model are replaced by a task-speciﬁc layer like\na classiﬁer or a linear regression layer. After that,\nthe entire model is trained, i.e., “ﬁne-tuned” with\na small learning rate, allowing the model to keep\npart of the learned representations and focus on the\nspeciﬁc task.\nAnother major advance in the last few years was\nthe Transformer (Vaswani et al., 2017) model. It\nallowed avoiding the need for purely recurrent mod-\nels. Subsequent models like BERT (Devlin et al.,\n2018), Generative Pre-trained Transformer (Rad-\nford et al., 2018), RoBERTa (Liu et al., 2019), and\nALBERT (Lan et al., 2019) use the Transformer\narchitecture, allowing deeper training. These mod-\nels have improved the state-of-the-art in several\nnatural language tasks.\nIn an effort to disseminate the use of these state-\nof-the-art methods, this work pre-trained BERT\nand ALBERT in Portuguese. Portuguese is a major\nlanguage in terms of the number of native speakers,\nbut it is still underrepresented in terms of linguistic\nresources compared to other European languages.\nWe believe our work can help mitigate this issue.\n3 Pre-training\nBERT and ALBERT frameworks are designed to be\nused in two steps: pre-training and ﬁne-tuning. In\nthe ﬁrst step, the unsupervised model is pre-trained\nusing a large corpus. At this stage, the model learns\nthe language features using two training objectives.\nBERT uses Masked Language Model (MLM) and\nNext Sentence Prediction (NSP), while ALBERT\nchanges the NSP objective to a Sentence Ordering\nobjective (SO). In MLM, at random, 15% of the\ntokens are replaced by a [MASK] token and the\nmodel is supposed to guess which was the best to-\nken to be put in its place. NSP requires that, during\npre-training, the input is always composed of two\nsentences and the model should learn if the second\nsentence correctly follows the ﬁrst. ALBERT’s au-\nthors showed that the NSP objective is easy, so they\nchanged it to a SO objective to force the model to\nlearn deeper features. The combination of these\nobjectives forces the model to learn many language\nfeatures.\nPreparation. Although pre-training is an unsu-\npervised task ( i.e., it just needs a raw document\ncorpus), it requires each training instance to be in a\nspeciﬁc format. The documents are split into sen-\ntences and the sentences are tokenized. BERT uses\nWordPiece (Wu et al., 2016), while ALBERT uses\nSentencePiece (Kudo and Richardson, 2018). We\nused the vocabulary size of 30,000 following the\nsame size used by both BERT and ALBERT when\ntraining their single language English model.\nPortuguese uses diacritics and, while the text\ncan be understood without them, removing them\nintroduces noise as some discriminating features\nare lost – e.g., the distinction between baby (bebˆe)\nand s/he drinks ( bebe) is on the diacritical mark.\nRemoving diacritics while pre-training could allow\nthe model to have a better interpretation of informal\ntexts that generally do not use them like tweets or\nshort message service (SMS). Nevertheless, we\ndecided to keep diacritics and the original casing.\nThe goal was to maintain discriminating features\nthat could be useful in some tasks.\nIdeally, the pre-trained model should be exposed\nto texts in the format that it will later be ﬁne-tuned\nwith. In other words, the same pre-processing steps\nshould be applied both when training and evaluat-\ning.\nWe used 4.8GB of text (992 million tokens) from\ndifferent kinds of sources. Because each model has\na different vocabulary and tokenizer, the creation\nof pre-training data must be done for either BertPT\nand AlbertPT.\nCorpora. Pre-training is unsupervised and it re-\nquires a large corpus. To allow for the model to\nencompass different text styles (formal and infor-\nmal), Brazilian (BP) and European (EP) variants,\nwe used corpora form a variety of sources.\n•Wikipedia-PT1, with 8M sentences (after\npre-processing) in formal writing, casing, and\nits contents include all regional variations of\nPortuguese.\n•The Open Subtitles corpus2 in BP was used\nas a source of informal language (as it repre-\nsents spoken language containing slangs and\ncurses). Sentences are short, frequently hav-\ning fewer than ﬁve words.\n•News articles from two corpora: ( i) the\nCHA VE corpus3, which contains full news\narticles from the Portuguese P´ublico4 and the\nBrazilian Folha de S ˜ao Paulo5. Combined,\nthey have a total of 106M tokens; and ( ii) a\nnews corpus from Kaggle6 containing 167K\nnews articles from Folha de S˜ao Paulo.\n•The EuroParl corpus extracted from the pro-\nceedings of the European Parliament 7. The\nPortuguese sub-corpus contains about 75M\n1https://dumps.wikimedia.org/\nbackup-index.html\n2http://opus.nlpl.eu/\nOpenSubtitles-v2016.php\n3https://www.linguateca.pt/CHAVE/\n4http://www.publico.pt/\n5https://www.folha.uol.com.br/\n6https://www.kaggle.com/marlesson/\nnews-of-the-site-folhauol\n7https://www.europarl.europa.eu/\ntokens and it is available at the Open Parallel\nCorpus site8. Texts are formally written in EP.\n•Research abstracts from 23K MSc theses\nand PhD dissertations from several areas. The\nsources were taken from the Brazilian web-\nsite Dom´ınio Publico 9. This corpus is also\nmade available together with our pre-trained\nmodels10. Texts are formally written in BP.\nParameters. During pre-training, we used whole\nword masking to avoid that a token being masked\nin the middle. For both BERT and ALBERT, the\ndefault base conﬁgurations were used – 12 layers,\n768 hidden size, and 12 heads of attention. With\nthis conﬁguration, the model used for BertPT has a\ntotal of 110M parameters. As for AlbertPT, there\nare only 12M parameters.\nFollowing the recommended pre-training proce-\ndures, BertPT was trained for 1M steps and Al-\nbertPT, 175K steps. Due to the high number of pa-\nrameters, BERT takes longer than Albert, and train-\ning with longer sequences becomes quite expen-\nsive. As the complexity is quadratic to the length\nof the sequence, our models were pre-trained with\nsequences of lengths 128 and 512. Their training\ntook 33 and 17 hours, respectively, on one cloud\nTPU v2.\n4 Evaluation\nIn this section, we report on experiments that ﬁne-\ntune the pre-trained modes BertPT and AlbertPT.\nThe goal was is to compare our trained monolin-\ngual models to the provided multilingual and to\nthe current state-of-the-art when applied to several\ndifferent Natural Language Understanding tasks.\nIn order to focus on the capacity of the models\nin extracting features from texts, our models have\nsimple architectures. They are composed of the\nstandard base model and the pooled output from\nthe last layer is then used as input for a classiﬁer or\nregression layer. The new parameters introduced\nare just the weights from this last layer. This pooled\noutput is a vector that should be able to capture the\ndesired features to solve the task. Fine-tuning to\neach speciﬁc task took around ﬁve minutes of train-\ning using one cloud TPU v2.\nNext, we report on our experiments on the fol-\nlowing tasks: ( i) semantic textual similarity, ( ii)\n8http://opus.nlpl.eu/\n9http://www.dominiopublico.gov.br/\n10https://github.com/diego-feijo/\nbertpt/\nrecognizing textual entailment, (iii) identifying of-\nfensive texts, (iv) detecting fake news, (v) catego-\nrizing news, (vi) classifying sentiment polarity, and\n(vii) identifying emotions.\nIn all tasks, BertPT and AlbertPT were com-\npared to the Multilingual BERT model released by\nGoogle Research, and also to published results for\neach dataset.\n4.1 Recognizing Textual Entailment (RTE)\nThe ASSIN dataset is used for semantic textual\nsimilarity and recognizing textual entailment. It\nwas ﬁrst used in a shared task in the PROPOR\n2016 Conference11. It has 10K pairs of sentences,\nwhich are equally split between Brazilian (BP) and\nEuropean Portuguese (EP). Each language variant\nis composed of three ﬁles: training (2,5K pairs),\ndevelopment (500 pairs), and testing (2K pairs).\nThe sentences were relatively short. As we said\nbefore, each tokenizer split tokens differently. The\nmaximum length in this dataset was 78 tokens.\nThis task involves determining whether the\nmeaning of one sentence is entailed (can be in-\nferred) from the other sentence. This is a three-\nclass classiﬁcation problem that requires assigning\na label (None, Entailment, or Paraphrase) to the\ngiven pair of sentences. These classes are highly\nimbalanced, with the None class having three times\nmore instances than Entailment and nine times\nmore instances than Paraphrase. The evaluation\nis done using Accuracy and Macro F1. Macro F1\nevaluates the F-measure for each class indepen-\ndently and then takes the average. The models\nwere trained using both training and development\nsets. Ten-fold stratiﬁed cross-validation was ap-\nplied over the training data. Table 1 shows results\nusing just EP, just BP, and a concatenation of both\nEP+BP training data. There were no published\nbaselines for the evaluation of only BP.\nFor this task, all models were superior to previ-\nously reported baselines. In this dataset, sentences\nare well-written, contain punctuation and diacritics.\nThe models were not harmed for having a few miss-\ning tokens in the vocabulary and so they were able\nto build very good representations of the sentences.\nAlso, these short sentences (up to 78 tokens) that\ndo not mix different subjects allow the models to\ngenerate more speciﬁc representations to be used\nby the output classiﬁer. Longer sentences would\nrequire representing all subjects covered. Conse-\n11http://propor2016.di.fc.ul.pt/\nTable 1: RTE results using 10-fold cross-validation\nData Model Acc F1-M\nEP\nRocha and Cardoso (2018) 0.83 0.73\nBertPT 0.86 0.76\nAlbertPT 0.87 0.80\nMultilingual 0.88 0.79\nBP\nBertPT 0.85 0.52\nAlbertPT 0.85 0.51\nMultilingual 0.86 0.57\nEP+BP\nRocha and Cardoso (2018) 0.82 0.70\nBertPT 0.87 0.75\nAlbertPT 0.88 0.79\nMultilingual 0.90 0.80\nquently, the vector representation of the sentences\nwould be more diluted.\nNext, we applied our ﬁne-tuned model to the\nentire training set and ran the evaluation over the\ntest set. The results are on Table 2. The ﬁrst ob-\nservation is that the BP setting is difﬁcult for all\nmodels. The EP+BP** in the last rows indicates\nthat the model was trained using both EP and BP\ntraining sets, but they were evaluated only using\nthe EP test set. The results of this evaluation can\nbe compared to the ﬁrst rows, in which the models\nwere trained and evaluated only on EP test set. Al-\nthough it makes sense to think that more training\ndata would help the model generalize, we reached\nthe same conclusion as (Fialho et al., 2016) that the\nimprovement, if any, in this dataset is meaningless.\nWe could not ﬁnd a published baseline for the case\nwhere the model was trained using EP+BP and\nevaluated using both test sets. Again, as expected,\nBERT models were superior to previous baselines\nfor all combinations. Analysing 10-fold evalua-\ntion and using only the test set, we can conclude\nthat BERT-based models were superior to previous\nbaselines.\n4.2 Semantic Textual Similarity (STS)\nThe goal of Semantic Textual Similarity is to quan-\ntify the degree of similarity between two sentences.\nIn the ASSIN dataset, similarity is a number rang-\ning from 1 (no similarity) to 5 (high similarity).\nThis is a linear regression problem as the output is\na similarity score. The evaluation measures how far\nthe predicted score is from the ground truth using\ntwo metrics: ( i) Pearson Correlation which mea-\nsures the correlation with the ground truth, so the\nhigher, the better, and (ii) and the Mean Squared\nError which measures the square of the difference\nbetween the prediction and the ground truth, so the\nlower the better. Following the same procedure of\nTextual Entailment, we used 3,000 pairs for train-\ning. The results are in Table 2. Since STS is related\nto RTE, again, all models obtained better results\nthan the baselines. Also, the multilingual model\nwas again superior to our pre-trained Portuguese\nmodels.\n4.3 Offensive Comment Identiﬁcation\nOffComBR-2 and OffComBR-3 (de Pelle and Mor-\neira, 2017) are variations of a dataset contain-\ning comments that were posted by readers about\npublished news in a Brazilian news portal. The\ncomments are annotated with yes or no meaning\nwhether the comment was considered offensive\nby human judges. In the OffComBR-2 variation,\nat least two judges found the comment offensive.\nIn the OffComBR-3 variation, all three judges\nagreed that the comment was offensive. Intuitively,\nOffComBR-3 should be less prone to judge bias,\nand thus be easier to classify. This dataset imposes\na challenge because the comments are written very\ninformally. They contain slangs, profanities, emoti-\ncons, and abbreviations. There is no punctuation,\nno diacritics, and casing is almost random.\nThe length of the comments varies signiﬁcantly.\nIn some cases, the offensive part is just one or two\ntokens and it may be not identiﬁed as a vocabulary\nword. Also, authors of the offensive comments typ-\nically try to obfuscate profane language ( e.g., by\ninserting spaces or other symbols among the char-\nacters of the offensive word) to make the task of\nidentifying these comments more difﬁcult. Word-\nPiece and SentencePiece tokenizers are able to rep-\nresent these misspelled tokens using a sequence of\nsingle characters and this is an advantage compared\nto the traditional vocabulary-based representations.\nWhile a sequence of letters may not be enough for\nthe model to build a good feature representation, in\nsome cases, this may sufﬁce.\nThe results in Table 3 indicate that this task is\nhard for BERT models. It was expected that deep\nlearning models would have a signiﬁcant advantage\nover the baseline which uses only shallow methods\nsuch as SVM and Na¨ıve Bayes over unigrams. We\nbelieve that the models were not able to achieve a\nsigniﬁcant advantage because, during pre-training,\nthey were never exposed to offensive language.\n4.4 Fake News Detection\nThe Fake.Br Corpus12 (Monteiro et al., 2018) con-\ntains real examples of fake news written in Brazil-\nian Portuguese. Each instance was manually fact-\nchecked and is annotated indicating whether it is\nfake or true. There are 7,200 instances evenly split\nbetween the classes. All sentences are well-written,\nuse punctuation, and diacritics according to the\ngrammar.\nWe ran the evaluation using 5-fold cross-\nvalidation over all training data. The results are\nin Table 4. The baseline reported achieving up to\n0.89 in F-measure when combining several clas-\nsiﬁcation models. BERT models have done an\nimpressive job in this dataset – they outperformed\nthe baseline and achieved almost perfect results.\nThree reasons may explain such good results. The\nsentences were well-written, so the model was able\nto build a good context representation. Despite\nthe maximum length of the texts being more than\n512, the majority of the texts have similar lengths.\nWe believe that this similarity allowed the model\nto better detect any variations that indicate that\nthe news was true or fake. Finally, we attribute\nthe good performance mostly to the fact that this\ndataset is reasonably large when compared to the\nothers evaluated here. The larger volume of train-\ning data allowed the model to learn enough features\nto distinguish fake from true.\n4.5 Sentiment Polarity Classiﬁcation on\nTweets\nIn (Araujo et al., 2016) the performance of several\nmodels for sentiment polarity classiﬁcation were\nevaluated. The best reported macro F1 with more\nthan 90% of messages classiﬁed was 0.71. We use\nthis score as the baseline for Portuguese, although\nthe authors did not report how it was obtained.\nThere are three possible sentiments: negative,\nneutral, or positive. The writing style is very infor-\nmal and many out of vocabulary were used. Ex-\npressions like “goooood daaaay”, “loool”, “kkk”,\n“=D” are frequent. So, if the model only relies\non known words, this task may be difﬁcult. In\nthis situation, the fact that the model uses subword\nunits to tokenize the text may help to at least try to\nbuild an interpretation using the sequence of units.\nAlso, some examples expose the sentiment through\nemoticons. So, correctly interpreting these symbols\n12https://github.com/roneysco/Fake.\nbr-Corpus\nTable 2: RTE and STS evaluated on Test Sets.\nData Model RTE STS\nAcc F1-M Pearson MSE\nEP\nFialho et al. (2016) 0.84 0.73 0.74 0.60\nBertPT 0.84 0.69 0.79 0.54\nAlbertPT 0.88 0.78 0.80 0.47\nMultilingual 0.89 0.81 0.84 0.43\nBP\nFialho et al. (2016) 0.86 0.64 0.73 0.36\nBertPT 0.86 0.53 0.76 0.32\nAlbertPT 0.87 0.65 0.79 0.30\nMultilingual 0.88 0.55 0.81 0.28\nEP+BP\nBertPT 0.87 0.72 0.79 0.39\nAlbertPT 0.89 0.76 0.78 0.39\nMultilingual 0.90 0.82 0.83 0.33\nEP+BP**\nFialho et al. (2016) 0.83 0.72 - -\nBertPT 0.86 0.66 - -\nAlbertPT 0.88 0.79 - -\nMultilingual 0.91 0.83 - -\nTable 3: Offensive Comment Identiﬁcation\nData Model Acc F1-W\nBR-2\nde Pelle and Moreira (2017) - 0.77\nBertPT 0.78 0.77\nAlbertPT 0.76 0.76\nMultilingual 0.76 0.74\nBR-3\nde Pelle and Moreira (2017) - 0.82\nBertPT 0.84 0.83\nAlbertPT 0.84 0.81\nMultilingual 0.83 0.81\ncan lead to the associated sentiment.\nIn order keep the results comparable, we use\nonly the positive and negative samples. For the\nresults shown in Table 5, we did a 5-fold cross-\nvalidation over all data. The fact that this corpus\nhas several emoticons and out-of-vocabulary ex-\npressions makes it hard for the models that were\nTable 4: Fake News Detection\nModel Acc F1-W\nMonteiro et al. (2018) 0.89 0.89\nBertPT 0.98 0.98\nAlbertPT 0.98 0.98\nMultilingual 0.98 0.98\nnot trained using a similar vocabulary.\nTable 5: Sentiment Polarity Classiﬁcation\nModel Acc F1-W\nAraujo et al. (2016) - 0.71\nBertPT 0.77 0.76\nAlbertPT 0.79 0.78\nMultilingual 0.71 0.70\n4.6 News Category Classiﬁcation\nThe Folha UOL News Dataset13 contains 167,053\nnews from Folha de S˜ao Paulo14, a Brazilian news-\npaper. It contains the headlines, complete articles,\nand their category (opinion, daily life, sports, cul-\nture, markets, world, and politics).\nThere is a public ranking of this classiﬁcation\ntask on Kaggle15. The best result so far used 15%\nof the data as validation set and achieved 87.39%\nof validation accuracy. To keep results as compa-\nrable as possible, we followed the ranking leader\nand randomly split 15% to be used as our test set.\nAs this could be an arbitrary split, we also ran the\n13https://www.kaggle.com/marlesson/\nnews-of-the-site-folhauol\n14https://www.folha.uol.com.br/\n15www.kaggle.com/marlesson/\nnews-of-the-site-folhauol/kernels\nevaluation using stratiﬁed 5-fold cross-validation.\nThe results are shown in Table 6. All models be-\nhaved almost identically, with a slight advantage\nfor the multilingual model.\nTable 6: Folha Uol (7-classes)\nModel Mode Acc F1-W\nKaggle Baseline15 Split 15% 0.87 -\nBertPT Split 15% 0.94 0.94\nAlbertPT Split 15% 0.93 0.93\nMultilingual Split 15% 0.94 0.94\nBertPT 5-fold 0.94 0.94\nAlbertPT 5-fold 0.93 0.93\nMultilingual 5-fold 0.94 0.94\nAnother experiment of news category classiﬁca-\ntion was done using P´ublico Dataset. P ´ublico16 is\na Portuguese Newspaper. The dataset containing\nits news is distributed as part of CHA VE corpus17.\nEach news text belongs to one out of nine cate-\ngories: Science, Culture, Sports, General, Econ-\nomy, Local, World, National, and Society. The\ntask is a multiclass classiﬁcation with a single la-\nbel. For this evaluation, we used a 5-fold cross-\nvalidation. The best reported baseline achieved\n0.84 in F-measure when combining several classiﬁ-\ncation models.\nThe news texts are well written, contain punc-\ntuation and diacritics. The major problem here is\nlength diversity – while the mean length is 728 to-\nkens, the standard deviation is 628. Both BERT\nand ALBERT models were not able to handle such\nlong texts due to memory limitations. We imposed\na limit of using only the ﬁrst 512 tokens of the text.\nTable 7 has the results for this task. Without any\ntask-speciﬁc architecture, the performance in this\ndataset was almost identical to the current state-of-\nthe-art in this task.\nTable 7: P´ublico News (9-classes) using 5-fold\nModel Acc F1-M\nGonc ¸alves and Quaresma (2008) 0.84 0.84\nBertPT 0.84 0.84\nAlbertPT 0.82 0.82\nMultilingual 0.84 0.84\n16https://www.publico.pt/\n17https://www.linguateca.pt/CHAVE/\n4.7 Emotion Classiﬁcation\nThe BRNews Dataset (Martinazzo et al., 2011) con-\ntains news extracted from major Brazilian news-\npapers. Each of the 1,002 instances was manually\nannotated for one within six possible emotions: joy,\nsurprise, anger, disgust, fear, and sadness.\nThe texts from the news are short and well-\nwritten. They use punctuation and diacritics. The\nlength in tokens varies from 21 to 62 tokens, which\nallows the model to capture most of the meaning\nof the sentences. The most represented class has\nup to ten times the frequency of the two least repre-\nsented.\nWe ran two kinds of experiments. In the ﬁrst, we\nemployed a standard multiclass classiﬁer. Unfortu-\nnately, we do not have any baseline results, so we\njust report the results for the BERT-based models.\nIn the second experimental setting, we followed\nthe same methodology as (Becker et al., 2017) and\ntransformed the problem into six binary classiﬁers\n(one for each sentiment). For each one of these\nbinary models, the accuracy and weighted F1 score\nwere calculated. Then, the mean result was taken.\nTable 8 shows the results for 10-fold cross-\nvalidation. The baseline for the binary version\nreaches up to 0.84 of F-measure. In this experi-\nment, BertPT has a result similar to the baseline,\nbut it was not able to overcome it.\nTable 8: Emotion Classiﬁcation\nModel 6 classes Binary\nAcc F1-W Acc F1-W\nBecker et al. (2017) - - - 0.84\nBertPT 0.51 0.47 0.84 0.83\nAlbertPT 0.41 0.28 0.84 0.81\nMultilingual 0.49 0.46 0.84 0.80\nInspecting some misclassiﬁcations, we see some\nunusual annotations. For example, “President Lula\nwill be in Sergipe this Friday for inauguration of\nworks: Lula will visit three cities. The ally Gover-\nnor will be joining him.” This instance was anno-\ntated as “fear”, but BertPT predicted “joy”. So, we\nthink that the fact that the models were unable to\nbeat the established baseline is more due to dataset\ncharacteristics than the ability of the model to in-\nterpret its meaning.\n5 Discussion\nThe different tasks and settings under which BertPT\nand AlbertPT were compared to Multilingual\nBERT and published baselines yield 38 possible\npairwise comparisons. We analyzed in how many\ntimes the performance of each model was better\nthan, worse than or equivalent to the performance\nof the others. If the proportional difference in\nscores was below 5%, then the two models were\nconsidered equivalent. These results are shown in\nTable 9. We can see that no single method is always\nbetter than the others. The baselines have fewer\nwins, which means that BERT-based models tend to\nyield better results. BertPT and AlbertPT achieved\nequivalent performances most of the time (28 out\nof 38). AlbertPT has an advantage (6 wins versus\n4 losses), and since its training is less expensive,\nthen it would be the model of choice.\nOverall, Multilingual BERT has more wins in\ncomparison to other alternatives. Although the\ndifferences lie most of the time within the 5% range,\nit achieved better results in more tasks. We attribute\nthis superiority to the use of more languages. It\nis possible that the presence of languages that are\nsimilar to Portuguese (such as Spanish, which has\ntwice the volume of texts of Portuguese) was able\nto produce richer semantic representations.\nTable 9: Comparison of the 3 models across all tasks\nModel BertPT AlbertPT Multilingual\n> = < > = < > = <\nBaseline 3 12 8 1 8 14 1 13 9\nBertPT 4 28 6 2 24 12\nAlbertPT 1 30 7\nThe limitation on the size of the input (which\ncould have a higher impact on the news categoriza-\ntion task) ended up proving not to be an issue – clas-\nsiﬁcation accuracy was very high in both datasets.\nClassiﬁcation errors happened in cases in which\nthe distinction of the categories was quite fuzzy\n(e.g., daily life and culture).\nMultilingual BERT was not trained on informal\ntexts, which were abundant in two of our tasks (of-\nfensive comment detection and sentiment polarity\nclassiﬁcation). As a result, the results of BertPT\nand AlbertPT were slightly superior in these tasks,\nshowing that having more text styles on the training\nset is beneﬁcial.\n6 Conclusion\nThis work presented BertPT and AlbertPT, pre-\ntrained language models for Portuguese. We de-\nscribed the pre-training procedure along with the\nresources we used. Then, the models were evalu-\nated in seven natural language understanding tasks.\nThe results were compared with the ofﬁcial Multi-\nlingual BERT and existing baselines for each task.\nThe good results achieved by AlbertPT and the\nMultilingual BERT, indicate that having an Albert-\nbased multilingual language model would be a very\nuseful resource. As future work, we intend to pre-\ntrain a multilingual model using languages with\ngood availability of training data that are similar to\nPortuguese such as Spanish, French, and Italian.\nReferences\nMatheus Araujo, Julio Reis, Adriano Pereira, and Fabri-\ncio Benevenuto. 2016. An evaluation of machine\ntranslation for multilingual sentence-level sentiment\nanalysis. In ACM SAC.\nKarin Becker, Viviane P. Moreira, and Aline G.L. dos\nSantos. 2017. Multilingual emotion classiﬁcation\nusing supervised learning. Inf. Process. Manage. ,\n53(3):684–704.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. TACL, 5:135–146.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nPedro Fialho, Ricardo Marques, Bruno Martins, Lu ´ısa\nCoheur, and Paulo Quaresma. 2016. Inesc-\nid@assin: Medic ¸˜ao de similaridade sem ˆantica e re-\nconhecimento de infer ˆencia textual. Linguam´atica,\n8(2):33–42.\nTeresa Gonc ¸alves and Paulo Quaresma. 2008. Us-\ning linguistic information to classify portuguese text\ndocuments. In Mexican International Conference\non Artiﬁcial Intelligence, pages 94–100.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nEMNLP, pages 66–71.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv:1907.11692.\nBarbara Martinazzo, Mariza Miola Dosciatti, and\nEmerson Cabrera Paraiso. 2011. Identifying emo-\ntions in short texts for brazilian portuguese. In In-\nternational Workshop on Web and Text Intelligence,\npage 16.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, pages 3111–3119.\nRafael A. Monteiro, Roney L. S. Santos, Thiago A. S.\nPardo, Tiago A. de Almeida, Evandro E. S. Ruiz,\nand Oto A. Vale. 2018. Contributions to the study of\nfake news in portuguese: New corpus and automatic\ndetection results. In PROPOR, pages 324–334.\nRogers P. de Pelle and Viviane P. Moreira. 2017. Of-\nfensive comments in the brazilian web: a dataset and\nbaseline results. BraSNAM.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In EMNLP, pages 1532–1543.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual bert?\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning. Technical re-\nport, Technical report, OpenAI.\nGil Rocha and Henrique L. Cardoso. 2018. Recogniz-\ning textual entailment: challenges in the portuguese\nlanguage. Information, 9(4):76.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7158501744270325
    },
    {
      "name": "Computer science",
      "score": 0.5504473447799683
    },
    {
      "name": "Natural language processing",
      "score": 0.4277631938457489
    },
    {
      "name": "Linguistics",
      "score": 0.3327972888946533
    },
    {
      "name": "Engineering",
      "score": 0.11313045024871826
    },
    {
      "name": "Electrical engineering",
      "score": 0.09532970190048218
    },
    {
      "name": "Voltage",
      "score": 0.08069843053817749
    },
    {
      "name": "Philosophy",
      "score": 0.06222382187843323
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130442723",
      "name": "Universidade Federal do Rio Grande do Sul",
      "country": "BR"
    }
  ]
}