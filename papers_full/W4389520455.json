{
  "title": "Text Classification via Large Language Models",
  "url": "https://openalex.org/W4389520455",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5014777126",
      "name": "Xiaofei Sun",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5100651417",
      "name": "Xiaoya Li",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5100758099",
      "name": "Jiwei Li",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5004882141",
      "name": "Fei Wu",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5073264981",
      "name": "Shangwei Guo",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5101591101",
      "name": "Tianwei Zhang",
      "affiliations": [
        "Chongqing University",
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5031220156",
      "name": "Guoyin Wang",
      "affiliations": [
        "Amazon (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3174396451",
    "https://openalex.org/W2147489358",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2265846598",
    "https://openalex.org/W2962946486",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3152740956",
    "https://openalex.org/W4288563585",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2964046515",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2963912736",
    "https://openalex.org/W2145658888",
    "https://openalex.org/W3212748247",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4366735603",
    "https://openalex.org/W4296845143",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4389523710",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W2407776548",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4385570306",
    "https://openalex.org/W4306294746",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W4385571271",
    "https://openalex.org/W3005187732",
    "https://openalex.org/W3160195908",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W4385574286",
    "https://openalex.org/W4287854450",
    "https://openalex.org/W4286795974",
    "https://openalex.org/W4287649493",
    "https://openalex.org/W3035542229",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W4281629162",
    "https://openalex.org/W4309811444",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4280569535",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification.This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning. In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for kNN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM’s generalization ability and the task-specific evidence provided by the full labeled dataset. Remarkably, CARP yields new SOTA performances on 4 out of 5 widely-used text-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on AGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance comparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP delivers impressive abilities on low-resource and domain-adaptation setups. Specifically, using 16 examples per class, CARP achieves comparable performances to supervised models with 1,024 examples per class.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8990–9005\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nText Classification via Large Language Models\nXiaofei Sun♦*, Xiaoya Li♣*, Jiwei Li♦, Fei Wu♦\nShangwei Guo▲, Tianwei Zhang♥, Guoyin Wang⋆\nAbstract\nDespite the remarkable success of large-\nscale Language Models (LLMs) such as\nGPT-3, their performances still significantly\nunderperform fine-tuned models in the task\nof text classification. This is due to (1) the\nlack of reasoning ability in addressing complex\nlinguistic phenomena (e.g., intensification,\ncontrast, irony etc); (2) limited number of\ntokens allowed in in-context learning.\nIn this paper, we introduce Clue And\nReasoning Prompting (CARP). CARP adopts\na progressive reasoning strategy tailored to\naddressing the complex linguistic phenomena\ninvolved in text classification: CARP first\nprompts LLMs to find superficial clues (e.g.,\nkeywords, tones, semantic relations, references,\netc), based on which a diagnostic reasoning\nprocess is induced for final decisions. To\nfurther address the limited-token issue, CARP\nuses a fine-tuned model on the supervised\ndataset for kNN demonstration search in the\nin-context learning, allowing the model to take\nthe advantage of both LLM’s generalization\nability and the task-specific evidence provided\nby the full labeled dataset.\nRemarkably, CARP yields new SOTA\nperformances on 4 out of 5 widely-used\ntext-classification benchmarks, 97.39 (+1.24)\non SST-2, 96.40 (+0.72) on AGNews, 98.78\n(+0.25) on R8 and 96.95 (+0.6) on R52, and\na performance comparable to SOTA on MR\n(92.39 v.s. 93.3). More importantly, we find\nthat CARP delivers impressive abilities on\nlow-resource and domain-adaptation setups:\nusing 16 examples per class, CARP achieves\ncomparable performances to supervised\nmodels with 1,024 examples per class. Code is\navailable at github.com/ShannonAI/GPT-CLS-\nCARP 1 2 3\n1 ∗denotes equal contributions.\n2 ♦Zhejiang University, ♣ Shannon.AI, ⋆Amazon,\n♥Nanyang Technological University,▲Chongqing University\n3xiaoya_li@shannonai.com,{xiaofei_sun,\njiwei_li}@zju.edu.cn\n1 Introduction\nLarge language models (LLMs) (Radford et al.,\n2019a; Xue et al., 2020; Zhang et al., 2022a; Rae\net al., 2021; Brown et al., 2020; Chowdhery et al.,\n2022; Ouyang et al., 2022; Thoppilan et al., 2022)\nhave shown the ability for in-context learning (ICL).\nGiven a few demonstration examples, LLMs are\nprompted to generate results for a new test example,\nand have achieved performance comparable to\nsupervised baselines or even state-of-the-art results\nin a variety of natural language processing (NLP)\ntasks such as question answering (Trivedi et al.,\n2022), natural language inference, (Schick and\nSchütze, 2020), named entity recognition (Wang\net al., 2023), relation extraction (Wan et al., 2023)\nand information extraction (Han et al., 2021).\nIn spite of the success, LLMs with ICL\nstill significantly underperform fine-tuned models\nfor text classification. This is due to two\nreasons: (1) Text classification requires models\nwith more powerful reasoning abilities to\nresolve complex linguistic phenomenon including\nclause composition (e.g., concession, negation,\nintensification), irony, etc. Recent efforts to\nimprove LLMs’ reasoning capabilities (Wei et al.,\n2022; Kojima et al., 2022; Ye and Durrett, 2022;\nZhang et al., 2022b) mainly focus on tackling\nmath problems, and thus are not tailored to\naddressing the reasoning process necessary for the\nmultitude of intricate linguistic phenomena in text\nclassification; (2) The number of demonstration\nexamples allowed in in-context learning is limited,\ne.g., the longest context allowed for GPT-3 is 4,096\nsubtokens. Therefore, LLMs are only able to take\nthe advantage of a small proportion of the training\nset, performing well below supervised baselines;\nIn this paper, we introduce Clue And Reasoning\nPrompting (CARP), an extensible, annotation-\nfree and efficient framework for text classification\nvia large language models. To address the\n8990\nThis is an overall sentiment classifier for movie reviews.First, present CLUES (i.e., keywords, phrases, contextual information, semantic relations, semantic meaning, tones, references) that support the sentiment determination of input.Second, deduce a diagnostic REASONING process from premises (i.e., clues, input) that supports the sentiment determination (Limit the number of words to 130).Third, determine theoverall SENTIMENT of INPUT as Positive or Negative considering CLUES, the REASONING process and the INPUT.INPUT: noycefilms it more as a shocking history lesson than as drama.CLUES: -Positive: \"more,\" \"shocking,\" \"history lesson,\" \"drama.\" -Negative: None.REASONING: The input contains positive words such as \"more,\" \"shocking,\" and \"drama,\" which suggest that the sentiment is positive. Additionally, the phrase \"history lesson\" implies that the film is educational and informative, which is generally seen as a positive thing. Therefore, the sentiment of the input is positive.SENTIMENT: PositiveINPUT: press the delete keyCLUES: -Positive: None. -Negative: \"delete key.\"REASONING: The input contains the phrase \"delete key,\" which implies that something is being removed or erased. This is generally seen as a negative action, suggesting that the sentiment of the input is negative.SENTIMENT: Negative\n (b) CARP Few-shot (k=1)\nThis is an overall sentiment classifier for movie reviews.First, list CLUES (i.e., keywords, phrases, contextual information, semantic relations, semantic meaning, tones, references) that support the sentiment determination of input..Second, deduce the diagnostic REASONING process from premises (i.e., clues, input) that supports the INPUT sentiment determination (Limit the number of words to 130).Third, based on clues, reasoning and input, determine the overall SENTIMENT of INPUT as Positive or Negative.INPUT: press the delete keyCLUES: delete keyREASONING: The phrase \"delete key\" implies an action of removing something, which could be interpreted as a negative sentiment.SENTIMENT: Negative\n (a) CARP Zero-shot\nFigure 1: Examples of CARP prompts under zero-shots and few-shot settings. Comparisons of different prompts\ncan be found in Appendix H.\nreasoning process necessary for handling the\nlinguistic phenomena in text classification, CARP\ndecomposes the reasoning process into three steps,\nwhere LLMs are first prompted to find superficial\nclues (e.g., keywords, tones, semantic relations,\netc) in the given text; next, CARP treats the clues\nand input as premises and induce a diagnostic\nreasoning process; and finally determine the final\nlabel considering the above two steps. We find\nthis progressive reasoning strategy to be effective\nin enhancing LLMs’ ability in language reasoning\ninvolved in text classification. Due to the limited\nnumber of tokens allowed in context, a more\neffective demonstration search is needed. CARP\nuses a fine-tuned (FT) model on the supervised\ndataset for kNN demonstration search for ICL.\nSince the fine-tuned model is trained based on task-\nspecific labels, it guarantees that retrieved samples\nare close to the input sequence with respect to the\ntask. FT-based demonstration search provides a\nchannel to connect LLMs with the full training set,\nin spite of the limited number of tokens allowed in\ndemonstrations. This strategy lets the model take\nthe advantage of both the LLMs’ generalization\nabilities and all task-specific evidence provided by\nthe training dataset.\nRemarkably, CARP yields new SOTA\nperformances on four out of 5 widely-used\ntext-classification benchmarks, 97.39 (+1.24)\non SST-2, 96.40 (+0.72) on AGNews, 98.78\n(+0.25) on R8 and 96.95 (+0.6) on R52, and\na performance comparable to SOTA on MR\n(92.39 v.s. 93.3). More importantly, we find that\nCARP delivers impressive ability on low-resource\nand domain adaptation setups with orders of\nmagnitude fewer training examples. Specifically,\nCARP achieves comparable performances with 16\nexamples per class to supervised models trained\non the full training set containing more than 1\nthousand examples per class. This demonstrates\nthe capabilities of CARP in real-world text\nclassification cases where training data is limited.\n2 Related Work\n2.1 Large Language Models\nLarge language models (LLMs) are models that\nare trained using self-teaching algorithms on\nlarge unlabeled corpora. LLMs can be broadly\ndivided into three categories based on the model\narchitecture. The first category is the encoder-only\nmodel like BERT (Devlin et al., 2018). BERT\n(300M) (Devlin et al., 2018) and its variants (Liu\net al., 2019; Sun et al., 2020; Clark et al., 2020;\nFeng et al., 2020; Sun et al., 2021) adopt the\npre-training then fine-tuning paradigm for NLP\ntasks: use masked language models as the main\ntraining objective for pretraining, and fine-tune\nthe pretrained model in the annotated downstream\ndatasets. The second category is the decoder-\nonly models like GPT (Radford et al., 2019a).\nGPT (Radford et al., 2019a) uses the decoder\n8991\nof an auto-regressive transformer (Vaswani et al.,\n2017) model for predicting the next token in a\nsequence. GPT (Radford et al., 2019a) and its\nvariants (Dai et al., 2019; Keskar et al., 2019;\nRadford et al., 2019b; Chowdhery et al., 2022;\nZhang et al., 2022a) also follow the pre-training\nthen fine-tuning paradigm. GPT-3 (175B) (Brown\net al., 2020) proposes to formalize all NLP tasks\nas generating textual responses condition on the\ngiven prompt. The third category is the encoder-\ndecoder models like T5 (Raffel et al., 2020). T5\n(11B) (Raffel et al., 2020) and its variants (Lewis\net al., 2019; Xue et al., 2020).\n2.2 In-context Learning\nIn-context learning (ICL) generates textual\nresponses (i.e., label words) conditioning on the\ngiven prompt (usually) with a few annotated\nexamples for downstream tasks. Li and Liang\n(2021); Zhong et al. (2021); Qin and Eisner\n(2021) propose to optimize prompts in the\ncontinuous space. Rubin et al. (2021); Das\net al. (2021); Liu et al. (2021); Su et al. (2022)\nintroduce different strategies for selecting in-\ncontext examples. Lampinen et al. (2022) show\nthat explanations of examples in a few-shot prompt\nlead to a performance boost. Marasovi ´c et al.\n(2021) find that GPT-3 outperforms other models\nby a large margin in the explanation generation\ntask. Wei et al. (2022) propose chain-of-thought\nreasoning and utilized <input, chain-of-thought,\noutput> triples as the prompt for LLMs. Wiegreffe\net al. (2021) traine a supervised filter to select\nexplanations generated by GPT-3 on the SNLI and\nCommonsenseQA tasks.\n2.3 Text Classification\nText classification is a task that aims to assign\npredefined labels (e.g., sentiment, topic, etc) to\na given text. Earlier work decouple the task into\ntwo steps: (1) extract features using neural models\nsuch as RNNs (Irsoy and Cardie, 2014; Yang et al.,\n2016; Wang et al., 2018; Liu et al., 2016; Xie et al.,\n2020), CNNs (Kim, 2014; Zhang et al., 2015; Lai\net al., 2015; Conneau et al., 2016; Wei and Zou,\n2019), GCN (Yao et al., 2019), LLMs (Howard and\nRuder, 2018; Sun et al., 2019; Chai et al., 2020;\nChen et al., 2020; Lin et al., 2021); and (2) feed\nextracted features into a classifier (Joulin et al.,\n2016; Tang and Surdeanu, 2023) to obtain the final\nlabel.\nWith LLMs, Schick and Schütze (2020)\nreformulate input examples into cloze-style phrases\nand annotate the unlabeled text. Han et al. (2021)\ndesign sub-prompts and applied logic rules to\ncompose sub-prompts into final prompts. Liu\net al. (2021) retrieve semantically-similar examples\nto a test sample to formulate its corresponding\nprompt. Shi et al. (2022) retrieve label-words-\nsimilar examples as demonstrations in prompts.\n3 Prompt Construction\n3.1 Overview\nWe follow the standard prompt-based in-context\nlearning paradigm. Given an input sequence\nxinput = {x1,x2,...,x l}, the task of assigning a\ntext-class label to an input text is transformed to\ngenerating a pre-defined textual responsey ∈Yverb\n(e.g., positive, negative, etc) conditioning on the\nprompt xprompt using a language model.\n3.2 Prompt Construction\nThe prompt xprompt, which is constructed based on\nx, consists of the following three components:\n(1) Task description xdesc generally describes\nthe task. For different classification tasks, e..g,\nsentiment classification, topic classification, etc,\ndescriptions are different. Take the sentiment\nclassification task as an example, the task\ndescription is given as follows:\nClassify the overall sentiment of the input as\npositive or negative\n(2) Demonstration consists of a sequence of\nannotated examples:\n{(x1\ndemo,y1\ndemo),..., (xk\ndemo,yk\ndemo)}\nwhere xj\ndemo,1 ≤ j ≤ k denotes the jth\ninput sequence and yj\ndemo denotes the text which\nis transformed from the label, e.g., positive or\nnegative for the binary sentiment classification\ntask. Demonstration serves as two purposes: (1)\nproviding the LLM with evidence to consult on\nfor decision making, which will significantly boost\nperformances; (2) provides an output format that\nLLM’s outputs need to follow, so that the output,\nwhich takes the form of natural language, can be\nfurther easily transformed to labels. It is worth\nnoting that demonstrations are only needed for the\nfew-shot setup, but not for the zero-shot setup.\n8992\n(3) Input xinput is the test text sequence to\nclassify.\nThe prompt xprompt for a test input is\nconstructed by concatenating the task\ndescription xdesc, a sequence of demonstrations\n{(x1\ndemo,y1\ndemo),..., (xk\ndemo,yk\ndemo)}, and the test\nsequence xtest, which can be given as follows:\n{xdesc; \\n; <demo>1; \\n; ...; <demo>k; \\n; xtest}\n3.3 Demonstration Sampling\nThe few-shot setup requires demonstrations\nsampled from the training set. Strategies that we\nexplore include:\nRandom Sampling a straightforward strategy\nfrom samplings is to randomly sample kexamples\nfrom the training set Dtrain for a text sequence xtest.\nkNN Sampling The key disadvantage for\nrandom sampling is that there is no guarantee that\nselected samples are semantically related to the\ninput sequence. One straightforward alternative\nis to sample examples that are similar to the test\nsequence using kNN search (Khandelwal et al.,\n2020). In this process, the test sequence xtest is\nfirst mapped to a vector vtest using an encoder\nmodel f. Then using vtest as the query, we search\nthrough the entire training set Dtrain to retrieve k\nnearest text sequence to getknearest data examples\nN = {xj,yj}k\nj=1 as demonstrations. We use\nthe following encoder models to obtain sentence\nrepresentations and similarity scores:\nSimCSE is a contrastive learning model for\nsentence embeddings(Gao et al., 2021). 4\nFinetuned Model FT for short. The key\ndisadvantage of SimCSE (Gao et al., 2021) and\nother general semantic encoding models (Reimers\nand Gurevych, 2019; Seonwoo et al., 2022; Sun\net al., 2022) is that it measures the general semantic\nsimilarity but is not specifically tailored to the text\nclassification task. To resolve this issue, CARP\nuses the model fine-tuned on the training dataset as\nthe kNN encoder model. Specifically, we first fine-\ntune a Roberta model on the training data. Next\nwe use the [CLS] embedding as the sentence level\nrepresentation for KNN search. Since the fine-\ntuned model is trained based on task-specific labels,\nit guarantees that retrieved samples are close to the\n4We use Sup-SimCSE-RoBERTa-Large model as an\nencoder model.\ninput sequence with respect to the task. Using fine-\ntuned model provides a channel to connect LLMs\nwith the full training set, in spite of the limited\nnumber of tokens allowed in demonstrations. This\nstrategy lets the model take the advantage of both\nthe LLMs’ generalization abilities and all task-\nspecific evidence provided by the training dataset.\n4 Clues Collecting and Reasoning\nTo enhance the models’ reasoning ability in\naddressing linguistic phenomenon tailored to text\nclassification, we propose a progressive reasoning\nstrategy that involves clue collection, reasoning and\ndecision making. This process also mimics how\nhuman decisions: where we first collect evidence\nfrom the input, separating chaff from wheat; next\nwe piece together local evidence to form a global\npicture, which leads to final decision making.\nNext we first given an overview of the the clue\ncollecting and reasoning process, and then describe\nimplementation details.\n4.1 Overview\nCollecting Clues For a test sequence, clues\nare local fact evidence such as keywords,\nphrases, contextual information, semantic meaning,\nsemantic relationships, tones, references, etc. The\nfollowing is an example for clues of an input:\nInput: Steers turns in a snappy screenplay that\ncurls at the edges; so clever you want to hate it.\nClues: \"snappy\", \"clever\", \"want to hate it\" are\nclues for determining the sentiment of the input\nsentence.\nReasoning For reasoning, the LLM is prompted\nto go beyond superficial keywords to mine deeper\nperspectives, considering language phenomenon\nsuch as negation, intensification, irony, etc), and\npiece together local evidence to form the final\ndecision. The following example shows the\nreasoning process to decide the sentiment of the\nabove example based on the evidence collected:\n1. The phrase \"snappy screenplay\" implies that the\nscreenplay is of a high quality and is well-crafted.\n2. The phrase \"curls at the edges\" implies that the\nscreenplay is cleverly written.\n3. ...\nDecision Making Based on the reasoning\nprocess, the model makes the decision for the\nsentiment of the given input:\n8993\nOverall, the clues and reasoning process point to\na positive sentiment for the input sentence.\nThe merits for the incorporation of clue finding\nand reasonings are as follows: (1) it prompts the\nmodel to progressively think and make decisions:\nclue finding focuses more on superficial features\nsuch as keywords, while reasoning makes deeper\njustifications based on superficial features; (2) clue\nfinding and reasoning serve as a tunnel to let human\nintervene: in the few-shot setup, where clues\nand reasons need to be prepared in advance for\ndemonstrations, we can modify them as we see fit.\nThis is extremely helpful for trouble shooting in the\nprompt-construction stage for error corrections; (3)\nfrom an interpretation and uncertainty estimation\nperspective, clues and reasoning in few-shot setups\nare human-readable influence functions;\n4.1.1 zero-shot scenario\nIn the zero-shot setup, as no demonstration is\nallowed, no concrete example for clues and reasons\ncan be provided in the prompt. In this way, we only\nadd requests asking the model to output clues and\nreasons in the prompt.\n4.1.2 few-shot scenario\nIn the few-shot setup , we need to prepare clues\nand reasonings for all examples in the training set\nin advance as all training examples have chances\nto be selected as demonstrations given different\ntest inputs. Previous efforts in math problems (Wei\net al., 2022; Kojima et al., 2022; Ye and Durrett,\n2022; Zhang et al., 2022b) prepare hand-drafted\nreasoning for a few examples, and always use these\nexample as demonstrations. This strategy does\nnot fit for our situation as it is extremely time-\nintensive to manually generate clues and reasonings\nfor all training examples. To resolve this issue, we\nharness LLMs for automatic clue and reasoning\ngeneration, where we ask LLMs to generate clues\nand reasoning based on both the input and its\ncorresponding label.\nClue Generation For a given training example\n<text> paired with the label word <label-word>\n(e.g., positive), we ask LLM to generate clues that\nindicate the label:\nList CLUES (i.e., keywords, phrases, contextual\ninformation, semantic meaning, semantic\nrelationships, tones, references) that support\nthe sentiment determination of the input (limit\nto 15 words).\nINPUT: <text>\nSENTIMENT: <label-word>\nReasoning Generation Based on clues\ngenerated clues, the input, and the label, we ask\nLLMs to generate reasoning details5:\nBased on the input and clues, articulate the\ndiagnostic reasoning process that supports the\nsentiment determination of the input.\nINPUT: <text>\nLABEL: <label-word>\nCLUES: <clues>\nREASONING:\nGiven the generated clues and reasonings\nfor all training examples, at test time, when\nK-nearest examples are selected demonstrations,\nits corresponding clues and reasons are\nconcatenated to the demonstration. In this\nway, each demonstration example is composed\nby a (text, clues, reasons, golden\nlabel word) pair. Examples for prompts with\nclues and reasons are shown in Figure 4. In this\nway, for a test example, by following the format of\ndemonstrations, the LLM will first output clues,\nthen reasons, and at last decisions.\n4.2 Voting\nUnlike conventional discriminative models for text\nclassification, which generate deterministic results\nduring inferences, LLMs for in-context learning\nare generative models and generate distinct textual\nresponses with diverse sampling strategies in\nmultiple runs. We consider the following voting\nstrategies in the paper:\n• Majority Vote: the final result is the most\nfrequent prediction among multiple runs.\n• Weighted Probability Vote: the final result\nis the one with weighted summed probability\nfrom multiple runs.\n5 Experiments\nIn order to evaluate the effectiveness of the\nproposed method, we conduct experiments on two\nsetups: (1) full training setup, where the model has\nthe access to the full training data; and (2) low-\nresource setup, where the model can only access\npartial training dataset. The low-resource setup\n5LLMs often generate long responses, in order to ensemble\nmore demonstrations in prompts, we use \"limit to 50 words\".\nAfter conducting an analysis of the generated responses, we\nfind that LLMs can explain the reason within limited words.\n8994\nSST-2 AGNews R8 R52 MR Average\nSupervised Methods\nRoBERTa-Large (Liu et al., 2019) 95.99 95.55 97.76 96.42 91.16 95.38DeBERTa (He et al., 2020) 94.75 95.32 98.33 96.32 90.19 94.99RoBERTa-GCN (Lin et al., 2021) 95.8095.68* 98.2 96.1 89.7 95.10XLNet (Yang et al., 2019)96.10*95.55 - - - -VLAWE (Ionescu and Butnaru, 2019) - - - - 93.3* -GCN-SB (Zeng et al., 2022) - - 98.53* 96.35*87.59 -\nZero-shot Setting\nVanilla (Brown et al., 2020) 91.55 90.72 90.19 89.06 88.69 90.04CoT (Kojima et al., 2022) 92.11 91.25 90.48 91.24 89.37 90.89CARP 93.01 92.60 91.75 91.80 89.94 91.82\nFew-shot Setting (k=16)\nRandom Sampler\nVanilla (Brown et al., 2020) 92.36 91.74 91.58 91.56 89.15 91.28CoT (Kojima et al., 2022) 94.56 95.02 92.49 92.03 89.91 92.80CARP 96.20 95.18 97.60 96.19 90.03 95.04\nSimCSEkNN-Sampler\nVanilla (Brown et al., 2020) 93.90 93.50 94.36 92.40 89.59 94.05CoT (Kojima et al., 2022) 94.21 94.28 95.07 92.98 90.27 93.69CARP 95.69 95.25 97.83 96.27 90.74 95.16\nFTkNN-Sampler\nVanilla (Brown et al., 2020) 94.01 94.14 95.57 95.79 90.90 94.08CoT (Kojima et al., 2022) 95.48 94.89 95.59 95.89 90.17 94.40CARP 96.80 95.99 98.29 96.82 91.90 95.97CARP(WP V ote) 97.39 96.40 98.78 96.95 92.39 96.38\nTable 1: Accuracy performances of different settings on\nbenchmarks. We report mean results over 5 runs. The\nGPT-3 denotes text-davinci-003. In few-shot\nexperiments, we sample 16 annotated examples (k=16)\nfor every test instance.* indicates existing SOTA results.\n\"WP V ote\" denotes weighted probability vote.\nbetter mimics real-world situations where training\ndata is limited. For the full training setup, we\nfollow the standard train/dev/test split. For the low-\nresource setup, we randomly sample ninstances\nper class ( n in {16,128,256,512,1024}) from\nthe benchmark training set. The sampled subset\nforms a new training set to test different models’\nabilities in the low-resource situations. During\nexperiments, we train models/demonstrations with\nthe new training set.\nWe conduct experiments on five widely-used\ndatasets, including SST-2 (Socher et al., 2013), R8,\nR526, AGNews (Zhang et al., 2015) and Movie\nReview (MR) (Pang and Lee, 2005). More details\nof the benchmarks and low-resource datasets can\nbe found in Appendix D.\nFor zero-shot and few-shot experiments,\nwe use InstructGPT-3 (Ouyang et al., 2022)\n(text-davinci-003, 175B) as the backbone.\nDue to the input token limitation, we use k = 16\nfor few-shot setups. Prompts on the five datasets\nare shown in Appendix H. Model hyper-parameters\ncan be found in Table 13 7. We use Vanilla to\ndenote the conventional ICL approach where\nLLMs are prompted to generate labels, use\nCoT (Kojima et al., 2022) to denote the baseline\n6R8 and R52 are original fromhttps://www.cs.umb.\nedu/~smimarog/textmining/datasets/\n7During experiments, we find that CARP is robust with\ndifferent hyper-parameters. Experimental results can be found\nin Appendix G.3\nDataset Model n=16 n=128 n=256 n=512 n=1024\nFT RoBERTa 51.52 52.31 53.89 70.49 90.30\nSST-2 GPT-3 Vanilla 90.15 90.36 91.70 93.86 94.68GPT-3 CoT 89.66 90.19 90.80 94.42 94.89GPT-3 CRAP 90.48 91.07 91.77 94.03 95.20\nAGNews\nFT RoBERTa 21.87 38.19 40.08 50.18 78.09GPT-3 Vanilla 89.47 89.63 90.54 93.02 94.79GPT-3 Zero-shot-CoT 89.66 90.16 91.70 94.86 95.28GPT-3 CRAP 90.16 90.94 91.07 94.08 95.48\nR8\nFT RoBERTa 11.29 48.19 60.18 70.70 88.68GPT-3 Vanilla 89.15 90.27 91.70 94.00 94.91GPT-3 CoT 90.49 90.88 91.81 95.42 95.75GPT-3 CRAP 90.23 91.03 91.77 95.56 96.67\nR52\nFT RoBERTa 38.29 39.10 59.18 67.19 81.53GPT-3 Vanilla 89.15 90.04 90.29 91.88 92.06GPT-3 CoT 89.46 90.02 90.73 93.20 94.12GPT-3 CRAP 90.82 91.00 95.85 94.36 96.27\nMR\nFT RoBERTa 51.20 52.11 53.58 68.29 88.37GPT-3 Vanilla 86.04 88.68 88.99 89.80 90.18GPT-3 CoT 86.26 89.00 90.01 90.16 90.89GPT-3 CRAP 86.54 87.19 89.63 90.01 91.20\nTable 2: Experimental results on low-resource ( n\nexample per class) settings. We compare fine-tuned\nRoBERTa-Large with16-shots GPT-3 setting. For GPT-\n3, we use SimCSE (Gao et al., 2021) to retrieve 16\nannotated examples from the low-resource train set.\nFT RoBERTa on FT RoBERTa onSST-2 Train Yelp Train\nSST-2 Test 95.99 88.78\nYelp Test 92.38 96.04\nCARP with CARP with\nSST-2 demon. Yelp demon.\nSST-2 Test 96.80 96.29Yelp Test 95.94 96.32\nTable 3: Results for Yelp test set when using\nin-domain/out-of-domain kNN sampler and\ndemonstrations source. We use FT kNN Sampler to\nretrieve demonstrations on the corresponding train set.\nthat mimics the chain-of-thought strategy and\nCARP to denote the proposed method.\n5.1 Models for Comparison\nSupervised models are trained on the trained\nset naturally constitute baselines to compare with.\nWe use the six models, including RoBERTa-Large,\nRoBERTa-GCN, DeBERTa, XLNet, GCN-SB,\nand VLAWE. Details of the models and hyper-\nparameters are shown in Appendix G.2:\nFew-shot Setup For demonstration sample\nstrategies in the few-shot setup, we consider the\nfollowing strategies for comparison: (more details\ncan be found in Section 3.3):\n• Random Sampler : randomly samples k\nexamples.\n• SimCSE kNN-Sampler: samples kNN based\non SimCSE.\n• FT kNN-Sampler: sample kNN using Fine-\nTuned RoBERTa representations.\n8995\n5.2 Results on the full training set\nExperimental results are shown in Table 1. As\ncan be seen, performances of few-shot setups\nconsistently outperform zero-shot setups. In terms\nof sampling strategies in the few-shot setups, we\nobserve that simcse KNN-sampler outperform\nrandom sampler, illustrating the importance of\nadding demonstrations that are relevant to the test\ninput in the few-shot setup. We also observe\nthat FT KNN-sampler consistently outperforms\nsimcse KNN-sampler. This shows that, the fine-\ntuned model, which takes the advantage of the full\ntraining set, serves as a better retriever for task-\nspecific demonstration retrieval than the general-\npurposed SimCSE retriever.\nFor different reasoning strategies, we first\nobserve that the CoT strategy outperforms the\nvanilla strategy, which straightforwardly asks\nLLMs to generate results without further reasoning\nsteps. CARP consistently outperforms CoT across\nall benchmarks, i.e., +1.48, +0.97, +2.76, +\n3.29, +0.47 respectively on SST-2, AGNews,\nR8, R52 and MR datasets. This demonstrates\nthe necessity of building models with complex\nlinguistic phenomena involved in text classification,\nand the effectiveness of CARP in doing this job.\nCompared with supervised learning baselines,\nwe find that the vanilla model using LLM\nunderperforms supervised baselines, while few-\nshot CoT is able to obtain slightly worse or\ncomparable results agains supervised baselines.\nNotably, single CARP outperforms fine-tuned\nRoBBERTa on all benchmarks. Using WP voting\nstrategies, CARP yields new SOTA performances\non four out of the 5 datasets, 97.39 on SST-2\n(+1.24), 96.40 (+0.72) on AGNews, 98.78 (+0.25)\non R8 and 96.95 (+0.6) on R52, and a performance\ncomparable to SOTA on MR (92.39 v.s. 93.3).\n5.3 Results on low-resource settings\nTo estimate low-resource circumstances, we\nsample n = {16,128,256,512,1024}instances\nfor each class as low-resource setups. Experimental\nresults are shown in Table 2. As can be seen,\nwhen the training set size is extremely small (i.e.,\n16 or 128 sentences), and the performance of the\nsupervised model is far below CARP. Even with\nonly 16 examples to train on, the accuracy of CARP\nof SST-2 already around 90%, whereas supervised\nmodels’ performance is similar to random guess.\nThis demonstrates the strong generalization ability\nof CARP in the low-resource setup. As we\nanticipated, the kNN search efficiency improved\nat a faster rate as the amount of the training\ndata increases; Enlarging the training dataset\nincreases the chances that the chosen examples\nwill correspond to the input, resulting in improved\nresults. Specifically, using 16 examples per\nclass, CARP achieves comparable performances to\nsupervised models with 1,024 examples per class;\nusing 512 instance per class annotation data, CARP\nachieves comparable performances to supervised\nmodels trained on the full set.\n5.4 Domain Adaptation\nIt is unclear whether training models on the specific\ndataset for retrieving demonstrations is essential.\nIn this subsection, we conduct an analysis on using\ndemonstrations from out-of-distribution datasets.\nWe use SST-2 and Yelp, and the task is\nto determine the positive or negative polarity\nof the given text. SST-2 and Yelp are from\ndifferent domains: SST-2 are snippets from Rotten\nTomatoes8, whereas Yelp consists of product\nreviews from the online website. Experimental\nresults are shown in Table 3. SST-2 train & SST-2\ntest means that demonstrations are from the SST-2\ndataset and test is performed on SST-2 dataset; Yelp\ntrain & SST-2 test means demonstrations are from\nYelp and test is performed on SST-2 dataset. We see\na significant decrease (-7.2%, 95.99% v.s.88.78% )\nin performance when switching SST-2 train to Yelp\ntrain using supervised RoBERTa, which illustrates\nthat supervised models are very sensitive to the\nout-of-distribution data. On the contrary, we only\nobserve a slight decrease in performance (-0.5%,\n96.80% v.s. 96.29%) when switching SST-2 train to\nYelp-2 train on SST-2 test, illustration the greater\ncapabilities of CARP on the domain adaptation\nsituations. This means CARP is very robust when\ntraining and test are not from the same domain.\n6 Ablation Studies\n6.1 Impact of the number of demonstrations\nWe explore the effect of the number of\ndemonstrations in prompts using SST-2 . Results\nfor CARP using different sampling strategies are\nshown in Figure 2. As can be seen, performances\nimprove as the number of demonstrations increases,\nwhich is in line with our expectation.\n8https://www.rottentomatoes.com/\n8996\n0 2 4 8 12 16 20 24\n92\n93\n94\n95\n96\n97\n98\nNumber of Demonstrations\nTest Accuracy (%) Random SamplerSimCSEkNN-SamplerFTkNN-Sampler\nFigure 2: Performances v.s. the number of\ndemonstrations for CARP.\nPrompts SST-2 R8\nCARP 96.80 98.29\nw/o Text 92.28 94.18\nw/o Clue 95.48 95.29\nw/o Reason 95.72 97.82\nw/o Label 96.53 98.18\nTable 4: The effect of components on the SST-2 dataset\nwith different strategies.\n6.2 The effect of components in\ndemonstrations\nCARP uses (text, clues, reasons,\ngolden label word) pairs as\ndemonstrations. In this subsection, we exploit the\ninfluence of each component in (text, clues,\nreasons, golden label word) by\nremoving it from prompts. Experimental results\nare shown in Table 4. As shown in Table 4, text in\ndemonstrations has the greatest impact, followed\nby clue, reason and label.\n6.3 The effect of different types of label words\nLabel words denote words generated by LLMs that\nindicate the label of the input. We explore the\nimpact of using different kinds of label words:\n• Position index: e.g., one, two, three, etc.\n• Annotation words: e.g., positive, negative. 9\n• Synonyms words: e.g., great, terrible.\n• Flipped words: words that are contrary to\noriginal target meanings. e.g., \"positive\" to\ndenote the negative polarity, \"negative\" to\ndenote the positive polarity.\n• Random words: randomly choose words in\nthe vocabulary.\n9GPT-3 generates the same label words for binary\nsentiment classification task.\nStrategy Label Words(+,-) CARP\nPosition Index One, Two 95.66\nAnnotation Words Positive, Negative 96.86\nSynonyms Words Great, Terrible 96.27\nFlipped Words Negative, Positive 64.63\nRandom Words Cf, Ng 95.06\nSpecial Tokens <POS>, <NEG> 96.65\nTable 5: Label words and results on the SST-2 dataset\nwith different strategies. \"+\" represents positive polarity;\n\"-\" denotes negative polarity.\nRanking SimCSE FT\nCARP\nRandom 95.39 95.99\nHigh-to-Low 95.22 96.71\nLow-to-High 96.39 96.80\nTable 6: Accuracy scores on SST-2 when assembling\ndemonstrations with different ranking strategies.\n• Special tokens : tokens that do not have\nsemantic meaning. They are independent of\nthe input and added for a certain purpose. e.g.,\n<cls>, <mask>.\nResults are shown in Table 5. As can be seen,\nusing annotation words as label words achieves the\nbest performances. We also observe a significant\nperformance decrease when flipped words are used\nas label words in demonstrations.\n6.4 The effect of demonstration order\nDuring experiments, we find that the ranking\norder of demonstration affect final results. In this\nsubsection, we further investigate the influence\nof orders of demonstrations. Orders the\ndemonstrations we investigate include:\n• Random: randomly shuffle retrieved\ndemonstrations.\n• Low-to-High: demonstrations with lower\nsimilarity scores come first. Therefore\ndemonstrations with higher similarity scores\nare placed closer to the test sequence, which\nis placed at the end of the prompt.\n• High-to-Low: demonstrations with lower\nsimilarity scores are placed closer to the test\nsequence.\nAs shown in Table 6, the performance is sensitive\nthe ordering of the demonstrations. The low-\nto-high ordering achieves the best performance\ncompared to the random and high-to-low ordering.\n8997\n7 Conclusion\nIn this paper, we introduce Clue And Reasoning\nPrompting (CARP) for text classification task.\nCARP yields new SOTA performances on 4 out\nof 5 widely-used text-classification benchmarks.\nMore importantly, we find that CARP delivers\nimpressive abilities on low-resource and domain-\nadaption setups. In the future, we would like\nto explore CARP on more natural language\nunderstanding tasks.\nAcknowledgements\nThis work is supported by the National Key R&D\nProgram of China (SQ2022AAA010214).\nLimitations\nDespite the overall promising results, CARP still\nfaces the following shortcomings: (1) clues that\nare contributing for making decisions are hard to\nannotate; (2) LLMs still suffer from the token limit\nissue.\nReferences\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large\nannotated corpus for learning natural language\ninference. In Proceedings of the 2015 Conference\non Empirical Methods in Natural Language\nProcessing (EMNLP). Association for Computational\nLinguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nDuo Chai, Wei Wu, Qinghong Han, Fei Wu, and Jiwei\nLi. 2020. Description based text classification with\nreinforcement learning. In International Conference\non Machine Learning. PMLR.\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mixtext:\nLinguistically-informed interpolation of hidden space\nfor semi-supervised text classification. arXiv preprint\narXiv:2004.12239.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\narXiv preprint arXiv:2003.10555.\nAlexis Conneau, Holger Schwenk, Loïc Barrault,\nand Yann Lecun. 2016. Very deep convolutional\nnetworks for text classification. arXiv preprint\narXiv:1606.01781.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime\nCarbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a fixed-length context. arXiv preprint\narXiv:1901.02860.\nRajarshi Das, Manzil Zaheer, Dung Thai, Ameya\nGodbole, Ethan Perez, Jay-Yoon Lee, Lizhen Tan,\nLazaros Polymenakos, and Andrew McCallum.\n2021. Case-based reasoning for natural language\nqueries over knowledge bases. arXiv preprint\narXiv:2104.08762.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training\nof deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,\nXiaocheng Feng, Ming Gong, Linjun Shou, Bing\nQin, Ting Liu, Daxin Jiang, et al. 2020. Codebert:\nA pre-trained model for programming and natural\nlanguages. arXiv preprint arXiv:2002.08155.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence\nembeddings. arXiv preprint arXiv:2104.08821.\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu,\nand Maosong Sun. 2021. Ptr: Prompt tuning\nwith rules for text classification. arXiv preprint\narXiv:2105.11259.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. ArXiv.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\narXiv preprint arXiv:1801.06146.\nRadu Tudor Ionescu and Andrei M Butnaru. 2019.\nVector of locally-aggregated word embeddings\n(vlawe): A novel document-level representation.\narXiv preprint arXiv:1902.08850.\nOzan Irsoy and Claire Cardie. 2014. Deep recursive\nneural networks for compositionality in language.\nAdvances in neural information processing systems,\n27.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2016. Bag of tricks for efficient text\nclassification. arXiv preprint arXiv:1607.01759.\n8998\nNitish Shirish Keskar, Bryan McCann, Lav R\nVarshney, Caiming Xiong, and Richard Socher.\n2019. Ctrl: A conditional transformer language\nmodel for controllable generation. arXiv preprint\narXiv:1909.05858.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Nearest\nneighbor machine translation. arXiv preprint\narXiv:2010.00710.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In Conference on Empirical\nMethods in Natural Language Processing.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid,\nYutaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. ArXiv.\nSiwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.\nRecurrent convolutional neural networks for text\nclassification. In Proceedings of the AAAI conference\non artificial intelligence.\nAndrew K Lampinen, Ishita Dasgupta, Stephanie CY\nChan, Kory Matthewson, Michael Henry Tessler,\nAntonia Creswell, James L McClelland, Jane X\nWang, and Felix Hill. 2022. Can language models\nlearn from explanations in context? arXiv preprint\narXiv:2204.02329.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190.\nYuxiao Lin, Yuxian Meng, Xiaofei Sun, Qinghong Han,\nKun Kuang, Jiwei Li, and Fei Wu. 2021. Bertgcn:\nTransductive text classification by combining gcn and\nbert. arXiv preprint arXiv:2105.05727.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nPengfei Liu, Xipeng Qiu, and Xuanjing Huang.\n2016. Recurrent neural network for text\nclassification with multi-task learning. arXiv preprint\narXiv:1605.05101.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692.\nAna Marasovi ´c, Iz Beltagy, Doug Downey, and\nMatthew E Peters. 2021. Few-shot self-\nrationalization with natural language prompts.\narXiv preprint arXiv:2111.08284.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\net al. 2022. Training language models to follow\ninstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. arXiv preprint cs/0506075.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying lms with mixtures of soft prompts.\narXiv preprint arXiv:2104.06599.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019a. Language\nmodels are unsupervised multitask learners.\nAlec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, Ilya Sutskever, et al.\n2019b. Language models are unsupervised multitask\nlearners. OpenAI blog.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah\nYoung, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-\ntext transformer. The Journal of Machine Learning\nResearch, 21(1):5485–5551.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\narXiv preprint arXiv:1908.10084.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2021. Learning to retrieve prompts for in-context\nlearning. arXiv preprint arXiv:2112.08633.\nTimo Schick and Hinrich Schütze. 2020. Exploiting\ncloze questions for few shot text classification\nand natural language inference. arXiv preprint\narXiv:2001.07676.\nYeon Seonwoo, Guoyin Wang, Sajal Choudhary,\nChangmin Seo, Jiwei Li, Xiang Li, Puyang Xu,\nSunghyun Park, and Alice Oh. 2022. Ranking-\nenhanced unsupervised sentence representation\nlearning. arXiv preprint arXiv:2209.04333.\nWeijia Shi, Julian Michael, Suchin Gururangan, and\nLuke Zettlemoyer. 2022. Nearest neighbor zero-shot\ninference. arXiv preprint arXiv:2205.13792.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empirical\n8999\nmethods in natural language processing, pages 1631–\n1642.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A Smith, et al. 2022.\nSelective annotation makes language models better\nfew-shot learners. arXiv preprint arXiv:2209.01975.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to fine-tune bert for text classification?\nIn Chinese Computational Linguistics: 18th China\nNational Conference, CCL 2019, Kunming, China,\nOctober 18–20, 2019, Proceedings 18. Springer.\nXiaofei Sun, Yuxian Meng, Xiang Ao, Fei Wu, Tianwei\nZhang, Jiwei Li, and Chun Fan. 2022. Sentence\nsimilarity based on contexts. Transactions of the\nAssociation for Computational Linguistics.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng,\nHao Tian, Hua Wu, and Haifeng Wang. 2020.\nErnie 2.0: A continual pre-training framework for\nlanguage understanding. In Proceedings of the AAAI\nconference on artificial intelligence, volume 34.\nZijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng,\nXiang Ao, Qing He, Fei Wu, and Jiwei Li. 2021.\nChinesebert: Chinese pretraining enhanced by\nglyph and pinyin information. arXiv preprint\narXiv:2106.16038.\nJian Tang, Meng Qu, and Qiaozhu Mei. 2015.\nPte: Predictive text embedding through large-scale\nheterogeneous text networks. In Proceedings of\nthe 21th ACM SIGKDD international conference on\nknowledge discovery and data mining, pages 1165–\n1174.\nZheng Tang and Mihai Surdeanu. 2023. It takes two\nflints to make a fire: Multitask learning of neural\nrelation and explanation classifiers. Computational\nLinguistics, 49(1):117–156.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\net al. 2022. Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-\nintensive multi-step questions. arXiv preprint\narXiv:2212.10509.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nZhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying\nLiu, Haiyue Song, Jiwei Li, and Sadao Kurohashi.\n2023. Gpt-re: In-context learning for relation\nextraction using large language models. arXiv\npreprint arXiv:2305.02105.\nGuoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe\nZhang, Dinghan Shen, Xinyuan Zhang, Ricardo\nHenao, and Lawrence Carin. 2018. Joint embedding\nof words and labels for text classification. arXiv\npreprint arXiv:1805.04174.\nShuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang,\nFei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang.\n2023. Gpt-ner: Named entity recognition via large\nlanguage models. arXiv preprint arXiv:2304.10428.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nJason Wei and Kai Zou. 2019. Eda: Easy data\naugmentation techniques for boosting performance\non text classification tasks. arXiv preprint\narXiv:1901.11196.\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,\nMark Riedl, and Yejin Choi. 2021. Reframing\nhuman-ai collaboration for generating free-text\nexplanations. arXiv preprint arXiv:2112.08674.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and\nQuoc Le. 2020. Unsupervised data augmentation for\nconsistency training. Advances in neural information\nprocessing systems.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2020. mt5: A massively multilingual\npre-trained text-to-text transformer. arXiv preprint\narXiv:2010.11934.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime\nCarbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretraining\nfor language understanding. Advances in neural\ninformation processing systems, 32.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong\nHe, Alex Smola, and Eduard Hovy. 2016.\nHierarchical attention networks for document\nclassification. In Proceedings of the 2016 conference\nof the North American chapter of the association\nfor computational linguistics: human language\ntechnologies, pages 1480–1489.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\nGraph convolutional networks for text classification.\nIn Proceedings of the AAAI conference on artificial\nintelligence, volume 33, pages 7370–7377.\nXi Ye and Greg Durrett. 2022. The unreliability\nof explanations in few-shot prompting for textual\nreasoning. Advances in neural information\nprocessing systems.\nFang Zeng, Niannian Chen, Dan Yang, and Zhigang\nMeng. 2022. Simplified-boosting ensemble\nconvolutional network for text classification. Neural\nProcess. Lett., 54(6).\n9000\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022a. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text\nclassification. Advances in neural information\nprocessing systems, 28.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022b. Automatic chain of thought\nprompting in large language models. arXiv preprint\narXiv:2210.03493.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [mask]: Learning vs. learning to\nrecall. arXiv preprint arXiv:2104.05240.\nA Prompts\nIn Figure 3, we present examples of vanilla,\nCoT (Kojima et al., 2022) and the proposedCARP\nprompts in the zero-shot setting.\nIn Figure 4, we present examples of vanilla,\nCoT (Kojima et al., 2022) and the proposedCARP\nprompts in the few-shot (k=1) setting.\nB Experimental results\nIn Table 7, we present the experimental results on\ntext classification subsets.\nC More ablation studies\nC.1 The influence of clues\nCclues includare keywords, phrases, contextual\ninformation, semantic meaning, semantic\nrelationships, tones, references that support\nmaking decisions. We remove different types\nof words in clues and evaluate its influence\non SST-2 and R8 datasets. Editing prompts\nachieve this goal. The original prompt for\nclue collecting is List CLUES (i.e., keywords,\nphrases, contextual information, semantic meaning,\nsemantic relationships, tones, references) that\nsupport the sentiment determination of the input.\nIf we want to remove keywords & phrases, we just\nremove them from the prompt.\n• w/o keywords & phrases : keywords and\nphrases are surface evidence for making\ndecisions such as \"like\", \"hate\".\n• w/o contextual information & semantic\nmeaning: contextual information and\nsemantic meaning are meaning in\nsentences/paragraphs such as The author\nexpress his happiness.\n• w/o semantic relationships : semantic\nrelationships refer to relations between\nsubjects such as \"emotional danger\" suggests\na romantic and thrilling relationship between\nIdemoto and Kim that creates a positive\nsentiment..\n• w/o tones: tones are the general mood of the\ntext such as The sentence is expressed in an\nobjective tone.\n• w/o references: references are mentions of\ncommonsense facts or books such as The\nreference to the popular, comedic character\n\"Ferris Bueller\" implies that the kid is seen in\na positive light..\nExperimental results are shown in Table 8. For\nR8 and SST-2 datasets, keywords play the key role\nfor GPT predictions.\nC.2 Quality of the reasoning process\nIn this paper, we use LLMs to generate rationable\nexplanations instead of human editing. Therefore,\nthe quality of generated reasoning process affects\nthe final results. In this subsection, we sample\n500 training (text, clues, reason, label) pairs and\nevaluate the generated reasoning process from the\nfollowing perspectives:\n(1) Reliability: Inspired by the emergent\ngeneralization ability of LLMs, we use zero-shot\nGPT-3 (175B) as the self-critique model to evaluate\nthe quality of generated reasoning processes. To\nbe specific, we ask the GPT-3 to return yes/no if\nthe generated reasoning process supports making\ndecisions for the input text. If the GPT-3 returns\n\"yes\", it denotes that the reasoning process is\nreliable for making decisions. If the GPT-3 returns\n\"no\", it represents that the reasoning process is not\nreliable.\nThe prompt for SST-2 is shown as follows:\nIs the following REASONING process supporting\ndeterminate sentiment label to INPUT? Please\nanswer Yes or No.\nINPUT: <text>\nREASONING: <reasoning-process>\nwhere <text> is the text sequence for the data\nand <reasoning-process> is generated reasoning\nprocess.\n(2) Fluency: use LLMs to generate reasoning\nexplanations is a reference-free text generation task.\nWe use perplexity to evaluate the generated text.\n(3) Logic Faithful: previous work often use\nmodels, which are trained on natural language\n9001\nThis is an overall sentiment classifier for movie reviews.First, list CLUES (i.e., keywords, phrases, contextual information, semantic relations, semantic meaning, tones, references) that support the sentiment determination of input..Second, deduce the diagnostic REASONING process from premises (i.e., clues, input) that supports the INPUT sentiment determination (Limit the number of words to 130).Third, based on clues, reasoning and input, determine the overall SENTIMENT of INPUT as Positive or Negative.INPUT: press the delete keyCLUES: delete keyREASONING: The phrase \"delete key\" implies an action of removing something, which could be interpreted as a negative sentiment.SENTIMENT: Negative\nThis is an overall sentiment classifier for movie reviews. Classify the overall SENTIMENT of the INPUT as Positive or Negative. INPUT: press the delete keySENTIMENT: Let's think step-by-step. The input does not contain any words that would indicate a sentiment, so it is not possible to classify the sentiment as either positive or negative.\nThis is an overall sentiment classifier for movie reviews. Classify the overall SENTIMENT of the INPUT as Positive or Negative.INPUT: press the delete keySENTIMENT: Neutral\n(b) CoTZero-shot \n(c) CARP Zero-shot\n(a) Vanilla Zero-shot\nFigure 3: Examples of zero-shot prompting methods for the text classification task: (a) represents for the vanilla\nprompting method; (b) denotes for the Chain-of-Thought (CoT) (Kojima et al., 2022) prompting method; c\nrepresents for the proposed CARP prompting method.\ninference datasets, to determine whether the given\n“hypothesis” logically follows from the “premise”.\nHowever, lacking annotation datasets, NLI-trained\nmodels can not generalize across multiple domains\n(e.g., opinion, reviews, news). Since then, we use\n16-shot ICL with GPT-3 to evaluate whether the\ngenerated rationable explanations can be entailed\nfrom the input text. If the InstructGPT responds\nwith \"entailment\", it denotes that the generated\nreasoning process is logic faithful with the text.\nOtherwise, it represents the reasoning process is not\nfaithful to the text. We sample training instances\nfrom the SNLI dataset (Bowman et al., 2015)\nas demonstrations. And prompts are shown as\nfollows:\nGiven the premise and hypothesis, please justify\nwhether the HYPOTHESIS can be entailed from\nthe PREMISE. Please return yes or no.\nPREMISE: <text>\nHYPOTHESIS: <reasoning-process>\nEvaluation results are shown in Table 9. As\ncan be seen, the reliability percentages for SST-\n2 and R5 are higher than 95%. This indicates\nthat it is feasible to use the model-generated\nreasoning process as part of the prompts to augment\nICL performances. The perplexity of generated\nreasoning text is smaller than 4, which denotes that\nthe generated reasoning text is fluent. And scores\nof logic faithful are larger than 93%, which is in\nline with our expectation that LLMs can generate\nreasonable explanations.\nD Dataset\nSST-2 (Socher et al., 2013), R8, R52 10,\nAGNews (Zhang et al., 2015) and MR (Movie\nReview) (Pang and Lee, 2005).\n• SST-2: The original data in SST-2 are sampled\nfrom snippets of Rotten Tomatoes HTML files.\nWe use the same train/dev/test splits with\nSocher et al. (2013).\n• R8 and R52 : R8 and R5211 are two\nsubsections of the Reuters collection,\ncontaining 8 and 52 classifications,\nrespectively. The R8 dataset is composed\nof 5,485 documents for training and 2,189\ndocuments for testing. The R52 dataset is\ncomposed of 6,532 training and 2,568 test\ndocuments.\n• AGNews: The AG News consists of news\narticles from the AG’s corpus. The dataset\ncontains 30,000 training and 1,900 testing\nexamples for each class.\n• MR (Movie Review) : The MR contains\nreviews of films for determining whether a\nsentiment is either positive or negative. The\ncorpus has 10,662 reviews. We follow (Tang\net al., 2015) and use the same train/test split.\n9002\nThis is an overall sentiment classifier for movie reviews.First, present CLUES (i.e., keywords, phrases, contextual information, semantic relations, semantic meaning, tones, references) that support the sentiment determination of input.Second, deduce a diagnostic REASONING process from premises (i.e., clues, input) that supports the sentiment determination (Limit the number of words to 130).Third, determine theoverall SENTIMENT of INPUT as Positive or Negative considering CLUES, the REASONING process and the INPUT.INPUT: noycefilms it more as a shocking history lesson than as drama.CLUES: -Positive: \"more,\" \"shocking,\" \"history lesson,\" \"drama.\" -Negative: None.REASONING: The input contains positive words such as \"more,\" \"shocking,\" and \"drama,\" which suggest that the sentiment is positive. Additionally, the phrase \"history lesson\" implies that the film is educational and informative, which is generally seen as a positive thing. Therefore, the sentiment of the input is positive.SENTIMENT: PositiveINPUT: press the delete keyCLUES: -Positive: None. -Negative: \"delete key.\"REASONING: The input contains the phrase \"delete key,\" which implies that something is being removed or erased. This is generally seen as a negative action, suggesting that the sentiment of the input is negative.SENTIMENT: Negative\n (c) CARP One-shot\nThis is an overall sentiment classifier for movie reviews. Classify the overall SENTIMENT of the INPUT as Positive or Negative. INPUT: noycefilms it more as a shocking history lesson than as drama.SENTIMENT: Let's think step-by-step. The input states that the films are \"more\" something than something else. This implies that the films are at least somewhat of the first thing mentioned. In this case, the films are more of a \"shocking history lesson\" than \"drama\". This implies that the sentiment is positive, as the films are more of a history lesson than drama. The overall sentiment is Positive.INPUT: press the delete keySENTIMENT: Let's think step-by-step. The input does not provide any context or opinion, so it is impossible to determine the sentiment. Therefore, the overall sentiment is Neutral.\nThis is an overall sentiment classifier for movie reviews. Classify the overall SENTIMENT of the INPUT as Positive or Negative. INPUT: noycefilms it more as a shocking history lesson than as drama.SENTIMENT: PositiveINPUT: press the delete keySENTIMENT: Neutral\n (a) Vanilla One-shot\n(b) CoTOne-shot\nFigure 4: Examples of few-shot ( k=1) prompting methods for the text classification task: (a) represents for the\nvanilla prompting method; (b) denotes for the Chain-of-Thought (CoT) (Kojima et al., 2022) prompting method;\n(c) represents for the proposed CARP prompting method.\nE Case Analysis\nWe show generated clues, reasoning explanations\nexamples in Table 12.\nF Baselines\nAs mentioned in Section 5.1, we use the following\nsupervised models as baselines. More details of the\nmodels are as follows:\n• RoBERTa-Large:We fine-tune RoBERTa-\nLarge (Liu et al., 2019) on the training set.\n• RoBERTa-GCN:Lin et al. (2021) constructs\nheterogeneous graph networks on top of the\nRoBERTa-Large (Liu et al., 2019) model.\n• DeBERTa:He et al. (2020) improve RoBERTa\nby using disentangled attention mechanism\nand an enhanced mask decoder.\n• XLNet:Yang et al. (2019) propose a\ngeneralized autoregressive pretraining\nmethod that enables learning bidirectional\ncontexts.\n10R8 and R52 are from https://www.cs.umb.edu/\n~smimarog/textmining/datasets/\n• GCN-SB:Zeng et al. (2022) propose a\nsimplified boosting algorithm, which makes\nCNN learn the samples misclassified by GCN\nagain.\n• VLA WE:Ionescu and Butnaru (2019) obtain\ndocument embeddings based on aggregating\nthe differences between each codeword vector\nand each word vector (from the document)\nassociated to the respective codeword.\nG Hyper-parameters\nG.1 GPT API Hyper-parameters\nHyper-parameters for GPT-3 are shown in Table 13.\nG.2 Fine-tuning Hyper-parameters\nWe fine-tune RoBERTa and RoBERT-GCN on\n4 NVIDIA 3090 GPUs with FP16. Model\nhyper-parameters are tuned on the validation set,\nwhere learning rate {2e-5, 3e-5, 4e-5}, batch size\n{16,32,32}, a dropout rate of 0.3, a weight decay\nof 0.01, a warmup proportion of 0.01.\n9003\nSST-2 AGNews R8 R52 MR Average\nSupervised Methods\nRoBERTa-Large 95.99 95.55 97.76 96.42 91.16 95.38\nRoBERTa-GCN 95.80 95.68 98.2 96.1 89.7 95.10\nZero-shot Setting\nVanilla 91.55 90.72 90.19 89.06 88.69 90.04\nZero-shot-CoT 92.11 91.25 90.48 91.24 89.37 90.89\nCARP 94.41 93.18 93.29 92.69 90.03 92.72\nFew-shot Setting\nRandom Sampler\nVanilla 91.36 91.48 90.60 90.68 89.15 90.65\nZero-shot-CoT 92.56 92.65 92.49 92.03 89.91 91.93\nCARP 94.41 93.18 93.29 92.69 90.03 92.72\nSimCSEkNN-Sampler\nVanilla 93.90 93.50 94.36 92.40 89.59 92.75\nZero-shot-CoT 94.21 94.28 95.07 92.98 90.27 93.36\nCARP 95.99 95.53 95.31 93.84 90.64 94.26\nFT kNN-Sampler\nVanilla 94.01 94.14 95.57 95.79 90.90 94.08\nZero-shot-CoT 95.48 94.89 95.59 95.89 90.17 94.40\nCARP 96.62 95.97 98.13 96.12 91.86 95.74\nTable 7: Accuracy performances of different settings on test subsets (results are over 5 runs). GPT-3 denotes\ntext-davinci-003. In few-shot experiments, we sample 16 annotated examples (k=16) per prompt. \"MJ V ote\"\nis short for majority vote. \"WP V ote\" denotes weighted probability vote.\nPrompts SST-2 R8\nClues 96.80 98.29\nw/o keyword&phrase 96.21 96.91\nw/o contextual info. 96.23 97.10\nw/o semantic relations 96.30 97.38\nw/o tones 96.40 97.35\nw/o reference 96.50 97.19\nTable 8: Label words and results on the SST-2 dataset\nwith different strategies.\nReliability(%)↑ Fluency(ppl)↓ Logic Faithful(%)↑\nSST-2 96.18 3.89 95.20R8 95.34 3.29 94.55\nTable 9: Results for evaluating the quality of generated\nreasoning explanation. We sample 500 (text, reason)\ninstances for SST-2 and R8.\nG.3 The influence of hyper-parameters\nWe investigate the effect of model hyper-\nparameters including temperature, frequency\npenalty. We conduct experiments with Instruct-\nGPT3 on the SST-2 dataset.\nTemperature The temperature τ controls the\ngenerated text variety when another hyper-\nparameter topp=1. More higher τ, more variety\nis introduced. When τ is close to 0, the model\ngenerates the same result with the greedy decoding\nDataset Task # Label Source # Train # Dev # Test\nSST-2 sentiment 2 review 6,920 872 1,821AGNews topic 4 news 96,000 24,000 7,600R8 topic 8 news 4,941 544 2,189R52 topic 52 news 5,905 627 2,568MR sentiment 2 reviews 6,398 710 3,554\nTable 10: Benchmark Dataset\nDataset Task # Label Source # Train # Dev # Subtest\nSST-2 sentiment 2 review 6,920 872 728AGNews topic 4 news 96,000 24,000 760R8 topic 8 news 4,941 544 875R52 topic 52 news 5,905 627 1,027MR sentiment 2 reviews 6,398 710 888\nTable 11: Dataset Subsets\nmethod. To exploit the effect of temperature τ,\nwe set τ from 0 to 1.0. Experimental results are\nshown in Table G.3. We tokenize the response text\nwith GPT-Tokenizer11 and then count the number\nof tokens.\nτ SST-2 Accuracy\nτ = 0.0 96.39\nτ = 0.2 96.48\nτ = 0.4 96.40\nτ = 0.6 96.59\nτ = 0.8 96.68\nτ = 1.0 96.70\nH Prompts\n11https://platform.openai.com/tokenizer\n9004\nINPUT & GOLD LABEL CLUES REASONING\nINPUT: johnnie to and wai ka\nfai are sure to find an enthusiastic\naudience among american action\nadventure buffs, but the film ’s\ninterests may be too narrow to\nattract crossover viewers\nGOLD LABEL: Negative\n- Positive Clues: enthusiastic, action,\nadventure.\n- Negative Clues: narrow, crossover\nThe diagnostic reasoning process supporting the\nsentiment determination of the input is that the phrase\n\"johnnie to and wai ka fai are sure to find an enthusiastic\naudience among american action adventure buffs\"\nsuggests that the film is likely to be well-received by\nthis specific demographic. However, the subsequent\nphrase \"but the film’s interests may be too narrow to\nattract crossover viewers\" implies that the film is unlikely\nto appeal to a broader audience, suggesting a negative\nsentiment. Therefore, the overall sentiment of the input\nis negative.\nINPUT: moore sell globe unit\nmoore resources inc said agreed\nprinciple sell globe inc unit cyprus\nminerals co undisclosed terms\nsale expected close mid april\nsubject certain conditions including\nboard approval companies globe\nproduces silicon metals products\nused chemical aluminum iron rubber\nsemiconductor industries reuter\nGOLD LABEL: Acquisitions\n- Acquisition (agreed, sell, unit, sale,\nclose, board approval)\n- Companies (Moore Resources Inc,\nGlobe Inc, Cyprus Minerals Co)\n- Products (silicon, metals, chemical,\naluminum, iron, rubber, semiconductor)\n- Reuter (news agency)\nThe input mentions an acquisition agreement between\nMoore Resources Inc and Globe Inc, and the sale\nis expected to close in mid-April, suggesting an\nAcquisitions topic. The input also mentions Cyprus\nMinerals Co, silicon and metals products which are used\nin chemical, aluminum, iron, rubber, and semiconductor\nindustries, and a Reuter news agency, all of which\nsupport the Acquisitions topic.\nTable 12: Examples of generated clues and reasoning for demonstrations.\nParameter Value\nEngine Name text-davinci-003\nMax Tokens 200\nTemperature 0.7\nTop P 1\nFrequency Penalty 0.0\nPresence Penalty 0.0\nBest Of 1\nTable 13: OpenAI API Hyper-parameters.\n9005",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6782228946685791
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6636989116668701
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6458636522293091
    },
    {
      "name": "Task (project management)",
      "score": 0.5912598371505737
    },
    {
      "name": "Natural language processing",
      "score": 0.5154198408126831
    },
    {
      "name": "Class (philosophy)",
      "score": 0.4932536482810974
    },
    {
      "name": "Generalization",
      "score": 0.48784899711608887
    },
    {
      "name": "Machine learning",
      "score": 0.4571816921234131
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4449729025363922
    },
    {
      "name": "Carp",
      "score": 0.4425666332244873
    },
    {
      "name": "Biology",
      "score": 0.10894334316253662
    },
    {
      "name": "Fishery",
      "score": 0.1025877296924591
    },
    {
      "name": "Fish <Actinopterygii>",
      "score": 0.09372851252555847
    },
    {
      "name": "Mathematics",
      "score": 0.08380565047264099
    },
    {
      "name": "Engineering",
      "score": 0.06863722205162048
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I158842170",
      "name": "Chongqing University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    }
  ]
}