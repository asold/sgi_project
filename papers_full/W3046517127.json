{
  "title": "Creative Culinary Recipe Generation Based on Statistical Language Models",
  "url": "https://openalex.org/W3046517127",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2558843408",
      "name": "Willian Antonio dos Santos",
      "affiliations": [
        "Pontifícia Universidade Católica de Minas Gerais"
      ]
    },
    {
      "id": "https://openalex.org/A3046798466",
      "name": "Joao Ribeiro Bezerra",
      "affiliations": [
        "Pontifícia Universidade Católica de Minas Gerais"
      ]
    },
    {
      "id": "https://openalex.org/A2630979151",
      "name": "Luis Fabricio Wanderley Goes",
      "affiliations": [
        "Pontifícia Universidade Católica de Minas Gerais"
      ]
    },
    {
      "id": "https://openalex.org/A2231393929",
      "name": "FLAVIA MAGALHAES FREITAS FERREIRA",
      "affiliations": [
        "Pontifícia Universidade Católica de Minas Gerais"
      ]
    },
    {
      "id": "https://openalex.org/A2558843408",
      "name": "Willian Antonio dos Santos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3046798466",
      "name": "Joao Ribeiro Bezerra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2630979151",
      "name": "Luis Fabricio Wanderley Goes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231393929",
      "name": "FLAVIA MAGALHAES FREITAS FERREIRA",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2604635588",
    "https://openalex.org/W1988499632",
    "https://openalex.org/W2019515298",
    "https://openalex.org/W2087220222",
    "https://openalex.org/W4298454635",
    "https://openalex.org/W6713750687",
    "https://openalex.org/W6713744151",
    "https://openalex.org/W6714019678",
    "https://openalex.org/W6640108777",
    "https://openalex.org/W6731242951",
    "https://openalex.org/W13968475",
    "https://openalex.org/W2082396928",
    "https://openalex.org/W2013326389",
    "https://openalex.org/W4214564745",
    "https://openalex.org/W6680934327",
    "https://openalex.org/W2087387194",
    "https://openalex.org/W7053576697",
    "https://openalex.org/W2057568625",
    "https://openalex.org/W2126626732",
    "https://openalex.org/W2081301924",
    "https://openalex.org/W2979392537",
    "https://openalex.org/W2404890635",
    "https://openalex.org/W1916044692",
    "https://openalex.org/W2563358120",
    "https://openalex.org/W2408727897",
    "https://openalex.org/W2405161148",
    "https://openalex.org/W2141656083",
    "https://openalex.org/W1578856370"
  ],
  "abstract": "Many works have been done in an effort to create systems for automatic generation of creative culinary recipes. Although most of them are related to the recipe ingredient lists, few works have been done to evaluate and generate the preparation steps of culinary recipes. This work proposes the use of statistical Language Models, as well as the perplexity metric, for the generation of culinary recipes. In this work, we also developed a system for automatic generation of creative culinary recipes using two approaches: one based on a genetic programming algorithm guided by the proposed language model; and the other based on a decomposition of existing recipes and recomposition of new recipes through a genetic algorithm guided by the proposed language model. This second approach achieved the best results. For this approach, a total of 6 recipes were generated to evaluate, through an online survey, the influence of the Language Model in the generation of recipes with better use of secondary ingredients, oils and seasonings, throughout the preparation steps. In the comparison between these two groups of recipes, the respondents considered the recipes generated using the language model as having the best quality, presenting an average evaluation of 63.6% of the scale (i.e. between medium and good use of oils and seasonings compared to recipes from the other group). In addition, a recipe from this approach was cooked and tasted for taste assessment, obtaining an average evaluation of 93% of the scale.",
  "full_text": "Received June 14, 2020, accepted July 10, 2020, date of publication July 31, 2020, date of current version August 20, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3013436\nCreative Culinary Recipe Generation Based\non Statistical Language Models\nWILLIAN ANTÔNIO DOS SANTOS1, (Member, IEEE), JOÃO RIBEIRO BEZERRA2, (Member, IEEE),\nLUÍS FABRÍCIO WANDERLEY GÓES\n2, (Member, IEEE),\nAND FLÁVIA MAGALHÃES FREITAS FERREIRA1\n1Electrical Engineering Departament, Pontical Catholic University of Minas Gerais (PUC Minas), Belo Horizonte 30535-060, Brazil\n2Computer Science Department, Pontical Catholic University of Minas Gerais (PUC Minas), Belo Horizonte 30535-060, Brazil\nCorresponding authors: Willian Antônio dos Santos (willian.antonio.bh@gmail.com), João Ribeiro Bezerra (joaorb64@gmail.com),\nLuís Fabrício Wanderley Góes (lfwgoes@pucminas.br), and Flávia Magalhães Freitas Ferreira (ﬂaviamagfreitas@pucminas.br)\nThis work was supported in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) under Grant 001,\nin part by Conselho Nacional de Desenvolvimento Cientíﬁco e Tecnológico (CNPq), and in part by Fundação de Amparo à Pesquisa do\nEstado de Minas Gerais (FAPEMIG).\nABSTRACT Many works have been done in an effort to create systems for automatic generation of creative\nculinary recipes. Although most of them are related to the recipe ingredient lists, few works have been done\nto evaluate and generate the preparation steps of culinary recipes. This work proposes the use of statistical\nLanguage Models, as well as the perplexity metric, for the generation of culinary recipes. In this work,\nwe also developed a system for automatic generation of creative culinary recipes using two approaches: one\nbased on a genetic programming algorithm guided by the proposed language model; and the other based on\na decomposition of existing recipes and recomposition of new recipes through a genetic algorithm guided\nby the proposed language model. This second approach achieved the best results. For this approach, a total\nof 6 recipes were generated to evaluate, through an online survey, the inﬂuence of the Language Model\nin the generation of recipes with better use of secondary ingredients, oils and seasonings, throughout the\npreparation steps. In the comparison between these two groups of recipes, the respondents considered the\nrecipes generated using the language model as having the best quality, presenting an average evaluation\nof 63.6% of the scale (i.e. between medium and good use of oils and seasonings compared to recipes from the\nother group). In addition, a recipe from this approach was cooked and tasted for taste assessment, obtaining\nan average evaluation of 93% of the scale.\nINDEX TERMS Language models, culinary recipe, computational creativity.\nI. INTRODUCTION\nComputational Creativity is a relatively new ﬁeld of Arti-\nﬁcial Intelligence (AI) [1], consisting of the creation of\nideas or artifacts that are considered novel and useful within\na given context for a group of people [2], [3]. There has\nbeen interest in applying Computational Creativity to the\ngeneration of culinary recipes. The process of creating a\nculinary recipe is two-fold: i) to deﬁne the steps that ingre-\ndientes undergoe in order to prepare a dish; and ii) the list of\ningredients. However, due to its complexity, particularly in\nidentifying the preparation steps, much of the work involving\nComputational Creativity explored mostly the creation of\ningredient combinations [1], [4], [5].\nAlthough the preparation steps are as important as the\nselection of the ingredients, there is a certain assumption that\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Sabah Mohammed\n.\nthe ﬂavor derives mainly from the ingredients that compose a\ndish. In fact, the taste sensation is multi-sensory and depends\non the chemical composition of the selected ingredients,\nbesides smell and taste [6], [7]. However, some actions in\nthe preparation instructions have physical (e.g. to smash,\nbeat and freeze) or chemical (e.g. to ferment, clarify and\nsmoke) transformations that may affect the taste experience\nof these selected ingredients. In addition, the taste sensation\ncan be affected by effects such as: visual, tactile, thermal and\nsonorous (e.g. potato chips, crisp and pretzel) [8]. In turn,\nthese effects can be also altered by the preparation actions, for\nexample, the dish’s visual with decoration actions or changes\nin the dish’s temperature with actions like cooking, boil-\ning or freezing.\nIn essence, the selection of ingredients even by humans is\nnot solely based on taste. In addition to circumstantial criteria,\nsuch as a regional abundance of particular ingredients, they\nmay also be chosen for the purpose of mechanical stability of\nVOLUME 8, 2020 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 146263\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nthe dish (e.g. eggs) [6], which is only achieved due to actions\nof physical transformations (e.g. to beat). Thus the value\n(ﬂavour) of a dish consists not only of the food chemistry\nbut also of how the actions of the preparation steps modify\nthe ingredients.\nDue to the great competitiveness of the market, innovation\nis a necessity. And creativity is considered a factor of disrup-\ntive innovation. With regard to the importance and impact of\ncreativity in the culinary domain, we can cite the example of\nThe Fat Duck1 restaurant, 3 Michelin-starred by the renowned\nEnglish chef Heston Blumenthal, which features innovative\ndishes based on scientiﬁc research on the inﬂuence of sound\nin taste perception [8]. The interest for creative and innovative\ndishes in high gastronomy has existed for at least a century.\nIn this sense, the system of this work can contribute by\nsupporting the creativity of chefs and restaurants (helping and\nworking together) to generate creative recipes that go beyond\njust the combination of ingredients.\nThis work proposes the use of language models in the\nevaluation of structural patterns in the steps of the preparation\nof culinary recipes, in order to help generate preparation steps\nwith a structural consistency similar to the recipes used in\nthe training. The main objective of the work was to develop\na system for generating complete culinary recipes (with the\nactions of the preparation instructions) from a list of creative\ningredients generated through the system provided in [5].\nTo achieve this main objective, two approaches were taken:\n(i) generation of the recipes using genetic programming and\nlanguage models, named ReProg (Recipe Programming);\n(ii) generation of the recipes using a process of recipe decom-\nposition of a training database, clustering of these recipes\nparts, and recomposition of new recipes through genetic algo-\nrithm and language models, named ReComp (Recipe Com-\nposition). The ReProg approach is less restricted in relation\nto the number of combinations of possible recipes, but the\nReComp approach is more robust in relation to the consis-\ntency of the processing of each ingredient.\nFor the implementation of the system it was also necessary\nto create a database of structured human recipes; and a NLP\nparser (Natural Language Processing) was also proposed to\ncarry out the structuring of these recipes (i.e. responsible for\nconverting the natural language text of the recipe preparation\nsteps into a data structure that the system can consume).\nThis work is organized as follows: Section 2 presents\nthe background on Computational Creativity and statistical\nlanguage modeling. Section 3 presents the related work.\nSection 4 presents the architecture of the proposed sys-\ntem. Section 5 presents the details of the implementation\nof both the developed model and system (in two proposed\napproaches). Section 6 details the performed experiments.\nSection 7 presents the experiments’ results. Section 8 presents\nthe conclusions. Section 9 proposes future work.\n1The Fat Duck is considered one of the best restaurants in the world. Its\nwebsite can be accessed at: http://www.thefatduck.co.uk/\nII. BACKGROUND\nComputational Creativity (CC) is a ﬁeld of Artiﬁcial Intel-\nligence (AI) that consists of replicating creative behavior in\ncomputational systems [1]. There are two approaches [9]: one\nfocused on the creative process, in which one tries to recreate\nthe human process that leads to creative behavior; while the\nother approach is focused on the creative artifact, in which\none tries to generate artifacts that are considered creative by\nhumans. The focus of this work is on the artifact. Two factors\nlead a group of individuals from a speciﬁc application context\nto judge an artifact as creative [2], [10]:\na) Novelty, how an artifact differs from artifacts known to\nthe individual.\nb) Value, the utility of an artifact in the application context.\nHighly creative artifacts are also associated with a third\nfactor [9]:\nc) Surprise, (which presupposes novelty) the distance\nbetween the actual artifact and the expectation from\nindividuals of a group in the application context.\nSurprise is interesting for the evaluation of artifacts\nbecause, in addition to presupposing novelty, it is used as\na guide in the exploration of unknown environments in the\nﬁeld of Autonomous Agents in AI [11]. This makes it a great\nmeta-heuristic to explore the space of artifacts in the search\nfor creative artifacts, which are unknown.\nA commonly used metric for suprise is the Bayesian sur-\nprise, based on the difference in the level of the a posteriori\ninformation (after observing the event) in relation to the a pri-\nori information (before the observation of the event) [12].\nA. COMPUTATIONAL CREATIVITY IN CULINARY\nA culinary recipe consists of a list of components (the ingre-\ndients) and a sequence of steps to be applied on these com-\nponents to generate a dish. The goal of a computational\ncreativity system in the culinary domain is to generate a recipe\nso that the resulting dish is considered creative by a group of\nindividuals.\nThe value of the ingredients list is usually calculated based\non a hypothesis called food pairing [6]. However, there is no\nwell-deﬁned value metric to evaluate the preparation steps.\nThe API used for the generation of ingredients list of\nthe present work was based on the food pairing hypothesis.\nIt states that ingredients that share more chemical compounds\nhave more synergy, which translates into tastier and more\naromatic combinations [6]. In [6] this hypothesis has been\ndemonstrated as a very strong criterion in the West, mainly in\nNorthern European and North American cuisines, although\nthe taste sensation involves more senses than the aroma and\ntaste themselves. Texture, consistency, antimicrobial proper-\nties [13], nutritional value, regional abundance and cultural\nthat can also affect the choice of ingredients along with\nﬂavor [6]. However, the hypothesis of food pairing is treated\nas a criterion of taste preference due to the chemistry of the\ningredients, with compounds related to the aroma and taste.\nSome authors distinguish between taste as a human sense\n146264 VOLUME 8, 2020\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nand ﬂavor experience, which is multi-sensory. Ingredients\ndo not contain the ﬂavors in their composition; they contain\nchemical compounds that are associated by the brain as the\ntaste sensation [7].\nB. LANGUAGE MODEL\nLanguage Models (LM) are models of the probability distri-\nbution of a sequence of words in a given language. Among the\nvarious types of models, the Statistical Language Model is the\nmost generic [14]. A Statistical Language Model consists of\ncalculating the probability of a sequence of words based on\nthe estimate of their occurrence. The probability P(w) of a\nsequence of T words w =w1,w2,··· ,wT is given by (1).\nP(w) =\nT∏\nt=1\nP(wt |w1,w2,··· ,wt−1) (1)\nThe traditional approach to the creation of such models is\nthe n-gram statistical model [14]. Such model consists of the\ntruncation of the historical context of a word in n (i.e. the\nprobability of a word occurring in a sequence is calculated\nonly with respect to n −1 predecessor words). The proba-\nbility of a word occurring in a given context is calculated\non the basis of the relation between the frequency of the\nsequences containing the n words divided by the frequency\nof the sequence containing the n −1 words.\nOthers approaches use Artiﬁcial Neural Networks (ANN)\nto predict conditional probabilities of the model (i.e. predic-\ntion of the word n based on n −1 predecessors) [15]–[17].\nThe simplest approach to an implementation of such model\nis through a Multilayer Perceptron (MLP) feedforward net-\nwork, where a vector representing the sequence of the n −1\npredecessor words is presented as the input of the network\nand another vector representing the word to be predicted\n(nth word of the sequence) is shown as the network output.\nFor the calculation of the nth word probability given to the\nn −1 predecessors (presented as the input of the network),\na Softmax is performed on the network’s output. Each word\nis represented by 1-to-K vector, in which only the position\nrepresenting the word in the vocabulary is set to 1, where\nK is the total number of words of the vocabulary. In order\nto represent sequences of words, other approaches can be\nused: the concatenation of vectors 1-to-K; or a Bag-of-Words\n(BOW) [17], consisting of the weighted sum of the 1-to-K\nvectors, where the weights are inversely proportional to the\ndistance in time (i.e. index t) of the word from the sequence\nto the nth word to be predicted. Fig. 1 exempliﬁes a LM based\non feedforward Neural Network with BOW input for n equal\nto 3 (i.e. predicting the third word based on the previous two).\nIn Fig. 1, x(t) is the state of the input layer (consisting\nof BOW); s(t) is the state of the hidden layer; and y(t) is\nthe result of the application of Softmax on the state of the\noutput layer, so that it generates the conditional probabil-\nity of the model. The vectors wt−1, wt correspond to the\n1-to-K representation of the words of the text at positions\nt −1 and t respectively; while the vector P(wt+1|wt−1,wt )\nFIGURE 1. Example of LM (equivalent to a trigram) based on Neural\nNetwork Feedforward through a BOW.\ncorresponds to the probability vector for the word at position\nt +1. The probability of wt+1, given the previous words\n(i.e. wt−1 and wt ), is obtained by the scalar product\nof the 1-to-K representation of wt+1 within the vector\nP(wt+1|wt−1,wt ). It should be noted that the probability\nvalues on the layer y(t) are only illustrative (as with other\nlanguage models throughout the remainder of the text) and\nthe smoothing promoted by softmax precludes the null values\npresented in this layer (this smoothing is necessary to add\nrobustness to unseen data in training).\nPerplexity is an information theory metric that measures\nhow much a model is able to reduce the uncertainty of the\nsymbols of an information source [18]. In the context of\nLanguage Models, perplexity consists of how much the LM is\nable to reduce the uncertainty in the prediction of the words\nof a text in the language for which it was trained. Roughly,\nperplexity measures the average number of viable symbols\nin the prediction of each word in the text, as if it reduced\nthe problem of prediction from the original vocabulary to an\nequiprobable vocabulary of the size of the perplexity value.\nThus, the smaller the perplexity, the better the predictive\nmodel predicts such text (i.e. the model is able to model\nthe language of the evaluated text with high accuracy). The\nformula for calculating perplexity is given in (2) and (3) [18].\nHp(w) = 1\n|w|log2( 1\nP(w)) (2)\nPP =2Hp(w) (3)\nwhere w is a test text in the language in which the model\nwas trained; |w|, the number of words in such text; P(w),\nthe probability of the text belonging to the language modeled\nby the LM, which is calculated by (1) through the conditional\nprobabilities provided by the model; Hp(w), the cross-entropy\nof the text in relation to the model; and PP is the perplexity\nof the model.\nIII. RELATED WORK\nIn [1], a system was proposed for the creative generation of\nsoups, concentrating on the list of ingredients. The genera-\ntion and evaluation of recipes are based on an inspiring set\nVOLUME 8, 2020 146265\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\n(i.e. artifacts that are known and evaluated by humans as\ncontaining quality) and on a metric of novelty based on a\nn-gram statistical model for evaluating rare combinations of\ningredients. The recipes are generated by a genetic algorithm\nand evaluated through two MLP Neural Networks, which\nwere trained for different levels of abstraction of the ingredi-\nents in the inspiring set, and a novelty metric calculated based\non the inspiring set.\nIn [19], a system for creative generation of salads was\nproposed (also concentrating only on the ingredient list).\nHowever, in this case, the evaluation consisted of a dis-\ncriminative classiﬁer, which indicated the best recipe, based\non the star rating of the allrecipes.com site, for each pair\nof evaluated recipes. Through this discriminative classiﬁer,\nthe search algorithm could exploit the space of salads and\ngenerate creative recipes.\nIn [5], a system was proposed to generate lists of ingredi-\nents for creative recipes. This system uses a genetic algorithm\nto create the lists of ingredients and evaluates the creativity of\nthese artifacts through the RDC metric (a metric for evaluat-\ning creativity for different domains, which integrates value\nand novelty). The calculation of the value was made from a\nsynergy graph based on the hypothesis of Food Pairing and\nthe Bayesian Surprise was used to calculate novelty.\nThe authors in [3] proposed a system for generating com-\nplete creative recipes (i.e. including the steps of the prepara-\ntion instructions). This system uses the following metrics in\nthe evaluation of creativity: the calculation of value is based\non the hypothesis of Food Pairing and the perception of taste\naccording to neurogastronomy; the novelty evaluation metric\nis the Bayesian Surprise. The approach used to create the\npreparation instructions was based on a clustering algorithm\nof the total path of actions that an ingredient suffers, consid-\nering the recipes where it occurs, in which are selected the\npaths of the clusters with the greatest number of occurrences\nfor each ingredient and the joining of these paths in a single\nexecution tree.\nThe present work uses the system available in [5] to cre-\nate the lists of ingredients. However, unlike [1], [5], [19],\nthe steps in the preparation instructions are also generated.\nThe ReProg approach, ﬁrst approach of this work to the cre-\nation of recipe preparation, differs greatly from [3]. Instead\nof using an algorithm to decompose and recompose the\nrecipes into the recipe database into a new execution plan,\nthis approach uses a Genetic Programming algorithm guided\nby the perplexity metric on a Language Model trained with\nthe internal structures of recipes. Secondly, in ReComp, other\napproach proposed in this work, we used the strategy of\ndecomposing and recomposing recipes similar to [3]. How-\never, this strategy was implemented in a genetic algorithm\nand uses the perplexity metric on a Language Model to eval-\nuate the mixtures of the ingredients along the recipe actions,\nin order to guide the recomposition of the recipes for better\nmixtures (e.g. with the proper use of seasoning, and oils in\nfrying).\nFIGURE 2. Simplified diagram of the system architecture.\nIV. SYSTEM ARCHITECTURE\nThe generic architecture of the system for generating creative\nculinary recipes proposed in this work is presented in Fig. 2.\nThe Recipe Database is a repository of recipes in a struc-\ntured format for training the system models. The Creative\nFlavor Pairing component is the API for generating creative\ningredient lists available in [5], in which lists of ingredients\nare generated based on the hypothesis of Food Pairingand the\nRDC metric (Regent-Dependent Creativity) [10]. The RDC\nis a domain independent metric that assesses the creativity\nof artifacts. This metric requires that artifacts are described\nwithin the Regent-Dependent Model, in which artifacts fea-\ntures are represented as a set of pairs between its features and\ntheir modiﬁers. This dependency relationship is deﬁned by a\npair P(regent; dependent) associated with a numeric value v.\nA regent is a feature that contributes to describe an artifact,\nand may be an action or attribute, while a dependent can\nchange the state of an attribute or connect an action to a target.\nThe Recipe Generator is the component responsible for\ngenerating the sequence of actions of the preparation from\nthe list of ingredients and also the composition of these parts\ninto a complete recipe. The Recipe Generator presents two\napproaches that are described in detail in Section V.\nA. RECIPE DATABASE\nA recipe is composed of a list of ingredients and a sequence of\nactions to be applied to the ingredients. In recipes created by\nhumans, the list of ingredients is usually well-structured, but\nthe steps of the actions is arranged in a natural-language text\nwithout structuring (i.e. the actions are arranged throughout\nthe text along with other words). Thus, for a computational\nsystem, it is easier to process the list of ingredients than the\nactions of the preparation instructions.\nProcessing the preparation instructions requires a pre-\nprocessing step, which consists of structuring the preparation\ninstruction actions that are arranged in the natural language\ntext of the recipe. This requires a Natural Language Pro-\ncessing (NLP) effort, culinary knowledge (through ontologies\nof ingredients, tools, and actions) and inference rules (to\nevaluate the disposition of actions temporarily and to use\nontologies to remove ambiguities, treat implicit references,\netc.). Through this preprocessing, the system can ﬁnd the\ndependency relationships between actions (i.e. sequences\nof actions, where one action is performed on the result\nof the other) and also between actions and ingredients to\nﬁnally be able to process the instructions of the preparation.\n146266 VOLUME 8, 2020\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nFIGURE 3. Example of recipe represented as an acyclic graph.\nThese dependencies of the preparation instructions form an\nacyclic graph [3] as that of Fig. 3.\nIn Fig. 3, the edges symbolize the relation of dependence\n(the direction of the edge indicates the sequence of actions -\nfrom past actions to future actions); and the nodes represent\nactions and ingredients. In Fig. 3, the nodes of ingredients\nwere highlighted in darker color to demonstrate that the\ningredients always occur at the ends of the graph.\nThe next subsections present the recipe databases used in\nthe context of this work.\n1) WIKITAAABLE\nWikitaaable2 is a database of recipes created on the\nsemantic wiki platform to enable reasoning about culinary\nknowledge [20]. The recipes in this database have three\nmain components: list of ingredients; text of the actions\nof the preparation instructions; and a formal description of\nthe actions of the preparation instructions. An example of\nsuch a formal description is shown in Fig. 4. This formal\ndescription resembles a graph list representation that shows\nthe state name and information on which other states it has\nan edge. Thus, this base does not require the preprocessing\nof the text of the preparation instructions, since it already\nhas a representation of the preparation instructions as an\nacyclic graph (in the form of a list of edges). All the recipes\navailable in the Wikitaaable dump, containing 1439 recipes,\nwas used.\nFIGURE 4. Formal preparation of the ‘‘Squash fluff’’ recipe.\nThe database of Wikitaaable also has an ontology of\nactions and ingredients. The ontologies of actions and ingre-\ndients used in the present work were based on these two\nontologies of Wikitaaable. The following modiﬁcations were\nmade: in the ontology of actions two levels of more generic\nclasses were used; on the ingredient ontology was used the\nmost speciﬁc level (e.g. \"Bacon\" was assigned to the class\n\"Pork\" instead of just \"Meat\"). The complete ontology of\n2Wikitaaable is a semantic Wiki in the ﬁeld of cooking. A cached\nversion can be accessed at: https://web.archive.org/web/20120615184112/\nhttp://wikitaaable.loria.fr/index.php/Main_Page\nFIGURE 5. Actions ontology.\nactions used in this work is presented in Fig. 5, where the\nmerged cells have the name of the classes, each with their set\nof subclasses and the actions (which actually appear in the\nrecipe preparation) on the right.\nB. WIKIA\nAs an alternative source of recipes, the online recipe database\nWikia3 has also been used (500 recipes of the category of\nAmerican recipes). Due to the fact that this database does not\ncontain the formal description of the preparation instructions,\nit was necessary to create an NLP parser, as mentioned in\nSection IV-A. Such a parser is explained in detail in the next\nsubsection.\n1) NLP PARSER\nThe architecture of the NLP parser is shown in Fig. 6.\nFIGURE 6. Parser NLP architecture.\nThe Dependency Parser consists of a parser that generates\ndependency tags (e.g. direct object, subject, etc.) and Part\nof Speech tags (PoS tags - some examples of these tags\nare nouns, verbs, and so on) of words in natural language\nphrases. In this work, we used the parser dependencies called\nParsey McParseface created from the framework of Google\nSyntaxNet. An example output of the dependency parser is\nshown in the text below:\n3Wikia is an online repository of textual culinary recipes. It can be\naccessed at: https://recipes.fandom.com/wiki/Category:American_Recipes\nVOLUME 8, 2020 146267\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nInput: cook onion in oil in large skillet.\nParse:\ncook VB~ROOT\n+-- onion NN dobj\n+-- in IN prep\n| +-- oil NN pobj\n+-- in IN prep\n| +-- skillet NN pobj\n| +-- large JJ amod\n+--. . punct\nIn the output of this dependency parser we have: the sentence\nevaluated in the ﬁrst line (preceded by ‘‘Input:’’); the result of\nthe dependency parser after the second line (below ‘‘Parse:’’).\nAt the end of each result line, there are 3 components: one\nword; the PoS Tag; and the dependency tag. The indentation\nof the result represents the hierarchy between the words (the\nmore left and higher, the higher in the hierarchy).\nTABLE 1. Tags for action detection.\nTABLE 2. Tags for ingredients and tools detection.\nThe References Evaluation step consists of analyzing some\nof the tags generated by the Dependency Parser in order to\ndetect references to actions (ontology actions), ingredients\n(from the list of recipe ingredients), and tools. Table 1 shows\nthe list of tags for action detection. The ingredients and tools\nare detected, on the branch of each action, from the tags\nin Table 2 (the distinction between ingredients or tools is\nmade by comparison of the detected words). In both tables,\nthe dependency tag is the criteria for detection, while the PoS\ntag serves only as a ﬁlter (only words with those PoS tags\nwill be detected). The hyphen symbol (minus) denotes an\nabsence of a ﬁlter (i.e. any PoS tag is accepted in this case).\nAn example of such detections is shown below:\nInput: layer lettuce , bacon , turkey and\ntomato on each tortilla.\nParse:\nlayer NN ROOT\n+-- lettuce NN dobj\n| +-- , , punct\n| +-- bacon NN conj\n| +-- , , punct\n| +-- turkey NN conj\n| +-- and CC cc\n| +-- tomato NN conj\n+-- on IN prep\n| +-- tortilla NN pobj\n| +-- each DT det\n+--. . punct\nDetections:\nAction = ‘‘layer’’\nIngredients = [‘‘lettuce’’, ‘‘bacon’’,\n‘‘turkey’’, ‘‘tomato’’,\n‘‘tortilla’’]\nThe Inference Engine step performs inferences based on\nculinary knowledge for the construction of a timeline con-\ntaining the sequences of actions and their ingredients. This\ntimeline contains initially (denoted as -1 time) only ingre-\ndients in the ingredient list (including the appropriate units\nand quantities available). For each action detected by the\nReference Evaluation step, a time record (starting from 0)\nis created with the action and ingredients (again with their\nunits and quantities available) that undergo such action. These\ningredients along the timeline represent the pathway of each\ningredient in mixtures (i.e. not always represent the isolated\ningredient). Each action generates a new time in the timeline.\nAnd for each ingredient name (word detected in the Reference\nEvaluation step) a ﬁltering is performed on all ingredient\nrecords present in the timeline (throughout all previous times)\nto select the ingredient record for the action of the current\ntime. Such ﬁltering is performed with the following criteria:\na) Similarity in the name.\nb) If there are verbs in the tree branch of the ingredient,\nthe similarity of the name of these verbs with the names\nof the actions of the timeline.\nc) If there are quantities in the tree branch of the ingredi-\nent, the compatibility of these values with the unit and\nquantity of the ingredients in the timeline (removal of\ninsufﬁcient quantity records).\nd) If there is more than one record of the ingredient in the\ntimeline, select only the one associated with the longest\ntime.\nAfter selecting the ingredient from the timeline, a copy\nof this record is made for the current action time by keep-\ning a reference to the original record and transferring the\noriginal record amount to the copy (partial, if there is a\nquantity or total reference in otherwise). If the original record\nbelongs to a mixture (i.e. the action of that record is being\nrun on more ingredients), all other ingredient records of that\ntime are also copied (in the same way as the selected one).\nIf no ingredient in the timeline is selected, it is evaluated\nwhether the ingredient name is listed as an ingredient class\nname (eg ‘‘all ingredients’’, ‘‘vegetables’’, etc.), therefore all\ningredients belonging to this class are added (according to the\nontology of ingredients) and containing quantity available.\nAfter an action is ﬁnished processing (i.e. selecting and\ncreating the ingredients in the timeline), the following analy-\nsis should be performed:\na) If the action is an ‘‘add’’, for a time greater than 0 and\ncontain all the ingredients like ‘‘dobj’’ and ‘‘iobj’’ or all\nthe ingredients as ‘‘pobj’’ on a single preposition, then\nall records of the ingredients from the previous time are\ningredients of this action.\n146268 VOLUME 8, 2020\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nb) If the action is different from an ‘‘add’’ and does not\ncontain any ingredient record in the timeline, then all\ningredient records from the previous time are treated\nas an ingredient of this action.\nTABLE 3. Example of timeline built up to time 2 (third action).\nIn Table 3, it is presented a small example of a timeline\ncreated by the stage of the inferences engine for the recipe\nbelow, where each timeline record corresponds to an ingredi-\nent tuple containing: ingredient quantity (and unit) available;\ningredient name; time of origin of the ingredient in the time-\nline (i.e. from where that ingredient was copied) or the symbol\n∅ (indicating the ingredient list).\nIngredients:\n- 2 potato\n- 1 kg pork\n- 1 orange\nDirections:\n- Cut the potatoes.\n- Fry.\n- Slice the orange.\n...\nFinally, the Formal Recipe Generation step generates\nrecipe from the timeline (i.e. the ingredient list and the prepa-\nration instructions graph as a list of edges). Each time from\n0 on the timeline will generate a node with the name of the\naction related to that time. In turn, each ingredient record in\nthe timeline will form an edge between the action node of that\ntime and the time action of the source record (from where that\nrecord was copied). If the ingredient record has been copied\nfrom the ingredient list (time −1), then an ingredient node is\ncreated with the ingredient name as the input of that edge.\nC. NORMALIZATION OF THE RECIPES\nIn order to remove redundancies in the recipes, the addition,\nsubstitution or removal of states from its graph in the prepara-\ntion instructions was performed. These transformations were\nbased on [3], aimed at promoting more efﬁcient processing\nof recipes by the rest of the system. They were applied on\nall recipes, regardless of the database (Wikia or Wikitaaable).\nThe following rules were established for the structure of the\ngraph of preparation:\na) It should contain only actions present in the ontology\nof actions (i.e. remove missing actions in the ontology).\nb) Actions should represent relevant transformations of\nthe ingredients (i.e. remove actions on tools or move\ningredients).\nc) There should be no synonymous actions (i.e. to replace\nsynonymous actions).\nd) Every recipe must contain a ‘‘serve_recipe’’ action by\njoining all the related components of the graph by the\nlast action (in time) of each component (i.e. a single\noutput action).\nThe removals required to satisfy rules a and b involve\npassing all input nodes (from the node to be removed)\nto each of their output nodes (those having the node\nremoved as an input). The actions that were removed\nbecause of rule b were the following: actions of the classes\n‘‘to_reconstitute’’, ‘‘to_move’’ (except the ‘‘to_shake’’ sub-\nclass), ‘‘to_pour’’, ‘‘to_clean’’ and ‘‘to_prepare’’; and also\nthe actions ‘‘reserve’’, ‘‘fold’’, ‘‘form’’, ‘‘drop’’, ‘‘discard’’\nand ‘‘preheat’’ (does not belong to ontology of actions, but\nalso occurs frequently in recipes).\nIn order for the recipes to comply with rule c, the substitu-\ntions shown in Table 4 were made.\nTABLE 4. Replacement of synonyms in the normalization of recipes.\nIn the case of the ReComp approach to generate the prepa-\nration instructions (i.e. the approach based on decomposition\nand recomposition of recipes), the following rule was also\nobserved: all actions should have arity to one (i.e. on a single\ninput), with the exception of ‘‘add’’ and ‘‘mix’’, which can\nhave more than one input (i.e. thus, insert an ‘‘add’’ on the\ninputs of any action that does not respect this rule). This addi-\ntional rule makes it possible to control the arity of the actions\nin a more rigid way (necessary in the ReComp approach), but\nit promotes a lot of modiﬁcation in the internal structure of\nthe recipe, by the addition of nodes, and for this reason, it was\nonly used in this approach.\nFig. 7 shows how the recipe ‘‘Squash ﬂuff’’, presented\nin Fig. 4, would look after its normalization.\nAfter the normalization stage, the recipe database is ready\nto be attached to the system.\nV. RECIPE GENERATION\nOnce the database of recipes is created and a list of ingre-\ndients is requested from the Creative Flavor Pairing API,\nwe can generate the graph of the actions of the preparation\ninstructions. Two approaches (i.e. two systems) have been\ndeveloped for this task.\nVOLUME 8, 2020 146269\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nFIGURE 7. Normalized ‘‘Squash fluff’’ recipe.\na) ReProg Approach, consisted of the generation of\nthe recipes using genetic programming and language\nmodels.\nb) ReComp Approach, uses a process of decomposition\nof the database recipes, clustering of these recipe parts\nand recomposition of new recipes through genetic algo-\nrithm and language models (on mixtures).\nThe following sections describe the two approaches.\nA. ReProg APPROACH\nThe system architecture for this approach is shown in Fig. 8.\nFIGURE 8. System architecture for ReProg Approach.\nThe Language Modeling is the component responsible for\ncreating language models (LM repository) from the Recipe\nDatabase, while the Statistics component is responsible for\nextracting recipe statistics and storing it in the Recipe Statis-\ntics repository. These statistics are related to the repetition\nof ingredients and actions of the preparation instructions;\nas well as the arity statistics (i.e. the number of ingredients\nparticipating in each type of preparation action). The Genetic\nProgramming component consists of the genetic program-\nming algorithm, responsible for generating the recipe prepa-\nration from the list of ingredients, language models and recipe\nstatistics.\n1) LANGUAGE MODELING FOR RECIPE EVALUATION:\nACTIONS ON ITS INPUTS\nJust like the order of words in a text, the order and sequence\nof steps are important in a recipe. The steps of the preparation\ncan promote physical or chemical transformations, such that\nsome of these actions only make sense to be performed on\ningredients in a given state. However, unlike texts, a culinary\nrecipe does not contain information arranged only sequen-\ntially. The steps of a preparation form a tree so that certain\nFIGURE 9. Architecture of the proposed Language Model being applied to\nthe structure of a culinary recipe.\nbranches can occur in parallel. In order to evaluate this paral-\nlel information, a Language Model applied to culinary recipes\nmust evaluate information between consecutive steps and\nparallel steps of the same branch. The architecture proposed\nin this work is shown in Fig. 9, in which for each action\nof the preparation instructions to be predicted (denoted by\nNt+1, which could be predicted, for example, as ‘‘sauté’’ with\nprobability 0.2 or ‘‘fry’’ with probability 0.8), the predecessor\nactions are evaluated at the level below the branch (i.e. all\nthe ‘‘children’’ ingredients and actions of the action to be\npredicted - denoted in Fig. 9 by Nt to Nt−2, which could be,\nfor example, ‘‘cut’’, ‘‘onion’’ and ‘‘oil’’), as if the nth word\nof the Language Model was the action to be predicted and all\nthe ‘‘children’’ of that branch were the predecessor sequence.\nOnce this model is trained on the database recipes,\nthe model will absorb and generalize certain syntactic pat-\nterns present in the structure of culinary recipes (i.e. a kind of\nlanguage of the recipe execution process). Then it is possible\nto use the perplexity metric as a selection criterion for recipes\nthat present these patterns. The more the recipes present the\nsyntactic patterns modeled, the lower the perplexity of the\nmodel, compared to the perplexity of the recipes.\nIt should be noted that Recurrent Neural Networks (RNN)\nare the state of the art for language models. The RNNs\nhave the greatest capacity to evaluate long sequences of\ntemporally arranged words (in relation to other networks),\nbut in our adaptation of language models for evaluating the\nrecipe preparation graph, there is no evaluation of very long\ntemporal sequences (basically just one time step - from a node\nto its input). Thus, the use of RNN-based language models in\nthis context is not justiﬁed.\n2) GENETIC PROGRAMMING FOR RECIPE GENERATION\nThe ingredients previously deﬁned for the ingredient list of\nthe new recipe are used as a set of variables in the GP\nalgorithm (i.e. it deﬁnes the ingredients available for the evo-\nlutionary process restricted to those generated by the Creative\nFlavor Pairing API). The function set consists of all the\nactions that occur on a recipe database (including the root\nnode of the recipe, which uniﬁes the ﬁnal parts of the recipe).\n146270 VOLUME 8, 2020\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nThe GP algorithm used was the DEAP (Distributed Evo-\nlutionary Algorithms in Python) framework. 4 Such algo-\nrithm was implemented for multiobjective optimization:\nto minimize the perplexity of the model in relation to the\nindividuals (i.e. to obtain better perplexity values); and maxi-\nmize the depth of a recipe (otherwise, minimizing perplexity\nwould always lead to very small recipes). On the objective\nof perplexity two constraints were imposed. In the event\nof such restrictions, perplexity is penalized: (i) if the indi-\nvidual presents repetions of some node above the maxi-\nmum or below the minimum repetions found for such node\nin the database; and (ii) if any of the ingredients in the initial\nlist are not used in the individual. The penalty function is\nshowed in (4).\nigd =(|I|+1)\n(|L|+1)\nPP∗=PP\n((rpt +1)\nigd\n)2\n(4)\nwhere PP is the original perplexity; PP∗, the penalized per-\nplexity; rpt, an indicator function of out-of-range repetitions\n(value equals to 1 if there is a repetition above the maxi-\nmum or below the minimum repetition recorded in the recipe\ndatabase and it equals to 0 if otherwise); I, the list of ingre-\ndients used in the recipe; and L, the list of ingredients in the\nlist provided by the Creative Flavor Pairing (i.e. available\ningredients).\nThe ﬁtness fuction of the individuals is presented in (5).\nThe objective space used for the ﬁtness calculation consisted\nof the Cartesian plane formed from the penalized perplexity,\nPP∗, and the depth of the preparation graph of the individuals.\nﬁtness =1\nv +1\nf (5)\nwhere v is the number of neighbors of the individual in the\nobjective space (perplexity and depth), which is calculated\nthrough a grid (where neighbors are individuals belonging to\nthe same cell, including the current individual); and f is the\nnumber of the Pareto front to which the individual belongs\n(the lowest possible value is 1).\nshould be noted that the variable v is one of the factors\nin the objective search space (i.e. it is not an objective in\nitself). While the variable f is the composition of the two\nobjectives (i.e. the penalized perplexity and the depth of the\nrecipe), but a composition that respects the pareto optimality.\nThat is, f always presents the same value for each set of\nsolutions, which forms a frontier (called a pareto frontier),\nfrom which a solution cannot be chosen as an optimal solu-\ntion of the problem in relation to the others of this same\nset without prioritize a speciﬁc objective. Pareto frontiers\nare based on the concept of dominance. Given two distinct\npoints (solutions) in the objective space, it is said that one\npoint dominates the other if for all objectives of that point\n4DEAP is a framework for implementing evolutionary algorithms. It can\nbe accessed at: https://github.com/DEAP/deap\nFIGURE 10. Example of objective space to minimize two conflicting\nobjectives.\nhave equal or better values optimized in relation to the other\npoint (i.e. the point that dominates can be said better than\nthe another point, even without prioritizing any objective).\nThe ﬁrst pareto frontier of a population of possible solu-\ntions (individuals of the evolutionary algorithm) is the set of\nsolutions that are not dominated by any other in this entire\npopulation. That is, for at least some objective, each of these\nfrontier solutions presents a more optimized value compared\nto the other solutions (including in relation to other solutions\non the frontier itself). For example, in Fig. 10, points A, B\nand C belong to the ﬁrst pareto frontier that minimizes the\n2 objectives illustrated in the image. Point B is better than\npoint A in the ﬁrst objective and better than point C in the sec-\nond objective; while A is better than B and C in the second\nobjective; and C is better than A and B in the ﬁrst objective.\nThus, none of the three points can be considered a better\nsolution than the other two without prioritizing any speciﬁc\nobjective. Removing all points from the ﬁrst pareto frontier,\nit is possible to create, by the same principle, a second pareto\nfrontier with the remaining points that are not dominated by\nany other of this new population. Therefore, although the\nﬁtness function is just a single equation, due to this multi-\nobjective nature of the variable f , this single function will\nbe able to optimize two objective functions (in this case,\npenalized perplexity and depth of the recipe).\nThe selection operation used for the crossover was the\ntournament of size 3. And the crossover used in the genetic\nprogramming algorithm of the present work is based on the\nfollowing steps: after the two parent individuals have been\nchosen, a copy of both is made to represent the genetic\nmaterial available for the generation of the children. Based on\nthis copy of the parents, a selection node is randomly selected\nfor each of these two trees. The only restriction for choosing\nthese two cutting nodes is that the root of the tree must be\ndisregarded (i.e. it cannot be chosen). From these two chosen\ncutting nodes, the entire branch of the trees is cut. Then the\nexchange of these cut branches is made between the two\ncopies of the parents’ trees (i.e. the branch removed from the\nﬁrst tree is connected in the place where the chosen cutting\nnode of the second tree was and vice versa). If the resulting\ntrees have a depth above the pre-established maximum limit\nfor recipes, then the children are discarded and the process\nis repeated in a similar way. However, at each unsuccessful\niteration, the new cut point of the largest branch (in depth)\nVOLUME 8, 2020 146271\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nFIGURE 11. Example of recombination operation for genetic\nprogramming.\nof the previous iteration is exchanged for a cut node closest\nto the leaf on that same branch of the previous iteration (i.e.\nthe branch is exchanged for a sub-branch). The process is\nrepeated until cutting nodes are chosen that are capable of\ngenerating two children with valid depth. An example of this\ncrossover operation is shown in Fig. 11.\nThe mutation does not occur individually. It was only\napplied to the whole set of genes of the population of the\ngenerated offspring. And the mutation operation consisted of\nchanging the content of the nodes at random (i.e. a portion of\nthe nodes in the entire population undergone on an exchange\nof the preparation action or ingredient with which it was\nassociated). The only three restrictions for this operation were\nthe following ones: i) the ‘‘serve_recipe’’ action can only\nappear at the root of the tree and cannot be changed; ii)\npreparation action nodes cannot be exchanged for ingredient\nnodes and vice versa; iii) and the space of possible ingredients\nfor mutation is restricted to the ingredients previously chosen\nby the step of generating the list of ingredients.\nAt each iteration of the algorithm, crossovers are per-\nformed in a quantity necessary to double the size of the\npopulation and then half of the total population that appear\nin the smaller nondominated fronts is chosen, eliminating\nthe rest. The ﬁrst nondominated front of the ﬁnal popu-\nlation is always compared to the Hall-of-Fame (containing\nindividuals from previous iterations that have never been\ndominated by any other), in order to update such a set. And the\nﬁnal answer of the algorithm is the Hall-of-Fame of the last\niteration.\nFIGURE 12. System architecture for ReComp Approach.\nB. ReComp APPROACH\nThe system architecture for this approach is shown in Fig. 12.\nLanguage Modeling is the component responsible for cre-\nating the language models for mixtures (LM repository)\nfrom the recipe database. The Decomposing and Clustering\nis the component responsible for the decomposition of the\nrecipe preparation (subgraph decomposition) and clustering\nof its parts (forming a repository of subgraph clusters). The\nGenetic Algorithm component consists of the genetic algo-\nrithm responsible for recomposing the graph of preparation of\nthe new recipe from the subgraph clusters (restricted to the list\nof ingredientes provided by the Creative Flavor Pairing and\nusing language models to evaluate the ingredient mixtures).\n1) LANGUAGE MODELING FOR RECIPE EVALUATION:\nACTIONS ON MIXTURES\nA second approach that we explore in this work is to evaluate\nin the recipes the mixtures of ingredients that occur along\nthe steps. Several actions occur on a single ingredient or a\nsingle mixture (e.g. ‘‘peel an apple’’). Such actions do not\npromote the formation of a new mixture (only the processing\nof an existing mixture or ingredient). On the other hand,\nthere are actions that promote the creation of new ingre-\ndient mixtures (e.g. ‘‘fry an egg in oil’’, ‘‘cook a chicken\nwith garlic’’, etc). As recipes are normalized according to\nSection IV-C, these actions that promote new mixtures always\ncontain an ‘‘add’’ or ‘‘mix’’ in their inputs. The proposed\nmodel for this approach is similar to that of ReProg Approach\n(Section V-A1). However, this model presents the following\ndifferences: actions to be predicted by the model (in the\nlayer y(t)) are only those that promote new mixtures (ignoring\nall other actions); and the BOW of the layer x(t) should be\ncomposed only of the ingredients of the mixture undergoing\nthe predicted action (i.e. any ingredients occurring within\nthe branch of action to be predicted - ignoring any inter-\nmediate processing that the ingredient undergoes). Fig. 13\nshows an example of applying this model to a recipe, where\nNt+1 is the action that promotes a mixture (action to be\npredicted by the model - for example, ‘‘fry’’ with probability\nof 0.3 and ‘‘cook’’ with probability 0.7); I0, I1, I2 are ingre-\ndients achieved in the branch and form the mixture promoted\nby the action Nt+1 (e.g. ‘‘oil’’, ‘‘garlic’’ and ‘‘pork’’).\nFIGURE 13. Architecture of the proposed Language Model for mixtures\nbeing applied to the structure of a culinary recipe.\nThis model was also trained on the recipe database, but\nthe goal, in this case, was to absorb the ingredients mixing\npatterns throughout the recipe. In this case, the perplexity\n146272 VOLUME 8, 2020\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nmetric indicates how well adjusted the structure is in terms of\nthe formation of ingredient mixtures compared to these mix-\nture patterns in the recipe database. The more the evaluated\nrecipe contains these mixture formation patterns, the lower\nthe perplexity value of the model compared to the evaluated\nrecipe.\n2) DECOMPOSITION AND RECOMPOSITION OF RECIPES\nThis recipe decomposition and recomposition approach is\nbased on [3]. The main difference is that the algorithm of\nrecomposition of the recipes was implemented on a genetic\nalgorithm and also counted on the use of language model.\nIn the process of recipes decomposition, they should be\nbroken down into pieces of recipes, which are basically\npaths on the graph of the preparation that begin in the last\naction of the graph (the root - which in this case is always\n‘‘serve_recipe’’) and ends in an ingredient (a leaf in the\ngraph). In this way, there will be a path to the sequence\nof actions each ingredient undergoes throughout the recipe.\nSuccessive repetitions of the same action along that path can\nbe removed. An example of this decomposition for the recipe\nof Fig. 7 is shown below:\nDecomposition of the ‘‘Squash fluff’’ recipe:\n[‘‘serve_recipe’’, ‘‘add’’, ‘‘mash’’, ‘‘cook’’, ‘‘squash’’]\n[‘‘serve_recipe’’, ‘‘add’’, ‘‘milk’’]\n[‘‘serve_recipe’’, ‘‘add’’, ‘‘bake’’, ‘‘add’’, ‘‘beat’’, ‘‘egg’’]\n[‘‘serve_recipe’’, ‘‘add’’, ‘‘bake’’, ‘‘add’’, ‘‘marshmallow’’]\nOnce all the database recipes have been decomposed into\nvectors, these vectors are separated into sets according to\nthe ingredients at the end of each vector. Thus, there will be\nfor each ingredient a set of all the sequences of actions that\noccur on it in the recipe database. In addition, sets are also\ncreated for classes of ingredients, for which a copy of each\ningredient vector belonging to the class is made by replacing\nthe ingredient name with the class name. These class sets\npresent the sequence of steps that ingredients of the same\nclass usually suffer (e.g. action sequences for ‘‘meats’’).\nOn each of these sets, a clustering is performed through the\nK-Medoids algorithm [21]. The K-Medoids algorithm works\nin a way equivalent to K-Means, but instead of calculating\na centroid for each cluster, this algorithm uses the most\ncentralized data point (with less distance to the other points)\nof the cluster to represent it, since we can not calculate a mean\ncoordinate of a vector of symbols [21]. The distance metric\nused to cluster these sets of recipe parts is the Levenshtein dis-\ntance metric, which consists of the least amount of addition,\nsubtraction, or substitution required to make two sequences\nequal [22].\nEach cluster in a set represents a sequence of actions on an\ningredient (or class of ingredients) that is repeated in several\nrecipes with small variations. The larger the cluster size the\nmore frequent the sequence of actions. In this way, the impor-\ntance (i.e. the weight) of each cluster, and consequently its\nsequence of actions, is measured by its size.\nAfter clustering, each sequence vector present in the set\nis associated with a tuple containing its numerical identiﬁer\n(the position of this vector in the set); the weight of its cluster\nTABLE 5. Example of cluster and triples for ingredient and its class.\n(the size of the cluster); and a boolean class indicator of the\nset type, with false for ingredient sets and true for class sets.\nWith this triple assignment (id, weight and class indicator),\nthe database recipe decomposition step is ﬁnished. Table 5\nshows an example of two clusters by set (i.e. in the table,\n4 clusters are presented, since they are two sets) and their\ntriples for the ‘‘potato’’ ingredient and its class (the set for\n‘‘tuber’’ class).\nThe recomposition step involves the creation of new\nrecipes from the vectors of the clusters and the language\nmodel (LM for mixtures). Such recomposition was performed\nthrough a genetic algorithm (GA) with the individual mod-\neled as follows: each gene is associated with an ingredient\nin the ingredient list of the new recipe (generated by the\nCreative Flavor Pairing API); and the value that each gene\ncan assume is the triple value (id, weight, and class indicator)\nof the set of recipes decomposed for that ingredient and its\nclass. Thus, each gene will contain a reference to a sequence\nof actions of one of the ingredients. The GA of this approach\npresents evaluation, selection, crossover and mutation opera-\ntions. This GA is similar to the PG described in Section V-A2\nin the following aspects: it was implemented using the DEAP\nframework; used the ﬁtness function presented in (5); used\nthe tournament of size 3 for selection operation; presented\na crossover based on population doubling and choosing the\nsmaller nondominated front; and the Hall-of-Fame always\ncontains the ﬁrst nondominated front between the population\nand the Hall-of-Fame of the previous iteration. However,\nthe crossover used in this genetic algorithm consisted of a\nsimpler operation compared to the GP approach. First, a copy\nof the vector was made containing the genetic material of the\ntwo chosen parents. After that, a random cut point was chosen\nfor these two vectors, which can be any position except the\nlast position (so that you can use the genetic material of both\nparents). Then, to compose the child, all genes up to the cutoff\npoint were chosen from one parent and after the cutoff point\nto the end of the vector were chosen from the other parent.\nAn example of this crossover operation is shown in Fig. 14.\nThe integer values of the content of the genes in the image is\nmerely illustrative to simplify visualization.\nTable 6 shows an example of an individual of this genetic\nalgorithm (with the true content of the genes).\nNote that by adding the weight of the triples of an indi-\nvidual, the total weight indicates how often (and suitable)\nVOLUME 8, 2020 146273\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nFIGURE 14. Example of recombination operation for a genetic algorithm.\nTABLE 6. Example of an individual of GA.\nthe preparations of the ingredients of this individual are\n(i.e. the total weight can be treated as a quality criterion).\nOn the other hand, triples with the active class indication\nshould be penalized because the most suitable preparation for\na class of ingredients is not always the best preparation for a\nparticular ingredient of this class. Another detail that we can\nrealize is that not necessarily the resultant preparation graph\nof an individual will respect the ﬁfth rule of normalization,\nabout arity, deﬁned in Section IV-C (e.g. if there was no\n‘‘add’’ after ‘‘fry’’ in the ﬁrst two ingredients of the Table 6,\nthen the ‘‘fry’’ action presents arity 2 - and only the ‘‘add’’\nand ‘‘mix’’ actions can have arity greater than 1). In order\nto quantify the quality of the preparation according to the\ncriterion of arity, it is performed a count of the actions that\ndo not follow this ﬁfth rule and it is divided by the quan-\ntity of actions in the recipe preparation (i.e. the lower this\naverage arity error, the better the quality of the recipe). Thus,\nthe evaluation of individuals in the genetic algorithm uses a\nmulti-objective criterion:\na) The ﬁrst objective was to maximize the sum of the total\nweight of the individuals (if any triple of a gene is of a\nclass of ingredients, the weight of triple falls to 10% of\nits value - for example, the total weight of the individual\nin Table 6 is 9.6).\nb) The second objective was to minimize the error of\narity and the perplexity of the language model (LM for\nmixtures).\nThis second objective can only be evaluated through the\ncomposition of the recipe using the gene vectors, from which\nwe obtain the graph of the recipe preparation. The algorithm\nused for this composition was based on the algorithm of\ndeterminization of the ﬁnite automaton (with an adaptation\nin the sense and content of the edges). Such an algorithm has\nthe following steps:\na) Creation of a graph containing the sequence of\nactions joined only in the last action, the root action\n‘‘serve_recipe’’ (depth 0).\nb) Uniﬁcation of nodes with equal content that has an\nedge with a common node (edge destination node),\neliminating the redundant edges.\nc) Evaluation of the union of the nodes in breadth (i.e. the\nneed to join all nodes of the same depth before leaping\nto achieve greater depth).\nFig. 15 shows the steps of this composition algorithm to\ngenerate the method of preparation of the recipe encoded in\nthe genes of the individual described in Table 6. The sequence\nof steps of the composition algorithm goes from the ﬁgure (a)\nthrough to (d), one ﬁgure per depth.\nFIGURE 15. Example of recipe composition.\nThe ﬁnal result of this approach, which is the recipe with\npreparation, is obtained from this algorithm of composition\napplied to each individual of the ﬁnal Hall-of-Fame of the\ngenetic algorithm.\nC. EVALUATION OF NOVELTY FOR CULINARY RECIPES\nIn order to evaluate the novelty of the preparation instructions\nof the culinary recipes, it was used the graph dissimilarity\nmetric based on the Kernel of simple paths. A simple path\nis any sequence, without repetition, of consecutive nodes\nin the graph structure. It is worth to mention that all the\npaths p must have one endpoint in the last action of the recipe\n(i.e. ‘‘serve_recipe’’) and the other endpoint in any other state\n(at any depth, including the ingredients). Example of some\nsimple paths of the graph of Fig. 15 (only those beginning\nwith the ‘‘serve_recipe’’ node) are: [‘‘serve_recipe’’, ‘‘add’’];\n[‘‘serve_recipe’’, ‘‘add’’, ‘‘salt’’]; [‘‘serve_recipe’’, ‘‘add’’,\n‘‘fry’’, ‘‘add’’]. Note that this simple path shows repetition\nof ‘‘add’’ content, however are from different nodes (i.e. the\nstructure node should not repeat); etc. The idea behind the\nKernel of simple paths technique is to create a transformation\nfunction φp that can indicate the existence (returning value\n1) or non-existence (returning value 0) of a simple path p in\na given graph [23]. Thus, in this new domain it is possible to\ncalculate the similarity between the graphs by (6) [23], and\nthe dissimilarity by (7):\nsim(G,H) =\n∑\n∀p min(φp(G),φp(H))\n∑\n∀p max(φp(G),φp(H)) (6)\ndis(G,H) =1.0 −sim(G,H) (7)\nwhere G and H are two graphs; sim(G,H) is the similarity\nvalue between G and H and dis(G,H) is their dissimilarity.\nThis distance metric (the dissimilarity) can then be used to\nevaluate the novelty of a recipe in relation to the database.\n146274 VOLUME 8, 2020\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nVI. EXPERIMENTAL SETUP\nIn this section the experimental setup is presented, with the\ncriteria, objectives, expected results and conﬁguration of the\nevaluations. The details of parameterization and training of\nLMs and Evolutionary Algorithms involved in such exper-\niments are also shown. In Section VII the results of these\nexperiments are presented.\nA. EVALUATION OBJECTIVES AND CRITERIA\nThe experiments carried out in the present work have the\nfollowing objectives: (i) to evaluate if the two architectures\nof LMs proposed for the ReProg and ReComp approaches\nwere able to absorb knowledge of the human recipe database\nthat would allow perplexity to be a good metric of qual-\nity for evaluation of recipes; and (ii) assessing whether the\nsystem as a whole (i.e. not just the LM, as in the previous\nitem), for the ReProg and ReComp approaches, was able to\ngenerate quality recipes by human assessment. Performing\nexperiments for objective (i) is necessary because there are\nno work in literature that uses LMs applied to recipes, whose\nstructure differs greatly from that of the one in texts addressed\nby them. The experiment (ii) is necessary to demonstrate,\nthrough human evaluation, that the LM quality metric reﬂects\nin quality recipes. The recipe quality criteria that LMs are\nexpected to learn varied between the two approaches, ReProg\nand ReComp, and will be presented below.\nThe ReProg approach is less restricted in terms of pos-\nsible combinations of the recipe preparation graph, but this\ngreater space of combinations allows the creation of nonvi-\nable recipes. Thus, one of the recipe quality criteria evaluated\nin the experiments is the consistency of the preparation,\nwhich is understood as the absence of sequences of incom-\npatible actions. That is, an incompatibility of an action with\nrespect to some other action that precedes it or with some\ningredient that undergoes this action. These incompatibilities\nmay be due to physical impossibilities (e.g. cutting liquid is\nan action impossible to perform) or different conditions from\nwhich the action is deﬁned, so that the action can not be\nperformed or has no effect on the preparation (e.g. frying is\nan action deﬁned only in the presence of oils).\nThe ReComp approach presents greater consistency in\nrecipe preparation at the cost of a reduction in the space of\ncombinations. Thus, it makes no sense to assess whether a\nrecipe is viable. This approach is similar to [3], however an\nLM was added with the function of promoting better mixtures\nof ingredients throughout the steps (e.g. the proper use of\nseasonings throughout the preparation). That is, the recipe\nquality criterion proposed in this model relates to the mixtures\nof ingredients throughout the steps, which are understood as\nthe points where the step sequence of two ingredients are\ncombined, which may occur in some action in the middle\nof the recipe or only in its last action (i.e. ‘‘serve_recipe’’).\nQuality mixtures for this LM are those that resemble the\nones which occur in the database of human recipes, among\nwhich we chose to evaluate: the mixture of seasonings to the\ningredients that need seasoning; and the mixture of oil to the\ningredients in actions requiring it (e.g. cooking processes by\nimmersing ingredients in oil). It is possible that there are other\nmixtures, as well as these two, in which one ingredient has a\nsecondary role (i.e. one ingredient has the function of mixing\nwith another to perform an auxiliary function).\nTwo other quality criteria for recipes are evaluated in the\nexperiments: taste perception, which consists of human eval-\nuation of the ingredients and preparation in the recipe text\nregarding the past experiences of ﬂavor (i.e. based on recipes\nthat people know, evaluate the ingredient and preparation\nmode of a new recipe); and taste itself, which involved human\nevaluation by tasting the recipe performed (i.e. taste assess-\nment of a dish).\nHaving presented the objectives and deﬁned the evaluation\ncriteria, the details of the evaluations are presented below.\nB. PARAMETRIZATION AND TRAINING\nOF THE APPROACHES\n1) RECIPE DATABASE USED IN EXPERIMENTS\nThe database of human recipes used in the experiments were\ndescribed in Section IV-A. Table 7 shows the number of\nrecipes used from each recipe source.\nTABLE 7. Database size for each recipe source.\nAll recipes (i.e. the 1939 recipes contained in the resulting\ndatabase) were normalized as explained in Section IV-C.\n2) PARAMETERIZATION OF RECIPE DECOMPOSITION\nIn the ReComp approach, the number of clusters used to\ndivide the sets of ingredients, as explained in Section V-B2,\nwas 3 (i.e. the K in the K-Medoids algorithm equals 3). In this\nway, each ingredient has a set with 3 divisions, each division\nbeing a typical preparation, raised by the K-Medoids algo-\nrithm, that this ingredient suffers in database human recipes.\nThe same was done for the class of ingredients, each class of\ningredient (e.g. ‘‘vegetables’’ for the lettuce, cabbage and etc.\ningredients) was divided into 3 clusters.\n3) LANGUAGE MODELS TRAINING\nThe language models of the two approaches, ReProg and\nReComp, were trained through K-fold cross-validation,\nwhich consists in dividing the data into K equal parts (one for\ntesting and the remaining K −1 for training) and generating\nK models (where the test portion is chosen, among these K,\ndifferent for each model). Table 8 presents the parameters of\nthis training for the LM of both approaches.\nThus, 8 LMs were created for each approach. However,\nin the experiments that involved the evaluation of the system,\nonly the LM with the lowest perplexity value of the test\nVOLUME 8, 2020 146275\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nTABLE 8. Training parameters of LMs.\nportion in the cross-validation was chosen to compose the\nsystem.\n4) PARAMETRIZATION OF EVOLUTIONARY ALGORITHMS\nFor both the PG of the ReProg approach and the AG of the\nReComp approach, the parameterization of the evolutionary\nalgorithm was presented in Table 9.\nTABLE 9. Parameters of evolutionary algorithms.\nWhere the density value of the grid of the neighbor-\nhood calculation indicates the dimensions of each grid cell\nin the objective space, where each objective is normalized\nbetween 0 and 1.\nIn addition to these parameters, the AG of the ReComp\napproach should penalize the weight of the triples derived\nfrom the sets of classes of ingredients during the process of\nrecomposing recipes. This penalty should reduce to 10% the\nvalue of the weight in these triples for the calculation of the\nﬁrst objective of the GA.\nC. EXPERIMENTAL DESIGN\nIn this section we present the types of analyses performed in\nthe experiments and a brief description of such experiments.\nThe experiments were divided into two types of analisis:\nobjective analysis (denoted by the preﬁx ‘‘OA’’), which seeks\nto evaluate the performance of a binary classiﬁer based on the\nperplexity metric (i.e. low perplexity indicating good quality\nand high indicating poor quality) in assessing recipes from\nan artiﬁcial base annotated according to the quality criteria\ndeﬁned in Section VI-A; and subjective analysis experiments\n(denoted by the preﬁx ‘‘SA’’), which consisted of human\nevaluations, in the quality criteria deﬁned in Section VI-A,\nof recipes generated by the system. Table 10 shows the\nexperiments chosen to perform these two types of analyses.\nWhere, in the ‘‘Name’’ column, names were assigned to the\nevaluation experiments; ‘‘to evaluate’’ indicates which part\nof the system, and from which approach, the evaluation was\nperformed, according to the evaluation objectives reported in\nSection VI-A; ‘‘LM used’’ indicates which language models,\nthose reported in Section VI-B3, were used in the evaluation\nTABLE 10. Summary description of the experiments.\n(if more than one, the experiment involves an evaluation for\neach LM); ‘‘Description’’ explains brieﬂy the evaluation.\nThe objective evaluation experiments (‘‘OA’’), imple-\nmented using classiﬁers, aimed to evaluate only the LMs\nalone (i.e. without the rest of the system). The subjective\nevaluation experiments (‘‘SA’’), through surveys or tasting,\nhad as purpose the evaluation of the system as a whole.\nFigures 16 and 17 present the ﬂowcharts of these two types\nof evaluation performed.\nFIGURE 16. Flowchart for objective evaluations (both approaches).\nFIGURE 17. Flowchart for subjective evaluations (both approaches).\nIn the following sections, these experiments are presented\nin more detail.\n1) EXPERIMENTS FOR THE ReProg APPROACH\nFor the LMs architecture of the ReProg approach, explained\nin Section V-A1, 4 objective evaluations, named as\nOA_cut, OA_reduce, OA_cool and OA_fry, were performed.\n146276 VOLUME 8, 2020\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nThese 4 evaluations involved using the 8 LM cross-validation,\nexplained in Section VI-B3, as binary recipe classiﬁers.\nThese classiﬁers are based on the perplexity metric, so that\nhigh perplexities indicate low quality and low perplexity indi-\ncates high quality of the recipes. The 4 evaluations consisted\nof the creation of 4 artiﬁcial recipe databases, with recipes\nselected based on the consistency of the preparation deﬁned\nin Section VI-A, and evaluation of the classiﬁers’ ability to\nseparate these recipes correctly (i.e. that the perplexity of the\n8 LMs presented high values for nonviable recipes and low\nvalues for viable recipes). The classiﬁers are evaluated by\nusing the Receiver Operating Characteristic (ROC) curve,\nwhich consists of asserting the relationship between false\npositives and true positives when sliding the class separation\nthreshold. The expected result for the ROC curve is high\nvalues of true positives and low values of false positives along\nthe whole curve, generating an Area Under the Curve (AUC)\nabove 0.5 and close to 1. It is important to note that the ﬁnal\nresult of the experiment is the average ROC curve from all\nthe LMs used (i.e. each ROC curve presented in the results\nsection is the composition of the ROC curves of the various\nmodels).\nDue to the fact that there were no works in the sense\nof evaluating inconsistencies in the preparation of recipes,\neven in the culinary literature, four intuitive inconsisten-\ncies were chosen, which derive from physical impossibil-\nities or from the culinary deﬁnition of actions. The four\ninconsistencies, and their respective artiﬁcial databases, are\ndetailed as follows:\na) OA_cut - The artiﬁcial database of this experiment is\nbased on the physical impossibility of cutting action\non liquid ingredients. This artiﬁcial database consisted\nof 50 small recipes composed of 6 cutting actions (cut,\nchop, dice, mince, slice and shred) on 25 liquid ingre-\ndients, noted as low-quality recipes, and those same\ncutting actions over 25 solid ingredients, noted as high\nquality recipes. Example of sequence of actions of\nthese small recipes: [‘‘serve_recipe’’, ‘‘cut’’, ‘‘milk’’],\n[‘‘serve_recipe’’, ‘‘chop’’, ‘‘onion’’] and etc.\nNot all the recipes created for the artiﬁcial database\nOA_cut are used for the evaluation of the classiﬁers\nvia the ROC curve. In order to make the analysis inde-\npendent of cuting actions speciﬁcities (e.g. cut shape),\nonly the least perplexing (better quality) recipes were\nused for the evaluation. For example, the recipe of the\nsequences of steps [‘‘serve_recipe’’, ‘‘chop’’, ‘‘onion’’]\nwas used in the evaluation of the classiﬁers, while the\nremaining recipes for ‘‘onion’’ (e.g. [‘‘serve_recipe’’,\n‘‘cut’’, ‘‘onion’’], [‘‘serve_recipe’’, ‘‘dice’’, ‘‘onion’’],\nand so on) were not used.\nb) OA_reduces - the artiﬁcial database of this experiment\nis based on the deﬁnition of the reduction action, which\nis the heating of liquid ingredients with the intent of\nincreasing its density (and consequently the concentra-\ntion of some ingredients). The reduce action is most\nuseful on liquid ingredients. This artiﬁcial database\nconsisted of 50 small recipes composed of the action\nreducing the same 50 ingredients of the OA_cut\ndatabase, but the annotations were reversed (i.e. the\nrecipes with liquid ingredients were noted as higher\nquality, while the recipes of solid ingredients were noted\nas of lower quality).\nc) OA_cool - The artiﬁcial database of this experiment is\nbased on the deﬁnition of cool action, which is the cool-\ning of hot ingredients or mixtures. Thus, the cool action\nis useless on frozen ingredients. This artiﬁcial database\nincluded all combinations (about 600) of recipes with\n2 cooling actions, ‘‘cool’’ and ‘‘chill’’, applied to the\nsame 50 ingredients of the OA_cut database, but hav-\ning these ingredients suffered before 2 freezing actions\n(‘‘frost’’ and ‘‘freeze’’ - recipes low quality) or 4 heating\nactions (‘‘cook’’, ‘‘heat’’, ‘‘bake’’ and ‘‘boil’’ - recipes\nhigh quality). Example of sequence of actions of these\nrecipes: [‘‘serve_recipe’’, ‘‘cool’’, ‘‘bake’’, ‘‘salmon’’],\n[‘‘serve_recipe’’, ‘‘chill’’, ‘‘frost’’, ‘‘orange_juice’’].\nAs in the OA_cut database, not all the recipes from this\nartiﬁcial database were used in the classiﬁer evaluation.\nIn order for the evaluation to be independent of the speci-\nﬁcities of the actions (cooling, freezing and heating),\ntwo recipes were chosen for each ingredient, one anno-\ntated as viable and the other noted as nonviable, with\nthe lowest perplexity values found. For example, recipes\nfor the sequences of actions [‘‘serve_recipe’’, ‘‘cool’’,\n‘‘cook’’, ‘‘chicken_broth’’], annotated as viable, and\n[‘‘serve_recipe’’, ‘‘chill’’, ‘‘freeze’’, ‘‘chicken_broth’’],\nannotated as nonviable, were used in the evalua-\ntion of the classiﬁers; while the remaining recipes\nfor ‘‘chicken_broth’’ (e.g. [‘‘serve_recipe’’, ‘‘cool’’,\n‘‘heat’’, ‘‘chicken_broth’’], [‘‘serve_recipe’’, ‘‘chill’’,\n‘‘cook’’, ‘‘chicken_broth’’], [‘‘serve_recipe’’, ‘‘cool’’,\n‘‘freeze’’, ‘‘chicken_broth’’] and etc.) were not used.\nd) OA_fry - The artiﬁcial database of this experiment is\nbased on the deﬁnition of the fry action, which con-\nsists of the process of cooking ingredients on some oil.\nThe database of artiﬁcial recipes included 5 low quality\nrecipes consisting of the fry action on 5 oils (i.e. only\nthe oils as an ingredient); and all combinations of fry\naction recipes on 12 ingredients alone (recipes also noted\nas low quality) and fry action on these 12 ingredients\nwith 5 oils (recipes noted as high quality). Example of\nrecipes: ‘‘fry’’ on ‘‘pork’’ and ‘‘olive oil’’.\nAs in the OA_cut database, not all the recipes from this\nartiﬁcial database were used in the classiﬁer evaluation.\nFor the evaluation to be independent of the speciﬁc oil\nused in the fry process, only for the recipes contain-\ning ingredients with oils, recipes were chosen the with\nthe lowest values of perplexity found (better quality)\nfor each ingredient. For example, the ‘‘fry’’ recipe for\n‘‘bacon’’ and ‘‘oil’’ was used to evaluate the classiﬁers,\nwhile the remaining recipes for ‘‘bacon’’ and some other\noil (e.g. ‘‘fry’’ on ‘‘bacon’’ and ‘‘canola oil’’ ) were not\nused.\nVOLUME 8, 2020 146277\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nThe recipes generated by the (complete) system were also\nevaluated. However, this evaluation was subjective (i.e. by\na human assessment) conducted through a survey, named\nSA_feasible.5 The LM used in the system that generated the\nrecipes of this survey was the model with the best perplexity\nof the test in the cross-validation presented in Section VI-B3.\nTABLE 11. Size of the recipes generated for theSA_feasible experiment.\nThe choice of the recipes to compose the SA_feasible\nevaluation survey was as follows: 18 recipes were chosen,\n9 of the database of human recipes and the other 9 generated\nby the system. Each of these groups of 9 recipes was divided\ninto 3 parts, containing 3 small, medium and large recipes.\nThese recipe sizes are described in Table 11. For each of\nthese recipe sizes, the system was run 3 times for 5 different\ningredient lists and the 3 recipes that appeared to be the\nmost viable among all generated were chosen. It should be\nnoted that the number of recipes generated by the system for\neach execution is not very large (i.e. Hall-of-Fame is small\ncompared to the PG population), being of the order of the\nmaximum depth chosen for the recipes (in this case, as seen\nin Table 11, was 5, 6 and 8 - that is, at most 8 recipes).\nThus, although it involved 3 runs of the system, the number of\nrecipes generated was small compared to the space of possible\npreparation combinations (which even in the smallest case\nis exponential - for example, soup space with 4 ingredients\nsuffering a single action per ingredient, out of 90 possible\nactions, is in the order of 90 4 combinations).\nFIGURE 18. A recipe created by the ReProg approach (in graph form).\nAfter choosing 9 human recipes and 9 recipes generated by\nthe system, a textual description was made for the preparation\ngraph in order to write the 18 recipes in natural language.\nFigures 18 and 19 exemplify these two versions, graph and\ntextual, of a these recipes. Some questions in this survey\nwere based on [24], but the questions related to SA_feasible\nassessment were as follows:\na) Is the recipe presented feasible?\nb) With respect to the ﬂavor, is the recipe tasty?\n5This survey is in Portuguese and can be accessed at:\nhttps://goo.gl/forms/nSI9HzVqo8meymWv2\nFIGURE 19. A recipe created by the ReProg approach (in textual form).\n2) EXPERIMENTS FOR THE ReComp APPROACH\nFor the LMs architecture of the ReComp approach, explained\nin Section V-B1, two objective evaluations, named as OA_oil\nand OA_seasoning, were performed. As in Section VI-C1,\nthese 2 evaluations involved using the 8 LM of cross-\nvalidation as binary classiﬁers of recipes based on perplex-\nity metrics. The ROC curve was calculated to evaluate the\nperformance of these classiﬁers. The criterion used to note\nthe recipes from the artiﬁcial databases of these 2 evaluations\nwas the criterion of mixture quality of ingredients throughout\nthe preparation deﬁned in Section VI-A (i.e. recipes noted as\nhaving high mixture quality or low mixture quality).\nThe two artiﬁcial databases created to evaluate the classi-\nﬁers were detailed as follows:\na) OA_oil - The artiﬁcial database of this experiment is\nbased on the secondary role of the oil ingredient in frying\nof other ingredients (i.e. mixtures of oil and other ingre-\ndients promoted by fry action). Thus, the oil mixtures\nwith other ingredients in frying consists of mixtures\nof good quality, while the mixtures of ingredients that\ncontains no oil in frying consists of mixtures of poor\nquality. The artiﬁcial database has 192 small recipes\nconsisting of the fry action on 2 ingredients, the ﬁrst\none being one of 12 ingredients and the second one may\nbe one of the other 11 remaining ingredients, for bad\nmixture quality recipes, or one of 5 oils, for good mixture\nquality recipes. Example recipes: ‘‘fry’’ on ‘‘onion’’ and\n‘‘pepper’’ (poor mixture quality); and ‘‘fry’’ on ‘‘onion’’\nand ‘‘vegetable_oil’’ (good mixture quality).\nNot all the recipes from this artiﬁcial database were\nused in the classiﬁer evaluation. For the evaluation to\nbe independent of the second ingredient of the mixture,\nit added some other ingredient or speciﬁc oil, for each\ningredient in the ﬁrst position of the mixture two recipes\nwere chosen, one noted as good mixture quality and\nthe other noted as poor mixture quality, with the lowest\nperplexity values found. For example, the ‘‘fry’’ on the\nmixture of ‘‘pork’’ and ‘‘oil’’ (good mixture quality) and\nthe ‘‘fry’’ on the mixture of ‘‘pork’’ and ‘‘potato’’ (poor\nmixture quality) were selected, while all recipes with\n‘‘fry’’ on the ‘‘pork’’ mixture with any other ingredient\nwere not used.\nb) OA_seasoning - The artiﬁcial database of this experi-\nment is based on the secondary role of seasoning ingre-\ndients in frying of other ingredients (i.e. mixtures of\n146278 VOLUME 8, 2020\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nseasonings and other ingredients, which bring season-\ning, promoted by fry action). Like the OA_oil experi-\nment, mixtures with these secondary ingredients, in this\ncase seasonings, frying with other ingredients consist\nof good quality mixtures, whereas the absence of these\nsecondary ingredients consists of poor quality mixtures.\nThe artiﬁcial database has 117 small recipes composed\nby the fry action on 2 ingredients, the ﬁrst one being\none of the 9 ingredients and the second can be one\nof the other 8 remaining ingredients (for bad mixture\nquality recipes) or one of 5 seasonings (for good mixture\nquality recipes). Example of recipes: ‘‘fry’’ on ‘‘beef’’\nand ‘‘salmon’’ (poor mixture quality); and ‘‘fry’’ on\n‘‘beef’’ and ‘‘garlic’’ (good mixture quality).\nAs with OA_oil, not all artiﬁcial database recipes were\nused in classiﬁer evaluation. In order to avoid speciﬁci-\nties of the second ingredient of the mixture, two recipes\nwere chosen for each ingredient in the ﬁrst position of\nthe mixture, one noted as good mixture quality and the\nother one noted as poor mixture quality, with the lowest\nperplexity values found. For example, the ‘‘fry’’ on the\n‘‘egg’’ and ‘‘salt’’ (good mixture quality) and the ‘‘fry’’\non the ‘‘egg’’ and ‘‘bacon’’ (poor mixture quality) were\nselected, while all recipes with ‘‘fry’’ on the ‘‘egg’’ with\nany other ingredient were not used.\nIn this approach we also evaluated the recipes generated\nby the system. Two evaluations were done by humans, one\nwas performed through one survey and the other through\ntasting of a recipe executed, these experiments were named as\nSA_mixture6 and SA_dish respectively. The model used in the\nsystem that generated the recipes for these evaluations was\nthe model with the best perplexity of the test in the cross-\nvalidation presented in Section VI-B3.\nThe choice of recipe to compose the SA_mixture evaluation\nsurvey was as follows: there were 6 recipes generated by\nthe system, divided into 3 pairs. Each pair contains recipes,\ndenoted by A and B in the survey, which have the same list\nof ingredients, but with different preparation steps. Recipe\nA was generated by the system proposed in this approach,\nincluding LM, while the recipe B was generated by the system\nproposed in this approach, but without the use of an LM. That\nis, in this second recipe we used a completely similar system\nto the one proposed in [3], without the aid of a LM to guide\nthe GA toward recipes with better mixtures of ingredients.\nAfter choosing the 6 recipes generated by the system,\na textual description was made for the preparation graph in\norder to write them in natural language. The questions present\nin the survey were as follows:\na) Which recipe (A or B) uses seasoning and oils that better\nmatch the ingredients?\nb) With respect to the ﬂavor, is the recipe tasty?\nA recipe for this approach, using LM, was chosen to\nbe executed in the SA_dish experiment because in the\n6This survey is in Portuguese and can be accessed at:\nhttps://goo.gl/forms/5lsrK3pib6Qxgu612\ncomparison of the results of SA_feasible and SA_mixture sur-\nveys, which will be shown in Section VII, this conﬁguration\nof the system was the one that presented the most promising\nresults of taste perception. The choice of only this approach\nis due to the higher cost of this experiment. The choice of\nthe recipe to be executed was as follows: 4 lists of creative\ningredients were generated with 6 to 7 common ingredients\nfound in Brazil. The system run for each of these 4 lists of\ningredients and the recipe with the lowest arity error was\nselected by ingredient list. Among the 4 selected recipes,\nwe chose the recipe with the preparation graph apparently\nmore consistent and with better mixture quality of the ingredi-\nents. The textual description of this recipe is shown in Fig. 20.\nFIGURE 20. Recipe generated by the system that was cooked (in textual\nform).\nThe SA_dish evaluation consisted of executing the recipe\nof Fig. 20 and presenting it for a small group of volunteers to\ntaste. After eating the dish, each volunteer indicated whether\nthe dish is tasty from a note on the following numerical\nscale: 1 means very bad, could not ﬁnish eating; 2, bad; 3,\nmore or less; 4, good; and 5 means very good, would even\nrepeat.\nVII. RESULTS\nA. RESULTS FOR THE ReProg APPROACH\nThe results for the objective analysis through classiﬁers\nexplained in Section VI-C, are presented in Fig. 21, where the\ngraphs from (a) to (d) are relative to the OA_cut, OA_reduce,\nOA_cool and OA_fry experiments, respectively; AUC (Area\nUnder Curve) summarizes the efﬁciency of the classiﬁer\n(1.00 is the ideal value); and ‘‘std. dev.’’ means the region\nwhich comprises up to a standard deviation of the mean curve\n(i.e. the value of the standard deviation along the mean curve).\nFor all 4 experiments in Fig. 21, an AUC greater than\n0.83 was obtained, which is considered a good value (well\nabove 0.50). The model (and the perplexity metric) were\nable to separate the viable recipes from the nonviable ones,\nso that there were several thresholds capable of bringing\nmost of the true positives (cases where the threshold and\nVOLUME 8, 2020 146279\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nFIGURE 21. ROC curves for the classifiers of the ReProg approach.\nlabel hit) without bringing many false positives. Particularly,\nthe classiﬁer of experiment OA_cool was practically perfect,\nobtaining an AUC of 0.99. One of the factors that may have\nled to this inconsistency to be better classiﬁed is the fact that\nit happens between actions, which occurs more frequently\nthan ingredients in the database (in addition to the vocabulary\nof actions being smaller than that of ingredients, each recipe\npresents, in general, more actions than ingredients), and thus\nthe amount of training data is more robust. Another detail that\nwe can see in the experiments of Fig. 21 is that the mean\ndeviation in the OA_fry experiment was the highest of these\n4 experiments, with a value of 0.05. This was mainly due\nto two reasons: i) some recipes presuppose the use of oil\nfor the ‘‘fry’’ action, leaving this ingredient often implicit;\nand ii) some recipes treat the action of preheating the oil\nfor frying as a different ‘‘fry’’ action, such as ‘‘heat’’ action,\nwhich makes the language model does not treat oil as a\ndirect ingredient of frying in these recipes. Since these two\nproblems depend on the origin of the recipes, they may\nvary throughout the folds of the cross-validation, generating\nmodels with adverse quality between the folds that explain\nthis greater variation.\nRegarding the second type of analysis, the experiment\nSA_feasible, a total of 36 respondents completed the survey.\nThe relative amount of positive evaluations regarding fea-\nsibility for each group of recipe size, which were deﬁned\nin Table 11, is shown in Fig. 22. We can see that both groups\nof recipes generated by the system and those generated by\nhumans had satisfactory values, being considered feasible by\nat least 78% of the evaluations. Overall, human evaluations\nhad slightly better results (5% to 6%), with the exception\nof the medium-sized (6-ingredient) recipes group. The aver-\nage evaluation for taste perception, where 0% means bad\ntaste and 100% means good taste is presented in Fig. 23\nfor each group of recipe sizes. The human-generated recipes\nhad satisfactory values, presenting an evaluation of at least\n57% (between medium and good taste). Larger-sized human\nrecipes achieved the best result, with an average of 82% (close\nto good taste), at least 22% above the other sizes. For the small\nFIGURE 22. Result of evaluating the recipes as feasible (ReProg\napproach).\nFIGURE 23. Result of evaluating the recipes as tasty (SA_feasible-\nReProg approach).\nand medium-sized human recipes, the result were very close,\nonly 3% difference. Thus, there seems to be a tendency to\nbetter evaluate the taste perception of larger recipes, possibly\nbecause people consider these recipes more complete. The\nsame behavior seems to occur, to a lesser extent, between\nlarge and medium-sized recipes generated by the system.\nLarger-sized recipes had an average evaluation score of 37%\n(medium to bad taste), while medium-sized recipes had a 23%\n(also medium to bad taste) result, 14% below the larger size.\nOn the other hand, the smallest recipes got a better result than\nboth, with an average evaluation of 49% (with a medium taste\nperception). This is due to the fact that the search space for the\ngeneration of preparation steps is smaller for the recipes with\nfewer ingredients, making the algorithm reach better recipes\n(with respect to preparation steps) faster than in the recipes\nwith more ingredients. This second factor conﬂicts with the\npredilection factor for larger recipes sizes.\nAnother important detail is that these 9 system-generated\nrecipes (the recipes of experiment SA_feasible) presented a\ndissimilarity more than 75% in relation to any recipe in the\ndatabase, which means that the recipes are also novel.\nB. RESULTS FOR THE ReComp APPROACH\nThe results for the ﬁrst type of analysis in this approach\nare presented in Fig. 24 (the ROC curves of the model as a\nclassiﬁer). For the classiﬁer of OA_oil an AUC of 0.97 was\nobtained, that is, the classiﬁer was almost perfect. As for\nOA_seasoning, an AUC 0.77 was obtained, which is a good\nperformance. For this second classiﬁer there was a deviation\nfrom the high average of 0.07, probably due to the fact\nthat the number of seasonings (salt, herbs, spices and even\n146280 VOLUME 8, 2020\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nFIGURE 24. ROC curves for the classifiers of the ReComp approach.\nsome oil infusions) is higher than that of frying oils. In this\nway, the model for each fold of the cross-validate would\nhave greater difﬁculty in generalizing the seasonings than the\noils, leading to adverse (specialized) performances for each\nsection of the training database.\nComparing the OA_oil with the OA_fry, presented in\nthe previous section, we can see an improvement in the\nperformance of the classiﬁer and reduction of the mean\ndeviation. Between these two classiﬁers, the recipes were\nalmost all the same (with the exception of nonviable recipes\nwith only one oil as an ingredient) the Language Mod-\nels were of different architecture. The language model of\nOA_oil is able to treat oil as an ingredient of frying even\nthough it is not a direct ingredient of the frying action,\nwhich reinforces the previously reported problem for the\nlargest deviation of the mean in OA_fry. Thus, the ReComp\napproach model is more robust than the ReProg approach\nin these cases. However, the AUC of the ReProg approach\npresented better overall results, with a minimum value\nfound of 0.83 compared to 0.77 of ReComp. However, both\napproaches still obtained quite satisfactory results, which\ndemonstrates that Language Models were able to learn, based\non human recipes, how to separate good quality recipes\nfrom those of poor quality (in the criteria adopted for the\nexperiments).\nAs can be seen in Section VI-C, the second type of anal-\nysis performed for this approach relied on two experiments:\nSA_mixture and SA_dish. The SA_mixture experiment, which\nconsisted of a survey, was completed by a total of 31 respon-\ndents. The average evaluation for the quality of the use of oils\nand seasonings between the two versions (generated by the\nsystem with and without language models) of each recipe is\npresented in Fig. 25, where 0% means that the quality of the\nrecipe is bad and 100% means that the quality is good on this\ncriterion. In general, the versions of the recipes generated by\nthe system with language model had the best results, having\nan evaluation of at least 52% and obtaining for the three\nrecipes an average of 63.7%. The recipes generated by the\nversion of the system without language models presented an\naverage quality score of 36.3%. As can be seen, the quality\nassessment between the two groups of recipes is complemen-\ntary (i.e. the sum gives 100%), this being because respondents\nwere asked to benchmark the quality of one group over the\nother. These results demonstrate that the language model can\nassist the system in generating recipes with better mixtures\nof the ingredients along the preparation steps (in this case,\nFIGURE 25. Result of evaluating the recipes as mixture quality (ReComp\napproach).\nFIGURE 26. Result of evaluating the recipes as tasty (SA_mixture-\nReComp approach).\nFIGURE 27. Dish for taste evaluation by volunteers.\nmixtures containing oils and seasonings). The average taste\nperception for these same recipes are shown in Fig. 26.\nIn recipes pairs 1 and 3, for both systems, the result obtained\nwas between medium and good, while for the recipe pair 2 it\nwas between medium and bad. But, in general, the recipes\nof the system with language model obtained a better average\ntaste perception, with an average value for the three recipes\nof 64.3%, between medium and good.\nThe SA_dish, which consists of tasting a recipe generated\nby the system with language model, had the participation\nof 18 volunteers. The amounts selected for the ingredients\nwere: 3 egg whites; salt to taste; 1 leaf of mint per serving;\n3 chicken tenderloin; 150 grams of cream cheese; and a pinch\nof saffron (just to color the cream cheese mixture). The time\nto bake the cream cheese mix was 50 minutes at 200 degrees\ncelsius. The chicken was fried until golden. The dish was like\nthat of Fig. 27.\nVOLUME 8, 2020 146281\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\nThe average score obtained in the evaluation of the vol-\nunteers was 4.72 out of 5, between good and very good\n(93% of the taste rating scale used in this experiment). This\nresult demonstrates that the proposed system not only gen-\nerates recipes with a good taste perception (results from the\nexperiment SA_mixture), but also was able to generate a tasty\ncreative recipe.\nThe 7 recipes generated by the system with this approach\n(the recipes of experiment SA_mixture and SA_dish) had a\ndissimilarity greater than 70% in relation to any recipe of the\ndatabase. This demonstrates that this approach was also able\nto generate the preparation instructions with novelty.\nVIII. CONCLUSION\nThe objective evaluations, using classiﬁers based on language\nmodels, have demonstrated the quality of such models in eval-\nuating nonviable recipes and bad mixtures of ingredients in\nthe preparation steps (e.g. bad use of seasonings), with AUC\nvalues above 0.83 and AUC of at least 0.77, respectively.\nIn the evaluation by online survey, an average evaluation of\nthe recipes as feasible of 85.6% and an evaluation of the good\nuse of oils and seasonings with an average of 63.6% (between\nmedium and good) were obtained. These results demonstrate\nthat language models and perplexity metrics can guide the\nrecipe generation algorithm toward recipes with more con-\nsistent preparation steps, in the case of a less restricted gen-\neration approach (ReProg Approach), Language Models also\nguide the algorithms for yielding recipes with better mixtures\nof ingredients throughout the preparation steps, in the case\nof the approach with the consistency of the preparation more\nguaranteed (ReComp Approach). Regarding the taste percep-\ntion of the generated recipes, only an intermediate result for\nsmall recipes was obtained in the ReProg Approach, with an\naverage taste perception around 49% of the scale, while for\nthe ReComp Approach the results were better. In the ReComp\nApproach, the average evaluation of taste perception was\naround 64.3% of the scale, between medium and good, and\nthe average taste of a cooked recipe was 93% of the scale,\nclose to good taste.\nIn addition, recipes generated from both approaches pre-\nsented a dissimilarity for any database recipe by at least 70%,\neven for the ReComp Approach, which were based on a direct\ndecomposition of the database recipes.\nIn general, the ReComp approach presented the best results\nand it was possible to achieve the goal of creating a system\ncapable of generating complete creative recipes (i.e. with a\ncreative ingredient list, a sequence of consistent preparation\nsteps and a good use of oils and seasoning in the preparation).\nIX. FUTURE WORK\nAs a future work, we propose to improve the database of\nrecipes by adding more recipes than the 1939 used in the\npresent work and improving the quality of the NLP Parser\n(e.g. by adding specialized ingredient inference strategies for\nevery possible action in the ontology of actions). For the\narchitecture of the language model, one could try to overcome\nthe problem of exponential growth of the information con-\nsidered in the prediction of the actions of the preparation\ngraph for a number of time steps greater than 1, so as to\nmake it possible to use an architecture based on recurrent\nneural networks (e.g. LSTM). A solution in this sense could\nbe the creation of a criterion of importance for the actions\nof the preparation in order to reduce the size of the branch\nconsidered in the prediction of the model. Another possibility\nwould be to evaluate other types of models (e.g. Bayesian\nmodels) to evaluate the preparation steps of the recipes and\nmixtures of the ingredients throughout these steps. In the\nAPI used to generate the list of ingredients, it will be useful\nto make changes so that the user can choose ingredients\naccording to their category (e.g. meats, vegetables, seasoning\nand etc) and kitchen templates (e.g. Italian, French and etc).\nACKNOWLEDGMENT\nThe authors would like to thank CAPES, PUC Minas, CNPq,\nand FAPEMIG for the ﬁnancial support.\nREFERENCES\n[1] R. G. Morris, S. H. Burton, P. Bodily, and D. Ventura, ‘‘Soup over bean of\npure joy: Culinary ruminations of an artiﬁcial chef,’’ in Proc. ICCC, 2012,\npp. 119–125.\n[2] R. K. Sawyer, Explaining Creativity : The Science of Human Innovation.\nLondon, U.K.: Oxford Univ. Press, 2011.\n[3] F. Pinel, L. R. Varshney, and D. Bhattacharjya, ‘‘A culinary computational\ncreativity system,’’ in Computational Creativity Research: Towards Cre-\native Machines. Paris, France: Springer, 2015, pp. 327–346.\n[4] K. Grace and M. L. Maher, ‘‘Surprise-triggered reformulation of design\ngoals,’’ in Proc. AAAI, 2016, pp. 3726–3732.\n[5] A. Amorim, L. F. W. Góes, A. R. D. Silva, and C. França, ‘‘Creative\nﬂavor pairing: Using RDC metric to generate and assess ingredients com-\nbinations,’’ in Proc. Lighth Int. Conf. Comput. Creativity, ICCC, Atlanta,\nGeorgia, 2017, pp. 1–8.\n[6] Y .-Y . Ahn, S. E. Ahnert, J. P. Bagrow, and A.-L. Barabási, ‘‘Flavor net-\nwork and the principles of food pairing,’’ Sci. Rep., vol. 1, no. 1, p. 196,\nDec. 2011.\n[7] G. M. Shepherd, Neurogastronomy: How Brain Creates Flavor Why it\nMatters. New York, NY , USA: Columbia Univ. Press, 2011.\n[8] M. Zampini and C. Spence, ‘‘The role of auditory cues in modulating the\nperceived crispness and staleness of potato chips,’’ J. Sensory Stud., vol. 19,\nno. 5, pp. 347–363, Oct. 2004.\n[9] K. Grace, M. L. Maher, D. Fisher, and K. Brady, ‘‘Data-intensive evaluation\nof design creativity using novelty, value, and surprise,’’ Int. J. Design\nCreativity Innov., vol. 3, nos. 3–4, pp. 125–147, Oct. 2015.\n[10] C. França, L. F. W. Góes, A. Amorim, R. Rocha, and A. R. D. Silva,\n‘‘Regent-dependent creativity: A domain independent metric for the\nassessment of creative artifacts,’’ inProc. 7th Int. Conf. Comput. Creativity,\n2016, pp. 68–75.\n[11] L. Macedo and A. Cardoso, ‘‘The exploration of unknown environments\npopulated with entities by a surprise–curiosity-based agent,’’ Cognit. Syst.\nRes., vol. 19, pp. 62–87, Sep. 2012.\n[12] P. Baldi and L. Itti, ‘‘Of bits and wows: A Bayesian theory of surprise\nwith applications to attention,’’ Neural Netw., vol. 23, no. 5, pp. 649–666,\nJun. 2010.\n[13] J. Billing and P. W. Sherman, ‘‘Antimicrobial functions of spices: Why\nsome like it hot,’’ Quart. Rev. Biol., vol. 73, no. 1, pp. 3–49, Mar. 1998.\n[14] G. A. Fink, Markov Models for Pattern Recognition : From Theory to\nApplications. London, U.K.: Springer, 2014.\n[15] E. Arisoy and M. Saraçlar, ‘‘Multi-stream long short-term memory neural\nnetwork language model,’’ in Proc. 16th Annu. Conf. Int. Speech Commun.\nAssoc., 2015, pp. 1413–1417.\n[16] R. Masumura, T. Asami, T. Oba, H. Masataki, S. Sakauchi, and A. Ito,\n‘‘Latent words recurrent neural network language models,’’ in Proc. 16th\nAnnu. Conf. Int. Speech Commun. Assoc., 2015, pp. 2380–2384.\n146282 VOLUME 8, 2020\nW. A. dos Santoset al.: Creative Culinary Recipe Generation Based on Statistical Language Models\n[17] K. Irie, R. Schlüter, and H. Ney, ‘‘Bag-of-words input for long history\nrepresentation in neural network-based language models for speech recog-\nnition,’’ in Proc. 16th Annu. Conf. Int. Speech Commun. Assoc. , 2015,\npp. 1–5.\n[18] X. Huang, A. Acero, H.-W. Hon, and R. Reddy, Spoken Language Pro-\ncessing : A Guide to Theory, Algorithm, and System Development, vol. 95.\nUpper Saddle River, NJ, USA: Prentice-Hall, 2001.\n[19] E. Cromwell, J. Galeota-Sprung, and R. Ramanujan, ‘‘Computational cre-\nativity in the culinary arts,’’ in Proc. FLAIRS Conf., 2015, pp. 38–42.\n[20] A. Cordier, J. Lieber, P. Molli, E. Nauer, H. Skaf-Molli, and Y . Toussaint,\n‘‘Wiki-Taaable: A semantic wiki as a blackboard for a textual case-based\nreasoning system,’’ in Proc. 4th Workshop Semantic Wikis (SemWiki). 6th\nEur. Semantic Web Conf., Heraklion, 2009, pp. 88–101.\n[21] R. T. Ng and J. Han, ‘‘CLARANS: A method for clustering objects for\nspatial data mining,’’ IEEE Trans. Knowl. Data Eng. , vol. 14, no. 5,\npp. 1003–1016, Sep. 2002.\n[22] J. B. Kruskal, ‘‘An overview of sequence comparison: Time warps, string\nedits, and macromolecules,’’ SIAM Rev., vol. 25, no. 2, pp. 201–237,\nApr. 1983.\n[23] L. Ralaivola, S. J. Swamidass, H. Saigo, and P. Baldi, ‘‘Graph kernels\nfor chemical informatics,’’ Neural Netw., vol. 18, no. 8, pp. 1093–1110,\nOct. 2005.\n[24] L. M. R. D. Souza and L. F. W. Góes, ‘‘Evaluation of the human perception\nof creativity on the combinations generated by the creative food pair-\ning system,’’ Dept. Inf. Syst., Pontifícia Universidade Católica de Minas\nGerais, Belo Horizonte, Brazil, Tech. Rep., 2017.\nWILLIAN ANTÔNIO DOS SANTOS(Member,\nIEEE) was born in Belo Horizonte, Minas Gerais,\nBrazil, in 1988. He received the B.S. degree in\ncomputer science and the M.S. degree in elec-\ntrical engineering from the Pontiﬁcal Catholic\nUniversity of Minas Gerais (PUC Minas), Belo\nHorizonte, in 2018. His current research interests\ninclude automatic speech recognition and compu-\ntational creativity.\nJOÃO RIBEIRO BEZERRA(Member, IEEE) was\nborn in Belo Horizonte, Minas Gerais, Brazil,\nin 1994. He received the B.S. degree in computer\nscience from the Pontiﬁcal Catholic University\nof Minas Gerais (PUC Minas), Belo Horizonte,\nin 2018. His research interests include artiﬁcial\nintelligence and applications in natural language\nprocessing.\nLUÍS FABRÍCIO WANDERLEY GÓES (Mem-\nber, IEEE) received the Ph.D. degree in computer\nscience from the University of Edinburgh, U.K.,\nin 2012. His main research interests include paral-\nlel programming and computational creativity.\nFLÁVIA MAGALHÃES FREITAS FERREIRA\nreceived the Ph.D. degree in electrical engineer-\ning from the Pontiﬁcal Catholic University of\nRio de Janeiro, Brazil, in 2004. Her research\ninterests include image, video and voice process-\ning, digital processing in hardware, and scientiﬁc\nvisualization.\nVOLUME 8, 2020 146283",
  "topic": "Recipe",
  "concepts": [
    {
      "name": "Recipe",
      "score": 0.9691400527954102
    },
    {
      "name": "Perplexity",
      "score": 0.7094851732254028
    },
    {
      "name": "Computer science",
      "score": 0.6776707172393799
    },
    {
      "name": "Language model",
      "score": 0.5393357872962952
    },
    {
      "name": "Natural language processing",
      "score": 0.4383874237537384
    },
    {
      "name": "Metric (unit)",
      "score": 0.4236876368522644
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4014746844768524
    },
    {
      "name": "Engineering",
      "score": 0.1154053807258606
    },
    {
      "name": "Food science",
      "score": 0.08344772458076477
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170935008",
      "name": "Pontifícia Universidade Católica de Minas Gerais",
      "country": "BR"
    }
  ]
}