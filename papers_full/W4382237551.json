{
    "title": "Pointerformer: Deep Reinforced Multi-Pointer Transformer for the Traveling Salesman Problem",
    "url": "https://openalex.org/W4382237551",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2054851606",
            "name": "Yan Jin",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4382230529",
            "name": "Yuandong Ding",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4382230527",
            "name": "Xuanhao Pan",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2116048628",
            "name": "Kun He",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2117608616",
            "name": "Li Zhao",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A1719067107",
            "name": "Tao Qin",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2048214674",
            "name": "Lei Song",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A1985857611",
            "name": "Jiang Bian",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2054851606",
            "name": "Yan Jin",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4382230529",
            "name": "Yuandong Ding",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4382230527",
            "name": "Xuanhao Pan",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2116048628",
            "name": "Kun He",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2117608616",
            "name": "Li Zhao",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1719067107",
            "name": "Tao Qin",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2048214674",
            "name": "Lei Song",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1985857611",
            "name": "Jiang Bian",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2111227040",
        "https://openalex.org/W2033320243",
        "https://openalex.org/W626292722",
        "https://openalex.org/W2002425996",
        "https://openalex.org/W2944956154",
        "https://openalex.org/W2117226423",
        "https://openalex.org/W2607264901",
        "https://openalex.org/W3115357289",
        "https://openalex.org/W1566652554",
        "https://openalex.org/W2008806816",
        "https://openalex.org/W2737740651",
        "https://openalex.org/W2901154086",
        "https://openalex.org/W6941249815",
        "https://openalex.org/W1597286183",
        "https://openalex.org/W6771626834",
        "https://openalex.org/W3132134635",
        "https://openalex.org/W3097232108",
        "https://openalex.org/W3175764193",
        "https://openalex.org/W2015164669",
        "https://openalex.org/W3203997865",
        "https://openalex.org/W3027899525",
        "https://openalex.org/W1561418801",
        "https://openalex.org/W1546369142",
        "https://openalex.org/W2787189213",
        "https://openalex.org/W2647779349",
        "https://openalex.org/W2029242918",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W6683861385",
        "https://openalex.org/W1739538851",
        "https://openalex.org/W2155027007",
        "https://openalex.org/W6725207838",
        "https://openalex.org/W2119717200",
        "https://openalex.org/W3113123584",
        "https://openalex.org/W3214598513",
        "https://openalex.org/W2163428398",
        "https://openalex.org/W4226053965",
        "https://openalex.org/W4295838474",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4230563027",
        "https://openalex.org/W2952332632",
        "https://openalex.org/W3208692779",
        "https://openalex.org/W4295138992",
        "https://openalex.org/W2948433391",
        "https://openalex.org/W3213077827",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3177337956"
    ],
    "abstract": "Traveling Salesman Problem (TSP), as a classic routing optimization problem originally arising in the domain of transportation and logistics, has become a critical task in broader domains, such as manufacturing and biology. Recently, Deep Reinforcement Learning (DRL) has been increasingly employed to solve TSP due to its high inference efficiency. Nevertheless, most of existing end-to-end DRL algorithms only perform well on small TSP instances and can hardly generalize to large scale because of the drastically soaring memory consumption and computation time along with the enlarging problem scale. In this paper, we propose a novel end-to-end DRL approach, referred to as Pointerformer, based on multi-pointer Transformer. Particularly, Pointerformer adopts both reversible residual network in the encoder and multi-pointer network in the decoder to effectively contain memory consumption of the encoder-decoder architecture. To further improve the performance of TSP solutions, Pointerformer employs a feature augmentation method to explore the symmetries of TSP at both training and inference stages as well as an enhanced context embedding approach to include more comprehensive context information in the query. Extensive experiments on a randomly generated benchmark and a public benchmark have shown that, while achieving comparative results on most small-scale TSP instances as state-of-the-art DRL approaches do, Pointerformer can also well generalize to large-scale TSPs.",
    "full_text": "Pointerformer: Deep Reinforced Multi-Pointer Transformer\nfor the Traveling Salesman Problem\nYan Jin1, Yuandong Ding1, Xuanhao Pan1, Kun He1,3*, Li Zhao2, Tao Qin2, Lei Song2, Jiang Bian2\n1School of Computer Science, Huazhong University of Science and Technology, China\n2Microsoft Research Asia\n3HopcroftCenter on Computing Science, Huazhong University of Science and Technology, China\nfjinyan, yuandong, xhpan, brooklet60 g@hust.edu.cn, flizo, taoqin, lei.song, jiang.biang@microsoft.com\nAbstract\nTraveling Salesman Problem (TSP), as a classic routing op-\ntimization problem originally arising in the domain of trans-\nportation and logistics, has become a critical task in broader\ndomains, such as manufacturing and biology. Recently, Deep\nReinforcement Learning (DRL) has been increasingly em-\nployed to solve TSP due to its high inference efﬁciency. Nev-\nertheless, most of existing end-to-end DRL algorithms only\nperform well on small TSP instances and can hardly generalize\nto large scale because of the drastically soaring memory con-\nsumption and computation time along with the enlarging prob-\nlem scale. In this paper, we propose a novel end-to-end DRL\napproach, referred to as Pointerformer, based on multi-pointer\nTransformer. Particularly, Pointerformer adopts both reversible\nresidual network in the encoder and multi-pointer network in\nthe decoder to effectively contain memory consumption of\nthe encoder-decoder architecture. To further improve the per-\nformance of TSP solutions, Pointerformer employs a feature\naugmentation method to explore the symmetries of TSP at\nboth training and inference stages as well as an enhanced\ncontext embedding approach to include more comprehensive\ncontext information in the query. Extensive experiments on\na randomly generated benchmark and a public benchmark\nhave shown that, while achieving comparative results on most\nsmall-scale TSP instances as state-of-the-art DRL approaches\ndo, Pointerformer can also well generalize to large-scale TSPs.\nIntroduction\nThe Traveling Salesman Problem (TSP) is a well-known com-\nbinatorial optimization problem. It can be stated as follows:\ngiven a set of cities/nodes, a salesman departing from one city\nneeds to traverse all other cities exactly once and ﬁnally re-\nturns to the start city. The objective of TSP is to ﬁnd the short-\nest route for the salesman. In addition to its well-recognized\ntheoretical importance as a classic combinatorial optimiza-\ntion problem, TSP also has a wide range of real-world ap-\nplications, such as drilling of printed circuit boards (Alkaya\nand Duman 2013), X-Ray crystallography (Bland and Shall-\ncross 1989), warehouse order picking (Madani, Batta, and\nKarwan 2020), transport routes optimization (Hacizade and\nKaya 2018), and many others (Matai, Singh, and Lal 2010).\n*Corresponding author.\nCopyright © 2023, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nDue to both of its theoretical and practical importance,\nTSP has attracted a great number of research efforts in the\npast decades that attempted to address it using either exact\nor heuristic algorithms. In fact, the NP-hardness nature of\nTSP makes it computationally intractable to leverage exact\nalgorithms to ﬁnd the optimal solutions over a large-scale\nTSP, since the corresponding computation complexity in-\ncreases exponentially with respect to the number of nodes.\nHence, facing most real-world TSP applications, heuristic al-\ngorithms are usually adopted to obtain near-optimal solutions.\nHowever, to ensure achieving high-quality solutions, a few\nheuristic algorithms are designed to further rely on ﬁne-tuned\nsearch strategies, which may signiﬁcantly increase the time\ncomplexity for solving large-scale TSP.\nRecently, there have been a soaring number of studies try-\ning to solve TSP using Deep Learning (DL) algorithms with\neither Supervised Learning (SL) or Reinforcement Learning\n(RL) (Vinyals, Fortunato, and Jaitly 2015; Nowak et al. 2017;\nKool, van Hoof, and Welling 2018; Kwon et al. 2020; Zheng\net al. 2021; Fu, Qiu, and Zha 2021; Jiang et al. 2022; Kwon\net al. 2021; Ma et al. 2021; Kim, Park et al. 2021). Depending\non the speciﬁc ways to construct solutions, DL algorithms\ncan be roughly divided into two main categories: search-\nbased DL (d O Costa et al. 2020; Fu, Qiu, and Zha 2021; Ma\net al. 2021) and end-to-end DL (Kool, van Hoof, and Welling\n2018; Kwon et al. 2020, 2021; Jiang et al. 2022; Kim, Park\net al. 2021). By incorporating heuristic search operators with\nlearning-based policy, search-based DL can solve larger-scale\nTSP instances. However, they usually suffer from two major\nlimitations. The ﬁrst one lies in the inference efﬁciency is-\nsue, meaning that the search component usually takes a long\ntime to terminate to obtain high quality solutions. Moreover,\nthe obtained solution performance is very sensitive to the\nselection of search operators which is highly dependent on\nsophisticated domain knowledge. In contrast, end-to-end DL\nalgorithms are very efﬁcient in generating solutions and bear\nmuch lower dependencies on domain knowledge. Therefore,\nend-to-end DL algorithms are more suitable for many emerg-\ning TSP application scenarios, such as on-call routing (Ghiani\net al. 2003) and ride hailing service (Xu et al. 2018), that\nrequire to generate solutions in almost real-time.\nCompared with supervised learning relying on the optimal\nsolution as the learning labels, which is usually unknown\nwhen facing the large-scale TSP, RL yields the advantage\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n8132\nsince it can be applied to attain near-optimal solutions with-\nout requiring the existence of ground truth. Therefore, most\nrecent studies tend to apply the Deep Reinforcement Learning\n(DRL) approach to solve the large-scale TSP. Nevertheless,\nmost of existing end-to-end DRL algorithms only perform\nwell on small TSP instances (no more than 100 nodes) and\nare hard to scale to larger instances. This is mainly due to the\ndrastically soaring memory consumption and computation\ntime along with the increasing nodes.\nIn this paper, we propose a novel scalable DRL method\nbased on multi-pointer Transformer, denoted as Pointer-\nformer, aiming to solve TSP in an end-to-end manner.\nWhile following the classical encoder-decoder architec-\nture (Vaswani et al. 2017), this new approach adopts re-\nversible residual network (Gomez et al. 2017; Kitaev, Kaiser,\nand Levskaya 2019) instead of the standard residual network\nin the encoder to signiﬁcantly reduce memory consumption.\nFurthermore, instead of employing the memory-consuming\nself-attention module as in (Kool, van Hoof, and Welling\n2018; Kwon et al. 2020), we propose a multi-pointer net-\nwork in the decoder to sequentially generate the next node\naccording to a given query. Besides addressing the issues of\nmemory consumption, Pointerformer contains delicate de-\nsign to further improve the model effectiveness. Particularly,\nto improve the effectiveness of obtained solutions, Pointer-\nformer employs a feature augmentation method to explore the\nsymmetries of TSP at both training and inference stages as\nwell as an enhanced context embedding approach to include\nmore comprehensive context information in the query.\nTo demonstrate the effectiveness of Pointerformer, we con-\nducted extensive experiments on two datasets, including ran-\ndomly generated instances and widely used public bench-\nmarks. Experimental results have shown that Pointerformer\nnot only achieves comparative results on small-scale TSP in-\nstances as State-Of-The-Art (SOTA) DRL approaches do, but\nalso can generalize to large-scale TSPs. More importantly,\nwhile being trained on randomly generated instances, our\napproach can achieve much better performance on instances\nwith different distributions, indicating a better generalization.\nOur main contributions can be summarized as follows.\n• We propose an effective end-to-end DRL algorithm without\nrelying on any hand-crafted heuristic operators, which is the\nﬁrst end-to-end DRL approach that can scale to TSP instances\nwith up to 500 nodes to the best of our knowledge.\n• Our algorithm applies an auto-regressive decoder with a\nproposed multi-pointer network to generate solutions sequen-\ntially without relying on any search components. Compared\nwith existing search-based DRL algorithms, we can achieve\ncomparable solutions while the inference time is reduced by\nalmost an order of magnitude.\n• Besides scalability, extensive experiments also show that\nour approach can generalize well to instances that have varied\ndistributions without re-training.\nRelated Work\nHere we highlight a few of the best traditional algorithms for\nsolving TSP, and then focus on presenting the DL algorithms\nthat are more related to our work.\nTraditional TSP algorithms. TSP is one of the most typ-\nical combinatorial optimization problems, and numerous al-\ngorithms have been proposed for solving TSP over the past\ndecades. Traditional TSP algorithms can be classiﬁed into\nthree categories, i.e., exact algorithms, approximate algo-\nrithms and heuristic algorithms. Concorde (Applegate et al.\n2007) is one of the fastest exact solvers. It models TSP as\na mixed-integer programming problem, and then adopts a\nbranch and cut algorithm (Padberg and Rinaldi 1991) to\nsearch the solution. Christoﬁdes et al., (Christoﬁdes 1976)\nproposed an approximation algorithm, and the approximation\nratio of 1.5 is achieved by constructing the minimum span-\nning tree and the minimum perfect matching of the graph.\nLKH-3 (Helsgaun 2017) is one of the SOTA heuristics, which\nuses the k-opt operators to search in the solution space, with\nthe guidance of an \u000b\u0000measure based on a variant of mini-\nmum spanning tree. Among these traditional algorithms, the\nheuristics are the most widely used algorithms in practice,\nyet they are still time-consuming and difﬁcult to be extended\nto other problems.\nBesides of these traditional algorithms, there are also\nworks that attempt to utilize the power of machine learning\nand reinforcement learning techniques. Earlier machine learn-\ning approaches include the Hopﬁeld neural network (Hopﬁeld\nand Tank 1985) and self-organising feature maps (Angeniol,\nVaubois, and Le Texier 1988). There are several works like\nAnt-Q (Gambardella and Dorigo 1995) and Q-ACS (Sun, Tat-\nsumi, and Zhao 2001) that combined reinforcement learning\nwith ant colony algorithm, and Liu and Zeng (Liu and Zeng\n2009) used reinforcement learning to improve the mutation\nof a successful genetic algorithm called EAX-GA (Nagata\n2006). It is worth mentioning that a recent work, called VSR-\nLKH (Zheng et al. 2021), deﬁned a novel Q-value based on\nreinforcement learning to replace the \u000b\u0000value used by the\nLKH algorithm, and achieved a better performance on TSP.\nDL-based TSP algorithms . DL-based TSP algorithms\nare mainly proposed in recent years, according to the way\nthe solution is generated, they can be classiﬁed into two\ncategories: end-to-end methods and search-based methods.\nEnd-to-end methods create a solution from the scratch\n(Bello et al. 2016; Dai et al. 2017; Kim, Park et al. 2021;\nKool, van Hoof, and Welling 2018; Kwon et al. 2020; Nazari\net al. 2018; Vinyals, Fortunato, and Jaitly 2015). Vinyalset\nal., (Vinyals, Fortunato, and Jaitly 2015) proposed a Pointer\nNetWork to solve TSP with supervised learning. Bello et\nal., (Bello et al. 2016) then used RL to train a PtrNet model\nto minimize the length of solutions. This method achieves\nbetter performance and has stronger generalization and scal-\nability. To deal with both static and dynamic information,\nNazari (Nazari et al. 2018) improved PtrNet, which is more\neffective than many traditional methods. Dai et al., (Dai\net al. 2017) proposed Structure2Vec which encodes partial\nsolutions and predicts the next node. The Q-learning method\nis used to train the whole policy model. Attention Model\nin (Kool, van Hoof, and Welling 2018) adopts the Trans-\nformer (Vaswani et al. 2017) architecture and the model is\ntrained through the REINFORCE algorithm with a greedy\nroll-out baseline. It shows the efﬁciency of Transformer in\nsolving TSP. Then Kwonet al., proposed POMO (Kwon et al.\n8133\n2020) using REINFORCE algorithm with a shared baseline.\nIt leverages the existence of multiple optimal solutions of a\ncombinatorial optimization problem. Currently, end-to-end\nmethods perform well on TSP instances with nodes less than\n100, but due to the complexity of the model and the low sam-\npling efﬁciency of reinforcement learning, it is hard to extend\nthem to a larger scale.\nSearch-based methods start from a feasible solution and\nlearn how to constantly improve the solution (Chen and Tian\n2019; d O Costa et al. 2020; Fu, Qiu, and Zha 2021; Joshi,\nLaurent, and Bresson 2019; Kool et al. 2022). The improve-\nment is often achieved by integrating with heuristic operators.\nFor instance, Chen et al., proposed NeuRewriter (Chen and\nTian 2019), which rewrites local components through region-\npick and rule-pick. They trained the model with Advantage\nActor-Critic, and the reduced cost per iteration is used as\nits reward. Two approaches (Joshi, Laurent, and Bresson\n2019; Kool et al. 2022) used supervised learning to generate\nthe heat maps of the given graphs, and then employed dy-\nnamic programming and beam search to ﬁnd near-optimal\nsolutions respectively. There is another method using Monte\nCarlo tree search (MCTS) to improve the solution such as\nAtt-GCRN+MCTS (Fu, Qiu, and Zha 2021). They ﬁrst train\na model to generate heat maps for guiding MCTS on small-\nscale instances by SL, based on which heat maps of larger\nTSP instances were then constructed by graph sampling,\ngraph converting and heat maps merging. Finally, MCTS\nis used to search for solutions based on these heat maps.\nHowever, performance of such approaches highly depends\non the number of iterations or search, which is usually time-\nconsuming and hinders their applications in time sensitive\ntasks.\nProblem Formulation\nWhile there are many varieties of TSP problems, we focus on\nthe classic two-dimensional Euclidean TSP in this paper. Let\nG(V;E) denote an undirected full connection graph, where\nV = fvi j1 \u0014i \u0014Ngrepresents all N cities/nodes and\nE = feij j 1 \u0014 i;j \u0014 Ngis the set of all edges. Let\ncost(i;j) be the cost of moving from vi to vj, which equates\nthe Euclidean distance between vi and vj. We further assume\ndepot 2V denoting the depot city, from which the salesman\nstarts the trip and will go back in the end. A route is deﬁned\nas a sequence of cities. A route is feasible if and only if it\nstarts from and ends at depot while traverses all other cities\nexactly once. Given a route \u001c, its total cost, denoted by L(\u001c),\ncan be calculated by Eq. (1), where \u001c[i] denotes the i-th node\non \u001c and N = j\u001cjis the length of \u001c.\nL(\u001c) = cost(\u001c[N];\u001c[1]) +\nN\u00001X\ni=1\ncost(\u001c[i];\u001c[i+1]) (1)\nA solution \u001c of TSP can be generated sequentially by\nselecting the next node from all nodes that are to be visited\nuntil returning to the depot. This can be seen as a Markov\ndecision process. The decision of each step can be modeled by\na deep neural network parameterized by\n\u0012: \u0019\u0012(\u001c[i] js;\u001c[: i)),\nwhere sdenotes a TSP instance and \u001c[: i) is the partial route\non \u001cbefore the i-th step. The reward of each step is deﬁned as\nthe negative cost of the newly added edge. For each problem\ninstance s, our goal is to maximize the expected cumulative\nreward deﬁned as follows:\nJ(\u0012js) = E\u001c\u0018p\u0012(\u001cjs)R(\u001c) (2)\nwhere R(\u001c) = \u0000L(\u001c) and p\u0012(\u001c js) = \u0005 N\ni=1\u0019\u0012(\u001c[i] js;\u001c[:\ni)).\nAccording to the policy gradient theorem (Sutton et al.\n2000), we can calculate the derivative of the objective func-\ntion to update the model using many existing policy gradient\nalgorithms.\nr\u0012J(\u0012js) = Ep\u0012(\u001cjs) [r\u0012log p\u0012(\u001c js)R(\u001c)] (3)\nThe Pointerformer Approach\nThe proposed Pointerformer is an end-to-end DRL algorithm\nbased on multi-pointer transformer which combines a trans-\nformer encoder and an auto-regressive decoder. The general\nframework of Pointerformer is illustrated in Figure 1.\nIn principle, Pointerformer applies multiple attention lay-\ners that consist of multi-head self-attention and feed-forward\nlayers to encode the input nodes for obtaining an embedding\nof each node. Then, a multi-pointer network with a single\nhead attention is employed to decode sequentially accord-\ning to a query composed of an enhanced context embedding.\nHere, the enhanced context embedding contains not only\ninformation about the instance itself and nodes that are to\nbe visited, but also information about nodes that have been\nvisited. The solution is generated by choosing a node at each\nstep according to the probability distribution given by the\ndecoder, where all the visited nodes are masked so that their\nprobability is 0. Finally, the proposed Pointerformer is trained\nwith a modiﬁed REINFORCE algorithm, which is based on a\nshared baseline for policy gradients while unifying the mean\nand variance of a batch of instances. In the following subsec-\ntions, we describe the key components of Pointerformer.\nReversible Residual Network Based Encoder\nThe encoder is an important ingredient for the Pointerformer\narchitecture. As we mentioned before, the resource consumed\nby the original Transformer (Vaswani et al. 2017) increases\ndramatically as the length of the input sequence increases,\nwhich equates the number of nodes in TSP. Therefore, we\nadopt a Transformer without positional encoding but includ-\ning a reversible residual network, in order to scale to large\nTSP instances. To our knowledge, the reversible residual net-\nwork has not been introduced into the DRL approaches of\ncombinatorial optimization problems before.\nIn the classic two-dimensional Euclidean TSP setting, each\nnode is solely denoted by its coordinates (x;y). To obtain\na robust embedding for each node, we propose a feature\naugmentation mechanism such that each node is denoted\nby (x;y;\u0011 ), where \u0011 = atanhy\nx. Furthermore, inspired by\nthe data augmentation in POMO (Kwon et al. 2020) that\ngenerates 8 equivalent instances of each instance by ﬂipping\nand rotating its underlying graph, we ﬁnally use them on the\ndeﬁned feature to obtain 24 features for each node. These\nfeatures will be the input of the initial embedding layer.\n8134\nFigure 1: The overall architecture of Pointerformer. First, multiple attention layers are applied to encode the nodes of the input\nTSP instance. Next, a multi-pointer network is used to sequentially decode the solution by a query composed of an enhanced\nembedding.\nAfter the initial embedding layer, nodes will go through\nthe encoder with multiple residual layers, each of which\nis constituted by a multi-head self-attention ( MHA) sub-\nlayer and a feed-forward (FF) sub-layer. Here, we employ\nthe reversible residual network (Gomez et al. 2017; Kitaev,\nKaiser, and Levskaya 2019) to save memory consumption.\nDifferent from residual networks where activation values of\nall residual layers need to be stored in order to calculate the\nderivations during back-propagation, in reversible residual\nnetworks, MHA and FF maintain a pair of input and output\nembedding features (X1;X2) and (Y1;Y2) so that derivations\ncan be calculated directly. Below we illustrate the details in\nEq. (4) and (5):\nY1 = X1 + MHA(X2);\nY2 = X2 + FF(Y1): (4)\nObviously, the input embedding features (X1;X2) can be\ncalculated from the output embeddings(Y1;Y2) easily during\nback-propagation:\nX2 = Y2 \u0000FF(Y1);\nX1 = Y1 \u0000MHA(X2): (5)\nNote that the deeper of the residual network, the more\nmemory the reversible residual network can save. In our\nwork, we apply MHA and FF of six layers, we can observe\ndramatic reduction of memory consumption without affecting\nthe performance.\nMulti-pointer Network Based Decoder\nThe decoder is an auto-regressive process that is to sequen-\ntially generate a feasible route for each TSP instance. A\ncontext embedding is used to represent the current state, and\nis used as a query to interact with embeddings of nodes that\nare to be selected. The context embedding is updated con-\nstantly as more nodes are selected until a feasible route is\nobtained. The auto-regressive decoder is generally very fast\nbut memory-consuming, mainly due to the attention module\nused in the query. To alleviate this, we improve our decoder\nby integrating the following distinguishing features.\nEnhanced Context Embedding. Recall that a route \u001c of\nTSP is composed of a sequence of nodes on it. We propose an\neffective and enhanced context embedding that contains the\nfollowing information h\u001c[1] ;h\u001c[t] ;hg, and h\u001c, where t= j\u001cj\nis used to denote the length of \u001c:\n• h\u001c[1] , embedding of the ﬁrst node on\u001c: A static information\nthat is the embedding of depot;\n• h\u001c[t] , embedding of the last node on \u001c: A dynamic infor-\nmation that is updated according to the current route;\n• hg, graph embedding: To encode the whole TSP instance,\nwhich is the summation of embeddings of all nodes in the\ninstance: hg = PN\ni=1 henc\ni , where henc\ni\nis the embedding of\nthe i-th node obtained by the encoder;\n• h\u001c, embedding of \u001c: To encode the current partial route,\nwhich is the summation of embeddings of all nodes on \u001c\nh\u001c = Pt\u00001\ni=1 henc\n\u001c[i] .\nThe enhanced context embedding is used as a query qt,\nwhich is computed by qt = 1\nN(hg + h\u001c) + h\u001c[t\u00001] + h\u001c[1] .\nSince the graph embedding is able to reﬂect different graph\nstructures while information about depot and the last visited\nnode is crucial for selecting future nodes, we include such\ninformation to guide the decoder similar as in the previous\nDRL algorithms (Kool, van Hoof, and Welling 2018; Kwon\net al. 2020). Additionally, we also utilize h\u001c in our decoder\nwhich is ignored in previous solutions. The motivation is\nthat even with the same ﬁrst and last nodes, two routes may\ncause different distributions over nodes that are to be visited.\nAs shown in our experiments, such information is crucial,\nparticularly for instances from practical applications. Notice\nthat we normalize the graph embedding and the current partial\nroute embedding by dividing the total number of nodes N.\nA Multi-pointer Network . At each step, the above en-\nhanced context embedding is used to interact with all nodes\n8135\nthat are to be visited to output a probability distribution over\nthem. We devise a multi-pointer network to better utilize the\ncontext embedding. More speciﬁcally, we linearly project the\nqueries qt and keys kj (embedding of the j-th node given by\nthe encoder) to dk dimensions by using H different linear\nprojections for each of them. For each projection, we are able\nto obtain an interaction between the query and node j via\na dot operator and normalization by pdk. The ﬁnal interac-\ntion is simply evaluated by an average operator over all H\ninteractions, namely, PN = 1\nH\nPH\nh=0\n(qtWq\nh)T(kjWk\nh)pdk\n:\nWe further minus PN by the cost between the last node\ni of the partial route and node j to obtain the interaction\nscore between iand j: scoreij = PN \u0000cost(i;j). By doing\nso, we encourage the approach to start from a good policy\nthat is always selecting the nearest node as the next one to\nvisit. Comparing to starting from a random policy, this will\naccelerate our training procedure considerably.\nSimilar to (Bello et al. 2016), the probability is obtained\nby Eq. (6), where we clip the score with tanh and mask\nall visited nodes. Here, C is a coefﬁcient that controls the\nrange of values. The larger the value of Cis, the smaller of\nthe entropy, hence it can be seen as a parameter to control\nthe trade-off between exploitation and exploration during\ntraining. We will show via ablation studies that the value of\nChas a signiﬁcant impact on performance.\nuij =\n\u001aC\u0001tanh (scoreij) node jis to be visited\n\u00001 otherwise (6)\nFinally, we are able to compute the output probability\nvector pusing a softmax function.\nA Modiﬁed REINFORCE Algorithm\nWe train our Pointerformer model by using the REINFORCE\nalgorithm (Williams 1992), whose baseline applies diverse\ngreedy roll-outs of all instances for policy gradient. Inspired\nby POMO (Kwon et al. 2020), our decoder also starts from\nN different nodes for each TSP instance with N nodes. By\ntaking each node as the depot, for each TSP instancei, we can\nsample N feasible routes \u001ci =\n\b\n\u001c1\ni ;\u001c2\ni ;:::;\u001c N\ni\n\t\nby Monte\nCarlo sampling method. Therefore, given a batch containing\nB TSP instances, we can obtain B\u0002N routes, which can\nbe used to train our policy according to Eq. (3). However,\ndirectly applying REINFORCE will cause the algorithm hard\nto converge because of high variance of costs among different\ninstances. In order to alleviate such a problem, we further\nuse a variance-consistent normalization mechanism before\ntraining, which can increase the speed of convergence while\nalso stabilizes the training. More details can be found in\nEq. (7), where \u0016(\u001ci) and \u001b(\u001ci) are the mean and variance\nof the N trajectories of instance i, respectively. One can\neasily observe that\nR(\u001cj\ni)\u0000\u0016(\u001ci)\n\u001b(\u001ci) is an unbiased estimation of\nthe TSP objective function, which eliminates the effect of\ndifferent rewards among different instances.\nr\u0012J(\u0012) \u0019\n1\nB\u0002N\nBX\ni=1\nNX\nj=1\n \nR\n\u0000\n\u001cj\ni\n\u0001\n\u0000\u0016(\u001ci)\n\u001b(\u001ci)\n!\nr\u0012log p\u0012\n\u0010\n\u001cj\ni js\n\u0011\n\u0016(\u001ci) = 1\nN\nNX\nj=1\nR\n\u0010\n\u001cj\ni\n\u0011\n\u001b(\u001ci) = 1\nN\nNX\nj=1\n\u0010\nR\n\u0010\n\u001cj\ni\n\u0011\n\u0000\u0016(\u001ci)\n\u00112\n(7)\nExperiments\nTo evaluate the efﬁciency of Pointerformer, we compare its\nperformance with SOTA DRL approaches. We train and test\nPointerformer on randomly generated instances, and verify\nits generalization on a public benchmark.\nBenchmark Instances\n• TSP random: Uniformly sample a certain number of\nnodes from the unit square of [0;1]2. It includes ﬁve sets\nof TSP instances with N = 20, 50, 100, 200, 500. Same as\nin Att-GCRN+MCTS (Fu, Qiu, and Zha 2021), for TSP in-\nstances with N \u0014100, we sample 10,000 instances for each\nset, while for larger instances with N \u0015200, the set size\nis 128. The same benchmark is also widely adopted to tes-\ntify existing DRL approaches except that they only consider\ninstances with N \u0014100;\n• TSPLIB: A well-known TSP library (Reinelt 1991) that\ncontains 100 instances with various node distributions. These\ninstances come from practical applications with size rang-\ning from 14 to 85,900. In our experiment, we consider all\ninstances with no more than 1,002 nodes.\nBaselines\nThe following SOTA DL algorithms are considered as our\nbaselines.\nEnd-to-end DL algorithms:\n• AM (Kool, van Hoof, and Welling 2018): A model based\non attention layer is trained using the REINFORCE algorithm\nwith a deterministic greedy roll-out baseline. AM can achieve\ngood performance on small-scale TSP instances;\n• POMO (Kwon et al. 2020): To reduce the variance of ad-\nvantage estimation, POMO improves the algorithm in AM\nsuch that it generates N trajectories for each instance with\nN nodes and uses data augmentation to improve the quality\nof solutions during validation;\n•\nAM+LCP (Kim, Park et al. 2021): It proposes a training\nparadigm for solving TSP called termed learning collabora-\ntive policy. It distinguishes policy seeder and policy reviser,\nwhich focus on exploration and exploitation, respectively.\nSearch-based DL algorithms:\n• DRL+2opt (d O Costa et al. 2020): DRL+2opt guides the\nsearch of 2-opt operator through DRL. The combination of re-\ninforcement learning and heuristic search operator constantly\nimprove solutions to achieve good results.\n8136\n• Att-GCN+MCTS (Fu, Qiu, and Zha 2021): It trains a model\nto generate heat maps for guiding MCTS on small-scale\ninstances by supervised learning, based on which heat maps\nof larger instances are then constructed by graph sampling,\ngraph converting and heat maps merging. Finally, MCTS is\nused to search for solutions based on the heat maps.\nHyper-Parameters\nIn our experiments, we only use instances fromTSP random\nto train various models corresponding to instances with dif-\nferent nodes. During each training epoch, 100,000 instances\nare randomly sampled. To train models for instances of size\nN \u0014200, we use a single GPU V100 (16G) with batch size\nB = 64, while for other cases the models are trained on four\nGPUs V100 (32G) with batch size B = 32. Adam is used as\nthe optimizer for all models with a learning rate \u0011 = 10\u00004\nand a weight decay != 10\u00006. We use 6 layers in the encoder\n(nt = 6) and let dk = 128 and H = 8 of multi-pointer in the\ndecoder. The number of heads is 8 in the MHA layer. When\nevaluating on TSP random, the batch size Bis 128 for in-\nstances with N \u0014200, while B = 64 for other cases. Our\nalgorithm is implemented based on PyTorch (Paszke et al.\n2019), the trained models and the related data are publicly\navailable. 1\nExperimental Results\nTo show the effectiveness of Pointerformer, we ﬁrst train\nmodels with different number of nodes, denoted by ModelN\nwith N = 20, 50, 100, 200, and 500, respectively. For train-\ning ModelN, random instances of size N are sampled from\nTSP random using parameters as stated in the above section.\nWe have conducted the experiment onTSP random and\na further study of generalization on TSPLIB, in all of which\nwe observe advantages of Pointerformer over others. For the\ngroup of TSP random benchmark, the results are shown in\nTable 1, from which we can see that Pointerformer has the\nbest trade-off between efﬁciency and optimality compared to\nothers. Pointerformer can achieve results of relatively small\ngaps to the optimal solutions that are achieved by the exact\nalgorithm Concorde, denoted by OPT. More importantly, one\neasily observes that Pointerformer can scale to TSP instances\nwith up to 500 nodes while other DRL algorithms except Att-\nGCN+MCTS quickly run out of memory for TSP instances\nwith N >100 (indicated by - in Table 1). In Fig. 2, we also\ncompare memory consumption of our model with the SOTA\nDRL approach POMO trained on instances of different size.\nOne easily observes that along with the enlarging problem\nsize, the memory consumption of POMO increases sharply,\nwhile our model increases gradually. Note that since the\narchitecture of POMO is most similar with ours, it is more\nfair to use POMO for comparison of memory consumption\nwhen comparing to other DRL models. Comparing to search-\nbased approach, the solutions obtained by Pointerformer may\nbe slightly worse than Att-GCN+MCTS on TSP instances\nwith 500 nodes. However, we can accelerate the computing\ntime by up to 6 times (5.9m to 59.35s). In particular, we can\n1https://github.com/Learning4Optimization-\nHUST/Pointerformer\nFigure 2: Comparison of memory consumption between\nPointerformer and POMO. Along with the enlarging problem\nsize, the memory consumption of POMO increases sharply,\nwhile our model increases gradually.\nattain better results on TSP instances with 200 nodes in less\ntime. We should mention that results of Att-GCN+MCTS\nare taken directly from (Fu, Qiu, and Zha 2021), where the\nsearch component is implemented in C++ and runs in a CPU\nwith 8 cores in parallel.\nTo the best of our knowledge, Pointerformer is the ﬁrst\nend-to-end DRL algorithm that can scale to TSP instances\nwith more than 100 nodes while still achieve comparable\nresults as search-based DRL approaches, but in shorter time.\nIn order to evaluate the generalization of the proposed\nPointerformer, we apply Model100 directly to the TSPLIB\ninstances, similar for the baseline algorithms AM, POMO,\nand DRL+2opt. Note that we do not compare with Att-\nGCN+MCTS and AM+LCP here, since we have not ﬁg-\nured out how to extend Att-GCN+MCTS to non-random set-\nting, while the implementation of AM+LCP is not publicly\navailable. To further verify the importance of scalability, we\nalso apply Model200 to these instances, which are unavail-\nable for the baselines due to lack of scalability. We see that\nModel200 has better generalization comparing to Model100,\nparticularly for large-scale instances. Table 2 summarizes the\nresults of Pointerformer in comparison with the three base-\nlines on instances fromTSPLIB, where we classify instances\nin TSPLIB into three groups according to their sizes, i.e.,\nTSPLIB1\u0018100, TSPLIB101\u0018500, and TSPLIB501\u00181002.\nFrom the results, we can see that POMO performs the best\non instances with no more than 100 nodes and the second\nbest on instances between 101 to 500 nodes. While Pointer-\nformer (Model100) performs the best on instances between\n101 to 500 nodes and the second best on the other two groups.\nOne notices that most instances of second group are around\n100 nodes, so Pointerformer (Model100) has the best perfor-\nmance and POMO has the second best performance for them.\nPointerformer (Model200) and Pointerformer ( Model100)\nperform the best and the second best on instances with more\nthan 500 nodes, indicating that our model generalizes best to\nlarge-scale instances.\nAblation Studies\nIn this section, we present some ablation studies that explain\nsome important choices of our approach.\nTo assess the inﬂuence of some key components to\nthe performance of Pointerformer, we carry out an addi-\n8137\nMethod TSP random20 TSP random50 TSP random100 TSP random200 TSP random500\nLen Gap T ime Len Gap T i\nme Len Gap T ime Len Gap T ime Len Gap T ime\n(%) (%) (%) (%) (%)\nOPT 3.83 5.69 7.76 10.72 16.55\nAM 3.83 0.06 5.22s 5.72 0.49 12.76m 7.94 23.20 32.72m - - - - - -\nPOMO 3.83 0.00 36.86s 5.69 0.02 1.15m 7.77 0.16 2.17m - - - - - -\nAM+LCP 3.84 0.00 30.00m 5.70 0.02 6.89h 7.81 0.54 11.94h - - - - - -\nDRL+2opt 3.83 0.00 3.33h 5.70 0.12 4.62m 7.82 0.78 6.57h - - - - - -\nAtt-GCN+MCTS 3.83 0.00 1.6m 5.69 0.01 7.90m 7.76 0.04 15m 10.81 0.88 2.5m 16.97 2.54 5.9m\nPointerformer 3.83 0.00 5.82s 5.69 0.02 11.63s 7.77 0.16 52.34s 10.79 0.68 5.54s 17.14 3.56 59.35s\nTSP20, TSP50 and TSP100: 10,000 instances; TSP200 and TSP500: 128 instances.\nTable 1: Comparison results on instances from TSP random.\nMethod TSPLIB1\u0018100 TSPLIB101\u0018500 TSP501\u00181002\nLen Gap T ime Len Gap T ime Len Gap T ime\n(%) (%) (%)\nOPT 19454.17 40842.43 62427.71\nAM 22283.67 15.36 0.23s 72137.93 78.18 0.86s 140664.29 139.02 5.79s\nPOMO 19628.67 1.20 1.41s 43652.77 6.99 1.55s 82162.29 26.93 3.49s\nDRL+2opt 19916.50 2.43 15.20m 46651.40 13.85 27.92m 82797.71 42.57 1.24h\nPointerformer (Model100) 19728.50 1.33 0.20s 42963.20 5.43 0.46s 75081.43 18.65 5.14s\nPointerformer\n(Model200) 20135.00 2.91 0.20s 43810.67 8.37 0.46s 73915.57 18.20 5.14s\nTable 2: Comparison results on practical instances from TSPLIB.\nAlgorithm Len Gap\nPointerformer 10.793 0.68%\nw.o. feature augmentation 10.813 0.87%\nw.o. enhanced context embedding 11.013 2.73%\nw.o. multi-pointer network 10.797 0.72%\nTable 3: Ablations of three key elements of Pointerformer on\nTSP random200.\ntional ablation study to compare Pointerformer and its four\nvariants on instances from TSP random with 200 nodes\n(TSP random200). The results are summarized in Table 3.\nThe ﬁrst variant only uses the coordinates of each node as\ninputs without any feature augmentation (denoted by w.o. fea-\nture augmentation in the table). The second variant removes\nthe embedding of the current partial route from the context\nembedding (denoted by w.o. enhanced context embedding).\nAnd the third variant does not use the multi-pointer net-\nworks, denoted by w.o. multi-pointer network. From Table 3,\nit is clear that Pointerformer achieves the best performance\ncomparing to all the variants, which indicates all components\nplay positive roles to our algorithm. Furthermore, we apply\nthese models directly test the instances from TSPLIB and\nprovide their comparisons in Figure 3. Pointerformer with all\ncomponents outperforms the three variants, indicating that\nthese components are also important for the generalization\nof Pointerformer.\nConclusion\nIn this paper, we propose an end-to-end DRL approach\ncalled Pointerformer to solve the traveling salesman problems\nPointerformer w.o.…feature\naugmentation\nw.o.…multi-\n…pointer…\n…network\nw.o.…enhanced…\n…context…\n…embedding\n0\n5\n10\n15\n20\n25\n30\n35\n40Number…of…best…solutions\nFigure 3: Ablations of three key elements of Pointerformer\non TSP random200.\n(TSPs). By integrating feature augmentation, reversible resid-\nual network, and enhanced context embedding with the well-\nknown Transformer architecture, Pointerformer can achieve\ncomparable results as SOTA algorithms do but using less\nresources (time or memory). While being memory-efﬁcient,\nPointerformer can be scaled to handle TSP instances with 500\nnodes, that existing end-to-end DRL approaches could not\nsolve. More importantly, we show via extensive experiments\non well-known TSP instances with different distributions that\nour approach has better generalization. For future work, we\nwill explore how to extend our approach to address the more\ncomplicated problem of vehicle routing and other combinato-\nrial optimization problems.\n8138\nAcknowledgements\nThis work is supported by National Natural Science Founda-\ntion (U22B2017) and MSRA Collaborative Research 2022\n(100338928).\nReferences\nAlkaya, A. F.; and Duman, E. 2013. Application of sequence-\ndependent traveling salesman problem in printed circuit\nboard assembly. IEEE Transactions on Components, Packag-\ning and Manufacturing Technology, 3(6): 1063–1076.\nAngeniol, B.; Vaubois, G. D. L. C.; and Le Texier, J.-Y . 1988.\nSelf-organizing feature maps and the travelling salesman\nproblem. Neural Networks, 1(4): 289–293.\nApplegate, D. L.; Bixby, R. E.; Chv´atal, V .; and Cook, W. J.\n2007. The Traveling Salesman Problem: a Computational\nStudy. Princeton Series in Applied Mathematics.\nBello, I.; Pham, H.; Le, Q. V .; Norouzi, M.; and Bengio, S.\n2016. Neural combinatorial optimization with reinforcement\nlearning. arXiv preprint arXiv:1611.09940.\nBland, R. G.; and Shallcross, D. F. 1989. Large travelling\nsalesman problems arising from experiments in X-ray crystal-\nlography: A preliminary report on computation. Operations\nResearch Letters, 8(3): 125–128.\nChen, X.; and Tian, Y . 2019. Learning to perform local\nrewriting for combinatorial optimization. In Proceedings of\nthe 33rd International Conference on Neural Information\nProcessing Systems, 6281–6292.\nChristoﬁdes, N. 1976. Worst-case analysis of a new heuris-\ntic for the travelling salesman problem. Technical report,\nCarnegie-Mellon Univ Pittsburgh Pa Management Sciences\nResearch Group.\nd O Costa, P. R.; Rhuggenaath, J.; Zhang, Y .; and Akcay,\nA. 2020. Learning 2-opt Heuristics for the Traveling Sales-\nman Problem via Deep Reinforcement Learning. In Asian\nConference on Machine Learning, 465–480. PMLR.\nDai, H.; Khalil, E. B.; Zhang, Y .; Dilkina, B.; and Song, L.\n2017. Learning Combinatorial Optimization Algorithms over\nGraphs. Advances in Neural Information Processing Systems,\n30: 6348–6358.\nFu, Z.-H.; Qiu, K.-B.; and Zha, H. 2021. Generalize a Small\nPre-trained Model to Arbitrarily Large TSP Instances. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence,\n35(8): 7474–7482.\nGambardella, L. M.; and Dorigo, M. 1995. Ant-Q: A rein-\nforcement learning approach to the traveling salesman prob-\nlem. In Machine learning proceedings 1995, 252–260. Else-\nvier.\nGhiani, G.; Guerriero, F.; Laporte, G.; and Musmanno, R.\n2003. Real-time vehicle routing: Solution concepts, algo-\nrithms and parallel computing strategies. European Journal\nof Operational Research, 151(1): 1–11.\nGomez, A. N.; Ren, M.; Urtasun, R.; and Grosse, R. B. 2017.\nThe reversible residual network: Backpropagation without\nstoring activations. In Proceedings of the 31st International\nConference on Neural Information Processing Systems, 2211–\n2221.\nHacizade, U.; and Kaya, I. 2018. GA Based Traveling\nSalesman Problem Solution and its Application to Transport\nRoutes Optimization. IFAC-PapersOnLine, 51(30): 620–625.\nHelsgaun, K. 2017. An extension of the Lin-Kernighan-\nHelsgaun TSP solver for constrained traveling salesman and\nvehicle routing problems. Roskilde: Roskilde University.\nHopﬁeld, J. J.; and Tank, D. W. 1985. “Neural” computation\nof decisions in optimization problems.Biological cybernetics,\n52(3): 141–152.\nJiang, Y .; Wu, Y .; Cao, Z.; and Zhang, J. 2022. Learning to\nSolve Routing Problems via Distributionally Robust Opti-\nmization. arXiv preprint arXiv:2202.07241.\nJoshi, C. K.; Laurent, T.; and Bresson, X. 2019. An efﬁ-\ncient graph convolutional network technique for the travelling\nsalesman problem. arXiv preprint arXiv:1906.01227.\nKim, M.; Park, J.; et al. 2021. Learning Collaborative Policies\nto Solve NP-hard Routing Problems. Advances in Neural\nInformation Processing Systems, 34.\nKitaev, N.; Kaiser, L.; and Levskaya, A. 2019. Reformer:\nThe Efﬁcient Transformer. In International Conference on\nLearning Representations.\nKool, W.; van Hoof, H.; Gromicho, J.; and Welling, M. 2022.\nDeep policy dynamic programming for vehicle routing prob-\nlems. In International Conference on Integration of Con-\nstraint Programming, Artiﬁcial Intelligence, and Operations\nResearch, 190–213. Springer.\nKool, W.; van Hoof, H.; and Welling, M. 2018. Attention,\nLearn to Solve Routing Problems! In International Confer-\nence on Learning Representations.\nKwon, Y . D.; Choo, J.; Kim, B.; Yoon, I.; Gwon, Y .; and Min,\nS. 2020. POMO: Policy optimization with multiple optima\nfor reinforcement learning. Advances in Neural Information\nProcessing Systems, 2020-December.\nKwon, Y .-D.; Choo, J.; Yoon, I.; Park, M.; Park, D.; and\nGwon, Y . 2021. Matrix encoding networks for neural com-\nbinatorial optimization. Advances in Neural Information\nProcessing Systems, 34: 5138–5149.\nLiu, F.; and Zeng, G. 2009. Study of genetic algorithm with\nreinforcement learning to solve the TSP. Expert Systems with\nApplications, 36(3): 6995–7001.\nMa, Y .; Li, J.; Cao, Z.; Song, W.; Zhang, L.; Chen, Z.; and\nTang, J. 2021. Learning to iteratively solve routing prob-\nlems with dual-aspect collaborative transformer. Advances in\nNeural Information Processing Systems, 34: 11096–11107.\nMadani, A.; Batta, R.; and Karwan, M. 2020. The balancing\ntraveling salesman problem: application to warehouse order\npicking. Top.\nMatai, R.; Singh, S.; and Lal, M. 2010. Traveling Salesman\nProblem: an Overview of Applications, Formulations, and\nSolution Approaches. Traveling Salesman Problem, Theory\nand Applications.\nNagata, Y . 2006. Fast EAX algorithm considering popula-\ntion diversity for traveling salesman problems. In European\nConference on Evolutionary Computation in Combinatorial\nOptimization, 171–182. Springer.\n8139\nNazari, M.; Oroojlooy, A.; Tak´aˇc, M.; and Snyder, L. V . 2018.\nReinforcement learning for solving the vehicle routing prob-\nlem. In Proceedings of the 32nd International Conference on\nNeural Information Processing Systems, 9861–9871.\nNowak, A.; Villar, S.; Bandeira, A. S.; and Bruna, J. 2017.\nA Note on Learning Algorithms for Quadratic Assign-\nment with Graph Neural Networks. ArXiv e-prints, 1706:\narXiv:1706.07450.\nPadberg, M.; and Rinaldi, G. 1991. A branch-and-cut algo-\nrithm for the resolution of large-scale symmetric traveling\nsalesman problems. SIAM review, 33(1): 60–100.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\nDesmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison, M.;\nTejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.;\nand Chintala, S. 2019. PyTorch: An Imperative Style, High-\nPerformance Deep Learning Library. In Advances in Neural\nInformation Processing Systems 32, 8024–8035. Curran As-\nsociates, Inc.\nReinelt, G. 1991. TSPLIB—A traveling salesman problem\nlibrary. ORSA journal on computing, 3(4): 376–384.\nSun, R.; Tatsumi, S.; and Zhao, G. 2001. Multiagent re-\ninforcement learning method with an improved ant colony\nsystem. In 2001 IEEE International Conference on Systems,\nMan and Cybernetics. e-Systems and e-Man for Cybernetics\nin Cyberspace (Cat. No. 01CH37236), volume 3, 1612–1617.\nIEEE.\nSutton, R. S.; McAllester, D. A.; Singh, S. P.; and Mansour,\nY . 2000. Policy gradient methods for reinforcement learn-\ning with function approximation. In Advances in neural\ninformation processing systems, 1057–1063.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser,\nŁ.; and Polosukhin, I. 2017. Attention\nis all you need. In Advances in neural information processing\nsystems, 5998–6008.\nVinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer\nNetworks. In Cortes, C.; Lawrence, N.; Lee, D.; Sugiyama,\nM.; and Garnett, R., eds., Advances in Neural Information\nProcessing Systems, volume 28. Curran Associates, Inc.\nWilliams, R. J. 1992. Simple Statistical Gradient-Following\nAlgorithms for Connectionist Reinforcement Learning. In\nReinforcement Learning, 5–32. Springer.\nXu, Z.; Li, Z.; Guan, Q.; Zhang, D.; Li, Q.; Nan, J.; Liu,\nC.; Bian, W.; and Ye, J. 2018. Large-scale order dispatch in\non-demand ride-hailing platforms: A learning and planning\napproach. Proceedings of the ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 905–\n913.\nZheng, J.; He, K.; Zhou, J.; Jin, Y .; and Li, C.-M. 2021.\nCombining Reinforcement Learning with Lin-Kernighan-\nHelsgaun Algorithm for the Traveling Salesman Problem. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence,\n12445–12452.\n8140"
}