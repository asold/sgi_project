{
  "title": "Improving text mining in plant health domain with GAN and/or pre-trained language model",
  "url": "https://openalex.org/W4321458671",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5048887518",
      "name": "Shufan Jiang",
      "affiliations": [
        "Centre de Recherche en Sciences et Technologies de l'Information et de la Communication",
        "Centre de Recherche en Économie et Statistique",
        "Institut Supérieur d'Électronique de Paris",
        "École Normale Supérieure - PSL"
      ]
    },
    {
      "id": "https://openalex.org/A5043361040",
      "name": "Stéphane Cormier",
      "affiliations": [
        "Centre de Recherche en Sciences et Technologies de l'Information et de la Communication",
        "Centre de Recherche en Économie et Statistique"
      ]
    },
    {
      "id": "https://openalex.org/A5026447069",
      "name": "Rafael Angarita",
      "affiliations": [
        "Laboratoire d’Électronique, Informatique et Image"
      ]
    },
    {
      "id": "https://openalex.org/A5085150504",
      "name": "Francis Rousseaux",
      "affiliations": [
        "Centre de Recherche en Sciences et Technologies de l'Information et de la Communication",
        "Centre de Recherche en Économie et Statistique"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6674385629",
    "https://openalex.org/W6839970929",
    "https://openalex.org/W3035454789",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2084450630",
    "https://openalex.org/W6765779288",
    "https://openalex.org/W4285055360",
    "https://openalex.org/W2290556040",
    "https://openalex.org/W3121396110",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W3210987863",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W6779629370",
    "https://openalex.org/W4220902505",
    "https://openalex.org/W3165973893",
    "https://openalex.org/W2563063371",
    "https://openalex.org/W2887311010",
    "https://openalex.org/W6786103567",
    "https://openalex.org/W3196496071",
    "https://openalex.org/W6718379498",
    "https://openalex.org/W1490137407",
    "https://openalex.org/W2755577605",
    "https://openalex.org/W3099864827",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2963373786",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W4301206121",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W2125389028",
    "https://openalex.org/W2097998348",
    "https://openalex.org/W4285819904"
  ],
  "abstract": "The Bidirectional Encoder Representations from Transformers (BERT) architecture offers a cutting-edge approach to Natural Language Processing. It involves two steps: 1) pre-training a language model to extract contextualized features and 2) fine-tuning for specific downstream tasks. Although pre-trained language models (PLMs) have been successful in various text-mining applications, challenges remain, particularly in areas with limited labeled data such as plant health hazard detection from individuals' observations. To address this challenge, we propose to combine GAN-BERT, a model that extends the fine-tuning process with unlabeled data through a Generative Adversarial Network (GAN), with ChouBERT, a domain-specific PLM. Our results show that GAN-BERT outperforms traditional fine-tuning in multiple text classification tasks. In this paper, we examine the impact of further pre-training on the GAN-BERT model. We experiment with different hyper parameters to determine the best combination of models and fine-tuning parameters. Our findings suggest that the combination of GAN and ChouBERT can enhance the generalizability of the text classifier but may also lead to increased instability during training. Finally, we provide recommendations to mitigate these instabilities.",
  "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/one.tnum February /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nOPEN ACCESS\nEDITED BY\nMathieu Roche,\nFrench Agricultural Research Centre for\nInternational Development (CIRAD), France\nBrett Drury,\nLiverpool Hope University, United Kingdom\nREVIEWED BY\nEmanuela Boros,\nUniversité de la Rochelle, France\nLentschat Martin,\nUniversité Grenoble Alpes, France\n*CORRESPONDENCE\nShufan Jiang\njiang.chou.fan@gmail.com\nSPECIALTY SECTION\nThis article was submitted to\nAI in Food, Agriculture and Water,\na section of the journal\nFrontiers in Artiﬁcial Intelligence\nRECEIVED /one.tnum/seven.tnum October /two.tnum/zero.tnum/two.tnum/two.tnum\nACCEPTED /one.tnum/eight.tnum January /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /two.tnum/one.tnum February /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nJiang S, Cormier S, Angarita R and Rousseaux F\n(/two.tnum/zero.tnum/two.tnum/three.tnum) Improving text mining in plant health\ndomain with GAN and/or pre-trained language\nmodel. Front. Artif. Intell./six.tnum:/one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Jiang, Cormier, Angarita and\nRousseaux. This is an open-access article\ndistributed under the terms of the\nCreative\nCommons Attribution License (CC BY) . The use,\ndistribution or reproduction in other forums is\npermitted, provided the original author(s) and\nthe copyright owner(s) are credited and that\nthe original publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nImproving text mining in plant\nhealth domain with GAN and/or\npre-trained language model\nShufan Jiang /one.tnum,/two.tnum*, Stéphane Cormier /one.tnum, Rafael Angarita /two.tnumand\nFrancis Rousseaux/one.tnum\n/one.tnumCReSTIC, Université de Reims Champagne Ardenne, Reims, Franc e, /two.tnumLISITE, Institut Supérieur d’Electronique\nde Paris, Paris, France\nThe Bidirectional Encoder Representations from Transformer s (BERT) architecture\noﬀers a cutting-edge approach to Natural Language Processing. It involves two\nsteps: /one.tnum) pre-training a language model to extract contextualized features and /two.tnum)\nﬁne-tuning for speciﬁc downstream tasks. Although pre-train ed language models\n(PLMs) have been successful in various text-mining application s, challenges remain,\nparticularly in areas with limited labeled data such as plant heal th hazard detection\nfrom individuals’ observations. To address this challenge, we propose to combine\nGAN-BERT, a model that extends the ﬁne-tuning process with unl abeled data through\na Generative Adversarial Network (GAN), with ChouBERT, a domai n-speciﬁc PLM.\nOur results show that GAN-BERT outperforms traditional ﬁne- tuning in multiple text\nclassiﬁcation tasks. In this paper, we examine the impact of furthe r pre-training on the\nGAN-BERT model. We experiment with diﬀerent hyper parameters t o determine the\nbest combination of models and ﬁne-tuning parameters. Our ﬁnd ings suggest that\nthe combination of GAN and ChouBERT can enhance the generalizabi lity of the text\nclassiﬁer but may also lead to increased instability during tr aining. Finally, we provide\nrecommendations to mitigate these instabilities.\nKEYWORDS\nGAN, social media, plant health monitoring, text classiﬁcatio n, pre-trained BERT\n/one.tnum. Introduction\nClimate change is causing massive yield losses due to the disruption of cycles and the\nemergence of crop-aﬀecting pests and plant diseases ( Massod et al., 2022 ). More pest attacks may\noccur than that reported earlier, and their population may increase due to warmer temperatures.\nThe CO 2 level and lower soil humidity can also aﬀect the nature of plant diseases (\nMozaﬀari,\n2022). To tackle the emerging risks and increasingly unpredictable hazards acting as a menace\nto crops and plants, precision agriculture emerges as an alternative–or improvement–to existing\nagricultural practices. Indeed, researchers have experimented with technological innovations to\nﬁnd solutions to some speciﬁc goals, such as predicting the climate for agricultural purposes\nusing simulation models (\nHammer et al., 2001 ), improving the eﬃciency and eﬀectiveness of\ngrain production using computer vision and Artiﬁcial Intelligence ( Patrício and Rieder, 2018 ),\nstudying and evaluating soils with drones ( Tripicchio et al., 2015 ), and collecting real-time\ndata from the ﬁelds using sensors following the IoT and cloud computing paradigms ( Patil\net al., 2012 ). Although the application of these technological innovations produces important\nresults, we suggest that the current observation data from precision agriculture cannot represent\nall forms of agricultural environments, especially small farms. Recently, the idea of how to\nencourage the participation of farmers to share their knowledge and observations is drawing\nthe attention of researchers (\nJiménez et al., 2016 ; Kenny and Regan, 2021 ). Indeed, new studies\nshow that social media might enable farmers to reveal diﬀerent aspects of their world and to\nshare their experiences and perspectives among colleagues and non-farming audiences (\nRiley\nand Robertson, 2021 ).\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nJiang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nThe role of social media such as Twitter in farmer-to-farmer\nand in farmer-to-rural-profession knowledge exchange is increasing,\nand it suggests that their use among rural professionals and\nfarmers is evolving with open participation (creating contributions),\ncollaboration (sharing contributions), and fuller engagement (asking\nquestions and providing answers/replies) dominating one-way\nmessaging (new/original contributions) (\nPhillips et al., 2018 ).\nFollowing the social sensing paradigm ( Wang et al., 2015 ),\nindividuals—whether they are farmers or not- have more and more\nconnectivity to information while on the move, at the ﬁeld level. Each\nindividual can become a broadcaster of information by posting real-\ntime hazard observations in social media. Indeed, Twitter enables\nfarmers to exchange experience with each other, to subscribe to\ntopics of interest using hashtags, and to share real-time information\non natural hazards. Compared to paid and specialized applications,\ninformation on Twitter, presented in the form of text, image, sound,\nvideo, or a mixture of the above, is more accessible to the public\nbut less formalized or structured. More and more farmers get\ninvolved in online Twitter communities by adding hashtags on their\npublications to categorize their tweets and help others ﬁnd them\neasily (\nDefour, 2018 ). Some hashtags are #AgriChatUK /one.tnum, #FrAgTw/two.tnum,\nand #Farming365.\nStill, the extraction of useful plant health information from social\nmedia poses some challenges, including lack of context, irrelevancy,\nhomographs, homophones, homonyms, slangs, and colloquialisms.\nIn an earlier study, we developed ChouBERT (\nJiang et al., 2022 )\nto detect farmers’ observations from tweets for pest monitoring.\nChouBERT takes a pre-trained CamemBERT (\nMartin et al., 2020 )\nmodel and further pre-trains it on a plant health domain corpus\nin French to improve the generalizability of plant health hazards\ndetection on Twitter. Some potential applications of ChouBERT are\nas follows:\n• The annotation and indexing of the key elements of plant health-\nrelated events in the text, including named entity recognition,\nentity linking, and relation extraction.\n• Topic modeling for detecting emerging issues from a collection\nof texts.\n• Natural language inference for ﬁnding precursors of pest attacks.\nIn this article, we explored the combination of GAN-\nBERT (\nCroce et al., 2020 ) and further pre-training with ChouBERT.\nWe present a discussion on the results and perspectives of\nthis combination on the text classiﬁcation task for plant health\nhazard detection.\n/two.tnum. Background\n/two.tnum./one.tnum. Pre-trained language models\nPre-trained language models (PLMs) are deep neural networks\nof pre-trained weights to vectorize sequences of words. Such\nvectorial representations obtain state-of-the-art results on NLP\ntasks, such as text classiﬁcation, text clustering, question-answering,\nand information extraction. PLMs suggest an objective engineering\n/one.tnumhttp://www.agrichatuk.org\n/two.tnumhttps://franceagritwittos.com\nparadigm for NLP: language model pre-training for extracting\ncontextualized features from text and ﬁne-tuning for downstream\ntasks. BERT (\nDevlin et al., 2019 ) is a PLM introduced in 2018 by\nGoogle that led to signiﬁcant improvements in this ﬁeld. BERT is pre-\ntrained in two stages: ﬁrst, a self-supervised task where the masked\nlanguage model (MLM) must retrieve masked words in a text; and\nsecond, a supervised task where the model must reﬁnd whether a\nsentence B is the continuation of a sentence A or not (next-sentence\nprediction, NSP). The pre-training produces in the end 12 stacked\nencoders which take a sequence of tokens as input and add a special\ntoken “[CLS]” at the beginning of the sequence and a “[SEP]” at\nthe end of each sentence, and calculates a ﬁxed-length vector for\neach token. Each dimension of these vectors represents how much\nattention that token should pay to the other tokens. For the text\nclassiﬁcation task, the vector of “[CLS]” represents the whole text.\nAmong the French varieties of BERT, CamemBERT (\nMartin et al.,\n2020) is a model based on the same architecture as BERT but trained\non a French corpus with MLM only. ChouBERT ( Jiang et al., 2022 )\ntakes a pre-trained CamemBERT-base checkpoint and further pre-\ntrains it with MLM over a corpus in French in the plant health\ndomain to improve performance in detecting plant health issues from\nshort texts, particularly, from Twitter.\n/two.tnum./two.tnum. Generative adversarial networks\nGenerative Adversarial Networks (GANs) (\nGoodfellow et al.,\n2014; Wang et al., 2017 ) are a family of neural networks that can\nbe commonly divided into two antagonistic parts: a generator and a\ndiscriminator, which compete during training. The generator aims to\nmimic real data by transforming noise, while the discriminator aims\nto determine if the data are real or produced by the generator. The\ndiscriminator’s classiﬁcation results then feed the generator’s training\nin turn. The training of GANs is known to suﬀer from the following\nfailure modes: gradient vanish, mode collapse, and non-convergence.\nGradient vanish occurs when the discriminator cannot give enough\ninformation to improve the generator. Mode collapse occurs when\nthe generator gets stuck generating only one mode. Non-convergence\noccurs when the generator tends to overﬁt to the discriminators\ninstead of reproducing the real data distribution.\nMany variants of GANs are proposed to improve sample\ngeneration and the stability of training. Some of these variants are the\nconditional GANs (CGANs), where the generator is conditional on\none or more labels (\nMirza and Osindero, 2014 ), and semi-supervised\nGANs (Salimans et al., 2016 ) (SS-GANs), where the discriminator is\ntrained over its k-labeled examples plus the data generated by the\ngenerator as a new label “ k + 1”(see in Figure 1).\n/two.tnum./three.tnum. GAN-BERT architecture\nGenerative adversarial network-bidirectional encoder\nrepresentations from transformers (GAN-BERT) ( Croce et al., 2020 )\nextends the ﬁne-tuning of BERT-like pre-trained language models\n(PLMs) for text classiﬁcation with a semi-supervised discriminator–\ngenerator setting, introduced in the study by\nSalimans et al. (2016).\nLet us project all the data points in a d-dimensional hidden space,\nthen the data vector h ∈ Rd. The generator GSSGAN is a multi-layer\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nJiang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nFIGURE /one.tnum\nTraining an SS-GAN architecture.\nperceptron (MLP) that considers a noise vector as input and attempts\nto mimic the PLM representation of real data. The discriminator\nDSSGAN is another MLP that obtains either PLM representation of\nreal labeled and unlabeled data hR = PLM(x), hR ∈ Rd, or a faked\nrepresentation hG = g(noise), hG ∈ Rd, produced by GSSGAN as\ninput, converts the input vector to inner representation hD ∈ Rd,\nand performs a multi-class classiﬁcation. DSSGAN is trained over two\nobjectives: 1) the correct classiﬁcation of real data into K classes from\nlabeled data (supervised learning) and 2) the distinction of generated\ndata from real unlabeled data (unsupervised learning).\nWe deﬁne pm(ˆy = y|x, y ∈ (1, ..., k)) as the probability given by\nthe model m that an example x belongs to one of the k target classes\nand pm(ˆy = y|x, y = k + 1) as the probability of x being fake data. Let\nPR and PG denote the real data distribution and the generated data,\nrespectively. The loss function for training DSSGAN becomes:\nLD = LDsup + LDunsup (1)\nLDsup evaluates how well the real labeled data are classiﬁed:\nLDsup = − Ex,y∼PR log[pm(ˆy) = y|x, y ∈ (1, ..., k)] (2)\nLDunsup punishes the discriminator when it fails to recognize a fake\nexample or when it classiﬁes a real unlabeled example to be fake. If\nthe discriminator is free to assign any of the k target classes to the\nunlabeled data.\nLDunsup = − Ex∼PR log[1 − pm(ˆy = y|x, y = k + 1)]\n−Ex∼PG log[pm(ˆy = y|x, y = k + 1)] (3)\nAs for the generator GSSGAN ,\nCroce et al. (2020) deﬁnes the loss\nfunction as:\nLG = LGunsup + LGfeat (4)\nLGunsup penalizes GSSGAN when DSSGAN correctly ﬁnds\nfake examples:\nLGunsup = − Ex∼PG log[1 − pm(ˆy = y|x, y = k + 1)] (5)\nLet fD(x) denote the activation that DSSGAN uses to convert\nthe input data to its inner representation hD. LGfeat\n/three.tnummeasures the\nstatistical distance between the inner representation of real data hDR\nand the inner representation of generated data hDG .\nLGunsup = ∥ Ex∼PR f (x) − Ex∼PG f (x)∥2\n2 (6)\nThe PLM is part of the discriminator DSSGAN ; that is, when\nupdating DSSGAN , the weights of the PLM are also ﬁne-tuned.\nMoreover, at the beginning of each training epoch, the [CLS] vector\nof real examples is recalculated by the updated PLM.\n/two.tnum./four.tnum. GAN-BERT applications\nGenerative adversarial network-bidirectional encoder\nrepresentations from transformers (GAN-BERT) has been assessed\non diﬀerent datasets with diﬀerent PLMs. The original authors of\nGAN-BERT have applied it to English sentence-level classiﬁcation\ntasks, including topic classiﬁcation, question classiﬁcation (QC),\nsentiment analysis, and natural language inference (NLI) with the\noriginal BERT model (\nDevlin et al., 2019 ; Croce et al., 2020 ).\nLater, MT-GAN-BERT ( Breazzano et al., 2021 ) extends\nGAN-BERT to a multi-task learning (MTL) architecture to\nsolve simultaneously several related sentence-level classiﬁcation\ntasks reducing overﬁtting. MT-GAN-BERT is assessed with\nEnglish and Italian datasets, using BERT and UmBERTo\n/four.tnum,\nrespectively, for sentence embedding generation. The results of\n/three.tnum In the PyTorch implementation of the feature reg loss, the authors use:\ng_feat_reg = torch.mean(torch.pow(torch.mean(D_real_featur es, dim=/zero.tnum) -\ntorch.mean(D_fake_features, dim=/zero.tnum), /two.tnum)), which is not exactlythe same as\nthe deﬁnition given by the article; however, according to other au thor’s\nexperiments and our experience, there is no signiﬁcant impact on the training\nresult.\n/four.tnum\nhttps://huggingface.co/Musixmatch/umberto-wikipedia-uncased -v/one.tnum\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nJiang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nMT-GAN-BERT show that GAN-BERT-based models outperform\nBERT-based models with 100 and 200 labeled data. However,\nthe performance worsens when training GAN-BERT with 500\nlabeled data.\nIn the study of\nTa et al. (2022), the authors applied GAN-\nBERT for paraphrase identiﬁcation. They propose to ﬁlter noises\nin the labeled set to improve the performance and claim that,\nfor their use case, a lower learning rate helps the model to learn\nbetter. However, a too-small learning rate makes the accuracy to\nincrease slowly. In the study of\nSantos et al. (2022), the authors\napplied GAN-BERT with Portuguese PLMs to ﬁnd hate speech\nin social media. This study shows that text cleaning, including\nremoving users’ mentions, links, and repeated punctuation, improves\nthe performance of GAN-BERT-based classiﬁcation. Finally, the\nauthors infer that GAN-BERT is nonetheless more susceptible\nto noise.\nIn the study of\nMyszewski et al. (2022), the authors showed that\nthe combination of a GAN-BERT setting with a domain-speciﬁc\nPLM BioBERT (\nLee et al., 2019 ) outperforms the original GAN-\nBERT on a sentiment classiﬁcation task for clinical trial abstracts.\nHowever, the authors do not compare the results with those of\nPLM-only classiﬁcation. They neither provide a detailed analysis\nof the training. In this study, the authors presented 108 labeled\nexamples. The small number (23) of labeled samples in their\ntest set also makes the result unconvincing, which calls for more\nstudies to validate the combination of GAN-BERT and domain-\nspeciﬁc PLMs.\nIn the study of\nDanielsson et al. (2022), the authors studied\nwhether and how GAN-BERT can help in the classiﬁcation of\npatients bearing implants with a relatively small set of labeled\nelectronic medical records (EMRs) written in Swedish. In practice,\nthey further pre-trained a Swedish BERT model\n/five.tnumto provide the\n[CLS] representations of 64 and 512 tokens to the discriminator of\nGAN-BERT and perform experiments over varying training set sizes.\nTheir results show that combining GAN-BERT and a domain-speciﬁc\nPLM improves the classiﬁcation performance in speciﬁc challenging\nscenarios. However, the eﬀective zone of such scenarios remains to\nbe studied. The numerous applications of GAN-BERT witness its\ncapacity for ﬁne-tuning PLM on sentence-level classiﬁcation tasks in\na low resource setting. However, none of the works presented in this\nsection have studied the correlation between the labeled/unlabeled\ndata ratio and the performance of GAN-BERT or the impact of\nusing domain-speciﬁc PLMs. The lack of speciﬁcations for these\nhyperparameters makes the GAN-BERT setting a black box to\nnewcomers and could lead to expensive grid search experiments\nfor optimization (\nBergstra and Bengio, 2012 ). Furthermore, the\ngranularities of diﬀerent classiﬁcation problems are not comparable.\nTherefore, it is unfair to compare the performances of GAN-\nBERT plus the PLMs pre-trained in diﬀerent languages or domains\nover these tasks. In this study, we address these shortcomings by\napplying the GAN-BERT settings to CamemBERT (\nMartin et al.,\n2020), ChouBERT-16, and ChouBERT-32 and probing the diﬀerent\nlosses over varying labeled and unlabeled data sizes to give more\ninsights into when and how to train GAN-BERT for domain-speciﬁc\ndocument classiﬁcation.\n/five.tnumhttps://github.com/Kungbib/swedish-bert-models\n/three.tnum. Method\n/three.tnum./one.tnum. Data\nData annotation by domain experts is expensive and time-\nconsuming. Therefore, the main challenge of detecting natural\nhazards from textual contents on social media is to identify unseen\nrisks with low resources for training. We reuse the labeled tweets\nproduced by ChouBERT (\nJiang et al., 2022 ), tweets about corn borer,\nbarley yellow dwarf virus (BYDV) and corvids for training and\nvalidation, and tweets about unseen and polysemous terms such\nas “taupin” (wireworm in English) for testing the generalizability\nof the classiﬁer. Since the binary cross entropy loss adopted by\nthe discriminator of GAN-BERT favors the majority class when\ndata are unbalanced, for the diﬀerent training experiments, we\nsampled ChouBERT’s training data to 16, 32, 64, 128, 256, and 512\nsubsets, each subset having equal number of observations and non-\nobservations. We used the same validation data and test sets for all\nthe experiments. In the validation set, there were 79 observations and\n213 non-observations; in the test set, there were 58 observations and\n447 non-observations.\nAmong the data collected by ChouBERT, there is not only a\nsmall set of labeled tweets but also many unlabeled tweets. For the\nunsupervised learning, we have 12,308 unlabeled tweets containing\ncommon insect pest names (other than those in the labeled data) in\nFrance. We sampled 0, 1,024, 4,096, and 8,192 unlabeled data to study\nthe eﬀect of adding unlabeled data.\n/three.tnum./two.tnum. Text classiﬁcation with a pre-trained\nlanguage model\nFollowing the study of\nJiang et al. (2022), the ChouBERT models /six.tnum\nare further-pre-trained CamemBERT-base models over French Plant\nHealth Bulletins and Tweets and the ChouBERT pre-trained for 16\nepochs (denoted as ChouBERT-16) and for 32 epochs (denoted as\nChouBERT-32) are the most eﬃcient in ﬁnding observations about\nplant health issues. Thus, in this study, we combine GAN-BERT\nsettings with CamemBERT, ChouBERT-16, and ChouBERT-32.\nTo make our state-of-the-art model, we ﬁne-tune CamemBERT,\nChouBERT-16, and ChouBERT-32 for the sequence classiﬁcation\ntask over the same training/validation/test sets by adding a linear\nregression layer a to the ﬁnal hidden state h of the [CLS] token to\npredict the probability of a label o:\np(o|h) = softmax(Wah), (7)\nWhere Wa is the parameter matrix of this linear classiﬁer.\nDuring the training, the weights of the PLM are aﬀected\nalong with Wa. We developed these experiments with\nCamemBertForSequenceClassiﬁcation of the transformer package\n/seven.tnum.\nTo make the probability outputs of this linear regression layer\ncomparable with the label outputs from the GAN-BERT classiﬁer, we\nﬁxed the threshold of 0.5. For the predicted probability greater than\n0.5, we considered it as an observation, else as a non-observation.\nIt is worth mentioning that we used 0.5 as a threshold to simplify\n/six.tnumhttps://huggingface.co/ChouBERT\n/seven.tnumhttps://huggingface.co/transformers/v/three.tnum./zero.tnum./two.tnum/\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nJiang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nFIGURE /two.tnum\nPLM + GAN-BERT vs. PLM only with same training steps over varying siz es of training datasets.\nthe comparison with the PLM plus GAN-BERT classiﬁcation. When\napplying the PLM-only classiﬁcation to other datasets in other\ndomains, we might need to ﬁnd an optimal threshold depending on\nthe real needs for precision or recall. Based on the results presented\nin the study of\nJiang et al. (2022), we ﬁxed the learning rate to 2e −5,\nthe maximum sequence length to 128, and ﬁt the classiﬁer for 10\nepochs. We set the batch size to ( training_data_size/ 8) to have the\nsame steps for the diﬀerent training data sizes.\n/three.tnum./three.tnum. Experimental setup\nFor our experiments, we used GAN-BERT’s latest PyTorch\nimplementation,\n/eight.tnumwhich is compatible with the transformer package.\nWe ﬁxed the max sequence length of the PLM to 128. We ﬁxed\nthe number of hidden layers in G and in D to 1 and the size of\nG’s input noisy vectors to 100. We used the following learning rate\ncombinations ( D, G): (5e-5, 5e-5), (1e-5, 1e-5), and (5e-6, 1e-6). We\napplied the AdamW (\nLoshchilov and Hutter, 2017 ) optimizer with\nand without a cosine scheduler. To limit the number of variables, we\n/eight.tnumhttps://github.com/crux/eight.tnum/two.tnum/ganbert-pytorch\nconduct two groups of experiments. In the ﬁrst group, we ﬁxed the\nbatch size per GPU to 32 and epochs to 30. We trained the GAN-\nBERT architectures over increasing labeled data sizes (16, 32, 64, 128,\n256, and 512) and unlabeled data sizes (1,024, 4,096, and 8,192). In\nthe second group, we ﬁxed the training steps of each (labeled and\nunlabeled) pair by setting the batch size to ( unlabeled_data_size/ 256).\nMoreover in the second group, we trained the GAN without\nunlabeled data. That is, in this group, the unsupervised learning\nlearns the features from labeled data only. We ﬁxed the batch size to\n4 and set epochs to (1, 024 / train_data_size + log2(train_data_size))\nto approximate the number of training steps in the experiments with\nunlabeled data.\n/four.tnum. Results and evaluation\n/four.tnum./one.tnum. Overall metrics\nAs the validation set and the test set were unbalanced and that\nour interest is to ﬁnd out the observations, we plot the F1 score of the\nobservation class F1observation and the macro average F1 score of the\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nJiang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nFIGURE /three.tnum\nPLM + GAN-BERT performance with ﬁxed steps, training dataset size s = /one.tnum/six.tnum, /three.tnum/two.tnum, /six.tnum/four.tnum, /one.tnum/two.tnum/eight.tnum, /two.tnum/five.tnum/six.tnum, and /five.tnum/one.tnum/two.tnum.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org\nJiang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nFIGURE /four.tnum\nPLM + GAN-BERT performance with ﬁxed batch size to /three.tnum/two.tnum, training dataset sizes = /one.tnum/six.tnum, /three.tnum/two.tnum, /six.tnum/four.tnum, /one.tnum/two.tnum/eight.tnum, /two.tnum/five.tnum/six.tnum, and /five.tnum/one.tnum/two.tnum.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/seven.tnum frontiersin.org\nJiang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nFIGURE /five.tnum\nA macroscopic view of the evolution of training losses, with label ed data sizes of /one.tnum/six.tnum and /six.tnum/four.tnum over /one.tnum/five.tnum epochs. We ﬁx unlabeled size to /four.tnum/zero.tnum/nine.tnum/six.tnum, Learning rates\nof the Discriminator and Generator to /five.tnume-/six.tnum and /one.tnume-/six.tnum.\nFIGURE /six.tnum\nTraining data set size = /one.tnum/six.tnum.\nwhole classiﬁcation.\nF1macro = (F1observation + F1non−observation)/ 2 (8)\nLet us consider a dummy classiﬁer as our baseline model.\nIf it predicts that all the examples in the validation set are non-\nobservations, the F1observation, F1macro, and accuracy become 0, 0.42,\nand 0.73, respectively; if it predicts that all are observations,\nthe F1observation, F1macro, and accuracy become 0.43, 0.21,\nand 0, respectively.\nWe present the overall results of the ﬁxed-step experiments in\nFigure 2, which are the most representative and stable. By comparing\nthe maximum F1 scores of each conﬁguration during the training in\nFigures 2–4, we believe the performance of the classiﬁers on both the\nFrontiers in Artiﬁcial Intelligence /zero.tnum/eight.tnum frontiersin.org\nJiang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nFIGURE /seven.tnum\nTraining data set size = /six.tnum/four.tnum.\nvalidation and test sets to be continuous and relatively stable in a\nperiod, once the training converges. In other words, overﬁtting will\nnot immediately cause huge drops. It is worth pointing out that, on\nthe unbalanced validation and test sets, the F1 score (the objective of\nour classiﬁcation task) and the binary cross entropy loss (the objective\nof GAN-BERT’s training) are not completely aligned and may lead to\nsuboptimal convergence.\nCompared to PLM-only classiﬁcation, the PLM plus GAN-BERT\nsetting improves the scores over the validation and test sets of unseen\nhazards with 32, 64, 128, and 256 training data. In\nFigure 3, we\ndepict the performance with varying unlabeled data sizes. In both\nﬁgures, we can see that the deep blue lines (ChouBERT-32) are above\nthe yellow lines (CamemBERT), which is clearly coherent with the\nresults presented in\nJiang et al. (2022), indicating that pre-training\nhelps to improve generalizability. The representational similarity\nanalysis in\nMerchant et al. (2020) shows that “ﬁne-tuning has a much\ngreater impact on the token representations of in-domain data” and\nsuggests ﬁne-tuning to be “conservative.” In our experiments, we\ndid not observe that the SSGAN setting without domain unlabeled\ndata helps the model generalization for the identiﬁcation of tweets\nabout upcoming unseen hazards. For small training data sizes, adding\nunlabeled data helps to improve the performance on the test set,\nbut adding more unlabeled data consumes more computational\nresources without making signiﬁcant diﬀerence. We observe similar\nphenomena in the ﬁxed batch size group results in\nFigure 4, where\nadding more unlabeled data brings more training steps per epoch and\neventually reduces the Lsup steadily within the same training epochs.\nIn all our experiments with 512 labeled data, PLM-only solutions\noutperform PLM plus GAN-BERT solutions, while, in experiments,\nwith between 32 and 256 labeled data, PLM+GAN-BERT improves\nthe performance on the validation and test sets, which corresponds to\nthe results presented in\nBreazzano et al. (2021) and Danielsson et al.\n(2022).\n/four.tnum./two.tnum. The instability of the GAN-BERT setting\nwith ChouBERT models\nEven though the ﬁne-tuning of pre-trained transformer-based\nlanguage models such as BERT has achieved state-of-the-art\nresults on NLP tasks, ﬁne-tuning is still an unstable process.\nTraining the same model with multiple random seeds can result\nin diﬀerent performances on a task as described in the study\nof\nMosbach et al. (2021). This training instability is the reason why\nwe have not found the best labeled/unlabeled ratio to maximize\nthe performance. In\nFigure 5, we illustrate the training losses of\nthe discriminator and the generator when given diﬀerent sizes\nof labeled data with ﬁxed unlabeled data size and learning rate.\nThe training with ChouBERT models has more diﬃculties to\nconverge than with CamemBERT. Thus, we explored the evolution\nof diﬀerent losses and the classiﬁers’ performance metrics on the\nvalidation and test sets in\nFigures 6, 7, where the discriminators’\nlosses with ChouBERT-16 take more epochs to decrease\nthan with CamemBERT.\nIt is immediately clear that discriminators’ losses had the same\nshape as Lsup. In particular, to present the evolution of LGfeat at the\nsame scale as the other losses, we multiplied its value by 10 to draw\nits line. Compared with ChouBERT-16, the Lsup had more diﬃculties\ndecreasing than with CamemBERT. We interpret the increase of LGfeat\nas the generator catching up with the ﬁne-tuning of PLM and the\ndecrease of LGfeat toward its initial value as the end of the major\nchanges of ﬁne-tuning.\nAccording to the authors of SSGAN (\nSalimans et al., 2016 ),\n“in practice, Lunsup will only help if it is not trivial to minimize\nfor our classiﬁer and we thus need to train G to approximate\nthe data distribution, ” which explains that, while the Lunsup of D\nand G converge at the same rhythm with CamemBERT and with\nChouBERT-16, the troubled decrease of Lsup with ChouBERT-16\nFrontiers in Artiﬁcial Intelligence /zero.tnum/nine.tnum frontiersin.org\nJiang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nrenders worse F1 scores than those with CamemBERT. For example,\nin the group with 16 training examples (see\nFigure 6), the test\nF1observation scores with ChouBERT-16 are switching between 0 and\n0.43, which means that the classiﬁer predicts either all as non-\nobservation or all as observation. Considering the unbalanced nature\nof our validation and test sets, all-observation predictions and all-\nnon-observation predictions are two local Nash equilibria for the\ntraining of our SSGAN.\nIt is also remarkable that, in the group with 64 training\nexamples (see\nFigure 7), ChouBERT-16 gives better F1 scores than\nCamemBERT in the early stages. However, after the bounces of Lsup,\ndespite the ﬁne-tuning, helps it to decrease again, the F1 scores\nare not as good as before because the eﬀect of Lunsup is already\ngone. We can also observe this phenomenon in the group with\n32 training examples. Interestingly, when repeating the experiments\nwith the same hyperparameters, the “troubled decrease” of Lsup does\nnot always happen, but statistically, most of them can happen with\nChouBERT models, especially ChouBERT-16. Our strategies against\nthe “troubled decrease” include the following:\n• Using a smaller learning rate with more training epochs at the\ncost of computational resources (see\nTa et al., 2022 ).\n• Applying a smaller learning rate to G than to D (see Heusel et al.,\n2017).\n• Applying schedulers and down-sampling the majority class to\nbalance the training data–in our case, the upsampling proposed\nby the original code of GAN-BERT does not help.\nWith the optimizations mentioned above, Lsup with CamemBERT\ndecreases at a steady pace and “troubled decrease” happens less\noften with ChouBERT models. When we examined the embeddings\nof [CLS] produced by the PLM, we found that there is more\nvariance in each dimension of CamemBERT embeddings than in\neach dimension of ChouBERT embeddings, before and after the\nﬁne-tuning: VarCamemBERT > VarChouBERT−32 > VarChouBERT−16.\nThus, ChouBERT models produce more homogeneous encodings\nthan CamemBERT. This explains why ChouBERT embeddings are\nmore generalizable for detecting unseen hazards: the embeddings of\ntexts containing unseen hazards are more similar to those of seen\nhazards, so the downstream classiﬁer is more familiar with these\nvectors. It also indicates that the diﬀerences between observations\nand non-observations are more subtle in ChouBERT’s latent space.\nThus, the training of GAN plus ChouBERT needs lower learning rates\nto converge, while GAN plus CamemBERT is a robust approach to\nconverge in most conﬁgurations.\n/five.tnum. Conclusion\nIn this article, we demonstrate that combining further-pre-\ntrained language models and GAN-BERT beneﬁts from the\ngeneralizability of the domain-speciﬁc PLM to classify unseen\nhazards. We also demonstrate that training such a combination may\nalso suﬀer from extra instabilities compared to using GAN-BERT\nwith CamemBERT, a general PLM. Our results validate that the GAN-\nBERT setting improves the task of natural hazard classiﬁcation for\ndatasets containing between 32 and 256 instances of labeled data.\nBased on our experimental studies, we give our suggestions to\nreduce the instability such as: (1) The Lsup needs a certain minimum\nnumber of steps to decrease to zero. For a ﬁxed batch size, adding\nunlabeled data makes more training steps to go through in each\nepoch, consequently allowing Lsup to decrease at a similar pace as\nLunsup. When the number of unlabeled data is limited, using smaller\nbatch sizes and training for more epochs is also a good approach.\n(2) If the task is not too domain-speciﬁc, in other words, when the\nfurther pre-trained language model cannot signiﬁcantly outperform\nthe general language model in the PLM-only classiﬁcation, using a\ngeneral language model with the GAN-BERT setting is safer. On the\nother hand, if the task is highly domain-speciﬁc, it is better to apply\nschedulers, downsample the majority class to balance the training\ndata, and use smaller learning rates to train GAN-BERT with further-\npre-trained language models. (3) We need to choose a suitable PLM.\nWe proved that ChouBERT-32 outperforms ChouBERT-16 in an\nSSGAN setting. The perspectives and developments are numerous\nto increase the stability of domain-speciﬁc text classiﬁcation using\nGAN-BERT; for example, how to further pre-train PLMs to adapt\nbetter SSGAN setting is yet to investigate.\nData availability statement\nThe data analysed in this study are subject to the following\nlicenses/restrictions: Redistribution of the collected Twitter data is\nrestricted by the Twitter Terms of Service, Privacy Policy, Developer\nAgreement, and Developer Policy. The tweet IDs of the labeled tweets\nand the labels will be made available by the authors.\nAuthor contributions\nSJ and SC were responsible for the conception. RA, SC, and\nFR were responsible for administrative support and the provision\nof study materials. SJ was responsible for data collection and\nexperiments. All authors contributed to the manuscript revision,\nread, and approved the submitted version.\nFunding\nThis study received funding from the University of Reims\nChampagne-Ardenne and ISEP Paris.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the reviewers.\nAny product that may be evaluated in this article, or claim that may\nbe made by its manufacturer, is not guaranteed or endorsed by the\npublisher.\nFrontiers in Artiﬁcial Intelligence /one.tnum/zero.tnum frontiersin.org\nJiang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/seven.tnum/two.tnum/three.tnum/two.tnum/nine.tnum\nReferences\nBergstra, J., and Bengio, Y. (2012). Random search for hyper- parameter optimization.\nJ. Mach. Learn. Res . 13, 281–305.\nBreazzano, C., Croce, D., and Basili, R. (2021). “Multi-task a nd generative adversarial\nlearning for robust and sustainable text classiﬁcation, ” in NL4AI@ AI* IA (Berlin;\nHeidelberg: Springer-Verlag), 228–244.\nCroce, D., Castellucci, G., and Basili, R. (2020). “GAN-BERT: gen erative adversarial\nlearning for robust text classiﬁcation with a bunch of labeled ex amples, ” inProceedings of\nthe 58th Annual Meeting of the Association for Computational Linguis tics (Association for\nComputational Linguistics), 2114–2119. doi: 10.18653/v1/ 2020.acl-main.191\nDanielsson, B., Santini, M., Lundberg, P., Al-Abasse, Y., Jön sson, A., Eneling, E., et\nal.tridsman, M. (2022). “Classifying implant-bearing patients via their medical histories:\na pre-study on swedish emrs with semi-supervised gan-bert, ” i n Proceedings of the 13th\nLREC Conference (LREC2022) (Marseille), 21–23.\nDefour, T. (2018). EIP-AGRI Brochure Agricultural Knowledge and Innovation\nSystems. EIP-AGRI-European Commission. Available online at: https://ec.europa.eu/eip/\nagriculture/en/publications/eip-agri-brochure-agricultur al-knowledge-and\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). “ BERT: pre-training\nof deep bidirectional transformers for language understand ing, ” in Proceedings\nof the 2019 Conference of the North American Chapter of the Association fo r\nComputational Linguistics: Human Language Technologies, Volume 1 (Lon g and\nShort Papers) (Minneapolis, MI: Association for Computational Linguistics ),\n4171–4186.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde- Farley, D., Ozair,\nS., et al. (2014). “Generative adversarial nets, ” in Proceedings of the 27th International\nConference on Neural Information Processing Systems – Volume 2 (Montreal, QC: MIT\nPress), 2672–2680.\nHammer, G. L., Hansen, J., Phillips, J., Mjelde, J. W., Hill, H., Love , A., et al. (2001).\nAdvances in application of climate prediction in agriculture. Agric Syst . 70, 515–553.\ndoi: 10.1016/S0308-521X(01)00058-0\nHeusel, M., ramsauer, H., Unterthiner, T., Nessler, B., and Ho chreiter, S. (2017).\n“GANs trained by a two time-scale update rule converge to a local na sh equilibrium, ” in\nProceedings of the 31st International Conference on Neural Information Pr ocessing Systems\n(Long Beach, CA: Curran Associates), 6629–6640.\nJiang, S., Angarita, R., Cormier, S., Orensanz, J., and Rous seaux, F. (2022).\n“ChouBERT: Pre-training french language model for crowdsen sing with tweets in\nphytosanitary context, ” in Research Challenges in Information Science: 16th International\nConference, RCIS 2022 (Barcelona: Springer), 653–661.\nJiménez, D., Dorado, H., Cock, J., Prager, S. D., Delerce, S., Grillon, A., et al. (2016).\nFrom observation to information: data-driven understandi ng of on farm yield variation.\nPLoS ONE 11, e0150015. doi: 10.1371/journal.pone.0150015\nKenny, U., and Regan, A. (2021). Co-designing a smartphone app for and with\nfarmers: empathising with end-users’ values and needs. J. Rural Stud . 82, 148–160.\ndoi: 10.1016/j.jrurstud.2020.12.009\nLee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., et al. (20 19). BioBERT:\na pre-trained biomedical language representation model for bi omedical text mining.\nBioinformatics 36, 1234–1240. doi: 10.1093/bioinformatics/btz682\nLoshchilov, I., and Hutter, F. (2017). Decoupled weight decay r egularization. arXiv\npreprint arXiv:1711.05101. doi: 10.48550/arXiv.1711.05101\nMartin, L., Muller, B., Ortiz, S. P. J., Dupont, Y., Romary, L., d e la Clergerie, E., et al.\n(2020). “Camem BERT: A tasty french language model, ” in Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics (Association for Computational\nLinguistics), 7203–7219. doi: 10.18653/v1/2020.acl-main .645\nMassod, N., Akram, R., Fatime, M., Mubeen, M., Hussain, S., S hakeel, M., et al. (2022).\nInsect pest management under climate change. Build. Clim. Resil. Agri. Theor. Pract.\nFuture Prespect. 225–237. doi: 10.1007/978-3-030-79408-8_15\nMerchant, A., Rahimtoroghi, E., Pavlick, E., and Tenney, I. ( 2020). “What happens\nto BERT embeddings during ﬁne-tuning?” in Proceedings of the Third BlackboxNLP\nWorkshop on Analyzing and Interpreting Neural Networks for NLP (Association for\nComputational Linguistics), 33–44. doi: 10.18653/v1/2020 .blackboxnlp-1.4\nMirza, M., and Osindero, S. (2014). Conditional generative adversarial nets . Technical\nReport arXiv:1411.1784, arXiv.\nMosbach, M., Andriushchenko, M., and Hlakow, D. (2021). “On t he stability\nof ﬁne-tuning BERT: Misconceptions, explanations, and strong baselines, ” in 9th\nInternational Conference on Learning Representations, ICLR 2021, V irtual Event, Austria\n(OpenReview.net).\nMozaﬀari, G. A. (2022). “Climate change and its consequences in agriculture, ” inThe\nNature, Causes, Eﬀects and Mitigation of Climate Change on the Env ironment, ed S. A.\nHarris (Rijeka: IntechOpen). doi: 10.5772/intechopen.1014 44\nMyszewski, J. J., Klossowski, E., Meyer, P., Bevil, K., Klesius , L., and Schroeder, K.\nM. (2022). Validating GAN-BioBERT: a methodology for assessin g reporting trends in\nclinical trials. Front. Digit. Health 4, 878369. doi: 10.3389/fdgth.2022.878369\nPatil, V., Al-Gaadi, K. A., Biradar, D. P., and Madugundu, R. (2 012). “Internet\nof things (iot) and cloud computing for agriculture: an overvie w, ” in Proceedings of\nAgro-Informatics and Precision Agriculture (AIPA 2012) (India), 292–296.\nPatrício, D. I., and Rieder, R. (2018). Computer vision and ar tiﬁcial intelligence in\nprecision agriculture for grain crops: a systematic review. Comput. Electron. Agric . 153,\n69–81. doi: 10.1016/j.compag.2018.08.001\nPhillips, T., Klerkx, L., and McEntee, M. (2018). “An investigat ion of social media’s\nroles in knowledge exchange by farmers, ” in 13th European International Farming Systems\nAssociation (IFSA) Symposium, Farming systems: facing uncertai nties and enhancing\nopportunities, 1–5 July 2018 (Chania: International Farming Systems Association Europe ),\n1–20.\nRiley, M., and Robertson, B. (2021). #farming365 - exploring fa rmers’ social\nmedia use and the (re)presentation of farming lives. J. Rural Stud . 87, 99–111.\ndoi: 10.1016/j.jrurstud.2021.08.028\nSalimans, T., Goodfellow, I., Zaremba, W., Chueng, V , Radford, A ., and Chen, X.\n(2016). “Improved techniques for training GANs, ” in Proceedings of the 30th International\nConference on Neural Information Processing Systems (Barcelona: Curran Associates),\n2234–2242.\nSantos, R. B., Matos, B. C., Carvalho, P., Batista, F., and Rib eiro, R. (2022). “Semi-\nsupervised annotation of portuguese hate speech across social media domains, ” in 11th\nSymposium on Languages, Applications and Technologies (SLATE 20 22), volume 104 of\nOpen Access Series in Informatics (OASIcs) (Dagstuhl), 11, 1–11, 14.\nTa, H. T., Rahman, A. B. S., Najjar, L., and Geibukh, A. (2022). JGAN-BERT, an\nadversarial learning architecture for paraphrase identiﬁcat ion. CEUR Worksho Proc. 3202.\nTripicchio, P., Satler, M., Dabisias, G., Ruﬀaldi, E., and Avizz ano, C. A. (2015).\n“Towards smart farming and sustainable agriculture with dron es, ” in2015 International\nConference on Intelligent Environments (Prague: IEEE), 140–143.\nWang, D., Abdelzaher, T., and Kaplan, L. (2015). Social Sensing: Building Reliable\nSystems on Unreliable Data, 1st Edn. San Francisco, CA: Morgan Kaufmann Publishers\nInc.\nWang, K., Gou, C., Duan, Y., Lin, Y., Zheng, X., and Wang, F.-Y. (2017). Generative\nadversarial networks: introduction and outlook. IEEE/CAA J. Autom. Sin . 4, 588–598.\ndoi: 10.1109/JAS.2017.7510583\nFrontiers in Artiﬁcial Intelligence /one.tnum/one.tnum frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7673425674438477
    },
    {
      "name": "Generalizability theory",
      "score": 0.6388768553733826
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5980467200279236
    },
    {
      "name": "Fine-tuning",
      "score": 0.5876362323760986
    },
    {
      "name": "Language model",
      "score": 0.5744747519493103
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5214097499847412
    },
    {
      "name": "Transformer",
      "score": 0.5150873064994812
    },
    {
      "name": "Generative grammar",
      "score": 0.4749051332473755
    },
    {
      "name": "Generative adversarial network",
      "score": 0.46255603432655334
    },
    {
      "name": "Machine learning",
      "score": 0.4612158536911011
    },
    {
      "name": "Natural language processing",
      "score": 0.366946816444397
    },
    {
      "name": "Deep learning",
      "score": 0.29583191871643066
    },
    {
      "name": "Engineering",
      "score": 0.09006452560424805
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}