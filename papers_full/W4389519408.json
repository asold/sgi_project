{
  "title": "A Pretrained Language Model for Cyber Threat Intelligence",
  "url": "https://openalex.org/W4389519408",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2111103537",
      "name": "Youngja Park",
      "affiliations": [
        "University of Pennsylvania",
        "IBM Research - Thomas J. Watson Research Center",
        "Philadelphia University"
      ]
    },
    {
      "id": "https://openalex.org/A3022210246",
      "name": "Weiqiu You",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4308408710",
    "https://openalex.org/W3089969421",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4226346873",
    "https://openalex.org/W2805641541",
    "https://openalex.org/W2757933620",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4319079731",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3005972702",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4287887967",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2741608233",
    "https://openalex.org/W3099950029"
  ],
  "abstract": "We present a new BERT model for the cybersecurity domain, CTI-BERT, which can improve the accuracy of cyber threat intelligence (CTI) extraction, enabling organizations to better defend against potential cyber threats. We provide detailed information about the domain corpus collection, the training methodology and its effectiveness for a variety of NLP tasks for the cybersecurity domain. The experiments show that CTI-BERT significantly outperforms several general-domain and security-domain models for these cybersecurity applications indicating that the training data and methodology have a significant impact on the model performance.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 113–122\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nA Pretrained Language Model for Cyber Threat Intelligence\nYoungja Park\nIBM T. J. Watson Research Center\nYorktown Heights, NY , USA\nyoung_park@us.ibm.com\nWeiqiu You\nUniversity of Pennsylvania\nPhiladelphia, PA, USA\nweiqiuy@seas.upenn.edu\nAbstract\nWe present a new BERT model for the cyberse-\ncurity domain, CTI-BERT, which can improve\nthe accuracy of cyber threat intelligence (CTI)\nextraction, enabling organizations to better de-\nfend against potential cyber threats. We provide\ndetailed information about the domain corpus\ncollection, the training methodology and its ef-\nfectiveness for a variety of NLP tasks for the\ncybersecurity domain. The experiments show\nthat CTI-BERT significantly outperforms sev-\neral general-domain and security-domain mod-\nels for these cybersecurity applications, indicat-\ning that the training data and methodology have\na significant impact on the model performance.\n1 Introduction\nIn response to rapidly growing cyber-attacks, cy-\nbersecurity experts publish many CTI reports, de-\ntailing on new security vulnerabilities and malware.\nWhile these reports help security analysts to better\nunderstand the cyber-threats, it is very difficult to\ndigest all the information in a timely manner. Thus,\nautomatic extraction of CTI from text has gained a\nlot of attention from the cybersecurity community.\nHowever, general-domain language models\n(LMs) are not effective for cybersecurity text due to\ndifferences in terminology and styles. Earlier stud-\nies have demonstrated that domain-specific LMs\nare crucial for domain-specific applications (Belt-\nagy et al., 2019; Lee et al., 2020; Huang et al., 2019;\nPeng et al., 2019; Gu et al., 2022; Chalkidis et al.,\n2020; Hu et al., 2022; Priyanka Ranade and Finin,\n2021; Aghaei et al., 2023).\nTwo different approaches have been used to pro-\nduce domain-specific language models: continual\npretraining and pretraining from scratch. The con-\ntinual pretraining method takes an existing general-\ndomain model and continues training the model\nusing a domain-specific corpus. While this ap-\nproach is useful, especially when the size of the\ndomain-specific corpus is small, the vocabulary\nof the new model remains largely same as that of\nthe original model. Most domain-specific terms\nare thus out of vocabulary. The pretraining from\nscratch approach trains a new tokenizer to con-\nstruct a domain-specific vocabulary and trains the\nlanguage model using only its own corpus. Beltagy\net al. (2019), Gu et al. (2022), and Hu et al. (2022)\nhave trained BERT models from scratch for the bio-\nmedicine, computer science, and political science\nareas. These studies show that pretraining from\nscratch outperforms the continual pretraining.\nRecently, a few transformers-based LMs\nhave been built for the cybersecurity domain.\nCyBERT (Priyanka Ranade and Finin, 2021) trains\na BERT model, and SecureBERT (Aghaei et al.,\n2023) trains a RoBERTa model using the contin-\nual pretraining method. jackaduma (2022) intro-\nduces SecBERT and SecRoBERTa models trained\nfrom scratch. However, these models either do\nnot provide training details or are not evaluated on\nmany cybersecurity tasks.\nWe present CTI-BERT, a BERT model pretrained\nfrom scratch with a high quality cybersecurity\ncorpus containing CTI reports and publications.\nIn CTI-BERT, both the vocabulary and the model\nweights are learned from our corpus. Further, we in-\ntroduce a variety of sentence-level and token-level\nclassification tasks and benchmark datasets for the\nsecurity domain. The experimental results demon-\nstrate that CTI-BERT outperforms other general-\ndomain and security domain models, confirming\nthat training a domain model from scratch with a\nhigh quality domain-specific corpus is critical.\nTo the best of our knowledge, this work provides\nthe most comprehensive evaluations for classifi-\ncation task within the security domain. Accom-\nplishing these tasks is a crucial part of the broader\nprocess of automatically extracting CTI, suggesting\nappropriate mitigation strategies, and implement-\ning counter-measurements to thwart attacks. Thus,\nwe see our work as an essential milestone towards\n113\nmore intelligent tools for cybersecurity systems.\nThe main contributions of our work are the fol-\nlowing:\n• We curate a large amount of high quality cy-\nbersecurity datasets specifically designed for\ncyber-threat intelligence analysis.\n• We develop a pre-trained BERT model tai-\nlored for the cybersecurity domain.\n• We perform extensive experiments on a wide\nrange of tasks and benchmark datasets for the\nsecurity domain and demonstrate the effective-\nness of our model.\n2 Training Datasets\nWe curated a cybersecurity corpus from various rep-\nutable data sources. The documents are profession-\nally written and cover key security topics including\ncyber-campaigns, malware, and security vulnerabil-\nities. Most of the documents are in HTML and PDF\nformats. We processed the files using the Apache\nTika parsers1 to extract the file content. Then, we\ndetected sentence boundaries and discarded sen-\ntences if the percentage of word tokens is less than\n10% in the sentences. Table 1 summarizes our\ndocument categories and their statistics.\nDocument Set # Sentences # Tokens\nAttack Description 22,086 544,260\nSecurity Textbook 20,371 438,720\nAcademic Paper 1,156,026 23,245,317\nSecurity Wiki 298,450 7,338,609\nThreat Report 84,639,372 1,195,547,581\nVulnerability Description 598,265 14,123,559\nTotal 86,734,570 1,241,238,046\nTable 1: Summary of our datasets\nAttack Description This dataset includes de-\nscriptions about known cyber-attack techniques\ncollected from MITRE ATT&CK 2 and CAPEC\n(Common Attack Pattern Enumeration and Clas-\nsification)3. They are carefully curated glossaries\ncontaining the attack technique name, the definition\nand examples, and potential mitigation approaches.\nSecurity Textbook The dataset contains two on-\nline text books for the CISSP (Certified Information\nSystems Security Professional) certification test.\n1https://tika.apache.org/\n2https://attack.mitre.org/\n3https://capec.mitre.org/\nThe CISSP textbooks cover all information secu-\nrity topics including access control, cryptography,\nhardware and network security, risk management\nand recovery planning.\nAcademic Paper This dataset contains all the pa-\npers in the proceedings of USENIX Security Sym-\nposium, a premier security conference, from year\n1990 through 2021.\nSecurity Wiki This dataset contains 7,919\nWikipedia pages belonging to the “Computer Se-\ncurity” category. We download the data starting\nfrom the ‘Computer Security’ category and recur-\nsively extracting pages from its subcategories. We\ndiscarded the subcategories not related to the cy-\nbersecurity domain.\nThreat Reports This dataset contains news arti-\ncles and white papers about cyber-campaigns, mal-\nware, and security vulnerabilities. These articles\nprovide in-depth analysis on a specific cyber-attack,\nincluding the attack techniques, any known charac-\nteristics of the perpetrator, and potential mitigation\nmethods. We collected the dataset from security\ncompanies and the APTnotes collection 4, which\nis a repository of technical reports on Advanced\nPersistent Threat (APT) groups.\nVulnerability This dataset contains records from\nCVE (Common Vulnerabilities and Exposures) 5\nand CWE (Common Weakness Enumeration) 6,\nwhich offer the catalogs of all known vulnerabil-\nities and provide information about the affected\nproducts, the vulnerability type, and the impact.\n3 Training Methodology\nWe first train the WordPiece tokenizer after lower-\ncasing the security text and produce a vocabulary\nwith 50,000 tokens. Training a tokenizer from\nscratch is beneficial, as it can recognize domain-\nspecific terms better. Table 13 in Appendix shows\nexamples of our tokenizer and BERT for recogniz-\ning security-related words.\nFollowing the observations by RoBERTa (Liu\net al., 2019), we trained CTI-BERT using only the\nMasked Language Modeling (MLM) objective us-\ning the HuggingFace’s MLM training script. The\nmodel was trained for 200,000 steps with 15% mlm\nprobability, the sequence length of 256, the total\n4https://github.com/aptnotes/data\n5https://cve.mitre.org\n6https://cwe.mitre.org/\n114\nbatch size of 2,048, the learning rate of 5e-4 with\nlearning rate warm-up to 10,000 steps and weight\ndecay of 0.01. We use the Adam optimizer with\nβ1 = 0.9, β2 = 0.98, ϵ= 1e− 6.\n4 Cybersecurity Applications\nWe evaluate CTI-BERT using several security\nNLP applications and compare its results with\nboth general-domain models and other cyberse-\ncurity domain models. The baseline models are\nbert-base-uncased, SecBERT (BERT models)\nand roberta-base, SecRoBERTa and SecureBERT\n(RoBERTa models). All the baseline models are\ndownloaded from HuggingFace.\nThe downstream applications can be categorized\nas sentence-level classification tasks and token-\nlevel classification tasks. The goal of the exper-\niments is to compare different pretrained models\nrather than optimizing the classification models for\nindividual tasks. Thus, we use the same model ar-\nchitecture and hyper-parameters to fine-tune mod-\nels for all sub-tasks in each application category.\n4.1 Masked Word Prediction\nFirst, we conduct the masked token prediction task\nto measure how well the models understand the do-\nmain knowledge. To ensure that the test sentences\nare not in the training data, we use five headlines\nfrom security news published in January and Febru-\nary, 20237. Table 2 shows the test sentences and\nthe models’ predictions. For each sentence, we\nconduct the masked token prediction twice with\ndifferent masked words. The upper line shows the\npredictions for <mask>1, and the lower line shows\nthe predictions for <mask>2 respectively.\nThe results clearly show thatCTI-BERT performs\nvery well in this test; its predictions are either the\nsame words (boldfaced) or synonyms (italicized).\nNote that CTI-BERT produces RAT for “PlugX\n<mask>”, which is a more specific term than the\nmasked word (‘malware’). RAT (Remote Access\nTrojan) is the malware family which PlugX be-\nlongs to. However, both SecBERT and SecRoBERTa\ndo not perform well for this test, even though\nthey were trained with security text. Interestingly,\nroberta-base performs better than these models\nand bert-base-uncased.\n7beepingcomputer.com\n4.2 Sentence Classification Tasks\nFor sentence or document-level classification, we\nadd onto the pretrained language models a classifi-\ncation head, with one hidden layer and one output\nprojection layer connected with tanh activation,\nwhich takes the average of the last hidden states of\nall tokens in sentences as the input. We fine-tune\nthe pretrained models together with the randomly\ninitialized classification layers, using 1,000 warm-\nup steps, with learning rate varied according to the\nformula in Vaswani et al. (2017). We use the Adam\noptimizer with β1 = 0.9, β2 = 0.999, and weight\ndecay of 0.01. All the models are trained for 50\nepochs with the batch size of 16 and the learning\nrate of 2e-5.\nFor the evaluation, we train five models with\nfive different seeds (42, 142, 242, 342, and 442)\nfor each task and report both the micro and macro\nmean F1 score (Mean) and the standard deviation\n(Std.) over the five models.\n4.2.1 ATT&CK Technique Classification\nThe key knowledge SoC analysts look for in CTI\nreports is information about malware behavior and\nthe adversary’s tactics and techniques. The MITRE\nATT&CK framework8 offers a knowledge base of\nthese adversary tactics and techniques, which has\nbeen used as a foundation for the threat models and\nmethodologies in many security products.\nTo facilitate research on identifying ATT&CK\ntechniques in prose-based CTI reports, MITRE cre-\nated TRAM9, a dataset containing sentences from\nCTI reports labeled with the ATT&CK techniques.\nWe observe that TRAM contains duplicate sen-\ntences across the splits. We remove the duplicates\nand keep only the classes with at least one sentence\nin train, development and test splits. The cleaned\ndataset contains 1,491 sentences, 166,284 tokens,\nand 73 distinct classes. More detailed statistics\nof the dataset is shown in Table 15 in Appendix.\nNote that this dataset is very sparse and imbal-\nanced. Table 3 shows the results of the six models\nfor this task. As we can see, CTI-BERToutperforms\nall other models by a large margin.\n4.2.2 IoT App Description Classification\nIoTSpotter is a tool for automatically identifying\nMobile-IoT (Internet of Things) apps, IoT-specific\nlibrary, and potential vulnerabilities in the IoT\n8https://attack.mitre.org\n9https://github.com/center-for-threat-informed-\ndefense/tram\n115\nMasked Sentence bert-base-uncased SecBERT CTI-BERT roberta-base SecRoBERTa SecureBERT\nNew Mirai <malware> 1 variant infects Linux de-\nvices to build DDoS <botnet>2.\nlinux . malware worm this malware\nattacks attacks botnets attacks commands botnets\nThe <Colonial>1 Pipeline incident is one of the most\ninfamous <ransomware>2 attacks\noil it colonial Pegasus the Olympic\npipeline targeted ransomware terrorist cyber cyber\nNew stealthy Beep malware focuses heavily on\n<evading>1 <detection>2\nintrusion antivirus evading stealth antivirus sandbox\n. 2009 detection detection . detection\nMicrosoft Exchange ProxyShell <flaws> 1\n<exploited>2 in new crypto-mining attack\nis previously vulnerability vulnerability Key Service\nresulting resulting exploited exploited eavesdrop used\nPlugX <malware> 1 hides on USB devices to\n<infect>2 new Windows hosts\nalso is rat 11 silently malware\ncreate open infect infect communicate infect\nTable 2: Masked Word Prediction (top-1). The actual words, instead of <mask>, are shown for reference.\nModel Micro-F1 Macro-F1\nMean Std. Mean Std.\nbert-base-uncased 61.13 0.73 38.58 0.70\nSecBERT 63.61 0.86 39.56 0.88\nCTI-BERT 69.30 0.96 46.62 1.66\nroberta-base 59.44 1.01 37.63 1.06\nSecRoBERTa 57.30 0.58 35.61 0.67\nSecureBERT 63.61 0.65 41.18 0.69\nTable 3: ATT&CK Technique Classification Results\napps (Jin et al., 2022). The authors created a dataset\ncontaining the descriptions of 7,237 mobile apps\nwhich are labeled with mobile IoT apps vs. non-IoT\napps with the distribution of approximately 45%\nand 55% respectively. They removed stopwords\nand put together all remaining tokens in the descrip-\ntion ignoring the sentence boundaries. We use the\ndatasets10 without any further processing. The data\nstatistics are shown in Table 16 in Appendix. The\nmodels’ classification results are shown in Table 4.\nModel Micro-F1 Macro-F1\nMean Std. Mean Std.\nbert-base-uncased 95.78 0.04 95.70 0.05\nSecBERT 94.22 0.21 94.12 0.21\nCTI-BERT 96.40 0.26 96.33 0.26\nroberta-base 95.88 0.26 95.82 0.26\nSecRoBERTa 94.59 0.39 94.48 0.40\nSecureBERT 96.27 0.13 96.19 0.13\nTable 4: Performance for IoT App Classification\n4.2.3 Malware Sentence Detection\nThe next two tasks, malware sentence detection\nand malware attribute classification, are borrowed\n10https://github.com/Secure-Platforms-Lab-W-\nM/IoTSpotter/tree/main/data/dataset\nModel Micro-F1 Macro-F1\nMean Std. Mean Std.\nbert-base-uncased 83.24 1.40 64.80 3.13\nSecBERT 83.82 1.13 70.06 2.69\nCTI-BERT 85.18 0.98 69.26 2.79\nroberta-base 83.30 1.37 66.5 1.44\nSecRoBERTa 84.24 1.01 70.95 2.04\nSecureBERT 83.59 1.14 61.74 6.32\nTable 5: Malware Sentence Classification Results\nfrom the SemEval-2018 Task 8, which consisted\nof four subtasks to measure NLP capabilities for\ncybersecurity reports (Phandi et al., 2018). The\ntask provided 12,918 annotated sentences extracted\nfrom 85 APT reports, based on the MalwareTextDB\nwork (Lim et al., 2017).\nThe first sub-task is to build models to ex-\ntract sentences about malware. The dataset is bi-\nased with the ratios of malware and non-malware\nsentences being 21% and 79% respectively as\nshown in Table 17 in Appendix. The results are\nlisted in Table 5 which shows that CTI-BERT and\nSecRoBERTaperform well on this task.\n4.2.4 Malware Attribute Classification\nThis task classifies sentences into the malware at-\ntribute categories as defined in MAEC (Malware\nAttribute Enumeration and Characterization) vo-\ncabulary11. MAEC defines the malware attributes\nin a 2-level hierarchy with four high-level attribute\ntypes—ActionName, Capability, StrategicObjec-\ntives and TacticalObjectives—and 444 low-level\ntypes. This sub-task was conducted by building\nmodels for each of the four high-level attributes.\nTable 23 in Appendix shows more details of this\n11https://maecproject.github.io/\n116\ndataset for the four high-level attributes. As we\ncan see, the datasets are very sparse with a large\nnumber of classes.\nTables 6–9 show the classification results for\nthe four malware attribute types. We can see that\nCTI-BERT performs well, being the best or second\nbest model, for all four attributes types.\n4.3 Token Classification Tasks\nHere, we compare the models’ effectiveness\nfor token-level classification using two security-\ndomain NER tasks and a token type detection task.\nWe use the standard sequence tagging setup and\nadd one dense layer as the classification layer on\ntop of the pretrained language models. The clas-\nsification layer assigns each token to a label us-\ning the BIO tagging scheme. Our system is im-\nplemented in PyTorch using HuggingFace’s trans-\nformers (Wolf et al., 2019). The training data is\nrandomly shuffled, and a batch size of 16 is used\nwith post-padding. We set the maximum sequence\nlength to 256 and use cross entropy loss for model\noptimization with the learning rate of 2e-5. All\nother training parameters were set to the default\nvalues in transformers. Similarly to the sentence\nclassification tasks, we train five models for each\ntask with the same five seeds for 50 epochs and\ncompare the average mention-level precision, re-\ncall and F1-score.\n4.3.1 NER1: Coarse-grained Security Entities\nCybersecurity entities have very distinct charac-\nteristics, and many of them are out of vocabulary\nterms. Here, we investigate if domain specific lan-\nguage models can alleviate the vocabulary gap. We\ncollected 967 CTI reports on malware and vulnera-\nbilities. The documents are labeled with the 8 entity\ntypes defined in STIX (Structured Threat Informa-\ntion Expression)12, which is a standard framework\nfor cyber intelligence exchange. The 8 types are\nCampaign (names of cyber campaigns), Course-\nOfAction (tools or actions to take to deter cyber\nattacks), ExploitTarget (vulnerabilities targeted for\nexploitation), Identity (individuals, groups or orga-\nnizations involved in attacks), Indicator (objects\nused to detect suspicious or malicious cyber ac-\ntivity), Malware (malicious codes used in cyber\ncrimes), Resource (tools used for cyber attacks);\nand ThreatActor (individuals or groups that commit\ncyber crimes). The size of the dataset and detailed\n12https://stixproject.github.io/releases/1.2\nstatistics of the entity types in the corpus are shown\nin Table 18 and Table 19 in Appendix. Table 10\nshows the NER results using the mention-level mi-\ncro average scores.\n4.3.2 NER2: Fine-grained Security Entities\nWe note that some STIX entity types (esp. Indi-\ncator) are very broad containing many different\nsub-types and, thus, are difficult to be directly used\nby automatic threat investigation applications. We\nredesigned the type system into 16 types by divid-\ning broad categories into their subcategories and\nannotated the test dataset from the NER1 task. We\nthen split the dataset into a 80:10:10 ratio for the\ntrain, dev and test sets. Table 20 and Table 21 in\nAppendix show the statistics of this dataset. The\nNER results in Table 11 show that most models\nperform better for the finer-grained types, and espe-\ncially CTI-BERT outperforms all other models by a\nlarge margin.\n4.3.3 Token Type Classification\nThe token type detection task is the sub-task2 from\nSemEval2018 Task8 which aims to classify tokens\nto Entity, Action and Modifier, and Other cate-\ngories. Action refers to an event. Entity refers to\nthe initiator of the Action (i.e., Subject) or the recip-\nient of the Action (i.e., Object). Modifier refers to\ntokens that provide elaboration on the Action. All\nother tokens are assigned to Other. More details\non the dataset are shown in Table 22 in Appendix.\nEven though the categories are not semantic\ntypes as in NER, this task can also be solved as\na token sequence tagging problem, and, thus, we\napply the same system used for the NER tasks.\nThe classification results are shown in Table 12.\nOverall, the models don’t perform very well likely\nbecause the mentions are long and semantically het-\nerogeneous. The results show that the BERT based\nmodels perform better than the RoBERTa-based\nmodels.\n5 Related Work\nMotivated by the large-scale foundational models’s\nsuccesses in many general domain NLP tasks, sev-\neral domain-specific language models have been\ndeveloped (Roy et al., 2017, 2019; Mumtaz et al.,\n2020). In scientific and bio-medical domains,\nthere are SciBERT (Beltagy et al., 2019), Blue-\nBERT (Peng et al., 2019), ClinicalBERT (Huang\net al., 2019), BioBERT (Lee et al., 2020) and Pub-\nMedBERT (Gu et al., 2022). In political and legal\n117\nModel Micro-F1 Macro-F1\nMean Std. Mean Std.\nbert-base-uncased 38.79 19.68 30.37 15.79\nSecBERT 43.64 3.09 33.25 2.97\nCTI-BERT 55.76 4.92 43.37 4.92\nroberta-base 56.36 4.11 44.04 3.41\nSecRoBERTa 40.00 2.27 29.03 2.39\nSecureBERT 52.12 2.97 39.97 3.32\nTable 6: Performance for ActionName attributes\nModel Micro-F1 Macro-F1\nMean Std. Mean Std.\nbert-base-uncased 60.68 3.91 51.51 5.41\nSecBERT 53.18 1.82 43.39 1.9\nCTI-BERT 60.91 2.34 52.23 4.39\nroberta-base 59.77 3.71 50.86 3.80\nSecRoBERTa 46.82 1.96 37.70 4.26\nSecureBERT 61.59 2.73 54.12 4.66\nTable 7: Performance for Capability attributes\nModel Micro-F1 Macro-F1\nMean std Mean std\nbert-base-uncased 43.71 1.46 29.31 2.27\nSecBERT 38.57 2.86 21.12 2.42\nCTI-BERT 45.14 4.30 28.11 4.69\nroberta-base 47.14 2.02 33.22 3.81\nSecRoBERTa 37.71 4.00 22.42 4.76\nSecBERT 44.00 4.98 30.74 6.98\nTable 8: Performance for StrategicObjective attributes\nModel Micro-F1 Macro-F1\nMean std Mean std\nbert-base-uncased 36.19 1.56 19.00 1.55\nSecBERT 35.24 2.54 19.58 2.75\nCTI-BERT 49.84 1.62 31.49 2.24\nroberta-base 42.54 0.63 23.95 1.15\nSecRoBERTa 35.87 3.27 20.37 4.18\nSecureBERT 40.32 4.33 24.37 4.38\nTable 9: Performance for TacticalObjective attributes\nModel Type Precison Recall F1\nbert-base-uncased 72.04 68.67 70.31\nSecBERT 69.74 63.98 66.73\nCTI-BERT 75.63 75.88 75.75\nroberta-base 72.52 68.99 70.70\nSecRoBERTa 68.00 59.46 63.44\nSecureBERT 73.47 72.51 72.99\nTable 10: NER1 Results (mention-level micro average)\nModel Precison Recall F1\nbert-base-uncased 73.44 68.23 70.73\nSecBERT 68.58 60.90 64.43\nCTI-BERT 83.35 78.62 80.91\nroberta-base 72.17 73.51 72.80\nSecRoBERTa 71.91 55.01 62.34\nSecureBERT 76.66 75.98 76.30\nTable 11: NER2 Results (mention-level micro average)\nModel Type Precison Recall F1\nbert-base-uncased 22.97 44.51 30.27\nSecBERT 21.63 36.20 27.02\nCTI-BERT 22.67 47.77 30.70\nroberta-base 15.05 17.44 15.97\nSecRoBERTa 14.18 20.71 16.81\nSecureBERT 22.58 46.97 30.46\nTable 12: Token Type Classification Results (mention-\nlevel micro average)\ndomains, there are ConflictBERT (Hu et al., 2022)\nand LegalBERT (Chalkidis et al., 2020). These\ndomain models have shown to improve the perfor-\nmance of downstream applications for the domain.\nThere have been several attempts to construct\nlanguage models for the cybersecurity domain.\nRoy et al. (2017, 2019) propose techniques to effi-\nciently learn domain-specific language models with\na small-size in-domain corpus by incorporating ex-\nternal domain knowledge. They train Word2Vec\nmodels using malware descriptions. Similarly,\nMumtaz et al. (2020) train a Word2Vec model\nusing security vulnerability-related bulletins and\nWikipedia pages.\nRecently, transformers-based models have\nbeen built for the cybersecurity domain: Cy-\nBERT (Priyanka Ranade and Finin, 2021),\nSecBERT (jackaduma, 2022) and Secure-\nBERT (Aghaei et al., 2023). CyBERT is trained\nwith a relatively small corpus consisting of 500\nsecurity blogs, 16,000 CVE records, and the\nAPTnotes collection. Further, CyBERT applies the\ncontinual pretraining and uses the BERT model’s\nvocabulary after adding 1,000 most frequent words\nin their corpus which do not exist in the base\nvocabulary. SecBERT provides both BERT and\nRoBERTa models trained on a security corpus\nconsisting of APTnotes, the SemEval2018 Task8\ndataset and Stucco-Data13 which contains security\nblogs and reports. However, the details about\nthe data and any experimental results are not\navailable. SecureBERT trains a RoBERTa model\nusing security reports, white papers, academic\n13https://stucco.github.io/data/\n118\nbooks, etc., which are similar to our dataset\nboth in terms of the size and document type.\nHowever, the model is built using the continual\npretraining method while CTI-BERT is trained from\nscratch. We believe that the main difference comes\nfrom CTI-BERT being trained from scratch and\nhaving the vocabulary specialized to the domain,\ncompared to the extended vocabulary used in\nCyBERT and SecureBERT. Table 14 compares\ndifferent training strategies used for these models.\n6 Conclusion\nWe presented a new pretrained BERT model tai-\nlored for the cybersecurity domain. Specifically,\nwe designed the model to improve the accuracy of\ncyber-threat intelligence extraction and understand-\ning, such as security entity (IoCs) extraction and\nattack technique (TTPs) classification. As demon-\nstrated by the experiments in Section 4, our model\noutperforms existing general domain and other cy-\nbersecurity domain models with the same base ar-\nchitecture. For future work, we plan to collect more\ndocuments to improve the model and also to train\nother language models to support different security\napplications.\nLimitations\nThe model is pretrained using only English data.\nWhile the majority of cybersecurity-related in-\nformation is distributed in English, we consider\nadding support for multiple languages in the fu-\nture work. Further, while we demonstrate that\nCTI-BERT outperforms other security-specific LMs\nfor a variety of tasks, the benchmark datasets are\nrelatively small. Thus, the findings may not be con-\nclusive, and further evaluations with more data are\nneeded.\nEthical Considerations\nTo our knowledge, this research has a very low risk\nfor ethical perspectives. All datasets were collected\nfrom reputable sources, which are publicly avail-\nable. The only person information in our corpus\nis the authors’ names and their affiliations in the\nUSENIX Security proceedings. However, we do\nnot expose their identities nor use the information\nin this work.\nReferences\nEhsan Aghaei, Xi Niu, Waseem Shadid, and Ehab Al-\nShaer. 2023. SecureBERT: A Domain-Specific Lan-\nguage Model for Cybersecurity, pages 39–56.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT:\nA pretrained language model for scientific text. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAssociation for Computational Linguistics.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 2898–\n2904. Association for Computational Linguistics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jian-\nfeng Gao, and Hoifung Poon. 2022. Domain-specific\nlanguage model pretraining for biomedical natural\nlanguage processing. ACM Trans. Comput. Heal. ,\n3(1):2:1–2:23.\nYibo Hu, MohammadSaleh Hosseini, Erick Skorupa\nParolin, Javier Osorio, Latifur Khan, Patrick Brandt,\nand Vito D’Orazio. 2022. Conflibert: A pre-trained\nlanguage model for political conflict and violence. In\nThe Conference of the North American Chapter of the\nAssociation for Computational Linguistics NAACL.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and pre-\ndicting hospital readmission. CoRR.\njackaduma. 2022. SecBERT.\nhttps://github.com/jackaduma/SecBERT.\nXin Jin, Sunil Manandhar, Kaushal Kafle, Zhiqiang\nLin, and Adwait Nadkarni. 2022. Understanding iot\nsecurity from a market-scale perspective. In Pro-\nceedings of the 2022 ACM SIGSAC Conference on\nComputer and Communications Security, CCS, pages\n1615–1629. ACM.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nSwee Kiat Lim, Aldrian Obaja Muis, Wei Lu, and\nOng Chen Hui. 2017. MalwareTextDB: A database\nfor annotated malware articles. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL, pages 1557–1567.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\n119\nSara Mumtaz, Carlos Rodriguez, Boualem Benatallah,\nMortada Al-Banna, and Shayan Zamanirad. 2020.\nLearning word representation for the cyber security\nvulnerability domain. In International Joint Confer-\nence on Neural Networks (IJCNN), pages 1–8.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of BERT and elmo on ten bench-\nmarking datasets. In Proceedings of the 18th BioNLP\nWorkshop and Shared Task, BioNLP@ACL, pages 58–\n65.\nPeter Phandi, Amila Silva, and Wei Lu. 2018. Semeval-\n2018 task 8: Semantic extraction from cybersecurity\nreports using natural language processing (securenlp).\nIn Proceedings of The 12th International Workshop\non Semantic Evaluation, SemEval@NAACL-HLT\n2018, pages 697–706.\nAnupam Joshi Priyanka Ranade, Aritran Piplai and Tim\nFinin. 2021. CyBERT: Contextualized Embeddings\nfor the Cybersecurity Domain. In IEEE International\nConference on Big Data.\nArpita Roy, Youngja Park, and Shimei Pan. 2017. Learn-\ning domain-specific word embeddings from sparse\ncybersecurity texts. CoRR.\nArpita Roy, Youngja Park, and Shimei Pan. 2019. In-\ncorporating domain knowledge in learning word em-\nbedding. In 31st IEEE International Conference on\nTools with Artificial Intelligence, ICTAI, pages 1568–\n1573. IEEE.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019. Hug-\ngingface’s transformers: State-of-the-art natural lan-\nguage processing.\n120\nA Details on Model Training\nTerm CTI-BERT bert-base-uncased\napt* apt, apt1, apt10, apt28, apt29, apt41, apts apt\nbackdoor* backdoor, backdoored, backdoors –\n*bot abbot, agobot, bot, gaobot, ircbot, ourbot, qakbot, qbot, rbot, robot, sabot,\nsdbot, spybot, syzbot, trickbot, zbot\nabbot, bot, robot, talbot\n*crime* crime, crimes, crimeware, cybercrime crime, crimea, crimean,\ncrimes\ncrypto* crypto, cryptoc, cryptocurr, cryptocurrencies, cryptocurrency, cryptograph,\ncryptographers, cryptographic, cryptographically, cryptography, cryptojack-\ning, cryptol, cryptolocker, cryptology, cryptom, cryptomining, cryptosystem,\ncryptosystems, cryptow, cryptowall\n–\ncyber* cyber, cyberark, cyberattack, cyberattackers, cyberattacks, cyberb, cybercri,\ncybercrime, cybercrimes, cybercriminal, cybercriminals, cyberdefense, cybere,\ncybereason, cyberespionage, cybers, cybersec, cybersecurity, cyberspace,\ncyberthre, cyberthreat, cyberthreats, cyberwar, cyberwarfare, cyberweap\ncyber\ndark* dark, darknet, darkreading, darks, darkside dark, darkened, dark-\nening, darker, darkest,\ndarkly, darkness\nhijack* [hijack, hijacked, hijacker, hijackers, hijacking, hijacks –\nkey* key, keybase, keyboard, keyboards, keychain, keyctl, keyed, keygen, key-\ning, keylog, keylogger, keyloggers, keylogging, keynote, keypad, keyring,\nkeyrings, keys, keyspan, keyst, keystone, keystore, keystream, keystro,\nkeystroke, keystrokes, keytouch, keyword, keywords\nkey, keyboard, key-\nboardist, keyboards,\nkeynes, keynote, keys,\nkeystone\n*kit applewebkit, bootkit, kit, rootkit, toolkit, webkit bukit, kit\nmalware* malware, malwarebytes, malwares –\n*net botnet, cabinet, cnet, darknet, dotnet, ethernet, fortinet, genet, honeynet, inet,\ninternet, intranet, kennet, kinet, kuznet, magnet, monet, net, phonet, planet,\nstuxnet, subnet, technet, telnet, vnet, x9cinternet, zdnet\nbarnet, baronet, bon-\nnet, cabinet, clarinet,\nethernet, hornet, inter-\nnet, janet, magnet, net,\nplanet\ntrojan* trojan, trojanized, trojans trojan\n*virus* antivirus, coronavirus, virus, viruses, virusscan, virustotal virus, viruses\nweb* web, webapp, webapps, webassembly, webc, webcam, webcams, webcast, we-\nbcasts, webclient, webcore, webd, webdav, webex, webgl, webhook, webin,\nwebinar, webkit, webkitbuild, webkitgtk, weblog, weblogic, webm, web-\nmail, webmaster, webpage, webpages, webresources, webroot, webrtc, webs,\nwebsense, webserver, webshell, website, websites, websocket, webspace,\nwebsphere, webtools, webview\nweb, webb, webber, we-\nber, website, websites,\nwebster\n*ware adware, antimalware, aware, beware, coveware, crimeware, delaware, de-\nsignware, firmware, foxitsoftware, freeware, hardware, malware, middleware,\nradware, ransomware, scareware, shareware, slackware, software, spyware,\nunaware, vmware, ware, x9cmalware\naware, delaware, hard-\nware, software, unaware,\nware\nTable 13: Comparison of V ocabulary. For a fair compar-\nison, we generated our tokenizer with 30,000 tokens.\nModel Base Training mode V ocab. Seq. Batch Train Steps\nCTI-BERT\nBERT-base\nscratch 50,000 256 2,048 200,000\nCyBERT continual 29,996 (base+1,000 security) 128 – 1 epoch\nSecBERT scratch 52,000 – – –\nSecRoBERTa RoBERTa-base scratch 52,000 – – –\nSecureBERT continual 50,265 (base+17,673 security) 512 144 250,000\nTable 14: Comparison of Model Training.\n“– indicates the information is not available.\n121\nB Details on Experiment Datasets\nTrain Dev. Test Total\n# Sentences 754 355 382 1,491\n# Tokens 138,721 19,578 7,985 166,284\nTable 15: Summary of TRAM Data\nTrain Dev Test Total\n# Documents 5,214 1,058 965 7,237\n# Tokens 635,220 133,546 106,084 874,850\nTable 16: Summary of IoTSpotter Data\nTrain Dev. Test Total\n# Sentences 9,424 1,213 618 11,255\n# Tokens 1,020,655 146,362 56,216 1,223,233\nTable 17: Summary of the Malware Sentence Data\nTrain Dev Test Total\n# Documents 667 167 133 967\n# Sentences 38,721 6,322 9,837 54,880\n# Tokens 465,826 92,788 119,613 678,227\nTable 18: Summary of the NER1 Dataset\nEntity Type Train Dev Test\nCampaign 247 27 85\nCourseOfAction 1,938 779 329\nExploitTarget 5,839 1,412 1,282\nIdentity 6,175 1,262 1,692\nIndicator 3,718 1,071 886\nMalware 4,252 776 1,027\nResource 438 91 114\nThreatActor 755 91 144\nTable 19: Entity Types and Distributions in the NER1\nDataset\nTrain Dev Test Total\n# Documents 106 14 13 133\n# Sentences 5,206 561 671 6,438\n# Tokens 75,969 8,106 9,984 94,059\nTable 20: NER2 Dataset\nEntity Type Train Dev Test Total\nCampaign 39 0 4 43\nSecurityAdvisory 54 12 30 96\nVulnerability 401 50 86 537\nDomainName 169 3 16 188\nEmailAddress 6 1 1 8\nEndpoint 3 0 0 3\nFileName 210 37 24 271\nHash 93 5 3 101\nIpAddress 37 0 2 39\nNetwork 3 0 0 3\nURL 181 20 27 228\nWindowsRegistry 9 0 0 9\nAvSignature 99 13 10 122\nMalwareFamily 554 53 47 654\nTechnique 334 39 76 449\nThreatActor 89 4 7 100\nTable 21: Entity Types and Distributions in the NER2\nDataset\n#Doc. #Sent. #Action #Entity #Mod.\nTrain 65 9,424 3,202 6,875 2,011\nDev 5 1,213 122 254 79\nTest 5 618 125 249 79\nTotal 75 11,255 3,449 7,378 2,169\nTable 22: Dataset for Token Type Classification\nSplit ActionName Capability\n#Doc. #Sent. #Class #Doc. #Sent #Class\nTrain 65 1,154 99 65 2,817 20\nDev. 5 46 20 5 102 13\nTest 5 33 18 5 88 14\nSplit StrategicObjectives TacticalObjectives\n#Doc. #Sent. #Class #Doc. #Sent. #Class\nTrain 65 2,206 53 65 1,783 93\nDev. 5 77 28 5 63 26\nTest 5 70 21 5 63 27\nTable 23: Data Statistics for Malware Attribute Classifi-\ncation\n122",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8191940784454346
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.7330388426780701
    },
    {
      "name": "Cyber threats",
      "score": 0.6367756128311157
    },
    {
      "name": "Computer security",
      "score": 0.5347821712493896
    },
    {
      "name": "Language model",
      "score": 0.5175320506095886
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5124345421791077
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.47671669721603394
    },
    {
      "name": "Training set",
      "score": 0.4599972665309906
    },
    {
      "name": "Machine learning",
      "score": 0.36818164587020874
    },
    {
      "name": "Natural language processing",
      "score": 0.34119439125061035
    },
    {
      "name": "Data science",
      "score": 0.3401390016078949
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210114115",
      "name": "IBM Research - Thomas J. Watson Research Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I922845939",
      "name": "Philadelphia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I79576946",
      "name": "University of Pennsylvania",
      "country": "US"
    }
  ],
  "cited_by": 8
}