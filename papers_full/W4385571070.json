{
    "title": "Team:PULSAR at ProbSum 2023:PULSAR: Pre-training with Extracted Healthcare Terms for Summarising Patients’ Problems and Data Augmentation with Black-box Large Language Models",
    "url": "https://openalex.org/W4385571070",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2097763185",
            "name": "Hao Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2112400869",
            "name": "Yuping Wu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2916497633",
            "name": "Viktor Schlegel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224909576",
            "name": "Riza Batista-Navarro",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2140367265",
            "name": "Thanh Tung NGUYEN",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2803287635",
            "name": "Abhinav Ramesh Kashyap",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2123579633",
            "name": "Xiao-Jun Zeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098044952",
            "name": "Daniel Beck",
            "affiliations": [
                "University of Melbourne"
            ]
        },
        {
            "id": "https://openalex.org/A1948727508",
            "name": "Stefan Winkler",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A241805189",
            "name": "Goran Nenadic",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4385570712",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W3034356621",
        "https://openalex.org/W2953486306",
        "https://openalex.org/W3170218295",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2286763525",
        "https://openalex.org/W4206636317",
        "https://openalex.org/W3134354193",
        "https://openalex.org/W3035252911",
        "https://openalex.org/W3135413417",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W3166702501",
        "https://openalex.org/W3127800866",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4297895085",
        "https://openalex.org/W3168558807",
        "https://openalex.org/W2996264288",
        "https://openalex.org/W2168041406",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3201710275",
        "https://openalex.org/W3170450419",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W4226470220"
    ],
    "abstract": "Hao Li, Yuping Wu, Viktor Schlegel, Riza Batista-Navarro, Thanh-Tung Nguyen, Abhinav Ramesh Kashyap, Xiao-Jun Zeng, Daniel Beck, Stefan Winkler, Goran Nenadic. The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023.",
    "full_text": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 503–509\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nPULSAR: Pre-training with Extracted Healthcare Terms for Summarising\nPatients’ Problems and Data Augmentation with Black-box Large\nLanguage Models\nHao Li♣∗†, Yuping Wu♣∗, Viktor Schlegel♢♣, Riza Batista-Navarro♣\nThanh-Tung Nguyen♢Abhinav Ramesh Kashyap♢♡, Xiaojun Zeng♣\nDaniel Beck♠, Stefan Winkler♢♡ and Goran Nenadic♣\n♣University of Manchester, United Kingdom ♠University of Melbourne, Australia\n♢ASUS Intelligent Cloud Services (AICS), Singapore\n♡National University of Singapore, Singapore\nAbstract\nMedical progress notes play a crucial role in\ndocumenting a patient’s hospital journey, in-\ncluding his or her condition, treatment plan,\nand any updates for healthcare providers. Au-\ntomatic summarisation of a patient’s problems\nin the form of a “problem list” can aid stake-\nholders in understanding a patient’s condition,\nreducing workload and cognitive bias. BioNLP\n2023 Shared Task 1A focuses on generating\na list of diagnoses and problems from the\nprovider’s progress notes during hospitalisa-\ntion. In this paper, we introduce our proposed\napproach to this task, which integrates two com-\nplementary components ‡. One component em-\nploys large language models (LLMs) for data\naugmentation; the other is an abstractive sum-\nmarisation LLM with a novel pre-training ob-\njective for generating the patients’ problems\nsummarised as a list. Our approach was ranked\nsecond among all submissions to the shared\ntask. The performance of our model on the\ndevelopment and test datasets shows that our\napproach is more robust on unknown data, with\nan improvement of up to 3.1 points over the\nsame size of the larger model.\n1 Introduction\nMedical progress notes are used to document a pa-\ntient’s course in a hospital, including their current\ncondition, treatment plan, and any updates to the\nplan (Li et al., 2022). Automated identification\nof treated problems from the assessment sections\nof a progress note in form of a “problem list” can\nhelp healthcare stakeholders to gain an accurate\nunderstanding of the patient’s condition, reducing\nworkload and cognitive bias (Gao et al., 2022a).\nThis problem list is then used to outline and pursue\na detailed treatment plan.\n*These authors contributed equally to this work\n†Corresponding email: hao.li-2@manchester.ac.uk\n‡Our code is avaliable at https://github.com/yuping-\nwu/PULSAR\nThe majority of studies on clinical summarisa-\ntion have focused on clinical notes; radiology re-\nports (Zhang et al., 2018; MacAvaney et al., 2019;\nGharebagh et al., 2020; Kondadadi et al., 2021; Dai\net al., 2021), and progress notes (Moen et al., 2016;\nLiang et al., 2019; Adams et al., 2021; Gao et al.,\n2022a). In contrast, some studies have focused on\ndialogues (Yim and Yetisgen-Yildiz, 2021; Manas\net al., 2021; Zhang et al., 2021). Recently, Gao et al.\n(2022b) proposed the task of “progress note under-\nstanding”, where the goal is to generate problem\nlists given the assessment sections of a progress\nnote. They further explored the performance of\nT5 (Raffel et al., 2020), BART (Kondadadi et al.,\n2021) based on pre-training tasks with masked\nhealthcare concepts (Gao et al., 2022a). To draw\nfurther attention to this task, The BioNLP 2023\nShared Task 1A (Gao et al., 2023) invited external\nparticipants to develop approaches to advance the\nstate-of-the-art on the proposed task.\nThe main contribution of this work is a novel\nframework for data augmentation and summarisa-\ntion of diagnoses/problems. In our approach, first,\nwe optimise a domain-specific Language Model\n(LM) using a combination of different pre-training\nobjectives, depicted in Figure 1; this model signifi-\ncantly outperforms the state-of-the-art, even when\noptimised on a limited number of manually anno-\ntated progress notes. Second, we instruct Large\nLanguage Models (LLMs) to generate synthetic\ndata, in order to reduce the reliance on large, high-\nquality annotated datasets. Finally, we use the gen-\nerated data to fine-tune the domain-specific LM\non the task of problem list generation, given ap-\npropriate progress note sections. Our approach\nranked second among all submissions to the shared\ntask without additional annotated data. The results\nof our evaluation suggest that our pre-training ob-\njectives are aligned with the downstream task of\nsummarisation and can significantly improve per-\nformance.\n503\nPULSAR\nPre-training / Data  Augmentation\nInfant remains on prong CPAP of 5. Occaisional brief O2 sat drifts noted. Breath\nsounds are clear and equal. Remains on caffeine, no spells thus far tonight.  Infant\nremains in off isolette with stable temp. He is alert and active with cares.\nInput Text\n67 y/o M CAD s [Name] stent with the recent cecum polypectomy who presents\nwith bright red blood per rectum and hypotension following.\nDA Instances\nTarget Text\nPulmonary\nembolism; LLE\nDVT; # HTN\nBlack-Box LLM\nPT Instances\n54 yo M with HTN, obesity p/w saddle emboli from OSH s/p heparin bolus and TPATest Instance\nInfant remains on prong [MASK 1,2] of 5. Occaisional brief O2 [MASK 1] noted.\nBreath sounds are clear and equal. Remains on [MASK 2], no spells thus far\ntonight.  Infant remains in off isolette with stable temp. [Sentence].\nSentence 1:\n67 y/o M\nCAD s ...\nInstructions\nSentence 1:\n67 y/o M\nCAD s ...\nSentence 1:\n67 y/o M\nCAD s ...\nWrite two sentences\nthat mean the same\nthing.\nWrite two\nsentences that are\nsomewhat similar.\nWrite two sentences\nthat are on comple-\ntely different topics.\nLabel:1\nLabel:0.5\nLabel:0\nExampleiOutputi\nA patient with a history\nof coronary artery\ndisease and recent\ncecum polypectomy\n...\nA patient experiencing\nlower GI bleed may\nalso experience\nhypotension ...\nThe patient complained\nof dizziness , indica-\nting a possible hypote-\nnsive episode ...\n1:\n0.5:\n0 ： \nLabel\nUMLSN2C2\n70%\n30%\nGSGMLM\nCPAP\nsat driftsCPAP He is alert and active with cares.\ncaffeine\nUMLS\nN2C2\nFlan-T5-3B/11B\nPolicy: Inputs\nOutputs\nFigure 1: Overview of PULSAR. The left component represents the pre-training process with three different mask\npolicies depicted in different colours. Both Gap Sentences Generation (GSG) and Masked Language Modelling\n(MLM) are applied simultaneously to this example as pre-training objectives. The right component shows the\nworkflow for data augmentation where the three labels {1, 0.5, 0}represent SAME THING , SOMEWHAT SIMILAR\nand COMPLETELY DIFFERENT TOPICS , respectively. PT I NSTANCES and DA INSTANCES stand for PRE-TRAINING\nINSTANCES and DATA AUGMENTATION INSTANCES , respectively.\n2 Methodology\nFigure 1 shows the two components of our frame-\nwork: first we pre-train an encoder-decoder model\non MIMIC-III progress notes (Johnson et al., 2016)\nusing three different concept masking pre-training\nobjectives. Then we employ data augmentation\nwhen fine-tuning our model for the summarisation\ntask.\n2.1 Pre-training Model\nThe items on the problem list are not necessar-\nily directly extracted from the original progress\nnotes and hence we cast the problem as abstractive\nsummarisation. Drawing inspiration from PEGASUS\n(Zhang et al., 2020a), we used an objective which\nclosely resembles the abstractive summarisation\nobjective, to gain better and faster fine-tuning per-\nformance.\nFollowing the success obtained through masking\nwords and contiguous spans (Joshi et al., 2020; Raf-\nfel et al., 2020), we propose to select and mask text\nInfant remains on prong CPAP of 5. Occaisional brief O2 sat drifts noted.\nOriginal text\nInputs\nTargets\nInfant remains on prong <extra_id_0> of 5. Occaisional brief O2 <extra_id_1> noted.\n<extra_id_0> CPAP <extra_id_1> sat drifts <extra_id_2>\nFigure 2: Our pre-training objective. The terms \"CPAP\"\nand \"sat drifts\" are identified by the NER models and\nreplaced by a unique sentinel token respectively. The\nobjective is to predict these masked-out spans.\nspans or whole sentences from input documents.\nWe concatenate these “gap text spans (sentences)”\ninto a pseudo-summary. Gap text spans were se-\nlected by the QuickUMLS entity linking (Soldaini\nand Goharian, 2016) and an NER model trained\non the i2b2-2010 challenge (Uzuner et al., 2011).\nSimilar to the T5 pre-training procedure (Raffel\net al., 2020), these text spans were replaced by\n“sentinel” mask tokens < extraid i >to inform\nthe model that input was masked. Here, i indicates\nthe number of the mask (from left to right). The\noutput sequence thus consists of the dropped-out\ntext spans, delimited by the sentinel token between\nterms and the last < extraid i >input represent-\ning the end of the output. Figure 2 illustrates our\npre-training objective.\nSpecifically, we considered three masking poli-\ncies in our pre-training objective. For each sen-\ntence, When both tools identified entities, we se-\nlected UMLS terms with the probability of 0.7 and\ni2b2 terms with the probability of 0.3. When only\none tool identified entities, these entities were se-\nlected. Finally, when no entities were identified,\nthe entire sentence was masked with a probabil-\nity of 0.15. In order to provide the model with\nthe necessary medical knowledge and reduce do-\nmain barriers (Pandey et al., 2022), we leverage\nall progress notes from MIMIC-III (Johnson et al.,\n2016) to train Flan-T5 (Chung et al., 2022) on\nthis objective. The processed pre-training corpus\nhad 2.08m rows of data, with 2.2k containing no\nUMLS terms, 23k containing no i2b2 entities, and\n797 where neither tool recognised any entities.\n504\nDev set Test set\nApproach (Setting) R-1/R-2/R-L R-F1/R-P/R-R\nPULSAR3B(DA) 36.27/16.78/33.83 30.48/38.02/29.72\nPULSAR11B(DA) 35.92/15.87/33.1431.15/40.93/28.73\nPULSAR3B 33.60/13.70/31.3231.14/44.30/27.18\nPULSAR11B 33.38/13.14/30.6330.34/42.68/27.12\nFlanT511B(DA) 32.57/13.07/29.95 -\nFlanT511B 31.24/11.42/28.25 30.06/40.61/27.25\nFlanT53B(DA) 29.46/09.85/26.1530.47/38.01/29.72\nClinicalT5LARGE(DA) 28.60/11.13/26.11 25.43/25.67/32.05\nFlanT53B 28.90/08.93/25.2630.60/41.09/28.58\nPULSAR3B(−A) 27.70/10.60/24.34 28.29/38.24/26.54\nClinicalT5LARGE 31.09/12.85/28.15 19.92/18.93/28.89\nFlanT5LARGE 29.86/10.19/27.08 -\nTable 1: Performance of evaluated models on the de-\nvelopment set measured in terms of Rouge-1/2/LCS,\nand on the test set measured in terms of Rouge-\nF1/Precision/Recall, respectively. The composition of\nthe input content is ASSESSMENT + S UBJECT + O B-\nJECT , except where only the ASSESSMENT section of\nthe input was used, indicated by -A. DA means that\ndata augmentation was employed. The Rouge-L score\non the development set was used for official ranking.\nColours (i.e. 1st , 2nd , 3rd , 4th , 5th , 6th ) indicate\nthe highest to lowest performance.\n2.2 Data Augmentation (DA)\nThe lack of high-quality annotated data is a bottle-\nneck that inhibits supervised learning methods in\nthe healthcare field. For example, BioNLP Task 1A\n(Gao et al., 2023) has only 764 annotated training\nexamples. Therefore, we rely on data augmentation\ntechniques to obtain more training samples. Specif-\nically, we propose a novel healthcare data gener-\nation (DG) framework based on DINO (Schick\nand Schütze, 2021; Li et al., 2023), which exploits\nthe generative abilities of LLMs by relying on in-\nstruction following rather than model training. Our\ninstructions to the LLMs include task-specific de-\nscriptions (i.e., “Write two sentences that mean the\nsame thing but keep these two healthcare terms\n[Term 1], [Term 2]. Sentence 1: [Source] Sen-\ntence 2: ”) to make the model generate a paraphrase\nof [Source], which is selected from the annotated\ntraining data. The instructions to keep terms aim\nto keep relevant terms from [Source] which also\nappear in the problem list (i.e. the output). In ad-\ndition, we might expect that the text generated by\nthe LLM would only fit well into the correspond-\ning instruction but would not be applicable as a\nreasonable output for other instructions. For ex-\nample, in Figure 1 (i.e. label {1}is the expected\nlabel in blue and label{0}is the count label in red),\nit is expected that the generated text should have\nApproach(MaxLen) R-1/R-2/R-L\nBaselines\nT5LARGE(512) 29.901/10.81/28.21\nFlanT5BASE(512) 27.16/8.9435/24.90\nClinicalT5SCRATCH(512) 26.68/9.51/23.94\nT5BASE(512) 25.07/7.72/23.36\nFlanT5BASE(1024) 25.51/7.96/23.07\nClinicalT5BASE(512) 22.27/7.61/20.49\nPEGASUSXSUM(512) 22.39/6.86/20.36\nClinicalT5BASE(1024) 21.13/7.19/19.55\nClinicalT5SCI(512) 14.12/4.61/13.22\nTable 2: Performance of baseline models on the de-\nvelopment measured in terms of Rouge-1/2/LCS. The\ncomposition of the input content is ASSESSMENT +\nSUBJECT + OBJECT . The same colour represents the\nsame model with different input lengths.\nthe same meaning as [Source], but at the same\ntime not have a completely different meaning from\n[Source]. Following previous work, we employ\nthe self-debiasing (Schick et al., 2021) algorithm to\nachieve this objective, i.e. when predicting the next\ntoken, not only the probability of the correspond-\ning label is considered, but also the counter label\nis taken into account. We then use BERTScore\n(Zhang et al., 2020b) and BLEURT (Sellam et al.,\n2020) to assess the similarity between each gener-\nated sample and the source, removing 85% of the\nlowest scoring generated sentences. The backbone\nof the framework can be any generative LLM, such\nas GPT3.5§, GPT3 (Brown et al., 2020) and GPT2\n(Radford et al., 2019) series models. Limited by\nthe data use agreement, we used BioMedLM (Bolton\net al., 2022), an open-source GPT-style model pre-\ntrained on the biomedical abstracts and papers,\n¶.\n2.3 Implementation Details\nPre-training: We choose FlanT5-3B and\nFlanT5-11B (Chung et al., 2022) as our LM.\nPULSAR-3B and PULSAR-11B are pre-trained on\ntwo NVIDIA Tesla A100 80GB GPUs and four\nNVIDIA Tesla A100 80GB GPUs for 1 epoch re-\nspectively¶. During the pre-training, we rely on\nFully Sharded Data Parallel (FSDP) with CPU of-\nfloading (Baines et al., 2021) to fit LLMs into GPU\nmemory.\nData Augmentation : We employ BioMedLM\n§chat.openai.com\n¶The official test set result for PULSAR-11B was fine-\ntuned after the 0.33 pre-training epoch.\n505\n(Radford et al., 2019) as the data augmentation\nmodel with default settings, setting maximum out-\nput length to 40. Finally, the generated data are\nmatched with the corresponding summaries, sub-\njective and objective to create a training set of 1k\ninstances. The DA model (Schick and Schütze,\n2021) is run on a single NVIDIA Tesla V100 32G\nGPU, with each run taking up to twelve hours. Ex-\nample templates and the full dataset description\ncan be seen in Appendix A.\n3 Experimental Setup\nBaselines: We have chosen to adapt T5-base\nas one of our baselines, similar to the approach\ntaken by Gao et al. (2022a). Additionally, we\nhave incorporated various state-of-the-art models\nsuch as FlanT5 (Chung et al., 2022), ClinicalT5\n(Lehman and Johnson, 2023) and PEGASUS (Zhang\net al., 2020a). Whereas FlanT5 is an enhanced\nversion of T5 that has been finetuned in a mix-\nture of tasks (Chung et al., 2022) and ClinicalT5\npre-trained on MIMIC-III (Johnson et al., 2016).\nPEGASUS is an abstractive summarisation model\nwith Gap Sentences Generation and Masked Lan-\nguage Model (Devlin et al., 2019) as pre-train tasks.\nEvaluation metrics: We calculate ROUGE (Lin,\n2004) scores on the test set, by comparing the gen-\nerated summaries to their corresponding references,\naveraging for all generation samples. For all ex-\nperiments, the data set was divided into a “train”\nand a “dev” set with a ratio of 8:2 for training and\nevaluation, respectively. The results are presented\nin Table 1, left column, and Table 2. Table 1, right\ncolumn, shows the performance of the models on\nthe official withheld test set. In this case, both train\nand dev splits were used for training.\n4 Results and Analysis\nPre-training helps : Both Table 1 and Table 2\ndemonstrate that the pre-training objective im-\nproves task performance (compare 3B and 11B\nPULSAR to corresponding FlanT5 models). The\nbest performance of PULSAR was 3.1 points\nhigher than the FlanT5-11B on the development\nset as the training set and 11.2 points higher\nthan ClinicalT5-large on the official test set.\nThe small difference in performance between\nPULSAR-11B and PULSAR-3B is primarily because\nthe former has only completed 1/3 of the first pre-\ntraining epoch, potentially resulting in a lack of\nrelevant medical knowledge and familiarity with\ndownstream task patterns.\nData augmentation is effective when the data\ndistribution is consistent; It is significantly more\nhelpful for small models when on a random data\ndistribution: Table 1 shows that, data augmenta-\ntion improves performance (3 point on average,\ncompared to not using DA). This shows that the\nproposed DA approach can effectively alleviate the\nlack of annotated healthcare data, when the distri-\nbution of training and test set data is consistent.\nFrom Table 1, it becomes evident that smaller mod-\nels (ClinicalT5-large) can improve by up to 6\npoints with the help of data augmentation, but the\neffect diminishes with model size as it increases\nmax to 2.5 on LLMs. The potential reason is that\nthe test set for the sharing task differs significantly\nfrom the training set, in the vary of length of the\nsummary.\nThe model is capable of discriminating irrele-\nvant information, but longer input lengths may\nresult in decreased performance: We conducted\nablation experiments on PULSAR-3B to verify the\nimpact of the input text type. In contrast to Gao\net al. (2022b)’s findings on the small model, the\nresults (PULSAR-3B vs. PULSAR-3B-A) in Table 1\nshow that if the input is ASSESSMENT + SUBJEC -\nTIVE + OBJECTIVE , the model performs better (by\n2.9 points on the official test set and by 7 points on\nthe development set) compared with only usingAS-\nSESSMENT as input. This indicates that while most\nof the relevant information can be inferred from the\nASSESSMENT section alone, additional input can\nbe beneficial. However, increasing the input length\nappears to not be useful: Table 2 shows that mod-\nels trained with longer input lengths (1024 tokens)\ndo not improve over models that were trained on\n512-token-long input.\n5 Conclusion\nThis paper contributed to the development of sum-\nmarising patients’ problems. Firstly, we proposed\na novel task-specific pre-training LLM objective.\nCompared with other submissions, we rank 2nd\nat the official shared task without using additional\nmanually annotated training samples. Secondly, we\npropose a new data augmentation framework and\ndemonstrate its effectiveness in the healthcare do-\nmain. In the future, we will explore the applicabil-\nity of our approach to other domain-specific gener-\native tasks and conduct a deeper analysis of factors\nthat contribute to overall model performance.\n506\nLimitations\nThe proposed model is computationally demand-\ning. Recent work on parameter-efficient fine-tuning\nmethods, such as LoRA (Hu et al., 2022), sug-\ngests that they can significantly reduce the number\nof trainable parameters at a minimal performance\ncost, which may help further democratise the de-\nvelopment of domain- and task-specific models. In\naddition, as we continued to pretrain, to obtain the\nPULSAR models, their tokenizer was inherited from\ncorresponding Flan-T5 model. Thus it does not\ncontain domain-specific terminology, which may\nbe a limitation in terms of representation density\n(i.e. frequent clinical terms may be split in multiple\nrare sub-tokens).\nEthics Statement\nFor the present work, we used an existing\nanonymised dataset from BioNLP 2023 Shared\nTask 1A without any data protection issues. In addi-\ntion, data augmentation only uses an open-source,\noff-line model which is not offensive to the data\nuser agreement that is shared with a third party.\nAcknowledgements\nWe thank the anonymous reviewers from the\nBioNLP 2023 Shared Task for their valuable feed-\nback. We would also like to acknowledge the use\nof the Computational Shared Facility at The Uni-\nversity of Manchester.\nReferences\nGriffin Adams, Emily Alsentzer, Mert Ketenci, Jason\nZucker, and Noémie Elhadad. 2021. What’s in a\nsummary? laying the groundwork for advances in\nhospital-course summarization. In NAACL-HLT,\npages 4794–4811. Association for Computational\nLinguistics.\nMandeep Baines, Shruti Bhosale, Vittorio Caggiano,\nNaman Goyal, Siddharth Goyal, Myle Ott, Benjamin\nLefaudeux, Vitaliy Liptchinsky, Mike Rabbat, Sam\nSheiffer, et al. 2021. Fairscale: A general purpose\nmodular pytorch library for high performance and\nlarge scale training.\nElliot Bolton, David Hall, Michihiro Yasunaga, Tony\nLee, Chris Manning, and Percy Liang. 2022. Pub-\nmedgpt 2.7b.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\nSongtai Dai, Quan Wang, Yajuan Lyu, and Yong Zhu.\n2021. BDKG at MEDIQA 2021: System report\nfor the radiology report summarization task. In\nBioNLP@NAACL-HLT, pages 103–111. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT (1), pages 4171–4186. As-\nsociation for Computational Linguistics.\nYanjun Gao, Dmitriy Dligach, Timothy Miller, Dong-\nfang Xu, Matthew M. Churpek, and Majid Afshar.\n2022a. Summarizing patients’ problems from hos-\npital progress notes using pre-trained sequence-to-\nsequence models. In COLING, pages 2979–2991.\nInternational Committee on Computational Linguis-\ntics.\nYanjun Gao, Dmitriy Dligach, Timothy A. Miller,\nSamuel Tesch, Ryan Laffin, Matthew M. Churpek,\nand Majid Afshar. 2022b. Hierarchical annotation for\nbuilding A suite of clinical natural language process-\ning tasks: Progress note understanding. In LREC,\npages 5484–5493. European Language Resources\nAssociation.\nYanjun Gao, Timothy Miller, Majid Afshar, and Dmitriy\nDligach. 2023. Bionlp workshop 2023 shared task\n1a: Problem list summarization. \"Proceedings of the\n22nd Workshop on Biomedical Language Process-\ning\".\nSajad Sotudeh Gharebagh, Nazli Goharian, and Ross W.\nFilice. 2020. Attend to medical ontologies: Content\nselection for clinical abstractive summarization. In\nACL, pages 1899–1905. Association for Computa-\ntional Linguistics.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In ICLR. OpenReview.net.\n507\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H\nLehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi,\nand Roger G Mark. 2016. Mimic-iii, a freely accessi-\nble critical care database. Scientific data, 3(1):1–9.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Trans. Assoc. Comput. Linguistics, 8:64–\n77.\nRavi Kondadadi, Sahil Manchanda, Jason Ngo, and Ro-\nnan McCormack. 2021. Optum at MEDIQA 2021:\nAbstractive summarization of radiology reports using\nsimple BART finetuning. In BioNLP@NAACL-HLT,\npages 280–284. Association for Computational Lin-\nguistics.\nEric Lehman and Alistair Johnson. 2023. Clinical-t5:\nLarge language models built using mimic clinical\ntext.\nHao Li, Viktor Schlegel, Riza Batista-Navarro, and\nGoran Nenadic. 2023. Do you hear the peo-\nple sing? key point analysis via iterative cluster-\ning and abstractive summarisation. arXiv preprint\narXiv:2305.16000.\nIrene Li, Jessica Pan, Jeremy Goldwasser, Neha\nVerma, Wai Pan Wong, Muhammed Yavuz Nuzum-\nlali, Benjamin Rosand, Yixin Li, Matthew Zhang,\nDavid Chang, Richard Andrew Taylor, Harlan M.\nKrumholz, and Dragomir Radev. 2022. Neural natu-\nral language processing for unstructured data in elec-\ntronic health records: A review. Comput. Sci. Rev.,\n46:100511.\nJennifer Liang, Ching-Huei Tsou, and Ananya Poddar.\n2019. A novel system for extractive clinical note\nsummarization using ehr data. In Proceedings of the\n2nd clinical natural language processing workshop,\npages 46–54.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nSean MacAvaney, Sajad Sotudeh, Arman Cohan, Nazli\nGoharian, Ish A. Talati, and Ross W. Filice. 2019.\nOntology-aware clinical abstractive summarization.\nIn SIGIR, pages 1013–1016. ACM.\nGaur Manas, Vamsi Aribandi, Ugur Kursuncu,\nAmanuel Alambo, Valerie L Shalin, Krishnaprasad\nThirunarayan, Jonathan Beich, Meera Narasimhan,\nAmit Sheth, et al. 2021. Knowledge-infused abstrac-\ntive summarization of clinical diagnostic interviews:\nFramework development study. JMIR Mental Health,\n8(5):e20865.\nHans Moen, Laura-Maria Peltonen, Juho Heimonen,\nAntti Airola, Tapio Pahikkala, Tapio Salakoski, and\nSanna Salanterä. 2016. Comparison of automatic\nsummarisation methods for clinical free text notes.\nArtif. Intell. Medicine, 67:25–37.\nBabita Pandey, Devendra Kumar Pandey, Brijen-\ndra Pratap Mishra, and Wasiur Rhmann. 2022. A\ncomprehensive survey of deep learning in the field\nof medical imaging and medical natural language\nprocessing: Challenges and research directions. J.\nKing Saud Univ. Comput. Inf. Sci., 34(8 Part A):5083–\n5099.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nTimo Schick and Hinrich Schütze. 2021. Generating\ndatasets with pretrained language models. In EMNLP\n(1), pages 6943–6951. Association for Computational\nLinguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for\nreducing corpus-based bias in NLP. Trans. Assoc.\nComput. Linguistics, 9:1408–1424.\nThibault Sellam, Dipanjan Das, and Ankur P. Parikh.\n2020. BLEURT: learning robust metrics for text\ngeneration. In ACL, pages 7881–7892. Association\nfor Computational Linguistics.\nLuca Soldaini and Nazli Goharian. 2016. Quickumls:\na fast, unsupervised approach for medical concept\nextraction. In MedIR workshop, sigir, pages 1–4.\nÖzlem Uzuner, Brett R. South, Shuying Shen, and\nScott L. DuVall. 2011. 2010 i2b2/va challenge on\nconcepts, assertions, and relations in clinical text. J.\nAm. Medical Informatics Assoc., 18(5):552–556.\nWen-wai Yim and Meliha Yetisgen-Yildiz. 2021. To-\nwards automating medical scribing: Clinic visit dia-\nlogue2note sentence alignment and snippet summa-\nrization. In Proceedings of the Second Workshop on\nNatural Language Processing for Medical Conversa-\ntions, pages 10–20.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2020a. PEGASUS: pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn ICML, volume 119 of Proceedings of Machine\nLearning Research, pages 11328–11339. PMLR.\nLongxiang Zhang, Renato Negrinho, Arindam Ghosh,\nVasudevan Jagannathan, Hamid Reza Hassanzadeh,\nThomas Schaaf, and Matthew R. Gormley. 2021.\nLeveraging pretrained models for automatic summa-\nrization of doctor-patient conversations. In EMNLP\n(Findings), pages 3693–3712. Association for Com-\nputational Linguistics.\n508\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020b. Bertscore: Eval-\nuating text generation with BERT. In ICLR. OpenRe-\nview.net.\nYuhao Zhang, Daisy Yi Ding, Tianpei Qian, Christo-\npher D. Manning, and Curtis P. Langlotz. 2018.\nLearning to summarize radiology findings. In\nLouhi@EMNLP, pages 204–213. Association for\nComputational Linguistics.\nA Example Appendix\nExample of data augmentation input and output\nSentence 1: \"67 y/o M CAD s [Name] stent with recent cecum\npolypectomy who presents with bright red blood per rectum and\nhypotension following.\"\nSentence 2: \"A patient with a history of coronary artery disease\nand recent cecum polypectomy is experiencing lower\ngastrointestinal bleeding and hypotension after undergoing\npreparation.\"\nSentence 1: \"67 y/o M CAD s [Name] stent with recent cecum\npolypectomy who presents with bright red blood per rectum and\nhypotension following.\"\nSentence 2: \"A patient experiencing lower GI bleed may also\nexperience hypotension due to the significant loss of blood from the\nlower gastrointestinal tract.\"\nSentence 1: \"67 y/o M CAD s [Name] stent with recent cecum\npolypectomy who presents with bright red blood per rectum and\nhypotension following.\"\nSentence 2: \"The patient complained of dizziness and\nlightheadedness, indicating a possible hypotensive episode. On a\ndifferent note, the recent research study showed a link between a\nhigh fiber diet and reduced lower blood pressure.\"\nInstruction:  \"Task: Write two sentences that are somewhat similar,\nbut keep these two healthcare terms ['blood', 'hypotension'].\"\nInstruction:  \"Task: Write two sentences that are on completely\ndifferent topics, but keep these two healthcare terms ['blood',\n'hypotension'].\"\nInstruction: \"Task: Write two sentences that mean the same thing,\nbut keep these two healthcare terms ['blood', 'hypotension'].\"\nFigure 3: Continuation text generated by prompted\nlearning data augmented methods with three different\ntemplate descriptions. We chose to give input sentence\n1 and generate only sentence 2, which helps to generate\nsentence similarity datasets.\n509"
}