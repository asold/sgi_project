{
  "title": "Predicting dementia from spontaneous speech using large language models",
  "url": "https://openalex.org/W4312091558",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3092209942",
      "name": "Felix Agbavor",
      "affiliations": [
        "Drexel University"
      ]
    },
    {
      "id": "https://openalex.org/A2118545256",
      "name": "Hualou Liang",
      "affiliations": [
        "Drexel University"
      ]
    },
    {
      "id": "https://openalex.org/A3092209942",
      "name": "Felix Agbavor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118545256",
      "name": "Hualou Liang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4211238543",
    "https://openalex.org/W3007853567",
    "https://openalex.org/W1847168837",
    "https://openalex.org/W2978201752",
    "https://openalex.org/W4211073013",
    "https://openalex.org/W2984124903",
    "https://openalex.org/W3101650868",
    "https://openalex.org/W3094326096",
    "https://openalex.org/W1853705225",
    "https://openalex.org/W2991435809",
    "https://openalex.org/W2968208899",
    "https://openalex.org/W2089109585",
    "https://openalex.org/W2576151354",
    "https://openalex.org/W2526050071",
    "https://openalex.org/W3154143698",
    "https://openalex.org/W4284667718",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W4298071820",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3200538644",
    "https://openalex.org/W3092857159",
    "https://openalex.org/W2074037951",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W6687152286",
    "https://openalex.org/W2239141610",
    "https://openalex.org/W3128293209",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2058161128",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2191779130",
    "https://openalex.org/W2148080316",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2167311298",
    "https://openalex.org/W2098386490",
    "https://openalex.org/W2008854521"
  ],
  "abstract": "Language impairment is an important biomarker of neurodegenerative disorders such as Alzheimer’s disease (AD). Artificial intelligence (AI), particularly natural language processing (NLP), has recently been increasingly used for early prediction of AD through speech. Yet, relatively few studies exist on using large language models, especially GPT-3, to aid in the early diagnosis of dementia. In this work, we show for the first time that GPT-3 can be utilized to predict dementia from spontaneous speech. Specifically, we leverage the vast semantic knowledge encoded in the GPT-3 model to generate text embedding, a vector representation of the transcribed text from speech, that captures the semantic meaning of the input. We demonstrate that the text embedding can be reliably used to (1) distinguish individuals with AD from healthy controls, and (2) infer the subject’s cognitive testing score, both solely based on speech data. We further show that text embedding considerably outperforms the conventional acoustic feature-based approach and even performs competitively with prevailing fine-tuned models. Together, our results suggest that GPT-3 based text embedding is a viable approach for AD assessment directly from speech and has the potential to improve early diagnosis of dementia.",
  "full_text": "RESEA RCH ARTICL E\nPredicting dementia from spontaneous\nspeech using large language models\nFelix Agbavor, Hualou Liang\nID\n*\nSchool of Biomedica l Enginee ring, Science and Health Systems, Drexel University, Philadelph ia, United\nStates of America\n* hualou .liang@drex el.edu\nAbstract\nLanguage impairment is an important biomarker of neurodegenerative disorders such as\nAlzheimer’s disease (AD). Artificial intelligence (AI), particularly natural language process-\ning (NLP), has recently been increasingly used for early prediction of AD through speech.\nYet, relatively few studies exist on using large language models, especially GPT-3, to aid in\nthe early diagnosis of dementia. In this work, we show for the first time that GPT-3 can be\nutilized to predict dementia from spontaneous speech. Specifically, we leverage the vast\nsemantic knowledge encoded in the GPT-3 model to generate text embedding, a vector\nrepresentation of the transcribed text from speech, that captures the semantic meaning of\nthe input. We demonstrate that the text embedding can be reliably used to (1) distinguish\nindividuals with AD from healthy controls, and (2) infer the subject’s cognitive testing score,\nboth solely based on speech data. We further show that text embedding considerably out-\nperforms the conventional acoustic feature-ba sed approach and even performs competi-\ntively with prevailing fine-tuned models. Together, our results suggest that GPT-3 based\ntext embedding is a viable approach for AD assessment directly from speech and has the\npotential to improve early diagnosis of dementia.\nAuthor summary\nAlzheimer’s disease is a currently incurable brain disorder. Speech, a quintessentially\nhuman ability, has emerged as an important biomarker of neurodegenerative disorders\nlike AD. Can AI-driven speech analysis help identify AD? We show in this study that\nGPT-3, a specific language model produced by OpenAI, could be a step towards early pre-\ndiction of AD through speech. Specifically, we demonstrate that text embedding, powered\nby GPT-3, can be reliably used to (1) distinguish individuals with AD from healthy con-\ntrols, and (2) infer the subject’s cognitive testing score, both solely based on speech data.\nWe further show that text embedding considerably outperforms the conventional feature-\nbased approach and even performs competitively with the mainstream use of fine-tuned\nmodels. Our results suggest that there is a huge potential to develop and translate a fully\ndeployable AI-driven tools for early diagnosis of dementia and direct tailored interven-\ntions to individual needs, thereby improving quality of life for individuals with dementia.\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 1 / 14\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Agbavor F, Liang H (2022) Predictin g\ndementia from spontaneous speech using large\nlanguage models. PLOS Digit Health 1(12):\ne0000168. https://d oi.org/10.1371/j ournal.\npdig.00001 68\nEditor: Benjamin P. Geisler, Massachu setts\nGeneral Hospital, UNITED STATES\nReceived: August 2, 2022\nAccepted: November 21, 2022\nPublished: December 22, 2022\nCopyright: © 2022 Agbavor, Liang. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: All the data are\navailable at https://deme ntia.talkbank .org.\nFunding: The authors received no specific funding\nfor this work.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nIntroduction\nAlzheimer’s disease (AD) is a neurodegenerative disease that involves progressive cognitive\ndeclines, including speech and language impairments. It is the most common etiology of\ndementia, affecting 60–80% cases [1]. Given its prevalence and still no cure available for AD\ntreatment [2], there is an urgent need for the early diagnosis of dementia, which would yield\nclear benefits in improving quality of life for individuals with dementia.\nCurrent diagnoses for AD are still primarily made through clinical assessments such as\nbrain imaging or cognitive tests e.g., Mini-Mental State Examination (MMSE) [3] for evaluat-\ning the progression of AD [4,5]. However, they are often expensive and involve lengthy medi-\ncal evaluations. Previous studies have shown that spontaneous speech contains valuable\nclinical information in AD [6]. The use of speech as a biomarker provides quick, cheap, accu-\nrate and non-invasive diagnosis of AD and clinical screening. Previous works on speech analy-\nsis are mainly based on the feature-based approach using acoustic features extracted from the\nspeech audio and the linguistic features derived from the written texts or speech transcripts\nthrough NLP techniques [7]. Both the linguistic and acoustic features, sometimes along with\nother speech characteristics, have been extensively used for dementia classification based on\nspeech data [8–17]. This feature-based approach, however, relies heavily upon domain specific\nknowledge and hand-crafted transformations. As a result, it often fails to extract more abstract,\nhigh-level representations [18,19], hence is hard to generalize to other progression stages and\ndisease types, which may correspond to different linguistic features. AI-enabled speech and\nlanguage analysis has emerged as a promising approach for early screening of Alzheimer’s\ndementia [9,10,15,20–22].\nLarge language models (LLMs), which have demonstrated impressive performance on\nmany NLP tasks, provide powerful universal language understanding and generation [23–25].\nGPT- 3, or Generative Pre-trained Transformer 3, one of the largest existing language models\nproduced by OpenAI [26], has been shown to be particularly effective in (1) zero-shot learning\n(i.e., zero-data learning), where the language model is adapted to downstream tasks, such as\ntranslation, text summarization, question-answering and dialogue systems, without the need\nfor additional, task-specific data [26], and (2) encoding a wealth of semantic knowledge about\nthe world and producing a learned representation (embedding), typically a fixed-size vector,\nthat lends itself well to discriminative tasks [27]. The text embeddings entail meaningful vector\nrepresentations that can uncover additional patterns and characteristics, as captured in the\nsemantic meaning of the input, that might not be evident even to trained experts. It has been\nextremely successful to learn text embeddings in NLP [23,24,28–30]. However, so far there is\nno study on the use of GPT-3 for AD detection.\nIn this work, we study the extent to which text embeddings generated by GPT-3 are utilized\nto predict the dementia. We use the data from the ADReSSo (Alzheimer’s Dementia Recogni-\ntion through Spontaneous Speech only) Challenge [21], a shared task for the systematic com-\nparison of approaches to the detection of cognitive impairment and decline based on\nspontaneous speech. With this dataset, we perform two tasks: an AD classification task for dis-\ntinguishing individuals with AD from healthy controls, and an MMSE score regression task to\ninfer the cognitive test score of the subject, both solely based on the demographically matched\nspontaneous speech data. We show that the text embedding can be reliably used for detection\nof Alzheimer’s dementia and inference of the cognitive testing score. We further show that text\nembedding (Fig 1B) considerably outperforms the conventional acoustic feature-based\napproach (Fig 1A) and is even competitive with fine-tuned models. Taken together, our results\ndemonstrate that text embedding, derived from GPT-3 model, is a viable approach for the\nassessment of AD status with great promise in assisting with early diagnosis of dementia.\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 2 / 14\nResults\nWe report the results from two tasks which include AD vs non-AD classification and AD\nseverity prediction using a subject’s MMSE score. For the classification task, either the acoustic\nfeatures or GPT-3 embeddings (Ada and Babbage) or both are fed into a machine-learning\nmodel such as support vector classifier (SVC), logistic regression (LR) or random forest (RF).\nAs a comparison, we further perform finetuning on the GPT-3 model to see if there is any\nadvantage over the GPT-3 embedding.\nFor the AD severity prediction, we perform the regression analysis based on both the acous-\ntic features and GPT-3 embeddings to estimate a subject’s MMSE score using three regression\nmodels, i.e., support vector regressor (SVR), ridge regression (Ridge) and random forest\nregressor (RFR).\nAD vs Non-AD classification\nIn this section we present the AD classification results between AD and non-AD (or healthy\ncontrol) subjects based on different features: our proposed GPT-3 based text embeddings, the\nacoustic features, and their combination. We also benchmark the GPT-3 based text embed-\ndings against the mainstream fine-tuning approach. We show that the GPT-3 based text\nembeddings considerably outperform both the acoustic feature-based approach and the fine-\ntuned model.\nUsing acoustic features\nThe classification performance in terms of accuracy, precision, recall and F1 score for all the\nmodels with the acoustic features is shown in Table 1 for both the 10-fold cross-validation\n(CV) and evaluation on the held-out test set not used in any way during model development.\nFig 1. Schematic showin g two different feature representa tions that are derived from speech. A. The acoustic\nfeatures are engineered to capture the acoustic characteristi cs of speech and therefore the pathological speech behavior.\nB. The linguistic features, represented as text embeddin gs, are derived from the transcribed text. Central to our\nproposed approach is the GPT-3 based text embeddings (shaded), which entail meaningfu l vector representati ons that\ncan capture lexical, syntactic, and semantic properties for dementia classifi cation.\nhttps://d oi.org/10.1371/j ournal.pdig. 0000168.g001\nTable 1. Model performan ce obtained by the 10-fold CV (top) where the mean (standard deviation ) are reported , and evaluate d on test set (bottom) for AD classifi-\ncation using acoustic features. Bold indicates the best overall performan ce for the metric.\nModel Accuracy Precision Recall F1\n10-fold CV SVC 0.697 (0.095) 0.722 (0.091) 0.660 (0.120) 0.678 (0.084)\nLR 0.632 (0.120) 0.645 (0.136) 0.656 (0.131) 0.647 (0.121)\nRF 0.668 (0.101) 0.705 (0.156) 0.704 (0.114) 0.686 (0.084)\nTest Set SVC 0.634 0.657 0.622 0.639\nLR 0.620 0.600 0.618 0.609\nRF 0.746 0.771 0.730 0.750\nhttps://do i.org/10.1371/j ournal.pdig. 0000168.t00 1\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 3 / 14\nFrom Table 1, we can see that for the evaluation on the test set, RF performs the best among\nthe three models in all the metrics used. For the 10-fold CV, RF also has the highest recall and\nF1 score among all the models, although the SVC performs better than other two models in\nboth accuracy and precision.\nUsing GPT-3 embeddings\nThe classification performance of the GPT-3 embedding models is shown in Table 2 for the\n10-fold CV (top, unshaded) and for the evaluation on the test set (bottom, shaded). Several\nobservations can be made: (1) the use of GPT-3 embeddings yields a substantial improvement\nin performance when compared to the acoustic feature-based approach (Table 1); (2) the Bab-\nbage outperforms the Ada, a result consistent with the general notion that larger model is\nmore powerful in various tasks [27]; (3) the performance of the 10-fold CV is comparable to\nthat for the evaluation on the test set on the hold-out test set; and (4) direct comparison with\nthe best baseline of the classification accuracy of 0.6479 [21] on the same test set reveals that\nGPT-3 performs remarkably well with the best accuracy of 0.8028 by SVC, showing clear\nadvantage of using GPT-3 models.\nTo examine how the GPT-3 based text embeddings fare with the fine-tuning approach, we\nuse the GPT-3 Babbage as the pretrained model and fine tune it with the speech transcripts.\nThe results are shown in Table 3 for both the 10-fold CV and evaluation on the test set. We see\nfrom Table 3 that, while the overall performance is comparable for both the 10-fold CV and\nthe evaluation on the test set, the fine-tuned Babbage model underperforms the GPT-3 based\ntext embeddings, a result in line with the recent findings that GPT-3 embedding model is even\ncompetitive with fine-tuned models [27]. We note, however, that there is no statistically signif-\nicant difference of the accuracy between the finetuned model and the GPT-3 embedding based\non a Kruskal-Wallis H-test (H = 0.8510, p > 0.05).\nTable 2. Model performan ce obtained by the 10-fold CV (top, unshaded) where the mean (standard deviation ) are reported, and evaluated on test set (bottom,\nshaded) for AD classific ation using text embeddings from the GPT-3 base models (Babbage and Ada). Bold indicates the best overall performan ce of each metric sepa-\nrately for the top and the bottom panels.\nEmbeddings Model Accuracy Precision Recall F1\n10-fold CV Ada SVC\nLR\nRF\n0.788 (0.075)\n0.796 (0.107)\n0.734 (0.090)\n0.798 (0.109)\n0.798 (0.126)\n0.738 (0.109)\n0.819 (0.098)\n0.835 (0.129)\n0.763 (0.149)\n0.799 (0.066)\n0.808 (0.100)\n0.743 (0.103)\nBabbage SVC\nLR\nRF\n0.802 (0.054)\n0.809 (0.112)\n0.760 (0.052)\n0.823 (0.092)\n0.843 (0.148)\n0.780 (0.102)\n0.804 (0.103)\n0.811 (0.091)\n0.781 (0.110)\n0.806 (0.053)\n0.818 (0.091)\n0.770 (0.047)\nTest Set Ada SVC 0.788 0.708 0.971 0.819\nLR 0.718 0.653 0.914 0.762\nRF 0.732 0.690 0.829 0.753\nBabbage SVC 0.803 0.723 0.971 0.829\nLR\nRF\n0.718\n0.761\n0.647\n0.714\n0.943\n0.857\n0.767\n0.779\nhttps://do i.org/10.1371/j ournal.pdig. 0000168.t00 2\nTable 3. Results for the fine-tuned GPT-3 Babbage model obtained by the 10-fold CV where the mean (standard deviation ) are reported, and evaluated on test set\nfor AD classification .\nAccurac y Precision Recall F1\n10-fold CV 0.797 (0.058) 0.810 (0.127) 0.809 (0.071) 0.797 (0.105)\nTest Set 0.803 0.806 0.806 0.806\nhttps://do i.org/10.1371/j ournal.pdig. 0000168.t00 3\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 4 / 14\nCombination of acoustic features and GPT-3 embeddings\nTo evaluate whether the acoustic features and the text embeddings can provide complemen-\ntary information to augment the AD classification, we combine the acoustic features from\nspeech audio data and the GPT-3 based text embeddings by simply concatenating them.\nTable 4 shows the results for both the 10-fold CV and evaluation on the test set for different\nmachine learning models. With additional acoustic features, we observe only marginal\nimprovement in the classification performance on the 10-fold CV. There is no clear difference\nin predicting the test set in terms of accuracy and F1 score when the acoustic features are com-\nbined with GPT-3 based text embeddings, but we instead observe higher precision at the\nexpense of lower recall. This observation indicates that the combined approach could be well-\nsuited in screening AD when high precision is much more important than the recall.\nComparison of acoustic features with GPT-3 embeddings\nTo compare the acoustic features with the GPT-3 embeddings, we perform further analysis\nbased on the performance measurement of the area under the Receiver Operating Characteris-\ntic (ROC) curve (AUC). Fig 2 shows the ROC curves for RF model using the acoustic features\n(the best-performing acoustic model) and the GPT-3 embeddings (both Ada and Babbage).\nThe mean and standard deviation of AUCs from the 10-fold CV are also reported, which indi-\ncate that the GPT-3 embeddings outperform the RF model using the acoustic features and the\nBabbage is marginally better than Ada. The Kruskal-Wallis H-test reveals a significant differ-\nence between the GPT-3 embeddings and the RF acoustic model (H = 5.622, p < 0.05).\nComparison with several existing models\nWe benchmark our proposed GPT-3 embedding (Babbage) method against other state-of-the-\nart AD detection models. The existing methods include the studies from Luz et al [21], Balago-\npalan & Novikova [8] and Pan et al [31], which all used the ADReSSo Challenge data. The\nmodels selected are all trained based on the 10-fold CV and evaluated on the same unseen test\nset to ensure fair comparison. For example, we do not include Model 4 & 5 in Pan et al [31] as\nthe models were trained by holding out 20% of the training set. Instead, we select the best\nmodel (Model 2), which was trained using 10-fold CV. The comparison is presented in\nTable 5, from which we can see that our method overall outperforms all other models in terms\nof accuracy, recall, and F1 score, though the precision is relatively low.\nMMSE Score Prediction\nWe perform the regression analysis using three different models: Support Vector Regression\n(SVR), Ridge Regression (Ridge) and Random Forest Regressor (RFR). The regression results,\nreported as root mean squared error (RMSE), using acoustic features and text embeddings\nTable 4. Model performan ce for the 10-fold CV with standard deviation and the evaluation on test set using a combination of the GPT-3 Babbage embeddings and\nthe acoustic features.\nModel Accuracy Precision Recall F1\n10-fold CV SVC 0.814 (0.115) 0.838 (0.133) 0.802 (0.136) 0.814 (0.119)\nLR 0.800 (0.108) 0.831 (0.137) 0.803 (0.097) 0.809 (0.093)\nRF 0.731 (0.121) 0.741 (0.141) 0.762 (0.119) 0.745 (0.109)\nTest Set SVC 0.802 0.971 0.723 0.829\nLR 0.676 0.971 0.607 0.747\nRF 0.788 0.914 0.727 0.810\nhttps://do i.org/10.1371/j ournal.pdig. 0000168.t00 4\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 5 / 14\nfrom GPT-3 (Ada and Babbage) are shown in Tables 6 and 7, respectively. In each table, we\nprovide the RMSE scores of the MMSE prediction for both the 10-fold CV and evaluation on\nthe test set.\nWith acoustic features, Table 6 shows that Ridge has the lowest RMSE score (6.2498) for\nMMSE prediction on the evaluation on the test set and the lowest RMSE of 6.7683 on the\n10-fold CV. With the GPT-3 based text embeddings, Table 7 shows that Babbage has better\nprediction performance than Ada in terms of RMSE score in both the 10-fold CV and evalua-\ntion on the test set. When comparing the overall regression results in relation to what kinds of\nfeatures are used, the GPT-3 based text embeddings provide clear advantage, as they always\noutperform the acoustic features.\nFig 2. ROC curves, along with the averaged AUC scores and standard deviations, obtained by the 10-fold CV for\nthe best acoustic, Ada and Babbage embedd ing models.\nhttps://d oi.org/10.1371/j ournal.pdig. 0000168.g002\nTable 5. Performance comparis on between our model and other models on the ADReSS o 2021 unseen test set.\nModel Accurac y Precision Recall F1\nGPT-3 Embedding (ours)\nPan et al 2021\nBalagopalan et al 2021\nLuz et al 2021\nSVC\nBERT\nbase\nSVC\nSVC\n0.803\n0.803\n0.676\n0.789\n0.723\n0.862\n0.636\n0.778\n0.971\n0.714\n0.800\n0.800\n0.829\n0.781\n0.709\n0.789\nhttps://do i.org/10.1371/j ournal.pdig. 0000168.t00 5\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 6 / 14\nDiscussion\nThe current NLP landscape has been revolutionized by large language models [23,26]. GPT- 3,\na specific language model produced by OpenAI [26], is particularly powerful in encoding a\nwealth of semantic knowledge about the world and producing a high-quality vector represen-\ntation (embedding) that lends itself well to discriminative tasks [27]. Given its impressive per-\nformance, we probe in this work the ability of GPT-3 to predict dementia from speech by\nutilizing the vast semantic knowledge encoded in the model. Our results demonstrate that the\ntext embedding, generated by GPT-3, can be reliably used to not only detect individuals with\nAD from healthy controls but also infer the subject’s cognitive testing score, both solely based\non speech data. We further show that text embedding outperforms the conventional acoustic\nfeature-based approach and even performs competitively with fine-tuned models. These\nresults, all together, suggest that GPT-3 based text embedding is a promising approach for AD\nassessment and has the potential to improve early diagnosis of dementia. We should note that\nour study performed model development and internal validation based mainly on the\nADReSSo Challenge data; thus, further independent external validation is needed to confirm\nour findings.\nThere are four GPT-3 models available to the public via the OpenAI API, each having dif-\nferent number of embedding size and parameter: Ada (1024 dimensions, 300M), Babbage\n(2048 dimensions, 1.2B), Curie (4096 dimensions, 6B) and Davinci (12288 dimensions, 175B).\nThese models have different capabilities and price points. Ada is the fastest and most\nTable 6. MMSE predictio n in terms of RMSE scores for three different models (SVR, Ridge and RFR) using acous-\ntic features on the 10-fold CV (top) with standard deviation and on the inference on test set (bottom). Bold indi-\ncates the best RMSE score.\nModel RMSE\n10-fold CV SVR 7.049 (2.355)\nRidge 6.768 (1.524)\nRFR 6.901 (1.534)\nTest Set SVR 6.285\nRidge 6.250\nRFR 6.434\nhttps://d oi.org/10.1371/j ournal.pdig. 0000168.t0 06\nTable 7. MMSE predictio n in terms of RMSE scores for three different models (SVR, Ridge and RFR) using text\nembedd ings from GPT-3 (Ada and Babbage) on the 10-fold CV (top) with standard deviation and on the inference\non test set (bottom ). Bold indicates the best RMSE score.\nEmbeddi ngs Model RMSE\n10-fold CV Ada SVR 6.097 (2.057)\nRidge 6.058 (1.298)\nRFR 6.300 (1.129)\nBabbage SVR 5.976 (1.173)\nRidge 5.843 (1.037)\nRFR 6.330 (1.032)\nTest Set Ada SVR 5.6307\nRidge 5.8735\nRFR 6.0010\nBabbage SVR 5.4999\nRidge 5.4645\nRFR 5.8142\nhttps://d oi.org/10.1371/j ournal.pdig. 0000168.t0 07\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 7 / 14\naffordable model but is the least capable, while Davinci is the most powerful but is most expen-\nsive among the models. As of this writing, we note the OpenAI paid-API is now more afford-\nable. It is expected that GPT-3 will be eventually made free available to the community. When\nthe decision is made as to which model should be used, the embedding size and the number of\nparameters are two important factors that ought to be taken into consideration. In general,\nlarger models incur higher cost in terms of storage, memory, and computation time, which\ninevitably has direct impact on the model deployment in real-world applications such as AD\ndiagnosis. Given the budget consideration and especially small data sample in the ADReSSo\nChallenge, we decided to go with the Ada and Babbage models in this application. Otherwise,\nthere is a risk of overfitting when the data are not abundant, especially with the larger models\n(Curie and Davinci). Indeed, when we tested with the Curie and Davinci, we found the model\noverfitting by observing almost perfect recall and extremely low precision in AD classification\ntask. We note that, while large sample sizes certainly help, we have taken precautious steps to\ntest model generalizability with both the 10-fold CV and evaluation on the test set to guard\nagainst the problem of small sample size.\nFine-tuning has become the de facto standard to leverage large pretrained models to per-\nform downstream tasks [24,25,32]. When we used the GPT-3 Babbage as the pretrained model\nand fine-tuned it with the speech transcripts, we however did not see the improvement in per-\nformance, as generally expected. While our results are in line with the recent findings that\nGPT-3 embedding model performs competitively with fine-tuned models [27], there is a possi-\nbility that the underperformance could be due to the insufficient data available in this task, as\nit is well known that the fine-tuning may predispose the pretrained model to overfitting due to\nthe huge model and relatively small size of the domain-specific data. Such a possibility remains\nto be tested in the future when more data is available.\nThere is a huge potential to develop and translate a fully deployable AI-driven speech analy-\nsis for early diagnosis of dementia and direct tailored interventions to individual needs.\nDespite promising, major challenges lie with data quality (inconsistency and instability), data\nquantity (limited data), and diversity. For any models to work well, we need to have a very\nlarge, diverse and robust set of data. Leveraging AI with the growing development of large-\nscale, multi-modal data such as neuroimaging, speech and language, behavioral biomarkers,\nand patient information on electronic medical records, will help alleviate the data problem and\nallow for more accurate, efficient, and early diagnosis [33].\nOur AI model could be deployed as a web application or even a voice-powered app used at\nthe doctor’s office to aid clinicians in AD screening and early diagnosis. When applying AI\nand machine learning to predict dementia in clinical settings, there are however a number of\npotential problems. First, the bias should be considered in model development. It is mandated\nto have speech data from around the world, in many different languages, to guard against this\nproblem, and to ensure the models work equitably for all patients, regardless of age, gender,\nethnicity, nationality and other demographic criteria. It is preferred to develop ethical and\nlegal systems for the implementation, validation and control of AI in clinical care [34]. Second,\nthe privacy is a major concern in this nascent field, particularly speech data, which can be used\nto identify individuals. Third, there is a need to establish trust in AI, especially pertinent to the\nso-called ‘black box’ problem. This often arises in machine learning models where even the\ndevelopers themselves can’t fully explain, particularly which information are used to make pre-\ndictions. This can be problematic in clinical practice to explain how a diagnosis of dementia is\nascertained and what can determine personalized treatments. Explainable AI aims to address\nthe questions about the decision-making processes. Therefore, it is important to acknowledge\nthat AI is not a replacement for human, but rather provides augmented decision making in\ndriving efficient care and helping make accurate diagnoses. Before the AI-driven technologies\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 8 / 14\nenter mainstream use in aiding the diagnosis of dementia, it is essential to have rigorous vali-\ndation from large-scale, well-designed representative studies through multidisciplinary collab-\noration between AI researchers and clinicians. This will ultimately allow AI to improve early\ndiagnosis, which is crucial to improve quality of life for individuals with dementia.\nMaterials and methods\nDataset description\nThe dataset used in this study is derived from the ADReSSo Challenge [21], which consists of\nset of speech recordings of picture descriptions produced by cognitively normal subjects and\npatients with an AD diagnosis, who were asked to describe the Cookie Theft picture from the\nBoston Diagnostic Aphasia Examination [6,35]. There are totally 237 speech recordings, with\n70/30 split balanced for demographics, resulting in 166 and 71 in the training set and the test\nset, respectively. In the training set, there are 87 samples from AD subjects and 79 from non-\nAD (or healthy control) subjects. The datasets were matched so as to avoid potential biases\noften overlooked in assessment of AD detection methods, including incidences of repetitive\nspeech from the same individual, variations in speech quality, and imbalanced distribution of\ngender and age. The detailed procedures to match the data demographically according to pro-\npensity scores were described in Luz et al. [21]. In the final dataset, all standardized mean dif-\nferences for the age and gender covariates are < 0.001.\nEthics statement\nThe studies involving human participants were reviewed and approved by DementiaBank con-\nsortium. All enrolled participants provided informed written consent to participate in this\nstudy. All data analyses in this work are conducted using the de-identified data.\nComputational approaches\nAt the core of our proposed approach (Fig 1B) is the text embedding from GPT-3 [26], which\ncan be readily accessed via OpenAI Application Programming Interface (API). The OpenAI\nAPI, powered by a family of models with different capabilities and price points, can be applied\nto virtually any task that involves understanding or generating natural language or code. We\nuse the GPT-3 for text embedding, which is powerful representation of the semantic meaning\nof a piece of text. We benchmark our GPT-3 embedding approach against both the conven-\ntional acoustic feature-based approach (Fig 1A) and the prevailing fine-tuned model.\nText embeddings from GPT-3\nCentral to our approach is the innovative use of text embeddings, powered by GPT-3. To our\nknowledge, this is the first application of GPT-3 to predicting dementia from speech. In our\napproach (Fig 1B), we first convert voice to text using Wav2Vec 2.0 pretrained model [36], a\nstate-of-the-art model for automatic speech recognition. We use the base model wav2vec2--\nbase-960h that was pretrained and fine-tuned on 960 hours of Librispeech on 16 kHz sampled\nspeech audio, which can be accessed from Huggingface [37]. Each audio file is loaded as a\nwaveform with librosa [38], a python package dedicated to analyzing sounds. The waveform is\nthen tokenized using Wav2Vec2Tokenizer and if necessary, divided them into smaller chunks\n(with the maximum size of 100,000 in our case) to fit into memory, which is subsequently fed\ninto the Wav2Vec2ForCTC (a wav2vec model for speech recognition) and decoded as text\ntranscripts.\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 9 / 14\nGPT-3 based text embeddings are afterwards derived from the transcribed text obtained via\nwav2vec2. We use the endpoint in the OpenAI API, which is available to the registered\nresearchers, to access GPT-3 embedding models [27]. These embeddings entail meaningful\nvector representations that can capture lexical, syntactic, and semantic properties useful for\ndementia classification. It is recently shown that with GPT-3 based text embeddings new state-\nof-the-art results can be achieved in a variety of tasks including semantic search, clustering,\nand classification [27].\nThere are four GPT-3 models on a spectrum of embedding size: Ada (1024 dimensions),\nBabbage (2048 dimensions), Curie (4096 dimensions) and Davinci (12288 dimensions).\nDavinci is the most powerful but is more expensive than the other models, whereas Ada is the\nleast capable but is significantly faster and cheaper. As of this writing, the embeddings are\nbilled at 10 times of the base prices. Specifically, they charge $0.2 and $0.004 per 1000 tokens\nfor the largest model (Davinci) and the smallest model (Ada), respectively. These embeddings\nare finally used as features to train machine learning models for AD assessment. Given the cost\nconsideration, especially the small sample size in the ADReSSo Challenge, we report the results\nobtained by the Ada and Babbage models.\nAcoustic feature extraction from speech\nConventional acoustic feature-based approach (Fig 1A) will be used as benchmark for compar-\nison. The acoustic features considered are mainly related to temporal analysis (e.g. pause rate,\nphonation rate, periodicity of speech, etc.), frequency analysis (e.g. mean, variance, kurtosis of\nMel frequency cepstral coefficients) and different aspects of speech production (e.g. prosody,\narticulation, or vocal quality). In this work, acoustic features are extracted directly from speech\nusing OpenSMILE (open-source Speech and Music Interpretation by Large-space Extraction),\na widely used open-source toolkit for audio feature extraction and classification of speech and\nmusic signals [39]. We primarily used the extended Geneva Minimalistic Acoustic Parameter\nSet (eGeMAPS) features due to their potential to detect physiological changes in voice produc-\ntion, as well as theoretical significance and proven usefulness in previous studies [40]. There\nare, in total, 88 features: the arithmetic mean and coefficient of variation of 18 low-level\ndescriptors (e.g., pitch, jitter, formant 1–3 frequency and relative energy, shimmer, loudness,\nalpha ratio and Hammarberg index etc), 8 functionals applied to pitch and loudness, 4 statis-\ntics over the unvoiced segments, 6 temporal features, and 26 additional cepstral parameters\nand dynamic parameters. This feature set once obtained can be used directly as inputs to a\nmachine learning model.\nFine-tuning with speech transcripts\nFine-tuning is the prevalent paradigm for using LLMs [23–25] to perform downstream tasks.\nIn this approach, the pretrained models such as the BERT (Bidirectional Encoder Representa-\ntions from Transformers) [32], either some or all the model parameters, can be finetuned or\nupdated with downstream task-specific data. Recent work has shown encouraging results with\nfine-tuned BERT for AD detection [20,41]. In this study, we will also benchmark our proposed\nGPT-based embedding approach against the mainstream use of fine-tuned model. As such, we\nuse the GPT-3 as the pretrained model and fine-tune it with speech transcripts obtained by\nwav2vec2 from raw audio files.\nTo fine tune our own custom GPT-3 models, we use the OpenAI command-line interface,\nwhich is released to the public. We simply follow the instructions about fine-tuning, provided\nby OpenAI, to prepare the training data that consists of 166 paragraphs, totaling 19,123 words\nthat are used to fine tune one of the base models (Babbage and Ada in our case) with speech\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 10 / 14\ntranscripts. Tokens used to train a model are relatively cheaper, as billed at 50% of the base\nprices.\nExperimental tasks\nAD vs non-AD classification. The AD classification task consists of creating a binary\nclassification model to distinguish between AD and non-AD speech. The model may use\nacoustic features from speech, linguistic features (embeddings) from transcribed speech, or\nboth. As such, we use (1) the acoustic features extracted from speech audio data, (2) the text\nembeddings from each GPT-3 base model (Babbage or Ada), and (3) the combination of both\nas inputs for three different kinds of commonly used machine learning models, including Sup-\nport Vector Classifier (SVC), Random Forest (RF), and Logistic Regression (LR). We use the\nscikit-learn library for the implementation of these models [42]. The hyperparameters for each\nmodel are tuned using the 10-fold cross-validation. Specifically, there are two key parameters\n(the regularization parameter and the kernel coefficient) for SVC trained with a radial basis\nfunction kernel, the L2-penalty parameter for LR and two key parameters (the number of esti-\nmators and the maximum depth of the tree) for RF. As a comparison, we also fine tune the\nGPT-3 model (Babbage) with the speech transcripts to assess if the GPT-3 based text embed-\ndings can be better used to predict the dementia.\nMMSE score prediction. MMSE is perhaps the most common measure for assessing the\nseverity of AD. We perform regression analysis using both the acoustic features and text\nembeddings from GPT-3 (Ada and Babbage) to predict the MMSE score. The scores normally\nrange from 0 to 30, with scores of 26 or higher being considered normal [3]. A score of 20 to\n24 suggests mild dementia, 13 to 20 suggests moderate dementia, and less than 12 indicates\nsevere dementia. As such, the prediction is clipped to a range between 0 and 30. Three kinds of\nregression models are employed, including Support Vector Regression (SVR), Ridge regres-\nsion (Ridge) and Random Forest Regressor (RFR). The models are similarly implemented with\nthe scikit-learn library [42], with the hyperparameters for each model determined using grid-\nsearch 10-fold cross-validation on the training dataset.\nPerformance evaluation\nFor AD classification task, the performance is evaluated by a panel of metrics such as the accu-\nracy, precision, recall and F1-score, where the threshold of 0.5 is used. The ADReSSo Chal-\nlenge dataset was already split into the training set and the test set, with 70% of samples\nallocated to the former and 30% allocated to the latter. To evaluate the generalization ability of\nthe model, we have two ways to report the performance: 10-fold cross-validation (CV) and\nevaluation on the test set. The model is well calibrated before testing. The first way is to test for\ngeneralizability within a dataset using the 10-fold CV approach. This way, we partition all the\navailable data (i.e., the entire data including the training set and test set) into three sets (train-\ning, validation and test sets) in an 80/10/10 ratio using the 10-fold CV. That is, we use 8-fold\nfor training, 1-fold for validation, and the remaining for testing in each run. We report the\naverage of the ten independent runs in which the test data is different in each run. As such, we\ncan reduce the potential sampling bias where the results can depend on a particular random\nchoice of the data sets. We also report the averaged AUC scores, along with the corresponding\nstandard deviations over the 10-fold CV when comparing the different models using acoustic\nfeatures, GPT-3 embeddings (both Ada and Babbage) for AD classification.\nThe second way to report the performance is that the model is evaluated on an unseen test\nset not used in any way during model development. Since we have a separate test set that was\nalready set aside, we use it as the independent, held-out dataset. We still perform the 10-fold\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 11 / 14\ncross-validation, but only the existing training set. That is, we split the training dataset into ten\nfolds, with 9 folds for training, and the remaining for validation to tune hyperparameters in\neach run for ten independent runs. We then fit the model on all the training dataset using the\nhyperparameters of the best model, and then use the final model on the held-out test data. The\nuse of the held-out test data allows us to directly compare the different models as well as with\nChallenge baseline on the same dataset. We stress that the test dataset is different between\n10-fold CV and evaluation on the test set.\nFor AD regression task, we similarly conduct the 10-fold CV and inference on the test set.\nWe report root mean squared error (RMSE) for the MMSE score predictions on the testing\ndata using the models obtained by 10-fold CV. The hyperparameters for each model are deter-\nmined based on performance in grid-search 10-fold cross-validation on the training dataset.\nIn finetuning GPT-3 for the AD classification task, the hyperparameters we used are consis-\ntent with the recommendations by OpenAI for its Babbage model. Specifically, the hyperpara-\nmeters available to be tuned include the number of epochs, batch size and learning rate\nmultiplier. We vary the number of epochs from 1 to 5, learning rate multiplier between 0.02 to\n0.2 and the batch size between 4 to 10 and compare with the results from default internal\nparameters originally set by OpenAI. It turned out that the recommended hyperparameters by\nOpenAI work best for the finetuning.\nIn doing the 10-fold CV, all the results we reported are the average of the ten folds, together\nwith its standard deviation. The statistical significance between the models is performed via\nthe Kruskal-Wallis H-test. We use the Kruskal-Wallis H test for sample comparison because it\nis non-parametric and hence does not assume that the samples are normally distributed.\nAcknowledgmen ts\nWe thank the ADReSSo Challenge data that was available via DementiaBank and partly sup-\nported by NIH AG03705 and AG05133. Work reported here was run on hardware supported\nby Drexel’s University Research Computing Facility.\nAuthor Contributions\nConceptualization: Hualou Liang.\nData curation: Felix Agbavor.\nFormal analysis: Felix Agbavor.\nMethodology: Felix Agbavor.\nSupervision: Hualou Liang.\nValidation: Felix Agbavor.\nWriting – original draft: Felix Agbavor.\nWriting – review & editing: Felix Agbavor, Hualou Liang.\nReferences\n1. 2021 Alzheime r’s disease facts and figures. Alzheime rs Deme nt. 2021; 17(3):327– 406. https:/ /doi.org/\n10.1002/ alz.12328 PMID: 337560 57\n2. Yiannopoul ou KG, Papageorg iou SG. Current and future treatments in Alzheime r disease: an update. J\nCent Nerv Syst Dis. 2020; 12:117957352 090739 7. https://doi.or g/10.1177 /11795735209 07397 PMID:\n32165850\n3. Folstein MF, Folstein SE, McHugh PR. “Mini-me ntal state”: a practical method for grading the cognitive\nstate of patients for the clinician. J Psychiatr Res. 1975; 12(3):189– 98.\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 12 / 14\n4. Gupta Y, Lee KH, Choi KY, Lee JJ, Kim BC, Kwon GR, et al. Early diagnosis of Alzheimer’s disease\nusing combined feature s from voxel-base d morphome try and cortical, subcortical, and hippoca mpus\nregions of MRI T1 brain images. PLOS ONE. 2019 Oct 4; 14(10):e02 22446. https://doi.or g/10.137 1/\njournal.pon e.02224 46 PMID: 31584953\n5. Seitz DP, Chan CC, Newton HT, Gill SS, Herrmann N, Smailagic N, et al. Mini-Cog for the diagnos is of\nAlzheime r’s disease dementia and other dementias within a primar y care setting. Cochrane Database\nSyst Rev [Internet]. 2018 [cited 2022 Jul 14];(2). Available from: https://www .cochrane library.com /cdsr/\ndoi/10.100 2/14651858.C D011415.p ub2/full PMID: 294708 61\n6. Goodglass H, Kaplan E, Weintra ub S. BDAE: The Boston Diagnostic Aphasia Examinati on. Third. Lip-\npincott Williams & Wilkins Philadelph ia, PA; 2001.\n7. Voleti R, Liss JM, Berisha V. A review of automated speech and languag e features for assessmen t of\ncognitive and thought disorde rs. IEEE J Sel Top Signal Process. 2019; 14(2):282– 98. https://doi.or g/\n10.1109/ jstsp.2019.2 952087 PMID: 339075 90\n8. Balagopala n A, Novikova J. Comp aring Acoustic -based Approaches for Alzheimer’s Disease Detection\n[Internet ]. arXiv; 2021 [cited 2022 Jul 14]. Available from: http://arxiv .org/abs/2 106.01555\n9. de la Fuente Garcia S, Ritchie CW, Luz S. Artificial Intellige nce, Speech , and Language Processin g\nApproaches to Monitoring Alzheime r’s Disease : A Systematic Review. J Alzheime rs Dis. 2020 Jan 1;\n78(4):1547 –74. https:// doi.org/10.32 33/JAD-20 0888 PMID: 331856 05\n10. Eyigoz E, Mathur S, Santama ria M, Cecchi G, Naylor M. Linguistic markers predict onset of Alzheime r’s\ndisease. EClinicalMe dicine. 2020; 28:100583. https://doi.or g/10.1016/ j.eclinm.202 0.100583 PMID:\n33294808\n11. Fraser KC, Meltzer JA, Rudzicz F. Linguisti c Features Identify Alzheimer’s Disease in Narrative\nSpeech. J Alzheime rs Dis. 2016 Jan 1; 49(2):407– 22. https://doi.or g/10.323 3/JAD-150 520 PMID:\n26484921\n12. Haider F, de la Fuente S, Luz S. An Assessme nt of Paralingu istic Acoustic Features for Detecti on of\nAlzheime r’s Dementia in Spontaneou s Speech. IEEE J Sel Top Signal Process. 2020 Feb; 14(2):272–\n81.\n13. Kong W, Jang H, Carenini G, Field TS. Explor ing neural models for predicti ng dementia from language.\nComput Speech Lang. 2021 Jul 1; 68:101181.\n14. Ko ¨ nig A, Satt A, Sorin A, Hoory R, Toledo-Ronen O, Derreuma ux A, et al. Automatic speech analysis\nfor the assessmen t of patients with predemen tia and Alzheime r’s disease. Alzheimers Deme nt Diagn\nAssess Dis Monit. 2015 Mar 1; 1(1):112–2 4. https://doi.or g/10.101 6/j.dadm.20 14.11.012 PMID:\n27239498\n15. Luz S, Haider F, de la Fuente S, Fromm D, MacWhinney B. Alzheime r’s Dement ia Recog nition through\nSpontaneous Speech: The ADReSS Challenge [Interne t]. arXiv; 2020 [cited 2022 Jul 14]. Available\nfrom: http://arxiv.or g/abs/200 4.06833\n16. Orimaye SO, Tai KY, Wong JS-M, Wong CP. Learning Linguistic Biomarkers for Predict ing Mild Cogni-\ntive Impairmen t using Compound Skip-gram s [Interne t]. arXiv; 2015 [cited 2022 Jul 14]. Available from:\nhttp://arxiv.o rg/abs/1511.0 2436\n17. Orimaye SO, Wong JS, Golden KJ, Wong CP, Soyiri IN. Predic ting probable Alzheimer’s disease using\nlinguistic deficits and biomarke rs. BMC Bioinformatic s. 2017; 18(1):1–13 .\n18. Aytar Y, Vondrick C, Torralba A. SoundNet: Learning Sound Representa tions from Unlabeled Video. In:\nAdvances in Neural Information Processin g System s [Internet ]. Curran Associates , Inc.; 2016 [cited\n2022 Jul 14]. Available from: https://procee dings.neu rips.cc/pape r/2016/ hash/\n7dcd340 d84f762eba80 aa538b 0c527f7-Abst ract.html\n19. Hershey S, Chaudhur i S, Ellis DPW, Gemmeke JF, Jansen A, Moore RC, et al. CNN architectures for\nlarge-scale audio classi fication. In: 2017 IEEE International Conferen ce on Acoustics , Speech and Sig-\nnal Processin g (ICASSP). 2017. p. 131–5.\n20. Balagopala n A, Eyre B, Rudzicz F, Novikova J. To BERT or Not To BERT: Comparing Speech and Lan-\nguage-base d Approaches for Alzheime r’s Disease Detecti on [Internet] . arXiv; 2020 [cited 2022 Jul 14].\nAvailable from: http://arxiv.or g/abs/200 8.01551\n21. Luz S, Haider F, de la Fuente S, Fromm D, MacWhinney B. Detecti ng cognitive decline using speech\nonly: The ADReSSo Challeng e. ArXiv Prepr ArXiv21040 9356. 2021;\n22. Amini S, Hao B, Zhang L, Song M, Gupta A, Karjadi C, et al. Automate d detection of mild cognitive\nimpairme nt and dementia from voice recordings: A natura l language processing approach. Alzheime rs\nDement [Interne t]. 2022 Jul 7 [cited 2022 Oct 6];n/a(n/a). Availab le from: http://alz.jou rnals.on linelibrary.\nwiley.com /doi/full/10 .1002/alz.1 2721 PMID: 35796399\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 13 / 14\n23. Devlin J, Chang M-W, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirection al Transfor mers for\nLanguage Understand ing [Interne t]. arXiv; 2019 [cited 2022 Jul 14]. Available from: http://arxiv.or g/abs/\n1810.04805\n24. Radford A, Narasimhan K, Saliman s T, Sutskev er I. Improving langua ge understanding by generative\npre-traini ng. 2018;\n25. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Exploring the limits of transfer learn-\ning with a unified text-to-te xt transforme r. J Mach Learn Res. 2020; 21(140 ):1–67.\n26. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, et al. Language Models are Few-Sho t\nLearners. In: Advances in Neural Informatio n Processin g Systems [Internet]. Curran Associates, Inc.;\n2020 [cited 2022 Jul 14]. p. 1877–901. Available from: https://proce edings.neurip s.cc/pap er/2020/hash/\n1457c0d 6bfcb49674 18bfb8ac142f 64a-Abstract. html\n27. Neelakantan A, Xu T, Puri R, Radford A, Han JM, Tworek J, et al. Text and Code Embeddings by Con-\ntrastive Pre-Train ing [Internet]. arXiv; 2022 [cited 2022 Jul 14]. Available from: http://arxiv.or g/abs/\n2201.10005\n28. Mikolov T, Chen K, Corrado G, Dean J. Efficient Estimation of Word Representa tions in Vector Space\n[Internet ]. arXiv; 2013 [cited 2022 Jul 14]. Available from: http://arxiv .org/abs/1 301.3781\n29. Pennington J, Socher R, Manning CD. Glove: Global vectors for word represent ation. In: Proceeding s\nof the 2014 conferen ce on empirical methods in natural language processing (EMNLP) . 2014. p. 1532–\n43.\n30. Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, et al. Deep contextualize d word represen-\ntations [Internet]. arXiv; 2018 [cited 2022 Jul 14]. Availab le from: http://arxiv. org/abs/1802. 05365\n31. Pan Y, Mirheidari B, Harris JM, Thompson JC, Jones M, Snowden JS, et al. Using the Outputs of Differ-\nent Automatic Speech Recognition Paradigms for Acoustic- and BERT-Bas ed Alzheimer’s Deme ntia\nDetection Throug h Spontaneous Speech . In: Interspee ch 2021 [Interne t]. ISCA; 2021 [cited 2022 Oct\n6]. p. 3810–4 . Available from: https://www .isca-speech.o rg/arch ive/intersp eech_2021/p an21c_\ninterspeec h.html\n32. Devlin J, Chang M-W, Lee K, Toutanova K. Bert: Pre-train ing of deep bidirectional transfor mers for lan-\nguage understa nding. ArXiv Prepr ArXiv1 81004805. 2018;\n33. Yamada Y, Shinkawa K, Kobayashi M, Caggiano V, Nemoto M, Nemoto K, et al. Combining multimod al\nbehavio ral data of gait, speech, and drawing for classification of Alzheime r’s disease and mild cognitive\nimpairme nt. J Alzheime rs Dis. 2021; 84(1):315– 27. https://doi.or g/10.323 3/JAD-210 684 PMID:\n34542076\n34. Segato A, Marzullo A, Calimeri F, De Momi E. Artificial intelligence for brain diseases: a systematic\nreview. APL Bioeng. 2020; 4(4):04150 3. https:// doi.org/10.10 63/5.001 1697 PMID: 33094213\n35. Becker JT, Boiler F, Lopez OL, Saxton J, McGonigle KL. The Natural History of Alzheime r’s Disease:\nDescription of Study Cohort and Accuracy of Diagnosis . Arch Neurol. 1994 Jun 1; 51(6):585– 94.\n36. Baevski A, Zhou Y, Moham ed A, Auli M. wav2vec 2.0: A Framework for Self-Supe rvised Learning of\nSpeech Represent ations. In: Advances in Neural Informatio n Processin g Systems [Internet]. Curran\nAssociates , Inc.; 2020 [cited 2022 Jul 14]. p. 12449– 60. Available from: https://procee dings.neu rips.cc/\npaper/2020/h ash/92d1 e1eb1cd 6f9fba322 7870bb6d7f07 -Abstrac t.html\n37. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al. Transforme rs: State-of-the -Art Natural\nLanguage Processin g. In: Proceedings of the 2020 Conferen ce on Empirica l Methods in Natural Lan-\nguage Processin g: System Demonstration s [Internet]. Online: Associat ion for Computationa l Linguis-\ntics; 2020 [cited 2022 Jul 14]. p. 38–45. Available from: https://aclant hology.org/202 0.emnlp- demos.6\n38. McFee B, Raffel C, Liang D, Ellis DP, McVicar M, Battenberg E, et al. librosa: Audio and music signal\nanalysis in python. In: Proceedings of the 14th python in science conferen ce. 2015. p. 18–25.\n39. Eyben F, Wo ¨ llmer M, Schuller B. Opensmi le: the munich versatile and fast open-so urce audio feature\nextractor. In: Proceeding s of the 18th ACM internat ional conferenc e on Multimedia [Internet]. New\nYork, NY, USA: Associat ion for Computing Machiner y; 2010 [cited 2022 Jul 14]. p. 1459–62. (MM ‘10).\nAvailable from: https://doi.or g/10.114 5/1873951.187 4246\n40. Eyben F, Scherer KR, Schulle r BW, Sundberg J, Andre ´ E, Busso C, et al. The Geneva minimalis tic\nacoustic parame ter set (GeMAPS ) for voice research and affective comput ing. IEEE Trans Affect Com-\nput. 2015; 7(2):190–2 02.\n41. Yuan J, Cai X, Bian Y, Ye Z, Church K. Pauses for detection of Alzheime r’s disease. Front Comput Sci.\n2021; 2:62448 8.\n42. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, et al. Scikit-le arn: Machine learn-\ning in Python. J Mach Learn Res. 2011; 12:2825 –30.\nPLOS DIGI TAL HEALT H\nPredict ing dementia from speech\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000016 8 Decemb er 22, 2022 14 / 14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.657454252243042
    },
    {
      "name": "Natural language processing",
      "score": 0.5935873985290527
    },
    {
      "name": "Embedding",
      "score": 0.5872273445129395
    },
    {
      "name": "Dementia",
      "score": 0.5747306942939758
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5331520438194275
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5115369558334351
    },
    {
      "name": "Language model",
      "score": 0.4473131597042084
    },
    {
      "name": "Speech recognition",
      "score": 0.41217541694641113
    },
    {
      "name": "Disease",
      "score": 0.26344984769821167
    },
    {
      "name": "Medicine",
      "score": 0.16130319237709045
    },
    {
      "name": "Pathology",
      "score": 0.08222755789756775
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I72816309",
      "name": "Drexel University",
      "country": "US"
    }
  ]
}