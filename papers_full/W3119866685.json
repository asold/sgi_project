{
  "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
  "url": "https://openalex.org/W3119866685",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4225466497",
      "name": "Fedus, William",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2933996894",
      "name": "Zoph, Barret",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224379215",
      "name": "Shazeer, Noam",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1593114658",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2784823820",
    "https://openalex.org/W2405210557",
    "https://openalex.org/W2612431505",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2767175863",
    "https://openalex.org/W2977720775",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3035839559",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W3025935268",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2807147113",
    "https://openalex.org/W3122317902",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2922485096",
    "https://openalex.org/W3088708006",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2025653905",
    "https://openalex.org/W2970401203",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W3087547017",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2982295985",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W1998498767",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2970597249"
  ],
  "abstract": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.",
  "full_text": "Journal of Machine Learning Research 23 (2022) 1-40 Submitted 8/21; Revised 3/22; Published 4/22\nSwitch Transformers: Scaling to Trillion Parameter Models\nwith Simple and Eﬃcient Sparsity\nWilliam Fedus∗\nliamfedus@google.com\nBarret Zoph∗\nbarretzoph@google.com\nNoam Shazeer\nnoam@google.com\nGoogle, Mountain View, CA 94043, USA\nEditor: Alexander Clark\nAbstract\nIn deep learning, models typically reuse the same parameters for all inputs. Mixture\nof Experts (MoE) models defy this and instead select diﬀerent parameters for each in-\ncoming example. The result is a sparsely-activated model—with an outrageous number\nof parameters—but a constant computational cost. However, despite several notable suc-\ncesses of MoE, widespread adoption has been hindered by complexity, communication costs,\nand training instability. We address these with the introduction of the Switch Transformer.\nWe simplify the MoE routing algorithm and design intuitive improved models with reduced\ncommunication and computational costs. Our proposed training techniques mitigate the\ninstabilities, and we show large sparse models may be trained, for the ﬁrst time, with lower\nprecision (bﬂoat16) formats. We design models based oﬀ T5-Base and T5-Large (Raﬀel\net al., 2019) to obtain up to 7x increases in pre-training speed with the same computational\nresources. These improvements extend into multilingual settings where we measure gains\nover the mT5-Base version across all 101 languages. Finally, we advance the current scale\nof language models by pre-training up to trillion parameter models on the “Colossal Clean\nCrawled Corpus”, and achieve a 4x speedup over the T5-XXL model. 12\nKeywords: mixture-of-experts, natural language processing, sparsity, large-scale machine\nlearning, distributed computing\n∗. Equal contribution.\n1. JAX code for Switch Transformer and all model checkpoints are available at https://github.com/\ngoogle-research/t5x\n2. Tensorﬂow code for Switch Transformer is available at https://github.com/tensorflow/mesh/blob/\nmaster/mesh_tensorflow/transformer/moe.py\n©2022 William Fedus, Barret Zoph and Noam Shazeer.\nLicense: CC-BY 4.0, seehttps://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided\nat http://jmlr.org/papers/v23/21-0998.html.\narXiv:2101.03961v3  [cs.LG]  16 Jun 2022\nFedus, Zoph and Shazeer\nContents\n1 Introduction 3\n2 Switch Transformer 4\n2.1 Simplifying Sparse Routing . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Eﬃcient Sparse Routing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3 Putting It All Together: The Switch Transformer . . . . . . . . . . . . . . . 8\n2.4 Improved Training and Fine-Tuning Techniques . . . . . . . . . . . . . . . . 8\n3 Scaling Properties 11\n3.1 Scaling Results on a Step-Basis . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.2 Scaling Results on a Time-Basis . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.3 Scaling Versus a Larger Dense Model . . . . . . . . . . . . . . . . . . . . . . 13\n4 Downstream Results 14\n4.1 Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4.2 Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n4.3 Multilingual Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n5 Designing Models with Data, Model, and Expert-Parallelism 18\n5.1 Data Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.2 Model Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5.3 Model and Data Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5.4 Expert and Data Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n5.5 Expert, Model and Data Parallelism . . . . . . . . . . . . . . . . . . . . . . 22\n5.6 Towards Trillion Parameter Models . . . . . . . . . . . . . . . . . . . . . . . 22\n6 Related Work 24\n7 Discussion 25\n8 Future Work 26\n9 Conclusion 27\nA Switch for Attention 27\nB Preventing Token Dropping with No-Token-Left-Behind 29\nC Encouraging Exploration Across Experts 29\nD Switch Transformers in Lower Compute Regimes 29\nE Relation of Upstream to Downstream Model Performance 32\nF Pseudo Code for Switch Transformers 33\n2\nSwitch Transformers\n1. Introduction\nLarge scale training has been an eﬀective path towards ﬂexible and powerful neural language\nmodels (Radford et al., 2018; Kaplan et al., 2020; Brown et al., 2020). Simple architectures—\nbacked by a generous computational budget, data set size and parameter count—surpass\nmore complicated algorithms (Sutton, 2019). An approach followed in Radford et al. (2018);\nRaﬀel et al. (2019); Brown et al. (2020) expands the model size of a densely-activated\nTransformer (Vaswani et al., 2017). While eﬀective, it is also extremely computationally\nintensive (Strubell et al., 2019). Inspired by the success of model scale, but seeking greater\ncomputational eﬃciency, we instead propose a sparsely-activated expert model: the Switch\nTransformer. In our case the sparsity comes from activating a subset of the neural network\nweights for each incoming example.\n109 1010\nSparse Model Parameters\n4.8\n5.0\n5.2\n5.4\n5.6\n5.8\n6.0Test Loss\n1e\n2e\n4e\n8e\n16e\n32e\n64e\n128e\n256e\n0 1 2 3 4\nTraining Step 1e5\n2.0\n1.9\n1.8\n1.7\n1.6\n1.5\n1.4\n1.3\n1.2\nNeg Log Perplexity\nSwitch-Base: 128e\nSwitch-Base: 64e\nSwitch-Base: 32e\nSwitch-Base: 16e\nT5-Base\nFigure 1: Scaling and sample eﬃciency of Switch Transformers. Left Plot: Scaling prop-\nerties for increasingly sparse (more experts) Switch Transformers. Right Plot:\nNegative log perplexity comparing Switch Transformers to T5 (Raﬀel et al., 2019)\nmodels using the same compute budget.\nSparse training is an active area of research and engineering (Gray et al., 2017; Gale\net al., 2020), but as of today, machine learning libraries and hardware accelerators still cater\nto dense matrix multiplications. To have an eﬃcient sparse algorithm, we start with the\nMixture-of-Expert (MoE) paradigm (Jacobs et al., 1991; Jordan and Jacobs, 1994; Shazeer\net al., 2017), and simplify it to yield training stability and computational beneﬁts. MoE\nmodels have had notable successes in machine translation (Shazeer et al., 2017, 2018; Lep-\nikhin et al., 2020), however, widespread adoption is hindered by complexity, communication\ncosts, and training instabilities.\nWe address these issues, and then go beyond translation, to ﬁnd that these class of\nalgorithms are broadly valuable in natural language. We measure superior scaling on a\ndiverse set of natural language tasks and across three regimes in NLP: pre-training, ﬁne-\ntuning and multi-task training. While this work focuses on scale, we also show that the\nSwitch Transformer architecture not only excels in the domain of supercomputers, but is\n3\nFedus, Zoph and Shazeer\nbeneﬁcial even with only a few computational cores. Further, our large sparse models can\nbe distilled (Hinton et al., 2015) into small dense versions while preserving 30% of the sparse\nmodel quality gain. Our contributions are the following:\n• The Switch Transformer architecture, which simpliﬁes and improves over Mixture of\nExperts.\n• Scaling properties and a benchmark against the strongly tuned T5 model (Raﬀel et al.,\n2019) where we measure 7x+ pre-training speedups while still using the same FLOPS\nper token. We further show the improvements hold even with limited computational\nresources, using as few as two experts.\n• Successful distillation of sparse pre-trained and specialized ﬁne-tuned models into\nsmall dense models. We reduce the model size by up to 99% while preserving 30% of\nthe quality gains of the large sparse teacher.\n• Improved pre-training and ﬁne-tuning techniques: (1) selective precision training that\nenables training with lower bﬂoat16 precision (2) an initialization scheme that allows\nfor scaling to a larger number of experts and (3) increased expert regularization that\nimproves sparse model ﬁne-tuning and multi-task training.\n• A measurement of the pre-training beneﬁts on multilingual data where we ﬁnd a\nuniversal improvement across all 101 languages and with 91% of languages beneﬁting\nfrom 4x+ speedups over the mT5 baseline (Xue et al., 2020).\n• An increase in the scale of neural language models achieved by eﬃciently combining\ndata, model, and expert-parallelism to create models with up to a trillion parameters.\nThese models improve the pre-training speed of a strongly tuned T5-XXL baseline by\n4x.\n2. Switch Transformer\nThe guiding design principle for Switch Transformers is to maximize the parameter count of\na Transformer model (Vaswani et al., 2017) in a simple and computationally eﬃcient way.\nThe beneﬁt of scale was exhaustively studied in Kaplan et al. (2020) which uncovered power-\nlaw scaling with model size, data set size and computational budget. Importantly, this work\nadvocates training large models on relatively small amounts of data as the computationally\noptimal approach.\nHeeding these results, we investigate a fourth axis: increase the parameter count while\nkeeping the ﬂoating point operations (FLOPs) per example constant. Our hypothesis is\nthat the parameter count, independent of total computation performed, is a separately\nimportant axis on which to scale. We achieve this by designing a sparsely activated model\nthat eﬃciently uses hardware designed for dense matrix multiplications such as GPUs and\nTPUs. Our work here focuses on TPU architectures, but these class of models may be\nsimilarly trained on GPU clusters. In our distributed training setup, our sparsely activated\nlayers split unique weights on diﬀerent devices. Therefore, the weights of the model increase\nwith the number of devices, all while maintaining a manageable memory and computational\nfootprint on each device.\n4\nSwitch Transformers\nRouter\nFFN 1 FFN 2 FFN 4FFN 3\nAdd + Normalize\nFFN 1 FFN 2 FFN 4FFN 3\nRouter\nSelf-Attention\nAdd + Normalize\nx1 x2\ny1 y2\np = 0.65 p = 0.8\nPositional\nembedding\nPositional\nembedding\nAdd + Normalize\nSelf-Attention\nAdd + Normalize\nSwitching FFN Layer\ny\nx\nMore Parameters\nFigure 2: Illustration of a Switch Transformer encoder block. We replace the dense feed\nforward network (FFN) layer present in the Transformer with a sparse Switch\nFFN layer (light blue). The layer operates independently on the tokens in the\nsequence. We diagram two tokens ( x1 = “More” and x2 = “Parameters” below)\nbeing routed (solid lines) across four FFN experts, where the router independently\nroutes each token. The switch FFN layer returns the output of the selected FFN\nmultiplied by the router gate value (dotted-line).\n2.1 Simplifying Sparse Routing\nMixture of Expert Routing. Shazeer et al. (2017) proposed a natural language Mixture-\nof-Experts (MoE) layer which takes as an input a token representation x and then routes\nthis to the best determined top- k experts, selected from a set {Ei(x)}N\ni=1 of N experts.\nThe router variable Wr produces logits h(x) = Wr ·x which are normalized via a softmax\ndistribution over the available N experts at that layer. The gate-value for expert iis given\nby,\npi(x) = eh(x)i\n∑N\nj eh(x)j\n. (1)\nThe top-k gate values are selected for routing the token x. If T is the set of selected top- k\nindices then the output computation of the layer is the linearly weighted combination of\neach expert’s computation on the token by the gate value,\ny=\n∑\ni∈T\npi(x)Ei(x). (2)\nSwitch Routing: Rethinking Mixture-of-Experts. Shazeer et al. (2017) conjec-\ntured that routing to k >1 experts was necessary in order to have non-trivial gradients to\nthe routing functions. The authors intuited that learning to route would not work without\nthe ability to compare at least two experts. Ramachandran and Le (2018) went further to\n5\nFedus, Zoph and Shazeer\nstudy the top- k decision and found that higher k-values in lower layers in the model were\nimportant for models with many routing layers. Contrary to these ideas, we instead use\na simpliﬁed strategy where we route to only a single expert. We show this simpliﬁcation\npreserves model quality, reduces routing computation and performs better. This k = 1\nrouting strategy is later referred to as a Switch layer. Note that for both MoE and Switch\nRouting, the gate value pi(x) in Equation 2 permits diﬀerentiability of the router.\nThe beneﬁts for the Switch layer are three-fold: (1) The router computation is reduced\nas we are only routing a token to a single expert. (2) The batch size (expert capacity) of\neach expert can be at least halved since each token is only being routed to a single expert. 3\n(3) The routing implementation is simpliﬁed and communication costs are reduced. Figure\n3 shows an example of routing with diﬀerent expert capacity factors.\nDevice 0\nDevice 0 Device 1 Device 2\nTokens\nExpert 1 Expert 2 Expert 3\n(Capacity Factor: 1.0)\nTokens\nExpert 1 Expert 2 Expert 3\n(Capacity Factor: 1.5)\nAcross Device\nCommunication\nDevice 0 Device 1 Device 2 Device 1\nDevice 0\nTerminology\nExperts: Split across devices,\neach having their own unique\nparameters. Perform standard feed-\nforward computation.\nExpert Capacity: Batch size of\neach expert. Calculated as \n(tokens_per_batch / num_experts) *\ncapacity_factor\nCapacity Factor: Used when\ncalculating expert capacity. Expert\ncapacity allows more buffer to help\nmitigate token overﬂow during\nrouting. \nFigure 3: Illustration of token routing dynamics. Each expert processes a ﬁxed batch-size\nof tokens modulated by the capacity factor. Each token is routed to the expert\nwith the highest router probability, but each expert has a ﬁxed batch size of\n(total tokens / num experts) ×capacity factor. If the tokens are unevenly dis-\npatched then certain experts will overﬂow (denoted by dotted red lines), resulting\nin these tokens not being processed by this layer. A larger capacity factor allevi-\nates this overﬂow issue, but also increases computation and communication costs\n(depicted by padded white/empty slots).\n2.2 Eﬃcient Sparse Routing\nWe use Mesh-Tensorﬂow (MTF) (Shazeer et al., 2018) which is a library, with similar seman-\ntics and API to Tensorﬂow (Abadi et al., 2016) that facilitates eﬃcient distributed data and\nmodel parallel architectures. It does so by abstracting the physical set of cores to a logical\nmesh of processors. Tensors and computations may then be sharded per named dimensions,\nfacilitating easy partitioning of models across dimensions. We design our model with TPUs\nin mind, which require statically declared sizes. Below we describe our distributed Switch\nTransformer implementation.\n3. See Section 2.2 for a technical description.\n6\nSwitch Transformers\nDistributed Switch Implementation. All of our tensor shapes are statically deter-\nmined at compilation time, but our computation is dynamic due to the routing decisions at\ntraining and inference. Because of this, one important technical consideration is how to set\nthe expert capacity. The expert capacity—the number of tokens each expert computes—is\nset by evenly dividing the number of tokens in the batch across the number of experts, and\nthen further expanding by a capacity factor,\nexpert capacity =\n( tokens per batch\nnumber of experts\n)\n×capacity factor. (3)\nA capacity factor greater than 1.0 creates additional buﬀer to accommodate for when to-\nkens are not perfectly balanced across experts. If too many tokens are routed to an expert\n(referred to later as dropped tokens), computation is skipped and the token representa-\ntion is passed directly to the next layer through the residual connection. Increasing the\nexpert capacity is not without drawbacks, however, since high values will result in wasted\ncomputation and memory. This trade-oﬀ is explained in Figure 3. Empirically we ﬁnd en-\nsuring lower rates of dropped tokens are important for the scaling of sparse expert-models.\nThroughout our experiments we didn’t notice any dependency on the number of experts\nfor the number of tokens dropped (typically <1%). Using the auxiliary load balancing loss\n(next section) with a high enough coeﬃcient ensured good load balancing. We study the\nimpact that these design decisions have on model quality and speed in Table 1.\nA Diﬀerentiable Load Balancing Loss. To encourage a balanced load across experts\nwe add an auxiliary loss (Shazeer et al., 2017, 2018; Lepikhin et al., 2020). As in Shazeer\net al. (2018); Lepikhin et al. (2020), Switch Transformers simpliﬁes the original design in\nShazeer et al. (2017) which had separate load-balancing and importance-weighting losses.\nFor each Switch layer, this auxiliary loss is added to the total model loss during training.\nGiven N experts indexed by i= 1 to N and a batch Bwith T tokens, the auxiliary loss is\ncomputed as the scaled dot-product between vectors f and P,\nloss = α·N ·\nN∑\ni=1\nfi ·Pi (4)\nwhere fi is the fraction of tokens dispatched to expert i,\nfi = 1\nT\n∑\nx∈B\n1 {argmax p(x) = i} (5)\nand Pi is the fraction of the router probability allocated for expert i, 2\nPi = 1\nT\n∑\nx∈B\npi(x). (6)\nSince we seek uniform routing of the batch of tokens across the N experts, we desire both\nvectors to have values of 1/N. The auxiliary loss of Equation 4 encourages uniform routing\nsince it is minimized under a uniform distribution. The objective can also be diﬀerentiated as\n2. A potential source of confusion: pi(x) is the probability of routing token x to expert i. Pi is the\nprobability fraction to expert i across all tokens in the batch B.\n7\nFedus, Zoph and Shazeer\nthe P-vector is diﬀerentiable, but the f-vector is not. The ﬁnal loss is multiplied by expert\ncount N to keep the loss constant as the number of experts varies since under uniform\nrouting ∑N\ni=1(fi ·Pi) = ∑N\ni=1( 1\nN ·1\nN ) = 1\nN . Finally, a hyper-parameter αis a multiplicative\ncoeﬃcient for these auxiliary losses; throughout this work we use an α = 10−2 which was\nsuﬃciently large to ensure load balancing while small enough to not to overwhelm the\nprimary cross-entropy objective. We swept hyper-parameter ranges of αfrom 10−1 to 10−5\nin powers of 10 and found 10−2 balanced load quickly without interfering with training loss.\n2.3 Putting It All Together: The Switch Transformer\nOur ﬁrst test of the Switch Transformer starts with pre-training on the “Colossal Clean\nCrawled Corpus” (C4), introduced in (Raﬀel et al., 2019). For our pre-training objective,\nwe use a masked language modeling task (Taylor, 1953; Fedus et al., 2018; Devlin et al.,\n2018) where the model is trained to predict missing tokens. In our pre-training setting, as\ndetermined in Raﬀel et al. (2019) to be optimal, we drop out 15% of tokens and then replace\nthe masked sequence with a single sentinel token. To compare our models, we record the\nnegative log perplexity.4 Throughout all tables in the paper, ↑indicates that a higher value\nfor that metric is better and vice-versa for ↓. A comparison of all the models studied in\nthis work are in Table 9.\nA head-to-head comparison of the Switch Transformer and the MoE Transformer is\npresented in Table 1. Our Switch Transformer model is FLOP-matched to ‘T5-Base’ (Raﬀel\net al., 2019) (same amount of computation per token is applied). The MoE Transformer,\nusing top-2 routing, has two experts which each apply a separate FFN to each token and\nthus its FLOPS are larger. All models were trained for the same number of steps on identical\nhardware. Note that the MoE model going from capacity factor 2.0 to 1.25 actually slows\ndown (840 to 790) in the above experiment setup, which is unexpected. 5\nWe highlight three key ﬁndings from Table 1: (1) Switch Transformers outperform\nboth carefully tuned dense models and MoE Transformers on a speed-quality basis. For\na ﬁxed amount of computation and wall-clock time, Switch Transformers achieve the best\nresult. (2) The Switch Transformer has a smaller computational footprint than the MoE\ncounterpart. If we increase its size to match the training speed of the MoE Transformer,\nwe ﬁnd this outperforms all MoE and Dense models on a per step basis as well. (3) Switch\nTransformers perform better at lower capacity factors (1.0, 1.25). Smaller expert capacities\nare indicative of the scenario in the large model regime where model memory is very scarce\nand the capacity factor will want to be made as small as possible.\n2.4 Improved Training and Fine-Tuning Techniques\nSparse expert models may introduce training diﬃculties over a vanilla Transformer. Insta-\nbility can result because of the hard-switching (routing) decisions at each of these layers.\nFurther, low precision formats like bﬂoat16 (Wang and Kanwar, 2019) can exacerbate issues\n4. We use log base- e for this metric so the units are nats.\n5. Note that speed measurements are both a function of the algorithm and the implementation details.\nSwitch Transformer reduces the necessary computation relative to MoE (algorithm), but the ﬁnal speed\ndiﬀerences are impacted by low-level optimizations (implementation).\n8\nSwitch Transformers\nModel Capacity Quality after Time to Quality Speed ( ↑)\nFactor 100k steps ( ↑) Threshold ( ↓) (examples/sec)\n(Neg. Log Perp.) (hours)\nT5-Base — -1.731 Not achieved † 1600\nT5-Large — -1.550 131.1 470\nMoE-Base 2.0 -1.547 68.7 840\nSwitch-Base 2.0 -1.554 72.8 860\nMoE-Base 1.25 -1.559 80.7 790\nSwitch-Base 1.25 -1.553 65.0 910\nMoE-Base 1.0 -1.572 80.1 860\nSwitch-Base 1.0 -1.561 62.8 1000\nSwitch-Base+ 1.0 -1.534 67.6 780\nTable 1: Benchmarking Switch versus MoE. Head-to-head comparison measuring per step\nand per time beneﬁts of the Switch Transformer over the MoE Transformer and\nT5 dense baselines. We measure quality by the negative log perplexity and the\ntime to reach an arbitrary chosen quality threshold of Neg. Log Perp.=-1.50. All\nMoE and Switch Transformer models use 128 experts, with experts at every other\nfeed-forward layer. For Switch-Base+, we increase the model size until it matches\nthe speed of the MoE model by increasing the model hidden-size from 768 to 896\nand the number of heads from 14 to 16. All models are trained with the same\namount of computation (32 cores) and on the same hardware (TPUv3). Further\nnote that all our models required pre-training beyond 100k steps to achieve our\nlevel threshold of -1.50. †T5-Base did not achieve this negative log perplexity in\nthe 100k steps the models were trained.\nin the softmax computation for our router. We describe training diﬃculties here and the\nmethods we use to overcome them to achieve stable and scalable training.\nSelective precision with large sparse models. Model instability hinders the ability\nto train using eﬃcient bﬂoat16 precision, and as a result, Lepikhin et al. (2020) trains with\nﬂoat32 precision throughout their MoE Transformer. However, we show that by instead\nselectively casting to ﬂoat32 precision within a localized part of the model, stability may be\nachieved, without incurring expensive communication cost of ﬂoat32 tensors. This technique\nis inline with modern mixed precision training strategies where certain parts of the model\nand gradient updates are done in higher precision Micikevicius et al. (2017). Table 2 shows\nthat our approach permits nearly equal speed to bﬂoat16 training while conferring the\ntraining stability of ﬂoat32.\nTo achieve this, we cast the router input to ﬂoat32 precision. The router function takes\nthe tokens as input and produces the dispatch and combine tensors used for the selection and\nrecombination of expert computation (refer to Code Block 15 in the Appendix for details).\nImportantly, the ﬂoat32 precision is only used within the body of the router function—on\ncomputations local to that device. Because the resulting dispatch and combine tensors\nare recast to bﬂoat16 precision at the end of the function, no expensive ﬂoat32 tensors\n9\nFedus, Zoph and Shazeer\nModel Quality Speed\n(precision) (Neg. Log Perp.) ( ↑) (Examples/sec) ( ↑)\nSwitch-Base (ﬂoat32) -1.718 1160\nSwitch-Base (bﬂoat16) -3.780 [ diverged] 1390\nSwitch-Base (Selective precision) -1.716 1390\nTable 2: Selective precision. We cast the local routing operations to ﬂoat32 while preserving\nbﬂoat16 precision elsewhere to stabilize our model while achieving nearly equal\nspeed to (unstable) bﬂoat16-precision training. We measure the quality of a 32\nexpert model after a ﬁxed step count early in training its speed performance. For\nboth Switch-Base in ﬂoat32 and with Selective prevision we notice similar learning\ndynamics.\nare broadcast through all-to-all communication operations, but we still beneﬁt from the\nincreased stability of ﬂoat32.\nSmaller parameter initialization for stability. Appropriate initialization is critical\nto successful training in deep learning and we especially observe this to be true for Switch\nTransformer. We initialize our weight matrices by drawing elements from a truncated\nnormal distribution with mean µ= 0 and standard deviation σ=\n√\ns/n where s is a scale\nhyper-parameter and n is the number of input units in the weight tensor (e.g. fan-in). 6\nAs an additional remedy to the instability, we recommend reducing the default Trans-\nformer initialization scale s= 1.0 by a factor of 10. This both improves quality and reduces\nthe likelihood of destabilized training in our experiments. Table 3 measures the improve-\nment of the model quality and reduction of the variance early in training. We ﬁnd that\nModel (Initialization scale) Average Quality Std. Dev. of Quality\n(Neg. Log Perp.) (Neg. Log Perp.)\nSwitch-Base (0.1x-init) -2.72 0.01\nSwitch-Base (1.0x-init) -3.60 0.68\nTable 3: Reduced initialization scale improves stability. Reducing the initialization scale\nresults in better model quality and more stable training of Switch Transformer.\nHere we record the average and standard deviation of model quality, measured by\nthe negative log perplexity, of a 32 expert model after 3.5k steps (3 random seeds\neach).\nthe average model quality, as measured by the Neg. Log Perp., is dramatically improved\nand there is a far reduced variance across runs. Further, this same initialization scheme is\nbroadly eﬀective for models spanning several orders of magnitude. We use the same ap-\nproach to stably train models as small as our 223M parameter baseline to enormous models\nin excess of one trillion parameters.\n6. Values greater than two standard deviations from the mean are resampled.\n10\nSwitch Transformers\nRegularizing large sparse models. Our paper considers the common NLP approach\nof pre-training on a large corpus followed by ﬁne-tuning on smaller downstream tasks such\nas summarization or question answering. One issue that naturally arises is overﬁtting since\nmany ﬁne-tuning tasks have very few examples. During ﬁne-tuning of standard Trans-\nformers, Raﬀel et al. (2019) use dropout (Srivastava et al., 2014) at each layer to prevent\noverﬁtting. Our Switch Transformers have signiﬁcantly more parameters than the FLOP\nmatched dense baseline, which can lead to more severe overﬁtting on these smaller down-\nstream tasks.\nModel (dropout) GLUE CNNDM SQuAD SuperGLUE\nT5-Base (d=0.1) 82.9 19.6 83.5 72.4\nSwitch-Base (d=0.1) 84.7 19.1 83.7 73.0\nSwitch-Base (d=0.2) 84.4 19.2 83.9 73.2\nSwitch-Base (d=0.3) 83.9 19.6 83.4 70.7\nSwitch-Base (d=0.1, ed=0.4) 85.2 19.6 83.7 73.0\nTable 4: Fine-tuning regularization results. A sweep of dropout rates while ﬁne-tuning\nSwitch Transformer models pre-trained on 34B tokens of the C4 data set (higher\nnumbers are better). We observe that using a lower standard dropout rate at\nall non-expert layer, with a much larger dropout rate on the expert feed-forward\nlayers, to perform the best.\nWe thus propose a simple way to alleviate this issue during ﬁne-tuning: increase the\ndropout inside the experts, which we name as expert dropout. During ﬁne-tuning we simply\nincrease the dropout rate by a signiﬁcant amount only at the interim feed-forward com-\nputation at each expert layer. Table 4 has the results for our expert dropout protocol.\nWe observe that simply increasing the dropout across all layers leads to worse performance.\nHowever, setting a smaller dropout rate (0.1) at non-expert layers and a much larger dropout\nrate (0.4) at expert layers leads to performance improvements on four smaller downstream\ntasks.\n3. Scaling Properties\nWe present a study of the scaling properties of the Switch Transformer architecture dur-\ning pre-training. Per Kaplan et al. (2020), we consider a regime where the model is not\nbottlenecked by either the computational budget or amount of data. To avoid the data\nbottleneck, we use the large C4 corpus with over 180B target tokens (Raﬀel et al., 2019)\nand we train until diminishing returns are observed.\nThe number of experts is the most eﬃcient dimension for scaling our model. Increasing\nthe experts keeps the computational cost approximately ﬁxed since the model only selects\none expert per token, regardless of the number of experts to choose from. The router\nmust compute a probability distribution over more experts, however, this is a lightweight\ncomputation of cost O(dmodel ×num experts) where dmodel is the embedding dimension of\n11\nFedus, Zoph and Shazeer\ntokens passed between the layers. In this section, we consider the scaling properties on a\nstep-basis and a time-basis with a ﬁxed computational budget.\n3.1 Scaling Results on a Step-Basis\nFigure 4 demonstrates consistent scaling beneﬁts with the number of experts when training\nall models for a ﬁxed number of steps. We observe a clear trend: when keeping the FLOPS\nper token ﬁxed, having more parameters (experts) speeds up training. The left Figure\ndemonstrates consistent scaling properties (with ﬁxed FLOPS per token) between sparse\nmodel parameters and test loss. This reveals the advantage of scaling along this additional\naxis of sparse model parameters. Our right Figure measures sample eﬃciency of a dense\nmodel variant and four FLOP-matched sparse variants. We ﬁnd that increasing the number\nof experts leads to more sample eﬃcient models. Our Switch-Base 64 expert model achieves\nthe same performance of the T5-Base model at step 60k at step 450k, which is a 7.5x\nspeedup in terms of step time. In addition, consistent with the ﬁndings of Kaplan et al.\n(2020), we ﬁnd that larger models are also more sample eﬃcient —learning more quickly\nfor a ﬁxed number of observed tokens.\n109 1010\nSparse Model Parameters\n4.8\n5.0\n5.2\n5.4\n5.6\n5.8\n6.0Test Loss\n1e\n2e\n4e\n8e\n16e\n32e\n64e\n128e\n256e\n0 1 2 3 4\nTraining Step 1e5\n2.0\n1.9\n1.8\n1.7\n1.6\n1.5\n1.4\n1.3\n1.2\nNeg Log Perplexity\nSwitch-Base: 128e\nSwitch-Base: 64e\nSwitch-Base: 32e\nSwitch-Base: 16e\nT5-Base\nFigure 4: Scaling properties of the Switch Transformer. Left Plot: We measure the quality\nimprovement, as measured by perplexity, as the parameters increase by scaling\nthe number of experts. The top-left point corresponds to the T5-Base model with\n223M parameters. Moving from top-left to bottom-right, we double the number of\nexperts from 2, 4, 8 and so on until the bottom-right point of a 256 expert model\nwith 14.7B parameters. Despite all models using an equal computational budget,\nwe observe consistent improvements scaling the number of experts. Right Plot:\nNegative log perplexity per step sweeping over the number of experts. The dense\nbaseline is shown with the purple line and we note improved sample eﬃciency of\nour Switch-Base models.\n12\nSwitch Transformers\n3.2 Scaling Results on a Time-Basis\nFigure 4 demonstrates that on a step basis, as we increase the number of experts, the\nperformance consistently improves. While our models have roughly the same amount of\nFLOPS per token as the baseline, our Switch Transformers incurs additional communication\ncosts across devices as well as the extra computation of the routing mechanism. Therefore,\nthe increased sample eﬃciency observed on a step-basis doesn’t necessarily translate to a\nbetter model quality as measured by wall-clock. This raises the question:\nFor a ﬁxed training duration and computational budget, should one train a dense or a\nsparse model?\n50 100 150 200 250 300 350\nTraining Time\n2.0\n1.9\n1.8\n1.7\n1.6\n1.5\n1.4\n1.3\n1.2\nNeg Log Perplexity\n7x Speedup\nSwitch-Base: 128e\nSwitch-Base: 64e\nSwitch-Base: 32e\nT5-Base\nFigure 5: Speed advantage of Switch Transformer. All models trained on 32 TPUv3 cores\nwith equal FLOPs per example. For a ﬁxed amount of computation and training\ntime, Switch Transformers signiﬁcantly outperform the dense Transformer base-\nline. Our 64 expert Switch-Base model achieves the same quality in one-seventh\nthe time of the T5-Base and continues to improve.\nFigures 5 and 6 address this question. Figure 5 measures the pre-training model quality\nas a function of time. For a ﬁxed training duration and computational budget, Switch\nTransformers yield a substantial speed-up. In this setting, our Switch-Base 64 expert model\ntrains in one-seventh the time that it would take the T5-Base to get similar perplexity.\n3.3 Scaling Versus a Larger Dense Model\nThe above analysis shows that a computationally-matched dense model is outpaced by its\nSwitch counterpart. Figure 6 considers a diﬀerent scenario: what if we instead had allocated\nour resources to a larger dense model? We do so now, measuring Switch-Base against the\nnext strong baseline, T5-Large. But despite T5-Large applying 3.5x more FLOPs per token,\n13\nFedus, Zoph and Shazeer\nSwitch-Base is still more sample eﬃcient and yields a 2.5x speedup. Furthermore, more\ngains can be had simply by designing a new, larger sparse version, Switch-Large, which is\nFLOP-matched to T5-Large. We do this and demonstrate superior scaling and ﬁne-tuning\nin the following section.\n0 1 2 3 4\nTraining Step 1e5\n2.0\n1.9\n1.8\n1.7\n1.6\n1.5\n1.4\n1.3\n1.2\nNeg Log Perplexity\nSwitch-Base: 64e\nT5-Large\nT5-Base\n50 100 150 200 250 300 350\nTraining Time\n2.0\n1.9\n1.8\n1.7\n1.6\n1.5\n1.4\n1.3\n1.2\nNeg Log Perplexity\n7.0x Speedup\n2.5x Speedup\nSwitch-Base: 64e\nT5-Large\nT5-Base\nFigure 6: Scaling Transformer models with Switch layers or with standard dense model\nscaling. Left Plot: Switch-Base is more sample eﬃcient than both the T5-Base,\nand T5-Large variant, which applies 3.5x more FLOPS per token. Right Plot: As\nbefore, on a wall-clock basis, we ﬁnd that Switch-Base is still faster, and yields a\n2.5x speedup over T5-Large.\n4. Downstream Results\nSection 3 demonstrated the superior scaling properties while pre-training, but we now val-\nidate that these gains translate to improved language learning abilities on downstream\ntasks. We begin by ﬁne-tuning on a diverse set of NLP tasks. Next we study reducing\nthe memory footprint of our sparse models by over 90% by distilling into small—and easily\ndeployed—dense baselines. Finally, we conclude this section measuring the improvements\nin a multi-task, multilingual setting, where we show that Switch Transformers are strong\nmulti-task learners, improving over the multilingual T5-base model across all 101 languages.\n4.1 Fine-Tuning\nBaseline and Switch models used for ﬁne-tuning. Our baselines are the highly-tuned\n223M parameter T5-Base model and the 739M parameter T5-Large model (Raﬀel et al.,\n2019). For both versions, we design a FLOP-matched Switch Transformer, with many more\nparameters, which is summarized in Table 9. 7 Our baselines diﬀer slightly from those in\nRaﬀel et al. (2019) because we pre-train on an improved C4 corpus which removes intra-\nexample text duplication and thus increases the eﬃcacy as a pre-training task Lee et al.\n7. FLOPS are calculated for the forward pass as done in Kaplan et al. (2020).\n14\nSwitch Transformers\n(2021). In our protocol we pre-train with 2 20 (1,048,576) tokens per batch for 550k steps\namounting to 576B total tokens. We then ﬁne-tune across a diverse set of tasks using a\ndropout rate of 0.1 for all layers except the Switch layers, which use a dropout rate of 0.4\n(see Table 4). We ﬁne-tune using a batch-size of 1M for 16k steps and for each task, we\nevaluate model quality every 200-steps and report the peak performance as computed on\nthe validation set.\nFine-tuning tasks and data sets. We select tasks probing language capabilities in-\ncluding question answering, summarization and knowledge about the world. The language\nbenchmarks GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) are handled\nas composite mixtures with all the tasks blended in proportion to the amount of tokens\npresent in each. These benchmarks consist of tasks requiring sentiment analysis (SST-\n2), word sense disambiguation (WIC), sentence similarty (MRPC, STS-B, QQP), natural\nlanguage inference (MNLI, QNLI, RTE, CB), question answering (MultiRC, RECORD,\nBoolQ), coreference resolution (WNLI, WSC) and sentence completion (COPA) and sen-\ntence acceptability (CoLA). The CNNDM (Hermann et al., 2015) and BBC XSum (Narayan\net al., 2018) data sets are used to measure the ability to summarize articles. Question an-\nswering is probed with the SQuAD data set (Rajpurkar et al., 2016) and the ARC Reasoning\nChallenge (Clark et al., 2018). And as in Roberts et al. (2020), we evaluate the knowledge\nof our models by ﬁne-tuning on three closed-book question answering data sets: Natural\nQuestions (Kwiatkowski et al., 2019), Web Questions (Berant et al., 2013) and Trivia QA\n(Joshi et al., 2017). Closed-book refers to questions posed with no supplemental reference\nor context material. To gauge the model’s common sense reasoning we evaluate it on the\nWinogrande Schema Challenge (Sakaguchi et al., 2020). And ﬁnally, we test our model’s\nnatural language inference capabilities on the Adversarial NLI Benchmark (Nie et al., 2019).\nFine-tuning metrics. The following evaluation metrics are used throughout the paper:\nWe report the average scores across all subtasks for GLUE and SuperGLUE. The Rouge-2\nmetric is used both the CNNDM and XSum. In SQuAD and the closed book tasks (Web,\nNatural, and Trivia Questions) we report the percentage of answers exactly matching the\ntarget (refer to Roberts et al. (2020) for further details and deﬁciency of this measure).\nFinally, in ARC Easy, ARC Challenge, ANLI, and Winogrande we report the accuracy of\nthe generated responses.\nFine-tuning results. We observe signiﬁcant downstream improvements across many\nnatural language tasks. Notable improvements come from SuperGLUE, where we ﬁnd\nFLOP-matched Switch variants improve by 4.4 and 2 percentage points over the T5-Base\nand T5-Large baselines, respectively as well as large improvements in Winogrande, closed\nbook Trivia QA, and XSum. 8 In our ﬁne-tuning study, the only tasks where we do not\nobserve gains are on the AI2 Reasoning Challenge (ARC) data sets where the T5-Base\noutperforms Switch-Base on the challenge data set and T5-Large outperforms Switch-Large\non the easy data set. Taken as a whole, we observe signiﬁcant improvements spanning both\nreasoning and knowledge-heavy tasks. This validates our architecture, not just as one that\npre-trains well, but can translate quality improvements to downstream tasks via ﬁne-tuning.\n8. Our T5 and Switch models were pre-trained with 2 20 tokens per batch for 550k steps on a revised C4\ndata set for fair comparisons.\n15\nFedus, Zoph and Shazeer\nModel GLUE SQuAD SuperGLUE Winogrande (XL)\nT5-Base 84.3 85.5 75.1 66.6\nSwitch-Base 86.7 87.2 79.5 73.3\nT5-Large 87.8 88.1 82.7 79.1\nSwitch-Large 88.5 88.6 84.7 83.0\nModel XSum ANLI (R3) ARC Easy ARC Chal.\nT5-Base 18.7 51.8 56.7 35.5\nSwitch-Base 20.3 54.0 61.3 32.8\nT5-Large 20.9 56.6 68.8 35.5\nSwitch-Large 22.3 58.6 66.0 35.5\nModel CB Web QA CB Natural QA CB Trivia QA\nT5-Base 26.6 25.8 24.5\nSwitch-Base 27.4 26.8 30.7\nT5-Large 27.7 27.6 29.5\nSwitch-Large 31.3 29.5 36.9\nTable 5: Fine-tuning results. Fine-tuning results of T5 baselines and Switch models across\na diverse set of natural language tests (validation sets; higher numbers are better).\nWe compare FLOP-matched Switch models to the T5-Base and T5-Large base-\nlines. For most tasks considered, we ﬁnd signiﬁcant improvements of the Switch-\nvariants. We observe gains across both model sizes and across both reasoning and\nknowledge-heavy language tasks.\n4.2 Distillation\nDeploying massive neural networks with billions, or trillions, of parameters is inconvenient.\nTo alleviate this, we study distilling (Hinton et al., 2015) large sparse models into small\ndense models. Future work could additionally study distilling large models into smaller\nsparse models.\nDistillation techniques. In Table 6 we study a variety of distillation techniques.\nThese techniques are built oﬀ of Sanh et al. (2019), who study distillation methods for\nBERT models. We ﬁnd that initializing the dense model with the non-expert weights yields\na modest improvement. This is possible since all models are FLOP matched, so non-expert\nlayers will have the same dimensions. Since expert layers are usually only added at every\nor every other FFN layer in a Transformer, this allows for many of the weights to be\ninitialized with trained parameters. Furthermore, we observe a distillation improvement\nusing a mixture of 0.25 for the teacher probabilities and 0.75 for the ground truth label. By\ncombining both techniques we preserve ≈30% of the quality gains from the larger sparse\nmodels with only ≈1/20th of the parameters. The quality gain refers to the percent of\n16\nSwitch Transformers\nthe quality diﬀerence between Switch-Base (Teacher) and T5-Base (Student). Therefore, a\nquality gain of 100% implies the Student equals the performance of the Teacher.\nTechnique Parameters Quality ( ↑)\nT5-Base 223M -1.636\nSwitch-Base 3,800M -1.444\nDistillation 223M (3%) -1.631\n+ Init. non-expert weights from teacher 223M (20%) -1.598\n+ 0.75 mix of hard and soft loss 223M (29%) -1.580\nInitialization Baseline (no distillation)\nInit. non-expert weights from teacher 223M -1.639\nTable 6: Distilling Switch Transformers for Language Modeling. Initializing T5-Base with\nthe non-expert weights from Switch-Base and using a loss from a mixture of teacher\nand ground-truth labels obtains the best performance. We can distill 30% of the\nperformance improvement of a large sparse model with 100x more parameters back\ninto a small dense model. For a ﬁnal baseline, we ﬁnd no improvement of T5-Base\ninitialized with the expert weights, but trained normally without distillation.\nAchievable compression rates. Using our best distillation technique described in\nTable 6, we distill a wide variety of sparse models into dense models. We distill Switch-\nBase versions, sweeping over an increasing number of experts, which corresponds to varying\nbetween 1.1B to 14.7B parameters. Through distillation, we can preserve 37% of the quality\ngain of the 1.1B parameter model while compressing 82%. At the extreme, where we\ncompress the model 99%, we are still able to maintain 28% of the teacher’s model quality\nimprovement.\nDistilling a ﬁne-tuned model. We conclude this with a study of distilling a ﬁne-\ntuned sparse model into a dense model. Table 8 shows results of distilling a 7.4B parameter\nSwitch-Base model, ﬁne-tuned on the SuperGLUE task, into the 223M T5-Base. Similar\nto our pre-training results, we ﬁnd we are able to preserve 30% of the gains of the sparse\nmodel when distilling into a FLOP matched dense variant. One potential future avenue,\nnot considered here, may examine the speciﬁc experts being used for ﬁne-tuning tasks and\nextracting them to achieve better model compression.\n4.3 Multilingual Learning\nIn our ﬁnal set of downstream experiments, we measure the model quality and speed trade-\noﬀs while pre-training on a mixture of 101 diﬀerent languages. We build and benchmark oﬀ\nthe recent work of mT5 (Xue et al., 2020), a multilingual extension to T5. We pre-train on\nthe multilingual variant of the Common Crawl data set (mC4) spanning 101 languages in-\ntroduced in mT5, but due to script variants within certain languages, the mixture contains\n107 tasks.\nIn Figure 7 we plot the quality improvement in negative log perplexity for all languages\nof a FLOP-matched Switch model, mSwitch-Base to the T5 base variant, mT5-Base. After\n17\nFedus, Zoph and Shazeer\nDense Sparse\nParameters 223M 1.1B 2.0B 3.8B 7.4B 14.7B\nPre-trained Neg. Log Perp. ( ↑) -1.636 -1.505 -1.474 -1.444 -1.432 -1.427\nDistilled Neg. Log Perp. ( ↑) — -1.587 -1.585 -1.579 -1.582 -1.578\nPercent of Teacher Performance — 37% 32% 30 % 27 % 28 %\nCompression Percent — 82 % 90 % 95 % 97 % 99 %\nTable 7: Distillation compression rates. We measure the quality when distilling large sparse\nmodels into a dense baseline. Our baseline, T5-Base, has a -1.636 Neg. Log Perp.\nquality. In the right columns, we then distill increasingly large sparse models\ninto this same architecture. Through a combination of weight-initialization and\na mixture of hard and soft losses, we can shrink our sparse teachers by 95%+\nwhile preserving 30% of the quality gain. However, for signiﬁcantly better and\nlarger pre-trained teachers, we expect larger student models would be necessary\nto achieve these compression rates.\nModel Parameters FLOPS SuperGLUE (↑)\nT5-Base 223M 124B 74.6\nSwitch-Base 7410M 124B 81.3\nDistilled T5-Base 223M 124B (30%) 76.6\nTable 8: Distilling a ﬁne-tuned SuperGLUE model. We distill a Switch-Base model ﬁne-\ntuned on the SuperGLUE tasks into a T5-Base model. We observe that on smaller\ndata sets our large sparse model can be an eﬀective teacher for distillation. We\nﬁnd that we again achieve 30% of the teacher’s performance on a 97% compressed\nmodel.\npre-training both versions for 1M steps, we ﬁnd that on all 101 languages considered,\nSwitch Transformer increases the ﬁnal negative log perplexity over the baseline. In Figure\n8, we present a diﬀerent view and now histogram the per step speed-up of using Switch\nTransformer over the mT5-Base. 9 We ﬁnd a mean speed-up over mT5-Base of 5x and\nthat 91% of languages achieve at least a 4x speedup. This presents evidence that Switch\nTransformers are eﬀective multi-task and multi-lingual learners.\n5. Designing Models with Data, Model, and Expert-Parallelism\nArbitrarily increasing the number of experts is subject to diminishing returns (Figure 4).\nHere we describe complementary scaling strategies. The common way to scale a Transformer\nis to increase dimensions in tandem, like dmodel or dff . This increases both the parameters\n9. The speedup on a step basis is computed as the ratio of the number of steps for the baseline divided by\nthe number of steps required by our model to reach that same quality.\n18\nSwitch Transformers\nja\nny\nxh\nmr\nsu\nzh\neo\nbg-latn\nms\nzu\nso\net\nta\nla\nhi-latn\nha\nsn\nht\nmy\njv\naf\nmi\nfi\nfil\nsw\nno\neu\nlo\nsv\nyo\nde\nen\nth\nco\ntr\nml\nfy\nes\nar\nsl\nmn\nky\nsi\niw\nhu\nis\nko\nja-latn\nro\nuz\nzh-latn\nte\nam\nkm\nkk\nku\nnl\nst\nit\nda\nmg\nlt\nsr\nsq\nmt\ngl\nhi\ncs\nhmn\nig\nhaw\nlv\nfr\nsm\nru\nga\nsk\nur\ntg\nyi\npt\naz\nps\nbg\nru-latn\nca\npl\nmk\nid\nkn\nne\nfa\nbe\nka\nel-latn\nuk\ngu\nbn\ncy\nhy\nlb\npa\nel\nceb\nvi\nsd\ngd\nLanguage\n1.8\n1.6\n1.4\n1.2\n1.0\n0.8\n0.6\n0.4\nNeg. Log Perplexity\nSwitch\nDense\nFigure 7: Multilingual pre-training on 101 languages. Improvements of Switch T5 Base\nmodel over dense baseline when multi-task training on 101 languages. We observe\nSwitch Transformers to do quite well in the multi-task training setup and yield\nimprovements on all 101 languages.\n4 6 8 10 12 14 16\nSwitch Speedup over Dense Baseline\n0\n10\n20\n30\n40\n50Number of Languages\nFigure 8: Multilingual pre-training on 101 languages. We histogram for each language, the\nstep speedup of Switch Transformers over the FLOP matched T5 dense baseline\nto reach the same quality. Over all 101 languages, we achieve a mean step speed-\nup over mT5-Base of 5x and, for 91% of languages, we record a 4x, or greater,\nspeedup to reach the ﬁnal perplexity of mT5-Base.\nand computation performed and is ultimately limited by the memory per accelerator. Once\nit exceeds the size of the accelerator’s memory, single program multiple data (SPMD) model-\nparallelism can be employed. This section studies the trade-oﬀs of combining data, model,\nand expert-parallelism.\nReviewing the Feed-Forward Network (FFN) Layer. We use the FFN layer as\nan example of how data, model and expert-parallelism works in Mesh TensorFlow (Shazeer\net al., 2018) and review it brieﬂy here. We assume B tokens in the batch, each of dimension\n19\nFedus, Zoph and Shazeer\ndmodel. Both the input ( x) and output ( y) of the FFN are of size [ B, dmodel] and the inter-\nmediate (h) is of size [B, dff ] where dff is typically several times larger than dmodel. In the\nFFN, the intermediate is h= xWin and then the output of the layer is y= ReLU(h)Wout.\nThus Win and Wout are applied independently to each token and have sizes [ dmodel, dff ]\nand [dff , dmodel].\nWe describe two aspects of partitioning: how the weights and batches of data divide\nover cores, depicted in Figure 9. We denote all cores available as N which Mesh Tensorﬂow\nmay then remap into a logical multidimensional mesh of processors. Here we create a\ntwo-dimensional logical mesh, with one dimension representing the number of ways for\ndata-parallel sharding (n) and the other, the model-parallel sharding ( m). The total cores\nmust equal the ways to shard across both data and model-parallelism, e.g. N = n×m.\nTo shard the layer across cores, the tensors containing that batch of B tokens are sharded\nacross n data-parallel cores, so each core contains B/n tokens. Tensors and variables with\ndff are then sharded across m model-parallel cores. For the variants with experts-layers,\nwe consider E experts, each of which can process up to C tokens.\nTerm Description\nB Number of tokens in the batch.\nN Number of total cores.\nn Number of ways for data-parallelism sharding.\nm Number of ways for model-parallelism sharding.\nE Number of experts in Switch layers.\nC Expert capacity, the batch size of each expert.\n5.1 Data Parallelism\nWhen training data parallel models, which is the standard for distributed training, then all\ncores are allocated to the data-parallel dimension or n= N,m = 1. This has the advantage\nthat no communication is needed until the entire forward and backward pass is ﬁnished and\nthe gradients need to be then aggregated across all cores. This corresponds to the left-most\ncolumn of Figure 9.\n5.2 Model Parallelism\nWe now consider a scenario where all cores are allocated exclusively to the model-parallel\ndimension and so n = 1 ,m = N. Now all cores must keep the full B tokens and each\ncore will contain a unique slice of the weights. For each forward and backward pass, a\ncommunication cost is now incurred. Each core sends a tensor of [B, dmodel] to compute the\nsecond matrix multiplication ReLU(h)Wout because the dff dimension is partitioned and\nmust be summed over. As a general rule, whenever a dimension that is partitioned across\ncores must be summed, then an all-reduce operation is added for both the forward and\nbackward pass. This contrasts with pure data parallelism where an all-reduce only occurs\nat the end of the entire forward and backward pass.\n20\nSwitch Transformers\nExpert and Data\nParallelism\nModel \nParallelism\nExpert, Model and Data\nParallelism\nHow the model weights are split over cores\nHow the data is split over cores\nModel and Data \nParallelism\nData \nParallelism\nExpert and Data\nParallelism\nModel \nParallelism\nExpert, Model and Data\nParallelism\nModel and Data \nParallelism\nData \nParallelism\nFigure 9: Data and weight partitioning strategies. Each 4 ×4 dotted-line grid represents 16\ncores and the shaded squares are the data contained on that core (either model\nweights or batch of tokens). We illustrate both how the model weights and the\ndata tensors are split for each strategy. First Row: illustration of how model\nweights are split across the cores. Shapes of diﬀerent sizes in this row represent\nlarger weight matrices in the Feed Forward Network (FFN) layers (e.g larger dff\nsizes). Each color of the shaded squares identiﬁes a unique weight matrix. The\nnumber of parameters per core is ﬁxed, but larger weight matrices will apply\nmore computation to each token. Second Row: illustration of how the data\nbatch is split across cores. Each core holds the same number of tokens which\nmaintains a ﬁxed memory usage across all strategies. The partitioning strategies\nhave diﬀerent properties of allowing each core to either have the same tokens or\ndiﬀerent tokens across cores, which is what the diﬀerent colors symbolize.\n5.3 Model and Data Parallelism\nIt is common to mix both model and data parallelism for large scale models, which was done\nin the largest T5 models (Raﬀel et al., 2019; Xue et al., 2020) and in GPT-3 (Brown et al.,\n2020). With a total of N = n×m cores, now each core will be responsible for B/n tokens\nand dff /m of both the weights and intermediate activation. In the forward and backward\npass each core communicates a tensor of size [ B/n,dmodel] in an all-reduce operation.\n21\nFedus, Zoph and Shazeer\n5.4 Expert and Data Parallelism\nNext we describe the partitioning strategy for expert and data parallelism. Switch Trans-\nformers will allocate all of their cores to the data partitioning dimension n, which will also\ncorrespond to the number of experts in the model. For each token per core a router locally\ncomputes assignments to the experts. The output is a binary matrix of size [ n, B/n, E,\nC] which is partitioned across the ﬁrst dimension and determines expert assignment. This\nbinary matrix is then used to do a gather via matrix multiplication with the input tensor\nof [n, B/n, dmodel].\neinsum([n,B/n,d model],[n,B/n,E,C ],dimension = [B/n]) (7)\nresulting in the ﬁnal tensor of shape [ n, E, C, dmodel], which is sharded across the ﬁrst\ndimension. Because each core has its own expert, we do an all-to-all communication of\nsize [E, C, dmodel] to now shard the E dimension instead of the n-dimension. There are\nadditional communication costs of bﬂoat16 tensors of size E×C×dmodel in the forward pass\nto analogusly receive the tokens from each expert located on diﬀerent cores. See Appendix F\nfor a detailed analysis of the expert partitioning code.\n5.5 Expert, Model and Data Parallelism\nIn the design of our best model, we seek to balance the FLOPS per token and the parameter\ncount. When we scale the number of experts, we increase the number of parameters, but do\nnot change the FLOPs per token. In order to increase FLOPs, we must also increase thedff\ndimension (which also increases parameters, but at a slower rate). This presents a trade-oﬀ:\nas we increase dff we will run out of memory per core, which then necessitates increasing\nm. But since we have a ﬁxed number of cores N, and N = n×m, we must decrease n,\nwhich forces use of a smaller batch-size (in order to hold tokens per core constant).\nWhen combining both model and expert-parallelism, we will have all-to-all communica-\ntion costs from routing the tokens to the correct experts along with the internal all-reduce\ncommunications from the model parallelism. Balancing the FLOPS, communication costs\nand memory per core becomes quite complex when combining all three methods where the\nbest mapping is empirically determined. See our further analysis in section 5.6 for how the\nnumber of experts eﬀects the downstream performance as well.\n5.6 Towards Trillion Parameter Models\nCombining expert, model and data parallelism, we design two large Switch Transformer\nmodels, one with 395 billion and 1.6 trillion parameters, respectively. We study how these\nmodels perform on both up-stream pre-training as language models and their downstream\nﬁne-tuning performance. The parameters, FLOPs per sequence and hyper-parameters of\nthe two diﬀerent models are listed below in Table 9. Standard hyper-parameters of the\nTransformer, includingdmodel, dff , dkv, number of heads and number of layers are described,\nas well as a less common feature, FFNGEGLU , which refers to a variation of the FFN layer\nwhere the expansion matrix is substituted with two sets of weights which are non-linearly\ncombined (Shazeer, 2020).\nThe Switch-C model is designed using only expert-parallelism, and no model-parallelism,\nas described earlier in Section 5.4. As a result, the hyper-parameters controlling the width,\n22\nSwitch Transformers\nModel Parameters FLOPs/seqdmodel FFNGEGLU dff dkv Num. Heads\nT5-Base 0.2B 124B 768 ✓ 2048 64 12\nT5-Large 0.7B 425B 1024 ✓ 2816 64 16\nT5-XXL 11B 6.3T 4096 ✓ 10240 64 64\nSwitch-Base 7B 124B 768 ✓ 2048 64 12\nSwitch-Large26B 425B 1024 ✓ 2816 64 16\nSwitch-XXL 395B 6.3T 4096 ✓ 10240 64 64\nSwitch-C 1571B 890B 2080 6144 64 32\nModel Expert Freq. Num. Layers Num Experts Neg. Log Perp. @250k Neg. Log Perp. @ 500k\nT5-Base – 12 – -1.599 -1.556\nT5-Large – 24 – -1.402 -1.350\nT5-XXL – 24 – -1.147 -1.095\nSwitch-Base 1/2 12 128 -1.370 -1.306\nSwitch-Large 1/2 24 128 -1.248 -1.177\nSwitch-XXL 1/2 24 64 -1.086 -1.008\nSwitch-C 1 15 2048 -1.096 -1.043\nTable 9: Switch model design and pre-training performance. We compare the hyper-\nparameters and pre-training performance of the T5 models to our Switch Trans-\nformer variants. The last two columns record the pre-training model quality on the\nC4 data set after 250k and 500k steps, respectively. We observe that the Switch-\nC Transformer variant is 4x faster to a ﬁxed perplexity (with the same compute\nbudget) than the T5-XXL model, with the gap increasing as training progresses.\ndepth, number of heads, and so on, are all much smaller than the T5-XXL model. In\ncontrast, the Switch-XXL is FLOP-matched to the T5-XXL model, which allows for larger\ndimensions of the hyper-parameters, but at the expense of additional communication costs\ninduced by model-parallelism (see Section 5.5 for more details).\nSample eﬃciency versus T5-XXL. In the ﬁnal two columns of Table 9 we record\nthe negative log perplexity on the C4 corpus after 250k and 500k steps, respectively. After\n250k steps, we ﬁnd both Switch Transformer variants to improve over the T5-XXL version’s\nnegative log perplexity by over 0.061.10 To contextualize the signiﬁcance of a gap of 0.061,\nwe note that the T5-XXL model had to train for an additional 250k steps to increase\n0.052. The gap continues to increase with additional training, with the Switch-XXL model\nout-performing the T5-XXL by 0.087 by 500k steps.\nTraining instability. However, as described in the introduction, large sparse models\ncan be unstable, and as we increase the scale, we encounter some sporadic issues. We\nﬁnd that the larger Switch-C model, with 1.6T parameters and 2048 experts, exhibits no\ntraining instability at all. Instead, the Switch XXL version, with nearly 10x larger FLOPs\nper sequence, is sometimes unstable. As a result, though this is our better model on a\nstep-basis, we do not pre-train for a full 1M steps, in-line with the ﬁnal reported results of\nT5 (Raﬀel et al., 2019).\n10. This reported quality diﬀerence is a lower bound, and may actually be larger. The T5-XXL was pre-\ntrained on an easier C4 data set which included duplicated, and thus easily copied, snippets within\nexamples.\n23\nFedus, Zoph and Shazeer\nReasoning ﬁne-tuning performance. As a preliminary assessment of the model\nquality, we use a Switch-XXL model partially pre-trained on 503B tokens, or approximately\nhalf the text used by the T5-XXL model. Using this checkpoint, we conduct multi-task\ntraining for eﬃciency, where all tasks are learned jointly, rather than individually ﬁne-tuned.\nWe ﬁnd that SQuAD accuracy on the validation set increases to 89.7 versus state-of-the-art\nof 91.3. Next, the average SuperGLUE test score is recorded at 87.5 versus the T5 version\nobtaining a score of 89.3 compared to the state-of-the-art of 90.0 (Wang et al., 2019). On\nANLI (Nie et al., 2019), Switch XXL improves over the prior state-of-the-art to get a 65.7\naccuracy versus the prior best of 49.4 (Yang et al., 2020). We note that while the Switch-\nXXL has state-of-the-art Neg. Log Perp. on the upstream pre-training task, its gains have\nnot yet fully translated to SOTA downstream performance. We study this issue more in\nAppendix E.\nKnowledge-based ﬁne-tuning performance. Finally, we also conduct an early ex-\namination of the model’s knowledge with three closed-book knowledge-based tasks: Natural\nQuestions, WebQuestions and TriviaQA, without additional pre-training using Salient Span\nMasking (Guu et al., 2020). In all three cases, we observe improvements over the prior state-\nof-the-art T5-XXL model (without SSM). Natural Questions exact match increases to 34.4\nversus the prior best of 32.8, Web Questions increases to 41.0 over 37.2, and TriviaQA\nincreases to 47.5 versus 42.9.\nSumming up, despite training on less than half the data of other models, we already\nﬁnd comparable, and sometimes state-of-the-art, model quality. Currently, the Switch\nTransformer translates substantial upstream gains better to knowledge-based tasks, than\nreasoning-tasks (see Appendix E). Extracting stronger ﬁne-tuning performance from large\nexpert models is an active research question, and the pre-training perplexity indicates future\nimprovements should be possible.\n6. Related Work\nThe importance of scale in neural networks is widely recognized and several approaches have\nbeen proposed. Recent works have scaled models to billions of parameters through using\nmodel parallelism (e.g. splitting weights and tensors across multiple cores) (Shazeer et al.,\n2018; Rajbhandari et al., 2019; Raﬀel et al., 2019; Brown et al., 2020; Shoeybi et al., 2019).\nAlternatively, Harlap et al. (2018); Huang et al. (2019) propose using pipeline based model\nparallelism, where diﬀerent layers are split across devices and micro-batches arepipelined to\nthe diﬀerent layers. Finally, Product Key networks (Lample et al., 2019) were proposed to\nscale up the capacity of neural networks by doing a lookup for learnable embeddings based\non the incoming token representations to a given layer.\nOur work studies a speciﬁc model in a class of methods that doconditional computation,\nwhere computation decisions are made dynamically based on the input. Cho and Bengio\n(2014) proposed adaptively selecting weights based on certain bit patterns occuring in the\nmodel hidden-states. Eigen et al. (2013) built stacked expert layers with dense matrix\nmultiplications and ReLU activations and showed promising results on jittered MNIST and\nmonotone speech. In computer vision Puigcerver et al. (2020) manually route tokens based\non semantic classes during upstream pre-training and then select the relevant experts to be\nused according to the downstream task.\n24\nSwitch Transformers\nMixture of Experts (MoE), in the context of modern deep learning architectures, was\nproven eﬀective in Shazeer et al. (2017). That work added an MoE layer which was stacked\nbetween LSTM (Hochreiter and Schmidhuber, 1997) layers, and tokens were separately\nrouted to combinations of experts. This resulted in state-of-the-art results in language\nmodeling and machine translation benchmarks. The MoE layer was reintroduced into the\nTransformer architecture by the Mesh Tensorﬂow library (Shazeer et al., 2018) where MoE\nlayers were introduced as a substitute of the FFN layers, however, there were no accom-\npanying NLP results. More recently, through advances in machine learning infrastructure,\nGShard (Lepikhin et al., 2020), which extended the XLA compiler, used the MoE Trans-\nformer to dramatically improve machine translation across 100 languages. Finally Fan et al.\n(2021) chooses a diﬀerent deterministic MoE strategy to split the model parameters into\nnon-overlapping groups of languages.\nSparsity along the sequence length dimension ( L) in the Transformer attention patterns\nhas been a successful technique to reduce the attention complexity fromO(L2) (Child et al.,\n2019; Correia et al., 2019; Sukhbaatar et al., 2019; Kitaev et al., 2020; Zaheer et al., 2020;\nBeltagy et al., 2020). This has enabled learning longer sequences than previously possi-\nble. This version of the Switch Transformer does not employ attention sparsity, but these\ntechniques are complimentary, and, as future work, these could be combined to potentially\nimprove learning on tasks requiring long contexts.\n7. Discussion\nWe pose and discuss questions about the Switch Transformer, and sparse expert models\ngenerally, where sparsity refers to weights, not on attention patterns.\nIsn’t Switch Transformer better due to sheer parameter count? Yes, and by\ndesign! Parameters, independent of the total FLOPs used, are a useful axis to scale neural\nlanguage models. Large models have been exhaustively shown to perform better (Kaplan\net al., 2020). But in this case, our model is more sample eﬃcient and faster while using the\nsame computational resources.\nI don’t have access to a supercomputer—is this still useful for me? Though\nthis work has focused on extremely large models, we also ﬁnd that models with as few as two\nexperts improves performance while easily ﬁtting within memory constraints of commonly\navailable GPUs or TPUs (details in Appendix D). We therefore believe our techniques are\nuseful in small-scale settings.\nDo sparse models outperform dense models on the speed-accuracy Pareto\ncurve? Yes. Across a wide variety of diﬀerent models sizes, sparse models outperform\ndense models per step and on wall clock time. Our controlled experiments show for a ﬁxed\namount of computation and time, sparse models outperform dense models.\nI can’t deploy a trillion parameter model—can we shrink these models? We\ncannot fully preserve the model quality, but compression rates of 10 to 100x are achievable\nby distilling our sparse models into dense models while achieving ≈30% of the quality gain\nof the expert model.\nWhy use Switch Transformer instead of a model-parallel dense model? On a\ntime basis, Switch Transformers can be far more eﬃcient than dense-models with sharded\nparameters (Figure 6). Also, we point out that this decision is not mutually exclusive—we\n25\nFedus, Zoph and Shazeer\ncan, and do, use model-parallelism in Switch Transformers, increasing the FLOPs per token,\nbut incurring the slowdown of conventional model-parallelism.\nWhy aren’t sparse models widely used already? The motivation to try sparse\nmodels has been stymied by the massive success of scaling dense models (the success of\nwhich is partially driven by co-adaptation with deep learning hardware as argued in Hooker\n(2020)). Further, sparse models have been subject to multiple issues including (1) model\ncomplexity, (2) training diﬃculties, and (3) communication costs. Switch Transformer\nmakes strides to alleviate these issues.\n8. Future Work\nThis paper lays out a simpliﬁed architecture, improved training procedures, and a study\nof how sparse models scale. However, there remain many open future directions which we\nbrieﬂy describe here:\n1. A signiﬁcant challenge is further improving training stability for the largest models.\nWhile our stability techniques were eﬀective for our Switch-Base, Switch-Large and\nSwitch-C models (no observed instability), they were not suﬃcient for Switch-XXL.\nWe have taken early steps towards stabilizing these models, which we think may be\ngenerally useful for large models, including using regularizers for improving stability\nand adapted forms of gradient clipping, but this remains unsolved.\n2. Generally we ﬁnd that improved pre-training quality leads to better downstream re-\nsults (Appendix E), though we sometimes encounter striking anomalies. For instance,\ndespite similar perplexities modeling the C4 data set, the 1.6T parameter Switch-C\nachieves only an 87.7 exact match score in SQuAD, which compares unfavorably to\n89.6 for the smaller Switch-XXL model. One notable diﬀerence is that the Switch-\nXXL model applies ≈10x the FLOPS per token than the Switch-C model, even though\nit has ≈4x less unique parameters (395B vs 1.6T). This suggests a poorly understood\ndependence between ﬁne-tuning quality, FLOPS per token and number of parameters.\n3. Perform a comprehensive study of scaling relationships to guide the design of ar-\nchitectures blending data, model and expert-parallelism. Ideally, given the specs of\na hardware conﬁguration (computation, memory, communication) one could more\nrapidly design an optimal model. And, vice versa, this may also help in the design of\nfuture hardware.\n4. Our work falls within the family of adaptive computation algorithms. Our approach\nalways used identical, homogeneous experts, but future designs (facilitated by more\nﬂexible infrastructure) could support heterogeneous experts. This would enable more\nﬂexible adaptation by routing to larger experts when more computation is desired—\nperhaps for harder examples.\n5. Investigating expert layers outside the FFN layer of the Transformer. We ﬁnd pre-\nliminary evidence that this similarly can improve model quality. In Appendix A,\nwe report quality improvement adding these inside Self-Attention layers, where our\n26\nSwitch Transformers\nlayer replaces the weight matrices which produce Q, K, V. However, due to training\ninstabilities with the bﬂoat16 format, we instead leave this as an area for future work.\n6. Examining Switch Transformer in new and across diﬀerent modalities. We have thus\nfar only considered language, but we believe that model sparsity can similarly provide\nadvantages in new modalities, as well as multi-modal networks.\nThis list could easily be extended, but we hope this gives a ﬂavor for the types of\nchallenges that we are thinking about and what we suspect are promising future directions.\n9. Conclusion\nSwitch Transformers are scalable and eﬀective natural language learners. We simplify Mix-\nture of Experts to produce an architecture that is easy to understand, stable to train and\nvastly more sample eﬃcient than equivalently-sized dense models. We ﬁnd that these models\nexcel across a diverse set of natural language tasks and in diﬀerent training regimes, includ-\ning pre-training, ﬁne-tuning and multi-task training. These advances make it possible to\ntrain models with hundreds of billion to trillion parameters and which achieve substantial\nspeedups relative to dense T5 baselines. We hope our work motivates sparse models as\nan eﬀective architecture and that this encourages researchers and practitioners to consider\nthese ﬂexible models in natural language tasks, and beyond.\nAcknowledgments\nThe authors would like to thank Margaret Li who provided months of key insights into\nalgorithmic improvements and suggestions for empirical studies. Hugo Larochelle for sage\nadvising and clarifying comments on the draft, Irwan Bello for detailed comments and\ncareful revisions, Colin Raﬀel and Adam Roberts for timely advice on neural language\nmodels and the T5 code-base, Yoshua Bengio for advising and encouragement on research\nin adaptive computation, Jascha Sohl-dickstein for interesting new directions for stabilizing\nnew large scale models and paper revisions, and the Google Brain Team for useful discussions\non the paper. Blake Hechtman who provided invaluable help in proﬁling and improving the\ntraining performance of our models.\nA. Switch for Attention\nShazeer et al. (2018); Lepikhin et al. (2020) designed MoE Transformers (Shazeer et al.,\n2017) by adding MoE layers into the dense feedfoward network (FFN) computations of\nthe Transformer. Similarly, our work also replaced the FFN layer in the Transformer, but\nwe brieﬂy explore here an alternate design. We add Switch layers into the Transformer\nSelf-Attention layers. To do so, we replace the trainable weight matrices that produce the\nqueries, keys and values with Switch layers as seen in Figure 10.\nTable 10 records the quality after a ﬁxed number of steps as well as training time\nfor several variants. Though we ﬁnd improvements, we also found these layers to be more\nunstable when using bﬂoat16 precision and thus we did not include them in the ﬁnal variant.\n27\nFedus, Zoph and Shazeer\nRouter\nFFN 1 FFN 2 FFN 4FFN 3\nAdd + Normalize\nFFN 1 FFN 2 FFN 4FFN 3\nAdd + Normalize\nx1 x2\ny1 y2\np = 0.5 p = 0.7\nPositional\nembedding\nPositional\nembedding\nAdd + Normalize\nSwitching Self-Attention\nAdd + Normalize\nFeed Forward Layer\ny\nx\nMore Parameters\nSelf-Attention\nQ        K        V\nRouter\nFeed-Forward Layer\nSelf-Attention\nQ        K        V\nFigure 10: Switch layers in attention. We diagram how to incorporate the Switch layer into\nthe Self-Attention transformer block. For each token (here we show two tokens,\nx1 = “More” and x2 = “Parameters”), one set of weights produces the query\nand the other set of unique weights produces the shared keys and values. We\nexperimented with each expert being a linear operation, as well as a FFN, as\nwas the case throughout this work. While we found quality improvements using\nthis, we found this to be more unstable when used with low precision number\nformats, and thus leave it for future work.\nHowever, when these layers do train stably, we believe the preliminary positive results\nsuggests a future promising direction.\nModel Precision Quality Quality Speed\n@100k Steps (↑) @16H ( ↑) (ex/sec) ( ↑)\nExperts FF ﬂoat32 -1.548 -1.614 1480\nExpert Attention ﬂoat32 -1.524 -1.606 1330\nExpert Attention bﬂoat16 [diverges] [diverges] –\nExperts FF + Attention ﬂoat32 -1.513 -1.607 1240\nExpert FF + Attention bﬂoat16 [diverges] [diverges] –\nTable 10: Switch attention layer results. All models have 32 experts and train with 524k to-\nkens per batch. Experts FF is when experts replace the FFN in the Transformer,\nwhich is our standard setup throughout the paper. Experts FF + Attention is\nwhen experts are used to replace both the FFN and the Self-Attention layers.\nWhen training with bﬂoat16 precision the models that have experts attention\ndiverge.\n28\nSwitch Transformers\nB. Preventing Token Dropping with No-Token-Left-Behind\nDue to software constraints on TPU accelerators, the shapes of our Tensors must be stat-\nically sized. As a result, each expert has a ﬁnite and ﬁxed capacity to process token\nrepresentations. This, however, presents an issue for our model which dynamically routes\ntokens at run-time that may result in an uneven distribution over experts. If the number of\ntokens sent to an expert is less than the expert capacity, then the computation may simply\nbe padded – an ineﬃcient use of the hardware, but mathematically correct. However, when\nthe number of tokens sent to an expert is larger than its capacity (expert overﬂow), a proto-\ncol is needed to handle this. Lepikhin et al. (2020) adapts a Mixture-of-Expert model and\naddresses expert overﬂow by passing its representation to the next layer without processing\nthrough a residual connection which we also follow.\nWe suspected that having no computation applied to tokens could be very wasteful,\nespecially since if there is overﬂow on one expert, that means another expert will have extra\ncapacity. With this intuition we create No-Token-Left-Behind, which iteratively reroutes\nany tokens that are at ﬁrst routed to an expert that is overﬂowing. Figure 11 shows a\ngraphical description of this method, which will allow us to guarantee almost no tokens\nwill be dropped during training and inference. We hypothesised that this could improve\nperformance and further stabilize training, but we found no empirical beneﬁts. We suspect\nthat once the network learns associations between diﬀerent tokens and experts, if this as-\nsociation is changed (e.g. sending a token to its second highest expert) then performance\ncould be degraded.\nC. Encouraging Exploration Across Experts\nAt each expert-layer, the router determines to which expert to send the token. This is a\ndiscrete decision over the available experts, conditioned on information about the token’s\nrepresentation. Based on the incoming token representation, the router determines the\nbest expert, however, it receives no counterfactual information about how well it would\nhave done selecting an alternate expert. As in reinforcement learning, a classic exploration-\nexploitation dilemma arises (Sutton and Barto, 2018). These issues have been similarly\nnoted and addressed diﬀerently by Rosenbaum et al. (2017) which demonstrated success\nin multi-task learning. This particular setting most closely matches that of a contextual\nbandit (Robbins, 1952). Deterministically selecting the top expert always amounts to an\nexploitative strategy – we consider balancing exploration to seek better expert assignment.\nTo introduce exploration, we consider several approaches: 1) deterministic or argmax 2)\nsampling from the softmax distribution 3) input dropout on the incoming representation 4)\nmultiplicative jitter noise on the incoming representation. The resulting impact on model\nquality is reported in Table 11. Throughout this work, we use input jitter to inject noise as\nwe have found it to empirically perform the best.\nD. Switch Transformers in Lower Compute Regimes\nSwitch Transformer is also an eﬀective architecture at small scales as well as in regimes\nwith thousands of cores and trillions of parameters. Many of our prior experiments were\n29\nFedus, Zoph and Shazeer\nTokens\nExpert 1 Expert 2 Expert 3\nRouter\nProbabilities\n0.1\n0.7 \n0.2\n0.7 \n0.2\n0.1\n0.5 \n0.3\n0.2\n0.8 \n0.1\n0.1\n0.3\n0.1\n0.6 \n0.7 \n0.1\n0.2\nRoute token to\nhighest probability\nStage-1 \nRoute token to\nsecond highest\nprobability if not\nrouted\nStage-2 \nFigure 11: Diagram of the No-Token-Left-Behind Routing. Stage 1 is equivalent to Switch\nrouting where tokens are routed to the expert with the highest probability from\nthe router. In Stage 2 we look at all tokens that have overﬂowed and route them\nto the expert with which has the second highest probability. Tokens can still be\noverﬂowed if their second highest expert has too many tokens, but this allows\nmost of the tokens to be routed. This process can be iterated to guarantee\nvirtually no tokens are dropped at all.\nModel Quality (Neg. Log Perp.) ( ↑)\nArgmax -1.471\nSample softmax -1.570\nInput dropout -1.480\nInput jitter -1.468\nTable 11: Router Exploration Strategies. Quality of the Switch Transformer, measured by\nthe negative log perplexity, under diﬀerent randomness-strategies for selecting\nthe expert (lower is better). There is no material speed performance diﬀerence\nbetween the variants.\nat the scale of 10B+ parameter models, but we show in Figure 12 as few as 2 experts\nproduce compelling gains over a FLOP-matched counterpart. Even if a super computer is\nnot readily available, training Switch Transformers with 2, 4, or 8 experts (as we typically\nrecommend one expert per core) results in solid improvements over T5 dense baselines.\n30\nSwitch Transformers\n0.0 0.2 0.4 0.6 0.8\nTraining Step 1e5\n2.0\n1.9\n1.8\n1.7\n1.6\n1.5\nNeg Log Perplexity\nSwitch-Base: 8e\nSwitch-Base: 4e\nSwitch-Base: 2e\nT5-Base\nFigure 12: Switch Transformer with few experts. Switch Transformer improves over the\nbaseline even with very few experts. Here we show scaling properties at very\nsmall scales, where we improve over the T5-Base model using 2, 4, and 8 experts.\n31\nFedus, Zoph and Shazeer\nE. Relation of Upstream to Downstream Model Performance\nThere is no guarantee that a model’s quality on a pre-training objective will translate to\ndownstream task results. Figure 13 presents the correlation of the upstream model quality,\nfor both dense and Switch models, on the C4 pre-training task with two downstream task\nmeasures: average SuperGLUE performance and TriviaQA score. We choose these two\ntasks as one probes the model’s reasoning and the other factual knowledge.\n1.7\n 1.6\n 1.5\n 1.4\n 1.3\n 1.2\n 1.1\n 1.0\nC4 Neg. Log Perplexity\n65\n70\n75\n80\n85\n90SuperGLUE Score\nSOTA\nDense\nSwitch\n1.7\n 1.6\n 1.5\n 1.4\n 1.3\n 1.2\n 1.1\n 1.0\nC4 Neg. Log Perplexity\n10\n20\n30\n40\n50TriviaQA Score\nSOTA\nDense\nSwitch\nFigure 13: Upstream pre-trained quality to downstream model quality. We correlate the\nupstream performance with downstream quality on both SuperGLUE and Triv-\niaQA (SOTA recorded without SSM), reasoning and knowledge-heavy bench-\nmarks, respectively (validation sets). We ﬁnd that, as with the baseline, the\nSwitch model scales with improvements in the upstream pre-training task. For\nSuperGLUE, we ﬁnd a loosely linear relation between negative log perplexity\nand the average SuperGLUE score. However, the dense model often performs\nbetter for a ﬁxed perplexity, particularly in the large-scale regime. Conversely,\non the knowledge-heavy task, TriviaQA, we ﬁnd that the Switch Transformer\nmay follow an improved scaling relationship – for a given upstream perplexity,\nit does better than a dense counterpart. Further statistics (expensive to collect\nand left to future work) would be necessary to conﬁrm these observations.\nWe ﬁnd a consistent correlation, indicating that for both baseline and Switch models,\nimproved pre-training leads to better downstream results. Additionally, for a ﬁxed up-\nstream perplexity we ﬁnd that both Switch and dense models perform similarly in the small\nto medium model size regime. However, in the largest model regime (T5-11B/T5-XXL)\nour largest Switch models, as mentioned in Section 5.6, do not always translate their up-\nstream perplexity well to downstream ﬁne-tuning on the SuperGLUE task. This warrants\nfuture investigation and study to fully realize the potential of sparse models. Understand-\ning the ﬁne-tuning dynamics with expert-models is very complicated and is dependent on\nregularization, load-balancing, and ﬁne-tuning hyper-parameters.\n32\nSwitch Transformers\nF. Pseudo Code for Switch Transformers\nPseudocode for Switch Transformers in Mesh Tensorﬂow (Shazeer et al., 2018). No model\nparallelism is being used for the below code (see 5.4 for more details).\nimport mesh tensorflow as mtf\ndef load balance loss(router probs, expert mask):\n\"\"\"Calculate load=balancing loss to ensure diverse expert routing.\"\"\"\n# router probs is the probability assigned for each expert per token.\n# router probs shape: [num cores, tokens per core, num experts]\n# expert index contains the expert with the highest router probability in one=hot format.\n# expert mask shape: [num cores, tokens per core, num experts]\n# For each core, get the fraction of tokens routed to each expert.\n# density 1 shape: [num cores, num experts]\ndensity 1 = mtf.reduce mean(expert mask, reduced dim=tokens per core)\n# For each core, get fraction of probability mass assigned to each expert\n# from the router across all tokens.\n# density 1 proxy shape: [num cores, num experts]\ndensity 1 proxy = mtf.reduce mean(router probs, reduced dim=tokens per core)\n# density l for a single core: vector of length num experts that sums to 1.\n# density l proxy for a single core: vector of length num experts that sums to 1.\n# Want both vectors to have uniform allocation (1/num experts) across all num expert elements.\n# The two vectors will be pushed towards uniform allocation when the dot product is minimized.\nloss = mtf.reduce mean(density 1 proxy ∗ density 1) ∗ (num experts ˆ 2)\nreturn loss\nFigure 14: Pseudo code for the load balance loss for Switch Transformers in Mesh Tensor-\nﬂow.\n33\nFedus, Zoph and Shazeer\nimport mesh tensorflow as mtf\ndef router(inputs, capacity factor):\n\"\"\"Produce the combine and dispatch tensors used for sending and\nreceiving tokens from their highest probability expert. \"\"\"\n# Core layout is split across num cores for all tensors and operations.\n# inputs shape: [num cores, tokens per core, d model]\nrouter weights = mtf.Variable(shape=[d model, num experts])\n# router logits shape: [num cores, tokens per core, num experts]\nrouter logits = mtf.einsum([inputs, router weights], reduced dim=d model)\nif is training:\n# Add noise for exploration across experts.\nrouter logits += mtf.random uniform(shape=router logits.shape, minval=1=eps, maxval=1+eps)\n# Convert input to softmax operation from bfloat16 to float32 for stability.\nrouter logits = mtf.to float32(router logits)\n# Probabilities for each token of what expert it should be sent to.\nrouter probs = mtf.softmax(router logits, axis==1)\n# Get the top=1 expert for each token. expert gate is the top=1 probability\n# from the router for each token. expert index is what expert each token\n# is going to be routed to.\n# expert gate shape: [num cores, tokens per core]\n# expert index shape: [num cores, tokens per core]\nexpert gate, expert index = mtf.top 1(router probs, reduced dim=num experts)\n# expert mask shape: [num cores, tokens per core, num experts]\nexpert mask = mtf.one hot(expert index, dimension=num experts)\n# Compute load balancing loss.\naux loss = load balance loss(router probs, expert mask)\n# Experts have a fixed capacity, ensure we do not exceed it. Construct\n# the batch indices, to each expert, with position in expert\n# make sure that not more that expert capacity examples can be routed to\n# each expert.\nposition in expert = mtf.cumsum(expert mask, dimension=tokens per core) ∗ expert mask\n# Keep only tokens that fit within expert capacity.\nexpert mask ∗= mtf.less(position in expert, expert capacity)\nexpert mask flat = mtf.reduce sum(expert mask, reduced dim=experts dim)\n# Mask out the experts that have overflowed the expert capacity.\nexpert gate ∗= expert mask flat\n# combine tensor used for combining expert outputs and scaling with router probability.\n# combine tensor shape: [num cores, tokens per core, num experts, expert capacity]\ncombine tensor = (\nexpert gate ∗ expert mask flat ∗\nmtf.one hot(expert index, dimension=num experts) ∗\nmtf.one hot(position in expert, dimension=expert capacity))\n# Cast back outputs to bfloat16 for the rest of the layer.\ncombine tensor = mtf.to bfloat16(combine tensor)\n# Create binary dispatch tensor that is 1 if the token gets routed to the corresponding expert.\n# dispatch tensor shape: [num cores, tokens per core, num experts, expert capacity]\ndispatch tensor = mtf.cast(combine tensor, tf.bool)\nreturn dispatch tensor, combine tensor, aux loss\nFigure 15: Pseudo code for the router for Switch Transformers in Mesh Tensorﬂow.\n34\nSwitch Transformers\nimport mesh tensorflow as mtf\ndef switch layer(inputs, n, capacity factor, num experts):\n\"\"\"Distributed switch transformer feed=forward layer.\"\"\"\n# num cores (n) = total cores for training the model (scalar).\n# d model = model hidden size (scalar).\n# num experts = total number of experts.\n# capacity factor = extra buffer for each expert.\n# inputs shape: [batch, seq len, d model]\nbatch, seq len, d model = inputs.get shape()\n# Each core will route tokens per core tokens to the correct experts.\ntokens per core = batch ∗ seq len / num cores\n# Each expert will have shape [num cores, expert capacity, d model].\n# Each core is responsible for sending expert capacity tokens\n# to each expert.\nexpert capacity = tokens per core ∗ capacity factor / num experts\n# Reshape to setup per core expert dispatching.\n# shape: [batch, seq len, d model] => [num cores, tokens per core, d model]\n# Core layout: [n, 1, 1] => [n, 1, 1]\ninputs = mtf.reshape(inputs, [num cores, tokens per core, d model])\n# Core Layout: [n, 1, 1] => [n, 1, 1, 1], [n, 1, 1, 1]\n# dispatch tensor (boolean) shape: [num cores, tokens per core, num experts, expert capacity]\n# dispatch tensor is used for routing tokens to the correct expert.\n# combine tensor (float) shape: [num cores, tokens per core, num experts, expert capacity]\n# combine tensor used for combining expert outputs and scaling with router\n# probability.\ndispatch tensor, combine tensor, aux loss = router(inputs, expert capacity)\n# Matmul with large boolean tensor to assign tokens to the correct expert.\n# Core Layout: [n, 1, 1], => [1, n, 1, 1]\n# expert inputs shape: [num experts, num cores, expert capacity, d model]\nexpert inputs = mtf.einsum([inputs, dispatch tensor], reduce dims=[tokens per core])\n# All=to=All communication. Cores split across num cores and now we want to split\n# across num experts. This sends tokens, routed locally, to the correct expert now\n# split across different cores.\n# Core layout: [1, n, 1, 1] => [n, 1, 1, 1]\nexpert inputs = mtf.reshape(expert inputs, [num experts, num cores, expert capacity, d model])\n# Standard feed forward computation, where each expert will have its own\n# unique set of parameters.\n# Total unique parameters created: num experts ∗ (d model ∗ d ff ∗ 2).\n# expert outputs shape: [num experts, num cores, expert capacity, d model]\nexpert outputs = feed forward(expert inputs)\n# All=to=All communication. Cores are currently split across the experts\n# dimension, which needs to be switched back to being split across num cores.\n# Core Layout: [n, 1, 1, 1] => [1, n, 1, 1]\nexpert outputs = mtf.reshape(expert outputs, [num experts, num cores, expert capacity, d model])\n# Convert back to input shape and multiply outputs of experts by the routing probability.\n# expert outputs shape: [num experts, num cores, tokens per core, d model]\n# expert outputs combined shape: [num cores, tokens per core, d model]\n# Core Layout: [1, n, 1, 1] => [n, 1, 1]\nexpert outputs combined = mtf.einsum([expert outputs, combine tensor], reduce dims=[tokens per core])\n# Remove tokens per core shapes used for local routing dispatching to match input shape.\n# Core Layout: [n, 1, 1] => [n, 1, 1]\noutputs = mtf.reshape(expert outputs combined, [batch, seq len, d model])\nreturn outputs, aux loss\nFigure 16: Pseudo code of the Switch Transformer layer in Mesh Tensorﬂow.\n35\nFedus, Zoph and Shazeer\nReferences\nMart´ ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey Dean,\nMatthieu Devin, Sanjay Ghemawat, Geoﬀrey Irving, Michael Isard, et al. Tensorﬂow:\nA system for large-scale machine learning. In 12th {USENIX}symposium on operating\nsystems design and implementation ( {OSDI}16), pages 265–283, 2016.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document trans-\nformer. arXiv preprint arXiv:2004.05150 , 2020.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on free-\nbase from question-answer pairs. In Proceedings of the 2013 conference on empirical\nmethods in natural language processing , pages 1533–1544, 2013.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences\nwith sparse transformers. arXiv preprint arXiv:1904.10509 , 2019.\nKyunghyun Cho and Yoshua Bengio. Exponentially increasing the capacity-to-computation\nratio for conditional computation in deep learning. arXiv preprint arXiv:1406.7362, 2014.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa\nSchoenick, and Oyvind Tafjord. Think you have solved question answering? try arc,\nthe ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 , 2018.\nGon¸ calo M Correia, Vlad Niculae, and Andr´ e FT Martins. Adaptively sparse transformers.\narXiv preprint arXiv:1909.00015 , 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations\nin a deep mixture of experts. arXiv preprint arXiv:1312.4314 , 2013.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal,\nMandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond\nenglish-centric multilingual machine translation. Journal of Machine Learning Research,\n22(107):1–48, 2021.\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: Better text generation via\nﬁlling in the . arXiv preprint arXiv:1801.07736 , 2018.\nTrevor Gale, Matei Zaharia, Cliﬀ Young, and Erich Elsen. Sparse gpu kernels for deep\nlearning. arXiv preprint arXiv:2006.10901 , 2020.\nScott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights.\nhttps://openai.com/blog/block-sparse-gpu-kernels/, 2017.\n36\nSwitch Transformers\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm:\nRetrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909 ,\n2020.\nAaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur,\nGreg Ganger, and Phil Gibbons. Pipedream: Fast and eﬃcient pipeline parallel dnn\ntraining. arXiv preprint arXiv:1806.03377 , 2018.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\nMustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend.\nIn C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems , volume 28, pages 1693–1701. Cur-\nran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/\nafdec7005cc9f14302cd0474fd0f3c96-Paper.pdf.\nGeoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531 , 2015.\nSepp Hochreiter and J¨ urgen Schmidhuber. Long short-term memory.Neural computation,\n9(8):1735–1780, 1997.\nSara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489 , 2020.\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,\nHyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Eﬃcient training\nof giant neural networks using pipeline parallelism. In Advances in neural information\nprocessing systems, pages 103–112, 2019.\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoﬀrey E Hinton. Adaptive\nmixtures of local experts. Neural computation, 3(1):79–87, 1991.\nMichael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em\nalgorithm. Neural computation, 6(2):181–214, 1994.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large\nscale distantly supervised challenge dataset for reading comprehension. arXiv preprint\narXiv:1705.03551, 2017.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeﬀrey Wu, and Dario Amodei. Scaling laws for neural\nlanguage models. arXiv preprint arXiv:2001.08361 , 2020.\nNikita Kitaev,  Lukasz Kaiser, and Anselm Levskaya. Reformer: The eﬃcient transformer.\narXiv preprint arXiv:2001.04451 , 2020.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural\nquestions: a benchmark for question answering research. Transactions of the Association\nfor Computational Linguistics , 7:453–466, 2019.\n37\nFedus, Zoph and Shazeer\nGuillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, and\nHerv´ e J´ egou. Large memory layers with product keys. InAdvances in Neural Information\nProcessing Systems, pages 8548–8559, 2019.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris\nCallison-Burch, and Nicholas Carlini. Deduplicating training data makes language models\nbetter. arXiv preprint arXiv:2107.06499 , 2021.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping\nHuang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models\nwith conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668,\n2020.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David\nGarcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al.\nMixed precision training. arXiv preprint arXiv:1710.03740 , 2017.\nShashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the\nsummary! topic-aware convolutional neural networks for extreme summarization. arXiv\npreprint arXiv:1808.08745, 2018.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.\nAdversarial nli: A new benchmark for natural language understanding. arXiv preprint\narXiv:1910.14599, 2019.\nJoan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, Andr´ e Susano Pinto,\nSylvain Gelly, Daniel Keysers, and Neil Houlsby. Scalable transfer learning with expert\nmodels. arXiv preprint arXiv:2009.13239 , 2020.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training, 2018.\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683 , 2019.\nSamyam Rajbhandari, Jeﬀ Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory opti-\nmization towards training a trillion parameter models. arXiv preprint arXiv:1910.02054,\n2019.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+\nquestions for machine comprehension of text. arXiv preprint arXiv:1606.05250 , 2016.\nPrajit Ramachandran and Quoc V Le. Diversity and depth in per-example routing models.\nIn International Conference on Learning Representations, 2018.\nHerbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the\nAmerican Mathematical Society, 58(5):527–535, 1952.\n38\nSwitch Transformers\nAdam Roberts, Colin Raﬀel, and Noam Shazeer. How much knowledge can you pack into\nthe parameters of a language model? arXiv preprint arXiv:2002.08910 , 2020.\nClemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive\nselection of non-linear functions for multi-task learning. arXiv preprint arXiv:1711.01239,\n2017.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 34, pages 8732–8740, 2020.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter, 2019.\nNoam Shazeer. Glu variants improve transformer, 2020.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoﬀrey Hin-\nton, and Jeﬀ Dean. Outrageously large neural networks: The sparsely-gated mixture-of-\nexperts layer. arXiv preprint arXiv:1701.06538 , 2017.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn\nKoanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliﬀ Young, et al.\nMesh-tensorﬂow: Deep learning for supercomputers. In Advances in Neural Information\nProcessing Systems, pages 10414–10423, 2018.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using\ngpu model parallelism. arXiv preprint arXiv:1909.08053 , 2019.\nNitish Srivastava, Geoﬀrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. Dropout: a simple way to prevent neural networks from overﬁtting.\nJournal of Machine Learning Research , 15(1):1929–1958, 2014. URL http://www.cs.\ntoronto.edu/~rsalakhu/papers/srivastava14a.pdf.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations\nfor deep learning in nlp. arXiv preprint arXiv:1906.02243 , 2019.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive\nattention span in transformers. arXiv preprint arXiv:1905.07799 , 2019.\nRich Sutton. The Bitter Lesson.http://www.incompleteideas.net/IncIdeas/BitterLesson.html,\n2019.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Stanford\nUniversity, 2018.\nWilson L Taylor. “cloze procedure”: A new tool for measuring readability. Journalism\nquarterly, 30(4):415–433, 1953.\n39\nFedus, Zoph and Shazeer\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances\nin neural information processing systems , pages 5998–6008, 2017.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bow-\nman. Glue: A multi-task benchmark and analysis platform for natural language under-\nstanding. arXiv preprint arXiv:1804.07461 , 2018.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-\npurpose language understanding systems. In Advances in Neural Information Processing\nSystems, pages 3266–3280, 2019.\nShibo Wang and Pankaj Kanwar. Bﬂoat16: The secret to high performance on cloud tpus.\nGoogle Cloud Blog, 2019.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,\nAditya Barua, and Colin Raﬀel. mt5: A massively multilingual pre-trained text-to-text\ntransformer. arXiv preprint arXiv:2010.11934 , 2020.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and\nQuoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding,\n2020.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santi-\nago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird:\nTransformers for longer sequences. arXiv preprint arXiv:2007.14062 , 2020.\n40",
  "topic": "Speedup",
  "concepts": [
    {
      "name": "Speedup",
      "score": 0.8002458810806274
    },
    {
      "name": "Computer science",
      "score": 0.7949891686439514
    },
    {
      "name": "Reuse",
      "score": 0.6529406309127808
    },
    {
      "name": "Transformer",
      "score": 0.6089218258857727
    },
    {
      "name": "Scaling",
      "score": 0.555837869644165
    },
    {
      "name": "Language model",
      "score": 0.48917725682258606
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3788003623485565
    },
    {
      "name": "Computer engineering",
      "score": 0.35013335943222046
    },
    {
      "name": "Machine learning",
      "score": 0.34437239170074463
    },
    {
      "name": "Parallel computing",
      "score": 0.22768008708953857
    },
    {
      "name": "Electrical engineering",
      "score": 0.09313327074050903
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}