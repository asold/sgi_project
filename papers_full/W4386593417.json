{
    "title": "Validity of the large language model ChatGPT (GPT4) as a patient information source in otolaryngology by a variety of doctors in a tertiary otorhinolaryngology department",
    "url": "https://openalex.org/W4386593417",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3189984020",
            "name": "Jacob P. S. Nielsen",
            "affiliations": [
                "Rigshospitalet",
                "Copenhagen University Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A1206747743",
            "name": "Christian von Buchwald",
            "affiliations": [
                "Rigshospitalet",
                "Copenhagen University Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2594916538",
            "name": "Christian Grønhøj",
            "affiliations": [
                "Rigshospitalet",
                "Copenhagen University Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4200535791",
        "https://openalex.org/W4367310920",
        "https://openalex.org/W2902199828",
        "https://openalex.org/W4324020464",
        "https://openalex.org/W4379136378",
        "https://openalex.org/W2913713488"
    ],
    "abstract": "A high number of patients seek health information online, and large language models (LLMs) may produce a rising amount of it. This study evaluates the performance regarding health information provided by ChatGPT, a LLM developed by OpenAI, focusing on its utility as a source for otolaryngology-related patient information. A variety of doctors from a tertiary otorhinolaryngology department used a Likert scale to assess the chatbot’s responses in terms of accuracy, relevance, and depth. The responses were also evaluated by ChatGPT. The composite mean of the three categories was 3.41, with the highest performance noted in the relevance category (mean = 3.71) when evaluated by the respondents. The accuracy and depth categories yielded mean scores of 3.51 and 3.00, respectively. All the categories were rated as 5 when evaluated by ChatGPT. Despite its potential in providing relevant and accurate medical information, the chatbot’s responses lacked depth and were found to potentially perpetuate biases due to its training on publicly available text. In conclusion, while LLMs show promise in healthcare, further refinement is necessary to enhance response depth and mitigate potential biases.",
    "full_text": null
}