{
  "title": "SyFormer: Structure-Guided Synergism Transformer for Large-Portion Image Inpainting",
  "url": "https://openalex.org/W4393148515",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5100440114",
      "name": "Jie Wu",
      "affiliations": [
        "Zhejiang University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5053079749",
      "name": "Yuchao Feng",
      "affiliations": [
        "Zhejiang University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5012632036",
      "name": "Honghui Xu",
      "affiliations": [
        "Zhejiang University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5112907511",
      "name": "Chuanmeng Zhu",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5026233608",
      "name": "Jianwei Zheng",
      "affiliations": [
        "Zhejiang University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221151705",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4221049315",
    "https://openalex.org/W4318767292",
    "https://openalex.org/W3193807319",
    "https://openalex.org/W4309287184",
    "https://openalex.org/W4384201152",
    "https://openalex.org/W2766527293",
    "https://openalex.org/W3176219570",
    "https://openalex.org/W4221154378",
    "https://openalex.org/W3011757465",
    "https://openalex.org/W2798365772",
    "https://openalex.org/W4327671723",
    "https://openalex.org/W2991377405",
    "https://openalex.org/W6704369950",
    "https://openalex.org/W2969017826",
    "https://openalex.org/W6755188876",
    "https://openalex.org/W3199003182",
    "https://openalex.org/W3136958399",
    "https://openalex.org/W6757934690",
    "https://openalex.org/W6846935491",
    "https://openalex.org/W3136952737",
    "https://openalex.org/W6747977641",
    "https://openalex.org/W3157168116",
    "https://openalex.org/W4221004921",
    "https://openalex.org/W6740934225",
    "https://openalex.org/W4327673664",
    "https://openalex.org/W3158999687",
    "https://openalex.org/W4360889388",
    "https://openalex.org/W4312383858",
    "https://openalex.org/W2964148878",
    "https://openalex.org/W2981682056",
    "https://openalex.org/W4312238440",
    "https://openalex.org/W4312161071",
    "https://openalex.org/W4386075524",
    "https://openalex.org/W3043547428",
    "https://openalex.org/W3106725490",
    "https://openalex.org/W3206082266",
    "https://openalex.org/W4287254423",
    "https://openalex.org/W2891485065",
    "https://openalex.org/W2963420272",
    "https://openalex.org/W2732026016",
    "https://openalex.org/W4294643831",
    "https://openalex.org/W3203538104",
    "https://openalex.org/W4382459088",
    "https://openalex.org/W4382240256"
  ],
  "abstract": "Image inpainting is in full bloom accompanied by the progress of convolutional neural networks (CNNs) and transformers, revolutionizing the practical management of abnormity disposal, image editing, etc. However, due to the ever-mounting image resolutions and missing areas, the challenges of distorted long-range dependencies from cluttered background distributions and reduced reference information in image domain inevitably rise, which further cause severe performance degradation. To address the challenges, we propose a novel large-portion image inpainting approach, namely the Structure-Guided Synergism Transformer (SyFormer), to rectify the discrepancies in feature representation and enrich the structural cues from limited reference. Specifically, we devise a dual-routing filtering module that employs a progressive filtering strategy to eliminate invalid noise interference and establish global-level texture correlations. Simultaneously, the structurally compact perception module maps an affinity matrix within the introduced structural priors from a structure-aware generator, assisting in matching and filling the corresponding patches of large-proportionally damaged images. Moreover, we carefully assemble the aforementioned modules to achieve feature complementarity. Finally, a feature decoding alignment scheme is introduced in the decoding process, which meticulously achieves texture amalgamation across hierarchical features. Extensive experiments are conducted on two publicly available datasets, i.e., CelebA-HQ and Places2, to qualitatively and quantitatively demonstrate the superiority of our model over state-of-the-arts.",
  "full_text": "SyFormer: Structure-Guided Synergism Transformer for Large-Portion Image\nInpainting\nJie Wu1, Yuchao Feng1, Honghui Xu1, Chuanmeng Zhu2, Jianwei Zheng1*,\n1Zhejiang University of Technology\n2Zhejiang University\nwuj@zjut.edu.cn, fyc@zjut.edu.cn, xhh@zjut.edu.cn, cmzhu@zju.edu.cn, zjw@zjut.edu.cn\nAbstract\nImage inpainting is in full bloom accompanied by the\nprogress of convolutional neural networks (CNNs) and trans-\nformers, revolutionizing the practical management of abnor-\nmity disposal, image editing, etc. However, due to the ever-\nmounting image resolutions and missing areas, the challenges\nof distorted long-range dependencies from cluttered back-\nground distributions and reduced reference information in im-\nage domain inevitably rise, which further cause severe per-\nformance degradation. To address the challenges, we propose\na novel large-portion image inpainting approach, namely the\nStructure-Guided Synergism Transformer (SyFormer), to rec-\ntify the discrepancies in feature representation and enrich the\nstructural cues from limited reference. Specifically, we devise\na dual-routing filtering module that employs a progressive fil-\ntering strategy to eliminate invalid noise interference and es-\ntablish global-level texture correlations. Simultaneously, the\nstructurally compact perception module maps an affinity ma-\ntrix within the introduced structural priors from a structure-\naware generator, assisting in matching and filling the corre-\nsponding patches of large-proportionally damaged images.\nMoreover, we carefully assemble the aforementioned mod-\nules to achieve feature complementarity. Finally, a feature\ndecoding alignment scheme is introduced in the decoding\nprocess, which meticulously achieves texture amalgamation\nacross hierarchical features. Extensive experiments are con-\nducted on two publicly available datasets, i.e., CelebA-HQ\nand Places2, to qualitatively and quantitatively demonstrate\nthe superiority of our model over state-of-the-arts.\nIntroduction\nGiven corrupted images, inpainting technique endeavors to\nrestore obscure regions with semantically coherent contents.\nPropelled by rapid advancements in digital media, this tech-\nnique has garnered significant attention and found wide-\nranging applications in real-world scenarios, such as pic-\nture editing (Song et al. 2019), restoration of polluted im-\nages(Jin et al. 2023; Feihong et al. 2023), and disposal of\nundesired objects (Li, Wang, and Hu 2021). However, with\nongoing iterations of hardware and software, the captured\nimages enjoy a consistently higher resolution, which poses\nnew challenges to the mission. In practice, high resolution\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(a)\n (b)\n (c)\nFigure 1: Inpainted results in cases of background distur-\nbance (upper row) and large missing regions (bottom row).\n(a) Corrupted images. (b) Inpainted results. (c) GT.\nimplies more information as well as more complex prop-\nerties, whereas the input incompleteness inevitably suffers\ndrastic changes in feature distribution, which further leads to\ncluttered image domain and brings about a catastrophic per-\nformance drop for existing methods. Especially in cases with\nlarge-proportion pixel loss, most previous research loses its\noriginal luster due to the severe scarcity of available refer-\nences.\nTo address these challenges, massive studies based on\nconvolutional neural networks (CNNs) (Xiong et al. 2019;\nJin et al. 2023) have come up with well-thought-out ideas.\nThe main solution is to add auxiliary priors to guide pixel\nfilling on top of the powerful learning capability of CNNs,\nwhich proves pivotal in mitigating the issue of limited in-\nformation within the occluded regions. In particular, the\nauxiliary information is typically inspired by contextual\ncues, e.g., edge maps (Nazeri et al. 2019), semantic maps\n(Liao et al. 2020), etc. Following this line, most current ap-\nproaches bifurcate the inpainting process into two branches.\nThe initial one revolves around the latent cues associated\nwith the impaired regions, succeded by the intricate texture\nsynthesis. For example, CTSDG (Guo, Yang, and Huang\n2021) develops a parallel architecture that simultaneously\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6021\nmodels intermediate prior generation and texture inpainting.\nAdditionally, Lama (Suvorov et al. 2022) introduces an in-\nnovative Fast Fourier Convolution (FFC) module, harness-\ning frequency domain transformations to extract high recep-\ntive field structures at minimal computational expense. De-\nspite the integration of auxiliary priors to mitigate the chal-\nlenge of data scarcity, these methods remain insufficient to\nfully excavate the precise information concealed within in-\ntricate image distributions. Furthermore, they are suscepti-\nble to entrapment within localized matching predicaments.\nThis phenomenon is attributed to the intrinsic inductive bias\ninherent in CNNs, which restricts their efficacy in captur-\ning long-range dependencies. In light of these insights, there\nexists significant scope unexplored for the more comprehen-\nsive apprehension of global image distributions.\nIn contrast to CNNs, vision transformer (Dosovitskiy\net al. 2020) has emerged as a formidable instrument for\nmodeling non-local dependencies, sparking a revolution in\nthe community of computer vision. Leveraging the power of\nself-attention, researchers have devised tailored approaches\n(Yu et al. 2021; Wan et al. 2021) to achieve high-fidelity\ninpainting outcomes. However, these methods show appeal-\ning performance only when dealing with low-resolution im-\nages, e.g., 256 × 256. High-quality inpainting of incoherent\nbackground textures remains a challenge when faced with\nhigh-resolution inputs, e.g., 512 × 512. For instance, MAT\n(Li et al. 2022) exclusively uses pertinent tokens to model\nthe contextual information during the low resolution stage,\nfollowed by the dynamic update stage. Yet, it still leaves the\nquestion of whether low-resolution self-attention can be per-\nfectly matched to high-resolution scenes. On the contrary,\nZITS (Dong, Cao, and Fu 2022) uses transformer directly to\nobtain holistic edges rather than acting on the detailed in-\npainting process. Although these treatments have opened a\nnew avenue for high-resolution inpainting, the efficient mod-\neling of the global incoherent textures, especially in cases\nwith large-proportion occlusions, is still in short considera-\ntion. Moreover, transformer-based approaches also face re-\nsource constraints when the input resolution increases, as\nthe attention mechanism necessitates pairwise affinity com-\nputations across all spatial positions, resulting in exorbitant\ncomputational complexity and memory requirements. This\npressing concern necessitates the formulation of a judicious\nmethodology capable of effectively tackling high-resolution\nimage-inpainting scenarios. Some studies (Zhu et al. 2023)\nfocus on improving the functioning of transformers to tackle\nthe problems mentioned above. In their self-attention calcu-\nlations, each query is linked to a restricted number of key-\nvalue pairs, thus decreasing the computational load.\nInspired by these insights, we propose a new large-portion\ninpainting approach, namely Structure-Guided Synergism\nTransformer (SyFormer), which taps the potential of model-\ning comprehensive contexts and capturing prototypical cues.\nTo achieve this, we decouple image inpainting into two\nstages: 1) A structure-aware generator is deployed, which\ntogether with a Contextual Attention Block (CAB) engen-\nders coarse representations of the impaired inputs. In con-\ntrast to the predominant techniques that rely on edge maps\nas reconstruction priors (Nazeri et al. 2019), our approach\ncrafts structure maps imbued with richer semantics, thus\nmitigating the intrinsic information paucity endemic to im-\nage domains. 2) A synergism transformer is proposed to\nsimultaneously process the corrupted image and the struc-\nture map in a parallel symphony, effectively resolving entan-\nglements and enhancing the completion performance. More\nimportantly, within the SyFormer framework, a Structurally\nCompact Perception (SCP) module plays the central role,\nby a lightweight self-attention mechanism, which molds\nthe global correlations of initially generated features and\nthen guides the adaptive matching of corresponding patches\nfrom the original image. In addition, a Dual-Routing Fil-\ntering (DRF) module is designed to dynamically appraise\nthe region-wise affinity, which further aggregates a prede-\ntermined tally of region tokens and enables global model-\ning of complex image distributions. Note that the overall\nmechanism facilitates seamless collaboration between the\ntwo attention modules, affording an extensive exploration\nof the inherent pixel entwinements. Besides, for better tex-\nture quality, we delve into a Feature Decoding Alignment\n(FDA) methodology that effectively amalgamates feature in-\nformation from hierarchical levels. As shown in Fig. 1, our\nSyFormer is capable of removing unwanted objects and fill-\ning in the large holes with visually authentic content. The\nmain contributions are given as follows.\n• We propose a dual-stage architecture to tackle the chal-\nlenge of limited reference information due to large areas\nof missing pixels. The initial stage is dedicated to the ex-\ntraction of structural cues as prior information. Then, a\nparallel-disentanglement framework is introduced, which\nconcurrently processes the harmonious structure and cor-\nrupted images, enabling a meticulous examination and\ndiscernment of visual properties.\n• To overcome the challenges posed by chaotic back-\nground distributions, a lightweight synergism trans-\nformer is proposed, whose main architecture can be di-\nvided into two parallel branches. In terms of the cor-\nrupted image, the lower branch introduces a dual-routing\nfiltering module that employs a coarse-to-fine scheme\nto achieve specificity of large-proportion incoherent tex-\ntures. In terms of structure map, the upper branch lever-\nages a structurally compact perception module to capture\ncontextual semantic information through global mod-\neling, thereby guiding the matching of corresponding\npatches.\n• To facilitate precise texture synthesis, a feature decoding\nalignment scheme is designed, which gradually blends\nthe multiscale spatial-channel features to achieve the in-\ntact finals. Extensive experiments on CelebA-HQ (Kar-\nras et al. 2018) and Places2 (Zhou et al. 2017) datasets\nare conducted for assessment, whose results demonstrate\nthat our model significantly outperforms state-of-the-art\napproaches, both qualitatively and quantitatively.\nRelated Work\nAuxiliary-Prior Inpainting\nImage inpainting, as a long-standing problem in computer\nvision, has made significant strides in recent years with the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6022\nFigure 2: The overall architecture of SyFormer. A disentanglement architecture processes both the structure map and the original\ninput in a parallel manner, allowing meticulous semantic examination of contextual features.\nadvent of deep learning techniques based on convolutional\nneural networks (CNNs) (Zuo et al. 2023; Pathak et al. 2016;\nJin et al. 2022). In contrast to earlier approaches those de-\npend heavily on hand-crafted priors (Xu et al. 2021, 2022),\ndeep learning-based methods have demonstrated great po-\ntential in extracting hierarchical representations from im-\nages. This enables the reconstruction of damaged content\nwith a high-level understanding of the underlying seman-\ntics. However, while remarkable performance boosts have\nbeen achieved, challenges still exist due to the inherent un-\ncertainty when large proportions of pixels are missed. In ad-\ndition, the absence of a well-defined constraint to facilitate\na better convergence is also a daunting issue. To that end,\nvarious auxiliary sources of prior information have been ex-\nplored, such as edge maps, foreground contours (Xiong et al.\n2019), and structural guidance (Ren et al. 2019), which are\ncurrently known as the key ingredient for performance guar-\nantee. Specifically, EdgeConnect (Nazeri et al. 2019) de-\nvised an edge generator to obtain contour sketches, which\ntogether with a texture generator contributes to a stable re-\nconstruction. Analogously, CTSDG (Guo, Yang, and Huang\n2021) introduced a dual-stream network to couple structure-\nconstrained texture synthesis and texture-guided structure\nreconstruction. Using the power of FFC, Lama (Suvorov\net al. 2022) achieves remarkable efficacy in high-resolution\nimage inpainting tasks, successfully reconciling complex\ndetails and significant damage. In this work, we also em-\nploy the structure map as an auxiliary prior. However, rather\nthan merely joining the auxiliary prior with the damaged im-\nage through concatenation, we map the structural prior to an\naffinity matrix to help fill in the damaged areas.\nVisual Transformer Inpainting\nTransformer has made remarkable achievements in the field\nof natural language processing (NLP) by virtue of the im-\npressive capacity to model long-range dependencies. Re-\ncently, its applicability has been greatly extended to the\nrealm of computer vision. For instance, Vision Transformer\n(ViT) (Dosovitskiy et al. 2020) demonstrated that pure trans-\nformer networks can achieve the same level of classifica-\ntion performance as the CNN-based counterparts. More re-\ncently, massive efforts have been dedicated to mitigate the\nheavy computational burden of self-attention, including dis-\ncrete representation learning (Ramesh et al. 2021) and some\nlinear treatments (Feng et al. 2023). Moreover, transformers\nhave gained widespread adoption in the field of image in-\npainting, enjoying superior reconstruction quality with finer\ndetails. Representatively, Yu et al. (Yu et al. 2021) employed\nreordering techniques and autoregressive modeling to ef-\nfectively capture global priors from both valid and masked\npixels. Specific to the high-resolution challenges, MAT (Li\net al. 2022) introduced a dynamic mask updating mechanism\nto incorporate long-range dependencies across relevant to-\nkens. Rather than using a transformer to generate intermedi-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6023\nFigure 3: Illustration of the synergism transformer.\nate clues and then employing a CNN for texture filling, our\nresearch instead uses a transformer variant throughout the\nentire filling process. This enables us to create a parallelized\ndisentanglement architecture that enriches information rep-\nresentations.\nMethodology\nThe goal of inpainting is to restore the corrupted regions\nof a masked image, IM = I ⊙ M, by filling in visually\nplausible content that is consistent with the context, where\nI is the ground truth and M denotes the mask. As shown in\nFig. 2, the Structure-Guided Synergism Transformer is made\nup of two parts: a structure-aware generator that produces\nrough sketches and a synergism transformer that generates\nrealistic texture details. The U-shaped architecture of the\nstructure-aware generator allows it to investigate the intrin-\nsic connections within the image, enabling the production of\nstructural maps ISTRU . Taking into account both the coarse\nstructure ISTRU and the corrupted imageIM , the synergism\ntransformer facilitates the synthesis of high-resolution fine-\ngrained textures to produce the inpainted result Iret. In par-\nticular, our synergism transformer architecture further con-\nsists of two concurrents, i.e. structurally compact perception\nand dual-routing filtering. Finally, a feature decoding align-\nment technique is applied to amalgamate texture informa-\ntion across neighboring layers and rectify semantic discrep-\nancies.\nStructure-Aware Generator\nThe bottom segment of Fig. 2 provides the entire pipeline of\nstructure-aware generator. The paired inputs of the masked\nimage and an initial sketch are fed into a U-shaped encoder-\ndecoder architecture. To discover the latent possibilities of\ncoherent structure within damaged regions, encoded low\nresolution features undergo the contextual attention block\n(CAB) (Yu et al. 2018), which facilitates the learning of an\naffinity matrix surrounding damaged regions, thus enhanc-\ning the extraction of attention for potential texture patches.\nFollowing the mechanism of CAB, we first extract 3 ×3\npatches both from the intact and corrupted region of the en-\ncoded feature mapF, and then compute the cosine similarity\nbetween the patches:\nSi,j\natt =\n\u001c pi\n∥pi∥2\n, pj\n∥pj∥2\n\u001d\n, (1)\nwhere pi is the i-th patch taken from the intact region, while\npj is the j-th patch of the corrupted region. The attention\nscore of each patch is then calculated by applying softmax\nto the cosine similarity Si,j\natt:\n˜Si,j\natt = exp(Si,j\natt)PN\nj=1 exp(Si,j\natt)\n, (2)\nwhere N is the number of patches outside the missing hole.\nAfter obtaining the attention scores from F, the deconvo-\nlution operation is used to fill the corresponding holes of the\nlow-level feature maps with weighted contextual patches.\nThe process can be formulated as follows:\nˆpi =\nNX\nj=1\nˆpj ˜Si,j\natt, (3)\nwhere ˆpj denotes a 3×3 patch extracted from low-level fea-\nture maps Fl, and ˆpi is the newly learned patch.\nSynergism Transformer\nStructurally Compact Perception Note again that the\nprimary aim of image inpainting is to systematically search\nfor prospective pixels that fill the masked regions. However,\nconventional deep inpainting methods tend to encounter ob-\nstacles such as insufficient in-domain information for ac-\ncurate pixel recovery in cases of large-portion pixel miss-\ning. Accordingly, to effectively guide pixel matching within\nthe damaged regions and mitigate interference from mask-\nrelated noise, we incorporate the structure map ISTRU gen-\nerated in the first stage as an extra guide. In practice, as illus-\ntrated in Fig. 3, we conceive the SCP module that is rooted\nin typical self-attention mechanism (as given in Eq. (4)):\nselfAttention(Q, K, V ) =Softmax (Q · KT ) · V ,\n(4)\nwhere sequences Q, K, V ∈ RN×C are attained by three\nlearnable weight matrices, N = H × W(C ≪ N), H and\nW are the height and width of the features, C is the chan-\nnel dimension, and T represents the transpose operation. It\nis notorious that Eq. (4) suffers from O(N2) space and time\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6024\nFigure 4: Feature decoding alignment block.\ncomplexity, which imposes substantial resource consump-\ntion, particularly in cases of high-resolution inputs.\nTo control the computational consumption, we commence\nby employing the Deformable Downsample Update Module\n(Jin et al. 2023) to extract multi-scale feature information,\nIm and Istru, from the original inputs, IM and ISTRU , re-\nspectively. For the structural image Istru, a C/2 convolu-\ntional kernel with reduced channel dimension is used to ac-\ncess the intrinsic feature I1. Furthermore, I1 is augmented\nwith an additional map of auxiliary features I2, earning for\nthe progressive restoration through partial convolution op-\nerations of Im. The introduction of the progressively en-\nriched features I2 plays the role as adding an adversarial\nconvolutional process between newly filled pixels and the\nexisting structural pixels, thus improving the effectiveness\nof the prior cues. Subsequently, to achieve enhanced fea-\nture compression, we incorporate the operations of multi-\nlevel pooling and linear projections. At this juncture, three\nfeature sequences K1 ∈ RC/8×L, V1 ∈ RC/2×L and\nQ1 ∈ RC/2×(H×W) (L ≪ H × W) are acquired that have\nundergone compression. Then, specific self-attention scores\nA1 can be obtained using Q1 and K1:\nA1 = QT\n1 · K1 = (Conv(I1))T · Com (ε (I1)) , (5)\nwhere Conv (·) and Com (·) stand for convolution and com-\npression operations, respectively. ε (·) denotes the multi-\nlevel pooling. Finally, the feature map Iout1 is derived by\naggregating global structural information and semantic dis-\ncriminative features:\nIout1 = SCP (I1) =ε (I1) · Softmax (A1)T + I1. (6)\nLikewise, the progressively inpainted feature map Iout2 can\nbe acquired through the same procedure from I2.\nDual-Routing Filtering Current methods often directly\napply transformer to incomplete inputs, overlooking the fact\nthat the fundamental nature of transformer is to discover\ninterdependencies between tokens. Consequently, patches\nwith a substantial number of missing pixels would cause\npronounced errors during self-attention computations. To es-\ntablish the inter-patch connections and meanwhile avoid the\ninterference caused by mask noise, we design a dual-routing\nfiltering module, as depicted in the lower segment of Fig.\n3. Initially, to control the computational burden, the channel\ndimension of the corrupted input is reduced through a convo-\nlution operation. Afterwards, operations of window partition\nand weight mapping are executed, resulting in the genera-\ntion of non-overlapping sets of query, key, and value tensors,\nQw, Kw, V w ∈ RHW/s 2×C/2. These operations enable\nthe model to focus on valuable tokens over long distances,\nthereby fostering robust contextual relationships. The ten-\nsors Qw and Kw are then put through linear projection of\nflattened patches (LPFP) and average operations to obtain\nthe region-level responses Qmean, Kmean, which reduce\nthe interference from large amount of masking. Finally, the\nadjacency matrix A2, representing the region-level affinity\ngraph, can be calculated using the following formula:\nA2 = Softmax\n\u0010\nKmean · (Qmean)T\n\u0011\n. (7)\nBased on A2, most of the ineffective areas can be filtered\nout, with the more pertinent regions reserved. To that end,\nwe transform A2 with a gating threshold to a sparse affinity\nmatrix Fr, allows us to reshape a more refined ourcome of\nthe original image. The process is described as follows:\nIr = δ (Fr) =δ (F ilter(A2)) , (8)\nwhere δ (·) denotes the reshaping operation. On that ba-\nsis, each query establishes strong ties with tokens from\nthe most pertinent areas. By exclusively considering ele-\nments within these regions and performing aggregation op-\nerations, DRF is able to achieve global pixel matching,\nparticularly in scenarios with inconsistent textures. Simul-\ntaneously, the implementation of this filtering-aggregation\nmechanism successfully mitigates the computational bur-\nden of transformer. Specifically, we perform aggregation\nof pixels from affinity regions to derive the corresponding\nKr ∈ Rs2×kHW/s 2×C/2 and Vr ∈ Rs2×kHW/s 2×C/2. The\nresultant feature map Iout3 can be derived by computing the\nfollowing self-attention formula:\nIout3 = Vr · AT\n3 = Vr · Softmax\n\u0000\nQT\nr · Kr\n\u0001T\n. (9)\nUltimately, the overall outcome can be generated by\nIret = σ (Conv (Concat (Iout1, Iout2, Iout3))) , (10)\nwhere σ (·) denotes the Tanh activation function.\nFeature Decoding Alignment The intricate details\npresent in the multi-level features from SyFormer, de-\nnoted as Iret, are crucial for the successful decoding of\nrealistic images. Among them, lower-level features with\nhigh resolution Ihigh offer the more refined textures, but\nmay contain inconsistent content that leads to inaccurate\nsampling results. On the other hand, higher-level features\nIlow encompass a wide field of view and contain a lot of\nsemantic information, which also assists in refining textures.\nThat is to say, purely using low-level features or simply\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6025\n(a) Input\n (b) GT\n (c) A\nOT\n (d) Big-Lama\n (e) Coordfill\n (f) Lama\n (g) MADF\n (h) MA\nT\n (i) ours\nFigure\n5: Qualitative comparisons on representative examples from CelebA-HQ and Places2. (a) Corrupted inputs, (b) GT\nimages, (c) AOT, (d) Big-Lama, (e) Coordfill, (f) Lama, (g) MADF, (h) MAT, and (i) our SyFormer.\nupsampling high-level features both cause severe distortions\ndue to the limited information during the decoding phase.\nFollowing these perspectives, efficiently matching multi-\nlevel features become a pivotal step in promoting the inpait-\ning quality. Thus, we introduce feature decoding alignment\ntechnique as shown in Fig. 4. Specifically, the size ofIlow is\nfirstly upsampled to the resolution of Ihigh. Afterwards, to\ngenerate lightweight sequences,Kc, Vc ∈ RM×(H×W), and\nQc ∈ RC×(H×W), a series of convolutions and linear pro-\njection operations are used. Following this, Eq. (4) is used to\ncalculate channel-level attention scores Ac, which is further\nused to generate feature Is. Moreover, to establish multi-\nscale channel-spatial relationships, global spatial-attention\noperations are applied to Is. Specifically, Ks, Qs, and Vs\nare generated using analogous feature mapping and lin-\near transformation. Drawing insights from the self-attention\ncomputation in Eq. (4), a feature response output character-\nized by both spatial and channel-level attributes is achieved.\nLoss Functions For rendering visual authenticity and\nglobal continuity, our loss function is a mixture of perceptual\nloss, style loss, reconstruction loss, and adversarial loss.\n• Perceptual Loss Perceptual Loss is implemented by\nthe pre-trained VGG16 model, which can optimize the\nglobal structure of the image. The specific formula with\nl1 norm is:\nLper =\nX\nr\n∥ϕr (Iret) − ϕr (I)∥1, (11)\nwhere ϕr() denotes the r-th pooling layer of VGG16, and\nthe value range of pool r is {1, 2, 3} in our implementa-\ntion.\n• Style Loss Style consistency is controlled with the fol-\nlowing constraint with φr (·) =ϕT\nr (·) ϕr (·).\nLsty =\nX\nr\n∥φr (Iret) − φr (I)∥1. (12)\n• Reconstruction Loss The standard reconstruction loss\nis defined as the average absolute error of Iret and the\ntruth image I, which easily results in problems such as\nartifacts, distorted contours, etc. Thus, we update the re-\nconstruction loss as follows:\nLrec =\nX\nx,y∈M\n∥Ix,y\nret − Ix,y∥1 +\nX\nx,y/∈M\n∥Ix,y\nret − Ix,y∥1,\n(13)\nwhere x, y∈ M denotes the location (x, y) in the cor-\nrupted region M.\n• Adversarial LossTo generate the more realistic details,\nadversarial loss is also involved, such as:\nLadv = E [log (1− Dw (Iret))] + E [logDw (I)] ,\n(14)\nwhere D is the discriminator parameterized by w.\nWith all the sub-loss prepared, the final Lhybrid can be\nexpressed as:\nLhybrid = λperLper+λrecLrec+λstyLsty+λadvLadv,\n(15)\nwhere λper, λrec, λsty, and λadv are all hyper-parameters\nfor balancing the contribution of each sub-loss function.\nExperiments\nDatasets and Metrics\nTwo well-known datasets, i.e., CelebA-HQ (Karras et al.\n2018) and Place2 (Zhou et al. 2017), are used for the perfor-\nmance investigation. Images of these datasets cover a wide\nrange of scenes and contents, allowing us to train a more\ngeneral model for real-world applications. The CelebA-HQ\ndata is split into training, validation, and test sets in a ratio of\n24:1:5. We keep 220,000 and 5000 images from the original\nplaces2 sets for training and testing, respectively. The spatial\nresolution of these images are adjusted to 512×512 by crop-\nping and scaling. Similar to (Liu et al. 2018), 6000 irregular\nmasks are generated, whose covering area occupies 30%-\n60% of the total image. In addition, the masks are equally\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6026\nMetric SSIM(↑) PSNR(↑) MAE(↓) LPIPS(↓)\nMask Ratio 30-40% 40-50% 50-60% 30-40% 40-50% 50-60% 30-40% 40-50% 50-60% 30-40% 40-50% 50-60%\nCelebA-HQ\nAOT 0.9193 0.8774 0.7792 27.4578 25.2925 22.0181 0.0193 0.0278 0.0471 0.0767 0.1089 0.1707\nLama 0.8996 0.8508 0.7426 26.4725 24.4377 21.3557 0.0258 0.0351 0.0565 0.1009 0.1369 0.2017\nMAT 0.9098 0.8659 0.7698 26.8392 24.7488 21.6119 0.0204 0.0292 0.0482 0.0779 0.1082 0.1644\nMADF 0.8958 0.8437 0.7285 26.3758 24.2502 20.7712 0.0224 0.0325 0.0565 0.1137 0.1594 0.2374\nCoordfill 0.9151 0.8739 0.7781 27.5136\n25.5431 22.4907 0.0196 0.0277 0.0454 0.0887 0.1217 0.1802\nBig-Lama 0.9022 0.8511 0.7221 26.6291 24.4942 20.9228 0.0246 0.0335 0.0583 0.0921 0.1292 0.2107\nours 0.9184\n0.8818 0.8001 27.9467 26.0252 22.6334 0.0187 0.0259 0.0408 0.0763 0.1084 0.1640\nPlaces2\nAOT 0.8291 0.7668 0.6746 23.3894 21.4761 18.8301 0.0328 0.0465 0.0726 0.1214 0.1691 0.2562\nLama 0.8483 0.7841 0.6581 23.4259 21.7237 19.3171 0.0389 0.0506 0.0742 0.1245 0.1689 0.2534\nMAT 0.8156 0.7377 0.6001 21.9216 20.0850 17.4618 0.0382 0.0539 0.0839 0.1337 0.1841 0.2675\nMADF 0.8405 0.7705 0.6403 23.1857 21.4208 18.9999 0.0332 0.0463 0.0702 0.1384 0.1917 0.2823\nCoordfill 0.8498 0.7844 0.6634 23.7492\n22.0542 19.7105 0.0315 0.0436 0.0663 0.1324 0.1801 0.2531\nBig-Lama 0.8529 0.7905 0.6663 23.6102 21.9125 19.5266 0.0379 0.0491 0.0717 0.1172 0.1591 0.2397\nours 0.8552 0.7922 0.6695 24.0885 22.3871 19.9352 0.0314 0.0431 0.0639 0.1161 0.1685 0.2423\nTable 1: Quantitative comparison on CelebA-HQ and Places2 datasets (↑Higher is better,↓Lower is better). The best values are\nhighlighted by boldface and the second-best values are highlighted by underlines.\ndivided into three parts according to different hole-to-image\nratios, e.g., 30-40%, 40-50%, and 50-60%.\nExperimental Settings\nAll experiments are conducted using PyTorch with a batch\nsize of 8. Our model is optimized by Adam with a learn-\ning rate of 2× 10−4. The hyper-parameters in Eq. (15) are\nset as λadv = 0.1, λrec = 40, λsty = 120,λper = 0.05 to\ngenerate the sensuously optimal results. All experiments are\nconducted on two GPUs of RTX 3090 with a single 12G of\nvideo memory. To substantiate the efficacy of our proposal,\nwe conduct a comprehensive comparison against state-of-\nthe-art inpainting methods. Specifically, AOT (Zeng et al.\n2023), MAT (Li et al. 2022), Coordfill (Liu et al. 2023), Big-\nLama (Suvorov et al. 2022), Lama (Suvorov et al. 2022),\nand MADF (Zhu et al. 2021) are selected as the baselines\nin this evaluation. We select four canonical metrics, includ-\ning structural similarity (SSIM), Peak Signal to Noise Ratio\n(PSNR), Mean Absolute Error (MAE), and Learned Percep-\ntual Image Patch Similarity (LPIPS), to quantitatively mea-\nsure the scores of all competitors.\nQualitative Comparison\nFig. 5 illustrates several inpainted images. Evidently, from\nthe facial instances in the top two rows, it can be observed\nthat most previous approaches, including Big-Lama, Lama,\nand MADF, produce visually defective results such as ge-\nometric distortions, blurred artifacts, incongruous eye col-\nors, or identity disparities. AOT, Coordfill, and MAT per-\nform relatively better. However, compared to these three, our\nSyFormer still stands out as producing the more visually au-\nthentic content, as can be easily witnessed from the hand\nsegment. Furthermore, with respect to the scene images in\nthe third row, the compared techniques again produce un-\nwarranted textures at fence and ceiling positions, whereas\nour outcomes evince superior stability and competitiveness.\nQuantitative Comparison\nTable 1 presents the numerical results with different mask\nratios, i.e., 30-40%, 40-50%, and 50-60%. As can be\nDataset Places2\nMask Ratio 30-40% 40-50% 50-60%\nSSIM(↑)\nw/o DRF 0.8308 0.7641 0.6414\nw/o FDA 0.8483 0.7845 0.6631\nw/o SCP 0.8361 0.7631 0.6489\nFull Model 0.8552 0.7922 0.6695\nPSNR(↑)\nw/o DRF 23.3944 21.7255 19.3818\nw/o FDA 24.0317 22.3508 19.8796\nw/o SCP 23.4699 21.8667 19.6183\nFull Model 24.0885 22.3871 19.9352\nMAE(↓)\nw/o DRF 0.0344 0.0471 0.0695\nw/o FDA 0.0323 0.0438 0.0643\nw/o SCP 0.0333 0.0462 0.0661\nFull Model 0.0314 0.0431 0.0639\nLPIPS(↓)\nw/o DRF 0.1372 0.1927 0.2831\nw/o FDA 0.1237 0.1727 0.2643\nw/o SCP 0.1369 0.1833 0.2812\nFull Model 0.1161 0.1685 0.2423\nTable 2: Ablation study on Places2. The best values are high-\nlighted by boldface and the second-best values are high-\nlighted by underlines.\nseen, compared to existing methods, our model consistently\nachieves superior or highly competitive performance across\nboth datasets. Taking PSNR as an example, the average\nvalue of SyFormer from the two datasets is significantly bet-\nter than AOT, Lama, MAT, MADF, Big-Lama, and Coordfill,\nwith an improvement of 0.7587 dB, 1.0472 dB, 1.7247 dB,\n1.3354 dB, 0.9868 dB, and 0.3258 dB, respectively.\nAblation Experiments\nTaking Places2 as an example, the roles of our proposals,\nincluding DRF, FDA, and SCP, are ablated in Table 2. The\nthree labels, i.e., “w/o DRF”, “w/o FDA”, and “w/oSCP”,\nin Table 2 denote the models with the concerned module re-\nmoved and replaced with common treatments. Functionally,\nin comparison to the model “w/o DRF”, the full version en-\njoys the ability to extract information from incomplete im-\nages and skills in global modeling of valid tokens. Similarly,\nthe structural information captured by the SCP module is\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6027\nDataset CelebA-HQ\nMask Ratio 30-40% 40-50% 50-60%\nSSIM(↑)\nw/o DRF 0.9071 0.8615 0.776\nw/o FDA 0.9159 0.8773 0.7919\nw/o SCP 0.8888 0.8517 0.7667\nFull Model 0.9184 0.8818 0.8001\nPSNR(↑)\nw/o DRF 27.1105 25.1704 22.2953\nw/o FDA 27.8494 25.9098 22.9472\nw/o SCP 26.5621 24.9421 21.7414\nFull Model 27.9467 26.0252 23.1334\nMAE(↓)\nw/o DRF 0.0209 0.0307 0.0456\nw/o FDA 0.0188 0.0262 0.0417\nw/o SCP 0.0234 0.0326 0.0501\nFull Model 0.0187 0.0259 0.0408\nLPIPS(↓)\nw/o DRF 0.1036 0.1582 0.2234\nw/o FDA 0.0896 0.1284 0.1911\nw/o SCP 0.1094 0.1588 0.2251\nFull Model 0.0763 0.1084 0.1640\nTable 3: Ablation study on CelebA-HQ. The best values are\nhighlighted by boldface and the second-best values are high-\nlighted by underlines.\nbeneficial for accurately matching pertinent patches, which\nrelieves the shortcomings caused by the lack of image do-\nmain data. Besides, the FDA module is effective in combin-\ning multi-level information. As can be seen from Table 2,\nall these advantages are evidenced. Taking PSNR as an ex-\nample, the gains of our SyFormer over “w/o DRF”, “w/o\nFDA”, and “w/o SCP” reach 0.6363 dB, 0.0495 dB, and\n0.4853 dB, respectively.\nConclusion\nIn this paper, we present a new image inpainting net-\nwork for large-portion missing case, namely the Structure-\nGuided Synergism Transformer (SyFormer), which success-\nfully overcomes the challenges of modeling incoherent im-\nage distributions and assembling information from limited\nreferences. Technically, the proposal combines the power of\npixel matching bootstrapping via structural priors with the\nrobust long-range modeling via progressive filtering strate-\ngies. Concurrently, facilitated by a multi-level upsampling\ntechnique, SyFormer culminates in achieving high-quality\ninpainting results. Experimental evaluations on CelebA-HQ\nand Places2 datasets demonstrate the superior performance\nof SyFormer over state-of-the-art methods, both qualita-\ntively and quantitatively.\nAcknowledgements\nThis work was supported in part by the Pioneer and\nLeading Goose R&D Program of Zhejiang, under Grant\n2023C01241, in part by the Key Program of Natural Science\nFoundation of Zhejiang, under Grant LZ24F030012, and in\npart by the National Natural Science Foundation of China\nunder Grant 62276232.\nReferences\nDong, Q.; Cao, C.; and Fu, Y . 2022. Incremental transformer\nstructure enhanced image inpainting with masking posi-\ntional encoding. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n11358–11368.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2020.\nAn image is worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on Learn-\ning Representations.\nFeihong, L.; Hang, C.; Kang, L.; Qiliang, D.; Jian, Z.;\nKaipeng, Z.; and Hong, H. 2023. Toward high-quality face-\nmask occluded restoration. ACM Transactions on Multime-\ndia Computing, Communications and Applications, 19(1):\n1–23.\nFeng, Y .; Jiang, J.; Xu, H.; and Zheng, J. 2023. Change\ndetection on remote sensing images using dual-branch mul-\ntilevel intertemporal network. IEEE Transactions on Geo-\nscience and Remote Sensing, 61: 1–15.\nGuo, X.; Yang, H.; and Huang, D. 2021. Image inpaint-\ning via conditional texture and structure dual generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 14134–14143.\nJin, Y .; Wu, J.; Wang, W.; Wang, Y .; Yang, X.; and Zheng,\nJ. 2022. Dense vehicle counting estimation via a synergism\nattention network. Electronics, 11(22): 3792.\nJin, Y .; Wu, J.; Wang, W.; Yan, Y .; Jiang, J.; and Zheng,\nJ. 2023. Cascading blend network for image inpainting.\nACM Transactions on Multimedia Computing Communica-\ntions and Applications. DOI: 10.1145/3608952.\nKarras, T.; Aila, T.; Laine, S.; and Lehtinen, J. 2018. Pro-\ngressive growing of GANs for improved quality, stability,\nand variation. In International Conference on Learning Rep-\nresentations.\nLi, J.; Wang, Z.; and Hu, X. 2021. Learning intact features\nby erasing-inpainting for few-shot classification. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence ,\nvolume 35, 8401–8409.\nLi, W.; Lin, Z.; Zhou, K.; Qi, L.; Wang, Y .; and Jia, J.\n2022. Mat: Mask-aware transformer for large hole image\ninpainting. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 10758–\n10768.\nLiao, L.; Xiao, J.; Wang, Z.; Lin, C.-W.; and Satoh, S. 2020.\nGuidance and evaluation: Semantic-aware image inpainting\nfor mixed scenes. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), 683–700.\nLiu, G.; Reda, F. A.; Shih, K. J.; Wang, T.-C.; Tao, A.; and\nCatanzaro, B. 2018. Image inpainting for irregular holes\nusing partial convolutions. In Proceedings of the European\nConference on Computer Vision (ECCV), 85–100.\nLiu, W.; Cun, X.; Pun, C.-M.; Xia, M.; Zhang, Y .; and Wang,\nJ. 2023. CoordFill: Efficient high-resolution image inpaint-\ning via parameterized coordinate querying. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol-\nume 37, 1746–1754.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6028\nNazeri, K.; Ng, E.; Joseph, T.; Qureshi, F.; and Ebrahimi, M.\n2019. Edgeconnect: Structure guided image inpainting using\nedge prediction. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV) Workshops ,\n3265–3274.\nPathak, D.; Krahenbuhl, P.; Donahue, J.; Darrell, T.; and\nEfros, A. A. 2016. Context encoders: Feature learning by\ninpainting. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2536–\n2544.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; V oss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-to-\nimage generation. In International Conference on Machine\nLearning, 8821–8831.\nRen, Y .; Yu, X.; Zhang, R.; Li, T. H.; Liu, S.; and Li, G.\n2019. Structureflow: Image inpainting via structure-aware\nappearance flow. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), 181–190.\nSong, L.; Cao, J.; Song, L.; Hu, Y .; and He, R. 2019.\nGeometry-aware face completion and editing. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol-\nume 33, 2506–2513.\nSuvorov, R.; Logacheva, E.; Mashikhin, A.; Remizova, A.;\nAshukha, A.; Silvestrov, A.; Kong, N.; Goka, H.; Park, K.;\nand Lempitsky, V . 2022. Resolution-robust large mask in-\npainting with fourier convolutions. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision (WACV), 2149–2159.\nWan, Z.; Zhang, J.; Chen, D.; and Liao, J. 2021. High-\nfidelity pluralistic image completion with transformers. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 4692–4701.\nXiong, W.; Yu, J.; Lin, Z.; Yang, J.; Lu, X.; Barnes, C.; and\nLuo, J. 2019. Foreground-aware image inpainting. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 5840–5848.\nXu, H.; Jiang, J.; Feng, Y .; Jin, Y .; and Zheng, J. 2022. Ten-\nsor completion via hybrid shallow-and-deep priors. Applied\nIntelligence, 1–22.\nXu, H.; Zheng, J.; Yao, X.; Feng, Y .; and Chen, S. 2021. Fast\ntensor nuclear norm for structured low-rank visual inpaint-\ning. IEEE Transactions on Circuits and Systems for Video\nTechnology, 32(2): 538–552.\nYu, J.; Lin, Z.; Yang, J.; Shen, X.; Lu, X.; and Huang, T. S.\n2018. Generative image inpainting with contextual atten-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 5505–5514.\nYu, Y .; Zhan, F.; Wu, R.; Pan, J.; Cui, K.; Lu, S.; Ma, F.; Xie,\nX.; and Miao, C. 2021. Diverse image inpainting with bidi-\nrectional and autoregressive transformers. In Proceedings\nof the 29th ACM International Conference on Multimedia ,\n69–78.\nZeng, Y .; Fu, J.; Chao, H.; and Guo, B. 2023. Aggre-\ngated contextual transformations for high-resolution image\ninpainting. IEEE Transactions on Visualization and Com-\nputer Graphics, 29(7): 3266–3280.\nZhou, B.; Lapedriza, A.; Khosla, A.; Oliva, A.; and Tor-\nralba, A. 2017. Places: A 10 million image database for\nscene recognition. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 40(6): 1452–1464.\nZhu, L.; Wang, X.; Ke, Z.; Zhang, W.; and Lau, R. W.\n2023. BiFormer: Vision transformer with bi-Level routing\nattention. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 10323–\n10333.\nZhu, M.; He, D.; Li, X.; Li, C.; Li, F.; Liu, X.; Ding, E.;\nand Zhang, Z. 2021. Image inpainting by end-to-end cas-\ncaded refinement with mask awareness. IEEE Transactions\non Image Processing, 30: 4855–4866.\nZuo, Z.; Zhao, L.; Li, A.; Wang, Z.; Zhang, Z.; Chen, J.;\nXing, W.; and Lu, D. 2023. Generative image inpainting\nwith segmentation confusion adversarial training and con-\ntrastive learning. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 37, 3888–3896.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6029",
  "topic": "Inpainting",
  "concepts": [
    {
      "name": "Inpainting",
      "score": 0.8452193737030029
    },
    {
      "name": "Transformer",
      "score": 0.5461432337760925
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46092697978019714
    },
    {
      "name": "Computer science",
      "score": 0.43560469150543213
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3915441036224365
    },
    {
      "name": "Computer vision",
      "score": 0.3892742097377777
    },
    {
      "name": "Engineering",
      "score": 0.16580721735954285
    },
    {
      "name": "Electrical engineering",
      "score": 0.09557196497917175
    },
    {
      "name": "Voltage",
      "score": 0.04564139246940613
    }
  ]
}