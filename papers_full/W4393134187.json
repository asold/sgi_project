{
  "title": "Using Large Language Models for sentiment analysis of health-related social media data: empirical evaluation and practical tips",
  "url": "https://openalex.org/W4393134187",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2106623083",
      "name": "Lu He",
      "affiliations": [
        "University of Wisconsin–Milwaukee"
      ]
    },
    {
      "id": "https://openalex.org/A4220351791",
      "name": "Samaneh Omranian",
      "affiliations": [
        "University of Wisconsin–Milwaukee"
      ]
    },
    {
      "id": "https://openalex.org/A2165166150",
      "name": "Susan McRoy",
      "affiliations": [
        "University of Wisconsin–Milwaukee"
      ]
    },
    {
      "id": "https://openalex.org/A1999257866",
      "name": "Kai Zheng",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2106623083",
      "name": "Lu He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4220351791",
      "name": "Samaneh Omranian",
      "affiliations": [
        "University of Wisconsin–Milwaukee"
      ]
    },
    {
      "id": "https://openalex.org/A2165166150",
      "name": "Susan McRoy",
      "affiliations": [
        "University of Wisconsin–Milwaukee"
      ]
    },
    {
      "id": "https://openalex.org/A1999257866",
      "name": "Kai Zheng",
      "affiliations": [
        "University of California, Irvine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2619822900",
    "https://openalex.org/W3134403479",
    "https://openalex.org/W4285010599",
    "https://openalex.org/W3114713384",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3157639471",
    "https://openalex.org/W4375870056",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W4391292768",
    "https://openalex.org/W3027070829",
    "https://openalex.org/W4389421203",
    "https://openalex.org/W4388777264",
    "https://openalex.org/W4389577293",
    "https://openalex.org/W4391031145",
    "https://openalex.org/W2292070666",
    "https://openalex.org/W2915057938",
    "https://openalex.org/W2594911318",
    "https://openalex.org/W1504725289",
    "https://openalex.org/W4385571470",
    "https://openalex.org/W4390745503",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W4283168218",
    "https://openalex.org/W2955483668",
    "https://openalex.org/W4221143046"
  ],
  "abstract": "Abstract Health-related social media data generated by patients and the public provide valuable insights into patient experiences and opinions toward health issues such as vaccination and medical treatments. Using Natural Language Processing (NLP) methods to analyze such data, however, often requires high-quality annotations that are difficult to obtain. The recent emergence of Large Language Models (LLMs) such as the Generative Pre-trained Transformers (GPTs) has shown promising performance on a variety of NLP tasks in the health domain with little to no annotated data. However, their potential in analyzing health-related social media data remains underexplored. In this paper, we report empirical evaluations of LLMs (GPT-3.5-Turbo, FLAN-T5, and BERT-based models) on a common NLP task of health-related social media data: sentiment analysis for identifying opinions toward health issues. We explored how different prompting and fine-tuning strategies affect the performance of LLMs on social media datasets across diverse health topics, including Healthcare Reform, vaccination, mask wearing, and healthcare service quality. We found that LLMs outperformed VADER, a widely used off-the-shelf sentiment analysis tool, but are far from being able to produce accurate sentiment labels. However, their performance can be improved by data-specific prompts with information about the context, task, and targets. The highest performing LLMs are BERT-based models that were fine-tuned on aggregated data. We provided practical tips for researchers to use LLMs on health-related social media data for optimal outcomes. We also discuss future work needed to continue to improve the performance of LLMs for analyzing health-related social media data with minimal annotations.",
  "full_text": "1 \n \nUsing Large Language Models for sentiment analysis of health-related social \nmedia data: empirical evaluation and practical tips \nLu He, PhD1, Samaneh Omranian2, Susan McRoy, PhD2, Kai Zheng, PhD3  \n1Joseph J. Zilber College of Public Health, University of Wisconsin-Milwaukee, USA \n2Department of Computer Science, University of Wisconsin-Milwaukee, USA \n3Department of Informatics, Donald Bren School of Information and Computer Science, \nUniversity of California, Irvine, USA \n \nAbstract \nHealth-related social media data generated by patients and the public provide valuable insights into patient \nexperiences and opinions toward health issues such as vaccination and medical treatments. Using Natural Language \nProcessing (NLP) methods to analyze such data, however, often requires high-quality annotations that are difficult to \nobtain. The recent emergence of Large Language Models (LLMs) such as the Generative Pre -trained Transformers \n(GPTs) has shown promising performance on a variety of NLP tasks in the health domain  with little to no annotated \ndata. However, their potential in analyzing health -related social media data remains underexplored. In this paper, \nwe report empirical evaluations of LLMs  (GPT-3.5-Turbo, FLAN-T5, and BERT-based models) on a common NLP \ntask of health-related social media data: sentiment analysis for identifying opinions toward health issues. We explored \nhow different prompt ing and fine-tuning strategies affect the performance of LLMs on social media datasets across \ndiverse health topics, including Health care Reform, vaccination, mask wearing, and healthcare service quality . We \nfound that LLMs outperformed VADER, a widely used off -the-shelf sentiment analysis tool , but are far from being \nable to produce accurate sentiment labels. However, their performance can be improved by data-specific prompts \nwith information about the context, task, and targets. The highest performing LLMs are BERT-based models that were \nfine-tuned on aggregated data. We provided practical tips for researchers to use LLMs on health-related social media \ndata for optimal outcomes. We also discuss future work needed to continue to improve the performance of LLMs for \nanalyzing health-related social media data with minimal annotations. \n \nIntroduction \nPatients, caregivers, and the public produce rich content related to health on social media platforms. Extensive research \nhas analyzed health-related social media data to gain insights into important health issues such as healthcare service \nquality, medical treatments and interventions, and healthcare policies1–3. Due to the large scale of health-related social \nmedia data, Natural Language Processing (NLP) techniques such as sentiment analysis and topic modeling prove to \nbe valuable in this line of research. Sentiment analysis can be used in place of survey questionnaires to quickly obtain \ninformation about the uptake or rejection of important public health efforts (such as promotion of vaccines or \nmasking), or how the public views particular policy initiatives such as Unites State s health care reform. Related \ncomments for all of these have been mentioned extensively in Tweets, and samples have been manually annotated for \nsentiment by experts, to create datasets (e.g. the Health Care Reform (HCR) dataset). Assessing public opinion in this \nway simplifies the process of repeated sampling during a rapidly evolving situation. However, accurate NLP \ntechniques often require high-quality annotated data, which is particularly challenging to obtain in the health domain  \ndue to the variety of health topics discussed and the heterogeneity of these topics . Off-the-shelf tools, on the other \nhand, require no annotated data but suffer from low performances when applied in health contexts due  to domain \nincompatibility. Our prior work found that most such studies have inconsistent and poor research design and reporting \npractices, which remains a significant concern around the validity and reliability of NLP -based analysis of health -\nrelated social media data4,5.  \nThe recent emergence of Large Language Models ( LLMs) brings excitement and new opportunities to  NLP \nand the health domain. LLMs are pre-trained on gigantic corpora such as Wikipedia articles  to learn and memorize \nsemantic representations and relationships of natural language and are further tuned on various downstream NLP \ntasks, such as sentiment analysis or named entity recognition (NER)  to enhance their capabilities6. Representative \nLLMs include the Bi-directional Encoder-Decoder Recurrent Transformer (BERT), Generative Pre -Trained LLM \n(GPT) by Open-AI, FLAN-T5 by Google, and LLaMA by Meta. These LLMs have shown superior performance and \nversatility in a variety of NLP tasks, such as question answering, NER, text summarization, and language translation. \nResearchers have set out to evaluate the capabilities of the LLMs on diverse datasets and tasks. For example, Ziems \net al. evaluated LLMs from the GPT family and FLAN -T5 on multiple computational social science tasks such as \ntoxicity detection and positive framing7. These studies can involve either so -called “zero-shot” evaluations, where \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 20, 2024. ; https://doi.org/10.1101/2024.03.19.24304544doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n2 \n \nonly unlabeled test data is provided for analysis or “few-shot” tests or “fine-tuning”, where a sample of annotated data \nis provided for analysis prior to the presentation of test cases. \nIn the health domain, most recent evaluations of LLMs focus on clinical data such as clinical notes, Electronic \nHealth Records (EHR), and medical images 8. For example, Hu et al. developed multiple prompting strategies to \nevaluate the capabilities of zero-shot LLMs such as GPT-3.5 and GPT-4 on classic clinical NER tasks and found that, \nalthough they have lower performance compared to fine -tuning BioClinicalBERT on clinical data, they have \ndemonstrated promising results with no annotated data required 9. Goel et al. used LLMs as additional annotators in \nclinical NER tasks and found that they can lower clinical annotators’ burden while maintaining high performance of \nthe NLP models 10. Despite these promising results in evaluating LLMs on clinical NLP tasks, few assessments \nconsidered consumer health data. For example, Lossio-Ventura et al. evaluated zero-shot ChatGPT for analyzing the \nsentiments conveyed through COVID -19 free -text survey responses . With no annotated data provided, ChatGPT \nachieved an F-measure of 0.8668, outperforming many commonly used off-the-shelf tools including VADER and the \nLinguistic Inquiry and Word Count (LIWC) 11. Fu et al., evaluated ChatGPT on patient-generated texts in Cantonese \nthrough online counseling sessions and found that the zero-shot LLM can identify sentiments with over 90% accuracy, \noutperforming multiple machine learning models developed with annotated dataset s12. Deiner et al. evaluated GPTs’ \ncapacity in rating the probabilities of tweets that indicate conjunctive outbreaks and found that GPT-4 had satisfying \nagreements with two human raters with Pearson correlation scores of 0.73 and 0.7713.  \nDespite these promising results suggesting that LLMs may change the landscape of NLP in the health domain,  \nthere is limited work that investigates their capacity to analyze the sentiments conveyed through health -related social \nmedia data. This type of data presents unique challenges such as the abundance of informal expressions created by \npatients and the public, and the constantly evolving languages that change in response to social media platform policies \n14 or health crises2. Analyzing the sentiments and opinions from patient and public-generated health social media data \nremains a critical topic, as it provides valuable insights into patient and public concerns toward health issues and \nenables patient -centered care by incorporating their voices. Despite the critical need to analyze large -scale health \nsocial media data, research shows that existing computational tools were not compatible with social media data and \nhealth contexts. Our previous studies found that commonly used off-the-shelf tools performed extremely poorly when \nanalyzing the sentiments of health-related social media data. Developing machine and deep learning models on study \ndata also require high-quality annotated data which can be challenging to obtain.  \nTo address the critical need of assessing the feasibility of utilizing LLMs in analyzing health -related social \nmedia data, we conducted a study to evaluate  the performance of zero-shot and few-shot LLMs against LLMs fine-\ntuned on annotated data on various health -related social media datasets , covering vaccin ation for human papilloma \nvirus (HPV), mask wearing during COVID-19, the US healthcare reform in the Obama era, and reviews of healthcare \nproviders. A commonly used off-the-shelf tool was used as baseline for comparison. We varied prompting and fine -\ntuning strategies for the LLMs such as by incorporating aspect  information in prompts and by assembling annotated \ndata from multiple datasets for fine -tuning. We found that zero -shot and few -shot LLMs, though still far from \nsatisfying, generally have better performance than the off-the-shelf tool. Incorporating data-specific information into \nthe prompts improved model performance by a large margin on most of the datasets. BERT-based models that were \nfine-tuned on ensembled data  showed the highest performance. Finally, we discuss the role of LLMs in analyzing \npatient and public-generated health social media data as well as practical tips for health informatics researchers to \nleverage them with little annotated data for better performance.  \n \nMethods \nEvaluation datasets \nWe used datasets that were evaluated in our previous work4. The datasets were selected based on our systematic review \nand must include human coders’ annotation as ground truth labels. 5 More detailed description of the datasets can be \nfound in He et al. 4 The evaluation datasets included four health -related social media datasets: Health Care Reform \n(“HCR”), Human Papillomavirus (“HPV”) Vaccine, COVID -19 Masking (“Mask”) , and Vitals.com Physician \nReviews (“Vitals”). A non-health dataset, the IMDB Dataset (“IMDB”) created from movie reviews, was used for \nbaseline comparison. To reduce the cost of OpenAI API usage, we randomly sampled 1,000 posts from the Vitals \ndataset and the IMDB dataset, respectively.  \n The HCR dataset was originally created by Speriosu et al. to study the public’s reactions to the Health Care \nReform in the Obama era 15. The dataset includes 2,315 tweets, with 701 labeled as positive, 1,034 as negative, and \n580 neutral. The HPV dataset was released by Du et al. for identifying public’s opinions toward the HPV vaccination16. \nThe data consists of 964 positive, 1,044 negative, and 1,085 neutral tweets. The Mask dataset was collected, labeled, \nand maintained by our research team to study public attitudes and concerns toward mask wearing during the COVID-\n19 pandemic.2 The dataset was collected from Twitter between January 1 and November 1, 2020.  A random sample \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 20, 2024. ; https://doi.org/10.1101/2024.03.19.24304544doi: medRxiv preprint \n3 \n \nof 609 tweets were annotated; 530 tweets supported mask wearing and 79 opposed mask wearing. The Vitals data was \ncollected from one of the largest online physician review websites, Vitals.com, where patients and caregivers wrote \nreviews to rate the quality of healthcare services they received 3. The sentiment labels were determined based on the \nnumeric rating provided by the users (e.g., reviews with 5 -star were labeled as positive and reviews with 1 -star were \nnegative). In the 1,000 random samples used in this study, 744 are positive reviews and 256 are negative reviews. The \nIMDB dataset was released by Maas et al. , which included movie reviews collected from one of the largest movie \nreview websites, IMDB.com,  is a commonly used benchmark dataset for sentiment analysis and is used as a \ncomparison dataset in this study17. In the 1,000 random samples included in this study, 623 are negative movie reviews \nand 377 are positive.  \n \nModels and tools selected for evaluation  \nZero-shot LLMs \nWe selected widely used LLMs that can generate textual responses to  zero-shot NLP tasks with natural language \nprompts. Based on recent studies that evaluated LLMs in the general domain and on clinical notes 7,9,18,19, we selected \nFLAN-T5 by Google 20 and GPT-3.5-Turbo21 by OpenAI. We initially also experimented with LLaMA2 provided by \nMeta, but it failed to produce answers in consistent formats and sometimes merely repeated the question prompts. \nTherefore, we did not include LLaMA2 in this evaluation study. We accessed GPT-3.5-Turbo through OpenAI’s Chat \nCompletion API22. FLAN-T5 was accessed via Huggingface Transformer API23. For all experiments involving GPT, \nthe temperature was set to 0 to reduce randomness and ensure the consistency of outputs from the models.  \n Extensively experimenting with all checkpoints of the LLMs is beyond the scope and capacity of this study. \nWe instead focused on the commonly used versions of the LLMs. The GPT-3.5-Turbo-1106 checkpoint and FLAN-\nT5-Base were used.20 The FLAN -T5-Base checkpoint is a fine -tuned T5 model optimized for  more than 1000 \ndownstream tasks and contains more than 250 million parameters.  \n \nFine-tuned LLMs \nWe also selected commonly used pre-trained LLMs and fine-tuned them on a held-out set of our evaluation datasets. \nWe used  a general LLM  (BERT_base_uncased) and domain and task -specific LLMs (BioClinicalBERT25 and \nSiEBERT26). BERT-based models have been widely used in downstream NLP tasks such as text classification  and \nnamed entity extraction27. We accessed the pre -trained models through the HuggingFace model repository and fine-\ntuned the models using the Trainer API. It is beyond the scope of this study to comprehensively evaluate all available \npre-trained LLMs. We only selected commonly used models that are relevant to the task of sentiment analysis and the \nhealth domain.  \n \nBaseline off-the-shelf tools \nWe used Valence Aware Dictionary and sEntiment Reasoner (VADER) as the baseline off -the-shelf tool for \ncomparison, as it produced the highest performance in an evaluation study on the same datasets4. VADER is an open-\nsource Python package using lexicons and manually curated rules to classify sentiments of social media data 28. It \ncomputes a sentiment score on a piece of input text . Based on a threshold provided in the package, a sentiment label \nwill be produced. More information about the tool and its performance is documented in He et al 4.  \n \nExperiments and setups \nExperiment 1: comparing zero-shot LLMs with fine-tuned LLMs and the baseline off-the-shelf tool \nThe goal of Experiment 1 was to evaluate zero-shot LLMs against fine -tuned LLMs on the evaluation datasets with \nVADER being the baseline. In this experiment, zero -shot LLMs received no annotation data. Natural language \nprompts were curated moderately  so that the LLMs can produce usable and comprehensible results , without any \nspecific references to the data or the context of the data. The zero-shot LLMs were evaluated on full datasets because \nno training or validation data were needed. The finalized prompt used for all zero -shot LLMs across all datasets is: \n“What is the sentiment of the text? Only return the letter representing the answer. A. positive B. negative C. neutral \nD. not sure Text: [REPLACE WITH DATA].” \nPre-trained BERT-based LLMs (BERT_base_uncased and BioClinicalBERT) were fine-tuned on a random \nsample of the evaluation datasets. Each evaluation dataset was split into a training, validation, and test set, following \na 0.8-0.1-0.1 ratio. SiEBERT (Sentiment in English)  was already fine -tuned on various texts  with sentiment labels , \nincluding reviews and tweets26. Therefore, we directly adopted the fine-tuned check point of the model, without further \ntuning using our study data. Since the maximum token size allowed for BERT-based models is 512, we truncated the \ninput text to include only 512 tokens; texts containing less than 512 tokens were padded with extra tokens so that input \ntexts are of the same length.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 20, 2024. ; https://doi.org/10.1101/2024.03.19.24304544doi: medRxiv preprint \n4 \n \n \nExperiment 2: comparing zero-shot LLMs using generic versus data-specific prompts \nThe goal of Experiment 2 was to investigate whether and how different prompting strategies affect the performance \nof the zero-shot LLMs. Subsequently, we tested basic prompts against data -specific prompts. We also tested chain -\nof-thought prompts which required LLMs to reason and have shown promising results in prior studies29. However, the \nperformance from our pilot study was extremely poor when using chain -of-thought prompts and even worse than \ngeneric prompts. This is probably because chain -of-thought prompts are useful for simple reasoning tasks such as \nsolving mathematical problems or answering questions. Therefore, we decided to exclude chain -of-thought prompts \nin our evaluation.  \nWhile there are many viable prompts, to make a fair baseline for the basic and generic prompts, we only \nconducted moderate prompt engineering, following instructions provided in previous studies 7. Because sentiment \nanalysis is essentially a text classification task with pre -defined sentiment labels, we formatted the prompts as \nmultiple-choice problems, where each option is listed after a letter, followed by a new line character. We also \ninstructed the LLMs to only produce the letter, both to reduce the number of tokens generated and to make the \nevaluation easier. After rounds of testing, we finalized the generic prompt and used it  across all LLMs and on all \nevaluation datasets. To ensure that LLMs do not produce hallucinations on data that they are not confident in, we \nadded another choice “Unsure”.  \nFor designing data -specific prompts, we injected data -specific information into the generic prompt by \nproviding the task information (classification of attitude as supporting or opposing , or classif ication of overall \nsentiment as positive and negative ), target (mask, HPV, HCR , movie, provider ), data (tweet, movie review, online \nphysician review ), and context if applicable (e.g., during the pandemic, Healthcare Reform by Obama ). Each \nevaluation dataset has its own data -specific prompt. For example, the data -specific prompt for the Mask dataset is: \n“Does the tweet support mask wearing during the pandemic?”  The data-specific prompts used for the datasets are \nshown in Figure 1 below.  \n   \nFigure 1. Data-specific prompts in zero and one-shot experiments. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 20, 2024. ; https://doi.org/10.1101/2024.03.19.24304544doi: medRxiv preprint \n5 \n \n \nExperiment 3: comparing zero-shot and one-shot LLMs \nThe goal of Experiment 3 was to investigate whether providing LLMs with annotated examples can outperform zero-\nshot LLMs, where no annotated example was provided. As it is not our focus to comprehensively assess the differences \nbetween the number of examples, we evaluated one-shot LLMs to control the cost of using OpenAI API. We extended \neach data-specific prompt by adding an example post from the data, along with a short explanation and the answer. \nThe prompts used for one-shot LLMs are shown in Figure 1. \n \nExperiment 4: comparing LLMs fine-tuned on single datasets versus models fine-tuned on an ensembled dataset \nThe goal of Experiment 4 was to investigate whether fine-tuning LLMs on ensembled datasets from multiple sources \noutperform LLMs fine-tuned on single-source datasets. To create the ensembled dataset, we aggregated all data from \nthe five evaluation datasets and then randomly partitioned the ensembled dataset into training, validation, and test sets \nfollowing the 0.8 -0.1-0.1 ratio. The BERT -based models were fine -tuned on the training and validation sets and \nevaluated on individual test sets from each evaluation dataset so that the performance is a fair comparison with results \nfrom the BERT-based models fine-tuned on individual datasets. For example, BERT_base_ensemble was fine -tuned \non a random set of 80% of all data aggregated from the five evaluation datasets. It was then evaluated individually on \nrandom sets of 10% of each of the five datasets.  \n \nResults \nExperiment 1: Performance comparison between LLMs and off-the-shelf tools \nTable 1 shows the weighted F1-scores of the zero-shot LLMs with generic prompts on all datasets, fine-tuned LLMs \nevaluated on sampled test sets of the data, and VADER on all data. Even without substantial prompt engineering or \nannotated examples to the zero-shot LLMs, GPT3.5-Turbo outperformed VADER on four out of five datasets and \nFLAN-T5 outperformed VADER on three out of five datasets.  \nThough zero-shot LLMs have demonstrated higher performance than VADER, their results are still far from \nusable for research on most of the datasets , with weighted F1-scores lower than 0.6. The LLMs achieved the highest \nperformance on the Vitals dataset, with a weighted F1-score of 0.97. Notably, both GPT3.5-Turbo and FLAN-T5 have \nextremely poor performance on the HPV and HCR data. The three twitter datasets (HCR, HPV, and Mask) are still \nchallenging for the LLMs models, despite the improved performance over the baseline, VADER.  \n Out of the six tools/LLMs evaluated, SiEBERT performed the best on three out of five datasets, suggesting \nthat LLMs fine-tuned on large scale datasets can indeed generalize to new datasets, though their applicability to health-\nrelated social media data such as HPV and Mask is still limited.  \n \nTable 1. Weighted F1-scores of the models on the evaluation datasets . GPT-3.5-Turbo and FLAN-T5 are all zero -\nshot. BERT-based models are evaluated on held-out test sets. Highest weighted F1-scores are shown bolded. \n Zero-shot LLMs Fine-tuned LLMs Baseline tool \n GPT3.5-\nTurbo \nFLAN-\nT5 \nBERT_base SiEBERT BioClinicalBERT VADER \nHCR 0.57 0.40 0.52 0.60 0.57 0.41 \nHPV 0.55 0.34 0.59 0.39 0.51 0.49 \nMask 0.22 0.64 0.48 0.58 0.73 0.59 \nVitals 0.95 0.97 0.64 0.97 0.68 0.89 \nIMDB 0.75 0.94 0.21 0.95 0.45 0.65 \n \n \nExperiment 2 & 3: Performance comparison between LLMs using basic and data-specific prompts in zero and one -\nshot settings \nTable 2 shows weighted F1-scores based on results from zero  and one-shot LLMs using generic prompts and data -\nspecific prompts. For zero-shot GPT-3.5-Turbo, the performance increased by large margins when using data-specific \nprompts on three out of the five datasets. However, it had a lower performance with data-specific prompts on the HPV \ndataset. The Mask and IMDB datasets benefit ed the most from using data -specific prompts. While GPT-3.5-Turbo \ngenerally benefited from data-specific prompts, FLAN-T5 produced much poorer results when data-specific prompts \nwere used, except for on the Mask dataset. For both the HCR and HPV datasets, none of the models performed better \nwith data-specific prompts, although G PT-3.5-Turbo saw slight i mprovement on the HCR data in the data -specific, \none-shot condition. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 20, 2024. ; https://doi.org/10.1101/2024.03.19.24304544doi: medRxiv preprint \n6 \n \n Across the datasets, the Mask dataset gained the most from data-specific prompts. This is in part attributable \nto the importance of the context in which the Mask dataset was created. Without the contexts of the aspect (mask \nwearing) and during the pandemic, it is even challenging for human annotators to label the attitudes.  \n Comparing the LLMs in zero and one -shot settings, we did not observe  benefits from the addition of an \nannotated example for GPT -3.5-Turbo and FLAN -T5. For FLAN-T5, the  performance even decre ased on three \ndatasets (Mask, Vitals, IMDB). For GPT3.5 -Turbo, adding an annotated example marginally improved the \nperformance on the HCR and HPV dataset,  \n Overall, data-specific prompts improved performance for GPT3.5-Turbo but not for FLAN-T5. The benefits \nfrom adding an annotated example were not observed for both models.  \n \nTable 2. Weighted F1-scores of the LLMs using generic and data-specific prompts and with an annotated example. \n GPT3.5-Turbo \n(generic) \nGPT3.5-Turbo \n(specific) \nGPT3.5-Turbo \n(specific + 1 shot) \nFLAN-T5  \n(generic) \nFLAN-T5 \n(specific) \nFLAN-T5 \n(specific + 1 shot) \nHCR 0.57 0.57 0.61 0.40 0.27 0.35 \nHPV 0.55 0.45 0.46 0.34 0.25 0.30 \nMask 0.22 0.85 0.85 0.64 0.79 0.74 \nVitals 0.95 0.96 0.96 0.97 0.95 0.86 \nIMDB 0.75 0.90 0.68 0.94 0.94 0.89 \n \n \nExperiment 4: Performance comparison between fine -tuning LLMs on individual datasets versus  an ensembled \ndataset  \nTable 3 below compares the weighted F1 -scores across the BERT -based models that were fine -tuned on individual \ntest sets with those fine-tuned on an ensembled test set. In general, those tuned on the ensembled set outperformed by \na large margin. Even the most basic BERT model fine -tuned on ensembled data achieved the highest weigh ted F1-\nscores. Notably, the BERT_base_ensemble also achieved the highest performance on the HPV datasets among all \nmodels, with a weighted F1-score of 0.94, whereas the performance of other models was less than 0.6. This finding is \nnot surprising, given that more and diverse annotated data for fine -tuning can usually improve model performance. \nThe large margin of improvement, however, suggests that annotated data from other contexts are still useful for \ndownstream tasks in different contexts. Notably, the performance achieved by the BERT-based models fine-tuned on \nthe ensembled data is also the highest among all tools and models. For the HCR, HPV, and Mask data, the fine-tuned \nBERT-based models even achieved weighted F1 -scores higher than 0.85. BERT_base_ensemble consistently \noutperformed BioClinicalBERT_ensemble, possibly because BERT_base was pre-trained on general English corpora \nthat aligns better with social media data, while BioClinicalBERT was pre-trained on clinical notes and the underlying \nsemantic representations are quite different.  \n \nTable 3. Weighted F1-scores of the pre-trained, BERT-based models fine-tuned using different samples. \n BERT_base_single BERT_base_ensemble BioClinicalBERT_single BioClinicalBERT_ensemble \nHCR 0.52 0.93 0.57 0.93 \nHPV 0.59 0.94 0.51 0.92 \nMask 0.48 0.88 0.73 0.86 \nVitals 0.64 0.98 0.68 0.99 \nIMDB 0.21 0.97 0.45 0.88 \n \n \nDiscussion \nOverall performance and observations from the experiments \nOur evaluation results show that while LLMs demonstrated promising performance on a variety of NLP tasks in other \ndomains and on clinical data, they are still far from reliable when applied on health -related social media data , \nespecially Twitter data that are known to be challenging for NLP . The performance of LLMs highly depends on the \nnature of the health-related social media data and how researchers formulate the tasks and design the prompts. Online \nphysician reviews are less challenging for LLMs, as they most directly  express sentiments and opinions . The \nperformance of zero -shot LLMs on the Vitals dataset is above 0.95 for GPT -3.5-Turbo and FLAN -T5, with no \nannotated data provided or prompting strategies used. However, on the more challenging datasets such as tweets  \nrelated to HCR, HPV, and mask wearing, the highest weighted F1-score is only slightly above 0.6, achieved by FLAN-\nT5-base on the Mask dataset.   \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 20, 2024. ; https://doi.org/10.1101/2024.03.19.24304544doi: medRxiv preprint \n7 \n \n The datasets and models also responded differently to further prompt tuning. Embedding data -specific \ninformation into prompts may enhance the performance of zero -shot LLMs, but the benefits depend on the data and \ncontexts. For example, the Mask dataset benefited the most from data-specific prompts, but the HCR and HPV datasets \ndid not. FLAN-T5 saw reduced performance when data -specific prompts were used. Adding an annotated example \nalso did not improve the performance of GPT-3.5-Turbo and FLAN-T5. \n The highest performing models from our evaluation study are BERT -based models fine -tuned on an \nensembled dataset . The fine -tuned models achieved weighted F1 -scores higher than 0.85, even on the more \nchallenging datasets such as the HCR, HPV, and Mask dataset. The basic BERT model consistently outperformed \nBioClinicalBERT, possibly due to the alignment between health social media data and the generic English corpora \nthat BERT was pre-trained on. Surprisingly, BioClinicalBERT that was pre-trained on clinical notes from Intensive \nCare Unit settings still showed good performance, despite the development domain highly differing from the social \nmedia contexts. This may suggest that having a diverse ensembled data for further fine-tuning can still improve model \nperformance and efficiently transfer the previously learned semantic representations to a new domain.  \n Overall, our evaluation results suggest that GPT and FLAN-T5 are still not yet directly usable for analyzing \nhealth-related social media data, especially data related to healthcare policy  and vaccination. Using data-specific \nprompts can improve the performance of the models on some datasets but not consistently. For more reliable results, \nresearchers should still leverage annotated data to fine-tune pre-trained models such as BERT and BioClinicalBERT. \nAssembling diverse data for fine-tuning can greatly improve the performance of these models.  \n \nPractical tips for using LLMs on health-related social media data \nWhile the model behaviors and performances are difficult to analyze , we provide a set of practical tips derived from \nthe series of experiments we conducted. However, it is worth noting that the tips may only be applicable to certain \nsocial media and health contexts. Researchers should explore their study data and investigate what works the best for \ntheir specific study contexts.  \n First, correctly formulating the sentiment analysis task and providing this information to LLMs is essential \nfor better performance. While sentiment analysis generally refers to the task of identifying sentiments, opinions, and \nattitudes from texts, it can be manifested very differently in different study contexts. For example, on the Mask dataset, \nthe task is formulated as “identifying the attitudes as supporting or opposing” (a form of argumentation), while on the \nVitals dataset, the task is “identifying the emotion toward the provider as positive or negative”.  \n Second, health-related social media data often contain multiple aspects at the same time, therefore, specifying \nthe intended focus where  sentiment is wanted is important for the LLMs. For example, in the Mask data, users \ncommonly used negative words to express their supporting attitudes toward mask wearing. This, however, often \nrequires researchers to first conduct exploratory qualitative review of their data to identify aspects of interest to \ncorrectly instruct the LLMs. \n Third, while LLMs are all pre-trained on gigantic corpora and have a huge parameter space, their underlying \narchitecture and therefore their specialties still differ. For example, according to Ziems et al., FLAN-T5 performed \nbetter than GPT models on sentiment classification on most of the datasets7. Our evaluation results also found that the \nmodels responded differently to data -specific prompts and the additional annotated examples.  This suggests that \nresearchers should familiarize themselves with the unique features of the LLMs to find the model that best fits their \nstudy data and contexts. In general, we observed that FLAN-T5 works best with simple and straightforward prompts, \nwhile GPT models benefit from detailed natural language prompts with information about study data and contexts.  \n \nThe role of zero and few-shot learning LLMs in analyzing health-related social media text \nWhile our evaluation results show that zero-shot LLMs are still far from capable of producing reliable sentiment labels \non health-related social media data, they have several strengths that make them good candidates for being incorporated \ninto the analysis workflow. First, researchers can interact with zero or few -shot LLMs such as GPT and F LAN-T5 \nthrough natural language-based prompts, lowering the technical requirements of training, developing, and fine-tuning \ntheir own deep learning models. In addition, researchers can easily extend the analysis to more granular levels, such \nas aspect-based sentiment analysis, with zero -shot LLMs. Analyzing patient and public -generated social media data \nmay particularly benefit from the power and promises of zero and few -shot LLMs. Unlike clinical NLP tasks where \nthe clinical information and entities of intere st are relatively fixed, health -related social media data are often \nheterogeneous and evolving . The nature of health -related social media data often requires researchers to conduct \nmanual review at the beginning to identify aspects of interest. Zero and few -shot LLMs and their versatility and \nflexibility can potentially help researchers to conduct exploratory analysis of health -related social media data, \nespecially when the content of the data is not clear. In the process, researchers can also improve or tu ne the model \nusing natural language input, leading to efficient synergies between LLMs and researchers.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 20, 2024. ; https://doi.org/10.1101/2024.03.19.24304544doi: medRxiv preprint \n8 \n \n While researchers can be freed from tedious data annotation, feature engineering, model development and \nvalidation, their roles may be as critical in the LLM era . For example, it is important for the LLMs to have accurate \nand relevant prompts for the tasks, especially the context, sentiment analysis target, and data. This requires researchers \nto have a good understanding of their study data and invest time to conduct exploratory analysis of the data.  \n Our results from Experiment 4 also shows that BERT -based LLMs performed the best when fine-tuned on \nensembled data from all evaluation datasets. This, however, requires a larger amount of annotated data for fine-tuning. \nA possible workaround is to use the zero-shot LLMs to generate synthetic data with labels and use the generated data \nto augment the fine-tuning dataset with BERT-based models. Though zero or few-shot LLMs themselves may not be \nable to produce accurate sentiment labels, their superior ability to generate human -like texts can be leveraged in \naugmenting annotated data to improve the performance on downstream tasks.  \n \nChallenges and issues with using LLMs in analyzing patient and public-generated social media data  \nDespite the promising performance and the ease of use of the zero-shot LLMs on health-related social media data, we \nalso note challenges and issues that researchers should be aware of when using these LLMs. First, most of the LLMs \nare proprietary products made available through commercial companies such as OpenAI, Google, and Meta. While \nFLAN-T5 by Google and Llama2 by Meta are currently freely available or upon request, it is hard to predict whether \nand at what price they will charge for using the ir models. The rising costs of close-sourced LLMs such as GPTs also \nraise concerns about the feasibility of large -scale analysis of social media data. Currently, it costs $10 for 1 million \ntokens with GPT-4-Turbo, which is 200 times the price for GPT -3-Turbo. It is unknown whether they will continue \nto increase and what consequences this may have on researchers with limited financial resources.  \nOur results also show that there are no consistent patterns of which LLM and what prompts work the best for \nwhat types of data. OpenAI’s models are not open -sourced, making the use and explanation of the model output \nopaque for researchers in the health domain. Second, recent research has revealed the risk of LLMs in leaking training \ndata, which could be particularly concerning due to the sensitive nature of health-related data.30 While OpenAI stated \nthat user input will only be stored on their servers and not be used for training their models, patients and the public \nwho produced the health -related social media data may still perceive this as risky. The recent deal between Reddit, \none of the largest online communities, and an unknown company to access all public Reddit data for developing LLMs \nalso raised concerns of how public social media data will be used in the LLM era. 31 This could be particularly \nconcerning for patients who produce health -related content on social media platforms, as their posts may contain \npersonal and sensitive information. This warrants future studies to investigate how to best protect patient and public-\ngenerated social media data when LLMs are trained on and used for analyzing them.  \n \nLimitations \nOur study is limited to the selection of health -related social media data that are publicly accessible  and have been \nannotated by experts. While we tried to diversify the health topics covered  and social media platforms studied, three \nout of the five datasets we evaluated were from Twitter data. In addition, due to the cost of using the OpenAI API, we \nwere not able to comprehensively experiment with an exhaustive set of prompting strategies or model checkpoints . \nThe number of available LLMs is also increasing rapidly; it is beyond the scope and capacity of this work to compare \nall available LLMs on the study data.  \n \nFuture directions \nThis evaluation study sets the stage for exciting future work. First, we will further explore how LLM sizes affect their \nperformance, ie, whether larger LLMs always have better p erformance or whether more easily distributed smaller \nmodels would suffice. Second, we will leverage the retrieval-augmented generation (RAG)  framework with a small \nset of annotated health-related social media data to improve the LLMs and further reduce the risk of hallucination. We \nwill also explore innovative use of LLMs to improve the performance, such as by using LLMs to generate synthetic \ndata to augment the size of training data and improve downstream tasks. Third, we will further evaluate the capabilities \nof zero -shot LLMs on more NLP tasks such as topic modeling  and misinformation detection  using health -related \nsocial media data. Last, we will evaluate the capabilities of LLM s on multimodal health -related social media data, \ncombining texts, images, audios, and videos for a more comprehensive analysis of patient and public-generated data.  \n \nConclusions \nLLMs such as GPT -3.5-Turbo and FLAN-T5 consistently perform better than a highly regarded, rule -based off-the-\nshelf tool when analyzing the sentiments and opinions of health -related social media data, even when no annotated \ndata is provided. Further, enriching prompts with data-specific details about targets of the opinions  and social media \nplatforms improves the performance of current LLMs on most of the evaluation datasets. This evaluation study shows \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 20, 2024. ; https://doi.org/10.1101/2024.03.19.24304544doi: medRxiv preprint \n9 \n \nthe potential of using LLMs in computational analysis of health -related social media data with little to no annotated \ndata. However, researchers will still need to have a good understanding of the study data they are working with, to \ncorrectly instruct LLMs to produce accurate results. Future work should further investigate how to continuously tune \nLLMs with domain data for better performance.  \nReferences \n1. Davis MA, Zheng K, Liu Y, Levy H. Public Response to Obamacare on Twitter. J Med Internet Res. 2017;19(5). \ndoi:10.2196/jmir.6946 \n2. He L, He C, Reynolds TL, et al. Why do people oppose mask wearing? A comprehensive analysis of U.S. tweets \nduring the COVID-19 pandemic. J Am Med Inform Assoc. 2021;28(7):1564-1573. doi:10.1093/jamia/ocab047 \n3. He L, He C, Wang Y, Hu Z, Zheng K, Chen Y. What Do Patients Care About? Mining Fine -grained Patient \nConcerns from Online Physician Reviews Through Computer-Assisted Multi-level Qualitative Analysis. AMIA \nAnnu Symp Proc. 2021;2020:544-553. \n4. He L, Yin T, Zheng K. They May Not Work! An evaluation of eleven sentiment analysis tools on seven social \nmedia datasets. J Biomed Inform. 2022;132:104142. doi:10.1016/j.jbi.2022.104142 \n5. He L, Yin T, Hu Z, Chen Y, Hanauer DA, Zheng K. Developing a standardized protocol for computational \nsentiment analysis research using health-related social media data. J Am Med Inform Assoc JAMIA. \n2020;28(6):1125-1134. doi:10.1093/jamia/ocaa298 \n6. Brown TB, Mann B, Ryder N, et al. Language Models are Few-Shot Learners. Published online July 22, 2020. \ndoi:10.48550/arXiv.2005.14165 \n7. Ziems C, Held W, Shaikh O, Chen J, Zhang Z, Yang D. Can Large Language Models Transform Computational \nSocial Science? Published online December 7, 2023. doi:10.48550/arXiv.2305.03514  \n8. Tian S, Jin Q, Yeganova L, et al. Opportunities and challenges for ChatGPT and large language models in \nbiomedicine and health. Brief Bioinform. 2024;25(1):bbad493. doi:10.1093/bib/bbad493 \n9. Hu Y, Chen Q, Du J, et al. Improving large language models for clinical named entity recognition via prompt \nengineering. J Am Med Inform Assoc. Published online January 27, 2024:ocad259. doi:10.1093/jamia/ocad259 \n10. Goel A, Gueta A, Gilon O, et al. LLMs Accelerate Annotation for Medical Information Extraction. In: \nProceedings of the 3rd Machine Learning for Health Symposium. PMLR; 2023:82-100. Accessed December \n15, 2023. https://proceedings.mlr.press/v225/goel23a.html \n11. Lossio-Ventura JA, Weger R, Lee AY, et al. A Comparison of ChatGPT and Fine -Tuned Open Pre-Trained \nTransformers (OPT) Against Widely Used Sentiment Analysis Tools: Sentiment Analysis of COVID -19 \nSurvey Data. JMIR Ment Health. 2024;11(1):e50150. doi:10.2196/50150 \n12. Fu Z, Hsu YC, Chan CS, Lau CM, Liu J, Yip PSF. Efficacy of ChatGPT in Cantonese Sentiment Analysis: \nComparative Study. J Med Internet Res. 2024;26(1):e51069. doi:10.2196/51069 \n13. Deiner MS, Deiner NA, Hristidis V, et al. Use of Large Language Models to Assess the Likelihood of Epidemics \nFrom the Content of Tweets: Infodemiology Study. J Med Internet Res. 2024;26(1):e49139. doi:10.2196/49139 \n14. Chancellor S, Pater JA, Clear TA, Gilbert E, De Choudhury M. #thyghgapp: Instagram Content Moderation and \nLexical Variation in Pro-Eating Disorder Communities. In: Proceedings of the 19th ACM Conference on \nComputer-Supported Cooperative Work & Social Computing - CSCW ’16. ACM Press; 2016:1199-1211. \ndoi:10.1145/2818048.2819963 \n15. Speriosu M, Sudan N, Upadhyay S, Baldridge J. Twitter polarity classification with label propagation over \nlexical links and the follower graph. In: Proceedings of the First Workshop on Unsupervised Learning in NLP. \nEMNLP ’11. Association for Computational Linguistics; 2011:53-63. \n16. Du J, Xu J, Song H, Liu X, Tao C. Optimization on machine learning based approaches for sentiment analysis on \nHPV vaccines related tweets. J Biomed Semant. 2017;8. doi:10.1186/s13326-017-0120-6 \n17. Maas AL, Daly RE, Pham PT, Huang D, Ng AY, Potts C. Learning word vectors for sentiment analysis. In: \nProceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language \nTechnologies - Volume 1. HLT ’11. Association for Computational Linguistics; 2011:142-150. \n18. Ramachandran GK, Fu Y, Han B, et al. Prompt-based Extraction of Social Determinants of Health Using Few-\nshot Learning. In: Proceedings of the 5th Clinical Natural Language Processing Workshop. Association for \nComputational Linguistics; 2023:385-393. doi:10.18653/v1/2023.clinicalnlp-1.41 \n19. Guevara M, Chen S, Thomas S, et al. Large language models to identify social determinants of health in \nelectronic health records. Npj Digit Med. 2024;7(1):1-14. doi:10.1038/s41746-023-00970-0 \n20. Chung HW, Hou L, Longpre S, et al. Scaling Instruction-Finetuned Language Models. Published online \nDecember 6, 2022. doi:10.48550/arXiv.2210.11416 \n21. Qin C, Zhang A, Zhang Z, Chen J, Yasunaga M, Yang D. Is ChatGPT a General -Purpose Natural Language \nProcessing Task Solver? In: Bouamor H, Pino J, Bali K, eds. Proceedings of the 2023 Conference on Empirical \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 20, 2024. ; https://doi.org/10.1101/2024.03.19.24304544doi: medRxiv preprint \n10 \n \nMethods in Natural Language Processing. Association for Computational Linguistics; 2023:1339-1384. \ndoi:10.18653/v1/2023.emnlp-main.85 \n22. OpenAI Platform. Accessed March 13, 2024. https://platform.openai.com  \n23. google/flan-t5-base · Hugging Face. Published January 4, 2024. Accessed March 13, 2024. \nhttps://huggingface.co/google/flan-t5-base \n24. OpenAI, Achiam J, Adler S, et al. GPT-4 Technical Report. Published online March 4, 2024. \ndoi:10.48550/arXiv.2303.08774 \n25. Alsentzer E, Murphy J, Boag W, et al. Publicly Available Clinical BERT Embeddings. In: Rumshisky A, Roberts \nK, Bethard S, Naumann T, eds. Proceedings of the 2nd Clinical Natural Language Processing Workshop . \nAssociation for Computational Linguistics; 2019:72-78. doi:10.18653/v1/W19-1909 \n26. Hartmann J, Heitmann M, Siebert C, Schamp C. More than a Feeling: Accuracy and Application of Sentiment \nAnalysis. Int J Res Mark. 2023;40(1):75-87. doi:10.1016/j.ijresmar.2022.05.005 \n27. Si Y, Wang J, Xu H, Roberts K. Enhancing clinical concept extraction with contextual embeddings. J Am Med \nInform Assoc. 2019;26(11):1297-1304. doi:10.1093/jamia/ocz096 \n28. Hutto CJ, Gilbert E. VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text. \nIn: Eighth International AAAI Conference on Weblogs and Social Media. ; 2014. Accessed July 15, 2019. \nhttps://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/view/8109 \n29. Wei J, Wang X, Schuurmans D, et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language \nModels. Adv Neural Inf Process Syst. 2022;35:24824-24837. \n30. Liu Y, Yan C, Yin Z, et al. Biomedical Research Cohort Membership Disclosure on Social Media. AMIA Annu \nSymp Proc. 2020;2019:607-616. \n31. Reddit has a new AI training deal to sell user content - The Verge. Accessed March 15, 2024. \nhttps://www.theverge.com/2024/2/17/24075670/reddit-ai-training-license-deal-user-content \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 20, 2024. ; https://doi.org/10.1101/2024.03.19.24304544doi: medRxiv preprint ",
  "topic": "Sentiment analysis",
  "concepts": [
    {
      "name": "Sentiment analysis",
      "score": 0.6806678175926208
    },
    {
      "name": "Social media",
      "score": 0.6609255075454712
    },
    {
      "name": "Computer science",
      "score": 0.5523444414138794
    },
    {
      "name": "Context (archaeology)",
      "score": 0.44052284955978394
    },
    {
      "name": "Data science",
      "score": 0.43525558710098267
    },
    {
      "name": "Health care",
      "score": 0.4190525710582733
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3561682403087616
    },
    {
      "name": "Political science",
      "score": 0.1921100616455078
    },
    {
      "name": "World Wide Web",
      "score": 0.17111963033676147
    },
    {
      "name": "Geography",
      "score": 0.08221098780632019
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I43579087",
      "name": "University of Wisconsin–Milwaukee",
      "country": "US"
    }
  ],
  "cited_by": 7
}