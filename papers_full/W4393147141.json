{
    "title": "Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis",
    "url": "https://openalex.org/W4393147141",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2324946313",
            "name": "James R. Kirk",
            "affiliations": [
                "Cambridge Cognition (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2151249208",
            "name": "Robert E. Wray",
            "affiliations": [
                "Cambridge Cognition (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2339909993",
            "name": "Peter Lindes",
            "affiliations": [
                "Cambridge Cognition (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2098086841",
            "name": "John E. Laird",
            "affiliations": [
                "Cambridge Cognition (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2324946313",
            "name": "James R. Kirk",
            "affiliations": [
                "VA Center for Clinical Management Research"
            ]
        },
        {
            "id": "https://openalex.org/A2151249208",
            "name": "Robert E. Wray",
            "affiliations": [
                "VA Center for Clinical Management Research"
            ]
        },
        {
            "id": "https://openalex.org/A2339909993",
            "name": "Peter Lindes",
            "affiliations": [
                "VA Center for Clinical Management Research"
            ]
        },
        {
            "id": "https://openalex.org/A2098086841",
            "name": "John E. Laird",
            "affiliations": [
                "VA Center for Clinical Management Research"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6803096969",
        "https://openalex.org/W4321855256",
        "https://openalex.org/W6850764995",
        "https://openalex.org/W2966636162",
        "https://openalex.org/W4296415305",
        "https://openalex.org/W4292958252",
        "https://openalex.org/W6742739335",
        "https://openalex.org/W6839092860",
        "https://openalex.org/W3192651000",
        "https://openalex.org/W2295722319",
        "https://openalex.org/W3172480024",
        "https://openalex.org/W4363671832",
        "https://openalex.org/W3130319171",
        "https://openalex.org/W4288043535",
        "https://openalex.org/W6856464235",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W4226112058",
        "https://openalex.org/W4376864573",
        "https://openalex.org/W2745881055",
        "https://openalex.org/W4388720459",
        "https://openalex.org/W4385468994",
        "https://openalex.org/W4386528753",
        "https://openalex.org/W4321011818",
        "https://openalex.org/W4387835442",
        "https://openalex.org/W4224912544",
        "https://openalex.org/W4285428875",
        "https://openalex.org/W4312807436",
        "https://openalex.org/W4361866080",
        "https://openalex.org/W4383108457",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W4287523898",
        "https://openalex.org/W4287888476",
        "https://openalex.org/W3160638507"
    ],
    "abstract": "Large language models (LLMs) offer significant promise as a knowledge source for task learning. Prompt engineering has been shown to be effective for eliciting knowledge from an LLM, but alone it is insufficient for acquiring relevant, situationally grounded knowledge for an embodied agent learning novel tasks. We describe a cognitive-agent approach, STARS, that extends and complements prompt engineering, mitigating its limitations and thus enabling an agent to acquire new task knowledge matched to its native language capabilities, embodiment, environment, and user preferences. The STARS approach is to increase the response space of LLMs and deploy general strategies, embedded within the autonomous agent, to evaluate, repair, and select among candidate responses produced by the LLM. We describe the approach and experiments that show how an agent, by retrieving and evaluating a breadth of responses from the LLM, can achieve 77-94% task completion in one-shot learning without user oversight. The approach achieves 100% task completion when human oversight (such as an indication of preference) is provided. Further, the type of oversight largely shifts from explicit, natural language instruction to simple confirmation/discomfirmation of high-quality responses that have been vetted by the agent before presentation to a user.",
    "full_text": "Improving Knowledge Extraction from LLMs\nfor Task Learning through Agent Analysis\nJames R. Kirk, Robert E. Wray, Peter Lindes, John E. Laird\nCenter for Integrated Cognition at IQMRI\nAnn Arbor, MI 48105 USA\n{james.kirk,robert.wray,peter.lindes,john.laird}@cic.iqmri.org\nAbstract\nLarge language models (LLMs) offer significant promise as a\nknowledge source for task learning. Prompt engineering has\nbeen shown to be effective for eliciting knowledge from an\nLLM, but alone it is insufficient for acquiring relevant, situa-\ntionally grounded knowledge for an embodied agent learn-\ning novel tasks. We describe a cognitive-agent approach,\nSTARS, that extends and complements prompt engineering,\nmitigating its limitations and thus enabling an agent to ac-\nquire new task knowledge matched to its native language ca-\npabilities, embodiment, environment, and user preferences.\nThe STARS approach is to increase the response space of\nLLMs and deploy general strategies, embedded within the\nautonomous agent, to evaluate, repair, and select among can-\ndidate responses produced by the LLM. We describe the ap-\nproach and experiments that show how an agent, by retriev-\ning and evaluating a breadth of responses from the LLM, can\nachieve 77 −94% task completion in one-shot learning with-\nout user oversight. The approach achieves100% task comple-\ntion when human oversight (such as an indication of prefer-\nence) is provided. Further, the type of oversight largely shifts\nfrom explicit, natural language instruction to simple confir-\nmation/discomfirmation of high-quality responses that have\nbeen vetted by the agent before presentation to a user.\nIntroduction\nPrompt engineering (Reynolds and McDonell 2021), along\nwith in-context learning (OpenAI 2023), has been shown\nto be an effective strategy for extracting knowledge from\na large language model (LLM). However, embodied agents\nlearning task knowledge (e.g., goals and actions) face far\nmore stringent requirements. LLM responses must be:\n1. Interpretable by the agent’s parsing capabilities. LLM re-\nsponses must be understandable by the agent, meaning\ngrammar and terminology are presented in a form that\nthe agent can actually process.\n2. Situated to the agent’s environment. Objects, features,\nand relations referenced in an LLM response must be per-\nceivable and identifiable in the environment for the agent\nto ground the response successfully.\n3. Matched to agent’s embodiment and affordances. An\nLLM, trained on a large corpus describing human activi-\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nties, will (generally) generate responses conforming with\nhuman embodiment and affordances. Responses that do\nnot consider an agent’s often non-human embodiment\n(e.g., a single-armed robot) will often be infeasible for\nthat agent to execute.\n4. Aligned with individual human preferences and values.\nUsers will have individual expectations about how tasks\nshould be performed and what constitutes appropriate\noutcomes in the current situation. Task success requires\nidentifying and conforming to these preferences.\nThe first three requirements are necessary for an embod-\nied agent to use an LLM response to act in its world. We\ndefine responses that meet these requirements asviable. The\nfinal requirement is necessary to achieve the task as a spe-\ncific human user prefers. A response issituationally relevant\nif it is viable and matches the user’s preferences.\nTo attempt to elicit viable responses from the LLM, we\npreviously (Kirk et al. 2023) employed a template-based\nprompting approach (TBP; Olmo, Sreedharan, and Kamb-\nhampati 2021; Kirk et al. 2022; Reynolds and McDonell\n2021). We developed prompt templates that included exam-\nples of desired task knowledge, instantiated them with con-\ntext from the current task, and retrieved multiple responses\n(varying LLM temperature to generate different responses).\nUnfortunately, this TBP strategy produced responses that of-\nten violated one or more of the first three requirements. Hu-\nman feedback could be used to overcome these limitations,\nbut required substantial input to correct responses (as well as\nto align them with agent needs and user preferences), mak-\ning TBP impractical for an embodied agent.\nMotivated by these inadequacies, we present a novel\nstrategy: Search Tree, Analyze and Repair, and Selection\n(STARS). Similar to “agentic” uses of LLMs (Sumers et al.\n2023; Park et al. 2023; Richards 2023), we employ the LLM\nas a component within a larger system. Like self-consistency\n(Wang et al. 2023), STARS generates a large space of re-\nsponses from the LLM (multiple responses to a query). In\ncontrast with the voting in self-consistency, the agent ana-\nlyzes and evaluates each response for potential issues (e.g.,\nmismatched embodiment, unknown words, ungrounded ref-\nerences). It attempts to repair problematic responses via tar-\ngeted re-prompting of the LLM. To select among candidates,\nthe agent queries the LLM for a “preferred” response. The\nSTARS agent can still solicit human feedback, but the pri-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18390\nmary purpose of oversight is to ensure that agent behavior\n(and learning) incorporates user preferences.\nTo evaluate STARS against TBP, we embed both methods\nwithin an existing embodied agent (Mohan and Laird 2014;\nMininger 2021; Kirk and Laird 2016). This agent uses inter-\nactive task learning (ITL; Laird et al. 2017; Gluck and Laird\n2019) to learn novel tasks via natural language instruction\nfrom a human user. Instead of querying a human for a goal\ndescription of the task (e.g., “the goal is that the can is in\nthe recycling bin”), the new agents (using TBP or STARS)\naccess the LLM for that goal.\nWe compare STARS to TBP and also evaluate the indi-\nvidual components of STARS (i.e., Search Tree, Analysis &\nRepair, Selection) in a simulated robotic environment. We\nassess both task completion rate and the amount of over-\nsight needed to achieve 100% task completion. We hypoth-\nesize STARS will eliminate the need to solicit human feed-\nback for unviable responses, resulting in a much higher task\ncompletion rate (without oversight) and reducing how much\noversight is required when human input is available.\nAs we show below, over three different tasks, STARS\nachieves 77-94% task completion without oversight (in com-\nparison to 35-66% with TBP). With oversight, STARS re-\nduces the number of words needed from the user by 52-68%\n(compared to TBP). Further, providing oversight is much\nsimpler for the user. The user no longer needs to evaluate the\nviability of responses nor provide (many) goal descriptions;\nnow, the user largely indicates preference, simply confirm-\ning or disconfirming from the LLM responses that the agent\nhas determined to be viable. Finally, because the original\nITL agent learns long-term task and subtask knowledge in\none shot, this new agent also demonstrates one-shot perfor-\nmance: it achieves 100% task completion when prompted to\nperform the same task in the future, without accessing the\nLLM or requiring further human input.\nRelated Work\nCore features of our approach are 1) online task learning\n(no pre-training for domain or task), 2) the exploitation of\nmultiple sources of knowledge, 3) proactive evaluation of\nLLM responses, and 4) one-shot task learning. We review\nrelated work in terms of these solution features.\nInner Monologue (Huang et al. 2022) modifies its prompts\nbased on feedback from the environment, agent, and user\nto elicit new responses when an action fails. Repair focuses\non a single response at a time; STARS analyzes a set of re-\nsponses to evaluate the result of using them, making evalu-\nations and repairs before any response is selected and used.\nLogeswaran et al. (2022) plan sequences of subgoals from\nmultiple LLM responses obtained from beam search (as in\nSTARS) that does re-ranking based on feedback from the\nenvironment. SayCan (Ahn et al. 2022) uses an LLM and\na trained set of low-level robot skills with short language\ndescriptions for objects. The LLM is prompted multiple\ntimes for a high-level task to retrieve one low-level step at\na time until a complete plan is found. To obtain knowledge\nof low-level tasks, SayCan is trained on over 68K teleoper-\nated demonstrations and human-rated simulations. STARS\nencodes properties for object classes (e.g., whether an object\ncan be “grabbed” by the robot) but requires no pre-training\nor prior exposure to the domain.\nTidyBot (Wu et al. 2023) and TIDEE (Sarch et al. 2022)\naddress robotic problems similar to one of our experimen-\ntal tasks (tidying a kitchen). They also account for human\npreferences. TidyBot tries to elicit human preferences by\nhaving the LLM summarize a few answers given by a hu-\nman. TIDEE attempts to learn preferences by using “com-\nmonsense priors” learned previously by performing tasks in\na “training house.” STARS does not depend on pre-training,\nbut does elicit human preferences via NL dialogues.\nPROGPROMPT (Singh et al. 2022) produces task plans\nby prompting an LLM with Python code that specifies the\naction primitives, objects, example tasks, and task name.\nThe LLM returns a task plan in Python which includes asser-\ntions about states of the environment that are checked during\nexecution, and recovery steps if an assertion fails. STARS re-\ntrieves NL descriptions of goals, rather than plans, and eval-\nuates goals before they are used.\nSTARS attempts to verify LLM responses before attempt-\ning to achieve the goal indicated by a response. There are\nmany approaches to verification of LLM knowledge, includ-\ning 1) response sampling (Wang et al. 2023), 2) use of other\nsources of knowledge such as planning (Valmeekam et al.\n2023) or an LLM (Kim, Baldi, and McAleer 2023), and\n3) human feedback/annotation (TidyBot). Recursively Crit-\nicizes and Improves (RCI; Kim, Baldi, and McAleer 2023)\nverifies LLM output by prompting the LLM again to iden-\ntify (potential) issues. Cobbe et al. (2021) train a verifier to\nrank responses, while self-consistency (Wang et al. 2023)\nuses voting to select an answer. Diao et al. (2023) combine\nall three of the above verification strategies by eliciting re-\nsponses from an LLM, ranking them using an uncertainty\nmetric (a source of knowledge other than the LLM), and then\nhaving humans annotate responses for further exploration.\nWhile these efforts address similar challenges (or aspects\nof them), a unique aspect of STARS is the proactive anal-\nysis of many responses retrieved via prompting an LLM\nthrough embodied reasoning. The analysis enables the iden-\ntification of known problems and targeted repairs. STARS\nalso learns goal states for tasks, rather than action sequences\nto achieve tasks. The STARS agent learns task knowledge in\none shot, during performance, without prior training. When\nconfronted with the same or similar tasks in the future, the\nagent can efficiently execute the task without the use of the\nLLM (or STARS). Encoding persistent task knowledge con-\ntrasts with in-context learning (OpenAI 2023).\nPrior Baseline: Template-based Prompting\nThe agent employs template-based prompting (TBP) to elicit\nresponses from the LLM. Templates enable the agent to con-\nstruct prompts using context from the task and environment\nand introduce prompt examples matched to the agent’s ca-\npabilities and embodiment. Figure 1 outlines the baseline\ntemplate-based prompting approach for generating task-goal\ndescriptions (i.e., it replaces the NL-dialogue for “Get goal\ndescription” in Figure 4). A prompt template is chosen and\ninstantiated with relevant context, the LLM is queried (po-\ntentially soliciting multiple responses using varying temper-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18391\nFigure 1: Baseline approach to elicitation of goal descrip-\ntions via template-based prompting (TBP).\natures), and response(s) are chosen for execution. In this\nbaseline approach, choices are ranked by the mean log prob-\nabilities of tokens in each response. Oversight is used to se-\nlect an LLM response or to give a goal description when all\nLLM-generated choices are unacceptable. The agent uses\nthe chosen response to attempt to perform the task and, if\nsuccessful, learns a policy to execute the task in the future\n(see Figure 4). Few-shot examples in the prompt bias the\nLLM toward responses that are viable and relevant, match-\ning the agent’s NLP capabilities, desired semantic content\n(e.g., simple goal statements), and embodiment limitations\n(Kirk et al. 2022). This baseline approach learns the task in\none shot but requires substantial user oversight to overcome\nerrors (Kirk et al. 2023).\nThe STARS Approach\nSTARS extends the TBP baseline with three processes: re-\ntrieving a tree of LLM responses via beam search (ST:\nSearch Tree), analyzing and repairing responses (AR: Anal-\nysis and Repair), and using the LLM to select a goal re-\nsponse from the candidates (S: Selection). After presenting\neach of these components of STARS, we describe the over-\nsight strategy of soliciting user feedback.\nFigure 2 outlines the process of the STARS approach\n(blue boxes are re-purposed elements from TBP; green\nboxes are new components of STARS). With STARS, the\nagent retrieves goal descriptions from the LLM (the rest of\nthe task-learning process is the same). STARS ensures that\nthe goal descriptions it retrieves from the LLM are viable\nfor the agent. Acquiring goal knowledge is crucial to learn-\ning novel tasks, enabling an agent with planning capabilities\nto perform the new task. Goal learning enables greater flexi-\nbility than learning a sequence of actions because goal-state\nknowledge can transfer to other situations that require dif-\nferent action sequences to achieve the same goal.\nSearch Tree (ST)\nIn prior work with TBP (Figure 1), we increased the tem-\nperature parameter iteratively to retrieve multiple responses\nfor the same prompt. This approach resulted in many du-\nplicate responses and more responses that were not viable,\ndeviating from targeted content and form. Similar to others\n(Logeswaran et al. 2022; Wang et al. 2023), here we enable\nthe agent to use a beam-search strategy to generate a breadth\nof high-probability responses from a single prompt.\nAnalyze and Repair (AR)\nWhile many responses retrieved from the LLM are rea-\nsonable, they often fail to meet other requirements: being\nFigure 2: Summary of STARS approach.\nmatched to the agent’s embodiment, language capabilities,\nand situation. An agent that attempts to use a mismatched\nresponse will fail. Analysis and Repair detects and catego-\nrizes mismatches, drawing on the cognitive agent’s knowl-\nedge and capabilities to identify problems, and then attempts\nto repair responses with identifiable mismatches.\nThe overall process for Analysis and Repair is illustrated\nin Figure 3. The agent performs a mental simulation ofwhat\nwould happen if it attempted to use a response from the\nLLM, using the same knowledge of parsing and grounding it\nuses when performing the task. The analysis evaluates inter-\npretability (orange: whether the agent can parse and interpret\nthe language and terms), grounding (green: whether each\nreferent in the response can be grounded to an object observ-\nable in the environment), and affordances (blue: whether the\nagent can achieve the actions on objects implied by clauses\nin the goal response). The “AR” process currently addresses\nthese three sources of mismatch:\n• Language: The agent parses the response with its native\nNLP capabilities and examines the output. The language\nprocessor indicates if a sentence can be interpreted and\nidentifies unknown words.\n• Situation: To detect grounding issues, the agent evalu-\nates the results of its language comprehension process.\nWhen a sentence contains a referring expression to an\nobject, such as a cabinet, the agent’s language processing\nidentifies grounding candidates observable by the agent.\nFailure to ground a referent indicates a mismatch with\nthe current situation.\n• Embodiment and Affordance: The agent detects em-\nbodiment and affordance mismatches using its knowl-\nedge of objects (semantic memory) and properties de-\ntected from perception (environment). E.g., when it pro-\ncesses a clause in a goal response such as “the dish rack\nis in the cabinet,” it evaluates if the object to be moved\n(“dish rack”) has the property “grabbable.”\nRepair is coupled to these diagnostic mismatches detected\nduring analysis. For each type of diagnosis, the agent con-\nstructs a new prompt using a repair template for that cat-\negory of mismatch. The agent instantiates the template by\nappending the non-viable response with an instruction indi-\ncating the specific mismatch that occurred, e.g., “No. Cannot\nsee a cabinet.” or “No. Rack is not grabbable.”1 ST then uses\n1A Technical Appendix provides complete examples of\nprompts for repairs and selection: https://arxiv.org/abs/2306.06770.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18392\nFigure 3: Agent analysis of mismatches via internal simula-\ntion\nthis repair prompt to generate a new tree of responses.\nSelection (S)\nST and AR are designed to generate viable candidate re-\nsponses. However, the agent must select a single response\nto use. Rather than using mean log probability (as in TBP;\nFigure 1) or voting (as in self-consistency Wang et al. 2023),\nthe new Selection strategy employs the LLM for choosing a\nresponse. The agent constructs a prompt with the candidates\nand asks which of a numbered list of candidate responses is\nthe most reasonable goal given the task context. The prompt\nsolicits a single integer response from the LLM, indicating\nwhich response is the best.\nUser Oversight (O)\nThe correct goal for some tasks depends on human pref-\nerences (e.g., some users prefer storing cereal in the cup-\nboard, others, the pantry). The original ITL agent solicited\nall task knowledge from a human, which naturally captured\nthis preference knowledge. STARS reduces user interaction\nwhile still ensuring capture of preference. Having the human\nin the loop also ensures correct learning. The agent solicits\nuser feedback by asking if a retrieved goal is correct (yes/no)\nbefore using it (below). Selection determines which option\nto present. If the first response is rejected, Selection is re-\npeated with the rejected option removed. If all responses are\nrejected, the user must provide the correct goal description.\nAgent: For a mug in the dish rack is the goal that the\nmug is in the cupboard and the cupboard is closed?\nUser: Yes.\nExperiment Design\nIn order to evaluate STARS, we first describe the embodied\nagent that incorporates STARS, an experimental design, and\nmeasures. In the next section, we present results for online\nFigure 4: ITL process for learning goals and policy.\nlearning of three different tasks: tidying the kitchen, storing\ngroceries, and organizing an office. We evaluate how well\nSTARS addresses the above requirements and also examine\nthe relative impact of components of STARS. STARS learns\ndescriptions of goal states, while systems such as SayCan,\nInnerMonologue, and TidyBot learn action sequences. We\ndo not directly compare performance for these tasks against\nthese systems because of their different learning targets.\nAgent: We embed STARS in an existing embodied ITL\nagent, replacing the human interaction that provided natu-\nral language descriptions of goals for tasks and subtasks. 2\nThe original agent learns a variety of diverse tasks (from\npuzzles to mobile patrol tasks) in many different physical\n(Fetch robot, mobile robot, and tabletop arm) and simulated\n(AI2Thor, April simulator) robotic domains (Mohan et al.\n2012; Mininger 2021; Kirk and Laird 2019).\nFigure 4 depicts the ITL process for learning goals. The\nITL agent can also learn new concepts, new actions (when\nplanning knowledge is insufficient), and lower-level skills\nvia instruction (not shown here). We focus on the goal-\nlearning pipeline here because STARS exploits an LLM to\nlearn goal descriptions (replacing the green box) without\nchanging other aspects of the pipeline. The ITL learning\nprocess depended on substantial user input to provide inter-\npretable and accurate descriptions of goals. When a policy\nfor achieving a goal is unknown, internal planning finds a\nsequence of actions that achieves the goal. A side effect of\nsuccessful planning is that the agent learns long-term policy\nknowledge in one shot via the agent architecture’s procedu-\nral learning mechanism. When the task arises in the future,\nthat learned knowledge guides agent decision-making with-\nout planning or human interaction.\nSetting: A simulated office and kitchen with a mobile\nrobot created in the APRIL MAGIC simulator. The robot\ncan move around the room, approach objects, and has a sin-\ngle arm that can grasp and manipulate all objects relevant\nto the task to be learned. For the “tidy kitchen” task (the\nlargest task), the kitchen is populated with 35 objects that\ncommonly exist in a kitchen (plates, condiments, utensils,\netc.). Objects are initially distributed on a table, counter, and\nin the dish rack. For the “store groceries” task, 15 objects\nare contained in bags on the kitchen floor that must be stored\n(into the fridge, cupboard, or pantry). For the “organize of-\nfice” task, 12 objects are distributed on a desk that must be\ncleared (into the drawer, bookshelf, trash, recycling bin, or\n2Code for the ITL agent with STARS, simulator, and data\nanalysis are available at https://github.com/Center-for-Integrated-\nCognition/STARS.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18393\nCondition Description\nTBP Template-Based Prompting (Baseline)\nTBP+O TBP with human Oversight\nST Beam Search Tree\nSTS Beam search with LLM Selection\nSTAR Beam search with Analysis (check viabil-\nity) and Repair\nSTARS Search-tree, A&R, LLM Selection\nSTARS+O STARS with human oversight.\nTrial #2 Task performance on second presentation\nafter learning with STARS+O.\nTable 1: Definition of experimental conditions.\nfiling cabinet). The three tasks contain 58 unique objects for\nwhich the agent needs to learn a goal.\nSimulation: Although prior work with the ITL agent has\nused a physical robot, this experiment is done in simula-\ntion, which is sufficient for investigating the grounding of\nconcepts and interpreting and learning from the descriptions\nprovided by STARS.\nLearning Task: For each experiment, the user presents\nthe task (e.g., “tidy kitchen”) and primary subtasks (e.g.,\nclearing, storing, and unloading all the objects from the ta-\nble, counter, and dish rack). For all tasks, task success is\nmeasured by the fraction of objects moved to a location con-\nsistent with user preferences. Also, when another object is\nmanipulated to achieve a task (e.g., opening a refrigerator\ndoor to put away ketchup), it must be in its desired state for\nthe task-success evaluation (e.g., the door must be closed).\nFor the “tidy kitchen” task, four object types have multi-\nple instances that must be treated differently based on their\npositions (e.g., a mug on the table must be put in the dish-\nwasher or sink, but a mug in the dish rack must be put in\nthe cupboard). Using the approach in Figure 2 (or a STARS\nvariant as below), the agent acquires goal descriptions for\neach perceived object. It then uses the processing described\nin Figure 4 to learn the goal and action policy, enabling it to\ncorrectly process that object in the future without the LLM,\nplanning, or oversight.\nExperimental conditions: Experimental conditions are\nenumerated in Table 1. The TBP conditions are baselines for\nassessing the impact of the components of STARS. For all\nconditions, the LLM used is GPT-3 (for TBP, Search Tree,\nand Repair) and GPT-4 (for Selection). 3 In all conditions,\na user provides the initial task. In the Oversight conditions,\nthe user reviews up to 5 responses. In non-oversight condi-\ntions, the choice of the goal is based on the highest mean\nlog probability of candidates (ST and STAR) or the Selec-\ntion strategy (STS and STARS).\nMeasures: We assess conditions in three dimensions: per-\nformance, response quality, and cost. For performance, task\ncompletion rate (number of goal assertions achieved / total\nnumber of goal assertions) is the primary measure. For re-\n3GPT-4 does not currently expose logprobs, making it inapt\nfor beam search. Selection does not use beam search and GPT-4\ndemonstrated better, more consistent results.\nCondition Comp.\n(%)\nGoals\nretvd\nTotal\ntokens\n#\ninstrct\n#\nwords\nTidy kitchen\nTBP 52.5 93 41407 14 76\nTBP+O 100.0 89 42469 92 403\nST 50.0 243 56874 14 76\nSTS 40.0 247 66458 14 76\nSTAR 77.5 353 126086 14 76\nSTARS 77.5 368 139871 14 76\nSTARS+O 100.0 361 138096 65 127\nTrial #2 100.0 0 0 1 2\nStore groceries\nTBP 66.7 39 17078 6 28\nTBP+O 100.0 37 18689 29 92\nST 66.7 96 21518 6 28\nSTS 66.7 99 25690 6 28\nSTAR 77.8 170 57709 6 28\nSTARS 94.4 171 61808 6 28\nSTARS+O 100.0 177 64501 22 44\nTrial #2 100.0 0 0 1 2\nOrganize office\nTBP 35.7 34 12992 6 28\nTBP+O 100.0 35 11662 41 184\nST 21.4 95 21082 6 28\nSTS 21.4 97 24717 6 28\nSTAR 64.3 204 75509 6 28\nSTARS 92.9 201 76056 6 28\nSTARS+O 100.0 206 77722 22 60\nTrial #2 100.0 0 0 1 2\nTable 2: Summary of outcomes by condition for three tasks.\nsponse quality, we evaluate how well responses align with\nrequirements for situational relevance and viability, as well\nas reasonableness. User effort is the largest factor impact-\ning cost, but cannot be measured directly. To estimate effort,\nwe use the number of interactions and words as well as the\npercentage of accepted goals. LLM costs are evaluated via\ntokens presented (prompts) and generated (responses).\nExperimental Results\nThe discussion of experimental results is organized around\nthe three measures introduced above. Table 2 summarizes\nperformance (task completion) and costs (tokens; oversight)\nfor each condition for the three tasks. The Trial #2 condi-\ntion shows task performance after successful learning from\nSTARS+O when given a second direction to perform the\ntask; all tasks are completed successfully without further in-\nteraction beyond receiving the task (e.g., “tidy kitchen”).4\nFor each task we ran the STARS condition 10 times. Ta-\nble 3 shows the mean values and standard deviation for task\ncompletion for each task. Due to the lack of variation be-\ntween runs (attributable to the LLM and STARS) as well\nas experimental costs (GPT budget and the time to conduct\neach condition for all task experiments) we report results\n4A video demonstration of STARS with a few objects is avail-\nable at http://tinyurl.com/STARS-AAAI24.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18394\nFigure 5: Performance and user cost measures for experi-\nmental conditions for the “tidy kitchen” task.\nfrom one run for each condition (Table 2). The overall vari-\nance for STARS is small and has a marginal effect on key\noutcomes (See Section D in the Technical Appendix for fur-\nther exploration of variability in outcomes).\nPerformance: Table 2 shows the task completion rates\nfor all experimental conditions for the the three tasks. Fig-\nure 5(a) graphically compares task completion rates for the\nlargest task: “tidy kitchen.” The baseline condition, TBP,\nachieves the experiment-defined targets (e.g., “mug in the\ndishwasher”) only 52.5% (tidy kitchen), 66.7% (store gro-\nceries), and 35.7% (organize office) of the time. Adding\nOversight to the baseline condition (TBP+O) results in\n100% task completion but vastly increases the number of re-\nquired words (5b). Because many responses from the LLM\nare not viable and situationally relevant, the user must pro-\nvide goal descriptions, resulting in many more words of in-\nstruction. Without oversight, STARS delivers a large gain in\ntask completion, increasing to 77.5% (tidy), 94.4% (store),\nand 92.9% (organize). Analysis and Repair (AR) prevents\nthe agent from using unviable responses and increases the\nnumber of viable responses via repair. Search Tree (ST)\nalone results in no improvement but is a prerequisite for AR.\nTask: Kitchen Groceries Office\nMean 77.5 93.89 92.14\nStd Dev. 2.04 1.76 2.26\nTable 3: Variation in task completion rate for three tasks\n(STARS condition only).\nFigure 6: Categorization of responses retrieved from the\nLLM (STARS condition from “tidy kitchen” task).\nThe task completion for “tidy kitchen” (77.5%) is sig-\nnificantly lower than for the other tasks using STARS. For\nthe “store groceries” and “organize office” tasks, the addi-\ntion of Selection (S) improved task completion, but did not\nfor “tidy kitchen.” From detailed analysis, we determined\nthat the agent lacks context specific to the tidy task. For in-\nstance, the agent (in this instantiation) lacks the ability to\ndiscriminate between a “clean” and “dirty” mug. In the “tidy\nkitchen” experiment, dishware on the table is assumed to be\ndirty (in terms of defining the target outcomes in the design),\nbut the agent lacks this context. When such context is pro-\nvided to the LLM (a variation we label STARS*),5 Selection\nachieves 92.5% task completion for “tidy kitchen” (without\nuser oversight), comparable to the STARS task completion\nresults for the other two tasks. In the future, we will enable\nthe user to provide this context directly.\nWith oversight, STARS task completion rises to 100%\nfor all tasks with much-reduced user input compared to\nTBP. This gain comes from shifting user input from pro-\nviding goal descriptions (often needed in TBP) to confirm-\ning LLM-generated goal descriptions with yes/no responses\n(STARS+O). In addition, as highlighted in Figure 5(c), the\ngreater precision of STARS in generating acceptable goal\ndescriptions results in the user accepting a larger fraction of\nthe goals in the oversight condition. The fraction of accepted\ngoals increases from 33% to 69% (tidy kitchen), 62% to 94%\n(store groceries), and 18% to 73% (organize office).\nQuality of Responses: Figure 6 shows the percentage of\ndifferent classifications of the responses retrieved from the\nLLM for STARS for tidying the kitchen. 6 Responses are\n5Context provided to GPT-4 as a System prompt: “Assume that\ndishware on the table or counter are dirty. Assume that bottles and\ncans are empty. Non-perishable food belongs in the pantry.”\n6Chart is representative of all conditions except TBP and Over-\nsight; see appendix for each condition for all tasks.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18395\nFigure 7: Fraction of responses used by the robot that are\nreasonable/sit. relevant for the “tidy kitchen” task.\nclassified as unviable (red), viable but not reasonable (or-\nange), reasonable (yellow), or situationally relevant (green).\nFurther categorization identifies the type of mismatch for\nunviable responses (unknown word, ungrounded object, un-\ninterpretable, affordance mismatch) and reasonable ones\n(reasonable alternative location, post-completion error, em-\nbodiment limitation). “Post-completion error” indicates a\nreasonable failure to close a door in situations where an ob-\nject might not have a door. “Embodiment limitation” cap-\ntures when the robot places an object in a location that would\notherwise be reasonable if its sensing were not limited.\nOver 70% of responses are not viable, leading to failure\nif the robot executed them; only 13% are situationally rel-\nevant, meeting all four requirements. For storing groceries\n58% were not viable and 14% were situationally relevant,\nand for organizing the office 85% were not viable and only\n5% were situationally relevant. Thus, analysis of responses\nappears essential for reliable use of an LLM by an embodied\nagent to prevent the use of unviable goal descriptions. In the\nbaseline (TBP) for tidying the kitchen, the agent retrieves\nat least one situationally relevant responses for only 15 of\nthe 35 objects, while STARS results in 100% of the objects\nhaving at least one situationally relevant response.7\nFigure 7 shows the quality of response by evaluating how\nfrequently the robot receives a viable and (at least) rea-\nsonable response (situationally relevant for some user but\nnot necessarily this one). For “tidy kitchen,” STARS (and\nSTAR) results in 100% of the used responses being at least\nreasonable. This indicates that STARS’ 77.5% task comple-\ntion is close to the best it can achieve without oversight (or\nadditional context). Human input is necessary to differenti-\nate situationally relevant goals from reasonable ones.\nCost: Table 2 shows that oversight, in the form of instruc-\ntions and words, is reduced by STARS (from 403 words to\n127 for tidy kitchen, 92 to 44 words for store groceries, and\n184 to 60 words for organize office). While the magnitude\nof the reduction is modest, the user now confirms a goal\nwith a single word in comparison to supplying a complete\ngoal description. STARS+O also increases the precision of\npresented responses (Figure 5c); 69% (kitchen), 94% (gro-\n7See appendix for graphical analysis of all conditions and tasks.\nFigure 8: LLM tokens sent (hatched) and received (solid).\ncery), and 73% (office) of responses are accepted. Figure 8\nsummarizes LLM tokens used for prompting and generation\nfor “tidy kitchen.” For this task and the others, token cost in-\ncreases substantially in Search Tree (ST) and Analysis and\nRepair (AR), because of the recursive beam search.\nConclusion\nUsing LLMs as the sole source of knowledge for an embod-\nied agent is challenging due to the specific requirements that\narise in operationalizing that knowledge. STARS enables an\nagent to more effectively exploit an LLM, ensuring that the\nresponses are viable (interpretable and grounded in the sit-\nuation and agent capabilities). STARS shifts the role of the\nLLM from being the sole knowledge source to one source\nwithin a more comprehensive task-learning process (Kirk\net al. 2023). It both addresses LLM limitations and takes\nadvantage of the knowledge, reasoning, and online learning\ncapabilities of cognitive agents.\nWhile STARS provides significant improvements, further\nexploration and development are warranted. In particular,\nSelection does not provide a consistent improvement over\nthe mean log prob choice strategy for “tidy kitchen” due to\na lack of context. For future work, we will explore improve-\nments to Selection, especially via the use of additional con-\ntext that the agent can obtain from the user and (for some\ncontexts) the LLM as briefly outlined here (STARS*).\nFinally, STARS also helps highlight the necessity of hu-\nman oversight in the use of LLMs for agent task learning.\nMinimally, oversight ensures that an agent that uses an LLM\nis not led astray by the LLM, which can produce unsafe, bi-\nased, and unethical responses (Weidinger et al. 2021). Fur-\nther, a human user will often be the only source of cer-\ntain knowledge of what goals and outcomes are appropri-\nate for the task (Requirement 4). STARS, by ensuring that\nall candidates presented to the user are viable, simplifies\nand streamlines human oversight, reserving it for knowledge\nonly a human can provide. This streamlining not only re-\nduces the tedium of interaction (as suggested by the experi-\nmental results), it also potentially allows users to better focus\non alignment with their needs, goals, and values.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18396\nEthical Statement\nThis work uses large language models which can present\nethical and social risks (Weidinger et al. 2021) such as dis-\ncrimination, exclusion, and toxicity or malicious uses. We\nconsider each of these risks.\nBecause large language models are generative, depend-\ning on their corpus and training, they can produce language\nthat reflects cultural biases, offensive stereotypes, deroga-\ntory usages, etc. In this work, where LLM queries are fo-\ncused solely on producing goal descriptions for a particu-\nlar task environment, we have not seen any responses from\nGPT-3 that include such language.\nIn terms of exclusion, the specific tasks we have chosen\ndo reflect (and mirror) cultural specificity to US/Western set-\ntings, in that the items in the kitchen and office (and the\nlabels used to describe them) are both specific to the En-\nglish language and typical of the objects one would find in\na kitchen in a home or in an office environment. One of the\nlong-term potential outcomes of this work (and ITL more\ngenerally) is that the agent is taught by human users in a\nparticular setting, allowing the human to customize agent\nbehavior to their specific setting (including its cultural con-\ntext). Investigating whether this potential can be realized in\nthe subject of future work.\nMalicious use is also a potential risk, in that this research\naims to enable human users to instruct agents to do their\nbidding. Users could theoretically instruct agents to cause\ndirect harm to others, violate laws, etc. At this point in our\nresearch, this risk is minimal because implementations are\nconfined to controlled, laboratory experiments. We are ac-\ntively investigating in other work how an ITL agent can be\nboth instructed while also following and conforming to both\ncodified rules (like laws) and social norms to further mitigate\nthe potential for malicious use.\nAcknowledgements\nThis work was supported by the Office of Naval Research,\ncontract N00014-21-1-2369. The views and conclusions\ncontained in this document are those of the authors and\nshould not be interpreted as representing the official policies,\neither expressed or implied, of the Department of Defense\nor Office of Naval Research. The U.S. Government is autho-\nrized to reproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation hereon.\nReferences\nAhn, M.; Brohan, A.; Brown, N.; Chebotar, Y .; Cortes, O.;\nDavid, B.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Her-\nzog, A.; et al. 2022. Do As I Can, Not As I Say: Grounding\nLanguage in Robotic Affordances. In 6th Annual Confer-\nence on Robot Learning.\nCobbe, K.; Kosaraju, V .; Bavarian, M.; Chen, M.; Jun, H.;\nKaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano,\nR.; Hesse, C.; and Schulman, J. 2021. Training Verifiers\nto Solve Math Word Problems. ArXiv:2110.14168 [cs],\narXiv:2110.14168.\nDiao, S.; Wang, P.; Lin, Y .; and Zhang, T. 2023. Ac-\ntive Prompting with Chain-of-Thought for Large Language\nModels. ArXiv:2302.12246 [cs].\nGluck, K.; and Laird, J., eds. 2019. Interactive Task Learn-\ning: Agents, Robots, and Humans Acquiring New Tasks\nthrough Natural Interactions, volume 26 ofStr¨ungmann Fo-\nrum Reports. Cambridge, MA: MIT Press.\nHuang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Flo-\nrence, P.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar, Y .;\net al. 2022. Inner Monologue: Embodied Reasoning through\nPlanning with Language Models. In 6th Annual Conference\non Robot Learning.\nKim, G.; Baldi, P.; and McAleer, S. 2023. Language Models\ncan Solve Computer Tasks. ArXiv:2303.17491 [cs].\nKirk, J. R.; and Laird, J. E. 2016. Learning General and Ef-\nficient Representations of Novel Games Through Interactive\nInstruction. In Proceedings of the Advances in Cognitive\nSystems Conference. ISBN 0021-9967.\nKirk, J. R.; and Laird, J. E. 2019. Learning Hierarchi-\ncal Symbolic Representations to Support Interactive Task\nLearning and Knowledge Transfer. In Proceedings of IJ-\nCAI 2019, 6095–6102. International Joint Conferences on\nArtificial Intelligence.\nKirk, J. R.; Wray, R. E.; Lindes, P.; and Laird, J. E. 2022.\nImproving Language Model Prompting in Support of Semi-\nautonomous Task Learning. In Proceedings of the Advances\nin Cognitive Systems (ACS) Conference.\nKirk, J. R.; Wray, R. E.; Lindes, P.; and Laird, J. E. 2023.\nIntegrating Diverse Knowledge Sources for Online One-\nshot Learning of Novel Tasks. ArXiv:2208.09554 [cs],\narXiv:2208.09554.\nLaird, J. E.; Gluck, K.; Anderson, J. R.; Forbus, K.; Jenk-\nins, O.; Lebiere, C.; Salvucci, D.; Scheutz, M.; Thomaz, A.;\nTrafton, G.; Wray, R. E.; Mohan, S.; and Kirk, J. R. 2017.\nInteractive Task Learning. IEEE Int. Sys., 32(4): 6–21.\nLogeswaran, L.; Fu, Y .; Lee, M.; and Lee, H. 2022. Few-shot\nSubgoal Planning with Language Models. In Proceedings\nof the 2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Lan-\nguage Technologies (NAACL), 5493–5506. Association for\nComputational Linguistics.\nMininger, A. 2021. Expanding Task Diversity in\nExplanation-Based Interactive Task Learning. Ph.D. The-\nsis, University of Michigan, Ann Arbor.\nMohan, S.; and Laird, J. E. 2014. Learning Goal-Oriented\nHierarchical Tasks from Situated Interactive Instruction. In\nProceedings of the 28 th AAAI Conference on Artificial In-\ntelligence, volume 2, 113–130. AAAI Press.\nMohan, S.; Mininger, A.; Kirk, J.; and Laird, J. E. 2012.\nAcquiring Grounded Representation of Words with Situated\nInteractive Instruction. Advances in Cognitive Systems, 2:\n113–130.\nOlmo, A.; Sreedharan, S.; and Kambhampati, S. 2021.\nGPT3-to-plan: Extracting plans from text using GPT-3. In\nProceedings of ICAPS FinPlan. ArXiv: 2106.07131 [cs].\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18397\nPark, J. S.; O’Brien, J.; Cai, C. J.; Morris, M. R.; Liang,\nP.; and Bernstein, M. S. 2023. Generative Agents: Interac-\ntive Simulacra of Human Behavior. In Proceedings of the\n36th Annual ACM Symposium on User Interface Software\nand Technology, 1–22.\nReynolds, L.; and McDonell, K. 2021. Prompt Program-\nming for Large Language Models: Beyond the Few-Shot\nParadigm. In Extended Abstracts of the 2021 CHI Confer-\nence on Human Factors in Computing Systems, CHI EA ’21.\nNew York, NY , USA: ACM. ISBN 9781450380959.\nRichards, T. B. 2023. Auto-GPT: An Autonomous GPT-4\nExperiment.\nSarch, G.; Fang, Z.; Harley, A. W.; Schydlo, P.; Tarr, M. J.;\nGupta, S.; and Fragkiadaki, K. 2022. TIDEE: Tidying Up\nNovel Rooms using Visuo-Semantic Commonsense Priors.\nIn Computer Vision–ECCV 2022, 480–496. Springer.\nSingh, I.; Blukis, V .; Mousavian, A.; Goyal, A.; Xu, D.;\nTremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2022.\nProgPrompt: Generating Situated Robot Task Plans using\nLarge Language Models. arXiv:2209.11302.\nSumers, T. R.; Yao, S.; Narasimhan, K.; and Griffiths,\nT. L. 2023. Cognitive Architectures for Language Agents.\nArXiv:2309.02427 [cs].\nValmeekam, K.; Sreedharan, S.; Marquez, M.; Olmo, A.;\nand Kambhampati, S. 2023. On the Planning Abilities of\nLarge Language Models (A Critical Investigation with a Pro-\nposed Benchmark). arXiv:2302.06706.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,\nS.; Chowdhery, A.; and Zhou, D. 2023. Self-Consistency\nImproves Chain of Thought Reasoning in Language Mod-\nels. In The Eleventh International Conference on Learning\nRepresentations (ICLR 2023).\nWeidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato, J.;\nHuang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh,\nA.; Kenton, Z.; Brown, S.; Hawkins, W.; Stepleton, T.; Biles,\nC.; Birhane, A.; Haas, J.; Rimell, L.; Hendricks, L. A.;\nIsaac, W.; Legassick, S.; Irving, G.; and Gabriel, I. 2021.\nEthical and social risks of harm from Language Models.\nArXiv:2112.04359 [cs].\nWu, J.; Antonova, R.; Kan, A.; Lepert, M.; Zeng, A.; Song,\nS.; Bohg, J.; Rusinkiewicz, S.; and Funkhouser, T. 2023.\nTidyBot: Personalized Robot Assistance with Large Lan-\nguage Models. ArXiv:2305.05658 [cs], arXiv:2305.05658.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18398"
}