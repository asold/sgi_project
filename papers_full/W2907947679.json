{
    "title": "Transformer to CNN: Label-scarce distillation for efficient text classification",
    "url": "https://openalex.org/W2907947679",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221573425",
            "name": "Chia, Yew Ken",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4295186180",
            "name": "Witteveen, Sam",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4286922568",
            "name": "Andrews, Martin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2953291403",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2886845974",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2949541494",
        "https://openalex.org/W2963153906",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W1816313093",
        "https://openalex.org/W2149684865",
        "https://openalex.org/W2963912736",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W2250539671"
    ],
    "abstract": "Significant advances have been made in Natural Language Processing (NLP) modelling since the beginning of 2018. The new approaches allow for accurate results, even when there is little labelled data, because these NLP models can benefit from training on both task-agnostic and task-specific unlabelled data. However, these advantages come with significant size and computational costs. This workshop paper outlines how our proposed convolutional student architecture, having been trained by a distillation process from a large-scale model, can achieve 300x inference speedup and 39x reduction in parameter count. In some cases, the student model performance surpasses its teacher on the studied tasks.",
    "full_text": "arXiv:1909.03508v1  [cs.LG]  8 Sep 2019\nT ransformer to CNN: Label-scarce distillation for\nefﬁcient text classiﬁcation\nY ew Ken Chia\nRed Dragon AI\nSingapore\nken@reddragon.ai\nSam Witteveen\nRed Dragon AI\nSingapore\nsam@reddragon.ai\nMartin Andrews\nRed Dragon AI\nSingapore\nmartin@reddragon.ai\nAbstract\nSigniﬁcant advances have been made in Natural Language Proc essing (NLP) mod-\nelling since the beginning of 2018. The new approaches allow for accurate results,\neven when there is little labelled data, because these NLP mo dels can beneﬁt from\ntraining on both task-agnostic and task-speciﬁc unlabelle d data. However, these\nadvantages come with signiﬁcant size and computational cos ts.\nThis workshop paper outlines how our proposed convolutiona l student architec-\nture, having been trained by a distillation process from a la rge-scale model, can\nachieve 300× inference speedup and 39× reduction in parameter count. In some\ncases, the student model performance surpasses its teacher on the studied tasks.\n1 Introduction\nThe last year has seen several major advances in NLP modellin g, stemming from previous innova-\ntions in embeddings [1] [2] [3] and attention models [4] [5] [ 6] that allow Language Models (LMs)\nto be trained on very large corpuses : For instance ELMo [7], O penAI Transformer [8] and recently\nBER T [9].\nIn addition, the power of building on LM-enhanced contextua lised embeddings, using a ﬁne-tuning\napproach on task-speciﬁc unlabelled data [10], has shown hu ge beneﬁts for downstream tasks (such\nas text classiﬁcation) - especially in a typical industrial setting where labelled data is scarce.\nIn order to make use of these advances, this work shows how a mo del distillation process [11] can\nbe used to train a novel ‘student’ CNN structure from a much la rger ‘teacher’ Language Model. The\nteacher model can be ﬁne-tuned on the speciﬁc task at hand, us ing both unlabelled data, and the\n(small number of) labelled training examples available. Th e student network can then be trained\nusing both labelled and unlabelled data, in a process akin to pseudo-labelling [12] [13].\nOur results show it is possible to achieve similar performan ce to (and surpass in some cases) large\nattention-based models with a novel, highly efﬁcient stude nt model with only convolutional layers.\n2 Model distillation\nIn this work, we used the OpenAI Transformer [8] model as the ‘ teacher’ in a model-distillation\nsetting, with a variety of different ‘student’ networks (se e Figure 1).\nThe OpenAI Transformer model consists of a Byte-Pair Encode d subword [14] embedding layer fol-\nlowed by 12-layers of “decoder-only transformer with maske d self-attention heads” [4], pretrained\non the standard language modelling objective on a corpus of 7 000 books. This LM’s ﬁnal layer\noutputs were then coupled with classiﬁcation modules and th e entire model was discriminatively\nﬁne-tuned with an auxiliary language modelling objective, achieving excellent performance on vari-\nous NLP tasks.\n32nd Conference on Neural Information Processing Systems ( NIPS 2018), Montréal, Canada.\nE1 E2 ... EN\nT1\nTrm\nTNT2 ...\n...\n...\nTrmTrm\nTrm TrmTrm\nCTN\nLTN\nE1 E2 ... EN\nCNN\nCNN\nCNN\nPool\nPool\nPool\n...\n...\nCCNN\nLCNN\nConcat + MLP\nDistillation Loss\nBlendCNN (student)OpenAI GPT\n+ classi\ner\n(teacher)\nFigure 1: Model architecture with distillation across logi ts\nT o optimize for speed and memory constraints of industrial d eployment, a variety of different models\nwere trained (a) on the classiﬁcation task directly; and (b) via distillation [11] of the logit layer output\nby the pretrained OpenAI classiﬁcation model.\nT o combat label-scarcity and improve distillation quality , we inferred distillation logits for unla-\nbelled samples in a pseudo-labelling manner [12] [13], whil e using transfer learning through pre-\ntrained GloV e embeddings [2].\nStudent models\nA number of common network structures were tested in the stud ent role, speciﬁcally:\n• a two-layer BiLSTM network [15]\n• a wide-but shallow CNN network [16]\n• a novel CNN structure, dubbed here ‘BlendCNN’\nThe BlendCNN architecture was inspired by the ELMo ‘somethi ng from every layer’ paradigm, and\naims to be capable of leveraging hierarchical representati ons for text classiﬁcation [6] [17].\nThe BlendCNN model is illustrated in Figure 1, and comprises a number of CNN layers (with\nn_channels=100, kernel_width=5, activation=relu), each of which exposes a global pooling\noutput as a ‘branch’. These branches are then concatenated t ogether and “blended” through a dense\nnetwork ( width=100), followed by the usual classiﬁcation logits layer.\nT able 1: Parameter counts and inference timing\nT otal Sentences\nparameters2 per second 3\n2-Layer BiLSTM 1 2,406,114 173.01\nKimCNN 2,124,824 3154.57\nOpenAI Transformer 116,534,790 11.76\n8-layer BlendCNN 3,617,426 2392.34\n3-layer BlendCNN 2,975,236 3676.47\n1 BiLSTM and KimCNN model scores were\nlower\n2 Parameter count estimate using a\nvocab_size of 20000 words, except\nfor OpenAI Transformer (which uses a\nbyte-pair encoding embedding)\n3 Timing measurement used n_samples=1000,\nbatch_size=32, based on actual time taken\nfor K-80 GPU implementations\n2\nT able 2: Scores for standard datasets\nAG News DBpedia Y ahoo Answers\nTR A I N E D O N 100 L A B E L L E D E X A M P L E S P E R C L A S S\nTFIDF + SVM 81.9 94.1 54.5\nfastT ext 75.2 91.0 44.9\n8-Layer BlendCNN 87.6 94.6 58.3\nOpenAI Transformer 88.7 97.5 70.4\nTR A I N E D B Y D I S T I L L AT I O N 1 O F OP E NAI T R A N S F O R M E R\n2-Layer BiLSTM 91.2 97.0 70.5\nKimCNN 90.9 97.6 70.4\n3-Layer BlendCNN 91.2 / 88.42 98.2 / 95.5 71.0 / 63.4\n8-Layer BlendCNN 91.2 / 89.9 98.5 / 96.0 70.8 / 63.4\n1 Distillation training used 100 labelled examples per class , plus 10 times as many\nunlabelled examples as pseudo-labelled by the OpenAI LM\n2 Small ﬁgures are results where distillation was conducted w ithout unlabelled data\n* All CNNs use 100-dimensional trainable GloV e embeddings as input\n* Adam optimisation was used, with a constant learning rate of 10− 3\n3 Experiments\nEach of the models was trained and tested on the 3 standard dat asets described in [18] : AG News,\nDBpedia and Y ahoo Answers. The experiment proceeded in two p hases, the ﬁrst being to evaluate\nthe performance of two baseline methods (TFIDF+SVM [19] and fastT ext [20]) along with that of\nthe student networks (without the beneﬁt of a LM teacher), an d the large LM, with a classiﬁcation\n‘head’ trained on the task. The second phase used the large LM in a ‘teacher’ role, to train the other\nnetworks as students via distillation of the LM classiﬁer lo gits layer (with a Mean Absolute Error\nloss function).\n4 Results\nReferring to T able 2, the 3-Layer and 8-Layer variants of the proposed BlendCNN architecture\nachieve the top scores across all studied datasets. However , the performance of the proposed archi-\ntecture is lower without the ‘guidance’ of the teacher teach er logits during training, implying the\nmarked improvement is due to distillation. The additional r esults given for BlendCNN quantiﬁes\nthe advantage of adding unlabelled data into the distillati on phase of the student model training.\nNotably from T able 1, the 3-Layer BlendCNN student has 39× fewer parameters and performs\ninference 300× faster than the OpenAI Transformer which it empirically out -scores.\n5 Discussion\nFor text classiﬁcations, mastery may require both high-lev el concepts gleaned from language under\nstanding and ﬁne-grained textual features such as key phras es. Similar to the larval-adult form\nanalogy made in [11], high-capacity models with task-agnos tic pre-training may be well-suited for\ntask mastery on small datasets (which are common in industry ). On the other hand, convolutional\nstudent architectures may be more ideal for practical appli cations by taking advantage of massively\nparallel computation and a signiﬁcantly reduced memory foo tprint.\nOur results suggest that the proposed BlendCNN architectur e can efﬁciently achieve higher scores\non text classiﬁcation tasks due to the direct leveraging of h ierarchical representations, which are\nlearnable (even in a label-sparse setting) from a strong tea ching model.\nFurther development of specialized student architectures could similarly surpass teacher perfor-\nmance if appropriately designed to leverage the knowledge g ained from a pretrained, task-agnostic\nteacher model whilst optimizing for task-speciﬁc constrai nts.\n3\nReferences\n[1] T omas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed repre-\nsentations of words and phrases and their compositionality . In Advances in neural information\nprocessing systems , pages 3111–3119, 2013.\n[2] Jeffrey Pennington, Richard Socher, and Christopher Ma nning. Glove: Global vectors for\nword representation. In Proceedings of the 2014 conference on empirical methods in n atural\nlanguage processing (EMNLP) , pages 1532–1543, 2014.\n[3] Y e Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmana bhan, and Graham Neubig. When\nand why are pre-trained word embeddings useful for neural ma chine translation? In Proceed-\nings of the 2018 Conference of the North American Chapter of t he Association for Computa-\ntional Linguistics: Human Language T echnologies, V olume 2 (Short P apers), volume 2, pages\n529–535, 2018.\n[4] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkor eit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you ne ed. In Advances in Neural Infor-\nmation Processing Systems , pages 5998–6008, 2017.\n[5] Maha Elbayad, Laurent Besacier, and Jakob V erbeek. Perv asive attention: 2d convolutional\nneural networks for sequence-to-sequence prediction. In Conference on Computational Natu-\nral Language Learning , 2018.\n[6] Zichao Y ang, Diyi Y ang, Chris Dyer, Xiaodong He, Alex Smo la, and Eduard Hovy. Hierarchi-\ncal attention networks for document classiﬁcation. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for Computati onal Linguistics: Human Lan-\nguage T echnologies, pages 1480–1489, 2016.\n[7] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardne r, Christopher Clark, Ken-\nton Lee, and Luke Zettlemoyer. Deep contextualized word rep resentations. arXiv preprint\narXiv:1802.05365 , 2018.\n[8] Alec Radford, Karthik Narasimhan, Time Salimans, and Il ya Sutskever. Improving language\nunderstanding with unsupervised learning. T echnical report, OpenAI , 2018.\n[9] Jacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina T outanova. Bert: Pre-training of\ndeep bidirectional transformers for language understandi ng. arXiv preprint arXiv:1810.04805 ,\n2018.\n[10] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association fo r Computational\nLinguistics (V olume 1: Long P apers) , volume 1, pages 328–339, 2018.\n[11] Geoffrey Hinton, Oriol V inyals, and Jeff Dean. Distill ing the knowledge in a neural network.\narXiv preprint arXiv:1503.02531 , 2015.\n[12] Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient s emi-supervised learning method\nfor deep neural networks. In W orkshop on Challenges in Representation Learning, ICML ,\nvolume 3, page 2, 2013.\n[13] Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc V Le. Semi-supervised\nsequence modeling with cross-view training. arXiv preprint arXiv:1809.08370 , 2018.\n[14] Rico Sennrich, Barry Haddow , and Alexandra Birch. Neur al machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[15] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Recurrent neural network for text classiﬁcation\nwith multi-task learning. In Proceedings of the T wenty-Fifth International Joint Confe rence on\nArtiﬁcial Intelligence , pages 2873–2879. AAAI Press, 2016.\n[16] Y oon Kim. Convolutional neural networks for sentence c lassiﬁcation. arXiv preprint\narXiv:1408.5882, 2014.\n4\n[17] Jason Y osinski, Jeff Clune, Y oshua Bengio, and Hod Lips on. How transferable are features\nin deep neural networks? In Advances in neural information processing systems , pages 3320–\n3328, 2014.\n[18] Guoyin W ang, Chunyuan Li, W enlin W ang, Y izhe Zhang, Din ghan Shen, Xinyuan Zhang, Ri-\ncardo Henao, and Lawrence Carin. Joint embedding of words an d labels for text classiﬁcation.\nIn Proceedings of the 56th Annual Meeting of the Association fo r Computational Linguistics\n(V olume 1: Long P apers) , pages 2321–2331. Association for Computational Linguist ics, 2018.\nURL http://aclweb.org/anthology/P18-1216.\n[19] Thorsten Joachims. T ext categorization with support v ector machines: Learning with many\nrelevant features. In European conference on machine learning , pages 137–142. Springer,\n1998.\n[20] Edouard Grave, T omas Mikolov, Armand Joulin, and Piotr Bojanowski. Bag of tricks for\nefﬁcient text classiﬁcation. In Proceedings of the 15th Conference of the European Chapter o f\nthe Association for Computational Linguistics, EACL , pages 3–7, 2017.\n5"
}