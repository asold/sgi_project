{
  "title": "DoodleFormer: Creative Sketch Drawing with¬†Transformers",
  "url": "https://openalex.org/W4226266780",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5038804855",
      "name": "Ankan Kumar Bhunia",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5012655817",
      "name": "Salman A. Khan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5009362997",
      "name": "Hisham Cholakkal",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5022929869",
      "name": "Rao Muhammad Anwer",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5317838346",
      "name": "Deleted Author ID",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5036133390",
      "name": "Jorma Laaksonen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5042087981",
      "name": "Michael Felsberg",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1965142824",
    "https://openalex.org/W2904759000",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6600002382",
    "https://openalex.org/W6743198006",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3176381160",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W2280900767",
    "https://openalex.org/W3035058753",
    "https://openalex.org/W4214684804",
    "https://openalex.org/W2945895945",
    "https://openalex.org/W3107547765",
    "https://openalex.org/W3194134402",
    "https://openalex.org/W2983248633",
    "https://openalex.org/W3163884521",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3107803166",
    "https://openalex.org/W6727693337",
    "https://openalex.org/W2963966654",
    "https://openalex.org/W2964024144",
    "https://openalex.org/W2892901150"
  ],
  "abstract": null,
  "full_text": "DoodleFormer: Creative Sketch Drawing with\nTransformers\nAnkan Kumar Bhunia1, Salman Khan1,2, Hisham Cholakkal1, Rao Muhammad\nAnwer1,3, Fahad Shahbaz Khan1,4, Jorma Laaksonen3, Michael Felsberg4\n1 Mohamed bin Zayed University of AI, UAE 2 Australian National University, Australia\n3 Aalto University, Finland 4 Link¬®oping University, Sweden\nankan.bhunia@mbzuai.ac.ae\nFig. 1: Examples of visual creative sketches generated using the proposed DoodleFormer. Here,\nwe show creative sketches generated based on (a) the random input strokes, (b) text inputs and (c)\nincomplete sketch images provided by the user. In all three scenarios, the generated sketches are\nwell aligned with the user provided inputs (e.g., the creative sketches generated for the text inputs\n‚Äúwalking forward‚Äù and ‚Äúfly up high‚Äù in (b)). Similarly, the diversity in terms of appearance, pos-\nture and part size can be observed within the generated creative bird sketches in (a). Furthermore,\nDoodleFormer accurately completes the missing bird wings, legs and beak in the bottom right\nexample in (c). Additional examples are available in Fig. 2, Fig. 4, Fig. 6 and the supplementary.\nAbstract. Creative sketching or doodling is an expressive activity, where imagi-\nnative and previously unseen depictions of everyday visual objects are drawn. Cre-\native sketch image generation is a challenging vision problem, where the task is\nto generate diverse, yet realistic creative sketches possessing the unseen com-\nposition of the visual-world objects. Here, we propose a novel coarse-to-fine\ntwo-stage framework, DoodleFormer, that decomposes the creative sketch gen-\neration problem into the creation of coarse sketch composition followed by the\nincorporation of fine-details in the sketch. We introduce graph-aware transformer\nencoders that effectively capture global dynamic as well as local static struc-\ntural relations among different body parts. To ensure diversity of the generated\ncreative sketches, we introduce a probabilistic coarse sketch decoder that explic-\nitly models the variations of each sketch body part to be drawn. Experiments\nare performed on two creative sketch datasets: Creative Birds and Creative Crea-\ntures. Our qualitative, quantitative and human-based evaluations show that Doo-\ndleFormer outperforms the state-of-the-art on both datasets, yielding realistic and\ndiverse creative sketches. On Creative Creatures, DoodleFormer achieves an ab-\nsolute gain of 25 in Fr`echet inception distance (FID) over state-of-the-art. We also\ndemonstrate the effectiveness of DoodleFormer for related applications of text to\ncreative sketch generation, sketch completion and house layout generation. Code\nis available at: https://github.com/ankanbhunia/doodleformer\narXiv:2112.03258v3  [cs.CV]  15 Sep 2022\n2 A. K. Bhunia et al.\nFig. 2: A visual comparison of creative sketch images generated by DoodlerGAN [10] (top row)\nand the proposed DoodleFormer (bottom row) for the same initial random input strokes. We show\nexamples from both Creative Birds (a) and Creative Creatures (b) datasets. DoodlerGAN suffers\nfrom topological artefacts (e.g., more than one head like region in the third bird sketch from the\nleft), disconnected body parts (e.g., the fifth sketch from the left in creatures). Further, the Dood-\nlerGAN generated creative sketches have lesser diversity in terms of size, appearance and posture.\nThe proposed DoodleFormer alleviates the issues of topological artefacts and disconnected body\nparts, generating creative sketches that are more realistic and diverse.\n1 Introduction\nHumans have an outstanding ability to easily communicate and express abstract ideas\nand emotions through sketch drawings. Generally, a sketch comprises several strokes,\nwhere each stroke can be considered as a group of points. In automatic sketch image\ngeneration, the objective is to generate recognizable sketches that are closely related to\nthe real-world visual concepts. Here, the focus is to learn more canonical and mundane\ninterpretations of everyday objects.\nDifferent from the standard sketch generation problem discussed above, creative\nsketch generation [10] involves drawing more imaginative and previously unseen depic-\ntions of everyday visual concepts (see Fig. 1(a)). In this problem, creative sketches are\ngenerated according to externally provided random input strokes. Example of creative\nsketch generation includes doodling activity, where diverse, yet recognizable sketch im-\nages are generated through unseen composition of everyday visual concepts. Automatic\ngeneration of creative sketches can largely assist human creative process e.g., inspiring\nfurther ideas by providing a possible interpretation of initial sketches by the user. How-\never, such a creative task is more challenging compared to mimicking real-world scenes\nin to sketch images. This work investigates the problem of creative sketch generation.\nRecently, Ge et al. [10] address the creative sketch image generation problem by\nproposing a part-based Generative Adversarial Network called DoodlerGAN. It utilizes\na part-specific generator to produce each body part of the sketch. The generated body\nparts are then sequentially integrated with the externally provided random input, for ob-\ntaining final sketch image. Although DoodlerGAN utilizes a part-specific generator for\ncreating each body part of the sketch, it does not comprise an explicit mechanism to en-\nsure that each body part is placed appropriately with respect to the rest of the parts. This\nleads to topological artifacts and connectivity issues (see Fig. 2). Further, DoodlerGAN\nstruggles to generate diverse sketch images, which is an especially desired property in\ncreative sketch generation.\nIn this work, we argue that the aforementioned problems of topological artefacts,\nconnectivity and diversity issues can be alleviated by imitating the naturalcoarse-to-fine\ncreative sketch drawing process, where the artist first draws the holistic coarse structure\nDoodleFormer: Creative Sketch Drawing with Transformers 3\nof the sketch and then fills the fine-details to generate the final sketch. By first drawing\nthe holistic coarse structure of the sketch aids to appropriately decide the location and\nthe size of each sketch body part to be drawn. To imitate such a coarse-to-fine creative\nsketch generation process, we look into a two-stage framework where the global as\nwell as local structural relations among different body parts can be first captured at a\ncoarse-level followed by obtaining the fine-level sketch. The coarse-to-fine framework\nis expected to further improve the diversity of the creative sketch images by explicitly\nmodeling the variations in the location and size of each sketch body part to be drawn.\n1.1 Contributions\nWe propose a novel two-stage encoder-decoder framework, DoodleFormer, for creative\nsketch generation. DoodleFormer decomposes the creative sketch generation problem\ninto the construction of holistic coarse sketch composition followed by injecting fine-\ndetails to generate final sketch image. To generate realistic sketch images, we introduce\ngraph-aware transformer (GAT) encoders that effectively encode the local structural\nrelations between different sketch body parts by integrating a static adjacency-based\ngraph into the dynamic self-attention block. We further introduce a probabilistic coarse\nsketch decoder that utilizes Gaussian mixture models (GMMs) to obtain diverse loca-\ntions of each body part, thereby improving the diversity of output sketches (see Fig. 2).\nWe evaluate the proposed DoodleFormer by conducting extensive qualitative, quan-\ntitative and human-based evaluations on the recently introduced Creative Birds and Cre-\native Creatures datasets. Our DoodleFormer performs favorably against DoodlerGAN\non all three evaluations. For instance, DoodleFormer sketches were interpreted to be\ndrawn by a human 86%, having better strokes integration 85% and being more creative\n82%, over DoodlerGAN in terms of human-based evaluation. Further, DoodleFormer\noutperforms DoodlerGAN with absolute gains of 25 and 23 in terms of Fr `echet incep-\ntion distance (FID) on Creative Creatures and Creative Birds, respectively. In addition to\nsketch generation based on externally provided random initial strokes, we validate the\neffectiveness of DoodleFormer to generate creative sketches based on text inputs, in-\ncomplete sketch images provided by user as well as generating complete house layouts\ngiven coarse-level bubble diagrams. DoodleFormer achieves impressive performance\nfor text to sketch generation, sketch completion (see Fig. 1 (b) and (c)) as well as house\nlayout generation (see Fig. 8).\n2 Related Work\nThe problem of sketch generation [12,16,20,5,37,7] has been studied extensively in lit-\nerature. These methods generally aim to mimic the visual world by capturing its impor-\ntant aspects in the generated sketches. SketchRNN [12], utilizes sequence-to-sequence\nVariational Autoencoder (V AE) for conditional and unconditional generation of vector\nsketches. Cao et al. [5] propose a generative model that generates multi-class sketches.\nMoreover, alternative strategies such as differentiable rendering [37], attention-based\narchitectures [27] and reinforcement learning [3,38,9] have been investigated for sketch\ngeneration. The work of [7] incorporates a convolutional encoder to capture the spatial\n4 A. K. Bhunia et al.\nlayout of sketches, whereas [23,29,25] aim at completing the missing parts of sketches.\nThe work of [21] targets recovering the masked parts of points in sketches. A few works\n[27,21] have also studied related tasks of sketch classification and retrieval.\nDifferent from the aforementioned standard sketch generation task, creative sketch\ngeneration has been recently explored [10]. This task focuses on drawing more imagi-\nnative and previously unseen depictions of common visual concepts rather than generat-\ning canonical and mundane interpretations of visual objects. To this end, DoodlerGAN\n[10] introduces a part-based Generative Adversarial Network built on StyleGAN2 [17]\nto sequentially produce each body part of the creative sketch image. Here, the part-\nbased GAN model needs to be trained separately for individual body parts (eye, head,\nbeak, etc.) using part annotations. However, such a separate model for each body part\nresults in a large computational overhead. During inference, these individual part-based\nGAN models are sequentially used to generate their respective body parts within the\ncreative sketch. While generating recognizable creative sketches, DoodlerGANs strug-\ngles with topological artifacts, connectivity and diversity issues. In this work, we set\nout to overcome these issues to generate diverse, yet realistic creative sketches.\n3 Our Approach\nMotivation: To motivate our framework, we first distinguish two desirable properties\nto be considered when designing an approach for creative sketch generation.\nHolistic Sketch Part Composition:As discussed earlier, DoodlerGAN employs a part-\nspecific generator to produce each body part of the sketch. However, it does not utilize\nany explicit mechanism to ensure that the generated part is placed in an appropriate lo-\ncation relative to other parts, thereby suffering from topological artifacts and connectiv-\nity issues (see Fig. 2). Here, we argue that explicitly capturing the holistic arrangement\nof the sketch parts is desired to generate realistic sketch images that avoid topological\nartifacts and connectivity issues.\nFine-level Diverse Sketch Generation: Creative sketches exhibit a large diversity in ap-\npearance, posing a major challenge when generating diverse, yet realistic fine-detailed\nsketch images. Existing work of DoodlerGAN struggles to generate diverse sketch im-\nages since it typically ignores the noise input in the sketch generation process [26]. Al-\nthough DoodlerGAN attempts to partially address this issue by introducing heuristics\nin the form of randomly translating the input partial sketch, the diversity of generated\nsketch images is still far from satisfactory (see Fig. 2). Instead, we argue that having an\nexplicit probabilistic modeling within the framework is expected to further improve the\ndiversity of the generated sketch images.\nOverall Framework: The proposed two-stage DoodleFormer framework combines the\ntwo aforementioned desired properties by decomposing the creative sketch generation\nproblem to first capture the holistic coarse sketch composition and then injecting fine-\ndetails to generate the final sketch. The overall architecture of the proposed two-stage\nDoodleFormer is shown in Fig. 3. DoodleFormer comprises two stages: Part Locator\n(PL-Net) and Part Sketcher (PS-Net). The first stage, Part Locator (PL-Net), learns to\nexplicitly capture the holistic arrangement of the sketch parts conditioned on the ex-\nternally provided random initial stroke points C represented in a vector form. PL-Net\nDoodleFormer: Creative Sketch Drawing with Transformers 5\nùíû\n~\nùíõ\n‚Ä¶ ‚Ä¶\n‚Ä¶\n‚Ñãùë•ùë¶ ‚Ñãùë§‚Ñé\n‡¥§ùê∏ùëê\n‡¥§ùê∏ùëè\n‚Ä¶\n‡¥§ùë∞ùëñùëöRasterization ‚Ñõùê∏ ‚Ñõùê∑\n‚Ä¶\nDiscriminators\nùíüùëùùëéùëüùë°\nùíüùëñùëö\nùíüùëéùëùùëù\nùë∞ùíû\nGenerated Boxes (‡¥•‚Ñ¨)\n‚Ñíùëùùëéùëüùë°\n‚Ñíùëñùëö\n‚Ñíùëéùëùùëù\nùíñùë°\n: train & test : only train ùíû: initial stroke points C : concatenation : sampling~\n(a) Part Locator Network (PL-Net) (b) Part Sketcher Network (PS-Net)\n‚Ä¶\nùê∏ùëê\nùê∏ùëè\nKL Div.\nLegend\nGenerated Image (‡¥§ùë∞ùëñùëö)\nRecog-Net \nC\nùë£ùë°\n‚Ñ¨ C\nùíû\n“ßùë£ùë°\nC\nùë£ùë°: part embedding \n‚Ä¶\nMask \nRegressor\n‚Ñ¨: target boxes\nLinearwings\ntail\nbody\nhead\nbeak\nLinear Linear\nLinear Linear\nheadtailbody\nùë£ùë°\nGAT Encoders GAT Encoders \nProbabilistic Coarse Sketch Decoder\nPrior-Net \nGMMGMM\n‚Ä¶\n: cls token\nFig. 3: The proposed DoodleFormer comprises two stages: Part Locator (PL-Net) and Part\nSketcher (PS-Net). (a) The first stage, PL-Net, takes the initial stroke points C as the conditional\ninput and learns to return the bounding boxes corresponding to each body part (coarse structure\nof the sketch) to be drawn. PL-Net contains two graph-aware transformer (GAT) encoders ( Eb,\nEc) and a probablistic coarse sketch decoder utilizing GMM modelling for the coarse box pre-\ndiction. Within the decoder, the bounding box parameters are predicted by the location-predictor\n(Hxy) and size-predictor (Hwh) modules. (b) The second stage, PS-Net, then takes the predicted\nbox locations along with C as inputs and generates the final sketch image ¬ØIim. Following the de-\nsign of Eb and Ec, PS-Net also comprises GAT block-based encoders ( ¬ØEb, ¬ØEc). Further, PS-Net\ncontains a convolutional encoder-decoder network ( RE, RD) and a mask regressor to generate\nrasterized high quality sketch image ¬ØIim.\ncomprises graph-aware transformer (GAT) block-based encoders to capture structural\nrelationship between different regions within a sketch. To the best of our knowledge,\nwe are the first to introduce a GAT block-based transformer encoder for the problem\nof creative sketch image generation. Instead of directly predicting the box parameters\nas deterministic points from the transformer decoder, we further introduce probabilistic\ncoarse sketch decoders that utilize GMM modelling for box prediction. This enables\nour DoodleFormer to achieve diverse, yet plausible coarse structure (bounding boxes)\nfor sketch generation. The second stage, Part Sketcher (PS-Net), creates the final sketch\nimage with appropriate line segments based on the coarse structure obtained from PL-\nNet. PS-Net also comprises GAT block-based encoders, as in PL-Net, along with a\nconvolutional encoder-decoder network to generate the final rasterized sketch image.\nOur carefully designed two-stage DoodleFormer architecture possesses both desired\nproperties (holistic sketch part composition as well as fine-level diverse sketch gener-\nation) and creates diverse, yet realistic sketch images in a coarse-to-fine manner (see\nFig. 4). Next, we describe in detail PL-Net (Sec. 3.1) and PS-Net (Sec. 3.2).\n3.1 Part Locator Network (PL-Net)\nAs discussed above, PL-Net takes the initial stroke points C as the conditional input,\nand learns to return a coarse structure capturing the holistic part composition of the\ndesired sketch. The encoders in PL-Net contain graph-aware transformer (GAT) blocks\nto encode the structural relationship between different parts (holistic sketch part com-\nposition), leading to realistic sketch image generation. The decoder in PL-Net utilizes\nGMM modeling for box prediction, enabling the generations of diverse sketch images.\n6 A. K. Bhunia et al.\nFig. 4: A visual comparison in terms of progressively integrating one contribution at the time,\nfrom top to bottom, for common initial strokes. Compared to the single-stage baseline (first row),\nthe two-stage framework (without the GAT block and probabilistic modeling in the decoder)\ngenerates sketch in a coarse-to-fine manner. As a result, the two-stage framework (second row)\nproduces a more complete sketch where each body part is placed at an appropriate location rel-\native to other parts. The introduction of GAT block (third row) in the encoders of the two-stage\nframework improves the realism of the generated sketches by capturing the structural relation-\nship between different parts ( e.g., the tenth image from the left, where there is a discontinuity\nbetween the beak and the head of the bird). Further, the introduction of probabilistic modelling in\nthe decoder of the two-stage framework (last row), improves the diversity (e.g., appearance, size,\norientation and posture) of generated sketch images. Our final two-stage framework (last row)\nproduces realistic and diverse sketch images.\nGraph-aware Transformer Block-based Encoder PL-Net consists of two graph-\naware transformer (GAT) block-based encoders Eb and Ec, which are used to obtain\ncontextualized representation of the coarse (holistic) structure B and the conditional\ninput C, respectively.\nVKQ\n(a) (b)\n√óùêø\nFeed Forward\nAdd & Norm\nAdd & Norm\nMHSA block VKQ\n.\nùëÑùêæùëá\nùëõ(ùëô)\n(c)\nùëõ(ùëô+1)\nùëíùëñùëó\nùëñ ùëó ùëí ùõº\nùëÑùêæùëá\nAdjacency graph\nFig. 5: Our proposed graph-aware transformer\n(GAT) block (c) replaces the standard self-\nattention (b) in the conventional transformer en-\ncoder layer (a). Our GAT block injects the graph\nstructure into self-attention by learning to re-\nweight the attention matrix based on the pair-\nwise relations between the graph nodes. In this\nway, the proposed GAT block combines the lo-\ncal connectivity patterns from the learned adja-\ncency graph with the dynamic attention from the\nself-attention block.\nTo encode the identity t of each body\npart present in a sketch, we define vt ‚àà\nRd as a learned part embedding. We con-\ncatenate vt with a feature representation\nobtained from bt ‚àà B(box location and\nsize information ( xt, yt, wt, ht) of each\nbody part). This concatenated feature is\nthen used as an input to the encoder\nEb. The conditional input strokes C are\npassed through a linear layer before be-\ning input to the encoder Ec. We add spe-\ncial cls tokens [8] at the beginning of in-\nput sequences to the encoders ( Eb and\nEc). The output of this token is consid-\nered as the contextualized representation\nof the whole sequence. Further, we use\nfixed positional encodings to the input of\neach attention layer to retain information\nregarding the sequence order. Next, we\nintroduce our GAT block used in both en-\ncoders (Eb, Ec) to encode the holistic structural composition of (sketch) body parts.\nGraph-aware Transformer (GAT) Block: The structure of our GAT block is shown\nin Fig. 5(c). Each GAT block consists of a graph-aware multi-headed self-attention\nDoodleFormer: Creative Sketch Drawing with Transformers 7\n(MHSA) module followed by a feed-forward network [18]. Given the queries Q, keys\nK, and values V the standard self-attention module [32] computes the attention ac-\ncording to the following equation (also shown in Fig. 5(b)),\nŒ± = softmax\n\u0012QKT\n‚àö\nd\n\u0013\n. (1)\nWhile the standard self-attention module is effective towards learning highly con-\ntextualized feature representation, it does not explicitly emphasize on the local struc-\ntural relation. However, creative sketches are structured inputs with definite connec-\ntivity patterns between sketch parts. To model this structure, we propose to encode an\nadjacency based graph implemented with spectral graph convolution [19]. Our pro-\nposed GAT block combines the definite connectivity patterns from the learned adja-\ncency graph with the dynamic attention from self-attention block. Let us consider a\ngraph where each node i is associated with a structure-aware representationni and cor-\nresponding neighbour set Nr(i). To represent the neighbor set Nr(i) for each node i,\nwe define an adjacency matrix A where each entry represents whether two nodes i and\nj are adjacent. The edge weight eij between two adjacent nodes i and j is given by,\neij = WT\nb ReLU (Wa [ni, nj]) ‚àÄj ‚àà Nr(i), (2)\nwhere Wa and Wb are learned parameters and [¬∑, ¬∑] is a concatenation operator. We set\neij = 0 ‚àÄj /‚àà Nr(i). For each GAT blockl, the spectral graph convolution operation is,\nn(l+1)\ni = ReLU\nÔ£´\nÔ£≠n(l)\ni +\nX\nj‚ààNr(i)\neijWcn(l)\nj\nÔ£∂\nÔ£∏, (3)\nwhere Wc is a learned matrix. Our main intuition is that the adjacency matrix represent-\ning the neighbourhood graph structure is static which is computed over the connected\ncomponents in the graph and predetermined for each input, it is also symmetric and\ngenerally sparse. In contrast, attention learned from the self-attention layer is dynamic,\ncan be dense and also non-symmetric. We propose to combine these two complemen-\ntary representations through the following equation where we calculate the attention\nweight Œ±ij for nodes j ‚àà Nr(i) as follows,\nŒ±ij = eij exp(œÜij)P\nj‚ààNr(i) eij exp(œÜij), s.t. œÜij ‚àà QKT\n‚àö\nd\n, (4)\nwhere œÜij is an element of the standard attention matrix.\nThe special token ( cls) output from Ec is then utilized as an input to a Prior-Net\nfor approximating the conditional prior latent distribution. Similarly, the cls token out-\nputs of both Eb and Ec are provided as input to a Recog-Net for approximating the\nvariational latent distribution. Both the Prior-Net and the Recog-Net are parameterized\nby multi-layer perceptrons (MLPs) to approximate prior and variational latent normal\ndistributions. During training, we sample the latent variable z from the variational dis-\ntribution and provide it as input to the probabilistic coarse sketch decoder.\n8 A. K. Bhunia et al.\nProbabilistic Coarse Sketch Decoder The probabilistic coarse sketch decoder within\nour PL-Net utilizes probabilistic modelling to generate diverse coarse structure. The\ndecoder comprises two modules: a location-predictor Hxy and a size-predictor Hwh.\nHere, the location-predictor Hxy estimates the center coordinates ( xt, yt) of bounding\nboxes around body parts, while the size-predictor Hwh predicts their width and height\n(wt, ht). Both these modules consist of multi-headed self- and encoder-decoder atten-\ntion mechanisms [32]. The encoder-decoder attention obtains the key and value vectors\nfrom the output of the encoder Ec. This allows every position in the decoder to attend\nto all positions in the conditional input sequence. The part embedding vt from the en-\ncoder is used as a query positional encoding to each attention layer of the decoder. Over\nmultiple consecutive decoding layers, the decoder modules produce respective output\nfeatures fxy\nt ‚àà Rd and fwh\nt ‚àà Rd that lead to the distribution parameters of bounding\nboxes being associated with each body part, representing the coarse structure of the\nfinal sketch to be generated.\nTo enhance the diversity of generated sketch images, we model the box predic-\ntions from each decoder module by Gaussian Mixture Models (GMMs) [4,11]. Differ-\nent from the conventional box prediction [6,39] that directly maps the decoder output\nfeatures as deterministic box parameters, our GMM-based box prediction is modeled\nwith M normal distributions N (¬∑) where each distribution is parameterized by Œ∏k and\na mixture weight œÄk,\np(bt|C, z) =\nMX\nk=1\nœÄk,tN (bt; Œ∏k,t) , for\nMX\nk=1\nœÄk,t = 1. (5)\nThe GMM parameters can be obtained by minimizing the negative log-likehood for all\nP body parts in a sketch,\nLb = ‚àí 1\nP\nPX\nt=1\nlog\n MX\nk=1\nœÄk,tN(bt; Œ∏k,t)\n!\n. (6)\nHere, we simplify the quadvariate distribution of GMMs in Eq. 6 by decomposing it\ninto two bivariate distributions as p(bt|C, z) = p(xt, yt|C, z)p(wt, ht|xt, yt, C, z). The\nparameters of these bivariate GMMs are obtained by employing linear layers and appro-\npriate normalization on the outputsfxy\nt , fwh\nt of Hxy and Hwh, respectively. In addition\nto GMM parameters, these linear layers also estimate the presence of a body part using\nan indicator variable, which is trained with a binary cross entropy loss Lc.\nPL-Net Loss function (LPL ): The overall loss function LPL to train the PL-Net is the\nweighted sum of the reconstruction loss Lrec, and the KL divergence loss LKL ,\nLPL = Lrec + ŒªKLLKL. (7)\nHere, the reconstruction loss term is Lrec = Lb + Lc. The KL divergence loss term\nLKL regularizes the variational distribution [28] from the Recog-Net to be closer to the\nprior distribution from the conditional Prior-Net, whereas ŒªKL is a scalar loss weight.\nOur carefully designed PL-Net architecture, presented above, provides a coarse\nstructure of the sketch that is used to generate a diverse, yet realistic final sketch im-\nage in the second stage (PS-Net) of the proposed two-stage DoodleFormer framework.\nDoodleFormer: Creative Sketch Drawing with Transformers 9\nNext, we present the PS-Net that takes the coarse structure of the sketch along with\ninitial partial sketch C as inputs and generates the final sketch image.\n3.2 Part Sketcher Network (PS-Net)\nOur PS-Net comprises two graph-aware transformer (GAT) block-based encoders ¬ØEb\nand ¬ØEc, following the design of encoders Eb and Ec in the PL-Net. Here, the encoder\n¬ØEb produces a contextualized feature representation of bounding boxbt associated with\neach body part. Similarly, the encoder ¬ØEc outputs a contextualized feature representa-\ntion of initial stroke points C. Both these contextualized feature representations from\n¬ØEb and ¬ØEc are then concatenated and passed through a linear layer to obtain ut.\nThe initial stroke points C is converted to its raster form IC and passed through\na convolutional encoder RE that outputs a spatial representation g = RE(IC). Con-\nsequently, g and {ut}P\nt=1 are provided as input to a convolutional decoder RD for\ngenerating the final sketch image ¬ØIim,\n¬ØIim = RD\n\u0000\ng, {ut}P\nt=1\n\u0001\n. (8)\nThe decoder network RD utilizes the ResNet [13] architecture as a backbone. To intro-\nduce diversity in the generated images, a zero-mean unit-variance multivariate random\nnoise is added with g before passing it to the decoder network. For fine-grained shape\nprediction, we utilize a mask regressor [30,31] having up-sampling convolutions, fol-\nlowed by sigmoid transformation to generate an auxiliary mask for each bounding box.\nThe predicted masks are resized to the sizes of corresponding bounding boxes, which\nare then used to compute the instance-specific and structure-aware affine transformation\nparameters in the normalization layer of the decoder RD.\nThe training of PS-Net follows the standard GAN formulation where the PS-Net\ngenerator G is followed by additional discriminator networks Dim, Dpart, and Dapp to\nobtain image-level (Lim), part-level (Lpart), and appearance (Lapp) adversarial losses\n[14,30], respectively. The loss function is then given by,\nLPS =Lim + ŒªpLpart + ŒªaLapp, (9)\nwhere Œªp and Œªa are the loss weight hyper-parameters.\nThe introduction of the GAT block in the PL-Net and PS-Net encoders contributes\ntowards the generation of realistic sketch images, whereas the effective utilization of\nprobabilistic modelling in the PL-Net decoder leads to improved diversity. In summary,\nour two-stage DoodleFormer generates diverse, yet realistic sketch images (see Fig. 4).\n4 Experiments\nDatasets: We perform extensive experiments on the recently introduced Creative Birds\nand Creative Creatures datasets [10]. The Creative Birds has 8067 sketches of birds,\nwhereas the Creative Creatures contains 9097 sketches of various creatures. In both\ndatasets, all sketches come with part annotations. Both datasets also contain free-form\nnatural language phrase as a text description for each sketch.\n10 A. K. Bhunia et al.\nTable 1: Comparison of DoodleFormer with DoodlerGAN [10], StyleGAN2 [17] and\nSketchRNN [12] in terms of Fr `echet inception distance (FID), generation diversity (GD), char-\nacteristic score (CS) and semantic diversity score (SDS). Our DoodleFormer performs favorably\nagainst existing methods on both datasets.\nCreative Birds Creative CreaturesMethods FID(‚Üì) GD(‚Üë) CS(‚Üë) FID(‚Üì) GD(‚Üë) CS(‚Üë) SDS(‚Üë)\nTraining Data - 19.40 0.45 - 18.06 0.60 1.91\nSketchRNN [12] 82.17 17.29 0.18 54.12 16.11 0.48 1.34\nStyleGAN2 [17] 130.93 14.45 0.12 56.81 13.96 0.37 1.17\nDoodlerGAN [10] 39.95 16.33 0.69 43.94 14.57 0.55 1.45\nDoodleFormer (Ours)16.45 18.33 0.55 18.71 16.89 0.56 1.78\nImplementation Details: As discussed, both PL-Net and PS-Net utilize graph-aware\ntransformer (GAT) block-based encoders. In each GAT-block based encoder, we define\nan adjacency matrix A based on the connectivity patterns of the adjacency graph. Every\npair of overlapping bounding boxes on a coarse structure is connected in the adjacency\ngraph. Similarly, for initial strokes, the corresponding adjacency graph connects adja-\ncent points on each single stroke. Each of these encoders consist of L=6 graph-aware\ntransformer blocks. Here, each block comprises multi-headed attention having 8 heads.\nIn the probabilistic coarse sketch decoder, the location-predictor and size-predictor uti-\nlize 3 self- and encoder-decoder attention layers. Further, we set the embedding size\nd=512. We augment the vector sketch images by applying small affine transforma-\ntions and these vector sketches are converted to raster images of size 128 √ó 128. Our\nDoodleFormer is trained as follows. In the first stage, for training PL-Net, we initially\nobtain the bounding boxes for all body parts in a sketch using the part annotations. The\nbounding boxes are normalized to values between 0 and 1. In the second stage, we train\nPS-Net using the raster sketch images and their corresponding ground-truth boxes. In\nboth stages, the initial stroke points are provided in a vector form as a conditional input.\nIn all experiments, we use a batch size of 32. The learning rate is set to 1e‚àí4 and the\nloss weights ŒªKL, Œªp, Œªa are set to 1, 10 and 10.\n4.1 Quantitative and Qualitative Comparisons\nWe first present a comparison (Tab. 1) of our DoodleFormer with state-of-the-art ap-\nproaches [12,33,10] on both Creative Birds and Creative Creatures. For a fair compari-\nson, we evaluate all methods using two widely used metrics, namely Fr `echet inception\ndistance (FID) [15] and generation diversity (GD) [5], as in DoodlerGAN [10]. Here,\nthe FID and GD scores are computed using an Inception model trained on the Quick-\nDraw3.8M dataset [34], that embeds the images onto a feature space [10]. Tab. 1 shows\nthat DoodleFormer outperforms existing methods in terms of both FID and GD scores,\non the two datasets. The higher GD score indicates the ability of DoodleFormer to gen-\nerate diverse sketch images, whereas the lower FID score indicates the superior quality\nof its generated creative sketches.\nFurthermore, similar to DoodlerGAN [10], we use two additional metrics: charac-\nteristic score (CS) and semantic diversity score (SDS). The CS metric evaluates how\nDoodleFormer: Creative Sketch Drawing with Transformers 11\nFig. 6: Applications of DoodleFormer. (a) Creative sketch generation based on random input\nstrokes. (b) Creative sketch completion: Here, DoodleFormer accurately completes missing parts\n(e.g., beak, head and body of bird is well connected in second row and column in (b)), com-\npared to DoodlerGAN (DG). (c) Text to creative sketch generation: We compare DoodleFormer\nwith AttnGAN [35] (AG), StackGAN [36] (SG). DoodleFormer produces sketches that are well\naligned with user provided input texts. Best viewed zoomed in.\noften a generated sketch is classified to be a bird (for Creative Birds) or creature (for\nCreative Creatures) by the Inception model trained on the QuickDraw3.8M dataset. The\nSDS metric measures the diversity of the sketches in terms of the different creature cat-\negories they represent. While the CS score can give us a basic understanding of the\ngeneration quality, it does not necessarily reflects the creative abilities of a model. For\ninstance, if a model generates only canonical and mundane sketches of birds, then the\ngenerated sketches would more likely to be correctly classified by the trained Inception\nmodel. In that case, the CS score will still be high. In contrast, the SDS score is more\nreliable in measuring the diversity of the generated sketch images. Tab. 1 shows that\nDoodleFormer performs favourably against existing methods, in terms of SDS score,\non both datasets. Fig. 6(a) shows a visual comparison of DoodleFormer with Doodler-\nGAN for creative sketch generation1.\n4.2 User Study\nHere, we present our user study to evaluate the human plausibility of creative sketches\ngenerated by our DoodleFormer. Specifically, we show 100 participants pairs of sketches\n‚Äì one generated by DoodleFormer and the other by a competing approach. For each\npair of images, similar to DoodlerGAN [10], each participant is provided with 5 ques-\ntions which are shown in the legend of Fig. 7 (a-e). DoodleFormer performs favor-\nably against DoodlerGAN for all five questions on both datasets. For instance, Doodle-\nFormer sketches were interpreted to be drawn by a human 86%, having better initial\nstrokes integration 85% and being more creative 82%, over DoodlerGAN on Creative\n12 A. K. Bhunia et al.\nBirds dataset. Further, for all the five questions, the DoodleFormer generated sketch im-\nages were found to be comparable with the human drawn sketches in Creative Datasets1.\n4.3 Ablation Study\n(a)\nLooks  \nLike Bird/ \nCreatures \n(b) (c) (d) (e) (a) (b) (c) (d) (e)\n(a) more creative (b) looks like birds/creatures (c) likely drawn \nby human (d) initial strokes integrated (e)  like the most overall\nDoodlerGAN Creative Dataset\n80\n60\n40\n20DoodleFormer wins (%)\nCreative Birds Creative Creatures\nFig. 7: User study results on Creative Birds (left)\nand Creative Creatures (right) based on the five\nquestions (a-e) mentioned in the legend. Higher\nvalues indicate DoodleFormer is preferred more\noften over the compared approaches (Doodler-\nGAN and human drawn Creative datasets).\nWe perform multiple ablation studies to\nvalidate the impact of proposed contri-\nbutions in our framework. Tab. 2 shows\nthe impact of two-stage framework, GAT\nblocks and GMM-based modeling on\nCreative Birds. Our single-stage base-\nline (referred as baseline ‚àó) is a stan-\ndard transformer-based encoder-decoder\narchitecture, where initial strokes are\ngiven as input to the transformer encoder.\nThe decoder sequentially generates all\nbody parts which are then integrated to\nobtain final output sketch. The gener-\nated sketches using baseline‚àó are unreal-\nistic and suffer from body parts misplace-\nment. The introduction of two-stage framework leads to an absolute gain of 26.3 in\nterms of FID score, highlighting the importance of a coarse-to-fine framework for real-\nistic creative sketch generation. Our two-stage framework baseline neither uses the GAT\nblock in encoders nor employs the GMM-based modeling in decoder. Instead of GMM-\nbased modeling, we use a deterministic L1 loss in first stage (PL-Net) of two-stage\nbaseline. While this two-stage baseline improves realism of generated sketches, it stills\nsuffers from topological artifacts. The introduction of GAT block in encoders of two-\nstage baseline improves realism of generated sketches by capturing the structural rela-\ntionship between different parts. Although GAT blocks improve FID score by a margin\nof 3.25, the generated sketches still lack diversity as indicated by only a marginal change\nin GD score. The introduction of GMM-based modelling in decoder improves diversity\n(e.g., appearance, size, orientation and posture) of generated sketches that leads to an\nabsolute gain of 1.17 in GD (see also Fig. 4). Our final DoodleFormer (two-stage base-\nline + GAT blocks + GMM) achieves absolute gains of 30.0 and 3.7 in terms of FID\nscore over baseline‚àó and two-stage baseline, respectively.\nWe also evaluate the design choices of our GAT blocks (see Tab. 3). First, we re-\nplace GAT blocks in encoders with simple GCN layers. The standard transformer-based\nencoders outperforms this GCN-based baseline by a margin of 7.31. Further, we adapt\nthe Mesh Graphormer [22] by using their Graphormer in encoders of our framework.\nMesh Graphormer stacks transformer encoder layer and GCN block together in series.\nIn our experiments, we observe this design based on loosely connected components\nperforms slightly worse than standard transformer-based baseline. In contrast, an inte-\ngrated design like ours performs comparatively better.\n1 Additional details and results are provided in supplementary material.\nDoodleFormer: Creative Sketch Drawing with Transformers 13\nTable 2: Impact of our two-stage frame-\nwork, GAT blocks and GMM-based prob-\nabilistic modelling on Creative Birds.\nDesign ChoicesMethods FID(‚Üì) GD(‚Üë)\nSingle-stage baseline‚àó 46.45 16.87\nTwo-stage\nbaseline 20.14 17.05\nbaseline + GAT 16.89 17.16\nbaseline + GAT + GMM16.45 18.33\nTable 3: Comparison of alternative design\nchoices for the proposed GAT blocks on\nCreative Birds.\nMethods FID(‚Üì) GD(‚Üë)\nGCN layers 27.45 17.23\nTransformer layers 20.14 17.05\nMesh Graphormer [22]20.34 16.78\nGAT layers (ours) 16.45 18.33\nTable 4: House Layout Generation: We compare our approach with the existing methods in\nterms of FID and Compatibility scores (obtained by the graph edit distance). The dataset samples\nare split into five groups based on the room counts (1-3, 4-6, 7-9, 10-12, and 13+).\nFID (‚Üì) Compatibility (‚Üì)Methods 1-3 4-6 7-9 10-12 13+ 1-3 4-6 7-9 10-12 13+\nAshualet al. [12] 64.0 92.2 87.6 122.8 149.9 0.2 2.7 6.2 19.2 36.0\nJohnsonet al. [17] 69.8 86.9 80.1 117.5 123.2 0.2 2.6 5.2 17.5 29.3\nHouse-GAN [10]13.6 9.4 14.4 11.6 20.1 0.1 1.1 2.9 3.9 10.8\nOurs 9.6 10.1 11.2 9.7 18.2 0.1 1.0 2.1 2.4 8.3\n4.4 Related Applications\nWe also analyze DoodleFormer on three related tasks: user provided text to creative\nsketch generation, creative sketch completion and house layout generation.\nText to Creative Sketch Generation: Here, the text description is given as conditional\ninput to encoder Ec in PL-Net, yielding a coarse structure of desired sketch which is\nfed to PS-Net to generate final sketch. We remove RE from the PS-Net, and the cls\ntoken output from the encoder ¬ØEc is directly passed as input to RD. We use 80%-20%\ntrain-test split. We compare DoodleFormer with two popular text-to-image methods:\nStackGAN [36], AttnGAN [35] on Creative Birds and Creative Creatures. Doodle-\nFormer performs favorably against these methods in terms of FID and GD scores on\nboth datasets. On Creative Birds, StackGAN, AttnGAN and DoodleFormer achieve re-\nspective FID scores of [53.1, 45.2, 18.5], and GD scores of [16.7, 16.5, 17.3]. Fig. 6 (c)\nshows a qualitative comparison1.\nCreative Sketch Completion: Given an incomplete sketch as input, DoodleFormer\nattempts to creatively complete the rest of the sketch. First, PL-Net obtains the bound-\ning boxes for missing parts. Then, PS-Net generates an image containing the required\nmissing parts which is then integrated with the incomplete sketch input to obtain the\nfinal output. On both Creative Birds and Creative Creatures, DoodleFormer achieves\nfavorable results compared to DoodlerGAN in terms of FID and GD scores. On Cre-\native Birds, DoodlerGAN and DoodleFormer achieve respective FID scores of [44.2 and\n18.3] and GD scores of [15.1 and 17.8]. Fig. 6 (b) shows the qualitative comparison1.\nHouse Layout Generation: Finally, we use our proposed PL-Net architecture for the\nhouse-plan generation [24] task. The goal is to take a bubble diagram as an input, and\ngenerate a diverse set of realistic and compatible house layouts. A bubble diagram is\nrepresented by a graph where each node contains information about rooms and edges\n14 A. K. Bhunia et al.\nFig. 8: Qualitative results of House Layout Generation. Given the input bubble diagram, We com-\npare the house layout sample generated using our method with the House-GAN [24]. Our method\nproduces house layouts that are well aligned with input bubble diagram texts.\nindicate their spatial adjacency. The output house layout is represented as axis-aligned\nbounding boxes. The Encoder Ec takes the room type information as input and Prob-\nabilistic Decoder subsequently outputs boundary boxes for each room. To transform\nthe obtained boundary box layout to a floor plan layout, we employ a floor-plan post-\nprocessing strategy. In this process, we first extract boundary lines of the generated\nboundary boxes. Next, we merge the adjacent line segments together and further align\nthem to obtain a closed polygon.\nWe perform the house-plan generation experiments on LIFULL HOME‚Äôs dataset\n[1]. For fair comparison, we follow the same setting used by House-GAN [24]. We di-\nvide the samples into five groups based on the number of rooms: 1-3, 4-6, 7-9, 10-12,\nand 13+. To test the generalization ability in each group, we train a model while exclud-\ning samples in the same group. At test time, we randomly pick a bubble input diagram\nfrom each group and generate 10 samples. Similar to House-GAN [24], we quantita-\ntively measure the performance of our method in terms of FID and compatibility scores.\nThe compatibility score is the graph editing distance [2] between the input bubble di-\nagram and the bubble diagram constructed from the output layout. Tab. 4 shows that\nour method outperforms existing house-plan generation methods both in terms of FID\nand compatibility scores. Fig. 8 shows the qualitative comparison of our house layout\ngeneration approach with House-GAN [24].\n5 Conclusion\nWe proposed a novel coarse-to-fine two-stage approach, DoodleFormer, for creative\nsketch generation. We introduce graph-aware transformer encoders that effectively cap-\nture global dynamic as well as local static structural relations among different body\nparts. To ensure diversity of generated creative sketches, we introduce a probabilis-\ntic coarse sketch decoder that explicitly models variations of each sketch body part to\nbe drawn. We show the effectiveness of DoodleFormer on two datasets by perform-\ning extensive qualitative, quantitative and human-based evaluations. In addition, we\ndemonstrate promising results on related applications such as text to creative sketch\ngeneration, sketch completion and house layout generation.\nDoodleFormer: Creative Sketch Drawing with\nTransformers\nSupplementary Material\nAnkan Kumar Bhunia1, Salman Khan1,2, Hisham Cholakkal1, Rao Muhammad\nAnwer1,3, Fahad Shahbaz Khan1,4, Jorma Laaksonen3, Michael Felsberg4\n1 Mohamed bin Zayed University of AI, UAE 2 Australian National University, Australia\n3 Aalto University, Finland 4 Link¬®oping University, Sweden\nankan.bhunia@mbzuai.ac.ae\neye beak body head legs mouth tail wings\nFig. 1: Visualization of thecoarse-to-fine creative sketch drawing process. The model first draws\nthe holistic coarse structure of the sketch and then fills the fine-details to generate the final sketch.\nBoth the coarse structure and the final sketch output are shown side-by-side in the figure. By first\ndrawing the holistic coarse structure of the sketch aids to appropriately decide the location and\nthe size of each sketch body part to be drawn.\nIn this supplementary material, we present additional qualitative results and addi-\ntional user study details. In Sec. 1, we present the visualizations depicting the coarse-to-\nfine sketch generation process . Sec. 2 presents the additional details for the calculation\nof the evaluation metrics. Sec. 3 shows additional quantitative results. Sec. 4 provides\nadditional details of user study experiments.\n1 Coarse-to-fine Sketch Generation\nFig. 1 presents example visualizations depicting the intermediate results of our coarse-\nto-fine creative sketch generation. The proposed two-stage DoodleFormer framework\ndecomposes the creative sketch generation problem to first capture the holistic coarse\nstructure of the sketch and then injecting fine-details to generate the final sketch. Both\nthe coarse structure from the first-stage PL-Net, and the final sketch output from the\nsecond-stage PS-Net, are shown side-by-side in Fig. 1. By first drawing the holistic\ncoarse structure of the sketch aids to appropriately determine the location and the size\nof each sketch body part to be drawn.\n2 A. K. Bhunia et al.\nFig. 2: Additional qualitative comparisons on Creative Birds and Creative Creatures datasets. In\nthis figure, we compare the generated sketches using the proposed DoodleFormer (in the middle\ncolumn) with DoodlerGAN [10] (in the right column). The human drawn creative sketch images\nfrom the datasets are shown in the left column. DoodlerGAN suffers from topological artifacts.\nAlso, DoodlerGAN generated sketches have lesser diversity in terms of size, appearance and pos-\nture. The proposed DoodleFormer alleviates the issues of topological artifacts and disconnected\nbody parts, generating creative sketches that are more realistic and diverse.\n2 Additional Details of Evaluation Metrics\nWe quantitatively evaluate our proposed approach based on four metrics: Fr`echet incep-\ntion distance (FID), generation diversity (GD), characteristic score (CS), and semantic\nDoodleFormer: Creative Sketch Drawing with Transformers Supplementary Material 3\nFig. 3: Additional qualitative results for text to creative sketch generation of DoodleFormer on\nCreative Birds and Creative Creatures dataset. For each input text, we show three different sam-\nples generated using DoodleFormer. The results demonstrate that our approach is capable of\ngenerating diverse sketch images while ensuring that the generated sketches are well matched\nwith the user provided text inputs.\nFig. 4: Additional qualitative results for text to creative sketch completion of DoodleFormer on\nCreative Birds and Creative Creatures dataset. The results depict that DoodleFormer accurately\ncompletes the missing regions of the given partial input sketch.\ndiversity score (SDS). Following [10], we use an inception model trained on the Quick-\nDraw3.8M dataset [34] to calculate these metrics. We generate 10, 000 sketches based\non a randomly sampled set of previously unseen initial strokes. Then, the generated\nsketches are resized into64√ó64 images and passed through the trained inception model\nto calculate the above mentioned metrics.\n3 Additional Qualitative Results\nFig. 2 shows additional qualitative results of our proposed DoodleFormer for creative\nsketch generation on both datasets (Creative Birds and Creative Creatures). For better\nvisual comparison, we also show the creative sketch images from the datasets and the\nDoodlerGAN [10] generated sketch images in the same figure.\nIn Fig. 3, we present additional qualitative results for text to creative sketch gener-\nation on both datasets (Creative Birds and Creative Creatures). For each user provided\ninput text, we show three samples generated using our proposed DoodleFormer. The\nresults demonstrate the effectiveness of our approach towards generating diverse sketch\nimages while ensuring that the generated sketches are well matched with the user pro-\nvided text inputs. Fig. 4 shows qualitative results for creative sketch completion. The\nresults show that DoodleFormer accurately completes the missing regions of these chal-\nlenging incomplete sketches. Fig. 5 shows that GMM-based modelling can generate\nmultiple bound- ing box layouts with more diverse sketch results (w.r.t appearance, size,\n4 A. K. Bhunia et al.\nFig. 5: Qualitative analysis of GMM-based modelling for common initial strokes. The introduc-\ntion of the GMM-based modelling substantially improves the diversity of generated sketches.\nFig. 6: A visual comparison of creative sketches generated by DoodleFormer and\nSketchRNN [12] on Creative Birds dataset.\norientation and posture). Fig. 6 presents a qualitative comparison of DoodleFormer with\nSketchRNN [12].\n4 User Study Additional Details\nHere, we present additional details of our user studies on 100 human participants to\nevaluate the creative abilities of our proposed DoodleFormer. We compare the Doo-\ndleFormer generated sketches with DoodlerGAN and human-drawn Creative dataset\nsketches. Specifically, we show participants pairs of sketches ‚Äì one generated by Doo-\ndleFormer and the other by a competing approach. Each participant has to answer 5\nquestions for each of these pairs. The questions are: which one (a) is more creative? (b)\nlooks more like a bird/creature? (c) is more likely to be drawn by a human? (d) in which\ncase, the initial strokes are well integrated? (e) like the most overall. Our proposed Doo-\ndleFormer performs favorably against DoodlerGAN [10] for all five questions on both\ndatasets. Further, the DoodleFormer generated sketch images were observed to be com-\nparable with the sketches drawn by human in Creative Datasets for all the five questions.\nAcknowledgements. This work has been funded by the Academy of Finland in project\nUSSEE (345791) and supported by the Aalto Science-IT project.\nReferences\n1. Lifull home‚Äôs dataset. https://www.nii.ac.jp/dsc/idr/lifull, accessed:\n2010-09-30 14\n2. Abu-Aisheh, Z., Raveaux, R., Ramel, J.Y ., Martineau, P.: An exact graph edit distance algo-\nrithm for solving pattern recognition problems. In: 4th International Conference on Pattern\nRecognition Applications and Methods 2015 (2015) 14\n3. Balasubramanian, S., Balasubramanian, V .N., et al.: Teaching gans to sketch in vector format.\narXiv preprint arXiv:1904.03620 (2019) 3\n4. Bishop, C.M.: Mixture density networks. Aston University (1994) 8\nDoodleFormer: Creative Sketch Drawing with Transformers Supplementary Material 5\n5. Cao, N., Yan, X., Shi, Y ., Chen, C.: Ai-sketcher: a deep generative model for producing\nhigh-quality sketches. In: AAAI (2019) 3, 10\n6. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end\nobject detection with transformers. In: ECCV (2020) 8\n7. Chen, Y ., Tu, S., Yi, Y ., Xu, L.: Sketch-pix2seq: a model to generate sketches of multiple\ncategories. arXiv preprint arXiv:1709.04121 (2017) 3\n8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In: NAACL (2019) 6\n9. Ganin, Y ., Kulkarni, T., Babuschkin, I., Eslami, S.A., Vinyals, O.: Synthesizing programs for\nimages using reinforced adversarial learning. In: ICML (2018) 3\n10. Ge, S., Goswami, V ., Zitnick, C.L., Parikh, D.: Creative sketch generation. In: ICLR (2021)\n2, 4, 9, 10, 11, 13, 3\n11. Graves, A.: Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 (2013) 8\n12. Ha, D., Eck, D.: A neural representation of sketch drawings. In: ICLR (2018) 3, 10, 13, 4\n13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR\n(2016) 9\n14. He, S., Liao, W., Yang, M.Y ., Yang, Y ., Song, Y .Z., Rosenhahn, B., Xiang, T.: Context-aware\nlayout to image generation with enhanced object appearance. In: CVPR (2021) 9\n15. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium. In: NeurIPS (2017) 10\n16. Hinton, G.E., Nair, V .: Inferring motor programs from images of handwritten digits. In:\nNeurIPS (2006) 3\n17. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving\nthe image quality of StyleGAN. In: CVPR (2020) 4, 10, 13\n18. Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.: Transformers in vision:\nA survey. arXiv preprint arXiv:2101.01169 (2021) 7\n19. Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks.\nIn: ICLR (2017) 7\n20. Li, Y ., Song, Y .Z., Hospedales, T.M., Gong, S.: Free-hand sketch synthesis with deformable\nstroke models. In: IJCV (2017) 3\n21. Lin, H., Fu, Y ., Xue, X., Jiang, Y .G.: Sketch-bert: Learning sketch bidirectional encoder\nrepresentation from transformers by self-supervised learning of sketch gestalt. In: CVPR\n(2020) 4\n22. Lin, K., Wang, L., Liu, Z.: Mesh graphormer. In: ICCV (2021) 12, 13\n23. Liu, F., Deng, X., Lai, Y .K., Liu, Y .J., Ma, C., Wang, H.: Sketchgan: Joint sketch completion\nand recognition with gan. In: CVPR (2019) 4\n24. Nauata, N., Chang, K.H., Cheng, C.Y ., Mori, G., Furukawa, Y .: House-gan: Relational gen-\nerative adversarial networks for graph-constrained house layout generation. In: European\nConference on Computer Vision. pp. 162‚Äì177. Springer (2020) 13, 14\n25. Qi, Y ., Su, G., Chowdhury, P.N., Li, M., Song, Y .Z.: Sketchlattice: Latticed representation\nfor sketch manipulation. In: ICCV (2021) 4\n26. Ramasinghe, S., Farazi, M., Khan, S., Barnes, N., Gould, S.: Rethinking conditional gan\ntraining: An approach using geometrically structured latent manifolds. In: NeurIPS (2021) 4\n27. Ribeiro, L.S.F., Bui, T., Collomosse, J., Ponti, M.: Sketchformer: Transformer-based repre-\nsentation for sketched structure. In: CVPR (2020) 3, 4\n28. Sohn, K., Lee, H., Yan, X.: Learning structured output representation using deep conditional\ngenerative models. In: NeurIPS (2015) 8\n29. Su, G., Qi, Y ., Pang, K., Yang, J., Song, Y .Z.: Sketchhealer: A graph-to-sequence network\nfor recreating partial human sketches. In: BMVC (2020) 4\n6 A. K. Bhunia et al.\n30. Sun, W., Wu, T.: Image synthesis from reconfigurable layout and style. In: ICCV (2019) 9\n31. Sun, W., Wu, T.: Learning layout and style reconfigurable gans for controllable image syn-\nthesis. PAMI (2021) 9\n32. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, ≈Å.,\nPolosukhin, I.: Attention is all you need. In: NeurIPS (2017) 7, 8\n33. Viazovetskyi, Y ., Ivashkin, V ., Kashin, E.: Stylegan2 distillation for feed-forward image ma-\nnipulation. In: ECCV (2020) 10\n34. Xu, P., Hospedales, T.M., Yin, Q., Song, Y .Z., Xiang, T., Wang, L.: Deep learning for free-\nhand sketch: A survey and a toolbox. arXiv preprint arXiv:2001.02600 (2020) 10, 3\n35. Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X.: Attngan: Fine-grained\ntext to image generation with attentional generative adversarial networks. In: CVPR (2018)\n11, 13\n36. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., Metaxas, D.N.: Stackgan: Text\nto photo-realistic image synthesis with stacked generative adversarial networks. In: ICCV\n(2017) 11, 13\n37. Zheng, N., Jiang, Y ., Huang, D.: Strokenet: A neural painting environment. In: ICLR (2018)\n3\n38. Zhou, T., Fang, C., Wang, Z., Yang, J., Kim, B., Chen, Z., Brandt, J., Terzopoulos, D.: Learn-\ning to doodle with stroke demonstrations and deep q-networks. In: BMVC (2018) 3\n39. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers\nfor end-to-end object detection. In: ICLR (2021) 8",
  "topic": "Sketch",
  "concepts": [
    {
      "name": "Sketch",
      "score": 0.9699145555496216
    },
    {
      "name": "Computer science",
      "score": 0.717049241065979
    },
    {
      "name": "Creatures",
      "score": 0.681297242641449
    },
    {
      "name": "Sketch recognition",
      "score": 0.6224846243858337
    },
    {
      "name": "Probabilistic logic",
      "score": 0.5306837558746338
    },
    {
      "name": "Artificial intelligence",
      "score": 0.483563631772995
    },
    {
      "name": "Encoder",
      "score": 0.41849127411842346
    },
    {
      "name": "Algorithm",
      "score": 0.14734983444213867
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.0
    },
    {
      "name": "Gesture recognition",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Gesture",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 5
}