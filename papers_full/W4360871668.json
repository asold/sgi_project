{
  "title": "EEGformer: A transformer–based brain activity classification method using EEG signal",
  "url": "https://openalex.org/W4360871668",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2629271073",
      "name": "Zhijiang Wan",
      "affiliations": [
        "Nanchang University",
        "First Affiliated Hospital of Nanchang University"
      ]
    },
    {
      "id": "https://openalex.org/A2127233574",
      "name": "Manyu Li",
      "affiliations": [
        "Nanchang University"
      ]
    },
    {
      "id": "https://openalex.org/A2110362176",
      "name": "Shichang Liu",
      "affiliations": [
        "Shaanxi Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2099663556",
      "name": "Jiajin Huang",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101520373",
      "name": "Hai Tan",
      "affiliations": [
        "Nanjing Audit University"
      ]
    },
    {
      "id": "https://openalex.org/A2109621909",
      "name": "Wenfeng Duan",
      "affiliations": [
        "First Affiliated Hospital of Nanchang University",
        "Nanchang University"
      ]
    },
    {
      "id": "https://openalex.org/A2629271073",
      "name": "Zhijiang Wan",
      "affiliations": [
        "Nanchang Institute of Technology",
        "Nanchang Institute of Science & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2127233574",
      "name": "Manyu Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110362176",
      "name": "Shichang Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099663556",
      "name": "Jiajin Huang",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101520373",
      "name": "Hai Tan",
      "affiliations": [
        "Nanjing Audit University"
      ]
    },
    {
      "id": "https://openalex.org/A2109621909",
      "name": "Wenfeng Duan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2337159199",
    "https://openalex.org/W6846271455",
    "https://openalex.org/W4292731283",
    "https://openalex.org/W1970727126",
    "https://openalex.org/W3138314257",
    "https://openalex.org/W3198315730",
    "https://openalex.org/W2953654852",
    "https://openalex.org/W3111342233",
    "https://openalex.org/W2559463885",
    "https://openalex.org/W4304087145",
    "https://openalex.org/W3098199041",
    "https://openalex.org/W4298000093",
    "https://openalex.org/W3037233591",
    "https://openalex.org/W3029573260",
    "https://openalex.org/W3153768134",
    "https://openalex.org/W3185598997",
    "https://openalex.org/W4307702870",
    "https://openalex.org/W3086601978",
    "https://openalex.org/W3134550510",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3004599123",
    "https://openalex.org/W2895904856",
    "https://openalex.org/W2792885690",
    "https://openalex.org/W2896297654",
    "https://openalex.org/W3102003537",
    "https://openalex.org/W3080766837",
    "https://openalex.org/W2966567899",
    "https://openalex.org/W1947251450",
    "https://openalex.org/W3006527936",
    "https://openalex.org/W3037286204",
    "https://openalex.org/W4220675915",
    "https://openalex.org/W4226072312",
    "https://openalex.org/W3106355052",
    "https://openalex.org/W4368341067",
    "https://openalex.org/W3102455230"
  ],
  "abstract": "Background The effective analysis methods for steady-state visual evoked potential (SSVEP) signals are critical in supporting an early diagnosis of glaucoma. Most efforts focused on adopting existing techniques to the SSVEPs-based brain–computer interface (BCI) task rather than proposing new ones specifically suited to the domain. Method Given that electroencephalogram (EEG) signals possess temporal, regional, and synchronous characteristics of brain activity, we proposed a transformer–based EEG analysis model known as EEGformer to capture the EEG characteristics in a unified manner. We adopted a one-dimensional convolution neural network (1DCNN) to automatically extract EEG-channel-wise features. The output was fed into the EEGformer, which is sequentially constructed using three components: regional, synchronous, and temporal transformers. In addition to using a large benchmark database (BETA) toward SSVEP-BCI application to validate model performance, we compared the EEGformer to current state-of-the-art deep learning models using two EEG datasets, which are obtained from our previous study: SJTU emotion EEG dataset (SEED) and a depressive EEG database (DepEEG). Results The experimental results show that the EEGformer achieves the best classification performance across the three EEG datasets, indicating that the rationality of our model architecture and learning EEG characteristics in a unified manner can improve model classification performance. Conclusion EEGformer generalizes well to different EEG datasets, demonstrating our approach can be potentially suitable for providing accurate brain activity classification and being used in different application scenarios, such as SSVEP-based early glaucoma diagnosis, emotion recognition and depression discrimination.",
  "full_text": "fnins-17-1148855 March 20, 2023 Time: 14:36 # 1\nTYPE Original Research\nPUBLISHED 24 March 2023\nDOI 10.3389/fnins.2023.1148855\nOPEN ACCESS\nEDITED BY\nXin Huang,\nRenmin Hospital of Wuhan University, China\nREVIEWED BY\nHongzhi Kuai,\nMaebashi Institute of Technology, Japan\nYaofei Xie,\nXuzhou Medical University, China\n*CORRESPONDENCE\nWenfeng Duan\nndyfy02345@ncu.edu.cn\nSPECIALTY SECTION\nThis article was submitted to\nVisual Neuroscience,\na section of the journal\nFrontiers in Neuroscience\nRECEIVED 20 January 2023\nACCEPTED 06 March 2023\nPUBLISHED 24 March 2023\nCITATION\nWan Z, Li M, Liu S, Huang J, Tan H and Duan W\n(2023) EEGformer: A transformer–based\nbrain activity classiﬁcation method using EEG\nsignal.\nFront. Neurosci.17:1148855.\ndoi: 10.3389/fnins.2023.1148855\nCOPYRIGHT\n© 2023 Wan, Li, Liu, Huang, Tan and Duan. This\nis an open-access article distributed under the\nterms of the Creative Commons Attribution\nLicense (CC BY). The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with\nthese terms.\nEEGformer: A transformer–based\nbrain activity classiﬁcation\nmethod using EEG signal\nZhijiang Wan1,2,3, Manyu Li2, Shichang Liu4, Jiajin Huang5,\nHai Tan6 and Wenfeng Duan1*\n1The First Afﬁliated Hospital of Nanchang University, Nanchang University, Nanchang, Jiangxi, China,\n2School of Information Engineering, Nanchang University, Nanchang, Jiangxi, China, 3Industrial Institute\nof Artiﬁcial Intelligence, Nanchang University, Nanchang, Jiangxi, China, 4School of Computer Science,\nShaanxi Normal University, Xi’an, Shaanxi, China,5Faculty of Information Technology, Beijing University\nof Technology, Beijing, China, 6School of Computer Science, Nanjing Audit University, Nanjing, Jiangsu,\nChina\nBackground: The effective analysis methods for steady-state visual evoked\npotential (SSVEP) signals are critical in supporting an early diagnosis of glaucoma.\nMost efforts focused on adopting existing techniques to the SSVEPs-based brain–\ncomputer interface (BCI) task rather than proposing new ones speciﬁcally suited\nto the domain.\nMethod: Given that electroencephalogram (EEG) signals possess temporal,\nregional, and synchronous characteristics of brain activity, we proposed a\ntransformer–based EEG analysis model known as EEGformer to capture the EEG\ncharacteristics in a uniﬁed manner. We adopted a one-dimensional convolution\nneural network (1DCNN) to automatically extract EEG-channel-wise features.\nThe output was fed into the EEGformer, which is sequentially constructed using\nthree components: regional, synchronous, and temporal transformers. In addition\nto using a large benchmark database (BETA) toward SSVEP-BCI application to\nvalidate model performance, we compared the EEGformer to current state-of-\nthe-art deep learning models using two EEG datasets, which are obtained from\nour previous study: SJTU emotion EEG dataset (SEED) and a depressive EEG\ndatabase (DepEEG).\nResults: The experimental results show that the EEGformer achieves the best\nclassiﬁcation performance across the three EEG datasets, indicating that the\nrationality of our model architecture and learning EEG characteristics in a uniﬁed\nmanner can improve model classiﬁcation performance.\nConclusion: EEGformer generalizes well to different EEG datasets, demonstrating\nour approach can be potentially suitable for providing accurate brain activity\nclassiﬁcation and being used in different application scenarios, such as\nSSVEP-based early glaucoma diagnosis, emotion recognition and depression\ndiscrimination.\nKEYWORDS\nbrain activity classiﬁcation, SSVEPs, EEGformer, EEG characteristics, deep learning\nFrontiers in Neuroscience 01 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 2\nWan et al. 10.3389/fnins.2023.1148855\n1. Introduction\nGlaucoma is known as a “silent thief of sight, ” meaning\nthat patients do not notice the health condition of their visual\nfunction until vision loss and even blindness occur (Abdull et al.,\n2016). According to the world health organization, the number\nof people with glaucoma worldwide in 2020 is 76 million, and\nthe patient number would be increased to 95.4 million in 2030.\nAs the population ages, the number with this condition will also\nincrease substantially (Guedes, 2021). Glaucoma causes irreversible\noptic nerve vision damage. It is crucial to provide accurate early\nscreening to diagnose patients in their early stages so that they\ncan receive appropriate early treatment. Steady-state visual evoked\npotentials (SSVEPs), which refer to a stimulus-locked oscillatory\nresponse to periodic visual stimulation commonly exerted in the\nvisual pathway of humans, can be used to evaluate the functional\nabnormality of the visual pathway that is essential for the complete\ntransmission of visual information (Zhou et al., 2020). SSVEPs are\nalways measured using electroencephalogram (EEG) measurement\nand have been widely used in the study of brainñcomputer interface\n(BCI). Because peripheral vision loss is a key diagnostic sign of\nglaucoma, patients cannot be evoked by certain repetitive stimuli\nwith a constant frequency from vision loss regions (Khok et al.,\n2020). Therefore, stimuli with the corresponding frequency are not\ndetected by the primary visual cortex. Thus, the SSVEPs-based BCI\napplications can be used in the early diagnosis of visual function\ndetection for patients with glaucoma.\nThe eﬀective analysis method for SSVEPs is critical in the\naccurate early diagnosis of glaucoma. SSVEPs are EEG activity with\na spatial-spectral-temporal (SST) pattern. It is easy to understand\nthat SSVEP signals, such as the EEG signal measured over time,\ncould be analyzed using time series analysis methods. Brain\nfunctional connectivity (BFC) can be used to capture spatial\npatterns from multiple brain regions by analyzing the correlations\nbetween brain activities detected from diﬀerent regions. The\nspectral pattern extraction method is the most popular method for\nanalyzing the frequency characteristics of EEG signals. For instance,\npower spectra densityñbased analysis (PSDA) is a commonly used\nfrequency detection method that can classify various harmonic\nfrequencies from EEG signals (Zhang et al., 2020). In addition,\ncanonical correlation analysis (CCA) (Zhuang et al., 2020) and\nother similar algorithms, such as multivariate synchronization\nindex (MSI) (Qin et al., 2021) and correlated component analysis\n(COCA) (Zhang et al., 2019), are eﬀective frequency detection\nalgorithms based on the multivariate statistical analysis method.\nAlthough SST pattern extraction algorithms have demonstrated\nsatisfactory results, most patterns or features extracted from raw\nEEG data require a manually predeﬁned algorithm based on expert\nknowledge. The procedure of learning handcrafted features for\nSSVEP signals is not ﬂexible and might limit the performance of\nthese systems in brain activity analysis tasks.\nIn recent years, deep learning (DL) methods have achieved\nexcellent performance in processing EEG-based brain activity\nanalysis tasks (Li Z. et al., 2022; Schielke and Krekelberg, 2022).\nCurrently, the mainstream technologies of using DL to process\nSSVEP signal could be divided into two aspects: convolutional\nneural network (CNN) based methods and transformer-based\nmethods. For the CNN-based methods, Li et al. (2020) propose a\nCNN-based nonlinear model, i.e. convolutional correlation analysis\n(Conv-CA), to transform multiple channel EEGs into a single EEG\nsignal and use a correlation layer to calculate correlation coeﬃcients\nbetween the transformed single EEG signal and reference signals.\nGuney et al. (2021) propose a deep neural network architecture\nfor identifying the target frequency of harmonics. Waytowich\net al. (2018) design a compact convolutional neural network\n(Compact-CNN) for high-accuracy decoding of SSVEPs signal.\nFor the transformer-based methods, Du et al. (2022) propose\na transformer-based approach for the EEG person identiﬁcation\ntask that extracts features in the temporal and spatial domains\nusing a self-attention mechanism. Chen et al. (2022) propose\nSSVEPformer, which is the ﬁrst application of the transformer\nto the classiﬁcation of SSVEP. Li X. et al. (2022) propose a\ntemporal-frequency fusion transformer (TFF-Former) for zero-\ntraining decoding across two BCI tasks. The aforementioned\nstudies demonstrate the competitive model performance of DL\nmethods in performing SSVEPs-based BCI tasks. However, most\nexisting DL eﬀorts focused on applying existing techniques to the\nSSVEPs-based BCI task rather than proposing new ones speciﬁcally\nsuited to the domain. Standard well-known network architectures\nare designed for data collected in natural scenes and do not consider\nthe peculiarities of the SSVEP signals. Therefore, further research is\nrequired to understand how these architectures can be optimized\nfor EEG-based brain activity data.\nThe main question is what is the speciﬁcity of the SSVEP\nsignal analysis domain and how to use machine learning methods\n(particularly DL methods) to deal with the signal characteristics.\nBecause the SSVEP signal is EEG-based brain activity, we can\nanswer the question by analyzing the EEG characteristics in the\nbrain activity analysis domain. Speciﬁcally, EEG characteristics are\nreﬂected in three aspects: temporal, regional, and synchronous\ncharacteristics. The temporal characteristics (e.g., mean duration,\ncoverage, and frequency of occurrence) are easily traceable in\nstandard EEG data and provide numerous sampling points in a\nshort time (Zhang et al., 2021), thereby providing an eﬃcient way\nto investigate trial-by-trial ﬂuctuations of functional spontaneous\nactivity. The regional characteristics refer to diﬀerent brain regions\nthat are linked to distinct EEG bands (Nentwich et al., 2020).\nThe synchronous characteristics refer to the synchronous brain\nactivity pattern over a functional network including several brain\nregions with similar spatial orientations (Raut et al., 2021).\nTraditionally, brain response to a ﬂickering visual stimulation\nhas been considered steady-state, in which the elicited eﬀect is\nbelieved to be unchanging in time. In fact, the SSVEPs belongs\nto a signal with non-stationary nature, which indicates dynamical\npatterns and complex synchronization between EEG channels can\nbe used to further understand brain mechanisms in cognitive and\nclinical neuroscience. For instance, Ibáñez-Soria et al. explored\nthe dynamical character of the SSVEP response by proposing\na novel non-stationary methodology for SSVEP detection, and\nfound dynamical detection methodologies signiﬁcantly improves\nclassiﬁcation in some stimulation frequencies (Ibáñez-Soria et al.,\n2019). Tsoneva et al. (2021) studied the mechanisms behind SSVEPs\ngeneration and propagation in time and space. They concluded that\nthe SSVEP spatial properties appear sensitive to input frequency\nwith higher stimulation frequencies showing a faster propagation\nspeed. Thus, we hypothesize that a machine learning method that\ncan capture the EEG characteristics in a uniﬁed manner can suit the\nSSVEPs-based BCI domain and improve the model performance in\nEEG-based brain activity analysis tasks.\nFrontiers in Neuroscience 02 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 3\nWan et al. 10.3389/fnins.2023.1148855\nIn this study, we propose a transformerñbased EEG analysis\nmodel known as the EEGformer (Vaswani et al., 2017) to capture\nthe EEG characteristics in the SSVEPs-based BCI task. The\nEEGformer is an end-to-end DL model, processing SSVEP signals\nfrom the EEG to the prediction of the target frequency. The\ncomponent modules of the EEG former are depicted as follows:\n(1) Depth-wise convolution-based one-dimensional convolution\nneural network (1DCNN). The depth-wise convolution-based\n1DCNN is ﬁrst used to process the raw EEG input. Assuming\nthe raw data is collected from C EEG channels, there are\nM depth-wise convolutional ﬁlters for generating M feature\nmaps. Each convolutional ﬁlter is responsible for shifting\nacross the raw data in an EEG-channel-wise manner and\nextracting convolutional features from the raw data of each\nEEG channel to form a feature map. Unlike other techniques\nthat manually extract temporal or spectrum features based\non the time course of the EEG signal, we use the depth-wise\nconvolutional ﬁlter to extract the EEG features in a completely\ndatañdriven manner. Because the feature map is generated\nby the same depth-wise convolutional ﬁlter, each row of the\nfeature map shares the same convolutional property. Follow-\nup convolutional layers are allocated with several depth-\nwise convolutional ﬁlters to enrich the convolutional features\nand deepen the 1DCNN network. A three-dimensional\n(3D) feature matrix is used to represent the output of the\n1DCNN network. The x, y, and z dimensions of the 3D\nfeature matrix represent temporal, spatial, and convolutional\nfeatures, respectively.\n(2) EEGformer encoder. This component module consists of\nthree sub-modules: temporal, synchronous, and regional\ntransformers, which are used in learning the temporal,\nsynchronous, and regional characteristics, respectively. The\ncore strategy of learning EEG characteristics by our model\nmainly include two steps: input tokens that serve as the basic\nelements of learning the temporal, synchronous, and regional\ncharacteristics are sliced from the 3D feature matrix along the\ntemporal, convolutional, and spatial dimension, respectively.\nAnd then, self-attention mechanism is employed to measure\nthe relationships between pairs of input tokens and give\ntokens more contextual information, yielding more powerful\nfeatures for representing the EEG characteristics. The three\ncomponents could be performed in a sequential computing\norder, allowing the encoder to learn the EEG characteristics in\na uniﬁed manner.\n(3) EEGformer decoder. This module contains three\nconvolutional layers and one fully connected (FC) layer.\nThe output of the last FC layer is fed to a softmax function\nwhich produces a distribution over several category labels.\nThe categorical cross entropy combined with regularization\nwas used as the loss function for training the entire\nEEGformer pipeline. The EEGformer decoder is used to deal\nwith speciﬁc tasks, such as target frequency identiﬁcation,\nemotion recognition, and depression discrimination. In\naddition to using a large benchmark database (BETA) (Liu\net al., 2020) to validate the performance of the SSVEP-BCI\napplication, we validate the model performance on two\nadditional EEG datasets, one for emotion analysis using EEG\nsignals [SJTU emotion EEG dataset (SEED)] (Duan et al.,\n2013; Zheng and Lu, 2015) and the other for a depressive\nEEG database (DepEEG) (Wan et al., 2020) obtained from\nour previous study, to support our hypothesis that highlights\nthe signiﬁcance of learning EEG characteristics in a uniﬁed\nmanner for EEG-related data analysis tasks.\nThe main contributions of this study are as follows: (1) current\nmainstream DL models have superior ability in processing data\ncollected in natural scenes and might not adept at dealing with\nSSVEP signals. To achieve a DL model that can be applied to the\nspeciﬁcity of the SSVEP signal analysis domain and obtain better\nmodel performance in SSVEPs-based frequency recognition task,\nwe propose a transformerñbased EEG analysis model known as\nthe EEGformer to capture the EEG characteristics in a uniﬁed\nmanner. (2) To obtain a ﬂexible method for addressing the SSVEPs-\nbased frequency recognition and avoid the model performance\nlimited by manual feature extraction, we adopt 1DCNN to\nautomatically extract EEG-channel-wise features and fed them into\nthe EEGformer. This operation transforms our method into a\ncomplete datañdriven manner for mapping raw EEG signals into\ntask decisions. (3) To ensure the eﬀectiveness and generalization\nability of the proposed model, we validate the performance of the\nEEGformer on three datasets for three diﬀerent EEG-based data\nanalysis tasks: target frequency identiﬁcation, emotion recognition,\nand depression discrimination.\n2. Materials and methods\n2.1. Dataset preparation\nTable 1 shows some detailed information about the three\ndatasets (BETA, SEED, and DepEEG) that we used as benchmarks\nto validate the eﬀectiveness of this study. The participants’\ncolumn in the table describes how many subjects joined in the\ncorresponding data collection. The experiment per participant\n(EPP) column shows how many experiments were performed by\neach participant. The trails per experiment (TPE) column shows\nhow many trails are executed in one experiment. The channel\nnumber (CHN) column shows the CHN of the EEG dataset.\nThe sampling rate (SR) column shows the down-sampling rate of\nthe EEG signal. The time length per trail (TLPT) column shows\nthe time length of a single trail in seconds. The labels column\nshows the categorical emotion labels for the classiﬁcation task and\nemotional intensity for the regression task. Speciﬁcally, for the\ntarget frequency identiﬁcation task, we classiﬁed 40 categories of\nharmonic frequencies and the frequency range is 8ñ15.8 HZ with\n0.2 HZ intervals. For the emotion recognition task, we used arousal,\nvalence, and dominance rating scores as the dataset labels. For the\ndepression discrimination task, we classiﬁed EEG samples from\ndepressive or normal control.\n2.2. Pipeline of EEGformer–based brain\nactivity analysis\nFigure 1shows the pipeline of EEGformerñbased brain activity\nanalysis. The core modules of the pipeline include 1DCNN,\nEEGformer encoder, and decoder. The input of the 1DCNN is an\nFrontiers in Neuroscience 03 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 4\nWan et al. 10.3389/fnins.2023.1148855\nTABLE 1 Detail information on the three datasets.\nDataset Participants EPP TPE CHN SR (HZ) Labels TLPT\nBETA 70 healthy subjects 4 40 64 250 40 harmonics, e.g., fj ∈{8,8.2,. . . ,15.8} 2/3 s\nSEED 15 healthy subjects 3 15 62 200 Positive, neutral, negative 305 s\nDepEEG 12 healthy subjects and 23 depressives 1 1 6 500 Depressive, normal control ≥480 s\nFIGURE 1\nPipeline of EEGformer for different tasks of brain activity analysis.\nEEG segment represented using a two-dimensional (2D) matrix\nof size S ×L, where S represents the number of EEG channels,\nand L represents the segment length. The EEG segment is de-trend\nand normalized before being fed into the 1DCNN module, and the\nnormalized EEG segment is represented by x∈RS ×L. The 1DCNN\nadopts multiple depth-wise convolutions to extract EEG-channel-\nwise features and generate 3D feature maps. It shifts across the data\nalong the EEG channel dimension for each depth-wise convolution\nand generates a 2D feature matrix of size S ×Lf , where Lf is the\nlength of the extracted feature vector. The output of the 1DCNN\nmodule is a 3D feature matrix of size S ×C ×Le, where C is\nthe number of depth-wise convolutional kernels used in the last\nlayer of the 1DCNN module, Le is the features length outputted\nby the last layer of the 1DCNN module. More speciﬁcally, the\n1DCNN is comprised of three depth-wise convolutional layers.\nHence, we have the processing x →z1 →z2 →z3, where z 1,\nz2, and z 3 denote the outputs of the three layers. The size of\nthe depth-wise convolutional ﬁlters used in the three layers is\n1 ×10, valid padding mode is applied in the three layers and\nthe stride of the ﬁlters is set to 1. The number of the depth-\nwise convolutional ﬁlter used in the three layers is set to 120,\nensuring suﬃcient frequency features for learning the regional and\nsynchronous characteristics. We used a 3D coordinate system to\ndepict the axis meaning of the 3D feature matrix. The X, Y, and\nZ axes represent the temporal, spatial, and convolutional feature\ninformation contained in the 3D feature matrix, respectively. The\noutput of the 1DCNN module is fed into the EEGformer encoder\nfor encoding the EEG characteristics (regional, temporal, and\nsynchronous characteristics) in a uniﬁed manner. The decoder is\nresponsible for decoding the EEG characteristics and inferencing\nthe results according to the speciﬁc task.\n2.3. EEGformer encoder\nThe EEGformer encoder is used to provide a uniform\nfeature reﬁnement for the regional, temporal, and synchronous\ncharacteristics contained in the output of the 1DCNN module.\nFigure 2 illustrates the EEGformer architecture and shows that\nthe EEGformer encoder uses a serial structure to sequentially\nreﬁne the EEG characteristics. The temporal, regional, and\nsynchronous characteristics are reﬁned using temporal, regional,\nand synchronous transformers, respectively. The outputs of the\n1DCNN are deﬁned as z3 ∈RS ×C ×Le and are represented using\nblack circles in the green rectangle box.\nThe speciﬁc computing procedures of each transformer module\nare depicted as follows:\n2.3.1. Regional transformer module\nThe input of the regional transformer module is represented\nby z3 ∈RC ×Le ×S. The 3D matrix z3 is ﬁrst segmented into S\n2D submatrices along the spatial dimension. Each submatrix is\nrepresented by X reg\ni ∈RC ×Le (i = 1,2,3, . . .,S). The input of the\nregional transformer module is represented by S black circles in\nthe green rectangle box and each circle represents a submatrix.\nThe vector Xreg\n(i,c) ∈RLe is sequentially taken out from the X reg\ni\nalong the convolutional feature dimension and fed into the linear\nmapping module. According to the terminology used in the vision\nof transformer (ViT) studies, we deﬁned the vector Xreg\n(i,c) as a patch\nof the regional transformer module. Each Xreg\n(i,c) is represented by a\ntiny yellow block in the Figure 2. The Xreg\n(i,c) is linearly mapped into\na latent vector z(reg,0)\n(i,c) ∈RD using a learnable matrix M ∈RD ×Le :\nz(reg,0)\n(i,c) =MXreg\n(i,c) +epos\n(i,c), (1)\nwhere epos\n(i,c) ∈RD denotes a positional embedding added to encode\nthe position for each convolutional feature changing over time.\nThe regional transformer module also consists of K ≥1 encoding\nblocks, each block contains two layers: a multi-head self-attention\nlayer and a position-wise fully connected feed-forward network.\nThe resulting z(reg,0)\n(i,c) is deﬁned as a token representing the inputs\nof each block, and the z(reg,0)\n(0,0) indicates the classiﬁcation token.\nThe l-th block produces an encoded representation z(reg,l)\n(i,c) for each\ntoken in the input sequence by incorporating the attention scores.\nSpeciﬁcally, at each blockl, three core vectors, includingq(l,a)\n(i,c) , k(l,a)\n(i,c) ,\nand v(l,a)\n(i,c) are computed from the representation z(reg,l−1)\n(i,c) encoded\nby the preceding layer:\nq(l,a)\n(i,c) =W(l,a)\nQ LN(z(reg,l−1)\n(i,c) ) ∈RDh , (2)\nFrontiers in Neuroscience 04 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 5\nWan et al. 10.3389/fnins.2023.1148855\n                   ………\n                   ………\nDecoder\nEEGformer Encoder\n                 ………\n1DCNN\n0 1 2 3 n\nSynchronous \ntransformer\nAdd & Norm\nForward\nAdd & Norm\nMulti-Head\nAttention\nclass token\nEmbedded Patches\n(S, C, D)\nTemporal \ntransformer\nRegional transformer\nEEG segment\nFIGURE 2\nIllustration of the EEGformer architecture.\nk(l,a)\n(i,c) =W(l,a)\nK LN(z(reg,l−1)\n(i,c) ) ∈RDh , (3)\nv(l,a)\n(i,c) =W(l,a)\nV LN(z(reg,l−1)\n(i,c) ) ∈RDh , (4)\nwhere W(l,a)\nQ , W(l,a)\nK , and W(l,a)\nV are the matrixes of query, key,\nand value in the regional transformer module, respectively. LN()\ndenotes the LayerNorm operation, and a ∈{1, 2, 3, . . ., A} is an\nindex over the multi-head self-attention units. A is the number of\nunits in a block. Dh is the quotient computed by D /A and denotes\nthe dimension number of three vectors. The regional self-attention\n(RSA) scores for z(reg,l−1)\n(i,c) in the a-th multi-head self-attention unit\nis given as follows:\nα(l,a)\n(i,c)\nreg\n=σ\n\nq(l,a)\n(i,c)√Dh\n·\n[\nk(l,a)\n(0,0)\n{\nk(l,a)\n(i,c)\n}\nc =1,...,C\n]\n∈RC, (5)\nwhere σ denotes the softmax activation function, and the symbol ·\ndenotes the dot product for computing the similarity between the\nquery and key vectors. k(l,a)\n(i,c) and q(l,a)\n(i,c) represent the corresponding\nkey and query vectors, respectively. The equation shows that the\nRSA scores are merely computed over convolutional features of\nsingle brain region. That is, the RSA can calculate the contribution\nof a changing mono-electrode convolutional feature to the ﬁnal\nmodel decision at a speciﬁc EEG channel. An intermediate vector\ns(l,a)\n(i,c) for encoding z(reg,l−1)\n(i,c) is given as follows:\ns(l,a)\n(i,c) =α(l,a)\n(i,0)v(l,a)\n(i,0) +\nC∑\nj =1\nα(l,a)\n(i,j)v(l,a)\n(i,j) ∈RDh . (6)\nThe encoded feature z(reg,l)\n(i) ∈RC ×D by the l-th block is computed\nby ﬁrst concatenating the intermediate vectors from all heads,\nand the vector concatenation is projected by matrix WO ∈RD ×L,\nwhere L is equal to A = Dh. z\n′(reg,l)\n(i) is the residual connection\nresult of the projection of the intermediate vectors and thez(reg,l−1)\n(i)\nencoded by the preceding block. Finally, the z\n′(reg,l)\n(i) normalized\nby LN() is passed through a multilayer perceptron (MLP) using\nthe residual connection. The output of the regional transformer is\nrepresented by z4 ∈RS ×C × D.\n2.3.2. Synchronous transformer module\nThe input of the synchronous transformer module is\nrepresented by z4 ∈RS ×Le ×C. The 3D matrixz4 is ﬁrst segmented\ninto C 2D submatrices along the convolutional feature dimension.\nEach submatrix is represented by X syn\ni ∈RS ×D (i = 1,2,3, . . .,C).\nThe vector Xsyn\n(i,s) ∈RD is sequentially taken out from the Xsyn\ni along\nthe spatial dimension and fed into the linear mapping module. The\nXsyn\n(i,s) is deﬁned as a patch and is linearly mapped into a latent vector\nz(syn,0)\n(i,s) ∈RD using a learnable matrix M ∈RD ×D:\nz(syn,0)\n(i,s) =MXsyn\n(i,s) +epos\n(i,s), (7)\nwhere epos\n(i,s) ∈RD denotes a positional embedding added to encode\nthe spatial position for each EEG channel changing over time. The\nsynchronous transformer also consists of K ≥1 encoding blocks,\nand each block contains two layers: a multi-head self-attention\nlayer and a position-wise fully connected feed-forward network.\nThe resulting z(syn,0)\n(i,s) is deﬁned as a token representing the inputs\nof each block, and the z(syn,0)\n(0,0) indicates the classiﬁcation token.\nThe l-th block produces an encoded representation z(syn,l)\n(i,s) for each\ntoken in the input sequence by incorporating the attention scores.\nSpeciﬁcally, at each blockl, three core vectors, includingq(l,a)\n(i,s) , k(l,a)\n(i,s) ,\nand v(l,a)\n(i,s) are computed from the representation z(syn,l−1)\n(i,s) encoded\nby the preceding layer:\nq(l,a)\n(i,s) =W\n′(l,a)\nQ LN(z(syn,l−1)\n(i,s) ) ∈RDh , (8)\nk(l,a)\n(i,s) =W\n′(l,a)\nK LN(z(syn,l−1)\n(i,s) ) ∈RDh , (9)\nv(l,a)\n(i,s) =W\n′(l,a)\nV LN(z(syn,l−1)\n(i,s) ) ∈RDh , (10)\nwhere W\n′(l,a)\nQ , W\n′(l,a)\nK , and W\n′(l,a)\nV are the matrixes of query, key,\nand value in the synchronous transformer module, respectively.\nFrontiers in Neuroscience 05 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 6\nWan et al. 10.3389/fnins.2023.1148855\nSynchronous e self-attention (SSA) scores for z(syn,l−1)\n(i,s) in the\na-th multi-head self-attention unit are given as follows:\nα(l,a)\n(i,s)\nsyn\n=σ\n\nq(l,a)\n(i,s)√Dh\n·\n[\nk(l,a)\n(0,0)\n{\nk(l,a)\n(i,s)\n}\ns =1,...,S\n]\n∈RS, (11)\nwhere k(l,a)\n(i,s) and q(l,a)\n(i,s) denote the corresponding key and query\nvectors, respectively. The equation shows that the SSA scores are\nmerely computed over the feature map extracted by the same\ndepth-wise convolution. The SSA can calculate the contribution of\nconvolution features changing over time to the ﬁnal model decision\nat a speciﬁc EEG channel. An intermediate vectors(l,a)\n(i,s) for encoding\nz(syn,l−1)\n(i,s) is given as follows:\ns(l,a)\n(i,s) =α(l,a)\n(i,0)v(l,a)\n(i,0) +\nC∑\nj =1\nα(l,a)\n(i,j)v(l,a)\n(i,j) ∈RDh . (12)\nThe encoded feature z(syn,l)\n(i) ∈RS ×D by the l-th block is computed\nby ﬁrst concatenating the intermediate vectors from all heads,\nand the vector concatenation is projected by matrix WO ∈RD ×L.\nz\n′(syn,l)\n(i) is the residual connection result of the projection of the\nintermediate vectors and the z(syn,l−1)\n(i) encoded by the preceding\nblock. Finally, the z\n′(syn,l)\n(i) normalized by LN() is passed through\na multilayer perceptron (MLP) using the residual connection. The\noutput of the synchronous transformer is represented by z5 ∈\nRC ×S × D.\n2.3.3. Temporal transformer module\nThe input of the temporal transformer module is z5 ∈\nRC ×S ×D. To avoid huge computational complexity, we compress\nthe original temporal dimensionalityD of z5 into dimensionality M.\nThat is, the 3D matrix z 5 is ﬁrst segmented and then averaged into\nM 2D submatrices along the temporal dimension. Each submatrix\nis represented by X temp\ni ∈RS ×C (i = 1,2,3, . . .,M) and the M\nsubmatrices are concatenated to form Xtemp ∈RM ×S ×C. Each\nsubmatrix Xtemp\ni is ﬂattened into a vector X\n′temp\ni ∈RL1, where L1\nis equal to S ×C. The X\n′temp\ni is deﬁned as a patch and is linearly\nmapped into a latent vector z(temp,0)\n(i) ∈RD using a learnable matrix\nM ∈RD ×L:\nz(temp,0)\n(i) =MX\n′temp\n(i) +epos\n(i) , (13)\nwhere epos\n(i) ∈RD denotes a positional embedding added to encode\nthe temporal position for each EEG channel changing over the\nfeatures extracted by diﬀerent depth-wise convolutional kernels.\nThe module consists of K ≥1 encoding blocks, each block contains\ntwo layers: a multi-head self-attention layer and a position-wise\nfully connected feed-forward network. The resulting z(temp,0)\n(i) is\ndeﬁned as a token representing the inputs of each block, and the\nz(temp,0)\n(0) indicates the classiﬁcation token. The l-th block produces\nan encoded representation z(temp,l)\n(i) for each token in the input\nsequence by incorporating the attention scores. Speciﬁcally, at\neach block l, three core vectors, including q(l,a)\n(i) , k(l,a)\n(i) , and v(l,a)\n(i)\nare computed from the representation z(temp,l−1)\n(i) encoded by the\npreceding layer:\nq(l,a)\n(i) =W\n′′(l,a)\nQ LN(z(temp,l−1)\n(i) ) ∈RDh , (14)\nk(l,a)\n(i) =W\n′′(l,a)\nK LN(z(temp,l−1)\n(i) ) ∈RDh , (15)\nv(l,a)\n(i) =W\n′′(l,a)\nV LN(z(temp,l−1)\n(i) ) ∈RDh , (16)\nwhere W\n′′(l,a)\nQ , W\n′′(l,a)\nK , and W\n′′(l,a)\nV are the matrixes of query, key,\nand value in the temporal transformer, respectively. The temporal\nself-attention (TSA) score for z(T,l−1)\n(i,s) in the a-th multi-head self-\nattention unit is given as follows:\nα(l,a)\n(i)\ntemp\n=σ\n\nq(l,a)\n(i)√Dh\n·\n[\nk(l,a)\n(0)\n{\nk(l,a)\n(i)\n}\ni =1,...,M\n]\n∈RM. (17)\nThe equation shows that the TSA scores are merely computed over\nthe temporal dimension. The TSA can calculate the contribution of\nmultiple electrode features changing over diﬀerent convolutional\nfeatures to the ﬁnal model decision at a speciﬁc time. An\nintermediate vector s(l,a)\n(i) for encoding z(temp,l−1) is given as follows:\ns(l,a)\n(i) =α(l,a)\n(i,0)v(l,a)\n(i,0) +\nM∑\nj =1\nα(l,a)\n(i,j)v(l,a)\n(i,j) ∈RDh . (18)\nThe encoded feature z(temp,l) ∈RM ×L1 by the l-th block is\ncomputed by ﬁrst concatenating the intermediate vectors from\nall heads, and the vector concatenation is projected by matrix\nWO ∈RL1 ×L. z\n′(temp,l) is the residual connection result of the\nprojection of the intermediate vectors and the z(temp,l−1) encoded\nby the preceding block. Finally, the z\n′(temp,l) normalized by LN() is\npassed through a multilayer perceptron (MLP) using the residual\nconnection. The output of the temporal transformer is represented\nby O ∈RM ×L1.\n2.4. EEGformer decoder\nThe EEGformer is used to extract the temporal, regional,\nand synchronous characteristics in a uniﬁed manner, as well\nas to deal with various EEG-based brain activity analysis tasks.\nUnlike the original transformer decoder, which uses a multi-\nhead self-attention mechanism to decode the feature output of\nthe corresponding encoder, we designed a convolution neural\nnetwork (CNN) to perform the corresponding task. The CNN\ncontains three convolutional layers and one fully connected layer.\nSpeciﬁcally, the output O ∈RM ×L1 of the EEGformer encoder is\nreshaped to X ∈RS ×C ×M, where M is the dimensional length of\nthe encoded temporal feature. The ﬁrst layer of our EEGformer\ndecoder (with the weights w1 ∈RC ×1) linearly combined diﬀerent\nconvolutional features for normalization across the convolutional\ndimension. Thus, the output data shape of the ﬁrst layer is X1 ∈\nRS ×M. The motivation to convolve C feature maps along the\nconvolutional dimension of X into one is to allow the network\nto make datañdriven decisions about the contribution of diﬀerent\nconvolutional features to the ﬁnal model decision. The second layer\nof our CNN was responsible for combining information across\nspatial dimensions of X and extracting the entire information\nwhile discarding redundancy or noninformative variations. To\nthis end, our CNN convolved X along the spatial dimension\nusing the weights w2 ∈RS ×N and returns the plane X2 ∈RM ×N ,\nFrontiers in Neuroscience 06 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 7\nWan et al. 10.3389/fnins.2023.1148855\nwhere N denotes the number of convolutional ﬁlters used in the\nsecond layer. The third layer halved the dimension and reduced\nthe parameter complexity using the weights w3 ∈R(M/2) ×N to\nproduce the output plane X3 ∈R(M/2) ×N . The fourth layer of our\nCNN is a fully connected layer that produced classiﬁcation results\nfor the brain activity analysis task. The corresponding equation of\nthe loss function is given as follows:\nLoss = 1\nDn\nDn∑\ni =1\n−log\n(\npi\n(\nyi\n))\n+λ |w| (19)\nwhere Dn is the number of data samples in the training dataset,\npi and yi are the prediction results produced by the model and\nthe corresponding ground truth label for the i-th data sample,\nrespectively, and λ is the constant of the L1 regularization.\n3. Experiment results\n3.1. Experimental setup\nFor generating the input of the EEGformer and other\ncomparison models, we ﬁrst extract the raw EEG data of each\ntrial of the three datasets to form data samples and assign the\ncorresponding label to each data sample. Further, we apply a sliding\nwindow with the step of ratio ×SR (i.e., SR) on each data sample\nand generate the ﬁnal input samples in a non-overlapping manner.\nThe data shape of each input sample is ratio ×SR ×Nc, and the\nNc denotes the number of EEG channels (i.e., 64). The equation\nfor representing the relationship between segment lengthT and the\ntotal number of input samples N is given as follows:\nN = Nsub ×EPP ×TPE ×TLPT\nratio , (20)\nwhere Nsub denotes how many subjects joined in the corresponding\ndata collection experiment. Taking the data splitting method for\nBETA dataset as an example, we remove the EEG data collected\nduring the gaze shifting of 0.5 s guided by a visual cue and an\noﬀset of 0.5 s followed by the visual stimulation. The ﬁnal BETA\ndataset consists of 11,200 trials and 40 categories. For the ﬁrst 15\nparticipants and the remaining 55 participants in the BETA dataset,\nthe time length of the ﬂickering visual stimulation in each trial is 2\nand 3 s, respectively. When the number of data points of each input\nsample is 100, meaning the ratio is set to 0.4 and the SR is equal\nto 250 Hz, and the time length of each input sample is 0.4 s, the\ntotal number of input samples of the BETA dataset for training and\ntesting models is 78,000. Under the same setting, the total number\nof input samples of the SEED and DepEEG dataset for training and\ntesting models is 514,687 and 42,000.\nThe state-of-the-art DL models, which have performed well in\nprevious studies, were tested on the three datasets to compare their\nmodel performance with ours. In our comparison, we followed the\nsame test procedures for all these methods. The EEGformer and\nother comparison baselines were trained with a batch size of 64 and\nAdam optimizer with a learning rate of 0.001. In each transformer\nmodule, the number of encoding blocks is equal to three. The\nmodels were trained using an early-stop training strategy. Note\nthat all training hypermeters were optimized using the testing\ndata. Pytorch was used to implement these models, which were\ntrained on an NVIDIA Tesla A100 GPU. As mentioned above, we\ntested our model on three datasets (BETA, SEED, and DepEEG).\nFivefold cross-validation was applied to separate the dataset, and\nthe average classiﬁcation accuracy (ACC) rate, sensitivity (SEN),\nand speciﬁcity (SPE) and the corresponding standard deviation\n(SD) of them were used as model performance metrics. For multi-\ncategory classiﬁcation, the accuracy rate, which means how many\ndata samples are corrected and labeled out of all the data samples,\nis calculated as the sum of true positive and true negative divided\nby the sum of true positive, false positive, false negative, and true\nnegative. The above metrics are calculated using the following\nformula:\nACC =(TP+TN)/(TP+FP+FN+TN) (21)\nSEN = TP/(TP +FN), (22)\nSPE = TN/(TN +FP), (23)\nwhere TP denotes true positives, TN denotes true negatives, FP\ndenotes false positives, and FN denotes false negatives.\n3.2. Comparison baselines\nTo show the eﬀective model performance of EEGformer, we\ncompared several commonly used DL methods in other studies\nof EEG-based data analysis tasks, which were target frequency\nidentiﬁcation, emotion recognition, and depression discrimination.\nThe comparison models are described as follows:\n(1) EEGNet (Lawhern et al., 2018). It is a Compact-CNN for EEG-\nbased BCIs. The network starts with a temporal convolution\noperation to learn frequency ﬁlters. The operation is made up\nof F1 convolutional ﬁlters, and each size equals 1×N, where N\nrepresents the length of the convolutional ﬁlter. It usedD ×F1\ndepth-wise convolutional ﬁlters to learn frequency-speciﬁc\nspatial ﬁlters and the size of each ﬁlter is C ×1. The separable\nconvolution followed by point-wise convolution was used\nto learn the summary for each feature map and optimally\ncombine them. The network architecture shows that EEGNet\nconsiders temporal and spatial information of EEG signals.\n(2) Conv-CCA (Waytowich et al., 2018). It is designed for\nSSVEPs-based target frequency identiﬁcation and can be\nused in other EEG-based classiﬁcation tasks. Unlike pure\nDL models, the Conv-CCA uses a signal-CNN with three-\nlayers to transform multiple channel EEGs ( Ns ×Nc ×1)\ninto a single ¯x with a shape of Ns ×1 ×1, where Ns\nand Nc are the numbers of sampling points and channels,\nrespectively. Another reference CNN with two-layers was\nused to transform the reference signal ( Ns ×Nf ×Nc) into\na 2D signal ¯Y with a shape of Ns ×Nf , where Nf is the\nnumber of target frequencies. Correlation analysis was used\nto calculate the coeﬃcients of ¯x and each ¯Yn for all n ∈[1, Nf ].\nA dense layer with Nf units and a softmax activation function\nwas used as the ﬁnal layer for classiﬁcation.\nFrontiers in Neuroscience 07 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 8\nWan et al. 10.3389/fnins.2023.1148855\n(3) 4DCRNN (Shen et al., 2020). It is a DL model known\nas a four-dimensional (4D) convolutional recurrent neural\nnetwork that extracts and fuses frequency, spatial and\ntemporal information from raw EEG data to improve model\nperformance of emotion recognition. It is not an end-to-\nend model for BCI tasks because it requires the Butterworth\nﬁlter to decompose frequency bands and manually extract\ndiﬀerential entropy features from each frequency band.\nThe model input is represented as a 4D structure X ∈\nRh ×w ×d ×2T, where h and w are the height and width of\nthe 2D brain topographical map, respectively, d denotes the\nnumber of frequency bands and T denotes the length of the\nsignal segment. CNN was used to extract the frequency and\nspatial information from each temporal segment of an EEG\nsample, and long short-term memory (LSTM) was adopted to\nextract temporal information from CNN outputs.\n(4) EmotionNet (Wang et al., 2018). Instead of using 2D\nconvolution ﬁlters to extract features from input data,\nEmotionNet used a 3D convolution ﬁlter to learn spatial and\ntemporal features from raw EEG data. The ﬁrst two layers\nand the third layer of the model used a 3D convolution ﬁlter\nto learn spatiotemporal and fuse spatial features, respectively.\nThe fourth and ﬁfth layers of the model performed temporal\nfeature extraction using a 2D convolutional ﬁlter. The sixth\nlayer of the model is a fully connected layer for dense\npredictions.\n(5) PCRNN (Y ang et al., 2018). The model is an end-to-end DL\nmodel known as a parallel convolutional recurrent neural\nnetwork for EEG-based emotion classiﬁcation tasks. It also\ntakes 3D shape (X ∈Rh ×w ×T) of raw EEG data as model\ninput. CNN model was ﬁrst used to learn spatial feature\nmaps from each 2D map, and the LSTM was used to extract\ntemporal features from the CNN outputs. Note that the CNN\nand LSTM were organized by a parallel structure to extract\nthe spatial and temporal features from the model input. The\noutputs of the parallel structure were integrated to classify\nemotions.\n3.3. Ablation studies\n3.3.1. Effect of the EEGformer decoder\nconstructed by different transformer\ncombination\nWe conducted an ablation study to show the eﬀectiveness\nof the EEGformer by constructing the encoder with diﬀerent\ncombinations of temporal, synchronous, and regional\ntransformers. The classiﬁcation results (ACC, SPE, SEN, and\ntheir corresponding SDs) on the three EEG datasets using diﬀerent\ntransformer module combinations to construct the EEGformer\nencoder are shown in Table 2. The table shows that the EEGformer\nencoder constructed by the combinations of the three transformers\nachieves the best classiﬁcation results. For BETA dataset, the\naverage sensitivity, speciﬁcity, and accuracy are 69.86, 75.86, and\n70.15%, respectively. For SEED dataset, the average sensitivity,\nspeciﬁcity, and accuracy are 89.14, 92.75, and 91.58%, respectively.\nFor DepEEG dataset, the average sensitivity, speciﬁcity, and\naccuracy are 77.83, 70.95, and 72.19%, respectively. The result\nsupports our hypothesis that a machine learning method can\ncapture EEG characteristics in a uniﬁed manner that can suit the\nEEG-based brain activity analysis tasks.\nThe table also demonstrates that the EEGformer that contains\na synchronous transformer achieves better model performance\nthan the EEGformer without a synchronous transformer. For\ninstance, the EEGformer constructed using a single synchronous\ntransformer outperforms the EEGformer constructed using the\nother two types of single transformers, with better accuracy\nof 57.29, 80.12, and 60.12% on BETA, SEED, and DepEEG,\nrespectively. The EEGformer constructed using a transformer\npair consisting of a synchronous transformer outperforms the\nEEGformer constructed using the transformer pair without a\nsynchronous transformer, with better accuracy on the BETA,\nSEED, and DepEEG datasets. The results indicate the signiﬁcance\nof learning spatial distribution characteristics of EEG activity\ngenerated by multiple brain regions for the task of SSVEPs-\nbased frequency discrimination. In addition, the EEGformer\nconstructed using synchronous transformer and regional\ntransformer outperforms the EEGformer constructed using\nother transformer pairs, with better classiﬁcation results on SEED\nand DepEEG dataset. On the one hand, the result demonstrates\nthat the convolutional features could represent regional and spatial\ncharacteristics of EEH signal well. On the other hand, the result\nindicates that the integration of the synchronous and regional EEG\ncharacteristics improves discrimination ability of our model.\n3.3.2. Effect of using 1DCNN or not to construct\nthe EEGformer pipeline\nThe model performance aﬀected by using 1DCNN or not\nis validated to show the rationality of using a 1D depth-\nwise convolutional ﬁlter to learn regional characteristics in a\ncompletely datañdriven manner. Figure 3compares the results of\nusing 1DCNN or not constructing the EEGformer pipeline. The\nﬁgure shows that using a 1D depth-wise convolutional ﬁlter to\nlearn regional characteristics is beneﬁcial for improving model\nperformance in EEG-based classiﬁcation tasks.\n3.3.3. Effect of EEG channel number on the\nmodel performance\nTable 3reports the classiﬁcation results (ACC, SPE, SEN, and\ntheir corresponding SDs) of our model with varying number of\nEEG channel. The EEG CHN and the corresponding name of brain\nregions are illustrated as follows: 3 (O1, Oz, and O2), 6 (O1, Oz,\nO2, POz, PO3, and PO4), 9 (O1, Oz, O2, Pz, PO3, PO5, PO4,\nPO6, and POz), 32 channels (all channels from occipital, parietal,\ncentral-parietal regions and C3, C1, Cz, C2, C4, and FCz) as well\nas all 64 channels. From the table, we can know that as the EEG\nCHN increases, the classiﬁcation results of the EEGformer show an\nupward trend. This result indicates that although the EEG channels\nthat are placed over the occipital and parietal regions provide\nperhaps the most informative SSVEP signals, other channels are\ninformative as well. The result also illustrates the data mining\nability of our model, which can learn representational features from\ncomplex data structure.\n3.4. Comparison studies\nLeave-one-subject-out (LOSO) cross-validation method is\nutilized to compare the model performance between EEGformer\nFrontiers in Neuroscience 08 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 9\nWan et al. 10.3389/fnins.2023.1148855\nTABLE 2 Classiﬁcation results (ACC, SPE, SEN, and their corresponding SDs) on the three EEG datasets by using different transformer module\ncombinations to construct EEGformer encoders.\nCombinations BETA SEED DepEEG\nACC (%) SPE (%) SEN (%) ACC (%) SPE (%) SEN (%) ACC (%) SPE (%) SEN (%)\nReg 41.63 ±5.91 46.59 ±3.58 35.67 ±3.26 76.53 ±1.68 77.26 ±2.41 73.58 ±1.94 58.78 ±5.21 60.51 ±2.58 57.25 ±3.42\nSyn 57.29 ±6.50 62.86 ±5.89 55.28 ±4.69 80.12 ±5.12 82.83 ±4.65 78.86 ±2.71 60.12 ±4.86 65.94 ±3.59 55.26 ±4.27\nTemp 45.36 ±7.18 53.38 ±6.38 43.86 ±5.68 77.28 ±4.12 78.29 ±3.83 76.69 ±3.82 61.73 ±4.12 65.82 ±4.78 60.83 ±2.65\nTemp + Syn 66.52 ±3.82 70.25 ±2.97 62.23 ±4.32 85.36 ±3.61 88.36 ±4.75 83.45 ±2.86 70.15 ±3.18 68.97 ±3.56 75.65 ±4.81\nTemp + Reg 59.29 ±3.27 65.93 ±2.65 58.79 ±3.54 80.12 ±3.19 82.33 ±2.08 79.16 ±3.19 65.21 ±2.89 62.14 ±4.72 72.31 ±3.75\nSyn + Reg 65.72 ±2.91 70.85 ±2.58 61.23 ±5.12 86.73 ±2.95 88.04 ±2.36 83.77 ±3.76 71.46 ±2.85 61.96 ±2.36 75.64 ±3.19\nTemp + Syn + Reg 70.15 ±2.18 75.86 ±2.04 69.86 ±3.29 91.58 ±2.77 92.75 ±3.72 89.14 ±2.98 72.19 ±2.67 70.95 ±2.38 77.83 ±2.15\nFIGURE 3\nComparison results of using 1DCNN or not to construct the EEGformer pipeline.\nTABLE 3 Classiﬁcation results (ACC, SPE, SEN, and their corresponding SDs) of our model is reported versus varying number of channels and\n1.0 s of stimulation.\nChannel number BETA SEED DepEEG\nACC (%) SPE (%) SEN (%) ACC (%) SPE (%) SEN (%) ACC (%) SPE (%) SEN (%)\n3 42.73 ±3.60 50.73 ±5.17 36.83 ±4.39 69.54 ±3.86 70.49 ±2.96 66.76 ±4.85 51.29 ±2.99 50.86 ±3.75 55.71 ±4.51\n6 50.86 ±4.49 63.69 ±2.38 55.17 ±6.73 73.21 ±2.83 74.62 ±3.79 73.61 ±2.73 56.74 ±3.85 54.14 ±2.64 60.26 ±3.29\n9 56.52 ±2.17 70.46 ±3.96 65.89 ±5.26 76.37 ±3.72 77.24 ±4.21 78.18 ±3.82 61.21 ±4.74 59.75 ±3.82 65.78 ±2.79\n32 65.21 ±3.05 72.17 ±2.57 65.36 ±4.74 85.98 ±3.16 86.91 ±2.64 86.27 ±4.54 68.56 ±2.38 65.37 ±3.57 70.39 ±4.26\n64 70.15 ±2.18 75.86 ±2.04 69.86 ±3.29 91.58 ±2.77 92.75 ±3.72 89.14 ±2.98 72.19 ±2.67 70.95 ±2.38 77.83 ±2.15\nand other ﬁve comparison methods. As shown in Figure 4,\nthe upper ﬁgure shows accuracy comparison results between\nEEGformer and Conv-CCA across using BETA dataset, and\nthe lower ﬁgure shows standard deviation comparison between\nEEGformer and other ﬁve comparison methods across subjects\nusing BETA dataset. The reason of only choosing Conv-CCA to\ncompare with EEGformer is both of them achieve high accuracy on\nthe BETA dataset. From the Figure 5, we can ﬁnd that EEGformer\nachieves the lowest standard deviation among other comparison\nmethods, indicating the proposed method generalizes well on\nunseen data and potentially requires little to model training and\ncalibration for new users, suitable for SSVEP classiﬁcation tasks.\n1. Accuracy comparison between EEGformer and Conv-CCA\nacross subjects using BETA dataset.\nFrontiers in Neuroscience 09 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 10\nWan et al. 10.3389/fnins.2023.1148855\nFIGURE 4\nPerformance comparison between EEGformer and other ﬁve comparison methods using leave-one-subject-out cross-validation method based on\nBETA dataset. (A) Accuracy comparison between EEGformer and Conv-CCA across subjects using BETA dataset. (B) Standard deviation comparison\nbetween EEGformer and other ﬁve comparison methods across subjects using BETA dataset.\nFIGURE 5\nPerformance (average ACC ±SD %) of segment length T using the EEGformer and other comparable models on the three EEG datasets.\n2. Standard deviation comparison between EEGformer and\nother ﬁve comparison methods across subjects using BETA\ndataset.\nFurthermore, according to the SSVEP studies, they pursue a\nhigher information transfer rate by not using long EEG segments\nto execute the target frequency identiﬁcation task. The model\nperformance can be improved by increasing the segment length\nT because longer EEG segments contain more information about\nbrain activity. Therefore, we investigated the impact of segment\nlength T ranges [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5]\non model performance. The performance (average ACC and SD)\nof segment length T using the EEGformer and other comparable\nmodels on the three EEG datasets are shown in Figure 5. The\nﬁgure shows that our model achieves the best accuracy rate across\nthe three datasets. For other comparison baseline models, the\nmodel performance reduces in some cases if the segment length\nT exceeds 1.2 s. The model performance of the EEGformer on\nthe three datasets showed an increasing trend as the segment\nlength T increases, indicating that our method can extract inherent\ntemporal information from EEG and is unaﬀected by segment\nlength. In addition, the model performance of 4DRCNN and\nEmotionNet outperforms the performance of other comparison\nbaselines. Because 4DRCNN and the EmotionNet are models that\nlearn spatiotemporal features simultaneously, this operation may\nfacilitate the DL model to learn better feature representation of EEG\nregional and synchronous characteristics.\nFrontiers in Neuroscience 10 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 11\nWan et al. 10.3389/fnins.2023.1148855\n4. Discussion\nThe abovementioned ablation and comparison studies show the\nrationality of our EEGformer architecture and demonstrate that\nour model performs outperforms other comparison baselines. This\nsection covers several noteworthy points and future works:\n(1) The uniﬁed manner, sequentially maps an input sequence\ninto an abstract continuous representation that holds\ntemporal, convolutional, and spatial information of that\ninput outperforms the 2D and 3D structures that integrate\nfrequency, spatial and temporal information of EEG. The\nEEGformer achieved the highest accuracy rate compared\nwith other comparison baselines, which could be due to\nthe uniﬁed EEG characteristics learning manner. Compared\nwith 4DRCNN, which requires the user to manually\nextract frequency information from raw EEG data and\nuse it as model input, our model is an end-to-end deep\nmethod because it uses depth-wise 1DCNN to learn the\nfeature in an EEG-channel-wise manner. In the EEGformer\nencoder, we sequentially encode the convolutional results\ngenerated by the 1DCNN from temporal, convolutional, and\nspatial dimensions. The temporal, regional, and synchronous\ntransformers were responsible for learning the temporal,\nregional, and synchronous characteristics of EEG signals.\nThis type of feature learning strategy contains more cues of\nEEG characteristics than other model structures and performs\nbetter than them.\n(2) EEG signals are well-known to exhibit data statistics that\ncan drastically change from one subject to another in\nvarious aspects (e.g., regional characteristics), but also\nshare similarities in certain other aspects (e.g., synchronous\ncharacteristics). To exploit the commonalities while tackling\nvariations, we require a large data sample to train the\nmodel and improve its generalization ability. However, the\nperformance of a DL model is always aﬀected by the\ndataset size. Compare with the dataset size in the computer\nvision studies, researchers ﬁnd it diﬃcult to collect a dataset\nwith a similar size in EEG-based clinical studies. Therefore,\nincreasing the number of EEG datasets used for training DL\nmodels is crucial to reduce the inﬂuence of small dataset size\non model performance. To this end, many studies separate the\nEEG signal collected in a trial into several segments and label\nthem with the same label. Those segments were then used\nin cross-subject and within-subject classiﬁcations, which are\ntwo commonly used experimental designs, to execute model\ntraining and validate model performance. Meanwhile, those\nstudies also designed model training strategies to improve the\nmodel generalization ability. For instance, Guney et al. (2021)\ntrained their model in two stages: the ﬁrst stage trains globally\nwith all the available data from all the subjects, and then the\nsecond stage ﬁne-tunes the model individually using the data\nof each subject separately. In the future, we can also design a\ntraining strategy to reduce the inﬂuence of small dataset size\non model performance.\n(3) Although the experimental results demonstrated that learning\ntemporal, regional, and spatial characteristics in a uniﬁed\nmanner facilitates the EEGformer to achieve promising\nclassiﬁcation performance across three EEG datasets, this\nresult might be unable to provide strong support for clinical\ntreatment that is associated with EEG biomarkers. Because\nDL methods are essentially considered black boxes, we require\nnovel methods to open the box and visualize the feature\nlearned by the DL model. To this end, an emerging technique\nknown as explainable artiﬁcial intelligence (AI) enables the\nunderstanding of how DL methods work and what drives their\ndecision-making. The competitive model performance of DL\nmethods and the explainable AI provided a promising way to\nsupport eﬀective EEG-based brain activity analysis. By using\nthe explainable AI method, we could visualize the form of the\ntemporal, regional, and spatial characteristics learned by the\nEEGformer and use it to connect with BFC, as well as perform\nbrain activity analysis.\n5. Conclusion\nIn this study, we proposed a transformerñbased EEG analysis\nmodel known as EEGformer to capture EEG characteristics\nin a uniﬁed manner. The EEGformer consists of 1DCNN,\nan EEGformer encoder (sequentially constructed by three\ncomponents: regional, synchronous, and temporal transformers),\nand an EEGformer decoder. We conducted ablation studies to\ndemonstrate the rationality of the EEG former. The results not\nonly supported our hypothesis that a machine learning method\ncapable of capturing the EEG characteristics in a uniﬁed manner\ncan be applied to EEG-based brain activity analysis tasks but\nalso demonstrated that convolutional features could accurately\nrepresent regional and spatial characteristics of EEG signals. The\nLOSO cross-validation method is utilized to compare the model\nperformance between EEGformer and other ﬁve comparison\nmethods, the result shows the proposed method generalizes well\non unseen data and potentially requires little to model training\nand calibration for new users, suitable for SSVEP classiﬁcation\ntasks. We also investigate the impact of segment lengthT on model\nperformance, and the results show that our method can extract\ninherent temporal information from EEG and is unaﬀected by\nthe segment length. The proposed EEGformer outperforms the\ncomparison models, which perform well in other studies on the\nthree EEG datasets.\nData availability statement\nPublicly available datasets were analyzed in this study. This data\ncan be found here: http://bci.med.tsinghua.edu.cn/download.html\nand https://bcmi.sjtu.edu.cn/home/seed/seed.html.\nEthics statement\nThe studies involving human participants were reviewed and\napproved by the Institutional Review Board of Beijing Anding\nHospital of Capital Medical University. The patients/participants\nprovided their written informed consent to participate in the\nFrontiers in Neuroscience 11 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 12\nWan et al. 10.3389/fnins.2023.1148855\ndata collection. Written informed consent was obtained from the\nindividual(s) for the publication of any potentially identiﬁable data\nincluded in this article.\nAuthor contributions\nZW, ML, and WD contributed to the conception and design\nof the study. SL and ML performed the data analysis. ZW and\nWD drafted the manuscript. JH and HT participated in editing the\nmanuscript. All authors contributed to the article and approved the\nsubmitted version.\nFunding\nThis work was supported in part by the National\nNatural Science Foundation of China (NSFC) under Grant\n(62161024), China Postdoctoral Science Foundation under Grant\n(2021TQ0136 and 2022M711463), and the State Key Laboratory\nof Computer Architecture (ICT, CAS) Open Project under\nGrant (CARCHB202019).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAbdull, M. M., Chandler, C., and Gilbert, C. J. (2016). Glaucoma,“the silent thief of\nsight”: Patients’ perspectives and health seeking behaviour in Bauchi, northern Nigeria.\nBMC Ophthalmol.16:44. doi: 10.1186/s12886-016-0220-6\nChen, J., Zhang, Y., Pan, Y., Xu, P., and Guan, C. (2022). A Transformer-based deep\nneural network model for SSVEP classiﬁcation. arXiv [Preprint]. arXiv:2210.04172.\nDu, Y., Xu, Y., Wang, X., Liu, L., and Ma, P. (2022). EEG temporalñspatial\ntransformer for person identiﬁcation. Sci. Rep.12:14378.\nDuan, R. N., Zhu, J. Y., and Lu, B. L. (2013). “Diﬀerential entropy feature for\nEEG-based emotion classiﬁcation, ” in Proceedings of the international IEEE/EMBS\nconference on neural engineering (San Diego, CA), 81ñ84. doi: 10.1109/NER.2013.\n6695876\nGuedes, R. A. P. (2021). Glaucoma, collective health and social impact. Rev.\nBras.Oftalmol. 05ñ07. doi: 10.5935/0034-7280.20210001\nGuney, O. B., Oblokulov, M., and Ozkan, H. J. (2021). A deep neural network\nfor ssvep-based brain-computer interfaces. IEEE Trans. Biomed. Eng.69, 932ñ944.\ndoi: 10.1109/TBME.2021.3110440\nIbáñez-Soria, D., Soria-Frisch, A., Garcia-Ojalvo, J., and Ruﬃni, G. (2019).\nCharacterization of the non-stationary nature of steady-state visual evoked potentials\nusing echo state networks. PLoS One14:e0218771. doi: 10.1371/journal.pone.0218771\nKhok, H. J., Koh, V. T., and Guan, C. (2020). “Deep multi-task learning for SSVEP\ndetection and visual response mapping, ” in Proceedings of the IEEE international\nconference on systems, man, and cybernetics (SMC), (Toronto, ON), 1280ñ1285. doi:\n10.1109/SMC42975.2020.9283310\nLawhern, V. J., Solon, A. J., Waytowich, N. R., Gordon, S. M., Hung, C. P., and\nLance, B. J. (2018). EEGNet: a compact convolutional neural network for EEG-\nbased brainñcomputer interfaces.J. Neural Eng.15:056013. doi: 10.1088/1741-2552/aa\nce8c\nLi, X., Wei, W., Qiu, S., and He, H. (2022). “TFF-Former: Temporal-frequency\nfusion transformer for zero-training decoding of two BCI tasks, ” in Proceedings of\nthe 30th ACM international conference on multimedia, (Lisboa), 51ñ59. doi: 10.1145/\n3503161.3548269\nLi, Y., Xiang, J., and Kesavadas, T. J. (2020). Convolutional correlation analysis for\nenhancing the performance of SSVEP-based brain-computer interface. IEEE Trans.\nNeural Syst. Rehabil. Eng.28, 2681ñ2690. doi: 10.1109/TNSRE.2020.3038718\nLi, Z., Wang, Q., Zhang, S. F., Huang, Y. F., and Wang, L. Q. (2022). Timing of\nglaucoma treatment in patients with MICOF: A retrospective clinical study. Front.\nMed. 9:986176. doi: 10.3389/fmed.2022.986176\nLiu, B., Huang, X., Wang, Y., Chen, X., and Gao, X. J. (2020). BETA: A large\nbenchmark database toward SSVEP-BCI application. Front. Neurosci. 14:627. doi:\n10.3389/fnins.2020.00627\nNentwich, M., Ai, L., Madsen, J., Telesford, Q. K., Haufe, S., Milham, M. P., et al.\n(2020). Functional connectivity of EEG is subject-speciﬁc, associated with phenotype,\nand diﬀerent from fMRI. Neuroimage 218:117001. doi: 10.1016/j.neuroimage.2020.\n117001\nQin, K., Wang, R., and Zhang, Y. (2021). Filter bank-driven multivariate\nsynchronization index for training-free SSVEP BCI. IEEE Trans. Neural Syst. Rehabil.\nEng. 29, 934ñ943. doi: 10.1109/TNSRE.2021.3073165\nRaut, R. V., Snyder, A. Z., Mitra, A., Yellin, D., Fujii, N., Malach, R., et al. (2021).\nGlobal waves synchronize the brain’s functional systems with ﬂuctuating arousal. Sci.\nAdv. 7:eabf2709. doi: 10.1126/sciadv.abf2709\nSchielke, A., and Krekelberg, B. (2022). Steady state visual evoked potentials in\nschizophrenia: A review. Front. Neurosci.16:988077. doi: 10.3389/fnins.2022.988077\nShen, F., Dai, G., Lin, G., Zhang, J., Kong, W., and Zeng, H. (2020). EEG-\nbased emotion recognition using 4D convolutional recurrent neural network. Cogn.\nNeurodyn. 14, 815ñ828. doi: 10.1007/s11571-020-09634-1\nTsoneva, T., Garcia-Molina, G., and Desain, P. (2021). SSVEP phase synchronies and\npropagation during repetitive visual stimulation at high frequencies. Sci. Rep.11:4975.\ndoi: 10.1038/s41598-021-83795-9\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., et al. (2017).\nAttention is all you need. Adv. Neural Inf. Process. Syst.30:15. doi: 10.48550/arXiv.\n1706.03762\nWan, Z., Huang, J., Zhang, H., Zhou, H., Y ang, J., and Zhong, N. (2020).\nHybridEEGNet: A convolutional neural network for EEG feature learning and\ndepression discrimination. IEEE Access8, 30332ñ30342. doi: 10.1109/ACCESS.2020.\n2971656\nWang, Y., Huang, Z., McCane, B., and Neo, P. (2018). “EmotioNet: A 3-D\nConvolutional Neural Network for EEG-based Emotion Recognition, ” in Proceedings\nof the 2018 International Joint Conference on Neural Networks (IJCNN)(Rio de Janeiro,\nBrazil), 1ñ7. doi: 10.1109/IJCNN.2018.8489715\nWaytowich, N., Lawhern, V. J., Garcia, J. O., Cummings, J., Faller, J., Sajda, P., et al.\n(2018). Compact convolutional neural networks for classiﬁcation of asynchronous\nsteady-state visual evoked potentials. J. Neural Eng.15, 066031. doi: 10.1088/1741-\n2552/aae5d8\nY ang, Y., Wu, Q., Qiu, M., Wang, Y., and Chen, X. (2018). “Emotion\nrecognition from multi-channel EEG through parallel convolutional recurrent\nneural network, ” in Proceedings of the international joint conference on\nneural networks , (Rio de Janeiro: IEEE), 1ñ7. doi: 10.1109/IJCNN.2018.848\n9331\nZhang, X., Y ao, L., Wang, X., Monaghan, J., McAlpine, D., and Zhang, Y. (2021). A\nsurvey on deep learning-based non-invasive brain signals: Recent advances and new\nfrontiers. J. Neural Eng.18:031002. doi: 10.1088/1741-2552/abc902\nFrontiers in Neuroscience 12 frontiersin.org\nfnins-17-1148855 March 20, 2023 Time: 14:36 # 13\nWan et al. 10.3389/fnins.2023.1148855\nZhang, Y., Xie, S. Q., Wang, H., and Zhang, Z. J. (2020). Data analytics in steady-\nstate visual evoked potential-based brainñcomputer interface: A review. IEEE Sens. J.\n21, 1124ñ1138. doi: 10.1109/JSEN.2020.3017491\nZhang, Y., Yin, E., Li, F., Zhang, Y., Guo, D., Y ao, D., et al. (2019). Hierarchical\nfeature fusion framework for frequency recognition in SSVEP-based BCIs. Neural\nNetw. 119, 1ñ9. doi: 10.1016/j.neunet.2019.07.007\nZheng, W. L., and Lu, B. L. (2015). Investigating critical frequency bands\nand channels for EEG-based emotion recognition with deep neural networks.\nIEEE Trans. Auton. Ment. Dev. 7, 162ñ175. doi: 10.1109/TAMD.2015.243\n1497\nZhou, Y., He, S., Huang, Q., and Li, Y. J. (2020). A hybrid asynchronous brain-\ncomputer interface combining SSVEP and EOG signals. IEEE Trans. Biomed Eng.67,\n2881ñ2892. doi: 10.1109/TBME.2020.2972747\nZhuang, X., Y ang, Z., and Cordes, D. J. (2020). A technical review of canonical\ncorrelation analysis for neuroscience applications. Hum. Brain Mapp.41, 3807ñ3833.\ndoi: 10.1002/hbm.25090\nFrontiers in Neuroscience 13 frontiersin.org",
  "topic": "Electroencephalography",
  "concepts": [
    {
      "name": "Electroencephalography",
      "score": 0.8518103361129761
    },
    {
      "name": "Computer science",
      "score": 0.7886689901351929
    },
    {
      "name": "Brain–computer interface",
      "score": 0.746103048324585
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5832738280296326
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.508878231048584
    },
    {
      "name": "Transformer",
      "score": 0.43507814407348633
    },
    {
      "name": "Speech recognition",
      "score": 0.43236905336380005
    },
    {
      "name": "Brain activity and meditation",
      "score": 0.4209243059158325
    },
    {
      "name": "Machine learning",
      "score": 0.37954747676849365
    },
    {
      "name": "Neuroscience",
      "score": 0.09231230616569519
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}