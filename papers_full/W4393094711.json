{
    "title": "Artificial intelligence large language model ChatGPT: is it a trustworthy and reliable source of information for sarcoma patients?",
    "url": "https://openalex.org/W4393094711",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5111262196",
            "name": "Marisa Valentini",
            "affiliations": [
                "Medical University of Graz"
            ]
        },
        {
            "id": "https://openalex.org/A2044852609",
            "name": "Joanna Szkandera",
            "affiliations": [
                "Medical University of Graz"
            ]
        },
        {
            "id": "https://openalex.org/A2128547733",
            "name": "Maria Anna Smolle",
            "affiliations": [
                "Medical University of Graz"
            ]
        },
        {
            "id": "https://openalex.org/A2063634308",
            "name": "Susanne Scheipl",
            "affiliations": [
                "Medical University of Graz"
            ]
        },
        {
            "id": "https://openalex.org/A1882166381",
            "name": "Andreas Leithner",
            "affiliations": [
                "Medical University of Graz"
            ]
        },
        {
            "id": "https://openalex.org/A2303142729",
            "name": "Dimosthenis Andreou",
            "affiliations": [
                "Medical University of Graz"
            ]
        },
        {
            "id": "https://openalex.org/A5111262196",
            "name": "Marisa Valentini",
            "affiliations": [
                "Medical University of Graz"
            ]
        },
        {
            "id": "https://openalex.org/A2044852609",
            "name": "Joanna Szkandera",
            "affiliations": [
                "Medical University of Graz"
            ]
        },
        {
            "id": "https://openalex.org/A2128547733",
            "name": "Maria Anna Smolle",
            "affiliations": [
                "Medical University of Graz"
            ]
        },
        {
            "id": "https://openalex.org/A2063634308",
            "name": "Susanne Scheipl",
            "affiliations": [
                "Medical University of Graz"
            ]
        },
        {
            "id": "https://openalex.org/A1882166381",
            "name": "Andreas Leithner",
            "affiliations": [
                "Medical University of Graz"
            ]
        },
        {
            "id": "https://openalex.org/A2303142729",
            "name": "Dimosthenis Andreou",
            "affiliations": [
                "Medical University of Graz"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4244970282",
        "https://openalex.org/W2932727825",
        "https://openalex.org/W4211038654",
        "https://openalex.org/W3080385769",
        "https://openalex.org/W2900218258",
        "https://openalex.org/W3197355167",
        "https://openalex.org/W4363650777",
        "https://openalex.org/W4324304837",
        "https://openalex.org/W4321499561",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4322743583",
        "https://openalex.org/W4377942506",
        "https://openalex.org/W4367310920",
        "https://openalex.org/W4322622443",
        "https://openalex.org/W4381988571",
        "https://openalex.org/W4362641141",
        "https://openalex.org/W4387636935",
        "https://openalex.org/W4377563830",
        "https://openalex.org/W4379599010",
        "https://openalex.org/W4368372176",
        "https://openalex.org/W1814681724",
        "https://openalex.org/W5947646",
        "https://openalex.org/W3006799932",
        "https://openalex.org/W2088303301",
        "https://openalex.org/W772589830"
    ],
    "abstract": "Introduction Since its introduction in November 2022, the artificial intelligence large language model ChatGPT has taken the world by storm. Among other applications it can be used by patients as a source of information on diseases and their treatments. However, little is known about the quality of the sarcoma-related information ChatGPT provides. We therefore aimed at analyzing how sarcoma experts evaluate the quality of ChatGPT’s responses on sarcoma-related inquiries and assess the bot’s answers in specific evaluation metrics. Methods The ChatGPT responses to a sample of 25 sarcoma-related questions (5 definitions, 9 general questions, and 11 treatment-related inquiries) were evaluated by 3 independent sarcoma experts. Each response was compared with authoritative resources and international guidelines and graded on 5 different metrics using a 5-point Likert scale: completeness, misleadingness, accuracy, being up-to-date, and appropriateness. This resulted in maximum 25 and minimum 5 points per answer, with higher scores indicating a higher response quality. Scores ≥21 points were rated as very good, between 16 and 20 as good, while scores ≤15 points were classified as poor (11–15) and very poor (≤10). Results The median score that ChatGPT’s answers achieved was 18.3 points (IQR, i.e., Inter-Quartile Range, 12.3–20.3 points). Six answers were classified as very good, 9 as good, while 5 answers each were rated as poor and very poor. The best scores were documented in the evaluation of how appropriate the response was for patients (median, 3.7 points; IQR, 2.5–4.2 points), which were significantly higher compared to the accuracy scores (median, 3.3 points; IQR, 2.0–4.2 points; p = 0.035). ChatGPT fared considerably worse with treatment-related questions, with only 45% of its responses classified as good or very good, compared to general questions (78% of responses good/very good) and definitions (60% of responses good/very good). Discussion The answers ChatGPT provided on a rare disease, such as sarcoma, were found to be of very inconsistent quality, with some answers being classified as very good and others as very poor. Sarcoma physicians should be aware of the risks of misinformation that ChatGPT poses and advise their patients accordingly.",
    "full_text": "Frontiers in Public Health 01 frontiersin.org\nArtificial intelligence large \nlanguage model ChatGPT: is it a \ntrustworthy and reliable source of \ninformation for sarcoma patients?\nMarisa Valentini 1, Joanna Szkandera 2, Maria Anna Smolle 1, \nSusanne Scheipl 1, Andreas Leithner 1 and Dimosthenis Andreou 1*\n1 Department of Orthopaedics and Trauma, Medical University of Graz, Graz, Austria, 2 Division of \nOncology, Department of Internal Medicine, Medical University of Graz, Graz, Austria\nIntroduction: Since its introduction in November 2022, the artificial intelligence \nlarge language model ChatGPT has taken the world by storm. Among other \napplications it can be used by patients as a source of information on diseases \nand their treatments. However, little is known about the quality of the sarcoma-\nrelated information ChatGPT provides. We  therefore aimed at analyzing how \nsarcoma experts evaluate the quality of ChatGPT’s responses on sarcoma-\nrelated inquiries and assess the bot’s answers in specific evaluation metrics.\nMethods: The ChatGPT responses to a sample of 25 sarcoma-related questions \n(5 definitions, 9 general questions, and 11 treatment-related inquiries) were \nevaluated by 3 independent sarcoma experts. Each response was compared \nwith authoritative resources and international guidelines and graded on 5 \ndifferent metrics using a 5-point Likert scale: completeness, misleadingness, \naccuracy, being up-to-date, and appropriateness. This resulted in maximum \n25 and minimum 5 points per answer, with higher scores indicating a higher \nresponse quality. Scores ≥21 points were rated as very good, between 16 and 20 \nas good, while scores ≤15 points were classified as poor (11–15) and very poor \n(≤10).\nResults: The median score that ChatGPT’s answers achieved was 18.3 points \n(IQR, i.e., Inter-Quartile Range, 12.3–20.3 points). Six answers were classified as \nvery good, 9 as good, while 5 answers each were rated as poor and very poor. \nThe best scores were documented in the evaluation of how appropriate the \nresponse was for patients (median, 3.7 points; IQR, 2.5–4.2 points), which were \nsignificantly higher compared to the accuracy scores (median, 3.3 points; IQR, \n2.0–4.2 points; p =  0.035). ChatGPT fared considerably worse with treatment-\nrelated questions, with only 45% of its responses classified as good or very \ngood, compared to general questions (78% of responses good/very good) and \ndefinitions (60% of responses good/very good).\nDiscussion: The answers ChatGPT provided on a rare disease, such as sarcoma, \nwere found to be of very inconsistent quality, with some answers being classified \nas very good and others as very poor. Sarcoma physicians should be  aware \nof the risks of misinformation that ChatGPT poses and advise their patients \naccordingly.\nKEYWORDS\nartificial intelligence, ChatGPT, sarcoma, patient information, information quality\nOPEN ACCESS\nEDITED BY\nUlises Cortés,  \nUniversitat Politecnica de Catalunya, Spain\nREVIEWED BY\nJames C. L. Chow,  \nUniversity of Toronto, Canada\nShailesh Tripathi,  \nRajendra Institute of Medical Sciences, India\nSumeet Patiyal,  \nNational Cancer Institute (NIH), United States\n*CORRESPONDENCE\nDimosthenis Andreou  \n dimosthenis.andreou@medunigraz.at\nRECEIVED 30 September 2023\nACCEPTED 06 March 2024\nPUBLISHED 22 March 2024\nCITATION\nValentini M, Szkandera J, Smolle MA, \nScheipl S, Leithner A and Andreou D (2024) \nArtificial intelligence large language model \nChatGPT: is it a trustworthy and reliable \nsource of information for sarcoma patients?\nFront. Public Health 12:1303319.\ndoi: 10.3389/fpubh.2024.1303319\nCOPYRIGHT\n© 2024 Valentini, Szkandera, Smolle, Scheipl, \nLeithner and Andreou. This is an open-access \narticle distributed under the terms of the \nCreative Commons Attribution License \n(CC BY). The use, distribution or reproduction \nin other forums is permitted, provided the \noriginal author(s) and the copyright owner(s) \nare credited and that the original publication \nin this journal is cited, in accordance with \naccepted academic practice. No use, \ndistribution or reproduction is permitted \nwhich does not comply with these terms.\nTYPE Original Research\nPUBLISHED 22 March 2024\nDOI 10.3389/fpubh.2024.1303319\nValentini et al. 10.3389/fpubh.2024.1303319\nFrontiers in Public Health 02 frontiersin.org\nIntroduction\nSarcomas are a heterogeneous group of rare malignant tumors, \naccounting for merely 1% of all cancer diagnoses (1). Their overall \nincidence is estimated at approximately 7.1–7.4 per 100,000 patients \nper year ( 1, 2). Due to their rarity and complexity, international \nguidelines recommend a multidisciplinary diagnostic and therapeutic \napproach at specialized sarcoma centers (3–6). Finding accurate and \nreliable information can be challenging for patients, caregivers, and \nhealthcare professionals who are not specialized in sarcoma \ntreatment (7).\nSince its introduction in November 2022, the Artificial Intelligence \n(AI) Chat Generative Pre-trained Transformer (ChatGPT) has taken \nthe world by storm ( 8–12). ChatGPT is a 175-billion-parameter \nnatural language processing model (GPT 3.5), able to generate \nconversation-style responses to user input (10). As a large language \nmodel trained on a massive dataset of text and data available online, \nit is able to generate responses to a wide range of questions (13). Due \nto its very nature, the artificial intelligence chatbot can address an \nalmost limitless range of inquiries, but it is not capable of verifying the \naccuracy of its responses and may not provide the most up-to-date or \ncomprehensive information. Among other applications, it has been \nused and will likely be increasingly used by patients as a source of \ninformation on diseases and their treatments, but its potential to \ngenerate inaccurate or false information is a major cause for concern \n(14–18). Previous studies have shown that the bot may be a useful \nsource of information for common rheumatic diseases ( 19) and \nprovide more empathetic answers to general public questions \ncompared to physicians (14), but also demonstrated that the quality \nof the bot’s responses is worse when confronted with more complex \nmedical questions (20, 21).\nVery little is known about the quality of the sarcoma-related \ninformation ChatGPT provides. The authors chose to focus on this \nrare group of tumors, as their complexity and the lack of safe online \ninformation on this topic are well known and a cause for concern \n(22–24). Therefore, we aimed at evaluating how complete, misleading, \naccurate, up-to-date, and appropriate the Open AI chatbot’s answers \nto sarcoma-related inquiries are, assessing the quality of the \ninformation it imparts. Specifically, we analyzed how sarcoma experts \nevaluate the quality of ChatGPT’s responses on sarcoma-related \ninquiries, how the bot’s responses perform in specific metrics of the \nevaluation, and if ChatGPT fares better with a specific type \nof questions.\nMaterials and methods\nA sample of 25 representative sarcoma-related questions were \nposed to ChatGPT ( ChatGPT 3.5 free version ) (Table 1). These \nincluded 5 definitions (e.g., what is a tenosynovial giant cell \ntumor?), 9 general questions (e.g., which imaging modalities are \nbest in follow-up after treatment of soft tissue sarcoma, or what \nTABLE 1 The 25 sarcoma-related questions which were posed to ChatGPT.\nN° Questions\n1 What is the optimal treatment of a desmoid tumor?\n2 What is the optimal treatment of Ewing sarcoma?\n3 What are the most helpful chemotherapeutic agents in the treatment of Ewing sarcoma?\n4 Is follow-up necessary after treatment of soft tissue sarcoma?\n5 Which imaging modalities are best in follow-up after treatment of soft tissue sarcoma?\n6 What is the preferred surgical treatment for clear cell chondrosarcoma?\n7 What is the optimal treatment of a retroperitoneal liposarcoma?\n8 Is preoperative radiotherapy better than postoperative radiotherapy in patients with myxoid liposarcoma?\n9 What late effects are possible after successful multidisciplinary treatment of osteosarcoma?\n10 How can I enroll in a clinical trial for Ewing sarcoma?\n11 Which clinical trials are available for Ewing sarcoma in Germany?\n12 What is a biopsy for Ewing sarcoma?\n13 What is the difference between enchondromas and atypical cartillaginous tumors?\n14 What are common side effects of chemotherapy for Ewing sarcoma?\n15 What is the difference between a lipoma and an atypical lipomatous tumor?\n16 What is a tenosynovial giant cell tumor?\n17 I have a Ewing sarcoma of the upper thigh bone. What is my prognosis?\n18 Is an allograft-prosthetic-composite better than a megaprosthesis for an osteosarcoma of the proximal tibia?\n19 What is a rotationplasty?\n20 Is rotationplasty better or worse than above-knee amputation for osteosarcoma?\n21 When is postoperative radiotherapy recommended for Ewing sarcoma?\n22 When is postoperative radiotherapy recommended for osteosarcoma?\n23 What are the advantages and disadvantages of preoperative denosumab treatment for giant cell tumor of bone?\n24 What functional outcome can be expected after proximal humerus replacement with megaprosthesis for osteosarcoma?\n25 What is the best treatment for gastrointestinal stromal tumors?\nValentini et al. 10.3389/fpubh.2024.1303319\nFrontiers in Public Health 03 frontiersin.org\nare common side effects of chemotherapy for Ewing sarcoma?), \nand 11 treatment-related inquiries (e.g., what is the optimal \ntreatment of a desmoid tumor?). Three sarcoma experts (2 \northopedic oncologists and 1 medical oncologist) evaluated the \nartificial intelligence chatbot’s responses, comparing them to \ninternational guidelines and authoritative resources. The \nevaluation was performed independently by and without contact \nbetween these experts. Each response was graded with regards to \n5 different aspects/evaluation metrics, using a 5-point Likert \nscale: completeness, misleadingness, accuracy (i.e., whether the \nresponse contained relevant factual errors), being up-to-date, and \nappropriateness (i.e., whether it’ d be a good source of information \nfor patients) ( Table 2). This resulted in a maximum of 25 and a \nminimum of 5 points per answer, with higher scores indicating \nhigher quality of the ChatGPT response. Scores ≥21 were defined \nas very good, between 16 and 20 points as good, while responses \nthat scored less than 15 points were classified as poor (11–15) \nand very poor ( ≤10).\nAn approval from our local ethic committee was not required, as \nthe study did not involve human subjects.\nStatistical analyses were performed with Stata Version 16.1 \nfor Mac ( StataCorp, College Station, TX, US ). Continuous \nvariables were checked for normality with the Shapiro–Wilk test. \nMedian values with the respective Inter-Quartile Ranges (IQR) \nwere reported for non-normally distributed variables. The values \nof different aspects of a ChatGPT response were compared with \nthe Wilcoxon signed-rank test. The overall scores of the responses \nin the three pre-defined categories (definitions, general questions, \nand treatment-related inquiries) were compared with Kruskal-\nWallis and post-hoc Dunn tests. A p-value of <0.05 was \nconsidered significant.\nResults\nThe ChatGPT responses achieved a median score of 18.3 points \n(IQR, 12.3–20.3 points). The individual scores of each of the 5 \nevaluated aspects amounted to a median of 3.5 points (IQR, 2.4–4.2 \npoints). Six of the 25 responses (24%) were classified as very good, \n9/25 (36%) as good, while 5/25 answers each (20%) were defined as \npoor and very poor, respectively (Figure 1).\nConcerning the 5 evaluated aspects, the best scores ( Figure 2) \nwere recorded in the evaluation metric of how appropriate the \nresponse was for patients (median, 3.7 points; IQR, 2.5–4.2 points), \nwhich were significantly higher compared to the accuracy scores \n(median, 3.3 points; IQR, 2.0–4.2 points; p = 0.035). On the other \nhand, with the numbers we had the differences between the accuracy \nand completeness scores (median, 3.5 points; IQR, 2.8–4.0; p = 0.066) \ndid not reach statistical significance. The remaining comparisons \nbetween the evaluation metrics showed no statistically \nsignificant differences.\nAs for the 3 categories of questions, ChatGPT fared best with general \ninquiries, achieving good and very good overall scores in 3/9 (33%) and \n4/9 (44%) questions, respectively. Only 1/9 (11%) response each was \nrated as poor and very poor, respectively (Figure 3). On the other hand, \nthe bot fared considerably worse on treatment-related questions, \nachieving good and very good overall scores in 3/11 (27%) and 2/11 \n(18%), respectively. 3/11 (27%) responses each were classified as poor \nand very poor, respectively (Figure  4). However, with the numbers \navailable for this analysis, these differences did not reach statistical \nsignificance (p = 0.063). Finally, the bot’s responses on definitions ranged \nTABLE 2 The aspects of each ChatGPT response that were evaluated using a 5-point Likert scale.\nEvaluated aspects Score\n1 Is the provided information complete? 5 strongly agree 4 agree 3 neutral 2 disagree 1 strongly disagree\n2 Is the provided answer misleading? 1 strongly agree 2 agree 3 neutral 4 disagree 5 strongly disagree\n3 Are there relevant factual errors in the provided information? 1 strongly agree 2 agree 3 neutral 4 disagree 5 strongly disagree\n4 Is the provided information up to date? 5 strongly agree 4 agree 3 neutral 2 disagree 1 strongly disagree\n5 Is the provided answer a good source of information for patients? 5 strongly agree 4 agree 3 neutral 2 disagree 1 strongly disagree\nFIGURE 1\nThis figure depicts the evaluation of the quality of ChatGPT \nresponses by sarcoma experts. The percentage value in each bar is \nbased on the total number of questions (25).\nFIGURE 2\nThe graph shows the scores that the ChatGPT responses achieved in \neach specific metric of the evaluation. On the X axis the individual \nmetrics are presented as A (completeness), B (misleadingness), C \n(accuracy), D (being up-to-date), and E (appropriateness). The Y axis \nshows the score per aspect on a 5-point Likert scale with the \nrespective medians and IQRs.\nValentini et al. 10.3389/fpubh.2024.1303319\nFrontiers in Public Health 04 frontiersin.org\nbetter than treatment-related replies; 60% of the ChatGPT responses \nwere classified as good (2/5, 40%) or very good (1/5, 20%), while 1/5 \n(20%) response each was classified as poor and very poor, respectively. \nNo statistical significance was detected in this case as well.\nDiscussion\nBased on the extraordinary popularity the artificial intelligence bot \nChatGPT achieved in only a few months, it is expected to quickly \nbecome an everyday health information source for patients (8, 14–17). \nHowever, little is known about the quality of information it can provide \nregarding rare diseases, such as sarcoma. Our study demonstrated that \nthe responses provided by ChatGPT to sarcoma-related questions were \nvery inconsistent in quality, ranging from very good to very poor ones. \nThe responses scored better in the metric of appropriateness for patients \nand worse in their accuracy, while the bot generally fared better with \ngeneral questions and worse with specific treatment-related inquiries.\nWe acknowledge that our study has several limitations. First of all, \ngiven the variety in presentation, prognosis, and treatment of bone and \nsoft tissue sarcomas, our sample of 25 questions cannot be expected to \ncover all aspects of these rare diseases. However, we deliberately opted for \na relatively small sample to avoid a bloated analysis, while the individual \nquestions were carefully chosen based on our clinical experience to \nbe representative of the wide range of questions patients, relatives, or \ncaregivers might ask the Open AI chatbot. Another possible limitation of \nour study is that ChatGPT 3.5 was used. This free version of the AI model \nwas trained on a massive dataset of information before its release in \nNovember 2022 and does not undergo regular updates. A newer GPT 4 \nmodel was released in March 2023. Its enhanced capabilities include \nbeing a multimodal model, taking also images as input, and the ability to \ninteract with external interfaces. On the other hand, it needs to \nbe considered that ChatGPT has 180.5 million active users, but only an \nestimated 1% subscribe to “ChatGPT Plus” (giving access to the GPT 4 \nmodel for 20$/month).1 This aspect of the accessibility and actual use of \nthe GPT 4 (paid version) is of great importance. Most patients with \nsarcoma will most likely access the free version (ChatGPT 3.5) to seek \ninformation. Therefore, the authors believe that this study’s results are \nrelevant as they are based on the ChatGPT version that most patients and \ntheir relatives will actually use. As such, our results reflect the information \nmost patients will receive through the free model. Furthermore, it is not \nguaranteed that the GPT 4 model provides more accurate information in \na rare disease, such as sarcoma, taking into consideration the long-known \nproblem of inaccurate, outdated, and misleading sarcoma information \neven in reputable online sources (22–24).\nThe overall quality of ChatGPT responses on sarcoma-related \ninquiries in our study varied from very good to very poor. This \nvariability harbors a great risk for patients in case they use ChatGPT as \nan information source. If the first ChatGPT responses to patient queries \nhappen to be similar in quality and context as those provided by the \ntreating physicians, patients would likely deem the bot to be trustworthy, \nwithout realizing that further answers might be of inferior or even very \npoor quality. Chow et al. (17) pointed out similar concerns regarding \nChatGPT’s use as a medical chatbot: as it draws information from the \ninternet, this “disruptive technology” can cause for “questionable and \nuncontrollable” accuracy and currency of medical information. \nContrary to our findings on sarcoma-related responses, Uz and Umay \nrecently evaluated the responses of ChatGPT to frequently searched \nkeywords relating to common rheumatic diseases and found them to \nbe a reliable and useful source of information for patients (19).\nA possible reason for this discrepancy is the rarity of sarcomas, \ncompared to the relatively high prevalence of rheumatic disorders. \nGiven that ChatGPT is trained on massive datasets of online available \ninformation, incomplete, erroneous, or outdated online data on a \nspecific topic would lead to poorer bot responses. Zade et  al. ( 24) \npreviously analyzed the quality of online resources for orthopedic \noncology in 48 websites and found a general lack of quality and accuracy, \nan issue that has been reported by other studies as well (23). As such, it \nappears unlikely that ChatGPT will be able to consistently provide high-\nquality responses to sarcoma-related queries in the foreseeable future.\nOur evaluation of different parameters of ChatGPT’s responses \ndemonstrated that the bot achieved its worse scores in the accuracy \nmetric, a finding well in-line with the previously mentioned weaknesses \nof the artificial intelligence’s sources on a rare disease like sarcoma.\nOn the other hand, its best scores in our study were documented \nin the metric “appropriateness for patients. ” Our results are in line with \nthe findings of Ayers et  al. ( 14), who performed a blinded study \ncomparing physicians’ and ChatGPT’s responses on public questions \n1 https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4; \nhttps://nerdynav.com/gpt-4-statistics-facts/#how-to-access-gpt4-free-and-paid- \nmethods\nFIGURE 3\nThis figure shows the evaluation of the quality of ChatGPT responses \nto general questions. The percentage value in each bar is based on \nthe total number of questions in this category (9).\nFIGURE 4\nThis figure shows the evaluation of the quality of ChatGPT responses \nto treatment related questions. The percentage value in each bar is \nbased on the total number of questions in this category (11).\nValentini et al. 10.3389/fpubh.2024.1303319\nFrontiers in Public Health 05 frontiersin.org\nasked by patients on a social media forum. The bot’s responses were \nrated significantly more empathetic than the physicians’ replies and \nachieved the highest empathy scores on a Likert scale approximately \n10 times more often compared to the physicians’ responses (14). The \nauthors concluded that the addition of artificial intelligence assistants \nto patient messaging workflows appeared to be promising, stressing \nhowever the need for human review of generated content for accuracy \nand potential false or fabricated information (14).\nFinally, we  were able to show that ChatGPT fared better with \ngeneral questions and definitions, and considerably worse with \ntreatment-related inquiries. Several other studies have also \ndemonstrated that the quality of the bot’s responses in a specific \nmedical field may vary depending on the complexity of the posed \ninquiries. Hoch et  al. ( 20) analyzed the accuracy of ChatGPT’s \nresponses to practice multiple choice questions designed for \notolaryngology board certification and found significant variations in \nthe rates of correct responses between different subspecialties. The \nauthors suggested that this finding might be explained due to a varying \navailability and quality of training data in the different categories, with \nthe bot performing better in most common categories and worse in \nrarer subspecialties with potentially more limited literature data (20). \nAnother study by Jung et  al. ( 21) evaluated the performance of \nChatGPT in answering questions from the German state examinations \nfor medical students. While the bot was able to pass both parts of the \nexam, it fared better with questions on facts and definitions and worse \nwith questions necessitating an understanding of complex relationships \nand multimodal diagnostics or applied knowledge ( 21). The \nimportance of a multidisciplinary approach at specialized centers for \nsarcoma patients has been well documented (3–7), and it is considered \na prerequisite for optimal patient care (3). We therefore believe that \nsarcoma patients should be discouraged from using ChatGPT as a \nsource of information for treatment options and approaches.\nIn conclusion, the answers ChatGPT provided on a rare disease, \nsuch as sarcoma, were found to be of very inconsistent quality, with \nsome answers being classified as very good and others as very poor, \ndepending on the complexity and nature of the question. Taken the \nextraordinary popularity ChatGPT achieved in only a few months, \nsarcoma physicians should be aware of the risks of misinformation that \nChatGPT poses and advise their patients accordingly. However, given \nthat ChatGPT achieved higher scores in the evaluation of how \nappropriate its responses are for patients, future studies should evaluate \nwhether it can be used by sarcoma physicians as a supervised tool to \nbetter communicate complex aspects of their disease to affected patients.\nData availability statement\nThe raw data supporting the conclusions of this article will \nbe made available by the authors, without undue reservation.\nEthics statement\nAn approval from our local ethic committee was not required, as \nthe study did not involve human subjects.\nAuthor contributions\nMV: Writing – review & editing, Writing – original draft, \nMethodology, Investigation, Data curation, Conceptualization. JS: \nWriting – review & editing, Supervision, Investigation. MS: Writing \n– review & editing, Formal analysis, Data curation. SS: Writing – \nreview & editing, Supervision. AL: Writing – review & editing, \nSupervision, Methodology, Investigation, Conceptualization. DA: \nWriting – review & editing, Validation, Supervision, Methodology, \nInvestigation, Formal analysis, Conceptualization.\nFunding\nThe author(s) declare that no financial support was received for \nthe research, authorship, and/or publication of this article.\nConflict of interest\nMV reports travel support by Alphamed, outside the submitted \nwork. JS reports participation in advisory boards and invited speaker \nfees by PharmaMar, Bayer, Roche, Lilly, and Amgen, receipt of travel \nexpenses by PharmaMar, Roche, Merck, Lilly, Amgen, and Bristol \nMyers Squibb, and research funding by PharmaMar, Roche, and Eisai, \noutside the submitted work. MS reports travel support by Alphamed, \noutside the submitted work. SS reports travel support and funding for \nconference participation from PharmaMar and Alphamed, and \nresearch funding from Roche Austria, outside the submitted work. AL \nreports institutional educational grants by Johnson & Johnson, \nAlphamed and Medacta, outside the submitted work. DA reports \nreceipt of honoraria to institution for an invited presentation from \nPharmaMar, outside the submitted work.\nPublisher’s note\nAll claims expressed in this article are solely those of the \nauthors and do not necessarily represent those of their affiliated \norganizations, or those of the publisher, the editors and the \nreviewers. Any product that may be evaluated in this article, or \nclaim that may be made by its manufacturer, is not guaranteed or \nendorsed by the publisher.\nReferences\n 1. Stiller CA, Trama A, Serraino D, Rossi S, Navarro C, Chirlaque MD,  \net al. Descriptive epidemiology of sarcomas in Europe: report from the  \nRARECARE project. Eur J Cancer . (2013) 49:684–95. doi: 10.1016/j.ejca.2012.  \n09.011\n 2. Gage MM, Nagarajan N, Ruck JM, Canner JK, Khan S, Giuliano K, et al. Sarcomas \nin the United States: recent trends and a call for improved staging. Oncotarget. (2019) \n10:2462–74. doi: 10.18632/oncotarget.26809\n 3. Gronchi A, Miah AB, Dei Tos AP , Abecassis N, Bajpai J, Bauer S, et al. ESMO \nguidelines committee, EURACAN and GENTURIS. Soft tissue and visceral sarcomas: \nESMO-EURACAN-GENTURIS clinical practice guidelines for diagnosis, treatment and \nfollow-up☆. Ann Oncol. (2021) 32:1348–65. doi: 10.1016/j.annonc.2021.07.006\n 4. Nakayama R, Mori T, Okita Y , Shiraishi Y , Endo M. A multidisciplinary approach \nto soft-tissue sarcoma of the extremities. Expert Rev Anticancer Ther. (2020) 20:893–900. \ndoi: 10.1080/14737140.2020.1814150\nValentini et al. 10.3389/fpubh.2024.1303319\nFrontiers in Public Health 06 frontiersin.org\n 5. Pollock RE, Payne JE, Rogers AD, Smith SM, Iwenofu OH, Valerio IL, et al. \nMultidisciplinary sarcoma care. Curr Probl Surg . (2018) 55:517–80. doi: 10.1067/j.\ncpsurg.2018.10.006\n 6. Strauss SJ, Frezza AM, Abecassis N, Bajpai J, Bauer S, et al. Bone sarcomas: ESMO-\nEURACAN-GENTURIS-ERN Paed can clinical practice guideline for diagnosis, \ntreatment and follow-up. Ann Oncol. (2021) 32:1520–36. doi: 10.1016/j.annonc.2021. \n08.1995\n 7. Strönisch A, Märdian S, Flörcken A. Centralized and interdisciplinary therapy \nManagement in the Treatment of sarcomas. Life (Basel). (2023) 13:979. doi: 10.3390/\nlife13040979\n 8. Biswas SS. Role of chat GPT in public health. Ann Biomed Eng. (2023) 51:868–9. \ndoi: 10.1007/s10439-023-03172-7\n 9. Sallam M. The utility of chatGPT as an example of large language models in \nhealthcare education, research and practice: systematic review on the future perspectives \nand potential limitations. med Rxiv. (2023) 2023:2. doi: 10.1101/2023.02.19.23286155\n 10. Gilson A, Safranek CW , Huang T, Socrates V , Chi L, Taylor RA, et al. How does \nchat GPT perform on the United States medical licensing examination? The implications \nof large language models for medical education and knowledge assessment. JMIR Med \nEduc. (2023) 9:e45312. doi: 10.2196/45312\n 11. Kung TH, Cheatham M, Medenilla A, Sillos C, De Leon L, Elepaño C, et al. \nPerformance of chat GPT on USMLE: potential for AI-assisted medical education using \nlarge language models. PLOS digital. Health. (2023) 2:e0000198. doi: 10.1371/journal.\npdig.0000198\n 12. Hill-Y ardin EL, Hutchinson MR, Laycock R, Spencer SJ. A chat (GPT) about the \nfuture of scientific publishing. Brain Behav Immun. (2023) 110:152–4. doi: 10.1016/j.\nbbi.2023.02.022\n 13. Orrù G, Piarulli A, Conversano C, Gemignani A. Human-like problem-solving \nabilities in large language models using chat GPT. Front Artif Intell. (2023) 6:1199350. \ndoi: 10.3389/frai.2023.1199350\n 14. Ayers JW , Poliak A, Dredze M, Leas EC, Zhu Z, Kelley JB, et al. Comparing \nphysician and artificial intelligence Chatbot responses to patient questions posted to a \npublic social media forum. JAMA Intern Med . (2023) 183:589–96. doi: 10.1001/\njamainternmed.2023.1838\n 15. Johnson D, Goodman R, Patrinely J, Stone C, Zimmerman E, Donald R, et al. \nAssessing the accuracy and reliability of AI-generated medical responses: an evaluation \nof the chat-GPT model. Res Sq [Preprint]. (2023). doi: 10.21203/rs.3.rs-2566942/v1\n 16. Karako K, Song P , Chen Y , Tang W . New possibilities for medical support systems \nutilizing artificial intelligence (AI) and data platforms. Biosci Trends. (2023) 17:186–9. \ndoi: 10.5582/bst.2023.01138\n 17. Chow JCL, Sanders L, Li K. Impact of chat GPT on medical chatbots as a disruptive \ntechnology. Front Artif Intell. (2023) 6:1166014. doi: 10.3389/frai.2023.1166014\n 18. Semrl N, Feigl S, Taumberger N, Bracic T, Fluhr H, Blockeel C, et al. AI language \nmodels in human reproduction research: exploring chat GPT's potential to assist \nacademic writing. Hum Reprod. (2023) 38:2281–8. doi: 10.1093/humrep/dead207\n 19. Uz C, Umay E. \"Dr chat GPT\": is it a reliable and useful source for common \nrheumatic diseases? Int J Rheum Dis. (2023) 26:1343–9. doi: 10.1111/1756-185X.14749\n 20. Hoch CC, Wollenberg B, Lüers JC, Knoedler S, Knoedler L, Frank K, et al. Chat \nGPT's quiz skills in different otolaryngology subspecialties: an analysis of 2576 single-\nchoice and multiple-choice board certification preparation questions. Eur Arch \nOtorrinolaringol. (2023) 280:4271–8. doi: 10.1007/s00405-023-08051-4\n 21. Jung LB, Gudera JA, Wiegand TLT, Allmendinger S, Dimitriadis K, Koerte IK. \nChat GPT passes German state examination in medicine with picture questions omitted. \nDtsch Arztebl Int. (2023) 120:373–4. doi: 10.3238/arztebl.m2023.0113\n 22. Leithner A, Maurer-Ertl W , Glehr M, Friesenbichler J, Leithner K, Windhager R. \nWikipedia and osteosarcoma: a trustworthy patients' information? J Am Med Inform \nAssoc. (2010) 17:373–4. doi: 10.1136/jamia.2010.004507\n 23. Schippinger M, Ruckenstuhl P , Friesenbichler J, Leithner A. Osteosarcoma: \nreliability and quality of the information in the internet. Wien Med Wochenschr. (2014) \n164:353–7. doi: 10.1007/s10354-014-0304-y\n 24. Zade RT, Tartaglione JP , Chisena E, Adams CT, DiCaprio MR. The quality of \nonline Orthopaedic oncology information. J Am Acad Orthop Surg Glob Res Rev. (2020) \n4:e19.00181. doi: 10.5435/JAAOSGlobal-D-19-00181"
}