{
  "title": "EG-TransUNet: a transformer-based U-Net with enhanced and guided models for biomedical image segmentation",
  "url": "https://openalex.org/W4323364877",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2103084250",
      "name": "Shaoming Pan",
      "affiliations": [
        "State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing",
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2097339281",
      "name": "Xin Liu",
      "affiliations": [
        "State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing",
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A4227592877",
      "name": "Ningdi Xie",
      "affiliations": [
        "State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing",
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2119004616",
      "name": "Yanwen Chong",
      "affiliations": [
        "Wuhan University",
        "State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing"
      ]
    },
    {
      "id": "https://openalex.org/A2103084250",
      "name": "Shaoming Pan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097339281",
      "name": "Xin Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227592877",
      "name": "Ningdi Xie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119004616",
      "name": "Yanwen Chong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4213007876",
    "https://openalex.org/W3149710509",
    "https://openalex.org/W4285219596",
    "https://openalex.org/W2395611524",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2293078015",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W2991139962",
    "https://openalex.org/W2989028599",
    "https://openalex.org/W3168491317",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2980998394",
    "https://openalex.org/W2288892845",
    "https://openalex.org/W2945325951",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W3025800305",
    "https://openalex.org/W3035376943",
    "https://openalex.org/W3114814504",
    "https://openalex.org/W3081752372",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2937845726",
    "https://openalex.org/W3148874463",
    "https://openalex.org/W3204166336",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2008359794",
    "https://openalex.org/W2623808523",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3162386519",
    "https://openalex.org/W2964101377",
    "https://openalex.org/W2999580839",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W2793703004",
    "https://openalex.org/W2899986319",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W3092344722",
    "https://openalex.org/W4226351492",
    "https://openalex.org/W4229019683",
    "https://openalex.org/W4226056226",
    "https://openalex.org/W3016052689",
    "https://openalex.org/W4220859008",
    "https://openalex.org/W4229448367"
  ],
  "abstract": "Abstract Although various methods based on convolutional neural networks have improved the performance of biomedical image segmentation to meet the precision requirements of medical imaging segmentation task, medical image segmentation methods based on deep learning still need to solve the following problems: (1) Difficulty in extracting the discriminative feature of the lesion region in medical images during the encoding process due to variable sizes and shapes; (2) difficulty in fusing spatial and semantic information of the lesion region effectively during the decoding process due to redundant information and the semantic gap. In this paper, we used the attention-based Transformer during the encoder and decoder stages to improve feature discrimination at the level of spatial detail and semantic location by its multihead-based self-attention. In conclusion, we propose an architecture called EG-TransUNet, including three modules improved by a transformer: progressive enhancement module, channel spatial attention, and semantic guidance attention. The proposed EG-TransUNet architecture allowed us to capture object variabilities with improved results on different biomedical datasets. EG-TransUNet outperformed other methods on two popular colonoscopy datasets (Kvasir-SEG and CVC-ClinicDB) by achieving 93.44% and 95.26% on mDice. Extensive experiments and visualization results demonstrate that our method advances the performance on five medical segmentation datasets with better generalization ability.",
  "full_text": "Open Access\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nPan et al. BMC Bioinformatics           (2023) 24:85  \nhttps://doi.org/10.1186/s12859-023-05196-1\nBMC Bioinformatics\nEG-TransUNet: a transformer-based \nU-Net with enhanced and guided models \nfor biomedical image segmentation\nShaoming Pan, Xin Liu, Ningdi Xie and Yanwen Chong* \nAbstract \nAlthough various methods based on convolutional neural networks have improved the \nperformance of biomedical image segmentation to meet the precision requirements \nof medical imaging segmentation task, medical image segmentation methods based \non deep learning still need to solve the following problems: (1) Difficulty in extracting \nthe discriminative feature of the lesion region in medical images during the encoding \nprocess due to variable sizes and shapes; (2) difficulty in fusing spatial and semantic \ninformation of the lesion region effectively during the decoding process due to redun-\ndant information and the semantic gap. In this paper, we used the attention-based \nTransformer during the encoder and decoder stages to improve feature discrimination \nat the level of spatial detail and semantic location by its multihead-based self-attention. \nIn conclusion, we propose an architecture called EG-TransUNet, including three mod-\nules improved by a transformer: progressive enhancement module, channel spatial \nattention, and semantic guidance attention. The proposed EG-TransUNet architecture \nallowed us to capture object variabilities with improved results on different biomedi-\ncal datasets. EG-TransUNet outperformed other methods on two popular colonos-\ncopy datasets (Kvasir-SEG and CVC-ClinicDB) by achieving 93.44% and 95.26% on \nmDice. Extensive experiments and visualization results demonstrate that our method \nadvances the performance on five medical segmentation datasets with better gener-\nalization ability.\nKeywords: Medical image segmentation, Transformer, Self-attention, Progressive \nenhancement module, Channel spatial attention, Semantic guidance attention\nIntroduction\nWith the help of medical imaging technology, physicians can now understand the \npatient’s condition more clearly and intuitively to make a clearer diagnosis. However, \nmedical images often suffer from issues, such as low image resolution, similar organi -\nzational structure, uneven distribution of foreground and background, which pose great \nchallenges in clinical diagnostics. The traditional segmentation process relies heavily on \nthe experience and energy of physicians, and inevitably leads to misdiagnoses or missed \ndiagnoses. Therefore, quick and efficient image analysis has become a valuable research \n*Correspondence:   \nywchong@whu.edu.cn\nThe State Key Laboratory \nof Information Engineering \nin Surveying, Mapping, \nand Remote Sensing, Wuhan \nUniversity, Wuhan, China\nPage 2 of 22Pan et al. BMC Bioinformatics           (2023) 24:85 \ntopic in the field of medical-clinical diagnosis to overcome these significant challenges. \nThe application of artificial intelligence in medical field is multifaceted. Chakraborty \net al. [1] proposed a IoMT-based cloud-fog diagnostics for heart disease. Kishor et al. \n[2] proposes a hybrid Machine Learning Classification Techniques to analyze the com -\nplex biomedical data. Chakraborty et  al. [3] proposed a reinforcement learning-based \nmethod with Medical Information system to improve the quality of service over a het -\nerogeneous network.\nConvolutional neural network (CNNs) perform excellently on the medical segmen -\ntation task with strong feature extraction and adaptive learning ability. In particular, \nsegmentation models based on Full Convolutional Neural Networks (FCNs) [4] can sig -\nnificantly improve the general technical level of medical image segmentation. However, \ndue to the loss of detailed information in the high-level semantic expression, the up-\nsampling process of FCNs lacks sufficient spatial detail information, leading to blurred \nboundaries of the segmentation results. The U-Net [5] integrates semantic information \nwith spatial information using skip connection, which has become the most commonly \nused structure in medical image segmentation tasks. The reason for the success of U-Net \nis that the skip connection directly provides an upsampling process with low-level tex -\nture information, which is critical for high-level semantic features. Consequently, these \nconnections can provide deconvolution [6] layers with essential high resolution fea -\ntures. Several researchers have made considerable efforts to capture distinguishing fea -\ntures based on U-Net. For example, the U-Net++ [7] can obtain discriminative features \nthrough its nesting architecture and dense skip connection. Attention U-Net [8] and \nChannel U-Net [9], attempt to combine the U-Net with various attention mechanisms \nwith the purpose of enhancing enhancing discriminative features, which have been \napplied to optimize the significant information extracted from encoders and decoders, \nrespectively. The U-Net [5] ignores the semantic gap in feature fusion, while AUNet [10] \nproposes an attention-guided upsampling module to optimize the skip connection pro -\ncess, which could remove redundant spatial information and reduce the semantic gap. \nDue to the variable size and shape of the lesion areas in medical images, uneven distri -\nbution of the foreground and background areas, blurred boundaries, and semantic gap, \nit is still difficult to extract the discriminative features of lesions and improve the effect \nof feature fusion when decoding. The model proposed in this paper enhances the expres-\nsiveness of lesion area features by means of enhancing the expressiveness of spatial fea -\ntures and improving the accuracy of semantic location information. At the same time, \nsemantic features are enhanced at both channel and spatial levels to provide accurate \nsemantic location expression for medical image segmentation, and jointly improve the \ndiscrimination of lesion area features.\nRecently, the global context modeling capability of Transformer [11], TransUnet [12], \nand TransAttUnet [13] was found to improve global semantic information on the loca -\ntion and shape of lesions, and thus enhance discriminative semantic feature. DS-Tran -\nsUnet [14] directly uses the swin Transformer [15] to complete information fusion and \ndisregards the semantic gap between spatial and semantic information. These meth -\nods ignore limitations in capturing fine-grained details of the Transformer, especially \nfor medical images, which can still be optimized for discriminative feature extraction \nand fusion. From the perspective of improving task and semantic position correlations \nPage 3 of 22\nPan et al. BMC Bioinformatics           (2023) 24:85 \n \nof spatial detail information, our method uses transformer-based multihead attention \nmechanism to remove redundant information in spatial features and reduce semantic \ndeviation. This in turn optimizes the fusion of spatial and semantic information of the \nlesion area and improves the accuracy of medical image segmentation.\nIn this paper, we propose a novel U-Net variant with a transformer called EG-TransU-\nNet, which can effectively preserve reliable discriminative features and achieve effective \nfusion of spatial and semantic information in U-Net. This is achieved by jointly utilizing \nthe progressive enhancement module (PEM), the semantic guidance attention (SGA), \nand the channel spatial attention (CSA).\nTo solve the issue of distinguishing the extraction of features in lesion areas, PEM \nbased on a self-attention mechanism cascades the feedback of various receptive field fea-\ntures of medical images and uses the global self-attention mechanism to obtain more \nextensive guidance information. CSA captures the global relationship of the self-atten -\ntion mechanism to calculate the similarity between each channel feature and each spa -\ntial position. This in turn enhances semantic position information at the channel and \nspatial level to obtain more accurate information.\nConsidering the difficulties in feature fusion of medical image segmentation tasks, \nSGA starts its process by improving the task and semantic position correlation of \ndetailed spatial information and fully exploits prior knowledge of medical images in \nthe skip connection part. The relationship between semantic and spatial details is fully \nexplored and improved, the redundant information is removed, and the subsequent fea -\nture fusion process is optimized.\nWe evaluated the effectiveness of the proposed EG-TransUNet using four typical med-\nical image segmentation tasks covering polyp segmentation [13, 14], nuclei segmenta -\ntion [16], melanoma segmentation [17], and gland segmentation (GLAS) [18], and the \nexperimental results demonstrated the consistent effectiveness of the proposed EG-\nTransUNet. Our main contributions are summarized as follows:\n(1) This paper proposes a Transformer-based U-shaped framework called EG-TransU-\nNet, which exploits three novel modules, namely the PEM, SGA, and channel spa -\ntial attention (CSA), to improve the performance of medical segmentation.\n(2) PEM consists of dilated self-attention convolution (DSA) and gated convolution \n(GC), which can capture spatial features of the target region. CSA can capture long-\nrange contextual information in channel and spatial dimensions using self-atten -\ntion. SGA is used to remove redundant information and reduce the semantic gap.\n(3) In comparison to certain state-of-the-art methods, the effectiveness and generaliz -\nability of the proposed EG-TransUnet is demonstrated by extensive experiments on \nmedical image segmentation that consistently showed that the proposed method \noutperforms these previous methods, especially when it comes to polyp segmenta -\ntion tasks.\nThe remaining of this paper is organized as follows: “Related work ” Section provides \nan overview of some related works on automatic medical image segmentation, and \n“Methods” Section describes the proposed EG-TransUNet in detail. Next, compre -\nhensive experiments and ablation studies are presented in “Experimental analysis” and \nPage 4 of 22Pan et al. BMC Bioinformatics           (2023) 24:85 \n“Generalization and discussion ” sections. Finally, “ Conclusion” section summarizes the \npresent work.\nRelated work\nIn this section, we provide a brief overview of research related to medical image segmen-\ntation tasks. We first summarize the most typical U-shaped CNNs methods in medi -\ncal image segmentation and then review the application of vision transformers in recent \nyears, especially in image segmentation tasks.\nMedical image segmentation based on CNNs\nCNNs, particularly U-Net [5] and its encoder-decoder-based variants, have proven their \nexceptional performance in segmenting medical images.\nIn order to successfully detect and segment each individual breast slice in the DCE-\nMRI breast tumor dataset, Benjelloun et al. [19] developed a fully convolutional neural \nnetwork architecture based on U-Net [5] for the first time. Consequently, some stud -\nies attempted to combine the low-level feature of the shallow layer with the high-level \nfeature of the deep layer to take full advantage of multiscale information and ameliorate \ndetail restoration issues. U-Net++ [7], U-Net 3+ [20] and DenseUNet [21] used full-\nscale skip connections and deep supervisions to learn hierarchical representations from \nfull-scale aggregated feature maps. U2-Net [22] was able to capture more contextual \ninformation from different scales with a mixture of different receptive field sizes in the \nproposed Residual U Blocks (RSU). KiU-Net [23] introduced a novel structure that could \nproject data to higher dimensions and obtain both incomplete and complete features \nthat improved segmentation of small anatomical structures. Furthermore, MA-UNet \n[24] established a multiscale mechanism to remove semantic ambiguity in skip con -\nnections by adding attentional gates (AGs) which can explicitly model the relationship \nbetween channels. In addition, MA-U-Net used multiscale predictive fusion to exploit \nglobal information at different scales by combining local features with their correspond -\ning global dependencies. Finally, DoubleU-Net [25] used two U-Net and an atrous spa -\ntial pyramid pooling [26] to obtain accurate spatial, semantic, and contextual features.\nThereafter, many attention-guided methods have been proposed to optimize the seg -\nmentation performance of U-Net by enhancing the discriminative features in medical \nimages obtained from different imaging modalities. Oktay et  al. [8] proposed a novel \nAG mechanism based on U-Net that allowed the model to focus on targets of different \nshapes and sizes. Chen et al. [9] proposed a spatial channel-wise convolution, which a \nconvolution along the direction of the channel of feature maps, to extract the relation -\nship of spatial information between pixels, and thus discriminate the lesion areas. Tang \net al. [27] proposed a criss-cross attention module to capture rich global context infor -\nmation in both horizontal and vertical directions for all pixels, thus facilitating accurate \nlung segmentation. Chen et  al. [28] used the Aggregated Residual Transformations to \nlearn a robust and expressive feature representation. The soft attention mechanism was \nthen applied to improve the capability of the model to discriminate a variety of symp -\ntoms of the COVID-19 in chest CT. Tomar et al. [29] proposed a feedback attention net-\nwork (FANet) that unified the previous epoch mask with the feature map of the current \ntraining epoch, allowing the predictions to be iteratively corrected during testing time.\nPage 5 of 22\nPan et al. BMC Bioinformatics           (2023) 24:85 \n \nAltogether, current studies often improve the expressiveness of feature and opti -\nmize the use of spatial information in skip connections through multiscale feature \nfusion and attention mechanisms. The U-Net++ model does not consider semantic \nbias when fusing features at different scales, but instead obtains the optimal solution \nusing a simple dense connection search, and fails to optimize the process of seman -\ntic feature extraction. Therefore, the extracted features of the lesion area still suffer \nfrom insufficient discrimination. However, multiscale feature fusion may cause valu -\nable detail loss and may suffer from information redundancy. Attention mechanisms \noften fail to recognize the boundaries of images with similar organizational struc -\ntures, leading to the loss of available feature representation.\nTransformers in medical segmentation\nTransformers [11] have triggered great achievements in the field of computer vision \ndue to their ability to model long-range contextual interactions. In medical image \nsegmentation, TransUNet [12] proved that Transformers could serve as powerful \nencoders for medical image segmentation tasks, with the combination of U-Net to \nenhance finer details by recovering localized spatial information. TransFuse [30] \ncombined Transformers and CNNs in a parallel style, where both global depend -\nency and low-level spatial details could be efficiently captured and fused in a much \nshallower manner. MedT [31] proposed a gated axial attention model that used a \ntransformer-based gating position-sensitive axial attention mechanism to segment \nmedical images based on Axial-DeepLab [32]. In TransAttUnet [13], multilevel \nguided attention and multiscale skip connection were co-developed to effectively \nimprove the functionality and flexibility of the traditional U-shaped architecture. \nDS-TransUNet [14] applied the swin-transformer block [15] to both the encoder and \nthe decoder. This was probably the first attempt to simultaneously incorporate the \nadvantages of hierarchical Swin Transformer into both the encoder and the decoder \nof the standard U-shaped architecture with the purpose of enhancing the segmenta -\ntion quality of varying medical images.\nHowever, the above medical image segmentation model fails to take full advan -\ntage of the spatial detail information of the lesion area, resulting in low accuracy in \nmedical image segmentation tasks in complex environments. Although the TransU -\nnet and TransAttUnet models use the transformer structure to enhance the global \nexpression of features, they only focus on the acquisition of semantic location infor -\nmation and do not improve the acquisition process of spatial features. Therefore, \nthese models cannot use distinctive feature texture during decoding. On one hand, \nthe DS-TransUNet model uses the swin transformer model to complete information \nfusion, which completely ignores the semantic deviation between spatial and seman -\ntic information. On the other hand, the swin transformer structure lacks interpret -\nability in the segmentation process.\nInspired by these approaches, we propose a U-shaped structure called EG-TransU -\nNet that applies a Transformer, specifically multihead attention, to both the encoder \nand the decoder. We believe that this Transformer-based structure can outperform \nprevious models and optimize medical image segmentation.\nPage 6 of 22Pan et al. BMC Bioinformatics           (2023) 24:85 \nMethods\nThis section explicitly introduces the proposed EG-TransUNet. First, an overview of \nthe proposed EG-TransUNet is presented. Then, we present the principles and struc -\nture of EG-TransUNet, followed by a detailed description of each component. Finally, \nwe elaborate the loss function used in our EG-TransUNet.\nOverview of the EG‑TransUNet\nThe input of medical image is X ∈ RC×H×W  , where C is the number of channels and \nH × W  represents the spatial resolution of image. Consequently, the goal of medi -\ncal image segmentation task is to predict the corresponding pixel-wise semantic label \nmaps with H × W  size. Consistent with the previous work on medical image segmen -\ntation tasks, the EG-TransUNet is also built on a U-shaped architecture, whose brief \nstructure is illustrated in Fig. 1 .\nThe EG-TransUNet consists of three processes, namely encoding, decoding and \nsemantic feature enhancement. The encoding process uses ResNet50 [33] as the back -\nbone to obtain feature from different receptive fields of the input medical images. The \nstructure of the residual block is shown in Fig.  2a. The combination of the latter two \nblocks of the ResNet50 [33] network generates four encoding blocks, each of which \ndownsamples the feature maps by a factor of two. The structure of each encoding \nFig. 1 Illustration of the proposed EG-TransUNet for medical image segmentation\nFig. 2 Illustration of the encoding and decoding. a The residual block (RB) used in the process of encoding. b \nThe structure of each encoding and the location of PEM. c The structure of each decoding\nPage 7 of 22\nPan et al. BMC Bioinformatics           (2023) 24:85 \n \nprocess and the location of PEM are shown in Fig.  2b. The decoding process is con -\nsistent with the standard U-Net and constructs the segmentation results on a step by \nstep approach, including two convolutions and one upsampling, as shown in Fig.  2c. \nThe semantic feature enhancement is used to improve the representation ability of \nthe semantic feature, which refers to the CSA in Fig.  1. Considering the computation \ncost, we only embed PEM and SGA into the third encoding and the second decoding \nprocesses, respectively.\nProgressive enhancement module (PEM)\nPEM consists mainly of two parts, namely dilated self-attention convolution and gated \nconvolution. As shown in Fig. 3, we use one 3 × 3 convolution operation and two dilated \nself-attention convolution modules with dilated 3 × 3 convolution rates of two and three, \nrespectively, to obtain features from different receptive fields. Then, we feed the features \nobtained by the 3 × 3 convolution operation and the dilated self-attention convolution \nmodule with a dilated rate of two into the GC, and make the larger receptive field feature \nguide the discriminative extraction process of the original feature. Consequently, the \nfeature from the first GC module and the feature from the dilated self-attention convolu-\ntion module with a dilated rate of three are fed into the GC again, and the discriminative \nfeature is further extracted. Finally, the original 3 × 3 convolution feature is combined \nwith the output feature of the two-GCs as the final output.\n(1) Dilated Self-attention Convolution: The DSA is built on the multihead self-attention \nof a Transformer and allows the model to care only for information stemming from \nglobal representation subspaces. We use convolution embedding instead of linear \nembedding, so that the DSA cannot only aggregate global contextual information, \nbut also account for local spatial information. Compared to traditional convolution, \ndilated convolution can flexibly change the receptive field by changing the rate of \ndilation while ensuring the consistency of the feature size. The DSA can selectively \nFig. 3 Illustration of the proposed progressive enhancement module (PEM)\nPage 8 of 22Pan et al. BMC Bioinformatics           (2023) 24:85 \naggregate the global context to the learned feature and encode broader contextual \npositional information into the local feature using convolutional embedding and \nmatrix multiplication, which can improve intraclass compactness and optimize fea -\nture representations.\nThe pipeline of the dilated DSA component is depicted in Fig.  4, and we refer to the \ndescription of the TransAttUnet Transformer to describe DSA.\nFirst, we apply three dilated convolution operations on the encoder feature x to \ngenerate the feature maps q, k, and v. Subsequently, we reshape q and k and perform a \nmatrix multiplication with softmax normalization, resulting in the position relevance \nattention map. T he above operation can be defined as follows:\nwhere B i,j measures the impact of the ith position on the  jth position, n = h × w  is the \nnumber of pixels, and M , N , and T represent the reshaped features. B represents the \nposition relevance attention map. Then, T is multiplied by B , and we reshape the opti -\nmized feature maps to obtain the output of DSA.\n(2) GC: The gated convolution module consists of two inputs, indicating one large and \none small receptive field feature, as shown in Fig. 3. Then, two different convolutional \noperations are applied to the input features to generate the gate maps. Finally, a mul-\ntiplication operation is performed to obtain the final output. The calculation process \ncan be formulated as follows:\nwhere W g and W f are the embedding matrices of different convolution projections \nand Fhigh  and Flow represent two inputs. Gate is the attention map and σ  is the \nsigmoid  function, which maps all values to the interval between 0 and 1. Finally, F is \nthe feature embedding and ∅ means ReLU  activation.\nChannel spatial attention\nThe CSA helps our model to capture the wider and richer contextual representations and \nobtain more accurate semantic location representation of the lesion region. Inspired by \nCBAM [34], the two self-attention mechanisms, i.e., Channel and Spatial MHSA (multi -\nhead self-attention) are connected in series to form the CSA module, as shown in Fig. 5.\nBoth channels can use the self-attention mechanism to calculate the global correlation \nbetween channel feature and spatial features and enhance channel and spatial informa -\ntion under the guidance of autocorrelation as used in transformer [ 11]. Specifically, we \n(1)\nM = reshape(q)\nN = reshape(k)\nT = reshape(v)\nBi,j = exp M i ·N j\nn\nk= 1 exp M k ·N j)\nDSA (B,T ) = B ·T\n(2)\nGate = W g ·Fhigh\nF = W f ·Flow\nG =∅ (F ) ∗ σ( Gate)\nPage 9 of 22\nPan et al. BMC Bioinformatics           (2023) 24:85 \n \nuse absolute position embedding to capture the spatial relationship of features in spatial \nMHSA, while no position embedding is used in channel MHSA. The process of absolute \nposition embedding [35] can be formulated as follows:\nEach output element, zi , is computed as a weighted sum of a linearly transformed input \nelements. W q , W k , and W v are parameter matrixes, which are unique for each layer and \nattention head. Each weight coefficient αi,j is computed using a softmax function, while \ne i,j represents the correlation between two input elements, which is computed by the \nscaled dot product. The absolute position embedding between the input elements x i and \nx j is reflected by the matrix p i,j , which is shared across the attention heads and opti -\nmized by backward propagating. Finally,a k\ni,j is a trainable position parameter matrix.\n(3)\nei,j= (xiW q) ·\n(\nxjW k)T\n√ dz\npi,j=\n(\nxiW v)\n·\n(\nak\ni,j\n)T\nα i,j= exp\n(\nei,j+ pi,j\n)\n∑ n\nk=1 exp\n(\nei,k + pi,k\n)\nzi =\n∑ n\nj=1 α i,j·xj\nFig. 4 Illustration of the proposed dilated self-attention convolution (DSA)\nFig. 5 Illustration of the proposed channel spatial attention (CSA)\nPage 10 of 22Pan et al. BMC Bioinformatics           (2023) 24:85 \nSemantic guide attention (SGA)\nPertaining to the decoding process, the MHSA can calculate the correlation between \nthe corresponding positions between spatial and semantic features. Hence, the SGA \ncan remove redundant textual information, reduce the semantic gap, and improve the \neffect of feature fusion in skip connection with the correlation. The pipeline of SGA is \ndepicted in Fig. 6, demonstrating that the overall structure is similar to the self-attention \nmechanism.\nThe low-level input from the encoder feature is firstly reshaped to generate K and V, \nrespectively. Then, channel selection (CS) is applied to obtain the important channels of \nK. The high-level input from the decoder feature is reshaped into the Q matrix and CS is \napplied to select the important channels. A multihead scaled dot-product operation with \nsoftmax normalization between Q and the transposed version of K is conducted to gen -\nerate the contextual attention map, which represents the global similarities of the given \nelements from the semantic and spatial features. To obtain the aggregation of values \nweighted by contextual attention, the map should be multiplied by V. Finally, we concat-\nenate the reshaped low-level and high-level features to obtain the final output of SGA.\nThe channel selection (CS) can be formulated as follows:\nwhere Fc is the feature of the C th channel of feature F . W refers to the weight, which is \nconstantly optimized in the model training process, enabling the key channel feature to \nbe accurately selected. Aw refers to the task correlation of all channel features. Finally, \nthe Aw is multiplied by the input feature F to obtain the key channel feature ˜F.\n(4)\nPc = 1\nh × w\nh∑\ni=1\nw∑\nj=1\n(Fc(i,j)),P ∈ Rc,F ∈ Rc×h× w\nA w = sigmoid(W ·P),W ∈ Rc×c,A w ∈ Rc\n˜F = A w ·F,F ∈ Rc× h× w ,˜F ∈ c× h × w\nFig. 6 Illustration of the proposed semantic guidance attention (SGA)\nPage 11 of 22\nPan et al. BMC Bioinformatics           (2023) 24:85 \n \nLoss function\nDuring the training phase, the EG-TransUNet uses an end-to-end training manner. We \nhave used binary cross-entropy loss LBCE and dice loss LDice . The calculation formulas of \nLBCE and LDice are as follows:\nwhere n is the total number of pixels in each image, yi represents the ground-truth value \nof the ith pixel, and p i represents the confidence score of the ith pixel in the prediction \nresults. In our experiment, α = β = 0.5 , and ε = 10−6.\nExperimental analysis\nIn this section, we introduced five segmentation datasets and conducted some experi -\nments to compare our proposed model with SOTA methods.\nDescription of data sets\nTo evaluate the effectiveness of EG-TransUNet, we used five public biomedical datasets \nnamely Kvasir-SEG [36], CVC-ClinicDB [37], 2018 Data Science Bowl [16], ISIC-2018 \nChallenge [17] and 2015 MICCAI Gland Segmentation (GLAS) [18]. An example of each \ndataset can be found in Fig. 7.\n(1) Kvasir-SEG [36]: The Kvasir-SEG is a dataset of gastric polyp images for developing \napplications on the automated diagnosis of polyps from endoscopic images. It is \nan extension of Kvasir [38] which contains images from the inside of the gastroin -\ntestinal (GI) tract. The Kvasir-SEG contains 1000 images with the corresponding \nannotations and these images are randomly split into 800 images for training, 100 \nimages for validation and 100 images for testing.\n(5)\nLBCE =−\nn∑\ni=1\n(\nyilog(pi) + (1 − yi)log(1 − pi)\n)\nLDice = 1 −\n∑n\ni=1 yipi + ε∑n\ni=1\n(\nyi + pi\n)\n+ ε\nLTotal = α ·LBCE + β ·LDice\nFig. 7 Illustration of examples of medical images with the corresponding semantic segmentation \nannotations\nPage 12 of 22Pan et al. BMC Bioinformatics           (2023) 24:85 \n(2) CVC-ClinicDB [37]: The CVC-ClinicDB is a dataset of 612 images from 31 colo -\nnoscopy sequences with a resolution of 384 × 288. It is used for polyp segmentation \nin colonoscopy videos. These images are randomly split into 490 images for train -\ning, 61 images for validation and 61 images for testing.\n(3) 2018 Data Science Bowl [16]: The purpose of this dataset was to find the nuclei \nin divergent images, including a total of 670 images. These images are randomly \ndivided into 536 images for training, 67 images for validation, and 67 images for \ntesting.\n(4) ISIC -2018 Challenge [17]: The ISIC-2018 is a comprehensive dataset of dermos -\ncopy images for developing applications on the automated diagnosis of melanoma \nusing dermoscopic images. Their work [17] focuses on lesion segmentation from \ndermoscopic images acquired with a variety of dermatoscopy types. It contains \n2596 images with the corresponding annotations, and these images are randomly \nsplit into 2078 images for training, 259 images for validation, and 259 images for \ntesting.\n(5) GLAS [18]: The GLAS dataset is published by the Colon Histology Images Chal -\nlenge Contest of MICCAl’2015 and consists of 165 colon histology images derived \nfrom 16 H&E stained histological sections of stage T3 or T4 colorectal adenocarci -\nnomas from different patients. In particular, each sample is processed on different \noccasions in the laboratory, resulting in high inter-subject variability in both stain \ndistribution and tissue architecture. In our experiments, the GLAS dataset is split \ninto two subsets: 85 images for training and 80 for testing, which is consistent with \nprevious works [23, 31]\nEvaluation metrics\nTo compare our proposed EG-TransUNet to the SOTA methods, the standard evalua -\ntion indicators used included the Dice coefficient (Dice) [39], intersection over Union \n(IoU), precision and recall, which are related to four values, namely true positive (TP) \ntrue negative (TN) false positive (FP) and false negative (FN), respectively.\nImplementation details\nWe implemented the proposed EG-TransUNet using PyTorch [40], and all experiments \nwere conducted on a NVIDIA GeForce 3090 GPU with 12 GB memory. Furthermore, we \nadopted the stochastic gradient descent optimizer with a momentum of 0.9 and a weight \ndecay 0.001 to optimize the training process.\n(6)\nDice= 2 × TP\n2 × TP + FP + FN\nIoU = 2 × TP\nTP + FP + FN\nPrecision= TP\nTP + FP\nRecall= TP\nTP + FN\nPage 13 of 22\nPan et al. BMC Bioinformatics           (2023) 24:85 \n \nFor each dataset, the images were resized into 320 × 320. Data augmentation, such as \nrandom cropping, random rotation, horizontal flipping, vertical flipping, and grid dis -\ntortion, were also used. Furthermore, the EG-TransUNet was trained for 300 epochs \nwith a batch size of four. Besides, the initial learning rate was 5e−3 , decaying by a factor \nof 10 for every 40 epochs.\nResults\n(1) Comparison on Kvasir-SEG\nIn our experiment, we selected two popular colonoscopy datasets of which the first one \nwas Kvasir-SEG. Compared to other models, our quantitative results on the Kvasir-SEG \ndataset achieved SOTA performance, as presented in Table  1 and Fig.  10. Compared to \nMSRF-Net [41], our results demonstrated that Kvasir-SEG could be improved by 1.27% \nTable 1 Comparisons with the state-of-the-art baselines on the Kvasir-SEG dataset terms\nThe “–” denotes the corresponding result is not provided. For each column, the best results are highlighted\nMethod Year mDice mIoU Recall Precision\nResUNet [42] 2018 0.7907 0.4287 0.6909 0.8713\nResUNet++ [43] 2019 0.8133 0.7927 0.8774 0.7064\nU-Net [5] 2015 0.8180 0.7460 0.6306 0.9222\nU-Net++ [7] 2018 0.8210 0.7430 – –\nHRNetV2-W48 [44] 2020 0.8896 0.8262 0.8973 0.9056\nDS-TransUNet-B [14] 2021 0.9110 0.8561 0.9352 0.9143\nDS-TransUNet-L [14] 2021 0.9130 0.8592 0.9360 0.9164\nTransFuse [30] 2021 0.9180 0.8680 – –\nMSRF-Net [41] 2021 0.9217 0.8914 0.9198 0.9666\nEG-TransUNet (ours) – 0.9344 0.8927 0.9401 0.9436\nFig. 8 Qualitative results of EG-TransUNet for Kvasir-SEG and CVC-ClinicDB\nPage 14 of 22Pan et al. BMC Bioinformatics           (2023) 24:85 \non mDice and 0.13% on mIoU, respectively. Our method also achieveed an improvement \nof 0.41% on the recall compared to DS-TransUNet-L [14]. Although MSRF-Net [41] was \nslightly ahead with respect to precision, our method provided significantly better results \nin the other three indices, suggesting that this model could achieve a more balanced and \nexcellent segmentation effect. The perfect qualitative result compared to the ground \ntruth can be observed in Fig. 8.\n(2) Comparison on CVC-ClinicDB\nThe second colonoscopy dataset was CVC-ClinicDB on which we achieved SOTA \nperformance compared to other models, as shown in Table  2. Our model achieved an \nmDice of 0.9523, which corresponded to a 1.01% improvement in mDice over the best \nperforming DS-TransUNet-L [14]. We also found that mIoU was 0.9130, reflecting an \nimprovement of 0.87% over the SOTA performance of MSRF-Net [41]. In addition, \nEG-TransUNet achieved precision and recall values of 0.9536 and 0.9540, respec -\ntively, values that are competitive with the best performing MSRF-Net and Dou -\nbleU-net [25]. Figure  8 demonstrates that our method produced almost exactly the \nsame boundaries and shapes as the ground truth masks. The two above experiments \nTable 2 Comparisons with the state-of-the-art baselines on the CVC-ClinicDB dataset\nThe “–” denotes the corresponding result is not provided. For each column, the best results are highlighted\nMethod Year mDice mIoU Recall Precision\nFCN [45] 2017 0.7732 0.8999\nCNN [46] 2018 0.87 – – –\nSegNet [47] 2018 – – 0.8824 –\nU-Net [5] 2015 0.8781 0.7881 0.7865 0.9329\nResUNet++ [43] 2019 0.9199 0.8892 0.9391 0.8445\nDoubleU-Net [25] 2020 0.9239 0.8611 0.8457 0.9592\nTransUNet [12] 2021 0.9350 0.8870 – –\nDS-TransUNet-B [14] 2021 0.9350 0.8845 0.9464 0.9306\nDS-TransUNet-L [14] 2021 0.9422 0.8939 0.9500 0.9369\nMSRF-Net [41] 2021 0.9420 0.9043 0.9567 0.9427\nEG-TransUNet – 0.9523 0.9130 0.9540 0.9536\nTable 3 Comparisons with the state-of-the-art baselines on the 2018 data science bowl (DSB) \ndataset\nThe “–” denotes the corresponding result is not provided. For each column, the best results are highlighted\nMethod Year mDice mIoU Recall Precision\nU-Net [5] 2015 0.7573 0.9103 – –\nPraNet [48] 2020 0.8103 0.7108 0.8062 0.8231\nU-Net++ [7] 2018 0.8974 0.9255 – –\nDoubleU-Net [25] 2020 0.9133 0.8407 0.6407 0.9406\nTransAttUnet_R [13] 2021 0.9162 0.8498 0.9185 0.9193\nDS-TransUNet-B [14] 2021 0.9200 0.8589 0.9427 0.9054\nDS-TransUNet-L [14] 2021 0.9219 0.8612 0.9378 0.9124\nMSRF-Net [41] 2021 0.9224 0.8534 0.9402 0.9022\nEG-TransUNet – 0.9349 0.8908 0.9482 0.9336\nPage 15 of 22\nPan et al. BMC Bioinformatics           (2023) 24:85 \n \nrevealed that our method could identify the lesion area in colonoscopy image data \nmore accurately compared to conventional models, which offer a small segmentation \ntarget and blurred boundaries.\n(3) Comparison on 2018 Data Science Bowl\nTable 3 shows the comparison results of the proposed EG-TransUNet with some \nof the presented approaches on the 2018 Data Science Bowl dataset. We obtained \nan mDice value of 0.9349, mIoU of 0.8908, recall of 0.9482, and precision of 0.9336, \nwhich outperformed the best performing DoubleU-Net, MSRF-Net, and DS-TransU -\nNet in most metrics. The qualitative results shown in Fig.  9 show that our predictions \nwere almost identical to the ground truth masks. Our model is also suitable for data \nsets with a large number of irregularly distributed targets and blurred boundaries, \nmaintaining high performance and providing accurate results for clinical medical \nimage analysis.\n(4) Comparison on ISIC-2018 Skin Lesion Segmentation challenge\nFig. 9 Qualitative results of EG-TransUNet for GLAS, DSB and ISIC-2018\nTable 4 comparisons with the state-of-the-art baselines on the isic-2018 dataset\nThe “–” denotes the corresponding result is not provided. For each column, the best results are highlighted\nMethod Year mDice mIoU Recall Precision\nU-Net [5] 2015 0.6740 0.5490 0.7080 –\nPraNet [48] 2020 0.8746 0.8023 0.9128 0.8759\nMSRF-Net [41] 2021 0.8813 0.8325 0.8903 0.9267\nDoubleU-Net [25] 2020 0.8962 0.8212 0.8780 0.9459\nTransAttUnet_D [13] 2021 0.9014 0.8304 0.9042 0.9217\nTransAttUnet_R [13] 2021 0.9074 0.8380 0.9093 0.9242\nEG-TransUNet – 0.9075 0.8441 0.9169 0.9165\nPage 16 of 22Pan et al. BMC Bioinformatics           (2023) 24:85 \nThe quantitative comparison results of ISIC-2018 are presented in Table  4, and the \ncorresponding qualitative results are illustrated in Fig.  9. Our method achieved an \nmDice value of 0.9075, mIoU of 0.8441, and recall of 0.9169, reflecting an improvement \nof 2.62%, 1.16%, and 2.26%, respectively, over MSRF-Net. Moreover, our model obtained \na precision value of 0.9165, which is competitive with other models. Our qualitative \nresults revealed that our model could accurately segment skin lesions of varying sizes \nand shapes.\n(5) Comparison on GLAS\nThe quantitative comparison results of GLAS are presented in Table  5, and the cor -\nresponding qualitative results are illustrated in Fig.  9. Our method achieved an mDice \nvalue of 0.9003, mIoU of 0.8247, recall of 0.9025, and precision of 0.9027. Compared \nto DS-TransUNet, our method improved mDice and mIoU by 2.84% and 4.02%, \nTable 5 Comparisons with the state-of-the-art baselines on the GLAS dataset terms\nThe “–” denotes the corresponding result is not provided. For each column, the best results are highlighted\nMethod Year mDice mIoU Recall Precision\nSegNet [47] 2018 0.7861 0.6596 – –\nU-Net [5] 2015 0.7976 0.6763 – –\nResUNet [42] 2018 0.8088 0.6911 0.8511 0.8001\nMedT [31] 2021 0.8102 0.6961 – –\nU-Net++ [7] 2018 0.8113 0.6961 – –\nAttention U-Net [8] 2018 0.8159 0.7006 – –\nKiU-Net [23] 2020 0.8325 0.7278 – –\nDS-TransUNet [14] 2021 0.8719 0.7845 – –\nEG-TransUNet – 0.9003 0.8247 0.9025 0.9027\nTable 6 Generalizability results of the models trained on Kvasir-SEG and tested on CVC-Clinicdb\nMethod Year mDice mIoU Recall Precision\nU-Net [5] 2015 0.6302 0.5015 0.5612 0.8249\nU-Net++ [7] 2018 0.4267 0.3623 0.4337 0.6877\nHRNetV2-W18-Smallv2 [44] 2020 0.6428 0.5513 0.6811 0.7253\nHRNetV2-W48 [44] 2020 0.7901 0.6953 0.8796 0.7694\nMSRF-Net [41] 2021 0.7921 0.6498 0.9001 0.7694\nEG-TransUNet – 0.8939 0.8420 0.9020 0.9147\nTable 7 generalizability results of the models trained on CVC-Clinicdb and tested on Kvasir-SEG\nMethod Year mDice mIoU Recall Precision\nU-Net [5] 2015 0.5621 0.4050 0.4364 0.8466\nU-Net++ [7] 2018 0.6783 0.5494 0.7311 0.6885\nHRNetV2-W18-Smallv2 [44] 2020 0.2107 0.1363 0.2038 0.3347\nHRNetV2-W48 [44] 2020 0.2349 0.2461 0.3372 0.1523\nMSRF-Net [41] 2021 0.7575 0.6337 0.7197 0.8414\nEG-TransUNet – 0.8337 0.7647 0.8600 0.8698\nPage 17 of 22\nPan et al. BMC Bioinformatics           (2023) 24:85 \n \nrespectively. The qualitative results obtained suggested that our model could accu -\nrately segment glands of varying sizes and shapes.\nGeneralization and discussion\nIn medical imaging, the generalization ability refers to the adaptability of algorithms \non datasets from different institutions. In this paper, we used the Kvasir-SEG for \ntraining the model, which was then tested on CVC-ClinicDB. Similarly, we conducted \nthis study on an opposite setup as well, i.e., training on CVC-ClinicDB and testing on \nKvasir-SEG. Tables  4 and 7  show the results of the generalization study. Furthermore, \nwe discuss ablation studies in detail and present a visual analysis.\nGeneralizability results on CVC‑ClinicDB\nThe Table  6 shows the generalization performance results of our model trained on \nKvasir-SEG and tested on CVC-ClinicDB. Our EG-TransUNet achieved an mDice \nvalue of 0.8939, a mIoU of 0.8420, a recall of 0.9020, and a precision of 0.9147. All \nthe above results demonstrated that our model had higher generalizability than \nother SOTA methods. Moreover, the high recall value obtained indicates that our \nTable 8 Ablation study of EG-Transunet on the Kvasir-SEG\nFor each column, the best results are highlighted\nMethod mDice mIoU Recall Precision Flops\nEG-N w/o PEM + CSA + SGA 0.8820 0.8269 0.8956 0.8990 16.3G\nEG-N w/o PEM + SGA 0.9248 0.8796 0.9342 0.9368 16.9G\nPAS w/o PEM + CSA 0.9227 0.8790 0.9258 0.9336 21.8G\nEG-N w/o SGA + CSA 0.9263 0.8815 0.9303 0.9345 18.8G\nEG-N w/o PEM 0.9284 0.8754 0.9388 0.9281 22.2G\nEG-N w/o SGA 0.9301 0.8867 0.9363 0.9404 19.3G\nEG-N w/o CSA 0.9334 0.8849 0.9369 0.9353 24.4G\nEG-TransUNet 0.9344 0.8927 0.9401 0.9436 24.9G\nFig. 10 Qualitative results of PEM\nPage 18 of 22Pan et al. BMC Bioinformatics           (2023) 24:85 \nmodel has high medical sensitivity and can effectively reduce the rates of missed \ndiagnoses.\nGeneralizability results on Kvasir‑SEG\nThe corresponding generalization performance results of our model trained on the \nCVC-ClinicDB dataset and tested on the Kvasir-SEG dataset are shown in Table  7. \nOur EG-TransUNet obtained an mDice value of 0.8337, mIoU of 0.7647, recall of \n0.8600, and precision of 0.8698, which outperformed other SOTA methods in all pre -\nsented metrics. Our method outperformed the second performing method MSRF-\nNet by 7.62% in mDice, 13.1% in mIoU, 14.03% in recall, and 2.84% in precision.\nAblation study\nWe conducted an ablation study on the Kvasir-SEG data set to demonstrate the effects \nof PEM, semantic guided attention, and CSA with floating-point calculations. The \nFig. 11 Qualitative results of CSA\nFig. 12 Qualitative results of SGA\nPage 19 of 22\nPan et al. BMC Bioinformatics           (2023) 24:85 \n \nrelated quantitative results are shown in Table  8 and the qualitative results are shown \nin Figs. 10, 11, and 12.\nThe U-shaped network was the benchmark network used in this paper, which is rep -\nresented as “EG-N W/O PEM + CSA + SGA, ” while “EG-N” is considered as the “full” \nmodel. The results of “EG-N w/o PEM + SGA, ” “EG-N w/o PEM + CSA, ” and “EG-N w/o \nSGA + CSA” indicated that the proposed PEM, CSA, and SGA modules could improve \nthe segmentation quality, with almost equally effectiveness. In terms of Flops in Table  8, \nEG-TransUNet has only half more computation than the baseline, with no differences \nregarding the point of computation and magnitude, while greatly improving the segmen-\ntation performance.\nThe spatial feature before and after the PEM were visualized for some channels, and \nthe 84th, 103rd, 175th, and 203rd channels were randomly selected, as shown in Fig.  10. \nBy comparing the above and below, it can be seen that PEM could remove and under -\nmine irrelevant texture information and enhance relevant texture information, confirm -\ning the effectiveness of PEM in enhancing feature expression and improving feature \ndiscrimination.\nThe 14th, 234th, 436th, and 701st channels were randomly selected to visualize the \nfeature before and after CSA, as shown in Fig.  11. An updown comparison showed that \nCSA could optimize the semantic feature and improve the accuracy of semantic location \ninformation of the input medical images.\nSome images of the Kvasir-SEG dataset were selected for SGA visualization analysis. \nAs shown in Fig. 12, the overall contrast of the feature map decreased after SGA, but the \nvalue of the target region increased, thus providing more important texture information \nin the feature fusion stage. This in turn confirmed the effectiveness of SGA in reducing \nsemantic gap and promoting the effect of feature fusion.\nWe also performed additional ablation studies by removing single module to further \nverify the effectiveness of our work. Compared to EG-TransUNet, the evaluation scores \nof “EG-N w/o PEM, ” “EG-N w/o SGA, ” and “EG-N w/o CSA” were reduced to varying \ndegrees, demonstrating the effectiveness and necessity of each module. Our experi -\nmental results clearly show that all three modules could enhance each other and jointly \nimprove the segmentation effect.\nFig. 13 Visualizations of feature maps produced by vanilla U-Net and the proposed EG-TransUNet in different \ndecoder stages based on the Kvasir-SEG dataset\nPage 20 of 22Pan et al. BMC Bioinformatics           (2023) 24:85 \nVisualizations of the decoder stages\nCompared with vanilla U-Net, the proposed EG-TransUNet benefits greatly from the \nlong-range feature dependencies and global contextual information. To further verify the \nability of the proposed EG-TransUNet, we visualized feature maps from each decoder \nstage for both U-Net and EG-TransUNet, as illustrated in Fig. 13.\nThe following observations can be made based on the comparative results: (1) With \nthe deepening of the decoding process, the high-level semantic feature and the low-\nlevel texture feature could be combined and gradually improve the edge part. In addi -\ntion, the resolution of the decoded image was gradually improved, and the details of the \nimage were gradually enriched, rendering the decoded image closer to the real result. (2) \nWhen PEM and CSA were used, the comparison of the decoding images in the first stage \nclearly suggests that the method in this chapter provided clearer location information \nand edge information compared with vanilla U-Net. (3) Although the decoding image of \nvanilla U-Net is gradually improved and clear, the fuzzy edge and unclearness were still \nunsolved. This finding underlines that the spatial feature was not fully learned to provide \na discriminative edge of the lesion area and did not exploit the spatial texture details \nduring the feature fusion. By comparison, the edge of EG-TransUNet was clear and the \ndecoding effect was better due to SGA.\nWe believe that the application of EG-TransUNet architecture should not only be \nlimited to biomedical image segmentation, but also be extended to natural image seg -\nmentation and other pixel-level classification tasks; however, further detailed valida -\ntions will be necessary.\nConclusion\nIn this paper, we propose a novel U-Net variant with Transformer, called EG-TransU -\nNet, which implements the PEM, the feature fusion module based on semantic guid -\nance attention, and the CSA module into U-Net simultaneously, and can thus greatly \nimprove the segmentation quality of biomedical images.\nIn particular, PEM could enhance information with a stronger representation of \nthe target location, optimize the inference process of fuzzy edge information, and \nimprove feature discrimination effectively.\nMeanwhile, SGA could explore and exploit the relationship between semantic and \nspatial texture information, eliminate the semantic gap, and realize an effective fusion \nof spatial texture and semantic information.\nIn addition, CSA could effectively capture the long-range contextual information in \nchannel and spatial levels by using the self-attention mechanism, which improves the \nrepresentation ability of the semantic feature and the accuracy of semantic location \ninformation.\nCompared with previous advanced works, the proposed EG-TransUNet greatly \nbenefits from the long-range feature dependencies of the transformer, ensuring the \ndiscriminative representations of the spatial feature, the accuracy of semantic loca -\ntion information, and efficient feature fusion. Consequently, we can effectively miti -\ngate problems occurring when using the traditional U-shape architecture and obtain \na competitive segmentation and generalization performance. In clinical practice, \nPage 21 of 22\nPan et al. BMC Bioinformatics           (2023) 24:85 \n \nthe network proposed in this paper has the ability to extract reliable discriminative \nfeatures and fuse spatial and semantic information. At the same time, it can reduce \nvarious noise interference in medical image data and provide reliable high-preci -\nsion medical image segmentation results that can significantly improve diagnostic \naccuracy.\nAcknowledgements\nThe authors would like to thank the supercomputing system in the Supercomputing Center of Wuhan University for the \nnumerical calculations.\nAuthor contributions\nXL, NX, YC, SP designed and conceived the study. XL conducted the experiments, XL and SP analysed the results. XL \ndrafted the manuscript. SP , YC and NX reviewed the manuscript and provided critical feedback. All authors read and \napproved the final manuscript.\nFunding\nThis work was supported by the National Natural Science Foundation of China (Grant Nos. 62072345, 41671382), LIES-\nMARS Special Research Funding.\nAvailability of data and materials\nAll datasets used in this paper are publicly available. The Kvasir-SEG is publicly available at https:// datas ets. simula. no// \nkvasir- seg/# downl oad. The CVC-ClinicDB is publicly available at https:// polyp. grand- chall enge. org/ CVCCl inicDB/. The \n2018 Data Science Bowl is publicly available at https:// www. kaggle. com/ compe titio ns/ data- scien ce- bowl- 2018/ data. \nThe ISIC -2018 Challenge is publicly available at https:// chall enge. isic- archi ve. com/ data/# 2018. The GLAS is publicly avail-\nable at https:// warwi ck. ac. uk/ fac/ cross_ fac/ tia/ data/ glasc ontest/ downl oad/.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 11 July 2022   Accepted: 20 February 2023\nReferences\n 1. Chakraborty C, Kishor A, Rodrigues J. Novel enhanced-grey wolf optimization hybrid machine learning technique \nfor biomedical data computation. Comput Electr Eng. 2022;99:107778.\n 2. Kishor A, Chakraborty C, Jeberson W. Reinforcement learning for medical information processing over heterogene-\nous networks. Multimed Tools Appl. 2021;80:23983–4004.\n 3. Chakraborty C, Kishor A. Real-time cloud-based patient-centric monitoring using computational health systems. \nIEEE Trans Comput Social Syst. 2022;9(6):1613–23.\n 4. Shelhamer E, Long J, Darrell T. Fully convolutional networks for semantic segmentation. IEEE Trans Pattern Anal \nMach Intell. 2017;39:640–51.\n 5. Ronneberger O, Fischer P , Brox T. U-Net: convolutional networks for biomedical image segmentation. In: MICCAI. \n2015.\n 6. Zeiler MD, Krishnan D, Taylor GW, Fergus R. Deconvolutional networks. In: 2010 IEEE Computer Society Conference \non Computer Vision and Pattern Recognition. 2010. pp. 2528–2535\n 7. Zhou Z, Siddiquee MMR, Tajbakhsh N, Liang J. UNet++: a nested U-Net architecture for medical image segmenta-\ntion. In: Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th Inter-\nnational Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, held in conjunction with MICCAI. \nGranada; 2018. pp. 3–11, 2018.\n 8. Oktay O et al. Attention U-Net: learning where to look for the pancreas. 2018. arXiv:abs/1804.03999.\n 9. Chen Y, et al. Channel-Unet: a spatial channel-wise convolutional neural network for liver and tumors segmentation. \nFront Genet. 2019;10:1110.\n 10. Sun H et al. AUNet: attention-guided dense-upsampling networks for breast mass segmentation in whole mam-\nmograms. Phys Med Biol. 2019.\n 11. Vaswani A et al. Attention is all you need. 2017. arXiv:abs/1706.03762.\n 12. Chen J et al. TransUNet: transformers make strong encoders for medical image segmentation. 2021. \narXiv:abs/2102.04306.\n 13. Chen B, Liu Y, Zhang Z, Lu G, Zhang D. TransAttUnet: multi-level attention-guided U-Net with transformer for medi-\ncal image segmentation. 2021. arXiv:abs/2107.05274.\nPage 22 of 22Pan et al. BMC Bioinformatics           (2023) 24:85 \n 14. Lin A-J, Chen B, Xu J, Zhang Z, Lu G. DS-TransUNet: dual swin transformer U-Net for medical image segmentation. \n2021. arXiv:abs/2106.06716.\n 15. Liu Z et al. Swin transformer: hierarchical vision transformer using shifted windows. In: 2021 IEEE/CVF International \nConference on Computer Vision (ICCV). 2021. pp. 9992–10002.\n 16. Caicedo JC, et al. Nucleus segmentation across imaging experiments: the 2018 data science bowl. Nat Methods. \n2019;16:1247–53.\n 17. Codella NCF et al. Skin lesion analysis toward melanoma detection 2018: a challenge hosted by the international \nskin imaging collaboration (ISIC). 2019. arXiv:abs/1902.03368.\n 18. Sirinukunwattana K, et al. Gland segmentation in colon histology images: the GLAS challenge contest. Med Image \nAnal. 2017;35:489–502.\n 19. Benjelloun M, Adoui ME, Larhmam MA, Mahmoudi SA. Automated breast tumor segmentation in DCE-MRI using \ndeep learning. In: 2018 4th International Conference on Cloud Computing Technologies and Applications (Cloud-\ntech), 2018. pp 1–6.\n 20. Huang H et al. UNet 3+: a full-scale connected UNet for medical image segmentation. In: ICASSP 2020 - 2020 IEEE \nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. pp. 1055–1059.\n 21. Li X, Chen H, Qi X, Dou Q, Fu CW, Heng PA. H-DenseUNet: hybrid densely connected UNet for liver and tumor seg-\nmentation from CT volumes. IEEE Trans Med Imaging. 2018;37:2663–74.\n 22. Qin X, Zhang ZV, Huang C, Dehghan M, Zaiane OR, Jägersand M. \"U2-Net: going deeper with nested U-structure for \nsalient object detection. 2020. arXiv:abs/2005.09007.\n 23. Jose VJM, Sindagi VA, Hacihaliloglu I, Patel VM. KiU-Net: towards accurate segmentation of biomedical images using \nover-complete representations. MICCAI. 2020.\n 24. Cai Y, Wang Y. MA-Unet: an improved version of Unet based on multi-scale and attention mechanism for medical \nimage segmentation. 2022. arXiv:abs/2012.10952.\n 25. Jha D, Riegler M, Johansen D, Halvorsen P , Johansen HD. DoubleU-Net: a deep convolutional neural network for \nmedical image segmentation. In: 2020 IEEE 33rd International Symposium on Computer-Based Medical Systems \n(CBMS), 2020. pp. 558–564.\n 26. Chen LC, Zhu Y, Papandreou G, Schroff F, Adam H. Encoder-decoder with atrous separable convolution for semantic \nimage segmentation. 2018. arXiv:abs/1802.02611.\n 27. Tang Y, Tang Y, Xiao J, Summers RM. XLSor: a robust and accurate lung segmentor on chest X-rays using criss-cross \nattention and customized radiorealistic abnormalities generation. MIDL. 2019.\n 28. Chen X, Yao L, Zhang Y. Residual attention U-Net for automated multi-class segmentation of COVID-19 chest CT \nimages. 2020. arXiv:abs/2004.05645.\n 29. Tomar NK et al. FANet: a feedback attention network for improved biomedical image segmentation. IEEE Trans \nNeural Netw Learn Syst. 2022.\n 30. Zhang Y, Liu H, Hu Q. TransFuse: fusing transformers and CNNs for medical image segmentation. MICCAI. 2021.\n 31. Valanarasu JMJ, Oza P , Hacihaliloglu I, Patel VM. Medical transformer: gated axial-attention for medical image seg-\nmentation. MICCAI. 2021.\n 32. Wang H, Zhu Y, Green B, Adam H, Yuille AL, Chen L-C. Axial-DeepLab: stand-alone axial-attention for panoptic seg-\nmentation. ECCV. 2020.\n 33. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: 2016 IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), 2016. pp. 770–778.\n 34. Woo SH, Park J, Lee JY, Kweon IS. CBAM: convolutional block attention module. Lect Notes Comput Sc. 2018;11211:3–19.\n 35. Shaw P , Uszkoreit J, Vaswani A. Self-attention with relative position representations. NAACL. 2018.\n 36. Jha D et al. Kvasir-SEG: a segmented polyp dataset. 2020. arXiv:abs/1911.07069\n 37. Bernal J, Sánchez FJ, Fernández-Esparrach G, Gil D, Miguel CRD, Vilariño F. WM-DOVA maps for accurate polyp high-\nlighting in colonoscopy: validation vs. saliency maps from physicians. Comput Med Imaging Graph Off J Comput \nMed Imaging Soc. 2015;43:99–111.\n 38. Pogorelov K et al. KVASIR: a multi-class image dataset for computer aided gastrointestinal disease detection. In: \nProceedings of the 8th ACM on Multimedia Systems Conference, 2017.\n 39. Milletari F, Navab N, Ahmadi S-A. V-Net: fully convolutional neural networks for volumetric medical image segmen-\ntation. In: 2016 Fourth International Conference on 3D Vision (3DV), 2016. pp. 565–571.\n 40. Paszke A et al. PyTorch: an imperative style, high-performance deep learning library. NeurIPS, 2019.\n 41. Srivastava A et al. MSRF-Net: a multi-scale residual fusion network for biomedical image segmentation. IEEE J \nBiomed Health Inf. 2021.\n 42. Zhang Y, Tian Y, Kong Y, Zhong B, Fu YR. Residual dense network for image super-resolution. In: 2018 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2018. pp. 2472–2481.\n 43. Jha D et al. ResUNet++: an advanced architecture for medical image segmentation. In: 2019 IEEE International \nSymposium on Multimedia (ISM), 2019. pp. 225–2255.\n 44. Wang J, et al. Deep high-resolution representation learning for visual recognition. IEEE Trans Pattern Anal Mach \nIntell. 2021;43:3349–64.\n 45. Li Q et al. Colorectal polyp segmentation using a fully convolutional neural network. In: 2017 10th International \nCongress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), 2017. pp. 1–5.\n 46. Nguyen Q-T, Lee S-W. Colorectal segmentation using multiple encoder-decoder network in colonoscopy images. In: \n2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE), 2018. pp. 208–211.\n 47. Badrinarayanan V, Kendall A, Cipolla R. SegNet: a deep convolutional encoder-decoder architecture for image \nsegmentation. IEEE Trans Pattern Anal Mach Intell. 2017;39:2481–95.\n 48. Fan DP et al. PraNet: parallel reverse attention network for polyp segmentation. 2020. arXiv:abs/2006.11392.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.499370813369751
    },
    {
      "name": "Image segmentation",
      "score": 0.47545358538627625
    },
    {
      "name": "DNA microarray",
      "score": 0.465999960899353
    },
    {
      "name": "Segmentation",
      "score": 0.4575161337852478
    },
    {
      "name": "Transformer",
      "score": 0.4430127441883087
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4154367446899414
    },
    {
      "name": "Computational biology",
      "score": 0.3469197750091553
    },
    {
      "name": "Biology",
      "score": 0.2539038062095642
    },
    {
      "name": "Engineering",
      "score": 0.17587289214134216
    },
    {
      "name": "Genetics",
      "score": 0.10860994458198547
    },
    {
      "name": "Electrical engineering",
      "score": 0.07940417528152466
    },
    {
      "name": "Gene",
      "score": 0.0784626305103302
    },
    {
      "name": "Gene expression",
      "score": 0.077593594789505
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}