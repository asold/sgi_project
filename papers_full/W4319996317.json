{
  "title": "Fine-Tuning Swin Transformer and Multiple Weights Optimality-Seeking for Facial Expression Recognition",
  "url": "https://openalex.org/W4319996317",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2127811339",
      "name": "Hongqi Feng",
      "affiliations": [
        "Changzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2112653396",
      "name": "Wei-Kai Huang",
      "affiliations": [
        "Changzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2120339633",
      "name": "Denghui Zhang",
      "affiliations": [
        "Zhejiang Shuren University"
      ]
    },
    {
      "id": "https://openalex.org/A4209881040",
      "name": "Bangze Zhang",
      "affiliations": [
        "Zhejiang University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4388315058",
    "https://openalex.org/W2799041689",
    "https://openalex.org/W2802907016",
    "https://openalex.org/W2724899977",
    "https://openalex.org/W3199434393",
    "https://openalex.org/W4282972532",
    "https://openalex.org/W2904483377",
    "https://openalex.org/W3003720578",
    "https://openalex.org/W3035336958",
    "https://openalex.org/W3184571633",
    "https://openalex.org/W3194949249",
    "https://openalex.org/W3209798173",
    "https://openalex.org/W6632670727",
    "https://openalex.org/W2944523338",
    "https://openalex.org/W2982153880",
    "https://openalex.org/W6675224631",
    "https://openalex.org/W2145310492",
    "https://openalex.org/W2016485878",
    "https://openalex.org/W6864202710",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6749107692",
    "https://openalex.org/W6796913975",
    "https://openalex.org/W6810127701",
    "https://openalex.org/W2738672149",
    "https://openalex.org/W2481681431",
    "https://openalex.org/W2745497104",
    "https://openalex.org/W2341528187",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W6745136726",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3118530108",
    "https://openalex.org/W3173787706",
    "https://openalex.org/W3122081138",
    "https://openalex.org/W2101176070",
    "https://openalex.org/W2963173418",
    "https://openalex.org/W4394645697",
    "https://openalex.org/W3124054989",
    "https://openalex.org/W3101998545"
  ],
  "abstract": "Facial expression recognition plays a key role in human-computer emotional interaction. However, human faces in real environments are affected by various unfavorable factors, which will result in the reduction of expression recognition accuracy. In this paper, we proposed a novel method which combines Fine-tuning Swin Transformer and Multiple Weights Optimality-seeking (FST-MWOS) to enhanced expression recognition performance. FST-MWOS mainly consists of two crucial components: Fine-tuning Swin Transformer (FST) and Multiple Weights Optimality-seeking (MWOS). FST takes Swin Transformer Large as the backbone network to obtain multiple groups of fine-tuned model weights for the homologous data domains by hyperparameters configurations, data augmentation methods, etc. In MWOS a greedy strategy was used to mine locally optimal generalizations in the optimal epoch interval of each group of fine-tuned model weights. Then, the optimality-seeking for multiple groups of locally optimal weights was utilized to obtain the global optimal solution. Experiments results on RAF-DB, FERPlus and AffectNet datasets show that the proposed FST-MWOS method outperforms various state-of-the-art methods.",
  "full_text": "Received 20 December 2022, accepted 15 January 2023, date of publication 18 January 2023, date of current version 2 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3237817\nFine-Tuning Swin Transformer and Multiple\nWeights Optimality-Seeking for Facial\nExpression Recognition\nHONGQI FENG\n 1, WEIKAI HUANG\n 1, DENGHUI ZHANG\n 2, AND BANGZE ZHANG\n3\n1School of Computer Science and Artificial Intelligence, Changzhou University, Changzhou 213100, China\n2College of Information Technology, Zhejiang Shuren University, Hangzhou 310000, China\n3School of Computer Science and Technology, Zhejiang University of Technology, Hangzhou 310023, China\nCorresponding author: Denghui Zhang (dhzhang@zjsru.edu.cn)\nThis work was supported in part by the Zhejiang Provincial Natural Science Foundation of China under Agreement LGF21F020024; and in\npart by the Natural Science Foundation of Zhejiang Province, China, under Agreement LQ21F020025.\nABSTRACT Facial expression recognition plays a key role in human-computer emotional interaction.\nHowever, human faces in real environments are affected by various unfavorable factors, which will\nresult in the reduction of expression recognition accuracy. In this paper, we proposed a novel method\nwhich combines Fine-tuning Swin Transformer and Multiple Weights Optimality-seeking (FST-MWOS)\nto enhanced expression recognition performance. FST-MWOS mainly consists of two crucial components:\nFine-tuning Swin Transformer (FST) and Multiple Weights Optimality-seeking (MWOS). FST takes Swin\nTransformer Large as the backbone network to obtain multiple groups of fine-tuned model weights for the\nhomologous data domains by hyperparameters configurations, data augmentation methods, etc. In MWOS a\ngreedy strategy was used to mine locally optimal generalizations in the optimal epoch interval of each group\nof fine-tuned model weights. Then, the optimality-seeking for multiple groups of locally optimal weights\nwas utilized to obtain the global optimal solution. Experiments results on RAF-DB, FERPlus and AffectNet\ndatasets show that the proposed FST-MWOS method outperforms various state-of-the-art methods.\nINDEX TERMS Facial expression recognition, greedy strategy, multiple weights optimality-seeking, swin\ntransformer.\nI. INTRODUCTION\nFacial expression is one of the most natural, powerful and\nuniversal signals for human beings to express their emotional\nstates and intentions [1]. Facial expression recognition\ntechnology has a wide range of applications in the field of\nhuman-computer interaction such as social robots, medical\ndiagnosis and fatigue monitoring [2]. With the increasing\nnumber of people living alone, how to provide them with\nemotional comfort has become a key concern for the\nsociety [3]. Many researchers have focused on emotionally\ninteractive robots [4], [5], [6]. However, in the process of\nreal human-computer interaction, the human face is usually\naffected by various interference factors, which undoubtedly\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Syed Islam\n.\nincreases the difficulty of expression recognition. As facial\nexpression recognition methods have been intensively stud-\nied, many researchers have introduced attention mechanisms\nto perceive occlusion and posture changes [7], [8], designed\nmethods to suppress label annotation ambiguity [9], [10].\nIn addition, Visual Transformer (ViT) is also applied to facial\nexpression recognition [11], [12] to enhance the correlation\nbetween detailed features and achieve the most advanced\nfacial expression recognition performance. However, the\nrecognition performance using only the best-performing\nindividual model is approaching a bottleneck. Meanwhile the\nremaining sub-optimal models which took a lot of time and\nresources to obtain are unable to achieve their value.\nIn order to tap the value in the multi-group model to\neffectively improve the accuracy of facial expression recog-\nnition, we proposed a method that combines Fine-tuning\nVOLUME 11, 2023 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 9995\nH. Feng et al.: Fine-Tuning Swin Transformer and Multiple Weights Optimality-Seeking for Facial Expression Recognition\nSwin Transformer and Multiple Weights Optimality-seeking\n(FST-MWOS). FST-MWOS mainly consists of two cru-\ncial components: Fine-tuning Swin Transformer (FST) and\nMultiple Weights Optimality-seeking (MWOS). These two\nkey components complement each other to maximize facial\nexpression recognition performance. First, Swin Transformer\nLarge (Swin-L) is used as the backbone network to extract\nthe basic facial features while enhancing the correlation\nbetween the feature sequences. Then, the classification head\nis adjusted to apply to the expression recognition task. Next,\nmultiple groups of model weights for the homologous data\ndomains are obtained by fine-tuning hyperparameters and\ndata augmentation. Finally, MWOS employs local greedy\nstrategy and global greedy strategy to filter invalid weight\ngroups and to mine the local-global optimal recognition\nperformance.\nOverall, our main contributions are summarized as follows:\n• A novel FST-MWOS method was proposed to perform\nFER. FST-MWOS can improve the accuracy of expres-\nsion recognition by finding the optimal extreme points\nfrom multiple fine-tuned model weights of homologous\ndata domains.\n• We designed a simple but effective multiple weights\noptimality-seeking component named MWOS. This\ncomponent combines local greedy strategy and global\ngreedy strategy to select information about model\nweights that play a positive role in recognition perfor-\nmance.\n• Our FST-MWOS method is extensively evaluated on\nthe three public FER datasets. FST-MWOS achieved\n90.38%, 90.41% and 63.33% recognition accuracy on\nthe RAF-DB, FERPlus and AffectNet, outperforms a\nvariety of state-of-the-art methods.\nII. RELATED WORK\nA. FACIAL EXPRESSION RECOGNITION\nWith the development of deep learning, learning-based\nmethods [13], [14], [15] have replaced traditional manual\nmethods [16], [17], [18] with their powerful feature extraction\ncapabilities. However, facial images are subject to multiple\nuncertainties in the wild, and it’s difficult to extract dis-\ncriminative features used CNN alone. For this reason, some\nresearchers have applied attention mechanisms to the net-\nwork. Li et al. [7] proposed a CNN with attention mechanism\n(ACNN) that can perceive the occlusion regions of the face\nand focus on the most discriminative un-occluded regions.\nWang et al. [8] proposed a Region Attention Networks\n(RAN), to adaptively capture the importance of facial regions\nfor occlusion and pose variant. In addition to addressing\nfacial occlusion and posture variation, several methods work\nto suppress label annotation ambiguity due to subjective\nannotations and inter-class similarity of facial expressions.\nWang et al. [9] proposed a Self-Cure Network (SCN)\nwith rank regularization and lowest-ranked group relabeling,\nwhich suppresses the uncertainties efficiently. Ruan et al. [10]\nproposed a Feature Decomposition and Reconstruction\nLearning method (FDRL), which reconstructs the expression\nfeatures with similarity by decomposes the basic features to\nperceive latent features and captures intra-feature and inter-\nfeature relationships for latent features. With the rise of\nVision Transformer in facial expression recognition tasks,\nHuang et al. [11] combined a grid-wise attention mechanism\nand Visual Transformer to learn feature dependencies and\nglobal representations. Ma et al. [12] proposed the Visual\nTransformers with Feature Fusion (VTFF), which used a\nmulti-layer Transformer encoder to strengthen the correlation\nbetween multi-branch fusion features, effectively improves\nthe accuracy of expression recognition. Li et al. [19] designed\na pure transformer-based mask vision transformer (MVT) to\nfilter out complex backgrounds and occlusion of face images,\nand correct incorrect expression labels. However, the current\nmainstream learning-based FER methods focuses only on\npick the individual model which performs best on a held-out\nvalidation set, ignore the correlation between multiple groups\nof fine-tuned weights.\nB. TRANSFORMER IN COMPUTER VISION\nTransformer [20] presents a dominant position in the NLP\nfield with its superior sequence modeling capability and\nglobal information perception. Some researchers have tried\nto apply Transformer to computer vision tasks. Dosovit-\nskiy et al. [21] segmented the input images into multiple\npatches and flattened them into 1D patch embeddings, which\nwere fed to the Transformer encoder. This is the first time\nTransformer has been applied to an computer vision task.\nHe et al. [22] demonstrates the great promise of Vision\nTransformer by using ViT as an encoder to improve training\nspeed and recognition performance. Researchers continued\nto explore deeper and obtained many variants of ViT\n[23], [24], [25]. Liu et al. [26] proposed Swin Transformer,\nwhich designed a shifted window scheme allows flexibility\nin modeling image features at various scales and enhances\nlong-term dependencies between feature sequences. Our FST\ncomponent is fine-tunes the Swin Transformer to make it\nsuitable for facial expression recognition in human-computer\ndialogue scenarios.\nC. WEIGHTS OPTIMALITY\nSeveral researchers have worked on designing weight\noptimization methods to maximize the accuracy of the\nmodels, which are widely used in various tasks because\nthey brings no extra burden for inference and memory.\nIzmailov et al. [27] proposed a Stochastic Weight Averaging\n(SWA) method, which simple averaging of multiple points\nalong the trajectory of gradient. Cha et al. [28] by a more\ndense random weight sampling strategy to suppress the\noverfitting phenomenon present in SWA. But, all the above\nmethods only studied the weights of individual models.\nWortsman et al. [29] further improves the performance of\nthe model by obtaining multiple groups of fine-tuned model\nweights with different hyper-parameter configurations and\nmining the optimal extremes uses a greedy averaging strategy.\n9996 VOLUME 11, 2023\nH. Feng et al.: Fine-Tuning Swin Transformer and Multiple Weights Optimality-Seeking for Facial Expression Recognition\nFIGURE 1. Overview of our proposed FST-MWOS method. The input FER dataset is fed into the fine-tuned Swin-L to extract and strengthen the\ncorrelation between the facial feature vectors, thereby obtaining multiple sets of fine-tuned model weights. Then, the MWOS component mines the\noptimal generalization from the multiple sets of weights by local greedy and global greedy strategies.\nInspired by this mind, we hypothesize that it is feasible and\neffective to improve expression recognition performance by\ngreedy strategy from a local-global perspective.\nIII. OUR METHOD\nA. OVERVIEW\nAn overview of the proposed method is shown in Fig 1.\nGiven a training samples of facial expression recognition\ndatabase D = (Xi, yi), where Xi is the input image, its size is\n3×H ×W , and yi ∈ {1, . . . ,N} is its corresponding label for\na N-class expression recognition problem. First, let the deep\ndependency feature map X∗\ni ∈ RCout ×(H∗W ) be the output\nof a Swin-L network pre-trained on ImageNet22K database,\nwhere Cout = 1536 is the number of output channels.\nThen, a classification head (contains average pooling layer\nand fully connected layer) take X∗\ni as input and mapped\nit to a raw score vector xi ∈ RN×1. The probability\ndistribution of an N-class expression recognition P (y = p|xi)\nis then calculated using the softmax function. At the same\ntime, the model weights Wi =\n{\nWi1, . . . ,Wiep\n}\nfor each\nepoch of the training process are saved, where ep is total\nnumber of training epochs. Fine-tune the hyper-parameter\nconfigurations S = {S1, . . . ,Sn} and repeat the above process\nto obtain n groups of model weights W = {W1, . . . ,Wn} for\nthe homologous data domains but with different recognition\nperformance. Finally, a local-global greedy strategy is used\nto mine the most superior performing Wbest among multiple\ngroups of fine-tuned model weights, which will be introduced\nin Section III-B in detail.\nB. MULTIPLE WEIGHTS OPTIMALITY-SEEKING\nThe current mainstream FER methods pick only the individ-\nual model with the most superior performance and ignore the\nvalue of the remaining model weights. Thus, we designed the\nMWOS component, which consists of local greedy strategy\nand global greedy strategy. It is to mine the best performance\nof the multi-group fine-tuned model weights with no extra\nburden for inference and memory.\n1) LOCAL GREEDY STRATEGY (LGS)\nLGS seeks weight-optimal generalization from the best\nepoch interval of a individual model, as can be seen\nin Fig 2. Given a groups of model weights WmBEI ={\nWm(best−k), . . . ,Wmbest , . . . ,\nWm(best+k)\n}\n, where Wmbest indicates the model weights of\nthe best performing epochs of the training process and\n[best − k, best + k] notes upper and lower limits of the\ninterval, the k is set to 4 in this paper. With equation (1),\nwe compute the Wmg as the groups of local greed weights,\nthat is:\nWmg = Greedy Strategy (WmBEI ) . (1)\nwhere the implementation steps of Greedy Strategy are\nrepresented in Algorithm 1:\n1) Sort the groups of model weights in decreasing order of\nperformance on the validation set;\n2) Initialize the greedy model weights Wg = {} and best\nperformance on the validation set Bestval = 0;\n3) Add Wg in order and compare to Bestval ;\n4) Only model weights that have a positive improvement\non recognition performance are retained.\nBased on equation (2), we use an average of Wmg to\ndetermine the local optimal generalization Wm:\nWm = 1\ng\ng∑\ni=1\nWmgi, 0 < g ≤ (2k + 1) . (2)\nVOLUME 11, 2023 9997\nH. Feng et al.: Fine-Tuning Swin Transformer and Multiple Weights Optimality-Seeking for Facial Expression Recognition\nFIGURE 2. Our local greedy strategy collects weights from best epoch\ninterval, i.e. from extreme value point and its upper and lower k epochs.\nwhere g is the number of model weights being retained\nin Wmg.\n2) GLOBAL GREEDY STRATEGY (GGS)\nGGS aims to improve recognition performance by screening\nout valuable information from multiple groups of local\noptimal generalizations. Given a sets of hyperparameter and\ndata augmentation configurations S = {S1, . . . ,Sn}, fine-\ntune the Swin-L network to obtain a groups of local greedy\nweights W = {W1, . . . ,Wn}, that is:\nW = LGS (FST (W0, S)) . (3)\nwhere W0 denote pre-trained initialization (pre-trained on\nImageNet22K database), the n is set to 15 in this paper. Next,\nthe Greedy Strategy is used to find the groups of global\ngreedy weights Wg that has a positive effect on the recognition\nperformance, that is:\nWg = GreedyStrategy (W ) . (4)\nFinally, averaging over the Wg to obtain the local-global\noptimal extremum Wbest , that is:\nWbest = 1\nh\nh∑\ni=1\nWgi, 0 < h ≤ n. (5)\nwhere h is the number of model weights being retained\nin Wg.\nIV. EXPERIMENTS\nIn this section, we first describe three publicly available wild\nFER datasets (i.e., RAF-DB, FERPlus and AffectNet). Then,\nwe carry out extensive experiments on these three wild FER\ndatasets and compared our method with various state-of-the-\nart methods. Finally, the impact of each component of the\nproposed FST-MWOS method is investigated with ablation\nstudies.\nA. DATASETS\n1) RAF-DB\nReference [30] is a real-world FER database, which contains\n30,000 images. Images are annotated with basic or compound\nAlgorithm 1Greedy Strategy.\nInput: Multiple model weights W = {W1, . . . ,Wn} and\nvalidation samples on datasets\nOutput: Greedy model weights Wg = {W1, . . . ,Wk } , k ∈\n{1, . . . ,n} and best ValAcc Bestval\n/* Optimality-seeking */\n1: Weights are sorted in decreasing order of ValAcc(Wi)\n2: Wg ← {}, Bestval ← 0\n3: for i = 1 to n do\n4: if ValAcc(avg(Wg ∪ {Wi})) >\nBestval then\n5: Wg ← Wg ∪ {Wi},\nBestval ← ValAcc(avg(Wg))\n6: end if\n7: end for\n8: return Wg, Bestval\nFIGURE 3. Samples from RAF-DB, FERPlus and AffectNet. RAF-DB is with\nseven basic expression labels, FERPlus and AffectNet are with eight\nexpression labels, contains the contempt category.\nexpressions by 40 trained human annotators. RAF-DB\ncontains two different subsets: single-label subset and multi-\nlabel subset. In our experiments, we only use single-label\nsubset, including 6 basic emotions and 1 neutral emotion. The\nimages from the single-label subset involves 12,271 images\nfor training and 3,068 images for testing. Example of the\nsamples is shown in Fig 3.\n2) FERPlus\nReference [31] is extended from the original FER2013\ndataset, which contains 28,709 training images and 3,589\ntest images. Each facial image is labelled by 10 crowd-\nsourced annotators voting. For a fair comparison, the\nmajority voting mode is picked to obtain the annota-\ntion for each image (removal of unknown images and\nnon-face images). Example of the samples is shown in\nFig 3.\n3) AffectNet\nReference [32] is currently the largest FER dataset with\nmore than 450,000 images facial images collected from\nthe Internet, which have been annotated manually. In our\nexperiment, we utilize images are annotated with eight basic\nexpressions, providing 286621 images for training and 4,000\nimages for testing. Example of the samples is shown in\nFig 3.\n9998 VOLUME 11, 2023\nH. Feng et al.: Fine-Tuning Swin Transformer and Multiple Weights Optimality-Seeking for Facial Expression Recognition\nFIGURE 4. Multiple Weights optimality-seeking performance distribution on RAF-DB, FERPlus and AffectNet datasets.\nB. DATA AUGMENTATION\n1) RESIZED CROP\nis a data augmentation method that comes with the Pytorch\ntoolbox. It randomly crop a new image on the original image\naccording to the given H × W .\n2) RANDOM ERASING\nReference [34], a data augmentation method which effective\nreduces the risk of over-fitting. It randomly selects a rectangle\nregion in an image and erases its pixels with random values.\nIn this process, the training samples with different occlusion\nlevels can be generated by adjusting the erasure values.\n3) MIX UP\nReference [35] is a simple and data-agnostic data augmen-\ntation method. It generates new sample-label data\n(˜X,˜y\n)\nby\nsumming two sample-label data pairs (Xi, yi) and\n(\nXj, yj\n)\nproportionally to increase the generalization ability of the\nmodel.\nC. IMPLEMENTATION DETAILS\nFor each datasets, all the facial images are detected and\ncropped by MTCNN [33], and the cropped images are further\nresized to the size of 256 × 256. At training time, the input\nfacial images are randomly cropped to the size of 224 × 224\nand we augment these by horizontal flips, random erasing and\nmix up. During the test precess, we obtain the input images\nof size 224 × 224 by center crop.\nIn our experiments, the FST-MWOS method is imple-\nmented with the Pytorch toolbox and conduct all the experi-\nments on a single NVIDIA GTX 3090Ti GPU card. We use\nthe Swin-L network as the backbone, which pre-trained on the\nImageNet22K dataset. The AdamW optimizer [36] is used to\noptimize the whole networks with a batch size of 32 and train\nthe model for 50 epochs on three datasets. We use a linear\nlearning rate warmup of 5 epochs and cosine learning rate\ndecay. The label smoothing cross-entropy loss [37] is utilized\nto supervise the model to generalize well for expression\nrecognition. Multiple groups of model weights are obtained\nby fine-tuning the initial learning rate, weight decay, and\ndata augmentation, the fine-tuning parameter configurations\nis shown in Table 1.\nTABLE 1. Set 15 groups of fine-tuning parameters (including the size of\nthe learning rate, the size of the weight decay and whether to use\nResizedCrop, random erasing and mix up). RandomCrop is used instead\nwhen ResizedCrop is not used.\nD. VISUALIZATION ANALYSIS\nTo further our method, we conduct visualizations of the\nperformance distribution and the confusion matrices.\n1) DISTRIBUTION OF MULTIPLE WEIGHTS\nOPTIMALITY-SEEKING PERFORMANCE\nAs shown in Fig 4, we visualize the distribution of expression\nrecognition performance on the RAF-DB, FERPlus and\nAffectNet datasets. Specifically, the figure depicts the\nperformance of the best epoch interval (green diamonds) with\n15 groups of fine-tuned weights. The local greedy perfor-\nmance (blue circle) and the global greedy performance (red\npentagram) are marked on the horizontal coordinate point\n’best’. We can observe that the global greedy recognition\naccuracies of 90.38%, 90.41% and 63.33% on the RAF-DB,\nFERPlus and AffectNet datasets are much higher than the\nperformance of individual model, which is most evident on\nthe RAF-DB. The results illustrate that our proposed method\ncan effectively tap the correlation between the weights of\nthe multi-group fine-tuning model and further enhance the\nexpression recognition performance.\nVOLUME 11, 2023 9999\nH. Feng et al.: Fine-Tuning Swin Transformer and Multiple Weights Optimality-Seeking for Facial Expression Recognition\nFIGURE 5. Confusion matrices obtained by optimal individual model and our method on RAF-DB, FERPlus and AffectNet datasets. Top raw: optimal\nindividual model, and bottom row: FST-MWOS method.\n2) CONFUSION MATRICES OF EXPRESSION CATEGORIES\nAs shown in Fig 5, we use confusion matrices to visualize\nthe recognition accuracy of various types of expressions\non three public facial expression datasets, where RAF-DB\nhas 7 basic expression categories, FERPlus and AffectNet\nadd contempt category to it. The main diagonal of the\nmatrix indicates the recognition accuracy of various kinds of\nexpression, and the remaining positions indicates the false\npositive rate. Observing the confusion matrix, we found\nthat the fear expressions were still easily confused with the\nsurprise expressions after adopting the FST-MWOS method,\nand their misclassification rates were 20.27%, 13.33% and\n18.80%, respectively. The reason for this phenomenon may\nbe that most of these two expressions show a facial form\nwith mouth slightly open and eyes wide open, and thus there\nare many similar features that are difficult to distinguish. But\noverall, Our FST-MWOS method achieves a more significant\nimprovement in the recognition rate of various types of\nexpressions compared to the best individual model.\nE. COMPARISON WITH STATE-OF-THE-ART METHODS\nWe compare FST-MWOS method with several state-of-the-\nart methods on 3 public datasets in Table 2. Since gaCNN [7],\nDACL [38] and FDRL [10] did not report the expression\nrecognition accuracy for the FERPlus dataset, FER-VT [11]\nand FDRL did not report the specific expression recognition\naccuracy for the AffectNet dataset, the corresponding places\nare marked with ′−′.\nAmong all the competing methods, gaCNN and RAN [8]\naim to disentangle the disturbing factors in facial expression\nimages, such as occlusion and posture changes. SCN [9],\nDACL, DMUE [39] and FDRL are proposed to solve the\nnoise label problem. VTFF [12], FER-VT and MVT [19]\ncombined with Vision Transformer to improve model recog-\nnition performance. The above advanced methods improve\nthe FER performance by suppressing the influence of\ndifferent disturbing factors or combined ViT, but they focus\nonly on the best performance individual model.\n1) COMPARISON RESULTS ON RAF-DB DATASET\nOverall, our proposed method achieves 90.38% recognition\nperformance on RAF-DB. As shown in Table 2, our method\nachieves the best results among all methods. In detail,\nour FST-MWOS has obtained gains of 0.91% over FDRL,\nwhich is the previous state-of-the-art method. This result\ndemonstrates the effectiveness and the superiority of our\nmethod in performance enhancement.\n2) COMPARISON RESULTS ON FERPlus DATASET\nAs we can see in Table 2, our proposed FST-MWOS\nachieves 90.41% recognition accuracy on FERPlus. Under\nthe same experiment settings, the total improvements of\nour FST-MWOS on FERPlus are 0.37% and 0.90% when\ncompared with FER-VT and DMUE. From the above results\non the RAF-DB and FERPlus datasets, our method has\nsuperior performance on small datasets.\n3) COMPARISON RESULTS ON AffectNet DATASET\nIt should be noted that AffectNet has imbalanced training and\nbalanced validation sets, where the distribution of training\n10000 VOLUME 11, 2023\nH. Feng et al.: Fine-Tuning Swin Transformer and Multiple Weights Optimality-Seeking for Facial Expression Recognition\nFIGURE 6. The distribution of training samples before and after oversampling on the AffectNet dataset. Obtain the\ndistribution of training samples every 100 batches and randomly display the results of 5 of these distributions.\nTABLE 2. Comparison with state-of-the-art methods on RAF-DB, FERPlus\nand AffectNet. The best results are in bold.† indicates that seven basic\nexpression labels are used in AffectNet.\nsamples as shown in Fig 6.(a). To deal with the imbalance\nissue, we adopt the oversampling strategy (WeightedRan-\ndomSampler) provided by Pytorch toolbox. The treated\nresults are shown in Figure 6.(b), where the distribution of\nthe training samples is greatly improved compared to that\nbefore the treatment. As shown in Table 2, we achieved\n63.33% recognition accuracy on the oversampled AffectNet,\nwhich outperforms VTFF and DMUE by 1.48% and 0.22%.\nThe improvements of our method over previous methods\nsuggest that the FST-MWOS indeed has better generalization\nability even on large-scale expression recognition datasets\nlike AffectNet.\nOverall, the above comparison results prove the effec-\ntiveness and superiority of our proposed method. The\nFST-MWOS method further improves the expression recog-\nnition performance by focusing on mining the correlation\nbetween multiple groups of fine-tuned model weights.\nF. ABLATION STUDY\nTo show the effectiveness of our method, we perform\nablation studies to evaluate the influence of key parameters\nand components on the final performance. For all the\nexperiments, we use RAF-DB, FERPlus and AffectNet\ndatasets to evaluate the performance.\n1) INFLUENCE OF THE KEY COMPONENTS\nOur proposed method FST-MWOS consists of fine-tuning\nSwin Transformer (FST) and multiple weights optimality-\nseeking (MWOS). To validate the effectiveness of these\ncomponents, we perform ablation studies for FST and\nMWOS on RAF-DB, FERPlus and AffectNet databases,\nrespectively. The experimental setup and results are reported\nin Table 3, where the MWOS contains two parts: local greedy\nstrategy (LGS) and global greedy strategy (GGS). Since\nGGS is optimality-seeking for n groups of fine-tuning model\nweights, it cannot be effective without the FST component.\nSettings (I, III) demonstrate that integrating the FST\ncomponent improves the baselines (the best individual\nmodel) on RAF-DB, FERPlus and AffectNet by 1.01%,\n0.09% and 0.85%, which suggests the FST component are\nbeneficial in improving expression recognition performance.\nThis can be explained by that the FST component provides\nmultiple groups of model weights to support the subsuent\noptimality-seeking by fine-tuning the parameters configura-\ntions of the Swin Transformer network. According to settings\n(II, III, IV), the MWOS leads to an increase in recognition\naccuracy when fusing the LGS and the GGS, showing the\neffectiveness of the proposed MWOS. Specifically, we can\nsee from settings (III,IV) that the designed LGS further\nimproves the performance by 0.35%, 0.25% and 0.23%.\nMWOS aggregates local and global greedy model weights\nto mine optimal generalization, which further improves the\nrecognition performance.\n2) INFLUENCE OF THEk\nThe LGS is to find the optimal solution in the best epoch\ninterval of an individual model. Therefore, we setup different\nepoch intervals to study their effect on recognition accuracy,\nVOLUME 11, 2023 10001\nH. Feng et al.: Fine-Tuning Swin Transformer and Multiple Weights Optimality-Seeking for Facial Expression Recognition\nTABLE 3. Ablation studies for two key components of FST-MWOS on\nthree public datasets. Recognition accuracy (%) for performance\nevaluation. The best results are in bold.\nTABLE 4. Ablation studies for the number of epoch interval on three\npublic datasets. Recognition accuracy (%) for performance evaluation.\nThe best results are in bold.\nTABLE 5. Ablation studies for the number of groups of fine-tuned\nweights on three public datasets. Recognition accuracy (%) for\nperformance evaluation. The best results are in bold.\nand the detailed settings are shown in Table 4. From settings\n(I, II, III), we can clearly see that as the range of epoch\nintervals is expanded, we are able to find the extremes of\nbetter performance. However, settings (III, IV) reflects that\nthe recognition performance has reached saturation values at\nk = 4.\n3) INFLUENCE OF THEn\nTable 5 reflects the performance of model with different n,\nwhere setting I indicates the individual model which performs\nbest on validation set, which is used as the reference group.\nFrom the table, it can be seen that our method achieves the\nbest recognition accuracy when the number of groups n is\nset to 15. At the same time, we observe that the optimal\nextremum cannot be found effectively when n is small, while\nincreasing n excessively does not have a positive effect and\nincreases the cost of training.\n4) INFLUENCE OF TYPES OF SWIN TRANSFORMER\nTo evaluate the impact of different types of Swin Transformer\non recognition performance. We selected four types (Tiny,\nSmall, Base and Large) for our experiments and the results are\nshown in Table 6. We found that the recognition performance\nimproves as the size of model gradually increases.\nTABLE 6. Ablation studies for the types of Swin Transformer on three\npublic datasets. Recognition accuracy (%), Params (M) and FLOPs (G) for\nperformance evaluation. The best results are in bold.\nV. CONCLUSION\nIn this paper, we proposed a novel method to further enhance\nthe facial expression recognition performance. FST-MWOS\nconsists of two main components: FST and MWOS. FST\nmakes the Swin-L model suitable for the FER task by\nfine-tuning it to extract facial features more accurately\nand to strengthen the correlation between features. MWOS\nmines the optimal generalization with positive performance\nimprovement by local-global greedy strategy to effectively\nimprove expression recognition accuracy. Experimental\nresults on three publicly available facial expression datasets\nhave shown the superiority of our method to perform FER.\nOur future work includes the continuation of research\nrelated to methods for improving the performance of expres-\nsion recognition and its application to emotion chatbots,\nwhich will ultimately be used in accompany people living\nalone.\nREFERENCES\n[1] C. Darwin and P. Prodger, The Expression of the Emotions in Man and\nAnimals. New York, NY, USA: Oxford Univ. Press, 1998.\n[2] S. Li and W. Deng, ‘‘Deep facial expression recognition: A survey,’’ IEEE\nTrans. Affect. Comput., vol. 13, no. 3, pp. 1195–1215, Jul. 2022.\n[3] D. Reher and M. Requena, ‘‘Living alone in later life: A global\nperspective,’’ Population Develop. Rev., vol. 44, no. 3, pp. 427–454,\nSep. 2018.\n[4] N. Masuyama, C. K. Loo, and M. Seera, ‘‘Personality affected robotic\nemotional model with associative memory for human–robot interaction,’’\nNeurocomputing, vol. 272, pp. 213–225, Jan. 2018.\n[5] J. Wang, Y. Wang, Y. Liu, T. Yue, C. Wang, W. Yang, P. Hansen, and F. You,\n‘‘Experimental study on abstract expression of human–robot emotional\ncommunication,’’Symmetry, vol. 13, no. 9, p. 1693, Sep. 2021.\n[6] J. Heredia, Y. Cardinale, I. Dongo, A. Aguilera, and J. Diaz-Amado,\n‘‘Multimodal emotional understanding in robotics,’’ in Proc. 18th Int.\nConf. Intell. Environments (IE), Jun. 2022, pp. 46–55.\n[7] Y. Li, J. Zeng, S. Shan, and X. Chen, ‘‘Occlusion aware facial expression\nrecognition using CNN with attention mechanism,’’ IEEE Trans. Image\nProcess., vol. 28, no. 5, pp. 2439–2450, May 2019.\n[8] K. Wang, X. Peng, J. Yang, D. Meng, and Y. Qiao, ‘‘Region attention\nnetworks for pose and occlusion robust facial expression recognition,’’\nIEEE Trans. Image Process., vol. 29, pp. 4057–4069, 2020.\n[9] K. Wang, X. Peng, J. Yang, S. Lu, and Y. Qiao, ‘‘Suppressing uncertainties\nfor large-scale facial expression recognition,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 6897–6906.\n[10] D. Ruan, Y. Yan, S. Lai, Z. Chai, C. Shen, and H. Wang, ‘‘Feature\ndecomposition and reconstruction learning for effective facial expression\nrecognition,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2021, pp. 7660–7669.\n[11] Q. Huang, C. Huang, X. Wang, and F. Jiang, ‘‘Facial expression recognition\nwith grid-wise attention and visual transformer,’’ Inf. Sci., vol. 580,\npp. 35–54, Nov. 2021.\n[12] F. Ma, B. Sun, and S. Li, ‘‘Facial expression recognition with visual\ntransformers and attentional selective fusion,’’ IEEE Trans. Affect.\nComput., early access, Oct. 26, 2021, doi: 10.1109/TAFFC.2021.3122146.\n[13] Y. Tang, ‘‘Deep learning using linear support vector machines,’’ in Proc.\nInt. Conf. Mach. Learn. (ICML), Jul. 2013, pp. 1–6.\n10002 VOLUME 11, 2023\nH. Feng et al.: Fine-Tuning Swin Transformer and Multiple Weights Optimality-Seeking for Facial Expression Recognition\n[14] J. Shao and Y. Qian, ‘‘Three convolutional neural network models for facial\nexpression recognition in the wild,’’ Neurocompting, vol. 355, pp. 82–92,\nAug. 2019.\n[15] C. Wang, S. Wang, and G. Liang, ‘‘Identity-and pose-robust facial\nexpression recognition through adversarial feature learning,’’ in Proc. 27th\nACM Int. Conf. Multimedia, Oct. 2019, pp. 238–246.\n[16] X. Feng, M. Pietikainen, and A. Hadid, ‘‘Facial expression recognition\nwith local binary patterns and linear programming,’’ Pattern Recognit.\nImage Anal., vol. 15, no. 2, p. 546, 2005.\n[17] C. Shan, S. Gong, and P. W. McOwan, ‘‘Facial expression recognition\nbased on local binary patterns: A comprehensive study,’’ Image Vis.\nComput., vol. 27, no. 6, pp. 803–816, 2009.\n[18] M. Kyperountas, A. Tefas, and I. Pitas, ‘‘Salient feature and reliable\nclassifier selection for facial expression classification,’’ Pattern Recognit.,\nvol. 43, no. 3, pp. 972–986, 2010.\n[19] H. Li, M. Sui, F. Zhao, Z. Zha, and F. Wu, ‘‘MVT: Mask vision transformer\nfor facial expression recognition in the wild,’’ 2021, arXiv:2106.04520.\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, p. 30.\n[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words:\nTransformers for image recognition at scale,’’ in Proc. Int. Conf. Learn.\nRepresent. (ICLR), May 2021, pp. 1–22.\n[22] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, ‘‘Masked\nautoencoders are scalable vision learners,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2022, pp. 16000–16009.\n[23] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh, ‘‘Rethinking spatial\ndimensions of vision transformers,’’ in Proc. IEEE/CVF Int. Conf. Comput.\nVis. (ICCV), Oct. 2021, pp. 11936–11945.\n[24] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, ‘‘CvT:\nIntroducing convolutions to vision transformers,’’ in Proc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2021, pp. 22–31.\n[25] C.-F.-R. Chen, Q. Fan, and R. Panda, ‘‘CrossViT: Cross-attention multi-\nscale vision transformer for image classification,’’ in Proc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2021, pp. 357–366.\n[26] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B.\nGuo, ‘‘Swin transformer: Hierarchical vision transformer using shifted\nwindows,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021,\npp. 10012–10022.\n[27] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. Gordon Wilson,\n‘‘Averaging weights leads to wider optima and better generalization,’’\n2018, arXiv:1803.05407.\n[28] J. Cha, S. Chun, K. Lee, H. C. Cho, S. Park, Y. Lee, and S. Park, ‘‘SWAD:\nDomain generalization by seeking flat minima,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 34, 2021, pp. 22405–22418.\n[29] M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes,\nA. S. Morcos, S. Lin, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith,\nand L. Schmidt, ‘‘Model soups: Averaging weights of multiple fine-tuned\nmodels improves accuracy without increasing inference time,’’ in Proc. Int.\nConf. Mach. Learn. (ICML), Jul. 2022, pp. 23965–23998.\n[30] S. Li, W. Deng, and J. Du, ‘‘Reliable crowdsourcing and deep locality-\npreserving learning for expression recognition in the wild,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 2852–2861.\n[31] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang, ‘‘Training deep\nnetworks for facial expression recognition with crowd-sourced label\ndistribution,’’ in Proc. 18th ACM Int. Conf. Multimodal Interact.,\nOct. 2016, pp. 279–283.\n[32] A. Mollahosseini, B. Hasani, and M. H. Mahoor, ‘‘AffectNet: A database\nfor facial expression, valence, and arousal computing in the wild,’’ IEEE\nTrans. Affect. Comput., vol. 10, no. 1, pp. 18–31, Jan. 2019.\n[33] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, ‘‘Joint face detection and alignment\nusing multitask cascaded convolutional networks,’’ IEEE Signal Process.\nLett., vol. 23, no. 10, pp. 1499–1503, Oct. 2016.\n[34] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, ‘‘Random erasing\ndata augmentation,’’ in Proc. AAAI Conf. Artif. Intell. (AAAI), Feb. 2020,\npp. 13001–13008.\n[35] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, ‘‘Mixup: Beyond\nempirical risk minimization,’’ in Proc. Int. Conf. Learn. Represent. (ICLR),\nMay 2018, pp. 1–13.\n[36] I. Loshchilov and F. Hutter, ‘‘Decoupled weight decay regularization,’’ in\nProc. Int. Conf. Learn. Represent. (ICLR), May 2019, pp. 1–19.\n[37] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‘‘Rethinking\nthe inception architecture for computer vision,’’ in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 2818–2826.\n[38] A. H. Farzaneh and X. Qi, ‘‘Facial expression recognition in the wild via\ndeep attentive center loss,’’ in Proc. IEEE Winter Conf. Appl. Comput. Vis.\n(WACV), Jan. 2021, pp. 2402–2411.\n[39] J. She, Y. Hu, H. Shi, J. Wang, Q. Shen, and T. Mei, ‘‘Dive into ambiguity:\nLatent distribution mining and pairwise uncertainty estimation for facial\nexpression recognition,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2021, pp. 6248–6257.\nHONGQI FENG received the B.S. degree in\nprocess equipment and control engineering from\nthe Jiangsu University of Chemical Technology.\nHe is currently a Professor and the master’s degree\nSupervisor with Changzhou University, Jiangsu,\nChina. He has presided over and participated\nin the completion of a number of scientific\nresearch projects, including the National Natural\nScience Foundation of China, and provincial and\nministerial fund projects. His research interests\ninclude deep learning and natural language processing and data mining and\nbig data analytics.\nWEIKAI HUANG was born in Wenzhou,\nZhejiang, China. He is currently pursuing the\nM.E. degree with Changzhou University. During\nhis M.E. study, he was mainly responsible for\nthe completion of the Provincial Research Fund\nProject related to healthy aging for the elderly.\nHis research interests include facial expression\nrecognition and deep learning.\nDENGHUI ZHANG is currently a Professor and\nthe master’s degree Supervisor with Zhejiang\nShuren University, Zhejiang, China. Up to now,\nhe published more than 30 academic articles and\npresided over and participated in the comple-\ntion of a number of scientific research projects,\nincluding the National Natural Science Foundation\nof China, and provincial and ministerial fund\nprojects. His research interest includes intelligent\nhuman–computer interaction.\nBANGZE ZHANG received the B.S. degree\nin information and computing sciences from\nWenzhou University, Wenzhou, China, in 2020.\nHe is currently pursuing the M.S.E. degree in\ncomputer technology with the School of Computer\nScience and Technology, Zhejiang University\nof Technology, Hangzhou, China. His research\ninterest includes medical image processing.\nVOLUME 11, 2023 10003",
  "topic": "Hyperparameter",
  "concepts": [
    {
      "name": "Hyperparameter",
      "score": 0.7502719759941101
    },
    {
      "name": "Transformer",
      "score": 0.7065445184707642
    },
    {
      "name": "Computer science",
      "score": 0.642001211643219
    },
    {
      "name": "Facial expression",
      "score": 0.5698531270027161
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5046783685684204
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43695729970932007
    },
    {
      "name": "Machine learning",
      "score": 0.33495303988456726
    },
    {
      "name": "Voltage",
      "score": 0.09780985116958618
    },
    {
      "name": "Engineering",
      "score": 0.08756956458091736
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210153482",
      "name": "Changzhou University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210127700",
      "name": "Zhejiang Shuren University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I55712492",
      "name": "Zhejiang University of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 22
}