{
  "title": "TMFormer: Token Merging Transformer for Brain Tumor Segmentation with Missing Modalities",
  "url": "https://openalex.org/W4393147960",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5102941617",
      "name": "Zheyu Zhang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5049125386",
      "name": "Gang Yang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5103136560",
      "name": "Yueyi Zhang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5033035049",
      "name": "Huanjing Yue",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A5100756739",
      "name": "Aiping Liu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5113828780",
      "name": "Yunwei Ou",
      "affiliations": [
        "Capital Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5102192497",
      "name": "Jian Gong",
      "affiliations": [
        "Capital Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5100656317",
      "name": "Xiaoyan Sun",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2979787545",
    "https://openalex.org/W3203934147",
    "https://openalex.org/W3174738881",
    "https://openalex.org/W2962678076",
    "https://openalex.org/W2613041730",
    "https://openalex.org/W4288099773",
    "https://openalex.org/W6639540389",
    "https://openalex.org/W6724422434",
    "https://openalex.org/W3090336986",
    "https://openalex.org/W2902930830",
    "https://openalex.org/W3089464490",
    "https://openalex.org/W4323927070",
    "https://openalex.org/W3117575391",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4379539164",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4387211619",
    "https://openalex.org/W2741295496",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3173665739",
    "https://openalex.org/W2903356598",
    "https://openalex.org/W3183162191",
    "https://openalex.org/W4295936460",
    "https://openalex.org/W3143351114",
    "https://openalex.org/W3117981784",
    "https://openalex.org/W4281654711",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W4382450643",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W1884191083",
    "https://openalex.org/W4312872526",
    "https://openalex.org/W4386075588",
    "https://openalex.org/W4221156361",
    "https://openalex.org/W3025646610",
    "https://openalex.org/W3203976604",
    "https://openalex.org/W4382466174",
    "https://openalex.org/W3203934469",
    "https://openalex.org/W2900298334",
    "https://openalex.org/W2502805798",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W4306886919"
  ],
  "abstract": "Numerous techniques excel in brain tumor segmentation using multi-modal magnetic resonance imaging (MRI) sequences, delivering exceptional results. However, the prevalent absence of modalities in clinical scenarios hampers performance. Current approaches frequently resort to zero maps as substitutes for missing modalities, inadvertently introducing feature bias and redundant computations. To address these issues, we present the Token Merging transFormer (TMFormer) for robust brain tumor segmentation with missing modalities. TMFormer tackles these challenges by extracting and merging accessible modalities into more compact token sequences. The architecture comprises two core components: the Uni-modal Token Merging Block (UMB) and the Multi-modal Token Merging Block (MMB). The UMB enhances individual modality representation by adaptively consolidating spatially redundant tokens within and outside tumor-related regions, thereby refining token sequences for augmented representational capacity. Meanwhile, the MMB mitigates multi-modal feature fusion bias, exclusively leveraging tokens from present modalities and merging them into a unified multi-modal representation to accommodate varying modality combinations. Extensive experimental results on the BraTS 2018 and 2020 datasets demonstrate the superiority and efficacy of TMFormer compared to state-of-the-art methods when dealing with missing modalities.",
  "full_text": "TMFormer: Token Merging Transformer for Brain Tumor\nSegmentation with Missing Modalities\nZheyu Zhang1,*, Gang Yang1,*, Yueyi Zhang1,2,†, Huanjing Yue4,\nAiping Liu1, Yunwei Ou3,2,†, Jian Gong3,2,, Xiaoyan Sun1,2\n1University of Science and Technology of China, Hefei 230026, China\n2Hefei Comprehensive National Science Center, Institute of Artificial Intelligence, Hefei 230088, China\n3Beijing Tiantan Hospital, Capital Medical University, Beijing 100050, China\n4Tianjin University, Tianjin 300072, China\nzhyuey@ustc.edu.cn, ouyunwei@sina.com\nAbstract\nNumerous techniques excel in brain tumor segmentation\nusing multi-modal magnetic resonance imaging (MRI) se-\nquences, delivering exceptional results. However, the preva-\nlent absence of modalities in clinical scenarios hampers per-\nformance. Current approaches frequently resort to zero maps\nas substitutes for missing modalities, inadvertently introduc-\ning feature bias and redundant computations. To address these\nissues, we present the Token Merging transFormer (TM-\nFormer) for robust brain tumor segmentation with missing\nmodalities. TMFormer tackles these challenges by extract-\ning and merging accessible modalities into more compact\ntoken sequences. The architecture comprises two core com-\nponents: the Uni-modal Token Merging Block (UMB) and\nthe Multi-modal Token Merging Block (MMB). The UMB\nenhances individual modality representation by adaptively\nconsolidating spatially redundant tokens within and outside\ntumor-related regions, thereby refining token sequences for\naugmented representational capacity. Meanwhile, the MMB\nmitigates multi-modal feature fusion bias, exclusively lever-\naging tokens from present modalities and merging them into\na unified multi-modal representation to accommodate varying\nmodality combinations. Extensive experimental results on the\nBraTS 2018 and 2020 datasets demonstrate the superiority\nand efficacy of TMFormer compared to state-of-the-art meth-\nods when dealing with missing modalities.\nIntroduction\nGiven the emergence of malignant brain tumors as a severe\nhealth threat, timely diagnosis is imperative for minimizing\ntheir impact. Brain tumor segmentation plays a pivotal role\nby identifying and delineating tumor boundaries in cerebral\nmedical images (Havaei et al. 2017; Jia et al. 2020; She et al.\n2023). Magnetic Resonance Imaging (MRI) sequences, in-\ncluding T1-weighted (T1), contrast-enhanced T1-weighted\n(T1ce), T2-weighted (T2), and Fluid Attenuation Inversion\nRecovery (FLAIR) modalities, are extensively employed for\nbrain tumor segmentation. Several multi-modal techniques\n*Equal contribution.\n†Corresponding authors.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(a) Catch-all methods adopt zero maps\nto compensate for the missing modali-\nties for fusion.\n(b) The T-SNE maps\nof catch-all methods are\nrelatively sparse.\n(c) Our proposed TMFormer only uti-\nlizes available modalities.\n(d) The T-SNE map of\nproposed TMFormer.\nFigure 1: Comparison between catch-all methods and our\nproposed TMFormer. Our proposed TMFormer reduces not\nonly redundant information but also feature bias for cases of\nmissing modalities. In (b) and (d), each point denotes a sam-\nple from the BraTS 2020. The points of the same color be-\ntween (b) and (d) belong to the same case of missing modal-\nities, while different colors mean different cases.\nleverage these four MRI modalities to enhance brain tu-\nmor segmentation by integrating complementary informa-\ntion. However, in clinical settings, missing modalities are\ncommon due to image corruption, varying acquisition pro-\ntocols, and contrast allergies (Liu et al. 2021a; Tran et al.\n2017). Such absence of modalities significantly impairs the\nsegmentation performance of multi-modal methods.\nVarious strategies have emerged to address diverse sce-\nnarios of missing modalities. One approach entails train-\ning dedicated networks for every potential combination of\navailable modalities (Wang et al. 2021b; Zhang et al. 2021),\nyet this leads to extensive training costs and deployment\nspace requirements. Some researchers seek to synthesize ab-\nsent modalities to create complete multi-modal sets (Wang\net al. 2018; Shen et al. 2020), but the segmentation accuracy\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7414\nis bound by the quality of the generated modalities, which\ncan introduce unexpected noise and artifacts. Predominantly,\ncatch-all methods prevail, utilizing a single model to handle\nall modality combinations (Havaei et al. 2016; Chen et al.\n2019; Zhou et al. 2021). Nonetheless, these approaches use\nzero maps to compensate for missing modalities, introduc-\ning dramatic variations, called ‘feature bias’, in multi-modal\nfeature fusion and superfluous computations in feature ex-\ntraction and fusion for the absent modalities, as illustrated\nin Fig. 1. Hence, mitigating the impact of zero maps is cru-\ncial for enhancing brain tumor segmentation outcomes.\nTo address these challenges in this task, we introduce an\ninnovative approach termed the Token Merge Transformer\n(TMFormer), designed to tackle the diverse combinations\nof modalities essential for brain tumor segmentation. TM-\nFormer interprets these varied amalgamations of modalities\nas sequences of tokens with varying and adaptable lengths.\nThis architecture incorporates two pivotal components: the\nUni-modal Token Merging Block (UMB), which focuses\non information extraction from individual modalities, and\nthe Multi-modal Token Merging Block (MMB), responsi-\nble for the fusion of multiple modalities. The UMB ap-\nplies an adaptive token merging strategy that intelligently\ncompresses spatially redundant tokens in regions associ-\nated with tumors. It retains more representative tokens in\ntumor-related regions and merges tokens more extensively\nin tumor-unrelated regions. This streamlined token sequence\nis subsequently refined to enhance global representation.\nMeanwhile, the MMB exclusively fuses and augments to-\nkens from available modalities. The tokens from modalities\nthat exert a relatively minor impact on segmentation into\nthose with higher contributions to the segmentation process.\nThe resultant multi-modal features are projected into a uni-\nfied representative space, effectively mitigating feature bi-\nases that may arise when dealing with different scenarios in-\nvolving missing modalities. This innovative TMFormer ap-\nproach demonstrates its prowess in efficiently handling the\nintricate challenges of brain tumor segmentation by expertly\nmanaging diverse modalities and their nuanced interactions.\nIn summary, the contributions of our work are as follows:\n• We introduce a Token Merging Transformer aiming to al-\nleviate feature bias in scenarios involving missing modal-\nities and reduce redundant computations.\n• We propose the UMB to reduce the spatially redundant\ninformation and augment the global representation of\navailable modalities.\n• We propose the MMB to merge multi-modal tokens\nbased on the respective contributions of modalities to\nsegmentation and project the fused tokens into a unified\nrepresentative space.\n• We conduct extensive experiments on the BraTS 2018\nand 2020 datasets and demonstrate the superiority and\neffectiveness of our TMFormer for brain tumor segmen-\ntation with missing modalities.\nRelated Work\nMulti-modal brain tumor segmentation with missing\nmodalities. Several methods have been developed to ad-\ndress brain tumor segmentation with missing modalities,\nwhich can be divided into three categories: 1) ‘dedicated’\nmethods that train a dedicated segmentation model for each\npossible combination of available modalities, 2) ‘genera-\ntive’ methods that synthesize missing modalities and train-\ning a segmentation model with complete modalities, and\n3) ‘catch-all’ methods that train a single model for various\ncombinations of modalities.\nFor the dedicated methods, KDNet (Hu et al. 2020) dis-\ntills knowledge from the multi-modal network to the dedi-\ncated network. Wang et al. (2021b) adopt a co-training strat-\negy between the multi-modal network and the dedicated net-\nwork, aligning the feature distribution in latent space. Since\nthere are fifteen possible combinations of modalities, these\nmethods suffer from high training costs.\nFor the generative methods, Hu et al. (2020) adopt a local-\nadaptive convolutional network to fuse the available modal-\nities for generating the absent modalities. Shen et al. (2020)\ndisentangle modality sequences into the content code and\nthe style code, and the missing modalities are generated\nbased on the content code. M3AE (Liu et al. 2023) adopts\nmulti-modal masked auto-encoder and model inversion to\nbuild substitutes of multi-modal sequences, which performs\na segmentation process on these substitutes. Nevertheless,\nas the generated modalities potentially have noises and arti-\nfacts, it is challenging to acquire an accurate segmentation\nresult based on the synthesized modalities.\nFor the catch-all methods, HeMiS (Havaei et al. 2016)\nfuses the multi-modal features using their mean and vari-\nance, which obtains the segmentation based on the fusion.\nRFNet (Ding, Yu, and Yang 2021) conducts multi-modal fu-\nsion according to the tumor regions, and UNet-MFI (Zhao,\nYang, and Sun 2022) builds the graph for fusing multi-modal\nfeatures. Zhang et al. (2022) propose the hybrid CNN-\nTransformer architecture termed the mmFormer that utilizes\nmulti-head self-attention to fuse multi-modal features only\nin the smallest scale. However, due to their convolutional\noperation defined by fixed-size kernels, these methods must\nuse zero maps to compensate for the missing modalities dur-\ning subsequent processing. This inadvertently introduces the\nfeature bias. Besides, the feature extraction and fusion for\nzero maps lead to unnecessary computations. In contrast,\nwe treat varying modality combinations as variable-length\ntoken sequences, which avoid the involvement of zero maps.\nEfficient vision transformers. Since transformers have\nquadratic complexity, many works aim to improve their ef-\nficiency from different aspects. Some focus on the atten-\ntion mechanisms (Huang et al. 2019; Liu et al. 2021b;\nDong et al. 2022). Swin Transformer (Liu et al. 2021b)\nadopts self-attention in shifted local windows. Huang et al.\n(2019) and Dong et al. (2022) propose cross-shaped win-\ndows for computing attention. Besides, PVT (Wang et al.\n2021a) employs the pyramid structure within the downsam-\npled key and value tokens. Reducing the number of tokens\nprocessed in the network is an alternative way to improve\nefficiency. Several works attempt to prune less informative\ntokens based on the predicted importance (Rao et al. 2021)\nor token similarity (Liang et al. 2022; Fayyaz et al. 2022;\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7415\nFigure 2: Overview of the proposed TMFormer, which is composed of four convolution-tokenization encoders, four UMBs, an\nMMB, an unmerging block, and a decoder.\nKong et al. 2023). Bolya et al. (2022) propose to merge ad-\njacent similar tokens to accelerate the inference of ViT. To-\nken Learner (Liang et al. 2022) fuse inattentive tokens that\ncontribute less to the class token. The mentioned methods\nhave achieved promising performance for image classifica-\ntion. Recently, Lu, de Geus, and Dubbelman (2023) propose\nto share values of tokens belonging to the same class for\nsemantic segmentation. However, to the best of our knowl-\nedge, reducing redundant tokens remains unexplored within\nbrain tumor segmentation, particularly in scenarios involv-\ning missing modalities. It is non-trivial to propose a token-\nreducing strategy as 3D MRI has redundant information in\nintra-modal and inter-modal spatial dimensions.\nMethod\nIn this section, we first briefly explain the motivation. Then\nwe illustrate the overall architecture of our TMFormer in\nFig. 2 and its components. Finally, we describe its corre-\nsponding optimization loss.\nMotivation\nThe prevalent catch-all methods utilize zero maps as substi-\ntutes for absent modalities, which inevitably introduce fea-\nture bias and the wastage of computational resources. Draw-\ning inspiration from Transformer’s proficiency in handling\nvariable-length token sequences, we propose to employ the\nTransformer architecture to deal with diverse scenarios of\nmissing modalities. The different combinations of available\nmodalities are regarded as variable-length token sequences\nwhile the substitutes are no longer utilized.\nGiven the considerable length and potential redundancy\nof 3D medical image token sequences, we undertake the\nmerging of modality tokens along intra-modal and inter-\nmodal spatial dimensions. Furthermore, to alleviate feature\nbias across distinct scenarios involving absent modalities,\nwe introduce the modeling of global dependencies within\nthe compact token sequences and project them into a unified\nmulti-modal representative space.\nThe Overall Architecture\nA full-modal complete image set consists of four different\nmodalities, i.e., FLAIR, T1ce, T1, and T2 modalities. In the\ntask of missing modalities, a multi-modal data x is given\nwith the dimension M ×D ×H ×W. D, H, and W are the\ndepth, height, and width of the image, respectively, and M\nis the number of available modalities. Its multi-modal code\nis represented by h = {h1, h2, h3, h4}, where hi ∈ {0,1}\nindicates whether the corresponding modality is available.\nWe illustrate the overall architecture in Fig. 2. The TM-\nFormer employs a multi-scale network structure, facilitat-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7416\n(a) The partition for getting A uni and Buni\nin the UMB.\n(b) The partition for getting\nAmul and Bmul in the MMB.\n(c) The token merging process after dividing A and B in\nUMB and MMB.\nFigure 3: The token merging process of UMB and MMB. In (a) and (b), sample red tokens to set A and blue tokens to set B.\nIn (c), merge uni-modal tokens from Auni into Buni in the UMB, while merge multi-modal tokens from Amul into Bmul in the\nMMB. The most similar tokens are merged as mixed tokens, while the dissimilar tokens are preserved as red and blue tokens.\ning the integration of information across distinct hierarchi-\ncal levels. At each scale, we first utilize encoders to project\nthe data x into token sequences. Then, we adopt the UMB\nto decrease the spatial redundancy of token sequences and\nmodel the intra-modal global relationships for each available\nmodality. After extracting features, we use MMB to decrease\nthe redundancy among modalities and modal the inter-modal\nglobal relationship. Subsequently, the fused multi-modal to-\nken sequences are unmerged and re-arranged to their initial\nshape. Finally, the fused features are sent to convolutional\ndecoders to yield the final segmentation result. Notably, as\ndepicted in Fig. 2, the encoder branches marked as miss-\ning modalities by hi = 0are not involved in computations,\nwhich is different from the other methods dedicated to miss-\ning modalities (Yang et al. 2022; Zhang et al. 2022).\nConvolution-tokenization Encoder We adopt individual\nencoders to extract local features for available modalities.\nSimilar to the encoder part design of U-Net (Ronneberger,\nFischer, and Brox 2015), we stack four convolutional blocks\nto extract multi-scale features, and each convolutional block\nconsists of three convolutional layers. At each scale, the lo-\ncal features of each modality are projected into the token se-\nquence xtok ∈ R( D\n2l−1 × H\n2l−1 × W\n2l−1 )×C through a patch em-\nbedding layer, wherel indicates the scale, D\n2l−1 × H\n2l−1 × W\n2l−1\ndenotes the length of token sequence, and C is the channel\ndimension. We set the patch size to1×1×1 since the pixel-\nlevel information is essential to segmentation.\nUni-modal Merging Block (UMB) Due to the inherently\nlocal nature of convolutional networks, we aim to improve\nthe uni-modal feature representation by capturing long-\nrange dependencies. For each modality, the input for the\nUMB is xtok ∈ R( D\n2l−1 × H\n2l−1 × W\n2l−1 )×C. At the l = 0scale,\nthe length of the sequence is D × H × W, which will lead\nto high complexity of computations. We firstly merge the to-\nken sequence with the UMB, which transforms the sequence\nlength from D\n2l−1 × H\n2l−1 × W\n2l−1 to Nl−1, i.e.,\nxuni = UTM (xtok|Segcoarse), (1)\nwhere UTM denotes the uni-modal token merging process,\nand Segcoarse is the coarse segmentation map predicted\nfrom the previous scale. Since the segmentation only focuses\non the tumor region, we useSegcoarse to identify the tumor-\nrelated region and the tumor-unrelated region to merge spa-\ntial tokens adaptively. Based on Segcoarse, the uni-modal\ntoken merging process is as follows: 1) Partitioning thextok\ninto parts that each part belongs to a cubic window in 3D\ndimensions; 2) Sampling tokens from parts that more tokens\nare sampled for the tumor-related part while fewer tokens are\nsampled for the tumor-unrelated part; 3) Putting the sampled\ntokens into Buni and the rest tokens are in Auni; 4) Calculat-\ning the similarity scores between Auni and Buni; 5) Select-\ning the most similar token in B uni for each token in A uni,\nwhich builds the similar token pairs and preserves its similar\nscore; 6) Merging the topNuni\nl−1 most similar token pairs that\nare sorted based on the similar score; 7) Appending the dis-\nsimilar tokens of Auni to Buni, and the length of dissimilar\ntokens is Nl−1 − Nuni\nl−1 .\nSignificantly, a subset of tumor-unrelated tokens is allo-\ncated to Buni as these tokens estimated by coarse segmenta-\ntion maps, may include false negatives, which should be pre-\nserved. The merging process is shown in Fig. 3a and Fig. 3c.\nAfter this process, we then establish a global feature rela-\ntionship for the merged token sequence xuni ∈ RNl−1×C\nvia multi-head self-attention (MSA) and multi-layer percep-\ntrons (MLP) to fully mine the feature information within the\nmodality. The corresponding outputs are:\nxMSA = MSA( LN(xuni)) +xuni, (2)\nxMLP = MLP (LN(xMSA )) +xMSA , (3)\nwhere LN(·) denotes the layer normalization, and the\nxMLP is the final output of the UMB.\nMulti-modal Merging Block (MMB) After the feature\nextraction by UMB, we obtain the token sequences Xtok ∈\nRM×Nl−1×C for M available modalities. To enhance the\nmulti-modal representation capacity, we propose the MMB\nto model the long-range dependencies among different\nmodalities. In the MMB, we first merge the inter-modal re-\ndundant information, i.e.,\nxmul = MTM (Xtok|h), (4)\nwhere MTM (·) denotes the multi-modal token merging\nprocess and h is the modality code.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7417\nDifferent from the UTM, the partition of the MTM is\nbased on the observation that the FLAIR and T1ce modal-\nities have a more pronounced impact on tumor segmenta-\ntion (Ding, Yu, and Yang 2021; Yang et al. 2022). Conse-\nquently, we construct the multi-modal token sequencesXtok\nin the order of [xFLAIR\ntok , xT1ce\ntok , xT1\ntok, xT2\ntok], maintaining se-\nquential continuity even with the absence of modalities. For\nexample, if the T1ce modality is missing, the multi-modal\ntoken sequences Xtok are organized as [xFLAIR\ntok , xT1\ntok, xT2\ntok]\nthat do not add zero maps. We partition the multi-modal to-\nken sequences Xtok into two parts, set A mul and set Bmul,\nwhich are illustrated as follows:\n\n\nAmul =Xtok[0:1] , Bmul =Xtok[2: M), ifM ≥3,\nAmul =Xtok[0], Bmul =Xtok[1], ifM =2,\nAmul =Xtok, Bmul =None, ifM =1,\n(5)\nTo reduce the inter-modal redundancy, we fuse the tokens of\nAmul into the tokens of B mul that are highly similar, con-\ncurrently appending dissimilar tokens of A mul to Bmul. In\nsummary, the MTM process is as follows: 1) Constructing\nthe multi-modal token sequences Xtok; 2) Partitioning the\nXtok into Amul and Bmul based on Eq. 5; 3) Calculating the\nsimilarity scores between A mul and Bmul; 4) Selecting the\nmost similar token in Bmul for each token in Amul; 5) Merg-\ning the top Nmul\nl−1 most similar token pairs; 6) Appending the\ndissimilar tokens of Amul to Bmul. The merging process is\nshown in Fig. 3b and Fig. 3c. If only one modality is avail-\nable, we send it to the following layers without merging.\nWe then adopt MSA to facilitate information exchange\nbetween modalities. Finally, we utilize MLP to project the\nfused feature into a unified representation space for the fol-\nlowing segmentation, which improves the robustness of seg-\nmentation in case of different scenarios of missing modali-\nties. Similarly,the process is presented as follows:\nxMSA = MSA( LN(xmul)) +xmul, (6)\nxMLP = MLP (LN(xMSA )) +xMSA . (7)\nUnmerging Block and Decoder We employ the unmerg-\ning block to restore the initial length of the token sequence\nand re-arrange the token sequence into the feature maps of\nD\n2l−1 × H\n2l−1 × W\n2l−1 . We use one multi-scale decoder to grad-\nually restore spatial resolution, smoothly transitioning from\nthe high-level latent space to the original mask space.\nIn the unmerging block, the tokens that are merged out\nshare the identical value as the merged token, effectively re-\ninstating the length of the enhanced token sequences to their\noriginal state before merging. In the decoder, the decoder\nblocks employ the feature maps for generating coarse-to-\nfine segmentation maps within our four-level structure. The\ncoarse segmentation map plays a vital role in guiding the\nprocess of UTM within our UMB.\nOptimization Loss\nFollowing previous works (Dou et al. 2017; Ding, Yu, and\nYang 2021; Zhao, Yang, and Sun 2022), we employ the\nweighted cross-entropy lossLCE and Dice loss LDice to op-\ntimize our TMFormer at each scale, which is defined as:\nL = LCE (y, ˆy) +LDice(y, ˆy), (8)\n(a) Input\n (b) Similarity map\n (c) Merged visualization\nFigure 4: Visualization of token merging. (a) depicts the ini-\ntial input that has D × H × W tokens. (b) illustrates to-\nken similarities, where tokens of the same color indicate\ntheir merging. (c) shows the visualization of merged tokens,\nwhose length is near 1\n64 of the initial tokens.\nwhere y and ˆy are the segmentation prediction and the\nground truth, respectively.\nExperiments and Results\nDatasets & Evaluation Metrics. The proposed method is\nevaluated on two publicly available multi-modal brain tumor\nsegmentation (BraTS) datasets: BraTS 2018 (Bakas et al.\n2018) and 2020 (Andres et al. 2020). The BraTS 2018 and\n2020 datasets include 285 and 369 cases with ground truth\npublicly available, respectively. For the BraTS 2018 dataset,\nwe split it into 199 : 29 : 57for training, validation, and\ntesting, respectively. Additionally, we use a three-fold val-\nidation with the same split as (Dorent et al. 2019). For the\nBraTS 2020 dataset, following (Ding, Yu, and Yang 2021),\nwe randomly split it into 219 : 50 : 100.\nThe BraTS datasets consist of four different modalities:\nFLAIR, T1ce, T1, and T2 modalities. Each of them cap-\ntures different properties of brain tumor subregions: GD-\nenhancing tumor (ET), peritumoral edema (ED), and the\nnecrotic and non-enhancing tumor core (NCR/NET). Dif-\nferent subregions of brain tumors are combined into three\nnested subregions: the whole tumor (WT), the tumor core\n(TC), and the enhancing tumor (ET). Following (Ding, Yu,\nand Yang 2021; Zhao, Yang, and Sun 2022), we adopt Dice\nSimilarity Coefficient (DSC) to evaluate the segmentation\nperformance.\nImplementation Details. We implement our TMFormer\nin the PyTorch framework (1.13) and train all parameters on\nfour NVIDIA GeForce RTX 3090 GPUs for 500 epochs. Our\nmethod is optimized by the ADAM optimizer with a batch\nsize of 4. The initial learning rate is2×10−4 with the weight\ndecay of 1 × 10−5.\nIn the training stage, we randomly crop each volume to\na fixed size of 80 × 80 × 80 voxels that are further aug-\nmented with random flip, random rotation, and random in-\ntensity shift. In the inference stage, we crop the volumes\nfrom 240 × 240 × 155 to 128 × 128 × 128 with an over-\nlap rate of 0.5.\nMotivation Verification. As illustrated in motivation, we\nfirst regard the different combinations of available modal-\nities as variable-length token sequences, avoiding unnec-\nessary calculations on missing modalities. As is shown in\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7418\nMethods HeMiS U-HVED\nRFNet UNet-MFI mmFormer M3AE 1 Mod.\n2 Mods. 3 Mods. 4 Mods.\nParam.\n(M) 0.57 1.25\n8.98 34.12 36.65 40.42 8.93 8.93\n8.93 8.93\nGFLOPs 2.27 4.58\n102.28 499.52 30.23 36.14 57.24 72.62\n88.00 103.37\nTable 1: The parameters and GFLOPs of compared methods on the BraTS 2020 dataset. ‘1/2/3/4 Mods.’ means that we send\n1/2/3/4 available modalities into our model, respectively.\nM\nF • ◦ ◦ ◦ • • • ◦ ◦ ◦ • • • ◦ •\nAV\nGT1ce ◦ • ◦ ◦ • ◦ ◦ • • ◦ • • ◦ • •\nT1 ◦ ◦ • ◦ ◦ • ◦ • ◦ • • ◦ • • •\nT2 ◦ ◦ ◦ • ◦ ◦ • ◦ • • ◦ • • • •\nWT\n1 71.60 67.71 68.96 68.19 69.17 68.67 69.83 69.01 69.78 69.40 70.21 71.28 70.73 71.58 72.06 69.88\n2 69.85 46.82 46.77 54.03 61.45 58.25 64.50 62.91 65.76 64.29 66.99 69.70 68.38 70.35 71.41 62.76\n3 86.42 77.34 76.46 86.21 89.55 89.30 89.35 81.00 87.45 87.95 90.39 90.20 90.42 88.59 90.77 86.76\n4 82.27 73.18 72.10 82.45 83.64 84.34 84.85 77.30 83.44 83.52 85.45 85.70 85.85 84.20 84.93 82.21\n5 82.40 74.25 74.37 83.07 84.54 84.61 85.82 77.98 84.05 84.00 85.34 86.11 86.22 84.64 86.38 82.92\n6 86.53 73.85 76.71 86.09 89.48 89.38 89.25 78.11 87.37 87.20 89.99 90.18 90.42 88.61 90.56 86.25\nOurs 87.45 78.53 78.94 86.46 89.67 89.64 89.98 81.97 88.01 87.75 90.23 90.53 90.51 88.54 90.83 87.27\nTC\n1 53.43 51.41 51.56 51.11 51.70 51.08 51.85 51.88 52.35 51.51 52.95 53.76 52.97 54.38 55.03 52.46\n2 34.62 35.51 27.30 37.67 42.15 38.26 43.41 44.93 47.53 44.97 49.13 51.30 49.40 52.72 54.17 43.53\n3 65.04 82.37 64.31 68.47 84.69 71.45 72.62 83.15 84.06 72.11 84.71 84.70 74.28 84.11 84.74 77.39\n4 63.94 77.63 59.38 68.05 79.92 68.23 70.72 77.61 80.09 70.21 80.03 80.94 71.40 80.75 81.28 74.01\n5 66.19 77.96 61.17 69.18 80.36 69.58 71.55 79.93 80.79 70.90 80.18 81.31 72.02 81.12 81.22 74.90\n6 68.04 81.39 66.00 70.27 82.01 73.82 74.95 82.39 83.01 72.54 82.44 83.06 75.09 84.06 84.40 77.56\nOurs 70.19 82.59 67.12 71.84 84.62 74.65 74.76 83.13 84.24 73.33 84.69 84.64 75.17 84.00 84.88 78.66\nET\n1 43.77 42.41 41.59 41.45 41.83 40.29 41.19 42.08 42.39 41.00 43.67 44.16 42.95 45.27 46.33 42.69\n2 12.88 24.94 7.27 24.26 30.02 21.95 29.40 33.64 36.18 32.12 39.39 40.91 38.09 43.18 45.33 30.64\n3 40.47 74.27 37.51 43.59 76.45 43.81 46.99 75.22 73.94 46.37 77.01 76.38 48.95 76.38 76.64 60.93\n4 39.70 69.42 29.38 46.00 70.13 40.06 48.69 69.25 72.32 45.71 71.28 70.88 46.55 72.00 71.41 57.52\n5 40.47 68.91 33.97 45.61 69.81 43.63 48.09 71.10 70.72 45.92 70.08 71.60 48.38 70.65 71.36 58.02\n6 40.49 72.43 39.93 45.97 74.66 43.20 47.30 75.42 76.81 46.63 75.94 77.08 48.19 77.40 78.00 61.30\nOurs 42.28 76.21 38.21 46.94 76.37 48.20 51.67 78.68 78.25 48.81 78.64 78.45 51.23 78.51 78.98 63.43\nTable 2: Performance comparison (DSC%) with SOTA methods, including 1 HeMiS (Havaei et al. 2016), 2 U-\nHVED (Dorent et al. 2019), 3 RFNet (Ding, Yu, and Yang 2021), 4 UNet-MFI (Zhao, Yang, and Sun 2022), 5 mm-\nFormer (Zhang et al. 2022) and 6 M3AE (Liu et al. 2023) on the BraTS 2020 dataset. Available and missing modalities are\ndenoted by • and ◦, respectively.\nTab. 1, our model presents variable GFLOPs with different\ncombinations of modalities. Conversely, the other methods\nexhibit fixed GFLOPs that consume redundant computations\non zero maps for the missing modalities.\nSecondly, we visualize an example of merging uni-modal\ntokens in Fig. 4. We obtain the similarity map for merging in\nthe feature space and employ it on the input image for bet-\nter understanding. The merged image demonstrates balanced\npreservation of edge details without excessive smoothing.\nThus, the UMB preserves the essential edges for brain tumor\nsegmentation while decreasing the redundant information.\nFinally, as shown in Fig. 1, we obtain fused multi-modal\nfeatures, which further go through the high-dimensional em-\nbedding pooling followed by the dimensionality reduction\nfor 2D visualization via t-SNE on the BraTS 2020 dataset.\nWe use 100 samples to simulate 4 cases of missing modali-\nties, including 1/2/3/4 available modalities. After processing\nby our MMB, those samples tend to cluster more densely\nin Fig. 1d, in contrast to their sparser distribution of fused\nmulti-modal features with zero maps in Fig. 1b. This ob-\nservation demonstrates the capability of our MMB to unify\nmulti-modal features, thus resulting in a consistent represen-\ntation across varying modality combinations.\nComparison with the state-of-the-art (SOTA) methods\non missing modalities. To demonstrate the superiority of\nour method, we compare our TMFormer with six state-of-\nthe-art methods on different cases with missing MRI modal-\nities.The involved methods contain HeMiS (Havaei et al.\n2016), U-HVED (Dorent et al. 2019), RFNet (Ding, Yu, and\nYang 2021), UNet-MFI (Zhao, Yang, and Sun 2022), mm-\nFormer (Zhang et al. 2022), M3AE (Liu et al. 2023). For a\nfair comparison, all methods are trained under their recom-\nmended hyper-parameters within the same dataset split.\nAs shown in Tab. 2, our method achieves preferable\nresults for most combinations of missing modalities. We\nachieve improvements of 0.5%, 1.1%, and 2.5% over the\nsecond-ranked method on the average DSC for WT, TC, and\nET. In Fig. 5, we provide the segmentation visualizations\nthat our method yields more accurate segmentation results\nin different combinations of modalities.\nAblation Study. We evaluate the proposed components on\nthe BraTS 2020 dataset, employing the average DSC to mea-\nsure the performance of the WT, TC, and ET. Ablative ex-\nperiments are partitioned into three parts, i.e., the UMB, the\nMMB, and stages, as depicted in Tab. 3. We assess each part\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7419\nFigure 5: Segmentation results of different methods with various available modalities on the BraTS 2020 dataset.\nConfiguration UMB MMB #Stage\nPooling Sconv SA SWC\nSRW SRW+SAR SA DO\nCO 1 2\n3 4\nDSC\nWT 82.92 82.21 85.11 86.00\n87.23 87.27 87.04 86.98 87.27 86.81 87.12\n87.21 87.27\nTC 74.90 74.01 72.15 74.38 78.97 78.66 77.91 78.23 78.66 77.81 78.04\n78.04 78.66\nET 53.68 53.73 48.00 49.43\n62.20 63.43 62.89 62.45 63.43 60.06 62.53\n62.67 63.43\nTable 3: Ablation results of proposed components on the BraTS 2020 dataset.\nwhile keeping others constant.\nTo evaluate the UMB, we use ‘Pooling’ or ‘S conv’ for\nmerging uni-modal tokens within cubic windows. ‘Pooling’\ncomputes the average mean, while ‘Sconv’ employs the con-\nvolution with a stride equal to the kernel size. In contrast,\n‘SA’, ‘SWC’, ‘SRW’, and ‘SRW+SAR’ perform global to-\nken merging, albeit with token sampling within cubic win-\ndows. ‘SA’ means alternate token sampling. ‘SWC’ stands\nfor central token sampling, and ‘SRW’ signifies random to-\nken sampling. Both ‘SA’ and ‘SWC’ involve fixed sam-\npled tokens. ‘SAR’ denotes adaptive token sampling, and\n‘SRW+SAR’ is our chosen approach in the UMB. From\nTab. 3, we find that global merging outperforms local merg-\ning. The random sampling with the guidance of a coarse seg-\nmentation map also boosts the performance from 62.20 to\n63.43 on ET.\nTo assess our MMB, we replace the ‘CO’ with\nboth ‘SA’ and ‘DO’. In MMB, ‘SA’ involves alter-\nnately sampling tokens from multi-modal sequences,\nwithout regard to the modality of the tokens. ‘DO’\nconstructs multi-modal sequences Xtok in a differ-\nent order, i.e., [xT1\ntok, xT2\ntok, xFLAIR\ntok , xT1ce\ntok ]. ‘CO’ repre-\nsents constructing tokens in our proposed order, i.e.,\n[xFLAIR\ntok , xT1ce\ntok , xT1\ntok, xT2\ntok]. The results show that our\nMMB improves the multi-modal feature fusion.\nWe verify the efficacy of TMFormer’s multi-scale de-\nsign by progressively integrating our UMB and MMB into\neach scale. The outcomes demonstrate that incorporating our\nproposed blocks within a multi-scale framework yields im-\nprovements in performance. More results are in Appendix.\nConclusion\nIn this paper, we introduce a novel Token Merging\ntransFormer (TMFormer) to tackle the challenge of miss-\ning modalities in brain tumor segmentation. This addresses\nthe issues caused by using zero maps as substitutes, which\nlead to feature bias and redundant computations. TMFormer\ntreats modalities’ diverse combinations as variable-length\ntoken sequences, considering only available modalities. Our\nTMFormer comprises two pivotal modules: the UMB for\nuni-modal feature extraction and the MMB for multi-modal\nfeature fusion. The UMB initially reduces spatially redun-\ndant tokens guided by a coarse segmentation map and mod-\nels global dependencies for each available modality. The\nMMB merges uni-modal tokens based on modalities’ con-\ntribution order to segmentation. The fused multi-modal to-\nken sequence is then projected into a unified representation\nto alleviate feature bias in different combinations of modal-\nities. These proposed components collectively demonstrate\nthe potential to mitigate feature bias and avoid unnecessary\ncomputations for missing modalities. Extensive experiments\nshow the proficiency of our method.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7420\nAcknowledgments\nThis work was in part supported by the National Natural\nScience Foundation of China under grants 62032006 and\n62021001.\nReferences\nAndres, E. A.; Fidon, L.; Vakalopoulou, M.; Lerousseau,\nM.; Carr ´e, A.; Sun, R.; Klausner, G.; Ammari, S.; Benza-\nzon, N.; Reuz ´e, S.; et al. 2020. Dosimetry-driven quality\nmeasure of brain pseudo computed tomography generated\nfrom deep learning for MRI-only radiation therapy treatment\nplanning. International Journal of Radiation Oncology* Bi-\nology* Physics, 108(3): 813–823.\nBakas, S.; Reyes, M.; Jakab, A.; Bauer, S.; Rempfler, M.;\nCrimi, A.; Shinohara, R. T.; Berger, C.; Ha, S. M.; Rozycki,\nM.; et al. 2018. Identifying the best machine learning al-\ngorithms for brain tumor segmentation, progression assess-\nment, and overall survival prediction in the BRATS chal-\nlenge. arXiv preprint arXiv:1811.02629.\nBolya, D.; Fu, C.-Y .; Dai, X.; Zhang, P.; Feichtenhofer, C.;\nand Hoffman, J. 2022. Token merging: Your vit but faster.\narXiv preprint arXiv:2210.09461.\nChen, C.; Dou, Q.; Jin, Y .; Chen, H.; Qin, J.; and Heng,\nP.-A. 2019. Robust multimodal brain tumor segmentation\nvia feature disentanglement and gated fusion. In Medi-\ncal Image Computing and Computer Assisted Intervention–\nMICCAI 2019: 22nd International Conference, Shenzhen,\nChina, October 13–17, 2019, Proceedings, Part III 22, 447–\n456. Springer.\nDing, Y .; Yu, X.; and Yang, Y . 2021. RFNet: Region-aware\nfusion network for incomplete multi-modal brain tumor seg-\nmentation. In Proceedings of the IEEE/CVF international\nconference on computer vision, 3975–3984.\nDong, X.; Bao, J.; Chen, D.; Zhang, W.; Yu, N.; Yuan, L.;\nChen, D.; and Guo, B. 2022. Cswin transformer: A general\nvision transformer backbone with cross-shaped windows. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 12124–12134.\nDorent, R.; Joutard, S.; Modat, M.; Ourselin, S.; and Ver-\ncauteren, T. 2019. Hetero-modal variational encoder-\ndecoder for joint modality completion and segmenta-\ntion. In Medical Image Computing and Computer As-\nsisted Intervention–MICCAI 2019: 22nd International Con-\nference, Shenzhen, China, October 13–17, 2019, Proceed-\nings, Part II 22, 74–82. Springer.\nDou, Q.; Yu, L.; Chen, H.; Jin, Y .; Yang, X.; Qin, J.; and\nHeng, P.-A. 2017. 3D deeply supervised network for auto-\nmated segmentation of volumetric medical images. Medical\nimage analysis, 41: 40–54.\nFayyaz, M.; Koohpayegani, S. A.; Jafari, F. R.; Sengupta,\nS.; Joze, H. R. V .; Sommerlade, E.; Pirsiavash, H.; and Gall,\nJ. 2022. Adaptive token sampling for efficient vision trans-\nformers. In European Conference on Computer Vision, 396–\n414. Springer.\nHavaei, M.; Davy, A.; Warde-Farley, D.; Biard, A.;\nCourville, A.; Bengio, Y .; Pal, C.; Jodoin, P.-M.; and\nLarochelle, H. 2017. Brain tumor segmentation with deep\nneural networks. Medical image analysis, 35: 18–31.\nHavaei, M.; Guizard, N.; Chapados, N.; and Bengio, Y .\n2016. Hemis: Hetero-modal image segmentation. In Medi-\ncal Image Computing and Computer-Assisted Intervention–\nMICCAI 2016: 19th International Conference, Athens,\nGreece, October 17-21, 2016, Proceedings, Part II 19, 469–\n477. Springer.\nHu, M.; Maillard, M.; Zhang, Y .; Ciceri, T.; La Barbera,\nG.; Bloch, I.; and Gori, P. 2020. Knowledge distilla-\ntion from multi-modal to mono-modal segmentation net-\nworks. In Medical Image Computing and Computer Assisted\nIntervention–MICCAI, 772–781. Springer.\nHuang, Z.; Wang, X.; Huang, L.; Huang, C.; Wei, Y .; and\nLiu, W. 2019. Ccnet: Criss-cross attention for semantic seg-\nmentation. In Proceedings of the IEEE/CVF international\nconference on computer vision, 603–612.\nJia, H.; Xia, Y .; Cai, W.; and Huang, H. 2020. Learning high-\nresolution and efficient non-local features for brain glioma\nsegmentation in MR images. In Medical Image Computing\nand Computer Assisted Intervention–MICCAI 2020: 23rd\nInternational Conference, Lima, Peru, October 4–8, 2020,\nProceedings, Part IV 23, 480–490. Springer.\nKong, Z.; Ma, H.; Yuan, G.; Sun, M.; Xie, Y .; Dong, P.;\nMeng, X.; Shen, X.; Tang, H.; Qin, M.; et al. 2023. Peel-\ning the onion: Hierarchical reduction of data redundancy\nfor efficient vision transformer training. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 37,\n8360–8368.\nLiang, Y .; Ge, C.; Tong, Z.; Song, Y .; Wang, J.; and Xie,\nP. 2022. Not all patches are what you need: Expediting vi-\nsion transformers via token reorganizations. arXiv preprint\narXiv:2202.07800.\nLiu, H.; Wei, D.; Lu, D.; Sun, J.; Wang, L.; and Zheng,\nY . 2023. M3AE: Multimodal Representation Learning for\nBrain Tumor Segmentation with Missing Modalities. InPro-\nceedings of the AAAI Conference on Artificial Intelligence ,\n1657–1665.\nLiu, Y .; Fan, L.; Zhang, C.; Zhou, T.; Xiao, Z.; Geng, L.;\nand Shen, D. 2021a. Incomplete multi-modal representation\nlearning for Alzheimer’s disease diagnosis. Medical Image\nAnalysis, 69: 101953.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021b. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n10012–10022.\nLu, C.; de Geus, D.; and Dubbelman, G. 2023. Content-\naware Token Sharing for Efficient Semantic Segmentation\nwith Vision Transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n23631–23640.\nRao, Y .; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh, C.-J.\n2021. Dynamicvit: Efficient vision transformers with dy-\nnamic token sparsification. Advances in neural information\nprocessing systems, 34: 13937–13949.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7421\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-net:\nConvolutional networks for biomedical image segmenta-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, 234–241. Springer.\nShe, D.; Zhang, Y .; Zhang, Z.; Li, H.; Yan, Z.; and Sun, X.\n2023. EoFormer: Edge-Oriented Transformer for Brain Tu-\nmor Segmentation. In International Conference on Medi-\ncal Image Computing and Computer-Assisted Intervention ,\n333–343. Springer.\nShen, L.; Zhu, W.; Wang, X.; Xing, L.; Pauly, J. M.; Turk-\nbey, B.; Harmon, S. A.; Sanford, T. H.; Mehralivand, S.;\nChoyke, P. L.; et al. 2020. Multi-domain image comple-\ntion for random missing input data. IEEE transactions on\nmedical imaging, 40(4): 1113–1122.\nTran, L.; Liu, X.; Zhou, J.; and Jin, R. 2017. Missing modal-\nities imputation via cascaded residual autoencoder. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, 1405–1414.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021a. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. In Proceedings of the IEEE/CVF international\nconference on computer vision, 568–578.\nWang, Y .; Zhang, Y .; Liu, Y .; Lin, Z.; Tian, J.; Zhong,\nC.; Shi, Z.; Fan, J.; and He, Z. 2021b. Acn: Adver-\nsarial co-training network for brain tumor segmentation\nwith missing modalities. In Medical Image Computing\nand Computer Assisted Intervention–MICCAI 2021: 24th\nInternational Conference, Strasbourg, France, September\n27–October 1, 2021, Proceedings, Part VII 24, 410–420.\nSpringer.\nWang, Y .; Zhou, L.; Yu, B.; Wang, L.; Zu, C.; Lalush, D. S.;\nLin, W.; Wu, X.; Zhou, J.; and Shen, D. 2018. 3D auto-\ncontext-based locality adaptive multi-modality GANs for\nPET synthesis. IEEE Transactions on Medical Imaging,\n38(6): 1328–1339.\nYang, Q.; Guo, X.; Chen, Z.; Woo, P. Y .; and Yuan, Y . 2022.\nD 2-Net: Dual disentanglement network for brain tumor seg-\nmentation with missing modalities. IEEE Transactions on\nMedical Imaging, 41(10): 2953–2964.\nZhang, Y .; He, N.; Yang, J.; Li, Y .; Wei, D.; Huang, Y .;\nZhang, Y .; He, Z.; and Zheng, Y . 2022. mmformer: Mul-\ntimodal medical transformer for incomplete multimodal\nlearning of brain tumor segmentation. In International\nConference on Medical Image Computing and Computer-\nAssisted Intervention, 107–117. Springer.\nZhang, Y .; Yang, J.; Tian, J.; Shi, Z.; Zhong, C.; Zhang, Y .;\nand He, Z. 2021. Modality-aware mutual learning for multi-\nmodal medical image segmentation. InMedical Image Com-\nputing and Computer Assisted Intervention–MICCAI 2021:\n24th International Conference, Strasbourg, France, Septem-\nber 27–October 1, 2021, Proceedings, Part I 24, 589–599.\nSpringer.\nZhao, Z.; Yang, H.; and Sun, J. 2022. Modality-adaptive fea-\nture interaction for brain tumor segmentation with missing\nmodalities. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, 183–192.\nSpringer.\nZhou, T.; Canu, S.; Vera, P.; and Ruan, S. 2021. Latent corre-\nlation representation learning for brain tumor segmentation\nwith missing MRI modalities. IEEE Transactions on Image\nProcessing, 30: 4263–4274.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7422",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5632984638214111
    },
    {
      "name": "Security token",
      "score": 0.5412594079971313
    },
    {
      "name": "Segmentation",
      "score": 0.49094584584236145
    },
    {
      "name": "Computer science",
      "score": 0.4895668923854828
    },
    {
      "name": "Modalities",
      "score": 0.47309091687202454
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4220947027206421
    },
    {
      "name": "Computer network",
      "score": 0.14683496952056885
    },
    {
      "name": "Engineering",
      "score": 0.11141714453697205
    },
    {
      "name": "Electrical engineering",
      "score": 0.07929977774620056
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}