{
  "title": "Quark/gluon discrimination and top tagging with dual attention transformer",
  "url": "https://openalex.org/W4389476104",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2135195779",
      "name": "Minxuan He",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Academy of Mathematics and Systems Science",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2228519143",
      "name": "Daohan Wang",
      "affiliations": [
        "Konkuk University"
      ]
    },
    {
      "id": "https://openalex.org/A2135195779",
      "name": "Minxuan He",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Academy of Mathematics and Systems Science",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2228519143",
      "name": "Daohan Wang",
      "affiliations": [
        "Konkuk University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2613081657",
    "https://openalex.org/W2967177183",
    "https://openalex.org/W3026610215",
    "https://openalex.org/W3179176799",
    "https://openalex.org/W2100507804",
    "https://openalex.org/W2952645975",
    "https://openalex.org/W2047792789",
    "https://openalex.org/W1987435915",
    "https://openalex.org/W2257617748",
    "https://openalex.org/W4312881242",
    "https://openalex.org/W3134779028",
    "https://openalex.org/W4221140049",
    "https://openalex.org/W3111535274",
    "https://openalex.org/W2586557507",
    "https://openalex.org/W2895846750",
    "https://openalex.org/W2756071324",
    "https://openalex.org/W3081565690",
    "https://openalex.org/W2883681588",
    "https://openalex.org/W2586765566",
    "https://openalex.org/W4387452997",
    "https://openalex.org/W2792435888",
    "https://openalex.org/W2999026142",
    "https://openalex.org/W3128540831",
    "https://openalex.org/W2968312879",
    "https://openalex.org/W2915621743",
    "https://openalex.org/W3215932716",
    "https://openalex.org/W3045164669",
    "https://openalex.org/W2125102738",
    "https://openalex.org/W6600339963",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W3101073376",
    "https://openalex.org/W3002851826",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W3172441158",
    "https://openalex.org/W3101501459",
    "https://openalex.org/W3104674222",
    "https://openalex.org/W3098163999",
    "https://openalex.org/W3101867861",
    "https://openalex.org/W3101363214",
    "https://openalex.org/W3098229010",
    "https://openalex.org/W3099190102"
  ],
  "abstract": "Abstract Jet tagging is a crucial classification task in high energy physics. Recently the performance of jet tagging has been significantly improved by the application of deep learning techniques. In this study, we introduce a new architecture for jet tagging: the particle dual attention transformer (P-DAT). This novel transformer architecture stands out by concurrently capturing both global and local information, while maintaining computational efficiency. Regarding the self attention mechanism, we have extended the established attention mechanism between particles to encompass the attention mechanism between particle features. The particle attention module computes particle level interactions across all the particles, while the channel attention module computes attention scores between particle features, which naturally captures jet level interactions by taking all particles into account. These two kinds of attention mechanisms can complement each other. Further, we incorporate both the pairwise particle interactions and the pairwise jet feature interactions in the attention mechanism. We demonstrate the effectiveness of the P-DAT architecture in classic top tagging and quark–gluon discrimination tasks, achieving competitive performance compared to other benchmark strategies.",
  "full_text": "Eur. Phys. J. C (2023) 83:1116\nhttps://doi.org/10.1140/epjc/s10052-023-12293-1\nRegular Article - Theoretical Physics\nQuark/gluon discrimination and top tagging with dual attention\ntransformer\nMinxuan He 1,2,a, Daohan Wang 3,b\n1 University of Chinese Academy of Sciences, Beijing 100049, People’s Republic of China\n2 Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, People’s Republic of China\n3 Department of Physics, Konkuk University, Seoul 05029, Republic of Korea\nReceived: 18 July 2023 / Accepted: 27 November 2023 / Published online: 8 December 2023\n© The Author(s) 2023\nAbstract Jet tagging is a crucial classiﬁcation task in high\nenergy physics. Recently the performance of jet tagging has\nbeen signiﬁcantly improved by the application of deep learn-\ning techniques. In this study, we introduce a new architec-\nture for jet tagging: the particle dual attention transformer\n(P-DAT). This novel transformer architecture stands out by\nconcurrently capturing both global and local information,\nwhile maintaining computational efﬁciency. Regarding the\nself attention mechanism, we have extended the established\nattention mechanism between particles to encompass the\nattention mechanism between particle features. The particle\nattention module computes particle level interactions across\nall the particles, while the channel attention module com-\nputes attention scores between particle features, which nat-\nurally captures jet level interactions by taking all particles\ninto account. These two kinds of attention mechanisms can\ncomplement each other. Further, we incorporate both the\npairwise particle interactions and the pairwise jet feature\ninteractions in the attention mechanism. We demonstrate the\neffectiveness of the P-DAT architecture in classic top tagging\nand quark–gluon discrimination tasks, achieving competitive\nperformance compared to other benchmark strategies.\n1 Introduction\nIn high-energy physics experiments, tagging jets, which are\ncollimated sprays of particles produced from high-energy\ncollisions, is a crucial task for discovering new physics\nbeyond the Standard Model. Jet tagging involves distinguish-\ning boosted heavy particle jets from those of QCD initiated\nquark/gluon jets. Since jets initiated by different particles\na e-mail: hemx@amss.ac.cn\nb e-mail: wdh9508@gmail.com (corresponding author)\nexhibit different characteristics, two key issues arise: how to\nrepresent a jet and how to analyze its representation. Conven-\ntionally, jet tagging has been performed using hand-crafted\njet substructure variables based on physics motivation. Nev-\nertheless, these methods can often fall short in capturing intri-\ncate patterns present in the raw data.\nOver the past decade, deep learning approaches have been\nextensively adopted to enhance the jet tagging performance\n[19]. Various jet representations have been proposed, includ-\ning image-based representation using Convolutional Neural\nNetwork (CNN) [ 2,8,11,17,20,21,25,32], sequence-based\nrepresentation with Recurrent Neural Network [ 1,10], tree-\nbased representation with Recursive Neural Network [7,23]\nand graph-based representation with Graph Neural Network\n(GNN) [3,4,14,16,24,33]. More recently, one representation\napproach that has gained signiﬁcant attention is to view the\nset of constituent particles inside a jet as points in a point\ncloud. Point clouds are used to represent a set of objects in an\nunordered manner, described in a deﬁned space. By adopting\nthis approach, each jet can be interpreted as a particle cloud,\nwhich treats a jet as a permutation-invariant set of parti-\ncles, allowing us to extract meaningful information with deep\nlearning method. Based on the particle cloud representation,\nvarious deep learning architectures have been introduced,\nsuch as Deep Set Framework [18], ABCNet [26], LorentzNet\n[14] and ParticleNet [ 30]. Deep Set Framework provides a\ncomprehensive explanation of how to parametrize permuta-\ntion invariant functions for inputs with variable lengths, tak-\ning into consideration both infrared and collinear safety. Par-\nticleNet adapts the Dynamic Graph CNN architecture [ 37],\nwhile ABCNet takes advantage of attention mechanisms to\nenhance the local feature extraction. The LorentzNet focused\nmore on incorporating inductive biases derived from physics\nprinciples into the architecture design, utilizing an efﬁcient\nMinkowski dot product attention mechanism. All of these\n123\n1116 Page 2 of 10 Eur. Phys. J. C (2023) 83 :1116\narchitectures realize substantial performance improvement\non top tagging and quark/gluon discrimination benchmarks.\nOver the past few years, attention mechanisms have\nbecome as a powerful tool for capturing intricate patterns in\nsequential and spatial data. The transformer architecture [35],\nwhich leverages attention mechanisms, has been highly suc-\ncessful in natural language processing and computer vision\ntasks such as image recognition. However, when dealing\nwith point cloud representation, which inherently lack a spe-\nciﬁc order, modiﬁcations to the original transformer struc-\nture are required to establish a self-attention operation that\nis invariant to input permutations. To address these issues, a\nrecent approach known as point cloud transformer (PCT)\n[15,27] was proposed, which entails passing input points\nthrough a feature extractor to create a high-dimensional rep-\nresentation of particle features. The transformed data is then\npassed through a self-attention module that introduces atten-\ntion coefﬁcients for each pair of particles. Another notable\napproach is the particle transformer [31], which incorporates\npairwise particle interactions within the attention mecha-\nnism and obtains higher tagging performance than a plain\ntransformer and surpasses the previous state-of-the-art, Par-\nticleNet, by a large margin.\nIn recent studies, the dual attention vision transformer\n(DaViT) [12] has exhibited promising results for image clas-\nsiﬁcation. The DaViT introduces the dual attention mecha-\nnism, comprising spatial window attention and channel group\nattention, enabling the effective capture of both global and\nlocal features in images. In this paper, we utilize the dual\nattention mechanism for jet tagging based on point cloud\nrepresentation. We expanded the particle self-attention estab-\nlished by existing works by introducing the channel self-\nattention. In the particle self-attention, the particle num-\nber deﬁnes the scope, and the dimension of particle feature\ndeﬁnes the feature dimension. While in the channel self-\nattention, the channel dimension deﬁnes the scope, and the\nparticle number deﬁnes the feature dimension. Thus each\nchannel contains an abstract representation of the entire jet.\nBy performing self-attention on these channels, we capture\nthe global interaction by considering all the particles when\ncomputing attention scores between each pair of channels.\nCompared to existing particle self-attention, the channel self-\nattention is naturally imposed from a global jet perspective\nrather than a particle one. To achieve the dual attention mech-\nanism, we introduce the channel attention module. By alter-\nnately applying the particle attention module and the chan-\nnel attention module to combine both the local information\nof the particle representation and the global information of\nthe jet representation for jet tagging, we build a new network\nstructure, called particle dual attention transformer (P-DAT).\nFurthermore, inspired by Ref. [31], we design the pairwise jet\nfeature interaction. We incorporate both the pairwise particle\ninteraction and the pairwise jet feature interaction to increase\nthe expressiveness of the attention mechanism. We evaluate\nthe performance of P-DAT on top tagging and quark/gluon\ndiscrimination tasks and compare its performance against\nother baseline models. Our analysis demonstrates the effec-\ntiveness of P-DAT in jet tagging and highlights its potential\nfor future applications in high-energy physics experiments.\nThis article is organized as follows. In Sect. 2,w ei n t r o -\nduce the particle dual attention transformer for jet tagging,\nproviding a detailed description of model implementation. In\nSect. 3, we present the performance of P-DAT and the exist-\ning algorithms obtained for top tagging task and quark/gluon\ndiscrimination task, utilizing several evaluation metrics and\nprovide an extensive discussion of these results. In Sect. 4,\nwe conduct a comprehensive comparison of computational\nresource requirements for evaluating each model, including\nthe number of trainable weights and the number of ﬂoating-\npoint operations (FLOPs). Finally, our conclusions are pre-\nsented in Sect. 5.\n2 Model architecture\nThe focus of this paper is to introduce the particle dual atten-\ntion transformer (P-DAT), which is designed to capture both\nthe local particle-level information and the global jet level\ninformation. In this section, we ﬁrst introduce overall struc-\nture of the model architecture. Then we delve into the details\nof the channel attention module and its combination with\nthe particle attention module. Finally, we present the model\nimplementation.\n2.1 Overall structure\nThe whole model architecture is illustrated in Fig. 1. It con-\ntains three key components, namely the feature extractor, the\nparticle attention module and the channel attention module.\nFirst of all, we employ the same feature extractor as in\nRef. [ 27] to transform the inputs from P × 7 to a higher\ndimensional representation P × N , where P represents the\nnumber of particles within the jet, and N denotes the dimen-\nsion of the embedding features for each particle. As shown in\nFig. 2(left), the feature extractor block incorporates an Edge\nConvolution (EdgeConv) operation [36] followed by 3 two-\ndimensional convolutional (Conv2D) layers and an average\npooling operation across all neighbors of each particle. The\nEdgeConv operation adopts a k-nearest neighbors approach\nwith\nk = 20 to extract local information for each particle\nbased on the proximity in theη −φ space. All convolutional\nlayers are implemented with stride and kernel size of 1 and\nare followed by a batch normalization operation and GELU\nactivation function. Same as in Ref. [27], we employed two\nfeature extractors with N = 128 and N = 64, respectively.\n123\nEur. Phys. J. C (2023) 83 :1116 Page 3 of 10 1116\nFig. 1 Illustration of the whole model architecture\nSubsequently, we alternately stack two particle attention\nmodules and two channel attention modules to combine both\nthe local information of the particle representation and the\nglobal information of the jet representation. A dropout rate\nof 0.1 is applied to all particle attention blocks and chan-\nnel attention blocks. Furthermore, inspired by Ref. [31], we\ndesigned a channel interaction matrix based on physics prin-\nciples. Then we incorporate the particle interaction matrix\nto the particle attention module and incorporate the chan-\nnel interaction matrix to the channel attention module. For\nthe particle interaction matrix, we utilize a 3-layer two-\ndimensional convolution with (32,16,8) channels with stride\nand kernel size of 1 to map the particle interaction matrix\nto a new embedding P × P × N\nh , where Nh is the num-\nber of heads in the particle self attention module. As for the\nchannel interaction matrix, we utilize an upsampling opera-\ntion and a 3-layer two-dimensional convolution to map the\nchannel interaction matrix to a higher dimensional represen-\ntation N × N , with N the input particle embedding dimen-\nsion. Therefore, to process a jet of P particles, the P-DAT\nrequires three inputs: the jet dataset, the particle interaction\nmatrix and the jet feature interaction matrix derived from the\nkinetic information of each particle inside the jet.\nNext, the outputs of the particle attention blocks and\nchannel attention blocks are concatenated, followed by an\n1 dimensional Convolutional Neural Network (CNN) layer\nwith 256 nodes and an average pooling operation across all\nparticles. This output is then directly fed into a 3-layer MLP\nwith (256, 128, 2) nodes, as shown in Fig. 2(right). In addi-\ntion, a batch normalization operation, a dropout rate of 0.5\nand the GELU activation function are applied to the second\nlayer. Finally, the last layer employs a softmax operation to\nproduce the ﬁnal classiﬁcation scores. It is worth noting that\nFig. 2 Illustration of the feature extractor block and the MLP block\nthe inclusion of class attention blocks, as described in Ref.\n[31], did not lead to an improvement in performance of P-\nDAT, as observed in our experiments.\n2.2 Particle attention module\nThe particle self-attention block, which is already estab-\nlished in the existing papers, aims to establish the relation-\nship between all particles within the jet using an attention\nmechanism. As presented in Fig.3, three matrices, which are\ncalled query (Q), key (K), and value (V), are built from linear\ntransformations of the original inputs. Attention weights are\n123\n1116 Page 4 of 10 Eur. Phys. J. C (2023) 83 :1116\nFig. 3 Illustration of the particle multi-head attention block\ncomputed by matrix multiplication between Q and K, rep-\nresenting the matching between them. Same as the particle\ntransformer work [31], we incorporate the particle interaction\nmatrix U1 as a bias term to enhance the scaled dot-product\nattention. This incorporation of particle interaction features,\ndesigned from physics principles, modiﬁes the dot-product\nattention weights, thereby enhancing the expressiveness of\nthe attention mechanism. The same U\n1 is shared across\nthe two particle attention blocks. After normalization, these\nattention weights reﬂect the weighted importance between\neach pair of particles. The self-attention is then obtained by\nthe weighted elements of V , which results from multiplying\nthe attention weights and the value matrix. It is important to\nnote that P represents the number of particles, andN denotes\nthe total number of features. The attention weights are com-\nputed as:\nA(Q,K,V) = Concat(head\n1,..., headNh )\nwhere head i = Attention(Qi ,Ki ,Vi )\n= softmax\n[ Qi (Ki )T\n√Ch\n+ U1\n]\nVi (1)\nwhere Qi = Xi WQ\ni , Ki = Xi WK\ni , and Vi = Xi WV\ni\nare RP×Nh dimensional visual features with Nh heads, Xi\ndenotes the ith head of the input feature and Wi denotes\nthe projection weights of the ith head for Q,K,V, and\nN = Ch ∗ Nh . The particle attention block incorporates a\nLayerNorm layer both before and after the multi-head atten-\ntion module. A two-layer MLP, with LayerNorm preceding\neach linear layer and GELU nonlinearity in between, fol-\nlows the multi-head attention module. Residual connections\nare applied after the multi-head attention module and the\ntwo-layer MLP. In our study, we set N\nh = 8 and N = 64.\nFig. 4 Illustration of the channel attention block\n2.3 Channel attention module\nThe main contribution of this paper is to explore the self-\nattention mechanism from another perspective and propose\nthe channel-wise attention mechanism for jet tagging. Unlike\nthe previous particle self-attention mechanism which com-\nputes the attention weights between each pair of particles,\nwe apply attention mechanisms on the transpose of particle-\nlevel inputs and compute the attention weights between each\npair of particle features. In this way, the channel-wise atten-\ntion mechanism naturally capture the global interaction of\neach pair of particle features by taking all the particles into\naccount, which can be viewed as the interaction of each pair\nof jet features. Additionally, taking inspiration from Ref.\n[31], we have devised a jet feature interaction matrix based\non physics principles, which can be added to enhance the\nexpressiveness of the channel attention mechanism.\nAs depicted in Fig. 4, the channel self-attention block\napplies attention mechanisms to the jet features, enabling\ninteractions among the channels. To capture global informa-\ntion in the particle dimension, we set the number of heads to\n1, where each channel represents a global jet feature. Conse-\nquently, all the channels interact with each other. This global\nchannel attention mechanism is deﬁned as follows:\nA(Q\ni ,Ki ,Vi ) = softmax\n[\nQT\ni Ki√\nC\n+ U2\n]\nVT\ni (2)\nwhere Qi ,Ki ,Vi ∈ RC×P are channel-wise jet-level\nqueries, keys, and values. Note that although we perform the\ntranspose in the channel attention block, the projection layers\nW and the scaling factor 1√\nC are computed along the chan-\nnel dimension, rather than the particle dimension. Similar as\n123\nEur. Phys. J. C (2023) 83 :1116 Page 5 of 10 1116\nthe particle self-attention block, we incorporate the designed\nchannel interaction matrix U2 as a bias term to enhance the\nscaled dot-product attention. The same U2 matrix is shared\nacross the two channel attention blocks. After normaliza-\ntion, the attention weights indicate the weighted importance\nof each pair of global features. The self-attention mechanism\nproduces the weighted elements of V , obtained by multiply-\ning the attention weights and the value matrix. Addition-\nally, the channel attention block includes a LayerNorm layer\nbefore and after the attention module, followed by a two-layer\nMLP. Each linear layer is preceded by a LayerNorm layer,\nand a GELU nonlinearity is applied between them. Residual\nconnections are added after the channel attention module and\nthe two-layer MLP.\n2.4 Combination of particle attention module and channel\nattention module\nThroughout the whole architecture, all the particle attention\nmodules and the channel attention modules are stacked while\nmaintaining a consistent feature dimension of N = 64. The\nchannel attention module captures global information and\ninteractions, while the particle attention module extracts local\ninformation and interactions. In the context of the channel\nself-attention mechanism, a C × C-dimensional attention\nmap is computed, involving all the particles, resulting in a\ncomputation of the form(C ×P)·(P ×C). This global atten-\ntion map enables the channel attention module to dynam-\nically fuse multiple global perspectives of the jet. Subse-\nquently, a transpose operation is performed, yielding outputs\nwith new channel information, which are then passed to the\nsubsequent particle attention module. Conversely, in the par-\nticle self-attention mechanism, a P × P-dimensional atten-\ntion map is computed by considering all particle features,\nresulting in a computation of the form(P ×C)·(C ×P).T h i s\nlocal attention map empowers the particle attention module\nto dynamically fuse multiple local views of the jet, generat-\ning new particle features and passing the information to the\nfollowing channel attention module. By alternatively apply-\ning these two types of modules, the local information and\nglobal information can complement each other.\n2.5 Model implementation\nThe PYTORCH [ 29] deep learning framework is utilized\nto implement the model architecture with the CUDA plat-\nform. The training and evaluation steps are accelerated using\na NVIDIA GeForce RTX 3070 GPU for acceleration. We\nadopt the binary cross-entropy as the loss function. To opti-\nmize the model parameters, we employ the AdamW opti-\nmizer [22] with an initial learning rate of 0.0005, which is\ndetermined based on the gradients calculated on a mini-batch\nof 64 training examples. The network is trained up to 100\nepochs, with the learning rate decreasing by a factor of 2\nevery 10 epochs to a minimal of 10\n−6. In addition, we employ\nthe early-stopping technique to prevent over-ﬁtting.\nFurthermore, as mentioned in Ref. [ 31], the introduction\nof the pairwise interaction matrix based on physics principle\nsigniﬁcantly increases the computational time and memory\nconsumption, therefore limiting the number of pairwise inter-\naction matrix which is the prior knowledge based on physics\nprinciple. In this paper, in order to address the memory issue\ncaused by huge input data, we implemented the Chunk Load-\ning strategy, a commonly used technique in the ﬁeld of deep\nlearning for data loading. This approach entails continuously\nimporting and deleting data during the training, validation\nand test process, enabling us to train our model on a large\ndataset while mitigating the memory load. We give a detailed\ndescription of this approach in the following:\nWithin a loop, input data batches are dynamically loaded\nfor training, validation, and test. Each batch contains 1280\nevents. Regardless of whether it’s for training, validation,\nor testing, the data loading process remains consistent.\nThis uniformity ensures that the iteration counts for train-\ning, validation, and testing may vary, but the data-handling\napproach remains the same. During each iteration, we employ\nNumPy’s memory-mapped ﬁle access to efﬁciently retrieve\ntraining data, corresponding labels, particle interaction matri-\nces, and jet interaction matrices. Once this batch is pro-\ncessed for training/validation/testing, the loaded data is sub-\nsequently removed to free up memory resources. Subse-\nquently, we proceed to load the next batch of data for next\niteration. This method signiﬁcantly reduces memory con-\nsumption by allowing us to access the necessary data without\nthe need to load the entire dataset into memory all at once.\nThis strategic approach not only optimizes memory utiliza-\ntion but also effectively mitigates the challenges associated\nwith handling substantial input data. It allows us to train our\nmodel efﬁciently while preventing memory exhaustion.\n3 Results of jet classiﬁcation\nThe P-DAT architecture is designed to process input data con-\nsisting of particles inside the jets. Based on the point cloud\nrepresentation, we regard each constituent particle as a point\nin theη−φ space and the whole jet as a point cloud. To ensure\nconsistency and facilitate meaningful comparisons, we ﬁrst\nsorted the particles inside the jets by transverse momentum\nand a maximum of 100 particles per jet are employed. The\ninput jet is truncated if the particle number inside the jet is\nmore than 100 and the input jet is zero-padded up to the 100\nif fewer than 100 particles are present. In this process, the\nzero-padded constituent particles were directly introduced\nas zeros into the model, without the utilization of any addi-\ntional masking. This selection of 100 particles is sufﬁcient to\n123\n1116 Page 6 of 10 Eur. Phys. J. C (2023) 83 :1116\nTable 1 The jet feature pairwise interaction matrix used as the inputs for the P-DAT. Here PID represents the particle identiﬁcation\nIE pT\n∑ pTf\n∑ E f Δη Δφ ΔR PID\nE1 pT\nE 01 0 0 0 E PID\nE\npT\npT\nE 11 0 0 0 0 pTP ID\npT∑ pTf 01 1 0 0 0 0 pTfP ID\n∑ E f 10 0 1 0 0 0 E fP I D\nΔη 00 0 0 1 0 Δη\nΔR ΔηPID\nΔφ 00 0 0 0 1 Δφ\nΔR ΔφPID\nΔR 00 0 0 Δη\nΔR\nΔφ\nΔR 1 ΔR PID\nPID E PID\nE\npTP ID\npT pTfP ID E fP I D ΔηPID ΔφPID ΔR PID 1\ncover the vast majority of jets contained within all datasets,\nensuring comprehensive coverage. Each jet is characterized\nby the 4-momentum of its constituent particles. Based on this\ninformation, we reconstructed 7 features for each particle.\nAdditionally, for the quark–gluon dataset, we included the\nParticle Identiﬁcation (PID) information as the 8-th feature.\nThese features are as follows:\n{\nlog E , log p\nT, pT\npTJ\n, E\nE J\n,Δ ηΔ φ ,Δ R, PID\n}\n. (3)\nFor the pairwise particle interaction matrix, we adopt the\nsame four features as employed in Refs. [ 13,31]. Addition-\nally, we include the difference in transverse momentum as\nan additional feature. To summarize, we calculated the fol-\nlowing 5 features for any pair of particles a and b with four-\nmomentum p\na and pb, respectively:\nΔR =\n√\n(ya − yb)2 + (φa − φb)2,\nkT = min(pT,a , pT,b)Δ,\nz = min(pT,a , pT,b)/(pT,a + pT,b),\nm2 = (Ea + Eb)2 −∥ pa + pb∥2,\nΔpT =| pT,a − pT,b| (4)\nwhere yi represents the rapidity, φi denotes the azimuthal\nangle, pT,i = (p2\nx ,i + p2\ny,i )1/2 denotes the transverse\nmomentum, pi = (px ,i , py,i , pz,i ) represents the momen-\ntum 3-vector and ∥·∥ is the norm, for i = a, b.A s\nmentioned in Ref. [ 31], we take the logarithm and use\n(ln Δ, ln kT,ln z,ln m2,ln ΔpT) as the interaction features\nfor each particle pair to avoid the long tail problem. More-\nover, apart from the 5 interaction features, we design one\nmore feature for the quark–gluon benchmark dataset, deﬁned\nas δ\ni,j , where i and j are the Particle Identiﬁcation of the par-\nticles a and b.\nFurthermore, as mentioned in Sect. 2, we have designed\na pairwise jet feature interaction matrix, drawing inspiration\nfrom the work Ref. [ 31]. The list of all jet features used in\nthis study is presented below. Note that all the jet features are\ncalculated based on the four-momentum of all the constituent\nparticles within the jet. The interaction matrix is constructed\nbased on a straightforward yet effective ratio relationship, as\nillustrated in Table 1.\n{\nE, p\nT,\n∑\npTf ,\n∑\nE f , Δη, Δφ, ΔR, PID\n}\n. (5)\nTo provide a clearer explanation of the concept of the\njet feature pairwise interaction matrix, we will now present\na detailed description. The ﬁrst variable E represents the\nenergy of the input jet. pT denotes the transverse momen-\ntum of the input jet, while ∑ pTf and ∑ E f represent the\nsum of the transverse momentum fractions and the energy\nfractions of all the constituent particles inside the input jet,\nrespectively. Additionally,\nΔη, Δφ and ΔR correspond to the\ntransverse momentum weighted sum of the Δη, Δφ, ΔR of\nall the constituent particles inside the input jet, respectively.\nHere Δη, Δφ and ΔR refer to the distances in the η − φ\nspace between each constituent particle and the input jet.\nFurthermore, for the quark–gluon dataset, we incorporated\nthe 8th feature based on the particle identiﬁcation informa-\ntion. It represents the particle identiﬁcation associated with\nthe speciﬁc particle type whose sum of transverse momentum\naccounts for the largest proportion of the entire jet transverse\nmomentum. The entire jet feature pairwise interaction matrix\nis deﬁned as a symmetric block matrix with diagonal ones.\nFor convenience, we named{E, p\nT, ∑ pTf , ∑ E f }as vari-\nable set 1 and {Δη, Δφ, ΔR} as variable set 2. We build\nthe pairwise interactions among variable set 1 and variable\nset 2, respectively. Firstly, we employ a ratio relationship to\ndeﬁne the interaction between E and p\nT}. Additionally, we\nestablish that the interaction between∑ E f and E is 1, while\nno interactions exist between∑ E f and any other variables,\nexcept for E and particle identiﬁcation. Similarly, we deﬁne\nthe interaction between ∑ pTf and pT as 1, with no inter-\nactions between ∑ pTf and any other variables, except for\npT and particle identiﬁcation.\n123\nEur. Phys. J. C (2023) 83 :1116 Page 7 of 10 1116\nTable 2 Comparison between\nthe performance reported for\nP-DAT and existing\nclassiﬁcation algorithms on the\nquark–gluon discrimination\ndataset. The uncertainty is\ncalculated by taking the\nstandard deviation of 5 training\nruns with different random\nweight initialization\nAccuracy AUC Rej 50% Rej30%\nResNeXt-50 [30] 0.821 0.9060 30.9 80.8\nP-CNN [30] 0.827 0.9002 34.7 91.0\nPFN [18] – 0.9005 34.7 ± 0.4 –\nParticleNet-Lite [30] 0.835 0.9079 37.1 94.5\nParticleNet [30] 0.840 0.9116 39.8 ± 0.2 98.6 ± 1.3\nABCNet [26] 0.840 0.9126 42.6 ± 0.4 118.4 ± 1.5\nSPCT [27] 0.815 0.8910 31.6 ± 0.3 93.0 ± 1.2\nPCT [27] 0.841 0.9140 43.2 ± 0.7 118.0 ± 2.2\nLorentzNet [14] 0.844 0.9156 42.4 ± 0.4 110.2 ± 1.3\nParT [31] 0.849 0.9203 47.9 ± 0.5 129.5 ± 0.9\nP-DAT 0.839 0.9092 39.2 ± 0.6 95.1 ± 1.3\nSecondly, we apply a ratio relationship to deﬁne the inter-\naction between ΔR and {Δη, Δφ}, while no interaction is\nspeciﬁed between Δη and Δφ. Finally, we determine the\ninteractions between particle identiﬁcation and all other vari-\nables as the ratio of the sum of the corresponding variables\nof the particles associated with the particle identiﬁcation to\nthe variable of the jet.\n3.1 Quark/gluon discrimination\nThe quark–gluon benchmark dataset [ 18] was produced\nusing Pythia8 [ 34] without detector simulation. It includes\nquark-initiated samples q\nq → Z → νν + (u,d,s) as signal\nand gluon-initiated data qq → Z → νν + g as background.\nJet clustering was performed using the anti-kT algorithm\nwith R = 0.4. Only jets with transverse momentum pT ∈\n[500, 550] GeV and rapidity |y| < 1.7 were selected for\nfurther analysis. Each particle within the dataset comprises\nnot only the four-momentum, but also the particle identiﬁ-\ncation information, which classiﬁes the particle type as elec-\ntron, muon, charged hadron, neutral hadron, or photon. The\nofﬁcial dataset compromises of 1.6M training events, 200k\nvalidation events and 200k test events, respectively. In this\npaper, we focused on the leading 100 constituents within\neach jet, utilizing their four-momenta and particle identiﬁ-\ncation information for training purposes. For jets with fewer\nthan 100 constituents, zero-padding was applied. For each\nparticle, a set of 8 input features was used, based solely on\nthe four-momenta and identiﬁcation information of the par-\nticles clustered within the jet. The accuracy, area under the\ncurve (AUC), and background rejection results are presented\nin Table 2.\nFrom Table 2, we can see that in the context of the\nquark/gluon discrimination task, P-DAT exhibits powerful\nclassiﬁcation performance, surpassing the majority of mod-\nels while falling slightly behind other two transformer-based\nmodels, PCT and ParT. The superior results of ParT can\nbe attributed to its signiﬁcantly more complex architecture\nwith a total of L = 8 particle attention blocks and 2 class\nattention blocks. The model complexity of ParT exceeds\nthe P-DAT model by a substantial margin. As for the PCT\nmodel, all self-attention layers employ query, key, and value\nmatrices obtained through one-dimensional convolutional\nlayers, resulting in a larger number of FLOPs compared\nto our model. P-DAT strikes a favorable balance between\nperformance and model complexity. Additionally, our P-\nDAT model incorporates the channel attention module, offer-\ning greater ﬂexibility in leveraging abundant jet information\ncompared to the other two methods.\n3.2 Top tagging\nThe benchmark dataset [ 5] used for top tagging comprises\nhadronic tops as the signal and QCD di-jets as the back-\nground. Pythia8 [ 34] was employed for event generation,\nwhile Delphes [ 9] was utilized for detector simulation. All\nthe particle-ﬂow constituents were clustered into jets using\nthe anti-kT algorithm [ 6] with a radius parameter of R =\n0.8. Only jets with transverse momentum p\nT ∈ [550, 650]\nGeV and rapidity |y| < 2 were included in the analysis.\nThe ofﬁcial dataset contains 1.2M training events, 400k val-\nidation events and 400k test events, respectively. Only the\nenergy-momentum 4-vectors for each particles inside the\njets are provided. In this paper, the leading 100 constituent\nfour-momenta of each jet were utilized for training purposes.\nFor jets with fewer than 100 constituents, zero-padding was\napplied. For each particle, a set of 7 input features based\nsolely on the four-momenta of the particles clustered inside\nthe jet was utilized. The accuracy, area under the curve\n(AUC), and background rejection results can be found in\nTable 3.\nFrom Table 3, a similar pattern emerges when analyz-\ning the performance of models in the top tagging task. P-\nDAT exhibits competitive classiﬁcation performance. While\n123\n1116 Page 8 of 10 Eur. Phys. J. C (2023) 83 :1116\nTable 3 Comparison between\nthe performance reported for\nP-DAT and existing\nclassiﬁcation algorithms on the\ntop tagging dataset. The\nuncertainty is calculated by\ntaking the standard deviation of\n5 training runs with different\nrandom weight initialization\nAccuracy AUC Rej 50% Rej30%\nResNeXt-50 [30] 0.936 0.9837 302 ± 5 1147 ± 58\nP-CNN [30] 0.930 0.9803 201 ± 4 759 ± 24\nPFN [18] – 0.9819 247 ± 3 888 ± 17\nParticleNet-Lite [30] 0.937 0.9844 325 ± 5 1262 ± 49\nParticleNet [30] 0.940 0.9858 397 ± 7 1615 ± 93\nJEDI-net [28] 0.9263 0.9786 – 590.4\nSPCT [27] 0.928 0.9799 201 ± 9 725 ± 54\nPCT [27] 0.940 0.9855 392 ± 7 1533 ± 101\nLorentzNet [14] 0.942 0.9868 498 ± 18 2195 ± 173\nParT [31] 0.940 0.9858 413 ± 16 1602 ± 81\nP-DAT 0.932 0.9768 228 ± 8 876 ± 39\nother two transformer-based models, PCT and ParT, achieve\nmodestly enhanced performance, especially in terms of back-\nground rejection rates, which reach nearly twice that of our\nP-DAT model, this advantage comes at the cost of increased\nmodel complexity and resource demands.\nFurthermore, given that our P-DAT model includes the\nchannel attention module and considering the distinct jet\nsubstructure characteristics observed in boosted top jets and\nboosted QCD jets, we have the opportunity to formulate a set\nof jet substructure variables and develop an additional self-\nattention module to calculate attention weights for every pair\nof these jet substructure variables. The resulting attention\nweight matrix can be employed as a bias term to augment\nchannel scaled dot-product attention. This can be an inter-\nesting research direction in the future to enhance the per-\nformance of top tagging. While we acknowledge that Parti-\ncleNet Lite achieves higher background rejection rates with\nsmaller model complexity regarding top tagging task, we\nbelieve that the adaptability and innovation inherent in the P-\nDAT model, combining the global jet information and local\nparticle information, pave the way for exciting possibilities\nin this ﬁeld.\n4 Computational complexity\nIn addition to evaluating the algorithm’s performance, it’s\ncrucial to consider the computational cost involved. To\ngauge the computational resources needed for assessing each\nmodel, we calculate both the number of trainable parameters\nand the number of ﬂoating-point operations (FLOPs). Table4\npresents a comparative analysis of these factors across vari-\nous algorithms.\nIn the context of computational complexity comparison\namong various models, our P-DAT model emerges as a\nnotable candidate. While the number of P-DAT trainable\nparameters is increased by more than 2.6 times compared\nTable 4 Comparison between the number of trainable weights and\nﬂoating point operations (FLOPs) reported for P-DAT and existing clas-\nsiﬁcation algorithms\nParameters FLOPs\nResNeXt-50 [30] 1.46M –\nP-CNN [30] 354k 15.5M\nPFN [18] 86.1k 4.62M\nParticleNet-Lite [30] 26k –\nParticleNet [30] 370k 540M\nABCNet [26] 230k –\nSPCT [27]7 k 2 . 4 M\nPCT [27] 193.3k 266M\nLorentzNet [14] 224k –\nParT [31] 2.13M 260M\nP-DAT 498k 144M\nto PCT, the number of ﬂoating point operations (FLOPs) is\nactually 45% lower. Notably, when compared to ParticleNet,\nPCT, and ParT, P-DAT features the smallest FLOPs. P-DAT\ndistinguishes itself by maintaining a comparatively modest\nparameter count at 498k while offering a reasonable level of\ncomputational efﬁciency with 144 M FLOPs. This balance\nbetween model complexity and computational demands posi-\ntions P-DAT as an attractive choice for practical applications,\nwhere it can potentially deliver competitive performance with\nfewer computational resources, making it a promising option\nfor deployment and further research.\n5 Conclusion\nIn this study, we introduced the particle dual attention trans-\nformer (P-DAT) as an innovative model architecture for jet\ntagging. We designed the channel attention module and alter-\nnately employed the particle attention module and the chan-\n123\nEur. Phys. J. C (2023) 83 :1116 Page 9 of 10 1116\nnel attention module to capture both jet-level global informa-\ntion and particle-level local information, while maintaining\ncomputational efﬁciency. Additionally, we incorporate both\nthe pairwise particle interactions and the pairwise jet fea-\nture interactions in the attention mechanism. We evaluate\nthe P-DAT architecture on the classic top tagging task and\nthe quark–gluon discrimination task and achieve competi-\ntive results compared to other benchmark strategies. Notably,\nour P-DAT maintains a relatively modest parameter count\n498k while simultaneously delivering a reasonable level of\ncomputational efﬁciency with 144 M FLOPs, which strikes\na balance between computational complexity and model\nperformance. Besides, given the substantial computational\ndemands posed by introducing a pairwise interaction matrix\nbased on physics principles, which can impact both time and\nmemory resources, we have introduced the Chunk loading\nstrategy which involves dynamic data import and deletion\nthroughout the training, validation, and testing phases, effec-\ntively addressing memory usage constraints.\nFinally, channel attention module opens up more possi-\nbilities for future exploration. For instance, in this study we\nproposed the channel attention module and designed the jet\nfeature interaction matrix as our primary contributions. As an\nalternative approach to utilizing simple ratio-based interac-\ntion matrix, we could explore the possibility of constructing\na dedicated attention module for jet features. By incorpo-\nrating the resulting attention weight matrix into the channel\nattention module, we may potentially enhance performance.\nThis strategy offers the advantage of incorporating valuable\nsupplementary jet information and leveraging the intrinsic\npatterns within jet features revealed by the jet feature atten-\ntion mechanism.\nAcknowledgements This work of Daohan Wang is funded by the\nNational Research Foundation of Korea, Grant no. NRF-2022R1A2C1\n007583. The work of Minxuan He is supported by the Fundamental\nResearch Funds for the Central Universities.\nData Availability Statement The manuscript has associated data in a\ndata repository. [Authors’ comment: The quark-gluon dataset and the\ntop dataset utilized in our study correspond to the dataset in [ 18]a n d\n[5], respectively. The detailed preprocessing procedures applied to these\ndatasets have been comprehensively elucidated within the manuscript.]\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adaptation,\ndistribution and reproduction in any medium or format, as long as you\ngive appropriate credit to the original author(s) and the source, pro-\nvide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article\nare included in the article’s Creative Commons licence, unless indi-\ncated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permit-\nted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nFunded by SCOAP\n3.S C O A P3 supports the goals of the International\nYear of Basic Sciences for Sustainable Development.\nReferences\n1. Identiﬁcation of jets containing b-hadrons with recurrent neu-\nral networks at the ATLAS experiment. Technical report, CERN,\nGeneva (2017)\n2. Quark versus gluon jet tagging using jet images with the ATLAS\ndetector. 7 (2017)\n3. M. Abdughani, J. Ren, L. Wu, J.M. Yang, Probing stop pair pro-\nduction at the LHC with graph neural networks. JHEP 08, 055\n(2019)\n4. M. Abdughani, D. Wang, L. Wu, J.M. Yang, J. Zhao, Probing the\ntriple Higgs boson coupling with machine learning at the LHC.\nP h y s .R e v .D104(5), 056003 (2021)\n5. L. Benato et al., Shared data and algorithms for deep learning in\nfundamental physics. Comput. Softw. Big Sci. 6(1), 9 (2022)\n6. M. Cacciari, G.P. Salam, G. Soyez, The anti-k\nt jet clustering algo-\nrithm. JHEP 04, 063 (2008)\n7. T. Cheng, Recursive neural networks in quark/gluon tagging. Com-\nput. Softw. Big Sci. 2(1), 3 (2018)\n8. J. Cogan, M. Kagan, E. Strauss, A. Schwarztman, Jet-images: com-\nputer vision inspired techniques for jet tagging. JHEP 02, 118\n(2015)\n9. J. de Favereau, C. Delaere, P. Demin, A. Giammanco, V . Lemaître,\nA. Mertens, M. Selvaggi, DELPHES 3, a modular framework for\nfast simulation of a generic collider experiment. JHEP 02, 057\n(2014)\n10. R.T. de Lima, Sequence-based machine learning models in jet\nphysics. 2 (2021). arXiv:2102.06128\n11. L. de Oliveira, M. Kagan, L. Mackey, B. Nachman, A. Schwartz-\nman, Jet-images—deep learning edition. JHEP 07, 069 (2016)\n12. M. Ding, B. Xiao, N. Codella, P. Luo, J. Wang, L. Yuan, Davit: dual\nattention vision transformers. in Computer Vision—ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23–27, 2022\nProceedings, Part XXIV (Springer, 2022), pp. 74–92\n13. F.A. Dreyer, H. Qu, Jet tagging in the Lund plane with graph net-\nworks. JHEP 03, 052 (2021)\n14. S. Gong, Q. Meng, J. Zhang, H. Qu, C. Li, S. Qian, D. Weitao,\nZ.-M. Ma, T.-Y . Liu, An efﬁcient Lorentz equivariant graph neural\nnetwork for jet tagging. JHEP 07, 030 (2022)\n15. M.-H. Guo, J.-X. Cai, Z.-N. Liu, M. Tai-Jiang, R.R. Martin, S.-\nM. Hu, PCT: point cloud transformer. Comput. Vis. Media 7(2),\n187–199 (2021)\n16. X. Ju et al, Graph neural networks for particle reconstruction in high\nenergy physics detectors. in 33rd Annual Conference on Neural\nInformation Processing Systems, vol. 3 (2020)\n17. G. Kasieczka, T. Plehn, M. Russell, T. Schell, Deep-learning top\ntaggers or the end of QCD? JHEP 05, 006 (2017)\n18. P.T. Komiske, E.M. Metodiev, J. Thaler, Energy ﬂow networks:\ndeep sets for particle jets. JHEP 01, 121 (2019)\n19. A.J. Larkoski, I. Moult, B. Nachman, Jet substructure at the large\nhadron collider: a review of recent advances in theory and machine\nlearning. Phys. Rep. 841, 1–63 (2020)\n20. J. Li, T. Li, F.-Z. Xu, Reconstructing boosted Higgs jets from event\nimage segmentation. JHEP 04, 156 (2021)\n21. J. Lin, M. Freytsis, I. Moult, B. Nachman, Boosting H → b ¯b with\nmachine learning. JHEP 10, 101 (2018)\n22. I. Loshchilov, F. Hutter, Decoupled weight decay regularization\nin International Conference on Learning Representations . (2019).\nhttps://openreview.net/forum?id=Bkg6RiCqY7\n23. G. Louppe, K. Cho, C. Becot, K. Cranmer, QCD-aware recursive\nneural networks for jet physics. JHEP 01, 057 (2019)\n123\n1116 Page 10 of 10 Eur. Phys. J. C (2023) 83 :1116\n24. F. Ma, F. Liu, W. Li, A jet tagging algorithm of graph net-\nwork with Haar pooling message passing. Phys. Rev. D 108(7),\n072007 (2023). https://doi.org/10.1103/PhysRevD.108.072007.\narXiv:2210.13869\n25. S. Macaluso, D. Shih, Pulling out all the tops with computer vision\nand deep learning. JHEP 10, 121 (2018)\n26. V . Mikuni, F. Canelli, ABCNet: an attention-based method for par-\nticle tagging. Eur. Phys. J. Plus 135(6), 463 (2020)\n27. V . Mikuni, F. Canelli, Point cloud transformers applied to collider\nphysics. Mach. Learn. Sci. Tech. 2(3), 035027 (2021)\n28. E.A. Moreno, O. Cerri, J.M. Duarte, H.B. Newman, T.Q. Nguyen,\nA. Periwal, M. Pierini, A. Serikova, M. Spiropulu, J.-R. Vlimant,\nJEDI-net: a jet identiﬁcation algorithm based on interaction net-\nworks. Eur. Phys. J. C 80(1), 58 (2020)\n29. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT .K i l l e e n ,Z .L i n ,N .G i m e l s h e i n ,L .A n t i g a ,A .D e s m a i s o n ,A .\nKopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,\nB. Steiner, L. Fang, J. Bai, S. Chintala, Pytorch: an imperative\nstyle, high-performance deep learning library. inAdvances in Neu-\nral Information Processing Systems ,v o l .3 2 ,e d .b yH .W a l l a c h ,H .\nL a r o c h e l l e ,A .B e y g e l z i m e r ,F . D .A l c h e - B u c ,E .F o x ,R .G a r n e t t\n(Curran Associates Inc, Red Hook, 2019), pp. 8024–8035\n30. H. Qu, L. Gouskos, ParticleNet: jet tagging via particle clouds.\nP h y s .R e v .D101(5), 056019 (2020)\n31. H. Qu, C. Li, S. Qian, Particle transformer for jet tagging. in\nInternational Conference on Machine Learning (2022), pp. 18281–\n18292\n32. J. Ren, D. Wang, L. Wu, J.M. Yang, M. Zhang, Detecting an axion-\nlike particle with machine learning at the LHC. JHEP 11, 138\n(2021)\n33. J. Shlomi, P. Battaglia, J.-R. Vlimant, Graph neural networks in\nparticle physics. Mach. Learn. Sci Tech. 2(2), 021001\n34. T. Sjöstrand, S. Ask, J.R. Christiansen, R. Corke, N. Desai, P. Ilten,\nS. Mrenna, S. Prestel, C.O. Rasmussen, P.Z. Skands, An intro-\nduction to PYTHIA 8.2. Comput. Phys. Commun. 191, 159–177\n(2015)\n35. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N.\nGomez, Ł. Kaiser, I. Polosukhin, Attention is all you need. Adv.\nNeural Inform. Proc. Syst. 30 (2017)\n36. Y . Wang, Y . Sun, Z. Liu, S.E. Sarma, M.M. Bronstein, J.M.\nSolomon, Dynamic graph CNN for learning on point clouds.CoRR.\n(2018). arXiv:abs/1801.07829\n37. Y . Wang, Y . Sun, Z. Liu, S.E. Sarma, M.M. Bronstein, J.M.\nSolomon, Dynamic graph CNN for learning on point clouds. ACM\nTrans. Graph. (TOG) 38(5), 1–12 (2019)\n123",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6811788082122803
    },
    {
      "name": "Pairwise comparison",
      "score": 0.5606482625007629
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5596050024032593
    },
    {
      "name": "Computer science",
      "score": 0.5119670629501343
    },
    {
      "name": "Quark",
      "score": 0.5002434253692627
    },
    {
      "name": "Gluon",
      "score": 0.4863720238208771
    },
    {
      "name": "Particle physics",
      "score": 0.4739435017108917
    },
    {
      "name": "Architecture",
      "score": 0.47268038988113403
    },
    {
      "name": "Artificial intelligence",
      "score": 0.460570365190506
    },
    {
      "name": "Jet (fluid)",
      "score": 0.4145655035972595
    },
    {
      "name": "Physics",
      "score": 0.37503230571746826
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I24062138",
      "name": "Konkuk University",
      "country": "KR"
    }
  ]
}