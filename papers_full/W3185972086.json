{
  "title": "Enhancing argumentation component classification using contextual language model",
  "url": "https://openalex.org/W3185972086",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5054726242",
      "name": "Hidayaturrahman Hidayaturrahman",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A5048794693",
      "name": "Emmanuel Dave",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A5080650089",
      "name": "Derwin Suhartono",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A5039645915",
      "name": "Aniati Murni Arymurthy",
      "affiliations": [
        "University of Indonesia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3178696750",
    "https://openalex.org/W1523838641",
    "https://openalex.org/W2409484708",
    "https://openalex.org/W2766738867",
    "https://openalex.org/W2343649478",
    "https://openalex.org/W2963591087",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2760996147",
    "https://openalex.org/W4205797662",
    "https://openalex.org/W3099640445",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W1925537772"
  ],
  "abstract": null,
  "full_text": "Enhancing argumentation component \nclassification using contextual language model\nHidayaturrahman1* , Emmanuel Dave1, Derwin Suhartono1 and Aniati Murni Arymurthy2 \nIntroduction\nArguing is a daily activity carried out both in speaking and thinking. Arguments are \nmade to increase or decrease the level of acceptance of a controversial idea [1]. They \nemphasize one’s opinions as well by showing their point of view on certain discussions. \nGood arguments can support the decision-making process and reach a conclusion that \ncan be accepted by many people. Arguments can appear in everyday conversation or \nin written form. In order to strengthen the validity of an argument, several premises or \nsupporting sentences in form of facts must support the argument. In certain literatures, \nthey can be called as claims and premises [2], fact, value [3], policy [4] and many other \nproposed argument schemes.\nAbstract \nArguments facilitate humans to deliver their ideas. The outcome of the discussion \nheavily relies on the validity of the argument. If an argument is well-composed, it is \nmore effective to grasp the core idea behind the argument. To grade the argument, \nmachines can be utilized by decomposing into semantic label components. In natural \nlanguage processing, multiple language models are available to perform this task. It is \ndivided into context-free and contextual models. The majority of previous studies used \nhand-crafted features to perform argument component classification, while state of \nthe art language models utilize machine learning. The majority of these language mod-\nels ignore the context in an argument. This research paper aims to analyze whether by \nincluding the context in the classification process may improve the accuracy of the lan-\nguage model which will enhance the argumentation mining process as well. The same \ndocument corpus is fed into several language models. Word2Vec and GLoVe represent \nthe context free models, while BERT and ELMo as context sensitive language models. \nAccuracy and time from each model are then compared to determine the importance \nof context. The result shows that contextual language models are proven to be able \nto boost classification accuracy by approximately 20%. However, time comes as a cost \nwhere contextual models require longer training and prediction time. The benefit \nfrom the increase in accuracy outweighs the burden of time. Thus, as a contextual task, \nargumentation mining is suggested to use contextual model where context must be \nincluded to achieve promising results.\nKeywords: Argumentation mining, Language model, Context free model, Contextual \nmodel, BERT, ELMo, Word2Vec, GloVe\nOpen Access\n© The Author(s), 2021. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/.\nRESEARCH\nHidayaturrahman et al. J Big Data           (2021) 8:103  \nhttps://doi.org/10.1186/s40537-021-00490-2\n*Correspondence:   \nhidayaturrahman@binus.ac.id \n1 Computer Science \nDepartment, School \nof Computer Science, \nBina Nusantara University, \nJakarta 11480, Indonesia\nFull list of author information \nis available at the end of the \narticle\nPage 2 of 17Hidayaturrahman et al. J Big Data           (2021) 8:103 \nBy understanding the structure of an argument, it can help us improve the quality of \nan argument even to the point of being able to test the validity of the argument itself. \nThis process is called as argumentation mining. It aims to provide machines with the \ncapability to decompose a text and retrieve its main argument and point of view by \nperforming sentiment analysis. With the help of machines, students and authors can \nbe assisted in real time to structure their writing that can strengthen their argumen -\ntation position in their texts. There are several steps that must be taken to arrive at \na more practical job and to be able to provide direct insight into an argument. Cur -\nrently, several techniques have been developed to carry out the argumentation min -\ning process.\nAccording to previous research, there are several examples of tasks performed in \nargumentation mining. Among them are identifying arguments that do not have suf -\nficient support [5 ], classifying argument component [6 ], identifying whether an argu -\nment is present or not [7 ], identifying argumentative relations [8 ], making argument \ngraph [9], and automatic annotator for text containing argument [10].\nThe majority of argument mining research starts with an argument component clas -\nsification. Currently, several approaches are available to perform the classification \nprocess which depends on the goal of the task. In general, an argument can be divided \ninto 2 main components, namely claim and premise.\nIn the argumentation mining process, there are several mandatory steps. Initially, \nsentences must be categorized into claim and non-claim. Afterwards, sentences in the \nclaim category are further classified into the multiple types of claim (11). Another \napproach would be by classifying sentences directly into claim, premise and non-\nargumentative (12). Other research grouped sentences into claim and premise types \n(13). Lastly, sentences can also be classified into major claim, claim, premise, and \nnon-argumentative categories (14).\nIn terms of the techniques used to perform argument component classification, two \napproaches are commonly applied, handcrafted features and deep learning. For this \nresearch, the deep learning approach is taken as it is proven in the literature that it \nachieves better results compared to the machine learning approach as it has the abil -\nity to learn high-level features from the input data. For techniques that utilize deep \nlearning, two types language models can be used for feature extraction process which \nare context-free models, such as the Global Vector (GloVe) [11], Word2Vec [12] and \nFastText [13] and contextual models such as the Bidirectional Encoder Representa -\ntions from Transformers (BERT) [14] and the Embeddings from Language Models \n(ELMo) [15].\nIn argumentation mining, context has an important role to provide additional infor -\nmation, especially to classify argumentation components. This paper will show how a \ncontext-sensitive language model can outperform a context-free language model. This \npaper will also show how a context-sensitive language model trained in a bidirectional \nmanner (e.g. BERT) performs better than a shallow bidirectional model (e.g. ELMo). \nIn order to make the comparison is apple to apple, the experiment is only make use of \nlanguage model itself then classify the word vector directly using perceptron. Since the \nmodel is the only independent variable, time cost will be considered to measure the per -\nformance of language models.\nPage 3 of 17\nHidayaturrahman et al. J Big Data           (2021) 8:103 \n \nRelated work\nArgumentation mining\nArgumentation is a type of discourse where speakers try to persuade their audience \nabout the reasonableness of a claim by presenting supportive arguments. Argumenta -\ntion mining usually aims to identify argument components contained in the text and \nthen predict the relations between them.\nArgumentation mining has been performed using data in the form of an abstract Ran -\ndomized Controlled Trial (RCT) from the MEDLINE database which is self-annotated. \nThe dataset used in this study has 4,198 argument components and 2,601 argument \nrelations retrieved from several types of diseases (i.e., neoplasm, glaucoma, hepatitis, \ndiabetes, hypertension). In this study, a pipeline is proposed in which argument com -\nponents are classified into evidence or claim, then predicting the relationship as support \nor attack. Experiments were performed using bidirectional transformers in combination \nwith various neural architectures, such as the Long Short Term Memory (LSTM), the \nGated Recurrent Units (GRU), and the Conditional Random Field (CRF). The result of \nthis experiment show an F1-score of 0.87 for component detection and 0.68 for relation \nprediction [16].\nAnother approach is proposed to analyse the support relation between two-argument \nsentences. In conducting this research, the presence or absence of support is empha -\nsized on the similarity of the nuances in the two sentences. Experiments were carried \nout using a siamese network whose extractor feature was replaced with Long Short \nTerm Memory and using the cosine distance as an energy function. This model accepts \ninput in the form of a pair of sentences then tries to predict whether there is a support \nrelation or not. The main objective of this research is to prove that the level of similarity \nbetween two sentences is related to the support relationship between sentences. In the \nexperiment, an attention mechanism was also implemented. The accuracy value of this \nexperiment is 67.33% [17].\nThe trend in classifying argument components is polarized into the use of statistical \nmodels with heavy feature-engineering or the use of direct deep neural-networks with -\nout considering prior knowledge or another secondary feature. Therefore, this research \nproposes to add lightweight features to deep models to classify argument components. \nExperiments were carried out by comparing the performance of the proposed model \nwith previous studies. It managed to outperform the others by achieving a macro \nF1-Score of 0.805. Based on these findings, it is concluded that adding prior knowledge, \ncan help improve the performance of the model [7].\nArgumentation mining is used to enhance a lot of activity such as peer review assis -\ntance in research paper [18], mining argumentation on tweets in twitter [19] and other \nsocial media [20]. Multilingual argumentation mining has also been done in a research \nthat is conducted by performing transfer learning and utilizing machine translation [21].\nLanguage model\nSince the deep learning trend has increased, many language models have been \nbuilt using a deep learning approach in order to achieve a more promising result. \nFrom the perspective of the representation of a word in the language model, there \nPage 4 of 17Hidayaturrahman et al. J Big Data           (2021) 8:103 \nis a context-free model and a contextual model. In the context-free model the word \n\"bank\" in \"deposit bank\" and \"river bank\" have the same value. On the other hand, a \ncontextual model will assign different values to the word “bank” in this scenario as it \nhas different meanings. Examples of context-free models are Word2Vec [12], GloVe \n[11] and FastText [13]. Meanwhile, examples of contextual models are BERT [14] and \nELMo [15].\nWord2Vec is a language model that is trained using the skip-gram approach. The \nimprovement made in this training model is by subsampling frequent words to make \nthe training process faster and also learn more regular word representations. This \ntechnique is later known as negative sampling.\nThe GloVe is a language model that is created to produce a more explainable model. \nDuring the modeling process, its properties are given explicitly, such as regularities \nto emerge in word vectors. The resulting model is a global log-bilinear regression that \ncombines a global matrix factorization and a local context window. This model can \nproduce vector space with a meaningful substructure.\nAs for FastText, it is actually extended version of Word2Vec. The different is Fast -\nText optimize the use of subword to do skipgram training. It make FastText is more \nsensitive to rare world. In their research, it found that the time needed to do the tran -\ning is 1.5 × slower than the skipgram baseline.\nUnlike recent language representation models, BERT is designed to pre-train deep \nbidirectional representations from an unlabeled text by jointly conditioning on both \nleft and right context in all layers. BERT does not simply concatenate left-to-right and \nright-to-left contexts like in ELMo model. Concatenation is a weakness as it is unable to \nsimultaneously consider left and right contexts. The transformer encoder in each layer \nreads the whole sequence of inputs all at once. This way, transformers can learn the con-\ntext of a specific word based on its neighboring words (both left and right). As a result, \nthe pre-trained BERT model can be fine-tuned with just one additional output layer to \ncreate state-of-the-art model for a wide range of tasks, such as question answering and \nlanguage inference, without substantial task-specific architecture modifications. At the \ntime BERT was created, it was reported that this model obtained new state-of-the-art \nresults on eleven natural language processing tasks, including pushing the General Lan -\nguage Understanding Evaluation (GLUE) score to 80.5% (7.7% point absolute improve -\nment), the Multi-Genre Natural Language Inference (MultiNLI) accuracy to 86.7% (4.6% \nabsolute improvement), the Stanford Question Answering Dataset (SQuAD) v1.1 ques -\ntion answering Test F1 to 93.2% (1.5% points absolute improvement) and SQuAD v2.0 \nTest F1 to 83.1% (5.1% points absolute improvement).\nSeveral preprocessing methods are required to fit data into the BERT model. Data \nshould be initially tokenized using Bert Tokenizer as the model has its own fixed vocabu-\nlary. In addition, data should be annotated using special tokens recognized by BERT. For \ninstance, [CLS] token symbolizes the start of a sentence and [SEP] token separates one \nsentence from another. In addition, the [CLS] token signals a classification layer in order \nto perform classification tasks. Even though the input is a single sentence, [SEP] token is \nstill required to be added before fitting inputs into the model. Another important token \nis [PAD]. This token is represented with value 0. Considering that BERT is an absolute \npositional embedding, padding tokens are preferably placed on the right of the inputs.\nPage 5 of 17\nHidayaturrahman et al. J Big Data           (2021) 8:103 \n \nAn additional sequence of input is needed to annotate which tokens that the BERT \nmodel should pay attention to. This way the model can differentiate positions of a pad -\nded token with a tokenized input. In BERT, attention mask with value of 1 represents \nvalues that it should pay attention to and 0 represents a padded value.\nDistilBERT is a smaller and lighter version of BERT. It is designed to be more effective \nin terms of speed as it is trained for a general-purpose language model. The main objec -\ntive is to cut cost to make it more reliable for production. Other BERT models are too \nlarge for production, especially when there are certain constraints to be followed, such \nas latency and servers. This model mimics the BERT model with the addition of sev -\neral changes. Starting from cutting token-type embeddings and the pooler. This model \nis also trained using the same corpus used for training the BERT model (English Wiki -\npedia and Toronto Book Corpus). The technique used that can make DistilBERT much \nlighter compared to BERT model is distillation, a compression technique that helps a \nchild model (DistilBERT) to be trained and show similar behavior with the parent model \n(BERT) [22] [23]\nELMo is also a contextual model. The word representation in ELMo is a combination \nof each existing layer to produce a deep representation. Also, the word representation \nin this model is character-based. If there is a word that is not densely populated in the \ncorpus, the similarity will be sought. This model is trained using a bidirectional language \nmodel.\nMethodology\nData\nArgumentation dataset was gathered from an online forum and classified into 4 classes, \nnamely: Major Claim, Claim, Premise and None. Data were gathered from a total of 90 \nessays. The distribution of data is represented in Fig.  1. The dataset is dominated by the \nPremise class with 3,832 statements, followed by the None class which contains state -\nments that cannot be classified to neither Major Claim, Claim nor Premise with 1,610 \nstatements. Similarly to the None class, the claim class contains 1,506 statements. The \nclass with the smallest data is Major Claim with only 751 sentences. [6]\nFig. 1 Dataset distribution\nPage 6 of 17Hidayaturrahman et al. J Big Data           (2021) 8:103 \nAfter dividing data into each class, data was randomly separated for training and test -\ning by using a random seed. Random seeding is crucial to ensure that the training and \ntesting dataset are the same for all models. The ratio between the 2 classes is 80% train -\ning and 20% testing.\nPreprocessing\nContextual language model\nBefore fitting data into the model, raw text data were encoded based on the correspond -\ning tokenizer of each model. The example code below shows the encoding process for \nthe BERT base cased model by using a BERT tokenizer. BERT tokenizer is required as it \nhas its own fixed vocabulary.\nFrom our data, the maximum statement length is 76, thus statements were padded to a \nsingle constant length of 80. Padding tokens are set to value of 0 as BERT will automati -\ncally treat it as a padding token.\nFinally, attention masks were added to differentiate tokens with padding tokens.\nFor comparison purposes, the dataset was tested using multiple BERT models \n(i.e. BERT Base Cased with 80 sequences, BERT Base Cased with 256 sequences, and \nBERT Base Uncased with 80 sequences). On BERT base cased, BERT model is trained \nusing cased word data. It would be compared with the same BERT model but with a \nlonger sequence length of 256. The difference between BERT Base Cased and BERT \nBase Uncased is the BERT base uncased is trained using uncased words. To compare \nthese BERT models, the training time and final accuracy result from each model were \ncompared.\nPage 7 of 17\nHidayaturrahman et al. J Big Data           (2021) 8:103 \n \nFinally, data were converted into PyTorch data types and fitted into a torch data \nloader for iterative sequence insertion so that the entire dataset is not required to be \nloaded into memory for training. Figure  2 represents the training pipeline for BERT \nmodels.\nIn this research, the ELMo embedding is added as a lambda layer in the deep neu -\nral network architecture as shown in Fig.  3. Therefore, the tokenization and embedding \nprocess are executed inside the training pipeline. As soon as the neural network accepts \nan input text, the input text is directly tokenized and embedded by the ELMo language \nmodel. Prior to passing the input text to the neural network model for training, the data-\nset is initially preprocessed to convert label values into categorical values.\nContext free language model\nSimilar with another language model, context free language model has its own encoding \nfor each token in its dictionary. So before doing the modeling process, all sentences in \ndatasets are converted into tokens using its own dictionary.\nFor the FastText model, labels must be configured to meet the required format as \nshown below. It requires adding a “__label__” prefix before passing it into the supervised \nFig. 2 BERT training pipeline\nPage 8 of 17Hidayaturrahman et al. J Big Data           (2021) 8:103 \ntraining. This preprocessing is needed as the model will treat the label as the subword \nmarker.\nModeling\nFor comparison purposes, all models were trained using 4 epochs and batch size of 32. \nAll models were trained and tested in a Google Colab notebook with a Tesla T4 GPU.\nFig. 3 ELMo training pipeline\nTable 1 BERT architectures\nBERT model Encoders Attention heads Hidden layers\nBERT Base 12 12 768\nDistilBERT Base 6 12 768\nPage 9 of 17\nHidayaturrahman et al. J Big Data           (2021) 8:103 \n \nContext‑free\nIn this research, Word2Vec, GloVe and FastText are used as Context-Free Language \nModels. In context-free language models, each word has its own fixed embedding. Since \nthese models share the same behavior, the modelling process is similar.\nWord2Vec and GloVe models experienced similar preprocessing pipeline. First, the \ntext that have been padded with length of 80 is transformed into their token. Then, these \ntokens are fed into the language models. In this modelling, a 100 dimension language \nmodel is used. This means that for each word, it will be translated into 100 array. After \nthat, these sentences vector will be flattened by concatenating all of the word array. \nThese vectors will go through 768 Dense layers before being classified into 4 different \nclasses.\nFastText model underwent a different pipeline as the model is already well encapsu -\nlated and further data preprocessing occurred during training. After labels are added by \nthe “__label__” prefix, it is directly passed onto training.\nContextual\nThe learning rate used is 2e-5 and the epsilon is 1e-8.\nThere are Two types of BERT model, DistilBERT and BERT were tested and compared. \nDetails regarding the architecture of these models are shown in Table  1. The differences \nbetween cased and uncased models are not shown in the table as there is only a small \ndifference in the number of parameters, while the other attributes are the same. Uncased \nBERT models are trained on text that are initially lower cased before it is passed into \ntraining, while cased BERT models are trained on case sensitive text. In other words, \ncased BERT models process input text as it is. Figure  4 shows how cased BERT models \nand uncased BERT models process the same input text differently.\nBoth BERT and DistilBERT follow the same training pipeline as DistilBERT is derived \nfrom the BERT model.\nThe training pipeline used for ELMo text classification is shown in Fig.  3. The train -\ning pipeline is similar to the context-free training pipeline shown in Fig.  5. However, the \ndifference is that this model can take in sequences of various lengths. Therefore, it is \nunnecessary to pad to a fixed length of 80. The embedding is inserted as a lambda layer \nin the model architecture, followed by a fully connected layer to complete the neural \nnetwork model.\nFig. 4 Cased BERT vs uncased BERT\nPage 10 of 17Hidayaturrahman et al. J Big Data           (2021) 8:103 \nResults and discussion\nModel performance\nThe difference between using a cased and uncased model does not produce a signifi -\ncant difference on the result. For this specific task, letter casing can be ignored as it \ndoes not affect the accuracy of the model. It may be important to pick either cased or \nuncased model when working with other languages, where cased and uncased letters \nshould be highlighted as it may have different context/meaning.\nFor context-free models, GloVe model and Word2Vec model share similar perfor -\nmance in general regarding to argumentation classification task. However, for clas -\nsifying Claim text, Word2Vec perform better than GloVe. In contrast, GloVe model is \nbetter on evaluating Major Claim class. For both Premise and None class, these two \nmodels performance is quite similar. The difference advantage of these two models is \nreally affected by the corpus that used to train these models. Also Word2Vec model \nhas disadvantages handling Out-of-Vocabulary (OOV) words while GloVe can handle \nit.On the other hand, the FastText model is the worst performing context-free model. \nHowever, it achieved best performance in the largest class dataset which is the Prem -\nise class.\nFig. 5 Context free training pipeline\nPage 11 of 17\nHidayaturrahman et al. J Big Data           (2021) 8:103 \n \nTo measure the performance of each model, the F1 metric is used because it can \nfairly judge the performance of each model considering that there is an uneven class \ndistribution in the dataset. In addition, the performance metric is also unbiased \ntowards either false positives or false negatives as it is a weighted average of precision \nand recall. Based on Fig.  6, the accuracy of contextual models is significantly higher \ncompared to context-free models.\nThe least accurate fine-tuned contextual model is ELMo. This model achieves an \naverage F1 score of 0.62. However, this model still outperforms the best performing \ncontext free model that achieved an average F1 score of 0.38. Details of how each \nmodel performed in each class is shown in Table  2. Hence, it shows that context is \ncritical for text classification as contextual models outperform context-free models. \nSince time is equally important to the performance of each model, the prediction \ntime and training time of each model are compared and shown in Table  3. In order to \nkeep this comparison fair, the training and testing dataset are kept constants for all \n00 .1 0. 20 .3 0. 40 .5 0. 60 .7 0. 8\nBe rt  B as e Case d (Seq. 80 )\nBe rt  B as e Case d (Seq. 2 56)\nBe rt  B as e Un case d (Seq . 80 )\nBe rt  B as e Case d wi thout Fin e Tu ni ng\nEl mo\nDi s/g415 lb ert-base-cas ed  (Seq.  80)\nDi s/g415 lb ert-base-unc as ed ( Se q.  80)\nWo rd 2Vec\nGl oV e\nFas tT ex t\nAverage F1 Scor e\nPe rf ormanc e\nContext-Fre e Contextual\nFig. 6 Performance results\nTable 2 F1 score\nBold emphasis data highlight the best performing model for each column\nLanguage Model Class Average\nMajor Claim Claim Premise None\nBert Base Cased (Seq. 80) 0.54 0.50 0.80 1.00 0.71\nBert Base Cased (Seq. 256) 0.47 0.47 0.82 1.00 0.69\nBert Base Uncased (Seq. 80) 0.53 0.49 0.82 1.00 0.71\nBert Base Cased without Fine Tuning 0.00 0.31 0.00 0.00 0.08\nElmo 0.47 0.23 0.79 0.98 0.62\nDistilbert-base-cased (Seq. 80) 0.44 0.44 0.79 1.00 0.67\nDistilbert-base-uncased (Seq. 80) 0.44 0.43 0.80 1.00 0.67\nWord2Vec 0.10 0.31 0.66 0.44 0.38\nGloVe 0.21 0.21 0.67 0.43 0.38\nFastText 0.10 0.00 0.72 0.44 0.32\nPage 12 of 17Hidayaturrahman et al. J Big Data           (2021) 8:103 \nmodels by using a random seed. In addition, the environment that the training and \ntesting took place is also the same for all models. The purpose of tracking the time is \nto compare the speed between models on the same device and dataset and to deter -\nmine whether there is a tradeoff between time and performance. It is to answer the \nquestion “Does high performing models require high training time?” .\nElmo model takes the longest training time with 1 and a half hour training time from \nall models as shown in Fig.  7. This figure also proves that transformer-based contextual \nmodels succeed to achieve state-of-the-art results with minimum time.\nThe shortest processing time for the contextual model is in DistilBERT models taking \nonly 3 min training time and 2 s prediction time. This shows that DistilBERT is lighter \nand faster compared to BERT models that approximately needs 5 min training time and \n4 s prediction time. The DistilBERT model takes only half of the prediction time taken \nin the BERT model. Compared to the context free model that took the longest time, the \nDistilBERT model still takes 30 times more training time and approximately 13 times \nTable 3 Modelling time\nTime Training time Prediction time\nBert Base Cased (Seq. 80) 0:06:12 0:00:04\nBert Base Cased (Seq. 256) 0:17:50 0:00:08\nBert Base Uncased (Seq. 80) 0:05:47 0:00:04\nBert Base Cased without Fine tuning 0:00:00 0:00:03\nDistilbert Base Cased (Seq. 80) 0:03:00 0:00:02\nDistilbert Base Uncased (Seq. 80) 0:03:03 0:00:02\nElmo 1:33:28 0:04:55\nWord2Vec 0:00:03.95 0:00:00.11\nGlove 0:00:05.82 0:00:00.16\nFastText 0:00:00.01 0:00:00.01\n00:00:00\n00:00:43\n00:01:26\n00:02:10\n00:02:53\n00:03:36\n00:04:19\n00:05:02\n00:05:46\n00:00:00\n00:14:24\n00:28:48\n00:43:12\n00:57:36\n01:12:00\n01:26:24\n01:40:48\nPredic/g415on Time\nTraining Time\nTime Performance\nTraining /g415me Predic/g415on /g415me\nFig. 7 Time results\nPage 13 of 17\nHidayaturrahman et al. J Big Data           (2021) 8:103 \n \nmore prediction time. This proves that even though DistilBERT is able to outrun BERT \nmodel by half the time, it is still much longer compared to context free models.\nIn contrast with the model’s accuracy, changes in sequence length affects the process -\ning time quite significantly. The same BERT model with sequence length of 256 needs \nan additional 11  min and 38  s training time compared to a sequence length of 80. In \naddition, prediction time in the 256-sequence model is almost twice of the 80-sequence \nmodel.\nCased and Uncased BERT model does not affect the processing time quite signifi -\ncantly. For this task, the cased BERT model needs an additional 25 s compared to the \nuncased model. This is because more variations of embeddings are present in the cased \nmodel, as same words with the same context may have different word embeddings when \nthere are differences in letter case.\nFor context free models, the time taken for training and prediction varies between \nmodels. In general, they all took less than 6 s training time and less than a second predic-\ntion time. The longest training time is with the GloVe model taking almost 6 s training \ntime. On the other hand, the FastText model is the fastest context free model. It only \ntook less than a second training time and prediction time. It probably happened because \nGloVe model deals with subwords.\nContext free model takes significantly lesser time to train compared to contextual \nmodel. This is proven by the fastest contextual model is trained for 3 min, while the long-\nest context free model is only trained for approximately 6 s. This is because a contextual \nmodel has a far larger vocabulary size compared to a context free model, as same words \nwith different context will have different embeddings. As mentioned before, the word \n“bank” in a context free model only has 1 token, while it can potentially have multiple \ndifferent tokens in a contextual model depending on the context of the word “bank” used \nin the statement. Hence, contextual models will have a far larger embed/token input size \n00:00:00\n00:00:01\n00:00:02\n00:00:03\n00:00:03\n00:00:04\n00:00:05\n00:00:06\n00:00:07\n00:00:08\n00:00:09\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPredic/g415on Time\nAverage F1-Score\nBERT Based Models\nBERT Based Performance -\nPredic/g415on Time\nTraining Time Predic/g415on Time\nFig. 8 BERT based performance—prediction time\nPage 14 of 17Hidayaturrahman et al. J Big Data           (2021) 8:103 \nwhich is why the time taken for contextual models is much longer compared to context \nfree models.\nPrediction time is more important to be taken into consideration for performance \nanalysis compared to training time. Because training time can be handled using a high \ncomputing system, while the prediction process can take place in a variety of devices \nwhen deployed for commercial use. Figure  8 highlights the comparison of performance \nand prediction time for the BERT-based models. It shows that DistilBERT models are \nmore efficient compared to BERT based models, as it achieves similar F1-Score but with \nlesser time.\nError analysis\nFor analysis, the BERT based cased model with sequence length of 80 is selected as it is \nthe best performing model.\nAs shown in Table  4 a premise statement is falsely classified as a Major Claim. To \nget deeper insights towards the behavior of the model, frequencies of words appeared \nTable 4 Premise sentence prediction\nSentence True label Predicted label\nThere are several reasons why animal testing should be banned Claim Claim\nSentence True label Predicted label\nFrom the health point of view, schools should not only deliver academic subjects Premise Major claim\na Predicted None Class \nWord Appearances\nb Predicted Premise Class \nWord Appearances\nc Predicted Claim Class \nWord Appearances\nd Predicted Major Claim \nClass Word Appearances\nFig. 9 Claim sentence prediction\nPage 15 of 17\nHidayaturrahman et al. J Big Data           (2021) 8:103 \n \nin each predicted class were calculated. Figure  9a–d shows the visualization of word \nappearances.\nInterestingly, a majority of 46% of statements that contain the word “should” in the \ntesting dataset is predicted as a Major Claim. This is because the original dataset that \nis classified as a Major Claim has the word “should” as the 8th most frequent word in \nthe dataset, while the words that appear more than “should” are articles and conjunc -\ntions like “the” , “a” , “and” , “of” and more that have little meaning towards the class of the \nstatement as shown in Fig. 9d. This can potentially show that the word “should” is biased \ntowards the Major Claim class, hence it can be the reason why Table 4 exists.\nThe sentence in Fig. 9 contains the word “should” , but the model can accurately classify \nthe sentence as a Claim instead of a Major Claim. The BERT model is able to identify the \ncontext of the word “should” used in the statement. Even though this statement uses the \nword “should” , but it does not emphasize any arguments that a major claim should has. \nThe BERT model successfully identifies this problem.\nConclusion\nIn terms of classifying argument component, contextual language models outperform \ncontext-free models. This is shown by contextual models achieving a minimum F1 of \n0.69, while context free models achieve an average F1 of 0.38 each. However, context free \nmodels are significantly faster than contextual models as all context free models can pre-\ndict in less than a second. The advantage of the performance still far outweighs the cost. \nContext is proven to have a critical role in an argument. Thus, it is a factor that must be \nincluded in performing context sensitive tasks such as argumentation mining with the \nhelp of contextual language models.\nContext is proven to be able to boost classification accuracy by approximately 20%. \nUnfortunately, the highest F1 score obtained in this research is only 71%. This shows \nthat there is still a lot of room for improvement. As shown in Table  4, some contex -\ntual errors still exist. For future work, it is hypothesized that instead of considering only \ncontextual individual words to classify the semantic label of the statement, contextual \nphrases can be considered which may have higher contribution towards the semantic \nlabel of the statement. For instance, instead of just using the context of individual words \nlike “believe” , phrases like “we strongly believe that” may have a deeper meaning towards \nthe semantic label of the statement.\nIn this work, the models used for classification is just a vanilla network. Since this \nwork is about enhancing the performance of classification for argument component \nusing contextual model, then vanilla network is apparently enough. For the future works, \nthe similar testing scenario like what mayers had done [16] can be applied by using argu-\nmentation data.\nAbbreviations\nBERT: Bidirectional Encoder from Transformer; CRF: Conditional Random Field; DistilBERT: Distilated BERT; ELMo: Embed-\ndings from Language Models; GloVe: Global Vector; GLUE: General Language Understanding Evaluation; GRU : Gated \nRecurrent Units; LSTM: Long Short Term Memory; MultiNLI: Multi-Genre Natural Language Inference; RCT : Randomized \nControlled Trial; SQuAD: Stanford Question Answering Dataset; Word2Vec: Word to Vector.\nAcknowledgements\nWe would like to thank Universitas Indonesia for Grant namely TADOK (Tugas Akhir Doktor) 2018.\nPage 16 of 17Hidayaturrahman et al. J Big Data           (2021) 8:103 \nAuthors’ contributions\nH contributed as the research principal in this work as well as the technical issues. ED contributed on technical issues. \nDS and AMA advise all process for this work. Regarding the manuscript, H, ED, DS and AMA wrote and revised the manu-\nscript. All authors read and approved the final manuscript.\nAuthors’ information\nHidayaturrahman is faculty member of Bina Nusantara University, Indonesia. He got his Master Degree in computer \nscience from Institut Teknologi Bandung in 2018. His research fields are computer vision, natural language processing, \nand machine learning. Recently he is doing research in neural style stranfer. He also do some research on chatbot and \nagnostic modelling. Priorly, he was a data scientist that built credit scoring system and collection intelligent systems.\nEmmanuel Dave is a third year student at Bina Nusantara University, Indonesia. His first research is forecasting Indonesian \nExports with a hybrid model LSTM-ARIMA. He is passionate to conduct more research especially in the field of Computer \nVision.\nDerwin Suhartono is faculty member of Bina Nusantara University, Indonesia. He got his PhD degree in computer sci-\nence from Universitas Indonesia in 2018. His research fields are natural language processing. Recently, he is continually \ndoing research in argumentation mining and personality recognition. He actively involves in Indonesia Association of \nComputational Linguistics (INACL), a national scientific association in Indonesia. He has his professional memberships in \nACM, INSTICC, and IACT. He also takes role as reviewer in several international conferences and journals.\nAniati Murni Arymurthy is professor in computer science with specialty in computer vision and image processing. She \ngot her MSc from Computer and Information Sciences Department in The Ohio State University (OSU), Columbus, Ohio, \nUSA. She got her PhD from Universitas Indonesia with sandwich program in Pattern Recognition and Image Process-\ning Lab (PRIP Lab), Department of Computer Science, Michigan State University (MSU), East Lansing, Michigan, USA. \nCurrently, she is active as lecturer in Faculty of Computer Science, Universitas Indonesia. Her research interests include \npattern recognition, image processing, and spatial data.\nFunding\nAll of this works is fully supported by Universitas Indonesia research Grant namely TADOK (Tugas Akhir Doktor) 2018.\nAvailability of data and materials\nThe datasets for this study are available on request to the corresponding author.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1 Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta 11480, Indonesia. \n2 Machine Learning and Computer Vision Laboratory, Faculty of Computer Science, Universitas Indonesia, Depok 16424, \nIndonesia. \nReceived: 15 April 2021   Accepted: 28 June 2021\nReferences\n 1. Van Eemeren FH, Grootendorst R, Henkemans FS, Blair JA, Johnson RH, Krabbe ECW, Plantin C, Walton DN, Willard \nCA, et al. Fundamentals of argumentation theory: a handbook of historical backgrounds and contemporary devel-\nopments. Mahwah: Lawrence Erlbaum Associates, Inc; 1996.\n 2. Stab C, Gurevych I. Annotating argument components and relations in persuasive essays. In: The 25th International \nConference on Computational Linguistics, Ireland, Dublin, 2014.\n 3. Hollilan TA, Baaske KT. Arguments and arguing: the products and process of human decision making. Long Grove: \nWaveland Press Inc; 2015.\n 4. Wagemans J. Constructing a periodic table of arguments. In: Proceedings of the 11th international conference of \nthe Ontario Society for the Study of Argumentation (OSSA), Windsor, 2016.\n 5. Suhartono D, Gema AP , Winton S, David T, Fanany MI, Arymurthy AM. Hierarchical attention network with XGBoost \nfor recognizing insufficiently supported argument. In: 11th Multi-disciplinary international workshop on artificial \nintelligence, MIWAI 2017, Gadong, 2017.\n 6. Stab C, Gurevych I. Parsing argumentation structures in persuasive essays Christian stab, Iryna Gurevych. Comput \nLinguist. 2017;43(3):619–59.\n 7. Xue L, Lynch C. Incorporating task-specific features into deep models to classify argument components. EDM 2020.\nPage 17 of 17\nHidayaturrahman et al. J Big Data           (2021) 8:103 \n \n 8. Paul D, Opitz J, Becker M, Kobbe J, Hirst G, Frank A. Argumentative relation classification with background knowl-\nedge. In: Frontiers in artificial intelligence and applications, 2020, pp. 319–30.\n 9. Lenz M, Sahitaj P , Kallenberg S, Coors C, Dumani L, Schenkel R, Bergmann R. Towards an argument mining pipeline \ntransforming texts to argument graphs. ArXiv 2020.\n 10. Eger S, Daxenberger, Gurevych I. Neural end-to-end learning for computational argumentation mining. In: Proceed-\nings of the 55th annual meeting of the association for computational linguistics, Vol 1, Long Papers, Vancouver, \nCanada; 2017.\n 11. Pennington J, Socher R, Manning C. Glove: global vectors for word representation. In: EMNLP , 2014.\n 12. Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. In: ar \nXiv:1301.3781, 2013.\n 13. Joulin A, Grave E, Bojanowski P , Mikolov T. Bag of tricks for efficient text classification; 2016.\n 14. Jacob Devlin KT. BERT: pre-training of deep bidirectional transformers for language understanding. In: Proceedings \nof the 2019 conference of the North American chapter of the association for computational linguistics: human \nlanguage technologies, Vol 1, Long and Short Papers, Minneapolis, Minnesota; 2019.\n 15. Peters M, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L. Deep contextualized word representa-\ntions. In: Proceedings of the 2018 conference of the North American chapter of the association for computational \nlinguistics: human language technologies, Vol 1 (Long Papers), New Orleans, Louisiana; 2018.\n 16. Mayer T, Cabrio E, Villata S. Transformer-based argument mining for healthcare applications. In: 24th European \nconference on artificial intelligence—ECAI, Santiago de Compostela, Spain, 2020.\n 17. Gema AP , Winton S, David T, Suhartono D, Shodiq M, Gazali W, Shodiq M, Gazali W. It takes two to tango: modifica-\ntion of siamese long short term memory network with attention mechanism in recognizing memory network \nwith attention mechanism in recognizing argumentative relations in persuasive essay argumentative relations in \npersuasive. In: 2nd international conference on computer science and computational intelligence, Bali; 2017.\n 18. Fromm M, Faerman E, Berrendorf M, Bhargava S, Qi R, Zhang Y, Dennert L, Selle S, Mao Y, Seidl T. Argument mining \ndriven analysis of peer-reviews. In: https:// arxiv. org/ abs/ 2012. 07743, 2020.\n 19. Schaefer R, Stede M. Annotation and detection of arguments in tweets. In: Proceedings of the 7th workshop on \nargument mining; 2020.\n 20. Bauwelinck N, Lefever E. Annotating topics, stance, argumentativeness and claims in Dutch Social Media comments: \na pilot study. In: Proceedings of the 7th workshop on argument mining; 2020.\n 21. Toledo-Ronen O, Orbach M, Bilu Y, Spector A, Slonim N. Multilingual argument mining: datasets and analysis. In: \nEMNLP , 2020.\n 22. Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. In: \narXiv preprint ar Xiv:1910.01108; 2019.\n 23. Sun C, Qiu X, Xu Y, Huang X. How to fine-tune BERT for text classification. In: China National Conference on Chinese \nComputational Linguistics, 2019, October, p. 194–206.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8474886417388916
    },
    {
      "name": "Argument (complex analysis)",
      "score": 0.7580263614654541
    },
    {
      "name": "Argumentation theory",
      "score": 0.7281773090362549
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6266932487487793
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6080005168914795
    },
    {
      "name": "Natural language processing",
      "score": 0.6072169542312622
    },
    {
      "name": "Language model",
      "score": 0.5560688376426697
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.5507829189300537
    },
    {
      "name": "Context model",
      "score": 0.46832042932510376
    },
    {
      "name": "Process (computing)",
      "score": 0.45656758546829224
    },
    {
      "name": "Machine learning",
      "score": 0.45585310459136963
    },
    {
      "name": "Task (project management)",
      "score": 0.44678229093551636
    },
    {
      "name": "Linguistics",
      "score": 0.17160087823867798
    },
    {
      "name": "Programming language",
      "score": 0.1415417492389679
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I166073570",
      "name": "Binus University",
      "country": "ID"
    },
    {
      "id": "https://openalex.org/I29617571",
      "name": "University of Indonesia",
      "country": "ID"
    }
  ]
}