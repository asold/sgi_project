{
  "title": "Walert: Putting Conversational Information Seeking Knowledge into Action by Building and Evaluating a Large Language Model-Powered Chatbot",
  "url": "https://openalex.org/W4391011343",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Cherumanal, Sachin Pathiyan",
      "affiliations": [
        "RMIT University"
      ]
    },
    {
      "id": "https://openalex.org/A2105452872",
      "name": "Tian Lin",
      "affiliations": [
        "RMIT University"
      ]
    },
    {
      "id": "https://openalex.org/A4299744247",
      "name": "Abushaqra, Futoon M.",
      "affiliations": [
        "RMIT University"
      ]
    },
    {
      "id": "https://openalex.org/A4227006695",
      "name": "de Paula, Angel Felipe Magnoss√£o",
      "affiliations": [
        "Universitat Polit√®cnica de Val√®ncia"
      ]
    },
    {
      "id": "https://openalex.org/A4202120594",
      "name": "Ji, Kaixin",
      "affiliations": [
        "RMIT University"
      ]
    },
    {
      "id": "https://openalex.org/A3195579072",
      "name": "Hettiachchi, Danula",
      "affiliations": [
        "RMIT University"
      ]
    },
    {
      "id": "https://openalex.org/A4227578743",
      "name": "Trippas, Johanne R.",
      "affiliations": [
        "RMIT University"
      ]
    },
    {
      "id": "https://openalex.org/A3112393025",
      "name": "Ali Halil",
      "affiliations": [
        "RMIT University"
      ]
    },
    {
      "id": "https://openalex.org/A3163758984",
      "name": "Scholer Falk",
      "affiliations": [
        "RMIT University"
      ]
    },
    {
      "id": "https://openalex.org/A4227822277",
      "name": "Spina, Damiano",
      "affiliations": [
        "RMIT University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4284671426",
    "https://openalex.org/W3209067643",
    "https://openalex.org/W2069870183",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3180230246",
    "https://openalex.org/W4304142428",
    "https://openalex.org/W3130801419",
    "https://openalex.org/W4385571034",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2783549597",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4379473327",
    "https://openalex.org/W4376653761",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W1596771353"
  ],
  "abstract": "Creating and deploying customized applications is crucial for operational success and enriching user experiences in the rapidly evolving modern business world. A prominent facet of modern user experiences is the integration of chatbots or voice assistants. The rapid evolution of Large Language Models (LLMs) has provided a powerful tool to build conversational applications. We present Walert, a customized LLM-based conversational agent able to answer frequently asked questions about computer science degrees and programs at RMIT University. Our demo aims to showcase how conversational information-seeking researchers can effectively communicate the benefits of using best practices to stakeholders interested in developing and deploying LLM-based chatbots. These practices are well-known in our community but often overlooked by practitioners who may not have access to this knowledge. The methodology and resources used in this demo serve as a bridge to facilitate knowledge transfer from experts, address industry professionals' practical needs, and foster a collaborative environment. The data and code of the demo are available at https://github.com/rmit-ir/walert.",
  "full_text": "Walert: Putting Conversational Search Knowledge into Action by\nBuilding and Evaluating a Large Language Model-Powered\nChatbot\nSachin Pathiyan Cherumanal\nRMIT University\nMelbourne, Australia\ns3874326@student.rmit.edu.au\nLin Tian\nRMIT University\nMelbourne, Australia\nlin.tian2@student.rmit.edu.au\nFutoon M. Abushaqra\nRMIT University\nMelbourne, Australia\nfutoon.abu.shaqra@student.rmit.edu.au\nAngel Felipe Magnoss√£o de\nPaula\nUniversitat Polit√®cnica de Val√®ncia\nValencia, Spain\nadepau@doctor.upv.es\nKaixin Ji\nRMIT University\nMelbourne, Australia\nkaixin.ji@student.rmit.edu.au\nHalil Ali\nRMIT University\nMelbourne, Australia\nhalil.ali@rmit.edu.au\nDanula Hettiachchi\nRMIT University\nMelbourne, Australia\ndanula.hettiachchi@rmit.edu.au\nJohanne R. Trippas\nRMIT University\nMelbourne, Australia\nj.trippas@rmit.edu.au\nFalk Scholer\nRMIT University\nMelbourne, Australia\nfalk.scholer@rmit.edu.au\nDamiano Spina\nRMIT University\nMelbourne, Australia\ndamiano.spina@rmit.edu.au\nABSTRACT\nCreating and deploying customized applications is crucial for oper-\national success and enriching user experiences in the rapidly evolv-\ning modern business world. A prominent facet of modern user expe-\nriences is the integration of chatbots or voice assistants. The rapid\nevolution of Large Language Models (LLMs) has provided a pow-\nerful tool to build conversational applications. We present Walert,\na customized LLM-based conversational agent able to answer fre-\nquently asked questions about computer science degrees and pro-\ngrams at RMIT University. Our demo aims to showcase how conver-\nsational information-seeking researchers can effectively communi-\ncate the benefits of using best practices to stakeholders interested in\ndeveloping and deploying LLM-based chatbots. These practices are\nwell-known in our community but often overlooked by practition-\ners who may not have access to this knowledge. The methodology\nand resources used in this demo serve as a bridge to facilitate knowl-\nedge transfer from experts, address industry professionals‚Äô practical\nneeds, and foster a collaborative environment. The data and code\nof the demo are available at https://github.com/rmit-ir/walert.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCHIIR ‚Äô24, March 10‚Äì14, 2024, Sheffield, United Kingdom\n¬© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0434-5/24/03.\nhttps://doi.org/10.1145/3627508.3638309\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíEvaluation of retrieval results ;\nSearch interfaces; ‚Ä¢ Human-centered computing ‚ÜíNatural lan-\nguage interfaces .\nKEYWORDS\nconversational information seeking, large language models, retrieval-\naugmented generation\nACM Reference Format:\nSachin Pathiyan Cherumanal, Lin Tian, Futoon M. Abushaqra, Angel Fe-\nlipe Magnoss√£o de Paula, Kaixin Ji, Halil Ali, Danula Hettiachchi, Johanne\nR. Trippas, Falk Scholer, and Damiano Spina. 2024. Walert: Putting Con-\nversational Search Knowledge into Action by Building and Evaluating a\nLarge Language Model-Powered Chatbot. In Proceedings of the 2024 ACM\nSIGIR Conference on Human Information Interaction and Retrieval (CHIIR ‚Äô24),\nMarch 10‚Äì14, 2024, Sheffield, United Kingdom. ACM, New York, NY, USA,\n5 pages. https://doi.org/10.1145/3627508.3638309\n1 INTRODUCTION\nConversational agents based on Large Language Models (LLMs)\nsuch as OpenAI‚Äôs ChatGPT1 provide many benefits to stakeholders,\nreducing the cost of various tasks by saving time and resources, es-\npecially for closed-domain questions such as translating responses\ninto different formats, retrieving policy documents, and preparing\nlegal document drafts [5]. However, two major concerns arise in this\n1https://openai.com/\narXiv:2401.07216v1  [cs.IR]  14 Jan 2024\nCHIIR ‚Äô24, March 10‚Äì14, 2024, Sheffield, United Kingdom Pathiyan Cherumanal et al.\nsetting: (i) the risk of giving away sensitive data about the organiza-\ntion [7, 19]; and (ii) the limited access to structured and comprehen-\nsible documentation might impede practitioners (e.g., data scientists\nwithout a strong background on information retrieval) grasp of the\nprinciples, theories, and best practices influencing product quality.\nEfforts have been taken in this direction of bringing together re-\nsearchers and experts in conversational information-seeking with\nindustry practitioners to deal with real-world problems ‚Äì as in the\ncase of Amazon‚Äôs Alexa Prize challenges [1, 14, 16].\nWe aim to combine our diverse research expertise in machine\nlearning, natural language processing, and information retrieval, to\nbring conversational search knowledge into action and address best\npractices for building an LLM-powered chatbot. Our goals are to\nexplore the challenges and approaches for implementing a chatbot,\ncompare various methods, and offer best practices in evaluation that\nstakeholders (not necessarily experts in conversational information\nseeking) can effectively use to assess and improve the quality of\nchatbot products.\nUsing a manually curated Frequently Asked Questions (FAQ)\nguide from RMIT University‚Äôs School of Computing Technologies as\na Knowledge Base (KB), we were able to implement a conversational\nagent, named Walert2, that allows potential future students to get\nanswers to questions related to the computer science programs\noffered at RMIT University.3\nIn this demo, the primary focus was to characterize the chal-\nlenges associated with integrating LLMs into the development pro-\ncess of voice-based conversational information-seeking systems\nbased on an existing KB. The process allowed us to address chal-\nlenges related to (i) handling private/sensitive information by de-\nploying our instance of an open-source LLM; (ii) monitoring the\nproblem of hallucinations (i.e., the introduction of facts that are\nnot true) [9] and generation of inaccurate information by using\na human-in-the-loop approach [6] and the inclusion of out of KB\nquestions in our testbed; and (iii) evaluating the effectiveness of\nintent-based using Natural Language Understanding (NLU) and\nRetrieval-Augmented Generation (RAG), both at component and\nend-to-end levels. Our evaluation highlights shortcomings in recent\nRAG pipeline studies, particularly regarding the lack of ranking\nevaluation.\n2 METHODOLOGY\nTwo different approaches were used to implement the prototype\nof our application: Intent-Based (IB) and RAG. The first one, IB, is\nsuitable for structured and predictable interactions (i.e., pre-defined\nintents), while RAG chatbots retrieve information from a KB and are\nbetter for open domain and more dynamic conversations. Figure 1\npresents the overall framework for both approaches.\n2.1 Data Collection and Testbed\nWe utilized a manually curated FAQ from RMIT University‚Äôs School\nof Computing Technologies as a KB. The FAQ contains a wide range\nof common questions that incoming students ask regarding course\n2The term ‚ÄúWalert‚Äù means ‚Äúpossum‚Äù in the native languages of the Woi Wurrung and\nBoon Wurrung peoples. Possum skin cloaks are essential to the Traditional Owners\nand Custodians of the land where the authors live and work. Our chatbot, Walert, is\nnamed as a tribute to this cultural heritage [4, 15].\n3https://www.rmit.edu.au/partner/hubs/race/news\nLarge\nLanguage\nModel (Falcon)\nPassages\nQuestions\nHuman in\nthe Loop\nConversational\nAgent\nTraining\nParaphrases\nAmazon\nEcho\nIntent-Based (IB)\nRetrieval-Augmented \nGeneration (RAG)\nGenerated\nAnswer\nIndex Retrieved\nPassages\nLarge\nLanguage\nModel (Falcon)\nRetrieval\nModel\n1\n2\n3\nUser's Question\nFigure 1: Overall architecture of the two approaches imple-\nmented in Walert: IB and RAG.\nofferings and academic programs related to computer science. Us-\ning the question-answer pairs in the FAQ, we generate a set of\nquestions ùëÑ and passages ùëÉ. The question set ùëÑ = {ùëû1,ùëû2,...,ùëû ùëõ}\ncontains existing questions in the FAQ that have known answers\n(i.e., passages directly extracted from the FAQ), new questions with\ninferred answers (i.e., manually generated questions that have no\ndirect answers but the answer can be inferred from multiple pas-\nsages in the FAQ), and questions that do not have an answer in\nthe KB (questions were manually generated by checking that they\ncannot be answered with the passages in the KB).\nTo simulate the scenario of different users expressing similar\nquestions differently, we generated multiple semantically equiva-\nlent variations for each unique question initially included in the\nFAQ. The passages setùëÉ = {ùëù1,ùëù2,...,ùëù ùëö}is a corpus of passages\nextracted from the FAQ, representing our KB. Finally, each question\nis associated with a gold answer from a set ùê¥= {ùëé1,...,ùëé ùëõ}. Each\nanswer ùëé is obtained from one or more passages. We created a\ntestbed that consists of relevance judgments at the passage level\nand gold answers for three types of questions:\nQuestions with Known Answers (Known).These questions have\na direct answer in the FAQ. Therefore, the corresponding passageùëù\nto the answer ùëéin the FAQ is judged as Highly Relevant (label = 2).\nAll questions related to the same topic have the same passage judged\nas relevant, which is also the gold answer (ùëù = ùëé).\nQuestions with Inferred Answers (Inferred).Questions that do\nnot have a direct answer in the FAQ, but have an answer that can\nbe extracted from the KB, i.e., from one or more passages. Passages\nthat partially contain relevant information to answer the question\nare judged as Partially Relevant (label = 1). The gold answer is\nmanually generated by combining multiple passages.\nOut-of-Knowledge Base Questions (Out of KB).These questions\ncannot be answered with the information available in the KB ‚Äì\neven though, these questions are within the domain and likely\nto be asked. Therefore, there are no relevant passages, and the\ngold answer consists of communicating to the user that there is no\ninformation available to answer that question.\nTable 1 shows example passages and answers for each question\ntype. We have 106 questions (including variations) and 120 passages.\nIn our collection, 84 questions have known answers (passages di-\nrectly extracted from the FAQ), 12 have inferred answers, and 10\ndo not have an answer in the KB. These three question types in\nWalert CHIIR ‚Äô24, March 10‚Äì14, 2024, Sheffield, United Kingdom\nour testbed provide a comprehensive evaluation that allows us to\ncompare IB against RAG conversational approaches.\nIn particular, using the questions with known answers, we can\nassess the system‚Äôs ability to correctly respond to questions for\nwhich we know a passage in the KB contains the complete answer.\nBy evaluating the effectiveness of the answers for the questions\nwith inferred answers, we can assess the system‚Äôs ability to gen-\nerate answers by combining multiple (partially) relevant passages.\nFinally, including questions not covered in the FAQ helps assess\nthe chatbot‚Äôs ability to identify unanswerable questions ‚Äì which is\na critical step in controlling hallucinations.\n2.2 Intent-Based (IB)\nThe IB approach consists of a conversational model built using\nAmazon Alexa Skills [16] (upper part of Figure 1). Each question\nin the FAQ is mapped to an intent in the conversational model, i.e.,\none of the possible actions recognizable by the system.4 Intuitively,\nthis approach aims to optimize the correctness of the answers (high\nprecision) but only handles a limited number of questions (low re-\ncall), i.e., those present in the FAQ (questions with known answers).\nBuilding effective intent recognition models requires multiple in-\nstances to train each intent. Since manually creating variations\nof training utterances is time-consuming, we experimented with\nusing open-source LLMs to automatically create semantically equiv-\nalent variations of utterances (i.e., training data augmentation). In\nour case, these instances would be the semantic variations of the\nquestions from the FAQ. We deployed Falcon-7B [2] in Amazon\nSageMaker Studio with a 5xlarge-GPU configuration5 to generate\nup to eight question variations for each intent (i.e., a question in the\nFAQ), using a zero-shot approach and the following prompt:‚Äúgen-\nerate up to eight paraphrases of the following question: QUESTION‚Äù .\nAfter manually inspecting the variations generated, we established\na threshold and selected the top five. These variations were then\nused to train the conversational model. We also normalized the\ninstances by resolving the acronyms (e.g., replacing CS with Com-\nputer Science) and used them along with the original questions for\ntraining, making it a total of six training instances per the intent of\nthe conversational model. The answers associated with the original\nquestions in the FAQ were used as responses returned for each\nquestion. The Alexa Skill was finally deployed in an Amazon Echo\n(5ùë°‚Ñé generation) device, which allowed users to interact with the\nsystem in an audio-only setting. To enable this, we utilized the\nAutomatic Speech Recognition and NLU features built into Amazon\nAlexa Skill.\n2.3 Retrieval-Augmented Generation (RAG)\nIn contrast to the precision-oriented IB approach, we sought to\ninvestigate a more open-ended methodology known as RAG [11].\nHere, we cover two use cases: (i) instances where the questions the\nuser raises vary from what is available in the FAQ, and(ii) scenarios\n4https://developer.amazon.com/en-US/docs/alexa/custom-skills/create-intents-\nutterances-and-slots.html\n5We explored different alternatives that would allow us to better understand the\nprocess of managing a privacy-aware LLM solution in-house to avoid the potential\nleaking of sensitive data by using third-party solutions. We found this alternative to be\na good fit for our needs, also the most flexible for our future research, e.g., experiments\ninvolving fine-tuning.\nin which questions can only be answered using abstractive multi-\ndocument summarization from multiple passages in the KB. The\nRAG approach consists of two main stages (bottom part of Figure 1).\nThe initial stage involves retrieving potential passages that may\ncontain the answer, while the second stage involves generating a\nsummary from the top-ùêæ retrieved passages.\nFor the retrieval model, we experimented with two approaches:\n(i) Okapi BM25 [17] with default parameters (ùëò1 = 1.2; ùëè = 0.75)\nand (ii) dense retrieval using Dense Passage Retrieval (DPR) [10] im-\nplementations in the pyserini toolkit [13]. To generate a summary\nfrom the top-ùêæ retrieved passages, we used the same LLM that was\nused to generate semantically equivalent variations of the questions\nfor the IB approach, i.e., falcon-7b-instruct (refer Section 2.2)\nalong with the following prompt to generate the summaries:\nGenerate an answer to be synthesized with text-to-speech for a\nvirtual assistant, the answer should be based on the retrieved\ndocuments for the following question. If the retrieved documents\nare not related to the question, then answer NA.\n[QUESTION + LIST OF ùëò PASSAGES]\nWe experimented with three top-heavy ùëò cutoffs: 1 (which is\ncomparable to IB), 3, and 5 top retrieved passages.\n3 QUANTITATIVE EVALUATION\nThe test collection created from the knowledge base described in\nSection 2, allows us to apply effectiveness evaluation practices to\ncompare our proposed approaches at both the component and end-\nto-end levels. Table 2 displays results for IB and RAG approaches\nusing evaluation measures across two dimensions: (i) retrieval ef-\nfectiveness (Normalized Discounted Cumulative Gain, NDCG [8])\nand (ii) natural language generation (BERTScore [20] and ROUGE-\n1 [12]). ROUGE and BERTScore have been used to quantify hal-\nlucinations in LLMs automatically [ 9]. All the results in Table 2\nhave been tested for statistical significance using Tukey‚Äôs HSD and\nsignificance level ùõº = 0.01. Below, we discuss the results across two\ndimensions separately.\nRetrieval Effectiveness. When it comes to retrieving a response,\nthe IB approach only retrieves one passage (i.e., the response associ-\nated with the recognized intent), whereas RAG approaches retrieve\nmultiple passages for a given question (and the final response is\ngenerated using the top-ùëò passages). In our evaluation, we explore\ntop-heavy cutoffs ùëò={1,3,5} to minimize the risk of hallucinations.\nTable 2 shows that, for the Known questions, IB and RAG using DPR\nhave comparable performance in terms of NDCG@1 and RAG ap-\nproaches perform better with more aggressive ranking truncation\n(ùëò = 1). For the Inferred questions, RAG approaches outperform IB\n(which can only return, at most, a passage partially relevant to the\nquestion). RAG approaches benefit from more context, and BM25\nwith ùëò = 3 obtains the highest NDCG score. In terms of out-of-KB\nquestions, IB performs substantially better than RAG approaches,\nbeing able to identify 80% of the unanswerable questions. RAG-\nbased approaches fail by attempting to generate an answer for most\nof the questions, which means that it is likely to hallucinate instead\nof not warning the user about the lack of information in the KB.\nEnd-to-end evaluation. BERTScore and ROUGE-1 scores indi-\ncate that IB performs significantly better for the Known questions,\nwhereas RAG approaches are likely to generate better answers for\nCHIIR ‚Äô24, March 10‚Äì14, 2024, Sheffield, United Kingdom Pathiyan Cherumanal et al.\nTable 1: Examples of questions, relevant passages and gold answers in our testbed.\nQuestion Type Question Passage(s) Answer\nQuestions with\nKnown Answers\nIs the transfer from Asso-\nciate Degree to Bachelors au-\ntomatic?\nNo, it is not. You are required to apply when you are closer to\nthe completion of the Associate Degree. (Highly Relevant )\nNo, it is not. You are re-\nquired to apply when you\nare closer to the comple-\ntion of the Associate De-\ngree.\nQuestions with\nInferred Answers\nWhat does the final year of\nComputer Science (CS) pro-\ngram include?\n(1) [. . . ] Software Engineering (SE) students will do another\nlarge in-house project and more SE electives, while CS students\nwill do a slightly smaller project and a few more core [. . . ].\n(Partially Relevant ) (2) [. . . ] students are required by RMIT\nrules to do a capstone project in their final year. [. . . ] with an\nindustry partner [. . . ] (Partially Relevant )\nIt includes a small cap-\nstone project with a super-\nvisor that work with an in-\ndustry partner, as well as a\nfew more core courses and\nelectives.\nOut-of-KB Ques-\ntions\nWhen does the application for\nprogram transfer open?\nNot available (No Relevant Passages ) I‚Äôm sorry, I don‚Äôt have an\nanswer.\nTable 2: Quantitative evaluation of the retrieval phase (NDCG) and generated answers (BERTScore and ROUGE-1), broken\ndown by type of questions. Effectiveness for Out of KB base questions is reported with the percentage of empty rankings / ‚ÄúI\ndon‚Äôt have an answer‚Äù responses. Boldface indicates the best score for each measure and ‚àóindicates statistically significant\ndifferences against all the other approaches according to Tukey‚Äôs HSD and significance level ùõº = 0.01.\nApproach Retrieval Known (84 Questions) Inferred (12 Questions) Out of KB (10 Questions)\nCutoff ùëò NDCG BERTScore ROUGE-1 NDCG BERTScore ROUGE-1 % Unanswered BERTScore ROUGE-1\nIntent-Based (IB) 0.643 0.771‚àó 0.671‚àó 0.083 0 .332‚àó 0.062 80.00 0 .866‚àó 0.813‚àó\nRAG (BM25 + Falcon)\n1 0 .512 0 .536 0 .179 0 .167 0 .493 0 .185 0 .00 0 .336 0 .045\n3 0 .491 0 .543 0 .209 0 .256 0 .447 0 .106 10 .00 0 .293 0 .054\n5 0 .473 0 .543 0 .209 0.329 0.447 0 .106 10 .00 0 .293 0 .054\nRAG (DPR + Falcon)\n1 0.691 0.545 0 .193 0 .250 0.513 0 .192 10.00 0 .340 0 .048\n3 0 .612 0 .564 0 .244 0 .235 0 .476 0 .123 20 .00 0 .311 0 .045\n5 0 .581 0 .564 0 .244 0 .235 0 .476 0 .123 20 .00 0 .311 0 .045\nthe Inferred questions. It is worth noting that the LLM tend to ben-\nefit from having less context (i.e., fewer passages in the prompt),\nachieving higher BERTScore and ROUGE-1 scores for lower cutoffs\nfor both RAG approaches. Results also corroborate that IB performs\nsignificantly better than RAG approaches for Out of KB questions.\n4 IMPACT AND FUTURE WORK\nWe built Walert, a conversational agent that answers FAQs about\nprograms of study offered in the School of Computing Technologies\nat RMIT University. The IB approach, deployed on an Amazon Echo\ndevice, was showcased as a demo at the university‚Äôs Open Day in\nAugust 2023 where potential future students learned about the use\nof LLM-based conversational systems and its risks and limitations.\nThe demo, which was also showcased to visiting high-school stu-\ndents in September 2023, generated university-wide interest and\nconnections, including the IT service team building a university-\nwide solution.\nThere are several limitations that we are aiming to address in fu-\nture work. The current demo relies upon a limited knowledge base\n(i.e., a manually curated FAQ). We aim to reproduce our methodol-\nogy with a more extensive set of documents, including brochures\nand internal web pages related to the delivery of CS programs.\nFurther research on evaluation measures (beyond BERTScore and\nROUGE) is needed to evaluate the validity of generated responses.\nWe aim to explore other evaluation measures, including those for\ntruncated rankings [3] and other dimensions of LLM-based conver-\nsational systems [18]. Finally, we plan to deploy RAG approaches\nto perform online experimentation.\nThe process of building Walert helped us not only to share com-\nplementary knowledge across our group but also to facilitate knowl-\nedge translation within the university. We believe the approach and\nthe lessons learned can help other researchers aiming to bridge the\ngap between experts and practitioners interested in building (and\ntesting) LLM-based conversational information-seeking systems.\nACKNOWLEDGMENTS\nWalert was designed and developed in the unceded lands of the\nWurundjeri and Boon Wurrung peoples of the eastern Kulin Nation.\nWe pay our respects to their Ancestors and Elders, past, present,\nand emerging. This research is partially supported by the Australian\nResearch Council (DE200100064, CE200100005) and is undertaken\nwith the assistance of computing resources from RACE (RMIT AWS\nCloud Supercomputing). We thank Amina Hossain and Santha\nSumanasekara for their valuable contributions.\nWalert CHIIR ‚Äô24, March 10‚Äì14, 2024, Sheffield, United Kingdom\nREFERENCES\n[1] Eugene Agichtein, Yoelle Maarek, and Oleg Rokhlenko. 2022. Alexa Prize\nTaskBot Challenge. In Alexa Prize TaskBot Challenge 1 Proceedings . https:\n//www.amazon.science/alexa-prize/proceedings/alexa-prize-taskbot-challenge\n[2] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,\nRuxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien\nLaunay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme\nPenedo. 2023. Falcon-40B: An Open Large Language Model with State-of-the-art\nPerformance. Findings of the Association for Computational Linguistics: ACL 2023\n(2023), 10755‚Äì10773.\n[3] Enrique Amig√≥, Stefano Mizzaro, and Damiano Spina. 2022. Ranking Interruptus:\nWhen Truncated Rankings Are Better and How to Measure That. In Proceedings\nof the 45th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (Madrid, Spain) (SIGIR ‚Äô22) . Association for Computing Ma-\nchinery, New York, NY, USA, 588‚Äì598. https://doi.org/10.1145/3477495.3532051\n[4] Vicki Couzens, Jeph Neale, Hilary Jackman, Grace Leone, and Jes-\nsica Clark. 2019. Wurrunggi Biik: Law Of The Land. https:\n//issuu.com/rmitculture/docs/wurrunggi_biik_law_of_the_land Accessed: 15\nDec 2023.\n[5] Aniket Deroy, Kripabandhu Ghosh, and Saptarshi Ghosh. 2023. How Ready\nare Pre-trained Abstractive Models and LLMs for Legal Case Judgement Sum-\nmarization?. In Proceedings of the 3rd Workshop on Artificial Intelligence and\nIntelligent Assistance for Legal Professionals in the Digital Workplace (LegalAIIA\n2023). https://ceur-ws.org/Vol-3423/paper2.pdf\n[6] Zihan Gao and Jiepu Jiang. 2021. Evaluating Human-AI Hybrid Conversational\nSystems with Chatbot Message Suggestions. In Proceedings of the 30th ACM\nInternational Conference on Information & Knowledge Management (Virtual Event,\nQueensland, Australia) (CIKM ‚Äô21) . Association for Computing Machinery, New\nYork, NY, USA, 534‚Äì544. https://doi.org/10.1145/3459637.3482340\n[7] Christian G√∂bel. 2013. The Information Dilemma: How ICT Strengthen or Weaken\nAuthoritarian Rule. Statsvetenskaplig tidskrift 115, 2013 (2013), 367‚Äì384.\n[8] Kalervo J√§rvelin and Jaana Kek√§l√§inen. 2002. Cumulated Gain-Based Eval-\nuation of IR Techniques. ACM Trans. Inf. Syst. 20, 4 (oct 2002), 422‚Äì446.\nhttps://doi.org/10.1145/582415.582418\n[9] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\nYe Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in\nNatural Language Generation. ACM Comput. Surv. 55, 12, Article 248 (mar 2023),\n38 pages. https://doi.org/10.1145/3571730\n[10] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\nDomain Question Answering. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) . Association for Computational\nLinguistics, Online, 6769‚Äì6781. https://doi.org/10.18653/v1/2020.emnlp-main.550\n[11] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel,\nSebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation\nfor Knowledge-Intensive NLP Tasks. In Proceedings of the 34th International\nConference on Neural Information Processing Systems (Vancouver, BC, Canada)\n(NIPS‚Äô20). Curran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages.\n[12] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\nIn Text Summarization Branches Out . Association for Computational Linguistics,\nBarcelona, Spain, 74‚Äì81. https://aclanthology.org/W04-1013\n[13] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep,\nand Rodrigo Nogueira. 2021. Pyserini: A Python Toolkit for Reproducible In-\nformation Retrieval Research with Sparse and Dense Representations. In Pro-\nceedings of the 44th International ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval (Virtual Event, Canada) (SIGIR ‚Äô21) . Asso-\nciation for Computing Machinery, New York, NY, USA, 2356‚Äì2362. https:\n//doi.org/10.1145/3404835.3463238\n[14] Yoelle Maarek. 2022. Alexa, Let‚Äôs Work Together! How Alexa Helps Customers\nComplete Tasks with Verbal and Visual Guidance in the Alexa Prize TaskBot\nChallenge. In Proceedings of the 30th ACM International Conference on Multimedia\n(Lisboa, Portugal) (MM ‚Äô22) . Association for Computing Machinery, New York,\nNY, USA, 1‚Äì2. https://doi.org/10.1145/3503161.3549912\n[15] City of Port Phillip . 2016. Boonwurung Walert (Possum Skin) Cloak.\nhttps://www.portphillip.vic.gov.au/explore-the-city/first-peoples/first-peoples-\narts/boonwurung-walert-possum-skin-cloak Accessed: 15 Dec 2023.\n[16] Ashwin Ram, Rohit Prasad, Chandra Khatri, Anushree Venkatesh, Raefer Gabriel,\nQing Liu, Jeff Nunn, Behnam Hedayatnia, Ming Cheng, Ashish Nagar, Eric King,\nKate Bland, Amanda Wartick, Yi Pan, Han Song, Sk Jayadevan, Gene Hwang,\nand Art Pettigrue. 2017. Conversational AI: The science behind the Alexa Prize.\n(2017). https://www.amazon.science/publications/conversational-ai-the-science-\nbehind-the-alexa-prize\n[17] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\nMike Gatford, et al. 1995. Okapi at TREC-3. Nist Special Publication Sp 109 (1995),\n109.\n[18] Tetsuya Sakai. 2023. SWAN: A Generic Framework for Auditing Textual Conver-\nsational Systems. arXiv preprint arXiv:2305.08290 (2023).\n[19] Winson Ye and Qun Li. 2020. Chatbot security and privacy in the age of personal\nassistants. In 2020 IEEE/ACM Symposium on Edge Computing (SEC) . IEEE, 388‚Äì393.\nhttps://doi.org/10.1109/SEC50012.2020.00057\n[20] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.\n2020. BERTScore: Evaluating Text Generation with BERT. In Proceeding sof\nthe International Conference on Learning Representations (ICLR‚Äô20) . https:\n//openreview.net/forum?id=SkeHuCVFDr",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.707177460193634
    },
    {
      "name": "Chatbot",
      "score": 0.6760371923446655
    },
    {
      "name": "Bridge (graph theory)",
      "score": 0.645620584487915
    },
    {
      "name": "Knowledge management",
      "score": 0.4929084777832031
    },
    {
      "name": "World Wide Web",
      "score": 0.48945796489715576
    },
    {
      "name": "Action (physics)",
      "score": 0.47269773483276367
    },
    {
      "name": "Best practice",
      "score": 0.4312620759010315
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82951845",
      "name": "RMIT University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I60053951",
      "name": "Universitat Polit√®cnica de Val√®ncia",
      "country": "ES"
    }
  ]
}