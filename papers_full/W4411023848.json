{
  "title": "Rethinking VLMs and LLMs for image classification",
  "url": "https://openalex.org/W4411023848",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5064217008",
      "name": "A. Feder Cooper",
      "affiliations": [
        "Fujitsu (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5025156619",
      "name": "Keizo Kato",
      "affiliations": [
        "Fujitsu (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A5113498220",
      "name": "Chung‐Yang Shih",
      "affiliations": [
        "Fujitsu (United States)",
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5087989444",
      "name": "Hiroaki Yamane",
      "affiliations": [
        "Fujitsu (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A5114534210",
      "name": "Kasper Vinken",
      "affiliations": [
        "Fujitsu (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5114534211",
      "name": "Kentaro Takemoto",
      "affiliations": [
        "Fujitsu (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A5114534212",
      "name": "Taro Sunagawa",
      "affiliations": [
        "Fujitsu (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A5047357648",
      "name": "Hao-Wei Yeh",
      "affiliations": [
        "Fujitsu (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A5020961850",
      "name": "Jin Yamanaka",
      "affiliations": [
        "Fujitsu (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5111683056",
      "name": "Ian R. Mason",
      "affiliations": [
        "Fujitsu (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5020830959",
      "name": "Xavier Boix",
      "affiliations": [
        "Fujitsu (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3213454282",
    "https://openalex.org/W4385574156",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4296406182",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4312298950",
    "https://openalex.org/W3023989664",
    "https://openalex.org/W4296605665",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2963890019",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W3199693760",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W6600798642",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W4390874575",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6600008909",
    "https://openalex.org/W6850503672",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4390871773",
    "https://openalex.org/W4404783780",
    "https://openalex.org/W6813345622",
    "https://openalex.org/W2145680191",
    "https://openalex.org/W4390872747",
    "https://openalex.org/W4388626886",
    "https://openalex.org/W4386065691",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W4385573131",
    "https://openalex.org/W2277195237"
  ],
  "abstract": null,
  "full_text": "Rethinking VLMs and LLMs for \nimage classification\nAvi Cooper1, Keizo Kato2,4, Chia-Hsien Shih1,3, Hiroaki Yamane2, Kasper Vinken1, \nKentaro Takemoto2, Taro Sunagawa2, Hao-Wei Yeh2, Jin Yamanaka1, Ian Mason1,4 & \nXavier Boix1\nVisual Language Models (VLMs) are now increasingly being merged with Large Language \nModels (LLMs) to enable new capabilities, particularly in terms of improved interactivity and open-\nended responsiveness. While these are remarkable capabilities, the contribution of LLMs to enhancing \nthe longstanding key problem of classifying an image among a set of choices remains unclear. Through \nextensive experiments involving seven models, ten visual understanding datasets, and multiple \nprompt variations per dataset, we find that, for object and scene recognition, VLMs that do not \nleverage LLMs can achieve better performance than VLMs that do. Yet at the same time, leveraging \nLLMs can improve performance on tasks requiring reasoning and outside knowledge. In response to \nthese challenges, we propose a pragmatic solution: a lightweight fix involving a relatively small LLM \nthat efficiently routes visual tasks to the most suitable model for the task. The LLM router undergoes \ntraining using a dataset constructed from more than 2.5 million examples of pairs of visual task and \nmodel accuracy. Our results reveal that this lightweight fix surpasses or matches the accuracy of state-\nof-the-art alternatives, including GPT-4V and HuggingGPT, while improving cost-effectiveness.\nMany of the best methods for visual representation learning now make use of Visual Language Models (VLMs)1–3. \nThese models combine vision and language (e.g. with contrastive training) in order to learn general vision-\nlanguage representations. Recently, several new VLMs have been developed that combine VLMs with pre-trained \nLarge Language Models (VLM+LLMs)14–6. Since LLMs are trained on large corpora of text and computer code, \ncombining pre-trained LLMs with VLMs has brought many benefits to VLMs, including the addition of outside \nknowledge and reasoning, iterative interaction, and an ability to provide open-ended responses.\nWhile these new abilities are impressive, image classification tasks in which the image is categorized into a \nclosed set of possible choices, such as in classic object recognition, constitute one of the primary goals of VLMs \nand computer vision in general. At the moment, it is unclear whether the additional capabilities of VLM+LLMs \nalso contribute to an overall improvement to image classification. In the literature on VLM+LLMs, the evaluation \nfocus has primarily centered on assessing the new abilities introduced by LLMs, i.e. open-ended answering \nand interactivity, with limited attention given to evaluating their impact on classifying the image into a given \nset of possible choices as it was traditionally done in image classification. Thus, in this work we re-evaluate \nVLM+LLMs on a diverse set of visual tasks with the goal of answering: when does leveraging LLMs improve \nmodel performance in traditional image classification setup?\nWe evaluate the performance of existing open source VLMs and VLM+LLMs across a range of benchmarks \ndesigned to test object and scene recognition as well as visual reasoning and outside knowledge. There are three \npossible outcomes to this experiment that a priori are not possible to discern: \n 1. VLM+LLMs are superior in all the tasks for image classification. This result may be the most expected, as \nVLM+LLMs currently dominate state-of-the-art accuracy in most vision tasks.\n 2. VLMs outperform VLM+LLMs depending on datasets. If we observe this, it is worth investigating when \nVLMs are better than VLM+LLMs.\n1 We use the term LLM as it is commonly used, i.e. an auto-regressive text-generating model trained exclusively on text data. \nTo distinguish VLMs that do and do not leverage an LLM as part of their architecture, we use the terms VLM+LLM and \nVLM respectively.\n1Fujitsu Research of America, Santa Clara, CA, USA. 2Fujitsu Limited, Kawasaki, Japan. 3University of Illinois at \nUrbana-Champaign, Urbana, IL, USA. 4Avi Cooper, Keizo Kato, Ian Mason and Xavier Boix have contributed equally \nto this work. email: kato.keizo@jp.fujitsu.com\nOPEN\nScientific Reports |        (2025) 15:19692 1| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports\n\n 3. VLMs always surpass VLM+LLMs on image classification tasks. This result sounds contrary to expectations \nat first. However, we argue this is possible since unnecessary linguistic knowledge may interfere with the \nvisual representations.\nResults show that despite increasing parameter counts, VLM+LLMs are less performant than VLMs on object \nand scene recognition, while being superior on reasoning and outside knowledge tasks. In a series of experiments, \nwe show that these results are consistent over seven models, ten datasets and multiple prompt variations. Our \nresults are illustrated with examples in Fig. 1.\nThese observations show that different models excel at different vision tasks. We therefore introduce a \ncomputationally efficient system to route the task to the best model. To do so, we fine-tune GPT-2 to act as a router \nusing the results from our experiments (about 2.5 million examples of pairs of visual task and model accuracy) \nand evaluate the router on held-out tasks. Because of this specialized training, our approach demonstrates a \nremarkable effectiveness in held-out visual tasks — it outperforms HuggingGPT 7, and matches the accuracy \nof GPT-4V8 while improving in cost-effectiveness. Therefore, a lightweight LLM can effectively be trained to \narbitrate between models and, based on user input, select one that is most suitable for the task at hand.\nDo VLM+LLMs always beat VLMs?\nIn this section we analyze the capabilities of VLM+LLMs across multiple image classification tasks. We first \nintroduce the VLMs and VLM+LLMs that we analyze and then introduce the evaluation methodology, which \nincludes an overview of the datasets and prompting strategies used. Finally, we present the findings derived from \nour analysis.\nComparing VLM+LLMs with VLMs\nTo fairly compare the effect of leveraging LLMs for vision-language processing, we compare VLM+LLMs and \nVLMs that have exactly the same visual representations. That is, that use identical vision encoders. In this way we \nassess the effectiveness of different strategies for handling text, one that uses LLMs and the other that does not.\nNote that including an LLM in a VLM necessitates the inclusion of additional machinery around the LLM \nto integrate it with the vision encoder (fusion modules, training objective, additional data, etc.). Two prevalent \nstrategies exist in designing VLM+LLM models, namely that visual information can be incorporated into the \nLLM via embeddings (e.g. Flamingo 5) or via text descriptions of the image (e.g. PNP-VQA 4). We investigate \nmodels from each approach.\nFlamingo vs. CLIP  The architecture of Flamingo 5 consists of the pre-trained and frozen vision encoder \nfrom CLIP and an LLM, connected with trainable attention layers. Flamingo is trained on a mixture of large-\nscale multimodal data from the web. We use the open-source OpenFlamingo 9 implementation, utilizing CLIP \nViT-L/141 as the vision encoder and MPT10 as the LLM. Thus, we directly compare Flamingo with CLIP as they \nhave the same vision encoder but different techniques for handling text. To understand the impact of LLM size \nFig. 1. VLMs achieve higher performance on object and scene recognition tasks than VLM+LLMs with \nthe same vision encoder. Q: prompt provided to the models. RO: response options provided to the models. \nLeft: on tasks involving visual reasoning and outside knowledge the VLM+LLM (Flamingo) outperforms the \nVLM (CLIP) with the same vision encoder. Right: on object and scene recognition the VLM is superior to the \nVLM+LLM. Note that Flamingo only uses CLIP’s vision encoder, but since this vision encoder is pre-trained \nwith the full CLIP architecture, in the figure we show CLIP as part of Flamingo. Pictures used in this figure \ncome from the dataset explained in the \"Appendix A\".\n \nScientific Reports |        (2025) 15:19692 2| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\non performance, we conducted experiments with Flamingo employing different parameter LLM configurations, \nincluding 3 billion (3b) and 9 billion (9b) parameters.\nPNP-VQA vs. BLIP PNP-VQA4 facilitates communication between pre-trained and frozen models through \nlanguage, without any additional training. First, PNP-VQA uses a VLM, namely BLIP2, to identify the relevant \nparts in the image for the task at hand and then generates relevant captions (again, with BLIP) for each of the \nidentified parts. Then an LLM, based on UnifiedQA11, provides a final answer by taking into account the prompt \nalong with the captions. Since PNP-VQA makes use of BLIP internally, directly comparing them provides \ninsights into handling text with and without leveraging LLMs.\nLiT vs. CLIP and Flamingo  Locked-image text Tuning (LiT) 3 is a VLM with the same vision encoder as \nCLIP and Flamingo but a larger text encoder than CLIP . It is a VLM and not a VLM+LLM as the text encoder \nis trained in a contrastive manner between text and image features, rather than leveraging pre-trained LLMs. \nWe test two variations of LiT text encoder: BERT-base with 110M parameters (LiT-B/16) and BERT-large with \n340M parameters12 (LiT-L/16), which are much larger than the text encoder in CLIP (vanilla Transformer with \n63M parameters1). We include LiT in our evaluation to assess whether the differences in performance between \nVLMs and VLM+LLMs come from the pre-trained LLM rather than from simply having larger text encoders to \nhandle text.\nReturning to closed-ended evaluation\nIn language modelling tasks, such as translation and image captioning, evaluation is typically open-ended where \nmodels engage in open-ended text generation. On the other hand, image classification is inherently a closed-\nended task, requiring a selection from a set of options rather than general description. In the open-ended case, \nevaluation is unclear because the degree of detail required is somewhat ambiguous. It is not sufficient to say that \nthere is a tree in the image — the model must decide whether it is an elm, oak or willow tree if the task requires \nso. Thus, we adopt closed-ended evaluation for all experiments.\nThe methods we employ to compute closed-ended predictions differ across model types. For VLMs, \nthe prediction of the model is the closed-ended option whose representation is best aligned with the image \nrepresentation through a dot product. For VLM+LLMs, like PNP-VQA and Flamingo, the logits of each of the \nclosed-ended options are used to compute a sequence probability. The sequence of logits with the highest average \nprobability is the prediction of the model. As we show below in the results, differences in how models handle \nclosed-ended prediction does not affect the overall conclusions.\nZero-shot visual tasks\nLLMs excel at zero-shot tasks 13 and can capture nuanced relationships and contextual information that might \nnot be explicitly present in visual data. Conceivably, the extensive knowledge encapsulated by LLMs could be \nharnessed to address visual tasks when there is a lack of training data.\nZero-shot visual tasks allow us to assess the effectiveness of the synergy between VLMs and LLMs in a \nmore controlled manner than if there would be task-specific training samples for fine-tuning. To avoid adding \nconfounding factors arising from fine-tuning design choices (like deciding which layers to unfreeze) all \nexperiments in this paper revolve around zero-shot visual tasks. We next introduce the 10 zero-shot datasets \nwe leverage for our analysis, each characterized by varying degrees of reasoning complexity and knowledge \nrequirements (see \"Appendix A\" for further details).\nCIFAR-10014–a widely used benchmark for general object recognition. OOD-CV15–an out-of-distribution \nobject classification, where objects undergo diverse pose, shape, occlusion and texture variations. Weather16–a \nmulti-class weather recognition from images. Skin Cancer 17–diagnosis for melanoma detection. Hateful \nMemes18–hatred identification in memes through a combination of image and text analysis. ScienceQA19–\ncontains questions with images about scientific topics that require outside knowledge and reasoning capabilities. \nIt resembles a high-school science exam. Visual Genome Relation20–given an image and a relation of the form \n“X relation Y” , this benchmarks test whether the model can pick the correct order “X relation Y” , instead of “Y \nrelation X” . Visual Genome Attribution20–evaluates the ability to attribute properties to objects appropriately, \nstructured in a similar fashion as the Visual Genome Relation dataset. Abstract Scenes VQA21–a commonly used \ndataset to assess the ability to answer questions related to high-level semantic and abstract information from \nscenes. Binary Abstract Scenes22–the same as Abstract Scenes VQA but the response options are restricted to ‘yes’ \nand ‘no’ in order to remove biased responses.\nNote that we do not use ImageNet or Visual Genome for object recognition datasets as some VLMs use them \nfor training their vision encoders and it is unclear whether these datasets can be effectively used to assess zero-\nshot capabilities.\nPrompt choice\nIt is well known that prompt choice can have a significant impact on model performance23–28, and work has been \nconducted to specifically improve LLM’s reasoning capabilities, through Chain-of-Thought29, Tree of Thought30 \nand similar processes.\nDuring evaluation we observed that varying the prompt dramatically affected the evaluation performance of \nthe various networks. For example, when evaluating CLIP on CIFAR-100, using prompts of the form “What is \nthis? This is aquarium_fish” or “What is this? This is beaver” (with class names inserted as-is from the CIFAR-100 \ndataset), resulted in an accuracy of 44.6%. We modified some of the class names, separating words and using \nsimpler forms where appropriate (for example, “aquarium_fish” to “aquarium fish” and “aeroplane” to “plane”). \nWe also prepended the appropriate article (a/an) in object classification tasks. The result was prompts of the \nform “What is this? This is an aquarium fish” and “What is this? This is a beaver. ” These changes alone increased \naccuracy of CLIP on CIFAR-100 up from 44.7 to 69.1%.\nScientific Reports |        (2025) 15:19692 3| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nWe also found that alternative phrasing of the question could impact performance. For example, on the \nAbstract VQA dataset, prompting with the form “Is the dog asleep?” yielded an accuracy of 8.3% in one Flamingo \nmodel, while prompting with the form “Using the image, the answer to Is the dog asleep? is most likely” yielded \nan accuracy of 29.3% for the same model.\nFor this reason, we evaluated all models on a range of prompt formulations appropriate for each dataset. A \nfull description of the class name alterations and prompt variations can be found the in \"Appendix B\".\nResults\nWe analyze results on 462 experiments, including ten datasets, seven models and between two to nine prompt \nstrategies per dataset. Results are displayed in Table 1 and Fig. 2. Table 1 reports the average accuracy among \nprompts and also the best accuracy among prompts, for each model and dataset. We highlight in boldface the \nbest model per dataset, and underline the best model among each group of VLM+LLM and corresponding \nVLM. In order to ensure that the winning models displayed in Table 1 are largely prompt-independent, in Fig. \n2 we depict the dependency of our results on the prompting strategy. Concretely, in Fig. 2a, we display the \ndistribution of accuracy across prompt variations, indicating with different colors whether the model was a VLM \nor a VLM+LLM. As expected, there is substantial variability in accuracy among models across different prompt \nvariations, sometimes more than 50%. Despite this high variability, Fig. 2b shows the proportion of times a VLM \nor VLM+LLM obtains higher accuracy for each prompt variation. We observe consistent model type across \ndatasets, which highlight that the trends are prompt-independent.\nThe key trends that we observe by analyzing the results in Table 1 and Fig. 2 are twofold: \n 1. For most datasets that require visual reasoning and outside knowledge, VLM+LLMs achieves superior accu-\nracy (see Fig. 2b and last six columns in Table 1). VLM+LLMs showcase superior capabilities in extracting \nhigh-level semantic information and adapting to conceptual understanding, which may be an expected re -\nFig. 2. Input prompts affect model performance but mostly do not affect winning model type. (a) Model \nperformance for each model and prompt combination (markers) per dataset (y-axis). The accuracy varies \nacross prompts, in particular for VLM+LLM models in recognition tasks (yellow markers in the first three \nrows). (b) Proportion of times a VLM or VLM+LLM wins, across all prompts. Most datasets (y-axis) show a \nproportion of 1 for either VLM or VLM+LLM, indicating the winning model type is not affected by the specific \nprompt.\n \nTable 1. Accuracies of VLMs and VLM+LLMs across datasets (left: average across prompts / right: best \nprompt accuracy).\nA VLM+LLM and its constituent VLM are color-coded, with the result for the better performing model \nunderlined for each dataset. The highest accuracy achieved for each dataset is in bold. The first four datasets \nare object and scene recognition where VLM models are almost always superior to VLM+LLMs. The final six \ncolumns show tasks requiring reasoning and outside knowledge on which VLM+LLMs outperform VLMs.\n \nScientific Reports |        (2025) 15:19692 4| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nsult. Note that the superior performance of VLM+LLMs cannot be attributed to the fact that the VLM+LLMs \ncontain more parameters to process text. Despite the fact that LiT leverages a text encoder with many more \nparameters than CLIP’s, CLIP achieves superior results to LiT on visual tasks that require reasoning. Thus, \nparameter count alone does not explain the superior performance of Flamingo over CLIP .\n 2. For most datasets of object and scene recognition, VLMs consistently outperform their corresponding \nVLM+LLMs (see Fig. 2b and the first four result column in Table 1), i.e. BLIP outperforms PNP-VQA and \nCLIP outperforms Flamingo. Note that this result cannot be attributed to the possibility that VLM+LLMs \nmight struggle with closed-ended evaluation, as VLM+LLMs still excel in closed-ended visual tasks that \nrequire reasoning, as discussed before in point 1. We also observe that comparing the two versions of Fla -\nmingo, i.e. with the LLMs with 3B and 9B parameters, provides some improvements in some cases, but it is \nnot sufficient to outperform VLMs. All this suggests that the reduction in accuracy may be attributed to the \nway the LLM is integrated with the vision encoder.\nIt is notable that all models struggle in some of these benchmarks, with accuracies close to chance, specifically \nfor the Hateful Memes and Skin Cancer datasets. This highlights the need for further improvement in zero-shot \nvisual tasks, despite the significant strides in recent years.\nRegarding the best-performing model among all, there is no clear winner. For recognition datasets, CLIP \nand LiT dominate. In datasets that require a higher degree of reasoning and outside knowledge, Flamingo and \nPNP-VQA excel depending on the dataset. Thus, exploring the integration of these models into a unified system \nbecomes a key research direction, allowing us to leverage the strengths of each model for optimal performance \nin all scenarios. In the next section, we pursue this idea by introducing a computationally efficient solution that \ncombines the strengths of all the models into one system.\nA fix: routing the task to the best model\nConsider the following set up: a user provides a machine learning system with an image and a question about the \nimage, and the response options. In Sect. 2 we have seen that VLMs and VLM+LLMs complement each other \nin the tasks they excel at. Since there is no single best model, we aim to design a system that can select the right \nmodel based on the user input. This capability opens up two exciting possibilities: \n 1. Enhanced Generalization through Model Diversity: The router enables a hybrid integration of models that \nspecialize in different modalities or tasks, allowing the system to achieve superior performance by utilizing \nthe most capable model for each specific input. The router’s ability to combine outputs from diverse models \nincreases the system’s robustness and generalization capability. By selecting models that offer complemen -\ntary perspectives on the input data, the router reduces the risk of overfitting or bias associated with a single \nmodel’s limitations. This is particularly valuable in tasks with high variability or ambiguity, such as natural \nlanguage understanding or complex visual scene interpretation.\n 2. Resource-Efficient AI Deployments: In resource-constrained environments, such as mobile devices or edge \ncomputing, the router can optimize computational efficiency by selecting lightweight models when high pre-\ncision is not required, and more powerful models when accuracy is critical. This flexible approach balances \nperformance and resource consumption, making it ideal for real-world deployment scenarios.\nTo implement this model selection system, we propose to pass the user input to an LLM which then learns to \nroute the input to the best model for the user’s task. Figure 3 shows an overview of this approach.\nTraining an LLM to act as a router\nPrevious LLM-based model selection systems make use of human written descriptions of the model capabilities7 \nor synthetically generated data 31. In contrast, we use real model execution results calculated during our \nexperiments (i.e. the experiments done to create Table  1). Importantly, this means our router learns from \nexperience with actual data rather than some possibly inaccurate proxy as in previous works.\nAfter running experiments over all models, datasets and prompts we create a dataset of more than 2.5 million \nsamples containing (i) an input prompt and image, (ii) a set of options to respond with, (iii) model performance. \nAll of the input is represented in natural language. For the input image, we calculate simple image metadata in \nthe form of image resolution and per channel mean and standard deviations which are prepended to the prompt. \nThe set of response options is a list of possible answers to the visual task. We find that the router performs best \nwhen the list of response options are not included as input to the LLM router, even though these are necessary to \nexecute the predicted model for closed-ended evaluation. Per sample model performance is represented firstly \nby a boolean that is true if the sample is correctly classified by the model and false otherwise. Secondly, for each \nsample we additionally include average model performance over each dataset. \"Appendix  D\" shows the full \nformat of our data.\nThis initial dataset is then filtered by first removing any samples that use a prompt strategy that is not \nvalid across all models (see \"Appendix B\"). By including in the training set different variations of prompts, we \nencourage the router to become robust to prompt variations. Following this, for a given input prompt and image \nwe compare performance across all models to select the single best model, first by discarding any model that did \nnot correctly classify the input and then selecting the model with the best average performance for the prompt \non the dataset from the models remaining. In the case where all models incorrectly classify an image, we also \nselect the model with best average performance. After this filtering process we are left with a training dataset \ncontaining approximately 280,000 training samples.\nAs we want our router to select a model based on the user’s natural language query, we require an LLM. In \norder to keep the routing process relatively lightweight we pick a pre-trained GPT-232 to use as a router. We then \nScientific Reports |        (2025) 15:19692 5| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nfine-tune GPT-2 taking the input prompt and image metadata as input, and the correct model as output, which \nis predicted as the next token after the input. The fine-tuning is done using the standard cross-entropy loss. \nOther implementation details are shown in \"Appendix D\". We also experiment with the router architecture by \nfine-tuning LLaMa33 to assess whether our method is base model agnostic.\nEvaluating on held-out datasets\nTo test the generalization abilities of the router on new tasks, we test on unseen datasets in a cross-validation \nstyle. In this way, we test the ability of the router to generalize to datasets where accuracy measurements are not \navailable during training. Thus, we train ten separate models, each time holding out one of the ten datasets which \nis then used to evaluate the performance of the router. Thus, by testing the LLM router on out-of-distribution \ndatasets, we can properly evaluate its generalization capabilities.\nWe initially compare the performance of our LLM router against basic model selection techniques both in \nterms of accuracy on unseen data and computational cost, expressed as the number of models that must be run \nin order to use the technique. These model selection baselines are the following:\nChance–no models are run and a random label is assigned to each image. Average–we report the average \nperformance over all models, note that in the limit this is the same as randomly sampling a model each time \na new input is received. Voting–all models are run and the modal output is selected, this can be seen as a form \nof model ensemble. Oracle–requires knowing the average accuracies for all models on the held-out dataset and \nthen selecting the best performing model for a given input. Upper bound–per image oracle that shows how often \nat least one of the available models correctly classifies a given input.\nIn Table 2 we see that our router outperforms all baselines, achieving the best performance on the largest \nnumber of datasets (seven of ten) as well as the best average performance. Most notably our method outperforms \nthe strongest baseline of ensemble voting, whilst being more computationally efficient at inference time. \nEnsemble voting requires executing every model on the inputs, making it linear in the number of models. On the \nother hand, our method only executes the single model chosen by the router, so is of constant order. Compared \nto the best baseline with the same computational cost ( average), our method achieves almost a ten-percentage \npoint improvement on average. Regarding the baselines that require access to ground truth results on the held-\nout dataset (oracle and upper bound), our method is very close to the oracle performance, suggesting our router \noften picks the best model despite not having access to ground truth data. Finally, the upper bound column \nshows that by collating only a relatively small number of models we can theoretically achieve extremely high \nperformance if the perfect model could be picked for every individual image, which indicates that there is still \nroom for future improvements.\nComparison with state-of-the-art\nWe additionally compare against two modern state-of-the-art solutions that are applicable to our use case. Firstly \nHuggingGPT7 which uses prompt design and in-context learning to get GPT-3 13 to provide a model, or set of \nmodels, from the HuggingFace Library34 to execute the task and then summarize the results. Secondly, we consider \nthe newly released GPT-4V(ision)8,35. We use GPT-4V to evaluate a very recent approach which, motivated by \nscaling laws36,37, focuses on massive compute and data as a solution for improving visual understanding. Whilst \ntechnical details about GPT-4V are relatively limited we do know that this system is substantially more compute \nheavy than our relatively small LLM router and open-source models, any one of which can run on a single V100 \nGPU. Details of our implementations of these methods can be found in \"Appendix C\".\nFig. 3. Our router approach where an LLM selects a suitable VLM/VLM+LLM to obtain high accuracy. The \nuser input is first provided to the LLM router (1), which uses this information to select a model. The chosen \nmodel (2) is then provided with the user input (3) and generates a prediction (4). Pictures used in this figure \ncome from the dataset explained in the \"Appendix A\".\n \nScientific Reports |        (2025) 15:19692 6| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nTable 3 compares our LLM routing approach against these baselines. We note that due to rate restrictions and \nhigh monetary costs (\"Appendix C\") this table evaluates a total of 365 samples randomly selected (40 per dataset, \nexcept Weather and Skin Cancer, which have 24 and 21, respectively). Whilst this limited number of samples \nmeans we must be careful not to read too much into small differences in performance for individual datasets, \nsome interesting trends emerge overall.\nWhen comparing to HuggingGPT, our approach outperforms HuggingGPT on nine of ten datasets and by \nmore than ten percent absolute accuracy on average. Although HuggingGPT has access to many more models \nthan our system, its reliance on text descriptions of the models and in-context learning means it fails to perform \nas well as our approach which is trained on actual model accuracy. What’s more, due to the careful prompt \nengineering and in-context samples, queries to HuggingGPT run for several thousand tokens which at current \npricing for GPT-3 (text-davinci-003) means we faced an inference cost of approximately $0.05 per sample.\nWhen comparing to GPT-4V our method performs competitively with the average performance difference \nbeing less than one percent, this is despite our method requiring vastly less computational resources 2. For \ninstance, the largest VLM + LLM model in our experiment, Flamingo-9B, consists of 9 billion parameters. Even \nwhen augmented with a router powered by GPT-2 or LLaMA (15 billion/13 billion parameters, respectively), \nthe total parameter count remains significantly smaller than that of GPT-4V . While the router could add latency \nas it adds an additional step to generate the response, this latency is negligible because we use a relatively small \nmodel as a router (GPT-2).\n2 Very conservatively assuming GPT-4V is at least as big as GPT-3 this means it has a minimum of 175 billion parameters. In \ncontrast our largest models use around 10 billion parameters.\nHuggingGPT GPT-4V Router (GPT-2) Router (LLaMA)\nCIFAR-100 15.0 55.0 60.0 60.0\nOOD-CV 75.0 82.5 77.5 72.5\nWeather 62.5 91.7 95.8 75.0\nScienceQA 57.5 65.0 50.0 47.5\nSkin Cancer 38.1 38.1 61.9 61.9\nHateful Memes 42.5 45.0 65.0 65.0\nVisual genome attribution 87.5 90.0 87.5 85.0\nVisual genome relation 65.0 62.5 57.5 57.5\nAbstract scenes VQA 37.5 50.0 30.0 27.5\nBinary abstract scenes 50.0 77.5 62.5 57.5\nAverage 53.1 65.7 64.8 60.9\nAverage (Weighted) 53.4 66.0 63.6 60.3\nTable 3. Comparison against state-of-the-art models on a separate smaller test set. Our method outperforms \nHuggingGPT and is competitive with GPT-4V with much lower computational cost.\n \nChance Average Voting Router (GPT-2) Oracle Upper Bound\nCIFAR-100 1.0 60.9 76.5 60.7 72.7 92.3\nOOD-CV 16.1 81.4 89.5 85.9 89.3 97.5\nWeather 31.9 70.9 84.1 84.1 86.4 96.4\nSkin Cancer 57.1 41.7 40.8 48.7 50.0 86.3\nHateful Memes 58.7 49.4 48.4 49.7 53.4 98.2\nScienceQA 36.0 44.7 44.0 57.7 56.6 92.5\nVG Attribution 50.1 75.3 80.6 88.8 91.6 99.8\nVG Relation 50.2 55.8 57.9 60.4 60.4 97.3\nAbstract Scenes 5.7 17.2 16.9 42.3 42.3 70.2\nB. Abstract Scenes 50.8 51.4 52.3 58.6 58.5 92.1\nAverage 35.7 54.9 59.1 63.7 66.1 92.9\nAverage (Weighted) 18.0 55.5 61.2 64.9 68.4 89.6\nCost (N. of Models) - O(1) O(N) O(1) O(N) O(N)\nTable 2. Performance comparison for different model selection baselines. Accuracy is averaged across prompt \nstrategies. Each row evaluates a separately trained router which sees no samples from the listed evaluation \ndataset during training. As well as surpassing other methods in terms of accuracy our router also has constant, \nrather than linear, computational cost in the number of models. We report average performance over datasets \nas well as the average weighted by the different sizes of the datasets.\n \nScientific Reports |        (2025) 15:19692 7| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nWe also evaluated an LLM router with LLaMA as the LLM. However, we did not observe an improvement \nof accuracy, possibly because more training data is needed given LLaMA has more parameters than GPT-2. In \n\"Appendix E\" we show further results with the LLaMA router.\nThe success of our LLM router approach can be attributed to its specific training on a dataset that includes \npairs of visual tasks and model accuracy. In contrast, other approaches rely on proxies like human-generated \ndescriptions of the models.\nAnalyses and ablations\nIn this section we further analyze our results, first by exploring how the router makes decisions on held-out \ndatasets and then by conducting an ablation study where we experiment with the input information provided \nto the router.\nTo explore how the router makes decisions we plot how often each model is selected by the router for each \nheld-out dataset in Fig. 4a. We can then compare this to how often each model is used for each dataset in the \ntraining data in Fig. 4b. From this figure, we obtain the following insights about the decision-making process of \nthe router. If we consider the dataset OOD-CV , we see that LiT-B/16 is always selected by the router. Note that \nthis is the best model for CIFAR-100. Since OOD-CV is not seen during the training of the router, results suggest \nthat the router perceives the unseen OOD-CV data is most similar to the input from CIFAR-100 and as a result, \nuses the best model for CIFAR-100 when presented with OOD-CV samples. We see similar patterns for other \ndatasets like Visual Genome Attribution and Relation. This figure shows that the router can transfer knowledge \nbetween similar datasets due to the similarity in prompts for similar tasks.\nWe additionally provide an ablation study using the GPT-2 router in Table 4. We evaluated different input \nformats for the router (i) with or without the response options (RO) (ii) with or without image metadata (MD). \nFirstly, eliminating response options from the input prompt largely improves the performance. Thus, although \nwe expected that including response options would help to find similar datasets in the training data, it seems that \nresponse options hinder rather than help. One possible explanation is that the response options are very specific \nto each dataset and act as shortcuts that lead to overfitting rather than generalization across datasets.\nRecall that as input to the LLM router, we include simple metadata from the image (see Sect. 3.1). Table 4 \nshows (a small) improvement due to the use of the image metadata. The dataset which has the largest performance \ngap when metadata is ablated is Weather (84.1% vs. 73.9%). This difference in performance suggests there may \nbe image-level (rather than dataset-level) patterns that can be used to improve routing; an interesting avenue \nfor further investigation is multimodal routers with stronger image processing abilities. See \"Appendix E\" for a \nmore in-depth analysis.\nFinally, in Table 4, we show in-distribution performance, where the router is trained on all datasets and \nevaluated on held-out samples from datasets it has seen during training. To do so, we created an 80/10/10 train/\nvalidate/test split. Here, we see that even when datasets are held-out the router performs similarly to the in-\ndistribution case, suggesting our router has learned to generalize well across datasets. A similar ablation study \nfor LLaMa along with further heat maps are shown in \"Appendix E\".\nRelated work\nFoundation models: VLMs & LLMs\nFoundation models38 have changed the landscape of vision and language, with large image39–41, language13,42,43 \nand explicitly multimodal 5,6,35,44–46 transformer based models 47 creating new capabilities in terms of \ngeneralization and low-shot performance48. Of these various models, our work is most concerned with Vision \nLanguage Models1–3 and methods that combine VLMs with LLMs4,5,49.\nSome recent intriguing findings highlight the nuanced relationship between LLMs and VLMs 5,50,51. In the \nstudy by Roth et al.50, CLIP evaluated on images with randomly generated query texts demonstrates comparable \nFig. 4. Heat maps of the LLM router’s model selection distribution and the distribution of models in the \ntraining data.\n \nScientific Reports |        (2025) 15:19692 8| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nperformance with CLIP evaluated on images with LLM-generated query texts. This is further evidence, \ncomplementary to ours, that how best to use LLMs to improve VLMs is remains an open question.\nMeanwhile, a careful examination of the Flamingo paper, reveals that it lags behind state-of-the-art VLM \nmodels in ImageNet and Kinetics700 classification tasks as shown in \"Appendix B\" in5. This trend can also be \nseen by comparing Tables 5 and 19 in the results of PaLI6. Although several large multimodal models have been \nintroduced since Flamingo, their primary advancements lie in aspects such as increased size and expanded \nmodality coverage. The core architecture and fusion methodologies, however, remain largely consistent with \nearlier designs. For instance, while IDEFICS52 is a larger-scale model compared to Flamingo, its architecture is \nfundamentally identical to that of Flamingo. Similarly, Gemini53, a state-of-the-art multimodal model, handles \na variety of modalities-including image, text, audio, and video. While transformer layers are employed to \nfuse features across these modalities, there is no explicit mechanism to address the issue we have discussed. \nOur findings makes this observation explicit and general. By focusing on the difference in performance of \nVLM+LLMs and their corresponding VLMs on classification tasks we reveal VLM+LLMs’ limitations across \nmultiple datasets and models and then, we propose a solution to it.\nRouting, model selection & augmented LLMs\nModel selection is a standard problem in machine learning54 but is most commonly examined in the case where \nvalidation data is used to select the best single model by estimating test error for future i.i.d. test data. In our \ncase we instead use other datasets to learn a model which can dynamically select the best model for a given task, \nwhich we refer to as routing. Most similar to our work is concurrent work on routing for language benchmarks55, \nwhich has interesting links to meta-learning 56. Y et, the results of Shnitzer et al. 55 are limited to only natural \nlanguage tasks.\nMany tool augmented LLMs57–61, that is LLMs that can reference external tools, can be viewed as performing \nrouting to enable tasks beyond their initial training data. Several systems have been proposed to allow LLMs to \nreference external machine learning models however, these works are either aspirational in nature62, use synthetic \ndata itself generated by an LLM 31, or rely on prompt engineering and in-context learning 7,63. In contrast, our \nmethod for routing learns cost-effective strategies from accuracies obtained by running and evaluating models, \nwhich serve as training data for the LLM router.\nSome visual programming studies 63,64 have attempted to use LLMs to transform complex visual problems \ninto ones that are easier for VLMs to solve. These works demonstrate how LLMs can analyze and decompose \nquestions, enabling the completion of complex tasks by integrating components like object detectors, VLMs, \nVLM+LLMs, and other specialized systems, rather than relying exclusively on a single model. These studies do \nnot challenge the contribution we present in our paper-namely, that VLMs and VLM+LLMs possess distinct \nstrengths depending on the type of question. Consequently, incorporating VLMs and VLM+LLMs as an option, \nsuch as through a router, in these previous works remains a promising approach for enhancing performance.\nConclusions\nWe found that VLM+LLMs were better at advanced reasoning and knowledge tasks but were worse on object \nand scene recognition tasks when compared with VLMs with the same vision encoder. To address this issue, we \nproposed the use of a low-resource LLM that serves as a router, intelligently selecting the most suitable model \namong VLMs and VLM+LLMs for each visual task query. This LLM router strategy achieved higher accuracy \nthan other model selection and ensemble baselines, and is superior or on par with several other state-of-the-art \napproaches, like HuggingGPT and GPT-4Vision, which use far more computational resources. Our LLM router’s \neffectiveness is rooted in its specialized training on a corpus of examples, consisting of pairs of visual tasks and \nw/ MD w/o MD InD w/ MD\nw/ RO w/o RO w/ RO w/o RO w/o RO\nCIFAR-100 56.8 60.7 56.6 60.7 72.7\nOOD-CV 85.9 85.9 85.9 85.9 89.4\nWeather 85.2 84.1 86.4 73.9 75.0\nSkin cancer 48.7 48.7 48.7 47.4 48.7\nHateful Memes 49.0 49.7 49.9 50.4 53.7\nScienceQA 52.4 57.7 52.9 57.3 56.0\nVisual Genome Attr. 88.8 88.8 89.0 88.8 88.8\nVisual Genome Rel. 60.4 60.4 60.4 60.4 58.5\nAbstract scenes VQA 16.1 42.3 25.8 38.5 42.7\nBinary abstract scenes 58.5 58.6 58.5 58.8 58.5\nAverage 60.2 63.7 61.4 62.2 64.4\nAverage (Weighted) 57.3 64.9 59.9 63.9 68.6\nTable 4. Ablation Study. We experiment with different input formats for the LLM router testing with and \nwithout image metadata (MD) and with and without the set of possible response options (RO). The first four \ncolumns evaluate on held-out datasets, for comparison, we also show the in-distribution case (InD) where all \ndatasets are seen during training. Accuracy is averaged across prompt strategies.\n \nScientific Reports |        (2025) 15:19692 9| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nmodel accuracy. In contrast, competing approaches use proxies such as human-generated descriptions of the \nmodels.\nThe LLM router solution employs an LLM to address issues arising from the leveraging of LLMs to improve \nVLMs, i.e. an LLM is used to solve an issue that stems from another LLM. The effectiveness of the router solution \nindicates that LLMs alone have the potential to always augment the VLM capabilities across all visual tasks. \nTherefore, future investigations aimed at leveraging LLMs for VLMs should focus on identifying more effective \nstrategies for integrating them, as suggested by the success of our LLM router solution.\nLimitations\nThis study has a number of limitations that future work could build upon. First, since this paper focuses on \nrouting we limited the study to zero-shot model evaluation; it needs to be seen whether the conclusions of this \npaper extend beyond zero-shot regime. Also, while the models tested were state-of-the-art models at the time \nwhen we ran the experiments for this study, the field of AI advances very rapidly and new models have already \nsurpassed the capabilities of the models tested in this study. It is unclear if the conclusions of this study would \ngeneralize to newer models. Finally, we have only tested the router in held-out datasets, and thus, it is unclear \nhow users in practical applications would assess it.\nData availability\nThe datasets used and/or analysed during the current study available from the corresponding author on reason-\nable request. The detail is described in \"Appendix A\".\nAppendix A: Datasets\nWe utilize multiple datasets with distinct characteristics to evaluate the performance of our proposed model. \nThe datasets can be divided into two categories, tasks of Object & Scene Recognition (i.e., CIFAR-100, OOD-CV , \nWeather, and Skin Cancer) and those of Visual Reasoning & Outside Knowledge (i.e., Hateful Memes, ScienceQA, \nVisual Genome Attribution, Visual Genome Relation, Abstract Scenes VQA, and Binary Abstract Scenes.). \nWhen evaluating zero-shot model performance we use the test sets of the datasets to reduce the chance that \nthe models we evaluate have seen the images during training. We show example images from datasets in Fig. 5. \nOriginal images for each dataset can be accessed through the provided URL.\nCIFAR-100 The CIFAR-100 dataset14 consists of 60,000 32x32 pixel color images. Each image contains an \nobject from one of 100 classes. This dataset is widely utilized for general object recognition tasks. We adhere to \nthe same train and test split as originally proposed. The test split contains 10,000  i m a g e s .   h t t p s : / / w w w . c s . t o r o n t \no . e d u / ~ k r i z / c i f a r . h t m l      \nOOD-CV OOD-CV15 is a benchmark dataset introduced to enhance the evaluation of vision algorithms’ ro-\nbustness in real-world scenarios. The dataset includes out-of-distribution examples across 10 object categories, \nFig. 5. Example samples from the datasets used to evaluate VLMs and VLM+LLMs. Images in this figure come \nfrom the datasets described in \"Appendix A\", captions are the ground truth labels..\n \nScientific Reports |        (2025) 15:19692 10| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nwith variations in pose, shape, texture, context, and weather conditions. It enables benchmarking models on \ntasks including image classification, object detection, and 3D pose estimation. The dataset is composed of \nimages from PASCAL3D+ and additional images collected and annotated by the creators. We combine phase-1 \nand phase-2 for a total of 21,502 images in the test set. https://github.com/wufeim/OOD-CV-Data\nWeather The Multi-class Weather Dataset16 is designed to test classification capabilities on images of various \nweather conditions. It consists of a total of 1,125 images, categorized into four classes: Sunrise, Shine, Rain, and \nCloudy. The test split is 226 randomly sampled images (20% of the dataset).  h t t p s : / / d a t a . m e n d e l e y . c o m / d a t a s e t \ns / 4 d r t y f  t f y / 1      \nSkin cancer While prior studies have applied models like CLIP in the medical domain, specifically for diagnos-\ning conditions such as COVID-19 and pneumonia from chest X-rays65, these images differ significantly from \nvisible light RGB images. The skin cancer dataset17, which focuses on melanoma detection natural images, is \nan excellent task candidate. The dataset is maintained by the University of Waterloo. It consists of images ex-\ntracted from public databases DermIS and DermQuest, with response options healthy/cancerous and manual \ncropping to center the lesion in the image. Since this is the smallest dataset, we sample 95% of the dataset for \nevaluation, resulting in 196 images for testing. https://vip.uwaterloo.ca/skin-cancer-detection/\nHateful memes The task in the Hateful Memes dataset18 is to determine whether a meme is hateful or non-hate-\nful. The dataset is designed to enforce multimodailty — correctly classifying samples based on image or text \nalone is (essentially) impossible since individually the image or text may seem innocuous but combined they \nconvey a mean or hateful message. 10,000 memes (image with text written on top) are collected/generated. We \ncombine test_seen and test_unseen as test dataset, resulting in 3,000 images in total.  h t t p s : / / a i . m e t a . c o m / t o o l s / \nh a t e f u l m e m e s /      \nScienceQA The ScienceQA dataset19 is a benchmark consisting of approximately 21,000 multimodal, multi-\nple-choice questions spanning a diverse set of science topics, including abstract conceptual diagrams. Each \nquestion is annotated with corresponding lectures and explanations, providing a rich context for assessment. \nThe dataset is designed to test the multi-hop reasoning ability of AI systems. We exclude any samples from the \ntest split that do not contain images, resulting in 2,017 images. https://github.com/lupantech/ScienceQA\nVisual genome attribution & relation The Visual Genome Attribution & Relation dataset20 is crafted from the \nVisual Genome dataset66 and is a crucial part of the Attribution and Relation (AR) benchmark. This bench-\nmark is designed to assess the capabilities of vision and language models in understanding and processing \ncomplex relationships and attributes within images. The dataset is structured to challenge models with tasks \nsuch as determining the correct order in relations (e.g., “X relation Y” vs “Y relation X”) and accurately attrib-\nuting properties to objects. It includes 23,937 cases related to relations and 28,748 cases focused on attributes. \nWhen tested on this dataset, existing models have shown deficiencies in the complex reasoning required to \nsucceed at this task. The test split contains 5,736 images for Visual Genome Attribution and 4,753 images for \nVisual Genome Relation.  h t t p s :  / / h o m e  s . c s . w  a s h i n g  t o n . e d u / ~ r a n j a y / v i s u a l g e n o m e / a p i . h t m l\nAbstract scenes VQA and binary abstract scenes This study utilizes Abstract Scenes, a part of the VQA v1 da-\ntaset21, which we refer to it as Abstract Scenes VQA. The images are characterized by simplified, clip-art style \nrepresentations, which tests the generalization capabilities of VQA models. The tasks focuses on high-level \nsemantics in vision and language, ensuring that both are essential for accurate responses. We use 30,000 test \nimages for Abstract Scenes VQA and 11,327 test images for Binary Abstract Scenes.  h t t p s : / / v i s u a l q a . o r g / v q a _ v \n1 _ d o w n l o a d . h t m l      \nAppendix B: Prompt variations\nAs discussed in Sect.  2.4, varying the prompt had a significant impact on model performance.\nNote that we employ closed-ended evaluation in this study, as discussed in Sect.  2.2. Therefore, when we say \nwe prompt the model, what we mean is that we provide it with a set of text prompts that consists of a single \nquestion combined with each of the options (from the set of response options).\nPrompt grammar\nIn order to methodically explore a range of different prompts, we outlined a small grammar to compose prompts. \nThe grammar is as follows:\n• key: Datasets have several text fields, including the class name and additional text-context. One value that is \nused across datasets is class_name. When this value is used, n prompts are generated, where n is the number of \nclasses in the dataset (for ex, 100 in the case of CIFAR-100) and each prompt is set to one of the class names. \nSome datasets have dataset-specific keys\n• a_an: An argument that adds an a/an appropriately to the key\n• rename_classes: If the key text has a defined substitution, the key will be swapped for its rename\n• arg: Can be one of [a_an, rename_classes]\n• {[arg]* |+ key}: The tag is fully described by zero or multiple args, a  character (which can be omitted if no \nargs are specified) and the keyAdditional text can surround the tags.\nFurther, each dataset defines a set of questions and options. If a set of options is not defined, this means that \nthe options are provided by the dataset for each sample. A prompt is formed by concatenating a question to an \noption formulation. Sometimes we leave the question empty to test whether the class name alone is the best \nprompt. The cartesian product between all question and option formulations defines the full set of prompts. \nBelow we outline the prompts, options and additional text fields for each dataset.\nScientific Reports |        (2025) 15:19692 11| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nPrompts for each dataset\nCIFAR-100\n• Question Formulations\n – “”\n – “This is ”\n – “What is this? This is ”\n• Option Formulations\n – “{class_name}”\n – “{rename_classes  class_name}”\n – “{a_an, rename_classes  class_name}”\n• Class Renames\n – aquarium_fish: aquarium fish\n – pickup_truck: pickup truck\n – lawn_mower: lawn mower\n – sweet_pepper: pepper\n – maple_tree: maple\n – oak_tree: oak\n – palm_tree: palm\n – pine_tree: pine\n – willow_tree: willow\n• OOD-CV Has the same question and option formulations as CIFAR-100Class Renames\n – aeroplane: plane\n – diningtable: table\nWeather\n• Question Formulations\n – “”\n – “It is ”\n – “The weather is ”\n• Option Formulations\n – “{class_name}”\n – “{rename_classes  class_name}”\n• Class Renames\n – Sunrise: sunrise\nw/ MD w/o MD\nw/ RO w/o RO w/ RO w/o RO\nCIFAR100 72.7 72.7 71.5 71.5\nOODCV 89.1 89.4 92.0 92.0\nWeather 77.3 75.0 84.6 84.6\nSkin cancer 48.7 48.7 36.8 42.1\nHateful memes 55.0 53.7 60.0 60.0\nScienceQA 58.6 56.0 56.0 59.5\nVisualGenomeAttribution 91.6 88.8 94.5 94.5\nVisualGenomeRelation 58.5 58.5 61.0 60.5\nAbstractScenesVQA 42.3 42.7 46.5 45.0\nBinaryAbstractScenes 58.5 58.5 54.0 54.5\nAverage 65.2 64.4 65.7 66.4\nAverage (Weighted) 68.4 68.6 66.8 67.2\nTable 5. GPT-2: Ablation Study for the in-distribution case where all datasets are seen during training. We \nexperiment with different input formats for the LLM router testing with and without image metadata (MD) \nand with and without the set of possible response options (RO). Accuracy is averaged across prompt strategies.\n \nScientific Reports |        (2025) 15:19692 12| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\n – Cloudy: cloudy\n – Shine: sunny\n – Rain: rainy\nSkin Cancer\n• Question Formulations\n – “”\n – “This is ”\n – “This skin is ”\n• Option Formulations\n – “{class_name}”\n – “{rename_classes  class_name}”\n• Class Renames\n – melanoma: cancerous\n – notmelanoma: healthy\nHateful Memes\nFig. 7. GPT-2 Router, model selection across datasets and prompt strategies. Left: with metadata and without \nresponse options vs Right: without metadata and without response options.\n \nFig. 6. GPT-2 Router, model selection across datasets and prompt strategies. Left: with metadata and with \nresponse options vs Right: without metadata and with response options.\n \nScientific Reports |        (2025) 15:19692 13| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nFig. 9. LLaMA Router, model selection across datasets and prompt strategies. Left: with metadata and without \nresponse options vs right: without metadata and without response options.\n \nFig. 8. LLaMA Router, model selection across datasets and prompt strategies. Left: with metadata and with \nresponse options vs right: without metadata and with response options.\n \nw/ MD w/o MD\nw/ RO w/o RO w/ RO w/o RO\nCIFAR-100 52.5 64.5 58.8 66.1\nOOD-CV 89.4 84.7 82.4 88.0\nWeather 62.5 76.1 73.9 86.4\nSkin cancer 50.0 43.4 52.6 42.1\nHateful memes 50.8 51.5 48.8 49.5\nScienceQA 58.5 52.2 54.8 54.8\nVisual genome attribution 91.0 90.0 91.4 89.7\nVisual genome relation 62.1 63.1 61.5 61.5\nAbstract Scenes VQA 34.2 44.2 16.9 38.9\nBinary abstract scenes 54.8 60.5 62.5 61.1\nAverage 60.6 63.0 60.4 63.8\nAverage (Weighted) 62.4 65.8 57.1 65.9\nTable 6. Ablation Study for LLaMA (Held out). We experiment with different input formats for the LLM \nrouter testing with and without image metadata (MD) and with and without the set of possible response \noptions (RO). Like GPT-2 it’s evaluated on held-out datasets. Accuracy is averaged across prompt strategies.\n \nScientific Reports |        (2025) 15:19692 14| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nBelow, the key text is provided by the dataset for each sample. It is the text-string of the text written on the \nmeme.\n• Question Formulations\n – “”\n – “{text}. ”\n – “{text}. This meme is ”\n – “This is an image of a meme. It contains the text: {text}. The meme is ”\n• Option Formulations\n – “{class_name}”\n – “{rename_classes  class_name}”\n• Class Renames\n – not mean: nice\n – mean: mean\nScienceQA\nBelow, the keys class_question and class_hint are provided by the dataset for each sample. class_question is the \nquestion for each sample, while class_hint is some additional (unnecessary but helpful) context for each sample \nprovided by the dataset.\n• Question Formulations\n – “”\n – “{class_question} ”\n – “{class_hint} {class_question} ”\n – “Question: {class_hint} {class_question} ”\n – “{class_hint} Question: {class_question} ”\nVisual genome attribution & relation\nBelow, the key class_question is provided by the dataset for each sample. It is the question for each sample.\n• Question Formulations\n – “”\n – “{class_question} ”\n – “This best describes the image: ”\nAbstract scenes VQA and binary abstract scenes\nBelow, the key class_question is provided by the dataset for each sample. It is the question for each sample.\n• Question Formulations\n – “”\n – “Question: {class_question} Answer: ”\n – “Using the image, the answer to {class_question} is most likely ”\nPrompting PNP-VQA\nThe implementation of PNP-VQA requires a question to serve as the prompt, separately from the options. How-\never for several datasets, we tried prompts that have an empty question, denoted with “” . In these cases, we \nprovide the empty string to PNP-VQA, to ensure that evaluation is the same across models. However PNP-VQA \ncannot provide a response in these cases. For the experiments in Table 1 we use all the prompts described in this \nsection. When training the planner, to ensure all models have the same number of prompts and to avoid prompt/\nmodel dependence, we remove this question from further analysis for all models.\nAppendix C: Baseline implementation\nThis section describes how the baselines GPT-4V and HuggingGPT were implemented to create a fair compar-\nison with our LLM router.\nGPT-4V(ision)\nGPT-4V(ision) was released for developer access on 11/6/23. Our experiments took place between 11/13/23 and \n11/16/23. During this period GPT-4V was in preview, meaning organizations could only make 100 API requests \nper day, which limited the number of images we were able to test with it.\nFurther, though the API documentation provides for options that might be used to approximate close-ended \nevaluation, we were unable to make these parts of the API perform as documented for GPT4-V . We therefore \ntook an alternative approach to simulate closed-ended evaluation. We provided the following system prompt:\n“Complete the prompt from the list called OPTIONS, enclosed by [ and ]. Y our response can only include a \nScientific Reports |        (2025) 15:19692 15| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nsingle element from this list”\nand in the user message, we provided the prompt and the closed set of response options within brackets. For \neach dataset we pick the prompt that is valid for all models and achieved the highest accuracy in the experi-\nments to create Table 1.\nTo evaluate GPT-4V each completion was determined to be one of:\n• Correct Prediction: if the response of the model contained only one element from OPTIONS and it was the \ncorrect label.\n• Incorrect Prediction: if the response of the model contained only one element from OPTIONS and it was an \nincorrect label.\n• No Guess: if the response contained no elements from OPTIONS\n• Multiple Guesses: if the response contained multiple elements from OPTIONSTo calculate the accuracies in \nTable 3 we considered Incorrect Predictions, No Guess and Multiple Guesses as errors. This means the accuracy \nwas calculated as the number of correct predictions divided by the total number of samples.\nTo more accurately measure the performance of GPT-4V we allow ten completions from the model. We select \nthe most common prediction (correct or incorrect) from GPT-4V as the final prediction, only selecting No \nGuess or Multiple Guesses if all of the ten completions fail to make a valid prediction. This approximates choos-\ning the most probable option from the closed set.\nHuggingGPT\nTo run HuggingGPT we use the code made publicly available at https://github.com/microsoft/JARVIS, the code \nwas pulled on 10/19/23, HuggingGPT uses text-davinci-003 as the LLM. Due to making multiple requests with \nlong context prompts, we found HuggingGPT cost approximately $0.05 per-sample, or around $20 to calculate \nresults on the small subset of data in Table 3 (excluding experimentation and development requests). We evalu-\nated HuggingGPT in the same manner as GPT-4V , but calculated only one completion per sample due to these \ncosts.\nFor the most part we used the default settings provided in the repository, making the following changes to the \nprompting to encourage the system to select from the set of valid response options.\n – System Prompt:Original: “#4 Response Generation Stage: With the task execution logs, the AI assistant \nneeds to describe the process and inference results. ”\n – Ours: “#4 Response Generation Stage: With the task execution logs, the AI assistant needs to provide the \nbest completion for the prompt provided by the user. ”\n – User Message:Original: “Y es. Please first think carefully and directly answer my request based on the infer-\nence results. Some of the inferences may not always turn out to be correct and require you to make care -\nful consideration in making decisions. Then please detail your workflow including the used models and \ninference results for my request in your friendly tone. Please filter out information that is not relevant to \nmy request. Tell me the complete path or urls of files in inference results. If there is nothing in the results, \nplease tell me you can’t make it. ”\n – Ours: “Y es. Please complete the prompt from the list called OPTIONS, enclosed by [ and ]. Y our response \ncan only include a single element from this list. ”The user message is then additionally given the instruction:\n“Using the image and the options provided please complete the following prompt. {prompt}... OPTIONS: {re-\nsponse options}” , where {prompt} and {response options} are replaced with the relevant prompt and response \noptions for the given sample. As with GPT-4V , for each dataset we pick the prompt that is valid for all models \nand achieved the highest accuracy in the experiments to create Table 1.\nWe also found HuggingGPT was rarely able to actually access and run models from the Hugging Face Hub. \nDue to this limitation and to improve reproducibility we ran HuggingGPT in local mode, downloading all \nrelevant implemented models. The models available locally to HuggingGPT were downloaded on 11/11/23. The \nfull list of models we made available to HuggingGPT is:\n• nlpconnect/vit-gpt2-image-captioning\n• lllyasviel/ControlNet\n• lllyasviel/sd-controlnet-canny\n• lllyasviel/sd-controlnet-depth\n• lllyasviel/sd-controlnet-hed\n• lllyasviel/sd-controlnet-mlsd\n• lllyasviel/sd-controlnet-openpose\n• lllyasviel/sd-controlnet-scribble\n• lllyasviel/sd-controlnet-seg\n• runwayml/stable-diffusion-v1-5\n• damo-vilab/text-to-video-ms-1.7b\n• microsoft/speecht5_asr\n• JorisCos/DCCRNet_Libri1Mix_enhsingle_16k\n• espnet/kan-bayashi_ljspeech_vits\n• facebook/detr-resnet-101\n• microsoft/speecht5_hifigan\n• microsoft/speecht5_vc\nScientific Reports |        (2025) 15:19692 16| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\n• openai/whisper-base\n• Intel/dpt-large\n• facebook/detr-resnet-50-panoptic\n• facebook/detr-resnet-50\n• google/owlvit-base-patch32\n• impira/layoutlm-document-qa\n• ydshieh/vit-gpt2-coco-en\n• dandelin/vilt-b32-finetuned-vqa\n• lambdalabs/sd-image-variations-diffusers\n• facebook/maskformer-swin-base-coco\n• Intel/dpt-hybrid-midas\n• Salesforce/blip-image-captioning-large\n• facebook/maskformer-swin-large-ade\n• microsoft/beit-base-patch16-224-pt22k-ft22k\n• openai/clip-vit-large-patch14\n• deepset/roberta-base-squad2\n• google/tapas-base-finetuned-wtq\n• distilbert-base-uncased-finetuned-sst-2-english\n• gpt2\n• Dataset: Matthijs/cmu-arctic-xvectors\nAppendix D: Implementation detail\nIn this section, we provide additional implementation details for evaluations and experiments.\nClosed-ended evaluation\nIn Sect. 2.2, we motivated our use of closed-ended evaluation. As mentioned, the implementation of closed-end-\ned evaluation differed across models. Here we outline the implementation in various model architectures. This \nevaluation is computed for each sample in the dataset, where each sample is an image and the corresponding set \nof text prompts (o ∈ O): question and response options.\nConstrastive models\nThis group includes CLIP , BLIP , and LiT. The model has two different encoders, one for images and one for \ntext. For each sample, we extract the N-dimensional image embedding, vi ∈ RN , and the text embedding \nmatrix, VO ∈ R|O|×N, where rows of VO are the N-dimensional embedding of the prompts o ∈ O. We then \ntake the prediction to be the option indexed by argmax(vT\ni · VO).\nLanguage models\nThis group includes PNP-VQA, and Flamingo. Models of this type can return a log-probability for the next \ntoken, across all tokens in the vocabulary, given the preceding tokens. These probabilities are generally used for \ngenerating new text, but they can also be used for closed-ended evaluation. For each prompt, o ∈ O, we sum \nthe log probabilities for each of the tokens in the prompt. The prediction is then the response option used to \ncreate the prompt which has the highest probability.\nData format\nSection 3.1 describes the creation of training data for the LLM router. After this process a single sample in the \ndataset looks like\nThe values following the [img] marker are the resolution of the image along with per-channel mean and stand-\nard deviation of the pixel values. Then the prompt that should be completed is provided along with the set of \nresponse options (a car, a sofa etc.). These components are the possible inputs to the router that we experiment \nwith. After the [SEP] token is a label for the model (CLIP). The final part is the boolean that marks whether \nor not the model gave the correct response for the image in questions as well as the average accuracy for the \nmodel on the dataset the image is from. The boolean and the average accuracy are used to select the best model \nfor each image which is then used to train the LLM router as described in Sect. 3.1.\nGPT-2 implementation\nFor GPT-2, we used the pre-trained model from https://huggingface.co/gpt2 The optimizer was Adam with an \ninitial learning rate of 2 × 10−4. The batch size was 1, we trained for 5000 iterations, after every 1000 iterations, \nan evaluation was done and the checkpoint with the best result was used as the final model.\nScientific Reports |        (2025) 15:19692 17| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nLLaMA implementation\nFor LLaMA, we used a pre-trained model from  h t t p s :   /  / h u g g i n g f a c  e . c  o /  b a ff   o   3 2 / d e c  a p o  d a  - r e s e a   r c h - l l  a m  a - 7 B - h f. \nLoRA with rank 8 and model precision of fp16 were applied to save computational cost. We optimized models \nwith the AdamW optimizer with an initial learning rate of 1 × 10−3. The batch size was 2. we trained for 500 \niterations, an evaluation was done every 100 iterations and the checkpoint with the best result was used as the \nfinal model.\nAppendix E: Further results\nFurther ablation studies for GPT-2\nFirst, we examine the results of training the planner on all datasets and evaluating on unseen samples from those \ndatasets, see Table 5. This is slightly different from holding out individual datasets as was done in the main text, \ninstead this is ‘in-distribution’ evaluation. When compared with Table  4 we see that the router is only slightly \nbetter in this case despite being evaluated in-distribution. This shows that the LLM router described in the main \ntext is able to generalize well to held-out datasets.\nWe also include heatmaps for the GPT-2 Router performance on held-out datasets with and without metadata \nand with and without response options, see Figs. 6, 7. Conclusions are the same as in the ablation study in \nTable 4. Note that routers trained with response options are more likely to choose an inferior model type (VLM \nor VLM+LLM) for a given dataset type (classification or reasoning). Instead, relying only on the prompts \nseems to make it easier for the router to determine the task type.\nNext, we compare the routers trained with and without metadata, and without the response options. The router \nalways selects CLIP as the best model when trained with metadata (Fig. 7, left), but when trained without \nmetadata the router chooses CLIP and BLIP with equal probability (Fig. 7, right). Since CLIP is significantly \nbetter than BLIP on the Weather dataset, the router trained with metadata achieves higher performance. In the \ntraining data (Fig. 4, right), BLIP is selected by the router relatively often for the OOD-CV and Skin Cancer \ndatasets. Since the images from Weather and Skin Cancer datasets look very different, it is likely they can be \ndistinguished even with the simple metadata provided to the router, and therefore the router may learn to \navoid using BLIP for such visually different images.\nLLaMa ablation\nWe also include an ablation and heatmaps for the LLaMA router performance on held-out datasets with and \nwithout metadata and with and without response options, see Table 6 and Figs. 8, 9. Similarly to the GPT-2 \nrouter, the LLaMA router trained without response options performed better. On the other hand, when com-\nparing performance with and without metadata, training without metadata achieves marginally higher average \naccuracy. As with GPT-2, by examining the heatmaps (Figs. 8, 9), we see that including response options tends \nto lead to less optimal model selection and hence lower performance.\nReceived: 28 October 2024; Accepted: 27 May 2025\nReferences\n 1. Radford, A. et al. Learning transferable visual models from natural language supervision. In International Conference on Machine \nLearning, 8748–8763 PMLR, (2021).\n 2. Li, J., Li, D., Xiong, C. & Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and \ngeneration. In International Conference on Machine Learning, 12888–12900 PMLR, (2022).\n 3. Zhai, X. et al. LiT: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on Computer \nVision and Pattern Recognition (CVPR), 18123–18133 (2022).\n 4. Tiong, A. M. H., Li, J., Li, B., Savarese, S. & Hoi, S. C. Plug-and-play VQA: Zero-shot VQA by conjoining large pretrained models \nwith zero training. In Goldberg, Y ., Kozareva, Z. & Zhang, Y . (eds.) Findings of the Association for Computational Linguistics: \nEMNLP 2022 , 951–967, https://doi.org/10.18653/v1/2022.findings-emnlp.67 Association for Computational Linguistics, Abu \nDhabi, United Arab Emirates, (2022).\n 5. Alayrac, J.-B. et al. Flamingo: a visual language model for few-shot learning. Adv. Neural. Inf. Process. Syst. 35, 23716–23736 (2022).\n 6. Chen, X. et al. Pali: A jointly-scaled multilingual language-image model. In The Eleventh International Conference on Learning \nRepresentations (2023).\n 7. Shen, Y . et al. Hugginggpt: Solving AI tasks with chatgpt and its friends in hugging face. arXiv preprint arXiv:2303.17580 (2023).\n 8. OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023).\n 9. Awadalla, A. et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv \npreprint arXiv:2308.01390 (2023).\n 10. Team, MN. Introducing mpt-7b: A new standard for open-source, commercially usable llms (2023). Accessed: 2023-05-05.\n 11. Khashabi, D. et al. Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700 (2020).\n 12. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. \narXiv preprint arXiv:1810.04805 (2018).\n 13. Brown, T. et al. Language models are few-shot learners. Adv. Neural. Inf. Process. Syst. 33, 1877–1901 (2020).\n 14. Krizhevsky, A., Hinton, G. et al. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/  k r i z / l e a r n i n g - f e \na t u r e s - 2 0 0 9 - T R . p d f  (2009).\n 15. Zhao, B. et al. OOD-CV: a benchmark for robustness to out-of-distribution shifts of individual nuisances in natural images. In \nEuropean Conference on Computer Vision, 163–180 Springer, (2022).\n 16. Ajayi, G. Multi-class weather dataset for image classification. Mendeley Data (2018). V1.\n 17. Vision & Lab, I. P . Skin cancer detection. University of Waterloo (Available).\n 18. Kiela, D. et al. The hateful memes challenge: Detecting hate speech in multimodal memes. Adv. Neural. Inf. Process. Syst.  33, \n2611–2624 (2020).\nScientific Reports |        (2025) 15:19692 18| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\n 19. Lu, P . et al. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Koyejo, S. et al. (eds.) \nAdvances in Neural Information Processing Systems, vol. 35, 2507–2521 Curran Associates, Inc., (2022).\n 20. Yuksekgonul, M., Bianchi, F ., Kalluri, P ., Jurafsky, D. & Zou, J. When and why vision-language models behave like bags-of-words, \nand what to do about it? In The Eleventh International Conference on Learning Representations (2023).\n 21. Antol, S. et al. VQA: Visual Question Answering. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) \n(2015).\n 22. Zhang, P ., Goyal, Y ., Summers-Stay, D., Batra, D. & Parikh, D. Yin and Y ang: Balancing and answering binary visual questions. In \nConference on Computer Vision and Pattern Recognition (CVPR) (2016).\n 23. Reynolds, L. & McDonell, K. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended \nAbstracts of the 2021 CHI Conference on Human Factors in Computing Systems, 1–7 (2021).\n 24. Zhou, Y . et al. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning \nRepresentations (2023).\n 25. Y ang, Z. et al. An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on Artificial \nIntelligence 36, 3081–3089 (2022).\n 26. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y . & Iwasawa, Y . Large language models are zero-shot reasoners. Adv. Neural. Inf. Process. \nSyst. 35, 22199–22213 (2022).\n 27. Wang, X. et al. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).\n 28. Pitis, S., Zhang, M. R., Wang, A. & Ba, J. Boosted prompt ensembles for large language models. arXiv preprint arXiv:2304.05970 \n(2023).\n 29. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. Adv. Neural. Inf. Process. Syst. 35, 24824–\n24837 (2022).\n 30. Y ao, S. et al. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601 (2023).\n 31. Patil, S. G., Zhang, T., Wang, X. & Gonzalez, J. E. Gorilla: Large language model connected with massive apis. arXiv preprint \narXiv:2305.15334 (2023).\n 32. Radford, A. et al. Language models are unsupervised multitask learners. OpenAI blog (2019).\n 33. Touvron, H. et al. Llama: Open and efficient foundation language models (2023). arXiv:2302.13971.\n 34. Wolf, T. et al.  Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical \nMethods in Natural Language Processing: System Demonstrations , 38–45 Association for Computational Linguistics, Online, \n(2020).\n 35. OpenAI. GPT-4V(ision) system card. OpenAI blog (2023).\n 36. Kaplan, J. et al. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n 37. Zhai, X., Kolesnikov, A., Houlsby, N. & Beyer, L. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition (CVPR), 12104–12113 (2022).\n 38. Bommasani, R. et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).\n 39. Kirillov, A. et al. Segment anything. arXiv:2304.02643 (2023).\n 40. Zou, X. et al. Segment everything everywhere all at once. arXiv preprint arXiv:2304.06718 (2023).\n 41. Rombach, R., Blattmann, A., Lorenz, D., Esser, P . & Ommer, B. High-resolution image synthesis with latent diffusion models \n(2021). arXiv:2112.10752.\n 42. Chowdhery, A. et al. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res. 24, 1–113 (2023).\n 43. Touvron, H. et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n 44. Zhu, D., Chen, J., Shen, X., Li, X. & Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large \nlanguage models. arXiv preprint arXiv:2304.10592 (2023).\n 45. Li, J., Li, D., Savarese, S. & Hoi, S. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large \nlanguage models. In ICML (2023).\n 46. Driess, D. et al. Palm-e: An embodied multimodal language model. In arXiv preprint arXiv:2303.03378 (2023).\n 47. Vaswani, A. et al. Attention is all you need. Advances in neural information processing systems30 (2017).\n 48. Bubeck, S. et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023).\n 49. Huang, S. et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045 (2023).\n 50. Roth, K. et al. Waffling around for performance: Visual classification with random words and broad concepts. arXiv preprint \narXiv:2306.07282 (2023).\n 51. Zhang, Y ., McKinzie, B., Shankar, V ., Gan, Z. & Toshev, A. Pre-trained language models do not help auto-regressive text-to-image \ngeneration. In NeurIPS Workshop. I Can’t Believe It’s Not Better (ICBINB): Failure Modes in the Age of Foundation Models (2023).\n 52. Laurencon, H. et al.  Obelics: An open web-scale filtered dataset of interleaved image-text documents. arXiv preprint \narXiv:2306.16527 (2023).\n 53. Gemini Team, R. A. e. a. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2024).\n 54. Raschka, S. Model evaluation, model selection, and algorithm selection in machine learning. arXiv preprint arXiv:1811.12808 \n(2018).\n 55. Shnitzer, T. et al. Large language model routing with benchmark datasets (2023). arXiv:2309.15789.\n 56. Vilalta, R. & Drissi, Y . A perspective view and survey of meta-learning. Artif. Intell. Rev. 18, 77–95 (2002).\n 57. Parisi, A., Zhao, Y . & Fiedel, N. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255 (2022).\n 58. Schick, T. et al. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 (2023).\n 59. Mialon, G. et al. Augmented language models: a survey. Transactions on Machine Learning Research (2023). Survey Certification.\n 60. Thoppilan, R. et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022).\n 61. Surís, D., Menon, S. & Vondrick, C. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128 \n(2023).\n 62. Liang, Y . et al.  Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. arXiv preprint \narXiv:2303.16434 (2023).\n 63. Gupta, T. & Kembhavi, A. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition (CVPR), 14953–14962 (2023).\n 64. Lu, P . et al.  Chameleon: Plug-and-play compositional reasoning with large language models. Advances in Neural Information \nProcessing Systems36 (2024).\n 65. Wang, Z., Wu, Z., Agarwal, D. & Sun, J. Medclip: Contrastive learning from unpaired medical images and text. arXiv preprint \narXiv:2210.10163 (2022).\n 66. Krishna, R. et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. \nVision 123, 32–73 (2017).\nAuthor contributions\nA. Cooper, K. Kato, C. Shih, I. Mason and X. Boix conceptualized the research with contributions from K. \nTakemoto, H. Y eh and H. Y amane; A. Cooper, C. Shih and I. Mason wrote teh code and ran the experiments in \nSect. 2; K. Kato and I. Mason wrote the code and run the experiments in Section 3; K. Takemoto, T. Sunagawa, H. \nY eh and J. Y amanaka and H. Y amane contributed to the code to run the experiments; H. Y amane and K. Vinken \nScientific Reports |        (2025) 15:19692 19| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/\nanalyzed and visualized the data, with contributions from I. Mason and A. Cooper; H. Y amane imported the \ndatasets with contributions from K. Takemoto; A. Cooper, K. Kato, C. Shih, I. Mason, K. Vinken and X. Boix \nwrote the paper with contributions from K. Takemoto and H. Y amane, H. Y eh; X. Boix ideated and supervised \nthe research with contributions of I. Mason.\nDeclarations\nCompeting interest\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to K.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:19692 20| https://doi.org/10.1038/s41598-025-04384-8\nwww.nature.com/scientificreports/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4284428656101227
    },
    {
      "name": "Data science",
      "score": 0.34662920236587524
    },
    {
      "name": "Medicine",
      "score": 0.32433027029037476
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210094759",
      "name": "Fujitsu (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2252096349",
      "name": "Fujitsu (Japan)",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ],
  "cited_by": 3
}