{
    "title": "Hierarchical Transformers for Multi-Document Summarization",
    "url": "https://openalex.org/W2947881255",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2102234800",
            "name": "Liu Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221535453",
            "name": "Lapata, Mirella",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962849707",
        "https://openalex.org/W2963768805",
        "https://openalex.org/W2791751435",
        "https://openalex.org/W2963047186",
        "https://openalex.org/W2913407944",
        "https://openalex.org/W2270627573",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2025475564",
        "https://openalex.org/W2012561700",
        "https://openalex.org/W2586050494",
        "https://openalex.org/W2963899396",
        "https://openalex.org/W2964144561",
        "https://openalex.org/W2036484499",
        "https://openalex.org/W2146502635",
        "https://openalex.org/W3158986179",
        "https://openalex.org/W2886540288",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W2963045354",
        "https://openalex.org/W2963015915",
        "https://openalex.org/W2577896103",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2026191715",
        "https://openalex.org/W2141514700",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2612675303",
        "https://openalex.org/W2963241389",
        "https://openalex.org/W2889518897",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2885185669",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2963385935",
        "https://openalex.org/W3101913037",
        "https://openalex.org/W2962972512",
        "https://openalex.org/W2902706465",
        "https://openalex.org/W2896919131",
        "https://openalex.org/W2890627034"
    ],
    "abstract": "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.",
    "full_text": "Hierarchical Transformers for Multi-Document Summarization\nYang Liu and Mirella Lapata\nInstitute for Language, Cognition and Computation\nSchool of Informatics, University of Edinburgh\nyang.liu2@ed.ac.uk, mlap@inf.ed.ac.uk\nAbstract\nIn this paper, we develop a neural summa-\nrization model which can effectively process\nmultiple input documents and distill abstrac-\ntive summaries. Our model augments a previ-\nously proposed Transformer architecture (Liu\net al., 2018) with the ability to encode docu-\nments in a hierarchical manner. We represent\ncross-document relationships via an attention\nmechanism which allows to share information\nas opposed to simply concatenating text spans\nand processing them as a ﬂat sequence. Our\nmodel learns latent dependencies among tex-\ntual units, but can also take advantage of ex-\nplicit graph representations focusing on simi-\nlarity or discourse relations. Empirical results\non the WikiSum dataset demonstrate that the\nproposed architecture brings substantial im-\nprovements over several strong baselines.1\n1 Introduction\nAutomatic summarization has enjoyed renewed\ninterest in recent years, thanks to the popular-\nity of neural network models and their ability to\nlearn continuous representations without recourse\nto preprocessing tools or linguistic annotations.\nThe availability of large-scale datasets (Sandhaus,\n2008; Hermann et al., 2015; Grusky et al., 2018)\ncontaining hundreds of thousands of document-\nsummary pairs has driven the development of\nneural architectures for summarizing single doc-\numents. Several approaches have shown promis-\ning results with sequence-to-sequence models that\nencode a source document and then decode it into\nan abstractive summary (See et al., 2017; Celiky-\nilmaz et al., 2018; Paulus et al., 2018; Gehrmann\net al., 2018).\nMulti-document summarization — the task of\nproducing summaries from clusters of themati-\n1Our code and data is available at https://github.\ncom/nlpyang/hiersumm.\ncally related documents — has received signif-\nicantly less attention, partly due to the paucity\nof suitable data for the application of learning\nmethods. High-quality multi-document summa-\nrization datasets (i.e., document clusters paired\nwith multiple reference summaries written by hu-\nmans) have been produced for the Document Un-\nderstanding and Text Analysis Conferences (DUC\nand TAC), but are relatively small (in the range\nof a few hundred examples) for training neu-\nral models. In an attempt to drive research fur-\nther, Liu et al. (2018) tap into the potential of\nWikipedia and propose a methodology for cre-\nating a large-scale dataset (WikiSum) for multi-\ndocument summarization with hundreds of thou-\nsands of instances. Wikipedia articles, speciﬁcally\nlead sections, are viewed as summaries of various\ntopics indicated by their title, e.g.,“Florence” or\n“Natural Language Processing”. Documents cited\nin the Wikipedia articles or web pages returned\nby Google (using the section titles as queries) are\nseen as the source cluster which the lead section\npurports to summarize.\nAside from the difﬁculties in obtaining train-\ning data, a major obstacle to the application of\nend-to-end models to multi-document summariza-\ntion is the sheer size and number of source doc-\numents which can be very large. As a result, it\nis practically infeasible (given memory limitations\nof current hardware) to train a model which en-\ncodes all of them into vectors and subsequently\ngenerates a summary from them. Liu et al. (2018)\npropose a two-stage architecture, where anextrac-\ntive model ﬁrst selects a subset of salient passages,\nand subsequently an abstractive model generates\nthe summary while conditioning on the extracted\nsubset. The selected passages are concatenated\ninto a ﬂat sequence and the Transformer (Vaswani\net al., 2017), an architecture well-suited to lan-\nguage modeling over long sequences, is used to\narXiv:1905.13164v1  [cs.CL]  30 May 2019\ndecode the summary.\nAlthough the model of Liu et al. (2018) takes\nan important ﬁrst step towards abstractive multi-\ndocument summarization, it still considers the\nmultiple input documents as a concatenated ﬂat\nsequence, being agnostic of the hierarchical struc-\ntures and the relations that might exist among doc-\numents. For example, different web pages might\nrepeat the same content, include additional con-\ntent, present contradictory information, or discuss\nthe same fact in a different light (Radev, 2000).\nThe realization that cross-document links are im-\nportant in isolating salient information, elimi-\nnating redundancy, and creating overall coherent\nsummaries, has led to the widespread adoption\nof graph-based models for multi-document sum-\nmarization (Erkan and Radev, 2004; Christensen\net al., 2013; Wan, 2008; Parveen and Strube,\n2014). Graphs conveniently capture the relation-\nships between textual units within a document col-\nlection and can be easily constructed under the as-\nsumption that text spans represent graph nodes and\nedges are semantic links between them.\nIn this paper, we develop a neural summariza-\ntion model which can effectively process multi-\nple input documents and distill abstractive sum-\nmaries. Our model augments the previously pro-\nposed Transformer architecture with the ability to\nencode multiple documents in a hierarchical man-\nner. We represent cross-document relationships\nvia an attention mechanism which allows to share\ninformation across multiple documents as opposed\nto simply concatenating text spans and feeding\nthem as a ﬂat sequence to the model. In this\nway, the model automatically learns richer struc-\ntural dependencies among textual units, thus in-\ncorporating well-established insights from earlier\nwork. Advantageously, the proposed architecture\ncan easily beneﬁt from information external to the\nmodel, i.e., by replacing inter-document attention\nwith a graph-matrix computed based on the basis\nof lexical similarity (Erkan and Radev, 2004) or\ndiscourse relations (Christensen et al., 2013).\nWe evaluate our model on the WikiSum dataset\nand show experimentally that the proposed archi-\ntecture brings substantial improvements over sev-\neral strong baselines. We also ﬁnd that the ad-\ndition of a simple ranking module which scores\ndocuments based on their usefulness for the target\nsummary can greatly boost the performance of a\nmulti-document summarization system.\n2 Related Work\nMost previous multi-document summarization\nmethods are extractive operating over graph-based\nrepresentations of sentences or passages. Ap-\nproaches vary depending on how edge weights\nare computed e.g., based on cosine similarity with\ntf-idf weights for words (Erkan and Radev, 2004)\nor on discourse relations (Christensen et al., 2013),\nand the speciﬁc algorithm adopted for ranking text\nunits for inclusion in the ﬁnal summary. Sev-\neral variants of the PageRank algorithm have been\nadopted in the literature (Erkan and Radev, 2004)\nin order to compute the importance or salience of\na passage recursively based on the entire graph.\nMore recently, Yasunaga et al. (2017) propose a\nneural version of this framework, where salience\nis estimated using features extracted from sen-\ntence embeddings and graph convolutional net-\nworks (Kipf and Welling, 2017) applied over the\nrelation graph representing cross-document links.\nAbstractive approaches have met with limited\nsuccess. A few systems generate summaries\nbased on sentence fusion, a technique which iden-\ntiﬁes fragments conveying common information\nacross documents and combines these into sen-\ntences (Barzilay and McKeown, 2005; Filippova\nand Strube, 2008; Bing et al., 2015). Although\nneural abstractive models have achieved promis-\ning results on single-document summarization\n(See et al., 2017; Paulus et al., 2018; Gehrmann\net al., 2018; Celikyilmaz et al., 2018), the ex-\ntension of sequence-to-sequence architectures to\nmulti-document summarization is less straightfor-\nward. Apart from the lack of sufﬁcient training\ndata, neural models also face the computational\nchallenge of processing multiple source docu-\nments. Previous solutions include model trans-\nfer (Zhang et al., 2018; Lebanoff and Liu, 2018),\nwhere a sequence-to-sequence model is pretrained\non single-document summarization data and ﬁne-\ntuned on DUC (multi-document) benchmarks, or\nunsupervised models relying on reconstruction ob-\njectives (Ma et al., 2016; Chu and Liu, 2018).\nLiu et al. (2018) propose a methodology for\nconstructing large-scale summarization datasets\nand a two-stage model which ﬁrst extracts salient\ninformation from source documents and then uses\na decoder-only architecture (that can attend to very\nlong sequences) to generate the summary. We fol-\nlow their setup in viewing multi-document sum-\nmarization as a supervised machine learning prob-\n............\nranked paragraphs\nsource paragraphs\nparagraph ranker encoder\npara 1\npara L         \npara L\ndecoder\nabstractive summarizer\ntarget summary\nFigure 1: Pipeline of our multi-document summariza-\ntion system. Lsource paragraphs are ﬁrst ranked and\nthe L′-best ones serve as input to an encoder-decoder\nmodel which generates the target summary.\nlem and for this purpose assume access to large,\nlabeled datasets (i.e., source documents-summary\npairs). In contrast to their approach, we use a\nlearning-based ranker and our abstractive model\ncan hierarchically encode the input documents,\nwith the ability to learn latent relations across doc-\numents and additionally incorporate information\nencoded in well-known graph representations.\n3 Model Description\nWe follow Liu et al. (2018) in treating the gen-\neration of lead Wikipedia sections as a multi-\ndocument summarization task. The input to a hy-\npothetical system is the title of a Wikipedia arti-\ncle and a collection of source documents, while\nthe output is the Wikipedia article’s ﬁrst section.\nSource documents are webpages cited in the Ref-\nerences section of the Wikipedia article and the\ntop 10 search results returned by Google (with\nthe title of the article as the query). Since source\ndocuments could be relatively long, they are split\ninto multiple paragraphs by line-breaks. More\nformally, given title T, and L input paragraphs\n{P1,··· ,PL}(retrieved from Wikipedia citations\nand a search engine), the task is to generate the\nlead section Dof the Wikipedia article.\nOur summarization system is illustrated in Fig-\nure 1. Since the input paragraphs are numerous\nand possibly lengthy, instead of directly applying\nan abstractive system, we ﬁrst rank them and sum-\nmarize the L′-best ones. Our summarizer follows\nthe very successful encoder-decoder architecture\n(Bahdanau et al., 2015), where the encoder en-\ncodes the input text into hidden representations\nand the decoder generates target summaries based\non these representations. In this paper, we focus\nexclusively on the encoder part of the model, our\ndecoder follows the Transformer architecture in-\ntroduced in Vaswani et al. (2017); it generates a\nsummary token by token while attending to the\nsource input. We also use beam search and a\nlength penalty (Wu et al., 2016) in the decoding\nprocess to generate more ﬂuent and longer sum-\nmaries.\n3.1 Paragraph Ranking\nUnlike Liu et al. (2018) who rank paragraphs\nbased on their similarity with the title (using tf-idf-\nbased cosine similarity), we adopt a learning-\nbased approach. A logistic regression model is\napplied to each paragraph to calculate a score in-\ndicating whether it should be selected for summa-\nrization. We use two recurrent neural networks\nwith Long-Short Term Memory units (LSTM;\nHochreiter and Schmidhuber 1997) to represent ti-\ntle T and source paragraph P:\n{ut1,··· ,utm}= lstmt({wt1,··· ,wtm}) (1)\n{up1,··· ,upn}= lstmp({wp1,··· ,wpn}) (2)\nwhere wti,wpj are word embeddings for tokens in\nT and P, and uti,upj are the updated vectors for\neach token after applying the LSTMs.\nA max-pooling operation is then used over title\nvectors to obtain a ﬁxed-length representation ˆut:\nˆut = maxpool({ut1,··· ,utm}) (3)\nWe concatenate ˆut with the vector upi of each to-\nken in the paragraph and apply a non-linear trans-\nformation to extract features for matching the title\nand the paragraph. A second max-pooling opera-\ntion yields the ﬁnal paragraph vector ˆp:\npi = tanh(W1([upi; ˆut])) (4)\nˆp= maxpool({p1,··· ,pn}) (5)\nFinally, to estimate whether a paragraph should be\nselected, we use a linear transformation and a sig-\nmoid function:\ns= sigmoid(W2 ˆ(p)) (6)\nwhere s is the score indicating whether para-\ngraph P should be used for summarization.\nAll input paragraphs {P1,··· ,PL} receive\nscores {s1,··· ,sL}. The model is trained by\nminimizing the cross entropy loss between si and\nground-truth scores yi denoting the relatedness of\na paragraph to the gold standard summary. We\nadopt ROUGE-2 recall (of paragraph Pi against\ngold target text D) as yi. In testing, input para-\ngraphs are ranked based on the model predicted\nscores and an ordering {R1,··· ,RL}is gener-\nated. The ﬁrst L′paragraphs {R1,··· ,RL′ }are\nselected as input to the second abstractive stage.\n3.2 Paragraph Encoding\nInstead of treating the selected paragraphs as\na very long sequence, we develop a hierarchi-\ncal model based on the Transformer architecture\n(Vaswani et al., 2017) to capture inter-paragraph\nrelations. The model is composed of several lo-\ncal and global transformer layers which can be\nstacked freely. Let tij denote the j-th token in the\ni-th ranked paragraph Ri; the model takes vectors\nx0\nij (for all tokens) as input. For the l-th trans-\nformer layer, the input will bexl−1\nij , and the output\nis written as xl\nij.\n3.2.1 Embeddings\nInput tokens are ﬁrst represented by word embed-\ndings. Let wij ∈ Rd denote the embedding as-\nsigned to tij. Since the Transformer is a non-\nrecurrent model, we also assign a special posi-\ntional embedding peij to tij, to indicate the po-\nsition of the token within the input.\nTo calculate positional embeddings, we follow\nVaswani et al. (2017) and use sine and cosine func-\ntions of different frequencies. The embedding ep\nfor the p-th element in a sequence is:\nep[i] = sin(p/100002i/d) (7)\nep[2i+ 1] = cos(p/100002i/d) (8)\nwhere ep[i] indicates the i-th dimension of the em-\nbedding vector. Because each dimension of the\npositional encoding corresponds to a sinusoid, for\nany ﬁxed offset o, ep+o can be represented as a\nlinear function of ep, which enables the model to\ndistinguish relative positions of input elements.\nIn multi-document summarization, tokentij has\ntwo positions that need to be considered, namely i\n(the rank of the paragraph) and j (the position\nof the token within the paragraph). Positional\nembedding peij ∈ Rd represents both positions\n(via concatenation) and is added to word embed-\nding wij to obtain the ﬁnal input vector x0\nij :\npeij = [ei; ej] (9)\nx0\nij = wij + peij (10)\n3.2.2 Local Transformer Layer\nA local transformer layer is used to encode con-\ntextual information for tokens within each para-\ngraph. The local transformer layer is the same\nas the vanilla transformer layer (Vaswani et al.,\n2017), and composed of two sub-layers:\nh= LayerNorm(xl−1 + MHAtt(xl−1)) (11)\nxl = LayerNorm(h+ FFN(h)) (12)\nwhere LayerNorm is layer normalization pro-\nposed in Ba et al. (2016); MHAtt is the multi-\nhead attention mechanism introduced in Vaswani\net al. (2017) which allows each token to attend\nto other tokens with different attention distribu-\ntions; and FFN is a two-layer feed-forward net-\nwork with ReLU as hidden activation function.\n3.2.3 Global Transformer Layer\nA global transformer layer is used to exchange in-\nformation across multiple paragraphs. As shown\nin Figure 2, we ﬁrst apply a multi-head pooling op-\neration to each paragraph. Different heads will en-\ncode paragraphs with different attention weights.\nThen, for each head, an inter-paragraph attention\nmechanism is applied, where each paragraph can\ncollect information from other paragraphs by self-\nattention, generating a context vector to capture\ncontextual information from the whole input. Fi-\nnally, context vectors are concatenated, linearly\ntransformed, added to the vector of each token,\nand fed to a feed-forward layer, updating the rep-\nresentation of each token with global information.\nMulti-head Pooling To obtain ﬁxed-length\nparagraph representations, we apply a weighted-\npooling operation; instead of using only one rep-\nresentation for each paragraph, we introduce a\nmulti-head pooling mechanism, where for each\nparagraph, weight distributions over tokens are\ncalculated, allowing the model to ﬂexibly encode\nparagraphs in different representation subspaces\nby attending to different words.\nLet xl−1\nij ∈Rd denote the output vector of the\nlast transformer layer for token tij, which is used\nas input for the current layer. For each paragraph\nRi, for head z ∈{1,··· ,nhead}, we ﬁrst trans-\nform the input vectors into attention scores az\nij\nand value vectors bz\nij. Then, for each head, we\ncalculate a probability distribution ˆaz\nij over tokens\nwithin the paragraph based on attention scores:\naz\nij = Wz\na xl−1\nij (13)\nbz\nij = Wz\nb xl−1\nij (14)\nˆaz\nij = exp(az\nij)/\nn∑\nj=1\nexp(az\nij) (15)\nwhere Wz\na ∈ R1∗d and Wz\nb ∈ Rdhead∗d are\nweights. dhead = d/nhead is the dimension of\neach head. nis the number of tokens in Ri.\nWe next apply a weighted summation with an-\nother linear transformation and layer normaliza-\ntion to obtain vector headz\ni for the paragraph:\nheadz\ni = LayerNorm(Wz\nc\nn∑\nj=1\naz\nijbz\nij) (16)\nwhere Wz\nc ∈Rdhead∗dhead is the weight.\nThe model can ﬂexibly incorporate multiple\nheads, with each paragraph having multiple at-\ntention distributions, thereby focusing on different\nviews of the input.\nInter-paragraph Attention We model the de-\npendencies across multiple paragraphs with an\ninter-paragraph attention mechanism. Similar to\nself-attention, inter-paragraph attention allows for\neach paragraph to attend to other paragraphs by\ncalculating an attention distribution:\nqz\ni = Wz\nq headz\ni (17)\nkz\ni = Wz\nk headz\ni (18)\nvz\ni = Wz\nv headz\ni (19)\ncontextz\ni =\nm∑\ni=1\nexp(qz\ni\nT kz\ni′ )∑m\no=1 exp(qz\ni\nT kzo)vz\ni′ (20)\nwhere qz\ni ,kz\ni ,vz\ni ∈ Rdhead∗dhead are query,\nkey, and value vectors that are linearly trans-\nformed from headz\ni as in Vaswani et al. (2017);\ncontextz\ni ∈ Rdhead represents the context vec-\ntor generated by a self-attention operation over\nall paragraphs. m is the number of input para-\ngraphs. Figure 2 provides a schematic view of\ninter-paragraph attention.\nFeed-forward Networks We next update token\nrepresentations with contextual information. We\nﬁrst fuse information from all heads by concate-\nnating all context vectors and applying a linear\ntransformation with weight Wc ∈Rd∗d:\nci = Wc[context1\ni ; ··· ; contextnhead\ni ] (21)\nMulti-head Pooling Multi-head Pooling\nhead 1\nhead 2\nhead 3\nhead 1\nhead 2\nhead 3\ncontext 1 context 2 context 3 context 1 context 2 context 3\nInter-paragraph Attention\nInter-paragraph Attention\nInter-paragraph Attention\ncontext \nthis is para one\nFeed Forward Feed Forward Feed Forward Feed Forward\ncontext \nthis is para two\nFeed Forward Feed Forward Feed Forward Feed Forward\nthis is para one this is para two\nFigure 2: A global transformer layer. Different col-\nors indicate different heads in multi-head pooling and\ninter-paragraph attention.\nWe then add ci to each input token vector xl−1\nij ,\nand feed it to a two-layer feed-forward network\nwith ReLU as the activation function and a high-\nway layer normalization on top:\ngij = Wo2ReLU(Wo1(xl−1\nij + ci)) (22)\nxl\nij = LayerNorm(gij + xl−1\nij ) (23)\nwhere Wo1 ∈Rdff ∗d and Wo2 ∈Rd∗dff are the\nweights, dff is the hidden size of the feed-forward\nlater. This way, each token within paragraph Ri\ncan collect information from other paragraphs in a\nhierarchical and efﬁcient manner.\n3.2.4 Graph-informed Attention\nThe inter-paragraph attention mechanism can be\nviewed as learning a latent graph representation\n(self-attention weights) of the input paragraphs.\nAlthough previous work has shown that simi-\nlar latent representations are beneﬁcial for down-\nstream NLP tasks (Liu and Lapata, 2018; Kim\net al., 2017; Williams et al., 2018; Niculae et al.,\n2018; Fernandes et al., 2019), much work in\nmulti-document summarization has taken advan-\ntage of explicit graph representations, each focus-\ning on different facets of the summarization task\n(e.g., capturing redundant information or repre-\nsenting passages referring to the same event or\nentity). One advantage of the hierarchical trans-\nformer is that we can easily incorporate graphs ex-\nternal to the model, to generate better summaries.\nWe experimented with two well-established\ngraph representations which we discuss brieﬂy be-\nlow. However, there is nothing inherent in our\nmodel that restricts us to these, any graph mod-\neling relationships across paragraphs could have\nbeen used instead. Our ﬁrst graph aims to capture\nlexical relations; graph nodes correspond to para-\ngraphs and edge weights are cosine similarities\nbased on tf-idf representations of the paragraphs.\nOur second graph aims to capture discourse re-\nlations (Christensen et al., 2013); it builds an\nApproximate Discourse Graph (ADG) (Yasunaga\net al., 2017) over paragraphs; edges between para-\ngraphs are drawn by counting (a) co-occurring en-\ntities and (b) discourse markers (e.g., however,\nnevertheless) connecting two adjacent paragraphs\n(see the Appendix for details on how ADGs are\nconstructed).\nWe represent such graphs with a matrix G,\nwhere Gii′ is the weight of the edge connecting\nparagraphs iand i′. We can then inject this graph\ninto our hierarchical transformer by simply substi-\ntuting one of its (learned) heads z′with G. Equa-\ntion (20) for calculating the context vector for this\nhead is modiﬁed as:\ncontextz′\ni =\nm∑\ni′=1\nGii′\n∑m\no=1 Gio\nvz′\ni′ (24)\n4 Experimental Setup\nWikiSum Dataset We used the scripts and urls\nprovided in Liu et al. (2018) to crawl Wikipedia\narticles and source reference documents. We suc-\ncessfully crawled 78.9% of the original documents\n(some urls have become invalid and correspond-\ning documents could not be retrieved). We fur-\nther removed clone paragraphs (which are exact\ncopies of some parts of the Wikipedia articles);\nthese were paragraphs in the source documents\nwhose bigram recall against the target summary\nwas higher than 0.8. On average, each input\nhas 525 paragraphs, and each paragraph has 70.1\ntokens. The average length of the target sum-\nmary is 139.4 tokens. We split the dataset with\n1,579,360 instances for training, 38,144 for vali-\ndation and 38,205 for test.\nMethods ROUGE-L Recall\nL′= 5 L′= 10 L′= 20 L′= 40\nSimilarity 24.86 32.43 40.87 49.49\nRanking 39.38 46.74 53.84 60.42\nTable 1: ROUGE-L recall against target summary for\nL′-best paragraphs obtained with tf-idf cosine similar-\nity and our ranking model.\nFor both ranking and summarization stages,\nwe encode source paragraphs and target sum-\nmaries using subword tokenization with Sentence-\nPiece (Kudo and Richardson, 2018). Our vocabu-\nlary consists of 32,000 subwords and is shared for\nboth source and target.\nParagraph Ranking To train the regression\nmodel, we calculated the ROUGE-2 recall (Lin,\n2004) of each paragraph against the target sum-\nmary and used this as the ground-truth score. The\nhidden size of the two LSTMs was set to 256,\nand dropout (with dropout probability of 0.2) was\nused before all linear layers. Adagrad (Duchi\net al., 2011) with learning rate 0.15 is used for\noptimization. We compare our ranking model\nagainst the method proposed in Liu et al. (2018)\nwho use the tf-idf cosine similarity between each\nparagraph and the article title to rank the input\nparagraphs. We take the ﬁrst L′paragraphs from\nthe ordered paragraph set produced by our ranker\nand the similarity-based method, respectively. We\nconcatenate these paragraphs and calculate their\nROUGE-L recall against the gold target text. The\nresults are shown in Table 1. We can see that our\nranker effectively extracts related paragraphs and\nproduces more informative input for the down-\nstream summarization task.\nTraining Conﬁguration In all abstractive mod-\nels, we apply dropout (with probability of 0.1) be-\nfore all linear layers; label smoothing (Szegedy\net al., 2016) with smoothing factor0.1 is also used.\nTraining is in traditional sequence-to-sequence\nmanner with maximum likelihood estimation. The\noptimizer was Adam (Kingma and Ba, 2014) with\nlearning rate of 2,β1 = 0.9, and β2 = 0.998;\nwe also applied learning rate warmup over the\nﬁrst 8,000 steps, and decay as in (Vaswani et al.,\n2017). All transformer-based models had 256 hid-\nden units; the feed-forward hidden size was1,024\nfor all layers. All models were trained on 4 GPUs\n(NVIDIA TITAN Xp) for500,000 steps. We used\nModel ROUGE-1 ROUGE-2 ROUGE-L\nLead 38.22 16.85 26.89\nLexRank 36.12 11.67 22.52\nFT (600 tokens, no ranking) 35.46 20.26 30.65\nFT (600 tokens) 40.46 25.26 34.65\nFT (800 tokens) 40.56 25.35 34.73\nFT (1,200 tokens) 39.55 24.63 33.99\nT-DMCA (3000 tokens) 40.77 25.60 34.90\nHT (1,600 tokens) 40.82 25.99 35.08\nHT (1,600 tokens) + Similarity Graph 40.80 25.95 35.08\nHT (1,600 tokens) + Discourse Graph 40.81 25.95 35.24\nHT (train on 1,600 tokens/test on 3000 tokens) 41.53 26.52 35.76\nTable 2: Test set results on the WikiSum dataset using ROUGE F1.\ngradient accumulation to keep training time for all\nmodels approximately consistent. We selected the\n5 best checkpoints based on performance on the\nvalidation set and report averaged results on the\ntest set.\nDuring decoding we use beam search with beam\nsize 5 and length penalty with α= 0.4 (Wu et al.,\n2016); we decode until an end-of-sequence token\nis reached.\nComparison Systems We compared the pro-\nposed hierarchical transformer against several\nstrong baselines:\nLead is a simple baseline that concatenates the ti-\ntle and ranked paragraphs, and extracts the\nﬁrst k tokens; we set k to the length of the\nground-truth target.\nLexRank (Erkan and Radev, 2004) is a widely-\nused graph-based extractive summarizer; we\nbuild a graph with paragraphs as nodes and\nedges weighted by tf-idf cosine similarity; we\nrun a PageRank-like algorithm on this graph\nto rank and select paragraphs until the length\nof the ground-truth summary is reached.\nFlat Transformer (FT) is a baseline that applies\na Transformer-based encoder-decoder model\nto a ﬂat token sequence. We used a 6-layer\ntransformer. The title and ranked paragraphs\nwere concatenated and truncated to 600,800,\nand 1,200 tokens.\nT-DMCA is the best performing model of Liu\net al. (2018) and a shorthand for Transformer\nDecoder with Memory Compressed Atten-\ntion; they only used a Transformer decoder\nand compressed the key and value in self-\nattention with a convolutional layer. The\nmodel has 5 layers as in Liu et al. (2018).\nIts hidden size is 512 and its feed-forward\nhidden size is 2,048. The title and ranked\nparagraphs were concatenated and truncated\nto 3,000 tokens.\nHierarchical Transformer (HT) is the model\nproposed in this paper. The model archi-\ntecture is a 7-layer network (with 5 local-\nattention layers at the bottom and 2 global at-\ntention layers at the top). The model takes\nthe title and L′ = 24 paragraphs as input to\nproduce a target summary, which leads to ap-\nproximately 1,600 input tokens per instance.\n5 Results\nAutomatic Evaluation We evaluated summa-\nrization quality using ROUGEF1 (Lin, 2004). We\nreport unigram and bigram overlap (ROUGE-1\nand ROUGE-2) as a means of assessing infor-\nmativeness and the longest common subsequence\n(ROUGE-L) as a means of assessing ﬂuency.\nTable 2 summarizes our results. The ﬁrst\nblock in the table includes extractive systems\n(Lead, LexRank), the second block includes sev-\neral variants of Flat Transformer-based models\n(FT, T-DMCA), while the rest of the table presents\nthe results of our Hierarchical Transformer (HT).\nAs can be seen, abstractive models generally out-\nperform extractive ones. The Flat Transformer,\nachieves best results when the input length is set\nto 800 tokens, while longer input (i.e., 1,200 to-\nkens) actually hurts performance. The Hierarchi-\ncal Transformer with 1,600 input tokens, outper-\nModel R1 R2 RL\nHT 40.82 25.99 35.08\nHT w/o PP 40.21 24.54 34.71\nHT w/o MP 39.90 24.34 34.61\nHT w/o GT 39.01 22.97 33.76\nTable 3: Hierarchical Transformer and versions thereof\nwithout (w/o) paragraph position (PP), multi-head\npooling (MP), and global transformer layer (GT).\nforms FT, and even T-DMCA when the latter is\npresented with 3,000 tokens. Adding an external\ngraph also seems to help the summarization pro-\ncess. The similarity graph does not have an ob-\nvious inﬂuence on the results, while the discourse\ngraph boosts ROUGE-L by 0.16.\nWe also found that the performance of the Hi-\nerarchical Transformer further improves when the\nmodel is presented with longer input at test time.2\nAs shown in the last row of Table 2, when test-\ning on 3,000 input tokens, summarization quality\nimproves across the board. This suggests that the\nmodel can potentially generate better summaries\nwithout increasing training time.\nTable 3 summarizes ablation studies aiming to\nassess the contribution of individual components.\nOur experiments conﬁrmed that encoding para-\ngraph position in addition to token position within\neach paragraph is beneﬁcial (see row w/o PP), as\nwell as multi-head pooling (w/o MP is a model\nwhere the number of heads is set to 1), and the\nglobal transformer layer (w/o GT is a model with\nonly 5 local transformer layers in the encoder).\nHuman Evaluation In addition to automatic\nevaluation, we also assessed system performance\nby eliciting human judgments on 20 randomly se-\nlected test instances. Our ﬁrst evaluation study\nquantiﬁed the degree to which summarization\nmodels retain key information from the documents\nfollowing a question-answering (QA) paradigm\n(Clarke and Lapata, 2010; Narayan et al., 2018).\nWe created a set of questions based on the gold\nsummary under the assumption that it contains the\nmost important information from the input para-\ngraphs. We then examined whether participants\nwere able to answer these questions by reading\nsystem summaries alone without access to the gold\nsummary. The more questions a system can an-\nswer, the better it is at summarization. We cre-\nated 57 questions in total varying from two to\n2This was not the case with the other Transformer models.\nModel QA Rating\nLead 31.59 -0.383\nFT 35.69 0.000\nT-DMCA 43.14 0.147\nHT 54.11 0.237\nTable 4: System scores based on questions answered\nby AMT participants and summary quality rating.\nfour questions per gold summary. Examples of\nquestions and their answers are given in Table 5.\nWe adopted the same scoring mechanism used\nin Clarke and Lapata (2010), i.e., correct answers\nare marked with 1, partially correct ones with 0.5,\nand 0 otherwise. A system’s score is the average\nof all question scores.\nOur second evaluation study assessed the over-\nall quality of the summaries by asking partici-\npants to rank them taking into account the fol-\nlowing criteria: Informativeness (does the sum-\nmary convey important facts about the topic in\nquestion?), Fluency (is the summary ﬂuent and\ngrammatical?), and Succinctness (does the sum-\nmary avoid repetition?). We used Best-Worst Scal-\ning (Louviere et al., 2015), a less labor-intensive\nalternative to paired comparisons that has been\nshown to produce more reliable results than rating\nscales (Kiritchenko and Mohammad, 2017). Par-\nticipants were presented with the gold summary\nand summaries generated from 3 out of 4 systems\nand were asked to decide which summary was the\nbest and which one was the worst in relation to\nthe gold standard, taking into account the criteria\nmentioned above. The rating of each system was\ncomputed as the percentage of times it was chosen\nas best minus the times it was selected as worst.\nRatings range from −1 (worst) to 1 (best).\nBoth evaluations were conducted on the Ama-\nzon Mechanical Turk platform with 5 responses\nper hit. Participants evaluated summaries pro-\nduced by the Lead baseline, the Flat Transformer,\nT-DMCA, and our Hierarchical Transformer. All\nevaluated systems were variants that achieved the\nbest performance in automatic evaluations. As\nshown in Table 4, on both evaluations, participants\noverwhelmingly prefer our model (HT). All pair-\nwise comparisons among systems are statistically\nsigniﬁcant (using a one-way ANOV A with post-\nhoc Tukey HSD tests; p< 0.01). Examples of\nsystem output are provided in Table 5.\nPentagoet Archeological District\nGOLD\nThe Pentagoet Archeological District is a National Historic Landmark District located at the southern edge of\nthe Bagaduce Peninsula in Castine, Maine. It is the site of Fort Pentagoet, a 17th-century fortiﬁed trading post\nestablished by fur traders of French Acadia. From 1635 to 1654 this site was a center of trade with the local\nAbenaki, and marked the effective western border of Acadia with New England. From 1654 to 1670 the site\nwas under English control, after which it was returned to France by the Treaty of Breda. The fort was destroyed\nin 1674 by Dutch raiders. The site was designated a National Historic Landmark in 1993. It is now a public\npark.\nQA\nWhat is the Pentagoet Archeological District? [a National Historic Landmark District]\nWhere is it located? [Castine , Maine]\nWhat did the Abenaki Indians use the site for? [trading center]\nLEAD\nThe Pentagoet Archeological District is a National Historic Landmark District located in Castine, Maine. This\ndistrict forms part of the traditional homeland of the Abenaki Indians, in particular the Penobscot tribe. In\nthe colonial period, Abenakis frequented the fortiﬁed trading post at this site, bartering moosehides, sealskins,\nbeaver and other furs in exchange for European commodities. ”Pentagoet Archeological district” is a National\nHistoric Landmark District located at the southern edge of the Bagaduce Peninsula in Treaty Of Breda.\nFT\nthe Pentagoet Archeological district is a National Historic Landmark District located at\nthe southern edge of the Bagaduce Peninsula in Treaty Of Breda. It was listed on the\nnational register of historic places in 1983.\nT-DMCA\nThe Pentagoet Archeological District is a national historic landmark district located in castine , maine . this\ndistrict forms part of the traditional homeland of the abenaki indians , in particular the Penobscot tribe. The\ndistrict was listed on the national register of historic places in 1982.\nHT\nThe Pentagoet Archeological district is a National Historic Landmark District located in Castine, Maine. This\ndistrict forms part of the traditional homeland of the Abenaki Indians, in particular the Penobscot tribe. In\nthe colonial period, Abenaki frequented the fortiﬁed trading post at this site, bartering moosehides, sealskins,\nbeaver and other furs in exchange for European commodities.\nMelanesian Whistler\nGOLD\nThe Melanesian whistler or Vanuatu whistler (Pachycephala chlorura) is a species of passerine bird in the\nwhistler family Pachycephalidae. It is found on the Loyalty Islands, Vanuatu, and Vanikoro in the far south-\neastern Solomons.\nQA\nWhat is the Melanesian Whistler? [a species of passerine bird in the whistler family Pachycephalidae]\nWhere is it found? [Loyalty Islands , Vanuatu , and Vanikoro in the far south-eastern Solomons]\nLEAD\nThe Australian golden whistler (Pachycephala pectoralis) is a species of bird found in forest, woodland, mallee,\nmangrove and scrub in Australia (except the interior and most of the north) Most populations are resident, but\nsome in south-eastern Australia migrate north during the winter.\nFT\nThe Melanesian whistler (P. Caledonica) is a species of bird in the family Muscicapidae. It is endemic to\nMelanesia.\nT-DMCA\nThe Australian golden whistler (Pachycephala chlorura) is a species of bird in the family Pachycephalidae,\nwhich is endemic to Fiji.\nHT\nThe Melanesian whistler (Pachycephala chlorura) is a species of bird in the family Pachycephalidae, which is\nendemic to Fiji.\nTable 5: G OLD human authored summaries, questions based on them (answers shown in square brackets) and\nautomatic summaries produced by the L EAD -3 baseline, the Flat Transformer (FT), T-DMCA (Liu et al., 2018)\nand our Hierachical Transformer (HT).\n6 Conclusions\nIn this paper we conceptualized abstractive multi-\ndocument summarization as a machine learning\nproblem. We proposed a new model which is\nable to encode multiple input documents hierar-\nchically, learn latent relations across them, and ad-\nditionally incorporate structural information from\nwell-known graph representations. We have also\ndemonstrated the importance of a learning-based\napproach for selecting which documents to sum-\nmarize. Experimental results show that our model\nproduces summaries which are both ﬂuent and in-\nformative outperforming competitive systems by a\nwide margin. In the future we would like to apply\nour hierarchical transformer to question answering\nand related textual inference tasks.\nAcknowledgments\nWe would like to thank Laura Perez-Beltrachini\nfor her help with preprocessing the dataset. This\nresearch is supported by a Google PhD Fellow-\nship to the ﬁrst author. The authors gratefully ac-\nknowledge the ﬁnancial support of the European\nResearch Council (award number 681760).\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In In Proceedings of\nthe 3rd International Conference on Learning Rep-\nresentations, San Diego, California.\nRegina Barzilay and Kathleen R. McKeown. 2005.\nSentence fusion for multidocument news summa-\nrization. Computational Linguistics , 31(3):297–\n327.\nLidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo,\nand Rebecca Passonneau. 2015. Abstractive multi-\ndocument summarization via phrase selection and\nmerging. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 1587–1597, Beijing, China.\nAsli Celikyilmaz, Antoine Bosselut, Xiaodong He, and\nYejin Choi. 2018. Deep communicating agents for\nabstractive summarization. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 1662–1675, New Orleans, Louisiana.\nJanara Christensen, Mausam, Stephen Soderland, and\nOren Etzioni. 2013. Towards coherent multi-\ndocument summarization. In Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1163–1173, At-\nlanta, Georgia. Association for Computational Lin-\nguistics.\nEric Chu and Peter J Liu. 2018. Unsupervised neural\nmulti-document abstractive summarization. arXiv\npreprint arXiv:1810.05739.\nJames Clarke and Mirella Lapata. 2010. Discourse\nconstraints for document compression. Computa-\ntional Linguistics, 36(3):411–441.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine\nLearning Research, 12(Jul):2121–2159.\nG¨unes Erkan and Dragomir R Radev. 2004. Lexrank:\nGraph-based lexical centrality as salience in text\nsummarization. Journal of artiﬁcial intelligence re-\nsearch, 22:457–479.\nPatrick Fernandes, Miltiadis Allamanis, and Marc\nBrockschmidt. 2019. Structured neural summariza-\ntion. In Proceedings of the 7th International Con-\nference on Learning Representations, New Orleans,\nLouisiana.\nKatja Filippova and Michael Strube. 2008. Sentence\nfusion via dependency graph compression. In Pro-\nceedings of the 2008 Conference on Empirical Meth-\nods in Natural Language Processing , pages 177–\n185, Honolulu, Hawaii.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4098–4109, Brussels, Belgium.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNewsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 708–719, New Orleans, Louisiana.\nKarl Moritz Hermann, Tomas Kocisky, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. Teaching ma-\nchines to read and comprehend. InAdvances in Neu-\nral Information Processing Systems 28, pages 1693–\n1701. Curran Associates, Inc.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nYoon Kim, Carl Denton, Luong Hoang, and Alexan-\nder M Rush. 2017. Structured attention networks.\nIn Proceedings of the 5th International Conference\non Learning Representations, Toulon, France.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nThomas N Kipf and Max Welling. 2017. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. In Proceedings of the 4th International\nConference on Learning Representations, San Juan,\nPuerto Rico.\nSvetlana Kiritchenko and Saif Mohammad. 2017.\nBest-worst scaling more reliable than rating scales:\nA case study on sentiment intensity annotation. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 465–\n470, Vancouver, Canada.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nLogan Lebanoff and Fei Liu. 2018. Automatic detec-\ntion of vague words and sentences in privacy poli-\ncies. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 3508–3517, Brussels, Belgium.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text Summarization\nBranches Out: Proceedings of the ACL-04 Work-\nshop, pages 74–81, Barcelona, Spain. Association\nfor Computational Linguistics.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating Wikipedia by summariz-\ning long sequences. In Proceedings of the 6th Inter-\nnational Conference on Learning Representations ,\nVancouver, Canada.\nYang Liu and Mirella Lapata. 2018. Learning struc-\ntured text representations. Transactions of the Asso-\nciation for Computational Linguistics, 6:63–75.\nJordan J Louviere, Terry N Flynn, and Anthony Al-\nfred John Marley. 2015. Best-worst scaling: The-\nory, methods and applications. Cambridge Univer-\nsity Press.\nShulei Ma, Zhi-Hong Deng, and Yunlun Yang. 2016.\nAn unsupervised multi-document summarization\nframework based on neural document model. In\nProceedings of COLING 2016, the 26th Interna-\ntional Conference on Computational Linguistics:\nTechnical Papers, pages 1514–1523, Osaka, Japan.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Ranking sentences for extractive summariza-\ntion with reinforcement learning. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1747–1759, New Orleans, Louisiana.\nVlad Niculae, Andr ´e F. T. Martins, and Claire Cardie.\n2018. Towards dynamic computation graphs via\nsparse latent structure. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 905–911, Brussels, Bel-\ngium.\nDaraksha Parveen and Michael Strube. 2014. Multi-\ndocument summarization using bipartite graphs. In\nProceedings of TextGraphs-9: the workshop on\nGraph-based Methods for Natural Language Pro-\ncessing, pages 15–24, Doha, Qatar.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018. A deep reinforced model for abstractive sum-\nmarization. In Proceedings of the 6th International\nConference on Learning Representations , Vancou-\nver, Canada.\nDragomir Radev. 2000. A common theory of infor-\nmation fusion from multiple text sources step one:\nCross-document structure. In 1st SIGdial Workshop\non Discourse and Dialogue , pages 74–83, Hong\nKong, China.\nEvan Sandhaus. 2008. The New York Times Annotated\nCorpus. Linguistic Data Consortium, Philadelphia,\n6(12).\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083, Vancouver, Canada.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nThe IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nXiaojun Wan. 2008. An exploration of document\nimpact on graph-based multi-document summariza-\ntion. In Proceedings of the 2008 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 755–762, Honolulu, Hawaii.\nAdina Williams, Andrew Drozdov, and Samuel R.\nBowman. 2018. Do latent tree learning models iden-\ntify meaningful structure in sentences? Transac-\ntions of the Association for Computational Linguis-\ntics, 6:253–267.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. In arXiv preprint\narXiv:1609.08144.\nMichihiro Yasunaga, Rui Zhang, Kshitijh Meelu,\nAyush Pareek, Krishnan Srinivasan, and Dragomir\nRadev. 2017. Graph-based neural multi-document\nsummarization. In Proceedings of the 21st Confer-\nence on Computational Natural Language Learning\n(CoNLL 2017), pages 452–462, Vancouver, Canada.\nJianmin Zhang, Jiwei Tan, and Xiaojun Wan. 2018.\nAdapting neural single-document summarization\nmodel for abstractive multi-document summariza-\ntion: A pilot study. In Proceedings of the Interna-\ntional Conference on Natural Language Generation.\nA Appendix\nWe describe here how the similarity and discourse\ngraphs discussed in Section 3.2.4 were created.\nThese graphs were added to the hierarchical trans-\nformer model as a means to enhance summary\nquality (see Section 5 for details).\nA.1 Similarity Graph\nThe similarity graph S is based on tf-idf cosine\nsimilarity. The nodes of the graph are paragraphs.\nWe ﬁrst represent each paragraph pi as a bag of\nwords. Then, we calculate the tf-idf value vik for\neach token tik in a paragraph:\nvik = Nw(tik)log( Nd\nNdw(tik)) (25)\nwhere Nw(t) is the count of word t in the para-\ngraph, Nd is the total number of paragraphs,\nand Ndw(t) is the total number of paragraphs con-\ntaining the word. We thus obtain a tf-idf vector\nfor each paragraph. Then, for all paragraph pairs\n<pi,pi′ >, we calculate the cosine similarity of\ntheir tf-idf vectors and use this as the weight Sii′\nfor the edge connecting the pair in the graph. We\nremove edges with weights lower than 0.2.\nA.2 Discourse Graphs\nTo build the Approximate Discourse Graph\n(ADG) D, we follow Christensen et al. (2013) and\nYasunaga et al. (2017). The original ADG makes\nuse of several complex features. Here, we create\na simpliﬁed version with only two features (nodes\nin this graph are again paragraphs).\nCo-occurring Entities For each paragraph pi,\nwe extract a set of entities Ei in the paragraph\nusing the Spacy 3 NER recognizer. We only use\nentities with type {PERSON, NORP, FAC,\nORG, GPE, LOC, EVENT, WORK OF ART,\nLAW}. For each paragraph pair <pi,pj >, we\ncount eij, the number of entities with exact match.\nDiscourse Markers We use the following36 ex-\nplicit discourse markers to identify edges between\ntwo adjacent paragraphs in a source webpage:\nagain, also, another, comparatively, fur-\nthermore, at the same time,however, im-\nmediately, indeed, instead, to be sure,\nlikewise, meanwhile, moreover, never-\ntheless, nonetheless, notably, otherwise,\nregardless, similarly, unlike, in addition,\neven, in turn, in exchange, in this case,\nin any event, ﬁnally, later, as well, espe-\ncially, as a result, example, in fact, then,\nthe day before\n3https://spacy.io/api/entityrecognizer\nIf two paragraphs <pi,pi′ >are adjacent in one\nsource webpage and they are connected with one\nof the above 36 discourse markers, mii′ will be 1,\notherwise it will be 0.\nThe ﬁnal edge weight Dii′ is the weighted sum\nof eii′ and mii′\nDii′ = 0.2 ∗eii′ + mii′ (26)"
}