{
    "title": "Foundation models for geospatial reasoning: assessing the capabilities of large language models in understanding geometries and topological spatial relations",
    "url": "https://openalex.org/W4410957795",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2337063480",
            "name": "Ji Yuhan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2113633793",
            "name": "Gao Song",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097221238",
            "name": "Nie Ying",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3115459211",
            "name": "Majic Ivan",
            "affiliations": [
                "University of Vienna"
            ]
        },
        {
            "id": "https://openalex.org/A3205987712",
            "name": "Janowicz, Krzysztof",
            "affiliations": [
                "University of Vienna"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4225323055",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2911964244",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W1979180917",
        "https://openalex.org/W1993692165",
        "https://openalex.org/W2215149882",
        "https://openalex.org/W2802180604",
        "https://openalex.org/W2079249289",
        "https://openalex.org/W6600050674",
        "https://openalex.org/W116506501",
        "https://openalex.org/W4390695694",
        "https://openalex.org/W2016125828",
        "https://openalex.org/W1526337814",
        "https://openalex.org/W2047927510",
        "https://openalex.org/W4400024312",
        "https://openalex.org/W1871668702",
        "https://openalex.org/W2240909318",
        "https://openalex.org/W1521171126",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W1919734042",
        "https://openalex.org/W1482327797",
        "https://openalex.org/W2133039566",
        "https://openalex.org/W2014590262",
        "https://openalex.org/W2038647915",
        "https://openalex.org/W2599172154",
        "https://openalex.org/W2078019102",
        "https://openalex.org/W4246034910",
        "https://openalex.org/W3121972911",
        "https://openalex.org/W4401167869",
        "https://openalex.org/W1964373891",
        "https://openalex.org/W4389474133",
        "https://openalex.org/W2568931349",
        "https://openalex.org/W4404481617",
        "https://openalex.org/W4220748468",
        "https://openalex.org/W6601399257",
        "https://openalex.org/W2028237718",
        "https://openalex.org/W2115069155",
        "https://openalex.org/W4391324074",
        "https://openalex.org/W2020694859",
        "https://openalex.org/W2129667801",
        "https://openalex.org/W4387460165",
        "https://openalex.org/W4398255720",
        "https://openalex.org/W6849137788",
        "https://openalex.org/W4389475164",
        "https://openalex.org/W2982441053",
        "https://openalex.org/W2132537081",
        "https://openalex.org/W1885702489",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W4390874575",
        "https://openalex.org/W6838865847",
        "https://openalex.org/W2051826382",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4388834734",
        "https://openalex.org/W4388616070",
        "https://openalex.org/W183505914",
        "https://openalex.org/W1978188545",
        "https://openalex.org/W4309651788",
        "https://openalex.org/W4393002303",
        "https://openalex.org/W3023770275",
        "https://openalex.org/W3214340375",
        "https://openalex.org/W3041136315",
        "https://openalex.org/W4404480476",
        "https://openalex.org/W4388834509",
        "https://openalex.org/W6600033781",
        "https://openalex.org/W4387294423",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W1584890717",
        "https://openalex.org/W1568262744",
        "https://openalex.org/W6633093605",
        "https://openalex.org/W3011565817",
        "https://openalex.org/W1995875735",
        "https://openalex.org/W4380534756",
        "https://openalex.org/W2474096096",
        "https://openalex.org/W4384525653",
        "https://openalex.org/W2236233024",
        "https://openalex.org/W1996712938",
        "https://openalex.org/W2235353364",
        "https://openalex.org/W2011292982",
        "https://openalex.org/W4389520758",
        "https://openalex.org/W2342933002",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4253206356",
        "https://openalex.org/W4297679000",
        "https://openalex.org/W4389455500",
        "https://openalex.org/W4390100389",
        "https://openalex.org/W6600513322",
        "https://openalex.org/W2776890924",
        "https://openalex.org/W6600648412",
        "https://openalex.org/W4312220150",
        "https://openalex.org/W2112360088",
        "https://openalex.org/W4399730268",
        "https://openalex.org/W4285042349",
        "https://openalex.org/W3102423243",
        "https://openalex.org/W4238149920",
        "https://openalex.org/W1555471913",
        "https://openalex.org/W4300038550",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W852874",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4253107119"
    ],
    "abstract": "Applying AI foundation models directly to geospatial datasets remains challenging due to their limited ability to represent and reason with geographical entities, specifically vector-based geometries and natural language descriptions of complex spatial relations. To address these issues, we investigate the extent to which a well-known-text (WKT) representation of geometries and their spatial relations (e.g., topological predicates) are preserved during spatial reasoning when the geospatial vector data are passed to large language models (LLMs) including GPT-3.5-turbo, GPT-4, and DeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the spatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt engineering-based, and everyday language-based evaluation. Our experiment results demonstrate that both the embedding-based and prompt engineering-based approaches to geospatial question-answering tasks with GPT models can achieve an accuracy of over 0.6 on average for the identification of topological spatial relations between two geometries. Among the evaluated models, GPT-4 with few-shot prompting achieved the highest performance with over 0.66 accuracy on topological spatial relation inference. Additionally, GPT-based reasoner is capable of properly comprehending inverse topological spatial relations and including an LLM-generated geometry can enhance the effectiveness for geographic entity retrieval. GPT-4 also exhibits the ability to translate certain vernacular descriptions about places into formal topological relations, and adding the geometry-type or place-type context in prompts may improve inference accuracy, but it varies by instance. The performance of these spatial reasoning tasks offers valuable insights for the refinement of LLMs with geographical knowledge towards the development of geo-foundation models capable of geospatial reasoning.",
    "full_text": "arXiv:2505.17136v1  [cs.CL]  22 May 2025\nRESEARCH ARTICLE\nFoundation Models for Geospatial Reasoning: Assessing the Capabilities of\nLarge Language Models in Understanding Geometries and Topological\nSpatial Relations\nYuhan Ji a, Song Gao a,*, Ying Nie a, Ivan Maji´ cb, and Krzysztof Janowicz b,c\na GeoDS Lab, Department of Geography, University of Wisconsin-Madison, USA;\nb Department of Geography and Regional Research, University of Vienna, Austria;\nc Department of Geography, University of California-Santa Barbara, USA\nARTICLE HISTORY\nCompiled June 12, 2025\nABSTRACT\nAI foundation models have demonstrated some capabilities for the understanding of geospatial\nsemantics. However, applying such pre-trained models directly to geospatial datasets remains\nchallenging due to their limited ability to represent and reason with geographical entities,\nspecifically vector-based geometries and natural language descriptions of complex spatial\nrelations. To address these issues, we investigate the extent to which a well-known-text\n(WKT) representation of geometries and their spatial relations (e.g., topological predicates)\nare preserved during spatial reasoning when the geospatial vector data are passed to\nlarge language models (LLMs) including GPT-3.5-turbo, GPT-4, and DeepSeek-R1-14B.\nOur workflow employs three distinct approaches to complete the spatial reasoning tasks\nfor comparison, i.e., geometry embedding-based, prompt engineering-based, and everyday\nlanguage-based evaluation. Our experiment results demonstrate that both the embedding-based\nand prompt engineering-based approaches to geospatial question-answering tasks with GPT\nmodels can achieve an accuracy of over 0.6 on average for the identification of topological\nspatial relations between two geometries. Among the evaluated models, GPT-4 with few-shot\nprompting achieved the highest performance with over 0.66 accuracy on topological spatial\nrelation inference. Additionally, GPT-based reasoner is capable of properly comprehending\ninverse topological spatial relations and including an LLM-generated geometry can enhance the\neffectiveness for geographic entity retrieval. GPT-4 also exhibits the ability to translate certain\nvernacular descriptions about places into formal topological relations, and adding the geometry-\ntype or place-type context in prompts may improve inference accuracy, but it varies by instance.\nThe performance of these spatial reasoning tasks unveils the strengths and limitations of the\ncurrent LLMs in the processing and comprehension of geospatial vector data and offers valuable\ninsights for the refinement of LLMs with geographical knowledge towards the development of\ngeo-foundation models capable of geospatial reasoning.\nKEYWORDS\nGeoAI; geospatial reasoning; large language models; GPT; topological spatial relations\n1. Introduction\nOur interaction with Artificial Intelligence (AI) based systems is changing radically due to\nprogress in generative Foundation Models (FM) and the conversational, natural-language-driven\nstyle of interaction with many of these models. While most prior AI models were developed\nwith a limited range of downstream tasks in mind, foundation models aim to be general-\npurpose building blocks supporting a broad range of applications. Essentially, they are trained\non a substantially broader set of data and, while giving up accuracy for any specific task\nduring development, are easily fine-tuned before or during deployment. Large language models\n*A preprint draft and the final version will be available on the International Journal of Geographical Information Science;\nCorresponding Author. Email: song.gao@wisc.edu\n(LLMs) (Brown et al., 2020; Radford et al., 2019), such as Generative Pre-trained Transformers\n(GPT) (Achiam et al., 2023; Radford et al., 2018), and text-to-image models (Frolov et al.,\n2021), such as DALL-E (Ramesh et al., 2021), are specific types of foundation models. Most of\nthese models are generative, i.e., they return novel, synthetic output such as natural language\nanswers or imagery instead of providing answers by (information) retrieval as was common\nin prior systems, e.g., from the field of expert systems. While foundation models may not\ninherently prescribe a specific interaction style, they can be trained or fine-tuned for various\ntypes of interactions by carefully crafting the training dataset for the intended purpose. For\nexample, OpenAI’s Codex is trained using paired code examples and comments, enabling\nnatural language instructions to guide code generation effectively (Chen et al., 2021). Similarly,\nContrastive language-image pre-training (CLIP) facilitates tasks like image search from paired\ntextual descriptions. Reinforcement learning with human feedback (RLHF) is another approach\nthat aligns model outputs with user intent, improving conversational dialog flow, adherence to\nprompts, and reducing harmful content. The resulting conversational style of interaction is part\nof their broad appeal but also causes new challenges.\nTogether, these breakthroughs have opened the door towards conversation-style artificial GIS\nanalysts (“GeoMachina”) (Janowicz et al., 2020). For instance, ChatGPT-4 can understand\ninstructions for frequent GIS tasks like reading in a dataset (Mooney et al., 2023), performing\nsimple spatial analysis steps (by generating PySAL code), or even suggesting appropriate next\nsteps. Consequently, researchers started exploring the capabilities and limits of current AI in\nrepresenting spatial data (Ji and Gao, 2023), generating maps (Zhang et al., 2023), extracting\nplace semantics (Hu et al., 2023), automating GIS operations (Li and Ning, 2023; Zhang et al.,\n2024), generating code (Gramacki et al., 2024), and drawing inferences from such data (Mai\net al., 2024). Interestingly, the gaps this early research revealed are not unexpected as they have\nbeen documented as pain points of prior AI systems before (Janowicz et al., 2015). Prominently\nfeatured among these shortcomings is the representation of and reasoning with topological\nspatial relations (Cohn and Renz, 2008). Even more, this is true across foundation models,\ni.e., LLMs and text-to-image models struggle similarly. For instance, ChatGPT (OpenAI, 2022)\nwill provide a metric distance (e.g., several kilometers) when asked about the border of two\nneighboring countries. Similarly, DALL-E frequently fails to generate images of regions or parts\ndescribed using terms such as bordering, adjacent, contained, or specific types of maps (Zhang\net al., 2023). This is a critical insight as it implies that current work on geo-foundation\nmodels (Xie et al., 2023), e.g., location embeddings (Mai et al., 2022b), may benefit the broader\nAI community across models.\nTo better understand the limitations of LLMs in handling spatial data and to develop\nfoundation models for advancing geospatial artificial intelligence (GeoAI) (Gao et al., 2023), this\nwork aims to explore the potential of representing spatial object geometries in the WKT format\nto enable LLMs to perform GIS operations and enhance geospatial reasoning. In this work,\nwe present intensive experiments with well-known text (WKT) representation of geometries\nas inputs for LLMs and with natural language descriptions of (vague) spatial configurations.\nHowever, it is important to note that, unlike other types of data, accurate geometries (e.g.,\npoints, polylines, and polygons) and their spatial relations, as used in GIS, are not usually\nexpressed in natural language text for such models to consume during training. Without\nexplicitly addressing such structural deficiencies, the proposed approach is not suggested to\nbe directly applicable in practice.\nThe research contributions (RC) of our work are as follows:\n• RC1: We develop a workflow to assess the ability of LLMs to reason with topological\nspatial relations, more specifically, a subset of topological relations specified according to\nthe Dimensionally Extended 9-Intersection Model (DE-9IM). To do so, we will compare\ntwo approaches. First, we will encode the geometries and their topological relations in an\nembedding space using LLMs. Second, we will use a prompt engineering method to pass\nWKT format of geometries directly to the LLMs.\n• RC2: To test the capabilities of LLMs, we firstly utilize the WKT representation of two\n2\ngeometries to predict the topological spatial relation between them, and then we use one of\nthe geometries and the topological spatial relation to predict the second geometry. To do\nso, we will utilize the pre-trained text embedding models and also use prompt engineering\nto elicit the target geometry.\n• RC3: Finally, we study the ability of LLMs to extract the formalized topological\nspatial relations between geographic entities from vernacular descriptions (i.e., everyday\nlanguage) of the relations between geographic entities, e.g., as found in administrative\nplace descriptions from DBpedia/Wikipedia.\nThe remaining paper is organized as follows. We first review the literature on spatial relations,\nparts of the qualitative spatial reasoning and conceptual neighborhoods, large language models,\nand GeoAI foundation models in Section 2. We then introduce the methodology and workflow\nused in this research in Section 3, followed by the experiments design and dataset processing\nin Section 4. After that, we present the experiment results about topological spatial relation\nqualification and retrieval tasks using LLMs in Section 5. We further discuss the the confusion\nbetween the topological predicates with their corresponding conceptual neighborhoods in\nSection 6. Finally, we conclude this paper and offer insights into future work in Section 7.\n2. Related work\n2.1. Spatial relations\nSpatial relations refer to the connection between spatial objects regarding their geometric\nproperties (Guo, 1998), which specify the location of one object related to another one (Carlson\nand Logan, 2001) or more other objects (Majic et al., 2021). On the one hand, describing\nspatial relations in natural language is essential for understanding our surroundings in spatial\ncognition and navigating through space (Freksa et al., 1998). On the other hand, a reverse\nparsing process, where exact spatial relations are identified from natural language descriptions,\nis vital to improving the quality of information retrieval and human-computer interaction in\ntasks such as map reading (Head, 1984), geographic question answering (Gao and Goodchild,\n2013; Mai et al., 2020; Scheider et al., 2021), spatial query and reasoning (Du et al., 2005;\nGuo et al., 2022; Wang, 2000), disaster management (Cervone et al., 2016; Wang et al., 2016),\ndriving and robotics navigation (Tellex et al., 2011; Wallgr¨ un et al., 2014).\nTypically, binary spatial relations use the format of a triplet {subject, predicate (preposition),\nobject} to describe the relative positions of objects in space. In this format, the subject is an\nentity being described in relation to another entity, the predicate (preposition) is the descriptor\nbetween the subject and object, and the object is the entity that the subject is being related\nto in terms of position or location. For example, “Santa Barbara is situated northwest of Los\nAngeles” would be expressed as {Santa Barbara, northwest of, Los Angeles } in the format of\nspatial relations. Even though spatial relations pervade in our daily life conversations, people\ntend to frequently use a limited number of predicates to describe topological, directional, and\ndistance relations (Frank, 1992; Mark and Egenhofer, 1994). These expressions are qualitative in\nnature, offering approximate descriptions of an infinite range of possible spatial configurations.\nNevertheless, speakers can convey complex spatial layouts by combining these basic predicates\nwith contextual cues. For example, we might describe the locale of Santa Barbara as “Santa\nBarbara is connected via U.S. Highway 101 to Los Angeles about 100 miles to the southeast.”,\nor the position of a person as standing “in front of the building, facing east.” The ability to\ncombine and modify spatial predicates allows us to express a wide range of spatial relationships\nwith a relatively small vocabulary but increases the difficulty of representing and understanding\nthe meanings of such spatial relation descriptions for computers. The flexibility and ambiguity\ninherent in natural language often obscure the precise geometry of spatial arrangements, creating\na disconnection between semantic interpretation and physical spatial layout. The abundance of\nweb documents containing geographical references offers the opportunity to retrieve spatially-\naware information and support qualitative spatial reasoning from natural language texts (Jones\n3\net al., 2004). To bridge the semantic-physical gap, prior work has focused on extracting spatial\nrelations between named geographic entities by interpreting linguistic cues in text. These efforts\ninclude parsing grammatical and spatial semantic structures (Kordjamshidi et al., 2011; Loglisci\net al., 2012; Skoumas et al., 2016), as well as applying supervised machine learning models\ntrained on annotated data with spatial linguistic features (Wu et al., 2023a; Yuan, 2011). The\nresulting qualitative spatial relations, enriched by contextual narratives (Wallgr¨ un et al., 2015),\nprovide a foundation for computational models that link natural language semantics to the\nstructured representations of physical space.\n2.2. Formalism of topological relations and conceptual neighborhoods\nIn the field of GIS, attempts have been made to formalize the conversion between quantitative\ncomputational models of spatial relations and qualitative spatial representations from human\ndiscourse (Chen et al., 2015; Cohn and Hazarika, 2001). In Clementini et al. (1994), topological\nrelations are defined as spatial relations that are preserved under such transformations as\nrotation, scaling, and rubber sheeting. For topological spatial relations, region connection\ncalculus (RCC) (Randell et al., 1992) and point-set topology intersection models (IM), e.g., 4-IM\nbased on intersections of the boundaries and interiors of two objects (Egenhofer and Franzosa,\n1991), and 9-IM which also considers the exteriors of two objects (Egenhofer and Herring,\n1991), are widely used approaches. RCC-8 (Cui et al., 1993) is a set of eight jointly exhaustive\nand pairwise disjoint relations defined for regions. The basic relations include topological\npredicates: equal (EQ), externally connected (EC), disconnected (DC), partially overlaps (PO),\ntangential (TPP/TPPi) and nontangential (NTPP/NTPPi) relations, which have been shown\nto be cognitively adequate to be well distinguished by humans (Renz and Nebel, 1998).\nPoint-set topology intersection models analyze whether intersections between the interiors,\nboundaries, and exteriors of two objects are empty or non-empty point sets. The Dimensionally\nExtended 9-intersection model (DE-9IM) (Clementini et al., 1993) further considered the\ndimensionality of each geometry in the intersection matrix so that the 9-IM is not a binary\noperation of intersects. Based on the DE-9IM model, five mutually exclusive relations are\nidentified (Clementini and Di Felice, 1996), including {disjoint, touches (meets), crosses,\noverlaps, within}. the Open Geospatial Consortium (OGC) later added {intersects, contains,\nequals} to the set for the convenience of GIS software users, and included in the GeoPandas\nPython package for programmers. The recent development of RCC∗-9 expands the dimensions of\nRCC-8 and allows for a unified framework to model topological spatial relations (Clementini and\nCohn, 2014, 2024). However, since DE-9IM predicates were selected for better user interaction\nand have been implemented by OGC, this work focuses on DE-9IM. In Mark and Egenhofer\n(1994), human subject testing was conducted to evaluate their model for spatial relations\nbetween lines and regions. The participants were presented with pairs of lines or regions and\nwere asked to rate the spatial relation between them using a Likert scale that ranged from “no\nrelation” to “strongly related”. The pairs of lines and regions were generated based on the 19\ntopologically distinct spatial relations defined in the authors’ model. The human judgments\nwere then compared to the predicted spatial relations generated by their model. The results\nshowed that the model’s predicted topological spatial relations matched the human judgments\nwith a high degree of accuracy, indicating the effectiveness of the model in capturing human\nperception of topological spatial relations.\nIn both RCC and IM lineage, the idea of smooth transitions from one topological relation\nto another has been discussed early on. This means that, for example, if two polygon objects\nare disjoint, they would first require a touch relationship before moving to overlap. In this\nsense, some relationships are more similar or closer to each other than others, and this is\nknown as the conceptual neighborhood of topological relations. Figure 1 shows the neighborhood\ngraphs using the RCC-8 (Figure 1a) and 9-IM (Figure 1b) nomenclature. Since the DE-9IM\nexample only preserves the connection of a topological relation with its “closest” relation,\nthe inside/contains do not connect with equal in the graph. In addition to the conceptual\n4\nneighborhood, Egenhofer and Al-Taha (1992) proposed a formula for calculating the topological\ndistance between topological relations using matrix representations, where the smaller distance\nmeans more similar between the two topological relations. We adopt the topological distance\nfor evaluation later in this paper to provide a more nuanced perspective on whether LLMs’\ndifferentiation of topological relations aligns with human perception.\n(a)\n (b)\nFigure 1.: The conceptual neighborhood of topological relations in RCC-8 (Randell et al., 1992)\non the left (redrawn for comparison) and 9-IM (Egenhofer and Al-Taha, 1992) on the right.\n2.3. Large language models and GeoAI foundation models\nThe launch of ChatGPT by OpenAI (2022) marked a significant turning point, drawing\nwidespread interest in Large Language Models (LLMs) and conversational AI from the public.\nLanguage-based foundation models boast an impressive range of parameters, from 110 million\nin BERT (Devlin et al., 2018) to 1.5 billion in GPT-2 (Radford et al., 2019), and up to 137\nbillion in LaMDA (Google’s Bard) (Thoppilan et al., 2022) and 175 billion in GPT-3 (Brown\net al., 2020), demonstrating a significant variation in network architectures, scale, and purposes.\nDespite these differences, they share a common achievement: they have acquired a sophisticated\nunderstanding of language patterns and semantics, setting new performance standards in\nnatural language processing tasks. Other types of foundation models include vision-based (e.g.,\nvision transformer–ViT (Dosovitskiy et al., 2020) and segment anything model–SAM (Kirillov\net al., 2023)) and vision-language multimodal foundation models (e.g., Flamingo with 80 billion\nparameters (Alayrac et al., 2022) and GPT-4 with over 1 trillion parameters (Achiam et al.,\n2023)). These pre-trained foundation models have been applied directly or transferred to a\nwide range of cross-domain tasks after fine-tuning or few-shot/zero-shot learning, e.g. education\n(Kasneci et al., 2023), healthcare (Yang et al., 2022), transportation (Zheng et al., 2023), etc.\nThese foundation models have been trained on large-scale datasets that also contain\ngeographical knowledge such as descriptions of locations and places in textual documents as well\nas spatial elements in maps, geo-referenced photos, and satellite imagery. Recently, researchers\nand institutions have begun the early exploration of integrating foundation models into GeoAI\nresearch and education. For example, Mai et al. (2024) found that task-agnostic LLMs have\nthe capability to surpass fully supervised deep learning models designed for specific tasks in\nunderstanding geospatial semantics, including toponym recognition, health data time-series\nforecasting, urban function, and scene classifications. Hu et al. (2023) fused a few geo-knowledge\nexamples into GPT models to improve the extraction of location descriptions from disaster-\nrelated social media messages. Manvi et al. (2023) found that geospatial knowledge can be\neffectively extracted from LLMs with auxiliary map data from OpenStreetMap. Additionally,\nspatial-context-aware prompts with pre-trained visual-language models can improve the\naccuracy of urban land use classification and urban function inference (Huang et al., 2024; Wu\net al., 2023b), In GIS, evaluations have been conducted to assess the qualitative spatial reasoning\ncapabilities of LLMs in identifying and reasoning spatial relations using symbolic representations\nof spatial objects, such as RCC-8 (Cohn, 2023; Cohn and Blackwell, 2024a) and cardinal\n5\ndirections (Cohn and Blackwell, 2024b). While LLMs perceive the spatial structure through\nsequences of textual input (Yamada et al., 2023) and leverage commonsense reasoning during\ntheir inference process (Cohn and Hernandez-Orallo, 2023), they also demonstrate human-like\nmisconceptions and distortions about space (Fulman et al., 2024). Several studies (Fernandez\nand Dube, 2023; Mai et al., 2022a; Tucker, 2024) have proposed integrating vector data as a\nbackbone for spatial reasoning. GPT-4 has shown the capability to generate coordinates for\noutlines of countries, rivers, lakes, and continents that approximate their actual geographic\nlocations (Das, 2023). In Ji and Gao (2023), LLM-generated embeddings can preserve geometry\ntypes and some coordinate information in the WKT representation of geometries. However,\nperforming qualitative spatial reasoning and executing spatial tasks from implicit textual\ndescriptions involving coordinates remains a significant challenge (Majic et al., 2024). In\naddition, geospatial analysis workflows and operations can be automated when combing LLMs\nwith spatial analysis tools (Li and Ning, 2023; Zhang et al., 2024). ChatGPT can even achieve a\npromising grade when taking an introduction to GIS exam (Mooney et al., 2023). In the field of\nCartography, Tao and Xu (2023) explored the use of ChatGPT-4 for creating thematic maps and\nmental maps with appropriate prompts. However, Zhang et al. (2023) pointed out the ethical\nconcerns on AI-generated maps’ inaccuracies, misleading information, unanticipated features,\nand reproducibility. In August 2023, NASA and IBM released their GeoAI Foundation Model–\nPrithvi, which was trained on NASA’s Earth Observation remote sensing imagery (i.e., the\nharmonized Landsat and Sentinel-2 satellite dataset) (Jakubik et al., 2023) and has been found\nto have a good performance and transferability on flood inundation mapping (Li et al., 2023).\nAlongside such remarkable achievements, there are concerns that need to be addressed together\nwith the development and advancement of foundation models for GeoAI and geosciences (i.e.,\nGeo-Foundation Models), such as geographical bias, diversity, spatial heterogeneity, limited\nhuman annotations, sustainability, privacy and security risks (Hu et al., 2024; Janowicz, 2023;\nRao et al., 2023; Xie et al., 2023).\n3. Methodology\n3.1. Preliminaries and Workflow\nThis research focuses on assessing the ability of LLMs to represent textual descriptions\nof geometries and understand topological spatial relations between geometric objects. The\noverall framework of this research is shown in Figure 2. Given a study area, we first\nretrieve spatial objects from both a spatial database and a textual description about places\nfrom a Web document knowledge database (e.g., DBpedia/Wikipedia). When the documents\ncontain vernacular description of topological relations between two places, formalized DE-\n9IM topological spatial relations will be extracted from the spatial footprints (geometries) in\nthe format of triplets as ground truth. The obtained geometric, attributive, and relational\ninformation is used as input for downstream tasks (e.g., qualify topological relations, process\nspatial query, and convert vernacular description of relations), where task-specific prompts are\ndesigned accordingly. The task output from the LLMs is then compared to the ground truth\ntopological relation triplets to evaluate their ability to encode and reason about geometries\nand topological spatial relations. The following subsections will further provide details on each\nevaluation task and the corresponding workflow. The definitions and notations used in this\npaper are listed in Table 1.\n3.2. Determining topological spatial relations\nIn the original work of DE-9IM (Clementini et al., 1993), the five defined topological predicates\n{disjoint, touches (meets), crosses, overlaps, within } were considered mutually exclusive.\nHowever, the statement no longer holds with the introduction of “contains” and “equals” to\nthe set by the OGC standard. Therefore, to ensure the uniqueness of the topological spatial\n6\nFigure 2.: Overview of the workflow in this research.\nTable 1.: Notations\nNotation Description\nA/B The objectID of spatial objects A or B\ngA The geometry of A that can be processed in GIS tools\nGeomType (A) The geometry type of A, (e.g. Point, LineString, and Polygon when gA is a simple feature)\ng◦\nA The interior of gA\ndim(g)\nThe dimension of a geometry g.\ndim(g) =\n\n\n\n− g = ∅\n0 g contains at least one Point without Linestrings or Polygons\n1 g contains at least one Linestrings without Polygons\n2 g contains at least one Polygon\nWKT (A) The WKT format of gA\nEnc(A) The location encoding of gA using an LLM model to encode WKT (A)\nR The set of predicates to represent the topological spatial relations in this research, i.e.{equals,\ndisjoint, crosses, touches, contains, within, overlaps }, as defined by OGC and implemented\nin GeoPandas.\nrel A predicate that can be used to represent the topological spatial relation, rel ∈ R\nRel(A, B) The topological spatial relation between the subject A and the object B\n[Enc(A); Enc(B)] The concatenation of the embeddings of A and B\nD(rel1, rel2) The topological distance between two relationsrel1 and rel2 on the conceptual neighborhood\ngraph (Egenhofer and Al-Taha, 1992)\nSA(rel, B) The relevancy score of a retrieved subject A given the reference object B and the desired\ntopological spatial relation rel.\nrelations between two objects, we interpret “within” as “within (but not equals)” and “contains”\nas “contains (not equals)” in this work. Accordingly, we modify the decision tree in Clementini\net al. (1993) to do the reasoning about the topological relations between two spatial objects,\nas illustrated in Figure 3. Based on the decision process, the topological spatial relations do\nnot apply to every combination of geometry types. The definitions and possible geometry type\ncombinations of the seven predicates used in this research are listed in table 2. Several visual\nexamples of the topological spatial relations between two geometries can be seen in Figure 10.\n7\nFigure 3.: The decision tree for the topological spatial relations.\nTable 2.: The named topological spatial predicates with the 9-intersection Boolean code (T:\ntrue; F: false; *: free value) and corresponding applicable geometry type combinations of a\npredicate.\nPredicate with 9-intersection code Geometry Type Combination\nequals: T*F**FFF* Point/Point, LineString/LineString, Polygon/Polygon\nwithin: T*F**F***\nPoint/LineString, Point/Polygon,\nLineString/LineString, LineString/Polygon,\nPolygon/Polygon\ncontains: T*****FF* LineString/Point, LineString/LineString,\nPolygon/Point, Polygon/LineString, Polygon/Polygon\noverlaps: T*T***T** LineString/LineString, Polygon/Polygon\ntouches: FT******* or F***T****\nPoint/LineString, Point/Polygon,\nLineString/Point, LineString/LineString, LineString/Polygon,\nPolygon/Point, Polygon/LineString, Polygon/Polygon\ncrosses: T*T****** LineString/LineString, LineString/Polygon, Polygon/LineString\ndisjoint: FF*FF**** Applicable to ALL\n3.3. Representing geospatial data as text\nAn embedding is a multi-dimensional numeric vector representation of objects to capture\nthe complex patterns and relationships in the data. While researchers have explored different\napproaches to embed geometries using spatially explicit models (Mai et al., 2022b; Yan et al.,\n2017; Zhu et al., 2022), this study presents a novel perspective by hypothesizing that LLMs can\neffectively encode the WKT format of geospatial vector data (points, polylines, and polygons)\nand preserve crucial geometric information. We adopt sentence embedding models (Logeswaran\nand Lee, 2018; Neelakantan et al., 2022; Reimers and Gurevych, 2019) to generate neural\nembeddings of the input geometry WKT strings, which allows for the comparison and retrieval\nof spatial information through the semantic search (Hu et al., 2015; Muennighoff, 2022).\n3.4. Evaluation Tasks\n3.4.1. Topological spatial relation qualification\nIn Wolter and Wallgr¨ un (2012), spatial relation qualification is defined as the process of inferring\nqualitative spatial relations from quantitative data. The first task aims to leverage LLMs to\nclassify the topological spatial relationships between subject entity A and object entity B into\none of seven predefined topological predicates (in Section 3.2), combined with their geometry\n8\ntypes. The input and output of Task 1are described as follows:\nInput: The input for this task is the WKT representations of geometries A and B, denoted\nas WKT(A) and WKT(B). Example inputs:\n• WKT (A): POINT (-89.3551 43.123)\n• WKT (B): POLYGON ((-89.3552 43.124, -89.355 43.124, -89.355 43.122, -89.3552 43.122,\n-89.3552 43.124))\nOutput: The output is a tuple that describes the topological spatial relationship between the\ntwo geometries, in the format of (GeomType (A), predicate, GeomType(B)). Given the example\ninputs, the expected output of a correct classification would be:\n• (Point, within, Polygon)\nUse Case: Task 1 can be relevant to linking the geometries that occur in the same spatial\ncontext. For example, suppose one document already provides location, geometry and attribute\ninformation on housing resources and public transportation facilities. In that case, the LLM\nmay directly use geographic information and other contexts to suggest affordable and accessible\nhousing by public transportation facilities.\nFigure 4.: The workflow for the topological spatial relation qualification task.\nThe workflow of task 1 is shown in Figure 4. Given an input triplet that describes the\ntopological spatial relation between subject A and object B, i.e., (subject, predicate, object), we\nfirst retrieve the WKT strings, and Geometry types of A and B. We then adopt two approaches\n(embedding-based and prompt-based) to perform the task, utilizing an appropriate LLM, to\nfunction as either a text encoder or a reasoner. For encoding, a pre-trained sentence embedding\nmodel generates the embeddings of the geometries of A and B. The embeddings are concatenated\nas the input for a random forest classifier (Breiman, 2001). For reasoning, a more powerful\ngenerative model, such as GPT-4 and DeepSeek-R1, are employed to perform the task defined\nin the prompt. Four prompt engineering techniques are adopted to potentially guide the LLMs\ntowards producing a more valid and accurate output of the topological spatial relation, including\nstandard zero-shot learning, standard few-shot learning (Radford et al., 2019), few-shot chain-\nof-thought (CoT) prompting (Wei et al., 2022), and zero-shot COT prompting(Kojima et al.,\n2022). In few-shot CoT, we follow the decision tree in Figure 3 to generate the intermediate\nsteps to determine the topological spatial relations as examples. While the identification of\ntopological spatial relations might appear straightforward to the human brain, it involves multi-\nstep reasoning. The DE-9IM framework (Clementini et al., 1993) decomposes the problem\ninto intersections of the boundaries, interiors, and exteriors of two geographic entities, with\n9\ndimensional requirements that map to topological predicates intuitive to users. We hypothesized\nthat few-shot prompting and explicit reasoning steps, guided by CoT, could improve the model’s\nperformance on this qualification task. The example inputs and outputs of the topological spatial\nrelation qualification task using the above-mentioned different prompt engineering techniques\nare illustrated in Figure 5.\nFigure 5.: Topological spatial relation qualification example inputs and outputs with different\nprompt techniques.\nThe metrics for evaluating the topological spatial relation qualification task are as follows.\n(1) Validity\na. Valid format of the output: LLMs should follow the instructions to use the given format\nof the output in (Geometry Type A, Predicate, Geometry Type B).\nb. Valid geometry types: LLMs should preserve the Geometry Type A and Geometry Type\nB from the given WKT format of geometries.\nc. Valid combinations of geometry types for the topological predicates as shown in table 2.\n(2) Accuracy\nFor valid outputs, we can compute the accuracy when the output topological spatial predicate\nmatches the ground truth.\n(3) Topological distance in the conceptual neighborhood graph\nIn this work, we use the shortest path distance between two topological predicates in the\nconceptual neighborhood graph (see Figure 1), where the distance of each edge equals 1. Since\nFigure 1 was originally proposed for region-to-region (Polygon/Polygon) relations in 9IM, we\nmapped their topological predicates to the seven DE-9IM predicates that we use. For other\ngeometry type combinations, we refer to Mark and Egenhofer (1994) and Reis et al. (2008)\nto extract the conceptual neighborhood graphs. With the topological distance measurement,\n10\nwe can further analyze which pairs of predicates can easily confuse LLMs and whether such\nconfusion is directed, by comparing the false-negative and false-positive results.\n3.4.2. Spatial query processing\nIn Sack and Urrutia (1999), a generic spatial query is defined as the retrieval of subjects from a\nset of candidate geometric entities that are in a specific relation rel with the query object B on\nthe basis of geometric information only. Our second task aims to evaluate whether LLMs can\njointly encode a topological relation and one geometry to capture the feasible geometries that\nmeet the query requirement. The input and output of the Task 2are as follows:\nInput: The input for this task is the WKT representations of geometry B, denoted as\nWKT (B), and a given predicate of topological spatial relations rel. Example input:\n• Predicate: within\n• WKT (B): POLYGON ((-89.3552 43.124, -89.355 43.124, -89.355 43.122, -89.3552 43.122,\n-89.3552 43.124))\nOutput: The output is the identifier of a subject entity A whose topological spatial\nrelationship with B is described by the predicate.\nUse Case: Task 2 is valuable for retrieving textual reports that involve locations, spatial\nlayouts, and geospatial semantics. This analysis relies on accurate queries using spatial\npredicates. For instance, it would be beneficial to analyze the selection of a nearby competitor’s\nsite report when considering opening a business in the same neighborhood.\nFigure 6.: The workflow for the spatial query processing task.\nThe evaluation workflow of Task 2 is shown in Figure 6. Given a query specifying the\ntopological spatial relation rel with the query object B (WKT (B)), we first retrieve the\nsubjects from the study area spatial database as ground truth. We format the query as the\ninput to an LLM using two approaches. First, this query can be directly formulated as a\nsentence, such as “Retrieve a geometry within POLYGON ((-89.3552 43.124, -89.355 43.124,\n-89.355 43.122, -89.3552 43.122, -89.3552 43.124).” Alternatively, synthetic geometries can be\ncreated using a generative model to expand the query, connecting the query with the search\nspace. The (expanded) query text is inputted into the sentence embedding model to generate\nthe embeddings. The geometries in WKT format for spatial entities are also processed by\nthe same embedding model, to generate the embeddings ( Enc(g1), Enc(g2), ..., Enc(gn)). The\nmost relevant subject geometries are retrieved based on the cosine similarity of their geometry\n11\nembeddings and the query embeddings. We perform the evaluation as a link prediction task in\nthe “filtered” setting (Bordes et al., 2013), which excludes other subjects related to B by the\ntopological predicate rel from the database and concentrates on the retrieval of the subject in\nthe triplet. This approach addresses the biases introduced by the significant difference in the\nnumber of spatially related subjects across predicates and the objects. Finally, the retrieved\nsubjects are evaluated by their actual topological spatial relation to the reference subject.\nIn the following, we introduce how to format the direct query and the expanded query with\nLLM-generated geometries in detail:\n(1) Direct Query\nGiven the WKT format of the geometry of a known reference object (e.g., LINESTRING (-\n89.4534 43.035, -89.454 43.0351)) and a designated topological spatial relation (e.g., “crosses”),\nthe query formulation is as follows: “Retrieve a geometry that crosses the LINESTRING (-\n89.4534 43.035, -89.454 43.0351).” If the search focuses on a specific geometry type, the query\ncan be articulated as “Retrieve a LINESTRING geometry that crosses the LINESTRING (-\n89.4534 43.035, -89.454 43.0351).”\n(2) Expanded query with LLM-generated geometries\nIn Carpineto and Romano (2012) and Hu et al. (2015), (geospatial) query expansion is used to\naugment the user’s original query with new features (e.g., geographic or thematic characteristics)\nthat share a similar meaning as the expected output of semantic search. The method can address\nthe lack of semantic similarity between the query and the desired geometry. We extend the\nQuery2Doc model (Wang et al., 2023) to the spatial query expansion, where we leverage an\nLLM to generate a synthetic geometry that can possibly be the response to the query. The\nprompt template for the generation of geometric objects or subjects is listed in Figure 7. We\nadopt the following prompting approaches for geometry generation.\n• Zero-shot: LLMs generate geometries directly from the given spatial query.\n• Zero-shot + Self-check: LLMs are asked to verify the spatial relations before generating\nthe output.\n• Few-shot: Give a few pairs of example queries and corresponding subjects while\nmaintaining spatial relations and object geometry type.\n• Few-shot + Negative examples: Apart from the plausible examples, we also include the\nnegative examples that are not the correct responses for the given query. The examples\nare formatted as “Retrieve a Geometry Type which ... Good Response:... Bad Response:\n...”\nWe further incorporate the LLM-generated geometries into the spatial queries to assess the\nusefulness of the expanded queries.\nThe evaluation includes two parts: First, LLMs’ ability to generate valid synthetic geometries\nas a basis for the expanded queries. Second, query processing performance through semantic\nsearch using both direct queries and expanded queries.\n(1) Validity of the LLM-generated geometries\na. Valid WKT format of geometries to be successfully parsed by the GIS tool for creating\ngeometry instances.\nb. Correct topological spatial relation Rel with the query object B.\n(2) Mean Reciprocal Rank (MRR) and Hits@K of the retrieval performance\nWe employ two commonly used metrics in geographic information retrieval, Mean Reciprocal\nRank (MRR) (Yan et al., 2017) and Hits@K, in the “filtered” setting (Bordes et al., 2013). A\ndesirable model is characterized by higher MRR and Hits@K values.\n12\nFigure 7.: Prompt template used for geometry generation in the spatial query processing task.\n3.4.3. Conversion of vernacular relation descriptions\nIn Chen et al. (2018), a vernacular description of spatial relations between places is an alternative\nto formal spatial relations in metric space, which occurs in everyday communication in a flexible\nformat of a preposition, verb, phrase, or even implicit text description. The third task aims\nto evaluate how LLMs can convert the vernacular description (i.e., everyday language) of a\ntopological relationship between two geographic entities into one of seven predefined topological\npredicates based on the given context. This task is inspired by LLM’s commonsense model of\nthe world and naive geographical knowledge about space, and the domain-specific knowledge of\nthe formalism in calculus to bridge the gap between the vernacular(narrative) descriptions and\nthe formalized topological predicates. For example, ChatGPT is able to provide the rationale\nbehind the statement “When an island is in the middle of a lake, the island touches the lake if\nthe lake is considered as a separate region (not fully containing the island)” by identifying the\nlake in this scenario is a double-border object using commonsense knowledge reasoning (rather\nthan precise geometries). It then maps this understanding to the “touches” topological relation,\napplying expertise in the GIS domain. The input and output of the Task 3are as follows:\nInput: The input for this task includes a sentence that describes the topological relationship\nbetween two places in everyday language, along with the contextual information of the two\nplaces. Example input:\n• Sentence: Place A is home to Place B\n• Context: Place A is a city. Place B is a university\nOutput: The output will rephrase the sentence using the formalized topological predicates.\n• Answer: Place A contains Place B\nUse Case: Parsing vernacular descriptions of spatial relations between places into formal ones\ncan better support the users interacting in natural language and the use of spatial analysis tools\nthat rely on formal topological relations. For example, interpreting vague terms in travel reports\nto determine if cross-border human behavior exists and interpreting the territorial changes and\nalignment of contemporary boundaries in the historical context.\nWe adopt the workflow in Figure 8 to evaluate the capability of LLMs in task 3. The workflow\nbegins with collecting geographic entities from a Web document knowledge database (DBpedia,\nstructured knowledge based on Wikipedia), where named entity recognition is used to extract\n13\nplace names and vernacular spatial relation descriptions. These place names, such as “UW-\nMadison” and “Madison, Wisconsin” are then used to retrieve relevant geographic data and\ncorresponding attributes from a spatial database to provide context such as geometry type and\nplace type, with their topological spatial relations identified through GIS tools. The collected\nspatial relations between two places are formatted as “A {vernacular topological relation } B”\n(e.g., A is home to B) for evaluation, where A and B are symbolic placeholders representing\ntwo places. The specific locations in geometries are not disclosed, allowing for a generalized\ndiscussion of topological spatial relations without actual geographic context. The context will\nbe provided at the end of the text input to support in-context reasoning. The context evaluated\nin our experiments is in Table 3. The prompts are crafted with the template as shown in\nFigure 9 and fed into an LLM (e.g., GPT-4) to convert vernacular descriptions to topological\nspatial relations. We run the model multiple times to identify the possible converted topological\npredicates and the preference of an LLM. The output topological predicates are then compared\nwith the ground truth predicates calculated by the GIS tool for evaluation. We also compare the\nperformance when no contextual information is provided. This workflow allows us to evaluate\nthe effectiveness of LLMs for analyzing informal topological relations between two entities and\nto assess the impact of contextual information on performance.\nFigure 8.: The workflow for the vernacular relations conversion task.\nFigure 9.: Prompt template used in the vernacular relation conversion task.\n14\nTable 3.: The textual description from DBPedia and topological predicate conversion examples.\nDescription Context Predicate Example\nbordered by No context touches Glendora is bordered by Azusa.\n→ A is bordered by B.\nalong Geometry type crosses Luling is along the San Marcos River.\n→ A is along B. A is Polygon, and B is LineString.\nlocated on Place type crosses Located on Interstate 10, Weimar is a small community.\n→ A is located on B. A is city, B is highway.\non the shore of Place name overlaps Racine is located on the shore of Lake Michigan.\n→ A is on the shore of B. A is Racine in Wisconsin, B is lake Michigan.\nThe evaluation metrics for the vernacular relation conversion task are as follows.\n(1) Frequency: The count of correctly returned predicates across all experiments.\n(2) Accuracy: The ratio of the frequency of correctly returned predicates to the total number\nof generated outputs for each conversion pair.\n(3) Entropy: The information entropy (Shannon, 1948) of the returned predicates assesses\nthe level of randomness in converting vernacular descriptions into topological predicates.\nSmaller entropy values indicate a higher likelihood of certain predicates being preferred\nover others. The metric is computed as:\nH = −\nX\nrel∈R\nprel log(prel),\nwhere prel represents the probability of a specific topological predicate rel appearing in\nthe outputs for the given context-conditioned conversion pair.\n4. Data and Experiments\n4.1. Data processing\n4.1.1. Extracting topological spatial relations from spatial database\nWe construct real-world multi-sourced geospatial datasets for our study. The study area for\nTask 1 and Task 2is the city of Madison, Wisconsin, United States. The following datasets\nare collected.\n• OpenStreetMap road network data (including links and intersections) using OSMnx. 1\n• Points of interest (POIs) categorized by SLIPO. 2\n• Land parcels from Wisconsin Statewide Parcel Map Initiative . 3\n• Census block groups from U.S. Census Bureau. 4\nOur evaluation tasks focus on the spatial objects with Point, LineString , and Polygon\ngeometry types, assessing their topological spatial relations. All the computations are performed\nby using the GeoPandas package in Python.\nTask 1 and Task 2share the same dataset of triplets. For each combination of {geometry\ntype A, predicate, geometry type B}, we obtain 200 triplets. Among these, 160 are allocated\nfor training the random forest classification model, while the remaining 40 triplets are reserved\nfor evaluation. Additionally, we set aside 25 extra triplets as candidate examples to facilitate\n1http://osmnx.readthedocs.io/\n2http://slipo.eu/\n3https://www.sco.wisc.edu/parcels/data/\n4https://www2.census.gov/geo/tiger/TIGER2020PL/LAYER/BG/\n15\nfew-shot learning. Due to the imbalanced distribution of topological spatial relations within the\nreal-world dataset, we employ multiple strategies for sampling a sufficient number of triplets for\nfair comparisons:\n(1) For topological spatial relations including “within”, “contains”, “overlaps”, “touch” and\n“crosses”, we opt to select a subset of spatial objects and conduct spatial joins to obtain\nthe required triplets.\n(2) Regarding the “equals” relationship. we manually created the equivalent spatial entities to\npreserve the topological spatial relations while making direct identification from geometry\ncoordinate matching challenging.\na. For Point, include only points with identical coordinates.\nb. For LineString, interpolate an additional 10% of points along the lines, ensuring that\nthe added points did not alter the original shape.\nc. For Polygon, loop the origin point and interpolate additional points along the\nboundaries.\n(3) We restrict the occurrence of “disjoint” to cases where the subject geometry does not touch\nor overlap but lies within a smaller buffer of the objects (i.e., nearby entities), to avoid\neasy identification when two spatial entities are far apart, thus enhancing the evaluation\non the differentiation of topological predicates.\nIn Task 2, we further exclude the “disjoint” relations since most real-world geographic entities\nare disjoint from each other, yielding 40 ×26=1040 triplets for retrieving the subject or object\ngeographic entities.\n4.1.2. Topological spatial relations from DBpedia/Wikipedia\nFor Task 3, we have gathered a total of 1078 unique triplets based on the recognized geographic\nentities from DBpedia/Wikipedia documents, which we combine with everyday descriptions\nof topological spatial relationships. We then utilize this extracted data to evaluate GPT-4’s\ncapabilities in task 3 as described in Section 3.4.3.\nSpecifically, we downloaded and refined place descriptions within the States of Wisconsin,\nTexas, and California from the knowledge base DBpedia 5, which is the linked data format of\nWikipedia and has been previously used in place name disambiguation task (Hu et al., 2014).\nThe data extraction and processing steps are structured as follows:\n(1) Named entity recognition: From each administrative region’s abstract “dbo:abstract”),\nwe extract all place names that can be found in OpenStreetMap, forming the basis for\nsubsequent topological spatial relation identification.\n(2) Textual spatial relation extraction: For each pair of place names within a DBpedia\nabstract, we use GPT-4 to extract topological spatial relation terms found between the\nentities in the text. When hierarchical place relationships are described, our approach only\ncaptures direct relations between a subject and each individual object, omitting implicit\ntransitive relations among the objects themselves. For instance, from the sentence “a city\nA in a County B, State C,” we extract (A, in, B) but skip (A, in, C) and (B, in, C).\n(3) Manual verification: We manually review all the extracted spatial relation descriptions to\nensure that they indicate topological relations and that the use of the two place names as\nsubjects or objects in the sentence is semantically correct.\n(4) Description unification: The text descriptions on DBpedia are standardized for\nconsistency. For example, phrases like “is home to”, “home to” or “home of”, are unified\nas “is home to”.\n(5) Context-conditioned conversion pairs extraction: We identify how vernacular descriptions\ndepend on the following context to convert them to formal topological predicates.\n5https://www.dbpedia.org/\n16\na. Invariant to context: If a vernacular description consistently corresponds to the same\ntopological predicate, we create a context-conditioned conversion pair (description,\npredicate, N/A).\nFor descriptions that can be converted to multiple formal topological predicates, we\nassociate them with specific contexts for one-on-one conversion.\nb. Place types as context: If grouping by description, place type A and place type B,\nresults in a unique topological predicate, we create the pair (description, predicate,\nplace type A/place type B). Place types are extracted from OpenStreetMap data\ntags.\nc. Geometry types as context: If grouping by description, geometry typeA and geometry\ntype B, results in a unique predicate, we create the pair (description, predicate,\ngeometry type A/geometry type B).\nd. Place names as context: Each pair of places can have a unique topological\nrelationship. We create the pair (description, predicate, place name A/place name\nB), assuming the LLMs have some knowledge about place names.\nIt is possible that more than one context can assist with one-on-one mapping from a\nvernacular description to a formal topological predicate. We may retain multiple contexts\nto compare their effectiveness. For example, the conversion between “is bordered by”\nand “touches” can be identified using place types (is bordered by, town/city, touches),\ngeometry types (is bordered by, Polygon/MultiPolygon, touches), and place names (is\nbordered by, Aliso Viejo, California/Laguna Beach, California, touches).\n(6) Data filtering: Only frequently observed context-conditioned conversion pairs are retained\nfor evaluation.\na. In the cases of invariant to context, place type as context, and geometry types as\ncontext, we retain pairs that occur at least 5 times for evaluation.\nb. In the case of place names as context, we first filter (description, predicate) that\noccur at least 5 times, and then sample 5 pairs for each combination.\nAmong the 1078 records extracted from DBPedia abstracts of places in the states of\nWisconsin, Texas, and California, 212 explicitly refer to directional and distance spatial relations\nand were thus removed as this research focused on topological relations. The analytical results\nof task 3 using the remaining records will be presented in Section 5.3.\n4.2. Experiment Models\nIn this research, we perform evaluation tasks based on the following models:\n4.2.1. Embedding models\nWe encode WKT geometries into embeddings and process spatial queries using “text-\nembedding-ada-002” and “text-embedding-3-large” provided by OpenAI 6, with output\nembedding dimensions of 1536 and 3072 respectively.\n4.2.2. Reasoning models\nIn our evaluation tasks, we employ GPT-3.5-turbo, GPT-4, and DeepSeek-R1-14B as the LLM-\nbased reasoning models. While performance varies by task, these models have demonstrated\npotential in commonsense reasoning and in-context learning on certain benchmarks. GPT-3.5-\nturbo and GPT-4 are primarily optimized for few-shot learning, whereas DeepSeek-R1-14B\nemphasizes zero-shot capabilities and may experience a decline in performance when few-shot\nprompting is applied (Guo et al., 2025).\n6https://openai.com/blog/new-embedding-models-and-api-updates\n17\n4.2.3. Model settings\n(1) Random Forest classifier: The number of estimators (trees) in the Task 1 classifier is set\nto 100.\n(2) Temperature settings for GPT-3.5-turbo and GPT-4: For the topological relation\nqualification task, we set the temperature to 0 to encourage more deterministic outputs.\nHowever, achieving full reproducibility remains challenging, even with a temperature of\n0, as discussed by Blackwell et al. (2024). Conversely, generating synthetic geometries\nto support semantic search in Task 2, employs a higher temperature of 0.7 for greater\ncreativity.\n(3) Temperature settings for DeepSeek-R1-14B: The temperature of the topological relation\nqualification task is set to be 0.6 to better exploit the reasoning ability of DeepSeek, given\nits emphasis on deeper, more deliberate thinking.\n5. Results\n5.1. Topological spatial relation qualification\n5.1.1. Validity of the output\nBefore diving into the effectiveness of using LLMs to qualify spatial relationships, a validity\ncheck is necessary because of the inherent nondeterministic nature of generative AI models.\nFurthermore, beyond validating the output as a valid format of {Geometry type A, predicate,\nGeometry type B}, it is essential to focus on grounding the qualitative spatial reasoning in the\nmatched geometry types and topological relations.\nThe validity results of the output are shown in Table 4. The random forest classifier using\nthe LLM-generated embeddings consistently produced valid output on the test dataset. This\nhighlights that the sentence embedding models can effectively preserve geometry types in the\nWKT format of geometries, aligning with previous research which encoded WKT by aggregating\nthe token embeddings from GPT-2 and BERT (Ji and Gao, 2023). While GPT-4 and GPT-3.5-\nturbo largely adhere to the instructions in the desired format, even with the CoT generation, it\nis more challenging for DeepSeek-R1-14B to strictly output the desired format (but still achieved\nover 0.9 validity accuracy). When tested with few-shot prompting, the DeepSeek-R1-14B largely\nignored the provided examples and adhered to its typical reasoning patterns. As a result, we\ndid not include these results in our evaluation. The highest validity of GPT-4 model suggests\nthat a language model that is characterized by a larger number of parameters, broader training\ndata, and stronger alignment with human instructions, may also possess a better understanding\nof the definitions of the DE-9IM topological predicates.\n5.1.2. Classification metrics\nTable 5 presents the results of the topological predicate classification task. Both the embedding-\nbased random forest and geospatial question-answering with GPT models can achieve an\naccuracy of over 0.6. This suggests that identifying topological spatial relationships from the\nWKT format of geometries with LLMs is promising but remains challenging. Failure to recover\nthe topological spatial relations from embeddings suggests a potential information loss through\ntext tokenization. Incorrectly classified topological relations often cluster within the conceptual\nneighborhoods or resemble each other (with a small distance), while confusion may also arise\nfrom the diverse semantics of topological spatial predicates.\nFor GPT-3.5-turbo and GPT-4, among the four types of prompts (introduced in\nSection 3.4.1), few-shot learning achieved the best performance with pairs of geometries and\ntheir topological relationships for LLMs to learn in context. GPT-4 with few-short promoting\nachieved 0.66 accuracy. The findings highlight the importance of prompt engineering in the\nuse of LLMs and the critical role of understanding spatial contexts in improving geospatial\n18\nTable 4.: The validity accuracy of the outputs (N/A: not available).\nApproach LLM Prompt Format Geometry type Predicate\nRandom\nForest\ntext-embedding\n-ada-002 N/A 1 1 1\ntext-embedding\n-3-large N/A 1 1 1\nQuestion\nanswering\nGPT-3.5-turbo\nZero-shot 0.959 1 0.911\nZero-shot-dim 0.999 1 0.927\nFew-shot 1 1 0.901\nZero-shot-CoT 0.944 1 0.944\nFew-shot-CoT 0.998 1 0.894\nGPT-4\nZero-shot 1 0.996 0.997\nZero-shot-dim 1 0.999 0.999\nFew-shot 1 0.999 0.992\nZero-shot-CoT 0.984 0.990 0.968\nFew-shot-CoT 1 0.999 0.999\nDeepSeek-R1-14B Zero-shot 0.936 0.996 0.913\nZero-shot-dim 0.919 0.998 0.913\nTable 5.: The topological predicate classification metrics.\nApproach LLM Prompt Accuracy Dist(Incorrect)\nRandom\nForest\ntext-embedding\n-ada-002 N/A 0.633 1.449\ntext-embedding\n-3-large N/A 0.632 1.419\nQuestion\nanswering\nGPT-3.5-turbo\nZero-shot 0.423 1.331\nZero-shot-dim 0.408 1.360\nFew-shot 0.479 1.595\nZero-shot-CoT 0.443 1.370\nFew-shot-CoT 0.465 1.174\nGPT-4\nZero-shot 0.632 1.238\nZero-shot-dim 0.635 1.212\nFew-shot 0.666 1.272\nZero-shot-CoT 0.610 1.256\nFew-shot-CoT 0.627 1.225\nDeepSeek-R1-14B Zero-shot 0.534 1.257\nZero-shot-dim 0.557 1.260\nquery processing accuracy and reliability. However, chain-of-thought (CoT) prompts, which have\ndemonstrated improvement in many other tasks (Wei et al., 2022), did not yield the expected\nbenefit in our spatial reasoning evaluation experiments. As mentioned by Yang et al. (2024), CoT\nreasoning can sometimes induce unreliable or counterproductive outputs in spatial reasoning\ntasks. Upon analyzing the generated rationale, we observed that when LLMs are prompted\nwith “Let’s think step by step”, they attempt to check the topological spatial predicates one\nby one based on their respective definitions from the OGC standard. Few-shot-CoT prompts,\non the other hand, were explicitly designed with examples grounded in scientific definitions and\nlogical decision processes proposed in Clementini et al. (1993), aiming to “teach” the models to\nreason about topological spatial relations from analysis on interiors, boundaries, and exteriors.\nDespite this structured approach, the accuracy declined due to cascading errors in intermediate\nsteps, such as failing to determine whether the interiors of two geometries intersect at the\nvery beginning. With an explicit reasoning process, DeepSeek-R1-14B outperformed GPT-3.5-\nturbo (20B parameters). Analysis of its thought generation reveals that, rather than always\niteratively checking candidate answers, the model often employed more intuitive reasoning\nstrategies, such as mental mapping (e.g., “Let me plot them mentally”) and self-verification\n(e.g., “In WKT, a LINESTRING is just a sequence of points connected by straight lines. If it\nstarts and ends at the same point, it doesn’t automatically become a polygon”). Although these\nreasoning patterns may appear convincing when interpreting individual geometries, they often\nfall short when reasoning about spatial relations between two geometries. This is mainly due\n19\nto an overreliance on superficial, linear interpretations of coordinate information, rather than a\nholistic understanding of topological spatial relationships across the plane.\n5.2. Spatial query processing\nBased on the superior performance in task 1, the experiments in task 2 only used GPT-4 as the\ngeometry generator and text-embedding-3-large as the embedding model.\n5.2.1. Direct query\nWe first identified an effective query format for geospatial semantic search (Hu et al., 2015),\nwhich is the foundation for applying query expansion in understanding geospatial semantics.\nAs shown in Table 6, specifying the subject geometry type would achieved higher performance\ndue to a narrowed mapping space to the same geometry type. In the following experiments,\nwe assumed that the user query with the geometry type (e.g. retrieving a street from the\nspatial database implies LineString), and further investigated the factors that may impact the\neffectiveness of query expansion using LLMs.\nTable 6.: Spatial query performance comparison results.\nTarget Query Format MRR Hits@5 Hits@10 Hits@20\nSubject\nDirect query Abstract as “geometry” 0.081 0.131 0.161 0.194\nSpecify the subject geometry type 0.152 0.212 0.26 0.29\nExpanded query\nDirect query +\none LLM-generated geometry 0.18 0.238 0.278 0.328\nDirect query +\nthree LLM-generated geometry 0.169 0.232 0.28 0.32\nObject\nDirect query Original predicate 0.105 0.131 0.17 0.211\nReversed predicate 0.152 0.219 0.256 0.297\nExpanded query\nOriginal predicate +\none LLM-generated geometry 0.15 0.215 0.261 0.302\nReversed predicate +\none LLM-generated geometry 0.179 0.248 0.294 0.333\n5.2.2. Synthetic geometry generation\nAn effective LLM-generated geometry is expected to maintain the same topological spatial\nrelation with the given object as the subject entity while being close to the subject entity in the\nembedding space. Table 7 compares the validity of the LLM-generated geometries produced by\ndifferent prompting approaches. We find that 1) GPT-4 effectively comprehends spatial queries\nand generates geometries in a valid WKT format, and 2) GPT-4 demonstrates a notable level of\nspatial reasoning regarding the reference object, even in the zero-shot setting, as indicated by the\nhigh relation-preserving accuracy (over 0.72) and the low topological distance in the conceptual\nneighborhood graph (Reis et al., 2008). Figure 10 presents examples of the LLM-generated\ngeometries generated by GPT-4. In the following section, we will check the usefulness of such\nsynthetic geometries generated by zero-shot prompts in enriching spatial query processing.\nTable 7.: Validity of LLM-generated geometries using different prompts.\nPrompt Valid WKT Geometry Type Predicate Topological Distance\nZero-shot 0.999 1 0.763 1.142\nZero-shot-Check 0.998 1 0.755 1.075\nFew-shot 0.996 1 0.728 1.177\nFew-shot-Negative 0.997 1 0.754 1.212\n20\nFigure 10.: Synthetic geometries generated by GPT-4 for LineString/LineString relations\n5.2.3. Expanded query on subject retrieval\nAs shown in Table 6, retrieving a subject based on the embeddings encoded from the expanded\nspatial queries remains challenging. However, including an LLM-generated geometry enhanced\nthe probability of ranking the target subject higher among all candidates. Over 23% of the\nsubjects were ranked within the top 5 candidates. But adding additional synthetic geometries\ndid not appear to provide further improvements.\n5.2.4. Performance comparison on object retrieval\nWhile the above experiment primarily focuses on retrieving the subject in a triplet, we proceed to\nevaluate the performance of object retrieval. For a given triplet, we tested the queries formulated\nwith either the original predicate describing the spatial relationship from the subject to the\nobject (e.g., “Retrieve a Point which A contains”), or the reversed predicate referring to the\nsubject (e.g., “Retrieve a Point which is within A”). The results of object retrieval are also\nsummarized in Table 6.\nAmong the object-retrieval query formats, the queries with the original predicate that\nmaintained the subject-to-object directionality yielded worse performance. When we manually\nreversed the topological spatial relation and treated the object as the subject, the performance\nmatched its counterpart for subject retrieval in Table 6, highlighting the importance of\nstructuring spatial queries to align with everyday language patterns commonly used for spatial\nreference.\n5.3. Vernacular topological relation conversion\nIn Task 3, we collected textual descriptions of topological spatial relations between two places\nand attempted to identify mapping patterns between these descriptions and the corresponding\ncontext-conditioned topological spatial relations. These mappings were then used as input to\nGPT-4 (giving its superior performance in previous tasks) to evaluate their ability to convert\ntextual descriptions into formal topological spatial relations.\n5.3.1. Conversion pairs invariant to context\nTable 8 lists the six descriptions that consistently map to the same topological relationship in\nour dataset. However, the results show varying levels of conversion accuracy from vernacular\ndescriptions to preferred formal topological predicates. While the ground truth topological\nrelationships were likely to be implied from “share border with” and “is the location of”, “is\nan enclave of” was interpreted as within instead of touches or disjoint. Even though GPT-4\ncould infer an overlaps relation from “has part of the population in” in all ten experiments,\nthe model might be unsure about its answer and would provide multiple topological predicate\nalternatives. Despite the subtle difference between “midway” and “halfway”, a higher entropy\nof “halfway” indicates greater randomness in the conversion.\n21\nTable 8.: Result of topological relation conversion pairs invariant to context.\nDescription Predicate Frequency Accuracy Entropy\nshare border with touches 10 1.000 0.000\nhas part of the population in overlaps 10 0.588 0.435\nis the location of contains 9 0.818 0.244\nis midway between C and disjoint 6 0.600 0.560\nis halfway between C and disjoint 6 0.500 0.817\nis an enclave of touches 1 0.100 0.167\n5.3.2. Conversion pairs conditioned on place types or geometry types\nThe comparisons between scenarios with and without place-type/geometry-type context are\nillustrated in Table 9 and Table 10 respectively. Our initial hypothesis was that including\ncontextual information in the prompt would reduce ambiguity, resulting in a higher frequency\nof correct predicate predictions, improved accuracy, and lower entropy using LLMs. However,\nthese improvements were highly instance-dependent and not consistently observed across all the\nconversion pairs evaluated in our experiments, indicating GPT-4’s limitation in considering all\npossible interpretations of the given vernacular description. This limitation was also evident in\npairs with 0 accuracy, where the model consistently outputted the same incorrect answer. In\nother instances, GPT-4 still struggled to determine the appropriate topological predicates for\ncertain vernacular descriptions.\nTable 9.: Result of topological relation conversions using place type as context\nDescription Predicate Spatial Context Frequency Accuracy\nAccuracy\nwithout Con-\ntext\nEntropy\nEntropy\nwithout Con-\ntext\nis home to contains city/amenity 10 1 1 0 0\nborders touches city/municipality 10 1 1 0 0\nis located in within town/county 10 1 1 0 0\nis located in within city/state 10 1 1 0 0\nis bordered by touches town/city 10 1 1 0 0\nis adjacent to touches city/municipality 10 1 0.909 0 0.157\nborders touches city/city 10 1 1 0 0\nis in within village/county 10 0.909 0.909 0.157 0.157\nis located in within village/county 10 0.909 1 0.157 0\nis partly in overlaps city/county 10 0.833 1 0.232 0\nis bounded by touches city/city 7 0.7 0.333 0.314 0.327\nconnect C and crosses industrial/city 8 0.4 0.474 0.52 0.355\nextend into overlaps city/county 5 0.357 0.421 0.561 0.491\nis surrounded by touches city/city 2 0.167 0 0.232 0\nis between C and touches town/town 0 0 0.3 0.211 0.773\nis surrounded by touches town/city 0 0 0 0 0\nis within touches city/municipality 0 0 0 0 0\nTable 10.: Result of topological relation conversions using geometry type as context\nDescription Predicate Spatial Context Frequency AccuracyAccuracy\nwithout\nContext\nEntropyEntropy\nwithout\nContext\nis in within Polygon/MultiPolygon 10 1 0.909 0 0.157\nis neighboring touches Polygon/Polygon 10 1 1 0 0\nis bordered by touches Polygon/MultiPolygon 10 1 1 0 0\nis the county seat of within Polygon/Polygon 10 1 1 0 0\nextend into overlaps Polygon/Polygon 8 0.714 0.421 0.307 0.491\nconnect C and crosses LineString/MultiPolygon 9 0.412 0.474 0.348 0.355\nis surrounded by touches Polygon/MultiPolygon 0 0 0 0 0\nis on crosses Polygon/LineString 0 0 0 0 0.327\n22\nTable 11.: Accuracy and Entropy changes for conversion pairs with place names\nAccuracy Topological relation conversion pairs\n(Order by the absolute values in change, ∗ with entropy reduction)\nImproves\n1) is bounded by →touches∗\n3) is suburb of →touches\n5) is part of →within∗\n7) is partly in →touches\n9) is between C and →disjoint∗\n2) is surrounded by→touches\n4) is on→crosses\n6) is between C and →touches∗\n8) is suburb of →disjoint\n10) is near→touches\nUnchanged\nRemains 1:\n1) includes→contains\n3) is bordered by →touches\n5) is located in →within\nRemains 0:\n7) is bordered by →disjoint\n9) on the shore of →overlaps\n2) borders→touches\n4) is neighboring→touches\n6) is the county seat of →within\n8) is in→overlaps∗\n10) is within→touches\nDeclines\n1) is mostly in →overlaps\n3) is along→crosses∗\n5) is situated on →overlaps\n7) extend into→overlaps\n9) is home to →contains\n2) is near→disjoint\n4) is partly in →overlaps\n6) is adjacent to→touches\n8) is in→within\n10) connect C and →crosses∗\n5.3.3. Conversion pairs with place names\nThe accuracy and entropy of the topological relation conversions with place names were also\ncompared to the metrics obtained without the context. As shown in Table 11, mentioning place\nnames did not necessarily improve the accuracy of the conversion or guide the LLM to a preferred\nanswer. GPT-4’s explanation indicates that 1) It focuses on the topological relationships between\ngeneral geographic locations or boundaries rather than leveraging specific knowledge about\neach place; 2) The approach tends to exclude predicates possibly with inaccurate and abstract\ngeometries. For instance, in analysis “A is along B” when A is Brazos Bend, Texas, and B\nis Brazos River, the reasoning begins with “This suggests a specific geographical relationship\nbetween a place (A) and a river (B). The term ‘along’ typically indicates that A is situated in a\nlinear arrangement adjacent to B, but not necessarily crossing it or being contained within it. ”\n6. Discussion\nIn this section, we would like to further discuss whether the confusion between the topological\npredicates aligns with the corresponding conceptual neighborhood of topological spatial\nrelations (Egenhofer and Al-Taha, 1992; Egenhofer and Mark, 1995; Formica et al., 2018),\nthe confusion in geometry generation, and the confusion in vernacular description conversion.\n6.1. Confusion between topological predicates in topological spatial relation\nqualification\nWhen using GPT-4 (zero-shot learning) for topological spatial relation qualification, the\nconfusion matrices for all the geometry type combinations are drawn in Figure 11. We compare\nthe topological predicate pairs that may confuse GPT models with the classic conceptual\nneighborhood graphs in Figure 1. The observations are twofold: 1) The most frequently confused\ntopological spatial relation for a given predicate depends on the geometry types involved. For\nexample, consider the predicate “overlaps.” In a Linestring/Linestring relationship, it is rarely\nclassified correctly and is often confused with “crosses”, “equals” or “touches”. However, in a\nPolygon/Polygon relationship, “overlaps” is more likely to be correctly identified, though it may\noccasionally be confused with “contains” or “disjoint”. Another illustrative example involves\nthe predicate “touches.” A Point “touches” a Linestring or Polygon, or a Linestring “touches” a\nPolygon is frequently mistaken as ”within” while such confusion is less between two geometries of\n23\nthe same dimension, such as two Polygons or two Linestrings. These examples suggest the varied\ndegree to which an LLM understands formal geometry boundaries associated with geometry\ntypes, particularly their dimensions, which is crucial in identifying formal topological spatial\nrelations. However, the constraint of formal definitions may contradict common conceptual\ninterpretations, such as excluding a polygon from containing its boundary, leading to fewer\noccurrences in GPT-4’s response. 2) For the same geometry type combination, distinguishing\ncertain pairs of topological spatial relations is more challenging than others. These pairs mostly\nfall within the conceptual neighborhood, though exceptions exist. Take “Linestring/Linestring”\nas an example. GPT-4 can identify “crosses”, “disjoint”, and “equals” more accurately. However,\nit struggles with predicates like “contains” and “overlaps,” frequently confusing them with\n“crosses” or “touches”. The four topological spatial relations all require that the two geometries\nshare elements like points or line segments and might be interchangeable in daily use. This\nchallenge highlights that the ambiguous semantics of these predicates can encompass scenarios\nbroader than their strict formal definitions. Overall, the issues observed in Task 1 actually\nreflect an alignment with everyday spatial reasoning. While formal definitions are precise and\ndimension-contingent, everyday language and intuitive reasoning often blur the distinctions.\nFigure 11.: Confusion matrices between topological predicates in relation qualification.\n6.2. Confusion between topological predicates in geometry generation\nThe confusion pattern changes when leveraging an LLM to generate geometries given a spatial\nquery and the required geometry type, as shown in Figure 12. The findings can be summarized\nas follows: 1) Directionality in describing the topological relation between two geometry types\nmatters. For example, generating a Polygon that “crosses” a LineString proves challenging for\n24\nGPT-4, while the reversed query—generating a LineString that “crosses” a Polygon—is handled\nmore effectively. Similarly, the model is more successful in generating a Polygon that “contains” a\nPoint, LineString, or another Polygon but struggles to produce a Point, LineString, or Polygon\nthat is “within” a Polygon. This asymmetry can be attributed to the model’s approach of\nextracting coordinates from the query geometry to construct the second geometry. This reliance\nlimits the model’s ability to conceptualize spatial relationships beyond the provided coordinates.\n2) The geometry type of the reference object affects the results. Figure 13 provides examples\nof LineString/LineString topological spatial relations where the generated topological spatial\nrelations were different from the predicate in the spatial query. As observed from these examples,\neven if the spatial queries specify the reference object geometry type as LineString, the model\nsometimes applies definitions for Polygons when a line forms a closed shape. In this case, when we\nmanually changed the reference object geometry type into Polygon and recompute its topological\nrelations with the LLM-generated geometry, 223 out of 391 queries (across all prompts) with\nclosed geometries were found to exhibit the desired topological relationship. This observation\nsuggests GPT-4’s perception based on the provided coordinates over the geometry type specified\nin the text, inspiring us to further explore the cognition potential of the LLMs.\nFigure 12.: Confusion matrices between spatial predicates in geometry generation.\n6.3. Confusion between topological spatial relations in conversion\nWhile GPT-4 can accurately convert several vernacular descriptions to corresponding formal\npredicates, there are instances where its performance falls short. This can be attributed to the\nmismatch between crispy geometry and the vague human perception of place boundaries. We can\nfurther divide it into three categories. 1) The abstraction of spatial entities’ shapes in the spatial\n25\nFigure 13.: Invalid synthetic geometries generated by GPT-4 for LineString/LineString relations\nwith close-shape objects.\ndataset may differ from those used in descriptions. For example, when converting “is along”\nfor Brazos Bend, Texas, and Brazos River, Texas, GPT-4 returned “touches” when considering\nBrazos Bend as a region (Polygon) and “within” when considering Brazos Bend as a Point. 2)\nThe computed topological spatial relations can be sensitive to the marked shape points, while\nhuman perception can tolerate such systematic errors, yielding the description of the relations in\nthe conceptual neighborhood of the ground truth. A typical example is when the ground truth\nlabel of “is suburb of” is “disjoint”, but the two cities look like they “touch” each other on the\nmap. 3) Official geographic boundaries might differ from people’s perception of a place (Gao\net al., 2017). In our dataset, “is within” and “is an enclave of” can map to “touches”. But the\nLLM would constantly output “within”. For instance, the City of Shullsburg, Wisconsin, and the\nTown of Shullsburg, Wisconsin, illustrate this discrepancy 7. Although the City of Shullsburg\nis enclosed by the Town of Shullsburg, the city boundary is separated from the town boundary,\ncreating a hole in the town boundary. In summary, even though GPT-4’s responses can be\npartly interpreted from the conceptual neighborhood of topological spatial relations, challenges\nremain due to the vagueness of real-world geographic entity boundaries and human perception\nof shapes and places.\n7. Conclusion and future work\nThis study focuses on the evaluation of the ability of LLMs including GPT-3.5, GPT-4,\nand DeepSeek-R1-14B to process, represent, and reason with topological spatial relations.\nConsequently, we designed a workflow to assess the efficacy of LLMs in addressing three typical\nproblems on topological spatial relations. The core idea involves converting geometric objects\ninto textual strings (WKT), which can then be decoded and utilized for spatial reasoning.\nThe first task, topological spatial relation qualification, focuses on determining if such textual\nrepresentation can retain the necessary geometric information for deriving named topological\npredicates. The second task explores the feasibility of conducting geospatial queries through\nsemantic search, where LLMs can generate a geometry to augment the query and also generate\nembeddings. The third task presents an everyday scenario where an LLM serves as a translator\nto convert vernacular descriptions of spatial relations into formalized topological predicates\nbased on their capability to understand linguistic patterns.\nFrom the multi-source geospatial datasets, we extract triplets to represent topological spatial\nrelations in real-world spatial configurations. Using the triplets as input, we have compared\nthe performance on the three evaluation tasks with ground truth data. In Task 1, both the\nrandom forest and GPT-based reasoning models can identify most relations correctly (over 0.6\naccuracy on average), while some relations can be confounding. For GPT-3.5-turbo and GPT-\n4, few-shot prompt engineering is essential to improve the performance while CoT prompting\n7https://en.wikipedia.org/wiki/Shullsburg_(town),_Wisconsin\n26\nstrategy had a negative impact on our topological spatial relation inference task. The thought\ngeneration process and the self-verification allow DeepSeek-R1-14B to perform spatial reasoning\nmore intuitively and outperformed GPT-3.5-turbo in accuracy. Further comparison with the\nconceptual neighborhood allows for a more quantitative understanding of the errors. Even\nthough task 2 further verifies the challenge of replacing spatial queries with semantic search.\nHowever, improvements can be observed when we customize the query and augment it with\nLLM-generated geometries. The LLM-generated geometries are not only valid WKT but also\nhave high accuracy (up to 0.76) in preserving topological spatial relations (or within their\nconceptual neighbors). In task 3, the improvement of LLMs to reduce ambiguity in spatial\nqueries is relatively limited. However, in most cases, the generated outputs fall into the\nconceptual neighborhood of the ground-truth topological predicate. Moreover, given various\ncontexts, the changes in the preferred response show the ability of the LLM to reason about it\nusing commonsense knowledge and the typical spatial configurations. Interestingly, adding the\ngeometry-type context in prompts has derived more performance improvement compared to the\ncases with adding the place-type context, but the performance of adding context or without\ncontext varies by instance.\nIn conclusion, through the three tasks and intensive experiments, we systematically approach\nthe overarching question of LLM’s ability in understanding geometry information and their\ntopological spatial relations, moving from the broader challenge to more targeted strategies\ninvolving spatial context, tailored prompting techniques, and specialized domain knowledge in\nGIScience.\nHowever, it is essential to acknowledge the limitations of our work. First, our focus\nwas primarily on in-context learning, and we did not explore fine-tuning approaches, which\ncould potentially yield further performance improvements. Retrieval-Augmented Generation\n(RAG) (Lewis et al., 2020) presents a promising approach for enhancing the qualitative\nspatial reasoning capabilities LLMs by integrating external spatial databases, GIS tools\nand domain-specific knowledge from GIScience. Unlike in-context learning, which allows for\nintuitive qualitative spatial reasoning, the effective implementation of RAG relies on the precise\ngeneration of formalism-based spatial queries from natural language input along with reliable\nhigh-resolution datasets. Improving the translation from natural language to into symbolic\nform and logic also opens the door to neurosymbolic approaches (Sheth et al., 2023), in which\nLLMs serve as translators that convert user text input into symbolic representations, which\nare then processed by symbolic engines with strong capabilities in explanation, verification,\nand formal reasoning. Realizing this potential requires addressing the challenges identified\nin this work, such as ambiguities in linguistic spatial descriptions, conceptual neighborhood\nrelationships in topological reasoning, and the correct use of available GIS functions and\nanalytical workflows. Additionally, our dataset is currently limited to the city and state levels,\nand further investigation into multi-scale spatial relations is still needed to fully capture\nthe complexity of spatial interactions across different geographical scales and heterogeneous\ndatasets. Moreover, the scope of this work is limited to topological relations in natural languages.\nWhile we can handcraft datasets with ground truth labels of formalized predicates for evaluation,\nthe mathematical computation makes LLMs less competent than spatial databases. Directional\nand distance relations introduce more vagueness in language use due to factors such as shape\nand scale, especially when two places cannot be viewed as points. In future work, we plan to\nexplore other spatial relations using datasets such as the Geograph project 8, which provides\nrich expressions of various spatial relations associated with geometries, text descriptions and\nphotos (M Hall et al., 2011). This will enable us to evaluate LLMs’ or multi-modal foundation\nmodels’ capabilities (e.g., vision-language geo-foundation models) in geospatial reasoning from a\nmore comprehensive perspective (Mai et al., 2024). Lastly, we rephrased our own text instead of\ndirectly using paragraphs from DBpedia to allow for flexibility in introducing different context\ninformation. However, this approach may result in some loss of authenticity in language use,\nsuch as anaphora, which is prevalent in original text documents and worth exploring in future\n8http://www.geograph.org.uk\n27\nresearch.\nIn summary, this research demonstrates the promise and limitations of using state-of-the-\nart LLMs to analyze topological spatial relations, while offering insights for future research\nof advancing LLMs with geographical knowledge, aiming to develop GeoAI foundation models\ncapable of qualitative spatial reasoning and other spatial intelligence tasks.\nDisclosure statement\nThe authors report there are no competing interests to declare.\nData and Codes Availability Statement\nThe data and codes supporting the main findings of this study are available at Figshare:\nhttps://doi.org/10.6084/m9.figshare.25127135.v1 and the GitHub repository at https:\n//github.com/GeoDS/GeoFM-TopologicalRelations.\nAcknowledgments\nSong Gao acknowledges the funding support from the National Science Foundation funded AI\ninstitute [Grant No. 2112606] for Intelligent Cyberinfrastructure with Computational Learning\nin the Environment (ICICLE). Any opinions, findings, and conclusions or recommendations\nexpressed in this material are those of the author(s) and do not necessarily reflect the views of\nthe funder(s).\nNotes on contributors\nYuhan Ji: Yuhan Ji is a PhD student GIScience at the Department of Geography, University of\nWisconsin-Madison. Her main research interests include transportation, geospatial data science,\nand GeoAI approaches to human mobility.\nSong Gao: Dr. Song Gao is an associate professor in GIScience at the Department of Geography,\nUniversity of Wisconsin-Madison. He holds a Ph.D. in Geography at the University of California,\nSanta Barbara. His main research interests include GeoAI, geospatial data science, spatial\nnetworks, human mobility and social sensing.\nYing Nie: Ying Nie is an undergraduate student at the Department of Computer Sciences,\nUniversity of Wisconsin-Madison. Her main research interests include geospatial data science\nand GeoAI.\nKrzysztof Janowicz: Dr. Krzysztof Janowicz is a Professor in Geoinformatics at the\nDepartment of Geography and Regional Research, University of Vienna. His research interests\nare knowledge representation and reasoning as they apply to spatial and geographic data, e.g.\nin the form of knowledge graphs.\nIvan Majic: Dr. Ivan Majic is a Postdoc researcher in Geoinformatics at the Department of\nGeography and Regional Research, University of Vienna. His research interests include Spatial\nData Science, GeoAI, and Qualitative Spatial Reasoning.\nReferences\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt,\nJ., Altman, S., Anadkat, S., et al. (2023). GPT-4 technical report. arXiv preprint arXiv:2303.08774 .\n28\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K.,\nReynolds, M., et al. (2022). Flamingo: a visual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716–23736.\nBlackwell, R. E., Barry, J., and Cohn, A. G. (2024). Towards reproducible llm evaluation: Quantifying\nuncertainty in llm benchmark scores. arXiv preprint arXiv:2410.03492 .\nBordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O. (2013). Translating\nembeddings for modeling multi-relational data. Advances in neural information processing systems ,\n26.\nBreiman, L. (2001). Random forests. Machine Learning, 45:5–32.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in Neural\nInformation Processing Systems, 33:1877–1901.\nCarlson, L. A. and Logan, G. D. (2001). Using spatial terms to select an object. Memory & Cognition ,\n29(6):883–892.\nCarpineto, C. and Romano, G. (2012). A survey of automatic query expansion in information retrieval.\nACM Computing Surveys (CSUR) , 44(1):1–50.\nCervone, G., Sava, E., Huang, Q., Schnebele, E., Harrison, J., and Waters, N. (2016). Using Twitter\nfor tasking remote-sensing data collection and damage assessment: 2013 boulder flood case study.\nInternational Journal of Remote Sensing , 37(1):100–124.\nChen, H., Vasardani, M., Winter, S., and Tomko, M. (2018). A graph database model for knowledge\nextracted from place descriptions. ISPRS International Journal of Geo-Information , 7(6):221.\nChen, J., Cohn, A. G., Liu, D., Wang, S., Ouyang, J., and Yu, Q. (2015). A survey of qualitative spatial\nrepresentations. The Knowledge Engineering Review , 30(1):106–136.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph,\nN., Brockman, G., et al. (2021). Evaluating large language models trained on code. arXiv preprint\narXiv:2107.03374.\nClementini, E. and Cohn, A. G. (2014). RCC*-9 and CBM. In International Conference on Geographic\nInformation Science, pages 349–365. Springer.\nClementini, E. and Cohn, A. G. (2024). Extension of RCC*-9 to complex and three-dimensional features\nand its reasoning system. ISPRS International Journal of Geo-Information , 13(1):25.\nClementini, E. and Di Felice, P. (1996). A model for representing topological relationships between\ncomplex geometric features in spatial databases. Information sciences, 90(1-4):121–136.\nClementini, E., Di Felice, P., and Van Oosterom, P. (1993). A small set of formal topological relationships\nsuitable for end-user interaction. In International symposium on spatial databases , pages 277–295.\nSpringer.\nClementini, E., Sharma, J., and Egenhofer, M. J. (1994). Modelling topological spatial relations:\nStrategies for query processing. Computers & graphics , 18(6):815–822.\nCohn, A. G. (2023). An evaluation of ChatGPT-4’s qualitative spatial reasoning capabilities in RCC-8.\narXiv preprint arXiv:2309.15577 .\nCohn, A. G. and Blackwell, R. E. (2024a). Can large language models reason about the region connection\ncalculus? arXiv preprint arXiv:2411.19589 .\nCohn, A. G. and Blackwell, R. E. (2024b). Evaluating the Ability of Large Language Models to\nReason About Cardinal Directions. In Adams, B., Griffin, A. L., Scheider, S., and McKenzie, G.,\neditors, 16th International Conference on Spatial Information Theory (COSIT 2024) , volume 315\nof Leibniz International Proceedings in Informatics (LIPIcs) , pages 28:1–28:9, Dagstuhl, Germany.\nSchloss Dagstuhl – Leibniz-Zentrum f¨ ur Informatik.\nCohn, A. G. and Hazarika, S. M. (2001). Qualitative spatial representation and reasoning: An overview.\nFundamenta informaticae, 46(1-2):1–29.\nCohn, A. G. and Hernandez-Orallo, J. (2023). Dialectical language model evaluation: An initial appraisal\nof the commonsense spatial reasoning abilities of llms. arXiv preprint arXiv:2304.11164 .\nCohn, A. G. and Renz, J. (2008). Qualitative spatial representation and reasoning. Foundations of\nArtificial Intelligence, 3:551–596.\nCui, Z., Cohn, A. G., and Randell, D. A. (1993). Qualitative and topological relationships in spatial\ndatabases. In Advances in Spatial Databases: Third International Symposium, SSD’93 Singapore, June\n23–25, 1993 Proceedings 3, pages 296–315. Springer.\nDas, S. (2023). Evaluating the capabilities of large language models for spatial and situational\nunderstanding. PhD thesis, PhD thesis, Thesis (MA). University of Cambridge.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). BERT: Pre-training of deep bidirectional\n29\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 .\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M.,\nMinderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929 .\nDu, S., Qin, Q., Chen, D., and Wang, L. (2005). Spatial data query based on natural language spatial\nrelations. In Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005.\nIGARSS’05., volume 2, pages 1210–1213. IEEE.\nEgenhofer, M. J. and Al-Taha, K. K. (1992). Reasoning about gradual changes of topological\nrelationships. In Goos, G., Hartmanis, J., Frank, A. U., Campari, I., and Formentini, U., editors,\nTheories and Methods of Spatio-Temporal Reasoning in Geographic Space, volume 639 of Lecture Notes\nin Computer Science , pages 196–219. Springer Berlin Heidelberg, Berlin, Heidelberg.\nEgenhofer, M. J. and Franzosa, R. D. (1991). Point-set topological spatial relations.International journal\nof geographical information systems, 5(2):161–174.\nEgenhofer, M. J. and Herring, J. R. (1991). Categorizing binary topological relations between regions,\nlines, and points in geographic databases. Technical report, Department of Surveying Engineering,\nUniversity of Maine, Orono, Maine, USA.\nEgenhofer, M. J. and Mark, D. M. (1995). Modelling conceptual neighbourhoods of topological line-region\nrelations. International journal of geographical information systems , 9(5):555–565.\nFernandez, A. and Dube, S. (2023). Core building blocks: Next gen geo spatial gpt application. arXiv\npreprint arXiv:2310.11029.\nFormica, A., Mazzei, M., Pourabbas, E., and Rafanelli, M. (2018). Approximate answering of queries\ninvolving polyline–polyline topological relationships. Information Visualization, 17(2):128–145.\nFrank, A. U. (1992). Qualitative spatial reasoning about distances and directions in geographic space.\nJournal of Visual Languages & Computing , 3(4):343–371.\nFreksa, C., Habel, C., and Wender, K. F. (1998). Spatial cognition: An interdisciplinary approach to\nrepresenting and processing spatial knowledge, volume 1404. Springer Science & Business Media.\nFrolov, S., Hinz, T., Raue, F., Hees, J., and Dengel, A. (2021). Adversarial text-to-image synthesis: A\nreview. Neural Networks, 144:187–209.\nFulman, N., Memduho˘ glu, A., and Zipf, A. (2024). Distortions in judged spatial relations in large\nlanguage models. The Professional Geographer, 76(6):703–711.\nGao, S. and Goodchild, M. F. (2013). Asking spatial questions to identify GIS functionality. In 2013\nFourth International Conference on Computing for Geospatial Research and Application , pages 106–\n110. IEEE.\nGao, S., Hu, Y., and Li, W. (2023). Handbook of Geospatial Artificial Intelligence . CRC Press.\nGao, S., Janowicz, K., Montello, D. R., Hu, Y., Yang, J.-A., McKenzie, G., Ju, Y., Gong, L., Adams,\nB., and Yan, B. (2017). A data-synthesis-driven method for detecting and extracting vague cognitive\nregions. International Journal of Geographical Information Science , 31(6):1245–1271.\nGramacki, P., Martins, B., and Szyma´ nski, P. (2024). Evaluation of code llms on geospatial code\ngeneration. In Proceedings of the 7th ACM SIGSPATIAL International Workshop on AI for Geographic\nKnowledge Discovery, pages 54–62.\nGuo, D., Ge, S., Zhang, S., Gao, S., Tao, R., and Wang, Y. (2022). DeepSSN: A deep convolutional\nneural network to assess spatial scene similarity. Transactions in GIS, 26(4):1914–1938.\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al.\n(2025). Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv\npreprint arXiv:2501.12948.\nGuo, R. (1998). Spatial objects and spatial relationships. Geo-spatial Information Science, 1(1):38–42.\nHead, C. G. (1984). The map as natural language: a paradigm for understanding. Cartographica: The\nInternational Journal for Geographic Information and Geovisualization , 21(1):1–32.\nHu, Y., Goodchild, M., Zhu, A.-X., Yuan, M., Aydin, O., Bhaduri, B., Gao, S., Li, W., Lunga, D., and\nNewsam, S. (2024). A five-year milestone: reflections on advances and limitations in GeoAI research.\nAnnals of GIS , 30(1):1–14.\nHu, Y., Janowicz, K., and Prasad, S. (2014). Improving wikipedia-based place name disambiguation\nin short texts using structured data from dbpedia. In Proceedings of the 8th workshop on geographic\ninformation retrieval, pages 1–8.\nHu, Y., Janowicz, K., Prasad, S., and Gao, S. (2015). Metadata topic harmonization and semantic\nsearch for linked-data-driven geoportals: A case study using ArcGIS Online. Transactions in GIS ,\n19(3):398–416.\nHu, Y., Mai, G., Cundy, C., Choi, K., Lao, N., Liu, W., Lakhanpal, G., Zhou, R. Z., and Joseph,\n30\nK. (2023). Geo-knowledge-guided gpt models improve the extraction of location descriptions from\ndisaster-related social media messages. International Journal of Geographical Information Science ,\n37(11):2289–2318.\nHuang, W., Wang, J., and Cong, G. (2024). Zero-shot urban function inference with street view\nimages through prompting a pretrained vision-language model. International Journal of Geographical\nInformation Science, 38(7):1414–1442.\nJakubik, J., Roy, S., Phillips, C., Fraccaro, P., Godwin, D., Zadrozny, B., Szwarcman, D., Gomes,\nC., Nyirjesy, G., Edwards, B., et al. (2023). Foundation models for generalist geospatial artificial\nintelligence. arXiv preprint arXiv:2310.18660 .\nJanowicz, K. (2023). Philosophical foundations of GeoAI: Exploring sustainability, diversity, and bias in\nGeoAI and spatial data science. arXiv preprint arXiv:2304.06508 , pages 1–17.\nJanowicz, K., Gao, S., McKenzie, G., Hu, Y., and Bhaduri, B. (2020). GeoAI: spatially explicit artificial\nintelligence techniques for geographic knowledge discovery and beyond. International Journal of\nGeographical Information Science, 34(4):625–636.\nJanowicz, K., Van Harmelen, F., Hendler, J. A., and Hitzler, P. (2015). Why the data train needs\nsemantic rails. AI Magazine, 36(1):5–14.\nJi, Y. and Gao, S. (2023). Evaluating the effectiveness of large language models in representing textual\ndescriptions of geometry and spatial relations (short paper). In 12th International Conference on\nGeographic Information Science (GIScience 2023) , volume 43, pages 1–6. Schloss-Dagstuhl-Leibniz\nZentrum f¨ ur Informatik.\nJones, C. B., Abdelmoty, A. I., Finch, D., Fu, G., and Vaid, S. (2004). The spirit spatial search engine:\nArchitecture, ontologies and spatial indexing. In Geographic Information Science: Third International\nConference, GIScience 2004, Adelphi, MD, USA, October 20-23, 2004. Proceedings 3 , pages 125–139.\nSpringer.\nKasneci, E., Seßler, K., K¨ uchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G.,\nG¨ unnemann, S., H¨ ullermeier, E., et al. (2023). ChatGPT for good? on opportunities and challenges\nof large language models for education. Learning and Individual Differences, 103:102274.\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg,\nA. C., Lo, W.-Y., et al. (2023). Segment anything. arXiv preprint arXiv:2304.02643 .\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022). Large language models are zero-shot\nreasoners. Advances in neural information processing systems , 35:22199–22213.\nKordjamshidi, P., Van Otterlo, M., and Moens, M.-F. (2011). Spatial role labeling: Towards extraction\nof spatial relations from natural language. ACM Transactions on Speech and Language Processing\n(TSLP), 8(3):1–36.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K¨ uttler, H., Lewis, M., Yih, W.-\nt., Rockt¨ aschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems , 33:9459–9474.\nLi, W., Lee, H., Wang, S., Hsu, C.-Y., and Arundel, S. T. (2023). Assessment of a new GeoAI foundation\nmodel for flood inundation mapping. In Proceedings of the 6th ACM SIGSPATIAL International\nWorkshop on AI for Geographic Knowledge Discovery , pages 102–109.\nLi, Z. and Ning, H. (2023). Autonomous GIS: the next-generation AI-powered GIS.International Journal\nof Digital Earth , 16(2):4668–4686.\nLogeswaran, L. and Lee, H. (2018). An efficient framework for learning sentence representations. arXiv\npreprint arXiv:1803.02893.\nLoglisci, C., Ienco, D., Roche, M., Teisseire, M., and Malerba, D. (2012). An unsupervised framework\nfor topological relations extraction from geographic documents. In Database and Expert Systems\nApplications: 23rd International Conference, DEXA 2012, Vienna, Austria, September 3-6, 2012.\nProceedings, Part II 23, pages 48–55. Springer.\nM Hall, M., Smart, P. D., and Jones, C. B. (2011). Interpreting spatial language in image captions.\nCognitive processing, 12:67–94.\nMai, G., Cundy, C., Choi, K., Hu, Y., Lao, N., and Ermon, S. (2022a). Towards a foundation model for\ngeospatial artificial intelligence (vision paper). In Proceedings of the 30th International Conference on\nAdvances in Geographic Information Systems , pages 1–4.\nMai, G., Huang, W., Sun, J., Song, S., Mishra, D., Liu, N., Gao, S., Liu, T., Cong, G., Hu, Y., et al.\n(2024). On the opportunities and challenges of foundation models for GeoAI (vision paper). ACM\nTransactions on Spatial Algorithms and Systems , 10(2):1–46.\nMai, G., Janowicz, K., Cai, L., Zhu, R., Regalia, B., Yan, B., Shi, M., and Lao, N. (2020). SE-KGE:\nA location-aware knowledge graph embedding model for geographic question answering and spatial\n31\nsemantic lifting. Transactions in GIS, 24(3):623–655.\nMai, G., Janowicz, K., Hu, Y., Gao, S., Yan, B., Zhu, R., Cai, L., and Lao, N. (2022b). A review\nof location encoding for GeoAI: methods and applications. International Journal of Geographical\nInformation Science, 36(4):639–673.\nMajic, I., Naghizade, E., Winter, S., and Tomko, M. (2021). RIM: a ray intersection model for the analysis\nof the between relationship of spatial objects in a 2D plane. International Journal of Geographical\nInformation Science, 35(5):893–918.\nMajic, I., Wang, Z., Janowicz, K., and Karimi, M. (2024). Spatial task-explicity matters in prompting\nlarge multimodal models for spatial planning. In Proceedings of the 7th ACM SIGSPATIAL\nInternational Workshop on AI for Geographic Knowledge Discovery , pages 99–105.\nManvi, R., Khanna, S., Mai, G., Burke, M., Lobell, D., and Ermon, S. (2023). GeoLLM: Extracting\ngeospatial knowledge from large language models. arXiv preprint arXiv:2310.06213 .\nMark, D. M. and Egenhofer, M. J. (1994). Modeling spatial relations between lines and regions: combining\nformal mathematical models and human subjects testing. Cartography and geographic information\nsystems, 21(4):195–212.\nMooney, P., Cui, W., Guan, B., and Juh´ asz, L. (2023). Towards understanding the geospatial skills of\nChatGPT: Taking a geographic information systems (GIS) exam. In Proceedings of the 6th ACM\nSIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery , pages 85–94.\nMuennighoff, N. (2022). Sgpt: GPT sentence embeddings for semantic search. arXiv preprint\narXiv:2202.08904.\nNeelakantan, A., Xu, T., Puri, R., Radford, A., Han, J. M., Tworek, J., Yuan, Q., Tezak, N., Kim, J. W.,\nHallacy, C., et al. (2022). Text and code embeddings by contrastive pre-training. arXiv preprint\narXiv:2201.10005.\nOpenAI (2022). Introducing ChatGPT.\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. (2018). Improving language understanding\nby generative pre-training. OpenAI blog, 0(1):12.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):24.\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. (2021).\nDALL·E: Creating images from text.\nRandell, D. A., Cui, Z., and Cohn, A. G. (1992). A spatial logic based on regions and connection.\nPrinciples of Knowledge Representation and Reasoning , 92:165–176.\nRao, J., Gao, S., Mai, G., and Janowicz, K. (2023). Building privacy-preserving and secure geospatial\nartificial intelligence foundation models (vision paper). In Proceedings of the 31st ACM International\nConference on Advances in Geographic Information Systems , pages 1–4.\nReimers, N. and Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using siamese BERT-\nnetworks. arXiv preprint arXiv:1908.10084 .\nReis, R. M., Egenhofer, M. J., and Matos, J. L. (2008). Conceptual neighborhoods of topological relations\nbetween lines. In Headway in Spatial Data Handling: 13th International Symposium on Spatial Data\nHandling, pages 557–574. Springer.\nRenz, J. and Nebel, B. (1998). Spatial reasoning with topological information. In Spatial Cognition:\nAn Interdisciplinary Approach to Representing and Processing Spatial Knowledge , pages 351–371.\nSpringer.\nSack, J.-R. and Urrutia, J. (1999). Handbook of computational geometry. Elsevier.\nScheider, S., Nyamsuren, E., Kruiger, H., and Xu, H. (2021). Geo-analytical question-answering with\nGIS. International Journal of Digital Earth , 14(1):1–14.\nShannon, C. E. (1948). A mathematical theory of communication. The Bell system technical journal ,\n27(3):379–423.\nSheth, A., Roy, K., and Gaur, M. (2023). Neurosymbolic artificial intelligence (why, what, and how).\nIEEE Intelligent Systems , 38(3):56–62.\nSkoumas, G., Pfoser, D., Kyrillidis, A., and Sellis, T. (2016). Location estimation using crowdsourced\nspatial relations. ACM Transactions on Spatial Algorithms and Systems (TSAS) , 2(2):1–23.\nTao, R. and Xu, J. (2023). Mapping with ChatGPT. ISPRS International Journal of Geo-Information ,\n12(7):284.\nTellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., and Roy, N. (2011).\nUnderstanding natural language commands for robotic navigation and mobile manipulation. In\nProceedings of the AAAI Conference on Artificial Intelligence , volume 25, pages 1507–1514.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T.,\n32\nBaker, L., Du, Y., et al. (2022). LaMDA: Language models for dialog applications. arXiv preprint\narXiv:2201.08239.\nTucker, S. (2024). A systematic review of geospatial location embedding approaches in large language\nmodels: A path to spatial ai systems. arXiv preprint arXiv:2401.10279 .\nWallgr¨ un, J. O., Klippel, A., and Baldwin, T. (2014). Building a corpus of spatial relational expressions\nextracted from web documents. In Proceedings of the 8th workshop on geographic information retrieval,\npages 1–8.\nWallgr¨ un, J. O., Klippel, A., and Karimzadeh, M. (2015). Towards contextualized models of spatial\nrelations. In Proceedings of the 9th Workshop on Geographic Information Retrieval , pages 1–2.\nWang, F. (2000). A fuzzy grammar and possibility theory-based natural language user interface for\nspatial queries. Fuzzy sets and systems , 113(1):147–159.\nWang, L., Yang, N., and Wei, F. (2023). Query2doc: Query expansion with large language models. arXiv\npreprint arXiv:2303.07678.\nWang, Z., Ye, X., and Tsou, M.-H. (2016). Spatial, temporal, and content analysis of Twitter for wildfire\nhazards. Natural Hazards, 83:523–540.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022). Chain-\nof-thought prompting elicits reasoning in large language models. Advances in Neural Information\nProcessing Systems, 35:24824–24837.\nWolter, D. and Wallgr¨ un, J. O. (2012). Qualitative spatial reasoning for applications: New challenges\nand the sparq toolbox. Qualitative Spatio-Temporal Representation and Reasoning: Trends and Future\nDirections: Trends and Future Directions, page 336.\nWu, K., Zhang, X., Dang, Y., and Ye, P. (2023a). Deep learning models for spatial relation extraction\nin text. Geo-spatial Information Science, 26(1):58–70.\nWu, M., Huang, Q., Gao, S., and Zhang, Z. (2023b). Mixed land use measurement and mapping with\nstreet view images and spatial context-aware prompts via zero-shot multimodal learning.International\nJournal of Applied Earth Observation and Geoinformation , 125:103591.\nXie, Y., Wang, Z., Mai, G., Li, Y., Jia, X., Gao, S., and Wang, S. (2023). Geo-foundation models: Reality,\ngaps and opportunities. In Proceedings of the 31st ACM International Conference on Advances in\nGeographic Information Systems, pages 1–4.\nYamada, Y., Bao, Y., Lampinen, A. K., Kasai, J., and Yildirim, I. (2023). Evaluating spatial\nunderstanding of large language models. arXiv preprint arXiv:2310.14540 .\nYan, B., Janowicz, K., Mai, G., and Gao, S. (2017). From ITDL to Place2Vec: Reasoning about place type\nsimilarity and relatedness by learning embeddings from augmented spatial contexts. In Proceedings of\nthe 25th ACM SIGSPATIAL international conference on advances in geographic information systems ,\npages 1–10.\nYang, J., Yang, S., Gupta, A. W., Han, R., Fei-Fei, L., and Xie, S. (2024). Thinking in space: How\nmultimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171.\nYang, X., Chen, A., PourNejatian, N., Shin, H. C., Smith, K. E., Parisien, C., Compas, C., Martin, C.,\nCosta, A. B., Flores, M. G., et al. (2022). A large language model for electronic health records. npj\nDigital Medicine, 5(1):194.\nYuan, Y. (2011). Extracting spatial relations from document for geographic information retrieval. In\n2011 19th International Conference on Geoinformatics , pages 1–5. IEEE.\nZhang, Q., Kang, Y., and Roth, R. (2023). The Ethics of AI-Generated Maps: DALLE-2 and\nAI’s Implications for Cartography (Short Paper). In 12th International conference on geographic\ninformation science (GIScience 2023), pages 93–98. Schloss Dagstuhl–Leibniz-Zentrum f¨ ur Informatik.\nZhang, Y., Wei, C., He, Z., and Yu, W. (2024). GeoGPT: An assistant for understanding and processing\ngeospatial tasks. International Journal of Applied Earth Observation and Geoinformation, 131:103976.\nZheng, O., Abdel-Aty, M., Wang, D., Wang, Z., and Ding, S. (2023). ChatGPT is on the horizon: Could\na large language model be all we need for intelligent transportation? arXiv preprint arXiv:2303.05382.\nZhu, R., Janowicz, K., Cai, L., and Mai, G. (2022). Reasoning over higher-order qualitative spatial\nrelations via spatially explicit neural networks. International Journal of Geographical Information\nScience, 36(11):2194–2225.\n33"
}