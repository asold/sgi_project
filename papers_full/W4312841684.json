{
    "title": "Generating Role-Playing Game Quests With GPT Language Models",
    "url": "https://openalex.org/W4312841684",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Värtinen, Susanna",
            "affiliations": [
                "Aalto University"
            ]
        },
        {
            "id": "https://openalex.org/A2749177391",
            "name": "Hämäläinen, Perttu",
            "affiliations": [
                "Aalto University"
            ]
        },
        {
            "id": "https://openalex.org/A2744754140",
            "name": "Guckelsberger Christian",
            "affiliations": [
                "Aalto University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2147755281",
        "https://openalex.org/W6736179767",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3112103703",
        "https://openalex.org/W2621430944",
        "https://openalex.org/W2326742050",
        "https://openalex.org/W2328072371",
        "https://openalex.org/W2518462",
        "https://openalex.org/W2037790799",
        "https://openalex.org/W2294933977",
        "https://openalex.org/W2765792593",
        "https://openalex.org/W2115867107",
        "https://openalex.org/W2140064549",
        "https://openalex.org/W6752207002",
        "https://openalex.org/W2087428711",
        "https://openalex.org/W3130943768",
        "https://openalex.org/W2995782662",
        "https://openalex.org/W6767489169",
        "https://openalex.org/W3210757389",
        "https://openalex.org/W2100506586",
        "https://openalex.org/W2028140375",
        "https://openalex.org/W6754157886",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W6790978476",
        "https://openalex.org/W6776218486",
        "https://openalex.org/W3010387158",
        "https://openalex.org/W6773518419",
        "https://openalex.org/W3118973736",
        "https://openalex.org/W6767858076",
        "https://openalex.org/W3211808832",
        "https://openalex.org/W3094204664",
        "https://openalex.org/W2149433103",
        "https://openalex.org/W6761551260",
        "https://openalex.org/W7057634235",
        "https://openalex.org/W2171960331",
        "https://openalex.org/W6696699814",
        "https://openalex.org/W6676713131",
        "https://openalex.org/W6767859106",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W2973379954",
        "https://openalex.org/W4287802874"
    ],
    "abstract": "Tallenna OA-tiedosto, kun julkaistu",
    "full_text": "1\nGenerating Role-Playing Game Quests\nWith GPT Language Models\nSusanna V ¨artinen1, Perttu H ¨am¨al¨ainen1, and Christian Guckelsberger 1, 2, 3 , *\n1Department of Computer Science, Aalto University, Espoo, Finland, 2Finnish Center for Artificial Intelligence,\n3School of Electronic Engineering and Computer Science, Queen Mary University of London, London, UK\nAbstract—Quests represent an integral part of role-playing\ngames (RPGs). While evocative, narrative-rich quests are still\nmostly hand-authored, player demands towards more and richer\ngame content, as well as business requirements for continu-\nous player engagement necessitate alternative, procedural quest\ngeneration methods. While existing methods produce mostly\nuninteresting, mechanical quest descriptions, recent advances in\nAI have brought forth generative language models with promising\ncomputational storytelling capabilities. We leverage two of the\nmost successful Transformer models, GPT-2 and GPT-3, to\nprocedurally generate RPG video game quest descriptions. We\ngathered, processed and openly published a data set of 978 quests\nand their descriptions from six RPGs. We fine-tuned GPT-2\non this data set with a range of optimizations informed by\nseveral mini studies. We validated the resulting Quest-GPT-2\nmodel via an online user study involving 349 RPG players. Our\nresults indicate that one in five quest descriptions would be\ndeemed acceptable by a human critic, yet the variation in quality\nacross individual quests is large. We provide recommendations\non current applications of Quest-GPT-2. This is complemented\nby case-studies on GPT-3 to highlight the future potential of\nstate-of-the-art natural language models for quest generation.\nIndex Terms—artificial intelligence, generative models, games,\nprocedural content generation, computational storytelling, quests.\nI. I NTRODUCTION\nQ\nUESTS in role-playing games (RPGs) represent explic-\nitly posed, challenging tasks for the player to accom-\nplish. Main quests are vital to progressing in a game, while\nside quests can yield auxiliary rewards to the player. Quests\nare often narrative-driven and woven into a game’s larger story\nline. At present, most such quests are written by people.\nHowever, players’ growing demand for more game content,\ne.g. in dynamic and open-ended games [1], poses a challenge\nto human quest designers on both the developer and commu-\nnity side: writing a large number of quests that are meaningful\nand of sufficient quality to warrant continuous player engage-\nment requires time, skill and creativity. To alleviate the quest\ncreation task, designers could either draw inspiration from, or\nco-create [2], computationally generated quests and the narra-\ntives that communicate their objectives, i.e. quest descriptions.\nAutonomous computational quest generation methods could\nmoreover enable quests that adapt online to a player’s actions,\nincluding user-generated content. Next to these practical con-\ncerns, we deem it a fascinating scientific question whether\nhigh-quality quests can be generated by procedural means.\nManuscript received Dec 20, 2021.\n*Corresponding author. Email: christian.guckelsberger@aalto.fi.\nExisting approaches to procedurally generate quests and\ntheir descriptions are lacking as their products are often\nformulaic and repetitive. Meanwhile, AI research has brought\nforth novel text-generating language models with powerful\ncomputational storytelling capabilities. Arguably the most\nprominent such model at present is OpenAI’s Generative\nPre-trained Transformer (GPT), which has been leveraged\nto produce various types of realistic, human-like texts with\nunprecedented quality, from poetry to fictional news [3]–[5].\nThis paper investigates the potential of GPT-2 and GPT-\n3, the latest two models in the GPT family, to automatically\ngenerate quest descriptions for RPGs. By quest descriptions,\nwe denote short texts that explain the quest to the player\nfrom the perspective of a quest-giving non-playable character\n(NPC). We thus focus on one building block of a larger\npipeline, preceded by e.g. a dynamic quest ingredient generator\naccounting for the narrative and gameplay context, a dialogue\ngenerator for the quest giver, and a game logic generator\nlinking the quest’s progression to game events and objects.\nGPT-3 has more than 100 times more parameters than its\npredecessor GPT-2, but it cannot be trained or sampled on\nhardware that players and game studios typically have access\nto. In this work, we hence focus on fine-tuning GPT-2, based\non a custom-made RPG quest description data set. We have\nvalidated the resulting Quest-GPT-2 model both objectively,\nwith training and validation loss as well as conditional per-\nplexity scores, and subjectively via an online user study. To\nprovide indications for the future potential of text generation\nmodels, we complement these fine-tuning experiments with\ncase-studies on generating quest descriptions with the vanilla\nGPT-3 model. Our contributions are threefold:\n1) A novel and publicly available quest data set with 978\nquests and descriptions from six RPG games.\n2) Quest-GPT-2, a fine-tuned variant of GPT-2 to generate\nRPG quest descriptions, provided the quest as input. The\nmodel has been evaluated in a comprehensive user study,\ninvolving 349 participants and 500 quest descriptions.\n3) A comparison of different language model fine-tuning\ntext formatting techniques, including the use of place-\nholders for proper nouns and numbers [6] to reduce\nvariance in the Transformer model fine-tuning.\nWe have made our quest data set publicly available 1 for use\nin other creative applications and to support the development\nof next-generation procedural quest systems.\n1https://doi.org/10.17605/OSF.IO/JTQDB\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nII. R ELATED WORK\nProcedural quest generation is a long-lasting challenge in\ngame AI, with related work dating back more than 15 years\n[7]. We provide a brief, incomplete overview of related work,\nfocusing on the generation techniques and main shortcomings.\nEarly research on procedural quest generation focused on\nplanning and rule-based approaches. Ware and Young [8] made\nan interactive narrative adventure game The Best Laid Plans\nthat utilises computational models of intentionality and conflict\nin controlling its NPCs. Thue et al.[9] have built an interactive\nstorytelling system, Player-Specific Stories via Automatically\nGenerated Events (PaSSAGE), which uses player modelling to\nautomatically determine players’ preferred styles of play. Si\net al. [10] have presented Thespian, a framework for creating\ninteractive drama from user-modifiable agents, i.e. characters\nwith different personality styles and action policies.\nSome authors have also attempted more emergent, dynamic\nquest generation methods. McCoy et al. [11] developed the\naward-winning social puzzle game Prom Week that utilizes\na “social physics” engine named Comme il Faut (CiF). CiF\nuses character traits, relationships, and desires to influence\nplayer–NPC interactions while also utilizing thousands of pre-\nprogrammed sociocultural considerations. Guimaraes et al.\n[12] implemented CiF into the popular RPG The Elder Scrolls\nV: Skyrim[13] as a freely downloadable modification.\nMany existing quest generation algorithms construct quests\nbased on graphs. Kybartas and Verbrugge [14] used narrative\ngraph rewriting in their REwriting Graphs for Enhanced Nar-\nratives (ReGEN) system to create complex branching stories.\nCalvin and Michael [15] leveraged graphs to generate quests\nfor key and lock puzzles in their experimental game Charbitat,\nPita et al.[16] created dynamically linked quests in persistent\nmultiplayer worlds, and Stocker and Alvin [17] generated\nnon-linear quests based on implementation-specific rules and\nnatural language. Doran and Parberry [18] analyzed 750 quests\nfrom four popular RPGs to identify a common structure to be\nleveraged in their prototype quest generator through context-\nfree grammars. The latter has been further expanded by Breault\net al. [19] in their Creation Of Novel Adventure Narrative\n(CONAN) system. Soares de Lima et al. [20] combine au-\ntomated planning with evolutionary search guided by story\narcs. We note two main shortcomings in the above body of\nrelated work. Firstly, the used techniques produce formulaic\nand repetitive quests, and do not generalize well to other\ngames and genres. Secondly, the generated quests have only\nbeen evaluated against computational metrics and quests from\nexisting games, but not against players’ experiences.\nRecent work has overcome these shortcomings through the\nuse of language models for quest generation and user studies\nfor their evaluation. Ammanabrolu et al.[21] fine-tuned GPT-2\nfor creating quests in the form of cooking instructions in a text-\nbased cooking game. Based on a small user study with 75 par-\nticipants, they found that the GPT-2 quests were experienced\nas more valuable and coherent, but less surprising and novel\nthan quests produced by random assignment or Markov chains.\nMost closely related to our work, van Stegeren and My ´sliwiec\n[22] have recently fine-tuned GPT-2 for the generation of quest\ndescriptions told from the perspective of an NPC. Crucially\nthough, they solely use data from the Massively Multiplayer\nOnline Role-Playing Game (MMORPG) World of Warcraft\n[23]. This is problematic in that such a homogeneous data set\nreduces the generalizability of the generator, as supported by\nthe study’s authors. Moreover, while MMORPGs contain tens\nof thousands of quests and thus represent an easy data source,\nthe quests are typically simpler in structure and less varied than\ntheir RPG counterparts: rather than functioning as vehicles\nfor role-playing or captivating story-heavy adventures, they\noften provide mere busywork for player character progression.\nUnsurprisingly, their model input only consists of the quest\ntitle and objective. Our approach affords more control for\nintegration in a specific game by incorporating more differen-\ntiated and essential input information such as the quest-giver,\nlocation, involved characters and quest reward. Van Stegeren\nand My ´sliwiec’s user study motivates our use of GPT-2 for\nquest generation, in that at least some generated descriptions\nscored higher than user’s ratings for human authored texts.\nThis finding must however be taken with a grain of salt,\nas their study only involved 20 quest descriptions rated by\n32 participants, and each corresponding to exactly one quest.\nOur study in contrast involved 349 participants, rating a total\nof 500 quest descriptions generated from 50 quests from\nsix RPGs. Our study is thus not only more representative,\nbut also allowed us to investigate quality variations in quest\ndescriptions produced from the same quest input.\nIII. L ANGUAGE MODELS AND THE GPT F AMILY\nLanguage modeling and generation has a long history in AI\nand computational creativity research [24]–[26]. Typically, text\ngeneration is approached statistically as sampling each token\n– a character, word, or word part – conditional on previous\ntokens, ci ∼ p(ci|c1 . . . ci−1; θ), where ci denotes the i:th\ntoken in the text sequence, and θ denotes the parameters of\nthe sampling distribution. In this statistical view, the model-\ning/learning task amounts to optimizing θ based on training\ndata, e.g., to maximize the probabilities of all tokens in the\ntraining data conditional on up to N preceding tokens, where\nN is the context size. Modern language models use deep neural\nnetworks to learn the regularities in the data, and θ become the\nparameters of the network. For the text generation/sampling\ntask, such a neural network takes in a sequence of tokens\nand outputs the sampling probabilities of each possible next\ntoken. There is ample empirical evidence that large enough\nneural language models can reach beyond memorizing their\ninput and exhibit remarkable creative and intelligent behavior,\ne.g., in handling novel concepts not included in the training\ndata and only introduced in the prompt.\nThe GPT model family is based on the Transformer neural\nnetwork architecture introduced in 2017 [4], [27], which is\ncharacterized by encoder and decoder blocks as well as a\nself-attention mechanism. Encoder blocks transforms variable\nlength input data into fixed-sized feature maps, whereas de-\ncoder blocks attempt to transform the maps back into the as-\nsumed input. The self-attention mechanism relates each input\nword to each other to establish links between related words,\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\nsuch as names and pronouns, modulating which previous to-\nkens influence each generated token. Transformer models have\nbeen proven capable in a wide range of challenging tasks, e.g.,\ngenerating music and images [28], [29], synthesizing proteins\nwith desired properties [30], and logical and counterfactual\nreasoning with facts and rules defined using natural language\n[31]. Most relevant here, they have been shown to produce\nrealistic, human-like text with unprecedented quality [3]–[5].\nGPT models are trained with a diverse collection of unla-\nbeled textual data and, optionally, fine-tuned with a small set of\ntask-specific labeled training data. The pre-training allows to\nencode a large amount of common knowledge and learn long-\nrange dependencies between tokens, but fine-tuning has been\nshown to improve performance on specific tasks considerably\n[32]. The different models in the GPT family not only differ\nfrom each other in terms of the used training data, but also\nnotably in scale: GPT-2 has ten times more parameters than\nGPT-1, whereas GPT-3 has over one hundred times more\nparameters than GPT-2 [3], [4]. Training and sampling GPT-\n3 is at present not possible on the hardware that players and\ngame studios typically have access to. In the rest of this article,\nwe consequently focus on fine-tuning GPT-2, and only use\nthe vanilla GPT-3 model for comparative case-studies on the\nenhanced capabilities of this more complex model generation.\nIV. T RAINING DATA SET\nWe adopt the hypothesis from related work [22] that the data\nused to pre-train GPT-2 does not contain a sufficient amount of\nquest examples to facilitate high-quality quest generation with-\nout additional fine-tuning based on a separate, specialized data\nset. We confirmed this hypothesis by investigating the output\nof the vanilla GPT-2 model with 744M parameters, if presented\nwith different quests (cf. Sec.V). Unfortunately, most of the\nquest data sets used in previous related work have not been\nmade public, a state of affairs which is discussed more widely\nby van Stegeren and Theune [33]. We consequently collected,\nprocessed and published 1a data set of 978 quests and quest\ndescriptions from six RPGs to fine-tune GPT-2, and for others\nto adopt and potentially extend in their projects.\nA. Collecting Data\nFine-tuning a language model can require a few thousand\nexamples to produce good results, depending on the task and\nmodel size. For instance, GPT-2-774M has been shown to\nrequire around 5,000 text samples, when fine-tuning the model\nfor text continuation tasks [34]. Video game descriptions are\ntypically longer than these text samples. and we consequently\nassumed that a data set of roughly 1000 quests and quest\ndescriptions would suffice for fine-tuning Quest-GPT-2. This\nis also supported by the observation that GPT variants with\nmore parameters, such as our target model GPT-2-1.5B, are\nbetter at learning patterns from few examples [4].\nHand-authoring this amount of quest data for our study\nwould have been too time-intense, hinder comparison to quests\nin actual games, and introduce the risk of experimenter bias.\nWe consequently decided to use quests from existing RPG\ngames. We collected quests from multiple games for two\nTable I\nTHE QUEST DATA SET (978 QUESTS )\nGame Sourcing Quests\nBaldur’s Gate[35] collected (game files) 100\nBaldur’s Gate II: Shadows of Amn[36] collected (game files) 94\nThe Elder Scrolls IV: Oblivion[37] collected (game files) 215\nThe Elder Scrolls V: Skyrim[13] collected (game files) 389\nMinecraft[38] written by the authors 100\nTorchlight II[39] collected previously by [33] 80\nTable II\nNATURAL LANGUAGE PROCESSING METRICS (MEAN ± STDDEV ) ON THE\nQUEST DESCRIPTIONS FROM THE QUEST DATA SET (TBL. I)\nGame Readability(Flesch-Kincaid Grade)smaller easier\nSyntactics Complexity(Dependency Distance)larger more complex\nLexical Richness(Type-Token Ratio)larger richer\nWordCount\nBaldur’s Gate[35] 3.03±1.61 2 .33±0.31 0 .73±0.08 99 ±42\nBaldur’s Gate II[36] 2.88±1.34 2 .18±0.25 0 .66±0.08 134 ±58\nThe Elder Scrolls IV[37] 3.00±1.65 2 .19±0.27 0 .66±0.08 143 ±77\nThe Elder Scrolls V[13] 2.78±1.53 2 .18±0.30 0 .71±0.08 105 ±47\nMinecraft[38] 3.36±1.48 2 .30±0.28 0 .71±0.06 91 ±29\nTorchlight II[39] 4.58±2.15 2 .45±0.40 0 .74±0.08 79 ±28\nOverall 3.07±1.67 2 .23±0.31 0 .70±0.08 112±57\nreasons. Firstly, RPGs from different game series have distinct\nstyles of quest writing, and collecting a diverse set of writing\nstyle holds the promise to increase the expressive range of the\nlearned model. Secondly, we were unlikely to find the required\namount of quests in a single, regular RPG. As argued earlier,\nwe discarded MMORPGs as less constrained data source to\navoid a negative impact on the quality of our model output.\nThere are two main techniques for obtaining video game\ntexts [33]: (i) extracting text directly from game files and\n(ii) scraping text from unofficial, fan-curated online sources.\nHowever, game files are often either encrypted or use poorly\ndocumented proprietary file formats, whereas fan-written\nsources, such as online wikis, typically only paraphrase the\ncontents of the in-game texts, e.g. character dialogue, instead\nof directly documenting how they appear to the players.\nWe consequently focused on (i) and extracted quest texts\ndirectly from the game files with modding tools (more detail in\nAppendix A). We appealed to (ii) by drawing on fan wisdom,\nselecting the RPG games not only based on quest quality, but\nalso based on the presence of high-quality fan wikis and active\nmodding scenes. Information from fan wikis made it easier\nto retrieve quest data from games files, while modding tools\nallowed us to sidestep the file format and encryption issues.\nTo obtain a sufficiently large data set of varied and complex\nquests, we first collected a total of 878 quest examples\nfrom five RPGs. These games share a medieval-esque fantasy\nsetting, which should improve the quality of the model, but\ncan also limit the its expressive range. To counteract this,\nwe extended our data set with one hundred manually written\nMinecraft [38] quests. In total, our data set comprises 978\nquests from six games as summarized in Tbl. I. Additionally,\nTbl. II shows how our quest data set performs on some well-\nknown natural language processing metrics. Overall, all RPGs\nin our data set produce similar scores on the depicted metrics:\na considerable exception to this is the readability metric, which\nimplies that the Torchlight II[39] quest descriptions are more\ndifficult to read than the descriptions from the other RPGs in\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\nTable III\nQUEST INGREDIENTS IDENTIFIED FROM OUR DATA SET\nQuest Ingredient Description Essential\nQuest-giver The person giving the quest to the player yes\nObjective The overarching goal of the quest yes\nTasks The actions that have to be done to fulfill the objective of the quest yes\nTask locations The locations where the tasks can be completed no\nRewards The rewards given to the player upon the completion of the quest objective no\nFacts Important facts related to the quest no\nItems Important items related to the quest no\nCharacters Important characters related to the quest no\nLocations Some secondary locations related to the quest no\nGroups Important groups, e.g. factions, related to the quest no\nEnemies Enemies that the player will face during the quest no\nDescription The quest description shown to the player yes\nthe data set. This disparity is likely caused by the fact that\nfictional names make up a larger portion of the Torchlight II\ndescriptions, relative to their shorter average length.\nB. Data Formatting\nTo generate a quest description, a language model must\nbe given an outline with the desired “ingredients” of a\nquest as input. We analyzed the collected quests to recog-\nnize these ingredients (Tbl. III). Our quest ingredients align\npartially with classical narrative analyses in literature, such\nas Vladimir Propp’s Morphology of the Folktale [40]. For\nexample, Propp’s definitions of various types of dispatchers\nand character archetypes bear similarities to our quest-givers.\nExisting narrative analyses were only of limited use, as they\ntypically span the entire duration of a story while we are more\ninterested in the circumstances at the beginning of a quest.\nNot only what information is provided, but also how it is\nlaid out is crucial to training a language model: semantically\nequivalent pieces of input text can yield wildly different\nresults, likely because some text formats synergize better\nwith the model’s pre-training data. We devised and compared\nthree distinct input formats, i.e. quest metadata formats, for\nrepresenting the quests via their quest ingredients: a highly\nstructured format that resembles XML, later referred to as\nXML-like, a simple format that is inspired by dramatis per-\nsonae, i.e. character listings in plays and movie scripts, and a\nnarrative format that reads like a small story. The first format,\nXML-like is adopted from Lee [41], who has successfully used\na similar format to generate patent claims with GPT-2. Fig. 1\nillustrates all three formats based on an example quest.\nWe devised a generic JSON representation for storing our\nquests in an organized manner (Appendix B), and to derive\nour training data in the three metadata formats. We also hope\nthat storing our quests in a canonical format makes it easier\nfor other researchers to adopt our data set in their work.\nC. Data Processing\nWhile collecting the quest data set, candidate quests were\nevaluated by the authors based on the following criteria:\n• Novelty and interestingness of narrative and content [42].\n• The existence of clearly defined goals.\n• The length of the quest description.\nWe excluded quest descriptions that lacked the essential\nquest ingredients in Tbl. III. As a side-effect, these descriptions\nwere typically very short. We also discarded too long descrip-\ntions ( >256 words), as they might exceed GPT-2’s context\nwindow that holds 1,024 tokens (i.e., roughly 256 English\nwords), resulting in the model forgetting ingredients.\nSome candidates did not meet one or multiple criteria\nand were consequently omitted. Other quests only met these\ncriteria to a limited extent, and were consequently manually\nedited. For instance, quests are usually delivered through\nsprawling dialogue between the player and the quest-giver, not\nlinearly through monolithic pieces of text. As a consequence,\nquest rewards are commonly discussed after the player has\nalready completed the quest; we had to make some tense\nchanges to accommodate the rewards into the quest descrip-\ntions. Moreover, some candidate quests were split into multiple\nindependent quests, as they either (i) involved the quest-giver\ndirecting the player to another NPC, or (ii) had distinct paths\nfor the player to follow based on their actions in the game.\nV. D EVELOPING QUEST -GPT-2\nOur text generation example in Fig. 2 demonstrates that\nGPT-2 can generate some short, rudimentary quest descrip-\ntions even without fine-tuning, if one provides few quest\nexamples in the input text. However, the output quality is not\nconvincing. Moreover, quest descriptions typically incorporate\nmany small elements, such as world knowledge, as well\nas character relationships and archetypes. It is difficult to\nincorporate those elements into a few quest examples in the\ninput, especially considering the fact that the context window\nof GPT-2 holds only 1,024 tokens, i.e. byte-pair encoded sets\nof characters. In the following, we describe the process of fine-\ntuning GPT-2 with our custom data set into Quest-GPT-2. We\nmade all code publicly available on Github 2.\n2https://github.com/svartinen/gpt2-quest-descriptions\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\n<|b e g i n q u e s t|>\n<|b e g i n o b j e c t i v e|>\nk i l l D y n a h e i r\n<|e n d o b j e c t i v e|>\n<|b e g i n t a s k s|>\nf i n d D y n a h e i r\n<|e n d t a s k s|>\n<|b e g i n t a s k l o c a t i o n s|>\nwest o f Nashkel , n e a r t h e g n o l l s t r o n g h o l d\n<|e n d t a s k l o c a t i o n s|>\n<|b e g i n q u e s t g i v e r|>\nEdwin : a pompous w i z a r d\n<|e n d q u e s t g i v e r|>\n<|b e g i n r e w a r d s|>\none y e a r o f Edwin ’ s s e r v i c e s a s a w i z a r d\n<|e n d r e w a r d s|>\n<|b e g i n c h a r a c t e r s|>\nD y n a h e i r : a t r e a c h e r o u s f e m a l e w i t c h\n<|e n d c h a r a c t e r s|>\n<|b e g i n l o c a t i o n s|>\nNashkel : a town\n<|e n d l o c a t i o n s|>\n<|b e g i n t o o l s|>\nNONE\n<|e n d t o o l s|>\n<|b e g i n d e s c r i p t i o n|>\nI am Edwin , a wizard , and I r e q u i r e you ! ( Yes ,\nt h e y w i l l do n i c e l y . )\nI would have you k i l l a witch , t h e w i t c h D y n a h e i r .\nShe i s t r e a c h e r o u s , b u t w i t h your p a r t i c i p a t i o n I\nf o r e s e e no d i f f i c u l t y . L a s t I h e a r d o f her , she\nwas t r a v e l i n g t o t h e west o f Nashkel , c l o s e t o t h e\ng n o l l s t r o n g h o l d l o c a t e d t h e r e . W i l l you a s s i s t ?\nThe p r i z e I o f f e r would s u r e l y be beyond measure\ni n your meager u n d e r s t a n d i n g . Your payment s h a l l\nbe one y e a r o f my s e r v i c e s a s a w i z a r d . I am s u r e\nyou a g r e e t h a t my g u i d a n c e w i l l be f a r more\nv a l u a b l e t h a n any monetary sum .\n<|e n d d e s c r i p t i o n|>\n<|e n d q u e s t|>\n(a) XML-like\nT h i s i s an RPG q u e s t from Baldur ’ s Gate .\nO b j e c t i v e :\nk i l l D y n a h e i r\nTasks :\nf i n d D y n a h e i r\nTask l o c a t i o n s :\nwest o f Nashkel , n e a r t h e g n o l l s t r o n g h o l d\nQuest − g i v e r :\nEdwin , a pompous w i z a r d\nRewards :\none y e a r o f Edwin ’ s s e r v i c e s a s a w i z a r d\nC h a r a c t e r s :\nD y n a h e i r : a t r e a c h e r o u s f e m a l e w i t c h\nL o c a t i o n s :\nNashkel : a town\nQuest d e s c r i p t i o n , t h e q u e s t − g i v e r e x p l a i n i n g t h e q u e s t\nt o t h e p l a y e r :\nI am Edwin , a wizard , and I r e q u i r e you ! ( Yes , t h e y\nw i l l do n i c e l y . )\nI would have you k i l l a witch , t h e w i t c h D y n a h e i r . She\ni s t r e a c h e r o u s , b u t w i t h your p a r t i c i p a t i o n I f o r e s e e\nno d i f f i c u l t y . L a s t I h e a r d o f her , she was t r a v e l i n g\nt o t h e west o f Nashkel , c l o s e t o t h e g n o l l s t r o n g h o l d\nl o c a t e d t h e r e . W i l l you a s s i s t ?\nThe p r i z e I o f f e r would s u r e l y be beyond measure i n\nyour meager u n d e r s t a n d i n g . Your payment s h a l l be one\ny e a r o f my s e r v i c e s a s a w i z a r d . I am s u r e you a g r e e\nt h a t my g u i d a n c e w i l l be f a r more v a l u a b l e t h a n any\nmonetary sum .\n(b) Simple\nT h i s i s an RPG q u e s t from Baldur ’ s Gate .\nThe q u e s t − g i v e r i s c a l l e d Edwin . Edwin i s a\npompous w i z a r d .\nThe q u e s t − g i v e r g i v e s a q u e s t t o t h e p l a y e r .\nThe p l a y e r ’ s o b j e c t i v e i s t o k i l l D y n a h e i r .\nThe p l a y e r s h o u l d f i r s t f i n d D y n a h e i r t o\nc o m p l e t e t h e i r o b j e c t i v e . T h i s t a s k can be\nc o m p l e t e d i n t h e f o l l o w i n g l o c a t i o n : west o f\nNashkel , n e a r t h e g n o l l s t r o n g h o l d .\nThe p l a y e r w i l l r e c e i v e t h e f o l l o w i n g r e w a r d s\nf o r c o m p l e t i n g t h e q u e s t o b j e c t i v e : one y e a r\no f Edwin ’ s s e r v i c e s a s a w i z a r d .\nThe f o l l o w i n g c h a r a c t e r s a r e r e l a t e d t o t h i s\nq u e s t : D y n a h e i r ( a t r e a c h e r o u s f e m a l e w i t c h ) .\nThe f o l l o w i n g l o c a t i o n s a r e r e l a t e d t o t h i s\nq u e s t : Nashkel ( a town ) .\nT h i s i s t h e q u e s t d e s c r i p t i o n , t h e\nq u e s t − g i v e r e x p l a i n i n g t h e q u e s t t o t h e\np l a y e r :\n” I am Edwin , a wizard , and I r e q u i r e you !\n( Yes , t h e y w i l l do n i c e l y . )\nI would have you k i l l a witch , t h e w i t c h\nD y n a h e i r . She i s t r e a c h e r o u s , b u t w i t h your\np a r t i c i p a t i o n I f o r e s e e no d i f f i c u l t y . L a s t I\nh e a r d o f her , she was t r a v e l i n g t o t h e west\no f Nashkel , c l o s e t o t h e g n o l l s t r o n g h o l d\nl o c a t e d t h e r e . W i l l you a s s i s t ?\nThe p r i z e I o f f e r would s u r e l y be beyond\nmeasure i n your meager u n d e r s t a n d i n g . Your\npayment s h a l l be one y e a r o f my s e r v i c e s a s a\nw i z a r d . I am s u r e you a g r e e t h a t my g u i d a n c e\nw i l l be f a r more v a l u a b l e t h a n any monetary\nsum . ”\n(c) Narrative\nFigure 1. Comparison of the example quest Edwin and Dynaheirfrom Baldur’s Gate, expressed in our three proposed quest metadata formats.\no b j e c t i v e : k i l l a l l c r e e p e r s\nl o c a t i o n : woods\nq u e s t g i v e r : a b u t c h e r\nreward : a diamond axe\nd e s c r i p t i o n : C r e e p e r s have t a k e n o v e r t h e woods ! H u n t e r s can ’ t p r o c u r e game f o r\nme ! K i l l a l l c r e e p e r s ! I ’ l l reward you w i t h a diamond axe .\no b j e c t i v e : s a v e v i l l a g e r s from a w i t c h\nl o c a t i o n : a v i l l a g e\nq u e s t g i v e r : a v i l l a g e r\nreward : 16 e m e r a l d s\nd e s c r i p t i o n : A w i t c h i s h o l d i n g my f e l l o w v i l l a g e r s c a p t i v e . Someone o u g h t t o s a v e\nthem ! T r a v e l e r , i f you d i d t h i s t a s k f o r me , I ’ d g i v e you 16 e m e r a l d s .\no b j e c t i v e : k i l l a l l zombies\nl o c a t i o n : c a v e s\nq u e s t g i v e r : a v i l l a g e r\nreward : 32 g o l d e n c a r r o t s\nd e s c r i p t i o n : Zombies are out for blood! Kill all zombies! I’ll reward you with 32 golden carrots.\nFigure 2. Quest generation with (not fine-tuned) GPT-2-774M. Here, we\nprovide two full quests as examples (top two). This is followed by a list\nof ingredients for a new quest (bottom). The system completed the quest\ndescription based on this input (in bold).\nA. Preliminary Fine-Tuning Experiments\nWe informed the model fine-tuning through a series of\nquick, small experiments on an Nvidia GTX 1070 8GB GPU\nwith the two smallest GPT-2 variants (124M and 355M param-\neters) and the XML-like quest metadata format. We used the\ntraining script from @nshepperd’s fork of the official OpenAI\nGPT-2 Github release, and adopted the default optimizer\nsettings, i.e. Adam with an initial learning rate of 2∗10−5. We\nset the batch size to 1, because larger batch sizes generated\nout-of-memory exceptions with 8GB of VRAM.\nThese early experiments showed promise for generating rel-\natively coherent quest descriptions, and even complete quests.\nWe made some small observations in-between adjustments to\nand repetitions of this setup. Firstly, if the characters have\nnot been explicitly gendered in the metadata, both employed\n0 1000 2000 3000 4000 5000\nIteration\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nTraining loss\nGPT-2-124M\nGPT-2-355M\nFigure 3. Cross-entropy loss for our early fine-tuning experiments.\nvariants of GPT-2 might either choose a binary gender, or\nrandomly flip between male or female pronouns. This behavior\nwas fixed by explicitly including the characters’ genders in\ntheir descriptions in later experiments. Secondly, both models\ndisplayed signs of over-fitting in all experiments, and we con-\nsequently employed early stopping later on. Thirdly, the gen-\nerated descriptions do not always encompass all quest ingredi-\nents from the input, and entities might be treated incorrectly.\nMost strikingly, a character who is referenced multiple times\nin the input quest outline might appear as several separate\npeople in the output quest description. When comparing the\ntwo differently sized GPT-2 variants, the larger GPT-2-355M\nproduced noticeably more coherent quest descriptions than the\nsmaller GPT-2-124M, while also transmitting the ingredients\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\n<|b e g i n q u e s t|>\n<|b e g i n o b j e c t i v e|>\nk i l l c h a r a c t e r 0\n<|e n d o b j e c t i v e|>\n<|b e g i n t a s k s|>\nf i n d c h a r a c t e r 0\n<|e n d t a s k s|>\n<|b e g i n t a s k l o c a t i o n s|>\nwest o f l o c a t i o n 0 , n e a r t h e g n o l l s t r o n g h o l d\n<|e n d t a s k l o c a t i o n s|>\n<|b e g i n q u e s t g i v e r|>\nq u e s t g i v e r : a pompous w i z a r d\n<|e n d q u e s t g i v e r|>\n<|b e g i n r e w a r d s|>\none y e a r o f q u e s t g i v e r ’ s s e r v i c e s a s a w i z a r d\n<|e n d r e w a r d s|>\n<|b e g i n c h a r a c t e r s|>\nc h a r a c t e r0 : a t r e a c h e r o u s f e m a l e w i t c h\n<|e n d c h a r a c t e r s|>\n<|b e g i n l o c a t i o n s|>\nl o c a t i o n 0 : a town\n<|e n d l o c a t i o n s|>\n<|b e g i n t o o l s|>\nNONE\n<|e n d t o o l s|>\n<|b e g i n d e s c r i p t i o n|>\nI am q u e s t g i v e r , a wizard , and I r e q u i r e you ! ( Yes , t h e y w i l l do n i c e l y . )\nI would have you k i l l a witch , t h e w i t c h c h a r a c t e r 0 . She i s t r e a c h e r o u s ,\nb u t w i t h your p a r t i c i p a t i o n I f o r e s e e no d i f f i c u l t y . L a s t I h e a r d o f her ,\nshe was t r a v e l i n g t o t h e west o f l o c a t i o n 0 , c l o s e t o t h e g n o l l s t r o n g h o l d\nl o c a t e d t h e r e . W i l l you a s s i s t ?\nThe p r i z e I o f f e r would s u r e l y be beyond measure i n your meager\nu n d e r s t a n d i n g . Your payment s h a l l be one y e a r o f my s e r v i c e s a s a w i z a r d .\nI am s u r e you a g r e e t h a t my g u i d a n c e w i l l be f a r more v a l u a b l e t h a n any\nmonetary sum .\n<|e n d d e s c r i p t i o n|>\n<|e n d q u e s t|>\nFigure 4. An example quest in the XML-like format with placeholder text\nof the input quest outlines into output quest descriptions more\ncomprehensively. Additionally, the cross-entropy loss for the\nlarger GPT-2-355M converges noticeably faster towards zero\nthan the loss for the smaller GPT-2-124M (Fig. 3).\nB. Substituting Proper Nouns and Numbers With Placeholders\nTo address these consistency issues, we employ the place-\nholder token technique introduced by Martin et al.[6]: proper\nnouns (i.e. unique names) and numbers are replaced in the\nquest metadata with placeholder tokens. The original names\nand numbers are substituted back into the generated output\nin a post-processing step. Fig. 4 displays the example quest\nfrom Fig. 1 in XML-like format with placeholders. Generative\nmodels like GPT-2 learn complex multivariate probability den-\nsities p(x, y, ...), which becomes more difficult as the number\nof variables grows. We assume that names and numbers are\nindependent from other quest content, and that the joint distri-\nbution can thus be factorized into p(x, y, ...) = p(x)p(y, ...).\nWe hypothesized that this factorization via placeholders will\nallow the model to learn content independently from the name\nand number information that bears no significant meaning.\nC. Fine-tuning Quest-GPT-2\nWe split the 978 quests in our data set (Tbl. I) into training,\nvalidation, and test sets with 80:15:5 percent ratios. We used\nthe validation set to mitigate over-fitting, and the test set for\nevaluation against human judgment in our user study (Sec. VI).\nTo represent all six source games equally in all sets, the quests\nwere first split proportionally per game, and then combined\ninto the complete training, validation, and test sets. Afterwards,\nwe converted the sets into the three proposed quest metadata\nformats, producing both raw textand placeholder textfor each\nformat for performance comparison.\nTable IV\nCONDITIONAL PERPLEXITIES OF THE FINE -TUNED MODELS\nMetadata Format Text Type Conditional Perplexity\nnarrative\nraw text 10.63\nplaceholder text 10.50\nsimple\nraw text 10.95\nplaceholder text 10.55\nXML-like\nraw text 11.05\nplaceholder text 10.78\n0.5\n1.0\n1.5\n2.0\n2.5\nTraining loss\nsimple (raw text)\nnarrative (raw text)\nXML-like (raw text)\nsimple (placeholder text)\nnarrative (placeholder text)\nXML-like (placeholder text)\n0 200 400 600 800 1000\nIteration\n1.5\n2.0\n2.5\nValidation loss\nFigure 5. Fine-tuning results, moving averages of cross-entropy loss.\nIn contrast to the preliminary experiments, we fine-tuned\nthe largest GPT-2 model with 1.5B parameters. We trained\nthe model six times, once for each combination of metadata\nformat and the two placeholder conditions. We used the\nsame fine-tuning settings as in the preliminary experiments\n(Sec. V-A) for 1,000 iterations at most and stopped early once\nthe validation loss increased again. On an Nvidia V100 32GB\nGPU, the fine-tuning took ca. 50 minutes per combination.\nFig. 5 shows the fine-tuning loss. The placeholder substi-\ntution performs unanimously best in terms of training and\nvalidation loss for all metadata formats. Amongst the metadata\nformats, the XML-like format achieves the smallest training\nand validation loss, while the simple format performs worst.\nCrucially though, comparing metadata formats based on\nfine-tuning loss only can be misleading: the model might\nlearn repetitive formatting easily without respecting format-\nindependent quest ingredients, thus “masking” the loss values\nsmaller when using heavier formatting. To rule this out, we\ncompared the fine-tuned models with perplexity, an established\nlanguage model metric that measures how well a model can\npredict each token in a piece of text, with lower values\nbeing better. We calculated the conditional and normalized\nperplexities of the quest descriptions in the validation set when\ngiven a certain quest outline as input. If a model has a low\nfine-tuning loss but a high conditional perplexity, it most likely\npredicts the formatting tokens correctly while displaying a\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\nT h i s i s an RPG q u e s t\nfrom a m e d i e v a l f a n t a s y\nv i d e o game .\nThe q u e s t − g i v e r i s\nc a l l e d q u e s t g i v e r .\nq u e s t g i v e r i s a g u a r d\nw i t h a s t r o n g s e n s e o f\nj u s t i c e .\nThe q u e s t − g i v e r g i v e s a\nq u e s t t o t h e p l a y e r . The\np l a y e r ’ s o b j e c t i v e i s t o\nk i l l c h a r a c t e r 0 .\nThe p l a y e r s h o u l d f i r s t\nf i n d k i l l c h a r a c t e r 0 t o\nc o m p l e t e t h e i r\no b j e c t i v e . T h i s t a s k can\nbe c o m p l e t e d i n t h e\nf o l l o w i n g l o c a t i o n :\nt a s k l o c a t i o n 0 ( a\ns p a r k l i n g f a i r y f o r e s t ) .\nThe p l a y e r w i l l r e c e i v e\nt h e f o l l o w i n g r e w a r d s\nf o r c o m p l e t i n g t h e q u e s t\no b j e c t i v e : number 0\np l a t i n u m c h i p s .\nThe f o l l o w i n g c h a r a c t e r s\na r e r e l a t e d t o t h i s\nq u e s t : c h a r a c t e r 0 ( a\nmale w i z a r d and a\nd a n g e r o u s l u n a t i c ) .\nThere a r e some i m p o r t a n t\nf a c t s c o n c e r n i n g t h i s\nq u e s t . c h a r a c t e r 0 has\nk i l l e d s e v e r a l p e o p l e\nb e f o r e .\nT h i s i s t h e q u e s t\nd e s c r i p t i o n , t h e\nq u e s t − g i v e r e x p l a i n i n g\nt h e q u e s t t o t h e p l a y e r :\n”\n(a) Input quest outline\nin the narrative format\nwith placeholder text\nYou ’ r e g o i n g t o k i l l c h a r a c t e r 0 . That man has\nk i l l e d p e o p l e b e f o r e .\nHe i s a d a n g e r o u s l u n a t i c . The o n l y one t h a t can\ns t o p him now i s c h a r a c t e r 0 h i m s e l f .\nGet up on t h a t t o w e r and k i l l c h a r a c t e r 0 . Make\ns u r e t h a t he n e v e r g e t s o u t o f t h i s t o w e r . U n t i l\nhe i s dead , none o f t h i s w i l l e v e r happen .\nDo you want t o h e l p us ? q u e s t g i v e r o f J u s t i c e\ng i v e s you number 0 p l a t i n u m c h i p s ! I need t o g e t\nr i d o f someone who has k i l l e d many o f my f a m i l y\nmembers . I t was some guy , c h a r a c t e r 0 , a w i z a r d .\nc h a r a c t e r0 must burn i n h e l l f o r h i s c r i m e s .\nYou must h e l p q u e s t g i v e r . Do t h i s q u e s t and be\nr e w a r d e d . Get r i d o f c h a r a c t e r 0 , and you ’ l l\nr e c e i v e number 0 p l a t i n u m c h i p s ! Be q u i c k on\nyour f e e t !\nWhat i n b l a z e s i s h a p p e n i n g i n t h e\nt a s k l o c a t i o n 0 ?\nSomeone ’ s t r y i n g t o murder p e o p l e r i g h t u n d e r\nour n o s e s !\nThey ’ ve been k i l l i n g , they ’ ve been m u r d e r i n g i n\na dangerous , m y s t e r i o u s , u n e a r t h l y f o r c e ! There\nwas a group o f w i z a r d s i n t h a t t a s k l o c a t i o n 0 .\nNow I don ’ t know , I don ’ t even know what t h e y\nwere d o i n g . But t h e y k i l l e d p e o p l e so I could , I\ncouldn ’ t do a n y t h i n g !\nWell , one o f them d i d s o m e t h i n g t h a t was a b i t\nmore t r i c k y , l e t me t e l l you what . He wanted t o\nbecome a wizard , and he s t a r t e d t o l e a r n , and\nt h e w i z a r d s t r i e d t o s t o p him . So t h e w i z a r d\nbecame a w i z a r d . And now he ’ s , I don ’ t know what\nhe i s , a bad person , b u t he ’ s a v e r y d a n g e r o u s\nl u n a t i c . No one knows where he i s . I want t o\nk i l l him !\n(b) Random output quest descriptions\ngenerated with the fine-tuned Quest-GPT-2\nmodel\nFigure 6. Quest generation examples after the fine-tuning and before the\noptimization of sampling settings. Here using aitextgen’s default settings.\nhigh degree of uncertainty with respect to the quest ingredient\ntokens. The results in Tbl. IV show that placeholder text\nachieves lower perplexity than raw textwith all three metadata\nformats, thus supporting our previous findings. While XML-\nlike always produces the highest perplexities, the narrative\nformat consistently achieves the lowest perplexity regardless\nof the placeholder use and is thus to be preferred.\nBased on these objective metrics, we selected the Quest-\nGPT-2 model fine-tuned with the narrative format and place-\nholder text for the final subjective evaluation.\nD. Exploring Quest-GPT-2 Text Generation Settings\nWe anticipate that even after fine-tuning, many generated\nquests would not convince a human audience. For example,\nFig. 6 shows quest descriptions generated by the fine-tuned\nmodel that might be considered somewhat nonsensical by\npeople. Instead of merely sampling the most probable tokens\nfrom the output probability distribution, methods such as\ntop-k sampling and nucleus sampling have been successfully\nemployed to generate more natural-sounding text [43]. Holtz-\nman et al. [43] have argued that natural language does not\nmaximize probability; humans favor non-obvious language.\nAs a final step before our user study, we determined the\noptimal sampling settings for Quest-GPT-2 model inference\nthrough four mini-studies. The studies were performed among\nthe members of the game AI research group at Aalto Uni-\nversity, and had three participants on average. We generated\nsix to ten quest descriptions for two quests and each of the\nbelow sampling setting configurations, and asked participants\nto rate the descriptions according to their perceived quality\non a 7-point Likert scale. The scale was accompanied with\nthe statement “The quest description fits the quest great”. We\ncompared the following sampling setting configurations:\n• Nucleus sampling with top-p values 0.5, 0.7, and 0.9\n• Top-k sampling with top-k 40\n• Baseline pure sampling\nwith or without the following additional modifiers:\n• Temperature: 0.7\n• Repetition penalty: 1.2\nThe first mini-study compared all sampling setting con-\nfigurations without the additional modifiers, the second one\nintroduced the temperature modifier, the third added in rep-\netition penalty, and the last compared two nucleus sampling\nconfigurations, top-p values 0.5 and 0.9, to each other with\nboth modifiers and two Likert scale statements “The quest\ndescription fits the quest great narratively” and “The quest\ndescription fits the quest great in terms of correctness.”\nIt is difficult to balance the narrative quality and the cor-\nrectness of details: one needs to find the sampling settings\nthat produce an optimal degree of randomness to generate\ninteresting yet sensible quest descriptions. We found that nu-\ncleus sampling with top-p=0.5, temperature=0.7, and repetition\npenalty=1.2 produced the best results with Quest-GPT-2.\nE. Rejecting Quest-GPT-2 Outputs\nTo further improve the model outputs, we implemented two\nsimple heuristic filters that reject bad samples. Both filters\nexploit our special placeholder tokens (Fig. 4).\nThe first filter performs token verification, i.e. it checks\nwhether the special tokens in the output also exist in the\ninput. For instance, the example quest input in Fig. 4 (i.e.,\nlines up to and including the < |begin description| >) does\nnot include any named groups or related group n tokens.\nConsequently, the resulting output quest description (i.e., lines\nafter < |begin description| >) should not contain said tokens\neither. The second filter complements the first: it checks that\nimportant, user-configurable special tokens in the input are\npresent in the output. This filter can ascertain that only outputs\nare retained which contain certain desired quest elements, e.g.\nthe output description in Fig. 4 should mention character 0.\nVI. E VALUATING QUEST -GPT-2\nWriting RPG quest descriptions is usually considered a cre-\native activity, and we thus want Quest-GPT-2 to be a creative\nsystem. Assessing creativity however is not easy, and defining\ncreativity alone is a source of debate among (computational)\ncreativity researchers [44, p. 77ff.]. Most researchers however\nagree that a creative product must be novel and valuable [45]\nto be deemed creative. Assessing the novelty of generated\nartifacts however is not straight-forward, as perceived nov-\nelty is highly contingent on individual experience [46]. We\nconsequently focus on assessing the quality of the generated\nquests, and complement ratings with open-ended questions to\ngather further information on what influenced our participants’\nassessment. We next present our evaluation methods, describe\nthe results, and, finally, discuss them critically.\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nA New DebtVald’s Debt\nAssassin at LargeAshes to EternityA Talkative PetThe Golden Claw\nTaking Notes\nThe Prey\nBreaking the Siege\nBuying a House in Skingrad\nPain in the Necklace\nPut the Spirit of the Child Wellyn to Rest\nBrielbara’s BabySet Aerie FreeClimb the Steps\nThe Paranoid Roof Watcher\nBody Work\nAﬀairs of a WizardThe Potato Snatcher\nAwakening\nKyne’s Sacred Trials\nA Shattered Visage Lies\nWeather the StormThe Sunken OneElytra Air ShowThe Impatient JarlA Precious HorseThe Firewine Ruins\nPilgrimage\nA Fancy New Shirt\nGet Outﬁtted\nThe Path of KnowledgeKill the Forsworn LeaderBuying a House in Bravil\nSanctuary\nInvestigate the Fallen PaladinsThe Tale of Commander Brage\nFleeting StrengthThe Blue Boletus\nMephala’s Web of Lies\nNature’s FuryTrinity RestoredA Drink to WinFirebead’s ScrollBlood Money\nHeeding the Spirit of the Grove\nRefuge for the Elves\nThrough A Nightmare, Darkly\nSpies\nVai’s Bounty Upon Bandits\n1\n2\n3\n4Rating\nFigure 7. Box plots of quest description ratings for each of the 50 quests in the test set, sorted by the median in ascending order. Each point represents\nparticipants’ mean ratings on a quest description produced for the corresponding quest.\n(a) All quests\n1\n2\n3\n4Rating\ndeliveryfetch\ninvestigate\notherrescuekill\n(b) Quest types\nshortmedium\nlong\n(c) Quest outline length\nFigure 8. Box plots of quest description ratings distinguished by quest types.\nEach point represents the mean rating for a quest in the test set.\n1–4 hours 5–8 hours 9–12 hours 13–16 hours\nMore than 16 hours\n1\n2\n3\n4Rating\nFigure 9. Box plots of averaged ratings per participant, grouped by their\naverage weekly playtime (groups holding <5% participants were omitted).\nBaldur’s GateBaldur’s Gate II\nMinecraft\nTES IV: OblivionTES V: SkyrimTorchlight II\n1\n2\n3\n4Rating\nFigure 10. Box plots of averaged ratings per participant, grouped by game.\nA. Experiment Design\nWe performed a randomized mixed design user study in the\nform of an online questionnaire in which participants were\npresented with quests and asked to rate corresponding quest\ndescriptions. We chose a mixed design to obtain ratings on\nmany quest descriptions produced from many quests, while\navoiding fatigue that could negatively impact response quality.\nB. Materials\nParticipants were presented with a quest from the test set\nthat was set aside during fine-tuning (Sec. V-C). For each quest\nin the test set, we generated ten quest descriptions with Quest-\nGPT-2, utilizing the improvements from Sec. V-D and V-E.\nBased on the 50 random quests in the test set (sampled pro-\nportionally from each game in our quest data set as mentioned\nin Sec. V-C), we obtained a total of 500 quest descriptions\nas stimuli in the study. Table V illustrates the same natural\nlanguage metrics as Table II on the generated descriptions.\nThe generated descriptions are noticeably simpler, i.e. easier\nto read and shorter, than the original human-authored ones. All\nquests and quest descriptions are available in a public Open\nScience Foundation repository 1.\nThe quests and their generated descriptions were embedded\nin an online questionnaire. For improved readability, the\nquests were presented in the simple format (Fig. 1b) without\nplaceholders, instead of the narrative format with placeholders\nwhich was used in fine-tuning Quest-GPT-2.\nTo keep the individual workload manageable, each par-\nticipant received five quest descriptions from five randomly\nsampled test set quests, i.e. 25 quest descriptions in total. To\ncounteract fatigue, the five quests were always presented along\nwith their description instead of interleaving the quests with\neach other. The presentation order of the quest descriptions\nfor each quest was randomized to avoid order effects.\nC. Participants\nThe study participants were recruited from various RPG\nsub-communities on Reddit and r/SampleSize, a sub-\ncommunity dedicated to (scientific) surveys. The study was ad-\nvertised toward everyone aged over 18 years with RPG playing\nexperience. We did not offer any incentives for participation.\nOverall, 349 respondents participated in the questionnaire,\nof which 345 responses were retained. We excluded three\nrespondents, as they only provided empty or one-word answers\nto our free-form questions. Additionally, one respondent was\nexcluded due to being under 18 years old. The gender break-\ndown of participants was 71.9% male, 20.0% female, 4.9%\ngender variant / non-conforming, 0.6% other, and 2.6% pre-\nferred not to state their gender. 97.1% of participants stated\ntheir age, ranging from 18–62 years (M=28.7, SD=8.1).\nThe participants reported their average weekly gaming time\nas follows: 0.9% played less than an hour, 7.5% 1–4 hours,\n15.1% 5–8 hours, 23.8% 9–12 hours, 15.7% 13–16 hours,\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nTable V\nMEAN NATURAL LANGUAGE PROCESSING METRICS (MEAN ± STDDEV ) ON\nTHE GENERATED QUEST DESCRIPTIONS\nGame Readability(Flesch-Kincaid Grade)smaller easier\nSyntactics Complexity(Dependency Distance)larger more complex\nLexical Richness(Type-Token Ratio)larger richer\nWordCount\nBaldur’s Gate[35] 2.53±1.20 2 .18±0.24 0 .78±0.06 90 ±36\nBaldur’s Gate II[36] 1.93±1.47 2 .13±0.32 0 .74±0.07 98 ±39\nThe Elder Scrolls IV[37] 2.74±1.18 2 .21±0.23 0 .71±0.07 127 ±50\nThe Elder Scrolls V[13] 2.39±1.37 2 .18±0.21 0 .73±0.08 104 ±46\nMinecraft[38] 1.32±0.98 2 .04±0.25 0 .78±0.07 65 ±22\nTorchlight II[39] 2.98±1.06 2 .17±0.23 0 .72±0.08 95 ±29\nOverall 2.38±1.33 2 .17±0.24 0 .74±0.08 103±46\n35.1% more than 16 hours, and 2.0% preferred not to say.\nRegarding the participants’ familiarity with RPGs, 35.4%\nhad played Baldur’s Gate, 30.1% Baldur’s Gate II, 58.8%\nMinecraft, 58.6% The Elder Scrolls IV: Oblivion, 83.2% The\nElder Scrolls V: Skyrim, 26.7% Torchlight II, 76.8% other\nRPGs, and 0.3% preferred not to say. When asked about\nother RPG games, the participants listed dozens of Western,\nJapanese, table-top inspired and MMORPGs, confirming that\nmost participants were avid, experienced RPG fans.\nD. Measures\nWe gathered demographic data on age and gender, as well\nas player expertise data based on the number of hours spent on\nplaying games per week, and players’ favourite RPGs (detailed\nquestions and answer options provided in our public reposi-\ntory1). Participants were asked to rate each quest description\non a 4-point Likert scale (Strongly Disagree – Strongly Agree),\nindicating their agreement with the statement “I would be\nhappy to see this quest description in a video game.” An even\nscale was chosen to disallow neutral ratings and support the\nratings’ interpretation as separating unsuitable (mean rating\n≪ 2.5) from suitable (mean rating ≫ 2.5) descriptions. We\nmoreover asked the following free-form questions:\nQn 1. Which criteria did you use to assess the suitability\nof each quest description?\nQn 2. What upset you most about the unsuitable quest\ndescriptions?\nQn 3. What did you like most about the suitable quest\ndescriptions?\nThe first question was used to understand participant’s criteria\nin assessing quest descriptions, and the last two were used to\ndetermine the strengths and weaknesses of the descriptions.\nE. Procedure\nFirstly, the participants were asked to read and agree to\nan informed consent form. They were then asked to provide\ndetails on demographics and expertise. In the main part of\nthe questionnaire, the participants were shown blocks of (i) a\nrandom quest, and (ii) five different descriptions generated\nfor this quest. After rating all five quest descriptions, they\nwere presented with another quest with the corresponding\ndescriptions. This process was repeated five times, until each\nparticipant rated five quest descriptions for five quests. Finally,\nthe participants were given the previously described free-form\nquestions. Each step is illustrated in our public 1 materials.\nI am Mogrul , a money− l e n d i n g\nm e r c h a n t . I ’m n o t s u r e i f you ’ ve\nh e a r d o f me b e f o r e , b u t I ’m one o f\nt h o s e p e o p l e who can l e n d a hand t o\nanyone i n need .\nMy name i s Mogrul . You might know me\na s Mogrul , o r maybe a s Drovas R e l v i .\nI was h i r e d by N e l o t h t o h e l p him\no u t w i t h a problem he had . I t t u r n e d\ni n t o q u i t e t h e h e a d a c h e f o r b o t h o f\nus .\nYou ’ r e g o i n g t o h e l p me d e a l w i t h\ni t . I ’ l l g i v e you s o m e t h i n g c a l l e d\n1 ,000 s e p t i m s . T h i s i s a new form o f\nc u r r e n c y t h a t we ’ r e t r y i n g o u t h e r e\na t my l o c a t i o n . We don ’ t have much\no f i t yet , so I ’m g i v i n g you a s m a l l\namount t o h e l p you g e t s t a r t e d .\nI f you e v e r need a n y t h i n g e l s e , j u s t\nask .\nI ’m n o t s u r e i f you ’ r e aware o f i t ,\nb u t I ’ ve been l o o k i n g f o r someone t o\nh e l p me o u t w i t h my c u r r e n t\ns i t u a t i o n .\nMy name i s Vex . You might know me a s\nVald . I was once your l o v e r , u n t i l\nwe were b e t r a y e d by our mutual\nf r i e n d Mercer Frey . We had a good\nrun t o g e t h e r , b u t now he has t u r n e d\non us b o t h . He owes us money , so I\nneed t o g e t h i s d e b t o r s o f f our\nb a c k s .\nI f you c o u l d s p e a k t o Maven\nBlack − B r i a r a b o u t g e t t i n g r i d o f\nVald ’ s d e b t s , t h a t would be g r e a t .\nI f you can ’ t do t h a t , t h e n I ’ l l have\nt o k i l l him m y s e l f .\nI ’m s o r r y . . . I was j u s t h a v i n g a bad\nday . I don ’ t know what happened .\nI t a l l s t a r t e d when someone t r i e d t o\nk i l l me . I t was a Thalmor . They were\na f t e r my j o b and t h e y t h o u g h t I was\none o f them . I had t o f i g h t them o f f\nb u t i t t o o k t o o l o n g . I couldn ’ t g e t\naway f a s t enough . I r a n i n t o a\nDunmer named t h e K h a j i i t . He wanted\nt o k i l l me so he came a f t e r me .\nHe was g o i n g t o k i l l me anyway , b u t\nnow he doesn ’ t want t o . He s a y s i f I\ngo back t o Windhelm , I ’ l l d i e . I\nt o l d him I ’ d s t a y o u t o f Morrowind .\nI didn ’ t t e l l him where I was g o i n g .\nI f you e v e r need a n y t h i n g , come s e e\nme .\n(a) The worst rated quest\ndescriptions for the three worst\nrated quests: A New Debt, Vald’s\nDebt, and Assassin at Large.\nI am O f f i c e r Vai , a member o f t h e\nFlaming F i s t . I ’m h e r e t o t a l k a b o u t\ns o m e t h i n g t h a t has been p l a g u i n g us\nf o r q u i t e some t i m e now .\nWe’ ve had r e p o r t s o f b a n d i t s\no p e r a t i n g i n t h e a r e a around Sword\nC o a s t f o r q u i t e some t i m e now . They\nseem t o be t a r g e t i n g t r a v e l e r s and\nm e r c h a n t s a l i k e . We’ r e n o t s u r e who\nt h e y work f o r , b u t we know i t i s n ’ t\ngood .\nI t seems a s though t h e s e b a n d i t s a r e\nworking f o r someone e l s e . I f you\nc o u l d t r a c k down t h e p e r s o n o r\np e o p l e b e h i n d t h i s , we would reward\nyou w i t h 50 g o l d p i e c e s p e r b a n d i t\ns c a l p .\nI am J a u f f r e , m i l i t a r y l e a d e r h e r e\na t Cloud R u l e r . I ’ ve been h e a r i n g\nrumors a b o u t s t r a n g e p e o p l e coming\ni n t o our town .\nWe need t o f i n d t h e s e p e o p l e b e f o r e\nsomeone e l s e does ! We don ’ t want any\nt r o u b l e w i t h t h e C o u n t e s s o f Bruma ,\nb u t we do need t o g e t r i d o f t h e s e\np e o p l e .\nYou ’ r e g o i n g t o h e l p us t r a c k down\nt h e s e p e o p l e . You ’ l l go t o Cloud\nR u l e r and s p e a k t o S t e f f a n , one o f\nt h e g a t e g u a r d s . He ’ l l t e l l you what\nhe knows . Then you ’ l l head o v e r t o\nBruma and s p e a k t o C a p t a i n Burd , t h e\nc a p t a i n o f t h e g u a r d . T e l l him what\nyou found . I f t h e r e ’ s a n y t h i n g\ns u s p i c i o u s , l e t me know .\nI ’ ve g o t a problem h e r e i n Arcane\nU n i v e r s i t y . A s t u d e n t named\nH e n a n t i e r . He was s t u d y i n g w i t h me\nl a s t s e m e s t e r , b u t he g o t h i m s e l f\ni n t o t o o much t r o u b l e . I ’m n o t s u r e\nwhat went wrong , b u t i t seems l i k e\nhe ’ s been d o i n g some v e r y d a n g e r o u s\nt h i n g s .\nHe doesn ’ t seem t o c a r e a b o u t\na n y t h i n g anymore . He j u s t k e e p s\ng e t t i n g i n t o more and more t r o u b l e .\nI t ’ s a l l my f a u l t . I t o l d him t o\ns t o p messing around w i t h magic , b u t\nhe didn ’ t l i s t e n . Now he ’ s s t u c k i n\na p l a c e c a l l e d Leyawiin . There ’ s\nn o t h i n g I can do f o r him .\nI f you c o u l d g e t H e n a n t i e r o u t o f\nt h e r e , I ’ d g i v e you a s i z e a b l e\nreward .\n(b) The best rated quest\ndescriptions for the three best\nrated quests: Vai’s Bounty Upon\nBandits, Spies, and Through A\nNightmare, Darkly.\nFigure 11. Examples of best and worst rated quest descriptions.\nF . Results\nWe found strong variations in the perceived quality of quest\ndescriptions (Fig. 7) within and beyond individual quests. If\nwe interpreted strong deviations from the Likert midpoint as a\nreliable indicator of suitability, then many quests had a mix of\nsuitable and unsuitable quest descriptions. The median rating\nover all quests is slightly above 2 and thus below the midpoint\nof our 4-point Likert scale (Fig. 8a). We did not find any\nstriking differences in ratings when categorizing quests by\ntheir type (Fig. 8b), outline length (Fig. 8c), or the game they\noriginated from (Fig. 10). Participants generally appear more\ncritical the more they played (Fig. 9). The exception are those\nwho reported playing for more than 16 hours per week, which\nalso includes “hard-core” gamers. We performed a one-way\nANOV A to further investigate the effect of reported playtime\non the participants’ ratings, yielding that differences between\nthe groups are only slightly significant (F=2.3, p=0.063).\nBased on participants’ rich answers to our free-form ques-\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\ntions, we learned that players used various criteria to assess\nthe suitability of the quest descriptions (Question 1). The most\noften mentioned criteria include correctness in regards to the\ngiven quest outline, internal logic as well as coherence, tone\nand immersiveness. Other common criteria were interesting-\nness, the lack of repetition, grammar, narrative flow, and clear\ninstructions. Sporadically, participants noted humor, the length\nof the quest description, and the feelings that are evoked while\nreading the quest descriptions as assessment criteria. There\nwere notable differences in how the participants applied their\ncriteria. In particular, participants were not equally-minded\nabout the importance of criteria, such as grammar, and a\nsmall subset of participants’ answers indicate that they were\nlenient with their ratings, as (i) they knew that they were\nreading AI-generated text (“If these numbers went from 1-10\ninstead of 1-4, I think they’d get the same ratings, for the\nmost part”), (ii) they were not native English speakers (“note:\nI’m not native speaker”), or (iii) they appreciated the unin-\ntentional humor often found within computer-generated text\n(“They [suitable descriptions] were humorous at times”). Our\nparticipants’ comments on unsuitable (Question 2) and suitable\n(Question 3) quest descriptions echoed their assessment crite-\nria. The unsuitable quest descriptions failed and the suitable\nones fulfilled them. Unsuitable descriptions were lamented to\nbe non-sensible or illogical, contained unnecessary details,\nrepetition and conflicting information, had poor grammar to\nthe point of “reading ‘off’ as if poorly translated from a\nChinese comic”, or were simply boring lists of facts. On the\ncontrary, suitable descriptions were found clear, surprising,\nfun, original, and believable even to the point of being\nseemingly human-authored, thus supporting that our model\nmarks a step forward in achieving less repetitive and formulaic\ncomputer-generated quests. Some participants noted that there\nwere no suitable quest descriptions in their subset, supporting\nour finding that the descriptions vary greatly in quality.\nOn a general note, it seems that there is no objective con-\nsensus for what makes a good quest description: some study\nparticipants preferred short, no-nonsense descriptions without\nunnecessary details, whereas others liked longer descriptions\nlaced with in-game lore. Regarding quest objectives, there\nwere participants who would rather only receive hints about\nwhat to do, and others who preferred in-dept instructions.\nFig. 11 shows examples of the worst and best rated quest\ndescriptions. In addition to highlighting many of the partic-\nipants’ thoughts on unsuitable quest descriptions, the badly\nrated descriptions indicate that Quest-GPT-2 sometimes fails\nto discern different entities from each other even if unique\nnames are substituted with generic placeholders. This behavior\nis likely inherent to GPT-2, and made worse with compli-\ncated relationships between different characters. For instance,\nMogrul, the quest-giver of “A New Debt”, and Drovas Relvi,\nMogrul’s debtor in the same quest, are supposed to be different\npeople, yet in the top-most quest description in Fig. 11a the\nquest-giver states that “My name is Mogrul. You might know\nme as Mogrul, or maybe as Drovas Relvi.”\nWe provide all responses, quantitative and qualitative,\nas well as the computation of the summary statistics, in\nanonymized form in our public repository 1.\nG. Discussion\nOur results suggest that even the largest variant of GPT-2,\nfine-tuned on our well-curated data set, cannot be used to au-\ntonomously generate high-quality quest descriptions reliably.\nThis confirms findings in related work [22]. We especially\nfound that Quest-GPT-2 lacks the ability (i) to distinguish\nbetween multiple entities, and (ii) to “glue” quest ingredients\nwell together while not relaying illogical information.\nThe model’s direct successor, GPT-3, has been shown to\noffer vast, general improvements in text quality [4], and we\nhypothesize that GPT-3 would handle these two aspects of\nquest description generation better. To support this hypothe-\nsis, we have provided the quest with the worst rated quest\ndescription in our experiment, “A New Debt” (Fig. 11a), as\ninput to the vanilla GPT-3 model. In comparison to Quest-\nGPT-2, the descriptions generated by GPT-3 (Fig. 12) are\nnoticeably more coherent than the worst rated Quest-GPT-\n2 descriptions. Given suitable hardware for fine-tuning and\ntweaks such as our placeholder text, we believe that this\nnext generation of models can bring fully autonomous quest\ndescription generation within the reach of game developers.\nWe advocate several use-cases for our present model. Firstly,\nmany of the poorly rated quest descriptions outputted by\nQuest-GPT-2 only contain few issues, such as a single illogical\nsentence. Therefore, the model could be used as an assistant\nfor co-creative quest writing: a professional RPG writer could\nfirst give a rough, simplified quest outline to Quest-GPT-2, and\nthen fill in more complex details into the generated output.\nSecondly, Quest-GPT-2 could be used to generate quest ideas:\none can supply the starting sentence of a quest outline to gen-\nerate the rest of the outline and the quest description. Thirdly,\nQuest-GPT-2 could be used to generate quest descriptions\noffline which can, after only little human curation, be used\nin a video game without further changes. This is supported\nby the observation that some quest descriptions were rated\nhighly by people. The curation coefficient, i.e. the ratio of\nhuman-acceptable outputs from any given creative system [47]\nis 0.22, indicating that roughly one in five quest descriptions\nwould be deemed acceptable.\nWe finally reflect on the limitations of our study. Firstly,\nwe observed both positive and negative bias toward AI-\ngenerated text. The former was evident from the participants\nusing lenient ratings as described previously, and the latter\nwas observed from e.g. one of the participants describing\nbad experiences with procedurally generated quests from The\nElder Scrolls V: Skyrim [13]. Such biases are well known\nwhen people judge computer-generated artifacts [47]. To al-\nleviate them, we recommend comparing human-written and\nAI-generated quest descriptions in future studies. Turing-style\ntests on creative systems have been criticized [48], and we\nhence suggest to omit any explicit mention of this dichotomy.\nA second limitation of our study is given by its focus on RPG\ngames with medieval-esque fantasy settings. Generalizing our\nfindings to other settings is not advisable, as the model’s\ncapacity to generate text on a specific theme depends on the\npresence of this theme in the original pre-training data set. A\nthird limitation is given by the gender imbalance which was\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\nT h i s i s an RPG q u e s t from\nThe E l d e r S c r o l l s V:\nSkyrim .\nThe q u e s t − g i v e r i s c a l l e d\nMogrul . Mogrul i s a\nmoney− l e n d e r .\nThe q u e s t − g i v e r g i v e s a\nq u e s t t o t h e p l a y e r . The\np l a y e r ’ s o b j e c t i v e i s t o\nh e l p Drovas R e l v i w i t h h i s\nd e b t t o Mogrul .\nThe p l a y e r s h o u l d f i r s t\npay Drovas R e l v i ’ s d e b t t o\nc o m p l e t e t h e i r o b j e c t i v e .\nThe p l a y e r w i l l r e c e i v e\nt h e f o l l o w i n g r e w a r d s f o r\nc o m p l e t i n g t h e q u e s t\no b j e c t i v e : Mogrul w i l l\ns t o p s e n d i n g h i s d e b t\nc o l l e c t o r s a f t e r t h e\np l a y e r .\nThe f o l l o w i n g c h a r a c t e r s\na r e r e l a t e d t o t h i s q u e s t :\nDrovas R e l v i ( a man t h e\np l a y e r r e c r u i t e d f o r\nNeloth , and t h u s became\nu n t o u c h a b l e ) , N e l o t h ( a\np o w e r f u l male w i z a r d ) .\nThe f o l l o w i n g i t e m s a r e\nr e l a t e d t o t h i s q u e s t :\n1 ,000 s e p t i m s ( a c u r r e n c y ) .\nThere a r e some i m p o r t a n t\nf a c t s c o n c e r n i n g t h i s\nq u e s t . Drovas R e l v i owes\nMogrul 1 ,000 s e p t i m s :\nMogrul t h i n k s t h a t t h e\np l a y e r s h o u l d pay Drovas\nR e l v i ’ s d e b t .\nT h i s i s t h e q u e s t\nd e s c r i p t i o n , t h e\nq u e s t − g i v e r e x p l a i n i n g t h e\nq u e s t t o t h e p l a y e r :\n”\n(a) Input quest outline in\nthe narrative format\nwith raw text\nI h e a r you ’ ve been h e l p i n g o u t N e l o t h .\nI ’m n o t s u r p r i s e d . N e l o t h i s n ’ t v e r y good\na t r e p a y i n g h i s d e b t s . I h e a r you h e l p e d\no u t Drovas R e l v i , t o o .\nDrovas R e l v i owes me 1 ,000 s e p t i m s . I ’ d\nl i k e f o r you t o pay h i s d e b t . Then I ’ l l\ns t o p s e n d i n g my d e b t c o l l e c t o r s a f t e r you .\nI ’m s u r e you ’ r e wondering what you ’ r e\nd o i n g h e r e . Well , I need your h e l p .\nDrovas R e l v i owes me 1 ,000 s e p t i m s , b u t I\ncan ’ t go a f t e r him m y s e l f . He works f o r\nNeloth , you s e e . I f I a t t a c k him , N e l o t h\nw i l l know .\nI ’m s u r e you can u n d e r s t a n d . I ’m w i l l i n g\nt o pay you 500 g o l d t o h e l p me . I w i l l\nf o l l o w you t o R e l v i ’ s house . You j u s t\nhave t o d e l i v e r t h e money .\nMy a s s o c i a t e w i l l t a k e c a r e o f t h e r e s t .\nFellow , what ’ s t h e word on t h e s t r e e t ?\nDrovas R e l v i owes me 1 ,000 s e p t i m s . That\ncan ’ t be good f o r h i s h e a l t h . Has he\nt a l k e d t o you ?\nI don ’ t c a r e what you g e t him t o do . J u s t\ng e t my money .\nThere ’ s a f e l l o w i n town who owes me a\nf a i r b i t o f c o i n . Drovas R e l v i . I ’ d l i k e\nt o c o l l e c t , b u t he ’ s Neloth ’ s f r i e n d and\nI don ’ t want t o make t r o u b l e .\nI ’m t r y i n g t o h e l p him out , b u t I ’m n o t\ns u r e how . I f you c o u l d t a l k t o him , maybe\nyou c o u l d c o n v i n c e him t o pay me back .\nI f you do t h a t , I ’ l l make s u r e my\nc o l l e c t o r s don ’ t b o t h e r you a g a i n .\n(b) Four random output quest\ndescriptions generated with GPT-3\nFigure 12. A quest generation demo with the quest A New Debtand GPT-3\n(OpenAI API Playground, default text generation settings apart from response\nlength of 700). The quests Ashes to Eternity, Assassin at Large, and Vald’s\nDebt were given as examples to GPT-3 beforehand.\ninherited from the Reddit communities that participants were\nrecruited from, and should in the future be compensated for\nvia other communities and additional recruitment channels.\nVII. C ONCLUSIONS AND FUTURE WORK\nWe have investigated the use of the GPT-2 and GPT-3 lan-\nguage models to generate quest descriptions for RPG games.\nWe built and published a novel quest data set, and employed a\nstrategy for improving learning from limited training data by\nplaceholder substitution similar to [6]. We fine-tuned GPT-2\ninto the quest description generating Quest-GPT-2 model, and\nconducted an online user study to evaluate its output.\nWhile our results are encouraging, the quality of the gener-\nated descriptions varied greatly. Despite the name substitution\nstrategy, Quest-GPT-2 often makes mistakes related to han-\ndling a large number of entities, such as characters, groups, and\nlocations. Moreover, Quest-GPT-2 often generates descriptions\nwith questionable logic, repetition, poor grammar, and unnec-\nessary information. While using our model automatically and\nonline is not yet viable, we have proposed three means on how\nQuest-GPT-2 can already be used by designers offline.\nBased on our case-studies on generating quest descriptions\nwith the vanilla GPT-3 model, we hypothesize that the next\ngeneration of language models could be fine-tuned with (an\nextension of) our quest data set to alleviate the discussed\nissues. Other potential areas of future work are personalizing\nquest descriptions for different kinds of RPG players and\nplayer characters; replacing our simple heuristic filters with\nan AI critic for rejecting dissatisfying model outputs as well\nas using grammar checking tools or other algorithms for\nimproving text quality; and generating other quest-related\nartifacts, e.g. quest names, journal entries and dialogue trees, in\naddition to quest descriptions. Moreover, one could investigate\nexpanding the quest generation system to continuous quest\nlines or multi-step quests by including previous quests or\nquest steps alongside quest ingredients. Bidirectional language\nmodels such as BERT [49] could be investigated to provide\nindividual, fill-in suggestions for all quest ingredients, not only\nthe quest descriptions. Finally, we highlight the opportunity\nfor collaborations between games industry and researchers on\nboth, the use of existing data sets to improve new models, and\nthe latter’s integration in tools for design-time co-creation.\nWe encourage researchers and the general public to adopt\nthe techniques presented here, and extend our publicly avail-\nable code and data set to investigate the future use of large\nlanguage models for video game quest generation.\nAPPENDIX A\nQUEST COLLECTING IN DETAIL\nWe gathered the quests in the following manner. Firstly, the\nquests from Baldur’s Gate I-IIwere extracted by first identi-\nfying the quest-giving non-playable characters by reading the\nBaldur’s Gate Wiki quest descriptions, then looking for and\nselecting the relevant game dialogue files with Near Infinity,\na browser and editor software for games that use the Infinity\ngame engine, and finally using the relevant pieces of dialogue\nto construct proper quest descriptions. Secondly, the skeletons\nfor The Elder Scrolls IV-Vquests were first scraped from the\nUnofficial Elder Scrolls Pages in JSON format: each quest\ncontained information on objective, locations, quest giver, and\nreward. The final quest descriptions were then formulated\nby reading the relevant game files with either The Elder\nScrolls Construction Set (The Elder Scrolls IV) or the Creation\nKit ( The Elder Scrolls V). Lastly, the Torchlight II quests\noriginally collected by van Stegeren and Theune [33] were\nin .csv format with the following fields: speaker (quest-giver),\ntext, dialogue type, quest name as seen in-game, quest name\nin game data, quest file, speaker unit type, unit file, and raw\nquest text. We converted these quests to our JSON schema\n(Appendix B), cleaned them up, and added any missing,\nrelevant information, such as archetypal character descriptions.\nAPPENDIX B\nJSON R EPRESENTATION FOR QUESTS\n” name ” : ” t h e name o f t h e q u e s t ” ,\n” o b j e c t i v e ” : ” q u e s t o b j e c t i v e ” ,\n” f i r s t t a s k s ” : [ ” a l i s t o f t a s k s t h a t s h o u l d be done t o f u l f i l l t h e o b j e c t i v e ” ] ,\n” f i r s t t a s k l o c a t i o n s ” : [ ” a l i s t o f l o c a t i o n s c o r r e s p o n d i n d w i t h t h e t a s k s ,\ns i m i l a r t o t h e l o c a t i o n s f i e l d ” ] ,\n” q u e s t g i v e r ” : {\n” name ” : ” t h e name o r t i t l e o f t h e q u e s t g i v e r ” ,\n” d e s c r i p t i o n ” : ” a b r i e f , a r c h e t y p a l d e s c r i p t i o n o f t h e q u e s t g i v e r ” ,\n” l o c a t i o n ” : ” t h e w h e r e a b o u t s o f t h e q u e s t g i v e r ”\n},\n” reward ” : [ a l i s t rewards , a reward i s d e f i n e d {\n” name ” : ” t h e name o f t h e reward ” ,\n” d e s c r i p t i o n ” : ” a b r i e f , common d e s c r i p t i o n o f t h e reward ” ,\n” amount ” : t h e number o f r e c e i v e d r e w a r d s\n}] ,\n” c h a r a c t e r s ” : [ ( o p t i o n a l ) a l i s t o f r e l a t e d c h a r a c t e r s , a c h a r a c t e r i s d e f i n e d\ns i m i l a r l y t o t h e q u e s t g i v e r ] ,\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\n” enemies ” : [ ( o p t i o n a l ) a l i s t o f r e l a t e d g r o u p s o f enemies , m o s t l y used f o r\nd e c l a r i n g a s e t number o f enemies f o r a q u e s t , a group o f enemies i s d e f i n e d\ns i m i l a r l y t o a reward ] ,\n” i t e m s ” : [ ( o p t i o n a l ) a l i s t o f r e l a t e d i t e m s , e . g t a n g i b l e i t e m s , o r even some\nmore a b s t r a c t ones l i k e r i t u a l s , an i t e m i s d e f i n e d s i m i l a r l y t o a reward ] ,\n” g r o u p s ” : [ ( o p t i o n a l ) a l i s t o f r e l a t e d groups , e . g . f a c t i o n s , r a c e s , o r\nc r e a t u r e s , where a group i s d e f i n e d {\n” name ” : ” t h e name o f t h e group ” ,\n” d e s c r i p t i o n ” : ” a b r i e f , common d e s c r i p t i o n o f t h e group ”\n}] ,\n” l o c a t i o n s ” : [ ( o p t i o n a l ) a l i s t o f r e l a t e d l o c a t i o n s , where a l o c a t i o n i s d e f i n e d {\n” name ” : ” t h e name o f t h e l o c a t i o n ” ,\n” d e s c r i p t i o n ” : ” a b r i e f , common d e s c r i p t i o n o f t h e l o c a t i o n ”\n}] ,\n” t o o l s ” : [ ” i m p o r t a n t f a c t s r e l a t e d t o t h e q u e s t ” ] ,\n” d e s c r i p t i o n ” : ” t h e q u e s t d e s c r i p t i o n ”\nACKNOWLEDGMENTS\nWe thank our reviewers for their excellent, helpful feedback.\nCG was partly funded by the Academy of Finland (AoF)\nflagship program “Finnish Center for Artificial Intelligence”\n(FCAI). The Aalto University School of Science “Science-IT”\nproject provided the GPT-2 fine-tuning infrastructure.\nREFERENCES\n[1] K. Merrick, “Modeling motivation for adaptive nonplayer characters in\ndynamic computer game worlds”, Computers in Entertainment, vol. 5,\nno. 4, pp. 1–32, 2008.\n[2] A. Kantosalo and H. Toivonen, “Modes for creative human-computer\ncollaboration: Alternating and task-divided co-creativity”, in Proc. Int.\nConf. on Computational Creativity, 2016, pp. 77–84.\n[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\nLanguage models are unsupervised multitask learners, OpenAI Blog,\n2019.\n[4] T. B. Brown, B. Mann, N. Ryder, et al., Language models are few-shot\nlearners, 2020. arXiv: 2005.14165 [cs.CL].\n[5] R. Dale, “GPT-3: What’s it good for?”, Natural Language Engineering,\nvol. 27, no. 1, pp. 113–118, 2021.\n[6] L. Martin, P. Ammanabrolu, X. Wang, et al., “Event representations\nfor automated story generation with deep neural nets”, Proc. AAAI,\nvol. 32, no. 1, 868–875, 2018.\n[7] B. Kybartas and R. Bidarra, “A survey on story generation techniques\nfor authoring computational narratives”, IEEE Trans. Computational\nIntelligence and AI in Games, vol. 9, no. 3, pp. 239–253, 2016.\n[8] S. G. Ware and R. M. Young, “Intentionality and conflict in the best\nlaid plans interactive narrative virtual environment”, IEEE Trans. Com-\nputational Intelligence and AI in Games, vol. 8, no. 4, pp. 402–411,\n2015.\n[9] D. Thue, V . Bulitko, M. Spetch, and E. Wasylishen, “Interactive\nstorytelling: A player modelling approach”, in Proc. AAIDE, 2007,\npp. 43–48.\n[10] M. Si, S. C. Marsella, and D. V . Pynadath, “Thespian: Using multi-\nagent fitting to craft interactive drama”, in Proc. AAMAS, 2005, pp. 21–\n28.\n[11] J. McCoy, M. Treanor, B. Samuel, A. A. Reed, N. Wardrip-Fruin, and\nM. Mateas, “Prom week”, in Proc. ACM FDG, 2012, 235–237.\n[12] M. Guimaraes, P. Santos, and A. Jhala, “CiF-CK: An architecture\nfor social NPCS in commercial games”, in Proc. IEEE CIG, 2017,\npp. 126–133.\n[13] Bethesda Game Studios, The Elder Scrolls V: Skyrim, Game [PC],\nBethesda Softworks, Rockville, Maryland, United States., 2011.\n[14] B. Kybartas and C. Verbrugge, “Analysis of regen as a graph-rewriting\nsystem for quest generation”, IEEE Trans. Computational Intelligence\nand AI in Games, vol. 6, no. 2, pp. 228–242, 2013.\n[15] A. Calvin and N. Michael, “The quest in a generated world”, in Proc.\nDiGRA, 2007, pp. 503–509.\n[16] J. Pita, B. Magerko, and S. Brodie, “True story: Dynamically gener-\nated, contextually linked quests in persistent systems”, in Proc. Conf.\non Future Play, 2007, pp. 145–151.\n[17] A. Stocker and C. Alvin, “Non-linear quest generation”, in Proc. Int.\nFlorida AI Research Society Conference, 2018, pp. 213–216.\n[18] J. Doran and I. Parberry, “A prototype quest generator based on a\nstructural analysis of quests from four MMORPGs”, in Proc. Int.\nWorkshop Procedural Content Generation in Games, 2011, pp. 1–8.\n[19] V . Breault, S. Ouellet, and J. Davies, “Let CONAN tell you a story:\nProcedural quest generation”, Entertainment Computing, vol. 38, no. 3,\np. 100 422, 2021.\n[20] E. Soares de Lima, B. Feij ´o, and A. L. Furtado, “Procedural generation\nof quests for games using genetic algorithms and automated planning”,\nin Proc. Brazilian Symposium on Computer Games and Digital Enter-\ntainment, 2019, pp. 144–153.\n[21] P. Ammanabrolu, W. Broniec, A. Mueller, J. Paul, and M. Riedl,\n“Toward automated quest generation in text-adventure games”, inProc.\nWorkshop on Computational Creativity in Language Generation, 2019,\npp. 1–12.\n[22] J. van Stegeren and J. My ´sliwiec, “Fine-tuning gpt-2 on annotated\nrpg quests for npc dialogue generation”, in Proc. ACM FDG, 2021,\npp. 1–8.\n[23] Blizzard Entertainment, World of Warcraft, Game [PC], Blizzard\nEntertainment, Irvine, California, United States, 2004.\n[24] R. Rosenfeld, “Two decades of statistical language modeling: Where\ndo we go from here?”, Proc. of the IEEE, vol. 88, no. 8, pp. 1270–1278,\n2000.\n[25] E. Cambria and B. White, “Jumping nlp curves: A review of natu-\nral language processing research”, IEEE Computational Intelligence\nMagazine, vol. 9, no. 2, pp. 48–57, 2014.\n[26] R. Loughran and M. O’Neill, “Application domains considered in com-\nputational creativity.”, in Proc. Int. Conf. on Computational Creativity,\n2017, pp. 197–204.\n[27] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is all you need”,\nin Proc. Advances in Neural Information Processing Systems, 2017,\npp. 5998–6008.\n[28] A. Ramesh, M. Pavlov, G. Goh, et al., “Zero-shot text-to-image\ngeneration”, arXiv preprint arXiv:2102.12092, 2021.\n[29] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I.\nSutskever, “Jukebox: A generative model for music”, arXiv preprint\narXiv:2005.00341, 2020.\n[30] A. Madani, B. McCann, N. Naik, et al., “Progen: Language modeling\nfor protein generation”, arXiv preprint arXiv:2004.03497, 2020.\n[31] P. Clark, O. Tafjord, and K. Richardson, “Transformers as soft reason-\ners over language”, in Proc. IJCAI, 2020.\n[32] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding by generative pre-training”, Preprint, 2018.\n[33] J. van Stegeren and M. Theune, “Fantastic strings and where to find\nthem: The quest for high-quality video game text corpora”, in Proc.\nWorkshop Intelligent Narrative Technologies, 2020, pp. 1–8.\n[34] D. M. Ziegler, N. Stiennon, J. Wu, et al., Fine-tuning language models\nfrom human preferences, 2020. arXiv: 1909.08593 [cs.CL].\n[35] BioWare, Baldur’s Gate, Game [PC], Interplay Entertainment, Los\nAngeles, California, United States., 1998.\n[36] BioWare, Baldur’s Gate II: Shadows of Amn, Game [PC], Interplay\nEntertainment, Los Angeles, California, United States., 2000.\n[37] Bethesda Game Studios, The Elder Scrolls IV: Oblivion, Game [PC],\nBethesda Softworks, Rockville, Maryland, United States., 2006.\n[38] Mojang Studios, Minecraft, Game [PC], Mojang Studios, Stockholm,\nSweden., 2011.\n[39] Runic Games, Torchlight II, Game [PC], Runic Games, Seattle, Wash-\nington, United States., 2012.\n[40] V . I. Propp, Morphology of the Folktale, 2nd, trans. by L. Scott. Austin,\nUnited States: University of Texas Press, 1968.\n[41] J.-S. Lee, “Controlling patent text generation by structural metadata”,\nin Proc. ACM Int. Conf. on Information & Knowledge Management,\n2020, pp. 3241–3244.\n[42] P. Gerv ´as, “Computational approaches to storytelling and creativity”,\nAI Magazine, vol. 30, no. 3, pp. 49–62, 2009.\n[43] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi, The curious\ncase of neural text degeneration, 2020. arXiv: 1904.09751 [cs.CL].\n[44] C. Guckelsberger, “Intrinsic motivation in computational creativity ap-\nplied to videogames”, English, 306 pages, Ph.D. dissertation, School of\nElectrical Engineering and Computer Science, Queen Mary, University\nof London, 2020.\n[45] M. A. Runco and G. J. Jaeger, “The standard definition of creativity”,\nCreativity Research Journal, vol. 24, no. 1, pp. 92–96, 2012.\n[46] S. McGregor, “Algorithmic information theory and novelty genera-\ntion”, in Proc. Int. Conf. on Computational Creativity, 2007, 109–112.\n[47] S. Colton and G. Wiggins, “Computational creativity: The final fron-\ntier?”, in Proc. ECAI, 2012, pp. 21–26.\n[48] A. Pease and S. Colton, “On impact and evaluation in computational\ncreativity: A discussion of the turing test and an alternative proposal”,\nin Proc. AISB Symposium on AI and Philosophy, vol. 39, 2011.\n[49] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding”, arXiv\npreprint arXiv:1810.04805, 2018.\nThis article has been accepted for publication in IEEE Transactions on Games. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TG.2022.3228480\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}