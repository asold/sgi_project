{
  "title": "A Canary in the AI Coal Mine: American Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training",
  "url": "https://openalex.org/W4393064092",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3134350212",
      "name": "Heila Precel",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A2141787720",
      "name": "Allison McDonald",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A1985391146",
      "name": "Brent Hecht",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A2117801018",
      "name": "Nicholas Vincent",
      "affiliations": [
        "Simon Fraser University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4224990248",
    "https://openalex.org/W3004877964",
    "https://openalex.org/W2611520706",
    "https://openalex.org/W3160343688",
    "https://openalex.org/W2554033510",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4224051134",
    "https://openalex.org/W4226040281",
    "https://openalex.org/W3186466008",
    "https://openalex.org/W4387122748",
    "https://openalex.org/W2557469473",
    "https://openalex.org/W3036414338",
    "https://openalex.org/W2612403777",
    "https://openalex.org/W2552723724",
    "https://openalex.org/W4330336443",
    "https://openalex.org/W4231203432",
    "https://openalex.org/W3212368439",
    "https://openalex.org/W2795908329",
    "https://openalex.org/W2777647957",
    "https://openalex.org/W2731904609",
    "https://openalex.org/W2119188105",
    "https://openalex.org/W2080348803",
    "https://openalex.org/W2060171532",
    "https://openalex.org/W4386242492",
    "https://openalex.org/W2899136066",
    "https://openalex.org/W3121257585",
    "https://openalex.org/W2243047430",
    "https://openalex.org/W2611909381",
    "https://openalex.org/W2898923666",
    "https://openalex.org/W2095420684",
    "https://openalex.org/W4387789198",
    "https://openalex.org/W3162866579",
    "https://openalex.org/W4377865288",
    "https://openalex.org/W3014972121",
    "https://openalex.org/W3190961011",
    "https://openalex.org/W528981",
    "https://openalex.org/W4385714201",
    "https://openalex.org/W2038392907",
    "https://openalex.org/W4385516110",
    "https://openalex.org/W3212464620",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2027293871",
    "https://openalex.org/W4384155024",
    "https://openalex.org/W4250957820",
    "https://openalex.org/W4388455152",
    "https://openalex.org/W2015074633",
    "https://openalex.org/W2550521820",
    "https://openalex.org/W4387985331",
    "https://openalex.org/W4200047321",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3160735296",
    "https://openalex.org/W2945417260",
    "https://openalex.org/W4205725079",
    "https://openalex.org/W4312981273",
    "https://openalex.org/W4366158488",
    "https://openalex.org/W4229065825",
    "https://openalex.org/W4319722964",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W785016865",
    "https://openalex.org/W2072606160",
    "https://openalex.org/W2915567993"
  ],
  "abstract": "Systemic property dispossession from minority groups has often been carried\\nout in the name of technological progress. In this paper, we identify evidence\\nthat the current paradigm of large language models (LLMs) likely continues this\\nlong history. Examining common LLM training datasets, we find that a\\ndisproportionate amount of content authored by Jewish Americans is used for\\ntraining without their consent. The degree of over-representation ranges from\\naround 2x to around 6.5x. Given that LLMs may substitute for the paid labor of\\nthose who produced their training data, they have the potential to cause even\\nmore substantial and disproportionate economic harm to Jewish Americans in the\\ncoming years. This paper focuses on Jewish Americans as a case study, but it is\\nprobable that other minority communities (e.g., Asian Americans, Hindu\\nAmericans) may be similarly affected and, most importantly, the results should\\nlikely be interpreted as a \"canary in the coal mine\" that highlights deep\\nstructural concerns about the current LLM paradigm whose harms could soon\\naffect nearly everyone. We discuss the implications of these results for the\\npolicymakers thinking about how to regulate LLMs as well as for those in the AI\\nfield who are working to advance LLMs. Our findings stress the importance of\\nworking together towards alternative LLM paradigms that avoid both disparate\\nimpacts and widespread societal harms.\\n",
  "full_text": "A Canary in the AI Coal Mine: American Jews May Be\nDisproportionately Harmed by Intellectual Property\nDispossession in Large Language Model Training\nHeila Precel\nBoston University\nBoston, United States\nAllison McDonald\nBoston University\nBoston, United States\nBrent Hecht\nNorthwestern University\nEvanston, United States\nNicholas Vincent\nSimon Fraser University\nBurnaby, Canada\nABSTRACT\nSystemic property dispossession from minority groups has often\nbeen carried out in the name of technological progress. In this paper,\nwe identify evidence that the current paradigm of large language\nmodels (LLMs) likely continues this long history. Examining com-\nmon LLM training datasets, we find that a disproportionate amount\nof content authored by Jewish Americans is used for training with-\nout their consent. The degree of over-representation ranges from\naround 2x to around 6.5x. Given that LLMs may substitute for the\npaid labor of those who produced their training data, they have\nthe potential to cause even more substantial and disproportionate\neconomic harm to Jewish Americans in the coming years. This pa-\nper focuses on Jewish Americans as a case study, but it is probable\nthat other minority communities (e.g., Asian Americans, Hindu\nAmericans) may be similarly affected and, most importantly, the\nresults should likely be interpreted as a “canary in the coal mine”\nthat highlights deep structural concerns about the current LLM\nparadigm whose harms could soon affect nearly everyone. We dis-\ncuss the implications of these results for the policymakers thinking\nabout how to regulate LLMs as well as for those in the AI field who\nare working to advance LLMs. Our findings stress the importance\nof working together towards alternative LLM paradigms that avoid\nboth disparate impacts and widespread societal harms.\nCCS CONCEPTS\n• Social and professional topics →Intellectual property ; •\nComputing methodologies →Machine learning.\nKEYWORDS\nlarge language models, economic impacts, dataset documentation\nACM Reference Format:\nHeila Precel, Allison McDonald, Brent Hecht, and Nicholas Vincent. 2024.\nA Canary in the AI Coal Mine: American Jews May Be Disproportionately\nHarmed by Intellectual Property Dispossession in Large Language Model\nTraining. In Proceedings of the CHI Conference on Human Factors in Comput-\ning Systems (CHI ’24), May 11–16, 2024, Honolulu, HI, USA. ACM, New York,\nNY, USA, 17 pages. https://doi.org/10.1145/3613904.3642749\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0330-0/24/05. . . $15.00\nhttps://doi.org/10.1145/3613904.3642749\n1 INTRODUCTION\nOne of the most prominent critiques of large language models\n(LLMs) is that they train on massive amounts of content without the\nconsent of the authors of that content [8, 59, 73, 91, 98, 103, 109, 124–\n126]. This concern is exacerbated by one of the core promises of\nLLMs: their ability to use patterns in their training data to substitute\nfor the paid labor of those who created said data. People in a wide\nrange of professions (e.g., fiction-writing and journalism) are now\naccusing language modeling companies of not only stealing their\ncontent (e.g., novels and news stories), but also of using this very\ncontent to put them out of a job (e.g., [ 58, 98, 105, 116]). Indeed,\nthe dominant approach to training LLMs has been called LLMs’\n“original sin” [60] and a “property land grab” that is “so brazen\nit has unified a wide range of interests” [ 91]. According to well-\nknown novelist Margaret Atwood [8], LLMs enable an author such\nas herself to be “dispensed with—murdered by my replica...who\nneeds the cow when the milk’s free?”\nLarge-scale property dispossession in the name of progress—\nwhether the property is physical or intellectual—is far from an\nunprecedented event, and history teaches us that it often does not\noccur uniformly across demographic lines (e.g., [ 30, 93]). In this\npaper, we seek to explore whether the “[intellectual] property land\ngrab” [91] by LLM companies continues this historical pattern and\ndisproportionately affects certain groups more than others. The\nstakes here are very high: it is not just the right to control what\npeople can do with their content that is at risk (i.e., intellectual\nproperty dispossession harms ); the labor substitution potential of\nLLMs means that the ability to pursue one’s chosen profession\nmay also be seriously affected [1, 16, 39, 130] (i.e., labor substitution\nharms).\nWe focus on property dispossession from American Jews as a\ncase study in this paper, motivated by 1) the long history of prop-\nerty dispossession—including intellectual property (IP)—suffered by\nJewish populations [5, 10, 84], and 2) the contextual expertise of the\nauthor group, which can be particularly valuable given the sensitive\nnature of studies like this one. Importantly, however, this line of\ninvestigation is likely relevant to many other minority groups (e.g.,\nAsian Americans, Hindu Americans) and, as we discuss below, far\nbeyond these groups as well. Model builders’ intentional decision\nto not provide public documentation of the data used for training\ncontributes substantially to the difficulty in studying dispossession\nat scale, making a case-study based approach much more tractable\nat this time.\nThe results in this paper are clear and concerning: our findings\nindicate that American Jews are likely to be disproportionately\naffected by language models’ alleged theft of intellectual property\narXiv:2403.13073v1  [cs.CY]  19 Mar 2024\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Precel et al.\nand the corresponding downstream effects on the value of their\nlabor. Using inference methods developed in Jewish demographic\nliterature, we find that content authored by American Jews is sub-\nstantially over-represented in important LLM training datasets by\nbetween approximately 2x and approximately 6.5x. These findings\nraise the possibility that if the current language modeling paradigm\nis allowed to continue, the initial wave of economic disruption\ncould introduce a novel and significant material bias against Jewish\npopulations.\nThis paper likely has a number of significant implications for\nlegal experts, policymakers, researchers, and the tech industry. Per-\nhaps most urgently, our results highlight that any legal finding that\ncurrent LLM training practices are allowed under some definition of\n“fair use” (or any new law along those lines; see [68] for an overview)\ncould have a substantial disparate negative impact on American\nJews, and probably other minority groups as well. Critically, our re-\nsults should also encourage policymakers, funders, and researchers\nto shift resources away from the current language model paradigm\nand towards promising new approaches that allow for power and\nrevenue to be distributed to content owners in a fashion that is\nmore aligned with the value they create for LLM companies. We\nprovide some discussion of these alternative paradigms below.\nThe results in this paper also add urgency to calls (e.g., [16]) to fo-\ncus significantly more research and development resources on LLM\napplications that augment or complement human labor and signifi-\ncantly fewer resources on applications that substitute for human\nlabor. This requires the creation of significant extrinsic incentives\nto make it more likely that this occurs (e.g., policy incentives). As\nwe discuss below, the labor substitution harms highlighted in this\npaper can be significantly mitigated if we as an industry are suc-\ncessful in this endeavor. Many researchers think extensive labor\nsubstitution is likely (e.g., [16, 39, 52]) and, importantly, near-total\nlabor substitution is the explicitly stated goal of key LLM actors\nlike OpenAI [90]. However, it is possible that through changes in\nthe sociotechnical landscape of LLMs and the technologies they\npower—changes that researchers and practitioners in the HCI com-\nmunity can help drive—some of this family of harms can be averted\nand even reversed.\nIt is important to acknowledge that this paper deals with unusu-\nally sensitive issues, the very highlighting of which in a research\npaper could cause harm. While we made every attempt to make\nevidence-backed decisions to minimize any negative effects to the\nJewish community and others (e.g., from anti-Semitic actors), some\nrisk could remain. Ultimately, the author group—of which half is\nJewish—estimated that the benefits of highlighting the uncovered\nevidence far outweighed the potential harms of publishing it. Ad-\nditionally, in developing our methodologies, we consulted with\nseveral demographers of Jewish populations, who have extensive\nexperience handling these challenges in their work.\nFinally, before continuing with Related Work, it is useful to\nreflect on the title we selected for the paper. It is often said that\nwhen new forms of prejudice emerge against the Jewish population,\nthey are a “canary in the coal mine” [42, 88] for serious systemic\nissues whose harms will soon spread well beyond Jews. Applying\nthis analogy to the findings in this paper is imperfect in some\nways, e.g., we know of no evidence of the antisemitic intent that\nis common when the analogy is typically employed. However, the\n“canary in the coal mine” phrase does capture possibly the most\nimportant property of our results: rather than anything specific\nto the American Jewish community, the effects we observe here\nmust be viewed as an additional flashing-red warning sign for the\nfoundational flaws in the current LLM paradigm that are already\naffecting hundreds of millions of non-Jews as well. Indeed, without\nstrong action, these harms may extend to nearly all participants\nin the economy; for example, researchers are working to use LLM\ntechniques for robotics in ways that would create similar challenges\nfor those who earn their living from manual labor [ 133]. All of\nthat said, shared harms also means shared solutions: the many\npromising alternatives to the current LLM paradigm that are being\nexplored—changes that need more attention and resources—can\nnot only remedy issues for American Jews, but can do the same\nfor the much larger group of people outside the Jewish community\nwho will otherwise be similarly harmed.\n2 BACKGROUND\n2.1 Broader Historical Context on Property\nDispossession\nThere is an extensive literature on the long history of property\ndispossession from marginalized groups, which we present here\nnot to draw a direct comparison but rather to situate the current\ndispossession within historical context. One key teaching of this\nliterature is that the effects of dispossession can be both tremen-\ndous and long-lasting. The nearly total dispossession of property\nrights—including bodily rights—suffered by African slaves in the\nUnited States led to a wealth gap that has lasted for generations\nsince slavery ended [29]. The effect of property dispossession on\nindigenous peoples around the world is similarly long-lasting [18].\nAdvocates of the current LLM paradigm argue that non-\nconsensual training on content is necessary to unlock the signif-\nicant technological progress manifest in LLMs. The literature on\nproperty dispossession highlights that this type of justification is\ncommon: “progress”, including technological progress, is often a\nkey part of the stated justification for the seizing of property rights\nfrom marginalized groups. For example, during the English Enclo-\nsure movement, large swaths of the poor English population lost\nrights to farm on land they had been using for generations. New\nagricultural techniques were one stated reason Enclosure policies\nwere enacted [93]. This justification ignored ways the “technologi-\ncal dividend” from the use of new techniques could be distributed\nmore equitably to those who had the original property rights [40].\nThe dispossession of property rights that occurred in Enclosure\nled to decades of major riots and civil unrest [ 74]. An interest-\ning precedent also comes from the story of Henrietta Lacks, an\nAfrican-American woman whose cells helped create countless new\ninnovations in healthcare but were acquired and used without her\nknowledge [30]. Her descendants recently reached a “groundbreak-\ning” [92] settlement with a major biotechnology company over\ntheir claims that the company was “unjustly enriched” by using her\ncells without her consent, an argument that some have made about\nthe LLM industry [103].\nMass property dispossession from Jews has occurred for cen-\nturies. Jews in most of Eastern and Central Europe were forbidden\nfrom owning land for much of the second millennium [ 118], for\nAmerican Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nexample. Similarly, the Expulsion of Jews from Spain in 1492 was\npreceded by the forced liquidation of all Jewish property in that\nregion (at significantly depressed values) [10]. Unsurprisingly, the\nNazi government in Europe in the 1930s and 1940s coupled its\npolicies that led to the death of nearly half the Jewish popula-\ntion [85] with policies that seized nearly all Jewish property in\nNazi-controlled territories [84]. These policies, which are grouped\nunder the label “Aryanization” [84], forced Jews to either sell prop-\nerty at greatly reduced prices or, beginning in 1938, have it seized\nby the government.\nThe Nazi dispossession of property rights from Jews was not\nlimited to physical property: the government also seized Jewish in-\ntellectual property. For example, in 1939, all Jews in Nazi-controlled\nterritories had to change their middle names to “Israel” or “Sara” to\nincrease the visibility of their Jewish identities, which had the effect\nof making it very difficult to renew (or apply for) any IP protections.\nThis led to the complete reproduction of Jewish works without the\nconsent of their authors [5] and even to Jewish scientists leaving\ntheir names off of patents so that the patents would be granted [11].\nOf course, no direct comparison can be made between events\nlike the Holocaust and the IP dispossession we discuss in this paper.\nIndeed, we are aware of no evidence that dispossession by LLM\ndevelopers intends to target the affected parties. However, the IP\ndispossession we identify in this paper is not without historical\nparallels and this context is necessary to understand the lasting\neffects that IP dispossession has had on affected groups throughout\nhistory.\n2.2 LLMs and Economic Harms\nFor people who create content, current LLMs create two separate\nbut compounding economic harms: 1) dispossession of intellectual\nproperty rights, and 2) the use of the dispossessed IP to substitute for\ntheir labor. Below, we discuss research that highlights the presence\nand significance of both of these harms. We highlight how it is\nuseful to understand these harms separately, and while one may\ncome to dominate in material outcomes (e.g., substitution could\nbe the primary concern in a world in which LLMs act as labor-\nreplacing technologies), both require addressing.\n2.2.1 IP Dispossession Harms. It is now public knowledge that\nall prominent large language models have been trained on enor-\nmous amounts of digital content—both natively digital content (e.g.,\nWikipedia and forum comments) and digitized documents (e.g., nov-\nels and scientific publications). With few exceptions, this is done\nwithout the consent of the creators of that content and without\nany form of compensation going to those creators [91]. This is true\nboth for language models that generate text (e.g., OpenAI’s Chat-\nGPT [96] and Meta’s LLaMA [46]) and those that generate images\n(e.g., Stable Diffusion [59, 122]).\nThose who make content for a living are beginning to take signif-\nicant action against model builders in an attempt to reclaim some\nof the rights to their IP. OpenAI, Meta, and other companies that\nbuild and use LLMs are subject to a large and growing number of\nlawsuits, an effort that has been led in part by prominent Jewish\ncontent creators such as Michael Chabon [ 46] and Sarah Silver-\nman [32]. Major content-creating institutions are also beginning\nto sue model builders; most notably, The New York Times has sued\nMicrosoft and OpenAI [47, 96]. Legal action is far from the only\navenue being used. The Authors’ Guild and many others [48] are\nworking to enact new legislation in many parts of the world to\nprotect creator rights, and content owners are taking direct action\nto prevent their content from being used without their consent.\nFor example, most prominent news websites now forbid model\nbuilders from accessing their content by editing their robots.txt\nfiles [15, 96].\nThere is much legal debate about how these lawsuits will be\ndecided and in which jurisdictions. Similarly, there has been signifi-\ncant reporting about new laws drafted around the world to regulate\nlanguage models. We discuss the implications of the results of our\nstudy for the legal debate and policy conversations in Discussion.\nIt is important to note here, however, that courts and governments\nthroughout history have blessed mass property dispossessions that\nare now widely considered moral abominations and have had sig-\nnificant negative long-term implications for both the dispossessors\nand the dispossessed populations. This is true of nearly all the\nevents discussed above. As we are concerned with IP dispossession\nbroadly, we do not focus on any specific legal jurisdiction’s inter-\npretation of what constitutes a violation of IP laws—rather, we are\ninterested in potential harms from content use without consent.\nWe hope to contribute to the evolving conversations around which\nlegal doctrine(s), if any, should be used to prevent such harms.\nThis broad form of dispossession deprives creators of the right\nto choose which systems and organizations their outputs bolster,\nand potentially prevents their ability to receive compensation for\ntheir works. While highly related, this potential harm is distinct\nfrom the possibility that AI systems will lower the demand for\nfuture labor, which we address in the next subsection. In simple\neconomics terms, we can think of the difference as creators losing\nexpected compensation from content already created, or having\ntheir compensation redirected to organizations with which they\nare unaffiliated, versus creators losing new labor opportunities in\nthe future. This new dispossession represents a violation of the\nimplicit social contract that motivated people to invest time and\nmoney in training themselves; those who undertook training in\na pre-LLM era had different expectations about how their work\nmight be utilized.\n2.2.2 Labor Substitution Harms. The use of one’s creations with-\nout consent and without the ability to extract value from this use\nis a major harm in and of itself. However, the way that LLMs use\ncontent makes this harm potentially exponentially larger: a key\nstated reason for developing LLMs is so the models can learn from\nthe content on which they are trained to “do more and more of the\nwork” of content creators and cause “the price of many kinds of\nlabor to fall towards zero” [4]. Put another way, a core value propo-\nsition of large language models is to substitute for the paid labor of\nthe people who create their training content. This means that the\ncreators of content used in the model are not only fighting to get\ntheir share of the value their content is creating—they are fighting\nfor the ability to continue working in their profession of choice\nat all. Many American Jews (and members of other populations\nthat have suffered property dispossession) have grown up with the\nphrase “They can’t take your education away from you. ” In these\nways, LLMs might very much do this, or at least may take away\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Precel et al.\nthe ability to earn a living from an education (and other training\nand experience).\nWhile the degree of labor substitution from LLMs that will oc-\ncur remains unclear, there is a growing line of work attempting to\nforecast the impacts of LLMs on various labor markets. A key con-\nclusion of this work is that professions that have contributed a lot\nof training content to the models may be much more affected than\nthose that have not. For example, Eloundou et al. found that work-\ners with more formal education such as lawyers, graphic designers,\nand database administrators are more exposed to LLMs, which is in\nline with similar work forecasting the impact of ChatGPT on the\nlabor market [132].\n2.3 Dataset Documentation\nOur work aims to contribute to the ongoing discussion about data\ndocumentation, and to specifically highlight how current practices\nwithin the AI research community may be obfuscating potential\nharms. Many of the methodological challenges (though not all of\nthem) presented in this paper could have been avoided with better\ndataset documentation, a practice that scholars have called for many\ntimes over the years [44, 94]. The field of dataset documentation\nprovides important context for our methodological choices. Here,\nwe give some context on this research area.\nBroader interest in dataset documentation was highly influenced\nby work on “Datasheets for Datasets” [ 44], which proposed that\nevery dataset be accompanied by a datasheet. This would facili-\ntate greater transparency and enable practitioners to select more\nappropriate datasets for their tasks. The Datasheets concept has\nled to uptake in documentation practices (shaping, for instance,\nthe Dataset track at NeurIPS1). In practice, the ML community has\ntaken tangible steps to improve data documentation for some kinds\nof contributions.\nHowever, in the space of language modeling, the massive scale\n(“web-scale”) of data—and the choice by the ML community to\ndeprioritze documenting this web-scale data before using it—is a\nmajor barrier to documentation, a concern that was highlighted as\nearly as 2021 [12]. In short, web-scale data is simply expensive and\nchallenging to retroactively document, and typically lacks much in\nthe way of structured data. As a result, much of the LLM industry\nsuffers from “documentation debt”, making it difficult to know\neven basic information about how a model was trained, e.g., what\ndatasets were used, who authored an entry in a given dataset, etc.\nIt is important to note that this debt is intentional and explicitly so;\nmodel builders purposely avoid releasing information about their\ntraining data [89, 120].\nStill, efforts to document data relevant to generative AI have been\nundertaken. One such dataset that is open, documented in some\ndimensions, and widely used for LLM training is the The Pile [43].\nThis is the dataset we focused on studying, in part because the\nprevailing documentation practices left us few other choices. Prior\nwork has attempted to document the BookCorpus dataset, touch-\ning on topics like copyright and acknowledging the authors that\nunderlie the training data [9]. With the purpose of identifying the\n1See, e.g., information about the 2023 track at https://nips.cc/Conferences/2023/\nCallForDatasetsBenchmarks.\nauthors whose work was used in training Meta’s LLaMA, a journal-\nist recently processed and identified over 170,000 books contained\nin the Books3 dataset, finding that the majority were still under\ncopyright and were published in the last two decades [100]. There\nis also ongoing work aimed at urgently remedying the dataset doc-\numentation crisis in the context of LLMs. For instance, “The Data\nProvenance Project” has compiled metadata on popular LLM train-\ning datasets [75], revealing a large amount of prevailing ambiguity\nand the need for policy guidance.\nThe lack of consistent documentation in web-scale training\ndatasets is especially relevant to this research because author attri-\nbution is necessary for estimating group-level dispossession. As a\nresult, we were forced to exclude some datasets (most saliently, all\nweb crawl data) from our analysis.\n2.4 Contributor Attribution\n2.4.1 Challenges of Ethnic Identification. Compounding the data\ndocumentation issue for this research is the fundamental diffi-\nculty of ethnic identification, especially for small ethnic and ethno-\nreligious populations. Ethnic boundaries are often fluid and hard to\ndefine, and are only sometimes captured in demographic surveys.\nAssigning labels to individuals without directly surveying them\nis itself fraught and will inevitably miscategorize some members\nof a population, but a true survey of all group members is prohib-\nitively expensive in both time and resources. Yet, it is critical for\ncommunities to understand the needs and challenges facing their\nmembers, and formal population measurements can be the gateway\nto official recognition and institutional support [ 78]. As a result,\ngroups have developed a variety of alternate strategies for counting\nand surveying their members.\nIn this paper, we focus on the American Jewish community,\na small ethnoreligious group (about 1.8–2.4% of the U.S. popula-\ntion [22, 106, 107]) with a robust literature of community studies at\nlocal and federal levels (e.g., [21, 27, 57, 77, 111, 114]). These studies\nare designed to capture the size, character, demographic profile,\nand needs of U.S. Jews and synthesize findings into actionable in-\nsights for the Jewish community [ 7]. However, the U.S. Census\ndoesn’t collect information on religion and the Jewish community\nis small enough to make identifying a representative sample via\nRandom Digit Dialing (RDD) or other common random surveying\napproaches extremely costly [107]. As a result, Jewish demographic\nstudies have relied on probabilistic methods to identify a represen-\ntative sample and extrapolate findings to communities at large.2\n2.4.2 Distinctive Jewish Names (DJNs). One method developed by\nthe American Jewish community for in-group surveying is the\nDistinctive Jewish Names (DJN) frame. In this method, lists of po-\ntential survey respondents (whether via landline RDD, cellphone\ndialing, or otherwise) are filtered to candidates with a surname\nthat is distinctively Jewish: that is, both common in the Jewish\ncommunity and largely unique to Jews. This increases the chances\nthat a respondent will be Jewish, thus potentially lowering survey\ncosts. The DJN frame has a rich history: it was initially proposed\n2Defining membership in the Jewish community is a complex topic, discussion of\nwhich is beyond the scope of this paper. We rely on definitions from Brandeis’s AJPP\nReport [107], the AJYB population counts [114], and Pew Research Center’s Jewish\nAmerican studies [21, 22]. For an overview of how these definitions were developed,\nsee pages 3-6 of [107].\nAmerican Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nin the 1940s by Kohs [ 57] and has since been used in numerous\ncommunity studies through to the present day [114]. In some cases,\nDJN-based lists were used alongside other sampling methods, pri-\nmarily RDD; in others, they were the primary or only sampling\nframe. Some studies were designed to actually survey community\nmembers while others estimated a Jewish population count in a\nparticular context based on DJNs in membership lists or phone\nbooks. See the “United States Jewish Population” chapters of the\nAmerican Jewish Yearbook series for a detailed account of such\nstudies [114].\nIn this study, we use DJNs as our primary form of analysis. We\nestimate the size of a Jewish population from a total list of names,\nusing content authors identified from model training data as our\npopulation. Here, we provide a brief overview of our rationale\nbefore elaborating on the method itself in Section 3. For a more de-\ntailed overview of the literature on DJNs and the frame’s limitations,\nsee Appendix A.4.\nIn general, a DJN-only approach is recommended for identifying\nthe size of a Jewish population a) only as a rough estimate, and b)\nonly when one already has prior knowledge of Jewish population\nestimates for the area of interest. Both of these are true for our\nstudy: we are reporting order of magnitude estimates at the national\nscale, for which we have a number of high-authority population\nestimates [22, 107].\nAdditionally, the DJN-only approach to population estimation\nassumes that Jews in the sample with DJNs are representative of\nthose without; in this case, that Jews with DJNs are no more likely\nthan Jews without DJNs to produce content that appears in LLM\ndatasets. There is some prior evidence to suggest that DJN sam-\nples do not show significant differences in income [ 26, 112] and\neducation [57]—two potential proxies for IP generation—as com-\npared to the general Jewish population. The greatest disparities are\naround Jewish religious knowledge and engagement with Jewish\nlife. Absent a compelling hypothesis for why DJN samples would\nsignificantly differ from the general population on occupation or\nwriting output, one can have reasonable confidence that our esti-\nmates provide order-of-magnitude bounds.\nUltimately, the research in this paper assumes that characteriz-\ning the nature of IP dispossession by LLMs with respect to potential\nimpact on the Jewish community is important enough to use the\nbest method available, even if it won’t provide a highly-precise\npoint estimate. Our analysis accounts for this by making reason-\nable assumptions that lead to upper and lower bound estimates\nwith robustness checks to ensure order-of-magnitude correctness.\nDetails on these assumptions are in Section 3.2.\n2.5 New LLM Paradigms\nResearchers have begun to explore new LLM paradigms that seek\nto minimize the ethical, legal, and other risks of the current ap-\nproach which depends on uncompensated access to vast amounts\nof content used without the owner’s consent. These explorations\nare a burgeoning area of research.\nOne promising direction emerges out of the retrieval augmen-\ntation and enhancement literature (e.g., [131]). These techniques\nallow a small base model, trained on either public domain or full-\nconsent content [82] and combined with document retrieval tech-\nniques at runtime to dynamically generate output (e.g., [71]). This\napproach allows enough transparency and control for individual\ncontent owners to be able to make decisions about where they want\ntheir data to end up, and bargain for specific contracts that pass\nvalue back to content owners based on usage or related metrics.\nRecent work has begun to explore exactly how markets operated\nwith carefully designed sharing incentives (e.g., “data consortia” in\nwhich multiple organizations pool their content together) might\nwork in practice [19, 129]. Scaling up support for this kind of data\nsharing is another way to shift LLMs towards sharing some portion\nof their economic winnings with content creators. Finding ways\nto make participation appealing to LLM developers could be an\neffective way to work towards an alternative LLM data paradigm.\nAnother idea that has been the subject of early discussions about\nthe economic impacts of AI and automation is to implement some\nkind of broad “data dividend” [123], through which the profits from\nAI technologies are shared with training data creators. A criticism\nagainst this idea is that a very broad remuneration system might\nhurt incentives for the creation of new content (compared to e.g.\nnew content markets). However, this option could be complemen-\ntary to other approaches: because many groups have content and IP\nin the training sets for web-scale models, there is a strong argument\nfor at least some degree of broad remuneration.\nAs noted above, the individual viability of these alternative\nparadigms may change suddenly if certain legal decisions are\nreached (e.g., if “fair use”-based training is broadly supported, or\nbroadly banned) or if new regulations are passed. Thus, navigat-\ning the sea of possible paradigms will require the consideration of\nall possible options, and ideally will include experiments with the\nproposals listed here and more.\n3 METHODS\nThe key methodological challenge of this research is figuring out\nhow a group concerned about disproportionate IP dispossession and\nlabor substitution in the wake of language models might go about\nquantifying the costs it is likely to face. Addressing this question\nas it pertains to Jewish Americans involves dealing with a large\namount of unavoidable noise.\nIn this section, we discuss how we sought to reduce the amount\nof noise in our estimates to a minimum. We describe our methods\nin two parts: first, we describe the LLM training datasets we use in\nour analysis and the process we use to collect metadata; and second,\nwe present our strategy for assessing Jewish authorship.\n3.1 Datasets\nHere, we describe the datasets we used, starting with the rationale\nunderlying dataset selection.\nWe wanted to analyze datasets that would give a reasonable\nestimate of the overall relative magnitude of intellectual property\ndispossession faced by Jewish authors. We assume that an IP dis-\npossession event occurs each time an author’s work is included in\na training set without that author’s explicit consent. For this analy-\nsis, a single work equals a single document (e.g., scientific paper,\nlaw paper, code repository, book) and works with n authors count\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Precel et al.\nfor n IP dispossession events. We note that, as discussed in Sec-\ntion 2.2.1, dispossession is separate from (but related to) copyright,\nand copyright practices vary between the types of documents.\nIf we had metadata describing the author(s) of every document in\na web-scale training dataset and the group identifications of those\nauthors, we could very quickly identify which groups are most\nimpacted. For instance, if a firm were able to scrape the entire web\nand performed no filtering, the degree of disparate impact would\nmap directly to group-level differences in web content creation.\nWhichever group published the most documents on the web would\nsee the most IP dispossession in absolute and relative terms. How-\never, in practice the entire web is not used directly for training (see,\ne.g., [89]). Major firms use various filtering processes to select only\nsome works for use in training data. The processes used by most\nmajor firms are currently proprietary [89], creating a significant\nbarrier to analyses like those in this paper.\nHowever, there exist “open” LLMs that, because they release\ntheir training data, implicitly reveal their filtering methods. One\nvery popular dataset used by these LLMs—and the one we analyze\nin this research—is the Pile [43], curated by EleutherAI. The Pile\nincludes a variety of high-quality data sources that are largely\nassociated with specific professions and institutionalized platforms\n(e.g., ArXiv, GitHub, FreeLaw). Many of these include some level of\nauthor attribution for individual works.\nOpen models, such as those trained using the Pile, are seeing\nsubstantial gains and catching up in performance with private\nofferings (see, e.g., comparison between GPT-NeoX-20B and GPT-3\n[14], as well as the performance of the LLama models [120]). This\nsuggests that the data quality filtering and weighting used in the\nPile is somewhat comparable to private filtering strategies, and that\nby studying this heuristically filtered (i.e., carefully selected) data,\nwe can make claims that generalize reasonably well to LLMs as a\nclass of technology. In other words, we expect the general practice\nof studying and documenting “open” web-scale training data to\nprovide insights that apply to commercial LLM products. While it\nis unknown if Meta’s “open” Llama 2 model directly used the Pile\nas details about pretraining are omitted [120], it seems likely (the\noriginal LLaMA paper did report using The Pile [119]).\nWe focused in our analysis on high quality subsets of the Pile\nthat clearly map to content that is unambiguously composed of\nindividual pieces of literary, scientific, artistic, and/or professional\nworks that are typically subject to intellectual property governance\nand norms. By estimating the number of (document×author) pairs\npresent in the Pile, we get a general assessment of the magnitude of\nIP dispossession and potential labor substitution faced by authors\nwhose works are in the Pile.\n3.1.1 Dataset Selection and Curation. Below, we describe the spe-\ncific datasets from the Pile that we studied in order to work towards\nan estimate of the relative exposure of Jewish Americans to IP dis-\npossession. We also describe some of the dataset-specific processing\nsteps we followed, as well as our data processing pipeline at a high\nlevel.\nThe Pile consists of a set of plaintext documents derived from\na set of datasets with minimal additional processing. At time\nof writing, it is hosted by EleutherAI [ 38], and many of the\nsubsets are also available via original sources. For our analysis,\nwe selected five Pile components ordered by weight, which the\nPile documentation defines as “percentage of bytes in the final\ndataset occupied by each dataset” [43]: PubMed Central, Books3,\nArXiv, GitHub, and FreeLaw. We excluded web scrape components\n(Pile-CC and OpenWebText2) because we were unable to identify\nusable author metadata from web scrape data and re-linking\nthis metadata would be prohibitively difficult. Overall, the five\ncomponents used in our analysis total 49.14% of the final Pile\ndataset by weight [43]. We processed each of the datasets as follows:\n(1) PubMed Central . We downloaded the PubMed Open\nAccess Subset directly from NCBI [ 23]. We used the June\n2023 baseline bulk files for our analysis and PubMed\nParser [2] to parse metadata (including author surname) for\neach article dropping 5 unreadable files (out of 3,529,109\ntotal).\n(2) Books3. We obtained the Books3 metadata from the website\nof the creator of the Books3 dataset [ 99]. The json file\ncontained fields such as title, authors, publication details,\nand description. Any errors in the author name field will\nresult in an undercount of Jewish authors, as mislabeled\ndocuments will still be included in the denominator, regard-\nless of whether the authors have DJNs. It is important to\nnote that Books3 is a particularly controversial component\nof the Pile; large model builders are currently being sued by\nbook authors for their use of Books3 (e.g., [103]).\n(3) ArXiv We downloaded ArXiv metadata from the official\nArXiv Kaggle dataset maintained by Cornell University [24].\nData was downloaded on July 23rd, 2023. We parsed author\nnames using Clement et al. ’s ArXiv data tools [24, 25].\n(4) GitHub We downloaded the list of GitHub repositories\nused in the Pile from EleutherAI’s GitHub Downloader [37]\nand used the GitHub API [ 45] to collect author names for\na uniformly distributed random sample of ∼5% (9,980) of\nrepositories. We used a random sample because limitations\nset by the GitHub API prevented us from downloading data\nfor all repositories. The “Name” field is often unpopulated\non GitHub profiles, and even when populated is not\nstandardized. We parsed this field by selecting the most\ncommonly occurring pattern (<First Name> <Last Name>).\n(5) FreeLaw We downloaded FreeLaw’s CourtListener Opinion\nand People datasets from FreeLaw’s bulk data files [28] on\nMay 31, 2023. We joined these datasets based on the Author\nID column and filtered out opinions with no authors.\n3.2 Assessing Jewish Authorship\nHere, we describe the process we used to identify Jewish authors in\nthe datasets described above and produce an estimate of the relative\nmagnitude of IP dispossession experienced by Jewish Americans.\n3.2.1 Name Classification. The DJN list we used for this analysis is\na 35-name list that has been used in studies from the 1940s until the\npresent day with very few changes, and has consistently maintained\nAmerican Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training CHI ’24, May 11–16, 2024, Honolulu, HI, USA\na relative proportion of roughly∼10–12% of Jews with DJNs to Jews\nas a whole in large American Jewish communities [51, 56, 111]. It\nwas also designed to have very high precision: i.e., people with\nthese surnames are highly likely to be Jewish. See Section 2.4.2 for\nadditional details on these numbers.\n3.2.2 Data Processing Procedure. We conducted several levels of\nadditional processing that culminated in calculating an estimated\npercent Jewish authorship via a DJN-based frame. At a high level,\nour data processing consisted of five steps. We follow these steps\nfor each of our five Pile subsets.\n(1) Extract author names\n(2) Match DJNs to (document ×name) pairs and calculate per-\ncentage of pairs with DJNs\n(3) Adjust estimated percentage to account for non-Jews with\nDJNs\n(4) Adjust estimated percentage to account for Jews without\nDJNs\n(5) Estimate an expected percentage of U.S. Jewish authors per\ndataset (i.e., the number we would expect to see if U.S. Jews\nwere proportionally represented) and compare it to the ob-\nserved percentage\nBelow, we explain each of these steps in more detail.\nExtract author names. First, we attempted to extract the au-\nthors of each document in each dataset to create a list of (document\n×name) pairs. Using (document ×name) as our unit of measure-\nment allows us to center authors as those experiencing harm: each\npair represents one instance of a given author’s work being used\nin LLM training data.\nMatch DJNs to (document ×name) pairs and calculate\npercentage of pairs with DJNs. We use our DJN list as a filter on\nauthor names to produce a subset of DJN-matched documents. We\ncalculate the percentage of resulting (document ×name) pairs for\nwhich the name is a DJN.\nAdjust estimated percentage to account for non-Jews with\nDJNs. To account for the potential that DJN matching may iden-\ntify some people who are not Jewish, we rely on false positive\nestimates from Himmelfarb and colleagues [57] (“about 90–92% of\nthese names are Jewish”), Rosenwaike’s analysis of leading Jewish\nsurnames, which includes estimates of 76.6–95.1% (m=88.7%) preci-\nsion for 15 of the 35 names [102], and Phillips’ 91.8% Boston-area\nestimate [97].\nBecause these are all approximations, we use a range of 80–\n90% for our analysis. In other words: if our method found 1000\n(document ×name) pairs that matched DJNs, we assumed this\nrepresented between 800-900 Jewish authors.\nAdjust estimated percentage to account for Jews without\nDJNs. For this step, we again refer to [57, 67, 97], which estimate\nthat Jews with names from the DJN list we used comprise ∼10%–\n12% of the U.S. Jewish population. We validated this number by\ncalculating the percentage of people included in the 2010 U.S. census\n[20] with one of the DJNs, accounting for false positives as above,\nand comparing it to estimates of the Jewish U.S. population in\n2010 [21, 106].3\nWe used the resulting figure of 9.15–11.18% for our analysis.\nFollowing from the above example, if our method finds 1000 DJN\npairs (adjusted in step four to a range of 800-900 Jewish authors),\nthis means we’d extrapolate in step four to a lower bound of 800 ∗\n1\n0.1118 = 7156 Jewish authors total.\nWe note that both of these adjustment steps simply involve mul-\ntiplying DJN percentage values, so their order does not matter. The\nability to perform these adjustment steps with only multiplication\nrests on two distributional assumptions:\n•That the distribution of names among Jewish Americans\nwho contributed to LLM training data is roughly the same as\nthe population of Himmelfarb et al. [57] and Rosenwaike’s\n[102] respective studies.\n•That there is no difference in job category representation and\npropensity to contribute IP to LLM training data between\nJewish Americans with DJNs and those without (i.e., for each\nDJN document-name pair, there is a proportionate number\nof document-name pairs that would be attributable to Jewish\nAmericans without DJNs upon deep investigation).\nEstimate an expected percentage of U.S. Jewish authors per\ndataset. Finally, the fifth step is to contextualize these estimated\npercentages in terms of relative dispossession magnitude. We want\nto know how the observed amount of IP from Jewish American au-\nthors compares to the expected amount of IP from Jewish American\nauthors if LLM operators were to representatively sample works\nfrom the whole population. Because we focus here on the relative\nrepresentation of Jewish American authors, we introduce two new\nfactors we must account for: changing demographics over time and\nhow much data in each dataset comes from American authors.\nSome of the datasets we investigate represent content that\nwas produced over many decades. In Appendix A.3, we explored\nwhether our estimates would change if we accounted for changes in\nthe Jewish population over time. We tested this using FreeLaw—the\ndataset with by far the largest time window—and found minimal\nimpact on our results.\nTo account for country-level distribution of training data, we\nestimated the percentage of documents from each dataset published\nin the U.S. and account for this in calculating our “expected per-\ncentage” of U.S. Jewish content. One can think of these numbers as\nchecking how overrepresented U.S. authors might be in general in\norder to correctly calculate the percentage of American Jews.\n•Free Law: 100% published in the U.S., as CourtListener only\nindexes U.S. opinions.\n•Books3: We do not have data on what percent of Books3\nis international. As a result, we act as if 100% of the dataset\nwere published in the U.S. to intentionally use a conservative\nlower-bound estimate of the actual figure, even at the top\nend of our ranges.\n•GitHub: 24.6% published in the U.S., based on data from\n[127].\n3We use the following equation, with a precision of 85% and a Jewish population of\n1.8-2.2%: # of DJNs in the population ∗precision\nU.S. population ∗% of Jews in the U.S.\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Precel et al.\n•PubMed Central: 28.6% published in the U.S., based on our\nown estimation (details in Appendix A.2.1).\n•ArXiv: 27.3% published in the U.S., based on our own esti-\nmation (details in Appendix A.2.2).\nWe note that there is another source of uncertainty relating to\ngeography: some of the authors with DJNs could be non-American\nJewish authors. We don’t expect this to change our results substan-\ntially because we expect surname distributions and spellings to be\ndifferent in other countries, and around 40% of the world Jewish\npopulation lives in the U.S.\n3.3 Ethical Considerations and Broader Impacts\nThe approaches we use in this work—and indeed the choice to\nconduct the research at all—were carefully weighed against the\nmethodological challenges and ethical questions of doing so. Specif-\nically, many of the methodological choices above were designed\nin part to minimize potential harm to the Jewish community and\nother stakeholders.\nFirst, we considered the risks and challenges of labeling authors\nas Jewish. Identity inference from attributes like names is not new,\nbut it is controversial [35]. One area with significant prior work\nand critique is the realm of gender: multiple studies in HCI have\ndocumented the harms of gender detection and recognition sys-\ntems, from individual harms of misgendering [49] to societal harms\nfrom operationalizing reductive and exclusionary definitions of\ngender [61]. As in our study, it is not always possible to get affirma-\ntive self-identification for attributes; yet knowing the demographic\ndistribution in a dataset can be a critical aspect of evaluating the\nimpact of systems.\nWe took a number of actions to mitigate common risks in identity\ninference. The list of DJNs and other methodological approaches we\nused were selected in consultation with Jewish demographers, who\nare experts at navigating the fraught ethical choices surrounding\nthe inference of Jewish identity. One alternative approach we con-\nsidered but ultimately rejected was deploying a large-scale survey\nto all of the authors of works in the Pile we could identify to in-\nquire as to their claimed ethno-religious identity. However, surveys\nlike these targeting the Jewish population are known to be very\ndifficult to execute due to the small size of the Jewish population.\nAlso, deploying a survey asking about Jewish identity of course has\nits own ethical considerations, and even if successful, would not\nremove the significant noise present due to the LLM community’s\npoor documentation practices.\nAdditionally, as we are attempting to calculate the proportion\nof Jewish authors in these datasets, our method does not require\nthat any specific author be labeled Jewish with 100% accuracy, thus\nminimizing the harm of misclassifying specific individuals (either\nas Jewish or as non-Jewish).\nThe second major ethical consideration for our study was the\nrisk that our findings would be used to harm the Jewish commu-\nnity should they be leveraged by anti-Semitic actors, which we\nfurther discuss in Appendix A.4. Ultimately, we decided that the\nconsequences of not doing the work were greater than the poten-\ntial harms, a decision-making process that was led by the Jewish\nmembers of the authorship group. We stress that IP dispossession\nis happening regardless of how well it is documented, and that it\nwill continue to happen until the broader LLM community takes\nmeasures to change the approaches in their work.\n3.4 Methodological Limitations\nAs described above, our method required us to make several careful\nassumptions in order to obtain reasonable, bounded estimations for\nthe proportion of Jewish Americans in each of these datasets. We\nnote that these estimation approaches are somewhat atypical in the\nHCI community, which often operates with better documented data.\nWhen deciding to do this work, we took inspiration from the carbon\nimpact estimation research literature, which also operates in a very\nhigh-stakes domain and has to use a wide variety of estimation\ntechniques with large informal error bounds [117]. That literature\nhas shown that if the research question is important enough, es-\ntimates with somewhat wide ranges and important qualifiers can\ngreatly assist with decision-making towards critical goals [117].\n4 RESULTS\nUsing the method described above, we calculated relative dispos-\nsession magnitude —a ratio of observed to expected numbers of\ndocuments written by U.S. Jews in the dataset.\nRelative Dispossession Magnitude =\n% U.S.-Jewish authored documents in dataset\nExpected % U.S.-Jewish authored documents in dataset\nWe first calculated lower and upper bounds for the relative dis-\npossession magnitude of each individual dataset based on the lower-\nbound and upper-bound estimation techniques discussed above.\nThen, we calculated two averages: a total relative dispossession\nmagnitude (the mean across datasets with each dataset weighted\nequally) and a weighted total relative dispossession magnitude (the\nmean across datasets with each dataset weighted by number of\ndocuments ×document size). In other words, weighted total rela-\ntive dispossession magnitude reflects the total overrepresentation\nof American Jewish authorship accounting for the size of each\nindividual dataset and the length of its average documents.\nLooking at the first column in Table 1 (which is not limited to\nU.S. documents), we see that the percent of (document ×name)\npairs whose authors have DJNs—who represent a small fraction of\nJewish authors—is already greater in almost every case than the\npercent of Jews in the world (0.19–0.28%) [33, 86]. Although we have\nless certainty about world statistics as many of our variables are\ndesigned to focus on the U.S. Jewish population, this is strong early\nsupport for the hypothesis that American Jews are over-represented\nin these datasets and suggests that this is true of Jewish authorship\nglobally.\nIn the second column, we have lower and upper bounds for\nthe percent of IP dispossession events—(document ×name) pairs—\nfrom U.S. Jewish authorship. As noted above, we consider a range of\nparameter values to account for some of the uncertainty introduced\nby our methods. Our estimate here is parameterized by precision\n(how unique are the DJNs to the Jewish population) and coverage\n(how much of the Jewish population do the DJNs represent). Our\nlower bound uses lowest precision / highest coverage estimates;\nour upper bound uses highest precision / lowest coverage.\nAmerican Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nDataset % IP with\nDJN author\n% IP with\nU.S. Jewish\nauthor\n% Expected\nIP with U.S.\nJewish author\nRelative Dispossession\nMagnitude\nPubMed Central 0.19 1.39-1.91 0.5-0.7 2.02-3.71 X\nBooks3 0.98 7.01-9.64 1.8-2.4 2.92-5.36 X\nArXiv 0.28 2.01-2.77 0.5-0.7 3.07-5.63 X\nGitHub 0.29 2.08-2.86 0.4-0.6 3.53-6.46 X\nFreeLaw 0.93 6.65-9.14 1.8-2.4 2.77-5.08 X\nTotal 0.54 3.83-5.26 1.0-1.3 2.86-5.25 X\nWeighted Total 0.37 2.63-3.61 0.8-1.0 2.46-4.51 X\nTable 1: From left to right: (1) percentage of dataset (documents ×authors) with DJNs; (2) estimated percentage of dataset\n(documents ×authors) with U.S. Jewish authors; (3) expected percentage of dataset (documents ×authors) with U.S. Jewish\nauthors; (4) relative dispossession magnitude.\nTable 2 shows the parameters we use for each estimate: precision\nranges from 80% to 90%, coverage from 9.15–11.18%, and percent\nof the U.S. that is Jewish from 1.8%–2.4% (see Section 3.2.2 for\ndetails). Critically, these ranges are not uncertainty ranges: they are\nassumption-based and indicate a range of reasonable possibilities\nfor a parameter in our equation. Our lower bound is the lowest\npossible estimate given our assumptions; our upper bound is the\nhighest. In Appendix A.1, we include a robustness check in which\nwe consider the most extreme possibilities for each parameter that\nwould still demonstrate a relative dispossession magnitude > 1.\nWe found that U.S. Jewish authorship ranged, per dataset, be-\ntween 1.39–9.64%. As expected, the more U.S.-centric sources see\nhigher percentages of U.S. Jewish-authored documents (6.7–9.1%\nfor FreeLaw; 7.0–9.64% for Books3) while less U.S.-centric sources\nsee lower percentages (PubMed Central: 1.4–1.9%, ArXiv: 2.0–2.8%,\nGitHub: 2.1–2.9%).\nParameter Lower bound Upper bound\n% precision of DJNs 80% 90%\n% coverage of DJNs 11.18% 9.15%\n% of US population 2.4% 1.8%\nthat is Jewish\nTable 2: Parameters used in estimation calculation. From\nleft to right: (1) name of parameter; (2) value used for lower\nbound estimations in Table 1; (3) value used for upper bound\nestimations in Table 1.\nThe final column in Table 1 shows the amount of dispossession\nexperienced by U.S. Jewish authors relative to U.S. content pro-\nducers more generally, i.e., the numbers we are most interested in\nfor the purposes of this paper. The results in this column clearly\nshow a structural bias against U.S. Jews across all datasets: the low-\nest lower-bound dispossession magnitude we observed was 2.02,\nmeaning that U.S. Jewish suffer double the dispossession of the U.S.\npopulation as a whole at the very minimum (across the datasets we\nconsidered). The highest upper-bound magnitude was 6.46, which\ncorresponds to over six times more dispossession than the general\npopulation. This table presents a strong argument that, at least\nwith respect to U.S. content, LLMs rely disproportionately on Jew-\nish American intellectual property obtained without the creator’s\nconsent, and do so extensively.\n5 DISCUSSION\nOur results indicate there is very real risk that Jewish Americans\nmay see substantial and disproportionate economic harms as LLM-\nbased technologies are deployed more widely. Below, we discuss the\nimplications of these results for a number of key discussions hap-\npening around AI law, regulation, and practice. We also highlight\nkey areas of future work.\n5.1 Implications for Legal and Policy\nDiscussions\nThe findings above have important implications for the rapidly-\ndeveloping legal and policy debates surrounding language models.\nThe introduction of new structural material biases against minori-\nties has not been broadly considered in these debates, and our\nresults suggest they very much should be.\nWith regards to developments in the legal sphere, many dimen-\nsions of LLM training practices are being examined by courts in\ndifferent jurisdictions, e.g., “fair use, ” privacy rights, publicity rights,\nlabor law, contract law and many others [69, 104, 110]. However,\nour results suggest that disparate material impact suffered by pro-\ntected groups is another dimension that needs to be explored. It is\nclear, for instance, that any U.S. decision that current LLM training\npractices constitute “fair use” could introduce a significant new\nstructural bias that disproportionately harms American Jews—and\nlikely other minority groups—in the short term. As discussed in the\nIntroduction, should these disparate harms play out, they are likely\nto represent a “first wave” of harms, with almost all participants in\nthe economy eventually being affected.\nThe evidence above also suggests that policymakers should more\ndeeply consider structural bias against minority groups in the dis-\ncussions about language models. Lawmakers in the United States\nconsidering encoding current LLM content usage into law must\nwrestle with the new systemic biases against American Jews they\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Precel et al.\nwould be creating. Similarly, those working on efforts to strengthen\nthe rights of content owners and producers (e.g., [ 63]) may con-\nsider our findings to be an additional reason to push forward in\nthat direction. Our results also highlight the importance of agency\ndecision-makers, regulators, and policymakers shifting the grow-\ning amounts of public funding for LLM research towards the many\npromising approaches that do not create the structural biases of\ncurrent approaches. There are a number of other reasons that have\nbeen identified in the literature to do so, e.g., the potential of the\ncurrent paradigm to substantially decrease the material welfare of\nthe general public [4, 16], which likely contradicts the mandates of\nmany national research funding agencies.\nThis study represents a first step towards understanding group-\nlevel contributions to training data. Policymakers will likely want to\nconsider additional analyses that extend this line of thinking. This\ncould involve adapting similar methods to those we employed here,\nperhaps augmented with recent work on detecting pre-training data\n[115] or incorporating new methods for estimating racial disparities\nusing surnames [79], though this may require support from dataset\ncreators and curators.\n5.2 Implications for Developers and\nResearchers of AI Systems\nAt the highest level, the results above add to the growing list of\nreasons AI developers and researchers should consider shifting\nresources and attention away from the current LLM paradigm and\ntowards both 1) LLM systems and techniques that only train on\ncontent with consent, and 2) mitigating the negative impacts that\nprevious decision-making in the LLM industry may have caused.\nThis paper highlights that those simply seeking to advance the cur-\nrent paradigm must reckon with the new structural biases against\nminority groups to which they may be contributing. They must also\nknow that they are asking the Jewish community (and likely other\nminority communities as well) to make disproportionate sacrifices\nfor the benefit of their mission. This is especially true for organi-\nzations like OpenAI, whose non-profit mandate requires that they\nbuild AI for the public good. More generally, like a number of other\nrecent papers and opinion pieces [8, 91], this paper highlights that\nif we—as design researchers and practitioners—do not make signifi-\ncant changes to our approach, even if the community is successful\nat building something like an artificial general intelligence, such\nan accomplishment risks being forever tarnished with legitimacy\nissues originating from how it learned what it knows.\n5.3 Implications for Jewish Americans Who\nAuthor Digital Content\nOur results suggest a few strategies for action that can help Jewish\nAmericans. It seems that if the groundwork were laid for easily\naccessible “data opt out actions”—via national laws (e.g., [63]) or\nnormatively adopted data use policies (e.g., [81])—Jewish Ameri-\ncans may be able to organize a very impactful opt out campaign,\ni.e., a “data strike” [124]. Additionally, this would suggest a natural\nincentive for AI firms to tackle the concerns laid out in this paper\nhead on: if legal or technical tools for exerting data agency at the\ngroup level proliferate, any groups that currently see high levels of\nexposure to property dispossession could create significant leverage\nif there is sufficient buy-in (which may require pressuring or con-\nvincing institutions that own the rights to some members’ content).\nThese dynamics also mean that affinity and interest groups seeking\nto protect the welfare of minority groups—e.g., organizations that\nsupport the Jewish community—may have a natural alliance with\nefforts to promote a content generator-friendly AI paradigm.\n5.4 Implications for Other Minority Groups\nOur results suggest that Jewish Americans are not the only minority\ngroup that will likely serve as a “canary in the coal mine” and expe-\nrience negative effects from the current LLM paradigm sooner than\nthe general population. Much of the highest-value content used\nby language models requires significant education to create (e.g.,\nconsider the typical author of a paper in PubMed). Jewish Ameri-\ncans have a high relative average educational attainment [83] and,\nalthough other factors may be involved (e.g., a tendency to choose\ncareers involving more public knowledge sharing), that is likely one\nreason we saw the effects that we did. Jewish Americans, however,\nare of course far from the only minority group in the United States\n(let alone outside of it) with high average educational attainment:\nthis is true of Asian Americans [ 41] and Hindu Americans [ 83]\nas well, for instance. Assuming the link between education and\nvaluable technical content for LLM training datasets is quite strong,\nAsian Americans and Hindu Americans are likely to be similarly\naffected by language models. Replicating and extending this work\nto examine the effect of non-consensual content training on these\ngroups is a critical area of future work.\nOur hypotheses are less clear regarding the potential effects of\nLLM-caused intellectual property dispossession for minority groups\noutside the United States. Countries around the world are rapidly\nengaging in policy discussions around language models, and doing\nwork to understand if trends similar to those observed here affect\nminority groups in places like the EU, the UK, and elsewhere is also\ncritical research that should happen quickly.\n5.5 Balancing Dispossession Concerns and\nGroup-level Performance Gap Concerns\nThere is a large body of early and field-defining work in algorithmic\nfairness that has highlighted issues with performance gaps that\narise when minoritized groups are underrepresented in training\ndata (see, e.g., Mehrabi et al. for a survey [80]). Though concerns\nwith under-representation could be seen as in tension with the\nargument we’ve put forth here, these two ideas are not mutually\nincompatible.\nGenerally, under-representation is most concerning when tech-\nnologies downstream from the dataset impact people subject to the\ntechnologies. For instance, facial recognition systems—which can\nbe used to unlock a phone screen or for policing—have been shown\nto have serious issues with respect to skin tone and gender [17].\nIn the context of LLM-based technologies enabling labor substi-\ntution, however, data contributors do not necessarily derive utility\nfrom the technology nor are they subject to model outputs; rather,\nthey are primarily subject to labor market dynamics.\nThis suggests that while there may be cases in which under-\nrepresentation concerns dominate (and vice versa), in general the\nML community will need to adopt a balanced approach to data\nAmerican Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nrepresentation. It can be simultaneously the case that some groups\nsee harms related to lack of representation and others see harms be-\ncause they have provided a large share of creative works. Ultimately,\ndataset curators are charged with the challenging but necessary\ntask of gathering data that both accurately represents the world\nitself and accounts for the preferences of data subjects (and in some\ncases, includes some element of fair remuneration).\n5.6 Implications for HCI\nHCI has long been a leader in identifying and working to address\nchallenges in the power dynamics between content producers and\nAI systems that consume their content. While HCI’s contributions\nhave primarily focused on crowd markets and their use in older\ngenerations of AI (e.g., [3, 50, 62]), the research in this paper high-\nlights the importance of continuing and strengthening this line of\nwork in the LLM era. What kind of platforms could be created to\nhelp content producers receive a fair share of the value they are\ncreating? (e.g., [65, 66]) How can we empower collective action\namong content producers to maximize their ability to do so? (e.g.,\n[72]) These are all questions that can help to create new and more\nequitable LLM paradigms, and they require significant leadership\nfrom HCI researchers.\nThe large area of research addressing “algorithmic bias” can trace\nsome of its origins to work at HCI venues like CHI (e.g., [55]). This\npaper highlights the need for the HCI community to continue to\npush this line of work forward. While algorithmic bias research\nhas mostly focused on “representation harms” [31] and gender and\nracial dimensions, this work highlights the urgent need to expand\n(but certainly not shift) our lens to consider direct material harms\n[53, 54] and additional dimensions.\nHCI researchers have also looked at how we can best communi-\ncate AI design and fairness concepts to end users, including how\nthe presentation of information about transparency can impact\nperceptions [121]. For instance, Anik et al. [ 6] investigate how\nexplanations of training data in ML systems can increase trans-\nparency and influence trustworthiness of systems. Further work\nin HCI could explore how disparate impacts and IP dispossession\ninteract with perceptions of accuracy, trustworthiness, and fairness\nof LLMs. Similarly, HCI has also contributed to toolkits for ML\npractitioners to support better decision-making around fairness,\nbias, and transparency [70]. As one example, Madaio et al. [76] co-\ndesigned AI fairness checklists with practitioners to ground them\nin the realities of the day-to-day work, paying particular attention\nto the organizational and sociotechnical factors that inhibit fairness\nwork. Future work from HCI researchers might consider how dis-\npossession and disproportionate impact on minority communities\ncan be reckoned with within organizations.\nFinally, as noted above, many of the harms from property dispos-\nsession identified in this paper can be mitigated if the net outcome\nof LLM-based technologies is to augment human labor rather than\nto substitute for it, a net outcome that the HCI community can help\nmanifest. Indeed, if we were to be successful in doing so, groups\nmay want their data to appear in training sets as it could an enhance\nLLM’s ability to augment their labor. While substituting for human\nlabor is a core tenant of important LLM actors like OpenAI [ 90],\nas discussed above, economists have suggested that large-scale net\nsubstitution is not a certainty (e.g., [16]). These economists argue\nthat researchers and practitioners in the computing community\nshould focus on building LLM-based applications that keep human\nlabor as input and create entirely new capabilities or augment exist-\ning ones, rather than building tools that implicitly seek to remove\nhuman inputs. In broad productivity terms, this means focusing on\ncreating novel outputs rather than trying to drive human inputs to\nzero. As key problem-definers and developers of novel applications,\nas well as being a research community responsible for understand-\ning users and their needs, the HCI community is well-positioned\nto lead the push for more augmentative LLM-based applications.\n6 CONCLUSION\nWe find evidence that Jewish Americans likely have experienced a\ndisproportionate share of the potential IP dispossession stemming\nfrom longstanding model training practices of AI companies and\nthe broader AI research community. These practices may lead to\nserious economic harms with disturbing historical parallels, and\ncall for urgent reflection about the future of the AI ecosystem. We\ndiscuss implications for a range of impacted groups and society as\na whole.\nACKNOWLEDGMENTS\nThe authors would like to thank the many other scholars—in the\nJewish community and beyond—who took the time to give us feed-\nback on this work. We in particular thank Glen Weyl, Moshe Vardi,\nand Gideon Taylor for their essential advice & feedback on earlier\ndrafts. We also thank Pearl Beck, Elizabeth Tighe, and Ira Sheskin\nfor their invaluable inputs on demographic methods and DJN use.\nThis research was not supported by institutions other than our\nhome universities. Other lines of research by the authors have\nbeen supported by companies that produce LLMs and LLM-based\napplications including Microsoft, OpenAI, and Google.\nREFERENCES\n[1] Daron Acemoglu, David Autor, Jonathon Hazell, and Pascual Restrepo. 2022.\nArtificial intelligence and jobs: evidence from online vacancies. Journal of Labor\nEconomics 40, S1 (2022), S293–S340.\n[2] Titipat Achakulvisut, Daniel E. Acuna, and Konrad Kording. 2020. Pubmed\nParser: A Python Parser for PubMed Open-Access XML Subset and MEDLINE\nXML Dataset XML Dataset. https://doi.org/10.21105/joss.01979\n[3] Ali Alkhatib, Michael S. Bernstein, and Margaret Levi. 2017. Examining Crowd\nWork and Gig Work Through The Historical Lens of Piecework. InProceedings\nof the 2017 CHI Conference on Human Factors in Computing Systems (Denver,\nColorado, USA) (CHI ’17) . Association for Computing Machinery, New York,\nNY, USA, 4599–4616. https://doi.org/10.1145/3025453.3025974\n[4] Sam Altman. 2021. Moore’s Law for Everything. https://moores.samaltman.\ncom/\n[5] Steve Andreadis. 2022. The Seizure of Jewish Intellectual Property Ahead of\nWorld War II. https://blogs.loc.gov/copyright/2022/04/the-seizure-of-jewish-\nintellectual-property-ahead-of-world-war-ii\n[6] Ariful Islam Anik and Andrea Bunt. 2021. Data-Centric Explanations: Explain-\ning Training Data of Machine Learning Systems to Promote Transparency. In\nProceedings of the 2021 CHI Conference on Human Factors in Computing Systems\n(CHI ’21) . Association for Computing Machinery, New York, NY, USA, Article\n75, 13 pages. https://doi.org/10.1145/3411764.3445736\n[7] Janet Krasner Aronson, Matthew Boxer, and Leonard Saxe. 2016. ‘All Politics is\nLocal’: Challenges in the Study of Local Jewish Communities. Contemporary\nJewry 36, 3 (Oct 2016), 361–380. https://doi.org/10.1007/s12397-016-9200-7\n[8] Margaret Atwood. 2023. Murdered by My Replica? https:\n//www.theatlantic.com/books/archive/2023/08/ai-chatbot-training-books-\nmargaret-atwood/675151/\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Precel et al.\n[9] John Bandy and Nicholas Vincent. 2021. Addressing \"Documentation Debt\"\nin Machine Learning: A Retrospective Datasheet for BookCorpus. In Pro-\nceedings of the Neural Information Processing Systems Track on Datasets and\nBenchmarks, J. Vanschoren and S. Yeung (Eds.), Vol. 1. Curran, San Diego, CA,\nUSA. https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/\n2021/file/54229abfcfa5649e7003b83dd4755294-Paper-round1.pdf\n[10] Miranda Bannister. 2022. A 1492 Letter Regarding Jewish Property in Spain.\nhttps://mjhnyc.org/blog/1492-letter-regarding-jewish-property-in-spain/\n[11] Lida Barner. 2017. “Aryanization” Expanded?: Patent Rights of Jews under the\nNazi Regime . Central European University Press, Budapest, Hungary, 127–144.\nhttps://www.jstor.org/stable/10.7829/j.ctt1t6p66t.10\n[12] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models\nBe Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Account-\nability, and Transparency (Virtual Event, Canada) (FAccT ’21). Association for\nComputing Machinery, New York, NY, USA, 610–623. https://doi.org/10.1145/\n3442188.3445922\n[13] Edwin Black. 2012. IBM and the Holocaust: The Strategic Alliance Between Nazi\nGermany and America’s Most Powerful Corporation-Expanded Edition (2nd edition\ned.). Dialog Press, USA.\n[14] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence\nGolding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow,\nBen Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source\nAutoregressive Language Model. https://doi.org/10.48550/arXiv.2204.06745\narXiv:2204.06745 [cs].\n[15] Ariel Bogle. 2023. New York Times, CNN and Australia’s ABC\nblock OpenAI’s GPTBot web crawler from accessing content.\nhttps://www.theguardian.com/technology/2023/aug/25/new-york-times-\ncnn-and-abc-block-openais-gptbot-web-crawler-from-scraping-content\n[16] Erik Brynjolfsson. 2022. The Turing Trap: The Promise and Peril of Human-Like\nArtificial Intelligence. https://digitaleconomy.stanford.edu/news/the-turing-\ntrap-the-promise-peril-of-human-like-artificial-intelligence/\n[17] Joy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accu-\nracy Disparities in Commercial Gender Classification. In Proceedings of the 1st\nConference on Fairness, Accountability and Transparency (Proceedings of Machine\nLearning Research, Vol. 81) , Sorelle A. Friedler and Christo Wilson (Eds.). PMLR,\nNew York, NY, USA, 77–91. https://proceedings.mlr.press/v81/buolamwini18a.\nhtml\n[18] Ann M. Carlos, Donna L. Feir, and Angela Redish. 2022. Indigenous Nations\nand the Development of the U.S. Economy: Land, Resources, and Dispossession.\nThe Journal of Economic History 82, 2 (June 2022), 516–555. https://doi.org/10.\n1017/S0022050722000080 Publisher: Cambridge University Press.\n[19] Raul Castro Fernandez. 2023. Data-Sharing Markets: Model, Protocol, and\nAlgorithms to Incentivize the Formation of Data-Sharing Consortia. Proceedings\nof the ACM on Management of Data 1, 2 (June 2023), 172:1–172:25. https:\n//doi.org/10.1145/3589317\n[20] Census.gov. 2021. Frequently Occurring Surnames from the 2010 Census. https:\n//www.census.gov/topics/population/genealogy/data/2010_surnames.html\n[21] Pew Research Center. 2013. A portrait of Jewish Americans: Findings from a\nPew research center survey of US Jews.\n[22] Pew Research Center. 2021. Jewish Americans in 2020. https://www.\npewresearch.org/religion/2021/05/11/jewish-americans-in-2020/\n[23] PubMed Central. 2023. PMC Open Access Subset. https://www.ncbi.nlm.nih.\ngov/pmc/tools/openftlist/\n[24] Colin B. Clement, Matthew Bierbaum, Kevin P. O’Keeffe, and Alexander A.\nAlemi. 2019. arxiv-public-datasets. https://github.com/mattbierbaum/arxiv-\npublic-datasets/blob/master/README.md\n[25] Colin B. Clement, Matthew Bierbaum, Kevin P. O’Keeffe, and Alexander A.\nAlemi. 2019. On the Use of ArXiv as a Dataset. ArXiv abs/1905.00075 (2019),\n1–7. https://api.semanticscholar.org/CorpusID:141496572\n[26] Steven M. Cohen. 2016. Deficient, If Not Distorted: Jewish Community Studies\nThat Totally Rely upon Known Jewish Households. Contemporary Jewry 36, 3\n(Oct 2016), 343–360. https://doi.org/10.1007/s12397-016-9187-0\n[27] Steven M. Cohen, Frank Mott, Lorraine Blass, Jim Schwartz, Jonathon Ament,\nVivian Klaff, and Laurence Kotler-Berkowitz. 2001. 2000-01 National Jewish\nPopulation Survey. https://www.jewishdatabank.org/databank/search-results/\nstudy/307\n[28] CourtListener. 2010. CourtListener. https://www.courtlistener.com/help/api/\nbulk-data/\n[29] Thomas Craemer, Trevor Smith, Brianna Harrison, Trevon Logan, Wesley Bel-\nlamy, and William Darity. 2020. Wealth Implications of Slavery and Racial\nDiscrimination for African American Descendants of the Enslaved. The Review\nof Black Political Economy 47, 3 (Sept. 2020), 218–254. https://doi.org/10.1177/\n0034644620926516 Publisher: SAGE Publications Inc.\n[30] Maria Cramer. 2021. Henrietta Lacks, Whose Cells Were Taken Without Her\nConsent, Is Honored by W.H.O. https://www.nytimes.com/2021/10/13/science/\nhenrietta-lacks-cells-who.html\n[31] Kate Crawford. 2017. The Trouble With Bias . The Annual Conference on Neural\nInformation Processing Systems (NeruIPS), San Diego, CA, USA.\n[32] Wes Davis. 2023. Sarah Silverman is suing OpenAI and Meta for copyright\ninfringement. https://www.theverge.com/2023/7/9/23788741/sarah-silverman-\nopenai-meta-chatgpt-llama-copyright-infringement-chatbots-artificial-\nintelligence-ai\n[33] Sergio DellaPergola. 2022. World Jewish Population, 2021 . Springer International\nPublishing, Cham, 313–412. https://doi.org/10.1007/978-3-030-99750-2_8\n[34] Nick Diakopoulos. 2023. Finding Evidence of Memorized News Content in GPT\nModels. https://generative-ai-newsroom.com/finding-evidence-of-memorized-\nnews-content-in-gpt-models-d11a73576d2\n[35] Catherine D’Ignazio and Lauren Klein. 2020. 4. “What Gets Counted Counts” .\nMIT Press, Cambridge, MA, USA, Chapter 4, 1–27. https://data-feminism.\nmitpress.mit.edu/pub/h1w0nbqp\n[36] David Dutwin. 2016. Everything You Need to Consider When Deciding to Field a\nSurvey of Jews: Choices in Survey Methods and Their Consequences on Quality.\nContemporary Jewry 36, 3 (Oct 2016), 297–318. https://doi.org/10.1007/s12397-\n016-9189-y\n[37] EleutherAI. 2020. github-downloader. https://github.com/EleutherAI/github-\ndownloader\n[38] EleutherAI. 2023. EleutherAI. https://www.eleuther.ai\n[39] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs\nare GPTs: An Early Look at the Labor Market Impact Potential of Large Language\nModels. https://doi.org/10.48550/arXiv.2303.10130 arXiv:2303.10130 [cs, econ,\nq-fin]\n[40] Simon Fairlie. 2009. A Short History of Enclosure in Britain. https://www.\nthelandmagazine.org.uk/articles/short-history-enclosure-britain\n[41] National Center for Education Statistics. 2023. Educational Attainment of\nYoung Adults. https://nces.ed.gov/programs/coe/indicator/caa/young-adult-\nattainment\n[42] Jonathan Freedland. 2018. Antisemitism matters: Jews are the canary in\nthe coalmine. https://www.theguardian.com/commentisfree/2018/mar/30/\nantisemitism-jews-canary-coalmine-fake-news\n[43] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles\nFoster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,\nand Connor Leahy. 2021. The Pile: An 800GB Dataset of Diverse Text for\nLanguage Modeling. CoRR abs/2101.00027 (2021), 1–39. arXiv:2101.00027 https:\n//arxiv.org/abs/2101.00027\n[44] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. Datasheets\nfor Datasets. Commun. ACM 64, 12 (2021), 86–92.\n[45] GitHub. 2022. GitHub REST API Documentation. https://docs.github.com/en/\nrest?apiVersion=2022-11-28\n[46] Jill Goldsmith. 2023. Michael Chabon, David Henry Hwang, Other Writers\nSue Meta AI Platform LLaMA For Copyright Infringement, Seek Class Action\nStatus. https://deadline.com/2023/09/michael-chabon-david-henry-hwang-\nwriters-sue-meta-ai-llama-copyright-1235544842/\n[47] Michael M. Grynbaum and Ryan Mac. 2023. The Times Sues OpenAI and\nMicrosoft Over A.I. Use of Copyrighted Work. https://www.nytimes.com/2023/\n12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html\n[48] The Authors Guild. 2023. Authors Guild Submits Written Testimony in Senate\nAI Hearing. https://authorsguild.org/news/ag-submits-written-testimony-in-\nsenate-ai-hearing/\n[49] Foad Hamidi, Morgan Klaus Scheuerman, and Stacy M. Branham. 2018. Gender\nRecognition or Gender Reductionism? The Social Implications of Embedded\nGender Recognition Systems. In Proceedings of the 2018 CHI Conference on\nHuman Factors in Computing Systems (CHI ’18) . Association for Computing\nMachinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3173574.3173582\n[50] Kotaro Hara, Abigail Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch,\nand Jeffrey P. Bigham. 2018. A Data-Driven Analysis of Workers’ Earnings on\nAmazon Mechanical Turk. In Proceedings of the 2018 CHI Conference on Human\nFactors in Computing Systems (CHI ’18) . Association for Computing Machinery,\nNew York, NY, USA, 1–14. https://doi.org/10.1145/3173574.3174023\n[51] Harriet Hartman and Ira M Sheskin. 2013. Estimating the Jewish student\npopulation of a college campus. Journal of Jewish Communal Service 88, 1-2\n(2013), 95–109.\n[52] Jan Hatzius, Joseph Briggs, Devesh Kodnani, and Giovanni Pierdomenico.\n2023. The Potentially Large Effects of Artificial Intelligence on Economic\nGrowth. https://www.gspublishing.com/content/research/en/reports/2023/03/\n27/d64e052b-0f6e-45d7-967b-d7be35fabd16.html\n[53] Brent Hecht. 2017. HCI and the U.S. Presidential Election: A Few Thoughts on\na Research Agenda. In CHI ’18 Panel Presentation: The 2016 US Election and HCI:\nTowards a Research Agenda . The Conference on Human Factors in Computing\nSystems (CHI), Denver, CO, 1–5. https://brenthecht.com/publications/chi17_\nbhecht_election2016panel.pdf\n[54] Brent Hecht. 2017. The Origins, Present, and Future of Algorithmic Bias. (2017).\n[55] Brent Hecht and Darren Gergle. 2010. The Tower of Babel Meets Web 2.0: User-\nGenerated Content and Its Applications in a Multilingual Context. In CHI ’10:\nAmerican Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training CHI ’24, May 11–16, 2024, Honolulu, HI, USA\n28th International Conference on Human Factors in Computing Systems (CHI ’10) .\nACM, Atlanta, GA, 291–300. https://doi.org/10.1145/1753326.1753370 ACM ID:\n1753370.\n[56] Harold S. Himmelfarb. 1986. Further comments on the use of DJN.Contemporary\nJewry 7, 1 (Jan 1986), 99–102. https://doi.org/10.1007/BF02967946\n[57] Harold S. Himmelfarb, R. Michael Loar, and Susan H. Mott. 1983. Sampling by\nEthnic Surnames: The Case of American Jews. The Public Opinion Quarterly 47,\n2 (1983), 247–260. http://www.jstor.org/stable/2749024\n[58] Ayesha Javed. 2023. AI Could Destroy Journalism as We Know It. Media\nMogul Barry Diller Hopes to Save It. https://time.com/6279147/barry-diller-ai-\njournalism/\n[59] Harry H. Jiang, Lauren Brown, Jessica Cheng, Mehtab Khan, Abhishek Gupta,\nDeja Workman, Alex Hanna, Johnathan Flowers, and Timnit Gebru. 2023. AI\nArt and its Impact on Artists. In Proceedings of the 2023 AAAI/ACM Conference\non AI, Ethics, and Society (AIES ’23) . Association for Computing Machinery, New\nYork, NY, USA, 363–374. https://doi.org/10.1145/3600211.3604681\n[60] Kevin Roose and Casey Newton. 2023. Casey Goes to the White House + The\nCopyright Battle Over Artificial Intelligence + HatGPT. https://www.nytimes.\ncom/2023/11/03/podcasts/hard-fork-executive-order-ai-copyright.html?\n[61] Os Keyes. 2018. The Misgendering Machines: Trans/HCI Implications of Au-\ntomatic Gender Recognition. Proceedings of the ACM on Human-Computer\nInteraction 2, CSCW (Nov 2018), 88:1–88:22. https://doi.org/10.1145/3274357\n[62] Aniket Kittur, Jeffrey V. Nickerson, Michael Bernstein, Elizabeth Gerber, Aaron\nShaw, John Zimmerman, Matt Lease, and John Horton. 2013. The Future of\nCrowd Work. In Proceedings of the 2013 Conference on Computer Supported\nCooperative Work (CSCW ’13) . ACM, New York, NY, USA, 1301–1318. https:\n//doi.org/10.1145/2441776.2441923 00108.\n[63] Kate Knibbs. 2024. Congress Wants Tech Companies to Pay Up for AI Training\nData. https://www.wired.com/story/congress-senate-tech-companies-pay-ai-\ntraining-data/\n[64] Barry A. Kosmin and Stanley Waterman. 1985. The Use and Misuse of Distinctive\nJewish Names in Research on Jewish Populations. Jewish Population Studies 19\n(1985), 1–9. https://api.semanticscholar.org/CorpusID:146336902\n[65] Airi Lampinen and Barry Brown. 2017. Market Design for HCI: Successes and\nFailures of Peer-to-Peer Exchange Platforms. In Proceedings of the 2017 CHI\nConference on Human Factors in Computing Systems (Denver, Colorado, USA)\n(CHI ’17). Association for Computing Machinery, New York, NY, USA, 4331–4343.\nhttps://doi.org/10.1145/3025453.3025515\n[66] Airi Lampinen, Christoph Lutz, Gemma Newlands, Ann Light, and Nicole Im-\nmorlica. 2018. Power Struggles in the Digital Economy: Platforms, Workers,\nand Markets. In Companion of the 2018 ACM Conference on Computer Sup-\nported Cooperative Work and Social Computing (CSCW ’18 Companion) . As-\nsociation for Computing Machinery, New York, NY, USA, 417–423. https:\n//doi.org/10.1145/3272973.3273004\n[67] Bernard Lazerwitz. 1986. Some comments on the use of distinctive Jewish names\nin surveys. Contemporary Jewry 7, 1 (Jan 1986), 83–91. https://doi.org/10.1007/\nBF02967944\n[68] Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito.\n2023. AI and Law: The Next Generation. In GenLaw ’23 (at ICML ’23) . The\nInternational Conference on Machine Learning (ICML), San Diego, CA, USA,\n1–21. http://dx.doi.org/10.2139/ssrn.4580739\n[69] Katherine Lee, A. Feder Cooper, FatemehSadat Mireshghallah, Madiha Zahrah,\nJames Grimmelmann, David Mimno, Deep Ganguli, and Ludwig Schubert. 2023.\nGenerative AI + Law (GenLaw) ’23 . The GenLaw Center. https://genlaw.github.\nio/\n[70] Michelle Seng Ah Lee and Jat Singh. 2021. The Landscape and Gaps in Open\nSource Fairness Toolkits. In Proceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems (CHI ’21) . Association for Computing Machinery,\nNew York, NY, USA, Article 699, 13 pages. https://doi.org/10.1145/3411764.\n3445261\n[71] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\nRocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP Tasks. In Advances in Neural In-\nformation Processing Systems , Vol. 33. Curran Associates, Inc., San Dieco,\nCA, USA, 9459–9474. https://proceedings.neurips.cc/paper/2020/hash/\n6b493230205f780e1bc26945df7481e5-Abstract.html\n[72] Hanlin Li, Bodhisattva Alarcon, Sara Milkes Espinosa, and Brent Hecht. 2018.\nOut of Site: Empowering a New Approach to Online Boycotts. InCSCW ’18: 2018\nACM Conference on Computer Supported Cooperative Work . ACM, New York, NY,\nUSA, 1–28.\n[73] Hanlin Li, Nicholas Vincent, Stevie Chancellor, and Brent Hecht. 2023. The\nDimensions of Data Labor: A Road Map for Researchers, Activists, and Policy-\nmakers to Empower Data Producers. In Proceedings of the 2023 ACM Conference\non Fairness, Accountability, and Transparency . ACM, New York, NY, USA, 1151–\n1161.\n[74] Christian D. Liddy. 2015. Urban Enclosure Riots: Risings of the Commons in\nEnglish Towns, 1480–1525. Past & Present 226, 1 (Feb. 2015), 41–77. https:\n//doi.org/10.1093/pastj/gtu038\n[75] Shayne Longpre, Robert Mahari, Niklas Muennighoff, Anthony Chen, Kartik\nPerisetla, William Brannon, Jad Kabbara, Luis Villa, and Sara Hooker. 2023.\nThe Data Provenance Project. In GenLaw Workshop at ICML . The International\nConference on Machine Learning (ICML), San Diego, CA, USA, 1–8.\n[76] Michael A. Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach.\n2020. Co-Designing Checklists to Understand Organizational Challenges and\nOpportunities around Fairness in AI. InProceedings of the 2020 CHI Conference on\nHuman Factors in Computing Systems (Honolulu, HI, USA) (CHI ’20). Association\nfor Computing Machinery, New York, NY, USA, 1–14. https://doi.org/10.1145/\n3313831.3376445\n[77] David A. Marker, Shelley Brock, Darby Steiger, Jill DeMatteis, and Hanna Popick.\n2021. Jewish Community Studies in the Twenty-First Century. Contemporary\nJewry 41, 2 (Jun 2021), 349–368. https://doi.org/10.1007/s12397-021-09388-w\n[78] Pablo Mateos. 2014. Classifying Ethnicity Through People’s Names. In Names,\nEthnicity and Populations (Advances in Spatial Science) . Springer, Berlin, Heidel-\nberg, 117–144. https://doi.org/10.1007/978-3-642-45413-4_6\n[79] Cory McCartan, Jacob Goldin, Daniel E. Ho, and Kosuke Imai. 2023. Estimating\nRacial Disparities When Race is Not Observed. arXiv preprint arXiv:2303.02580\nabs/2303.02580 (2023), 1–29. arXiv:2303.02580 [stat.AP]\n[80] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram\nGalstyan. 2021. A Survey on Bias and Fairness in Machine Learning. ACM\ncomputing surveys (CSUR) 54, 6 (2021), 1–35. http://arxiv.org/abs/1908.09635\n[81] Cullen Miller. 2023. ai.txt: A new way for websites to set permissions for AI.\nhttps://spawning.substack.com/p/aitxt-a-new-way-for-websites-to-set\n[82] Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A.\nSmith, and Luke Zettlemoyer. 2023. SILO Language Models: Isolating Legal\nRisk In a Nonparametric Datastore. https://doi.org/10.48550/arXiv.2308.04430\narXiv:2308.04430 [cs].\n[83] Caryle Murphy. 2016. The most and least educated U.S. religious\ngroups. https://www.pewresearch.org/short-reads/2016/11/04/the-most-and-\nleast-educated-u-s-religious-groups/\n[84] United States Holocaust Memorial Museum. 2017. Aryanization. https://\nencyclopedia.ushmm.org/content/en/article/aryanization\n[85] United States Holocaust Memorial Museum. 2023. Jewish Population of Eu-\nrope. https://encyclopedia.ushmm.org/content/en/gallery/jewish-population-\nof-europe\n[86] United Nations. 2021. Global Population. https://www.un.org/en/global-\nissues/population\n[87] NCBI. 2023. National Center for Biotechnology Information. https://www.ncbi.\nnlm.nih.gov/\n[88] U.S. Department of State. 2020. Building Coalitions and Alliances - The Canary\nin the Coal Mine? The Need for Cooperation. https://www.youtube.com/\nwatch?v=Ne3dGStTN8Q\n[89] OpenAI. 2023. GPT-4 Technical Report. https://doi.org/10.48550/arXiv.2303.\n08774 arXiv:2303.08774 [cs].\n[90] OpenAI. 2023. OpenAI Charter . OpenAI. https://openai.com/charter\n[91] Andrew Orlowski. 2023. The internet’s ‘original sin’ means AI will be a night-\nmare. https://www.telegraph.co.uk/business/2023/08/21/internets-original-\nsin-ai-nightmare/\n[92] Anil Oza and Mariana Lenharo. 2023. How the ‘groundbreaking’ Henrietta Lacks\nsettlement could change research. https://doi.org/10.1038/d41586-023-02479-8\n[93] UK Parliament. 2023. Enclosing the land. https://www.parliament.uk/\nabout/living-heritage/transformingsociety/towncountry/landscape/overview/\nenclosingland/\n[94] Amandalynne Paullada, Inioluwa Deborah Raji, Emily M Bender, Emily Den-\nton, and Alex Hanna. 2021. Data and its (dis) contents: A survey of dataset\ndevelopment and use in machine learning research. Patterns 2, 11 (2021), 1–14.\nhttps://doi.org/10.1016/j.patter.2021.100336\n[95] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outper-\nforming curated corpora with web data, and web data only. arXiv preprint\narXiv:2306.01116 abs/2306.01116 (2023), 1–32. arXiv:2306.01116 [cs.CL] https:\n//arxiv.org/abs/2306.01116\n[96] Jay Peters and Wes Davis. 2023. The New York Times blocks OpenAI’s web\ncrawler. https://www.theverge.com/2023/8/21/23840705/new-york-times-\nopenai-web-crawler-ai-gpt\n[97] Benjamin Phillips. 2007. Numbering the Jews: Evaluating and Improv-\ning Surveys of American Jews. Brandeis University ProQuest Univer-\nsity Publishing I (Feb 2007), 1–506. https://www.semanticscholar.\norg/paper/Numbering-the-Jews%3A-Evaluating-and-Improving-of-\nPhillips/4fb989e9344020d0a3ca85caf27417e15417077a\n[98] Associated Press. 2023. James Patterson, Margaret Atwood among thousands of\nwriters urging AI companies to honor copyrights. https://apnews.com/article/\npatterson-atwood-ai-open-letter-f2c434694ed22a64bd09abbb5742c1e5\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Precel et al.\n[99] Shawn Presser. 2023. Books3 Metadata. https://web.archive.org/web/\n20230000000000*/https://battle.shawwn.com/books3-metadata.jsonl, last ac-\ncessed 11/30/2023.\n[100] Alex Reisner. 2023. Revealed: The Authors Whose Pirated Books Are Powering\nGenerative AI. https://www.theatlantic.com/technology/archive/2023/08/\nbooks3-ai-meta-llama-pirated-books/675063/\n[101] Adam Roberts, Colin Raffel, Katherine Lee, Michael Matena, Noam Shazeer,\nPeter J. Liu, Sharan Narang, Wei Li, and Yanqi Zhou. 2020. Exploring the Limits\nof Transfer Learning with a Unified Text-to-Text Transformer.The Journal of\nMachine Learning Research 21, 1 (2020), 5485–5551.\n[102] Ira Rosenwaike. 1990. Leading Surnames Among American Jews. Names 38\n(Jun 1990), 31–38. https://doi.org/10.1179/nam.1990.38.1-2.31\n[103] Emma Roth. 2023. Another group of writers is suing OpenAI over copyright\nclaims. https://www.theverge.com/2023/9/11/23869145/writers-sue-openai-\nchatgpt-copyright-claims\n[104] Pamela Samuelson. 2023. Generative AI meets copyright. Science 381, 6654 (July\n2023), 158–161. https://doi.org/10.1126/science.adi0656 Publisher: American\nAssociation for the Advancement of Science.\n[105] Melissa Santos. 2024. ChatGPT and AI replacing jobs is a worry for workers,\nper WSU survey. https://www.axios.com/local/seattle/2024/02/09/chat-gpt-ai-\nworkers-replace-employees\n[106] Jonathan D. Sarna. 2019. Appendix: American Jewish Population Estimates,\n1660–2015. Yale University Press, New Haven, CT, USA, 391–392. https://doi.\norg/10.2307/j.ctvhrczf4.13\n[107] Leonard Saxe, Daniel Parmer, Elizabeth Tighe, Raquel Magidin de Kramer,\nDaniel Kallista, Daniel Nussbaum, Xajavion Seabrum, and Joshua Mandell. 2021.\nAmerican Jewish Population Estimates 2020: Summary & Highlights . Brandeis\nUniversity.\n[108] Kevin Schaul, Szu Yu Chen, and Nitasha Tiku. 2023. Inside the secret list of\nwebsites that make AI like ChatGPT sound smart. https://www.washingtonpost.\ncom/technology/interactive/2023/ai-chatbot-learning/\n[109] Martin Senftleben. 2023. Generative AI and Author Remuneration. IIC - Inter-\nnational Review of Intellectual Property and Competition Law 54, 10 (Nov. 2023),\n1535–1560. https://doi.org/10.1007/s40319-023-01399-4\n[110] Congressional Legal Service. 2023. Generative Artificial Intelligence and Copy-\nright Law. Technical Report. Congressional Legal Service. https://crsreports.\ncongress.gov/product/pdf/LSB/LSB10922\n[111] Ira M. Sheskin. 1998. A Methodology for Examining the Changing Size and\nSpatial Distribution of a Jewish Population: A Miami Case Study. Shofar: An\nInterdisciplinary Journal of Jewish Studies 17, 1 (1998), 97–116. https://doi.org/\n10.1353/sho.1998.0041\n[112] Ira M. Sheskin. 2016. Good Practices in Local Jewish Community Studies.\nContemporary Jewry 36, 3 (Oct 2016), 319–341. https://doi.org/10.1007/s12397-\n016-9184-3\n[113] Ira M. Sheskin and Arnold Dashefsky. 2012. Jewish Population in the United\nStates, 2012. The American Jewish Year Book 109/112 (2012), 143–211.\n[114] Ira M. Sheskin and Arnold Dashefsky (Eds.). 2012-2023. American Jewish Year\nBook (series) . Jewish Publication Society; American Jewish Committee, USA.\nhttps://www.springer.com/series/11193\n[115] Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra\nBlevins, Danqi Chen, and Luke Zettlemoyer. 2023. Detecting Pretraining Data\nfrom Large Language Models. https://doi.org/10.48550/arXiv.2310.16789\narXiv:2310.16789 [cs].\n[116] Ray A. Smith. 2024. AI Is Starting to Threaten White-Collar Jobs. Few Industries\nAre Immune. https://www.wsj.com/lifestyle/careers/ai-is-starting-to-threaten-\nwhite-collar-jobs-few-industries-are-immune-9cdbcb90 Section: Management.\n[117] Yanqiu Tao, Debbie Steckel, Jiří Jaromír Klemeš, and Fengqi You. 2021. Trend\ntowards virtual and hybrid conferences may be an effective climate change\nmitigation strategy. Nature Communications 12, 1 (Dec. 2021), 7324. https:\n//doi.org/10.1038/s41467-021-27251-2 Number: 1 Publisher: Nature Publishing\nGroup.\n[118] Adam Teller. 2010. Economic Life. https://yivoencyclopedia.org/article.aspx/\neconomic_life\n[119] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume\nLample. 2023. LLaMA: Open and Efficient Foundation Language Models. https:\n//doi.org/10.48550/arXiv.2302.13971\n[120] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,\nYasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288 abs/2307.09288 (2023), 1–77. https://arxiv.org/\nabs/2307.09288\n[121] Niels van Berkel, Jorge Goncalves, Daniel Russo, Simo Hosio, and Mikael B. Skov.\n2021. Effect of Information Presentation on Fairness Perceptions of Machine\nLearning Predictors. In Proceedings of the 2021 CHI Conference on Human Factors\nin Computing Systems (CHI ’21) . Association for Computing Machinery, New\nYork, NY, USA, Article 245, 13 pages. https://doi.org/10.1145/3411764.3445365\n[122] James Vincent. 2023. Getty Images sues AI art generator Stable Diffusion in the\nUS for copyright infringement. https://www.theverge.com/2023/2/6/23587393/\nai-art-copyright-lawsuit-getty-images-stable-diffusion\n[123] Nicholas Vincent and Brent Hecht. 2023. Sharing the Winnings of AI with Data\nDividends: Challenges with “Meritocratic” Data Valuation.\n[124] Nicholas Vincent, Brent Hecht, and Shilad Sen. 2019. “Data Strikes”: Evaluating\nthe Effectiveness of a New Form of Collective Action Against Technology\nCompanies. In The World Wide Web Conference (San Francisco, CA, USA)(WWW\n’19). Association for Computing Machinery, New York, NY, USA, 1931–1943.\nhttps://doi.org/10.1145/3308558.3313742\n[125] Nick Vincent and Hanlin Li. 2023. ChatGPT Stole Your Work. So What Are\nYou Going to Do? https://www.wired.com/story/chatgpt-generative-artificial-\nintelligence-regulation/\n[126] Nicholas M. Vincent. 2020. Don’t give OpenAI all the credit for GPT-3: You might\nhave helped create the latest “astonishing” advance in AI too . People, Space, and\nAlgorithms Research Group. https://www.psagroup.org/blogposts/62\n[127] Johannes Wachs, Mariusz Nitecki, William Schueller, and Axel Polleres. 2022.\nThe Geography of Open Source Software: Evidence from GitHub. Technological\nForecasting and Social Change 176 (Mar 2022), 121478. https://doi.org/10.1016/j.\ntechfore.2022.121478\n[128] Wikipedia. 2023. René Carmille. Wikipedia. https://fr.wikipedia.org/w/index.\nphp?title=Ren%C3%A9_Carmille&oldid=210288825 Page Version ID: 210288825.\n[129] Siyuan Xia, Zhiru Zhu, Chris Zhu, Jinjin Zhao, Kyle Chard, Aaron J. Elmore,\nIan Foster, Michael Franklin, Sanjay Krishnan, and Raul Castro Fernandez. 2022.\nData station: delegated, trustworthy, and auditable computation to enable data-\nsharing consortia with a data escrow. Proceedings of the VLDB Endowment 15,\n11 (July 2022), 3172–3185. https://doi.org/10.14778/3551793.3551861\n[130] Erdem Dogukan Yilmaz, Ivana Naumovska, and Vikas A. Aggarwal. 2023. AI-\nDriven Labor Substitution: Evidence from Google Translate and ChatGPT. https:\n//doi.org/10.2139/ssrn.4400516\n[131] Hamed Zamani, Fernando Diaz, Mostafa Dehghani, Donald Metzler, and Michael\nBendersky. 2022. Retrieval-Enhanced Machine Learning. In Proceedings of the\n45th International ACM SIGIR Conference on Research and Development in Infor-\nmation Retrieval. Special Interest Group on Information Retrieval (SIGIR), NYC,\nNY, USA, 2875–2886. https://doi.org/10.1145/3477495.3531722 arXiv:2205.01230\n[cs].\n[132] Ali Zarifhonarvar. 2023. Economics of ChatGPT: A Labor Market View on the\nOccupational Impact of Artificial Intelligence. https://doi.org/10.2139/ssrn.\n4350925\n[133] Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, and Philip S. Yu.\n2023. Large Language Models for Robotics: A Survey. ArXiv abs/2311.07226\n(2023), 1–19. https://api.semanticscholar.org/CorpusID:265149884\nA APPENDIX\nThis appendix covers three topics across five sections:\n•Additional robustness checks and analyses we performed to\nincrease confidence in our methods (A.1, A.2, A.3).\n•More information on the history and use of DJNs (A.4).\n•A supplementary analysis that measures IP dispossession by\nindustry. We include it here as both a robustness check (the\nresults seem to support our conclusions) and as a topic for\nfuture research (A.5).\nA.1 How Wrong Would Our Estimates Have To\nBe To Achieve No Impact\nIn the main body of this study, we identify a reasonable range for\neach parameter to determine lower and upper bounds for each\nestimate. Here, we check how robust the results are to even more\nconservative parameter values. All of these values are massive\ndepartures from the best available data discussed in the main body\nof our paper; we include this section to validate the robustness of\nour claims. To do this, we calculated what value each parameter\nwould have to have in order to indicate no over-representation. We\nheld all parameters other than the one under investigation constant\nat the original lower bound estimates. We found that in order for\nthe weighted relative dispossession magnitude to be ∼1 X (ie: no\noverrepresentation), the following would have to be true:\nAmerican Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training CHI ’24, May 11–16, 2024, Honolulu, HI, USA\n•32.6% or fewer of people with DJNs are Jewish (the remaining\n67.4% are non-Jews).\n•27.4% or more of the U.S. Jewish population have DJNs.\n•5.9% of the U.S. population is Jewish.\nThese precise values for each parameter are in Tables 3-5.\nParameter Estimates\n% precision of DJNs 32.6%\n% coverage of DJNs 11.18%\n% of US population that is Jewish 2.4%\nRelative Dispossession Magnitude\nTotal 1.17 X\nWeighted Total 1.00 X\nTable 3: All parameters other than precision are held con-\nstant at lower bound estimates. With precision at 32.6%, the\nweighted total relative dispossession magnitude would be\n1.00 X.\nParameter Estimates\n% precision of DJNs 80%\n% coverage of DJNs 27.4%\n% of US population that is Jewish 2.4%\nRelative Dispossession Magnitude\nTotal 1.17 X\nWeighted Total 1.00 X\nTable 4: All parameters other than coverage are held con-\nstant at lower bound estimates. With coverage at 27.4%, the\nweighted total relative dispossession magnitude would be\n1.00 X.\nParameter Estimates\n% precision of DJNs 80%\n% coverage of DJNs 11.18%\n% of US population that is Jewish 5.9%\nRelative Dispossession Magnitude\nTotal 1.16 X\nWeighted Total 1.00 X\nTable 5: All parameters other than the % of the U.S. popula-\ntion that is Jewish are held constant. With U.S. Jews at 5.9%\nof the population, the weighted total relative dispossession\nmagnitude would be 1.00 X.\nA.2 Estimation of U.S. Representation In\nDataset\nHere, we discuss how we investigated the extent to which each\ndataset included content from people based in the U.S. This helped\nus increase our confidence in our estimate of the relative representa-\ntion of Jewish Americans relative other Americans. An alternative\napproach in future work might be to consider representative purely\nat the global level.\nA.2.1 Pubmed Central. We used journal countries of publica-\ntion [87] as a proxy for author countries to estimate an expected\n0.6% of Jewish authorship as compared to the 1.95-2.21% we see\n(Table 6).\nJournals\nin PMC\nJournals by Country\nof Publication\nU.S. Other\n3,499 1,000 2,499\nTable 6: Publication countries of PMC journals, intended as a\nrough estimate of percentage of the dataset that is U.S. based\nor affiliated.\nA.2.2 ArXiv. We estimated the number of U.S. authors who had\nsubmitted work to ArXiv. We scoped U.S. authorship to those who\nwork at U.S. institutions, as they are most likely U.S. residents. Using\ndata from the U.S. Department of Education’s National Center for\nEducation Statistics (NCES),4 which tracks educational institutions\nthat accept federal funding, we identified U.S. higher education\ninstitutions. We then manually inspected all ArXiv institutions con-\ntaining the word “hospital” or “medical” to find research originating\nin U.S. hospital systems. We then sorted the remaining institutions\nby highest number of submissions and added U.S. research insti-\ntutes (e.g., NIST, Los Alamos National Laboratory) and companies\n(e.g., Intel, IBM) to our list, until no U.S.-based institutions remained\nin the top 500 institutions. From this list, we calculated the number\nof U.S. submissions. This is likely to be an undercount, as there are\nover 6200 educational institutions and there are likely formatting\ninconsistencies between ArXiv and NCES that will have uninten-\ntionally excluded U.S. institutions.\nA.3 Robustness Check for Document Age in\nFreeLaw\nWe considered constructing a weighted average of Jewish popula-\ntion over time based on the publication dates of documents in the\ndataset. When we used FreeLaw as a case study (because it is the\ndataset with the oldest records, covers the greatest time span, and\nis entirely U.S.-based), we found that it barely affected our expected\nJewish population percentage. Because documents in the rest of\nthe sources are more recent, and the Jewish population has stayed\nsomewhat constant over the past 10-15 years (when many of the\ndocuments across datasets were produced), we decided not to stray\nfrom the naive approach to account for document age. We note\nhere that if a significant number of documents in a source were\npublished between 2000-2010 we’d expect to see a slightly lower\npercentage of Jews, and if a significant number of documents were\npublished between 1920-1970 we’d expect to see a slightly higher\npercentage.\n4https://nces.ed.gov/\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Precel et al.\nA.4 Additional Detail On the Distinctive Jewish\nNames-Based Methodology\nAs summarized briefly in Section 2.4.2, DJNs have a long history in\nthe world of Jewish demography. The original concept is attributed\nto Kohs, who found that the most common surnames on Jewish\nFederation membership lists represented a significant proportion of\noverall membership [57]. Later surveys in the 1960s-80s confirmed\nthat, depending on the list used, the national proportion of Jews\nwith DJNs remained roughly constant at∼11-12%. Since then, DJNs\nhave been used in numerous studies, including (a) as the basis\nfor the sampling frame (or more likely, as the basis for one of\nseveral sampling frames) for local Jewish community studies; (b)\nto measure the change in size of a Jewish community over time;\nand (c) to estimate the overall size of a Jewish community within\na larger local population (e.g., the number of Jews on a college\ncampus [51]).5 Any potential use of DJNs should be measured\non at least two axes—coverage and representativeness—which we\nelaborate on below.\nA.4.1 Coverage. Research that uses DJN-based lists as a sole sam-\npling frame by definition can only cover the percentage of the\npopulation with a Jewish surname. In local community studies, this\nputs a strong cap on possible participants [36]. As the current study\ndoes not directly survey a sub-population, coverage in that sense\nis not an issue so long as we can calculate the proportion of the\nU.S. Jewish community that our sample captures. In the earliest\nstudies of DJNs, researchers estimated that between 11–12% of the\nnational Jewish population had one of ∼35 surnames ( [57, 64, 67]).\nHowever, they caution that these numbers vary significantly across\nlocal subsets of the Jewish community, so smaller studies should be\nwary of applying the 11–12% figure “unless they have prior knowl-\nedge about the actual size of the proportion of the population that\nDJN persons constitute and the stability of that proportion over\ntime. ” [56].\nThese numbers have remained largely consistent over time for\nlarge U.S. Jewish communities. In 2007, Phillips found that 12.4%\nof Greater Boston Area Jews had a DJN, and 91.8% of people with\na DJN were Jewish. In a 2012 review of local area studies, Sheskin\nand Dashefsky noted, “the fact that about 8–12% of American Jews,\ndespite rising intermarriage, continue to have one of 36 Distinctive\nJewish Names... facilitates making reasonable estimates of the Jew-\nish population. ” [113] As additional confirmation, we used Hartman\nand Sheskin’s method to calculate an “expansion factor” based on\nthe Pew 20136 Jewish population estimate [21] in conjunction with\nthe U.S. Census’s 2010 surname frequency data [ 20].7 We found\nthat ∼9.15–11.18% of the U.S. Jewish population at the time was\ncovered by the DJN frame, which is aligned with prior estimates.\nA.4.2 Representativeness. Another concern with using a DJN-only\nframe is as follows: making claims about all Jews on the basis of\nJews with DJNs relies on the assumption that the latter group does\n5See the “United States Jewish Population” chapters of the American Jewish Yearbook\nseries for a detailed account of such studies [33, 114].\n6We use the 2013 estimate rather than the 2020 estimate in order to align with Census\ndata, of which 2010 is the latest available.\n7To do this, we counted the number of people with DJNs represented on the list,\naccounted for non-Jews with DJNs as described in 3.2.2, and compared the resulting\nnumber to the total number of Jews in the U.S. counted by Pew.\nnot differ significantly in character from the former. A number of\nstudies have been conducted on the representativeness of DJNs\nwith mixed conclusions (in part due to lack of standardization with\nregards to which DJNs are used in a given study). It seems to be\nlargely the case that DJN samples underrepresent intermarried Jews\nand their children, Jews with self-defined partial, mixed, or non-\nreligions connections to Judaism, younger Jews, and (expectedly)\nJews without Jewish parents [26, 36, 67, 112]. Critics of DJNs gener-\nally argue that the type of Jews least likely to be counted by DJNs\nare “on the margin, ” which is especially problematic for studies\nwhose goal it is to help Jewish community leaders best serve their\nconstituents [26]. However, because our study strictly estimates\npopulation size without surveying individuals, representation is\nonly important insofar as it relates to likelihood of producing IP\nthat appears in our dataset. As long as this is the case, our expansion\nfactor should account for any undercount.\nWhile we do not have prior reason to suspect such a bias, it is a\nlimitation of our method that we cannot test for it directly. There is\nsome evidence to suggest that DJN samples do not show significant\ndifferences in income [26, 112] and education [57] as compared to\nthe general population; we did not find direct comparisons between\nthe occupations of DJN and non-DJN samples (which would be\nthe most direct proxy). Absent a compelling hypothesis for why\nDJN samples would significantly differ from the general population\non occupation or writing output, it is reasonable to assume that\nour estimates provide order-of-magnitude bounds for the target\nvariables.\nA.4.3 Broader Considerations. While it is not the focus of this pa-\nper and we defer to the large literature on this topic for further\ndiscussion (e.g. [ 13]), it is useful to reflect on the broader con-\nsiderations surrounding our need to rely on methodologies like\nthose described above. Historical context is critical here. Previous\ndatabases of people with Jewish identity have been extremely dan-\ngerous to the Jewish community, with notable examples of these\ndatabases contributing to the deaths of millions of Jews in World\nWar II [13]. Indeed, there is at least one person who was killed by\nthe Nazi government for intentionally introducing noise into these\ndatabases [128].\nGiven this history, and although this is out of scope for this work\nand not necessarily a consensus view, there is a reasonable belief\nthat the best balance between being able to make data-driven assess-\nments relevant to the Jewish community and protecting members\nof the community from serious material harm is through a noisy\nsensor like those provided by DJNs.\nA.5 Measuring IP Dispossession by Industry\nAbove, our analysis focused heavily on looking at carefully selected\nsubsets of training data, each of which mapped to a particular job\ncategory or industry.\nMany LLMs have been trained using filtered subset of the wide-\nranging Common Crawl dataset. This data is not as structured\nas any of the the specific Pile subsets we looked at, which each\nhave underlying institutional norms that drive some of the implicit\nformatting, tone, and content standards.\nPast work has already identified evidence of IP related to spe-\ncific jobs in LLM training data. One case study showed that GPT-4\nappears to have memorized content from the New York Times [34],\nAmerican Jews May Be Disproportionately Harmed by Intellectual Property Dispossession in Large Language Model Training CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nand the lawsuit against OpenAI filed by the New York Times pro-\nvided further evidence of this memorization [ 47]. Investigation\nby the Washington Post and AI2 looked into the domains that\ncontributed the most tokens to Google’s C4 [101] dataset (one fil-\ntered version of Common Crawl) [ 108]. In the top 10 domains\nalone, we see domains with IP produced by people in law (e.g.\npatents.google.com), media and journalism (e.g. nytimes.com), and\nscience and medicine (e.g. journals.plos.org). The descriptive stats\nfrom Schaul et al.’s analysis [108] further substantiate the idea that\nIP-heavy job categories like law, media, journalism, science, and\nmedicine help constitute much of the LLM training data outside\ncareful subsets.\nAs a supplementary analysis, we conducted a small investigation\ninto another filtered Common Crawl variant, RefinedWeb [95]. We\napproximated a random sample by randomly downloading 0.4% of\nthe roughly 5300 data files shared by the RefinedWeb curators on\nHuggingFace. We ranked the domains in this sample by number of\ntotal words. We found found very similar results to the C4 investiga-\ntion – journalists, scientists, medical researchers, lawyers, and other\nprofessional classes were dominant. Specifically, of the top 200 do-\nmains in C4, 131 were in at least the top 1000 of RefinedWeb. There\nare numerous subjective design choices involved in this minor anal-\nysis (when to consider subdomains like ‘patents.google.com’, how\nto label a domain as pertaining to a specific job category), so we\nleave a full comparison along these lines to future work beyond the\nscope of our case study focusing on Jewish Americans.\nThe prior results related to job-specific memorization and po-\ntential IP dispossession further suggests that any groups whose\neconomic well-being is highly tied to job market participation in IP\nheavy fields may be especially vulnerable to economic harms from\nthe deployment of any labor-replacing LLM-based technologies.\nWe compared the self-reported job category numbers from a\nPew Research study on Jewish Americans to U.S. Bureau of Labor\nStatistics numbers on the distribution of American workers by job\ncategory. While the BLS and job categories reported by Jewish\nrespondents to the Pew survey did not map directly to each other,\nwe were able to manually find some close mappings between them.\nThis process required us to manually map the BLS categories to the\nPew categories (which were not based on any formal taxonomy).\nBased on this mapping, it seems likely that Jewish American\nworkers participate in several IP-relevant professions at high rates,\npotentially around 4x for law and STEM.\nThus, it is also likely possible to support the broad argument\nof our paper – that Jewish Americans and other minority groups\nmight be especially prone to IP dispossession and therefore to\neconomic harms – primarily using data on relative representation\nof group members in different job categories and connecting these\njob categories to LLM training data and LLM-replaceable labor.\nIn the future, it could make sense to investigate IP dispossession\ntargeted at other groups with many members in these fields. Alter-\nnatively, professional organizations may wish to lead the charge\nthemselves.",
  "topic": "Harm",
  "concepts": [
    {
      "name": "Harm",
      "score": 0.625102698802948
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4340672194957733
    },
    {
      "name": "Judaism",
      "score": 0.42696383595466614
    },
    {
      "name": "Political science",
      "score": 0.42221498489379883
    },
    {
      "name": "Development economics",
      "score": 0.3813948333263397
    },
    {
      "name": "Psychology",
      "score": 0.32144004106521606
    },
    {
      "name": "Law",
      "score": 0.2656210958957672
    },
    {
      "name": "History",
      "score": 0.25415846705436707
    },
    {
      "name": "Economics",
      "score": 0.2056027054786682
    },
    {
      "name": "Geography",
      "score": 0.18846499919891357
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I111088046",
      "name": "Boston University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I111979921",
      "name": "Northwestern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I18014758",
      "name": "Simon Fraser University",
      "country": "CA"
    }
  ],
  "cited_by": 4
}