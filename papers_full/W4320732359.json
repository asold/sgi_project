{
  "title": "A transformer-based synthetic-inflow generator for spatially developing turbulent boundary layers",
  "url": "https://openalex.org/W4320732359",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3116814587",
      "name": "Mustafa Z. Yousif",
      "affiliations": [
        "Pusan National University"
      ]
    },
    {
      "id": "https://openalex.org/A2067694021",
      "name": "Meng Zhang",
      "affiliations": [
        "Pusan National University"
      ]
    },
    {
      "id": "https://openalex.org/A2133713078",
      "name": "Linqi Yu",
      "affiliations": [
        "Pusan National University"
      ]
    },
    {
      "id": "https://openalex.org/A2169096561",
      "name": "Ricardo Vinuesa",
      "affiliations": [
        "KTH Royal Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2112167934",
      "name": "HeeChang Lim",
      "affiliations": [
        "Pusan National University"
      ]
    },
    {
      "id": "https://openalex.org/A3116814587",
      "name": "Mustafa Z. Yousif",
      "affiliations": [
        "Pusan National University"
      ]
    },
    {
      "id": "https://openalex.org/A2067694021",
      "name": "Meng Zhang",
      "affiliations": [
        "Pusan National University"
      ]
    },
    {
      "id": "https://openalex.org/A2133713078",
      "name": "Linqi Yu",
      "affiliations": [
        "Pusan National University"
      ]
    },
    {
      "id": "https://openalex.org/A2169096561",
      "name": "Ricardo Vinuesa",
      "affiliations": [
        "KTH Royal Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2112167934",
      "name": "HeeChang Lim",
      "affiliations": [
        "Pusan National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4286909924",
    "https://openalex.org/W3149676208",
    "https://openalex.org/W4309822039",
    "https://openalex.org/W2891158090",
    "https://openalex.org/W6630602946",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W3045549273",
    "https://openalex.org/W4205317834",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6829887170",
    "https://openalex.org/W2157960017",
    "https://openalex.org/W2796910131",
    "https://openalex.org/W3012591061",
    "https://openalex.org/W2402685789",
    "https://openalex.org/W2045182000",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2059084955",
    "https://openalex.org/W2970612144",
    "https://openalex.org/W2057393236",
    "https://openalex.org/W2030158310",
    "https://openalex.org/W2067153599",
    "https://openalex.org/W3090502885",
    "https://openalex.org/W3124389259",
    "https://openalex.org/W2074295499",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W2008179361",
    "https://openalex.org/W4225377386",
    "https://openalex.org/W2594632285",
    "https://openalex.org/W3017770080",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W3198740648",
    "https://openalex.org/W6828894009",
    "https://openalex.org/W2074447412",
    "https://openalex.org/W2148516303",
    "https://openalex.org/W2888317899",
    "https://openalex.org/W2973119841",
    "https://openalex.org/W2888137290",
    "https://openalex.org/W3206302539",
    "https://openalex.org/W3163453057",
    "https://openalex.org/W3093027387",
    "https://openalex.org/W2795982117",
    "https://openalex.org/W3197659304",
    "https://openalex.org/W2087910288",
    "https://openalex.org/W2902480423",
    "https://openalex.org/W2949921181",
    "https://openalex.org/W3136255988",
    "https://openalex.org/W2585298970",
    "https://openalex.org/W2045971153",
    "https://openalex.org/W4210550828",
    "https://openalex.org/W2951279763",
    "https://openalex.org/W2617519253",
    "https://openalex.org/W4211029392",
    "https://openalex.org/W2047973169",
    "https://openalex.org/W3185689115",
    "https://openalex.org/W3101223387",
    "https://openalex.org/W3131838898",
    "https://openalex.org/W3101378293",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W3100989476",
    "https://openalex.org/W3105469151",
    "https://openalex.org/W3103462314",
    "https://openalex.org/W3120515765",
    "https://openalex.org/W3104657094",
    "https://openalex.org/W3114871366"
  ],
  "abstract": "This study proposes a newly developed deep-learning-based method to generate turbulent inflow conditions for spatially developing turbulent boundary layer (TBL) simulations. A combination of a transformer and a multiscale-enhanced super-resolution generative adversarial network is utilised to predict velocity fields of a spatially developing TBL at various planes normal to the streamwise direction. Datasets of direct numerical simulation (DNS) of flat plate flow spanning a momentum thickness-based Reynolds number, $Re_\\theta = 661.5\\unicode{x2013}1502.0$ , are used to train and test the model. The model shows a remarkable ability to predict the instantaneous velocity fields with detailed fluctuations and reproduce the turbulence statistics as well as spatial and temporal spectra with commendable accuracy as compared with the DNS results. The proposed model also exhibits a reasonable accuracy for predicting velocity fields at Reynolds numbers that are not used in the training process. With the aid of transfer learning, the computational cost of the proposed model is considered to be effectively low. Furthermore, applying the generated turbulent inflow conditions to an inflow–outflow simulation reveals a negligible development distance for the TBL to reach the target statistics. The results demonstrate for the first time that transformer-based models can be efficient in predicting the dynamics of turbulent flows. They also show that combining these models with generative adversarial networks-based models can be useful in tackling various turbulence-related problems, including the development of efficient synthetic-turbulent inflow generators.",
  "full_text": "J. Fluid Mech.(2023), vol. 957, A6, doi:10.1017/jfm.2022.1088\nA transformer-based synthetic-inﬂow generator\nfor spatially developing turbulent boundary\nlayers\nMustafa Z. Yousif1, Meng Zhang1, Linqi Yu1, Ricardo Vinuesa2 and\nHeeChang Lim1,†\n1School of Mechanical Engineering, Pusan National University, 2, Busandaehak-ro 63beon-gil,\nGeumjeong-gu, Busan 46241, Republic of Korea\n2FLOW, Engineering Mechanics, KTH Royal Institute of Technology, Stockholm, Sweden\n(Received 18 May 2022; revised 26 November 2022; accepted 14 December 2022)\nThis study proposes a newly developed deep-learning-based method to generate turbulent\ninﬂow conditions for spatially developing turbulent boundary layer (TBL) simulations.\nA combination of a transformer and a multiscale-enhanced super-resolution generative\nadversarial network is utilised to predict velocity ﬁelds of a spatially developing TBL at\nvarious planes normal to the streamwise direction. Datasets of direct numerical simulation\n(DNS) of ﬂat plate ﬂow spanning a momentum thickness-based Reynolds number, Reθ =\n661.5–1502.0, are used to train and test the model. The model shows a remarkable ability\nto predict the instantaneous velocity ﬁelds with detailed ﬂuctuations and reproduce the\nturbulence statistics as well as spatial and temporal spectra with commendable accuracy as\ncompared with the DNS results. The proposed model also exhibits a reasonable accuracy\nfor predicting velocity ﬁelds at Reynolds numbers that are not used in the training\nprocess. With the aid of transfer learning, the computational cost of the proposed model is\nconsidered to be effectively low. Furthermore, applying the generated turbulent inﬂow\nconditions to an\ninﬂow–outﬂow simulation reveals a negligible development distance\nfor the TBL to reach the target statistics. The results demonstrate for the ﬁrst time that\ntransformer-based models can be efﬁcient in predicting the dynamics of turbulent ﬂows.\nThey also show that combining these models with generative adversarial networks-based\nmodels can be useful in tackling various turbulence-related problems, including the\ndevelopment of efﬁcient synthetic-turbulent inﬂow generators.\nKey words:turbulent boundary layers, turbulence simulation, machine learning\n† Email address for correspondence: hclim@pusan.ac.kr\n© The Author(s), 2023. Published by Cambridge University Press. This is an Open Access article,\ndistributed under the terms of the Creative Commons Attribution licence ( http://creativecommons.org/\nlicenses/by/4.0/), which permits unrestricted re-use, distribution and reproduction, provided the\noriginal article is properly cited. 957 A6-1\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\n1. Introduction\nThe generation of turbulent inﬂow conditions is essential in simulating spatially\ndeveloping turbulent boundary layers (TBLs), considering its effect on the accuracy of\nthe simulations and computational cost. It is also a challenging topic due to the need\nfor the time-dependent turbulent inﬂow data to be accurately described. The generated\ndata should satisfy the momentum and continuity equations and consequently match the\nturbulent statistics and spectra of the ﬂow. Several approaches have been proposed to\ngenerate turbulent inﬂow conditions with different levels of success (Wu 2017). Adding\ninﬁnitesimal perturbations on the laminar mean velocity proﬁle at the inlet section\nof the computational domain and allowing the transition of the boundary layer is a\nstraightforward approach that guarantees a realistic\nspatially developing TBL. However,\nthe need for a development distance that is long enough for the ﬂow to reach the fully\nturbulent state can result in a high computational cost, making this approach not applicable\nfor most turbulent ﬂow simulations, where fully turbulent inﬂow conditions are required.\nThe use of precursor (auxiliary) parallel ﬂow (fully developed ﬂow) simulations with\nperiodic boundary conditions applied to the streamwise direction is another approach that\ncan be used by extracting ﬂow ﬁelds from a plane normal to the streamwise direction and\napplying the data as inﬂow conditions to the main simulations. Although this method can\nproduce accurate turbulence statistics and spectra for fully developed ﬂows, it requires a\nhigh computational cost. Additionally, the streamwise periodicity effect, caused by the\nrecycling of the ﬂow within a limited domain size, can lead to physically unrealistic\nstreamwise-repetitive features in the ﬂow ﬁelds (Wu 2017). Furthermore, using parallel\nﬂow data as inﬂow for a simulation of a\nspatially developing TBL can result in a long\ndevelopment distance downstream of the domain inlet to produce the correct boundary\nlayer characteristics (Lund 1993).\nTo address this issue, a recycling–rescaling method was introduced by Lund, Wu &\nSquires ( 1998), which is a modiﬁed version of the method by Spalart ( 1988). Here the\nvelocity ﬁelds in the auxiliary simulation are rescaled before being reintroduced at the\ninlet section. Another well-known approach for generating turbulent inﬂow conditions is\nadding random ﬂuctuations based on known turbulence statistics. The methods that are\nbased on this approach are usually called synthetic turbulent inﬂow generation methods.\nSeveral methods, such as the synthetic random Fourier method (Le, Moin & Kim 1997),\nsynthetic digital ﬁltering method (Klein, Sadiki & Janicka 2003), synthetic coherent eddy\nmethod (Jarrin et al. 2006), synthetic vortex method (Mathey et al. 2006;S e r g e n t2002;\nYousif & Lim 2021), synthetic volume-force method (Spille-Kohoff & Kaltenbach 2001;\nSchlatter & Örlü 2012) and numerical counterpart of the experimental tripping methods\n(Sanmiguel Vila et al.2017) have been proposed to feature a fast generation of turbulence\nwith various levels of precision. However, a long-distance downstream of the domain inlet\nis required to allow the boundary layer to recover from the unphysical random ﬂuctuations\nof the generated velocity ﬁelds and produce the right ﬂow characteristics, resulting in a\nhigh computational cost. Another approach based on proper orthogonal decomposition\n(POD) and Galerkin projection has been proposed to build a reduced-order ﬂow model\nand generate turbulent inﬂow conditions by utilising the most energetic eddies (Johansson\n& Andersson 2004). A similar approach has been applied to experimental measurements\n(Druault et al.2004;P e r r e tet al.2008) to reconstruct turbulent inﬂow velocity ﬁelds from\nhot-wire anemometry and particle image velocimetry using POD and linear stochastic\nestimation. This approach showed the possibility of utilising the experimental results as\nturbulent inﬂow conditions. However, the costly experimental set-up makes this approach\nnot applicable as a general method to generate turbulent inﬂow data.\n957 A6-2\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\nThe rapid development of deep learning algorithms and the increase in the graphic\nprocessing unit (GPU) capability, accompanied by the enormous amounts of high-ﬁdelity\ndata generated from experimental and numerical simulations, encourage exploring new\ndata-driven approaches that can efﬁciently tackle various ﬂuid-ﬂow problems (Kutz 2017;\nBrunton, Noack & Koumoutsakos 2020; Vinuesa & Brunton 2022). Deep learning is\na subset of machine learning, where deep neural networks are used for classiﬁcation,\nprediction and feature extraction (LeCun, Bengio & Hinton 2015). Recently, several\nmodels have shown great potential in solving different problems in the ﬁeld of turbulence,\nsuch as turbulence modelling (Wang, Wu & Xiao 2017; Duraisamy, Iaccarino & Xiao\n2019), turbulent ﬂow prediction (Lee & You 2019;S r i n i v a s a net al.2019), reduced-order\nmodelling (Nakamura et al.2021; Yousif & Lim 2022), ﬂow control (Rabault et al.2019;\nFan et al.2020; Park & Choi 2020; Vinuesa et al.2022), non-intrusive sensing (Guastoni\net al. 2021; Güemes et al. 2021) and turbulent ﬂow reconstruction (Deng et al. 2019;\nFukami, Fukagata & Taira 2019a;K i m et al.2021; Yousif, Yu & Lim 2021, 2022b; Eivazi\net al.2022;Y u et al.2022).\nFurthermore, recent studies on the generation of turbulent inﬂow conditions using\ndeep learning models have shown promising results. Fukami et al. (2019b)s h o w e d\nthat convolutional neural networks (CNNs) could be utilised to generate turbulent\ninﬂow conditions using turbulent channel ﬂow data by proposing a model based on\na convolutional autoencoder (CAE) with a multilayer perceptron (MLP). Kim & Lee\n(2020) proposed a generative adversarial network (GAN) and a recurrent neural network\n(RNN)-based model as a representative of unsupervised deep learning to generate\nturbulent inﬂow conditions at various Reynolds numbers using data of turbulent channel\nﬂow at various friction Reynolds numbers. Recently, Yousif, Yu & Lim ( 2022a) utilised a\ncombination of a multiscale CAE with a subpixel convolution layer (MSC\nSP-AE) having\na physical constraints-based loss function and a long short-term memory (LSTM) model\nto generate turbulent inﬂow conditions from turbulent channel ﬂow data.\nIn all of those models, the prediction of the turbulent inﬂow conditions is based on\nparallel ﬂows, which, as mentioned earlier, is more suitable as inﬂow for fully developed\nTBLs. Therefore, it is necessary to develop a model that considers the spatial development\nof TBLs (Jiménez et al.2010). In this context, this paper proposes a deep learning model\n(DLM) consisting of a transformer and a multiscale-enhanced super-resolution generative\nadversarial network (MS-ESRGAN) to generate turbulent inﬂow conditions for\nspatially\ndeveloping TBL simulations.\nThe remainder of this paper is organised as follows. In § 2, the methodology of\ngenerating the turbulent inﬂow data using the proposed DLM is explained. The direct\nnumerical simulation (DNS) datasets used for training and testing the model are described\nin § 3. Section 4 presents the results obtained from testing the proposed model. Finally, § 5\npresents the conclusions of the study.\n2. Methodology\nThe proposed DLM is a combination of two architectures. The ﬁrst one is the transformer\n(Vaswani et al. 2017) and the second one is the MS-ESRGAN (Yousif et al. 2021). The\ntransformer is used to predict the temporal evolution of coarse velocity ﬁelds obtained\nby selecting distributed points at various planes normal to the streamwise direction of a\nspatially developing TBL ﬂow, as shown in ﬁgure 1 (a). Here the ﬂow data are obtained\nthrough DNS. Meanwhile, the MS-ESRGAN is used to perform a super-resolution\nreconstruction of the data for all the planes predicted by the transformer, leading to ﬁnal\nhigh-resolution (HR) data, i.e. velocity ﬁelds with the same resolution as the ground\n957 A6-3\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\nTransformer\nMS-\nESRGAN\n. . .\nt0 . . . tn t1 . . . tn+1 t2 . . . tn+2\ntn+2tn+1\nMS-\nESRGAN\ntn+2tn+1\nCoarse data\nHR data\nTraining dataReθ = 661.5Training data Reθ = 905.7Training data Reθ = 1362.0\n(a) Laminar Transition Turbulent\ny\nx\nMS-\nESRGAN\nTransformer\nTransformer\nTransformer TransformerTransformer\n(c)\nTraining dataReθ = 661.5Training data Reθ = 905.7Training data Reθ = 1362.0\n(b) Laminar Transition Turbulent\ny\nx\nFigure 1. Schematic of ( a) training procedure for the transformer, ( b) training procedure for the\nMS-ESRGAN and ( c) turbulent inﬂow generation using the proposed DLM.\ntruth data, as shown in ﬁgure 1 (b). In other words, the transformer is trained for the\ndata at each plane, whereas MS-ESRGAN is trained for all the planes used in the\ntraining process. Figure 1 (c) shows the schematic representation of the proposed DLM\nfor generating turbulent inﬂow conditions. As shown in the ﬁgure, the initial input to\nthe DLM is represented by coarse velocity data obtained from a plane normal to the\nstreamwise direction with time interval [ t\n0,..., tn], and the output is represented by\npredicted high-resolution velocity data at an instant, tn+1, where n is set to 12 in this\nstudy. This process is repeated recursively such that the input data is advanced by one time\nstep at each prediction.\nIn this study, the open-source library TensorFlow 2.4.0 (Abadi et al. 2016)i su s e df o r\nthe implementation of the presented model. The source code of the model is available at\nhttps://ﬂuids.pusan.ac.kr/ﬂuids/65416/subview.do.\n2.1. Transformer\nA LSTM (Hochreiter & Schmidhuber 1997) is an artiﬁcial neural network that can handle\nsequential data and time-series modelling. A LSTM is a type of RNN (Rumelhart, Hinton\n& Williams 1986). It has also played an essential role in modelling the temporal evolution\nof turbulence in various problems (Srinivasan et al. 2019;K i m&L e e 2020; Eivazi\net al. 2021; Nakamura et al. 2021; Yousif & Lim 2022). Although LSTM is designed\nto overcome most of the traditional RNN limitations, such as vanishing gradients and\nexplosion of gradients (Graves 2012), it is usually slow in terms of training due to\nits architecture, which requires that the time-series data be introduced to the network\nsequentially. This prevents parallelisation of the training process, which is why\naG P Ui s\nused in deep learning calculations. Furthermore, LSTM has shown a limitation in dealing\nwith long-range dependencies.\n957 A6-4\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\nMultihead\nattention\nMasked multihead\nSelf-attention\nFeed forward\nPE + PE +\nQKV\nQKV\nQKV\nDense(ReLU)\nLayerNorm.\n+\nt0 tn–3\nEncoder-\nlayer Decoder-\nlayer\nEncoder inputs\nOutputs\nMatMul\nScale\nSoftMax\nMatMul\n+\nQKV\n...Scaled dot\nproduct attention\nLayerNorm.\nDense\n(linear)\nQ K V\nMatMul\nScale\nSoftMax\nMatMul\nMask\nDense\n(linear)\nDense\n(linear)\n+\nLayerNorm.\n...Masked\nScaled dot\nproduct attention\nQKV\nDense(linear)\nDense(linear) Dense(linear)\nDense(linear)\ntn\nDecoder inputs\ntn–3\n. . .tn–2 tn+1\nDense\n(linear)\nQ K V\nDense\n(linear)\nDense\n(linear)\nMultihead\nSelf-attention\nFigure 2. Architecture of the transformer. The dashed line box represents the scaled dot-product attention.\nThe transformer (Vaswani et al.2017) was introduced to deal with these limitations by\napplying the self-attention concept to compute the representations of its input and output\ndata without feeding the data sequentially. In this study, a transformer is used to model the\ntemporal evolution of the velocity ﬁelds that represent the turbulent inﬂow data.\nFigure 2 shows the architecture of the transformer used in this study. Similar to the\noriginal transformer proposed by Vaswani et al. (2017), it has two main components:\nencoder and decoder. The inputs of both components are passed through a positional\nencoder using sine and cosine functions, which can encode the order information of the\ninput data into a vector and add it directly to the input vector. The encoder consists of\nsix stacked encoder layers. Each layer contains a\nmultihead self-attention sublayer and\na feed-forward sublayer. The input of the multihead self-attention sublayer consists of\nqueries ( Q), keys ( K) and values ( V). Note that attention is a function that can map a\nquery and set of key-value pairs to output, where the queries, keys, values and output are\nall vectors. The output can be calculated as the weighted sum of the values. The attention\nfunction is represented by scaled dot-product attention, which is an attention mechanism\nwhere the dot products are scaled down by √\ndk. In addition, dk is the dimension of Q, K\nand it is equal to the dimension of V,i . e .dv.\nThe scaled dot-product attention is calculated simultaneously for Q, K, V by packing\nthem into the matrices Q, K, V:\nAttention (Q, K, V) = softmax\n(\nQKT\n√dk\n)\nV, (2.1)\nwhere softmax is a function that takes an input vector and normalises it to a probability\ndistribution so that the output vector has values that sum to 1 (Goodfellow et al.2014). In\n957 A6-5\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\nthe multihead self-attention sublayer, dv = dmodel/h, where dmodel and h are the dimension\nof the input data to the model and number of heads, respectively.\nThe multihead attention allows the model to jointly attend to information from different\nrepresentation subspaces at different positions such that\nMultihead (Q, K, V) = Concat (head1,..., headh)Wo, (2.2)\nheadi = Attention\n(\nQWQ\ni , KWK\ni , VWV\ni\n)\n, (2.3)\nwhere WQ\ni , WK\ni and WV\ni are the weights corresponding to Q, K, V at every head,\nrespectively; Wo represents the weights of the concatenated heads. WQ\ni ∈ Rdmodel×dk ,\nWK\ni ∈ Rdmodel×dk , WV\ni ∈ Rdmodel×dv and Wo ∈ Rhdv×dmodel .\nThe multihead self-attention sublayer contains six heads of scaled dot-product attention.\nA residual connection (He et al.2016) is applied around the multihead attention, followed\nby layer normalisation (Ba, Kiros & Hinton 2016).\nThe second part of the encoder layer, i.e. the feed-forward sublayer, contains two dense\nlayers with linear and rectiﬁed linear unit (ReLU) activation functions. This layer projects\nthe vector to a larger space, where it is easier to extract the required information and then\nprojects it back to the original space. As in the\nmultihead self-attention sublayer, a residual\nconnection is employed before applying layer normalisation.\nSimilar to the encoder, the decoder contains six decoder layers. In addition to the\nmultihead self-attention and feed-forward sublayers, the decoder layer has a third sublayer\nthat performs multihead attention over the output of the encoder stack. Furthermore, the\nmultihead self-attention sublayer is changed to a masked multihead self-attention sublayer,\nas shown in ﬁgure 2 , which is similar to the multihead self-attention sublayer with the\ndifference that the scaled dot-product attention is changed to a masked scaled dot-product\nattention (Vaswani et al.2017). The masking operation ensures that the prediction can only\ndepend on the known outputs, a fact that prevents later information leakage. In this study,\nthe dropout technique is applied to every sublayer before the residual connection and the\nrate of dropout is set to 0.1.\nThe square of the L\n2 norm error is chosen as a loss function for the transformer such\nthat\nLtransformer = 1\nM\nM∑\nm=1\nOutputm − Targetm\n2\n2 , (2.4)\nwhere Output and Target represent the output from the transformer and ground truth\ndata, respectively, at a speciﬁc time step, m. Here M represents the size of the training\nmini-batch, which is set to 64. The adaptive moment estimation (Adam) optimisation\nalgorithm (Kingma & Ba 2017) is used to update the weights of the model.\n2.2. MS-ESRGAN\nGenerative adversarial networks (Goodfellow et al. 2014) have shown great success in\nimage transformation and super-resolution problems (Mirza & Osindero 2014;L e d i get al.\n2017; Zhu et al. 2017;W a n g et al. 2018). Generative adversarial network-based models\nhave also shown promising results in reconstructing HR turbulent ﬂow ﬁelds from coarse\ndata (Fukami et al. 2019a; Fukami, Fukagata & Taira 2021; Güemes et al. 2021;K i m\net al. 2021; Yousif et al. 2021, 2022b;Y u et al. 2022). In a GAN model that is used\nfor image generation, two adversarial neural networks called the generator ( G)a n dt h e\ndiscriminator ( D) compete with each other. The G tries to generate artiﬁcial images with\n957 A6-6\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\n. . .\n. . .\nConv2D\nConv2D\nDense block\nDense block\nRRDB\nRRDB\nMSP\nCoarse data\nConv2D\nLReLU\nConv2D\nBN\nLReLU\nConv2D\nBN\nLReLU\n. . .\nC(χr) C(χa)\nχa\nχr χa\nRRDB\nConv2D\n×γ\nConv2D\nLReLU\nConv2D\nLReLU\nConv2D\nLReLU\nConv2D\nLReLU\nConv2D\n(a) (b)\n×γ\n×γ\nFigure 3. Architecture of MS-ESRGAN. ( a) The generator, where γ is the residual scaling parameter which\nis set to 0.2 in this study, and ( b) the discriminator.\nthe same statistical properties as those of the real ones, whereas D tries to distinguish the\nartiﬁcial images from the real ones. After successful training, G should be able to generate\nartiﬁcial images that are difﬁcult to distinguish by D. This process can be expressed as a\nmin–max two-player game with a value function V(D, G) such that\nmin\nG\nmax\nD V(D, G) = Eχr∼Pdata(χr)[log D(χr)] + Eξ∼Pξ (ξ)[log(1 − D(G(ξ)))], (2.5)\nwhere χr is the image from the ground truth data (real image) and Pdata(χr) is the real\nimage distribution. Here E represents the operation of calculating the average of all the\ndata in the training mini-batch. In the second right-hand term of ( 2.5), ξ is a random vector\nused as an input to G and D(χr) represents the probability that the image is real and not\ngenerated by the generator. The output from G,i . e .G(ξ), is expected to generate an image\nthat is similar to the real one, such that the value of D(G(ξ)) is close to 1. Meanwhile,\nD(χr) returns a value close to 1, whereas D(G(ξ)) returns a value close to 0. Thus, in the\ntraining process, G is trained in a direction that minimises V(D, G), whereas D is trained\nin a direction that maximises V(D, G).\nIn this study, MS-ESRGAN (Yousif et al. 2021) is used to perform super-resolution\nreconstruction of the velocity ﬁelds predicted by the transformer. The MS-ESRGAN is\nbased on the enhanced super-resolution GAN (ESRGAN) (Wang et al. 2018). Figure 3\nshows the architecture of MS-ESRGAN. As shown in ﬁgure 3 (a), G consists of a deep\nCNN represented by residual in residual dense blocks (RRDBs) and multiscale parts\n(MSP). Note that the input to G is low-resolution data, which are ﬁrst passed through a\nconvolutional layer and then through a series of RRDBs. The MSP, consisting of three\n957 A6-7\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\nparallel convolutional submodels with different kernel sizes, is applied to the data features\nextracted by RRDBs. More details for MSP can be found in Yousif et al. (2021, 2022b).\nThe outputs of the three submodels are summed and passed through a ﬁnal convolutional\nlayer to generate HR artiﬁcial data (χa). Figure 3 (b) shows that the artiﬁcial and real\ndata are fed to D and passed through a series of convolutional, batch normalisation\nand leaky ReLU layers. As a ﬁnal step, the data are crossed over a convolutional\nlayer. The non-transformed discriminator outputs using the real and artiﬁcial data, i.e.\nC(χr) and C(χa), are used to calculate the relativistic average discriminator value DRa\n(Jolicoeur-Martineau 2018):\nDRa(χr, χa) = σ(C\n(\nχr)\n)\n− Eχa\n[\nC(χa)\n]\n, (2.6)\nDRa(χa, χr) = σ(C\n(\nχa)\n)\n− Eχr\n[\nC(χr)\n]\n, (2.7)\nwhere σ is the sigmoid function. In ( 2.6)a n d( 2.7), DRa represents the probability that the\noutput from D using the real image is relatively more realistic than the output using the\nartiﬁcial image.\nThen, the discriminator loss function is deﬁned as follows:\nℓRa\nD =− Eχr\n[\nlog(DRa(χr, χa))\n]\n− Eχa\n[\nlog(1 − DRa(χa, χr))\n]\n. (2.8)\nThe adversarial loss function of the generator can be expressed in a symmetrical form\nas follows:\nℓRa\nG =− Eχr\n[\nlog(1 − DRa(χr, χa))\n]\n− Eχa\n[\nlog(DRa(χa, χr))\n]\n. (2.9)\nThe total loss function of the generator is deﬁned as\nLG = ℓRa\nG + βℓ pixel + ℓperceptual, (2.10)\nwhere ℓpixel is the error calculated based on the pixel difference of the generated and\nground truth data; ℓperceptual represents the difference between features that are extracted\nfrom the real and the artiﬁcial data. The pretrained CNN VGG-19 (Simonyan & Zisserman\n2014) is used to extract the features using the output of three different layers (Yousif et al.\n2021). Here, β is a weight coefﬁcient and its value is set to be 5000. The square of the\nL2 norm error is used to calculate ℓpixel and ℓperceptual. The size of the mini-batch is set to\n32. As in the transformer model, the Adam optimisation algorithm is used to update the\nweights of the model.\n3. Data description and preprocessing\nThe transitional boundary layer database (Lee & Zaki 2018) available at the Johns Hopkins\nturbulence databases (JHTDB) is considered in this study for the training and testing of\nthe DLM. The database was obtained via DNS of incompressible ﬂow over a ﬂat plate\nwith an elliptical leading edge. In the simulation, the half-thickness of the plate L and the\nfree stream velocity U\n∞ are used as a reference length and velocity. In addition, x, y and\nz are deﬁned as the streamwise, wall-normal and spanwise coordinates, respectively, with\nthe corresponding velocity components u, v and w. Note that the same deﬁnitions of the\ncoordinates and velocity components are used in this study.\nIn the simulation, the length of the plate, Lx = 1050L measured from the leading edge\n(x = 0), the domain height, Ly = 40L and the width of the plate, Lz = 240L. The stored\ndatabase in JHTDB is in the range x ∈ [30.2185, 1000.065]L, y ∈ [0.0036, 26.488]L and\nz ∈ [0, 240]L. The corresponding number of grid points is Nx × Ny × Nz = 3320 × 224 ×\n957 A6-8\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\n2048 ≈ 1.5 × 109. The database time step is /Delta1t = 0.25L/U∞. The stored database spans\nthe following range in momentum-thickness-based Reynolds number, Reθ = U∞θ/ν ∈\n[105.5, 1502.0], where θ represents the momentum thickness and ν is the kinematic\nviscosity. More details for the simulation and database can be found in Lee & Zaki ( 2018)\nand on the website of JHTDB.\nIn this study, the datasets within the range of Reθ ∈ [661.5, 1502.0] are considered for\ntraining and testing the DLM. This range of Reθ in the database represents the fully\nturbulent part of the ﬂow (Lee & Zaki 2018). Datasets of the velocity components are\ncollected from various y–z planes along the streamwise direction, with a number of\nsnapshots = 4700 for every plane. To reduce the computational cost, the original size of\neach plane, Ny × Nz = 224 × 2048 is reduced to 112 × 1024. Furthermore, to increase\nthe amount of training and testing data, every selected plane is divided into four identical\nsections ( y–z planes) along the spanwise direction, resulting in Ny × Nz = 112× 256\nfor each section. To obtain the coarse data, the size of the data is further reduced to\nN\ny × Nz = 14 × 32, which is obtained by selecting distributed points in the ﬁelds. The\ndistribution of the points is obtained in a stretching manner such that more points can be\nselected near the wall. A time series of 4000 snapshots for each section are used to train\nthe DLM, resulting in a total number of training snapshots = 4000 × 4 × 3 = 48 000.\nThe ﬂuctuations of the velocity ﬁelds are used in the training and prediction processes.\nThe input data to the DLM are normalised using the\nmin–max normalisation function to\nproduce values between 0 and 1.\n4. Results and discussion\n4.1. Results from the DLM trained at various Reθ\nThis section examines the capability of the proposed DLM to generate turbulent inﬂow\ndata at three different Reynolds numbers for which the network has already been trained,\nReθ = 661.5, 905. 7 and 1362.0. Figures 4–6 show the instantaneous streamwise velocity\n(u+) and vorticity ( ω+\nx ) ﬁelds of the DNS and the predicted data for three different time\nsteps, where the superscript ‘ +’ denotes normalisation by viscous inner scale; in the\nﬁgures, δ represents the boundary layer thickness. The ﬁgures show that the instantaneous\nﬂow ﬁelds can be predicted using the model with a commendable agreement with the DNS\ndata. Note that the model has shown a capacity to predict the instantaneous ﬂow ﬁelds for\na long period of time, more than the one required for the ﬂow data to reach a statistically\nstationary state (reaching ﬁxed ﬁrst and second-order statistics over time), i.e. for a number\nof time steps = 10 000.\nThe shape factor (ratio of displacement to momentum thickness of the TBL) values of\nthe DNS and predicted velocity ﬁelds are shown in table 1 . A slight under-prediction can\nbe seen in all the predicted values with the highest deviation of 2.95 % at Re\nθ = 661.5.\nFigure 7 shows the probability density functions (p.d.f.s) of the velocity components\n(u+, v+ and w+) plotted against the wall-normal distance ( y+). The ﬁgure reveals that\nthe p.d.f. plots of the generated velocity components are in agreement with the p.d.f.\nplots obtained from the DNS data, indicating the capability of the model in predicting\nthe velocity ﬁelds with distributions of the velocity components that are consistent with\nthose of the DNS data.\nFigures 8–10 compare the turbulence statistics of the generated velocity ﬁelds with the\nturbulence statistics of the DNS data. As shown in the ﬁgures, the mean streamwise\nvelocity proﬁle ( U\n+) for all the three Reynolds numbers shows excellent agreement\nwith the results obtained from the DNS. The comparison of root mean square (r.m.s.)\n957 A6-9\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\nDNS y/δ\ny/δ\nz/δ\nDNS y/δ\ny/δ\nDLM\nDLM\nt+ = 23.86 t+ = 95.45 t+ = 286.36\n22.0\n0\n11.0\n16.5\n5.5\n0.2\n–0.2\n0\n0.1\n–0.1\nu+\nωx\n+\n(a)\n(b) t+ = 23.86 t+ = 95.45 t+ = 286.36\n1.0\n0.5\n1.0\n0.5\n1.0\n0.5\n1.0\n0.5\n1.0\n0.5\n1.0\n0.5\n0 1234567\nz/δ\n0 1234567\nz/δ\n0 1234567\n0 1234567 0 1234567 0 1234567\n0 1234567 0 1234567 0 1234567\n0 1234567 0 1234567 0 1234567\n1.0\n0.5\n1.0\n0.5\n1.0\n0.5\n1.0\n0.5\n1.0\n0.5\n1.0\n0.5\nFigure 4. Instantaneous streamwise ( a) velocity and ( b) vorticity ﬁelds at Reθ = 661.5, for three different\ninstants. Reference (DNS) and predicted (DLM) data are shown.\nDNS y/δ\ny/δ\nz/δ z/δ z/δ\nDNS y/δ\ny/δ\nDLM\nDLM\nt+ = 20.77 t+ = 83.06 t+ = 249.19(a)\n(b) t+= 20.77 t+= 83.06 t+= 249.19\n22.0\n1.0\n0.5\n01 2 3 4 5\n1.0\n0.5\n01 2 3 4 5\n1.0\n0.5\n0 12345\n1.0\n0.5\n0 12345\n1.0\n0.5\n0 12345\n1.0\n0.5\n0 12345\n1.0\n0.5\n0 12345\n1.0\n0.5\n01 2 3 4 5\n1.0\n0.5\n01 2 3 4 5\n1.0\n0.5\n0 12345\n1.0\n0.5\n0 12345\n1.0\n0.5\n0 12345\n0\n11.0\n16.5\n5.5\n0.2\n–0.2\n0\n0.1\n–0.1\nu+\nωx\n+\nFigure 5. Instantaneous streamwise ( a) velocity and ( b) vorticity ﬁelds at Reθ = 905.7, for three different\ninstants. Reference (DNS) and predicted (DLM) data are shown.\n957 A6-10\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\nt+ = 18.58 t+ = 74.30 t+ = 222.91\nt+ = 18.58 t+ = 74.30 t+ = 222.91\n(a)\n(b)\nDNS y/δ\ny/δ\nDNS y/δ\ny/δ\nDLM\nDLM\nz/δ\n22.0\n0\n11.0\n16.5\n5.5\n0.2\n–0.2\n0\n0.1\n–0.1\nu+\nωx\n+\n1.0\n0.5\n0 1234\nz/δ\n1.0\n0.5\n01 2 3 4\nz/δ\n1.0\n0.5\n0 1234\n1.0\n0.5\n0 1234\n1.0\n0.5\n01 2 3 4\n1.0\n0.5\n0 1234\n1.0\n0.5\n0 1234\n1.0\n0.5\n01 2 3 4\n1.0\n0.5\n0 1234\n1.0\n0.5\n0 1234\n1.0\n0.5\n01 2 3 4\n1.0\n0.5\n0 1234\nFigure 6. Instantaneous streamwise ( a) velocity and ( b) vorticity ﬁelds at Reθ = 1362.0, for three different\ninstants. Reference (DNS) and predicted (DLM) data are shown.\nReθ 661.5 905 .7 1362 .0\nDNS 1.458 1.457 1.453\nDLM 1 .415 1 .423 1 .429\nTable 1. Shape factor values of the reference (DNS) and predicted (DLM) data.\nproﬁles of the velocity components ( u+\nrms, v+\nrms and w+\nrms) reveals good agreement with\nthe DNS results. However, the proﬁle of the Reynolds shear stress ( u′v′+\n) shows a slight\nunder-prediction in the region between near the wall and the maximum Reynolds shear\nstress, and the proﬁle values in this region improve as the Reynolds number increases.\nThis might be attributed to the fact that with the increase in the boundary-layer thickness,\nthe effect of zero padding in the convolution processes is decreased in MS-ESRGAN,\nresulting in a better prediction of the velocity ﬁelds in this region of the boundary layer.\nThese results are consistent with the results from table 1.\nThe capability of the proposed DLM to produce realistic spatial spectra of the velocity\nﬁelds is investigated by employing the premultiplied spanwise wavenumber spectrum,\nk\nzΦαα, where Φαα represents the spanwise wavenumber spectrum, α represents the\nvelocity component and kz is the spanwise wavenumber. Figure 11 shows the contour\nplots of k+\nz Φ+\nαα as a function of y+ and the spanwise wavelength, λ+\nz . The ﬁgure shows\nthat the spectra of the velocity components are generally consistent with those obtained\nfrom the DNS data with a slight deviation at the high wavenumbers. This indicates that\nthe two-point correlations of the generated velocity components are consistent with those\nobtained from the DNS data, further supporting the excellent performance of the proposed\n957 A6-11\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\np.d.f.(u)\ny+ y+ y+\nu+ v+\n(a)( b)( c)\n(d )( e)( f )\n(g)( h)( i)\np.d.f.(v) p.d.f.( w)\nu+ v+ w+\nw+\nw+u+ v+\n20\n15\n10\n5\n0\n20\n15\n10\n5\n0\n20\n15\n10\n5\n0\n4\n2\n0\n–2\n–4\n4\n2\n0\n–2\n–4\n4\n2\n0\n–2\n–4\n4\n2\n0\n–2\n–4\n4\n2\n0\n–2\n–4\n4\n2\n0\n–2\n–4\n10\n0 101 102 100 101 102 100 101 102\n100 101 102 100 101 102 100 101 102\n100 101 102 100 101 102 100 101 102\nFigure 7. Probability density functions of the velocity components as a function of the wall-normal distance.\nThe shaded contours represent the results from the DNS data and the dashed ones represent the results from\nthe predicted data. The contour levels are in the range of 20 %–80 % of the maximum p.d.f. with an increment\nof 20 %\n:( a–c) Reθ = 661.5;( d–f ) Reθ = 905.7;( g–i) Reθ = 1362.0.\n(a)\nU+\ny+\n(b)\ny+ y+\n(c)\n–u′v′——+\nDNS\nDLM\nDNS\nDLM DNS3.5\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n10\n0 101 102 103100 101 102 103100 101 102 103\n25\n20\n15\n10\n5\n0\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0\nDLM\nw\n+\nrmsu+\nrms v+\nrms\nw+\nrms\nu+\nrms\nv+\nrms\nFigure 8. Turbulence statistics of the ﬂow at Reθ = 661.5:( a) mean streamwise velocity proﬁle ;( b) r.m.s.\nproﬁles of the velocity components ;( c) Reynolds shear stress proﬁle.\nDLM to properly represent the spatial distribution of the velocity ﬁelds. It is worth noting\nthat the ability of the model to reproduce accurate spectra is essential in generating\nthe turbulent inﬂow conditions to guarantee that the turbulence will be sustained after\nintroducing the synthetic-inﬂow conditions; otherwise, the generated inﬂow would require\nvery long distances to reach ‘well-behaved’ turbulent conditions, and these ﬂuctuations\ncould also dissipate.\nTo evaluate the performance of the proposed DLM to generate the velocity ﬁelds with\naccurate dynamics, the frequency spectrum, φ\n+\nαα, as a function of y+ and the frequency,\n957 A6-12\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\n(a)\nU+\ny+\n(b)\ny+ y+\n(c)\n–u′v′——+\nDNS\nDLM\nDNS\nDLM DNS3.5\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n10\n0 101 102 103100 101 102 103100 101 102 103\n25\n20\n15\n10\n5\n0\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0\nDLM\nw\n+\nrmsu+\nrms v+\nrms\nw+\nrms\nu+\nrms\nv+\nrms\nFigure 9. Turbulence statistics of the ﬂow at Reθ = 905.7:( a) mean streamwise velocity proﬁle ;( b) r.m.s.\nproﬁles of the velocity components ;( c) Reynolds shear stress proﬁle.\n(a)\nU+\ny+\n(b)\ny+ y+\n(c)\n–u′v′——+\nDNS\nDLM\nDNS\nDLM DNS3.5\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n10\n0 101 102 103100 101 102 103100 101 102 103\n25\n20\n15\n10\n5\n0\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0\nDLM\nw\n+\nrmsu+\nrms v+\nrms\nw+\nrms\nu+\nrms\nv+\nrms\nFigure 10. Turbulence statistics of the ﬂow at Reθ = 1362.0:( a) mean streamwise velocity proﬁle ;( b) r.m.s.\nproﬁles of the velocity components ;( c) Reynolds shear stress proﬁle.\nf + is represented in ﬁgure 12 . Note that the spectra obtained from the generated velocity\nﬁelds show a commendable agreement with those of the DNS data, indicating that the\nproposed DLM can produce turbulent inﬂow conditions with a temporal evolution of the\nvelocity ﬁelds that is consistent with that of the DNS.\n4.2. Interpolation and extrapolation capability of the DLM\nThis section investigates the performance of the proposed DLM to generate turbulent\ninﬂow conditions at Reynolds numbers that are not used in the training process. The\nvelocity ﬁelds at Re\nθ = 763.8 and 1155.1 are used as examples of the velocity ﬁelds\nthat fall between the Reynolds numbers used in the training process, i.e. the interpolation\nability of the model is investigated using the ﬂow ﬁelds at these Reynolds numbers.\nFigure 13 shows the instantaneous streamwise velocity and vorticity ﬁelds for the ﬂow\nat Reθ = 763.8. It is worth noting that the transformer trained for the ﬂow at the nearest\nReθ,i . e . Reθ = 661.5 is used to predict the temporal evolution of the velocity ﬁelds. The\nﬁgure shows that the main features of the ﬂow ﬁelds can be obtained with relatively good\nprecision; however, the details of the predicted velocity ﬂuctuations are not clearly shown.\nSimilar results can be observed in ﬁgure 14 for the predicted velocity ﬁelds at Reθ =\n1155.1. Here, the current transformers trained for the ﬂow at Reθ = 905.7 and 1362.0 are\nused to predict the temporal evolution of the velocity ﬁelds.\nThe turbulence statistics of the ﬂow at Reθ = 763.8 and 1155.1 are shown in ﬁgures 15\nand 16, respectively. Although the mean streamwise velocity and the r.m.s. proﬁles of the\nspanwise and wall-normal velocity components show an ability of the DLM to predict\nreasonably well, the r.m.s. proﬁle of the streamwise velocity component and the Reynolds\n957 A6-13\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\ny+\nk+\nz Φ+\nuu k+\nz Φ+\nwwk+\nz Φ+\nvv\n102\n101\n102\n101\n102\n101\n102\n101\n102\n101\n102\n101\n102\n101\n102\n101\n102\n101\n102 103 102 103 102 103\n102 103 102 103 102 103\n102 103 102 103 102 103\nλz\n+\n(a)( b)( c)\n(d )( e)( f )\n(g)( h)( i)\ny+\ny+\nλz\n+ λz\n+\nFigure 11. Premultiplied spanwise wavenumber energy spectra of the velocity components as a function of the\nwall-normal distance and the spanwise wavelength. The shaded contours represent the results from the DNS\ndata and the dashed ones represent the results from the predicted data. The contour levels are in the range\nof 10 %–90 % of the maximum k\n+\nz Φ+\nαα with an increment of 10 % :( a–c) Reθ = 661.5;( d–f ) Reθ = 905.7;\n(g–i) Reθ = 1362.0.\nshear stress show an under-prediction due to the lack of detailed information on the\nvelocity ﬂuctuations.\nThe extrapolation ability of the DLM is evaluated using the ﬂow ﬁelds at Reθ = 1502.0,\nwhich is higher than the maximum Reθ used to train the transformer and MS-ESRGAN,\ni.e. Reθ = 1362.0. The transformer trained for the ﬂow at Reθ = 1362.0i su s e dt op r e d i c t\nthe dynamics of the velocity ﬁelds. Figure 17 shows that the generated instantaneous\nstreamwise velocity and vorticity ﬁelds generally have similar accuracy to the interpolated\nﬂow ﬁelds. Meanwhile, the turbulence statistics show a deviation from the DNS statistics,\nas shown in ﬁgure 18 . This can be attributed to the lack of details of the velocity\nﬂuctuations and the extrapolation process that relies on the ﬂow information at one\nReynolds number compared with the interpolation process where the ﬂow falls within\nthe range of the Reynolds numbers that the MS-ESRGAN is trained for.\n957 A6-14\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\nf +f +f +\nϕ+\nuu ϕ+\nwwϕ+\nvv\ny+\n102\n101\n102\n101\n102\n101\n102\n101\n102\n101\n102\n101\n102\n101\n102\n101\n102\n101\n10–2 10–1 10–2 10–1 10–2 10–1\n10–2 10–1 10–2 10–1 10–2 10–1\n10–2 10–1 10–2 10–1 10–2 10–1\n(a)( b)( c)\n(d )( e)( f )\n(g)( h)( i)\ny+\ny+\nFigure 12. Frequency spectra of the velocity components as a function of the wall-normal distance and the\nfrequency. The shaded contours represent the results from the DNS data and the dashed ones represent the\nresults from the predicted data. The contour levels are in the range of 10 %–90 % of the maximum φ\n+\nαα with an\nincrement of 10 % :( a–c) Reθ = 661.5;( d–f ) Reθ = 905.7;( g–i) Reθ = 1362.0.\nFinally, the accuracy of the spectral content of the interpolated and extrapolated\nvelocity components is examined in ﬁgure 19 by employing the premultiplied spanwise\nwavenumber spectrum. These results indicate that the spectra are produced with relatively\ngood accuracy for the\nlow–moderate wavenumbers.\n4.3. Error analysis, transfer learning and computational cost\nThe performance of the proposed DLM is further statistically investigated using the L2\nnorm error of the predicted data for all the Reynolds numbers used in this study,\nε = 1\nJ\nJ∑\nj=1\n\nα\nDNS\nj − αDLM\nj\n\n\n2\n\nα\nDNS\nj\n\n\n2\n, (4.1)\n957 A6-15\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\nt+ = 21.51 t+ = 86.06 t+ = 258.17\nt+ = 21.51 t+ = 86.06 t+ = 258.17(a)\n(b)\nDNS y/δ\ny/δ\nDNS y/δ\ny/δ\nDLM\nDLM\nz/δ\n22.0\n0\n11.0\n16.5\n5.5\n0.2\n–0.2\n0\n0.1\n–0.1\nu+\nωx\n+\n1.0\n0.5\n01 2 4 36 51 2 4 36 51 2 4 36 5\n12 4 36 51 2 4 36 51 2 4 36 5\n12 4 36 51 2 4 36 51 2 4 36 5\n12 4 36 51 2 4 36 51 2 4 36 5\nz/δ\n1.0\n0.5\n0\nz/δ\n1.0\n0.5\n0\n1.0\n0.5\n0\n1.0\n0.5\n0\n1.0\n0.5\n0\n1.0\n0.5\n0\n1.0\n0.5\n0\n1.0\n0.5\n0\n1.0\n0.5\n0\n1.0\n0.5\n0\n1.0\n0.5\n0\nFigure 13. Instantaneous streamwise ( a) velocity and ( b) vorticity ﬁelds at Reθ = 763.8, for three different\ninstants. Reference (DNS) and predicted (DLM) data are shown.\nwhere αDNS\nj and αDLM\nj represent the ground truth (DNS) and the predicted velocity\ncomponents using the DLM, respectively, and J represents the number of the test\nsnapshots.\nFigure 20 shows that as the Reynolds number increases, no signiﬁcant differences can\nbe seen in the error values of the predicted velocity ﬁelds. However, as expected, the error\nshows higher values for the interpolated and extrapolated velocity ﬁelds compared with\nthe error of the predicted velocity ﬁelds at the Reynolds numbers that the DLM is trained\nfor. Additionally, in contrast with the aforementioned statistical results, the error values\nare relatively high for the wall-normal and spanwise velocity ﬁelds. This indicates that\nthe DLM has learned to model the structure of the ﬂow with generally accurate turbulence\nstatistics and\nspatiotemporal correlations, rather than reproducing the time sequence of the\nﬂow data. This observation is consistent with the results obtained by Fukami et al.(2019b),\nKim & Lee ( 2020) and Yousif et al.(2022a). Furthermore, using input data of size 7 × 16\nin the training of the DLM shows a slight reduction in the model performance, indicating\nthe capability of the DLM to generate the turbulent inﬂow data even if it is trained with\nextremely coarse input data.\nIt is worth mentioning that the transfer learning (TL) technique is used in this study\n(Guastoni et al. 2021; Yousif et al. 2021, 2022a). The weights of the transformer are\nsequentially transferred for every training y–z plane in the ﬂow. First, the transformer is\ntrained for the ﬂow at the lowest Reynolds number, i.e. Reθ = 661.5. After that, the weights\nof the model are transferred for the training using the next Reθ data and so on. The results\nfrom using TL in this study show that with the use of only 25 % of the training data for\nthe transformer model, the computational cost (represented by the training time) can be\nreduced by 52 % without affecting the prediction accuracy. These results are consistent\nwith the results obtained by Guastoni et al.(2021) and Yousif et al.(2021, 2022a).\n957 A6-16\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\nDNS\nCase 1\n22.0\n0\n11.0\n16.5\n5.5\nu+\n(a)\nCase 2\nDNS y/δ\nCase 1\n0.2\n–0.2\n0\n0.1\n–0.1\nω+\nx\n(b)\nCase 2\nt+ = 19.32 t+ = 77.26 t+ = 231.79\nt+ = 19.32 t+ = 77.26 t+ = 231.79\n0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234\ny/δ\n0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234\ny/δ\n0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234\ny/δ\n0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234\ny/δ\n0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234\ny/δ\n0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234 0\n0.5\n1.0\n1234\nz/δ z/δ z/δ\nFigure 14. Instantaneous streamwise ( a) velocity and ( b) vorticity ﬁelds at Reθ = 1155.1, for three different\ninstants. Cases 1 and 2 represent the prediction using the transformer that is trained for the ﬂow at Reθ = 905.7\nand 1362.0, respectively.\n——\n101\ny+\nU+\n100 102 103\n5\n0\n10\n15\n20\n25 DNS\nDLM\n101\ny+\n100 102 103\n0.5\n0\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n101\ny+\n100 102 103\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nDNS\nu+\nrms w+\nrms\nv+\nrms\nDLM\nDNS\nDLM\n–u′v′+\n(a)( b)( c)\nw+\nrms\nu+\nrms\nv+\nrms\nFigure 15. Turbulence statistics of the ﬂow at Reθ = 763.8:( a) mean streamwise velocity proﬁle ;( b) r.m.s.\nproﬁles of the velocity components ;( c) Reynolds shear stress proﬁle.\n957 A6-17\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\n——\n101\ny+\nU+\n100 102 103\n5\n0\n10\n15\n20\n25 DNS\nCase 1\nCase 2\n101\ny+\n100 102 103\n0.5\n0\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n101\ny+\n100 102 103\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nDNS u+\nrms w+\nrms\nv+\nrms\nCase 1Case 2\nDNS\nCase 1\nCase 2\n–u′v′+\n(a)( b) (c)\nw+\nrms\nu+\nrms\nv+\nrms\nFigure 16. Turbulence statistics of the ﬂow at Reθ = 1155.1. Cases 1 and 2 represent the prediction using the\ntransformer model trained for the ﬂow at Reθ = 905.7 and 1362.0, respectively. ( a) Mean streamwise velocity\nproﬁle;( b) r.m.s. proﬁles of the velocity components ;( c) Reynolds shear stress proﬁle.\n.\\DNS_X3320_u_200.png .\\DNS_X3320_u_600.pngDNS y/δ\nDLM\nt+ = 18.12 t+ = 72.49 t+ = 217.48\nt+ = 18.12 t+ = 72.49 t+ = 217.48\nDNS y/δ\nDLM\nz/δ\n22.0\n0\n11.0\n16.5\n5.5\nu+\n0.2\n–0.2\n0\n0.1\n–0.1\nω+\nx\n(a)\n(b)\n0\n0.5\n1.0\n123\ny/δ\n0\n0.5\n1.0\n123\nz/δ\n0\n0.5\n1.0\n123\n0\n0.5\n1.0\n123\nz/δ\n0\n0.5\n1.0\n123\n0\n0.5\n1.0\n123\n0\n0.5\n1.0\n123\ny/δ\n0\n0.5\n1.0\n123\n0\n0.5\n1.0\n123\n0\n0.5\n1.0\n123\n0\n0.5\n1.0\n123\n0\n0.5\n1.0\n123\nFigure 17. Instantaneous streamwise ( a) velocity and ( b) vorticity ﬁelds at Reθ = 1502.0 for three different\ninstants. Reference (DNS) and predicted (DLM) data are shown.\n——\n101\ny+\nU+\n100 102 103\n5\n0\n10\n15\n20\n25 DNS\nDLM\n101\ny+\n100 102 103\n0.5\n0\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n10\n1\ny+\n100 102 103\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nDNS\nu\n+\nrms w+\nrms\nv+\nrms\nDLM\nDNS\nDLM\n–u′v′+\n(a)( b)( c)\nw+\nrms\nu+\nrms\nv+\nrms\nFigure 18. Turbulence statistics of the ﬂow at Reθ = 1502.0. (a) mean streamwise velocity proﬁle ;( b) r.m.s.\nproﬁles of the velocity components ;( c) Reynolds shear stress proﬁle.\n957 A6-18\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\ny+\nk+\nzΦ+\nuu\nk+\nzΦ+\nvv\nk+\nzΦ+\nww\nλ+\nz λ+\nz\nλ+\nz\ny+\ny+\n101\n102 103\n102\n101\n102 103\n102\n101\n102 103\n102\n101\n102 103\n102\n101\n102 103\n102\n101\n102 103\n102\n101\n102 103\n102\n101\n102 103\n102\n101\n102 103\n102\n(a)( b)( c)\n(d )( e)( f )\n(g)( h)( i)\nFigure 19. Premultiplied spanwise wavenumber energy spectra of the velocity components as a function of the\nwall distance and wavelength. The shaded contours represent the results from the DNS data; the dashed-black\ncontours represent the results from the predicted velocity data at Re\nθ = 763.8 and 1502.0; the dashed-brown\nand grey contours represent the results from the velocity data at Reθ = 1155.1 predicted using the transformer\nmodel trained for the ﬂow at Reθ = 905.7 and 1362.0, respectively. The contour levels are in the range of\n10 %–90 % of the maximum k+\nz Φ+\nαα with an increment of 10 % :( a–c) Reθ = 763.8;( d–f ) Reθ = 1155.1;\n(g–i) Reθ = 1502.0.\nThe total number of trainable parameters of the DLM is 356.5 × 106 (305.5 × 106 for\nthe transformer and 51 × 106 for the MS-ESRGAN). The training of the transformer model\nfor all the three Reynolds numbers used in this study using a single NVIDIA TITAN RTX\nGPU with the aid of TL requires approximately 23 hours. Meanwhile, the training of the\nMS-ESRGAN requires approximately 32 hours. Thus, the total training time of the DLM\nmodel is 55 hours, indicating that the computational cost of the model is relatively lower\nthan the cost of the DNS that is required to generate the velocity ﬁelds. Furthermore,\nthis computational cost is required only once, i.e. for the training of the model. Since\nthe prediction process is computationally inexpensive and does not need any data for\n957 A6-19\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\nε(u) ε(v) ε(w)\n(a)\nReθ\n(c)(b)\n7×16\n14×32\n7×16\n14×32\n7×16\n14×32\nCase 2\nCase 1\nCase 2\nCase 1\nCase 2\nCase 1\n0 0.04 0.08 0.12 0.16 0.20\n661.5\n763.8\n905.7\n1155.1\n1155.1\n1362.0\n1502.0\n0 0.4 0.8 1.2 1.6 2.0\n661.5\n763.8\n905.7\n1155.1\n1155.1\n1362.0\n1502.0\n0 0.4 0.8 1.2 1.6 2.0\n661.5\n763.8\n905.7\n1155.1\n1155.1\n1362.0\n1502.0\nFigure 20. The L2 norm error of the predicted velocity ﬁelds :( a) streamwise velocity ;( b) wall-normal\nvelocity;( c) spanwise velocity. Cases 1 and 2 represent the results from the velocity data at Reθ = 1155.1\npredicted using the transformer model trained for the ﬂow at Reθ = 905.7 and 1362.0, respectively.\nthe prediction (except the initial instantaneous ﬁelds), the DLM can also be considered\nefﬁcient in terms of storing and transferring the inﬂow data.\n4.4. Simulation of spatially developing TBL using the turbulent inﬂow data\nIn order to examine the feasibility of applying the DLM-based turbulent inﬂow conditions,\nthe generated data are utilised to perform an inﬂow–outﬂow large-eddy simulation (LES)\nof ﬂat plate TBL spanning Reθ = 1362–1820. The open-source computational ﬂuid\ndynamics ﬁnite-volume code OpenFOAM-5.0x is used to perform the simulation. The\ndimensions of the computational domain are 20 δ0,1 . 8 δ0 and 4 δ0 in the streamwise,\nwall-normal and spanwise directions, respectively, where δ0 represents the boundary\nlayer thickness at the inlet section of the domain. The corresponding grid size = 320 ×\n90 × 150. The grid points have a uniform distribution in the streamwise and spanwise\ndirections while local grid reﬁnement is applied near the wall using the stretching grid\ntechnique in the wall-normal direction. The spatial spacing at the midpoint of the domain\nis /Delta1x+ ≈ 15.4, /Delta1y+\nwall ≈ 0.2a n d /Delta1z+ ≈ 6.5, where y+\nwall represents the spatial spacing\nin the wall-normal direction near the wall. A no-slip boundary condition is applied to the\nwall, while periodic boundary conditions are applied to the spanwise direction. A slip\nboundary condition is assigned to the top of the domain, whereas an advection boundary\ncondition is applied to the outlet of the domain. The pressure implicit split\noperator\nalgorithm is employed to solve the coupled pressure momentum system. The dynamic\nSmagorinsky model (Germano et al. 1991) is applied for the subgrid-scale modelling.\nAll the discretisation schemes used in the simulation have second-order accuracy. The\ngenerated inﬂow data are linearly interpolated in time to have a simulation time step\n/Delta1t = 0.0017δ\n0/U∞ yielding a maximum Courant number of 0.8. The statistics from the\nsimulation are accumulated over a period of 620 δ0/U∞ after an initial run with a period\nof 60 δ0/U∞ or 3 ﬂow through.\nThe formation of the instantaneous vortical structures of the ﬂow is visualised\nby utilising the Q-criterion vortex identiﬁcation method (Hunt, Wray & Moin 1988)\nin ﬁgure 21 . Smooth development of the coherent structures represented by the\nhairpin-vortex-like structures (Adrian 2007) can be observed from the ﬁgure with\nno noticeable formation of artiﬁcial turbulence at the inlet section of the domain.\n957 A6-20\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\nx/δ0\nz/δ0\ny/δ0\n0 0\n0\n1\n1\n2\n3\n4\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n0.50\n0.75\n0\n1.00\n0.25\nu/U∞\nFigure 21. Isosurfaces of instantaneous vortical structures ( Q-criterion= 0.54U2\n∞/δ2\n0)f r o mt h e\ninﬂow–outﬂow simulation coloured by the streamwise velocity.\n(b)\n–u′v′—— +\ny/δ\n(a)\nU+\ny+\nDLM\nDNS\nDLM\nDNS\n0\n5\n10\n15\n20\n25\n0\n0 0.2 0.4 0.6 0.8 1.0 1.2\n0.2\n0.4\n0.6\n0.8\n10\n0 101 102 103\nFigure 22. Turbulence statistics from the inﬂow–outﬂow simulation at Reθ = 1400 compared with the DNS\nresults:( a) mean streamwise velocity proﬁle ;( b) Reynolds shear stress proﬁle.\nThis indicates that the inﬂow data obtained from the DLM could represent most of the\nﬂow physics at the inlet section, resulting in a negligible developing distance upstream of\nthe domain.\nA comparison of the mean streamwise velocity and Reynolds shear stress proﬁles at\nReθ = 1400 with the DNS results are provided in ﬁgure 22. The mean streamwise velocity\nproﬁle is in excellent agreement with the DNS results. Furthermore, the Reynolds shear\nstress proﬁle is consistent with the DNS results in most of the boundary layer regions.\nTo further evaluate the accuracy of the inﬂow conditions, statistics obtained from the\nsimulation are compared with the inﬂow–outﬂow LES results of Lund et al. (1998)a n d\nDNS results of Spalart ( 1988). Figure 23 shows the proﬁles of the mean streamwise\nvelocity and Reynolds shear stress proﬁles at Re\nθ = 1530. An agreement can be observed\nwith Lund et al. (1998) results of the modiﬁed Spalart ( recycling–rescaling) method and\nthe results of Spalart ( 1988)( Reθ = 1410) in the inner region of the boundary layer,\nhowever, a deviation can be observed in the outer region. This might be attributed to\nthe fact that the original DNS data that are used to train the DLM contain free stream\nturbulence (Lee & Zaki 2018).\nThe evolution of the shape factor H is shown in ﬁgure 24 (a). Here the result from the\nsimulation is generally consistent with the DNS and Spalart ( 1988) results, and within 5 %\nof the modiﬁed Spalart method from Lund et al. (1998). The result of the skin-friction\ncoefﬁcient ( Cf )i n ﬁgure 24 (b) shows an agreement with the results from Lund et al.\n(1998) and Spalart ( 1988) with an over-prediction of approximately 8 % compared with the\n957 A6-21\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\n(b)\ny/δ\n(a)\nU+\ny+\nDLMModified spalart method\nParallel-flow (recycling) method  (Lund et al. 1998) (Spalart 1988), Reθ = 1410\n–u′v′—— +\n0\n5\n10\n15\n20\n25\n0\n0 0.2 0.4 0.6 0.8 1.0 1.2\n0.2\n0.4\n0.6\n0.8\n1.0\n10\n0 101 102 103\nFigure 23. Turbulence statistics from the inﬂow–outﬂow simulation at Reθ = 1530 compared with the results\nof Lund et al. (1998) and Spalart ( 1988):( a) mean streamwise velocity proﬁle ;( b) Reynolds shear stress\nproﬁle.\nDLMModified Spalart method\nParallel-flow (recycling) method  (Lund et al. 1998) DNS\n(Spalart 1988)\nCf\n(b)(a)\nH\n1.30\n1400 1500 1600 1700 1800\nReθ Reθ\n1.35\n1.40\n1.45\n1.50\n1.55\n0.0025\n1400 1500 1600 1700 1800\n0.0030\n0.0035\n0.0040\n0.0045\n0.0050\nFigure 24. Evolution of the shape factor and skin-friction coefﬁcient in the inﬂow–outﬂow simulation\ncompared with the results of the DNS (Lund et al. 1998;S p a l a r t1988):( a) shape factor ;( b) skin-friction\ncoefﬁcient.\nDNS results. The change in the shape factor slope and the over-prediction of the\nskin-friction coefﬁcient compared with the DNS result can be attributed to the numerical\nset-up of the inﬂow–outﬂow simulation. Note that in the work of Lund et al. (1998), the\ninﬂow data were generated from precursor simulations that have the same y–z plane size\nas the inlet section of the inﬂow-outﬂow simulations, and no spatial or time interpolation\nw a sa p p l i e dt ot h ei n ﬂ o wd a t a .\nThe above results suggest that the turbulent inﬂow data that are generated by the\nproposed DLM can be practically used as inﬂow conditions for simulations that do not\nnecessarily have the same spatial and time resolutions as the generated data, which is the\ncase in the simulation described in this section.\n5. Conclusions\nThis study proposed a deep-learning-based method to generate turbulent inﬂow\nconditions for spatially developing TBL simulations. A combination of a transformer and\nMS-ESRGAN was used to build the inﬂow generator. The transformer was trained to\nmodel the temporal evolution of the velocity ﬁelds represented by various ( y–z)p l a n e so f\nspatially limited data. Meanwhile, MS-ESRGAN was trained to perform super-resolution\nreconstruction of the predicted velocity ﬁelds.\n957 A6-22\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\nThe generated instantaneous velocity ﬁelds showed an excellent agreement with the\nDNS results for the velocity ﬁelds at Reynolds numbers that the DLM was trained for. The\nmodel also successfully reproduced the turbulence statistics with commendable accuracy.\nFurthermore, the model reproduced the spectra of the velocity components with accurate\nprecision, indicating accurate spatial and temporal correlations of the generated velocity\ncomponents, which further supports the ability of the model to maintain the realistic\nbehaviour of the velocity ﬁelds.\nThe performance of the proposed model was further examined using velocity ﬁelds\nat Reynolds numbers that were not used in the training process. The instantaneous and\nstatistical results showed a reasonable accuracy for the interpolated and extrapolated\nvelocity ﬁelds. The spectra of the velocity components revealed a relatively good\nagreement with the results from the actual velocity data, with a deviation that can be\nobserved at high wavenumbers. These results suggest that the model can generate the\nturbulent inﬂow conditions for the ﬂow at Reynolds numbers that are not necessarily used\nin the training of the model.\nThe results obtained from the error analysis showed that the increase in the Reynolds\nnumber has no signiﬁcant effect on the error values of the predicted velocity ﬁelds,\nindicating that the model is robust to the increase of the Reynolds number. The use of\nTL in the training of the transformer revealed a noticeable reduction in the computational\ncost of the DLM without affecting the precision of the prediction.\nThe\ninﬂow–outﬂow simulation results showed the feasibility of applying the generated\nturbulent inﬂow conditions to turbulent ﬂow simulations as a negligible developing\ndistance upstream of the domain is required for the TBL to reach the target statistics.\nThis study showed for the ﬁrst time that a transformer-based model could be effectively\nused for modelling the dynamics of turbulent ﬂows with the ability to perform parallel\ncomputing during the training process, which is not possible in LSTM-based models.\nIt also paves the way for utilising synthetic-inﬂow generators for large-scale turbulence\nsimulations using deep learning, with signiﬁcant promise in terms of computational\nsavings.\nFunding. This work was supported by the ‘Human Resources Program in Energy Technology’ of the Korea\nInstitute of Energy Technology Evaluation and Planning (KETEP) and granted ﬁnancial resources from the\nMinistry of Trade, Industry & Energy, Republic of Korea (no. 20214000000140). In addition, this work was\nsupported by the National Research Foundation of Korea (NRF) with a grant funded by the Korean government\n(MSIP) (no. 2019R1I1A3A01058576). This work was also supported by the National Supercomputing Center\nwith supercomputing resources including technical support (KSC-2021-CRE-0244). R.V. acknowledges the\nﬁnancial support from the ERC grant no. ‘2021-CoG-101043998, DEEPCONTROL’.\nDeclaration of interests.The authors report no conﬂict of interest.\nAuthor ORCIDs.\nMustafa Z. Yousif https://orcid.org/0000-0002-5542-5474;\nMeng Zhang https://orcid.org/0000-0003-2393-5215;\nLinqi Yu https://orcid.org/0000-0002-5674-6261;\nRicardo Vinuesa https://orcid.org/0000-0001-6570-5499;\nHeeChang Lim https://orcid.org/0000-0001-8504-0797.\nREFERENCES\nABADI ,M . , et al. 2016 Tensorﬂow: large-scale machine learning on heterogeneous distributed systems.\narXiv:1603.04467.\nADRIAN , R.J. 2007 Hairpin vortex organization in wall turbulence. Phys. Fluids19, 041301.\nBA, J.L., K IROS ,J . R .&H INTON , G.E. 2016 Layer normalization. arXiv:1607.06450.\n957 A6-23\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\nBRUNTON , S.L., N OACK,B . R .&K OUMOUTSAKOS , P. 2020 Machine learning for ﬂuid mechanics. Annu.\nRev. Fluid Mech.52 (1), 477–508.\nDENG , Z., H E,C . ,L IU,Y .&K IM, K.C. 2019 Super-resolution reconstruction of turbulent velocity ﬁelds\nusing a generative adversarial network-based artiﬁcial intelligence framework. Phys. Fluids31 (12), 125111.\nDRUAULT,P . ,L ARDEAU ,S . ,B ONNET , J.P., C OIFFET ,F . ,D ELVILLE ,J . ,L AMBALLAIS , E., L ARGEAU ,J . F .\n&P ERRET , L. 2004 Generation of three-dimensional turbulent inlet conditions for large-eddy simulation.\nAIAA J.42 (3), 447–456.\nDURAISAMY ,K . ,I ACCARINO ,G .&X IAO, H. 2019 Turbulence modeling in the age of data. Annu. Rev. Fluid\nMech. 51 (1), 357–377.\nEIVAZI ,H . ,C LAINCHE , S.L., H OYAS,S .&V INUESA , R. 2022 Towards extraction of orthogonal and\nparsimonious non-linear modes from turbulent ﬂows. Expert Syst. Appl.202, 117038.\nEIVAZI ,H . ,G UASTONI , L., S CHLATTER ,P . ,A ZIZPOUR ,H .&V INUESA , R. 2021 Recurrent neural networks\nand koopman-based frameworks for temporal predictions in a low-order model of turbulence. Intl J. Heat\nFluid Flow90, 108816.\nFAN,D . ,Y ANG , L., W ANG , Z., T RIANTAFYLLOU ,M . S .&K ARNIADAKIS , G.E. 2020 Reinforcement\nlearning for bluff body active ﬂow control in experiments and simulations. Proc. Natl Acad. Sci. USA\n117 (42), 26091–26098.\nFUKAMI ,K . ,F UKAGATA ,K .&T AIRA ,K .2 0 1 9 a Super-resolution reconstruction of turbulent ﬂows with\nmachine learning. J. Fluid Mech.870, 106–120.\nFUKAMI ,K . ,F UKAGATA ,K .&T AIRA , K. 2021 Machine-learning-based spatio-temporal super resolution\nreconstruction of turbulent ﬂows. J. Fluid Mech.909,A 9 .\nFUKAMI ,K . ,N ABAE ,Y . ,K AWAI,K .&F UKAGATA ,K .2 0 1 9 b Synthetic turbulent inﬂow generator using\nmachine learning. Phys. Rev. Fluids4 (6), 064603.\nGERMANO ,M . ,P IOMELLI ,U . ,M OIN ,P .&C ABOT , W.H. 1991 A dynamic subgrid-scale eddy viscosity\nmodel. Phys. FluidsA 3, 1760–1765.\nGOODFELLOW ,I . ,P OUGET -ABADIE ,J . ,M IRZA ,M . ,X U,B . ,W ARDE -FARLEY ,D . ,O ZAIR ,S . ,\nCOURVILLE ,A .&B ENGIO , Y. 2014 Generative adversarial nets. In Advances in Neural Information\nProcessing Systems 27 (NIPS 2014) (ed. Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence &\nK.Q. Weinberger), pp. 2672–2680. NeurIPS.\nGRA VES,A .2 0 1 2 Long Short-Term Memory, pp. 37–45. MIT.\nGUASTONI , L., G UEMES ,A . ,I ANIRO ,A . ,D ISCETTI ,S . ,S CHLATTER ,P . ,A ZIZPOUR ,H .&V INUESA ,R .\n2021 Convolutional-network models to predict wall-bounded turbulence from wall quantities. J. Fluid\nMech. 928,A 2 7 .\nGÜEMES ,A . ,D ISCETTI ,S . ,I ANIRO ,A . ,S IRMACEK ,B . ,A ZIZPOUR ,H .&V INUESA , R. 2021 From coarse\nwall measurements to turbulent velocity ﬁelds through deep learning. Phys. Fluids33 (7), 075121.\nHE,K . ,Z HANG ,X . ,R EN,S .&S UN, J. 2016 Deep residual learning for image recognition. In IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, pp. 770–778. IEEE.\nHOCHREITER ,S .&S CHMIDHUBER , J. 1997 Long short-term memory. Neural Comput.9 (8), 1735–1780.\nHUNT, J.C.R., W RAY,A . A .&M OIN , P. 1988 Eddies, streams, and convergence zones in turbulent ﬂows. in\nProceedings of the Summer Program 1988, pp. 193–208. Center for Turbulence Research.\nJARRIN ,N . ,B ENHAMADOUCHE ,S . ,L AURENCE ,D .&P ROSSER , R. 2006 A synthetic-eddy-method for\ngenerating inﬂow conditions for large-eddy simulations. Intl J. Heat Fluid Flow27 (4), 585–593.\nJIMÉNEZ ,J . ,H OYAS,S . ,S IMENS ,M . P .&M IZUNO , Y. 2010 Turbulent boundary layers and channels at\nmoderate reynolds numbers. J. Fluid Mech.657, 335–360.\nJOHANSSON ,P . S .&A NDERSSON , H.I. 2004 Generation of inﬂow data for inhomogeneous turbulence. Theor.\nComput. Fluid Dyn.18 (5), 371–389.\nJOLICOEUR -MARTINEAU , A. 2018 The relativistic discriminator: a key element missing from standard GAN.\narXiv:1807.00734.\nKIM,H . ,K IM,J . ,W ON,S .&L EE, C. 2021 Unsupervised deep learning for super-resolution reconstruction\nof turbulence. J. Fluid Mech.910, A29.\nKIM,J .&L EE, C. 2020 Deep unsupervised learning of turbulence for inﬂow generation at various Reynolds\nnumbers. J. Comput. Phys.406, 109216.\nKINGMA ,D . P .&B A, J. 2017 ADAM: a method for stochastic optimization. arXiv:1412.6980.\nKLEIN ,M . ,S ADIKI ,A .&J ANICKA , J. 2003 A digital ﬁlter based generation of inﬂow data for spatially\ndeveloping direct numerical or large eddy simulations. J. Comput. Phys.186 (2), 652–665.\nKUTZ , J.N. 2017 Deep learning in ﬂuid dynamics. J. Fluid Mech.814, 1–4.\nLE,H . ,M OIN ,P .&K IM, J. 1997 Direct numerical simulation of turbulent ﬂow over a backward-facing step.\nJ. Fluid Mech.330, 349–374.\nLECUN,Y . ,B ENGIO ,Y .&H INTON , G. 2015 Deep learning. Nature 521 (7553), 436–444.\n957 A6-24\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nA transformer-based turbulent-inﬂow generator\nLEDIG ,C . ,T HEIS , L., H USZAR ,F . ,C ABALLERO ,J . ,C UNNINGHAM ,A . ,A COSTA ,A . ,A ITKEN ,A . ,\nTEJANI ,A . ,T OTZ ,J .&S HI, Z.W. 2017 Photo-realistic single image super-resolution using a generative\nadversarial network. arXiv:1609.04802.\nLEE,J .&Z AKI , T.A. 2018 Detection algorithm for turbulent interfaces and large-scale structures in\nintermittent ﬂows. Comput. Fluids175, 142–158.\nLEE,S .&Y OU, D. 2019 Data-driven prediction of unsteady ﬂow over a circular cylinder using deep learning.\nJ. Fluid Mech.879, 217–254.\nLUND ,T . ,W U,X .&S QUIRES , K. 1998 Generation of turbulent inﬂow data for spatially-developing boundary\nlayer simulations. J. Comput. Phys.140 (2), 233–258.\nLUND , T.S. 1993 Large eddy simulation of a boundary layer with concave streamwise curvature. In Annual\nResearch Briefs 1993, pp. 91–99. Center for Turbulence Research.\nMATHEY ,F . ,C OKLJAT ,D . ,B ERTOGLIO ,J . P .&S ERGENT , E. 2006 Assessment of the vortex method for\nlarge eddy simulation inlet conditions. Prog. Comput. Fluid Dyn.6 (1–3), 58–67.\nMIRZA ,M .&O SINDERO , S. 2014 Conditional generative adversarial nets. arXiv:1411.1784v1.\nNAKAMURA ,T . ,F UKAMI ,K . ,H ASEGAWA ,K . ,N ABAE ,Y .&F UKAGATA , K. 2021 Convolutional neural\nnetwork and long short-term memory based reduced order surrogate for minimal turbulent channel ﬂow.\nPhys. Fluids33 (2), 025116.\nP\nARK ,J .&C HOI , H. 2020 Machine-learning-based feedback control for drag reduction in a turbulent channel\nﬂow. J. Fluid Mech.904, A24.\nPERRET , L., D ELVILLE ,J . ,M ANCEAU ,R .&B ONNET , J.P. 2008 Turbulent inﬂow conditions for large-eddy\nsimulation based on low-order empirical model. Phys. Fluids20 (7), 75–107.\nRABAULT,J . ,K UCHTA ,M . ,J ENSEN ,A . ,R EGLADE ,U .&C ERARDI , N. 2019 Artiﬁcial neural networks\ntrained through deep reinforcement learning discover control strategies for active ﬂow control. J. Fluid\nMech. 865, 281–302.\nRUMELHART , D.E., H INTON , G.E. & W ILLIAMS , R.J. 1986 Learning representations by back-propagating\nerrors. Nature 323 (6088), 533–536.\nSANMIGUEL VILA ,C . ,V INUESA ,R . ,D ISCETTI ,S . ,I ANIRO ,A . ,S CHLATTER ,P .&O RLU,R .2 0 1 7O nt h e\nidentiﬁcation of well-behaved turbulent boundary layers. J. Fluid Mech.822, 109–138.\nSCHLATTER ,P .&Ö RLÜ , R. 2012 Turbulent boundary layers at moderate reynolds numbers: inﬂow length\nand tripping effects. J. Fluid Mech.710, 5–34.\nSERGENT , E. 2002 Vers une méthodologie de couplage entre la simulation des grandes échelles et les modèles\nstatistiques. PhD thesis, Ecole centrale de Lyon.\nSIMONYAN ,K .&Z ISSERMAN , A. 2014 Very deep convolutional networks for large-scale image recognition.\narXiv:1409.1556.\nSPALART , P.R. 1988 Direct simulation of a turbulent boundary layer up to Reθ = 1410.J. Fluid Mech.187,\n61–98.\nSPILLE -KOHOFF ,A .&K ALTENBACH , H.J. 2001 Generation of turbulent inﬂow data with a prescribed\nshear-stress proﬁle. In DNS/LES Progress and Challenges. Proceedings of the Third AFOSR International\nConference on DNS/LES, pp. 319–326. University of Texas at Arlington.\nSRINIVASAN , P.A., G UASTONI , L., A ZIZPOUR ,H . ,S CHLATTER ,P .&V INUESA , R. 2019 Predictions of\nturbulent shear ﬂows using deep neural networks. Phys. Rev. Fluids4 (5), 054603.\nVASWANI ,A . ,S HAZEER ,N . ,P ARMAR ,N . ,U SZKOREIT ,J . ,J ONES , L., G OMEZ , A.N., K AISER ,L .&\nPOLOSUKHIN , I. 2017 Attention is all you need. In Advances in Neural Information Processing Systems\n30 (NIPS 2017)(ed. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan & R.\nGarnett), pp. 5998–6008. NeurIPS.\nVINUESA ,R .&B RUNTON , S.L. 2022 Enhancing computational ﬂuid dynamics with machine learning. Nat.\nComput. Sci.2, 358–366.\nVINUESA ,R . ,L EHMKUHL ,O . ,L OZANO -DURAN ,A .&R ABAULT, J. 2022 Flow control in wings and\ndiscovery of novel approaches via deep reinforcement learning. Fluids 7 (2), 62.\nWANG ,J . - X . ,W U, J.-L. & X IAO, H. 2017 A physics informed machine learning approach for reconstructing\nreynolds stress modeling discrepancies based on dns data. Phys. Rev. Fluids2 (3), 034603.\nWANG ,X . ,Y U,K . ,W U,S . ,G U,J . ,L IU,Y . ,D ONG ,C . ,L OY, C.C., Q IAO,Y .&T ANG , X. 2018 ESRGAN:\nenhanced super-resolution generative adversarial networks. arXiv:1809.00219.\nWU, X. 2017 Inﬂow turbulence generation methods. Annu. Rev. Fluid Mech.49 (1), 23–49.\nYOUSIF , M.Z. & L IM, H. 2021 Improved delayed detached-eddy simulation and proper orthogonal\ndecomposition analysis of turbulent wake behind a wall-mounted square cylinder. AIP Adv.11, 045011.\nYOUSIF , M.Z. & L IM, H. 2022 Reduced-order modeling for turbulent wake of a ﬁnite wall-mounted square\ncylinder based on artiﬁcial neural network. Phys. Fluids34 (1), 015116.\n957 A6-25\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press\nM.Z. Yousif, M. Zhang, L. Yu, R. Vinuesa and H.C. Lim\nYOUSIF , M.Z., Y U,L .&L IM, H. 2021 High-ﬁdelity reconstruction of turbulent ﬂow from spatially limited\ndata using enhanced super-resolution generative adversarial network. Phys. Fluids33 (12), 125119.\nYOUSIF , M.Z., Y U,L .&L IM, H. 2022 a Physics-guided deep learning for generating turbulent inﬂow\nconditions. J. Fluid Mech.936,A 2 1 .\nYOUSIF , M.Z., Y U,L .&L IM, H. 2022 b Super-resolution reconstruction of turbulent ﬂow ﬁelds at various\nReynolds numbers based on generative adversarial networks. Phys. Fluids34 (1), 015130.\nYU, L., Y OUSIF , M.Z., Z HANG , M. Hoyas, S., Vinuesa, R. & Lim, H. 2022 Three-dimensional ESRGAN for\nsuper-resolution reconstruction of turbulent ﬂows with tricubic interpolation-based transfer learning. Phys.\nFluids 34 (12), 125126.\nZHU,J . - Y . ,P ARK ,T . ,I SOLA ,P .&E FROS , A.A. 2017 Unpaired image-to-image translation using\ncycle-consistent adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV),\nVenice, Italy, pp. 2242–2251. IEEE\n957 A6-26\nhttps://doi.org/10.1017/jfm.2022.1088\n Published online by Cambridge University Press",
  "topic": "Inflow",
  "concepts": [
    {
      "name": "Inflow",
      "score": 0.763344407081604
    },
    {
      "name": "Turbulence",
      "score": 0.6943386793136597
    },
    {
      "name": "Computer science",
      "score": 0.5370126962661743
    },
    {
      "name": "Reynolds number",
      "score": 0.5110529661178589
    },
    {
      "name": "Mechanics",
      "score": 0.49005210399627686
    },
    {
      "name": "Boundary layer",
      "score": 0.44745180010795593
    },
    {
      "name": "Physics",
      "score": 0.39014723896980286
    }
  ]
}