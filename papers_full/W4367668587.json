{
    "title": "PathologyBERT - Pre-trained Vs. A New Transformer Language Model for Pathology Domain.",
    "url": "https://openalex.org/W4367668587",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2126060361",
            "name": "Thiago Santos",
            "affiliations": [
                "Emory University"
            ]
        },
        {
            "id": "https://openalex.org/A2231377352",
            "name": "Amara Tariq",
            "affiliations": [
                "Mayo Clinic Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2103249276",
            "name": "Susmita Das",
            "affiliations": [
                "Indian Institute of Technology Kharagpur"
            ]
        },
        {
            "id": "https://openalex.org/A4367863393",
            "name": "Kavyasree Vayalpati",
            "affiliations": [
                "Arizona State University"
            ]
        },
        {
            "id": "https://openalex.org/A2240591377",
            "name": "Geoffrey H. Smith",
            "affiliations": [
                "Emory University"
            ]
        },
        {
            "id": "https://openalex.org/A2156339742",
            "name": "Hari Trivedi",
            "affiliations": [
                "Emory University"
            ]
        },
        {
            "id": "https://openalex.org/A2122079672",
            "name": "Imon Banerjee",
            "affiliations": [
                "Mayo Clinic Hospital",
                "Arizona State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2805185296",
        "https://openalex.org/W2754809768",
        "https://openalex.org/W2892695393",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W2911489562"
    ],
    "abstract": "Pathology text mining is a challenging task given the reporting variability and constant new findings in cancer sub-type definitions. However, successful text mining of a large pathology database can play a critical role to advance 'big data' cancer research like similarity-based treatment selection, case identification, prognostication, surveillance, clinical trial screening, risk stratification, and many others. While there is a growing interest in developing language models for more specific clinical domains, no pathology-specific language space exist to support the rapid data-mining development in pathology space. In literature, a few approaches fine-tuned general transformer models on specialized corpora while maintaining the original tokenizer, but in fields requiring specialized terminology, these models often fail to perform adequately. We propose PathologyBERT - a pre-trained masked language model which was trained on 347,173 histopathology specimen reports and publicly released in the Huggingface<sup>1</sup> repository<sup>2</sup>. Our comprehensive experiments demonstrate that pre-training of transformer model on pathology corpora yields performance improvements on Natural Language Understanding (NLU) and Breast Cancer Diagnose Classification when compared to nonspecific language models.",
    "full_text": null
}