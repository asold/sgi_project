{
  "title": "PathologyBERT - Pre-trained Vs. A New Transformer Language Model for Pathology Domain.",
  "url": "https://openalex.org/W4367668587",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2126060361",
      "name": "Thiago Santos",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2231377352",
      "name": "Amara Tariq",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2103249276",
      "name": "Susmita Das",
      "affiliations": [
        "Indian Institute of Technology Kharagpur"
      ]
    },
    {
      "id": "https://openalex.org/A4367863393",
      "name": "Kavyasree Vayalpati",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A2240591377",
      "name": "Geoffrey H. Smith",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2156339742",
      "name": "Hari Trivedi",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2122079672",
      "name": "Imon Banerjee",
      "affiliations": [
        "Mayo Clinic Hospital",
        "Arizona State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2805185296",
    "https://openalex.org/W2754809768",
    "https://openalex.org/W2892695393",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2911489562"
  ],
  "abstract": "Pathology text mining is a challenging task given the reporting variability and constant new findings in cancer sub-type definitions. However, successful text mining of a large pathology database can play a critical role to advance 'big data' cancer research like similarity-based treatment selection, case identification, prognostication, surveillance, clinical trial screening, risk stratification, and many others. While there is a growing interest in developing language models for more specific clinical domains, no pathology-specific language space exist to support the rapid data-mining development in pathology space. In literature, a few approaches fine-tuned general transformer models on specialized corpora while maintaining the original tokenizer, but in fields requiring specialized terminology, these models often fail to perform adequately. We propose PathologyBERT - a pre-trained masked language model which was trained on 347,173 histopathology specimen reports and publicly released in the Huggingface<sup>1</sup> repository<sup>2</sup>. Our comprehensive experiments demonstrate that pre-training of transformer model on pathology corpora yields performance improvements on Natural Language Understanding (NLU) and Breast Cancer Diagnose Classification when compared to nonspecific language models.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7554035782814026
    },
    {
      "name": "Terminology",
      "score": 0.7151516675949097
    },
    {
      "name": "SNOMED CT",
      "score": 0.6206678748130798
    },
    {
      "name": "Natural language processing",
      "score": 0.5472476482391357
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5039348006248474
    },
    {
      "name": "Transformer",
      "score": 0.4616038203239441
    },
    {
      "name": "Language model",
      "score": 0.4417400360107422
    },
    {
      "name": "Risk stratification",
      "score": 0.42269617319107056
    },
    {
      "name": "Pathology",
      "score": 0.40143823623657227
    },
    {
      "name": "Medicine",
      "score": 0.25762590765953064
    },
    {
      "name": "Linguistics",
      "score": 0.12433856725692749
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Cardiology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}