{
  "title": "Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change",
  "url": "https://openalex.org/W4385573949",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2907323499",
      "name": "Zhaochen Su",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2990396704",
      "name": "Zecheng Tang",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2488618492",
      "name": "Xinyan Guan",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A214709701",
      "name": "Lijun Wu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2005452123",
      "name": "Min Zhang",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2126452540",
      "name": "Juntao Li",
      "affiliations": [
        "Soochow University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2573889668",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2990949918",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2177801600",
    "https://openalex.org/W1570098300",
    "https://openalex.org/W2738734060",
    "https://openalex.org/W2073459066",
    "https://openalex.org/W2399378566",
    "https://openalex.org/W2146950091",
    "https://openalex.org/W2894850180",
    "https://openalex.org/W2949573100",
    "https://openalex.org/W2132469061",
    "https://openalex.org/W250892164",
    "https://openalex.org/W3083515246",
    "https://openalex.org/W3173537960",
    "https://openalex.org/W152717974",
    "https://openalex.org/W2158108973",
    "https://openalex.org/W2163302275",
    "https://openalex.org/W3153269634",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2133286915",
    "https://openalex.org/W3198975419",
    "https://openalex.org/W1987971958",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2798837230",
    "https://openalex.org/W2804830075",
    "https://openalex.org/W2790109590",
    "https://openalex.org/W3035067238",
    "https://openalex.org/W4280652569",
    "https://openalex.org/W3211920512",
    "https://openalex.org/W2094236250",
    "https://openalex.org/W4200057797",
    "https://openalex.org/W2931922640",
    "https://openalex.org/W2963780471",
    "https://openalex.org/W3213460052",
    "https://openalex.org/W1662133657",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3021480884",
    "https://openalex.org/W2102343050",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3171460770"
  ],
  "abstract": "Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., language model pre-trained on static data from past years performs worse over time on emerging data. Existing methods mainly perform continual training to mitigate such a misalignment. While effective to some extent but is far from being addressed on both the language modeling and downstream tasks. In this paper, we empirically observe that temporal generalization is closely affiliated with lexical semantic change, which is one of the essential phenomena of natural languages. Based on this observation, we propose a simple yet effective lexical-level masking strategy to post-train a converged language model. Experiments on two pre-trained language models, two different classification tasks, and four benchmark datasets demonstrate the effectiveness of our proposed method over existing temporal adaptation methods, i.e., continual training with new data. Our code is available at https://github.com/zhaochen0110/LMLM.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6380–6393\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nImproving Temporal Generalization of Pre-trained Language Models with\nLexical Semantic Change\nZhaochen Su1, Zecheng Tang1∗, Xinyan Guan1, Juntao Li1†, Lijun Wu2, Min Zhang1\n1Institute of Computer Science and Technology, Soochow University, China\n2Microsoft Research Asia\n{suzhaochen0110,zctang2000,guanxy0406}@gmail.com;\n{ljt,minzhang}@suda.edu.cn; lijuwu@microsoft.com\nAbstract\nRecent research has revealed that neural lan-\nguage models at scale suffer from poor tem-\nporal generalization capability, i.e., language\nmodel pre-trained on static data from past years\nperforms worse over time on emerging data.\nExisting methods mainly perform continual\ntraining to mitigate such a misalignment. While\neffective to some extent but is far from be-\ning addressed on both the language model-\ning and downstream tasks. In this paper, we\nempirically observe that temporal generaliza-\ntion is closely affiliated with lexical semantic\nchange, which is one of the essential phenom-\nena of natural languages. Based on this ob-\nservation, we propose a simple yet effective\nlexical-level masking strategy to post-train a\nconverged language model. Experiments on\ntwo pre-trained language models, two differ-\nent classification tasks, and four benchmark\ndatasets demonstrate the effectiveness of our\nproposed method over existing temporal adap-\ntation methods, i.e., continual training with\nnew data. Our code is available at https:\n//github.com/zhaochen0110/LMLM.\n1 Introduction\nNeural language models (LMs) are one of the fron-\ntier research fields of deep learning. With the ex-\nplosion of model parameters and data scale, these\nlanguage models demonstrate superior generaliza-\ntion capability, which can enhance many down-\nstream tasks even under the few-shot and zero-\nshot settings (Radford et al., 2018, 2019; Brown\net al., 2020; Zhang and Li, 2021). Although these\nmodels have achieved remarkable success, they are\ntrapped by the time-agnostic setting in which the\nmodel is trained and tested on data with signifi-\ncant time overlap. However, real-world applica-\ntions usually adopt language models pre-trained\non past data (e.g., BERT (Devlin et al., 2019)\n∗Zhaochen Su and Zecheng Tang contribute equally.\n†Juntao Li is the Corresponding Author.\nFigure 1: Examples of lexical semantic change across\ntime, where the semantic of word church is stable as\nit often refers to a building or a local congregation for\nChristian religious activities. However, the semantic of\nword consistent varies dramatically at different times.\nand RoBERTa (Liu et al., 2019)) to enhance the\ndownstream task-specific models for future data,\nresulting in a temporal misalignment (Luu et al.,\n2021). Recent works have empirically demon-\nstrated that such a misalignment hurts the per-\nformance of both the upstream language models\nand downstream task-specific methods (Lazaridou\net al., 2021; Röttger and Pierrehumbert, 2021).\nTo better understand and solve the temporal\nmisalignment problem, a series of studies have\nbeen launched on pre-trained language models\n(PLMs) and downstream tasks. The analysis on\nPLMs (Lazaridou et al., 2021) revealed that PLMs\n(even with larger model sizes) encounter a seri-\nous temporal generalization problem, and the mis-\nalignment degree increases with time. They also\nfound that continually pre-training PLMs with up-\nto-the-minute data does mitigate the temporal mis-\nalignment problem but suffers from catastrophic\nforgetting and massive computational cost since\nfurther pre-training the converged PLMs is as dif-\nficult as pre-training from scratch. The study on\ndownstream tasks further indicates that temporal\nadaptation (i.e., continually pre-training with unla-\nbelled data that is mostly overlapped in time), while\neffective, has no apparent advantages over domain\nadaptation (Röttger and Pierrehumbert, 2021) (i.e.,\ncontinually pre-training with domain-specific un-\n6380\nlabelled data) and fine-tuning on task-specific data\nfrom the target time (Luu et al., 2021).\nTo analyze the reason behind the limited per-\nformance of temporal adaptation, we launch a\nstudy from the lexical level, which also matches\nthe token-level masking operation in advanced\nPLMs. Unlike existing research that launches\nanalysis on part-of-speech (POS), topic words,\nand newly emerging words, we mainly explore\nthe correlations between language model perfor-\nmance and tokens/words with salient lexical se-\nmantic change, which is also an extensively studied\nconcept in computational linguistics (Dubossarsky\net al., 2015; Hamilton et al., 2016; Giulianelli\net al., 2020) to investigate how the semantics of\nwords change over time. Experimental results\ndemonstrate that tokens/words with salient lexi-\ncal semantic change do contribute much more than\nthe rest of tokens/words to the temporal misalign-\nment problem, manifested as their significantly\nhigher perplexity ( ppl.) over randomly sampled\ntokens from the target time. However, the widely-\nadopted masked language model (MLM) objec-\ntive in state-of-the-art PLMs uniformly deals with\neach token/word, letting the salient lexical-level\nsemantic change information over time being over-\nwhelmed by other tokens/words, which can also\nexplain why temporal adaptation has no obvious\nadvantage compared with domain adaptation.\nBased on the above findings, we propose a\nlexical-based masked Language Model (LMLM)\nobjective to capture the lexical semantic change\nbetween different temporal splits. Experimental re-\nsults demonstrate that our proposed method yields\nsignificant performance improvement over domain\nadaptation methods on two different PLMs and four\nbenchmark datasets. Extensive studies also show\nthat LMLM is effective when utilizing different\nlexical semantic change metrics.\nIn a nutshell, our contributions are shown below:\n• We empirically study the temporal misalignment\nof PLMs at the lexical level and reveal that the to-\nkens/words with salient lexical semantic change\ncontribute much more to the misalignment prob-\nlem than other tokens/words. We also disclose\nthat such lexical temporal misalignment informa-\ntion can be overwhelmed by the masked language\nmodel training objective of PLMs, resulting in\nlimited performance improvement over temporal\nand domain adaptation methods.\n• We propose a simple yet effective Lexical-based\nMasked Language Model (LMLM) objective to\nimprove the temporal generalization of PLMs.\n• Experiments on two PLMs and four different\nbenchmark datasets confirm that our proposed\nmethod is extensively effective in addressing the\ntemporal misalignment problem for downstream\ntasks, which can significantly outperform exist-\ning temporal and domain adaptation methods.\n2 Linking Temporal Misalignment with\nLexical Semantic Change\nRecent work on temporal adaptation (Röttger and\nPierrehumbert, 2021) has found that post-tuning\nthe converged PLMs with unlabeled time-specific\ndata by reusing the MLM objective can make the\nPLMs perceive related event-driven changes in lan-\nguage usage. Such adaptation can achieve decent\nperformance because the widely-adopted MLM ob-\njective can capture the overall changes in the data\ndistribution by randomly masking a specific ratio\nof the whole sequence. However, such a training\nobjective makes the lexical-level temporal informa-\ntion ignored or overwhelmed by the time-agnostic\ntokens/words, resulting in little to no performance\nsuperiority over domain adaptation methods. Based\non the above background, it is natural to explore the\nrole of lexical-level temporal information in tem-\nporal adaptation, i.e., whether these tokens/words\nwith salient lexical-semantic changes 1 over time\nimpair the temporal adaptation performance. As\na result, we launch a thorough study from the per-\nspective of lexical semantic change to figure out\nthe reason behind the limited performance of tem-\nporal adaptation in the specific domain. To the best\nof our knowledge, this is the first study that ex-\nplores the correlation between the lexical-semantic\nchange and the temporal adaptation of PLMs. We\nwill firstly illustrate our methods to find those se-\nmantic changed words in Section 2.1 and introduce\nthe discovery experiment as well as analyze the\nresults in Section 2.2.\n2.1 Lexical Semantic Change Detection\nTo obtain the semantic changed words, we de-\nsign a lexical semantic change detection process.\nFor better illustration, we decompose the process\n1The concept of semantic change is also essential in com-\nputational linguistics (Gulordava and Baroni, 2011; Bamler\nand Mandt, 2017; Rosenfeld and Erk, 2018; Del Tredici et al.,\n2018; Giulianelli et al., 2020).\n6381\nFigure 2: The pipeline of detecting semantic change words (①∼③) and the Lexical-based Masked Language Model\n(LMLM) objective (④). In step ④, the words colored with red are randomly sampled and the words with salient\nsemantic change are colored with green. ALL colored words/tokens are masked during the pre-training stage.\ninto three steps: candidate words selection, fea-\nture extraction & Clustering, and semantic change\nquantification, which are correspond with the\nstep ①∼③ in Figure 2.\nCandidate Words Selection Before obtaining\nthe representation of each word, we sample\na certain number of candidate words Wt =\n{wt\n1,wt\n2,··· ,wt\nk}from the textsDt of time t. Con-\nsidering that different texts have different domains\n(politics, culture, history), most keyword extraction\nmethods either heavily rely on dictionaries and a\nfussy training process (Witten et al., 2005; Rose\net al., 2010) or are too simple to handle such intri-\ncate domain changes, i.e., TF-IDF (Ramos et al.,\n2003). Instead, we turn to YAKE! (Campos et al.,\n2018), a feature-based and unsupervised system to\nextract keywords in one document. Since the goal\nis to measure the lexical semantic change among\ndifferent time splits, we further filter the Wt by\ncalculating the number of the candidate words in\ndifferent periods and removing the words that are\nrepetitive, too few, or have no real meanings, e.g.,\npronouns, particles, mood words, etc.\nFeature Extraction and Clustering Given a\nword wi and one text dt\ni = (t1,··· ,ti,··· ,tn),\nwhere dt\ni ∈ Dt and ti = wi, we utilize a pre-\ntrained language model BERT (Devlin et al., 2019)\nto contextualise each text as the representation rt\ni.\nSpecifically, we look up the sentences in Dt which\ncontain the same candidate words in Wt and feed\nthem into BERT to extract the corresponding word\nrepresentations followed by aggregating them to-\ngether (Giulianelli et al., 2020). It is worth noting\nthat we extract the representations from the last\nlayer of the BERT model in all experiments, but\nwe also consider extracting the features from the\nshallow layers of the BERT model. More details\ncan be referred to in Appendix E.\nTo prevent too much information brought by the\nlong sequences overwhelming the meaning of the\ncandidate words, we specify 128 as the size of\noccurrence window around the word wi, i.e., trun-\ncating the redundant part of each sentence. After\nobtaining N usage representations for each word,\nwe combine them together as representation matrix\nRt\ni = (rt\n1,rt\n2,··· ,rt\nN ) and normalise it.\nTo distinguish the different semantic representa-\ntions of each word, we utilize the K-Means algo-\nrithm, which can automatically cluster the similar\nword usage type into K groups after p turns ac-\ncording to the representation matrix of each word.\nDetails about the K-Means algorithm is elaborated\nin Appendix A. After clustering, we count the num-\nber of sentences in each cluster and calculate the\nfrequency distribution for the candidate word wi.\nWhen normalized, the frequency distribution can\nbe viewed as the probability distribution pt\ni over\nusage types for the candidate word wi at the time\nt. To meet our temporal settings, we should get\nthe probability distributions for the same candidate\nword in different periods for comparison.\nSemantic Change Quantification To measure\nthe difference between the probability distributions\npt\ni and pt′\ni of the same candidate words in different\nperiods over word usages, we utilize the Jensen-\nShannon divergence (Lin, 1991) metric:\nJSD(pt\ni,pt′\ni ) =H\n[1\n2(pt\ni + pt′\ni )\n]\n−1\n2\n[\nH(pt\ni) −H(pt′\ni )\n]\n,\n(1)\nwhere H is the Boltzmann-Gibbs-Shannon en-\ntropy (Ochs, 1976). High JSD represents the differ-\nent frequency distributions, i.e., significant lexical\nsemantic change of the word ti, and visa versa. We\n6382\nutilize a hyper-parameter kto control the degree of\nthe lexical semantic change. Specifically, we rank\nthe candidate words according to their JSD values\nand sample the top-kwords as the salient seman-\ntic changed words. Several other metrics can also\nquantify the lexical semantic change, e.g., Entropy\nDifference (ED) (Nardone, 2014) and Average pair-\nwise distance (APD) (Bohonak, 2002), and we will\ncompare the performance among them below.\n2.2 Discovery Experiment & Analysis\nTo highlight the influence of the salient seman-\ntic changed words, we design a special masked\nlanguage modeling objective LMLM, which first\nmasks the candidate words Wt in the texts. Details\nof the LMLM objective are elaborated in section 3.\nAll the experiments in this section are conducted\nwith the ARXIV dataset2, which contains the ab-\nstracts of five subjects in different periods, e.g.,\nCS, Math, etc. We apply the pre-trained BERT-\nbase model3 which has been pre-trained on a large\ncorpus and evaluate it with the latter-released test-\ning sets4 by reporting the Perplexity (ppl.) value.\nAll the above data are tokenized with Moses5, and\nnon-English documents are removed.\nInfluence of the Semantic Changed Tokens For\ncomparison, we introduce four masking strategies:\nrandom masking, frequency masking, importance\nmasking, and LMLM. The masking ratio of the\nstrategies above is 15%. The random masking strat-\negy, as mentioned above, masks the tokens in the\ntexts randomly, while the frequency masking strat-\negy masks the tokens according to the lexical oc-\ncurrence frequency, and the importance masking\nstrategy masks the tokes according to the YAKE!\nscores. Details of the masking strategies are illus-\ntrated in Appendix B. The results are shown in the\nfigure 3(a). We can observe that the ppl. of the\nLMLM (blue dotted curve) is much higher than the\nothers, which indicates that it is hard for the PLM\nto predict the lexical semantic changed tokens. The\nrising trend of four curves shows that the PLM per-\nforms increasingly worse when predicting future\nutterances further away from their training period.\n2https://arxiv.org/help/oa/index\n3https://github.com/google-research/\nbert\n4The BERT model is pre-trained with the data in 2015,\nwhile the three testing sets are after 2017.\n5https://github.com/alvations/\nsacremoses\n2017 2019 2021\nyear\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50PPL\nrandom\nlmlm\nfrequency\nimportance\n(a)\n2017 2019 2021\nyear\n26\n28\n30\n32\n34\n36\n38\n40PPL\nJSD\nED\nAPD (b)\nFigure 3: Results of the ppl. value. Figure (a) and (b)\nshows the effect of the semantic changed words and the\nresults of different quantification metrics respectively.\nInfluence of the Quantification Metric Further-\nmore, We utilize three current popular metrics:\nJensen-Shannon divergence (JSD), Entropy Differ-\nence (ED), and Average Pairwise Distance (APD)\nto measure the semantic change. The results are\nshown in the figure 3(b). Since the slope of the red\ncurve (JSD) is much higher than the others, which\nmeans the candidate words selected with the JSD\nmetric are hard to predict, i.e., the semantic change\nphenomenon of those words is more significant, we\napply the JSD metric in the later experiments to\nfind the candidate words Wt.\n3 LMLM Objective\nMasked Language Model (MLM) objective is a\nwidely-adopted unsupervised task proposed by\nBERT (Devlin et al., 2019), which randomly masks\nproportional tokens and predicts them. Since the\ndegradation of the PLM in a specific domain over\ntime is mainly attributed to the words with salient\nsemantic change, we should make the PLM more\naware of them. Thus, we propose our Lexical-\nbased Masked Language Model (LMLM) objec-\ntive. Contrary to the traditional random masking\nstrategy (MLM), LMLM preferentially masks the\nwords with salient semantic change over time. For-\nmally, given the text setDt = {dt\n1,dt\n2,··· ,dt\nn}at\ntime t, we select the candidate words Wt with\nthe aforementioned detection method and rank\nthem according to their JSD value. Then, we se-\nlect k (k ∈{100,200,··· ,1000}) words which\nhave relative high scores as the masking candidates\nWt\nmask. Given masking ratio α, LMLM firstly se-\nlects the words in the Wt\nmask to mask. If there\nare not enough candidates to meet the total num-\nber of masking tokens, LMLM masks the other\n6383\nDataset Usage Time #Sentences\nARXIV Fine. 2007∼2019 † 160,000\nPre. 2021 3,800,000\nPoliAff Fine. 2015, 2016 10,000\nPre. ‡ 2017 1,000,000\nRTC\nFine. Apr. 2017 20,000\nPre. Apr. 2018 2,000,000\nPre. Aug. 2019 2,000,000\nTable 1: Statistics of the datasets, where the time splits\nof the ARIXV fine-tuning data (marked with †) is on\na four-year cycle, and the pre-training data of PoliAff\ndataset (marked with ‡) is WMT17.\nwords in the text randomly. The whole process is\ncorresponding to the step ④ in Figure 2. Assum-\ning it masks m tokens in total and the sentence\nafter masking is dt′\ni . The optimization objective of\nLMLM can be formulated as:\nLLMLM = −\nm∑\nj=1\nlog P(x= wj|dt′\ni ; θ). (2)\n4 Experiments\nWe conduct experiments on the classification task\nby employing the pre-trained BERT model im-\nplemented with the Hugging-Face transformers\npackage6 in all experiments. Further details about\nmodel training and parameters can be found in Ap-\npendix C. We will introduce the datasets and the\ntime-stratified settings in Section 4.1, the baselines\nin Section 4.2, and show the results in Section 4.3.\n4.1 Basic Settings\nDatasets To ensure the PLM is trained with the\ndata in the specific domain, we select the data with\nthe same or similar distributions between the up-\nstream and downstream stages. We choose the\nARXIV dataset for the scientific domain and Red-\ndit Time Corpus (RTC) dataset 7 for the political\ndomain. We also turn to two different datasets\nwith a similar distribution for pre-training and fine-\ntuning, respectively. Specifically, we select WMT\nNews Crawl (WMT)8 dataset, which contains news\ncovering various topics, e.g., finance, politics, etc,\nas unlabeled data and PoliAff 9 dataset in politic\ndomain as labeled data.\n6https://huggingface.co\n7https://github.com/paul-rottger/\ntemporal-adaptation\n8https://data.statmt.org/news-crawl/\n9https://github.com/Kel-Lu/\ntime-waits-for-no-one\nTime-Stratified Settings Generally, the PLM is\nadapted to temporality using unlabelled data, fine-\ntuned with the downstream labeled data, and then\nevaluated with the testing data which has the same\ntime as the pre-training data. We set the kas 500\nin all experiments. As for the ARXIV dataset, we\nutilize the unlabeled data in 2021 for pre-training\nand extract five years of data from 2011 to 2019\non a four-year cycle for fine-tuning as well as the\ndata in 2021 for testing. Similarly, we collect the\ndata in 2015 and 2016 from the PoliAff dataset as\nthe fine-tuning data and test the model with the\ndata in 2017. For the RTC dataset, we follow the\nprevious work (Röttger and Pierrehumbert, 2021)\nto select the unlabeled News Comments dataset for\npost-training and the political subreddit subset for\nfine-tuning. However, the number of masking can-\ndidates kis less than 500 in most RTC fine-tuning\nsets of different time splits, which could make the\nLMLM strategy be regarded as the random mask-\ning strategy. Thus, we select the data in April 2017\nfor fine-tuning (where k≥500 in this subset) and\nthe data in April 2018 and August 2019 for testing.\nDetailed data statistics are shown in Table 1.\n4.2 Baselines\nTo meet the time-stratified settings, we select the\ntemporal adaptation TAda method (Röttger and\nPierrehumbert, 2021) as baseline, which first in-\ncorporates the temporal information into the PLM\nby utilizing the time-specific unlabeled data for\npre-training and then adapt the PLM to the down-\nstream task with the supervised data. Besides the\ntemporal adaptation method, we also turn to some\nup-to-date domain adaptation methods since previ-\nous work (Röttger and Pierrehumbert, 2021) points\nout that such method can mitigate the temporal\nmisalignment problems to some extent. Specif-\nically, we select PERL (Ben-David et al., 2020)\nand DILBERT (Lekhtman et al., 2021) methods,\nand implement them under the time-stratified set-\ntings. Details of the domain adaptation methods\nare shown in the Appendix D. We calculate the F1\nscore as the testing results for all the experiments.\n4.3 Main Results\nARXIV Dataset The results of the ARXIV\ndataset are shown in Table 2, and we can observe\nthat applying domain adaptation methods under\nthe time-stratified settings aggravate the temporal\nmisalignment problem as the scores of the PERL\nand DILBERT methods are not as high as those\n6384\nMethod Fine-tuning Data\n2007 2011 2015 2019 Avg.\nTAda 82.97 84.72 84.82 84.99 84.38\n+ PERL 75.67 79.20 78.89 78.77 78.13\n+ DILBERT 82.62 83.89 84.04 84.22 83.69\n+ LMLM 84.93 86.52 86.49 87.22 86.29\nTable 2: Results of the ARXIV dataset.\nMethod Fine-tuning Data\n2015 2016 Avg.\nTAda 66.05 72.94 69.50\n+ PERL 61.79 68.21 65.00\n+ DILBERT 63.89 69.86 66.88\n+ LMLM 67.00 74.10 70.55\nTable 3: Results of the PoliAff dataset.\nfor utilizing the TAda directly. However, the per-\nformance of LMLM is much better than the other\nthree methods.\nPoliAff Dataset We report the results of the Po-\nliAff dataset in Table 3. Although there is a slight\ndomain difference between the pre-training and\nfine-tuning data, i.e., news and politic, the LMLM\ncan still achieve the best results, and the domain\nadaptation methods still perform worse than the\ntemporal adaptation methods.\nRTC Dataset The results of the RTC dataset is\nshown in Table 4, and we can find the similar ten-\ndency as the previous results, i.e., LMLM still\nachieve the best performance. However, the dif-\nferences among the four methods in RTC dataset\nare much smaller compared with the previous re-\nsults, which is largely due to the slight dynamic\ntemporality of the RTC dataset.\n5 Study\nIn this section, we conduct extensive studies to\nhelp better understand our method. It is worth\nnoting that all the experiments in this section are\nconducted on the BERT model with the ARXIV\ndataset unless there is a clear explanation.\n5.1 Effect of Pre-training Data Selection\nTo explore the temporal impact brought by pre-\ntraining data, we launch experiments with MLM\nobjective under two different pre-training settings:\n• Source Year Consistent Pre-training (SYCP)\nWe keep the time of pre-training data consistent\nMethod Testing Data\nApr. 2018 Aug. 2019 Avg.\nTAda 41.78 38.14 39.96\n+ PERL 40.21 37.14 38.68\n+ DILBERT 42.99 38.20 40.60\n+ LMLM 43.91 39.38 41.65\nTable 4: Results of the RTC dataset.\nMethod Fine-tuning Data\n2007 2011 2015 2019 Avg.\nSYCP 82.81 84.62 85.54 85.17 84.54\nTYCP (TAda)82.97 84.72 84.82 84.99 84.38\nSYCP + LMLM83.52 85.41 86.36 87.03 85.58\nTYCP + LMLM84.93 86.52 86.49 87.22 86.29\nTable 5: Results of different pre-training strategies.\nwith that of fine-tuning data to ensure the consis-\ntency between the two stages.\n• Target Year Consistent Pre-training (TYCP)\nFollowing the previous work (Röttger and Pierre-\nhumbert, 2021; Lazaridou et al., 2021), we utilize\nthe pre-training data in consistent with the eval-\nuation data in temporal dimension, i.e., the time\nof pre-training data and evaluation data is same.\nWe also implement our LMLM objective in the\npre-training stage for comparison, where the mask-\ning ratio is 15%, and k is 1000. The results are\nshown in the table 5. We can find that the perfor-\nmance of SYCP gradually overwhelms the TYCP\nas the time passes towards the target year. When the\nPLM is pre-trained with MLM objective under the\nSYCP setting, it can even outcome the performance\nof TYCP, and the PLM pre-trained with LMLM\nobjective under the TYCP setting can achieve the\nbest performance. On the one hand, we can infer\nthat the temporal adaptation method is effective\nsince TYCP beats the SYCP. On the other hand,\nthe LMLM objective can make the PLM pay more\nattention to the salient semantic changed words as\npre-training with the LMLM objective under the\nSYCP settings (SYCP+LMLM) can even surpass\nthe original temporal adaptation method (TYCP).\n5.2 Hyper-Parameter Analysis\nSince there is a strong relationship between the\nmasking ratio and the model’s performance, we\nconduct experiments to look for the best masking\nstrategy for the LMLM objective. Furthermore, we\nalso want to know whether the temporal misalign-\nment problem can be better mitigated by masking\n6385\nJSD (↑) #Sen Prec. SeC.\n0.00∼0.05 902 90.2% micro\n0.05∼0.10 78 7.8% medium\n0.10∼0.15 18 1.8% great\n0.15∼0.2 2 0.2% great\nTable 6: Distribution of the semantic changed words,\nwhere SeC. represents for the Semantic Change.\n100 300 500 800 1000\nnumber of candidate words\n0.150.30.50.8\nmasking ratios\n86.86 86.90 86.93 86.57 86.77\n87.11 87.23 87.22 87.30 87.32\n87.10 87.02 86.98 87.02 87.00\n86.91 86.86 86.86 87.02 86.89\n 86.6\n86.8\n87.0\n87.2\n87.4\nFigure 4: Results of the different masking strategies of\nLMLM. The horizontal axis indicates the number of\nmasked semantic changed words kand the vertical axis\nrepresents for the masking ratio α.\nmore salient semantic changed words. Thus, we\nselect the data in 2021 for pre-training with our\nLMLM objective and the data in 2009 for fine-\ntuning, followed by testing the model with the data\nin 2021. For better comparison, we utilize a heat\nmap (Figure 4) to display the results, where the ver-\ntical axis of this graph represents the masking ratio\nα, and the horizontal axis represents the number of\nmasked salient semantic changed words k.\nThe Influence of α We calculate the average\nvalue of the results of each masking ratio under\ndifferent settings of k and observe that when the\nmasking ratio is around 30%, the PLM can achieve\nthe best performance.\nThe Influence of k No doubt forcing the model\nto predict more high semantic change words can\nbetter mitigate the temporal problem generally.\nHowever, it is surprising to observe that the im-\nprovement is slight across different settings of k.\nThus, we quantify the semantic change of 1,000\nrandom sampled words from the candidatesWt\nmask\naccording to the JSD value and the distribution of\nthose words is shown in Table 6. We can find that\nPLMs Fine-tuning Data\n2007 2011 2015 2019 Avg.\nBERT 82.97 84.72 84.82 84.99 84.38\n+ TSC-Ada 84.93 86.52 86.49 87.22 85.80\nRoBERTa 81.72 84.37 84.46 84.95 83.88\n+ TSC-Ada 82.32 84.92 84.59 85.40 84.36\nTable 7: Results of different PLMs under the time-\nstratified settings.\nMetrics Fine-tuning Data\n2007 2011 2015 2019 Avg.\nED 84.99 86.62 86.43 87.60 86.41\nAPD 85.02 86.34 86.24 87.08 86.17\nJSD 84.93 86.52 86.49 87.22 86.29\nTable 8: Results of different quantification metrics.\nMethod Fine-tuning Data\n2014 2015 2016 Avg.\nTAda 81.23 80.91 81.94 81.36\n+ LMLM 81.41 82.50 82.63 82.18\nTable 9: Results of the CoNLL dataset.\nonly around 10% words have relative significant\nsemantic change (JSD value ≥0.05) while around\n72% words have little or no semantic change (JSD\nvalue ≈0.00). We can conclude that the improve-\nment mainly comes from predicting a few key-\nwords, i.e., topic words and newly emerging words,\nwhich have relatively salient semantic change.\n5.3 Pre-trained Language Model Analysis\nTo verify the generalization of our methods on dif-\nferent PLMs, we implement our method on two\nPLMs, i.e., BERT and RoBERTa, and utilize the\ntemporal adaptation method (Röttger and Pierre-\nhumbert, 2021) as the baseline for comparison. As\nshown in table 7, we find there is a dramatic im-\nprovement of each PLM, i.e., 1.42 points improve-\nment of the BERT model and 0.48 points improve-\nment of the RoBERTa model on average.\n5.4 Quantification Metrics\nAs mentioned above, there are several metrics to\nquantify the semantic change, and we primarily\nconduct the experiment to select the JSD metric\nand we compare three commonly used metrics, i.e.,\nED, APD, and JSD, in this section. As shown in the\ntable 8, we can find that although different metrics\nhave their advantages, the differences among them\nare slight. For example, the maximum difference\n6386\nSettings 2007 2011 2015 2019\nLMLM TAda LMLM TAda LMLM TAda LMLM TAda\nResults (Total) 84.32 82.27 86.28 84.60 86.17 85.13 87.17 85.18\nResults (w/o Temp) 83.81 81.80 85.99 83.84 86.38 85.68 87.60 85.99\nResults (w/ Temp) 84.02 82.79 86.40 84.97 85.62 84.30 86.66 84.35\nMask (Failed Sets) 12.87 12.91 10.75 11.25 8.95 9.96 10.41 13.04\nPAD (Failed Sets) 12.62 13.34 11.15 11.02 8.81 10.14 10.58 12.77\nREP (Failed Sets) 13.58 13.23 10.89 11.75 8.81 9.78 9.92 13.57\nTable 10: Error analysis of the LMLM method, where the first group shows the results on the hierarchical data (w/\nand w/o temporal information) while the second group shows the results on the failed examples.\nis 0.24 points among three metrics on average.\n5.5 Open-Domain Temporal Adaptation\nAs mentioned above, we conduct all the experi-\nments under the domain-specific setting. In this\nsection, we explore the effect of the LMLM objec-\ntive with the name entity recognition task under the\nopen-domain setting, i.e., the downstream dataset\nhas no specific domain. Specifically, we select the\nWMT dataset in 2017 as the unlabeled data and the\nsubset in 2015 and 2016 from the CoNLL dataset10\nas the fine-tuning data. In the end, we evaluate the\nmodel with the data in 2017. The results are shown\nin Table 9. We can find that the LMLM method out-\nperforms the original temporal adaptation method\nwith around 1 point improvement.\n5.6 Error Analysis\nWe also conduct fine-grained experiments to study\nwhy our method fails with some examples. Specifi-\ncally, we utilize the ARXIV dataset from 2007 to\n2017 to fine-tune the model and the data in 2021\nfor testing. We first select the top 100 lexical se-\nmantic changed tokens for each testing set. Then,\nwe divide the testing data into two parts: a subset\nwith temporal information and a subset without\ntemporal information by judging whether the texts\ncontain the selected tokens. As shown in the first\ngroup of Table 10, the LMLM method can achieve\nbetter results than TAda on both testing subsets,\nand the improvement on the subset with temporal\ninformation is more significant than that on the\nsubset without the temporal information. A pos-\nsible explanation for why LMLM performs better\non the subset without temporal information is that\nthere is still some temporal information left in this\ndata since we distinguish the subset with only 100\nlexical semantic changed tokens.\n10https://github.com/shrutirij/\ntemporal-twitter-corpus\nBesides, we collect the failed testing sets, i.e.,\nthe model predicting wrong labels on those data,\nand mask those mentioned above top 100 lexical\nsemantic changed tokens in the texts with two\nstrategies: (1) replace those tokens with special\nplaceholder <MASK> or <PAD>, and (2) randomly\nutilize other tokens in the vocabulary (except the\naforementioned lexical semantic changed tokens)\nfor substitution. The results are shown in the sec-\nond group of Table 10, where we can observe that\nTAda surpasses our method in general11. We think\nthose masked/replaced lexical semantic changed to-\nkens, which LMLM pays more attention to, may be\nthe critical messages for the model to help the de-\ncision. The missing of that important information\ncan cause a negative impact on the model, which\nleads to the performance decreasing.\n6 Related Work\n6.1 Temporal Misalignment\nPrevious studies have shown that models trained\non texts from one time period perform poorly when\ntested on texts in later periods for NLP tasks like\nmachine translation (Levenberg et al., 2010), re-\nview and news article classification (Huang and\nPaul, 2019, 2018), named entity recognition (Rijh-\nwani and Preo¸ tiuc-Pietro, 2020) and so on. Within\nthe current paradigm of using PLMs (Devlin et al.,\n2019), studies have focused more on the expansion\nof dataset (Liu et al., 2019; Lewis et al., 2020; Yang\net al., 2019) and model capacity (Raffel et al., 2019;\nLan et al., 2019; Brown et al., 2020) to achieve bet-\nter performance but ignore the temporal effects.\nFew studies focus on such problem, Lazaridou\net al. (2021) have empirically studied the degraded\nperformance of PLMs over time, and Röttger and\n11It is worth noting that LMLM surpasses the TAda on the\nREP testing set in 2007, which can be attributed to the pos-\nsibility of replacing the original tokens with lexical semantic\nchanged tokens.\n6387\nPierrehumbert (2021) focus on post-tuning BERT\nwith the data in specific periods to mitigate the\ntemporal misalignment problems. Furthermore,\nAmba Hombaiah et al. (2021) propose sampling\nmethods to help PLMs achieve better performance\non the evolving content. In this paper, we conduct\na detailed investigation from the perspective of lexi-\ncal semantic change to figure out the reason behind\nthe limited performance of the PLMs under the\ntime-stratified settings.\n6.2 Lexical Semantic Change\nLexical semantic change is an extensively studied\nconcept in the computational linguistics, which\nmainly focuses on deciding whether the concept\nof a word has changed over time (semantic change\ndetection) (Gulordava and Baroni, 2011; Kulka-\nrni et al., 2015; Dubossarsky et al., 2015; Hamil-\nton et al., 2016) or discovering the instances with\nhigh semantic change (semantic change discov-\nery) (Hengchen et al., 2021; Kurtyigit et al., 2021;\nJatowta et al., 2021). Among them, most studies\nutilize contextualized word representations (Tur-\nney and Pantel, 2010; Giulianelli et al., 2020) and\nmeasure the distance among them in different pe-\nriods (Cook and Stevenson, 2010; Gulordava and\nBaroni, 2011; Hamilton et al., 2016) to detect or\ndiscover the instances with salient semantic change.\nPrevious studies mainly concentrate on applying\nPLMs to discover the semantic change phenomena,\nwhile our work focuses on solving such problems\nintrinsic in the PLMs. Thus, besides observing\nsuch semantic changed phenomenon, we aim to\nfind the corresponding words and apply the LMLM\nobjective to make the PLMs more aware of them to\nmitigate the temporal misalignment problem. Most\nstudies focused on obtaining those words are under\nthe supervised settings (Kim et al., 2014; Basile\net al.; Basile and McGillivray, 2018; Tsakalidis\net al., 2019) by scoring and selecting the top-ranked\nwords through author intuitions or known historical\ndata (Kurtyigit et al., 2021). While Giulianelli et al.\npropose one unsupervised method, adding one clus-\ntering process to the traditional selecting methods.\nTo our best knowledge, this is the first work that\nlinks semantic change with temporal adaptation.\n7 Conclusion & Future Work\nIn this paper, we investigate the temporal misalign-\nment of the PLMs from the lexical level and ob-\nserve that the words with salient lexical seman-\ntic change contribute significantly to the tempo-\nral problems. We propose a lexical-based masked\nLanguage Model (LMLM) objective based on the\nabove observation. Experiments on two PLMs with\nthe sequence classification task on three datasets\nunder the specific domain setting and one name en-\ntity recognition task under the open-domain setting\nconfirm that our proposed method performs better\nthan the previous temporal adaptation methods and\nthe state-of-the-art domain adaptation methods. In\nthe future, we will keep discovering such temporal\nmisalignment problems in the text generation tasks,\ne.g., machine translation, and improve our method\nby reducing the extra offline computational cost on\nprocedures like Semantic Change Detection.\n8 Limitation\nThere are still some limitations in our work which\nare listed below:\n• The other tokens in the text influence the mean-\ning of the target word to some extent since we\nutilize sentence contextualization to represent the\nmeaning of the target word. To this end, it is hard\nto interpret why some candidate words are se-\nlected by the detection step, e.g., name entities\nor numbers, whose meaning remains unchanged.\nWe will design a better unsupervised word selec-\ntion strategy in the future.\n• We utilize the lexical-level masking strategy,\nwhile the semantic change can also be reflected\nwith the whole sequence, e.g., the topic of\n“Malaysia Airlines crashed into the sea” may be\none hypothesis before 2014, but it became a se-\nvere accident in 2014. Current famous MLM\nobjectives like span masking objective (Raffel\net al., 2020) or sentence masking objective (Tay\net al., 2022) have observed that the performance\nof denoising the whole sequence is better than\ndenoising the single token in some NLU tasks. In\nthe future, we will explore whether the sequence\nmasking objective mentioned above can mitigate\nthe temporal misalignment problem inherent in\nthe PLMs.\nAcknowledgement\nThis work was supported by the National Science\nFoundation of China (NSFC No. 62206194), the\nNatural Science Foundation of Jiangsu Province,\nChina (Grant No. BK20220488), and the Project\nFunded by the Priority Academic Program Devel-\nopment of Jiangsu Higher Education Institutions.\n6388\nReferences\nSpurthi Amba Hombaiah, Tao Chen, Mingyang Zhang,\nMichael Bendersky, and Marc Najork. 2021. Dy-\nnamic language models for continuously evolving\ncontent. In Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining,\npages 2514–2524.\nRobert Bamler and Stephan Mandt. 2017. Dynamic\nword embeddings. In International conference on\nMachine learning, pages 380–389. PMLR.\nPierpaolo Basile, Annalina Caputo, Roberta Luisi, and\nGiovanni Semeraro. Diachronic analysis of the ital-\nian language exploiting google ngram.\nPierpaolo Basile and Barbara McGillivray. 2018. Ex-\nploiting the web for semantic change detection. In In-\nternational Conference on Discovery Science, pages\n194–208. Springer.\nEyal Ben-David, Carmel Rabinovitz, and Roi Reichart.\n2020. Perl: Pivot-based domain adaptation for\npre-trained deep contextualized embedding models.\nTransactions of the Association for Computational\nLinguistics, 8:504–521.\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classification. In\nProceedings of the 45th annual meeting of the asso-\nciation of computational linguistics, pages 440–447.\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 con-\nference on empirical methods in natural language\nprocessing, pages 120–128.\nAJ Bohonak. 2002. Ibd (isolation by distance): a pro-\ngram for analyses of isolation by distance. Journal\nof Heredity, 93(2):153–154.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nRicardo Campos, Vítor Mangaravite, Arian Pasquali,\nAlípio Mário Jorge, Célia Nunes, and Adam Jatowt.\n2018. Yake! collection-independent automatic key-\nword extractor. In European Conference on Informa-\ntion Retrieval, pages 806–810. Springer.\nPaul Cook and Suzanne Stevenson. 2010. Automati-\ncally identifying changes in the semantic orientation\nof words. In Proceedings of the Seventh International\nConference on Language Resources and Evaluation\n(LREC’10).\nMarco Del Tredici, Raquel Fernández, and Gemma\nBoleda. 2018. Short-term meaning shift: A distribu-\ntional exploration. arXiv preprint arXiv:1809.03169.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nHaim Dubossarsky, Yulia Tsvetkov, Chris Dyer, and\nEitan Grossman. 2015. A bottom up approach to\ncategory mapping and meaning change. In NetWordS,\npages 66–70.\nMario Giulianelli, Marco Del Tredici, and Raquel Fer-\nnández. 2020. Analysing lexical semantic change\nwith contextualised word representations. arXiv\npreprint arXiv:2004.14118.\nKristina Gulordava and Marco Baroni. 2011. A distribu-\ntional similarity approach to the detection of semantic\nchange in the google books ngram corpus. In Pro-\nceedings of the GEMS 2011 workshop on geometrical\nmodels of natural language semantics, pages 67–71.\nWilliam L Hamilton, Jure Leskovec, and Dan Jurafsky.\n2016. Diachronic word embeddings reveal statistical\nlaws of semantic change. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1489–\n1501.\nSimon Hengchen, Ruben Ros, Jani Marjanen, and\nMikko Tolonen. 2021. A data-driven approach to\nstudying changing vocabularies in historical newspa-\nper collections. Digital Scholarship in the Humani-\nties, 36(Supplement_2):ii109–ii126.\nXiaolei Huang and Michael Paul. 2019. Neural tem-\nporality adaptation for document classification: Di-\nachronic word embeddings and domain adaptation\nmodels. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4113–4123.\nXiaolei Huang and Michael J Paul. 2018. Examining\ntemporality in document classification. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\nvolume 2.\nAdam Jatowta, Nina Tahmasebib, and Lars Borinb.\n2021. Computational approaches to lexical seman-\ntic change: Visualization systems and novel appli-\ncations. Computational approaches to semantic\nchange, 6:311.\nYoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde,\nand Slav Petrov. 2014. Temporal analysis of lan-\nguage through neural language models. ACL 2014,\npage 61.\nVivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and\nSteven Skiena. 2015. Statistically significant detec-\ntion of linguistic change. In Proceedings of the 24th\ninternational conference on world wide web, pages\n625–635.\n6389\nSinan Kurtyigit, Maike Park, Dominik Schlechtweg,\nJonas Kuhn, and Sabine Schulte im Walde. 2021.\nLexical semantic change discovery. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 6985–6998.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nAngeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya,\nDevang Agrawal, Adam Liska, Tayfun Terzi, Mai\nGimenez, Cyprien de Masson d’Autume, Tomas Ko-\ncisky, Sebastian Ruder, et al. 2021. Mind the gap:\nAssessing temporal generalization in neural language\nmodels. Advances in Neural Information Processing\nSystems, 34.\nEntony Lekhtman, Yftah Ziser, and Roi Reichart. 2021.\nDilbert: Customized pre-training for domain adapta-\ntion with category shift, with an application to aspect\nextraction. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 219–230.\nAbby Levenberg, Chris Callison-Burch, and Miles Os-\nborne. 2010. Stream-based translation models for\nstatistical machine translation. In Human Language\nTechnologies: The 2010 Annual Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics, pages 394–402.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7871–7880.\nJianhua Lin. 1991. Divergence measures based on the\nshannon entropy. IEEE Transactions on Information\ntheory, 37(1):145–151.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-\nishma Mandyam, and Noah A Smith. 2021. Time\nwaits for no one! analysis and challenges of temporal\nmisalignment. arXiv preprint arXiv:2111.07408.\nPasquale Nardone. 2014. Entropy of difference. arXiv\npreprint arXiv:1411.0506.\nW Ochs. 1976. Basic properties of the generalized\nboltzmann-gibbs-shannon entropy. Reports on Math-\nematical Physics, 9(2):135–155.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nJuan Ramos et al. 2003. Using tf-idf to determine word\nrelevance in document queries. In Proceedings of the\nfirst instructional conference on machine learning ,\nvolume 242, pages 29–48. Citeseer.\nShruti Rijhwani and Daniel Preo¸ tiuc-Pietro. 2020.\nTemporally-informed analysis of named entity recog-\nnition. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7605–7617.\nStuart Rose, Dave Engel, Nick Cramer, and Wendy\nCowley. 2010. Automatic keyword extraction from\nindividual documents. Text mining: applications and\ntheory, 1:1–20.\nAlex Rosenfeld and Katrin Erk. 2018. Deep neural\nmodels of semantic shift. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 474–484.\nPaul Röttger and Janet Pierrehumbert. 2021. Temporal\nadaptation of bert and performance on downstream\ndocument classification: Insights from social media.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2400–2412.\nPeter J Rousseeuw. 1987. Silhouettes: a graphical aid\nto the interpretation and validation of cluster analysis.\nJournal of computational and applied mathematics,\n20:53–65.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nNeil Houlsby, and Donald Metzler. 2022. Unify-\ning language learning paradigms. arXiv preprint\narXiv:2205.05131.\n6390\nAdam Tsakalidis, Marya Bazzi, Mihai Cucuringu, Pier-\npaolo Basile, and Barbara McGillivray. 2019. Min-\ning the uk web archive for semantic change detection.\nIn Proceedings of the International Conference on\nRecent Advances in Natural Language Processing\n(RANLP 2019), pages 1212–1221.\nPeter D Turney and Patrick Pantel. 2010. From fre-\nquency to meaning: Vector space models of se-\nmantics. Journal of artificial intelligence research,\n37:141–188.\nSergei Vassilvitskii and David Arthur. 2006. k-\nmeans++: The advantages of careful seeding. In Pro-\nceedings of the eighteenth annual ACM-SIAM sym-\nposium on Discrete algorithms, pages 1027–1035.\nIan H Witten, Gordon W Paynter, Eibe Frank, Carl\nGutwin, and Craig G Nevill-Manning. 2005. Kea:\nPractical automated keyphrase extraction. In Design\nand Usability of Digital Libraries: Case Studies in\nthe Asia Pacific, pages 129–152. IGI global.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nMin Zhang and Juntao Li. 2021. A commentary of\ngpt-3 in mit technology review 2021. Fundamental\nResearch, 1(6):831–833.\n6391\nA Implementation of the K-Means\nAlgorithm\nGiven a representation matrix Rt\ni = {rt\ni}i∈(1,···,N)\nfor the word wt\ni we utilize the silhouette\nscore (Rousseeuw, 1987) to obtain the optimal\nK for K-Means algorithm. We experiment with\nK ∈[2,10] in a heuristic way. For each K, the\nclustering result is the one that yields the minimal\ndistortion value, i.e., the minimal sum of squared\ndistances of each data point from its closest cen-\ntroid, and we execute ten iterations to alleviate the\ninfluence of different initialization values (Vassil-\nvitskii and Arthur, 2006). Since there are several\nmonosemous words, i.e., the number of Kis 1, we\nfilter those words with a thresholdd. Specifically, if\nthe intra-cluster dispersion value of a word is below\nd, we would allocate K = 1, otherwise, K ≥2.\nThe optimal Kis the one that can simultaneously\nminimize the dispersion score and maximize the\nsilhouette score.\nB Implementation of the Masking\nStrategies\nIn this section, we illustrate the frequency masking\nstrategy and importance masking strategy in detail.\nGiven a dataset that contains n texts, we firstly\nutilize the NLTK tool12 to tokenize each text and\nfollow the below processes:\nFrequency Masking Strategy We add each tok-\nenized token into the dictionary Dand record the\nnumber of its occurrence. We sort the tokens in D\naccording to the occurrence times and select the to-\nkens to mask in descending order until the masking\nratio is satisfied.\nImportance Masking Strategy We utilize the\nY AKE! method as mentioned above to sort the to-\nkenized tokens according to the scores calculated\nwith the task label, e.g., the label of CS in the\nARXIV dataset. Finally, we select the tokens to\nmask in descending order until the masking ratio is\nsatisfied.\nC Model Training & Parameters\nArchitecture We utilize the BERT-base uncased\nmodel pretrained on a large corpus of English data\nwith the MLM objective. The model contains 12\ntransformer layers, 12 attention heads, and the hid-\nden layer size is 768. The total number of parame-\n12https://github.com/nltk/nltk\nters is 110 million. We add a linear layer after the\nlast BERT layer for the downstream classification\ntask and generate the output with softmax. The\nmaximum input sequence length is 512.\nTraining Details We utilize cross-entropy loss\nin the pre-training and fine-tuning stages and ap-\nply AdamW (Loshchilov and Hutter, 2017) as the\noptimizer. Specifically, the learning rate is 5e-5,\nand the weight decay is 0.01. Moreover, we set\na 10% dropout probability for regularisation, We\npre-train the model for one epoch and fine-tune the\nmodel until convergency. We set the batch size as\n128 and conduct the experiments on eight NVIDIA\nGTX3090 GPUs.\nEvaluation Metric We utilize the F1 score13 as\nthe evaluation metric in all the experiments.\nD Implementation of the Baselines\nThis section will elaborate on how to apply the do-\nmain adaptation methods under the temporal adap-\ntation settings.\nPERL (Ben-David et al., 2020) This method\nmodel parameters using a pivot-based (Blitzer\net al., 2006, 2007) variant of the MLM object with\nunlabeled datasets from both the source and tar-\nget temporal split. Instead of masking each token\nwith the same probability, we divided token into\npivots and non-pivots to learn the pivot/non-pivot\ndistinction on unlabeled data from the source and\ntarget time span. The encoder weights are frozen\nduring training for the downstream task. Specif-\nically, we rank those frequent features (occurs at\nleast 20 times in the unlabeled data from the source\nand target time split) based on the mutual informa-\ntion with the task label according to source domain\nlabeled data. Then, we select top 100 which have\nrelative high scores as pivot features. The non-pivot\nfeature subset consists of features that do not match\nthe two requirements.\nDILBERT (Lekhtman et al., 2021) is the SOTA\nin Aspect Extraction while using a fraction of the\nunlabeled data. Different from PERL, they chal-\nlenge the “high MI with the task label\" criterion\nin the pivot definition. In our settings, we harness\nthe information about the golden label(physics, cs.,\netc) in the source and target temporal split to mask\n13https://scikit-learn.org/stable/\nmodules/generated/sklearn.metrics.f1_\nscore.html\n6392\nwords that are more likely to bridge the gap be-\ntween the different periods. Specifically, we com-\npute the cosine similarity between each input word\nand the label from both the source and the target.\nWe keep the highest similarity score for each word\nand mask the top 0.15% of the input words. For\nthe downstream task, they add a logistic regression\nhead on top of all outputs and fine-tune the model\non the source period labeled data.\nE Feature Extracting\nOne point that worth discussing is the hidden\nstates from the last layer of the BERT model\n(LMLMLAST) contains massive amounts of con-\ntextual information, which may overwhelm the lex-\nical information. Thus we turn to the representa-\ntion from the shallow layer of the BERT model,\ne.g., representation from the second BERT layer\n( LMLMSECOND). Specifically, we conduct the ex-\nperiment on the ARXIV testing set in 2013, and\nthe results are shown in Table 11.\nModel F1\nLMLMLAST 87.45\nLMLMSECOND 86.67\nTAda 85.04\nTable 11: Results on the ARXIV testing set.\nAs we can observe from the table that the\nLMLMSECOND can achieve a better result than\nTAda, which indicates that the representations\nfrom the earlier layer are strong enough to help\nachieve a decent performance improvement. Be-\nsides, the LMLMLAST achieves a better result than\nLMLMSECOND, which means that the hidden states\nwhich contain sentence-level information can help\npromote the accuracy in the detection process.\n6393",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8593859672546387
    },
    {
      "name": "Generalization",
      "score": 0.7419351935386658
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6450445652008057
    },
    {
      "name": "Natural language processing",
      "score": 0.6200600862503052
    },
    {
      "name": "Language model",
      "score": 0.5364708304405212
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5353325605392456
    },
    {
      "name": "Natural language",
      "score": 0.4279179573059082
    },
    {
      "name": "Language identification",
      "score": 0.42242249846458435
    },
    {
      "name": "Code (set theory)",
      "score": 0.4176381528377533
    },
    {
      "name": "Cache language model",
      "score": 0.4100547730922699
    },
    {
      "name": "Machine learning",
      "score": 0.33592331409454346
    },
    {
      "name": "Universal Networking Language",
      "score": 0.21678102016448975
    },
    {
      "name": "Comprehension approach",
      "score": 0.07892674207687378
    },
    {
      "name": "Programming language",
      "score": 0.07467269897460938
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3923682",
      "name": "Soochow University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ],
  "cited_by": 11
}