{
  "title": "Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation",
  "url": "https://openalex.org/W3136071852",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2004970051",
      "name": "Alexandra Chronopoulou",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2221112761",
      "name": "Dario Stojanovski",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2099684778",
      "name": "Alexander Fraser",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2025768430",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964266061",
    "https://openalex.org/W3015504467",
    "https://openalex.org/W2962824887",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2963602293",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W2913659301",
    "https://openalex.org/W2890007195",
    "https://openalex.org/W2971031524",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963443683",
    "https://openalex.org/W3101286153",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W1828724394",
    "https://openalex.org/W3104723404"
  ],
  "abstract": "Successful methods for unsupervised neural machine translation (UNMT) employ crosslingual pretraining via self-supervision, often in the form of a masked language modeling or a sequence generation task, which requires the model to align the lexical- and high-level representations of the two languages. While cross-lingual pretraining works for similar languages with abundant corpora, it performs poorly in low-resource and distant languages. Previous research has shown that this is because the representations are not sufficiently aligned. In this paper, we enhance the bilingual masked language model pretraining with lexical-level information by using type-level cross-lingual subword embeddings. Empirical results demonstrate improved performance both on UNMT (up to 4.5 BLEU) and bilingual lexicon induction using our method compared to a UNMT baseline.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 173–180\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n173\nImproving the Lexical Ability of Pretrained Language Models\nfor Unsupervised Neural Machine Translation\nAlexandra Chronopoulou, Dario Stojanovski, Alexander Fraser\nCenter for Information and Language Processing, LMU Munich, Germany\n{achron, stojanovski, fraser}@cis.lmu.de\nAbstract\nSuccessful methods for unsupervised neural\nmachine translation ( UNMT ) employ cross-\nlingual pretraining via self-supervision, often\nin the form of a masked language modeling\nor a sequence generation task, which requires\nthe model to align the lexical- and high-level\nrepresentations of the two languages. While\ncross-lingual pretraining works for similar lan-\nguages with abundant corpora, it performs\npoorly in low-resource and distant languages.\nPrevious research has shown that this is be-\ncause the representations are not sufﬁciently\naligned. In this paper, we enhance the bilin-\ngual masked language model pretraining with\nlexical-level information by using type-level\ncross-lingual subword embeddings. Empiri-\ncal results demonstrate improved performance\nboth on UNMT (up to 4.5 BLEU) and bilingual\nlexicon induction using our method compared\nto a UNMT baseline.\n1 Introduction\nUNMT is an effective approach for translation with-\nout parallel data. Early approaches transfer in-\nformation from static pretrained cross-lingual em-\nbeddings to the encoder-decoder model to provide\nan implicit bilingual signal (Lample et al., 2018a;\nArtetxe et al., 2018c). Lample and Conneau (2019)\nsuggest to instead pretrain a bilingual language\nmodel (XLM ) and use it to initialize UNMT , as it\ncan successfully encode higher-level text represen-\ntations. This approach largely improves transla-\ntion scores for language pairs with plentiful mono-\nlingual data. However, while UNMT is effective\nfor high-resource languages, it yields poor results\nwhen one of the two languages is low-resource\n(Guzmán et al., 2019). Marchisio et al. (2020)\nshow that there is a strong correlation between bilin-\ngual lexicon induction (BLI) and ﬁnal translation\nperformance when using pretrained cross-lingual\nembeddings, converted to phrase-tables, as initial-\nization of a UNMT model (Artetxe et al., 2019).\nVuli´c et al. (2020) observe that static cross-lingual\nembeddings achieve higher BLI scores compared\nto multilingual language models ( LMs), meaning\nthat they obtain a better lexical-level alignment.\nSince bilingual LM pretraining is an effective form\nof initializing a UNMT model, improving the over-\nall representation of the masked language model\n(MLM ) is essential to obtaining a higher translation\nperformance.\nIn this paper, we propose a new method to en-\nhance the embedding alignment of a bilingual lan-\nguage model, entitled lexically aligned MLM , that\nserves as initialization for UNMT . Speciﬁcally, we\nlearn type-level embeddings separately for the two\nlanguages of interest. We map these monolingual\nembeddings to a common space and use them to ini-\ntialize the embedding layer of an MLM . Then, we\ntrain the MLM on both languages. Finally, we trans-\nfer the trained model to the encoder and decoder of\nan NMT system. We train the NMT system in an un-\nsupervised way. We outperform a UNMT baseline\nand demonstrate the importance of cross-lingual\nmapping of token-level representations. We also\nconduct an analysis to investigate the correlation\nbetween BLI, 1-gram precision and translation re-\nsults. We ﬁnally investigate whether cross-lingual\nembeddings should be updated or not during the\nMLM training process, in order to preserve lexical-\nlevel information useful for UNMT . We make the\ncode used for this paper publicly available1.\n2 Proposed Approach\nOur approach has three distinct steps, which are\ndescribed in the following subsections.\n2.1 VecMap Embeddings\nInitially, we split the monolingual data from both\nlanguages using BPE tokenization (Sennrich et al.,\n1https://github.com/alexandra-chron/\nlexical_xlm_relm\n174\nen en en en en en en enLanguage \nembeddings\nPosition \nembeddings\nToken \nembeddings\nTransformer\ncat the\n+ + + + + + + + \n+ \ncat\nмачка matподлога\nsat\nседеше\ndog\nкуче carpetкилим\nstandстојат\nunsupervised\n  cross-lingual  embeddings\n0\n+ \n1\n+ \n2\n+ \n3\n+ \n4\n+ \n5\n+ \n6\n+ \n7\n[s] the [MASK] sat on [MASK] mat [/s]\nFigure 1: Lexically aligned cross-lingual masked language model.\n2016b). We build subword monolingual embed-\ndings with fastText(Bojanowski et al., 2017). Then,\nwe map the monolingual embeddings of the two\nlanguages to a shared space, usingVecMap(Artetxe\net al., 2018a), with identical tokens occurring in\nboth languages serving as the initial seed dictionary,\nas we do not have any bilingual signal. This is dif-\nferent from the original VecMap approach, which\noperates at the word level. We use the mapped\nembeddings of the two languages to initialize the\nembedding layer of a Transformer-based encoder\n(Vaswani et al., 2017).\n2.2 Masked Language Model Training\nWe initialize the token embedding layer of the\nMLM Transformer encoder with pretrained VecMap\nembeddings, which provide an informative map-\nping, i.e., cross-lingual lexical representations. We\ntrain the model on data from both languages, using\nmasked language modeling. Training a masked\nlanguage model enhances the cross-lingual signal\nby encoding contextual representations. This step\nis illustrated in Figure 1.\n2.3 Unsupervised NMT\nFinally, we transfer theMLM -trained encoder Trans-\nformer to an encoder-decoder translation model.\nWe note that the encoder-decoder attention of the\nTransformer is randomly initialized. We then train\nthe model for NMT in an unsupervised way, using\ndenoising auto-encoding (Vincent et al., 2008) and\nback-translation (Sennrich et al., 2016a), which is\nperformed in an online manner. This follows work\nby Artetxe et al. (2018b); Lample et al. (2018a,c).\n3 Experiments\nDatasets. We conduct experiments on English-\nMacedonian (En-Mk) and English-Albanian (En-\nSq), as Mk, Sq are low-resource languages, where\nlexical-level alignment can be most beneﬁcial. We\nuse 3K randomly sampled sentences of SETIMES\n(Tiedemann, 2012) as validation/test sets. We also\nuse 68M En sentences from NewsCrawl. For Sq\nand Mk we use all the CommonCrawl corpora from\nOrtiz Suárez et al. (2019), which are 4M Sq and\n2.4M Mk sentences.\nBaseline. We use a method that relies on cross-\nlingual language model pretraining, namely XLM\n(Lample and Conneau, 2019). This approach trains\na bilingual MLM separately for En-Mk and En-Sq,\nwhich is used to initialize the encoder-decoder of\nthe corresponding NMT system. Each system is\nthen trained in an unsupervised way.\nComparison to state-of-the-art. We apply our\nproposed approach to RE-LM (Chronopoulou et al.,\n2020), a state-of-the-art approach for low-resource\nUNMT . This method trains a monolingual En MLM\nmodel (monolingual pretraining step). Upon con-\nvergence, a vocabulary extension method is used,\nthat randomly initializes the newly added vocab-\nulary items. Then, the MLM is ﬁne-tuned to the\ntwo languages (MLM ﬁne-tuning step) and used to\ninitialize an encoder-decoder model. This method\noutperforms XLM on low-resource scenarios.\nLexically aligned language models. When ap-\nplied to the baseline, our method initializes the\nembedding layer of XLM with unsupervised cross-\nlingual embeddings. Then, we train XLM on the\ntwo languages of interest with a masked language\nmodeling objective. Upon convergence, we trans-\nfer it to the encoder and decoder of an NMT model,\nwhich is trained in an unsupervised way.\nIn the case of RE-LM, our method is applied to\nthe MLM ﬁne-tuning step. Instead of randomly ini-\ntializing the new embedding vectors added in this\nstep, we use pretrained unsupervised cross-lingual\nembeddings. We obtain them by applying VecMap\nto fastText pretrained Albanian/Macedonian em-\nbeddings and the English MLM token-level embed-\ndings. Then, the MLM is ﬁne-tuned on both lan-\nguages. Finally, it is used to initialize an encoder-\ndecoder NMT model.\n175\nMk→En En →Mk Sq →En En →Sq\nBLEU ↑ CHR F1 ↑ BLEU ↑ CHR F1 ↑ BLEU ↑ CHR F1 ↑ BLEU ↑ CHR F1 ↑\nXLM 20.7 48.5 19.8 42.4 31.1 56.8 31.3 56.2\nlexically aligned XLM 25.2 49.9 22.9 43.1 32.8 58.2 33.5 56.8\nRE-LM 25.0 51.1 23.9 45.8 30.1 55.8 32.2 56.4\nlexically aligned RE-LM 25.3 51.5 25.6 47.6 30.5 56.0 32.9 56.7\nTable 1: UNMT results for translations to and from English. The ﬁrst column indicates the pretraining method used.\nThe scores presented are signiﬁcantly different (p < 0.05) from the respective baseline. CHR F1 refers to character\nn-gram F1 score (Popovi´c, 2015). The models in italics are ours.\nUnsupervised VecMap bilingual embeddings.\nWe build monolingual embeddings with the fast-\nText skip-gram model with 1024 dimensions, using\nour BPE-split (Sennrich et al., 2016b) monolingual\ncorpora. We map them to a shared space, using\nVecMap with identical tokens. We concatenate the\naligned embeddings of the two languages and use\nthem to initialize the embedding layer of XLM , or\nthe new vocabulary items of RE-LM.\nPreprocessing. We tokenize the monolingual\ndata and validation/test sets using Moses (Koehn\net al., 2006). For XLM (Lample and Conneau,\n2019), we use BPE splitting with 32K operations\njointly learned on both languages. For RE-LM\n(Chronopoulou et al., 2020), we learn 32K BPEs\non En for pretraining, and then 32K BPEs on both\nlanguages for the ﬁne-tuning and UNMT steps. The\nBPE merges are learned on a subset of the En cor-\npus and the full Sq or Mk corpus.\nModel hyperparameters. We use a Transformer\narchitecture for both the baselines and UNMT mod-\nels, using the same hyperparameters as XLM . For\nthe encoder Transformer used for masked language\nmodeling, the embedding and model size is 1024\nand the number of attention heads is 8. The encoder\nTransformer has 6 layers, while theNMT model is a\n6-layer encoder/decoder Transformer. The learning\nrate is set to 10−4 for XLM and UNMT . We train the\nmodels on 8 NVIDIA GTX 11 GB GPUs. To be\ncomparable with RE-LM, we retrain it on 8 GPUs,\nas that work reports UNMT results with only 1 GPU.\nThe per-GPU batch size is 32 during XLM and 26\nduring UNMT . Our models are built on the publicly\navailable XLM and RE-LM codebases. We generate\nﬁnal translations with beam search of size 5 and\nwe evaluate with SacreBLEU2 (Post, 2018).\n4 Results\nTable 1 shows the results of our approach com-\npared to two pretraining approaches that rely on\n2Signature “BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.9”\nMLM training, namely XLM and RE-LM. The lexi-\ncally aligned XLM improves translation results over\nthe baseline XLM model. We obtain substantial\nimprovements on En-Sq in both directions, of at\nmost 2.2 BLEU and 1.4 CHR F1, while on En-Mk,\nwe get an even larger performance boost of up to\n4.5 points in terms of BLEU and 1.4 in terms of\nCHR F1. Our lexically aligned RE-LM also con-\nsistently outperforms RE-LM, most notably in the\nEn→Mk direction, by up to1.7 BLEU. At the same\ntime, CHR F1 score improves by up to 1.8 points\nusing the lexically aligned pretraining approach\ncompared to RE-LM.\nIn the case of XLM , the effect of cross-lingual\nlexical alignment is more evident for En-Mk, as\nMk is less similar to En, compared to Sq. This\nis mainly the case because the two languages use\na different alphabet (Latin for En and Cyrillic for\nMk). This is also true for RE-LM when translating\nout of En, showing that enhancing the ﬁne-tuning\nstep of MLM with pretrained embeddings is helpful\nand improves the ﬁnal UNMT performance.\nIn general, our method provides better alignment\nof the lexical-level representations of the MLM ,\nthanks to the transferred VecMap embeddings. We\nhypothesize that static cross-lingual embeddings\nenhance the knowledge that a cross-lingual masked\nlanguage model obtains during training. As a re-\nsult, using them to bootstrap the pretraining pro-\ncedure improves the ability of the model to map\nthe distributions of the two languages and yields\nhigher translation scores. Overall, our approach\nconsistently outperforms two pretraining models\nfor UNMT , providing for the highest BLEU and\nCHR F1 scores on all translation directions.\n5 Analysis\nWe conduct an analysis to assess the contribution\nof lexical-level alignment in the MLM training. We\npresent Bilingual Lexicon Induction (BLI) and\n176\nEn-Mk En-Sq\nNN CSLS NN CSLS\nXLM 6.3 6.5 43.0 40.7\nlexically aligned XLM 15.5 16.5 51.6 50.6\nRE-LM 29.8 16.1 52.0 35.9\nlexically aligned RE-LM 32.0 17.2 53.0 36.9\nTable 2: P@5 results for the BLI task on the MUSE\n(Lample et al., 2018b) dictionaries. We evaluate the\nalignment of the embedding layer of each trainedMLM .\nBLEU 1-gram precision scores. We also investigate\nthe best method to leverage pretrained cross-lingual\nembeddings during MLM training, in terms of ﬁnal\nUNMT performance.\nBilingual Lexicon Induction (BLI). We use BLI,\na standard way of evaluating lexical quality of em-\nbedding representations (Gouws et al., 2015; Ruder\net al., 2019), to explore the effect of the alignment\nof our method. We compare the BLI score of differ-\nent cross-lingual pretrained language models. We\nreport precision@5 (P@5) using nearest neighbors\n(NN) and cross-lingual semantic similarity (CSLS).\nThe results are presented in Table 2. We use the em-\nbedding layer of each MLM for this task. We also\nexperimented with averages over different layers,\nbut noticed the same trend in terms of BLI scores.\nWe obtain word-level representations by averaging\nover the corresponding subword embeddings. It is\nworth noting that we compute the type-level rep-\nresentation of each vocabulary word in isolation,\nsimilar to Vuli´c et al. (2020).\nIn Table 2, we observe that lexical alignment is\nmore beneﬁcial for En-Mk. This can be explained\nby the limited vocabulary overlap of the two lan-\nguages, which does not provide sufﬁcient cross-\nlingual signal for the training of MLM . By contrast,\ninitializing an MLM with pretrained embeddings\nlargely improves performance, even for a higher-\nperforming model, such as RE-LM. In En-Sq, the\neffect of our approach is smaller yet consistent.\nThis can be attributed to the fact that the two lan-\nguages use the same script.\nOverall, our method enhances the lexical-level\ninformation captured by pretrained MLM s, as\nshown empirically. This is consistent with our intu-\nition that cross-lingual embeddings capture a bilin-\ngual signal that can beneﬁt MLM representations.\n1-gram precision scores. To examine whether the\nimproved translation performance is a result of the\nlexical-level information provided by static embed-\ndings, we present 1-gram precision scores in Ta-\nEn-Mk En-Sq\n← → ← →\nXLM 53.1 41.4 62.1 60.4\nlexically aligned XLM 56.0 51.8 63.6 61.5\nRE-LM 56.0 52.8 61.6 61.2\nlexically aligned RE-LM 56.6 53.9 62.0 61.7\nTable 3: BLEU 1-gram precision scores.\nble 3, as they can be directly attributed to lexical\nalignment. The biggest performance gains (up to\n+10.4) are obtained when the proposed approach\nis applied to XLM . This correlates with the BLEU\nscores of Table 1. Moreover, the En-Mk language\npair beneﬁts more than En-Sq from the lexical-\nlevel alignment both in terms of 1-gram precision\nand BLEU. These results show that the improved\nBLEU scores can be attributed to the enhanced\nlexical representations.\nAlignment Method En-Mk En-Sq\n← → ← →\nlexically alignedMLM\nfrozen embeddings 24.7 22.1 31.0 32.1\nﬁne-tuned embeddings (ours)25.2 22.9 32.8 33.5\nTable 4: BLEU scores using different initializations of\nthe XLM embedding layer. XLM is then trained on the\nrespective language pair and used to initialize a UNMT\nsystem. Both embeddings are aligned using VecMap.\nHow should static embeddings be integrated in\nthe MLM training? We explore different ways of\nincorporating the lexical knowledge of pretrained\ncross-lingual embeddings to the second, masked\nlanguage modeling stage of our approach (§2.2).\nSpeciﬁcally, we keep the aligned embeddings ﬁxed\n(frozen) during XLM training and compare the per-\nformance of the ﬁnal UNMT model to the proposed\n(ﬁne-tuned) method. We point out that, after we\ntransfer the trained MLM to an encoder-decoder\nmodel, all layers are trained for UNMT .\nTable 4 summarizes our results. The ﬁne-\ntuning approach, which is adopted in our proposed\nmethod, provides a higher performance both in En-\nMk and En-Sq, with the improvement being more\nevident in En-Sq. Our ﬁndings generally show that\nit is preferable to train the bilingual embeddings\ntogether with the rest of the model in the MLM step.\n6 Related Work\nArtetxe et al. (2018c); Lample et al. (2018a) ini-\ntialize UNMT models with word-by-word transla-\n177\ntions, based on a bilingual lexicon inducted in an\nunsupervised way by the same monolingual data,\nor simply with cross-lingual embeddings. Lam-\nple et al. (2018c) also use pretrained embeddings,\nlearned on joint monolingual corpora of the two\nlanguages of interest, to initialize the embedding\nlayer of the encoder-decoder. Lample and Con-\nneau (2019) remove pretrained embeddings from\nthe UNMT pipeline and align language distributions\nby simply pretraining a MLM on both languages,\nin order to learn a cross-lingual mapping. How-\never, it has been shown that this pretraining method\nprovides a weak alignment of the language distri-\nbutions (Ren et al., 2019). While that work iden-\ntiﬁed as a cause the lack of sharing n-gram level\ncross-lingual information, we address the lack of\ncross-lingual information at the lexical level.\nMoreover, most prior work on UNMT focuses\non languages with abundant, high-quality mono-\nlingual corpora. In low-resource scenarios though,\nespecially when the languages are not related, pre-\ntraining a cross-lingual MLM for unsupervised NMT\ndoes not yield good results (Guzmán et al., 2019;\nChronopoulou et al., 2020). We propose a method\nthat overcomes this issue by enhancing the MLM\nwith cross-lingual lexical-level representations.\nAnother line of work tries to enrich the repre-\nsentations of multilingual MLM s with additional\nknowledge (Wang et al., 2020; Pfeiffer et al., 2020)\nwithout harming the already-learned representa-\ntions. In our work, we identify lexical informa-\ntion as a source of knowledge that is missing from\nMLM s, especially when it comes to low-resource\nlanguages. Surprisingly, static embeddings, such\nas fastText, largely outperform representations ex-\ntracted by multilingual MLM s in terms of cross-\nlingual lexical alignment (Vuli´c et al., 2020). Mo-\ntivated by this, we aim to narrow the gap between\nthe lexical representations of bilingual MLM s and\nstatic embeddings, in order to achieve a higher\ntranslation quality, when transferring the MLM to\nan encoder-decoder UNMT model.\n7 Conclusion\nWe propose a method to improve the lexical ability\nof a Transformer encoder by initializing its em-\nbedding layer with pretrained cross-lingual embed-\ndings. The Transformer is trained for masked lan-\nguage modeling on the language pair of interest.\nAfter that, it is used to initialize an encoder/de-\ncoder model, which is trained for UNMT and out-\nperforms relevant baselines. Results conﬁrm our\nintuition that masked language modeling, which\nprovides contextual representations, beneﬁts from\ncross-lingual embeddings, which capture lexical-\nlevel information. In the future, we would like to in-\nvestigate whether lexical knowledge can be infused\nto multilingual MLM s. We would also like to exper-\niment with other schemes of training the MLM in\nterms of how the embedding layer is updated, such\nas regularizer annealing strategies, which would\nenable keeping the embeddings relatively ﬁxed, but\nstill allow for some limited training.\n8 Ethical Considerations\nIn this work, we propose a novel unsupervised neu-\nral machine translation approach, which is tailored\nto low-resource languages in terms of monolingual\ndata. We experiment with unsupervised translation\nbetween English, Albanian and Macedonian.\nFor English, we use high-quality data from news\narticles. The Albanian and Macedonian monolin-\ngual data originates from the OSCAR project (Or-\ntiz Suárez et al., 2019). The corpora are shufﬂed\nand stripped of all metadata. Therefore, the data\nshould not be easily attributable to speciﬁc indi-\nviduals. Nevertheless, the project offers easy ways\nto remove data upon request. The En-Sq and En-\nMk parallel development and test data are obtained\nfrom OPUS (Tiedemann, 2012) and consist of high-\nquality news articles.\nOur work is partly based on training type-level\nembeddings which are not computationally expen-\nsive. However, training cross-lingual masked lan-\nguage models requires signiﬁcant computational\nresources. To lower environmental impact, we do\nnot conduct hyper-parameter search and use well-\nestablished values for all hyper-parameters.\nAcknowledgments\nThis project has received funding from the Euro-\npean Research Council under the European Union’s\nHorizon 2020 research and innovation program\n(grant agreement # 640550). This work was also\nsupported by DFG (grant FR 2829/4-1). We thank\nKaterina Margatina, Giorgos Vernikos and Viktor\nHangya for their thoughtful comments/suggestions\nand valuable feedback.\n178\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018a. A robust self-learning method for fully un-\nsupervised cross-lingual mappings of word embed-\ndings. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 789–798, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018b. Unsupervised statistical machine transla-\ntion. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3632–3642.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2019.\nAn effective approach to unsupervised machine\ntranslation. In Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics,\npages 194–203. Association for Computational Lin-\nguistics.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018c. Unsupervised neural ma-\nchine translation. In International Conference on\nLearning Representations.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nAlexandra Chronopoulou, Dario Stojanovski, and\nAlexander Fraser. 2020. Reusing a Pretrained Lan-\nguage Model on Languages with Limited Corpora\nfor Unsupervised NMT. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 2703–2711, On-\nline. Association for Computational Linguistics.\nStephan Gouws, Yoshua Bengio, and Greg Corrado.\n2015. Bilbowa: Fast bilingual distributed represen-\ntations without word alignments. In Proceedings\nof the 32nd International Conference on Machine\nLearning, volume 37 of Proceedings of Machine\nLearning Research , pages 748–756, Lille, France.\nPMLR.\nFrancisco Guzmán, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato. 2019. The\nﬂores evaluation datasets for low-resource machine\ntranslation: Nepali–english and sinhala–english. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing and the Inter-\nnational Joint Conference on Natural Language Pro-\ncessing, pages 6100–6113.\nDan Hendrycks and Kevin Gimpel. 2017. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. ArXiv.\nPhilipp Koehn, Marcello Federico, Wade Shen,\nNicola Bertoldi, Ondrej Bojar, Chris Callison-Burch,\nBrooke Cowan, Chris Dyer, Hieu Hoang, Richard\nZens, et al. 2006. Open source toolkit for statisti-\ncal machine translation: Factored translation models\nand confusion network decoding. In Final Report of\nthe 2006 JHU Summer Workshop.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems , page\n7057–7067.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018a. Unsupervised\nmachine translation using monolingual corpora only.\nIn International Conference on Learning Represen-\ntations.\nGuillaume Lample, Alexis Conneau, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018b.\nWord translation without parallel data. In Interna-\ntional Conference on Learning Representations.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018c.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5039–5049.\nKelly Marchisio, Kevin Duh, and Philipp Koehn. 2020.\nWhen does unsupervised machine translation work?\nIn Proceedings of the Fifth Conference on Machine\nTranslation, pages 571–583, Online. Association for\nComputational Linguistics.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous Pipeline for Process-\ning Huge Corpora on Medium to Low Resource In-\nfrastructures. In Workshop on the Challenges in the\nManagement of Large Corpora.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nMaja Popovi ´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Conference on Ma-\nchine Translation: Research Papers , pages 186–\n191.\nOﬁr Press and Lior Wolf. 2017. Using the output\nembedding to improve language models. In Pro-\nceedings of the Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 157–163.\n179\nShuo Ren, Yu Wu, Shujie Liu, Ming Zhou, and Shuai\nMa. 2019. Explicit cross-lingual pre-training for\nunsupervised machine translation. In Proceedings\nof the Conference on Empirical Methods in Nat-\nural Language Processing and the International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 770–779. Association for\nComputational Linguistics.\nSebastian Ruder, Ivan Vuli ´c, and Anders Søgaard.\n2019. A survey of cross-lingual word embedding\nmodels. J. Artif. Int. Res., 65(1):569–630.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the An-\nnual Meeting of the Association for Computational\nLinguistics, pages 86–96.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural machine translation of rare words\nwith subword units. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1715–1725.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation (LREC), pages 2214–2218.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, page 5998–6008.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008. Extracting and\ncomposing robust features with denoising autoen-\ncoders. In Proceedings of the International Confer-\nence on Machine Learning, pages 1096–1103.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Probing\npretrained language models for lexical semantics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7222–7240, Online. Association for Computa-\ntional Linguistics.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-\nanjing Huang, Jianshu ji, Guihong Cao, Daxin Jiang,\nand Ming Zhou. 2020. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters.\n180\nA Appendix\nA.1 Datasets\nWe remove sentences longer than 100 words after\nBPE splitting. We split the data using the fastBPE\ncodebase3.\nA.2 Model Conﬁguration\nWe tie the embedding and output (projection) lay-\ners of both LM and NMT models (Press and Wolf,\n2017). We use a dropout rate of0.1 and GELU acti-\nvations (Hendrycks and Gimpel, 2017). We use the\ndefault parameters of Lample and Conneau (2019)\nin order to train our models.\nRegarding the runtimes for the En-Sq experi-\nments: the baseline XLM was trained for 3 days on\n8 GPUs while our approach for 6 days and 14h. The\nexperiment with freezing the embeddings provided\nfor faster training, 2 days and 17h. The three meth-\nods needed 23h, 21h, and 1d and 8h for the UNMT\npart, respectively. Fine-tuning with RE-LM took 2\ndays and 14h on 1 GPU and with our approach it\ntook 1 day and 5h. UNMT for these models took\n2 days and 11h, and 13h, respectively. We get a\ncheckpoint every 50K sentences processed by the\nmodel.\nA.3 Validation Scores of Results\nIn Table 5 we show the dev scores of the main re-\nsults, in terms of BLEU scores. This table extends\nTable 1 of the main paper.\nIn Table 6, we show the dev scores of the extra\nﬁne-tuning experiments we did for the analysis.\nThe table corresponds to Table 4 of the main paper.\nEn-Mk En-Sq\n← → ← →\nXLM - - 30.7 32.0\nlexically aligned XLM 24.6 23.3 31.9 33.8\nRE-LM 25.0 25.7 29.9 32.8\nlexically aligned RE-LM 25.3 26.6 29.5 30.3\nTable 5: UNMT BLEU scores on the development set.\nWe note that the dev scores are obtained using\ngreedy decoding, while the test scores are obtained\nwith beam search of size 5. We clarify that we\ntrain each NMT model using as training criterion\nthe validation BLEU score of the Sq, Mk→En di-\nrection, with a patience of 10. We speciﬁcally use\nthe multi-bleu.perl script from Moses.\n3https://github.com/glample/fastBPE\nAlignment Method En-Mk En-Sq\n← → ← →\nlexically alignedMLM\nfrozen embeddings 24.8 23.0 31.0 32.1\nﬁne-tuned embeddings (ours) 24.623.3 31.1 32.0\nTable 6: Development BLEU scores using different ini-\ntializations of the XLM embedding layer.\nA.4 Joint vs VecMapembeddings.\nUsing joint embeddings to initialize the MLM , be-\nfore training it on data from the respective language\nis less effective for UNMT . This is mostly the case\nfor En-Mk, since the two languages use a different\nalphabet (Latin and Cyrillic). In this case, simply\nlearning fastText embeddings on the concatenation\nof the two corpora is not useful, because the lan-\nguages do not have a big lexical overlap.\nAlignment Method En-Mk En-Sq\n← → ← →\njoint fastText 21.5 19.8 32.3 33.1\nV EC M AP 25.2 22.9 32.8 33.5\nTable 7: BLEU scores using different initializations of\nthe XLM embedding layer. XLM is then trained on the\nrespective language pair and used to initialize a UNMT\nsystem. Joint fastText embsrefers to jointly learned em-\nbeddings following Lample et al. (2018c).",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8166693449020386
    },
    {
      "name": "Machine translation",
      "score": 0.7851922512054443
    },
    {
      "name": "Natural language processing",
      "score": 0.7563323974609375
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7213396430015564
    },
    {
      "name": "Lexicon",
      "score": 0.5894777178764343
    },
    {
      "name": "Task (project management)",
      "score": 0.5650540590286255
    },
    {
      "name": "Bilingual dictionary",
      "score": 0.48437437415122986
    },
    {
      "name": "Baseline (sea)",
      "score": 0.47191956639289856
    },
    {
      "name": "Parallel corpora",
      "score": 0.471760094165802
    },
    {
      "name": "Language model",
      "score": 0.4192754626274109
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    }
  ],
  "cited_by": 2
}