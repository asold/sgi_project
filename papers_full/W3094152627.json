{
  "title": "Rethinking embedding coupling in pre-trained language models",
  "url": "https://openalex.org/W3094152627",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2747279576",
      "name": "Chung, Hyung Won",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222913935",
      "name": "Févry, Thibault",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226052266",
      "name": "Tsai, Henry",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221359262",
      "name": "Johnson, Melvin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202039502",
      "name": "Ruder, Sebastian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2919290281",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3006130287",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3023585950",
    "https://openalex.org/W3035317797",
    "https://openalex.org/W2964007535",
    "https://openalex.org/W2176085882",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2947915358",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3030045039",
    "https://openalex.org/W2953958347",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2112184938",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034709122",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3088675891",
    "https://openalex.org/W3085479580",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W2142625445",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W2508316494",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2975429091"
  ],
  "abstract": "We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art pre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to significantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By reallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on standard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that allocating additional capacity to the output embedding provides benefits to the model that persist through the fine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the number of parameters at the fine-tuning stage.",
  "full_text": "Preprint. Under review.\nRETHINKING EMBEDDING COUPLING\nIN PRE -TRAINED LANGUAGE MODELS\nHyung Won Chung∗†\nGoogle Research\nhwchung@google.com\nThibault F´evry∗†\nthibaultfevry@gmail.com\nHenry Tsai\nGoogle Research\nhenrytsai@google.com\nMelvin Johnson\nGoogle Research\nmelvinp@google.com\nSebastian Ruder\nDeepMind\nruder@google.com\nABSTRACT\nWe re-evaluate the standard practice of sharing weights between input and out-\nput embeddings in state-of-the-art pre-trained language models. We show that\ndecoupled embeddings provide increased modeling ﬂexibility, allowing us to sig-\nniﬁcantly improve the efﬁciency of parameter allocation in the input embedding\nof multilingual models. By reallocating the input embedding parameters in the\nTransformer layers, we achieve dramatically better performance on standard nat-\nural language understanding tasks with the same number of parameters during\nﬁne-tuning. We also show that allocating additional capacity to the output embed-\nding provides beneﬁts to the model that persist through the ﬁne-tuning stage even\nthough the output embedding is discarded after pre-training. Our analysis shows\nthat larger output embeddings prevent the model’s last layers from overspecializ-\ning to the pre-training task and encourage Transformer representations to be more\ngeneral and more transferable to other tasks and languages. Harnessing these ﬁnd-\nings, we are able to train models that achieve strong performance on the XTREME\nbenchmark without increasing the number of parameters at the ﬁne-tuning stage.\n1 I NTRODUCTION\nThe performance of models in natural language processing (NLP) has dramatically improved in\nrecent years, mainly driven by advances in transfer learning from large amounts of unlabeled data\n(Howard & Ruder, 2018; Devlin et al., 2019). The most successful paradigm consists of pre-training\na large Transformer (Vaswani et al., 2017) model with a self-supervised loss and ﬁne-tuning it on\ndata of a downstream task (Ruder et al., 2019). Despite its empirical success, inefﬁciencies have\nbeen observed related to the training duration (Liu et al., 2019b), pre-training objective (Clark et al.,\n2020b), and training data (Conneau et al., 2020a), among others. In this paper, we reconsider a\nmodeling assumption that may have a similarly pervasive practical impact: the coupling of input\nand output embeddings1 in state-of-the-art pre-trained language models.\nState-of-the-art pre-trained language models (Devlin et al., 2019; Liu et al., 2019b) and their multi-\nlingual counterparts (Devlin et al., 2019; Conneau et al., 2020a) have inherited the practice of em-\nbedding coupling from their language model predecessors (Press & Wolf, 2017; Inan et al., 2017).\nHowever, in contrast to their language model counterparts, embedding coupling in encoder-only\npre-trained models such as Devlin et al. (2019) is only useful during pre-training since output em-\nbeddings are generally discarded after ﬁne-tuning.2 In addition, given the willingness of researchers\nto exchange additional compute during pre-training for improved downstream performance (Raffel\n∗equal contribution\n†Work done as a member of the Google AI Residency Program.\n1Output embedding is sometimes referred to as “output weights”, i.e., the weight matrix in the output\nprojection in a language model.\n2We focus on encoder-only models, and do not consider encoder-decoder models like T5 (Raffel et al.,\n2020) where none of the embedding matrices are discarded after pre-training. Output embeddings may also be\n1\narXiv:2010.12821v1  [cs.CL]  24 Oct 2020\nPreprint. Under review.\nTable 1: Overview of the number of parameters in (coupled) embedding matrices of state-of-the-art\nmultilingual (top) and monolingual (bottom) models with regard to overall parameter budget. |V|:\nvocabulary size. N, Nemb: number of parameters in total and in the embedding matrix respectively.\nModel Languages |V| N N emb %Emb.\nmBERT (Devlin et al., 2019) 104 120k 178M 92M 52%\nXLM-RBase (Conneau et al., 2020a) 100 250k 270M 192M 71%\nXLM-RLarge (Conneau et al., 2020a) 100 250k 550M 256M 47%\nBERTBase (Devlin et al., 2019) 1 30k 110M 23M 21%\nBERTLarge (Devlin et al., 2019) 1 30k 335M 31M 9%\net al., 2020; Brown et al., 2020) and the fact that pre-trained models are often used for inference mil-\nlions of times (Wolf et al., 2019), pre-training-speciﬁc parameter savings are less important overall.\nOn the other hand, tying input and output embeddings constrains the model to use the same dimen-\nsionality for both embeddings. This restriction limits the researcher’s ﬂexibility in parameterizing\nthe model and can lead to allocating too much capacity to the input embeddings, which may be\nwasteful. This is a problem particularly for multilingual models, which require large vocabularies\nwith high-dimensional embeddings that make up between 47–71% of the entire parameter budget\n(Table 1), suggesting an inefﬁcient parameter allocation.\nIn this paper, we systematically study the impact of embedding coupling on state-of-the-art pre-\ntrained language models, focusing on multilingual models. First, we observe that while na ¨ıvely\ndecoupling the input and output embedding parameters does not consistently improve downstream\nevaluation metrics, decoupling their shapes comes with a host of beneﬁts. In particular, it allows\nus to independently modify the input and output embedding dimensions. We show that the input\nembedding dimension can be safely reduced without affecting downstream performance. Since the\noutput embedding is discarded after pre-training, we can increase its dimension, which improves\nﬁne-tuning accuracy and outperforms other capacity expansion strategies. By reinvesting saved\nparameters to the width and depth of the Transformer layers, we furthermore achieve signiﬁcantly\nimproved performance over a strong mBERT (Devlin et al., 2019) baseline on multilingual tasks\nfrom the XTREME benchmark (Hu et al., 2020). Finally, we combine our techniques in a Rebalanced\nmBERT (RemBERT) model that outperforms XLM-R (Conneau et al., 2020a), the state-of-the-art\ncross-lingual model while having been pre-trained on 3.5×fewer tokens and 10 more languages.\nWe thoroughly investigate reasons for the beneﬁts of embedding decoupling. We observe that an in-\ncreased output embedding size enables a model to improve on the pre-training task, which correlates\nwith downstream performance. We also ﬁnd that it leads to Transformers that are more transferable\nacross tasks and languages—particularly for the upper-most layers. Overall, larger output embed-\ndings prevent the model’s last layers from over-specializing to the pre-training task (Zhang et al.,\n2020; Tamkin et al., 2020), which enables training of more general Transformer models.\n2 R ELATED WORK\nEmbedding coupling Sharing input and output embeddings in neural language models was pro-\nposed to improve perplexity and motivated based on embedding similarity (Press & Wolf, 2017) as\nwell as by theoretically showing that the output probability space can be constrained to a subspace\ngoverned by the embedding matrix for a restricted case (Inan et al., 2017). Embedding coupling is\nalso common in neural machine translation models where it reduces model complexity (Firat et al.,\n2016) and saves memory (Johnson et al., 2017), in recent state-of-the-art language models (Melis\net al., 2020), as well as all pre-trained models we are aware of (Devlin et al., 2019; Liu et al., 2019b).\nTransferability of representations Representations of large pre-trained models in computer vi-\nsion and NLP have been observed to transition from general to task-speciﬁc from the ﬁrst to the\nuseful for domain-adaptive pre-training (Howard & Ruder, 2018; Gururangan et al., 2020), probing (Elazar &\nGoldberg, 2019), and tasks that can be cast in the pre-training objective (Amrami & Goldberg, 2019).\n2\nPreprint. Under review.\nlast layer (Yosinski et al., 2014; Howard & Ruder, 2018; Liu et al., 2019a). In Transformer models,\nthe last few layers have been shown to become specialized to the MLM task and—as a result—less\ntransferable (Zhang et al., 2020; Tamkin et al., 2020).\nMultilingual models Recent multilingual models are pre-trained on data covering around 100\nlanguages using a subword vocabulary shared across all languages (Devlin et al., 2019; Pires et al.,\n2019; Conneau et al., 2020a). In order to achieve reasonable performance for most languages, these\nmodels need to allocate sufﬁcient capacity for each language, known as the curse of multilinguality\n(Conneau et al., 2020a; Pfeiffer et al., 2020). As a result, such multilingual models have large vocab-\nularies with large embedding sizes to ensure that tokens in all languages are adequately represented.\nEfﬁcient models Most work on more efﬁcient pre-trained models focuses on pruning or distilla-\ntion (Hinton et al., 2015). Pruning approaches remove parts of the model, typically attention heads\n(Michel et al., 2019; V oita et al., 2019) while distillation approaches distill a large pre-trained model\ninto a smaller one (Sun et al., 2020). Distillation can be seen as an alternative form of allocating\npre-training capacity via a large teacher model. However, distilling a pre-trained model is expensive\n(Sanh et al., 2019) and requires overcoming architecture differences and balancing training data and\nloss terms (Mukherjee & Awadallah, 2020). Our proposed methods are simpler and complementary\nto distillation as they can improve the pre-training of compact student models (Turc et al., 2019).\n3 E XPERIMENTAL METHODOLOGY\nEfﬁciency of models has been measured along different dimensions, from the number of ﬂoating\npoint operations (Schwartz et al., 2019) to their runtime (Zhou et al., 2020). We follow previous\nwork (Sun et al., 2020) and compare models in terms of their number of parameters during ﬁne-\ntuning (see Appendix A.1 for further justiﬁcation of this setting). For completeness, we generally\nreport the number of pre-training (PT) and ﬁne-tuning (FT) parameters.\nBaseline Our baseline has the same architecture as multilingual BERT (mBERT; Devlin et al.,\n2019). It consists of 12 Transformer layers with a hidden size H of 768 and 12. Input and output\nembeddings are coupled and have the same dimensionality E as the hidden size, i.e. Eout = Ein =\nH. The total number of parameters during pre-training and ﬁne-tuning is 177M (see Appendix\nA.2 for further details). We train variants of this model that differ in certain hyper-parameters but\notherwise are trained under the same conditions to ensure a fair comparison.\nTasks For our experiments, we employ tasks from the XTREME benchmark (Hu et al., 2020) that\nrequire ﬁne-tuning, including the XNLI (Conneau et al., 2018), NER (Pan et al., 2017), PAWS-X\n(Yang et al., 2019), XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), and TyDiQA-GoldP\n(Clark et al., 2020a) datasets. We provide details for them in Appendix A.4. We average results\nacross three ﬁne-tuning runs and evaluate on the dev sets unless otherwise stated.\n4 E MBEDDING DECOUPLING REVISITED\nNa¨ıve decoupling Embeddings make up a large fraction of the parameter budget in state-of-the-\nart multilingual models (see Table 1). We now study the effect of embedding decoupling on such\nmodels. In Table 2, we show the impact of decoupling the input and output embeddings in our\nbaseline model with coupled embeddings. Na¨ıvely decoupling the output embedding matrix slightly\nimproves the performance as evidenced by a 0.4 increase on average. However, the gain is not uni-\nformly observed in all tasks. Overall, these results suggest that decoupling the embedding matrices\nna¨ıvely while keeping the dimensionality ﬁxed does not greatly affect the performance of the model.\nWhat is more important, however, is that decoupling the input and output embeddings decouples the\nshapes, endowing signiﬁcant modeling ﬂexibility, which we investigate in the following.\nInput vs output embeddings Decoupling input and output embeddings allows us to ﬂexibly\nchange the dimensionality of both matrices and to determine which one is more important for good\ntransfer performance of the model. To this end, we compare the performance of a model with\n3\nPreprint. Under review.\nTable 2: Effect of decoupling the input and output embedding matrices on performance on multiple\ntasks in XTREME . PT: Pre-training. FT: Fine-tuning.\n# PT # FT XNLI NER PAWS-X XQuAD MLQA TyDi-GoldP Avgparams params Acc F1 Acc EM/F1 EM/F1 EM/F1\nCoupled 177M 177M 70.7 69.2 85.3 46.2/63.2 37.3/53.1 40.7/56.7 62.3\nDecoupled 269M 177M 71.3 68.9 85.0 46.9/63.8 37.3/53.1 42.8/58.1 62.7\nTable 3: Performance of models with a large input and small output embedding size and vice versa.\n# PT # FT XNLI NER PAWS-X XQuAD MLQA TyDi-GoldP Avgparams params Acc F1 Acc EM/F1 EM/F1 EM/F1\nEin = 768, Eout= 128 192M 177M 70.0 68.3 84.3 42.0/ 60.8 34.7/50.9 35.2/52.2 60.1\nEin = 128, Eout= 768 192M 100M 70.4 67.6 84.9 43.9 /60.0 34.6/49.5 37.8/51.0 60.2\nEin = 768, Eout = 128 to that of a model with Ein = 128, Eout = 7683. During ﬁne-tuning, the\nlatter model has 43% fewer parameters. We show the results in Table 3. Surprisingly, the model pre-\ntrained with a larger output embedding size slightly outperforms the comparison method on average\ndespite having 77M fewer parameters during ﬁne-tuning.4\nReducing the input embedding dimension saves a signiﬁcant number of parameters at a noticeably\nsmaller cost to accuracy than reducing the output embedding size. In light of this, the parameter\nallocation of multilingual models (see Table 1) seems particularly inefﬁcient. For a multilingual\nmodel with coupled embeddings, reducing the input embedding dimension to save parameters as\nproposed by Lan et al. (2020) is very detrimental to performance (see Appendix A.5 for details).\nThe results in this section indicate that the output embedding plays an important role in the transfer-\nability of pre-trained representations. For multilingual models in particular, a small input embedding\ndimension frees up a signiﬁcant number of parameters at a small cost to performance. In the next\nsection, we study how to improve the performance of a model by resizing embeddings and layers.\n5 E MBEDDING AND LAYER RESIZING FOR MORE EFFICIENT FINE -TUNING\nIncreasing the output embedding size In §4, we observed that reducing Eout hurts performance\non the ﬁne-tuning tasks, suggesting Eout is important for transferability. Motivated by this result,\nwe study the opposite scenario, i.e., whether increasing Eout beyond H improves the performance.\nWe experiment with an output embedding sizeEout in the range {128, 768, 3072}while keeping the\ninput embedding size Ein = 128 and all other parts of the model the same.\nWe show the results in Table 4. In all of the tasks we consider, increasing Eout monotonically im-\nproves the performance. The improvement is particularly impressive for the more complex question\nanswering datasets. It is important to note that during ﬁne-tuning, all three models have the exact\nsame sizes for Ein and H. The only difference among them is the output embedding, which is dis-\ncarded after pre-training. These results show that the effect of additional capacity during pre-training\npersists through the ﬁne-tuning stage even if the added capacity is discarded after pre-training. We\nperform an extensive analysis on this behavior in §6. We show results with an English BERT Base\nmodel in Appendix A.6, which show the same trend.\nAdding capacity via layers We investigate alternative ways of adding capacity during pre-training\nsuch as increasing the number of layers and discarding them after pre-training. For a fair comparison\nwith the Eout = 768 model, we add 11 additional layers (total of 23) and drop the 11 upper layers\nafter pre-training. This setting ensures that both models have the same pre-training and ﬁne-tuning\nparameters. We show the results in Table 5. The model with additional layers performs poorly on the\n3We linearly project the embeddings from Ein to H and from H to Eout.\n4We observe the same trend if we control for the number of trainable parameters during ﬁne-tuning by\nfreezing the input embedding parameters.\n4\nPreprint. Under review.\nTable 4: Effect of an increased output embedding size Eout on tasks in XTREME (Ein = 128).\n# PT # FT XNLI NER PAWS-X XQuAD MLQA TyDi-GoldP Avgparams params Acc F1 Acc EM/F1 EM/F1 EM/F1\nEout = 128 115M 100M 68.1 65.2 83.3 38.6/54.8 30.9/45.2 32.2/44.2 56.6\nEout = 768 192M 100M 70.4 67.6 84.9 43.9/60.0 34.6/49.5 37.8/51.0 60.2\nEout = 3072 469M 100M 71.1 68.1 85.1 45.3/63.3 37.2/53.1 39.4/54.7 61.8\nTable 5: Effect of additional capacity via more Transformer layers during pre-training (Ein = 128).\n# PT # FT XNLI NER PAWS-X XQuAD MLQA TyDi-GoldP Avgparams params Acc F1 Acc EM/F1 EM/F1 EM/F1\nEout = 768 192M 100M 70.4 67.6 84.9 43.9/60.0 34.6/49.5 37.8/51.0 60.2\n11 add. layers 193M 100M 71.2 67.3 85.0 38.8/55.5 31.4/46.6 31.3/45.5 58.0\nquestion answering tasks, likely because the top layers contain useful semantic information (Tenney\net al., 2019). In addition to higher performance, increasing Eout relies only a more expensive dense\nmatrix multiplication, which is highly optimized on typical accelerators and can be scaled up more\neasily with model parallelism (Shazeer et al., 2018) because of small additional communication\ncost. We thus focus on increasing Eout to expand pre-training capacity and leave an exploration of\nalternative strategies to future work.\nReinvesting input embedding parameters Reducing Ein from 768 to 128 reduces the number of\nparameters from 177M to 100M. We redistribute these 77M parameters for the model with Eout =\n768 to add capacity where it might be more useful by increasing the width or depth of the model.\nSpeciﬁcally, we 1) increase the hidden dimension H of the Transformer layers from 768 to 1024 5\nand 2) increase the number of Transformer layers (L) from 12 to 23 at the same Hto obtain models\nwith similar number of parameters during ﬁne-tuning.\nTable 6 shows the results for these two strategies. Reinvesting the input embedding parameters in\nboth Hand Limproves performance on all tasks while increasing the number of Transformer layers\nLresults in the best performance, with an average improvement of 3.9 over the baseline model with\ncoupled embeddings and the same number of ﬁne-tuning parameters overall.\nA rebalanced mBERT We ﬁnally combine and scale up our techniques to design a rebalanced\nmBERT model that outperforms the current state-of-the-art unsupervised model, XLM-R (Conneau\net al., 2020a). As the performance of Transformer-based models strongly depends on their number of\nparameters (Raffel et al., 2020), we propose a Rebalanced mBERT (RemBERT) model that matches\nXLM-R’s number of ﬁne-tuning parameters (559M) while using a reduced embedding size, resized\nlayers, and more effective capacity during pre-training. The model has a vocabulary size of 250k,\nEin = 256, Eout = 1536, and 32 layers with 1152 dimensions and 18 attention heads per layer and\nwas trained on data covering 110 languages. We provide further details in Appendix A.7.\nWe compare RemBERT to XLM-R and the best-performing models on the XTREME leaderboard\nin Table 16 (see Appendix A.8 for the per-task results). 6 The models in the ﬁrst three rows use\nadditional task or translation data for ﬁne-tuning, which signiﬁcantly boosts performance (Hu et al.,\n2020). XLM-R and RemBERT are the only two models that are ﬁne-tuned using only the English\ntraining data of the corresponding task. XLM-R was trained with a batch size of213 sequences each\nwith 29 tokens and 1.5M steps (total of 6.3T tokens). In comparison, RemBERT is trained with\n211 sequences of 29 tokens for 1.76M steps (1.8T tokens). Even though it was trained with 3.5×\nfewer tokens and has 10 more languages competiting for the model capacity, RemBERT outperforms\nXLM-R on all tasks we considered. This strong result suggests that our proposed methods are\nalso effective at scale. We will release the pre-trained model checkpoint and the source code for\nRemBERT in order to promote reproducibility and share the pre-training cost with other researchers.\n5We choose 1024 dimensions to optimize efﬁcient use of our accelerators.\n6We do not consider retrieval tasks as they require intermediate task data (Phang et al., 2020).\n5\nPreprint. Under review.\nTable 6: Effect of reinvesting the input embedding parameters to increase the hidden dimension H\nand number Lof Transformer layers on XTREME tasks. Ein = 128,Eout = 768 model is included\nfor an ablation study.\n# PT # FT XNLI NER PAWS-X XQuAD MLQA TyDi-GoldP Avgparams params Acc F1 Acc EM/F1 EM/F1 EM/F1\nBaseline 177M 177M 70.7 69.2 85.3 46.2/63.2 37.3/53.1 40.7/56.7 62.3\nEin = 128,Eout= 768 192M 100M 70.4 67.6 84.9 43.9/60.0 34.6/49.5 37.8/51.0 60.2\nReinvested inH 260M 168M 72.8 69.2 85.6 50.2/67.2 40.7/56.4 44.8/60.0 64.5\nReinvested inL 270M 178M 73.6 71.0 86.7 51.7/68.8 42.4/58.2 48.2/62.9 66.2\nTable 7: Comparison of our model to other models on the XTREME leaderboard. Details about\nVECO are due to communication with the authors.\n# PTparams# FTparams\nAdd.taskdata\nTrans-lationdata\nSentence-pair Structured QuestionLangs Classiﬁcation Prediction Answering AvgAcc F1 EM/F1\nModels ﬁne-tuned on translations or additional task data\nSTiLTs (Phang et al., 2020) 559M 559M 100 ✓ 83.9 69.4 67.2 73.5FILTER (Fang et al., 2020) 559M 559M 100 ✓ 87.5 71.9 68.5 76.0VECO (Anonymous, 2021) 662M 662M 50 ✓ 87.0 70.4 68.0 75.1\nModels ﬁne-tuned only on English task data\nXLM-R (Conneau et al., 2020a) 559M 559M 100 82.8 69.0 62.3 71.4RemBERT (ours) 995M 575M 110 84.2 73.3 68.6 75.4\n6 O N THE IMPORTANCE OF THE OUTPUT EMBEDDING SIZE\nWe carefully design a set of experiments to analyze the impact of an increased output embedding\nsize on various parts of the model. We study the nature of the decoupled input and output repre-\nsentations (§6.1) and the transferability of the Transformer layers with regard to task-speciﬁc (§6.2)\nand language-speciﬁc knowledge (§6.3).\n6.1 N ATURE OF INPUT AND OUTPUT EMBEDDING REPRESENTATIONS\nWe ﬁrst investigate to what extent the representations of decoupled input and output embeddings\ndiffer based on word embedding association tests (Caliskan et al., 2017). Similar to (Press & Wolf,\n2017), for a given pair of words, we evaluate the correlation between human similarity judgements\nof the strength of the relationship and the dot product of the word embeddings. We evaluate on MEN\n(Bruni et al., 2014), MTurk771 (Halawi et al., 2012), Rare-Word (Luong et al., 2013), SimLex999\n(Hill et al., 2015), and Verb-143 (Baker et al., 2014). As our model uses subwords, we average the\ntoken representations for words with multiple subwords.\nWe show the results in Table 8. In the ﬁrst two rows, we can observe that the input embedding of\nthe decoupled model performs similarly to the embeddings of the coupled model while the output\nembeddings have lower scores. 7 We note that higher scores are not necessarily desirable as they\nonly measure how well the embedding captures semantic similarity at the lexical level. Focusing on\nthe difference in scores, we can observe that the input embedding learns representations that capture\nsemantic similarity in contrast to the decoupled output embedding. At the same time, the decoupled\nmodel achieves higher performance in masked language modeling.\nThe last three rows of Table 8 show that as Eout increases, the difference in the input and output\nembedding increases as well. With additional capacity, the output embedding progressively learns\nrepresentations that differ more signiﬁcantly from the input embedding. We also observe that the\nMLM accuracy increases with Eout. Collectively, the results in Table 8 suggest that with increased\n7This is opposite from what Press & Wolf (2017) observed in 2-layer LSTMs. They ﬁnd that performance\nof the output embedding is similar to the embedding of a coupled model. This difference is plausible as the\ninformation encoded in large Transformers changes signiﬁcantly throughout the model (Tenney et al., 2019).\n6\nPreprint. Under review.\nTable 8: Results on word embedding association tests for the input (I) and output (O) embeddings\nof models (left) and the models’ masked language modeling performance (right). The ﬁrst two\nrows show the performance of coupled and decoupled embeddings with the same embedding size\nEin = Eout = 768. The last three rows show the performance as we increase the output embedding\nsize with Ein = 128.\nMEN MTurk771 Rare-Word Simlex999 Verb-143\nI O I O I O I O I O\nCoupled 40.8 37.5 25.0 20.1 56.0\nDecoupled 39.2 27.7 37.5 24.3 24.0 12.2 17.6 16.1 59.4 43.9\nEout = 128 40.7 36.6 37.7 32.8 23.6 16.4 17.5 17.3 48.9 46.4\nEout = 768 38.6 27.8 35.2 23.9 22.6 11.5 19.7 15.6 50.6 45.5\nEout = 3072 40.1 10.8 36.2 8.8 22.6 -1.2 18.9 13.0 43.3 19.5\nMLM acc.\nCoupled 61.1\nDecoupled 61.6\nEout = 128 59.0\nEout = 768 60.7\nEout = 3072 62.3\ncapacity, the output embeddings learn representations that are worse at capturing traditional semantic\nsimilarity (which is purely restricted to the lexical level) while being more specialized to the MLM\ntask (which requires more contextual representations). Decoupling embeddings thus give the model\nthe ﬂexibility to avoid encoding relationships in its output embeddings that may not be useful for\nits pre-training task. As pre-training performance correlates well with downstream performance\n(Devlin et al., 2019), forcing output embeddings to encode lexical information can hurt the latter.\n6.2 C ROSS -TASK TRANSFERABILITY OF TRANSFORMER LAYER REPRESENTATIONS\nWe investigate to what extent more capacity in the output embeddings during pre-training reduces\nthe MLM-speciﬁc burden on the Transformer layers and hence prevents them from over-specializing\nto the MLM task.\nDropping the last few layers We ﬁrst study the impact of an increased output embedding size\non the transferability of the last few layers. Previous work (Zhang et al., 2020; Tamkin et al.,\n2020) randomly reinitialized the last few layers to investigate their transferability. However, those\nparameters are still present during ﬁne-tuning. We propose a more aggressive pruning scheme where\nwe completely remove the last few layers. This setting demonstrates more drastically whether a\nmodel’s upper layers are over-specialized to the pre-training task by assessing whether performance\ncan be improved with millions fewer parameters.8\nWe show the performance of models with 8–12 remaining layers (removing up to 4 of the last layers)\nfor different output embedding sizesEout on XNLI in Figure 1. For bothEout = 128 and Eout = 768,\nremoving the last layer improves performance. In other words, the model performs better even with\n7.1M fewer parameters. With Eout = 128, the performance remains similar when removing the last\nfew layers, which suggests that the last few layers are not critical for transferability.\nAs we increase Eout, the last layers become more transferable. With Eout = 768, removing more\nthan one layer results in a sharp reduction in performance. Finally when Eout = 3072, every layer is\nuseful and removing any layer worsens the performance. This analysis demonstrates that increasing\nEout improves the transferability of the representations learned by the last few Transformer layers.\nProbing analysis We further study whether an increased output embedding size improves the\ngeneral natural language processing ability of the Transformer. We employ the probing analysis\nof Tenney et al. (2019) and the mix probing strategy where a 2-layer dense network is trained on\ntop of a weighted combination of the 12 Transformer layers. We evaluate performance with regard\nto core NLP concepts including part-of-speech tagging (POS), constituents (Consts.), dependencies\n(Deps.), entities, semantic role labeling (SRL), coreference (Coref.), semantic proto-roles (SPR),\nand relations (Rel.). For a thorough description of the task setup, see Tenney et al. (2019).9\n8Each Transformer layer with H = 768has about 7.1M parameters.\n9The probing tasks are in English while our encoder is multilingual.\n7\nPreprint. Under review.\n8 9 10 11 12\nNumber of layers\n68\n69\n70\n71XNLI accuracy\nEout = 3072\nEout = 768\nEout = 128\nFigure 1: XNLI accuracy with the last layers\nremoved. Larger Eout improves transferability.\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n30\n40\n50\n60\n70\n80Match accuracy [%] Eout = 3072\nEout = 768\nEout = 128\nFigure 2: Nearest-neighbor English-to-German\ntranslation accuracy of each layer.\nTable 9: Probing analysis of Tenney et al. (2019) with mix strategy.\n# PT params # FT params POS Const. Deps. Entities SRL Coref. O Coref. W SPR1 SPR2 Rel. Avg\nEout= 128 115M 100M 96.7 87.9 94.3 93.7 91.7 95.0 67.2 83.0 82.7 77.0 86.9Eout= 768 192M 100M 96.7 87.9 94.4 94.0 91.8 95.0 67.0 83.1 82.8 78.6 87.1Eout= 3072 469M 100M 96.8 88.0 94.5 94.2 92.0 95.3 67.6 84.1 82.6 78.9 87.4\nWe show the results of the probing analysis in Table 9. As we increase Eout, the model improves\nacross all tasks, even though the number of parameters is the same. This demonstrates that increasing\nEout enables the Transformer layers to learn more general representations.10\n6.3 C ROSS -LINGUAL TRANSFERABILITY OF TRANSFORMER LAYER REPRESENTATIONS\nSo far, our analyses were not specialized to multilingual models. Unlike monolingual models, multi-\nlingual models have another dimension of transferability: cross-lingual transfer, the ability to trans-\nfer knowledge from one language to another.\nPrevious work (Pires et al., 2019; Artetxe et al., 2020) has found that MLM on multilingual data\nencourages cross-lingual alignment of representations without explicit cross-lingual supervision.\nWhile it has been shown that multilingual models learn useful cross-lingual representations, over-\nspecialization to the pre-training task may result in higher layers being less cross-lingual and fo-\ncusing on language-speciﬁc phenomena necessary for predicting the next word in a given language.\nTo investigate to what extent this is the case and whether increasing Eout improves cross-lingual\nalignment, we evaluate the model’s nearest neighbour translation accuracy (Pires et al., 2019) on\nEnglish-to-German translation (see Appendix A.9 for a description of the method).\nWe show the nearest neighbor translation accuracy for each layer in Figure 2. As Eout increases,\nwe observe that a) the Transformer layers become more language-agnostic as evidenced by higher\naccuracy and b) the language-agnostic representation is maintained to a higher layer as indicated by a\nﬂatter slope from layer 7 to 11. In all cases, the last layer is less language-agnostic than the previous\none. The sharp drop in performance after layer 8 at Eout = 128 is in line with previous results on\ncross-lingual retrieval (Pires et al., 2019; Hu et al., 2020) and is partially mitigated by an increased\nEout. In sum, not only does a larger output embedding size improve cross-task transferability but it\nalso helps with cross-lingual alignment and thereby cross-lingual transfer on downstream tasks.\n10In (Tenney et al., 2019), going from a BERT-base to a BERT-large model (with 3× more parameters)\nimproves performance on average by 1.1 points, compared to our improvement of 0.5 points without increasing\nthe number of ﬁne-tuning parameters.\n8\nPreprint. Under review.\n7 C ONCLUSION\nWe have assessed the impact of embedding coupling in pre-trained language models. We have iden-\ntiﬁed the main beneﬁt of decoupled embeddings to be the ﬂexibility endowed by decoupling their\nshapes. We showed that input embeddings can be safely reduced and that larger output embeddings\nand reinvesting saved parameters lead to performance improvements. Our rebalanced multilingual\nBERT (RemBERT) outperforms XLM-R with the same number of ﬁne-tuning parameters while\nhaving been trained on 3.5×fewer tokens. Overall, we found that larger output embeddings lead to\nmore transferable and more general representations, particularly in a Transformer’s upper layers.\nREFERENCES\nRoee Aharoni, Melvin Johnson, and Orhan Firat. Massively Multilingual Neural Machine Transla-\ntion. In Proceedings of NAACL 2019, 2019.\nAsaf Amrami and Yoav Goldberg. Towards better substitution-based word sense induction. arXiv\npreprint arXiv:1905.12598, 2019.\nAnonymous. {VECO}: Variable encoder-decoder pre-training for cross-lingual understanding and\ngeneration. In Submitted to International Conference on Learning Representations , 2021. URL\nhttps://openreview.net/forum?id=YjNv-hzM8BE. under review.\nMikel Artetxe and Holger Schwenk. Massively Multilingual Sentence Embeddings for Zero-Shot\nCross-Lingual Transfer and Beyond. Transactions of the ACL 2019, 2019.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the Cross-lingual Transferability of Mono-\nlingual Representations. In Proceedings of ACL 2020, 2020.\nSimon Baker, Roi Reichart, and Anna Korhonen. An unsupervised model for instance level subcat-\negorization acquisition. In Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pp. 278–289, 2014.\nOndˇrej Bojar, Yvette Graham, Amir Kamran, and Milo ˇs Stanojevi´c. Results of the wmt16 metrics\nshared task. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared\nTask Papers, pp. 199–231, 2016.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCand lish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv\ne-prints, art. arXiv:2005.14165, May 2020.\nElia Bruni, Nam-Khanh Tran, and Marco Baroni. Multimodal distributional semantics. Journal of\nArtiﬁcial Intelligence Research, 49:1–47, 2014.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from\nlanguage corpora contain human-like biases. Science, 356(6334):183–186, 2017.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev,\nand Jennimaria Palomaki. TyDi QA: A Benchmark for Information-Seeking Question Answer-\ning in Typologically Diverse Languages. In Transactions of the Association of Computational\nLinguistics, 2020a.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. ELECTRA: Pre-\ntraining Text Encoders as Discriminators Rather Than Generators. In Proceedings of ICLR 2020,\n2020b.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger\nSchwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In\nProceedings of EMNLP 2018, pp. 2475–2485, 2018.\n9\nPreprint. Under review.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzm ´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-\nsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , pp. 8440–8451, Online, July 2020a.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https:\n//www.aclweb.org/anthology/2020.acl-main.747.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. Emerging cross-\nlingual structure in pretrained language models. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics , pp. 6022–6034, Online, July 2020b. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.acl-main.536. URL https://www.\naclweb.org/anthology/2020.acl-main.536.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\n//www.aclweb.org/anthology/N19-1423.\nYanai Elazar and Yoav Goldberg. oLMpics - On what Language Model Pre-training Captures.arXiv\npreprint arXiv:1912.13283, 2019.\nYuwei Fang, Shuohang Wang, Zhe Gan, Siqi Sun, and Jingjing Liu. FILTER: An Enhanced Fusion\nMethod for Cross-lingual Language Understanding. arXiv preprint arXiv:2009.05166, 2020.\nOrhan Firat, Baskaran Sankaran, Yaser Al-onaizan, Fatos T. Yarman Vural, and Kyunghyun Cho.\nZero-Resource Translation with Multi-Lingual Neural Machine Translation. In Proceedings of\nEMNLP 2016, pp. 268–277, 2016.\nSuchin Gururangan, Ana Marasovi ´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks. In\nProceedings of ACL 2020, 2020.\nGuy Halawi, Gideon Dror, Evgeniy Gabrilovich, and Yehuda Koren. Large-scale learning of word\nrelatedness with constraints. In Proceedings of the 18th ACM SIGKDD international conference\non Knowledge discovery and data mining, pp. 1406–1414, 2012.\nFelix Hill, Roi Reichart, and Anna Korhonen. Simlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguistics, 41(4):665–695, 2015.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network.arXiv\npreprint arXiv:1503.02531, 2015.\nJeremy Howard and Sebastian Ruder. Universal Language Model Fine-tuning for Text Classiﬁca-\ntion. In Proceedings of ACL 2018, 2018.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson.\nXTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Gen-\neralization. In Proceedings of the 37th International Conference on Machine Learning (ICML) ,\n2020.\nHakan Inan, Khashayar Khosravi, and Richard Socher. Tying Word Vectors and Word Classiﬁers:\nA Loss Framework for Language Modeling. In Proceedings of ICLR 2017, 2017.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil\nThorat, Fernanda Vi´egas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean.\nGoogle’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation.\nTransactions of the ACL 2017, 2017.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and Dan Roth. Cross-lingual ability of multilingual\nbert: An empirical study. In International Conference on Learning Representations, 2020. URL\nhttps://openreview.net/forum?id=HJeT3yrtDr.\n10\nPreprint. Under review.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling Laws for Neural Language\nModels. arXiv e-prints, art. arXiv:2001.08361, January 2020.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language Processing: System Demonstrations , pp.\n66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-2012. URL https://www.aclweb.org/anthology/D18-2012.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In\nInternational Conference on Learning Representations, 2020. URL https://openreview.\nnet/forum?id=H1eA7AEtvS.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling Giant Models with Condi-\ntional Computation and Automatic Sharding. arXiv e-prints, art. arXiv:2006.16668, June 2020.\nPatrick Lewis, Barlas O˘guz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. MLQA: Evaluat-\ning Cross-lingual Extractive Question Answering. In Proceedings of ACL 2020, 2020.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic\nKnowledge and Transferability of Contextual Representations. In Proceedings of NAACL 2019,\n2019a.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pre-\ntraining Approach. arXiv preprint arXiv:1907.11692, 2019b.\nMinh-Thang Luong, Richard Socher, and Christopher D Manning. Better word representations with\nrecursive neural networks for morphology. In Proceedings of the Seventeenth Conference on\nComputational Natural Language Learning, pp. 104–113, 2013.\nG´abor Melis, Tom´aˇs Koˇcisk´y, and Phil Blunsom. Mogriﬁer LSTM. In Proceedings of ICLR 2020,\n2020.\nPaul Michel, Omer Levy, and Graham Neubig. Are Sixteen Heads Really Better than One? In\nProceedings of NeurIPS 2019, 2019.\nSubhabrata Mukherjee and Ahmed Hassan Awadallah. XtremeDistil : Multi-stage Distillation for\nMassive Multilingual Models. In Proceedings of ACL 2020, pp. 2221–2234, 2020.\nJoakim Nivre, Mitchell Abrams, ˇZeljko Agi´c, Lars Ahrenberg, Lene Antonsen, Maria Jesus Aran-\nzabe, Gashaw Arutie, Masayuki Asahara, Luma Ateyah, Mohammed Attia, et al. Universal de-\npendencies 2.2. 2018.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. Cross-\nlingual name tagging and linking for 282 languages. InProceedings of ACL 2017, pp. 1946–1958,\n2017.\nJonas Pfeiffer, Ivan Vuli, Iryna Gurevych, and Sebastian Ruder. MAD-X: An Adapter-based Frame-\nwork for Multi-task Cross-lingual Transfer. In Proceedings of EMNLP 2020, 2020.\nJason Phang, Phu Mon Htut, Yada Pruksachatkun, Haokun Liu, Clara Vania, Katharina Kann, Iacer\nCalixto, and Samuel R Bowman. English intermediate-task training improves zero-shot cross-\nlingual transfer too. arXiv preprint arXiv:2005.13013, 2020.\nTelmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual BERT? In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,\npp. 4996–5001, Florence, Italy, July 2019. Association for Computational Linguistics. doi:\n10.18653/v1/P19-1493. URL https://www.aclweb.org/anthology/P19-1493.\n11\nPreprint. Under review.\nOﬁr Press and Lior Wolf. Using the output embedding to improve language models. InProceedings\nof the 15th Conference of the European Chapter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pp. 157–163, Valencia, Spain, April 2017. Association for Computational\nLinguistics. URL https://www.aclweb.org/anthology/E17-2025.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Research , 21(140):1–67, 2020. URL http:\n//jmlr.org/papers/v21/20-074.html.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. In Proceedings of EMNLP 2016, 2016.\nSebastian Ruder, Matthew E Peters, Swabha Swayamdipta, and Thomas Wolf. Transfer learning\nin natural language processing. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Tutorials, pp. 15–18, 2019.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green AI. arXiv preprint\narXiv:1907.10597, 2019.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn\nKoanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan\nSepassi, and Blake Hechtman. Mesh-tensorﬂow: Deep learning for supercomput-\ners. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems 31 , pp. 10414–\n10423. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\n8242-mesh-tensorflow-deep-learning-for-supercomputers.pdf .\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Par-\nallelism. arXiv e-prints, art. arXiv:1909.08053, September 2019.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. MobileBERT\n: a Compact Task-Agnostic BERT for Resource-Limited Devices. In Proceedings of ACL 2020,\npp. 2158–2170, 2020.\nAlex Tamkin, Trisha Singh, Davide Giovanardi, and Noah Goodman. Investigating Transferability\nin Pretrained Language Models. arXiv e-prints, art. arXiv:2004.14975, April 2020.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp.\n4593–4601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/\nv1/P19-1452. URL https://www.aclweb.org/anthology/P19-1452.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-Read Students Learn Better:\nOn the Importance of Pre-training Compact Models. arXiv preprint arXiv:1908.08962, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neu-\nral Information Processing Systems 30 , pp. 5998–6008. Curran Associates, Inc., 2017. URL\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\nElena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing Multi-Head\nSelf-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. InProceedings\nof ACL 2019, 2019.\nAdina Williams, Nikita Nangia, and Samuel R. Bowman. A Broad-Coverage Challenge Corpus for\nSentence Understanding through Inference. In Proceedings of NAACL-HLT 2018, 2018.\n12\nPreprint. Under review.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, and Jamie Brew. HuggingFace’s Trans-\nformers: State-of-the-art Natural Language Processing. arXiv preprint arXiv:1910.03771, 2019.\nShijie Wu and Mark Dredze. Beto, bentz, becas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pp. 833–844, Hong Kong, China, November 2019. Association for Computational Lin-\nguistics. doi: 10.18653/v1/D19-1077. URL https://www.aclweb.org/anthology/\nD19-1077.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial\ndataset for paraphrase identiﬁcation. In Proceedings of EMNLP 2019, pp. 3685–3690, 2019.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep\nneural networks? In Advances in neural information processing systems, pp. 3320–3328, 2014.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, and Yoav Artzi. Revisiting Few-\nsample BERT Fine-tuning. arXiv e-prints, art. arXiv:2006.05987, June 2020.\nXiyou Zhou, Zhiyu Chen, Xiaoyong Jin, and William Yang Wang. HULK: An Energy Ef-\nﬁciency Benchmark Platform for Responsible Natural Language Processing. arXiv preprint\narXiv:2002.05829, 2020.\nPierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. Overview of the third bucc shared task:\nSpotting parallel sentences in comparable corpora. In Proceedings of 11th Workshop on Building\nand Using Comparable Corpora, pp. 39–42, 2018.\nA A PPENDIX\nA.1 E FFICIENCY COMPARISON BASED ON PARAMETER COUNT DURING FINE -TUNING\nWe compare the efﬁciency of models based on their number of parameters. We believe this to be a\nreasonable proxy for a model’s efﬁciency as the performance of Transformer-based language models\nhas been shown to improve monotonically with the number of parameters (Kaplan et al., 2020; Raffel\net al., 2020; Lepikhin et al., 2020; Brown et al., 2020; Shoeybi et al., 2019; Aharoni et al., 2019).\nAs the number of parameters during pre-training and ﬁne-tuning may differ 11, we compare models\nbased on their number of parameters during the ﬁne-tuning stage (without the task-speciﬁc head).\nWe argue that this is the most practically relevant number as a model is generally pre-trained only\nonce but may be ﬁne-tuned or used for inference millions of times.\nA.2 B ASELINE MODEL DETAILS\nOur baseline model has the same architecture as multilingual BERT (mBERT; Devlin et al., 2019).\nIt consists of 12 Transformer layers with a hidden size H of 768 and 12 attention heads with 64\ndimensions each. Input and output embeddings are coupled and have the same dimensionality E\nas the hidden size, i.e. Eout = Ein = H. The total number of parameters during pre-training\nand ﬁne-tuning is 177M. We do not use dropout following the recommendation from Lan et al.\n(2020). We use the SentencePiece tokenizer (Kudo & Richardson, 2018) and a shared vocabulary of\n120k subwords. The model is trained on Wikipedia dumps in 104 languages following Devlin et al.\n(2019) using masked language modeling (MLM). We choose this baseline as its behavior has been\nthoroughly studied (K et al., 2020; Conneau et al., 2020b; Pires et al., 2019; Wu & Dredze, 2019).\n11For encoder-only models such as BERT, parameters after the last Transformer layer (e.g. the output em-\nbeddings and the pooling layer) are discarded after pre-training.\n13\nPreprint. Under review.\nTable 10: Fine-tuning hyperparameters for all models except RemBERT.\nLearning rate Batch size Train epochs\nPAWS-X [3 ×10−5, 4 ×10−5, 5 ×10−5] 32 3\nXNLI [1 ×10−5, 2 ×10−5, 3 ×10−5] 32 3\nSQuAD [2 ×10−5, 3 ×10−5, 4 ×10−5] 32 3\nNER [1 ×10−5, 2 ×10−5, 3 ×10−5,4 ×10−5,5 ×10−5] 32 3\nTable 11: Statistics for the datasets in XTREME , including the number of training, development, and\ntest examples as well as the number of languages for each task.\nTask Corpus |Train| |Dev| | Test| |Lang.| Task Metric Domain\nClassiﬁcationXNLI 392,702 2,490 5,010 15 NLI Acc. Misc.\nPAWS-X 49,401 2,000 2,000 7 Paraphrase Acc. Wiki / Quora\nStructured POS 21,253 3,974 47-20,436 33 POS F1 Misc.\nprediction NER 20,000 10,000 1,000-10,000 40 NER F1 Wikipedia\nQA\nXQuAD 87,599 34,726 1,190 11 Span extraction F1 / EM Wikipedia\nMLQA 4,517–11,590 7 Span extraction F1 / EM Wikipedia\nTyDiQA-GoldP 3,696 634 323–2,719 9 Span extraction F1 / EM Wikipedia\nRetrieval BUCC - - 1,896–14,330 5 Retrieval F1 Wiki / news\nTatoeba - - 1,000 33 Retrieval Acc. misc.\nA.3 T RAINING DETAILS\nFor all pre-training except for the large scale RemBERT, we trained using 64 Google Cloud TPUs.\nWe trained over 26B tokens of Wikipedia data. All ﬁne-tuning experiments were run on 8 Cloud\nTPUs. For all ﬁne-tuning experiments other than RemBERT, we use batch size of 32. We sweep\nover the learning rate values speciﬁed in Table 10.\nWe used the SentencePiece tokenizer trained with unigram language modeling\nA.4 XTREME TASKS\nFor our experiments, we employ tasks from the XTREME benchmark (Hu et al., 2020). We show\nstatistics for them in Table 11. XTREME includes the following datasets: The Cross-lingual Natu-\nral Language Inference (XNLI; Conneau et al., 2018) corpus, the Cross-lingual Paraphrase Adver-\nsaries from Word Scrambling (PAWS-X; Yang et al., 2019) dataset, part-of-speech (POS) tagging\ndata from the Universal Dependencies v2.5 (Nivre et al., 2018) treebanks, the Wikiann (Pan et al.,\n2017) dataset for named entity recognition (NER), the Cross-lingual Question Answering Dataset\n(XQuAD; Artetxe et al., 2020), the Multilingual Question Answering (MLQA; Lewis et al., 2020)\ndataset, the gold passage version of the Typologically Diverse Question Answering (TyDiQA; Clark\net al., 2020a) dataset, data from the third shared task of the workshop on Building and Using Parallel\nCorpora (BUCC; Zweigenbaum et al., 2018), and the Tatoeba dataset (Artetxe & Schwenk, 2019).\nWe refer the reader to (Hu et al., 2020) for more details. We average results across three ﬁne-tuning\nruns and evaluate on the dev sets unless otherwise stated.\nA.5 C OMPARISON TO LAN ET AL . (2020)\nCrucially, our ﬁnding differs from the dimensionality reduction in ALBERT (Lan et al., 2020).\nWhile they show that smaller embeddings can be used, their input and output embeddings are cou-\npled and use a much smaller vocabulary (30k vs 120k). In contrast, we ﬁnd that simultaneously\ndecreasing both the input and output embedding size drastically reduces the performance of multi-\nlingual models.\nIn Table 12, we show the impact of their factorized embedding parameterization on a monolingual\nand a multilingual model. While the English model suffers a smaller (0.8%) drop in accuracy, the\n14\nPreprint. Under review.\nTable 12: Effect of reducing the embedding size E for monolingual vs. multilingual models on\nMNLI and XNLI performance respectively. Monolingual numbers are from Lan et al. (2020) and\nhave vocabulary size of 30k.\nEnglish # PT params # FT params MNLI\nE=H= 768 110M 110M 84.5\nE=H= 128 89M 89M 83.7\nMultilingual # PT params # FT params XNLI\nE=H= 768 177M 177M 70.7\nE=H= 128 100M 100M 68.1\nTable 13: Effect of an increased output embedding sizeEout and additional layers during pre-training\nL= 15 on English BERTBase (Ein = 128).\n# PT # FT MNLI SQuAD\nparams params Acc EM/F1\nBERTBase (ours) 110M 110M 79.8 78.4/86.2\nEout = 128 93M 89M 75.9 75.5/84.2\nEout = 768 112M 89M 77.5 77.5/85.5\nEout = 3072 181M 89M 79.5 78.4/86.2\nL= 15 114M 89M 80.1 78.7/86.3\nL= 24 178M 89M 79.0 77.8/85.5\nmultilingual model’s performance drops by 2.6%. Direct application of a factorized embedding\nparameterization (Lan et al., 2020) is thus not viable for multilingual models.\nA.6 E NGLISH MONOLINGUAL RESULTS\nSo far, we have focused on multilingual models as the number of saved parameters when reducing\nthe input embedding size is largest for them. We now apply the same techniques to the English\n12-layer BERTBase with a 30k vocabulary (Devlin et al., 2019). Speciﬁcally, we decouple the em-\nbeddings, reduce Ein to 128, and increase the output embedding size or the number of layers during\npre-training. We show the performance on MNLI (Williams et al., 2018) and SQuAD (Rajpurkar\net al., 2016) in Table 13. By adding more capacity during pre-training, performance monotonically\nincreases similar to the multilingual models. Interestingly, pruning a 24-layer model to 12 layers\nreduces performance, presumably because some upper layers still contain useful information.\nA.7 R EMBERT DETAILS\nWe design a Rebalanced mBERT (RemBERT) to leverage capacity more effectively during pre-\ntraining. The model has 995M parameters during pre-training and 575M parameters during ﬁne-\ntuning. We pre-train on large unlabeled text using both Wikipedia and Common Crawl data, covering\n110 languages. The details of hyperparameters and architecture are shown in Table 14.\nFor each language l, we deﬁne the empirical distribution as\npl = nl∑\nl′∈L nl′\n(1)\nwhere nl is the number of sentences in l’s pre-training corpus. Following Devlin et al. (2019), we\nuse an exponentially smoothed distribution, i.e., we exponentiaatepl by α= 0.5 and renormalize to\nobtain the sampling distribution.\nHyperparameters and pre-training details are summarized in Table 14. Hyperparameters used for\nthe leaderboard submission are shown in Table 15.\n15\nPreprint. Under review.\nTable 14: Hyperparameters for RemBERT architecture and pre-training.\nHyperparameter RemBERT\nNumber of layers 32\nHidden size 1152\nV ocabulary size 250,000\nInput embedding dimension 256\nOutput embedding dimension 1536\nNumber of attention heads 18\nAttention head dimension 64\nDropout 0\nLearning rate 0.0002\nBatch size 2048\nTrain steps 1.76M\nAdam β1 0.9\nAdam β2 0.999\nAdam ϵ 10−6\nWeight decay 0.01\nGradient clipping norm 1\nWarmup steps 15000\nTable 15: Hyperparameters for RemBERT ﬁne-tuning.\nLearning rate Batch size Train epochs\nPAWS-X 8 ×10−6 128 3\nXNLI 1 ×10−5 128 3\nSQuAD 9 ×10−6 128 3\nPOS 3 ×10−5 128 3\nNER 8 ×10−6 64 3\nA.8 XTREME TASK RESULTS\nWe show the detailed results for RemBERT and the comparison per task on theXTREME leaderboard\nin Table 16. Compared to Table 7, which shows the average across task categories, this table shows\nthe average across tasks.\nTable 16: Comparison of our model to other models on the XTREME leaderboard. Details about\nVECO are due to communication with the authors. Avg task is averaged over tasks whereas Avg is\naveraged over task categories just like Table 7.\n# PT # FT XNLI POS NER PAWS-X XQuAD MLQA TyDi-GoldPAvgtask Avgparams params Acc F1 F1 Acc EM/F1 EM/F1 EM/F1\nModels ﬁne-tuned on translations or additional task data\nSTiLTs (Phang et al., 2020) 559M 559M 80.0 74.9 64.0 87.9 63.3/78.7 53.7/72.4 59.5/76.0 72.7 73.5FILTER (Fang et al., 2020) 559M 559M 83.9 76.2 67.7 91.4 68.0/82.4 57.7/76.2 50.9/68.3 74.9 76.0VECO (Anonymous, 2021) 662M 662M 83.0 75.1 65.7 91.1 66.3/79.9 54.9/73.1 58.9/75.0 74.1 75.1\nModels ﬁne-tuned only on English task data\nXLM-R (Conneau et al., 2020a) 559M 559M 79.2 73.8 65.4 86.4 60.8/76.6 53.2/71.6 45.0/65.1 70.1 71.4RemBERT (ours) 995M 575M 80.8 76.5 70.1 87.5 64.0/79.6 55.0/73.1 63.0/77.0 74.4 75.4\n16\nPreprint. Under review.\nA.9 N EAREST -NEIGHBOR TRANSLATION COMPUTATION\nFor an English-to-German translation, we sample M = 5000 pairs of sentences from WMT16\n(Bojar et al., 2016). For each sentence in each language, we obtain a representation v(l)\nLANG at each\nlayer lby averaging the activations of all tokens (except the[CLS] and [SEP] tokens) at that layer.\nWe then compute a translation vector from English to German by averaging the difference between\nthe vectors of each sentence pair across all pairs: ¯v(l)\nEN→DE = 1\nM\n∑M\ni=1\n(\nv(l)\nDEi −v(l)\nENi\n)\n.\nFor each English sentence v(l)\nENi , we can now translate it with this vector: v(l)\nENi + ¯v(l)\nEN→DE. We\nlocate the closest German sentence vector based on ℓ2 distance and measure how often the nearest\nneighbour is the correct pair.\n17",
  "topic": "Embedding",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.926869809627533
    },
    {
      "name": "Transformer",
      "score": 0.7651914358139038
    },
    {
      "name": "Computer science",
      "score": 0.7169601917266846
    },
    {
      "name": "Language model",
      "score": 0.5685489177703857
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5623117089271545
    },
    {
      "name": "Fine-tuning",
      "score": 0.5611540079116821
    },
    {
      "name": "Coupling (piping)",
      "score": 0.4709430932998657
    },
    {
      "name": "Task (project management)",
      "score": 0.44790101051330566
    },
    {
      "name": "Natural language understanding",
      "score": 0.42803680896759033
    },
    {
      "name": "Flexibility (engineering)",
      "score": 0.4236515164375305
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3494186997413635
    },
    {
      "name": "Natural language",
      "score": 0.2296239137649536
    },
    {
      "name": "Voltage",
      "score": 0.17692601680755615
    },
    {
      "name": "Mathematics",
      "score": 0.14584222435951233
    },
    {
      "name": "Engineering",
      "score": 0.09154677391052246
    },
    {
      "name": "Electrical engineering",
      "score": 0.08435377478599548
    },
    {
      "name": "Particle physics",
      "score": 0.0754372775554657
    },
    {
      "name": "Physics",
      "score": 0.07477140426635742
    },
    {
      "name": "Mechanical engineering",
      "score": 0.07303529977798462
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 11
}