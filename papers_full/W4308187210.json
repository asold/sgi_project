{
  "title": "Graph–sequence attention and transformer for predicting drug–target affinity",
  "url": "https://openalex.org/W4308187210",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4294831845",
      "name": "Xiangfeng Yan",
      "affiliations": [
        "Heilongjiang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098300734",
      "name": "Yong Liu",
      "affiliations": [
        "Heilongjiang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4294831845",
      "name": "Xiangfeng Yan",
      "affiliations": [
        "Heilongjiang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098300734",
      "name": "Yong Liu",
      "affiliations": [
        "Heilongjiang University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2273267066",
    "https://openalex.org/W1989129581",
    "https://openalex.org/W3152586663",
    "https://openalex.org/W3206585172",
    "https://openalex.org/W3096561213",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2984339599",
    "https://openalex.org/W2018408116",
    "https://openalex.org/W2105668062",
    "https://openalex.org/W1971106435",
    "https://openalex.org/W1984084871",
    "https://openalex.org/W2184981123",
    "https://openalex.org/W3094060150",
    "https://openalex.org/W1527782499",
    "https://openalex.org/W2109991441",
    "https://openalex.org/W2605952223",
    "https://openalex.org/W2785947426",
    "https://openalex.org/W2911871527",
    "https://openalex.org/W2807704635",
    "https://openalex.org/W2155744824",
    "https://openalex.org/W3000369508",
    "https://openalex.org/W2860192827",
    "https://openalex.org/W3031648794",
    "https://openalex.org/W2991049471",
    "https://openalex.org/W2898227364",
    "https://openalex.org/W2960646647",
    "https://openalex.org/W2968734407",
    "https://openalex.org/W2502312327",
    "https://openalex.org/W4231842606",
    "https://openalex.org/W2085126686"
  ],
  "abstract": "We proposed a novel model based on self-attention, called GSATDTA, to predict the binding affinity between drugs and targets. Experimental results show that our model outperforms the state-of-the-art methods on two independent datasets.",
  "full_text": "Graph–sequence attention and transformer for\npredicting drug–target aﬃnity\nXiangfeng Yan and Yong Liu*\nDrug–target binding a ﬃnity (DTA) prediction has drawn increasing interest due to its substantial\nposition in the drug discovery process. The development of new drugs is costly, time-consuming,\nand often accompanied by safety issues. Drug repurposing can avoid the expensive and lengthy\nprocess of drug development byﬁnding new uses for already approved drugs. Therefore, it is of great\nsigniﬁcance to develop eﬀective computational methods to predict DTAs. The attention mechanisms\nallow the computational method to focus on the most relevant parts of the input and have been\nproven to be useful for various tasks. In this study, we proposed a novel model based on self-\nattention, called GSATDTA, to predict the binding a ﬃnity between drugs and targets. For the\nrepresentation of drugs, we use Bi-directional Gated Recurrent Units (BiGRU) to extract the SMILES\nrepresentation from SMILES sequences, and graph neural networks to extract the graph\nrepresentation of the molecular graphs. Then we utilize an attention mechanism to fuse the two\nrepresentations of the drug. For the target/protein, we utilized an eﬃcient transformer to learn the\nrepresentation of the protein, which can capture the long-distance relationships in the sequence of\namino acids. We conduct extensive experiments tocompare our model with state-of-the-art models.\nExperimental results show that our model outperforms the current state-of-the-art methods on two\nindependent datasets.\n1 Introduction\nDrug discovery is a complicated process, which has the risks of\nlong research cycles, high costs, and low success rates. It takes\nbillions of dollars and more than ten years to develop a new\ndrug from development to approval.\n1,2 The eﬀective prediction\nof drug–target binding aﬃnity (DTA) is one of the signicant\nissues in drug discovery.3–5 Drugs are usually represented as\nas t r i n go b t a i n e df r o mt h es i m p l ied molecular-input line-\nentry system (SMILES)6 or represented by a molecule graph\nwith atoms as nodes and chemical bonds as edges. Targets (or\nproteins) are sequences of amino acids. Binding aﬃnity indi-\ncates the strength of drug–target pair interaction. Through\nbinding, drugs can have a positive or negative inuence on\nfunctions carried out by proteins, a ﬀecting the disease\nconditions.\n7 By understanding drug–target binding aﬃnity, it\nis possible to nd out candidate drugs that can inhibit the\ntarget/protein and bene t many other bioinformatics\napplications. 8,9\nEarly computational attempts were focused on biologically\nintuitive methods, such as ligand-similarity based approaches\nand docking simulations.10 Ligand-similarity based methods\npredict interactions by comparing a new ligand to known\nligands of proteins. However, ligand-similarity methods\nperform poorly when the number of known ligands is insuf-\ncient. Docking simulation methods require the 3D structures\nof the target proteins hence becoming inapplicable when there\na r en u m e r o u sp r o t e i n sw i t hu n a v a i l a b l e3 Ds t r u c t u r e s .\n11 In the\npast few years, some research has begun predicting DTAs from\na network perspective.\n12–14 However, the prediction qualities of\nnetwork-based approaches are strongly limited by available\nlinked information. Thus, these methods do not perform very\nwell on association predictions for new drugs or targets with\nscarce linked information. Addi tionally, some useful infor-\nmation, such as drug and targetfeature information, cannot\nbe fully utilized to improve prediction accuracy for these\nmethods.\nWith the development of articial intelligence, deep learning\napproaches for DTA prediction have become popular and can be\ncategorized into two main groups according to the input data:\nsequence-based and graph-based methods. The sequence-\nbased methods learned the representations from sequential\ndata, which are SMILES sequences of drugs and amino acid\nsequences of proteins. The graph-based methods represented\ndrugs as molecular graphs, which learned the representations\nfrom molecular graphs and amino acid sequences of proteins.\nAlthough deep learning models show excellent performance\nimprovement in DTA prediction, two main challenges remain to\nstudy. First, these methods consider either SMILES sequences\nSchool of Computer Science and Technology, Heilongjiang University, Harbin, China.\nE-mail: 2010023@hlju.edu.cn\nCite this:RSC Adv., 2022,12,2 9 5 2 5\nReceived 4th September 2022\nAccepted 4th October 2022\nDOI: 10.1039/d2ra05566j\nrsc.li/rsc-advances\n© 2022 The Author(s). Published by the Royal Society of Chemistry RSC Adv., 2022,12,2 9 5 2 5–29534 | 29525\nRSC Advances\nPAPER\nOpen Access Article. Published on 14 October 2022. Downloaded on 11/5/2025 6:14:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nor molecular graphs, which failed to capture comprehensive\nrepresentations of drugs. A SMILES sequence can oﬀer the\nfollowing features as a representation: (i) ionic groups and\natomic groups are represented in the canonical way, which\navoids confusion with their surrounding atomic groups. For\ninstance, ammonium is denoted as [NH4+] rather than\nHHNHH; (ii) some specially de ned symbols are used to\npreserve chemical properties such as chemical valence,\nisotopes, etc. However, merely taking drugs with sophisticated\ninternal connectivity as simple sequential data lacks suﬃcient\ninterpretable and expressive capabilities. Molecular graph\nbrings two unique benets as compared to SMILES sequence: (i)\nmolecular graph can capture the spatial connectivity of diﬀerent\natoms, especially for star structures and ring structures (e.g.,\nalkyl and benzene ring); (ii) chemical molecular bonds are well\npreserved, which might inuence the molecular properties. For\ninstance, carbon dioxide has divalent bonds between carbon\nand oxygen. However, similar to sequence modeling in SMILES,\nsimply using molecular graphs to model molecules cannot\nenable methods to comprehensively learn molecular represen-\ntations. It is diﬃcult to capture information on some specic\nmolecular properties, such as atoms’ chirality, using molecular\ngraphs. Second, most existing methods utilized convolutional\nneural networks (CNNs) to learn low-dimensional feature\nrepresentations of proteins from the sequence of amino acids,\nwhich ignored the long-distance relationships in the protein\nsequences.\nTo overcome the mentioned challenges of current methods\nfor DTA prediction, we propose a novel triple-channel model,\nnamed Graph–Sequence Attention and Transformer for Pre-\ndicting Drug–Target Aﬃnity (GSATDTA), to predict the binding\naﬃnity between drugs and targets. Recently, Guo et al.\n15\nproposed that integrating the capabilities of both molecular\ngraphs and SMILES sequences can further enhance molecule\nrepresentation expressive power. Enlightened by this work, we\nuse a graph neural network to learn the graph representation\nfrom molecular graphs and a BiGRU to learn the SMILES\nrepresentation from SMILES sequences. Then, we propose\na graph–sequence attention mechanism to capture signicant\ninformation from both the SMILES sequence and molecular\ngraph. For the protein representations, we replace CNN with an\neﬃcient transformer to learn the representation of the protein,\nwhich can capture the long-distance relationships in the\nsequence of amino acids.\n16 The main contributions of this\npaper are summarized as follows:\n/C15 We leveraged both the graph and sequence information\nand proposed a graph–sequence attention mechanism to learn\neﬀective drug representations for DTA prediction.\n/C15 We utilized an eﬃcient transformer to learn the repre-\nsentation of the protein, which can capture the long-distance\nrelationships in the sequence of amino acids.\n/C15 We conduct extensive experiments on two benchmark\ndatasets to investigate the performance of our proposed model.\nThe experimental results show that our proposed model ach-\nieves the best performance in the drug–target binding aﬃnity\nprediction task.\n2 Related work\n2.1 Simulation methods\nMost previous works have focused on simulation-based\nmethods ( i.e., molecular docking and descriptors). For\nexample, Li et al.17 proposed a docking method based on\nrandom forest (RF). The RF model was also adopted in\nKronRLS,18 which uses the similarity score obtained by the\nKronecker product of the similarity matrix to improve the\npredictive performance. To alleviate the limitation of linear\ndependence in KronRLS, a gradient boosting method is\nproposed in SimBoost\n19 to construct the similarity between\ndrugs and targets. While classical methods have shown rational\nperformance in DTA prediction, they are usually computation-\nally expensive or rely on external expert knowledge or the 3D\nstructure of the target/protein.\n2.2 Sequence-based methods\nThe sequence-based methods learned the representations from\nsequential data, which are SMILES sequences of drugs and\namino acid sequences of proteins. For example, DeepDTA\n20 uses\nthe 1D representation of the drug and protein, and uses con-\nvolutional neural networks (CNNs) to learn representations\nfrom the raw protein sequences and SMILES strings. Then, they\ncombined these representations to feed into a fully connected\nlayer to predict the drug –target a ﬃnity scores. Similarly,\nWideDTA\n21 also relies only on the 1D representation, but\ndiﬀerent from DeepDTA, the SMILES and protein sequence\nwere represented as words (instead of characters) that corre-\nspond to an eight-character sequence and a three-residual\nsequence, respectively. In addition, WideDTA utilized the\nligand maximum common substructure (LMCS)\n22 of drugs and\nmotifs and domains of proteins (PDM),23 which formed a four-\nbranch architecture together with the ligand SMILES and\nprotein sequence branches. The WideDTA model is an exten-\nsion of DeepDTA, which also used CNN to learn the represen-\ntations of drugs and proteins. GANsDTA\n24 utilized a generative\nadversarial networks (GAN) to learn benecial patterns within\nlabeled and unlabeled sequences and used convolutional\nregression to forecast binding aﬃnity scores. MATT_DTI\n3 also\nutilizes the 1D representation, it proposed a relation-aware self-\nattention block to model the relative position between atoms in\ndrugs, considering the correlation between atoms. As for the\nprotein, it utilizes three convolutional layers as the feature\nextractor, followed by a max pooling layer. Then, a multi-head\nattention block is built to model the similarity of drug–target\npairs as the interaction information for DTA prediction.\n2.3 Graph-based methods\nThe graph-based methods represented drugs as molecular\ngraphs, and learned the representations from molecular graphs\nand amino acid sequences of proteins. Tsubakiet al.\n25 proposed\nthe application of the graph neural network (GNN) to DTA (or\ncompound–protein interaction, CPI) prediction. In their model,\nthe chemical structures of drugs (provided in SMILES notation)\nare represented as graphs. Therefore, they propose the use of\n29526 | RSC Adv., 2022,12,2 9 5 2 5–29534 © 2022 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 14 October 2022. Downloaded on 11/5/2025 6:14:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nGNN and CNN, which can learn low-dimensional real-valued\nvector representations of molecular graphs and protein\nsequences. Similarly, Gao et al.26 utilized the GNN for drug\nrepresentation, whereas the protein descriptors were obtained\nusing long short-term memory (LSTM). GraphDTA\n5 also intro-\nduced graph representation to take advantage of the topological\nstructure information of the molecular graph. GraphDTA used\na three-layer GCN as an alternative for drug representation\nwhile utilizing the CNN to learn protein representation as in\nDeepDTA. Among the research on deep learning for drug\ndiscovery, DeepGS\n27 is the most relevant to our work. DeepGS\nconsiders both the molecular graphs and SMILES sequences of\ndrugs, however, they directly contacted the two representations,\nwhich failed to capture comprehensive information about\ndrugs. Furthermore, DeepGS and other deep learning methods\nutilized CNN to learn local representations of protein\nsequences, which ignored the long-distance relationships in the\nprotein sequences.\nCompared with these deep learning models, we designed\na graph–sequence attention mechanism, which can capture\nsignicant information from both SMILES sequences and\nmolecular graphs. As for the protein, we replace CNN with an\neﬃcient transformer to extract the long-distance relationships\nin the sequence of amino acids.\n3 Materials and methods\nWe regard DTA prediction as a regression task to predict the\nbinding aﬃnity value between a drug–target pair. We denote the\nSMILES sequence set asD ¼ {D\ni}jDj\ni¼1 and the protein sequence set\nas P ¼ {Pj}jPj\nj¼1. We use RDKit to convert the SMILES setD into\na molecular graph setG ¼ {Gi}jGj\ni¼1, where Gi ¼ (Vi, Ei) denotes\na molecular graph. Specically, a node on a molecular graph\nrepresents an atom, and an edge represents a chemical bond\nbetween two atoms. Formally, the problem of drug –target\nbinding aﬃnity prediction is dened as follows.\nGiven a SMILES setD and a protein setP, and their inter-\naction labels Y ¼ {Y\ni,jj1 # i # jDj,1 # j # jPj, Yi,j ˛ R}, the\nbinding aﬃnity prediction problem is to learn a functionf: D /C2\nP / Y such thatf(Di, Pj) / Yi,j.\n3.1 Overview\nWe proposed a triple-channel model, called GSATDTA; the\noverall architecture of the model is shown in Fig. 1. It takes the\nsymbolic sequences of the target/protein and drug as inputs, as\nwell as the molecular structure of the drug, and outputs the\nbinding aﬃnity of the drug to the target. For the drug repre-\nsentations, we use a graph neural network to learn the topo-\nlogical structure information from molecular graphs and\nBiGRU to learn contextual information about drugs represented\nby SMILES sequences. Then, we utilized the graph–sequence\nattention mechanism to capture signicant information from\nboth the SMILES sequence and molecular graph. For the\nprotein representations, we employed an eﬃcient transformer\nto capture the long-distance relationships in the sequence of\namino acids. Thus, we obtained the target representation and\nthe fusion drug representation. The connection of two repre-\nsentations is inputted into several dense layers and ends with\nFig. 1 Illustration of the proposed GSATDTA. We leveraged both the graph and sequence information and utilized the graph–sequence attention\nmechanism to learn eﬀective drug representations. For the protein, we employed an eﬃcient transformer to learn the representation of the\nprotein sequence. Finally, the two representations were concatenated and passed through several dense fully connected layers to estimate the\noutput as the drug–target aﬃnity value.\n© 2022 The Author(s). Published by the Royal Society of Chemistry RSC Adv., 2022,12,2 9 5 2 5–29534 | 29527\nPaper RSC Advances\nOpen Access Article. Published on 14 October 2022. Downloaded on 11/5/2025 6:14:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\na regression layer to predict the drug–target binding aﬃnity\nvalue. Next, we will present the details of our model.\n3.2 Representation learning of drug\n3.2.1 Representation learning of sequence. Simplied\nMolecular-Input Line-Entry System (SMILES) is a non-unique\nrepresentation that encodes the molecular graph into a string\nof ASCII characters. For example, the SMILES“CN(C)CC]CC(]\nO).” for a drug in Fig. 1 is a sequence of atoms and chemical\nbonds. SMILES rules can cover atoms, ions, chemical bonds,\nvalences, and chemical reactions, which can accurately express\nbranched, cyclic, tetrahedral, aromatic, and chiral structures, as\nwell as the expression of isomers and isotopes.\nMost previous studies adopted one-hot encoding to encode\nthe symbols in the SMILES sequence. However, one-hot\nencoding ignores the contextual value of the symbol, and thus\ncannot reveal the functionality of the symbol within the\ncontext.\n28,29 To address this problem, we utilized Smi2Vec,30\na method similar to Word2Vec,31 to encode the tokens in the\nSMILES sequences. According to DeepGS,27 we xed maximum\nlengths of 100 for SMILES sequences. We cut the SMILES\nsequence if the length of the SMILES sequence is longer than\n100. Otherwise, we use zero-padding at the end of the SMILES\nsequence. In a typical case, axed-length SMILES string, for\nexample, C, is partitioned into individual atoms or symbols. It is\nthen mapped to atoms by seeking out each atom embedded\nfrom a pretrained dictionary, and if not in the dictionary, it is\nobtained from randomly generated values.\n32,33 Then, atom\nembedding vectors are aggregated to form thenal embedding\nmatrix. Enlightened by the gate function in GRU,34 we applied\nthe three layers technique of BiGRU to the generated matrix to\nobtain a latent representation of the drug, which allows us to\nmodel the local chemical context. Finally, we obtain the SMILES\nrepresentation S\ni ˛ RN through the max-pooling layer and the\nfully connected layer, whereN is the output dimensional of the\nfully connected layer.\n3.2.2 Representation learning of graph. A vital indication\nfor the estimation of DTA is to eﬀectively exploit molecular\nstructure information to reveal the interconnections between\natoms in the drug.5,25 To achieve this, we transformed the\nSMILES sequence into the molecule graph through the RDKit.\nGraph Isomorphic Network (GIN)\n35 has shown its superiority for\nmodeling graph representation in many studies and supposedly\nachieves maximum discriminative power among graph neural\nnetworks. Our model consists ofve GIN layers, each GIN layer\nis followed by a batch normalization layer, activated by a ReLU\nfunction. Specically, GIN uses a multi-layer perceptron (MLP)\nmodel to update the node features as formula (1):\nMLP\n \nð1 þ 3ÞX\nj\ni þ\nX\nk˛NðjÞ\nXk\ni\n!\n(1)\nwhere 3 is either a learnable parameter orxed scalar,Xj\ni ˛ RF is\nthe feature vector of nodej in the molecular graphi, andN(j)i s\nthe set of nodes adjacent to nodej. Finally, a global max-pooling\nlayer is added to aggregate the entire graph representationHi ˛\nRN.\n3.2.3 Graph –sequence attention. The attention mecha-\nnisms allow the network to focus on the most relevant parts of\nthe input and have been proven to be useful for various tasks.\n4,36\nTo capture signi cant information from both the SMILES\nsequence and molecular graph, we designed an attention\nmechanism, called Graph –Sequence Attention. Speci cally,\ngiven the SMILES representationSi ˛ RN and the graph repre-\nsentation Hi ˛ RN, we transformSi and Hi into the vectorsdSi and\ndHi through formula (2) for feature extraction and attention\nmodeling:\ndSi ¼ ReLUðW1$Si þ bÞ\ndHi ¼ ReLUðW2$Hi þ bÞ (2)\nwhere W1 ˛ RN and W2 ˛ RN are trainable parameters, and\nb represents the bias vector. Then, we combine the outputdSi\nwith dHi through a dimensional-wise fusion gateF. F is accom-\nplished by thesigmoid activation function to encode two parts of\nthe representation:\nF ¼ sigmoid(WG$dHi + WS$dSi) (3)\nwhere WG ˛ RN and WS ˛ RN are trainable parameters of the\nfusion gate. Finally, the nal vector representation output of\na specic drug is generated throughF:\nMd ¼ F/C12 Hi +( 1− F)/C12 Si (4)\nwhere /C12 is the element-wise product.\n3.3 Representation learning of protein\nThe protein sequence is a string of ASCII characters, which\nrepresents amino acids. Mathematically, a protein sequence is\nexpressed asPj ¼ {p1, p2, ., pi, .}, wherepi ˛ N* and the length\nof Pj depends on the proteins. Wex the length of the input\nprotein sequence as Lp to ensure the same size of inputs.\nAccording to the token embedding and position embedding in\nthe transformer,\n16 the input of the eﬃcient transformer is the\nsum of token embedding and position embedding of protein\nsequences. The token embeddingT\np\ntok ˛ RLp/C2 M has a trainable\nweight Wm ˛ Rvp/C2 M, wherevp is the vocabulary size of proteins\nand M is the embedding size of proteins. The position embed-\nding Tp\npos ˛ RLp/C2 M has a trainable weightWp ˛ RLp/C2 M. The output\nof the embedding operations is\nTp ¼ Tp\ntok + Tp\npos, (5)\nwhere Tp ˛ RLp/C2 M.\n3.3.1 E ﬃcient transformer. Most protein sequences are\nlong sequences, so simple CNN cannot capture the long-\ndistance relationships in protein sequences well. Therefore,\ncurrent DTA prediction models are limited due to that CNN\ncannot capture the long-distance relationships in protein\nsequences. The transformer can alleviate this limitation,\nwhich oﬀers a more exible mechanism to model the long-\ndistance relationships. The multi-head self-attention (MSA)\nin the transformer encoder layer has been modeled and inte-\ngrated to improve the model’s ability to learn long-distance\n29528 | RSC Adv., 2022,12,2 9 5 2 5–29534 © 2022 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 14 October 2022. Downloaded on 11/5/2025 6:14:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nrelationships. Multi-head attention can jointly attend to\ninformation from the extracted features at diﬀerent sequence\npositions. The self-attention layer takes three inputs, the keys,\nK,t h ev a l u e s ,V,a n dt h eq u e r i e s ,Q, and calculates the atten-\ntion as follows:\nattentionðQ; K; VÞ¼ softmax\n/C18\nQKT\nﬃﬃﬃﬃﬃﬃdK\np\n/C19\nV (6)\nwhere ﬃﬃﬃﬃﬃﬃdK\np is a scaling factor depending on the layer size.\nHowever, the memory and computation for multi-head self-\nattention in the traditional transformer scale quadratically with\nspatial or embedding dimensions (i.e., the number of chan-\nnels), causing vast overheads for training and inference. Thus,\nwe replace the multi-head self-attention with Eﬃcient Multi-\nhead Self-Attention (EMSA).\n37 The architecture of the eﬃcient\nmulti-head self-attention is shown in Fig. 2.\nSimilar to MSA, EMSA rst adopts a set of projections to\nobtain queryQ. To compress memory, the 2D inputTp ˛ RLp/C2 M\nis reshaped to a 3D one along the spatial dimension ( i.e.,\ncTp ˛RM/C2 h/C2 w) and then fed to a depth-wise convolution operation\nto reduce the height and width dimension by a factors. To make\nit simple,s is an adaptive set by the feature map size or the stage\nnumber. The kernel size, stride and padding ares +1 ,s, ands/2,\nrespectively. The new token map a er spatial reduction\ncTp ˛RM/C2 h=s/C2 w=s is then reshaped to a 2D one,i.e., cTp ˛RL\n0\np/C2 M ,\nL\n0\np ¼ h=s /C2 w=s. Then cTp is fed to two sets of projection to get\nkey K and valueV.A er that, we adopt eqn (7) to compute the\nattention function on queryQ, K and valueV.\nEMSAðQ; K; VÞ¼ IN\n/C18\nsoftmax\n/C18\nconv\n/C18QKT\nﬃﬃﬃﬃﬃﬃdK\np\n/C19/C19/C19\nV (7)\nHere, conv($) is a standard 1 /C2 1 convolutional operation,\nwhich models the interactions among di ﬀerent heads. As\na result, the attention function of each head can depend on all\nof the keys and queries. However, this will impair the ability of\nMSA to jointly attend to information from diﬀerent represen-\ntation subsets at diﬀerent positions. To restore this diversity\nability, we add an Instance Normalization\n38 (i.e., IN($)) for the\ndot product matrix (aer somax). Then, the output values of\neach head are concatenated and linearly projected to form the\nnal output. Finally, the protein representation is obtained\nthrough the max-pooling layer and the fully connected layer,\nwhich we denote asT\nj.\n3.4 Drug –target binding aﬃnity prediction\nIn this paper, we treat the drug–target binding aﬃnity predic-\ntion task as a regression task. With the representation learned\nfrom the previous sections, we can integrate all the information\nabout the drug and target to predict the binding aﬃnity value.\nFirstly, we concatenated the drug representationM\nd and the\nprotein representationTj. Secondly, we feed it into three dense\nfully connected layers to predict the binding aﬃnity value.\nBesides, we use ReLU as the activation function for increasing\nthe nonlinear relationship. Given the set of drug–target pairs\nand the ground-truth labels, we use the mean squared error\n(MSE) as the loss function.\n4 Experiments and results\n4.1 Datasets\nFollowing previous works, we employ two widely used datasets\ndedicated to DTA prediction:\n/C15 Davis:20 the Davis dataset, which contains 68 drugs and 442\ntargets, with 30 056 drug–target interactions. Aﬃnity values\nrange from 5.0 to 10.8.\n/C15 Kiba:20 the Kiba dataset, which contains 2111 drugs and\n229 targets, with 118 254 drug –target interactions. A ﬃnity\nvalues range from 0.0 to 17.2.\nFor both datasets, we use the same training/testing data ratio\nas MATT_DTA,3 DeepDTA,20 and GraphDTA5 in our experi-\nments, making the comparison as fair as possible. That is, 80%\nof the data is used for training and the remaining 20% is used\nfor testing the model. For the same purpose, we use the same\nevaluation metrics as MATT_DTA, GraphDTA, and DeepDTA for\nevaluating model performance: the mean squared error (MSE,\nthe smaller the better), r\nm\n2 (the larger the better), and the\nconcordance index (CI, the larger the better). Table 1 summa-\nrizes the details of the Davis and Kiba datasets.\nFig. 2 Eﬃcient multi-head self-attention.\nTable 1 Summary of the benchmark datasets\nDavis Kiba\nCompounds 68 2111\nProteins 442 229\nInteractions 30 056 118 254\nTraining data 25 046 98 545\nTest data 5010 19 709\n© 2022 The Author(s). Published by the Royal Society of Chemistry RSC Adv., 2022,12,2 9 5 2 5–29534 | 29529\nPaper RSC Advances\nOpen Access Article. Published on 14 October 2022. Downloaded on 11/5/2025 6:14:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n4.2 Evaluation metrics\nMSE is a common metric to measure the diﬀerence between the\npredicted value and the real value:\nMSE ¼ 1\nN\nXN\ni¼1\nð^yi /C0 yiÞ2 (8)\nwhere ˆyi is the predicted value,yi is the true value, andN is the\nnumber of drug–target pairs.\nCI is used to measure whether the predicted binding aﬃnity\nvalues of two random drug–target pairs were predicted in the\nsame order as their true values:\nCI ¼ 1\nZ\nX\ndi .dj\nh\n/C0\nbi /C0 bj\n/C1\n(9)\nwhere bi is the prediction value for the larger aﬃnity di, bj is the\nprediction value for the smaller aﬃnity dj, and h is the step\nfunction, as shown in eqn (10), and Z is a normalization\nconstant that equals the number of drug –target pairs with\ndiﬀerent binding aﬃnity values.\nhðxÞ¼\n8\n>>\n<\n>>\n:\n1; if x .0\n0:5; if x ¼ 0\n0; if x\\0\n(10)\nThe metric r\nm\n2 is used to evaluate the external prediction\nperformance of QSAR (Quantitative Structure –Activity Rela-\ntionship) models. A model is acceptable if and only if\nrm2 ¼ r2 /C2ð 1 /C0 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃr2 /C0 r02p Þ, where r2 and r0\n2 are the squared\ncorrelation coeﬃcient values between the observed and pre-\ndicted values with and without intercept, respectively.\n4.3 Baseline methods\nWe compare our model with the following state-of-the-art\nmodels:\n/C15 KronRLS:18 this baseline formulates the problem of\nlearning a prediction functionf as nding a minimizer of the\nfollowing objective function:\nJðf Þ¼\nXN\ni¼1\nðyi /C0 f ðxiÞÞ2 þ lkf kk\n2 (11)\nwhere N is the number of drug–target pairs,kfkk\n2 is the norm of\nf, which is related to the kernel functionk, andl > 0 is a regu-\nlarization hyper-parameter dened by the user.\n/C15 SimBoost:19 this baseline is a gradient-boosting machine-\nbased method that constructs features of drug, target, and\ndrug–target pairs. These features are fed into a supervised\nlearning method named gradient boosting regression trees,\nwhich is derived from the gradient boosting machine model.\nUsing a gradient regression tree, for a given drug–target paird\nti,\nthe binding aﬃnity scoreˆyi is calculated as follows:\n^yi ¼ qðdti Þ¼\nXM\nm¼1\nfmðdti Þ; fm˛F (12)\nwhere M represents the number of regression trees, fm is\na regression tree andF represents the space of all possible trees.\nThe objective function with regularization in the regression tree\nset is described in the following form:\nRðqÞ¼\nX\nN\ni¼1\nlðyi; ^yiÞþ\nXM\nm¼1\naðfmÞ (13)\nwhere N is the number of drug–target pairs, l is the loss func-\ntion, yi is the true value, ˆyi is the predicted value, and a is\na tuning parameter that controls the complexity of the model.\n/C15 DeepDTA:20 this baseline trains two 3-layer CNNs using\nlabel/one-hot encoding to encode drug and protein sequences\nfor DTA prediction. The CNN model contains two independent\nCNN blocks that capture features from SMILES sequences and\nprotein sequences, respectively. The drug and target represen-\ntations are concatenated and passed to a fully connected layer\nfor DTA prediction.\n/C15 WideDTA:\n21 this baseline represents SMILES strings and\nprotein sequences as word sequences and represents the cor-\nresponding drugs and proteins through the most common\nsubsequence. In particular, drugs are described by the most\ncommon subsequences as Ligand Maximum Common\nSubstructures (LMCS); proteins are represented by the most\nconserved subsequences that are Protein Domain proles or\nMotifs (PDM) retrieved from the PROSTE database. WideDTA\ncontains four independent CNN blocks that learn features from\nSMILES sequences, LMCS, protein sequences, and PDM. The\ndrug and target representations are all concatenated and passed\nto a fully connected layer for DTA prediction.\n/C15 GANsDTA:\n24 this baseline proposes a semi-supervised\ngenerative adversarial networks (GANs)-based method to\npredict binding aﬃnity. This method comprises two types of\nnetworks, two partial GANs for the feature extraction from the\nraw protein sequences and SMILES strings separately, and\na regression network using convolutional neural networks for\nprediction.\n/C15 DeepGS:\n27 DeepGS considers both the molecular graphs\nand SMILES sequences of drugs, and uses BiGRU to extract the\nlocal chemical context of SMILES sequences and GAT to capture\nthe topological structure of molecular graphs. For the protein\nsequences, the CNN module is utilized to learn protein repre-\nsentations from the sequences of amino acids. Then, the\nrepresentations of drugs and targets are concatenated and\npassed to a fully connected layer for DTA prediction.\n/C15 GraphDTA:\n5 this baseline converts drugs from SMILES\nsequences to molecular graphs. GraphDTA consists of two\nseparate modules, a GNN module for modeling molecular\ngraphs to obtain drug representations, and a CNN module for\nmodeling protein sequences to obtain target representations.\nThe drug and target representations are concatenated and\npassed to a fully connected layer for DTA prediction.\n/C15 MATT_DTI:\n3 this baseline use SMILES sequences and\nprotein sequences as inputs. Unlike DeepDTA, MATT_DTI\nproposes a relation-aware self-attention module to model\nSMILES sequences. The relative self-attention module can\nenhance the relative position information between atoms in\n29530 | RSC Adv., 2022,12,2 9 5 2 5–29534 © 2022 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 14 October 2022. Downloaded on 11/5/2025 6:14:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\na compound while considering the relationship between\nelements. Aer the drug and target representations are ob-\ntained, a multi-head attention mechanism is used to model the\ninteraction of drug representations and protein representations\nfor DTA prediction.\n4.4 Results and discussion\nTo examine the competitiveness of our proposed model, we\ncompared the model with the current state-of-the-art models on\nthe DTA prediction task. Tables 2 and 3 show the performance\nof diﬀerent models on the Davis and Kiba datasets based on\nMSE, CI, andr\nm\n2 metrics. As shown in Tables 2 and 3, those\nclassical methods such as SimBoost19 perform worse than deep\nlearning methods. This is because classical methods rely heavily\non manually annotated features provided by a domain expert\nand drug –target similarity matrices. In comparison, deep\nlearning methods can capture more hidden features by using\nthe automatic feature extraction ability of CNN and GNN.\nFirst, we consider a few recent textual representation\napproaches such as: DeepDTA,\n20 WideDTA,21 GANsDTA,24 and\nMATT_DTI.3 Among these approaches, MATT_DTI achieved the\nbest results in terms of CI and MSE. MATT_DTI achieved CI of\n0.890, and MSE of 0.229 on the Davis dataset; also, on the Kiba\ndataset, it achieved CI of 0.889 and MSE of 0.150. This explains\nthe eﬀectiveness of the attention mechanism in learning drug\ninformation in the case of MATT_DTI.\nSecond, we also consider some graph network approaches,\nGraphDTA and DeepGS. These graph representation\napproaches can eﬀectively capture topological relationships of\ndrug molecules, which enable further performance improve-\nment. Amongst them, the GraphDTA shows a higher CI value of\n0.893 and a lower MSE of 0.229 on the Davis dataset; and on the\nKiba dataset, GraphDTA achieved 0.889 in terms of CI and 0.147\nin terms of MSE. From Tables 2 and 3, we cannd that although\nDeepGS considers both the molecular graphs and SMILES\nsequences of drugs, it performs worse than GraphDTA in terms\nof CI, MSE, andr\nm\n2. The reason is that DeepGS directly con-\ntacted the SMILES representation and graph representation,\nwhich failed to capture comprehensive information about\ndrugs.\nAs shown in Tables 2 and 3, our proposed GSATDTA has\na robust performance on both datasets. For the Davis dataset,\nour model achieved 0.906 (0.013 improvement), 0.200 (reduced\nby 0.029), and 0.732 (0.046 improvement) for CI, MSE, andr\nm\n2,\nrespectively. For the Kiba dataset, we achieved 0.902 for CI\n(0.013 improvement), 0.126 for MSE (reduced by 0.021), and\n0.790 forr\nm\n2 (0.034 improvement). We observe that our model\noutperforms existing deep-learning methods on three\nmeasures, which can be explained due to two factors:\n(1) Compared with these models, we replaced CNN with an\neﬃcient transformer to learn the representation of the protein,\nwhich can capture the long-distance relationships in the\nsequence of amino acids.\n(2) We adopted the GIN architecture to learn the structural\ninformation of the molecular graphs and employed BiGRU to\nobtain extra contextual information for the SMILES sequences.\nThen, we utilized the graph–sequence attention mechanism to\ncapture signicant information from both SMILES representa-\ntion and graph representation.\n4.5 Ablation experiment\nTo further validate the eﬀect of the diﬀerent components in\nGSATDTA, we designed two variants: GSATDTA-a and\nGSATDTA-b. GSATDTA-a is mainly for investigating the graph–\nsequence attention, while GSATDTA-b is used to demonstrate\nthe eﬀectiveness of the eﬃcient transformer.\n/C15 GSATDTA-a used BiGRU to learn the SMILES representa-\ntion and GIN to learn the graph representation. Then, it directly\ncontacted the two representations without graph –sequence\nattention. For the protein, GSATDTA-a utilized the e ﬃcient\ntransformer to learn the protein representation.\n/C15 GSATDTA-b used BiGRU to learn the SMILES representa-\ntion and GIN to learn the graph representation. Then, it utilized\nthe graph–sequence attention to fuse the SMILES representa-\ntion and the graph representation. For the protein, GSATDTA-\nb replaced the eﬃcient transformer with CNN to learn the\nprotein representation.\nFrom Table 4, we cannd that in the DTA prediction task,\nthe performance of GSATDTA-a is worse than GSATDTA. These\nresults demonstrate that the graph–sequence attention applied\nTable 3 Prediction performance on Kiba dataset\nMethod CI MSE rm\n2\nKronRLS 0.782 0.411 0.342\nSimBoost 0.836 0.222 0.629\nSequence-based approaches\nDeepDTA 0.863 0.194 0.673\nWideDTA 0.875 0.179 0.675\nGANsDTA 0.866 0.224 0.675\nMATT_DTI 0.889 0.150 0.756\nGraph-based approaches\nDeepGS 0.860 0.193 0.684\nGraphDTA 0.889 0.147 0.674\nGSATDTA (ours) 0.902 0.126 0.790\nTable 2 Prediction performance on Davis dataset\nMethod CI MSE rm\n2\nKronRLS 0.871 0.379 0.407\nSimBoost 0.872 0.282 0.644\nSequence-based approaches\nDeepDTA 0.878 0.261 0.630\nWideDTA 0.886 0.262 0.633\nGANsDTA 0.881 0.276 0.653\nMATT_DTI 0.890 0.229 0.682\nGraph-based approaches\nDeepGS 0.882 0.252 0.686\nGraphDTA 0.893 0.229 0.649\nGSATDTA (ours) 0.906 0.200 0.732\n© 2022 The Author(s). Published by the Royal Society of Chemistry RSC Adv., 2022,12,2 9 5 2 5–29534 | 29531\nPaper RSC Advances\nOpen Access Article. Published on 14 October 2022. Downloaded on 11/5/2025 6:14:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nin GSATDTA is benecial to learning a comprehensive repre-\nsentation of the drugs. Furthermore, we can also observe that\nGSATDTA-b performs worse than GSATDTA. The result of\nablation experiments indicates that the eﬃcient transformer is\neﬃcient and eﬀective to learn a good representation of proteins.\nFurther, in an attempt to verify our hypothesis about the\neﬀectiveness of BiGRU in capturing contextual information\nfrom input SMILES, we evaluated the performance of the\nproposed GSATDTA on the Davis dataset using di ﬀerent\nversions of RNNs as presented in Fig. 3. Wend that simple\nRNN attains the highest MSE value with 0.224, and there is\n0.006 improvement achieved when using GRU. Also, BiLSTM\nachieved an extra improvement of 3.7%, while BiGRU obtained\nthe lowest MSE value with 0.200, which outperformed the\nBiLSTM performance by 4.8%. This experiment demonstrates\nthe eﬀectiveness of using BiGRU for modeling the SMILES\nsequence input.\nIn the molecular property prediction task, Guo et al.\n15\nproposed that integrating the capabilities of both molecular\ngraphs and SMILES sequences can further enhance the model\nperformance. To verify this work in the DTA prediction task, we\nalso conduct an ablation experiment to investigate whether\nintegrating the capabilities of both molecular graphs and\nSMILES sequences can further enhance the model performance\nin the drug–target binding aﬃnity prediction task. We designed\nanother two variants: GSATDTA-G and GSATDTA-S. GSATDTA-G\nuses GIN to learn the graph representation while GSATDTA-S\nutilizes BiGRU to learn the SMILES representation. For the\nprotein, these two variants both employ the eﬃcient trans-\nformer to learn the protein representation. To make the\ncomparison as fair as possible, we chose GSATDTA-a to\ncompare with them. Furthermore, for the evaluation metrics of\nGSATDTA-G and GSATDTA-S, we use the same metrics as our\nproposed method, which are the MSE,r\nm\n2, and CI.\nFig. 4 and 5 illustrate the predicted against measured\n(actual) binding aﬃnity values for the Kiba dataset. A perfect\nmodel is expected to provide ap ¼ y line where predictions (p)\nare equal to the measured (y) values. From Fig. 4 and 5, we can\nobserve that compared with GSATDTA-S and GSATDTA-G,\nGSATDTA-a is denser around thep ¼ y line. More specically,\nin regions ① and ④, GSATDTA-S performed better than\nGSATDTA-G, while in regions②, ③, and ⑤, GSATDTA-G per-\nformed better than GSATDTA-S. For GSATDTA-a, only a few\npoints are spread in these areas. From Fig. 4 and 5, we can also\nnd that the overall trend of GSATDTA-a is more similar to\nGSATDTA-G, but in regions ① and ④, GSATDTA-a is more\nsimilar to GSATDTA-S and performs better than GSATDTA-G.\nWe believe that the topological structure information of the\nmolecular graph is critical to the DTA prediction task, but the\nlocal context features in SMILES sequences can be used as\nsupplemental information to predict drug –target binding\naﬃnity. In conclusion, the results of visualization indicate that\nintegrating the capabilities of both molecular graphs and\nSMILES sequences indeed can further enhance the model\nperformance in the drug–target binding aﬃnity prediction task,\nwhich is consistent with the viewpoint of Guoet al.\n15\nTable 4 Ablation experiments on Davis and Kiba\nMethod\nDavis Kiba\nCI MSE rm\n2 CI MSE rm\n2\nGSATDTA 0.906 0.200 0.732 0.902 0.126 0.790\nGSATDTA-a 0.899 0.211 0.712 0.894 0.133 0.764\nGSATDTA-b 0.894 0.216 0.697 0.890 0.136 0.752\nFig. 3 The MSE value attained by implementing GSATDTA using\ndiﬀerent types of RNN on the Davis dataset.\nFig. 4 Predictions of the GSATDTA-a and GSATDTA-G model against\nmeasured (real) binding aﬃnity values for the Kiba dataset.\nFig. 5 Predictions of the GSATDTA-a and GSATDTA-S model against\nmeasured (real) binding aﬃnity values for the Kiba dataset.\n29532 | RSC Adv., 2022,12,2 9 5 2 5–29534 © 2022 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 14 October 2022. Downloaded on 11/5/2025 6:14:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nFig. 6 and 7 show the visualization of predicted against\nmeasured (actual) binding aﬃnity values for the Davis dataset.\nWe cannd similar performance in Fig. 6 and 7. In region①,\nGSATDTA-G performed better than GSATDTA-S, while in region\n②, GSATDTA-S performed better than GSATDTA-G. GSATDTA-\na performed better than GSATDTA-G and GSATDTA-S in\nregion ③ and there are only a few points spread in these areas\nfor GSATDTA-a.\nFurthermore, we also represent the prediction performance\nof our proposed model based on the predicted value and\nmeasured (actual) value (Fig. 8 and 9).\nIn the Kiba dataset, we analyzed the samples with large\nerrors on the test set and found that when the protein sequence\nlength exceeds 1000, there will be large errors. This is because\nwe xed the length of the protein sequence to 1000 according to\nDeepDTA\n20 and DeepGS,27 which leads to information loss when\nextracting protein features. There are 4086 samples in the test\nset with protein sequence lengths longer than 1000. We calcu-\nlated that the MSE of these samples is 0.174, which is higher\nthan the overall MSE (0.126) of the test set. The MSE of the\nremaining 15 623 samples in the test set is 0.120, which is lower\nthan the overall MSE of the test set. Therefore, when the\nsequence length of the protein is longer than 1000, our model\nsuﬀers from large prediction errors.\nWe cannd the same performance in the Davis dataset. In\nthe Davis test set, there are 1333 samples with protein sequence\nlengths longer than 1000. We calculate that the MSE of these\nsamples is 0.214, which is higher than the overall MSE (0.200) of\nthe test set. The MSE of the remaining 3677 samples in the test\nset is 0.192, which is lower than the overall MSE of the test set.\n5 Conclusions\nIn this paper, we proposed a novel model based on self-\nattention, called GSATDTA, to predict the binding a ﬃnity\nbetween drugs and targets. We leveraged both the graph and\nsequence information and proposed a graph–sequence atten-\ntion mechanism to learn eﬀective drug representations for DTA\nprediction. Furthermore, we utilized an eﬃcient transformer to\nlearn the representation of proteins, which can capture the\nlong-distance relationships in the sequence of amino acids.\nExtensive experimental results show that our model outper-\nforms the state-of-the-art models in terms of MSE, CI, andr\nm\n2\non two independent datasets. Since the transformer has ach-\nieved great success in various tasks, in the future, we will\nFig. 6 Predictions of the GSATDTA-a and GSATDTA-G model against\nmeasured (real) binding aﬃnity values for the Davis dataset.\nFig. 7 Predictions of the GSATDTA-a and GSATDTA-S model against\nmeasured (real) binding aﬃnity values for the Davis dataset.\nFig. 8 Predictions of the GSATDTA model against measured (real)\nbinding aﬃnity values for the Kiba dataset.\nFig. 9 Predictions of the GSATDTA model against measured (real)\nbinding aﬃnity values for the Davis dataset.\n© 2022 The Author(s). Published by the Royal Society of Chemistry RSC Adv., 2022,12,2 9 5 2 5–29534 | 29533\nPaper RSC Advances\nOpen Access Article. Published on 14 October 2022. Downloaded on 11/5/2025 6:14:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nconsider investigating the graph transformer to learn the\nrepresentations of the drug to predict drug –target binding\naﬃnity.\nConﬂicts of interest\nThere are no conicts to declare.\nAcknowledgements\nThis work was supported by the National Natural Science\nFoundation of China (No. 61972135), the Natural Science\nFoundation of Heilongjiang Province in China (No.\nLH2020F043).\nNotes and references\n1 J. A. DiMasi, H. G. Grabowski and R. W. Hansen,J. Health\nEcon., 2016,47,2 0–33.\n2 A. D. Roses,Nat. Rev. Drug Discovery, 2008,7, 807–817.\n3 Y. Zeng, X. Chen, Y. Luo, X. Li and D. Peng,Briengs Bioinf.,\n2021, 22, bbab117.\n4 Q. Zhao, H. Zhao, K. Zheng and J. Wang,Bioinformatics,\n2022, 38, 655–662.\n5 T. Nguyen, H. Le, T. P. Quinn, T. Nguyen, T. D. Le and\nS. Venkatesh,Bioinformatics, 2021,37, 1140–1147.\n6 D. Weininger,J. Chem. Inf. Comput. Sci., 1988,28,3 1–36.\n7 J. You, B. Liu, Z. Ying, V. Pande and J. Leskovec,Adv. Neural\nInf. Process. Syst., 2018,31, 6412–6422.\n8 X. Lin, Z. Quan, Z.-J. Wang, H. Huang and X. Zeng,Briengs\nBioinf., 2020,21, 2099–2111.\n9 Z. Quan, Y. Guo, X. Lin, Z.-J. Wang and X. Zeng,2019 IEEE\nInternational Conference on Bioinformatics and Biomedicine,\nBIBM, 2019, pp. 717–722.\n10 A. C. Cheng, R. G. Coleman, K. T. Smyth, Q. Cao, P. Soulard,\nD. R. Caﬀrey, A. C. Salzberg and E. S. Huang,Nat. Biotechnol.,\n2007, 25,7 1–75.\n11 G. M. Morris, R. Huey, W. Lindstrom, M. F. Sanner,\nR. K. Belew, D. S. Goodsell and A. J. Olson, J. Comput.\nChem., 2009,30, 2785–2791.\n12 X. Chen, M.-X. Liu and G.-Y. Yan,Mol. BioSyst., 2012, 8,\n1970–1978.\n13 F. Cheng, C. Liu, J. Jiang, W. Lu, W. Li, G. Liu, W. Zhou,\nJ. Huang and Y. Tang,PLoS Comput. Biol., 2012,8, e1002503.\n14 X.-Y. Yan, S.-W. Zhang and S.-Y. Zhang,Mol. BioSyst., 2016,\n12, 520–531.\n15 Z. Guo, W. Yu, C. Zhang, M. Jiang and N. V. Chawla,\nProceedings of the 29th ACM International Conference on\nInformation & Knowledge Management, 2020, pp. 435–443.\n16 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, Ł . Kaiser and I. Polosukhin,Adv. Neural Inf.\nProcess. Syst., 2017,30, 5998–6008.\n17 H. Li, K.-S. Leung, M.-H. Wong and P. J. Ballester,Molecules,\n2015, 20, 10947–10962.\n18 T. Pahikkala, A. Airola, S. Pietil¨a, S. Shakyawar, A. Szwajda,\nJ. Tang and T. Aittokallio,Briengs Bioinf., 2015,16, 325–337.\n19 T. He, M. Heidemeyer, F. Ban, A. Cherkasov and M. Ester,\nSimBoost: a read-across approach for predicting drug-\ntarget binding aﬃnities using gradient boosting machines,\nJ. Cheminf., 2017,9, 24.\n20 H. ¨Ozt¨urk, A. ¨Ozg¨ur and E. Ozkirimli,Bioinformatics, 2018,\n34, i821–i829.\n21 H. ¨Ozt¨urk, E. Ozkirimli and A.¨Ozg¨ur, arXiv, 2019, preprint,\narXiv:1902.04166, DOI:10.48550/arXiv.1902.04166.\n22 M. Wo ´zniak, A. Wo łos, U. Modrzyk, R. L. G ´orski,\nJ. Winkowski, M. Bajczyk, S. Szymku´c, B. A. Grzybowski\nand M. Eder,Sci. Rep., 2018,8,1 –10.\n23 C. J. Sigrist, L. Cerutti, E. De Castro, P. S. Langendijk-\nGenevaux, V. Bulliard, A. Bairoch and N. Hulo, Nucleic\nAcids Res., 2010,38, D161–D166.\n24 L. Zhao, J. Wang, L. Pang, Y. Liu and J. Zhang,Front. Genet.,\n2020, 10, 1243.\n25 M. Tsubaki, K. Tomii and J. Sese,Bioinformatics, 2019, 35,\n309–318.\n26 K. Y. Gao, A. Fokoue, H. Luo, A. Iyengar, S. Dey, P. Zhang,\net al., Int. Joint Conf. Artif. Intell., 2018, 3371–3377.\n27 X. Lin, K. Zhao, T. Xiao, Z. Quan, Z.-J. Wang and P. S. Yu,\nECAI 2020, IOS Press, 2020, pp. 1301–1308.\n28 T. Song, J. Jiang, W. Li and D. Xu,IEEE J. Sel. Top. Appl. Earth\nObs. Remote Sens., 2020,13, 2853–2860.\n29 T. Song, F. Meng, A. Rodriguez-Paton, P. Li, P. Zheng and\nX. Wang,IEEE Access, 2019,7, 166823–166832.\n30 Z. Quan, X. Lin, Z.-J. Wang, Y. Liu, F. Wang and K. Li,2018\nIEEE International Conference on Bioinformatics and\nBiomedicine, BIBM, 2018, pp. 728–733.\n31 T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado and J. Dean,\nAdv. Neural Inf. Process. Syst., 2013,26, 3111–3119.\n32 T. Song, S. Pang, S. Hao, A. Rodr´ıguez-Pat´on and P. Zheng,\nNeural Process. Lett., 2019,50, 1485–1502.\n33 F. Gong, C. Li, W. Gong, X. Li, X. Yuan, Y. Ma and T. Song,\nComput. Intell. Neurosci., 2019,2019, 1939171.\n34 J. Chung, C. Gulcehre, K. Cho and Y. Bengio,International\nconference on machine learning, 2015, pp. 2067–2075.\n35 K. Xu, W. Hu, J. Leskovec and S. Jegelka,7th International\nConference on Learning Representations, ICLR, 2019, p. 2019.\n36 Z. Xiong, D. Wang, X. Liu, F. Zhong, X. Wan, X. Li, Z. Li,\nX. Luo, K. Chen, H. Jiang,et al., J. Med. Chem., 2020, 63,\n8749–\n8760.\n37 Q. Zhang and Y.-B. Yang,Adv. Neural Inf. Process. Syst., 2021,\n34, 15475–15485.\n38 D. Ulyanov, A. Vedaldi and V. Lempitsky, arXiv, 2016,\npreprint, arXiv:1607.08022, DOI:10.48550/arXiv.1607.08022.\n29534 | RSC Adv., 2022,12,2 9 5 2 5–29534 © 2022 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 14 October 2022. Downloaded on 11/5/2025 6:14:43 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.685981273651123
    },
    {
      "name": "Computer science",
      "score": 0.5427058935165405
    },
    {
      "name": "Graph",
      "score": 0.5059313178062439
    },
    {
      "name": "Drug target",
      "score": 0.46421411633491516
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4341464042663574
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4192036986351013
    },
    {
      "name": "Chemistry",
      "score": 0.35114938020706177
    },
    {
      "name": "Machine learning",
      "score": 0.3222104012966156
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2441917061805725
    },
    {
      "name": "Engineering",
      "score": 0.1218823790550232
    },
    {
      "name": "Voltage",
      "score": 0.09282451868057251
    },
    {
      "name": "Biochemistry",
      "score": 0.08352741599082947
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}