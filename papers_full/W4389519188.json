{
    "title": "Findings of the 2023 Conference on Machine Translation (WMT23): LLMs Are Here but Not Quite There Yet",
    "url": "https://openalex.org/W4389519188",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2557422917",
            "name": "Tom Kocmi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1951607105",
            "name": "Eleftherios Avramidis",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2654261484",
            "name": "Rachel Bawden",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2003230525",
            "name": "Ondřej Bojar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2510260199",
            "name": "Anton Dvorkovich",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A141155639",
            "name": "Christian Federmann",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2040094215",
            "name": "Mark Fishel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2111277974",
            "name": "Markus Freitag",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2790711779",
            "name": "Thamme Gowda",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A175854727",
            "name": "Roman Grundkiewicz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2179150568",
            "name": "Barry Haddow",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2095651410",
            "name": "Philipp Koehn",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2170157735",
            "name": "Benjamin Marie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109806231",
            "name": "Christof Monz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096380874",
            "name": "Makoto Morishita",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2704763796",
            "name": "Kenton Murray",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1996651523",
            "name": "Makoto Nagata",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2134014694",
            "name": "Toshiaki Nakazawa",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2161956338",
            "name": "Martin Popel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099167224",
            "name": "Maja Popović",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2640962835",
            "name": "Mariya Shmatova",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2963697731",
        "https://openalex.org/W4389519475",
        "https://openalex.org/W2252166243",
        "https://openalex.org/W3105214104",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2922158773",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4389524491",
        "https://openalex.org/W4389519548",
        "https://openalex.org/W2889330990",
        "https://openalex.org/W4307204061",
        "https://openalex.org/W2614606538",
        "https://openalex.org/W2903193068",
        "https://openalex.org/W2159107349",
        "https://openalex.org/W4389519073",
        "https://openalex.org/W4387724381",
        "https://openalex.org/W2250600805",
        "https://openalex.org/W2147192413",
        "https://openalex.org/W4389523797",
        "https://openalex.org/W3039695075",
        "https://openalex.org/W3120000557",
        "https://openalex.org/W3100880133",
        "https://openalex.org/W3152788712",
        "https://openalex.org/W3104976898",
        "https://openalex.org/W2885096256",
        "https://openalex.org/W3186081172",
        "https://openalex.org/W3029164262",
        "https://openalex.org/W4287889965",
        "https://openalex.org/W2169279899",
        "https://openalex.org/W2970279348",
        "https://openalex.org/W4389523816",
        "https://openalex.org/W2970038984",
        "https://openalex.org/W4389524035",
        "https://openalex.org/W4389524602",
        "https://openalex.org/W2963532001",
        "https://openalex.org/W4389524454",
        "https://openalex.org/W22168010",
        "https://openalex.org/W3035408261",
        "https://openalex.org/W630532510",
        "https://openalex.org/W2564739834",
        "https://openalex.org/W2115081467",
        "https://openalex.org/W4287727391",
        "https://openalex.org/W4285077564",
        "https://openalex.org/W2726119835",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W4296932804",
        "https://openalex.org/W4385569911",
        "https://openalex.org/W4389519456",
        "https://openalex.org/W4389519324",
        "https://openalex.org/W3159892921",
        "https://openalex.org/W3175458736",
        "https://openalex.org/W4389519214",
        "https://openalex.org/W3106146701",
        "https://openalex.org/W4389519594",
        "https://openalex.org/W4389524023",
        "https://openalex.org/W2250342921",
        "https://openalex.org/W3174634068",
        "https://openalex.org/W4389519440",
        "https://openalex.org/W3120929527",
        "https://openalex.org/W2184135559",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2087735403",
        "https://openalex.org/W4321472057",
        "https://openalex.org/W3035016936"
    ],
    "abstract": "Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popović, Mariya Shmatova. Proceedings of the Eighth Conference on Machine Translation. 2023.",
    "full_text": "Proceedings of the Eighth Conference on Machine Translation (WMT), pages 1–42\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n1\nFindings of the 2023 Conference on Machine Translation (WMT23):\nLLMs Are Here But Not Quite There Yet\nTom Kocmi\nMicrosoft\nEleftherios Avramidis\nDFKI\nRachel Bawden\nInria, Paris\nOndˇrej Bojar\nCharles University\nAnton Dvorkovich\nDubformer\nChristian Federmann\nMicrosoft\nMark Fishel\nUniversity of Tartu\nMarkus Freitag\nGoogle\nThamme Gowda\nMicrosoft\nRoman Grundkiewicz\nMicrosoft\nBarry Haddow\nUniversity of Edinburgh\nPhilipp Koehn\nJohns Hopkins University\nBenjamin Marie\n4i.ai\nChristof Monz\nUniversity of Amsterdam\nMakoto Morishita\nNTT\nKenton Murray\nJohns Hopkins University\nMasaaki Nagata\nNTT\nToshiaki Nakazawa\nUniversity of Tokyo\nMartin Popel\nCharles University\nMaja Popovi´c\nDublin City University\nMariya Shmatova\nDubformer\nJun Suzuki\nTohoku University\nAbstract\nThis paper presents the results of the General\nMachine Translation Task organised as part of\nthe 2023 Conference on Machine Translation\n(WMT). In the general MT task, participants\nwere asked to build machine translation sys-\ntems for any of 8 language pairs (correspond-\ning to 14 translation directions), to be evaluated\non test sets consisting of up to four different do-\nmains. We evaluate system outputs with profes-\nsional human annotators using a combination\nof source-based Direct Assessment and scalar\nquality metric (DA+SQM).\n1 Introduction\nThe Eighth Conference on Machine Translation\n(WMT23)1 was held at EMNLP 2023 and hosted\na number of shared tasks on various aspects of\nmachine translation (MT). This conference built\non 17 previous editions of WMT as a workshop\nor a conference (Koehn and Monz, 2006; Callison-\nBurch et al., 2007, 2008, 2009, 2010, 2011, 2012;\nBojar et al., 2013, 2014, 2015, 2016, 2017, 2018;\nBarrault et al., 2019, 2020; Akhbardeh et al., 2021;\nKocmi et al., 2022).\nFollowing last year’s shift from focusing mainly\non the news domain, we have continued to explore\nthe capabilities of “General Machine Translation”.\n1http://www2.statmt.org/wmt23/\nWhile the news domain provided a clear and famil-\niar benchmark, we realized the need to test MT in\nmore diverse settings. Our goal is to assess MT\nsystems’ ability to handle a broader range of lan-\nguage use. How to test general MT performance is\na research question in itself. Countless phenomena\ncould be evaluated, the most important being:\n• various domains (news, medicine, IT, patents,\nlegal, social, gaming, etc.)\n• style of text (formal or spoken language, fic-\ntion, technical reports, etc.)\n• robustness to non-standard (or noisy) user-\ngenerated content (grammatical errors, code-\nswitching, abbreviations, etc.)\nEvaluating all phenomena is nearly impossible\nand creates numerous unforeseen problems. There-\nfore, we decided to simplify the problem and start\nwith an evaluation of different domains. We se-\nlected the following domains: news, e-commerce,\nsocial/user-generated content (UGC), speech, and\nmanuals. They were chosen to represent topics\nwith different content styles and to be understand-\nable for humans without special in-domain knowl-\nedge, thus not requiring specialized translators or\nhuman raters for evaluation. Due to limited access\n2\nto monolingual data across all languages, each lan-\nguage direction contains only a subset of up to four\ndomains.\nIn addition to language pairs evaluated last year:\nCzech→Ukrainian,\nEnglish↔Chinese,\nEnglish→Czech,\nEnglish↔German,\nEnglish↔Japanese,\nEnglish↔Russian,\nUkrainian→English,\nwe introduce a new language pair to WMT, namely:\nEnglish↔Hebrew.\nOther than language pairs, there are several dif-\nferences with respect to last year’s task. All lan-\nguage pairs are provided with the sentence bound-\naries marked except for English↔German, where\nwe decided to experiment with paragraph-level\ntranslation. Another significant change for this\nyear is the unification of our human evaluation\nprotocol. We no longer rely on reference-based\nMTurk evaluation and move the evaluation towards\nsource-based DA+SQM evaluation (introduced last\nyear) with professional annotators. Finally, this\nyear’s shared task included an increased number\nof test suites (Section 6), allowing the evaluation\nof MT outputs from different perspectives, includ-\ning a range of linguistic phenomena, purposely\ndifficult sentences, specialist domains, gendered\ntranslations and non-standard UGC translation.\nAll General MT task submissions, sources, ref-\nerences and human judgements are available at\nGithub 2. The interactive visualization and com-\nparison of differences between systems can be\nbrowsed online on an interactive leaderboard 3\nusing MT-ComparEval (Klejch et al., 2015; Su-\ndarikov et al., 2016).\nThe structure of the paper is as follows. We\ndescribe the process of collecting, cleaning and\ntranslating the test sets in Section 2 followed by\na summary of the permitted training data for the\nconstrained track Section 3. We list all submit-\nted systems in Section 4. The human evaluation\napproach of DA+SQM is described in Section 5.\nFinally, Section 6 describes the test suites and sum-\nmarises their conclusions.\n2https://github.com/wmt-conference/\nwmt23-news-systems\n3http://wmt.ufal.cz\nSummary of the WMT2023 General MT task\nThe main findings are as follows:\n• Large Language Models (LLMs) exhibit\nstrong performance across the majority of lan-\nguage pairs, although this is based only on two\nLLM-based system submissions. Test suite\nanalysis revealed that although GPT4 excelled\nin some areas (e.g. UGC translation) strug-\ngled with other aspects such as speaker gender\ntranslation and specific domains (e.g. legal),\nwhereas it ranked lower than encoder-decoder\nsystems when translating from English into\nless-represented languages (e.g. Czech and\nRussian)\n• We have observed a decline in the number of\nsubmissions into the constrained track. Conse-\nquently, we plan to re-evaluate the definition\nand the incentives of the constrained track and\nconsider incorporating open-source LLMs in\nfuture evaluations.\n• We demonstrate the feasibility of paragraph-\nlevel German↔English tasks, although more\ninvestigation would be required before gener-\nalising to all language pairs.\n• Professional human translations do not always\nguarantee high quality. For Hebrew↔English,\nour references are likely to be post-edited\nMT, while for Chinese →English, the refer-\nence translation is worse than the majority of\nautomatic translations.\n• The manual evaluation results obtained from\nDA+SQM and MQM methods yield compara-\nble cluster rankings.\n2 Test Data\nIn this section, we describe the process of collect-\ning data in Section 2.1, followed by the explanation\nof preprocessing steps in Section 2.2. Producing\nhuman references is summarized in Section 2.3 and\nlastly test set analysis is conducted in Section 2.4.\n2.1 Collecting test data\nAs in the previous years, the test sets consist of\nunseen translations collected especially for the task.\nThis has become even more important with the\nrise of LLMs trained on unspecified training data.\nTo prevent possible contamination, we focused on\ncollecting as recent data as possible across various\n3\nLang. pair Domain name Domain type #docs #segs #segs/#docs\ncs→uk * * 156 2017 12.93\ngames News 17 180 10.59\nnews News 35 567 16.20\nofficial Social/UGC 26 347 13.35\npersonal Social/UGC 31 390 12.58\nvoice Speech 47 533 11.34\nde→en * * 210 549 2.61\nmanuals Manuals 15 74 4.93\nmastodon Social/UGC 95 103 1.08\nnews News 47 277 5.89\nuser_review E-commerce 53 95 1.79\nen→{cs,he,ja,ru,uk,zh} * * 192 2074 10.80\nmastodon Social/UGC 79 504 6.38\nnews News 30 516 17.20\nspeech Meeting notes 25 547 21.88\nuser_review E-commerce 58 507 8.74\nen→de * * 192 557 2.90\nmastodon Social/UGC 79 212 2.68\nnews News 30 139 4.63\nspeech Meeting notes 25 113 4.52\nuser_review E-commerce 58 93 1.60\nhe→en * * 94 1910 20.32\nnews News 68 1558 22.91\nreviews Social/UGC 26 352 13.54\nja→en * * 282 1992 7.06\nad Social/UGC 53 245 4.62\nec Social/UGC 25 255 10.20\nnews News 37 495 13.38\nqa Conversational 118 497 4.21\nuser_review E-commerce 49 500 10.20\nru→en * * 162 1723 10.64\nmanuals Manuals 15 505 33.67\nnews News 54 676 12.52\nreviews Social/UGC 93 542 5.83\nuk→en * * 132 1826 13.83\nclipboard Social/UGC 30 504 16.80\nnews News 26 514 19.77\nother Social/UGC 27 538 19.93\nvoice Speech 49 270 5.51\nzh→en * * 179 1976 11.04\nmanuals Manuals 14 487 34.79\nnews News 38 763 20.08\nuser_review E-commerce 127 726 5.72\nTable 1: Test set statistics per direction and domain (rows marked * are over all domains). Note that en→de shares source test\ndata with the other from-English directions, but as translation and evaluation for both en→de and de→en were carried out on the\nparagraph level (a segment therefore being a paragraph rather than a sentence), this results in a lower number of segments per\ndocument. The domain name is as indicated in the released test sets and domain type indicates the broader domain category.\ndomains. This task is incredibly difficult and needs\nfurther investigation in future years. There are three\nmain limitations:\n• Finding sources with different domains.\n• Finding data that are in the public domain or\nunder open licenses.\n• Finding recently created data to minimize\nthe risk of them being part of the training\npipelines.\nThe test sets are publicly released to be used as\ntranslation benchmarks. Here we describe the test\nsets’ production and composition.\nWe decided to collect data from 5 domains\n(news, social/user-generated, e-commerce, man-\nuals, and speech). For all language pairs, we aimed\nfor a test set size of 2,000 sentences and to ensure\nthat the test sets were “source-original”, namely\nthat the source text was first written in the source\nlanguage, and then the target text is the human\n4\ntranslation. This is to avoid “translationese” effects\non the source language, which can have a detri-\nmental impact on the accuracy of evaluation (Toral\net al., 2018; Freitag et al., 2019; Läubli et al., 2020;\nGraham et al., 2020). We collected roughly the\nsame number of sentences for each domain. For\nsome languages, we could not locate high qual-\nity data and therefore we selected more sentences\nfrom other domains. Note that descriptions in this\nsection refer to source monolingual data when men-\ntioning a language.\nNews domain For most languages this domain\ncontains data prepared in the same way as in previ-\nous years (Akhbardeh et al., 2021). We collected\nnews articles from February 2023 extracted from\nonline news sites, preserving document boundaries.\nWe expect that news domain text will generally be\nof high quality. The news in Hebrew was kindly\nprovided by the Israeli Association of Human Lan-\nguage Technologies (IAHLT).4 These are samples\nof originally Hebrew texts from news published in\nIsrael Hayom5 in 2022.\nE-commerce domain (product reviews) This\ndomain consists of user reviews of different Ama-\nzon products selected from the publicly available\nmultilingual corpus (Keung et al., 2020). This\ncorpus was designed for multilingual text classi-\nfication and consists of reviews written in English,\nJapanese, German, French, Spanish, and Chinese,\nbetween 2015 and 2019. We used the test parts of\nthe English, German, Japanese and Chinese cor-\npora for extracting the source part of the WMT test\nset. The reviews were selected so that the resulting\ncorpus covers each product, all rating scores for\nthe product, and the lexical diversity is maximized.\nThe lexical diversity was estimated as a simple\nratio between the number of distinct words/char-\nacters (vocabulary) divided by the total number of\nwords/characters.\nSocial/user-generated domain For English and\nGerman, we relied on the Mastodon Social API.6\nMastodon is a federated social network that is com-\npatible with the W3C standard ActivityPub (Web-\nber et al., 2018). Users publish short-form content\nsimilar to tweets that are referred to as “toots” for\nhistorical reasons. As this is a decentralized social\n4https://www.iahlt.org\n5https://www.israelhayom.co.il\n6https://mastodon.social/api/v1/timelines/\npublic\nmedia network, different servers have very different\ndata, policies, communities, and uses. We decided\nto use mastodon.social, the original server, as\nit has a large community as well as publicly avail-\nable toots. We collected data in early May of 2023.\nWe used the reported language ID label, but were\nonly able to collect enough data in German and\nEnglish. We only collected toots with more than\n150 characters in length in order to allow for data\nthat was more likely to be semantically interesting\nfor evaluating translation systems.\nFor Hebrew, we used comments on news articles\nfrom the Israel Hayom site mentioned above. This\ndata was also provided by IAHLT.\nFor Russian, we used data from the Geo Re-\nviews Dataset containing reviews about organiza-\ntions published on Yandex Maps and open for aca-\ndemic and research purposes.7\nFor Japanese, we used product descriptions of\na b2b e-commerce site and search advertising text\nads for the social and user-generated domain, be-\ncause we could not obtain high-quality data for this\ndomain type. MonotaRo Co., Ltd. provided prod-\nuct descriptions of their private label brands listed\non their b2b e-commerce site.8 We defined a doc-\nument for a product description as a combination\nof a title, product description, and cautionary note.\nCyberAgent, Inc.9 provided search advertising text\nads with their client’s consent. We defined a docu-\nment for an ad as the longest possible combination\nof multiple titles and descriptions.\nManuals For this domain, we primarily sourced\nscanned versions of different mostly gaming man-\nuals provided by Centific10. These were then con-\nverted to digital text format using Optical Character\nRecognition (OCR) technology. Given the inaccu-\nracies of OCR, the digitized content underwent a\nsubsequent post-editing phase, where humans re-\nviewed and corrected any errors. The selection of\nmanuals ranged across various sources, and none\nof them were older than five years.\nSpeech The exact data types used in the “conver-\nsational” or “speech” domain vary across language\npairs.\nFor English→Czech, the data comes from the\ntest set which was created for the 2023 instance of\n7https://github.com/yandex/\ngeo-reviews-dataset-2023\n8https://www.monotaro.com/\n9https://www.cyberagent.co.jp\n10https://www.centific.com\n5\nAutoMin 2023 (Ghosal et al., 2022). 11 The texts\nare manually curated transcripts of project meet-\nings, same in style as released in ELITR Minuting\nCorpus (Nedoluzhko et al., 2022). The meetings\nwere held mostly remotely or in a hybrid form, all\nmeeting participants were non-native speakers of\nEnglish and the meetings were always on rather\ntechnical and in-depth topics. Our manual cura-\ntion corrected ASR errors (but not errors in English\ngrammar or vocabulary) and de-identified the tran-\nscripts, replacing names with placeholders (“PER-\nSONxy”, “PROJECTxy” and similar). For person\nnames, round brackets are used at the beginnings\nof lines to indicate the speaker and square brack-\nets are used in the text when the person was men-\ntioned. The data contain also some markup, e.g.\n“<unintelligible/>”. These conventions are\nlikely to be distorted by translation systems and\nwe also noticed that they were distorted in the ref-\nerence translation (the style of the brackets was\nignored). This tiny detail can influence both man-\nual and automatic scoring on this domain.\nFor Japanese, we used question-answer pairs\nfrom a community question-answering service.\nNTT Resonant Inc., which recently merged with\nNTT DOCOMO, INC., provided question-answer\npairs from their website, Oshiete! goo.12 For every\nquestion-answer pair, we defined a document as\na combination of a question and its best answer\nmarked by the user.\nCzech and Ukrainian source texts Source texts\nfor Czech →Ukrainian and Ukrainian →English\ntranslation included the News domain as described\nabove and texts collected through the Charles\nTranslator for Ukraine.13 With users’ consent, the\nservice can log their inputs for the purpose of cre-\nating a dataset of real use cases. The datasets are\nextracted from the inputs collected from May 2022\nto April 2023.\nThe Charles Translator mobile app supports\nvoice input, which is converted to text using Google\nASR (automatic speech recognition). The texts\ncollected this way were marked as the voice\ndomain. For Ukrainian →English, the remain-\ning Ukrainian inputs were classified either as\nclipboard (texts inserted to the Charles Trans-\nlator using the Paste from clipboard button) and\nother. The clipboard texts are more likely to in-\n11https://ufal.github.io/automin-2023/\n12https://oshiete.goo.ne.jp/\n13http://translator.cuni.cz\nclude formal communication copied from web sites,\nbut we noticed it includes personal communication\n(copied from chat applications) as well. Thus for\nCzech→Ukrainian, we decided to classify the re-\nmaining Czech inputs either as official (formal\ncommunication) or personal (personal communi-\ncation), ignoring whether they were inserted from\na clipboard or written using a keyboard.\nThe texts were filtered and pseudonymized in the\nsame way as last year (Kocmi et al., 2022), so for\nexample we asked the annotators not to delete or\nfix noisy inputs as long as they are comprehensible.\nThere was one exception from this rule this year:\nthe Czech voice domain data was post-edited to\nfix ASR errors, including missing punctuation and\ncasing.\nThe source texts were translated by professional\ntranslators principally following the brief in Ap-\npendix C. Last year, parts of the Ukrainian→Czech\ntest set was detected to be post-edited MT. There-\nfore this year, we decided to hire two professional\ntranslators directly without the mediation of a trans-\nlation agency, we emphasised the rule that the trans-\nlations must be done from scratch (without MT\npostediting and without translation memories). We\ncould not detect any MT postediting in the resulting\ntranslations.\n2.2 Human preprocessing of test data\nAlthough testing of robustness of MT is an impor-\ntant task, the noisy data introduces problems for\nhuman translators and annotators. Therefore, we\ndecided to discard data considered too noisy. Fur-\nthermore, publicly available data often contains\ninappropriate content, which can stress either hu-\nman translators or human annotators, leading to\na decrease in the quality (for example, translators\nrefuse to translate political content considered cen-\nsored in their countries).\nTherefore, we asked humans to check collected\ndata and carry out minor corrections (mainly check-\ning sentence splits and discarding similar or re-\npeated content). This was sufficient for the news\ndomain because it was often clean and without\nserious problems. However, with the expansion to-\nwards general MT, we find ourselves running into\nan issue of source data being noisier and less well\nformatted and that therefore needs to be handled\nbefore translation. Furthermore, we asked them to\nremove shortest documents to keep longer context.\nThe source data for test sets therefore goes through\n6\nhuman validation checks involving linguists dis-\ncarding inappropriate content altogether and carry-\ning out minor textual corrections to the data. You\ncan find the linguistic brief for prepossessing in\nAppendix B.\n2.3 Test set translation\nThe translation of the test sets was performed by\nprofessional translation agencies, according to the\nbrief in Appendix C. Different partners sponsored\neach language pair and various translation agencies\nwere therefore used, which may affect the quality\nof the translation.\nRegrettably, upon reviewing translations pro-\ncured from one of the agencies (the one respon-\nsible for English to Hebrew and Hebrew to English\ntranslations), it appeared that the translations might\nhave been post-edited from publicly available on-\nline translation systems. This observation contra-\ndicts the initial instruction provided for agency that\nprecluded the use of any automated translation plat-\nforms. While the agency has asserted that their pro-\nfessional translations conducted translations from\nscratch, our evaluation suggested otherwise. Mov-\ning forward, we propose to build a step-by-step\nverification system to avoid such discrepancies.\nHuman translations would not be possible with-\nout the sponsorship of our partners: Microsoft,\nToloka AI, Google, Charles University, NTT, and\nDubformer.\n2.4 Test set analysis\nAs described previously, the chosen domains,\nsources for the data and the number of sentences\nper domain was subject to the availability of high\nquality data in each language direction. For exam-\nple, while the news domain was available for all lan-\nguage directions, social media data was only avail-\nable for English, German (both from Mastodon)\nand Hebrew (from comments on news articles).\nThe number of documents, segments, average doc-\nument length and type-token ratio (of the source\nside of the test sets) are given in Table 1.\nDocument context Document context is avail-\nable for all language directions, although the av-\nerage document length varies both by domain and\nlanguage direction. Manuals tend to represent the\nlongest domains, followed by the news domain.\nThe social media domain tends to represent the\nshortest documents. along with reviews. Note\nthat this year, we piloted translation and evalua-\ntion of en→de and de→en at the paragraph level\n(with each segment therefore containing several\nsentences), with the aim of avoiding the constraint\nof having a one-to-one mapping at the level of the\nsentence between source texts and their translations.\nThis is visible in the statistics in Table 1 as the num-\nber of segments is lower for these two directions,\nas is the average document length.\nLexical diversity We can compare the type-\ntoken ratio (TTR) to get an idea of the relative\nlexical diversity of (i) domains and (ii) original\nvs. translated sentences.14,15 Raw TTRs for each\nlanguage pair and domain are shown in Table 11 in\nAppendix D. Regarding domains, the TTR appears\nhighest for texts mastodon, perhaps illustrating the\ndiversity of conversational topics and also of the\npotentially non-standard nature of the texts. User\nreviews appear to have the lowest TTR, most likely\ndue to the fact that similar vocabulary is used across\nreviews. The TTR of course differs according to\nthe language in question, according to the differing\nmorphological properties.\nAnonymisation and markup A particularity of\nthe ‘speech’ domain is the presence of placeholders\nfor anonymised elements and markup (in the form\nof tags). For example, there are 35 placeholders\nsurrounded either by square or rounded brackets to\nindicate different people, organisations and projects\n(e.g. (PERSON1), [PERSON9], [ORGANIZA-\nTION4], [PROJECT8], etc.). The ‘person’ tags\nare used both in-text to replace the names of people\nand at the beginning of lines to indicate who is talk-\ning. Markup is added to indicate speakers talking\nat the same time ( <parallel_talk>), unintelli-\ngible passages ( <unintelligible/>), laughter\n(<laugh/>) and other noise (<other_noise/>).\n2.5 Test suites\nIn addition to the test sets of the regular domains,\nthe test sets given to the system participants were\naugmented with several test suites , i.e. custom-\nmade test sets focusing on particular aspects of\nMT translation. The test suites were contributed\nand evaluated by test suite providers as part of a\n14The TTR is the ratio of unique tokens to total tokens,\nand it is higher the diverse the vocabulary of a text is. It is\ndependent on the morphological complexity of a language,\nbut can also vary due to other factors.\n15Texts are tokenised using the language-specific Spacy\nmodels (Honnibal and Montani, 2017) where available. For\nHebrew, we took the multilingual Spacy model, since a\nlanguage-specific one was not available.\n7\ndecentralized sub-task, which will be detailed in\nSection 6.\n3 Training Data\nSimilar to the previous years, we provide a se-\nlection of parallel and monolingual corpora for\nmodel training. The provenance and statistics\nof the selected parallel datasets are provided in\nAppendix in Table 9 and Table 10. Specifi-\ncally, our parallel data selection include large mul-\ntilingual corpora such as Europarl-v10 (Koehn,\n2005), Paracrawl-v9 (Bañón et al., 2020), Com-\nmonCrawl, NewsCommentary-v18, WikiTitles-v3,\nWikiMatrix (Schwenk et al., 2021), TildeCor-\npus (Rozis and Skadin, š, 2017), OPUS (Tiedemann,\n2012), UN Parallel Corpus (Ziemski et al., 2016),\nand language-specific corpora such as CzEng-\nv2.0 (Kocmi et al., 2020), YandexCorpus,16 ELRC\nEU Acts, JParaCrawl (Morishita et al., 2020),\nJapanese-English Subtitle Corpus (Pryzant et al.,\n2018), KFTT(Neubig, 2011), TED (Cettolo et al.,\n2012), CCMT, and back-translated news. Links for\ndownloading these datasets were provided on the\ntask web page;17 in addition, we automated the data\npreparation pipeline using MTDATA (Gowda et al.,\n2021).18 MTDATA downloads all the mentioned\ndatasets, except CCMT and CzEng-v2.0, which\nrequired user authentication. This year’s mono-\nlingual data include the following: News Crawl,\nNews Discussions, News Commentary, Common-\nCrawl, Europarl-v10 (Koehn, 2005), Extended\nCommonCrawl (Conneau et al., 2020), Leipzig\nCorpora (Goldhahn et al., 2012), UberText and Le-\ngal Ukrainian.\n4 System submissions\nThis year, we received a total of 72 primary sub-\nmissions from 17 participants. In addition, we col-\nlected translations from online MT systems across\nall language pairs. Online system outputs come\nfrom 6 public MT services and were anonymized\nas ONLINE-{A,B,G,M,W,Y}, which added addi-\ntional 77 system outputs. The participating systems\nare listed in Table 2 and detailed in the rest of this\nsection.\nFinally, we added translations by three con-\ntrastive systems. Two of them are based on\n16https://github.com/mashashma/WMT2022-data\n17https://statmt.org/wmt23/translation-task.\nhtml\n18http://www2.statmt.org/wmt23/mtdata\nthe NLLB translation model (NLLB Team et al.,\n2022) modified by (Freitag et al., 2023) to have a\nsuboptimal performance, using (i) greedy search\n(NLLB_Greedy) and (ii) following minimum\nBayes risk decoding (MBR) optimizing the BLEU\nmetric (NLLB_MBR_BLEU). Neither of them is\nthe official (and better performing) NLLB model.\nThe third contrastive translation is produced by the\nlarge language model GPT4 using 5-shot prompt-\ning with fixed random translation examples, using\nthe exact prompt by Hendy et al. (2023) together\nwith their predefined few-shot examples. For lan-\nguages not evaluated in their study, we took exam-\nples from the last WMT test sets.\nAppendix E provides details of the submitted\nsystems if the authors provided such details.\n4.1 Constrained and unconstrained tracks\nFor presentation of the results, systems are treated\nas either constrained or unconstrained. A system\nis classified as constrained if the authors reported\ntraining only on the provided data and adhering to\nthe rules describing the use of publicly available\npre-trained models. The constrained track imposes\nrestrictions on training data, metrics, and pretrained\nmodels, while the unconstrained track provides\nunrestrained flexibility.\nThe constrained track limitations are mainly\naround the training and testing data, together with\nthe limitation on pretrained models:\n• Training data: Only data specified for the\ncurrent year are permissible, see Section 3.\nMultilingual systems can be used as long as\nthey only use WMT23 data.\n• Metrics: The training pipeline can use pre-\ntrained metrics evaluated in previous WMT\nMetrics shared tasks, e.g., COMET (Rei et al.,\n2022), Bleurt (Yan et al., 2023).\n• Pretrained models: only the following list\nof models is allowed together with all their\npublic sizes: mBART (Liu et al., 2020),\nBERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019), XLM-RoBERTa (Conneau et al.,\n2020), sBERT (Reimers and Gurevych, 2019),\nand LaBSE (Feng et al., 2022).\n• Linguistic tools: Basic tools like taggers,\nparsers, and morphology analyzers are al-\nlowed.\n8\nSubmission Name Language Pairs System Description\nAIRC de-en, en-ja, ja-en, en-de (Rikters and Miwa, 2023)\nANVITA ja-en, zh-en, en-ja, en-zh (no associated paper)\nCUNI-D OCTRANSFORMER en-cs (Popel, 2020)\nCUNI-GA en-cs, cs-uk (Jon et al., 2023)\nCUNI-T RANSFORMER en-cs, cs-uk (Popel, 2020)\nGPT4-5 SHOT All language pairs (Hendy et al., 2023)\nGTCOM de-en, ja-en, he-en, en-cs, en-he, cs-uk, en-uk, uk-en (Zong, 2023)\nHW-TSC de-en, en-zh, zh-en (Wu et al., 2023b)\nIOL-R ESEARCH zh-en, en-zh (Zhang, 2023)\nTEAM KYB ja-en, en-ja (LI et al., 2023)\nLAN-BRIDGE MT All language pairs (Wu and Hu, 2023)\nMUNI-NLP cs-uk (Rychlý and Teslia, 2023)\nNAIST-NICT en-ja, ja-en (Deguchi et al., 2023)\nNLLB_G REEDY All language pairs (Freitag et al., 2023)\nNLLB_MBR_BLEU All language pairs (Freitag et al., 2023)\nONLINE-A All language pairs -\nONLINE-B All language pairs -\nONLINE-G All language pairs -\nONLINE-M en-ru, zh-en, en-zh, de-en, en-cs, ja-en, en-de, en-ja,\nru-en\n-\nONLINE-W en-uk, ja-en, de-en, en-ja, ru-en, en-de, uk-en, en-ru,\nzh-en, en-cs, en-zh, cs-uk\n-\nONLINE-Y All language pairs -\nPROMT en-ru, ru-en (Molchanov and Kovalenko,\n2023)\nSRPH he-en, en-he (Cruz, 2023)\nSKIM en-ja, ja-en (Kudo et al., 2023)\nUPC ITE -CLILLF fr-en, en-fr (no associated paper)\nUVA-LTL he-en, en-he (Wu et al., 2023a)\nYISHU zh-en, en-zh (Min et al., 2023)\nLANGUAGE X en-zh, en-uk, ru-en, uk-en, en-de, he-en, ja-en, zh-en,\nen-he, de-en, en-cs, en-ja, en-ru\n(Zeng, 2023)\nTable 2: Participants in the General MT shared task. Online system translations were not submitted by their respective companies\nbut were obtained by us, and are therefore anonymized in a fashion consistent with previous editions of the task.\n9\nThe online systems and contrastive systems are\ntreated as unconstrained during the automatic and\nhuman evaluation.\n4.2 OCELoT\nWe used the open-source OCELoT platform 19\nto collect system submissions again this year.\nThe platform provides anonymized public leader-\nboards20 and was also used for two other WMT23\nshared tasks: Biomedical (Neves et al., 2023) and\nSign Language Translation (Müller et al., 2023).\nAs in previous years, only registered and verified\nteams with correct contact information were al-\nlowed to submit their system outputs and each ver-\nified team was limited to 7 submissions per test\nset. Submissions on leaderboards with BLEU (Pa-\npineni et al., 2002) and CHR F (Popovi´c, 2015)\nscores from SacreBLEU (Post, 2018) were dis-\nplayed anonymously to avoid publishing rankings\nbased on automatic scores during the submission\nperiod. Until one week after the submission period,\nteams could select a single primary submission per\ntest set, specify if the primary submission followed\na constrained or unconstrained setting, and submit\na system description paper abstract. These were\nmandatory for a system submission to be included\nin the human evaluation campaign.\n5 Human Evaluation\nHuman evaluation for all language translation direc-\ntions is performed with source-based (“bilingual”)\nDirect Assessment (DA, Graham et al., 2013) of in-\ndividual segments in document context with Scalar\nQuality Metrics (SQM) guidelines, mostly follow-\ning the setup established at WMT22 (DA+SQM,\nKocmi et al., 2022). DA+SQM asks the annotators\nto provide a score between 0 and 100 on a sliding\nscale, but the slider is presented with seven labelled\ntick marks, as demonstrated in Figure 1.\nTwo different annotation platforms and four\ndistinct pools of annotators (Table 3) are used\nfor annotation of different language pairs. We\nuse the open-source framework Appraise (Feder-\nmann, 2018) for the evaluation of English→Czech,\nEnglish↔{Chinese, German, Japanese}, and\nCzech→Ukrainian. Toloka AI 21 hosts the eval-\nuation of English↔{Hebrew, Russian, Ukrainian}\nusing their own implementation of the source-based\n19https://github.com/AppraiseDev/OCELoT\n20https://ocelot-wmt23.mteval.org\n21https://toloka.ai\ndocument-level DA+SQM task, which is as close\nas possible to the Appraise user interface.\nWe keep the selection process of documents for\nannotation mostly the same as in the previous year.\nThe only change made in order to align closer\nwith the MQM-based evaluation run at the Met-\nrics shared task (Freitag et al., 2023) is to present\nthe first 10 segments from a document instead of\nrandom 10 consecutive segments.\nWe again collect both segment-level scores and\ndocument-level scores, but compute rankings based\non segment scores only.\n5.1 Human annotators\nAnnotations for different language pairs are pro-\nvided by four different parties with their pool of\nannotators of distinct profiles as presented in Ta-\nble 3. We shift towards more professional or semi-\nprofessional annotators’ pools and decide not to use\nMTurk annotations as in past years for reference-\nbased DA evaluation for into-English language di-\nrections.\nAssessments for English ↔{Chinese, German,\nJapanese} are provided by Microsoft and their pool\nof bilingual target-language native speakers, profes-\nsional translators or linguists, highly experienced in\nMT evaluation. Microsoft monitors the annotators’\nperformance over time and permanently removes\nfrom the pool those who fail quality control, which\nincreases the overall quality of the human assess-\nment.\nCharles University provides annotators for\nlanguage pairs involving the Czech language,\ni.e., English→Czech and Czech→Ukrainian. Their\nannotators are linguists, translators, researchers and\nstudents who are native speakers of the target lan-\nguage with high proficiency in the source language.\nDA scores for English ↔{Hebrew, Russian,\nUkrainian} are collected by Toloka AI using their\npaid crowd of bilingual target-language native\nspeakers. Toloka AI tests proficiency of their anno-\ntator crowd across different NLP annotation tasks\nand allowed only annotators who deemed reliable\naccording to their quality control measures.\n5.2 Document selection and quality control\nThe document selection process remains the same\nas in the previous year with minor changes. We first\nrandomly sample a subset of document snippets\nfrom each of the domains for annotations, sam-\npling the domains with approximately the same\nnumber of segments per domain. This ensures that\n10\n(a) Top part of the screen with segment-level scoring.\n (b) Bottom part of the screen with document-level scoring.\nFigure 1: Screenshot of the document-level DA+SQM configuration in the Appraise interface for an example assessment\nfrom the human evaluation campaign for out of English language pairs. The annotator is presented with the entire translated\ndocument snippet randomly selected from competing systems (anonymized) with additional static contexts, and is asked to rate\nthe translation of individual segments and then the entire document on sliding scales between 0 and 100.\nall systems in the given language pairs are evalu-\nated on the same subset of the test set, allowing fair\ncomparison between them. As in previous years,\nwe aim to collect approximately 1,500 assessments\nper system per language pair. Due to concerns\nabout having sufficient annotations, we create two\nbatches of HITs, each providing half of the required\nassessments, such that at least all segments in the\nfirst batch could be covered for all systems, with\nthe second campaign completed if possible.\nFor HIT generation for English ↔German,\nwhich feature paragraph-level test sets (documents\nconsist of paragraphs instead of sentences), we sim-\nply consider a whole paragraph as a “segment”, col-\nlecting paragraph-level assessments. In that regard,\nwe collect fewer DA scores per system comparing\nto other language pairs, but the human evaluation\ncovers a larger subset of the testsets.\nLast year, we used snippets of at most 10 ran-\ndomly selected consecutive segments from a doc-\nument as “documents” for document-level annota-\ntion. This year, we use 10 first segments from a\ndocument instead, in order to align with the MQM-\nbased evaluation used at the Metrics shared task\n(Freitag et al., 2023).\nAll HITs consist of exactly 100 segments and\nare generated as in the past:\n1. Snippet-system pairs are randomly sampled\n(from the restricted set of pre-sampled snip-\npets) to create up to 80 segments;\n2. Random snippets for the remaining 20 (or\nmore) segments are duplicated from the first\n80 to serve as quality control items;\n3. BAD references are introduced to the random\nsegments in the duplicated snippets to have\nabout 12-14% of quality control segments per\nHIT.\nBAD translations are created by replacing an em-\nbedded sequences of tokens in the segment with a\nrandom phrase of the same length from a different\nreference segment.22\nWe perform quality control by measuring an an-\nnotator’s ability to reliably score BAD translations\n22For full details, see the HIT and batch gener-\nation code: https://github.com/wmt-conference/\nwmt23-news-systems\n11\nLanguage pairs Annotators’ profile Tool\nEnglish↔Chinese/German/Japanese Microsoft annotators: bilingual target-language native speakers, pro-\nfessional translators or linguists, experienced in MT evaluation Appraise\nCzech→Ukrainian Paid translators and target-language native speakers Appraise\nEnglish→Czech Czech paid linguists, annotators, researchers, students with high profi-\nciency in English Appraise\nEnglish↔Hebrew/Russian/Ukrainian Toloka AI paid crowd: bilingual target-language native speakers high-\nperforming in other task types Toloka.ai\nTable 3: Annotators’ profiles and annotation tools for each language pair in human evaluation.\nLanguage Pair Sys. Assess. Assess/Sys\nChinese→English 16 20,535 1283.4\nCzech→Ukrainian 14 23,191 1656.5\nGerman→English 14 13,573 969.5\nEnglish→Chinese 16 24,551 1534.4\nEnglish→Czech 16 25,527 1595.4\nEnglish→German 13 14,267 1097.5\nEnglish→Japanese 17 26,115 1536.2\nJapanese→English 18 27,858 1547.7\nTable 4: Amount of segments evaluated in the WMT23 man-\nual evaluation campaign; including human references as sys-\ntems; after excluding quality control items and document-level\nscores.\nLanguage Pair Ann. HITs HITs/Ann.\nChinese→English 13 128 9.8\nCzech→Ukrainian 9 146 16.2\nGerman→English 21 82 3.9\nEnglish→Czech 36 162 4.5\nEnglish→German 22 87 4.0\nEnglish→Japanese 21 164 7.8\nEnglish→Chinese 13 154 11.8\nJapanese→English 20 174 8.7\nTable 5: Numbers of individual annotators taking part in the\nWMT23 human evaluation campaign and the average number\nof HITs collected per annotator.\nsignificantly lower than corresponding original sys-\ntem outputs using a paired significance test with\np <0.05. We pair two HITs into a single annota-\ntion task with about 24-28 quality control segments\nto ensure a sufficient sample size for the statisti-\ncal test. In campaigns hosted on Appraise, if an\nannotator is not able to demonstrate reliability on\nBAD references, they are excluded from further\nannotations, the HITs are reset and annotated from\nscratch by another annotator if possible.\nThe total number of assessments collected for\neach language pair and the average number of as-\nsessments per system in WMT23 manual evalua-\ntion are presented in Table 4.\n5.3 Calibration HITs\nLast year we introduced calibration HITs, which\nthis year we collect for all language pairs. A cali-\nbration HIT is a HIT with 100 randomly selected\nsegments, which is identical for and completed by\nall annotators, in addition to their regular annota-\ntion HITs. We release these alongside the other\nannotations and the anonymized mapping between\nannotators and HITs in order to enable additional\nanalysis. With a small set of sentences annotated by\nall annotators, we are better able to examine ques-\ntions about inter-annotator consistency and provide\ndata for future research in this area.\nTable 5 shows the number of unique annotators\nper language pair along with the total number of\nHITs and average number of HITs per annotator.\nWe leave more detailed analysis of collected cali-\nbration data to future work.\n5.4 Human ranking computation\nThe official rankings shown in Table 6 are gen-\nerated on the basis of the segment-level raw\nDA+SQM scores that are collected within docu-\nment context for all language pairs.23 Whole doc-\numents with at least one quality control segment\n(i.e., BAD references) and HITs that failed to pass\nquality control are removed prior to computing the\nrankings.24\nIn this year’s evaluation, we have chosen not\nto normalize scores by discontinuing the use of\nz-scores, given their potential to exacerbate sys-\ntem comparisons (Knowles, 2021). While utilizing\nraw scores is not flawless—considering each an-\nnotator employs distinct annotation strategies —\nwe have sought to counteract this by distributing\n23The code used to generate the rankings in Table 6 can\nbe found here: https://github.com/AppraiseDev/\nAppraise/blob/main/Campaign/management/\ncommands/ComputeWMT23Results.py\n24Two HITs for Czech →Ukrainian and one HIT for\nEnglish→Czech.\n12\nGerman→English\nRank Ave. System\n1-3 90.3 GPT4-5shot\n1-3 89.9 Human-refA\n1-5 89.6 ONLINE-A\n3-6 89.1 ONLINE-B\n3-6 88.8 ONLINE-W\n4-7 88.0 ONLINE-Y\n6-8 87.7 ONLINE-G\n8-9 86.5 GTCOM_DLUT\n7-9 85.3 ONLINE-M\n10-11 81.8 LanguageX\n10-13 80.0 Lan-BridgeMT\n11-14 79.6 NLLB_MBR_BLEU\n12-14 78.8 AIRC\n11-14 77.9 NLLB_Greedy\nEnglish→German\nRank Ave. System\n1-5 89.0 GPT4-5shot\n1-5 88.8 ONLINE-B\n1-4 88.3 ONLINE-W\n2-6 88.1 ONLINE-A\n4-6 88.0 ONLINE-Y\n1-6 87.7 Human-refA\n7-8 86.7 ONLINE-M\n7-8 85.5 ONLINE-G\n9 84.0 Lan-BridgeMT\n10 82.7 LanguageX\n11-12 76.8 NLLB_MBR_BLEU\n11-12 75.7 NLLB_Greedy\n13 73.6 AIRC\nEnglish→Czech\nRank Ave. System\n1 85.4 Human-refA\n2 84.1 ONLINE-W\n3-5 81.8 GPT4-5shot\n3-4 80.4 CUNI-GA\n5-8 80.3 ONLINE-A\n5-8 79.4 CUNI-DocTransformer\n4-7 78.8 ONLINE-B\n8-14 78.6 NLLB_MBR_BLEU\n6-11 78.4 GTCOM_DLUT\n8-12 77.4 CUNI-Transformer\n10-14 76.8 NLLB_Greedy\n9-14 75.7 ONLINE-M\n10-15 75.2 ONLINE-G\n13-15 75.0 ONLINE-Y\n8-15 75.0 Lan-BridgeMT\n16 74.1 LanguageX\nCzech→Ukrainian\nRank Ave. System\n1-3 83.7 ONLINE-B\n1-3 83.6 GPT4-5shot\n1-3 83.2 Human-refA\n4-8 82.8 ONLINE-W\n4-8 82.4 CUNI-GA\n4-8 81.8 CUNI-Transformer\n4-8 81.3 GTCOM_DLUT\n4-8 80.6 ONLINE-A\n9-11 79.5 ONLINE-G\n9-13 78.7 ONLINE-Y\n9-13 78.7 MUNI-NLP\n10-13 77.4 Lan-BridgeMT\n10-13 76.9 NLLB_MBR_BLEU\n14 76.7 NLLB_Greedy\nChinese→English\nRank Ave. System\n1-2 82.9 Lan-BridgeMT\n1-2 80.9 GPT4-5shot\n3-8 80.3 Yishu\n3-7 80.2 ONLINE-W\n5-10 80.0 ONLINE-G\n3-7 79.8 ONLINE-B\n4-9 79.7 ONLINE-Y\n3-8 79.1 HW-TSC\n6-10 77.8 ONLINE-A\n10-11 77.7 IOL_Research\n8-11 77.2 LanguageX\n12-13 76.9 ONLINE-M\n13-16 76.2 NLLB_MBR_BLEU\n12-15 76.1 Human-refA\n14-16 74.0 NLLB_Greedy\n13-16 72.6 ANVITA\nEnglish→Chinese\nRank Ave. System\n1-5 82.2 Yishu\n1-5 82.1 Human-refA\n1-7 82.1 GPT4-5shot\n3-8 82.0 Lan-BridgeMT\n1-6 81.8 ONLINE-B\n1-8 81.5 HW-TSC\n4-8 81.4 ONLINE-W\n5-8 80.2 ONLINE-Y\n9-10 79.8 IOL_Research\n9-10 79.7 ONLINE-A\n11-13 78.6 LanguageX\n11-13 78.2 ONLINE-M\n11-13 77.1 ONLINE-G\n14 64.5 ANVITA\n15 64.3 NLLB_Greedy\n16 57.2 NLLB_MBR_BLEU\nJapanese→English\nRank Ave. System\n1 81.3 GPT4-5shot\n2-4 80.6 SKIM\n3-8 80.4 Human-refA\n3-8 79.5 ONLINE-Y\n2-8 79.4 ONLINE-B\n3-9 79.2 ONLINE-A\n2-8 78.8 ONLINE-W\n3-8 78.4 NAIST-NICT\n8-9 76.9 GTCOM_DLUT\n10-13 76.4 Lan-BridgeMT\n10-13 75.8 ANVITA\n10-13 74.8 ONLINE-G\n10-13 74.6 LanguageX\n14-15 72.9 ONLINE-M\n14-15 72.4 KYB\n16 68.9 AIRC\n17-18 66.7 NLLB_MBR_BLEU\n17-18 66.1 NLLB_Greedy\nEnglish→Japanese\nRank Ave. System\n1-2 80.7 Human-refA\n2-6 79.5 GPT4-5shot\n1-5 78.8 ONLINE-B\n2-6 78.6 ONLINE-Y\n2-5 78.5 SKIM\n4-6 78.4 ONLINE-W\n7-10 76.6 LanguageX\n7-10 76.2 ONLINE-A\n7-10 76.1 NAIST-NICT\n7-10 75.2 Lan-BridgeMT\n11-12 73.1 ANVITA\n11-12 72.6 ONLINE-M\n13-15 70.8 KYB\n13-15 69.6 AIRC\n13-15 69.6 ONLINE-G\n16 64.5 NLLB_Greedy\n17 61.3 NLLB_MBR_BLEU\nTable 6: Official results of WMT23 General Translation Task. Systems ordered by DA score; systems within a cluster are\nconsidered tied; lines indicate clusters according to Wilcoxon rank-sum test p <0.05; rank ranges indicate the number of\nsystems a system significantly underperforms or outperforms; grayed entry indicates resources that fall outside the constraints\nprovided. All language pairs used document-level evaluation.\n13\nsystems evenly across annotators. This approach\naims to minimize the potential bias of a particularly\nstringent annotator disproportionately penalizing a\nsingle system. Ideally, every annotator would as-\nsess documents translated by all systems; however,\nthis could introduce task repetitiveness concerns.\nFor future considerations, employing calibration\nHITs (see Section 5.3) to normalize each annota-\ntor’s behaviour could offer a promising solution.\nAll segment-level scores are averaged per system\nto compute the system-level scores. The clusters\nare computed using the Wilcoxon rank-sum test\nwith p <0.05. Rank ranges indicate the number\nof systems a particular system underperforms or\noutperforms: the top end of the rank range is l + 1\nwhere l is the number of losses, while the bottom\nis n − w where n is the total number of systems\nand w is the number of systems that the system in\nquestions significantly wins against.\nTables with head-to-head comparisons between\nall systems are included in Appendix G.\nAt the time of preparation of the camera-ready\nversion of the paper, we have not been able to col-\nlect the required number of high-quality assess-\nments for language pairs run through Toloka AI\nthat would meet WMT standards for human eval-\nuation. In that regard, we decided not to publish\nofficial rankings based on manual evaluation for\nEnglish↔{Hebrew, Russian, Ukrainian} until the\nconference, we are planning to address it later.\n5.5 Comparison of human evaluation methods\nIn collaboration with the metrics shared task (Fre-\nitag et al., 2023), human annotation data for the\nChinese→English and English→German direction\nwas collected using two different approaches: the\nsource-based DA+SQM approach, and the Multi-\ndimensional Quality Metrics (MQM) framework\n(Freitag et al., 2021). We present the rankings pro-\nduced by the two approaches in Table 7.\nUpon examining the system rankings and in-\ndividual clusters produced by both techniques, it\nis evident that DA+SQM produces fewer clusters.\nThis suggests that it might not be sufficiently robust\nto differentiate smaller system differences, whereas\nMQM creates more detailed clusters. One potential\nexplanation is that DA+SQM, constrained by bud-\ngetary restrictions, might be under-powered. As\nhighlighted by Wei et al. (2022), the 1500 segments\nwe gather per system might not suffice to segregate\nsystems in a more detailed manner.\nConversely, the largest difference in the evalua-\ntion techniques is the cost. While MQM manages\nto establish more refined clusters, its deployment is\nsignificantly more costly and complex, especially\nwhen training professionals. An interesting ques-\ntion would be determining the number of MQM\nlabels that could be procured within the budget\nallocated for DA+SQM.\nIt is also important to note that the set of data\nover which each of these rankings was produced\nmay have differed slightly due to the sampling (e.g.,\nthe distribution over topic domains or the amount\nof coverage of the full test set), making it difficult\nto determine whether these differences in rankings\nrepresent differences due to data or due to different\nannotation methods.\n6 Test Suites\nAs can be seen in the general MT task, the improve-\nment of translation quality has made it difficult to\ndiscriminate MT output from human translation\nwith the current evaluation methods. Nevertheless,\nthere are still cases where MT has difficulties, de-\nlivering outputs which despite seeming fluent and\nbeing surrounded by other seemingly perfect trans-\nlations, entail serious flaws. In general evaluation\nmethods, such flaws can get “hidden in the aver-\nage” or simply get missed altogether. In an effort to\nshed light to these cases, evaluation via test suites\nis embedded in the shared task.\n6.1 Setup of the sub-task\nTest suites are custom extensions to standard test\nsets, constructed so that they can focus on particular\naspects of the MT output. Here, the evaluation\nof the MT outputs takes place in a decentralized\nmanner as a part of a sub-task, where test suite\nproviders were invited to submit their customized\ntest sets, following the setting introduced at the\nThird Conference on Machine Translation (Bojar\net al., 2018).\nEvery test suite provider submitted a source-side\ntest set, which the shared task organizers appended\nto the standard test sets of the shared task. The\ncorresponding outputs from the MT systems of the\nshared task were returned to the test suite providers,\nwho were responsible for running the evaluation,\nbased on their own custom evaluation methods.\nThe results of each test suite evaluation, together\nwith the relevant analysis, appear in separate de-\nscription papers.\n14\nRank Ave. ↑ System (En-De)\n1-5 89.0 GPT4-5shot\n1-5 88.8 ONLINE-B\n1-4 88.3 ONLINE-W\n2-6 88.1 ONLINE-A\n4-6 88.0 ONLINE-Y\n1-6 87.7 Human-refA\n7-8 86.7 ONLINE-M\n7-8 85.5 ONLINE-G\n9 84.0 Lan-BridgeMT\n10 82.7 LanguageX\n11-12 76.8 NLLB_MBR_BLEU\n11-12 75.7 NLLB_Greedy\n13 73.6 AIRC\nSystem (En-De) MQM ↓\nrefA 2.96\nGPT4-5shot 3.72\nONLINE-W 3.95\nONLINE-B 4.71\nONLINE-Y 5.64\nONLINE-A 5.67\nONLINE-G 6.57\nONLINE-M 6.94\nLan-BridgeMT 8.67\nLanguageX 9.25\nNLLB_Greedy 9.54\nNLLB_MBR_BLEU 10.79\nAIRC 14.23\nRank Ave. ↑ System (Zh-En)\n1-2 82.9 Lan-BridgeMT\n1-2 80.9 GPT4-5shot\n3-8 80.3 Yishu\n3-7 80.2 ONLINE-W\n5-10 80.0 ONLINE-G\n3-7 79.8 ONLINE-B\n4-9 79.7 ONLINE-Y\n3-8 79.1 HW-TSC\n6-10 77.8 ONLINE-A\n10-11 77.7 IOL_Research\n8-11 77.2 LanguageX\n12-13 76.9 ONLINE-M\n13-16 76.2 NLLB_MBR_BLEU\n12-15 76.1 Human-refA\n14-16 74.0 NLLB_Greedy\n13-16 72.6 ANVITA\nSystem (Zh-En) MQM ↓\nLan-BridgeMT 2.10\nGPT4-5shot 2.31\nYishu 3.23\nONLINE-B 3.39\nHW-TSC 3.40\nONLINE-A 3.79\nONLINE-Y 3.79\nONLINE-G 3.86\nONLINE-W 4.06\nLanguageX 4.23\nIOL_Research 4.59\nrefA 4.83\nONLINE-M 5.43\nANVITA 6.08\nNLLB_MBR_BLEU 6.36\nNLLB_Greedy 6.57\nTable 7: Comparison of system clustering as done by DA+SQM and MQM technique. Top two tables are for English to German,\nwhile bottom two are for Chinese to German.\n6.2 Submissions\nThe test suite sub-task received 5 submissions with\n6 test suites, whose overview can be seen in Table 8.\nThe descriptions of each submission and their main\nfindings are given below.\nDFKI (Manakhimova et al., 2023) test suite of-\nfers a fine-grained linguistically motivated anal-\nysis of the shared task MT outputs, based on\nmore than 11,500 manually devised test items,\nwhich cover up to 110 phenomena in 14 cate-\ngories per language direction. Extending their\nprevious test suite efforts (e.g. Avramidis et al.,\n2018; Macketanz et al., 2022), the submission of\nthis year includes an updated test set featuring\nnew linguistic phenomena and focuses addition-\nally on the participating LLMs. The evaluation\nspans German→English, English→German, and\nEnglish→Russian language directions.\nSome of the phenomena with the lowest accu-\nracies for German→English are idioms and resul-\ntative predicates. For English→German, these in-\nclude mediopassive voice, and noun formation(er).\nAs for English→Russian, these include idioms and\nsemantic roles. GPT4 performs equally or compa-\nrably to the best systems in German→English and\nEnglish→German but falls in the second signifi-\ncance cluster for English→Russian.\nHW-TSC (Chen et al., 2023) propose a system-\natic approach to select test sentences with high-\nlevel of difficulty from the Wiki Corpus. The strat-\negy considers the difficulty level of a sentence from\nfour dimensions: word difficulty, length difficulty,\ngrammar difficulty and model learning difficulty.\nThey open-source two Multifaceted Challenge Sets\nfor Chinese→English and English→Chinese, each\nof them containing 2,000 sentences. Then, they use\nthese challenge sets to test the shared task systems,\npresenting results by three automatic metrics.\nThe resulting system ranks are quite different\nfrom the official results. The authors point out that\nsystems that perform well on average test sets may\nnot perform as well on sets with high difficulty.\nIf the ranking difference is caused by domain is-\nsues, the top-ranked systems on the official test sets\nmay not be so general. GPT4 is ranked in the first\ntwo positions in Chinese→English but its rank in\n15\nTest suite Directions Phenomena #Sentences Citation Link\nDFKI de–en, en–de,\nen–ru\n110 linguistic phenomena 11,517 Manakhimova et al. (2023) DFKI-NLP\nHW-TSC zh–en, en–zh 4 difficulty dimensions 4,000 Chen et al. (2023) HwTsc\nIIIT HYD en–de 5 domains, 5 writing styles 2,268 Mukherjee and Shrivastava (2023) wmt23\nINES en–de Inclusive language forms 162 Savoldi et al. (2023) fbk.eu\nMuST-SHE en–de Binary gender bias 200 Savoldi et al. (2023) fbk.eu\nRoCS-MT en–de, en–cs,\nen–uk, en–ru\nNon-standard user-\ngenerated content\n1,922 Bawden and Sagot (2023) RoCS-MT\nTable 8: Overview of the participating test suites.\nEnglish→Chinese is much lower (ranks 4-9).\nIIIT HYD (Mukherjee and Shrivastava, 2023)\nThis test suite covers five specific domains (en-\ntertainment, environment, health, science, legal)\nand spans five distinct writing styles (descriptive,\njudgments, narrative, reporting, technical-writing)\nfor English–German. The authors conduct their\nanalysis through a combination of au- tomated as-\nsessments and manual evaluations.\nBased on their evaluation, it is evident that both\nONLINE-B and ONLINE-Y consistently surpassed\nother MT systems in performance across a diverse\narray of writing styles and domains. When fo-\ncusing on GPT4, whereas it performs comparably\nto the best systems for most domains and writing\nstyles, it gives considerably worse results when ap-\nplied to the legal domain, and the writing style of\njudgments.\nMuST-SHEWMT23 and INES (Savoldi et al.,\n2023) By focusing on the en-de and de-en lan-\nguage pairs, the authors rely on these newly created\ntest suites to investigate systems’ ability to trans-\nlate feminine and masculine gender and produce\ngender-inclusive translations. Furthermore, they\ndiscuss metrics associated with the test suites and\nvalidate them by means of human evaluations.\nThe results indicate that systems achieve rea-\nsonable and comparable performance in correctly\ntranslating both feminine and masculine gender\nforms for naturalistic gender phenomena. Instead,\nthe generation of inclusive language forms in trans-\nlation emerges as a challenging task for all the\nevaluated MT models, indicating room for future\nimprovements and research on the topic.\nConcerning GPT 4, it is noticeable that its overall\naccuracy is 2% worse than the best MT system,\nwhereas it achieves a relatively low accuracy with\nregard to the feminine gender, when evaluating\nwhether the first-person singular references to the\nspeaker are translated according to the speaker’s\nlinguistic expression of gender.\nRoCS-MT (Bawden and Sagot, 2023) The\nRoCS-MT Challenge Set is designed to test MT\nsystems’ robustness to user-generated content\n(UGC) displaying non-standard characteristics,\nsuch as spelling errors, devowelling, acronymi-\nsation, etc. It is composed of non-standard En-\nglish comments from Reddit, manually normalised\nand professionally translated into four of the WMT\n2023 target languages, German, Czech, Ukrainian\nand Russian, and also French.\nThrough automatic and manual analysis of sys-\ntem outputs, we find that many of the phenomena\nremain challenging for most systems, but to varying\ndegrees depending on the phenomenon, the particu-\nlar instance (notably how frequent the non-standard\nword is) and the system, especially with respect to\nthe quantity of training data. For example, non-\nstandard instances of words (e.g. through devow-\nelling or through phonetically inspired spelling) are\noften either omitted in the translation or copied un-\nchanged. When non-standard words are translated,\nit is often in their standard form, but with some ex-\nceptions, for example capitalisation is sometimes\npreserved. However, there is often inconsistency\nwithin a same system’s outputs.\nGPT4-5shot has a clear lead over all other sys-\ntems, correctly translating even some of the most\nchallenging examples. It sometimes (although in-\nconsistently) reproduces non-standardness in its\noutputs, but also does not always remain entirely\nfaithful to the source sentence. However, aside\nthe huge disparity in the amount of training data\ncompared to other systems, notably the constrained\nones, the lack of access to its training data is a\nserious obstacle to any meaningful scientific com-\nparison; we cannot know which phenomena were\nseen during training and how frequently, and more\n16\ncrucially, we cannot verify whether RoCS-MT sen-\ntences were seen during training.\n7 Conclusions\nThe General Machine Translation Task at WMT\n2023 covered 14 translation pairs, where the only\nnon-English language pair was Czech→Ukrainian.\nSource based DA+SQM was the main human\ngolden truth. The evaluation included 72 pri-\nmary submissions from 17 participants, 6 online\nsystems and 3 additional contrastive systems in-\ncluding GPT4. It was performed by 155 human\n(semi-)professional annotators, who contributed\nmore than 175,000 judgments altogether. For most\nlanguage pairs (apart from English→Czech), MT\nsystems produce outputs that cannot be identified\nas being worse than the manually produced refer-\nences translations in a statistically significant way,\nusing our current evaluation methods.\nIt is apparent that this year, the amount of un-\nconstrained submissions are lower thank in past\nyears (27 submissions by 11 participants). Addi-\ntionally, for some language pairs there are only few\nsubmissions by participants, and therefore they are\ndominated by many online systems, of whom we\nhave no technical descriptions. We are therefore\nconsidering ways to encourage participation in the\nfuture, whereas redefining the constrained setting\nmay be needed.\nIt is the first time that Large Language Models\n(LLMs) are included in the Shared Task as trans-\nlation systems. Although the technology is very\napparent in NLP research, we received only one\nsubmission using LLM methods (Lan-BridgeMT),\nwhereas one dominant commercial LLM (GPT4)\nwas included via our own efforts. GPT4 was in the\nfirst significance cluster for all systems translating\ntowards English, but fell in the second significance\ncluster (rank 3-5) for English→Czech, whereas a\nsimilar sign was given by one of the test suites\nfor English→Russian (rank 3; Manakhimova et al.,\n2023). Additionally, test suites providers noted\nthat GPT4 outputs are not always faithful to the\nsource sentence (Bawden and Sagot, 2023) and\nthat they have some issues with speaker gender\ntranslation (Savoldi et al., 2023) and specific do-\nmains (Mukherjee and Shrivastava, 2023, e.g. le-\ngal;). Due to the closed-source nature of commer-\ncial tools, it is hard to know the exact reasons for\nthese findings, although they confirm previous ob-\nservations that GPT models have difficulties with\nunder-represented languages (Hendy et al., 2023).\nWe believe that a more transparent comparison in-\ncluding open source LLMs should be sought for\nthe future.\n8 Limitations\nWe investigated a research question of testing gen-\neral capabilities of MT systems. However, we have\nsimplified this approach. Firstly, we only used four\ndomains that are not specialized. Secondly, we\nused only cleaner sentences, avoiding noisy in the\nsource sentences.\nAlthough we accept human judgement as a gold\nstandard, giving us more reliable signal than au-\ntomatic metrics, we should mention that human\nannotations are noisy (Wei and Jia, 2021) and their\nperformance is affected by quality of other evalu-\nated systems (Mathur et al., 2020).\nDifferent annotators are using different ranking\nstrategy which may have an effect on the system\nranking as we are using raw scores.\n9 Ethical Consideration\nSeveral of the domains contained texts that in-\ncluded personal data, for example the speech data\n(See Section 2.4 for more details). Entities were\nreplaced by anonymisation tags (e.g. #NAME#,\n#EMAIL#) to preserve the anonymity of the users\nbehind the content.\nThe sentences in Ukrainian datasets were col-\nlected with users’ opt-in consent, and any personal\ndata related to people other than well-known people\nwas pseudonymized (using random first names and\nsurnames). Sentences where such pseudonymiza-\ntion would not be enough to preserve reasonable\nanonymity of the users (e.g. describing events\nuniquely identifying the persons involved) were\nnot included in the test set.\nAs described in Section 2.2 and in the linguis-\ntic brief (Appendix Section B), inappropriate, con-\ntroversial and/or explicit content was filtered out\nprior to translation, particularly keeping in mind the\ntranslators and not exposing them to such content\nor obliging them to translate it. A few sentences\ncontaining explicit content managed to escape the\nfilter, and we removed these sentences from the test\nsets without translation.\nHuman evaluation using Appraise for collecting\nhuman judgements was fully anonymous. Auto-\nmatically generated accounts associated with an-\nnotation tasks with single-sign-on URLs were dis-\n17\ntributed randomly among pools of annotators and\ndid not allow for storing personal information. For\nlanguage pairs for which we used calibration HITs,\nwe received lists of tasks completed by an individ-\nual anonymous annotator. Annotators have been\nwell paid in respect to their countries.\nAcknowledgments\nThis task would not have been possible without the\nsponsorship of monolingual data, test sets trans-\nlation and evaluation from our partners. Namely\nMicrosoft, Charles University, Toloka AI, Google,\nNTT Resonant, Dubformer, and Centific.\nAdditionally, we would like to thank Rebecca\nKnowles, Sergio Bruccoleri, Mariia Anisimova and\nmany others who provided help and recommenda-\ntions.\nBarry Haddow’s participation was funded by UK\nResearch and Innovation (UKRI) under the UK\ngovernment’s Horizon Europe funding guarantee\n[grant number 10039436 – UTTER].\nRachel Bawden’s participation was funded by\nher chair position in the PRAIRIE institute funded\nby the French national agency ANR as part of the\n“Investissements d’avenir” programme under the\nreference ANR-19-P3IA-0001 and by her Emer-\ngence project, DadaNMT, funded by Sorbonne Uni-\nversité.\nMaja Popovi´c’s participation was funded by the\nADAPT SFI Centre for Digital Media Technology,\nfunded by Science Foundation Ireland through the\nSFI Research Centres Programme and co-funded\nunder the European Regional Development Fund\n(ERDF) through Grant 13/RC/2106.\nMartin Popel’s participation was funded by\nGA ˇCR EXPRO grant LUSyD (GX20-16819X).\nEleftherios Avramidis’s participation was funded\nby the German Research Foundation (DFG)\nthrough the project TextQ (grant num. MO\n1038/31-1, 436813723), and by the German\nFederal Ministry of Education and Research\n(BMBF) through the project SocialWear (grant\nnum. 01IW2000).\nThis work has been using data and tools provided\nby the LINDAT/CLARIAH-CZ Research Infras-\ntructure (https://lindat.cz), supported by the Min-\nistry of Education, Youth and Sports of the Czech\nRepublic (Project No. LM2023062).\nReferences\nFarhad Akhbardeh, Arkady Arkhangorodsky, Mag-\ndalena Biesialska, Ond ˇrej Bojar, Rajen Chatter-\njee, Vishrav Chaudhary, Marta R. Costa-jussa,\nCristina España-Bonet, Angela Fan, Christian Fe-\ndermann, Markus Freitag, Yvette Graham, Ro-\nman Grundkiewicz, Barry Haddow, Leonie Harter,\nKenneth Heafield, Christopher Homan, Matthias\nHuck, Kwabena Amponsah-Kaakyire, Jungo Kasai,\nDaniel Khashabi, Kevin Knight, Tom Kocmi, Philipp\nKoehn, Nicholas Lourie, Christof Monz, Makoto\nMorishita, Masaaki Nagata, Ajay Nagesh, Toshiaki\nNakazawa, Matteo Negri, Santanu Pal, Allahsera Au-\nguste Tapo, Marco Turchi, Valentin Vydrin, and Mar-\ncos Zampieri. 2021. Findings of the 2021 conference\non machine translation (WMT21). In Proceedings of\nthe Sixth Conference on Machine Translation, pages\n1–88, Online. Association for Computational Linguis-\ntics.\nEleftherios Avramidis, Vivien Macketanz, Arle Lommel,\nand Hans Uszkoreit. 2018. Fine-grained evaluation\nof quality estimation for machine translation based on\na linguistically motivated test suite. In Proceedings\nof the AMTA 2018 Workshop on Translation Quality\nEstimation and Automatic Post-Editing, pages 243–\n248, Boston, MA. Association for Machine Transla-\ntion in the Americas.\nMarta Bañón, Pinzhen Chen, Barry Haddow, Kenneth\nHeafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L.\nForcada, Amir Kamran, Faheem Kirefu, Philipp\nKoehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere,\nGema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec,\nBrian Thompson, William Waites, Dion Wiggins, and\nJaume Zaragoza. 2020. ParaCrawl: Web-scale acqui-\nsition of parallel corpora. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4555–4567, Online. Association\nfor Computational Linguistics.\nLoïc Barrault, Magdalena Biesialska, Ond ˇrej Bo-\njar, Marta R. Costa-jussà, Christian Federmann,\nYvette Graham, Roman Grundkiewicz, Barry Had-\ndow, Matthias Huck, Eric Joanis, Tom Kocmi,\nPhilipp Koehn, Chi-kiu Lo, Nikola Ljubeši´c, Christof\nMonz, Makoto Morishita, Masaaki Nagata, Toshi-\naki Nakazawa, Santanu Pal, Matt Post, and Marcos\nZampieri. 2020. Findings of the 2020 conference on\nmachine translation (WMT20). In Proceedings of\nthe Fifth Conference on Machine Translation, pages\n1–55, Online. Association for Computational Linguis-\ntics.\nLoïc Barrault, Ond ˇrej Bojar, Marta R. Costa-jussà,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias Müller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\nFindings of the 2019 conference on machine trans-\nlation (WMT19). In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1), pages 1–61, Florence, Italy. As-\nsociation for Computational Linguistics.\n18\nRachel Bawden and Benoît Sagot. 2023. RoCS-MT:\nRobustness Challenge Set for Machine Translation.\nIn Proceedings of the Eighth Conference on Machine\nTranslation (WMT), Singapore, Singapore (Hybrid).\nAssociation for Computational Linguistics.\nOndˇrej Bojar, Christian Buck, Chris Callison-Burch,\nChristian Federmann, Barry Haddow, Philipp Koehn,\nChristof Monz, Matt Post, Radu Soricut, and Lucia\nSpecia. 2013. Findings of the 2013 Workshop on\nStatistical Machine Translation. In Proceedings of\nthe Eighth Workshop on Statistical Machine Trans-\nlation, pages 1–44, Sofia, Bulgaria. Association for\nComputational Linguistics.\nOndˇrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-\nAmand, Radu Soricut, Lucia Specia, and Aleš Tam-\nchyna. 2014. Findings of the 2014 workshop on\nstatistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation,\npages 12–58, Baltimore, Maryland, USA. Associa-\ntion for Computational Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara\nLogacheva, Christof Monz, Matteo Negri, Matt Post,\nRaphael Rubino, Lucia Specia, and Marco Turchi.\n2017. Findings of the 2017 conference on machine\ntranslation (WMT17). In Proceedings of the Second\nConference on Machine Translation, pages 169–214,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, Matteo Negri, Aurélie\nNévéol, Mariana Neves, Martin Popel, Matt Post,\nRaphael Rubino, Carolina Scarton, Lucia Specia,\nMarco Turchi, Karin Verspoor, and Marcos Zampieri.\n2016. Findings of the 2016 conference on machine\ntranslation. In Proceedings of the First Conference\non Machine Translation: Volume 2, Shared Task Pa-\npers, pages 131–198, Berlin, Germany. Association\nfor Computational Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nBarry Haddow, Matthias Huck, Chris Hokamp,\nPhilipp Koehn, Varvara Logacheva, Christof Monz,\nMatteo Negri, Matt Post, Carolina Scarton, Lucia\nSpecia, and Marco Turchi. 2015. Findings of the\n2015 workshop on statistical machine translation. In\nProceedings of the Tenth Workshop on Statistical\nMachine Translation, pages 1–46, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nOndˇrej Bojar, Christian Federmann, Mark Fishel, Yvette\nGraham, Barry Haddow, Matthias Huck, Philipp\nKoehn, and Christof Monz. 2018. Findings of the\n2018 conference on machine translation (WMT18).\nIn Proceedings of the Third Conference on Machine\nTranslation: Shared Task Papers , pages 272–303,\nBelgium, Brussels. Association for Computational\nLinguistics.\nChris Callison-Burch, Cameron Fordyce, Philipp Koehn,\nChristof Monz, and Josh Schroeder. 2007. (meta-)\nevaluation of machine translation. In Proceedings of\nthe Second Workshop on Statistical Machine Transla-\ntion, pages 136–158, Prague, Czech Republic. Asso-\nciation for Computational Linguistics.\nChris Callison-Burch, Cameron Fordyce, Philipp Koehn,\nChristof Monz, and Josh Schroeder. 2008. Further\nmeta-evaluation of machine translation. In Proceed-\nings of the Third Workshop on Statistical Machine\nTranslation, pages 70–106, Columbus, Ohio. Associ-\nation for Computational Linguistics.\nChris Callison-Burch, Philipp Koehn, Christof Monz,\nKay Peterson, Mark Przybocki, and Omar Zaidan.\n2010. Findings of the 2010 joint workshop on sta-\ntistical machine translation and metrics for machine\ntranslation. In Proceedings of the Joint Fifth Work-\nshop on Statistical Machine Translation and Metrics-\nMATR, pages 17–53, Uppsala, Sweden. Association\nfor Computational Linguistics.\nChris Callison-Burch, Philipp Koehn, Christof Monz,\nMatt Post, Radu Soricut, and Lucia Specia. 2012.\nFindings of the 2012 workshop on statistical machine\ntranslation. In Proceedings of the Seventh Workshop\non Statistical Machine Translation , pages 10–51,\nMontréal, Canada. Association for Computational\nLinguistics.\nChris Callison-Burch, Philipp Koehn, Christof Monz,\nand Josh Schroeder. 2009. Findings of the 2009\nWorkshop on Statistical Machine Translation. In\nProceedings of the Fourth Workshop on Statistical\nMachine Translation, pages 1–28, Athens, Greece.\nAssociation for Computational Linguistics.\nChris Callison-Burch, Philipp Koehn, Christof Monz,\nand Omar Zaidan. 2011. Findings of the 2011 work-\nshop on statistical machine translation. In Proceed-\nings of the Sixth Workshop on Statistical Machine\nTranslation, pages 22–64, Edinburgh, Scotland. As-\nsociation for Computational Linguistics.\nMauro Cettolo, Christian Girardi, and Marcello Fed-\nerico. 2012. WIT3: Web inventory of transcribed and\ntranslated talks. In Proceedings of the 16th Annual\nConference of the European Association for Machine\nTranslation, pages 261–268, Trento, Italy. European\nAssociation for Machine Translation.\nXiaoyu Chen, Daimeng Wei, Zhanglin Wu, Ting Zhu,\nHengchao Shang, Zongyao Li, Jiaxin GUO, Ning\nXie, Lizhi Lei, Hao Yang, and Yanfei Jiang. 2023.\nMultifaceted Challenge Set for Evaluating Machine\nTranslation Performance. In Proceedings of the\nEighth Conference on Machine Translation (WMT),\nSingapore, Singapore (Hybrid). Association for Com-\nputational Linguistics.\n19\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJan Christian Blaise Cruz. 2023. Samsung R&D In-\nstitute Philippines at WMT 2023. In Proceedings\nof the Eighth Conference on Machine Translation\n(WMT), Singapore, Singapore (Hybrid). Association\nfor Computational Linguistics.\nHiroyuki Deguchi, Kenji Imamura, Yuto Nishida,\nYusuke Sakai, Justin Vasselli, and Taro Watanabe.\n2023. NAIST-NICT WMT’23 general mt task sub-\nmission. In Proceedings of the Eighth Conference on\nMachine Translation (WMT), Singapore, Singapore\n(Hybrid). Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nChristian Federmann. 2018. Appraise evaluation frame-\nwork for machine translation. In Proceedings of the\n27th International Conference on Computational Lin-\nguistics: System Demonstrations, pages 86–88, Santa\nFe, New Mexico. Association for Computational Lin-\nguistics.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n878–891, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nPatrick Fernandes, António Farinhas, Ricardo Rei,\nJosé G. C. de Souza, Perez Ogayo, Graham Neubig,\nand Andre Martins. 2022. Quality-aware decoding\nfor neural machine translation. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1396–1412,\nSeattle, United States. Association for Computational\nLinguistics.\nMarkus Freitag, Isaac Caswell, and Scott Roy. 2019.\nAPE at scale and its implications on MT evaluation\nbiases. In Proceedings of the Fourth Conference on\nMachine Translation (Volume 1: Research Papers),\npages 34–44, Florence, Italy. Association for Com-\nputational Linguistics.\nMarkus Freitag, George Foster, David Grangier, Viresh\nRatnakar, Qijun Tan, and Wolfgang Macherey. 2021.\nExperts, errors, and context: A large-scale study of\nhuman evaluation for machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1460–1474.\nMarkus Freitag, Nitika Mathur, Chi kiu Lo, Elefthe-\nrios Avramidis, Ricardo Rei, Brian Thompson, Tom\nKocmi, Frédéric Blain, Daniel Deutsch, Craig Stew-\nart, Chrysoula Zerva, Sheila Castilho, Alon Lavie,\nand George Foster. 2023. Results of WMT23 Metrics\nShared Task: Metrics might be Guilty but References\nare not Innocent. In Proceedings of the Seventh Con-\nference on Machine Translation (WMT), Singapore,\nSingapore (Hybrid). Association for Computational\nLinguistics.\nTirthankar Ghosal, Marie Hledíková, Muskaan Singh,\nAnna Nedoluzhko, and Ondˇrej Bojar. 2022. The sec-\nond automatic minuting (AutoMin) challenge: Gener-\nating and evaluating minutes from multi-party meet-\nings. In Proceedings of the 15th International Confer-\nence on Natural Language Generation: Generation\nChallenges, pages 1–11, Waterville, Maine, USA\nand virtual meeting. Association for Computational\nLinguistics.\nDirk Goldhahn, Thomas Eckart, and Uwe Quasthoff.\n2012. Building large monolingual dictionaries at the\nLeipzig corpora collection: From 100 to 200 lan-\nguages. In Proceedings of the Eighth International\nConference on Language Resources and Evaluation\n(LREC’12), pages 759–765, Istanbul, Turkey. Euro-\npean Language Resources Association (ELRA).\nThamme Gowda, Zhao Zhang, Chris Mattmann, and\nJonathan May. 2021. Many-to-English machine\ntranslation tools, data, and pretrained models. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing: System Demonstrations, pages 306–316,\nOnline. Association for Computational Linguistics.\nYvette Graham, Timothy Baldwin, Alistair Moffat, and\nJustin Zobel. 2013. Continuous measurement scales\nin human evaluation of machine translation. In Pro-\nceedings of the 7th Linguistic Annotation Workshop\nand Interoperability with Discourse , pages 33–41,\nSofia, Bulgaria. Association for Computational Lin-\nguistics.\nYvette Graham, Barry Haddow, and Philipp Koehn.\n2020. Statistical power and translationese in machine\ntranslation evaluation. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 72–81, Online.\nAssociation for Computational Linguistics.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. How good are GPT models at ma-\nchine translation? a comprehensive evaluation. arXiv\npreprint arXiv:2302.09210.\n20\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremental\nparsing. To appear.\nJosef Jon, Martin Popel, and Ondˇrej Bojar. 2023. CUNI-\nGA submission at WMT23. In Proceedings of the\nEighth Conference on Machine Translation (WMT),\nSingapore, Singapore (Hybrid). Association for Com-\nputational Linguistics.\nPhillip Keung, Yichao Lu, György Szarvas, and Noah A.\nSmith. 2020. The multilingual Amazon reviews cor-\npus. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4563–4568, Online. Association for\nComputational Linguistics.\nOndrej Klejch, Eleftherios Avramidis, Aljoscha Bur-\nchardt, and Martin Popel. 2015. MT-ComparEval:\nGraphical evaluation interface for machine transla-\ntion development. Prague Bull. Math. Linguistics ,\n104:63–74.\nRebecca Knowles. 2021. On the stability of system\nrankings at WMT. In Proceedings of the Sixth Con-\nference on Machine Translation, pages 464–477, On-\nline. Association for Computational Linguistics.\nTom Kocmi, Rachel Bawden, Ond ˇrej Bojar, Anton\nDvorkovich, Christian Federmann, Mark Fishel,\nThamme Gowda, Yvette Graham, Roman Grund-\nkiewicz, Barry Haddow, Rebecca Knowles, Philipp\nKoehn, Christof Monz, Makoto Morishita, Masaaki\nNagata, Toshiaki Nakazawa, Michal Novák, Martin\nPopel, and Maja Popovi´c. 2022. Findings of the 2022\nconference on machine translation (WMT22). In\nProceedings of the Seventh Conference on Machine\nTranslation (WMT), pages 1–45, Abu Dhabi, United\nArab Emirates (Hybrid). Association for Computa-\ntional Linguistics.\nTom Kocmi, Christian Federmann, Roman Grund-\nkiewicz, Marcin Junczys-Dowmunt, Hitokazu Mat-\nsushita, and Arul Menezes. 2021. To ship or not to\nship: An extensive evaluation of automatic metrics\nfor machine translation. In Proceedings of the Sixth\nConference on Machine Translation, pages 478–494,\nOnline. Association for Computational Linguistics.\nTom Kocmi, Martin Popel, and Ondrej Bojar. 2020.\nAnnouncing CzEng 2.0 Parallel Corpus with over 2\nGigawords. CoRR, abs/2007.03006.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of\nMachine Translation Summit X: Papers, pages 79–86,\nPhuket, Thailand.\nPhilipp Koehn and Christof Monz, editors. 2006. Pro-\nceedings on the Workshop on Statistical Machine\nTranslation. Association for Computational Linguis-\ntics, New York City.\nKeito Kudo, Takumi Ito, Makoto Morishita, and Jun\nSuzuki. 2023. SKIM at WMT 2023 general transla-\ntion task. In Proceedings of the Eighth Conference on\nMachine Translation (WMT), Singapore, Singapore\n(Hybrid). Association for Computational Linguistics.\nBen LI, Yoko Matsuzaki, and Shivam Kalkar. 2023.\nKYB general machine translation systems for\nWMT23. In Proceedings of the Eighth Conference on\nMachine Translation (WMT), Singapore, Singapore\n(Hybrid). Association for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach.\nSamuel Läubli, Sheila Castilho, Graham Neubig, Rico\nSennrich, Qinlan Shen, and Antonio Toral. 2020.\nA Set of Recommendations for Assessing Hu-\nman–Machine Parity in Language Translation. Jour-\nnal of Artificial Intelligence Research (JAIR), 67.\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian\nHe, Liangke Gui, Graham Neubig, Jonathan May,\nand Zettlemoyer Luke. 2022. Mega: Moving av-\nerage equipped gated attention. arXiv preprint\narXiv:2209.10655.\nVivien Macketanz, Eleftherios Avramidis, Aljoscha\nBurchardt, He Wang, Renlong Ai, Shushen Man-\nakhimova, Ursula Strohriegel, Sebastian Möller, and\nHans Uszkoreit. 2022. A linguistically motivated test\nsuite to semi-automatically evaluate German–English\nmachine translation output. In Proceedings of the\nThirteenth Language Resources and Evaluation Con-\nference, pages 936–947, Marseille, France. European\nLanguage Resources Association.\nShushen Manakhimova, Eleftherios Avramidis, Vivien\nMacketanz, Ekaterina Lapshinova-Koltunski, Sergei\nBagdasarov, and Sebastian Möller. 2023. Linguisti-\ncally motivated Evaluation of the 2023 State-of-the-\nart Machine Translation: Can ChatGPT Outperform\nNMT? In Proceedings of the Eighth Conference on\nMachine Translation (WMT), Singapore, Singapore\n(Hybrid). Association for Computational Linguistics.\nNitika Mathur, Timothy Baldwin, and Trevor Cohn.\n2020. Tangled up in BLEU: Reevaluating the eval-\nuation of automatic machine translation evaluation\nmetrics. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4984–4997, Online. Association for Computa-\ntional Linguistics.\n21\nLuo Min, yixin tan, and Qiulin Chen. 2023. Yishu:\nYishu at WMT2023 translation task. In Proceedings\nof the Eighth Conference on Machine Translation\n(WMT), Singapore, Singapore (Hybrid). Association\nfor Computational Linguistics.\nAlexander Molchanov and Vladislav Kovalenko. 2023.\nPROMT systems for WMT23 shared general transla-\ntion task. In Proceedings of the Eighth Conference on\nMachine Translation (WMT), Singapore, Singapore\n(Hybrid). Association for Computational Linguistics.\nMakoto Morishita, Jun Suzuki, and Masaaki Nagata.\n2020. JParaCrawl: A large scale web-based English-\nJapanese parallel corpus. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 3603–3609, Marseille, France. European\nLanguage Resources Association.\nAnanya Mukherjee and Manish Shrivastava. 2023. IIIT\nHYD’s Submission for WMT23 Test-suite task. In\nProceedings of the Eighth Conference on Machine\nTranslation (WMT), Singapore, Singapore (Hybrid).\nAssociation for Computational Linguistics.\nMathias Müller, Malihe Alikhani, Eleftherios\nAvramidis, Richard Bowden, Annelies Braffort,\nNecati Cihan Camgöz, Sarah Ebling, Cristina\nEspaña-bonet, Anne Göhring, Roman Grund-\nkiewicz, Mert Inan, Zifan Jiang, Oscar Koller,\nAmit Moryossef, Annette Rios, Dimitar Shterionov,\nSandra Sidler-miserez, Katja Tissi, and Davy\nVan Landuyt. 2023. Findings of the second WMT\nshared task on sign language translation (WMT-\nSLT23). In Proceedings of the Eight Conference on\nMachine Translation (WMT), Singapore. Association\nfor Computational Linguistics.\nAnna Nedoluzhko, Muskaan Singh, Marie Hledíková,\nTirthankar Ghosal, and Ondˇrej Bojar. 2022. ELITR\nminuting corpus: A novel dataset for automatic\nminuting from multi-party meetings in English and\nCzech. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages 3174–\n3182, Marseille, France. European Language Re-\nsources Association.\nGraham Neubig. 2011. The Kyoto free translation task.\nhttp://www.phontron.com/kftt.\nMariana Neves, Antonio Jimeno Yepes, Aurélie Névéol,\nRachel Bawden, Giorgio Maria Di Nunzio, Roland\nRoller, Philippe Thomas, Federica Vezzani, Maika Vi-\ncente Navarro, Lana Yeganova, Dina Wiemann, and\nCristian Grozea. 2023. Findings of the WMT 2023\nBiomedical Translation Shared Task: Evaluation of\nChatGPT 3.5 as a Comparison System. In Proceed-\nings of the Seventh Conference on Machine Transla-\ntion (WMT), Singapore, Singapore (Hybrid). Associ-\nation for Computational Linguistics.\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur\nÇelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Safiyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022. No Language Left Behind: Scal-\ning Human-Centered Machine Translation. arXiv\npreprint arXiv:2207.04672.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nStephan Peitz, Sarthak Garg, Udhay Nallasamy,\nand Matthias Paulik. 2019. Cross+Self-Attention\nfor Transformer Models. https://github.com/\npytorch/fairseq/files/3561282/paper.pdf.\nMartin Popel. 2020. CUNI English-Czech and English-\nPolish systems in WMT20: Robust document-level\ntraining. In Proceedings of the Fifth Conference on\nMachine Translation, pages 269–273, Online. Asso-\nciation for Computational Linguistics.\nMaja Popovi´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nReid Pryzant, Youngjoo Chung, Dan Jurafsky, and\nDenny Britz. 2018. JESC: Japanese-English subtitle\ncorpus. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nRicardo Rei, José G. C. de Souza, Duarte Alves,\nChrysoula Zerva, Ana C Farinha, Taisiya Glushkova,\nAlon Lavie, Luisa Coheur, and André F. T. Martins.\n2022. COMET-22: Unbabel-IST 2022 submission\nfor the metrics shared task. In Proceedings of the\nSeventh Conference on Machine Translation (WMT),\npages 578–585, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685–2702, Online. Association\nfor Computational Linguistics.\n22\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nMat¯ıss Rikters. 2018. Impact of Corpora Quality on\nNeural Machine Translation. In In Proceedings of\nthe 8th Conference Human Language Technologies\n- The Baltic Perspective (Baltic HLT 2018) , Tartu,\nEstonia. IOS Press.\nMat¯ıss Rikters and Makoto Miwa. 2023. AIST AIRC\nsubmissions to the WMT23 shared task. In Proceed-\nings of the Eighth Conference on Machine Transla-\ntion (WMT), Singapore, Singapore (Hybrid). Associ-\nation for Computational Linguistics.\nRoberts Rozis and Raivis Skadin, š. 2017. Tilde MODEL\n- multilingual open data for EU languages. In Pro-\nceedings of the 21st Nordic Conference on Computa-\ntional Linguistics, pages 263–265, Gothenburg, Swe-\nden. Association for Computational Linguistics.\nPavel Rychlý and Yuliia Teslia. 2023. MUNI-NLP\nsubmission for Czech-Ukrainian translation task at\nWMT23. In Proceedings of the Eighth Conference on\nMachine Translation (WMT), Singapore, Singapore\n(Hybrid). Association for Computational Linguistics.\nBeatrice Savoldi, Marco Gaido, Matteo Negri, and Luisa\nBentivogli. 2023. Test Suites Task: Evaluation of\nGender Fairness in MT with MuST-SHE and INES.\nIn Proceedings of the Eighth Conference on Machine\nTranslation (WMT), Singapore, Singapore (Hybrid).\nAssociation for Computational Linguistics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzmán. 2021. Wiki-\nMatrix: Mining 135M parallel sentences in 1620 lan-\nguage pairs from Wikipedia. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 1351–1361, Online. Association for Computa-\ntional Linguistics.\nRoman Sudarikov, Martin Popel, Ond ˇrej Bojar,\nAljoscha Burchardt, and Ondˇrej Klejch. 2016. Using\nMT-ComparEval. In Translation Evaluation: From\nFragmented Tools and Data Sets to an Integrated\nEcosystem, pages 76–82.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation (LREC’12), pages 2214–2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nAntonio Toral, Sheila Castilho, Ke Hu, and Andy Way.\n2018. Attaining the unattainable? reassessing claims\nof human parity in neural machine translation. InPro-\nceedings of the Third Conference on Machine Trans-\nlation: Research Papers, pages 113–123, Brussels,\nBelgium. Association for Computational Linguistics.\nChristopher Lemmer Webber, Jessica Tallon, Erin Shep-\nherd, Amy Guy, and Evan Prodromou. 2018. Ac-\ntivityPub, W3C Recommendation. Technical report,\nW3C.\nJohnny Wei and Robin Jia. 2021. The statistical advan-\ntage of automatic NLG metrics at the system level.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n6840–6854, Online. Association for Computational\nLinguistics.\nJohnny Wei, Tom Kocmi, and Christian Federmann.\n2022. Searching for a higher power in the human\nevaluation of MT. In Proceedings of the Seventh\nConference on Machine Translation (WMT), pages\n129–139, Abu Dhabi, United Arab Emirates (Hybrid).\nAssociation for Computational Linguistics.\nDi Wu and Christof Monz. 2023. Beyond shared vocab-\nulary: Increasing representational word similarities\nacross languages for multilingual machine translation.\narXiv preprint arXiv:2305.14189.\nDi Wu, Shaomu Tan, David Stap, Ali Araabi, and\nChristof Monz. 2023a. UvA-MT’s participation in\nthe WMT 2023 general translation shared task. In\nProceedings of the Eighth Conference on Machine\nTranslation (WMT), Singapore, Singapore (Hybrid).\nAssociation for Computational Linguistics.\nYangjian Wu and Gang Hu. 2023. Exploring prompt en-\ngineering with GPT language models for document-\nlevel machine translation: Insights and findings. In\nProceedings of the Eighth Conference on Machine\nTranslation (WMT), Singapore, Singapore (Hybrid).\nAssociation for Computational Linguistics.\nZhanglin Wu, Daimeng Wei, Zongyao Li, Zhengzhe\nYU, Shaojun Li, Xiaoyu Chen, Hengchao Shang,\nJiaxin GUO, Yuhao Xie, Lizhi Lei, Hao Yang, and\nYanfei Jiang. 2023b. Treating general mt shared\ntask as a multi-domain adaptation problem: Hw-tsc’s\nsubmission to the WMT23 general mt shared task.\nIn Proceedings of the Eighth Conference on Machine\nTranslation (WMT), Singapore, Singapore (Hybrid).\nAssociation for Computational Linguistics.\nYiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang,\nJiajun Chen, and Mingxuan Wang. 2023. BLEURT\nhas universal translations: An analysis of automatic\nmetrics by minimum risk training. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 5428–5443, Toronto, Canada. Association for\nComputational Linguistics.\n23\nHui Zeng. 2023. Achieving state-of-the-art multilingual\ntranslation model with minimal data and parameters.\nIn Proceedings of the Eighth Conference on Machine\nTranslation (WMT), Singapore, Singapore (Hybrid).\nAssociation for Computational Linguistics.\nWenbo Zhang. 2023. IOL Research machine transla-\ntion systems for WMT23 general machine translation\nshared task. In Proceedings of the Eighth Conference\non Machine Translation (WMT), Singapore, Singa-\npore (Hybrid). Association for Computational Lin-\nguistics.\nMichał Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The United Nations parallel cor-\npus v1.0. In Proceedings of the Tenth International\nConference on Language Resources and Evaluation\n(LREC’16), pages 3530–3534, Portorož, Slovenia.\nEuropean Language Resources Association (ELRA).\nHao Zong. 2023. GTCOM neural machine translation\nsystems for WMT23. In Proceedings of the Eighth\nConference on Machine Translation (WMT), Singa-\npore, Singapore (Hybrid). Association for Computa-\ntional Linguistics.\n24\nA Statistics of training data\nThis section describes statistics of the training corpora.\nDataset ID Segs Tokens Chars\neng-ces eng ces eng ces\nFacebook-wikimatrix-1-ces-eng 2.09M 33.56M 29.66M 206.82M 216.62M\nParaCrawl-paracrawl-9-eng-ces 50.63M 692.12M 626.34M 4.33B 4.68B\nStatmt-commoncrawl_wmt13-1-ces-eng 161.84k 3.35M 2.93M 20.66M 20.75M\nStatmt-europarl-10-ces-eng 644.43k 15.63M 13.00M 94.31M 98.14M\nStatmt-news_commentary-16-ces-eng 253.27k 5.46M 4.96M 34.58M 37.97M\nStatmt-wikititles-3-ces-eng 410.94k 1.03M 965.62k 7.47M 7.57M\nTilde-ecb-2017-ces-eng 3.10k 52.12k 45.21k 327.57k 339.24k\nTilde-eesc-2017-ces-eng 1.33M 28.78M 25.63M 188.53M 205.14M\nTilde-ema-2016-ces-eng 495.23k 7.64M 7.28M 50.31M 57.01M\nTilde-rapid-2019-ces-eng 263.29k 5.79M 5.30M 37.36M 41.26M\n(Total) 56.29M 793.41M 716.10M 4.97B 5.36B\neng-deu eng deu eng deu\nFacebook-wikimatrix-1-deu-eng 6.23M 100.50M 96.95M 623.66M 701.23M\nParaCrawl-paracrawl-9-eng-deu 278.31M 4.27B 3.99B 26.37B 29.46B\nStatmt-commoncrawl_wmt13-1-deu-eng 2.40M 51.40M 47.05M 314.18M 340.51M\nStatmt-europarl-10-deu-eng 1.82M 45.51M 42.41M 272.94M 312.14M\nStatmt-news_commentary-16-deu-eng 388.48k 8.55M 8.77M 54.40M 65.94M\nStatmt-wikititles-3-deu-eng 1.47M 3.61M 3.08M 26.48M 25.50M\nTilde-airbaltic-1-deu-eng 0.84k 17.60k 15.08k 104.34k 105.52k\nTilde-czechtourism-1-deu-eng 6.76k 128.29k 114.44k 769.04k 829.41k\nTilde-ecb-2017-deu-eng 4.15k 85.52k 74.81k 545.51k 582.63k\nTilde-eesc-2017-deu-eng 2.86M 61.47M 58.28M 400.37M 469.94M\nTilde-ema-2016-deu-eng 347.63k 5.09M 5.01M 33.48M 39.43M\nTilde-rapid-2016-deu-eng 1.03M 20.65M 19.85M 134.26M 158.13M\nTilde-rapid-2019-deu-eng 939.81k 19.90M 19.30M 129.03M 153.08M\n(Total) 295.81M 4.59B 4.29B 28.36B 31.73B\neng-heb eng heb eng heb\nELRC-wikipedia_health-1-eng-heb 3.16k 69.71k 54.76k 442.38k 583.87k\nFacebook-wikimatrix-1-eng-heb 2.04M 35.83M 28.96M 218.77M 300.61M\nNeulab-tedtalks_train-1-eng-heb 211.82k 4.45M 3.44M 22.36M 29.00M\nOPUS-bible_uedin-v1-eng-heb 62.20k 1.55M 830.23k 8.16M 7.46M\nOPUS-ccmatrix-v1-eng-heb 25.23M 313.87M 249.49M 1.81B 2.45B\nOPUS-elrc_2922-v1-eng-heb 3.16k 69.73k 54.77k 442.40k 583.54k\nOPUS-elrc_3065_wikipedia_health-v1-eng-heb 3.16k 69.71k 54.76k 442.31k 583.51k\nOPUS-elrc_wikipedia_health-v1-eng-heb 3.16k 69.71k 54.76k 442.31k 583.51k\nOPUS-globalvoices-v2018q4-eng-heb 1.03k 20.31k 15.03k 122.39k 158.63k\nOPUS-gnome-v1-eng-heb 0.15k 0.42k 0.40k 2.89k 3.96k\nOPUS-kde4-v2-eng-heb 79.32k 338.22k 347.35k 2.09M 3.13M\nOPUS-multiccaligned-v1-eng-heb 5.33M 60.55M 52.81M 380.74M 518.33M\nOPUS-opensubtitles-v2018-eng-heb 29.89M 195.98M 154.25M 1.03B 1.40B\nOPUS-php-v1-eng-heb 27.82k 83.46k 93.03k 498.72k 789.34k\nOPUS-qed-v2.0a-eng-heb 464.35k 6.37M 4.48M 34.70M 42.34M\nOPUS-tatoeba-v20220303-eng-heb 164.20k 1.02M 806.38k 5.41M 7.37M\nOPUS-tatoeba-v2-eng-heb 54.36k 357.09k 277.32k 1.87M 2.56M\nOPUS-ubuntu-v14.10-eng-heb 1.44k 6.13k 5.78k 38.78k 54.69k\nOPUS-wikimedia-v20210402-eng-heb 226.83k 8.51M 7.56M 57.58M 78.26M\nOPUS-wikipedia-v1.0-eng-heb 139.85k 2.69M 2.27M 16.45M 22.43M\nOPUS-xlent-v1.1-eng-heb 3.19M 9.61M 7.93M 60.53M 73.11M\nStatmt-ccaligned-1-eng-heb_IL 5.33M 60.55M 52.81M 380.76M 518.34M\n(Total) 72.46M 702.05M 566.59M 4.04B 5.45B\nTable 9: Statistics for parallel training set provided for General/News Translation Task. Suffixes, k, M, and B, are short for\nthousands, millions, and billions, respectively. Dataset ID is the unique identifier created by MTData, example mtdata echo\n<dataset_id>.\n25\nDataset ID Segs Tokens Chars\neng-jpn eng eng jpn\nFacebook-wikimatrix-1-eng-jpn 3.90M 61.63M 379.09M 454.97M\nKECL-paracrawl-3-eng-jpn 25.74M 599.02M 3.69B 4.58B\nPhontron-kftt_train-1-eng-jpn 440.29k 9.74M 59.91M 49.08M\nStanfordNLP-jesc_train-1-eng-jpn 2.80M 19.34M 104.00M 119.62M\nStatmt-news_commentary-16-eng-jpn 1.84k 39.50k 247.70k 310.56k\nStatmt-ted-wmt20-eng-jpn 241.74k 4.03M 23.02M 27.32M\nStatmt-wikititles-3-jpn-eng 757.04k 1.94M 13.96M 18.67M\n(Total) 33.88M 695.74M 4.27B 5.25B\neng-rus eng rus eng rus\nFacebook-wikimatrix-1-eng-rus 5.20M 86.79M 76.48M 537.73M 965.44M\nOPUS-unpc-v1.0-eng-rus 25.17M 563.82M 520.71M 3.70B 7.31B\nParaCrawl-paracrawl-1_bonus-eng-rus 5.38M 101.31M 80.41M 632.54M 1.06B\nStatmt-backtrans_enru-wmt20-eng-rus 36.77M 736.20M 670.93M 4.31B 7.73B\nStatmt-commoncrawl_wmt13-1-rus-eng 878.39k 18.77M 17.40M 116.16M 214.59M\nStatmt-news_commentary-16-eng-rus 331.51k 7.67M 7.13M 48.79M 97.41M\nStatmt-wikititles-3-rus-eng 1.19M 3.13M 2.88M 22.80M 39.34M\nStatmt-yandex-wmt22-eng-rus 1.00M 21.25M 18.68M 130.99M 250.76M\nTilde-airbaltic-1-eng-rus 1.09k 23.98k 18.79k 142.52k 252.73k\nTilde-czechtourism-1-eng-rus 7.33k 140.09k 110.10k 838.09k 1.50M\nTilde-worldbank-1-eng-rus 25.85k 588.58k 573.93k 3.85M 8.21M\n(Total) 75.96M 1.54B 1.40B 9.50B 17.67B\neng-ukr eng ukr eng ukr\nELRC-acts_ukrainian-1-eng-ukr 129.94k 3.04M 2.60M 19.55M 35.69M\nFacebook-wikimatrix-1-eng-ukr 2.58M 41.55M 35.59M 257.56M 447.33M\nParaCrawl-paracrawl-1_bonus-eng-ukr 13.35M 505.83M 487.47M 3.28B 6.04B\nTilde-worldbank-1-eng-ukr 1.63k 36.07k 34.18k 237.96k 477.91k\n(Total) 16.06M 550.46M 525.68M 3.55B 6.52B\neng-zho eng eng zho\nFacebook-wikimatrix-1-eng-zho 2.60M 49.87M 311.07M 277.84M\nOPUS-unpc-v1.0-eng-zho 17.45M 417.25M 2.75B 2.14B\nParaCrawl-paracrawl-1_bonus-eng-zho 14.17M 217.60M 1.34B 1.18B\nStatmt-backtrans_enzh-wmt20-eng-zho 19.76M 364.22M 2.16B 1.96B\nStatmt-news_commentary-16-eng-zho 313.67k 6.92M 44.14M 38.83M\nStatmt-wikititles-3-zho-eng 921.96k 2.37M 17.82M 16.28M\n(Total) 55.22M 1.06B 6.62B 5.61B\nces-ukr ces ukr ces ukr\nELRC-acts_ukrainian-1-ces-ukr 130.00k 2.48M 2.56M 19.61M 35.26M\nFacebook-wikimatrix-1-ces-ukr 848.96k 10.43M 10.07M 75.97M 127.31M\nOPUS-bible_uedin-v1-ces-ukr 7.95k 140.03k 132.06k 904.31k 1.33M\nOPUS-ccmatrix-v1-ces-ukr 3.99M 45.13M 45.10M 330.68M 566.27M\nOPUS-elrc_5179_acts_ukrainian-v1-ces-ukr 130.00k 2.48M 2.56M 19.61M 35.26M\nOPUS-elrc_wikipedia_health-v1-ces-ukr 0.19k 3.23k 3.18k 24.27k 41.63k\nOPUS-eubookshop-v2-ces-ukr 1.51k 23.71k 19.15k 187.30k 275.14k\nOPUS-gnome-v1-ces-ukr 0.15k 0.42k 0.41k 3.53k 5.82k\nOPUS-kde4-v2-ces-ukr 133.67k 593.82k 677.35k 4.45M 7.97M\nOPUS-multiccaligned-v1.1-ces-ukr 1.61M 19.75M 19.77M 146.44M 244.36M\nOPUS-multiparacrawl-v9b-ces-ukr 2.20M 25.62M 25.55M 188.08M 325.50M\nOPUS-opensubtitles-v2018-ces-ukr 730.80k 3.88M 3.90M 24.20M 40.62M\nOPUS-qed-v2.0a-ces-ukr 161.02k 2.02M 2.04M 13.44M 22.80M\nOPUS-tatoeba-v20220303-ces-ukr 2.93k 10.85k 11.40k 68.70k 118.67k\nOPUS-ted2020-v1-ces-ukr 114.23k 1.57M 1.56M 10.70M 17.93M\nOPUS-ubuntu-v14.10-ces-ukr 0.23k 1.67k 1.76k 13.02k 20.86k\nOPUS-wikimedia-v20210402-ces-ukr 1.96k 39.18k 34.91k 285.74k 414.20k\nOPUS-xlent-v1.1-ces-ukr 695.41k 1.78M 1.58M 12.92M 18.30M\n(Total) 10.76M 115.95M 115.57M 847.58M 1.44B\nTable 10: Statistics for parallel training set provided for General/News Translation Task. Suffixes, k, M, and B, are short for\nthousands, millions, and billions, respectively.\n26\nB Preprocessing cleanup brief for linguists\nHuman check briefing \nIn this task, we wish to check the data to remove all inappropriate content, remove repetitive \ncontent, or correct minor problems with the text. \nThe data is automatically broken down into individual sentences, which may contain wrong \nsentence splitting that needs to be fixed. Each paragraph is separated by empty lines. Keep the \ndocument-separators intact. \nWe ask you to read each document and either: \n• Delete document completely if it contains any of following issues. Be on the save side, \nrather remove documents where you are uncertain \no Remove documents written in different language (natural code-switching is fine) \no Remove inappropriate content (such as sexually explicit, vulgar, or otherwise \ninappropriate) \no Remove controversial content (propagandist, controversial political topics, etc.) \no Remove content that is too noisy or doesn't resemble natural text (such as \ndocuments badly formatted, hard to understand, containing unusual language, \nlists of numbers/data, or other structured data generated automatically) \n• Keep document while checking \no Fix sentence-breaking, each line must be one sentence (do not reformulate, \nsimply remove or add end of lines on a proper place).  \no Remove or move fragments of sentences to previous or following sentence (for \nexample emoticons, one or few words sentences) \no Fix minor issues and keep it (do not spent too much time on fixing it). \n▪ It is fine to keep some errors or problems \n▪ Remove boilerplates (segments that break the document, for example \nads, page numbers, signatures, artefacts, …)  \no If a given document has more than around 30 sentences, consider splitting it by \nadding an empty line on a meaningful place splitting it into paragraphs \nThis task shouldn’t take much longer than reading through documents. \n \n27\nC Translator Brief for General MT\nTranslator Brief  \nIn this project we wish to translate online news articles for use in evaluation of Machine \nTranslation (MT). The translations produced by you will be compared against the translations \nproduced by a variety of different MT systems.  They will be released to the research \ncommunity to provide a benchmark, or “gold-standard” measure for translation quality. The \ntranslation therefore needs to be a high-quality rendering of the source text into the target \nlanguage, as if it was news written directly in the target language. However, there are some \nconstraints imposed by the intended usage:  \n● All translations should be “from scratch”, without post-editing from MT. Using post-\nediting would bias the evaluation, so we need to avoid it. We can detect post-editing \nso will reject translations that are post-edited.   \n● Translation should preserve the sentence boundaries. The source texts are  \nprovided with exactly one sentence per line, and the translations should be the \nsame, one sentence per line. Blank lines should be preserved in the translation.  \n● Translators should avoid inserting parenthetical explanations into the translated text \nand obviously avoid losing any pieces of information from the source text.  We will \ncheck a sample of the translations for quality, and we will check the entire set for \nevidence of post-editing.   \n● Please do not translate the anonymization tags (e.g. #NAME#), but use the same \nform as in the source text. These tags are used to de-identify names and various \nother sensitive data. In other words, translation must contain given tag #NAME# on a \nposition where it would naturally be placed before anonymization. \n● If the original data contain errors, typos, or other problems, do not try to fix them (or \nintroduce them in the translation), instead try to prepare correct translation as if the \nerror wouldn’t be in the source. \n  \nThe source files will be delivered as text files (sometimes known as “notepad” files), with one \nsentence per line. We need the translations to be returned in the same format. If you prefer \nto receive the text in a different format, then please let us know as we may be able to \naccommodate it.   \n28\nD Additional statistics of the test sets\nTable 11 shows the type-token ratios for the source and target side of each of the test sets, shown for\nthe four main domains. As mentioned previously, texts are tokenised using the language-specific Spacy\nmodels (Honnibal and Montani, 2017) where available. For Hebrew, we use the multilingual Spacy model\nas no language-specific model is available. The type-token ratio is calculated as the number of unique\ntokens divided by the total number of tokens. The absolute value depends not only on the lexical diversity\nof the text but also on the morphological complexity of the language in question.\nmanuals mastodon news user_review\nsrc trg src trg src trg src trg\nFrom English\nen–cs – – 0.30 0.42 0.27 0.39 0.22 0.35\nen–de – – 0.30 0.32 0.27 0.29 – –\nen–he – – 0.30 0.30 0.27 0.29 0.22 0.24\nen–ja – – 0.30 0.23 0.27 0.19 0.22 0.17\nen–ru – – 0.30 0.41 0.27 0.38 0.22 0.33\nen–uk – – 0.30 0.41 0.27 0.38 0.22 0.34\nen–zh – – 0.30 0.29 0.27 0.26 0.22 0.21\nOther language directions\ncs–uk – – – – 0.43 0.41 – –\nde–en 0.32 0.23 0.49 0.42 0.34 0.26 – –\nhe–en – – – – 0.34 0.09 – –\nja–en – – – – 0.22 0.23 0.22 0.21\nru–en 0.47 0.28 – – 0.40 0.24 – –\nuk–en – – – – 0.36 0.21 – –\nzh–en 0.25 0.25 – – 0.23 0.19 0.22 0.17\nTable 11: Type-token ratio for individual source languages used in the general translation test sets.\nE News Task System Submission Summaries\nThis section lists all the submissions to the translation task and provides the authors’ descriptions of their\nsubmission.\nE.1 AIRC (Rikters and Miwa, 2023)\nAIRC trained constrained track models for translation between English, German, and Japanese. Before\ntraining the final models we first filtered the parallel and monolingual data (Rikters, 2018), then performed\niterative back-translation as well as parallel data distillation to be used for non-autoregressive model\ntraining. We experimented with training Transformer models, Mega (Ma et al., 2022) models, and\ncustom non-autoregressive sequence-to-sequence models with encoder and decoder weights initialised\nby multilingual BERT base. Our primary submissions contain translations from ensembles of two Mega\nmodel checkpoints and our contrastive submissions are generated by our non-autoregressive models.\nE.2 ANVITA (no associated paper)\nANVITA-ZhJa Machine Translation system for WMT2023 Shared Task:General MT(News). This\npaper describes ANVITA-ZhJa MT system, architected for submission to WMT 2023 General Machine\nTranslation(News) shared task by the ANVITA team, where the team participated in 4 translation directions:\nChinese, Japanese→English and English→Chinese, Japanese. ANVITA-ZhJa MT system comprised of\nfour NMT models.Chinese, Japanese→English and English→Chinese, Japanese multilingual models for\nprimary and Chinese→English and English→Chinese bilingual models for contrastive submissions. Base\nMT models are built using transformer(base) architecture, trained over the organizer provided parallel\ncorpus and subsequently used deep transformer with added layers and other parameters. We also distilled\ncorpus using heuristics based filtering and used model ensemble for enhanced performance.\n29\nE.3 CUNI-DocTransformer (Popel, 2020)\nExactly the same system as submitted in WMT20, document-level Transformer trained with Block\nBacktranslation.\nE.4 CUNI-GA (Jon et al., 2023)\nOur submission is a result of applying a novel n-best list reranking and modification method on translation\ncandidates produced by two other competing systems, CUNI-Transformer and CUNI-DocTransformer.\nOur method uses a genetic algorithm and MBR decoding to search for optimal translation under a given\nmetric (in our case, a weighted combination of ChrF, BLEU, COMET22-DA, and COMET22-QE-DA).\nE.5 CUNI-Transformer (Popel, 2020)\nThe English↔Czech sentence-level models are exactly the same as submitted in WMT20 (Popel, 2020).\nThe Ukrainian↔Czech models are very similar, also trained with Block Backtranslation.\nE.6 GTCOM (Zong, 2023)\nGTCOM uses transformer as the basic architecture and leverages multilingual models to improve transla-\ntion quality. Besides, GTCOM does a lot of data cleaning and data augmentation work.\nE.7 HW-TSC (Wu et al., 2023b)\nHW-TSC’s submission is a standard Transformer model equipped with our recent technique.\nE.8 IOL-Research (Zhang, 2023)\nThis paper describes the IOL Research team’s submission system for the WMT23 General Machine\nTranslation shared task. We participate in two language translation directions, including English-to-\nChinese and Chinese-to-English. Our final primary submissions belong to constrained systems, which\nmeans for both translation directions we only use officially provided monolingual and bilingual data\nto train the translation systems. Our systems are based on Transformer architecture with pre-norm or\ndeep-norm, which has been proven to be helpful for training deeper models. We employ methods such\nas back-translation, data diversification, domain fine-tuning and model ensemble to build our translation\nsystems. Another important aspect is that we carefully conduct data cleaning and use as much monolingual\ndata as possible for data augmentation.\nE.9 TeamKYB (LI et al., 2023)\nWe here describe our neural machine translation system for the general machine translation shared task in\nWMT 2023. Our systems are based on the Transformer with base settings. We trained our model with\npreprocessed train data. We collect multiple checkpoint from our model and performed inference with\nseveral hyperparameter settings. Collected translations were processed via some rule-based corrections.\nWe chose best translation from the results by using N-best ranking method.\nE.10 Lan-BridgeMT (Wu and Hu, 2023)\nWith the emergence of large-scale models, various industries have undergone significant transformations,\nparticularly in the realm of document-level machine translation. This has introduced a novel research\nparadigm that we have embraced in our participation in the WMT23 competition. Focusing on advance-\nments in models such as chatGPT and GPT4, we have undertaken numerous prompt-based experiments.\nOur objective is to achieve optimal human evaluation results for document-level machine translation,\nresulting in our submission of the final outcomes in the general track.\nE.11 MUNI-NLP (Rychlý and Teslia, 2023)\nMUNI-NLP system is a standard transformer.\n30\nE.12 NAIST-NICT (Deguchi et al., 2023)\nIn this paper, we describe our NAIST-NICT submission to the WMT’23 English-Japanese general machine\ntranslation task. Our system generates diverse translation candidates and reranks them with a two-stage\nreranking system to find the best translation. We first generate 50 candidates each from 18 different\ntranslation methods using a variety of techniques to increase the diversity of the translation candidates. We\ntrained 7 different models per language direction using different combinations of hyperparameters. From\nthese models we used various decoding algorithms, ensembling the models, and using kNN-MT. The\n900 translation candidates go through a two-stage reranking system in order to find the most promising\ncandidate.The first step compares the 50 candidates from each translation method using DrNMT and\nreturns the one with the highest score. The final 18 candidates are ranked using COMET-MBR, and the\nhighest scoring is returned as the system output. We found that generating diverse translation candidates\nimproves the translation quality by using the well-designed relanker model.\nE.13 PROMT (Molchanov and Kovalenko, 2023)\nThis paper describes the PROMT submissions for the WMT23 Shared General Translation Task. This\nyear we participated in two directions of the Shared Translation Task: English to Russian and Russian to\nEnglish. Our models are trained with the MarianNMT toolkit using the transformer-big configuration. We\nuse BPE for text encoding, both models are unconstrained. We achieve competitive results according to\nautomatic metrics in both directions.\nE.14 SRPH (Cruz, 2023)\nWe submit single-model encode-decoder Transformer systems for the constrained English to Hebrew\nand Hebrew to English translation directions. Our dataset is cleaned and filtered via a combination of\nheuristic-based, ratio-based, and embedding-based (LaBSE) methods, resulting in a dataset with high\nalignment. We train models with heavy use of back-translation and decode using Noisy Channel Reranking\nusing a reverse model and a language model trained with contest data.\nE.15 SKIM (Kudo et al., 2023)\nThe SKIM team submission took a standard procedure of building ensemble Transformer models, including\nbase-model training, data augmentation using back-translation of base models, and retraining several final\nmodels using back-translated training data. Each final model has its own architecture and configuration,\nincluding a 10.5B parameter at most, substituting self and cross sublayers in decoder with cross+self-\nattention sub-layer (Peitz et al., 2019). We select the best candidate from large candidate pools, namely 70\ntranslations generated from 16 distinct models for each sentence, with an MBR reranking method using\nCOMET and COMET-QE (Fernandes et al., 2022). We also applied data augmentation and selection\ntechniques to training data of the Transformer models.\nE.16 UPCite-CLILLF (no associated paper)\nIn this biomedical shared task, we have created data filters to better \"choose\" relevant training data for\nfine-tuning, among provided training data sources. In particular, we have used the textometric analysis tool\nITRAMEUR to filter the segments and terms that characterize the test set and then extracted them from train-\ning data to fine-tune MBart-50 baseline (decoder_attention_heads: 16, decoder_ffn_dim: 4096,\ndecoder_layers: 12, encoder_attention_heads: 16, encoder_ffn_dim: 4096,\nencoder_layers: 12, num_hidden_layers: 12, max_length: 200, epoch: 3). In doing\nso, we hope to meet several objectives : to build feasible fine-tuning strategy to train biomedical\nin-domain fr<->en models ; to specify filtering criteria of in-domain training data and to compare models’\npredictions, fine-tuning data and test set in order to better understand how neural machine translation\nsystems work. We will also compare the pipeline of the shared task of this year to those of the past 2\nyears to evaluate the benefits of our training strategies of in-domain machine translation models.\n31\nE.17 UvA-LTL (Wu et al., 2023a)\nWe present our WMT system, UvA-MT, in the WMT 2023 shared general translation task. This year, we\ndeveloped a single Multilingual Machine Translation (MMT) system to participate in the two-directional\ntranslation track between English and Hebrew. The main architecture is based on the prior work of\nBeyond Shared V ocabulary (Wu and Monz, 2023). We scaled it up to a transformer-large level (422M\nparameters). Additionally, we employed back translation to generate synthetic data and labeled them with\na new language tag. After convergence, we further fine-tuned the system without using synthetic data.\nSeveral domain shift techniques were also introduced, such as the domain-aware language model, to filter\nmonolingual data.\nE.18 YiShu (Min et al., 2023)\nYishu’s team participated in WMT23 Machine Translation Competition and adopted the most advanced\nneural machine translation method. They use Transformer model structure and use large-scale parallel\ncorpus for training. In order to improve the translation quality, the team adopted cutting-edge data\npreprocessing technology, various attention mechanisms and improved decoding strategies. In addition,\nthey also carried out in-depth parameter adjustment and model optimization. Yishu team incorporated\nevaluation indicators such as BLEU and TER into the training constraints of the model to achieve better\ntranslation performance. They strive for high accuracy and fluency in the competition, and strive to\nachieve excellent results in the field of translation.\nE.19 LanguageX (Zeng, 2023)\nLanguageX’s submission is a many-to-many encoder decoder transformer model.\n32\nF Automatic scores\nThis section contains automatic metric scores. While human judgement is the official ranking of systems\nand their performance, we share automatic scores to show expected system performance for various\ntestsets.\nWe use COMET (Rei et al., 2020) as the primary metric and chrF (Popovi´c, 2015) as the secondary\nmetric, following recommendation by (Kocmi et al., 2021). We also present BLEU (Papineni et al.,\n2002) scores as it is still a widely used metric. The COMET scores are calculated with the default model\nUnbabel/wmt22-comet-da. The chrF and BLEU scores are calculated using SacreBLEU (Post, 2018).\nScores are multiplied by 100. We ranked the systems according to their scores. Unconstrained systems\nare indicated with a grey background in the tables.\nSystem COMET\nCUNI-GA 90.9\nGPT4-5shot 90.8\nONLINE-W 89.4\nGTCOM_Peter 88.9\nONLINE-B 88.8\nONLINE-A 88.2\nCUNI-Transformer 88.0\nONLINE-G 87.7\nMUNI-NLP 87.0\nONLINE-Y 86.5\nNLLB_Greedy 86.3\nNLLB_MBR_BLEU 86.3\nLan-BridgeMT 86.0\nSystem chrF\nGPT4-5shot 61.0\nCUNI-GA 57.9\nGTCOM_Peter 57.6\nCUNI-Transformer 57.4\nMUNI-NLP 57.0\nLan-BridgeMT 55.7\nONLINE-W 55.0\nONLINE-B 54.7\nONLINE-A 54.4\nONLINE-G 53.7\nONLINE-Y 53.4\nNLLB_Greedy 52.5\nNLLB_MBR_BLEU 52.3\nSystem BLEU\nGPT4-5shot 32.8\nCUNI-Transformer 30.2\nGTCOM_Peter 29.8\nCUNI-GA 29.5\nMUNI-NLP 28.3\nLan-BridgeMT 27.5\nONLINE-W 26.8\nONLINE-B 25.7\nONLINE-A 25.4\nNLLB_MBR_BLEU 25.1\nNLLB_Greedy 24.9\nONLINE-G 24.8\nONLINE-Y 24.2\nTable 12: Scores for the cs→uk translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\nSystem COMET\nONLINE-W 91.8\nCUNI-GA 90.8\nONLINE-B 89.9\nGPT4-5shot 89.4\nONLINE-A 88.4\nCUNI-DocTransformer 88.3\nGTCOM_Peter 87.7\nONLINE-M 87.4\nLan-BridgeMT 87.3\nCUNI-Transformer 87.2\nNLLB_Greedy 87.1\nONLINE-Y 87.0\nNLLB_MBR_BLEU 86.9\nONLINE-G 85.9\nZengHuiMT 85.4\nSystem chrF\nONLINE-W 76.3\nONLINE-B 70.4\nZengHuiMT 67.5\nONLINE-A 66.3\nCUNI-GA 65.9\nGTCOM_Peter 65.4\nCUNI-DocTransformer 65.1\nONLINE-Y 64.6\nCUNI-Transformer 63.9\nLan-BridgeMT 63.8\nONLINE-G 63.7\nONLINE-M 63.2\nGPT4-5shot 62.3\nNLLB_Greedy 60.0\nNLLB_MBR_BLEU 59.1\nSystem BLEU\nONLINE-W 59.4\nONLINE-B 50.1\nONLINE-A 43.4\nCUNI-GA 43.3\nZengHuiMT 43.1\nCUNI-DocTransformer 42.5\nGTCOM_Peter 42.3\nCUNI-Transformer 41.4\nONLINE-Y 40.8\nLan-BridgeMT 40.7\nONLINE-G 39.6\nONLINE-M 39.6\nGPT4-5shot 37.8\nNLLB_Greedy 35.9\nNLLB_MBR_BLEU 35.1\nTable 13: Scores for the en→cs translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\nSystem COMET\nGPT4-5shot 86.3\nONLINE-W 86.0\nONLINE-B 85.6\nONLINE-A 85.5\nONLINE-Y 84.9\nONLINE-M 84.8\nONLINE-G 84.6\nGTCOM_Peter 82.7\nNLLB_MBR_BLEU 81.4\nZengHuiMT 81.1\nLan-BridgeMT 80.9\nNLLB_Greedy 79.9\nAIRC 78.7\nSystem chrF\nONLINE-W 72.1\nONLINE-A 70.0\nGPT4-5shot 69.8\nONLINE-B 69.1\nONLINE-G 69.1\nONLINE-Y 68.4\nZengHuiMT 67.6\nLan-BridgeMT 66.7\nGTCOM_Peter 66.6\nONLINE-M 66.5\nNLLB_MBR_BLEU 57.6\nNLLB_Greedy 57.3\nAIRC 57.2\nSystem BLEU\nONLINE-W 51.8\nGPT4-5shot 47.9\nONLINE-A 47.9\nONLINE-B 46.3\nONLINE-G 46.0\nONLINE-Y 43.9\nGTCOM_Peter 42.2\nLan-BridgeMT 42.1\nONLINE-M 41.3\nZengHuiMT 40.8\nNLLB_Greedy 33.1\nAIRC 32.4\nNLLB_MBR_BLEU 32.4\nTable 14: Scores for the de→en translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\n33\nSystem COMET\nONLINE-W 85.5\nGPT4-5shot 85.0\nONLINE-B 84.8\nONLINE-Y 84.1\nONLINE-A 83.7\nONLINE-G 82.5\nONLINE-M 81.7\nLan-BridgeMT 80.4\nZengHuiMT 79.4\nNLLB_MBR_BLEU 78.0\nNLLB_Greedy 77.9\nAIRC 72.9\nSystem chrF\nONLINE-W 71.8\nONLINE-A 69.7\nZengHuiMT 69.4\nGPT4-5shot 69.1\nONLINE-B 69.1\nONLINE-Y 69.1\nONLINE-G 69.0\nONLINE-M 66.9\nLan-BridgeMT 66.1\nNLLB_Greedy 56.2\nNLLB_MBR_BLEU 55.4\nAIRC 52.2\nSystem BLEU\nONLINE-W 47.8\nONLINE-A 43.7\nGPT4-5shot 43.6\nONLINE-Y 43.6\nONLINE-G 43.2\nONLINE-B 42.7\nONLINE-M 40.5\nZengHuiMT 40.5\nLan-BridgeMT 39.4\nNLLB_Greedy 31.1\nNLLB_MBR_BLEU 29.6\nAIRC 26.5\nTable 15: Scores for the en→de translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\nSystem COMET\nONLINE-B 89.9\nONLINE-A 87.0\nGPT4-5shot 86.9\nGTCOM_Peter 86.7\nONLINE-G 85.6\nZengHuiMT 85.6\nONLINE-Y 84.9\nUvA-LTL 84.7\nNLLB_MBR_BLEU 82.9\nNLLB_Greedy 82.8\nSamsung_Research_Philippines 82.6\nLan-BridgeMT 82.4\nSystem chrF\nONLINE-B 87.5\nZengHuiMT 76.3\nGTCOM_Peter 76.2\nONLINE-A 73.3\nGPT4-5shot 71.4\nUvA-LTL 70.9\nONLINE-Y 70.5\nONLINE-G 69.8\nNLLB_Greedy 64.4\nLan-BridgeMT 63.5\nNLLB_MBR_BLEU 63.0\nSamsung_Research_Philippines 55.5\nSystem BLEU\nONLINE-B 76.5\nGTCOM_Peter 59.2\nZengHuiMT 56.6\nONLINE-A 53.9\nGPT4-5shot 51.2\nUvA-LTL 51.0\nONLINE-Y 49.8\nONLINE-G 49.3\nNLLB_Greedy 42.5\nLan-BridgeMT 41.4\nNLLB_MBR_BLEU 40.7\nSamsung_Research_Philippines 34.0\nTable 16: Scores for the he→en (refA) translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6 |nw:0|space:no|version:2.2.1),\nBLEU (nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\nSystem COMET\nGPT4-5shot 86.4\nONLINE-B 85.6\nONLINE-A 85.3\nGTCOM_Peter 84.5\nONLINE-G 84.0\nUvA-LTL 83.3\nZengHuiMT 83.3\nONLINE-Y 82.9\nNLLB_MBR_BLEU 81.8\nNLLB_Greedy 81.7\nLan-BridgeMT 81.3\nSamsung_Research_Philippines 81.3\nSystem chrF\nGPT4-5shot 69.5\nONLINE-B 66.5\nONLINE-A 65.6\nGTCOM_Peter 65.3\nZengHuiMT 65.1\nUvA-LTL 63.3\nONLINE-G 62.8\nONLINE-Y 62.0\nNLLB_Greedy 59.6\nLan-BridgeMT 59.0\nNLLB_MBR_BLEU 58.6\nSamsung_Research_Philippines 51.3\nSystem BLEU\nGPT4-5shot 50.4\nONLINE-B 45.0\nGTCOM_Peter 44.4\nONLINE-A 44.4\nUvA-LTL 41.7\nZengHuiMT 41.7\nONLINE-G 40.9\nONLINE-Y 38.5\nNLLB_Greedy 37.1\nLan-BridgeMT 36.2\nNLLB_MBR_BLEU 36.2\nSamsung_Research_Philippines 29.8\nTable 17: Scores for the he→en (refB) translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6 |nw:0|space:no|version:2.3.1),\nBLEU (nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1), COMET (Unbabel/wmt22-comet-da).\nSystem COMET\nONLINE-B 86.4\nONLINE-A 85.7\nGPT4-5shot 84.9\nGTCOM_Peter 84.7\nONLINE-Y 84.7\nUvA-LTL 84.2\nSamsung_Research_Philippines 83.7\nLan-BridgeMT 83.0\nNLLB_Greedy 82.9\nZengHuiMT 82.7\nNLLB_MBR_BLEU 82.5\nONLINE-G 82.2\nSystem chrF\nONLINE-B 66.4\nZengHuiMT 62.1\nONLINE-A 61.7\nGTCOM_Peter 61.1\nONLINE-Y 60.4\nUvA-LTL 59.0\nONLINE-G 58.1\nSamsung_Research_Philippines 57.3\nLan-BridgeMT 54.9\nNLLB_Greedy 54.8\nNLLB_MBR_BLEU 54.3\nGPT4-5shot 54.0\nSystem BLEU\nONLINE-B 47.8\nONLINE-A 38.9\nGTCOM_Peter 37.2\nONLINE-Y 37.2\nZengHuiMT 36.5\nUvA-LTL 35.0\nSamsung_Research_Philippines 33.3\nONLINE-G 33.2\nNLLB_MBR_BLEU 30.8\nLan-BridgeMT 30.5\nNLLB_Greedy 30.3\nGPT4-5shot 27.0\nTable 18: Scores for the en→he translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\n34\nSystem COMET\nSKIM 84.0\nGPT4-5shot 83.4\nONLINE-W 82.3\nNAIST-NICT 81.9\nONLINE-Y 81.6\nONLINE-B 81.5\nONLINE-A 81.0\nGTCOM_Peter 80.2\nANVITA 79.5\nLan-BridgeMT 79.3\nZengHuiMT 79.2\nONLINE-G 77.8\nONLINE-M 77.5\nKYB 76.6\nNLLB_MBR_BLEU 75.2\nAIRC 74.5\nNLLB_Greedy 74.3\nSystem chrF\nONLINE-W 51.4\nGPT4-5shot 51.2\nSKIM 51.1\nONLINE-A 49.6\nNAIST-NICT 49.5\nONLINE-Y 49.5\nZengHuiMT 49.5\nONLINE-B 49.3\nGTCOM_Peter 48.7\nLan-BridgeMT 47.3\nANVITA 46.7\nONLINE-G 45.5\nKYB 43.9\nONLINE-M 43.9\nAIRC 40.5\nNLLB_MBR_BLEU 39.2\nNLLB_Greedy 39.0\nSystem BLEU\nONLINE-W 25.9\nSKIM 24.8\nGPT4-5shot 24.1\nONLINE-B 23.9\nNAIST-NICT 23.0\nONLINE-A 23.0\nZengHuiMT 22.6\nGTCOM_Peter 22.3\nONLINE-Y 22.3\nANVITA 20.9\nLan-BridgeMT 20.2\nONLINE-G 18.3\nKYB 17.6\nONLINE-M 17.2\nAIRC 14.9\nNLLB_MBR_BLEU 14.7\nNLLB_Greedy 14.2\nTable 19: Scores for the ja →en translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\nSystem COMET\nONLINE-B 88.2\nONLINE-W 87.5\nONLINE-Y 87.3\nGPT4-5shot 87.0\nSKIM 86.6\nNAIST-NICT 86.2\nZengHuiMT 85.3\nONLINE-A 85.2\nLan-BridgeMT 84.5\nONLINE-M 13.3\nANVITA 82.7\nKYB 80.8\nAIRC 80.7\nONLINE-G 80.4\nNLLB_Greedy 79.3\nNLLB_MBR_BLEU 77.7\nSystem chrF\nONLINE-B 35.2\nONLINE-Y 34.1\nONLINE-W 33.5\nSKIM 33.5\nZengHuiMT 32.9\nNAIST-NICT 32.0\nONLINE-A 31.4\nGPT4-5shot 31.0\nLan-BridgeMT 30.4\nONLINE-M 29.6\nANVITA 29.3\nKYB 27.7\nAIRC 27.6\nONLINE-G 27.3\nNLLB_Greedy 20.9\nNLLB_MBR_BLEU 18.7\nSystem BLEU\nONLINE-B 25.3\nONLINE-W 24.5\nONLINE-Y 24.5\nSKIM 24.3\nNAIST-NICT 22.6\nZengHuiMT 22.6\nONLINE-A 21.4\nGPT4-5shot 21.3\nLan-BridgeMT 20.5\nONLINE-M 19.8\nANVITA 19.4\nKYB 17.8\nAIRC 17.6\nONLINE-G 17.2\nNLLB_Greedy 11.3\nNLLB_MBR_BLEU 9.0\nTable 20: Scores for the en →ja translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:ja-mecab-0.996-IPA|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\nSystem COMET\nGPT4-5shot 83.5\nONLINE-Y 82.5\nONLINE-B 82.3\nONLINE-W 82.2\nONLINE-G 82.0\nONLINE-A 81.9\nPROMT 80.9\nONLINE-M 80.7\nNLLB_MBR_BLEU 80.5\nNLLB_Greedy 80.1\nLan-BridgeMT 79.9\nZengHuiMT 79.5\nSystem chrF\nGPT4-5shot 60.4\nONLINE-G 59.6\nONLINE-A 59.4\nONLINE-B 59.4\nZengHuiMT 58.9\nONLINE-Y 58.6\nPROMT 58.4\nONLINE-W 58.3\nLan-BridgeMT 57.4\nONLINE-M 56.7\nNLLB_MBR_BLEU 55.8\nNLLB_Greedy 55.5\nSystem BLEU\nONLINE-B 34.5\nGPT4-5shot 34.4\nONLINE-G 34.0\nONLINE-A 33.8\nONLINE-Y 33.2\nONLINE-W 33.1\nPROMT 32.8\nLan-BridgeMT 31.8\nZengHuiMT 31.3\nNLLB_MBR_BLEU 31.0\nONLINE-M 30.7\nNLLB_Greedy 30.3\nTable 21: Scores for the ru→en translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\nSystem COMET\nONLINE-G 86.6\nONLINE-W 86.6\nONLINE-B 86.2\nGPT4-5shot 86.1\nONLINE-Y 85.5\nONLINE-A 85.3\nONLINE-M 83.2\nLan-BridgeMT 83.1\nNLLB_Greedy 82.9\nNLLB_MBR_BLEU 82.7\nPROMT 82.3\nZengHuiMT 81.3\nSystem chrF\nONLINE-B 61.9\nONLINE-A 59.0\nONLINE-G 58.9\nZengHuiMT 58.8\nONLINE-W 56.6\nONLINE-Y 56.4\nGPT4-5shot 56.2\nLan-BridgeMT 55.7\nPROMT 55.4\nONLINE-M 55.1\nNLLB_Greedy 53.3\nNLLB_MBR_BLEU 53.1\nSystem BLEU\nONLINE-B 40.4\nONLINE-A 34.8\nONLINE-G 32.9\nONLINE-Y 32.0\nZengHuiMT 31.6\nONLINE-W 31.4\nONLINE-M 30.9\nLan-BridgeMT 30.7\nGPT4-5shot 30.6\nPROMT 30.5\nNLLB_MBR_BLEU 28.4\nNLLB_Greedy 28.2\nTable 22: Scores for the en→ru translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\n35\nSystem COMET\nONLINE-W 87.5\nGPT4-5shot 87.1\nONLINE-B 86.8\nGTCOM_Peter 86.3\nONLINE-A 86.3\nONLINE-G 86.2\nONLINE-Y 85.8\nLan-BridgeMT 84.8\nZengHuiMT 84.4\nNLLB_MBR_BLEU 84.3\nNLLB_Greedy 84.2\nSystem chrF\nGTCOM_Peter 69.3\nONLINE-W 69.2\nONLINE-B 69.0\nZengHuiMT 68.5\nONLINE-A 68.3\nONLINE-Y 68.2\nGPT4-5shot 68.1\nONLINE-G 68.0\nLan-BridgeMT 66.2\nNLLB_Greedy 62.4\nNLLB_MBR_BLEU 62.4\nSystem BLEU\nONLINE-W 47.4\nGTCOM_Peter 46.4\nONLINE-B 46.0\nONLINE-A 45.9\nONLINE-Y 45.7\nONLINE-G 44.9\nGPT4-5shot 43.9\nZengHuiMT 43.5\nLan-BridgeMT 42.3\nNLLB_MBR_BLEU 38.1\nNLLB_Greedy 37.8\nTable 23: Scores for the uk→en translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\nSystem COMET\nONLINE-W 86.7\nONLINE-B 85.6\nGPT4-5shot 85.3\nONLINE-G 85.3\nONLINE-A 83.2\nONLINE-Y 82.9\nGTCOM_Peter 82.1\nNLLB_Greedy 82.1\nNLLB_MBR_BLEU 81.7\nLan-BridgeMT 80.4\nZengHuiMT 79.0\nSystem chrF\nONLINE-B 61.7\nONLINE-W 59.2\nZengHuiMT 56.4\nONLINE-G 56.1\nONLINE-A 55.8\nONLINE-Y 55.4\nGTCOM_Peter 54.4\nGPT4-5shot 53.0\nLan-BridgeMT 51.9\nNLLB_Greedy 50.8\nNLLB_MBR_BLEU 50.5\nSystem BLEU\nONLINE-B 39.8\nONLINE-W 34.9\nONLINE-A 30.3\nONLINE-Y 29.5\nONLINE-G 28.6\nZengHuiMT 27.8\nGTCOM_Peter 27.5\nGPT4-5shot 25.2\nNLLB_MBR_BLEU 24.9\nLan-BridgeMT 24.6\nNLLB_Greedy 24.5\nTable 24: Scores for the en→uk translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\nSystem COMET\nHW-TSC 82.8\nONLINE-B 82.7\nYishu 82.7\nGPT4-5shot 81.6\nLan-BridgeMT 81.2\nONLINE-G 80.9\nONLINE-Y 80.6\nONLINE-A 80.3\nZengHuiMT 79.6\nONLINE-W 79.3\nIOL_Research 79.2\nONLINE-M 77.7\nNLLB_MBR_BLEU 76.8\nANVITA 76.6\nNLLB_Greedy 76.4\nSystem chrF\nHW-TSC 57.5\nONLINE-B 57.5\nYishu 57.4\nZengHuiMT 54.6\nONLINE-G 53.9\nONLINE-A 53.4\nGPT4-5shot 53.1\nLan-BridgeMT 53.1\nONLINE-W 52.5\nIOL_Research 52.4\nONLINE-Y 52.3\nONLINE-M 49.7\nANVITA 47.1\nNLLB_Greedy 46.1\nNLLB_MBR_BLEU 45.8\nSystem BLEU\nHW-TSC 33.6\nONLINE-B 33.5\nYishu 33.4\nONLINE-A 28.3\nLan-BridgeMT 27.3\nIOL_Research 27.2\nZengHuiMT 27.0\nGPT4-5shot 26.8\nONLINE-G 26.6\nONLINE-W 26.4\nONLINE-Y 25.0\nONLINE-M 23.5\nANVITA 21.8\nNLLB_Greedy 20.5\nNLLB_MBR_BLEU 19.8\nTable 25: Scores for the zh→en translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\nSystem COMET\nONLINE-B 88.1\nYishu 88.1\nHW-TSC 87.3\nGPT4-5shot 87.1\nONLINE-W 86.8\nLan-BridgeMT 86.6\nONLINE-Y 86.5\nONLINE-A 86.2\nIOL_Research 85.3\nZengHuiMT 84.3\nONLINE-M 84.2\nONLINE-G 83.8\nNLLB_Greedy 75.7\nANVITA 75.6\nNLLB_MBR_BLEU 71.5\nSystem chrF\nHW-TSC 53.8\nYishu 53.0\nONLINE-B 52.9\nONLINE-A 52.8\nIOL_Research 51.9\nONLINE-M 50.6\nONLINE-Y 49.8\nONLINE-G 49.4\nONLINE-W 47.3\nZengHuiMT 47.0\nLan-BridgeMT 46.8\nGPT4-5shot 46.5\nANVITA 36.9\nNLLB_Greedy 26.3\nNLLB_MBR_BLEU 21.1\nSystem BLEU\nHW-TSC 58.6\nONLINE-A 58.5\nYishu 57.6\nONLINE-B 57.5\nIOL_Research 56.9\nONLINE-M 54.9\nONLINE-Y 54.2\nONLINE-G 54.1\nZengHuiMT 52.9\nONLINE-W 52.1\nLan-BridgeMT 50.2\nGPT4-5shot 49.6\nANVITA 38.9\nNLLB_Greedy 27.4\nNLLB_MBR_BLEU 19.1\nTable 26: Scores for the en→zh translation task: chrF (nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.1), BLEU\n(nrefs:1|case:mixed|eff:no|tok:zh|smooth:exp|version:2.2.1), COMET (Unbabel/wmt22-comet-da).\n36\nG Head to head comparisons\nFollowing tables show differences in average human scores for each language pair. The numbers in each\nof the tables’ cells indicate the difference in average human scores for the system in that column and the\nsystem in that row.\nBecause there were so many systems and data conditions the significance of each pairwise comparison\nneeds to be quantified. We applied Wilcoxon rank-sum test to measure the likelihood that such differences\ncould occur simply by chance. In the following tables ⋆ indicates statistical significance at p <0.05,\n† indicates statistical significance at p < 0.01, and ‡ indicates statistical significance at p < 0.001,\naccording to Wilcoxon rank-sum test.\nEach table contains final rows showing the average score achieved by that system and the rank range\naccording to Wilcoxon rank-sum test (p <0.05). Gray lines separate clusters based on non-overlapping\nrank ranges.\nCzech→Ukrainian\nONLINE-B\nGPT4-5shot\nHuman-refA\nONLINE-W\nCUNI-GA\nCUNI-Transformer\nGTCOM_DLUT\nONLINE-A\nONLINE-G\nONLINE-Y\nMUNI-NLP\nLan-BridgeMT\nNLLB_MBR_BLEU\nNLLB_Greedy\nONLINE-B — 0.1 0.4 0.9 ⋆ 1.3‡ 1.8† 2.4⋆ 3.1⋆ 4.1‡ 5.0‡ 5.0‡ 6.2‡ 6.7‡ 7.0‡\nGPT4-5shot -0.1 — 0.4 0.8 † 1.2‡ 1.8‡ 2.3† 3.1‡ 4.1‡ 4.9‡ 4.9‡ 6.2‡ 6.7‡ 6.9‡\nHuman-refA -0.4 -0.4 — 0.5 ‡ 0.9‡ 1.4‡ 1.9‡ 2.7‡ 3.7‡ 4.6‡ 4.6‡ 5.8‡ 6.3‡ 6.6‡\nONLINE-W -0.9 -0.8 -0.5 — 0.4 0.9 1.5 2.2 3.2 ‡ 4.1‡ 4.1‡ 5.3‡ 5.8‡ 6.1‡\nCUNI-GA -1.3 -1.2 -0.9 -0.4 — 0.6 1.1 1.8 2.9 † 3.7‡ 3.7† 5.0‡ 5.5‡ 5.7‡\nCUNI-Transformer -1.8 -1.8 -1.4 -0.9 -0.6 — 0.5 1.3 2.3 † 3.1‡ 3.2‡ 4.4‡ 4.9‡ 5.1‡\nGTCOM_DLUT -2.4 -2.3 -1.9 -1.5 -1.1 -0.5 — 0.8 1.8 ‡ 2.6‡ 2.6‡ 3.9‡ 4.4‡ 4.6‡\nONLINE-A -3.1 -3.1 -2.7 -2.2 -1.8 -1.3 -0.8 — 1.0 ‡ 1.9‡ 1.9‡ 3.1‡ 3.6‡ 3.9‡\nONLINE-G -4.1 -4.1 -3.7 -3.2 -2.9 -2.3 -1.8 -1.0 — 0.8 0.9 2.1 ⋆ 2.6† 2.8‡\nONLINE-Y -5.0 -4.9 -4.6 -4.1 -3.7 -3.1 -2.6 -1.9 -0.8 — 0.0 1.3 1.8 2.0 †\nMUNI-NLP -5.0 -4.9 -4.6 -4.1 -3.7 -3.2 -2.6 -1.9 -0.9 -0.0 — 1.2 1.7 2.0 ‡\nLan-BridgeMT -6.2 -6.2 -5.8 -5.3 -5.0 -4.4 -3.9 -3.1 -2.1 -1.3 -1.2 — 0.5 0.7 ⋆\nNLLB_MBR_BLEU -6.7 -6.7 -6.3 -5.8 -5.5 -4.9 -4.4 -3.6 -2.6 -1.8 -1.7 -0.5 — 0.2 ⋆\nNLLB_Greedy -7.0 -6.9 -6.6 -6.1 -5.7 -5.1 -4.6 -3.9 -2.8 -2.0 -2.0 -0.7 -0.2 —\nscore 83.7 83.6 83.2 82.8 82.4 81.8 81.3 80.6 79.5 78.7 78.7 77.4 76.9 76.7\nrank 1-3 1-3 1-3 4-8 4-8 4-8 4-8 4-8 9-11 9-13 9-13 10-13 10-13 14\nTable 27: Head to head comparison for Czech→Ukrainian systems\n37\nGerman→English\nGPT4-5shot\nHuman-refA\nONLINE-A\nONLINE-B\nONLINE-W\nONLINE-Y\nONLINE-G\nGTCOM_DLUT\nONLINE-M\nLanguageX\nLan-BridgeMT\nNLLB_MBR_BLEU\nAIRC\nNLLB_Greedy\nGPT4-5shot — 0.4 0.8 1.2 † 1.5† 2.3† 2.6‡ 3.8‡ 5.0‡ 8.5‡ 10.3‡ 10.7‡ 11.5‡ 12.4‡\nHuman-refA -0.4 — 0.4 0.8 ⋆ 1.1⋆ 1.9† 2.2‡ 3.4‡ 4.6‡ 8.1‡ 9.9‡ 10.3‡ 11.1‡ 12.0‡\nONLINE-A -0.8 -0.4 — 0.4 0.7 1.6 ⋆ 1.9† 3.0‡ 4.2‡ 7.7‡ 9.6‡ 9.9‡ 10.8‡ 11.7‡\nONLINE-B -1.2 -0.8 -0.4 — 0.3 1.1 1.4 ⋆ 2.6‡ 3.8‡ 7.3‡ 9.2‡ 9.5‡ 10.3‡ 11.2‡\nONLINE-W -1.5 -1.1 -0.7 -0.3 — 0.8 1.1 ⋆ 2.3‡ 3.5‡ 7.0‡ 8.9‡ 9.2‡ 10.0‡ 10.9‡\nONLINE-Y -2.3 -1.9 -1.6 -1.1 -0.8 — 0.3 1.5 ‡ 2.7† 6.2‡ 8.0‡ 8.4‡ 9.2‡ 10.1‡\nONLINE-G -2.6 -2.2 -1.9 -1.4 -1.1 -0.3 — 1.2 ‡ 2.4 5.9 ‡ 7.7‡ 8.1‡ 8.9‡ 9.8‡\nGTCOM_DLUT -3.8 -3.4 -3.0 -2.6 -2.3 -1.5 -1.2 — 1.2 4.7 ‡ 6.6‡ 6.9‡ 7.8‡ 8.6‡\nONLINE-M -5.0 -4.6 -4.2 -3.8 -3.5 -2.7 -2.4 -1.2 — 3.5 ‡ 5.3‡ 5.7‡ 6.5‡ 7.4‡\nLanguageX -8.5 -8.1 -7.7 -7.3 -7.0 -6.2 -5.9 -4.7 -3.5 — 1.9 2.2 † 3.0‡ 3.9†\nLan-BridgeMT -10.3 -9.9 -9.6 -9.2 -8.9 -8.0 -7.7 -6.6 -5.3 -1.9 — 0.3 1.2 ⋆ 2.1\nNLLB_MBR_BLEU -10.7 -10.3 -9.9 -9.5 -9.2 -8.4 -8.1 -6.9 -5.7 -2.2 -0.3 — 0.8 1.7\nAIRC -11.5 -11.1 -10.8 -10.3 -10.0 -9.2 -8.9 -7.8 -6.5 -3.0 -1.2 -0.8 — 0.9\nNLLB_Greedy -12.4 -12.0 -11.7 -11.2 -10.9 -10.1 -9.8 -8.6 -7.4 -3.9 -2.1 -1.7 -0.9 —\nscore 90.3 89.9 89.6 89.1 88.8 88.0 87.7 86.5 85.3 81.8 80.0 79.6 78.8 77.9\nrank 1-3 1-3 1-5 3-6 3-6 4-7 6-8 8-9 7-9 10-11 10-13 11-14 12-14 11-14\nTable 28: Head to head comparison for German→English systems\nEnglish→Czech\nHuman-refA\nONLINE-W\nGPT4-5shot\nCUNI-GA\nONLINE-A\nCUNI-DocTransformer\nONLINE-B\nNLLB_MBR_BLEU\nGTCOM_DLUT\nCUNI-Transformer\nNLLB_Greedy\nONLINE-M\nONLINE-G\nONLINE-Y\nLan-BridgeMT\nLanguageX\nHuman-refA — 1.3 ⋆ 3.6‡ 5.0‡ 5.1‡ 6.0‡ 6.6‡ 6.8‡ 7.0‡ 8.0‡ 8.6‡ 9.7‡ 10.2‡ 10.4‡ 10.4‡ 11.3‡\nONLINE-W -1.3 — 2.3 ‡ 3.7‡ 3.8‡ 4.7‡ 5.3‡ 5.5‡ 5.7‡ 6.7‡ 7.3‡ 8.4‡ 8.9‡ 9.1‡ 9.1‡ 10.0‡\nGPT4-5shot -3.6 -2.3 — 1.4 1.5 † 2.5⋆ 3.0 3.2 ‡ 3.4† 4.4‡ 5.1‡ 6.1‡ 6.6‡ 6.8‡ 6.8‡ 7.7‡\nCUNI-GA -5.0 -3.7 -1.4 — 0.0 ‡ 1.0‡ 1.5⋆ 1.8‡ 2.0‡ 3.0‡ 3.6‡ 4.7‡ 5.1‡ 5.3‡ 5.4‡ 6.3‡\nONLINE-A -5.1 -3.8 -1.5 -0.0 — 1.0 1.5 1.7 ‡ 1.9 2.9 ⋆ 3.6‡ 4.7† 5.1‡ 5.3‡ 5.4⋆ 6.3‡\nCUNI-DocTransformer -6.0 -4.7 -2.5 -1.0 -1.0 — 0.5 0.7 ‡ 0.9 1.9 † 2.6‡ 3.7‡ 4.1‡ 4.3‡ 4.4† 5.3‡\nONLINE-B -6.6 -5.3 -3.0 -1.5 -1.5 -0.5 — 0.2 ‡ 0.4⋆ 1.4‡ 2.1‡ 3.2‡ 3.6‡ 3.8‡ 3.9‡ 4.8‡\nNLLB_MBR_BLEU -6.8 -5.5 -3.2 -1.8 -1.7 -0.7 -0.2 — 0.2 1.2 1.9 3.0 3.4 3.6 ⋆ 3.7 4.5 ‡\nGTCOM_DLUT -7.0 -5.7 -3.4 -2.0 -1.9 -0.9 -0.4 -0.2 — 1.0 1.7 ‡ 2.8† 3.2‡ 3.4‡ 3.5 4.3 ‡\nCUNI-Transformer -8.0 -6.7 -4.4 -3.0 -2.9 -1.9 -1.4 -1.2 -1.0 — 0.7 ⋆ 1.7 2.2 † 2.4‡ 2.4 3.3 ‡\nNLLB_Greedy -8.6 -7.3 -5.1 -3.6 -3.6 -2.6 -2.1 -1.9 -1.7 -0.7 — 1.1 1.5 1.7 ⋆ 1.8 2.7 ‡\nONLINE-M -9.7 -8.4 -6.1 -4.7 -4.7 -3.7 -3.2 -3.0 -2.8 -1.7 -1.1 — 0.4 0.6 † 0.7 1.6 ‡\nONLINE-G -10.2 -8.9 -6.6 -5.1 -5.1 -4.1 -3.6 -3.4 -3.2 -2.2 -1.5 -0.4 — 0.2 0.3 1.1 †\nONLINE-Y -10.4 -9.1 -6.8 -5.3 -5.3 -4.3 -3.8 -3.6 -3.4 -2.4 -1.7 -0.6 -0.2 — 0.1 1.0 ⋆\nLan-BridgeMT -10.4 -9.1 -6.8 -5.4 -5.4 -4.4 -3.9 -3.7 -3.5 -2.4 -1.8 -0.7 -0.3 -0.1 — 0.9 ‡\nLanguageX -11.3 -10.0 -7.7 -6.3 -6.3 -5.3 -4.8 -4.5 -4.3 -3.3 -2.7 -1.6 -1.1 -1.0 -0.9 —\nscore 85.4 84.1 81.8 80.4 80.3 79.4 78.8 78.6 78.4 77.4 76.8 75.7 75.2 75.0 75.0 74.1\nrank 1 2 3-5 3-4 5-8 5-8 4-7 8-14 6-11 8-12 10-14 9-14 10-15 13-15 8-15 16\nTable 29: Head to head comparison for English→Czech systems\n38\nEnglish→German\nGPT4-5shot\nONLINE-B\nONLINE-W\nONLINE-A\nONLINE-Y\nHuman-refA\nONLINE-M\nONLINE-G\nLan-BridgeMT\nLanguageX\nNLLB_MBR_BLEU\nNLLB_Greedy\nAIRC\nGPT4-5shot — 0.1 0.7 0.8 1.0 † 1.3 2.3 ‡ 3.4‡ 5.0‡ 6.3‡ 12.1‡ 13.2‡ 15.4‡\nONLINE-B -0.1 — 0.6 0.7 0.8 ⋆ 1.2 2.2 ‡ 3.3‡ 4.8‡ 6.2‡ 12.0‡ 13.1‡ 15.2‡\nONLINE-W -0.7 -0.6 — 0.2 ⋆ 0.3‡ 0.6 1.6 ‡ 2.7‡ 4.3‡ 5.6‡ 11.5‡ 12.5‡ 14.7‡\nONLINE-A -0.8 -0.7 -0.2 — 0.1 0.5 1.4 ‡ 2.6‡ 4.1‡ 5.5‡ 11.3‡ 12.4‡ 14.5‡\nONLINE-Y -1.0 -0.8 -0.3 -0.1 — 0.3 1.3 † 2.5⋆ 4.0‡ 5.3‡ 11.2‡ 12.3‡ 14.4‡\nHuman-refA -1.3 -1.2 -0.6 -0.5 -0.3 — 1.0 ‡ 2.1‡ 3.7‡ 5.0‡ 10.8‡ 11.9‡ 14.1‡\nONLINE-M -2.3 -2.2 -1.6 -1.4 -1.3 -1.0 — 1.1 2.7 † 4.0‡ 9.9‡ 10.9‡ 13.1‡\nONLINE-G -3.4 -3.3 -2.7 -2.6 -2.5 -2.1 -1.1 — 1.5 ‡ 2.9‡ 8.7‡ 9.8‡ 11.9‡\nLan-BridgeMT -5.0 -4.8 -4.3 -4.1 -4.0 -3.7 -2.7 -1.5 — 1.4 ⋆ 7.2‡ 8.3‡ 10.4‡\nLanguageX -6.3 -6.2 -5.6 -5.5 -5.3 -5.0 -4.0 -2.9 -1.4 — 5.8 ‡ 6.9‡ 9.1‡\nNLLB_MBR_BLEU -12.1 -12.0 -11.5 -11.3 -11.2 -10.8 -9.9 -8.7 -7.2 -5.8 — 1.1 3.2 ‡\nNLLB_Greedy -13.2 -13.1 -12.5 -12.4 -12.3 -11.9 -10.9 -9.8 -8.3 -6.9 -1.1 — 2.2 ‡\nAIRC -15.4 -15.2 -14.7 -14.5 -14.4 -14.1 -13.1 -11.9 -10.4 -9.1 -3.2 -2.2 —\nscore 89.0 88.8 88.3 88.1 88.0 87.7 86.7 85.5 84.0 82.7 76.8 75.7 73.6\nrank 1-5 1-5 1-4 2-6 4-6 1-6 7-8 7-8 9 10 11-12 11-12 13\nTable 30: Head to head comparison for English→German systems\n39\nEnglish→Japanese\nHuman-refA\nGPT4-5shot\nONLINE-B\nONLINE-Y\nSKIM\nONLINE-W\nLanguageX\nONLINE-A\nNAIST-NICT\nLan-BridgeMT\nANVITA\nONLINE-M\nKYB\nAIRC\nONLINE-G\nNLLB_Greedy\nNLLB_MBR_BLEU\nHuman-refA — 1.2 ‡ 1.9 2.1 † 2.2⋆ 2.3‡ 4.1‡ 4.5‡ 4.6‡ 5.5‡ 7.6‡ 8.1‡ 9.9‡ 11.1‡ 11.1‡ 16.2‡ 19.4‡\nGPT4-5shot -1.2 — 0.7 0.9 1.0 1.1 2.9 ⋆ 3.3† 3.4† 4.3‡ 6.4‡ 6.9‡ 8.8‡ 9.9‡ 10.0‡ 15.0‡ 18.3‡\nONLINE-B -1.9 -0.7 — 0.2 0.3 0.4 ⋆ 2.3‡ 2.7‡ 2.7‡ 3.6‡ 5.7‡ 6.2‡ 8.1‡ 9.3‡ 9.3‡ 14.3‡ 17.6‡\nONLINE-Y -2.1 -0.9 -0.2 — 0.1 0.2 2.0 ‡ 2.4‡ 2.5‡ 3.4‡ 5.5‡ 6.0‡ 7.8‡ 9.0‡ 9.0‡ 14.1‡ 17.3‡\nSKIM -2.2 -1.0 -0.3 -0.1 — 0.1 ⋆ 1.9‡ 2.3‡ 2.4‡ 3.3‡ 5.4‡ 5.9‡ 7.7‡ 8.9‡ 8.9‡ 13.9‡ 17.2‡\nONLINE-W -2.3 -1.1 -0.4 -0.2 -0.1 — 1.8 † 2.2† 2.3‡ 3.2‡ 5.3‡ 5.8‡ 7.6‡ 8.8‡ 8.8‡ 13.8‡ 17.1‡\nLanguageX -4.1 -2.9 -2.3 -2.0 -1.9 -1.8 — 0.4 0.5 1.4 3.5 ‡ 4.0‡ 5.8‡ 7.0‡ 7.0‡ 12.0‡ 15.3‡\nONLINE-A -4.5 -3.3 -2.7 -2.4 -2.3 -2.2 -0.4 — 0.0 1.0 3.1 ‡ 3.5‡ 5.4‡ 6.6‡ 6.6‡ 11.6‡ 14.9‡\nNAIST-NICT -4.6 -3.4 -2.7 -2.5 -2.4 -2.3 -0.5 -0.0 — 0.9 3.0 ‡ 3.5‡ 5.4‡ 6.5‡ 6.6‡ 11.6‡ 14.9‡\nLan-BridgeMT -5.5 -4.3 -3.6 -3.4 -3.3 -3.2 -1.4 -1.0 -0.9 — 2.1 † 2.6‡ 4.5‡ 5.6‡ 5.6‡ 10.7‡ 14.0‡\nANVITA -7.6 -6.4 -5.7 -5.5 -5.4 -5.3 -3.5 -3.1 -3.0 -2.1 — 0.5 2.3 ‡ 3.5‡ 3.5‡ 8.5‡ 11.8‡\nONLINE-M -8.1 -6.9 -6.2 -6.0 -5.9 -5.8 -4.0 -3.5 -3.5 -2.6 -0.5 — 1.9 † 3.0‡ 3.1† 8.1‡ 11.4‡\nKYB -9.9 -8.8 -8.1 -7.8 -7.7 -7.6 -5.8 -5.4 -5.4 -4.5 -2.3 -1.9 — 1.2 1.2 6.2 ‡ 9.5‡\nAIRC -11.1 -9.9 -9.3 -9.0 -8.9 -8.8 -7.0 -6.6 -6.5 -5.6 -3.5 -3.0 -1.2 — 0.0 5.0 ‡ 8.3‡\nONLINE-G -11.1 -10.0 -9.3 -9.0 -8.9 -8.8 -7.0 -6.6 -6.6 -5.6 -3.5 -3.1 -1.2 -0.0 — 5.0 ‡ 8.3‡\nNLLB_Greedy -16.2 -15.0 -14.3 -14.1 -13.9 -13.8 -12.0 -11.6 -11.6 -10.7 -8.5 -8.1 -6.2 -5.0 -5.0 — 3.3 ‡\nNLLB_MBR_BLEU -19.4 -18.3 -17.6 -17.3 -17.2 -17.1 -15.3 -14.9 -14.9 -14.0 -11.8 -11.4 -9.5 -8.3 -8.3 -3.3 —\nscore 80.7 79.5 78.8 78.6 78.5 78.4 76.6 76.2 76.1 75.2 73.1 72.6 70.8 69.6 69.6 64.5 61.3\nrank 1-2 2-6 1-5 2-6 2-5 4-6 7-10 7-10 7-10 7-10 11-12 11-12 13-15 13-15 13-15 16 17\nTable 31: Head to head comparison for English→Japanese systems\n40\nEnglish→Chinese\nYishu\nHuman-refA\nGPT4-5shot\nLan-BridgeMT\nONLINE-B\nHW-TSC\nONLINE-W\nONLINE-Y\nIOL_Research\nONLINE-A\nLanguageX\nONLINE-M\nONLINE-G\nANVITA\nNLLB_Greedy\nNLLB_MBR_BLEU\nYishu — 0.0 0.1 0.2 ⋆ 0.3 0.7 0.8 ⋆ 2.0⋆ 2.3‡ 2.5‡ 3.6‡ 4.0‡ 5.0‡ 17.7‡ 17.9‡ 25.0‡\nHuman-refA -0.0 — 0.0 0.1 † 0.3 0.7 0.8 ⋆ 1.9† 2.3‡ 2.5‡ 3.6‡ 3.9‡ 5.0‡ 17.7‡ 17.8‡ 25.0‡\nGPT4-5shot -0.1 -0.0 — 0.1 0.3 0.6 0.7 1.9 ⋆ 2.3‡ 2.4‡ 3.5‡ 3.9‡ 5.0‡ 17.6‡ 17.8‡ 24.9‡\nLan-BridgeMT -0.2 -0.1 -0.1 — 0.2 0.5 0.6 1.8 2.2 † 2.3‡ 3.4‡ 3.8‡ 4.9‡ 17.5‡ 17.7‡ 24.8‡\nONLINE-B -0.3 -0.3 -0.3 -0.2 — 0.3 0.4 † 1.6† 2.0‡ 2.2‡ 3.2‡ 3.6‡ 4.7‡ 17.3‡ 17.5‡ 24.7‡\nHW-TSC -0.7 -0.7 -0.6 -0.5 -0.3 — 0.1 1.3 1.7 ‡ 1.8‡ 2.9‡ 3.3‡ 4.4‡ 17.0‡ 17.2‡ 24.3‡\nONLINE-W -0.8 -0.8 -0.7 -0.6 -0.4 -0.1 — 1.2 1.6 † 1.7‡ 2.8‡ 3.2‡ 4.3‡ 16.9‡ 17.1‡ 24.2‡\nONLINE-Y -2.0 -1.9 -1.9 -1.8 -1.6 -1.3 -1.2 — 0.4 ⋆ 0.5† 1.6‡ 2.0‡ 3.1‡ 15.7‡ 15.9‡ 23.0‡\nIOL_Research -2.3 -2.3 -2.3 -2.2 -2.0 -1.7 -1.6 -0.4 — 0.2 1.2 † 1.6† 2.7‡ 15.3‡ 15.5‡ 22.7‡\nONLINE-A -2.5 -2.5 -2.4 -2.3 -2.2 -1.8 -1.7 -0.5 -0.2 — 1.1 ⋆ 1.5⋆ 2.5‡ 15.2‡ 15.4‡ 22.5‡\nLanguageX -3.6 -3.6 -3.5 -3.4 -3.2 -2.9 -2.8 -1.6 -1.2 -1.1 — 0.4 1.5 14.1 ‡ 14.3‡ 21.4‡\nONLINE-M -4.0 -3.9 -3.9 -3.8 -3.6 -3.3 -3.2 -2.0 -1.6 -1.5 -0.4 — 1.1 13.7 ‡ 13.9‡ 21.0‡\nONLINE-G -5.0 -5.0 -5.0 -4.9 -4.7 -4.4 -4.3 -3.1 -2.7 -2.5 -1.5 -1.1 — 12.6 ‡ 12.8‡ 20.0‡\nANVITA -17.7 -17.7 -17.6 -17.5 -17.3 -17.0 -16.9 -15.7 -15.3 -15.2 -14.1 -13.7 -12.6 — 0.2 ‡ 7.3‡\nNLLB_Greedy -17.9 -17.8 -17.8 -17.7 -17.5 -17.2 -17.1 -15.9 -15.5 -15.4 -14.3 -13.9 -12.8 -0.2 — 7.1 ‡\nNLLB_MBR_BLEU -25.0 -25.0 -24.9 -24.8 -24.7 -24.3 -24.2 -23.0 -22.7 -22.5 -21.4 -21.0 -20.0 -7.3 -7.1 —\nscore 82.2 82.1 82.1 82.0 81.8 81.5 81.4 80.2 79.8 79.7 78.6 78.2 77.1 64.5 64.3 57.2\nrank 1-5 1-5 1-7 3-8 1-6 1-8 4-8 5-8 9-10 9-10 11-13 11-13 11-13 14 15 16\nTable 32: Head to head comparison for English→Chinese systems\n41\nJapanese→EnglishGPT4-5shot\nSKIM\nHuman-refA\nONLINE-Y\nONLINE-B\nONLINE-A\nONLINE-W\nNAIST-NICT\nGTCOM_DLUT\nLan-BridgeMT\nANVITA\nONLINE-G\nLanguageX\nONLINE-M\nKYB\nAIRC\nNLLB_MBR_BLEU\nNLLB_Greedy\nGPT4-5shot — 0.7 ⋆ 0.9‡ 1.8‡ 1.9‡ 2.1‡ 2.5† 2.9‡ 4.4‡ 4.8‡ 5.5‡ 6.5‡ 6.7‡ 8.4‡ 8.9‡ 12.4‡ 14.6‡ 15.2‡\nSKIM -0.7 — 0.2 † 1.0⋆ 1.2 1.3 † 1.7 2.2 ⋆ 3.6‡ 4.1‡ 4.7‡ 5.8‡ 5.9‡ 7.7‡ 8.1‡ 11.6‡ 13.8‡ 14.5‡\nHuman-refA -0.9 -0.2 — 0.9 1.0 1.1 1.5 2.0 3.5 ⋆ 3.9‡ 4.5‡ 5.6‡ 5.7‡ 7.5‡ 7.9‡ 11.4‡ 13.7‡ 14.3‡\nONLINE-Y -1.8 -1.0 -0.9 — 0.1 0.3 0.7 1.1 2.6 † 3.1‡ 3.7‡ 4.7‡ 4.9‡ 6.6‡ 7.1‡ 10.6‡ 12.8‡ 13.4‡\nONLINE-B -1.9 -1.2 -1.0 -0.1 — 0.2 0.6 1.0 2.5 † 2.9‡ 3.6‡ 4.6‡ 4.8‡ 6.5‡ 7.0‡ 10.5‡ 12.7‡ 13.3‡\nONLINE-A -2.1 -1.3 -1.1 -0.3 -0.2 — 0.4 0.8 2.3 2.8 ‡ 3.4‡ 4.4‡ 4.6‡ 6.3‡ 6.8‡ 10.3‡ 12.5‡ 13.2‡\nONLINE-W -2.5 -1.7 -1.5 -0.7 -0.6 -0.4 — 0.4 1.9 † 2.4‡ 3.0‡ 4.0‡ 4.2‡ 6.0‡ 6.4‡ 9.9‡ 12.1‡ 12.8‡\nNAIST-NICT -2.9 -2.2 -2.0 -1.1 -1.0 -0.8 -0.4 — 1.5 † 2.0‡ 2.6‡ 3.6‡ 3.8‡ 5.5‡ 6.0‡ 9.5‡ 11.7‡ 12.3‡\nGTCOM_DLUT -4.4 -3.6 -3.5 -2.6 -2.5 -2.3 -1.9 -1.5 — 0.5 ‡ 1.1† 2.1‡ 2.3† 4.0‡ 4.5‡ 8.0‡ 10.2‡ 10.9‡\nLan-BridgeMT -4.8 -4.1 -3.9 -3.1 -2.9 -2.8 -2.4 -2.0 -0.5 — 0.6 1.7 1.8 3.6 ‡ 4.0‡ 7.5‡ 9.7‡ 10.4‡\nANVITA -5.5 -4.7 -4.5 -3.7 -3.6 -3.4 -3.0 -2.6 -1.1 -0.6 — 1.1 1.2 3.0 ‡ 3.4‡ 6.9‡ 9.1‡ 9.8‡\nONLINE-G -6.5 -5.8 -5.6 -4.7 -4.6 -4.4 -4.0 -3.6 -2.1 -1.7 -1.1 — 0.2 1.9 ‡ 2.4‡ 5.9‡ 8.1‡ 8.7‡\nLanguageX -6.7 -5.9 -5.7 -4.9 -4.8 -4.6 -4.2 -3.8 -2.3 -1.8 -1.2 -0.2 — 1.8 ‡ 2.2‡ 5.7‡ 7.9‡ 8.6‡\nONLINE-M -8.4 -7.7 -7.5 -6.6 -6.5 -6.3 -6.0 -5.5 -4.0 -3.6 -3.0 -1.9 -1.8 — 0.5 4.0 ‡ 6.2‡ 6.8‡\nKYB -8.9 -8.1 -7.9 -7.1 -7.0 -6.8 -6.4 -6.0 -4.5 -4.0 -3.4 -2.4 -2.2 -0.5 — 3.5 ‡ 5.7‡ 6.4‡\nAIRC -12.4 -11.6 -11.4 -10.6 -10.5 -10.3 -9.9 -9.5 -8.0 -7.5 -6.9 -5.9 -5.7 -4.0 -3.5 — 2.2 † 2.9†\nNLLB_MBR_BLEU -14.6 -13.8 -13.7 -12.8 -12.7 -12.5 -12.1 -11.7 -10.2 -9.7 -9.1 -8.1 -7.9 -6.2 -5.7 -2.2 — 0.6\nNLLB_Greedy -15.2 -14.5 -14.3 -13.4 -13.3 -13.2 -12.8 -12.3 -10.9 -10.4 -9.8 -8.7 -8.6 -6.8 -6.4 -2.9 -0.6 —\nscore 81.3 80.6 80.4 79.5 79.4 79.2 78.8 78.4 76.9 76.4 75.8 74.8 74.6 72.9 72.4 68.9 66.7 66.1\nrank 1 2-4 3-8 3-8 2-8 3-9 2-8 3-8 8-9 10-13 10-13 10-13 10-13 14-15 14-15 16 17-18 17-18\nTable 33: Head to head comparison for Japanese→English systems\n42\nChinese→English\nLan-BridgeMT\nGPT4-5shot\nYishu\nONLINE-W\nONLINE-G\nONLINE-B\nONLINE-Y\nHW-TSC\nONLINE-A\nIOL_Research\nLanguageX\nONLINE-M\nNLLB_MBR_BLEU\nHuman-refA\nNLLB_Greedy\nANVITA\nLan-BridgeMT — 1.9 2.6 ‡ 2.7‡ 2.9‡ 3.1‡ 3.2‡ 3.8‡ 5.1‡ 5.2‡ 5.6‡ 6.0‡ 6.7‡ 6.8‡ 8.9‡ 10.3‡\nGPT4-5shot -1.9 — 0.6 ‡ 0.8‡ 1.0‡ 1.1† 1.2‡ 1.9† 3.1‡ 3.3‡ 3.7‡ 4.1‡ 4.7‡ 4.9‡ 6.9‡ 8.3‡\nYishu -2.6 -0.6 — 0.2 0.3 ⋆ 0.5 0.6 1.3 2.5 2.6 ‡ 3.1‡ 3.5‡ 4.1‡ 4.3‡ 6.3‡ 7.7‡\nONLINE-W -2.7 -0.8 -0.2 — 0.2 ⋆ 0.4 0.5 1.1 2.3 ⋆ 2.5‡ 2.9‡ 3.3‡ 4.0‡ 4.1‡ 6.2‡ 7.6‡\nONLINE-G -2.9 -1.0 -0.3 -0.2 — 0.2 0.3 0.9 2.2 2.3 ⋆ 2.8 3.1 ‡ 3.8‡ 3.9‡ 6.0‡ 7.4‡\nONLINE-B -3.1 -1.1 -0.5 -0.4 -0.2 — 0.1 † 0.8 2.0 ‡ 2.1‡ 2.6‡ 3.0‡ 3.6‡ 3.8‡ 5.8‡ 7.2‡\nONLINE-Y -3.2 -1.2 -0.6 -0.5 -0.3 -0.1 — 0.7 1.9 2.0 † 2.5⋆ 2.9‡ 3.5‡ 3.7‡ 5.7‡ 7.1‡\nHW-TSC -3.8 -1.9 -1.3 -1.1 -0.9 -0.8 -0.7 — 1.2 ‡ 1.4‡ 1.8‡ 2.2‡ 2.8‡ 3.0‡ 5.0‡ 6.5‡\nONLINE-A -5.1 -3.1 -2.5 -2.3 -2.2 -2.0 -1.9 -1.2 — 0.1 ⋆ 0.6 1.0 ‡ 1.6‡ 1.8‡ 3.8‡ 5.2‡\nIOL_Research -5.2 -3.3 -2.6 -2.5 -2.3 -2.1 -2.0 -1.4 -0.1 — 0.4 0.8 † 1.5‡ 1.6‡ 3.7‡ 5.1‡\nLanguageX -5.6 -3.7 -3.1 -2.9 -2.8 -2.6 -2.5 -1.8 -0.6 -0.4 — 0.4 † 1.0‡ 1.2‡ 3.2‡ 4.6‡\nONLINE-M -6.0 -4.1 -3.5 -3.3 -3.1 -3.0 -2.9 -2.2 -1.0 -0.8 -0.4 — 0.6 † 0.8 2.8 ‡ 4.3†\nNLLB_MBR_BLEU -6.7 -4.7 -4.1 -4.0 -3.8 -3.6 -3.5 -2.8 -1.6 -1.5 -1.0 -0.6 — 0.2 2.2 3.6\nHuman-refA -6.8 -4.9 -4.3 -4.1 -3.9 -3.8 -3.7 -3.0 -1.8 -1.6 -1.2 -0.8 -0.2 — 2.0 ⋆ 3.4\nNLLB_Greedy -8.9 -6.9 -6.3 -6.2 -6.0 -5.8 -5.7 -5.0 -3.8 -3.7 -3.2 -2.8 -2.2 -2.0 — 1.4\nANVITA -10.3 -8.3 -7.7 -7.6 -7.4 -7.2 -7.1 -6.5 -5.2 -5.1 -4.6 -4.3 -3.6 -3.4 -1.4 —\nscore 82.9 80.9 80.3 80.2 80.0 79.8 79.7 79.1 77.8 77.7 77.2 76.9 76.2 76.1 74.0 72.6\nrank 1-2 1-2 3-8 3-7 5-10 3-7 4-9 3-8 6-10 10-11 8-11 12-13 13-16 12-15 14-16 13-16\nTable 34: Head to head comparison for Chinese→English systems"
}