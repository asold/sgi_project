{
  "title": "Compact, Efficient and Unlimited Capacity: Language Modeling with Compressed Suffix Trees",
  "url": "https://openalex.org/W2250988012",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A108414181",
      "name": "Ehsan Shareghi",
      "affiliations": [
        "Monash University",
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A1986955645",
      "name": "Matthias Petri",
      "affiliations": [
        "Monash University",
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A1432492132",
      "name": "Gholamreza Haffari",
      "affiliations": [
        "Monash University",
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2188741563",
      "name": "Trevor Cohn",
      "affiliations": [
        "Monash University",
        "University of Melbourne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2073921136",
    "https://openalex.org/W1962019683",
    "https://openalex.org/W2041445031",
    "https://openalex.org/W2107082304",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W2030962049",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2067974452",
    "https://openalex.org/W1549037892",
    "https://openalex.org/W2172097231",
    "https://openalex.org/W2161488606",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W126222424",
    "https://openalex.org/W2069074882",
    "https://openalex.org/W1916559533",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2171913306",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W2128808215",
    "https://openalex.org/W2135208303",
    "https://openalex.org/W2295986819",
    "https://openalex.org/W2533248932",
    "https://openalex.org/W2158874082",
    "https://openalex.org/W3211848854",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2952380754",
    "https://openalex.org/W1560013842",
    "https://openalex.org/W2059513841",
    "https://openalex.org/W2106540279",
    "https://openalex.org/W2122429665",
    "https://openalex.org/W2250193734",
    "https://openalex.org/W2057147815"
  ],
  "abstract": "Efficient methods for storing and querying language models are critical for scaling to large corpora and high Markov orders.In this paper we propose methods for modeling extremely large corpora without imposing a Markov condition.At its core, our approach uses a succinct index -a compressed suffix tree -which provides near optimal compression while supporting efficient search.We present algorithms for on-the-fly computation of probabilities under a Kneser-Ney language model.Our technique is exact and although slower than leading LM toolkits, it shows promising scaling properties, which we demonstrate through ∞-order modeling over the full Wikipedia collection.",
  "full_text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2409–2418,\nLisbon, Portugal, 17-21 September 2015.c⃝2015 Association for Computational Linguistics.\nCompact, Efﬁcient and Unlimited Capacity: Language Modeling with\nCompressed Sufﬁx Trees\nEhsan Shareghi,♭ Matthias Petri,♮ Gholamreza Haffari♭ and Trevor Cohn♮\n♭ Faculty of Information Technology, Monash University\n♮ Computing and Information Systems, The University of Melbourne\nfirst.last@{monash.edu,unimelb.edu.au}\nAbstract\nEfﬁcient methods for storing and querying\nlanguage models are critical for scaling to\nlarge corpora and high Markov orders. In\nthis paper we propose methods for mod-\neling extremely large corpora without im-\nposing a Markov condition. At its core,\nour approach uses a succinct index – a\ncompressed sufﬁx tree – which provides\nnear optimal compression while support-\ning efﬁcient search. We present algorithms\nfor on-the-ﬂy computation of probabilities\nunder a Kneser-Ney language model. Our\ntechnique is exact and although slower\nthan leading LM toolkits, it shows promis-\ning scaling properties, which we demon-\nstrate through ∞-order modeling over the\nfull Wikipedia collection.\n1 Introduction\nLanguage models (LMs) are critical components\nin many modern NLP systems, including machine\ntranslation (Koehn, 2010) and automatic speech\nrecognition (Rabiner and Juang, 1993). The most\nwidely used LMs are mgram models (Chen and\nGoodman, 1996), based on explicit storage of\nmgrams and their counts, which have proved\nhighly accurate when trained on large datasets. To\nbe useful, LMs need to be not only accurate but\nalso fast and compact.\nDepending on the order and the training corpus\nsize, a typical mgram LM may contain as many\nas several hundred billions of mgrams (Brants\net al., 2007), raising challenges of efﬁcient stor-\nage and retrieval. As always, there is a trade-off\nbetween accuracy, space, and time, with recent\npapers considering small but approximate lossy\nLMs (Chazelle et al., 2004; Talbot and Osborne,\n2007; Guthrie and Hepple, 2010), or loss-less\nLMs backed by tries (Stolcke et al., 2011), or re-\nlated compressed structures (Germann et al., 2009;\nHeaﬁeld, 2011; Pauls and Klein, 2011; Sorensen\nand Allauzen, 2011; Watanabe et al., 2009). How-\never, none of these approaches scale well to very\nhigh-order m or very large corpora, due to their\nhigh memory and time requirements. An impor-\ntant exception is Kennington et al. (2012), who\nalso propose a language model based on a sufﬁx\ntree which scales well with mbut poorly with the\ncorpus size (requiring memory of about 20×the\ntraining corpus).\nIn contrast, we1 make use of recent advances in\ncompressed sufﬁx trees (CSTs) (Sadakane, 2007)\nto build compact indices with much more mod-\nest memory requirements ( ≈the size of the cor-\npus). We present methods for extracting frequency\nand unique context count statistics for mgram\nqueries from C STs, and two algorithms for com-\nputing Kneser-Ney LM probabilities on the ﬂy us-\ning these statistics. The ﬁrst method uses two\nCSTs (over the corpus and the reversed corpus),\nwhich allow for efﬁcient computation of the num-\nber of unique contexts to the left and right of an\nmgram, but is inefﬁcient in several ways, most\nnotably when computing the number of unique\ncontexts to both sides. Our second method ad-\ndresses this problem using a single CST backed by\na wavelet tree based FM-index (Ferragina et al.,\n2007), which results in better time complexity and\nconsiderably faster runtime performance.\nOur experiments show that our method is prac-\ntical for large-scale language modelling, although\nquerying is substantially slower than a S RILM\nbenchmark. However our technique scales much\nmore gracefully with Markov order m, allowing\nunbounded ‘non-Markov’ application, and enables\ntraining on large corpora as we demonstrate on the\ncomplete Wikipedia dump. Overall this paper il-\nlustrates the vast potential succinct indexes have\n1For the implementation see: https://github.com/eehsan/\nlm-sdsl.\n2409\nfor language modelling and other ‘big data’ prob-\nlems in language processing.\n2 Background\nSufﬁx Arrays and Sufﬁx Trees Let T be a\nstring of size ndrawn from an alphabet Σ of size\nσ. Let T[i..n−1] be a sufﬁx of T. The sufﬁx\ntree (Weiner, 1973) of T is the compact labeled\ntree of n+ 1 leaves where the root to leaf paths\ncorrespond to all sufﬁxes of T$, where $ is a ter-\nminating symbol not in Σ. The path-label of each\nnode v corresponds to the concatenation of edge\nlabels from the root node to v. The node depth\nof vcorresponds to the number of ancestors in the\ntree, whereas the string depth corresponds to the\nlength of the path-label. Searching for a pattern α\nof size min T translates to ﬁnding the locus node\nv closest to the root such that αis a preﬁx of the\npath-label of vin O(m) time. We refer to this ap-\nproach as forward search. Figure 1a shows a sufﬁx\ntree over a sample text. A sufﬁx tree requiresO(n)\nspace and can be constructed inO(n) time (Ukko-\nnen, 1995). The children of each node in the sufﬁx\ntree are lexicographically ordered by their edge la-\nbels. The i-th smallest sufﬁx in T corresponds to\nthe path-label of thei-th leaf. The starting position\nof the sufﬁx can be associated its corresponding\nleaf in the tree as shown in Figure 1a. All occur-\nrences of α in T can be retrieved by visiting all\nleaves in the subtree of the locus of α. For exam-\nple, pattern “the night” occurs at positions 12 and\n19 in the sample text. We further refer the number\nof children of a node vas its degree and the num-\nber of leaves in the subtree rooted at v as the size\nof v.\nThe sufﬁx array (Manber and Myers, 1993) of\nTis an array SA[0 ...n −1] such that SA[i] corre-\nsponds to the starting position of the i-th smallest\nsufﬁx in T or the i-th leaf in the sufﬁx tree of T.\nThe sufﬁx array requires nlog nbits of space and\ncan also be constructed in O(n) time (K¨arkk¨ainen\net al., 2006). Using only the sufﬁx array and the\ntext, pattern search can be performed using bi-\nnary search in O(mlog n) time. For example, the\npattern “the night” is found by performing binary\nsearch using SA and Tto determine SA[18,19], the\ninterval in SA corresponding the the sufﬁxes in T\npreﬁxed by the pattern. In practice, sufﬁx arrays\nuse 4 −8n bytes of space whereas the most ef-\nﬁcient sufﬁx tree implementations require at least\n20nbytes of space (Kurtz, 1999) which are both\nmuch larger than T and prohibit the use of these\nstructures for all but small data sets.\nCompressed Sufﬁx Structures Reducing the\nspace usage of sufﬁx based index structure has\nrecently become an active area of research. The\nspace usage of a sufﬁx array can be reduced sig-\nniﬁcantly by utilizing the compressibility of text\ncombined with succinct data structures. A suc-\ncinct data structure provides the same function-\nality as an equivalent uncompressed data struc-\nture, but requires only space equivalent to the\ninformation-theoretic lower bound of the underly-\ning data. For simplicity, we focus on theFM-Index\nwhich emulates the functionality of a sufﬁx array\nover T using nHk(T) + o(nlog σ) bits of space\nwhere Hk refers to the k-th order entropy of the\ntext (Ferragina et al., 2007). In practice, the FM-\nIndex of T uses roughly space equivalent to the\ncompressed representation of T using a standard\ncompressor such as bzip2. For a more compre-\nhensive overview on succinct text indexes, see the\nexcellent survey of Ferragina et al. (2008).\nThe FM-Index relies on the duality between\nthe sufﬁx array and the BWT (Burrows and\nWheeler, 1994), a permutation of the text such that\nTbwt[i] = T[SA[i] −1] (see Figure 1). Search-\ning for a pattern using the FM-Index is performed\nin reverse order by performing RANK (Tbwt,i,c )\noperations O(m) times. Here, RANK (Tbwt,i,c )\ncounts the number of times symbol c occurs in\nTbwt[0 ...i −1]. This process is usually referred\nto as backward search. Let SA[li,ri] be the in-\nterval corresponding to the sufﬁxes in T match-\ning α[i...m −1]. By deﬁnition of the BWT,\nTbwt[li,ri] corresponds to the symbols in T pre-\nceding α[i...m −1] in T. Due to the lexico-\ngraphical ordering of all sufﬁxes inSA, the interval\nSA[li−1,ri−1] corresponding to all occurrences of\nα[i−1 ...m −1] can be determined by comput-\ning the rank of all occurrences of c = α[i−1] in\nTbwt[li,ri]. Thus, we compute RANK (Tbwt,li,c),\nthe number of times symbol coccurs before li and\nRANK (Tbwt,ri+ 1,c), the number of occurrences\nof c in Tbwt[0,ri]. To determine SA[li−1,ri−1],\nwe additionally store the starting positions Cs of\nall sufﬁxes for each symbol s in Σ at a negligi-\nble cost of σlog n bits. Thus, the new interval\nis computed as li−1 = Cc+RANK (Tbwt,li,c) and\nri−1 = Cc+RANK (Tbwt,ri + 1,c).\nThe time and space complexity of the FM-\nindex thus depends on the cost of storing and pre-\n2410\n22\n2\n10\n21\n11\n0\n18\n8\n17\n7\n14\n4\n15\n5\n20\n13\n3\n16\n6\n19\n12\n1\n9\n$\n#\nthe in\nin keep\n..keeperkeeps\nnight\nold..$the\ntown..$\n$\nthe\n$..night\nold night..$\n$#night\n$..keeper night the #town\n$#night\n..night the town\n$#night\n..the town\n$#night\ntown# the night..\n$#\nkeeper..\n$#night\ntown# the..$\nthe in keep\nnight\n$#\nkeeper keeps..$\nold..$\ntown..$\n$#night\ntown# the night..$\n(a) Word-based Sufﬁx Tree.\n#nT$ppttnnrrttotssi##it\n01100011110011111100001\n#$pprri##i\n0011110000\nnTttnnttotsst\n0111001101001\n#$i##i\n001001\npprr\n0011\np r#$##\n1011 i\n$ #\nnnnoss\n000100\nTtttttt\n1000000\nt Tnnnss\n11100 o\ns n\n10\n0 10 1\n10\n1 10 0\n0 110\n0 1\n(b) Wavelet tree and RANK (Tbwt,17,‘t’) = 5.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22\n$ # # # i i p p r r s s n n n o t t t t t t T\n22 21 11 0 18 8 17 7 14 4 15 5 20 13 3 2 16 6 19 12 1 9 10\n# n T $ p p t t n n r r t t o t s s i # # i t\nT[SA[i]]\nSA\nTbwt\nFigure 1: Data structures for the sample text T=“#the old night keeper keeps the keep in the town#\nthe night keeper keeps the keep in the night#$” with alphabet Σ={the, old, night, keeper, keeps, keep,\nin, town, #}and code words $=0000, #=0001, i=in=001, p=keep=010, r=keeper=011, s=keeps=1000,\no=old=101, t=the=110, n=night=1001 and T=town=111.\nprocessing Tbwt to answer RANK efﬁciently. A\nwavelet tree can be used to answer RANK over\nTbwt in O(log σ) time. The wavelet tree re-\nduces RANK over an alphabet Σ into multiple\nRANK operations over a binary alphabet which\ncan be answered in O(1) time and o(n) bits\nextra space by periodically storing absolute and\nrelative RANK counts (Munro, 1996). The al-\nphabet is reduced by recursively splitting sym-\nbols based on their code words into subgroups\nto form a binary tree as shown in Figure 1b\nfor Tbwt. To answer RANK (Tbwt,i,c ), the tree\nis traversed based on the code word of c, per-\nforming binary RANK at each level. For exam-\nple, RANK (Tbwt,17,‘t’) translates to performing\nRANK (WTroot,17,1) = 12 on the top level of\nthe wavelet tree, as t=the=110. We recurse\nto the right subtree of the root node and com-\npute RANK (WT1,12,1) as there were 12 ones\nin the root node and the next bit in the code-\nword of ‘the’ is also one. This process contin-\nues until the correct leaf node is reached to answer\nRANK (Tbwt,17,‘t’) = 5 in O(log σ) time. The\nspace usage of a regular wavelet tree is nlog σ+\no(nlog σ) bits which roughly matches the size of\nthe text.2 If locations of matches are required, ad-\n2However, if code-words for each symbol are chosen\nbased on their Huffman-codes the size of the wavelet tree\nditional space is needed to access SA[i] or the in-\nverse sufﬁx array SA−1[SA[i]] = i. In the sim-\nplest scheme, both values are periodically sam-\npled using a given sample rate SAS (e.g. 32) such\nthat SA[i] mod SAS = 0 . Then, for any SA[i]\nor SA−1[i], at most O(SAS) RANK operations on\nTbwt are required to access the value. Differ-\nent sample rates, bitvector implementations and\nwavelet tree types result in a wide variety of time-\nspace tradeoffs which can be explored in prac-\ntice (Gog et al., 2014).\nIn the same way the FM-index emulates the\nfunctionality of the sufﬁx array in little space,\ncompressed sufﬁx trees (CST) provide the func-\ntionality of sufﬁx trees while requiring signiﬁ-\ncantly less space than their uncompressed coun-\nterparts (Sadakane, 2007). A C ST uses a com-\npressed sufﬁx array (C SA) such as the FM-Index\nbut stores additional information to represent the\nshape of the sufﬁx tree as well as information\nabout path-labels. Again a variety of different stor-\nage schemes exist, however for simplicity we fo-\ncus on the C ST of Ohlebusch et al. (2010) which\nwe use in our experiments. Here, the shape of the\ntree is stored using a balanced-parenthesis (BP) se-\nquence which for a tree of pnodes requires ≈2p\nreduces to nH0(T)(1 +o(1)) bits which can be further be\nreduced to to nHk(T) +o(nlog σ) bits by using entropy\ncompressed bitvectors.\n2411\nbits. Using little extra space and advanced bit-\noperations, the BP-sequence can be used to per-\nform operations such as string-depth(v), parent(v)\nor accessing the i-th leaf can be answered in con-\nstant time. To support more advanced operations\nsuch as accessing path-labels, the underlying CSA\nor a compressed version of the LCP array are re-\nquired which can be more expensive.3 In practice,\na CST requires roughly 4 −6nbits in addition to\nthe cost of storing the C SA. For a more extensive\noverview of CSTs see Russo et al. (2011).\nKneser Ney Language Modelling Recall our\nproblem of efﬁcient mgram language modeling\nbacked by a corpus encoded in a succinct index.\nAlthough our method is generally applicable to\nmany LM variants, we focus on the Kneser-Ney\nLM (Kneser and Ney, 1995), speciﬁcally the inter-\npolated variant described in Chen and Goodman\n(1996), which has been shown to outperform other\nngram LMs and has become the de-facto standard.\nInterpolated Kneser-Ney describes the condi-\ntional probability of a word wi conditioned on the\ncontext of m−1 preceding words, wi−1\ni−m+1, as\nP(wi|wi−1\ni−m+1) = max\n[\nc(wi\ni−m+1) −Dm,0\n]\nc(wi−1\ni−m+1)\n+ DmN1+(wi−1\ni−m−1 ·)\nc(wi−1\ni−m+1)\n¯P(wi|wi−1\ni−m+2), (1)\nwhere lower-order smoothed probabilities are de-\nﬁned recursively (for 1 <k<m ) as\n¯P(wi|wi−1\ni−k+1) = max\n[\nN1+(· wi\ni−k+1) −Dk,0\n]\nN1+(· wi−1\ni−k+1 ·)\n+ DkN1+(wi−1\ni−k+1 ·)\nN1+(· wi−1\ni−k+1 ·)\n¯P(wi|wi−1\ni−k+2) . (2)\nIn the above formula, Dk is the kgram-speciﬁc\ndiscount parameter, and the occurrence count\nN1+(α·) = |{w: c(αw) >0}|is the number of\nobserved word types following the pattern α; the\noccurrence counts N1+(· α) and N1+(· α·) are\ndeﬁned accordingly. The recursion stops at uni-\ngram level where the unigram probabilities are de-\nﬁned as ¯P(wi) = N1+(· wi)/N1+(· ·).4\n3See Supplementary Materials Table 1 for an overview of\nthe complexities of the functionality of the C ST that is used\nin our experiments.\n4Modiﬁed Kneser-Ney, proposed by Chen and Good-\nman (1996), typically outperforms interpolated Kneser-Ney\nthrough its use of context-speciﬁc discount parameters. The\n3 Using C STs for KN Computation\nThe key requirements for computing probability\nunder a Kneser-Ney language model are two types\nof counts: raw frequencies of mgrams and occur-\nrence counts, quantifying how many different con-\ntexts the mgram has occurred in. Figure 2 (right)\nillustrates the requisite counts for calculating the\nprobability of an example 4-gram. In electing to\nstore the corpus directly in a sufﬁx tree, we need to\nprovide mechanisms for computing these counts\nbased on queries into the sufﬁx tree.\nThe raw frequency counts are the simplest to\ncompute. First we identify the locus node v in\nthe sufﬁx tree for the query mgram; the frequency\ncorresponds to the node’s size, an O(1) operation\nwhich returns the number of leaves belowv. To il-\nlustrate, consider searching forc(the night) in Fig-\nure 1a, which matches a node with two leaves (la-\nbelled 19 and 12), and thus c= 2.\nMore problematic are the occurrence counts,\nwhich come in several ﬂavours: right contexts,\nN1+(α·), left contexts, N1+(· α), and contexts\nto both sides of the pattern, N1+(· α·). The ﬁrst\nof these can be handled easily, as\nN1+(α· ) =\n{ degree(v), if α= label(v)\n1, otherwise\nwhere v is the node matching α, and label(v) de-\nnotes the path-label of v.5 For example, keep\nin has two child nodes in Figure 1a, and thus\nthere are two unique contexts in which it can oc-\ncur, N1+(keep in ·) = 2 , while the keep par-\ntially matches an edge in the forward sufﬁx tree\nin Figure 1a as it can only be followed by in,\nN1+(the keep ·) = 1 . A similar line of reason-\ning applies to computing N1+(· α). Assuming\nwe also have a second sufﬁx tree representing the\nreversed corpus, we ﬁrst identify the reversed pat-\ntern (e.g., in keepR) and then use above method to\ncompute the occurrence count (denoted hereafter\nN1P(t,v,α )6, where tis the CST.).\nimplementation of this with our data structures is straight-\nforward in principle, but brings a few added complexities\nin terms of dynamic computing other types of occurrence\ncounts, which we leave for future work.\n5See the Supplementary Materials for the explicit algo-\nrithm, but note there are some corner cases involving sen-\ntinels # and $, which must be excluded when computing oc-\ncurrence counts. Such tests have been omitted from the pre-\nsentation for clarity.\n6In the presented algorithms, we overload the pattern ar-\ngument in function calls for readability, and use · to denote\nthe query context.\n2412\nroot:%[0,n] root:%[0,n] root:%[0,n]\ntown,\nthe%town,\nin%the%town,\nkeep%in%the%town,\nthe,\nin%the,\nthe2\nin%the2\nkeep%in%the2\nc(keep%in%the%town) c(keep%in%the) N¹⁺(keep%in%the%•)\nN¹⁺(•%in%the%town) N¹⁺(•%in%the%•) N¹⁺(in%the%•)\nN¹⁺(•%the%town) N¹⁺(•%the%•) N¹⁺(the%•)\nN¹⁺(•%town) N¹⁺(••)\nFigure 2: Counts required for computing P(town|keep in the) (right) and the sufﬁx tree nodes required\nfor computing each value (left). The two left-most columns correspond to vall\nR and vR and are updated\nusing forward-search in the reverse CST, while the righter-most column correspond to vF and is updated\nusing backward-search in the forward CST. See Algorithm 2 for details.\nThe ﬁnal component of the Kneser-Ney LM\ncomputation is N1+(· α·), the number of unique\ncontexts considering symbols on both sides of the\npattern. Unfortunately this does not map to a sim-\nple sufﬁx tree operation, but instead requires enu-\nmeration, N1+(· α·) = ∑\ns∈F(α) N1+(· αs),\nwhere F(α) is the set of symbols that can follow\nα. Algorithm 1 shows how this is computed, with\nlines 7 and 8 enumerating s ∈ F(α) using the\nedge labels of the children of v. For each symbol,\nline 9 searches for an extended pattern incorporat-\ning the new symbol sin the reverse C SA (part of\nthe reverse C ST), by reﬁning the existing match\nvR using a single backward search operation af-\nter which we can compute N1+(· αs).7 Line 5\ndeals with the special case where the pattern does\nnot match a complete edge, in which case there\nis only only unique right context and therefore\nN1+(· α·) = N1+(· α).\nN1P and N1 PFRONT BACK can compute the\nrequisite occurence counts for mgram language\nmodelling, however at considerable cost in terms\nof space and time. The need for twin reverse and\nforward C STs incurs a signiﬁcant storage over-\nhead, as well as the search time to match the pat-\ntern in both C STs. We show in Section 5 how\nwe can avoid the need for the reversed sufﬁx tree,\ngiving rise to lower memory requirements and\nfaster runtime. Beyond the need for twin suf-\nﬁx trees, the highest time complexity calls are\nstring-depth, edge and backward-search. Calling\nstring-depth is constant time for internal nodes,\nbut O(SAS log σ) for leaf nodes; fortunately we\n7Backward search in the reverse tree corresponds to\nsearching for the reversed pattern appended with one symbol.\nAlgorithm 1 Two-sided occ., N1+(· α·)\nPrecondition: vF in forward CST tF matches α\nPrecondition: vR in reverse CST tR matches α\n1: function N1PF RONT BACK(tF,vF,tR,vR,α)\n2: o←0\n3: d←string-depth(vF)\n4: if d> |α|then\n5: o←N1P(tR,vR,· α)\n6: else\n7: for uF ←children(vF) do\n8: s←edge(uF,d + 1)\n9: uR ←back-search(vR,s)\n10: o←o+ N1P(tR,uR,· αs)\n11: return o\ncan avoid this call for leaves, which by deﬁnition\nextend to the end of the corpus and consequently\nextend further than our pattern. 8 The costly calls\nto edge and backward-search however cannot be\navoided. This leads to an overall time complex-\nity of O(1) for N1 P and O(F(α) ×SAS ×log σ)\nfor N1 PFRONT BACK, where F(α) is the number\nof following symbols and SAS is the sufﬁx array\nvalue sample rate described in Section 2.\n4 Dual C ST Algorithm\nThe methods above for computing the frequency\nand occurrence counts provide the ingredients\nnecessary for computing mgram language model\nprobabilities. This leaves the algorithmic problem\n8We assume search patterns do not extend beyond a single\nsentence, and thus will always be shorter than the edge labels.\n2413\nAlgorithm 2 KN probability P\n(\nwk|wk−1\nk−(m−1)\n)\n1: function PROB KNESER NEY(tF,tR,w,m)\n2: vF ←root(tF) ⊿match for sufﬁx of wk−1\nk−(m−1)\n3: vR ←root(tR) ⊿match for sufﬁx of wk−1\nk−(m−1)\n4: vall\nR ←root(tR) ⊿match for sufﬁx of wk\nk−(m−1)\n5: p←1\n6: for i←1 to mdo\n7: vall\nR ←forw-search(vall\nR ,wk−i+1)\n8: if i> 1 then\n9: vF ←back-search(vF,wk−i+1)\n10: if i<m then\n11: vR ←forw-search(vR,wk−i+1)\n12: Di ←lookup discount for igram\n13: if i= mthen\n14: c←size(vall\nR )\n15: d←size(vF)\n16: else\n17: c←N1P(tR,vall\nR ,· wk\nk−i+1)\n18: d←\nN1PFRONT BACK(tF,vF,tR,vR,· wk−1\nk−i+1 ·)\n19: if i> 1 then\n20: if vF is valid then\n21: q←N1P(tF,vF,wk−1\nk−i+1 ·)\n22: p← 1\nd (max(c−Di,0) +Diqp)\n23: else if i= 1then\n24: p←c/N1+(· ·)\n25: return p\nof efﬁciently ordering the search operations in for-\nward and reverse CST structures.\nThis paper considers an interpolated LM for-\nmulation, in which probabilities from higher or-\nder contexts are interpolated with lower order es-\ntimates. This iterative process is apparent in Fig-\nure 2 (right) which shows the quantities required\nfor probability scoring for an example mgram.\nEquivalently, the iteration can be considered in re-\nverse, starting from unigram estimates and suc-\ncessively growing to large mgrams, in each stage\nadding a single new symbol to left of the pattern.\nThis suits incremental search in a C ST in which\nsearch bounds are iteratively reﬁned, which has a\nsubstantially lower time complexity compared to\nsearching over the full index in each step.\nAlgorithm 2 presents an outline of the approach.\nThis uses a forward C ST, tF, and a reverse C ST,\ntR, with three C ST nodes (lines 2–4) tracking the\nmatch progress for the full igram ( vall\nR ) and the\n(i−1)gram context ( vF,vR), i = 1 ...m . The\nneed to maintain three concurrent searches arises\nfrom the calls to size, N1+(· α), N1+(α·) and\nN1+(· α·) (lines 14, 15; 17; 21; and 18, respec-\ntively). These calls impose conditions on the di-\nrection of the sufﬁx tree, e.g., such that the edge\nlabels and node degree can be used to compute\nAlgorithm 3 Precompute KN discounts\n1: function PRECOMPUTE DISCOUNTS (tR,m)\n2: ck,f ←0 ∀k∈[1,m],f ∈[1,2]\n3: N1\nk,g ←0 ∀k∈[1,m],g ∈[1,2]\n4: N1+(· ·) ←0\n5: for vR ←descendents(root(tR)) do ⊿depth-ﬁrst\n6: dP ←string-depth(parent(vR))\n7: d←string-depth(vR)\n8: for k←dP + 1to min (d,dP + m) do\n9: s←edge(vR,k)\n10: if sis the end of sentence sentinel then\n11: skip all children of vR\n12: else\n13: if k= 2then\n14: N1+(· ·) ←N1+(· ·) + 1\n15: f ←size(vR)\n16: if 1 ≤f ≤2 then\n17: ck,f ←ck,f + 1\n18: if k<d then\n19: g←1\n20: else\n21: g←degree(vR)\n22: if 1 ≤g≤2 then\n23: N1\nk,g ←N1\nk,g + 1\n24: return c,N1,N1+(· ·)\nthe number of left or right contexts in which a\npattern appears. The matching process is illus-\ntrated in Figure 2 where the three search nodes are\nshown on the left, considered bottom to top, and\ntheir corresponding count operations are shown to\nthe right. The N1+(· α) calls require a match\nin the reverse C ST (left-most column, vall\nR ), while\nthe N1+(α·) require a match in the forward CST\n(right-most column, vF, matching the (i−1)gram\ncontext). The N1+(· α·) computation reuses the\nforward match while also requiring a match for the\n(i−1)gram context in the reversed CST, as tracked\nby the middle column ( vR). Because of the mix\nof forward and reverse CSTs, coupled with search\npatterns that are revealed right-to-left, incremen-\ntal search in each of the C STs needs to be han-\ndled differently (lines 7–11). In the forward C ST,\nwe perform backward search to extend the search\npattern to the left, which can be computed very ef-\nﬁciently from the BWT in the C SA.9 Conversely\nin the reverse CST, we must use forward searchas\nwe are effectively extending the reversed pattern\nto the right; this operation is considerably more\ncostly.\nThe discounts Don line 12 of Algorithm 2 and\nN1+(· ·) (a special case of line 18) are precom-\nputed directly from the CSTs thus avoiding several\ncostly computations at runtime. The precomputa-\n9See Supplementary Materials Table 1 for the time com-\nplexities of this and other CSA and CST methods.\n2414\ntion algorithm is provided in Algorithm 3 which\noperates by traversing the nodes of the reverse\nCST and at each stage computing the number of\nmgrams that occur 1–2 times (used for computing\nDm in eq. 1), or with N1+(· α) ∈[1 −2] (used\nfor computing Dk in eq. 2), for various lengths\nof mgrams. These quantities are used to compute\nthe discount parameters, which are then stored for\nlater use in inference. 10 Note that the P RECOM -\nPUTE DISCOUNTS algorithm can be slow, although\nit is signiﬁcantly faster if we remove theedge calls\nand simply include in our counts all mgrams ﬁn-\nishing a sentence or spanning more than one sen-\ntence. This has a negligible (often beneﬁcial) ef-\nfect on perplexity.\n5 Improved Single C ST Approach\nThe above dual C ST algorithm provides an el-\negant means of computing LM probabilities of\narbitrary order and with a limited space com-\nplexity ( O(n), or roughly n in practice). How-\never the time complexity is problematic, stem-\nming from the expensive method for computing\nN1PFRONT BACK and repeated searches over the\nCST, particularly forward-search. Now we out-\nline a method for speeding up the algorithm by\ndoing away with the reverse CST. Instead the crit-\nical counts, N1+(· α) and N1+(· α·) are com-\nputed directly from a single forward C ST. This\nconfers the beneﬁt of using only backward search\nand avoiding redundant searches for the same pat-\ntern (cf. lines 9 and 11 in Algorithm 2).\nThe full algorithm for computing LM prob-\nabilities is given in Algorithm 4, however for\nspace reasons we will not describe this in de-\ntail. Instead we will focus on the method’s most\ncritical component, the algorithm for computing\nN1+(· α·) from the forward C ST, presented in\nAlgorithm 5. The key difference from Algorithm 1\nis the loop from lines 6–9, which uses theinterval-\nsymbols (Schnattinger et al., 2010) method. This\nmethod assumes a wavelet tree representation of\nthe SA component of the C ST, an efﬁcient encod-\ning of the BWT as describes in section 2. The\ninterval-symbols method uses RANK operations\nto efﬁciently identify for a given pattern the set of\npreceding symbols P(α) and the ranges SA[ls,rs]\ncorresponding to the patterns sαfor all s∈P(α)\n10Discounts are computed up to a limit on mgram size,\nhere set to 10. The highest order values are used for comput-\ning the discount of mgrams above the limit at runtime.\nAlgorithm 4 KN probability P\n(\nwk|wk−1\nk−(m−1)\n)\nusing a single CST\n1: function PROB KNESER NEY1(tF,w,m)\n2: vF ←root(tF) ⊿match for context wk−1\nk−i\n3: vall\nF ←root(tF) ⊿match for wk\nk−i\n4: p←1\n5: for i←1 to mdo\n6: vall\nF ←back-search([lb(vall\nF ),rb(vall\nF )],wk−i+1)\n7: if i> 1 then\n8: vF ←back-search([lb(vF),rb(vF)],wk−i+1)\n9: Di ←discount parameter for igram\n10: if i= mthen\n11: c←size(vall\nF )\n12: d←size(vF)\n13: else\n14: c←N1PBACK 1(tF,vall\nF ,· wk−1\nk−i+1)\n15: d←N1PFRONT BACK 1(tF,vF,· wk−1\nk−i+1 ·)\n16: if i> 1 then\n17: if vF is valid then\n18: q←N1P(tF,vF,wk−1\nk−i+1 ·)\n19: p← 1\nd (max(c−Di,0) +Diqp)\n20: else\n21: p←c/N1+(· ·)\n22: return p\nAlgorithm 5 N1+(· α·), using forward CST\nPrecondition: vF in forward CST tF matches α\n1: function N1PFRONT BACK 1(tF,vF,α)\n2: o←0\n3: if string-depth(vF) >|α|then\n4: o←N1PBACK 1(tF,vF,· α)\n5: else\n6: for ⟨l,r,s⟩← int-syms(tF,[lb(vF),rb(vF)]) do\n7: l′←Cs + l\n8: r′←Cs + r\n9: o←o+ N1P(tF,node(l′,r′),sα ·)\n10: return o\nby visiting all leaves of the wavelet tree of sym-\nbols occurring in Tbwt[l,r] (corresponding to α)\nin O(|P(α)|log σ) time (lines 6-8). These ranges\nSA[l′,r′] can be used to ﬁnd the corresponding suf-\nﬁx tree node for each sα in O(1) time. To illus-\ntrate, consider the pattern α = “night” in Fig-\nure 1a. From Tbwt we can see that this is pre-\nceeded by s =“old” (1st occurrence in Tbwt) and\ns =“the” (3rd and 4th); from which we can com-\npute the sufﬁx tree nodes, namely [15,15] and\n[16 + (3 −1),16 + (4 −1)] = [18,19] for “old”\nand “the” respectively.11\nN1PBACK 1 is computed in a similar way, us-\ning the interval-symbols method to compute the\nnumber of unique preceeding symbols (see Sup-\nplementary Materials, Algorithm 7). Overall the\ntime complexity of inference for both N1PBACK 1\n11Using the offsets into the SA for each symbol, Cold = 15\nand Cthe = 16, while −1 adjusts for counting from 1.\n2415\nLanguage Size(MiB) Tokens(M) Word Types Sentences(K)\nBG 36.11 8.53 114930 329\nCS 53.48 12.25 174592 535\nDE 171.80 44.07 399354 1785\nEN 179.15 49.32 124233 1815\nFI 145.32 32.85 721389 1737\nFR 197.68 53.82 147058 1792\nHU 52.53 12.02 318882 527\nIT 186.67 48.08 178259 1703\nPT 187.20 49.03 183633 1737\nWikipedia 8637 9057 196 87835\nTable 1: Dataset statistics, showing total un-\ncompressed size; and tokens, types and sentence\ncounts for the training partition. For Wikipedia\nthe Word Types, and Tokens are computed based\non characters.\nand N1 PFRONT BACK 1 is O(P(α) logσ) where\nP(α) is the number of preceeding symbols of α, a\nconsiderable improvement over N1PFRONT BACK\nusing the forward and reverse C STs. Overall\nthis leads to considerably faster computation of\nmgram probabilities compared to the two CST ap-\nproach, and although still slower than highly opti-\nmised LM toolkits like SRILM , it is fast enough to\nsupport large scale experiments, and has consider-\nably better scaling performance with the Markov\norder m (even allowing unlimited order), as we\nwill now demonstrate.\n6 Experiments\nWe used Europarl dataset and the data was num-\nberized after tokenizing, splitting, and excluding\nXML markup. The ﬁrst 10k sentences were used\nas the test data, and the last 80% as the train-\ning data, giving rise to training corpora of be-\ntween 8M and 50M tokens and uncompressed size\nof up to 200 MiB (see Table 1 for detailed cor-\npus statistics). We also processed the full 52 GiB\nuncompressed “20150205” English Wikipedia ar-\nticles dump to create a character level language\nmodel consisting of 72M sentences. We excluded\n10k random sentences from the collection as test\ndata. We use the SDSL library (Gog et al., 2014) to\nimplement all our structures and compare our in-\ndexes to S RILM (Stolcke, 2002). We refer to our\ndual-CST approach as D-CST, and the single-CST\nas S-CST.\nWe evaluated the perplexity across different lan-\nguages and using mgrams of varying order from\nm = 2 to ∞(unbounded), as shown on Figure 3.\nOur results matched the perplexity results from\n100%\n102%\n105%\n110%\n120%\n150%\n200%\n2 3 4 5 6 7 8 9 10 15 20 ∞mgram size\nPerplexity [Relative to∞] Language (2-gram pplx∞-gram pplx)\nBG (2-gram117.39∞-gram73.01)CS (2-gram232.36 ∞-gram161.15)DE (2-gram178.11∞-gram108.27)EN (2-gram67.14 ∞-gram59.92)FI (2-gram446.29∞-gram314.22)FR (2-gram89.95 ∞-gram47.91)HU (2-gram251.49∞-gram182.18)IT (2-gram132.26∞-gram77.80)PT (2-gram121.42∞-gram68.58)\nFigure 3: Perplexity results on several Europarl\nlanguages for different mgram sizes, m =\n2 ... 10,15,20,∞.\nSRILM (for smaller values of min which S RILM\ntraining was feasible, m≤10). Note that perplex-\nity drops dramatically from m = 2 ... 5 however\nthe gains thereafter are modest for most languages.\nDespite this, several large mgram matches were\nfound ranging in size up to a 34-gram match. We\nspeculate that the perplexity plateau is due to the\nsimplistic Kneser-Ney discounting formula which\nis not designed for higher order mgram LMs and\nappear to discount large mgrams too aggressively.\nWe leave further exploration of richer discounting\ntechniques such as Modiﬁed Kneser-Ney (Chen\nand Goodman, 1996) or the Sequence Memoizer\n(Wood et al., 2011) to our future work.\nFigure 4 compares space and time of our in-\ndexes with S RILM on the German part of Eu-\nroparl. The construction cost of our indexes in\nterms of both space and time is comparable to\nthat of a 3/4-gram S RILM index. The space us-\nage of D-CST index is comparable to a compact\n3-gram SRILM index. Our S-CST index uses only\n177 MiB RAM at query time, which is compara-\nble to the size of the collection ( 172 MiB). How-\never, query processing is signiﬁcantly slower for\nboth our structures. For 2-grams, D-CST is 3 times\nslower than a 2-gram S RILM index as the expen-\nsive N1+(· α·) is not computed. However, for\nlarge mgrams, our indexes are much slower than\nSRILM. For m >2, the D-CST index is roughly\nsix times slower than S-CST. Our fastest index, is\n10 times slower than the slowest S RILM 10-gram\nindex. However, our run-time is independent of\nm. Thus, as mincreases, our index will become\nmore competitive to SRILM while using a constant\namount of space.\n2416\nConstruction Cost Query Cost\n2\n3 4 5678910\n2\n3\n4 5678910\n2−∞2−∞\n2\n3−5\n2\n3−∞\n2\n3\n4 5678910\n2\n3\n4 5 678910\n10\n100\n1 k\n10 k\n100 M 1 G 10 G100 M 1 G 10 GSpace Usage [bytes]\nTime [sec]\nD-CST S-CST srilm-compactsrilm-default\nFigure 4: Time versus space tradeoffs measured on\nEuroparl German (de) dataset, showing memory\nand time requirements.\n0\n500\n1000\n1500\n0\n100\n200\n300\nD-CSTS-CST\n2 3 4 5 6 8 10 ∞mgram size\nTime per Sentence [msec]\nN1PFRONTBACKfw-searchback-searchN1PBACKN1PFRONT\nFigure 5: Runtime breakdown of a single pattern\naveraged over all patterns for both methods over\nthe Wikipedia collection.\nNext we analyze the performance of our in-\ndex on the large Wikipedia dataset. The S-CST,\ncharacter level index for the data set requires\n22 GiB RAM at query time whereas theD-CST re-\nquires 43 GiB. Figure 5 shows the run-time per-\nformance of both indexes for different mgrams,\nbroken down by the different components of the\ncomputation. As discussed above, 2-gram per-\nformance is much faster. For both indexes, most\ntime is spent computing N1 PFRONT BACK (i.e.,\nN1+(· α·)) for all m> 2. However, the wavelet\ntree traversal used in S-CST roughly reduces the\nrunning time by a factor of three. The complex-\nity of N1PFRONT BACK depends on the number of\ncontexts, which is likely small for larger mgrams,\nbut can be large for small mgrams, which sug-\ngest partial precomputation could signiﬁcantly in-\ncrease the query performance of our indexes. Ex-\nploring the myraid of different CST and CSA con-\nﬁgurations available could also lead to signiﬁcant\nimprovements in runtime and space usage also re-\nmains future work.\n7 Conclusions\nThis paper has demonstrated the massive poten-\ntial that succinct indexes have for language mod-\nelling, by developing efﬁcient algorithms for on-\nthe-ﬂy computing of mgram counts and language\nmodel probabilities. Although we only consid-\nered a Kneser-Ney LM, our approach is portable to\nthe many other LM smoothing method formulated\naround similar count statistics. Our complexity\nanalysis and experimental results show favourable\nscaling properties with corpus size and Markov or-\nder, albeit running between 1-2 orders of magni-\ntude slower than a leading count-based LM. Our\nongoing work seeks to close this gap: preliminary\nexperiments suggest that with careful tuning of the\nsuccinct index parameters and caching expensive\ncomputations, query time can be competitive with\nstate-of-the-art toolkits, while using less memory\nand allowing the use of unlimited context.\nAcknowledgments\nEhsan Shareghi and Gholamreza Haffari are grate-\nful to National ICT Australia (NICTA) for gen-\nerous funding, as part of collaborative machine\nlearning research projects. Matthias Petri is the\nrecipient of an Australian Research Councils Dis-\ncovery Project scheme (project DP140103256).\nTrevor Cohn is the recipient of an Australian Re-\nsearch Council Future Fellowship (project number\nFT130101105).\nReferences\nThorsten Brants, Ashok C Popat, Peng Xu, Franz J\nOch, and Jeffrey Dean. 2007. Large language\nmodels in machine translation. In Proc. EMNLP-\nCoNLL.\nM. Burrows and D. Wheeler. 1994. A block sorting\nlossless data compression algorithm. Technical Re-\nport 124, DEC.\nBernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and\nAyellet Tal. 2004. The bloomier ﬁlter: An efﬁcient\ndata structure for static support lookup tables. In\nProc. SODA, pages 30–39.\nStanley F Chen and Joshua Goodman. 1996. An em-\npirical study of smoothing techniques for language\nmodeling. In Proc. ACL, pages 310–318.\n2417\nP. Ferragina, G. Manzini, V . M¨akinen, and G. Navarro.\n2007. Compressed representations of sequences\nand full-text indexes. ACM Trans. on Algorithms ,\n3(2):article 20.\nPaolo Ferragina, Rodrigo Gonz´alez, Gonzalo Navarro,\nand Rossano Venturini. 2008. Compressed text in-\ndexes: From theory to practice. ACM J. of Exp. Al-\ngorithmics, 13.\nUlrich Germann, Eric Joanis, and Samuel Larkin.\n2009. Tightly packed tries: How to ﬁt large models\ninto memory, and make them load fast, too. In Proc.\nof the Workshop on Software Engineering, Testing,\nand Quality Assurance for Natural Language Pro-\ncessing, pages 31–39.\nSimon Gog, Timo Beller, Alistair Moffat, and Matthias\nPetri. 2014. From theory to practice: Plug and play\nwith succinct data structures. In Proc. SEA, pages\n326–337.\nDavid Guthrie and Mark Hepple. 2010. Storing the\nweb in memory: Space efﬁcient language models\nwith constant time retrieval. In Proc. EMNLP, pages\n262–272.\nKenneth Heaﬁeld. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proc. WMT.\nJuha K¨arkk¨ainen, Peter Sanders, and Stefan Burkhardt.\n2006. Linear work sufﬁx array construction. J.\nACM, 53(6):918–936.\nCasey Redd Kennington, Martin Kay, and Annemarie\nFriedrich. 2012. Sufﬁx trees as language models.\nIn Proc. LREC, pages 446–453.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In\nProc. ICASSP, volume 1, pages 181–184.\nPhilipp Koehn. 2010. Statistical Machine Translation.\nCambridge University Press, New York, NY , USA.\nStefan Kurtz. 1999. Reducing the space requirement\nof sufﬁx trees. Softw., Pract. Exper., 29(13):1149–\n1171.\nUdi Manber and Eugene W. Myers. 1993. Sufﬁx\narrays: A new method for on-line string searches.\nSIAM J. Comput., 22(5):935–948.\nIan Munro. 1996. Tables. In Proc. FSTTCS, pages\n37–42.\nEnno Ohlebusch, Johannes Fischer, and Simon Gog.\n2010. CST++. In Proc. SPIRE, pages 322–333.\nAdam Pauls and Dan Klein. 2011. Faster and smaller\nn-gram language models. In Proc. ACL-HLT.\nLawrence Rabiner and Biing-Hwang Juang. 1993.\nFundamentals of speech recognition. Prentice-Hall.\nL. Russo, G. Navarro, and A. Oliveira. 2011. Fully-\ncompressed sufﬁx trees. ACM Trans. Algorithms ,\n7(4):article 53.\nKunihiko Sadakane. 2007. Compressed sufﬁx trees\nwith full functionality. Theory Comput. Syst. ,\n41(4):589–607.\nThomas Schnattinger, Enno Ohlebusch, and Simon\nGog. 2010. Bidirectional search in a string with\nwavelet trees. In Proc. CPM, pages 40–50.\nJeffrey Sorensen and Cyril Allauzen. 2011. Unary\ndata structures for language models. In Proc. IN-\nTERSPEECH, pages 1425–1428.\nAndreas Stolcke, Jing Zheng, Wen Wang, and Victor\nAbrash. 2011. Srilm at sixteen: Update and out-\nlook. In Proc. ASRU, page 5.\nAndreas Stolcke. 2002. SRILM–an extensible lan-\nguage modeling toolkit. In Proc. INTERSPEECH.\nDavid Talbot and Miles Osborne. 2007. Randomised\nlanguage modelling for statistical machine transla-\ntion. In Proc. ACL.\nEsko Ukkonen. 1995. On-line construction of sufﬁx\ntrees. Algorithmica, 14(3):249–260.\nTaro Watanabe, Hajime Tsukada, and Hideki Isozaki.\n2009. A succinct n-gram language model. In Proc.\nACL Short Papers, pages 341–344.\nPeter Weiner. 1973. Linear pattern matching algo-\nrithms. In Proc. SWAT, pages 1–11.\nFrank Wood, Jan Gasthaus, C ´edric Archambeau,\nLancelot James, and Yee Whye Teh. 2011. The se-\nquence memoizer. CACM, 54(2):91–98.\n2418",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8566913604736328
    },
    {
      "name": "Compressed suffix array",
      "score": 0.8040934801101685
    },
    {
      "name": "Suffix",
      "score": 0.7847779989242554
    },
    {
      "name": "Suffix tree",
      "score": 0.688392698764801
    },
    {
      "name": "Language model",
      "score": 0.5893069505691528
    },
    {
      "name": "Markov chain",
      "score": 0.5811296701431274
    },
    {
      "name": "Computation",
      "score": 0.5433180928230286
    },
    {
      "name": "Scaling",
      "score": 0.502305269241333
    },
    {
      "name": "Theoretical computer science",
      "score": 0.43619561195373535
    },
    {
      "name": "Tree (set theory)",
      "score": 0.42661598324775696
    },
    {
      "name": "Generalized suffix tree",
      "score": 0.4179793894290924
    },
    {
      "name": "Algorithm",
      "score": 0.39539268612861633
    },
    {
      "name": "Artificial intelligence",
      "score": 0.31618139147758484
    },
    {
      "name": "Data structure",
      "score": 0.290407657623291
    },
    {
      "name": "Machine learning",
      "score": 0.1338936984539032
    },
    {
      "name": "Programming language",
      "score": 0.12666592001914978
    },
    {
      "name": "Mathematics",
      "score": 0.09859326481819153
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I165779595",
      "name": "The University of Melbourne",
      "country": "AU"
    }
  ],
  "cited_by": 11
}