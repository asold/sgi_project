{
  "title": "Medical transformer for multimodal survival prediction in intensive care: integration of imaging and non-imaging data",
  "url": "https://openalex.org/W4382797771",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3127350163",
      "name": "Firas Khader",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A2056217728",
      "name": "Jakob Nikolas Kather",
      "affiliations": [
        "University Hospital Carl Gustav Carus",
        "National Center for Tumor Diseases",
        "University Hospital Heidelberg",
        "Universitätsklinikum Aachen",
        "Heidelberg University",
        "University of Leeds"
      ]
    },
    {
      "id": "https://openalex.org/A3214579944",
      "name": "Gustav Müller-Franzes",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A2109385209",
      "name": "Tianci Wang",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A2097930329",
      "name": "Tianyu Han",
      "affiliations": [
        "RWTH Aachen University"
      ]
    },
    {
      "id": "https://openalex.org/A2623000210",
      "name": "Soroosh Tayebi Arasteh",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A4257568183",
      "name": "Karim Hamesch",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A2954959487",
      "name": "Keno Bressem",
      "affiliations": [
        "Charité - Universitätsmedizin Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A2485285558",
      "name": "Christoph Haarburger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116195817",
      "name": "Johannes Stegmaier",
      "affiliations": [
        "RWTH Aachen University"
      ]
    },
    {
      "id": "https://openalex.org/A2171454352",
      "name": "Christiane Kuhl",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A11407780",
      "name": "Sven Nebelung",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A2029259259",
      "name": "Daniel Truhn",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A3127350163",
      "name": "Firas Khader",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A2056217728",
      "name": "Jakob Nikolas Kather",
      "affiliations": [
        "University of Leeds",
        "University Hospital Heidelberg",
        "National Center for Tumor Diseases",
        "Universitätsklinikum Aachen",
        "University Hospital Carl Gustav Carus",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A3214579944",
      "name": "Gustav Müller-Franzes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109385209",
      "name": "Tianci Wang",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A2097930329",
      "name": "Tianyu Han",
      "affiliations": [
        "RWTH Aachen University"
      ]
    },
    {
      "id": "https://openalex.org/A2623000210",
      "name": "Soroosh Tayebi Arasteh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4257568183",
      "name": "Karim Hamesch",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A2954959487",
      "name": "Keno Bressem",
      "affiliations": [
        "Charité - Universitätsmedizin Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A2485285558",
      "name": "Christoph Haarburger",
      "affiliations": [
        "MunEDA (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2116195817",
      "name": "Johannes Stegmaier",
      "affiliations": [
        "RWTH Aachen University"
      ]
    },
    {
      "id": "https://openalex.org/A2171454352",
      "name": "Christiane Kuhl",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A11407780",
      "name": "Sven Nebelung",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    },
    {
      "id": "https://openalex.org/A2029259259",
      "name": "Daniel Truhn",
      "affiliations": [
        "Universitätsklinikum Aachen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2072492033",
    "https://openalex.org/W3135446575",
    "https://openalex.org/W2150765167",
    "https://openalex.org/W2538637016",
    "https://openalex.org/W2764165920",
    "https://openalex.org/W2795884566",
    "https://openalex.org/W2904183610",
    "https://openalex.org/W2995942064",
    "https://openalex.org/W3008097860",
    "https://openalex.org/W4310781980",
    "https://openalex.org/W2177636238",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3182134864",
    "https://openalex.org/W2964962196",
    "https://openalex.org/W1998473914",
    "https://openalex.org/W4221022534",
    "https://openalex.org/W3158315073",
    "https://openalex.org/W4226281901",
    "https://openalex.org/W3011727199",
    "https://openalex.org/W3112990870",
    "https://openalex.org/W3216182232",
    "https://openalex.org/W4225746601",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2009245149",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W2162800060",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2597505554",
    "https://openalex.org/W2087600499",
    "https://openalex.org/W2617595617",
    "https://openalex.org/W4226196842",
    "https://openalex.org/W4298001211",
    "https://openalex.org/W3048545344",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W3101973032",
    "https://openalex.org/W4386470319"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:10666  | https://doi.org/10.1038/s41598-023-37835-1\nwww.nature.com/scientificreports\nMedical transformer for multimodal \nsurvival prediction in intensive \ncare: integration of imaging \nand non‑imaging data\nFiras Khader 1*, Jakob Nikolas Kather 2,3,4,5, Gustav Müller‑Franzes 1, Tianci Wang 1, \nTianyu Han 6, Soroosh Tayebi Arasteh 1, Karim Hamesch 2, Keno Bressem 7, \nChristoph Haarburger 8, Johannes Stegmaier 9, Christiane Kuhl 1, Sven Nebelung 1,10 & \nDaniel Truhn 1,10\nWhen clinicians assess the prognosis of patients in intensive care, they take imaging and non‑imaging \ndata into account. In contrast, many traditional machine learning models rely on only one of these \nmodalities, limiting their potential in medical applications. This work proposes and evaluates a \ntransformer‑based neural network as a novel AI architecture that integrates multimodal patient \ndata, i.e., imaging data (chest radiographs) and non‑imaging data (clinical data). We evaluate the \nperformance of our model in a retrospective study with 6,125 patients in intensive care. We show \nthat the combined model (area under the receiver operating characteristic curve [AUROC] of 0.863) \nis superior to the radiographs‑only model (AUROC  = 0.811, p < 0.001) and the clinical data‑only model \n(AUROC = 0.785, p < 0.001) when tasked with predicting in‑hospital survival per patient. Furthermore, \nwe demonstrate that our proposed model is robust in cases where not all (clinical) data points are \navailable.\nBy definition, patients in intensive care are seriously and critically ill. In caring for those patients, intensive care \nprovides a cornerstone of contemporary clinical medicine. Consequently, major hospitals usually operate at \nleast one intensive care unit (ICU) to admit and treat those patients. Substantial financial resources that amount \nto about 1% of the gross domestic product in the United States are utilized annually to care for those  patients1. \nThese resources are used to improve patient monitoring and treatment. During the ICU stay, increasing amounts \nof clinical data are collected during patient diagnosis, treatment, and monitoring. Nowadays, most of these \ndata are stored digitally and can be harvested from Electronic Health Records (EHR) systems and from picture \narchiving and communication systems (PACS) to be used in translational  research2. Even with the advent of \never more powerful machine learning models, this plethora of data has not been used to the full extent. Machine \nlearning models have predominantly used clinical data, i.e., EHR  data3–6 or imaging data  alone7–10. This approach \ncontrasts with how physicians incorporate clinical data and patient information. Experts interpret imaging \nstudies in clinical contexts to help distinguish between different disease states. Ideally, chest radiographs from \nthe ICU should be interpreted with complete clinical data available to assess the patient’s state optimally, yet this \nmay not always be the case. Combining expert knowledge from different specialties requires time-consuming \nconsultations and may be challenging to realize on a 24/7  basis11. Accordingly, machine learning models that \nintegrate non-imaging and imaging data are needed. Recent advances have seen the rise of transformer models \nOPEN\n1Department of Diagnostic and Interventional Radiology, University Hospital Aachen, Aachen, \nGermany. 2Department of Medicine III, University Hospital Aachen, Aachen, Germany. 3Else Kroener \nFresenius Center for Digital Health, Medical Faculty Carl Gustav Carus, Technical University Dresden, Dresden, \nGermany. 4Division of Pathology and Data Analytics, Leeds Institute of Medical Research at St James’s, University \nof Leeds, Leeds, UK. 5Medical Oncology, National Center for Tumor Diseases (NCT), University Hospital \nHeidelberg, Heidelberg, Germany. 6Physics of Molecular Imaging Systems, Experimental Molecular Imaging, \nRWTH Aachen University, Aachen, Germany. 7Department of Radiology, Charité-University Medicine Berlin, \nBerlin, Germany. 8Ocumeda GmbH, Munich, Germany. 9Institute of Imaging and Computer Vision, RWTH Aachen \nUniversity, Aachen, Germany.  10 These authors contributed equally: Sven Nebelung and Daniel Truhn.  *email: \nfkhader@ukaachen.de\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:10666  | https://doi.org/10.1038/s41598-023-37835-1\nwww.nature.com/scientificreports/\nthat constitute the state-of-the-art technique in natural language processing and are applied to image processing \nwith competitive performance as convolutional neural networks (CNNs)12,13.\nPrevious methods for predicting the survival of patients in intensive care have predominantly utilized \ncombinations of CNNs and recurrent neural networks (RNNs)14,15. On the one hand, integrating non-imaging \ndata into CNNs is challenging. It requires novel methods such as rescaling the feature  maps16 or devising \nalternative means for presenting the non-imaging data in matrix  form17. The latter approach means that the data \nare concatenated to the input image prior to feeding them into the neural  network17. On the other hand, RNNs \nsuffer from vanishing or exploding gradients, which limits the possible time horizon of extracted laboratory \n data18. Combining CNNs and RNNs necessitates a laborious multi-step approach. Modality-specific feature \nextractors are trained initially, followed by a fusion step combining the features for the final  prediction14. In \ncontrast, the transformer neural network is an input-agnostic method with a dedicated attention mechanism. \nA set of tokens is the only input, which may be easily created from various non-imaging and imaging  data12,13. \nThis approach enables end-to-end training and an intuitive combination of variable data sources. Imaging data-\nrelated tokens can now attend to non-imaging data-related tokens and vice versa. Furthermore, unlike RNNs, \ntransformer neural networks do not rely on a long chain of sequential processing steps but on parallel processing, \ntherefore mitigating the problem of vanishing and exploding  gradients13.\nTo our best knowledge, transformer neural networks have not yet been used for survival predictions of \npatients in intensive care. The accurate prognosis is clinically relevant for these patients because (i) physicians \nmay be better supported to decide if and how a patient may benefit from intensive care, and (ii) families may be \nbetter informed about the goals and potential advantages and disadvantages of intensive care.\nThis work presents the multimodal Medical Transformer (MeTra) that can process non-imaging and imaging \ndata. Our architecture can learn from imaging data, non-imaging data, or a combination of both. We test our \nmodel on bedside chest radiographs, likely the most frequently ordered imaging study worldwide, accounting \nfor approximately 20–25% of all diagnostic imaging activities in  healthcare19,20. The accompanying non-imaging \ndata (synonymous with clinical data and clinical parameters [CP]) to these radiographs represents the situation \nphysicians encounter in the clinical routine. It comprises clinical tests (i.e., Glasgow Coma Scale), physiological \nparameters (i.e., heart rate, respiratory rate), blood serum parameters (i.e., glucose concentration, oxygen \nsaturation), and information on body constitution (i.e., height and weight).\nThe overarching objective of this study was to apply and systematically evaluate the multimodal MeTra \nnetwork architecture to integrate non-imaging and imaging data in the survival prediction of patients in intensive \ncare, i.e., in the medical domain. We hypothesized that (i) the MeTra model would predict the survival of \npatients in intensive care more accurately when trained with imaging data, i.e., bedside chest radiographs, and \nnon-imaging data, i.e., clinical data, than when trained with each data category alone. We also hypothesized that \n(ii) the MeTra model’s predictive performance would be robust and maintained when missing pertinent data.\nResults\nCharacteristics of the dataset. Within the MIMIC-IV  dataset21, 6125 patients had chest radiographs and \nclinical parameters, resulting in 6,798 bedside chest radiographs with corresponding clinical parameters (see \nFig. 1). At the time of recording, patient age ranged from 18 to 91 years with a mean of 64 years ± 16 [standard \ndeviation]. To preserve anonymity, all patients older than 89 years had been assigned the age of 91 years by the \ndataset providers. Of all patients, 55% (n = 3382) were male and 45% (n = 2743) were female. A total of n = 1,002 \npatients died in the hospital. A detailed description of the data is given in Table 1.\nResults of MeTra model training on unimodal data only. Table 2 and Fig.  2 summarize the MeTra \nmodel’s performance when trained on single data categories. When trained on 15 clinical parameters only, MeTra \nwas characterized by an AUROC (area under the receiver operating characteristic curve) value of 0.785 [95% CI \n[confidence interval] 0.751, 0.819], a sensitivity of 0.703 [0.640, 0.766], a specificity of 0.731 [0.706, 0.756], and \na positive predictive value of 0.320 [0.278, 0.363]. When trained on the chest radiographs only, MeTra reached \nan AUROC value of 0.811 [0.779, 0.841], a sensitivity of 0.713 [0.650, 0.773], a specificity of 0.767 [0.743, 0.791], \nand a positive predictive value of 0.355 [0.310, 0.401]. In all metrics, training on chest radiographs only tended \ntowards better performance than training on clinical parameters only. Nevertheless, statistical significance \nwas only found for specificity (p = 0.02), while the other statistical measures were not significantly different \n(AUROC, p = 0.14; sensitivity, p = 0.41; positive predictive value, p = 0.14). Exemplary images for correct and \nincorrect model predictions are given in Fig.  3. By trend, the combined model could correctly predict survival \neven when the unimodal models were contradictory in their predictions, e.g., when the radiograph was largely \ninconspicuous. Variable pulmonary opacifications and pleural effusions were noted in false negative and false \npositive predictions. Additional results can be found in Supplementary Fig. S1.\nMeTra can be trained on multimodal data. When trained on both chest radiographs and clinical \nparameters, MeTra reached an AUROC value of 0.863 [0.835, 0.889], which was superior to both unimodal \ntraining settings (p < 0.001). Similarly, specificity (0.861 [0.841, 0.880], p < 0.001) and positive predictive \nvalue (0.486 [0.432, 0.541], p < 0.001) were significantly higher after multimodal training than after unimodal \ntraining (Fig.  2). Sensitivity was higher, too, yet not statistically significant (0.732 [0.670, 0.792], vs. \n unimodal(chest radiographs only) = 0.33, vs.  unimodal(clinical parameters only) = 0.26).\nMeTra can deal with missing data. The MeTra model can deal with missing data. However, like a physician \nwith less data, MeTra’s predictions become less accurate when the number of available clinical parameters is \nreduced. For AUROC and the positive predictive values, a close-to-linear decrease is demonstrated as a function \n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:10666  | https://doi.org/10.1038/s41598-023-37835-1\nwww.nature.com/scientificreports/\nof reduced parameter availability (Fig.  4). Intentionally, we included the clinical parameters Glasgow Coma \nScale (total) and capillary refill rate even though their content was empty for all the test samples. The upheld \nperformance demonstrates robustness to the fact that labels might be missing a priori.\nFigure 1.  Visualization of the data extraction pipeline. For training, we only make use of those patients who \nwere admitted to the Intensive Care Unit (n = 53,150) and who had clinical data (clinical parameters—CP) with \nmatching chest radiographs available (n = 6,125). The data is split into the training (n = 4,396 patients), validation \n(n = 472 patients), and test sets (n = 1,257 patients).\nTable 1.  Characteristics of the dataset. Data in parentheses are percentages. Note that the number \n(percentage) of deaths during the hospital stay for a given set is relative to the number of patients in each set.\nParameter All patients Training set Validation set Test set\nNo. of paired chest radiographs and clinical parameters 6798 4885 540 1373\nNo. of patients 6125 4396 472 1257\nNo. of male patients 3382 (55%) 2414 (55%) 264 (56%) 704 (56%)\nAge (years) (range) 18—91 18—91 19—91 18—91\nMean age ± SD [Median] (years) 64 ± 16 [65] 64 ± 16 [65] 64 ± 16 [65] 64 ± 16 [65]\nNo. of deaths during hospital stay 1002 (16%) 717 (16%) 76 (16%) 209 (17%)\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:10666  | https://doi.org/10.1038/s41598-023-37835-1\nwww.nature.com/scientificreports/\nDiscussion\nIn this work, we developed and evaluated the medical transformer architecture MeTra to integrate imaging \nand non-imaging data for survival predictions in patients in critical care. While MeTra can predict the survival \nof critically ill patients when trained on clinical data or imaging data exclusively, the model can combine both \ndata sources for improved model predictions. We also demonstrate that MeTra can deal with missing data \nand that there is a smooth transition between high diagnostic accuracy when all data is available to reduced \ndiagnostic accuracy when data are missing. Consequently, MeTra may be considered a blueprint for how to \nutilize multimodal medical data in AI models.\nOther groups have worked on survival prediction without transformer architectures and only achieved \ncomparable performance when training on considerably more data and using extensive hyperparameter tuning \n(Table 3). The present study is the first to investigate the performance of a fully transformer-based architecture \nin the survival prediction of patients in intensive care and proves its viability when handling imaging and non-\nimaging data. However, alternative transformer-based approaches have been introduced to the medical domain. \nZheng et al. used the attention mechanism of transformers in combination with a graph-based method to model \npatient relations and utilize modality-specific  data22. Our study distinguishes itself by eliminating the need for \nmore complex fusion mechanisms. Song et al. used transformers to combine optical coherence tomography \nimages and visual field exams to diagnose  glaucoma23. The data had to be presented in matrix view, which \nallowed the authors to tailor their architecture to the available format. The authors also resorted to a CNN for \nfeature extraction prior to employing the transformer for modality fusion. This approach seems unsuitable for \nour clinical question that aims to combine non-imaging data, such as laboratory values (typically not available \nin matrix view), with imaging data. Moreover, using an additional CNN does not align with our objective of \nimplementing a purely transformer-based model. Nguyen et al. introduced the CLIMAT (Clinically-Inspired \nMulti-Agent Transformers) model as a fully-transformer-based model for predicting the progression of knee \nosteoarthritis using imaging and non-imaging  data24. The authors used three distinct transformer modules to (i) \nextract features from imaging data, (ii) extract features from non-imaging data, and (iii) combine the extracted \nfeatures to provide a set of output predictions, where each corresponds to the disease severity at a specific point in \ntime. While conceptionally, the authors followed a similar approach in using transformer blocks exclusively, the \ndifferent clinical question necessitates architectural distinctions. In the CLIMAT model, multiple class tokens are \nadded to the last transformer module to extract predictions for multiple time steps. Furthermore, a compressed \nrepresentation of the non-imaging features is used and concatenated to each output token of the imaging-specific \ntransformer module before the tokens are fed to the final transformer module. In contrast, we intentionally did \nnot compile the non-imaging data before the multimodal data fusion to ensure that all information is visible to \nthe model. Moreover, to make sure that each imaging token attends to all non-imaging tokens and vice versa, \nwe feed the joint set of features as tokens through the last transformer module.\nBeyond, our work is clinically and scientifically relevant in several aspects:\nFirst, our clinical experience teaches us that any predictive model used clinically must deal with missing \ndata. Not all patients are treated and diagnosed equally, and the diagnostic toolset—from imaging to laboratory \nstudies to clinical tests—is not consistently applied to all patients. The resultant data inconsistency and scarcity \nare problems for conventional machine learning models since the number of patients with “complete” datasets \nfor training is inherently limited. MeTra solves this problem as it can both be trained  on incomplete data and \ncan also deal with missing data during inference.\nTable 2.  Overview of the clinical parameters used in conjunction with the chest radiographs. The column \n“Missing (%)” denotes the percentage of samples in the dataset that did not have any entry for this item.\nVariable Type Missing (%) Mean (± std) Impute value\nCapillary refill rate Categorical 100 – –\nDiastolic blood pressure Continuous 0.04 59.04 (8.87) mmHg 59.0 mmHg\nFraction inspired oxygen Continuous 26 0.45 (0.07) FiO2 0.21 FiO2\nGlasgow coma scale—eye opening Categorical 0 3.51 (0.66) 4\nGlasgow coma scale—motor response Categorical 0 5.13 (1.52) 6\nGlasgow coma scale—verbal response Categorical 0 4.35 (1.16) 5\nGlasgow coma scale—total Categorical 100 – –\nGlucose Continuous 0.02 128.98 (28.72) mg/dL 128.0 mg/dL\nHeart rate Continuous 0 85.15 (12.96) bpm 86 bpm\nBody height Continuous 97.7 169.77 (9.00) cm 170.0 cm\nMean blood pressure Continuous 0 74.15 (8.99) mmHg 77.0 mmHg\nOxygen saturation Continuous 0 97.69 (1.98) % 98.0%\nRespiratory rate Continuous 0 18.95 (3.73) breaths per minute 19 breaths per minute\nSystolic blood pressure Continuous 0.04 113.87 (14.21) mmHg 118.0 mmHg\nTemperature Continuous 2.28 36.90 (0.32) °C 36.6 °C\nBody weight Continuous 9.13 79.73 (15.05) kg 81.0 kg\npH Continuous 13.86 7.37 (0.06) 7.4\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:10666  | https://doi.org/10.1038/s41598-023-37835-1\nwww.nature.com/scientificreports/\nSecond, medical diagnosis is based on data from various sources: Medical doctors assess radiographs in \nconjunction with laboratory values, clinical tests, and history findings, among others. Developing machine \nlearning models that rival human expertise will eventually require including data from all these sources. MeTra \nsuggests one possible path forward by providing an architecture encompassing data from any source. Flexible \ndata integration into the model is a beneficial feature of the transformer architecture that contrasts with other \nstate-of-the-art network architectures such as CNNs. CNNs are specifically designed to work well on images and \n-even though possible- including non-imaging data remains  challenging25,26.\nThird, an improved survival prediction in intensive care can help assess illness severity and direct intensive \ncare where needed to save lives and improve  outcomes3. As detailed above, MeTra achieves state-of-the-art \nFigure 2.  Detailed performance metrics of the Medical Transformer (MeTra). MeTra was trained on the \nclinical parameters only (CP), on the chest radiographs only (CXR), and the combined multimodal data \n(CP + CXR). Receiver operating characteristic (ROC) curves (a) and areas under the ROC curves (b). To \ndetermine discrimination thresholds, the operating point was determined by maximizing Y ouden’s criterion \n(sensitivity + specificity), resulting in specific values for the positive predictive value (c), sensitivity (d), and \nspecificity (e). The combined model performed superior to the uni-modal models for every metric.\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:10666  | https://doi.org/10.1038/s41598-023-37835-1\nwww.nature.com/scientificreports/\nperformance in this task. It may support physicians in clinical decision-making once clinical applicability beyond \nthis proof-of-concept study has been demonstrated. We make the trained model open-source to facilitate future \ntranslational research efforts. For full transparency and comparability, we used the identical training test splits \nas  others14, and this information is published with the MeTra model itself.\nPrevious research has utilized ensembles of conventional machine learning  algorithms3, CNNs in conjunction \nwith attention  mechanisms27, or recurrent neural  networks14 to predict patient survival. By comparison, the \ntransformer architecture employed in MeTra has several advantages: It employs the same backbone architecture \nas the Vision  Transformer12 and upholds its advantages in incorporating global information at shallow layers \nwhile being more robust to adversarial attacks than  CNNs28.\nOur work has limitations: first, the survival prediction and validation data originate from a single center due \nto the unique availability of imaging and non-imaging data alongside survival data. Consequently, no external \nvalidation was performed, and the model’s generalizability remains to be confirmed using multimodal datasets \nFigure 3.  Exemplary chest radiographs and associated patient survival predictions. The upper row shows \nchest radiographs of four patients dismissed from the ICU alive. The lower row shows chest radiographs of four \npatients who died in intensive care. Predictions of the model were correct or incorrect depending on \nwhether all data, i.e., imaging and non-imaging data (“CP + CXR”) was provided, or whether only the imaging \ndata (“Only CXR”) or only the clinical parameters (“Only CP”) were provided. Please refer to Fig. 2 for an \nexplanation of the abbreviations.\nFigure 4.  Performance of MeTra in terms of the AUROC values (a) and the positive predictive values (b) as a \nfunction of the number of clinical parameters available to the model. The x-axis denotes the number of clinical \nparameters fed into the model alongside the chest radiograph. For each number of clinical parameters, the \nexperiment was repeated 100 times with randomly chosen subsets of variables to prevent bias due to the choice \nof variables.\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:10666  | https://doi.org/10.1038/s41598-023-37835-1\nwww.nature.com/scientificreports/\nfrom other institutions and through other researchers. However, we hope our work stimulates collective efforts \nto assemble comparable large-scale databases. Perspectively, collective work on transformer models may be \naccelerated further by decentralized peer-to-peer collaborations, for example, using a swarm learning  approach29. \nSecond, we only included relatively basic physiologic measures used for patient monitoring, while more complex \nmeasures of hemodynamics, oxygen metabolism, and microcirculation were not considered. Third, because the \nnumber of deaths in the ICU was unbalanced, the resultant class imbalance is an issue that needs consideration. \nFuture work may address the class imbalance during training, for example, by including a weight factor into the \nloss function (accounting for the class imbalance) or by oversampling the underrepresented  class30. Additionally, \na hybrid approach of transformer layers and a CNN backbone may be used to further improve the  performance31. \nA more comprehensive analysis of hyperparameter choices could also be performed, e.g., the choice of vision \ndropout. Future studies should investigate the association between specific vision dropout settings and model \nperformance. Fourth, the clinical dataset had missing data, and any imputation may introduce bias, increase \nthe variability of the model’s performance, and affect the results. On scientific grounds, we intentionally used \nthe same (inconsistent) impute values as other groups to compare our MeTra model to their models. A more \nsystematic approach would benefit and result in more robust models. On clinical grounds, a thorough analysis \nof the model’s performance regarding missing and spurious data is required before deployment and use in \nthe clinic. Specifically, excluding clinical parameter values by zero-tokens may lead to distribution shifts and \nimpaired prediction performance. While we account for the distribution shifts through dropout layers in the \nmodel architecture of MeTra, future work should explore alternative methods to exclude zero-masked tokens \nfrom the input (for example, as introduced by He et al.32). Adopting their approach would involve masking out \nmissing clinical events at specific time points that are fed into the model individually. However, the computational \nburden caused by the quadratic scaling and associated memory requirements should be considered. Fifth, when \ninterpreting our results in the context of the pertinent literature, it is essential to realize that the referenced \nresults of other groups’ models only indicate the range of potential outcomes. A more thorough comparison \nwould require strict standardization of all aspects, i.e., the models would have to be trained on the same data, \nand the data processing pipeline would have to be identical with a fixed random seed for augmentations. Sixth, \nanother limitation relates to the variable time difference between imaging and non-imaging data. The non-\nimaging (clinical) data were collected during the first 48 h after a patient had been admitted to the ICU. In \ncontrast, the last chest radiograph acquired during a patient’s ICU stay was included as the (paired) imaging \n data14. In the patient subpopulation of the MIMIC dataset that was included in our study (for whom clinical \nTable 3.  Comparison of MeTra to current state-of-the-art methods for survival prediction in patients in \nintensive care in terms of area under the receiver operating characteristic curve (AUROC) and the area under \nthe precision recall curve (AUPRC). Means [95% confidence intervals].\nMethod AUROC AUPRC Comments\nEarly14 0.827 [0.801, 0.854] 0.485 [0.417, 0.555]\nClinical parameters (CP) and imaging data are first pre-trained separately. Subsequently, the latent \nrepresentation of both modalities is concatenated, and a final classification layer is trained to merge the \ninputs\nJoint14 0.825 [0.798, 0.853] 0.506 [0.436, 0.574]\nCP and imaging data are fed through separate feature extraction layers and then concatenated and fed \nthrough a final classification head to form the final prediction. Training is performed in an end-to-end \nsetting\nMMTM14,42 0.819 [0.788, 0.846] 0.474 [0.402, 0.544] A Multimodal Transfer  Module14,42 (MMTM) is added after feature extraction of both modalities to merge \nthe inputs\nDAFT14,16 0.828 [0.799, 0.854] 0.492 [0.427, 0.572] A Dynamic Affine Feature Map  Transform14,16 (DAFT) is used after feature extraction of both modalities to \nscale and shift the resulting feature maps to merge the modalities\nUnified14,15 0.835 [0.808, 0.861] 0.495 [0.424, 0.567]\nIn every training iteration, a two-step approach is performed. First, feature extractors for the CP and \nimaging data (which do not necessarily have to be paired) are trained separately to extract meaningful \nfeatures. Second, the previously learned feature extractors extract features for a set of paired samples, which \nare then concatenated and fed through a learnable classification head\nMedFuse (PT)14 0.841 [0.813, 0.868] 0.544 [0.477, 0.609]\nFor CP and imaging data, separate feature extractors are learned on modality-specific labels. The final \nprediction is then formed by feeding both feature representations sequentially into a neural network of \nLSTM (Long Short-Term Memory) layers \nThis AUROC cannot directly be compared to our method: The configuration of MedFuse(PT) uses \nconsiderably more imaging data (pre-training on 340,470 additional radiographs) and more CP (22,356 \nsamples) as compared to MeTra (6,798 samples for both CP and imaging data)\nMedFuse (OPTIMAL)14 0.865 [0.837, 0.889] 0.594 [0.526, 0.655]\nFor CP and imaging data, separate feature extractors are learned on modality-specific labels. The final \nprediction is then formed by feeding both feature representations sequentially into a neural network of \nLSTM layers \nThis AUROC cannot directly be compared to our method: MedFuse(OPTIMAL) uses the same additional \nimaging data as MedFuse(PT) and performs extensive selection on the CP data based on 22,356 samples\nMedFuse (RI)14 0.817 [0.785, 0.846] 0.471 [0.404, 0.545]\nFor CP and imaging data, separate feature extractors are learned on modality-specific labels. The final \nprediction is then formed by feeding both feature representations sequentially into a neural network of \nLSTM layers\nThis AUROC cannot directly be compared to our method: MedFuse (RI) is not pre-trained on additional \nimaging data (as PT or OPTIMAL) but still uses more CP data (22,356 samples) as compared to MeTra \n(6,798 samples)\nMeTra (CP + CXR) 0.863 [0.835, 0.889] 0.594 [0.526, 0.662]\nMeTra is based on the transformer model, where data is processed as a set of tokens. The CP and imaging \ndata are fed through corresponding transformer-based backbones to extract latent feature tokens merged in \na final transformer encoder\nMeTra is trained on fewer data than MedFuse OPTIMAL, PT, and RI\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:10666  | https://doi.org/10.1038/s41598-023-37835-1\nwww.nature.com/scientificreports/\nparameters and chest radiographs were available), patients had an average ICU stay length of 5.4 ± 4.9 d (range \n1.1–99.6 d [n = 6125 patients]). In our clinical experience, ICU stay lengths are affected by admission diagnosis, \npatient demographics, constitution, comorbidities, complications, type of treatment, and others, which affect \nthe variability of associated clinical parameters. Consequently, the substantial time difference outlined above is \nworth considering when drawing clinical conclusions. For any meaningful clinical insights, more specific clinical \nquestions need to be asked, more refined patient populations need to be studied, and more fine-granular analyses \nneed to be conducted. In addition, mortality may be determined by a range of conditions with limited bearings on \nthe chest radiograph, which is inherently limited in differentiating pathologic processes characterized by similar \nradiographic changes, e.g., pulmonary  opacifications33. In the clinic, the availability of clinical parameters aids \nin interpreting equivocal findings on chest radiographs and vice versa. Therefore, our findings of significantly \nimproved survival predictions based on imaging and non-imaging data become clinically plausible, yet the real \nclinical benefit remains to be determined.\nIn conclusion, we developed and validated a multimodal medical transformer model that can be easily \ntrained without specifically tweaking the architecture for specific input modalities and exhibits robustness to \nmissing and heterogeneous data. We achieved excellent performance in the survival prediction of patients in \ncritical care. We also make our model an open source for clinicians and researchers as a benchmark model on \na well-defined dataset.\nOnline methods\nStudy design. Following approval by the local ethical committee (Reference No. 028/19), this retrospective \nstudy followed local data protection regulations. All networks were trained on publicly available datasets \ndescribed below and tested for their performance in predicting the survival of patients in intensive care.\nDescription of dataset. The MIMIC-IV (Medical Information Mart for Intensive Care) dataset is a large \nUS database of retrospectively collected data from two in-hospital database systems: a custom hospital-wide \nEHR and an ICU-specific clinical information system. The MIMIC-IV dataset contains EHR data and is linked \nto the MIMIC-chest-X ray (MIMIC-CXR) database, which provides the corresponding imaging data of the same \n patients21,34. All data is publicly available via  physionet35. For full transparency and optimal comparability, we \nhave used the same training test splits as other  groups14, and we publish this split alongside the model. Table  1 \nprovides a detailed description of the dataset.\nData preprocessing. The imaging and non-imaging data were extracted from the MIMIC database \nand preprocessed as described by Hayat et al. 14 (Fig.  1). In detail, a subset of the MIMIC data was compiled, \ncontaining millions of clinical events corresponding to 17 clinical parameters (Table  2). Of these, the capillary \nrefill rate and Glasgow Coma Scale (total) were missing for all patients and, thus, disregarded from our analysis, \nleaving 15 clinical parameters to be included in the model. The chest radiographs (obtained as anterior–posterior \nprojections) from the MIMIC-CXR database were extracted and matched to the EHR data. The chest radiographs \nwere first normalized to match the dataset statistics of  ImageNet36 (in terms of means and standard deviations) \nand resized to a resolution of 384 × 384 to use pre-trained models (see below). Data were split into training \n(72%), validation (8%), and test (20%) data using patient-wise stratification but otherwise random allocation.\nThe multimodal medical transformer architecture. Building on the transformer architecture \nproposed by Vaswani et al.13, which was subsequently extended for use in vision  problems12, we designed our \nmedical transformer model to provide a direct way to incorporate imaging and non-imaging data into the \nlearning process. Principally, as data inside transformer models is processed in tokens, there are no restrictions \nfor its application on other modalities. More precisely, MeTra takes input data from two different modalities. \nChest radiographs xC×R ∈ RH×W  of image height H and width W were first processed by a vision backbone \nto extract high-level image features zC×R ∈ RN ×D  that could be fused with the data of other modalities later. \nHere, N denotes the number of tokens and D denotes the dimensionality of the latent representation for each \ntoken. Any vision transformer model can be used for this task, thus allowing us to leverage models pre-trained \non different datasets. In particular, MeTra uses a Vision Transformer (ViT)12 with a patch size of 16 that has been \npre-trained on ImageNet without the final classification head as its backbone. Additionally, clinical parameters \nretrieved from the EHRs xCP ∈ RK ×T   are projected into the latent representation zCP ∈ RM ×D  using a linear \nlayer to match the dimensionality D of the image tokens. Here, K denotes the number of EHR items and T denotes \nthe number of recorded time steps for each item. We set T to 48 in all experiments, representing the values of the \nrespective item for each hour within the first 48 h of patient admission to the ICU. A missing value is imputed \nby setting it to the most recent measurement value if available or by setting it to a pre-specified value (Table 2) as \nsuggested by Harutyunyan et al.37. To fuse imaging and non-imaging data efficiently, the latent representations \nof both backbones are concatenated to form the latent representation zMULTI ∈R(N +M )×D . The self-attention \nmechanism used inside transformers to process the input sequence does not consider the order of the elements \nin the sequence. To address this issue, we define a set of N + M learnable tokens of dimension D that are added \nelement-wise to the latent representation zMULTI  . Subsequently, a learnable class token CLS is prepended to \nzMULTI  , and the resulting multimodal representation is processed with a transformer encoder, where the multi-\nhead self-attention  layers13 allow cross-modality information transfers. A multi-layer perceptron with a Sigmoid \nactivation function is applied to the output to form the final prediction pSURVIVAL  that quantifies the likelihood \nof in-hospital survival of the patient. The MeTra architecture is visualized in Fig. 5.\nWe trained three variants to compare the different modalities’ influence on the models’ final performance. \nThe model only using the clinical parameters as retrieved from the EHR (“clinical parameters only-model”) was \n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:10666  | https://doi.org/10.1038/s41598-023-37835-1\nwww.nature.com/scientificreports/\nrestricted to this source of data by setting the pixel values of the corresponding chest radiograph xC×R to zero. \nSimilarly, for the corresponding model that only used the chest radiographs for predictions (“radiographs-only \nmodel”), the clinical parameters xCP were set to zero. Finally, the combined model was trained by resuming \nthe training routine from the checkpoint of the clinical parameters only-model with the highest area under \nthe receiver operating characteristic curve (AUROC) value on the validation set (which is different from the \ntest set). Motivated by preliminary findings [not shown] that indicated severe disbalance in the model’s focus \nand substantial disregard of the non-imaging data when trained on imaging and non-imaging data at once, we \nmodified the training strategy of the combined model as follows: The imaging information was excluded during \ninitial training and only provided (alongside the non-imaging information) during the subsequent training \nsteps. Consequently, the combined model uses a similar setting as the unimodal models, i.e., starting from the \nsame initial random states, but applying a full dropout of the imaging information during the initial epochs of \ntraining. No further restrictions on the available data were made; therefore, all information present in xC×R  and \nxCP  were used. To further prevent the multimodal transformer encoder from relying exclusively on information \noriginating from the vision backbone, all pixels in xC×R were randomly set to zero with probability  pVDO (chosen \nto be 30% and based on preliminary studies). We coined this procedure vision dropout.\nThe training was performed on an NVIDIA Quadro RTX 6000 for 200 epochs to guarantee the convergence \nof each model. As the learning objective, we minimized the binary cross-entropy loss:\nwhere y ∈{ 0, 1} represents the ground truth value for survival. 1 denotes that the patient died during the hospital \nstay, and 0 denotes that the patient was discharged alive. We used the  AdamW38 optimizer with a learning rate of \n5e − 6, which was decreased over time using the cosine annealing  procedure39 until a final learning rate of 1e − 7 \nwas reached. The entire code was written using Python v3.8, and MeTra was implemented using PyTorch v1.11.0. \nFor more information regarding our training procedure, please refer to Supplementary Table S1.\nDescription of experiments. In the first experiment, the model was trained only on the clinical parameters \nand subsequently evaluated with these data as exclusive input.\nLBCE = y ·log(pSURVIVAL ) + (1 − y) ·log(1 − pSURVIVAL ),\nFigure 5.  Medical Transformer (MeTra) architecture. The chest radiograph is first processed in the vision \nbackbone, where it is split into patch embeddings and subsequently fed through a transformer encoder. \nSimilarly, the clinical parameter items are fed through the clinical backbone, where they are projected to an \nembedding space with a dimensionality that matches that of the image embeddings. In the next step, a learnable \nposition encoding token is added to the embeddings of both modalities. Finally, the modalities are fused by \nprocessing the embeddings with a transformer encoder that applies multi-head self-attention to all input tokens, \nthus allowing cross-modality information transfer. A multilayer-perceptron (MLP) is applied to the output to \nform the final prediction for in-hospital survival.\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:10666  | https://doi.org/10.1038/s41598-023-37835-1\nwww.nature.com/scientificreports/\nIn the second experiment, the model was trained only on the imaging data and evaluated with these data only.\nIn the third experiment, the model was trained on all data and evaluated using all data.\nThe combined model (third experiment) was provided with the full imaging data set but only parts of \nthe clinical parameters as input to study how missing data impact its performance. In detail, this experiment \nwas repeated 100 times with 2, 4, 6, 8, 10, 12, and 14 clinical parameters set to “missing” each time. Missing \nparameters were chosen randomly within each of the 100 runs to prevent bias in choosing variables.\nWe evaluated the AUROC, AUPRC, sensitivity, specificity, and positive predictive value for all experiments.\nStatistical analysis. Statistical analyses were conducted using Python v3.8 with its libraries NumPy  and \nSciPy. Bootstrapping was employed with 10,000 redraws for each measure to determine the statistical spread and \ncalculate p-values for  differences40. For calculating sensitivity and specificity, a threshold was chosen according \nto Y ouden’s  criterion41, i.e., a threshold that maximized (sensitivity + specificity). We included all patients for \nwhich both radiographs and clinical parameters were available.\nData availability\nAll data, including imaging and non-imaging data, is publicly available from the MIMIC  database 21,34 on \nPhysioNet (for MIMIC-IV , see https:// physi onet. org/ conte nt/ mimic iv/1. 0/. and for MIMIC-CXR-JPG, see https:// \nphysi onet. org/ conte nt/ mimic- cxr- jpg/2. 0.0/). The code to extract the chest radiographs and corresponding \nclinical parameters can be found in the GitHub repository linked in the code availability section.\nCode availability\nThe entire code is publicly available on GitHub via https:// github. com/ Firas Git/ MeTra. We also provide detailed \ninformation on all data splits into training and testing for other groups to compare their algorithms with ours.\nReceived: 10 January 2023; Accepted: 28 June 2023\nReferences\n 1. Halpern, N. A. & Pastores, S. M. Critical care medicine in the United States 2000–2005: An analysis of bed numbers, occupancy \nrates, payer mix, and costs. Crit. Care Med. 38, 65–71 (2010).\n 2. Syed, M. et al. Application of machine learning in intensive care unit (ICU) settings using MIMIC dataset: systematic review. \nInformatics (MDPI) 8, 16 (2021).\n 3. Pirracchio, R. et al. Mortality prediction in intensive care units with the Super ICU Learner Algorithm (SICULA): A population-\nbased study. Lancet Respir. Med. 3, 42–52 (2015).\n 4. Hoogendoorn, M., el Hassouni, A., Mok, K., Ghassemi, M. & Szolovits, P . Prediction using patient comparison vs. modeling: A \ncase study for mortality prediction. in 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology \nSociety (EMBC) 2464–2467 (2016).\n 5. Awad, A., Bader-El-Den, M., McNicholas, J. & Briggs, J. Early hospital mortality prediction of intensive care unit patients using \nan ensemble learning approach. Int. J. Med. Informatics 108, 185–195 (2017).\n 6. Weissman, G. E. et al. Inclusion of unstructured clinical text improves early prediction of death or prolonged ICU stay. Crit. Care \nMed. 46, 1125–1132 (2018).\n 7. Irvin, J. et al. CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison. Proc. AAAI Conf. Artif. \nIntell. 33, 590–597 (2019).\n 8. Y adav, S. S. & Jadhav, S. M. Deep convolutional neural network based medical image classification for disease diagnosis. J. Big Data \n6, 113 (2019).\n 9. Bressem, K. K. et al. Comparing different deep learning architectures for classification of chest radiographs. Sci. Rep. 10, 13590 \n(2020).\n 10. Khader, F . et al. Artificial Intelligence for Clinical Interpretation of Bedside Chest Radiographs. Radiology  220510 (2022).\n 11. Spiritoso, R., Padley, S. & Singh, S. Chest X-ray interpretation in UK intensive care units: A survey 2014. J. Intens. Care Soc. 16, \n339–344 (2015).\n 12. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. Preprint at http:// arxiv. org/ abs/ \n2010. 11929 (2021).\n 13. Vaswani, A. et al. Attention is all you need. in Advances in Neural Information Processing Systems vol. 30 5998–6008 (Curran \nAssociates, Inc., 2017).\n 14. Hayat, N., Geras, K. J. & Shamout, F . E. Multi-modal fusion with clinical time-series data and chest X-ray images. Preprint at http:// \narxiv. org/ abs/ 2207. 07027 (2022).\n 15. Hayat, N., Geras, K. J. & Shamout, F . E. Towards dynamic multi-modal phenotyping using chest radiographs and physiological \ndata. http:// arxiv. org/ abs/ 2111. 02710 (2021).\n 16. Pölsterl, S., Wolf, T. N. & Wachinger, C. Combining 3D image and tabular data via the dynamic affine feature map transform. in \nMedical Image Computing and Computer Assisted Intervention—MICCAI 2021 688–698 (2021).\n 17. Sharma, A., Vans, E., Shigemizu, D., Boroevich, K. A. & Tsunoda, T. DeepInsight: A methodology to transform a non-image data \nto an image for convolution neural network architecture. Sci. Rep. 9, 11399 (2019).\n 18. Pascanu, R., Mikolov, T. & Bengio, Y . On the difficulty of training recurrent neural networks. in Proceedings of the 30th International \nConference on Machine Learning 1310–1318 (PMLR, 2013).\n 19. Dixon, S. Diagnostic Imaging Dataset 2020–21 Data. NHS England, UK, Tech. Rep (2021).\n 20. Mettler, F . A. et al. Radiologic and nuclear medicine studies in the United States and worldwide: Frequency, radiation dose, and \ncomparison with other radiation sources—1950–2007. Radiology 253, 520–531 (2009).\n 21. Johnson, A., et al. MIMIC-IV . 10.13026/S6N6-XD98.\n 22. Zheng, S. et al. Multi-modal graph learning for disease prediction. IEEE Trans. Med. Imaging 41, 2207–2216 (2022).\n 23. Song, D. et al. Deep relation transformer for diagnosing glaucoma with optical coherence tomography and visual field function. \nIEEE Trans. Med. Imaging 40, 2392–2402 (2021).\n 24. Nguyen, H. H., Saarakkala, S., Blaschko, M. B. & Tiulpin, A. CLIMAT: Clinically-Inspired Multi-Agent Transformers for Knee \nOsteoarthritis Trajectory Forecasting. in 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI) 1–5 (2022).\n 25. Gao, J., Li, P ., Chen, Z. & Zhang, J. A survey on deep learning for multimodal data fusion. Neural Comput. 32, 829–864 (2020).\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:10666  | https://doi.org/10.1038/s41598-023-37835-1\nwww.nature.com/scientificreports/\n 26. Huang, S.-C., Pareek, A., Zamanian, R., Banerjee, I. & Lungren, M. P . Multimodal fusion with deep neural networks for leveraging \nCT imaging and electronic health record: A case-study in pulmonary embolism detection. Sci. Rep.  10, 22147 (2020).\n 27. Schulz, S. et al. Multimodal deep learning for prognosis prediction in renal cancer. Front. Oncol. 11, 788740 (2021).\n 28. Laleh, N. G. et al. Adversarial attacks and adversarial robustness in computational pathology. 2022.03.15.484515. https:// doi. org/ \n10. 1101/ 2022. 03. 15. 48451 5v1 (2022).\n 29. Saldanha, O. L. et al. Swarm learning for decentralized artificial intelligence in cancer histopathology. Nat. Med. 28, 1232–1239 \n(2022).\n 30. Lin, T.-Y ., Goyal, P ., Girshick, R., He, K. & Dollar, P . Focal loss for dense object detection. in 2980–2988 (2017).\n 31. Tu, Z. et al. MaxViT: Multi-axis vision transformer. in Computer Vision – ECCV 2022 (eds. Avidan, S., Brostow, G., Cissé, M., \nFarinella, G. M. & Hassner, T.) 459–479 (Springer Nature Switzerland, 2022).\n 32. He, K. et al. Masked autoencoders are scalable vision learners. arXiv: 2111. 06377 [cs] (2021).\n 33. Self, W . H., Courtney, D. M., McNaughton, C. D., Wunderink, R. G. & Kline, J. A. High discordance of chest x-ray and computed \ntomography for detection of pulmonary opacities in ED patients: implications for diagnosing pneumonia. Am. J. Emerg. Med. 31, \n401–405 (2013).\n 34. Johnson, A. E. W . et al. MIMIC-CXR-JPG, a Large publicly available database of labelled chest radiographs. Preprint at http://  \narxiv. org/ abs/ 1901. 07042 (2019).\n 35. Goldberger, A. L. et al. PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic \nsignals. Circulation 101, E215-220 (2000).\n 36. Deng, J. et al. ImageNet: A large-scale hierarchical image database. in 2009 IEEE Conference on Computer Vision and Pattern \nRecognition 248–255 (2009).\n 37. Harutyunyan, H., Khachatrian, H., Kale, D. C., Ver Steeg, G. & Galstyan, A. Multitask learning and benchmarking with clinical \ntime series data. Sci. Data 6, 96 (2019).\n 38. Loshchilov, I. & Hutter, F . Decoupled weight decay regularization. Preprint at http:// arxiv. org/ abs/ 1711. 05101 (2019).\n 39. Loshchilov, I. & Hutter, F . SGDR: Stochastic gradient descent with warm restarts. Preprint at http:// arxiv. org/ abs/ 1608. 03983 \n(2017).\n 40. Konietschke, F . & Pauly, M. Bootstrapping and permuting paired t-test type statistics. Stat. Comput. 24, 283–296 (2014).\n 41. Unal, I. Defining an optimal cut-point value in ROC analysis: An alternative approach. Comput. Math. Methods Med. 2017, 3762651 \n(2017).\n 42. Joze, H. R. V ., Shaban, A., Iuzzolino, M. L. & Koishida, K. MMTM: Multimodal transfer module for CNN fusion. in Proceedings \nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition 13289–13299 (2020).\nAcknowledgements\nThe authors are grateful for the support from NVIDIA, who kindly provided an RTX6000 GPU. We thank the \nMIMIC consortium for providing the data used for analysis in our study.\nAuthor contributions\nThe experiment was designed by F .K., S.N., and D.T. The model architecture was set up by F .K., J.N.K. and D.T. \nThe code was written by F .K. Statistical analyses were performed by F .K., S.N., and D.T. All authors contributed to \nthe interpretation of the results and the writing of the final manuscript, and all authors agreed to the submission \nof this paper.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL. JNK is supported by the German Federal Ministry \nof Health (DEEP LIVER, ZMVI1-2520DAT111) and the Max-Eder-Programme of the German Cancer Aid \n(grant #70113864). SN is supported by the Deutsche Forschungsgemeinschaft (DFG) (No. NE 2136/3-1) and \nthe START Program of the Faculty of Medicine, RWTH Aachen, Germany. DT is supported by grants from the \nDeutsche Forschungsgemeinschaft (DFG) (TR 1700/7-1). STA is supported (partially) by the RACOON network \nunder BMBF grant number 01KX2021.\nCompeting interests \nThe authors declare no competing interests. For transparency, we provide the following information: JNK declares \nconsulting services for Owkin, France and Panakeia, UK. DT declares consulting services for Nano4Imaging, \nGermany and Ocumeda, Switzerland. KB declares speaker fees for Canon Medical Systems Cooperation, Japan.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 023- 37835-1.\nCorrespondence and requests for materials should be addressed to F .K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Medical imaging",
  "concepts": [
    {
      "name": "Medical imaging",
      "score": 0.5815207958221436
    },
    {
      "name": "Intensive care",
      "score": 0.5316634178161621
    },
    {
      "name": "Computer science",
      "score": 0.4982140064239502
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3671867251396179
    },
    {
      "name": "Data mining",
      "score": 0.35409361124038696
    },
    {
      "name": "Medical physics",
      "score": 0.33210182189941406
    },
    {
      "name": "Medicine",
      "score": 0.3185340166091919
    },
    {
      "name": "Intensive care medicine",
      "score": 0.20677343010902405
    }
  ]
}