{
  "title": "Efficient Training of Visual Transformers with Small-Size Datasets",
  "url": "https://openalex.org/W3177143648",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5100375513",
      "name": "Yahui Liu",
      "affiliations": [
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A5024183744",
      "name": "Enver Sangineto",
      "affiliations": [
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A5101671512",
      "name": "Wei Bi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5027171279",
      "name": "Nicu Sebe",
      "affiliations": [
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A5048877432",
      "name": "Bruno Lepri",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5081236916",
      "name": "Marco De Nadai",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2886335102",
    "https://openalex.org/W3095121901",
    "https://openalex.org/W3087549734",
    "https://openalex.org/W3202591369",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W3134652006",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2750549109",
    "https://openalex.org/W2883725317",
    "https://openalex.org/W3171007011",
    "https://openalex.org/W3110402800",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W343636949",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W2981720610",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3204575409",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3118062200",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2962877362",
    "https://openalex.org/W2894651257",
    "https://openalex.org/W3195830874",
    "https://openalex.org/W3108655343",
    "https://openalex.org/W3168405954",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2944223741",
    "https://openalex.org/W2935908327",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2937170809",
    "https://openalex.org/W3202163745",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3158424761",
    "https://openalex.org/W2335728318",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2962742544",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3165924482",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3167002466",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3163602117",
    "https://openalex.org/W3101821705",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W2887997457",
    "https://openalex.org/W3110446398",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2990500698",
    "https://openalex.org/W2931316642",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W3034781633",
    "https://openalex.org/W2995181141"
  ],
  "abstract": "Visual Transformers (VTs) are emerging as an architectural paradigm alternative to Convolutional networks (CNNs). Differently from CNNs, VTs can capture global relations between image elements and they potentially have a larger representation capacity. However, the lack of the typical convolutional inductive bias makes these models more data-hungry than common CNNs. In fact, some local properties of the visual domain which are embedded in the CNN architectural design, in VTs should be learned from samples. In this paper, we empirically analyse different VTs, comparing their robustness in a small training-set regime, and we show that, despite having a comparable accuracy when trained on ImageNet, their performance on smaller datasets can be largely different. Moreover, we propose a self-supervised task which can extract additional information from images with only a negligible computational overhead. This task encourages the VTs to learn spatial relations within an image and makes the VT training much more robust when training data are scarce. Our task is used jointly with the standard (supervised) training and it does not depend on specific architectural choices, thus it can be easily plugged in the existing VTs. Using an extensive evaluation with different VTs and datasets, we show that our method can improve (sometimes dramatically) the final accuracy of the VTs. The code will be available upon acceptance.",
  "full_text": "Efﬁcient Training of Visual Transformers with Small\nDatasets\nYahui Liu\nUniversity of Trento\nFondazione Bruno Kessler\nyahui.liu@unitn.it\nEnver Sangineto\nUniversity of Trento\nenver.sangineto@unitn.it\nWei Bi\nTencent AI Lab\nvictoriabi@tencent.com\nNicu Sebe\nUniversity of Trento\nniculae.sebe@unitn.it\nBruno Lepri\nFondazione Bruno Kessler\nlepri@fbk.eu\nMarco De Nadai\nFondazione Bruno Kessler\nwork@marcodena.it\nAbstract\nVisual Transformers (VTs) are emerging as an architectural paradigm alternative to\nConvolutional networks (CNNs). Differently from CNNs, VTs can capture global\nrelations between image elements and they potentially have a larger representation\ncapacity. However, the lack of the typical convolutional inductive bias makes these\nmodels more data hungry than common CNNs. In fact, some local properties of the\nvisual domain which are embedded in the CNN architectural design, in VTs should\nbe learned from samples. In this paper, we empirically analyse different VTs,\ncomparing their robustness in a small training set regime, and we show that, despite\nhaving a comparable accuracy when trained on ImageNet, their performance on\nsmaller datasets can be largely different. Moreover, we propose an auxiliary self-\nsupervised task which can extract additional information from images with only a\nnegligible computational overhead. This task encourages the VTs to learn spatial\nrelations within an image and makes the VT training much more robust when\ntraining data is scarce. Our task is used jointly with the standard (supervised)\ntraining and it does not depend on speciﬁc architectural choices, thus it can be\neasily plugged in the existing VTs. Using an extensive evaluation with different\nVTs and datasets, we show that our method can improve (sometimes dramatically)\nthe ﬁnal accuracy of the VTs. Our code is available at: https://github.com/\nyhlleo/VTs-Drloc.\n1 Introduction\nVisual Transformers (VTs) are progressively emerging architectures in computer vision as an alter-\nnative to standard Convolutional Neural Networks (CNNs), and they have already been applied to\nmany tasks, such as image classiﬁcation [19, 61, 70, 40, 66, 69, 37, 68], object detection [5, 76, 16],\nsegmentation [57], tracking [42], image generation [34, 32] and 3D data processing [74], to mention\na few. These architectures are inspired by the well known Transformer [63], which is the de facto\nstandard in Natural Language Processing (NLP) [17, 52], and one of their appealing properties is\nthe possibility to develop a uniﬁed information-processing paradigm for both visual and textual\ndomains. A pioneering work in this direction is ViT [ 19], in which an image is split using a grid\nof non-overlapping patches, and each patch is linearly projected in the input embedding space, so\nobtaining a \"token\". After that, all the tokens are processed by a series of multi-head attention and\nfeed-forward layers, similarly to how (word) tokens are processed in NLP Transformers.\nPreprint. Under review.\narXiv:2106.03746v2  [cs.CV]  14 Nov 2021\nA clear advantage of VTs is the possibility for the network to use the attention layers to model global\nrelations between tokens, and this is the main difference with respect to CNNs, where the receptive\nﬁeld of the convolutional kernels locally limits the type of relations which can be learned. However,\nthis increased representation capacity comes at a price, which is the lack of the typical CNN inductive\nbiases, based on exploiting the locality, the translation invariance and the hierarchical structure of\nvisual information [40, 66, 69]. As a result, VTs need a lot of data for training, usually more than what\nis necessary to standard CNNs [19]. For instance, ViT is trained with JFT-300M [19], a (proprietary)\nhuge dataset of 303 million (weakly) labeled high-resolution images, and performs worse than\nResNets [28] with similar capacity when trained on ImageNet-1K (∼1.3 million samples [55]). This\nis likely due to the fact that ViT needs to learn some local proprieties of the visual data using more\nsamples than a CNN, while the latter embeds these properties in its architectural design [54].\nTo alleviate this problem, a second generation of VTs has very recently been independently proposed\nby different groups [ 70, 40, 66, 69, 68, 37, 32]. A common idea behind these works is to mix\nconvolutional layers with attention layers, in such a way providing a local inductive bias to the VT.\nThese hybrid architectures enjoy the advantages of both paradigms: attention layers model long-range\ndependencies, while convolutional operations can emphasize the local properties of the image content.\nThe empirical results shown in most of these works demonstrate that these second-generation VTs can\nbe trained on ImageNet outperforming similar-size ResNets on this dataset [70, 40, 66, 69, 68, 37].\nHowever, it is still not clear what is the behaviour of these networks when trained on medium-small\ndatasets. In fact, from an application point of view, most of the computer vision tasks cannot rely on\n(supervised) datasets whose size is comparable with (or larger than) ImageNet.\nIn this paper, we compare to each other different second-generation VTs by either training them\nfrom scratch or ﬁne-tuning them on medium-small datasets, and we empirically show that, despite\ntheir ImageNet results are basically on par with each other, their classiﬁcation accuracy with smaller\ndatasets largely varies. We also compare VTs with same capacity ResNets, and we show that, in\nmost cases, VTs can match the ResNet accuracy when trained with small datasets. Moreover, we\npropose to use an auxiliary self-supervisedpretext task and a corresponding loss function to regularize\ntraining in a small training set or few epochs regime. Speciﬁcally, the proposed task is based on\n(unsupervised) learning the spatial relations between the output token embeddings. Given an image,\nwe densely sample random pairs from the ﬁnal embedding grid, and, for each pair, we ask the network\nto guess the corresponding geometric distance. To solve this task, the network needs to encode both\nlocal and contextual information in each embedding. In fact, without local information, embeddings\nrepresenting different input image patches cannot be distinguished the one from the others, while,\nwithout contextual information (aggregated using the attention layers), the task may be ambiguous.\nOur task is inspired by ELECTRA [13], in which the (NLP) pretext task is densely deﬁned for each\noutput embedding (Section 2). Clark et al. [ 13] show that their task is more sample-efﬁcient than\ncommonly used NLP pretext tasks, and this gain is particularly strong with small-capacity models\nor relatively smaller training sets. Similarly, we exploit the fact that an image is represented by a\nVT using multiple token embeddings, and we use their relative distances to deﬁne a localization task\nover a subset of all the possible embedding pairs. This way, for a single image forward pass, we\ncan compare many embedding pairs with each other, and average our localization loss over all of\nthem. Thus, our task is drastically different from those multi-crop strategies proposed, for instance,\nin SwA V [7], which need to independently forward each input patch through the network. Moreover,\ndifferently from \"ordering\" based tasks [49], we can deﬁne pairwise distances on a large grid without\nmodeling all the possible permutations (more details in Section 2).\nSince our auxiliary task is self-supervised, our dense relative localization loss (Ldrloc) does not\nrequire additional annotation, and we use it jointly with the standard (supervised) cross-entropy as a\nregularization of the VT training. Ldrloc is very easy-to-be-reproduced and, despite this simplicity, it\ncan largely boost the accuracy of the VTs, especially when the VT is either trained from scratch on\na small dataset, or ﬁne-tuned on a dataset with a large domain-shift with respect to the pretraining\nImageNet dataset. In our empirical analysis, based on different training scenarios, a variable amount\nof training data and different VT architectures, Ldrloc has always improved the results of the tested\nbaselines, sometimes boosting the ﬁnal accuracy of tens of points (and up to 45 points).\nIn summary, our main contributions are:\n1. We empirically compare to each other different VTs, showing that their behaviour largely\ndiffers when trained with small datasets or few training epochs.\n2\n2. We propose a relative localization auxiliary task for VT training regularization.\n3. Using an extensive empirical analysis, we show that this task is beneﬁcial to speed-up\ntraining and improve the generalization ability of different VTs, independently of their\nspeciﬁc architectural design or application task.\n2 Related work\nIn this section, we brieﬂy review previous work related to both VTs and self-supervised learning.\nVisual Transformers.Despite some previous work in which attention is used inside the convolutional\nlayers of a CNN [65, 30], the ﬁrst fully-transformer architectures for vision are iGPT [ 9] and ViT\n[19]. The former is trained using a \"masked-pixel\" self-supervised approach, similar in spirit to the\ncommon masked-word task used, for instance, in BERT [17] and in GPT [52] (see below). On the\nother hand, ViT is trained in a supervised way, using a special \"class token\" and a classiﬁcation head\nattached to the ﬁnal embedding of this token. Both methods are computationally expensive and,\ndespite their very good results when trained on huge datasets, they underperform ResNet architectures\nwhen trained from scratch using only ImageNet-1K [19, 9]. VideoBERT [58] is conceptually similar\nto iGPT, but, rather than using pixels as tokens, each frame of a video is holistically represented by a\nfeature vector, which is quantized using an off-the-shelf pretrained video classiﬁcation model. DeiT\n[61] trains ViT using distillation information provided by a pretrained CNN.\nThe success of ViT has attracted a lot of interest in the computer vision community, and different\nvariants of this architecture have been recently used in many tasks [ 61, 57, 34, 12]. However, as\nmentioned in Section 1, the lack of the typical CNN inductive biases in ViT, makes this model difﬁcult\nto train without using (very) large datasets. For this reason, very recently, a second-generation of VTs\nhas focused on hybrid architectures, in which convolutions are used jointly with long-range attention\nlayers [70, 40, 66, 69, 68, 37, 32]. The common idea behind all these works is that the sequence of\nthe individual token embeddings can be shaped/reshaped in a geometric grid, in which the position of\neach embedding vector corresponds to a ﬁxed location in the input image. Given this geometric layout\nof the embeddings, convolutional layers can be applied to neighboring embeddings, so encouraging\nthe network to focus on local properties of the image. The main difference among these works\nconcerns where the convolutional operation is applied (e.g., only in the initial representations [70]\nor in all the layers [ 40, 66, 69, 68, 37], in the token to query/key/value projections [ 66] or in the\nforward-layers [69, 37, 32], etc.). In most of the experiments of this paper, we use three state-of-\nthe-art second-generation VTs for which there is a public implementation: T2T [70], Swin [40] and\nCvT [66]). For each of them, we select the model whose number of parameters is comparable with a\nResNet-50 [28] (more details in Section 3 and Section 5). We do not modify the native architectures\nbecause the goal of this work is to propose a pretext task and a loss function which can be easily\nplugged in existing VTs.\nSimilarly to the original Transformer [63], in ViT, an (absolute) positional embedding is added to\nthe representation of the input tokens. In Transformer networks, positional embedding is used to\nprovide information about the token order, since both the attention and the (individual token based)\nfeed-forward layers are permutation invariant. In [ 40, 68], relative positional embedding [56] is\nused, where the position of each token is represented relatively to the others. Generally speaking,\npositional embedding is a representation of the token position which is provided as input to the\nnetwork. Conversely, our relative localization loss exploits the relative positions (of the ﬁnal VT\nembeddings) as a pretext task to extract additional information without manual supervision.\nSelf-supervised learning. Reviewing the vast self-supervised learning literature is out of the scope\nof this paper. However, we brieﬂy mention that self-supervised learning was ﬁrst successfully applied\nin NLP, as a means to get supervision from text by replacing costly manual annotations with pretext\ntasks [43, 44]. A typical NLP pretext task consists in masking a word in an input sentence and asking\nthe network to guess which is the masked token [43, 44, 17, 52]. ELECTRA [13] is a sample-efﬁcient\nlanguage model in which the masked-token pretext task is replaced by a discriminative task deﬁned\nover all the tokens of the input sentence. Our work is inspired by this method, since we propose\na pretext task which can be efﬁciently computed by densely sampling the ﬁnal VT embeddings.\nHowever, while the densely supervised ELECTRA task is obtained by randomly replacing (word)\ntokens and using a pre-trained BERT model to generate plausible replacements, we do not need\na pre-trained model and we do not replace input tokens, being our task based on predicting the\n3\ninter-token geometric distances. In fact, in NLP tasks, tokens are discrete and limited (e.g., the set of\nwords of a speciﬁc-language dictionary), while image patches are “continuous” and highly variable,\nhence a replacement-based task is hard to use in a vision scenario.\nIn computer vision, common pretext tasks with still images are based on extracting two different\nviews from the same image (e.g., two different crops) and then considering these as a pair of positive\nimages, likely sharing the same semantic content [ 10]. Most current self-supervised computer\nvision approaches can be categorised in contrastive learning [62, 29, 10, 26, 60, 64, 21], clustering\nmethods [3, 77, 33, 6, 1, 23, 7, 8], asymmetric networks [25, 11] and feature-decorrelation methods\n[22, 72, 2, 31]. While the aforementioned approaches are all based on ResNets, very recently, both\n[12] and [8] have empirically tested some of these methods with a ViT architecture [19].\nOne important difference of our proposal with respect to previous work, is that we do not propose a\nfully-self-supervised method, but we rather use self-supervision jointly with standard supervision\n(i.e., image labels) in order to regularize VT training, hence our framework is a multi-task learning\napproach [15]. Moreover, our dense relative localization loss is not based on positive pairs, and\nwe do not use multiple views of the same image in the current batch, thus our method can be used\nwith standard (supervised) data-augmentation techniques. Speciﬁcally, our pretext task is based on\npredicting the relative positions of pairs of tokens extracted from the same image.\nPrevious work using localization for self-supervision is based on predicting the input image rotation\n[24] or the relative position of adjacent patches extracted from the same image [18, 49, 50, 45]. For\ninstance, in [49], the network should predict the correct permutation of a grid of 3 ×3 patches (in\nNLP, a similar, permutation based pretext task, is deshufﬂing [53]). In contrast, we do not need\nto extract multiple patches from the same input image, since we can efﬁciently use the ﬁnal token\nembeddings (thus, we need a single forward and backward pass per image). Moreover, differently\nfrom previous work based on localization pretext tasks, our loss is densely computed between many\nrandom pairs of (non necessarily adjacent) token embeddings. For instance, a trivial extension of the\nordering task proposed in [49] using a grid of 7 ×7 patches would lead to 49! possible permutations,\nwhich becomes intractable if modeled as a classiﬁcation task. Finally, in [ 16], the position of a\nrandom query patch is used for the self-supervised training of a transformer-based object detector [5].\nHowever, the localization loss used in [16] is speciﬁc for the ﬁnal task (object localization) and the\nspeciﬁc DETR architecture [5], while our loss is generic and can be plugged in any VT.\n1\n1\n2\n3\n4\n5\nk\n2 3 4 5 k\nVisual\nTransformer\nInput image\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\nLocalization MLP\n(f )\nk\nk MLP\nMLP\nPooling/Cls token\ndu\ndv\nClass\nBird\nBall\nCar\n…\nb)a)\nFigure 1: A schematic representation of the VT architecture. (a) A typical second-generation VT. (b)\nOur localization MLP which takes as input (concatenated) pairs of ﬁnal token embeddings.\n3 Preliminaries\nA typical VT network takes as input an image split in a grid of (possibly overlapping)K×Kpatches.\nEach patch is projected in the input embedding space, obtaining a set ofK×Kinput tokens. A VT is\nbased on the typical Transformer multi-attention layers [63], which model pairwise relations over the\ntoken intermediate representations. Differently from a pure Transformer [63], the hybrid architectures\nmentioned in Section 1-2 usually shape or reshape the sequence of these token embeddings in a\nspatial grid, which makes it possible to apply convolutional operations over a small set of neighboring\ntoken embeddings. Using convolutions with a stride greater than 1 and/or pooling operations, the\nresolution of the initial K×Ktoken grid can possibly be reduced, thus simulating the hierarchical\n4\nstructure of a CNN. We assume that the ﬁnal embedding grid has a resolution ofk×k(where, usually,\nk≤K), see Fig. 1 (a).\nThe ﬁnal k×kgrid of embeddings represents the input image and it is used for the discriminative task.\nFor instance, some methods include an additional \"class token\" which collects contextual information\nover the whole grid [19, 70, 66, 69, 68, 37], while others [40] apply an average global pooling over\nthe ﬁnal grid to get a compact representation of the whole image. Finally, a standard, small MLP\nhead takes as input the whole image representation and it outputs a posterior distribution over the\nset of the target classes (Fig. 1 (a)). The VT is trained using a standard cross-entropy loss ( Lce),\ncomputed using these posteriors and the image ground-truth labels.\nWhen we plug our relative localization loss (Section 4) in an existing VT, we always use the native\nVT architecture of each tested method, without any change apart from the dedicated localization\nMLP (see Section 4). For instance, we use the class token when available, or the average pooling\nlayer when it is not, and on top of these we use the cross-entropy loss. We also keep the positional\nembedding (Section 2) for those VTs which use it (see Section 4.1 for a discussion about this choice).\nThe only architectural change we do is to downsample the ﬁnal embedding grid of T2T [ 70] and\nCvT [66] to make them of the same size as that used in Swin [ 7]. Speciﬁcally, in Swin, the ﬁnal\ngrid has a resolution of 7 ×7 (k = 7), while, in T2T and in CvT, it is 14 ×14. Thus, in T2T and\nin CvT, we use a 2 ×2 average pooling (without learnable parameters) and we get a ﬁnal 7 ×7\ngrid for all the three tested architectures. This pooling operation is motivated in Section 4.1, and\nit is used only together with our localization task (it does not affect the posterior computed by the\nclassiﬁcation MLP). Finally, note that T2T uses convolutional operations only in the input stage, and\nit outputs a sequence of 14 ×14 = 196embeddings, corresponding to its 14 ×14 input grid. In this\ncase, we ﬁrst reshape the sequence and then we use pooling. In the Supplementary Material, we\nshow additional experiments with a ViT architecture [19], in which we adopt the same reshaping and\npooling strategy.\n4 Dense relative localization task\nThe goal of our regularization task is to encourage the VT to learn spatial information without using\nadditional manual annotations. We achieve this by densely sampling multiple embedding pairs for\neach image and asking the network to guess their relative distances. In more detail, given an image\nx, we denote its corresponding k×kgrid of ﬁnal embeddings (Section 3), as Gx = {ei,j}1≤i,j≤k,\nwhere ei,j ∈Rd, and dis the dimension of the embedding space. For each Gx, we randomly sample\nmultiple pairs of embeddings and, for each pair (ei,j,ep,h), we compute the 2D normalized target\ntranslation offset (tu,tv)T , where:\ntu = |i−p|\nk , t v = |j−h|\nk , (tu,tv)T ∈[0,1]2. (1)\nThe selected embedding vectors ei,j and ep,h are concatenated and input to a small MLP ( f),\nwith two hidden layers and two output neurons, one per spatial dimension (Fig. 1 (b)), which\npredicts the relative distance between position (i,j) and position (p,h) on the grid. Let (du,dv)T =\nf(ei,j,ep,h)T . Given a mini-batch Bof nimages, our dense relative localization loss is:\nLdrloc =\n∑\nx∈B\nE(ei,j,ep,h)∼Gx[|(tu,tv)T −(du,dv)T |1]. (2)\nIn Eq. 2, for each image x, the expectation is computed by sampling uniformly at random mpairs\n(ei,j,ep,h) in Gx, and averaging the L1 loss between the corresponding (tu,tv)T and (du,dv)T .\nLdrloc is added to the standard cross-entropy loss ( Lce) of each native VT (Section 3). The ﬁnal\nloss is: Ltot = Lce + λLdrloc. We use λ= 0.1 in all the experiments with both T2T and CvT, and\nλ= 0.5 in case of Swin. Note that the same pairwise localization task can be associated with slightly\ndifferent loss formulations. In the Supplementary Material we present some of these variants and we\ncompare them empirically with each other.\n4.1 Discussion\nIntuitively, Ldrloc transforms the relative positional embedding (Section 2), used, for instance, in\nSwin [40], in a pretext task, asking the network to guess which is the relative distance of a random\n5\nsubset of all the possible token pairs. Thus a question arises: is the relative positional embedding used\nin some VTs sufﬁcient for the localization MLP (f) to solve the localization task? The experiments\npresented in Section 5.2-5.3 show that, when we plug Ldrloc on CvT, in which no kind of positional\nembedding is used [66], the relative accuracy boost is usuallysmaller than in case of Swin, conﬁrming\nthat the relative positional embedding, used in the latter, is not sufﬁcient to make our task trivial. We\nfurther analyze this point in the Supplementary Material.\nIn Section 3, we mentioned that, in case of T2T and CvT, we average-pool the ﬁnal grid and we\nobtain a 7 ×7 grid Gx. In fact, in preliminary experiments with both T2T and CvT at their original\n14 ×14 resolution, we observed a very slow convergence ofLdrloc. We presume this is due to the\nfact that, with a ﬁner grid, the localization task is harder. This slows down the convergence of f,\nand it likely generates noisy gradients which are backpropagated through the whole VT (see also the\nSupplementary Material). We leave this for future investigation and, in the rest of this article, we\nalways assume that our pretext task is computed with a 7 ×7 grid Gx.\nTable 1: The size of the datasets used in our empirical analysis.\nDataset Train size Test size Classes\nImageNet-1K [55] 1,281,167 100,000 1000\nImageNet-100 [60] 126,689 5,000 100\nCIFAR-10 [35] 50,000 10,000 10\nCIFAR-100 [35] 50,000 10,000 100\nOxford Flowers102 [48] 2,040 6,149 102\nSVHN [47] 73,257 26,032 10DomainNet\nClipArt 33,525 14,604\n345\nInfograph 36,023 15,582\nPainting 50,416 21,850\nQuickdraw 120,750 51,750\nReal 120,906 52,041\nSketch 48,212 20,916\n5 Experiments\nAll the experiments presented in this section are based on image classiﬁcation tasks, while in\nthe Supplementary Material we also show object detection, instance segmentation and semantic\nsegmentation tasks. In this section we use 11 different datasets: ImageNet-100 (IN-100) [ 60, 64],\nwhich is a subset of 100 classes of ImageNet-1K [ 55]; CIFAR-10 and CIFAR-100 [ 35], Oxford\nFlowers102 [48] and SVHN [47], which are four widely used computer vision datasets; and the six\ndatasets of DomainNet [51], a benchmark commonly used for domain adaptation tasks. We chose the\nlatter because of the large domain-shift between some of its datasets and ImageNet-1K, which makes\nthe ﬁne-tuning experiments non-trivial. Tab. 1 shows the size of each dataset.\nWe used, when available, the ofﬁcial VT code (for T2T [70] and Swin [40]) and a publicly available\nimplementation of CvT [66]1. In the ﬁne-tuning experiments (Section 5.3), we use only T2T and\nSwin because of the lack of publicly available ImageNet pre-trained CvT networks. For each of the\nthree baselines, we chose a model of comparable size to ResNet-50 (25M parameters): see Tab. 3 for\nmore details. In the Supplementary Material, we show additional results obtained with larger models\n(ViT-B [19]), larger datasets (e.g., ImageNet-1K) and more training epochs. When we plug our loss\non one of the adopted baselines, we follow Section 4, keeping unchanged the VT architecture apart\nfrom our localization MLP (f). Moreover, in all the experiments, we train the baselines, both with\nand without our localization loss, using the same data-augmentation protocol for all the models, and\nwe use the VT-speciﬁc hyper-parameter conﬁguration suggested by the authors of each VT. We do\nnot tune the VT-speciﬁc hyperparameters when we use our loss and we keep ﬁxed the values ofm\nand λ(Section 5.1) in all the experiments. We train each model using 8 V100 32GB GPUs.\n1https://github.com/lucidrains/vit-pytorch\n6\n5.1 Ablation study\nIn Tab. 2 (a) we analyze the impact on the accuracy of different values of m(the total number of\nembedding pairs used per image, see Section 4). Since we use the same grid resolution for all the\nVTs (i.e., 7 ×7, Section 3), also the maximum number of possible embeddings per image is the same\nfor all the VTs (k2 = 49). Using the results of Tab. 2 (a) (based on CIFAR-100 and Swin), we chose\nm = 64for all the VTs and all the datasets. Moreover, Tab. 2 (b) shows the inﬂuence of the loss\nweight λ(Section 4) for each of the three baselines, which motivates our choice of using λ= 0.1 for\nboth CvT and T2T and λ= 0.5 for Swin.\nThese values of mand λare kept ﬁxed in all the other experiments of this paper, independently of the\ndataset, the main task (e.g., classiﬁcation, detection, segmentation, etc.), and the training protocol\n(from scratch or ﬁne-tuning). This is done to emphasise the ease of use of our loss. Finally, in the\nSupplementary Material, we analyze the inﬂuence of the size of the localization MLP (f).\nTable 2: CIFAR-100, 100 training epochs: (a) the inﬂuence on the accuracy of the number of pair\nsamples (m) in Ldrloc using Swin, and (b) the inﬂuence of the λvalue using all the 3 VT baselines.\n(a)\nModel Top-1 Acc.\nA: Swin-T 53.28\nB: A + Ldrloc, m=32 63.70\nC: A + Ldrloc, m=64 66.23\nD: A + Ldrloc, m=128 65.16\nE: A + Ldrloc, m=256 64.87\n(b)\nModel λ=0.0 λ=0.1 λ=0.5 λ=1.0\nCvT-13 73.50 74.51 74.07 72.84\nSwin-T 53.28 58.15 66.23 64.28\nT2T-ViT-14 65.16 68.03 67.03 66.53\nTable 3: Top-1 accuracy on IN-100 using either 100 or 300 epochs. In the former case, we show the\naverage and the standard deviation values obtained by repeating each single experiment 5 times with\n5 different random seeds.\nModel # Params ImageNet-100\n(M) 100 epochs 300 epochs\nCvT CvT-13 20 85.62 ±0.05 90.16\nCvT-13+Ldrloc 20 86.09 ±0.12 (+0.47) 90.28 (+0.12)\nSwin Swin-T 29 82.66 ±0.10 89.68\nSwin-T+Ldrloc 29 83.95 ±0.05 (+1.29) 90.32 (+0.64)\nT2T T2T-ViT-14 22 82.67 ±0.01 87.76\nT2T-ViT-14+Ldrloc 22 83.74 ±0.08 (+1.07) 88.16 (+0.40)\n5.2 Training from scratch\nIn this section, we analyze the performance of both the VT baselines and our regularization loss using\nsmall-medium datasets and different number of training epochs, simulating a scenario with limited\ncomputational resources and/or limited training data. In fact, while ﬁne-tuning a model pre-trained\non ImageNet-1K is the most common protocol when dealing with small training datasets, this is not\npossible when, e.g., the network input is not an RGB image (e.g., in case of 3D point cloud data\n[74]) or when using a task-speciﬁc backbone architecture [36, 38]. In these cases, the network needs\nto be trained from scratch on the target dataset, thus, investigating the robustness of the VTs when\ntrained from scratch with relatively small datasets, is useful for those application domains in which a\nﬁne-tuning protocol cannot be adopted.\nWe start by analyzing the impact on the accuracy of the number of training epochs on IN-100. Tab. 3\nshows that, using Ldrloc, all the tested VTs show an accuracy improvement, and this boost is larger\nwith fewer epochs. As expected, our loss acts as a regularizer, whose effects are more pronounced\nin a shorter training regime. We believe this result is particularly signiﬁcant considering the larger\ncomputational times which are necessary to train typical VTs with respect to ResNets.\n7\nIn Tab. 4, we use all the other datasets and we train from scratch with 100 epochs (see the Supple-\nmentary Material for longer training protocols). First, we note that the accuracy of the VT baselines\nvaries a lot depending on the dataset (which is expected), but also depending on the speciﬁc VT\narchitecture. This is largely in contrast with the ImageNet-1K results, where the difference between\nthe three baselines is much smaller. As a reference, when these VTs are trained on ImageNet-1K\n(for 300 epochs), the differences of their respective top-1 accuracy is much smaller: Swin-T, 81.3\n[40]; T2T-ViT-14, 81.5 [70]; CvT-13, 81.6 [66]. Conversely, Tab. 4 shows that, for instance, the\naccuracy difference between CvT and Swin is about 45-46 points in Quickdraw and Sketch, 30 points\non CIFAR-10, and about 20 points on many other datasets. Analogously, the difference between CvT\nand T2T is between 20 and 25 points in Sketch, Painting and Flowers102, and quite signiﬁcant in\nthe other datasets. This comparison shows that CvT is usually much more robust in a small training\nset regime with respect to the other two VTs, a behaviour which is completely hidden when the\ntraining/evaluation protocol is based on large datasets only.\nIn the same table, we also show the accuracy of these three VTs when training is done using Ldrloc\nas a regularizer. Similarly to the IN-100 results, also in this case our loss improves the accuracy of all\nthe tested VTs in all the datasets. Most of the time, this improvement is quite signiﬁcant (e.g., almost\n4 points on SVHN with CvT), and sometimes dramatic (e.g., more than 45 points on Quickdraw with\nSwin). These results show that a self-supervised auxiliary task can provide a signiﬁcant \"signal\"\nto the VT when the training set is limited, and, speciﬁcally, that our loss can be very effective in\nboosting the accuracy of a VT trained from scratch in this scenario.\nIn Tab. 4 we also report the results we obtained using a ResNet-50, trained with 100 epochs and\nthe standard ResNet training protocol (e.g., using Mixup [73] and CutMix [71] data-augmentations,\netc.). These results show that the best performing VT (CvT) is usually comparable with a same\nsize ResNet, and demonstrate that VTs can potentially be trained from scratch with darasets smaller\nthan InageNet-1K. Finally, in the last row of the same table, we train the ResNet-50 baseline jointly\nwith our pretext task. In more detail, we replace the VT token embedding grid ( Gx in Eq. 2) with\nthe last convolutional feature map of the ResNet, and we apply our loss (Eq. 2) on top of this map.\nA comparison between the results of the last 2 rows of Tab. 4 shows that our loss is useful also\nwhen used with a ResNet (see the Supplementary Material for longer training protocols). When\nusing ResNets, the improvement obtained with our loss is marginal, but it is consistent in 9 out of\n10 datasets. The smaller improvement with respect to the analogous VT results may probably be\nexplained by the fact that ResNets already embed local inductive biases in their architecture, thus a\nlocalization auxiliary task is less helpful (Section 1).\nTable 4: Top-1 accuracy of VTs and ResNets, trained from scratch on different datasets (100 epochs).\nCIFAR-10\nCIFAR-100\nFlowers102\nSVHN\nClipArt\nInfograph\nPainting\nQuickdraw\nReal\nSketch\nCvT\nCvT-13 89.02 73.50 54.29 91.47 60.34 19.39 54.79 70.10 76.33 56.98\n90.30 74.51 56.29 95.36 60.64 20.05 55.26 70.36 77.05 57.56CvT-13+Ldrloc\n(+1.28) (+1.01) (+2.00) (+3.89) (+0.30) (+0.67) (+0.47) (+0.26) (+0.68) (+0.58)\nSwin\nSwin-T 59.47 53.28 34.51 71.60 38.05 8.20 35.92 24.08 73.47 11.97\n83.89 66.23 39.37 94.23 47.47 10.16 41.86 69.41 75.59 38.55Swin-T+Ldrloc\n(+24.42) (+12.95) (+4.86) (+22.63) (+9.42) (+1.96) (+5.94) (+45.33) (+2.12) (+26.58)\nT2T\nT2T-ViT-14 84.19 65.16 31.73 95.36 43.55 6.89 34.24 69.83 73.93 31.51\n87.56 68.03 34.35 96.49 52.36 9.51 42.78 70.16 74.63 51.95T2T-ViT-14+Ldrloc\n(+3.37) (+2.87) (+2.62) (+1.13) (+8.81) (+2.62) (+8.54) (+0.33) (+0.70) (+20.44)\nResNet\nResNet-50 91.78 72.80 46.92 96.45 63.73 19.81 53.22 71.38 75.28 60.08\n92.03 72.94 47.65 96.53 63.93 20.79 53.52 71.57 75.56 59.62ResNet-50+Ldrloc\n(+0.25) (+0.14) (+0.73) (+0.08) (+0.20) (+0.98) (+0.30) (+0.19) (+0.28) (-0.46)\n8\nTable 5: Pre-training on ImageNet-1K and then ﬁne-tuning on the target dataset (top-1 accuracy, 100\nﬁne-tuning epochs).\nCIFAR-10\nCIFAR-100\nFlowers102\nSVHN\nClipArt\nInfograph\nPainting\nQuickdraw\nReal\nSketch\nSwin\nSwin-T 97.95 88.22 98.03 96.10 73.51 41.07 72.99 75.81 85.48 72.37\n98.37 88.40 98.21 97.87 79.51 46.10 73.28 76.01 85.61 72.86Swin-T+Ldrloc\n(+0.42) (+0.18) (+0.18) (+1.77) (+6.00) (+5.03) (+0.29) (+0.20) (+0.13) (+0.49)\nT2T\nT2T-ViT-14 98.37 87.33 97.98 97.03 74.59 38.53 72.29 74.16 84.56 72.18\n98.52 87.65 98.08 98.20 78.22 45.69 72.42 74.27 84.57 72.29T2T-ViT-14+Ldrloc\n(+0.15) (+0.32) (+0.10) (+1.17) (+3.63) (+7.16) (+0.13) (+0.11) (+0.01) (+0.11)\nResNet\nResNet-50 97.65 85.44 96.59 96.60 75.22 44.30 66.58 72.12 80.40 67.77\n97.74 85.65 96.72 96.71 75.51 44.39 69.03 72.21 80.54 68.14ResNet-50+Ldrloc\n(+0.09) (+0.21) (+0.13) (+0.11) (+0.29) (+0.09) (+2.45) (+0.09) (+0.14) (+0.37)\n5.3 Fine-tuning\nIn this section, we analyze a typical ﬁne-tuning scenario, in which a model is pre-trained on a big\ndataset (e.g., ImageNet), and then ﬁne-tuned on the target domain. Speciﬁcally, inall the experiments,\nwe use VT models pre-trained by the corresponding VT authors on ImageNet-1K without our\nlocalization loss. The difference between the baselines and ours concerns only the ﬁne-tuning stage,\nwhich is done in the standard way for the former and using our Ldrloc regularizer for the latter.\nStarting from standard pre-trained models and using our loss only in the ﬁne-tuning stage, emphasises\nthe easy to use of our proposal in practical scenarios, in which ﬁne-tuning can be done without\nre-training the model on ImageNet. As mentioned in Section 5, in this analysis we do not include\nCvT because of the lack of publicly available ImageNet-1K pre-trained models for this architecture.\nThe results are presented in Tab. 5. Differently from the results shown in Section 5.2, the accuracy\ndifference between the T2T and Swin baselines is much less pronounced, and the latter outperforms\nthe former in most of the datasets. Moreover, analogously to all the other experiments, also in this\ncase, using Ldrloc leads to an accuracy improvement with all the tested VTs and in all the datasets.\nFor instance, on Infograph, Swin with Ldrloc improves of more than 5 points, and T2T more than 7\npoints.\nIn the last two rows of Tab. 5, we show the ResNet based results. The comparison between ResNet\nand the VT baselines shows that the latter are very competitive in this ﬁne-tuning scenario, even more\nthan with a training-from-scratch protocol (Tab. 4). For instance, the two VT baselines (without our\nloss) are outperformed by ResNet only in 2 out of 10 datasets. This conﬁrms that VTs are likely to be\nwidely adopted in computer vision applications in the near future, independently of the training set\nsize. Finally, analogously to the experiments in Section 5.2, Tab. 5 shows that our loss is (marginally)\nhelpful also in ResNet ﬁne-tuning.\n6 Conclusion\nIn this paper, we have empirically analyzed different VTs, showing that their performance largely\nvaries when trained with small-medium datasets, and that CvT is usually much more effective in\ngeneralizing with less data. Moreover, we proposed a self-supervised auxiliary task to regularize VT\ntraining. Our localization task, inspired by [13], is densely deﬁned for a random subset of ﬁnal token\nembedding pairs, and it encourages the VT to learn spatial information.\nIn our extensive empirical analysis, with 11 datasets, different training scenarios and three VTs,\nour dense localization loss has always improved the corresponding baseline accuracy, usually by\na signiﬁcant margin, and sometimes dramatically (up to +45 points). We believe that this shows\nthat our proposal is an easy-to-reproduce, yet very effective tool to boost the performance of VTs,\nespecially in training regimes with a limited amount of data/training time. It also paves the way to\ninvestigating other forms of self-supervised/multi-task learning which are speciﬁc for VTs, and can\nhelp VT training without resorting to the use of huge annotated datasets.\n9\nLimitations. A deeper analysis on why ﬁne-grained embedding grids have a negative impact on\nour auxiliary task (Section 4.1) was left as a future work. Moreover, despite in the Supplementary\nMaterial we show a few experiments with ViT-B, which conﬁrm the usefulness ofLdrloc when used\nwith bigger VT models, in our analysis we mainly focused on VTs of approximately the same size as\na ResNet-50. In fact, the goal of this paper is investigating the VT behaviour with medium-small\ndatasets, thus, high-capacity models most likely are not the best choice in a training scenario with\nscarcity of data.\nAcknowledgements\nThis work was partially supported by the EU H2020 AI4Media No. 951911 project and by the\nEUREGIO project OLIVER.\nReferences\n[1] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous\nclustering and representation learning. In ICLR, 2020.\n[2] Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regular-\nization for self-supervised learning. arXiv:2105.04906, 2021.\n[3] Miguel A Bautista, Artsiom Sanakoyeu, Ekaterina Tikhoncheva, and Bjorn Ommer. CliqueCNN:\ndeep unsupervised exemplar learning. In NeurIPS, 2016.\n[4] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into high quality object\ndetection. In CVPR, 2018.\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n[6] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for\nunsupervised learning of visual features. In ECCV, 2018.\n[7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.\n[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. arXiv:2104.14294,\n2021.\n[9] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In ICML, 2020.\n[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML, 2020.\n[11] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR,\n2021.\n[12] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised\nvision transformers. ICCV, 2021.\n[13] Kevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. ELECTRA:\nPre-training text encoders as discriminators rather than generators. In ICLR, 2020.\n[14] MMCV Contributors. Openmmlab foundational library for computer vision research, 2020.\n[15] Michael Crawshaw. Multi-task learning with deep neural networks: A survey.arXiv:2009.09796,\n2020.\n[16] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. UP-DETR: unsupervised pre-training\nfor object detection with transformers. In CVPR, 2021.\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In NAACL,, 2019.\n[18] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning\nby context prediction. In ICCV, 2015.\n10\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In ICLR, 2021.\n[20] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman.\nTemporal cycle-consistency learning. In CVPR, 2019.\n[21] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisser-\nman. With a little help from my friends: Nearest-neighbor contrastive learning of visual\nrepresentations. In ICCV, 2021.\n[22] Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for\nself-supervised representation learning. In ICML, 2021.\n[23] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and\nLuc Van Gool. SCAN: learning to classify images without labels. In ECCV, 2020.\n[24] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by\npredicting image rotations. In ICLR, 2018.\n[25] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi\nAzar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own\nlatent: A new approach to self-supervised learning. In NeurIPS, 2020.\n[26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In CVPR, 2020.\n[27] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask R-CNN. In ICCV, 2017.\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In CVPR, 2016.\n[29] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman,\nAdam Trischler, and Yoshua Bengio. Learning deep representations by mutual information\nestimation and maximization. In ICLR, 2019.\n[30] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image\nrecognition. In ICCV, 2019.\n[31] Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, and Hang Zhao. On feature\ndecorrelation in self-supervised learning. arXiv:2105.00470, 2021.\n[32] Drew A. Hudson and C. Lawrence Zitnick. Generative Adversarial Transformers. In ICML,\n2021.\n[33] Xu Ji, João F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised\nimage classiﬁcation and segmentation. In ICCV, 2019.\n[34] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. TransGAN: Two transformers can make one\nstrong GAN. arXiv:2102.07074, 2021.\n[35] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n2009.\n[36] Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints. Int. J. Comput. Vis.,\n128(3):642–656, 2020.\n[37] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. LocalViT: Bringing\nlocality to vision transformers. arXiv:2104.05707, 2021.\n[38] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, and Jian Sun. Detnet:\nDesign backbone for object detection. In ECCV, 2018.\n[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV,\n2014.\n[40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows. arXiv:2103.14030,\n2021.\n11\n[41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv:1711.05101,\n2017.\n[42] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixé, and Christoph Feichtenhofer. Track-\nFormer: Multi-object tracking with transformers. arXiv:2101.02702, 2021.\n[43] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word\nrepresentations in vector space. arXiv:1301.3781, 2013.\n[44] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed\nrepresentations of words and phrases and their compositionality. In NeurIPS, 2013.\n[45] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant repre-\nsentations. In CVPR, 2020.\n[46] Muzammal Naseer, Kanchana Ranasinghe, Salman H. Khan, Munawar Hayat, Fahad Shahbaz\nKhan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. arXiv:2105.10497,\n2021.\n[47] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng.\nReading digits in natural images with unsupervised feature learning. In NeurIPS Workshop on\nDeep Learning and Unsupervised Feature Learning, 2011.\n[48] Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower classiﬁcation over a large\nnumber of classes. In Indian Conference on Computer Vision, Graphics & Image Processing,\n2008.\n[49] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving\njigsaw puzzles. In ECCV, 2016.\n[50] Mehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Representation learning by learning to\ncount. In ICCV, 2017.\n[51] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment\nmatching for multi-source domain adaptation. In CVPR, 2019.\n[52] Alec Radford and Karthik Narasimhan. Improving language understanding by generative\npre-training. 2018.\n[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. Journal of Machine Learning Research, 21:140:1–140:67, 2020.\n[54] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy.\nDo vision transformers see like convolutional neural networks? arXiv:2108.08810, 2021.\n[55] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.\n[56] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position repre-\nsentations. In NAACL, 2018.\n[57] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for\nsemantic segmentation. In ICCV, 2021.\n[58] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. VideoBERT: A\njoint model for video and language representation learning. In ICCV, 2019.\n[59] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka,\nLei Li, Zehuan Yuan, Changhu Wang, et al. Sparse R-CNN: End-to-end object detection with\nlearnable proposals. arXiv:2011.12450, 2020.\n[60] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV,\n2020.\n[61] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles,\nand Hervé Jégou. Training data-efﬁcient image transformers & distillation through attention.\narXiv:2012.12877, 2020.\n[62] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. arXiv:1807.03748, 2018.\n12\n[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n[64] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through\nalignment and uniformity on the hypersphere. In ICML, 2020.\n[65] Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.\nIn CVPR, 2018.\n[66] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.\nCvT: Introducing convolutions to vision transformers. arXiv:2103.15808, 2021.\n[67] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Uniﬁed perceptual parsing\nfor scene understanding. In ECCV, 2018.\n[68] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image\ntransformers. arXiv:2104.06399, 2021.\n[69] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating\nconvolution designs into visual transformers. arXiv:2103.11816, 2021.\n[70] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token ViT: Training vision transformers from scratch on\nImageNet. In ICCV, 2021.\n[71] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk\nChoe. CutMix: Regularization strategy to train strong classiﬁers with localizable features. In\nICCV, 2019.\n[72] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-\nsupervised learning via redundancy reduction. In ICML, 2021.\n[73] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond\nempirical risk minimization. In ICLR, 2018.\n[74] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip H. S. Torr, and Vladlen Koltun. Point transformer.\narXiv:2012.09164, 2020.\n[75] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio\nTorralba. Semantic understanding of scenes through the ade20k dataset. International Journal\nof Computer Vision, 127(3):302–321, 2019.\n[76] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR:\nDeformable transformers for end-to-end object detection. In ICLR, 2021.\n[77] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised\nlearning of visual embeddings. In ICCV, 2019.\n13\nA Pseudocode of the dense relative localization task\nIn order to emphasise the simplicity and the ease of reproduction of our proposed method, in Figure 2\nwe show a PyTorch-like pseudocode of our auxiliary task with the associated Ldrloc loss.\n# n : batch size\n# m : number of pairs\n# k X k : resolution of the embedding grid\n# D : dimension of each token embedding\n# x : a tensor of n embedding grids, shape=[n, D, k, k]\ndef position_sampling(k, m, n):\npos_1 = torch.randint(k, size=(n, m, 2))\npos_2 = torch.randint(k, size=(n, m, 2))\nreturn pos_1, pos_2\ndef collect_samples(x, pos, n):\n_, c, h, w = x.size()\nx = x.view(n, c, -1).permute(1, 0, 2).reshape(c, -1)\npos = ((torch.arange(n).long().to(pos.device) * h * w).view(n, 1)\n+ pos[:, :, 0] * h + pos[:, :, 1]).view(-1)\nreturn (x[:, pos]).view(c, n, -1).permute(1, 0, 2)\ndef dense_relative_localization_loss(x):\nn, D, k, k = x.size()\npos_1, pos_2 = position_sampling(k, m, n)\ndeltaxy = abs((pos_1 - pos_2).float()) # [n, m, 2]\ndeltaxy /= k\npts_1 = collect_samples(x, pos_1, n).transpose(1, 2) # [n, m, D]\npts_2 = collect_samples(x, pos_2, n).transpose(1, 2) # [n, m, D]\npredxy = MLP(torch.cat([pts_1, pts_2], dim=2))\nreturn L1Loss(predxy, deltaxy)\nFigure 2: A PyTorch-like pseudocode of our dense relative localization task and the corresponding\nLdrloc loss.\nB Loss Variants\nIn this section, we present different loss function variants associated with our relative localization\ntask, which are empirically evaluated in Sec. B.1. The goal is to show that the auxiliary task proposed\nin the main paper can be implemented in different ways and to analyze the differences between these\nimplementations.\nThe ﬁrst variant consists in including negative target offsets:\nt′\nu = i−p\nk , t ′\nv = j−h\nk , (t′\nu,t′\nv)T ∈[−1,1]2. (3)\nReplacing (tu,tv)T in Eq. 2 in the main paper with (t′\nu,t′\nv)T computed as in Eq. 3, and keeping all\nthe rest unchanged, we obtain the ﬁrst variant, which we call L∗\ndrloc.\nIn the second variant, we transform the regression task in Eq. 2 in the main paper in a classiﬁcation\ntask, and we replace the L1 loss with the cross-entropy loss. In more detail, we use as target offsets:\ncu = i−p, c v = j−h, (cu,cv)T ∈{−k,...,k }2, (4)\nand we associate each of the2k+1 discrete elements in C = {−k,...,k }with a \"class\". Accordingly,\nthe localization MLP f is modiﬁed by replacing the 2 output neurons with 2 different sets of neurons,\none per spatial dimension (uand v). Each set of neurons represents a discrete offset prediction over\nthe 2k+ 1\"classes\" in C. Softmax is applied separately to each set of 2k+ 1neurons, and the\noutput of f is composed of two posterior distributions over C: (pu,pv)T = f(ei,j,ep,h)T , where\n14\npu,pv ∈[0,1]2k+1. Eq. 2 in the main paper is then replaced by:\nLce\ndrloc = −\n∑\nx∈B\nE(ei,j,ep,h)∼Gx[log(pu[cu]) +log(pv[cv])], (5)\nwhere pu[cu] indicates the cu-th element of pu (and similarly for pv[cv]).\nNote that, using the cross-entropy loss in Eq. 5, corresponds to considering C an unordered set of\n\"categories\". This implies that prediction errors in pu (and pv) are independent of the \"distance\" with\nrespect to the ground-truth cu (respectively, cv). In order to alleviate this problem, and inspired by\n[20], in the third variant we propose, we impose a Gaussian prior on pu and pv, and we minimize the\nnormalized squared distance between the expectation of pu and the ground-truth cu (respectively, pv\nand cv). In more detail, let µu = ∑\nc∈C pu[c] ∗cand σ2\nu = ∑\nc∈C pu[c] ∗(c−µu)2 (and similarly\nfor µv and σ2\nv). Then, Eq. 5 is replaced by:\nLreg\ndrloc =\n∑\nx∈B\nE(ei,j,ep,h)∼Gx\n[(cu −µu)2\nσ2u\n+ αlog(σu) +(cv −µv)2\nσ2v\n+ αlog(σv)\n]\n, (6)\nwhere the terms log(σu) and log(σv) are used for variance regularization and αweights the impor-\ntance of the Gaussian prior [ 20]. In preliminary experiments in which we tuned the αparameter\nusing Swin, we found that the default value of α= 0.001, as suggested in [20], works well in our\nscenario, thus we adopted it for all the experiments involving Lreg\ndrloc.\nThe fourth variant we propose is based on a \"very-dense\" localization loss, where Ldrloc is computed\nfor every transformer block of VT. Speciﬁcally, let Gl\nx be the kl ×kl grid of token embeddings\nproduced by the l-th block of the VT, and let Lbe the total number of these blocks. Then, Eq. 2 in\nthe main paper is replaced by:\nLall\ndrloc =\n∑\nx∈B\nL∑\nl=1\nE(ei,j,ep,h)∼Glx\n[|(tl\nu,tl\nv)T −(dl\nu,dl\nv)T |1], (7)\nwhere (tl\nu,tl\nv)T and (dl\nu,dl\nv)T are, respectively, the target (see main paper Eq. 1) and the prediction\noffsets computed at block lusing the randomly sampled pair (ei,j,ep,h) ∈Gl\nx. For each block, we\nuse a block-speciﬁc MLP fl to compute (dl\nu,dl\nv)T . Note that, using Eq. 7, the initial layers of VT\nreceive more \"signal\", because each block laccumulates the gradients produced by all the blocks\nl′≥l.\nApart from Lall\ndrloc, all the other proposed variants are very computationally efﬁcient, because they\ninvolve only one forward and one backward pass per image, and mforward passes through f.\nB.1 Empirical comparison of the loss variants\nIn Tab. 6, we compare the loss variants with each other, where the baseline model is Swin [40] (row\n(A)). For these experiments, we use IN-100, we train all the models for 100 epochs, and, as usual, we\nshow the top-1 classiﬁcation accuracy on the test set.\nWhen we plug Ldrloc on top of Swin (main paper, Sec. 4), the ﬁnal accuracy increases by 1.26 points\n(B). All the other dense localization loss variants underperform Ldrloc (C-F). A bit surprisingly,\nthe very-dense localization loss Lall\ndrloc is signiﬁcantly outperformed by the much simpler (and\ncomputationally more efﬁcient) Ldrloc. Moreover, Lall\ndrloc is the only variant which underperforms\nthe baseline. We presume that this is due to the fact that most of the Swin intermediate blocks\nhave resolution grids Gl\nx ﬁner than the last grid GL\nx (l < L, kl > kL, Sec. B), and this makes the\nlocalization task harder, slowing down the convergence offl, and likely providing noisy gradients to\nthe VT (see the discussion in the main paper, Sec. 4.1). In all the other experiments (both in the main\npaper and in this Supplementary Material), we always use Ldrloc as the relative localization loss.\nB.2 Relative positional embedding\nAll the loss variants presented in this section have been plugged on Swin, in which relative positional\nembedding is used (see the main paper, Sec. 3 and Sec. 4.1). However, the results reported in Tab. 6\nshow that almost all of these losses can boost the accuracy of the Swin baseline. Below, we intuitively\n15\nTable 6: IN-100, 100 epoch training: a comparison between different loss variants.\nModel Top-1 Acc.\nA: Swin-T 82.76\nB: A + Ldrloc 84.02 (+1.26)\nC: A + L∗\ndrloc 83.14 (+0.38)\nD: A + Lce\ndrloc 83.86 (+1.10)\nE: A + Lreg\ndrloc 83.24 (+0.48)\nF: A + Lall\ndrloc 81.88 (-0.88)\nexplain why the relative positional embedding is not sufﬁcient to allow the network to solve our\nlocalization task.\nThe relative positional embedding (called Bin [40]) used in Swin, is added to the query/key product\nbefore the softmax operation (Eq. 4 in [ 40]). The result of this softmax is then used to weight\nthe importance of each \"value\", and the new embedding representation of each query (i.e., ei,j, in\nour terminology) is given by this weighted sum of values. Thus, the content of B is not directly\nrepresented in ei,j, but only used to weight the values forming ei,j (note that there is also a skip\nconnection). For this reason, Bmay be useful for the task for which it is designed, i.e., computing\nthe importance (attention) of each key with respect to the current query. However, in order to\nsolve our auxiliary task (i.e., to predict tu and tv in Eq. 1 in the main paper), the VT should be\nable to recover and extract from a given embedding pair(ei,j,ep,h) the speciﬁc offset information\noriginally contained in B(i,j),(p,h) and then blended in the value weights. Probably this is a task\n(much) harder than exploiting appearance information contained in (ei,j,ep,h). This is somehow in\nline with different previous work showing the marginal importance of positional embedding in VTs.\nFor instance, Naseer et al. [ 46] show that the (absolute) positional embedding used in ViT [19] is\nnot necessary for the transformer to solve very challenging occlusion or patch permutation tasks,\nand they conclude that these tasks are solved by ViT thank to its “dynamic receptive ﬁeld” (i.e., the\ncontext represented in each individual token embedding).\nC Experiments with a larger training budget\nAlthough the focus of this work is on increasing the VT training efﬁciency in a scenario with a\nlimited training budget, in this section we instead investigate the effect of using our auxiliary task on\nscenarios with a larger training budget. Speciﬁcally, we test Ldrloc with a larger number of training\nepochs, using higher-capacity VT models and training the VTs on ImageNet-1K.\nIn Tab. 7 we train both Swin and T2T on ImageNet-1K following the standard protocol (e.g., 300\nepochs) and using the publicly available code of each VT baseline. When we use Ldrloc, we get a\nslight improvement with both the baselines, which shows that our loss is beneﬁcial also with larger\ndatasets and longer training schedules (although the margin is smaller with respect to IN-100, see\nTab. 3).\nTable 7: Top-1 accuracy on ImageNet-1K. (*) Results obtained in our run of the publicly available\ncode with the default hyperparameters of each corresponding VT baseline.\nModel Top-1 Acc.\nSwin Swin-T 81.2 (*)\nSwin-T+Ldrloc 81.33 (+0.13)\nT2T T2T-ViT-14 80.7 (*)\nT2T-ViT-14+Ldrloc 80.85 (+0.15)\nIn Tab. 8, we use the Infograph dataset and we train all the networks for 300 epochs. The results\nconﬁrm that Ldrloc can improve the ﬁnal accuracy even when a longer training schedule is adopted.\nFor instance, comparing the results of T2T in Tab. 8 with the T2T results in Tab. 4 (100 epochs), the\nrelative margin has signiﬁcantly increased (+8.06 versus +2.62).\nFinally, in Tab. 9, we use three datasets and we train from scratch ViT-B/16 [19], which has 86.4\nmillion parameters (about 4×the number of parameters of the other tested VTs and ResNets). Note\n16\nTable 8: Infograph, training from scratch with 300 epochs.\nModel Top-1 Acc.\nCvT-13 29.76\nCvT-13 + Ldrloc 30.31 (+0.55)\nSwin-T 17.17\nSwin-T + Ldrloc 20.72 (+3.55)\nT2T-ViT-14 12.62\nT2T-ViT-14 +Ldrloc 20.68 (+8.06)\nResNet-50 29.34\nResNet-50 + Ldrloc 30.00 (+0.66)\nthat \"16\" in ViT-B/16 stands for16 ×16 resolution patches, used as input without patch overlapping.\nFor a fair comparison, we used for ViT-B/16 the same image resolution (224 ×224) adopted for all\nthe other VTs (see Sec. F), thus we get a ﬁnal ViT-B/16 embedding grid of 14 ×14, which is pooled\nto get our 7 ×7 grid as explained in the main paper (Sec. 3). For ViT-B/16, we use λ= 0.01. Tab. 9\nshows that our loss is effective also with VT models bigger than the three baselines used in the rest of\nthe paper.\nTable 9: Training from scratch ViT-B/16 with 100 epochs.\nModel CIFAR-10 CIFAR-100 Infograph\nViT-B/16 71.70 59.67 11.79\nViT-B/16 +Ldrloc 73.91 (+2.21) 61.42 (+1.75) 12.22 (+0.43)\nD Transfer to object detection and image segmentation tasks\nIn this section, we provide additional ﬁne-tuning experiments using tasks different from classiﬁcation\n(i.e., object detection, instance segmentation and semantic segmentation). Moreover, we use a\ndifferent training protocol from the one used in the main paper (Sec. 5.3). Speciﬁcally, the ﬁne-\ntuning stage is standard (without our loss), while in the pre-training stage we either use the standard\ncross-entropy (only), or we pre-train the VT jointly using the cross-entropy and Ldrloc. We adopt the\nframework proposed in [40], where a pre-trained Swin VT is used as the backbone for detection and\nsegmentation tasks. In fact, note that Swin is based on a hierarchy of embedding grids, which can be\nused by the speciﬁc object detection/image segmentation architectures as they were convolutional\nfeature maps [40].\nThe pre-training dataset is either ImageNet-1K or IN-100, and in both cases we pre-train Swin using\n300 epochs. Hence, in case of ImageNet-1K pre-training, the baseline model is ﬁne-tuned starting\nfrom the Swin-T model corresponding to Tab. 7 (ﬁnal accuracy : 81.2), while Swin-T + Ldrloc refers\nto the model trained with our loss in the same table (ﬁnal accuracy: 81.33). Similarly, in case of\nIN-100 pre-training, the baseline model is ﬁne-tuned starting from the Swin-T model corresponding\nto Tab. 3 (ﬁnal accuracy : 89.68), while Swin-T + Ldrloc refers to the model trained with our loss in\nthe same table (ﬁnal accuracy: 90.32).\nThe goal of these experiments is to show that the image representation obtained using Ldrloc for\npre-training, can be usefully transferred to other tasks without modifying the task-speciﬁc architecture\nor the ﬁne-tuning protocol.\nD.1 Object detection and instance segmentation\nSetup. We strictly follow the experimental settings used in Swin [40]. Speciﬁcally, we use COCO\n2017 [39], which contains 118K training, 5K validation and 20K test-dev images. We use two\npopular object detection architectures: Cascade Mask R-CNN [4] and Mask R-CNN [27], in which\nthe backbone is replaced with the pre-trained Swin model. Moreover, we use the standard mmcv [14]\n17\nframework to train and evaluate the models. We adopt multi-scale training [5, 59] (i.e., we resize the\ninput image such that the shortest side is between 480 and 800 pixels, while the longest side is at most\n1333 pixels), the AdamW [41] optimizer (initial learning rate 0.0001, weight decay 0.05, and batch\nsize 16), and a 1x schedule (12 epochs with the learning rate decayed by 0.1 at epochs 8 and 11).\nResults. Tab. 10 shows that Swin-T, pre-trained on ImageNet-1K with our Ldrloc loss, achieves\nboth a higher detection and a higher instance segmentation accuracy with respect to the baselines.\nSpeciﬁcally, with both Mask RCNN and Cascade Mask RCNN, our pre-trained model outperforms\nthe baselines with respect to nearly all detection/segmentation metrics. When pre-training with a\nsmaller dataset (IN-100), the relative improvement is even higher (Tab. 11).\nTable 10: ImageNet-1K pre-training. Results on the COCO object detection and instance segmentation\ntasks. APbox\nx and APmask\nx are the standard object detection and segmentation Average Precision metrics,\nrespectively [39].\nArchitecture Pre-trained backbone APbox APbox\n50 APbox\n75 APmask APmask\n50 APmask\n75\nMask RCNN\nSwin-T 43.4 66.2 47.4 39.6 63.0 42.6\n43.8 66.5 48.0 39.7 63.1 42.5Swin-T + Ldrloc (+0.4) (+0.3) (+0.6) (+0.1) (+0.1) (-0.1)\nCascade Mask RCNN\nSwin-T 48.0 67.1 51.7 41.5 64.3 44.8\n48.2 67.4 52.1 41.7 64.7 44.8Swin-T + Ldrloc (+0.2) (+0.3) (+0.4) (+0.2) (+0.4) (+0.0)\nTable 11: IN-100 pre-training. Results on the COCO object detection and instance segmentation\ntasks.\nArchitecture Pre-trained backbone APbox APbox\n50 APbox\n75 APmask APmask\n50 APmask\n75\nMask RCNN\nSwin-T 41.8 60.3 45.1 36.7 57.4 39.4\n42.7 61.3 45.9 37.2 58.4 40.0Swin-T + Ldrloc (+0.9) (+1.0) (+0.8) (+1.0) (+1.0) (+0.6)\nCascade Mask RCNN\nSwin-T 36.0 58.2 38.6 33.8 55.2 35.9\n37.2 59.4 40.3 34.5 56.2 36.6Swin-T + Ldrloc (+1.2) (+1.2) (+1.7) (+0.7) (+1.0) (+0.7)\nD.2 Semantic segmentation\nSetup. We again follow the experimental settings adopted in Swin [40]. Speciﬁcally, for the semantic\nsegmentation experiments, we use the ADE20K dataset [75], which is composed of 150 semantic\ncategories, and contains 20K training, 2K validation and 3K testing images. Following [40], we use\nthe popular UperNet [67] architecture with a Swin backbone pre-trained either on ImageNet-1K or\non IN-100 (see above). We use the implementation released by mmcv [14] to train and evaluate all\nthe models.\nWhen ﬁne-tuning, we used the AdamW [ 41] optimizer with an initial learning rate of 6 ×10−5,\na weight decay of 0.01, a scheduler with linear learning-rate decay, and a linear warmup of 1,500\niterations. We ﬁne-tuned all the models on 8 Nvidia V100 32GB GPUs with 2 images per GPU for\n160K iterations. We adopt the default data augumentation techniques used for segmentation, namely\nrandom horizontal ﬂipping, random re-scaling with a [0.5, 2.0] ratio range and random photometric\ndistortion. We use stochastic depth with ratio 0.2 for all the models, which are trained with an input\nof 512×512 pixels. At inference time, we use a multi-scale testing, with image resolutions which are\n{0.5,0.75,1.0,1.25,1.5,1.75}×of the training resolution.\nResults. The results reported in Tab. 12 show that the models pre-trained on ImageNet-1K with the\nproposed loss always outperform the baselines with respect to all the segmentation metrics. Similarly\nto Sec. D.1, when a smaller dataset is used for pre-training (IN-100), the observed relative boost is\neven higher (Tab. 13).\n18\nTable 12: ImageNet-1K pre-training. Semantic segmentation on the ADE20K dataset (testing on\nthe validation set). mIoU and mAcc refer to the mean Intersection over Union and the mean class\nAccuracy, respectively. The base architecture is UperNet [67].\nPre-trained backbone mIoU mAcc\nSwin-T 43.87 55.22\n44.33 55.74Swin-T + Ldrloc (+0.46) (+0.52)\nTable 13: IN-100 pre-training. Semantic segmentation on the ADE20K dataset (testing on the\nvalidation set) with a UperNet architecture [67].\nPre-trained backbone mIoU mAcc\nSwin-T 36.93 47.76\n37.83 48.69Swin-T + Ldrloc (+0.90) (+0.93)\nE Training efﬁciency\nIn Fig. 3 we show the training curves corresponding to the top-1 accuracy of CvT, Swin and T2T,\ntrained from scratch on CIFAR-100, with or without our loss. These graphs show that our auxiliary\ntask is beneﬁcial over the whole training stage, and it can speed-up the overall training. For instance,\nin case of Swin, after 60 training epochs, or method is already signiﬁcantly better than the baseline\nfull-trained with 100 epochs (55.01 versus 53.28).\n0 25 50 75 100\nEpochs\n0\n20\n40\n60T op-1 Acc.\nCvT-13\n0 25 50 75 100\nEpochs\nSwin-T\n0 25 50 75 100\nEpochs\nT2T-ViT-14\nBaseline Baseline + drloc\nFigure 3: CIFAR-100, training from scratch, top-1 accuracy measured every 10 epochs.\nFinally, we compute the overhead of Ldrloc at training time. The results reported in Tab. 14 refer to\nseconds per batch (with a batch size equal to 1024), and show that, overall, the overhead due to our\nauxiliary task is negligible with respect to the whole training time.\nF Implementation details and an additional ablation study on the\nlocalization MLP\nOur localization MLP (f) is a simple feed-forward network composed of three fully connected layers.\nThe ﬁrst layer projects the concatenation of the two input token embeddings ei,j and ep,h into a\n512-dimensional vector and then it applies aRelu activation. Next, we use a linear layer of dimension\n512 followed by a Relu activation. Finally, we use a linear layer dedicated to the prediction, which\ndepends on the speciﬁc loss variant, see Sec. B. For instance, in Ldrloc, the last layer is composed of\n19\nTable 14: Training time comparison on CIFAR-100. The values are averaged over all training batches\nand jointly reported the corresponding standard deviations.\nModel Seconds per batch\nCvT-13 0.6037 ±0.0040\nCvT-13 + Ldrloc 0.6184 ±0.0070 (+2.43%)\nSwin-T 0.6684 ±0.0031\nSwin-T + Ldrloc 0.6842 ±0.0033 (+2.36%)\nT2T-ViT-14 0.5941 ±0.0053\nT2T-ViT-14 +Ldrloc 0.6046 ±0.0058 (+1.77%)\ntwo neurons which predict du and dv. The details of the MLP head are shown in Tab. 15, while in\nTab. 16 we show the inﬂuence of the number of neurons in the hidden layers of f.\nTable 15: The details of the localization MLP head. dis the dimension of a token embedding. The\nnumber of outputs oand the ﬁnal nonlinearity (if used) depend on the speciﬁc loss. In Ldrloc, L∗\ndrloc\nand Lall\ndrloc, we use o= 2without any nonlinearity. Converesely, in bothLce\ndrloc and Lreg\ndrloc, the last\nlayer is split in two branches of 2k+ 1neurons each, and, on each branch, we separately apply a\nSoftMax layer.\nLayer Activation Output dimension\nInput - d* 2\nLinear ReLU 512\nLinear ReLU 512\nLinear - / SoftMax o\nTable 16: CIFAR-100, 100 epochs, training from scratch: the inﬂuence of the number of neurons\nused in each of the two hidden layers of the localization MLP.\nModel Number of neurons\n256 512 1024\nCvT-13 + Ldrloc 74.19 74.51 73.80\nSwin-T + Ldrloc 65.06 66.23 64.33\nT2T-ViT-14 +Ldrloc 66.49 68.03 67.83\nIn our experiments, we used the ofﬁcially released framework of Swin [40]2, which also provides all\nthe necessary code to train and test VT networks (including the object detection and segmentation\ntasks of Sec. D). For a fair comparison, we use the ofﬁcial code of T2T-ViT [ 70]3 and a publicly\nreleased code of CvT [66]4 and we insert them in the training framework released by the authors\nof Swin. At submission time of this paper, the ofﬁcial code of CvT [66] was not publicly available.\nFinally, the ViT-B/16 model used in Sec. C is based on a public code4.\nWhen we train the networks from scratch (100 epochs), we use the AdamW [ 41] optimizer with\na cosine decay learning-rate scheduler and 20 epochs of linear warm-up. We use a batch size of\n1024, an initial learning rate of 0.001, and a weight decay of 0.05. When we ﬁne-tune the networks\n(100 epochs), we use the AdamW [41] optimizer with a cosine decay learning-rate scheduler and\n10 epochs of linear warm-up. We use a batch size of 1024, an initial learning rate of 0.0005, and a\nweight decay of 0.05. In all the experiments, the images of all the datasets are resized to the same\nﬁxed resolution (224 ×224).\n2https://github.com/microsoft/Swin-Transformer\n3https://github.com/yitu-opensource/T2T-ViT\n4https://github.com/lucidrains/vit-pytorch\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7963866591453552
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.7048736810684204
    },
    {
      "name": "Machine learning",
      "score": 0.6109626889228821
    },
    {
      "name": "Convolutional neural network",
      "score": 0.591659665107727
    },
    {
      "name": "Training set",
      "score": 0.5895347595214844
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5887752771377563
    },
    {
      "name": "Transformer",
      "score": 0.5546087622642517
    },
    {
      "name": "Inductive bias",
      "score": 0.5214716196060181
    },
    {
      "name": "Task (project management)",
      "score": 0.477667897939682
    },
    {
      "name": "External Data Representation",
      "score": 0.4158972501754761
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37977704405784607
    },
    {
      "name": "Multi-task learning",
      "score": 0.3562132716178894
    },
    {
      "name": "Data mining",
      "score": 0.3451809287071228
    },
    {
      "name": "Engineering",
      "score": 0.08028584718704224
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I193223587",
      "name": "University of Trento",
      "country": "IT"
    }
  ]
}