{
    "title": "Syllable-level Neural Language Model for Agglutinative Language",
    "url": "https://openalex.org/W2963402126",
    "year": 2017,
    "authors": [
        {
            "id": "https://openalex.org/A5036671957",
            "name": "Seunghak Yu",
            "affiliations": [
                "Samsung (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A5057446637",
            "name": "Nilesh Kulkarni",
            "affiliations": [
                "Samsung (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A5052875434",
            "name": "Haejun Lee",
            "affiliations": [
                "Samsung (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A5080664764",
            "name": "Jihie Kim",
            "affiliations": [
                "Samsung (South Korea)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2251811146",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2051780479",
        "https://openalex.org/W179875071",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2417736714",
        "https://openalex.org/W2402293560",
        "https://openalex.org/W2251849926",
        "https://openalex.org/W2964005834",
        "https://openalex.org/W2050469586",
        "https://openalex.org/W2394700483",
        "https://openalex.org/W1899794420",
        "https://openalex.org/W2949563612",
        "https://openalex.org/W1831014557",
        "https://openalex.org/W2963582782",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W4394643672",
        "https://openalex.org/W2963932686",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2101609803",
        "https://openalex.org/W2058641082"
    ],
    "abstract": "We introduce a novel method to diminish the problem of out of vocabulary words by introducing an embedding method which leverages the agglutinative property of language. We propose additional embedding derived from syllables and morphemes for the words to improve the performance of language model. We apply the above method to input prediction tasks and achieve state of the art performance in terms of Key Stroke Saving (KSS) w.r.t. to existing device input prediction methods.",
    "full_text": "Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 92–96,\nCopenhagen, Denmark, September 7, 2017.c⃝2017 Association for Computational Linguistics.\nSyllable-level Neural Language Model for Agglutinative Language\nSeunghak Yu∗ Nilesh Kulkarni∗ Haejun Lee Jihie Kim\nSamsung Electronics Co. Ltd., South Korea\n{seunghak.yu, n93.kulkarni, haejun82.lee, jihie.kim}@samsung.com\nAbstract\nLanguage models for agglutinative lan-\nguages have always been hindered in past\ndue to myriad of agglutinations possible\nto any given word through various af-\nﬁxes. We propose a method to diminish the\nproblem of out-of-vocabulary words by in-\ntroducing an embedding derived from syl-\nlables and morphemes which leverages the\nagglutinative property. Our model outper-\nforms character-level embedding in per-\nplexity by 16.87 with 9.50M parameters.\nProposed method achieves state of the art\nperformance over existing input prediction\nmethods in terms of Key Stroke Saving\nand has been commercialized.\n1 Introduction\nRecurrent neural networks (RNNs) exhibit dy-\nnamic temporal behavior which makes them ideal\narchitectures to model sequential data. In recent\ntimes, RNNs have shown state of the art perfor-\nmance on tasks of language modeling (RNN-LM),\nbeating the statistical modeling techniques by a\nhuge margin (Mikolov et al., 2010; Lin et al.,\n2015; Kim et al., 2016; Miyamoto and Cho, 2016).\nRNN-LMs model the probability distribution over\nthe words in vocabulary conditioned on a given in-\nput context. The sizes of these networks are pri-\nmarily dependent on their vocabulary size.\nSince agglutinative languages, such as Korean,\nJapanese, and Turkish, have a huge number of\nwords in the vocabulary, it is considerably hard\nto train word-level RNN-LM. Korean is aggluti-\nnative in its morphology; words mainly contain\ndifferent morphemes to determine the meaning of\nthe word hence increasing the vocabulary size for\nlanguage model training. A given word in Korean\n∗ Equal contribution\ncould have similar meaning with more than 10\nvariations in the sufﬁx as shown in Table 1.\nVarious language modeling methods that rely\non character or morpheme like segmentation of\nwords have been developed (Ciloglu et al., 2004;\nCui et al., 2014; Kim et al., 2016; Mikolov et al.,\n2012; Zheng et al., 2013; Ling et al., 2015). (Chen\net al., 2015b) explored the idea of joint train-\ning for character and word embedding. Morpheme\nbased segmentation has been explored in both\nLarge V ocabulary Continuous Speech Recognition\n(LVCSR) tasks for Egyptian Arabic (Mousa et al.,\n2013) and German newspaper corpus (Cotterell\nand Sch ¨utze, 2015). (Sennrich et al., 2015) used\nsubword units to perform machine translation for\nrare words.\nMorpheme distribution has a relatively smaller\nfrequency tail as compared to the word distribu-\ntion from vocabulary, hence avoids over-ﬁtting for\ntail units. However, even with morpheme segmen-\ntation the percentage of out-of-vocabulary (OOV)\nwords is signiﬁcantly high in Korean. Character\nembedding in Korean is unfeasible as the con-\ntext of the word is not sufﬁciently captured by\nthe long sequence which composes the word. We\nselect as features syllable-level embedding which\nhas shorter sequence length and morpheme-level\nembedding to capture the semantics of the word.\nWe deploy our model for input word predic-\ntion on mobile devices. To achieve desirable per-\nformance we are required to create a model that\nhas as small as possible memory and CPU foot-\nprint without compromising its performance. We\nuse differentiated softmax (Chen et al., 2015a) for\nthe output layer. This method uses more param-\neters for the words that are frequent and less for\nthe ones that occur rarely. We achieve better per-\nformance than existing approaches in terms of Key\nStroke Savings (KSS) (Fowler et al., 2015) and our\napproach has been commercialized.\n92\nWord Morpheme English\n그가 그 + 가 he\n그는 그 + 는 he\n그에게 그 + 에게 to him\n그도 그 + 도 him(he) also\n그를 그 + 를 him\n그의 그 + 의 his\nTable 1: Example of variation of a base word ‘ 그\n(He)’. It can have more than 10 variation forms\naccording to its postposition.\n2 Proposed Method\nFollowing sections propose a model for agglutina-\ntive language. In Section 2.1 we discuss the ba-\nsic architecture of the model as detailed in Fig-\nure 1, followed by Section 2.2 that describes our\nembeddings. In Section 2.3 we propose an adapta-\ntion of differentiated softmax to reduce the num-\nber of model parameters and improve computation\nspeed.\n2.1 Language Model\nOverall architecture of our language model con-\nsists of a) embedding layer, b) hidden layer, c)\nsoftmax layer. Embedding comprises of syllable-\nlevel and morpheme-level embedding as described\nin Section 2.2. We combine both embedding fea-\ntures and pass them through a highway network\n(Srivastava et al., 2015) which act as an input to\nthe hidden layers. We use a single layer of LSTM\nas hidden units with architecture similar to the\nnon-regularized LSTM model by (Zaremba et al.,\n2014). The hidden state of the LSTM unit is afﬁne-\ntransformed by the softmax function, which is a\nprobability distribution over all the words in the\noutput vocabulary.\n2.2 Syllable & Morphological Embedding\nWe propose syllable-level embedding that attenu-\nates OOV problem. (Santos and Zadrozny, 2014;\nKim et al., 2016) proposed character aware neural\nnetworks using convolution ﬁlters to create char-\nacter embedding for words. We use convolution\nneural network (CNN) based embedding method\nto get syllable-level embedding for words. We\nuse 150 ﬁlters that consider uni, bi, tri and quad\nsyllable-grams to create a feature representation\nfor the word. This is followed by max-pooling to\nconcatenate the features from each class of ﬁlters\nresulting in a syllable embedding representation\nHighway Network \nSyllable + Morpheme Embedding \nEmbedding layer \nSoftmax Layer \nDiﬀerenciated Softmax \nHidden Layer \nFigure 1: Overview of the proposed method.T and\nC are the transform gate and carry gate of the high-\nway network respectively\nfor the word. Figure 2 in the left half shows an ex-\nample sentence embedded using the syllable-level\nembedding.\nFigure 3 highlights the difference between vari-\nous embedding and the features they capture. The\nsyllable embedding is used along with a morpho-\nlogical embedding to provide richer features for\nthe word. The majority of words (95%) in Korean\nhas at most three morphological units. Each word\ncan be broken into start, middle, and end unit. We\nembed each morphological unit by concatenating\nto create a joint embedding for the word. Advan-\ntage of morphological embedding over syllable is\nall the sub-units have an abstract value in the lan-\nguage and this creates representation for words re-\nlying on the usage of these morphemes. Both mor-\nphological and syllable embeddings are concate-\nnated and fed through a highway network (Srivas-\ntava et al., 2015) to get a reﬁned representation for\nthe word as shown in the embedding layer for Fig-\nure 1.\n2.3 Differentiated Softmax\nThe output layer models a probability distribution\nover words in vocabulary conditioned on the given\ncontext. There is a trade-off between required\nmemory and computational cost which determines\nthe level of prediction. To generate a complete\nword, using morpheme-level predictions requires\nbeam search which is expensive as compared to\nword-level predictions. Using beam search to pre-\ndict the word greedily does not adhere to the com-\n93\nmax \nMorpheme Level \n\u0001׮\n1SPOPVO\u0001\u0010\u00011PTUQPTJUJPO\u0001\u0001\u0001/PVO\u0001\u0010\u00017FSC \n<\n׮ \n4UBSU\u001f \u001d.JEEMF\u001f \u001d&OE\u001f\nSyllable Level \n<HFV>\u0001\u0010\u0001<OFVO>\u0001\u0001\u0001<IBL>\u0001\u0010\u0001<TBFOH>\u0001\u0010\u0001<J>\u0001\u0010\u0001<EB> \n\u0001׮\n׮\n<\nX\u001f \u001d\u0010X\u001f \nmax \nFigure 2: Proposed embedding method for agglu-\ntinative languages. We take an input word as syl-\nlable and morpheme level, embed them separately\nand concatenate them to make an entire embed-\nding.\nputational requirements set forth for mobile de-\nvices. Thus, we have to choose word-level outputs\nalthough it requires having a vocabulary of over\n0.2M words to cover 95% of the functional word\nforms. Computing a probability distribution func-\ntion for 0.2M classes is computational intensive\nand overshoots the required run-time and the al-\nlocated memory to store the model parameters.\nTherefore, the softmax weight matrix,\nWsoftmax , needs to be compressed as it is\ncontributing to huge model parameters. We\ninitially propose to choose an appropriate rank\nfor the Wsoftmax in the following approximation\nproblem; Wsoftmax = WA ×WB, where WA and\nWB have ranks less than r. We extend the idea of\nlow rank matrix factorization in (Sainath et al.,\n2013) by further clustering words into groups and\nallowing a different low rank r′ for each cluster.\nThe words with high frequency are given a rank,\nr1, such that r1 ≥ r2 where r2 is the low rank\nfor the words with low frequency. The core idea\nbeing, words with higher frequency have much\nricher representation in higher dimensional space,\nwhereas words with low frequency cannot utilize\nthe higher dimensional space well.\nWe observe that 87% of the words appear in the\ntail of the distribution by the frequency of occur-\nrence. We provide a higher rank to the top 2.5%\nwords and much lower rank to the bottom 87%.\nThis different treatment reduces the number of pa-\nWord \nCharacter \nSyllable \nMorpheme \nu\n\u0001׮\n<\nŦ\u0001\u0010\u0001Ɩ\u0001\u0010\u0001ũ\u0001\u0010\u0001Ɩ\u0001\u0010\u0001ũ\u0001\u0001\u0001\nƃ\u0001\u0010\u0001Ƅ\u0001\u0010\u0001Ŧ\u0001\u0010\u0001ź\u0001\u0010\u0001ƅ\u0001\u0010\u0001ż\u0001\u0010\u0001ż\u0001\u0010\u0001Ƙ\u0001\u0010\u0001Ŭ\u0001\u0010\u0001Ƅ\u0001\n<\n\u0001׮\nHFV>\u0001\u0010\u0001<OFVO>\u0001\u0001\u0001<IBL>\u0001\u0010\u0001<TBFOH>\u0001\u0010\u0001<J>\u0001\u0010\u0001<EB> \n<\n\u0001׮\n1SPOPVO\u0001\u0010\u00011PTUQPTJUJPO\u0001\u0001\u0001/PVO\u0001\u0010\u00017FSC \n<\n“He is a student” \nHe is  a student \n<\n<\n<\nH / e   i / s   a   s / t / u / d / e / n / t\n<\n<\n<\nHe   is   a   stu / dent\n<IJ>\u0001\u0001\u0001<*[>\u0001\u0001\u0001< >\u0001\u0001\u0001<TUV\u001b>\u0001\u0010\u0001<EOU>F\n<\n<\n<\nHe   is   a   student\n1SPOPVO\u0001\u0001\u00017FSC\u0001\u0001\u0001*OEFGJOJUF\u0001BSUJDMF\u0001\u0001\u0001/PVO \n<\n<\n<\nFigure 3: Comparison of various embedding lev-\nels. In case of Korean, syllable can be used as a\nbasic unit of sequence to solve OOV with shorter\nsequence length compare to character level. Also,\nmorpheme level is effective to make the size of vo-\ncabulary smaller.\nrameters and leads to better modeling.\n3 Experiment Results\n3.1 Setup\nWe apply our method to web crawled dataset con-\nsisting on news, blogs, QA. Our dataset consists\nof over 100M words and over 10M sentences. For\nmorpheme-level segmentation, we use lexical an-\nalyzer and for syallable-level we just syllabify the\ndataset. We empirically test our model and its in-\nput vocabulary size is around 20K morphemes and\n3K syllables. The embedding size for morpheme is\n52 and that for syllable is 15. We use one highway\nlayer to combine the embeddings from syllable\nand morpheme. Our hidden layer consists of 500\nLSTM units. The differentiated softmax outputs\nthe model’s distribution over the 0.2M words in\nthe output vocabulary with top 5K (by frequency)\ngetting a representation dimension (low rank in\nWsoftmax ) of 152, next 20K use a representation\ndimension of 52 and the rest 175K get a repre-\nsentation dimension of 12. All the compared mod-\nels have word level outputs and use differentiated\nsoftmax.\n3.2 Comparison of embedding methods\nWe randomly select 10% of our crawled data\n(10M words, 1M sentences) to compare embed-\nding methods as shown in Table 2. We test char-\nacter, syllable, morpheme and word-level embed-\ndings. The word-level embedding has the highest\nnumber of parameters but has the worst perfor-\nmance. As expected breaking words into their sub-\nforms improves the language model. However, our\nexperiment reaches its peak performance when we\nuse syllable level embeddings. To improve the per-\nformance even further we propose using syllable\n94\nEmbedding Param. Perplexity V ocab.\nWord 15.72M 327.17 200K\nMorph 6.61M 283.54 20K\nCharacter 8.66M 235.52 40\nSyl 8.71M 231.30 3K\nSyl + Morph 9.50M 218.65 23K\nTable 2: Results of different embedding meth-\nods. Param. : Total model paramerters, V ocab: In-\nput vocabulary size, Syl : Syllable, Morph: Mor-\npheme.\nand morpheme which outperforms all the other ap-\nproaches in terms of perplexity.\n3.3 Performance evaluation\nProposed method shows the best performance\ncompared to other solutions in terms of Key Stroke\nSavings (KSS) as shown in Table 3. KSS is a per-\ncentage of key strokes not pressed compared to a\nvanilla keyboard which does not have any predic-\ntion or completion capabilities. Every user typed\ncharacters using the predictions of the language\nmodel counts as key stroke saving. The dataset 1\nused to evaluate KSS was manually curated to\nmimic user keyboard usage patterns.\nThe results in Table 3 for other commercialized\nsolutions are manually evaluated due to lack of ac-\ncess to their language model. We use three evalu-\nators from inspection group to cross-validate the\nresults and remove human errors. Each evaluator\nperformed the test independently for all the other\nsolutions to reach a consensus. We try to minimize\nuser personalization in predictions by creating a\nnew user proﬁle while evaluating KSS.\nThe proposed method shows 37.62% in terms\nof KSS and outperforms compared solutions. We\nhave achieved more than 13% improvement over\nthe best score among existing solutions which is\n33.20% in KSS. If the user inputs a word with\nour solution, we require on an average 62.38% of\nthe word preﬁx to recommend the intended word,\nwhile other solutions need 66.80% of the same.\nFigure 4 shows an example of word prediction\nacross different solutions. In this example, the pre-\ndictions from other solutions are same irrespective\n1The dataset consists of 67 sentences (825 words,\n7,531 characters) which are collection of formal\nand informal utterances from various sources. It is\navailable at https://github.com/meinwerk/\nSyllableLevelLanguageModel\nDeveloper KSS(%)\nProposed 37.62\nSwiftkey 33.20\nApple 31.90\nSamsung 31.40\nTable 3: Performance comparison of proposed\nmethod and other commercialized keyboard solu-\ntions by various developers.\nContext A\nProposed\nApple\nSwiftKey\nSamsung\nContext B\nProposed\nApple\nSwiftKey\nSamsung\n(rain heavily)\n(too much rice)\nFigure 4: Example of comparison with other com-\nmercialized solutions. Predicted words for the\nContext A (rain heavily) and Context B (too much\nrice). Other solutions make same prediction re-\ngardless of the context (only consider the last two\nwords of context).\nof the context, while the proposed method treats\nthem differently with appropriate predictions.\n4 Conclusion\nWe have proposed a practical method for modeling\nagglutinative languages, in this case Korean. We\nuse syllable and morpheme embeddings to tackle\nlarge portion of OOV problem owing to practical\nlimit of vocabulary size and word-level prediction\nwith differentiated softmax to compress size of\nmodel to a form factor making it amenable to run-\nning smoothly on mobile device. Our model has\n9.50M parameters and achieves better perplexity\nthan character-level embedding by 16.87. Our pro-\nposed method outperforms the existing commer-\ncialized keyboards in terms of key stroke savings\nand has been commercialized. Our commercial-\nized solution combines above model with n-gram\nstatistics to model user behavior thus supporting\npersonalization.\n95\nReferences\nWelin Chen, David Grangier, and Michael Auli. 2015a.\nStrategies for training large vocabulary neural lan-\nguage models. arXiv preprint arXiv:1512.04906 .\nXinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun,\nand Huanbo Luan. 2015b. Joint learning of charac-\nter and word embeddings. In Twenty-Fourth Inter-\nnational Joint Conference on Artiﬁcial Intelligence.\nT Ciloglu, M Comez, and S Sahin. 2004. Language\nmodelling for turkish as an agglutinative language.\nIn Signal Processing and Communications Appli-\ncations Conference, 2004. Proceedings of the IEEE\n12th. IEEE, pages 461–462.\nRyan Cotterell and Hinrich Sch ¨utze. 2015. Morpho-\nlogical word-embeddings. In HLT-NAACL. pages\n1287–1292.\nQing Cui, Bin Gao, Jiang Bian, Siyu Qiu, and Tie-Yan\nLiu. 2014. Learning effective word embedding us-\ning morphological word similarity. arXiv preprint\narXiv:1407.1687 .\nAndrew Fowler, Kurt Partridge, Ciprian Chelba, Xiao-\njun Bi, Tom Ouyang, and Shumin Zhai. 2015. Ef-\nfects of language modeling and its personalization\non touchscreen typing performance. In Proceedings\nof the 33rd Annual ACM Conference on Human Fac-\ntors in Computing Systems. ACM, pages 649–658.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In Thirtieth AAAI Conference on Artiﬁcial\nIntelligence.\nRui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou,\nand Sheng Li. 2015. Hierarchical recurrent neural\nnetwork for document modeling. In EMNLP. pages\n899–907.\nWang Ling, Tiago Lu´ıs, Lu´ıs Marujo, Ram´on Fernan-\ndez Astudillo, Silvio Amir, Chris Dyer, Alan W\nBlack, and Isabel Trancoso. 2015. Finding function\nin form: Compositional character models for open\nvocabulary word representation. arXiv preprint\narXiv:1508.02096 .\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In In-\nterspeech. volume 2, page 3.\nTom´aˇs Mikolov, Ilya Sutskever, Anoop Deoras, Hai-\nSon Le, Stefan Kombrink, and Jan Cernocky.\n2012. Subword language modeling with neu-\nral networks. preprint (http://www. ﬁt. vutbr.\ncz/imikolov/rnnlm/char. pdf).\nYasumasa Miyamoto and Kyunghyun Cho. 2016.\nGated word-character recurrent language model.\narXiv preprint arXiv:1606.01700 .\nAmr El-Desoky Mousa, Hong-Kwang Jeff Kuo, Lidia\nMangu, and Hagen Soltau. 2013. Morpheme-based\nfeature-rich language models using deep neural net-\nworks for lvcsr of egyptian arabic. In Acoustics,\nSpeech and Signal Processing (ICASSP), 2013 IEEE\nInternational Conference on . IEEE, pages 8435–\n8439.\nTara N Sainath, Brian Kingsbury, Vikas Sindhwani,\nEbru Arisoy, and Bhuvana Ramabhadran. 2013.\nLow-rank matrix factorization for deep neural net-\nwork training with high-dimensional output tar-\ngets. In Acoustics, Speech and Signal Processing\n(ICASSP), 2013 IEEE International Conference on.\nIEEE, pages 6655–6659.\nCicero D Santos and Bianca Zadrozny. 2014. Learning\ncharacter-level representations for part-of-speech\ntagging. In Proceedings of the 31st International\nConference on Machine Learning (ICML-14). pages\n1818–1826.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909 .\nRupesh Kumar Srivastava, Klaus Greff, and J ¨urgen\nSchmidhuber. 2015. Highway networks. arXiv\npreprint arXiv:1505.00387 .\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329 .\nXiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013.\nDeep learning for chinese word segmentation and\npos tagging. In EMNLP. pages 647–657.\n96"
}