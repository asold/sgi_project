{
  "title": "The Jazz Transformer on the Front Line: Exploring the Shortcomings of AI-composed Music through Quantitative Measures",
  "url": "https://openalex.org/W3047325651",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287356916",
      "name": "Wu, Shih-Lun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222713164",
      "name": "Yang, Yi-Hsuan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3007429516",
    "https://openalex.org/W2752731013",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963656263",
    "https://openalex.org/W2573741058",
    "https://openalex.org/W3020802295",
    "https://openalex.org/W2903285945",
    "https://openalex.org/W2991377373",
    "https://openalex.org/W1572460559",
    "https://openalex.org/W2343836010",
    "https://openalex.org/W2395084611",
    "https://openalex.org/W2887307793",
    "https://openalex.org/W37402343",
    "https://openalex.org/W3010903955",
    "https://openalex.org/W2989838269",
    "https://openalex.org/W763753110",
    "https://openalex.org/W2982753834",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2902052009",
    "https://openalex.org/W2991108091",
    "https://openalex.org/W2060720037",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2572559421",
    "https://openalex.org/W2741766967",
    "https://openalex.org/W2008765103",
    "https://openalex.org/W652429162",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2898827701",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2963681776",
    "https://openalex.org/W2788156829",
    "https://openalex.org/W2322115941",
    "https://openalex.org/W1544473997"
  ],
  "abstract": "This paper presents the Jazz Transformer, a generative model that utilizes a neural sequence model called the Transformer-XL for modeling lead sheets of Jazz music. Moreover, the model endeavors to incorporate structural events present in the Weimar Jazz Database (WJazzD) for inducing structures in the generated music. While we are able to reduce the training loss to a low value, our listening test suggests however a clear gap between the average ratings of the generated and real compositions. We therefore go one step further and conduct a series of computational analysis of the generated compositions from different perspectives. This includes analyzing the statistics of the pitch class, grooving, and chord progression, assessing the structureness of the music with the help of the fitness scape plot, and evaluating the model's understanding of Jazz music through a MIREX-like continuation prediction task. Our work presents in an analytical manner why machine-generated music to date still falls short of the artwork of humanity, and sets some goals for future work on automatic composition to further pursue.",
  "full_text": "THE JAZZ TRANSFORMER ON THE FRONT LINE:\nEXPLORING THE SHORTCOMINGS OF AI-COMPOSED MUSIC\nTHROUGH QUANTITATIVE MEASURES\nShih-Lun Wu1,2 and Yi-Hsuan Yang2,3\n1 National Taiwan University, 2 Taiwan AI Labs, 3 Academia Sinica, Taiwan\nb06902080@csie.ntu.edu.tw, yang@citi.sinica.edu.tw\nABSTRACT\nThis paper presents the Jazz Transformer, a generative\nmodel that utilizes a neural sequence model called the\nTransformer-XL for modeling lead sheets of Jazz music.\nMoreover, the model endeavors to incorporate structural\nevents present in the Weimar Jazz Database (WJazzD) for\ninducing structures in the generated music. While we are\nable to reduce the training loss to a low value, our lis-\ntening test suggests however a clear gap between the rat-\nings of the generated and real compositions. We there-\nfore go one step further and conduct a series of computa-\ntional analysis of the generated compositions from differ-\nent perspectives. This includes analyzing the statistics of\nthe pitch class, grooving, and chord progression, assess-\ning the structureness of the music with the help of the ﬁt-\nness scape plot, and evaluating the model’s understanding\nof Jazz music through a MIREX-like continuation predic-\ntion task. Our work presents in an analytical manner why\nmachine-generated music to date still falls short of the art-\nwork of humanity, and sets some goals for future work on\nautomatic composition to further pursue.\n1. INTRODUCTION\nMusic is a heart-touching form of art that strikes a chord\nwith people’s emotions, joyful or sorrowful; intense or re-\nlieved, through the twists and turns of notes. Despite its\nubiquity in our everyday lives, the composition and ar-\nrangement of music often requires substantial human ef-\nfort. This is a major reason why automatic music compo-\nsition is such a fascinating ﬁeld of study. Over the years,\nresearchers have sought strenuously ways for machines to\ngenerate well-formed music; such methods include metic-\nulously designed non deep learning-based algorithms like\nthe Markov chains [6] and formal grammars [19]; and, a\nproliferation of deep learning-based solutions in the past\ndecade [8]. In this work, we exclusively study the exten-\nsion and evaluation of Transformer-based models [43] for\nc⃝Shih-Lun Wu and Yi-Hsuan Yang. Licensed under a\nCreative Commons Attribution 4.0 International License (CC BY 4.0).\nAttribution: Shih-Lun Wu and Yi-Hsuan Yang. “The Jazz Transformer\non the Front Line: Exploring the Shortcomings of AI-composed Music\nthrough Quantitative Measures”, 21st International Society for Music In-\nformation Retrieval Conference, Montréal, Canada, 2020.\nFigure 1. The ﬁrst 8 bars of a piece (ﬁlename sample_\nB01.mp3 in Google Drive) composed by the Jazz Trans-\nformer, exhibiting clear rests between phrases.\nits claimed successes in natural language processing and\nmusic generation in recent years [12, 13, 20, 38].\nThe dataset chosen for our work is the Weimar Jazz\nDatabase (WJazzD) [2, 37]. As opposed to the commonly\nused piano MIDIs in recent works [20, 21], the choice of\nthis dataset represents a fresh endeavor to train the Trans-\nformer on Jazz music, and grants us the unique opportu-\nnity to integrate structure-related events, precisely anno-\ntated in the WJazzD, to the model. However, such an at-\ntempt involves no short of technical challenges, including\nthe quantization of the numerous short notes in Jazz impro-\nvisations; and, dealing with the complex chord representa-\ntions used in the WJazzD. In Section 3, we will elaborate\non how these difﬁculties are tackled in a detailed manner.\nFurthermore, while recent works in Transformer-based\nmusic generation often praised the model’s capabilities,\nlike being able to compose “compelling” music, or gen-\nerate pieces with “expressiveness, coherence, and clear\nstructures” as claimed in [20] and [21] respectively, rarely\ndo we admit that the machine is still far behind humans, as\nshown in our user study (Section 4), and take a step back to\n“face the music”, in other words, to identify what exactly\ngoes wrong in the model’s compositions.\nTherefore, the goal of the paper is two-fold. First, to\ndeploy Transformers to a new, more complex music genre,\nJazz, asking the model to compose melody lines, chord\nprogression, and structures all at once. Second, to de-\nvelop a set of objective metrics (Section 5) that evaluate\nthe generated music’s pitch usages, rhythmicity, consis-\ntency in chord progression, and structureness (see Sec. 5.4\nfor deﬁnition), to discover the culprits behind the model’s\nincompetence (Section 6).\narXiv:2008.01307v1  [cs.SD]  4 Aug 2020\nFigure 1 shows an example of a composition generated\nby our model, in which we may ﬁnd reasonable combi-\nnations of chords and melody; and, clear rests between\nphrases. Audio samples can be found in a Google Drive\nfolder, 1 which we encourage readers to listen to. We have\nalso open-sourced our implementation of the Jazz Trans-\nformer 2 and the proposed objective metrics. 3\n2. RELATED WORK\nThere has been a great body of research work on computa-\ntional analysis of human performance of Jazz [3, 4, 15, 18,\n44]. One prominent example is the Jazzomat Project [5],\nwhich established the WJazzD [2] to study the creative pro-\ncesses underpinning Jazz solo improvisations [37]. Weiss\net al. [44], for instance, used the dataset to explore the evo-\nlution of tonal complexity of Jazz improvisations in the\npast century. See Sec. 3.1 for more details of the dataset.\nThe use of Transformer-like architectures for training\nmusic composition models has drawn increasing attention\nrecently. These works enhanced the Transformer’s capa-\nbility in modeling music through relative positional encod-\ning schemes [20, 36], cross-domain pre-training [13], and\nevent token design [13, 21]. To the best of our knowledge,\nthis work represents the ﬁrst attempt in the literature to em-\nploy Transformers to compose exclusively Jazz music.\nAutomatic composition of general lead sheets has been\ninvestigated lately, mostly based on recurrent neural net-\nwork (RNN) models [7, 29, 30]. As for inducing struc-\ntures in the generated music, several RNN-based solutions\nhave also been proposed [24, 31, 39]. Since Transform-\ners have been shown to outperform RNNs in various tasks\n[9, 20, 26], we strive to be the forerunner in bringing them\nto these realms of research.\nRelatively little work has been done to train a model\nfor Jazz composition. JazzGAN [42] is a model employ-\ning a generative adversarial network (GAN) architecture\nfor chord-conditioned melody composition, using a dataset\nof only 44 lead sheets, approximately 1,700 bars. Another\nmodel presented in [22] explores the use of recurrent vari-\national auto-encoders for generating both the melody and\nchords of a lead sheet from scratch.\nA number of objective metrics have been employed\nfor measuring the performance of deep learning for music\ncomposition [10, 14, 41, 45]. However, most of them fo-\ncused on surface-level statistics only (e.g., pitch class his-\ntograms, note onset intervals, etc.). The introduction of\nstructureness indicators and the MIREX-like metric (see\nSec. 5.4–5.5) in this paper provide new insights into as-\nsessing music’s quality at piece level, and evaluating the\nmodel’s overall understanding of a certain music genre.\n3. THE JAZZ TRANSFORMER\nTransformers use self-attention modules to aggregate in-\nformation from the past events when predicting the next\n1 https://drive.google.com/drive/folders/\n1-09SoxumYPdYetsUWHIHSugK99E2tNYD?usp=sharing\n2 https://github.com/slSeanWU/jazz_transformer\n3 https://github.com/slSeanWU/MusDr\n# solos Total\nduration\nTotal #\nevents\nAvg. #\nevents\nper solo\nTrain 409 11h 19m 1,220 K 2,983\nVal. 22 33m 56 K 2,548\nTable 1. Statistics of the dataset we compile from the\nWJazzD [37]. See Section 3.2 for details of the “events”.\nevents [11,27,43]. Accordingly, it is natural that we model\nmusic as a language, namely, to represent each composi-\ntion by a sequence of event tokens. In this section, we\nwill explain in detail how we break down the components\nof the WJazzD [37] to construct the vocabulary of events,\nand how the pieces are converted into sequences that can\nbe fed into a Transformer-like model for training.\n3.1 Dataset\nThe WJazzD dataset [2,37] comprises of 456 monophonic\nJazz solos. Each solo is arranged in the lead sheet style\nand comes with two tracks: the melody track and the beat\ntrack. The melody track contains every note’s pitch, onset\ntime and duration (in seconds), with additional information\non loudness (in decibels), phrase IDs and “midlevel units”\n(MLUs) [17], a structure of ﬁner granularity than a phrase\nto capture the distinctive short-time ideas in Jazz impro-\nvisations. The beat track contains the beat onsets (in sec-\nonds), chord progressions and form parts (or sections, e.g.,\nA1, B1). The highlight of this dataset is that all the con-\ntents, including the notes, chords, metrical and structural\nmarkings, are human-annotated and cross-checked by the\nannotators [37], ensuring the data cleanliness that is often\ncrucial for machine learning tasks. To simplify the subse-\nquent processings, we retain only the pieces marked solely\nwith 4/4 time signature, resulting in 431 solos. For ob-\njective analysis, we leave 5% of the solos as the held-out\nvalidation data. See Table 1 for the statistics of the data.\n3.2 Data Representation\nThe event representation adopted here is a modiﬁed ver-\nsion of the “REvamped MIDI-derived event representa-\ntion” recently proposed in [21], extended to integrate the\nchord system and structural events of WJazzD. The result-\ning event encodings can be broken down into the following\n4 categories: note-related—N OTE -VELOCITY , N OTE -\nON, N OTE -DURATION ; metric-related—B AR, P OSI -\nTION , TEMPO -CLASS , TEMPO ; chord-related—C HORD -\nTONE , C HORD -TYPE , C HORD -SLASH ; and structure-\nrelated—P HRASE , MLU, P ART, REPETITION .\n3.2.1 Note-related Events\nEach note in the melody is represented by three events, i.e.,\nNOTE -VELOCITY , NOTE -ON, and NOTE -DURATION .\nThe NOTE -VELOCITY event decides how hard the note\nshould be played. We derive it according to the esti-\nmated loudness (in decibels) provided by the dataset, and\nquantize it into 32 bins, corresponding to MIDI velocities\n[3, 7, . . . ,127], through v = ⌊\n(\n80 + 3·(dB −65)\n)\n/4⌋,\nwhere dB is the decibel value of the note, and v, clipped\nsuch that v ∈[1, 32], is the resulting NOTE -VELOCITY (v)\nevent. This mapping scheme comes in handy in the process\nof converting the model’s compositions to MIDIs.\nThe N OTE -ON events, ranging from 0 to 127, corre-\nspond directly to the MIDI numbers, indicating the note’s\npitch. The N OTE -DURATION events represent the note’s\nlength in 64th note multiples, ranging from 1 to 32, ob-\ntained by taking the ratio of the note’s duration (in sec-\nonds) to the duration of the beat (also in seconds) where the\nnote situates. The reason why we use such a ﬁne-grained\nquantum, while previous work mainly consider only 16th\nnote multiples (e.g., [20, 21]), is as follows. Most notes\nin WJazzD are quite short, with a signiﬁcant portion being\n32th and 64th notes (12.9% and 2.7% respectively). The\nquantum is chosen such that the coverage of the 32 NOTE -\nDURATION events encompasses the most notes, which is\n99.6% with our choice of the 64th note. 4\n3.2.2 Metric-related Events\nTo model the progression of time, we use a combination of\nBAR and P OSITION events; as demonstrated in [21], this\ncombination leads to clearer rhythmic structure in the gen-\nerated music compared to using TIME -SHIFT events intro-\nduced in [20]. In addition, the pace the music should be\nplayed at is set by TEMPO -CLASS and TEMPO events.\nA BAR event is added at the beginning of each bar, and\na bar is quantized into 64 subunits, each represented by a\nPOSITION event; for example, P OSITION (16) marks the\nstart of the 2nd beat in a bar. A P OSITION event occurs\nwhenever there is a note onset, chord change, or tempo\nchange. It is worth mentioning that to minimize the quan-\ntization error, a note’s onset position is justiﬁed with the\nbeat it is in through the formula:\npn = pb + 16·\n(\ntn −tb\n)\n/db , (1)\nwhere pb, tb, db are the beat’s position (note that pb ∈\n{0, 16, 32, 48}), onset time, and duration; and tn is the\nnote’s onset time. The resulting pn is then rounded to the\nnearest integer to determine the note’s onset position.\nThe T EMPO -CLASS and T EMPO events always co-\noccur at every beat position. The 5 T EMPO -CLASS events\nrepresent the general “feeling” of speed (i.e. fast, or slow)\nwith interval boundaries of [50, 80, 110, 140, 180, 320]\nbeats per minute (bpm), while the 12 T EMPO events as-\nsigned to each tempo class in evenly-spaced steps (within\nthe interval, e.g., 50, 52.5, 55 bpm...) determine the exact\npace. The events can be derived simply by taking the recip-\nrocal of a beat’s duration (provided by WJazzD). The fre-\nquent appearance of these tempo events facilitates smooth\nlocal tempo changes common in Jazz performances.\n3.2.3 Chord-related Events\nChord progressions serve as the harmonic foundation of\nJazz improvisations [25], hence a complex chord represen-\ntation system is used in the WJazzD dataset. If we were to\n4 All notes shorter than a 64th note are discarded and those longer than\na half note are clipped.\ntreat each of the 418 unique chord representations present\nin the WJazzD as a token, the majority of chord tokens will\nhave very few occurrences—in fact, 287 (69%) of them ap-\npear in less than 5 solos, making it hard for the model to\nlearn the meaning of those chords well; plus, the process\nof translating chords to individual notes during the conver-\nsion to MIDIs would be extremely cumbersome.\nFortunately, thanks to the detailed clariﬁcation provided\nin [37], we are able to decompose each chord into 3 events,\nnamely, the C HORD -TONE , C HORD -TYPE , and C HORD -\nSLASH events, with the help of regular expressions (regex)\nand some rule-based exception handling.\nThe 12 CHORD -TONE events, one for each note on the\nchromatic scale (i.e. C, C#, D, ...), determine the root note,\nhence the tonality, of the chord. The 47 C HORD -TYPE\nevents affect the chord’s quality and emotion by the differ-\nent combination of notes played relative to the root note\n(or, key template as we call it); e.g., the key template of\na Dominant 7th chord (C HORD -TYPE (7)) is [0, 4, 7, 10].\nFinally, the 12 C HORD -SLASH events allow the freedom\nto alter the bass note to slightly tweak the chord’s quality.\nIf a chord contains no slash, its CHORD -SLASH event will\nshare the same key as its CHORD -TONE . For instance, the\nchord C7/G, a C Dominant 7th over G, is represented by\n[CHORD -TONE (C), CHORD -TYPE (7), CHORD -SLASH (G)].\nNote that after our decomposition, the number of unique\nchord-related events is greatly reduced to 71; and, the re-\nsulting set of events is still able to represent all 418 chords\nin WJazzD. It is easy to use the manually-constructed key\ntemplate accompanying each CHORD -TYPE , together with\nthe CHORD -TONE and CHORD -SLASH events to map each\nchord to notes during the conversion to MIDIs.\n3.2.4 Structure-related Events\nFor the melodies, we prepend a PHRASE event to the notes\nmarked as the start of a phrase. The presence of phrases\nmay be important as it informs the model to “take a breath”\nbetween streams of notes. And, we retain several common\ntypes and subtypes of midlevel units (e.g., line, rhythm,\nlick etc.) as MLU events [17], likewise prepended to the\nstarting note of each MLU, hoping that the model could\ncapture the short-term note patterns described by the MLU\ntypes. P ART and R EPETITION events are added to each\nbeginning and end of a form part, 5 guiding the model to\ngenerate repetitive chord progression and coherent melody\nlines for the parts marked with the same letter.\n3.3 Model and Training Setups\nDue to the large number of events per solo (check Table\n1), it is hard to feed the entire pieces into a Transformer at\nonce because of memory constraint. Therefore, we choose\nas the backbone sequence model the Transformer-XL [11],\nan improved variant of the Transformer which introduces\nrecurrence to the architecture. It remedies the memory\nconstraint and the resulting context fragmentation issue by\ncaching the computation record of the last segment, and\n5 For example, the entire A1 part is represented as [PART-START(A),\nREPETITION -START(1), . . .other events . . ., R EPETITION -END(1),\nPART-END(A) ].\nallowing the current segment to attend to the cache in the\nself-attention process. This allows information to ﬂow\nacross the otherwise separated segments, inducing better\ncoherence in the generated music.\nTo evaluate the effectiveness of adding the structure-\nrelated events (cf. Section 3.2.4), we consider the follow-\ning two variants in our objective analysis:\n• Model (A): trained with no structure-related events.\n• Model (B): trained with the complete set of events.\nThey both consist of 12 layers, 8 attention heads and about\n41 million learnable parameters. We train them on a single\nNVIDIA GTX 1080-Ti GPU (with 11 GB memory) with\nAdam optimizer, learning rate 1e−4, batch size 8, segment\nlength 512 and 100% teacher forcing. Besides, following\nthe Music Transformer’s data augmentation setting [20],\nwe randomly transpose each solo in the range of −3 to\n+3 keys in every epoch. It takes roughly a full day for\nthe negative log-likelihood losses of the models to drop to\n0.25, a level at which they are able to produce music of\ndistinctive Jazz feeling (see Section 6 for justiﬁcations).\n4. SUBJECTIVE STUDY\nTo discover how users feel about the Jazz Transformer’s\ncompositions, we set up a blind listening test in which\ntest-takers listen to four one-minute long pieces, two from\nthe Model (B)’s compositions (at loss level 0.25), and two\nfrom real data. We do not include Model (A) here to re-\nduce the burden on the test-takers, assuming that Model\n(B) is better. We inform them that the pieces are indepen-\ndent of one another, and they will be asked the same set of\nquestions after listening to each piece, namely, to rate it in\na ﬁve-point Likert scale on the following aspects:\n• Overall Quality (O):Does it sound good overall?\n• Impression (I):Can you remember a certain part or\nthe melody?\n• Structureness (S):Does it involve recurring music\nideas, clear phrases, and coherent sections?\n• Richness (R):Is the music diverse and interesting?\nWe distribute ﬁve test suites to our social circles and col-\nlect responses from 59 anonymized subjects, of which 27\nare classiﬁed as “pros” for they rate their musical back-\nground (in general, not restricted to Jazz) as 4/5 or 5/5\n(i.e., also on a ﬁve-point scale). The result shown in Fig-\nure 2 indicates that the Jazz Transformer receives mediocre\nscores and falls short of humans in every aspect, especially\nin overall quality(O) and structureness (S). Moreover, per-\nformed one-tailed Z-tests for the difference of means also\nsuggests the signiﬁcance of the gaps ( p <0.05 for all as-\npects), providing concrete evidence of the model’s defeat.\n5. OBJECTIVE EV ALUATION METRICS\nThe result of our subjective study poses to us an intriguing\nquestion: If the machine is still inferior to humans in creat-\ning music, then what exactly are the causes? To unravel the\nO I S R\nAspect\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0Score\n2.94\n2.7 2.69\n2.9\n3.4\n2.98\n3.14 3.17\nAll Responses (N=59)\nOurs\nReal\nFigure 2. Result of subjective study ( O: Overall Quality,\nI: Impression, S: Structureness, R: Richness), comparing\nfrom-scratch compositions created by the proposed model\nwith structure-related events (i.e., ‘Model (B)’) against the\nreal pieces from the WJazzD. We note that the gaps in all\naspects are statistically signiﬁcant (p <0.05).\nmystery, we develop a set of objective metrics which en-\nables us to scrutinize the Jazz Transformer’s compositions\nfrom various perspectives, and make comparisons with real\ndata. These metrics include the analyses of event distribu-\ntions, namely, the pitch class histogram, the grooving pat-\ntern, and the chord progressions; assessing the structure-\nness with the help of the ﬁtness scape plot; and, judging\nthe model’s performance on a discriminative task through\nthe MIREX-like continuation prediction challenge.\n5.1 Pitch Class Histogram Entropy\nTo gain insight into the usage of different pitches, we ﬁrst\ncollect the notes appeared in a certain period (e.g., a bar)\nand construct the 12-dimensional pitch class histogram− →h ,\naccording to the notes’ pitch classes (i.e.C, C#, ..., A#, B),\nnormalized by the total note count in the period such that∑\ni hi = 1. Then, we calculate the entropy of − →h :\nH(− →h ) =−\n11∑\ni=0\nhi log2(hi) . (2)\nThe entropy, in information theory, is a measure of “uncer-\ntainty” of a probability distribution [40], hence we adopt\nit here as a metric to help assessing the music’s quality in\ntonality. If a piece’s tonality is clear, several pitch classes\nshould dominate the pitch histogram (e.g., the tonic and\nthe dominant), resulting in a low-entropy − →h ; on the con-\ntrary, if the tonality is unstable, the usage of pitch classes\nis likely scattered, giving rise to an − →h with high entropy.\n5.2 Grooving Pattern Similarity\nThe grooving pattern represents the positions in a bar at\nwhich there is at least a note onset, denoted by − →g , a 64-\ndimensional binary vector in our setting. 6 We deﬁne the\nsimilarity between a pair of grooving patterns − →g a, − →g b as:\nGS(− →g a, − →g b) = 1−1\nQ\nQ−1∑\ni=0\nXOR(ga\ni , gb\ni ) , (3)\n6 For example, if a bar contains only two note onsets, at the beginning\nof the 1st beat and 2nd beat respectively, then the corresponding − →g will\nhave g0, g16 = 1, and the rest dimensions 0.\nwhere Q is the dimensionality of− →g a, − →g b, and XOR(·, ·) is\nthe exclusive OR operation. Note that the value of GS(·, ·)\nwould always lie in between 0 and 1.\nThe grooving pattern similarity helps in measuring the\nmusic’s rhythmicity. If a piece possesses a clear sense of\nrhythm, the grooving patterns between pairs of bars should\nbe similar, thereby producing high GSscores; on the other\nhand, if the rhythm feels unsteady, the grooving patterns\nacross bars should be erratic, resulting in low GSscores.\n5.3 Chord Progression Irregularity\nTo measure the irregularity of a chord progression, we be-\ngin by introducing the termchord trigram, which is a triple\ncomposed of 3 consecutive chords in a chord progression;\nfor example, (Dm7, G7, CM7). Then, the chord progression\nirregularity (CPI) is deﬁned as the percentage of unique\nchord trigrams in the chord progression of an entire piece.\nPlease note that 2 chord trigrams are considered different\nif any of their elements does not match.\nIt is common for Jazz compositions to make use of 8-\nor 12-bar-long templates of chord progressions (known as\nthe 8-, or 12-bar blues), which themselves can be broken\ndown into similar substructures [25, 35], as the foundation\nof a section, and more or less “copy-paste” them to form\nthe complete song with, say, AABA parts. Therefore, a\nwell-composed Jazz piece should have a chord progression\nirregularity that is not too high.\n5.4 Structureness Indicators\nThe structureness of music is induced by the repetitive mu-\nsical content in the composition. It can involve multiple\ngranularities, ranging from an instant musical idea to an\nentire section. From a psychological perspective, the ap-\npearance of repeated structures is the essence of the catch-\niness and the emotion-provoking nature of music [28].\nThe ﬁtness scape plot algorithm [32, 33] and the asso-\nciated SM Toolbox [34] offer an aesthetic way of detect-\ning and visualizing the presence of repeating structures in\nmusic. The ﬁtness scape plot is a matrix SN×N , 7 where\nsij ∈[0, 1] is the ﬁtness, namely, the degree of repeat in the\npiece derived from the self-similarity matrix (SSM) [16],\nof the segment speciﬁed by (i, j).\nOur structureness indicator is based on the ﬁtness scape\nplot and designed to capture the most salient repeat within\na certain duration interval. For brevity of the mathematical\nrepresentation, we assume the sampling frame rate of S is\n1 Hz (hence N will be the piece’s duration in seconds), and\ndeﬁne the structureness indicator as follows:\nSIu\nl (S) = max\nl≤i≤u\n1≤j≤N\nS , (4)\nwhere l, u8 are the lower and upper bounds of the dura-\ntion interval (in seconds) one is interested in. In our ex-\nperiments, we choose the structureness indicators of SI8\n3,\nSI15\n8 , and SI15 to examine the short-, medium-, and long-\nterm structureness respectively.\n7 N is the number of frames sampled from the audio of a piece, the\n1st axis represents the segment duration (in frames), and the 2nd axis\nrepresents the center of segment (in frames).\n8 If present, otherwise l defaults to 1, and u defaults to N.\n1.2 0.8 0.4 0.25 0.1\nTraining loss level\n0.65\n0.70\n0.75\n0.80\n0.85Accuracy\nMIREX-like Continuation Prediction Challenge\nModel (A)\nModel (B)\nFigure 3. Result of the MIREX-like continuation predic-\ntion challenge, each checkpoint is asked 100 questions.\nNotice that the accuracy of both Model (A) and (B) peaks\nat the loss level of 0.25, at 80% and 83% respectively.\n5.5 MIREX-like Continuation Prediction Challenge\nBeing inspired by the “Patterns for Prediction Challenge”\nheld as part of the Music Information Retrieval Evaluation\neXchange (MIREX) 2019 [1, 23], we developed a method\nto test the model’s capability to predict the correct continu-\nation given a musical prompt. The challenge is carried out\nas follows: First, the model is fed with the beginning 8 bars\nof a piece, denoted by − →s ; then, it is presented with a set\nof four 8-bar continuations X = {− →x 0, − →x 1, − →x 2, − →x 3}, in\nwhich one is the true continuation, and the rest are wrong\nanswers randomly drawn from other pieces. The way the\nmodel attempts to answer the multiple choice question is\nby calculating the average probability of generating the\nevents of each continuation:\nP(− →x i) = 1\nL\nL−1∑\nj=0\np(xi\nj |˜xj−1, . . . ,˜x0; − →s ), i∈{0, 1, 2, 3},\n(5)\nwhere L is the length of the shortest given continuation\n(in # events) in X, xi\nj is the j-th event token in − →x i,\nand ˜xj−1, . . . ,˜x0 are the events sampled from the model’s\noutput, hence the conditional probability p(xi\nj) at each\ntimestep can be obtained straightforward. Finally, the\nmodel returns arg maxi P(− →x i) as its answer, of which the\ncorrectness we can check.\nIf the model can achieve high accuracy on this continu-\nation prediction task, we may say it possesses a good over-\nall understanding of Jazz music, enough for it to tell right\nfrom wrong when given multiple choices.\n6. EXPERIMENT RESULTS AND DISCUSSIONS\nWe begin with the evaluation on the MIREX-like chal-\nlenge (Section 5.5). We pick 5 checkpoints of both Model\n(A) and Model (B) at different training loss levels to ask\neach of them 100 multiple choice questions (the prompt\nand continuation choices of each question are randomly\ndrawn from the held-out validation data). The result shown\nin Figure 3 indicates that, similarly for both models, the\naccuracy steadily goes up as the training loss decreases,\npeaks at the loss level of 0.25, and drops afterwards. This\nshows that the models are gradually gaining knowledge\nabout Jazz music along the training process until a certain\npoint, where they potentially start to overﬁt.\nModel (A) Model (B) Real\nloss 0.80 0.25 0.80 0.25 0.10 - -\nH1 2.29 2.45 2.26 2.20 2.17 1.94\nH4 3.12 3.05 3.04 2.91 2.94 2.87\nGS 0.76 0.69 0.75 0.76 0.76 0.86\nCPI 81.2 77.6 79.2 72.6 75.9 40.4\nSI8\n3 0.18 0.22 0.25 0.27 0.26 0.36\nSI15\n8 0.15 0.17 0.18 0.18 0.17 0.36\nSI15 0.11 0.14 0.10 0.12 0.11 0.35\nTable 2. Results of objective evaluations. H1, H4 are the\n1-, and 4-bar pitch class histogram entropy (see Sec. 5.1);\nGSis the grooving pattern similarity (Sec. 5.2) measured\non all pairs of bars within a piece; CPI is the chord pro-\ngression irregularity (in %; Sec. 5.3); ﬁnally, SI8\n3, SI15\n8 ,\nand SI15 are the short-, medium-, and long-term structure-\nness indicators (Sec. 5.4). Bold texts indicate the model\ncheckpoint performing the closest to real data, which is\nconsidered to be the best. It is observed that Model (B)\n(i.e., the model trained with structure-related events) with\na loss of 0.25 outperforms its counterparts at other loss lev-\nels and Model (A) on most of the metrics. Moreover, con-\nsistent with the result of the MIREX-like challenge (Fig.\n3), the performance of Model (B) plunges when the loss\ngoes too low (0.1 in this case).\nFollowing the MIREX-like challenge, we pick several\ncheckpoints of both Models (A) and (B) for objective eval-\nuations described in Sections 5.1–5.4. The chosen check-\npoints are at loss levels 0.8, 0.25, and 0.1 (for Model (B)\nonly, since in the MIREX-like challenge (Fig. 3), its ac-\ncuracy drastically drops when the loss reduces from 0.25\nto 0.1). In the experiments, 50 32-bar-long from-scratch\ncompositions from each checkpointed model are compared\nagainst the 409 pieces in the training dataset.\nFrom the results (Table 2), we can summarize the\nmodel’s deﬁciencies as follows: 1) the erraticity of the\ngenerated musical events; and, 2) the absence of medium-\nand long-term repetitive structures. Comparing with the\nreal data, the ﬁrst argument can be justiﬁed by the higher\nH1 and H4, manifesting the unstable usage of pitches at\nlocal scale; and, the lower GSand higher CPI of the en-\ntire pieces, marking the lack of consistency in rhythm and\nharmony from a global point of view; meanwhile, the sec-\nond argument can be explained directly by the signiﬁcantly\nlower values of structureness indicators SI15\n8 and SI15,\nsuggesting that while the model might be able to repeat\nsome short fragments of music, creating structures of a\nlonger time span is still beyond its capability.\nMuch to our delight, the introduction of structure-\nrelated events seems to be functional to some extent, no-\nticeable from the numbers that Model (B) at 0.25 loss level\nis for most of the time the closest competitor to humans,\nwith a substantial lead on the metrics focusing on shorter\ntimespans (i.e., H1, H4, and SI8\n3) when placed in compar-\nison with Model (A). This suggests that the use of PHRASE\nand MLU events provides some assistance to the model in\nmodeling music. Furthermore, resonating with the accu-\n0 10 20 30 40 50 60\nSegment center (sec.)\n0\n10\n20\n30\n40\n50\n60Segment duration (sec.)\nBest of Model (B)\n0 10 20 30 40 50 60\nSegment center (sec.)\n0\n10\n20\n30\n40\n50\n60\nReal Piece\n0.0\n0.1\n0.2\n0.3\n0.4\nFitness\nFigure 4. The ﬁtness scape plots of Model (B)’s best com-\nposition (according to the structureness (S) score in our\nsubjective study, see Sec. 4) versus a human composition\nin the WJazzD. Note that the piece by Model (B) contains\nalmost no signs of repetition longer than 10 seconds, while\nthe real piece’s repetitive structures extend well into the\n20–30 seconds range.\nracy trend in the MIREX-like challenge, the performance\nworsens when the loss is reduced to an overly low level.\nTo visualize the deﬁciency in structureness of the\nmodel’s compositions, we choose the piece which scores\nthe highest, 3.14, in the structureness (S) aspect in our\nsubjective study; and, a human composition of the same\nduration, receiving 3.54 in the aspect S, for a head-to-\nhead comparison of their ﬁtness scape plots. The rivalry\n(see Figure 4) reveals the stark contrast between their ﬁt-\nness values across all timescales. In the model’s work, all\ntraces of repetitive structures disappear at the timescale of\n10 seconds; whereas in the human composition, not only\ndo the ﬁtness values stay high in longer timespans, but a\nclear sense of section is also present, as manifested by the\n2 large, dark “triangles” in its scape plot.\n7. CONCLUSION AND FUTURE WORK\nIn this paper, we have presented the Jazz Transformer,\nwhose incorporation of structure-related events has been\nshown useful here in enhancing the quality of machine-\ngenerated music. Moreover, we have proposed a series of\nobjective metrics that shed light on the shortcomings of\nmachine-composed pieces, including the erratic usage of\npitch classes, inconsistent grooving pattern and chord pro-\ngression; and, the absence of repetitive structures. These\nmetrics not only show that the Transformer is in fact not\nthat good a music composer, but also serve as effective\nquantitative measures for future efforts in automatic music\ncomposition to assess their models’ performance, which\nby now still relies heavily on human evaluation.\nIn the future, we plan to carry out larger-scale stud-\nies to explicate the correlations between those quantita-\ntive metrics and the aspects of subjective evaluation; and,\nto continue working on inducing structures in machine-\ncomposed music; such endeavors may not stay on revamp-\ning events that ﬁt into Transformers as done, but involve\na complete redesign of the Transformer architecture, en-\nabling it to read the structural information directly com-\nputable from data, say, the ﬁtness scape plot, to grasp the\nblueprint of a piece before composing music at ﬁner scales.\n8. ACKNOWLEDGEMENTS\nThe authors would like to express the utmost gratitude\nto the Jazzomat Research Project(University of Mu-\nsic FRANZ LISZT Weimar), for compiling the WJazzD\ndataset and making it publicly available; Wen-Yi Hsiao\n(Taiwan AI Labs), for rendering the MIDIs to audios for\nsubjective study; and Yi-Jen Shih(National Taiwan Uni-\nversity), for the help in arranging our open-source codes.\n9. REFERENCES\n[1] The “Patterns for Prediction Challenge” of Music\nInformation Retrieval Evaluation eXchange. [Online]\nhttps://www.music-ir.org/mirex/wiki/\n2019:Patterns_for_Prediction.\n[2] The Weimar Jazz Database. [Online] https://\njazzomat.hfm-weimar.de/.\n[3] Jakob Abeßer, Stefan Balke, Klaus Frieler, Martin\nPﬂeiderer, and Meinard Müller. Deep learning for Jazz\nwalking bass transcription. In Proc. AES International\nConference on Semantic Audio, 2017.\n[4] Jakob Abeßer, Estefanía Cano, Klaus Frieler, Martin\nPﬂeiderer, and Wolf-Georg Zaddach. Score-informed\nanalysis of intonation and pitch modulation in Jazz\nsolos. In Proc. International Society for Music Infor-\nmation Retrieval Conference (ISMIR), pages 823–829,\n2015.\n[5] Jakob Abeßer, Klaus Frieler, Martin Pﬂeiderer, and\nWolf-Georg Zaddach. Introducing the Jazzomat project\n– Jazz solo analysis using music information re-\ntrieval methods. In Proc. International Symposium on\nComputer Music Multidisciplinary Research (CMMR),\n2013.\n[6] Christopher Anderson, Arne Eigenfeldt, and Philippe\nPasquier. The generative electronic dance music algo-\nrithmic system (GEDMAS). In Proc. Artiﬁcial Intel-\nligence and Interactive Digital Entertainment Confer-\nence, 2013.\n[7] Cedric De Boom, Stephanie Van Laere, Tim Verbelen,\nand Bart Dhoedt. Rhythm, chord and melody gener-\nation for lead sheets using recurrent neural networks.\narXiv preprint arXiv:2002.10266, 2020.\n[8] Jean-Pierre Briot, Gaëtan Hadjeres, and François Pa-\nchet. Deep Learning Techniques for Music Genera-\ntion, Computational Synthesis and Creative Systems .\nSpringer, 2019.\n[9] Tsung-Ping Chen and Li Su. Harmony Transformer:\nIncorporating chord segmentation into harmony recog-\nnition. In Proc. International Society for Music Infor-\nmation Retrieval Conference (ISMIR), pages 259–267,\n2019.\n[10] Ching-Hua Chuan and Dorien Herremans. Modeling\ntemporal tonal relations in polyphonic music through\ndeep networks with a novel image-based representa-\ntion. In Proc. AAAI Conference on Artiﬁcial Intelli-\ngence, 2018.\n[11] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime\nCarbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proc. Annual Meeting of\nthe Association for Computational Linguistics (ACL) ,\npages 2978–2988, 2019.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding.\narXiv preprint arXiv:1810.04805, 2018.\n[13] Chris Donahue, Huanru Henry Mao, Yiting Ethan Li,\nGarrison W. Cottrell, and Julian McAuley. LakhNES:\nImproving multi-instrumental music generation with\ncross-domain pre-training. In Proc. International So-\nciety for Music Information Retrieval (ISMIR) , pages\n685–692, 2019.\n[14] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-\nHsuan Yang. MuseGAN: Multi-track sequential gener-\native adversarial networks for symbolic music genera-\ntion and accompaniment. In Proc. AAAI Conference on\nArtiﬁcial Intelligence, pages 34–41, 2018.\n[15] Vsevolod Eremenko, Emir Demirel, Baris Bozkurt,\nand Xavier Serra. Audio-aligned Jazz harmony dataset\nfor automatic chord transcription and corpus-based re-\nsearch. In Proc. International Conference on Music In-\nformation Retrieval (ISMIR), pages 483–490, 2018.\n[16] Jonathan Foote. Visualizing music and audio using\nself-similarity. In Proc. ACM International Conference\non Multimedia, pages 77–80, 1999.\n[17] Klaus Frieler, Martin Pﬂeiderer, Wolf-Georg Zaddach,\nand Jakob Abeßer. Midlevel analysis of monophonic\nJazz solos: A new approach to the study of improvisa-\ntion. Musicae Scientiae, 20:143–162, 2016.\n[18] Jeff Gregorio and Youngmoo Kim. Phrase-level audio\nsegmentation of jazz improvisations informed by sym-\nbolic data. In Proc. International Society for Music In-\nformation Retrieval Conference (ISMIR), 2016.\n[19] Ryan Groves. Automatic melodic reduction using a su-\npervised probabilistic context-free grammar. In Proc.\nInternational Society for Music Information Retrieval\nConference (ISMIR), 2016.\n[20] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Ian Simon, Curtis Hawthorne, Noam\nShazeer, Andrew M. Dai, Matthew D. Hoffman, Mon-\nica Dinculescu, and Douglas Eck. Music Transformer:\nGenerating music with long-term structure. In Proc.\nInternational Conference on Learning Representations\n(ICLR), 2019.\n[21] Yu-Siang Huang and Yi-Hsuan Yang. Pop Music\nTransformer: Beat-based modeling and generation of\nexpressive Pop piano compositions. In Proc. ACM In-\nternational Conference on Multimedia, 2020.\n[22] Hsiao-Tzu Hung, Chung-Yang Wang, Yi-Hsuan Yang,\nand Hsin-Min Wang. Improving automatic jazz melody\ngeneration by transfer learning techniques. In Proc.\nAsia Paciﬁc Signal and Information Processing Associ-\nation Annual Summit and Conf. (APSIPA ASC), 2019.\n[23] Berit Janssen, Tom Collins, and Iris Ren. Algorithmic\nability to predict the musical future: Datasets and eval-\nuation. In Proc. International Society for Music Infor-\nmation Retrieval Conference (ISMIR), pages 208–215,\n2019.\n[24] Harsh Jhamtani and Taylor Berg-Kirkpatrick. Model-\ning self-repetition in music generation using structured\nadversaries. In Proc. Machine Learning for Media Dis-\ncovery Workshop, extended abstract, 2019.\n[25] Philip Johnson-Laird. How Jazz musicians improvise.\nMusic Perception — MUSIC PERCEPT , 19:415–442,\n2002.\n[26] Shigeki Karita et al. A comparative study on Trans-\nformer vs RNN in speech applications. In Proc.\nIEEE Automatic Speech Recognition and Understand-\ning Workshop, pages 449–456, 2019.\n[27] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. Transformers are RNNs:\nFast autoregressive transformers with linear attention.\nIn Proc. International Conference on Machine Learn-\ning, 2020.\n[28] Daniel J. Levitin. This is Your Brain on Music: The\nScience of a Human Obsession. Dutton, 2006.\n[29] Hyungui Lim, Seungyeon Rhyu, and Kyogu Lee.\nChord generation from symbolic melody using\nBLSTM networks. In Proc. International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 621–627, 2017.\n[30] Hao-Min Liu, Meng-Hsuan Wu, and Yi-Hsuan Yang.\nLead sheet generation and arrangement via a hybrid\ngenerative model. In Proc. International Society for\nMusic Information Retrieval Conference (ISMIR), late-\nbreaking demo, 2018.\n[31] Gabriele Medeot, Srikanth Cherla, Katerina Kosta,\nMatt McVicar, Samer Abdallah, Marco Selvi,\nEd Newton-Rex, and Kevin Webster. StructureNet:\nInducing structure in generated melodies. In Proc.\nInternational Society for Music Information Retrieval\nConference (ISMIR), pages 725–731, 2018.\n[32] Meinard Müller, Peter Grosche, and Nanzhu Jiang. A\nsegment-based ﬁtness measure for capturing repetitive\nstructures of music recordings. In Proc. International\nConference on Music Information Retrieval (ISMIR) ,\npages 615–620, 2011.\n[33] Meinard Müller and Nanzhu Jiang. A scape plot rep-\nresentation for visualizing repetitive structures of mu-\nsic recordings. In Proc. International Conference on\nMusic Information Retrieval (ISMIR) , pages 97–102,\nPorto, Portugal, 2012.\n[34] Meinard Müller, Nanzhu Jiang, and Harald G. Gro-\nhganz. SM Toolbox: MATLAB implementations for\ncomputing and enhancing similarity matrices. In Proc.\nAudio Engineering Society (AES), 2014.\n[35] Simon John Nelson. Melodic improvisation on a twelve\nbar blues model: an investigation of physical and his-\ntorical aspects and their contribution to performance .\nPhD thesis, City University London, 2001.\n[36] Christine McLeavy Payne. MuseNet. OpenAI Blog ,\n2019.\n[37] Martin Pﬂeiderer, Klaus Frieler, Jakob Abeßer, Wolf-\nGeorg Zaddach, and Benjamin Burkhart, editors. In-\nside the Jazzomat — New Perspectives for Jazz Re-\nsearch. Schott Campus, 2017.\n[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language mod-\nels are unsupervised multitask learners. Open AI Blog,\n1(8), 2019.\n[39] Shakeel Raja. Music generation with temporal struc-\nture augmentation. arXiv preprint arXiv:2004.10246 ,\n2020.\n[40] Claude E Shannon. A mathematical theory of commu-\nnication. Bell system technical journal, 27(3):379–423,\n1948.\n[41] Bob L. Sturm and Oded Ben-Tal. Taking the models\nback to music practice: Evaluating generative tran-\nscription models built using deep learning. Journal of\nCreative Music Systems, 2(1), 2017.\n[42] Nicholas Trieu and Robert M. Keller. JazzGAN: Im-\nprovising with generative adversarial networks. In\nProc. International Workshop on Musical Metacre-\nation, 2018.\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nIn Proc. Advances in Neural Information Processing\nSystems (NIPS), pages 5998–6008, 2017.\n[44] Christof Weiss, Stefan Balke, Jakob Abeßer, and\nMeinard Müller. Computational corpus analysis: A\ncase study on Jazz solos. InProc. International Society\nfor Music Information Retrieval Conference (ISMIR) ,\npages 416–423, 2018.\n[45] Li-Chia Yang and Alexander Lerch. On the evaluation\nof generative models in music. Neural Computing and\nApplications, 2018.",
  "topic": "Jazz",
  "concepts": [
    {
      "name": "Jazz",
      "score": 0.8473930954933167
    },
    {
      "name": "Transformer",
      "score": 0.6906943917274475
    },
    {
      "name": "Computer science",
      "score": 0.5840886235237122
    },
    {
      "name": "Generative grammar",
      "score": 0.4680335819721222
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4616685211658478
    },
    {
      "name": "Chord (peer-to-peer)",
      "score": 0.45787379145622253
    },
    {
      "name": "Active listening",
      "score": 0.4475376307964325
    },
    {
      "name": "Machine learning",
      "score": 0.3809363842010498
    },
    {
      "name": "Visual arts",
      "score": 0.20674464106559753
    },
    {
      "name": "Engineering",
      "score": 0.18882521986961365
    },
    {
      "name": "Art",
      "score": 0.1683075726032257
    },
    {
      "name": "Psychology",
      "score": 0.13365432620048523
    },
    {
      "name": "Communication",
      "score": 0.1074993908405304
    },
    {
      "name": "Voltage",
      "score": 0.0972299575805664
    },
    {
      "name": "Electrical engineering",
      "score": 0.0904531180858612
    },
    {
      "name": "Distributed computing",
      "score": 0.0
    }
  ]
}