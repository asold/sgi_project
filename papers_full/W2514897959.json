{
  "title": "A Trainable Spaced Repetition Model for Language Learning",
  "url": "https://openalex.org/W2514897959",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2911267484",
      "name": "Burr Settles",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1489662489",
      "name": "Brendan Meeder",
      "affiliations": [
        "Finnish Medical Society Duodecim"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4235351818",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2049428464",
    "https://openalex.org/W2119335255",
    "https://openalex.org/W2030007850",
    "https://openalex.org/W2461892043",
    "https://openalex.org/W1504264720",
    "https://openalex.org/W2026480899",
    "https://openalex.org/W2029507764",
    "https://openalex.org/W2058166043",
    "https://openalex.org/W2089365333",
    "https://openalex.org/W2126807068",
    "https://openalex.org/W2056513446",
    "https://openalex.org/W2049260718",
    "https://openalex.org/W2304127486",
    "https://openalex.org/W2084129588",
    "https://openalex.org/W1988737017",
    "https://openalex.org/W4248215222",
    "https://openalex.org/W4239813528",
    "https://openalex.org/W2139448469",
    "https://openalex.org/W234935650",
    "https://openalex.org/W2748755256",
    "https://openalex.org/W2158698691"
  ],
  "abstract": "We present half-life regression (HLR), a novel model for spaced repetition practice with applications to second language acquisition.HLR combines psycholinguistic theory with modern machine learning techniques, indirectly estimating the \"halflife\" of a word or concept in a student's long-term memory.We use data from Duolingo -a popular online language learning application -to fit HLR models, reducing error by 45%+ compared to several baselines at predicting student recall rates.HLR model weights also shed light on which linguistic concepts are systematically challenging for second language learners.Finally, HLR was able to improve Duolingo daily student engagement by 12% in an operational user study.",
  "full_text": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1848–1858,\nBerlin, Germany, August 7-12, 2016.c⃝2016 Association for Computational Linguistics\nA Trainable Spaced Repetition Model for Language Learning\nBurr Settles∗\nDuolingo\nPittsburgh, PA USA\nburr@duolingo.com\nBrendan Meeder†\nUber Advanced Technologies Center\nPittsburgh, PA USA\nbmeeder@cs.cmu.edu\nAbstract\nWe present half-life regression (HLR), a\nnovel model for spaced repetition practice\nwith applications to second language ac-\nquisition. HLR combines psycholinguis-\ntic theory with modern machine learning\ntechniques, indirectly estimating the “half-\nlife” of a word or concept in a student’s\nlong-term memory. We use data from\nDuolingo — a popular online language\nlearning application — to ﬁt HLR models,\nreducing error by 45%+ compared to sev-\neral baselines at predicting student recall\nrates. HLR model weights also shed light\non which linguistic concepts are system-\natically challenging for second language\nlearners. Finally, HLR was able to im-\nprove Duolingo daily student engagement\nby 12% in an operational user study.\n1 Introduction\nThe spacing effect is the observation that people\ntend to remember things more effectively if they\nuse spaced repetition practice(short study periods\nspread out over time) as opposed to massed prac-\ntice (i.e., “cramming”). The phenomenon was ﬁrst\ndocumented by Ebbinghaus (1885), using himself\nas a subject in several experiments to memorize\nverbal utterances. In one study, after a day of\ncramming he could accurately recite 12-syllable\nsequences (of gibberish, apparently). However,\nhe could achieve comparable results with half as\nmany practices spread out over three days.\nThe lag effect(Melton, 1970) is the related ob-\nservation that people learn even better if the spac-\ning between practices gradually increases. For ex-\nample, a learning schedule might begin with re-\n∗Corresponding author.\n†Research conducted at Duolingo.\nview sessions a few seconds apart, then minutes,\nthen hours, days, months, and so on, with each\nsuccessive review stretching out over a longer and\nlonger time interval.\nThe effects of spacing and lag are well-\nestablished in second language acquisition re-\nsearch (Atkinson, 1972; Bloom and Shuell, 1981;\nCepeda et al., 2006; Pavlik Jr and Anderson,\n2008), and beneﬁts have also been shown for gym-\nnastics, baseball pitching, video games, and many\nother skills. See Ruth (1928), Dempster (1989),\nand Donovan and Radosevich (1999) for thorough\nmeta-analyses spanning several decades.\nMost practical algorithms for spaced repetition\nare simple functions with a few hand-picked pa-\nrameters. This is reasonable, since they were\nlargely developed during the 1960s–80s, when\npeople would have had to manage practice sched-\nules without the aid of computers. However, the\nrecent popularity of large-scale online learning\nsoftware makes it possible to collect vast amounts\nof parallel student data, which can be used to em-\npirically train richer statistical models.\nIn this work, we propose half-life regression\n(HLR) as a trainable spaced repetition algorithm,\nmarrying psycholinguistically-inspired models of\nmemory with modern machine learning tech-\nniques. We apply this model to real student learn-\ning data from Duolingo, a popular language learn-\ning app, and use it to improve its large-scale, op-\nerational, personalized learning system.\n2 Duolingo\nDuolingo is a free, award-winning, online lan-\nguage learning platform. Since launching in 2012,\nmore than 150 million students from all over the\nworld have enrolled in a Duolingo course, either\nvia the website1 or mobile apps for Android, iOS,\n1https://www.duolingo.com\n1848\n(a) skill tree screen\n (b) skill screen\n (c) correct response\n (d) incorrect response\nFigure 1: Duolingo screenshots for an English-speaking student learning French (iPhone app, 2016).\n(a) A course skill tree: golden skills have four bars and are “at full strength,” while other skills have\nfewer bars and are due for practice. (b) A skill screen detail (for theGerund skill), showing which words\nare predicted to need practice. (c,d) Grading and explanations for a translation exercise.\n´etant un enfant il est petit\nˆetre.V.GER un.DET.INDF .M.SG enfant.N.SG il.PN.M.P3.SG ˆetre.V.PRES .P3.SG petit.ADJ .M.SG\nFigure 2: The French sentence from Figure 1(c,d) and its lexeme tags. Tags encode the root lexeme, part\nof speech, and morphological components (tense, gender, person, etc.) for each word in the exercise.\nand Windows devices. For comparison, that is\nmore than the total number of students in U.S. el-\nementary and secondary schools combined. At\nleast 80 language courses are currently available\nor under development2 for the Duolingo platform.\nThe most popular courses are for learning English,\nSpanish, French, and German, although there are\nalso courses for minority languages (Irish Gaelic),\nand even constructed languages (Esperanto).\nMore than half of Duolingo students live in\ndeveloping countries, where Internet access has\nmore than tripled in the past three years (ITU and\nUNESCO, 2015). The majority of these students\nare using Duolingo to learn English, which can\nsigniﬁcantly improve their job prospects and qual-\nity of life (Pinon and Haydon, 2010).\n2.1 System Overview\nDuolingo uses a playfully illustrated, gamiﬁed de-\nsign that combines point-reward incentives with\nimplicit instruction (DeKeyser, 2008), mastery\nlearning (Block et al., 1971), explanations (Fahy,\n2https://incubator.duolingo.com\n2004), and other best practices. Early research\nsuggests that 34 hours of Duolingo is equivalent\nto a full semester of university-level Spanish in-\nstruction (Vesselinov and Grego, 2012).\nFigure 1(a) shows an example skill tree for\nEnglish speakers learning French. This speciﬁes\nthe game-like curriculum: each icon represents\na skill, which in turn teaches a set of themati-\ncally or grammatically related words or concepts.\nStudents tap an icon to access lessons of new\nmaterial, or to practice previously-learned mate-\nrial. Figure 1(b) shows a screen for the French\nskill Gerund, which teaches common gerund verb\nforms such as faisant (doing) and ´etant (being).\nThis skill, as well as several others, have already\nbeen completed by the student. However, theMea-\nsures skill in the bottom right of Figure 1(a) has\none lesson remaining. After completing each row\nof skills, students “unlock” the next row of more\nadvanced skills. This is a gamelike implementa-\ntion of mastery learning, whereby students must\nreach a certain level of prerequisite knowledge be-\nfore moving on to new material.\n1849\nEach language course also contains a corpus\n(large database of available exercises) and a lex-\neme tagger(statistical NLP pipeline for automat-\nically tagging and indexing the corpus; see the\nAppendix for details and a lexeme tag reference).\nFigure 1(c,d) shows an example translation exer-\ncise that might appear in theGerund skill, and Fig-\nure 2 shows the lexeme tagger output for this sen-\ntence. Since this exercise is indexed with a gerund\nlexeme tag (ˆetre.V.GER in this case), it is available\nfor lessons or practices in this skill.\nThe lexeme tagger also helps to provide correc-\ntive feedback. Educational researchers maintain\nthat incorrect answers should be accompanied by\nexplanations, not simply a “wrong” mark (Fahy,\n2004). In Figure 1(d), the student incorrectly used\nthe 2nd-person verb form es (ˆetre.V.PRES .P2.SG)\ninstead of the 3rd-person est (ˆetre.V.PRES .P3.SG).\nIf Duolingo is able to parse the student response\nand detect a known grammatical mistake such as\nthis, it provides an explanation3 in plain language.\nEach lesson continues until the student masters all\nof the target wordsbeing taught in the session, as\nestimated by a mixture model of short-term learn-\ning curves (Streeter, 2015).\n2.2 Spaced Repetition and Practice\nOnce a lesson is completed, all the target words\nbeing taught in the lesson are added to thestudent\nmodel. This model captures what the student has\nlearned, and estimates how well she can recall this\nknowledge at any given time. Spaced repetition is\na key component of the student model: over time,\nthe strength of a skill will decay in the student’s\nlong-term memory, and this model helps the stu-\ndent manage her practice schedule.\nDuolingo uses strength metersto visualize the\nstudent model, as seen beneath each of the com-\npleted skill icons in Figure 1(a). These meters\nrepresent the average probability that the student\ncan, at any moment, correctly recall a random tar-\nget word from the lessons in this skill (more on\nthis probability estimate in §3.3). At four bars, the\nskill is “golden” and considered fresh in the stu-\ndent’s memory. At fewer bars, the skill has grown\nstale and may need practice. A student can tap the\nskill icon to access practice sessions and target her\nweakest words. For example, Figure 1(b) shows\n3If Duolingo cannot parse the precise nature of the mis-\ntake — e.g., because of a gross typographical error — it pro-\nvides a “diff” of the student’s response with the closest ac-\nceptable answer in the corpus (using Levenshtein distance).\nsome weak words from the Gerund skill. Practice\nsessions are identical to lessons, except that the\nexercises are taken from those indexed with words\n(lexeme tags) due for practice according to student\nmodel. As time passes, strength meters continu-\nously update and decay until the student practices.\n3 Spaced Repetition Models\nIn this section, we describe several spaced repeti-\ntion algorithms that might be incorporated into our\nstudent model. We begin with two common, estab-\nlished methods in language learning technology,\nand then present our half-life regression model\nwhich is a generalization of them.\n3.1 The Pimsleur Method\nPimsleur (1967) was perhaps the ﬁrst to make\nmainstream practical use of the spacing and lag ef-\nfects, with his audio-based language learning pro-\ngram (now a franchise by Simon & Schuster). He\nreferred to his method as graduated-interval re-\ncall, whereby new vocabulary is introduced and\nthen tested at exponentially increasing intervals,\ninterspersed with the introduction or review of\nother vocabulary. However, this approach is lim-\nited since the schedule is pre-recorded and can-\nnot adapt to the learner’s actual ability. Consider\nan English-speaking French student who easily\nlearns a cognate like pantalon (pants), but strug-\ngles to remember manteau (coat). With the Pim-\nsleur method, she is forced to practice both words\nat the same ﬁxed, increasing schedule.\n3.2 The Leitner System\nLeitner (1972) proposed a different spaced repeti-\ntion algorithm intended for use with ﬂashcards. It\nis more adaptive than Pimsleur’s, since the spac-\ning intervals can increase or decrease depending\non student performance. Figure 3 illustrates a pop-\nular variant of this method.\n1 2 4 8 16\ncorrectly-remembered cards\nincorrectly-remembered cards\nFigure 3: The Leitner System for ﬂashcards.\nThe main idea is to have a few boxes that corre-\nspond to different practice intervals: 1-day, 2-day,\n1850\n4-day, and so on. All cards start out in the 1-day\nbox, and if the student can remember an item after\none day, it gets “promoted” to the 2-day box. Two\ndays later, if she remembers it again, it gets pro-\nmoted to the 4-day box, etc. Conversely, if she is\nincorrect, the card gets “demoted” to a shorter in-\nterval box. Using this approach, the hypothetical\nFrench student from §3.1 would quickly promote\npantalon to a less frequent practice schedule, but\ncontinue reviewing manteau often until she can\nregularly remember it.\nSeveral electronic ﬂashcard programs use the\nLeitner system to schedule practice, by organiz-\ning items into “virtual” boxes. In fact, when it ﬁrst\nlaunched, Duolingo used a variant similar to Fig-\nure 3 to manage skill meter decay and practice.\nThe present research was motivated by the need\nfor a more accurate model, in response to student\ncomplaints that the Leitner-based skill meters did\nnot adequately reﬂect what they had learned.\n3.3 Half-Life Regression: A New Approach\nWe now describe half-life regression (HLR), start-\ning from psychological theory and combining it\nwith modern machine learning techniques.\nCentral to the theory of memory is the Ebbing-\nhaus model, also known as the forgetting curve\n(Ebbinghaus, 1885). This posits that memory de-\ncays exponentially over time:\np= 2−∆/h . (1)\nIn this equation, pdenotes the probability of cor-\nrectly recalling an item (e.g., a word), which is\na function of ∆, the lag time since the item was\nlast practiced, and h, the half-life or measure of\nstrength in the learner’s long-term memory.\nFigure 4(a) shows a forgetting curve (1) with\nhalf-life h= 1. Consider the following cases:\n1. ∆ = 0. The word was just recently practiced,\nso p= 20 = 1.0, conforming to the idea that\nit is fresh in memory and should be recalled\ncorrectly regardless of half-life.\n2. ∆ = h. The lag time is equal to the half-life,\nso p = 2−1 = 0.5, and the student is on the\nverge of being unable to remember.\n3. ∆ ≫h. The word has not been practiced for\na long time relative to its half-life, so it has\nprobably been forgotten, e.g., p≈0.\nLet x denote a feature vector that summarizes\na student’s previous exposure to a particular word,\nand let the parameter vectorΘ contain weights that\ncorrespond to each feature variable in x. Under\nthe assumption that half-life should increase expo-\nnentially with each repeated exposure (a common\npractice in spacing and lag effect research), we let\nˆhΘ denote the estimated half-life, given by:\nˆhΘ = 2Θ·x . (2)\nIn fact, the Pimsleur and Leitner algorithms can\nbe interpreted as special cases of (2) using a few\nﬁxed, hand-picked weights. See the Appendix for\nthe derivation of Θ for these two methods.\nFor our purposes, however, we want to ﬁtΘ em-\npirically to learning trace data, and accommodate\nan arbitrarily large set of interesting features (we\ndiscuss these features more in §3.4). Suppose we\nhave a data set D= {⟨p,∆,x⟩i}D\ni=1 made up of\nstudent-word practice sessions. Each data instance\nconsists of the observed recall rate p4, lag time ∆\nsince the word was last seen, and a feature vector\nx designed to help personalize the learning expe-\nrience. Our goal is to ﬁnd the best model weights\nΘ∗to minimize some loss function ℓ:\nΘ∗= arg min\nΘ\nD∑\ni=1\nℓ(⟨p,∆,x⟩i; Θ) . (3)\nTo illustrate, Figure 4(b) shows a student-word\nlearning trace over the course of a month. Each\n\u0016 indicates a data instance: the vertical position is\nthe observed recall ratepfor each practice session,\nand the horizontal distance between points is the\nlag time ∆ between sessions. Combining (1) and\n(2), the model prediction ˆpΘ = 2 −∆/ˆhΘ is plot-\nted as a dashed line over time (which resets to 1.0\nafter each exposure, since ∆ = 0). The training\nloss function (3) aims to ﬁt the predicted forget-\nting curves to observed data points for millions of\nstudent-word learning traces like this one.\nWe chose the L2-regularized squared loss func-\ntion, which in its basic form is given by:\nℓ(\u0016; Θ) = (p−ˆpΘ)2 + λ∥Θ∥2\n2 ,\nwhere \u0016 = ⟨p,∆,x⟩is shorthand for the training\ndata instance, and λis a parameter to control the\nregularization term and help prevent overﬁtting.\n4In our setting, each data instance represents a full lession\nor practice session, which may include multiple exercises re-\nviewing the same word. Thus p represents the proportion of\ntimes a word was recalled correctly in a particular session.\n1851\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  1  2  3  4  5  6  7\n!\n!\n!\n(a) Ebbinghaus model ( h = 1)\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  5  10  15  20  25  30\n✖\n✖\n✖\n✖\n✖\n✖ (b) 30-day student-word learning trace and predicted forgetting curve\nFigure 4: Forgetting curves. (a) Predicted recall rate as a function of lag time ∆ and half-life h = 1.\n(b) Example student-word learning trace over 30 days: \u0016 marks the observed recall rate p for each\npractice session, and half-life regression aims to ﬁt model predictions ˆpΘ (dashed lines) to these points.\nIn practice, we found it useful to optimize for\nthe half-life h in addition to the observed recall\nrate p. Since we do not know the “true” half-life\nof a given word in the student’s memory — this\nis a hypothetical construct — we approximate it\nalgebraically from (1) using p and ∆. We solve\nfor h= −∆\nlog2(p) and use the ﬁnal loss function:\nℓ(\u0016; Θ) = (p−ˆpΘ)2 + α(h−ˆhΘ)2 + λ∥Θ∥2\n2 ,\nwhere αis a parameter to control the relative im-\nportance of the half-life term in the overall train-\ning objective function. Since ℓis smooth with re-\nspect to Θ, we can ﬁt the weights to student-word\nlearning traces using gradient descent. See the Ap-\npendix for more details on our training and opti-\nmization procedures.\n3.4 Feature Sets\nIn this work, we focused on features that were eas-\nily instrumented and available in the production\nDuolingo system, without adding latency to the\nstudent’s user experience. These features fall into\ntwo broad categories:\n•Interaction features: a set of counters sum-\nmarizing each student’s practice history with\neach word (lexeme tag). These include the\ntotal number of times a student has seen the\nword xn, the number of times it was correctly\nrecalled x⊕, and the number of times incor-\nrect x⊖. These are intended to help the model\nmake more personalized predictions.\n•Lexeme tag features: a large, sparse set of\nindicator variables, one for each lexeme tag\nin the system (about 20k in total). These are\nintended to capture the inherent difﬁculty of\neach particular word (lexeme tag).\nrecall rate lag (days) feature vector x\np (⊕/n) ∆ xn x⊕ x⊖ xˆetre.V.GER\n1.0 (3/3) 0.6 3 2 1 1\n0.5 (2/4) 1.7 6 5 1 1\n1.0 (3/3) 0.7 10 7 3 1\n0.8 (4/5) 4.7 13 10 3 1\n0.5 (1/2) 13.5 18 14 4 1\n1.0 (3/3) 2.6 20 15 5 1\nTable 1: Example training instances. Each row\ncorresponds to a data point in Figure 4(b) above,\nwhich is for a student learning the French word\n´etant (lexeme tag ˆetre.V.GER ).\nTo be more concrete, imagine that the trace in\nFigure 4(b) is for a student learning the French\nword ´etant (lexeme tag ˆetre.V.GER ). Table 1 shows\nwhat ⟨p,∆,x⟩would look like for each session\nin the student’s history with that word. The inter-\naction features increase monotonically5 over time,\nand xˆetre.V.GER is the only lexeme feature to “ﬁre”\nfor these instances (it has value 1, all other lexeme\nfeatures have value 0). The model also includes a\nbias weight (intercept) not shown here.\n4 Experiments\nIn this section, we compare variants of HLR with\nother spaced repetition algorithms in the context of\nDuolingo. First, we evaluate methods against his-\ntorical log data, and analyze trained model weights\nfor insight. We then describe two controlled user\nexperiments where we deployed HLR as part of\nthe student model in the production system.\n5Note that in practice, we found that using the square root\nof interaction feature counts (e.g., √x⊕) yielded better re-\nsults than the raw counts shown here.\n1852\nModel MAE ↓ AUC↑ CORh↑\nHLR 0.128* 0.538* 0.201*\nHLR -lex 0.128* 0.537* 0.160*\nHLR -h 0.350 0.528* -0.143*\nHLR -lex-h 0.350 0.528* -0.142*\nLeitner 0.235 0.542* -0.098*\nPimsleur 0.445 0.510* -0.132*\nLR 0.211 0.513* n/a\nLR -lex 0.212 0.514* n/a\nConstant ¯p= 0.859 0.175 n/a n/a\nTable 2: Evaluation results using historical log\ndata (see text). Arrows indicate whether lower (↓)\nor higher ( ↑) scores are better. The best method\nfor each metric is shown in bold, and statistically\nsigniﬁcant effects (p< 0.001) are marked with *.\n4.1 Historical Log Data Evaluation\nWe collected two weeks of Duolingo log data,\ncontaining 12.9 million student-word lesson and\npractice session traces similar to Table 1 (for all\nstudents in all courses). We then compared three\ncategories of spaced repetition algorithms:\n•Half-life regression (HLR), our model from\n§3.3. For ablation purposes, we consider four\nvariants: with and without lexeme features\n(-lex), as well as with and without the half-\nlife term in the loss function (-h).\n•Leitner and Pimsleur, two established base-\nlines that are special cases of HLR, using\nﬁxed weights. See the Appendix for a deriva-\ntion of the model weights we used.\n•Logistic regression (LR), a standard machine\nlearning6 baseline. We evaluate two variants:\nwith and without lexeme features (-lex).\nWe used the ﬁrst 1 million instances of the data\nto tune the parameters for our training algorithm.\nAfter trying a handful of values, we settled on\nλ = 0.1, α = 0.01, and learning rate η = 0.001.\nWe used these same training parameters for HLR\nand LR experiments (the Leitner and Pimsleur\nmodels are ﬁxed and do not require training).\n6For LR models, we include the lag time x∆ as an addi-\ntional feature, since — unlike HLR — it isn’t explicitly ac-\ncounted for in the model. We experimented with polynomial\nand exponential transformations of this feature, as well, but\nfound the raw lag time to work best.\nTable 2 shows the evaluation results on the full\ndata set of 12.9 million instances, using the ﬁrst\n90% for training and remaining 10% for testing.\nWe consider several different evaluation measures\nfor a comprehensive comparison:\n•Mean absolute error (MAE) measures how\nclosely predictions resemble their observed\noutcomes: 1\nD\n∑D\ni=1 |p−ˆpΘ|i. Since the\nstrength meters in Duolingo’s interface are\nbased on model predictions, we use MAE as\na measure of prediction quality.\n•Area under the ROC curve (AUC)— or the\nWilcoxon rank-sum test — is a measure of\nranking quality. Here, it represents the proba-\nbility that a model ranks a random correctly-\nrecalled word as more likely than a random\nincorrectly-recalled word. Since our model is\nused to prioritize words for practice, we use\nAUC to help evaluate these rankings.\n•Half-life correlation (CORh) is the Spearman\nrank correlation between ˆhΘ and the alge-\nbraic estimate h described in §3.3. We use\nthis as another measure of ranking quality.\nFor all three metrics, HLR with lexeme tag fea-\ntures is the best (or second best) approach, fol-\nlowed closely by HLR -lex (no lexeme tags). In\nfact, these are the only two approaches with MAE\nlower than a baseline constant prediction of theav-\nerage recall rate in the training data (Table 2, bot-\ntom row). These HLR variants are also the only\nmethods with positive CORh, although this seems\nreasonable since they are the only two to directly\noptimize for it. While lexeme tag features made\nlimited impact, the hterm in the HLR loss func-\ntion is clearly important: MAE more than doubles\nwithout it, and the -hvariants are generally worse\nthan the other baselines on at least one metric.\nAs stated in §3.2, Leitner was the spaced repeti-\ntion algorithm used in Duolingo’s production stu-\ndent model at the time of this study. The Leitner\nmethod did yield the highest AUC7 values among\nthe algorithms we tried. However, the top two\nHLR variants are not far behind, and they also re-\nduce MAE compared to Leitner by least 45%.\n7AUC of 0.5 implies random guessing (Fawcett, 2006),\nso the AUC values here may seem low. This is due in part\nto an inherently noisy prediction task, but also to a range re-\nstriction: ¯p = 0.859, so most words are recalled correctly\nand predictions tend to be high. Note that all reported AUC\nvalues are statistically signiﬁcantly better than chance using\na Wilcoxon rank sum test with continuity correction.\n1853\nLg. Word Lexeme Tag θk\nEN camera camera.N.SG 0.77\nEN ends end.V.PRES .P3.SG 0.38\nEN circle circle.N.SG 0.08\nEN rose rise.V.PST -0.09\nEN performed perform.V.PP -0.48\nEN writing write.V.PRESP -0.81\nES liberal liberal.ADJ .SG 0.83\nES como comer.V.PRES .P1.SG 0.40\nES encuentra encontrar.V.PRES .P3.SG 0.10\nES est´a estar.V.PRES .P3.SG -0.05\nES pensando pensar.V.GER -0.33\nES quedado quedar.V.PP.M.SG -0.73\nFR visite visiter.V.PRES .P3.SG 0.94\nFR suis ˆetre.V.PRES .P1.SG 0.47\nFR trou trou.N.M.SG 0.05\nFR dessous dessous.ADV -0.06\nFR ceci ceci.PN.NT -0.45\nFR fallait falloir.V.IMPERF .P3.SG -0.91\nDE Baby Baby.N.NT.SG.ACC 0.87\nDE sprechen sprechen.V.INF 0.56\nDE sehr sehr.ADV 0.13\nDE den der.DET.DEF.M.SG.ACC -0.07\nDE Ihnen Sie.PN.P3.PL.DAT.FORM -0.55\nDE war sein.V.IMPERF .P1.SG -1.10\nTable 3: Lexeme tag weights for English ( EN),\nSpanish (ES), French (FR), and German (DE).\n4.2 Model Weight Analysis\nIn addition to better predictions, HLR can cap-\nture the inherent difﬁculty of concepts that are en-\ncoded in the feature set. The “easier” concepts\ntake on positive weights (less frequent practice re-\nsulting from longer half-lifes), while the “harder”\nconcepts take on negative weights (more frequent\npractice resulting from shorter half-lifes).\nTable 3 shows HLR model weights for sev-\neral English, Spanish, French, and German lexeme\ntags. Positive weights are associated with cog-\nnates and words that are common, short, or mor-\nphologically simple to inﬂect; it is reasonable that\nthese would be easier to recall correctly. Negative\nweights are associated with irregular forms, rare\nwords, and grammatical constructs like past or\npresent participles and imperfective aspect. These\nmodel weights can provide insight into the aspects\nof language that are more or less challenging for\nstudents of a second language.\nDaily Retention Activity\nExperiment Any Lesson Practice\nI. HLR (v. Leitner) +0.3 +0.3 -7.3*\nII. HLR -lex (v. HLR) +12.0* +1.7* +9.5*\nTable 4: Change (%) in daily student retention for\ncontrolled user experiments. Statistically signiﬁ-\ncant effects (p< 0.001) are marked with *.\n4.3 User Experiment I\nThe evaluation in §4.1 suggests that HLR is a bet-\nter approach than the Leitner algorithm originally\nused by Duolingo (cutting MAE nearly in half).\nTo see what effect, if any, these gains have on ac-\ntual student behavior, we ran controlled user ex-\nperiments in the Duolingo production system.\nWe randomly assigned all students to one of\ntwo groups: HLR (experiment) or Leitner (con-\ntrol). The underlying spaced repetition algorithm\ndetermined strength meter values in the skill tree\n(e.g., Figure 1(a)) as well as the ranking of target\nwords for practice sessions (e.g., Figure 1(b)), but\notherwise the two conditions were identical. The\nexperiment lasted six weeks and involved just un-\nder 1 million students.\nFor evaluation, we examined changes in daily\nretention: what percentage of students who en-\ngage in an activity return to do it again the fol-\nlowing day? We used three retention metrics: any\nactivity (including contributions to crowdsourced\ntranslations, online forum discussions, etc.), new\nlessons, and practice sessions.\nResults are shown in the ﬁrst row of Table 4.\nThe HLR group showed a slight increase in overall\nactivity and new lessons, but a signiﬁcant decrease\nin practice. Prior to the experiment, many stu-\ndents claimed that they would practice instead of\nlearning new material “just to keep the tree gold,”\nbut that practice sessions did not review what they\nthought they needed most. This drop in practice\n— plus positive anecdotal feedback about stength\nmeter quality from the HLR group — led us to\nbelieve that HLR was actually better for student\nengagement, so we deployed it for all students.\n4.4 User Experiment II\nSeveral months later, active students pointed out\nthat particular words or skills would decay rapidly,\nregardless of how often they practiced. Upon\ncloser investigation, these complaints could be\n1854\ntraced to lexeme tag features with highly negative\nweights in the HLR model (e.g., Table 3). This im-\nplied that some feature-based overﬁtting had oc-\ncurred, despite the L2 regularization term in the\ntraining procedure. Duolingo was also preparing\nto launch several new language courses at the time,\nand no training data yet existed to ﬁt lexeme tag\nfeature weights for these new languages.\nSince the top two HLR variants were virtually\ntied in our §4.1 experiments, we hypothesized that\nusing interaction features alone might alleviate\nboth student frustration and the “cold-start” prob-\nlem of training a model for new languages. In a\nfollow-up experiment, we randomly assigned all\nstudents to one of two groups: HLR -lex (experi-\nment) and HLR (control). The experiment lasted\ntwo weeks and involved 3.3 million students.\nResults are shown in the second row of Ta-\nble 4. All three retention metrics were signiﬁ-\ncantly higher for the HLR -lex group. The most\nsubstantial increase was for any activity, although\nrecurring lessons and practice sessions also im-\nproved (possibly as a byproduct of the overall ac-\ntivity increase). Anecdotally, vocal students from\nthe HLR -lex group who previously complained\nabout rapid decay under the HLR model were also\npositive about the change.\nWe deployed HLR -lex for all students, and be-\nlieve that its improvements are at least partially re-\nsponsible for the consistent 5% month-on-month\ngrowth in active Duolingo users since the model\nwas launched.\n5 Other Related Work\nJust as we drew upon the theories of Ebbinghaus\nto derive HLR as an empirical spaced repetition\nmodel, there has been other recent work drawing\non other (but related) theories of memory.\nACT-R (Anderson et al., 2004) is a cognitive\narchitecture whose declarative memory module 8\ntakes the form of a power function, in contrast to\nthe exponential form of the Ebbinghaus model and\nHLR. Pavlik and Anderson (2008) used ACT-R\npredictions to optimize a practice schedule for\nsecond-language vocabulary, although their set-\nting was quite different from ours. They assumed\nﬁxed intervals between practice exercises within\nthe same laboratory session, and found that they\ncould improve short-term learning within a ses-\n8Declarative (speciﬁcally semantic) memory is widely re-\ngarded to govern language vocabulary (Ullman, 2005).\nsion. In contrast, we were concerned with mak-\ning accurate recall predictions between multiple\nsessions “in the wild” on longer time scales. Ev-\nidence also suggests that manipulation between\nsessions can have greater impact on long-term\nlearning (Cepeda et al., 2006).\nMotivated by long-term learning goals, the mul-\ntiscale context model (MCM) has also been pro-\nposed (Mozer et al., 2009). MCM combines two\nmodern theories of the spacing effect (Staddon et\nal., 2002; Raaijmakers, 2003), assuming that each\ntime an item is practiced it creates an additional\nitem-speciﬁc forgetting curve that decays at a dif-\nferent rate. Each of these forgetting curves is ex-\nponential in form (similar to HLR), but are com-\nbined via weighted average, which approximates\na power law (similar to ACT-R). The authors\nwere able to ﬁt models to controlled laboratory\ndata for second-language vocabulary and a few\nother memory tasks, on times scales up to several\nmonths. We were unaware of MCM at the time of\nour work, and it is unclear if the additional compu-\ntational overhead would scale to Duolingo’s pro-\nduction system. Nevertheless, comparing to and\nintegrating with these ideas is a promising direc-\ntion for future work.\nThere has also been work on more heuris-\ntic spaced repetition models, such as Super-\nMemo (Wo ´zniak, 1990). Variants of this algo-\nrithm are popular alternatives to Leitner in some\nﬂashcard software, leveraging additional parame-\nters with complex interactions to determine spac-\ning intervals for practice. To our knowledge, these\nadditional parameters are hand-picked as well, but\none can easily imagine ﬁtting them empirically to\nreal student log data, as we do with HLR.\n6 Conclusion\nWe have introduced half-life regression (HLR), a\nnovel spaced repetition algorithm with applica-\ntions to second language acquisition. HLR com-\nbines a psycholinguistic model of human mem-\nory with modern machine learning techniques, and\ngeneralizes two popular algorithms used in lan-\nguage learning technology: Leitner and Pimsleur.\nWe can do this by incorporating arbitrarily rich\nfeatures and ﬁtting their weights to data. This ap-\nproach is signiﬁcantly more accurate at predict-\ning student recall rates than either of the previous\nmethods, and is also better than a conventional ma-\nchine learning approach like logistic regression.\n1855\nOne result we found surprising was that lexeme\ntag features failed to improve predictions much,\nand in fact seemed to frustrate the student learn-\ning experience due to over-ﬁtting. Instead of the\nsparse indicator variables used here, it may be bet-\nter to decompose lexeme tags into denser and more\ngeneric features of tag components9 (e.g., part of\nspeech, tense, gender, case), and also use corpus\nfrequency, word length, etc. This representation\nmight be able to capture useful and interesting reg-\nularities without negative side-effects.\nFinally, while we conducted a cursory analy-\nsis of model weights in §4.2, an interesting next\nstep would be to study such weights for even\ndeeper insight. (Note that using lexeme tag com-\nponent features, as suggested above, should make\nthis anaysis more robust since features would be\nless sparse.) For example, one could see whether\nthe ranking of vocabulary and/or grammar compo-\nnents by feature weight is correlated with external\nstandards such as the CEFR (Council of Europe,\n2001). This and other uses of HLR hold the poten-\ntial to transform data-driven curriculum design.\nData and Code\nTo faciliatate research in this area, we have pub-\nlicly released our data set and code from §4.1:\nhttps://github.com/duolingo/halﬂife-regression.\nAcknowledgments\nThanks to our collaborators at Duolingo, particu-\nlarly Karin Tsai, Itai Hass, and Andr ´e Horie for\nhelp gathering data from various parts of the sys-\ntem. We also thank the anonymous reviewers for\nsuggestions that improved the ﬁnal manuscript.\nReferences\nJ.R. Anderson, D. Bothell, M.D. Byrne, S. Douglass,\nC. Libiere, and Y . Qin. 2004. An intergrated theory\nof mind. Psychological Review, 111:1036–1060.\nR.C. Atkinson. 1972. Optimizing the learning of a\nsecond-language vocabulary. Journal of Experimen-\ntal Psychology, 96(1):124–129.\nJ.H. Block, P.W. Airasian, B.S. Bloom, and J.B. Car-\nroll. 1971. Mastery Learning: Theory and Practice.\nHolt, Rinehart, and Winston, New York.\n9Engineering-wise, each lexeme tag (e.g., ˆetre.V.GER ) is\nrepresented by an ID in the system. We used indicator vari-\nables in this work since the IDs are readily available; the over-\nhead of retreiving all lexeme components would be inefﬁcient\nin the production system. Of course, we could optimize for\nthis if there were evidence of a signiﬁcant improvement.\nK.C. Bloom and T.J. Shuell. 1981. Effects of massed\nand distributed practice on the learning and retention\nof second language vocabulary. Journal of Educa-\ntional Psychology, 74:245–248.\nN.J. Cepeda, H. Pashler, E. Vul, J.T. Wixted, and\nD. Rohrer. 2006. Distributed practice in verbal re-\ncall tasks: A review and quantitative synthesis. Psy-\nchological Bulletin, 132(3):354.\nCouncil of Europe. 2001. Common European Frame-\nwork of Reference for Languages: Learning, Teach-\ning, Assessment. Cambridge University Press.\nR. DeKeyser. 2008. Implicit and explicit learning.\nIn The Handbook of Second Language Acquisition,\nchapter 11, pages 313–348. John Wiley & Sons.\nF.N. Dempster. 1989. Spacing effects and their im-\nplications for theory and practice. Educational Psy-\nchology Review, 1(4):309–330.\nJ.J. Donovan and D.J. Radosevich. 1999. A meta-\nanalytic review of the distribution of practice effect:\nNow you see it, now you don’t. Journal of Applied\nPsychology, 84(5):795–805.\nJ. Duchi, E. Hazan, and Y . Singer. 2011. Adaptive sub-\ngradient methods for online learning and stochas-\ntic optimization. Journal of Machine Learning Re-\nsearch, 12(Jul):2121–2159.\nH. Ebbinghaus. 1885. Memory: A Contribution\nto Experimental Psychology. Teachers College,\nColumbia University, New York, NY , USA.\nP.J. Fahy. 2004. Media characteristics and online\nlearning technology. In T. Anderson and F. Elloumi,\neditors, Theory and Practice of Online Learning,\npages 137–171. Athabasca University.\nT. Fawcett. 2006. An introduction to ROC analysis.\nPattern Recognition Letters, 27:861–874.\nM.L. Forcada, M. Ginest ´ı-Rosell, J. Nordfalk,\nJ. O’Regan, S. Ortiz-Rojas, J.A. P ´erez-Ortiz,\nF. S ´anchez-Mart´ınez, G. Ram ´ırez-S´anchez, and\nF.M. Tyers. 2011. Apertium: A free/open-\nsource platform for rule-based machine trans-\nlation. Machine Translation , 25(2):127–144.\nhttp://wiki.apertium.org/wiki/Main Page.\nITU and UNESCO. 2015. The state of broadband\n2015. Technical report, September.\nS. Leitner. 1972. So lernt man lernen. Angewandte\nLernpsychologie – ein Weg zum Erfolg. Verlag\nHerder, Freiburg im Breisgau, Germany.\nA.W. Melton. 1970. The situation with respect to the\nspacing of repetitions and memory. Journal of Ver-\nbal Learning and Verbal Behavior, 9:596–606.\nM.C. Mozer, H. Pashler, N. Cepeda, R.V . Lindsey,\nand E. Vul. 2009. Predicting the optimal spacing\nof study: A multiscale context model of memory.\nIn Advances in Neural Information Processing Sys-\ntems, volume 22, pages 1321–1329.\n1856\nP.I. Pavlik Jr and J.R. Anderson. 2008. Using a\nmodel to compute the optimal schedule of prac-\ntice. Journal of Experimental Psychology: Applied,\n14(2):101—117.\nP. Pimsleur. 1967. A memory schedule. Modern Lan-\nguage Journal, 51(2):73–75.\nR. Pinon and J. Haydon. 2010. The beneﬁts of the En-\nglish language for individuals and societies: Quanti-\ntative indicators from Cameroon, Nigeria, Rwanda,\nBangladesh and Pakistan. Technical report, Eu-\nromonitor International for the British Council.\nJ.G.W. Raaijmakers. 2003. Spacing and repetition\neffects in human memory: Application of the sam\nmodel. Cognitive Science, 27(3):431–452.\nT.C. Ruth. 1928. Factors inﬂuencing the relative econ-\nomy of massed and distributed practice in learning.\nPsychological Review, 35:19–45.\nJ.E.R. Staddon, I.M. Chelaru, and J.J. Higa. 2002. Ha-\nbituation, memory and the brain: The dynamics of\ninterval timing. Behavioural Processes, 57(2):71–\n88.\nM. Streeter. 2015. Mixture modeling of individual\nlearning curves. In Proceedings of the International\nConference on Educational Data Mining (EDM).\nM.T. Ullman. 2005. A cognitive neuroscience\nperspective on second language acquisition: The\ndeclarative/procedural model. In C. Sanz, editor,\nMind and Context in Adult Second Language Acqui-\nsition: Methods, Theory, and Practice, pages 141–\n178. Georgetown University Press.\nR. Vesselinov and J. Grego. 2012. Duolingo effective-\nness study. Technical report, Queens College, City\nUniversity of New York.\nWikimedia Foundation. 2002. Wiktionary: A wiki-\nbased open content dictionary, retrieved 2012–2015.\nhttps://www.wiktionary.org.\nP.A. Wo´zniak. 1990. Optimization of learning. Mas-\nter’s thesis, University of Technology in Pozna´n.\nA Appendix\nA.1 Lexeme Tagger Details\nWe use a lexeme tagger, introduced in §2, to ana-\nlyze and index the learning corpus and student re-\nsponses. Since Duolingo courses teach a moderate\nset of words and concepts, we do not necessarily\nneed a complete, general-purpose, multi-lingual\nNLP stack. Instead, for each language we use a ﬁ-\nnite state transducer (FST) to efﬁciently parse can-\ndidate lexeme tags10 for each word. We then use a\n10The lexeme tag set is based on a large morphology dictio-\nnary created by the Apertium project (Forcada et al., 2011),\nwhich we supplemented with entries from Wiktionary (Wiki-\nmedia Foundation, 2002) and other sources. Each Duolingo\ncourse teaches about 3,000–5,000 lexeme tags.\nAbbreviation Meaning\nACC accusative case\nADJ adjective\nADV adverb\nDAT dative case\nDEF deﬁnite\nDET determiner\nFORM formal register\nF feminine\nGEN genitive case\nGER gerund\nIMPERF imperfective aspect\nINDF indeﬁnite\nINF inﬁnitive\nM masculine\nN noun\nNT neuter\nP1/P2/P3 1st/2nd/3rd person\nPL plural\nPN pronoun\nPP past participle\nPRESP present participle\nPRES present tense\nPST past tense\nSG singular\nV verb\nTable 5: Lexeme tag component abbreviations.\nhidden Markov model (HMM) to determine which\ntag is correct in a given context.\nConsider the following two Spanish sentences:\n‘Yo como manzanas’ (‘I eat apples’) and ‘Corro\ncomo el viento’ (‘I run like the wind’). For both\nsentences, the FST parses the word como into\nthe lexeme tag candidates comer.V.PRES .P1.SG\n([I] eat) and como.ADV.CNJ (like/as). The HMM\nthen disambiguates between the respective tags for\neach sentence. Table 5 contains a reference of the\nabbreviations used in this paper for lexeme tags.\nA.2 Pimsleur and Leitner Models\nAs mentioned in §3.3, the Pimsleur and Leitner\nalgorithms are special cases of HLR using ﬁxed,\nhand-picked weights. To see this, consider the\noriginal practice interval schedule used by Pim-\nsleur (1967): 5 sec, 25 sec, 2 min, 10 min, 1 hr,\n5 hr, 1 day, 5 days, 25 days, 4 months, and 2 years.\nIf we interpret this as a sequence of ˆhΘ half-lifes\n(i.e., students should practice when ˆpΘ = 0.5), we\ncan rewrite (2) and solve for log2(ˆhΘ) as a linear\n1857\nequation. This yields Θ = {xn : 2.4,xb : -16.5},\nwhere xn and xb are the number of practices and\na bias weight (intercept), respectively. This model\nperfectly reconstructs Pimsleur’s original schedule\nin days (r2 = 0.999, p ≪0.001). Analyzing the\nLeitner variant from Figure 3 is even simpler: this\ncorresponds to Θ = {x⊕ : 1,x⊖ : -1}, where x⊕\nis the number of past correct responses (i.e., dou-\nbling the interval), and x⊖is the number of incor-\nrect responses (i.e., halving the interval).\nA.3 Training and Optimization Details\nThe complete objective function given in §3.3 for\nhalf-life regression is:\nℓ(⟨p,∆,x⟩; Θ) = ( p−ˆpΘ)2\n+ α(h−ˆhΘ)2 + λ∥Θ∥2\n2 .\nSubstituting (1) and (2) into this equation produces\nthe following more explicit formulation:\nℓ(⟨p,∆,x⟩; Θ) =\n(\np−2− ∆\n2Θ·x\n)2\n+ α\n( −∆\nlog2(p) −2Θ·x\n)2\n+ λ∥Θ∥2\n2 .\nIn general, the search for Θ∗weights to minimize\nℓcannot be solved in closed form, but since it is a\nsmooth function, it can be optimized using gradi-\nent methods. The partial gradient of ℓwith respect\nto each θk weight is given by:\n∂ℓ\n∂θk\n= 2(ˆpΘ −p) ln2(2)ˆpΘ\n(∆\nˆhΘ\n)\nxk\n+ 2α\n(\nˆhΘ + ∆\nlog2(p)\n)\nln(2)ˆhΘxk\n+ 2λθk .\nIn order to ﬁt Θ to a large amount of student\nlog data, we use AdaGrad (Duchi et al., 2011),\nan online algorithm for stochastic gradient descent\n(SGD). AdaGrad is typically less sensitive to the\nlearning rate parameter η than standard SGD, by\ndynamically scaling each weight update as a func-\ntion of how often the corresponding feature ap-\npears in the training data:\nθ(+1)\nk := θk −η\n[\nc(xk)−1\n2\n] ∂ℓ\n∂θk\n.\nHere c(xk) denotes the number of times feature\nxk has had a nonzero value so far in the SGD pass\nthrough the training data. This is useful for train-\ning stability when using large, sparse feature sets\n(e.g., the lexeme tag features in this study). Note\nthat to prevent computational overﬂow and under-\nﬂow errors, we bound ˆpΘ ∈[0.0001,0.9999] and\nˆhΘ ∈[15 min,9 months] in practice.\n1858",
  "topic": "Repetition (rhetorical device)",
  "concepts": [
    {
      "name": "Repetition (rhetorical device)",
      "score": 0.8271746039390564
    },
    {
      "name": "Computer science",
      "score": 0.7009119987487793
    },
    {
      "name": "Language model",
      "score": 0.45840245485305786
    },
    {
      "name": "Speech recognition",
      "score": 0.36589691042900085
    },
    {
      "name": "Natural language processing",
      "score": 0.36031562089920044
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3217208981513977
    },
    {
      "name": "Linguistics",
      "score": 0.21193084120750427
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}