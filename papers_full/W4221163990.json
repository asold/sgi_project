{
    "title": "Task-guided Disentangled Tuning for Pretrained Language Models",
    "url": "https://openalex.org/W4221163990",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2344356506",
            "name": "Jiali Zeng",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2150032407",
            "name": "Yufan Jiang",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2770339140",
            "name": "Shuangzhi Wu",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2890699089",
            "name": "Yongjing Yin",
            "affiliations": [
                "Westlake University",
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2103262105",
            "name": "Mu Li",
            "affiliations": [
                "Tencent (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963777589",
        "https://openalex.org/W2975185270",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3124687886",
        "https://openalex.org/W2953163841",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W3007685714",
        "https://openalex.org/W2952984539",
        "https://openalex.org/W3172399575",
        "https://openalex.org/W4287692509",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W3098394092",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2963121782",
        "https://openalex.org/W3110664450",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3174864715",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W4287639392",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W3101609372",
        "https://openalex.org/W3102725307",
        "https://openalex.org/W3089659770",
        "https://openalex.org/W3104215796",
        "https://openalex.org/W3114651185",
        "https://openalex.org/W3100554158",
        "https://openalex.org/W4229686017",
        "https://openalex.org/W3199761064",
        "https://openalex.org/W3172318343",
        "https://openalex.org/W3016970897",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3173606101",
        "https://openalex.org/W3035204084",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W3182414670",
        "https://openalex.org/W3210120707",
        "https://openalex.org/W3035419191",
        "https://openalex.org/W2965373594"
    ],
    "abstract": "Pretrained language models (PLMs) trained on large-scale unlabeled corpus are typically fine-tuned on task-specific downstream datasets, which have produced state-of-the-art results on various NLP tasks. However, the data discrepancy issue in domain and scale makes fine-tuning fail to efficiently capture task-specific patterns, especially in low data regime. To address this issue, we propose Task-guided Disentangled Tuning (TDT) for PLMs, which enhances the generalization of representations by disentangling task-relevant signals from the entangled representations. For a given task, we introduce a learnable confidence model to detect indicative guidance from context, and further propose a disentangled regularization to mitigate the over-reliance problem. Experimental results on GLUE and CLUE benchmarks show that TDT gives consistently better results than fine-tuning with different PLMs, and extensive analysis demonstrates the effectiveness and robustness of our method. Code is available at https://github.com/lemon0830/TDT.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 3126 - 3137\nMay 22-27, 2022c‚Éù2022 Association for Computational Linguistics\nTask-guided Disentangled Tuning for Pretrained Language Models\nJiali Zeng1‚àó, Yufan Jiang 1, Shuangzhi Wu 1, Yongjing Yin2, Mu Li 1\n1Tencent Cloud Xiaowei, Beijing, China\n2Zhejiang University, Westlake University, Zhejiang, China\n{lemonzeng,garyyfjiang,frostwu,ethanlli}@tencent.com\nyinyongjing@westlake.edu.cn\nAbstract\nPretrained language models (PLMs) trained on\nlarge-scale unlabeled corpus are typically fine-\ntuned on task-specific downstream datasets,\nwhich have produced state-of-the-art results\non various NLP tasks. However, the data dis-\ncrepancy issue in domain and scale makes fine-\ntuning fail to efficiently capture task-specific\npatterns, especially in the low data regime. To\naddress this issue, we propose Task-guided\nDisentangled Tuning (TDT) for PLMs, which\nenhances the generalization of representations\nby disentangling task-relevant signals from the\nentangled representations. For a given task,\nwe introduce a learnable confidence model to\ndetect indicative guidance from context, and\nfurther propose a disentangled regularization to\nmitigate the over-reliance problem. Experimen-\ntal results on GLUE and CLUE benchmarks\nshow that TDT gives consistently better results\nthan fine-tuning with different PLMs, and ex-\ntensive analysis demonstrates the effectiveness\nand robustness of our method. Code is avail-\nable at https://github.com/lemon0830/TDT.\n1 Introduction\nIn recent years, pretrained language models (PLMs)\ntrained in a self-supervised manner like mask lan-\nguage modeling have achieved promising results\non various natural language processing (NLP) tasks\n(Devlin et al., 2019; Yang et al., 2019; Liu et al.,\n2019b), which learn general linguistic and seman-\ntic knowledge from massive general corpus. To\nadapt PLMs to specific NLP tasks, a commonly-\nused approach is fine-tuning, where the whole or\npart of model parameters are tuned by task-specific\nobjectives. Despite its success, the fine-tuned\nmodels have been proven ineffective to capture\ntask-specific patterns due to the gap between task-\nagnostic pre-training and the weak fine-tuning with\nlimited task-specific data (Gu et al., 2020; Gururan-\ngan et al., 2020; Kang et al., 2020).\n‚àóCorresponding author.\nTrain\nJobs founded apple in 1976\napple launches new apple phones\napple is interested in news content and started \nrecruiting editors on a large scale last year\nLabelSentence\ntech\napple\ntechTest\nThe total output of apples in arid regions \nhas fallen, and the price of high-quality \napples has risen.\nPred/LabelSentence\ntech/finance\nApple's founder's daughter bought a new \nmansion in San Francisco\ntech/house\ntech\ntech\nOurs LabelSentence\nJobs founded apple in 1976\nJobs founded apple in 1976\nJobs founded apple in 1976\nPositive:\nNegative:\ntech\nFigure 1: An over-reliance example of news classifica-\ntion task. The fine-tuned models tend to learn a simple\nrule that ‚ÄúApple‚Äù (red) indicates ‚Äútech‚Äù class while ig-\nnore the real meaning of ‚Äúapples‚Äù (green) A reliable\nmodel is expected to find out truly task-specific patterns\n(underlined words) instead of some high frequency but\ninsignificant words (‚Äúapple‚Äù).\nTo address this problem, most existing methods\nfocus on adapting PLMs to downstream tasks by\ncontinual pre-training on in-domain unsupervised\ndata (Gururangan et al., 2020; Gu et al., 2020; Wu\net al., 2021; Kang et al., 2020; Ye et al., 2021).\nFor example, Gu et al. (2020) propose intermedi-\nate continual pre-training with a selective masking\nstrategy, and Gururangan et al. (2020) adapt PLMs\nto in-domain tasks by domain-adaptive pretraining.\nAlthough straightforward, these kinds of methods\nheavily rely on the selection of large-scale addi-\ntional domain corpora and the design of appropri-\nate intermediate training tasks (Wang et al., 2019;\nAghajanyan et al., 2021a).\nIn this paper, we propose a Task-guided\nDisentangled Tuning (TDT) for PLMs by auto-\nmatically detecting task-specific informative inputs\nwithout the need of additional corpora and inter-\n3126\nmediate training. The core component of TDT is a\nconfidence model which assigns each token a confi-\ndence score, and we construct distilled samples by\nretaining informative tokens with high confidence\nscores while perturbing the rest. The confidence\nmodel performs a ‚Äúdeletion game‚Äù strategy, which\nencourages the model to perturb inputs as much\nas possible and to maintain the performance of\ndownstream tasks to the greatest extent with the\ndistilled samples. Although the informative tokens\nare important for downstream predictions, existing\nwork shows that over-relying on part of these words\nmay result in pool generalization, i.e., over-reliance\nproblem (Moon et al., 2020; Geirhos et al., 2020;\nSun et al., 2019). Take the sentences in Figure 1\nas an example, when the context word ‚ÄúApple\" fre-\nquently co-occurs with the label ‚Äútech\", fine-tuned\nmodels may learn a spurious association by binding\n‚ÄúApple\" and ‚Äútech\", leading to incorrect predictions\nof sentences which contain ‚Äúapple‚Äù but belong to\nother categories.\nBased on the observation, we further enhance\nour method with a disentangled regularization, aim-\ning to distinguish task-relevant and task-irrelevant\nfeatures. First, we construct two variants of the\noriginal input in a complementary view: (1) posi-\ntive variant, which maintains the high-confidence\nkeywords, and (2) negative variant, derived by\na ‚Äúcut-out-keyword‚Äù operation on the original in-\nput. Next, we propose a ‚Äútriplet-style loss‚Äù, which\nmakes predictions between the original input and\nthe positive variant similar while the predictions\nbetween the negative variant and the other two dif-\nferent. To illustrate the mechanism of our disentan-\ngled regularization, we go back to Figure 1 and take\nthe sentence ‚ÄúJobs founded apple in 1976‚Äù as an\nexample. Under the influence of the disentangled\nregularization, the positive variant tends to retain\nclue words for predictions (i.e., ‚Äúfounded apple‚Äù),\nwhile the negative variant, as the complement (i.e.,\n‚ÄúJobs in 1976‚Äù), tends to be task-irrelevant.\nWe evaluate our TDT on a wide range of neural\nlanguage understanding benchmark datasets in En-\nglish and Chinese, i.e., GLUE and CLUE, and our\nTDT affords strong predictive performance com-\npared with standard fine-tuning. Moreover, we\nconduct extensive analysis with respect to robust-\nness to perturbation, domain generalization, and\nlow-resource settings, from which we conclude:\n‚Ä¢ TDT learns reasonable confidence scores for\ninput tokens.\n‚Ä¢ TDT is robust to input perturbation and do-\nmain shift by encouraging the model to learn\nmore generalized features.\n‚Ä¢ TDT effectively captures the high-confidence\ndecisive cues for downstream tasks, thus alle-\nviating over-fitting in low-resource scenarios.\n2 Method\nIn this section, we begin with a brief introduc-\ntion of the vanilla Fine-tuning, and then introduce\nTask-guided Disentangled Tuning (TDT) in de-\ntail. Figure 2 shows the overall framework. TDT\nis composed of two parts: (1) token-level confi-\ndence model, which discovers the essential parts\nof inputs for the model prediction; (2) task-guided\nregularization, which promotes the model to de-\ncouple task-relevant keywords from non-keyword\ncontext.\n2.1 Vanilla Fine-tuning\nGiven an example of training data< X, y >, where\nX={x1, ..., xi, ..., xn} is the input sequence and y\nis its corresponding label. We first map each token\nxi to a real-valued vector ei by an embedding layer.\nThen, the packed embedding output E={ei} is fed\ninto the PLM to get the contextualized sentence\nrepresentations H={hcls, h1, ..., hn}, and the hid-\nden state hcls is used to conduct classification with\na MLP head. We fine-tune the parameters of the\nPLM with the cross entropy loss:\nLcla = ‚àílogP(y|H). (1)\n2.2 Token-level Confidence Model\nFor each token xi, we generate a scalar ci ‚àà [0, 1],\ncoined confidence score, by stacking a single-layer\nfeed-forward network with sigmoid activation on\nthe top of the embedding layer:\nci = œÉ(W ei + b), (2)\nwhere W and b are trainable parameters. Based on\nthe confidence score, we obtain a distilled sample\n{e+\ni } defined as\ne+\ni = ci ‚äô ei + (1‚àí ci) ‚äô ¬µ0, (3)\nwhere ¬µ0 is a perturbation term and ‚äô denotes\nelement-wise multiplication. Specifically, the per-\nturbation term ¬µ0 can be a zero vector, a random\nGaussian noise vector, or the average of the token\n3127\nùê∂ ùê∏\nùê∏\n1 ‚àí ùê∂ ùê∏\nùê∂ = ùúé(ùëäùê∏ + ùëè)\nConfidence score: ùê∂\nùê∏\nClassify \nloss\nPush Pull\nToken-level confidence model\n‚®Ä\n‚®Ä\nBert Encoder\nFigure 2: The overall framework of our proposed Task-guided Disentangled Tuning method.\nembedding, and we choose the last one in this pa-\nper. In this manner, for the distilled sample of each\ntraining instance, the higher the ci is, the more se-\nmantic information of the i-th token retains, while\nthe tokens with lower scores are perturbed.\nThen, the distilled sample { e+\ni } is fed into\nthe PLM to generate the sentence representations\nH+={h+\ni }. Inspired by ‚Äúdeletion game‚Äù (Fong\nand Vedaldi, 2017; V oita et al., 2019), the objective\nfunction of the confidence model is\nLC = ‚àílogP(y|H+) +Œ≥||C||2, (4)\nwhere C = {ci} is the set of confidence scores\nof X. The first term is the cross entropy loss of\nclassification on the distilled sample to encourage\nthe confidence model to assign higher scores to the\nmore decisive part of the input, and the second term\nserves as a penalty to prevent the model from mode\ncollapsing (i.e., always choosing ci=1).\n2.3 Task-Guided Regularization\nIt has been widely observed that the pretrained\nmodels tend to learn an easy-to-learn but not gener-\nalizable solution by vanilla fine-tuning on various\nNLP tasks (Sun et al., 2019; McCoy et al., 2019;\nMin et al., 2019; Niven and Kao, 2019). To alle-\nviate this issue, we further propose a triplet-style\nloss on the model predictions.\nSpecifically, for each input sequence, we derive\ntwo different variants: a positive variant and a neg-\native variant. The positive variant is expected to\nmaintain the most informative tokens to task pre-\ndiction and vice versa. As aforementioned, our\nconfidence model removes the meaningless tokens\nby setting the corresponding confidence scores to\nzero. Based on the confidence scores, we directly\ntreat the distilled sample generated by Eq. 3 as the\npositive variant and generate the negative variant\nas\ne‚àí\ni = (1‚àí ci) ‚äô ei. (5)\nGiven the original input and the two derived vari-\nants, we feed them into the PLM with the classifier,\nand obtain three prediction distributions P(y|H),\nP(y|H+), and P(y|H‚àí). Finally, we regularize\nthese distributions by a triplet ranking loss\nLR = max(m+d(P(y|H+), P(y|H)) ‚àí\nd(P(y|H‚àí), P(y|H)) ‚àí\nd(P(y|H‚àí), P(y|H+)), 0) (6)\nwhere m is a hyperparameter indicating a margin\nfor the loss and d(¬∑) denotes the Kullback-Leibler\n(KL) divergence. By minimizing LR, the positive\nvariant will be closer to the original input while\nthe negative variant will be farther from the other\ntwo. Thus, the model is encouraged to disentangle\ntask-relevant signals from task-irrelevant factors,\nand generate more general representations.\n2.4 Overall Training Objective\nThe final training objective is\nL = Lcla + Œ±LC + Œ≤LR, (7)\nwhere Œ± and Œ≤ are non-negative hyper-parameters\nto balance the effect of each loss term.\n3 Experiments\n3.1 Datasets\nWe evaluate our proposed method by fine-tuning\nthe pretrained models on the General Language Un-\nderstanding Evaluation (GLUE) (Wang et al., 2018)\nand the Chinese Language Understanding Evalu-\nation (CLUE) (Xu et al., 2020). Concretely, the\nGLUE benchmark has 8 different text classification\nor regression tasks including MNLI, MRPC, QNLI,\nQQP, RTE, SST-2, SST-B, and CoLA. The CLUE\nbenchmark includes 9 tasks spanning several single-\nsentence/sentence-pair classification tasks, and we\nchoose 5 tasks, OCNLI, IFLYTEK, CSL, TNEWS,\n3128\nModel MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg\nBERT-base\nFineTuning 84.5 90.9 91.3 92.8 60.5 88.7 85.1 67.5 82.66\nTDT 85.3 91.2 91.9 93.7 62.4 89.3 87.5 71.8 84.14\nBERT-large\nFineTuning ‚Ä† 86.6 91.3 92.3 93.2 60.6 90.0 88.0 70.4 84.05\nFineTuning 85.9 90.9 92.3 93.9 61.5 90.0 86.0 75.1 84.45\nTDT 86.4 91.4 92.6 94.3 66.2 89.9 88.5 75.8 85.64\nRoBERTa-large\nFineTuning ‚Ä† 90.2 92.2 94.7 96.4 68.0 92.4 90.9 86.6 88.92\nFineTuning 90.5 92.3 94.4 96.6 67.4 92.2 91.9 87.7 89.13\nTDT 90.6 91.9 94.7 97.0 69.3 92.5 93.1 91.0 90.01\nXLNet ‚Ä† 90.8 92.3 94.9 97.0 69.0 92.5 90.8 85.9 89.15\nELECRTA ‚Ä† 90.9 92.4 95.0 96.9 69.1 92.6 90.8 88.0 89.46\nDeBERTa ‚Ä† 91.1 92.4 95.3 96.8 70.5 92.6 91.9 88.3 89.86\nALBERT ‚Ä† 90.8 92.2 95.3 96.9 71.4 93.0 90.9 89.2 89.96\nTable 1: Experimental results on GLUE language understanding benchmark. When take RoBERTA-large as\nthe PLM, for RTE and STS, we follow Liu et al. (2019b) to finetune starting from the MNLI model instead of the\nbaseline pretrained model. Methods with ‚Ä† denote that we directly report the scores from corresponding paper, and\nothers are from our implementation.\nTask BERT-wwm-base MacBERT-large RoBERTa-wwm-large\nFineTuning TDT FineTuning TDT FineTuning TDT\nOCNLI 74.6 75.3 78.3 79.8 78.1 79.5\nIFLYTEK 60.8 62.2 61.5 61.8 61.8 62.9\nCSL 84.7 85.5 86.8 87.0 86.1 87.2\nTNEWS 56.9 57.3 58.5 58.7 59.0 59.2\nAFQMC 74.0 75.0 76.2 76.8 76.0 76.2\nAvg 70.20 71.06 72.26 72.82 72.20 73.00\nTable 2: Experimental results on CLUE language understanding benchmark. For TNEWS, we only use the\nraw ‚Äúsentence‚Äù for classification without the ‚Äúkeywords‚Äù information. For CSL, we only mask the ‚Äúabst‚Äù sequence\nand keep the ‚Äúkeywords‚Äù sequence unchanged in our proposed method.\nand AFQMC. The detailed data statistics and met-\nrics are provided in Appendix A.\n3.2 Model & Training\nWe use the pretrained models and codes provided\nby HuggingFace1. We take BERT-base (Devlin\net al., 2019), BERT-large (Devlin et al., 2019) and\nRoBERTa-large (Liu et al., 2019b) as our back-\nbones on GLUE, while BERT-wwm-base (Cui\net al., 2019), MacBERT-large (Cui et al., 2020),\nand RoBERTa-wwm-large (Cui et al., 2019) on\nCLUE. We tune the task specific hyper-parameters\nm ‚àà {0, 2}, Œ± ‚àà {0.5, 2, 4} and Œ≤ ‚àà {0.5, 1}. De-\ntailed experimental setups are shown in Appendix\nB. Following previous work (Lee et al., 2020; Agha-\n1https://github.com/huggingface/transformers\njanyan et al., 2020), we report results of the devel-\nopment sets, since the performance on the test sets\nis only accessible on the leaderboard with a limita-\ntion of the number of submissions.\n3.3 Main Results\nResults on GLUE. We illustrate the experimen-\ntal results on the GLUE benchmark in Table 1. We\ncan observe that the PLMs enhanced by TDT out-\nperforms FineTuning by a large margin across all\nthe tasks. Specifically, TDTs achieve 1.48 points,\n1.19 points and 0.88 points (on average) improve-\nment over BERT-base, BERT-large, and RoBERTa-\nlarge, respectively. In particular, BERT-base+TDT\nachieves competitive performance compared with\nBERT-large+FineTuning, showing that our method\n3129\nClass: 0\nClass: 1\nFigure 3: Visualization of representations of origi-\nnal input and two derived variants, where the triangle-\nshaped (pink), tri-up-shaped (purple), and tri-left-\nshaped (black) points denote the representations of orig-\ninal input, positive variants, and negative variants, re-\nspectively.\nis more efficient to find task-specific information\nfor downstream tasks. This may be because our\ntraining strategy prompts the models to predict with\nas little information as possible, isolating the task-\nrelated signals from the whole representations.\nRoBERT-large trained with TDT surpasses\nXLNet-large (Yang et al., 2019) ALBERT-xxlarge\n(Lan et al., 2019), DeBERTa-large (He et al., 2020),\nand ELECTRA-large (Clark et al., 2020), which\nare specially designed with different architectures\nand pre-training objectives.\nResults on CLUE. Table 2 shows the overall\nresults on the 5 tasks of CLUE benchmark. Con-\ncretely, TDT significantly outperforms FineTuning\non CSL, IFLYTEK, AFQMC, and OCNLI, and\nshows competitive results on the short text classi-\nfication task TNEWS, indicating the advantage of\nextracting important parts from long text or multi-\nple input sequences. Note that TNEWS generally\nrequires additional knowledge (e.g., keywords) as a\nsupplement due to the short input, and thus cannot\nshow the superiority of TDT.\n(a) MRPC\n(b) CoLA\nFigure 4: Distribution of confidence scores on MRPC\nand CoLA dev sets.\n4 Analysis & Discussion\n4.1 Visualization of Representations\nIn Figure 3, we plot t-SNE visualizations (van der\nMaaten and Hinton, 2008) of three kinds of repre-\nsentations generated by BERT-large trained with\nTDT on CoLA dev set. We can see that the repre-\nsentations of the original input are close to those\nof the positive variant in the same class. Although\nthe negative variant representations are really sim-\nilar to the original ones which derive the former,\nthey are clearly separated from the other represen-\ntations. The learned disentangled representations\nreveal that the model trained with TDT is able\nto distinguish task-specific keywords and non-\nkeyword context, which plays an important role\nin increasing models‚Äô robustness.\n4.2 Distribution of Confidence Scores\nWe investigate the learned confidence score dis-\ntributions in Figure 4. It shows that although the\ninitial distribution is consistent, the model learns\ndifferent task-specific patterns (confidence distribu-\ntions) on different tasks.\n4.3 Does our Confidence Model make a\nmeaningful estimation for input tokens?\nIn section 2.2, we mention that TDT uses a scalar\nfor evaluating the contribution of each input token.\nTo analyze whether the strategy can successfully\nlearn a meaningful importance estimation, we con-\n3130\n10%\n(a) Replacing token with ‚Äú[MASK]‚Äô‚Äô\n(b) Replacing token with a sampled word\n20% 30% 40% 50%\nFigure 5: Robustness to Input Perturbation. The Y-axis is the accuracy on the development set.\n10 20 30 40 50 60\n60.0\n70.0\n80.0\n90.0\ndropout rate %\nAccuracy\nFineT FineT-R\nOurs Ours-R\nFigure 6: Accuracy of BERT-large trained with dif-\nferent methods and evaluated on MPRC dev set with\ndifferent drop rates. We denote vanilla fine-tuning as\nFineT. The solid lines indicate results on the datasets\nconstructed by dropping tokens in descending order of\nconfidence scores. The dotted lines denotes results on\nthe datasets constructed by dropping tokens in increas-\ning order of confidence scores.\nstruct two sets of datasets based on MRPC dev set\nand then evaluate the performance of BERT-large\nwith TDT and standard fine-tuning. Specifically,\nwe convert the confidence scores to probability dis-\ntributions. We generate the first set of datasets by\ndropping input tokens in descending order of the\ndistributions and generate the second set in ascend-\ning order. In order to ensure language fluency, we\nreplace each dropped token with a ‚Äú[MASK]‚Äù to-\nken. The results are shown in Figure 6 and we\nobserve that:\n‚Ä¢ TDT is more robust to incomplete input\ncompared with Fine-tuning. Specifically, al-\nthough the performance of both FineTuning\nand TDT drops with the increase of dropout\nrate, our TDT achieves significantly better per-\nTask FineTuning TDT ‚àÜ\nMNLI (BERT-large)\nMNLI-m 85.8 86.4 +0.6\nQQP 73.1 74.2 +1.1\nOCNLI (MacBERT-large)\nCMNLI 70.6 71.8 +1.2\nBUSTM 64.8 66.4 +1.6\nTable 3: Performance of Domain Generalization .\nThe models are trained on MNLI/OCNLI but tested on\nout-of-domain data.\nformance than FineTuning over all datasets.\n‚Ä¢ Our learned confidence scores make rea-\nsonable assessments for each input token.\nParticularly, regardless of the dropout rates\nand the training methods, dropping input to-\nkens by the descending order of the masking\nscores always leads to worse performance.\n4.4 Robustness to Input Perturbation\nBased on the observation in Section 4.3, we fur-\nther investigate the robustness of TDT on perturbed\ndata. To construct perturbed data, we use the dev\nset of MRPC and possibly replace the input at each\nposition with a ‚Äú[MASK]‚Äù token or a token sam-\npled from the input sequence. For each dropout\nrate, we construct 10 datasets with different ran-\ndom seeds and draw violin plots of the performance\nof BERT-large trained with TDT and fine-tuning\n(Figure 5). We can see that Ours is consistently\nbetter than FineTuning in all groups, indicating the\nsuperior robustness to noisy data.\n3131\nTask FineTuning TDT ‚àÜ\nCLUE (MacBERT-large)\nOCNLI 60.85 ( ¬±2.66) 63.38 ( ¬±0.90) +2.53\nIFLYTEK 54.12 ( ¬±0.75) 54.78 ( ¬±0.94) +0.66\nCSL 80.25 ( ¬±1.36) 81.45 ( ¬±0.62) +1.20\nTNEWS 53.50 ( ¬±0.58) 53.33 ( ¬±0.25) -0.17\nAFQMC 64.77 ( ¬±3.87) 66.45 ( ¬±0.93) +1.68\nAvg 62.70 63.88 +1.18\nTable 4: Experimental results in low-resource scenar-\nios. We run 4 times for each task with different random\nseeds and report the average accuracy and the standard\ndeviation.\n4.5 Domain Generalization\nWe evaluate how well the trained models gener-\nalizes to out-of-domain data on MNLI and OC-\nNLI, Natural Language Inference (NLI) tasks of\nGLUE and CLUE respectively. In detail, we fine-\ntune BERT-large on MNLI, and test the accuracy\nof the fine-tuned models on other NLI datasets\nin different domains including MNLI-mismatch2\nand QQP. Besides, we fine-tune MacBERT-large\non OCNLI and conduct an evaluation on CMNLI3\nand BUSTM4. Detailed of Label Mapping is pro-\nvided in Appendix C. As Tabel 3 illustrates, TDT\noutperforms vanilla fine-tuning across different out-\nof-domain datasets. The results suggest that TDT\nencourages the model to learn more generalized\nfeatures rather than some superficial contextual\ncues unique to training data.\n4.6 Results in Low-resource Scenarios\nFine-tuning PLMs on very small amount of train-\ning data can be challenging and result in unstable\nperformance due to the serious over-fitting issue.\nIn this section, we explore the effectiveness of TDT\nin such scenarios. For each dataset in CLUE, we\nuse MacBERT-large and sample 1k training exam-\nples as its training data. As Table 4 demonstrates,\nTDT improves the accuracy by 1.18 on average and\nreduces the standard deviation by up to 2.94. It sug-\ngests that our TDT is more stable and efficient\nthan vanilla fine-tuning when training PLMs on\nlimited data.\n2MNLI-mismatch has different domains from MNLI train-\ning data\n3An NLI task of CLUE.\n4A short text matching task of FewCLUE (Xu et al., 2021a)\n4.7 Compared with Variants\nAblation Studies. We first conduct ablation stud-\nies to explore the effectiveness of two additional\nloss functions introduced in this paper and show\nthe results in Table 5. We find that removing any of\nthem leads to a performance drop, which indicates\ntheir effectiveness on regularization for training.\nSoft Perturbation vs. Hard Perturbation. The\nconfidence score in this paper is continuous value\nranging from 0 to 1, and we perturb the input in\na soft way. It is straightforward to investigate the\ndiscrete counterpart. To this end, we model the dis-\ncrete confidence score with the Gumbel-Softmax\ntrick (Jang et al., 2017). More detailed is intro-\nduced in Appendix D. We denote the model trained\nwith the hard strategy as TDT-hard and show the\ncomparison in Table 5. From the table, both TDT-\nhard and TDT yield better performance than vanilla\nfine-tuning. This observation supports our claim\nthat different tokens or phrases contribute differ-\nently to the final results, which can be detected by\ntask-guided signal and then used to model more\nreliable encoders by our proposed regularization.\nMoreover, the inferior performance of TDT-hard\nshows that naively removing tokens has an adverse\neffect on context modeling and thus it is better to\nregularize the over-reliance in a soft manner.\n4.8 Compared with Previous Methods\nTDT vs. Token Cutoff. Our method can also\nbe viewed as a soft variant of token cutoff (Shen\net al., 2020), which is a data augmentation strategy.\nTable 5 shows the results where we find thatTDT\nperforms better than TokenCutoff, which demon-\nstrates that the improvement of our method is not\nentirely due to the effect of data augmentation but\nstems from the design of the training objectives.\nTDT vs. R-drop & R3F. Recently, Liang et al.\n(2021) proposed R-drop to regularize the consis-\ntency of sub-models obtained through dropout.\nAghajanyan et al. (2021b) introduced R3F rooted\nin trust region theory, which adds noise into the\ninput embedding and minimize the KL divergence\nbetween prediction distributions given original in-\nput and noisy input. Both of them are task-agnostic,\nwhile our proposed method constructs two derived\nvariants with task signal, and concentrates on how\nto disentangle the task-relevant and task-irrelevant\nfactors. The better performance of TDT compared\nwith the strong R-drop and R3F baselines (Table 5)\n3132\nModel GLUE (RoBERTa-large) CLUE ( RoBERTa-www-large)\nSST-2 CoLA MRPC RTE Avg OCNLI IFLYTEK CSL TNEWS Avg\nFineTuning 96.6 67.4 91.9 87.7 85.90 78.1 61.8 86.1 59.0 71.25\nTokenCutoff ‚Ä† 96.9 70.0 90.9 90.6 87.10 78.2 61.8 86.1 59.2 71.33\nR-drop ‚Ä† 96.9 70.0 91.4 88.4 86.67 78.9 61.6 86.6 58.9 71.50\nR3F ‚Ä† 97.0 71.2 91.6 88.5 87.07 - - - - -\nPostTraining 95.0 64.7 91.2 84.1 83.75 76.5 62.1 87.0 58.9 71.13\nTDT w/oLC 96.4 69.3 91.9 89.5 86.77 78.6 61.9 86.9 59.0 71.60\nTDT w/oLR 96.4 66.7 91.4 90.6 86.28 79.2 62.1 86.9 58.9 71.77\nTDT-hard 96.7 67.6 92.2 90.3 86.70 79.1 62.5 87.0 59.1 71.93\nTDT 97.0 69.3 93.1 91.0 87.60 79.5 62.9 87.2 59.2 72.20\nTable 5: Results of RoBERTa-large trained with TDT, variants or previous methods on 4 GLUE tasks and 4 CLUE\ntasks. For GLUE, results with ‚Ä† are taken from the corresponding paper.\nverify the advantage of task-driven regularization.\nTDT vs. Post-Training. Post-training is an ef-\nfective approach to reduce the objective gap be-\ntween pretrained model and downstream tasks (Gu\net al., 2020), which continues to train PLMs on task\n(or in-domain) training data with mask language\nmodel (MLM) loss. The difference lies in that we\nfocus on the fine-tuning stage. Here, we compare\nTDT with the model first post-trained via MLM\non training set of each task and then fine-tuned.\nIt is surprising that post-training does not always\nhave a positive effect on downstream fine-tuning,\nwhile TDT shows effective performance without\nadditional post-training time consumption.\n5 Related Work\nFine-tuning large-scale PLMs tends to be a popular\nparadigm of various NLP tasks (Devlin et al., 2019;\nLiu et al., 2019a; Yang et al., 2019). However,\nthe fine-tuned models fail to capture task-specific\npatterns due to the imbalanced nature between the\nlarge number of parameters and limited training\ndata (Aghajanyan et al., 2020). To address this\nissue, two main research lines are proposed: (1)\ncontinual pretraining after general pre-training, (2)\nregularization techniques in fine-tuning.\nContinual pretraining of PLMs on unlabeled\ndata of a given downstream domain or task has\nbeen proved effective for the end-task performance\n(Gururangan et al., 2020), and various continual\npre-training objectives designed for different down-\nstream tasks have been proposed (Tian et al., 2020;\nWu et al., 2021). For example, Gu et al. (2020)\npropose a selective masking strategy to learn task-\nspecific patterns based on mid-scale in-domain data.\nHowever, such methods usually rely on extra in-\ndomain data and manually designed training objec-\ntives.\nDue to the overfitting problems of fine-tuning,\nlots of regularization techniques have been pro-\nposed. Lee et al. (2019) and Chen et al. (2020) reg-\nularize fine-tuned weights with original pretrained\nweights while others design adversarial training ob-\njectives or introduce noise into the input (Zhu et al.,\n2020; Jiang et al., 2020; Aghajanyan et al., 2020;\nShen et al., 2020; Yu et al., 2021; Hua et al., 2021;\nQu et al., 2020). Liang et al. (2021) regularize the\ntraining by minimizing the KL-divergence between\nthe output distributions of two sub-models sampled\nby dropout and Xu et al. (2021b) only updates a\nsub-set of the whole network during fine-tuning\nby selectively masking out the gradients in both\ntask-free and task-driven ways. Moon et al. (2020)\nhandle the over-reliance problem by reconstructing\nkeywords based on other words and making low-\nconfidence predictions without enough context.\n6 Conclusion\nIn this paper, we propose task-guided disentangled\ntuning for enhancing the efficiency and robustness\nof PLMs in downstream NLP tasks. Our method is\nable to efficiently distinguish task-specific features\nand task-agnostic ones, and bridges the gap be-\ntween pretraining and adaptation without the need\nof immediate continual training. Experiments on\nGLUE and CLUE benchmarks demonstrate the ef-\nfectiveness of our method, and extensive analysis\nshows the advantage in domain generalization and\nlow-resource setting over fine-tuning.\n3133\nReferences\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivas-\ntava, Xilun Chen, Luke Zettlemoyer, and Sonal\nGupta. 2021a. Muppet: Massive multi-task rep-\nresentations with pre-finetuning. arXiv preprint\narXiv:2101.11038.\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\n2020. Better fine-tuning by reducing representational\ncollapse. arXiv preprint arXiv:2008.03156.\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\n2021b. Better fine-tuning by reducing representa-\ntional collapse. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7870‚Äì7881, Online. As-\nsociation for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\narXiv preprint arXiv:2003.10555.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin\nWang, and Guoping Hu. 2020. Revisiting pre-trained\nmodels for Chinese natural language processing. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 657‚Äì668, Online. As-\nsociation for Computational Linguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing\nYang, Shijin Wang, and Guoping Hu. 2019. Pre-\ntraining with whole word masking for chinese bert.\narXiv preprint arXiv:1906.08101.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRuth C. Fong and Andrea Vedaldi. 2017. Interpretable\nexplanations of black boxes by meaningful pertur-\nbation. In IEEE International Conference on Com-\nputer Vision, ICCV 2017, Venice, Italy, October 22-\n29, 2017, pages 3449‚Äì3457. IEEE Computer Society.\nRobert Geirhos, J√∂rn-Henrik Jacobsen, Claudio\nMichaelis, Richard S. Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A. Wichmann. 2020.\nShortcut learning in deep neural networks. CoRR,\nabs/2004.07780.\nYuxian Gu, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan\nLiu, and Maosong Sun. 2020. Train no evil: Selective\nmasking for task-guided pre-training. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n6966‚Äì6974, Online. Association for Computational\nLinguistics.\nSuchin Gururangan, Ana Marasovi ¬¥c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don‚Äôt stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342‚Äì8360, Online. Association for Computational\nLinguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nHang Hua, Xingjian Li, Dejing Dou, Chengzhong Xu,\nand Jiebo Luo. 2021. Noise stability regularization\nfor improving BERT fine-tuning. In Proceedings of\nthe 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, pages 3229‚Äì3241,\nOnline. Association for Computational Linguistics.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Categori-\ncal reparameterization with gumbel-softmax. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xi-\naodong Liu, Jianfeng Gao, and Tuo Zhao. 2020.\nSMART: Robust and efficient fine-tuning for pre-\ntrained natural language models through principled\nregularized optimization. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2177‚Äì2190, Online. Association\nfor Computational Linguistics.\nMinki Kang, Moonsu Han, and Sung Ju Hwang. 2020.\nNeural mask generator: Learning to generate adap-\ntive word maskings for language model adaptation.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6102‚Äì6120, Online. Association for Computa-\ntional Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2019. Mixout: Effective regularization to fine-\ntune large-scale pretrained language models. arXiv\npreprint arXiv:1909.11299.\n3134\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2020. Mixout: Effective regularization to finetune\nlarge-scale pretrained language models. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nXiaobo Liang, Lijun Wu, Juntao Li, Yue Wang,\nQi Meng, Tao Qin, Wei Chen, Min Zhang, and Tie-\nYan Liu. 2021. R-drop: Regularized dropout for\nneural networks. CoRR, abs/2106.14448.\nHairong Liu, Mingbo Ma, Liang Huang, Hao Xiong,\nand Zhongjun He. 2019a. Robust neural machine\ntranslation with joint textual and phonetic embed-\nding. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n3044‚Äì3049, Florence, Italy. Association for Compu-\ntational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3428‚Äì3448, Florence,\nItaly. Association for Computational Linguistics.\nSewon Min, Eric Wallace, Sameer Singh, Matt Gardner,\nHannaneh Hajishirzi, and Luke Zettlemoyer. 2019.\nCompositional questions do not necessitate multi-hop\nreasoning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4249‚Äì4257, Florence, Italy. Association for\nComputational Linguistics.\nSeung Jun Moon, Sangwoo Mo, Kimin Lee, Jaeho Lee,\nand Jinwoo Shin. 2020. Masker: Masked keyword\nregularization for reliable text classification. arXiv\npreprint arXiv:2012.09392.\nTimothy Niven and Hung-Yu Kao. 2019. Probing neu-\nral network comprehension of natural language argu-\nments. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4658‚Äì4664, Florence, Italy. Association for Compu-\ntational Linguistics.\nYanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev,\nJiawei Han, and Weizhu Chen. 2020. Coda: Contrast-\nenhanced and diversity-promoting data augmentation\nfor natural language understanding. arXiv preprint\narXiv:2010.08670.\nDinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu,\nand Weizhu Chen. 2020. A simple but tough-to-beat\ndata augmentation approach for natural language un-\nderstanding and generation. CoRR, abs/2009.13818.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to fine-tune BERT for text classification?\nIn Chinese Computational Linguistics - 18th China\nNational Conference, CCL 2019, Kunming, China,\nOctober 18-20, 2019, Proceedings, volume 11856 of\nLecture Notes in Computer Science, pages 194‚Äì206.\nSpringer.\nHao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He,\nHua Wu, Haifeng Wang, and Feng Wu. 2020. SKEP:\nSentiment knowledge enhanced pre-training for sen-\ntiment analysis. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4067‚Äì4076, Online. Association for\nComputational Linguistics.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research, 9(86):2579‚Äì2605.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 5797‚Äì5808, Florence, Italy.\nAssociation for Computational Linguistics.\nAlex Wang, Jan Hula, Patrick Xia, Raghavendra Pappa-\ngari, R. Thomas McCoy, Roma Patel, Najoung Kim,\nIan Tenney, Yinghui Huang, Katherin Yu, Shuning\nJin, Berlin Chen, Benjamin Van Durme, Edouard\nGrave, Ellie Pavlick, and Samuel R. Bowman. 2019.\nCan you tell me how to get past sesame street?\nsentence-level pretraining beyond language model-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4465‚Äì4476, Florence, Italy. Association for Compu-\ntational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353‚Äì355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nHan Wu, Kun Xu, Linfeng Song, Lifeng Jin, Haisong\nZhang, and Linqi Song. 2021. Domain-adaptive pre-\ntraining methods for dialogue understanding. In Pro-\nceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 665‚Äì669,\nOnline. Association for Computational Linguistics.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,\nYudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong\nYu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,\nYiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,\nWeijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,\nYiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,\nQipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang\n3135\nYang, Kyle Richardson, and Zhenzhong Lan. 2020.\nCLUE: A chinese language understanding evaluation\nbenchmark. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 4762‚Äì4772. International Committee on\nComputational Linguistics.\nLiang Xu, Xiaojing Lu, Chenyang Yuan, Xuanwei\nZhang, Hu Yuan, Huilin Xu, Guoao Wei, Xiang\nPan, and Hai Hu. 2021a. Fewclue: A chinese\nfew-shot learning evaluation benchmark. CoRR,\nabs/2107.07498.\nRunxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan,\nBaobao Chang, Songfang Huang, and Fei Huang.\n2021b. Raise a child in large language model: To-\nwards effective and generalizable fine-tuning. arXiv\npreprint arXiv:2109.05687.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nQinyuan Ye, Belinda Z Li, Sinong Wang, Benjamin\nBolte, Hao Ma, Wen-tau Yih, Xiang Ren, and Ma-\ndian Khabsa. 2021. On the influence of masking\npolicies in intermediate pre-training. arXiv preprint\narXiv:2104.08840.\nYue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo\nZhao, and Chao Zhang. 2021. Fine-tuning pre-\ntrained language model with weak supervision: A\ncontrastive-regularized self-training approach. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1063‚Äì1077, Online. Association for Computa-\ntional Linguistics.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu. 2020. Freelb: Enhanced\nadversarial training for natural language understand-\ning. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nA GLUE and CLUE Benchmark\nIn this paper, we conduct experiments on 8 datasets\nin GLUE benchmark (Wang et al., 2018), and\n5 datasets in CLUE (Xu et al., 2020), including\nthe short text classification task TNEWS, the long\ntext classification tasks IFLYTEK and CSL, and\nsentence-pair classification tasks AFQMC and OC-\nNLI. The data statistics and evaluate metrics are\nillustrated in Table 6.\nDataset # Train # Dev Metrics\nGLUE\nMNLI 393k 9.8k Accuracy\nQQP 364k 40k Accuracy\nQNLI 105k 5.5k Accuracy\nSST-2 67k 872 Accuracy\nCoLA 8.5k 1.0k Matthews Corr\nSTS-B 5.7k 1.5k Spearman Corr\nMRPC 3.7k 408 Accuracy\nRTE 2.5k 277 Accuracy\nCLUE\nOCNLI 50k 3k Accuracy\nIFLYTEK 12.1k 2.6k Accuracy\nCSL 20k 3k Accuracy\nTNEWS 53.3k 10k Accuracy\nAFQMC 34.3k 4.3k Accuracy\nCMNLI 391k 12k Accuracy\nCLUEWSC 1.2k 304 Accuracy\nTable 6: Data Statistics and Evaluate Metrics.\nTask Batch Size Steps Warmup lr\nGLUE\nBERT-base\nMNLI 128 10000 1000 4e-5\nQQP 128 10000 1000 4e-5\nQNLI 64 3000 300 4e-5\nSST-2 64 3000 300 4e-5\nCoLA 64 2000 200 2e-5\nSTS-B 64 3000 300 4e-5\nMRPC 64 2000 200 1e-5\nRTE 64 2000 200 2e-5\nBERT-large & RoBERT-large\nMNLI 64 10000 1000 2e-5\nQQP 64 10000 1000 2e-5\nQNLI 64 3000 300 2e-5\nSST-2 64 3000 300 2e-5\nCoLA 32 3000 300 2e-5\nSTS-B 64 3000 300 2e-5\nMRPC 64 2000 200 2e-5\nRTE 64 2000 100 2e-5\nCLUE\nBERT-wwm-base\nOCNLI 64 3000 300 4e-5\nIFLYTEK 16 5000 300 3e-5\nCSL 32 3000 300 3e-5\nTNEWS 64 5000 300 3e-5\nAFQMC 32 3000 300 3e-5\nMacBERT-large & RoBERT-wwm-large\nOCNLI 32 3000 300 1e-5\nIFLYTEK 16 5000 300 1e-5\nCSL 32 3000 300 1e-5\nTNEWS 64 5000 300 1e-5\nAFQMC 32 3000 300 1e-5\nTable 7: Hyperparameters settings for different pre-\ntrained models on variant tasks.\nB Settings for Different Pretrained\nModels\nIn this paper, we fine-tuned different pretrained\nmodels with TDT, including BERT-base, BERT-\n3136\nlarge, RoBERTa-large for GLUE and BERT-wwm-\nbase, MacBERT-large, RoBERTa-wwm-large for\nCLUE. The batch size, training steps, warmup\nsteps, and learning rate are listed in Table 7.\nC Label Mapping in Domain\nGeneralization\nQQP has two labels, duplicate and not duplicate.\nWe map entailment to duplicate and map both neu-\ntral and contradiction to not duplicate. BUSTM\n5 is a short text matching task of FewCLUE (Xu\net al., 2021a). We use the public test set. BUSTM\nhas two labels, 0 and 1. We map entailment to label\n1, and map both neutral and contradiction to label\n0.\nD Detailed of TDT- hard\nGumbel-Softmax trick (Jang et al., 2017) is an ap-\nproximation to sampling from the argmax. For-\nmally, we replace Eq. 2 by:\nci = argmax(œÉGumbel(z(ei))), (8)\nœÉGumbel(zi) = exp((log(zi) +gi)/œÑ)PK\nj=1 exp((log(zj) +gj)/œÑ)\n,\n(9)\nwhere gi ‚àº Gumbel(0,1), z(¬∑) returns the logits pro-\nduced for a given input, and œÑ is the temperature.\nBy this way, if ci is 0, the embedding of the i-th to-\nken is set to the embedding of the ‚Äú[MASK]‚Äù token,\notherwise the embedding remains unchanged.\n5https://github.com/xiaobu-coai/BUSTM\n3137"
}