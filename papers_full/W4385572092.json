{
  "title": "Analyzing Bias in Large Language Model Solutions for Assisted Writing Feedback Tools: Lessons from the Feedback Prize Competition Series",
  "url": "https://openalex.org/W4385572092",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2731484145",
      "name": "Perpetual Baffour",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2874952297",
      "name": "Tor Saxberg",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214372288",
      "name": "Scott Crossley",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4292671242",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4200246476",
    "https://openalex.org/W2062022806",
    "https://openalex.org/W2049322609",
    "https://openalex.org/W2155243985"
  ],
  "abstract": "This paper analyzes winning solutions from the Feedback Prize competition series hosted from 2021-2022. The competition sought to improve Assisted Writing Feedback Tools (AWFTs) by crowdsourcing Large Language Model (LLM) solutions for evaluating student writing. The winning models are freely available for incorporation into educational applications, but the models need to be assessed for performance and other factors. This study reports the performance accuracy of Feedback Prize-winning models based on demographic factors such as student race/ethnicity, economic disadvantage, and English Language Learner status. Two competitions are analyzed. The first, which focused on identifying discourse elements, demonstrated minimal bias based on students' demographic factors. However, the second competition, which aimed to predict discourse effectiveness, exhibited moderate bias.",
  "full_text": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 242–246\nJuly 13, 2023c⃝2023 Association for Computational Linguistics\n \n \nAbstract \nThis paper analyzes winning solutions from \nthe Feedback Prize competition series  \nhosted from 2021-2022. The competitions \nsought to improve Assisted Writing \nFeedback Tools (AWFTs) by \ncrowdsourcing Large Language Model \n(LLM) solutions for evaluating student \nwriting. The winning LLM-based solutions \nare freely available for incorporation into \neducational applications, but the models \nneed to be assessed for performance and \nother factors. This  study reports the \nperformance accuracy of Feedback Prize -\nwinning models based on demographic \nfactors such as student race/ethnicity, \neconomic disadvantage, and English \nLanguage Learner status. Two competitions \nare analyzed. The first, which  focused on \nidentifying discourse elements, \ndemonstrated minimal bias based on \nstudents' demograp hic factors. However, \nthe second competition, which aimed to \npredict discourse effectiveness, exhibited  \nmoderate bias. \n1 Introduction \nAssisted writing feedback tools (AWFTs) are a \npromising example of educational applications \nusing Natural Language Processing (NLP) \nalgorithms that can innovate and accelerate student \nlearning (Nunes, Cordeiro, Limpo, & Castro, \n2022). Recent advances in large language models \n(LLMs) ha ve increased AWFTs’ capabilities to \nprocess and provide feedback on student writing \nwith human -like sophistication  (Kasneci et al., \n2023). The Feedback Prize competition series , \nhosted on Kaggle in 2021-2022, was an important \nstep in advancing AWFTs potential by \ncrowdsourcing innovative LLM solutions for \nassessing and evaluating student writing that were \nopen science (The Learning Agency Lab, n.d.).  \nThe competitions were a success with over \n6,000 teams participating and over 100,000 open-\nsource algorithms developed. (The Learning \nAgency Lab, n.d.) However, these algorithms have \nnot been reported outside of the Kaggle interface, \nlimiting knowledge of their use and minimizing \npotential ad option into educational applications. \nAdditionally, the algorithms have not been assessed \nfor bias, which may limit their effectiveness in a \nclassroom setting, especially if that bias is aimed \ntowards student populations that have been \nhistorically marginalized. The purpose of this study \nis to report initial performance for the winning \nFeedback Prize models and to disaggregate \nperformance accuracy in demographic factors \nincluding race/ethnicity, economic disadvantage, \nand English Language Learner (ELL) status.  \n2 PERSUADE Corpus \nThe first two competitions in the Feedback Prize \nseries were based on the PERSUADE (Persuasive \nEssays for Rating, Selecting, Analyzing, and \nUnderstanding Discourse Elements) corpus, a \ncollection of ~25,000 argumentative essays written \nby students in the U. S. in grades 6 through 12  \n(Crossley et al., 2022). The essays were annotated \nby experts for discourse elements and the \neffectiveness of the discourse elements. Discourse \nelements refer to a span of text that performs a \nspecific rhetorical or argumentative function, while \ndiscourse effectiveness is a rating of the quality of \nthe discourse element in supporting the writer's \noverall argument. The effectiveness scale included \nIneffective, Adequate, and Effective ratings. The \nannotation scheme for discourse elements is based \non an adapted or simplified version of the Toulmin \nargumentative framework (Stapleton & Wu, 2015). \nAnalyzing Bias in Large Language Model Solutions for  \nAssisted Writing Feedback Tools: Lessons from the  \nFeedback Prize Competition Series \n \n \n \nPerpetual Baffour, Tor Saxberg and Scott Crossley \n \n \n \n \n \n242\n \n \nThe discourse elements that were annotated for \neach essay were: \n• Lead. An introduction begins with a \nstatistic, a quotation, a description, or some \nother device to grab the reader’s attention \nand point toward the thesis. \n• Position. An opinion or conclusion on the \nmain question. \n• Claim. A claim that supports the position. \n• Counterclaim. A claim that refutes \nanother claim or gives an opposing reason \nto the position. \n• Rebuttal. A claim that refutes a \ncounterclaim. \n• Evidence. Ideas or examples that support \nclaims, counterclaims, rebuttals, or the \nposition. \n• Concluding Statement . A concluding \nstatement that restates the position and \nclaims. \nThe essays were annotated using a rigorous, \ndouble-blind rating process with 100 percent \nadjudication, such that each essay was \nindependently reviewed by two expert raters and \nadjudicated by a third rater. Overall inter -rater \nagreement for discourse elements assessed using a \nweighted Cohen’s Kappa was 0.73, which indicates \nrelatively high reliability. While the experts who \nannotated the corpus for discourse elements also \nrated each element's effectiveness in supporting the \nwriter’s argument, misalignment in segmentation \nbetween the raters in the discourse elements make \nit difficult to calculate inter-rater reliability for the \neffectiveness labels.  \n3 Feedback Prize 1.0 Models \nThe first Feedback Prize competition, \n(Feedback Prize 1.0: Evaluating Student Writing) \nwas hosted on Kaggle and  involved the task s of \nsegmenting essays into smaller sections and \nassigning each section a discourse label such as \nlead, position, claim, and evidence. To evaluate \nperformance, submissions were assessed based on \nthe word overlap between ground truth and \npredicted outputs. A model prediction was \nconsidered correct (true positive) if there was at \nleast a 50% word overlap between the machine -\nsegmented section and the human -segmented \nsection, as well as a match between their discourse \nlabel. False negatives were unmatched ground \ntruths, and false positives were unmatched \npredictions. The final score was calculated by  \nTable 1: True positive rate (TPR) by English Language \nLearner status of student writer, Feedback Prize 1.0 2nd \nplace  \n \nStatus N  TPR SD \nELL 7,565 0.717 0.235 \nNot ELL 81,207 0.726 0.220 \nAll 88,772 0.725 0.221 \n \nTable 2: True positive rate (TPR) by economic status of \nstudent writer, Feedback Prize 1.0 2nd place  \n \nStatus N  TPR SD \nDisadvantaged 35,696 0.713 0.226 \nNDA 42,698 0.743 0.214 \nAll 78,394 0.729 0.221 \n*Note: NDA refers to non-disadvantaged students. \n \ndetermining the number of true positives, false \npositives, and false negatives for each class (i.e., \ndiscourse label) and taking the macro F1 score \nacross all classes. \nThe analysis in this paper examines the second-\nplace, third -place, and sixth -place winning \nsolutions from this competition . Overall, the \nwinning solutions were broadly based on \nensembles of large-scale, pre-trained Transformers, \npaired with custom pre -processing and post -\nprocessing techniques to improve accuracy. The \nfirst-place model was not analyzed because its \ncomplexity made it difficult to replicate and \nimpractical in educational settings. The overall \nmacro F1 score did not differ significantly between \nthe second-place, third -place, and sixth -place  \nsolutions, with values of .740, .740, and .732, \nrespectively.  \nTo assess potential bias in the models, \nperformance accuracy was further disaggregated \nby demographic factors (race/et hnicity, English \nLanguage Learner  status, and economic \ndisadvantage) and discourse effectiveness \n(Ineffective, Adequate, Effective). Specifically, T-\ntests and ANOV As indicated that the average true \npositive rate (TPR) per essay of the second-place, \nthird-place, and sixth -place models significantly \nvaried based on demographic factors, but the effect \nsizes were small (see Tables 1-3). None of the t-\ntests or ANOV A tests reported any results with a p-\nvalue < 0.01 and a Cohen’s d > 0.2. For instance, \nthe t-test comparing TPR differences between ELL \nand non-ELL writing showed a p-value of 0.03 and \nCohen’s d of 0.103 for the second-place model,  \n243\n \n \nTable 3: True positive rate (TPR) by race/ethnicity of \nstudent writer, Feedback Prize 1.0 2nd place \n \nsuggesting a negligible difference in model \nperformance. \n4 Feedback Prize 2.0 Models \nThe second Feedback Prize competition \n(Feedback Prize 2.0: Predicting Effective \nArguments) also hosted on Kaggle required models \nto predict the effectiveness rating of discourse \nlabels, using multi -class logarithmic loss as the \nevaluation metric. More spec ifically, for each \ndiscourse label, the model had to submit the \nprobabilities (or the likelihood) that the label \nbelongs to each of the three effectiveness ratings \n(Ineffective, Adequate, Effective). The closer the \npredicted probabilities were to the actual true label, \nthe higher the model score would be. Feedback \nPrize 2.0 also prioritized computationally efficient \nalgorithms, with a prize -incentivized “Efficiency \nTrack” that evaluated submissions for both \naccuracy and speed.  \nFeedback Prize 2.0 comprised a smaller subset \nof the data from the first competition (around 6,900 \nout of the 26,000 essays), due to a need for greater \nbalance in effectiveness scores. In the complete \nPERSUADE corpus, only 4% of discourse \nelements were labeled Ineffective while 80% were \nlabeled Adequate and 16% were labeled Effective. \nThe subset used in Feedback Prize 2.0 corpus had \na distribution of 18% Ineffective, 24% Effective, \nand 58% Adequate, resulting in greater balance. \nThe analysis presented in this paper examines \nthe perform ance of the winning models (first, \nsecond, and third place) in the Efficiency Track on \nthe competition test set. A common trend among \nwinning solutions from the Efficiency Track was \nto fine -tune a single pre -trained Transformer \nmodel on the competition dat aset to minimize \nspace and runtime requirements.  The authors did \nnot analyze the winners from the non -efficiency \ntrack because performance was similar, but \ncomputational demands were much higher.  The \nanalysis consist s of two parts. The first part \nexamines the accuracy of the models in predicting \nthe three original effectiveness ratings (Ineffective, \nAdequate, Effective). In the second part, the \nwinning models' predictions were evaluated by \ngrouping Ineffective and Adequate labels into a \nNon-Effective label, creating a binary outcome \nvariable (Effective, Non-Effective). This analysis \nrecoded the labels 'post hoc,' after the model \nsubmitted probabilities for all three original ratings. \nIn both analyses, the model's predicted label was \ndetermined as the label with the highest predicted \nlikelihood among the outputted probabilities. \n \n4.1 Analysis of accuracy using original \neffectiveness ratings \nThe first part of the Feedback Prize 2.0 bias \nanalysis found that the selected win ning models \nRace/Ethnicity N TPR SD \nWhite 42,197 0.723 0.217 \nBlack 17,060 0.722 0.228 \nHispanic 23,055 0.712 0.229 \nAsian 6,814 0.777 0.198 \nAmerican Indian 574 0.728 0.226 \nMultiple  3,884 0.743 0.197 \nAll 93,584 0.726 0.221 \nFigure 1: Performance accuracy by ELL status of \nstudent writer and discourse effectiveness label, \nFeedback Prize 2.0 Efficiency Track 1st place  \n \nFigure 2: Performance accuracy by race/ethnicity of \nstudent writer and discourse effectiveness label, \nFeedback Prize 2.0  Efficiency Track 1st place \n \n244\n \n \nshowed higher levels of bias for certain students \ncompared to the winning models  from Feedback \nPrize 1.0.  This disparity can be attributed to \npatterns in the label distribution of the data. The \ndata sample for the Feedback Prize 2.0 competition \nhad a more balanced representation of minority and \nhistorically disadvantaged students in the overall \nsample, but there were roughly twice as many \ndiscourse elements labeled Ineffective from \neconomically disadvantaged students and almost  \nthree times as many Effective discourses from non-\ndisadvantaged students. \nAs a result, effective writing discourses  from \nwhite, non -ELL, and economically advantaged \nstudents were more likely to receive higher ratings \nand the models amplified the existing \ndisproportionate representation of effective writing \nfound in the human -rated dataset. As shown in \nFigure 1, the first-place model was more accurate \nin identifying effective discourses in non -ELL \nwriting (76% vs 27% accurate) with a statistically \nsignificant difference in likelihood scores (p-value \n~0.000) and a larger effect size (Cohen's d ~0.671), \nas shown in Table 4.  As shown in Table 5, the first-\nplace model was also less accurate in  predicting \neffective writing for economically disadvantaged \nstudents, and a t-test revealed that the difference in \nlikelihood scores for effective discourses was \nstatistically significant (p-value ~0.000) and the \neffect size was moderate (Cohen's d ~0.263). \nSimilarly, accuracy disaggregated by  the \nrace/ethnicity of each student writer also showed \nstatistically significant differences (p -values ~ \n0.000), but with small effect si zes (Cohen's d ~ \n0.15), as shown in Table 6 and Figure 2. \n \nTable 4: Likelihood scores for effective discourses by \nEnglish Language Learner status of student writer, \nFeedback Prize 2.0 Efficiency Track 1st place  \nStatus N Likelihood  SD \nELL 2,623 0.028 0.083 \nNot ELL 19,853 0.246 0.321 \nAll 22,476 0.221 0.311 \n \nTable 5: Likelihood scores for effective discourses by \neconomic status of student writer, Feedback Prize 2.0 \nEfficiency Track 1st place \n \nStatus N Likelihood  SD \nDisadvantaged 10,268 0.113 0.224 \nNDA 9,805 0.338 0.353 \nAll 20,073 0.223 0.315 \n*Note: NDA refers to non-disadvantaged students. \nTable 6: Likelihood scores for effective discourses by \nrace/ethnicity of student writer, Feedback Prize 2.0 \nEfficiency Track 1st place \n \nRace/ethnicity N Likelihood  SD \nWhite 9,816 0.270 0.328 \nBlack 4,157 0.133 0.246 \nHispanic 6,218 0.149 0.261 \nAsian 1,721 0.398 0.370 \nAm. Ind. 179 0.096 0.176 \nMultiple 888 0.250 0.321 \nAll 22,979 0.220 0.310 \n \n4.2 Analysis of accuracy using binary label of \neffectiveness \nThe second part of the analysis aimed to address \nthe low sample size of Ineffective discourses in the  \ndataset by recoding the effectiveness label as a \nbinary variable. This involved combining \nIneffective and Adequate discourses into  a Non-\nEffective label. The goal was to examine whether \nsimilar levels of bias persisted in the recoded label. \nCombining Adequate and Ineffective discourse  \nlabels into a Non -Effective category did achieve \ngreater balance in performance accuracy  for the \nNon-Effective label, but there remained bias in the \nprediction of Effective discourses because white, \nnon-ELL, and advantaged students  remain \noverrepresented in this category , as shown in \nFigure 3.  \n5. Discussion  \nThe winning solutions across the first two \nFeedback Prize competitions reported a degree of \naccuracy comparable to that of humans, which is an \nimportant indicator of the models’ strength. \nAdditionally, since the models are open -source, \nthey can quickly be a dapted into educational \napplications to not only assess student writing at a \nsummative level but to also provide fine -grained \nfeedback to students at the formative level.  \nHowever, as noted in the analyses above, the \nwinning solutions from the second competition that \nfocused on predicting effective arguments showed \na moderate degree of bias among factors related to \nrace/ethnicity, economic status, and English \nLanguage Learner (ELL) status while the winning \nsolutions from the first competition, which focused \non annotating discourse elements, showed minimal \nbias.  \nIt appears the models from Feedback Prize 2.0 \namplified the biases inherent in the data despite not \nbeing explicitly trained with demographic \n245\n \n \n \ninformation. Data bias in label distribution, label \nagreement, and demographic representation in the \nPERSUADE corpus may have contributed to the \nmodel bias, but it is unclear how well these factors \ncould be addressed  given current writing \nachievement disparities in the U.S. educational \nsystem (National Center for Education Statistics, \n2012). Using a binary classification for \neffectiveness (i.e., recoding the data as Effective or \nIneffective) helped to mitigate the bias in the \nmodels to some degree. However, the use of \nmodels from Feedback Prize 2.0 for educational \napplications should be handled with care, \nespecially when dealing with students from diverse \npopulations.  \nThese analyses demonstrate the importance of \nassessing algorithms for bias prior to wide -scale \nadoption. The results point to future work in \nbuilding educational NLP applications like AWFTs \nto identify potential data biases in label \ndistribution, agreeme nt, or demographic \nrepresentation before adoption to reduce bias in \nalgorithmic outputs and help ensure fairness in \nsystems. As can be seen with the PERSUADE \ncorpus, bias will likely be present in any dataset \nthat accurately represents populations in the United \nStates because of achievement disparities in the \neducational systems.    \nReferences  \nScott A. Crossley, Perpetual Baffour, Yu Tian, Aigner \nPicou, Meg Benner, and Ulrich Boser. 2022. The \npersuasive essays for rating, selecting, and \nunderstanding argumentative and discourse \nelements corpus 1.0 . Assessing Writing , 54. \nhttps://doi.org/10.1016/j.asw.2022.100667    \nKevin A. Hallgren . 2012. Computing inter -rater \nreliability for observational data: an overview and \ntutorial. Tutorials in Quantitative Methods for \nPsychology, 8(1):23–34. \nhttps://doi.org/10.20982%2Ftqmp.08.1.p023  \nEnkelejda Kasneci, Kathrin Sessler, Stefan \nKüchemann, Maria Bannert, Daryna Dementieva, \nFrank Fischer, Urs Gasser, Georg Groh, Stephan \nGünnemann, Eyke Hüllermeier, Stepha Krusche, \nGitta Kutyniok, Tilman Michaeli, Claudia Ne rdel, \nJürgen Pfeffer, Oleksandra Poquet, Michael Sailer, \nAlbrecht Schmidt, and Tina Seidel. 2023. ChatGPT \nfor good? On opportunities and challenges of large \nlanguage models for education . Learning and \nIndividual Differences , 103. \nhttps://doi.org/10.1016/j.lindif.2023.102274  \nThe Learning Agency Lab. (n.d.). The Feedback Prize: \nA case study in assisted writing feedback tools \nworking paper. https://www.the-learning-agency-\nlab.com/the-feedback-prize-case-study/  \nNational Center for Education Statistics. (2012). The \nNation’s Report Card: Writing 2011 (NCES 2012 -\n470). National Center for Education Statistics.  \nAndreia Nunes, Carolina Cordeiro, Teresa Limpo, and \nSão Luís Castro. 2021. Effectiveness of automated \nwriting evaluation systems in school settings: A \nsystematic review of studies from 2000 to 2020 . \nJournal of Computer Assisted Learning, 38(2):599–\n620. https://doi.org/10.1111/jcal.12635  \nE. Michael Nussbaum, CarolAnne M. Kardash, and \nSteve Graham. 2005. The effects of goal \ninstructions and text on the generation of \ncounterarguments during writing. Journal of \nEducational Psychology , 97(2):157–169. \nhttps://psycnet.apa.org/doi/10.1037/0022-\n0663.97.2.157  \nPaul Stapleton and Yanming (Amy) Wu. 2015. \nAssessing the quality of arguments in students' \npersuasive writing: A case study analyzing the \nrelationship between surface structure and \nsubstance. Journal of English for Academic \nPurposes, 17:12–23. \nhttps://doi.org/10.1016/j.jeap.2014.11.006  \n \nFigure 3: Performance accuracy for Non-Effective \nand Effective discourses, by student economic \nstatus, Feedback Prize 2.0 Efficiency Track 1st \nplace \n \n246",
  "topic": "Disadvantage",
  "concepts": [
    {
      "name": "Disadvantage",
      "score": 0.7553822994232178
    },
    {
      "name": "Competition (biology)",
      "score": 0.7528060078620911
    },
    {
      "name": "Computer science",
      "score": 0.6577616930007935
    },
    {
      "name": "Crowdsourcing",
      "score": 0.6360504627227783
    },
    {
      "name": "Language model",
      "score": 0.5152966976165771
    },
    {
      "name": "Series (stratigraphy)",
      "score": 0.4352056682109833
    },
    {
      "name": "Mathematics education",
      "score": 0.3531462550163269
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3402456045150757
    },
    {
      "name": "Psychology",
      "score": 0.27967381477355957
    },
    {
      "name": "World Wide Web",
      "score": 0.11773297190666199
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 5
}