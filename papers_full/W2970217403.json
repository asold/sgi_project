{
    "title": "Pretrained Language Models for Sequential Sentence Classification",
    "url": "https://openalex.org/W2970217403",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A1983754593",
            "name": "Arman Cohan",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2800204358",
            "name": "Iz Beltagy",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2088398548",
            "name": "Daniel King",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2081006310",
            "name": "Bhavana Dalvi",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A3020323472",
            "name": "Dan Weld",
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "University of Washington"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2886946814",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2962972512",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W2963691697",
        "https://openalex.org/W2147994374",
        "https://openalex.org/W2907644564",
        "https://openalex.org/W1572319397",
        "https://openalex.org/W2952138241",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2574535369",
        "https://openalex.org/W2963639288",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2307381258",
        "https://openalex.org/W2963940534",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W2951777553",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2251670640",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W4294554825",
        "https://openalex.org/W2963746755",
        "https://openalex.org/W2793978524",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2801930304",
        "https://openalex.org/W2896457183"
    ],
    "abstract": "As a step toward better document-level understanding, we explore\\nclassification of a sequence of sentences into their corresponding categories,\\na task that requires understanding sentences in context of the document. Recent\\nsuccessful models for this task have used hierarchical models to contextualize\\nsentence representations, and Conditional Random Fields (CRFs) to incorporate\\ndependencies between subsequent labels. In this work, we show that pretrained\\nlanguage models, BERT (Devlin et al., 2018) in particular, can be used for this\\ntask to capture contextual dependencies without the need for hierarchical\\nencoding nor a CRF. Specifically, we construct a joint sentence representation\\nthat allows BERT Transformer layers to directly utilize contextual information\\nfrom all words in all sentences. Our approach achieves state-of-the-art results\\non four datasets, including a new dataset of structured scientific abstracts.\\n",
    "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 3693‚Äì3699,\nHong Kong, China, November 3‚Äì7, 2019.c‚Éù2019 Association for Computational Linguistics\n3693\nPretrained Language Models for Sequential Sentence ClassiÔ¨Åcation\nArman Cohan 1‚ãÜ Iz Beltagy 1‚ãÜ Daniel King 1 Bhavana Dalvi 1 Daniel S. Weld 1,2\n1Allen Institute for ArtiÔ¨Åcial Intelligence, Seattle, W A\n2Allen School of Computer Science & Engineering, University of Washington, Seattle, W A\n{armanc,beltagy,daniel,bhavanad,danw}@allenai.org\nAbstract\nAs a step toward better document-level un-\nderstanding, we explore classiÔ¨Åcation of a se-\nquence of sentences into their corresponding\ncategories, a task that requires understanding\nsentences in context of the document. Recent\nsuccessful models for this task have used hier-\narchical models to contextualize sentence rep-\nresentations, and Conditional Random Fields\n(CRF s) to incorporate dependencies between\nsubsequent labels. In this work, we show\nthat pretrained language models,BERT (Devlin\net al., 2018) in particular, can be used for this\ntask to capture contextual dependencies with-\nout the need for hierarchical encoding nor a\nCRF . SpeciÔ¨Åcally, we construct a joint sen-\ntence representation that allows BERT Trans-\nformer layers to directly utilize contextual in-\nformation from all words in all sentences. Our\napproach achieves state-of-the-art results on\nfour datasets, including a new dataset of struc-\ntured scientiÔ¨Åc abstracts.\n1 Introduction\nInspired by the importance of document-level\nnatural language understanding, we explore clas-\nsiÔ¨Åcation of a sequence of sentences into their\nrespective roles or functions. For example, one\nmight classify sentences of scientiÔ¨Åc abstracts\naccording to rhetorical roles (e.g., Introduction,\nMethod, Result, Conclusion, etc). We refer to this\ntask as Sequential Sentence ClassiÔ¨Åcation ( SSC ),\nbecause the meaning of a sentence in a document\nis often informed by context from neighboring\nsentences.\nRecently, there have been a surge of new models\nfor contextualized language representation, result-\ning in substantial improvements on many natural\nlanguage processing tasks. These models use mul-\ntiple layers of LSTMs (Hochreiter and Schmidhu-\n‚ãÜEqual contribution.\nber, 1997) or Transformers (Vaswani et al., 2017),\nand are pretrained on unsupervised text with lan-\nguage modeling objectives such as next word pre-\ndiction (Peters et al., 2018; Radford et al., 2018)\nor masked token prediction (Devlin et al., 2018;\nDong et al., 2019). BERT is among the most suc-\ncessful models for many token- and sentence-level\ntasks (Devlin et al., 2018; Liu et al., 2019). In\naddition to a masked token objective, BERT opti-\nmizes for next sentence prediction, allowing it to\ncapture sentential context.\nThese objectives allow BERT to learn some\ndocument-level context through pretraining. In\nthis work we explore the use of BERT for SSC . For\nthis task, prior models are primarily based on hier-\narchical encoders over both words and sentences,\noften using a Conditional Random Field ( CRF )\n(Lafferty et al., 2001) layer to capture document-\nlevel context (Cheng and Lapata, 2016; Jin and\nSzolovits, 2018; Chang et al., 2019). These mod-\nels encode and contextualize sentences in two con-\nsecutive steps. In contrast, we propose an in-\nput representation which allows the Transformer\nlayers in BERT to directly leverage contextualized\nrepresentations of all words in all sentences, while\nstill utilizing the pretrained weights from BERT .\nSpeciÔ¨Åcally, we represent all the sentences in the\ndocument as one long sequence of words with spe-\ncial delimiter tokens in between them. We use the\ncontextualized representations of the delimiter to-\nkens to classify each sentence. The transformer\nlayers allow the model to Ô¨Ånetune the weights of\nthese special tokens to encode contextual informa-\ntion necessary for correctly classifying sentences\nin context.\nWe apply our model to two instances of theSSC\ntask in scientiÔ¨Åc text that can beneÔ¨Åt from better\ncontextualized representations of sentences: sci-\nentiÔ¨Åc abstract sentence classiÔ¨Åcation and extrac-\ntive summarization of scientiÔ¨Åc documents.\n3694\nOur contributions are as follows:\n(i) We present a BERT -based approach for SSC\nthat jointly encodes all sentences in the sequence,\nallowing the model to better utilize document-\nlevel context. (ii) We introduce and release\nCSA BSTRUCT , a new dataset of manually anno-\ntated sentences from computer science abstracts.\nUnlike biomedical abstracts which are written\nwith explicit structure, computer science abstracts\nare free-form and exhibit a variety of writing\nstyles, making our dataset more challenging than\nexisting datasets for this task. (iii) We achieve\nstate-of-the-art (SOTA) results on multiple datasets\nof two SSC tasks: scientiÔ¨Åc abstract sentence clas-\nsiÔ¨Åcation and extractive summarization of scien-\ntiÔ¨Åc documents.1\n2 Model\nIn Sequential Sentence ClassiÔ¨Åcation (SSC ), the\ngoal is to classify each sentence in a sequence\nof n sentences in a document. We propose an\napproach for SSC based on BERT to encode sen-\ntences in context. The BERT model architecture\nconsists of multiple layers of Transformers and\nuses a speciÔ¨Åc input representation, with two spe-\ncial tokens, [CLS] and [SEP], added at the be-\nginning of the input sentence pair and between\nthe sentences (or bag of sentences) respectively.\nThe pretrained multi-layer TRANSFORMER archi-\ntecture allows the BERT model to contextualize the\ninput over the entire sequence, allowing it to cap-\nture necessary information for correct classiÔ¨Åca-\ntion. To utilize this for the SSC task, we propose\na special input representation without any addi-\ntional complex architecture augmentation. Our ap-\nproach allows the model to better incorporate con-\ntext from all surrounding sentences.\nFigure 1 gives an overview of our model. Given\nthe sequence of sentences S = ‚ü®S1, ...,Sn‚ü© we\nconcatenate the Ô¨Årst sentence with BERT ‚Äôs delim-\niter, [SEP], and repeat this process for each sen-\ntence, forming a large sequence containing all to-\nkens from all sentences. After inserting the stan-\ndard [CLS] token at the beginning of this se-\nquence, we feed it into BERT . Unlike BERT , which\nuses the [CLS] token for classiÔ¨Åcation, we use\nthe encodings of the [SEP] tokens to classify\neach sentence. We use a multi-layer feedforward\nnetwork on top of the [SEP] representations of\n1Code & data: https://github.com/allenai/\nsequential_sentence_classification\nùêì ùêíùêÑùêèùêì ùê¨ùüèùêì ùêÇùêãùêí\nMLP\nBERT\n[SEP]Sentence\t1[CLS] [SEP]Sentence\t2 [SEP]Sentence\t3\nSentence\t3Sentence\t2Sentence\t1\nùêì ùêíùêÑùêèùêì ùê¨ùüè ùêì ùêíùêÑùêèùêì ùê¨ùüè\nùíöùüè ùíöùüê ùíöùüë\nDocument:\nMLP MLP\nFigure 1: Overview of our model. Each [SEP] token\nis mapped to a contextualized representation of its sen-\ntence and then used to predict a label yi for sentencei.\neach sentence to classify them to their correspond-\ning categories.2 Intuitively, through BERT ‚Äôs pre-\ntraining, the [SEP] tokens learn sentence struc-\nture and relations between continuous sentences\n(through the next sentence objective). The model\nis then Ô¨Ånetuned on task-speciÔ¨Åc training data,\nwhere most of the model parameters are already\npretrained using BERT and only a thin task-speciÔ¨Åc\nnetwork on top is needed. During Ô¨Ånetuning 3 the\nmodel learns appropriate weights for the [SEP]\ntoken to allow it to capture contextual information\nfor classifying sentences in the sequence. This\nway of representing a sequence of sentences al-\nlows the self-attention layers of BERT to directly\nleverage contextual information from all words in\nall sentences, while still utilizing the pretrained\nweights from BERT . This is in contrast to existing\nhierarchical models which encode then contextu-\nalize sentences in two consecutive steps.4\nHandling long sequences Released BERT pre-\ntrained weights support sequences of up to 512\nwordpieces (Wu et al., 2016). This is limiting for\nour model on datasets where the length of each\ndocument is large, as we represent all sentences\nin one single sequence. However, the semantics\nof a sentence are usually more dependent on local\ncontext, rather than all sentences in a long docu-\n2It is also possible to add another special token (e.g.,\n[CLS]) at the beginning of each sentence and perform classi-\nÔ¨Åcation on that token. Empirically, we found the approaches\nto perform similarly.\n3 Following terminology from Howard and Ruder (2018),\n‚ÄúÔ¨Ånetuning‚Äù refers to ‚Äútraining‚Äù a model that was previously\npretrained. We use both terms interchangeably.\n4It is possible to add a CRF layer or another contextual-\nizing layer on top of [SEP] tokens in our model, but em-\npirically, we did not Ô¨Ånd this addition to be helpful. One\nexplanation is that the self-attention layers of our model are\nalready capturing necessary contextual information from the\ndocument.\n3695\nPubMed NICTA CSAbstruct CSPubSum\n# docs 20K 1K 2.2K 21K\n# sents 225K 21K 15K 601K\nTable 1: Statistics of the evaluation datasets. The Ô¨Årst\nthree datasets are for the abstract sentence classiÔ¨Åcation\ntask and the last dataset is for summarization.\nment. Therefore, we set a threshold on the num-\nber of sentences in each sequence. We recursively\nbisect the document until each split has less sen-\ntences than the speciÔ¨Åed threshold. At a limit of 10\nsentences, only one division is needed to Ô¨Åt nearly\nall examples for the abstract sentence classiÔ¨Åca-\ntion datasets. A limitation of this approach is that\nsentences on the edge of the splits could lose con-\ntext from the previous(next) split. We leave this\nlimitation to future work.\n3 Tasks and Datasets\nThis section describes our tasks and datasets,\nand any model changes that are task-speciÔ¨Åc (see\nTable 1 for comparison of evaluation datasets).\n3.1 ScientiÔ¨Åc abstract sentence classiÔ¨Åcation\nThis task requires classifying sentences in scien-\ntiÔ¨Åc abstracts into their rhetorical roles (e.g., I N-\nTRODUCTION , METHOD , RESULTS , etc). We use\nthe following three datasets in our experiments.\nPUBMED-RCT (Dernoncourt and Lee, 2017)\ncontains 20K biomedical abstracts from PubMed,\nwith sentences classiÔ¨Åed as one of 5 categories\n{BACKGROUND , O BJECTIVE , M ETHOD , R E-\nSULT, C ONCLUSION }. We use the preprocessed\nversion of this dataset by Jin and Szolovits (2018).\nCSA BSTRUCT is a new dataset that we intro-\nduce. It has 2,189 manually annotated computer\nscience abstracts with sentences annotated accord-\ning to their rhetorical roles in the abstract, similar\nto the P UBMED-RCT categories. See ¬ß3.3 for de-\ntails.\nNICTA (Kim et al., 2011) contains 1,000\nbiomedical abstracts with sentences classiÔ¨Åed into\nPICO categories (Population, Intervention, Com-\nparison, Outcome) (Richardson et al., 1995).\n3.2 Extractive summarization of scientiÔ¨Åc\ndocuments\nThis task is to select a few text spans in a docu-\nment that best summarize it. When the spans are\nCSAbstruct characteristics\nDoc length (sentences) avg : 6.7 std : 1.99\nSentence length (words) avg : 21.8 std : 10.0\nLabel distribution\nBACKGROUND\nMETHOD\nRESULT\nOBJECTIVE\nOTHER\n0.33\n0.32\n0.21\n0.12\n0.03\nTable 2: Characteristics of our CSA BSTRUCT dataset\nsentences, this task can be viewed as SSC , classi-\nfying each sentence as a good summary sentence\nor not. Choosing the best summary sentences can\nbeneÔ¨Åt from context of surrounding sentences. We\ntrain on CSPUBSUMEXT (Collins et al., 2017), an\nextractive summarization dataset of 10k scientiÔ¨Åc\npapers, with sentences scored as good/bad sum-\nmary sentences using ROUGE overlap scores with\npaper highlights. For evaluation, a separate test\nset, CSP UBSUM, of 150 publications and their pa-\nper highlights is used.5\nA key difference between the training of our\nmodel and that of Collins et al. (2017) is that they\nuse the ROUGE scores to label the top (bottom) 20\nsentences as positive (negative), and the rest are\nneutral. However, we found it better to train our\nmodel to directly predict the R OUGE scores, and\nthe loss function we used is Mean Square Error.\n3.3 CSA BSTRUCT construction details\nCSA BSTRUCT is a new dataset of annotated com-\nputer science abstracts with sentence labels ac-\ncording to their rhetorical roles. The key differ-\nence between this dataset and P UBMED-RCT is\nthat PubMed abstracts are written according to\na predeÔ¨Åned structure, whereas computer science\npapers are free-form. Therefore, there is more va-\nriety in writing styles in CSA BSTRUCT . CSA B-\nSTRUCT is collected from the Semantic Scholar\ncorpus (Ammar et al., 2018). Each sentence is\nannotated by 5 workers on the Figure-eight plat-\nform,6 with one of 5 categories {BACKGROUND ,\nOBJECTIVE , M ETHOD , R ESULT , O THER }. Ta-\nble 2 shows characteristics of the dataset. We use\n8 abstracts (with 51 sentences) as test questions\nto train crowdworkers. Annotators whose accu-\nracy is less than 75% are disqualiÔ¨Åed from do-\ning the actual annotation job. The annotations are\n5Dataset generated using author provided\nscripts: https://github.com/EdCo95/\nscientific-paper-summarisation\n6http://figure-eight.com\n3696\nModel P UBMED CSA BST. NICTA\nJin and Szolovits (2018) 92.6 81.3 84.7\nBERT +Transformer 89.6 78.8 78.4\nBERT +Transformer+CRF 92.1 78.5 79.1\nOur model 92.9 83.1 84.8\nTable 3: Abstract sentence classiÔ¨Åcation (micro F1).\naggregated using the agreement on a single sen-\ntence weighted by the accuracy of the annotator\non the initial test questions. A conÔ¨Ådence score\nis associated with each instance based on the an-\nnotator initial accuracy and agreement of all anno-\ntators on that instance. We then split the dataset\n75%/15%/10% into train/dev/test partitions, such\nthat the test set has the highest conÔ¨Ådence scores.\nAgreement rate on a random subset of 200 sen-\ntences is 75%, which is quite high given the dif-\nÔ¨Åculty of the task. Compared with P UBMED-\nRCT , our dataset exhibits a wider variety of writ-\ning styles, since its abstracts are not written with\nan explicit structural template.\n4 Experiments\nTraining and Implementation We implement\nour models using AllenNLP (Gardner et al., 2018).\nWe use S CIBERT pretrained weights (Beltagy\net al., 2019) in both our model and BERT -based\nbaselines, because our datasets are from the sci-\nentiÔ¨Åc domain. As in prior work (Devlin et al.,\n2018; Howard and Ruder, 2018), for training, we\nuse dropout of 0.1, the Adam (Kingma and Ba,\n2015) optimizer for 2-5 epochs, and learning rates\nof 5e6, 1e5, 2e5, or 5e5. We use the largest batch\nsize that Ô¨Åts in the memory of a Titan V GPU (be-\ntween 1 to 4 depending on the dataset/model) and\nuse gradient accumulation for effective batch size\nof 32. We report the average of results from 3\nruns with different random seeds for the abstract\nsentence classiÔ¨Åcation datasets to control poten-\ntial non-determinism associated with deep neural\nmodels (Reimers and Gurevych, 2017). For sum-\nmarization, we use the best model on the valida-\ntion set. We choose hyperparameters based on the\nbest performance on the validation set. We release\nour code and data to facilitate reproducibility.7\nBaselines We compare our approach with two\nstrong BERT -based baselines, Ô¨Ånetuned for the\ntask. The Ô¨Årst baseline, BERT +Transformer, uses\n7https://github.com/allenai/\nsequential_sentence_classification\nModel R OUGE -L\nSAF + F Ens (Collins et al., 2017) 0.313\nBERT +Transformer 0.287\nOur model 0.306\nOur model + ABSTRACT ROUGE 0.314\nTable 4: Results on CSP UBSUM\nthe [CLS] token to encode individual sentences\nas described in Devlin et al. (2018). We add an\nadditional Transformer layer over the[CLS] vec-\ntors to contextualize the sentence representations\nover the entire sequence. The second baseline,\nBERT +Transformer+CRF , additionally adds a CRF\nlayer. Both baselines split long lists of sentences\ninto splits of length 30 using the method in ¬ß2 to\nÔ¨Åt into the GPU memory.\nWe also compare with existing SOTA mod-\nels for each dataset. For the P UBMED-RCT and\nNICTA datasets, we report the results of Jin and\nSzolovits (2018), who use a hierarchical LSTM\nmodel augmented with attention and CRF. We also\napply their model on our dataset, CSA BSTRUCT ,\nusing the authors‚Äô original implementation. 8 For\nextractive summarization, we compare to Collins\net al. (2017)‚Äôs model, SAF+F Ens, the model\nwith highest reported results on this dataset. This\nmodel is an ensemble of an LSTM-based model\naugmented with global context and abstract sim-\nilarity features, and a model trained on a set of\nhand-engineered features.\n4.1 Results\nTable 3 summarizes results for abstract sentence\nclassiÔ¨Åcation. Our approach achieves state-of-\nthe-art results on all three datasets, outperform-\ning Jin and Szolovits (2018). It also outperforms\nour BERT -based baselines. The performance gap\nbetween our baselines and our best model is\nlarge for small datasets (CSABSTRUCT , NICTA),\nand smaller for the large dataset (PUBMED-RCT ).\nThis suggests the importance of pretraining for\nsmall datasets.\nTable 4 summarizes results on CSP UB-\nSUM. Following Collins et al. (2017) we\ntake the top 10 predicted sentences as the\nsummary and use R OUGE -L scores for eval-\nuation. It is clear that our approach out-\nperforms BERT +TRANSFORMER . The BERT\n+TRANSFORMER +CRF baseline is not included\n8https://github.com/jind11/\nHSLN-Joint-Sentence-Classification\n3697\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP]\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP]\n(a) Before Ô¨Ånetuning\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP]\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP] (b) After Ô¨Ånetuning\nFigure 2: Self-attention weights of the top 2 layers of\nBERT for one abstract. Cell value in row i, column j, is\nthe maximum attention weight of token i attending to\ntoken j across all 12 Transformer attention heads.\nhere because, as mentioned in section 3, we train\nour model to predict R OUGE , not binary labels as\nin Collins et al. (2017). As in Collins et al. (2017),\nwe found the A BSTRACT -ROUGE feature to be\nuseful. Our model augmented with this feature\nslightly outperforms Collins et al. (2017)‚Äôs model,\nwhich is a relatively complex ensemble model and\nuses a number of carefully engineered features for\nthe task. Our model is a single model with only\none added feature.\nAnalysis To better understand the advantage\nof our joint sentence encoding relative to the\nBERT +Transformer baseline, we qualitatively ana-\nlyze examples from CSABSTRUCT that our model\ngets right and the baseline gets wrong. We found\nthat 34/134 of such examples require context to\nclassify correctly.9\nFor example, sentences 2 and 3 from one ab-\nstract are as follows: ‚Äú We present an improved\noracle for the arc-eager transition system, which\nprovides a set of optimal transitions [...].‚Äù, ‚ÄúIn\nsuch cases, the oracle provides transitions that\nwill lead to the best reachable tree [...].‚Äù. In iso-\nlation, the label for sentence 3 is ambiguous, but\nwith context from the previous sentence, it clearly\nfalls under the METHOD category.\nFigure 2 shows BERT self-attention weights\nfor the above-mentioned abstract before and after\nÔ¨Ånetuning. Before (Figure 2a), attention weights\ndon‚Äôt exhibit a clear pattern. After (Figure 2b),\nwe observe blocks along the matrix diagonal of\nsentences attending to themselves, except for the\nblock encompassing sentences 2 and 3. The words\nin these two sentences attend to each other, en-\nabling the encoding of sentence 3 to capture the\n9Of the 1349 examples in the test set, our model gets 134\ncorrect that the BERT +Transformer baseline gets wrong, and\nthe baseline gets 79 correct that our model gets wrong.\ninformation needed from sentence 2 to predict its\nlabel (see Appendix A for additional patterns).\n5 Related Work\nPrior work on scientiÔ¨Åc Sequential Sentence\nClassiÔ¨Åcation datasets (e.g. P UBMED-RCT and\nNICTA) use hierarchical sequence encoders (e.g.\nLSTMs) to encode each sentence and contextu-\nalize the encodings, and apply CRF on top (Der-\nnoncourt and Lee, 2017; Jin and Szolovits, 2018).\nHierarchical models are also used for summariza-\ntion (Cheng and Lapata, 2016; Nallapati et al.,\n2016; Narayan et al., 2018), usually trained in a\nseq2seq fashion (Sutskever et al., 2014) and eval-\nuated on newswire data such as the CNN/Daily\nmail benchmark (Hermann et al., 2015). Prior\nwork proposed generating summaries of scientiÔ¨Åc\ntext by leveraging citations (Cohan and Gohar-\nian, 2015) and highlights (Collins et al., 2017).\nThe highlights-based summarization dataset intro-\nduced by Collins et al. (2017) is among the largest\nextractive scientiÔ¨Åc summarization datasets. Prior\nwork focuses on speciÔ¨Åc architectures designed\nfor each of the tasks described in ¬ß3, giving them\nmore power to model each task directly. Our ap-\nproach is more general, uses minimal architecture\naugmentation, leverages language model pretrain-\ning, and can handle a variety of SSC tasks.\n6 Conclusion and Future Work\nWe demonstrated how we can leverage pre-\ntrained language models, in particular BERT , for\nSSC without additional complex architectures. We\nshowed that jointly encoding sentences in a se-\nquence results in improvements across multiple\ndatasets and tasks in the scientiÔ¨Åc domain. For fu-\nture work, we would like to explore methods for\nbetter encoding long sequences using pretrained\nlanguage models.\nAcknowledgments\nWe would like to thank Matthew Peters, Waleed\nAmmar and Hanna Hajishirzi for helpful discus-\nsions, Madeleine van Zuylen for help in crowd-\nsourcing and data analysis, and the three anony-\nmous reviewers for their comments and sugges-\ntions. Computations on beaker.org were sup-\nported in part by credits from Google Cloud.\nOther support includes ONR grant N00014-18-1-\n2193 and the WRF/Cable Professorship,\n3698\nReferences\nWaleed Ammar, Dirk Groeneveld, Chandra Bha-\ngavatula, Iz Beltagy, Miles Crawford, Doug\nDowney, Jason Dunkelberger, Ahmed Elgohary,\nSergey Feldman, Vu Ha, Rodney Kinney, Sebas-\ntian Kohlmeier, Kyle Lo, Tyler C. Murray, Hsu-\nHan Ooi, Matthew E. Peters, Joanna Power, Sam\nSkjonsberg, Lucy Lu Wang, Christopher Wilhelm,\nZheng Yuan, Madeleine van Zuylen, and Oren Et-\nzioni. 2018. Construction of the literature graph\nin semantic scholar. In Proceedings of the Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies (Proceedings of the Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL-HLT)).\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientiÔ¨Åc text. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nMing-Wei Chang, Kristina Toutanova, Kenton Lee, and\nJacob Devlin. 2019. Language model pre-training\nfor hierarchical document representations. CoRR,\nabs/1901.09128.\nJianpeng Cheng and Mirella Lapata. 2016. Neural\nsummarization by extracting sentences and words.\nCoRR, abs/1603.07252.\nArman Cohan and Nazli Goharian. 2015. ScientiÔ¨Åc ar-\nticle summarization using citation-context and arti-\ncle‚Äôs discourse structure. In Empirical Methods in\nNatural Language Processing (EMNLP).\nEd Collins, Isabelle Augenstein, and Sebastian Riedel.\n2017. A supervised approach to extractive summari-\nsation of scientiÔ¨Åc papers. In CoNLL.\nFranck Dernoncourt and Ji Young Lee. 2017. Pubmed\n200k rct: a dataset for sequential sentence classiÔ¨Åca-\ntion in medical abstracts. In IJCNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. CoRR, abs/1810.04805.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. UniÔ¨Åed language\nmodel pre-training for natural language understand-\ning and generation. ArXiv, abs/1905.03197.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew E.\nPeters, Michael Schmitz, and Luke S. Zettlemoyer.\n2018. Allennlp: A deep semantic natural language\nprocessing platform. ArXiv, abs/1803.07640.\nKarl Moritz Hermann, Tomas Kocisky, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. Teaching ma-\nchines to read and comprehend. In Advances in\nneural information processing systems, pages 1693‚Äì\n1701.\nSepp Hochreiter and J¬®urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9:1735‚Äì\n1780.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model Ô¨Åne-tuning for text classiÔ¨Åcation. In\nACL.\nDi Jin and Peter Szolovits. 2018. Hierarchical neu-\nral networks for sequential sentence classiÔ¨Åcation in\nmedical scientiÔ¨Åc abstracts. In Empirical Methods\nin Natural Language Processing (EMNLP).\nSu Kim, David Mart¬¥ƒ±nez, Lawrence Cavedon, and Lars\nYencken. 2011. Automatic classiÔ¨Åcation of sen-\ntences to support evidence based medicine. In BMC\nBioinformatics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nJohn D. Lafferty, Andrew McCallum, and Fernando\nPereira. 2001. Conditional random Ô¨Åelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data. In ICML.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint.\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2016.\nSummarunner: A recurrent neural network based se-\nquence model for extractive summarization of docu-\nments. In AAAI.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Ranking sentences for extractive summariza-\ntion with reinforcement learning. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (NAACL-HLT).\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee, and\nLuke S. Zettlemoyer. 2018. Deep contextualized\nword representations. In Proceedings of the Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies (NAACL-HLT).\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Preprint.\nNils Reimers and Iryna Gurevych. 2017. Reporting\nscore distributions makes a difference: Performance\nstudy of lstm-networks for sequence tagging. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\n3699\nWilliam S. Richardson, Mark C. Wilson, James A.\nNishikawa, and Robert S. Hayward. 1995. The well-\nbuilt clinical question: a key to evidence-based de-\ncisions. ACP journal club, 123 3:A12‚Äì3.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104‚Äì3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Jeff Klingner,\nApurva Shah, Melvin Johnson, Xiaobing Liu,\nLukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\nGregory S. Corrado, Macduff Hughes, and Jeffrey\nDean. 2016. Google‚Äôs neural machine translation\nsystem: Bridging the gap between human and ma-\nchine translation. CoRR, abs/1609.08144.\nA Additional analysis\nFigures 3 and 4 show attention weights ofBERT\nbefore and after Ô¨Ånetuning. We observe that be-\nfore Ô¨Ånetuning, the attention patterns on [SEP]\ntokens and periods is almost identical between\nsentences. However, after Ô¨Ånetuning, the model\nattends to sentences differently, likely based on\ntheir different role in the sentence that requires dif-\nferent contextual information.\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP]\nsen. 7\n[SEP]\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP]\nsen. 7\n[SEP]\n(a) Before Ô¨Ånetuning\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP]\nsen. 7\n[SEP]\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP]\nsen. 7\n[SEP] (b) After Ô¨Ånetuning\nFigure 3: Visualization of attention weights for layer 8\nof BERT before and after Ô¨Ånetuning.\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP]\nsen. 7\n[SEP]\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP]\nsen. 7\n[SEP]\n(a) Before Ô¨Ånetuning\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP]\nsen. 7\n[SEP]\n[CLS]\nsen. 1\n[SEP]\nsen. 2\n[SEP]\nsen. 3\n[SEP]\nsen. 4\n[SEP]\nsen. 5\n[SEP]\nsen. 6\n[SEP]\nsen. 7\n[SEP] (b) After Ô¨Ånetuning\nFigure 4: Visualization of attention weights in Ô¨Ånal\nlayer (layer 12) of BERT before and after Ô¨Ånetuning."
}