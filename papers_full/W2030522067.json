{
  "title": "Document Categorization with Modified Statistical Language Models for Agglutinative Languages",
  "url": "https://openalex.org/W2030522067",
  "year": 2010,
  "authors": [
    {
      "id": "https://openalex.org/A1911400333",
      "name": "Ahmet Cüneyd Tantuğ",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6634143331",
    "https://openalex.org/W6603916055",
    "https://openalex.org/W1579838312",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2087609354",
    "https://openalex.org/W10704533",
    "https://openalex.org/W2151565769",
    "https://openalex.org/W2150137772",
    "https://openalex.org/W1533946607",
    "https://openalex.org/W2773847562",
    "https://openalex.org/W4244175933",
    "https://openalex.org/W2075201173",
    "https://openalex.org/W2596331574",
    "https://openalex.org/W57290567",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W2075858744",
    "https://openalex.org/W137495139",
    "https://openalex.org/W2126163471",
    "https://openalex.org/W4234752074",
    "https://openalex.org/W57814663",
    "https://openalex.org/W2083217544",
    "https://openalex.org/W2151752770",
    "https://openalex.org/W2098162425",
    "https://openalex.org/W2148392518",
    "https://openalex.org/W4233559841",
    "https://openalex.org/W2998864486",
    "https://openalex.org/W2122384383",
    "https://openalex.org/W2062289558",
    "https://openalex.org/W2082092506",
    "https://openalex.org/W1924689489",
    "https://openalex.org/W2092067718",
    "https://openalex.org/W2004316800",
    "https://openalex.org/W2043909051",
    "https://openalex.org/W1500332810",
    "https://openalex.org/W2161628678",
    "https://openalex.org/W1482339734",
    "https://openalex.org/W1496310549",
    "https://openalex.org/W2094710333",
    "https://openalex.org/W2171886309",
    "https://openalex.org/W1552882209",
    "https://openalex.org/W1987680958",
    "https://openalex.org/W1650656906",
    "https://openalex.org/W1841959837",
    "https://openalex.org/W2132502917",
    "https://openalex.org/W274041255",
    "https://openalex.org/W2058373514",
    "https://openalex.org/W2093390569",
    "https://openalex.org/W182831726",
    "https://openalex.org/W2113641473",
    "https://openalex.org/W573488971",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W1904228841",
    "https://openalex.org/W2170332815",
    "https://openalex.org/W4206765718",
    "https://openalex.org/W2106429851",
    "https://openalex.org/W2050938027",
    "https://openalex.org/W2118020653",
    "https://openalex.org/W1576676390",
    "https://openalex.org/W1511932253",
    "https://openalex.org/W2123244406",
    "https://openalex.org/W1904835850",
    "https://openalex.org/W2022597264",
    "https://openalex.org/W1490796714"
  ],
  "abstract": "In this paper, we investigate the document categorization task with statistical language models. Our study mainly focuses on categorization of documents in agglutinative languages. Due to the productive morphology of agglutinative languages, the number of word forms encountered in naturally occurring text is very large. From the language modeling perspective, a large vocabulary results in serious data sparseness problems. In order to cope with this drawback, previous studies in various application areas suggest modified language models based on different morphological units. It is reported that performance improvements can be achieved with these modified language models. In our document categorization experiments, we use standard word form based language models as well as other modified language models based on root words, root words and part-of-speech information, truncated word forms and character sequences. Additionally, to find an optimum parameter set, multiple tests are carried out with different language model orders and smoothing methods. Similar to previous studies on other tasks, our experimental results on categorization of Turkish documents reveal that applying linguistic preprocessing steps for language modeling provides improvements over standard language models to some extent. However, it is also observed that similar level of performance improvements can also be acquired by simpler character level or truncated word form models which are language independent.",
  "full_text": " \nDOCUMENT CATEGORIZATION WITH MODIFIED STATISTICAL LANGUAGE \nMODELS FOR AGGLUTINATIVE LANGUAGES \nAhmet Cüneyd TANTUĞ \nComputer Engineering Department, Istanbul Technical University \nİstanbul Teknik Üniversitesi Ayazaga Yerleskesi Elektrik-Elektronik Fakültesi, Maslak \nIstanbul, 34469, Turkey \nE-mail : tantug@itu.edu.tr \nwww.itu.edu.tr \n \n \n \nAbstract \nIn this paper, we investigate the document categorization task with statistical language models. Our study mainly \nfocuses on categorization of documents in agglutinative languages. Due to the productive morphology of \nagglutinative languages, the number of word forms encountered in naturally occurring text is very large. From the \nlanguage modeling perspective, a large vocabulary results in serious data sparseness problems. In order to cope \nwith this drawback, previous studies in various application areas suggest modified language models based on \ndifferent morphological units. It is reported that performance improvements can be achieved with these modified \nlanguage models. In our document categorization experiments, we use standard word form based language models \nas well as other modified language models based on root words, root words and part-of -speech information, \ntruncated word forms and character sequences. Additionally, to find an optimum parameter set, multiple tests are \ncarried out with different language model orders and smoothing methods. Similar to previous studies on other tasks, \nour experimental results on categorization of Turkish documents reveal that applying linguistic preprocessing steps \nfor language modeling provides improvements over standard language models to some extent. However, it is also \nobserved that similar level of performance improvements can also be acquired by simpler character level or \ntruncated word form models which are language independent. \nKeywords: document categorization, statistical language modeling, n-gram, Turkish \n1. Introduction \nDuring past decades, the proliferation of documents \naccessible in digital form boosts the need for \ninformation retrieval related tasks. One of the \nchallenging information retrieval problems is document \ncategorization which is basically the task of assigning a \ndocument to one or more of the predefined set of \ncategories. The document categorization term \nsometimes referred as text/document classification\na. \nDocument classification has found many application \nareas such as document filtering (e.g. spam filtering), \ndocument indexing based on a controlled vocabulary, \ndocument organization, identification of topic or \nlanguage, and word sense disambiguation. Also some \napplications combine document classification and other \nmethods. For example, speech can be categorized by \n                                                 \na We use document/text classification and document/text \ncategorization terms interchangeably throughout this paper. \nInternational Journal of Computational Intelligence Systems, Vol.3, No. 5 (October, 2010), 632-645\nPublished by Atlantis Press \n    Copyright: the authors \n                    632\nA. C. Tantuğ \n \nmeans of text categorization performed after speech \nrecognition. Similarly, multi-media document \ncategorization can be done by analyzing the textual \ncaptions in the document. \nAlthough the history of document classification goes \nback to 1960s, the field gained its outstanding status in \n80s and 90s. While automatic document classification \nsystems of these years were generally rule based \nsystems built by knowledge and domain experts, a shift \noccurred towards machine learning based approaches in \nthe beginning of 90s. The state of the art document \nclassification techniques rely on both machine learning \nand information retrieval paradigms. Generally, \nautomatic document classification systems are \nimplemented by supervised machine learning \ntechniques where a set of hand-labeled documents must \nbe supplied for teaching algorithms how to classify a \nnew document. Some of the most popular techniques \nare Naïve Bayes classifiers, decision trees, Support \nVector Machines (SVM), Artificial Neural Networks \n(ANN), multi-variate regression models and k-nearest \nneighborhood classifiers (kNN). Sebastiani 1 gives a \ncomprehensive review of automatic document \ncategorization. Naïve Bayes technique is frequently \napplied in classification tasks because of its simplicity \nand relatively high accuracy. Despite of its \noversimplified assumptions, it is shown that Naïve \nBayes can achieve superior results among others 2-4. \nBriefly, Naïve Bayes assumes that each term (word) is \nindependent from others, which means the document is \nprocessed as a bag- of-words where the order of the \nwords is not taken into account. If the contexts of the \nwords are incorporated into the classification process, \nNaïve Bayes functions very similar to n -gram models. \nThis similarity emerged the recent idea of using n-gram \nbased statistical language models in text categorization5. \nEven though statistical language models was first \nused by the speech recognition community 6, a number \nof various applications like information retrieval 7, \nmachine translation 8, part- of-speech tagging 9 and \nparsing10 benefit from the advantages of statistical \nlanguage models. Language models have the ability to \nassign a concrete probability to a sequence of utterances \nsuch as words, letters or syllables. This basic language \nmodeling property appeal researchers from many fields, \neven from bioinformatics11. \nChoosing statistical language models as a classifier \npresents some advantages for document classification. \nAs all machine learning algorithms need a formal \nrepresentation of the samples ( documents in this \ncontext), a feature selection phase is essential before \nclassification. In this phase, according to their \nrepresentational and discriminative properties, some \nterms are selected to build a feature space on which the \nclassification algorithms can work. This stage is an \nimportant initial step which can affect not only the \nlearning process but also the efficiency of classification. \nUnfortunately, feature selection process often depends \non the task and the language. A number of pre-\nprocessing efforts like stop-word removal and stemming \nare language dependent. Furthermore, different \napplications require different choice of features; for \nexample text genre identification tasks look for \nlinguistic patterns whereas topic spotting tasks generally \nwork with bag- of-words. Statistical language models \neliminate the feature selection process since the features \nare selected implicitly. Additionally, a wide number of \nresearches on advanced smoothing methods provide \neligibility for language models over traditional methods \nin text classification. \nThe main motivation of our work is investigating \nand improving the performance of statistical language \nmodel based document categorization for agglutinative \nlanguages. Even though there exists an extensive \nliterature on text classification, the exploitation of \nstatistical language models in text classification is a \nrecent research topic5. In this study, Peng et al. obtained \nthe state of the art results with Naïve Bayes classifi ers \naugmented with statistical language models. Owing to \nthe vocabulary explosion in agglutinative languages, \nstatistical language models for these languages suffer \nfrom data sparseness problem. In previous works on \nother application areas excluding text classification, \nvarious language model modifications are offered for \nbetter modeling of the agglutinative structure, and \nconsiderable improvements are achieved. We aim to \nevaluate and improve the performance of the text \nclassification in agglutinative languages with modified \nlanguage models proposed in prior studies. Also, the \neffect of the document length on classification success \nis evaluated in the scope of our work.  \nThe rest of the paper is organized as follows. \nSection 2 gives brief information about related works. \nSection 3 describes the basics of statistical language \nmodels while Section 4 gives the details of statistical \nlanguage model based text categorization. Section 5 \nPublished by Atlantis Press \n    Copyright: the authors \n                  633\n Document Categorization for Agglutinative Languages \nintroduces problematic issues of language modeling for \nagglutinative languages. The modifications to language \nmodels are explained in Section 6. The experiments and \nrelated results are presented in Section 7. Finally, we \nevaluate the results and draw some conclusions in \nSection 8. \n2. Related Work \nFor text classification, n-grams are used in many ways. \nThe most common approach is considering n-gram \nphrases as terms during feature selection process prior \nto classification 12-14. On the other hand, several studies \ncan be found dealing with the text classification based \non prediction by partial matching (PPM) based language \nmodels15, 16 andtopic models 17. Text categorization with \nletter based PPM method performs better than word \nbased Naïve Bayes classifiers, and even close results to \nlinear support vector machine text classifiers 18. But, the \nimplementation of statistical language models in its \ntraditional fashion, which is assigning a probability to a \nsequence of words, is first applied by augmenting the \nNaïve Bayes classifier with statistical language \nmodels19. Their statistical language model based on \nchain augmented Naïve Bayes classifiers (CAN) are \napplied on three different text classification problems \n(topic detection, authorship determination and text \ngenre classification) in four different languages \n(English, Greek, Chinese and Japanese). Although some \ndata sets are limited to draw a generalization, an overall \nassessment states that they get state- of-the-art results \nand even better.  \nThe sophisticated morphosyntactic structures of \nagglutinative languages led researchers to take some \npreprocessing steps in document classification tasks. \nThe most common step for dimensionality reduction is \nword form normalization. It is shown that lemmatization \nor stemming can result in considerable performance \nincrements in information retrieval tasks for many \nagglutinative languages such as Finnish 20, Hungarian 21, \n22 and Basque 23. Also being an agglutinative language, \nnumerous similar studies on Turkish has been done. For \ndifferent applications, various methods are used to \nperform document classification on Turkish texts. For \nexample, Naïve Bayes, support vector machines, \ndecision trees are used to identify the authors of the \nTurkish documents 24. Another work concentrates on \nspam filtering task on Turkish e-mails employing \nartificial neural networks 25, 26. There exists two studies \nwith the focus on classification of Turkish news \nspecifically27, 28. In the former work, kNN and a time-\nefficient improvement of kNN, feature projection text \nclassification, techniques are used in the classification \nof a dataset consisting of 20K news articles. The latter \nresearch involves a comparative performance evaluation \nof Naïve Bayes and artificial neural network classifiers \non a dataset which consists only 50 news articles. \nCataltepe et al. 29 analyze the effect of stemming as a \npre-processing step when centroid classification \nalgorithm and support vector classifier are applied in \nTurkish document classification tasks . They conclude \nthat roots consisting only consonants can achieve the \nhighest performance for the cases where the \nrepresentational term vector cardinality should be small. \n3. Statistical Language Models \nStatistical language models define probability \ndistributions on word sequences. By using a language \nmodel, one can compute the probability of a sentence S \n1 2 1( ... ) k\nkw w w w\n by the following formula: \n \n1 2 1 k 1 k-1\nK\ni-1\ni1\n1\nP(S) = P(w )P(w |w )...P(w |w ...w )\n= P(w |w ).\ni\n\n \n(1) \nThis means that the probability of any word \nsequence can be calculated by decomposition using the \nchain rule, but usually due to sparseness, most terms \nabove would be zero , therefore n-gram approximations \nare used. N-gram models predict the probability of a \nword from the previous N-1 words by using Markov \nassumption. \nK\ni - 1\ni i - N + 1\n1\nP(S) = P(w |w ).\ni\n\n \n \n(2) \nThe most intuitive way to estimate the n-gram \nprobabilities is maximum likelihood estimation which is \nsimply counting word occurrences in the corpus and \ncalculating the relative frequency: \n-1\ni - 1 - 1 - 1\ni i - N + 1 -1 -1\n- 1 - 1\nCount( ) Count( )P( | ) . Count( ) Count( )\nii\ni N i i N\nii\ni N i N\nw w www ww\n\n\n\n  \n(3) \nHowever, some of the word sequences have zero \ncounts because every corpus is limited and cannot \ncontain all legal word sequences in the language. Also, \nPublished by Atlantis Press \n    Copyright: the authors \n                  634\nA. C. Tantuğ \n \nzero probabilities assigned to unseen word sequences in \nthe corpus cause any sentence S containing even one of \nthose unseen word sequences get P(S)=0 for the whole \nsentence. For these reasons, some of the probability \nmass from observed word sequences is distributed over \nzero counts, which is called smoothing. A good \ncomparison of simple and advanced smoothing \ntechniques can be found in 30, 31. Commonly used \nsmoothing techniques are absolute smoothing 32, Laplace \nsmoothing33-35, Witten-Bell smoothing 36, Good-Turing \nsmoothing37, 38 and Kneser-Ney smoothing32. \nThe correct way of evaluating the performance of a \nlanguage model is evaluating the total performance of \nthe application after embedding it in the application 39. \nEmploying such an evaluation scheme can be \nburdensome for many applications, so perplexity is \nfrequently used as a language model evaluation \nmeasure. However, the results in our work are presente d \nby means of document categorization performance \nscores instead of representing the perplexities of the \nrelated language models. \n4. Text Categorization with Statistical \nLanguage Models \nText categorization can be defined as assigning a \ndocument dj to the category ci from the set C, where \nciC={c1,c2,…, c N}. In order to apply supervised \nmachine learning techniques to build an automatic \nclassifier, a set of documents must be provided with \ntheir correct categories. Documents in this training set  \nare represented in the form (dj, ci), where dj represents \nthe j-th document in the collection D={d1,d2,…,dM} and \nci is the category of the document dj. Finally, a model \nM:D\nC that maps documents to classes must be \ngenerated by using a training procedure . It is very \ncommon to use probabilistic models focused on finding \nthe class \nc\n  that maximizes the probability P(ci|dj), \nwhich can be re-written as in Eq. (4) by the help of \nBayes rule: \n( | ) ( )( | ) . ()\nj i i\nij\nj\nP d c P cP c d Pd\n\n \n \n(4) \nSince the probability P(dj) is constant for a specific \ndocument dj, the probability of P(dj) can be ignored and \nthe most probable class \nc\n  for the document dj can be \ncalculated as in Eq. (5).  \narg max ( | )\n( | ) ( )arg max ()\narg max ( | ) ( ).\nij\ni\nj i i\nji\nj i i\ni\nc P c d\ncC\nP d c P c\nPdcC\nP d c P c\ncC\n\n\n\n\n\n\n \n(5) \nThe prior class probabilities P(ci), where i=1,2,…,N, \nare generally calculated straight forward by using \nmaximum likelihood method. Different training \nprocedures try to estimate P(dj|ci), the likelihood of \ndocument dj for category ci, in various ways. Statistical \nlanguage models can also be used in calculation of this \nconditional probability distribution. For each class ci, a \nseparate language model is generated by using training \ndocuments in category ci. Each language model LMi can \ncompute the likelihood of a new document dj for \ncategory ci, and argmax function searches for the most \nprobable class \nc\naccording to formula in Eq. (6). In \nother words, each class ci has its own language model \nwhich calculates how much the language of a new \ndocument resembles the language usage in this \ncategory. \n1 2 | |\nˆ argmax ( | ) ( )\nargmax ( ... | ) ( ).\ni\nj\ni\nj i i\ncC\nj j j d i i\ncC\nc P d c P c\nP w w w LM P c\n\n\n\n\n \n(6) \nThe choice of statistical language models for \nestimating P(dj|ci) presents a number of advantages \nagainst other ML techniques. The elimination of feature \nengineering is the most apparent one. Typically, vector \nspace model is used as data representation where each \ndocument is represented by a high-dimensional vector \nof word counts or binary flags for the existence of the \ncorresponding words in that document 40. The selection \nof the words constituting the representation space is \nimplemented by an external effort which may be \nsubjective. On the other hand, statistical language \nmodels calculate the contribution of each unit (word or \nn-gram) to the model and perform selection according to \nthe importance of the words implicitly. \nClassification with unigram language models using \nLaplace smoothing functions almost the same as Naïve \nBayes classification 5. However, choosing the language \nmodel order higher than one increases the ability to \nPublished by Atlantis Press \n    Copyright: the authors \n                  635\n Document Categorization for Agglutinative Languages \nmodel longer contexts which capture the discriminative \nproperties of word order. Additionally, statistical \nlanguage models can benefit from more efficient \nsmoothing methods for unseen words (or word \nsequences) as described in previous section. Chen and \nGoodman31 points out that the performance of Laplace \nsmoothing is outperformed by many advanced \nsmoothing methods. In a recent work about Naïve \nBayes based text classification, it is shown that \nimproved smoothing techniques yields better and more \nstable performance than Laplace smoothing41. \n5. Issues with Statistical Language Modeling \nfor Agglutinative Languages \nIn agglutinative languages such as Turkish, Finnish, \nHungarian and Estonian, words are formed by \nconcatenation of morphemes extensively. Due to the \nrich morphological structure, the number of word forms \nthat can be encountered in natural text is very large than \nother languages. For instance, it is reported that in a \n10M word corpora of English and Turkish, the number \nof distinct English word forms is 97,734 whereas the \nnumber of Turkish word forms is 417,775 9. The \nproductive derivational and inflectional suffixations of \nTurkish allows generation of more than one million \nlegitimate word forms from only one Turkish root \nword42. Even in some cases, just one word form may \nconvey the equivalent semantic information of a whole \nEnglish phrase, clause or sentence. Below is a popular \n(but exaggerated) example which demonstrates the \ncomplex morphological process of Turkish43: \n \nuygarlaştıramayabileceklerimizdenmişsinizcesine \n \nuygar+laş+tır+ama+yabil+ecek+ler+imiz+den+miş+siniz+cesine \n \n“(behaving) as if you were one of those whom we might not be \nable to civilize’’ \n \nAs another example, the list in Table 1\n shows the \nword forms of the root word “ futbol” (football) \nencountered in the corpora, along with their unigram \ncountsb. It is noted that a single Turkish verb root can \nhave around 40,000 forms excluding the derivational \nsuffixes44. Similarly, a Finnish verb may have 12,000 \nforms whereas a Finnish noun may have 2,000 forms \nbecause of the highly inflected Finnish morphology20. \n                                                 \nb Only first 10 of the total 42 word forms are included in the table \nbecause of space limitations. \nTable 1. Some example data set observations of the word \nforms whose root are “futbol” \nWord forms Count English Translation \nfutbol 59 football \nfutbola 2 to the football \nfutbolcu 105 football player \nfutbolcudan 1 from the football player \nfutbolcular 39 football players \nfutbolculara 5 to the football players \nfutbolcularda 1 at the football players \nfutbolculardan 10 from the football players \nfutbolcularla 1 with the football players \nfutbolcularımız 1 our football players \n \nFrom the point of view of statistical language \nmodeling, the large number of distinct word forms, i.e. \nvocabulary size, causes significant data sparseness \nproblems. The large vocabulary size not only \nnecessitates larger training sets for better statistical \nmodeling but also lead to estimate much more \nparameters even for small order language models.  \n6. Language Model Modifications \nIn order to alleviate the data sparseness drawback posed \nby morphology, a number of studies propose several \nmodifications over language modeling. Although the \napplication areas of these studies are varying, they all \nachieve to have remarkable levels of improvements \nwhen they incorporate the underlying morphology in the \nmodel. Arısoy et al. 45 investigate alternative language \nmodeling units like “stems and endings” , “stems and \nmorphemes”, and “syllables”, instead of ”words” in \nspeech recognition tasks. A recent work which splits \nwords into their stems and suffix components results in \na significant perplexity reduction in Turkish language \nmodeling46. Since some morphemes carry long distance \ndependencies in Turkish sentences, language models \nbased on units comprising word stem and its last \ninflectional morpheme group 9, 47, yield better results \nthan word form based models in machine translation48. \nSimilarly, information retrieval and indexing tasks \nalso suffer from the unlimited vocabulary properties of \nagglutinative languages. The problem is generally \ntackled by stemming approach49-51. \nOne of the targets in this work is the classification \nperformance evaluation of the modified language \nmodels proposed for different tasks in previous studies. \nThe basic model is classifying texts with statistical \nPublished by Atlantis Press \n    Copyright: the authors \n                  636\nA. C. Tantuğ \n \nlanguage models based on word forms where standard \nlanguage models are trained without any language \ndependent information. This model will construct a \nbaseline for future evaluations of classification \nperformance with different language model types. \nOur next language model type utilizes only root \nwords in training, similar to previous efforts 45, 46. In \nmost of the languages, roots of the word forms are \nobtained by basic suffix stripping stemmers such as \nPorter Stemmer for English 52. However, having root \nwords from word forms is a non-trivial task for Turkish. \nA morphological analyzer should be used to get root \nwords as well as other morphological features. \nMoreover, morphological disambiguation must be \napplied as the next step since roughly 50% of the \nmorphological analyzer output is ambiguous. We have \nused a two-level morphological analyzer 53 and a \nstatistical morphological disambiguation tool based on 9. \nIn our dataset, the total number of distinct word forms is \n68,420, whereas the total number of distinct root words \nis 23,739 after morphological analysis and \ndisambiguation. This means approximately 65% \nreduction in the vocabulary size of the language models.  \nThe part- of-speech tag may help discriminating \nsome synonymous word roots (e.g. ara+Verb (to \nsearch) and ara+Noun (distance/space)) in some \ncases 54. So, another type of language model, which is \nbased on part-of-speech (POS) tags being attached to \nroot words, is included in our evaluation scheme. \nLanguage model modifications mentioned above \nrequire language dependent tools. Thus, language \nindependency advantage, being one of the most useful \nproperties of language model based document \nclassification method, vanishes. To mitigate this \ndisadvantage, language independent methods are \ninvestigated for getting root words from word forms. \nSo, another type of language model is suggested based \non the fact that average root word length is 4.03 letters \nin Turkish55. This approach is basically inspired from an \ninformation retrieval oriented work 51. According to this \nFirstFLetters model, first F ( F=3,..,7) letters are \ntruncated from the beginning of every word form so that \na convergence can be maintained to the root word based \nmodels by evading any language specific process. \nRecently, it has been shown that using truncated words \nin indexing rather than the actual root words produced \nby a sophisticated stemmer simply improves the system \neffectiveness of search engines 51. A similar truncating \napproach is also suggested for rapid and feasible \nTurkish information retrieval system50.  \nLastly, character level language models are \nincorporated into our study. This type of language \nmodel uses character sequences as the base unit instead \nof words. Although no study on Turkish language \nmodeling applies character based fashion, it has been \nshown that character level language model classifiers \nare able to acquire high levels of accuracy in English, \nJapanese and Chinese texts 5. Particularly, this approach \nis well suited for the languages suffer from word \nsegmentation problems, such as Chinese and Japanese. \nNevertheless, it has been shown that even for Western \nlanguages like English and Greek, character level \nlanguage model classification can also perform higher \nclassification performance than word level models 19. \nWith this motivation, character level language model \ntypes are also participated in our tests. It is noteworthy \nto point out that our previous model which just truncates \nthe first F character of the word form is completely \ndifferent with the character level models. For example, \nthe first 3 character trigram language model run on first \n3 character truncated version of the word forms, but it \nstill operates on word level. However, a character level \ntrigram language model should be thought as a 3 \ncharacter width sliding window moving on all \ncharacters constituting the word forms (also \nwhitespaces). \nTable 2  shows base unit examples of a trigram \nsequence for the language models mentioned in this \nsection. \n7. Experiments \n7.1. Data set \nWe have generated a new data set from scratch by \nprocessing the news broadcasted by Anadolu Agency, \nthe national news agency of Turkey c. A similar dataset \nfrom the same source was used in a previous work 27, but \nthe dataset is not publicly available. The agency \nprovides news in eight different categories listed in \nTable 3. \nThe dataset is composed of 20,000 downloaded \ndocuments that are evenly distributed among categories. \nThese files are cleaned up from HTML tags and parsed \nto get useful information on the page. The final data set \n                                                 \nc http://www.aa.com.tr \nPublished by Atlantis Press \n    Copyright: the authors \n                  637\n Document Categorization for Agglutinative Languages \ncontains 2,500 news documents per category, each \nstored in separate XML files d. Sample file content is \ngiven in Fig. 1. \nTable 3. News categories in data set \nCode Category \n1 Turkey News \n2 World News \n3 Politics \n4 Economics \n5 Sports \n6 Education and Science \n7 Culture and Art \n8 Environment and Health \n \nAlthough some meta data such as broadcast date and \ntime are available for our documents, the classification \nis accomplished only on the basis of endogenous \nknowledge which means the knowledge extracted from \nthe documents. \n \n \n<id>100043</id> \n<topic>07-CultureArt</topic> \n<date>31.03.2007</date> \n<time>20:48:00</time> \n<title>Başkentte \"İz Resim Ve Heykel \nSergisi\"</title> \n<text>Başkentte, 15 sanatçının 200 eserinin \nyer aldığı, \"İZ resim ve heykel sergisi\" \naçıldı.</text> \nFig. 1. Sample File Content \nTable 4  presents some statistical properties of the \ndata set. \n \n \n                                                 \nd The compiled corpus can be downloaded from http://ddi.ce.itu.edu.tr \nTable 4. Some statistical properties of the data set \nTotal Number of  \ndocuments 20,000 \ndocuments per category 2,500 \nsentences 31,381 \nsentences per document 1.56 \ntokens 671,819 \ntokens per document 33.59 \ntokens per sentence 21.40 \n7.2. Experimental results \nIn order to have a fair evaluation, 10-fold cross-\nvalidation technique is used where the complete dataset \nis randomly divided in 10 parts. The usual train and test \nprocesses run 10 times; at each step, 9 parts are used for \ntraining and the testing is done on the remaining fold. \nSRILM toolkit 56 is used for training language models. \nWe have conducted experiments with different language \nmodel orders (n=1,…,5) and smoothing methods to find \nan optimum parameter set. To maintain the \ncomparability of our results with previous studies on \nclassification with statistical language models, the \nclassification results are given in terms of F1 score, \nwhich is the  harmonic mean of precision and recall. \nHowever, please note that F1 is not a golden metric for \nall kinds of applications  since the importance of recall \nand precision rates are not equal  for various \napplications. For instance, spam filtering is a text \nclassification task in which mistakenly classifying a \nlegitimate mail as spam is a much more severe error \nthan classifying a spam mail as legitimate. In this case, \nit would not be an appropriate  solution t o optimize an \nalgorithm by using F1 metric where recall and precision \nrates have equal weights. \nThe multi-class classification performance is \nrepresented by macro F1 measure computed by \naveraging the F1 score over individual confusion \nTable 2. Base unit examples for language models \nType Base Unit Example \nWord form … uçağında yaptığı açıklamada … \nRoot … uçak yap açıkla … \nRoot+POS … uçak+Noun yap+Verb açıkla+Verb … \nFirst3Letter … uça yap açı … \nFirst4Letter … uçağ yapı açık … \nFirst5Letter … uçağı yapıl açıkl … \nFirst6Letter … uçağın yapıla açıkla … \nCharacter level …u ç a ğ ı n d a # y a p t ı ğ ı # a ç ı k l a m a d a … \n \nPublished by Atlantis Press \n    Copyright: the authors \n                  638\nA. C. Tantuğ \n \nmatrices from each category 1. Table 5 shows results for \nthe baseline (word form) system. The classification \nperformances with each smoothing method are \nrepresented in two columns; average macro F1 scores \nare given in the first column and standard deviations are \nshown in the next column. \nPlease note that, the performance of the unigram \n(n=1) language model using Laplace smoothing \n(denoted by the bold characters in the table) is \napproximatelye the Naïve Bayes classifier performance. \nFor a better visual presentation, results are depicted in \nFig. 2 . Although the best classification performance is \nobtained by the language models with Good-Turing \nsmoothing and n=3,4,5; it will be reasonable to use \ntrigram ( n=3) models instead of higher order language \nmodels for the sake of simplicity. From the figure, apart \nfrom Kneser-Ney smoothing where n=1, the Laplace \nsmoothing is outperformed by all other smoothing \nmethods considered in this work. Furthermore, having \nlarger context ( n>1) slightly contributes to the \nperformance. \nEven with this baseline system, it can be said that n-\ngram based classification is able to achieve significant \nprogress over Naïve Bayes method with the advantage \nof better smoothing methods and ability to process \nlonger regularities in the text. \nIn a previous news classification effort on a very \nsmall Turkish news data set including only 25 \ndocuments, it is reported that Naïve Bayes achieves 0.76 \naccuracy28. However, in our study, which is carried out \non a relatively larger data set, the Naïve Bayes  \nperformance is computed as 0.7268. \n                                                 \ne Because the feature selection process is not automatic and subject to \nchange \nOn the other hand, from the point of view of \nperformance comparison between Naïve Bayes \nclassifiers in Turkish and other languages like English, \nresults on Turkish word form based text categorization \nshows that it fails to reach the accuracy on English. \nPeng et al. 5 states that their unigram n-gram classifier \nwith laplace smoothing has 0.8493 accuracy on \nclassification of English 20 newsgroup data set \ncontaining 19,974 documents. This important \nperformance difference (approximately 14%) reflects \nthe negative effects of the large vocabulary size in \nTurkish. However, the gap between Turkish and English \nclassification performance shrinks to some extent when \nhigh order n-grams with advanced smoothing methods \nare employed. The best performing word-level F1-score \non English is 0.8822 5 whereas it is calculated as 0.7852 \non Turkish side. \n \n \nFig. 2. Performance of word form based language models \nwith different smoothing methods along with language model \norder \nTable 5. Results for word form based classification \n Laplace \n(LP) \nKneser-Ney \n(KN) \nWitten-Bell \n(WB) \nAbsolute \n(AB) \nGood-Turing \n(GT) \nn macroF1 σ macroF1 σ macroF1 σ macroF1 σ macroF1 σ \n1 0.7268 0.0060 0.5821 0.0066 0.7160 0.0164 0.7452 0.0189 0.7719 0.0066 \n2 0.7243 0.0054 0.6827 0.0074 0.7275 0.0122 0.7451 0.0139 0.7855 0.0074 \n3 0.7161 0.0065 0.6963 0.0076 0.7284 0.0131 0.7434 0.0140 0.7852 0.0076 \n4 0.7117 0.0068 0.7006 0.0077 0.7284 0.0140 0.7433 0.0140 0.7853 0.0077 \n5 0.7099 0.0067 0.7015 0.0078 0.7288 0.0136 0.7432 0.0137 0.7855 0.0078 \n \n \nPublished by Atlantis Press \n    Copyright: the authors \n                  639\n Document Categorization for Agglutinative Languages \n \nFig. 3. Performance of root based language models with \ndifferent smoothing methods with respect to language model \norder \nRoot based language model classification results \ndemonstrate an improvement on baseline model in Fig. \n3. Except for unigrams, Good-Turing smoothing again \nyields superior results for all other cases. \nClassification with trigram language models trained \non root word units with Good-Turing smoothing can \nacquire 0.8198 F1-score which is slightly better than \n0.8140, the SVM performance with stemming on 1000 \nTurkish news documents splitted in 5 categories29. \nIn root+POS model, it was expected that POS \ninformation could contribute to the overall performance \nby the help of the discriminative property of POS \ninformation for homograph root words. On the contrary, \nexperimental results show no progress (even some \nreductions) for all smoothing methods (Fig. 4). \nIn our work, we have carried out some tests with th e \nFirstFLetter models which make use of the truncated \nfirst F letters of the words as base units. Fig. 5 shows \nhow different smoothing techniques affect the \nclassification performance. For a further investigation \non F parameter, we have conducted additional tests \nwhere F\n{3,…,7} and the results are presented in Fig. \n6. The best performing smoothing method for F=4 is \nGood-Turing, so it is fixed for all other tests. \n \n \nFig. 4. Performance of root + POS tag based language models \nwith different smoothing methods with respect to language \nmodel order \nAs the results indicate, FirstFLetter truncation \nmodels are able to achieve a classification performance \nclose to root based models. The only exception of that \nfinding is the case where F= 3 , which is probably too \nshort for discrimination. Consistently with the previous \nexperimental results, no specific improvement is \nobserved where n > 3. \n \n \nFig. 5. Performance of the First4Letter based language models \nwith smoothing methods with respect to language model order \nPublished by Atlantis Press \n    Copyright: the authors \n                  640\nA. C. Tantuğ \n \n \nFig. 6. Performance of the FirstFLetter (F = 3,…,7) based \nlanguage models with Good-Turing smoothing method along \nwith language model order \n \nFig. 7. Performance of character based language models with \nsmoothing methods along with language model order \nFinally, the results of the classification tests \nperformed with character level language models are \ndepicted in Fig. 7 . A comparison between figures \ndemonstrating classification performances of character \nlevel and word level models shows that character level \nlanguage models can get classification accuracies \ncomparable to word level models. Moreover, as \ncharacter level language models are trained on word \nforms, language independency can be preserved without \nany loss in performance. From the point of view of \nsmoothing impact on character level language model \nefficiency, no significant gain is observed with any \nmethod. This situation may be appropriately explained \nby the appearance of all possible regularities in the \ntraining data due to the extremely small vocabulary size \n(the number of distinct characters). In that case, all \nsmoothing methods tend to calculate similar probability \ndistributions for unseen sequences, so no clear disparity \ncan be measured among them. \n7.3. Performance comparisons of different \nlanguage model types \nIn this section, we discuss an overall overview of \nexperimental results and a comparative assessment of \nmodified language models. We have used five different \nlanguage model types for Turkish text classification: \nword form based, root word based, root word and POS \ntag based, first F character truncated word form based \nand character based language models. Each experiment \naims to optimize at least two parameters: smoothing \nmethod and language model order. Almost all of the \nexperiment outcomes share some common \ncharacteristics. For example, except for the character \nlevel models, no substantial progress can be observed \nwhere language model order is greater than three ( n>3). \nThus, similar to other applications like speech \nrecognition and machine translation, the selection of the \nlanguage model order parameter n as 3 is shown to be \nsuitable for text classification purposes. Another \ngeneralized outcome of the test results can be stated as \nthe success of Good-Turing method over other \nsmoothing methods. Language models using Good-\nTuring smoothing technique outperformed others \nwhereas, in the character level case, Witten-Bell \nsmoothing mechanism achieves slightly better accuracy \nthan the Good-Turing and others. \nThe classification performances of all language \nmodel types considered in this study are consolidated in \nTable 6  and visualized in Fig. 8 , with their best \nperforming (optimized) parameters. For character based \nlanguage model, the highest classification performance \nis measured where n is 4. \nTable 6. Best performances of different language model types \nn Word form \n(GT) \nRoot \n(GT) \nRoot+POS \n(GT) \nFirst4Char \n(GT) \nCharBased \n (WB) \n1 0.7721 0.8017 0.7749 0.7924 0.5023 \n2 0.7767 0.8181 0.7891 0.8133 0.7488 \n3 0.7772 0.8192 0.7838 0.8139 0.8078 \n4 0.7772 0.8192 0.7859 0.8134 0.8212 \n5 0.7769 0.8192 0.7850 0.8134 0.8209 \nPublished by Atlantis Press \n    Copyright: the authors \n                  641\n Document Categorization for Agglutinative Languages \nAs it can be drawn from the figure, highest \nperformance is attained by the root and character based \nmethods for n>3. In any case, all of the language model \ntypes outperform our baseline Naïve Bayes method for \nn > 1. \n7.4. Effect of document length in classification \nperformance \nThe effect of document length on the classification \nperformance is also studied as a part of this work. It can \nbe claimed that texts containing long and/or many \nsentences may possess more information which can \nguide classification algorithms work better. In that case, \nclassifying longer documents should get superior \nperformance than of shorter documents. Fig. 9 presents \nthe document length histogram of the data set. Here, the \nlength of the documents are measured in words \n(tokens).  \n \nFig. 9. Document length histogram in data set \nThe document length versus classification \nperformance tests carried out by root based trigram \nlanguage models with Good-Turing smoothing applied. \nAs the graph illustrates in Fig. 10, the document length \ndoesn’t have a significant effect on performance. The \nsharp performance falls for both short (length<20) and \nlong (length>80) documents are mainly caused by the \ninsufficienct number of documents in those length \nranges (see Fig. 9 ). It is also the main reason for the \nsalient standard deviation jumps seen on the left and \nright sides of the figure. \nFig. 10. Classification performance with respect to document \nlength (in words) \n8. Conclusions \nIn this work, we investigated the performance of \ndocument categorization with standard and modified \nlanguage models for agglutinative languages. We \nevaluated the effects of five different language models \non classification performance while three of them were \nsuggested to overcome data sparseness problems in \nother application areas. Our tests are carried on a large \nTurkish data set; however the results can be extended to \nother agglutinative languages easily. \nOne of the results derived from our experiments \nreveals that statistical language model based text \nclassifiers can outperform Naïve Bayes classifiers in \nTurkish document categorization task. Even word form \nbased standard implementation of statistical language \nmodels can improve Naïve Bayes classification \nperformance by approximately 8%, which is \naccomplished mainly because of advanced smoothing \ntechniques and longer contexts. Moreover, our \nexperimental outcomes show that root based n-gram \nclassifiers can achieve 0.8192 F1-score in  the \ncategorization accuracy, though they require \nsophisticated language dependent tools for stemming. \n \nFig. 8. Performance comparison of proposed language model \ntypes with best performing smoothing method \nPublished by Atlantis Press \n    Copyright: the authors \n                  642\nA. C. Tantuğ \n \nThis means a 5.40% performance gain with respect to \nword form based language models. By a simple trick \nused in information retrieval tasks, almost same \nclassification accuracy ( F1-score is 0.8139) is \naccomplished with n -grams based on truncation of first \n4 letters of the word forms. We eliminate the language \nspecific requirements by this way. A lthough previous \nstudies report the success of character level n -grams, we \nwere expecting a performance deterioration of character \nbased models for agglutinative languages. Contrary to \nexpectations, character level language models perform \nreasonably well a nd ranked at the top with 0.8218 F1-\nscore. \nSimilar to previous studies on other applications, \nhaving root words as base units improves the document \nclassification performance. However, same level of \naccuracy can be achieved by simpler models which do \nnot need complex language dependent tools. So, we \nsuggest that using character level or FirstFLetter \ntruncation models can perform well on statistical \nlanguage model based document categorization tasks for \nagglutinative languages and unlike previous efforts, \nthere is no need to apply language dependent \npreprocessing step. \nAlso, we concern with the influence of document \nlength on classification performance. Although longer \ndocuments are supposed to help improving the \nclassification performance by containing more \ndiscriminative words, experiments show that the \naccuracy of the classification remains almost steady for \ndocuments at every length, except some ignorable \nstatistically insignificant fluctuations. \nOur future work includes a comparative study on \nclassification of the same data set with other machine \nlearning algorithms such as support vector machines, \nkNN and others. Since character level n-gram models \nare shown to be effective in text categorization, we plan \nto investigate their efficiency in information retrieval \ntasks for agglutinative languages. \n \nAcknowledgments \nWe want to thank Y. Yaslan and G. Eryiğit for \nreviewing this paper and providing valuable feedbacks. \nAlso we would like to thank anonymous reviewers for \ntheir precious suggestions. \nReferences \n1. F. Sebastiani, Machine Learning in Automated Text \nCategorization, in ACM Computing Surveys , 34(1) \n(2002) 1-47. \n2. D. Lewis, Naive (Bayes) at forty: The independence \nassumption in information retrieval, in Lecture Notes in \nComputer Science, 1398 (1998) 4-18. \n3. S. Robertson and K. Jones, Relevance weighting of \nsearch terms, in Journal of the American Society for \nInformation Science, 27(3) (1976). \n4. Y. Li and A. Jain, Classification of text documents, in \nThe Computer Journal, 41(8) (1998) 537-546. \n5. F. Peng, D. Schuurmans, and S. Wang, Augmenting \nnaive bayes classifiers with statistical language models, \nin Information Retrieval, 7(3) (2004) 317-345. \n6. F. Jelinek, R. L. Mercer, L. Bahl, and J. K. Baker, \nPerplexity - A Measure of the Difficulty of Speech \nRecognition Tasks, in Journal of the Acoustical Society \nof America, 62(S1) (1977) S63. \n7. J. Ponte and W. Croft, A language modeling approach to \ninformation retrieval, in Proc. 21st annual international \nACM SIGIR conference on Research and development in \ninformation retrieval  (ACM New York, NY, USA, \n1998). \n8. P. F. Brown, et al., A Statistical Approach to Machine \nTranslation, in Computational Linguistics, 16(2) (1990) \n79-85. \n9. D. Z. Hakkani-Tür, K. Oflazer, and G. Tür, Statistical \nMorphological Disambiguation for Agglutinative \nLanguages, in Computers and the Humanities, 36 (2002) \n381-410. \n10. G. Eryiğit, J. Nivre, and K. Oflazer, Dependency Parsing \nof Turkish, in Computational Linguistics, 34(3) (2008). \n11. M. Ganapathiraju, D. Weisser, R. Rosenfeld, J. \nCarbonell, R. Reddy, and J. Klein-Seetharaman, \nComparative n-gram analysis of whole-genome protein \nsequences, in Human Language Technologies \nConference (2002). \n12. W. B. Cavnar and J. M. Trenkle, N-gram-based text \ncategorization, in 3rd Annual Symposium on Document \nAnalysis and Information Retrieval (SDAIR-94) (1994). \n13. M. F. Caropreso, S. Matwin, and F. Sebastiani, A \nlearner-independent evaluation of the usefulness of \nstatistical phrases for automated text categorization, in \nText Databases and Document Management: Theory \nand Practice (IGI Publishing, 2001). pp. 78-102. \n14. Z. Wei, D. Miao, J.-H. Chauchat, R. Zhao, and W. Li, \nN-grams based feature selection and text representation \nfor Chinese Text Classification, in International Journal \nof Computational Intelligence Systems, 2(4) (2009) 365-\n374. \nPublished by Atlantis Press \n    Copyright: the authors \n                  643\n Document Categorization for Agglutinative Languages \n15. E. Frank, C. Chui, and I. Witten, Text categorization \nusing compression models, in Conference on Data \nCompression, (IEEE Computer Society, Washington \nDC, 2000), pp. 555. \n16. J. Cleary and I. Witten, Data compression using adaptive \ncoding and partial string matching, in IEEE \nTransactions on Communications , 32(4) (1984) 396-\n402. \n17. S. Zhou, K. Li, and Y. Liu, Text Categorization Based \non Topic Model, in International Journal of \nComputational Intelligence Systems , 2(4) (2009) 398-\n409. \n18. W. Teahan and D. Harper, Using compression-based \nlanguage models for text categorization, in Language \nModeling for Information Retrieval,  (2003) 141-166. \n19. F. Peng and D. Schuurmans, Combining naive Bayes \nand n-gram language models for text classification, in \nLecture Notes in Computer Science,  (2003) 335-350. \n20. T. Korenius, J. Laurikkala, K. Järvelin, and M. Juhola, \nStemming and lemmatization in the clustering of Finnish \ntext documents, in Conference on Information and \nKnowledge Management (2004). \n21. P. Halacsy and V. Tron, Benefits of resource-based \nstemming in Hungarian information retrieval, in Lecture \nNotes in Computer Science, 4730 (2007) 99-106. \n22. A. Tordai and M. De Rijke, Four stemmers and a \nfuneral: Stemming in hungarian at clef 2005, in Lecture \nNotes in Computer Science, 4022 (2006) 179. \n23. A. Zelaia, I. Alegria, O. Arregi, and B. Sierra, Analyzing \nthe effect of dimensionality reduction in document \ncategorization for Basque, in Archives of Control \nSciences, 600 (2005) 202. \n24. M. Amasyali and B. Diri, Automatic Turkish Text \nCategorization in Terms of Author, Genre and Gender, \nin Lecture Notes in Computer Science, 3999 (2006) 221. \n25. L. Özgür, T. Güngör, and F. Gürgen, Adaptive anti -\nspam filtering for agglutinative languages: a special case \nfor Turkish, in Pattern Recognition Letters , 25(16) \n(2004) 1819-1831. \n26. A. C. Tantug and G. Eryiğit, Performance Analysis of \nNaϊve Bayes Classification, Support Vector Machines \nand Neural Networks for Spam Categorization in \nApplied Soft Computing Technologies: The Challenge of \nComplexity (Springer Berlin, 2006). pp. 495-504. \n27. U. Ilhan, Application of K-NN and FPTC based text \ncategorization algorithms to Turkish news reports, in \nDept. of Comp. Eng. , (Bilkent University, Ankara, \n2001). \n28. M. Amasyali and T. Yildirim, Automatic text \ncategorization of news articles, in IEEE 12th Signal \nProcessing and Communications Applications \nConference (2004). \n29. Z. Cataltepe, Y. Turan, and F. Kesgin, Turkish \nDocument Classification Using Shorter Roots, in IEEE \n15th Signal Processing and Communications \nApplications, (2007), pp. 1-4. \n30. J. T. Goodman, A Bit of Progress in Language Modeling \nExtended Version, (Microsoft Research, Redmond, US, \n2001). \n31. S. Chen and J. Goodman, An empirical study of \nsmoothing techniques for language modeling, in \nComputer Speech and Language, 13(4) (1999) 359-394. \n32. H. Ney, U. Essen, and R. Kneser, On structuring \nprobabilistic dependences in stochastic language \nmodeling, in Computer, Speech and Language , 8(1) \n(1994) 1-38. \n33. H. Jeffreys, Theory of Probability . 2nd ed. (Clarendon \nPress, Oxford, 1948). \n34. W. Johnson, Probability : The Deductive and Inductive \nProblems, in Mind, 41(164) (1932) 409. \n35. G. Lidstone, Note on the general case of the Bayes-\nLaplace formula for inductive or a posteriori \nprobabilities, in Transactions of the Faculty of \nActuaries, 8 (1920) 182-192. \n36. I. H. Witten and T. C. Bell, The Zero-Frequency \nProblem : Estimating the Probabilities of Novel Events \nin Adaptive Text Comprassion, in IEEE Transactions on \nInformation Theory, 37(4) (1991) 1085-1094. \n37. S. M. Katz, Estimation of Probabilities from Sparse Data \nfor Language Model Component of a Speech \nRecognizer, in IEEE Transactions on Acoustics, Speech \nand Signal Processing, 35(3) (1987) 400-401. \n38. I. Good, The population frequencies of species and the \nestimation of population parameters, in Biometrika, \n40(3-4) (1953) 237-264. \n39. D. Jurafsky and J. Martin, Speech and language \nprocessing (Prentice Hall, 2008). \n40. C. D. Manning and H. Schütze, Foundations of \nStatistical Natural Language Processing  (The MIT \nPress, Cambridge, 1999). \n41. F. He and X. Ding, Improving Naive Bayes Text \nClassifier Using Smoothing Methods, in 29th European \nConference on IR Research, ECIR 2007 , (Springer, \n2007), pp. 703. \n42. J. Hankamer, Finite State Morphology and Left to Right \nPhonology, in West Coast Conference on Formal \nLinguistics Forum (Stanford University, 1986). \n43. K. Oflazer, Error-tolerant finite-state recognition with \napplications to morphological analysis and spelling \ncorrection, in Computational Linguistics , 22(1) (1996) \n73-89. \n44. D. Jurafsky and J. H. Martin, Speech and Language \nProcessing: An Introduction to Natural Language \nPublished by Atlantis Press \n    Copyright: the authors \n                  644\nA. C. Tantuğ \n \nProcessing, Computational Linguistics and Speech \nRecognition (Prentice Hall 2000). \n45. E. Arısoy, H. Dutağacı, and L. Arslan, A unified \nlanguage model for large vocabulary continuous speech \nrecognition of Turkish, in Signal Processing , 86(10) \n(2006) 2844-2862. \n46. D. Yuret and E. Biçici, Modeling Morphologically Rich \nLanguages Using Split Words and Unstructured \nDependencies, in The Joint conference of the 47th \nAnnual Meeting of the Association for Computational \nLinguistics and the 4th International Joint Conference \non Natural Language Processing of the Asian \nFederation of Natural Language Processing (ACL-\nIJCNLP 2009), (2009). \n47. G. Eryiğit and K. Oflazer, Statistical dependency parsing \nof Turkish, in The 11th Conference of the European \nChapter of the Association for Computational \nLinguistics (EACL) (2006). \n48. A. C. Tantuğ, E. Adalı, and K. Oflazer, Lexical \nAmbiguity Resolution for Turkish in Direct Transfer \nMachine Translation Models, in Lecture Notes in \nComputer Science, 4263 (2006) 230-238. \n49. F. Ekmekcioglu, M. Lynch, and P. Willett, Stemming \nand n-gram matching for term conflation in Turkish \ntexts, in Information Research , 2(2) (1996) \nhttp://informationr.net/ir/2-2/paper13.htm\n. \n50. H. Sever and Y. Tonta, Truncation of content terms for \nTurkish, in Conference on Intelligent Text Processing \nand Computational Linguistics, CICLing, (2006). \n51. F. Can, S. Kocberber, E. Balcik, C. Kaynak, H. Ocalan, \nand O. Vursavas, Information retrieval on Turkish texts, \nin Journal of the American Society for Information \nScience and Technology, 59(3) (2008) 407-421. \n52. M. Porter, An algorithm for suffix stripping, in Readings \nin information retrieval  (Morgan Kaufmann Publishers \nInc., San Francisco, CA,, 1997). pp. 313-316. \n53. K. Oflazer, Two-level Description of Turkish \nMorphology, in Literary and Linguistic Computing, 9(2) \n(1995) 137-148. \n54. A. C. Tantuğ, E. Adalı, and K. Oflazer, A Prototype \nMachine Translation System Between Turkmen and \nTurkish, in Fifteenth Turkish Symposium on Artificial \nIntelligence and Neural Networks, TAINN, (2006). \n55. T. Güngör, Lexical and morpholo gical statistics for \nTurkish, in International Twelfth Turkish Symposium on \nArtificial Intelligence and Neural Networks, TAINN \n2003, (2003). \n56. A. Stolcke, SRILM - An Extensible Language Modeling \nToolkit, in International Conference on Spoken \nLanguage Processing, (2002). \n \n \nPublished by Atlantis Press \n    Copyright: the authors \n                  645",
  "topic": "Agglutinative language",
  "concepts": [
    {
      "name": "Agglutinative language",
      "score": 0.9508665800094604
    },
    {
      "name": "Categorization",
      "score": 0.8112213611602783
    },
    {
      "name": "Computer science",
      "score": 0.752731442451477
    },
    {
      "name": "Natural language processing",
      "score": 0.6856695413589478
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5835725665092468
    },
    {
      "name": "Statistical analysis",
      "score": 0.42006075382232666
    },
    {
      "name": "Text categorization",
      "score": 0.41073065996170044
    },
    {
      "name": "Linguistics",
      "score": 0.3363659679889679
    },
    {
      "name": "Mathematics",
      "score": 0.20915529131889343
    },
    {
      "name": "Statistics",
      "score": 0.1709950864315033
    },
    {
      "name": "Morpheme",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I48912391",
      "name": "Istanbul Technical University",
      "country": "TR"
    }
  ],
  "cited_by": 13
}