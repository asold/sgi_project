{
    "title": "Efficient Automatic Punctuation Restoration Using Bidirectional Transformers with Robust Inference",
    "url": "https://openalex.org/W3037101098",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2963595817",
            "name": "Maury Courtland",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097748619",
            "name": "Adam Faulkner",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2628615918",
            "name": "Gayle McElvain",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2787556430",
        "https://openalex.org/W1626209120",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2899902398",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2937375285",
        "https://openalex.org/W2964153283",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W2962989741",
        "https://openalex.org/W2513522215",
        "https://openalex.org/W2806626660",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2529548870",
        "https://openalex.org/W2888942278",
        "https://openalex.org/W1904365287",
        "https://openalex.org/W2890448475",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W2953993152",
        "https://openalex.org/W2743392628",
        "https://openalex.org/W2970120757",
        "https://openalex.org/W2612007440",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2759132769",
        "https://openalex.org/W2888060121",
        "https://openalex.org/W2398104528",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2994689640",
        "https://openalex.org/W3016128928",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W2561932123",
        "https://openalex.org/W2786179895",
        "https://openalex.org/W4239753979",
        "https://openalex.org/W2917668649",
        "https://openalex.org/W2602427488",
        "https://openalex.org/W3011866533",
        "https://openalex.org/W3008260965",
        "https://openalex.org/W2586117383",
        "https://openalex.org/W2575282817",
        "https://openalex.org/W2747964575",
        "https://openalex.org/W2741483887",
        "https://openalex.org/W2757416441",
        "https://openalex.org/W3011222885",
        "https://openalex.org/W2781510043",
        "https://openalex.org/W2911304011",
        "https://openalex.org/W2970803838",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4288265053",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2705373224",
        "https://openalex.org/W2750292073",
        "https://openalex.org/W2972411269",
        "https://openalex.org/W2902423035",
        "https://openalex.org/W2739681611",
        "https://openalex.org/W2937326859",
        "https://openalex.org/W2523602859"
    ],
    "abstract": "Though people rarely speak in complete sentences, punctuation confers many benefits to the readers of transcribed speech. Unfortunately, most ASR systems do not produce punctuated output. To address this, we propose a solution for automatic punctuation that is both cost efficient and easy to train. Our solution benefits from the recent trend in fine-tuning transformer-based language models. We also modify the typical framing of this task by predicting punctuation for sequences rather than individual tokens, which makes for more efficient training and inference. Finally, we find that aggregating predictions across multiple context windows improves accuracy even further. Our best model achieves a new state of the art on benchmark data (TED Talks) with a combined F1 of 83.9, representing a 48.7% relative improvement (15.3 absolute) over the previous state of the art.",
    "full_text": "Proceedings of the 17th International Conference on Spoken Language Translation (IWSLT), pages 272–279\nJuly 9-10, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n272\nEfﬁcient Automatic Punctuation Restoration Using Bidirectional\nTransformers with Robust Inference\nMaury Courtland, Adam Faulkner, Gayle McElvain\nCapital One, Vision and Language Technologies\n{maury.courtland, adam.faulkner, gayle.mcelvain}\n@capitalone.com\nAbstract\nThough people rarely speak in complete sen-\ntences, punctuation confers many beneﬁts to\nthe readers of transcribed speech. Unfortu-\nnately, most ASR systems do not produce\npunctuated output. To address this, we pro-\npose a solution for automatic punctuation that\nis both cost efﬁcient and easy to train. Our\nsolution beneﬁts from the recent trend in\nﬁne-tuning transformer-based language mod-\nels. We also modify the typical framing of this\ntask by predicting punctuation for sequences\nrather than individual tokens, which makes for\nmore efﬁcient training and inference. Finally,\nwe ﬁnd that aggregating predictions across\nmultiple context windows improves accuracy\neven further. Our best model achieves a new\nstate of the art on benchmark data (TED Talks)\nwith a combined F1 of 83.9, representing a\n48.7% relative improvement (15.3 absolute)\nover the previous state of the art.\n1 Introduction\nEnabling computers to use speech as input has long\nbeen an aspirational goal in the ﬁeld of human com-\nputer interaction. Recent advances have had dra-\nmatic impact across multiple domains (e.g. reliev-\ning medical professionals from having to transcribe\nmedical dictation (Edwards et al., 2017), improv-\ning real-time spoken language translation (Gu et al.,\n2017), and affording convenience through conver-\nsational interfaces like those in virtual personal\nassistants (McTear et al., 2016)). For use cases that\nrequire reading transcribed speech, however, it is\noften still a challenge to recover meaningful clause\nboundaries from disﬂuent, errorful utterances.\nHumans rely on punctuation for readability, per-\nhaps because it lessens the burden of ambiguous\nphrasing. Studies have found that removing punctu-\nation from manual transcriptions can be even more\ndetrimental to understanding than a word error rate\nof 15% or 20% (T ¨undik et al., 2018). Reading\ncomprehension is also signiﬁcantly slower with-\nout punctuation (Jones et al., 2003). For down-\nstream NLP models, the lack of clausal boundaries\ncan signiﬁcantly decrease accuracy (e.g. a 4.6%\nBLEU decrease in NMT; Vandeghinste et al. 2018).\nThis likely reﬂects the discrepancy between well-\nsegmented training corpora and ASR output.\nTo solve the lack of punctuation in ASR out-\nput, we propose an automatic punctuation model,\nwhich leverages the recent trend in unsupervised\npre-training (Devlin et al., 2019) and the parallel ar-\nchitecture of transformer networks (Vaswani et al.,\n2017). Unsupervised pre-training dramatically re-\nduces the amount of labeled data required for su-\nperior performance on this task. Additionally, the\nmodel’s departure from a recurrent architecture al-\nlows direct connections between all input tokens.\nThis enables the network to more easily model long-\ndistance dependencies (e.g. on one hand, ... on the\nother, ...) for improved punctuation performance.\nThe departure from a recurrent architecture also\nallows computations to be performed in parallel for\neach layer with the speed of computations limited\nby the number of layers rather than the number\nof time steps (usually fewer). In addition to the\nparallel nature of the hidden layers, our network\nalso predicts in parallel for all tokens in the input\nsimultaneously. This helps signiﬁcantly speed up\ninference compared with individual predictions for\neach token. During training, the parallel predic-\ntion task provides a richer signal compared with\na sequential task, thereby making more efﬁcient\nuse of each example. Furthermore, advancing the\nprediction window less than the window’s width\n(e.g. steps of 20 with a window of 50) allows ag-\ngregating multiple windows of context to predict a\ntoken’s label. This allows the network to effectively\nbecome its own prediction ensemble and boosts ac-\ncuracy further. Given that the aggregate predictions\n273\nare independently obtained, these calculations too\ncan be performed in parallel.\n2 Related Work\nOur biggest departure from previous approaches\nlies in the parallel nature of inference and the deep\nbidirectional information ﬂow of our model (for\nmore detail, see Devlin et al. 2019). This is in con-\ntrast with the vast majority of previous approaches\nwhich use a variant of Recurrent Neural Network\narchitecture (Tundik et al., 2017; Vandeghinste\net al., 2018; Ballesteros and Wanner, 2016; Alum¨ae\net al., 2019; Szasz´ak, 2019; ¨Oktem, 2018; Xu et al.,\n2016; Pahuja et al., 2017; Tundik and Szaszak,\n2018; T¨undik et al., 2017; Tilk and Alumae, 2015;\n˙Zelasko et al., 2018; Treviso and Alu ´ısio, 2018).\nThis includes those that incorporate acoustic infor-\nmation (B. Garg and Anika, 2018; Moro and Sza-\nszak, 2017; Szasz´ak and T ¨undik, 2019; Nanchen\nand Garner, 2019; Mor´o and Szasz´ak, 2017; Klejch\net al., 2016, 2017) and those that apply attention on\ntop (Tilk and Alum¨ae, 2016; Salloum et al., 2017;\n¨Oktem et al., 2017; Kim, 2019; Juin et al., 2017).\nThough non-sequential, several previous ap-\nproaches use simpler network architectures (e.g.\nDNNs (Yi et al., 2017; Che et al., 2016) or CNNs\n(B. Garg and Anika, 2018; Che et al., 2016;˙Zelasko\net al., 2018)), which have less predictive power.\nThe handful of approaches that make use of Trans-\nformer architectures are not bidirectional (Chen\net al., 2020; Nguyen et al., 2019; V¯aravs and Sal-\nimbajevs, 2018; Wang et al., 2018). Our model\nalso differs from the above in that it leverages pre-\ntraining to reduce training time and increase ac-\ncuracy. The one previous work that uses a pre-\ntrained bidirectional transformer (Cai and Wang,\n2019) only predicts punctuation one token at a\ntime, which signiﬁcantly increases both training\nand inference time. It is also unable to aggregate\npredictions across multiple contexts, limiting per-\nformance.\n3 Method\nArchitecture The network architecture can be\nseen in Figure 1. The ﬁrst component of\nour network is a pre-trained language model\n(RoBERTabase; Liu et al. 2019) employing the re-\ncent deep bidirectional Transformer architecture\n(Devlin et al., 2019; Vaswani et al., 2017). The net-\nwork’s input is a sequence of unpunctuated lower-\ncased words tokenized using RoBERTa’s tokeniza-\ntion scheme (see Liu et al. 2019 for more details).\nWe then add two additional linear layers after the\npre-trained network with each layer preserving the\nfully-connected nature of the entire network. The\nﬁrst linear layer maps from the masked language\nmodel output space to a hidden state space for each\ninput token with parameters shared across tokens.\nThe second linear layer concatenates the hidden\nstate representations into a vector for the prediction\nwindow which allows the tokens to interact arbi-\ntrarily within the window. We then apply batch nor-\nmalization (Ioffe and Szegedy, 2015) and dropout\n(best results obtained with a rate of 0.2; Hinton\net al. 2012) prior to predicting punctuation marks\nfor all tokens in the window.\nWhen aggregating predictions across contexts,\nactivations at the sequence layer are added for each\ntoken prior to classiﬁcation (see Figure 1 for visual-\nization). Prediction is performed in parallel during\nboth training and inference with the output size of\nthe ﬁnal classiﬁer being |classes|∗lengthwindow.\nVocab IDs\nVocabulary distributions\n(before Fine-Tuning)\nHidden token vectors\nConcatenate\nBatch Norm and Dropout\nLinear Classiﬁer Layer\nLinear Layer\nHidden sequence vector\nNormalized sequence vector\nSequence distributions\nPre-trained LM\nAggregate Across Contexts\nPunctuation Labels\nOn+1\nTn+1T2 Tn\nO2 On\n...\nYn+1\nAdvance sliding window and repeat for new context...\nT1 T2 ... Tn\nH1 H2 Hn\nZ1\n...\nZn\nZ1...n\nZ′ 1...n\nO1 O2 ... On\nZ2\nY1 Y2 ... Yn\nRoBERTa\nNorm\nFigure 1: The punctuation network takes as input a se-\nquence of unpunctuated words tokenized in the same\nmanner as RoBERTa. It outputs predictions for these\nsequences individually during training (layer On). For\nvalidation and testing, however, these labels are aggre-\ngated across overlapping context windows to obtain\nthe ﬁnal punctuation predictions (layer Yn). Note that\nwhile the pre-trained LM’s output begins as vocabulary\ndistributions, they cease to be so once the entire net-\nwork undergoes ﬁne-tuning.\n274\nComma Period Question Overall\nModels P R F P R F P R F P R F\nDNN-A (Che et al., 2016) 48.6 42.4 45.3 59.7 68.3 63.7 — — — 54.8 53.6 54.2\nCNN-2A (Che et al., 2016) 48.1 44.5 46.2 57.6 69.0 62.8 — — — 53.4 55.0 54.2\nT-LSTM (Tilk and Alumae, 2015)49.6 41.4 45.1 60.2 53.4 56.6 57.1 43.5 49.4 55.0 47.2 50.8\nT-BRNN (Tilk and Alum¨ae, 2016) 64.4 45.2 53.1 72.3 71.5 71.9 67.5 58.7 62.8 68.9 58.1 63.1\nT-BRNN-pre (Tilk and Alum¨ae, 2016) 65.5 47.1 54.8 73.3 72.5 72.9 70.7 63.0 66.7 70.0 59.7 64.4\nSingle-BiRNN (Pahuja et al., 2017)62.2 47.7 54.0 74.6 72.1 73.4 67.5 52.9 59.3 69.2 59.8 64.2\nCorr-BiRNN (Pahuja et al., 2017)60.9 52.4 56.4 75.3 70.8 73.0 70.7 56.9 63.0 68.6 61.6 64.9\nDRNN-LWMA (Kim, 2019) 63.4 55.7 59.3 76.0 73.5 74.7 75.0 71.7 73.3 70.0 64.6 67.2\nDRNN-LWMA-pre (Kim, 2019) 62.9 60.8 61.9 77.3 73.7 75.5 69.6 69.6 69.6 69.9 67.2 68.6\nRoBERTabase 76.9 75.4 76.2 86.1 89.3 87.7 88.9 87.0 87.9 84.0 83.9 83.9\nDifferent Pre-trained Language Models\nRoBERTalarge 74.3 76.9 75.5 85.8 91.6 88.6 83.7 89.1 86.3 81.3 85.9 83.5\nXLNetbase 76.6 74.9 75.8 84.6 90.6 87.5 82.0 89.1 85.4 81.1 84.9 82.9\nT5base 70.5 77.2 73.7 85.6 85.5 85.6 83.7 89.1 86.3 79.9 84.0 81.9\nBERTbase 72.8 70.8 71.8 81.9 86.6 84.2 80.8 91.3 85.7 78.5 82.9 80.6\nALBERTbase 69.4 69.3 69.4 80.9 84.5 82.7 76.7 71.7 74.2 75.7 75.2 75.4\nDistilRoBERTa 70.0 64.5 67.1 78.2 83.5 80.8 75.0 71.7 73.3 74.4 73.2 73.7\nTable 1: Compared to previous approaches, our model achieves state of the art performance on the reference\ntranscripts of the TED Talks dataset as measured by precision (P), recall (R), and F-1 score (F). Experimental\nresults with different pre-trained language models are included below for comparison with the best RoBERTa base\nmodel.\nTraining Schedule It is worth noting that we use\nonly the TED Talks dataset described below for\ntraining but enjoy signiﬁcant beneﬁts from a sizable\npre-training corpus (Liu et al., 2019). Although\nprediction is performed on multiple tokens at once,\nthe same number of training samples are generated\nfrom the corpus by moving the sliding window one\ntoken at a time over the input. To perform gradient\ndescent, we use LookAhead (Zhang et al., 2019)\nwith RAdam (Liu et al., 2020) as the base optimizer.\nWe use a simple cross-entropy function to calculate\nthe loss for each token’s classiﬁcation prediction.\nOur best performing model (see Table 1) uses\na prediction window size of 100, a ﬁnal-layer\ndropout of 0.2, and a hidden-state space of dimen-\nsionality 1500. The top two linear layers (hence-\nforth referred to as the “top layers”) are initially\ntrained from scratch while the transformer core\nremains frozen. Then, having selected the model\nversion with the lowest validation loss from train-\ning the top layers, the transformer core is unfrozen,\nand we ﬁne-tune the parameters of the entire net-\nwork. We then select the model version with the\nlowest validation loss to prevent overﬁtting.\nWe train the top layers for nine epochs with\na mini-batch size of 1000 (using 100-token se-\nquences) while the transformer is frozen. The\nlowest validation loss for the top layers is usually\nachieved around the sixth epoch. We then unfreeze\nthe transformer and ﬁne-tune the entire network for\nthree more epochs with a mini-batch size of 250.\nWe typically observe the lowest validation loss mid-\nway through the ﬁrst epoch while ﬁne-tuning. It is\nworth noting that a highly competitive model (82.6\noverall F1) can be trained with just 1 epoch each\nfor the top layers and ﬁne-tuning. This training\ncan be completed in slightly less than 1 hour on\na p3.16xlarge AWS instance (with 8x Tesla V100\nGPUs).\nFor the LookAhead optimizer, we use a sync\nrate of 0.5, and a sync period of 6. The RAdam\noptimizer—used as the model’s base optimizer—\nhas its learning rate set to 10−5, β1 = 0.9, β2 =\n0.999, ϵ= 10−8. We do not use weight decay.\nData To train the network and evaluate its per-\nformance (both at test and validation time), we use\nthe IWSLT 2012 TED Talks dataset (Cettolo et al.,\n2012). This dataset is a common benchmark in au-\ntomatic punctuation (e.g. Kim 2019) and consists\nof a 2.1M word training set, a 296k word valida-\ntion set, and a 12.6k word test set (for reference\ntranscription, 12.8k for ASR output). Each word is\nlabeled with the punctuation mark that follows it,\nyielding a 4-class classiﬁcation problem: comma,\nperiod, question mark, or no punctuation. The class\nbalance of the training dataset is as follows: 85.7%\nno punctuation, 7.53% comma, 6.3% period, 0.47%\nquestion mark.\n275\n4 Results\nThe results of our best performing model relative to\nprevious results published on this benchmark can\nbe found in Table 1. Additionally, we conducted a\nnumber of ablation experiments manipulating vari-\nous aspects of the architecture and training routine.\nIn providing accuracy comparisons, all results in\nthis section are reported in terms of the absolute\nchange in the overall F1 measure.\nIn place of the pre-trained RoBERTa base lan-\nguage model, which provided the best result, we\nalso evaluated (in order of decreasing performance\nrelative to RoBERTa as implemented by Wolf et al.\n(2020)1): XLNet base (-1.0%; Yang et al. 2020),\nT5base (-2.1%; Raffel et al. 2019), BERT base (-\n3.4%; Devlin et al. 2019), and ALBERTbase (-8.5%;\nLan et al. 2020). Full results from these models can\nbe seen at the bottom of Table 1. The performance\nbeneﬁt of RoBERTa base over BERTbase is likely\ndue to the signiﬁcant increase in pre-training cor-\npus size. The lower performance of ALBERTbase\nmay be due to the sharing of parameters across lay-\ners. It is interesting to note that XLNetbase provides\nhigher recall for periods and question marks and\nT5base for commas and question marks, but both\nsacriﬁce signiﬁcant precision to achieve this.\nIn addition to the LookAhead optimizer using\nRAdam as its base, we also evaluated: LookAhead\nwith Adam (-1.5%), RAdam alone (-1.6%), and\nAdam alone (-2.9%; Kingma and Ba 2017). Given\nthe class imbalance inherent in the dataset between\nthe no punctuation class and all the punctuation\nmarks, we tested focal loss (Lin et al., 2018), class\nweighting, and their combination, but found that\nnone outperformed simple cross-entropy loss.\nPerhaps the most noteworthy result is the com-\nparison between parallel prediction (described\nabove) and sequential prediction, wherein the for-\nward pass predicts punctuation for one token at\na time using a context window centered on that\ntoken. Sequential prediction requires longer in-\nference times ( >15x) yet yields only a marginal\nperformance beneﬁt (2.2%) relative to a parallel\nprediction without aggregation across multiple con-\ntexts. Ensembling predictions over multiple con-\ntexts overcomes the performance gap, while retain-\ning an advantage with respect to inference time.\nCompared to the self-ensemble approach, sequen-\ntial prediction is >4x slower and 5.4% less accu-\n1Available from https://github.com/\nhuggingface/transformers\nPredictions\nper token\nF1\nOverall\nCPU\nRuntime\nGPU\nRuntime\n1 76.3 1x 1x\n2 79.4 1.8x 1.1x\n3 81.8 2.6x 1.2x\n6 83.2 5.2x 1.5x\n9 83.9 7.7x 1.9x\nProcessor — 18x Intel Xeon\n(c5.18xlarge)\n8x Tesla V100\n(p3.16xlarge)\nTable 2: Aggregating multiple parallel predictions ex-\nhibits a tradeoff between runtime and accuracy. Run-\ntime results on the TED test set are presented relative to\nsingle predictions separately on CPU and GPU for ease\nof reading. To relate the two, the CPU single predic-\ntions are 9.4x slower than GPU. All runtime estimates\nare obtained from the mean of 10 runs.\nrate.\nA less obvious choice must be made between a\nsingle parallel prediction and multiple aggregated\npredictions, given the additional runtime of multi-\nple predictions (see Table 2 for details). For our\npurposes, the 7.6% improvement is worth the in-\ncrease in inference time, which is sub-linear given\nGPU parallelization but still appreciable. While\nour best method sums activations from different\ncontexts to obtain the aggregate predictions, we\nalso tested adding normalized probabilities across\nclasses and then renormalizing, but we found it\nresulted in slightly worse performance (-0.3%).\nIn addition to the RoBERTa base model whose\nresults are reported here, we also trained with a\nRoBERTalarge model. There was no appreciable\nperformance difference between the two sizes (the\nlarge being -0.4% worse) however the large model\nincurred a signiﬁcant slowdown (≈1.5x). This may\nimply that the base model size is adequately pow-\nered for punctuation tasks, at least on manually\ntranscribed English datasets similar to the bench-\nmark. This is supported by the ﬁndings of Kovaleva\net al. (2019), who found BERT base to be overpa-\nrameterized for most downstream tasks, implying\nRoBERTalarge would be extremely overparameter-\nized. A smaller pre-trained language model option\nis DistilRoBERTa, a knowledge distilled version\nof RoBERTa (analogous to DistilBERT: Sanh et al.\n2020). The DistilRoBERTa network is 12% smaller\nand performs inference ≈1.2x faster, but sacriﬁces\n9.1% in accuracy on the benchmark.\nThe previous state of the art approach was a\nmulti-headed attention network on top of multi-\nple stacked bidirectional GRU layers (Kim, 2019).\n276\nGiven the recurrent nature of the GRU layers, the\nnetwork is subject to the shortcomings of sequen-\ntial computation discussed in the Introduction. Our\nﬁndings illustrate yet another language task where\ntransformers outperform previous recurrent neural\nnetwork approaches.\nOur approach enjoys a 48.7% relative improve-\nment (15.3 absolute) over the previous state of the\nart (Kim, 2019). Given the ablation results pre-\nsented above, we attribute the performance gains to\nthe deeply bi-directional transformer architecture,\nthe beneﬁt of leveraging RoBERTa’s pre-trained\nlanguage model trained on ≈33B words, and the\naggregation of multiple prediction contexts for ro-\nbust inference. Some performance gain may also\nbe attributed to the addition of an encoding layer\ntrained solely on the punctuation task.\nOne of the more notable ﬁndings is that the non-\nrecurrent nature of the entire network allows for a\nlarge degree of parallelization resulting in a more\ncompetitive runtime compared to previous recur-\nrent approaches. While source code was not openly\navailable for benchmarking runtime against Kim\n(2019), we did compare against a similar approach\nfrom Tilk and Alum¨ae (2016)2, which was roughly\n78.8x slower on GPUs and 1.2x slower on a CPU,\nwhen evaluating the TED Talks test set.\nThe results presented here have not beneﬁted\nfrom any rigorous hyperparameter tuning (e.g. grid\nsearch or Bayesian optimization). We leave that\nto future work given that a rigorous systematic\napproach may yield appreciable improvements in\naccuracy.\n5 Conclusion\nWe have presented a state of the art automatic punc-\ntuation system which aggregates multiple predic-\ntion contexts for robust inference on transcribed\nspeech. The use of multiple prediction contexts, un-\nsupervised pre-training, and increased parallelism\nmakes it possible to achieve signiﬁcant perfor-\nmance gains without increased runtime or cost.\nOn a different dataset, Boh ´aˇc et al. (2017) re-\nported human agreement of around 76% for punc-\ntuation location and 70% for use of the same punc-\ntuation mark. Although we have yet to make a di-\nrect comparison, it’s possible our model is already\ncompetitive with human performance on this task.\nFuture work will explore how this performance\n2The source code is available from https://github.\ncom/ottokart/punctuator2\ntranslates in terms of readability and whether it is\nsufﬁcient to compensate for some amount of word\nerror, as suggested by T¨undik et al. (2018).\nReferences\nTanel Alum ¨ae, Ottokar Tilk, and Asadullah. 2019.\nAdvanced Rich Transcription System for Esto-\nnian Speech. arXiv:1901.03601 [cs] . ArXiv:\n1901.03601.\nB. Garg and Anika. 2018. Analysis of Punctuation\nPrediction Models for Automated Transcript Gener-\nation in MOOC Videos. In 2018 IEEE 6th Interna-\ntional Conference on MOOCs, Innovation and Tech-\nnology in Education (MITE) , pages 19–26. Journal\nAbbreviation: 2018 IEEE 6th International Confer-\nence on MOOCs, Innovation and Technology in Ed-\nucation (MITE).\nMiguel Ballesteros and Leo Wanner. 2016. A Neural\nNetwork Architecture for Multilingual Punctuation\nGeneration. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1048–1053, Austin, Texas. Association\nfor Computational Linguistics.\nMarek Boh ´aˇc, Michal Rott, and V ojtˇech Kov´aˇr. 2017.\nText Punctuation: An Inter-annotator Agreement\nStudy. In Kamil Ek ˇstein and V ´aclav Matouˇsek, ed-\nitors, Text, Speech, and Dialogue , volume 10415,\npages 120–128. Springer International Publishing,\nCham. Series Title: Lecture Notes in Computer Sci-\nence.\nYunqi Cai and Dong Wang. 2019. Question Mark Pre-\ndiction By Bert. In 2019 Asia-Paciﬁc Signal and\nInformation Processing Association Annual Sum-\nmit and Conference (APSIPA ASC), pages 363–367,\nLanzhou, China. IEEE.\nM Federico M Cettolo, L Bentivogli, M Paul, and\nS Stuker. 2012. Overview of the IWSLT 2012 Eval-\nuation Campaign. page 22.\nXiaoyin Che, Cheng Wang, Haojin Yang, and\nChristoph Meinel. 2016. Punctuation Prediction for\nUnsegmented Transcript Based on Word Vector. In\nProceedings of the Tenth International Conference\non Language Resources and Evaluation (LREC’16),\npages 654–658, Portoro ˇz, Slovenia. European Lan-\nguage Resources Association (ELRA).\nQian Chen, Mengzhe Chen, Bo Li, and Wen Wang.\n2020. Controllable Time-Delay Transformer for\nReal-Time Punctuation Prediction and Disﬂuency\nDetection. arXiv:2003.01309 [cs, eess] . ArXiv:\n2003.01309.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof Deep Bidirectional Transformers for Language\nUnderstanding. arXiv:1810.04805 [cs] . ArXiv:\n1810.04805.\n277\nErik Edwards, Wael Salloum, Greg P. Finley, James\nFone, Greg Cardiff, Mark Miller, and David\nSuendermann-Oeft. 2017. Medical Speech Recogni-\ntion: Reaching Parity with Humans. In Speech and\nComputer, pages 512–524, Cham. Springer Interna-\ntional Publishing.\nJiatao Gu, Graham Neubig, Kyunghyun Cho, and\nVictor O. K. Li. 2017. Learning to Translate\nin Real-time with Neural Machine Translation.\narXiv:1610.00388 [cs]. ArXiv: 1610.00388.\nGeoffrey E. Hinton, Nitish Srivastava, Alex\nKrizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-\ndinov. 2012. Improving neural networks by\npreventing co-adaptation of feature detectors.\narXiv:1207.0580 [cs]. ArXiv: 1207.0580.\nSergey Ioffe and Christian Szegedy. 2015. Batch\nNormalization: Accelerating Deep Network\nTraining by Reducing Internal Covariate Shift.\narXiv:1502.03167 [cs]. ArXiv: 1502.03167.\nDouglas Jones, Florian Wolf, Edward Gibson, Elliott\nWilliams, Evelina Fedorenko, Douglas Reynolds,\nand Marc Zissman. 2003. Measuring the readabil-\nity of automatic speech-to-text transcripts. In In EU-\nROSPEECH, pages 1585–1588.\nChin Char Juin, Richard Xiong Jun Wei, Luis Fernando\nD’Haro, and Rafael E. Banchs. 2017. Punctua-\ntion prediction using a bidirectional recurrent neural\nnetwork with part-of-speech tagging. In TENCON\n2017 - 2017 IEEE Region 10 Conference , pages\n1806–1811, Penang. IEEE.\nSeokhwan Kim. 2019. Deep Recurrent Neural\nNetworks with Layer-wise Multi-head Attentions\nfor Punctuation Restoration. In ICASSP 2019\n- 2019 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n7280–7284, Brighton, United Kingdom. IEEE.\nDiederik P. Kingma and Jimmy Ba. 2017.\nAdam: A Method for Stochastic Optimization.\narXiv:1412.6980 [cs]. ArXiv: 1412.6980.\nOndrej Klejch, Peter Bell, and Steve Renals. 2016.\nPunctuated transcription of multi-genre broadcasts\nusing acoustic and lexical approaches. In2016 IEEE\nSpoken Language Technology Workshop (SLT) ,\npages 433–440, San Diego, CA. IEEE.\nOndrej Klejch, Peter Bell, and Steve Renals. 2017.\nSequence-to-sequence models for punctuated tran-\nscription combining lexical and acoustic features.\nIn 2017 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n5700–5704, New Orleans, LA. IEEE.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the Dark Secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4364–4373, Hong Kong, China. Association for\nComputational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. 2020. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representations.\narXiv:1909.11942 [cs]. ArXiv: 1909.11942.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming\nHe, and Piotr Doll ´ar. 2018. Focal Loss for Dense\nObject Detection. arXiv:1708.02002 [cs]. ArXiv:\n1708.02002.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2020. On the Variance of the Adaptive Learn-\ning Rate and Beyond. arXiv:1908.03265 [cs, stat] .\nArXiv: 1908.03265.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs] . ArXiv:\n1907.11692.\nMichael McTear, Zoraida Callejas, and David Griol.\n2016. The conversational interface: talking to smart\ndevices. Electrical engineering. Springer, Cham.\nOCLC: 953421937.\nAnna Moro and Gyorgy Szaszak. 2017. A prosody in-\nspired RNN approach for punctuation of machine\nproduced speech transcripts to improve human read-\nability. In 2017 8th IEEE International Confer-\nence on Cognitive Infocommunications (CogInfo-\nCom), pages 000219–000224, Debrecen. IEEE.\nAnna Mor ´o and Gy ¨orgy Szasz ´ak. 2017. A Phono-\nlogical Phrase Sequence Modelling Approach for\nResource Efﬁcient and Robust Real-Time Punctua-\ntion Recovery. In Interspeech 2017, pages 558–562.\nISCA.\nAlexandre Nanchen and Philip N. Garner. 2019. Em-\npirical Evaluation and Combination of Punctuation\nPrediction Models Applied to Broadcast News. In\nICASSP 2019 - 2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 7275–7279, Brighton, United King-\ndom. IEEE.\nBinh Nguyen, Vu Bao Hung Nguyen, Hien\nNguyen, Pham Ngoc Phuong, The-Loc Nguyen,\nQuoc Truong Do, and Luong Chi Mai. 2019. Fast\nand Accurate Capitalization and Punctuation for\nAutomatic Speech Recognition Using Transformer\nand Chunk Merging. arXiv:1908.02404 [cs] .\nArXiv: 1908.02404.\nVardaan Pahuja, Anirban Laha, Shachar Mirkin, Vikas\nRaykar, Lili Kotlerman, and Guy Lev. 2017. Joint\nLearning of Correlated Sequence Labelling Tasks\nUsing Bidirectional Recurrent Neural Networks.\narXiv:1703.04650 [cs]. ArXiv: 1703.04650.\n278\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the Lim-\nits of Transfer Learning with a Uniﬁed Text-to-Text\nTransformer. arXiv:1910.10683 [cs, stat] . ArXiv:\n1910.10683.\nWael Salloum, Greg Finley, Erik Edwards, Mark Miller,\nand David Suendermann-Oeft. 2017. Deep Learn-\ning for Punctuation Restoration in Medical Re-\nports. In BioNLP 2017, pages 159–164, Vancouver,\nCanada,. Association for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. DistilBERT, a distilled ver-\nsion of BERT: smaller, faster, cheaper and lighter.\narXiv:1910.01108 [cs]. ArXiv: 1910.01108.\nGy¨orgy Szasz ´ak. 2019. An Audio-based Sequential\nPunctuation Model for ASR and its Effect on Human\nReadability. Acta Polytechnica Hungarica, 16(2).\nGy¨orgy Szasz´ak and M ´at´e ´Akos T¨undik. 2019. Lever-\naging a Character, Word and Prosody Triplet for an\nASR Error Robust and Agglutination Friendly Punc-\ntuation Approach. In Interspeech 2019, pages 2988–\n2992. ISCA.\nOttokar Tilk and Tanel Alumae. 2015. LSTM for Punc-\ntuation Restoration in Speech Transcripts. In IN-\nTERSPEECH, page 6.\nOttokar Tilk and Tanel Alum ¨ae. 2016. Bidirectional\nRecurrent Neural Network with Attention Mecha-\nnism for Punctuation Restoration. pages 3047–\n3051.\nMarcos Vin ´ıcius Treviso and Sandra Maria Alu ´ısio.\n2018. Sentence Segmentation and Disﬂuency Detec-\ntion in Narrative Transcripts from Neuropsycholog-\nical Tests. In Aline Villavicencio, Viviane Moreira,\nAlberto Abad, Helena Caseli, Pablo Gamallo, Carlos\nRamisch, Hugo Gonc ¸alo Oliveira, and Gustavo Hen-\nrique Paetzold, editors, Computational Processing\nof the Portuguese Language , volume 11122, pages\n409–418. Springer International Publishing, Cham.\nSeries Title: Lecture Notes in Computer Science.\nMate Akos Tundik and Gyorgy Szaszak. 2018. Joint\nWord- and Character-level Embedding CNN-RNN\nModels for Punctuation Restoration. In 2018 9th\nIEEE International Conference on Cognitive In-\nfocommunications (CogInfoCom) , pages 000135–\n000140, Budapest, Hungary. IEEE.\nMate Akos Tundik, Balazs Tarjan, and Gyorgy Sza-\nszak. 2017. ´A bilingual comparison of MaxEnt-\nand RNN-based punctuation restoration in speech\ntranscripts. In 2017 8th IEEE International Con-\nference on Cognitive Infocommunications (CogInfo-\nCom), pages 000121–000126, Debrecen. IEEE.\nM´at´e ´Akos T¨undik, Gy¨orgy Szasz´ak, G´abor Gosztolya,\nand Andr´as Beke. 2018. User-centric Evaluation of\nAutomatic Punctuation in ASR Closed Captioning.\nIn Interspeech 2018, pages 2628–2632. ISCA.\nM´at´e ´Akos T¨undik, Bal´azs Tarj´an, and Gy¨orgy Szasz´ak.\n2017. Low Latency MaxEnt- and RNN-Based Word\nSequence Models for Punctuation Restoration of\nClosed Caption Data. In Nathalie Camelin, Yan-\nnick Est `eve, and Carlos Mart ´ın-Vide, editors, Sta-\ntistical Language and Speech Processing , volume\n10583, pages 155–166. Springer International Pub-\nlishing, Cham. Series Title: Lecture Notes in Com-\nputer Science.\nVincent Vandeghinste, Lyan Verwimp, Joris Pelemans,\nand Patrick Wambacq. 2018. A Comparison of Dif-\nferent Punctuation Prediction Approaches in a Trans-\nlation Context. In Proceedings of the 21st Annual\nConference of the European Association for Ma-\nchine Translation, pages 269–278.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is\nAll You Need. arXiv:1706.03762 [cs] . ArXiv:\n1706.03762.\nAndris V¯aravs and Askars Salimbajevs. 2018. Restor-\ning Punctuation and Capitalization Using Trans-\nformer Models. In Thierry Dutoit, Carlos Mart ´ın-\nVide, and Gueorgui Pironkov, editors, Statistical\nLanguage and Speech Processing , volume 11171,\npages 91–102. Springer International Publishing,\nCham. Series Title: Lecture Notes in Computer Sci-\nence.\nFeng Wang, Wei Chen, Zhen Yang, and Bo Xu.\n2018. Self-Attention Based Network for Punctua-\ntion Restoration. In 2018 24th International Con-\nference on Pattern Recognition (ICPR), pages 2803–\n2808, Beijing. IEEE.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2020. HuggingFace’s Trans-\nformers: State-of-the-art Natural Language Process-\ning. arXiv:1910.03771 [cs]. ArXiv: 1910.03771.\nKaituo Xu, Lei Xie, and Kaisheng Yao. 2016. Investi-\ngating LSTM for Punctuation Prediction. In 2016\n10th International Symposium on Chinese Spoken\nLanguage Processing (ISCSLP), pages 1–5.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2020.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. arXiv:1906.08237 [cs] .\nArXiv: 1906.08237.\nJiangyan Yi, Jianhua Tao, Zhengqi Wen, and Ya Li.\n2017. Distilling Knowledge from an Ensemble of\nModels for Punctuation Prediction. In Interspeech\n2017, pages 2779–2783. ISCA.\nMichael R. Zhang, James Lucas, Geoffrey Hinton, and\nJimmy Ba. 2019. Lookahead Optimizer: k steps\nforward, 1 step back. arXiv:1907.08610 [cs, stat] .\nArXiv: 1907.08610.\n279\nAlp ¨Oktem. 2018. Incorporating Prosody into Neural\nSpeech Processing Pipelines. Ph.D. thesis.\nAlp ¨Oktem, Mireia Farr´us, and Leo Wanner. 2017. At-\ntentional Parallel RNNs for Generating Punctuation\nin Transcribed Speech. In Nathalie Camelin, Yan-\nnick Est `eve, and Carlos Mart ´ın-Vide, editors, Sta-\ntistical Language and Speech Processing , volume\n10583, pages 131–142. Springer International Pub-\nlishing, Cham. Series Title: Lecture Notes in Com-\nputer Science.\nPiotr ˙Zelasko, Piotr Szyma´nski, Jan Mizgajski, Adrian\nSzymczak, Yishay Carmiel, and Najim Dehak. 2018.\nPunctuation Prediction Model for Conversational\nSpeech. In Interspeech 2018 , pages 2633–2637.\nISCA."
}