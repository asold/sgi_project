{
  "title": "MMM : Exploring Conditional Multi-Track Music Generation with the Transformer",
  "url": "https://openalex.org/W3049272330",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4281459846",
      "name": "Ens, Jeff",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3209971325",
      "name": "Pasquier, Philippe",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2753868141",
    "https://openalex.org/W2959020461",
    "https://openalex.org/W1819710477",
    "https://openalex.org/W2752731013",
    "https://openalex.org/W2982753834",
    "https://openalex.org/W2891794946",
    "https://openalex.org/W2798702047",
    "https://openalex.org/W2774077477",
    "https://openalex.org/W2963408210",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2963032576",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2954957095",
    "https://openalex.org/W2919624000"
  ],
  "abstract": "We propose the Multi-Track Music Machine (MMM), a generative system based on the Transformer architecture that is capable of generating multi-track music. In contrast to previous work, which represents musical material as a single time-ordered sequence, where the musical events corresponding to different tracks are interleaved, we create a time-ordered sequence of musical events for each track and concatenate several tracks into a single sequence. This takes advantage of the Transformer's attention-mechanism, which can adeptly handle long-term dependencies. We explore how various representations can offer the user a high degree of control at generation time, providing an interactive demo that accommodates track-level and bar-level inpainting, and offers control over track instrumentation and note density.",
  "full_text": "MMM : Exploring Conditional Multi-Track\nMusic Generation with the Transformer\nJeﬀ Ens and Philippe Pasquier ⋆\nSimon Fraser University\njeffe@sfu.ca\nAbstract. We propose the Multi-Track Music Machine (MMM), a gen-\nerative system based on the Transformer architecture that is capable of\ngenerating multi-track music. In contrast to previous work, which rep-\nresents musical material as a single time-ordered sequence, where the\nmusical events corresponding to diﬀerent tracks are interleaved, we cre-\nate a time-ordered sequence of musical events for each track and con-\ncatenate several tracks into a single sequence. This takes advantage of\nthe Transformer’s attention-mechanism, which can adeptly handle long-\nterm dependencies. We explore how various representations can oﬀer the\nuser a high degree of control at generation time, providing an interactive\ndemo that accommodates track-level and bar-level inpainting, and oﬀers\ncontrol over track instrumentation and note density.\nKeywords: Symbolic Music Generation, Multi-Track\n1 Introduction\nResearch involving generative music systems has focused on modelling musi-\ncal material as an end-goal, rather than on the aﬀordances of such systems in\npractical scenarios (Sturm et al., 2019). As a result, there has been a focus on de-\nveloping novel architectures and demonstrating that music generated with these\narchitectures is of comparable quality to human-composed music, often via a\nlistening test. Although this is a necessary ﬁrst step, as systems must be capa-\nble of generating compelling material before they can be useful in a practical\ncontext, given the impressive capabilities of the Transformer-based models in\nthe music domain (Donahue, Mao, Li, Cottrell, & McAuley, 2019; Huang et al.,\n2019), we shift our focus to increasing the aﬀordances of a Transformer-based\nsystem. To achieve this goal, we develop a novel representation for multi-track\nmusical material that accommodates a variety of methods for generation.\nTo avoid any confusion, we will ﬁrst deﬁne what constitutes multi-track mu-\nsic. We consider a track to be a collection of notes played on a single instrument.\nAlthough alternate terminology has been employed to describe tracks, which\n⋆ We acknowledge the support of the Natural Sciences and Engineering Research\nCouncil of Canada (NSERC), and the Helmut & Hugo Eppich Family Graduate\nScholarship\narXiv:2008.06048v2  [cs.SD]  20 Aug 2020\n2 Jeﬀ Ens and Philippe Pasquier\nmay be referred to as voices or instruments in various contexts, we believe that\nthe term track is clearest, as there is a clear analog to the tracks present in a\ndigital audio workstation. We avoid using the term voice, as it commonly con-\nnotes a monophonic musical line, while we wish to refer to musical material\nthat may contain multiple notes sounding simultaneously. Furthermore, within\na piece there may be multiple tracks featuring the same instrument, each play-\ning a diﬀerent musical part, which would make usage of the term instrument\nproblematic. Consequently, multi-track music refers to material containing two\nor more tracks, where each track is played by a single instrument and may op-\ntionally contain multiple notes that sound simultaneously. It is also important\nto note the diﬀerence between polyphonic tracks, which contain simultaneously\nsounding notes, and monophonic tracks, which contain a single sequence of non-\noverlapping notes.\nGiven our interest in enhancing the usability of a system at generation time,\nit is worth reviewing diﬀerent methods for generation, which we group into four\ncategories: unconditioned, continuation, inpainting, and attribute-control. Un-\nconditioned generation is analogous to generating music from scratch. Besides\nchanging the data that the model is trained on, the user has limited control\nover the output of the model. Continuation involves conditioning the model\nwith musical material that precedes (temporally) the music that is to be gener-\nated. Since both unconditioned generation and continuation come for free with\nany auto-regressive model trained on a temporally ordered sequence of musical\nevents, most systems are capable of generating musical material in this man-\nner. Inpainting conditions generation on a subset of musical material, asking\nthe model to ﬁll in the blanks, so to speak. Note that inpainting can occur at\ndiﬀerent levels (i.e. note-level, bar-level, track-level). CoCoNet (Huang, Cooij-\nmans, Roberts, Courville, & Eck, 2017) allows for inpainting of Bach chorales on\nthe bar and track level, while InpaintNet (Pati, Lerch, & Hadjeres, 2019) allows\nfor inpainting of 2-8 bars of monophonic musical material. Attribute-control in-\nvolves conditioning generation on high-level attributes such as style, tempo or\ndensity. For example, music generated by MuseNet (Payne, 2019) can be con-\nditioned on a set of instruments and a musical style. In some circumstances,\ngeneration methods can be chained, resulting in an iterative generation process.\nFor example, a musical segment exhibiting a particular style could be generated\nvia attribute control, and the user could then select various sections they are\nunsatisﬁed with for inpainting.\nOur primary contribution is a novel representation for musical material,\nwhich when coupled with state-of-the-art transformer architectures, results in a\npowerful and expressive generative system. In contrast to previous work, which\nrepresents musical material as a single time-ordered sequence, where the musical\nevents corresponding to diﬀerent tracks are interleaved, we create a time-ordered\nsequence of musical events for each track and concatenate several tracks into a\nsingle sequence. Although the diﬀerence is subtle, this enables track-level in-\npainting, and attribute control over each track. We also explore variations on\nthis representation which allow for bar-level inpainting. Unto our knowledge,\nMMM : Exploring Conditional Multi-Track Music Generation 3\nboth inpainting and attribute control have not been integrated into a single\nmodel.\n2 Related Work\nThere are two main ways in which musical material is represented: as a matrix\n(i.e. a pianoroll), or as a sequence of tokens. A piano roll is a boolean ma-\ntrix x ∈{0, 1}T×P , where T is the number of time-steps and P is the number\nof pitches. Typically P = 128, allowing the piano roll to represent all possi-\nble MIDI pitches, however, it is not uncommon to reduce the range of pitches\nrepresented (Dong, Hsiao, Yang, & Yang, 2018). Multi-track musical material\ncan be represented using a boolean tensor x ∈{0, 1}M×T×P , where M is the\nnumber of tracks. However, using this type of representation is inherently inef-\nﬁcient, as the number number of inputs increases by T ×P for each track that\nis added, and accommodating small note lengths (ex. 32 nd triplets) substan-\ntially increases T. Despite these drawbacks, this representation has been used in\npractice (Boulanger-Lewandowski, Bengio, & Vincent, 2012; Dong et al., 2018;\nHuang et al., 2017). The alternative approach, is to represent musical material as\na sequence of tokens, where each token corresponds to a speciﬁc musical event or\npiece of metadata. For example, the PerformanceRNN (Oore, Simon, Dieleman,\nEck, & Simonyan, 2018) and Music Transformer (Huang et al., 2019) use a token\nbased representation comprised of 128 distinct NOTE ON tokens, which are used\nto indicate the onset of a particular pitch; 128 NOTE OFF tokens, which denote\nthe end of a particular pitch; and 100 TIME SHIFT tokens, which correspond to\ndiﬀerent time-shifts ranging from 10ms to 1 second. Although this type of repre-\nsentation can accommodate polyphony, it does not distinguish between diﬀerent\ntracks or instruments.\nOur work is most similar to LahkNES (Donahue et al., 2019), MusicVAE\n(Roberts, Engel, Raﬀel, Hawthorne, & Eck, 2018) and MuseNet (Payne, 2019),\nwhich all employ token-based representations to model multi-track music. Lahk-\nNES models Nintendo Entertainment System (NES) data, which is comprised\nof 3 monophonic tracks and a drum track, using a transformer architecture.\nMusicVAE is trained on bass, melody and drum trios extracted from the Lahk\nMIDI Dataset (LMD) (Raﬀel, 2016), which allows generation to be conditioned\non a latent vector. MuseNet is trained on a superset of the LMD, and accommo-\ndates 10 diﬀerent track types ranging from piano to guitar. Note that MuseNet\nsupports both polyphonic and monophonic tracks.\nHowever, in contrast to these methods, where the musical events from several\ntracks are interleaved into a single time-ordered sequence, we concatenate single-\ninstrument tracks, which allows for greater ﬂexibility in several areas. First of all,\nwe can decouple track information from NOTE ON and NOTE OFF tokens, allowing\nthe use of the same NOTE ON and NOTE OFF tokens in each track. This diﬀers from\nLahkNES, MusicVAE, and MuseNet, which use separate NOTE ON and NOTE OFF\ntokens for each track, placing inherent limitations on the number of tracks that\ncan be represented. Even MuseNet, which is the largest of these networks, can\n4 Jeﬀ Ens and Philippe Pasquier\nonly accommodate 10 diﬀerent tracks. Secondly, using our representation, we\nare able to accommodate a wide variety of instruments, including all 128 general\nmidi instruments, and a large number of tracks, without a employing a pro-\nhibitively large token vocabulary. In contrast to LahkNES and MusicVAE which\nare designed for a ﬁxed-schema of tracks, our system can handle an arbitrary set\nof tracks. MuseNet is similar in this regard, however, it only supports 10 distinct\ninstruments. Although MuseNet permits attribute control over the instrument,\nallowing the user to specify the set of instruments that will be featured in the\ngenerated excerpt, this information is only treated as a strong recommendation\nto the model, and does not guarantee which instruments will actually be used.\nOur system allows for speciﬁc attribute control over the instrument for each\ntrack, with the guarantee that a particular instrument will be used. Third, we\noﬀer the user control over the note-density of each track, which is not accomo-\ndated with LahkNES, MusicVAE or MuseNet. Finally, we allow for track-level\nand bar-level inpainting, which is not possible using LahkNES, MusicVAE and\nMuseNet. Collectively, these improvements aﬀord the end-user a high-degree of\ncontrol over the generated material, which has previously been proposed as a\ncritical area of research (Briot, Hadjeres, & Pachet, 2019).\n3 Motivation\nAlthough systems which generate high-quality music have been proposed in re-\ncent years (Huang et al., 2019; Payne, 2019; Liang, Gotham, Johnson, & Shotton,\n2017; Sturm & Ben-Tal, 2017), their usage in practical contexts is limited for\ntwo diﬀerent reasons. First of all, most models place restrictions on the nature\nof the input. In most cases, there are limitations placed on the number and type\nof tracks (Roberts et al., 2018; Payne, 2019). Secondly, the user is not aﬀorded\nﬁne-grained control over the generation process, which is critical for a system to\nbe useful in the context of computational assisted composition. Even MusicVAE\n(Roberts et al., 2018), which incorporates a latent model of musical space, al-\nlowing for interpolation between examples, does not aﬀord ﬁne-grained control\nof the individual tracks. For example, it is not possible to freeze the melody\nand generate a new drum-part and bassline. Although one-shot generation of\nmusical material is impressive from a technical standpoint, it is not that useful\nin a practical context, as the user may wish to create subtle variations on a ﬁxed\npiece of music.\nIn contrast to time-ordered sequences, where most of the important depen-\ndencies, such as the most recently played notes, are in the recent history, non-\ntime-ordered sequences frequently feature important dependencies in the distant\nhistory. For example, in our representation, simultaneously sounding notes in dif-\nferent tracks are spread far apart. The use of non-time-ordered representations is\ndirectly motivated the nature of the transformer attention mechanism (Vaswani\net al., 2017). In contrast to Recurrent Neural Networks (RNN), which sharply\ndistinguish nearby context (the most recent 50 tokens) from the distant history\n(Khandelwal, He, Qi, & Jurafsky, 2018), attention-based architectures allow for\nMMM : Exploring Conditional Multi-Track Music Generation 5\nBAR TRACK MULTI-TRACK BAR-FILL\nNOTE_ON=60\nTIME_DELTA=2\nNOTE_OFF=60\nNOTE_ON=64\nNOTE_ON=67\nTIME_DELTA=4\nNOTE_OFF=64\nTIME_DELTA=4\nNOTE_OFF=67\nINST=30\nDENSITY=5\nBAR_START\n<BAR>\nBAR_END\nBAR_START\n<BAR>\nBAR_END\nBAR_START\n<BAR>\nBAR_END\nBAR_START\n<BAR>\nBAR_END\nPIECE_START\nTRACK_START\n<TRACK>\nTRACK_END\nTRACK_START\n<TRACK>\nTRACK_END\nTRACK_START\n<TRACK>\nTRACK_END\nPIECE_START\nTRACK_START\nINST=30\nDENSITY=5\nBAR_START\nFILL_IN\nBAR_END\n...\nTRACK_END\nFILL_START\n<BAR>\nFILL_END\nFILL_START\n<BAR>\nFILL_END\nFig. 1.The MultiTrack and BarFill representations are shown. The <bar> tokens cor-\nrespond to complete bars, and the <track> tokens correspond to complete tracks.\ndistant tokens to be directly attended to if they are relevant to the current pre-\ndiction. Consequently, we do not pay a signiﬁcant penalty for training models on\nnon-time-ordered sequences, where important dependencies are predominantly\nin the distant history, provided the necessary tokens are within the attention\nwindow. This directly motivates the usage of non-time-ordered sequences, as\nthey facilitate rich conditional generation.\n4 Proposed Representation\nTo provide a comprehensive overview of the proposed representation, we ﬁrst\ndescribe how a single bar of musical material is represented. Based on represen-\ntations explored in previous studies (Oore et al., 2018; Huang et al., 2019), we\nrepresent musical material using 128 NOTE ON tokens, 128 NOTE OFF tokens, and\n48 TIME SHIFT tokens. Since musical events are quantized using 12 subdivisions\nper beat, 48 TIME SHIFT tokens allow for the representation of any rhythmic\nunit from sixteenth note triplets to a full 4-beat bar of silence. Each bar begins\nwith a BAR START token, and ends with a BAR END token. Tracks are simply a\nsequence of bars delimited by TRACK START and TRACK END tokens. At the start\nof each track, immediately following the TRACK START token, an INSTRUMENT to-\nken is used to specify the MIDI program which is to be used to play the notes\n6 Jeﬀ Ens and Philippe Pasquier\non this particular track. Since there are 128 possible MIDI programs, we have\n128 distinct INSTRUMENT tokens. A DENSITY LEVEL token follows theINSTRUMENT\ntoken, and indicates the note density of the current track. A piece is simply a\nsequence of tracks, however, all tracks sound simultaneously rather than being\nplayed one after the other. A piece begins with the PIECE START token. This\nprocess of nesting bars within a track and tracks within a piece is illustrated in\nFigure 1. Notably, we do not use a PIECE END token, as we can simply sample\nuntil we reach the nth TRACK END token if we wish to generate n tracks. We refer\nto this representation as the MultiTrack representation.\nUsing the MultiTrack representation, the model learns to condition the gen-\neration of each track on the tracks which precede it. At generation time, this\nallows for a subset of the musical material to be ﬁxed while generating addi-\ntional tracks. However, while the MultiTrack representation oﬀers control at the\ntrack level, it does not allow for control at the bar level, except in cases where\nthe model is asked to complete the remaining bars of a track. Without some\nchanges, it is not possible to generate the second bar in a track conditioned on\nthe ﬁrst, third, and fourth bars. In order to accommodate this scenario, we must\nguarantee that the bars on which we want to condition precede the bars we wish\nto predict, in the sequence of tokens that is passed to the model. To do this, we\nremove all the bars which are to be predicted from the piece, and replace each\nbar with a FILL PLACEHOLDER token. Then, at the end of the piece (i.e. imme-\ndiately after the last TRACK END token), we insert each bar, delimiting each bar\nwith FILL START and FILL END tokens instead of BAR START and BAR END tokens.\nNote that these bars must appear in the same order as the they appeared in the\noriginal MultiTrack representation. We refer to this representation as the BarFill\nrepresentation. Note that the MultiTrack representation is simply a special case\nof the BarFill representation, where no bars are selected for inpainting.\n5 Training\nWe use the Lahk MIDI Dataset (LMD) (Raﬀel, 2016), which is comprised of\n176,581 MIDI ﬁles. In order to explain how we derive token sequences from MIDI\nﬁles, it is necessary to provide an overview of the MIDI protocol. There are three\nformats for MIDI ﬁles. Type 0 MIDI ﬁles are comprised of a header-chunk and\na single track-chunk. Both Type 1 and 2 MIDI ﬁles contain a header-chunk and\nmultiple track-chunks, however, the tracks in a Type 1 MIDI ﬁle are played simul-\ntaneously, while tracks in a Type 2 MIDI ﬁle are played sequentially. Since only\n0.03% of the LMD are Type 2 MIDI ﬁles, and the library we use for midi parsing\ndoes not support this encoding, we simply ignore them. Within a track-chunk,\nmusical material is represented as a sequence of MIDI messages each which spec-\nify a channel and the time delta since the last message. In addition to note-on\nand note-oﬀ messages, which specify the onset and end of notes, patch-change\nmessages specify changes in timbre by selecting one of 128 diﬀerent instruments.\nTo formally deﬁne a track, consider a Type 1 MIDI ﬁle F = {t1, .., tk}comprised\nof k track-chunks, where each track-chunk ti = {mi\ni, .., mi\nni }is an ordered set of\nMMM : Exploring Conditional Multi-Track Music Generation 7\nni MIDI messages. Note that a Type 0 MIDI ﬁle is simply a special case where\nk = 1. Let chan(x) (resp. inst(x)) be a function that returns the channel (resp.\ninstrument) on which the message x is played. Then, we can deﬁne a track as the\nset of midi messages ti,c,k = {mk\nℓ : inst(mk\nℓ ) = i, chan(mk\nℓ ) = c, mk\nℓ ∈tk, tk ∈F}\nthat are found on the kth track-chunk, and played on the cth channel using\nthe ith MIDI instrument. For example, given a MIDI ﬁle F = {t1, t2}, where\nt1 = {m1\n1}, t2 = {m2\n1, m2\n2}, chan(m1\n1) = 0, inst(m1\n1) = 0, chan(m2\n1) = 3,\ninst(m2\n1) = 0, chan(m2\n2) = 3, and inst(m2\n2) = 34, there would be three tracks\n(t0,0,1, t0,3,2, t34,3,2).\nFor each of the 128 general MIDI instruments, we calculate the number of\nnote onsets for each bar in the dataset, and use the quantiles of the resulting\ndistributions to deﬁne distinct note-density bins for each MIDI instrument. Note\nthat using the same note-density bins for all instrument types would be problem-\natic as note-density varies signiﬁcantly between instruments. We use 10 diﬀerent\nnote-density bins, where the ith bin is bounded by the 10 i (lower) and 10(i + 1)\n(upper) quantiles. We train a GPT2 (Radford et al., 2019) model using the\nHuggingFace Transformers library (Wolf et al., 2019) with 8 attention heads, 6\nlayers, an embedding size of 512, and an attention window of 2048. We train two\ntypes of models: MMMBar, which is trained using the BarFill representation;\nMMMTrack, which is trained using the MultiTrack representation. We train 4-\nbar and 8-bar versions of MMMBar and MMMTrack. For 4-bar (resp. 8-bar)\nmodels we provide the model with at most 12 (resp. 6) tracks. Each time we\nselect a n-bar segment, we randomly order the tracks so that the model learns\neach possible conditional between diﬀerent types of tracks. When training the\nMMMBar models, we also select a random subset of bars for inpainting.\n6 Using MMM\nIn order to illustrate the ﬂexibility of MMM, we make available1 examples gener-\nated by the system, and an interactive demo. The demo was developed in Google\nColab, making it accessible to all users with a compatible internet browser. The\ninterface automatically selects the appropriate model, either MMMBar or MMM-\nTrack, based on the bars or tracks that are selected for generation. We brieﬂy\noutline the various ways that one can interact with MMM when generating\nmusical material.\n1. Track Inpainting : Given a possibly empty set of trackst = {t1, ..., tk}, we can\ngenerate n additional tracks. When the set of tracks is empty, this is equiv-\nalent to unconditioned generation. To do this, we condition the model with\nthe tokens representing k tracks and then sample until the nth TRACK END\ntoken is reached.\n2. Bar Inpainting : Given a set of tracks t = {t1, ..., tk}and a set of bars\nb = {b1, ..., bn}, we can resample each bar inb. For this method, we condition\nthe model with the tokens representing all the tracks, replacing each bi in b\n1 https://jeffreyjohnens.github.io/MMM/\n8 Jeﬀ Ens and Philippe Pasquier\nwith the FILL PLACEHOLDER. Then we sample until the nth FILL END token\nis reached.\n3. Attribute Control for Instruments: We can specify a set of MIDI instruments\nfor each generated track, from which the model will choose. Practically, this\nis accomplished by masking the MIDI instruments we wish to avoid before\nsampling the INSTRUMENT token at the start of a new track.\n4. Attribute Control for Note Density : We can specify the note density level\nfor each generated track.\n5. Iterative Generation : The user can chain together various generation meth-\nods to iteratively compose a piece of music. Alternatively, generation meth-\nods can be chained automatically using a meta-algorithm. For example, given\na set of tracks t = {t1, ..., tk}, we can progressively resample each track ti in\nt by asking the model to generate ( ti|{tj : tj ∈t, j̸= i}) for each 1 ≤i ≤k.\nThis bears some similarity to Gibbs sampling. The resulting output should\nbe more similar to the input than simply generating a set of tracks from\nscratch. Iterative generation also aﬀords the user the opportunity to itera-\ntively explore variations on generated material, or gradually reﬁne a piece\nby progressively resampling bars which are not to their liking.\n7 Conclusion\nOne current limitation, is that the model only allows for a ﬁxed number of bars to\nbe generated. Although approximately 99.8% of 10-track 4-bar segments, 86.8%\nof 10-track 8-bar segments and 38.8% of 10-track 16-bar segments in the LMD\ncan be represented using less than 2048 tokens, with some slight modiﬁcations\nto the architecture and representation, it should be possible to incorporate ad-\nditional musical material. The transformer-XL architecture (Dai et al., 2019)\nallows for extremely distant tokens to inﬂuence the current prediction via a hid-\nden state, combining the strengths of the attention and recurrent mechanisms.\nUsing this type of model, the current n-bar window could be conditioned on\nprevious and future (if they are known) n-bar windows via the hidden state. Im-\nplementing additional types of attribute-control is an interesting area for future\nwork. For example, conditioning generation on a particular genre or emotion\nwould oﬀer increased control at generation time. However, we must note that\nthis type of control is available to a certain extent in the current model. Since\nMMM oﬀers conditional generation, the genre or emotion of the generated bars\nor tracks should reﬂect the genre or emotion of the content they are conditioned\non. For example, if generation is conditioned on a jazz style drum track, gener-\nated tracks or bars should be consistent with this style. In addition, future work\nwill include a more rigorous evaluation of the system itself. We have introduced\na novel approach to representing musical material that oﬀers increased control\nover the generated output. This oﬀers a new and exciting avenue for future work,\nharnessing the strengths of the Transformer architecture to provide ﬁne-grained\ncontrol for the user at generation time.\nReferences 9\nReferences\nBoulanger-Lewandowski, N., Bengio, Y., & Vincent, P. (2012). Modeling tempo-\nral dependencies in high-dimensional sequences: Application to polyphonic\nmusic generation and transcription. International Conference on Machine\nLearning.\nBriot, J., Hadjeres, G., & Pachet, F. (2019). Deep learning techniques for music\ngeneration. Springer International Publishing.\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019).\nTransformer-xl: Attentive language models beyond a ﬁxed-length context.\narXiv preprint arXiv:1901.02860 .\nDonahue, C., Mao, H. H., Li, Y. E., Cottrell, G. W., & McAuley, J. (2019).\nLakhnes: Improving multi-instrumental music generation with cross-\ndomain pre-training. In Proc. of the 20th international society for music\ninformation retrieval conference (pp. 685–692).\nDong, H.-W., Hsiao, W.-Y., Yang, L.-C., & Yang, Y.-H. (2018). Musegan:\nMulti-track sequential generative adversarial networks for symbolic mu-\nsic generation and accompaniment. In Thirty-second aaai conference on\nartiﬁcial intelligence (pp. 34–41).\nHuang, C. A., Cooijmans, T., Roberts, A., Courville, A. C., & Eck, D. (2017).\nCounterpoint by convolution. In Proceedings of the 18th international so-\nciety for music information conference (pp. 211–218).\nHuang, C. A., Vaswani, A., Uszkoreit, J., Simon, I., Hawthorne, C., Shazeer, N.,\n. . . Eck, D. (2019). Music transformer: Generating music with long-term\nstructure. In 7th international conference on learning representations.\nKhandelwal, U., He, H., Qi, P., & Jurafsky, D. (2018). Sharp nearby, fuzzy\nfar away: How neural language models use context. arXiv preprint\narXiv:1805.04623 .\nLiang, F. T., Gotham, M., Johnson, M., & Shotton, J. (2017). Automatic\nstylistic composition of bach chorales with deep lstm. In Proc. of the 18th\ninternational society for music information retrieval conference (pp. 449–\n456).\nOore, S., Simon, I., Dieleman, S., Eck, D., & Simonyan, K. (2018). This time\nwith feeling: learning expressive musical performance. Neural Computing\nand Applications , 1–13.\nPati, A., Lerch, A., & Hadjeres, G. (2019). Learning to traverse latent spaces\nfor musical score inpainting. In Proc. of the 20th international society for\nmusic information retrieval conference (pp. 343–351).\nPayne, C. (2019, April). Musenet. OpenAI. (openai.com/blog/musenet)\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).\nLanguage models are unsupervised multitask learners. OpenAI Blog, 1 (8),\n9.\nRaﬀel, C. (2016). Learning-based methods for comparing sequences, with appli-\ncations to audio-to-midi alignment and matching (Unpublished doctoral\ndissertation). Columbia University.\n10 Jeﬀ Ens and Philippe Pasquier\nRoberts, A., Engel, J. H., Raﬀel, C., Hawthorne, C., & Eck, D. (2018). A\nhierarchical latent vector model for learning long-term structure in music.\nIn Proceedings of the 35th international conference on machine learning\n(pp. 4361–4370).\nSturm, B. L., & Ben-Tal, O. (2017). Taking the models back to music prac-\ntice: Evaluating generative transcription models built using deep learning.\nJournal of Creative Music Systems , 2 (1).\nSturm, B. L., Ben-Tal, O., Monaghan, ´U., Collins, N., Herremans, D., Chew, E.,\n. . . Pachet, F. (2019). Machine learning research that matters for music\ncreation: A case study. Journal of New Music Research , 48 (1), 36–55.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\n. . . Polosukhin, I. (2017). Attention is all you need. In Advances in neural\ninformation processing systems (pp. 5998–6008).\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., . . . Brew,\nJ. (2019). Huggingface’s transformers: State-of-the-art natural language\nprocessing. ArXiv, abs/1910.03771 .",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.756271243095398
    },
    {
      "name": "Transformer",
      "score": 0.7458328604698181
    },
    {
      "name": "Track (disk drive)",
      "score": 0.6453310251235962
    },
    {
      "name": "Generative grammar",
      "score": 0.5533791780471802
    },
    {
      "name": "Musical",
      "score": 0.5183571577072144
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4281895160675049
    },
    {
      "name": "Speech recognition",
      "score": 0.36902815103530884
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3166903257369995
    },
    {
      "name": "Engineering",
      "score": 0.11963725090026855
    },
    {
      "name": "Electrical engineering",
      "score": 0.1089373528957367
    },
    {
      "name": "Visual arts",
      "score": 0.095054030418396
    },
    {
      "name": "Art",
      "score": 0.06999117136001587
    },
    {
      "name": "Voltage",
      "score": 0.06903180480003357
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I18014758",
      "name": "Simon Fraser University",
      "country": "CA"
    }
  ]
}