{
  "title": "Eformer: Edge Enhancement based Transformer for Medical Image Denoising",
  "url": "https://openalex.org/W3199293888",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5081180131",
      "name": "Achleshwar Luthra",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5057395975",
      "name": "Harsh Sulakhe",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5078740324",
      "name": "Tanish Mittal",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5007456951",
      "name": "Abhishek M. Iyer",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101795498",
      "name": "Santosh Kumar Yadav",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2972491395",
    "https://openalex.org/W156779288",
    "https://openalex.org/W2584483805",
    "https://openalex.org/W2618025634",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2535388113",
    "https://openalex.org/W2510850936",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2621235041",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2914498953",
    "https://openalex.org/W2762996341",
    "https://openalex.org/W2902874468",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2304648132",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2743780012",
    "https://openalex.org/W3126582326",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2570202822"
  ],
  "abstract": "In this work, we present Eformer - Edge enhancement based transformer, a novel architecture that builds an encoder-decoder network using transformer blocks for medical image denoising. Non-overlapping window-based self-attention is used in the transformer block that reduces computational requirements. This work further incorporates learnable Sobel-Feldman operators to enhance edges in the image and propose an effective way to concatenate them in the intermediate layers of our architecture. The experimental analysis is conducted by comparing deterministic learning and residual learning for the task of medical image denoising. To defend the effectiveness of our approach, our model is evaluated on the AAPM-Mayo Clinic Low-Dose CT Grand Challenge Dataset and achieves state-of-the-art performance, $i.e.$, 43.487 PSNR, 0.0067 RMSE, and 0.9861 SSIM. We believe that our work will encourage more research in transformer-based architectures for medical image denoising using residual learning.",
  "full_text": "Eformer: Edge Enhancement based Transformer for Medical Image\nDenoising\nAchleshwar Luthra* Harsh Sulakhe* Tanish Mittal* Abhishek Iyer Santosh Yadav\nBirla Institute of Technology and Science, Pilani\n{f20180401, f20180186, f20190658, f20181105, santosh.yadav } @ pilani.bits-pilani.ac.in\nAbstract\nIn this work, we present Eformer - Edge enhance-\nment based transformer, a novel architecture that builds\nan encoder-decoder network using transformer blocks for\nmedical image denoising. Non-overlapping window-based\nself-attention is used in the transformer block that reduces\ncomputational requirements. This work further incorpo-\nrates learnable Sobel-Feldman operators to enhance edges\nin the image and propose an effective way to concatenate\nthem in the intermediate layers of our architecture. The ex-\nperimental analysis is conducted by comparing determin-\nistic learning and residual learning for the task of medi-\ncal image denoising. To defend the effectiveness of our ap-\nproach, our model is evaluated on the AAPM-Mayo Clinic\nLow-Dose CT Grand Challenge Dataset and achieves state-\nof-the-art performance, i.e., 43.487 PSNR, 0.0067 RMSE,\nand 0.9861 SSIM. We believe that our work will encourage\nmore research in transformer-based architectures for medi-\ncal image denoising using residual learning.\n1. Introduction\nModern methods for diagnosing medical conditions have\nbeen developing rapidly in recent times and a tool of utmost\nimportance is the Computerized Tomography (CT) scan. It\nis used often to help diagnose complex bone fractures, tu-\nmors, heart disease, emphysema, and more. It works in a\nmethod similar to that of the X-Ray scan. A rotating source\nof X-Ray beams is used to shoot narrow beams through a\ncertain section of your body with a highly sensitive detector\nbeing placed opposite to the source which picks up these X\nRays and uses a highly advanced mathematical algorithm to\ncreate 2D slices of a body part from one full rotation. This\nprocess is repeated until a number of slices are created. As\nhelpful as this procedure is in diagnosing, it does have some\n*equal contribution\ncause for concern as the patient is exposed to radioactive\nwaves for varying durations. CT scans have been mainly\nresponsible for increasing the radiation received by humans\nfrom medical procedures and have even led to medical pro-\ncedures becoming the second-largest source of radiation af-\nter background radiation to affect humans. Reducing the\ndose of the X-rays in CT scans is possible but leads to prob-\nlems such as increased noise, reduction of contrast in edges,\ncorners, and sharp features, and over smoothing of images.\nWe propose a method to help preserve the details and re-\nduce the noise generated from low dose scans so they may\nbecome a viable solution in place of high dose scans.\nMedical Image Denoising has garnered considerable\namount of attention from the computer vision research com-\nmunity. There has been extensive research [19, 27, 4, 10,\n14, 2] in this domain in the recent past. Although these\nmethods have shown excellent results, they implicitly as-\nsociate denoising with operations on a global scale rather\nthan leveraging the local visual information. We argue that\nwe can benefit from the patch embedding operations that\nform the basis of a vision transformer [8]. Recently, Vision\nTransformers (ViT) have shown great success in many com-\nputer vision tasks including image restoration [25] but they\nhave not been exploited on medical image datasets.\nTo the best of our knowledge, this is the first work that\nutilizes transformers for medical image denoising. The ma-\njor contributions of this paper are as follows:\n• We introduce a novel architecture - Eformer, for edge\nenhancement based medical image denoising using\ntransformers. We incorporate learnable Sobel filters\nfor edge enhancement which results in improved per-\nformance of our overall architecture. We outperform\nexisting state-of-the-art methods and show how trans-\nformers can be useful for medical image denoising.\n• We conduct extensive experimentations on training our\nnetwork following the residual learning paradigm. To\nInput Projection\nDownsampling\nSobel Convolution Output Projection\nInput Low-Dose Image Residual Noise\nLeWin Transformer\nBlock\nNormal Dose image\nConcatenation\nConvolution\nDownsampling\nLeWin Transformer\nBlock\nx2\nEdge Enhanced\nFeatures\nLC2U Block \n(n=1, k=1)\nLC2U Block \n(n=2, k=2)\nLC2D Block LC2U Block\nLC2D Block \n(n=2, k=2)\nLC2D Block \n(n=1, k=1)\nEdge Enhanced\nFeatures Concatenation\nConvolution\nUpsampling\nLeWin Transformer\nBlock\nx2\nFigure 1. Detailed description of our method. All the steps involved have been explained in 3.6. LC2(D/U) stands for LeWin Transformer,\nConcatenation block, Convolution block, and Downsampling/Upsampling.\nprove the effectiveness of residual learning in image\ndenoising tasks, we also show results using a deter-\nministic approach where our model directly predicts\ndenoised images. In medical image denoising, resid-\nual learning clearly outperforms traditional learning\napproaches where directly predicting denoised images\nbecomes similar to formulating an identity mapping.\nThis paper follows the following structure - in Section\n2 we discuss the previous work done in image denoising\nand the use of transformers in related tasks. In Section 3,\nwe have explained our approach in a detailed manner. In\nSection 4, we compare our results with existing methods\nwhich is followed by some conclusive statements and future\ndirections in Section 5.\n2. Related Work\nLow-dose CT (LDCT) image denoising is an active re-\nsearch area in medical image denoising due to its valuable\nclinical usability. Due to the limitations in the amount of\ndata and the consequent low accuracy of conventional ap-\nproaches [16], data-efficient deep learning approaches have\na huge potential in this domain. The pioneering work of\nChen et al.[6] showed that a simple Convolutional Neural\nNetwork (CNN) can be used for suppressing the noise of\nLDCT images. The models proposed in [11, 5, 23] show\nthat an encoder-decoder network is efficient in medical im-\nage denoising. REDCNN [5] combines shortcut connec-\ntions into the residual encoder-decoder network and CPCE\n[23] uses conveying-paths connections. Fully Convolu-\ntional Networks such as [10] uses dilated convolutions with\ndifferent dilation rates whereas [15] uses simple convolu-\ntion layers with residual learning for denoising medical im-\nage. GAN based models such as [27, 14] use WGAN [1]\nwith Wasserstein Distance and Perceptual Loss for image\ndenoising.\nRecently, transformer-based architectures have also\nachieved huge success in the computer vision domain pio-\nneered by ViT (Vision Transformer) [8], which successfully\nutilized transformers for the task of image classification.\nSince then, many models involving transformers have been\nproposed that have shown successful results for many low-\nlevel vision tasks including image super-resolution [26], de-\nnoising [25], deraining [3], and colorization [18]. Our work\nis also inspired by one such denoising transformer, Uformer\n[25], which employs non-overlapping window-based self-\nattention and depth-wise convolution in the feed forward\nnetwork to efficiently capture local context. We integrate\nthe edge enhancement module [19] and a Uformer-like ar-\nchitecture in an efficient novel manner that helps us achieve\nstate-of-the-art results.\n3. Our Approach\nIn this section, we provide a detailed description about\nthe components involved in our implementation.\n3.1. Sobel-Feldman Operator\nInspired by [19], we use Sobel–Feldman operator [24],\nalso called Sobel Filter, for our edge enhancement block.\nSobel Filter is specifically used in edge detection algorithms\nas it helps in emphasizing on the edges. Originally the oper-\nator had two variations - vertical and horizontal, but we also\ninclude diagonal versions similar to [19] (See Supplemental\nMaterial). Sample results of edge enhanced CT image have\nbeen shown in Figure 2. The set of image feature maps con-\nMethod MSE MSP Adv. VGG-P\nREDCNN [5] ✓ - - -\nWGAN [1] - - ✓ ✓\nCPCE [23] - - ✓ ✓\nEDCNN [19] ✓ ✓ - -\nEformer (ours) ✓ ✓ - -\nTable 1. Comparison between losses used by different methods;\nMSE - mean squared error, MSP - multi-scale perceptual, Adv. -\nadversarial, and VGG-P - VGG network based perceptual loss.\ntaining edge information are efficiently concatenated with\nthe input projection and other parts of the network (refer to\nFigure 1).\n3.2. Transformer based Encoder-Decoder\nDenoising Autoencoders [5, 23, 11], Fully Convolu-\ntional Networks [19, 15, 10], and GANs [27, 14] have been\nsuccessful in the past in the task of medical image de-\nnoising, but transformers have not yet been explored for\nthe same, despite their success in other computer vision\ntasks. Our novel network Eformer is one such step in\nthat direction. We take inspiration from Uformer [25] for\nthis work. At every encoder and decoder stage, convolu-\ntional feature maps are passed through a locally-enhanced\nwindow (LeWin) transformer block that comprises of a\nnon-overlapping window-based Multi-head Self-Attention\n(W-MSA) and a Locally-enhanced Feed-Forward Network\n(LeFF), integrated together (See Supplementary Material) .\nX′\nm = W-MSA(LN(Xm−1)) +Xm−1,\nXm = LeFF(LN(X′\nm)) +X′\nm\n(1)\nhere, LN represents the layer normalization. As shown in\nFigure 1, the transformer block is applied prior to the LC2D\nblock in each encoding stage and post the LC2U block in\neach decoding stage, and also serves as the bottleneck layer.\n3.3. Downsampling & Upsampling\nPooling layers are the most common way of downsam-\npling the input image signal in a convolutional network.\nThey work well in image classification tasks as they help\nin capturing the essential structural details but at the cost\nof losing finer details which we cannot afford, in our task.\nHence we choose strided convolutions in our downsampling\nlayer. More specifically, we use a kernel size of 3 × 3 with\nstride of 2 and padding of 1.\nUpsampling can be thought of as unpooling or reverse\nof pooling using simple techniques such as Nearest Neigh-\nbor. In our network, we use transpose convolutions [9].\nTranspose convolution reconstructs the spatial dimensions\nand learns its own parameters just like regular convolutional\nlayers. The issue with transpose convolutions is that they\nFigure 2. Example of results obtained after convolution of images\nwith Sobel-filter. Input (left) and edge-enhanced images (right).\ncan cause checkerboard artifacts which are not desirable for\nimage denoising. [21] states, to avoid uneven overlap, the\nkernel size should be divisble by the stride. Hence, in our\nupsampling layer, we use a kernel size of 4 × 4 and a stride\nof 2.\n3.4. Residual Learning\nThe goal of residual learning is to implicitly remove the\nlatent clean image in the hidden layers. We input a noisy\nimage x = y + v to our network, here x is the noisy im-\nage, in our case the low-dose image, y is the ground truth,\nand v is the residual noise. Rather than directly outputting\nthe denoised image ˆy, the proposed Eformer predicts the\nresidual image ˆv, i.e., difference between the noisy image\nand the ground truth. According to [12], when the origi-\nnal mapping is more like an identity mapping, the residual\nmapping is much easier to optimize. Discriminative denois-\ning models aim to learn a mapping function of F(x) = ˆy\nwhereas we adopt residual formulation to train our network\nto learn a residual mapping R(x) = ˆv and then we obtain\nˆy = x − R(x) =⇒ ˆy = x − ˆv.\n3.5. Optimization\nAs a part of the optimization process, we employ multi-\nple loss functions to achieve the best possible results. We\ninitially use Mean Squared Error (MSE) which calculates\nthe pixelwise distance between the output and the ground\ntruth image defined as follows.\nLmse = 1\nN\nNX\ni=1\n\r\r\r(xi − R(xi)) − yi\n\r\r\r\n2\n(2)\nHowever, it tends to create unwanted artifacts such as over-\nsmoothness and image blur. To overcome this, we employ\nboth, a ResNet [12] based Multi-scale Perceptual (MSP)\nInput Images Our Results\nFigure 3. Sample Results on AAPM Dataset [20]. More results\nhave been added in the supplementary material.\nLoss [19]. MSP can be described by the following equa-\ntion\nLmsp = 1\nNC\nNX\ni=1\nCX\ns=1\n\r\r\rϕs(xi − R(xi), ˆθ) − ϕs(yi, ˆθ)\n\r\r\r\n2\n(3)\nA ResNet-50 backbone was utilized as the feature extrac-\ntor ϕ. To be specific, the pooling layers from a ResNet-50\npretrained on the ImageNet dataset [7] were deleted, retain-\ning the convolutional blocks following which the weights\n(ˆθ) were frozen. To calculate perceptual loss, the denoised\noutput xi − R(xi), where R(xi) = ˆvi (as described in Sec-\ntion 3.4) and ground truth (yi) are passed to the extractor.\nFollowing this, feature maps are extracted from four stages\nof the backbone, as done in [19]. This perceptual loss, in\ncombination with MSE deals with both per-pixel similar-\nity in addition to overall structural information. Our final\nobjective is as follows,\nLfinal = λmseLmse + λmspLmsp (4)\nwhere, λmse and λmsp are pre-defined constants.\n3.6. Overall Network Architecture\nComposing the aforementioned individual modules, our\npipeline can be described as follows. An input image I is\nfirst passed through a Sobel Filter to produceS(I) followed\nby a GeLU activation [13]. As a part of the encoding stages,\nat each stage, we pass the input through a LeWin trans-\nformer block, proceeded by a concatenation with S(I) and\nconsequent convolution operations, similar to [19] to pro-\nduce an encoded feature map. The feature map, along with\nS(I) is then downsampled using the procedure described in\nSection 3.3. Post encoding, at the bottleneck, we pass the\nMethod PSNR ↑ SSIM ↑ RMSE ↓\nREDCNN 42.3891 0.9856 0.0076\nWGAN 38.6043 0.9647 0.0108\nCPCE 40.8209 0.9740 0.0093\nEDCNN 42.0835 0.9866 0.0079\nEformer 42.2371 0.9852 0.0077\nEformer-residual 43.487 0.9861 0.0067\nTable 2. Comparison with previous methods evaluated on AAPM\nDataset [20].\nencoded feature map to another LeWin Transformer block\nwhich is now ready to be decoded by the same number of\nstages as it was encoded. In each stage of the decoder, post\ndeconvolution, the earlier downsampledS(I) itself are con-\ncatenated with the upsampled feature maps which are then\npassed through a convolutional block. The decoder stage\ncan be viewed as an opposite of the encoder stage, with\na shared S(I). The final feature map produced after de-\ncoding is then passed through a ’output projection’ block\nto produce the desired residual. This ’output projection’ is\na convolutional layer, that simply projects the C-channel\nfeature map to a 1-channel grayscale image. In our experi-\nments, we set the depth of the LeWin block, attention heads\nand number of encoder-decoder stages each to 2. A concise\nrepresentation of the architecture can be seen in Figure 1\nwhich resembles the alphabet ’E’ hence the name Eformer.\n4. Results and Discussions\nThis subsection highlights the results attained by mea-\nsuring three different metrics to judge noise reduction and\nthe quality of the reconstructed low dose CT images. We\nuse the following metrics for the evaluation - Peak Sig-\nnal to Noise Ratio (PSNR), Structural Similarity (SSIM),\nand Root Mean Square Error (RMSE). PSNR is targeted at\nnoise reduction and is a measure of the quality of recon-\nstruction. SSIM is a perceptual metric that focuses on the\nvisible structures in an image and is a measure of the vi-\nsual quality. RMSE keeps track of the absolute pixel to\npixel loss between the two images. We compare our re-\nsults, examples shown in Figure 3, with architectures that\nshare similarities with our model in the sense they are based\non a convolutional architecture. As seen in Table 1, CPCE\n[23], WGAN [1] and EDCNN [19] like ours use a combi-\nnation of commonly used losses to train their model while\nREDCNN [5] only uses MSE. Table 2 shows that our pro-\nposed models, Eformer and Eformer-Residual, outperform\nthe state-of-the-art methods in both the PSNR and MSE\nmetrics, indicating efficient denoising and our comparable\nperformance in SSIM also suggests that the visual quality\nof the image is high and important details are not lost in the\nreconstruction.\n5. Conclusion\nTo conclude, this paper presents a residual learning\nbased image denoising model evaluated in the medical do-\nmain. We leverage transformers, and an edge enhance-\nment module to produce high quality denoised images, and\nachieve state-of-the-art performance using a combination of\nmulti-scale perceptual loss and the traditional MSE loss.\nWe believe our work will encourage the use of transformers\nin medical image denoising. In the future, we plan to ex-\nplore the capabilities of our model on a multitude of related\ntasks.\n6. Acknowledgements\nWe want to thank the members of Computer Vision Re-\nsearch Society (CVRS 1) for their helpful suggestions and\nfeedback.\nReferences\n[1] Martin Arjovsky, Soumith Chintala, and L ´eon Bottou.\nWasserstein gan, 2017. 2, 3, 4\n[2] Nicholas Bien, Pranav Rajpurkar, Robyn L. Ball, Jeremy\nIrvin, Allison Park, Erik Jones, Michael Bereket, Bhavik N.\nPatel, Kristen W. Yeom, Katie Shpanskaya, Safwan Halabi,\nEvan Zucker, Gary Fanton, Derek F. Amanatullah, Christo-\npher F. Beaulieu, Geoffrey M. Riley, Russell J. Stewart,\nFrancis G. Blankenberg, David B. Larson, Ricky H. Jones,\nCurtis P. Langlotz, Andrew Y . Ng, and Matthew P. Lun-\ngren. Deep-learning-assisted diagnosis for knee magnetic\nresonance imaging: Development and retrospective valida-\ntion of mrnet. PLOS Medicine, 15(11):1–19, 11 2018. 1\n[3] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer, 2021.\n2\n[4] Hu Chen, Yi Zhang, Mannudeep K. Kalra, Feng Lin,\nYang Chen, Peixi Liao, Jiliu Zhou, and Ge Wang. Low-\ndose ct with a residual encoder-decoder convolutional neu-\nral network. IEEE Transactions on Medical Imaging ,\n36(12):2524–2535, Dec 2017. 1\n[5] Hu Chen, Yi Zhang, Mannudeep K. Kalra, Feng Lin, Yang\nChen, Peixi Liao, Jiliu Zhou, and Ge Wang. Low-dose ct\nwith a residual encoder-decoder convolutional neural net-\nwork. IEEE Transactions on Medical Imaging, 36(12):2524–\n2535, 2017. 2, 3, 4\n[6] Hu Chen, Yi Zhang, Weihua Zhang, Peixi Liao, Ke Li, Jiliu\nZhou, and Ge Wang. Low-dose ct via convolutional neural\nnetwork. Biomed. Opt. Express, 8(2):679–694, Feb 2017. 2\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE Conference on Computer Vision and\nPattern Recognition, pages 248–255, 2009. 4\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\n1https://sites.google.com/view/thecvrs\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale, 2021. 1, 2\n[9] Vincent Dumoulin and Francesco Visin. A guide to convo-\nlution arithmetic for deep learning, 2018. 3\n[10] M. Gholizadeh-Ansari, J. Alirezaie, and P. Babyn. Deep\nLearning for Low-Dose CT Denoising Using Perceptual\nLoss and Edge Detection Layer.J Digit Imaging, 33(2):504–\n515, 04 2020. 1, 2, 3\n[11] Lovedeep Gondara. Medical image denoising using convo-\nlutional denoising autoencoders. In 2016 IEEE 16th Inter-\nnational Conference on Data Mining Workshops (ICDMW),\npages 241–246, Dec 2016. 2, 3\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition, 2015. 3\n[13] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus), 2020. 4\n[14] Zhanli Hu, Changhui Jiang, Fengyi Sun, Qiyang Zhang,\nYongshuai Ge, Yongfeng Yang, Xin Liu, Hairong Zheng,\nand Dong Liang. Artifact correction in low-dose dental ct\nimaging using wasserstein generative adversarial networks.\nMedical Physics, 46(4):1686–1696, 2019. 1, 2, 3\n[15] Worku Jifara, Feng Jiang, Seungmin Rho, Maowei Cheng,\nand Shaohui Liu. Medical image denoising using convo-\nlutional neural network: a residual learning approach. The\nJournal of Supercomputing, 75(2):704–718, Feb 2019. 2, 3\n[16] P. Kaur, Gurvinder Singh, and Parminder Kaur. A review of\ndenoising medical images using machine learning\napproaches. Current Medical Imaging Reviews, 14:675 –\n685, 2018. 2\n[17] Diederik P. Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization, 2014. cite\narxiv:1412.6980Comment: Published as a conference\npaper at the 3rd International Conference for Learning\nRepresentations, San Diego, 2015. 7\n[18] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner.\nColorization transformer, 2021. 2\n[19] Tengfei Liang, Yi Jin, Yidong Li, and Tao Wang. Edcnn:\nEdge enhancement-based densely connected network with\ncompound loss for low-dose ct denoising. 2020 15th IEEE\nInternational Conference on Signal Processing (ICSP), Dec\n2020. 1, 2, 3, 4\n[20] Cynthia H McCollough, Adam C Bartley, Rickey E Carter,\nBaiyu Chen, Tammy A Drees, Phillip Edwards, David R\nHolmes III, Alice E Huang, Farhana Khan, Shuai Leng, et al.\nLow-dose ct for the detection and classification of metastatic\nliver lesions: results of the 2016 low dose ct grand challenge.\nMedical physics, 44(10):e339–e352, 2017. 4, 7\n[21] Augustus Odena, Vincent Dumoulin, and Chris Olah. De-\nconvolution and checkerboard artifacts. Distill, 2016. 3\n[22] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in pytorch. 2017. 7\n[23] Hongming Shan, Yi Zhang, Qingsong Yang, Uwe Kruger,\nMannudeep K. Kalra, Ling Sun, Wenxiang Cong, and Ge\nWang. 3-d convolutional encoder-decoder network for low-\ndose ct via transfer learning from a 2-d trained network.\nIEEE Transactions on Medical Imaging, 37(6):1522–1534,\nJun 2018. 2, 3, 4\n[24] Irwin Sobel. An isotropic 3x3 image gradient operator. Pre-\nsentation at Stanford A.I. Project 1968, 02 2014. 2\n[25] Zhendong Wang, Xiaodong Cun, Jianmin Bao, and\nJianzhuang Liu. Uformer: A general u-shaped transformer\nfor image restoration, 2021. 1, 2, 3, 7, 8\n[26] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-\ning Guo. Learning texture transformer network for image\nsuper-resolution, 2020. 2\n[27] Q. Yang, P. Yan, Y . Zhang, H. Yu, Y . Shi, X. Mou, M. K.\nKalra, Y . Zhang, L. Sun, and G. Wang. Low-Dose CT Image\nDenoising Using a Generative Adversarial Network With\nWasserstein Distance and Perceptual Loss. IEEE Trans Med\nImaging, 37(6):1348–1357, 06 2018. 1, 2, 3\nSupplementary Material for Eformer: Edge Enhancement based\nTransformer for Medical Image Denoising\nAchleshwar Luthra* Harsh Sulakhe* Tanish Mittal* Abhishek Iyer Santosh Yadav\nBirla Institute of Technology and Science, Pilani\n{f20180401, f20180186, f20190658, f20181105, santosh.yadav } @ pilani.bits-pilani.ac.in\n7. Dataset Details\nFor our research work, we have utilized the AAPM-\nMayo Clinic Low-Dose CT Grand Challenge Dataset [20]\nprovided by The Cancer Imaging Archive (TCIA). The\ndataset contains 3 types of CT scans collected from 140\npatients. These 3 types of CT scans are abdomen, chest,\nand head which are collected from a total of 48, 49, and\n42 patients respectively. The data from each patient com-\nprises of low-dose CT scans paired with its corresponding\nnormal-dose CT scans. The low dose CT scans are syn-\nthetic CT scans which are generated by poisson noise in-\nsertion into the projection data. Poisson noise was inserted\nto reach a noise level of 25% of the full dose. Each CT\nscan is given in DICOM (Digital Imaging and Communica-\ntions in Medicine) file format. It is a standard format which\nestablishes rules for the exchange of medical images and\nassociated information between different vendors, comput-\ners and hospitals. This format meets health information ex-\nchange (HIE) standards and HL7 standards for transmission\nof health-related data. A DICOM file consists of a header\nand image pixel intensity data. The header contains infor-\nmation regarding the patient demographics, study parame-\nters, etc. stored in seperate ’tags’ and image pixel intensity\ndata contains the pixel data of the CT scan which in our\ncase contains pixel data of images of size512 ×512. In our\nmodel, for training, we extracted the image pixel data from\na Dicom file to a NumPy array using Pydicom library 1 and\nthen, the pixel data in NumPy array is scaled from 0 to 1 to\navoid heterogenous spanning of pixel data for different CT\nscans.\n8. Parameter Details and Network Training\nThe structure and architecture of the model have been\npreviously described in Section 3.6 and Figure 1 of the main\n*equal contribution\n1https://pydicom.github.io/\n- 2\n- \n 2\n0\nFigure 4. Four different sets of Sobel-filters in our implementation.\ntext. We use the Pytorch framework [22] to run our exper-\niments. The convolutional layers are initialized using the\ndefault scheme except the Sobel convolutional block. We\nenforce the filter parameters to follow the pattern as shown\nin Figure 4 where α is a learnable parameter. All our exper-\niments were run on a 16GB NVIDIA TESLA P100 GPU.\nThe model was trained with an ADAM [17] optimizer us-\ning a learning rate of 0.00002 and default parameters. The\nmodel was trained using an input size of 128 × 128 pixels\nby resizing each image from its original size of 512 × 512\npixels. The results obtained have been shown in Figure 6\n9. LeWin Transformer\nTo make our submission self-containing, we have pro-\nvided architecture details of the LeWin transformer block\n[25] in the supplementary material. LeWin transformer\nblock (Figure 5) contains 2 core designs which are de-\nscribed below. First, non-overlapping Window-based\nMulti-head Self-Attention (W-MSA), which works on\nlow-resolution feature maps and is sufficient to learn long-\nrange dependencies. Second, Locally-enhanced Feed-\nForward Network (LeFF), which integrates a convolution\n7\nLeFF\nLayer Normalisation\nW-MSA\nLayer Normalisation\n+ \n+ \nFigure 5. LeWin Transformer Block\noperator with a traditonal feed-forward network and is vi-\ntal in learning local context. In LeFF, the image patches\nare first passed through linear projection layers followed by\n3×3 depth-wise convolutional layers. Further the patch fea-\ntures are flattened and finally passed to another linear layer\nto match the dimension of input channels. The structure of\nthe LeWin Transformer Block is pictorially represented in\nFigure 5. Corresponding equations are as follows.\nX′\nm = W-MSA(LN(Xm−1)) +Xm−1, (5)\nXm = LeFF(LN(X′\nm)) +X′\nm (6)\nHere X′\nm and Xm are the outputs of the W-MSA module\nand LeFF module respectively, LN represents layer nor-\nmalization. In the W-MSA module, the given 2D feature\nmap X ∈ RC×H×W is split into N non-overlapping win-\ndows with window size M × M. Following this, self-\nattention is performed on the flattened features of each win-\ndow Xi ∈ RM2×C. Suppose the head number is j and the\nhead dimension is dj = C/j. Then, consequent computa-\ntions are,\nX = {X1, X2, . . . ,XN }, N = HW/M 2 (7)\nYi\nj = Attention(XiWQ\nj , XiWK\nj , XiWV\nj )\ni = 1. . . N\n(8)\nˆXj = {Y1\nj , Y2\nj , . . . ,YM\nj } (9)\nFigure 6. Results\nˆXj denotes the output for the j-th head. Now, output for all\nthe heads can be concatenated and then linearly projected to\nget the final results. We formulate attention calculation in\nthe same manner as done in [25].",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6546573638916016
    },
    {
      "name": "Transformer",
      "score": 0.5917457938194275
    },
    {
      "name": "Noise reduction",
      "score": 0.5752825736999512
    },
    {
      "name": "Sobel operator",
      "score": 0.5581327080726624
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5422175526618958
    },
    {
      "name": "Encoder",
      "score": 0.5055351257324219
    },
    {
      "name": "Residual",
      "score": 0.4742390513420105
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33716344833374023
    },
    {
      "name": "Computer vision",
      "score": 0.3307421803474426
    },
    {
      "name": "Computer engineering",
      "score": 0.32917648553848267
    },
    {
      "name": "Image (mathematics)",
      "score": 0.27942758798599243
    },
    {
      "name": "Image processing",
      "score": 0.2705664038658142
    },
    {
      "name": "Edge detection",
      "score": 0.2595473527908325
    },
    {
      "name": "Algorithm",
      "score": 0.2321683168411255
    },
    {
      "name": "Engineering",
      "score": 0.13026535511016846
    },
    {
      "name": "Electrical engineering",
      "score": 0.06954622268676758
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}