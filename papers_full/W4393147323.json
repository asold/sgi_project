{
  "title": "Hypothesis, Verification, and Induction: Grounding Large Language Models with Self-Driven Skill Learning",
  "url": "https://openalex.org/W4393147323",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2294772317",
      "name": "Shaohui Peng",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2096856075",
      "name": "Xing Hu",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2145132611",
      "name": "Qi Yi",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2135103550",
      "name": "Jiaming Guo",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2107737109",
      "name": "Di Huang",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2911406422",
      "name": "Zikang Tian",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2121155083",
      "name": "Ruizhi Chen",
      "affiliations": [
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2108470683",
      "name": "Zidong Du",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1981453253",
      "name": "Qi Guo",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2122743901",
      "name": "Yunji Chen",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1959451644",
      "name": "Ling Li",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2294772317",
      "name": "Shaohui Peng",
      "affiliations": [
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2096856075",
      "name": "Xing Hu",
      "affiliations": [
        "Shanghai Innovative Research Center of Traditional Chinese Medicine",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2145132611",
      "name": "Qi Yi",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2135103550",
      "name": "Jiaming Guo",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107737109",
      "name": "Di Huang",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2911406422",
      "name": "Zikang Tian",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2121155083",
      "name": "Ruizhi Chen",
      "affiliations": [
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2108470683",
      "name": "Zidong Du",
      "affiliations": [
        "Institute of Computing Technology",
        "Shanghai Innovative Research Center of Traditional Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A1981453253",
      "name": "Qi Guo",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2122743901",
      "name": "Yunji Chen",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1959451644",
      "name": "Ling Li",
      "affiliations": [
        "Institute of Software",
        "University of Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2189089430",
    "https://openalex.org/W2895560838",
    "https://openalex.org/W4319322519",
    "https://openalex.org/W2636355936",
    "https://openalex.org/W3096052269",
    "https://openalex.org/W2907923173",
    "https://openalex.org/W6850503672",
    "https://openalex.org/W4221153723",
    "https://openalex.org/W4378942474",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W4221159132",
    "https://openalex.org/W4296414573",
    "https://openalex.org/W3178987284",
    "https://openalex.org/W2229480318",
    "https://openalex.org/W4323922142",
    "https://openalex.org/W4223493866",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4286976660",
    "https://openalex.org/W4287778044",
    "https://openalex.org/W4383097638",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W2994943647",
    "https://openalex.org/W2949059942",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W2296135247",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4385573990",
    "https://openalex.org/W3126503612",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4312823397",
    "https://openalex.org/W4382468998"
  ],
  "abstract": "Large language models (LLMs) show their powerful automatic reasoning and planning capability with a wealth of semantic knowledge about the human world. However, the grounding problem still hinders the applications of LLMs in the real-world environment. Existing studies try to fine-tune the LLM or utilize pre-defined behavior APIs to bridge the LLMs and the environment, which not only costs huge human efforts to customize for every single task but also weakens the generality strengths of LLMs. To autonomously ground the LLM onto the environment, we proposed the Hypothesis, Verification, and Induction (HYVIN) framework to automatically and progressively ground the LLM with self-driven skill learning. HYVIN first employs the LLM to propose the hypothesis of sub-goals to achieve tasks and then verify the feasibility of the hypothesis via interacting with the underlying environment. Once verified, HYVIN can then learn generalized skills with the guidance of these successfully grounded subgoals. These skills can be further utilized to accomplish more complex tasks that fail to pass the verification phase. Verified in the famous instruction following task set, BabyAI, HYVIN achieves comparable performance in the most challenging tasks compared with imitation learning methods that cost millions of demonstrations, proving the effectiveness of learned skills and showing the feasibility and efficiency of our framework.",
  "full_text": "Hypothesis, Verification, and Induction:\nGrounding Large Language Models with Self-Driven Skill Learning\nShaohui Peng1, Xing Hu2, 4, Qi Yi2, 3, Rui Zhang2, Jiaming Guo2, Di Huang2,\nZikang Tian2, 5, Ruizhi Chen1, Zidong Du2, 4, Qi Guo2, Yunji Chen2, 5, Ling Li1, 5,*\n1 Intelligent Software Research Center, Institute of Software, CAS, Beijing, China\n2 SKL of Processors, Institute of Computing Technology, CAS, Beijing, China\n3 University of Science and Technology of China, USTC, Hefei, China\n4 Shanghai Innovation Center for Processor Technologies, SHIC, Shanghai, China\n5 University of Chinese Academy of Sciences, UCAS, Beijing, China\npengshaohui@iscas.ac.cn\nAbstract\nLarge language models (LLMs) show their powerful auto-\nmatic reasoning and planning capability with a wealth of\nsemantic knowledge about the human world. However, the\ngrounding problem still hinders the applications of LLMs in\nthe real-world environment. Existing studies try to fine-tune\nthe LLM or utilize pre-defined behavior APIs to bridge the\nLLMs and the environment, which not only costs huge human\nefforts to customize for every single task but also weakens\nthe generality strengths of LLMs. To autonomously ground\nthe LLM onto the environment, we proposed the Hypothesis,\nVerification, and Induction (HYVIN) framework to automati-\ncally and progressively ground the LLM with self-driven skill\nlearning. HYVIN first employs the LLM to propose the hy-\npothesis of sub-goals to achieve tasks and then verify the fea-\nsibility of the hypothesis via interacting with the underlying\nenvironment. Once verified, HYVIN can then learn general-\nized skills with the guidance of these successfully grounded\nsubgoals. These skills can be further utilized to accomplish\nmore complex tasks that fail to pass the verification phase.\nVerified in the famous instruction following task set, BabyAI,\nHYVIN achieves comparable performance in the most chal-\nlenging tasks compared with imitation learning methods that\ncost millions of demonstrations, proving the effectiveness of\nlearned skills and showing the feasibility and efficiency of our\nframework.\nIntroduction\nLarge language models (LLMs) have shown their powerful\ncapability in automatic reasoning and planning with a wealth\nof semantic knowledge about the human world (Wei et al.\n2022, 2021; OpenAI 2023; Kojima et al. 2022). However,\nthere still remains a large gap in adopting LLMs to auto-\nmatically solve problems in specific environments. This is\nbecause of the misalignment between the LLM’s semantic\nplanning and the grounded-specific implementation, which\nis also known as the grounding problem (Ichter et al. 2022;\nDriess et al. 2023). Solving this problem can unlock the\nLLMs’ capacity of understanding and affecting the real\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nworld, which is a solid step towards real-world applications\nof artificial intelligence.\nTo address the grounding problem, existing studies try to\nfine-tune the LLM to predict feasible actions (Carta et al.\n2023; Li et al. 2022; Wang et al. 2022) or utilize a set of\nbehavior APIs (i.e. low-level skills) that serve as a bridge\nbetween the LLMs and the environment (Ichter et al. 2022;\nRaman et al. 2022; Liang et al. 2022). On the one hand, fine-\ntuning LLMs is of low sample efficiency and may also dam-\nage the reasoning ability of LLMs. On the other hand, exist-\ning methods relying on behavior APIs often assume the APIs\nare pre-defined by the environment (Liang et al. 2022) or\npre-trained using expert demonstrations (Ichter et al. 2022),\nwhich not only costs huge human efforts to customize for ev-\nery single task but also weakens the generality strengths of\nLLMs. Therefore, how to autonomously ground the LLM\nonto the environment still remains an open problem and is\nthe key challenge of LLM-based agents.\nIt is challenging to achieve the goal of autonomous\ngrounding that maps the LLM’s semantic plan to practi-\ncal implementation, because of the following reasons: 1)\nThe prerequisite of grounding, obtaining successful experi-\nences, is difficult because of the sparse rewards in the phys-\nical world. 2) Even obtaining rare success experiences, the\ngrounding is usually closely related to specific tasks with-\nout a shared API library, therefore is of low generality and\ninvaluable for general tasks. To address these issues, we pro-\nduce intrinsic rewards based on LLM-generated subgoals\nand their check functions, which increase successful expe-\nriences by alleviating the sparse reward issue. We then pro-\npose the language-aligned general skill learning methodol-\nogy by forcing each skill to achieve a group of goals with\nsimilar semantic descriptions. These skills show good gen-\nerality in solving other or even more complex tasks.\nIn summary, we propose a Hypothesis, Verification, and\nInduction (HYVIN) framework that intimately combines the\nLLM and the reinforcement learning process within the fol-\nlowing key stages: 1) Hypothesis: the LLM not only acts\nas the planner by decomposing tasks into small subgoals but\nalso provides the check functions so that RL agents can eval-\nuate whether they can complete these subgoals. Such intrin-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n14599\nsic rewards from the LLM significantly alleviate the sparse\nreward issue. 2) Verification: with the subgoals and corre-\nsponding check functions, RL agents learn the policies of the\nsubgoals based on intrinsic rewards and finally are verified\nthrough whether the tasks are accomplished. 3) Induction:\nRL agents cluster verified subgoals through semantic sim-\nilarity and learn generalized skill policy upon them. With\nthese general skills, the LLM can generate solutions for un-\nseen or even more complex tasks through minimal and effi-\ncient interaction.\nWe validate the self-driven grounding framework in in-\nstruction following tasks, which is common and reason-\nable for LLM-based agents because of textural task instruc-\ntion. Verified in BabyAI, a grid world platform to study\nlanguage-grounded tasks, our automatic grounding frame-\nwork achieves comparable performance in the most difficult\ntasks compared with imitation learning methods that cost\nmillions of demonstrations. The experiment results not only\nproves the effectiveness of learned skills but also show the\nfeasibility and efficiency of our framework.\nRelated Work\nLLM-assisted Agents LLMs show their great power in\nautomatic reasoning and planning with a wealth of semantic\nknowledge about the human world. Therefore, it is promis-\ning to involve LLMs in developing intelligent agents. The\nkey challenge in LLM-assisted agents is how to ground the\nLLM’s knowledge (in linguistic form) to the tasks at hand.\nRegarding this challenge, there are two mainstream meth-\nods: (1) utilizing a set of behaviour APIs with detailed lin-\nguistic annotations. Such APIs can be pre-defined by the\nenvironment (Liang et al. 2022) or pre-trained using expert\ndemonstrations (Ichter et al. 2022; Huang et al. 2022; Yuan\net al. 2023). For example, Code as Policies (Liang et al.\n2022) uses the LLM to generate executable codes for ac-\ncomplishing instructions which can invoke behaviour APIs\nunder certain conditions. SayCan (Ichter et al. 2022) invites\nhumans to rate the success of given demonstrations, which\nare utilized to train API policies and then derive an affor-\ndance function. V oyager (Wang et al. 2023) stores and re-\ntrieves executable code, which calls pre-implemented basic\nAPIs, to handle complex scenarios. To better interact with\nthe environment, some approaches (Huang et al. 2022; Ra-\nman et al. 2022) introduce the environment feedback to re-\ngenerate new plans, which can be seen as another kind of\ntrial-and-error learning. Although these methods have made\nimpressive progress by utilizing APIs, their applications are\nalso limited by the behaviour APIs in that the agent can only\naccomplish tasks that can be solved by arranging these ba-\nsic APIs. (2) fine-tuning the LLMs. The LLM can be fine-\ntuned to predict the agent’s feasible action given the state\ndescriptions. Such fine-tuning can be performed using ex-\npert demonstrations (Wang et al. 2022; Li et al. 2022) or on-\nline RL (Carta et al. 2023). However, fine-tuning a model as\nlarge as the LLM is quite time expensive and requires much\ntraining data.\nInstruction Following In instruction following, an agent\nis given an instruction and the goal is to accomplish the\ntask described by the instruction. Such a paradigm makes\nthe agent able to assist human beings by following human\ninstructions, which has wide real-world applications. The\nworks for instruction following can be divided into three\ncategories: (1) Semantic-parsing methods (Artzi and Zettle-\nmoyer 2013; Misra et al. 2016), which directly parses the\ninstruction into the agent’s actions via lexical analysis and\nother pre-defined rules. These methods require great hu-\nman efforts to design proper rules, and can not general-\nize to complex environments. (2) Learning-based methods,\nwhich directly train a language-conditioned policy to ac-\ncomplish instructions (Peng et al. 2023). Many prior works\nrequire expert demonstrations in their training loops. For ex-\nample, expert demonstrations are often used in policy im-\nitation learning (Lynch and Sermanet 2021; Chaplot et al.\n2018), hindsight instruction relabelling (R ¨oder, Eppe, and\nWermter 2022; Chen, Gupta, and Marino 2021), and learn-\ning the language-conditioned reward function (Bahdanau\net al. 2019). Some works try to sidestep the need for expert\ndemonstrations (Ranzato et al. 2021; Huang et al. 2023), but\nat the cost of much lower sample efficiency. All learning-\nbased approaches are typically trained using hard-coded in-\nstruction templates, which can not provide diverse, ambigu-\nous and long-term planning instructions as humans. There-\nfore, they can only deal with simple and low-level instruc-\ntions such as pick-and-place tasks. (3) LLM-based methods,\nwhich use LLMs to assist the understanding and planning\nof instructions (Ichter et al. 2022; Liang et al. 2022; Raman\net al. 2022). See the last paragraph for more details.\nPreliminaries\nProblem Formulation\nWe consider adopting an LLM-based agent to solve instruc-\ntion following (IF) tasks.Each instruction I ∈ T describes a\ntask coarsely in the environment. Given the instruction, only\nwhen the agent accomplishes the task using primitive action\nset A can receive a positive reward from the environment.\nFor example, in BabyAI, which is a famous instruction fol-\nlowing task set, instructions like “Open the green door” or\n“Put the red box next to the blue ball” specify some macro-\nscopic object manipulation tasks, while the agent needs to\naccomplish them in a grid world using primitive actions like\n“turn right”, “move forward”, “pick” and so on.\nAn LLM-based agent takes instructions I and environ-\nment observation o as input and outputs actions to accom-\nplish tasks. As shown in Figure 1, the general framework of\nLLM-based agents contains a high-level planner and some\nlow-level skills that can unitize the semantic knowledge in\nthe LLM to accomplish instruction following tasks. Given\nthe coarse instruction, the planner (often LLMs) will de-\ncompose it into a sub-instruction sequence or generate a\nprogram to solve it. At the same time, the low-level skills\nconsist of pre-trained policies or pre-implemented scripts to\nexecute the plan or program. Researchers usually assume the\nenvironment provides textual descriptions of state and task-\nrelated feedback to adapt to the LLM setting. With the im-\nmediate translation from the semantic output to execution\nin the environment through low-level APIs, the high-level\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n14600\nplanner can get adequate feedback and iteratively refine its\nplan to accomplish tasks.\nCurrent methods leverage semantic knowledge in the\nhigh-level planner to reason and decompose the coarse in-\nstruction, but meanwhile, bypass the grounding problem by\ntranslating the semantic plan into implementation through\npre-defined low-level APIs. In order to maximize the use\nof priors in LLMs to reduce human effort, our framework\naims to automatically learn generalized skills in the environ-\nment to build low-level APIs to solve the grounding prob-\nlem.Besides, in this paper, we assume the LLM could call\nbasic perception functions P = {p1, p2, ...}, like “GetOb-\nservedObjects()”, to get environment status instead of pre-\ndesigned textual observation and feedback mechanisms.\nChallenges\nThe essential problem of building an LLM-based agent is\nto ground semantic knowledge of LLM in the environment.\nTo automatically solve the grounding problem, there are two\nmain challenges:\n• How to obtain successful grounding experiences from\nscratch? Without pre-defined low-level APIs, the agent\ncannot interact with the environment to attempt the se-\nmantic plan directly and efficiently explore the reward\nof task accomplishment. To address the challenge, we\nmake the LLM hypothesize the plan for each task and\ngenerate corresponding checks for each sub-step. Based\non these intrinsic rewards provided by checks, we can\nquickly train small policies to execute and verify the plan,\nthen collect successful trajectories as grounding experi-\nences. Although such experience obtained through quick\nattempts may belong to simple tasks, we can also lever-\nage them to enhance the grounding ability of the agent to\naccomplish more complex and long-term tasks progres-\nsively.\n• How to efficiently train generalized low-level behavior\nAPIs under the guidance of experience? The subgoals in\nsuccessful experience are proposed by the LLM based on\nspecific instruction, and cannot be applied to new scenar-\nios as general behavior APIs. Inspired by inductive ideas\nin mathematics, we introduce a mechanism to group sub-\ngoals with similar semantics together, then train skill\npolicies that can achieve a group of subgoals as gener-\nalized behavior APIs. Unlike standard online reinforce-\nment learning, which suffers from data efficiency issues,\nwe make skills training efficient through dense rewards\nprovided by checks, and initial state restoration by the\nsuccessful trajectories.\nMethod\nIn this section, we will first give an overview of our proposed\nHYVIN framework, which can automatically and progres-\nsively ground LLM in the environment.\nOverview\nAs shown in Figure 2, HYVIN can be divided into four\nphases.\nFigure 1: A general framework of LLM-based agents\n• Hypothesis: For each instruction, LLM tries to decom-\npose it into subgoals and generate check functions for\neach subgoal.\n• Verification: Based on the reward provided by the check\nfunction, we train separate policies for each subgoal\nwithin limited steps until the task is accomplished to ver-\nify the feasibility of the hypothesis of LLM.\n• Induction: We group the subgoals in successful hypothe-\nses with similar semantics to train generalized skills re-\ninforcement learning.\n• Deduction: Based on learned skills as low-level actors,\nwe use LLM as a few-shot high-level planner to generate\nprograms to solve unseen and more complex tasks.\nHypothesis\nThe hypothesis phase aims to solve tasks separately regard-\nless of generality to collect grounding experience. Consider-\ning the gap between the semantic knowledge in LLM and the\nenvironment, the hypothesis phase decomposes the task into\nseveral subgoals rather than directly giving the solution, and\nleaves correctness verification to the next process. As shown\nin Figure2(a), the hypothesis can be formed as I\nPrompt\n− − − − − →\nG, F. We use LLM as the zero-shot planner, which takes\nan instruction I and necessary decomposition prompt as in-\nput, then outputs a subgoal sequence G = {g1, g2, ...} and\ncorresponding check functions F = {f1, f2, ...}. The sub-\ngoal gi is a small instruction labeled with the explicit mark\n“Goal X” to facilitate further processing. Each check func-\ntion fi : S → {0,1} is a program that checks the achieve-\nment of corresponding sub-goal gi via invoking perception\nfunctions provided by the environment. To make the LLM\noutput as we want, except for the instruction I, we add role\ndefinition, perception APIs descriptions, and explanation of\nthe task space to the decomposition prompts.\nVerification\nAfter getting the subgoals and check functions, we need to\nverify their feasibility in the environment to collect success-\nful grounding experiences.The feasibility of decomposition\nis verified by the consistency between achieved signals gen-\nerated by check functions and the tasks accomplished sig-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n14601\nFigure 2: Overview of Hypothesis, Verification, and Induction (HYVIN) framework. (a) Hypothesis: try to decompose tasks;\n(b) Verification: leverage efficient interaction with the environment to verify the results of hypothesis; (c) Induction: group\nsuccessful experience to train generalized skills; (d) Deduction: build few-shot planner to solve tasks using acquired skills.\nnal. Specifically, as shown in Figure 2(b), we train indepen-\ndent policies for each subgoal based on the bool reward pro-\nvided by its check function. Once the subgoal is achieved,\nwe stop the training and save the action sequence. The saved\naction sequence can be used as the restoring mechanism to\nprepare the initial state for further skill training. Until all\nsub-goals are achieved and the task is also accomplished,\nwe can verify the decomposition is successful. Meanwhile,\nthe grounding experiences (including subgoal descriptions,\ncheck functions, and restore action sequences) are collected\nfor skill learning in the future phase. Considering some com-\nplex and long-term tasks cannot be solved by direct decom-\nposition, the above mechanism is only suitable for solving\nsimple tasks within an acceptable environment interaction\nsteps to collect experience. We set the maximum number of\nverification steps Tverify as a threshold to distinguish in-\ntractable complex tasks for the next stage to solve.\nInduction\nAfter collecting successful grounding experiences, the in-\nduction phase aims to discover and learn generalized skills\nfrom separate grounding trajectories of different instructions\nso that we can reuse them in more unseen and complex tasks.\nDiscovery As described above, we have collected task-\nsolving experience through efficient hypothesis and verifi-\ncation, including subgoal descriptions, corresponding check\nfunctions, and start state restore action sequences. However,\nsuch successful experiences can only be used on specific in-\nstructions that are easy to decompose for LLM. To make\nthe LLM-based agent able to solve more unseen and com-\nplex tasks, we must further abstract and learn skills to build\nthe generalized low-level actor.To this end, we cluster the\ncollected subgoals according to their semantics to ensure a\ncertain generalized skill can accomplish a category of sub-\ngoals as shown in Figure 2(c). Specifically, we first use LLM\nto translate each subgoal description gi into API descrip-\ntion gi,api and parameter gi,param. For example, the sub-\ngoal “discover the green box” is translated into the API “dis-\ncover” and parameter “green box”. Then we use the k-means\nalgorithm to conduct unsupervised clustering based on the\nsemantic distance between subgoal descriptions computed\nby the following cosine similarity:\nC(gi,api, gj,api) = emb(gi,api) · emb(gj,api)\n||emb(gi,api)|| · ||emb(gj,api)||, (1)\nwhere gi,api is the API description andemb(·) is the embed-\nding functions of LLM.\nTraining We have divided subgoals with similar seman-\ntics into different categories through the clustering pro-\ncess.Then we build reinforcement learning (RL) environ-\nments to train skills that can achieve a cluster of subgoals\nseparately. Unlike common RL environments, the skill train-\ning environment is like a multitask learning scenario. Each\nsubgoal of a cluster can be seen as a single task, the subgoal\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n14602\nFigure 3: An example of BossLevel task in BabyAI\nparameter gi,param is the task description, the reward is pro-\nvided by the corresponding check functionfi, and the initial\nstate is set by the saved restore action sequence. Trained in\nsuch a multiple tasks environment consists of subgoals that\nhave the same semantics, the skill is supposed to have gen-\neralization in tasks with similar scenarios. Besides, subgoals\nbelonging to the same cluster are divided into training and\nverification sets to monitor the generalization ability of the\ntrained skill to prevent overfitting.\nDeduction\nThrough the above process, we have overcome the chal-\nlenges of obtaining successful grounding experience from\nscratch and training generalized skills efficiently. In other\nwords, we have autonomously built the low-level skills for\nthe LLM-based agent without human effort. To apply auto-\nmatically learned skills to solve unseen and complex instruc-\ntions following tasks, we next introduce the high-level plan-\nner in this section. As shown in Figure 2(d), we use LLM\nas a few-shot planner to generate programs to accomplish\ntasks. The process can be divided into a program generation\nphase and a debugging phase.\nGeneration The generation prompt for LLM contains the\nrole definition and API descriptions (including skills and\nperception functions). Besides, considering the complexity\nof the task, the generation prompt follows a few-shot in-\ncontext learning paradigm. We include some skill API de-\nscriptions and a hand-written example that leverages the\nlearned skills to solve a complex instruction-following task.\nDebugging To better solve complex tasks, we also de-\nsigned an interaction debugging process in our high-level\nplanner. Besides the task instruction and the generated pro-\ngram to be modified, the debugging prompt also includes\nthe error message and some general debugging suggestions\nto fix the possible bugs. Benefiting from the feasibility and\nrobustness of adaptive learned skills, the basic error report-\ning mechanism based on illegal action detection can ef-\nfectively improve the accuracy of the generated programs,\nwhich greatly reduces human effort.\nTask\nMethod Type Demos Success rates\nGoTo\nOriginal\nIL 10K 99.8\nLID-Text IL 10K 99.5\nChatGPT LLM 0 44\nHYVIN LLM 0 99.9\nHYVIN-action LLM 0 55.1\nHYVIN-no-skills LLM 0 82.1\nPickup\nOriginal IL\n10K 99.8\nLID-Text IL 10K 99.8\nChatGPT LLM 0 0\nHYVIN LLM 0 92.9\nHYVIN-action LLM 0 47.6\nHYVIN-no-skills LLM 0 73.8\nPutNext\nOriginal\nIL 10K 97.7\nLID-Text IL 10K 99.9\nChatGPT LLM 0 0\nHYVIN LLM 0 91.9\nHYVIN-action LLM 0 0\nHYVIN-no-skills LLM 0 85.4\nOpen\nOriginal IL\n1M 100\nChatGPT LLM 0 0\nHYVIN LLM 0 92.4\nHYVIN-action LLM 0 0\nHYVIN-no-skills LLM 0 62.5\nSynth\nOriginal IL\n1M 87.7\nLISA IL 100K 61.2\nChatGPT LLM 0 0\nHYVIN LLM 0 78.9\nHYVIN-action LLM 0 0\nHYVIN-no-skills LLM 0 13.5\nBoss\nOriginal IL\n1M 77\nLISA IL 100K 69.8\nChatGPT LLM 0 0\nHYVIN LLM 0 75.9\nHYVIN-action LLM 0 0\nHYVIN-no-skills LLM 0 8.6\nHYVIN-GPT4 LLM 0 85.9\nTable 1: Overall results. “IL” means “Imitation Learning”,\n“LLM” means “LLM-based agent”.\nResults\nExperiment Setting\nEnvironment To evaluate the efficiency and effectiveness\nof our proposed framework that automatically discovers,\nlearns, and applies skills, we test HYVIN on the BabyAI en-\nvironment (Chevalier-Boisvert et al. 2019). BabyAI is a grid\nworld environment for instruction following. Given the lan-\nguage instruction and a 7 × 7 × 3 partial and local view, the\nagent must learn to accomplish various tasks of arbitrary dif-\nficulty levels. In this paper, we choose the following six lev-\nels of instruction with different types and difficulties (more\ndetails of the environment can be found in the Appendix):\nGoToLocal: Go to an object inside a single room. Picku-\npLocal: Pick up an object inside a single room. PutNext-\nLocal: Pick up an object and put it next to another object\ninside a single room. Open: Open a door in a 3 × 3 room\nmaze, the door may in another room. SynthSeq: Union of\nall instructions from PutNext, Open, Goto, and PickUp and\nmay with multiple commands. BossLevel: The hardest task\nof BabyAI as shown in Figure 3. The command can be any\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n14603\nFigure 4: Training curves of skills. The blue curve repre-\nsents skill learning through restoring the initial state, while\nthe green curve doesn’t. The dotted curve represents test per-\nformance in the held-out verification set.\nFigure 5: Successful rates of different interaction times\nsentence drawn from the Baby Language grammar.\nBaselines We verify the effectiveness of HYVIN by com-\nparing it with several baselines, including Imitation Learn-\ning methods relying on expert demonstrations and a variant\nof our LLM-based agent: ChatGPT takes textual observa-\ntion and obtains actions through dialog (following settings\nin GLAM (Carta et al. 2023)). Original is the baseline from\nthe original BabyAI paper, which trained the GRU + CONV\nmodel with imitation learning using one million demonstra-\ntion episodes for each level. LID-Text (Li et al. 2022) is\nan approach that first represents goals and observations as a\nsequence of embeddings, then uses a policy network initial-\nized with a pre-trained LM and trained with demonstrations\nto predict the next action. LISA (Garg et al. 2022) is a hier-\narchical imitation learning framework that can learn diverse,\ninterpretable skills from language-conditioned demonstra-\ntions. HYVIN-action is the variant of HYVIN that main-\nFigure 6: Successful rates of different attempts times\nFigure 7: Task verification results\ntains the same high-level planner but employs primitive ac-\ntions instead of the acquired skills. HYVIN-no-skills is the\nvariant of HYVIN that only has hypothesis and verification\nphases without inductive skill learning. HYVIN-GPT4 is\nthe variant that uses GPT-4 instead of ChatGPT to enhance\nperformance.\nImplementation In this paper, we use ChatGPT (GPT-\n3.5-turbo) as the large language model to complete task\ndecomposition, the semantic embedding of API, high-level\nplanning, and debugging. More details on the prompt con-\ntents are shown in the Appendix. In the verification and\nskill learning phase, we use the standard model proposed\nin BabyAI, and train the policy using the PPO algorithm.\nOverall Results Comparison\nThe main performance results are shown in Table 1. We sep-\narately compare HYVIN with baselines in each level task.\nFor each level task, we randomly sample 100 instructions\nthat never occurred in the skill training phase. Considering\nthe randomness of ChatGPT’s answers, we repeat the exper-\niment of each instruction 3 times to get the average results.\nThe results showed that HYVIN can achieve comparable\nperformance using automatically learned skills rather than\na large number of expert demonstrations, which shows the\neffectiveness of our framework. Besides, the results of Chat-\nGPT, HYVIN-action, and HYVIN-no-skills showed it can-\nnot directly solve BabyAI’s tasks. The main cause is LLMs\ncannot cooperate with low-level primitive actions without\nproper grounding, which also emphasizes the importance of\nself-driven skill learning in the LLMs grounding scenario.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n14604\nAblation Results\nSkill Learning Ablation We first investigate the data ef-\nficiency when learning skills through reinforcement learn-\ning as shown in Figure 4 . Although the difficulty varies be-\ncause the clustering process only relies on the semantics of\nsubgoals and ignored the difficulty, skills can be learned ef-\nficiently. The verification phase proves that the instruction\nhas been decomposed into small enough subgoals and can\nbe trained in limited steps. Therefore, the skill training en-\nvironment consisting of verified subgoals can lead to an ef-\nficient skill training process. Besides, results also demon-\nstrate that restoring actions that make the skill start at an ex-\npected state is important to learning efficiency. Without the\nstart state reset, the learning efficiency drops obviously, and\nsome skills even cannot be learned. Some learning curves\nshow overfitting trends in the late period, which is different\nfrom the normal reinforcement learning process and Illus-\ntrates the role of the hold-out validation set. For some skills\nlike “enter\nunexplored room”, the green curves seem better\nthan the blue ones. This is because without restoring the ini-\ntial state, the difficulty decreases a lot since the agent can\nenter any room.\nDeduction Ablation We also explored the effect of the in-\nteraction debugging times and multiple attempts of skills.\nThe ablation results show the importance of interactive de-\nbugging and multiple attempts.\nA. Interaction Times: Figure 5 shows the results of dif-\nferent interaction times between the high-level planner and\nthe environment. For some tasks like “GoToLocal”, the suc-\ncessful rate promotion is limited because of simplicity. For\ncomplex tasks like “BossLevel”, repeat debugging can bring\nmore than 40% promotion, which shows the adaptability and\nfeasibility of learned skills. However, when the performance\nreaches some ceiling bound, more interaction seems useless.\nB. Multiple Attempts: Different from pre-defined APIs\nwith scripts, our learned skills are stochastic policies. Thus,\nwe also investigate the effect of multiple attempts of skill\npolicy on the final success rate. Figure 6 shows similar re-\nsults with interaction times, the multiple attempts improve\ncomplex tasks greater.\nMethod Details\nTo show more insight into HYVIN, we show some key in-\ntermediate results. Task Verification: Figure 7 shows the\nverification results of different level tasks. In the implemen-\ntation, we random sample 100 instructions from each level\ntask, and set the verification steps threshold Tverify equals\nto 3000. The results prove our assumption, for some simple\ntask levels, like “GoToLocal”, “PickupLoc” and “PutNext-\nLocal”, the LLM can decompose the instruction into reason-\nable subgoals and check functions, so that the verification\ntraining be successful in limited steps. For hard levels which\nalso include some simple tasks, direct decomposition can\naccomplish few instructions, which means it cannot solve\ncomplex and long-term tasks. We also show more details of\nSkill Clustring in Appendix.\nTools 0 → wood wood → stone stone → iron\n66.9% 25.8% 2.1%\nTable 2: Verification in simple tasks with 50k steps\nHYVIN DreamerV2\n0 →wood\n92.3% 92.7%\n0 →wood\npickaxe 90.6% 59.6%\n0 →stone 31.4% 42.7%\n0 →stone pickax\ne 32.5% 0.2%\n0 →iron 0.7% 0%\n0 →iron pickax\ne 0% 0%\nTable 3: Deduction in complex tasks\nA Complex Case Study\nWe make a case study on a more complex Minecraft-like\nenvironment, Crafter (Hafner 2021), to show the effective-\nness of HYVIN. In the hypothesis and verification phases,\nHYVIN initially accomplishes simple tasks that only in-\nclude one-stage complexity and collects grounding experi-\nences, as shown in Table 2. Then in the induction phase,\nHYVIN learns some useful skills like collecting materials\nand making tools to solve complex tasks, as shown in Table\n3. The results show that HYVIN significantly outperforms\nthe SOTA learning-based method, DreamerV2, in Crafter.\nConclusion\nIn this paper, we propose a framework called Hypothesis,\nVerification, and Induction (HYVIN) to address the chal-\nlenge of automatically grounding LLM onto specific envi-\nronments. In order to alleviate the problem of grounding ex-\nperience acquisition, we make the LLM not only decompose\ntasks but also generate intrinsic rewards to help RL agents\nefficiently verify the decomposition results. We also pro-\npose a language-aligned general skill learning methodology\nby forcing each skill to achieve a group of goals with similar\nsemantic descriptions to enhance their generality. Compared\nwith imitation learning methods that cost millions of demon-\nstrations, HYVIN can achieve comparable performance in\nthe hardest tasks in BabyAI. The ablation study also shows\nthe flexibility and feasibility of learned skills in the interac-\ntions between the high-level planner and the environment.\nWe leave introducing multi-modal LLMs to extend the ap-\nplications of HYVIN as future works. Besides, considering\nHYVIN only contains a single cycle of hypothesis, verifica-\ntion, and induction. It is an interesting and promising direc-\ntion to design a mechanism of multiple cycles in HYVIN,\nallowing HYVIN to learn more powerful and diverse hierar-\nchical skills to accomplish more flexible tasks.\nAcknowledgements\nThis work is partially supported by the NSF of China (under\nGrant 92364202).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n14605\nReferences\nArtzi, Y .; and Zettlemoyer, L. 2013. Weakly Supervised\nLearning of Semantic Parsers for Mapping Instructions to\nActions. Trans. Assoc. Comput. Linguistics, 1: 49–62.\nBahdanau, D.; Hill, F.; Leike, J.; Hughes, E.; Hosseini, S. A.;\nKohli, P.; and Grefenstette, E. 2019. Learning to Understand\nGoal Specifications by Modelling Reward. In 7th Interna-\ntional Conference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019.\nCarta, T.; Romac, C.; Wolf, T.; Lamprier, S.; Sigaud, O.;\nand Oudeyer, P. 2023. Grounding Large Language Mod-\nels in Interactive Environments with Online Reinforcement\nLearning. CoRR, abs/2302.02662.\nChaplot, D. S.; Sathyendra, K. M.; Pasumarthi, R. K.; Ra-\njagopal, D.; and Salakhutdinov, R. 2018. Gated-Attention\nArchitectures for Task-Oriented Language Grounding. In\nProceedings of the Thirty-Second AAAI Conference on Ar-\ntificial Intelligence, (AAAI-18), the 30th innovative Applica-\ntions of Artificial Intelligence (IAAI-18), and the 8th AAAI\nSymposium on Educational Advances in Artificial Intelli-\ngence (EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, 2819–2826.\nChen, V .; Gupta, A.; and Marino, K. 2021. Ask Your Hu-\nmans: Using Human Instructions to Improve Generaliza-\ntion in Reinforcement Learning. In 9th International Con-\nference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021.\nChevalier-Boisvert, M.; Bahdanau, D.; Lahlou, S.; Willems,\nL.; Saharia, C.; Nguyen, T. H.; and Bengio, Y . 2019.\nBabyAI: A Platform to Study the Sample Efficiency of\nGrounded Language Learning. In 7th International Con-\nference on Learning Representations, ICLR 2019, New Or-\nleans, LA, USA, May 6-9, 2019.\nDriess, D.; Xia, F.; Sajjadi, M. S. M.; Lynch, C.; Chowdh-\nery, A.; Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q. H.;\nYu, T.; Huang, W.; Chebotar, Y .; Sermanet, P.; Duck-\nworth, D.; Levine, S.; Vanhoucke, V .; Hausman, K.; Tous-\nsaint, M.; Greff, K.; Zeng, A.; Mordatch, I.; and Florence,\nP. R. 2023. PaLM-E: An Embodied Multimodal Language\nModel. ArXiv, abs/2303.03378.\nGarg, D.; Vaidyanath, S.; Kim, K.; Song, J.; and Ermon, S.\n2022. LISA: Learning Interpretable Skill Abstractions from\nLanguage. ArXiv, abs/2203.00054.\nHafner, D. 2021. Benchmarking the Spectrum of Agent Ca-\npabilities. arXiv preprint arXiv:2109.06780.\nHuang, D.; Nan, Z.; Hu, X.; Jin, P.; Peng, S.; Wen, Y .; Zhang,\nR.; Du, Z.; Guo, Q.; Pu, Y .; and Chen, Y . 2023. ANPL:\nTowards Natural Programming with Interactive Decomposi-\ntion.\nHuang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Florence,\nP.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar, Y .; Ser-\nmanet, P.; Jackson, T.; Brown, N.; Luu, L.; Levine, S.; Haus-\nman, K.; and Ichter, B. 2022. Inner Monologue: Embod-\nied Reasoning through Planning with Language Models. In\nConference on Robot Learning, CoRL 2022, 14-18 Decem-\nber 2022, Auckland, New Zealand, 1769–1782.\nIchter, B.; Brohan, A.; Chebotar, Y .; Finn, C.; Hausman, K.;\nHerzog, A.; Ho, D.; Ibarz, J.; Irpan, A.; Jang, E.; Julian, R.;\nKalashnikov, D.; Levine, S.; Lu, Y .; Parada, C.; Rao, K.; Ser-\nmanet, P.; Toshev, A.; Vanhoucke, V .; Xia, F.; Xiao, T.; Xu,\nP.; Yan, M.; Brown, N.; Ahn, M.; Cortes, O.; Sievers, N.;\nTan, C.; Xu, S.; Reyes, D.; Rettinghouse, J.; Quiambao, J.;\nPastor, P.; Luu, L.; Lee, K.; Kuang, Y .; Jesmonth, S.; Joshi,\nN. J.; Jeffrey, K.; Ruano, R. J.; Hsu, J.; Gopalakrishnan, K.;\nDavid, B.; Zeng, A.; and Fu, C. K. 2022. Do As I Can, Not\nAs I Say: Grounding Language in Robotic Affordances. In\nConference on Robot Learning, CoRL 2022, 14-18 Decem-\nber 2022, Auckland, New Zealand, 287–318.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large Language Models are Zero-Shot Reasoners.\nArXiv, abs/2205.11916.\nLi, S.; Puig, X.; Paxton, C.; Du, Y .; Wang, C.; Fan, L.;\nChen, T.; Huang, D.; Aky ¨urek, E.; Anandkumar, A.; An-\ndreas, J.; Mordatch, I.; Torralba, A.; and Zhu, Y . 2022. Pre-\nTrained Language Models for Interactive Decision-Making.\nIn NeurIPS.\nLiang, J.; Huang, W.; Xia, F.; Xu, P.; Hausman, K.; Ichter,\nB.; Florence, P.; and Zeng, A. 2022. Code as Policies:\nLanguage Model Programs for Embodied Control. CoRR,\nabs/2209.07753.\nLynch, C.; and Sermanet, P. 2021. Language Conditioned\nImitation Learning Over Unstructured Data. In Robotics:\nScience and Systems XVII, Virtual Event, July 12-16, 2021.\nMisra, D. K.; Sung, J.; Lee, K.; and Saxena, A. 2016. Tell\nme Dave: Context-sensitive grounding of natural language\nto manipulation instructions. Int. J. Robotics Res., 35(1-3):\n281–300.\nOpenAI. 2023. GPT-4 Technical Report. ArXiv,\nabs/2303.08774.\nPeng, S.; Hu, X.; Zhang, R.; Guo, J.; Yi, Q.; Chen, R.; Du,\nZ.; Li, L.; Guo, Q.; and Chen, Y . 2023. Conceptual Re-\ninforcement Learning for Language-Conditioned Tasks. In\nAAAI Conference on Artificial Intelligence.\nRaman, S. S.; Cohen, V .; Rosen, E.; Idrees, I.; Paulius, D.;\nand Tellex, S. 2022. Planning with Large Language Models\nvia Corrective Re-prompting. CoRR, abs/2211.09935.\nRanzato, M.; Beygelzimer, A.; Dauphin, Y . N.; Liang, P.;\nand Vaughan, J. W., eds. 2021.Advances in Neural Informa-\ntion Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, De-\ncember 6-14, 2021, virtual.\nR¨oder, F.; Eppe, M.; and Wermter, S. 2022. Grounding\nHindsight Instructions in Multi-Goal Reinforcement Learn-\ning for Robotics. In IEEE International Conference on De-\nvelopment and Learning, ICDL 2022, London, United King-\ndom, September 12-15, 2022, 170–177.\nWang, G.; Xie, Y .; Jiang, Y .; Mandlekar, A.; Xiao, C.;\nZhu, Y .; Fan, L.; and Anandkumar, A. 2023. V oyager: An\nOpen-Ended Embodied Agent with Large Language Mod-\nels. CoRR, abs/2305.16291.\nWang, R.; Jansen, P. A.; C ˆot´e, M.; and Ammanabrolu, P.\n2022. ScienceWorld: Is your Agent Smarter than a 5th\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n14606\nGrader? In Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing, EMNLP\n2022, Abu Dhabi, United Arab Emirates, December 7-11,\n2022, 11279–11298.\nWei, J.; Bosma, M.; Zhao, V .; Guu, K.; Yu, A. W.; Lester, B.;\nDu, N.; Dai, A. M.; and Le, Q. V . 2021. Finetuned Language\nModels Are Zero-Shot Learners. ArXiv, abs/2109.01652.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; hsin Chi,\nE. H.; Xia, F.; Le, Q.; and Zhou, D. 2022. Chain of Thought\nPrompting Elicits Reasoning in Large Language Models.\nArXiv, abs/2201.11903.\nYuan, H.; Zhang, C.; Wang, H.; Xie, F.; Cai, P.; Dong, H.;\nand Lu, Z. 2023. Plan4MC: Skill Reinforcement Learn-\ning and Planning for Open-World Minecraft Tasks. CoRR,\nabs/2303.16563.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n14607",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5856465101242065
    },
    {
      "name": "Ground",
      "score": 0.5149641633033752
    },
    {
      "name": "Natural language processing",
      "score": 0.4015035629272461
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37509042024612427
    },
    {
      "name": "Cognitive science",
      "score": 0.34125444293022156
    },
    {
      "name": "Psychology",
      "score": 0.29506129026412964
    },
    {
      "name": "Engineering",
      "score": 0.14650565385818481
    },
    {
      "name": "Electrical engineering",
      "score": 0.0738166868686676
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210128818",
      "name": "Institute of Software",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210090176",
      "name": "Institute of Computing Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    }
  ]
}