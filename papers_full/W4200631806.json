{
    "title": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression",
    "url": "https://openalex.org/W4200631806",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3042728415",
            "name": "Runxin Xu",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2625368959",
            "name": "Fuli Luo",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2108884846",
            "name": "Chengyu Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2141034123",
            "name": "Baobao Chang",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2097175565",
            "name": "Jun Huang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2159562265",
            "name": "Songfang Huang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1936961387",
            "name": "Fei Huang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3042728415",
            "name": "Runxin Xu",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2625368959",
            "name": "Fuli Luo",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2108884846",
            "name": "Chengyu Wang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2141034123",
            "name": "Baobao Chang",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2097175565",
            "name": "Jun Huang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2159562265",
            "name": "Songfang Huang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1936961387",
            "name": "Fei Huang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3047763836",
        "https://openalex.org/W3019631794",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W2805003733",
        "https://openalex.org/W3096565276",
        "https://openalex.org/W2987283559",
        "https://openalex.org/W6775706467",
        "https://openalex.org/W2974875810",
        "https://openalex.org/W2741609678",
        "https://openalex.org/W6776700526",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W2975185270",
        "https://openalex.org/W3084930274",
        "https://openalex.org/W3165500975",
        "https://openalex.org/W3093346478",
        "https://openalex.org/W3168829766",
        "https://openalex.org/W2771655537",
        "https://openalex.org/W2707890836",
        "https://openalex.org/W3022969335",
        "https://openalex.org/W6718053083",
        "https://openalex.org/W6777017071",
        "https://openalex.org/W2973061659",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2969515962",
        "https://openalex.org/W2799054028",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W2607892599",
        "https://openalex.org/W3155534120",
        "https://openalex.org/W3100980998",
        "https://openalex.org/W3106070274",
        "https://openalex.org/W4288347855",
        "https://openalex.org/W4287614078",
        "https://openalex.org/W3173210704",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W4288256350",
        "https://openalex.org/W2963828549",
        "https://openalex.org/W4287692509",
        "https://openalex.org/W3176647794",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W4287812705",
        "https://openalex.org/W2976833415",
        "https://openalex.org/W4297808394",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W4287777801",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W3168125510",
        "https://openalex.org/W3115295967",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W3015233032",
        "https://openalex.org/W3015609966",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3035281298",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3104215796",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W3103754749"
    ],
    "abstract": "Pre-trained Language Models (PLMs) have achieved great success in various Natural Language Processing (NLP) tasks under the pre-training and fine-tuning paradigm. With large quantities of parameters, PLMs are computation-intensive and resource-hungry. Hence, model pruning has been introduced to compress large-scale PLMs. However, most prior approaches only consider task-specific knowledge towards downstream tasks, but ignore the essential task-agnostic knowledge during pruning, which may cause catastrophic forgetting problem and lead to poor generalization ability. To maintain both task-agnostic and task-specific knowledge in our pruned model, we propose ContrAstive Pruning (CAP) under the paradigm of pre-training and fine-tuning. It is designed as a general framework, compatible with both structured and unstructured pruning. Unified in contrastive learn- ing, CAP enables the pruned model to learn from the pre-trained model for task-agnostic knowledge, and fine-tuned model for task-specific knowledge. Besides, to better retain the performance of the pruned model, the snapshots (i.e., the intermediate models at each pruning iteration) also serve as effective supervisions for pruning. Our extensive experiments show that adopting CAP consistently yields significant improvements, especially in extremely high sparsity scenarios. With only 3% model parameters reserved (i.e., 97% sparsity), CAP successfully achieves 99.2% and 96.3% of the original BERT performance in QQP and MNLI tasks. In addition, our probing experiments demonstrate that the model pruned by CAP tends to achieve better generalization ability.",
    "full_text": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained\nLanguage Model Compression\nRunxin Xu1*, Fuli Luo2*, Chengyu Wang2, Baobao Chang1‚Ä†,\nJun Huang2, Songfang Huang2‚Ä†, Fei Huang2\n1Key Laboratory of Computational Linguistics, Peking University, MOE, China\n2Alibaba Group\nrunxinxu@gmail.com, chbb@pku.edu.cn\n{lfl259702,chengyu.wcy,huangjun.hj, songfang.hsf,f.huang}@alibaba-inc.com\nAbstract\nPre-trained Language Models (PLMs) have achieved great\nsuccess in various Natural Language Processing (NLP) tasks\nunder the pre-training and fine-tuning paradigm. With large\nquantities of parameters, PLMs are computation-intensive\nand resource-hungry. Hence, model pruning has been intro-\nduced to compress large-scale PLMs. However, most prior\napproaches only consider task-specific knowledge towards\ndownstream tasks, but ignore the essential task-agnostic\nknowledge during pruning, which may cause catastrophic for-\ngetting problem and lead to poor generalization ability. To\nmaintain both task-agnostic and task-specific knowledge in\nour pruned model, we propose C\nontrAstive Pruning ( CAP)\nunder the paradigm of pre-training and fine-tuning. It is de-\nsigned as a general framework, compatible with both struc-\ntured and unstructured pruning. Unified in contrastive learn-\ning, CAP enables the pruned model to learn from the pre-\ntrained model for task-agnostic knowledge, and fine-tuned\nmodel for task-specific knowledge. Besides, to better retain\nthe performance of the pruned model, the snapshots (i.e., the\nintermediate models at each pruning iteration) also serve as\neffective supervisions for pruning. Our extensive experiments\nshow that adopting CAP consistently yields significant im-\nprovements, especially in extremely high sparsity scenarios.\nWith only3% model parameters reserved (i.e.,97% sparsity),\nCAP successfully achieves 99.2% and 96.3% of the original\nBERT performance in QQP and MNLI tasks. In addition, our\nprobing experiments demonstrate that the model pruned by\nCAP tends to achieve better generalization ability.\nIntroduction\nPre-trained Language Models (PLMs), such as BERT (De-\nvlin et al. 2019), have achieved great success in a variety of\nNatural Language Processing (NLP) tasks. PLMs are pre-\ntrained in a self-supervised way, and then adapted to the\ndownstream tasks through fine-tuning. Despite the success,\nPLMs are usually resource-hungry with a large number of\nparameters, ranging from millions (e.g., BERT) to billions\n(e.g., GPT-3), which leads to high memory consumption and\ncomputational overhead in practice.\n* Equal Contribution. Joint work between Alibaba and Peking\nUniversity.\n‚Ä† Corresponding authors.\nCopyright ¬© 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nStructured Pruning Unstructured Pruning \n+1.9\n+4.3\n+1.0\n+1.3\n+2.0\nFigure 1: Comparison between BERT pruning with and\nwithout CAP. We report the average score across MNLI,\nQQP, and SQuAD tasks with different model sparsity (50%,\n90%, and 97%). CAP consistently yield improvements for\ndifferent pruning criterions, with larger gains in higher spar-\nsity (1.0 ‚Üí1.3 ‚Üí2.0). Please refer to Table 2 for details.\nIn fact, recent studies have observed that PLMs are over-\nparameterized with many redundant weights (Frankle and\nCarbin 2019; Prasanna, Rogers, and Rumshisky 2020). Mo-\ntivated by this, one major line of works to compress large-\nscale PLMs and speed up the inference is model pruning,\nwhich focuses on identifying and removing those unimpor-\ntant parameters. However, when adapting the pre-trained\nmodels to downstream tasks, most studies simply adopt\nthe vanilla pruning methods, but do not make full use of\nthe paradigm of pre-training and fine-tuning. Specifically,\nmost works only pay attention to the task-specific knowl-\nedge towards the downstream task during pruning, but ig-\nnore whether the task-agnostic knowledge of the origin\nPLM is well maintained in the pruned model. Losing such\ntask-agnostic knowledge can cause severe catastrophic for-\ngetting problem (Lee, Cho, and Kang 2020; Chen et al.\n2020a), which further damages the generalization ability of\nthe pruned model. Moreover, when facing extremely high\nsparsity scenarios (e.g., 97% sparsity with only 3% param-\neters reserved), the performance of the pruned model de-\ncreases sharply compared with the original dense model.\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n11547\nIn this paper, we propose ContrAstive Pruning ( CAP),\na general pruning framework under the pre-training and\nfine-tuning paradigm. The core of CAP is to encourage the\npruned model to learn from multiple perspectives to reserve\ndifferent types of knowledge, even in extremely high spar-\nsity scenarios. We adopt contrastive learning (He et al. 2020;\nChen et al. 2020b) to achieve the above objective with three\nmodules: PrC, SnC, and FiC. These modules contrast sen-\ntence representations derived from the pruned model with\nthose from other models, so that the pruned model is able\nto learn from others and reserve corresponding representa-\ntion ability. Specifically, PrC and FiC strive to pull the rep-\nresentation from pruned model towards that from the ori-\ngin pre-trained model and fine-tuned model, to learn the\ntask-agnostic and task-specific knowledge, respectively. As\na bridging mechanism, SnC further strives to pull the rep-\nresentation from pruned model towards that from the in-\ntermediate models during pruning (called snapshots), to ac-\nquire historical and diversified knowledge, so that the highly\nsparse model can still maintain comparable performance.\nOur CAP framework has the following advantages: 1)\nCAP maintains both task-agnostic and task-specific knowl-\nedge in the pruned model, which helps alleviate catastrophic\nforgetting problem and maintain model performance dur-\ning pruning, especially in extremely high sparsity cases; 2)\nCAP is based on contrastive learning that is proven to be\na powerful representation learning technique; 3) CAP is a\nframework rather than a specific pruning method. Hence,\nit is orthogonal to various pruning criteria, including both\nstructured and unstructured pruning, and can be flexibly in-\ntegrated with them to offer improvements.\nCAP is conceptually general and empirically powerful.\nAs shown in Figure 1, our experiments show that by equip-\nping different pruning criteria with CAP, the average scores\nacross several tasks are consistently improved by up to 4.3\npoint, achieving the state-of-the-art performance among dif-\nferent pruning mechanisms. The improvement even grows\nlarger in higher sparsity. Our experiments also demonstrate\nthat CAP succeeds to achieve 99.2% and 96.3% of the orig-\ninal BERT performance, with only 3% model parameters in\nQQP and MNLI tasks. Through task transferring probing ex-\nperiments, we also find that the generalization ability of the\npruned model is significantly enhanced with CAP.\nBackground\nModel Compression\nPre-trained Language Models (PLMs) have achieved re-\nmarkable success in NLP community, but the demanding\nmemory and latency also greatly increase. Different com-\npression methods, such as model pruning (Han et al. 2015;\nMolchanov et al. 2017), knowledge distillation (Jiao et al.\n2020; Wang et al. 2020), quantization (Shen et al. 2020), and\nmatrix decomposition (Lan et al. 2020), have been proposed.\nIn this paper, we mainly focus on model pruning, which\nidentifies and removes unimportant weights of the model.\nIt can be divided into two categories, that is, unstructured\npruning that prunes individual weights, and structured prun-\ning that prunes structured blocks of weights.\nFor unstructured pruning, magnitude-based methods\nprunes weights according to their absolute values (Han et al.\n2015; Xu et al. 2021), while movement-based methods con-\nsider the change of weights during fine-tuning (Sanh, Wolf,\nand Rush 2020). In addition, Louizos, Welling, and Kingma\n(2018) use a hard-concrete distribution to exert L0-norm reg-\nularization, and Guo et al. (2019) introduce reweighted L 1-\nnorm regularization instead.\nFor structured pruning, some studies use the first-order\nTaylor expansion to calculate the importance scores of dif-\nferent heads and feed-forward networks based on the varia-\ntion in the loss if we remove them (Molchanov et al. 2017;\nMichel, Levy, and Neubig 2019; Prasanna, Rogers, and\nRumshisky 2020; Liang et al. 2021). Lin et al. (2020) prune\nmodules whose outputs are very small. Although the above\nstructured pruning methods are matrix-wise, there are also\nsome studies focusing on layer-wise (Fan, Grave, and Joulin\n2020; Sajjad et al. 2020), and row/column-wise (Khetan and\nKarnin 2020; Li et al. 2020).\nDifferent pruning methods can be applied in a one-shot\n(prune for once) way, or iteratively (prune step by step) that\nwe use in this paper. However, most of the prior methods\nonly consider task-specific knowledge of downstream tasks,\nbut neglect to reserve task-agnostic knowledge in the pruned\nmodel, which leads to catastrophic forgetting problem.\nContrastive Learning\nContrastive learning serves as an effective mechanism for\nrepresentation learning. With similar instances considered as\npositive examples, and dissimilar instances as negative ones,\ncontrastive learning aims at pulling positive examples close\ntogether and pushing negative examples apart, which usually\nuses InfoNCE loss (van den Oord, Li, and Vinyals 2018). He\net al. (2020) and Chen et al. (2020b) propose self-supervised\ncontrastive learning in computer vision, with different views\nof the figure being positive examples, and different figures\nbeing negative examples. It is also successfully introduced to\nNLP community, such as sentence representation (Wu et al.\n2020; Gao, Yao, and Chen 2021), text summarization (Liu\nand Liu 2021), and so on. In order to take advantage of anno-\ntated labels of the data, some studies extend the contrastive\nlearning in a supervised way with an arbitrary number of\npositive examples (Khosla et al. 2020; Gunel et al. 2021).\nFormally, suppose that we have an example xi and it is\nencoded into a vector representation zi = œï(xi) ‚ààRd by\nmodel œï. Besides, there are also N examples being encoded\ninto S= {ÀÜzj}N\nj=1, which are used to contrast with zi. Sup-\npose there is one or multiple positive examples ÀÜzp ‚ààS and\nthe others S\\{ÀÜzp}are negative examples towards zi. Fol-\nlowing Khosla et al. (2020), the contrastive training objec-\ntive for example xi is defined as follows:\nLi = ‚àí 1\n‚à•P(i)‚à•\nX\nÀÜzj ‚ààP(i)\nlog esim(zi,ÀÜzj )/œÑ\nPN\nk=1 esim(zi,ÀÜzk)/œÑ\n(1)\nwhere P(i) ‚äÇS refers to the positive examples set for zi,\nsim(zi, zj) = z‚ä§\ni zj\n‚à•zi‚à•‚à•zj ‚à• refers to the cosine similarity func-\ntion, and œÑ denotes the temperature hyperparameter.\n11548\nPre-trained Model -\nFine-tuned Model -\nInput Tensor\nIterative\nPruning\nùúôùëùùëüùëí\nùúôùëìùëñùëõùëí\nùúô60\nùúô20\nùúô40\nùúôùëùùëüùëí(ùë•ùëñ)\nùúô20(ùë•ùëñ)\nùúô40(ùë•ùëñ)\nùúô60(ùë•ùëñ)\nùúôùëìùëñùëõùëí(ùë•ùëñ)\nPrC\nSnC\nFiC\nSnapshots - ùúôùëü‚Ä≤\nFigure 2: Overview ofCAP framework, which prunes model\nstep by step (œïpre ‚Üíœï20 ‚Üíœï40 ‚Üíœï60), where the num-\nber denotes the sparsity ratio (%). Overall, CAP consists of\nthree contrastive modules: PrC, SnC, and FiC. PrC (green\nlines): contrastive learning with the pre-trained model œïpre\nto maintain task-agnostic knowledge. SnC (yellow lines):\ncontrastive learning with snapshots œïr‚Ä≤ to bridge the gap be-\ntween pre-trained model and current pruned model, and gain\nhistoric and diversified knowledge. FiC (blue lines): con-\ntrastive learning with the fine-tuned model œïfine to gain the\ntask-specific knowledge. The solid lines indicate the learn-\ning of the current pruned model œï60, while the dashed lines\ndenote the learning of previous snapshots, œï20 and œï40.\nMethodology\nIn this paper, we propose a general pruning framework,\nContrAstive Pruning (CAP), which prunes model via super-\nvisions from pre-trained and fine-tuned models, and snap-\nshots during pruning to gain different types of knowledge.\nFollowing iterative pruning, we compress the pre-trained\nmodel œïpre to expected sparsity ratio R% progressively\n(œïpre ‚Üíœïr1 ‚Üíœïr2 ‚Üí¬∑¬∑¬∑‚Üí œïR), and arbitrary prun-\ning criteria can be used at each step. Figure 2 illustrates the\noverview of CAP that consists of three modules: PrC, SnC,\nand FiC. They are all based on contrastive learning, with dif-\nferent ways to construct positive examples shown in Table 1.\nPrC: Contrastive Learning with Pre-trained Model\nOn the transfer learning towards a specific downstream task,\nthe task-agnostic knowledge in the original PLM is inclined\nto be lost, which can cause catastrophic forgetting prob-\nlem. Hence, in this section, we introduce a PrC module to\nmaintain such general-purpose language knowledge based\non contrastive learning (green lines in Figure 2).\nSuppose that example xi is encoded into zi = œïr(xi) ‚àà\nRd by model œïr with r% sparsity ratio. The high-level idea\nis that we can contrast zi with {ÀÜzj = œïpre(xj)}N\nj=1 encoded\nby pre-trained model œïpre and enforce the model to cor-\nModule Supervised Unsupervised\nPrC {œïpre(xj) | yj = yi} { œïpre(xi)}\nSnC {œïr‚Ä≤(xj) | yj = yi, r‚Ä≤ < r} { œïr‚Ä≤(xi) | r‚Ä≤ < r}\nFiC {œïfine (xj) | yj = yi} { œïfine (xi)}\nTable 1: Positive examples P(i) in Eq. 1 for œïr(xi) in three\ncontrastive modules of CAP. œïr(xi) refers to representation\nof xi encoded by model œïr with r% sparsity ratio, and yi\ndenotes the annotated label for xi.\nrectly identify those semantically similar (positive) exam-\nples. In this way, the current pruned model œïr is able to\nmimic the representation modeling ability of the pre-trained\nmodel, and therefore maintain task-agnostic knowledge.\nSpecifically, we adopt contrastive learning in both un-\nsupervised and supervised settings. For unsupervised PrC,\nœïpre(xi) is considered as a positive example for œïr(xi),\nand {œïpre(xj)}jÃ∏=i are negative examples. The loss LPrC\nunsup\nis then calculated following Eq. 1. For supervised PrC, we\nfurther utilize the sentence-level annotations of the data. For\nexample, the sentences are labeled as entailment, neutral, or\ncontradiction in the MNLI task. Intuitively, we treat those\nhaving the same labels with xi as positive examples since\nthey share similar semantic features, and the others as neg-\native ones. Formally, we define the positive examples set as\n{œïpre(xj) |yj = yi}, where yi denotes the label ofxi. Then\nthe supervised loss LPrC\nsup is calculated as Eq. 1. Therefore,\nthe final training objective forPrC is LPrC = LPrC\nunsup +LPrC\nsup.\nSnC: Contrastive Learning with Snapshots\nPruning can be applied in a one-shot or iterative way. One-\nshot pruning drops out weights and retrains the model for\nonce. In contrast, iterative pruning removes weights step by\nstep, until reaching the expected sparsity, and the intermedi-\nate models at each pruning iterations are called snapshots.\nIn this paper, we adopt iterative pruning since it better suits\nhigh sparsity regimes. However, different from prior studies\nthat simply ignore these snapshots, we propose SnC to en-\nable the current pruned model to learn from these snapshots\nbased on contrastive learning (yellow lines in Figure 2).\nIn detail, suppose that we prune the model to r% spar-\nsity ratio progressively (œï pre ‚Üí œïr1 ‚Üí œïr2 ‚Üí ¬∑¬∑¬∑ ‚Üí\nœïr), and {œïr‚Ä≤}r‚Ä≤<r = {œïr1, œïr2, . . .}are snapshots. In-\ntuitively, these snapshots can bridge the gap between the\nsparse model (œïr) and dense models (œïpre, œïfine ), and pro-\nvide diversified supervisions with different sparse structures.\nUnder unsupervised settings, for example xi encoded into\nœïr(xi) ‚àà Rd by the current pruned model œïr, we treat\nthe representations encoded from the same example but\nby the snapshots {œïr‚Ä≤(xi) |r‚Ä≤ < r}as positive examples,\nand {œïr‚Ä≤(xj) |j Ã∏= i, r‚Ä≤ < r}as negative ones. Under super-\nvised settings, we utilize the annotation labels to consider in-\nstances with the same labels as positive examples. We calcu-\nlate the loss forSnC following Eq. 1,LSnC = LSnC\nunsup +LSnC\nsup.\nTable 3 show that snapshots serve as effective guidance dur-\ning pruning, with 0.58 ‚àº 2.47 average gains on MNLI,\nQQP, and SQuAD, especially in high sparsity regimes.\n11549\nFiC: Contrastive Learning with Fine-tuned Model\nTo better adapt to the downstream task, the pruned model\nœïr can also learn from the fine-tuned model œïfine that con-\ntains rich task-specific knowledge. To this end, we propose\na FiC module, which conducts contrastive learning between\nthe current pruned modelœïr and the fine-tuned modelœïfine .\nIt is almost identical to the PrC module, except that the tar-\nget model œïpre is replaced with the fine-tuned model œïfine\n(blue lines in Figure 2). Accordingly, the training loss is cal-\nculated as LFiC = LFiC\nunsup +LFiC\nsup based on Eq. 1. In addition\nto the contrastive supervision signal in representation space,\nwe can also introduce distilling supervision in label space\nthrough knowledge distillation mechanism.\nPruning with CAP Framework\nPutting PrC, SnC, and FiC together, we can reach our pro-\nposed CAP framework. Note that we can flexibly integrate\nwith different pruning criteria in CAP. In this paper, we try\nout both structured and unstructured pruning criteria.\nFor structured pruning, a widely used structured pruning\ncriterion is to derive the importance score of an element\nbased on the variation towards the lossLif we remove it, us-\ning the first-order Taylor expansion (Molchanov et al. 2017)\nof the loss. We denote this method as First-order pruning\nand absorb it into CAP, which we call CAP-f.\nIw = |Lw ‚àíLw=0|‚âà\n\f\f\f\f\n‚àÇL\n‚àÇw w\n\f\f\f\n\f (2)\nFor unstructured pruning, we apply the movement-based\npruning methods (Sanh, Wolf, and Rush 2020), which cal-\nculates importance score for parameter w as follows:\nIw = ‚àí\nX\nt\n‚àÇL(t)\n‚àÇw w(t) (3)\nwhere t is the training step. Based on it, Sanh, Wolf, and\nRush (2020) reserve parameters using Top-K selection strat-\negy or a pre-defined threshold, called Movement pruning or\nSoft-movement pruning, respectively. We absorb these meth-\nods into CAP, and denote them as CAP-m and CAP-soft.\nFinally, we prune and train the model using the final ob-\njective L= Œª1LCE +Œª2LPrC +Œª3LSnC +Œª4LFiC , where\nLCE is the cross-entropy loss towards the downstream task.\nExtra Memory Overhead\nIn our proposed CAP framework, the pruned model learns\nfrom the pre-trained, snapshots, and fine-tuned models.\nHowever, there is unnecessary to load all of these models\nin GPU, which can lead to large GPU memory overhead.\nIn fact, because only the sentence representations of exam-\nples are required for Eq. 1 in contrastive learning, and they\nalso do not back-propagate the gradients, we can simply pre-\nencode the examples and store them in CPU. When a nor-\nmal input batch arrives, we fetch N pre-encoded examples\nfor contrastive learning. In our paper, we use N = 4096 by\ndefault, and the dimensions of the sentence representation\nfor BERTbase are 768. Therefore, the extra GPU memory\noverhead is 4096 √ó768 = 3. 15M in total, and only takes\nup 3.15M / 110M = 2.86% of the memory consumption of\nBERTbase, which is low enough and acceptable.\nExperiments\nDatasets\nWe conduct experiments on various tasks to illustrate the\neffectiveness of CAP, including a) MNLI, the Multi-Genre\nNatural Language Inference Corpus (Williams, Nangia, and\nBowman 2018), a natural language inference task with in-\ndomain test set (MNLI-m), and cross-domain one (MNLI-\nmm). b) QQP, the Quora Question Pairs dataset (Wang\net al. 2019), a pairwise semantic equivalence task. c)\nSST-2, the Stanford Sentiment Treebank (Socher et al.\n2013), a sentiment classification task for an individual sen-\ntence. d) SQuAD v1.1, the Stanford Question Answering\nDataset (Rajpurkar et al. 2016), an extractive question an-\nswering task with crowdsourced question-answer pairs. Fol-\nlowing most prior works (Lin et al. 2020; Sanh, Wolf, and\nRush 2020), we report results for the dev sets. The detailed\nstatistics and the metrics are provided in Appendix.\nExperiment Setups\nWe conduct experiments based on BERT base (Devlin et al.\n2019) with 110M parameters, and follow their settings\nunless noted otherwise. Following Sanh, Wolf, and Rush\n(2020), we prune and report the sparsity ratio based on the\nweights of the encoder. For CAP-f, we prune 10% param-\neters each step and retrain the model to recover the per-\nformance until reaching the expected sparsity. For CAP-m\nand CAP-soft, we follow the cubic sparsity scheduling with\ncool-down strategy and hyperparameter settings the same\nas Sanh, Wolf, and Rush (2020). The number of exam-\nples for contrastive learning is N = 4096 . We use the fi-\nnal hidden state of [CLS] as the sentence representation,\nwhich is shown to be slightly better than mean pooling in our\nexploration experiments. We search the temperature from\nœÑ = {0.05, 0.1, 0.2, 0.3}. 1\nMain Results\nIn this section, we compare CAP with the following\nmodel compression methods: 1) knowledge distillation:\nDistillBERT (Sanh et al. 2019), BERT-PKD (Sun et al.\n2019), TinyBERT (Jiao et al. 2020), and MiniLM (Wang\net al. 2020). Note that we report the results of Tiny-\nBERT without data augmentation mechanism to ensure\nfairness. 2) structured pruning: the most standard First-\norder pruning (Molchanov et al. 2017) that CAP-f is based,\nTop-drop (Sajjad et al. 2020), SNIP (Lin et al. 2020),\nand schuBERT (Khetan and Karnin 2020). 3) unstruc-\ntured pruning: Magnitude pruning (Han et al. 2015), L 0-\nregularization (Louizos, Welling, and Kingma 2018), and\nthe state-of-the-art Movement pruning and Soft-movement\npruning (Sanh, Wolf, and Rush 2020) that our CAP-m and\nCAP-soft are based on. Please refer to the Appendix for\nmore details about the baselines. Table 2 illustrates the main\nresults, from which we have some important observations.\n(1) CAP removes a large proportion of BERT parameters\nwhile still maintaining comparable performance. With 50%\n1Our code is available at https://github.com/alibaba/AliceMind/\ntree/main/ContrastivePruning and https://github.com/PKUnlp-\nicler/ContrastivePruning.\n11550\nMethods Sparsity MNLI-m/-mm QQP ACC/F1 SST-2 SQuAD EM/F1\nBERTbase 0% 84.5 / 84.4 90.9 / 88.0 92.9 80.7 / 88.4\nKnowledge Distillation\nDistillBERT (Sanh et al. 2019) 50% 82.2 / - 88.5 / - 91.3 78.1 / 86.2\nBERT-PKD (Sun et al. 2019) 50% - / 81.0 88.9 / - 91.5 77.1 / 85.3\nTinyBERT (Jiao et al. 2020) 50% 83.5 / - 90.6 / - 91.6 79.7 / 87.5\nMiniLM (Wang et al. 2020) 50% 84.0 / - 91.0 / - 92.0 - / -\nTinyBERT (Jiao et al. 2020) 66.7% 80.5 / 81.0 - / - - 72.7 / 82.1\nStructured Pruning\nFirst-order (Molchanov et al. 2017) 50% 83.2 / 83.6 90.8 / 87.5 90.6 77.2 / 86.0\nTop-drop (Sajjad et al. 2020) 50% 81.1 / - 90.4 / - 90.3 - / -\nSNIP (Lin et al. 2020) 50% - / 82.8 88.9 / - 91.8 - / -\nschuBERT (Khetan and Karnin 2020) 50% 83.8 / - - / - 91.7 80.7 / 88.1\nCAP-f (Ours) 50% 84.5 / 85.0 91.6 / 88.6 92.7 81.4 / 88.7\nSNIP (Lin et al. 2020) 75% - / 78.3 87.8 / - 88.4 - / -\nFirst-order (Molchanov et al. 2017) 90% 79.1 / 79.5 88.7 / 84.9 86.9 59.8 / 72.3\nCAP-f (Ours) 90% 81.0 / 81.2 90.2 / 86.9 89.7 70.2 / 80.6\nUnstructured Pruning\nMovement (Sanh, Wolf, and Rush 2020) 50% 82.5 / 82.9 91.0 / 87.8 - 79.8 / 87.6\nCAP-m (Ours) 50% 83.8 / 84.2 91.6 / 88.6 - 80.9 / 88.2\nMagnitude (Han et al. 2015) 90% 78.3 / 79.3 79.8 / 65.0 - 70.2 / 80.1\nL0-regularization (Louizos, Welling, and Kingma 2018) 90% 78.7 / 79.7 88.1 / 82.8 - 72.4 / 81.9\nMovement (Sanh, Wolf, and Rush 2020) 90% 80.1 / 80.4 89.7 / 86.2 - 75.6 / 84.3\nSoft-Movement (Sanh, Wolf, and Rush 2020) 90% 81.2 / 81.8 90.2 / 86.8 - 76.6 / 84.9\nCAP-m (Ours) 90% 81.1 / 81.8 91.6 / 87.7 - 76.5 / 85.1\nCAP-soft (Ours) 90% 82.0 / 82.9 90.7 / 87.4 - 77.1 / 85.6\nMovement (Sanh, Wolf, and Rush 2020) 97% 76.5 / 77.4 86.1 / 81.5 - 67.5 / 78.0\nSoft-Movement (Sanh, Wolf, and Rush 2020) 97% 79.5 / 80.1 89.1 / 85.5 - 72.7 / 82.3\nCAP-m (Ours) 97% 77.5 / 78.4 88.8 / 85.0 - 69.5 / 79.7\nCAP-soft (Ours) 97% 80.1 / 81.3 90.2 / 86.7 - 73.8 / 83.0\nTable 2: Comparison between CAP with other model compression methods without data augmentation. CAP consistently\nachieve the best performance under the same sparsity ratio across different tasks. With only3% of the encoder‚Äôs parameter (i.e.,\n97% sparsity), CAP-soft still reaches 99.2% and 96.3% of the original BERT performance in QQP and MNLI task, respectively.\nsparsity ratio, CAP-f achieves an equal score in MNLI-m\ncompared with origin BERT, and even improves by 0.3 ‚àº\n0.7 score for MNLI-mm, QQP, and SQuAD tasks. More im-\nportantly, with only 3% of the encoder‚Äôs parameters (i.e.,\n97% sparsity ratio), our CAP-soft successfully achieves\n99.2% and 96.3% of the original BERT performance in QQP\n(90.9 ‚Üí90.2) and MNLI-mm (84.4 ‚Üí81.3).\n(2) CAP consistently yields improvements for different\npruning criteria, along with larger gains in higher spar-\nsity. Compared with structured First-order pruning, CAP-\nf improves by 1.9(85.6 ‚Üí 87.5) and 4.1(78.7 ‚Üí 82.8)\naverage score over all tasks under 50% and 90% sparsity.\nSimilarly, compared with unstructured Movement pruning,\nas the sparsity grows by 50% ‚Üí 90% ‚Üí 97%, CAP-\nm improves 1.0(85.2 ‚Üí 86.2), 1.3(82.7 ‚Üí 84.0), and\n2.0(77.8 ‚Üí79.8) scores, which also increases accordingly.\n(3) CAP consistently outperforms other pruning methods.\nFor example, CAP-f surpasses SNIP by 2.2 and 2.7 score\nin MNLI-mm and QQP tasks under 50% sparsity. Besides,\nwith higher sparsity, the90%-sparsified CAP-f can even beat\nSNIP with 75% sparsity, with 2.9 and 2.4 higher score in the\nMNLI-mm and QQP tasks.\n(4) CAP also outperforms knowledge distillation meth-\nods. For example, compared with TinyBERT under 66.7%\nsparsity ratio, CAP-f that is under 90% sparsity ratio can\nstill surpass it by 0.5 accuracy in the MNLI-m task.\nThe above observations support our claim that CAP helps\nthe pruned model to maintain both task-agnostic and task-\nspecific knowledge and hence benefits the pruning, espe-\ncially under extremely high sparsity scenarios.\nGeneralization Ability\nDifferent from prior methods, CAP can maintain task-\nagnostic knowledge in the pruned model, and therefore\nstrengthen its generalization ability. To justify our claim, we\nfollow the task transfer probing experiments from Agha-\njanyan et al. (2021). In detail, we freeze the representations\nderived from the pruned model trained on MNLI or QQP,\n11551\n+12.67\n(79.67)\n+5.73\n(80.29)\n+2.07\n(88.19)\n+5.14\n(87.86)\nQQP QNLI SST-2 MRPC\n+5.16\n(69.80)\n-0.33\n(76.33)\n+3.90\n(82.68)\n+2.44\n(86.32)\n+8.94\n(71.09)\n+14.64\n(79.88)\n+10.78\n(84.17)\n+3.15\n(86.29)\n+6.41\n(69.30)\n+5.16\n(78.77)\n+2.18\n(81.19)\n+2.69\n(85.52)\nf‚Äì50%\nf‚Äì90%\nm‚Äì50%\nm‚Äì90%\nMNLI       X\n+9.03\n(64.32)\n+5.11\n(80.25)\n+9.18\n(81.08)\n+1.56\n(85.43)\nMNLI QNLI SST-2 MRPC\n+5.96\n(59.80)\n+4.00\n(78.35)\n+16.52\n(76.61)\n+1.20\n(83.79)\n+13.07\n(58.79)\n+6.14\n(78.33)\n+25.23\n(76.15)\n+0.64\n(83.23)\n+20.58\n(58.07)\n+28.75\n(78.25)\n+22.48\n(75.00)\n+2.33\n(83.55)\nQQP       X\nDecreaseIncrease\nFigure 3: Generalization ability probing. We transfer the\npruned model trained on MNLI (left) and QQP (right) to\ntarget tasks under 50% and 90% sparsity ratio. We report\nthe improvement brought by CAP compared with its basic\npruning method, and the absolute performance of CAP (in\nbracket). CAP yields improvement in most cases, suggesting\nbetter generalization ability of the model pruned by CAP.\nand then only train a linear classifier on top of the model\nfor another task. Besides MNLI, QQP, and SST-2 tasks we\nhave used, we also include QNLI (Wang et al. 2019) and\nMRPC (Dolan and Brockett 2005) as our target tasks2.\nThe results under50% and 90% sparsity are shown in Fig-\nure 3, where the first two rows correspond to the improve-\nment of CAP-f over First-order pruning, and the last two\nrows correspond to the improvement of CAP-m over Move-\nment pruning. Improvements are shown at each cell, with\nthe performance score of CAP in the bracket. As is shown,\nCAP yields improvements in an overwhelming majority of\ncases. For example, CAP-m outperforms Movement prun-\ning by a large margin, with up to28.75 higher score in QNLI\ntransferred from QQP task, under 90% sparsity. The signifi-\ncant improvements in task transferring experiments suggest\nCAP can better maintain the task-agnostic knowledge and\nstrengthen the generalization ability of the pruned model.\nDiscussions\nUnderstanding Different Contrastive Modules\nCAP is mainly comprised of three major contrastive mod-\nules, PrC for learning from the pre-trained model, SnC for\nlearning from snapshots, and FiC for learning from the fine-\ntuned model. To better explore their effects, we remove one\nof them from CAP at a time, and prunes model with CAP-f\nand CAP-m methods under various sparsity ratios.\nWe report the accuracy for MNLI-m, F1 for QQP, and Ex-\nact Match score for SQuAD, along with the average score\ndecrease ‚àÜ. As shown in Table 3, removing any contrastive\nmodule would cause degradation of the model. For exam-\nple, with 90% sparsity ratio, removing PrC, SnC, or FiC\nleads to 1.83 ‚àº2.59 average score reduction for CAP-f, and\n0.58 ‚àº1.07 for CAP-m. Besides, the performance degra-\ndation gets larger as the sparsity gets higher in general. This\nis especially evident for CAP-f. For instance, using CAP-\nf without PrC module, when the sparsity ratio varies from\n2We report F1 for QQP/MRPC and accuracy for other tasks.\nMethods Ratio MNLI-m QQP SQuAD ‚àÜ\nCAP-f 50% 84.48 88.49 81.37 -\n- w/o PrC -0.47 -0.50 -0.67 -0.55\n- w/o SnC -0.47 -0.57 -1.10 -0.71\n- w/o FiC -0.42 -0.63 -0.58 -0.54\nCAP-f 90% 80.98 86.92 70.16 -\n- w/o PrC -1.78 -0.87 -5.11 -2.59\n- w/o SnC -1.57 -1.72 -4.11 -2.47\n- w/o FiC -1.53 -1.31 -2.64 -1.83\nCAP-m 50% 83.29 88.28 80.40 -\n- w/o PrC -0.84 -0.22 -0.74 -0.60\n- w/o SnC -0.74 -0.36 -0.66 -0.59\n- w/o FiC -1.31 -0.77 -1.07 -1.05\nCAP-m 90% 80.53 87.12 76.20 -\n- w/o PrC -0.58 -0.47 -0.68 -0.58\n- w/o SnC -0.45 -0.78 -0.66 -0.58\n- w/o FiC -1.54 -0.73 -0.94 -1.07\nCAP-m 97% 77.30 84.70 69.47 -\n- w/o PrC -0.27 -0.23 -1.87 -0.79\n- w/o SnC -0.29 -0.52 -1.67 -0.83\n- w/o FiC -1.31 -0.34 -2.11 -1.25\nTable 3: Ablation study of different contrastive modules. ‚àÜ\nrefers to the average score reduction across all tasks. Remov-\ning any contrastive module would cause degradation of the\npruned model, especially in highly sparse regimes.\n50% to 90%, the degradation increases from 0.55 to 2.59,\nThe results demonstrate that all contrastive modules play im-\nportant roles in CAP, and have complementary advantages\nwith each other, especially in highly sparse regimes.\nUnderstanding Supervised and Unsupervised\nContrastive Objectives\nIn CAP, the same example encoded by different models\nare considered as positive examples for unsupervised con-\ntrastive learning (unsup). If the sentence-level label anno-\ntations are available, we can also conduct supervised con-\ntrastive learning (sup) by considering examples with the\nsame labels as positive examples. To explore their effects,\nwe remove either of them for the ablation study.\nOur experiments 3 demonstrates that without either su-\npervised or unsupervised contrastive learning objectives, the\nperformance of the pruned model would markedly decline.\nSpecifically, removing the supervised contrastive learning\nobjective leads to 0.53 ‚àº0.91 average score decrease for\nCAP, while abandoning the unsupervised one also causes\n0.49 ‚àº 1.25 decrease. It suggests that both supervised\nand unsupervised objectives are essential and necessary for\nCAP, and their advantages are orthogonal to each other.\nPerformance under Various Sparsity Ratios\nIn this section, we compare CAP-f and CAP-m with their\nbasic pruning methods, First-order pruning and Movement\n3Due to limited space, you can find the experimental results in\nour arXiv version https://arxiv.org/abs/2112.07198.\n11552\n50 60 70 80 90 100\nSparsity Ratio (%)\n82\n83\n84\n85\n86\n87\n88F1\nFirst-order\nCAP-f\nMovement\nCAP-m\n(a) QQP\n50 60 70 80 90 100\nSparsity Ratio (%)\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5F1\nFirst-order\nCAP-f\nMovement\nCAP-m (b) SQuAD\nFigure 4: Comparison between CAP and its basic pruning\nmethods. CAP-f consistently outperforms First-order prun-\ning, and CAP-m also surpasses Movement pruning under\nvarious sparsity ratios.\n0.05 0.1 0.3 0.5 0.7\nTemperature (œÑ)\n85.0\n85.5\n86.0\n86.5\n87.0\n87.5\n88.0\n88.5F1\ncls-50%\nmean-50%\ncls-90%\nmean-90%\n(a) CAP-f\n0.05 0.1 0.3 0.5 0.7\nTemperature (œÑ)\n86.0\n86.5\n87.0\n87.5\n88.0\n88.5F1\ncls-50%\nmean-50%\ncls-90%\nmean-90% (b) CAP-m\nFigure 5: Performance of different pooling methods and\ntemperatures. The horizontal dashed line denotes the perfor-\nmance of First-order and Movement pruning under50% and\n90% sparsity. In general, using [CLS] as sentence represen-\ntations slightly outperforms mean pooling, and temperature\nœÑ = 0.1 tends to achieve the best results.\npruning, with sparsity ratios varying from 50% to 97%. The\nevolution of the performance in QQP and SQuAD task is\nshown in Figure 4. The performance of the pruned model for\nall methods decreases as the sparsity ratio increases. How-\never, we can observe that CAP-f consistently outperforms\nFirst-order pruning, and the improvement is even larger in\nhigher sparsity situations. A similar tendency also exists be-\ntween CAP-m and Movement pruning. These results sug-\ngest that with three core contrastive modules, CAP can bet-\nter maintain the model performance during the pruning.\nExploration of Pooling Methods and Temperatures\nThe contrastive loss function in Eq. 1 involves two important\npoints, the sentence representation zi and the temperature\nœÑ. For sentence representation zi, we explore two pooling\nmethods of the hidden states encoded by the model, using\nthe vector representation of the [CLS] or the mean pooling\nof all representations of the whole sentence. For temperature\nœÑ, we also explore different values ranging from0.05 to 0.7.\nWe conduct experiments on QQP. As shown in Figure 5, us-\ning [CLS] as sentence representations slightly outperforms\nthe mean pooling method. Besides, CAP can achieve bet-\nter performance than its basic pruning method under most\nMethods QQP SQuAD\n50% 90% 50% 90%\nMovement 87.80 86.20 87.58 84.29\n- w/o KD 87.30 83.20 83.16 81.72\nCAP-f 88.58 86.92 88.73 80.59\n- w/o KD 88.59 86.88 86.52 77.76\nCAP-m 88.62 87.65 88.22 85.06\n- w/o KD 88.60 87.67 85.94 82.46\nTable 4: Exploring of learning from the fine-tuned model.\nFor CAP, KD brings improvements for token-level task\n(SQuAD), but has little effect on sentence-level task (QQP).\ntemperatures, and setting œÑ = 0.1 tends to achieve the best\nperformance in most cases.\nExploration of Learning From Fine-tuned Model\nTo gain task-specific knowledge, we propose to perform\ncontrastive learning on the sentence representations from the\nfine-tuned model (FiC). Another common way is to perform\nthe knowledge distillation (KD) on the soft label that has\nalready been shown effective in pruning (Sanh, Wolf, and\nRush 2020; Hou et al. 2020). To explore the effect of KD,\nwe conduct further experiments in Table 4. It shows that KD\nboosts the performance of CAP in token-level task (SQuAD)\nwhile has little effect on sentence-level task (QQP). The\nreason can be that the contrastive learning on the sentence\nrepresentations is sufficient to capture the features of the\nsentence-level task, while the information still incurs losses\non token-level tasks. Thus, for token-level tasks, it is better\nto perform CAP with KD.\nConclusion\nIn this paper, we propose a general pruning framework,\nC\nontrAstive Pruning ( CAP), under the paradigm of pre-\ntraining and fine-tuning. Based on contrastive learning, we\nenhance the pruned model to maintain both task-agnostic\nand task-specific knowledge via pulling it towards the rep-\nresentations from the pre-trained modelœïpre, and fine-tuned\nmodel œïfine . Furthermore, the snapshots during the pruning\nprocess are also fully utilized to provide historic and diver-\nsified supervisions to retain the performance of the pruned\nmodel, especially in high sparsity regimes.CAP consistently\nyields significant improvements to different pruning criteria,\nand achieves the state-of-the-art performance among differ-\nent pruning mechanisms. Experiments also show that CAP\nstrengthen the generalization ability of the pruned model.\nAcknowledgments\nThis paper is supported by the National Key Research\nand Development Program of China under Grant No.\n2020AAA0106700, the National Science Foundation of\nChina under Grant No.61936012 and 61876004.\n11553\nReferences\nAghajanyan, A.; Shrivastava, A.; Gupta, A.; Goyal, N.;\nZettlemoyer, L.; and Gupta, S. 2021. Better Fine-Tuning by\nReducing Representational Collapse. In International Con-\nference on Learning Representations (ICLR).\nChen, S.; Hou, Y .; Cui, Y .; Che, W.; Liu, T.; and Yu, X.\n2020a. Recall and Learn: Fine-tuning Deep Pretrained Lan-\nguage Models with Less Forgetting. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020b.\nA Simple Framework for Contrastive Learning of Visual\nRepresentations. In Proceedings of the 37th International\nConference on Machine Learning (ICML).\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics (NAACL).\nDolan, W. B.; and Brockett, C. 2005. Automatically Con-\nstructing a Corpus of Sentential Paraphrases. In Proceed-\nings of the Third International Workshop on Paraphrasing\n(IWP).\nFan, A.; Grave, E.; and Joulin, A. 2020. Reducing Trans-\nformer Depth on Demand with Structured Dropout. InInter-\nnational Conference on Learning Representations (ICLR).\nFrankle, J.; and Carbin, M. 2019. The Lottery Ticket\nHypothesis: Finding Sparse, Trainable Neural Networks.\nIn International Conference on Learning Representations\n(ICLR).\nGao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Sim-\nple Contrastive Learning of Sentence Embeddings. arXiv,\narXiv:2104.08821.\nGunel, B.; Du, J.; Conneau, A.; and Stoyanov, V . 2021.\nSupervised Contrastive Learning for Pre-trained Language\nModel Fine-tuning. In International Conference on Learn-\ning Representations (ICLR).\nGuo, F.; Liu, S.; Mungall, F. S.; Lin, X.; and Wang, Y .\n2019. Reweighted Proximal Pruning for Large-Scale Lan-\nguage Representation. arXiv, arXiv:1909.12486.\nHan, S.; Pool, J.; Tran, J.; and Dally, W. 2015. Learn-\ning both Weights and Connections for Efficient Neural Net-\nwork. In Advances in Neural Information Processing Sys-\ntems (NeurIPS).\nHe, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. 2020.\nMomentum Contrast for Unsupervised Visual Representa-\ntion Learning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR).\nHou, L.; Huang, Z.; Shang, L.; Jiang, X.; Chen, X.; and Liu,\nQ. 2020. DynaBERT: Dynamic BERT with Adaptive Width\nand Depth. In Advances in Neural Information Processing\nSystems (NeurIPS).\nJiao, X.; Yin, Y .; Shang, L.; Jiang, X.; Chen, X.; Li, L.;\nWang, F.; and Liu, Q. 2020. TinyBERT: Distilling BERT\nfor Natural Language Understanding. In Findings of the As-\nsociation for Computational Linguistics: EMNLP 2020.\nKhetan, A.; and Karnin, Z. 2020. schuBERT: Optimizing El-\nements of BERT. InProceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics (ACL).\nKhosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y .; Isola,\nP.; Maschinot, A.; Liu, C.; and Krishnan, D. 2020. Super-\nvised Contrastive Learning. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS).\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.;\nand Soricut, R. 2020. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representations. In Inter-\nnational Conference on Learning Representations (ICLR).\nLee, C.; Cho, K.; and Kang, W. 2020. Mixout: Effective\nRegularization to Finetune Large-scale Pretrained Language\nModels. In International Conference on Learning Represen-\ntations (ICLR).\nLi, B.; Kong, Z.; Zhang, T.; Li, J.; Li, Z.; Liu, H.; and Ding,\nC. 2020. Efficient Transformer-based Large Scale Language\nRepresentations using Hardware-friendly Block Structured\nPruning. In Findings of the Association for Computational\nLinguistics: EMNLP 2020.\nLiang, C.; Zuo, S.; Chen, M.; Jiang, H.; Liu, X.; He, P.;\nZhao, T.; and Chen, W. 2021. Super Tickets in Pre-Trained\nLanguage Models: From Model Compression to Improving\nGeneralization. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language\nProcessing (ACL).\nLin, Z.; Liu, J.; Yang, Z.; Hua, N.; and Roth, D. 2020.\nPruning Redundant Mappings in Transformer Models via\nSpectral-Normalized Identity Prior. In Findings of the As-\nsociation for Computational Linguistics: EMNLP 2020.\nLiu, Y .; and Liu, P. 2021. SimCLS: A Simple Framework for\nContrastive Learning of Abstractive Summarization. InPro-\nceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint\nConference on Natural Language Processing (ACL).\nLouizos, C.; Welling, M.; and Kingma, D. P. 2018. Learn-\ning Sparse Neural Networks through L 0 Regularization.\nIn International Conference on Learning Representations\n(ICLR).\nMichel, P.; Levy, O.; and Neubig, G. 2019. Are Sixteen\nHeads Really Better than One? In Advances in Neural In-\nformation Processing Systems (NeurIPS).\nMolchanov, P.; Tyree, S.; Karras, T.; Aila, T.; and Kautz, J.\n2017. Pruning Convolutional Neural Networks for Resource\nEfficient Inference. In International Conference on Learn-\ning Representations (ICLR).\nPrasanna, S.; Rogers, A.; and Rumshisky, A. 2020. When\nBERT Plays the Lottery, All Tickets Are Winning. In Pro-\nceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension\nof Text. In Proceedings of the 2016 Conference on Em-\npirical Methods in Natural Language Processing (EMNLP),\n2383‚Äì2392.\n11554\nSajjad, H.; Dalvi, F.; Durrani, N.; and Nakov, P. 2020.\nPoor Man‚Äôs BERT: Smaller and Faster Transformer Models.\narXiv, abXiv:2004.03844.\nSanh, V .; Debut, L.; Chaumond, J.; and Wolf, T. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. arXiv, arXiv:1910.01108.\nSanh, V .; Wolf, T.; and Rush, A. 2020. Movement Pruning:\nAdaptive Sparsity by Fine-Tuning. In Advances in Neural\nInformation Processing Systems (NeurIPS).\nShen, S.; Dong, Z.; Ye, J.; Ma, L.; Yao, Z.; Gholami, A.;\nMahoney, M. W.; and Keutzer, K. 2020. Q-BERT: Hessian\nBased Ultra Low Precision Quantization of BERT. Pro-\nceedings of the AAAI Conference on Artificial Intelligence\n(AAAI).\nSocher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,\nC. D.; Ng, A.; and Potts, C. 2013. Recursive Deep Models\nfor Semantic Compositionality Over a Sentiment Treebank.\nIn Proceedings of the 2013 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\nSun, S.; Cheng, Y .; Gan, Z.; and Liu, J. 2019. Patient Knowl-\nedge Distillation for BERT Model Compression. In Pro-\nceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nvan den Oord, A.; Li, Y .; and Vinyals, O. 2018. Represen-\ntation Learning with Contrastive Predictive Coding. arXiv,\narXiv:1807.03748.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019. GLUE: A Multi-Task Benchmark\nand Analysis Platform for Natural Language Understand-\ning. In International Conference on Learning Representa-\ntions (ICLR).\nWang, W.; Wei, F.; Dong, L.; Bao, H.; Yang, N.; and\nZhou, M. 2020. MiniLM: Deep Self-Attention Distillation\nfor Task-Agnostic Compression of Pre-Trained Transform-\ners. In Advances in Neural Information Processing Systems\n(NeurIPS).\nWilliams, A.; Nangia, N.; and Bowman, S. 2018. A Broad-\nCoverage Challenge Corpus for Sentence Understanding\nthrough Inference. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Com-\nputational Linguistics (NAACL).\nWu, Z.; Wang, S.; Gu, J.; Khabsa, M.; Sun, F.; and Ma, H.\n2020. CLEAR: Contrastive Learning for Sentence Repre-\nsentation. arXiv, arXiv:2012.15466.\nXu, D.; Yen, I. E.-H.; Zhao, J.; and Xiao, Z. 2021. Rethink-\ning Network Pruning ‚Äì under the Pre-train and Fine-tune\nParadigm. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics (NAACL).\n11555"
}