{
    "title": "Better Sign Language Translation with STMC-Transformer",
    "url": "https://openalex.org/W3097571943",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4223111006",
            "name": "Yin, Kayo",
            "affiliations": [
                "Foundation for Human Potential"
            ]
        },
        {
            "id": "https://openalex.org/A2750103225",
            "name": "Read, Jesse",
            "affiliations": [
                "Amtrak (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2471695703",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W1988322615",
        "https://openalex.org/W2739151452",
        "https://openalex.org/W3003261425",
        "https://openalex.org/W2963212250",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2796108585",
        "https://openalex.org/W2146221819",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W1538839500",
        "https://openalex.org/W159724101",
        "https://openalex.org/W2154790436",
        "https://openalex.org/W2746301562",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2997931247",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W1972034101",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W2742124155",
        "https://openalex.org/W1506441995",
        "https://openalex.org/W2122399640",
        "https://openalex.org/W2759302818",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W2125834953",
        "https://openalex.org/W2964085268",
        "https://openalex.org/W2755802490",
        "https://openalex.org/W2172140247",
        "https://openalex.org/W2100526149",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2318997121",
        "https://openalex.org/W2140273971",
        "https://openalex.org/W1969724184",
        "https://openalex.org/W2964253156",
        "https://openalex.org/W1639784222",
        "https://openalex.org/W2799020610",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2017030536",
        "https://openalex.org/W328363538",
        "https://openalex.org/W196736872",
        "https://openalex.org/W2954798773",
        "https://openalex.org/W2540404261",
        "https://openalex.org/W2342890612",
        "https://openalex.org/W2046667777",
        "https://openalex.org/W2252218224",
        "https://openalex.org/W2052696978",
        "https://openalex.org/W2734715902",
        "https://openalex.org/W2953333557",
        "https://openalex.org/W3034765865",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2096976709",
        "https://openalex.org/W2463640844",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W2107603586",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2188882108",
        "https://openalex.org/W2034386547",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W1753482797",
        "https://openalex.org/W3016918797",
        "https://openalex.org/W2015551990"
    ],
    "abstract": "Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR) system to extract sign language glosses from videos. Then, a translation system generates spoken language translations from the sign language glosses. This paper focuses on the translation system and introduces the STMC-Transformer which improves on the current state-of-the-art by over 5 and 7 BLEU respectively on gloss-to-text and video-to-text translation of the PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase of over 16 BLEU. We also demonstrate the problem in current methods that rely on gloss supervision. The video-to-text translation of our STMC-Transformer outperforms translation of GT glosses. This contradicts previous claims that GT gloss translation acts as an upper bound for SLT performance and reveals that glosses are an inefficient representation of sign language. For future SLT research, we therefore suggest an end-to-end training of the recognition and translation models, or using a different sign language annotation scheme.",
    "full_text": "Better Sign Language Translation with STMC-Transformer\nKayo Yin∗\nLanguage Technologies Institute\nCarnegie Mellon University\nkayo@cmu.edu\nJesse Read\nLIX, Ecole Polytechnique\nInstitut Polytechnique de Paris\njesse.read@polytechnique.edu\nAbstract\nSign Language Translation (SLT) ﬁrst uses a Sign Language Recognition (SLR) system to ex-\ntract sign language glosses from videos. Then, a translation system generates spoken language\ntranslations from the sign language glosses. This paper focuses on the translation system and\nintroduces the STMC-Transformer which improves on the current state-of-the-art by over 5 and\n7 BLEU respectively on gloss-to-text and video-to-text translation of the PHOENIX-Weather\n2014T dataset. On the ASLG-PC12 corpus, we report an increase of over 16 BLEU.\nWe also demonstrate the problem in current methods that rely on gloss supervision. The video-\nto-text translation of our STMC-Transformer outperforms translation of GT glosses. This contra-\ndicts previous claims that GT gloss translation acts as an upper bound for SLT performance and\nreveals that glosses are an inefﬁcient representation of sign language. For future SLT research,\nwe therefore suggest an end-to-end training of the recognition and translation models, or using a\ndifferent sign language annotation scheme.\n1 Introduction\nCommunication holds a central position in our daily lives and social interactions. Yet, in a predominantly\naural society, sign language users are often deprived of effective communication. Deaf people face daily\nissues of social isolation and miscommunication to this day (Souza et al., 2017). This paper is motivated\nto provide assistive technology that allow Deaf people to communicate in their own language.\nIn general, sign languages developed independently of spoken language and do not share the grammar\nof their spoken counterparts (Stokoe, 1960). For this, Sign Language Recognition (SLR) systems on\ntheir own cannot capture the underlying grammar and complexities of sign language, and Sign Language\nTranslation (SLT) faces the additional challenge of taking into account the unique linguistic features\nduring translation.\nFigure 1: Sign language translation pipeline1.\n∗*Work carried out while at ´Ecole Polytechnique.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\narXiv:2004.00588v2  [cs.CL]  3 Nov 2020\nAs shown in Figure 1, current SLT approaches involve two steps. First, a tokenization system generates\nglosses from sign language videos. Then, a translation system translates the recognized glosses into\nspoken language. Recent work (Orbay and Akarun, 2020; Zhou et al., 2020) has addressed the ﬁrst step,\nbut there has been none improving the translation system. This paper aims to ﬁll this research gap by\nleveraging recent success in Neural Machine Translation (NMT), namely Transformers.\nAnother limit to current SLT models is that they use glosses as an intermediate representation of sign\nlanguage. We show that having a perfect continuous SLR system will not necessarily improve SLT re-\nsults. We introduce the STMC-Transformer model performing video-to-text translation that surpasses\ntranslation of ground truth glosses, which reveals that glosses are a ﬂawed representation of sign lan-\nguage.\nThe contributions of this paper can be summarized as:\n1. A novel STMC-Transformer model for video-to-text translation surpassing GT glosses translation\ncontrary to previous assumptions\n2. The ﬁrst successful application of Transformers to SLT achieving state-of-the-art results in both\ngloss to text and video to text translation on PHOENIX-Weather 2014T and ASLG-PC12 datasets\n3. The ﬁrst usage of weight tying, transfer learning, and ensemble learning in SLT and a comprehensive\nseries of baseline results with Transformers to underpin future research\n2 Methods\nDespite considerable advancements made in machine translation (MT) between spoken languages, sign\nlanguage processing falls behind for many reasons. Unlike spoken language, sign language is a mul-\ntidimensional form of communication that relies on both manual and non-manual cues which presents\nadditional computer vision challenges (Asteriadis et al., 2012). These cues may occur simultaneously\nwhereas spoken language follows a linear pattern where words are processed one at a time. Signs also\nvary in both space and time and the number of video frames associated to a single sign is not ﬁxed either.\n2.1 Sign Language Glossing\nGlossing corresponds to transcribing sign language word-for-word by means of another written language.\nGlosses differ from translation as they merely indicate what each part in a sign language sentence mean,\nbut do not form an appropriate sentence in the spoken language. While various sign language corpus\nprojects have provided different guidelines for gloss annotation (Crasborn et al., 2007; Johnston, 2013),\nthere is no universal standard which hinders the easy exchange of data between projects and consistency\nbetween different sign language corpora. Gloss annotations are also an imprecise representation of sign\nlanguage and can lead to an information bottleneck when representing the multi-channel sign language\nby a single-dimensional stream of glosses.\n2.2 Sign Language Recognition\nSLR consists of identifying isolated single signs from videos. Continuous sign language recognition\n(CSLR) is a relatively more challenging task that identiﬁes a sequence of running glosses from a running\nvideo. Works in SLR and CSLR, however, only perform visual recognition and ignore the underlying\nlinguistic features of sign language.\n2.3 Sign Language Translation\nAs illustrated in Figure 1, the SLT system takes CSLR as a ﬁrst step to tokenize the input video into\nglosses. Then, an additional step translates the glosses into a valid sentence in the target language.\nSLT is novel and difﬁcult compared to other translation problems because it involves two steps: extract\nmeaningful features from a video of a multi-cue language accurately then generate translations from an\nintermediate gloss representation, instead of translation from the source language directly.\n1Gloss annotation from https://www.handspeak.com/translate/index.php?id=288\nFigure 2: STMC-Transformer network for SLT. PE: Positional Encoding, MHA: Multihead Attention,\nFF: Feed Forward.\n3 Related Work\n3.1 Sign Language Recognition\nEarly approaches for SLR rely on hand-crafted features (Tharwat et al., 2014; Yang, 2010) and use\nHidden Markov Models (Forster et al., 2013) or Dynamic Time Warping (Lichtenauer et al., 2008) to\nmodel sequential dependencies. More recently, 2D convolutional neural networks (2D-CNN) and 3D\nconvolutional neural networks (3D-CNN) effectively model spatio-temporal representations from sign\nlanguage videos (Cui et al., 2017; Molchanov et al., 2016).\nMost existing work on CSLR divides the task into three sub-tasks: alignment learning, single-gloss\nSLR, and sequence construction (Koller et al., 2017; Zhang et al., 2014) while others perform the task in\nan end-to-end fashion using deep learning (Huang et al., 2015; Camgoz et al., 2017).\n3.2 Sign Language Translation\nSLT was formalized in Camgoz et al. (2018) where they introduce the PHOENIX-Weather 2014T dataset\nand jointly use a 2D-CNN model to extract gloss-level features from video frames, and a seq2seq model\nto perform German sign language translation. Subsequent works on this dataset (Orbay and Akarun,\n2020; Zhou et al., 2020) all focus on improving the CSLR component in SLT. A contemporaneous paper\n(Camgoz et al., 2020) also obtains encouraging results with multi-task Transformers for both tokenization\nand translation, however their CSLR performance is sub-optimal, with a higher Word Error Rate than\nbaseline models.\nSimilar work has been done on Korean sign language by Ko et al. (2019) where they estimate human\nkeypoints to extract glosses, then use seq2seq models for translation. Arvanitis et al. (2019) use seq2seq\nmodels to translate ASL glosses of the ASLG-PC12 dataset (Othman and Jemni, 2012).\n3.3 Neural Machine Translation\nNeural Machine Translation (NMT) employs neural networks to carry out automated text translation.\nRecent methods typically use an encoder-decoder architecture, also known as seq2seq models.\nEarlier approaches use recurrent (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and con-\nvolutional networks (Kalchbrenner et al., 2016; Gehring et al., 2017) for the encoder and the decoder.\nHowever, standard seq2seq networks are unable to model long-term dependencies in large input sen-\ntences without causing an information bottleneck. To address this issue, recent works use attention\nmechanisms (Bahdanau et al., 2015; Luong et al., 2015) that calculates context-dependent alignment\nscores between encoder and decoder hidden states. Vaswani et al. (2017) introduces the Transformer, a\nseq2seq model relying on self-attention that obtains state-of-the-art results in NMT.\n4 Model architecture\nFor translation from videos to text, we propose the STMC-Transformer network illustrated in Figure 2.\n4.1 Spatial-Temporal Multi-Cue (STMC) Network\nOur work is the ﬁrst to use STMC networks (Zhou et al., 2020) for SLT. A spatial multi-cue (SMC)\nmodule with a self-contained pose estimation branch decomposes the input video into spatial features of\nmultiple visual cues (face, hand, full-frame and pose). Then, a temporal multi-cue (TMC) module with\nstacked TMC blocks and temporal pooling (TP) layers calculates temporal correlations within (inter-cue)\nand between cues (intra-cue) at different time steps, which preserves each unique cue while exploring\ntheir relation at the same time. The inter-cue and intra-cue features are each analyzed by Bi-directional\nLong Short-Term Memory (BiLSTM) (Sutskever et al., 2014) and Connectionist Temporal Classiﬁcation\n(CTC) (Graves et al., 2006) units for sequence learning and inference.\nThis architecture efﬁciently processes multiple visual cues from sign language video in collaboration\nwith each other, and achieves state-of-the-art performance on three SLR benchmarks. On the PHOENIX-\nWeather 2014T dataset, it achieves a Word Error Rate of 21.0 for the SLR task.\n4.2 Transformer\nFor translation, we train a two-layered Transformer to maximize the log-likelihood\n∑\n(xi,yi)∈D\nlog P(yi|xi,θ)\nwhere Dcontains gloss-text pairs (xi,yi).\nTwo layers, compared to six in most spoken language translation, is empirically shown to be optimal\nin Section 6.1, likely because our datasets are limited in size. We refer to the original Transformer paper\n(Vaswani et al., 2017) for more architecture details.\n5 Datasets\nGerman Sign Gloss German American Sign Gloss English\nTrain Dev Test Train Dev Test Train Dev Test Train Dev Test\nPhrases 7,096 519 642 7,096 519 642 82,709 4,000 1,000 82,709 4,000 1,000\nV ocab. 1,066 393 411 2,887 951 1,001 15782 4,323 2,150 21,600 5,634 2,609\ntot. words 67,781 3,745 4,257 99,081 6,820 7,816 862,046 41,030 10,503 975,942 46,637 11,953\ntot. OOVs – 19 22 – 57 60 – 255 83 – 369 99\nsingletons 337 – – 1,077 – – 6,133 – – 8,542 – –\nTable 1: Statistics of the RWTH-PHOENIX-Weather 2014T and ASLG-PC12 datasets. Out-of-\nvocabulary (OOV) words are those that appear in the development and testing sets, but not in the training\nset. Singletons are words that appear only once during training.\nPHOENIX-Weather 2014T (Camgoz et al., 2018)\nThis dataset is extracted from weather forecast airings of the German tv station PHOENIX. This dataset\nconsists of a parallel corpus of German sign language videos from 9 different signers, gloss-level anno-\ntations with a vocabulary of 1,066 different signs and translations into German spoken language with a\nvocabulary of 2,887 different words. It contains 7,096 training pairs, 519 development and 642 test pairs.\nASLG-PC12 (Othman and Jemni, 2012)\nThis dataset is constructed from English data of Project Gutenberg that has been transformed into ASL\nglosses following a rule-based approach. This corpus with 87,709 training pairs allows us to evaluate\nTransformers on a larger dataset, where deep learning models usually require lots of data. It also allows\nus to compare performance across different sign languages. However, the data is limited since it does\nnot contain sign language videos, and is less complex due to being created semi-automatically. We make\nour data and code publicly available2.\n2https://github.com/kayoyin/transformer-slt\n6 Experiments and Discussions\nOur models are built using PyTorch (Paszke et al., 2019) and Open-NMT (Klein et al., 2017). We\nconﬁgure Transformers with word embedding size 512, gloss level tokenization, sinusoidal positional\nencoding, 2,048 hidden units and 8 heads. For optimization, we use Adam (Kingma and Ba, 2014) with\nβ1 = 0.9,β2 = 0.998, Noam learning rate schedule, 0.1 dropout, and 0.1 label smoothing.\nWe evaluate on the dev set each half-epoch and employ early stopping with patience 5. During de-\ncoding, generated ⟨unk⟩ tokens are replaced by the source token having the highest attention weight.\nThis is useful when ⟨unk⟩ symbols correspond to proper nouns that can be directly transposed between\nlanguages (Klein et al., 2017). We perform a series of experiments to ﬁnd the optimal setup for this novel\napplication. We equally experiment with various techniques often used in classic NMT to SLT such as\ntransfer learning, weight tying and ensembling to improve model performance.\nFor evaluation we use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and\nLavie, 2005). For BLEU, we report BLEU-1,2,3,4 scores and as ROUGE score we report the ROUGE-\nL F1 score. These metrics allow us to directly compare directly with previous works. METEOR is\ncalculated in addition as it demonstrates higher correlation with human evaluation than BLEU on several\nMT tasks. All reported results unless otherwise speciﬁed are averaged over 10 runs with different random\nseeds.\nWe organize our experiments into two groups:\n1. Gloss2Text (G2T) in which we translate GT gloss annotations to simulate perfect tokenization on\nboth PHOENIX-Weather 2014T and ASLG-PC12\n2. Sign2Gloss2Text (S2G2T) where we perform video-to-text translation on PHOENIX-Weather\n2014T with the STMC-Transformer\n6.1 Gloss2Text (G2T)\nG2T is a text-to-text translation task that is novel and challenging compared to classic translation tasks\nbetween spoken languages because of the high linguistic variance between source and target sentences,\nscarcity of resources, and information loss or imprecision in the source sentence itself.\nFor ASLG-PC12, many ASL glosses are English words with an added preﬁx so during data pre-\nprocessing we remove all such preﬁxes. We also set all glosses that appear less than 5 times during\ntraining as ⟨unk⟩ to reduce vocabulary size.\nRaw data Preprocessed data\nTrain Dev Test Train Dev Test\nASL en ASL en ASL en ASL en ASL en ASL en\nV ocab. 15,782 21,600 4,323 5,634 2,150 2,609 5,906 7,712 1,163 1,254 394 379\nShared vocab. 10,048 2,652 1,296 4,941 899 287\nBLEU-4 20.97 21.16 20.63 38.87 38.74 38.37\nTable 2: Statistics of the ASLG-PC12 dataset before and after preprocessing.\nTable 2 shows that the source and target corpora in ASLG-PC12 are more similar to each other with\nmany shared vocabulary and a relatively high BLEU-4 score on raw data. This allows us to compare\nTransformer performance on a larger and less challenging dataset.\nModel size\nThe original Transformer in (Vaswani et al., 2017) uses 6 layers for the encoder and decoder for NMT.\nHowever, our task differs from a standard MT task between two spoken languages so we ﬁrst train\nTransformers with 1, 2, 4 and 6 encoder-decoder layers. Networks are trained with batch size 2,048 and\ninitial learning rate 1.\nDev Set Test Set\nLayers BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR\n1 43.39 32.47 24.27 20.26 44.66 42.64 43.26 32.23 25.59 21.31 45.28 42.56\n2 45.31 33.65 26.73 22.23 45.74 43.92 44.57 33.08 26.14 21.65 40.47 42.97\n4 44.32 32.87 26.15 21.78 45.86 43.31 44.10 32.82 25.99 21.57 45.44 42.92\n6 44.04 32.46 25.67 21.34 44.09 42.32 43.74 32.44 25.67 21.32 41.69 42.58\nTable 3: G2T performance comparison of Transformers on PHOENIX-Weather 2014T with different\nnumber of enc-dec layers.\nTo choose the best model, we mainly take into account BLEU-4 as it is currently the most widely used\nmetric in MT. We do ﬁnd that our ﬁnal model outperforms the other models across all metrics. Table\n3 shows that on PHOENIX-Weather 2014T, using 2 layers obtains the highest BLEU-4. Because our\ndataset is much smaller than spoken language datasets, larger networks may be disadvantaged. Moreover,\na smaller model has the advantage of taking up less memory and computation time. Repeating the same\nexperiment on ASLG-PC12, we also ﬁnd 2 layers to be the optimal model size. ASLG-PC12 is larger\nbut less complex which may also explain why smaller networks are more suitable. We carry out the rest\nof our experiments using 2 enc-dec layers.\nEmbedding schemes\nPress and Wolf (2017) shows that tying the input and output embeddings while training language models\nmay provide better performance. Our decoder is in fact a language model conditioned on the encoding of\nthe source sentence and previous outputs, we can tie the decoder embeddings by using a shared weight\nmatrix for the input and output word embeddings.\nIn addition, models are often initialized with pre-trained embeddings for transfer learning. These\nembeddings are typically trained in an unsupervised manner on a large corpus of text in the desired\nlanguage. We perform experiments on PHOENIX-Weather 2014T using two popular word embeddings:\nGloVe3 (Pennington et al., 2014), and fastText (Bojanowski et al., 2017). To the best of our knowledge,\nweight-tying or pre-trained embeddings have never been employed in SLT.\nGloVe (de) fastText (de) GloVe (en) fastText (en)\nDimension 300 300 300 300\nSource match 0.08% 0.08% 96.23% 94.64%\nTarget match 90.53% 94.57% 97.71% 96.32%\nTable 4: German and English pre-trained embeddings statistics\nTable 4 shows there is only one matching token between German glosses and the pre-trained embed-\ndings, while over 90% of the words in the German text appear in both pre-trained embeddings. We\ntherefore initialize pre-trained embeddings on the decoder only, and keep random initialization for the\nencoder. The embedding layers are ﬁne-tuned during training.\nDev Set Test Set\nWE BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR\nVanilla embedding 45.81 34.0627.05 22.49 46.68 44.35 45.29 33.74 26.70 22.22 46.08 43.75Tied decoder 45.90 34.10 26.98 22.31 46.76 44.51 45.05 33.38 26.31 21.74 45.83 43.45GloVe 44.37 32.65 26.00 21.41 45.03 42.38 44.69 32.93 25.73 21.04 42.70 44.61fastText 44.91 33.23 26.60 22.04 46.17 43.70 44.21 32.90 25.94 21.64 45.55 42.95\nTable 5: G2T performance comparison using different embedding schemes on PHOENIX-Weather\n2014T.\nTable 5 shows that the new embedding schemes do not improve performance on PHOENIX-Weather\n2014T. It may be because pre-trained embeddings are shown to be more effective when used on the\nencoding layer (Qi et al., 2018). Another possible reason is the difference between the domain of our\ndataset and of the corpus the embeddings were trained on. We therefore keep random initialization of\n3https://deepset.ai/german-word-embeddings\nword embeddings for experiments on PHOENIX-Weather 2014T. Using this setting, we run a parameter\nsearch over the learning rate and warm-up steps, and we use initial learning rate 0.5 with 3,000 warm-up\nsteps for the remaining experiments. Details of the parameter search are included in Appendix A.1.\nBoth GloVe and fastText English vectors have a reasonable overlap with the vocabulary of ASL glosses\nas well as the English targets (Table 4). Therefore on ASLG-PC12 we load pre-trained embeddings on\nonly the decoder, as well as on both the encoder and decoder.\nDev Set Test Set\nModel BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR\nVanilla embedding 90.15 84.92 80.27 75.94 94.72 94.58 90.49 85.64 81.31 77.33 94.75 95.16Tied dec 91.00 86.26 82.00 78.02 95.24 95.12 91.25 86.76 82.76 79.02 95.32 95.75GloVe dec 90.13 85.14 80.67 76.49 94.16 94.69 90.51 85.83 81.65 77.74 94.80 95.27GloVe enc-dec 89.65 84.33 79.72 75.48 93.02 93.62 90.01 85.15 80.88 76.95 93.00 94.14fastText dec 90.64 85.63 81.14 76.94 94.75 95.02 91.20 86.62 82.53 78.72 94.73 95.57fastText enc-dec 90.02 85.01 80.56 76.41 93.68 94.10 90.94 86.58 82.01 76.23 93.61 94.42fastText tied dec 90.16 85.26 80.85 76.72 95.03 94.60 90.44 85.25 81.69 77.28 95.11 95.04\nTable 6: G2T performance comparison using different embedding schemes on ASLG-PC12.\nTable 6 shows that fastText pre-trained embeddings for the decoder improves performance, and tied\ndecoder embeddings with random initialization gives the best performance. Weight tying is more suited\non this dataset likely because it acts as regularization and combats overﬁtting, while the previous dataset\nis more complex and therefore less prone to overﬁtting. For the remaining experiments, we use tied\ndecoder embeddings, initial learning rate 0.2 and 8,000 warm-up steps.\nBeam width\n1 2 3 4 5 6 7 8 9 10 15 20 30 40 50 75 10022\n22.5\n23\n23.5\n24\nBeam width\nBLEU-4\nDev set\nTest set\nFigure 3: G2T decoding on RWTH-PHEONIX-WEATHER 2014T using different beam width.\nA naive method for decoding is greedy search, where the model simply chooses the word with the\nhighest probability at each time step. However, this approach may become sub-optimal in the context of\nthe entire sequence. Beam search addresses this by expanding all possible candidates at each time step\nand keeping a number of most likely sequences, or the beam width. Large beam widths do not always\nresult in better performance and take more space in memory and decoding time. We search and ﬁnd the\noptimal beam width value to be 4 on PHOENIX-Weather 2014T and 5 on ASLG-PC12.\nEnsemble decoding\nEnsemble methods combine multiple models to improve performance. We propose ensemble decoding,\nwhere we combine the output of different models by averaging their prediction distributions. We chose\n9 models from our experiments that gave the highest BLEU-4 during testing on PHOENIX-Weather\n2014T. The number of models is chosen empirically, as using fewer models will lead to less ensembling\nbut too many weaker models may lessen the quality of the ensemble model. These models are of the\nsame architecture, but are initialized with different seeds and trained using different batch sizes and/or\nlearning rates. These models give a BLEU-4 on testing between 22.92 and 23.41 individually.\nDev Set Test Set\nModel BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR\nRaw data 13.01 6.23 3.03 1.71 24.23 13.69 11.88 5.05 2.41 1.36 22.81 12.12RNN Seq2seq (Camgoz et al., 2018) 44.40 31.93 24.61 20.16 46.02 – 44.13 31.47 23.89 19.26 45.45 –Transformer (Camgoz et al., 2020)50.69 38.16 30.53 25.35– – 48.90 36.88 29.45 24.54 – –Transformer 49.05 36.20 28.53 23.52 47.36 46.09 47.69 35.52 28.17 23.32 46.58 44.85Transformer Ens. 48.85 36.62 29.23 24.38 49.01 46.96 48.40 36.90 29.70 24.90 48.51 46.24\nTable 7: G2T on PHEONIX-WEATHER 2014T ﬁnal results.\nTable 7 gives a performance comparison on PHOENIX-Weather 2014T of the recurrent seq2seq model\nby Camgoz et al. (2018), Transformer trained concurrently by Camgoz et al. (2020), our single model,\nand ensemble model. We also provide the scores on the gloss annotations to illustrate the difﬁculty of\nthis task.\nWithout any additional training, ensembling improves testing performance by over 1 BLEU-4. Also,\nwe report an improvement of over 5 BLEU-4 on the state-of-the-art. A single Transformer also gives\nan improvement of over 4 BLEU-4 more than the state-of-the-art, which shows the advantage of Trans-\nformers for SLT, as shown also in Camgoz et al. (2020).\nDev Set Test Set\nModel BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR\nRaw data 54.60 39.67 28.92 21.16 76.11 61.25 54.19 39.26 28.44 20.63 75.59 61.65Preprocessed data 69.25 56.83 46.94 38.74 83.80 78.75 68.82 56.36 46.53 38.37 83.28 79.06Seq2seq (Arvanitis et al., 2019) – – – – – – 86.7 79.5 73.2 65.9 – –Transformer 92.98 89.09 83.55 85.63 82.41 95.93 92.98 89.09 85.63 82.41 95.87 96.46Transformer Ens. 92.67 88.72 85.22 81.93 96.18 95.95 92.88 89.22 85.95 82.87 96.22 96.60\nTable 8: G2T on ASLG-PC12 ﬁnal results\nWe also use 5 of the best models from our experiments on ASLG-PC12 in an ensemble. Individually,\nthese models obtain between 81.72 and 82.41 BLEU-4 on testing. Table 8 shows that the performance\nof our single Transformer surpasses the recurrent seq2seq model by Arvanitis et al. (2019) by over 16\nBLEU-4. The ensemble model reports an improvement of 0.46 BLEU-4 over the single model. There is\nrelatively less increase from ensembling possibly because there is less variance across different models.\n6.2 German Sign2Gloss2Text (S2G2T)\nIn S2G2T, both gloss recognition from videos and its translation to text are performed automatically.\nCamgoz et al. (2018) claims the previous G2T setup to be an upper bound for translation performance,\nsince it simulates having a perfect recognition system. However, this claim assumes that the ground truth\ngloss annotations give a full understanding of sign language, which ignores the information bottleneck\nin glosses. Camgoz et al. (2020) hypothesizes that it is therefore possible to surpass G2T performance\nwithout using GT glosses, which we conﬁrm in this section.\nWe perform experiments on the PHOENIX-Weather 2014T dataset as it contains parallel video, gloss\nand text data. On the other hand, the ASLG-PC12 corpus does not have sign language video information.\nDev Set Test Set\nModel BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR\nG2T (Camgoz et al., 2018) 44.40 31.93 24.61 20.16 46.02 – 44.13 31.47 23.89 19.26 45.45 –S2G→G2T (Camgoz et al., 2018) 41.08 29.10 22.16 17.86 43.76 – 41.54 29.52 22.24 17.79 43.45 –S2G2T (Camgoz et al., 2018) 42.88 30.30 23.03 18.40 44.14 – 43.29 30.39 22.82 18.13 43.80 –Sign2(Gloss+Text) (Camgoz et al., 2020) 47.26 34.40 27.05 22.38 – – 46.61 33.73 26.19 21.32 – –\nS2G→G2T 46.75 34.99 27.79 23.06 47.29 45.23 47.49 35.89 28.62 23.77 47.32 45.54\nBahdanau 45.89 32.24 24.93 20.52 44.46 43.48 47.53 33.82 26.07 21.54 45.50 44.87Luong 45.61 32.54 26.33 21.00 46.19 44.93 47.08 33.93 26.31 21.75 45.66 44.84\nTransformer 48.27 35.20 27.47 22.47 46.31 44.95 48.73 36.53 29.03 24.00 46.77 45.78Transformer Ens. 50.31 37.60 29.81 24.68 48.70 47.45 50.63 38.36 30.58 25.40 48.78 47.60\nTable 9: SLT performance using STMC for CSLR. The ﬁrst set of rows correspond to the current state-\nof-the-arts included for comparison.\nS2G →G2T\nTo begin, we use the best performing model for German G2T to translate glosses predicted by a trained\nSTMC network. In Table 9 we can see that despite no additional training for translation, this model\nalready obtains a relatively high score that beats the current state-of-the-art by over 5 BLEU-4.\nRecurrent seq2seq networks\nFor comparison, we also train and evaluate STMC used with recurrent seq2seq networks for translation.\nThe translation models are composed of four stacked layers of Gated Recurrent Units (GRU) (Chung et\nal., 2014), with either Luong (Luong et al., 2015) or Bahdanau (Bahdanau et al., 2015) attention.\nIn Table 9, recurrent seq2seq models obtain slightly better performance with Luong attention. Surpris-\ningly, these models outperform previous models of similar architecture that translate GT glosses.\nTransformer\nFor the STMC-Transformer, we train Transformer models with the same architecture as in G2T. Parame-\nter search yields an initial learning rate 1 with 3,000 warm-up steps and beam size 4. We empirically ﬁnd\nusing the 8 best models in ensemble decoding to be optimal. These models individually obtain between\n23.51 and 24.00 BLEU-4.\nAgain, we observe that STMC-Transformer outperforms the previous system with ground truth glosses\nand Transformer. While STMC performs imperfect CSLR, its gloss predictions may be more useful\nthan ground-truth annotations during SLT and are more readily analyzed by the Transformer. Again,\nthe ground truth glosses represent merely a simpliﬁed intermediate representation of the actual sign\nlanguage, so it is not entirely unexpected that translating ground truth glosses does not give the best\nperformance.\nSTMC-Transformer also outperforms Transformers that translate GT glosses. While STMC performs\nimperfect CSLR, its gloss predictions may be better processed by the Transformer. Glosses are merely a\nsimpliﬁed intermediate representation of the actual sign language so they may not be optimal. This result\nalso reveals, training the recognition model to output more accurate glosses will not necessarily improve\ntranslation.\nBoth our STMC-Transformer and STMC-RNN also outperform Camgoz et al. (2020)’s model. Their\nbest model jointly train Transformers for recognition and translation, however it obtains 24.49 WER on\nrecognition whereas STMC obtains a better WER of 21.0, which suggests their model may be weaker in\nprocessing the videos.\nMoreover, Transformers outperform recurrent networks in this setup as well and STMC-Transformer\nimproves the state-of-the-art for video-to-text translation by 7 BLEU-4.\n7 Qualitative comparison\nExample outputs of the G2T and S2G2T models (Table 10) show that the translations are of generally\ngood quality, even with low BLEU scores. Most translations may have slight differences in word choice\nthat do not change the overall meaning of the sentence or make grammatical errors, which suggests\nBLEU is not a good representative of human useful features for SLT. As for the comparison between\nthe G2T and S2G2T networks, there does not seem to be a clear pattern between cases where S2G2T\noutperforms G2T and vice versa. One thing to note, though, is that the PHOENIX-Weather 2014T is\nrestricted to the weather forecast domain, and a SLT dataset with a wider domain would be required to\nfully assess the performance of our model in more general real-life settings.\nWe also provide sample G2T outputs on the ASLG-PC12 corpus in Appendix A.2.\n8 Conclusions and Future Work\nIn this paper, we proposed Transformers for SLT, notably the STMC-Transformer. Our experiments\ndemonstrate how Transformers obtain better SLT performance than previous RNN-based networks. We\nalso achieve new state-of-the-art results on different translation tasks on the PHOENIX-Weather 2014T\nand ASLG-PC12 datasets.\nA key ﬁnding is we obtain better performance by using a STMC network for tokenization instead\nof translating GT glosses. This calls into question current methods that use glosses as an intermediate\nrepresentation, since reference glosses themselves are suboptimal.\nEnd-to-end training without gloss supervision is one promising step, though Camgoz et al. (2020)’s\nend-to-end model does not yet surpass their joint training model. As future work, we suggest continuing\nwork on end-to-end training of the recognition and translation models, so the recognition model learns\nan intermediate representation that optimizes translation, or using a different sign language annotation\nscheme that has less information loss.\nBLEU-4\nREF: ¨ahnliches wetter auch am donnerstag .\n(similar weather on thursday .)\nG2T: GLEICH WETTER AUCH DONNERSTAG\n(SAME WEATHER ON THURSDAY)\n¨ahnliches wetter auch am donnerstag . 100.00\n(similar weather on thursday .)\nS2G2T: GLEICH WETTER DONNERSTAG\n(SAME WEATHER THURSDAY)\n¨ahnliches wetter dann auch am donnerstag . 48.89\n(similar weather then on thursday .)\nREF: der wind weht meist schwach aus unterschiedlichen richtungen .\n(the wind usually blows weakly from different directions .)\nG2T: WIND SCHW ACH UNTERSCHIED KOMMEN\n(WIND WEAK DIFFERENCE COME)\nder wind weht meist nur schwach aus unterschiedlichen richtungen . 65.80\n(the wind usually blows only weakly from different directions .)\nS2G2T: WIND SCHW ACH UNTERSCHIED\n(WIND WEAK DIFFERENCE)\nder wind weht schwach aus unterschiedlichen richtungen . 61.02\n(the wind is blowing weakly from different directions .)\nREF: sonnig geht es auch ins wochenende samstag ein herrlicher tag mit temperaturen bis siebzehn grad hier im westen .\n(the weekend is also sunny and saturday is a wonderful day with temperatures up to seventeen degrees here in the west .)\nG2T: WOCHENENDE SONNE SAMSTAG SCHOEN TEMPERATUR BIS SIEBZEHN GRAD REGION\n(WEEKEND SUN SATURDAY NICE TEMPERATURE UNTIL SEVENTEEN DEGREE REGION)\nund am wochenende da scheint die sonne bei temperaturen bis siebzehn grad . 13.49\n(and on the weekend the sun shines at temperatures up to seventeen degrees .)\nS2G2T: WOCHENENDE SONNE SAMSTAG TEMPERATUR BIS SIEBZEHN GRAD REGION\n(WEEKEND SUN SATURDAY TEMPERATURE UNTIL SEVENTEEN DEGREE REGION)\nam wochenende scheint die sonne bei temperaturen bis siebzehn grad . 12.55\n(on the weekend sun shines at temperatures up to seventeen degrees .)\nREF: es gelten entsprechende warnungen des deutschen wetterdienstes .\n(appropriate warnings from the german weather service apply .)\nG2T: IX SCHON W ARNUNG DEUTSCH WETTER DIENST STURM KOENNEN\n(IX ALREADY W ARNING GERMAN WEATHER SERVICE STORM CAN)\nes bestehen entsprechende unwetterwarnungen des deutschen wetterdienstes . 38.26\n(severe weather warnings from the german weather service exist .)\nS2G2T: DANN IX SCHON W ARNUNG DEUTSCH WETTER STURM KOENNEN\nes gelten entsprechende warnungen des deutschen wetterdienstes . 100.00\n(THEN IX ALREADY W ARNING GERMAN WEATHER STORM CAN)\n(appropriate warnings from the german weather service apply .)\nREF: richtung osten ist es meist sonnig .\n(it is mostly sunny towards the east .)\nG2T: OST MEISTENS SONNE\n(MOST EAST SUN)\nim osten bleibt es meist sonnig . 43.47\n(in the east it mostly stays sunny .)\nS2G2T: OST REGION MEISTENS SONNE\nim osten ist es meist sonnig . 80.91\n(MOST REGION EAST SUN)\n(in the east it is mostly sunny .)\nREF: am tag elf grad im vogtland und einundzwanzig grad am oberrhein .\n(during the day eleven degrees in vogtland and twenty one degrees in upper rhine .)\nG2T: AM-TAG ELF VOGEL LAND\n(IN-THE-DAY ELEVEN BIRD LAND)\nelf grad am oberrhein . 18.74\n(eleven degrees in upper rhine .)\nS2G2T: ELF VOGEL ZW ANZIG\n(ELEVEN BIRD TWENTY)\nam tag elf grad im vogtland und zwanzig grad im vogtland . 54.91\n(during the day eleven degrees in vogtland and twenty degrees in vogtland .)\nTable 10: Qualitative comparison of G2T and S2G2T on RWTH-PHEONIX-WEATHER 2014T. Glosses\nare capitalized. REF refers to the reference German translation.\nAcknowledgements\nThe Titan X Pascal used for this research was donated by the NVIDIA Corporation. The authors would\nalso like to thank Jean-Baptiste R ´emy for the helpful discussions and feedback with various aspects of\nthis work, and Hao Zhou for sharing the details of her previous work.\nReferences\nNikolaos Arvanitis, Constantinos Constantinopoulos, and Dimitris Kosmopoulos. 2019. Translation of sign lan-\nguage glosses to text using sequence-to-sequence attention models. pages 296–302, 11.\nStylianos Asteriadis, George Caridakis, and Kostas Karpouzis. 2012. Non-manual cues in automatic sign language\nrecognition. Personal and Ubiquitous Computing, 18.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to\nalign and translate. In ICLR.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved\ncorrelation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation\nMeasures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan. Association for\nComputational Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Association for Computational Linguistics, 5:135–146.\nN. C. Camgoz, S. Hadﬁeld, O. Koller, and R. Bowden. 2017. Subunets: End-to-end hand shape and continuous\nsign language recognition. In 2017 IEEE International Conference on Computer Vision (ICCV) , pages 3075–\n3084.\nN. C. Camgoz, S. Hadﬁeld, O. Koller, H. Ney, and R. Bowden. 2018. Neural sign language translation. In 2018\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7784–7793.\nNecati Cihan Camgoz, Oscar Koller, Simon Hadﬁeld, and Richard Bowden. 2020. Sign language transformers:\nJoint end-to-end sign language recognition and translation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June.\nJunyoung Chung, C ¸ aglar G¨ulc ¸ehre, Kyunghyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated\nrecurrent neural networks on sequence modeling. NIPS 2014 Deep Learning and Representation Learning\nWorkshop, abs/1412.3555.\nOnno Crasborn, Johanna Mesch, Dafydd Waters, Annika Nonhebel, Els van der kooij, Bencie Woll, and Brita\nBergman. 2007. Sharing sign language data online: Experiences from the echo project. International Journal\nof Corpus Linguistics, 12:535–562, 01.\nRunpeng Cui, Hu Liu, and Changshui Zhang. 2017. Recurrent convolutional neural networks for continuous\nsign language recognition by staged optimization. 2017 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1610–1618.\nJens Forster, Oscar Koller, Christian Oberd¨orfer, Yannick L. Gweth, and Hermann Ney. 2013. Improving contin-\nuous sign language recognition: Speech recognition techniques and system design. In SLPAT.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. 2017. Convolutional sequence to\nsequence learning. In ICML.\nAlex Graves, Santiago Fern ´andez, Faustino Gomez, and J ¨urgen Schmidhuber. 2006. Connectionist temporal\nclassiﬁcation: Labelling unsegmented sequence data with recurrent neural networks. pages 369–376, 01.\nJie Huang, Wengang Zhou, Houqiang Li, and Weiping Li. 2015. Sign language recognition using 3d convolutional\nneural networks. 2015 IEEE International Conference on Multimedia and Expo (ICME), pages 1–6.\nTrevor Johnston. 2013. Auslan corpus annotation guidelines.\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In EMNLP.\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan, A¨aron van den Oord, Alex Graves, and Koray Kavukcuoglu.\n2016. Neural machine translation in linear time. ArXiv, abs/1610.10099.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. International Conference\non Learning Representations.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. 2017. OpenNMT: Open-\nsource toolkit for neural machine translation. In Proc. ACL.\nSangki Ko, Chang Kim, Hyedong Jung, and Choongsang Cho. 2019. Neural sign language translation based on\nhuman keypoint estimation. Applied Sciences, 9:2683.\nOscar Koller, Sepehr Zargaran, and Hermann Ney. 2017. Re-sign: Re-aligned end-to-end sequence modelling\nwith deep recurrent cnn-hmms. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 3416–3424.\nJeroen Lichtenauer, Emile Hendriks, and Marcel Reinders. 2008. Sign language recognition by combining sta-\ntistical dtw and independent classiﬁcation. IEEE transactions on pattern analysis and machine intelligence ,\n30:2040–6, 12.\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.\nThang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neu-\nral machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language\nProcessing, pages 1412–1421, Lisbon, Portugal, September. Association for Computational Linguistics.\nPavlo Molchanov, Xiaodong Yang, Shalini Gupta, Kihwan Kim, Stephen Tyree, and Jan Kautz. 2016. Online\ndetection and classiﬁcation of dynamic hand gestures with recurrent 3d convolutional neural networks. pages\n4207–4215, 06.\nAlptekin Orbay and Lale Akarun. 2020. Neural sign language translation by learning tokenization. ArXiv,\nabs/2002.00479.\nAchraf Othman and Mohamed Jemni. 2012. English-asl gloss parallel corpus 2012: Aslg-pc12.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalua-\ntion of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational\nLinguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zem-\ning Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito,\nMartin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin-\ntala. 2019. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural\nInformation Processing Systems 32, pages 8024–8035. Curran Associates, Inc.\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word repre-\nsentation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.\nMartin Popel and Ondˇrej Bojar. 2018. Training tips for the transformer model. The Prague Bulletin of Mathemat-\nical Linguistics, 110.\nJanet L. Pray and I. King Jordan. 2010. The deaf community and culture at a crossroads: Issues and challenges.\nvolume 9, pages 168–193. Routledge. PMID: 20730674.\nOﬁr Press and Lior Wolf. 2017. Using the output embedding to improve language models. In Proceedings of the\n15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short\nPapers, pages 157–163, Valencia, Spain, April. Association for Computational Linguistics.\nYe Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. When and why are\npre-trained word embeddings useful for neural machine translation? In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 2 (Short Papers), pages 529–535, New Orleans, Louisiana, June. Association for Computational\nLinguistics.\nMaria Fernanda Neves Silveira de Souza, Amanda Miranda Brito Ara ´ujo, Luiza Fernandes Fonseca Sandes,\nDaniel Antunes Freitas, Wellington Danilo Soares, Raquel Schwenck de Mello Vianna, and Arlen Almeida\nDuarte de Sousa. 2017. Main difﬁculties and obstacles faced by the deaf community in health access: an\nintegrative literature review. Revista CEFAC, 19:395 – 405.\nWilliam C. Stokoe. 1960. Sign language structure: an outline of the visual communication systems of the american\ndeaf. Journal of deaf studies and deaf education, 10 1:3–37.\nIlya Sutskever, Oriol Vinyals, and Quoc Le. 2014. Sequence to sequence learning with neural networks.Advances\nin Neural Information Processing Systems, 4, 09.\nAlaa Tharwat, Tarek Gaber, Shahin MK, Basma Refaat, and Aboul Ella Hassanien Ali. 2014. Sift-based arabic\nsign language recognition system. In The 1st Afro-European Conference for Industrial Advancement, , Addis\nAbaba, Ethiopia, November 17-19,.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In NIPS.\nQ X Yang. 2010. Chinese sign language recognition based on video sequence appearance modeling. 2010 5th\nIEEE Conference on Industrial Electronics and Applications, pages 1537–1542.\nJihai Zhang, Wengang Zhou, and Houqiang Li. 2014. A threshold-based hmm-dtw approach for continuous sign\nlanguage recognition. ACM International Conference Proceeding Series, 07.\nHao Zhou, Wengang Zhou, Yun Zhou, and Houqiang Li. 2020. Spatial-temporal multi-cue network for continuous\nsign language recognition. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence , pages 13009–\n13016. AAAI Press.\nA Appendices\nA.1 Experiments on German G2T learning rate\nA learning rate that is too low results in a notably slower convergence, but setting the learning rate\ntoo high risks leading the model to diverge. To prevent the model from diverging, we apply the Noam\nlearning rate schedule where the learning rate increases linearly during the ﬁrst training steps, or the\nwarmup stage, then decreases proportionally to the inverse square root of the step number. The number\nof warmup steps is a parameter that has shown to inﬂuence Transformer performance (Popel and Bojar,\n2018) therefore we ﬁrst run a parameter search over the number of warmup steps before ﬁnding the\noptimal initial learning rate.\n1k 2k 3k 4k 5k 6k 7k 8k21\n21.5\n22\n22.5\n23\nWarmup steps\nBLEU-4\nDev set\nTest set\nFigure 4: G2T performance on RWTH-PHEONIX-WEATHER 2014T with different warmup steps. Ini-\ntial learning rate is ﬁxed to 0.2.\n0.01 0.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 121\n21.5\n22\n22.5\n23\nLearning rate\nBLEU-4\nDev set\nTest set\nFigure 5: G2T performance on RWTH-PHEONIX-WEATHER 2014T with various initial learning rate.\nA.2 Qualitative G2T Results on ASLG-PC12\nBecause quantitative metrics provide only a limited evaluation of translation performance, manual eval-\nuation by viewing the translation outputs directly may give a better assessment of the quality of trans-\nlations. Table 11 provides examples of SLT output on the ASLG-PC12 dataset. Here we can see how\nASL glosses include preﬁxes that are not necessary to encapture the meaning of the phrase, which we\nhave removed during data pre-processing before training. With a BLEU-4 testing score of 82.87, most\npredictions by our system are very close to the target English phrases and are able to convey the same\nmeaning. We have also selected translation examples with lower BLEU-4 score and we can see that\ncommon errors include mistranslation of numbers and proper nouns. These are likely corner cases with\ninfrequent examples during training.\nBLEU-4\nASL: X-I BE DESC-PARTICULARLY DESC-GRATEFUL FOR EUROPEAN PARLIAMENT X-POSS DRIVE ROLE WHERE BALTIC SEA COOPERATION BE CONCERNGT: i am particularly grateful for the european parliament’s driving role where the baltic sea cooperation is concerned .100.00Pred: i am particularly grateful for the european parliament’s driving role where the baltic sea cooperation is concerned .\nASL: DESC-REFORE , DESC-MUCH WORK NEED TO BE DO IN ORDER TO DESC-FURR SIMPLIFY RULEGT: therefore , much work needs to be done in order to further simplify the rules . 100.00Pred: therefore , much work needs to be done in order to further simplify the rules .\nASL: THIS PRESSURE BE DESC-PARTICULARLY DESC-GREAT ALONG UNION X-POSS DESC-SOURN AND DESC-EASTERN BORDERGT: this pressure is particularly great along the union’s southern and eastern borders . 100.00Pred: this pressure is particularly great along the union’s southern and eastern borders .\nASL: MORE WOMAN DIE FROM AGGRESSION DESC-DIRECT AGAINST X-Y THAN DIE FROM CANCER .GT: more women die from the aggression directed against them than die from cancer . 73.15Pred: more women die from aggression directed against them than die from cancer .\nASL: X-IT FUEL W AR IN CAMBODIUM IN 1990 AND X-IT BE ENEMY DEMOCRACYGT: it fuelled the war in cambodia in the 1990s and it is the enemy of democracy . 25.89Pred: it fuel war in the cambodium in 1990 and it is an enemy of democracy .\nASL: DESC-N CHIEF INVESTIGATOR X-HIMSELF BE TARGET AND HOUSE CARD COLLAPSE .GT: then the chief investigator himself is targeted and the house of cards collapses . 21.29Pred: then chief investigator himself is a target and a house card collapse .\nASL: U , X-WE TAKE DESC-DUE NOTE X-YOU OBSERV ATION . AMENDMENT THANK X-YOU MRGT: otherwise we have to vote on the corresponding part of amendment thank you mrs t ¸ic˘au , we take due note of your observation . 15.93Pred: mr president , we took due note of your observation .\nTable 11: Examples of ASL translation with varying BLEU-4 scores"
}