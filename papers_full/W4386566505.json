{
  "title": "ERATE: Efficient Retrieval Augmented Text Embeddings",
  "url": "https://openalex.org/W4386566505",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3037638521",
      "name": "Vatsal Raina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2027506410",
      "name": "Nora Kassner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2551091452",
      "name": "Kashyap Popat",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134084108",
      "name": "Patrick Lewis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A55244337",
      "name": "Nicola Cancedda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098658999",
      "name": "Louis Martin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4386576685",
    "https://openalex.org/W4309083387",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4224313754",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W582134693",
    "https://openalex.org/W3034439313",
    "https://openalex.org/W2790235966",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3175362188",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3102839769",
    "https://openalex.org/W4284670538",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4225576545",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4281251078",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W4386566503"
  ],
  "abstract": "Embedding representations of text are useful for downstream natural language processing tasks. Several universal sentence representation methods have been proposed with a particular focus on self-supervised pre-training approaches to leverage the vast quantities of unlabelled data. However, there are two challenges for generating rich embedding representations for a new document. 1) The latest rich embedding generators are based on very large costly transformer-based architectures. 2) The rich embedding representation of a new document is limited to only the information provided without access to any explicit contextual and temporal information that could potentially further enrich the representation. We propose efficient retrieval-augmented text embeddings (ERATE) that tackles the first issue and offers a method to tackle the second issue. To the best of our knowledge, we are the first to incorporate retrieval to general purpose embeddings as a new paradigm, which we apply to the semantic similarity tasks of SentEval. Despite not reaching state-of-the-art performance, ERATE offers key insights that encourages future work into investigating the potential of retrieval-based embeddings.",
  "full_text": "The Fourth Workshop on Insights from Negative Results in NLP, pages 11–18\nMay 5, 2023 ©2023 Association for Computational Linguistics\nERATE: Efficient Retrieval Augmented Text Embeddings\nVatsal Raina∗ Nora Kassner Kashyap Popat\nPatrick Lewis Nicola Cancedda Louis Martin\nMeta AI\nvr311@cam.ac.uk louismartin@meta.com\nAbstract\nEmbedding representations of text are useful\nfor downstream natural language processing\ntasks. Several universal sentence representa-\ntion methods have been proposed with a partic-\nular focus on self-supervised pre-training ap-\nproaches to leverage the vast quantities of unla-\nbelled data. However, there are two challenges\nfor generating rich embedding representations\nfor a new document. 1) The latest rich embed-\nding generators are based on very large costly\ntransformer-based architectures. 2) The rich\nembedding representation of a new document\nis limited to only the information provided with-\nout access to any explicit contextual and tem-\nporal information that could potentially further\nenrich the representation. We propose efficient\nretrieval-augmented text embeddings (ERATE)\nthat tackles the first issue and offers a method\nto tackle the second issue. To the best of our\nknowledge, we are the first to incorporate re-\ntrieval to general purpose embeddings as a new\nparadigm, which we apply to the semantic sim-\nilarity tasks of SentEval. Despite not reaching\nstate-of-the-art performance, ERATE offers key\ninsights that encourages future work into inves-\ntigating the potential of retrieval-based embed-\ndings.\n1 Introduction\nState-of-the-art sentence embedding models (Raf-\nfel et al., 2020; Neelakantan et al., 2022) have com-\npeted against one another to approach human-like\nperformance in several NLP tasks. Despite the\ngains observed in performance of sentence embed-\ndings on public benchmarks such as SentEval (Con-\nneau and Kiela, 2018a), the progress has come at\na large computational expense. For example, the\nlargest model amongst the Sentence-T5 series con-\nsists of up to billions of parameters while GPT-3\n∗Work done during internship.\nbased sentence embedding model released by Nee-\nlakantan et al. (2022) has 175 billion parameters\nwith marginal gains observed in performance when\ncompared against older, smaller models. Models of\nthese sizes are compute intensive and very difficult\nto host and use for most downstream use cases.\nWe propose a new paradigm that aims to main-\ntain the benefits of high-complexity rich embed-\nding models at reduced computational require-\nments. Our novel paradigm investigates whether\nretrieval can be used to bypass the compute in-\ntensive embedding model in a similar manner to\nthe application of retrieval for generation (Lewis\net al., 2020; Cai et al., 2022) tasks for real world\nlarge scale use cases with latency and compute\nconstraints. We propose to use a lightweight re-\ntrieval model combined with rich pre-computed\nrepresentations, in order to approximate the richer\nrepresentations of a large embedding model.\nWe find retrieval-based embeddings struggle\nagainst standard text embedding models but their\nperformance can be improved by aggregating\nneighbours from different light embedding repre-\nsentations and increasing the size of the datastore\nof precomputed embeddings.\nTo our knowledge, this is the first attempt to use\nretrieval approaches for developing general pur-\npose sentence embeddings. Our main contributions\ncan be summarised as follows:\n• Introduction of a novel paradigm for gen-\nerating sentence embeddings by exploiting\nretrieval-based approaches.\n• Releasing efficient retrieval augmented text\nembeddings (ERATE) baseline systems with\nan exploration of methods that work well and\ndon’t work as well to assess the scope of re-\ntrieval to recover the performance of rich em-\nbedding models with low compute.\n11\nWe hope other researchers will engage in this novel\nsetup to develop more efficient sentence embed-\ndings that will allow high-performing representa-\ntions to be accessible to a broader community.\nOur work focuses on developing lightweight em-\nbeddings that out-compete existing lightweight em-\nbeddings but we believe ERATE can be used for\na wider range of applications. Specifically, input\ndocuments often lack the full contextual informa-\ntion or temporal relevance to generate the necessary\nhigh-quality text embedding. ERATE offers the op-\nportunity for the embedding of a given document\nto encapsulate information from other similar doc-\numents to increase the information content whilst\nalso being more up-to-date with more recent docu-\nments added to a datastore.\n2 Related Work\nReimers and Gurevych (2019) introduced Sentence-\nBERT as an improvement to the sentence represen-\ntations from BERT (Devlin et al., 2019) by explic-\nitly training Siamese BERT-networks using pairs\nof similar/dissimilar sentences. Yan et al. (2021)\nreleased ConSERT to learn sentence representa-\ntions in an unsupervised manner by applying var-\nious forms of augmentations to a sentence to cre-\nate its pair for contrastive learning. In a similar\nvain, SimCSE (Gao et al., 2021) relied on unsuper-\nvised contrastive learning by using dropout masks\nas the augmentation technique. DiffCSE (Chuang\net al., 2022) further incorporated masked language\nmodelling as an augmentation technique. Ni et al.\n(2022), released a family of sentence-T5 models\nthat finetuned the T5 (Raffel et al., 2020) architec-\nture in a supervised manner with pairs of naturally\noccurring similar sentences. Most recently, Nee-\nlakantan et al. (2022) developed a model finetuned\nusing GPT-3 (Brown et al., 2020).\nSeveral works have looked at approaches to\nmake less expensive sentence embedding represen-\ntations. For example, embedding recycling (Saad-\nFalcon et al., 2022) for language models is pro-\nposed as a reduced compute approach for down-\nstream tasks. This involves caching activations\nfrom intermediate layers in large pre-trained mod-\nels such that when similar inputs are seen during in-\nference time, the cached output can be used in order\nto skip a part of the model structure. Embedding\nrecycling has been demonstrated to out-compete\ndistilled models, such as DistilBERT (Sanh et al.,\n2019). In contrast, we investigate whether fixed em-\nbedding representations can be generated more effi-\nciently using retrieval without any additional train-\ning, relying only on pre-computed embeddings.\nOther works have investigated efficient methods\nfor retrieval from a large set of documents such as\nColBERT (Khattab and Zaharia, 2020) and PLAID\n(Santhanam et al., 2022) interaction models that\nuse offline encoding of documents. Rather than\nmaking the retrieval step more efficient, our work\nfocuses on using retrieval as a tool for enhancing\nthe development of general purpose embeddings.\nText generation and language modelling has\nseen several works involving performance boost\nwith retrieval. Khandelwal et al. (2019) inves-\ntigates extending a pre-trained language model\nby including the k-nearest neighbours, which\nKassner and Schütze (2020) applies to question-\nanswering. Similarly, Lewis et al. (2020) intro-\nduced retrieval-augmented generation (RAG) mod-\nels where a pre-trained retriever and a pre-trained\nsequence-to-sequence model are fine-tuned end-to-\nend. Borgeaud et al. (2022) released RETRO as a\nsuccessor of REALM (Guu et al., 2020) where an\nautoregressive language work is retrieval-enhanced\nby making the training documents explicitly avail-\nable at inference time. Finally, Izacard et al. (2022)\npresent ATLAS for retrieval-enhanced language\nmodelling where the sequence-to-sequence model\ntakes the retrieved documents and the query to\ngenerate the output text for knowledge-intensive\ntasks. We probe whether retrieval-incorporated\napproaches can bring similar benefits for the de-\nvelopment of fixed embedding representations, not\nend-to-end sequence-to-sequence models.\n3 Retrieval for text embeddings\nThis section explains how efficient retrieval aug-\nmented text embeddings are developed. The main\nidea is that a query document only needs to be\nembedded using a light embedder and by outlin-\ning the nearest neighbours in the light space, the\ncorresponding pre-computed embeddings can be\ncombined to generate the rich query embedding.\nLet ˆd denote a new document, for which we want\nto determine the rich embedding representation, ˆx.\nLet flight(·) and frich(·) be embedding generators\nthat map a given document to the light and rich\nembedding spaces respectively:\nh = flight(d) x = frich(d) (1)\nNote, we assume that the operation frich(d) is pro-\n12\nFigure 1: Schematic for ERATE embedding generation.\nhibitively compute intensive while ˆh = flight( ˆd) is\nfeasible. Instead, there exists a set of documents\n{d}K\nk=1 for which the rich embeddings, {x}D\nk=1,\nhave been pre-computed. Let granker(·) denote a\nretrieval system that ranks all embeddings (with\npairwise cosine distance) in a set based upon a\nquery embedding. Hence, the ranks are:\n{r}K\nk=1 = granker(ˆh; {h}K\nk=1) (2)\nThe final rich embedding can then be calculated as\na combination of the rich embedding representa-\ntions of the top R documents:\nˆx = 1\nR\nK∑\nk=1\n1rk≤R ·xk (3)\nThe process is depicted in the pipeline of Figure 1.\nAlternative approaches can be considered for the\ncombination process of Equation 3 1.\n3.1 Dropout masks\nThe proposed set-up for ERATE relies on identify-\ning neighbours to the query document in the light\nspace. However, the set of neighbours identified in\nthe light space are correlated with the light embed-\nding model that may not necessarily align with the\ndesired neighbours in the rich space. Consequently,\nit is useful to create a neighbour set curated from\nmultiple light embedding models which reduces\nthe bias to a single light embedder (see Figure 2).\nDropout (Srivastava et al., 2014) is a common\nregularisation technique that has been extended to\ncreate diverse outputs at inference time such as\nMonte Carlo dropout (Gal and Ghahramani, 2016).\nSimilarly, randomly dropping out embedding di-\nmensions can be used to create a diverse set of light\nembedders that can expect to have different, poten-\ntially complementary, neighbour sets. Therefore\n1Empirical experiments indicated that weighing the im-\nportance of a retrieved embedding by its inverse distance to\nthe query in the light space did not improve performance and\nhence the simplest approach of a linear average was adopted.\ndropout masks are applied to the light embeddings\nprior to performing retrieval in the ERATE process\nto create enchanced neighbour sets.\n4 Experiments\n4.1 Setup\nSentEval (Conneau and Kiela, 2018b) is a popular\nbenchmark dataset for assessing the quality of sen-\ntence embeddings, consisting of semantic text sim-\nilarity (STS) tasks STS-12 to STS-16 and STS-B,\nSICK-R. These tasks evaluate how well the cosine\ndistances of embeddings from pairs of sentences\ncorrelate with human annotated similarity scores\nusing Spearman’s rank correlation coefficient2.\nFor ERATE to work effectively, a large datas-\ntore of documents/sentences must exist for which\nthe sentence embeddings must be pre-calculated\nusing both a light embedder and a rich embedder.\nWe select the average GloVe word embeddings 3\n(Pennington et al., 2014) as the light embedder as\nthe model involves a simple lookup for each word\nin the sentence to determine its word embedding\nand hence low compute. State-of-the-art perfor-\nmance on the STS tasks of SentEval is achieved\nby Sentence-T5-xxl4 (Ni et al., 2022). Hence, we\nadopt this Sentence-T5 model as our rich embedder.\nAdditionally, we consider anOracle ERATE model\nto breakdown the retrieval and combination stages\nof ERATE embeddings. Oracle embeddings are\ncalculated by retrieving the closest neighbours in\nthe rich space instead of the light space.\nWiki SNLI MNLI CC\n# sentences 1M 629K 519K 100M\navg. words 19±12 8±4 12±9 25±19\nTable 1: Statistics for unique datastore sentences.\nThe datastore of sentences with pre-computed\nembeddings is constructed from combining the 1\nmillion Wikipedia (Wiki) sentences that acted as\nthe unsupervised training data for SimCSE (Gao\net al., 2021) and DiffCSE (Chuang et al., 2022)\nwith the unique sentences of the premise and hy-\npothesis from the SNLI (Bowman et al., 2015) and\n2Consistent with previous works, the ‘all’ setting that ag-\ngregates the subsets in a given STS task is used from https:\n//github.com/facebookresearch/SentEval.\n3Available at: https://huggingface.co/\nsentence-transformers/average_word_\nembeddings_glove.840B.300d\n4Available at: https://huggingface.co/\nsentence-transformers/sentence-t5-xxl\n13\nMNLI (Williams et al., 2018) datasets. An addi-\ntional 100 million sentences sampled from com-\nmon crawl (CC)5 are included in an expanded data-\nstore to investigate the impact of increasing the\ndatastore size. Table 1 details the statistics for each\nof these subsets. Sentences from STS on average\nhave 13±10 words, which is of a similar length to\nthe sentences that are being used for the datastore\nas well as in terms of the diversity of topics.\n4.2 Results\nFor a 512 token sentence the vanilla ERATE model\n(with a datastore size of 1 million) requires 3 ×\n109 floating point operations (FLOPs) while the\nSentence-T5 model requires 8.7 ×1012 FLOPs.\nTable 2 presents the performance of the baseline\nERATE system against the existing state-of-the-art\nperformance from Sentence-T5. Using the com-\npute intensive rich embeddings directly achieves\nan average correlation coefficient of 84.8% across\nall the STS tasks while the light embedding model\nachieves a performance of 62.8% at a fraction of\nthe compute. In contrast, the ERATE embeddings\n(100 closest neighbours are selected in the retrieval\nstep), which have a similar compute to the light\nembedder, only achieve 55.3%. This low perfor-\nmance is underwhelming as let alone being close\nto state-of-the-art, it is not able to compete against\nthe light embedding model.\nAvg. sts12 sts13 sts14 sts15 sts16 stsB sickR\nRich 84.8 78.9 88.9 84.9 89.3 84.7 86.7 80.4\nLight 62.8 57.5 71.0 60.7 70.8 63.8 60.9 54.8\nOracle 72.3 66.8 76.9 70.9 73.6 73.7 75.2 69.1\nERATE 55.3 57.2 59.7 47.3 59.9 54.5 53.8 54.7\n+drop. 57.4 60.8 62.0 52.8 59.8 54.4 56.5 55.4\n+expand 57.9 55.4 60.0 52.9 64.1 60.0 58.8 53.8\nTable 2: Performance with Sentence-T5 (Rich), GloVe\n(Light), oracle neighbours and vanilla ERATE with\ndropout and an expanded datastore.\nThe significant boost in performance to 72.3%\nfrom the Oracle suggests that the combination pro-\ncess by averaging is somewhat successful and the\nloss in performance comes from a mismatch in\nthe surrounding neighbours for the light vs rich\nspace. Further work would benefit from investigat-\ning alignment between the light and rich spaces.\nFigure 2a further depicts an example PCA plot\n(using the two most dominant dimensions). Here,\nthe rich embedding of an example query sentence\nis compared to the rich embeddings of the closest\n5https://commoncrawl.org/\n(a) Query vs neighbours.\n (b) Neighbours with dropout.\nFigure 2: PCA on rich embeddings showing the query\nis closer to the centroid with multiple neighbour sets.\nneighbours identified from the light space. On\nobservation 6, the query lies on the periphery of\nthe neighbours, which leads to the the centroid of\nthe neighbours being afar from the desired query’s\nposition. We confirm the anisotropy hypothesis as\nthe ratio of the distance between the query to the\ncentroid and the averaged neighbour distance to\nthe centroid (averaged across all test examples) is\n1.1±0.4 while the equivalent ratio using the Oracle\nneighbours is 0.5±0.2 - about twice as close.\nConsequently, as discussed in Section 3.1, an\nexpanded neighbour set is considered by applying\ndifferent dropout masks on 50% of the dimensions.\nVisually, Figure 2b suggests that the neighbour set\nfrom each dropout mask is somewhat different and\nhence the centroid of all the neighbours is more\nlikely to approach the query’s rich embedding. The\nhypothesis is supported by Table 2 where the per-\nformance increases to 57.4% by using 10 dropout\nmasks simultaneously.\nThe performance can expect to be higher if the\nneighbours of the query are from a dense region as\nthe combination of the embeddings will have less\nerror. Therefore, Table 2 details the performance\nwhen using an expanded datastore size consisting\nof an additional 100 million sentences from Com-\nmon Crawl (see Table 1). The baseline ERATE\nsystem performance is boosted by 2.5%.\n5 Ablations\nThis section presents three ablations: (1) using an\nalternative light embedder; (2) an attempt to align\nthe light and rich embedding spaces; (3) distillation\nof a rich embedder onto a light embedder.\nTable 2 presents the results of ERATE where the\naverage GloVe embeddings are used for the light\nembedder and the Sentence-T5-xxl model is used\nas the rich embedder. Here, an alternative light\nembedder is considered: the embedding associated\n6Observed on several examples.\n14\nwith the [CLS] token of the DistilBERT (Sanh\net al., 2019) model7. From Table 3, the ERATE ap-\nproach successfully out-competes the DistilBERT\nlight embedder by an encouraging 3.7% but it is\nstill worse performing than the ERATE approach\nwith the average GloVe embedder from Table 2.\nAvg. sts12 sts13 sts14 sts15 sts16 stsB sickR\nRich 84.8 78.9 88.9 84.9 89.3 84.7 86.7 80.4\nLight∗ 39.6 32.1 38.0 31.3 44.1 52.8 31.0 47.7\nERATE∗ 45.0 37.6 34.3 51.1 47.0 43.1 50.1 44.0\nTable 3: Performance with Sentence-T5 (Rich), Distil-\nBERT (Light∗), oracle neighbours and vanilla ERATE∗.\nERATE relies on combining the rich embeddings\nof the neighbours identified from a light embedding\nspace. Table 2 showed that the Oracle neighbours\nfrom the rich space substantially out-compete ER-\nATE. Hence, it is expected that if the neighbour\nsets between the light and rich spaces have greater\nagreement, there will be improved performance for\nERATE. A projection system is trained from the\naverage GloVe embedding space to the ST5-xxl\nembedding space for better alignment.\nSpaces P@1 P@10 P@100\nGloVe vs ST5 13.31 13.92 15.64\nProjected[GloVe] vs ST5 12.51 13.10 14.33\nTable 4: Impact of aligning light and rich spaces\nwith a projection layer using Precision@ K for K ∈\n{1, 10, 100}.\nThe projection model consists of an input layer\nfollowed by a ReLU followed by a single hidden\nlayer that predicts an embedding in the target em-\nbedding space with a cosine embedding loss. The\nvanilla datastore embeddings are used as the train-\ning data with 10% of the data cut-out for valida-\ntion. Table 4 assesses the improved alignment\nby applying the projection layer. The averaged\nPrecision@K is used as an assessment metric that\nmeasures the fraction of the closest K neighbours\nthat match in each space for a given query. Despite\nthat the model is trained to project the light space\nonto the rich space, there is degradation in the align-\nment of neighbours, possibly because the ordering\nof surrounding neighbours is not maintained in the\ntraining regime that impacts the retrieved neigh-\nbours.\nA distillation inspired approach is considered\nwhere a light embedding model aims to mimic the\n7Available at: https://huggingface.co/\ndistilbert-base-uncased\nembeddings of the rich Sentence-T5 model as an\nalternative strategy to ERATE. DistilBERT is se-\nlected as the light model 8. For every datastore\nembedding, the light model is finetuned (all pa-\nrameters) to predict the output embedding from\nthe rich model. The distilled model achieves an\naverage score on the STS tasks of 45.6% which\nis lower than the light model from Table 3. The\nlower performance may occur due to no emphasis\non maintaining semantic similarity explicitly.\n6 Conclusions\nRetrieval-based embeddings are proposed as ER-\nATE that bypass inference through an expensive\nembedding generation model but hope to leverage\nits richness. However, the current set-up for ER-\nATE achieves subpar performance on text similarity\ntasks with some gains observed from combining\nneighbours of a unique dropout mask approach and\nextending the datastore size of pre-computed light\nand rich embeddings for retrieval. We highlight\nmultiple areas of future work.\nFuture work should investigate ERATE-based\napproaches in a hybrid setting: ERATE embed-\ndings are used for sentences where they are likely\nto work effectively (neighbours are in a dense space\nallowing accurate approximations) while the de-\nfault expensive embedder can be used when ER-\nATE is unlikely to be effective. ERATE can be in-\ncreasingly effective when only partial information\nis available in a query for which an embedding is\ndesired as combining the embeddings of neighbour-\ning documents can enrich the information content.\nHowever, sentence-level embeddings offer little\nopportunity to explore the gains by additional infor-\nmation and hence future work should investigate\nthe scope of ERATE at the document-level; MTEB\n(Muennighoff et al., 2022) potentially offers suit-\nable tasks. We should also investigate alternative\napproaches for aligning the light and rich spaces\nand better combining neighbours’ embeddings e.g.\nself-attention.\nReferences\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\n8The GloVe model is not used as there is no availability to\nfinetune the model.\n15\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.\nSamuel Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, pages 632–\n642.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nDeng Cai, Yan Wang, Lemao Liu, and Shuming Shi.\n2022. Recent advances in retrieval-augmented text\ngeneration. In Proceedings of the 45th International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, pages 3417–3419.\nYung-Sung Chuang, Rumen Dangovski, Hongyin Luo,\nYang Zhang, Shiyu Chang, Marin Solja ˇci´c, Shang-\nWen Li, Wen-tau Yih, Yoon Kim, and James\nGlass. 2022. Diffcse: Difference-based contrastive\nlearning for sentence embeddings. arXiv preprint\narXiv:2204.10298.\nAlexis Conneau and Douwe Kiela. 2018a. SentEval:\nAn evaluation toolkit for universal sentence represen-\ntations. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nAlexis Conneau and Douwe Kiela. 2018b. Senteval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a\nbayesian approximation: Representing model uncer-\ntainty in deep learning. In international conference\non machine learning, pages 1050–1059. PMLR.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6894–6910.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-wei Chang. 2020. Realm: Retrieval-\naugmented language model pre. Training.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nNora Kassner and Hinrich Schütze. 2020. Bert-knn:\nAdding a knn search component to pretrained lan-\nguage models for better qa. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020,\npages 3424–3430.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. arXiv preprint arXiv:1911.00172.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Effi-\ncient and effective passage search via contextualized\nlate interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR conference on research\nand development in Information Retrieval, pages 39–\n48.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nNiklas Muennighoff, Nouamane Tazi, Loïc Magne, and\nNils Reimers. 2022. Mteb: Massive text embedding\nbenchmark. arXiv preprint arXiv:2210.07316.\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-\nford, Jesse Michael Han, Jerry Tworek, Qiming\nYuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy,\nJohannes Heidecke, Pranav Shyam, Boris Power,\nTyna Eloundou Nekoul, Girish Sastry, Gretchen\nKrueger, David Schnurr, Felipe Petroski Such, Kenny\nHsu, Madeleine Thompson, Tabarak Khan, Toki\nSherbakov, Joanne Jang, Peter Welinder, and Lilian\nWeng. 2022. Text and code embeddings by con-\ntrastive pre-training.\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant,\nJi Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022.\nSentence-t5: Scalable sentence encoders from pre-\ntrained text-to-text models. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 1864–1874.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(EMNLP), pages 1532–1543.\n16\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3982–3992.\nJon Saad-Falcon, Amanpreet Singh, Luca Soldaini,\nMike D’Arcy, Arman Cohan, and Doug Downey.\n2022. Embedding recycling for language models.\narXiv preprint arXiv:2207.04993.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nKeshav Santhanam, Omar Khattab, Christopher Potts,\nand Matei Zaharia. 2022. Plaid: an efficient engine\nfor late interaction retrieval. In Proceedings of the\n31st ACM International Conference on Information\n& Knowledge Management, pages 1747–1756.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overfitting. The journal of machine learning\nresearch, 15(1):1929–1958.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122.\nYuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang,\nWei Wu, and Weiran Xu. 2021. Consert: A con-\ntrastive framework for self-supervised sentence repre-\nsentation transfer. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5065–5075.\n.\n17\nAppendix A Limitations\nThe experiments for ERATE are currently limited\nto the semantic text similarity tasks of SentEval.\nMore comprehensive experiments should investi-\ngate the applicability of ERATE against benchmark\ntext embedding representations for a wide range of\ndownstream NLP tasks.\nAppendix B Computational resources\nAll experiments were conducted using NVIDIA\nA100 graphical processing units.\nAppendix C Reproducibility\nThe experiments conducted in this work has only\nrelied on publicly available data and publicly avail-\nable models. There was no additional training of\nmodels. Additional hyperparameters for ERATE\nembeddings (e.g. the size of the datastore, the num-\nber of neighbours, the dropout rate) is detailed in\nthe relevant sections of the main paper.\nAppendix D Licenses\nThis section details the license agreements of the\nscientific artifacts used in this work. The Stan-\nford Natural Language Inference (SNLI) Corpus\nis licensed under a Creative Commons Attribution-\nShareAlike 4.0 International License. For MNLI,\nthe majority of the corpus is released under the\nOANC’s license, which allows all content to be\nfreely used, modified, and shared under permissive\nterms. The data in the FICTION section falls under\nseveral permissive licenses; Seven Swords is avail-\nable under a Creative Commons Share-Alike 3.0\nUnported License, and with the explicit permission\nof the author, Living History and Password Incor-\nrect are available under Creative Commons Attribu-\ntion 3.0 Unported Licenses; the remaining works of\nfiction are in the public domain in the United States\n(but may be licensed differently elsewhere). SentE-\nval is released under the BSD License. Common\nCrawl is released under the MIT License.\nAppendix E Additional experiments\nIn the main paper, ERATE relies on combining the\nrich embedding representations of the neighbours\nthat have been identified using the light embedding\nrepresentations. The number of neighbours was set\nto 100. In this section, the impact on the down-\nstream STS tasks is investigated when a different\nnumber of neighbours are considered instead. Ta-\nble Appendix E.1 details the performance when\nusing a different number of neighbours from the\ndatastore. The best averaged results are observed\nempirically when 100 neighbours are used from\nthe datastore.\n#neigh. Avg. sts12 sts13 sts14 sts15 sts16 stsB sickR\n1 40.9 30.2 40.7 35.1 50.8 43.7 39.8 46.0\n10 52.4 51.6 52.9 43.5 61.9 49.1 53.8 54.0\n100 55.3 57.2 59.7 47.3 59.9 54.5 53.8 54.7\n1000 54.7 54.2 58.8 48.6 61.3 54.0 52.3 53.8\n10,000 52.4 51.2 57.3 46.9 59.1 50.9 49.1 52.4\nTable Appendix E.1: Varying the number of neighbours\nfor ERATE.\n18",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7953608632087708
    },
    {
      "name": "Embedding",
      "score": 0.7920880317687988
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6547802686691284
    },
    {
      "name": "Feature learning",
      "score": 0.5491032600402832
    },
    {
      "name": "Transformer",
      "score": 0.5222930908203125
    },
    {
      "name": "Representation (politics)",
      "score": 0.5205215215682983
    },
    {
      "name": "Information retrieval",
      "score": 0.5070436000823975
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48161858320236206
    },
    {
      "name": "Natural language processing",
      "score": 0.4604180157184601
    },
    {
      "name": "Focus (optics)",
      "score": 0.44492650032043457
    },
    {
      "name": "Sentence",
      "score": 0.42930203676223755
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3393508791923523
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": []
}