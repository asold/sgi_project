{
    "title": "Exponential Reservoir Sampling for Streaming Language Models",
    "url": "https://openalex.org/W2150299430",
    "year": 2014,
    "authors": [
        {
            "id": "https://openalex.org/A2041005256",
            "name": "Miles Osborne",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2110873469",
            "name": "Ashwin Lall",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2113965177",
            "name": "Benjamin Van Durme",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2102439588",
        "https://openalex.org/W2171458318",
        "https://openalex.org/W2163546841",
        "https://openalex.org/W2120804083",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2044062612",
        "https://openalex.org/W1562125942",
        "https://openalex.org/W2250615155",
        "https://openalex.org/W2146867136",
        "https://openalex.org/W2106540279",
        "https://openalex.org/W2053154469",
        "https://openalex.org/W2065512234",
        "https://openalex.org/W2119885577",
        "https://openalex.org/W1975690018"
    ],
    "abstract": "We show how rapidly changing textual streams such as Twitter can be modelled in fixed space.Our approach is based upon a randomised algorithm called Exponential Reservoir Sampling, unexplored by this community until now.Using language models over Twitter and Newswire as a testbed, our experimental results based on perplexity support the intuition that recently observed data generally outweighs that seen in the past, but that at times, the past can have valuable signals enabling better modelling of the present.",
    "full_text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 687–692,\nBaltimore, Maryland, USA, June 23-25 2014.c⃝2014 Association for Computational Linguistics\nExponential Reservoir Sampling for Streaming Language Models\nMiles Osborne∗\nSchool of Informatics\nUniversity of Edinburgh\nAshwin Lall\nMathematics and Computer Science\nDenison University\nBenjamin Van Durme\nHLTCOE\nJohns Hopkins University\nAbstract\nWe show how rapidly changing textual\nstreams such as Twitter can be modelled in\nﬁxed space. Our approach is based upon\na randomised algorithm called Exponen-\ntial Reservoir Sampling, unexplored by\nthis community until now. Using language\nmodels over Twitter and Newswire as a\ntestbed, our experimental results based on\nperplexity support the intuition that re-\ncently observed data generally outweighs\nthat seen in the past, but that at times,\nthe past can have valuable signals enabling\nbetter modelling of the present.\n1 Introduction\nWork by Talbot and Osborne (2007), Van Durme\nand Lall (2009) and Goyal et al. (2009) consid-\nered the problem of building very large language\nmodels via the use of randomized data structures\nknown as sketches.1 While efﬁcient, these struc-\ntures still scale linearly in the number of items\nstored, and do not handle deletions well: if pro-\ncessing an unbounded stream of text, with new\nwords and phrases being regularly added to the\nmodel, then with a ﬁxed amount of space, errors\nwill only increase over time. This was pointed\nout by Levenberg and Osborne (2009), who inves-\ntigated an alternate approach employing perfect-\nhashing to allow for deletions over time. Their\ndeletion criterion was task-speciﬁc and based on\nhow a machine translation system queried a lan-\nguage model.\n∗Corresponding author: miles@inf.ed.ac.uk\n1Sketches provide space efﬁciencies that are measured on\nthe order of individual bits per item stored, but at the cost\nof being lossy: sketches trade off space for error, where the\nless space you use, the more likely you will get erroneous\nresponses to queries.\nHere we ask what the appropriate selection\ncriterion is for streaming data based on a non-\nstationary process, when concerned with an in-\ntrinsic measure such as perplexity. Using Twitter\nand newswire, we pursue this via a sampling strat-\negy: we construct models over sentences based on\na sample of previously observed sentences, then\nmeasure perplexity of incoming sentences, all on\na day by day, rolling basis. Three sampling ap-\nproaches are considered: A ﬁxed-width sliding\nwindow of most recent content, uniformly at ran-\ndom over the stream and a biased sample that\nprefers recent history over the past.\nWe show experimentally that a moving window\nis better than uniform sampling, and further that\nexponential (biased) sampling is best of all. For\nstreaming data, recently encountered data is valu-\nable, but there is also signal in the previous stream.\nOur sampling methods are based on reser-\nvoir sampling (Vitter, 1985), a popularly known\nmethod in some areas of computer science, but\nwhich has seen little use within computational lin-\nguistics.2 Standard reservoir sampling is a method\nfor maintaining a uniform sample over a dynamic\nstream of elements, using constant space. Novel\nto this community, we consider a variant owing to\nAggarwal (2006) which provides for an exponen-\ntial bias towards recently observed elements. This\nexponential reservoir samplinghas all of the guar-\nantees of standard reservoir sampling, but as we\nshow, is a better ﬁt for streaming textual data. Our\napproach is fully general and can be applied to any\nstreaming task where we need to model the present\nand can only use ﬁxed space.\n2Exceptions include work by Van Durme and Lall (2011)\nand Van Durme (2012), aimed at different problems than that\nexplored here.\n687\n2 Background\nWe address two problems: language changes over\ntime, and the observation that space is a problem,\neven for compact sketches.\nStatistical language models often assume either\na local Markov property (when working with ut-\nterances, or sentences), or that content is gener-\nated fully i.i.d. (such as in document-level topic\nmodels). However, language shows observable\npriming effects, sometimes called triggers, where\nthe occurrence of a given term decreases the sur-\nprisal of some other term later in the same dis-\ncourse (Lau et al., 1993; Church and Gale, 1995;\nBeeferman et al., 1997; Church, 2000). Conven-\ntional cache and trigger models typically do not\ndeal with new terms and can be seen as adjusting\nthe parameters of a ﬁxed model.\nAccounting for previously unseen entries in a\nlanguage model can be naively simple: as they ap-\npear in new training data, add them to the model!\nHowever in practice we are constrained by avail-\nable space: how many unique phrases can we\nstore, given the target application environment?\nOur work is concerned with modeling language\nthat might change over time, in accordance with\ncurrent trending discourse topics, but under a strict\nspace constraint. With a ﬁxed amount of memory\navailable, we cannot allow our list of unique words\nor phrases to grow over time, even while new top-\nics give rise to novel names of people, places, and\nterms of interest. Thus we need an approach that\nkeeps the size of the model constant, but that is\ngeared to what is being discussed now, as com-\npared to some time in the past.\n3 Reservoir Sampling\n3.1 Uniform Reservoir Sampling\nThe reservoir sampling algorithm (Vitter, 1985) is\nthe classic method of sampling without replace-\nment from a stream in a single pass when the\nlength of the stream is of indeterminate or un-\nbounded length. Say that the size of the desired\nsample is k. The algorithm proceeds by retain-\ning the ﬁrst k items of the stream and then sam-\npling each subsequent element with probability\nf(k,n) =k/n, where nis the length of the stream\nso far. (See Algorithm 1.) It is easy to show via in-\nduction that, at any time, all the items in the stream\nso far have equal probability of appearing in the\nreservoir.\nThe algorithm processes the stream in a single\npass—that is, once it has processed an item in the\nstream, it does not revisit that item unless it is\nstored in the reservoir. Given this restriction, the\nincredible feature of this algorithm is that it is able\nto guarantee that the samples in the reservoir are a\nuniformly random sample with no unintended bi-\nases even as the stream evolves. This makes it an\nexcellent candidate for situations when the stream\nis continuously being updated and it is computa-\ntionally infeasible to store the entire stream or to\nmake more than a single pass over it. Moreover,\nit is an extremely efﬁcient algorithm as it requires\nO(1) time (independent of the reservoir size and\nstream length) for each item in the stream.\nAlgorithm 1Reservoir Sampling Algorithm\nParameters:\nk: maximum size of reservoir\n1: Initialize an empty reservoir (any container\ndata type).\n2: n:= 1\n3: for each item in the stream do\n4: if n<k then\n5: insert current item into the reservoir\n6: else\n7: with probability f(n,k), eject an ele-\nment of the reservoir chosen uniformly\nat random and insert current item into the\nreservoir\n8: n:= n+ 1\n3.2 Non-uniform Reservoir Sampling\nHere we will consider generalizations of the reser-\nvoir sampling algorithm in which the sample\nitems in the reservoir are more biased towards the\npresent. Put another way, we will continuously\ndecay the probability that an older item will ap-\npear in the reservoir. Models produced using such\nbiases put more modelling stress on the present\nthan models produced using data that is selected\nuniformly from the stream. The goal here is to\ncontinuously update the reservoir sample in such\na way that the decay of older items is done consis-\ntently while still maintaining the beneﬁts of reser-\nvoir sampling, including the single pass and mem-\nory/time constraints.\nThe time-decay scheme we will study in this\npaper is exponential biastowards newer items in\nthe stream. More precisely, we wish for items that\n688\n0 2000 4000 6000 8000 10000\ntime\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0probability of appearing in reservoir\nuniform\nexponential (various beta)\nFigure 1: Different biases for sampling a stream\nhave age ain the stream to appear with probability\ng(a) =c·exp (−a/β),\nwhere ais the age of the item, βis a scale param-\neter indicating how rapidly older items should be\ndeemphasized, and cis a normalization constant.\nTo give a sense of what these time-decay proba-\nbilities look like, some exponential distributions\nare plotted (along with the uniform distribution)\nin Figure 1.\nAggarwal (2006) studied this problem and\nshowed that by altering the sampling probability\n(f(n,k) in Algorithm 1) in the reservoir sampling\nalgorithm, it is possible to achieve different age-\nrelated biases in the sample. In particular, he\nshowed that by setting the sampling probability to\nthe constant function f(n,k) =k/β, it is possible\nto approximately achieve exponential bias in the\nsample with scale parameter β (Aggarwal, 2006).\nAggarwal’s analysis relies on the parameter βbe-\ning very large. In the next section we will make\nthe analysis more precise by omitting any such as-\nsumption.\n3.3 Analysis\nIn this section we will derive an expression for the\nbias introduced by an arbitrary sampling function\nf in Algorithm 1. We will then use this expression\nto derive the precise sampling function needed to\nachieve exponential decay.3 Careful selection of\nf allows us to achieve anything from zero decay\n(i.e., uniform sampling of the entire stream) to\nexponential decay. Once again, note that since\nwe are only changing the sampling function, the\n3Specifying an arbitrary decay function remains an open\nproblem.\none-pass, memory- and time-efﬁcient properties\nof reservoir sampling are still being preserved.\nIn the following analysis, we ﬁx nto be the size\nof the stream at some ﬁxed time and k to be the\nsize of the reservoir. We assume that the ith el-\nement of the stream is sampled with probability\nf(i,k), for i ≤n. We can then derive the proba-\nbility that an element of age awill still be in the\nreservoir as\ng(a) =f(n−a,k)\nn∏\nt=n−a+1\n(\n1 −f(t,k)\nk\n)\n,\nsince it would have been sampled with probability\nf(n−a,k) and had independent chances of being\nreplaced at times t= n−a+1,...,n with proba-\nbility f(t,k)/k. For instance, when f(x,k) = k\nx,\nthe above formula simpliﬁes down to g(a) = k\nn\n(i.e., the uniform sampling case).\nFor the exponential case, we ﬁx the sampling\nrate to some constant f(n,k) = pk, and we wish\nto determine what value to use for pk to achieve\na given exponential decay rate g(a) = ce−a/β,\nwhere cis the normalization constant (to make ga\nprobability distribution) and β is the scale param-\neter of the exponential distribution. Substituting\nf(n,k) = pk in the above formula and equating\nwith the decay rate, we get that pk(1 −pk/k)a ≡\nce−a/β, which must hold true for all possible val-\nues of a. After some algebra, we get that when\nf(x,k) = pk = k(1 −e−1/β), the probability\nthat an item with age a is included in the reser-\nvoir is given by the exponential decay rate g(a) =\npke−a/β. Note that, for very large values ofβ, this\nprobability is approximately equal to pk ≈k/β\n(by using the approximation e−x ≈1 −x, when\n|x|is close to zero), as given by Aggarwal, but our\nformula gives the precise sampling probability and\nworks even for smaller values of β.\n4 Experiments\nOur experiments use two streams of data to illus-\ntrate exponential sampling: Twitter and a more\nconventional newswire stream. The Twitter data is\ninteresting as it is very multilingual, bursty (for ex-\nample, it talks about memes, breaking news, gos-\nsip etc) and written by literally millions of differ-\nent people. The newswire stream is a lot more well\nbehaved and serves as a control.\n4.1 Data, Models and Evaluation\nWe used one month of chronologically ordered\nTwitter data and divided it into 31 equal sized\n689\nStream Interval Total (toks) Test (toks)\nTwitter Dec 2013 3282M 105M\nGiga 1994 – 2010 635.5M 12M\nTable 1: Stream statistics\nblocks (roughly corresponding with days). We\nalso used the AFP portion of the Giga Word corpus\nas another source of data that evolves at a slower\npace. This data was divided into 50 equal sized\nblocks. Table 1 gives statistics about the data. As\ncan be seen, the Twitter data is vastly larger than\nnewswire and arrives at a much faster rate.\nWe considered the following models. Each one\n(apart from the exact model) was trained using the\nsame amount of data:\n•Static. This model was trained using data\nfrom the start of the duration and never var-\nied. It is a baseline.\n•Exact. This model was trained using all\navailable data from the start of the stream and\nacts as an upper bound on performance.\n•Moving Window. This model used all data\nin a ﬁxed-sized window immediately before\nthe given test point.\n•Uniform. Here, we use uniform reservoir\nsampling to select the data.\n•Exponential. Lastly, we use exponen-\ntial reservoir sampling to select the data.\nThis model is parameterised, indicating how\nstrongly biased towards the present the sam-\nple will be. The β parameter is a multiplier\nover the reservoir length. For example, a β\nvalue of 1.1 with a sample size of 10 means\nthe value is 11. In general, βalways needs to\nbe bigger than the reservoir size.\nWe sample over whole sentences (or Tweets)\nand not ngrams. 4 Using ngrams instead would\ngive us a ﬁner-grained control over results, but\nwould come at the expense of greatly complicat-\ning the analysis. This is because we would need to\nreason about not just a set of items but a multiset\nof items. Note that because the samples are large5,\nvariations across samples will be small.\n4A consequence is that we do not guarantee that each sam-\nple uses exactly the same number of grams. This can be tack-\nled by randomly removing sampled sentences.\n5Each day consists of approximately four million Tweets\nand we evaluate on a whole day.\nDay Uniform β value\n∞ 1.1 1.3 1.5 2.0\n5 619.4 619.4 619.4 619.4 619.4\n6 601.0 601.0 603.8 606.6 611.1\n7 603.0 599.4 602.7 605.6 612.1\n8 614.6 607.7 611.9 614.3 621.6\n9 623.3 611.5 615.0 620.0 628.1\n10 656.2 643.1 647.2 650.1 658.0\n12 646.6 628.9 633.0 636.5 644.6\n15 647.7 628.7 630.4 634.5 641.6\n20 636.7 605.3 608.4 610.8 618.4\n25 631.5 601.9 603.3 604.4 610.0\nTable 2: Perplexities for different β values over\nTwitter (sample size = ﬁve days). Lower is better.\nWe test the model on unseen data from all of the\nnext day (or block). Afterwards, we advance to the\nnext day (block) and repeat, potentially incorpo-\nrating the previously seen test data into the current\ntraining data. Evaluation is in terms of perplexity\n(which is standard for language modelling).\nWe used KenLM for building models and eval-\nuating them (Heaﬁeld, 2011). Each model was\nan unpruned trigram, with Kneser-Ney smoothing.\nIncreasing the language model order would not\nchange the results. Here the focus is upon which\ndata is used in a model (that is, which data is added\nand which data is removed) and not upon making\nit compact or making retraining efﬁcient.\n4.2 Varying the βParameter\nTable 2 shows the effect of varying the β param-\neter (using Twitter). The higher the β value, the\nmore uniform the sampling. As can be seen, per-\nformance improves when sampling becomes more\nbiased. Not shown here, but for Twitter, even\nsmaller β values produce better results and for\nnewswire, results degrade. These differences are\nsmall and do not affect any conclusions made here.\nIn practise, this value would be set using a devel-\nopment set and to simplify the rest of the paper, all\nother experiments use the same βvalue (1.1).\n4.3 Varying the Amount of Data\nDoes the amount of data used in a model affect re-\nsults? Table 3 shows the results for Twitter when\nvarying the amount of data in the sample and us-\ning exponential sampling ( β = 1.1). In paren-\ntheses for each result, we show the corresponding\nmoving window results. As expected, using more\ndata improves results. We see that for each sample\nsize, exponential sampling outperforms our mov-\ning window. In the limit, all sampling methods\nwould produce the same results.\n690\nDay Sample Size (Days)\n1 2 3\n5 652.5 (661.2) 629.1 (635.8) 624.8 (625.9)\n6 635.4 (651.6) 611.6 (620.8) 604.0 (608.7)\n7 636.0 (647.3) 611.0 (625.2) 603.7 (612.5)\n8 654.8 (672.7) 625.6 (641.6) 614.6 (626.9)\n9 653.9 (662.8) 628.3 (643.0) 618.8 (632.2)\n10 679.1 (687.8) 654.3 (666.8) 646.6 (659.7)\n12 671.1 (681.9) 645.8 (658.6) 633.8 (647.5)\n15 677.7 (697.9) 647.4 (668.0) 636.4 (652.6)\n20 648.1 (664.6) 621.4 (637.9) 612.2 (627.6)\n25 657.5 (687.5) 625.3 (664.4) 613.4 (641.8)\nTable 3: Perplexities for different sample sizes\nover Twitter. Lower is better.\n4.4 Alternative Sampling Strategies\nTable 4 compares the two baselines against the two\nforms of reservoir sampling. For Twitter, we see\na clear recency effect. The static baseline gets\nworse and worse as it recedes from the current\ntest point. Uniform sampling does better, but it\nin turn is beaten by the Moving Window Model.\nHowever, this in turn is beaten by our exponential\nreservoir sampling.\nDay Static Moving Uniform Exp Exact\n5 619.4 619.4 619.4 619.4 619.4\n6 664.8 599.7 601.8 601.0 597.6\n7 684.4 602.8 603.0 599.3 595.6\n8 710.1 612.0 614.6 607.7 603.5\n9 727.0 617.9 623.3 613.0 608.7\n10 775.6 651.2 656.2 642.0 640.5\n12 776.7 639.0 646.6 628.7 627.5\n15 777.1 638.3 647.7 626.7 627.3\n20 800.9 619.1 636.7 604.9 607.3\n25 801.4 621.7 631.5 601.5 597.6\nTable 4: Perplexities for differently selected sam-\nples over Twitter (sample size = ﬁve days, β =\n1.1). Results in bold are the best sampling results.\nLower is better.\n4.5 GigaWord\nTwitter is a fast moving, rapidly changing multi-\nlingual stream and it is not surprising that our ex-\nponential reservoir sampling proves beneﬁcial. Is\nit still useful for a more conventional stream that\nis drawn from a much smaller population of re-\nporters? We repeated our experiments, using the\nsame rolling training and testing evaluation as be-\nfore, but this time using newswire for data.\nTable 5 shows the perplexities when using the\nGigaword stream. We see the same general trends,\nalbeit with less of a difference between exponen-\ntial sampling and our moving window. Perplexity\nvalues are all lower than for Twitter.\nBlock Static Moving Uniform Exp\n11 416.5 381.1 382.0 382.0\n15 436.7 353.3 357.5 352.8\n20 461.8 347.0 354.4 344.6\n25 315.6 214.9 222.2 211.3\n30 319.1 200.5 213.5 199.5\n40 462.5 304.4 313.2 292.9\nTable 5: Perplexities for differently selected sam-\nples over Gigaword (sample size = 10 blocks,β =\n1.1). Lower is better.\n4.6 Why does this work for Twitter?\nAlthough the perplexity results demonstrate that\nexponential sampling is on average beneﬁcial, it\nis useful to analyse the results in more detail. For\na large stream size (25 days), we built models us-\ning uniform, exponential (β= 1.1) and our moving\nwindow sampling methods. Each approach used\nthe same amount of data. For the same test set\n(four million Tweets), we computed per-Tweet log\nlikelihoods and looked at the difference between\nthe model that best explained each tweet and the\nsecond best model (ie the margin). This gives us\nan indication of how much a given model better\nexplains a given Tweet. Analysing the results, we\nfound that most gains came from short grams and\nvery few came from entire Tweets being reposted\n(or retweeted). This suggests that the Twitter re-\nsults follow previously reported observations on\nhow language can be bursty and not from Twitter-\nspeciﬁc properties.\n5 Conclusion\nWe have introduced exponential reservoir sam-\npling as an elegant way to model a stream of un-\nbounded size, yet using ﬁxed space. It naturally al-\nlows one to take account of recency effects present\nin many natural streams. We expect that our lan-\nguage model could improve other Social Media\ntasks, for example lexical normalisation (Han and\nBaldwin, 2011) or even event detection (Lin et\nal., 2011). The approach is fully general and not\njust limited to language modelling. Future work\nshould look at other distributions for sampling and\nconsider tasks such as machine translation over\nSocial Media.\nAcknowledgments This work was carried out\nwhen MO was on sabbatical at the HLTCOE and\nCLSP.\n691\nReferences\nCharu C Aggarwal. 2006. On biased reservoir sam-\npling in the presence of stream evolution. In Pro-\nceedings of the 32nd international conference on\nVery large data bases, pages 607–618. VLDB En-\ndowment.\nDoug Beeferman, Adam Berger, and John Lafferty.\n1997. A model of lexical attractions and repulsion.\nIn Proceedings of the 35th Annual Meeting of the As-\nsociation for Computational Linguistics and Eighth\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 373–380.\nAssociation for Computational Linguistics.\nK. Church and W. A. Gale. 1995. Poisson mixtures.\nNatural Language Engineering, 1:163–190.\nKenneth W Church. 2000. Empirical estimates of\nadaptation: the chance of two noriegas is closer to\np/2 than p 2. In Proceedings of the 18th conference\non Computational linguistics-Volume 1, pages 180–\n186. Association for Computational Linguistics.\nAmit Goyal, Hal Daum ´e III, and Suresh Venkatasub-\nramanian. 2009. Streaming for large scale NLP:\nLanguage Modeling. In Proceedings of NAACL.\nBo Han and Timothy Baldwin. 2011. Lexical normal-\nisation of short text messages: Makn sens a #twit-\nter. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies - Volume 1, HLT ’11,\npages 368–378, Stroudsburg, PA, USA. Association\nfor Computational Linguistics.\nKenneth Heaﬁeld. 2011. KenLM: faster and smaller\nlanguage model queries. In Proceedings of the\nEMNLP 2011 Sixth Workshop on Statistical Ma-\nchine Translation, pages 187–197, Edinburgh, Scot-\nland, United Kingdom, July.\nRaymond Lau, Ronald Rosenfeld, and SaIim Roukos.\n1993. Trigger-based language models: A maximum\nentropy approach. In Acoustics, Speech, and Signal\nProcessing, 1993. ICASSP-93., 1993 IEEE Interna-\ntional Conference on, volume 2, pages 45–48. IEEE.\nAbby Levenberg and Miles Osborne. 2009. Stream-\nbased randomised language models for smt. In Pro-\nceedings of the 2009 Conference on Empirical Meth-\nods in Natural Language Processing: Volume 2-\nVolume 2, pages 756–764. Association for Compu-\ntational Linguistics.\nJimmy Lin, Rion Snow, and William Morgan. 2011.\nSmoothing techniques for adaptive online language\nmodels: topic tracking in tweet streams. In Proceed-\nings of the 17th ACM SIGKDD international con-\nference on Knowledge discovery and data mining,\npages 422–429. ACM.\nDavid Talbot and Miles Osborne. 2007. Randomised\nlanguage modelling for statistical machine transla-\ntion. In Proceedings of ACL.\nBenjamin Van Durme and Ashwin Lall. 2009. Proba-\nbilistic Counting with Randomized Storage. In Pro-\nceedings of IJCAI.\nBenjamin Van Durme and Ashwin Lall. 2011. Efﬁ-\ncient online locality sensitive hashing via reservoir\ncounting. In Proceedings of the 49th Annual Meet-\ning of the Association for Computational Linguis-\ntics: Human Language Technologies: short papers-\nVolume 2, pages 18–23. Association for Computa-\ntional Linguistics.\nBenjamin Van Durme. 2012. Streaming analysis of\ndiscourse participants. In Proceedings of the 2012\nJoint Conference on Empirical Methods in Natural\nLanguage Processing and Computational Natural\nLanguage Learning, pages 48–58. Association for\nComputational Linguistics.\nJeffrey S. Vitter. 1985. Random sampling with a reser-\nvoir. ACM Trans. Math. Softw., 11:37–57, March.\n692"
}