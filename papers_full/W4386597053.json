{
    "title": "LT-ViT: A Vision Transformer for Multi-Label Chest X-Ray Classification",
    "url": "https://openalex.org/W4386597053",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4287139659",
            "name": "Marikkar, Umar",
            "affiliations": [
                "University of Surrey"
            ]
        },
        {
            "id": "https://openalex.org/A4227480360",
            "name": "Atito, Sara",
            "affiliations": [
                "University of Surrey"
            ]
        },
        {
            "id": "https://openalex.org/A2288933954",
            "name": "Awais Muhammad",
            "affiliations": [
                "University of Surrey"
            ]
        },
        {
            "id": "https://openalex.org/A4224979161",
            "name": "Mahdi, Adam",
            "affiliations": [
                "University of Surrey"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6784333009",
        "https://openalex.org/W4296434097",
        "https://openalex.org/W4221163766",
        "https://openalex.org/W6846055702",
        "https://openalex.org/W4385573131",
        "https://openalex.org/W4312266387",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W3167456680",
        "https://openalex.org/W6798617289",
        "https://openalex.org/W4312642299",
        "https://openalex.org/W4312980231",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3092743670",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W2611650229",
        "https://openalex.org/W2963466845",
        "https://openalex.org/W4386598334",
        "https://openalex.org/W4296960042",
        "https://openalex.org/W4307972630",
        "https://openalex.org/W2995225687",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W4394659038",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W4308847350",
        "https://openalex.org/W4306177947",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4309956142",
        "https://openalex.org/W3101156210",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3184087575"
    ],
    "abstract": "Vision Transformers (ViTs) are widely adopted in medical imaging tasks, and\\nsome existing efforts have been directed towards vision-language training for\\nChest X-rays (CXRs). However, we envision that there still exists a potential\\nfor improvement in vision-only training for CXRs using ViTs, by aggregating\\ninformation from multiple scales, which has been proven beneficial for\\nnon-transformer networks. Hence, we have developed LT-ViT, a transformer that\\nutilizes combined attention between image tokens and randomly initialized\\nauxiliary tokens that represent labels. Our experiments demonstrate that LT-ViT\\n(1) surpasses the state-of-the-art performance using pure ViTs on two publicly\\navailable CXR datasets, (2) is generalizable to other pre-training methods and\\ntherefore is agnostic to model initialization, and (3) enables model\\ninterpretability without grad-cam and its variants.\\n",
    "full_text": "LT-VIT: A VISION TRANSFORMER FOR MULTI-LABEL CHEST X-RAY CLASSIFICATION\nUmar Marikkar1 Sara Atito1,2 Muhammad Awais1,2 Adam Mahdi3\n1Surrey Institute of People Centered AI, University of Surrey,\n2Centre of Vision, Speech and Signal Processing (CVSSP), University of Surrey,\n3Oxford Internet Institute, University of Oxford\nABSTRACT\nVision Transformers (ViTs) are widely adopted in medical\nimaging tasks, and some existing efforts have been directed\ntowards vision-language training for Chest X-rays (CXRs).\nHowever, we envision that there still exists a potential for\nimprovement in vision-only training for CXRs using ViTs,\nby aggregating information from multiple scales, which has\nbeen proven beneficial for non-transformer networks. Hence,\nwe have developed LT-ViT, a transformer that utilizes com-\nbined attention between image tokens and randomly initial-\nized auxiliary tokens that represent labels. Our experiments\ndemonstrate that LT-ViT (1) surpasses the state-of-the-art per-\nformance using pure ViTs on two publicly available CXR\ndatasets, (2) is generalizable to other pre-training methods\nand therefore is agnostic to model initialization, and (3) en-\nables model interpretability without grad-cam and its variants.\nIndex Terms— transformers, medical imaging, multi-\nlabel classification\n1. INTRODUCTION\nWith the advent of Vision Transformers (ViTs) [1] for com-\nputer vision tasks, many studies have applied ViTs to medical\nimages, including 2D images—radiographs [2] and 3D vol-\numes—MRI scans [3]. More recently, Chest X-Ray (CXR)\nbased image classification benchmarks have been fuelled by\nvision-language training, where CXRs use their correspond-\ning radiology reports as labels [4, 5, 6].\nThe attention mechanism inherently present in the trans-\nformer architecture [7] allows for learnable interaction be-\ntween tokens (feature vectors) in such a way that a single\ntoken can process information from any other token within\nor out of the same domain. Leveraging this, BERT [8] in-\ntroduced a class token [CLS], which aims to aggregate in-\nformation from, and propagate information to all the domain-\nspecific data tokens. Subsequently, the common practice in\nthe vision community is to utilize this class token that aggre-\ngates the global information to predict the output of the given\ntask, e.g., multi-class classification [1].\nUnlike multi-class classification problems, it is impera-\ntive for the multi-label classification tasks that the network\nis able to understand the connections between each label,\nsuch that it is able to infer information about a specific label\nfrom another. Using the transformer architecture, C-Tran\n[9] proposes a transformer encoder to model dependencies\nbetween labels and data, whilst Query2label [10] utilizes a\ntransformer decoder to do so. Both these studies use auxiliary\ntokens [11, 12, 13] to model dependencies, however, C-Tran\nenables bi-directional attention between data and labels, and\nQuery2label is built upon a framework where the auxiliary to-\nkens only attend to fully encoded data tokens. For the former,\nthis results in data tokens having to be transferred into a data-\nimage hybrid space, thereby damaging its domain-specificity,\nand in the latter, the auxiliary tokens only attend to one set of\ndata tokens, thereby preventing them from learning from the\ndata at multiple scales.\nA shortcoming noted from the aforementioned studies is\nthat they use a full vision backbone and then apply the aux-\niliary tokens to model dependencies. However, we note that\nfor medical images, information aggregation from multiple\nscales is predominant for better multi-label classification per-\nformance [14]. To this end, we propose Label Token Vision\nTransformer—LT-ViT, a simple transformer architecture de-\nsigned for multi-label CXR classification. LT-ViT enables\njoint learning of auxiliary label and data tokens, where the\nlabel tokens attend to data tokens within the actual vision\nbackbone. This enables multi-scale learning and avoids ex-\ntra layers of compute. Using LT-ViT, we propose to improve\nvision-only training using transformers for CXRs, that would\nresult in the potential generalizability towards non-CXR med-\nical images, where image-text paired data is not present. This\npaper aims to show the following:\n1. LT-ViT surpasses state-of-the-art performance using\npure ViTs on two publicly available CXR datasets\n(Section 4.1).\n2. LT-ViT is generalizable to other pre-training methods\nand therefore is agnostic to model initialization (Sec-\ntion 4.2).\n3. LT-ViT enables model interpretability without grad-\ncam [15] and its variants (Section 4.3).\narXiv:2311.07263v1  [cs.CV]  13 Nov 2023\nx N1\nx N2\nMSA MSA + MXA\n Q, K, V  K,V  Q, K, V \nFFN+ +\nViT block\n Q, K, V \nprediction\nK * Linear(D, 1)\n+ +\nFig. 1: LT-ViT architecture using auxiliary label tokens. [+]—skip connection. MSA—multi-head self-attention, MXA—multi-\nhead cross-attention. QKV denote Query, Key, and Value. In the LT-ViT blocks, [LBL] act as queries to all tokens within the\nnetwork, whilst the [IMG] act as queries only to [IMG].\n2. METHODOLOGY\nIn this paper, we propose LT-ViT, a generic framework for\nmulti-label medical image classification that leverages ViTs\nto exploit the dependencies among visual features and labels,\nwhich therefore improve the explainability of the predictions.\nWe briefly explain vision transformers in Section 2.1. Next,\nin Section 2.2, we explain the overall architecture of LT-ViT\nfor multi-label classification.\n2.1. Vision Transformers (ViTs)\nViT [1] receives as input a sequence of patches obtained by\ntokenizing the input image x ∈ RH×W×C into n flattened\n2D patches of size p × p × C pixels, where H, W, and C\nare the height, width, and the number of channels of the input\nimage and n is the total number of patches. Each patch is then\nprojected with a linear layer to D hidden dimensions. In or-\nder to retain the relative spatial relation between the patches,\nlearnable position embeddings are added to the patch em-\nbeddings as an input to the transformer encoder. Further, a\nclass token [CLS] ∈ R1×D is concatenated to the data to-\nkens, where its output is used to represent the entire image.\nThe standard transformer encoder consists of L consecutive\nmulti-head self-attention and multi-layer perceptron blocks.\n2.2. LT-ViT framework\nFor general single/multi label classification tasks involving\nViTs, the output of the class token [CLS] is fed to a predic-\ntion head, which projects it to c nodes corresponding to the\nnumber of classes in the given dataset. However, in a multi-\nlabel setting, [CLS] contains the global representation of all\nthe true labels in the image. Therefore, the representations for\neach label are not directly separable. To alleviate this prob-\nlem, we introduce learnable label tokens (asY ∈ Rc×D to the\nnetwork, where c is the total number of labels in the dataset.\nThe proposed label tokens possess the ability to update them-\nselves by selectively attending to both image and label tokens.\nSpecifically, at layer l, for each image token set Xl ∈\nRn×D, and label token set Y l ∈ Rc×D, we project these to-\nkens to their corresponding query, key and value vectors by,\n\u0014XQ XK XV\nYQ YK YV\n\u0015\n=\n\u0014X\nY\n\u0015\n· WQKV , (1)\nwhere WQKV ∈ RD×3D is the learnable projection matrix.\nThen the image-label dependencies are computed, and their\ntokens are updated as,\n\u0014 ˜X\n˜Y\n\u0015\n= FFN\n\n\n\n\nattn (XQ, XK, XV )\nattn\n\u0012\nYQ,\n\u0014XK\nYK\n\u0015\n,\n\u0014XV\nYV\n\u0015\u0013\n\n\n\n , (2)\nwhere ˜X and ˜Y are the updated image and label tokens,\nwhile FFN (·) and attn(Q, K, V) denote the feed-forward\nand multi-head attention computations as in [7]. Eq. 2 en-\nsures the one-way flow of information from image to label\ntokens, allowing each label token to learn from the data, while\nalso allowing the sharing of information between label tokens\nthrough self-attention, as needed. From here on, we will refer\nto the label tokens Y as [LBL].\nThe schematic of the LT-ViT architecture is shown in Fig-\nure 2. The label tokens can be introduced at any point in the\nViT, hence N1 and N2 are adjustable, under the constraint\nN1 + N2 = L, where L is the depth of the original ViT.\nTherefore, apart from the newly initialized label tokens, all\nlearnable parameters can be directly transferred from any pre-\ntrained model.\nAfter N2 LT-ViT blocks, each updated label token[LBL]k,\nwhere k := {1, . . . , c}, is passed through a linear layer with\noutput dimensionality equal to one, and so the process is for-\nmulated as c binary classification problems, following [10].\nThe binary cross-entropy loss (BCELoss) for each label is\nthen computed between the predicted value and the ground\ntruth logit in order to back-propagate through the network.\n3. EXPERIMENTS AND IMPLEMENTATION\nLT-ViT is a generic framework for multi-label classification\nthat can be trained from scratch (i.e., random initialization),\nor initialized from a pre-trained model from in-domain or out\nof domain data. To show the effectiveness of our proposed\nframework, we followed both scenarios and compare the per-\nformance with the state-of-the-art. We provide information\nabout the employed datasets and experiments, and the imple-\nmentation details in Section 3.1 and Section 3.2, respectively.\nIn Section 4, we discuss the performance and analysis of the\nproposed method.\n3.1. Datasets and Experiments\nFor pre-training, we used the NIH-CXR14 dataset [16] which\ncontains 112,120 frontal-view Chest X-ray images from\n30,805 patients. We used the official 80/20 train/test split\nfor all experiments. For fine-tuning and evaluation, we em-\nployed two datasets: NIH-CXR14 and CheXpert [17]. The\nlatter is an additional large scale CXR dataset consisting of\n224,316 X-ray images from 65,240 patients. It contains an\nexpertly annotated test set of 234 radiographs, which we have\nused as our default test set. We evaluate our methods by per-\nforming experiments on both the 5-label (the most commonly\nused) and 13-label versions of this dataset, to examine the\neffects of LT on the number of labels used.\nWe employed Group Masked Model Learning (GMML)\n[18] as our pre-training method, as it exhibits strong perfor-\nmance on small datasets, notably on radiographs [19, 20]. A\nsimple ViT-S pre-trained using GMML was set as our base-\nline, and this baseline was compared with existing state-of-\nthe-art pre-training methods [21, 2] on CXR’s to outline the\nperformance delta needed to be fulfilled by LT-ViT. Following\nthat, we compared the performance change obtained upon in-\ntroducing LT-ViT versus existing benchmarks that use a larger\nnumber of trainable parameters.\nFurthermore, we investigated the generalizability of our\nproposed label tokens by obtaining models pre-trained on dif-\nferent domains, specifically, MIMIC-CXR [22] (Vision and\nLanguage CXR) and ImageNet [23] (Natural images). We\npre-trained a ViT-S on MIMIC-CXR using MGCA [4], and\nobtained a pre-trained checkpoint on ImageNet from the of-\nficial DINO [24] repository. We introduced LT-ViT on these\npre-trained checkpoints and compared their performance on\nthe fine-tuning datasets.\n3.2. Implementation details\nTo obtain the pre-trained ViT on NIH-CXR8, we pre-trained\na ViT-S using GMML on 4 RTX 3090 GPU’s with a per-gpu\nbatch size of 64, AdamW optimizer with a learning rate of\n5e−4 and a weight decay of 0.04.\nIn the implementation of LT-ViT, theN2 value (number of\nlayers carrying label tokens) was set to 4, based on a search\nwithin the range [2:2:12]. We expanded on the official DeiT\n[25] code base for the fine-tuning experiments and ran them\non a single RTX 3090 GPU with batch size 64. We used the\nAdam optimizer with a learning rate of 1e−5 for CheXpert\nand 2.5e−6 for NIH-CXR14. Q2L [10], was trained using the\nimplementation details provided in their study, along with a\nquick hyper-parameter search for the downstream datasets.\n4. RESULTS\nThe potential of LT-ViT to reach state-of-the-art CXR clas-\nsification benchmarks using transformers is outlined in Sec-\ntion 4.1. Following that, we put forward the generalizability\nof LT-ViT and the qualitative analysis of our method in Sec-\ntion 4.2 and Section 4.3, respectively.\n4.1. LT-ViT obtains state-of-the-art performance on CXR\nclassification with ViTs using a fraction of parameters\nTable 1 presents the comparison between the proposed\nmethod and previously established multi-label classification\nmethods using transformers.\nTable 1: Comparison of LT-ViT vs. existing benchmarks pre-\ntrained on NIH-CXR14.\nMethod Model Params Img.res AUC (%)\nNIH-14 CheX-05 CheX-13\nDIRA [21] Resnet50 23M 2242 81.12 87.59 -\nPOPAR-3 [2] ViT-B 86M 2242 79.58 87.86 -\nPOPAR Swin-B 88M 4482 81.81 88.34 -\nGMML ViT-S 21M 2242 81.28 87.36 73.65\n+ Q2L [10] ViT-S 26M 2242 81.09 88.12 77.12\n+ LT-ViT (ours)ViT-S 21M 2242 81.98 88.90 77.34\nFrom Table 1, we find that GMML, in isolation, exhibit\ncomparable performance relative to DIRA and POPAR-3 un-\nder the same image resolution and therefore is suitable for\nour ViT baseline. Nonetheless, we note that as the image in-\nput resolution increases, the resulting increase in fine-grained\nstructural detail leads to improved accuracy, as observed for\nPOPAR.\nOur proposed LT-ViT, when applied to the ViT baseline,\nis able to outperform POPAR, while maintaining the original\ninput resolution and requiring only a negligible increase in the\nnumber of trainable parameters, caused by the token embed-\nding for each label. We also find that LT-ViT outperforms the\nexisting multi-label training paradigm Query2Label (Q2L) on\nTransformers.\nTable 2: Study of LT-ViT generalizability over model initial-\nizations. rnd indicates random initialization (no pre-training).\nPre-train\ndataset\nAUC (%)\nNIH-CXR14 CheXpert-5 CheXpert-13\nrnd - 72.37 83.30 68.05\n+ LT-ViT 73.32 85.90 70.19\nDINO [24]ImageNet-1k 80.79 87.38 73.23\n+ LT-ViT 81.11 88.44 75.43\nMGCA [4]MIMIC-CXR 81.45 87.98 71.88\n+ LT-ViT 81.87 88.32 72.13\n4.2. LT-ViT is agnostic to model initialization\nTable 2 shows the results obtained by using LT under various\ninitialization states/pre-trained datasets. In addition to pre-\ntraining using NIH-CXR14 as in Section 4.1, we evaluate our\nmethod on random initialization, and on models pre-trained\non two distinct datasets.\nBased on Table 2, we have found that the proposed LT\nmethod improves the performance of multi-label classifica-\ntion across all initialization states and fine-tuning datasets.\nWe observe that a weaker initialization (random) leads to a\nhigher performance increase compared to stronger initializa-\ntions from ImageNet [23] and MIMIC-CXR [22]..\nThese findings provides evidence that LT-ViT can be ap-\nplied to any ViT model for multi-label CXR classification.\nThese results could suggest the possibility of extending LT-\nViT to other multi-label tasks using transformers.\n4.3. Analysis of our framework\n4.3.1. Visualization\nFig. 2 illustrates the smoothed attention maps of a specific\nlabel token in relation to the visual feature tokens, as well as\nthe bounding box annotation for the corresponding pathology.\nWe observe from the visualization that the label token is ca-\npable of accurately localizing the affected area for the given\npathology, as shown by the ground truth annotation, repre-\nsented by the red box. In contrast, when using the commonly\nused [CLS] token (as performed in [24]), the attention map\nprovides a union of all pathologies present in the X-ray, lack-\ning the specificity achieved by the label token.\n4.3.2. Ablations\nWe conducted an ablation study to investigate the impact of\nthe label token related attention components within the net-\nwork architecture, as seen in Table 3. Firstly, we employed\nfull self-attention, which is equivalent to having ’ c’ amounts\nof [CLS] tokens and is architecturally similar to C-Tran [9].\nSecondly, we removed the self-attention component within\nthe label token space, which means that each label token only\nattends to the image tokens. This implies that no information\nis shared between label tokens during the forward pass.\nAcalectasis\n[CLS] token[LBL] token\nCardiomegaly\nPneumonia\n Nodule\nFig. 2: Visualization of attention maps for [CLS] and [LBL]\ntokens. Red text indicates the pathology denoted by the\nbounding box ground truth.\nTable 3: CheXpert-5 performance on different interactions\nbetween image and label tokens. [IMG], [LBL] indicate im-\nage and label tokens respectively.\nImage tokens Label tokens AUC (%)Query Key, Val Query Key, Val\n[LBL][IMG] [LBL][IMG][LBL][IMG] [LBL][IMG] 88.47\n[IMG] [IMG] [LBL] [LBL][IMG] 88.90\n[IMG] [IMG] [LBL] [IMG] 88.44\n[IMG] [IMG] n/a n/a 87.36\nWe note the significance of one-way attention in impeding\ndata tokens learning from label tokens in Table 3, as shown\nby the slight decrease in performance when utilizing full self-\nattention. Furthermore, it is imperative that label tokens are\npermitted to attend to other label tokens to capture the inter-\nlabel dependencies, as indicated by the decrease in accuracy\nwhen there is no self-attention between label tokens.\n5. CONCLUSION\nWe proposed LT-ViT, a simple transformer-based framework\nfor multi-label classification in CXRs. We showed that, with-\nout any bells and whistles during training, LT-ViT is able to\nincrease multi-label classification performance consistently\nthroughout the tested datasets. We also discovered that LT-\nViT outperforms the existing multi-label training paradigm\nwhen applied to CXRs, without the use of additional decoder\nlayers. Furthermore, we outlined the generalizability over nu-\nmerous model initializations, and found out that the weaker\nthe initialization is, the more the model becomes dependent\non LT-ViT to improve classification performance. In addi-\ntion, we showed that the label tokens in LT-ViT are able to\naccurately localize the pathologies present in the CXR image.\nGiven these promising results on CXR data, the next step will\nbe to study the generalizability of LT-ViT across more down-\nstream datasets, in both medical and non-medical domains.\nAcknowledgements This work was supported by the EPSRC\ngrants MVSE (EP/V002856/1) and JADE2 (EP/T022205/1).\n6. REFERENCES\n[1] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, et al., “An image is worth 16x16\nwords: Transformers for image recognition at scale,”\nICLR, 2021.\n[2] Jiaxuan Pang, Fatemeh Haghighi, DongAo Ma, et al.,\n“Popar: Patch order prediction and appearance recovery\nfor self-supervised medical image analysis,”DART, held\nin Conjunction with MICCAI, 2022.\n[3] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, et al.,\n“Swin unetr: Swin transformers for semantic segmenta-\ntion of brain tumors in mri images,” BrainLes held in\nConjunction with MICCAI, 2022.\n[4] Fuying Wang, Yuyin Zhou, Shujun Wang, Varut Vard-\nhanabhuti, et al., “Multi-granularity cross-modal align-\nment for generalized medical visual representation\nlearning,” NeurIPS, 2022.\n[5] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Ji-\nmeng Sun, “Medclip: Contrastive learning from un-\npaired medical images and text,” EMNLP, 2022.\n[6] Philip M ¨uller, Georgios Kaissis, Congyu Zou, et al.,\n“Joint learning of localized representations from med-\nical images and reports,” ECCV, 2022.\n[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, et al.,\n“Attention is all you need,” NeurIPS, 2017.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, et al.,\n“Bert: Pre-training of deep bidirectional transformers\nfor language understanding,” NAACL, 2018.\n[9] Jack Lanchantin, Tianlu Wang, Vicente Ordonez, et al.,\n“General multi-label image classification with trans-\nformers,” CVPR, 2021.\n[10] Shilong Liu, Lei Zhang, Xiao Yang, et al.,\n“Query2label: A simple transformer way to multi-\nlabel classification,” arXiv preprint arXiv:2107.10834,\n2021.\n[11] Mark Sandler, Andrey Zhmoginov, Max Vladymyrov,\net al., “Fine-tuning image transformers using learnable\nmemory,” CVPR, 2022.\n[12] Jiarui Xu, Shalini De Mello, Sifei Liu, et al., “Groupvit:\nSemantic segmentation emerges from text supervision,”\nCVPR, 2022.\n[13] Nicolas Carion, Francisco Massa, Gabriel Synnaeve,\net al., “End-to-end object detection with transformers,”\nECCV, 2020.\n[14] Hongyu Wang, Shanshan Wang, Zibo Qin, et al., “Triple\nattention learning for classification of 14 thoracic dis-\neases using chest radiography,” Medical Image Analy-\nsis, 2021.\n[15] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek\nDas, et al., “Grad-cam: Visual explanations from deep\nnetworks via gradient-based localization,” ICCV, 2017.\n[16] Xiaosong Wang, Yifan Peng, Le Lu, et al., “Chestx-\nray8: Hospital-scale chest x-ray database and bench-\nmarks on weakly-supervised classification and localiza-\ntion of common thorax diseases,” CVPR, 2017.\n[17] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, et al.,\n“Chexpert: A large chest radiograph dataset with un-\ncertainty labels and expert comparison,” AAAI, 2019.\n[18] Sara Atito, Muhammad Awais, and Josef Kit-\ntler, “Gmml is all you need,” arXiv preprint\narXiv:2205.14986, 2022.\n[19] Syed Muhammad Anwar, Abhijeet Parida, Sara Atito,\net al., “Ss-cxr: Multitask representation learning using\nself supervised pre-training from chest x-rays,” arXiv\npreprint arXiv:2211.12944, 2022.\n[20] Sara Atito, Syed Muhammad Anwar, Muhammad\nAwais, et al., “Sb-ssl: Slice-based self-supervised trans-\nformers for knee abnormality classification from mri,”\nMILLanD held in Conjunction with MICCAI, 2022.\n[21] Fatemeh Haghighi, Mohammad Reza Hosseinzadeh\nTaher, Michael B Gotway, et al., “Dira: discriminative,\nrestorative, and adversarial learning for self-supervised\nmedical image analysis,” CVPR, 2022.\n[22] Alistair EW Johnson, Tom J Pollard, Nathaniel R Green-\nbaum, et al., “Mimic-cxr-jpg, a large publicly available\ndatabase of labeled chest radiographs,” arXiv preprint\narXiv:1901.07042, 2019.\n[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\net al., “ImageNet Large Scale Visual Recognition Chal-\nlenge,” IJCV, vol. 115, no. 3, pp. 211–252, 2015.\n[24] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e\nJ´egou, et al., “Emerging properties in self-supervised\nvision transformers,” ICCV, 2021.\n[25] Hugo Touvron, Matthieu Cord, Matthijs Douze, et al.,\n“Training data-efficient image transformers & distilla-\ntion through attention,” ICML, 2021."
}