{
    "title": "Character n-Gram Embeddings to Improve RNN Language Models",
    "url": "https://openalex.org/W2905559537",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2140893311",
            "name": "Sho Takase",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1749670362",
            "name": "Jun Suzuki",
            "affiliations": [
                "Tohoku University"
            ]
        },
        {
            "id": "https://openalex.org/A2071111304",
            "name": "Masaaki Nagata",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2140893311",
            "name": "Sho Takase",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1749670362",
            "name": "Jun Suzuki",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2071111304",
            "name": "Masaaki Nagata",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2493916176",
        "https://openalex.org/W6764398373",
        "https://openalex.org/W2097927681",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W2212703438",
        "https://openalex.org/W2571859396",
        "https://openalex.org/W2549416390",
        "https://openalex.org/W6692563993",
        "https://openalex.org/W6640598943",
        "https://openalex.org/W2778718264",
        "https://openalex.org/W6744707562",
        "https://openalex.org/W1899794420",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2251012068",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W6741459021",
        "https://openalex.org/W6727099177",
        "https://openalex.org/W2743945814",
        "https://openalex.org/W6607333740",
        "https://openalex.org/W2800490029",
        "https://openalex.org/W2250716442",
        "https://openalex.org/W6697028822",
        "https://openalex.org/W2086161653",
        "https://openalex.org/W6726320248",
        "https://openalex.org/W1843891098",
        "https://openalex.org/W2284660317",
        "https://openalex.org/W2754586843",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W2510102199",
        "https://openalex.org/W6744128854",
        "https://openalex.org/W2888799392",
        "https://openalex.org/W6785647005",
        "https://openalex.org/W6734257279",
        "https://openalex.org/W6639657675",
        "https://openalex.org/W6600213771",
        "https://openalex.org/W1948566616",
        "https://openalex.org/W2463895987",
        "https://openalex.org/W2767321762",
        "https://openalex.org/W6635446068",
        "https://openalex.org/W2609482285",
        "https://openalex.org/W6720905350",
        "https://openalex.org/W6764043288",
        "https://openalex.org/W2553303224",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W2792376130",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W179875071",
        "https://openalex.org/W4300427683",
        "https://openalex.org/W4294555862",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2952436057",
        "https://openalex.org/W1895577753",
        "https://openalex.org/W2951672049",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2963537482",
        "https://openalex.org/W3102675873",
        "https://openalex.org/W2591264712",
        "https://openalex.org/W2964189376",
        "https://openalex.org/W2757047188",
        "https://openalex.org/W4299838440",
        "https://openalex.org/W2962813775",
        "https://openalex.org/W4919037",
        "https://openalex.org/W4298422451"
    ],
    "abstract": "This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks",
    "full_text": "The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)\nCharacter n-Gram Embeddings to Improve RNN Language Models\nSho Takase,†∗Jun Suzuki,†‡Masaaki Nagata†\n†NTT Communication Science Laboratories\n‡Tohoku University\nsho.takase@nlp.c.titech.ac.jp, jun.suzuki@ecei.tohoku.ac.jp, nagata.masaaki@lab.ntt.co.jp\nAbstract\nThis paper proposes a novel Recurrent Neural Network (RNN)\nlanguage model that takes advantage of character information.\nWe focus on charactern-grams based on research in the ﬁeld of\nword embedding construction (Wieting et al. 2016). Our pro-\nposed method constructs word embeddings from character n-\ngram embeddings and combines them with ordinary word em-\nbeddings. We demonstrate that the proposed method achieves\nthe best perplexities on the language modeling datasets: Penn\nTreebank, WikiText-2, and WikiText-103. Moreover, we con-\nduct experiments on application tasks: machine translation and\nheadline generation. The experimental results indicate that our\nproposed method also positively affects these tasks.\n1 Introduction\nNeural language models have played a crucial role in re-\ncent advances of neural network based methods in natural\nlanguage processing (NLP). For example, neural encoder-\ndecoder models, which are becoming the de facto standard\nfor various natural language generation tasks including ma-\nchine translation (Sutskever, Vinyals, and Le 2014), summa-\nrization (Rush, Chopra, and Weston 2015), dialogue (Wen et\nal. 2015), and caption generation (Vinyals et al. 2015) can be\ninterpreted as conditional neural language models. Moreover,\nneural language models can be used for rescoring outputs\nfrom traditional methods, and they signiﬁcantly improve the\nperformance of automatic speech recognition (Du et al. 2016).\nThis implies that better neural language models improve the\nperformance of application tasks.\nIn general, neural language models require word embed-\ndings as an input (Zaremba, Sutskever, and Vinyals 2014).\nHowever, as described by (Verwimp et al . 2017), this ap-\nproach cannot make use of the internal structure of words\nalthough the internal structure is often an effective clue for\nconsidering the meaning of a word. For example, we can\ncomprehend that the word ‘causal’ is related to ‘cause’ im-\nmediately because both words include the same character\nsequence ‘caus’. Thus, if we incorporate a method that han-\ndles the internal structure such as character information, we\ncan improve the quality of neural language models and prob-\nably make them robust to infrequent words.\n∗Current afﬁliation: Tokyo Institute of Technology.\nCopyright c⃝2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nTo incorporate the internal structure, (Verwimp et al. 2017)\nconcatenated character embeddings with an input word em-\nbedding. They demonstrated that incorporating character em-\nbeddings improved the performance of RNN language mod-\nels. Moreover, (Kim et al. 2016) and (Jozefowicz et al. 2016)\napplied Convolutional Neural Networks (CNN) to construct\nword embeddings from character embeddings.\nOn the other hand, in the ﬁeld of word embedding construc-\ntion, some previous researchers found that character n-grams\nare more useful than single characters (Wieting et al. 2016;\nBojanowski et al. 2017). In particular, (Wieting et al. 2016)\ndemonstrated that constructing word embeddings from char-\nacter n-gram embeddings outperformed the methods that\nconstruct word embeddings from character embeddings by\nusing CNN or a Long Short-Term Memory (LSTM).\nBased on their reports, in this paper, we propose a neural\nlanguage model that utilizes character n-gram embeddings.\nOur proposed method encodes character n-gram embeddings\nto a word embedding with simpliﬁed Multi-dimensional Self-\nattention (MS) (Shen et al. 2018). We refer to this constructed\nembedding as charn-MS-vec. The proposed method regards\ncharn-MS-vec as an input in addition to a word embedding.\nWe conduct experiments on the well-known benchmark\ndatasets: Penn Treebank, WikiText-2, and WikiText-103. Our\nexperiments indicate that the proposed method outperforms\nneural language models trained with well-tuned hyperparam-\neters and achieves state-of-the-art scores on each dataset. In\naddition, we incorporate our proposed method into a standard\nneural encoder-decoder model and investigate its effect on\nmachine translation and headline generation. We indicate that\nthe proposed method also has a positive effect on such tasks.\n2 RNN Language Model\nIn this study, we focus on RNN language models, which are\nwidely used in the literature. This section brieﬂy overviews\nthe basic RNN language model.\nIn language modeling, we compute joint probability by\nusing the product of conditional probabilities. Let w1:T be\na word sequence with length T, namely, w1, ..., wT . We for-\nmally obtain the joint probability of word sequence w1:T as\nfollows:\np(w1:T ) =p(w1)\nT−1∏\nt=1\np(wt+1|w1:t). (1)\n5074\np(w1) is generally assumed to be 1 in this literature, i.e.,\np(w1)=1 , and thus we can ignore its calculation1.\nTo estimate the conditional probability p(wt+1|w1:t),\nRNN language models encode sequence w1:t into a ﬁxed-\nlength vector and compute the probability distribution of each\nword from this ﬁxed-length vector. Let V be the vocabulary\nsize and let Pt ∈RV be the probability distribution of the\nvocabulary at timestep t. Moreover, let Dh be the dimension\nof the hidden state of an RNN and let De be the dimensions\nof embedding vectors. Then, RNN language models predict\nthe probability distribution Pt+1 by the following equation:\nPt+1 = softmax(Wht + b), (2)\nht = f(et, ht−1), (3)\net = Ext, (4)\nwhere W ∈RV ×Dh is a weight matrix,b ∈RV is a bias term,\nand E ∈RDe×V is a word embedding matrix. xt ∈{0, 1}V\nand ht ∈RDh are a one-hot vector of an input word wt and\nthe hidden state of the RNN at timestep t, respectively. We\ndeﬁne ht at timestep t = 0as a zero vector, that is, h0 = 0.\nLet f(·) represent an abstract function of an RNN, which\nmight be the LSTM, the Quasi-Recurrent Neural Network\n(QRNN) (Bradbury et al. 2017), or any other RNN variants.\n3 Incorporating Character n-gram\nEmbeddings\nWe incorporate charn-MS-vec, which is an embedding con-\nstructed from character n-gram embeddings, into RNN lan-\nguage models since, as discussed earlier, previous studies\nrevealed that we can construct better word embeddings by\nusing character n-gram embeddings (Wieting et al . 2016;\nBojanowski et al. 2017). In particular, we expect charn-MS-\nvec to help represent infrequent words by taking advantage\nof the internal structure.\nFigure 1 is the overview of the proposed method using\ncharacter 3-gram embeddings (char3-MS-vec). As illustrated\nin this ﬁgure, our proposed method regards the sum of char3-\nMS-vec and the standard word embedding as an input of an\nRNN. In other words, let ct be charn-MS-vec and we replace\nEquation 4 with the following:\net = Ext + ct. (5)\n3.1 Multi-dimensional Self-attention\nTo compute ct, we apply an encoder to character n-gram em-\nbeddings. Previous studies demonstrated that additive com-\nposition, which computes the (weighted) sum of embeddings,\nis a suitable method for embedding construction (Takase,\nOkazaki, and Inui 2016; Wieting et al . 2016). Thus, we\nadopt (simpliﬁed) multi-dimensional self-attention (Shen et\nal. 2018), which computes weights for each dimension of\ngiven embeddings and sums up the weighted embeddings\n(i.e., element-wise weighted sum) as an encoder. Let si be\nthe character n-gram embeddings of an input word, let I be\n1This deﬁnition is based on published implementations of\nprevious studies such as https://github.com/wojzaremba/lstm and\nhttps://github.com/salesforce/awd-lstm-lm.\nNotation\f\b\ttemporal\r#  \n\u001f\u001e\u0005De\u0004 \u0018\u0006\u0016\u0014\u0015\u0017\u000f\u001c\u0007\u0005\"\u000e\u001a\u0012\u000b\u0011\u0004\u001d\u0010\u0019$\u0005Wc\u0001\nx1\u0001… \u0001\nd\u0001\nMultiplied by\n\u0001\nMatrix\n(size = L × d)\nx2\u0001\nxL\u0001\nSoftmax function\nto each column\u0001\n… \u0001\n… \u0001\nWeighted\nvectors\nEach column is\na distribution by\nsoftmax function\nalternative    to    the    traditional epoxidian\u0001\nword\nembedding\u0001\nMultiplied by\n\u0001\ncharacter\n3-gram\nembeddings\n(^: BOW\n $: EOW)\u0001\nmultiplied by\n\u0001\nsoftmax function\nto each row\u0001\nweighted\ncharacter\n3-gram\nembeddings\nRNN (LSTM)\u0001… …\nto   the   traditional   epoxidian\u0001\nchar3-MS-vec\u0001\nMulti-dimensional\nSelf-attention (MS)\u0001\nFigure 1: Overview of the proposed method. The proposed\nmethod computes charn-MS-vec from character n-gram (3-\ngram in this ﬁgure) embeddings and inputs the sum of it and\nthe standard word embedding into an RNN.\nthe number of character n-grams extracted from the word,\nand let S be the matrix whose i-th column corresponds to si,\nthat is, S = [s1, ..., sI]. The multi-dimensional self-attention\nconstructs the word embedding ct by the following equations:\nct =\nI∑\ni=1\ngi ⊙si, (6)\n{gi}j = {softmax(\n[\n(WcS)T]\nj)}i, (7)\nwhere ⊙means element-wise product of vectors, Wc ∈\nRDe×De is a weight matrix, [·]j is the j-th column of a given\nmatrix, and {·}j is the j-th element of a given vector. In\nshort, Equation 7 applies the softmax function to each row of\n[WcS] and extracts the i-th column as gi.\nLet us consider the case where an input word is ‘the’ and\nwe use character 3-gram in Figure 1. We prepare special\ncharacters ‘ˆ’ and ‘$’ to represent the beginning and end of the\nword, respectively. Then, ‘the’ is composed of three character\n3-grams: ‘ˆth’, ‘the’, and ‘he$’. We multiply the embeddings\nof these 3-grams by transformation matrix Wc and apply the\nsoftmax function to each row2 as in Equation 7. As a result\n2(Shen et al. 2018) applied an activation function and one more\ntransformation matrix to embeddings but we omit these operations.\n5075\nPTB WT2 WT103\nV ocab 10,000 33,278 267,735\nTrain 929,590 2,088,628 103,227,021\n#Token Valid 73,761 217,646 217,646\nTest 82,431 245,569 245,569\nTable 1: Statistics of PTB, WT2, and WT103.\nof the softmax, we obtain a matrix that contains weights for\neach embedding. The size of the computed matrix is identical\nto the input embedding matrix: De ×I. We then compute\nEquation 6, i.e., the weighted sum of the embeddings, and add\nthe resulting vector to the word embedding of ‘the’. Finally,\nwe input the vector into an RNN to predict the next word.\n3.2 Word Tying\n(Inan, Khosravi, and Socher 2017) and (Press and Wolf 2017)\nproposed a word tying method (WT) that shares the word\nembedding matrix ( E in Equation 4) with the weight ma-\ntrix to compute probability distributions (W in Equation 2).\nThey demonstrated that WT signiﬁcantly improves the per-\nformance of RNN language models.\nIn this study, we adopt charn-MS-vec as the weight matrix\nin language modeling. Concretely, we use E + C instead of\nW in Equation 2, whereC ∈RDe×V contains charn-MS-vec\nfor all words in the vocabulary.\n4 Experiments on Language Modeling\nWe investigate the effect of charn-MS-vec on the word-level\nlanguage modeling task. In detail, we examine the following\nfour research questions;\n1. Can character n-gram embeddings improve the perfor-\nmance of state-of-the-art RNN language models?\n2. Do character n-gram embeddings have a positive effect\non infrequent words?\n3. Is multi-dimensional self-attention effective for word em-\nbedding construction as compared with several other simi-\nlar conventional methods?\n4. How many n should we use?\n4.1 Datasets\nWe used the standard benchmark datasets for the word-\nlevel language modeling: Penn Treebank (PTB) (Marcus,\nMarcinkiewicz, and Santorini 1993), WikiText-2 (WT2), and\nWikiText-103 (WT103) (Merity et al. 2017). (Mikolov et al.\n2010) and (Merity et al. 2017) published pre-processed PTB3,\nWT2, and WT1034. Following the previous studies, we used\nthese pre-processed datasets for our experiments.\nTable 1 describes the statistics of the datasets. Table 1\ndemonstrates that the vocabulary size of WT103 is too large,\nand thus it is impractical to compute char n-MS-vec for all\n3http://www.ﬁt.vutbr.cz/˜mikolov/rnnlm/\n4https://einstein.ai/research/the-wikitext-long-term-\ndependency-language-modeling-dataset\nwords at every step. Therefore, we did not use C for word\ntying. In other words, we used only word embeddings E as\nthe weight matrix W in WT103.\n4.2 Baseline RNN Language Model\nFor base RNN language models, we adopted the state-of-\nthe-art LSTM language model (Merity, Keskar, and Socher\n2018b) for PTB and WT2, and QRNN for WT103 (Bradbury\net al. 2017). (Melis, Dyer, and Blunsom 2018) demonstrated\nthat the standard LSTM trained with appropriate hyperpa-\nrameters outperformed various architectures such as Recur-\nrent Highway Networks (RHN) (Zilly et al. 2017). In addi-\ntion to several regularizations, (Merity, Keskar, and Socher\n2018b) introduced Averaged Stochastic Gradient Descent\n(ASGD) (Polyak and Juditsky 1992) to train the 3-layered\nLSTM language model. As a result, their ASGD Weight-\nDropped LSTM (AWD-LSTM) achieved state-of-the-art re-\nsults on PTB and WT2. For WT103, (Merity, Keskar, and\nSocher 2018a) achieved the top score with the 4-layered\nQRNN. Thus, we used AWD-LSTM for PTB and WT2, and\nQRNN for WT103 as the base language models, respectively.\nWe used their implementations5 for our experiments.\n4.3 Results\nTable 2 shows perplexities of the baselines and the proposed\nmethod. We varied n for charn-MS-vec from 2 to 4. For the\nbaseline, we also applied two word embeddings to investigate\nthe performance in the case where we use more kinds of word\nembeddings. In detail, we prepared E1, E2 ∈RDe×V and\nused E1 + E2 instead of E in Equation 4. Table 2 also shows\nthe number of character n-grams in each dataset. This table\nindicates that charn-MS-vec improved the performance of\nstate-of-the-art models except for char4-MS-vec on WT103.\nThese results indicate that charn-MS-vec can raise the quality\nof word-level language models. In particular, Table 2 shows\nthat char3-MS-vec achieved the best scores consistently. In\ncontrast, an additional word embedding did not improve the\nperformance. This fact implies that the improvement of charn-\nMS-vec is caused by using character n-grams. Thus, we\nanswer yes to the ﬁrst research question.\nTable 3 shows the training time spent on each epoch. We\ncalculated it on the NVIDIA Tesla P100. Table 3 indicates\nthat the proposed method requires more computational time\nthan the baseline unfortunately. We leave exploring a faster\nstructure for our future work.\nTable 4 shows perplexities on the PTB dataset where the\nfrequency of an input word is lower than 2,000 in the train-\ning data. This table indicates that the proposed method can\nimprove the performance even if an input word is infrequent.\nIn other words, charn-MS-vec helps represent the meanings\nof infrequent words. Therefore, we answer yes to the second\nresearch question in the case of our experimental settings.\nWe explored the effectiveness of multi-dimensional self-\nattention for word embedding construction. Table 5 shows\nperplexities of using several encoders on the PTB dataset.\nAs in (Kim et al. 2016), we applied CNN to construct word\nembeddings (charCNN in Table 5). Moreover, we applied the\n5https://github.com/salesforce/awd-lstm-lm\n5076\nPTB WT2 WT103\nMethod #Charn #Params Valid Test #Charn #Params Valid Test #Charn #Params Valid Test\nBaseline - 24M 58.88 56.36 - 33M 66.98 64.11 - 153M 31.24 32.19\n2 embeds - 28M 58.94 56.56 - 47M 66.71 64.00 - 261M 31.75 32.71\nchar2-MS-vec 773 25M 57.88 55.91 2,442 35M 65.60 62.96 8,124 158M 31.18 31.92\nchar3-MS-vec 5,258 26M 57.40 55.56 12,932 39M 65.05 61.95 51,492 175M 30.95 31.81\nchar4-MS-vec 15,416 31M 57.30 55.64 39,318 49M 64.84 62.19 179,900 226M 31.23 32.21\nTable 2: Perplexities on each dataset. We varied then for charn-MS-vec from 2 to 4.\nMethod Seconds / epoch\nBaseline 63.78\nchar2-MS-vec 173.77\nchar3-MS-vec 171.10\nchar4-MS-vec 165.46\nTable 3: Computational speed of the baseline and proposed\nmethod on NVIDIA Tesla P100.\nMethod Valid Test\nBaseline 44.02 41.14\nchar2-MS-vec 43.21 40.99\nchar3-MS-vec 42.74 40.50\nchar4-MS-vec 42.71 40.41\nTable 4: Perplexities on the PTB dataset where an input word\nis infrequent in the training data, which means its frequency\nis lower than 2,000.\nsummation and standard self-attention, which computes the\nscalar value as a weight for a character n-gram embedding,\nto construct word embeddings (char n-Sum-vec and charn-\nSS-vec, respectively). For CNN, we used hyperparameters\nidentical to (Kim et al. 2016) (“Original Settings” in Table 5)\nbut the setting has two differences from other architectures: 1.\nThe dimension of the computed vectors is much larger than\nthe dimension of the baseline word embeddings and 2. The\ndimension of the input character embeddings is much smaller\nthan the dimension of the baseline word embeddings. There-\nfore, we added two conﬁgurations: assigning the dimension\nof the computed vectors and input character embeddings a\nvalue identical to the baseline word embeddings (in Table\n5, “Small CNN result dims” and “Large embedding dims”,\nrespectively).\nTable 5 shows that the proposed char n-MS-vec outper-\nformed charCNN even though the original settings of char-\nCNN had much larger parameters. Moreover, we trained char-\nCNN with two additional settings but CNN did not improve\nthe baseline performance. This result implies that charn-MS-\nvec is better embeddings than ones constructed by applying\nCNN to character embeddings. Table 5 also indicates that\ncharn-Sum-vec was harmful to the performance. Moreover,\ncharn-SS-vec did not have a positive effect on the baseline.\nThese results answer yes to the third research question; our\nMethod #Params Valid Test\nBaseline 24M 58.88 56.36\nThe proposed method (charn-MS-vec)\nchar2-MS-vec 25M 57.88 55.91\nchar3-MS-vec 26M 57.40 55.56\nchar4-MS-vec 31M 57.30 55.64\nUsing CNN (charCNN)\nOriginal Settings 36M 60.59 58.03\nSmall CNN result dims 21M 82.39 80.01\nLarge embedding dims 36M 60.75 58.06\nUsing Sum (charn-Sum-vec)\nchar2-Sum-vec 25M 61.77 59.57\nchar3-Sum-vec 26M 59.72 57.39\nchar4-Sum-vec 30M 59.40 56.72\nUsing Standard self-attention (charn-SS-vec)\nchar2-SS-vec 25M 58.66 56.44\nchar3-SS-vec 26M 59.42 57.30\nchar4-SS-vec 30M 58.37 56.33\nExclude C from word tying\nchar2-MS-vec 25M 58.10 56.12\nchar3-MS-vec 26M 58.62 56.19\nchar4-MS-vec 31M 58.68 56.78\nRemove word embeddings E\nchar2-MS-vec 21M 74.22 72.39\nchar3-MS-vec 22M 60.60 58.88\nchar4-MS-vec 27M 57.65 55.74\nSame #Params as baseline\nchar2-MS-vec 24M 57.93 56.15\nchar3-MS-vec 24M 57.81 55.81\nchar4-MS-vec 24M 59.48 57.69\nTable 5: Perplexities of each structure on PTB dataset.\nuse of multi-dimensional self-attention is more appropriate\nfor constructing word embeddings from character n-gram\nembeddings.\nTable 5 also shows that excludingC from word tying (“Ex-\nclude C from word tying”) achieved almost the same score as\nthe baseline. Moreover, this table indicates that performance\n5077\nModel #Params Valid Test\nLSTM (medium) (Zaremba, Sutskever, and Vinyals 2014) 20M 86.2 82.7\nLSTM (large) (Zaremba, Sutskever, and Vinyals 2014) 66M 82.2 78.4\nCharacter-Word LSTM (Verwimp et al. 2017) - - 82.04\nLSTM-CharCNN-Large (Kim et al. 2016) 19M - 78.9\nVariational LSTM (medium) (Gal and Ghahramani 2016) 20M 81.9 ±0.2 79.7 ±0.1\nVariational LSTM (large) (Gal and Ghahramani 2016) 66M 77.9 ±0.3 75.2 ±0.2\nVariational RHN (Zilly et al. 2017) 32M 71.2 68.5\nVariational RHN + WT (Zilly et al. 2017) 23M 67.9 65.4\nVariational RHN + WT + IOG (Takase, Suzuki, and Nagata 2017) 29M 67.0 64.4\nNeural Architecture Search (Zoph and Le 2017) 54M - 62.4\nLSTM with skip connections (Melis, Dyer, and Blunsom 2018) 24M 60.9 58.3\nAWD-LSTM (Merity, Keskar, and Socher 2018b) 24M 60.0 57.3\nAWD-LSTM + Fraternal Dropout (Zolna et al. 2018) 24M 58.9 56.8\nAWD-LSTM-MoS (Yang et al. 2018) 22M 56.54 54.44\nAWD-LSTM-DOC (Takase, Suzuki, and Nagata 2018) 23M 54.12 52.38\nProposed method: AWD-LSTM + char3-MS-vec 26M 57.40 55.56\nProposed method: AWD-LSTM-MoS + char3-MS-vec 23M 56.13 53.98\nProposed method: AWD-LSTM-DOC + char3-MS-vec 24M 53.76 52.34\nTable 6: Perplexities of the proposed method and as reported in previous studies on the PTB dataset.\nModel #Params Valid Test\nLSTM (Grave, Joulin, and Usunier 2017) - - 99.3\nVariational LSTM + IOG (Takase, Suzuki, and Nagata 2017) 70M 95.9 91.0\nVariational LSTM + WT + AL (Inan, Khosravi, and Socher 2017) 28M 91.5 87.0\nLSTM with skip connections (Melis, Dyer, and Blunsom 2018) 24M 69.1 65.9\nAWD-LSTM (Merity, Keskar, and Socher 2018b) 33M 68.6 65.8\nAWD-LSTM + Fraternal Dropout (Zolna et al. 2018) 34M 66.8 64.1\nAWD-LSTM-MoS (Yang et al. 2018) 35M 63.88 61.45\nAWD-LSTM-DOC (Takase, Suzuki, and Nagata 2018) 37M 60.29 58.03\nProposed method: AWD-LSTM + char3-MS-vec 39M 65.05 61.95\nProposed method: AWD-LSTM-MoS + char3-MS-vec 39M 62.20 59.99\nProposed method: AWD-LSTM-DOC + char3-MS-vec 41M 59.47 57.30\nTable 7: Perplexities of the proposed method and as reported in previous studies on the WT2 dataset.\nModel #Params Valid Test\nLSTM (Grave, Joulin, and Usunier 2017) - - 48.7\nGCNN-8 (Dauphin et al. 2016) - - 44.9\nGCNN-14 (Dauphin et al. 2016) - - 37.2\nQRNN (Merity, Keskar, and Socher 2018a) 153M 32.0 33.0\nProposed method: QRNN + char3-MS-vec 175M 30.95 31.81\nTable 8: Perplexities of the proposed method and as reported in previous studies on the WT103 dataset.\nfails as the the number of parameters is increased. Thus, we\nneed to assign C to word tying to prevent over-ﬁtting for the\nPTB dataset. In addition, this result implies that the perfor-\nmance of WT103 in Table 2 might be raised if we can apply\nword tying to WT103.\nMoreover, to investigate the effect of only charn-MS-vec,\nwe ignore Ext in Equation 5. We refer to this setting as “Re-\nmove word embeddings E” in Table 5. Table 5 shows cahr3-\nMS-vec and char4-MS-vec are superior to char2-MS-vec.\nIn the view of perplexity, char3-MS-vec and char4-MS-vec\nachieved comparable scores to each other. On the other hand,\nchar3-MS-vec is composed of much smaller parameters. Fur-\nthermore, we decreased the embedding size De to adjust the\nnumber of parameters to the same size as the baseline (“Same\n#Params as baseline” in Table 5). In this setting, char3-MS-\nvec achieved the best perplexity. Therefore, we consider that\n5078\nchar3-MS-vec is more useful than char4-MS-vec, which is\nthe answer to the fourth research question. We use the com-\nbination of the char3-MS-vec ct and word embedding Ext\nin the following experiments.\nFinally, we compare the proposed method with the pub-\nlished scores reported in previous studies. Tables 6, 7, and\n8, respectively, show perplexities of the proposed method\nand previous studies on PTB, WT2, and WT103 6. Since\nAWD-LSTM-MoS (Yang et al . 2018) and AWD-LSTM-\nDOC (Takase, Suzuki, and Nagata 2018) achieved the state-\nof-the-art scores on PTB and WT2, we combined char3-MS-\nvec with them. These tables show that the proposed method\nimproved the performance of the base model and outper-\nformed the state-of-the-art scores on all datasets. In particular,\nchar3-MS-vec improved perplexity by at least 1 point from\ncurrent best scores on the WT103 dataset.\n5 Experiments on Applications\nAs described in Section 1, neural encoder-decoder models\ncan be interpreted as conditional neural language models.\nTherefore, to investigate if the proposed method contributes\nto encoder-decoder models, we conduct experiments on ma-\nchine translation and headline generation tasks.\n5.1 Datasets\nFor machine translation, we used two kinds of language pairs:\nEnglish-French and English-German sentences in the IWSLT\n2016 dataset 7. The dataset contains about 208K English-\nFrench pairs and 189K English-German pairs. We conducted\nfour translation tasks: from English to each language (En-Fr\nand En-De), and their reverses (Fr-En and De-En).\nFor headline generation, we used sentence-headline\npairs extracted from the annotated English Gigaword cor-\npus (Napoles, Gormley, and Van Durme 2012) in the same\nmanner as (Rush, Chopra, and Weston 2015). The training set\ncontains about 3.8M sentence-headline pairs. For evaluation,\nwe exclude the test set constructed by (Rush, Chopra, and\nWeston 2015) because it contains some invalid instances, as\nreported in (Zhou et al. 2017). We instead used the test sets\nconstructed by (Zhou et al. 2017) and (Kiyono et al. 2017).\n5.2 Experimental Settings\nWe employed the neural encoder-decoder with attention\nmechanism described in (Kiyono et al . 2017) as the base\nmodel. Its encoder consists of a 2-layer bidirectional LSTM\nand its decoder consists of a 2-layer LSTM with attention\nmechanism proposed by (Luong, Pham, and Manning 2015).\nWe refer to this neural encoder-decoder as EncDec. To in-\nvestigate the effect of the proposed method, we introduced\nchar3-MS-vec into EncDec. Here, we applied char3-MS-vec\nto both the encoder and decoder. Moreover, we did not apply\nword tying technique to EncDec because it is default setting\nin the widely-used encoder-decoder implementation8.\n6To compare models trained only on the training data, we ex-\ncluded methods that use the statistics of the test data (Grave, Joulin,\nand Usunier 2017; Krause et al. 2017).\n7https://wit3.fbk.eu/\n8http://opennmt.net/\nWe set the embedding size and dimension of the LSTM hid-\nden state to 500 for machine translation and 400 for headline\ngeneration. The mini-batch size is 64 for machine translation\nand 256 for headline generation. For other hyperparameters,\nwe followed the conﬁgurations described in (Kiyono et al .\n2017). We constructed the vocabulary set by using Byte-\nPair-Encoding9 (BPE) (Sennrich, Haddow, and Birch 2016)\nbecause BPE is a currently widely-used technique for vocab-\nulary construction. We set the number of BPE merge oper-\nations to 16K for machine translation and 5K for headline\ngeneration.\n5.3 Results\nTables 9 and 10 show the results of machine translation\nand headline generation, respectively. These tables show\nthat EncDec+char3-MS-vec outperformed EncDec in all test\ndata. In other words, these results indicate that our proposed\nmethod also has a positive effect on the neural encoder-\ndecoder model. Moreover, it is noteworthy that char3-MS-vec\nimproved the performance of EncDec even though the vocab-\nulary set constructed by BPE contains subwords. This implies\nthat character n-gram embeddings improve the quality of not\nonly word embeddings but also subword embeddings.\nIn addition to the results of our implementations, the lower\nportion of Table 10 contains results reported in previous stud-\nies. Table 10 shows that EncDec+char3-MS-vec also outper-\nformed the methods proposed in previous studies. Therefore,\nEncDec+char3-MS-vec achieved the top scores in the test sets\nconstructed by (Zhou et al. 2017) and (Kiyono et al. 2017)\neven though it does not have a task-speciﬁc architecture such\nas the selective gate proposed by (Zhou et al. 2017).\nIn these experiments, we only applied char3-MS-vec to\nEncDec but (Morishita, Suzuki, and Nagata 2018) indicated\nthat combining multiple kinds of subword units can improve\nthe performance. We will investigate the effect of combining\nseveral character n-gram embeddings in future work.\n6 Related Work\nRNN Language Model (Mikolov et al. 2010) introduced\nRNN into language modeling to handle arbitrary-length se-\nquences in computing conditional probability p(wt+1|w1:t).\nThey demonstrated that the RNN language model out-\nperformed the Kneser-Ney smoothed 5-gram language\nmodel (Chen and Goodman 1996), which is a sophisticated\nn-gram language model.\n(Zaremba, Sutskever, and Vinyals 2014) drastically im-\nproved the performance of language modeling by applying\nLSTM and the dropout technique (Srivastava et al . 2014).\n(Zaremba, Sutskever, and Vinyals 2014) applied dropout to\nall the connections except for recurrent connections but (Gal\nand Ghahramani 2016) proposed variational inference based\ndropout to regularize recurrent connections. (Melis, Dyer,\nand Blunsom 2018) demonstrated that the standard LSTM\ncan achieve superior performance by selecting appropriate hy-\nperparameters. Finally, (Merity, Keskar, and Socher 2018b)\nintroduced DropConnect (Wan et al . 2013) and averaged\nSGD (Polyak and Juditsky 1992) into the LSTM language\n9https://github.com/rsennrich/subword-nmt\n5079\nModel En-Fr En-De Fr-En De-En\nEncDec 34.37 23.05 34.07 28.18\nEncDec+char3-MS-vec 35.48 23.27 34.43 28.86\nTable 9: BLEU scores on the IWSLT16 dataset. We report the average score of 3 runs.\nTest set by (Zhou et al. 2017) Test set by (Kiyono et al. 2017)\nModel ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L\nEncDec 46.77 24.87 43.58 46.78 24.52 43.68\nEncDec+char3-MS-vec 47.04 25.09 43.80 46.91 24.61 43.77\nABS (Rush, Chopra, and Weston 2015) 37.41 15.87 34.70 - - -\nSEASS (Zhou et al. 2017) 46.86 24.58 43.53 - - -\n(Kiyono et al. 2017) 46.34 24.85 43.49 46.41 24.58 43.59\nTable 10: ROUGE F1 scores on the headline generation test sets provided by (Zhou et al. 2017) and (Kiyono et al. 2017). The\nupper part is the results of our implementation and the lower part shows the scores reported in previous studies. In the upper part,\nwe report the average score of 3 runs.\nmodel and achieved state-of-the-art perplexities on PTB and\nWT2. For WT103, (Merity, Keskar, and Socher 2018a) found\nthat QRNN (Bradbury et al. 2017), which is a faster architec-\nture than LSTM, achieved the best perplexity. Our experimen-\ntal results show that the proposed charn-MS-vec improved\nthe performance of these state-of-the-art language models.\n(Yang et al. 2018) explained that the training of RNN lan-\nguage models can be interpreted as matrix factorization. In\naddition, to raise an expressive power, they proposed Mix-\nture of Softmaxes (MoS) that computes multiple probability\ndistributions from a ﬁnal RNN layer and combines them\nwith a weighted average. (Takase, Suzuki, and Nagata 2018)\nproposed Direct Output Connection (DOC) that is a general-\nization of MoS. They used middle layers in addition to the\nﬁnal layer to compute probability distributions. These meth-\nods (AWD-LSTM-MoS and AWD-LSTM-DOC) achieved\nthe current state-of-the-art perplexities on PTB and WT2. Our\nproposed method can also be combined with MoS and DOC.\nIn fact, Tables 6 and 7 indicate that the proposed method\nfurther improved the performance of them.\n(Kim et al. 2016) introduced character information into\nRNN language models. They applied CNN to character em-\nbeddings for word embedding construction. Their proposed\nmethod achieved perplexity competitive with the basic LSTM\nlanguage model (Zaremba, Sutskever, and Vinyals 2014) even\nthough its parameter size is small. (Jozefowicz et al. 2016)\nalso applied CNN to construct word embeddings from char-\nacter embeddings. They indicated that CNN also positively\naffected the LSTM language model in a huge corpus. (Ver-\nwimp et al. 2017) proposed a method concatenating character\nembeddings with a word embedding to use character informa-\ntion. In contrast to these methods, we used character n-gram\nembeddings to construct word embeddings. To compare the\nproposed method to these methods, we combined the CNN\nproposed by (Kim et al. 2016) with the state-of-the-art LSTM\nlanguage model (AWD-LSTM) (Merity, Keskar, and Socher\n2018b). Our experimental results indicate that the proposed\nmethod outperformed the method using character embed-\ndings (charCNN in Table 5).\nSome previous studies focused on boosting the perfor-\nmance of language models during testing (Grave, Joulin, and\nUsunier 2017; Krause et al. 2017). For example, (Krause et\nal. 2017) proposed dynamic evaluation that updates model\nparameters based on the given correct sequence during eval-\nuation. Although these methods might further improve our\nproposed language model, we omitted these methods since it\nis unreasonable to obtain correct outputs in applications such\nas machine translation.\nEmbedding Construction Previous studies proposed vari-\nous methods to construct word embeddings. (Luong, Socher,\nand Manning 2013) applied Recursive Neural Networks to\nconstruct word embeddings from morphemic embeddings.\n(Ling et al. 2015) applied bidirectional LSTMs to character\nembeddings for word embedding construction. On the other\nhand, (Bojanowski et al . 2017) and (Wieting et al . 2016)\nfocused on character n-gram. They demonstrated that the\nsum of character n-gram embeddings outperformed ordi-\nnary word embeddings. In addition, (Wieting et al . 2016)\nfound that the sum of character n-gram embeddings also\noutperformed word embeddings constructed from character\nembeddings with CNN and LSTM.\nAs an encoder, previous studies argued that additive com-\nposition, which computes the (weighted) sum of embed-\ndings, is a suitable method theoretically (Tian, Okazaki,\nand Inui 2016) and empirically (Muraoka et al . 2014;\nTakase, Okazaki, and Inui 2016). In this paper, we used multi-\ndimensional self-attention to construct word embeddings be-\ncause it can be interpreted as an element-wise weighted sum.\nThrough experiments, we indicated that multi-dimensional\nself-attention is superior to the summation and standard self-\nattention as an encoder.\n7 Conclusion\nIn this paper, we incorporated character information with\nRNN language models. Based on the research in the ﬁeld\n5080\nof word embedding construction (Wieting et al . 2016), we\nfocused on character n-gram embeddings to construct word\nembeddings. We used multi-dimensional self-attention (Shen\net al. 2018) to encode character n-gram embeddings. Our\nproposed charn-MS-vec improved the performance of state-\nof-the-art RNN language models and achieved the best per-\nplexities on Penn Treebank, WikiText-2, and WikiText-103.\nMoreover, we investigated the effect of charn-MS-vec on ap-\nplication tasks, speciﬁcally, machine translation and headline\ngeneration. Our experiments show that charn-MS-vec also\nimproved the performance of a neural encoder-decoder on\nboth tasks.\nAcknowledgments\nThis work was supported by JSPS KAKENHI Grant Num-\nber JP18K18119. We would like to thank the anonymous\nreviewers for their helpful suggestions and comments.\nReferences\nBojanowski, P.; Grave, E.; Joulin, A.; and Mikolov, T. 2017.\nEnriching word vectors with subword information. TACL\n5:135–146.\nBradbury, J.; Merity, S.; Xiong, C.; and Socher, R. 2017.\nQuasi-recurrent neural networks. In Proceedings of ICLR.\nChen, S. F., and Goodman, J. 1996. An empirical study of\nsmoothing techniques for language modeling. InProceedings\nof ACL, 310–318.\nDauphin, Y . N.; Fan, A.; Auli, M.; and Grangier, D. 2016.\nLanguage modeling with gated convolutional networks.\nCoRR abs/1612.08083.\nDu, J.; Tu, Y .-H.; Sun, L.; Ma, F.; Wang, H.-K.; Pan, J.; Liu,\nC.; and Lee, C.-H. 2016. The ustc-iﬂytek system for chime-4\nchallenge. In Proceedings of CHiME-4, 36–38.\nGal, Y ., and Ghahramani, Z. 2016. A Theoretically Grounded\nApplication of Dropout in Recurrent Neural Networks. In\nProceedings of NIPS.\nGrave, E.; Joulin, A.; and Usunier, N. 2017. Improving\nNeural Language Models with a Continuous Cache. In Pro-\nceedings of ICLR.\nInan, H.; Khosravi, K.; and Socher, R. 2017. Tying Word Vec-\ntors and Word Classiﬁers: A Loss Framework for Language\nModeling. In Proceedings of ICLR.\nJozefowicz, R.; Vinyals, O.; Schuster, M.; Shazeer, N.; and\nWu, Y . 2016. Exploring the limits of language modeling.\nKim, Y .; Jernite, Y .; Sontag, D.; and Rush, A. M. 2016.\nCharacter-aware neural language models. In Proceedings of\nAAAI, 2741–2749.\nKiyono, S.; Takase, S.; Suzuki, J.; Okazaki, N.; Inui, K.; and\nNagata, M. 2017. Source-side prediction for neural headline\ngeneration. CoRR.\nKrause, B.; Kahembwe, E.; Murray, I.; and Renals, S. 2017.\nDynamic evaluation of neural sequence models. CoRR.\nLing, W.; Dyer, C.; Black, A. W.; Trancoso, I.; Fermandez,\nR.; Amir, S.; Marujo, L.; and Luis, T. 2015. Finding function\nin form: Compositional character models for open vocabulary\nword representation. In Proceedings of EMNLP, 1520–1530.\nLuong, T.; Pham, H.; and Manning, C. D. 2015. Effective\napproaches to attention-based neural machine translation. In\nProceedings of EMNLP, 1412–1421.\nLuong, T.; Socher, R.; and Manning, C. 2013. Better word\nrepresentations with recursive neural networks for morphol-\nogy. In Proceedings of CoNLL, 104–113.\nMarcus, M. P.; Marcinkiewicz, M. A.; and Santorini, B. 1993.\nBuilding a Large Annotated Corpus of English: The Penn\nTreebank. Computational Linguistics19(2):313–330.\nMelis, G.; Dyer, C.; and Blunsom, P. 2018. On the state of\nthe art of evaluation in neural language models. Proceedings\nof ICLR.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017.\nPointer Sentinel Mixture Models. In Proceedings of ICLR.\nMerity, S.; Keskar, N. S.; and Socher, R. 2018a. An Analysis\nof Neural Language Modeling at Multiple Scales. arXiv\npreprint arXiv:1803.08240.\nMerity, S.; Keskar, N. S.; and Socher, R. 2018b. Regularizing\nand Optimizing LSTM Language Models. In Proceedings of\nICLR.\nMikolov, T.; Karaﬁ´at, M.; Burget, L.; Cernock´y, J.; and Khu-\ndanpur, S. 2010. Recurrent neural network based language\nmodel. In Proceedings of INTERSPEECH, 1045–1048.\nMorishita, M.; Suzuki, J.; and Nagata, M. 2018. Improv-\ning neural machine translation by incorporating hierarchical\nsubword features. In Proceedings of COLING, 618–629.\nMuraoka, M.; Shimaoka, S.; Yamamoto, K.; Watanabe, Y .;\nOkazaki, N.; and Inui, K. 2014. Finding the best model\namong representative compositional models. In Proceedings\nof PACLIC, 65–74.\nNapoles, C.; Gormley, M.; and Van Durme, B. 2012. Anno-\ntated gigaword. In Proceedings of AKBC-WEKEX, 95–100.\nPolyak, B. T., and Juditsky, A. B. 1992. Acceleration of\nStochastic Approximation by Averaging. SIAM Journal on\nControl and Optimization30(4):838–855.\nPress, O., and Wolf, L. 2017. Using the Output Embedding\nto Improve Language Models. In Proceedings of EACL,\n157–163.\nRush, A. M.; Chopra, S.; and Weston, J. 2015. A Neural\nAttention Model for Abstractive Sentence Summarization. In\nProceedings of EMNLP, 379–389.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Improving\nneural machine translation models with monolingual data. In\nProceedings of ACL, 86–96.\nShen, T.; Zhou, T.; Long, G.; Jiang, J.; Pan, S.; and Zhang, C.\n2018. Disan: Directional self-attention network for rnn/cnn-\nfree language understanding. In Proceedings of AAAI.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.;\nand Salakhutdinov, R. 2014. Dropout: A simple way to\nprevent neural networks from overﬁtting. Journal of Machine\nLearning Research15(1):1929–1958.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence to\nSequence Learning with Neural Networks. In Proceedings\nof NIPS, 3104–3112.\n5081\nTakase, S.; Okazaki, N.; and Inui, K. 2016. Composing dis-\ntributed representations of relational patterns. In Proceedings\nof ACL, 2276–2286.\nTakase, S.; Suzuki, J.; and Nagata, M. 2017. Input-to-output\ngate to improve rnn language models. In Proceedings of\nIJCNLP, 43–48.\nTakase, S.; Suzuki, J.; and Nagata, M. 2018. Direct output\nconnection for a high-rank language model. In Proceedings\nof EMNLP, 4599–4609.\nTian, R.; Okazaki, N.; and Inui, K. 2016. The mechanism of\nadditive composition. CoRR abs/1511.08407.\nVerwimp, L.; Pelemans, J.; Van hamme, H.; and Wambacq, P.\n2017. Character-word lstm language models. In Proceedings\nof EACL, 417–427.\nVinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015.\nShow and tell: A neural image caption generator. In Proceed-\nings of CVPR, 3156–3164.\nWan, L.; Zeiler, M.; Zhang, S.; Cun, Y . L.; and Fergus, R.\n2013. Regularization of Neural Networks using DropConnect.\nIn Proceedings of ICML, 1058–1066.\nWen, T.-H.; Gasic, M.; Mrkˇsi´c, N.; Su, P.-H.; Vandyke, D.;\nand Young, S. 2015. Semantically Conditioned LSTM-based\nNatural Language Generation for Spoken Dialogue Systems.\nIn Proceedings of EMNLP, 1711–1721.\nWieting, J.; Bansal, M.; Gimpel, K.; and Livescu, K. 2016.\nCharagram: Embedding words and sentences via character\nn-grams. In Proceedings of EMNLP, 1504–1515.\nYang, Z.; Dai, Z.; Salakhutdinov, R.; and Cohen, W. W. 2018.\nBreaking the softmax bottleneck: A high-rank RNN language\nmodel. In Proceedings of ICLR.\nZaremba, W.; Sutskever, I.; and Vinyals, O. 2014. Recurrent\nneural network regularization. In Proceedings of ICLR.\nZhou, Q.; Yang, N.; Wei, F.; and Zhou, M. 2017. Selec-\ntive encoding for abstractive sentence summarization. In\nProceedings of ACL, 1095–1104.\nZilly, J. G.; Srivastava, R. K.; Koutn´ık, J.; and Schmidhuber,\nJ. 2017. Recurrent Highway Networks. Proceedings of ICML\n4189–4198.\nZolna, K.; Arpit, D.; Suhubdy, D.; and Bengio, Y . 2018.\nFraternal dropout. In Proceedings of ICLR.\nZoph, B., and Le, Q. V . 2017. Neural Architecture Search\nwith Reinforcement Learning. In Proceedings of ICLR.\n5082"
}