{
    "title": "Measuring the Mixing of Contextual Information in the Transformer",
    "url": "https://openalex.org/W4385574263",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Ferrando Monsonís, Javier",
            "affiliations": [
                "Universitat Politècnica de Catalunya"
            ]
        },
        {
            "id": null,
            "name": "Gallego Olsina, Gerard Ion",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Ruiz Costa-Jussà, Marta",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3105604018",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2950768109",
        "https://openalex.org/W2962851944",
        "https://openalex.org/W3153147196",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3101155149",
        "https://openalex.org/W3175752238",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W3164819786",
        "https://openalex.org/W4223554833",
        "https://openalex.org/W3187467055",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W2970120757",
        "https://openalex.org/W2964159778",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W3173902720",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W4229459936",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2346578521",
        "https://openalex.org/W3035563045",
        "https://openalex.org/W3177112880",
        "https://openalex.org/W3196813608",
        "https://openalex.org/W4230405732",
        "https://openalex.org/W3099143320",
        "https://openalex.org/W3176196997",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2594633041",
        "https://openalex.org/W3213645763",
        "https://openalex.org/W2988217457",
        "https://openalex.org/W4286902548",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W1849277567",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W3200704197",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2972498556",
        "https://openalex.org/W3085380432",
        "https://openalex.org/W3213531247",
        "https://openalex.org/W3125516434",
        "https://openalex.org/W3035503910",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W2977944219",
        "https://openalex.org/W4385572790",
        "https://openalex.org/W3174490235",
        "https://openalex.org/W4287110638",
        "https://openalex.org/W4288631803"
    ],
    "abstract": "The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block --multi-head attention, residual connection, and layer normalization-- and define a metric to measure token-to-token interactions within each layer. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides more faithful explanations and increased robustness than gradient-based methods.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8698–8714\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nMeasuring the Mixing of Contextual Information in the Transformer\nJavier Ferrando, Gerard I. Gállego and Marta R. Costa-jussà\nTALP Research Center, Universitat Politècnica de Catalunya\n{javier.ferrando.monsonis,gerard.ion.gallego,marta.ruiz}@upc.edu\nAbstract\nThe Transformer architecture aggregates input\ninformation through the self-attention mecha-\nnism, but there is no clear understanding of\nhow this information is mixed across the en-\ntire model. Additionally, recent works have\ndemonstrated that attention weights alone are\nnot enough to describe the flow of informa-\ntion. In this paper, we consider the whole at-\ntention block –multi-head attention, residual\nconnection, and layer normalization– and de-\nfine a metric to measure token-to-token inter-\nactions within each layer. Then, we aggregate\nlayer-wise interpretations to provide input at-\ntribution scores for model predictions. Experi-\nmentally, we show that our method, ALTI (Ag-\ngregation of Layer-wise Token-to-token Inter-\nactions), provides more faithful explanations\nand increased robustness than gradient-based\nmethods.\n1 Introduction\nThe Transformer (Vaswani et al., 2017) has be-\ncome ubiquitous in different tasks across multiple\ndomains, becoming the architecture of choice for\nmany NLP (Devlin et al., 2019; Brown et al., 2020)\nand computer vision (Dosovitskiy et al., 2021)\ntasks. The self-attention mechanism inside the\nTransformer is in charge of combining contextual\ninformation in its intermediate token representa-\ntions. Attention weights offer a straightforward\nlayer-wise interpretation, as they provide a distri-\nbution over input units, which is often presented as\ngiving the relative importance of each input.\nA prominent line of research has investigated the\nfaithfulness of attention weights (Jain and Wallace,\n2019; Serrano and Smith, 2019; Pruthi et al., 2020;\nWiegreffe and Pinter, 2019; Madsen et al., 2021b)\nwith contradictory conclusions. Some works have\nstudied layer-wise attention patterns by analyzing\nstandard attention (Kovaleva et al., 2019; Clark\net al., 2019; Vig and Belinkov, 2019) and effective\nGradℓ2\nwentherejustbeforeamovie. theservicewasfastbutthat’sit.\ni orderedthemangoandshrimpquesadilla. myfriend\norderednachos. thefoodwasnotgood. i andmyfriendcould\nnotfinishourfoodandwehadstomachachesimmediately.\nIGℓ2\nwentherejustbeforeamovie. theservicewasfastbutthat’sit.\ni orderedthemangoandshrimpquesadilla. myfriend\norderednachos. thefoodwasnotgood. i andmyfriendcould\nnotfinishourfoodandwehadstomachachesimmediately.\nALTI\nwentherejustbeforeamovie. theservicewasfastbutthat’sit.\ni orderedthemangoandshrimpquesadilla. myfriend\norderednachos. thefoodwasnotgood. i andmyfriendcould\nnotfinishourfoodandwehadstomachachesimmediately.\nTable 1: Saliency maps of BERT generated by two\ncommon gradient methods and by our proposed method,\nALTI, for a negative sentiment prediction example of\nYelp dataset.\nattention (Brunner et al., 2020; Sun and Maraso-\nvi´c, 2021), but explaining the Transformer beyond\nattention weights needs further investigation (Lu\net al., 2021).\nKobayashi et al. (2020) extended the explainabil-\nity of the model by also considering the magnitude\nof the vectors involved in the attention mechanism,\nand Kobayashi et al. (2021) went as far as incor-\nporating the layer normalization and the skip con-\nnection in their analysis. While these works have\nhelped better understand the layer-wise behavior\nof the Transformer, there is a mismatch between\nlayer-wise attention distributions and global input\nattributions (Pascual et al., 2021) since interme-\ndiate layers only attend to a mix of input tokens.\nBrunner et al. (2020) quantified the aggregation of\ncontextual information throughout the model with\na gradient attribution method. Although they found\nthe self-attention mechanism greatly mixes the in-\nformation of the model input, they were able to\nrecover the token identity from hidden layers with\nhigh accuracy with a learned linear mapping. This\nphenomenon is partially explained by Kobayashi\n8698\net al. (2021) and Lu et al. (2021), who have shown\nthe relatively small impact of the multi-head at-\ntention, which loses influence with respect to the\nresidual connection, consequently revealing a re-\nduced entanglement of contextual information in\nBERT. Finally, Abnar and Zuidema (2020) pro-\nposed the attention rollout method, which measures\nthe mixing of information by linearly combining\nattention matrices, a method that has been extended\nto Transformers in the visual domain (Chefer et al.,\n2021a,b). A drawback of this method is that it as-\nsumes an equal influence of the skip connection\nand the attention weights.\nIn this work, we propose ALTI, an interpretabil-\nity method that provides input tokens relevancies1\nto the model predictions by measuring the aggre-\ngation of contextual information across layers. We\nuse the attention block decomposition proposed by\nKobayashi et al. (2021) and refine the measure of\nthe contribution of each input token representation\nto the attention block output (layer-wise token-to-\ntoken interactions), based on the properties of the\nrepresentation space and the limitations of previ-\nously proposed metrics. We then aggregate the\nlayer-wise explanations and track the mixing of\ninformation in each token representation, yielding\ninput attributions for the model predictions. Fi-\nnally, in the Text Classification and Subject-Verb\nAgreement tasks, we show ALTI scores higher\nthan gradient-based methods and previous simi-\nlar approaches in two common faithfulness metrics,\nwhile showing greater robustness. The code to re-\nproduce the experiments is publicly available.2\n2 Background\n2.1 Attention Block Decomposition\nThe attention block 3 computations in each layer\n(highlighted parts in Figure 1) can be reformulated\n(Kobayashi et al., 2021) as a simple expression of\nthe layer input representations. Given a sequence of\ntoken representations X = (x1,··· ,xJ) ∈Rd×J,\nand a model with H heads and head dimension\ndh = d/H, the attention block output of the i-th\ntoken yi is computed by applying the layer normal-\nization (LN) over the sum of the residual vector xi,\nand the output of the multi-head attention module\n1We use ‘relevancies’, ‘attributions’, and ‘importances’\ninterchangeably.\n2https://github.com/mt-upc/\ntransformer-contributions.\n3We refer to ‘attention block’ as the multi-head attention,\nresidual connection and layer normalization components.\nLN \nFFN \nLN \nFigure 1: Transformer layer with the modules consid-\nered in the analysis. We compute layer-wise token-\nto-token interactions by measuring the contributions of\neach input token representation xj to the attention block\noutput yi.\n(MHA) ˆxi:\nyi = LN(ˆxi + xi) (1)\nEach head inside MHA computes4 zh\ni ∈Rdh:\nzh\ni =\nJ∑\nj\nAh\ni,jWh\nVxj (2)\nwith Ah\ni,j referring to the attention weight where\ntoken i attends token j, and Wh\nV ∈Rdh×d to a\nlearned weight matrix. ˆxi is calculated by con-\ncatenating each zh\ni and projecting the joint vector\nthrough WO ∈Rd×d:\nˆxi = WO Concat(z1\ni,··· ,zH\ni ) (3)\nThis is equivalent to a sum over heads where each\nzh\ni is projected through the partitioned weight ma-\ntrix Wh\nO ∈Rd×dh and adding the bias bO ∈Rd:\nˆxi =\nH∑\nh\nWh\nOzh\ni + bO (4)\n4The bias vector associated with Wh\nV is omitted for the\nsake of simplicity.\n8699\nFigure 2: Example of attention graph. The relevance(\nR2\n[CLS]\n)\nof the input token [CLS] token to its second\nlayer representation x2\n1 is obtained by summing all pos-\nsible paths (coloured).\nBy swapping summations we can now rewrite Eq. 1\nas:\nyi = LN\n\n\nJ∑\nj\nH∑\nh\nWh\nOAh\ni,jWh\nVxj + bO + xi\n\n\n(5)\nGiven a vector u, LN(u) can be reformulated as\n1\nσ(u)Lu + β(see Appendix A), where L is a linear\ntransformation. Thanks to the linearity of L, we\ncan express yi as:\nyi =\nJ∑\nj\nTi(xj) + 1\nσ(ˆxi + xi)LbO + β (6)\nwhere the transformed vectors Ti(xj) are:\nTi(xj) =\n{ 1\nσ(ˆxi+xi)L∑H\nh WhOAhi,jWhVxj ifj̸=i\n1\nσ(ˆxi+xi)L\n(∑H\nh WhOAhi,jWhVxj+xi\n)\nifj=i\nKobayashi et al. (2021) stated that the contribution\nci,j of each input vector xj to the layer output yi\ncan be estimated by how much its transformed vec-\ntor Ti(xj) affects the result in Eq. 6. They propose\nusing the Euclidean norm of the transformed vector\nas the metric of contribution:\nci,j = ∥Ti(xj)∥2 (7)\n2.2 Attention Rollout\nAbnar and Zuidema (2020) proposed to measure\nthe mixing of contextual information across the\nmodel by relying on attention weights, creating an\nFigure 3: Average cosine similarity of attention block\noutput representations (solid line) and transformed rep-\nresentations (dashed line) in 500 random samples of\nSST-2 dataset.\n\"attention graph\" where nodes represent tokens and\nhidden representations, and edges attention weights.\nTwo nodes in different layers are connected through\nmultiple paths. To add the residual connection, the\nattention weights matrix gets augmented with the\nidentity matrix ˆAl = 0.5Al + 0.5I.\nWe can compute the amount of information flow-\ning from one node to another in different layers by\nmultiplying the edges in each path, and summing\nover the different paths. In the example of Fig-\nure 2, the amount of input information of [CLS] in\nits second layer representation x2\n1 can be obtained\nas ˆA2\n1,1 ·ˆA1\n1,1 + ˆA2\n1,2 ·ˆA1\n2,1 + ˆA2\n1,3 ·ˆA1\n3,1. This\nis equivalent to the dot product between ˆA2\n1,: and\nˆA1\n:,1, which generalizes to the matrix multiplica-\ntion when considering all tokens, giving the input\nrelevance matrix at layer l, Rl ∈RJ×J:\nRl = ˆAl ·ˆAl−1 · ··· ·ˆA1 (8)\n3 Proposed Approach\nThe decomposition of the attention block, repre-\nsented as a sum of vectors in Eq. 6, allows us to\ninterpret token-to-token interactions within each\nlayer. Kobayashi et al. (2021) proposed to measure\nthe influence of each input token with the ℓ2 norm\nof the transformed vectors (Eq. 7). We present two\nreasons why this estimation may not be accurate:\n1. A property of the contextual representations\nin Transformer-based models is that they are\nhighly anisotropic (Ethayarajh, 2019), i.e.\nthe expected cosine similarity of randomly\nsampled token representations tend to be\nclose to 1 (solid lines in Figure 3). How-\never, transformed representations exhibit re-\nduced anisotropy, especially for the first lay-\ners, where there is almost isotropy (dashed\nlines in Figure 3), i.e. they are more randomly\n8700\n= =\nFigure 4: The self-attention block (left) at each position ican be decomposed as a summation of transformed input\nvectors (middle). The closest vector (T2(x2)) contributes the most to y2. We obtain a matrix of contributions C\n(right) reflecting layer-wise token-token interactions.\nspread across the space. This reinforces the\nneed of accounting for the vector’s orientation\nin space, as opposed to solely relying on their\nnorm.\n2. Recent studies (Kovaleva et al., 2021; Cai\net al., 2021; Luo et al., 2021) have found\nthat some embedding dimensions acquire dis-\nproportionately large values, dominating the\nsimilarity measures (Timkey and van Schijn-\ndel, 2021). ℓ2 normalized metrics, since they\nsquare each vector component, unavoidably\nweigh heavily the outlier dimensions.\nWe can analyze the expression in Eq. 6 asTi(xj)\nvectors contributing to the sum resultant yi. We\npropose to measure how much each transformed\nvector contributes to the sum by means of its dis-\ntance to the output vector yi. We expect that the\ncloser the vector is to yi, the higher its contribu-\ntion (Figure 4). In this way, we take into account\nwhere each transformed vector lies in the represen-\ntation space (Reason 1). Due to its robustness to\nthe aforementioned idiosyncratic dimensions (Rea-\nson 2), we use ℓ1 norm, i.e. the Manhattan distance\nbetween the attention block output and the trans-\nformed vector:\ndi,j = ∥yi −Ti(xj)∥1 (9)\nThe level of contribution ofxj to yi, ci,j, is propor-\ntional to the proximity of Ti(xj) to yi. The closer\nthe transformed vector is to yi, the larger its con-\ntribution. We measure proximity as the negative of\nManhattan distance −di,j. Finally, we neglect the\ncontributions of those vectors lying beyond the ℓ1\nlength of yi:\nci,j = max(0,−di,j + ∥yi∥1)∑\nkmax(0,−di,k + ∥yi∥1) (10)\n[CLS] a clever blend of fact and fiction . [SEP]\nFigure 5: Top: input (columns) attribution scores RL\nprovided by ALTI in BERT’s last layer token representa-\ntions (rows). Bottom: attribution scores corresponding\nto [CLS] token (RL\n[CLS]).\nComputing Eq. 9 and Eq. 10 for all yi gives us the\ncontributions matrix C ∈RJ×J containing every\ntoken-to-token interaction within the layer.\nWe also propose to consider contributions across\nthe model as a \"contribution graph\", similar to the\nattention graph in Section §2.2 but using the ob-\ntained contributions instead of attention weights.\nWe can then track the amount of contextual infor-\nmation from the input tokens in intermediate token\nrepresentations, which we use as input attribution\nscores. By combining linearly the contribution ma-\ntrices up to layer l(Figure 5 bottom) we get:\nRl = Cl ·Cl−1 · ··· ·C1 (11)\n4 Experimental Setup\nWe perform our experiments in the Text Classifica-\ntion (TC) and the Subject-Verb Agreement (SV A)\ntasks. The former evaluates how models classify\n8701\nan entire input sequence, the latter assesses the\nability of a model to capture syntactic phenom-\nena (Linzen et al., 2016; Goldberg, 2019). For\nthe TC task, we use the Stanford Sentiment Tree-\nbank v2 (SST-2) (Socher et al., 2013) composed\nof short sentences, IMDB (Maas et al., 2011) with\nmovies reviews and longer inputs than SST2, and\nYelp Dataset Challenge (Zhang et al., 2015) con-\ntaining user’s reviews from businesses of similar\nlength than in IMDB. All of them have positive\nand negative sentiment sentences. For the SV A\ntask, we use Linzen et al. (2016) dataset, which\nincludes sentences from Wikipedia containing a\npresent-tense verb that agrees in grammatical num-\nber (singular/plural) with the head of the subject.\nThe sentence is fed into the model with its verb\nmasked, and the model is asked to predict if the\nmasked verb is singular or plural5 (binary classifi-\ncation):\nAt least fourplayers from the 1983 draft now\n[MASK]\nplural\nas coaches.\n4.1 Models\nFor our experiments we consider three common\nTransformer pre-trained models 6 with different\nsizes and pre-training procedures: BERT (Devlin\net al., 2019), DistilBERT (Sanh et al., 2019) and\nRoBERTa (Liu et al., 2019).\nIn the TC task, we use fine-tuned models pro-\nvided by TextAttack (Morris et al., 2020). For\nthe robustness analysis in Section §5.2, we fine-\ntune 10 pre-trained BERT models on SST-2 with\nthe recommended hyperparameters in Devlin et al.\n(2019). We compute attribution scores from the\nrow RL\n[CLS] ∈ RJ (Figure 5 bottom) that corre-\nsponds to the final layer [CLS] embedding, con-\nsidered a sentence representation for classification\ntasks. Regarding the SV A task, we split Linzen\net al. (2016) dataset into 60%/20%/20% for train-\ning, validation, and testing respectively, and fine-\ntune a pre-trained BERT model. We use the input\nrelevances of RL\n[MASK].\n4.2 Faithfulness Metrics\nAn interpretation is considered to be faithful if it\naccurately reflects a model’s decision-making pro-\n5As a general rule, a singular verb has an ‘s’ added to it\nin the present tense, such as eats, plays, is, has. A plural verb\ndoes not have an ‘s’ added to it.\n6We use the models available at https://github.com/\nhuggingface/transformers (Wolf et al., 2020).\ncess. A well-established method for measuring\nfaithfulness is by deleting parts of the input sen-\ntence x and observing the change in the predicted\nprobability. Two common erasure-based metrics\nare comprehensiveness (comp.) and sufficiency\n(suff.) (DeYoung et al., 2020). Chan et al. (2022)\nhave demonstrated that they have higher diagnostic-\nity, i.e. they favor faithful interpretations over ran-\ndomly generated ones, and lower time complexity\nthan other well-known faithfulness metrics. Com-\nprehensiveness and sufficiency are defined as:\nComprehensiveness. Measures the change in\nprobability of the predicted class after removing\nimportant tokens:\nComp. = 1\n|B|+ 1\n∑\nk∈B\n(f(x) −f(x\\r:k%)) (12)\nwhere r:k% refers to the top-k% most important\ntokens obtained by an interpretability method. The\nhigher the drop in the probability, the more faithful\nthe interpretation.\nSufficiency. Captures if important tokens are\nenough to retain the original prediction:\nSuff. = 1\n|B|+ 1\n∑\nk∈B\n(f(x) −f(r:k%)) (13)\nLower values of sufficiency indicate a more faith-\nful interpretation, since, in that case, the prediction\ndoesn’t change when considering only the impor-\ntant tokens. As in the original paper, for both met-\nrics we use B = {0,5,10,20,50}.\n4.3 Input Attribution Methods\nInput attribution methods rank input tokens in ac-\ncordance with how they impact model predictions.\nThey can be divided into: gradient-based meth-\nods, perturbation-based, and those relying on the\nattention mechanism. The gradient of the model’s\noutput with respect to the input embeddings is of-\nten used as a baseline of faithfulness interpretation\n(Jain and Wallace, 2019). Atanasova et al. (2020);\nZaman and Belinkov (2022) show that gradient-\nbased methods perform better than other inter-\npretability methods, regarding different faithful-\nness metrics. Finally, perturbation-based methods\n(Zeiler and Fergus, 2014; Ribeiro et al., 2016) com-\npute attributions by replacing the original sentence\nwith a modification. Zaman and Belinkov (2022)\n8702\nshow that erasure-based methods, such as com-\nprehensiveness and sufficiency favor perturbation-\nbased methods attributed to noise due to the OOD\nperturbations.\nGradient. Considering the model f taking as in-\nput a sequence of embeddings X0 ∈Rd×J, f can\nbe approximated by the linear part of the Taylor-\nexpansion at a baseline point (Simonyan et al.,\n2014), f(X0) ≈∇f(X0) ·X0. Then, ∇f(X0)\ngives a score per embedding dimension, which is\noften considered as how sensitive the model is to\neach input dimension when predicting a certain\nclass. To get per token saliency scores (Li et al.,\n2016), we obtain the gradient vector corresponding\nto the j-th token ∇x0\nj\nf(X0) =∂f(X0)\n∂x0\nj\n. Then, we\naggregate the gradient vector into a scalar using the\nℓ2 norm (Gradℓ2 ):\nattr(xj) =\n∇x0\nj\nf(X0)\n\n2\n(14)\nRecently, Bastings et al. (2021) showcased (in\nBERT and SST-2) the high degree of faithfulness\nof Gradℓ2 method.\nGradient × input. This method (Shrikumar\net al., 2016) performs the multiplication of the gra-\ndient and the corresponding input embedding. Each\ncomponent of the gradient vector gets multiplied\nby the corresponding component of the embedding.\nFollowing (Atanasova et al., 2020; Zaman and Be-\nlinkov, 2022), we aggregate the component scores\ninto a single scalar by taking the ℓ2 norm (G ×Iℓ2 )\nas in Eq. 14 or by taking the mean ( G ×Iµ) as\nfollows:\nattr(xj) = 1\nN\nd∑\nk=1\n|∇x0\njk\nf(X0) ·x0\njk| (15)\nIntegrated Gradients. Integrated gradients (Sun-\ndararajan et al., 2017) approximates the integral of\ngradients of the model’s output with respect to the\ninputs along the straight line path from a baseline\ninput B, to the actual input. The attribution score\nfor each embedding dimension is defined as:\n(x0\njk −bjk) · 1\nm\nm∑\nc=1\n∇ˆx0\njk\nf( ˆX0\nc) (16)\nwhere ˆX0\nc = B + c\nm(X0 −B), and mnumber of\nsteps. As baseline, we use repeated [MASK] vectors\nfor each word except for [CLS] and [SEP] (Sajjad\net al., 2021), and 100 steps. We aggregate ( IGℓ2\nand IGµ) the attribution scores of the embedding\ndimensions of Eq. 16 to obtain attr(xj).\nFinally, we normalize the obtained attribution\nscores in the range so that they sum 1. We use the\nCaptum library implementations (Kokhlikyan et al.,\n2020).\nAttention. Attention-based methods that provide\ninput attributions include Attention Rollout (Abnar\nand Zuidema, 2020), as described in Section §2.2.\nConcurrent to this work, Modarressi et al. (2022)\npropose Globenc, which combines Attention Roll-\nout aggregation technique with Kobayashi et al.\n(2021) layer-wise contributions (Eq. 7), with the\naddition of the layer normalization of the FFN mod-\nule. In Section §5.4 we compare ALTI to Globenc.\n5 Results\nIn this section, we present quantitative and qual-\nitative results comparing ALTI with other input\nattribution methods.\n5.1 Faithfulness Results\nIn Table 2 we show comprehensiveness and suffi-\nciency results for the three models and four datasets.\nIt can be seen that across every different configu-\nration, our proposed ALTI method outperforms\nother input attribution methods. Regarding com-\nprehensiveness, datasets with short sentences like\nSST-2 and SV A (Figure 6 (c)) provide small dif-\nferences between methods. This is expected since\nthese datasets are simpler, and therefore, interpre-\ntations can more easily find the important tokens.\nHowever, for datasets containing longer inputs with\nmultiple sentences, like IMDB and Yelp, ALTI\nclearly stands out. This can be observed in Fig-\nure 6 (a) and (b), where the probability drop in\nthe model prediction is shown when removing one\ntoken at a time. We observe small differences in\nperformance within gradient-based methods across\ndatasets and models, with IGℓ2 performing the best\non average among them, agreeing with the obser-\nvations of Atanasova et al. (2020). However, ALTI\noutperforms IGℓ2 by 58% on average in compre-\nhensiveness, and by 38% in sufficiency. Results of\nRoBERTa and DistilBERT on every dataset can be\nfound in Appendix B.\nPrevious research concluded that faithfulness re-\nsults for evaluating different interpretability meth-\nods are task and model dependent (Bastings et al.,\n2021; Madsen et al., 2021a). Interestingly, al-\nthough for the rest of the methods results vary\n8703\nBERT RoBERTa DistilBERT\nSST-2 Yelp IMDB SV A SST-2 SST-2\nMethods Comp.↑Suff.↓ Comp. Suff. Comp. Suff. Comp. Suff. Comp. Suff. Comp. Suff.\nGradℓ2 0.204 0.076 0.083 0.101 0.192 0.052 0.284 0.145 0.190 0.075 0.230 0.066\nIGℓ2 0.223 0.084 0.111 0.024 0.214 0.06 0.315 0.171 0.223 0.074 0.295 0.048\nIGµ 0.211 0.112 0.106 0.022 0.179 0.063 0.317 0.174 0.231 0.067 0.279 0.064\nG ×Iℓ2 0.199 0.080 0.081 0.104 0.197 0.056 0.279 0.149 0.187 0.081 0.235 0.065\nG ×Iµ 0.207 0.073 0.087 0.098 0.213 0.054 0.285 0.145 0.191 0.078 0.237 0.065\nRollout 0.074 0.270 0.076 0.102 0.09 0.185 0.108 0.292 0.076 0.179 0.152 0.147\nGlobenc 0.174 0.125 0.118 0.053 0.207 0.117 0.251 0.178 0.161 0.119 0.24 0.072\nALTI 0.317 0.044 0.255 0.022 0.308 0.031 0.372 0.088 0.269 0.057 0.332 0.034\nTable 2: Faithfulness results of the different interpretability methods for BERT, RoBERTa and DistilBERT on four\ndifferent datasets. ↑means a higher number indicates better performance, while ↓means the opposite.\n(a)\n (b)\n (c)\nFigure 6: Probability drop in BERT predictions when removing important tokens, obtained by different interpretabil-\nity methods. We show results on three datasets.\nacross models and tasks, we observe ALTI repeat-\nedly performs the best across different tasks and\nmodels.\nIn the qualitative examples in Tables 1 and 3\nwe can observe that gradient-based methods often\nmiss the relevant tokens that drive the model’s neg-\native prediction. ALTI consistently assigns high\nrelevance to spans of text that have a negative con-\nnotation, such as ‘depressing’, ‘don’t plan on re-\nturning’ in Table 3, or ‘food not good’, ‘stomach\naches’ in Table 1, as expected from a negative sen-\ntiment prediction. We observe that, as opposed to\nALTI, gradient-based methods become less accu-\nrate with longer sequences. A very large example\nwith a positive sentiment prediction can be found in\nAppendix C Table 9, where ALTI accurately picks\nas important tokens those with positive meanings.\n5.2 Robustness Analysis\nWe perform a study to investigate the robustness of\ndifferent interpretability methods based on the im-\nplementation invarianceproperty defined by Sun-\ndararajan et al. (2017). Given a set of models with\nthe same architecture, and trained with the same\ndata, but only differing in their random weight ini-\ntialization, it compares how different are the input\nFigure 7: Jaccard-25% similarity score between the\ninterpretations of each method in 10 BERT’s random\nseeds.\nattribution scores between the models, for the same\ninterpretability method. If the predictions of the\nmodels are also identical, i.e. models are function-\nally equivalent, we would expect input attributions\nalso to be identical. Zafar et al. (2021) perform this\ntest on two identical neural text classifiers(i,j), dif-\nfering in their random weight initialization. Since\nthe vast majority of the predictions are the same\nfor both models, they consider them to be almost\nfunctionally identicalmodels. Then, they measure\nthe Jaccard similarity score between the top-25%\ntokens ranked based on their importance as speci-\nfied by an input attribution method, for modeliand\n8704\nGradℓ2\num . it ’ s okay , i guess . they have food at decent prices , but the isles are narrow , everything needs a good cleaning and\nrepainting , and it just felt dark and depressing . otherwise it ’ s all right , but i don ’ t plan on returning here .\nIGℓ2\num . it ’ s okay , i guess . they have food at decent prices , but the isles are narrow , everything needs a good cleaning and\nrepainting , and it just felt dark and depressing . otherwise it ’ s all right , but i don ’ t plan on returning here .\nALTI\num . it ’ s okay , i guess . they have food at decent prices , but the isles are narrow , everything needs a good cleaning and\nrepainting , and it just felt dark and depressing . otherwise it ’ s all right , but i don ’ t plan on returning here .\nTable 3: Saliency maps of BERT generated by two common gradient methods and by our proposed method, ALTI,\nfor a negative sentiment predictions of Yelp dataset.\nFigure 8: Spearman’s rank correlation between the inter-\npretations of each method in 10 BERT’s random seeds.\nj. If the top-25% tokens by the two attributions\ncoincide, Jaccard-25%(i,j) = 1. In case tokens\ndon’t overlap, Jaccard-25%(i,j) = 0.\nWe perform the robustness test with 10 pre-\ntrained BERT models from the MultiBERTs (Sel-\nlam et al., 2022), which only differ in their ran-\ndom weight initialization. For each interpretability\nmethod we compute Jaccard-25% score between\nall the different pairs of models. In Figure 7 we\nshow the distribution of the obtained scores. We\nalso compute the Spearman’s rank correlation co-\nefficient (Figure 8), which evaluates how well the\nrelationship between the two ranked interpretations\ncan be described using a monotonic function. We\ncan observe that ALTI provides more homogeneous\ninterpretations across identical models in terms of\nsimilarity and correlation, suggesting that it is a\nmore robust interpretability method than gradient-\nbased methods.\n5.3 Ablation Study\nWe inspect the importance of the different compo-\nnents conforming ALTI.\nLayer-wise token contributions. We compare\nthe effect of our token contributions’ measurement\nin Eq. 9 and Eq. 10 against previous approach by\nKobayashi et al. (2021) (Norms) described in Eq. 7\nBERT RoBERTa DistilBERT\nDataset Metric ALTIℓ2 Norms ALTIℓ2 Norms ALTIℓ2 Norms\nSST-2Comp.↑ 0.3 0.134 0.23 0.168 0.292 0.157Suff.↓ 0.045 0.18 0.075 0.135 0.051 0.152\nIMDBComp. 0.2860.148 0.203 0.133 0.254 0.175Suff. 0.031 0.115 0.086 0.162 0.053 0.104\nYelp Comp. 0.2120.082 0.108 0.064 0.2 0.093Suff. 0.02 0.057 0.104 0.18 0.031 0.114\nSV AComp. 0.3740.273 0.379 0.275 0.357 0.315Suff. 0.088 0.152 0.114 0.178 0.096 0.117\nTable 4: Faithfulness results of BERT, RoBERTa and\nDistilBERT comparing ALTI ℓ2 with the Norms ap-\nproach.\nby aggregating each type of contributions with the\nRollout method. To isolate the influence of the\nnorm choice, we use the ℓ2 in Eq. 9 and Eq. 10\n(ALTI ℓ2). Results in Table 4 show our proposed\nlayer-wise contribution measurement largely im-\nproves previous approach.\nNorm choice in ALTI. We also evaluate the elec-\ntion of the norm in our proposed approach. In\nTable 5 we show faithfulness results considering ℓ1\nand ℓ2. In almost every setting ℓ1 outperforms ℓ2.\nRemarkably, the advantage of the ℓ1 is less notice-\nable on BERT, which we hypothesize is explained\nby the reduced anisotropy of its representations\n(Figure 3).\nBERT RoBERTa DistilBERT\nDataset Metric ℓ1 ℓ2 ℓ1 ℓ2 ℓ1 ℓ2\nSST-2 Comp.↑ 0.317 0.3 0.269 0.23 0.332 0.292\nSuff.↓ 0.044 0.045 0.057 0.075 0.034 0.051\nIMDB Comp. 0.308 0.286 0.266 0.203 0.304 0.254\nSuff. 0.031 0.031 0.05 0.086 0.039 0.053\nYelp Comp. 0.221 0.212 0.138 0.108 0.237 0.2\nSuff. 0.02 0.02 0.07 0.104 0.017 0.031\nSV A Comp. 0.372 0.374 0.39 0.379 0.382 0.357\nSuff. 0.088 0.088 0.109 0.114 0.084 0.096\nTable 5: Faithfulness results of BERT, RoBERTa and\nDistilBERT using ℓ1 and ℓ2 norms.\n8705\nFigure 9: Probability drop in BERT predictions when\nremoving important tokens, results show mean and SD\nin BERT across 10 seeds in SST-2 dataset.\n5.4 Addition of Layer Norm 2\nConcurrent work (Modarressi et al., 2022) present\nGlobenc method, which aggregates the contribu-\ntions obtained by (Kobayashi et al., 2021) in Eq. 7\nwith Attention Rollout method. Moreover, they add\nthe layer normalization (LN2) of the Feed-forward\nmodule of the Transformer layer into their method.\nWe evaluate the faithfulness of the interpretations\nprovided by Globenc in Table 2, and although it\nimproves the Rollout baseline, is far from the re-\nsults obtained with ALTI. We consider analyzing\nthe influence of the second layer normalization by\nincluding it in ALTI method. The probability drop\nin SST-2 across 10 BERT seeds (Figure 9) shows\nthe influence of LN2 is negligible. We observe\nsimilar patterns across models and datasets.\n6 Conclusions\nIn this paper, we have presented ALTI, an input at-\ntribution method that quantifies the mixing of infor-\nmation in the Transformer. We have demonstrated\nthat with accurate layer-wise token-to-token contri-\nbution measurements relying on ℓ1-based metrics,\nthe interpretable attention decomposition of the at-\ntention block is a powerful tool when combined\nwith the rollout method. Empirically, we show that\nALTI outperforms every input attribution method\nwe have experimented with in two common faith-\nfulness metrics, while showing greater robustness.\nOverall, we believe this opens new possibilities for\nstudying contextual information aggregation across\nthe Transformer.\nLimitations\nALTI measures the amount of contextual informa-\ntion in each layer representation of the Transformer.\nFrom the influence of each input token to the last\nlayer representation we extract input attributions\nfor the model prediction. However, our method\ndoes not consider the classifier on top of the Trans-\nformer. Therefore, our proposed method doesn’t\nprovide explanations for each of the output classes,\nas opposed to gradient-based methods. We also\nunderline that faithfulness in this work is evaluated\nvia sufficiency and comprehensiveness metrics.\nEthical Considerations\nALTI provides explanations about input attributions\nin the Transformer. By itself, we are not aware of\nany ethical implications of the methodology, which\ndoes not take into account any subjective priors.\nTo prove its usefulness, we have used two differ-\nent benchmarks, text classification, and subject-\nverb agreement. As far as we are concerned, these\nbenchmarks have been used in the past without rais-\ning major ethical considerations. Therefore, we do\nnot have any major issue to report in this section.\n7 Acknowledgements\nWe would like to thank the anonymous review-\ners for their useful comments. Javier Fer-\nrando and Gerard I. Gállego are supported by\nthe Spanish Ministerio de Ciencia e Innovación\nthrough the project PID2019-107579RB-I00 / AEI\n/ 10.13039/501100011033.\nReferences\nSamira Abnar and Willem Zuidema. 2020. Quantify-\ning attention flow in transformers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4190–4197, On-\nline. Association for Computational Linguistics.\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020. A diagnostic\nstudy of explainability techniques for text classifi-\ncation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 3256–3274, Online. Association for\nComputational Linguistics.\nJasmijn Bastings, Sebastian Ebert, Polina Zablotskaia,\nAnders Sandholm, and Katja Filippova. 2021. \"will\nyou find these shortcuts?\" a protocol for evaluating\nthe faithfulness of input salience methods for text\nclassification.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\n8706\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Wat-\ntenhofer. 2020. On identifiability in transformers. In\nInternational Conference on Learning Representa-\ntions.\nXingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth\nChurch. 2021. Isotropy in the contextual embed-\nding space: Clusters and manifolds. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net.\nChun Sik Chan, Huanqi Kong, and Liang Guanqing.\n2022. A comparative study of faithfulness metrics\nfor model interpretability methods. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 5029–5038, Dublin, Ireland. Association for\nComputational Linguistics.\nHila Chefer, Shir Gur, and Lior Wolf. 2021a. Generic\nattention-model explainability for interpreting bi-\nmodal and encoder-decoder transformers. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), pages 397–406.\nHila Chefer, Shir Gur, and Lior Wolf. 2021b. Trans-\nformer interpretability beyond attention visualization.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n782–791.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\nEric Lehman, Caiming Xiong, Richard Socher, and\nByron C. Wallace. 2020. ERASER: A benchmark to\nevaluate rationalized NLP models. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4443–4458, Online.\nAssociation for Computational Linguistics.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on\nLearning Representations.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. CoRR, abs/1901.05287.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is not only a weight:\nAnalyzing transformers with vector norms. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7057–7075, Online. Association for Computa-\ntional Linguistics.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2021. Incorporating Residual and Nor-\nmalization Layers into Analysis of Masked Language\nModels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4547–4568, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin,\nEdward Wang, Bilal Alsallakh, Jonathan Reynolds,\nAlexander Melnikov, Natalia Kliushkina, Carlos\nAraya, Siqi Yan, and Orion Reblitz-Richardson. 2020.\nCaptum: A unified and generic model interpretability\nlibrary for pytorch.\nOlga Kovaleva, Saurabh Kulshreshtha, Anna Rogers,\nand Anna Rumshisky. 2021. BERT busters: Outlier\ndimensions that disrupt transformers. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 3392–3405, Online. Association\nfor Computational Linguistics.\n8707\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for Com-\nputational Linguistics.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 681–691, San Diego, California.\nAssociation for Computational Linguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Trans. Assoc. Comput. Lin-\nguistics, 4:521–535.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nKaiji Lu, Zifan Wang, Piotr Mardziel, and Anupam\nDatta. 2021. Influence patterns for explaining in-\nformation flow in BERT. In Advances in Neural\nInformation Processing Systems.\nZiyang Luo, Artur Kulmizev, and Xiaoxi Mao. 2021.\nPositional artefacts propagate through masked lan-\nguage model embeddings. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5312–5327, Online. Association\nfor Computational Linguistics.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nAndreas Madsen, Nicholas Meade, Vaibhav Adlakha,\nand Siva Reddy. 2021a. Evaluating the faithfulness of\nimportance measures in nlp by recursively masking\nallegedly important tokens and retraining.\nAndreas Madsen, Siva Reddy, and Sarath Chandar.\n2021b. Post-hoc interpretability for neural nlp: A\nsurvey.\nAli Modarressi, Mohsen Fayyaz, Yadollah\nYaghoobzadeh, and Mohammad Taher Pile-\nhvar. 2022. Globenc: Quantifying global token\nattribution by incorporating the whole encoder layer\nin transformers.\nJohn Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby,\nDi Jin, and Yanjun Qi. 2020. Textattack: A frame-\nwork for adversarial attacks, data augmentation, and\nadversarial training in nlp. In Proceedings of the\n2020 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations,\npages 119–126.\nDamian Pascual, Gino Brunner, and Roger Wattenhofer.\n2021. Telling BERT’s full story: from local attention\nto global aggregation. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 105–124, Online. Association for Computa-\ntional Linguistics.\nDanish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham\nNeubig, and Zachary C. Lipton. 2020. Learning to\ndeceive with attention-based explanations. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4782–\n4793, Online. Association for Computational Lin-\nguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \"why should i trust you?\": Explain-\ning the predictions of any classifier. In Proceedings\nof the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, KDD ’16,\npage 1135–1144, New York, NY , USA. Association\nfor Computing Machinery.\nHassan Sajjad, Narine Kokhlikyan, Fahim Dalvi, and\nNadir Durrani. 2021. Fine-grained interpretation and\ncausation analysis in deep NLP models. In Proceed-\nings of the 2021 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies: Tutorials,\npages 5–10, Online. Association for Computational\nLinguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. ArXiv,\nabs/1910.01108.\nThibault Sellam, Steve Yadlowsky, Ian Tenney, Jason\nWei, Naomi Saphra, Alexander D’Amour, Tal Linzen,\nJasmijn Bastings, Iulia Raluca Turc, Jacob Eisenstein,\nDipanjan Das, and Ellie Pavlick. 2022. The multiB-\nERTs: BERT reproductions for robustness analysis.\nIn International Conference on Learning Representa-\ntions.\nSofia Serrano and Noah A. Smith. 2019. Is attention in-\nterpretable? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2931–2951, Florence, Italy. Association for\nComputational Linguistics.\nAvanti Shrikumar, Peyton Greenside, Anna Shcherbina,\nand Anshul Kundaje. 2016. Not just a black box:\nLearning important features through propagating ac-\ntivation differences. CoRR, abs/1605.01713.\n8708\nKaren Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. 2014. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. In 2nd International Conference on Learn-\ning Representations, ICLR 2014, Banff, AB, Canada,\nApril 14-16, 2014, Workshop Track Proceedings.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nKaiser Sun and Ana Marasovi´c. 2021. Effective atten-\ntion sheds light on interpretability. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 4126–4135, Online. Association\nfor Computational Linguistics.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Pro-\nceedings of the 34th International Conference on\nMachine Learning, volume 70 of Proceedings of Ma-\nchine Learning Research, pages 3319–3328, Interna-\ntional Convention Centre, Sydney, Australia. PMLR.\nWilliam Timkey and Marten van Schijndel. 2021. All\nbark and no bite: Rogue dimensions in transformer\nlanguage models obscure representational quality.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4527–4546, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing\nthe structure of attention in a transformer language\nmodel. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 63–76, Florence, Italy. As-\nsociation for Computational Linguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 11–20, Hong Kong, China. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nMuhammad Bilal Zafar, Michele Donini, Dylan Slack,\nCedric Archambeau, Sanjiv Das, and Krishnaram\nKenthapadi. 2021. On the lack of robust interpretabil-\nity of neural text classifiers. In Findings of the Asso-\nciation for Computational Linguistics: ACL-IJCNLP\n2021, pages 3730–3740, Online. Association for\nComputational Linguistics.\nKerem Zaman and Yonatan Belinkov. 2022. A multilin-\ngual perspective towards the evaluation of attribution\nmethods in natural language inference.\nMatthew D. Zeiler and Rob Fergus. 2014. Visualizing\nand understanding convolutional networks. In Com-\nputer Vision – ECCV 2014, pages 818–833, Cham.\nSpringer International Publishing.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\n8709\nA Layer Normalization decomposition\nLayer normalization acting over an input u can be\ndefined as: LN(u) = u−µ(u)\nσ(u) ⊙γ+ β, where µ\nand σ compute the mean and standard deviation\nof u, and γand βrefer to the element-wise trans-\nformation and bias respectively. LN(u) can be\ndecomposed into 1\nσ(u)Lu + β, where L is a linear\ntransformation:\nL:=\n\n\nγ1 0 ··· 0\n0 γ2 ··· 0\n··· ··· ··· ···\n0 0 ··· γn\n\n\n\n\nn−1\nn −1\nn ··· −1\nn\n−1\nn\nn−1\nn ··· −1\nn\n··· ··· ··· ···\n−1\nn −1\nn ··· n−1\nn\n\n\nThe linear map on the right subtracts the mean\nto the input vector, u′= u −µ(u). The left ma-\ntrix performs the hadamard product with the layer\nnormalization weights (u′⊙γ).\nB RoBERTa and DistilBERT Results\nIMDB Yelp SV A\nMethods Comp.↑ Suff.↓ Comp. Suff. Comp. Suff.\nGradℓ2 0.266 0.054 0.111 0.073 0.338 0.107\nIGℓ2 0.275 0.055 0.128 0.054 0.372 0.108\nIGµ 0.244 0.059 0.108 0.056 0.365 0.118\nG×Iℓ2 0.281 0.053 0.115 0.075 0.333 0.111\nG×Iµ 0.282 0.053 0.117 0.074 0.335 0.11\nRollout 0.183 0.089 0.092 0.114 0.241 0.179\nGlobenc 0.252 0.066 0.124 0.066 0.341 0.108\nALTI 0.304 0.039 0.237 0.017 0.382 0.084\nTable 6: Faithfulness results of the different inter-\npretability methods for DistilBERT on IMDB, Yelp and\nSV A datasets.↑means a higher number indicates better\nperformance, while ↓means the opposite.\nIMDB Yelp SV A\nMethods Comp.↑ Suff.↓ Comp. Suff. Comp. Suff.\nGradℓ2 0.216 0.083 0.075 0.122 0.273 0.165\nIGℓ2 0.2 0.084 0.087 0.11 0.363 0.163\nIGµ 0.215 0.083 0.102 0.094 0.351 0.161\nG×Iℓ2 0.183 0.109 0.066 0.149 0.28 0.169\nG×Iµ 0.225 0.08 0.083 0.12 0.27 0.167\nRollout 0.077 0.197 0.031 0.208 0.223 0.183\nGlobenc 0.154 0.086 0.065 0.12 0.305 0.17\nALTI 0.266 0.05 0.138 0.07 0.39 0.109\nTable 7: Faithfulness results of the different inter-\npretability methods for RoBERTa on IMDB, Yelp and\nSV A datasets.↑means a higher number indicates better\nperformance, while ↓means the opposite.\nC Qualitative Examples\nGradℓ2\nfriendlystaffandniceselectionofvegetarianoptions.foodisjustokay,\nnotgreat.makesmewonderwhyeveryonelikesfoodfightsomuch.\nG×Iℓ2\nfriendlystaffandniceselectionofvegetarianoptions.foodisjustokay,\nnotgreat.makesmewonderwhyeveryonelikesfoodfightsomuch.\nIGℓ2\nfriendlystaffandniceselectionofvegetarianoptions.foodisjustokay,\nnotgreat.makesmewonderwhyeveryonelikesfoodfightsomuch.\nALTI\nfriendlystaffandniceselectionofvegetarianoptions.foodisjustokay,\nnotgreat.makesmewonderwhyeveryonelikesfoodfightsomuch.\nTable 8: Saliency maps of BERT generated by three\ncommon gradient methods and by our proposed method,\nALTI, for a negative sentiment prediction example of\nYelp dataset.\n8710\nGradℓ2\nlow budget horror movie . if you don ’ t raise your expectations too high , you ’ ll probably enjoy\nthis little flick . beginning and end are pretty good , middle drags at times and seems to go nowhere\nfor long periods as we watch the goings on of the insane that add atmosphere but do not advance\nthe plot . quite a bit of gore . i enjoyed bill mcghee ’ s performance which he made quite believable\nfor such a low budget picture , he managed to carry the movie at times when nothing much seemed\nto be happening . nurse charlotte beale , played by jesse lee , played her character well so be\nprepared to want to slap her toward the end ! she makes some really stupid mistakes but then , that ’\ns what makes these low budget movies so good ! i would have been out of that place and five states\naway long before she even considered that it might be a good idea to leave ! if you enjoy this movie\n, try committed from 1988 which is basically a rip off of this movie .\nG×Iℓ2\nlow budget horror movie . if you don ’ t raise your expectations too high , you ’ ll probably enjoy\nthis little flick . beginning and end are pretty good , middle drags at times and seems to go nowhere\nfor long periods as we watch the goings on of the insane that add atmosphere but do not advance\nthe plot . quite a bit of gore . i enjoyed bill mcghee ’ s performance which he made quite believable\nfor such a low budget picture , he managed to carry the movie at times when nothing much seemed\nto be happening . nurse charlotte beale , played by jesse lee , played her character well so be\nprepared to want to slap her toward the end ! she makes some really stupid mistakes but then , that\n’ s what makes these low budget movies so good ! i would have been out of that place and five states\naway long before she even considered that it might be a good idea to leave ! if you enjoy this movie\n, try committed from 1988 which is basically a rip off of this movie .\nIGℓ2\nlow budget horror movie . if you don ’ t raise your expectations too high , you ’ ll probably enjoy\nthis little flick . beginning and end are pretty good , middle drags at times and seems to go nowhere\nfor long periods as we watch the goings on of the insane that add atmosphere but do not advance\nthe plot . quite a bit of gore . i enjoyed bill mcghee ’ s performance which he made quite believable\nfor such a low budget picture , he managed to carry the movie at times when nothing much seemed\nto be happening . nurse charlotte beale , played by jesse lee , played her character well so be\nprepared to want to slap her toward the end ! she makes some really stupid mistakes but then , that\n’ s what makes these low budget movies so good ! i would have been out of that place and five states\naway long before she even considered that it might be a good idea to leave ! if you enjoy this movie\n, try committed from 1988 which is basically a rip off of this movie .\nALTI\nlow budget horror movie . if you don ’ t raise your expectations too high , you ’ ll probably enjoy\nthis little flick . beginning and end are pretty good , middle drags at times and seems to go nowhere\nfor long periods as we watch the goings on of the insane that add atmosphere but do not advance\nthe plot . quite a bit of gore . i enjoyed bill mcghee ’ s performance which he made quite believable\nfor such a low budget picture , he managed to carry the movie at times when nothing much seemed\nto be happening . nurse charlotte beale , played by jesse lee , played her character well so be\nprepared to want to slap her toward the end ! she makes some really stupid mistakes but then , that\n’ s what makes these low budget movies so good ! i would have been out of that place and five states\naway long before she even considered that it might be a good idea to leave ! if you enjoy this movie\n, try committed from 1988 which is basically a rip off of this movie .\nTable 9: Saliency maps of BERT generated by three common gradient methods and by our proposed method, ALTI,\nfor a positive sentiment prediction example of IMDB dataset.\n8711\nFigure 10: Attn Rollout relevancies Rl in BERT across layers.\n8712\nFigure 11: Globenc relevancies Rl in BERT across layers.\n8713\nFigure 12: ALTI method relevancies Rl in BERT across layers.\n8714"
}