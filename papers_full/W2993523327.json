{
  "title": "Latent Words Recurrent Neural Network Language Models for Automatic Speech Recognition",
  "url": "https://openalex.org/W2993523327",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2147766908",
      "name": "Ryo Masumura",
      "affiliations": [
        "NTT (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2091536226",
      "name": "Taichi Asami",
      "affiliations": [
        "NTT (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2121566519",
      "name": "Takanobu Oba",
      "affiliations": [
        "NTT (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2168410717",
      "name": "Sumitaka Sakauchi",
      "affiliations": [
        "NTT (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2036154538",
      "name": "Akinori Ito",
      "affiliations": [
        "Tohoku University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2100506586",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W2525707688",
    "https://openalex.org/W1996903695",
    "https://openalex.org/W1992300521",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W1575384945",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2140723372",
    "https://openalex.org/W2031284124",
    "https://openalex.org/W1976519809",
    "https://openalex.org/W2047506955",
    "https://openalex.org/W2397092652",
    "https://openalex.org/W2524475847",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W1488980900",
    "https://openalex.org/W2404890635",
    "https://openalex.org/W2100714283",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2154099718",
    "https://openalex.org/W2132957691",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W1797288984",
    "https://openalex.org/W37526647",
    "https://openalex.org/W2184045248",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W2394619680",
    "https://openalex.org/W2805965938"
  ],
  "abstract": "This paper demonstrates latent word recurrent neural network language models (LW-RNN-LMs) for enhancing automatic speech recognition (ASR). LW-RNN-LMs are constructed so as to pick up advantages in both recurrent neural network language models (RNN-LMs) and latent word language models (LW-LMs). The RNN-LMs can capture long-range context information and offer strong performance, and the LW-LMs are robust for out-of-domain tasks based on the latent word space modeling. However, the RNN-LMs cannot explicitly capture hidden relationships behind observed words since a concept of a latent variable space is not present. In addition, the LW-LMs cannot take into account long-range relationships between latent words. Our idea is to combine RNN-LM and LW-LM so as to compensate individual disadvantages. The LW-RNN-LMs can support both a latent variable space modeling as well as LW-LMs and a long-range relationship modeling as well as RNN-LMs at the same time. From the viewpoint of RNN-LMs, LW-RNN-LM can be considered as a soft class RNN-LM with a vast latent variable space. In contrast, from the viewpoint of LW-LMs, LW-RNN-LM can be considered as an LW-LM that uses the RNN structure for latent variable modeling instead of an n-gram structure. This paper also details a parameter inference method and two kinds of implementation methods, an n-gram approximation and a Viterbi approximation, for introducing the LW-LM to ASR. Our experiments show effectiveness of LW-RNN-LMs on a perplexity evaluation for the Penn Treebank corpus and an ASR evaluation for Japanese spontaneous speech tasks.",
  "full_text": "IEICE TRANS. INF. & SYST., VOL.E102–D, NO.12 DECEMBER 2019\n2557\nPAPER\nLatent Words Recurrent Neural Network Language Models for\nAutomatic Speech Recognition\nRyo MASUMURA†a), Taichi ASAMI†, Takanobu OBA†, Sumitaka SAKAUCHI†, and Akinori ITO††, Members\nSUMMARY This paper demonstrates latent word recurrent neural net-\nwork language models (LW-RNN-LMs) for enhancing automatic speech\nrecognition (ASR). LW-RNN-LMs are constructed so as to pick up advan-\ntages in both recurrent neural network language models (RNN-LMs) and\nlatent word language models (LW-LMs). The RNN-LMs can capture long-\nrange context information and o ﬀer strong performance, and the LW-LMs\nare robust for out-of-domain tasks based on the latent word space mod-\neling. However, the RNN-LMs cannot explicitly capture hidden relation-\nships behind observed words since a concept of a latent variable space is\nnot present. In addition, the LW-LMs cannot take into account long-range\nrelationships between latent words. Our idea is to combine RNN-LM and\nLW-LM so as to compensate individual disadvantages. The LW-RNN-LMs\ncan support both a latent variable space modeling as well as LW-LMs and\na long-range relationship modeling as well as RNN-LMs at the same time.\nFrom the viewpoint of RNN-LMs, LW-RNN-LM can be considered as a\nsoft class RNN-LM with a vast latent variable space. In contrast, from\nthe viewpoint of LW-LMs, LW-RNN-LM can be considered as an LW-\nLM that uses the RNN structure for latent variable modeling instead of\nan n-gram structure. This paper also details a parameter inference method\nand two kinds of implementation methods, an n-gram approximation and\na Viterbi approximation, for introducing the LW-LM to ASR. Our experi-\nments show e ﬀectiveness of LW-RNN-LMs on a perplexity evaluation for\nthe Penn Treebank corpus and an ASR evaluation for Japanese spontaneous\nspeech tasks.\nkey words: latent words recurrent neural network language models, n-\ngram approximation, Viterbi approximation, automatic speech recognition\n1. Overview\nLanguage models (LMs) are essential for many natural lan-\nguage processing tasks including automatic speech recog-\nnition (ASR) and statistical machine translation. The most\ncommon problems faced by LMs are data sparseness and\ncontext constraints. In most ASR systems, target domain\ntraining data sets are often limited since the data sets must\nbe obtained by manually transcribing speech samples. In\naddition, ASR systems consider only short context informa-\ntion when calculating the generative probabilities of words\nbecause they often use a traditional n-gram language model-\ning. In order to mitigate these problems, several techniques\nhave been proposed [1]–[3]. In particular, approaches in-\ntended to improve LM structure have been aggressively pur-\nsued. LM structure can be split into two main types; dis-\nManuscript received July 9, 2018.\nManuscript revised April 10, 2019.\nManuscript publicized September 25, 2019.\n†The authors are with NTT Media Intelligence Laboratories,\nNTT Corporation, Yokosuka-shi, 239–0847 Japan.\n††The author is with the Graduate School of Engineering,\nTohoku University, Sendai-shi, 980–8579 Japan.\na) E-mail: ryou.masumura.ba@hco.ntt.co.jp\nDOI: 10.1587/transinf.2018EDP7242\ncriminative models and generative models.\nThe former include intelligent models such as max-\nimum entropy model [4], decision tree [5], random for-\nest [6], and neural networks [7], [8] including deep neural\nnetworks [9]. Recurrent neural network LMs (RNN-LMs)\nhave, in particular, demonstrated signiﬁcant improvements\nin recent years [10], [11]. RNN-LMs can capture long-range\ncontext information via their recurrent structure and can e ﬃ-\nciently represent context information as a continuous vector.\nTherefore, RNN-LMs are known to be one of the most pow-\nerful LMs. On the other hand, among the generative models,\nlatent variable space modeling with Bayesian approach is\nattracting attention. Soft class-based LMs that have a latent\nvariable space have demonstrated better performance than\ntraditional class n-gram models [12], [13]. In addition, la-\ntent words LMs (LW-LMs) o ﬀer a more ﬂexible form of the\nsoft class-based LMs [14]. LW-LMs have a vast latent vari-\nable space whose size is equivalent to the vocabulary size of\nthe training data. This ﬂexible attributes helps to mitigate\ndata sparseness e ﬃciently. In fact, LW-LMs are robust for\nnot only in-domain tasks but also out-of-domain tasks [15]–\n[17].\nThis paper focuses on two successful LMs, RNN-LMs\nand LW-LMs. Although both models have their own ad-\nvantages, each also has problems at the same time. RNN-\nLMs can capture long-range context information and o ﬀer\nstrong performance. However, RNN-LMs cannot explicitly\ncapture the hidden relationship behind observed words since\nthey ignore the concept of the latent variable space. While\nLW-LMs have a latent variable space based on Bayesian in-\nference, the space is modeled as a simple n-gram structure.\nTherefore, LW-LMs cannot take into account the long-range\nrelationships among latent variables.\nTo overcome the problems of both RNN-LMs and LW-\nLMs, this paper presents a novel modeling method called\nthe latent word recurrent neural network language model\n(LW-RNN-LM). Our idea is to combine RNN-LM and LW-\nLM so as to ameliorate their individual disadvantages. Thus,\nLW-RNN-LM can support both latent variable space mod-\neling and LW-LM while providing long-range relationship\nmodeling as well as RNN-LMs at the same time. From the\nviewpoint of RNN-LMs, LW-RNN-LM can be considered\nas a soft class RNN-LM with a latent word space. In con-\ntrast, from the viewpoint of LW-LMs, LW-RNN-LM can be\nconsidered as an LW-LM whose latent word space modeling\nis based on an RNN structure instead of an n-gram struc-\nture. Consequently, it can be expected that LW-RNN-LM\nCopyright c⃝2019 The Institute of Electronics, Information and Communication Engineers\n2558\nIEICE TRANS. INF. & SYST., VOL.E102–D, NO.12 DECEMBER 2019\nhas di ﬀerent attributes from standard RNN-LMs and stan-\ndard LW-LMs and will yield further improvement through\ntheir combination.\nOur idea parallels studies that expand RNN-LMs us-\ning other models. Latent Dirichlet allocation has been used\nto exploit the topic representation from complete speech in-\nput [18]. In addition, paraphrase LMs have been combined\nwith RNN-LMs to create paraphrase RNN-LMs [19].T ot h e\nbest of our knowledge, LW-RNN-LM is the ﬁrst proposal to\ncombine RNN-LM with latent variable space models.\nThere are two issues posed by LW-RNN-LM. The ﬁrst\nis which training method should be used. It is impossible\nto estimate an LW-RNN-LM directly from a training data\nset since the RNN structure is not suitable for latent vari-\nable space modeling. To this end, an alternative training\nprocedure is presented that uses a standard LW-LM. In the\nprocedure, latent word sequences of the training data set are\nﬁrst decoded using the LW-LM, and then model parameters\nof the LW-RNN-LM are estimated using the latent word se-\nquences. The second issue is ASR implementation. As is\ntrue for LW-LMs, LW-RNN-LM cannot be applied to ASR\ndirectly because its latent word space is vast. To this end,\nthis paper presents two approximation methods, an n-gram\napproximation that converts a complex model structure into\na back-oﬀn-gram structure [15], [17], and a Viterbi approx-\nimation that uses a joint probability between an observed\nword sequence and an optimal latent word sequence [16].\nWhile these two methods were originally applied to LW-\nLMs, this paper customizes them to suit LW-RNN-LM.\nThis paper is an extended study of our previous\nwork [20]. It provides details of LW-RNN-LM, omitted\nfrom the previous work, that allow better understanding of\nits position relative to RNN-LM and LW-LM. Furthermore,\nwe provide an additional evaluation that more fully reveals\nthe properties of LW-RNN-LM.\nThis paper is organized as follows. In Sect. 2, we de-\nscribe the model structures of RNN-LM and LW-LM. Sec-\ntion 3 provides a deﬁnition of LW-RNN-LM. In addition,\na training method and two ASR implementation methods\nfor LW-RNN-LM are introduced in detail. Sections 4 and\n5 describe a perplexity evaluation and an ASR evaluation.\nSection 6 concludes this paper.\n2. Previous Work\n2.1 Recurrent Neural Network Language Models\nRecurrent neural network LMs (RNN-LMs) have attracted\nsigniﬁcant attention in recent years [10]. RNN-LMs have\ntwo characteristics: one is that the word space can be rep-\nresented as a continuous space vector based on neural net-\nworks, and the other is that long-range information can be\nﬂexibly taken into consideration based on its recurrent struc-\nture.\nA graphical rendering of RNN-LM is shown in Fig. 1.\nThe gray circle denotes a word that can be observed as a lin-\nguistic phenomenon. The gray square denotes a continuous\nFig. 1 Model structure of RNN-LMs.\nrepresentation that can be uniquely calculated. As shown in\nFig. 1, previous word wt−1 is converted into a hidden repre-\nsentation st that depends on previous context representation\nst−1, which includes long-range context information. Cur-\nrent word wt is generated depending on context information\nst.\nIn word-based RNN-LMs, the generative probability of\nword sequence w={w1,··· ,wT }is given as:\nP(w) =\nT∏\nt=1\nP(wt|wt−1,st−1,Θrnn), (1)\n=\nT∏\nt=1\nP(wt|st,Θrnn), (2)\nwhere Θrnnis a model parameter of RNN-LM and st is the\ncontext information of the RNN structure. In the RNN struc-\nture, context information st and P(wt|st,Θrnn) are calculated\nas:\nst =σ(Urnnst−1 +Vrnnφ(wt−1) +brnn), (3)\npt =[pt1,··· ,ptk,··· ,pt|V|]⊤, (4)\n=G(Ornnst +ornn), (5)\nP(wt =k|st,Θrnn) =ptk, (6)\nwhere Urnn, Vrnn, and brnn are model parameters of a hid-\nden layer of the RNN structure. Ornn and ornn are model\nparameters of an output layer of the RNN structure. pt is an\noutput vector in the output layer of the RNN structure, ptk\nis the k-th value in pt, and |V|is vocabulary size. φ(wt−1)\ndenotes 1-of-K coding of wt−1. σis a sigmoid function, and\nGis a softmax function.\nSince the cost of computing the probability estimation\nis proportional to vocabulary size |V|in word-based mod-\neling, class-based RNN-LMs are used most often [11].T h e\nidea was proposed for maximum entropy models and feed\nforward neural networks [21], [22]. The resulting probabil-\nity estimation is deﬁned as:\nP(w\nt|st,Θrnn) =P(wt|st,ct,Θrnn)P(ct|st,Θrnn), (7)\nwhere st is context information that includes the previous\nword and previous output in the hidden layer, and ct ∈C is\na word class where Crepresents the sets of classes and the\nclass size is |C|.\nNote that the mixing of several RNNs trained with dif-\nferent random initialization values is reported in [11].T h e\nMASUMURA et al.: LATENT WORDS RECURRENT NEURAL NETWORK LANGUAGE MODELS FOR AUTOMATIC SPEECH RECOGNITION\n2559\nFig. 2 Model structure of LW-LMs.\nprobability estimation of the ensemble of RNN-LMs is de-\nﬁned as:\nP(w) = 1\nM\nM∑\nm=1\nT∏\nt=1\nP(wt|sm\nt ,Θm\nrnn). (8)\nThe generative probability can be calculated using M in-\nstances of RNN-LMs. Θm\nrnnindicates the m-th model param-\neter and sm\nt is the context information of the m-th instance.\n2.2 Latent Words Language Models\nLatent words LMs (LW-LMs) are generative models that set\na latent variable for each observed word. A graphic render-\ning of LW-LM is shown in Fig. 2. The gray circles denote\nobserved words and the white circles denote latent variables.\nIn the generative process of LW-LM, a latent vari-\nable, called latent word ht, is generated depending on\nthe transition probability distribution given context lt =\n{ht−n+1,..., ht−1}, where n is n-gram order. Next, observed\nword wt is generated depending on the emission probability\ndistribution given latent word ht, i.e.,\nht ∼P(ht|lt,Θlw), (9)\nwt ∼P(wt|ht,Θlw), (10)\nwhere Θlw is a model parameter of LW-LM. Here,\nP(ht|lt,Θlw) is expressed as an n-gram model for latent\nwords, and P(wt|ht,Θlw) models the dependency between\nthe observed word and the latent word.\nAn important property of LW-LMs is that the latent\nword is expressed as a speciﬁc word that can be selected\nfrom complete vocabulary V. Thus, the number of latent\nwords is the same as vocabulary size |V|. For this reason,\nthe latent variable is called a latent word.\nLW-LM is generally modeled using the Bayesian ap-\nproach. LW-LM produces the following generative proba-\nbility for observed words w={w\n1,··· ,wT }:\nP(w) =\n∫∑\nh\nP(w|h,Θlw)P(h|Θlw)P(Θlw)dΘlw,(11)\n≃1\nM\nM∑\nm=1\n∑\nh\nP(w|h,Θm\nlw)P(h|Θm\nlw), (12)\n= 1\nM\nM∑\nm=1\n∑\nh\nT∏\nt=1\nP(wt|ht,Θm\nlw)P(ht|lt,Θm\nlw), (13)\nwhere h ={h1,··· ,hT }is a latent word assignment. Θm\nlw\nmeans the m-th instance of the point estimated model pa-\nrameter. Thus, the generative probability can be approxi-\nmated using M instances of Θ\nm\nlw.\nLWLMs are trained from a training data set W.I n\nLWLM training, the latent word assignment Hbehind W\nhave to be inferred. In fact, multiple latent word assign-\nments H\n1,··· ,HM are estimated for the Bayesian mod-\neling. Once a latent word assignment Hm is deﬁned,\nP(wt|ht,Θm\nlw) and P(ht|lt,Θm\nlw) can be calculated.\nTo estimate the latent word assignments, Gibbs sam-\npling can be utilized. Gibbs sampling samples a new value\nfor the latent word in accordance with its distribution and\nplaces it at position t in H. The conditional probability dis-\ntribution of possible values for latent word h\nt is given by:\nP(ht|W,H−t) ∝P(wt|ht,Θlw,−t)\nt+n−1∏\nj=t\nP(hj|lj,Θlw,−t),\n(14)\nwhere H−t represents all latent words except for ht and n\nis the n-gram order for the latent word modeling. In the\nsampling procedure, P(h\nt|lt,Θlw,−t) and P(wt|ht,Θlw,−t) can\nbe calculated from Wand H−t.\nThe transition probability distribution and the emis-\nsion probability distribution are calculated on the basis of\ntheir prior distributions. For the transition probability dis-\ntribution, this paper uses a prior hierarchical Pitman-Yor.\nP(h\nt|lt,Θlw) is given as:\nP(ht|lt,Θlw) =P(ht|lt,H), (15)\nP(ht|lt,H) =c(ht,lt) −d|lt |s(ht,lt)\nθ|lt |+c(lt)\n+θ|lt |+d|lt |s(lt)\nθ|lt |+c(lt) P(ht|π(lt),H), (16)\nwhere π(lt) is the shortened context obtained by removing\nthe earliest word from lt. c(ht,lt) and c(lt) are counts calcu-\nlated from a latent word assignment H. s(ht,lt) and s(lt)a r e\ncalculated from a seating arrangement deﬁned by the Chi-\nnese restaurant franchise representation of the Pitman-Yor\nprocess [23]. d|lt | and θ|lt | are discount and strength param-\neters of the Pitman-Yor process, respectively. Moreover, a\nDirichlet prior is used for the emission probability distribu-\ntion [24]. P(w\nt|ht,Θlw) is given as:\nP(wt|ht,Θlw) =P(wt|ht,W,H), (17)\nP(wt|ht,W,H) =c(wt,ht) +αP(wt)\nc(ht) +α , (18)\nwhere P(wt) is the maximum likelihood estimation value of\nunigram probability in the training data set W. c(wt,ht) and\nc(ht) are counts calculated from Wand latent word assign-\nment H. A hyper parameter αcan be optimized via a vali-\ndation data set.\n2560\nIEICE TRANS. INF. & SYST., VOL.E102–D, NO.12 DECEMBER 2019\n3. Latent Words Recurrent Neural Network Language\nModels\n3.1 Deﬁnition\nLatent words recurrent neural network LMs (LW-RNN-\nLMs) are generative models that combine RNN-LM and\nLW-LM. The models have a soft class structure based on a\nlatent word space as does LW-LM and the latent word space\nis modeled using an RNN-LM.\nA graphic rendering of LW-RNN-LM is shown in\nFig. 3. Gray circles denote words and white circles denote\nlatent words. White squares denote a hidden representation\nof latent words. As shown in Fig. 3, previous latent word\nh\nt−1 is converted into hidden representation st depending on\nprevious context representation st−1, which includes long-\nrange context information. Current latent word ht is gener-\nated depending on context information st. Finally, observed\nword wt is generated depending on latent word ht. The gen-\nerative process is formulated as:\nht ∼P(ht|st,Θlr), (19)\nwt ∼P(wt|ht,Θlr), (20)\nwhere Θlris a model parameter of LW-RNN-LM. The tran-\nsition probability distribution P(ht|st,Θlr)i se x p r e s s e da s\nan RNN-LM for latent words. The emission probability dis-\ntribution P(w\nt|ht,Θlr) models the dependency between the\nobserved word and the latent word as does LW-LM.\nIn the Bayesian approach, LW-RNN-LM deﬁnes the\nfollowing generative probability for observed words w =\n{w1,··· ,wT }:\nP(w) =\n∫∑\nh\nP(w|h,Θlr)P(h|Θlr)P(Θlr)dΘlr,(21)\n≃1\nM\nM∑\nm=1\n∑\nh\nP(w|h,Θm\nlr)P(h|Θm\nlr), (22)\n= 1\nM\nM∑\nm=1\n∑\nh\nT∏\nt=1\nP(wt|ht,Θm\nlr)P(ht|sm\nt ,Θm\nlr),(23)\nFig. 3 Model structure of LW-RNN-LM.\nwhere Θm\nlr is the m-th model parameter and sm\nt is the con-\ntext information of the m-th instance for the RNN structure.\nThus, the generative probability can be approximated using\nM instances of Θm\nlr.I n t h e m-th RNN structure, context\ninformation sm\nt and transition probability P(ht|sm\nt ,Θm\nlr)a r e\ncalculated as:\nsm\nt =σ(Um\nlrsm\nt−1\n+Vm\nlrφ(ht−1) +bm\nlr), (24)\npm\nt =[pm\nt1,··· ,pm\ntk,··· ,pm\nt|W|]⊤, (25)\n=G(Om\nlrsm\nt +om\nlr), (26)\nP(ht =k|sm\nt ,Θm\nlr) =pm\ntk, (27)\nwhere Um\nlr, Vm\nlr, and bm\nlr are model parameters in a hidden\nlayer of the m-th RNN structure. Om\nlr and om\nlr are model\nparameters in an output layer of the m-th RNN structure.\npm\nt is an output vector in the output layer of the m-th RNN\nstructure. φ(ht−1) denotes 1-of-K coding of ht−1.\nIn addition, class-based RNN structure described in\nSect. 2 is also utilized for LW-RNN-LM. In this case, latent\nwords are additionally map into classes. When the class-\nbased RNN structure is used in LW-RNN-LM, the genera-\ntive probability of h\nt is deﬁned as:\nP(ht|st,Θm\nlr) =P(ht|sm\nt ,ct,Θm\nlr)P(ct|sm\nt ,Θm\nlr), (28)\nwhere ct ∈Crepresents the class where Crepresents the sets\nof classes and the class size is |C|.\nLW-RNN-LM is closely related to conventional RNN-\nLMs and LW-LMs. LW-RNN-LM can be regarded as an\nRNN-LM with a soft class structure based on a latent word\nspace. In addition, LW-RNN-LM can be regarded as an LW-\nLM whose transition probability distribution is based on an\nRNN structure instead of an n-gram structure.\n3.2 Training\nLW-RNN-LM is trained using an observed word sequence\nof a training data set W={w\n1,··· ,wT }. It is impossible\nto estimate an LW-RNN-LM directly because it is founded\non latent word space and RNN structure. Therefore, this\npaper presents a method that preliminarily trains an LW-LM\nto decode the latent word assignment of the observed word\nsequence. The latent word assignment is used in estimating\nmodel parameters of an LW-RNN-LM.\nIn fact, the optimal latent word assignment of an ob-\nserved word sequence can be estimated for each LW-LM\ninstance. The optimal latent word assignment for the m-th\nLW-LM instance, H\nm ={hm\n1 ,··· ,hm\nT }, is given by:\nHm =arg max\nH\nP(W|H,Θm\nlw)P(H|Θm\nlw). (29)\nFor the estimation required, Gibbs sampling is suitable. The\nconditional probability distribution of possible values for la-\ntent word h\nt is given by:\nP(ht|W,H−t) ∝P(wt|ht,Θm\nlw)\nt+n−1∏\nj=t\nP(hj|lj,Θm\nlw), (30)\nMASUMURA et al.: LATENT WORDS RECURRENT NEURAL NETWORK LANGUAGE MODELS FOR AUTOMATIC SPEECH RECOGNITION\n2561\nwhere H−t is a latent word assignment except for ht.T h i s\nprocedure is applied to not only the training data set but also\nthe validation data set.\nThe m-th model parameter Θm\nlrcan be determined from\nthe m-th optimal latent word assignments Hm. The tran-\nsition probability distribution P(ht|sm\nt ,Θm\nlr), i.e., Um\nlr, Vm\nlr,\nbm\nlr, Om\nlr and om\nlr, is estimated from the m-th latent vari-\nable assignment of the training data set as in usual RNN-LM\ntraining. The m-th model parameter ˆΘ\nm\nlris optimized by:\nˆΘm\nlr=arg max\nΘlr\nT∏\nt=1\nP(hm\nt |sm\nt ,Θlr). (31)\nNote that the m-th latent variable assignment of the valida-\ntion data set is used for early stopping.\nEmission probability distribution P(wt|ht,Θm\nlr) can be\ndetermined from the LW-LM:\nP(wt|ht,Θm\nlr) =P(wt|ht,Θm\nlw), (32)\n=P(wt|ht,W,Hm), (33)\nP(wt|ht,W,Hm) =c(wt,hm\nt ) +αP(wt)\nc(hm\nt ) +α , (34)\nwhere P(wt) is the estimated maximum likelihood value of\nunigram probability in training data set W. c(wt,hm\nt ) and\nc(hm\nt ) are counts calculated from Wand latent word assign-\nment Hm. Hyper parameter αcan be optimized via the val-\nidation data set.\n3.3 N-Gram Approximation\nAn n-gram approximation can be used to apply LW-RNN-\nLM to ASR. The n-gram approximation is a technique that\ncan convert an LM with complex model structure into a\nback-oﬀn-gram structure. The n-gram approximation of\nLW-RNN-LM is the same as that for the LW-LM [15], [17];\nan LM with a back-o ﬀn-gram structure is trained from\nobserved words randomly sampled on LW-RNN-LM. The\napproximated LW-RNN-LM P(w|Θ\nlrng) has the following\nproperties:\nwlr∼P(w|Θ1\nlr,··· ,ΘM\nlr), (35)\nwlrng∼P(w|Θlrng), (36)\nwlr≃wlrng, (37)\nwhere wlris an observed word sequence generated from the\nLW-RNN-LM, and wlrngis an observed word sequence gen-\nerated from the approximated LW-RNN-LM with back-o ﬀ\nn-gram structure. The approximated LW-LM can be con-\nstructed from words generated from LW-RNN-LM.\nThe random sampling is based on Algorithm 1. As\nshown in the algorithm, context information of each RNN\nstructure is updated in advance. After that, instance index\nm\nt ∈{1,··· ,M}for model parameters, a latent word ht ∈V,\nand an observed word wt ∈V are recursively generated.\nThis iterative approach yields a large number of word\nsequences. T iterations can generate T latent words, and T\nAlgorithm 1Random sampling based on LW-RNN-LM.\nInput: Model parameters Θ1\nlr,··· ,ΘM\nlr,\nnumber of sampled words T\nOutput: Sampled words w\n1: l1 =<s>\n2: for t =1t o T do\n3: for m =1t o M do\n4: sm\nt =σ(sm\nt−1,ht−1,Θm\nlr)\n5: end for\n6: mt ∼P(mt) = 1\nM\n7: ht ∼P(ht|smt\nt ,Θmt\nlr)\n8: wt ∼P(wt|ht,Θmt\nlr)\n9: end for\n10: return w=w1,··· ,wT\nobserved words. The T observed words are used only for\nback-oﬀn-gram model estimation.\n3.4 Viterbi Approximation\nThe other applicable method is the Viterbi approximation\nthat simultaneously decodes a recognition hypothesis and\nits latent word sequence using the joint probability between\nthe two sequences. The joint probability is called the Viterbi\nprobability. Viterbi probability P(w,¯h) is deﬁned as:\nP(w,¯h) =max\nh\nP(w,h), (38)\n=max\nh\n1\nM\nM∑\nm=1\nP(w|h,Θm\nlr)P(h|Θm\nlr). (39)\nThe Viterbi approximation of LW-RNN-LM is not the same\nas is true for LW-LMs. LW-RNN-LM makes it impossible\nto e ﬃciently compute a Viterbi probability because neither\nthe Viterbi algorithm nor Gibbs sampling can be introduced\ndirectly due to the RNN structure. In order to tackle this\nissue, a standard LW-LM is used for the Viterbi approxi-\nmation. First, several latent word assignments are decoded\nusing an LW-LM, and the candidates are re-evaluated using\na LW-RNN-LM.\nThe latent word assignments can be decoded via Gibbs\nsampling. A conditional probability distribution of the pos-\nsible values for latent word h\nt is deﬁned as:\nP(ht|w,h−t) ∝\nM∑\nm=1\n{P(wt|ht,Θm\nlw)\nt+n−1∏\nj=t\nP(hj|lj,Θm\nlw)},\n(40)\nwhere h−t represents latent word assignment except for ht.\nThis sampling yields I samples of latent words assignments\n{h1,··· ,hI }. Viterbi probability P(w,¯h) is based on LW-\nRNN-LM and is calculated as:\nP(w,¯h)\n= max\nh∈{h1,···,hI }\n1\nM\nM∑\nm=1\nT∏\nt=1\nP(wt|ht,Θm\nlr)P(ht|sm\nt ,Θm\nlr).\n(41)\n2562\nIEICE TRANS. INF. & SYST., VOL.E102–D, NO.12 DECEMBER 2019\nIn addition, the Viterbi probability can be computed by com-\nbining LW-RNN-LM with LW-LM. That is, LW-RNN-LM\nand LW-LM are interpolated in the latent word space. The\nViterbi probability is deﬁned as:\nP(w,¯h) = max\nh∈{h1,···,hI }\n1\nM\nM∑\nm=1\nT∏\nt=1\nP(wt|ht,Θm\nlr)\n{λP(ht|sm\nt ,Θm\nlr) +(1 −λ)P(ht|lt,Θm\nlw)}, (42)\nwhere λis a mixture weight for latent word space modeling.\nAs described above, the emission probabilities of both LW-\nRNN-LM and LW-LM are shared, so the transition proba-\nbilities are simply interpolated.\n4. Experiment 1: Perplexity Evaluation\n4.1 Setups\nThe ﬁrst experiments used the Penn Treebank corpus in [25].\nSections 0–20 were used as a training data set (Train), sec-\ntions 21 and 22 were used as a validation data set (Valid),\nand sections 23 and 24 were used as a test data set (Test A).\nIn addition, a human-human discussion text data set (Test\nB) was prepared for evaluations in a domain di ﬀerent from\nthe training data set. Each vocabulary was limited to 10K\nwords and there were no out-of-vocabulary (OOV) words.\nActually, the OOV words were map into a speciﬁc word, i.e,\n“UNK”. The setups match those of many previous studies\nand allow us to evaluate perplexity of all words in a uniﬁed\nmanner. Table 1 shows details.\nIn this evaluation, the following LMs were prepared.\n• MKN5: A word-based 5-gram LM with modiﬁed\nKneser-Ney smoothing constructed from the training\ndata set [26].\n• HPY5: A word-based 5-gram hierarchical Pitman-Yor\nLM (HPYLM) constructed from the training data set.\nFor the training, 200 iterations were used for burn-in,\nand 10 instances were collected [27].\n• RNN: Word-based RNN-LM [11]. The hidden layer size\nwas set to 200 by referring to a preliminary experiment.\nThe number of instances ( M) was set to 1 since RNN-\nLMs with one instance were often used in most previ-\nous studies.\n• RNN-NA: A word-based 5-gram HPYLM constructed\nfrom data generated on the basis of RNN. The generated\ndata size was one billion words, which was determined\nin consideration of previous work [17]. W eu s e de n -\ntropy based pruning to n-gram entries that match the\ncomputation complexity of HPY5[28].\nTable 1 Data sets for perplexity evaluation.\nDomain Number of words\nTrain Penn Treebank 929,589\nValid Penn Treebank 70,390\nTest A Penn Treebank 78,669\nTest B Human-Human Discussion 50,507\n• LW-NA: A word-based 5-gram HPYLM constructed\nfrom data generated on the basis of 5-gram LW-LM\n(LW) constructed from the training data set. For train-\ning LW, 500 iterations were used for burn-in, and 10\ninstances were collected ( M =10). The generated data\nsize, one billion words, was determined in considera-\ntion of previous work [17]. We pruned n-gram entries\nas to be comparable computation complexity to HPY5\nusing entropy based pruning.\n• LW-VA: Viterbi approximation of LW[16]. To calculate\nthe Viterbi probability, 100 samples of latent words\nassignments were obtained via Gibbs sampling ( I =\n100).\n• LR-NA: A word-based 5-gram HPYLM constructed\nfrom data generated on the basis of LW-RNN-LM ( LR)\nconstructed from the training data set. Latent word\nspace was modeled by an RNN structure. The hidden\nunit size and the class size |C|in the RNN structure\nwere varied in the evaluation. The generated data size\nwas one billion words, which was determined in con-\nsideration of previous work [17].\n• LR-VA: Viterbi approximation of LR. To calculate the\nViterbi probability, 100 samples of latent words assign-\nments were sampled using LW(I =100).\nIn addition, several mixed models constructed by linearly\ninterpolating the above LMs were employed. Note that M\nand I were constant in our experiments. Hyper parameters\nincluding αin Eq. (28) and λin Eq. (36) and the interpola-\ntion weights were optimized using a validation data set.\n4.2 Results\nFirst, perplexity (PPL) results of LR-NAand LR-VAwere\ninvestigated; hidden unit size and class size |C|were var-\nied. When the class size was set to 1,000, LW-RNN-LM\nis a class-based model. On the other hand, when the class\nsize was set to 10,000, LW-RNN-LM is exactly a word-\nbased model. We compared LR-NAand LR-VAwith LW-NA\nand LW-VA. The results are shown in Table 2. The results\nshow that PPL was improved when hidden unit size and\nclass size were increased. The results indicate that rich pa-\nrameters are necessary to construct precise LW-RNN-LMs.\nIn the n-gram approximation results, LR-NAwas inferior\nTable 2 PPL results of LW-NA, LW-VA, LR-NAand LR-VA; hidden unit\nsize and class size |C|of RNN structure were varied for LW-RNN-LMs.\nUnit size Class size Valid Test A Test B\nLW-NA -- 138.7 131.7 205.5\nLR-NA 200 1,000 153.7 146.1 221.8\nLR-NA 400 1,000 151.6 143.6 217.4\nLR-NA 200 10,000 149.5 141.2 214.1\nLR-NA 400 10,000 148.6 140.6 212.4\nLW-VA -- 148.4 142.9 224.7\nLR-VA 200 1,000 155.1 147.5 226.4\nLR-VA 400 1,000 151.2 144.5 221.9\nLR-VA 200 10,000 149.5 142.2 221.1\nLR-VA 400 10,000 147.1 139.89 216.6\nMASUMURA et al.: LATENT WORDS RECURRENT NEURAL NETWORK LANGUAGE MODELS FOR AUTOMATIC SPEECH RECOGNITION\n2563\nto LW-NA. On the other hand, in the Viterbi approximation\nresults, LR-VAoutperformed LW-VAin both the in-domain\ntasks and the out-of-domain tasks. It can be considered that\nthe Viterbi approximation is an implementation method suit-\nable for LW-RNN-LMs because it can directly utilize the\nRNN structure for computing the generative probabilities of\nwords.\nNext, combinations of LR-VA(400 hidden units and\n10,000 classes) and LW-VAwere examined. The PPL results\nin which the mixture weight was varied are shown in Fig. 4.\nWhen the mixture weight was set to 0, the PPL result corre-\nsponds to LW-VA. When the mixture weight is set to 1, the\nPPL result corresponds to LR-VA. The results show that the\ncombination of LR-VAand LW-VAbased on linear interpo-\nlation can improve the PPL for all data sets. This suggests\nthat a combination of n-gram structure and RNN structure\nin a latent word space is e ﬀective as well as in an observed\nword space.\nPPL results including those for other LMs and their\ncombinations are summarized in Table 3. First, the model\ncombination was examined under the restriction of the back-\noﬀn-gram structure. Lines 1–9 show the results for LMs\nwith the back-o ﬀn-gram structure. LR-NAwas superior\nFig. 4 PPL results of combining LR-VAand LW-VA.\nTable 3 PPL results including those for other LMs and combined LMs.\nValid Test A Test B\n1. MKN5 148.0 141.2 238.6\n2. HPY5 145.1 139.3 232.7\n3. RNN-NA 160.4 150.4 286.4\n4. LW-NA 138.7 131.7 205.5\n5. LR-NA 148.6 140.6 212.4\n6. LW-NA+LR-NA 136.5 129.3 197.3\n7. RNN-NA+LR-NA 136.5 129.3 197.3\n8. HPY5+LW-NA 144.3 137.2 205.7\n9. HPY5+LW-NA+LR-NA (ALL5) 125.4 120.2 196.5\n10 LW-VA 148.4 142.9 224.7\n11 LR-VA 147.1 139.8 216.6\n12 LW-VA+LR-VA 138.0 132.3 211.3\n13 RNN 134.4 128.9 212.9\n14. ALL5+LR-VA 105.0 102.4 165.6\n15. ALL5+LW-VA+LR-VA 103.9 101.6 164.2\n16. ALL5+RNN 108.7 103.2 172.7\n17. ALL5+RNN+LW-VA+LR-VA 97.0 94.7 152.4\nto RNN-NAand slightly weaker than LW-NAin in-domain\ntasks and out-of-domain tasks. By combining LR-NAwith\nRNN-NAor LW-NA, the PPL was improved compared to\ntheir individual use. These results show that LR-NApos-\nsesses properties di ﬀerent from RNN-NAand LW-NAand\nthat their combination is e ﬀective. In each data set,\nHPY5+LW-NA+LR-NAoutperformed HPY5+LW-NA. These re-\nsults show the n-gram approximation of LW-RNN-LM is\nbeneﬁcial for constructing an LM with a back-o ﬀn-gram\nstructure.\nLines 10–17 show the results for RNN, LW-VA, LR-VA\nand their combination with ALL5(HPY5+LW-NA+LR-NA).\nWe only used ALL5as an n-gram LM when combining with\nLW-VA, LR-VAand RNN. RNNperformed strongly compared\nto LW-VAand LR-VA. Combining LR-VAwith ALL5yielded\nimproved PPL results. This indicates that the Viterbi ap-\nproximation of LW-RNN-LM is useful for improving back-\noﬀn-gram language modeling. The highest performance\nwas attained by ALL5+LW-VA+LR-VA+RNNfor all data sets.\nIt seems that RNN, LW-VAand LR-VAcomplement each other,\nand each model yields characteristics di ﬀerent from their n-\ngram approximation methods. These results conﬁrm that\nLW-RNN-LM is beneﬁcial for improving word prediction\nperformance.\n5. Experiment 2: ASR Evaluation\n5.1 Setups\nThe second experiment used the Corpus of Spontaneous\nJapanese (CSJ) [29]. CSJ was divided into a training data\nset (Train), a small validation data set (Valid), and a test data\nset (Test A). For evaluation in out-of-domain environments,\na contact center dialog task (Test B) and a voice mail task\n(Test C) were prepared. The vocabulary size of the training\ndata set was 83,536. For each data set, the number of words\nand OOV rate are detailed in Table 4.\nFor speech recognition evaluation, an acoustic model\nbased on hidden Markov models with deep neural networks\n(DNN-HMM) was prepared [30]. The DNN-HMM had 8\nhidden layers with 2048 nodes.\nIn this evaluation, the following LMs were prepared.\n• MKN3: A word-based 3-gram LM with modiﬁed\nKneser-Ney smoothing constructed from training data\nset [26].\n• HPY3: A word-based 3-gram HPYLM constructed\nfrom the training data set [27].\n• RNN: A class-based RNN-LM with 500 hidden nodes\nTable 4 Data sets for ASR evaluation.\nDomain Number of words OOV rate (%)\nTrain Lecture 7,317,392 -\nValid Lecture 28,046 0.72\nTest A Lecture 27,907 0.51\nTest B Contact center 24,665 3.66\nTest C V oice mail 21,044 4.41\n2564\nIEICE TRANS. INF. & SYST., VOL.E102–D, NO.12 DECEMBER 2019\nTable 5 PPL results of LW-NA, LW-VA, LR-NAand LR-VA; hidden layer size and class size |C|were\nvaried for LW-RNN-LMs.\nUnit size Class size Valid Test A Test B Test C\nLW-NA -- 79.64 66.93 141.34 147.87\nLR-NA 200 200 90.69 75.85 142.40 146.55\nLR-NA 200 500 89.95 75.38 140.81 145.88\nLR-NA 200 1,000 92.22 76.51 141.47 146.37\nLR-NA 500 500 90.17 75.17 140.72 145.09\nLW-VA -- 86.84 74.50 142.49 133.97\nLR-VA 200 200 87.95 74.41 156.49 163.55\nLR-VA 200 500 86.49 73.53 154.15 162.16\nLR-VA 200 1,000 86.49 73.23 152.31 162.54\nLR-VA 500 500 81.55 69.64 146.35 158.38\nand 500 classes by referring to a preliminary experi-\nment [11]. Actually, we did not construct word-based\nRNN-LM because it is di ﬃcult to construct the word-\nbased model from training data sets due to the compu-\ntation complexity. The number of instances ( M)w a s\nset to 1 since RNN-LMs with one instance were often\nused in most previous studies.\n• RNN-NA: A word-based 3-gram HPYLM constructed\nfrom data generated on the basis of RNN. The generated\ndata size was one billion words, which was determined\nin consideration of previous work [17]. We applied en-\ntropy based pruning to the n-gram entries to match the\ncomputation complexity of HPY3[28].\n• LW-NA: A word-based 3-gram HPYLM constructed\nfrom data generated on the basis of 3-gram LW-LM\n(LW) constructed from the training data set. For train-\ning LW, 500 iterations were used for burn-in, and 10\ninstances were collected ( M =10). The generated data\nsize, one billion words, was determined in considera-\ntion of previous work [17]. We applied entropy based\npruning as well as RNN-NA.\n• LW-VA: Viterbi approximation of LW. To calculate the\nViterbi probability, 100 samples of latent words assign-\nments were obtained using Gibbs sampling ( I =100).\n• LR-NA: A word-based 3-gram HPYLM constructed\nfrom data generated on the basis of LW-RNN-LM ( LR)\nconstructed from the training data set. Its latent word\nspace was modeled by an RNN structure. The hidden\nunit size and the class size |C|in the RNN structure\nwere varied in the evaluation. The generated data size,\none billion words, was determined in consideration of\nprevious work [17]. We applied entropy based pruning\nas well as RNN-NA.\n• LW-VA: Viterbi approximation of LR. To calculate the\nViterbi probability, 100 samples of latent words assign-\nments were sampled using LW(I =100).\nFor implementing RNN, LW-VA, and LR-VAto ASR, 1,000-\nbest hypotheses were generated in the ﬁrst pass. Note that\nM and I were constant in our experiments. Hyper param-\neters including αin Eq. (28) and λin Eq. (36) and the in-\nterpolation weights were optimized using a validation data\nset.\n5.2 Results\nFirst, PPL results of LR-NAand LR-VAwere investigated;\nhidden unit size and class size |C|were varied. The results,\nincluding LW-NAand LW-VA, are shown in Table 5.\nAmong the LW-RNN-LMs, the best results were at-\ntained with 500 hidden units and 500 classes for both the\nn-gram approximation and the Viterbi approximation. In\nin-domain tasks, LR-NAwas inferior to LW-NA, and LR-VA\nwas superior to LW-VA. On the other hand, in out-of-domain\ntasks, LR-NAwas superior to LW-NA, and LR-VAwas in-\nferior to LW-VA. The results indicate that LW-RNN-LM is\ncompatible with the Viterbi approximation while LW-LM is\ncompatible with the n-gram approximation. This is because\nthe Viterbi approximation directly utilizes the RNN struc-\nture for computing generative probabilities of words.\nTable 6 shows the PPL and word error rate (WER) re-\nsults for each condition. The results show that in-domain\ndata sets were showed lower PPL and WER than out-of-\ndomain data sets. This is because OOV rate in the in-domain\ndata sets were lower than the in the out-of-domain data sets.\nFirst, one-pass decoding results gained from LMs with the\nback-oﬀn-gram structure are examined. Lines 1–9 show\nthe results for the back-o ﬀn-gram structure. The results\nshow LR-NAoutperformed LW-NAin out-of-domain tasks al-\nthough the WER di ﬀerences between LW-NAand LW-NAin\nout-of-domain tasks were not statistically signiﬁcant ( p >\n0.05). On the other hand, LR-NAwas weaker than LW-NAin\nin-domain tasks. LR-NAalso outperformed RNN-NAin both\nin-domain tasks and out-of-domain tasks. The WER dif-\nferences between LR-NAand RNN-NAin each test sets were\nstatistically signiﬁcant ( p <0.01). These results indicate\nthat the n-gram approximation of LW-RNN-LM can yield\nASR performance improvements although it is comparable\nto LW-LM. In addition, RNN-NA+LR-NAand LW-NA+LR-NA\nattained better ASR performance than RNN-NAor LW-NA.\nThese results show the e ﬀectiveness of LW-RNN-LM com-\npared with the conventional methods. It is thought that\nthese improvements were attained because LW-RNN-LM\nhas di ﬀerent attributes from standard RNN-LMs and stan-\ndard LW-LMs. The highest ASR performance was attained\nby HPY3+LW-NA+LR-NAin each condition. In particular,\nin out-of-domain tasks, HPY3+LW-NA+LR-NAstrongly out-\nMASUMURA et al.: LATENT WORDS RECURRENT NEURAL NETWORK LANGUAGE MODELS FOR AUTOMATIC SPEECH RECOGNITION\n2565\nTable 6 PPL results and WER results [%] for in-domain tasks and out-of-domain tasks.\nSetup Valid Test A Test B Test C\n(In-domain) (In-domain) (Out-of-domain) (Out-of-domain)\nPPL WER PPL WER PPL WER PPL WER\n1. MKN3 81.38 19.98 69.36 24.79 167.61 38.67 189.93 32.00\n2. HPY3 79.32 19.74 67.50 24.67 158.13 38.29 175.63 31.69\n3. RNN-NA 98.65 21.63 82.23 26.24 153.89 39.32 163.99 31.96\n4. LW-NA 79.57 19.61 66.93 24.54 141.34 36.93 147.87 30.42\n5. LR-NA 90.17 19.89 75.17 25.30 140.72 36.64 145.09 29.75\n6. LW-NA+LR-NA 83.66 19.50 70.32 24.46 138.04 36.31 143.83 29.63\n7. RNN-NA+LR-NA 89.24 19.98 74.12 25.04 139.03 36.56 142.31 28.99\n8. HPY3+LW-NA 72.86 18.65 62.05 23.58 134.65 35.99 141.23 28.74\n9. HPY3+LW-NA+LR-NA (ALL3) 73.56 18.60 62.97 23.42 132.87 35.76 139.51 28.62\n10 LW-VA 86.84 - 74.50 - 142.49 - 133.97 -\n11 LR-VA 81.55 - 69.64 - 146.35 - 158.38 -\n12 LW-VA+LR-VA 77.56 - 67.36 - 134.21 - 132.59 -\n13 RNN 69.49 - 60.78 - 145.05 - 158.57 -\n14. ALL3+LR-VA 63.40 18.52 55.76 23.28 100.11 35.48 101.75 28.44\n15. ALL3+LW-VA+LR-VA 63.24 18.54 55.24 23.24 99.27 35.42 98.56 28.32\n16. ALL3+RNN 64.23 18.42 56.06 23.20 115.04 35.22 130.27 28.02\n17. ALL3+RNN+LW-VA+LR-VA 57.58 18.32 51.20 23.02 93.25 35.07 96.39 27.92\nperformed MKN3and HPY3. In terms of WER, statistically\nsigniﬁcant performance improvements ( p < 0.01) were\nachieved by HPY3+LW-NA+LR-NAcompared to MKN3and\nHPY3in each test set.\nNext, n-best rescoring results are investigated; they\nare shown in lines 10–17 in Table 6. We only used ALL3\nas a ﬁrst-pass decoding pass when combining an n-gram\nLM with rescoring LMs, i.e., LW-VA, LR-VA and RNN.\nNote that only PPL was evaluated for RNN, LW-VA, LR-VA,\nLW-VA+LR-VAsince they cannot be applied to ASR directly.\nALL3+RNNoutperformed ALL3in both the in-domain tasks\nand the out-of-domain tasks. This indicates that n-best\nrescoring approach can yield ASR performance improve-\nments. Only slight ASR performance improvements were\nattained by adding LR-VAto ALL3although PPL was re-\nmarkably improved. It is thought that the PPL improve-\nments attained by the Viterbi approximation are not related\nto the ASR performance improvement. The highest perfor-\nmance was attained by ALL3+RNN+LW-VA+LR-VAin each\ntest set although the WER di ﬀerences between ALL3+RNN\nwere not statistically signiﬁcant ( p >0.05).\n6. Conclusions\nThis paper presented the latent word recurrent neural net-\nwork language model (LW-RNN-LM); it employs a latent\nword space as well as LW-LMs in which the latent word\nspace is modeled using RNN-LMs. LW-RNN-LM can cap-\nture long range relationships in the latent word space unlike\nstandard LW-LMs which can take only very little context in-\nformation into consideration. For LW-RNN-LM, we intro-\nduced a training method and two implementation methods,\nn-gram approximation and Viterbi approximation, for ASR.\nExperiments showed that LW-RNN-LM, RNN-LM and LW-\nLM complement each other and their combinations attain\nperformance improvements in the both n-gram approxima-\ntion and Viterbi approximation forms. In future work, we\nwill introduce long short-term memory structures [31], [32]\ninstead of using RNN structure for improving LW-RNN-LM\nperformance. Furthermore, we will examine domain adap-\ntation for LW-RNN-LM by using latent word space mixture\nmodeling [33], [34].\nReferences\n[1] R. Rosenfeld, “Two decades of statistical language model-\ning: Where do we go from here?,” Proc. IEEE, vol.88, no.8,\npp.1270–1278, 2000.\n[2] J.T. Goodman, “A bit of progress in language modeling,” Computer\nSpeech & Language, vol.15, pp.403–434, 2001.\n[3] R. Masumura, T. Asami, T. Oba, H. Masataki, S. Sakauchi, and A.\nIto, “Investigation of combining various major language model tech-\nnologies including data expansion and adaptation,” IEICE Trans. Inf.\n& Syst., vol.E99-D, no.10, pp.2452–2461, 2016.\n[4] R. Rosenfeld, “A maximum entropy approach to adaptive statistical\nlanguage modeling,” Computer Speech & Language, vol.10, no.3,\npp.187–228, 1996.\n[5] G. Potamianos and F. Jelinek, “A study of n-gram and decision tree\nletter language modeling methods,” Speech Communication, vol.24,\nno.3, pp.171–192, 1998.\n[6] P. Xu and F. Jelinek, “Random forests in language modeling,” Proc.\nEMNLP 2004, pp.325–332, 2004.\n[7] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural prob-\nabilistic language model,” Journal of Machine Learning Research,\nvol.3, pp.1137–1155, 2003.\n[8] H. Schwenk, “Continuous space language models,” Computer\nSpeech & Language, vol.21, no.3, pp.492–518, 2007.\n[9] E. Arisoy, T.N. Sainath, B. Kingsbury, and B. Ramabhadran, “Deep\nneural network language models,” Proc. NAACL-HLT 2012, pp.20–\n28, 2012.\n[10] T. Mikolov, M. Karaﬁat, L. Burget, J. Cernocky, and S. Khudanpur,\n“Recurrent neural network based language model,” Proc. Inter-\nspeech, pp.1045–1048, 2010.\n[11] T. Mikolov, S.K. Stefan, L. Burget, J. Cernocky, and S. Khudanpur,\n“Extensions of recurrent neural network language model,” Proc.\nICASSP, pp.5528–5531, 2011.\n[12] Y . Su, “Bayesian class-based language models,” Proc. ICASSP\n2011, pp.5564–5567, 2011.\n[13] J.-T. Chien and C.-H. Chueh, “Dirichlet class language models for\nspeech recognition,” IEEE Transactions on Audio, Speech and Lan-\n2566\nIEICE TRANS. INF. & SYST., VOL.E102–D, NO.12 DECEMBER 2019\nguage Processing, vol.19, no.3, pp.1352–1365, 2011.\n[14] K. Deschacht, J.D. Belder, and M.-F. Moens, “The latent words\nlanguage model,” Computer Speech & Language, vol.26, no.5,\npp.384–409, 2012.\n[15] R. Masumura, H. Masataki, T. Oba, O. Yoshioka, and S. Takahashi,\n“Use of latent words language models in ASR: a sampling-based\nimplementation,” Proc. ICASSP, pp.8445–8449, 2013.\n[16] R. Masumura, T. Oba, H. Masataki, O. Yoshioka, and S. Takahashi,\n“Viterbi decoding for latent words language models using Gibbs\nsampling,” Proc. INTERSPEECH, pp.3429–3433, 2013.\n[17] R. Masumura, T. Adami, T. Oba, H. Masataki, S. Sakauchi, and S.\nTakahashi, “N-gram approximation of latent words language models\nfor domain robust automatic speech recognition,” IEICE Trans. Inf.\n& Syst., vol.E99-D, no.10, pp.2462–2470, 2016.\n[18] T. Mikolov and G. Zweig, “Context dependent recurrent neural net-\nwork language model,” Proc. SLT, pp.234–239, 2012.\n[19] X. Liu, X. Chen, M.J.F. Gales, and P.C. Woodland, “Paraphras-\ntic recurrent neural network language models,” Proc. ICASSP,\npp.5406–5410, 2015.\n[20] R. Masumura, T. Asami, T. Oba, H. Masataki, S. Sakauchi, and A.\nIto, “Latent words recurrent neural network language models,” Proc.\nINTERSPEECH, pp.2380–2384, 2015.\n[21] J.T. Goodman, “Classes for fast maximum entropy training,” Proc.\nICASSP, pp.561–564, 2001.\n[22] F. Morin and Y . Bengio, “Hierarchical probabilistic neural network\nlanguage model,” Proc. AISTATS, pp.246–252, 2005.\n[23] Y .W. Teh, “A hierarchical Bayesian language model based on Pit-\nman-Yor processes,” Proc. ACL, pp.985–992, 2006.\n[24] D.J.C. MacKay and L.C. Peto, “A hierarchical dirichlet language\nmodel,” Natural language engineering, vol.1, pp.289–308, 1995.\n[25] M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini, “Building a\nlarge annotated corpus of English: The penn treebank,” Computa-\ntional Linguistics, vol.19, pp.313–330, 1993.\n[26] S.F. Chen and J. Goodman, “An empirical study of smoothing tech-\nniques for language modeling,” Computer Speech & Language,\nvol.13, no.4, pp.359–383, 1999.\n[27] Y .W. Teh, “A hierarchical Bayesian language model based on Pit-\nman-Yor processes,” Proc. COLING-ACL, pp.985–992, 2006.\n[28] A. Stolcke, “Entropy-based pruning of backo ﬀlanguage models,”\nProc. DARPA Broadcast News Transcription and Understanding\nWorkshop, pp.270–274, 1998.\n[29] K. Maekawa, H. Koiso, S. Furui, and H. Isahara, “Spontaneous\nspeech corpus of Japanese,” Proc. LREC, pp.947–952, 2000.\n[30] G. Hinton, L. Deng, D. Yu, G. Dahl, A.R. Mohamed, N. Jaitly, A.\nSenior, V . Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury,\n“Deep neural networks for acoustic modeling in speech recogni-\ntion,” Signal Processing Magazine, pp.1–27, 2012.\n[31] M. Sundermeyer, R. Schluter, and H. Ney, “LSTM neural networks\nfor language modeling,” Proc. INTERSPEECH, pp.194–197, 2012.\n[32] M. Sundermeyer, H. Ney, and R. Schluter, “From feedforward to re-\ncurrent LSTM neural networks for language models,” IEEE /ACM\nTransactions of Audio, Speech and Language processing, vol.23,\nno.3, pp.517–529, 2015.\n[33] R. Masumura, T. Asami, T. Oba, H. Masataki, and S. Sakauchi,\n“Mixture of latent words language models for domain adaptation,”\nPorc. INTERPSEECH, pp.1425–1429, 2014.\n[34] R. Masumura, T. Asami, T. Oba, H. Masataki, S. Sakauchi, and A.\nIto, “Domain adaptation based on mixture of latent words language\nmodels for automatic speech recognition,” IEICE Trans. Inf. & Syst.,\nvol.E101-D, no.6, pp.1581–1590, 2018.\nRyo Masumura received B.E., M.E., and\nPh.D. degrees in engineering from Tohoku Uni-\nversity, Sendai, Japan, in 2009, 2011, 2016, re-\nspectively. Since joining Nippon Telegraph and\nTelephone Corporation (NTT) in 2011, he has\nbeen engaged in research on speech recogni-\ntion, spoken language processing, and natural\nlanguage processing. He received the Student\nAward and the Awaya Kiyoshi Science Promo-\ntion Award from the Acoustic Society of Japan\n(ASJ) in 2011 and 2013, respectively, the Sendai\nSection Student Awards The Best Paper Prize from the Institute of Elec-\ntrical and Electronics Engineers (IEEE) in 2011, the Yamashita SIG Re-\nsearch Award from the Information Processing Society of Japan (IPSJ) in\n2014, the Young Researcher Award from the Association for Natural Lan-\nguage Processing (NLP) in 2015, and the ISS Young Researcher’s Award in\nSpeech Field from the Institute of Electronic, Information and Communi-\ncation Engineers (IEICE) in 2015. He is a member of the ASJ, the IPSJ, the\nNLP, the IEEE, and the International Speech Communication Association\n(ISCA).\nTaichi Asami received B.E. and M.E. de-\ngrees in computer science from Tokyo Institute\nof Technology, Tokyo, Japan, in 2004 and 2006,\nrespectively. Since joining Nippon Telegraph\nand Telephone Corporation (NTT) in 2006, he\nhas been engaged in research on speech recog-\nnition and spoken language processing. He re-\nceived the Awaya Kiyoshi Science Promotion\nAward and the Sato Prize Paper Award from the\nAcoustic Society of Japan (ASJ) in 2012 and\n2014, respectively. He is a member of the ASJ,\nthe Institute of Electronics, Information and Communication Engineers\n(IEICE), Institute of Electrical and Electronics Engineers (IEEE), and the\nInternational Speech Communication Association (ISCA).\nTakanobu Oba received B.E. and M.E. de-\ngrees from Tohoku University, Sendai, Japan,\nin 2002 and 2004, respectively. In 2004, he\njoined Nippon Telegraph and Telephone Cor-\nporation (NTT), where he was engaged in the\nresearch and development of spoken language\nprocessing technologies including speech recog-\nnition at the NTT Communication Science Lab-\noratories, Kyoto, Japan. In 2012, he started\nthe research and development of spoken appli-\ncations at the NTT Media Intelligence Labora-\ntories, Yokosuka, Japan. Since 2015, he has been engaged in development\nof spoken dialogue services at the NTT Docomo Corporation, Yokosuka,\nJapan. He received the Awaya Kiyoshi Science Promotion Award from the\nAcoustical Society of Japan (ASJ) in 2007. He received Ph.D. (Eng.) de-\ngree from Tohoku University in 2011. He is a member of the Institute of\nElectrical and Electronics Engineers (IEEE), the Institute of Electronics,\nInformation, and Communication Engineers (IEICE) and the ASJ.\nMASUMURA et al.: LATENT WORDS RECURRENT NEURAL NETWORK LANGUAGE MODELS FOR AUTOMATIC SPEECH RECOGNITION\n2567\nSumitaka Sakauchi received M.S. degree\nfrom Tohoku University in 1995 and Ph.D. de-\ngree from Tsukuba University in 2005. Since\njoining Nippon Telegraph and Telephone Cor-\nporation (NTT) in 1995, he has been engaged\nin research on acoustics, speech and signal pro-\ncessing. He is now Senior Manager in the Re-\nsearch and Development Planning Department\nof NTT. He received the Paper Award from the\nInstitute of Electronics, Information and Com-\nmunication Engineers (IEICE) in 2001, and\nAwaya Kiyoshi Science Promotion Award from the Acoustic Society of\nJapan (ASJ) in 2003. He is a member of the IEICE and the ASJ.\nAkinori Ito received B.E., M.E., and Ph.D.\ndegrees from Tohoku University, Sendai, Japan.\nSince 1992, he has worked with Research Cen-\nter for Information Sciences and Education Cen-\nter for Information Processing, Tohoku Univer-\nsity. He was with the Faculty of Engineering,\nYamagata University, from 1995 to 2002. From\n1998 to 1999, he worked with the College of\nEngineering, Boston University, MA, USA, as\na Visiting Scholar. He is now a Professor of the\nGraduate School of Engineering, Tohoku Uni-\nversity. He is engaged in spoken language processing, statistical text pro-\ncessing, and audio signal processing. He is a member of the Acoustic So-\nciety of Japan, the Information Processing Society of Japan, and the IEEE.",
  "topic": "Recurrent neural network",
  "concepts": [
    {
      "name": "Recurrent neural network",
      "score": 0.8821377754211426
    },
    {
      "name": "Computer science",
      "score": 0.7851400375366211
    },
    {
      "name": "Latent variable",
      "score": 0.6635226607322693
    },
    {
      "name": "Language model",
      "score": 0.6116907596588135
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5030774474143982
    },
    {
      "name": "Context (archaeology)",
      "score": 0.44350558519363403
    },
    {
      "name": "Inference",
      "score": 0.4193256199359894
    },
    {
      "name": "Speech recognition",
      "score": 0.41144731640815735
    },
    {
      "name": "Artificial neural network",
      "score": 0.32128071784973145
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2251713219",
      "name": "NTT (Japan)",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I201537933",
      "name": "Tohoku University",
      "country": "JP"
    }
  ]
}