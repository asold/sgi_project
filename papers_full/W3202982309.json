{
  "title": "Sparse Spatial Transformers for Few-Shot Learning",
  "url": "https://openalex.org/W3202982309",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5002711413",
      "name": "Haoxing Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5076835782",
      "name": "Huaxiong Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101649330",
      "name": "Yaohui Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100697167",
      "name": "Chunlin Chen",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2884561390",
    "https://openalex.org/W2979689312",
    "https://openalex.org/W3173204135",
    "https://openalex.org/W2904218366",
    "https://openalex.org/W3095709006",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W2964105864",
    "https://openalex.org/W2125687350",
    "https://openalex.org/W3108975329",
    "https://openalex.org/W3044220283",
    "https://openalex.org/W3137611345",
    "https://openalex.org/W2753160622",
    "https://openalex.org/W3108877122",
    "https://openalex.org/W3173908982",
    "https://openalex.org/W2966993394"
  ],
  "abstract": "Learning from limited data is challenging because data scarcity leads to a poor generalization of the trained model. A classical global pooled representation will probably lose useful local information. Many few-shot learning methods have recently addressed this challenge using deep descriptors and learning a pixel-level metric. However, using deep descriptors as feature representations may lose image contextual information. Moreover, most of these methods independently address each class in the support set, which cannot sufficiently use discriminative information and task-specific embeddings. In this paper, we propose a novel transformer-based neural network architecture called sparse spatial transformers (SSFormers), which finds task-relevant features and suppresses task-irrelevant features. Particularly, we first divide each input image into several image patches of different sizes to obtain dense local features. These features retain contextual information while expressing local information. Then, a sparse spatial transformer layer is proposed to find spatial correspondence between the query image and the full support set to select task-relevant image patches and suppress task-irrelevant image patches. Finally, we propose using an image patch-matching module to calculate the distance between dense local representations, thus determining which category the query image belongs to in the support set. Extensive experiments on popular few-shot learning benchmarks demonstrate the superiority of our method over state-of-the-art methods. Our source code is available at \\url{https://github.com/chenhaoxing/ssformers}.",
  "full_text": "SCIENCE CHINA\nInformation Sciences\n. RESEARCH PAPER .\nSparse spatial transformers for few-shot learning\nHaoxing CHEN, Huaxiong LI*, Yaohui LI & Chunlin CHEN\nDepartment of Control Science and Intelligence Engineering, Nanjing University, Nanjing210093, China\nAbstract Learning from limited data is challenging because data scarcity leads to a poor generalization\nof the trained model. A classical global pooled representation will probably lose useful local information.\nMany few-shot learning methods have recently addressed this challenge using deep descriptors and learning\na pixel-level metric. However, using deep descriptors as feature representations may lose image contextual\ninformation. Moreover, most of these methods independently address each class in the support set, which\ncannot suﬃciently use discriminative information and task-speciﬁc embeddings. In this paper, we propose a\nnovel transformer-based neural network architecture called sparse spatial transformers (SSFormers), which\nﬁnds task-relevant features and suppresses task-irrelevant features. Particularly, we ﬁrst divide each input\nimage into several image patches of diﬀerent sizes to obtain dense local features. These features retain con-\ntextual information while expressing local information. Then, a sparse spatial transformer layer is proposed\nto ﬁnd spatial correspondence between the query image and the full support set to select task-relevant image\npatches and suppress task-irrelevant image patches. Finally, we propose using an image patch-matching\nmodule to calculate the distance between dense local representations, thus determining which category the\nquery image belongs to in the support set. Extensive experiments on popular few-shot learning benchmarks\ndemonstrate the superiority of our method over state-of-the-art methods. Our source code is available at\nhttps://github.com/chenhaoxing/ssformers.\nKeywords few-shot Learning, transformer, metric-learning, cross-attention\nCitation Chen H X, Li H X, Li Y H, et al. Sparse spatial transformers for few-shot learning. Sci China Inf Sci,\nfor review\n1 Introduction\nWith the availability of large-scale labeled data, visual understanding technology has made substantial\nprogress in many tasks [1, 2]. However, collecting and labeling such a large amount of data is time-\nconsuming and laborious. Few-shot learning is committed to solving this problem, enabling deep models\nto have better generalization ability even on a few samples [41–43].\nMany few-shot learning methods have recently been proposed and can be roughly divided into two\ncategories: optimation [3–5] and metric-learning [6–10]-based meta-learning methods. Optimation-based\nmethods aim to learn transferable meta-knowledge to handle new tasks in the process of learning multiple\ntasks [3]. Metric-learning-based methods focus on learning a good feature representation or distance\nmetric [7,10]. These methods have attracted widespread attention due to their simplicity and eﬀectiveness.\nTherefore, we mainly focus on metric-learning-based methods in this paper.\nFor feature representations, most of the existing metric-learning based methods [6, 7, 11] adopt global\nfeatures for recognition, which may cause helpful local information to be lost and overwhelmed. Recently,\nDN4 [9], MATANet [12], and DeepEMD [13] adopted dense feature representations (i.e., deep descriptors)\nfor few-shot learning tasks, which are more expressive and compelling than using global features. Another\n* Corresponding author (email: huaxiongli@nju.edu.cn)\narXiv:2109.12932v3  [cs.CV]  10 May 2023\nHaoxing Chen, et al. Sci China Inf Sci 2\n!\"\n!\"\n#\"\n#\"\n$% $&\n$'\n$% $&\n$'\n!!\"#\n//0#\"1-\". $23451\n!\"\n!\"\n#\"\n#\"\n$% $&\n$'\n$% $&\n$'\n!!\"#\n!\"#$#$%&'()*+,-$.+ $!%&''(&)(*'+,(-./01\n(a) Prototypical Net (b) SSFormers\nFigure 1 Prototypical Net [7] learns a global-level representation in an appropriate feature space and uses Euclidean\ndistance to measure similarities. In contrast, our model ﬁrst generates dense local representations through image patches\nand then uses the sparse spatial transformer layer (SSTL) to select task-relevant patches, generating task-speciﬁc prototypes.\nFinally, similarities are obtained by matching between attentioned image patches.\nbranch that enhances image representation uses the attention mechanism to align the query image with\nthe support set. For example, crossattention networks (CANs) [14] and SAML [15] use the semantic\ncorrelation between the support set and query image to highlight the target object.\nFor distance metrics, existing dense feature-based methods usually adopt a pixel-level metric, and the\nquery image is taken as a set of deep descriptors. For example, in DN4 [9], for each deep query descriptor,\nthese methods ﬁnd its nearest neighbor descriptors in each support class. Additionally, CovaMNet [16]\ncalculates a local similarity between each query deep descriptor and a support class using a covariance\nmetric.\nHowever, most existing methods use global features or deep descriptors, which are ineﬀective for few-\nshot image recognition. Because global features lose local information, deep descriptors lose the contextual\ninformation of images. Moreover, the above methods independently process each support class and cannot\nuse the context information of the entire task to generate task-speciﬁc features.\nThis paper proposes a novel transformer-based architecture for few-shot learning called sparse spatial\ntransformers (SSFormers). SSFormers extract the spatial correlation between the query image and the\ncurrent task (the entire support set) to align task-relevant image patches and suppress task-irrelevant\nimage patches. As shown in Figure 1, we ﬁrst divide each input image into several patches and obtain\ndense local features. Second, we select task-relevant query patches by a function of mutual feeling, i.e.,\nthe mutual nearest neighbor [17, 18]. The selected query patches are then used to generate task-speciﬁc\nprototypes. Finally, a patch-matching module (PMM) is proposed to measure the similarity between\nquery images and aligned support classes. For each patch from a query image, the PMM calculates its\nsimilarity scores to the nearest neighbor patch in each aligned class prototype. Then, similarity scores\nfrom all query patches are accumulated as a patch-to-class similarity.\nThe main contributions of this work are summarized as follows:\n(1) We propose a novel sparse spatial transformersfor few-shot learning, which can select task-relevant\npatches and generate a task-speciﬁc prototype.\n(2) We propose a patch-matching moduleto obtain similarity between query images and task-speciﬁc\nprototypes. Experiments prove that this approach is more suitable for image patch-based feature repre-\nsentation than directly using the cosine similarity.\nHaoxing Chen, et al. Sci China Inf Sci 3\n(3) We conduct extensive experiments on popular few-shot learning benchmarks and show that the\nproposed model achieves competitive results compared to other state-of-the-art methods.\n2 Related Work\nFew-shot recognition aims to learn transferable meta knowledge from seen classes. In general, most\nrepresentative few-shot learning methods based on meta-learning can be organized into two categories as\nfollows.\n2.1 Optimation-based Meta-learning Methods\nOptimation-based methods focus on training models that can perform well in unseen tasks with only a\nfew ﬁne-tuning steps. Finn et al. [3] proposed a model-agnostic meta-learning algorithm that learns the\nsensitive initial parameters of the networks to generalize to new tasks with several iterations. Ravi et\nal. [35] proposed a long short-term memory-based meta-learner, which ensures that initialization allows\nthe ﬁne-tuning step to start at an appropriate point. Because the initial model of the ordinary meta-\nlearning-based method could be too biased toward existing tasks to adapt to new tasks, Jamal et al.[36]\nproposed an entropy-based method that can learn an unbiased initial model. Instead of forcibly sharing an\ninitialization between tasks, Li et al.[37] learned to generate matching networks by learning transferable\nmeta-knowledge knowledge across tasks and directly producing network parameters for similar unseen\ntasks. However, how to reduce the computational cost and ensure the diversity and validity of synthetic\ndata are considerable challenges for the meta-learning-based method.\n2.2 Metric-learning-based Meta-learning Methods\nGlobal feature-based methods. The traditional metric-learning-based few-shot learning methods use\nan additional global average pooling layer to obtain the global feature representation at the end of the\nbackbone and use diﬀerent metrics for recognition. MatchingNet [6] uses the cosine distance to measure\nthe similarity between the query image and each support class. Prototypical Net [7] takes the empirical\nmean as the prototype representation of each category and uses Euclidean distance as the distance metric.\nRelationNet [8] proposes a nonlinear learnable distance metric. These methods based on global features\nlose much useful local information, which harms recognition tasks under few-shot learning settings.\nDense feature-based methods. Another metric-learning-based method branch uses pixel-level deep\ndescriptors as feature representations. DN4 [9] uses the k-nearest neighbor algorithm to obtain the pixel-\nlevel similarity between images. MATANet [12] proposes a multiscale task adaptive network to select\ntask-relevant deep descriptors at multiple scales. DeepEMD [13] proposes a diﬀerentiable earth mover’s\ndistance to calculate the similarity between image patches. Our SSFormers also belong to this method\nbased on dense features. An important diﬀerence in our method is that we divide input images into several\npatches of diﬀerent sizes and extract features. Compared with global features, the features extracted by\nour method can express local information. Furthermore, compared with deep descriptors, the extracted\nfeatures contain context information.\nAttention-based methods. CANs [14] propose a crossattention algorithm to highlight the common\nobjects in an image pair. SAML [15] proposes a collect-and-select strategy to align the main objects in\nan image pair. RENet [10] improves network generalization performance over unseen categories from a\nrelational perspective. Diﬀerently, our SSFormers select task-relevant patches in the query image to align\nthe support set with the query image by a sparse spatial crossattention algorithm.\nTransformer-based methods. FEAT [19] ﬁrst introduces transformer [20] to few-shot learning.\nFEAT uses a transformer to conduct support set sample relationships and generate task-speciﬁc support\nfeatures. CrossTransformers [21] proposes to use a self-supervised learning algorithm to enhance the\nfeature representation ability of the pretrained backbone and use a transformer to achieve alignment.\nHaoxing Chen, et al. Sci China Inf Sci 4\n!\nSupport setQuery\nCNN\nSparse cross attentionSparse spatial transformerlayer\nPatch matching module\nFC FC FCFC FCFC\nPatchesselectmask\nSparse attentionmap\nDense local features\nKey projectionheadFCQueryprojectionheadValueprojectionhead\nFC\nFCSupport keys\nSupport values\nQuery valueQuery query\nAligned support features\n(a) Architechture of SSFormers(b) Sparse spatial transformer layer\n\"×$\n\"×$\n\"×$\n\"×$!\n\"×%\"\n\"\n\"×%\"\n\"×$\nFigure 2 Illustration of the proposed SSFormers. We propose to generate dense local features and ﬁnd task-relevant\nfeatures through a sparse spatial transformer layer.\n3 Preliminary\nWe ﬁrst introduce the problem deﬁnition of few-shot learning. Few-shot learning is dedicated to learning\ntransferable knowledge between tasks and using the learned knowledge to solve new tasks. In the few-\nshot learning scenario, the task is usually set as N -way M -shot, where N is the number of categories,\nand M is the number of labeled samples in each category. Under this setting, the model is trained on a\ntraining set Dtrain with a large amount of labeled data. We use episodic training mechanisms to train our\nmodel to learn transferable knowledge. For training, the episodic training mechanism samples batched\ntasks from Dtrain. In each episode, we ﬁrst construct query set DQ = {(xq\ni,yq\ni)}N×B\ni=1 and support set\nDS = {(xs\ni,ys\ni)}N×M\ni=1 , where B is a hyperparameter that we must ﬁx in our experiments. Typically, B is\nset to 15 [14, 19]. Then, our model predicts to which support set category each sample in the query set\nbelongs. When the model training is completed, we sample tasks from unlabeled test sets Dtest to verify\nthe model’s performance.\n4 Our Method\nIn this section, we ﬁrst introduce our method for generating dense local representations. Then, we\ndescribe our sparse spatial transformers layer, which spatially aligns query images and support classes.\nFinally, we describe the PMM used to calculate the ﬁnal similarities. An overview of our framework is\nshown in Figure 2.\nHaoxing Chen, et al. Sci China Inf Sci 5\n4.1 Dense Local Feature Extractor\nThe metric-learning-based few-shot learning method aims to ﬁnd an eﬀective feature representation and\na good distance metric to calculate the similarity between images. In contrast to the methods that use\nglobal features, deep descriptor-based methods have achieved better results because deep descriptors\ncontain richer and more transferable semantic information. However, these methods based on deep\ndescriptors lose the contextual information of images. To combine the advantages of the two branches of\nmethods and reduce the disadvantages, our SSFormers aim to establish hierarchical local representations\nfor spatial comparison.\nAs illustrated in Figure 2, dense local representations extractor Fθ evenly divides the image into H×W\npatches, and the backbone network individually encodes each image patch to generate a feature vector.\nThe feature vectors generated by all patches constitute each image’s dense local representations set.\nBecause single-scale patches may not be able to fully obtain the local details and context information of\nthe image, we adopt a pyramid structure to generate hierarchical local representations. Thus, the feature\nrepresentation of an input image xcan be denoted as Fθ(x) ∈RK×C, where C is the number of channels,\nand K is the number of all local patch representations. Particularly, we adopt two image patch division\nstrategies of size 2 ×2 and 4×4 to obtain 20 dense local representations.\nIn each N -way M -shot few-shot image recognition task, for each support class, we haveM samples and\nobtain M feature representations. Instead of using the empirical mean of M feature representations [7] to\nobtain the class representation, we use all the patches in each support class, i.e., Sn ∈RMK×C, where Sn\nis the class representation of the n-th support class. The entire support set representation can be denoted\nas S ∈RN×MK×C. Similarly, for a query image xq\ni, through Fθ, we can obtain feature representation\nq= Fθ(xq\ni) ∈RK×C, where i= {1,...,BN }.\n4.2 Sparse Spatial Transformers Layer\nSparse spatial transformers aim to enhance the discriminant ability of local feature representations by\nmodeling the interdependencies between diﬀerent patches in the query image and the entire support\nset. In an N -way M -shot task, key kS and value vS are generated for support set feature S using two\nindependent linear projections: the key projection head hk: RC ↦→RC′\nand the value projection head\nhv: RC ↦→RC′\n. Similarly, the query image feature q is embedded using the value projection head hv and\nthe query projection head hq: RC ↦→RC′\nto obtain the value vq and query qq.\nInspired by [17], which proposed the MNN algorithm to eliminate batch eﬀects in single-cell RNA\nsequencing data, we argue that if patch qi feels that Sj is its closest patch and vice versa, then they\nare likely to have similar local features, where qi ∈qq,i ∈{1,...,K }and Sj ∈kS,j ∈{1,...,NMK }.\nConversely, if patch Sj feels that qi is not such a close patch, then even if qi feels that Sj is its closest\npatch, the actual relationship between them is relatively weak. In other words, the correlation between two\npatches is a function of mutual feeling, not a one-way feeling. Therefore, we can use this bidirectionality\nto select task-relevant patches in the current task.\nWe ﬁrst calculate the semantic relation matrix between the query image and support classnand obtain\nRn:\nWe ﬁrst calculate the semantic relation matrix between the query image and support class n, and get\nRn:\nRn = qq ×k⊤\nSn√\nC′ ∈RK×MK. (1)\nTo ﬁnd task-relevant patches, we concatenate all semantic relation matrixes Rn,n = {1,...,N }to\nobtain R ∈RK×NMK. Each row in R represents the semantic similarity of each patch in the query\nimage to all patches of all images in the support set.\nParticularly, we propose a novel sparse spatial crossattention algorithm to ﬁnd task-relevant patches in\nthe query image. For each patch qi ∈qq, we ﬁnd its nearest neighbor ni\nq in kS, and then ﬁnd the nearest\nneighbor ni\nS of ni\nq in qq. If i = ni\nS, then we consider qi to be a task-relevant patch. After collecting all\nHaoxing Chen, et al. Sci China Inf Sci 6\nAlgorithm 1Training strategy of SSFormers\nRequire: Training set Dtrain\n1: for all iteration=1, ..., MaxIteration do\n2: Sample N-way M-shot task (DQ,DS) from Dtrain\n3: Compute S = Fθ(DS) ∈RN×MK×C\n4: for i in {1,...,NB }do\n5: Compute q= Fθ(xq\ni) ∈RK×C\n6: Compute sparse attention map using Eqs. (1)?(5)\n7: Obtain task-speciﬁc prototype vSn|q using Eq. (6)\n8: Compute similarity Pi using Eqs. (7) and (8)\n9: end for\n10: Obtain cross-entropy loss L= ∑NB\ni=1 CE(Pi,yq\ni)\n11: Update parameters in SSFormers by SGD\n12: end for\n13: return Trained SSFormers\ntask-relevant patches in qq, we can get the sparse mask m= [m1; ...; mK], which can be computed as:\nni\nq = arg max\nj\nRi,j, (2)\nni\nS = arg max\nk\nRk,niq , (3)\nmi = 1 (i= ni\nS), (4)\nwhere 1 is the indicator function: When i= ni\nS, 1 is equal to 1; otherwise, it is 0. Using sparse mask m\nand semantic relation matrix Rn, we can obtain sparse attention map an and use it to align each support\nclass n to query image q and obtain task-speciﬁc prototype vSn|q, which can be computed as:\nan = m∗Rn ∈RK×MK, (5)\nvSn|q = an ×v⊤\nSn ∈RK×C′\n. (6)\n4.3 Patch-matching Module\nThe PMM is built as a similarity metric with no parameter to train. Given query value vq ∈RK×C′\nand\nthe aligned prototype of class n vSn|q ∈RK×C′\n, we can obtain their patch-to-patch similarity matrix as\nfollows:\nDn = vq ×vSn|q\n||vq||·||vSn|q||∈RK×K. (7)\nThen, for each patch in vq, we select the most similar patch among all patches from prototype vSn|q. We\nsum K selected patches as the similarity between the query image and support class n:\nPn =\nK∑\ni=1\nmax\nj∈{1,...,K}\nDn\ni,j. (8)\nUnder the N -way M -shot few-shot learning setting, we can obtain semantic similarity vectors P ∈RN.\nThe training procedure of SSFormers is shown in Algorithm 1.\n5 Experiments\nTo evaluate the eﬀectiveness of our method, we conducted extensive experiments on several commonly\nused benchmarks for few-shot image recognition. In this section, we ﬁrst present details about datasets\nand experimental settings in our network design. Then, we compare our method with state-of-the-art\nmethods on various few-shot learning tasks, i.e., standard few-shot learning and semisupervised few-shot\nlearning. Finally, we conduct comprehensive ablation studies to validate each component in our network.\nHaoxing Chen, et al. Sci China Inf Sci 7\nTable 1 Average recognition accuracy of 5-way 1-shot and 5-way 5-shot tasks with 95% conﬁdence intervals on\nminiImageNet and tieredImageNet. † denotes that it is our reimplementation under the same setting. (The top two\nperformances are shown in bold.)\nMethod Backbone miniImageNet tieredImageNet\n5-way 1-shot 5-way 5-shot 5-way 1-shot 5-way 5-shot\nPrototypical Net [7] Conv-64F 49.42 ±0.78 68.20±0.66 53.31±0.89 72.69±0.74\nCovaMNet [16] Conv-64F 51.19 ±0.76 67.65±0.63 54.98±0.90 71.51±0.75\nDN4 [9] Conv-64F 51.24 ±0.74 71.02±0.64 53.37±0.86 74.45±0.70\nSAML [15] Conv-64F 52.22 ±0.00 66.49±0.00 - -\nDSN [25] Conv-64F 51.78 ±0.96 68.99±0.69 53.22±0.66 71.06±0.55\nDeepEMD†[13] Conv-64F 52.15 ±0.28 65.52±0.72 50.89±0.30 66.12±0.78\nCFMN [33] Conv-64F 52.98±0.84 68.33±0.70 - -\nCTX†[21] Conv-64F 52.38 ±0.20 68.34±0.16 55.32±0.22 73.12±0.19\nSSFormers Conv-64F 55.00±0.22 70.55±0.17 55.54±0.19 73.72±0.21\nPrototypical Net [7] ResNet12 62.59 ±0.85 78.60±0.16 68.37±0.23 83.43±0.16\nCAN [14] ResNet12 63.85 ±0.48 79.44±0.34 69.89±0.51 84.23±0.37\nDSN [25] ResNet12 62.64 ±0.66 78.83±0.45 67.39±0.82 82.85±0.56\nDeepEMD [13] ResNet12 65.91 ±0.82 82.41±0.56 71.16±0.87 83.95±0.58\nFEAT [19] ResNet12 66.78 ±0.20 82.05±0.14 70.80±0.23 84.79±0.16\ngLoFA [26] ResNet12 66.12 ±0.42 81.37±0.33 69.75±0.33 83.58±0.42\nArL [27] ResNet12 65.21 ±0.58 80.41±0.49 - -\nPSST [28] ResNet12 64.05 ±0.49 80.24±0.45 - -\nRENet [10] ResNet12 67.60±0.44 82.58±0.30 71.61±0.51 85.28±0.35\nNCA(Nearest Centroid) [34] ResNet12 62.55 ±0.12 78.27±0.09 68.35±0.13 83.20±0.10\nCTX†[21] ResNet12 63.95 ±0.21 79.54±0.17 70.22±0.11 83.78±0.16\nSSFormers ResNet12 67.25±0.24 82.75±0.20 72.52±0.25 86.61±0.18\n5.1 Datasets\nWe conduct few-shot image recognition problems on four popular benchmarks, i.e., miniImageNet,\ntieredImageNet, CIFAR-FS and FC100.\nminiImageNet [6] is a subset randomly sampled from ImageNet and is an important benchmark in\nthe few-shot learning community. miniImageNet comprises 60,000 images in 100 categories. We follow\nthe standard partition settings [7], where 64/16/20 categories are for training, validation, and evaluation,\nrespectively.\ntieredImageNet [22] is also a subset randomly sampled from ImageNet, which comprises 779,165\nimages in 608 categories. All 608 categories are grouped into 34 broader categories. Following the\nsame partition settings [14], we use 20/6/8 broader categories for training, validation, and evaluation,\nrespectively.\nCIFAR-FS [23] is separated from CIFAR-100, which comprises 60,000 images in 100 categories.\nCIFAR-FS is divided into 64, 16, and 20 classes for training, validation, and evaluation, respectively.\nFC100 [24] is also separated from CIFAR-100, which is more diﬃcult because it is more diverse than\nCIFAR-FS. FC100 uses a split similar to tieredImageNet, where training, validation, and testing splits\ncontain 60, 20, and 20 classes, respectively.\nHaoxing Chen, et al. Sci China Inf Sci 8\nTable 2 Experimental results compared with other methods on CIFAR-FS and FC100. (The top two performances are\nshown in bold.)\nModel Backbone CIFAR-FS FC100\n5-way 1-shot 5-way 5-shot 5-way 1-shot 5-way 5-shot\nPrototypical Net [7] Conv-64F 55.50 ±0.70 72.00±0.60 35.30±0.60 48.60±0.60\nRelationNets [8] Conv-256F 55.00 ±1.00 69.30±0.80 - -\nR2D2 [23] Conv-512F 65.30 ±0.20 79.40±0.10 - -\nPrototypical Net [7] ResNet-12 72.20 ±0.70 83.50±0.50 37.50±0.60 52.50±0.60\nTADAM [24] ResNet-12 - - 40.10 ±0.40 56.10±0.40\nMetaOptNet [29] ResNet-12 72.60 ±0.70 84.30±0.50 41.10±0.60 55.50±0.60\nMABAS [30] ResNet-12 73.51 ±0.92 85.49±0.68 42.31±0.75 57.56±0.78\nFine-tuning [31] WRN-28-10 76.58±0.68 85.79±0.50 43.16±0.59 57.57±0.55\nRENet [10] ResNet12 74.51 ±0.46 86.60±0.32 - -\nSSFormers ResNet-12 74.50±0.21 86.61±0.23 43.72±0.21 58.92±0.18\nTable 3 The ablation study on our model shows that each part of our model has an important contribution. The\nexperiments are conducted with ResNet12 on miniImageNet. (SSTL: sparse spatial transformer layer, and PMM: patch-\nmatching module.)\nDense Local Feature SSTL SSTL(w/o m) PMM Cosine Classiﬁer 5-way 1-shot 5-way 5-shot\n\" \" 63.15±0.20 79.15±0.25\n\" \" 66.84±0.47 79.72±0.50\n\" \" \" 64.35±0.22 80.17±0.17\n\" \" \" 66.32±0.23 80.64±0.20\n\" \" \" \" 66.83±0.22 83.14±0.19\n\" \" \" 67.25±0.24 82.75±0.20\n5.2 Implementation Details\nBackbone networks. For a fair comparison, following [25], we use Conv-64F and ResNet-12 as our\nmodel backbone. The Conv-64F [7,19] contains four repeated blocks. Each block has a convolutional layer\nwith a 3 kernel, a batch normalization layer, a ReLU, and a max-pooling with a size of two. We set the\nnumber of convolutional channels in each block as 64. In slight contrast to the literature, we add a global\nmax-pooling layer at the end to reduce the dimension of the embedding and obtain patch embeddings.\nThe DropBlock is used in this ResNet architecture to avoid overﬁtting. In slight contrast to the ResNet-\n12 in [29], we apply a global average pooling after the ﬁnal layer, which leads to a 640-dimensional patch\nembedding.\nPatch dividing details. We select various grids and their combinations (see Table 4). All sampled\npatches are resized to the input size 84×84. To generate dense pyramid features, we add a global average\npooling layer at the end of the backbone, such that the backbone generates a vector for each input image\npatch. Moreover, we slightly expand the area of the local patches in the grid twice to merge the context\ninformation, which helps generate the local representations.\nTraining details. We train Conv-64F from scratch. For ResNet12, the training process can be\ndivided into pre-training and meta-training stages. Following [13], we apply a pre-train strategy. The\nbackbone networks are trained on training categories with a softmax layer. In this stage, we apply\ndata argumentation methods to increase the generalization ability of the model, i.e., color jitter, random\ncrop, and random horizontal ﬂip. The backbone network in our model is initialized with pretrained\nweights, which are then ﬁne-tuned along with other components in our model. In the meta-training\nstage, we conduct N -way M -shot tasks on all benchmarks, i.e., 5-way 1-shot and 5-way 5-shot. Conv-64F\nHaoxing Chen, et al. Sci China Inf Sci 9\nTable 4 5-way, 1-shot and 5-shot recognition accuracy (%)\nwith diﬀerent numbers of image patches on miniImageNet.\nEmbedding 5-way 1-shot 5-way 5-shot\n5 ×5 65.20 ±0.24 80.07±0.26\n4 ×4 66.37 ±0.23 81.06±0.25\n3 ×3 65.17 ±0.22 81.69±0.20\n2 ×2 65.18 ±0.22 80.10±0.16\n5 ×5 + 3×3 66.38 ±0.23 81.82±0.24\n4 ×4 + 3×3 66.08 ±0.24 81.50±0.26\n4 ×4 + 2×2 67.25±0.24 82.75±0.20\n3 ×3 + 2×2 67.05±0.22 82.53±0.18\nFigure 3 5-way semisupervised few-shot learning re-\nsults on miniImageNet. We show the results with (w/\nD) and without (w/o D) distractors. Additionally, we\ncompare our methods with PN, Nonmasked [32], PN,\nMasked [32] and DSN-semi [25].\nTable 5 5-way, 1-shot and 5-shot recognition accuracy (%) with diﬀerent attention methods on tieredImageNet.\nMethod Self-att. Cross-att. miniImageNet tieredImageNet\n1-shot 5-shot 1-shot 5-shot\nPrototypical Net [7] 62.59 ±0.85 78.60±0.16 68.37±0.23 83.43±0.16\nCAN [14] \" 63.85±0.48 79.44±0.34 69.89±0.51 84.23±0.37\nFEAT [19] \" 66.78±0.20 82.05±0.14 70.80±0.23 84.79±0.16\nRENet [10] \" \" 67.60±0.44 82.58±0.30 71.61±0.51 85.28±0.35\nSSFormers \" 67.25±0.24 82.75±0.20 72.52±0.25 86.61±0.18\nis optimized by Adam [39], and the initial learning rate is set to 0.1 and decays 0.1 every 10 epochs.\nMoreover, ResNet-12 is optimized by SGD, and the initial learning rate is set to 5e-4 and decays 0.5\nevery 10 epochs. We implemented the proposed network using PyTorch [38].\nEvaluation. During the test stage, we randomly sample 10,000 tasks from the meta-testing set and\ntake the averaged top-1 recognition accuracy as the performance of a method.\n5.3 Standard Few-shot Image Recognition\nTo verify the eﬀectiveness of our proposed SSFormers for the few-shot image recognition task, we con-\nduct comprehensive experiments and compare our methods with other state-of-the-art methods. Table\n1 and Table 2 list the recognition results of diﬀerent methods on the miniImageNet, tieredImageNet,\nCIFAR-FS, and FC100 datasets. The results show that our method achieves the best results in almost\nall settings. For instance, on the miniImageNet, tieredImageNet, CIFAR-FS, and FC100 datasets, with a\nResNet12 backbone, SSFormers have approximately 5.3%, 3.8%,3.7%, and 12.2% performance improve-\nments, respectively, compared with the vanilla Prototypical Net [7] under the 5-shot setting, while they\nhave 7.4%, 6.1%, 3.2%, and 16.6% performance improvements, respectively, under the 1-shot setting.\nNote that our model gains 3.3%/3.2% and 2.3%/2.8% improvements over the most relevant work\nCTX [21] under a 1-shot/5-shot setting with ResNet12 on miniImageNet and tieredImageNet, respec-\ntively. We obtain this improvement because SSFormers can ﬁnd task-relevant patches in the current task\nand perform sparse spatial crossattention algorithms based on dense hierarchical representations.\n5.4 Semi-supervised Few-Shot Learning\nWe further verify the eﬀectiveness of our model on more challenging semi-supervised few-shot learning\ntasks. Under semisupervised few-shot learning settings, we can select image patches from unlabeled\nHaoxing Chen, et al. Sci China Inf Sci 10\nTable 6 Stability evaluation on miniImageNet.\n+ GaussianBlur + PepperNoise + ColorJitter + CutMix\nRethink-D [40] 82.14 →49.30 82.14 →63.97 82.14→81.05 82.14 →71.95\nCAN [14] 79.44 →73.76 79.44 →65.17 79.44 →77.85 79.44 →70.85\nCTX [21] 79.54→75.32 79.54 →65.85 79.54→78.16 79.54 →69.35\nSSFormers 82.75→77.17 82.75 →66.74 82.75 →81.21 82.75 →72.46\nTable 7 Comparisons of computation time and FLOPs.\nMethod Time (1-shot) Time (5-shot) FLOPs (G)\nDeepEMD [13] 0.1440 0.0965 -\nCAN [14] 0.0029 0.0035 0.0021\nR2D2 [23] 0.0118 0.0044 -\nRelationNets [8] 0.0008 0.0010 0.0041\nMetaOptNet [29] 0.3522 1.4018 -\nSSFormers 0.0022 0.0027 0.0041\nsamples that meet the mutual perception function (Eqs. (2)-(4)) and add them to the support set to\nprovide more support features. Particularly, the workﬂow of SSFormers-semi is as follows. For the\nsupport set n, we ﬁrst search for all patches that satisfy the mutual perception function in unlabeled\nsets and put them into the set Un. Then we use Un to extend Sn: Sn = {S1\nn,...,S MK\nn }⋃Un. Then, we\nuse the original SSFormers to calculate the similarity. We use the same experimental setting in [32]. We\nuse Conv-64F as our backbone and train SSFormers-semi on 300,000 tasks on miniImageNet with 40%\nlabeled data. The results are shown in Figure 3, where SSFormers-semi shows competitive results with\nclassical baseline methods.\n5.5 Ablation Study\nAnalysis of our method. Our model comprises diﬀerent components: a dense local feature extractor,\nsparse spatial transformer layer (SSTL), and PMM. As shown in Table 3, we verify the indispensability\nof each component on the miniImageNet dataset. Note that we calculate the cosine similarity spatially\nwhen replacing the PMM. The results show that every component in SSFormers has an important con-\ntribution. For example, without our SSTL, the performance of the model drops by 3.73% on 5-shot\ntasks. Additionally, if we do not use sparse mask m, our model is equivalent to using transformers for\ncrossattention between two images. As shown in Table 3, performance is reduced by 1.38%/2.55% for\n1-shot/5-shot tasks, respectively. Moreover, if we introduce an additional cosine classiﬁer for SSFormers\nand dynamically adjust the weights of the PMM and cosine classiﬁer through a learnable weight param-\neter, the performance of the model is slightly reduced for 1-shot tasks and slightly improved for 5-shot\ntasks. These results are obtained because the cosine distance for global-level features is not very eﬀective\nwhen the samples are scarce, as veriﬁed by the results in [7,44].\nInﬂuence of the number of patches. While dividing input images into patches, we must deﬁne\nthe grid for patches. We select various grids and combinations to conduct analysis experiments on\nminiImageNet. As shown in Table 4, an approach is to use a combination of grids of diﬀerent sizes. A\npossible explanation is that the size of the main object diﬀers between images and using a single size may\nlose context information and make high-level semantic representations diﬃcult to generate.\nComparison with other attention methods. As shown in Table 5, our model achieves competitive\nresults with other state-of-the-art attention-based methods. Particularly, CANs propose a crossattention\nalgorithm to highlight the common objects in an image pair. FEAT uses a transformer to model support\nimage relationships and generate task-speciﬁc support features. FEAT neglects query image information,\nwhich could result in information loss. RENet uses 4-dimensional convolution for self-attention and\nHaoxing Chen, et al. Sci China Inf Sci 11\nSeen Class Unseen Class\n(a) SSFormers (b) Prototypical Net\nSeen ClassUnseen Class\nFigure 4 t-SNE visualization of features for 150 randomly sampled images from ﬁve randomly selected classes of the\nminiImageNet dataset. In our case, the learned embeddings provide better discrimination for seen and unseen classes.\n!\"#$%& \n'())*+, \n#-, \n.(-+/ \n01\"2- \n!\"#$%3 !\"#$%4 \nFigure 5 Visualization of the mask and sparse attention. Given 2-way 1-shot tasks, we plot the masked query and\nattentioned support images (brighter colors mean higher weight). Within each query image, we choose two image patches\n(red and yellow boxes) and plot the image patch that best matches each support class. These ﬁgures prove that our sparse\nspatial transformer layer can automatically highlight task-relevant areas.\ncrossattention between two images. Our SSFormers consider not only the relationship between two\nimages but also the relationship between the query image and all support images in the current task.\nSSFormers select task-relevant patches in the query image to align the support set with the query image\nusing a sparse spatial crossattention algorithm.\nAnalyze stability. A good model should have good robustness and adaptability to various environ-\nments. For this reason, we tested the stability of our model under three attacks on miniImageNet. As\nshown in Table 6, the performance of our SSFormers is relatively stable under various attacks, i.e., Gaus-\nsianBlur (δ ∈[0.1,2]), PepperNoise ( r = 0.01), ColorJitter ( B = 0.8), and CutMix [45]. For example,\nwhen this model was attacked by GaussianBlur, the performance decreased by 40.0% for Rethink-D [40]\nbut only by 6.7% for SSFormers.\nTime Complexity. We compare our SSFormers with other state-of-the-art methods regarding time\ncomplexity. The computation time is averaged by 10 4 forward inferences, and each task only contains\none query image. SSFormers are observed to have comparably little computation time compared to\na CAN [14]. However, a CAN [14] has lower FLOPs because the partitioning of patches and feature\nextracting brings additional computation.\nQualitative visualizations. We do a t-SNE visualization of the output embeddings from SSFormers\nfrom the seen and unseen images of miniImageNet to demonstrate the eﬀectiveness of our method (see\nHaoxing Chen, et al. Sci China Inf Sci 12\nFigure 4). Our method maintains good class discrimination compared to Prototypical Net, even for unseen\ntest classes. Moreover, the features generated by our method are more discriminative, and the boundaries\nbetween categories are more prominent than that in Prototypical Net. We provide visualization cases in\nFigure 5 to further qualitatively evaluate the proposed SSFormers. For each query image in the task, we\nplot the result of its mask, and it is seen that through the mutual perception function (Eqs. (2)-(4)), we\ncan obtain task-relevant query image patches. For support sets, we plot the prototype generated after\nSSTL. Our model is shown to highlight task-relevant image patches and suppress task-irrelevant features.\n6 Conclusion\nIn this article, we argue that global features and deep descriptors are ineﬀective for few-shot learning\nbecause global features lose local information, and deep descriptors lose the contextual information of\nimages. Moreover, a common embedding space fails to generate discriminative visual representations for\na target task. On the basis of this fact, we propose novel sparse spatial transformers (SSFormers) to help\nmeta-classiﬁers learn more discriminative features. In SSFormers, we use patch-level features for repre-\nsentation and propose a novel SSTL to customize task-speciﬁc prototypes through a transformer-based\narchitecture. We propose a nonparametric PMM to obtain patch-level similarity for ﬁnal recognition.\nExperimental results demonstrate that SSFormers can achieve competitive results with other state-of-\nthe-art few-shot learning methods. In the future, we will explore designing a pure vision transformer\nfor few-shot learning, which can substantially reduce the time cost of the patch dividing step and fully\nexploit the powerful learning representation of the vision transformer.\nAcknowledgements This work was partially supported by the National Natural Science Foundation of China (Nos.\n62176116, 62073160, 62276136), and the Natural Science Foundation of the Jiangsu Higher Education Institutions of China,\nNo. 20KJA520006.\nReferences\n1 Cao Z, Simon T, Wei S E, et al. Realtime multi-person 2d pose estimation using part aﬃnity ﬁelds. In: Proceedings\nof Computer Vision and Pattern Recognition (CVPR), 2017. 7291-7299\n2 Lin T Y, Goyal P, Girshick R, et al. Focal loss for dense object detection. In: Proceedings of International Conference\non Computer Vision (ICCV), 2017. 2980-2988\n3 Finn C, Abbeel P, Levine S. Model-agnostic meta-learning for fast adaptation of deep networks. In: Proceedings of\nInternational Conference on Machine Learning (ICML), 2017. 1126-1135\n4 Ravi S, Larochelle H. Optimization as a model for few-shot learning. In: Proceedings of International Conference on\nLearning Representations (ICLR), 2016.\n5 Chu W H, Li Y J, Chang J C, et al. Spot and learn: A maximum-entropy patch sampler for few-shot image classiﬁcation.\nIn: Proceedings of Computer Vision and Pattern Recognition (CVPR), 2019. 6251-6260\n6 Vinyals O, Blundell C, Lillicrap T, et al. Matching networks for one shot learning. In: Proceedings of Neural\nInformation Processing Systems (NeurIPS), 2016. 3630-3638\n7 Snell J, Swersky K, Zemel R. Prototypical networks for few-shot learning. In: Proceedings of Neural Information\nProcessing Systems (NeurIPS), 2017. 4077-4087\n8 Sung F, Yang Y, Zhang L, et al. Learning to compare: Relation network for few-shot learning. In: Proceedings of\nComputer Vision and Pattern Recognition (CVPR), 2018. 1199-1208\n9 Li W, Wang L, Xu J, et al. Revisiting local descriptor based image-to-class measure for few-shot learning. In:\nProceedings of Computer Vision and Pattern Recognition (CVPR), 2019. 7260-7268\n10 Kang D, Kwon H, Min J, et al. Relational Embedding for Few-Shot Classiﬁcation. In: Proceedings of International\nConference on Computer Vision (ICCV), 2021. 8822-8833\n11 Li A, Luo T, Xiang T, et al. Few-shot learning with global class representations. In: Proceedings of International\nConference on Computer Vision (ICCV), 2019. 9715-9724\n12 Chen H, Li H, Li Y, et al. Multi-scale adaptive task attention network for few-shot learning. In: Proceedings of\nInternational Conference on Pattern Recognition (ICPR), 2022.\n13 Zhang C, Cai Y, Lin G, et al. Deepemd: Few-shot image classiﬁcation with diﬀerentiable earth mover’s distance and\nstructured classiﬁers. In: Proceedings of Computer Vision and Pattern Recognition (CVPR), 2020. 12203-12213\n14 Hou R, Chang H, Ma B, et al. Cross attention network for few-shot classiﬁcation. In: Proceedings of Neural Information\nProcessing Systems (NeurIPS), 2019. 4005?4016\n15 Hao F, He F, Cheng J, et al. Collect and select: Semantic alignment metric learning for few-shot learning. In:\nProceedings of International Conference on Computer Vision (ICCV), 2019. 8460-8469\nHaoxing Chen, et al. Sci China Inf Sci 13\n16 Li W, Xu J, Huo J, et al. Distribution consistency based covariance metric networks for few-shot learning. In:\nProceedings of Association for the Advancement of Artiﬁcial Intelligence (AAAI), 2019. 8642-8649\n17 Haghverdi L, Lun A T L, Morgan M D, et al. Batch eﬀects in single-cell RNA-sequencing data are corrected by\nmatching mutual nearest neighbors. Nat Biotechnol, 2018, 36: 421-427\n18 Liu Y, Zheng T, Song J, et al. DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network.\n2021. ArXiv:2103.08160\n19 Ye H J, Hu H, Zhan D C, et al. Few-shot learning via embedding adaptation with set-to-set functions. In: Proceedings\nof Computer Vision and Pattern Recognition (CVPR), 2020. 8808-8817\n20 Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. In: Proceedings of Neural Information Processing\nSystems (NeurIPS), 2017. 8088-8017\n21 Doersch C, Gupta A, Zisserman A. Crosstransformers: spatially-aware few-shot transfer. In: Proceedings of Neural\nInformation Processing Systems (NeurIPS), 2020. 21981-21993\n22 Ren M, Triantaﬁllou E, Ravi S, et al. Meta-learning for semi-supervised few-shot classiﬁcation. In: Proceedings of\nInternational Conference on Learning Representations (ICLR), 2018.\n23 Bertinetto L, Henriques J F, Torr P H S, et al. Meta-learning with diﬀerentiable closed-form solvers. In: Proceedings\nof International Conference on Learning Representations (ICLR), 2019.\n24 Oreshkin B, Rodriguez Lopez P, Lacoste A. Tadam: Task dependent adaptive metric for improved few-shot learning.\nIn: Proceedings of Neural Information Processing Systems (NeurIPS), 2018. 719-729\n25 Simon C, Koniusz P, Nock R, et al. Adaptive subspaces for few-shot learning. In: Proceedings of Computer Vision\nand Pattern Recognition (CVPR), 2020. 4136-4145\n26 Lu S, Ye H J, Zhan D C. Tailoring embedding function to heterogeneous few-shot tasks by global and local feature\nadaptors. In: Proceedings of Association for the Advancement of Artiﬁcial Intelligence (AAAI), 2021. 8776-8783\n27 Zhang H, Koniusz P, Jian S, et al. Rethinking class relations: Absolute-relative supervised and unsupervised few-shot\nlearning. In: Proceedings of Computer Vision and Pattern Recognition (CVPR), 2021. 9432-9441\n28 Chen Z, Ge J, Zhan H, et al. Pareto self-supervised training for few-shot learning. In: Proceedings of Computer Vision\nand Pattern Recognition (CVPR), 2021. 13663-13672\n29 Lee K, Maji S, Ravichandran A, et al. Meta-learning with diﬀerentiable convex optimization. In: Proceedings of\nComputer Vision and Pattern Recognition (CVPR), 2019. 10657-10665.\n30 Kim J, Kim H, Kim G. Model-agnostic boundary-adversarial sampling for test-time generalization in few-shot learning.\nIn: Proceedings of European Conference on Computer Vision (ECCV), 2020. 599-617\n31 Dhillon G S, Chaudhari P, Ravichandran A, et al. A baseline for few-shot image classiﬁcation. In: Proceedings of\nInternational Conference on Learning Representations (ICLR), 2020.\n32 Ren M, Triantaﬁllou E, Ravi S, et al. Meta-learning for semi-supervised few-shot classiﬁcation. In: Proceedings of\nInternational Conference on Learning Representations (ICLR), 2018.\n33 Chen M, Wang X, Luo H, et al. Learning to focus: cascaded feature matching network for few-shot image recognition.\nSci China Inf Sci, 2021, 64: 1-13\n34 Laenen S, Bertinetto L. On episodes, prototypical networks, and few-shot learning. In: Proceedings of Neural Infor-\nmation Processing Systems (NeurIPS), 2021.\n35 Ravi S, Larochelle H. Optimization as a model for few-shot learning. In: Proceedings of International Conference on\nLearning Representations (ICLR), 2017.\n36 Jamal M A, Qi G J. Task agnostic meta-learning for few-shot learning. In: Proceedings of Computer Vision and\nPattern Recognition (CVPR), 2019. 11719-11727\n37 Li H, Dong W, Mei X, et al. LGM-Net: Learning to generate matching networks for few-shot learning. In: Proceedings\nof International Conference on Machine Learning (ICML), 2019. 3825-3834\n38 Paszke A, Gross S, Chintala S, et al. Automatic diﬀerentiation in PyTorch. In: Proceedings of Neural Information\nProcessing Systems (NeurIPS) Workshop, 2017.\n39 Kinga D, Adam J B. A method for stochastic optimization. In: Proceedings of International Conference on Learning\nRepresentations (ICLR), 2015.\n40 Tian Y, Wang Y, Krishnan D, et al. Rethinking few-shot image classiﬁcation: a good embedding is all you need?. In:\nProceedings of European Conference on Computer Vision (ECCV), 2020. 266-282\n41 Cheng G, Lang C, Han J. Holistic Prototype Activation for Few-Shot Segmentation. IEEE Trans Pattern Anal Mach\nIntell, 2022.\n42 Lang C, Cheng G, Tu B, et al. Learning what not to segment: A new perspective on few-shot segmentation. In:\nProceedings of Computer Vision and Pattern Recognition (CVPR). 2022. 8057-8067\n43 Cheng G, Li R, Lang C, et al. Task-wise attention guided part complementary learning for few-shot image classiﬁcation.\nSci China Inf Sci, 2021, 64(2): 1-14\n44 Chen H, Li H, Li Y, et al. Multi-level metric learning for few-shot image recognition. In: Proceedings of International\nConference on Artiﬁcial Neural Networks (ICANN), 2022.\n45 Yun S, Han D, Oh S J, et al. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In:\nProceedings of Computer Vision and Pattern Recognition (CVPR), 2019. 6023-6032",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5363788604736328
    },
    {
      "name": "Computer science",
      "score": 0.49339690804481506
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4588618278503418
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40098872780799866
    },
    {
      "name": "Materials science",
      "score": 0.23433783650398254
    },
    {
      "name": "Engineering",
      "score": 0.16415488719940186
    },
    {
      "name": "Electrical engineering",
      "score": 0.16235992312431335
    },
    {
      "name": "Metallurgy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}