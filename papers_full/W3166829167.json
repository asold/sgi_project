{
  "title": "Multi-Hop Transformer for Document-Level Machine Translation",
  "url": "https://openalex.org/W3166829167",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5100363535",
      "name": "Long Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5101753866",
      "name": "Tong Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5081699383",
      "name": "Haibo Zhang",
      "affiliations": [
        "Peking University",
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5028040391",
      "name": "Baosong Yang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5101538186",
      "name": "Wei Ye",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5101435571",
      "name": "Shikun Zhang",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2903728819",
    "https://openalex.org/W2963842551",
    "https://openalex.org/W3033962033",
    "https://openalex.org/W2606531491",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3035520602",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2962943802",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2964093087",
    "https://openalex.org/W2952446148",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2126209950",
    "https://openalex.org/W2964091467",
    "https://openalex.org/W2962739703",
    "https://openalex.org/W2962802109",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2752047430",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W2131494463",
    "https://openalex.org/W2521709538",
    "https://openalex.org/W3093404841",
    "https://openalex.org/W2608029998",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2964298349",
    "https://openalex.org/W2963344337",
    "https://openalex.org/W2799051177",
    "https://openalex.org/W2808508619",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2971278086",
    "https://openalex.org/W2750557179",
    "https://openalex.org/W2963652649",
    "https://openalex.org/W2963693355",
    "https://openalex.org/W2184135559",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2962882341",
    "https://openalex.org/W2983108239",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W2067815623",
    "https://openalex.org/W2964267515",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W2669742347",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2759173152",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2767019613",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W4300428972",
    "https://openalex.org/W3034351728",
    "https://openalex.org/W2964289193"
  ],
  "abstract": "Long Zhang, Tong Zhang, Haibo Zhang, Baosong Yang, Wei Ye, Shikun Zhang. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 3953–3963\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n3953\nMulti-Hop Transformer for Document-Level Machine Translation\nLong Zhang1,2,†, Tong Zhang1,2,†, Haibo Zhang3, Baosong Yang3,\nWei Ye1,∗, Shikun Zhang1\n1 National Engineering Research Center for Software Engineering, Peking University\n2 School of Software and Microelectronics, Peking University\n3 Alibaba Group\n{zhanglong418, zhangtong17, wye, zhangsk}@pku.edu.cn\n{zhanhui.zhb, yangbaosong.ybs}@alibaba-inc.com\nAbstract\nDocument-level neural machine translation\n(NMT) has proven to be of profound value for\nits effectiveness on capturing contextual infor-\nmation. Nevertheless, existing approaches 1)\nsimply introduce the representations of context\nsentences without explicitly characterizing the\ninter-sentence reasoning process; and 2) feed\nground-truth target contexts as extra inputs at\nthe training time, thus facing the problem of\nexposure bias. We approach these problems\nwith an inspiration from human behavior – hu-\nman translators ordinarily emerge a translation\ndraft in their mind and progressively revise it\naccording to the reasoning in discourse. To\nthis end, we propose a novel Multi-Hop Trans-\nformer (MHT) which offers NMT abilities to\nexplicitly model the human-like draft-editing\nand reasoning process. Speciﬁcally, our model\nserves the sentence-level translation as a draft\nand properly reﬁnes its representations by at-\ntending to multiple antecedent sentences itera-\ntively. Experiments on four widely used doc-\nument translation tasks demonstrate that our\nmethod can signiﬁcantly improve document-\nlevel translation performance and can tackle\ndiscourse phenomena, such as coreference er-\nror and the problem of polysemy.\n1 Introduction\nNeural machine translation (NMT) employs an end-\nto-end framework (Sutskever et al., 2014) and has\nadvanced promising results on various sentence-\nlevel translation tasks (Bahdanau et al., 2015;\nGehring et al., 2017; Vaswani et al., 2017; Wan\net al., 2020). However, most of NMT models han-\ndle sentences independently, regardless of the lin-\nguistic context that may appear outside the cur-\nrent sentence (Tiedemann and Scherrer, 2017a).\nThis makes NMT insufﬁcient to fully resolve the\ntypical context-dependent phenomena problematic,\n†These authors contributed equally to this work.\n*Corresponding author.\ne.g. coreference (Guillou, 2016), lexical cohe-\nsion (Carpuat, 2009), as well as lexical disambigua-\ntion (Gonzales et al., 2017).\nRecent studies (Tu et al., 2018; Maruf and Haf-\nfari, 2018; Maruf et al., 2019; Tan et al., 2019; Kim\net al., 2019; Zheng et al., 2020; Chen et al., 2020;\nSun et al., 2020; Ma et al., 2020) have proven to be\neffective on tackling discourse phenomena via feed-\ning NMT with contextual information, e.g. source-\nside (Wang et al., 2017; V oita et al., 2018; Zhang\net al., 2018) or target-side context sentences (Baw-\nden et al., 2018; Miculicich et al., 2018). De-\nspite their successes, these methods simply merge\nthe representations of context sentences together,\nlacking a mechanism to explicitly characterize the\ninter-sentence reasoning upon the context. Another\nshortage in existing document-level NMT is the\nproblem of exposure bias. Most of methods uti-\nlized the ground-truth target context for training\nbut the generated translations for inference, lead-\ning to inconsistent inputs at training and testing\ntime (Ranzato et al., 2015; Koehn and Knowles,\n2017).\nIntuitively, human translators tend to acquire\nuseful context information from the reasoning pro-\ncess among sentences, thus ﬁguring out the correct\nmeaning when they encounter ambiguity during\ntranslation. Sukhbaatar et al. (2015) and Shen\net al. (2017) empirically veriﬁed that modeling\nmulti-hop reasoning among sentences beneﬁts to\nthe language understanding task, e.g text compre-\nhension. V oita et al. (2019) showed that document-\nlevel NMT model can proﬁt from relative positions\nwith respect to context sentences, which to some\nextent conﬁrms the importance of the relationship\namong sentences. Meanwhile, Xia et al. (2017)\ndemonstrated that sentence-level NMT could be\nimproved by a two-pass draft-editing process, of\nwhich the second-pass decoder reﬁnes the target\nsentence generated by a ﬁrst-pass standard decoder.\nAccordingly, we propose to improve document-\n3954\nlevel NMT using a novel framework – Multi-Hop\nTransformer, which imitates draft-editing and rea-\nsoning process of human translators. Speciﬁcally,\nwe implement an explicit reasoning process by\nexploiting source and target antecedent sentences\nwith concurrently stacked attention layers, thus per-\nforming the progressive reﬁnement on the represen-\ntations of the current sentence and its translation.\nBesides, we leverage the draft to present context\ninformation on the target side during both training\nand testing, alleviating the problem of exposure\nbias.\nWe conduct experiments on four widely used\ndocument translation tasks: English-German and\nChinese-English TED, English-Russian Opensubti-\ntles, as well as English-German Europarl-7 datasets.\nExperimental results demonstrate that our method\nsigniﬁcantly outperforms both context-agnostic\nand context-aware methods. The qualitative analy-\nsis conﬁrms the effectiveness of the proposed multi-\nhop reasoning mechanism on resolving many lin-\nguistic phenomena, such as word sense disambigua-\ntion and coreference resolution. Our contributions\nare mainly in:\n• We propose the Multi-Hop Transformer. To\nthe best of our knowledge, this is the ﬁrst pi-\noneer investigation that introduces multi-hop\nreasoning into document-level NMT.\n• The proposed model takes target context\ndrafts into account at the training time, which\ndevotes to avoid the training-generation dis-\ncrepancy.\n• Our approach signiﬁcantly improves\ndocument-level translation performance\non four document-level translation tasks\nin terms of BLEU scores and solves some\ncontext-dependent phenomena, such as\ncoreference error and polysemy.\n2 Preliminary\nTransformer NMT is an end-to-end framework\nto build translation models. Vaswani et al. (2017)\npropose a new architecture called Transformer\nwhich adopts self-attention network for both en-\ncoding and decoding. Both its encoder and decoder\nconsist of multiple layers, each of which includes\na multi-head self-attention and a feed-forward sub-\nlayer. Additionally, each layer of the decoder ap-\nplys a multi-head cross attention to capture infor-\nmation from the encoder. Transformer has shown\nsuperiority in a variety of NLP tasks. Therefore,\nwe construct our models upon this advanced archi-\ntecture.\nDocument-level NMT In order to correctly\ntranslate the sentence with discourse phenomena,\nNMT models need to look beyond the current sen-\ntence and integrate contextual sentences as aux-\niliary inputs. Formally, let X = (x1,x2,..., xI)\nbe a source-language document composed of I\nsentences, where xi = (xi\n1,xi\n2,...,x i\nN) denotes\nthe ith sentence containing N words. Correspond-\ningly, the target-language document also consists\nof I sentences, Y = (y1,y2,..., yI), where yi =\n(yi\n1,yi\n2,...,y i\nM) denotes the ith sentence involving\nM words. Document-level NMT incorporates con-\ntextual information from both source side and tar-\nget side to autoregressively generate the best trans-\nlation result that has highest probability:\nPθ(yi|xi) =\nM∏\nm=1\nPθ(yi\nm|yi\n<m,xi,X−i,Y−i)\n(1)\nwhere yi\n<m is the sequence of proceeding tokens\nbefore position m. X−i and Y−i denote the con-\ntext sentences of the ith sentence.\nRelated Work Several studies have explored\nmulti-input models to leverage the contextual infor-\nmation from source-side (Jean et al., 2017; Kuang\nand Xiong, 2018) or target-side sentences (Kuang\net al., 2018; Miculicich et al., 2018). For the for-\nmer, Zhang et al. (2018) propose a new encoder\nto represent document-level context from previous\nsource-side sentences . Tiedemann and Scherrer\n(2017b) and Junczys-Dowmunt (2019) utilize the\nconcatenation of previous source-side sentences\nas input, while V oita et al. (2018) make use of\ngate mechanism to balance the weight between\ncurrent source sentence and its context. For the lat-\nter, Miculicich et al. (2018) propose a hierarchical\nattention (HAN) framework to capture the target\ncontextual information in the decoder. Bawden\net al. (2018), Maruf and Haffari (2018) and Maruf\net al. (2019) take both source-side and target-side\ncontext into account.\nMotivation As seen, both of the existing meth-\nods simply introduce the context sentences with-\nout explicitly characterizing the inter-sentence rea-\nsoning. Intuitively, when humans have difﬁculty\nin translation like encountering ambiguity phe-\nnomenon, they could acquire more information\n3955\nSource-side \nContext Encoder\nSource Embedding\nK-th Hop \nAttention Layer\nSecond Hop \nAttention Layer\nFirst Hop \nAttention Layer\nSelf-Attention\nLayer\n \u0000\nSource-side \nContext Encoder\nSource-side \nContext Encoder\nTarget Embedding\nK-th Hop \nAttention Layer\nSecond Hop \nAttention Layer\nFirst Hop \nAttention Layer\nDraft-Attention \nLayer\n \u0000  1 − \u0000\nEncoder-Decoder \nAttention Layer\n+  1 − \u0000\n+\nTarget-side \nContext Encoder\nSelf-Attention\nLayer\nTarget-side \nDraft Encoder\nSoftmax\n��\n� −� \n��\n� −�+� \n��\n� −� \n�  \n��\n� −� \n��\n� −�+� \n��\n� −� \n 6 × \n 6 ×\nTranslation\nSource Sentence i Target Sentence i\nDraft Sentence \nContext Draft \nSentence i-1\nContext Draft \nSentence i-k+1\nContext Draft \nSentence i-k\nSource Context \nSentence i-1\nSource Context \nSentence i-k+1\nSource Context \nSentence i-k\n Source-side \nSentence Encoder \n Multi-Hop Encoder  Multi-Hop Decoder \n Target-side\nSentence Encoder \nTarget-side \nContext Encoder\nTarget-side \nContext Encoder\n …\n …\n …\n …\n …\n …\nFigure 1: Illustration of Multi-Hop Transformer.ci−j\ns and ci−j\nt indicate the jth previous sentence in the source side\nand target side respectively.ddenotes the draft of current source sentence. All drafts are generated by a pre-trained\nsentence-level NMT model. The modules inside dashed box are the proposed multi-hop attention layers, which\ngradually reﬁne the representation of current sentence. Finally, the context gate αis used to control the contextual\ninformation.\nfrom the contexts sentence by sentence and then\nperform reasoning to ﬁgure out the exact mean-\ning. We attribute that such reasoning process is\nalso beneﬁcial to machine translation task. Re-\ncent successes in text comprehension communi-\nties have to some extent supported our hypothesis\n(Hill et al., 2015; Kumar et al., 2016). For exam-\nple, Sukhbaatar et al. (2015) propose a multi-hop\nend-to-end memory network, which can renew the\nquery representation with multiple computational\nsteps (which they term “hops”). Dhingra et al.\n(2016) extend an attention-sum reader to multi-turn\nreasoning with a gating mechanism. In addition,\nShen et al. (2017) introduce multi-hop attention,\nwhich used multiple turns to effectively exploit and\nreason over the relation among queries and docu-\nments.\nIn this paper, we propose to bring the idea of\nmulti-hop into document translation and aim at\nmimicking the multi-step comprehension and re-\nvising process of human translators. Contrast with\nthose models for text comprehension which scan\nthe query and document for multiple passes, our\nmodel iteratively focuses on different context sen-\ntences, which captures the inter-sentence reasoning\nsemantics of contextual sentences to incrementally\nreﬁne the representation of current sentence.\n3 Multi-Hop Transformer\nWith this mind, we propose a novel method called\nMulti-Hop Transformer, which models the reason-\ning process among multiple contextual sentences\nin both source side and target side. The source-side\ncontexts are directly acquired from the document.\nThe target-side contexts, called target-side drafts in\nthis paper, are generated by a sentence-level NMT\nmodel. These contexts are fed into the Multi-Hop\nTransformer with pre-trained encoders. The overall\narchitecture of our proposed model is illustrated in\nFigure 1, which consists of three components:\n• Sentence Encoder:This component contains\ntwo pre-trained encoders, one of which is\ncalled source-side sentence encoder and the\nother is called target-side sentence encoder.\nThese encoders generate representations for\n3956\nsource-side contexts and target-side drafts re-\nspectively.\n• Multi-Hop Encoder:We extend the original\nTransformer encoder with a novel multi-hop\nencoder to efﬁciently perform sentence-by-\nsentence reasoning on source-side contexts\nand generate the representation for the current\nsentence.\n• Multi-Hop Decoder:Similarly, a multi-hop\ndecoder is proposed to acquire information\nfrom the target-side drafts and models the\ntranslation probability distribution.\n3.1 Sentence Encoder\nWe use multi-layer and multi-head self-attention ar-\nchitecture (Vaswani et al., 2017) to obtain the repre-\nsentations for source-side contexts and target-side\ndrafts. Similar to the encoder of Transformer, sen-\ntence encoder contains a stack of six identical lay-\ners, each of which consists of two sub-layers. The\nﬁrst sub-layer is a multi-head attention( Q,K,V),\nwhich takes a query Q, a key K and a value V as\ninputs. The second sub-layer is a fully connected\nfeed-forward network (FFN).\nSource-Side Sentence Encoder. This encoder is\nutilized to generate the representations for source-\nside contexts, as shown in Figure 1.\nFor the current sentence s = xi to be trans-\nlated, we use the previous sentences X−i = (xi−k,\nxi−k+1, ..., xi−1) in the same document as the\nsource-side context, specially denoted as ci−k\ns ,\nci−k+1\ns , ..., ci−1\ns for clarity. k is the context win-\ndow size. For the jth context, we obtain the A(n)\nci−j\ns\nwhich denotes the nth hidden layer representation\nof ci−j\ns as follows:\nA(n)\nci−j\ns\n= MHA(H(n−1)\nci−j\ns\n,H(n−1)\nci−j\ns\n,H(n−1)\nci−j\ns\n) (2)\nwhere n = 1,2,..., 6. MHA represents the stan-\ndard Multi-Head Attention function (Vaswani et al.,\n2017). jdenotes the distance between the context\nsentence and current sentence.\nTarget-Side Sentence Encoder. Most existing\nworks use ground-truth target-side contexts as the\ninput of decoder during training (V oita et al., 2019).\nHowever, the target contexts at training and test-\ning are drawn from different distributions, leading\nto the inconsistency between training and testing.\nTo alleviate this problem, we instead make use\nof target-side context drafts generated from a pre-\ntrained sentence-level translation model. Similar\nto source-side sentence encoder, this target-side\ncontext draft encoder is used to obtain the con-\ntext representation A(n)\nci−j\nt\nof the jth target-side draft\nci−j\nt . Besides, we obtain a draft translation dof\nthe current sentence from the pre-trained sentence-\nlevel translation model and use a target-side draft\nencoder to obtain the representation A(n)\nd .\n3.2 Multi-Hop Encoder\nThe multi-hop encoder contains a stack of 6 iden-\ntical layers, each of which contains the following\nsub-layers:\nSelf-Attention Layer. The ﬁrst sub-layer makes\nuse of multi-head self-attention to encode the infor-\nmation of current source sentence sand obtains the\nrepresentation A(n)\ns .\nMulti-Hop Attention Layer. The second sub-\nlayer uses a multi-hop attention to perform\nsentence-by-sentence reasoning on cs in sentence\norder as shown in Figure 1. Each reasoning step,\nalso called a hop, is implemented by a multi-head\nattention layer. The ﬁrst hop takes representation\nA(n)\ns as the query and the representation A(n)\nci−k\ns\nof\nthe previous kth sentence as the key and value.\nB(n)\nsi−k = MHA(A(n)\ns ,A(n)\nci−k\ns\n,A(n)\nci−k\ns\n) (3)\nThe other hops are implemented:\nB(n)\nsi−j = MHA(B(n)\nsi−j−1 ,A(n)\nci−j\ns\n,A(n)\nci−j\ns\n) (4)\nwhere j = k−1,k−2,..., 1. jdenotes the distance\nbetween the context sentence and current sentence.\nContext Gating. The information of current\nsource sentence is crucial in translation while the\ncontextual information is auxiliary. In order to\navoid excessive utilization of contextual informa-\ntion, a context gating mechanism (Tu et al., 2017;\nYang et al., 2017, 2019) is introduced to dynami-\ncally control the weight between context sentences\nand current sentence:\nα= σ(WaA(n)\ns + WbB(n)\nsi−1 ), (5)\nwhere σis the logistic sigmoid function and αis\nthe context gate. Wa and Wb denote the weight\nmatrices of A(n)\ns and B(n)\nsi−1 , respectively.\nH(n)\ns = α⊙A(n)\ns + (1−α) ⊙B(n)\nsi−1 (6)\n3957\nFinally, we obtain the representation Encs =\nH(6)\ns as the ﬁnal output of the multi-hop encoder.\n3.3 Multi-Hop Decoder\nSimilarly, the multi-hop decoder involves a stack\nof 6 identical layers. Each of them contains ﬁve\nsub-layers.\nSelf-Attention Layer. The ﬁrst sub-layer utilizes\nmulti-head self-attention to encode the information\nof current target sentence tand obtains the repre-\nsentation A(n)\nt .\nDraft-Attention Layer. Inspired by Xia et al.\n(2017), we introduce the complete draft d trans-\nlated from current source sentence by a sentence-\nlevel NMT. Then this draft representation A(n)\nd is\nencoded by the target-side draft encoder in Section\n3.1. The draft attention is achieved by multi-head\nattention:\nF(n)\nt = MHA(A(n)\nt ,A(n)\nd ,A(n)\nd ). (7)\nMulti-Hop Attention Layer. Similar to the en-\ncoder, a multi-hop reasoning process is performed\non the target-side contexts. The target-side drafts\nare generated from corresponding source sentences\nby a pre-trained sentence-level NMT model. The\nﬁrst hop takes representation F(n)\nt as the query and\nthe representation A(n)\nci−k\nt\nof the previous kth draft\nas the key and value.\nB(n)\nti−k = MHA(F(n)\nt ,A(n)\nci−k\nt\n,A(n)\nci−k\nt\n) (8)\nThe other hops are achieved:\nB(n)\nti−j = MHA(B(n)\nti−j−1 ,A(n)\nci−j\nt\n,A(n)\nci−j\nt\n) (9)\nwhere j = k−1,k−2,..., 1. jdenotes the distance\nbetween the context draft and current target draft.\nContext Gating. Same as the multi-hop encoder,\nthe ﬁnal output of multi-hop decoder is computed\nas:\nG(n)\nt = α⊙F(n)\nt + (1−α) ⊙B(n)\nti−1 (10)\nwhere αis used to regulate the weight of target-side\ncontextual information.\nEncoder-Decoder Attention Layer. Finally, we\nuse an encoder-decoder attention layer to integrate\nthe output of multi-hop encoder Encs with the\ncurrent target representation G(n)\nt .\nH(n)\nt = MHA(G(n)\nt ,Encs,Encs) (11)\nwhere H(n)\nt represents the ﬁnal representation of\ndecoder.\n4 Experiments\n4.1 Datasets\nTo evaluate the effectiveness of the proposed MHT,\nwe conduct experiments on four widely used doc-\nument translation tasks, including the TED Talk\n(Cettolo et al., 2012) with two language pairs,\nOpensubtitles (Maruf et al., 2018) and Europarl7\n(Maruf et al., 2018). All datasets are tokenized\nand truecased with the Moses toolkit (Koehn et al.,\n2007), and splited into sub-word units with a joint\nBPE model (Sennrich et al., 2016) with 30K merge\noperations. The datasets are described as follows:\n• TED Talk (English-German): We use the\ndataset of IWSLT 2017 MT English-German\ntrack for training, which contains transcripts\nof TED talks aligned at sentence level.\ndev2010 is used for development and tst2016-\n2017 for evaluation. Statistically, there are\n0.21M sentences in the training set, 9K sen-\ntences in the development set, and 2.3K sen-\ntences in the test set.\n• TED Talk (Chinese-English): We use the\ncorpus consisting of 0.2M sentence pairs ex-\ntracted from IWSLT 2014 and 2015 Chinese-\nEnglish track for training. dev2010 involves\n0.8K sentences for development and tst2010-\n2013 contains 5.5K sentences for test.\n• Opensubtitles (English-Russian): We make\nuse of the parallel corpus from Maruf et al.\n(2018). The training set includes 0.3M sen-\ntence pairs. There are 6K sentence pairs in\ndevelopment set, and 9K in test set.\n• Europarl7 (English-German): The raw Eu-\nroparl v7 corpus (Koehn, 2005) contains\nSPEAKER and LANGUAGE tags where the\nlatter indicates the language the speaker was\nactually using. We process the raw data and\nextract the parallel corpus as same as Maruf\n3958\nMethod TED Opensubtitles Europarl7 Params A VGEn →De Zh →En En →Ru En →De\nTransformer⋆ 24.55 18.36 19.46 30.18 50M 23.14\nCA-Transformer† 25.04 18.77 20.21 30.67 72M 23.67\n(Maruf et al., 2018)† - - 19.13⋄ 26.49⋄ - -\nCA-HAN† 25.70 18.79 20.08 26.61 70M 22.79\n(Maruf et al., 2019)† 24.62⋄ - - - 54M⋄ -\nCADec† 26.08 19.01 19.46 30.36 91M 23.98\nMHT (Ours)† 26.22 19.52 20.46 31.25 80M 24.36\nTable 1: BLEU scores on TED Talk, Opensubtitles and Europarl7 tasks. ⋆mark indicates context-agnostic NMT\nmodels and †mark indicates context-aware NMT models. A VG indicates the average BLEU scores on test sets.\n⋄denotes that the value is reported by the corresponding paper. Our MHT model achieves better performance\nthan both context-agnostic and context-aware strong baseline on four examined tasks. The signiﬁcance tests are\nconducted for testing the robustness of approaches, and the results are statistically signiﬁcant with p < 0.05.\net al. (2018). 0.1M sentence pairs are used for\ntraining, 3K sentence pairs for development,\nand 5K sentence pairs for evaluation.\n4.2 Baselines\nWe compare our model against four NMT systems\nas follows:\n• Transformer: The state-of-the-art context-\nagnostic NMT model (Vaswani et al., 2017).\n• CA-Transformer: A context-aware trans-\nformer model (CA-Transformer) with an ad-\nditional context encoder to incorporate doc-\nument contextual information into model\n(Zhang et al., 2018).\n• CA-HAN: A context-aware hierarchical at-\ntention networks (CA-HAN) which integrate\ndocument contextual information from both\nsource side and target side (Miculicich et al.,\n2018).\n• CADec: A two-pass machine translation\nmodel (Context-Aware Decoder, CADec)\nwhich ﬁrst produces a draft translation of the\ncurrent sentence, then corrects it using context\n(V oita et al., 2019).\n4.3 Implementation Details\nOur model is implemented on the open-source\ntoolkit Thumt (Zhang et al., 2017). Adam opti-\nmizer (Kingma and Ba, 2014) is applied with an\ninitial learning rate 0.1. The size of hidden dimen-\nsion and feed-forward layer are set to 512 and 2048\nrespectively. Encoder and decoder have 6 layers\nwith 8 heads multi-head attention. Dropout is 0.1\nand batch size is set to 4096. Beam size is 4 for\ninference. Translation quality is evaluated by the\ntraditional metric BLEU (Papineni et al., 2002) on\ntokenized text. Context window size is set to 3,\nconsistent with the experiments in Section 5.2.\nTo initialize the source-side sentence encoder in\nSection 3.1, a sentence-level NMT model is trained\nfrom source language to target language using the\ncorresponding datasets without additional corpus.\nThe encoder of this trained model is used to ini-\ntialize the source-side context encoder. Also, we\nutilize the trained model to translate the source-side\nsentences and obtain the target-side drafts. Simi-\nlarly, we train a sentence-level model from target\nlanguage to source language to initialize the target-\nside encoders in Section 3.1. In order to reduce\nthe computational overhead, we share the parame-\nters among the sentence encoders on the same side.\nThe settings of these two sentence-level NMT mod-\nels are consistent with our baseline Transformer\nmodel.\n4.4 Results\nTable 1 summarizes the BLEU scores of different\nsystems on four tasks. As seen, our baseline and\nre-implemented existing methods outperform the\nreported results on the same data, which we believe\nmakes the evaluation convincing.\nClearly, our model MHT signiﬁcantly improves\ntranslation quality in terms of BLEU on these tasks,\nand obtains the best average results that gain 0.38,\n0.69 and 1.57 BLEU points over CADec, CA-\nTransformer and CA-HAN respectively. These re-\nsults demonstrate the universality and effectiveness\nof the proposed approach. Moreover, without in-\n3959\n0 1 2 3 4\nwindow size\n25.4\n25.6\n25.8\n26.0\n26.2BLEU\nTED (en-de)\n0 1 2 3 4\nwindow size\n18.8\n18.9\n19.0\n19.1\n19.2\n19.3\n19.4\n19.5BLEU\nTED (zh-en)\nFigure 2: The performance of the MHT model on TED (En-De) and TED (Zh-En) translation task using different\ncontext window sizes.\ntroducing large-scale pre-trained language models,\nour translation systems achieve new state-of-the-art\ntranslation qualities across three examined transla-\ntion tasks, which are TED (En-De), Opensubtitles\n(En-Ru) and Europarl7 (En-De). Overall, our ex-\nperiments indicate the following two points: 1) ex-\nplicitly modeling underlying reasoning semantics\nby a multi-hop mechanism indeed beneﬁts neural\nmachine translation, and 2) the improvements of\nour model are not from enlarging the network.\n5 Analysis\nIn this section, to gain further insight, we explore\nthe effectiveness of several factors of our model, in-\ncluding 1) multi-hop attention; 2) context window\nsize; 3) reasoning direction; 4) sides for introduc-\ning context; and 5) target contexts. Moreover, we\nshow qualitative analysis on discourse phenomena\nto better understand the advantage of our model.\n5.1 Multi-Hop Attention\nTo further investigate the effect of multi-hop rea-\nsoning, we compare our multi-hop attention with\ntwo baseline context modeling methods, includ-\ning “Concat” and “Hierarchical Attention”. Ta-\nble 2 shows the results of three different context\nmodeling modules on TED, which use same in-\nputs containing original training data and drafts.\n“Concat” denotes the MHT model simply using\nthe concatenation of the three context sentences\nrepresentations to get the ﬁnal context representa-\ntion. “Hierarchical Attention” denotes the MHT\nmodel with a hierarchical attention to model con-\ntext, which consists of a sentence-level attention\nand a token-level attention to capture information\nfrom the appropriate context sentences and tokens,\nas in Miculicich et al. (2018). As depicted in Ta-\nble 2, we replace multi-hop attention with these\ntwo baseline modules for experiments. “Hierar-\nchical Attention” slightly outperforms “Concat”,\nwhile multi-hop attention leads both of them by\na much larger margin. The results demonstrate\nthat multi-hop attention is capable of providing a\nmore ﬁne-grained representation of reasoning state\nover context and consequently capturing context\nsemantic information more accurately.\nMethod TED (En-De) TED (Zh-En)\nConcat 25.52 18.53\nHierarchical Attention 25.65 18.71\nMulti-Hop Attention 26.22 19.52\nTable 2: Comparison of different context modeling\nmethods.\n5.2 Context Window Size\nAs shown in Figure 2, we conduct experiments with\ndifferent context window sizes to explore its effect.\nWhen the window size is less than 4, the model ob-\ntains more information from contexts and achieves\nbetter performance as the window size gets larger.\nHowever, when window size is increased to 4, we\nﬁnd that the performance doesn’t improve further,\nbut decreases slightly. This phenomenon shows\nthat contexts far from the target sentence may be\nless relevant and cause noise (Kim et al., 2019).\nTherefore, we choose the window size 3 for our\nmodel MHT.\n5.3 Reasoning Direction\nIn Table 3, we conduct an ablation study to inves-\ntigate the effect of reasoning direction on MHT\nmodel. L2R denotes the MHT model with natural\nreasoning direction, which encodes context sen-\ntences from left to right by multi-hop layers, while\n3960\nDirection TED (En-De) TED (Zh-En)\nL2R 26.22 19.52\nR2L 25.80 19.18\nTable 3: The performance of the MHT model on TED\n(En-de) and TED (Zh-En) using different reasoning di-\nrection. L2R denotes the left to right direction for rea-\nsoning in context, while R2L is the opposite reasoning\ndirection.\nR2L indicates the MHT model encoding context\nsentences with an opposite direction. We observe\nthat integrating reasoning processes by multi-hop\nattention with both direction can improve the effect\nof Transformer due to the incorporation of extra\ncontext information. Besides, MHT model reason-\ning with natural sentence order outperforms the\nMHT model with an opposite reasoning direction.\nThis is within our expectation since the L2R rea-\nsoning is consistent with the reading and reasoning\ndirection of human being.\n5.4 Different Sides for Introducing Context\nAs shown in Table 4, we conduct an ablation study\nto explore how MTH model beneﬁts from con-\ntexts on source side and target side of MTH model.\n“None” indicates the MTH model without multi-\nhop attention module on any side of MHT model,\nbut only the draft of the current sentence. “Source”,\n“Target” and “Source & Target” indicate the MHT\nmodels with multi-hop attention module to intro-\nducing context on only source side, only target\nside and both sides respectively. We ﬁnd that inte-\ngrating source-side context or target-side context\ninto the model brings improvements over “None”\nthat ignores context on both side. Besides, MHT\nwith context on both sides achieves the best per-\nformance, indicating that the beneﬁcial context in-\nformation captured by multi-hop attention on the\nsource side and the target side are divergent and\ncomplementary.\nSide TED (En-De) TED (Zh-En)\nNone 25.40 18.80\nSource 25.86 19.24\nTarget 25.73 19.20\nSource & Target 26.22 19.52\nTable 4: Comparison of introducing context on differ-\nent sides of MHT model.\n5.5 Draft vs. Reference\nIn training, the context draft sentences can be the\ndrafts from a pre-trained MT system or the context\nreferences, while only the generated drafts are ac-\ncessible during inference. Table 5 shows the BLEU\nscores of the MHT models using generated drafts\nand context references during training. We can see\nthat the MHT model using drafts as contexts out-\nperforms the MHT model directly using target-side\ncontext references, possibly because using context\nreferences faces the problem of exposure bias and\nthe drafts generated from pre-trained translation\nsystem can bridge the gap between training and\ntesting data.\nTarget Contexts TED (En-De) TED (Zh-En)\nReference 26.03 19.21\nDraft 26.22 19.52\nTable 5: The performance of the MHT models using\ndrafts or context references.\n5.6 Qualitative Analysis\nWe present the translated results from baselines\nand our model in Table 6 to explore how multi-\nhop reasoning mitigate the impact of common dis-\ncourse phenomena in translation process. Accord-\ning to Case 1 in Table 6, the noun “hum” in source\nsentence is translated to “der Summen” by Trans-\nformer and CA-Transformer, which fail to under-\nstand the correct coreference. In German, “der” is\na masculine article. The correct article is neutral\narticle “das” because the “hum” is from a machine.\nMHT can perform a reasoning process to leverage\nthe context information effectively and ﬁgure out\nthe “hum” is from an engine according to Context 2.\nCase 2 indicates that MHT can understand the ex-\nact meaning of a polysemous word, beneﬁting from\nthe reasoning process among the contexts. In this\ncase, Transformer, CA-Transformer and CA-HAN\nall translates the noun “show” into “zeigt”, which\nmeans “display”. The translation is clearly wrong\nin this context. The correct meaning of “show” is\nTV shows like “Breaking Bad” according to the\nContext 1. In contrast, our model can take previous\ncontexts in consideration and reason out the exact\nmeaning of the polysemous word.\n6 Conclusion\nIn this paper, we propose a novel document-level\ntranslation model called Multi-Hop Transformer\n3961\nCase 1 Case 2\nContext3 They were 12, 3, and 1 when the hum stopped. So if your show gets a rating of nine points ...\nContext2 The hum of the engine died. Then you have a top two percent show.\nContext1 I stopped loving work. I couldn’t restart the engine. That’s shows like \"Breaking Bad,\" ...\nSource The hum would not come back. That kind of show .\nReference Das Summen kam nicht zurück. diese Art von Show .\nTransformer der Summen kam nicht zurück . das zeigt .\nCA-Transformer der Summen kam nicht zurück . das zeigt .\nCA-HAN das Summen würde nicht zurückkommen. das zeigt irgendwie .\nMHT das Summen kam nicht zurück . diese Art von Show .\nTable 6: Examples of the translation results of the baselines and MHT model.\nwith an inspiration from human reasoning behavior\nto explicitly model the human-like draft-editing and\nreasoning process. Experimental results on four\nwidely used tasks show that our model can achieve\nbetter performance than both context-agnostic and\ncontext-aware strong baseline. Furthermore, the\nqualitative analysis shows that the multi-hop rea-\nsoning mechanism is capable of solving some dis-\ncourse phenomena by capturing context semantics\nmore accurately.\nAcknowledgement\nThis work was supported by National Key R&D\nProgram of China (2018YFB1403202). We thank\nthe anonymous reviewers for their insightful com-\nments.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate.\nRachel Bawden, Rico Sennrich, Alexandra Birch, and\nBarry Haddow. 2018. Evaluating discourse phenom-\nena in neural machine translation. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1304–1313.\nMarine Carpuat. 2009. One translation per discourse.\nIn Proceedings of the Workshop on Semantic Evalu-\nations: Recent Achievements and Future Directions\n(SEW-2009), pages 19–27.\nMauro Cettolo, Christian Girardi, and Marcello Fed-\nerico. 2012. Wit3: Web inventory of transcribed and\ntranslated talks. In Conference of European Associ-\nation for Machine Translation, pages 261–268.\nJunxuan Chen, Xiang Li, Jiarui Zhang, Chulun\nZhou, Jianwei Cui, Bin Wang, and Jinsong Su.\n2020. Modeling discourse structure for document-\nlevel neural machine translation. arXiv preprint\narXiv:2006.04721.\nBhuwan Dhingra, Hanxiao Liu, Zhilin Yang,\nWilliam W Cohen, and Ruslan Salakhutdinov.\n2016. Gated-attention readers for text comprehen-\nsion. arXiv preprint arXiv:1606.01549.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 1243–1252. JMLR. org.\nAnnette Rios Gonzales, Laura Mascarell, and Rico Sen-\nnrich. 2017. Improving word sense disambigua-\ntion in neural machine translation with sense embed-\ndings. In Proceedings of the Second Conference on\nMachine Translation, pages 11–19.\nLiane Kirsten Guillou. 2016. Incorporating pronoun\nfunction into statistical machine translation.\nFelix Hill, Antoine Bordes, Sumit Chopra, and Jason\nWeston. 2015. The goldilocks principle: Reading\nchildren’s books with explicit memory representa-\ntions. arXiv preprint arXiv:1511.02301.\nSebastien Jean, Stanislas Lauly, Orhan Firat, and\nKyunghyun Cho. 2017. Does neural machine trans-\nlation beneﬁt from larger context? arXiv preprint\narXiv:1704.05135.\nMarcin Junczys-Dowmunt. 2019. Microsoft translator\nat wmt 2019: Towards large-scale document-level\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1), pages 225–233.\nYunsu Kim, Duc Thanh Tran, and Hermann Ney. 2019.\nWhen and why is document-level context useful in\nneural machine translation? In Proceedings of the\nFourth Workshop on Discourse in Machine Transla-\ntion (DiscoMT 2019), pages 24–34.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, et al. 2007. Moses: Open source\n3962\ntoolkit for statistical machine translation. In Pro-\nceedings of the 45th annual meeting of the associ-\nation for computational linguistics companion vol-\nume proceedings of the demo and poster sessions ,\npages 177–180.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the First Workshop on Neural Machine Trans-\nlation, pages 28–39.\nShaohui Kuang and Deyi Xiong. 2018. Fusing re-\ncency into neural machine translation with an inter-\nsentence gate model. In Proceedings of the 27th\nInternational Conference on Computational Linguis-\ntics, pages 607–617.\nShaohui Kuang, Deyi Xiong, Weihua Luo, and\nGuodong Zhou. 2018. Modeling coherence for\nneural machine translation with dynamic and topic\ncaches. In Proceedings of the 27th International\nConference on Computational Linguistics , pages\n596–606.\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,\nJames Bradbury, Ishaan Gulrajani, Victor Zhong,\nRomain Paulus, and Richard Socher. 2016. Ask me\nanything: Dynamic memory networks for natural\nlanguage processing. In International conference on\nmachine learning, pages 1378–1387.\nShuming Ma, Dongdong Zhang, and Ming Zhou. 2020.\nA simple and effective uniﬁed encoder for document-\nlevel machine translation. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 3505–3511.\nSameen Maruf and Gholamreza Haffari. 2018. Docu-\nment context neural machine translation with mem-\nory networks. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 1275–\n1284.\nSameen Maruf, André FT Martins, and Gholamreza\nHaffari. 2018. Contextual neural model for trans-\nlating bilingual multi-speaker conversations. In Pro-\nceedings of the Third Conference on Machine Trans-\nlation: Research Papers, pages 101–112.\nSameen Maruf, André FT Martins, and Gholamreza\nHaffari. 2019. Selective attention for context-aware\nneural machine translation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3092–3102.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018. Document-level neural\nmachine translation with hierarchical attention net-\nworks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2947–2954.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2015. Sequence level train-\ning with recurrent neural networks. arXiv preprint\narXiv:1511.06732.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725.\nYelong Shen, Po-Sen Huang, Jianfeng Gao, and\nWeizhu Chen. 2017. Reasonet: Learning to stop\nreading in machine comprehension. In Proceedings\nof the 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , pages\n1047–1055. ACM.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\n2015. End-to-end memory networks. In Advances\nin neural information processing systems , pages\n2440–2448.\nZewei Sun, Mingxuan Wang, Hao Zhou, Chengqi\nZhao, Shujian Huang, Jiajun Chen, and Lei Li. 2020.\nCapturing longer context for document-level neural\nmachine translation: A multi-resolutional approach.\narXiv preprint arXiv:2010.08961.\nI Sutskever, O Vinyals, and QV Le. 2014. Sequence to\nsequence learning with neural networks. Advances\nin NIPS.\nXin Tan, Longyin Zhang, Deyi Xiong, and Guodong\nZhou. 2019. Hierarchical modeling of global con-\ntext for document-level neural machine translation.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1576–\n1585.\nJörg Tiedemann and Yves Scherrer. 2017a. Neural\nmachine translation with extended context. arXiv\npreprint arXiv:1708.05943.\nJörg Tiedemann and Yves Scherrer. 2017b. Neural ma-\nchine translation with extended context. In Proceed-\nings of the Third Workshop on Discourse in Machine\nTranslation, pages 82–92, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nZhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu,\nand Hang Li. 2017. Context gates for neural ma-\nchine translation. Transactions of the Association\nfor Computational Linguistics, 5(1):87–99.\n3963\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang.\n2018. Learning to remember translation history with\na continuous cache. Transactions of the Association\nfor Computational Linguistics, 6:407–420.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019.\nWhen a good translation is wrong in context:\nContext-aware machine translation improves on\ndeixis, ellipsis, and lexical cohesion. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 1198–1212.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1264–1274.\nYu Wan, Baosong Yang, Derek F. Wong, Yikai Zhou,\nLidia S. Chao, Haibo Zhang, and Boxing Chen.\n2020. Self-Paced Learning for Neural Madchine\nTranslation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020.\nLongyue Wang, Zhaopeng Tu, Andy Way, and Qun Liu.\n2017. Exploiting cross-sentence context for neural\nmachine translation. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2826–2831.\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,\nNenghai Yu, and Tie-Yan Liu. 2017. Deliberation\nnetworks: Sequence generation beyond one-pass de-\ncoding. In Advances in Neural Information Process-\ning Systems, pages 1784–1794.\nBaosong Yang, Jian Li, Derek F Wong, Lidia S Chao,\nXing Wang, and Zhaopeng Tu. 2019. Context-aware\nself-attention networks. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , volume 33,\npages 387–394.\nBaosong Yang, Derek F Wong, Tong Xiao, Lidia S\nChao, and Jingbo Zhu. 2017. Towards bidirectional\nhierarchical representations for attention-based neu-\nral machine translation. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1432–1441.\nJiacheng Zhang, Yanzhuo Ding, Shiqi Shen, Yong\nCheng, Maosong Sun, Huanbo Luan, and Yang Liu.\n2017. Thumt: An open source toolkit for neural ma-\nchine translation. arXiv preprint arXiv:1706.06415.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.\nImproving the transformer translation model with\ndocument-level context. pages 533–542.\nZaixiang Zheng, Xiang Yue, Shujian Huang, Jiajun\nChen, and Alexandra Birch. 2020. Towards making\nthe most of context in neural machine translation. In\nIJCAI.",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.9547000527381897
    },
    {
      "name": "Machine translation",
      "score": 0.7434403896331787
    },
    {
      "name": "Computer science",
      "score": 0.5816820859909058
    },
    {
      "name": "Transformer",
      "score": 0.5649544596672058
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4546703100204468
    },
    {
      "name": "Natural language processing",
      "score": 0.43871238827705383
    },
    {
      "name": "Engineering",
      "score": 0.2053273618221283
    },
    {
      "name": "History",
      "score": 0.16675451397895813
    },
    {
      "name": "China",
      "score": 0.16067498922348022
    },
    {
      "name": "Electrical engineering",
      "score": 0.14927834272384644
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ],
  "cited_by": 10
}