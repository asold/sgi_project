{
  "title": "DLGNet: A Transformer-based Model for Dialogue Response Generation",
  "url": "https://openalex.org/W3045738072",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3045538295",
      "name": "Olabiyi, Oluwatobi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2473881307",
      "name": "Erik Mueller",
      "affiliations": [
        "Capital One (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6600234944",
    "https://openalex.org/W6812691129",
    "https://openalex.org/W6600547436",
    "https://openalex.org/W6600388300",
    "https://openalex.org/W2962896208",
    "https://openalex.org/W2972732298",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963688701",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962707484",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2963360026",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W2963167310",
    "https://openalex.org/W2963790827",
    "https://openalex.org/W1958706068",
    "https://openalex.org/W2418993857",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2951883832",
    "https://openalex.org/W2890969459",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2584185835",
    "https://openalex.org/W2972238628",
    "https://openalex.org/W2806935606",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4288122854",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W4288363941",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2938830017"
  ],
  "abstract": "Neural dialogue models, despite their successes, still suffer from lack of relevance, diversity, and in many cases coherence in their generated responses. On the other hand, transformer-based models such as GPT-2 have demonstrated an excellent ability to capture long-range structures in language modeling tasks. In this paper, we present DLGNet, a transformer-based model for dialogue modeling. We specifically examine the use of DLGNet for multi-turn dialogue response generation. In our experiments, we evaluate DLGNet on the open-domain Movie Triples dataset and the closed-domain Ubuntu Dialogue dataset. DLGNet models, although trained with only the maximum likelihood objective, achieve significant improvements over state-of-the-art multi-turn dialogue models. They also produce best performance to date on the two datasets based on several metrics, including BLEU, ROUGE, and distinct n-gram. Our analysis shows that the performance improvement is mostly due to the combination of (1) the long-range transformer architecture with (2) the injection of random informative paddings. Other contributing factors include the joint modeling of dialogue context and response, and the 100% tokenization coverage from the byte pair encoding (BPE).",
  "full_text": "Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 54–62\nJuly 9, 2020.c⃝2020 Association for Computational Linguistics\n54\nDLGNet: A Transformer-based Model for Dialogue Response Generation\nOluwatobi Olabiyi\nCapital One Conversation Research\nVienna V A\noluwatobi.olabiyi@capitalone.com\nErik T. Mueller\nCapital One Conversation Research\nVienna V A\nerik.mueller@capitalone.com\nAbstract\nNeural dialogue models, despite their suc-\ncesses, still suffer from lack of relevance,\ndiversity, and in many cases coherence in\ntheir generated responses. On the other hand,\ntransformer-based models such as GPT-2 have\ndemonstrated an excellent ability to capture\nlong-range structures in language modeling\ntasks. In this paper, we present DLGNet, a\ntransformer-based model for dialogue model-\ning. We speciﬁcally examine the use of DL-\nGNet for multi-turn dialogue response genera-\ntion. In our experiments, we evaluate DLGNet\non the open-domain Movie Triples dataset and\nthe closed-domain Ubuntu Dialogue dataset.\nDLGNet models, although trained with only\nthe maximum likelihood objective, achieve\nsigniﬁcant improvements over state-of-the-art\nmulti-turn dialogue models. They also pro-\nduce best performance to date on the two\ndatasets based on several metrics, including\nBLEU, ROUGE, and distinct n-gram. Our\nanalysis shows that the performance improve-\nment is mostly due to the combination of\n(1) the long-range transformer architecture\nwith (2) the injection of random informative\npaddings. Other contributing factors include\nthe joint modeling of dialogue context and re-\nsponse, and the 100% tokenization coverage\nfrom the byte pair encoding (BPE).\n1 Introduction\nRecent successes of pretrained transformer-based\nlanguage models, such as BERT (Devlin et al.,\n2019), GPT(-2) (Radford and Salimans, 2018; Rad-\nford et al., 2019), Transformer-XL (Dai et al.,\n2019), XLNet (Yang et al., 2019), and ERNIE(2.0)\n(Sun et al., 2019a,b), have led to state-of-the-art\nperformance on many natural language understand-\ning (NLU) tasks including sentence classiﬁcation,\nnamed entity recognition, sentence similarity, and\nquestion answering. The exceptional performance\nFigure 1: Positional Entropy for Movie and Ubuntu\ndatasets -Applying a greedy training objective to the\noriginal and BPE datasets can achieve low overall en-\ntropy just by overﬁtting to low entropy regions, result-\ning in short and generic responses. Injecting random\npaddings into the data does not suffer from this prob-\nlem and can be used to train transformer architectures\ndue to their lack of recurrent propagations.\nof transformer-based language models is due to\ntheir ability to capture long-term temporal depen-\ndencies in the input sequence. This attribute should\nbe very beneﬁcial to dialogue modeling, especially\nin multi-turn scenarios. Most of the existing neural\ndialogue response generation models are based on\nrecurrent neural networks (Sutskever et al., 2014;\nVinyals and Le, 2015; Li et al., 2016a; Serban et al.,\n2016; Xing et al., 2017; Serban et al., 2017b,a; Li\net al., 2016b; Zhang et al., 2018a; Olabiyi et al.,\n2018, 2019a).\nThese models have yielded promising results by\ngenerating mostly coherent responses given the di-\nalogue context. However, most of them, including\nthe state-of-the-art models trained with naturalis-\ntic dialogue data, still perform well below the hu-\nman level. Generated responses tend to be either\ngeneric, out-of-context, or disproportionately short.\nPrevious work points to some causes of these limi-\ntations:\n55\ni) Training data: The presence of high frequency\ngeneric utterances (utterance-level semantic redun-\ndancy), such as “I don’t know”, “I’m not sure”,\nand high frequency generic n-gram tokens (word-\nlevel syntactic redundancy), such as “I”, “I am”,\nleading to the concave positional entropy proﬁle\nof dialogue datasets, see Fig. 1), which makes\nlearning difﬁcult, resulting in short and generic\nresponses. ii) Short-range Model Architecture:\nShort-range model architectures that capture lim-\nited temporal dependencies. iii) Out-of-vocabulary\nProblem: Less frequent (usually more informa-\ntive) words mapped to the out-of-vocabulary token\n<UNK>, leading to generation of a large number\nof <UNK>tokens. iv) Exposure Bias: The dis-\ncrepancy in model behavior between training and\ninference, which limits the informativeness of the\nresponses iv) Training Objective: The limitations\nof the maximum likelihood training objective.\nIn this paper, we propose DLGNet, a\ntransformer-based model for multi-turn dialogue\nmodeling that addresses some of the highlighted\nproblems above. The use of a transformer architec-\nture allows DLGNet to capture long-term temporal\ndependencies in the dialogue data better than the\nexisting RNN-based architectures (Vaswani et al.,\n2017). However, applying a vanilla Seq2Seq trans-\nformer (Vaswani et al., 2017) and its multi-turn\nvariants, such as ReCoSa (Zhang et al., 2019), for\ndialogue modeling does not work well because\nof the semantic redundancy in dialogue data. To\novercome this, DLGNet models the joint distri-\nbution of the context and response instead of the\nconditional distribution of the response given the\ncontext, usually employed in Seq2Seq frameworks\n(Vinyals and Le, 2015; Serban et al., 2016; Olabiyi\net al., 2018; Vaswani et al., 2017). DLGNet also\naddresses the syntactic redundancy in dialogue data\nby appending random paddings before and after the\ninput data. This helps to break down the learning\nbarrier from the concave entropy proﬁle of human\nconversation data, as shown in Fig. 1. The ﬂatten-\ning of the entropy proﬁle also provides regulariza-\ntion during training, and reduces even the extent\nof the exposure bias problem. Finally, to avoid the\nout-of-vocabulary problem, DLGNet uses byte pair\nencoding (BPE) similar to GPT-2 (Radford et al.,\n2019) to provide 100% coverage for any Unicode\ninput and output texts. Given all these proposed\nchanges, we train DLGNet models using only the\nmaximum likelihood objective. DLGNet models,\nTable 1: Example of generated responses from DL-\nGNet models\nItem Utterance\nMovie\nContext 0 says he wanted food . <person> . he wanted the gold .\nContext 1 how ’ s he going to want the gold ? he couldn ’ t even know\nwe had it .\nGroundtruth he . he could have been watching us .\nDLGNet-117M 1 he knew we did .\nDLGNet-345M <person> ’ s just say he doesn ’ t have enough to buy a\ntrailer and seeds . i ’ ll write him up as a decorated veteran of\nthe united states army .\nDLGNet-117M Random 2 <person> , you know the break-in , i might not be able to\nafford it .\nDLGNet-345M Random are you going to preach now ?\nContext 0 what are you thinking ?\nContext 1 that i want you to take me home .\nGroundtruth now ? we just –\nDLGNet-117M now ? we ’ re going now ? what about tomorrow ?\nDLGNet-345M now ? we ’ re on the run !\nDLGNet-117M Random now ?\nDLGNet-345M Random no . keep me company . <person> offers its rewards .\n<person> appetit .\nUbuntu\nContext 0 The netboot one is suppose to download packages from the\nnet.\nContext 1 like the ones to be installed? or the installed to be run?\nGroundtruth Installed. The netbook also features the non-graphical in-\nstaller.\nDLGNet-117M the installed to be run.\nDLGNet-345M the ones to be installed.\nDLGNet-117M Random I think the netboot one is the one that is installed to the net.\nDLGNet-345M Random the ones to be installed to.\nContext 0 DJones: update manager won’t detect 12.04.1 as a new ver-\nsion if you are already running 12.04, because 12.04.1 = 12.04\n+ lots of package updates\nGroundtruth DJones I am running 10.04.\nDLGNet-117M I’m not sure what you mean by ”update manager won’t de-\ntect”\nDLGNet-345M I see. I guess I’ll just do a fresh install.\nDLGNet-117M Random I’m running 12.04.1, and I’m not running 12.04.1.\nDLGNet-345M Random I’m not sure what you mean by ”update manager”\ndespite being trained with only the maximum like-\nlihood objective, demonstrate state-of-the-art per-\nformance on the Movie and Ubuntu datasets, as\nmeasured in terms of BLEU, ROUGE, and distinct\nn-gram scores.\n2 Task Description\nConsider a dialogue sample consisting of a se-\nquence of N utterances, x =\n(\nx1,x2,··· ,xN\n)\n,\nwhere each utterance xi =\n(\nx1\ni,x2\ni,··· ,xMi\ni\n)\ncon-\ntains a variable-length sequence of Mi word to-\nkens such that xij ∈ V for vocabulary V. At\nany time step i, the dialogue history is given by\nxi =\n(\nx1,x2,··· ,xi\n)\n. The dialogue response\ngeneration task can be deﬁned as follows: Given\na dialogue history xi, generate a response yi =(\ny1\ni,y2\ni,··· ,yTi\ni\n)\n, where Ti is the number of gen-\nerated tokens such that the distribution of the gener-\nated response P(yi) is indistinguishable from that\nof the ground truth P(xi+1). The distribution of\nthe model output sequence can be factored by the\n1Model with pretraining\n2Model with random initialization (without pretraining)\n56\nFigure 2: An example of DLGNet input and outputconsisting of a 3-turn conversation sample separated by\n[TSEP] tokens, combined with random informative paddings, before and after. Paddings and conversations are\nseparated by [CSEP] tokens.\nproduct rule:\nP(yi|xi) =\nTi∏\nj=2\nP\n(\nyj\ni|y1:j−1\ni ,xi\n)\n(1)\nwhere y1:j−1\ni = (y1\ni,··· ,yj−1\ni ).\nThe MLE objective based on the conditional\ndistribution of (1) can be expressed as\nLCond = −log Pθ(yi|xi) (2)\nwhere θare the model parameters.\nThis formulation, known as Seq2Seq, originated\nfrom machine translation (Sutskever et al., 2014)\nand assumes that the context-response pair in the\ntraining examples are fairly unique. Seq2Seq is\nthe basis of most of the previous work on dialogue\nmodeling. The framework, however, does not ac-\ncount for the semantic and syntactic redundancy\nin human conversations as pointed out by Li et al.\n(2016a).\n3 DLGNet Model Description\nIn order to address the semantic redundancy, we\npropose to jointly model both the context and the\nresponse as an alternative to the mutual information\nobjective (Li et al., 2016a; Zhang et al., 2018b).\nThe resulting distribution and the objective function\ncan then be respectively expressed as\nP(yi,xi) =P(yi|xi)P(xi) (3)\nLJoint = −log Pθ(yi|xi) −log Pθ(xi) (4)\nWhile (3) addresses the semantic redundancy, it\ndoes not address the syntactic redundancy coming\nfrom the concave positional entropy proﬁle of dia-\nlogue data. To circumvent this, we append random\ninformative paddings (sampled from the dataset)\nbefore (xb\ni) and after (xa\ni), the dialogue example of\ninterest, leading to\nP(xa\ni,yi,xi,xb\ni) =P(xa\ni)P(yi|xi)P(xi)P(xb\ni)\n(5)\nand\nLDLGNet = −log Pθ(xa\ni) −log Pθ(yi|xi)\n−log Pθ(xi) −log Pθ(xb\ni) (6)\nsince xb\ni and xa\ni are independent of (yi,xi). As\nwe see from the resulting entropy proﬁle in Fig. 1,\nappending random paddings circumvents the ad-\nverse effect of syntactic redundancy in dialogue\ndata on model training. The conditional distribu-\ntion P(yi|xi) in (1) is then just an inference on the\njoint distribution of (5).\nDLGNet adopts GPT-2’s autoregressive trans-\nformer architecture (Radford et al., 2019) using\nonly the decoder part of the original transformer\narchitecture (Vaswani et al., 2017) since there is\nno need for a separate encoder network (see Fig.\n2). Autoregressive transformer models use multi-\nple layers of masked multi-head self-attention to\nmap a sequence of input tokens to a sequence of\noutput tokens (i.e., the input sequence token shifted\none position to the right). During inference, at\neach step, the model is autoregressive, consuming\nthe previously generated token as additional input\nwhen generating the next. There are some basic\nconceptual differences between autoregressive ar-\nchitectures based on transformers and those based\non recurrent neural networks (RNNs). For instance,\nwhile the output of an RNN layer depends on only\nthe immediate previous output, a transformer layer\noutput consists of attention over all previous out-\nputs. Due to this lack of ordering in transformer\narchitectures, the position representation is usually\npassed along with the input tokens into the model\n(Vaswani et al., 2017).\nIn order to take advantage and evaluate the im-\npact of pretrained parameters, we use two model\nconﬁgurations i.e., (i) DLGNet-117M - with 117M\nparameters, 12 attention layers, and a hidden state\nsize of 767, and (ii) DLGNet-345M - with 345M\nparameters, 24 attention layers, and a hidden state\nsize of 1024; similar to the publicly available GPT-\n2 models (Radford et al., 2019).\n57\nTable 2: Automatic Evaluation of Model Performance\nModel\nMovie Ubuntu\nRelevance Diversity Relevance Diversity\nBLEU ROUGE DIST-1/2 NASL BLEU ROUGE DIST-1/2 NASL\nHRED 0.0474 0.0384 0.0026/0.0056 0.535 0.0177 0.0483 0.0203/0.0466 0.892\nVHRED 0.0606 0.1181 0.0048/0.0163 0.831 0.0171 0.0855 0.0297/0.0890 0.873\nhredGAN u 0.0493 0.2416 0.0167/0.1306 0.884 0.0137 0.0716 0.0260/0.0847 1.379\nhredGAN w 0.0613 0.3244 0.0179/0.1720 1.540 0.0216 0.1168 0.0516/0.1821 1.098\nDAIM 0.0155 0.0077 0.0005/0.0006 0.721 0.0015 0.0131 0.0013/0.0048 1.626\naBoots u cat 0.0880 0.4063 0.0624/0.3417 0.918 0.0210 0.1491 0.0523/0.1795 1.040\naBoots w cat 0.0940 0.3973 0.0613/0.3476 1.016 0.0233 0.2292 0.1288/0.5190 1.208\nDLGNet-117M Random 0.1796 0.4338 0.1198/0.4578 1.011 0.0215 0.1978 0.1827/0.4074 0.829\nDLGNet-345M Random 0.2682 0.4881 0.1286/0.4612 0.907 0.0315 0.2041 0.1927/0.4468 0.794\nDLGNet-117M 0.1872 0.4346 0.1232/0.4506 0.982 0.0279 0.2191 0.2228/0.4953 0.746\nDLGNet-345M 0.2742 0.4945 0.1282/0.4736 0.895 0.0309 0.2409 0.2436/0.5632 0.759\n4 Model Training\nWe trained the small DLGNet-117M and the\nmedium DLGNet-345M models on multi-turn dia-\nlogue datasets initialized with either random noise\nor pretrained language model parameters. The mod-\nels are trained end-to-end using the Adaptive Mo-\nment Estimation (Adam) stochastic gradient de-\nscent algorithm with a learning rate of 0.001. The\nmaximum sequence length is 1024. Due to GPU\nmemory limitations, we use a batch size of 2 and\naccumulate gradients over 5 iterations, making the\neffective batch size 10. Both models are trained un-\ntil the training perplexity on the dialogue datasets\nreaches a steady state. Finally, the models are im-\nplemented, trained, and evaluated using Python and\nthe TensorFlow deep learning framework.\n5 Experiments\n5.1 Setup\nWe evaluated DLGNet models on the Movie Triples\nand Ubuntu Dialogue corpora randomly split into\ntraining, validation, and test sets, using 90%, 5%,\nand 5% proportions. Since we use BPE with 100%\ntokenization coverage, we performed no prepro-\ncessing of the datasets whatsoever. For each train-\ning example, however, we randomly sample a target\nconversation and two independent padding chunks\nfrom the dataset to ﬁll up the maximum input se-\nquence length. We append the paddings to the\ntarget conversation, one before, and one after, sepa-\nrated by token [C SEP]. The target conversation in\neach training example in turn consists of utterances\nthat are separated by token [T SEP] as shown in\nFig. 2.\nThe Movie dataset (Serban et al., 2016) spans\na wide range of topics with few spelling mis-\ntakes and contains about 240,000 dialogue triples,\nwhich makes it suitable for studying the relevance-\ndiversity tradeoff in multi-turn conversations\n(Zhang et al., 2018b). The Ubuntu dialog dataset\nextracted from the Ubuntu Relay Chat Channel\n(Serban et al., 2017b) contains about 1.85 million\nconversations with an average of 5 utterances per\nconversation. This dataset is ideal for training\ndialogue models that can provide expert knowl-\nedge/recommendation in domain-speciﬁc conver-\nsations.\nWe compare DLGNet multi-turn dialogue per-\nformance with existing state-of-the-art dialogue\nmodels including (V)HRED3 (Serban et al., 2016,\n2017b), DAIM4 (Zhang et al., 2018b), hredGAN\n(Olabiyi et al., 2018), and aBoots (Olabiyi et al.,\n2019b). Note that DAIM is single turn and does not\nuse a multi-turn dialogue context, but we have in-\ncluded it here for completeness. We compare how\nthe models perform based on informativeness (a\ncombination of relevance and diversity metrics) of\ngenerated responses. For relevance, we adopted\nBLEU-2 (Papineni et al., 2002) and ROUGE-2\n(Lin, 2014) scores. For diversity, we adopted dis-\ntinct unigram (DIST-1) and bigram (DIST-2) (Li\net al., 2016a) scores as well as normalized average\nsequence length (NASL), similar to Olabiyi et al.\n(2018).\nAll models are evaluated in autoregressive mode,\ni.e., we pass a multi-turn dialogue context to the\nmodel inputs and the models generate a sequence\nof response tokens using the context and all the pre-\nviously generated tokens until the end-of-sequence\n3implementation obtained from https://github.\ncom/julianser/hed-dlg-truncated\n4implementation obtained from https://github.\ncom/dreasysnail/converse_GAN\n58\ntoken is reached. All models are greedily sam-\npled to generate the model outputs. It is worth\nnoting that, for DLGNet models, we search for the\noptimum top k between 0 and 20 inclusive that\nmaximizes the overall BLEU-2 (relevance) score\nof the validation set using the top k sampling strat-\negy (Radford et al., 2019). It turns out that for all\nDLGNet models, the optimum top k is 1 across\ndatasets, which is equivalent to greedy sampling.\n6 Results and Discussion\n6.1 Quantitative Evaluation\nWe report the quantitative measures in Table 2.\nThe transformer-based DLGNet provides a signif-\nicant improvement in response generation perfor-\nmance over existing methods such as (V)HRED,\nhredGAN, DAIM, and adversarial bootstrapping\n(aBoots), all of which are based on recurrent neural\nnetworks. In fact, DLGNet achieves the best per-\nformance to date on the Movie triples and Ubuntu\ndialogue datasets in terms of BLEU, ROUGE, and\ndistinct n-gram scores. This indicates that, despite\nbeing trained only with the maximum likelihood\nobjective, the autoregressive transformer architec-\nture in conjunction with the random padding in-\njection, is able to overcome some of the problems\nthat have plagued existing dialogue models such as\nsemantic and syntactic redundancy, and exposure\nbias. Also contributing to the models’ performance\nimprovement is the 100% input coverage from the\nBPE encoding, which eliminates the generation\nof <UNK>tokens (this is especially helpful for\nthe Ubuntu dataset with a large number of out-of-\nvocabulary tokens) as well as the joint modeling\nof the context and response. Also, in contrast to\nexisting work reporting a trade-off between rele-\nvance and diversity (Zhang et al., 2018b; Li et al.,\n2016a,b), we observe that relevance performance\nimproves with diversity performance in DLGNet\nmodels. It is worth pointing out, however, that DL-\nGNet models tend to generate shorter responses\nthan adversarially trained models (hredGAN and\naBoots). This indicates that the models still suf-\nfer from the impact of using only the maximum\nlikelihood training objective. Alleviating this prob-\nlem with an adversarial training objective similar\nto aBoots and or hredGAN should further improve\nperformance and will be considered in our future\nwork.\n6.2 Qualitative Evaluation\nRandom samples of the model outputs are shown\nin Tables 1 and 4. One striking observation is the\nhigh level of coherence in the generated responses\nfrom DLGNet models. The models are able to cap-\nture both short- and long-term temporal dependen-\ncies in their responses. The models give responses\nthat are relevant to the topic of the discussion, and\nare able to answer posed questions with answer\nchoices. Also, they don’t simply generate the all-\ntoo-common phrase “I’m not sure” like existing\nmodels; they are able to point to areas of the context\nthey are uncertain about (see the Ubuntu section of\nTable 1).\nFigure 3: Relevance vs. diversity tradeoff with top k\nsampling for DLGNet-345M models.\nFigure 4: Relevance vs. diversity tradeoff with top p\nsampling for DLGNet-345M models.\n59\n7 Ablation Studies on DLGNet Models\nwith Random Informative Padding\nIn this section, we carry out a more detailed analy-\nsis and discussion of different conﬁgurations of DL-\nGNet models as well as their performance across\ndatasets, using the evaluation results in Table 2.\n7.1 Open vs. Closed Domain Dataset\nFrom Table 2, we observe that the performance\nimprovement achieved by DLGNet models over\nexisting models is higher for the open-domain\nMovie Triples dataset than for the closed-domain\nUbuntu Dialogue dataset with or without pretrain-\ning. While the performance difference could be due\nto the size of the dataset, it could also indicate that\nclosed-domain dialogue responses are inherently\nmore difﬁcult to learn, even for large and expres-\nsive models such as the DLGNet transformer.\n7.2 Effect of Model Pretraining\nAlthough models with pretraining generally per-\nform better than ones trained with random initial-\nization, we observe that the performance difference\nis not signiﬁcant. This shows that the performance\nof the DLGNet is mostly due to the multi-layer self\nattention model architecture rather than the scaf-\nfolding achieved from language model pretraining.\nWe observe similar behavior across datasets. How-\never, pretraining seems to be consistently more\nhelpful for open-domain datasets versus closed-\ndomain datasets. This might be because the dis-\ntribution of the language data used for pretraining\nis similar to the open-domain dataset but different\nfrom the closed-domain dataset. Also, models with-\nout pretraining tend to generate longer responses\non average compare to those with pretraining. This\nindicates that model pretraining also plays a role in\nthe relevance-diversity tradeoff.\n7.3 Effect of Model Size\nWe also compare the small (DLGNet-117M) and\nlarge (DLGNet-345M) models. We observe that\nthere is a signiﬁcant performance improvement of\nthe larger over the smaller model on the Movie\ndataset (about 50%), but a smaller performance\nimprovement on the Ubuntu dataset. It’s also sur-\nprising that the larger model doesn’t overﬁt to the\nMovie dataset. Overﬁtting might have been pre-\nvented by the injection of random padding into the\ninput data, which regularizes the model training by\nartiﬁcially inducing high entropy into the data.\n7.4 Relevance vs. Diversity Tradeoff\nThe results in Table 2 show state-of-the-art rele-\nvance performance with some compromise on the\nresponse length. Here, we explore the possibility of\ngenerating longer and more diverse responses with\nthe trained models and estimate the effect on the\nrelevance scores. For this experiment, we chose the\nlarger DLGNet-345M models of both datasets and\ntried two sampling techniques, i.e., top k (Radford\net al., 2019) and top p nucleus (Holtzman et al.,\n2019; Zellers et al., 2019) sampling strategies on\nthe validation sets. The trajectory of the evalua-\ntion metrics with increasing top k and top p values\nare shown Figs. 3 and 4 respectively. With top k\nsampling, increasing the top k value increases the\nresponse length at the expense of relevance metrics\nlike BLEU for both datasets, as expected. However,\nthe response length increase is more signiﬁcant on\nthe Ubuntu dataset than the Movie dataset. It is\nalso surprising that the ROGUE-2 score for Ubuntu\nincreases with increasing top k value, which is the\nreverse of the case for the Movie dataset. Also,\nFig. 3 shows that it is more advantageous to trade\noff relevance for diversity on the Ubuntu dataset\ncompare to the Movie dataset. This is probably due\nto the size and closed-domain nature of the Ubuntu\ndataset, which makes it more difﬁcult to learn with\nthe maximum likelihood estimation only.\nWe observe a similar pattern with the top p nu-\ncleus sampling in Fig. 4. This reinforces the fact\nthat greedy sampling may be sufﬁcient for open-\ndomain datasets such as Movie.\n8 Further Ablation Studies on DLGNet\nModels\nWe also set out to analyze the features of DLGNet\nthat make it suitable for multi-turn dialogue mod-\neling. We train both DLGNet-117M and DLGNet-\n345M models on both datasets, but replace the\nrandom informative paddings with static paddings\nusing a pad token. Below are the deﬁnitions of the\nmodel conﬁguration factors considered:\n1.) Multi-turn Data (M): Training data is\nvariable-length multi-turn data padded to a ﬁxed\nlength. This helps to evaluate the effect of using\nrandom informative padding.\n2.) Single-turn Data (S): Training data is\nvariable-length single-turn data padded to a ﬁxed\nlength. This helps to evaluate the effect of number\nof turns.\n3.) Joint model (Joint): DLGNet models are\n60\nTable 3: Ablation Performance of DLGNet Models with Static Padding\nModel\nMovie Ubuntu\nRelevance Diversity Relevance Diversity\nBLEU ROUGE DIST-1/2 NASL BLEU ROUGE DIST-1/2 NASL\nDLGNet-117M\nS-Joint with BPE ∼0.0 ∼0.0 0.0400/0.1502 0.072 ∼0.0 0.0004 0.1946/0.4636 0.064\nS-Cond with BPE 0.0013 0.0296 0.0134/0.0482 3.582 ∼0.0 0.0083 0.0723/0.1470 0.890\nM-Joint with BPE 0.1825 0.1321 0.0346/0.0838 0.610 0.0012 0.1172 0.1719/0.3482 0.2937\nM-Cond with BPE 0.0096 0.0628 0.0088/0.0394 3.425 0.0048 0.0766 0.0500/0.1454 2.372\nM-Joint with Basic Tokenizer 0.0518 0.0630 0.0176/0.0540 1.101 0.0030 0.0384 0.0465/0.0949 0.566\nM-Cond with Basic Tokenizer 0.0149 0.1628 0.0394/0.1770 1.472 ∼0.0 0.0136 0.2211/0.4192 0.281\nDLGNet-345M\nS-Joint with BPE ∼0.0 ∼0.0 ∼0.0/∼0.0 0.072 ∼0.0 0.0006 0.4741/0.9760 0.061\nS-Cond with BPE 0.0006 0.0212 0.0010/0.0419 3.582 0.0004 0.0158 0.0721/0.1671 3.437\nM-Joint with BPE 0.0449 0.1931 0.0460/0.1273 0.531 ∼0.0 0.0121 0.3323/0.4406 0.227\nM-Cond with BPE 0.0010 0.0125 0.0091/0.0422 3.918 0.0004 0.0158 0.0721/0.1671 4.108\nM-Joint with Basic Tokenizer 0.0376 0.1389 0.0232/0.0654 0.543 0.0042 0.0341 0.0568/0.1299 0.552\nM-Cond with Basic Tokenizer 0.0057 0.0970 0.1568/0.3785 0.331 0.0015 0.0345 0.1555/0.3990 0.470\ntrained by jointly modeling the dialogue context\nand response.\n4.) Conditional model (Cond): DLGNet models\nare trained in the traditional sequence-to-sequence\nmode with a bidirectional encoder and an autore-\ngressive decoder for a conditional modeling of the\ndialogue response given the context (Vaswani et al.,\n2017; Zhang et al., 2019).\n5.) Basic Tokenizer: We use a basic tokeniza-\ntion traditionally used in dialogue modeling instead\nof BPE tokenization to evaluate the effect of tok-\nenization coverage. It also provides an apples-to-\napples comparison between the transformer-based\nand RNN-based architectures.\n8.1 Effect of Random Padding Injection\nThe results in Table 3 are from models trained with\nstatic paddings. The models perform signiﬁcantly\nworse than those of Table 2. Without random\npadding injection, the models quickly overﬁt to\nthe low entropy regions of the training data, which\nleads generic and/or short responses.\n8.2 Single Turn vs. Multi-turn\nWe also observe that the multi-turn models perform\nbetter than single-turn models on BPE tokenized\ndata. This is expected because the multi-turn mod-\nels capture longer temporal dependencies in the\ninput data. It is also worth mentioning that the\nsingle-turn performance is further hurt by BPE to-\nkenization since it tends to work better with long\ninput sequences.\n8.3 Joint vs. Conditional Models\nFor multi-turn models, the joint modeling archi-\ntecture yields better performance than the condi-\ntional Seq2Seq architecture. This trend is how-\never reversed for single-turn models. This is be-\ncause a model that focuses on jointly modeling\nboth the context and the response performs better\nwith longer contextual information compared to\na model that focuses on modeling only the condi-\ntional distribution of the response given the context.\nTherefore, multi-turn dialogue model should rather\nemploy the joint structure instead of the conditional\nSeq2Seq structure.\n8.4 Effect of Tokenization Coverage\nFor a more fair comparison with previous work on\nmulti-turn dialogue not using random padding in-\njection and 100% BPE tokenization, we trained the\nDLGNet models on multi-turn data with basic tok-\nenization. The tokenization coverages of the basic\ntokenizer used are 83.9% and 4.19% for Movie and\nUbuntu datasets respectively. Basically, most of the\nUbuntu tokens are mapped to the <UNK>token.\nIn comparison with previous work on HRED, the\nresults in Table 3 show that the transformer-based\nDLGNet models under the same conditions per-\nform better than the basic HRED model but worse\nthan the improved HRED models (such as VHRED,\nhredGAN, and aBoots). In comparison with other\ntransformer-based conﬁgurations, the smaller size\nmulti-turn models perform better than their BPE\ncounterparts but the larger size models perform\nworse. This is probably due to the overﬁtting of the\nlarger models.\n61\n9 Conclusion\nIn this paper, we have proposed DLGNet, an ex-\ntension of autoregressive transformer models such\nas GPT-2 for multi-turn dialogue modeling. Our\nexperiments show that DLGNet models perform\nbetter than existing state-of-the-art multi-turn dia-\nlogue models. They also achieve the best perfor-\nmance to date on open-domain Movie and closed-\ndomain Ubuntu datasets based on BLEU, ROUGE\nand distinct n-gram scores. Our experiments reveal\nthat the combination of (i) the transformer archi-\ntecture with (ii) the injection of random paddings\nexploiting the large maximum input sequence is\nresponsible for the performance improvement over\nexisting methods. Other contributing factors in-\nclude joint modeling of dialogue context and re-\nsponse, and the 100% tokenization coverage from\nthe byte pair encoding (BPE). Our analysis also\nreveals some tradeoffs between response relevance\nand response length, and we showed how differ-\nent sampling strategies can be used to make an\ninformed decision about such response relevance-\ndiversity compromises. In our future work, we plan\nto investigate how to improve on the length of the\ngenerated responses without necessarily sacriﬁcing\ntheir coherence and their relevance to the dialogue\ncontext.\nReferences\nZ. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and\nR. Salakhutdinov. 2019. Transformer-xl: Attentive\nlanguage models beyond a ﬁxed-length context. In\nACL.\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova. 2019.\nBert: Pre-training of deep bidirectional transformers\nfor language understanding. In NAACL-HLT.\nA. Holtzman, J. Buys, M. Forbes, and Y . Choi. 2019.\nThe curious case of neural text degeneration. In\narXiv preprint arXiv:1904.09751.\nJ. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan.\n2016a. A diversity-promoting objective function\nfor neural conversation models. In Proceedings of\nNAACL-HLT.\nJ. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and\nD. Jurafsky. 2016b. Deep reinforcement learn-\ning for dialogue generation. In arXiv preprint\narXiv:1606.01541v4.\nC. Y . Lin. 2014. Rouge: a package for automatic evalu-\nation of summaries. In Proceedings of the Workshop\non Text Summarization Branches Out.\nTable 4: Sample responses of DAIM, (V)HRED,\nhredGAN, aBoots and DLGNet models.\nModel Response\nMovie\nContext 0 we ’ re grateful , <person> . it ’ s not that . those\ncraftsman tools . they ’ re a dream come true .\nContext 1 your dad and i were raised with a certain work ethic , that\n’ s all .\nGroundtruth we can ’ t start living like we ’ re the rockerfellers or\nsomething .\nHRED <person> .\nVHRED <person> , mr . <person> . <person> , i ’ m not\ngoing to tell you , <person> . i ’ m sorry . i ’ m sorry ,\n<person> . i ’ m sorry\nhredGAN w <person> , i know you ’ re see .< continued utterance\n> <person> , please .\naBoots w cat you ’ re a liar !\nDLGNet-117M <person> , come on , hang on . how do you like that ?\ni can barely believe you ’ re doing this .\nDLGNet-345M i have some very bad dreams .\nDLGNet-117M Random i ’ m sorry about all this . <person> , this is all their\nhouse .\nDLGNet-345M Random remember that shoe you lost .\nContext 0 <person> i ’ ve raised my rate . i might even cut my\nsessions to forty minutes .\nContext 1 <person> , i know you don ’ t like ﬂattery but if any-\nbody i know is going to win a nobel prize , it ’ s you .\nyou got ta be thinking about your next book so i know\nyou ’ ll ﬁnd this case particularly interesting .\nGroundtruth what sort of case is it , <person> ?\nHRED i ’ m sorry , < person> .\nVHRED: it ’ s not a lot of <unk> . i ’ m not going to tell you ,\n<person> . i ’ m not going to help you .\nhredGAN w i ’ m a little nervous , but i don ’ t care if there ’ s some-\nthing more important than <person> .\naBoots w cat if i ’ m sure they ’ ll call it ’ s place <person> .\nDLGNet-117M what sort of case is it , <person> ?\nDLGNet-345M what sort of case is it , <person> ?\nDLGNet-117M Random what sort of case is your next ?\nDLGNet-345M Random like what ?\nContext 0 what was that for ?\nContext 1 i ’ d forgotten how exciting it is working together .\nGroundtruth you ’ ve been so quiet lately ,<person> , i thought you\nno longer cared .\nHRED i ’ m sorry , <person> .\nVHRED: <person> , <person> . i don ’ t know .\nhredGAN w i ’ m sorry , <person> . so many people are so inter-\nested , but you ’ re a very nice guy .\naBoots w cat we ’ re going to take a look ?\nDLGNet-117M you look like working with <person> .\nDLGNet-345M you ’ re a helluva translator .\nDLGNet-117M Random you ’ ve been so nice to <person> ach .\nDLGNet-345M Random oh .\nUbuntu\nContext 0 DJones: update manager won’t detect 12.04.1 as a new\nversion if you are already running 12.04, because 12.04.1\n= 12.04 + lots of package updates\nContext 1 DJones I am running 10.04.\nContext 2 what does ’gksu update-manager c’ report?\nGroundtruth does not report that there is a new version out. So I am\njust holding. No rush.\nHRED I don’t know what the problem is\nVHRED you mean ?\nhredGAN w yes , I get the new one , I just want to get the new one\naBoots w cat it says : no such ﬁle or directory , that is what it says\nDLGNet-117M gksu: command not found\nDLGNet-345M it says No new release found\nDLGNet-117M Random http://pastebin.com/m7e8e0e7c\nDLGNet-345M Random it says ”update-manager: error: no new version is avail-\nable”\nContext 0 libgnome2-dev is the package that has that pkgconﬁg ﬁle\nContext 1 After I installed libgnome2-dev I got a different output.\nhttp://pastebin.com/zpDeGs81 sorry it is so long.\nContext 2 I thought that was what was going on thanks. :) I ran\nthe command ./conﬁgure && sudo make && sudo make\ninstall would that have worked.\nGroundtruth you shouldn’t run: sudo make; but yes\nHRED I don’t know what the problem is\nVHRED what do you mean ? eou what are you trying ?\nhredGAN w I have no idea , I can just copy the ﬁles and copy ﬁles to\nthe other one\naBoots w cat yeah , if you have a few times to put things in the ’ make\n’ command it is not in a ﬁle you can install it by hand .\nDLGNet-117M no, that’s not what you want to do\nDLGNet-345M no, you need to run it as root\nDLGNet-117M Random no, it won’t.\nDLGNet-345M Random yes, that’s what I did\n62\nO. Olabiyi, A. Khazan, A. Salimov, and E.T. Mueller.\n2019a. An adversarial learning framework for\na persona-based multi-turn dialogue model. In\nNAACL NeuralGen Workshop.\nO. Olabiyi, E.T. Mueller, C. Larson, and T. Lahlou.\n2019b. Adversarial bootstrapping for di-\nalogue model training. In arXiv preprint\narXiv:1909.00925.\nO. Olabiyi, A. Salimov, A. Khazane, and E. Mueller.\n2018. Multi-turn dialogue response generation in an\nadversarial learning framework. In arXiv preprint\narXiv:1805.11752.\nK. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.\nBleu: A method for automatic evalution of machine\ntranslation. In Proceedings of the 40th Annual Meet-\ning of the Association for Computational Linguistics,\npages 311–318.\nA. Radford and T. Salimans. 2018. Im-\nproving language understanding by gener-\native pre-training. In https://s3-us-west-\n2.amazonaws.com/openai-assets/research-\ncovers/language-unsupervised/language\nunderstanding paper.pdf.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever. 2019. Language mod-\nels are unsupervised multitask learners. In\nhttps://d4mucfpksywv.cloudfront.net/better-\nlanguage-models.\nI. Serban, A. Sordoni, Y . Bengio, A. Courville, and\nJ. Pineau. 2016. Building end-to-end dialogue sys-\ntems using generative hierarchical neural network\nmodels. In Proceedings of The Thirtieth AAAI Con-\nference on Artiﬁcial Intelligence (AAAI 2016), pages\n3776–3784.\nI. V . Serban, T. Klinger, G. Tesauro, K. Talamadupula,\nB. Zhou, Y . Bengio, and A. Courville. 2017a. Mul-\ntiresolution recurrent neural networks: An applica-\ntion to dialogue response generation. In Proceed-\nings of The Thirty-ﬁrst AAAI Conference on Artiﬁ-\ncial Intelligence (AAAI 2017).\nI. V . Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau,\nA. Courville, and Y . Bengio. 2017b. A hierarchi-\ncal latent variable encoder-decoder model for gener-\nating dialogue. In Proceedings of The Thirty-ﬁrst\nAAAI Conference on Artiﬁcial Intelligence (AAAI\n2017).\nY . Sun, S. Wang, Y . Li, S. Feng, X. Chen, H. Zhang,\nX. Tian, D. Zhu, H. Tian, and H. Wu. 2019a. Ernie:\nEnhanced representation through knowledge integra-\ntion. In arXiv preprint arXiv:1904.09223.\nY . Sun, S. Wang, Y . Li, S. Feng, H. Tian, H. Wu, and\nH. Wang. 2019b. Ernie 2.0: A continual pre-training\nframework for language understanding. In arXiv\npreprint arXiv:1907.12412.\nI. Sutskever, O. Vinyals, and Q. Le. 2014. Sequence to\nsequence learning with neural networks. In Proceed-\nings of Advances in Neural Information Processing\nSystems (NIPS), pages 3104–3112.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\n2017. Attention is all you need. In NIPS.\nO. Vinyals and Q. Le. 2015. A neural conversational\nmodel. In Proceedings of ICML Deep Learning\nWorkshop.\nC. Xing, W. Wu, Y . Wu, M. Zhou, Y . Huang, and\nW. Ma. 2017. Hierarchical recurrent attention net-\nwork for response generation. In arXiv preprint\narXiv:1701.07149.\nZ. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdi-\nnov, and Q. V . Le. 2019. Xlnet: Generalized autore-\ngressive pretraining for language understanding. In\narXiv preprint arXiv:1906.08237.\nR. Zellers, A. Holtzman, H. Rashkin, Y . Bisk,\nA. Farhadi, F. Roesner, and Y . Choi. 2019. De-\nfending against neural fake news. In arXiv preprint\narXiv:1905.12616.\nH. Zhang, Y . Lan, L. Pang, J. Guo, and Xueqi Cheng.\n2019. Recosa: Detecting the relevant contexts with\nself-attention for multi-turn dialogue generation. In\narXiv preprint arXiv:1907.05339.\nS. Zhang, E. Dinan, J. Urbanek, A. Szlam, D. Kiela,\nand J. Weston. 2018a. Personalizing dialogue\nagents: I have a dog, do you have pets too? In arXiv\npreprint arXiv:1801.07243v3.\nY . Zhang, M. Galley, J. Gao, Z. Gan, X. Li, C. Brock-\nett, and B. Dolan. 2018b. Generating informative\nand diverse conversational responses via adversarial\ninformation maximization. In NeurIPS.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8055073022842407
    },
    {
      "name": "Transformer",
      "score": 0.7537678480148315
    },
    {
      "name": "Language model",
      "score": 0.7013230919837952
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48457038402557373
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.4484100043773651
    },
    {
      "name": "Machine learning",
      "score": 0.4202454686164856
    },
    {
      "name": "Context model",
      "score": 0.41760149598121643
    },
    {
      "name": "Natural language processing",
      "score": 0.410493403673172
    },
    {
      "name": "Speech recognition",
      "score": 0.34804266691207886
    },
    {
      "name": "Engineering",
      "score": 0.09843862056732178
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1305444813",
      "name": "Capital One (United States)",
      "country": "US"
    }
  ]
}