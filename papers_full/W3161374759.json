{
  "title": "Understanding by Understanding Not: Modeling Negation in Language Models",
  "url": "https://openalex.org/W3161374759",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2887423291",
      "name": "Arian Hosseini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2169793708",
      "name": "Siva Reddy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2509101809",
      "name": "Dzmitry Bahdanau",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2638394809",
      "name": "R Devon Hjelm",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A65685553",
      "name": "Alessandro Sordoni",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2328522601",
      "name": "Aaron Courville",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3034779619",
    "https://openalex.org/W3025064182",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W3035352537",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2995404354",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2909970382",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W2509019445",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2740027944",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W2027155840",
    "https://openalex.org/W3035597164",
    "https://openalex.org/W2252095625",
    "https://openalex.org/W2105827650",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W2799112216",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4288623406",
    "https://openalex.org/W2964117978",
    "https://openalex.org/W3035331128",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2936377323",
    "https://openalex.org/W3099843385",
    "https://openalex.org/W2970950077",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2508309896",
    "https://openalex.org/W2911321984",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W3035221519",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2131687582"
  ],
  "abstract": "Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, R Devon Hjelm, Alessandro Sordoni, Aaron Courville. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 1301–1312\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n1301\nUnderstanding by Understanding Not:\nModeling Negation in Language Models\nArian Hosseini\nMila/Université de Montréal\nMontréal, Canada\narian.hosseini9@gmail.com\nSiva Reddy\nMila/McGill University\nMontréal, Canada\nDzmitry Bahdanau\nElement AI\na ServiceNow Company\nMontréal, Canada\nR Devon Hjelm\nMila/Université de Montréal\nand Microsoft Research\nMontréal, Canada\nAlessandro Sordoni\nMicrosoft Research\nMontréal, Canada\nAaron Courville\nMila/Université de Montréal\nMontréal, Canada\nAbstract\nNegation is a core construction in natural\nlanguage. Despite being very successful on\nmany tasks, state-of-the-art pre-trained lan-\nguage models often handle negation incor-\nrectly. To improve language models in this\nregard, we propose to augment the language\nmodeling objective with an unlikelihood objec-\ntive that is based on negated generic sentences\nfrom a raw text corpus. By training BERT with\nthe resulting combined objective we reduce the\nmean top 1 error rate to 4% on the negated\nLAMA dataset. We also see some improve-\nments on the negated NLI benchmarks.\n1 Introduction\nNegation is an important property in many lan-\nguage understanding tasks, such as sentiment anal-\nysis, question answering, knowledge base com-\npletion and natural language inference (Kassner\nand Schütze, 2019; Naik et al., 2018). While Pre-\ntrained Language Models (PLMs) such as BERT\npushed the state-of-the-art on these tasks (Devlin\net al., 2019; Petroni et al., 2019), they fail dra-\nmatically on instances that require understanding\nnegation.\nKassner and Schütze (2019) show that current\nPLMs cannot correctly distinguish between the\nnegated and non-negated forms of ﬁll-in-the-blank\ntests. For instance, when asked to predict the\n[MASK] token in sentences such as “ The capi-\ntal of Cuba is [MASK]” and “The capital of Cuba\nis not [MASK] ”, BERT often generate the same\nanswer “Havana”, indicating that it may not be\nappropriately modeling the distribution of negative\nsentences. Additional evidence is given by the fact\nthat, when ﬁne-tuned on natural language inference\ntasks, PLMs tend to mis-classify examples which\nFigure 1: An overview of the unlikelihood objective.\nA generic sentence is negated using our data aug-\nmentation method and an unlikelihood token is cho-\nsen and replaced with [MASK]. This new sentence is\nconcatenated with the original sentence and fed into\nthe model. The unlikelihood loss is computed using\np(improvements) from the language modeling head of\nBERT.\ncontain not or no as contradiction when the true\nlabel is neutral or entailment (Naik et al., 2018). Re-\ncently, Hossain et al. (2020b) proposed new natural\nlanguage inference test sets to speciﬁcally target\nthe model’s understanding of negation and show\nthat current state-of-the-art models perform poorly\non these test sets.\nIn this work, we investigate whether we can\nalleviate the modeling bias of PLMs on negated\nsentences. Our approach is composed of two\ncore contributions: i) a syntactic data augmenta-\ntion scheme to automatically generate negated sen-\ntences; ii) a new training paradigm, dubbed unlike-\nlihood training with reference(Fig. 1), based on the\nrecently proposed unlikelihood training (Welleck\n1302\net al., 2020).\nAt ﬁrst, we generate a large number of negated\nsentences by negating sentences mined from an\nopenly available text corpus (Wikipedia). Our sen-\ntence negator uses the dependency parse of the\nsentence, part of speech tags, and morphological\nfeatures of each word in the sentence and deter-\nministically negates the sentence. Given a negated\nversion of a sentence, we replace its object with\nthe [MASK] token and use unlikelihood training\nto make the object unlikely under the PLM distri-\nbution (e.g. we minimize the probability of “ im-\nprovements” as depicted in Fig. 1). Importantly, in\norder to ensure that the negated sentence is factu-\nally false, we use the positive sentence as context\n(i.e., as a reference) for the unlikelihood prediction\ntask. Concretely, we provide the concatenation of\nthe positive and the masked negated sentence as\ninput to the PLM. Our method can be thought of a\ntype data augmentation, which has be shown to be\neffective at improving robustness across many tasks\nin language, such as text classiﬁcation (Wei and\nZou, 2019), natural language inference (Min et al.,\n2020; McCoy et al., 2019) and semantic parsing\n(Andreas, 2019).\nFor our negation experiments, we ﬁne-tune pre-\ntrained BERT with our new objective and a knowl-\nedge distillation objective. We test our model on\nthe negated LAMA dataset (Kassner and Schütze,\n2019), which is the negated version of knowledge\nprobing dataset LAMA, introduced in Petroni et al.\n(2019). Our model achieves a mean error rate of 4%\n(a improvement of 5 points) on the negated LAMA\ndataset while maintaining the performance on the\noriginal LAMA dataset without any direct training\non the negated LAMA sentences. We also ﬁne-\ntune BERT on RTE (Dagan et al., 2005; Bar-Haim\net al., 2006; Giampiccolo et al., 2007; Bentivogli\net al., 2009), SNLI (Bowman et al., 2015) and\nMNLI (Williams et al., 2018) tasks and achieve\nbetter results on the language inference benchmark\nincluding negation from (Hossain et al., 2020b).\n2 Related Work\nPre-trained language models have shown impres-\nsive results across many tasks, such as question an-\nswering (Alberti et al., 2019) and natural language\ninference (Liu et al., 2019). These models are also\nknown to encode factual and common-sense knowl-\nedge (Radford et al., 2019; Petroni et al., 2019;\nBosselut et al., 2019). Despite these abilities, Kass-\nner and Schütze (2019) found that these models\nfail at understanding negation through analysing\nnegated factual statements.\nExtensive literature looks at the linguistic knowl-\nedge learned by language models (McCoy et al.,\n2019; Jumelet and Hupkes, 2018; Gulordava et al.,\n2018; Marvin and Linzen, 2018; Tenney et al.,\n2019; Warstadt and Bowman, 2019; Talmor et al.,\n2019). Recent work has also studied the short-\ncomings in negation scope detection (Jumelet and\nHupkes, 2018; Fancellu et al., 2016, 2017; Morante\nand Daelemans, 2009; Li and Lu, 2018; Zhao and\nBethard, 2020; Chen, 2019) and focus detection\n(Shen et al., 2019; Zou et al., 2014, 2015; Hossain\net al., 2020a). Naik et al. (2018) and McCoy et al.\n(2019) systematically study the linguistic abilities\nof these models using NLI, and show that these\nmodels rely on erroneous syntactic heuristics. Our\nwork is in this spirit for negations.\nNoji and Takamura (2020) propose taking advan-\ntage of negative examples and unlikelihood in the\ntraining of language models to increase their syn-\ntactic abilities. Similarly, Min et al. (2020) show\nthe effectiveness of syntactic data augmentation\nin the case of robustness in NLI. Neither of these\nworks focus on negations.\n3 Syntactic Negation Augmentation\nWe generate the negated versions of sentences us-\ning a syntactic augmentation method. The method\ngets as input the dependency parse of the sentence,\nPOS tags and morphological information of each\nword and negates the sentence using a set of rules.\nEach rule has a dependency tree regular expres-\nsion pattern (Semgrex; Chambers et al. 2007). We\nuse Semgrex patterns to identify different syntactic\ntemplates, and then transform the sentence based\non a list of actions deﬁned in the rule. These ac-\ntions can be move, replace, insert and lemmatize.\nThe unlikelihood token which will be discussed\nlater is also chosen using Semgrex patterns (see\nAppendix C for some examples).\nWe use Stanza (Qi et al., 2020) to get the de-\npendency parse of the sentences, parts of speech\ntags, lemma, and morphological features of the\nwords. We also ﬁlter out sentences with more than\n20 words.\nTo test the coverage of our Semgrex patterns, we\nrandomly sampled 930 sentences from Wikipedia.\nOnly 31 of them did not match any of our Semgrex\npatterns (See table 8 in Appendix B for the number\n1303\nModel SQuAD ConceptNet T-REx Google-RE\nBERT 13.53 15.65 29.10 10.24\nBERT + KL 13.64 15.64 29.28 10.27\nBERTNOT 13.97 15.49 29.25 10.31\nTable 1: Mean precision at k = 1 (p @ 1 ) for original LAMA queries (higher is better) of pre-trained BERT,\nBERT trained with distillation objective, and BERT with unlikelihood and distillation objectives (BERTNOT, sec\n4.2). The scores are averaged across 3 runs.\nModel SQuAD ConceptNet T-REx Google-RE\nBERT 8.61 2.24 21.42 3.76\nBERT + KL 4.97 1.19 21.77 3.99\nBERTNOT 2.10 0.73 11.86 1.10\nTable 2: Mean top 1 error rate for negated LAMA queries (lower is better) of pre-trained BERT, BERT trained with\ndistillation objective, and BERT with unlikelihood and distillation objectives (BERTNOT, sec 4.2). The scores are\naveraged across 3 runs.\nof matches for each rule in our rule set for these 930\nsentences). In addition, to get a better sense of the\ncorrectness of our method, 100 random sentences\n(from Wikipedia) were negated and reviewed by\na native English speaker. The precision for these\nnegations is 94.00%. Table 7 in Appendix B shows\nexamples of original and negated sentences.\n4 Unlikelihood Training With Reference\n4.1 Reference setup\nApplying unlikelihood to a word in any random sen-\ntence is problematic, unless the sentence is a factual\nstatement (e.g. unlikelihood on improvements in\n“He did not advocate navigationalimprovements\non the Sangamon River.” in Fig 1 is problematic as\nthis sentence is not grounded in reality). Moreover,\nusing solely factual sentences limits the application\nof this method.1 To be able to use any generic (not\nnecessarily factual) sentence and pick an unlike-\nlihood token in it, there needs to be some sort of\ngrounding or context. In this setup, each training\nexample is of the form <sentence A, sentence B>\nwhere sentence A is the reference for sentence B,\nand provides the grounding or context for it.\n4.2 Unlikelihood and knowledge distillation\nThe unlikelihood loss has recently been proposed\nby Welleck et al. (2020) to mitigate the problem\nof repetition in neural text generation. Noji and\nTakamura (2020) also adopted this loss to penalize\nthe desirability of an incorrect token in a sentence.\n1We did try to apply unlikelihood without any context or\nreference, but as expected it performed poorly for both LAMA\nand negated LAMA. See appendix E.\nWe adopt this method to penalize the likelihood\nof a token in sentence B that makes this sentence\ncontradictory with the reference sentence A.\n(1) A Humans have a rational soul.\nB Humans do not have a rational soul.\nIn the example 1, assuming that sentence A is true,\nwe want the model to avoid assigning “soul” in\nsentence B a high probability. To this end, the\nprobability of the unlikelihood token xu = “soul”\nis penalized with the unlikelihood loss LUL as:\nLUL (xu) =−log(1 −p(xu|x1:T )), (1)\nwhere x1:T is the whole input sequence (sentence A\nconcatenated with sentence B which is the negated\nversion of sentence A as illustrated in Fig 1). To\nhave a balanced augmentation data set, we also\ninclude examples where sentence B is the copy of\nsentence A and therefore not contradictory with it.\nIn this context, we want the model to perform as it\nwas untouched (before any ﬁne-tuning). The KL\ndivergence knowledge distillation loss is used for\nthese examples on the same token:\n(2) A Humans have a rational soul.\nB Humans have a rational [MASK].\nThe loss LKL for token xl = “[MASK]”is written\nas:\nLKL(xl) =DKL(pLM ||p) (2)\nwhere pLM is the probability distribution over the\nvocabulary for the masked token xl under the LM\nbefore any ﬁne-tuning.\nIn our experiments, we use the BERT-base model\nand further train it with two objectives, the un-\nlikelihood objective (Eq. 1) and the knowledge\n1304\nQuery Top 3 words with log probs from BERT Top 3 words with log probs from BERTNOT\niOS is developed by [MASK]. Apple (-1.8), Google (-2.6), Microsoft (-2.8) Apple (-1.8), Google (-2.5), Microsoft (-2.7)\niOS is not developed by [MASK]. Apple (-1.8), Google (-2.6), Microsoft (-2.8) Microsoft (-1.8), Google (-2.4), Apple (-3.1)\nThe majority of the amazon forest is in [MASK]. Brazil (-2.6), Bolivia (-2.7), Madagascar (-3.1) Brazil (-2.9), Bolivia (-3.1), Mexico (-3.2)\nThe majority of the amazon forest is not in [MASK]. cultivation (-1.0), Brazil (-3.5), Mexico (-3.5) cultivation (-2.0), Mexico (-4.1), France (-4.3)\nCharles Nodier died in [MASK]. Paris (-1.35), Rome (-3.2), ofﬁce (-3.4) Paris (-1.5), Rome (-3.3), France (-3.6)\nCharles Nodier did not die in [MASK]. Paris (-2.4), ofﬁce (-2.7), France (-2.8) vain (-3.5), error (-4.0), doubt (-4.5)\nMac OS is developed by [MASK]. Apple (-1.9), Microsoft (-2.0), Intel (-2.0) Apple (-2.0), Microsoft (-2.0), Intel (-2.1)\nMac OS is not developed by [MASK]. Apple (-1.3), Microsoft (-1.5), IBM (-2.3) Microsoft (-2.1), IBM (-2.7), itself (-3.4)\nTable 3: Examples from BERT base before and after training it with the unlikelihood (UL) and KL divergence\nknowledge distillation (KL) objectives (BERTNOT). Queries are from LAMA and negated LAMA.\ndistillation objective (Eq. 2). We also use origi-\nnal Wikipedia sentences for the latter to prevent\ncatastrophic forgetting of language modeling. The\nprobability of the unlikelihood token p(xu|x1:T )\nand the distribution for masked token xl are com-\nputed using the language modeling head of the\nBERT model by replacing xu and xl in the input\nsequences with the [MASK] token. Examples for\neach objective are sampled uniformly. We will\nrefer to our model as BERTNOT.\n5 Experiments\nWe report our main results on LAMA and Negated\nLAMA for knowledge base completion. The cloze\nstatements from LAMA are facts or commonsense\nknowledge generated from either subject-relation-\nobject triples (X, rel, Y) or question-answers pairs.\nThe cloze statements for the triples are generated\nusing a template for each relation which includes\nthe placeholders X and Y (e.g. “X is located in\nY”). X is replaced for the subject and Y is re-\nplaced with the [MASK] token to be predicted by\nthe model. In the question-answer pairs, the an-\nswer is replaced with [MASK] token. The facts\nin the LAMA dataset are from multiple sources:\n1) Google-RE relations, namely “place of birth”,\n“date of birth” and “place of death”; 2) T-REx, a\nsubset of Wikidata triples with 41 relations (ElSa-\nhar et al., 2018); 3) ConceptNet with 16 relations\n(Li et al., 2016); 4) SQuAD, a subset of 305 context-\ninsensitive questions manually rephrased as cloze-\nstyle questions (Rajpurkar et al., 2016). Negated\nLAMA was created by manually negating the tem-\nplates or questions (Kassner and Schütze, 2019).\nFollowing Petroni et al. (2019) we use mean preci-\nsion at k(P @ k) for LAMA. For negated LAMA\nwe report mean top 1 error rate.\n5.1 Knowledge Base Completion\nAs discussed in section 4.2, we train a pre-trained\nBERT base cased model for 5 epochs, with 20k\nexamples for each objective, a maximum sequence\nlength of 128 and a learning rate of 1e-5. To see the\neffects of the unlikelihood objective more clearly,\nwe also train a pre-trained BERT base cased model\nwith only the KL knowledge distillation objective\nwith the same data and hyper-parameters.\nTables 1 and 2 respectively show the mean pre-\ncision at rank 1 (averaged over all the relations)\nfor LAMA, and mean top 1 error rate for negated\nLAMA queries. 2 The mean error rate on the\nnegated LAMA queries decreases to below 4%\nwhile the results on original LAMA stay the same.\nThese results are achieved without any direct train-\ning on LAMA queries (negated or non-negated).\nTable 3 shows the top 3 predicted words for a pre-\ntrained BERT model and the model trained with\nour method. Pre-trained BERT seems to ignore\nnegation and mostly predict based on the subject\nof the query, but the prediction probability in the\nnegated queries seems to be generally lower. Our\nmethod is as good as the vanilla model (BERT)\non original queries. For the negated queries, our\nmodel predictions are far-superior than the vanilla\nmodel. We also tried out method on BERT-large.\nSee appendix E for results and discussion.\n5.2 Natural Language Inference\nWe ﬁne-tune our model with a language inference\nobjective on RTE, SNLI and MNLI tasks. Table 4\nshows the accuracies on the original development\nsplits and the new splits from Hossain et al. (2020b)\ncontaining negation for each task. We used the\nhyper-parameters from Hossain et al. (2020b) to\nﬁne-tune all of our models.\n2Baseline scores differ slightly from Petroni et al. (2019).\nWe were unable to get the same results with their code.\n1305\nModel RTE SNLI MNLI\ndev w/neg dev w/neg dev w/neg\nBERT 70.04±1.57 65.47±3.63 89.47±0.18 44.18±0.67 82.95±0.18 60.62±1.32\nBERTNOT 69.68±1.88 74.47±0.29 89.00±0.10 45.96±0.41 84.31±2.29 60.89±0.31\nTable 4: Accuracies on original development splits (dev) and new splits containing negation from Hossain et al.\n(2020b) (w/neg) for RTE, SNLI and MNLI (matched genres) tasks. Results are averaged across 3 runs.\nPremise Hypothesis T B BN\n1 It does not use the ﬁrst day of the ﬁrst month of\nthe Lunar Year as the start of the Chinese New\nYear.\nThe Chinese New Year’s Day falls on the ﬁrst day\nof the ﬁrst month of the Lunar Year.\nN E N\n2 The prosecutor told the court that the incident had\ncaused \"distress\" to one of the children.\nThe prosecutor did not tell the court that \"distress\"\nin one of the children is associated with the inci-\ndent.\nN E N\n3 Green cards are not becoming more difﬁcult to\nobtain.\nGreen card is now difﬁcult to receive. N E N\n4 Moog’s synthesiser, which bears his name, revolu-\ntionised music from the 1960s onwards, and was\nused by bands like The Beatles and The Doors.\nMoog’s instruments were not used by The Beatles\nand The Doors among others.\nN N E\n5 The board of Marks & Spencer will not take an-\nother look at Philip Green’s increased takeover\noffer.\nPhilip Green does not try to take over Marks &\nSpencer.\nE E N\n6 Albert Sabin developed an oral, attenuated (live)\nvaccine, which, with Salk’s discovery, did not\nbring polio under control.\nPolio is not under control in the world. E E N\nTable 5: Examples from the new split from Hossain et al. (2020b) containing negation for RTE. T, B and BN\ndenote true label, BERT’s prediction and BERTNOT’s prediction respectively. E and N are used for entailment\nand not entailment labels.\nOur model achieves superior results on RTE\n(low-resource setting) and slightly better accuracies\non SNLI and MNLI (high-resource setting) on all\nthe new splits containing negation, while keeping\nroughly the same scores on the original dev splits.\nWe conjecture that ﬁne-tuning on large-amounts\nof data (SNLI and MNLI) may have resulted in\ncatastrophic forgetting of the negation knowledge,\ndecreasing the gap between BERT and BERTNOT.\nWe tried to alleviate the catastrophic forgetting by\nmixing in some unlikelihood training and knowl-\nedge distillation along the NLI training, but that\ndid not help. You can see these results for MNLI\nin appendix D. We leave further exploration of\nbetter ﬁne-tuning objectives while preserving the\npretrained knowledge for future work.\nTable 5 shows some of the examples of the new\nRTE split containing negation from Hossain et al.\n(2020b), along with the predictions from BERT\nand BERTNOT. Examples 4 and 6 show the failure\ncases of BERTNOT. As it can be seen, for the ﬁfth\nexample, the true label is incorrect, but BERTNOT\npredicts the correct label for this pair of premise\nand hypothesis.\n6 Conclusion\nIn this work, we propose a combination of the un-\nlikelihood objective with a reference based setup\nfor input sentences to model negation. This al-\nlows us to utilize generic sentences, and negate\nthem with our data augmentation method to be\nused as examples for the unlikelihood objective.\nOur method notably improves the error rate on the\nnegated LAMA dataset while keeping the same\nperformance on the original LAMA queries.\nWe also test our method on the original devel-\nopment sets and new splits containing negation\nfrom Hossain et al. (2020b) of RTE, SNLI and\nMNLI tasks. We see large improvements on the\nnegated splits in low-resource setting (RTE) and\nslight improvements in high-resource setting (SNLI\nand MNLI), while also maintaining similar results\nas BERT on original splits.\n1306\nReferences\nChris Alberti, Kenton Lee, and Michael Collins. 2019.\nA BERT baseline for the natural questions. CoRR,\nabs/1901.08634.\nJacob Andreas. 2019. Good-enough compositional\ndata augmentation. CoRR, abs/1904.09545.\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nand Danilo Giampiccolo. 2006. The second pascal\nrecognising textual entailment challenge. Proceed-\nings of the Second PASCAL Challenges Workshop\non Recognising Textual Entailment.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. B.: The ﬁfth pascal recognizing\ntextual entailment challenge. In Proceedings of TAC\n9.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 4762–4779,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nNathanael Chambers, Daniel M. Cer, Trond Grenager,\nDavid Hall, Chloé Kiddon, Bill MacCartney, Marie-\nCatherine de Marneffe, Daniel Ramage, Eric Yeh,\nand Christopher D. Manning. 2007. Learning\nalignments and leveraging natural logic. In ACL-\nPASCAL@ACL, pages 165–170. Association for\nComputational Linguistics.\nLong Chen. 2019. Attention-based deep learning sys-\ntem for negation and assertion detection in clini-\ncal notes. International Journal of Artiﬁcial Intel-\nligence & Applications, 10:1–9.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entail-\nment challenge. In Machine Learning Challenges,\nEvaluating Predictive Uncertainty, Visual Object\nClassiﬁcation and Recognizing Textual Entailment,\nFirst PASCAL Machine Learning Challenges Work-\nshop, MLCW 2005, Southampton, UK, April 11-\n13, 2005, Revised Selected Papers , volume 3944 of\nLecture Notes in Computer Science, pages 177–190.\nSpringer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT (1), pages 4171–4186. As-\nsociation for Computational Linguistics.\nHady ElSahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon S. Hare, Frédérique\nLaforest, and Elena Simperl. 2018. T-rex: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Inter-\nnational Conference on Language Resources and\nEvaluation, LREC 2018, Miyazaki, Japan, May 7-\n12, 2018 . European Language Resources Associa-\ntion (ELRA).\nFederico Fancellu, Adam Lopez, and Bonnie Webber.\n2016. Neural networks for negation scope detection.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 495–504, Berlin, Germany.\nAssociation for Computational Linguistics.\nFederico Fancellu, Adam Lopez, Bonnie Webber, and\nHangfeng He. 2017. Detecting negation scope is\neasy, except when it isn’t. InProceedings of the 15th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 2, Short\nPapers, pages 58–63, Valencia, Spain. Association\nfor Computational Linguistics.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recogniz-\ning textual entailment challenge. In Proceedings of\nthe ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing, pages 1–9, Prague. Association\nfor Computational Linguistics.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1195–1205, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nMd Mosharaf Hossain, Kathleen Hamilton, Alexis\nPalmer, and Eduardo Blanco. 2020a. Predicting the\nfocus of negation: Model and error analysis. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8389–\n8401, Online. Association for Computational Lin-\nguistics.\nMd Mosharaf Hossain, Venelin Kovatchev, Pranoy\nDutta, Tiffany Kao, Elizabeth Wei, and Eduardo\nBlanco. 2020b. An analysis of natural language in-\nference benchmarks through the lens of negation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020 , pages 9106–\n9118. Association for Computational Linguistics.\nJaap Jumelet and Dieuwke Hupkes. 2018. Do lan-\nguage models understand anything? on the ability\nof LSTMs to understand negative polarity items. In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 222–231, Brussels, Belgium.\nAssociation for Computational Linguistics.\n1307\nNora Kassner and Hinrich Schütze. 2019. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot ﬂy.\nHao Li and Wei Lu. 2018. Learning with structured\nrepresentations for negation scope extraction. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers) , pages 533–539, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nXiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gim-\npel. 2016. Commonsense knowledge base comple-\ntion. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2016, August 7-12, 2016, Berlin, Germany, Volume\n1: Long Papers. The Association for Computer Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 3428–3448. Association for Computa-\ntional Linguistics.\nJunghyun Min, R. Thomas McCoy, Dipanjan Das,\nEmily Pitler, and Tal Linzen. 2020. Syntactic\ndata augmentation increases robustness to inference\nheuristics. CoRR, abs/2004.11999.\nRoser Morante and Walter Daelemans. 2009. A met-\nalearning approach to processing the scope of nega-\ntion. In Proceedings of the Thirteenth Confer-\nence on Computational Natural Language Learning,\nCoNLL 2009, Boulder, Colorado, USA, June 4-5,\n2009, pages 21–29. ACL.\nAakanksha Naik, Abhilasha Ravichander, Norman M.\nSadeh, Carolyn Penstein Rosé, and Graham Neubig.\n2018. Stress test evaluation for natural language in-\nference. In COLING, pages 2340–2353. Associa-\ntion for Computational Linguistics.\nHiroshi Noji and Hiroya Takamura. 2020. An analy-\nsis of the utility of explicit negative examples to im-\nprove the syntactic abilities of neural language mod-\nels. CoRR, abs/2004.02451.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language models\nas knowledge bases? In EMNLP/IJCNLP (1), pages\n2463–2473. Association for Computational Linguis-\ntics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D. Manning. 2020. Stanza: A\npython natural language processing toolkit for many\nhuman languages. CoRR, abs/2003.07082.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383–2392.\nThe Association for Computational Linguistics.\nLongxiang Shen, Bowei Zou, Yu Hong, Guodong\nZhou, Qiaoming Zhu, and AiTi Aw. 2019. Negative\nfocus detection via contextual attention mechanism.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019 , pages 2251–\n2261. Association for Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2019. olmpics - on what\nlanguage model pre-training captures. CoRR,\nabs/1912.13283.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. In Asso-\nciation for Computational Linguistics.\nAlex Warstadt and Samuel R. Bowman. 2019.\nGrammatical analysis of pretrained sentence en-\ncoders with acceptability judgments. CoRR,\nabs/1901.03438.\nJason W. Wei and Kai Zou. 2019. EDA: easy data\naugmentation techniques for boosting performance\non text classiﬁcation tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 6381–6387. Association for\nComputational Linguistics.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In\nICLR. OpenReview.net.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\n1308\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nYiyun Zhao and Steven Bethard. 2020. How does\nBERT’s attention change when you ﬁne-tune? an\nanalysis methodology and a case study in negation\nscope. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4729–4747, Online. Association for Computa-\ntional Linguistics.\nBowei Zou, Guodong Zhou, and Qiaoming Zhu. 2014.\nNegation focus identiﬁcation with contextual dis-\ncourse information. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2014, June 22-27, 2014, Baltimore,\nMD, USA, Volume 1: Long Papers , pages 522–530.\nThe Association for Computer Linguistics.\nBowei Zou, Guodong Zhou, and Qiaoming Zhu. 2015.\nUnsupervised negation focus identiﬁcation with\nword-topic graph model. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2015, Lisbon, Portugal,\nSeptember 17-21, 2015 , pages 1632–1636. The As-\nsociation for Computational Linguistics.\n1309\nA Training details\nHere are the hyper-parameters used in our ﬁne-tunings.\nTask Epochs Batch Size Learning Rate Weight Decay\nUnlikelihood training 5 32 1e-5 N\\A\nRTE 50 32 2e-5 N\\A\nSNLI 3 32 1e-5 0.1\nMNLI 3 32 2e-5 N\\A\nTable 6: Hyper-parameters\nAlgorithm 1 shows the details of further training the BERT base cased model with the unlikelihood and\nknowledge distillation objectives.\nnumber of training steps : T\nfor i←1 to T do\nLUL ←compute unlikelihood loss with contradictory <sentence A, sentence B>pairs;\nLKL ←compute knowledge distillation loss with non-contradictory <sentence A, sentence B>pairs;\ng1 ←compute gradient of γLUL + (1−γ)LKL;\nupdate the parameters with g1;\nLKL ←compute knowledge distillation loss with sentences from Wikipedia;\ng2 ←compute gradient of LKL;\nupdate the parameters with g2;\nend\nAlgorithm 1:Details of the training procedure of BERTNOT. The unlikelihood loss and knowledge\ndistillation loss are ﬁrst computed with the <sentence A, sentence B> inputs. These inputs are\ncontradictory for the UL loss, and non-contradictory for knowledge distillation (sec 4.2). We use\nγ = 0.4 in our experiments to sum these losses and compute the gradient g1. Then, we compute the\nknowledge distillation loss for inputs sampled from Wikipedia. These inputs do not have our reference\nbased format. The parameters are updated again using the gradient from this knowledge distillation\nloss (g2).\n1310\nB Examples of negated sentences\nHere are some examples and details of our syntactic negation method.\nOriginal Negated Unlikelihood Token\n1 That tournament helped demon-\nstrate the high caliber of play in\nwomen’s soccer.\nThat tournament did not help\ndemonstrate the high caliber of\nplay in women’s soccer.\ntournament\n2 The attributes of this vector (length\nand direction) characterize the ro-\ntation at that point.\nThe attributes of this vector (length\nand direction) do not characterize\nthe rotation at that point.\nrotation\n3 This was broadcast live on Nor-\nway's main national TV carrier\nNRK.\nThis was not broadcast live on\nNorway's main national TV carrier\nNRK.\nNorway\n4 The latter may occur implicitly\nthrough the use of a construct like\nDEFV AR or DEFPARAMETER.\nThe latter may not occur implicitly\nthrough the use of a construct like\nDEFV AR or DEFPARAMETER.\nlatter\n5 When Arjuna was ﬁghting Karna,\nthe latter 's chariot 's wheels sank\ninto the ground.\nWhen Arjuna was ﬁghting Karna,\nthe latter's chariot's wheels did not\nsank into the ground.\nwheels\n6 It also prohibits or restricts the use\nof certain accounts held at ﬁnancial\ninstitutions.\nIt also does not prohibit or restricts\nthe use of certain accounts held at\nﬁnancial institutions.\nuse\nTable 7: Examples of original and negated sentences with the chosen unlikelihood token. Examples 5 and 6 are\nincorrect negations since sank in example 5 and restricts in example 6 are incorrect word forms in the negated\ncontext.\nRule Name # of Sentences Matched\nsimple past 315\nsimple present 295\nImperative 93\npresent with auxiliary verb 37\npast perfect 35\ncopula statements 34\npresent with modal 24\nalready negated with not 14\nNPI words (anywhere, anyone, etc) 5\nnegative words (no, nobody, etc) 4\nother 13\nTable 8: Number of matches for each rule in our rule set over 930 sentences used to analyze the syntactic negation.\n1311\nC Example rules for transforming a sentence into its negation\nOriginal Sentence Rule Negated Sentence\nNowhere in his confession did\nhe mention the Monteagle letter. {\n\" name \" : \" aux b e f o r e s u b j \" ,\n\" p a t t e r n \" : \" {$ ; t a g : /VB . * / }=A >/\nadvmod | cc / { word : / n e v e r | nobody |\nno | n o t h i n g | nowhere | n e i t h e r |\nNever | Nobody | No | Nothing | Nowhere\n| N e i t h e r / }=npiword >/ aux . * / ( { }=\nB $++ { }= s u b j e c t ) >/ n s u b j . * / {\n}= s u b j e c t ?> o b j { t a g : /NN. * / }=\nobject\" ,\n\" a c t i o n s \" : [\n{\n\" t y p e \" : \" move \" ,\n\" to_move \" : \" B\" ,\n\" a n c h o r \" : \"A\" ,\n\" p o s i t i o n \" : \" b e f o r e \"\n} ,\n{\n\" t y p e \" : \" r e p l a c e \" ,\n\" t o k e n \" : \" \" ,\n\" t o _ r e p l a c e \" : \"npiword\"\n}\n]\n}\nin his confession he did mention\nthe Monteagle letter.\nMany fonts then made the right\nleg vertical. {\n\" name \" : \" s i m p l e p a s t \" ,\n\" p a t t e r n \" : \" {$ ; cpos : / . * Tense = P a s t . * /\n}=A >/ n s u b j | c s u b j / =E { }= s u b j e c t\n?> o b j { t a g : /NN. * / }=object\" ,\n\" a c t i o n s \" : [\n{\n\" t y p e \" : \" i n s e r t \" ,\n\" t o k e n \" : \" d i d \" ,\n\" r e l \" : \"AUX\" ,\n\" a n c h o r \" : \"A\" ,\n\" p o s i t i o n \" : \" b e f o r e \"\n} ,\n{\n\" t y p e \" : \" i n s e r t \" ,\n\" t o k e n \" : \" n o t \" ,\n\" r e l \" : \"ADV\" ,\n\" a n c h o r \" : \"A\" ,\n\" p o s i t i o n \" : \" b e f o r e \"\n} ,\n{\n\" t y p e \" : \" l e m m a t i z e \"\n}\n]\n}\nMany fonts then did not make\nthe right leg vertical.\nTable 9: Examples of how the syntactic negation augmentation method works. For the ﬁrst sentence, the matched\nrule has two actions, move and replace. The move action has moved the token B = did before token A = mention.\nThe replace action has replaced npiword = Nowhere with an empty token, which means removing this token. The\ntoken object = letter is chosen as the unlikelihood token in this sentence.\nIn the second sentence, the matched rule has three actions, twoinserts and onelemmatize action. The insert actions,\nadd the tokens “did not” before A = made, and the token A = made is replaced with its lemma by the lemmatize\naction. The token object = leg is chosen as the unlikelihood token in the negated sentence.\n1312\nD Mixing negation unlikelihood training and knowledge distillation with NLI training\nIn order to reduce the catastrophic forgetting behavior of the model during NLI training, we added the\nunlikelihood, knowledge distillation and MLM objectives to the original NLI classiﬁcation objective and\ntrained the model with the same hyper-parameters for the MNLI task. We also trained one version with\nonly the original NLI classiﬁcation objective and the MLM objective. As the results in table 10 show,\nthis method did not improve the scores for development split and the new split containing negation from\nHossain et al. (2020b) for MNLI.\nModel MNLI\ndev w/neg\nBERTNOT + UL + KL + MLM + NLI obj 81.17 60.20\nBERTNOT + MLM + NLI obj 81.42 62.00\nTable 10: Accuracies on original development split (dev) and new split containing negation from Hossain et al.\n(2020b) (w/neg) for MNLI (matched genres) task.\nE Supplementary Results\nModel lr SQuAD ConceptNet T-REx Google-RE\nBERTNOT without reference setup 1e-5 13.86 15.65 29.54 10.29\nBERT-large 1e-5 16.83 19.26 30.76 10.93\nBERTNOT-large 1e-5 14.19 19.14 32.09 11.02\nBERTNOT-large 5e-5 15.18 16.97 30.71 10.62\nBERTNOT-large 1e-4 11.55 13.58 28.41 9.25\nTable 11: Mean precision at k= 1(p @ 1) for original LAMA queries (higher is better) of BERT with unlikelihood\nand distillation objectives without references for sentences, BERT-large, and BERT-large with unlikelihood and\ndistillation objectives with different learning rates.\nModel lr SQuAD ConceptNet T-REx Google-RE\nBERTNOT without reference setup 1e-5 5.96 1.34 21.54 3.73\nBERT-large 1e-5 7.95 1.67 22.97 4.13\nBERTNOT-large 1e-5 8.28 1.87 23.49 4.22\nBERTNOT-large 5e-5 8.28 2.20 24.05 4.09\nBERTNOT-large 1e-4 4.97 1.47 20.86 3.60\nTable 12: Mean top 1 error rate for negated LAMA queries (lower is better) of BERT with unlikelihood and distil-\nlation objectives without references for sentences, BERT-large, and BERT-large with unlikelihood and distillation\nobjectives with different learning rates.\nAs the results in table 12 show, pre-trained BERT-large performs worse than pre-trained BERT-base on\nnegated LAMA queries. We decreased the batch-size to be able to ﬁne-tune BERT-large. As the scores for\nnegated LAMA queries from table 12 show, ﬁne-tuning BERT-large with our method using the same or\nslightly larger learning rate does not improve the results. We observe a decrease in the mean top 1 error\nrates for negated LAMA queries when we use a larger learning rate ( 1e−5), but this also hinders the\nperformance of the model on the original LAMA queries (table 11). This requires some hyper-parameter\ntuning and further investigation.",
  "topic": "Negation",
  "concepts": [
    {
      "name": "Negation",
      "score": 0.6598802804946899
    },
    {
      "name": "Computer science",
      "score": 0.5892302989959717
    },
    {
      "name": "Computational linguistics",
      "score": 0.5822247862815857
    },
    {
      "name": "Linguistics",
      "score": 0.4976983368396759
    },
    {
      "name": "Cognitive science",
      "score": 0.449362188577652
    },
    {
      "name": "Programming language",
      "score": 0.41765764355659485
    },
    {
      "name": "Natural language processing",
      "score": 0.4001295566558838
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3410720229148865
    },
    {
      "name": "Psychology",
      "score": 0.2077348828315735
    },
    {
      "name": "Philosophy",
      "score": 0.19973188638687134
    }
  ]
}