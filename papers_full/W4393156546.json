{
  "title": "MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models",
  "url": "https://openalex.org/W4393156546",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2100082781",
      "name": "Yan Cai",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2105221238",
      "name": "Linlin Wang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2100304378",
      "name": "Ye Wang",
      "affiliations": [
        "East China Normal University",
        "Shanghai Jiao Tong University",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2134233121",
      "name": "Gerard de Melo",
      "affiliations": [
        "Hasso Plattner Institute",
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A2100362878",
      "name": "Ya Zhang",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2098837876",
      "name": "Yanfeng Wang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "East China Normal University",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2105816867",
      "name": "Liang He",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2105221238",
      "name": "Linlin Wang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "East China Normal University",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2100304378",
      "name": "Ye Wang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence",
        "East China Normal University",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2134233121",
      "name": "Gerard de Melo",
      "affiliations": [
        "Hasso Plattner Institute",
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A2100362878",
      "name": "Ya Zhang",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2098837876",
      "name": "Yanfeng Wang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "Beijing Academy of Artificial Intelligence",
        "East China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1979350964",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W3088056511",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W4313451803",
    "https://openalex.org/W3212648282",
    "https://openalex.org/W4381930847",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W4221154592",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4362655261",
    "https://openalex.org/W4387947536",
    "https://openalex.org/W4387225871",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W4366327625",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4390833194",
    "https://openalex.org/W4286233477",
    "https://openalex.org/W4366198844",
    "https://openalex.org/W4362598952",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W4379540389",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W2049615058",
    "https://openalex.org/W4376632689",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4367623495"
  ],
  "abstract": "The emergence of various medical large language models (LLMs) in the medical domain has highlighted the need for unified evaluation standards, as manual evaluation of LLMs proves to be time-consuming and labor-intensive. To address this issue, we introduce MedBench, a comprehensive benchmark for the Chinese medical domain, comprising 40,041 questions sourced from authentic examination exercises and medical reports of diverse branches of medicine. In particular, this benchmark is composed of four key components: the Chinese Medical Licensing Examination, the Resident Standardization Training Examination, the Doctor In-Charge Qualification Examination, and real-world clinic cases encompassing examinations, diagnoses, and treatments. MedBench replicates the educational progression and clinical practice experiences of doctors in Mainland China, thereby establish- ing itself as a credible benchmark for assessing the mastery of knowledge and reasoning abilities in medical language learning models. We perform extensive experiments and conduct an in-depth analysis from diverse perspectives, which culminate in the following findings: (1) Chinese medical LLMs underperform on this benchmark, highlighting the need for significant advances in clinical knowledge and diagnostic precision. (2) Several general-domain LLMs surprisingly possess considerable medical knowledge. These findings elucidate both the capabilities and limitations of LLMs within the context of MedBench, with the ultimate goal of aiding the medical research community.",
  "full_text": "MedBench: A Large-Scale Chinese Benchmark\nfor Evaluating Medical Large Language Models\nYan Cai1, Linlin Wang1,2*, Ye Wang1, Gerard de Melo3,4, Ya Zhang2,5, Yanfeng Wang2,5, Liang He1\n1East China Normal University\n2 Shanghai Artificial Intelligence Laboratory\n3 Hasso Plattner Institute\n4 University of Potsdam\n5 Shanghai Jiao Tong University\n{51255901104,yewang}@stu.ecnu.edu.cn, {llwang,lhe}@cs.ecnu.edu.cn, gdm@demelo.org,\n{ya zhang,wangyanfeng}@sjtu.edu.cn\nAbstract\nThe emergence of various medical large language models\n(LLMs) in the medical domain has highlighted the need for\nunified evaluation standards, as manual evaluation of LLMs\nproves to be time-consuming and labor-intensive. To address\nthis issue, we introduce MedBench, a comprehensive bench-\nmark for the Chinese medical domain, comprising 40,041\nquestions sourced from authentic examination exercises and\nmedical reports of diverse branches of medicine. In particular,\nthis benchmark is composed of four key components: the Chi-\nnese Medical Licensing Examination, the Resident Standard-\nization Training Examination, the Doctor In-Charge Qual-\nification Examination, and real-world clinic cases encom-\npassing examinations, diagnoses, and treatments. MedBench\nreplicates the educational progression and clinical practice\nexperiences of doctors in Mainland China, thereby establish-\ning itself as a credible benchmark for assessing the mastery of\nknowledge and reasoning abilities in medical language learn-\ning models. We perform extensive experiments and conduct\nan in-depth analysis from diverse perspectives, which culmi-\nnate in the following findings: (1) Chinese medical LLMs un-\nderperform on this benchmark, highlighting the need for sig-\nnificant advances in clinical knowledge and diagnostic pre-\ncision. (2) Several general-domain LLMs surprisingly pos-\nsess considerable medical knowledge. These findings eluci-\ndate both the capabilities and limitations of LLMs within the\ncontext of MedBench, with the ultimate goal of aiding the\nmedical research community.\nIntroduction\nThe advent of large language models (LLMs) has demon-\nstrated substantial potential for diverse real-world applica-\ntions, thanks to their remarkable language understanding ca-\npabilities. In the medical domain, a notable number of Chi-\nnese medical LLMs have successively emerged, including\nHuaTuo (Wang et al. 2023), BianQue (Chen et al. 2023),\nand DoctorGLM (Xiong et al. 2023), to better assist doctors\nin diverse tasks ranging from clinical diagnosis to disease\nprevention (Singhal et al. 2023). This underscores an urgent\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nNMPQE(Medical Licensing Exam)Medical SchoolResidencyDoctor in-charge Real-world Case\nOurs\nMedical SchoolResidency\nJapan\nUSMLEfoundation sciencesMedical SchoolResidency\nUSA\nResident Standardization Training Exam CNMLE(Chinese Medical Licensing Exam)Doctor in-chargeQualification ExamReal-worldMedical Diagnosis\nUSMLEclinical knowledgeUSMLEgeneralist medical practice\nFigure 1: Comparison of procedures in different countries.\nneed for a standardized medical benchmark, capable of of-\nfering reliable and authoritative evaluations for such LLMs.\nAssessing the potential and inherent limitations of medical\nLLMs from diverse perspectives continues to present con-\nsiderable challenges (Singhal et al. 2023; Chang et al. 2023).\nThe primary cause of this issue lies in the pronounced dis-\ncrepancy between the existing benchmarks and the practical\nrealities of medicine, leading to an urgent need for advances\nin evaluation standards. Widely used medical benchmarks\nsuch as MedQA (Jin et al. 2021) and MedMCQA (Pal, Uma-\npathi, and Sankarasubbu 2022) typically encompass pub-\nlicly available medical question-answering datasets gathered\nfrom textbooks, research papers, and board exams. How-\never, in light of recent research employing these open-access\ndatasets for model training (Han et al. 2023), a process po-\ntentially resulting in data contamination with regard to the\nevaluation (Nori et al. 2023), it is increasingly clear that\nthe prevailing benchmarks exhibit considerable limitations.\nA recent study has made advances by introducing a hu-\nman evaluation framework along with MultiMedQA (Sing-\nhal et al. 2023), a multi-faceted benchmark that combines\nexisting QA datasets with a novel set of online medical\nquestions. Nonetheless, given the disparities among typo-\nlogically diverse languages, we are unable to rely on this\nbenchmark for assessing Chinese large language models, in\nparticular considering the unique clinical standards and pro-\ncedures across different countries illustrated in Figure 1.\nHence, several Chinese benchmarks have been proposed,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17709\n【few_shot_examples】\n问题：女性，58岁。9个月来巩膜及皮肤发黄，伴皮肤瘙痒来\n诊。体检：巩膜及皮肤明显黄染，肝肋下4cm，质硬无压痛，\n脾肋下5cm，考虑为原发性胆汁性肝硬化。原发性胆汁性肝硬\n化的主要病变在（　　）。\n(Question: A 58-year-old female. Has had jaundice of \nthe sclera and skin for 9 months, accompanied by \npruritus. Physical examination: obvious yellow staining \nof the sclera and skin, liver palpable 4 cm below the \ncostal margin, hard texture without tenderness, spleen \npalpable 5 cm below the costal margin. Considered to \nbe primary biliary cirrhosis. The main lesions of primary \nbiliary cirrhosis are located in the ( ).)\n选项(Options)：\nA. 门管区胆管和胆小管(Bile ducts in portal tracts)\nB. 肝胆管(Hepatic ducts)\nC. 毛细胆管(Bile canaliculi)\nD. 肝细胞(Hepatocytes)\nE. 胆总管(Common bile duct)\n回答(Answer): \nChatGPT：\nA✔\nChatGLM：\nA✔HuaTuo：\nA. 门管区胆管和胆\n小管✔\nChatMed：\n答案是A✔\n【few_shot_examples】\n症状: 检查中发现右甲状腺甲状腺包块，遂于2018-08-14我院甲状腺外科就诊\n(Symptoms: Examination revealed a nodule in the right lobe of the thyroid \ngland, so the patient visited the thyroid surgery department of our hospital on \nAugust 14, 2018.)\n检查(Examinations): \nGround Truth：\n甲状腺彩超\n(Thyroid ultrasound)\nBaichuan-13B：\n根据您的描述和提供的检查结果，您可\n能患有以下几种疾病(Based on your \ndescription and provided \nexamination results, you may have \nthe following conditions)：...\nHuaTuo：\n甲状腺彩超(Thyroid ultrasound)...\nChatMed：\n甲状腺穿刺活组织病理学报告\n(Thyroid fine needle aspiration \nbiopsy pathology report)\nBaichuan-13B：\nA✔\nChatGLM：\n根据症状和检查结果，患者可能存在以\n下疾病(Based on the symptoms and \nexamination results, the patient may \nhave the following conditions)：...\nChatGPT：\n甲状腺超声检查，甲状腺功能检查\n(Thyroid ultrasound, thyroid function \ntest)\nFigure 2: Examples of prompts and corresponding answers. The left side shows the prompt and responses of LLMs on an\nexample question from an exam. The right side is an example of a real-world case.\nincluding a medical NLP-task oriented one 1 and others\nbased on the Chinese National Medical Licensing Examina-\ntion (CNMLE), including MLEC-QA (Li, Zhong, and Chen\n2021) and CMExam (Liu et al. 2023). While these bench-\nmarks provide valuable insights, they are not exhaustive and\nmay fall short in enabling us to comprehensively gauge the\npotential of LLMs with respect to all-round medical knowl-\nedge and their practical utility in real-world clinical diagnos-\ntic scenarios. In mainland China, a unique three-stage ex-\namination process is employed, key components of which,\nincluding the Resident Standardization Training and Doc-\ntor in-charge Qualification Exams, have largely been over-\nlooked in prior work. Furthermore, the evaluation of real-\nworld clinical practical skills has not been adequately incor-\nporated in prior work.\nTo address these gaps and align with the learning and\ngrowth trajectory of Chinese doctors, we introduce Med-\nBench, a novel large-scale Chinese medical benchmark,\nwhich encompasses both authentic three-stage medical ex-\naminations and real-world clinical diagnosis cases. It sur-\npasses prior benchmarks by being exclusively sourced from\nthe latest validated exams and expert-annotated EHRs,\nthereby ensuring compliance with medical standards and\npractices. Furthermore, we conduct extensive experiments\nand offer detailed analyses to provide diverse perspectives\nfor evaluating clinical knowledge recall and reasoning capa-\nbilities of LLMs across a range of branches of medicine. The\nmain findings on this benchmark are as follows:\n• Chinese medical large language models underperform on\nthis benchmark, necessitating substantial improvements\nin clinical knowledge and diagnostic accuracy, as well as\nrefinement of their original in-context learning abilities.\n• Several large language models designed for general-\ndomain tasks possess substantial medical knowledge,\nthereby exhibiting promising potential.\n• Human evaluation reveals that ChatGPT possesses rich\n1https://github.com/michael-wzhu/PromptCBLUE\nknowledge for clinical practices, while current Chinese\nmedical LLMs lack high-quality conversational abilities\nand sufficient medical knowledge.\n• ChatGPT’s unsatisfactory performance in case analysis,\na question type that demands profound medical knowl-\nedge understanding and clinical reasoning abilities, re-\nveals significant room for improvement in medicine.\nIn summary, this study presents a comprehensive benchmark\nthat aligns with the practical realities of medicine in main-\nland China and provides profound empirical findings to re-\nveal the medical capabilities and limitations of LLMs. Fur-\nthermore, we incorporate Item Response Theory (Birnbaum\n1969) to further enhance our benchmark, with the ultimate\ngoal of aiding the medical research community.\nThe Proposed Benchmark\nOverview For better evaluating medical foundation mod-\nels, this paper proposes MedBench as a large benchmark\nwith 40,041 exercises originating from both authentic med-\nical examinations and real-world diagnostic and treatment\ncases. Specifically, we collect three-stage medical examina-\ntions that serve as a faithful reflection of the comprehensive\nprocess involved in obtaining medical licenses in mainland\nChina, exemplifying essential medical knowledge. More-\nover, we construct a number of real-world cases based on\nelectronic health records that provide examination plans, di-\nagnoses, and treatments based on patients’ symptoms, which\ncan reveal the medical knowledge utilization and reasoning\ncapabilities of LLMs in the real world.\nConstruction and Statistics Regarding the three-stage\nmedical examinations, we collect representative exercises\nfrom the Chinese Medical Licensing Exam (CNMLE), Res-\nident Standardization Training Exam, and Doctor in-charge\nQualification Exam 2 across many recent years, covering\n27,248, 2,841, and 8,927 questions, respectively. The ex-\namination exercises, depicted on the left of Figure 2, are\n2http://www.nmec.org.cn/\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17710\nFigure 3: Branches of Medicine in MedBench. Part (a) rep-\nresents the Resident Standardization Training Exam, and (b)\nreflects the Doctor in-charge Qualification Exam.\nmultiple-choice questions that can be categorized into three\ntypes:\n• A1/A2/B: Single statement questions with one correct\nanswer out of five options.\n• A3/A4: A series of questions accompanied by a clinical\ncase with one correct answer out of five options.\n• Case Analysis: Given a clinical case, a series of ques-\ntions are created with 6–12 options per question. Some\nquestions may have more than one correct answer.\nFigure 3 depicts the classification of branches of medicine\nfor some examinations in MedBench. Note that we use in-\nternal medicine and surgery as representative examples for\nthe latter two stages, considering the vast number of subcat-\negories within these fields.\nFurthermore, we collect over 2,000 real-world electronic\nhealth records, and employ experts to identify the symptoms,\ndiagnoses, treatments, and examinations from the reports,\nresulting in 701 high-quality ones with an average length\nof 60–100 words per report (Figure 2). The annotators com-\npleted professional training and strictly following predefined\nannotation standards to ensure accuracy. Based on these an-\nnotated reports, a total of 1,025 question–answer pairs are\neventually formed.\nCharacteristics MedBench surpasses existing bench-\nmarks in several aspects: (1) Authenticity and Novelty. It\nexclusively leverages expert-annotated EHRs and authentic\nup-to-date medical examinations to mitigate contamination.\n(2) Comprehensiveness and Multi-facetedness. It is metic-\nulously designed to align with Chinese medical standards\nand practices by incorporating three-stage multi-disciplinary\nexaminations and real-world clinical cases. (3) Practicality.\nHuman evaluation on clinical real-world cases ensures con-\ngruence with practical realities of medicine, while difficulty-\nstratified divisions in MedBench enable rapid assessment.\nEvaluation\nModels and Evaluation Metrics To evaluate the medi-\ncal capabilities, we conduct assessments using MedBench\nwith several representative LLMs from both the general\nand medical domains, including ChatGPT, ChatGLM (Zeng\net al. 2023; Du et al. 2022), Baichuan-13B 3, HuaTuo, and\nChatMed4. Furthermore, we evaluate other Chinese medical\nLLMs, such as BianQue, but we have observed that it lacks\nthe ability to deliver accurate and reasonable responses to\nmultiple-choice questions.\nFor the three-stage multiple-choice examinations, we em-\nploy accuracy as the evaluation metric. When dealing with\nreal-world cases, we combine expert-level human evaluation\nwith the additional automatic evaluation metrics BLEU (Pa-\npineni et al. 2002) and ROUGE (Lin 2004).\nExperimental Settings We conduct extensive experi-\nments to evaluate the five-shot performance of LLMs, ensur-\ning their capability to respond in a multiple-choice format.\nWe leverage the API for ChatGPT5 and opt for local deploy-\nment to facilitate evaluations for other LLMs. Furthermore,\nwe partition the MedBench dataset based on exams, medical\nsubdiscipline, and question types, and perform independent\ntesting on each subset to enable a comprehensive analysis.\nMain Results\nThree-stage examination results. In Table 1, we present\na comprehensive analysis of the accuracy metrics for vari-\nous LLMs across the three exams. A salient observation is\nthat ChatGPT consistently surpasses other models, despite\nthe latter being training on extensive Chinese corpora or pre-\nmium medical datasets. Nevertheless, our observations indi-\ncate that ChatGPT’s accuracy rate hovers around 50% for\nCNMLE and approximately 60% for other assessments, ex-\nposing substantial avenues for enhancement.\nAs depicted in Figure 5, ChatGPT exhibits subpar per-\nformance on questions pertaining to Traditional Chinese\nMedicine (TCM) and Chinese Western Medicine (CWM),\nwith accuracy metrics oscillating between 40-45%. Con-\nversely, LLMs trained on a more expansive Chinese dataset\ndemonstrate a narrower performance disparity on TCM and\nCWM questions relative to ChatGPT. This suggests that\na contributing factor to ChatGPT’s diminished efficacy on\nMedBench may be attributed to its limited exposure to Chi-\nnese data during its pretraining phase, consequently affect-\ning its proficiency in Chinese medical knowledge.\nIt is imperative to underscore that Baichuan-13B and Hua-\nTuo encounter difficulties on case analysis questions, high-\nlighting potential domains for refinement in their logical rea-\nsoning or multi-turn dialogue competencies.\nWe also perform comparative evaluations with a number\nof recently-developed models, including MedLLaMA (Wu\n3https://github.com/baichuan-inc/Baichuan-13B.\n4https://github.com/michael-wzhu/ChatMed.\n5https://chat.openai.com.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17711\nLLM CNMLE Resident Standardization\nTraining Doctor in-Char\nge Qualification\nTotal\nA1/A2/B A3/A4 Total\nA1/A2/B A3/A4 Cases Analysis Total\nA1/A2/B A3/A4 Cases Analysis\nGPT-4 64.88 63.08\n69.03 75.64 77.08\n75.13 75.00 68.45 71.91\n68.24 62.80\nChatGPT 49.57 49.40\n51.85 60.59 61.30\n58.72 62.96 58.75 58.04\n59.73 65.52\nChatGLM 27.39 27.32\n28.16 29.96 28.41\n33.59 29.63 27.52 26.06\n31.43 28.97\nBaichuan-13B 30.47 30.54\n29.63 34.97 37.26\n32.70 22.65 29.56 31.31\n31.65 17.89\nHuaTuo 22.31 22.38\n21.47 23.32 23.53\n23.85 13.58 21.93 22.14\n21.85 18.62\nChatMed 23.45 23.46\n23.33 24.33 23.03\n26.54 32.10 24.02 23.46\n24.66 30.34\nTable 1: Results on three-stage medical examinations of MedBench.\n0\n1\n2\n3\n4\n5\nCorrectness\nCompleteness\nFluency\nFriendliness\nGPT-4\nChatGPT\nHuaTuo\nChatMed\nFigure 4: Human evaluation on clinical cases.\nLLM BLEU-1 R\nOUGE-L\nE T\nD E T D\nGPT-4 16.54\n11.25 12.17 0.95 0.82 2.66\nChatGPT 11.50 8.37 7.29 15.39 11.30 10.04\nChatGLM 0.77 2.12 1.90 1.79 3.62 3.29\nBaichuan-13B 11.07 5.29 10.14 0.71 0.00 1.11\nHuaTuo 1.56 3.77 2.32 2.43 0.52 1.06\nChatMed 4.34 4.88 5.79 1.35 0.55 0.26\nTable 2: Evaluation on cases (E stands for Examinations, T\nstands for Treatments, and D stands for Diagnoses)\net al. 2023), Baize (Xu et al. 2023a), and ChatDoctor (Li\net al. 2023). However, we have not noticed any signifi-\ncant enhancements in performance on our benchmark com-\npared to these models. For example, MedLLaMA achieves\nan accuracy of 24.5% on the A1/A2/B type of questions,\nmarginally outperforming ChatMed with a modest improve-\nment of 1.5%.\nReal-world clinical case performance. Table 2 provides\nthe results of the assessment on real-world cases, and Fig-\nure 2 provides the prompt-response instances. We evalu-\nated the outcomes using the BLEU and ROUGE F1-score\nmetrics. It can be easily observed that ChatGPT and GPT-\n4 (OpenAI 2023) exhibit superior performance, underscor-\ning their remarkable aptitude in medical and conversational\nquestion answering. However, it is notable that even for\nChatGPT and GPT-4, the top-performers across multiple tri-\nals, the scores remain relatively moderate. This could be at-\ntributed, in part, to the fact that metrics such as BLEU and\nROUGE might not holistically capture result quality. Fur-\nthermore, considerable scope remains for enhancing these\nLLMs’ capabilities in real-world clinical cases.\nBeyond automated evaluations, the judgment of a post-\nLLM Vanilla prompt Chain-of-Thought\nChatGLM-6B 42.69 43.37\nBaichuan-13B 37.26 43.85\nTable 3: A comparative assessment of Baichuan-13B and\nChatGLM-6B using A1/A2 questions from the Resident\nStandardization Training Exam under different prompts.\ngraduate medical scholar was solicited to appraise the out-\nputs delivered by the various LLMs on the real-world cases.\nThe objective of this evaluation endeavor was to quanti-\ntatively assess the correctness, completeness, fluency, and\nfriendliness of HuaTuo, ChatMed, ChatGPT, and GPT-\n4. Figure 4 provides the outcomes of this human eval-\nuation. GPT-4 is found to consistently manifest superior\nperformance across all delineated criteria, while ChatGPT\nmarginally trails GPT-4, particularly on correctness and\ncompleteness. Conversely, HuaTuo’s rating in friendliness\nis somewhat diminished, predominantly attributable to spo-\nradic generation of incongruous content. However, it is im-\nportant to note that HuaTuo demonstrates pronounced levels\nof correctness and completeness, evincing profound medical\nknowledge, though its articulation warrants improvement.\nChatMed, while laudable for its fluency and friendliness,\nregisters suboptimal scores in correctness and completeness.\nSuch disparities might be indicative of a potential attenua-\ntion of medical expertise during the fine-tuning phase.\nQuantitative Analysis\nComparison on different question types. Figure 5 pro-\nvides the detailed results on CNMLE, the Resident Stan-\ndardization Training Exam (denoted as “Resident”) and the\nDoctor in-charge Qualification Exam (“In-charge”). Clearly,\nChatGPT continues to exhibit a substantial performance ad-\nvantage over the other LLMs. A3/A4 questions and case\nanalysis questions, characterized by their multi-question\nstructure, present a substantial challenge to the conversa-\ntional abilities of the models. If a given LLM fares poorly\non these question types, it indicates its inherent limitations\nin efficiently managing multi-turn dialogue or questions ne-\ncessitating intricate reasoning steps.\nChain-of-Thought In the experimental investigation, it is\nobserved that prompts tailored to the LLM have the poten-\ntial to enhance the reasoning capabilities of the LLM. Fig-\nure 7 illustrates the choice distribution of Baichuan-13B un-\nder both the vanilla prompt and Chain-of-Thought prompt-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17712\nFigure 5: Results for three-stage medical examinations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17713\n问题: 疟疾明确诊断方法是 ( 　）\n(Question: The definitive diagnostic method for malaria is (  ))\n选项 (Options):\n     A. 出血倾向和贫血 (Bleeding tendency and anemia)\n     B. 放射免疫测定 (Radioimmunoassay)\n     C. 血常规 (Blood routine)\n     D. 血/骨髓涂片 (Blood/bone marrow smea)\n     E. 腹部B超 (Abdominal ultrasound)\n回答 (Answer):\nD\n为什么选择这个选项 (Why choose this option) ?\n因为血/骨髓涂片是疟疾确诊的“金标准”, 可以通过检查患者血液或\n骨髓中的疟原虫来确证疟疾。其他选项如放射免疫测定、病史、血\n常规和腹部B超也可以作为疟疾诊断的参考指标，但是它们不能像\n血 / 骨 髓 涂 片 那 样 提 供 确 凿 的 诊 断 结 果  ( T h i s  i s  b e c a u s e \nblood/bone marrow smears are the “gold standard ” for \nmalaria diagnosis, they can confirm malaria by examining \nmalaria parasites in the patient ’s blood or bone marrow. \nOther options like radioimmunoassay, medical history, blood \nroutine, and abdominal ultrasound can also serve as \nreferences for malaria diagnosis, but they cannot provide \ndefinitive diagnosis like blood/bone marrow smears) 。\n ✔\n问题: 患者，牧民，35岁。因发热1周入院，呈波状热，血常规示白细胞6.0×109/L，可\n见少数异型淋巴细胞，SAT(＋＋＋)。诊断为( 　 ) \n(Question:  A 35-year-old herdsman was admitted for 1 week of fever, with \nundulant fever. Blood routine test showed leukocytes 6.0×109/L, with a few \natypical lymphocytes seen, SAT(＋＋＋). The diagnosis is (  ))\n选项: (Options:)\n    A. 伤寒(Typhoid fever)              B. 败血症( Sepsis)\n    C. 疟疾(Malaria)              D. 布鲁菌病( Brucellosis)\n    E. 霍奇金病(Hodgkin's disease)\n回答 (Answer) :\nA\n为什么选择这个选项 (Why choose this option)?\n因为根据病史和体征，患者可能患有伤寒。伤寒是一种由沙门氏菌属细菌引起的传染病，\n常见于牧民和有良好卫生习惯的人群。伤寒的临床表现包括持续高热、乏力、腹泻、肌\n肉和关节疼痛、结膜充血等，血液检查可见白细胞计数升高，分类可见异型淋巴细胞，\n同时，患者出现波状热，也支持了伤寒的诊断。因此，选项A是最合适的答案 (Based \non the medical history and symptoms, the patient is likely suffering from \ntyphoid fever. Typhoid fever is an infectious disease caused by Salmonella \nbacteria, commonly seen in herdsmen and people with poor hygiene habits. \nThe clinical manifestations of typhoid fever include persistent high fever, fatigue, \ndiarrhea, muscle and joint pain, conjunctival congestion, etc. Blood tests may \nshow elevated white blood cell count and atypical lymphocytes. Also, the \npatient has undulant fever, which further supports the diagnosis of typhoid \nfever. Therefore, option A is the most appropriate answer) 。                   \n❌                                                            \nFigure 6: Examples of conversation demonstrating the reasoning capability of ChatGLM.\nFigure 7: Distribution of choices made by Baichuan-13B un-\nder different prompts, where (a) and (b) depict the results\nusing vanilla and Chain-of-Thought prompt, respectively.\ning. Notably, under the vanilla prompt, Baichuan-13B shows\na strong inclination towards option F, although this option is\nnot part of the valid choice set (A-E). In contrast, when us-\ning Chain-of-Thought prompting, Baichuan-13B primarily\ngravitates towards valid choices. Furthermore, Table 3 de-\ntails the accuracy associated with both the vanilla prompt\nand the Chain-of-Thought one. The data suggests a signif-\nicant improvement in the accuracy of Baichuan-13B when\nusing Chain-of-Thought prompting. Parallel experiments\nare conducted on ChatGLM-6B, and the improvement in ac-\ncuracy when using Chain-of-Thought prompting is negligi-\nble for ChatGLM-6B.\nFewer questions, higher differentiation During the as-\nsessment process, some shortcomings became apparent:\n(1) Certain LLMs manifest suboptimal inference speed on\nGPUs or high computational cost when interfacing via\nan API. Given that MedBench comprises approximately\n40,000 questions, performing inference on the entirety of\nthis dataset consecutively takes considerable time for LLMs.\n(2) While categorization of questions based on the type pro-\nvides a rudimentary gauge of difficulty, it is important to ac-\nknowledge that questions within the same category can ex-\nhibit disparate levels of difficulty. (3) Assigning particularly\nchallenging questions to less capable LLMs is inadvisable,\nas it may culminate in uniformly diminished accuracy, mak-\ning meaningful distinctions impossible.\nTo ameliorate these challenges, we propose methodolog-\nical strategies to classify questions of analogous types ac-\ncording to their inherent difficulty gradients. By adopting\nthis paradigm, we can optimize the evaluation process, al-\nlowing LLMs to undertake inference on a curtailed set of\nquestions. As a result, this allows for a more nuanced align-\nment between LLMs and questions of commensurate dif-\nficulty tiers, congruent with their individual proficiencies.\nSpecifically, we introduce an advanced evaluative frame-\nwork for LLMs drawing inspiration from Item Response\nTheory (IRT). Our approach integrates the three-parameter\nlogistic model (IRT-3PL), given by:\nP(Xij = 1|θj) =ci + (1− ci) 1\n1 +e−ai(θj −bi) (1)\nHere, θj represents the proficiency of LLMj, and P(Xij =\n1|θj) is the probability that an LLM j with proficiency θj\ngives a correct response to question i. This equation high-\nlights three crucial parameters for each question i: discrim-\nination (ai), difficulty (b i), and the guessing factor (c i). In\nour approach, we assume constant values for both ai and\nci, and then group items based on the shared difficulty met-\nric bi. For a specific difficulty subset, these three parameters\nremain unchanged. The equation captures the LLM’s prob-\nability θj, determined from the inference results when inter-\nacting with a specific subset.\nA total of 7,335 questions were divided into 10 difficulty\nlevels. To validate question differentiation, we analyze them\nusing BLOOMZ-7.1B (Muennighoff et al. 2022), Qwen-\n7B6, ChatGLM-turbo (Zeng et al. 2023), Qwen-max (Bai\net al. 2023) across various difficulty levels. Part (a) of Fig-\nure 8 displays the accuracy trends of these LLMs across\n6https://github.com/QwenLM/Qwen-7B.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17714\nFigure 8: Evaluation of a few LLMs across varying levels of\nquestion difficulty, with Level 9 denoting the highest level\nof difficulty.\nthese levels, highlighting a decrease in accuracy with in-\ncreasing difficulty. Figure 8(b) shows the difference in ac-\ncuracy between BLOOMZ-7.1B and Qwen-7B, as well as\nbetween Qwen-max and ChatGLM-turbo, for different lev-\nels of question difficulty. It is noticeable that the differences\nbetween the LLMs are significant when the question diffi-\nculty is appropriate.\nReasoning Abilities\nIn MedBench, there exist several typical types of reasoning:\n• Multi-condition single-hop reasoning: A type of rea-\nsoning that requires the LLM to engage in single-hop\nreasoning based on a clinical scenario.\n• Statement identification: A type of reasoning that re-\nquires the LLM to judge the correctness of multiple state-\nments, which further tests the LLM’s knowledge base.\n• Multi-hop reasoning: A type of reasoning with multiple\nquestions, where the questions are related, requiring the\nLLM to perform multi-hop reasoning.\nTo ascertain the extent to which LLMs make accurate deci-\nsions based on their grasp of pertinent knowledge, we ex-\ntended our inquiry, which entailed requesting the LLMs to\nfurnish justifications for their responses, as depicted in Fig-\nure 6. Our observations reveal that LLMs can substantiate\ntheir answers when they are accurate. Conversely, in cases\nwhere erroneous responses are given, the accompanying ex-\nplanations often prove illogical, which provides compelling\nevidence that these LLMs either lack the requisite knowl-\nedge in the domain or are incapable of rationalizing towards\nthe correct solutions.\nRelated Work\nTraditionally, medical LLMs relied on classic medical QA\nbenchmarks for evaluation. Some studies used USMLE for\nevaluation and achieved satisfactory results (Kung et al.\n2023; Nori et al. 2023), with zero-shot GPT-4 achieving av-\nerage scores of 86.65% and 86.7% on the Self-Assessment\nand Sample Exam parts of USMLE, respectively. However,\nquestions from USMLE have a very distinct Western med-\nical perspective that is notably different from that of Chi-\nnese medicine. As two distinct medical systems, the latter\nincludes traditional Chinese medicine, with obvious differ-\nences in concepts, diagnosis, and treatment methods. There-\nfore, the same symptoms may lead to different diagnoses\nand treatments. As a result, the USMLE cannot serve as an\nadequate benchmark for Chinese medical practice. MedQA,\nPubMedQA (Jin et al. 2019; Li ´evin, Hother, and Winther\n2022), and MLEC-QA are also useful benchmarks for med-\nical QA, comprising a large number of high-quality ques-\ntions, but they were proposed early on, potentially leading\nto data contamination. Although MedMCQA is a relatively\nnew benchmark, its questions originate primarily from In-\ndian medical institutions. As a result, analogous to USMLE,\nit lacks Chinese medical content and thus cannot serve to\nevaluate Chinese medical question answering sufficiently\nwell. CMExam is a novel benchmark for medical QA in Chi-\nnese, comprising the latest exam questions from CNMLE,\nwhich has been manually annotated by medical experts with\nassistance from ChatGPT to ensure high quality. However,\nCMExam only comprises questions from CNMLE, while\noverlooking other major medical exams in mainland China.\nMoreover, as a predominantly multiple choice dataset with\nonly a small portion of fill-in-the-blank questions, CMExam\nlacks real-world medical data and scenarios. This constrains\nits ability to fully assess LLMs’ medical question answer-\ning capabilities. In fact, the optimal evaluation approach\nis to evaluate LLMs manually (Xu et al. 2023b). A re-\ncent work (Tu et al. 2024) employs specialist physicians to\nassess the quality. However, manual evaluation is a time-\nconsuming and labor-intensive task, making it difficult to\nconduct on a large scale.\nConclusion and Discussions\nIn this paper, we present MedBench, an exhaustive bench-\nmark specifically designed for the domain of Chinese med-\nical question answering. Preliminary empirical analyses un-\nderscore the suboptimal performance of Chinese medical\nLLMs when subjected to this benchmark, highlighting the\nneed for improved clinical acumen and diagnostic preci-\nsion. Furthermore, the adeptness of these LLMs in contex-\ntual learning requires further refinement.\nDuring our empirical investigations, we found that cer-\ntain models manifest pronounced hallucinatory behavior. As\nour research progresses, it is imperative to ensure data ve-\nracity while delving into the systematic evaluation of such\nhallucinatory phenomena. Furthermore, the appraisal of the\nLLM’s inferential competencies, as presented in this re-\nsearch, points to the need for further methodological refine-\nments. Within the realm of clinical diagnostics, a diagno-\nsis is typically predicated upon a plethora of corroborative\nevidence. In subsequent work, we plan to compile an en-\nriched dataset, encompassing patients’ antecedent medical\nrecords and comprehensive physical examination narratives,\nto strengthen the evaluative framework for medical LLMs.\nSimultaneously, our findings underscore the efficacy of psy-\nchometric methodologies in aiding the evaluation. Moving\nforward, we intend to enhance and further integrate such\nmethodologies into our model assessment paradigm.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17715\nAcknowledgments\nThis work was supported by the National Innovation\n2030 Major S&T Project of China (No.2022ZD0160700 &\n2022ZD0160705) and the National Natural Science Foun-\ndation of China under Grant 12105105. The computations\nwere performed in the ECNU Multifunctional Platform for\nInnovation (001). We also thank the financial support from\nShanghai Chinafortune Co., Ltd.\nReferences\nBai, J.; Bai, S.; Chu, Y .; Cui, Z.; Dang, K.; Deng, X.; Fan,\nY .; Ge, W.; Han, Y .; Huang, F.; Hui, B.; Ji, L.; Li, M.; Lin,\nJ.; Lin, R.; Liu, D.; Liu, G.; Lu, C.; Lu, K.; Ma, J.; Men,\nR.; Ren, X.; Ren, X.; Tan, C.; Tan, S.; Tu, J.; Wang, P.;\nWang, S.; Wang, W.; Wu, S.; Xu, B.; Xu, J.; Yang, A.; Yang,\nH.; Yang, J.; Yang, S.; Yao, Y .; Yu, B.; Yuan, H.; Yuan, Z.;\nZhang, J.; Zhang, X.; Zhang, Y .; Zhang, Z.; Zhou, C.; Zhou,\nJ.; Zhou, X.; and Zhu, T. 2023. Qwen Technical Report.\narXiv:2309.16609.\nBirnbaum, A. 1969. Statistical theory for logistic mental\ntest models with a prior distribution of ability. Journal of\nMathematical Psychology, 6(2): 258–276.\nChang, Y .; Wang, X.; Wang, J.; Wu, Y .; Yang, L.; Zhu,\nK.; Chen, H.; Yi, X.; Wang, C.; Wang, Y .; Ye, W.;\nZhang, Y .; Chang, Y .; Yu, P. S.; Yang, Q.; and Xie, X.\n2023. A Survey on Evaluation of Large Language Models.\narXiv:2307.03109.\nChen, Y .; Wang, Z.; Xing, X.; huimin zheng; Xu, Z.; Fang,\nK.; Wang, J.; Li, S.; Wu, J.; Liu, Q.; and Xu, X. 2023.\nBianQue: Balancing the Questioning and Suggestion Abil-\nity of Health LLMs with Multi-turn Health Conversations\nPolished by ChatGPT. arXiv:2310.15896.\nDu, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and\nTang, J. 2022. GLM: General Language Model Pretraining\nwith Autoregressive Blank Infilling. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 320–335.\nHan, T.; Adams, L. C.; Papaioannou, J.-M.; Grundmann,\nP.; Oberhauser, T.; L ¨oser, A.; Truhn, D.; and Bressem,\nK. K. 2023. MedAlpaca – An Open-Source Collection\nof Medical Conversational AI Models and Training Data.\narXiv:2304.08247.\nJin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; and\nSzolovits, P. 2021. What Disease Does This Patient Have?\nA Large-Scale Open Domain Question Answering Dataset\nfrom Medical Exams. Applied Sciences, 11(14).\nJin, Q.; Dhingra, B.; Liu, Z.; Cohen, W.; and Lu, X. 2019.\nPubMedQA: A Dataset for Biomedical Research Question\nAnswering. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP).\nKung, T. H.; Cheatham, M.; Medenilla, A.; Sillos, C.;\nDe Leon, L.; Elepa˜no, C.; Madriaga, M.; Aggabao, R.; Diaz-\nCandido, G.; Maningo, J.; and Tseng, V . 2023. Performance\nof ChatGPT on USMLE: Potential for AI-assisted medi-\ncal education using large language models. PLOS Digital\nHealth, 2(2): 1–12.\nLi, J.; Zhong, S.; and Chen, K. 2021. MLEC-QA: A Chi-\nnese multi-choice biomedical question answering dataset. In\nProceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, 8862–8874.\nLi, Y .; Li, Z.; Zhang, K.; Dan, R.; Jiang, S.; and Zhang, Y .\n2023. ChatDoctor: A Medical Chat Model Fine-Tuned on a\nLarge Language Model Meta-AI (LLaMA) Using Medical\nDomain Knowledge. Cureus.\nLin, C.-Y . 2004. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, 74–81.\nLiu, J.; Zhou, P.; Hua, Y .; Chong, D.; Tian, Z.; Liu, A.;\nWang, H.; You, C.; Guo, Z.; Zhu, L.; et al. 2023. Bench-\nmarking Large Language Models on CMExam–A Compre-\nhensive Chinese Medical Exam Dataset. NeurIPS 2023.\nLi´evin, V .; Hother, C. E.; and Winther, O. 2022. Can\nlarge language models reason about medical questions?\narXiv:2207.08143.\nMuennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.; Bi-\nderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong, Z.-X.;\nSchoelkopf, H.; Tang, X.; Radev, D.; Aji, A. F.; Almubarak,\nK.; Albanie, S.; Alyafeai, Z.; Webson, A.; Raff, E.; and Raf-\nfel, C. 2022. Crosslingual Generalization through Multitask\nFinetuning. arXiv:2211.01786.\nNori, H.; King, N.; McKinney, S. M.; Carignan, D.; and\nHorvitz, E. 2023. Capabilities of GPT-4 on Medical Chal-\nlenge Problems. arXiv:2303.13375.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nPal, A.; Umapathi, L. K.; and Sankarasubbu, M. 2022.\nMedmcqa: A large-scale multi-subject multi-choice dataset\nfor medical domain question answering. In Conference on\nHealth, Inference, and Learning, 248–260. Proceedings of\nMachine Learning Research (PMLR).\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine trans-\nlation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311–318.\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\nPayne, P.; Seneviratne, M.; Gamble, P.; Kelly, C.; Babiker,\nA.; Sch ¨arli, N.; Chowdhery, A.; Mansfield, P.; Demner-\nFushman, D.; Ag ¨uera y Arcas, B.; Webster, D.; Corrado,\nG. S.; Matias, Y .; Chou, K.; Gottweis, J.; Tomasev, N.; Liu,\nY .; Rajkomar, A.; Barral, J.; Semturs, C.; Karthikesalingam,\nA.; and Natarajan, V . 2023. Large language models encode\nclinical knowledge. Nature, 620(7972): 172–180.\nTu, T.; Palepu, A.; Schaekermann, M.; Saab, K.; Freyberg,\nJ.; Tanno, R.; Wang, A.; Li, B.; Amin, M.; Tomasev, N.;\net al. 2024. Towards Conversational Diagnostic AI. arXiv\npreprint arXiv:2401.05654.\nWang, H.; Liu, C.; Xi, N.; Qiang, Z.; Zhao, S.; Qin, B.; and\nLiu, T. 2023. HuaTuo: Tuning LLaMA Model with Chinese\nMedical Knowledge. arXiv:2304.06975.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17716\nWu, C.; Lin, W.; Zhang, X.; Zhang, Y .; Wang, Y .; and Xie,\nW. 2023. PMC-LLaMA: Towards Building Open-source\nLanguage Models for Medicine. arXiv:2304.14454.\nXiong, H.; Wang, S.; Zhu, Y .; Zhao, Z.; Liu, Y .; Huang,\nL.; Wang, Q.; and Shen, D. 2023. DoctorGLM: Fine-\ntuning your Chinese Doctor is not a Herculean Task.\narXiv:2304.01097.\nXu, C.; Guo, D.; Duan, N.; and McAuley, J. 2023a. Baize:\nAn open-source chat model with parameter-efficient tuning\non self-chat data. EMNLP 2023.\nXu, J.; Lu, L.; Yang, S.; Liang, B.; Peng, X.; Pang, J.; Ding,\nJ.; Shi, X.; Yang, L.; Song, H.; Li, K.; Sun, X.; and Zhang,\nS. 2023b. MedGPTEval: A Dataset and Benchmark to Eval-\nuate Responses of Large Language Models in Medicine.\narXiv:2305.07340.\nZeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang,\nZ.; Xu, Y .; Zheng, W.; Xia, X.; et al. 2023. Glm-130b: An\nopen bilingual pre-trained model. ICLR 2023.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17717",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.6638522148132324
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5850529670715332
    },
    {
      "name": "Computer science",
      "score": 0.5382224321365356
    },
    {
      "name": "Natural language processing",
      "score": 0.43512362241744995
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34391647577285767
    },
    {
      "name": "Geography",
      "score": 0.17531141638755798
    },
    {
      "name": "Cartography",
      "score": 0.1336268186569214
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I66867065",
      "name": "East China Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I143288331",
      "name": "Hasso Plattner Institute",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I176453806",
      "name": "University of Potsdam",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ],
  "cited_by": 17
}