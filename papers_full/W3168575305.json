{
  "title": "CogNLP-Sheffield at CMCL 2021 Shared Task: Blending Cognitively Inspired Features with Transformer-based Language Models for Predicting Eye Tracking Patterns",
  "url": "https://openalex.org/W3168575305",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1932014527",
      "name": "Peter Vickers",
      "affiliations": [
        "University of Sheffield"
      ]
    },
    {
      "id": "https://openalex.org/A3171444632",
      "name": "Rosa Wainwright",
      "affiliations": [
        "University of Sheffield"
      ]
    },
    {
      "id": "https://openalex.org/A2966945397",
      "name": "Harish Tayyar Madabushi",
      "affiliations": [
        "University of Sheffield"
      ]
    },
    {
      "id": "https://openalex.org/A2157257934",
      "name": "Aline Villavicencio",
      "affiliations": [
        "University of Sheffield"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2099656056",
    "https://openalex.org/W2963890755",
    "https://openalex.org/W2766642952",
    "https://openalex.org/W2946296745",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W4392281002",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2898914189",
    "https://openalex.org/W1587989021",
    "https://openalex.org/W3136687158",
    "https://openalex.org/W3029724270",
    "https://openalex.org/W1974865858",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2138747152",
    "https://openalex.org/W3019445788",
    "https://openalex.org/W1974991592",
    "https://openalex.org/W2765796597",
    "https://openalex.org/W2512721747",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2905110202",
    "https://openalex.org/W2053374418",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2115380778",
    "https://openalex.org/W3015850748",
    "https://openalex.org/W2394756230",
    "https://openalex.org/W3106388706",
    "https://openalex.org/W2991275804",
    "https://openalex.org/W3169754410",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2898936689",
    "https://openalex.org/W2113772582",
    "https://openalex.org/W1983359679",
    "https://openalex.org/W2724377488",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2964060211",
    "https://openalex.org/W2207674461",
    "https://openalex.org/W2013112874",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2771876412",
    "https://openalex.org/W3027353876",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W4236120890",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2044881364"
  ],
  "abstract": "The CogNLP-Sheffield submissions to the CMCL 2021 Shared Task examine the value of a variety of cognitively and linguistically inspired features for predicting eye tracking patterns, as both standalone model inputs and as supplements to contextual word embeddings (XLNet). Surprisingly, the smaller pre-trained model (XLNet-base) outperforms the larger (XLNet-large), and despite evidence that multi-word expressions (MWEs) provide cognitive processing advantages, MWE features provide little benefit to either model.",
  "full_text": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 125–133\nOnline Event, June 10, 2021. ©2021 Association for Computational Linguistics\n125\nCogNLP-Shefﬁeld at CMCL 2021 Shared Task: Blending Cognitively\nInspired Features with Transformer-based Language Models for\nPredicting Eye Tracking Patterns\nPeter Vickers*, Rosa Wainwright*,\nHarish Tayyar Madabushi and Aline Villavicencio\nDepartment of Computer Science\nUniversity of Shefﬁeld\nUnited Kingdom\n{pgjvickers1, rhwainwright1, h.tayyarmadabushi, a.villavicencio}\n@sheffield.ac.uk\nAbstract\nThe CogNLP-Shefﬁeld submissions to the\nCMCL 2021 Shared Task examine the value\nof a variety of cognitively and linguistically\ninspired features for predicting eye tracking\npatterns, as both standalone model inputs and\nas supplements to contextual word embed-\ndings (XLNet). Surprisingly, the smaller pre-\ntrained model (XLNet-base) outperforms the\nlarger (XLNet-large), and despite evidence\nthat multi-word expressions (MWEs) provide\ncognitive processing advantages, MWE fea-\ntures provide little beneﬁt to either model.\n1 Introduction and Motivation\nMany researchers now agree that eye movements\nduring reading are not random (Rayner, 1998); as a\nresult, eye-tracking has been used to study a variety\nof linguistic phenomena, such as language acqui-\nsition (Blom and Unsworth, 2010) and language\ncomprehension (Tanenhaus, 2007). Readers do not\nstudy every word in a sentence exactly once, so fol-\nlowing patterns of ﬁxations (pauses with the eyes\nfocused on a word for processing) and regressions\n(returning to a previous word) provides a relatively\nnon-intrusive method for capturing subconscious\nelements of subjects’ cognitive processes.\nRecently, cognitive signals like eye-tracking data\nhave been put to use in a variety of NLP tasks,\nsuch as POS-tagging (Barrett et al., 2016), de-\ntecting multi-word expressions (Rohanian et al.,\n2017) and regularising attention mechanisms (Bar-\nrett et al., 2018): the majority of research utilis-\ning eye-tracking data has focused on its revealing\nlinguistic qualities of the reading material and/or\nthe cognitive processes involved in reading. The\nCMCL 2021 Shared Task of Predicting Human\nReading Behaviour (Hollenstein et al., 2021) asks a\n*Equal Contribution\nslightly different question: given the reading mate-\nrial, is it possible to predict eye-tracking behaviour?\nOur ability to quantitatively describe linguistic\nphenomena has greatly increased since the ﬁrst\nfeature-based models of reading behaviour (i.e.\nCarpenter and Just (1983)). Informed by these\ntraditional models, our ﬁrst model tests ‘simple’\nfeatures that are informed by up-to-date expert lin-\nguistic knowledge. In particular, we investigate\ninformation about multi-word expressions (MWEs)\nas eye-tracking information has been used to de-\ntect MWEs in context (Rohanian et al., 2017;\nYaneva et al., 2017), and empirically MWEs appear\nhave processing advantages over non-formulaic lan-\nguage (Siyanova-Chanturia et al., 2017).\nOur second model is motivated by evidence that\nPre-trained Language Models (PLMs) outperform\nfeature based models in ways that do not corre-\nlate with identiﬁable cognitive processes (Sood\net al., 2020). Since many PLMs evolved from the\nstudy of human cognitive processes (Vaswani et al.,\n2017) but now perform in ways that do not cor-\nrelate with human cognition, we wished to inves-\ntigate how merging cognitively inspired features\nwith PLMs may impact predictive behaviour. We\nfelt this was a particularly pertinent question given\nthat PLMs have been shown to contain information\nabout crucial features for predicting eye tracking\npatterns such as parts of speech (Chrupała and Al-\nishahi, 2019; Tenney et al., 2019) and sentence\nlength (Jawahar et al., 2019).\nWe therefore had the goals of providing a com-\npetitive Shared Task entry, and investigating the\nfollowing hypotheses: A) Does linguistic/cognitive\ninformation that can be predicted by eye-tracking\nfeatures prove useful for predicting eye-tracking\nfeatures? B) Can adding cognitively inspired fea-\ntures to a model based on PLMs improve perfor-\nmance in predicting eye tracking features?\n126\n2 Task Description\nThe CMCL 2021 Shared Task of Predicting Read-\ning Behaviour formulates predicting gaze features\nfrom the linguistic information in their associated\nsentences as a regression task. The data for the task\nconsists of 991 sentences (800 training, 191 test)\nand their associated token-level gaze features from\nthe Zurich Cognitive Language Processing Corpora\n(Hollenstein et al., 2018, 2020). For each word, the\nfollowing measures were averaged over the reading\nbehaviour of the participants: FFD (ﬁrst ﬁxation du-\nration, the length of the ﬁrst ﬁxation on the given\nword); TRT ( total reading time , the sum of the\nlengths of all ﬁxations on the given word); GPT\n(go past time, the time taken from the ﬁrst ﬁxation\non the given word for the eyes to move to its right\nin the sentence); nFix (number of ﬁxations, the total\nquantity of ﬁxations on a word, regardless of ﬁxa-\ntion lengths) and ﬁxProp (ﬁxation proportion, the\nproportion of participants that ﬁxated the word at\nleast once). Solutions were evaluated using Mean\nAbsolute Error (MAE). For more details about the\nShared Task, see Hollenstein et al. (2021).\n3 Related Work\nTransformer architectures Bidirectional En-\ncoder Representations from Transformers (BERT)\n(Devlin et al., 2019) is a Language Representation\nmodel constructed from stacked Neural Network at-\ntention layers and ‘massively’ pre-trained on large\nNatural Language Corpora. In contrast with tradi-\ntional language models, BERT is pre-trained in two\nsettings: a ‘cloze’ task where a randomly masked\nword is to be predicted, and next sentence predic-\ntion. BERT or derivative models have been used\nto achieve state-of-the-art baselines on many NLP\ntasks (Devlin et al., 2019; Yang et al., 2019). Anal-\nysis studies have shown that BERT learns complex,\ntask-appropriate, multi-stage pipelines for reason-\ning over natural language, although there is evi-\ndence of model bias. XLNet (Yang et al., 2019)\nis an autoregressive formulation of BERT which\ntrains on all possible permutations of contextual\nwords, and removes the assumption that predicted\ntokens are independent of each other.\nSimilar studies To our knowledge, studies that\nattempt to predict cognitive signals using language\nmodels are fairly few and far between. Djokic et al.\n(2020) successfully used non-Transformer word\nembeddings to decode brain activity recorded dur-\ning literal and metaphorical sentence disambigua-\ntion. Since RNNs may be considered more ‘cog-\nnitively plausible’ than Transformer based mod-\nels, Merkx and Frank (2020) compared how well\nthese two types of language models predict differ-\nent measures of human reading behaviour, ﬁnding\nthat the Transformer models more accurately pre-\ndicted self-paced reading times and EEG signals,\nbut the RNNs were superior for predicting eye-\ntracking measures.\nIn a slightly different task, Sood et al. (2020)\ncompared LSTM, CNN, and XLNet attention\nweightings with human eye-tracking data on the\nMovieQA task (Tapaswi et al., 2016), ﬁnding sig-\nniﬁcant evidence that LSTMs display similar pat-\nterns to humans when performing well. XLNet\nused a more accurate strategy for the task but was\nless similar to human reading.\nThough these studies may indicate that Trans-\nformer models are not the most suited to eye-\ntracking prediction, they are still considered State\nof the Art in creating broad semantic representa-\ntions and general linguistic competence (Devlin\net al., 2019). As such, we hoped they would allow\nus to investigate Carpenter and Just’s speculation\nthat the dominance of word length and frequency\nfor predicting eye-tracking behaviour may reduce\n“as the metrics improve for describing higher-level\nfactors” like semantic meaning (1983, p. 290).\n4 Experimental Design 1\nWe pursued both feature engineering and deep\nlearning approaches to the task; though both meth-\nods performed well independently, there was little\nimprovement in predictive capability when com-\nbining their features (see Table 1). As such, we\ndeveloped and submitted two models: Model 1\n(Feature Rich) and Model 2 (XLNet). Additional\ndetails about the feature combinations used in our\nﬁnal models can be found in Appendices A and C.\n4.1 Linguistic Features\nEach word in the training vocabulary was encoded\nas a one-hot vector. Since function words are more\nlikely to be ﬁxated than open class words (Carpen-\nter and Just, 1983), we included POS information\ngenerated by Spacy (Honnibal et al., 2020) (hon-\nouring the tokenisation in the training data). We\nincluded a a binary indicator for whether a word\n1For reproducibility purposes, our program code (includ-\ning details of hyperparameters) is available here: CogNLP-\nShefﬁeld-CMCL-2021\n127\nwas the ﬁrst or last in its sentence to incorporate\nthe knowledge that ﬁrst and last ﬁxations on a line\nare 5-7 letter spaces from the two respective ends\n(Rayner, 1998). We generated raw frequencies\n(proportion per million words) and Zipf frequen-\ncies (Van Heuven et al., 2014).\nFinally, concreteness norms (a measure of how\n‘abstract’ a given word is) were included as features\n(mean, standard deviation, and the % of partici-\npants familiar enough with the word to accurately\njudge its concreteness; Brysbaert et al. (2014)). We\nspeciﬁcally tested concreteness due to the unusu-\nally large coverage of the norms.\n4.2 Reading Speciﬁc Features\nWord length has been empirically demonstrated as\na very good predictor of gaze features in many stud-\nies (i.e. Rayner and McConkie (1976); Carpenter\nand Just (1983). Duration of ﬁxation is observed\nto increase for words that exceed the mean saccade\nlength (7-9 letters), and probability of ﬁxation is re-\nduced for words shorter than half the mean saccade\nlength (Rayner and McConkie, 1976). Therefore,\nas features we included both the raw word lengths,\nand categorical variables representing word length\nas a proportion of a mean saccade length.\nSince readers may store information about ad-\njacent words (Rayner, 1975, 1998; Barrett, 2018),\nwe also experimented with supplying features from\nprevious and future words to each target word.\n4.3 Type Summary Statistics from GECO\nFollowing Barrett et al. (2016), we used the mono-\nlingual data from the GECO corpus (Cop et al.,\n2017) to generate type-level summary statistics for\neach word. Speciﬁcally, we averaged the gaze fea-\ntures across the 12 participants who completed the\nreading task, and normalised these features to re-\nﬂect the normalisation of the Shared Trask train-\ning data. We then averaged these values again at\nthe type (word) level. For words present in the\ntask training data but not the GECO data, we es-\ntimated the values using means for words in the\nGECO data of a similar frequency (according to\nthe wordfreq).\n4.4 Multi-word Expression Features\nWe generated an MWE lexicon and summary met-\nrics using the Wikitext-103 corpus (Merity et al.,\n2016) and mwetoolkit (Ramisch, 2012). We\nchose Wikitext-103 since it provided a large vari-\nety of possible MWEs in a similar context to the\nZuCo reading material (Hollenstein et al., 2020).\nWe produced two indicator features for the pres-\nence of MWEs: a binary indicator, and a categor-\nical variable summarising the syntactic pattern of\nthe MWE, motivated by Yaneva et al.’s evidence\nthat MWEs of different syntactic patterns display\ndifferent eye-tracking characteristics (2017).\nFollowing the method of Cordeiro et al. (2019),\nwe joined component words of MWEs in Wikitext-\n103 using underscores (i.e. climate change be-\ncame climate_change) and then generated Skip-\ngram word embeddings (Mikolov et al., 2013)\nfor all single words and MWEs identiﬁed in\nWikitext-103. Using the feat_comp function in\nmwetoolkit (Ramisch, 2012), these MWE em-\nbeddings were used to compute compositionality\nscores and weights (Cordeiro et al., 2019). 2\nMWEs identiﬁed in the training data were as-\nsigned MWE embeddings and compositionality\ninformation as features, and non-MWEs were as-\nsigned single word embeddings and zero values for\ncompositionality.\nFigure 1: XLNET Feature Prediction Model\n4.5 XLNet\nIn order to obtain Massively Pre-trained Language\nModel features we used XLNet. We ﬁnetuned a\nmodel that was pre-trained on BooksCorpus (Zhu\net al., 2015), English Wikipedia, Giga5 (Courtney\nNapoles, Matthew R. Gormley, 2012), ClueWeb\n2012-B (Callan et al., 2009), and Common Crawl\ntext (Crawl, 2019). For predictions, we took the\nﬁnal hidden representation of the ﬁrst sub-word\ntoken encoding of each word. We concatenated\nthis feature with an integer representing the total\nword length in characters to encourage the model\nto explicitly attend to word length. We tested the ef-\nfectiveness of sub-word aggregation but found this\n2The score represents the degree to which the meaning of\nthe MWE can be worked out from the meanings of its con-\nstituent words (i.e. ‘climate change’ has high compositionality,\n‘cloud nine’ has low compositionality), and the weights esti-\nmate the semantic contribution of each word in the expression.\n128\nFigure 2: Feature Importance by Target for Model 1 (Left) and Model 2 (Right).\nreduced the model’s accuracy by an average of 0.04\nMAE, which we speculate is due loss of informa-\ntion in the pooling operation whilst head sub-word\nunits already contain contextual information. We\nthen passed the concatenated sub-word and word-\nlength features to a 3-layer dense Neural Network\nwhich was used to predict the Shared Task’s ﬁve\ntarget features. This 3-layer multi-feature Network\nwas found to be optimal through experimentation.\nFor stability, we used the Huber loss objective,\nwhich approximates L2 loss for small values and L1\nloss for large values. We trained using the AdamW\noptimiser and with learning rates and training du-\nration chosen through grid search across 3-fold\ncross-validation, obtaining an optimal learning rate\nof 0.00001 and 800 epochs.\n4.6 Regressors\nTo form predictions for the Feature Rich model we\nused a Random Forest Regressor implemented by\nscikit-learn (Pedregosa et al., 2011) with\nparameters [max_depth = 7, n_estimators =\n100, max_features = None]. For the XLNet\nmodel, we collected the XLNet ﬁnal state embed-\ndings (identical to those fed into the DNN in Figure\n1) along with the features [word-len, CAT-pos,\nzipf-frequency, Is-EOS, Is-SOS]. We then\ntrained scikit-learn’s ElasticNetCV for\n5-fold validation with parameters [max_iter\n= 10000, l1_ratio=[0.1,0.3,0.5,0.7,1],\ncv=5].\n5 Results\nIn Table 1 we present the MAE on validation splits\nof the training data. This information informed our\nchoice of model submissions alongside a prefer-\nence for models using more cognitive features.\nModel/Split 1 2 3 MeanElasticNet(XLNet + ALL Features )3.918 3.9273.8913.912Feature Rich/Model 1 4.017 4.023 3.981 4.007BERT-base-cased 4.030 4.045 3.977 4.012ElasticNet(BERT-base-cased) 3.986 4.024 3.969 3.993XLNet-base-cased 3.988 3.956 3.935 3.959XLNet-base-cased (random init) 4.608 4.722 4.695 4.675XLNet-large-cased 3.929 4.039 3.960 3.976ElasticNet(XLNet-base-cased)/Model 2 3.9213.924 3.896 3.914\nTable 1: Model MAE on Development Splits\nWe submitted two sets of predictions from\nModel 2 ( ElasticNet(XLNet-base-cased)) and one\nset of predictions from Model 1 (Feature Rich).\nTable 2 shows the ranking of Models 1 and 2 in\n129\nRank Team (model) MAE\n1 LAST 3.8134\n2 TALEP 3.8328\n. . .\n5 CogNLP@Shefﬁeld\n(XLNet/Model 2) 3.9565\n. . .\n7 MTL782_IITD 4.0639\n- CogNLP@Shefﬁeld\n(Feature Rich/Model 1) 4.0689\n. . .\n- MEAN BASELINE 7.3699\n13 IIIT_DWD 9.7615\nTable 2: Ranking on the CMCL Shared Task Test Data.\nthe overall task. Our overall standing is shown to\nbe 5th, with an MAE delta of 0.143 behind the\nbest model. Whilst a prediction which combined\nModels 1 and 2 was slightly more accurate (see\nTable 1), we regard this improvement as within\nmargin of error. We therefore focussed on Models\n1 and 2 separately since this allowed for clearer\ncomparisons between the two approaches.\n6 Analysis and Discussion\nOur results (Table 1) support both our hypotheses\nintroduced in Section 1.\nWe did not anticipate that XLNet-base would\noutperform XLNet-large, which had more pre-\ntraining data and layers. This is possibly due to the\nlimited amount of training data speciﬁc to the task\nfor ﬁne-tuning, resulting in the larger model under-\nﬁtting. We are able to conﬁrm that the knowledge\nXLNet learns through massive pre-training crucial\nto its performance in this arena - removal of this\nknowledge through weight randomisation increases\nMAE from 3.959 to 4.675. Hence we believe that\nboth structure and pre-training of XLNet-base con-\ntribute to its success in this task.\nWe use normalised permutation feature impor-\ntance (see Appendix B) to better understand the\nvalue of different features and present it on a per-\ntarget basis for each model in Figure 2.\nThe most interesting outcome of our experiments\nwas the fact that XLNet embeddings subsume infor-\nmation contained across most features except word\nlength (especially in predicting nFix). It may be\nthat the use of word-pieces obfuscate word length\ninformation thus requiring the explicit addition of\nthat information. While the usefulness of features\nsuch as word length is consistent with the literature,\nwe were surprised by the relative unimportance\nof MWE information given that many neurocogni-\ntive studies have demonstrated differences in how\nthey are processed (Siyanova-Chanturia et al., 2011,\n2017; Cacciari and Tabossi, 1988). An additional\nsurprise is that even though the Skip-gram embed-\ndings provide semantic information about single\nwords as well as MWEs, the Feature Rich models\nmake little use of them. Many of the Feature Rich\nmodels utilize the GECO features, which may be\nbecause they provide approximate guidance about\nthe distributions of the various gaze features that\nwould be difﬁcult to learn directly given the spar-\nsity of the training data.\n7 Conclusion and Future Work\nThis work describes our submissions to the 2021\nCMCL Shared Task: we contributed a Feature Rich\nmodel inspired by cognitive and linguistic informa-\ntion, and model predominantly based on contextual\nXLNet-base embeddings. We ﬁnd that only a lim-\nited subset of the cognitive features (such as word\nlength) are helpful in the XLNet model. To our sur-\nprise, neither XLNet-large embeddings nor MWE\nfeatures provide performance improvements. How-\never, we believe this indicates a need for further\nresearch into MWE representations as opposed to\nsuggesting that MWEs are unimportant for creating\neffective cognitive models.\nAcknowledgements\nWe are grateful to Cheng Cao, Elham Khodaei,\nSrivishnu Ethirajulu Krishnaraj and Ronan Ram-\ndas Revadker for their help generating and testing\nthe feature sets. PV and RW are supported by the\nCentre for Doctoral Training in Speech and Lan-\nguage Technologies (SLT) and their Applications\nfunded by the UK Research and Innovation grant\nEP/S023062/1. RW is also supported by ZOO Dig-\nital. This work is also partially supported by the\nEPSRC grant EP/T02450X/1.\nReferences\nMaria Barrett. 2018. Improving natural language pro-\ncessing with human data: Eye tracking and other\ndata sources reﬂecting cognitive text processing .\nPh.D. thesis.\nMaria Barrett, Joachim Bingel, Nora Hollenstein,\nMarek Rei, and Anders Søgaard. 2018. Sequence\nclassiﬁcation with human attention. In Proceedings\nof CoNLL 2018, pages 302–312.\nMaria Barrett, Joachim Bingel, Frank Keller, and An-\nders Søgaard. 2016. Weakly Supervised Part-of-\nspeech Tagging Using Eye-tracking Data. In Pro-\n130\nceedings of ACL 2016: Short Papers , pages 579–\n584, Stroudsburg, PA, USA. Association for Com-\nputational Linguistics.\nE. Blom and S. Unsworth. 2010. Experimental Meth-\nods in Language Acquisition Research . Language\nlearning and language teaching. John Benjamins\nPub. Company.\nLeo Breiman. 2001. Random forests. Machine Learn-\ning, 45(1):5–32.\nMarc Brysbaert, Amy Beth Warriner, and Victor Ku-\nperman. 2014. Concreteness ratings for 40 thousand\ngenerally known English word lemmas. Behavior\nResearch Methods, 46(3):904–911.\nCristina Cacciari and Patrizia Tabossi. 1988. The com-\nprehension of idioms. Journal of Memory and Lan-\nguage, 27:668–683.\nJamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao.\n2009. Clueweb09 data set.\nP. A. Carpenter and M. A. Just. 1983. What your eyes\ndo while your mind is reading. In Keith Rayner, ed-\nitor, Eye movements in reading: Perceptual and lan-\nguage processes, pages 275–307. Academic Press.,\nNew York.\nGrzegorz Chrupała and Afra Alishahi. 2019. Corre-\nlating neural and symbolic representations of lan-\nguage. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 2952–2962, Florence, Italy. Association\nfor Computational Linguistics.\nUschi Cop, Nicolas Dirix, Denis Drieghe, and Wouter\nDuyck. 2017. Presenting GECO: An eyetracking\ncorpus of monolingual and bilingual sentence read-\ning. Behavior Research Methods, 49(2):602–615.\nSilvio Cordeiro, Aline Villavicencio, Marco Idiart, and\nCarlos Ramisch. 2019. Unsupervised composition-\nality prediction of nominal compounds. Computa-\ntional Linguistics, 45(1):1–57.\nBenjamin Van Durme Courtney Napoles, Matthew R.\nGormley. 2012. Annotated English Gigaword. Lin-\nguistic Data Consortium.\nCommon Crawl. 2019. Common Crawl.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nVesna G Djokic, Jean Maillard, Luana Bulat, and Eka-\nterina Shutova. 2020. Decoding Brain Activity As-\nsociated with Literal and Metaphoric Sentence Com-\nprehension Using Distributional Semantic Models.\nTransactions of the Association for Computational\nLinguistics.\nNora Hollenstein, Emmanuele Chersoni, Cassandra\nJacobs, Yohei Oseki, Laurent Prévot, and Enrico\nSantus. 2021. CMCL 2021 Shared Task on Eye-\nTracking Prediction. In Proceedings of the Work-\nshop on Cognitive Modelling and Computational\nLinguistics.\nNora Hollenstein, Jonathan Rotsztejn, Marius Troen-\ndle, Andreas Pedroni, Ce Zhang, and Nicolas Langer.\n2018. Zuco, a simultaneous eeg and eye-tracking re-\nsource for natural sentence reading. Scientiﬁc Data,\n5.\nNora Hollenstein, Marius Troendle, Ce Zhang, and\nNicolas Langer. 2020. ZuCo 2.0: A Dataset of Phys-\niological Recordings During Natural Reading and\nAnnotation. Technical report.\nMatthew Honnibal, Ines Montani, Soﬁe Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy:\nIndustrial-strength Natural Language Processing in\nPython.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nJing Lei, Max G’Sell, Alessandro Rinaldo, Ryan J. Tib-\nshirani, and Larry Wasserman. 2016. Distribution-\nFree Predictive Inference For Regression. Jour-\nnal of the American Statistical Association ,\n113(523):1094–1111.\nEdward Loper and Steven Bird. 2002. Nltk: The natu-\nral language toolkit. CoRR.\nLucas Mentch and Giles Hooker. 2016. Quantifying\nUncertainty in Random Forests via Conﬁdence In-\ntervals and Hypothesis Tests. Journal of Machine\nLearning Research, 17(26):1–41.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer Sentinel Mixture\nModels. Proceedings of ICLR 2017.\nDanny Merkx and Stefan L. Frank. 2020. Comparing\ntransformers and rnns on predicting human sentence\nprocessing data.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. In 1st International Con-\nference on Learning Representations, ICLR 2013 -\nWorkshop Track Proceedings. International Confer-\nence on Learning Representations, ICLR.\nKristin K. Nicodemus, James D. Malley, Carolin Strobl,\nand Andreas Ziegler. 2010. The behaviour of ran-\ndom forest permutation-based variable importance\nmeasures under predictor correlation. BMC Bioin-\nformatics, 11(1):110.\n131\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research ,\n12:2825–2830.\nCarlos Ramisch. 2012. A generic and open framework\nfor multiword expressions treatment: from acquisi-\ntion to applications. In Proceedings of the ACL 2012\nStudent Research Workshop, September, pages 61–\n66. Association for Computational Linguistics.\nKeith Rayner. 1975. The perceptual span and periph-\neral cues in reading. Cognitive Psychology, 7(1):65–\n81.\nKeith Rayner. 1998. Eye Movements in Reading and\nInformation Processing: 20 Years of Research. Psy-\nchological Bulletin, 124(3):372–422.\nKeith Rayner and George W. McConkie. 1976. What\nguides a reader’s eye movements? Vision Research,\n16(8):829–837.\nRadim Rehurek and Petr Sojka. 2011. Gensim–python\nframework for vector space modelling. NLP Centre,\nFaculty of Informatics, Masaryk University, Brno,\nCzech Republic, 3(2).\nOmid Rohanian, Shiva Taslimipoor, Victoria Yaneva,\nand Le An Ha. 2017. Using Gaze Data to Predict\nMultiword Expressions. In Proceedings of RANLP\n2017, pages 601–609.\nAnna Siyanova-Chanturia, Kathy Conklin, Sendy Caf-\nfarra, Edith Kaan, and Walter J.B. van Heuven. 2017.\nRepresentation and processing of multi-word expres-\nsions in the brain. Brain and Language , 175:111–\n122.\nAnna Siyanova-Chanturia, Kathy Conklin, and Norbert\nSchmitt. 2011. Adding more fuel to the ﬁre: an eye-\ntracking study of idiom processing by native and\nnon-native speakers. Second Language Research ,\n27(2):251–272.\nEkta Sood, Simon Tannert, Diego Frassinelli, Andreas\nBulling, and Ngoc Thang Vu. 2020. Interpreting At-\ntention Models with Human Visual Attention in Ma-\nchine Reading Comprehension. In Proceedings of\nCoNLL 2020, pages 12–25, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nRobyn Speer, Joshua Chin, Andrew Lin, Sara Jew-\nett, and Lance Nathan. 2018. Luminosoin-\nsight/wordfreq: v2.2.\nMichael K Tanenhaus. 2007. Spoken language compre-\nhension: Insights from eye movements. The oxford\nhandbook of psycholinguistics.\nMakarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,\nAntonio Torralba, Raquel Urtasun, and Sanja Fi-\ndler. 2016. MovieQA: Understanding Stories in\nMovies through Question-Answering. In IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR).\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nWalter J B Van Heuven, Pawel Mandera, Emmanuel\nKeuleers, and Marc Brysbaert. 2014. SUBTLEX-\nUK: A new and improved word frequency database\nfor British English. The Quarterly Journal of Exper-\nimental Psychology, 67(6):1176–1190.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nVictoria Yaneva, Shiva Taslimipoor, Omid Rohanian,\nand Le An Ha. 2017. Cognitive Processing of Mul-\ntiword Expressions in Native and Non-native Speak-\ners of English: Evidence from Gaze Data. Technical\nreport.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems , volume 32. Curran\nAssociates, Inc.\nY Zhu, R Kiros, R Zemel, R Salakhutdinov, R Urta-\nsun, A Torralba, and S Fidler. 2015. Aligning Books\nand Movies: Towards Story-Like Visual Explana-\ntions by Watching Movies and Reading Books. In\n2015 IEEE International Conference on Computer\nVision (ICCV), pages 19–27.\n132\nA Features Used\nWe use the following features for each model. +N\nand +P indicate that associated data for the two\nnext and two preceding words were included, re-\nspectively.\nA.1 Model One Features\n[CAT-pos+N+P, CAT-word+N+P,\nConc-M+N+P, Conc-SD+N+P,\nIs-EOS+N+P, Is-SOS+N+P,\nPercent-Known+N+P,\ncomp-score+N+P, comp-weights+N+P,\ngeco-FFD-mean+N+P,\ngeco-FFD-std+N+P,\ngeco-GPT-median+N+P,\ngeco-GPT-std+N+P,\ngeco-TRT-mean+N+P,\ngeco-fixProp-mean+N+P,\ngeco-fixProp-std+N+P,\ngeco-nFix-median+N+P,\ngeco-nFix-std+N+P,\nis-mwe+N+P, is-strange+N+P,\nmwe-cat+N+P, saccade-cat+N+P,\nsaccade-cat-binary+N+P,\nw2v-embedding+N+P,\nword-frequency+N+P, word-len+N+P,\nzipf-frequency+N+P]\nA.2 Model Two Features\n[XLNET-embed, CAT-pos, Is-EOS,\nIs-SOS, word-len, zipf-frequency]\nB Permutation Feature Importance\nWe use permutation feature importance (Breiman,\n2001) to better understand the impact of differ-\nent features on each of the different models. This\nmethod measures the base error of the model\nagainst the error when one feature is randomly per-\nmuted, allowing for quantiﬁcation of importance.\nThat is for feature i:\nFIi = Ebase − Epermi\nWe note that permutation methods have a ten-\ndency of attributing higher importance to corre-\nlated features (Nicodemus et al., 2010), whilst\nstill being informative. Alternatives include per-\nfeature retraining (Lei et al., 2016; Mentch and\nHooker, 2016) which was computationally in-\ntractable within the timeframe of the CMCL task\nduration.\n133\nC Description of features\nFeature (generated at the\nword-level unless speciﬁed)Description Data and tools used\nCAT_word One hot word encoding\nCAT_pos Categorical encoding of Part-of-Speech tag Honnibal et al. (2020)\nIs_EOS Binary variable indicating if word is the last in its sentence\nIs_SOS Binary variable indicating if word is the ﬁrst in its sentence\nConc_M\nMean concreteness norm assigned to the lemmatized form of the word.\nWords not covered by the dataset of norms were given a’neutral’ score\nof 3 (concreteness rated on a Likert scale from 1-5)\nBrysbaert et al. (2014)\nConc_SD\nStandard deviation of concreteness values assigned to lemmatized form\nof word. Words not covered by the dataset of norms were assigned the\nmean ofConc_SDfor all other words\nBrysbaert et al. (2014)\nPercent_Known\nProportion of participants asked to estimate concreteness norms that\nwere familiar enough with the word to judge its concreteness. Words\nnot covered by the dataset of norms were assigned a value of 1\nBrysbaert et al. (2014)\nword_len Number of characters in the word\nsaccade_cat Categorical representation of number of characters in relation to average\nsaccade length (categories were 1-3, 4-7, 8-10 and 11+ letters)\nsaccade _cat_binaryBinary categorical representation of number of characters in relation to\naverage saccade length (categories were 1-3 letters and 4+ letters)\nword_frequency Frequency of word per million words Speer et al. (2018)\nzipf_frequency Frequency of word per million words on the zipf scale Speer et al. (2018)\nNEXT_n_FEAT\nAttachesFEATfor the nextnwords to the current word\n(i.e.NEXT_1_Is_EOSattachesIs_EOSfor the next word to the\ncurrent word)\nPREV_n_FEAT AttachesFEATfor the previousnwords to the current word\ngeco_FEAT_mean\nMean average of all measurements ofFEATfor this word in GECO. If\nthe word was not present in GECO, the mean of means for words with\ncomparable frequency in natural language was used\nCop et al. (2017)\ngeco_FEAT_median\nMedian average of all measurements ofFEATfor this word GECO. If\nthe word was not present in GECO, the mean of medians for words with\ncomparable frequency was used\nCop et al. (2017)\ngeco_FEAT_std\nStandard deviation of all measurements ofFEATfor this word in GECO.\nIf the word was not present in GECO, mean of standard deviations for\nwords with comparable frequency was used\nCop et al. (2017)\nis_mwe Binary indicator showing if word is part of an MWE in this context Ramisch (2012)\nmwe_cat\nCategorical representation of whether the word is part of an MWE in this\ncontext, where categories are based on syntactic patterns (i.e. adjective\nnoun compound, verb + preposition phrase)\nRamisch (2012)\nLoper and Bird (2002)\nw2v_embedding\n300 dimensional Skip-gram embedding for the word or MWE. If the\nword is part of an MWE in this context, the Skip-gram embedding trained\nfor the MWE is used instead. Embeddings are trained using the\nWikitext-103 corpus, where multiword expressions are reformatted to be\nconcatenated using underscores (i.e.multiword_expression)\nRamisch (2012)\nMikolov et al. (2013)\nRehurek and Sojka (2011)\nMerity et al. (2016)\ncomp_score Compositionality score for the MWE calculated usingmwetoolkit.\nWords not part of MWEs are assigned a value of 0\nRamisch (2012)\nCordeiro et al. (2019)\ncomp_weights\nWeights used for each word to calculate the comp_score for the MWE\n(certain words may contribute more semantic meaning to an MWE than\nothers). Words not part of MWEs are assigned a value of 0\nRamisch (2012)\nCordeiro et al. (2019)\nis_strange Binary indicator of non-standard formatting or non-alphanumeric\ncharacters in the current word (generated using regular expressions)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7691227197647095
    },
    {
      "name": "Transformer",
      "score": 0.751396894454956
    },
    {
      "name": "Task (project management)",
      "score": 0.6348981857299805
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5875717401504517
    },
    {
      "name": "Eye tracking",
      "score": 0.585547685623169
    },
    {
      "name": "Cognition",
      "score": 0.5149789452552795
    },
    {
      "name": "Artificial intelligence",
      "score": 0.509332537651062
    },
    {
      "name": "Natural language processing",
      "score": 0.48150548338890076
    },
    {
      "name": "Tracking (education)",
      "score": 0.4691756069660187
    },
    {
      "name": "Language model",
      "score": 0.4320933520793915
    },
    {
      "name": "Word (group theory)",
      "score": 0.42334532737731934
    },
    {
      "name": "Neuroscience",
      "score": 0.08794459700584412
    },
    {
      "name": "Linguistics",
      "score": 0.08530417084693909
    },
    {
      "name": "Psychology",
      "score": 0.07865360379219055
    },
    {
      "name": "Engineering",
      "score": 0.0642855167388916
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}