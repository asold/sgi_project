{
  "title": "End-to-End Video Instance Segmentation with Transformers",
  "url": "https://openalex.org/W3108995912",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1584106083",
      "name": "Wang, Yuqing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2533326077",
      "name": "Xu Zhao-liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A780421308",
      "name": "Wang Xinlong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2360656043",
      "name": "Shen, Chunhua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2356386349",
      "name": "Cheng Bao-shan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096269477",
      "name": "Shen Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2708786869",
      "name": "Xia, Huaxia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3106546328",
    "https://openalex.org/W3034681942",
    "https://openalex.org/W2916797271",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2963849369",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2603203130",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2982723417",
    "https://openalex.org/W3100039191",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W607748843",
    "https://openalex.org/W3034549805",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W3109372619",
    "https://openalex.org/W3113410735",
    "https://openalex.org/W2470139095",
    "https://openalex.org/W2962825871",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W3034499084",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2964086649",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2194775991"
  ],
  "abstract": "Video instance segmentation (VIS) is the task that requires simultaneously classifying, segmenting and tracking object instances of interest in video. Recent methods typically develop sophisticated pipelines to tackle this task. Here, we propose a new video instance segmentation framework built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel sequence decoding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly. At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspective of similarity learning, thus considerably simplifying the overall pipeline and is significantly different from existing approaches. Without bells and whistles, VisTR achieves the highest speed among all existing VIS models, and achieves the best result among methods using single model on the YouTube-VIS dataset. For the first time, we demonstrate a much simpler and faster video instance segmentation framework built upon Transformers, achieving competitive accuracy. We hope that VisTR can motivate future research for more video understanding tasks.",
  "full_text": "End-to-End Video Instance Segmentation with Transformers\nYuqing Wang1, Zhaoliang Xu1, Xinlong Wang2, Chunhua Shen2, Baoshan Cheng1, Hao Shen1*, Huaxia Xia1\n1 Meituan 2 The University of Adelaide, Australia\nyuqingwang1029@gmail.com, shenhao04@meituan.com\nCNN transformer \nencoder-decoder \nno object \n sequence\nof images\nsequence of multiple\nimage features\nsequence of object\npredictions\nFigure 1– Overall pipeline of VisTR.The model takes a sequence of images as input and outputs a sequence of instance predictions.\nHere same shapes represent predictions in one image, and same colors represent predictions of the same object instance. Note that the\noverall predictions follow the input frame order, and the order of object predictions for different images keeps the same (Best viewed on\nscreen).\nAbstract\nVideo instance segmentation (VIS) is the task that re-\nquires simultaneously classifying, segmenting and tracking\nobject instances of interest in video. Recent methods typ-\nically develop sophisticated pipelines to tackle this task.\nHere, we propose a new video instance segmentation frame-\nwork built upon Transformers, termed VisTR, which views\nthe VIS task as a direct end-to-end parallel sequence de-\ncoding/prediction problem. Given a video clip consisting of\nmultiple image frames as input, VisTR outputs the sequence\nof masks for each instance in the video in order directly.\nAt the core is a new, effective instance sequence matching\nand segmentation strategy, which supervises and segments\ninstances at the sequence level as a whole. VisTR frames\nthe instance segmentation and tracking in the same perspec-\ntive of similarity learning, thus considerably simplifying the\noverall pipeline and is signiﬁcantly different from existing\napproaches.\nWithout bells and whistles, VisTR achieves the highest\n*Corresponding author.\nspeed among all existing VIS models, and achieves the best\nresult among methods using single model on the YouTube-\nVIS dataset. For the ﬁrst time, we demonstrate a much\nsimpler and faster video instance segmentation framework\nbuilt upon Transformers, achieving competitive accuracy.\nWe hope that VisTR can motivate future research for more\nvideo understanding tasks.\nCode is available at: https://git.io/VisTR\n1. Introduction\nInstance segmentation is one of the fundamental tasks in\ncomputer vision. While signiﬁcant progress has been wit-\nnessed in instance segmentation of images [5, 9, 22, 25–27],\nmuch less effort was spent on segmenting instances in\nvideos. Here we propose a new video instance segmentation\nframework built upon Transformers. Video instance seg-\nmentation (VIS), recently proposed in [30], requires one to\nsimultaneously classify, segment and track object instances\nof interest in a video sequence. It is more challenging in that\none needs to perform instance segmentation for each indi-\narXiv:2011.14503v5  [cs.CV]  8 Oct 2021\nvidual frame and at the same time to establish data associa-\ntion of instances across consecutive frames,a.k.a., tracking.\nState-of-the-art methods typically develop sophisticated\npipelines to tackle this task. Top-down approaches [2, 30]\nfollow the tracking-by-detection paradigm, relying heav-\nily on image-level instance segmentation models [6, 9] and\ncomplex human-designed rules to associate the instances.\nBottom-up approaches [1] separate object instances by clus-\ntering learned pixel embeddings. Due to heavy reliance on\nthe dense prediction quality, these methods often need mul-\ntiple steps to generate the masks iteratively, which makes\nthem slow. Thus, a simple, end-to-end trainable VIS frame-\nwork is highly desirable.\nHere, we take a deeper look at the video instance seg-\nmentation task. Video frames contain richer information\nthan single images such as motion patterns and temporal\nconsistency of instances, offering useful cues for instance\nsegmentation, and classiﬁcation. At the same time, the bet-\nter learned instance features can help tracking of instances.\nIn essence, the instance segmentation and instance tracking\nare both concerned with similarity learning: instance seg-\nmentation is to learn the pixel-level similarity and instance\ntracking is to learn the similarity between instances. Thus,\nit is natural to solve these two sub-tasks in a single frame-\nwork and beneﬁt each other. Here we aim to develop such\nan end-to-end VIS framework. The framework needs to be\nsimple and achieves strong performance without whistles\nand bells. To this end, we propose to employ the Trans-\nformers [23]. Importantly, for the ﬁrst time we demon-\nstrate that, as the Transformers provide building blocks, it\nenables one to design a simple and clean framework for\nVIS, and possibly for a much wider range of video process-\ning tasks in computer vision. Thus potentially, it is possible\nto unify most vision tasks of different input modalities—\nsuch as image, video and point clouds processing—into the\nTransformer framework. Transformers are widely used for\nsequence to sequence learning in NLP [23], and start to\nshow promises in vision [4, 8]. Transformers are capable\nof modeling long-range dependencies, and thus can be nat-\nurally applied to video for learning temporal information\nacross multiple frames. In particular, the core mechanism\nof Transformers, self-attention, is designed to learn and up-\ndate the features based on all pairwise similarities between\nthem. The above characteristics of Transformers make them\ngreat candidates for the VIS task.\nIn this paper, we propose the Video Instance Segmen-\ntation TRansformer (VisTR), which views the VIS task as\na parallel sequence decoding/prediction problem. Given a\nvideo clip that consists of multiple image frames as input,\nthe VisTR outputs the sequence of masks for each instance\nin the video in order directly. The output sequence for each\ninstance is referred to as instance sequence in this paper.\nThe overall VisTR pipeline is illustrated in Fig. 1. In the ﬁrst\nstage, given a sequence of video frames, a standard CNN\nmodule extracts features of individual image frames, then\nthe multiple image features are concatenated in the frame\norder to form the clip-level feature sequence. In the second\nstage, the Transformer takes the clip-level feature sequence\nas input, and outputs a sequence of object predictions in or-\nder. In Fig. 1 same shapes represent predictions for the same\nimage, and the same colors represent the same instance of\ndifferent images. The sequence of predictions follow the\norder of input images, and the predictions of each image\nfollows the same instance order. Thus, instance tracking is\nachieved seamlessly and naturally in the same framework\nof instance segmentation.\nTo achieve this goal, there are two main challenges: 1)\nhow to maintain the order of outputs and 2) how to obtain\nthe mask sequence for each instance out of the Transformer\nnetwork. Correspondingly, we introduce the instance se-\nquence matching strategy and the instance sequence seg-\nmentation module. The instance sequence matching per-\nforms bipartite graph matching between the output instance\nsequence and the ground-truth instance sequence, and su-\npervises the sequence as a whole. Thus, the order can be\nmaintained directly. The instance sequence segmentation\naccumulates the mask features for each instance across mul-\ntiple frames through self-attention and segments the mask\nsequence for each instance through 3D convolutions.\nOur main contributions are summarized as follows.\n• We propose a new video instance segmentation frame-\nwork built upon Transformers, termed VisTR, which\nviews the VIS task as a direct end-to-end parallel se-\nquence decoding/prediction problem. The framework\nis signiﬁcantly different from existing approaches,\nconsiderably simplifying the overall pipeline.\n• VisTR solves the VIS from a new perspective of sim-\nilarity learning. Instance segmentation is to learn the\npixel-level similarity and instance tracking is to learn\nthe similarity between instances. Thus, instance track-\ning is achieved seamlessly and naturally in the same\nframework of instance segmentation.\n• The key to the success of VisTR is a new strategy for\ninstance sequence matching and segmentation , which\nis tailored for our framework. This carefully-designed\nstrategy enables us to supervise and segment instances\nat the sequence level as a whole.\n• VisTR achieves strong results on the YouTube-VIS\ndataset, achieving 40.1% in mask mAP at the speed of\n57.7 FPS , which is the best and fastest among methods\nthat use a single model.\n2. Related work\nVideo object segmentation. VOS [18] is closely related\nto VIS. Analogue to object tracking, which is detecting\nboxes of foreground objects in a class-agnostic fashion,\nVOS is segmenting masks of foreground class-agnostic ob-\njects. Same as in tracking, usually one is allowed to use\nonly the ﬁrst few frames’ annotations for training. In con-\ntrast, VIS requires to segment and track all instance masks\nof a ﬁxed category set of objects in a video sequence.\nVideo instance segmentation.The VIS task [30] requires\nclassifying, segmenting instances in each frame and linking\nthe same instance across frames. State-of-the-art methods\ntypically develop sophisticated pipelines to tackle it. Mask-\nTrack R-CNN [30] extends the Mask R-CNN [9] with a\ntracking branch and external memory that saves the features\nof instances across multiple frames. Maskprop [2] builds on\nthe Hybrid Task Cascade Network [6], and re-uses the pre-\ndicted masks to crop the extracted features, then propagates\nthem temporally to improve the segmentation and tracking.\nSTEm-Seg [1] proposes to model video clips as 3D space-\ntime volumes and then separates object instances by clus-\ntering learned embeddings. Note that the above approaches\neither rely on complex heuristic rules to associate the in-\nstances or require multiple steps to generate and optimize\nthe masks iteratively. In contrast, here we aim to build a\nsimple and end-to-end trainable VIS framework.\nTransformers. Transformers were ﬁrst proposed in [23]\nfor the sequence-to-sequence machine translation task, and\nsince then have become the de facto method in most NLP\ntasks. The core mechanism of Transformers, self-attention,\nmakes it particularly suitable for modeling long-range de-\npendencies. Very recently, Transformers start to show\npromises in solving computer vision tasks. DETR [4]\nbuilds an object detection systems based on Transformers,\nwhich largely simpliﬁes the traditional detection pipeline,\nand achieves on par performances compared with highly-\noptimized CNN based detectors [19]. Our work here is\ninspired by DETR. ViT [8] introduces the Transformer to\nimage recognition and models an image as a sequence of\npatches, which attains excellent results compared to state-\nof-the-art convolutional networks. The above works show\nthe effectiveness of Transformers in image understanding\ntasks. To our knowledge, thus far there are no prior appli-\ncations of Transformers to video instance segmentation. It\nis intuitive to see that the Transformers’ advantage of mod-\neling long-range dependencies makes it an ideal candidate\nfor learning temporal information across multiple frames\nfor video understanding tasks. Here, we propose the VisTR\nmethod and provide an afﬁrmative answer to that. As the\noriginal Transformers are auto-regressive models, which\ngenerate output tokens one by one, for efﬁciency, VisTR\nemploys a non-auto-regressive variant of the Transformer\nto achieve parallel sequence generation.\n3. Our Method: VisTR\nWe tackle the video instance segmentation task by mod-\neling it as a direct sequence prediction problem. Given a\nvideo clip that consists of multiple image frames as input,\nthe VisTR outputs the sequence of masks for each instance\nin the video in order. To achieve this goal, we introduce\nthe instance sequence matching and segmentation strategy\nto supervise and segment the instances at the sequence level\nas a whole. In this section, we ﬁrst introduce the overall\narchitecture of the proposed VisTR in Sec. 3.1, then the de-\ntails of the instance sequence matching and segmentation\nmodule will be described in Sec. 3.2 and Sec. 3.3 respec-\ntively.\n3.1. VisTR Architecture\nThe overall VisTR architecture is depicted in Fig. 2. It\ncontains four main components: a CNN backbone to ex-\ntract compact feature representations of multiple frames,\nan encoder-decoder Transformer to model the similarity of\npixel-level and instance-level features, an instance sequence\nmatching module for supervising the model, and an instance\nsequence segmentation module.\nBackbone. The backbone extracts the original pixel-level\nfeature sequence of the input video clip. Assume that the\ninitial video clip with T frames of resolution H0 ×W0 is\ndenoted by xclip ∈RT×3×H0×W0 . First, a standard CNN\nbackbone generates a lower-resolution activation map for\neach frame, then the features for each frame are concate-\nnated to form the clip level feature mapf0 ∈RT×C×H×W.\nTransformer encoder. The Transformer encoder is em-\nployed to model the similarities among all the pixel level\nfeatures in the clip. First, a 1 ×1 convolution is applied to\nthe above feature map, reducing the dimension from Cto d\n(d<C ), resulting in a new feature map f1 ∈RT×d×H×W.\nTo form a clip level feature sequence that can be fed into\nthe Transformer encoder, we ﬂatten the spatial and tempo-\nral dimensions of f1 into one dimension, resulting in a 2D\nfeature map of size d×(T·H·W). Note that the temporal\norder is always in accordance with that of the initial input.\nEach encoder layer has a standard architecture that consists\nof a multi-head self-attention module and a fully connected\nfeed forward network (FFN).\nTemporal and spatial positional encoding.The Trans-\nformer architecture is permutation-invariant, while the seg-\nmentation task requires precise position information. To\ncompensate for this, we supplement the features with ﬁxed\npositional encodings information that contains the three di-\nmensional (temporal, horizontal and vertical) positional in-\nformation in the clip. Here we adapt the positional encoding\nin the original Transformer [23] for our 3D case. Specif-\nically, for the coordinates of each dimension we indepen-\ndently use d/3 sine and cosine functions with different fre-\nCNN \ntransformer\tencoder \nsequence\tof\tmultiple\nimage\tfeatures\nsequence\tof\tinstance\tqueries\nbackbone \ntransformer\tdecoder \nframe\t1 frame\t2 frame\t3 sequence\tof\timage\tfeatures\ninstance\t1 instance\t4\nsequence\tof\tencoded\tfeatures\nsequence\tof\tinstance\tpredictions\nencoder decoder \nFFN\nGT\t i\t \tseq GT\t j \tseq \ninstance\tsequence\tmatching \nbipartite\tmatching\nins1\tseq ins4\tseq \npositional\tencoding\nE O B \nE \nB \n3D\tconv\ninstance\tsequence\tsegmentation \nO instance\nmask\nsequence\nself-attention\nFigure 2– The overall architecture of VisTR.It contains four main components: 1) a CNN backbone that extracts feature representation\nof multiple images; 2) an encoder-decoder Transformer that models the relations of pixel-level features and decodes the instance-level\nfeatures; 3) an instance sequence matching module that supervises the model; and 4) an instance sequence segmentation module that\noutputs the ﬁnal mask sequences (Best viewed on screen).\nquencies:\nPE(pos,i) =\n{\nsin\n(\npos ·ωk\n)\n, for i= 2k,\ncos\n(\npos ·ωk\n)\n, for i= 2k+ 1; (1)\nwhere ωk = 1/100002k/d\n3 ; ‘pos’ is the position in the cor-\nresponding dimension. Note that the dshould be divisible\nby 3, as the positional encodings of the three dimensions\nshould be concatenated to form the ﬁnal d channel posi-\ntional encoding. These encodings are added to the input of\neach attention layer.\nTransformer decoder. The Transformer decoder aims to\ndecode the top pixel features that can represent the instances\nof each frame, which is called instance level features. Mo-\ntivated by DETR [4], we also introduce a ﬁxed number\nof input embeddings to query the instance features from\npixel features, termed as instance queries. Suppose that the\nmodel decodes n instances each frame, then for T frames\nthe instance query number is N = n ·T. The instance\nqueries are learned by the model and have the same dimen-\nsion with the pixel features. Taking the output of encoder\nE and N instance queries Qas input, the Transformer de-\ncoder outputs N instance features, denoted by Oin Fig. 2.\nThe overall predictions follow the input frame order, and\nthe order of instance predictions for different images is the\nsame. Thus, the tracking of instances in different frames\ncould be realized by linking the items of the corresponding\nindices directly.\n3.2. Instance Sequence Matching\nVisTR infers a ﬁxed-size sequence ofN predictions, in a\nsingle pass through the decoder. One of the main challenges\nfor this framework is to maintain the relative position of pre-\ndictions for the same instance in different images,a.k.a., in-\nstance sequence. In order to ﬁnd the corresponding ground\ntruth and supervise the instance sequence as a whole, we\nintroduce the instance sequence matching strategy.\nAs the VisTR decode ninstances each frame, the num-\nber of instance sequence is also n. Let us denote by ˆy =\n{ˆyi}n\ni=1 the predicted instance sequences, and ythe ground\ntruth set of instance sequences. Assuming nis larger than\nthe number of instances in the video clip, we consider y\nalso as a set of size npadded with ∅. In order to ﬁnd a bi-\npartite graph matching between the two sets, we search for\na permutation of nelements σ∈Sn with the lowest cost:\nˆσ= arg min\nσ∈Sn\nn∑\ni\nLmatch\n(\nyi,ˆyσ(i)\n)\n(2)\nwhere Lmatch\n(\nyi,ˆyσ(i)\n)\nis a pair-wise matching cost be-\ntween ground truth yi and an instance sequence prediction\nwith index σ(i). The optimal assignment could be com-\nputed efﬁciently by the Hungarian algorithm [11], follow-\ning prior work (e.g., [21]).\nAs computing the mask sequence similarity directly is\ncomputationally intensive, we ﬁnd a surrogate, the box se-\nquence to perform the matching. To obtain the box predic-\ntions, we apply a 3-layer feed forward network (FFN) with\nReLU activation function and a linear projection layer to\nthe object predictions O of Transformer decoder. Follow-\ning the same practice of DETR [4], the FFN predicts the\nnormalized center coordinates, height and width of the box\nw.r.t. input image, and the linear layer predicts the class la-\nbel using a softmax function. We also add a “background”\nclass to represent that no object is detected.\nGiven the N = n·T bounding box predictions for the\nobject predictions sequence, we could associate n box se-\nquences for each instance by their indices, referred to as\nins1 box seq...ins4 box seq in Fig. 2. The matching loss\ntakes both the class predictions and the similarity of pre-\ndicted and ground truth boxes into account. Each element i\nof the ground truth set can be seen as\nyi = {(ci,ci...,ci),(bi,0,bi,1...,bi,T)} (3)\nwhere ciis the target class label (which may be∅) for this in-\nstance, and bi,t ∈[0,1]4 is a vector that deﬁnes ground truth\nbox center coordinates and its relative height and width in\nthe frame t. T represent the number of input frames. Thus,\nfor the predictions of instance with index σ(i) we denoted\nthe probability of class ci as\nˆp(σ(i))(ci) ={ˆp(σ(i),0)(ci)...,ˆp(σ(i),T)(ci)} (4)\nand the predicted box sequence as\nˆbσ(i) =\n{\nˆb(σ(i),0),ˆb(σ(i),1)...,ˆb(σ(i),T)\n}\n(5)\nWith the above notation, we deﬁne\nLmatch\n(\nyi,ˆyσ(i)\n)\n= −ˆpσ(i) (ci) +Lbox\n(\nbi,ˆbσ(i)\n)\n, (6)\nwhere ci ̸= ∅. Based on the above criterion, we could\nﬁnd the one-to-one matching of the sequences by the Hun-\ngarian algorithm. Given the optimal assignment, we could\ncompute the loss function, the Hungarian loss for all pairs\nmatched in the previous step. The loss is a linear combina-\ntion of a negative log-likelihood for class prediction, a box\nloss and mask loss for the instance sequences:\nLHung(y,ˆy) =\nN∑\ni=1\n[\n(−log ˆpˆσ(i)(ci)) +Lbox (bi,ˆbˆσ(i))\n+ Lmask (mi, ˆmˆσ(i))\n]\n. (7)\nHere ci ̸= ∅, and ˆσ is the optimal assignment computed\nin Eq. (2). The Hungarian loss is used to train the whole\nframework.\nThe second part of the matching cost and the Hungarian\nloss is Lbox that scores the bounding boxes. We use a linear\ncombination of the sequence level L1 loss and the general-\nized IOU loss [20]:\nLbox\n(\nbi,ˆbσ(i)\n)\n= 1\nT\nT∑\nt=1\n[\nλiou ·Liou\n(\nbi,t,ˆbσ(i),t\n)\n+ λL1\nbi,t −ˆbσ(i),t\n\n1\n]\n. (8)\nHere λiou,λL1 ∈R are hyper-parameters. These two losses\nare normalized by the number of instances inside the batch.\nIn the sequel, we present the details.\n3.3. Instance Sequence Segmentation\nThe instance sequence segmentation module aims to pre-\ndict the mask sequence for each instance. To realize that,\nthe model needs to accumulate the mask features of multi-\nple frames for each instance ﬁrstly, then the mask sequence\nsegmentation is performed on the accumulated features.\nThe mask features are obtained by computing the simi-\nlarity map between the object predictions Oand the Trans-\nformer encoded features E. To simplify the calculation, we\nonly compute with the features of its corresponding frame\nfor each object prediction. For each frame, the object pre-\ndictions Oand the corresponding encoded feature maps E\nare fed into the self-attention module to obtain the initial\nattention maps. Then the attention maps will be fused with\nthe initial backbone featuresBand the transformed encoded\nfeatures Eof the corresponding frames, following a similar\npractice with the DETR [4]. The last layer of the fusion is\na deformable convolution layer [7]. In this way, the mask\nfeatures for each instance of different frames are obtained.\nFollowing the same spirit of taking the instance sequence\nas a whole, the mask features of the same instance in\ndifferent frames should be propagated and reinforce each\nother. We propose to utilize the 3D convolution to real-\nize that. Assume that the mask feature for instance i of\nframe t is gi,t ∈ R1×a×H0/4×W0/4, where a is the chan-\nnel number, then we concatenate the features of T frames\nto form the Gi ∈ R1×a×T×H0/4×W0/4. The instance se-\nquence segmentation module takes the instance sequence\nmask feature Gi as input, and output the mask sequence\nmi ∈ R1×1×T×H0/4×W0/4 for the instance directly. This\nmodule contains three 3D convolutional layers and Group\nNormalization [29] layers with ReLU activation function.\nNo normalization or activation is performed after the last\nconvolution layer, and the output channel number of the\nlast layer is 1. In this way, the masks of the instance for\nT frames are obtained. The mask loss for supervising the\npredictions in Eq. (7) is deﬁned as a combination of the\nDice [16] and Focal loss [13]:\nLmask\n(\nmi, ˆmσ(i)\n)\n= λmask\n1\nT\nT∑\nt=1\n[\nLDice(mi,t, ˆmσ(i),t)\n+ LFocal(mi,t, ˆmσ(i),t)\n]\n. (9)\n4. Experiments\nIn this section, we conduct experiments on the YouTube-\nVIS [30] dataset, which contains 2238 training, 302 valida-\ntion and 343 test video clips. Each video of the dataset is\nannotated with per pixel segmentation mask, category and\ninstance labels. The object category number is 40. As the\ntest set evaluation is closed, we evaluate our method in the\nvalidation set. The evaluation metrics are average precision\n(AP) and average recall (AR), with the video Intersection\nover Union (IoU) of the mask sequences as the threshold.\n4.1. Implementation Details\nModel settings. As the largest number of the annotated\nvideo length for YouTube-VIS [30] is 36, we take this value\nas the default input video clip length T. Thus, no post-\nprocessing is needed to associate different clips from one\nvideo, which makes our model totally end-to-end trainable.\nThe model predicts 10 objects for each frame, thus the total\nobject query number is 360. For the Transformer we use\n6 encoder, 6 decoder layers of width 384 with 8 attention\nheads. Unless otherwise speciﬁed, ResNet-50 [10] is used\nas our backbone networks and the same hyper-parameters\nof DETR [4] are used.\nTraining. The model is implemented with PyTorch-1.6\n[17], trained with AdamW [15] of initial Transformer’s\nlearning rate being 10−4 , the backbone’s learning rate be-\ning 10−5. The models are trained for 18 epochs, with the\nlearning rate decays by 10x at 12 epochs. We initialize our\nbackbone networks with the weights of DETR pretrained\non COCO [14]. The models are trained on 8 V100 GPUs of\n32G RAM, with 1 video clip per GPU. The frame sizes are\ndownsampled to 300×540 to ﬁt the GPU memory.\nInference. During inference, we follow the same scale set-\nting as training. No post-processing is needed for associ-\nating instances. Instances with scores larger than 0.001 are\nkept. The mean score for all the frames is used as the in-\nstance score. For instances that have been classiﬁed to dif-\nferent categories in different frames, we use the most fre-\nquently predicted category as the ﬁnal instance category.\n4.2. Ablation Study\nIn this section we conduct extensive ablation experi-\nments to study the core factors of VisTR. Comparison re-\nsults are reported in Table 1.\nThe main difference between video and image is that\nvideo contains temporal information. How to effectively\nlearn and exploit temporal information is the key to video\nunderstanding. Firstly, we study the importance of tempo-\nral information to VisTR in two dimensions: the amount\nand the order.\nVideo sequence length.To evaluate the importance of the\namount of temporal information to VisTR, we experiment\nwith models trained with different input video sequence\nlengths. As reported in Table 1a, with the length varying\nfrom 18 to 36, the AP increases monotonically from 29.7%\nto 33.3%. This result shows that more temporal information\nindeed helps the model learn better. As the largest video\nlength of the dataset is 36, we argue that, if with a larger\ndataset, VisTR can achieve even better results. Note that\nfor this experiment, if the clip length is less than the video\nlength, instance matching in overlapping frames is used for\nassociating them from different clips.\nVideo sequence order.As the movement of objects in real\nscenes are continuous, we believe that the order of temporal\ninformation is also important. To evaluate, we perform a\ncomparison of the model trained with input video sequence\nin random order vs. time order. Results in Table 1c show\nthat the model learned according to the time order informa-\ntion achieves 1 point higher, which veriﬁes the importance\nof the temporal order.\nPositional encoding.Position information is important for\nthe dense prediction problem of VIS. As the original fea-\nture sequence contains no positional information, we sup-\nplement with the spatial and temporal positional encodings,\nwhich indicate the relative positions in the video sequence.\nExperiments of models with and without positional encod-\ning are presented in Table 1d. The model without positional\nencoding manages to achieve 28.4% AP. Our explanation\nis that the ordered format of the sequence supervision and\nthe correspondence between the input and output order of\nthe Transformer provide some relative positional informa-\ntion implicitly. In the second experiment, the performance\nimproves by about 5 points, which veriﬁes the necessity of\nexplicit positional encoding.\nInstance queries.The instance queries are learned embed-\ndings for decoding the representative instance predictions.\nIn this experiment, we study the effect of instance queries\nand attempt to exploit the inner connections among them\nby varying the embedding number. Suppose the model de-\ncode n instances each frame, and the frame number is T.\nThe input instance query number should be n×T to de-\ncode the same number for predictions. In the default set-\nting, one embedding is responsible for one prediction, the\nmodel directly learns n×T unique embeddings, termed as\n‘prediction level’ in Table 1b. In the ‘video level setting’,\none embedding is learned for all the instance predictions,\ni.e., the same embedding is repeated n×T times as the\ninput of decoder. In the ‘frame-level’ setting, the model\nonly learns T unique embeddings and repeats them by n\ntimes. In the ‘instance level’ setting, the model only learns\nnunique embeddings and repeats them by T times. The n\nand T corresponds to the value of 10 and 36 in the table re-\nspectively. The result is 8.4% AP and 13.7% AP for ‘video\nlevel’ and ‘frame level’ settings respectively. Surprisingly,\nthe ‘instance level’ queries can achieve 32.0% AP, which\nis only 1.3 points lower than the default setting. The result\nshows that the queries for one instance can be shared for\nthe VisTR model, which makes the tracking natural. But\nthe queries for one frame can not be shared.\nTransformers for feature encoding.As illustrated in the\n‘instance sequence segmentation’ module of Fig. 2. The\nmodule takes three types of features as input: the features\n‘B’ from the backbone, the feature ‘E’ from the encoder\nand the attention map computed by the feature ‘E’ and\nLength AP AP50 AP75 AR1 AR10\n18 29.7 50.4 31.1 29.5 34.4\n24 30.5 47.8 33.0 29.5 34.4\n30 31.7 53.2 32.8 31.3 36.0\n36 33.3 53.4 35.1 33.1 38.5\n(a) Video sequence length. The performance improves as the se-\nquence length increases.\n# AP AP50 AP75 AR1 AR10\nvideo level 1 8.4 13.2 9.5 20.0 20.8\nframe level 36 13.7 23.3 14.5 30.4 35.1\nins. level 10 32.0 52.8 34.0 31.6 37.2\npred. level 360 33.3 53.4 35.1 33.1 38.5\n(b) Instance query embedding. Instance-level query is only 1.3%\nlower in AP than the prediction-level query with 36 × fewer embed-\ndings.\ntime order AP AP50 AP75 AR1 AR10\nrandom 32.3 52.1 34.3 33.8 37.3\nin order 33.3 53.4 35.1 33.1 38.5\n(c) Video sequence order.Sequence in time order is 1.0% better in\nAP than sequence in random order.\nAP AP50 AP75 AR1 AR10\nw/o 28.4 50.1 29.5 29.6 33.3\nw 33.3 53.4 35.1 33.1 38.5\n(d) Position encoding. Position encoding brings about 5% AP gains\nto VisTR.\nAP AP50 AP75 AR1 AR10\nCNN 32.0 54.5 31.5 31.6 37.7\nTransformer 33.3 53.4 35.1 33.1 38.5\n(e) CNN-encoded feature vs. Transformer-encoded feature for\nmask prediction. The transformer improves the feature quality.\nAP AP50 AP75 AR1 AR10\nw/o 33.3 53.4 35.1 33.1 38.5\nw 34.4 55.7 36.5 33.5 38.9\n(f) Instance sequence segmentation module.The module with 3D\nconvolutions brings 1.1% AP gains.\nTable 1– Ablation experiments for VisTR. All models are trained on YouTubeVIStrain for 10 epochs and tested on YouTubeVISval,\nusing the ResNet-50 backbone.\n‘O’. To show the superiority of Transformers in feature en-\ncoding, we compare the results of using the original input\n‘O’ vs. output ‘E’ of the encoder for the second feature,\na.k.a., CNN-encoded features vs. Transformer-encoded fea-\ntures. As reported in Table 1e, the CNN-encoded fea-\ntures achieves 32.0% AP, and the Transformer-encoded fea-\ntures achieve 1.3 points higher. This demonstrates that fea-\ntures are learned better after the Transformer updates them\nbased on all pairwise similarities between them through\nself-attention. The result also shows the superiority of mod-\neling the spatial and temporal features as a whole.\nInstance sequence segmentation.The segmentation pro-\ncess contains both the instance mask feature accumulation\nand instance sequence segmentation modules. The instance\nsequence segmentation module takes the instance sequence\nas a whole. We expect that it can strengthen the mask pre-\ndiction by learning the temporal information through 3D\nconvolutions. Thus, when objects are in challenging situ-\nations such as occlusions or motion blurs, the module can\nlearn to propagate information from other frames to help\nthe segmentation. Besides, the features of the same in-\nstance from multiple frames could help the network recog-\nnize the instance better. In this experiment, we perform a\nstudy of models with or without the 3D instance sequence\nsegmentation module. For the former case, we apply a 2D\nconvolutional layer with the output channel being 1 to the\nmask features for each instance of each frame to obtain the\nmasks. The comparison is shown in Table 1f. The in-\nstance sequence segmentation module improves the result\nby 1.1 points, which veriﬁes the effectiveness of the pro-\nposed module.\nWith these ablation studies, we conclude that in VisTR\ndesign: the temporal information, positional encodings, in-\nstance queries, global self-attention in the encoder and the\ninstance sequence segmentation module, all play important\nroles w.r.t.the ﬁnal performance.\n4.3. Main Results\nWe compare VisTR against some state-of-the-art meth-\nods in video instance segmentation in Table 2. The com-\nparison is performed in terms of both accuracy and speed.\nThe methods in the ﬁrst three rows are originally proposed\nfor tracking or VOS. We have cited the results reported by\nthe re-implementations in [30] for VIS. Other methods in-\ncluding the MaskTrack RCNN, MaskProp [2] and STEm-\nSeg [1] are originally proposed for the VIS task in the tem-\nporal order.\nFor the accuracy measured by AP, VisTR achieves the\nbest result among methods using a single model without any\nbells and whistles . Using the same backbone of ResNet-\n50 [10], VisTR achieves about 6 points higher in AP than\nthe MaskTrack R-CNN and the recently proposed STEm-\nSeg method. Besides, we argue the AP gap between VisTR\nand MaskProp mainly comes from its combination of mul-\ntiple networks, i.e., Spatiotemporal Sampling Network [3],\nFeature Pyramid Network [12], Hybrid Task Cascade Net-\nwork [6] and the High-Resolution Mask Reﬁnement post-\nprocessing. Since our aim is to design a conceptually sim-\nple and end-to-end framework, many improvements meth-\nods, such as complex video data augmentation and multi-\nstage mask reﬁnement are beyond the scope of this work.\nFor the speed measured by FPS (frames per second), VisTR\nshows a signiﬁcant advantage among all the reported re-\nsults, achieving 27.7 FPS with the ResNet-101 backbone.\nIf excluding the data loading process of multiple images,\nthe speed can achieve 57.7 FPS. Note that, as we load the\nMethod backbone FPS AP AP50 AP75 AR1 AR10\nDeepSORT [28] ResNet-50 - 26.1 42.9 26.1 27.8 31.3\nFEELVOS [24] ResNet-50 - 26.9 42.0 29.7 29.9 33.4\nOSMN [31] ResNet-50 - 27.5 45.1 29.1 28.6 33.1\nMaskTrack R-CNN [30] ResNet-50 20.0 30.3 51.1 32.6 31.0 35.5\nSTEm-Seg [1] ResNet-50 - 30.6 50.7 33.5 31.6 37.1\nSTEm-Seg [1] ResNet-101 2.1 34.6 55.8 37.9 34.4 41.6\nMaskProp [2] ResNet-50 - 40.0 - 42.9 - -\nMaskProp [2] ResNet-101 - 42.5 - 45.6 - -\nVisTR ResNet-50 30.0/69.9 36.2 59.8 36.9 37.2 42.4\nVisTR ResNet-101 27.7/57.7 40.1 64.0 45.0 38.3 44.9\nTable 2– Video instance segmentationAP (%) on the YouTube-VIS [30] validation dataset. Note that, for the ﬁrst three methods, we\nhave cited the results reported by the re-implementations in [30] for VIS. Other results are adopted from their original paper. For the\nspeed of VisTR we report the FPS results with and without the data loading process. Here we naively load the images serially, taking\nunnecessarily long time. The data loading process can be much faster by parallelizing.\n(a) \n(b) \n(c) \n(d) \nFigure 3– Visualization of VisTRon the YouTube-VIS [30] validation dataset. Each row contains images from the same video. For\neach video, here the same colors depict the mask sequences of the same instances (Best viewed on screen).\nimages in serial, the data loading process can be easily par-\nallelized. The fast speed of VisTR owes to its design of\nparallel decoding and no post-processing.\nThe visualization of VisTR on the YouTube-VIS [30]\nvalidation dataset is shown in Fig. 3, with each row con-\ntaining images sampled from the same video. VisTR can\ntrack and segment instances well in challenging situations\nsuch as: (a) instances overlapping, (b) changes of relative\npositions between instance, (c) confusion by the same cat-\negory instances that are close together and (d) instances in\nvarious poses.\n5. Conclusion\nIn this paper, we have proposed a new video instance\nsegmentation framework built upon Transformers, which\nviews the VIS task as a direct end-to-end parallel sequence\ndecoding/prediction problem. In this way, instance track-\ning is achieved seamlessly and naturally in the same frame-\nwork of instance segmentation, which is signiﬁcantly differ-\nent from and simpler than existing approaches, considerably\nsimplifying the overall pipeline. Without bells and whistles,\nVisTR achieves the best result and the highest speed among\nmethods using a single model on the YouTube-VIS dataset.\nTo our knowledge, our work is the ﬁrst one that applies the\nTransformer to video instance segmentation. We hope that\nsimilar approaches can be applied to many more video un-\nderstanding tasks in the future.\nAcknowledgements This work was in part supported by\nBeijing Science and Technology Project (No. Z181100008-\n918018). CS and his employer received no ﬁnancial support\nfor the research, authorship, and/or publication of this arti-\ncle.\nReferences\n[1] Ali Athar, Sabarinath Mahadevan, Aljo ˇsa Oˇsep, Laura Leal-\nTaix´e, and Bastian Leibe. Stem-seg: Spatio-temporal em-\nbeddings for instance segmentation in videos. In Proc. Eur.\nConf. Comp. Vis., 2020.\n[2] Gedas Bertasius and Lorenzo Torresani. Classifying, seg-\nmenting, and tracking object instances in video with mask\npropagation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,\npages 9739–9748, 2020.\n[3] Gedas Bertasius, Lorenzo Torresani, and Jianbo Shi. Object\ndetection in video with spatiotemporal sampling networks.\nIn Proc. Eur. Conf. Comp. Vis., pages 331–346, 2018.\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Proc. Eur. Conf.\nComp. Vis., 2020.\n[5] Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yong-\nming Huang, and Youliang Yan. Blendmask: Top-down\nmeets bottom-up for instance segmentation. In Proc. IEEE\nConf. Comp. Vis. Patt. Recogn., pages 8573–8581, 2020.\n[6] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox-\niao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi,\nWanli Ouyang, et al. Hybrid task cascade for instance seg-\nmentation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn. ,\npages 4974–4983, 2019.\n[7] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proc. IEEE Int. Conf. Comp. Vis., pages 764–\n773, 2017.\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. arXiv preprint arXiv:2010.11929, 2020.\n[9] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask R-CNN. In Proc. IEEE Int. Conf. Comp. Vis.,\npages 2961–2969, 2017.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proc. IEEE\nConf. Comp. Vis. Patt. Recogn., pages 770–778, 2016.\n[11] Harold W. Kuhn. The Hungarian method for the assignment\nproblem. Naval research logistics quarterly , 2(1-2):83–97,\n1955.\n[12] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In Proc. IEEE Conf. Comp.\nVis. Patt. Recogn., pages 2117–2125, 2017.\n[13] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Dollar. Focal loss for dense object detection. In Proc.\nIEEE Int. Conf. Comp. Vis., Oct 2017.\n[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nProc. Eur. Conf. Comp. Vis., pages 740–755, 2014.\n[15] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In Proc. Int. Conf. Learn. Representations ,\n2018.\n[16] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.\nV-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In Proc. Int. Conf. 3D Vis. ,\npages 565–571, 2016.\n[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An\nimperative style, high-performance deep learning library. In\nProc. Advances in Neural Inf. Process. Syst. , pages 8026–\n8037, 2019.\n[18] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc\nVan Gool, Markus Gross, and Alexander Sorkine-Hornung.\nA benchmark dataset and evaluation methodology for video\nobject segmentation. In Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., 2016.\n[19] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Proc. Advances in Neural Inf. Process.\nSyst., pages 91–99, 2015.\n[20] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding box\nregression. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn. ,\npages 658–666, 2019.\n[21] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng.\nEnd-to-end people detection in crowded scenes. In Proc.\nIEEE Conf. Comp. Vis. Patt. Recogn. , pages 2325–2333,\n2016.\n[22] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convo-\nlutions for instance segmentation. In Proc. Eur. Conf. Comp.\nVis., 2020.\n[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Proc. Advances\nin Neural Inf. Process. Syst., pages 5998–6008, 2017.\n[24] Paul V oigtlaender, Yuning Chai, Florian Schroff, Hartwig\nAdam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast\nend-to-end embedding learning for video object segmenta-\ntion. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn. , pages\n9481–9490, 2019.\n[25] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and\nLei Li. SOLO: Segmenting objects by locations. In Proc.\nEur. Conf. Comp. Vis., 2020.\n[26] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chun-\nhua Shen. SOLOv2: Dynamic and fast instance segmenta-\ntion. In Proc. Advances in Neural Inf. Process. Syst., 2020.\n[27] Yuqing Wang, Zhaoliang Xu, Hao Shen, Baoshan Cheng,\nand Lirong Yang. Centermask: single shot instance segmen-\ntation with point representation. In Proc. IEEE Conf. Comp.\nVis. Patt. Recogn., 2020.\n[28] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple\nonline and realtime tracking with a deep association metric.\nIn Proc. IEEE Int. Conf. Image Process., pages 3645–3649,\n2017.\n[29] Yuxin Wu and Kaiming He. Group normalization. In Proc.\nEur. Conf. Comp. Vis., pages 3–19, 2018.\n[30] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance seg-\nmentation. In Proc. IEEE Int. Conf. Comp. Vis., 2019.\n[31] Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang,\nand Aggelos K Katsaggelos. Efﬁcient video object segmen-\ntation via network modulation. In Proc. IEEE Conf. Comp.\nVis. Patt. Recogn., pages 6499–6507, 2018.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8144565224647522
    },
    {
      "name": "Segmentation",
      "score": 0.7571115493774414
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6664392948150635
    },
    {
      "name": "Transformer",
      "score": 0.5723698735237122
    },
    {
      "name": "Computer vision",
      "score": 0.5470848083496094
    },
    {
      "name": "Video tracking",
      "score": 0.5306253433227539
    },
    {
      "name": "Market segmentation",
      "score": 0.5218226313591003
    },
    {
      "name": "Pipeline (software)",
      "score": 0.44702836871147156
    },
    {
      "name": "Task (project management)",
      "score": 0.4313013553619385
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34740525484085083
    },
    {
      "name": "Video processing",
      "score": 0.2941514253616333
    },
    {
      "name": "Engineering",
      "score": 0.06938764452934265
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}