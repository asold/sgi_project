{
    "title": "Optimizing Deeper Transformers on Small Datasets",
    "url": "https://openalex.org/W3174726724",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A1982578470",
            "name": "Peng Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2154673630",
            "name": "Dhruv Kumar",
            "affiliations": [
                "University of Waterloo"
            ]
        },
        {
            "id": "https://openalex.org/A1987581894",
            "name": "Wei Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2152642213",
            "name": "Wenjie Zi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2168831647",
            "name": "Keyi Tang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2162115121",
            "name": "Chen-yang Huang",
            "affiliations": [
                "University of Alberta"
            ]
        },
        {
            "id": "https://openalex.org/A2148676791",
            "name": "Jackie Chi Kit Cheung",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2052310109",
            "name": "Simon J. D. Prince",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2153618501",
            "name": "Yanshuai Cao",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2994689640",
        "https://openalex.org/W3122604893",
        "https://openalex.org/W4288025992",
        "https://openalex.org/W3035747971",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W3035618017",
        "https://openalex.org/W2996164352",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2945102109",
        "https://openalex.org/W2890431379",
        "https://openalex.org/W3091229675",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4288265053",
        "https://openalex.org/W2979636403",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W4287550997",
        "https://openalex.org/W2970290486",
        "https://openalex.org/W3103334733",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W4288621368",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963959597",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W3034835156",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W2962761235",
        "https://openalex.org/W2796108585",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4289494028",
        "https://openalex.org/W2606964149",
        "https://openalex.org/W4287659415",
        "https://openalex.org/W2964045208",
        "https://openalex.org/W2890867094",
        "https://openalex.org/W3116083993"
    ],
    "abstract": "Peng Xu, Dhruv Kumar, Wei Yang, Wenjie Zi, Keyi Tang, Chenyang Huang, Jackie Chi Kit Cheung, Simon J.D. Prince, Yanshuai Cao. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 2089–2102\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2089\nOptimizing Deeper Transformers on Small Datasets\nPeng Xu1, Dhruv Kumar∗1,2, Wei Yang1, Wenjie Zi1, Keyi Tang1, Chenyang Huang∗1,5,\nJackie Chi Kit Cheung1,3,4, Simon J.D. Prince1, Yanshuai Cao1\n1Borealis AI 2University of Waterloo\n3McGill University 4Canada CIFAR Chair, Mila 5University of Alberta\n{peng.z.xu, wei.yang, wenjie.zi, keyi.tang, simon.prince, yanshuai.cao}@borealisai.com\ndhruv.kumar@uwaterloo.ca, chuang8@ualberta.ca, jcheung@cs.mcgill.ca\nAbstract\nIt is a common belief that training deep trans-\nformers from scratch requires large datasets.\nConsequently, for small datasets, people usu-\nally use shallow and simple additional lay-\ners on top of pre-trained models during ﬁne-\ntuning. This work shows that this does not al-\nways need to be the case: with proper initial-\nization and optimization, the beneﬁts of very\ndeep transformers can carry over to challeng-\ning tasks with small datasets, including Text-\nto-SQL semantic parsing and logical reading\ncomprehension. In particular, we success-\nfully train 48 layers of transformers, com-\nprising 24 ﬁne-tuned layers from pre-trained\nRoBERTa and24 relation-aware layers trained\nfrom scratch. With fewer training steps and\nno task-speciﬁc pre-training, we obtain the\nstate-of-the-art performance on the challeng-\ning cross-domain Text-to-SQL parsing bench-\nmark Spider 1. We achieve this by deriving\na novel Data-dependent Transformer Fixed-\nupdate initialization scheme (DT-Fixup), in-\nspired by the prior T-Fixup work (Huang et al.,\n2020). Further error analysis shows that in-\ncreasing depth can help improve generaliza-\ntion on small datasets for hard cases that re-\nquire reasoning and structural understanding.\n1 Introduction\nIn recent years, large-scale pre-trained language\nmodels (Radford et al., 2019; Devlin et al.,\n2018; Liu et al., 2019b) trained with transform-\ners (Vaswani et al., 2017) have become standard\nbuilding blocks of modern NLP systems to help\nimprove generalization when task-speciﬁc annota-\ntions are limited. In practice, it has been found\nthat deeper transformers generally yield better re-\nsults with sufﬁcient training data (Lan et al., 2019),\n∗Work done while the author was an intern in Borealis AI.\n1The code to reproduce our results can be found in:\nhttps://github.com/BorealisAI/DT-Fixup\nespecially on tasks involving reasoning and struc-\ntural understanding. This suggests that additional\ntransformer layers should be employed in conjunc-\ntion with pre-trained models, instead of simple and\nshallow neural components, such as a classiﬁer\nhead, currently used by models of many NLP tasks.\nHowever, the common belief in the literature is that\ntraining deep transformers from scratch requires\nlarge datasets, and few attempts have been made on\nsmall datasets, to the best of our knowledge. One\nimplication is that although extra transformer lay-\ners on top of pre-trained models should help with\nmore challenging problems in principle, it does\nnot work in practice due to limited training data.\nWe show that after resolving several optimization\nissues with the method proposed in this work, it\nis possible to train very deep transformers with\nimproved generalization even on small datasets.\nOne advantage of pre-trained models is the re-\nduced computational resources needed when ﬁne-\ntuning on small datasets. For instance, it allows\npractitioners to ﬁnetune on a single GPU and obtain\nstrong performance on a downstream task. How-\never, the large size of pre-trained models limits the\nbatch size that can be used in training new trans-\nformer layers on a small computational budget. De-\nspite their broad applications, training transformer\nmodels is known to be difﬁcult (Popel and Bojar,\n2018). The standard transformer training approach\nleverages learning rate warm-up, layer normaliza-\ntion (Ba et al., 2016) and a large batch size, and\nmodels typically fail to learn when missing any\none of these components. The restricted batch size\naggravates the training difﬁculties. Even if a large\nbatch size can be feasibly employed, poorer gener-\nalization results are often observed (Keskar et al.,\n2016), especially when the dataset size is only sev-\neral times larger than the batch size. Furthermore,\nmany recent works noticed a performance gap in\nthis training approach due to layer normalization\n2090\n(Xu et al., 2019; Nguyen and Salazar, 2019; Zhang\net al., 2019a; Wang et al., 2019b; Liu et al., 2020;\nHuang et al., 2020).\nInspired by the recent T-Fixup by Huang et al.\n(2020), which eliminates the need for learning rate\nwarm-up and layer normalization to train vanilla\ntransformers, we derive a data-dependent initial-\nization strategy by applying different analyses to\naddress several key limitations of T-Fixup. We\ncall our method the Data-dependent Transformer\nFixed-update initialization scheme, DT-Fixup. In\nthe mixed setup of additional yet-to-be-trained\ntransformers on top of pre-trained models, DT-\nFixup enables the training of signiﬁcantly deeper\ntransformers, and is generally applicable to differ-\nent neural architectures. Our derivation also ex-\ntends beyond vanilla transformers to transformers\nwith relational encodings (Shaw et al., 2018), al-\nlowing us to apply the results to one variant called\nrelation-aware transformer (Wang et al., 2019a).\nBy applying DT-Fixup on different tasks, we show\nthat the impression that deep transformers do not\nwork on small datasets stems from the optimization\nprocedure rather than the architecture. With proper\ninitialization and optimization, training extra trans-\nformer layers is shown to facilitate the learning of\ncomplex relations and structures in the data.\nWe verify the effectiveness of DT-Fixup on Spi-\nder (Yu et al., 2018), a complex and cross-domain\nText-to-SQL semantic parsing benchmark, and\nReColr (Yu et al., 2020b), a reading comprehension\ndataset requiring logical reasoning. While Text-to-\nSQL semantic parsing is inherently different from\nreading comprehension, they share similar charac-\nteristics which require certain levels of reasoning\nand structural understanding ability. Meanwhile,\nthe sizes of both datasets are less than 10k training\nsamples, which is tiny by deep learning standards\nand renders large-batch training undesirable due to\npoor generalization2.\nOn both datasets, DT-Fixup consistently out-\nperforms the standard approach with better gen-\neralization and allows the training of signiﬁcantly\ndeeper transformer models. For Spider, we suc-\ncessfully apply DT-Fixup to train a Text-to-SQL\nparser containing 48 transformer layers, with 24\nrelation-aware layers trained from scratch on top\nof 24 pre-trained layers from pre-trained RoBERTa\n2For a comparison, T-Fixup applies batch sizes of more\nthan 1k on machine translation to stabilize the training, which\nwould hurt the generalization signiﬁcantly on our datasets\nwhose sizes are less than 10k.\n(Liu et al., 2019b). Our parser achieves 70.9% ex-\nact match accuracy on the Spider test set, which\nis the state of the art at the time of writing. At the\nsame time, it requires less training steps and no\ntask-speciﬁc pre-training as compared to the prior\nart (Yu et al., 2020a). For ReClor, we rank the\nsecond on the public leaderboard by simply adding\n4 transformer layers on top of RoBERTa. Further\nerror analysis shows that the performance improve-\nments by increasing the depth mainly come from\nbetter generalization on the harder cases requiring\nreasoning and structural understanding. Even the\nfailed predictions from the deep models are more\nreasonable than from the shallow ones.\n2 Background\nIn this section, we present the necessary back-\nground by ﬁrst introducing the relation-aware trans-\nformer layer, which outperforms the vanilla trans-\nformer layer with limited data by injecting addi-\ntional inductive bias (Wang et al., 2019a). Then,\nwe introduce the T-Fixup technique (Huang et al.,\n2020) for optimizing deeper vanilla transformers\nand discuss why it does not directly apply in the\nmixed transformer optimization setup.\n2.1 Relative Position and Relational\nEncodings in Transformers\nConsider a set of inputs X = [xxx1,...,xxxn] where\nxxxi ∈Rdx. A transformer, introduced by Vaswani\net al. (2017), is a stack of blocks, with each block\nconsisting of a multi-headself-attention layer, layer\nnormalizations, a multi-layer perceptron and skip\nconnections. Each block (with one head in self-\nattention for notational simplicity) transforms each\nxxxi into yyyi ∈Rdx as follows:\nαij = softmax\n(\nxxxiqqq(xxxjkkk)⊤\n/√\ndz\n)\n(1)\nzzzi = ∑n\nj=1αijxxxjvvv; (2)\n˜y˜y˜yi = LayerNorm(xxxi + zzziwww⊤) (3)\nyyyi = LayerNorm(˜y˜y˜yi + MLP(˜y˜y˜yi)) (4)\nwhere the softmax operation is applied across the\nindex j, MLP is a two-layer perceptron, Layer-\nNorm is a layer normalization (Ba et al., 2016)\nlayer, and qqq,kkk,vvv∈Rdx×dz ,www∈Rdx×dz .\nIn order to bias the transformer toward some\npre-existing relational features between the inputs,\nShaw et al. (2018) described a way to represent rel-\native position information in a self-attention layer\n2091\nby changing Equation 1-2 as follows:\nαij = softmax\n(\nxxxiqqq(xxxjkkk+ rrrk\nij)⊤\n√dz\n)\nzzzi = ∑n\nj=1αij(xxxjvvv+ rrrv\nij)\n(5)\nHere the rrrij ∈Rdz terms encode the known re-\nlationship between two elements xxxi and xxxj in the\ninput. Wang et al. (2019a) adapted this framework\nto effectively encode the schema information using\nrrrij’s for Text-to-SQL parsers, and called it relation-\naware transformer (RAT).\n2.2 T-Fixup and its Limitations\nHuang et al. (2020) found that the requirement\nfor the warmup during the early stage training of\nthe transformers comes from a combined effect\nof high variance in the Adam optimizer and back-\npropagation through layer normalization. Bound-\ning the gradient updates would reduce the variance\nand make training stable, which can be achieved\nby appropriately initializing the model weights.\nThey derived a weight initialization scheme\ncalled T-Fixup for the vanilla transformer that fully\neliminates the need for layer normalization and\nlearning rate warmup, and stabilizes the training\nto avoid harmful plateaus of poor generalization.\nT-Fixup requires the inputs xxxto be Gaussian ran-\ndomly initialized embeddings with variance d−1\n2\nwhere dis the embedding dimension. Then, the\ninput and parameters of the encoder, xxx, vvv, wwwin the\nvanilla self-attention blocks as well as the weight\nmatrices in the MLP blocks deﬁned in Eq. 1-4 are\nre-scaled by multiplying with a factor of 0.67N−1\n4 ,\nwhere N are the number of transformer layers.\nHowever, there are two restrictions of T-Fixup\nnarrowing down the range of its application. First,\nT-Fixup is only designed for vanilla transformer\nbut not other variants like the relative position or\nrelation-aware version described previously. Sec-\nond, they make the critical assumption that the\ninputs xxx can be freely initialized then scaled to\nthe same magnitude as vvv, www and MLP weights.\nThis renders the method inapplicable for the mixed\nsetup where the inputs to the yet-to-be-trained trans-\nformer layers depend on the outputs from the pre-\ntrained models. The ﬁrst issue can be addressed by\nre-deriving the scaling factor following the method-\nology of T-Fixup but taking into account the addi-\ntional relational term. However, to lift the second\nrestriction requires changing the assumption and\nmore dramatic modiﬁcation to the analysis.\nFigure 1: Illustration of the general neural architecture\non which our method can be applied.\n3 Our Approach\nWe now follow the analysis framework of T-Fixup\n(Huang et al., 2020), but derive the conditions to\nbound the gradient updates of the self-attention\nblock in the presence of a pre-trained model. Based\non the derivation, we propose a data-dependent\ninitialization strategy for the mixed setup of the\nnew transformers on pre-trained encodings.\n3.1 Applicable Architectures\nOur analysis applies to the general architecture\ntype illustrated in Figure 1, where the input passes\nthrough a pre-transformer, a main transformer, and\na post-transformer module before outputting. The\npre and post transformer modules can be any ar-\nchitectures that can be stably trained with Adam\n(Kingma and Ba, 2014), including MLP, LSTM,\nCNN, or a pre-trained deep transformer module\nwhich can be stably ﬁne-tuned with a learning rate\nsigniﬁcantly smaller than the main learning rate\nused for the main transformer module. For this\nwork, we will just consider the case of the main\ntransformer containing only the encoder for sim-\nplicity, while our decoder will be an LSTM which\ncan be viewed as part of the post-transformer mod-\nule. Extending our analysis to include deep trans-\nformer decoder is straightforward following the\nframework of Huang et al. (2020).\nWe use fe to denote the pre-transformer mod-\n2092\nule (efor pre-trained encoder), and its parameters\nθθθe; similarly fo for post-transformer module (ofor\noutput) with parameters θθθo. The main transformer\nmodule fG is a stack of Ltransformer blocks, each\nconsisting of a self-attention block and a MLP\nblock. Let Gl,l = 1 ,..., 2N denote individual\nself-attention or MLP layers in the blocks (Gl’s do\nnot include the skip connections), with parameters\nθθθl and let L = 2N, fG’s parameters are denoted\nby θθθG =\nL⋃\nl=1\nθθθl.\n3.2 Theoretical Results for Stable Update\nLet the whole model with the output softmax\nlayer(s) and all layer normalization blocks removed\nbe denoted by f(·;θθθ) and the loss function by L,\nwhere θθθare all the learnable parameters. Follow-\ning Huang et al. (2020), we aim to derive a condi-\ntion under which, per each SGD update with learn-\ning rate η, the model output changes by Θ(η), i.e.\n∥∆f∥= Θ(η) where ∆f = f(·;θθθ−η∂L\n∂θθθ)−f(·;θθθ).\nBy Taylor expansion, the SGD update is:\n∆f = ∂f\n∂θθθo\n∆θθθo + ∂f\n∂θθθG\n∆θθθG + ∂f\n∂θθθe\n∆θθθe+\nO(∥θθθo∥2 + ∥θθθG∥2 + ∥θθθe∥2)\n= −η(∂fo\n∂θθθo\n∂fo\n∂θθθo\n⊤∂L\n∂fo\n⊤\n+\n∂fo\n∂fG\n∂fG\n∂θθθG\n∂fG\n∂θθθG\n⊤∂fo\n∂fG\n⊤∂L\n∂fo\n⊤\n+\n∂fo\n∂fG\n∂fG\n∂fe\n∂fe\n∂θθθe\n∂fe\n∂θθθe\n⊤∂fG\n∂fe\n⊤∂fo\n∂fG\n⊤∂L\n∂fo\n⊤\n)\n+ O(η2) (6)\nAs assumed in Sec. 3.1, we can stably train fe\nand fo coupled with L, i.e, ∥∂L\n∂fo ∥ = ∥∂fo\n∂θθθo\n∥ =\n∥∂fe\n∂θθθe\n∥ = ∥∂fo\n∂fG\n∥ = ∥∂fG\n∂fe ∥ = Θ(1) , we only\nneed to bound the magnitudes of ∂fG\n∂θθθG\nto bound\nthe overall SGD update. Since what we care\nis the magnitude of the update as it relates to\nthe depth, we can assume all parameters to be\nscalars, i.e, qqql,kkkl,vvvl,wwwl,rrrk\nl ,rrrv\nl reduce to scalars\nql,kl,vl,wl,rk\nl ,rv\nl ∈R. The next theorem states\nthe condition under which, ∥∂fG\n∂θθθG\n∥is bounded by\nΘ(1), achieving the overall ∥∆f∥= Θ(η).\nTheorem 3.1 Assuming ∥xxx∥ = Θ( µ) for some\nµ ≫1, then ∥∂fG\n∂θθθG\n∥= Θ(1) if ∥vl∥= ∥wl∥=\n∥rv\nl ∥ = Θ\n(\n((4µ2 + 2µ+ 2)N)−1\n2\n)\nfor all en-\ncoder layers lin relation-aware transformers; and\n∥vl∥ = ∥wl∥ = Θ\n(\n(4µ2N)−1\n2\n)\nin the case of\nvanilla transformers.\nThe proof is in Appendix A. One important imme-\ndiate observation is that our scaling as the depth\nN is to the power of −1/2, whereas T-Fixup has a\nscaling with power of −1/4.\nWhile this theorem is all we need for deriving\nour DT-Fixup approach, it is not immediately in-\ntuitive. So next we inspect what it takes to bound\nthe change in a individual layer output ∥∆Gl∥to\nΘ(η/L) in each gradient update. This will shine\nsome light on the particular form of the expressions\nin Theorem 3.1:\nTheorem 3.2 Let xxxxxxxxxl = [xl\n1,...,x l\nn] be the input\ninto l-th layer, and assume that∥∂L/∂Gl∥= Θ(1),\ni.e. the gradient signal from the layers above is\nbounded, then ∆Gl = Gl(xxxl −η∂L\n∂xxxl\n;θθθl −η∂L\n∂θθθl\n) −\nGl(xxxl;θθθl) satisﬁes ∥∆Gl∥= Θ(η/L) when for all\ni= 1,...,n :\n2∥vl∥2∥xl\ni∥2 + 2∥vl∥∥rv\nl ∥∥xl\ni∥+ ∥rv\nl ∥2\n+ ∥wl∥2(1 + 2∥xl\ni∥2) = Θ(1/N)\n(7)\nfor relation-aware transformers. Alternatively, in\nthe case of vannilla transformers:\n∥vl∥2∥xl\ni∥2 + ∥wl∥2∥xl\ni∥2 = Θ(1/L) (8)\nIn this case, the proof is straightforward by taking\npartial derivatives ofGl with respect to each param-\neter, and keep the terms with the lowest powers as\nthey dominate the norm when the scale is smaller\nthan one. Appendix B gives the detailed proof. The\ninsight from this theorem is: if the input xxxl has\nthe same norm as xxx, setting parameters vl,wl,rv\nl to\nhave the same norm and solve the equations would\nyield the scale factors in Theorem 3.1.\nRemark: In T-Fixup, the corresponding condi-\ntion to Eq. 8 keeps the term ∥vl∥2∥wl∥2 which is\ndropped by ours. It is due to the fact that T-Fixup\nassumes ∥xi∥can be controlled to be the same scale\nas vl and wl, so the lowest power terms (which are\ndominating the norms here) are the quartic ( 4th\npower) ones. For us, ∥xxx∥is treated separately by\na constant to be estimated from data, so the lowest\npower terms are the quadratic ones in vl,wl,rv\nl in\nEq. 7 and 8, and ∥vl∥2∥wl∥2 are dropped. Another\nimportant distinction from T-Fixup is that we as-\nsume the estimated ∥xxx∥to be much larger than the\nscale of vl and wl, unlike the case when they are\nalso controlled to be the same scale. As we will\n2093\nsee next, these changes imply our proposed method\nemploys more aggressive scaling for initialization\nas compared to T-Fixup, and the assumption that\n∥xxx∥has larger scale is satisﬁed naturally.\n3.3 Proposed Method: DT-Fixup\nUnlike previous works (Zhang et al., 2019b; Huang\net al., 2020), appropriate initialization is not enough\nto ensure Eq. 7 and 8 during the early stage of the\ntraining. This is due to the fact that the input xxx\noften depends on the pre-trained model weights\ninstead of being initialized by ourselves. Empiri-\ncally, we observe that the input norm ∥xxx∥are rela-\ntively stable throughout the training but difﬁculty\nto control directly by re-scaling. Based on this ob-\nservation, we treat ∥xxx∥as a constant and estimate\nit by a forward pass on all the training examples\nas µ= maxj[∥xxxj∥]. We then use this estimated µ\nin the factors of Theorem 3.1 to obtain the scaling\nneeded for initialization. Since parameters of all\nlayers are initialized to the same scale, we drop\nindex lfor brevity in this section. In practice, µis\non the order of 10 for pre-trained models, hence\nv, wand rv\ni are naturally two orders of magnitude\nsmaller. DT-Fixup is described as follows:\n•Apply Xavier initialization (Glorot and Ben-\ngio, 2010) on all free parameters except\nloaded weights from the pre-training models;\n•Remove the learning rate warm-up and all\nlayer normalization in the transformer layers,\nexcept those in the pre-trained transformer;\n•Forward-pass on all the training examples to\nget the max input norm µ= maxj[∥xxxj∥];\n•Inside each transformer layer, scale v,w,r v\nin the attention block and weight matrices in\nthe MLP block by (N ∗(4µ2 + 2µ+ 2))−1\n2\nfor relation-aware transformer layer; or scale\nv,w in the attention block and weight ma-\ntrices in the MLP block by N−1\n2 /(2µ) for\nvanilla transformer layer.\n4 Applications\n4.1 Text-to-SQL Semantic Parsing\nWe ﬁrst apply DT-Fixup on the task of cross-\ndomain Text-to-SQL semantic parsing. Given an\nunseen schema Sfor a database during training,\nour goal is to translate the natural question Qto\nthe target SQL T. The correct prediction depends\non the interplay between the questions and the\nschema structures and the generalization over un-\nseen schemas during inference. As a result, rea-\nsoning and structural understanding are crucial to\nperform well on this task, especially for the more\nchallenging cases. We denote our baseline model\nas SQL-SP3 and henceforth.\nImplementation. For modeling Text-to-SQL\ngeneration, we adopt the encoder-decoder frame-\nwork which can be directly ﬁt into the architecture\nshown in Fig. 1. First, the pre-transformer module\nfe is a pre-trained language model which embeds\nthe inputs Q and Sinto joint representations xxxi\nfor each column, table si ∈S and question word\nqi ∈Qrespectively. The joint representations are\npassed into a sequence of N relation-aware trans-\nformer layers. The post-transformer module fo is\na grammar-guided LSTM decoder, which uses the\ntransformer output yyyi to predict the target SQL T.\nWe follow prior arts (Wang et al., 2019a; Guo et al.,\n2019; Yin and Neubig, 2018) to implement SQL-\nSP. The implementation details and hyperparameter\nsettings are described in Appendix C.\nDataset. We evaluate SQL-SP on Spider (Yu\net al., 2018), a complex and cross-domain Text-\nto-SQL semantic parsing benchmark. The dataset\nsize is relatively small by deep learning standards,\nwith only 10,181 questions and 5,693 queries cov-\nering 200 databases in 138 domains.\n4.2 Logical Reading Comprehension\nThe second task where we apply DT-Fixup is multi-\nchoice reading comprehension requiring logical\nreasoning. Given a context, a question and four op-\ntions, the task is to select the right or most suitable\nanswer. Rather than extracting relevant informa-\ntion from a long context, this task relies heavily on\nthe logical reasoning ability of the models.\nImplementation. On top of the pre-trained en-\ncodings of the input context, question and options,\na stack of N vanilla transformer layers are added\nbefore the ﬁnal linear layer which gives the pre-\ndictions. The implementation details and hyper-\nparamter settings are described in Appendix D\nDataset. We evaluate on ReClor (Yu et al.,\n2020b), a newly curated reading comprehension\ndataset requiring logical reasoning. The dataset\ncontains logical reasoning questions taken from\n3SQL Semantic Parser.\n2094\nstandardized exams (such as GMAT and LSAT)\nthat are designed for students who apply for admis-\nsion to graduate schools. Similar to Spider, this\ndataset is also small, with only 6,139 questions.\n5 Experiments\nAll the experiments in this paper are conducted\nwith a signle 16GB Nvidia P100 GPU.\n5.1 Semantic Parsing: Spider Results\nAs the test set of Spider is only accessible through\nan evaluation server, most of our analyses are per-\nformed on the development set. We use the exact\nmatch accuracy4 on all examples following Yu et al.\n(2018), which omits evaluation of generated values\nin the SQL queries.\nModel Dev Test\nRAT-SQL v3 + BERT (Wang et al., 2019a) 69.7 65 .6\nRAT-SQL + GraPPa (Yu et al., 2020a) 73.4 69 .6\nRAT-SQL + GAP (Shi et al., 2020) 71.8 69 .7\nRAT-SQL + GraPPa + GP (Zhao et al., 2021) 72.8 69 .8\nSGA-SQL + GAP (Anonymous) 73.1 70 .1\nRAT-SQL + GraPPa + Adv (Anonymous) 75.5 70 .5\nDT-Fixup SQL-SP + RobERTa (ours) 75.0 70.9\nTable 1: Our accuracy on the Spider development and\ntest sets, as compared to the other approaches at the top\nof the Spider leaderboard as of May 27th, 2021.\nModel N Pretrain Epochs Acc.\nRAT-SQL + BERT 8 ∼200 69 .7\nRAT-SQL + RoBERTa 8 ∼200 69 .6\nRAT-SQL + GraPPa 8 ✓ ∼100 73 .4\nRAT-SQL + GAP 8 ✓ ∼200 71 .8\nSQL-SP + RoBERTa 8 60 66 .9\n+ More Epochs 8 100 69 .2\n+ DT-Fixup 8 60 73 .5\n+ DT-Fixup & More Layers 24 60 75 .0\n+ T-Fixup∗ & More Layers 24 60 Failed\nTable 2: Comparisons with the models leveraging re-\nlational transformers on the Spider development set.\nPretrain here denotes task-speciﬁc pre-training, which\nleverges additional data and tasks, and is orthorgonal\nto our contribution. Not only we converge faster and\nreach better solution, simply training longer from the\nsame baseline cannot close the performance gap. ∗We\ndrop the constraints on the inputs to allow the applica-\ntion of T-Fixup in the mixed setup.\nWe present our results on the Spider leader-\nboard5 in Table 1, where SQL-SP trained with DT-\nFixup outperforms all the other approaches and\n4We use the evaluation script provided in this repo:\nhttps://github.com/taoyds/spider\n5https://yale-lily.github.io/spider\nachieves the new state of the art performance. No-\ntably, the top four submissions on the previous\nleaderboard are all occupied by models leveraging\nrelation-aware transformers and task-speciﬁc pre-\ntraining. Table 2 compares our proposed models\nwith the publicly available works. With enough\ntraining steps, our baseline model trained with the\nstandard optimization strategy achieves the same\nlevel of performance as compared to RAT-SQL.\nHowever, models trained with standard optimiza-\ntion strategy obtain much lower performance with\nthe same epochs6 of training as compared to models\ntrained with DT-Fixup and require more training\nsteps to achieve the best accuracy. At the same\ntime, by adding more relation-aware transformer\nlayers, further gains can be obtained for models\ntrained with DT-Fixup, which achieves the state-\nof-the-art performance without any task-speciﬁc\npre-training on additional data sources. As men-\ntioned in Section 2.2, in the mixed setup, there is\nno way to apply T-Fixup as it was originally pro-\nposed. The closest thing to compare is to drop its\nconstraints on the inputs, but training then becomes\nhighly unstable and fails to converge 4 times out\nof 5 runs. These results demonstrate the necessity\nand effectiveness of DT-Fixup to improve and ac-\ncelerate the transformer training for Text-to-SQL\nparsers.\nModel Easy Medium Hard Extra All\nDev\nRAT-SQL 86.4 73 .6 62 .1 42 .9 69 .7\nBridge (ensemble) 89.1 71 .7 62 .1 51.8 71.1\nDT-Fixup SQL-SP 91.9 80.9 60.3 48 .8 75 .0\nTest\nRAT-SQL 83.0 71 .3 58 .3 38 .4 65 .6\nBridge (ensemble) 85.3 73 .4 59 .6 40 .3 67 .5\nDT-Fixup SQL-SP 87.2 77.5 60.9 46.8 70.9\nTable 3: Breakdown of Spider accuracy by hardness.\nTable 3 shows the accuracy of our best model\nas compared to other approaches 7 with different\nlevel of hardness deﬁned by Yu et al. (2018). We\ncan see that a large portion of the improvement of\nour model comes from the medium level on both\ndev and test set. Interestingly, while our model\nobtains similar performance for the extra hard level\non the dev set, our model performs signiﬁcantly\nbetter on the unseen test set. As most of the extra\n6One epoch iterates over the whole training set once. Wang\net al. (2019a) trained with a batch size of 20 for 90,000 steps,\nwhich is around 200 epochs on the Spider training set. Yu\net al. (2020a) trained with a batch size of 24 for 40, 000 steps,\nwhich is around 100 epochs on the Spider training set.\n7We choose the top two submissions which also report the\nbreakdown of the accuracy on the test set.\n2095\nhard cases involves implicit reasoning steps and\ncomplicated structures, it shows that our proposed\nmodels possess stronger reasoning and structural\nunderstanding ability, yielding better generalization\nover unseen domains and database schemas.\n5.2 Reading Comprehension: ReClor Results\nModel Dev Test\nno extra layers∗(Yu et al., 2020b) 62.6 55 .6\nno extra layers 63.6 56 .2\n4 extra layers 66.2 58 .2\n4 extra layers + DT-Fixup 66.8 61 .0\nTable 4: Our accuracy on ReClor. Star∗is the best base-\nline model result reported in (Yu et al., 2020b) without\nusing the additional RACE dataset (Lai et al., 2017).\nFor ReClor, we choose the best model in Yu\net al. (2020b) as the baseline which employs a lin-\near classiﬁer on top of RoBERTa. From the re-\nsults presented in Table 4, we can see that simply\nstacking additional vanilla transformer layers out-\nperforms the baseline and adding DT-Fixup further\nimproves the accuracy, which ranks the second on\nthe public leaderboard at the time of this submis-\nsion8. The result further validates the beneﬁt of\nadding extra transformer layers and the effective-\nness of DT-Fixup.\n5.3 Ablation Studies\nFor fair comparisons and better understanding, we\nconduct multiple sets of ablation with the same\narchitecture and implementation to validate the ad-\nvantages of DT-Fixup over the standard optimiza-\ntion strategy. Note that, the batch sizes in our exper-\niments are relatively small (16 for Spider and 24 for\nReClor) due to the size of the pre-trained models,\nwhile batch sizes for masked language modelling\n(Liu et al., 2019b) and machine translation (Huang\net al., 2020) are commonly larger than 1024.\nDeeper Models. As we can see from Table 5, the\nstandard optimization strategy fails completely to\ntrain deep transformers whose depths are larger\nthan 8 on both Spider and ReClor, showing that it\nstruggles to properly train the transformer model\nas the depth increases. At the same time, DT-Fixup\ncan successfully train deeper transformers up to 32\nlayers and consistently achieves better performance\nthan models trained by the standard optimization\nstrategy with the same depth on both Spider and\nReClor. With DT-Fixup, deep models generally\n8https://eval.ai/web/challenges/challenge-page/503/\nachieve better performance than the shallow ones\neven there are only thousands of training exam-\nples. It contradicts the common belief that increas-\ning depth of the transformer model is helpful only\nwhen there are enough training data.\nFaster Convergence. Demonstrated by the vali-\ndation curves on Spider plotted in Figure 2, models\ntrained with DT-Fixup converges to the same level\nof performance much faster than models trained\nwith the standard optimization strategy. While stan-\ndard optimization strategy struggles as the models\nbecome deeper, DT-Fixup can keep the model train-\ning smooth, showing that DT-Fixup can effectively\naccelerate the convergence of the transformer train-\ning, especially for the deep ones.\nBatch Sizes When Dataset Size is Small. As\nshown in Table 7, increasing batch size on Spi-\nder from 16 to 120, the average performance from\nﬁve runs drops from 73.24 to 71.08 and the gap\nwith the standard training approach becomes much\nnarrower. It empirically veriﬁes that large-batch\ntraining has a negative impact on the generalization\nwhen the dataset size is small, conﬁrming the need\nto stablize small batch training.\n5.4 Source of the Improvements\nFrom the results on the Spider benchmark, we\ncan see signiﬁcant improvements by applying DT-\nFixup and increasing the depth of the transformer\nmodel. However, why and where they help Text-\nto-SQL semantic parsing are still unclear. As an\nattempt to answer these questions, we investigate\ninto the predicted results from three variants of our\nproposed model: Baseline, the best model (N = 4)\ntrained with the standard training approach; Shal-\nlow, a shallow model ( N = 4 ) trained with DT-\nFixup; Deep, our best model ( N = 24 ) trained\nwith DT-Fixup, which is much deeper.\nTo better understand the models’ behavior, we\nmanually examine all the failed cases predicted by\nthese models and classify the errors into four cat-\negories: 1) Correct: equivalent in meaning but\nwith different SQL syntax ( e.g., ORDER BY X\nLIMIT 1 and SELECT MIN(X)); 2) Column:\nthe SQL structure is correct but there existed mis-\npredicted columns; 3) Sketch: the SQL structure\nis predicted different from the ground truth, while\nthe aligned column prediction are correct; 4) Both:\nthere exist both sketch and column errors in the\nprediction. Table 6 presents the overall statistics\nof our error analysis. Due to logically equivalent\n2096\nN Standard DT-Fixup\nSpider\n2 69.47 ±0.30 70 .73 ±0.18\n4 70.04 ±0.33 72 .22 ±0.61\n8 66.86 ±0.16 73 .24 ±0.51\n16 20.44 ±1.11 73 .52 ±0.47\n24 19.37 ±0.16 73 .79 ±0.49\n32 19.57 ±0.43 73 .02 ±0.52\nReClor\n4 64.05 ±0.44 64 .31 ±0.68\n8 56.96 ±6.12 65 .31 ±0.62\n16 27.10 ±1.50 65 .68 ±1.12\nTable 5: Ablation on the number of\ntransformer layers N. The means and\nstandard deviations are reported based\non 5 runs with different random seeds.\nFigure 2: Validation curves on Spider for models trained with different\nsettings.\nBase Shallow Deep\nFalse neg. 39 35 42\nColumn err. only 51 60 53\nSketch err. only 92 83 77\nBoth err. 124 105 88\nAll 306 283 260\nTable 6: Failures in each category.\nFigure 3: Error breakdown on exam-\nples where all models are wrong.\nFigure 4: Error breakdown on exam-\nples where any model is wrong.\nModel Batch Size Acc\n8 extra layers + Standard 16 69.60 ±0.40\n8 extra layers + DT-Fixup 16 73.24 ±0.51\n8 extra layers + DT-Fixup 120 71.08 ±0.37\nTable 7: Ablation on the batch sizes for the Spider\ndataset. To enable large-batch training, we implement\nthe trick of gradient accumulation at the expense of\ntraining speed. The means and standard deviations are\nreported based on 5 runs with different random seeds.\nqueries, there are a number of false negatives for\nall three models, conﬁrming that the current Spi-\nder evaluation metric is not ideal. At ﬁrst glance,\nthe improvements by applying DT-Fixup and in-\ncreasing the depth seem to come from correcting\nSketch and Both errors, while the three models\nmake similar number of Column only errors. It\nprovides evidence that applying DT-Fixup and in-\ncreasing the depth can help the transformer model\nhandle hard examples which are mispredicted com-\npletely (errors in Both category) by the baseline\nmodel. Typically, correct predictions on these hard\nexamples require a certain level of reasoning and\nstructural understanding ability.\nFine-grained Error Analysis. In order to better\nunderstand the errors made, we look into the com-\nposition of error types by each model on mistaken\nexamples common to all models, as well as on ex-\namples where at least one model is wrong. In Fig.\n3-4, “Column” means “proportion with column er-\nrors” (i.e., Column or Both); “Sketch” means “pro-\nportion with sketch errors” (i.e., Sketch or Both).\nThere are 190 examples mispredicted by all the\nthree models and 387 examples which at least one\nof the three models mispredict. Fig. 3-4 exclude\nfalse negatives due to equivalent logic queries, we\ncan see the real improvements from the deep model\nare even more signiﬁcant than what the exact match\naccuracy shows. Furthermore, among the common\nmistakes to all three models, the deep model has\na much smaller proportion in the sketch mistakes\nwhich usually involve more logic and structure un-\nderstanding. Some of column mistakes are due\nto missing domain knowledge or common sense,\nwhich is harder to improve without external data or\nknowledge. This shows that even among the failed\ncases, deeper transformer model can make more\nreasonable predictions.\n6 Related Work\nMany research efforts have been devoted to un-\nderstanding the training and improving the opti-\n2097\nmization of the transformer models. In particu-\nlar, transformer models often fail to learn unless\na gradual learning rate warm-up is applied at the\nbeginning of training. Chen et al. (2018); Nguyen\nand Salazar (2019); Wang et al. (2019b) noticed a\nperformance gap due to layer normalization, and\nintroduced various architecture changes as remedy.\nZhang et al. (2019b,a); Liu et al. (2020) proposed\ninitialization schemes to stabilize training, allow-\ning either to remove layer normalization or learning\nrate warmup. Liu et al. (2019a) demonstrated the in-\nstability of the Adam optimizer during early stages\nof optimization. Based on these results, Huang et al.\n(2020) proposed a weight initialization schema for\nthe transformer that eliminates the need for layer\nnormalization and warmup completely.\n7 Conclusion\nDespite the broad applications of the transformer\nmodel, it struggles to perform well for some NLP\ntasks with limited training data. In this work, we\npropose a theoretically justiﬁed optimization strat-\negy DT-Fixup to train deeper transformer model\nwith improved generalization and faster conver-\ngence speed on small datasets, which is generally\napplicable to different neural architectures. On\ntwo important tasks, Text-to-SQL semantic pars-\ning and logical reading comprehension that require\nreasoning and structural understanding, applying\nDT-Fixup achieves SOTA or near-SOTA results by\nsimplying using extra transformer layers on top of\nthe pre-trained models. Such observations suggest\neven boarder applicability of deeper transformers.\nAcknowledgements\nWe thank all the anonymous reviewers and area\nchair for their valuable inputs.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\net al. 2018. The best of both worlds: Combining\nrecent advances in neural machine translation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 76–86.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth interna-\ntional conference on artiﬁcial intelligence and statis-\ntics, pages 249–256.\nJiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao,\nJian-Guang Lou, Ting Liu, and Dongmei Zhang.\n2019. Towards complex text-to-sql in cross-domain\ndatabase with intermediate representation. ACL.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nXiao Shi Huang, Felipe P ´erez, Jimmy Ba, and Mak-\nsims V olkovs. 2020. Improving transformer opti-\nmization through better initialization. ICML.\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge No-\ncedal, Mikhail Smelyanskiy, and Ping Tak Peter\nTang. 2016. On large-batch training for deep learn-\ning: Generalization gap and sharp minima. arXiv\npreprint arXiv:1609.04836.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n785–794, Copenhagen, Denmark. Association for\nComputational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2019a. On the variance of the adaptive learning rate\nand beyond. arXiv preprint arXiv:1908.03265.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu\nChen, and Jiawei Han. 2020. Understanding the dif-\nﬁculty of training transformers. EMNLP.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nToan Q Nguyen and Julian Salazar. 2019. Transform-\ners without tears: Improving the normalization of\nself-attention. arXiv preprint arXiv:1910.05895.\n2098\nMartin Popel and Ond ˇrej Bojar. 2018. Training tips\nfor the transformer model. The Prague Bulletin of\nMathematical Linguistics, 110(1):43–70.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468.\nPeng Shi, Patrick Ng, Zhiguo Wang, Henghui\nZhu, Alexander Hanbo Li, Jun Wang, Cicero\nNogueira dos Santos, and Bing Xiang. 2020. Learn-\ning contextual representations for semantic pars-\ning with generation-augmented pre-training. arXiv\npreprint arXiv:2012.10309.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 2818–2826.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nBailin Wang, Richard Shin, Xiaodong Liu, Olek-\nsandr Polozov, and Matthew Richardson. 2019a.\nRat-sql: Relation-aware schema encoding and\nlinking for text-to-sql parsers. arXiv preprint\narXiv:1911.04942.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F Wong, and Lidia S Chao.\n2019b. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810–1822.\nHongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi\nXiong, and Jingyi Zhang. 2019. Lipschitz con-\nstrained parameter initialization for deep transform-\ners. arXiv preprint arXiv:1911.03179.\nPengcheng Yin and Graham Neubig. 2018. Tranx: A\ntransition-based neural abstract syntax parser for se-\nmantic parsing and code generation. arXiv preprint\narXiv:1810.02720.\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin\nWang, Yi Chern Tan, Xinyi Yang, Dragomir\nRadev, Richard Socher, and Caiming Xiong. 2020a.\nGrappa: Grammar-augmented pre-training for table\nsemantic parsing. arXiv preprint arXiv:2009.13845.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, et al. 2018. Spider: A\nlarge-scale human-labeled dataset for complex and\ncross-domain semantic parsing and text-to-sql task.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3911–3921.\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi\nFeng. 2020b. Reclor: A reading comprehension\ndataset requiring logical reasoning. arXiv preprint\narXiv:2002.04326.\nBiao Zhang, Ivan Titov, and Rico Sennrich. 2019a. Im-\nproving deep transformer with depth-scaled initial-\nization and merged attention. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 897–908.\nHongyi Zhang, Yann N Dauphin, and Tengyu Ma.\n2019b. Fixup initialization: Residual learning with-\nout normalization. ICLR.\nLiang Zhao, Hexin Cao, and Yunsong Zhao. 2021.\nGp: Context-free grammar pre-training for text-to-\nsql parsers. arXiv preprint arXiv:2101.09901.\n2099\nA Full Proof\nTheorem 3.1 Assuming ∥xxx∥= Θ(µ) for some µ≫1, then ∥∂fG\n∂θθθG\n∥= Θ(1) if ∥vl∥= ∥wl∥= ∥rv\nl ∥=\nΘ\n(\n((4µ2 + 2µ+ 2)N)−1\n2\n)\nfor all encoder layers l in relational transformers; and ∥vl∥= ∥wl∥=\nΘ\n(\n(4µ2N)−1\n2\n)\nin the case of vanilla transformers.\nProof. First, let’s inspect the feedforward pass through the transformer blocks, which have nonlinear\nlayers Gl’s and skip connections:xxx1 = xxx; xxx2 = xxx1 + G1(xxx1,θθθ1); ... ; xxxl+1 = xxxl + Gl(xxxl,θθθl) For\nl%2 = 1 (i.e. odd layers), Gl is a (relational) self-attention layer, whereas for even layers, Gl is a MLP\nlayer. Using\nΘ\n= to denote bounded in norm as in Huang et al. (2020), then at initialization:\nxxxl+1\nΘ\n= xxxl + vlwlxxxl + wlrv\nl For relational self-attention (9)\nxxxl+1\nΘ\n= xxxl + vlwlxxxl For vanilla self-attention and MLP (10)\nThis is due to the fact that the probability from softmax sums to one, so does not alter the overall norm;\nat initialization, values are at the linear identity range of the nonlinearities. Therefore, for all three\ntypes of layers: ∂xxxl+1\n∂xxxl\nΘ\n= 1 + vlwl and ∂Gl\n∂xxxl\nΘ\n= vlwl. And for relational self-attention: ∂xxxl+1\n∂θθθl\n= ∂Gl\n∂θθθl\nΘ\n=\n[wlxxxl,vlxxxl + rv\nl ,wl,000], where 000 are due to q, k, rrrk which appear only inside the softmax and do not\nasymptotically affect the norm. And for vanilla self-attention and MLP, ∂xxxl+1\n∂θθθl\n= ∂Gl\n∂θθθl\nΘ\n= [wlxxxl,vlxxxl,000].\nNext, let’s look at ∂fG\n∂θθθG\n= [∂fG\n∂θθθ1\n,..., ∂fG\n∂θθθl\n,..., ∂fG\n∂θθθL\n]. First note that:\nfG(xxx,θθθG) = xxx1 + G1(xxx1,θθθ1) + G2(xxx2,θθθ2) + ... + GL(xxx2,θθθL) (11)\nWorking backwards, for the last layer, ∂fG\n∂θθθL\n= ∂GL\n∂θθθL\n. For ∂fG\n∂θθθl\n, terms with index lower than lvanish, so:\n∂fG/∂θθθl = ∂Gl/∂θθθl + ∂Gl+1/∂xxxl+1∂xxxl+1/∂θθθl + ... + ∂GL/∂xxxL∂xxxL/∂xxxL−1 ...∂xxxl+1/∂θθθl (12)\nΘ\n= (1 + vl+1wl+1 + ... + vLwL(1 + vL−1wL−1) ... (1 + vl+1wl+1)) ∂Gl/∂θθθl (13)\nAssuming v1\nΘ\n= v2 ...\nΘ\n= vL and w1\nΘ\n= w2 ...\nΘ\n= wL, and both ≪1, then the above reduces to:\n∂fG/∂θθθl\nΘ\n= (1 + (L−l)vlwl)∂Gl/∂θθθl (14)\nRecall that we want to bound ∂fG\n∂θθθG\n∂fG\n∂θθθG\n⊤\n= ∑\nl\n∂fG\n∂θθθl\n∂fG\n∂θθθl\n⊤\n. For vanilla self-attention or MLP layers:\n∂fG\n∂θθθl\n∂fG\n∂θθθl\n⊤\nΘ\n=\n(\n∥wl∥2∥xxxl∥2 + ∥vl∥2∥xxxl∥2)\n(1 + (L−l)∥vl∥∥wl∥)2 (15)\nAnd for relational self-attention:\n∂fG\n∂θθθl\n∂fG\n∂θθθl\n⊤\nΘ\n=\n(\n∥wl∥2∥xxxl∥2+∥vl∥2∥xxxl∥2+2∥vl∥∥xxxl∥∥rv\nl ∥+∥rv\nl ∥2+∥wl∥2)\n(1+(L−l)∥vl∥∥wl∥)2 (16)\nAt initialization, we want vl, wl, rv\nl of all layers to have the same norm, i.e. ∥vl∥\nΘ\n= ∥wl∥\nΘ\n= ∥rv\nl ∥\nΘ\n=\n∥vj∥\nΘ\n= ∥wj∥\nΘ\n= ∥rv\nj ∥for all l and j, so denoting them using ξ. And recall that N is the number of\ntransformer blocks, with each block containing two layers, so that 2N = L. So we have:\n∂fG\n∂θθθG\n∂fG\n∂θθθG\n⊤\nΘ\n=∑\nl%2=0\n(\n2ξ2∥xxxl∥2)(\n1+(L−l)ξ2)\n+∑\nl%2=1\n(\n2ξ2∥xxxl∥2 +2ξ2∥xxxl∥+2ξ2)(\n1+(L−l)ξ2)\nΘ\n= ∑N\nl=1\n(\n4ξ2∥xxxl∥2 + 2ξ2∥xxxl∥+ 2ξ2)\n(1 + (2N −l)ξ2) (17)\nSimilarly if fG is vanilla transformer instead of a relational one, we have:\n∂fG\n∂θθθG\n∂fG\n∂θθθG\n⊤\nΘ\n= ∑N\nl=1\n(\n4ξ2∥xxxl∥2)\n(1 + (2N −l)ξ2) (18)\n2100\nThe only variable that still depends on lis xxxl, which by expanding the recursion in Eq. 9-10, gives:\nxxxl\nΘ\n= (1 + ξ2)lxxx\nΘ\n= (1 + lξ2 + Θ(ξ4))xxx For vanillla transformer (19)\nxxxl\nΘ\n= (1 + ξ2)lxxx+ l/2ξ2 Θ\n= (1 + lξ2 + Θ(ξ4))xxx+ l/2ξ2 For relational transformer (20)\nNow let ∥xxx∥\nΘ\n= µ, and we have assumed that µ≫1, which is very common for output of pre-trained\nencoders, and due to the high dimensionality. And let\nξ=\n(\nN(4µ2 + 2µ+ 2)\n)−1\n2 (21)\nThen substituting it into Eq. 19-20, we have xxxl\nΘ\n= xxxfor all types of layers. Similarly, plugging Eq. 21 into\nthe expression (1 + (2N −l)ξ2) in Eq. 17 yields (1 + (2N −l)ξ2)\nΘ\n= 1, together with xxxl\nΘ\n= xxx, and Eq.\n21, Eq. 17 becomes:\n∂fG\n∂θθθG\n∂fG\n∂θθθG\n⊤\nΘ\n= ∑N\nl=1\n4µ2\nN(4µ2 + 2µ+ 2)+ 2µ\nN(4µ2 + 2µ+ 2)+ 2\nN(4µ2 + 2µ+ 2)\nΘ\n= ∑N\nl=11/N = Θ(1)\nThis concludes the proof for relational transformers. For vanilla transformers, with ξ =\n(\nN(4µ2)\n)−1\n2 ,\nand following the same steps, but plugging into Eq. 18, we have ∂fG\n∂θθθG\n∂fG\n∂θθθG\n⊤ Θ\n= 1. Q.E.D.\nB Proof of Theorem 3.2\nFor brevity, we drop the layer index. But for the relation embeddings, for clarity, we will consider the\nindividual components of rrrv,rrrk instead of considering the scalar case.\nProof. We will focus the self-attention layer, as the skip connection and MLP layers are analyzed in\nHuang et al. (2020). As mentioned in the main text, since what we care is the magnitude of the update,\nwe assume dx = 1 and drop layer index lwithout loss of generality. In this case, the projection matrices\nqqq,kkk,vvv,wwwreduce to scalars q,k,v,w ∈R. The input xxxand the relational embeddings rrrk,rrrv are n×1\nvectors. For a single query input x′∈xxx, the attention layer (without skip connection) is deﬁned as follows:\nG(x′) = softmax\n( 1√dx\nx′q(kxxx+ rrrk)⊤\n)\n(xxxv+ rrrv)w= ∑n\ni=1\nex′q(kxi+rk\ni )\n∑n\nj=1ex′q(kxj+rk\nj ) (xiv+ rv\ni )w\nNote that we are abusing the notation and take Gto be just the self-attention layer output here. Let\nsi = ex′q(kxi+rk\ni )/∑n\nj=1ex′q(kxj+rk\nj ) and δij = 1 if i= jand 0 otherwise, we can get:\n∂G/∂k = x′qw∑n\ni=1(xiv+ rv\ni )si\n(\nxi −∑n\nj=1xjsj\n)\n∂G/∂q = x′w∑n\ni=1(xiv+ rv\ni )si\n(\nkxi + rk\ni −∑n\nj=1(kxj + rk\nj )sj\n)\n∂G/∂rk\ni = x′qw\n(\n−(xiv+ rv\ni )si + ∑n\nj=1(xjv+ rv\nj )sj\n)\n; ∂G/∂v = w∑n\ni=1xisi\n∂G/∂w = ∑n\ni=1(xiv+ rv\ni )si ; ∂G/∂rv\ni = wsi ; ∂G/∂xi = vwsi + w∑n\nj=1∂sj/∂xi(xjv+ rv\nj )\nWhen xi ̸= x′, we have: ∂sj\n∂xi\n= sj(δij −si)x′qk; When xi = x′, we have: ∂sj\n∂xi\n=\nq\n(\n(1 + δij)kxi + rk\ni\n)\nsj −∑n\nt=1q\n(\n(1 + δit)kxt + rk\nt\n)\nsjst Using Taylor expansion, we get that the\nSGD update ∆Gis proportional to the magnitude of the gradient:\n∆G= −η∂L\n∂G\n(\n∂G\n∂k\n∂G\n∂k\n⊤\n+ ∂G\n∂q\n∂G\n∂q\n⊤\n+ ∂G\n∂v\n∂G\n∂v\n⊤\n+ ∂G\n∂w\n∂G\n∂w\n⊤\n+∑n\ni=1\n∂G\n∂rk\ni\n∂G\n∂rk\ni\n⊤\n+ ∑n\ni=1\n∂G\n∂rv\ni\n∂G\n∂rv\ni\n⊤\n+ ∑n\ni=1\n∂G\n∂xi\n∂G\n∂xi\n⊤\n)\n+ O(η2)\n2101\nBy the assumption that ∥η∂L\n∂G ∥= Θ( η), we need to bound the term inside the main parentheses by\nΘ(1/L). The desired magnitude Θ(1/L) is smaller than 1 so terms with lower power are dominating.\nWith si ≥0 and ∑si = 1, the following terms have the lowest power inside the main parentheses:\n∂G\n∂v\n∂G\n∂v\n⊤\n= w2(∑n\ni=1xisi)2 = Θ(∥w∥2∥xi∥2), i= 1, . . . , n\n∂G\n∂w\n∂G\n∂w\n⊤\n= (∑n\ni=1(xiv + rv\ni )si)2 = Θ(∥v∥2∥xi∥2) + 2Θ(∥v∥∥rv\ni ∥∥xi∥) + Θ(∥rv\ni ∥2), i= 1, . . . , n\n∑n\ni=1\n∂G\n∂rv\ni\n∂G\n∂rv\ni\n⊤\n= w2∑n\ni=1s2\ni = Θ(∥w∥2).\nFor the MLP layer, all terms related to rv\ni disappear, including the single Θ(∥w∥2) in the last row. By\ncombining the update norm terms from both the self-attention and the MLP layers give the result. Q.E.D.\nNote: The above theorem and analysis applies to a single layer, not the whole transformer module of\nmany layers. In order to derive the scaling factor, one needs ensure that the output scale for each block is\nbounded by its input scale. This indeed holds for our scheme, but the complete proof is in Sec. A.\nC Implementation Details of SQL-SP\nGiven a schema Sfor a relational database, our goal is to translate the natural question Qto the target\nSQL T. Here the question Q= q1 ...q |Q|is a sequence of words, and the schema S= {s1,...,s |S|}\nconsists of tables and their columns. s ∈S can be either a table name or a column name containing\nwords si,1,...,s i,|si|. Following Wang et al. (2019a), a directed graph G= ⟨V,E⟩can be constructed to\nrepresent the relations between the inputs. Its nodes V= Q∪S include question tokens (each labeled\nwith a corresponding token) and the columns and tables of the schema (each labeled with the words in its\nname). The edges Eare deﬁned following Wang et al. (2019a). The target SQL T is represented as an\nabstract syntax tree in the context-free grammar of SQL.\nC.1 Encoder\nFollowing (Wang et al., 2019a; Guo et al., 2019), our pre-transformer module fe leverages pre-trained\nlanguage models to obtain the input X to the main transformer module. First, the sequence of words in\nthe question Qare concatenated with all the items (either a column or a table) in the schema S. In order\nto prevent our model from leveraging potential spurious correlations based on the order of the items, the\nitems in the schema are concatenated in random order during training. We feed the concatenation into the\npre-trained model and extract the last hidden states xxx(q)\ni and hhhi = hhhi,1,...,hhhi,|si|for each word in Qand\neach item in Srespectively. For each item si in the schema, we run an additional bidirectional LSTM\n(BiLSTM) (Hochreiter and Schmidhuber, 1997) over the hidden states of the words in its namehhhi. We\nthen add the average hidden state and the ﬁnal hidden state of the BiLSTM as the schema representations\nxxx(s)\ni . X is the set of all the obtained representations from Q∪S: X = (xxx(q)\n1 ,...,xxx(q)\n|Q|,xxx(s)\n1 ,...,xxx(s)\n|S|).\nAlong with the relational embeddingsrrrk,rrrv speciﬁed by G, Xis passed into the main transformer module.\nC.2 Schema Linking\nThe goal of schema linking is to identify the implicit relations between Q and S. The relations are\ndeﬁned by whether there exist column/table references in the question to the corresponding schema\ncolumns/tables, given certain heuristics. Following Wang et al. (2019a), possible relations for each (i,j)\nwhere xi ∈Q,xj ∈S (or vice versa) can be ExactMatch, PartialMatch, or NoMatch, which\nare based on name-based linking. Depending on the type of xi and xj, the above three relations are\nfurther expanded to four types: Question-Column, Question-Table, Column-Question, or\nTable-Question. We also use the value-based linking from Wang et al. (2019a) and Guo et al. (2019)\nto augment the ExactMatch relation by database content and external knowledge.\nC.3 Decoder\nFor our decoder (as the post-transformer module)fo, we employ a transition-based abstract syntax decoder\nfollowing Yin and Neubig (2018). It requires a transition system to converts between the surface SQL\nand a AST-tree constructing action sequences, and can ensure grammarticality of generation. The neural\nmodel then predicts the action sequences. There are three types of actions to generate the target SQL T,\n2102\nincluding (i) ApplyRule which applies a production rule to the last generated node; (ii) Reduce which\ncompletes a leaf node; (iii)SelectColumn which chooses a column from the schema. For our transition\nsystem, each column is attached with their corresponding table so that the tables in the target SQL T\ncan be directly inferred from the predicted columns. As a result, action SelectTable can be omitted\nfrom the generation. Formally, the generation process can be formulated as Pr(T|Y) = ∏\nt Pr(at|a<t,Y)\nwhere Yis the outputs of the last layer of the relational transformers. We use a parent-feeding LSTM\nas the decoder. The LSTM state is updated as mmmt,hhht = fLSTM([aaat−1∥zzzt−1∥hhhpt∥aaapt∥nnnpt],mmmt−1,hhht−1),\nwhere mmmt is the LSTM cell state, hhht is the LSTM output at step t, aaat−1 is the action embedding of the\nprevious step, zzzt−1 is the context feature computed using multi-head attention on hhht−1 over Y, pt is the\nstep corresponding to the parent AST node of the current node, and nnnis the node type embedding. For\nApplyRule[R], we compute Pr(at = ApplyRule[R]|a<t,y) = softmaxR(g(zzzt)) where g(·) is a\n2-layer MLP. ForSelectColumn, we use the memory augmented pointer net Guo et al. (2019).\nC.4 Regularization\nBesides using dropout (Srivastava et al., 2014) employed on X and zzzt to help regularize the model,\nwe further apply uniform label smoothing (Szegedy et al., 2016) on the objective of predicting\nSelectColumn. Formally, the cross entropy for a ground-truth column c∗ we optimize becomes:\n(1 −ϵ) ∗log p(c∗) +ϵ/K∗∑\nc log p(c), where Kis the number of columns in the schema, ϵis the weight\nof the label smoothing term, and p(·) ≜ Pr(at = SelectColumn[·]|a<t,y).\nC.5 Experiment Conﬁguration\nWe choose RoBERTa (Liu et al., 2019b) as the pre-trained language models. A sequence of 24 relation-\naware transformer layers are stacked on top of fe. The Adam optimizer (Kingma and Ba, 2014) with the\ndefault hyperparameters is used to train the model with an initial learning rateηof 4 ×10−4. ηis annealed\nto 0 with 4 ×10−4(1 −steps/max steps)0.5. A separate learning rate is used to ﬁne-tune the RoBERTa\nby multiplying ηa factor of 8 ×10−3. The BiLSTM to encode the schema representations has hidden\nsize 128 per direction. For each transformer layer, dx = dz = 256, H = 8 and the inner layer dimension\nof the position-wise MLP is 1024. For the decoder, we use action embeddings of size 128, node type\nembeddings of size of 64, and LSTM hidden state of size 512. We apply dropout rate of 0.6 on the input\nto the relational transformers Xand the context representation zzzt. The weight of the label smoothing term\nis set to be 0.2. We use a batch size of 16 and train 60 epochs (around 25,000 steps). During inference,\nbeam search is used with beam size as 5. Most of the hyperparameters are chosen following Wang et al.\n(2019a). We only tune the learning rate (4 ×10−4 to 8 ×10−4 with step size 1 ×10−4), dropout (0.3,\n0.4, 0.5, 0.6), the weight of the label smoothing ϵ(0.0, 0.1, 0.2) by grid search. The average runtime is\naround 30 hours and the number of parameters is around 380 millions.\nD Implementation Details for Logical Reading Comprehension\nWe build on the code9 by Yu et al. (2020b) and use it for evaluation. For each example, the encoder\nembeds the input context, question and options which are then passed to the linear layer for classiﬁcation.\nThe exact input format to the encoder is “⟨s⟩Context ⟨/s⟩⟨/s⟩Question ||Option ⟨pad⟩... ”, where “||”\ndenotes concatenation. The linear layer uses the embedding of the ﬁrst token ⟨s⟩for classiﬁcation.\nD.1 Experimental Conﬁguration\nRoBERT is chosen as the pre-trained model, and we stack 4 transformer layers on top. The Adam\noptimizer (Kingma and Ba, 2014) with ϵ= 10−6 and betas of (0.9,0.98) is used. The learning rate to\nﬁnetune RoBERTa is 1 ×10−5 while the learning rate for the additional transformer layers is 3 ×10−4.\nFor all models in our ablation study, the learning rate for the additional transformer layers is1×10−4. The\nlearning rate is annealed linearly to 0 with weight decay of 0.01. We use a batch size of 24 and ﬁne-tune\nfor 12 epochs. For each transformer layer, dx = dz = 1024, H = 8 and the inner layer dimension of the\nposition-wise MLP is 2048. We use dropout rate of 0.4 on the input to the additional transformer layers\nand 0.1 for the linear layer. We follow the hyperparameters used in Yu et al. (2020b) for the pretrained\nlanguage model. For the additional transformer layers, we only tune the dropout values (0.3,0.4,0.5,0.6).\nThe average runtime is around 6 hours and the number of parameters is around 39 millions.\n9https://github.com/yuweihao/reclor"
}