{
  "title": "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking",
  "url": "https://openalex.org/W4388650545",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4221458676",
      "name": "Loukas, Lefteris",
      "affiliations": [
        "Helbio (Greece)"
      ]
    },
    {
      "id": null,
      "name": "Stogiannidis, Ilias",
      "affiliations": [
        "Helbio (Greece)"
      ]
    },
    {
      "id": null,
      "name": "Diamantopoulos, Odysseas",
      "affiliations": [
        "Helbio (Greece)"
      ]
    },
    {
      "id": "https://openalex.org/A4221458680",
      "name": "Malakasiotis, Prodromos",
      "affiliations": [
        "Athens University of Economics and Business"
      ]
    },
    {
      "id": "https://openalex.org/A2529564052",
      "name": "Vassos, Stavros",
      "affiliations": [
        "Helbio (Greece)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2949676527",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W4243739713",
    "https://openalex.org/W4312651322",
    "https://openalex.org/W4385574438",
    "https://openalex.org/W2966087730",
    "https://openalex.org/W3156669901",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4285333908",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4386303029",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2914304175",
    "https://openalex.org/W4285263469",
    "https://openalex.org/W4206729293",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4385573636",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W3106092787",
    "https://openalex.org/W3045492832",
    "https://openalex.org/W4297683418",
    "https://openalex.org/W4296959557",
    "https://openalex.org/W2106411961",
    "https://openalex.org/W2986193249",
    "https://openalex.org/W2786860129",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4285310604",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385573009",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4285137661",
    "https://openalex.org/W3167376363",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W3138154797",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W4389523909"
  ],
  "abstract": "Standard Full-Data classifiers in NLP demand thousands of labeled examples,\\nwhich is impractical in data-limited domains. Few-shot methods offer an\\nalternative, utilizing contrastive learning techniques that can be effective\\nwith as little as 20 examples per class. Similarly, Large Language Models\\n(LLMs) like GPT-4 can perform effectively with just 1-5 examples per class.\\nHowever, the performance-cost trade-offs of these methods remain underexplored,\\na critical concern for budget-limited organizations. Our work addresses this\\ngap by studying the aforementioned approaches over the Banking77 financial\\nintent detection dataset, including the evaluation of cutting-edge LLMs by\\nOpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We\\ncomplete the picture with two additional methods: first, a cost-effective\\nquerying method for LLMs based on retrieval-augmented generation (RAG), able to\\nreduce operational costs multiple times compared to classic few-shot\\napproaches, and second, a data augmentation method using GPT-4, able to improve\\nperformance in data-limited scenarios. Finally, to inspire future research, we\\nprovide a human expert's curated subset of Banking77, along with extensive\\nerror analysis.\\n",
  "full_text": "Making LLMs Worth Every Penny:\nResource-Limited Text Classification in Banking\nLefteris Loukas\nlefteris.loukas@helvia.ai\nHelvia.ai\nDept. of Informatics, Athens Univ. of\nEconomics and Business, Greece\nIlias Stogiannidis\nilias.stogiannidis@helvia.ai\nHelvia.ai\nDept. of Informatics, Athens Univ. of\nEconomics and Business, Greece\nOdysseas Diamantopoulos\nodysseas.diamantopoulos@helvia.ai\nHelvia.ai\nDept. of Informatics, Athens Univ. of\nEconomics and Business, Greece\nProdromos Malakasiotis\nDept. of Informatics, Athens Univ. of\nEconomics and Business, Greece\nStavros Vassos\nstavros@helvia.ai\nHelvia.ai\nABSTRACT\nStandard Full-Data classifiers in NLP demand thousands of la-\nbeled examples, which is impractical in data-limited domains. Few-\nshot methods offer an alternative, utilizing contrastive learning\ntechniques that can be effective with as little as 20 examples per\nclass. Similarly, Large Language Models (LLMs) like GPT-4 can\nperform effectively with just 1-5 examples per class. However,\nthe performance-cost trade-offs of these methods remain under-\nexplored, a critical concern for budget-limited organizations. Our\nwork addresses this gap by studying the aforementioned approaches\nover the Banking77 financial intent detection dataset, including the\nevaluation of cutting-edge LLMs by OpenAI, Cohere, and Anthropic\nin a comprehensive set of few-shot scenarios. We complete the pic-\nture with two additional methods: first, a cost-effective querying\nmethod for LLMs based on retrieval-augmented generation (RAG),\nable to reduce operational costs multiple times compared to classic\nfew-shot approaches, and second, a data augmentation method us-\ning GPT-4, able to improve performance in data-limited scenarios.\nFinally, to inspire future research, we provide a human expert’s\ncurated subset of Banking77, along with extensive error analysis.\nCCS CONCEPTS\n• Computing methodologies →Natural language processing .\nKEYWORDS\nLLMs, OpenAI, GPT, Anthropic, Claude, Cohere, Few-shot, NLP\nACM Reference Format:\nLefteris Loukas, Ilias Stogiannidis, Odysseas Diamantopoulos, Prodromos\nMalakasiotis, and Stavros Vassos. 2023. Making LLMs Worth Every Penny:\nResource-Limited Text Classification in Banking. In 4th ACM International\nConference on AI in Finance (ICAIF ’23), November 27–29, 2023, Brooklyn, NY,\nUSA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3604237.\n3626891\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICAIF ’23, November 27–29, 2023, Brooklyn, NY, USA\n© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0240-2/23/11. . . $15.00\nhttps://doi.org/10.1145/3604237.3626891\nIntent Label\nMy card was declined. Declined Card Payment\nIt declined my transfer. Declined Transfer\nHow do you calculate your exchange rates? Exchange Rate\nMy card was eaten by the cash machine. Card Swallowed\nI lost my card in the ATM. Card Swallowed\nI got married, I need to change my name. Edit Personal Details\n... ...\nMy card is needed soon. Card Delivery Estimate\nWhat is the youngest age for an account? Age Limit\nTable 1: Example banking intents and their labels from the\nBanking77 dataset. In total, there are 77 different labels in\nthe dataset with highly overlapping semantic similarities.\n1 INTRODUCTION\nThe field of Natural Language Processing (NLP) has seen impressive\nadvancements in the past few years, with particular emphasis on\ntext classification. Traditional full-data classifiers require thousands\nof labeled samples, making them infeasible for data-limited domains\nsuch as finance [5]. Modern Few-Shot techniques, which include\ncontrastive learning [41], aim to alleviate this issue by requiring\nonly 10 to 20 examples per class. Also, recent advancements focus\non prompting large Language Models (LLMs) like GPT-3 with as few\nas 1-5 examples per class, typically via a managed API. However, the\ntradeoffs concerning the performance and Operating Costs (OpEx)\nof available methods remain under-explored.\nIn this paper, we bridge this gap by evaluating the aforemen-\ntioned approaches in a comprehensive set of few-shot scenarios\nover a financial intent classification dataset, Banking77 [5], includ-\ning the evaluation of cutting-edge LLMs by OpenAI, Cohere, and\nAnthropic. Banking77 is a real-life dataset containing customer\nservice intents and their classification labels. Contrary to other in-\ntent detection datasets, Banking77 contains a large number (77) of\nlabels with semantic overlaps (Table 1). These characteristics make\nit suitable for investigating methodological perspectives while, at\nthe same time, solving a business use case.\nFirst, we fine-tune MPNet [38], a pre-trained Masked Language\nModel (MLM) by providing it with the complete dataset (Full-Data\nSetting with ∼10k training examples). Second, we fine-tune MPNet\nagain, but this time, we use SetFit [41], a contrastive learning tech-\nnique that can achieve comparable results when shown only up to\n20 examples per class (Few-Shot), instead of the complete dataset.\narXiv:2311.06102v1  [cs.CL]  10 Nov 2023\nICAIF ’23, November 27–29, 2023, Brooklyn, NY, USA Loukas et al.\nThird, we leverage in-context learning with a wide range of popu-\nlar conversational LLMs, including GPT-3.5, GPT-4, and Cohere’s\nCommand-nightly and Anthropic’s Claude 2. We provide the LLMs\nwith only 1 and 3 examples per class (Few-Shot), which we assume\nis practical from a business perspective in data-limited domains\nlike finance [25]. In this setting, we also demonstrate that curated\nsamples picked by a hired human expert outperform randomly\nselected ones with up to 10 points difference. Overall, we show that\nin-context learning with LLMs can outperform fine-tuned masked\nlanguage models (MLMs) in financial few-shot text classification\ntasks, even when presented with fewer examples.\nApart from studying performance, we also present a cost analysis\non employing LLMs. Motivated by this, we extend our study with a\ncost-effective LLM inference method based on Retrieval-Augmented\nGeneration (RAG) that reduces costs significantly, enabling small\norganizations to leverage LLMs without incurring substantial ex-\npenses. We demonstrate that with this approach, we retrieve only a\ntiny but crucial fraction (2.2%) of examples compared to the classic\nfew-shot settings, and we are able to surpass the performance of\nthe most competitive method (GPT-4) by 1.5 points while costing\n700$ less in the context of the test set (∼3k examples). Furthermore,\nwe simulate a low-resource data scenario, where we augment the\ntraining dataset by leveraging GPT-4, demonstrating improved per-\nformance. We also analyze the threshold where data augmentation\nloses effectiveness, aiding AI practitioners in their decision-making.\nTo the best of our knowledge, this is the first study that presents\ntogether a comprehensive evaluation of such methods in a resource-\nlimited industrial context, where data and budget availability are\nkey factors.\n2 RELATED WORK\n2.1 Studies on Banking77\nCasanueva et al. [5] introduced Banking77 and achieved baseline ac-\ncuracy scores of 93.6% and 85.1% for Full-Data and 10-shot settings\nby fine-tuning BERT [11] and using Universal Sentence Encoders\n[6]. Ying and Thomas [46] later improved these results by address-\ning label errors in the dataset, utilizing confident learning and cosine\nsimilarity approaches for mislabeled utterance detection [29]. Their\ntrimmed dataset yielded a significant performance boost, with a\n92.4% accuracy and 92.0% F1 score. Li et al. [20] demonstrated that\npre-training intent representations can improve performance in\nfinancial intent classification. They achieved an 82.76% accuracy\nand 87.3% Macro-F1 score on the Banking77 Full-Data benchmark\nusing prefix-tuning and fine-tuning of the last LLM layer. Lastly,\nMehri and Eric [28] proposed two dialogue system text classification\napproaches: \"observers\" and example-driven training. Observers\noffered an alternative to the [CLS] token for semantic representa-\ntion and their example-driven training method leveraged sentence\nsimilarity for classification. They achieved an accuracy of 85.9% in\nthe 10-shot setting and 93.8% in the Full-Data setting.\n2.2 Few-Shot Text Classification\nLearning from only a few training instances is crucial, especially in\nreal-world use cases where there is no prior dataset and typically\nthere are limited or no resources to create one. In such cases with\nBanking77 Statistics Train Test\nNumber of examples 10,003 3,080\nAverage length (in characters) 59.5 ±40.9 54.2 ±34.7\nAverage length (in words) 11.9 ±7.9 10.9 ±6.7\nNumber of intents (classes) 77 77\nTable 2: Banking77 dataset statistics. The average lengths are\nshown along with their corresponding standard deviations.\nvery limited data, fine-tuning often performs poorly [12] and actu-\nally becoming more challenging as language models grow in size.\nAn alternative approach is in-context learning [4], which involves\nprompting a generative large language model (LLM) with a con-\ntext and asking it to complete NLP tasks without fine-tuning. The\ncontext usually includes a brief task description, some examples\n(the context), and the instance to be classified. The idea behind\nin-context learning is that the language model has already learned\nseveral tasks during pre-training, and the prompt attempts to iden-\ntify the appropriate one [36]. However, selecting the right prompt\nis not easy as language models cannot understand the meaning of\nthe prompt [43]. To address this issue, LLMs have been fine-tuned\nto follow human instructions [30, 31]. Despite this improvement,\nin-context learning is still correlated by term frequencies encoun-\ntered during pre-training [ 35]. At the same time, instruct-tuned\nLLMs (like GPT-3.5 and GPT-4 by OpenAI), carry the biases of the\nhuman annotators who provided the training instructions. To over-\ncome these challenges, prompt-tuning has emerged as a promising\nresearch direction [16, 18].\n3 TASK AND DATASET\nIntent detection is a particular case of text classification, and it\nis a vital component of task-oriented conversational systems in\nvarious domains, including finance. It reflects the complexity of\nactual commercial systems, which can be attributed to the partially\noverlapping intent categories, the need for fine-grained decisions,\nand the lack of comprehensive datasets in finance [5, 25, 26, 48].\nHowever, publicly available intent detection datasets are limited,\nand the existing ones fail to represent the complexity of real-world\nindustrial systems [3, 10]. In response to the need for industry-ready\ndatasets [17, 23], PolyAI released Banking77 [5], which focuses on\na single domain and consists of 77 fine-grained intents related to\nbanking. By concentrating on a specific domain and offering a\ndiverse set of intents, the dataset emulates a more realistic and chal-\nlenging intent detection task than most generic benchmarks. Also, it\nis worth noting that some intent categories partially overlap, requir-\ning fine-grained decisions that cannot rely solely on the semantics\nof individual words, further highlighting the task’s difficulty.\nThe dataset comprises 13,083 annotated customer service queries\nlabeled with 77 intents and is split into two subsets: train (10,003\nexamples) and test (3,080 samples) (Table 2). The label distribution is\nheavily imbalanced in the training subset (Figure 1), demonstrating\nthe challenge of developing classifiers in the Full-Data setting.12\n1The test subset comprises 40 instances for every label.\n2This paper is an extended version of our previous work [27].\nMaking LLMs Worth Every Penny:\nResource-Limited Text Classification in Banking ICAIF ’23, November 27–29, 2023, Brooklyn, NY, USA\nFigure 1: Class distribution of the 77 intents in the training\nset. Intent indices are shown instead of tag names for brevity.\n4 METHODOLOGY\n4.1 Fine-tuning MLMs\nMPNet [38] is a transformer-based model [11, 42] designed with a\nunique pre-training objective, employing permuted language mod-\neling to capture token dependencies and utilizing auxiliary position\ninformation as input. Pre-trained on a substantial 160GB text cor-\npora, MPNet demonstrates superior performance compared to BERT\n[11], XLNet [45], and RoBERTa [24] across various downstream\ntasks. We utilize all-mpnet-base-v23, a top-performing variation\nof MPNet in the sentence transformers leaderboard, making it a\nprominent choice for our task.4\n4.2 Few-Shot Contrastive Learning with SetFit\nSetFit [41] is a recent methodology developed by HuggingFace\nwhich fine-tunes a Sentence Transformer model on a minimal num-\nber of labeled text pairs for each class.5 SetFit utilizes contrastive\nlearning [8] in a Siamese manner, where positive and negative pairs\nare constructed by in- and out-class selection. As a result, transform-\ners using SetFit produce highly descriptive text representations,\nwhere a classification head is later trained on. Despite using limited\ntraining data (such as eight training examples per class), Tunstall\net al. showed that SetFit’s performance is comparable to models\ntrained on complete datasets with standard fine-tuning [41].\n4.3 In-Context Learning\nFor in-context learning, we use closed-source LLMs like GPT-3.5\n[30] and GPT-4[31], which are based on the Generative Pre-trained\nTransformer (GPT) [33, 34]. These pre-trained models are further\ntrained to follow instructions with Reinforcement Learning from\nHuman Preferences (RLHF) [9, 32]. GPT-3.5 is a 175B-parameter\nmodel and, at the time of the writing, has 2 variants able to con-\nsume a context of 4,096 and 16,384 tokens. GPT-4 is a multi-modal\nmodel with 2 variants available, able to consume 8,192 and 32,768\ntokens. We also used Cohere’s Command-nightly with a 4,096\ntokens context window and Anthropic Claude 2 , which provides\na 100,000 tokens context window.6\n3https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n4https://www.sbert.net/docs/pretrained_models.html\n5https://github.com/huggingface/setfit\n6See https://cohere.com/ and https://www.anthropic.com/. We use the Command-\nnightly version available during the week of 2-9 October 2023. Anthropic does not\nspecify their checkpoints for the Claude 2 model.\n4.4 Human Expert Annotation for Robustness\nCasanueva et al. [5] discovered class overlaps in creating Banking77.\nTo address this issue, previous studies like Ying and Thomas [46]\nused additional annotation to curate subsets and enhance perfor-\nmance on real-life noisy datasets like Banking77. However, they\ndid not share their curated subset for reproducibility.\nTo tackle these challenges, we curated a subset of Banking77 by\nhiring a subject matter expert. We provided the human expert with\n10, randomly-picked examples per class, and they selected the top\n3 based on their alignment with the corresponding intents. This\nmeticulous approach reduced class overlaps and ensured the high\nrelevance of each example to its intended label. As we show later,\nthis is fundamental in the few-shot scenario since these expert-\nselected training instances outperform randomly selected instances\nper class. To support robust financial research, we provide this\ncurated subset as a free resource for the financial AI community.7\n5 EXPERIMENTS & RESULTS\n5.1 Experimental Setup\nFor the fine-tuning methods, we use TensorFlow and HuggingFace.\nFor all in-context learning methods, we instruct the model to\nreturn only the financial intent label of the test example that is\npresented in each query. The prompt we use can be broken down\ninto 3 parts and is the same for all LLMs, i.e., GPT-3.5 and GPT-4\n(OpenAI),8 Command (Cohere), and Claude (Anthropic). The first\npart contains the description of the task and the available classes.\nThe second provides a few examples, and the third presents the\ntext to be classified. The prompt template is shown in Figure 2.\nYou are an expert assistant in the field of customer service.\nYour task is to help workers in the customer service\ndepartment of a company. Your task is to classify the\ncustomer's question in order to help the customer service\nworker to answer the question.\nIn order to help the worker, you MUST respond\nwith the number and the name of one of the\nfollowing classes you know. If you cannot answer the question,\nrespond: \"-1 Unknown\".\nIn case you reply with something else, you will be penalized.\nThe classes are:\n0 activate_my_card\n1 age_limit\n2 apple_pay_or_google_pay\n3 atm_support\n.. ..\n75 wrong_amount_of_cash_received\n76 wrong_exchange_rate_for_cash_withdrawal\nHere are some examples of questions and their classes:\nHow do I top-up while traveling? automatic_top_up\nHow do I set up auto top-up? automatic_top_up\n... ...\nIt declined my transfer. declined_transfer\nFigure 2: The prompt template used to query the Large Lan-\nguage Models (LLMs).\n7The curated subset is hosted at https://anonymous.4open.science/r/data-prompt-4805\n8We use the available gpt-3.5-turbo variants of 4K and 16K contexts for the 1-\nshot and 3-shot settings accordingly, and the gpt-4 8K context model, which is less\nexpensive than the 32K one. In all cases, we use the 0613 checkpoints.\nICAIF ’23, November 27–29, 2023, Brooklyn, NY, USA Loukas et al.\n5.2 Hyperparameter tuning in MLMs\nWe tuned the MLM (MPNet-v2) using Optuna’s [1] implementation\nof the Tree-structured Parzen Estimator (TPE) algorithm [2]. We\nspecified 10 trials, and we defined a search space of (1e-5, 5e-5) for\nthe body’s learning rate and (1e-2, 5e-5) for the head’s learning\nrate with logarithmic intervals. During tuning, we maximized the\nvalidation Micro-F1. We deployed the MLM pipelines in an NVIDIA\nA100 SXM GPU with 40GB memory.\n5.3 Prompt Engineering in LLMs\nWe experiment with two different prompt settings using GPT-4 in\na 3-shot setting on a held-out validation subset, which was created\nby using 5% of the training subset. In the first setting, we include\nthe few-shot examples as part of the chat history, i.e., the query is\npresented as a user message and the class is presented as an assistant\nmessage that follows. In the second setting, we include the few-\nshot examples in the so-called system, which is intended to give\nan initial context to the assistant. The second setting yielded the\nbest results (Table 3), and we use it for the rest of our experiments.\nFew-shot examples given as µ-F1 m-F1\nPrevious chat history 75.5 74.4\nSystem context 77.7 77.0\nTable 3: Validation Micro-F1 and Macro-F1 scores for our two\nprompt settings with GPT-4 in the 3-Shot scenario.\n5.4 Results\nTo comprehensively understand how accurately the models can\nperform, we report micro-F1 (µ-F1) and macro-F1 (m-F1). Table 4\nshows MPNet-v2 achieving competitive results across all few-shot\nsettings using SetFit. When trained on only 3 samples, it achieves\nscores of 76.7 µ-F1 and 75.9 m-F1. As we increase the number of\nsamples, the performance improves, reaching a 91.2 micro-F1 and\n91.3 macro-F1 score with 20 samples. This is only 3 percentage\npoints (pp) lower than fine-tuning the model with all the data,\ndemonstrating the effectiveness of SetFit, especially in domains\nwhere acquiring data points is difficult. Lastly, our MPNet-v2 solu-\ntion outperforms the previous results from [28], both in the 10-shot\nand Full-Data settings (by 2.2 pp and 0.3pp).\nDespite being presented with only 1 sample per class (either ran-\ndom or representative), GPT-4 achieves competitive results (80.39\nand 77.6 µ-F1). It outperforms MPNet-v2 by a large margin (over\n20 pp) in the 1-shot setting, showing the potential for effective few-\nshot classification in domains where data is limited [25]. Using our\nhuman-curated representative samples leads to better in-context\nlearning results (both in GPT-3.5 and GPT-4). We also employed\ntwo alternative closed-source LLMs by Cohere and Anthropic on\nthe representative samples. Cohere’s Command-nightly performs\npoorly with a low 58.4µ-F1 while Anthropic’s Claude 1 yields a 73.8\nµ-F1, comparable to GPT-3.5’s 75.2. At the same time, Anthropic’s\nClaude 2 performs slightly better with a 76.8 µ-F1.\nWe proceed with 3-shot classification with the top-performing\nmodel, GPT-4, and the less powerful GPT-3.5 model of the OpenAI\nmodels family. GPT-4 outperforms both previous models on the\n3-shot setting by more than 6 pp (MPNet-v2) and 17 pp (GPT-3.5).\nMethods Setting µ-F1 m-F1\nMehri and Eric [28] Full-Data 93.8 NA\nMehri and Eric [28] 10-shot 85.9 NA\nYing and Thomas [46] Full-Data NA 92.0\nMPNet-v2 Full-Data 94.1 94.1\nMPNet-v2 (SetFit) 1-shot 57.4 55.9\nGPT-3.5 (representative samples) 1-shot 75.2 74.3\nGPT-3.5 (random samples) 1-shot 74.0 72.3\nGPT-4 (representative samples) 1-shot 80.4 78.1\nGPT-4 (random samples) 1-shot 77.6 76.7\nCommand-nightly (representative samples) 1-shot 58.4 57.8\nAnthropic Claude 1 (representative samples) 1-shot 73.8 72.1\nAnthropic Claude 2 (representative samples) 1-shot 76.8 75.1\nMPNet-v2 (SetFit) 3-shot 76.7 75.9\nGPT-3.5 (random samples) 3-shot 57.9 59.8\nGPT-3.5 (representative samples) 3-shot 65.5 65.3\nGPT-4 (representative samples) 3-shot 83.1 82.7\nGPT-4 (random samples) 3-shot 74.2 73.7\nMPNet-v2 (SetFit) 5-shot 83.5 83.3\nMPNet-v2 (SetFit) 10-shot 88.1 88.1\nMPNet-v2 (SetFit) 15-shot 90.6 90.5\nMPNet-v2 (SetFit) 20-shot 91.2 91.3\nTable 4: Classification results for all models on the test data,\nwith N-Shot indicating the number of samples used during\ntraining. The MPNet model is fine-tuned without the SetFit\nmethod on the Full-Data setting.\nMore notably, GPT-3.5, performs poorly on its 3-shot variant (65.5\nµ-F1) compared to its promising 1-shot variant (75.2 µ-F1). This\nprobably verifies recent reports on GPT-3.5 getting lost in the middle\nof long contexts [22]. Similarly to the 1-shot experiments, GPT-4’s\nperformance drops substantially (approximately 9 pp) when shown\nrandom samples instead of representative ones. Thus, it is better\nto present more examples to a more powerful engine like GPT-4\ninstead of GPT-3.5.\n6 COST-EFFECTIVE LLM INFERENCE USING\nRETRIEVAL-AUGMENTED GENERATION\n6.1 Cost Analysis\nApart from studying the models’ performance, we investigate the\nsignificant Operating Costs (OpEx) associated with popular LLMs\nlike GPT-4. We provide a budget analysis for our experiments in\nTable 5 (cost refers to∼3k instances). This can be seen as a guideline\nfor researchers and practitioners to evaluate the trade-offs between\nperformance and budget when selecting a closed-source LLM of-\nfered via a managed API, as well as a data point when deciding\nbetween a “build vs. buy” approach, which requires developing\ndatasets and setting up and hosting their own custom model.\nFocusing on the two upper groups of the table, we see how\npopular LLMs perform in the financial intent detection task of\nBanking77 in a standard few-shot approach. We observe that in the\n1-shot setting, Anthropic Claude 2 has a performance that comes on\npar with the GPT-3.5 model and comes at half of GPT-3.5’s cost. In\nthe 3-shot setting, GPT-4 scores nearly 20 points more than GPT-3.5,\nbut comes with a 10x cost, which amounts to 740$.\nMaking LLMs Worth Every Penny:\nResource-Limited Text Classification in Banking ICAIF ’23, November 27–29, 2023, Brooklyn, NY, USA\nModel Setting Micro-F1 ↑ Cost↓\nGPT-4 1-shot 80.4 620$\nGPT-3.5 1-shot 75.2 31$\nAnthropic Claude 2 1-shot 76.8 15$\nCommand-nightly 1-shot 58.4 22$\nGPT-3.5 3-shot 65.5 62$\nGPT-4 3-shot 83.1 740$\nGPT-4 5 similar (RAG) 84.5 205$\nAnthropic Claude 2 5 similar (RAG) 84.8 33$\nGPT-4 10 similar (RAG) 81.2 230$\nAnthropic Claude 2 10 similar (RAG) 85.2 37$\nGPT-4 20 similar (RAG) 87.7 270$\nAnthropic Claude 2 20 similar (RAG) 85.5 42$\nTable 5: Cost analysis of various closed-source LLMs. The first\ntwo groups represent the 1- and 3-shot results, as shown in\nSection 5.4 (Results). The rest groups come from Section 6.2,\nwhere we utilize Retrieval-Augmented Generation (RAG) for\ncost-effective LLM inference. We perform 3,080 queries to\neach LLM, i.e., 1 query per sample in the test set.\n6.2 Retrieval-Augmented Generation (RAG)\nMotivated by the high costs associated with closed-source LLMs\n[39], we present a methodology to query LLMs efficiently by creat-\ning a Retriever component before plugging it into our Generative\nText Classifier (Reader) pipeline.\nIn the standard few-shot setting, we provide examples for all\nintents; e.g., in the 3-shot setting, we use a prompt with 231 samples\n(3 samples x 77 classes) as context in each inference call to the LLMs.\nHowever, our intuition is that a subset of them can be sufficient,\nallowing room for a cost-effective approach by narrowing down\nthe number of representative examples used in each inference call.\nThus, we consider including only the most similar examples to\nbe utilized during each inference call. This is based on the active\nlearning algorithms proposed by Lewis et al . and Liu et al., who\naugment in-context examples for text generation language models\nusing kNN. The rationale is: for each inference sentence to be\nclassified, instead of providing all the 231 examples for the classes\ninside the prompt, we provide only the 5 (2.2%), 10 (4.3%), and 20\n(8.7%) most similar examples, as shown in Figure 3. We use the\nall-mpnet-base-v2 vector embeddings and the cosine similarity\nmetric for the distance calculation.\nThe results of this approach, as seen in the three lower groups\nof Table 5, show improved results both in performance and budget.\nFor instance, when we retrieve only five similar instances (2.2% of\nthe 231 total examples) for each inference call, Anthropic Claude 2\nhas an 84.8 µ-F1 vs. GPT-4’s 84.5. At the same time, it costs 172$\nless (1/6 of GPT-4’s cost). Most importantly, this score is also higher\nthan 1.5 percentage points than GPT-4 in the classic 3-shot setting,\nwhich costs 740$ and is 22 times multiple of its cost.\nBy increasing the retrieved similar samples to 10 and 20, we see\na slight increase to a maximum of 87.7% µ-F1 for GPT-4 (270$) and\n85.5% µ-F1 of Claude 2 (42$). Anthropic Claude 2 is consistent in\nFigure 3: Dynamic LLM prompt construction through\nRetrieval-Augmented Generation (RAG), using cosine simi-\nlarity for in-context data selection. We use K=5, 10, 20.\nimproving over the number of retrieved examples, while GPT-4\nshows a slight drop in performance when presented with 10 similar\nexamples vs. 5 similar examples. One possible explanation for this\nis the non-deterministic behavior of LLMs.\n7 LLMS FOR DATA GENERATION IN\nLOW-RESOURCE SETTINGS\nData-centric AI is increasingly gaining attention, as evidenced by\nrelevant scientific venues.9 Such approaches become invaluable in\nareas where collecting vast quantities of data is either infeasible, pro-\nhibited by privacy constraints, or challenging such as in finance.10\nThey focus on maximizing the utility of limited datasets rather than\nonly focusing on scaling up model complexity (model-centric AI),\nwhich is often impractical to maintain or deploy [14, 15].\nIn line with this low-resource paradigm, we simulate a scenario\nwhere our dataset is limited. We assume that providing 3 examples\nper class is feasible in practice, we start with the 231 original hand-\npicked examples from the expert (3 per each of the 77 classes), and\nwe leverage GPT-4 to generate additional examples per class. The\nintention here is to explore how much we can rely to data augmenta-\ntion to increase the performance, and identify the threshold beyond\nwhich the quality of generated examples starts to degrade, and thus,\nwe cannot continue with adding more (artificial) examples.\nWhile past existing studies show that just writing simple prompts\nboost performance on various classifications tasks, they come short\ndue to two reasons: (a) they consider vanilla tasks with up to 6\nclasses (where in our scenario, we consider 77) [47] and (b) they\nuse older text generation models rather than instruct-based LLMs.\nSince Sahu et al. found that data augmentation for tasks with\nlarge label volume is likely to not benefit at all (at least when using\n9https://datacentricai.org/neurips21/\n10https://sites.google.com/view/icaif-synthetic/home\nICAIF ’23, November 27–29, 2023, Brooklyn, NY, USA Loukas et al.\nFigure 4: The Micro-F1 Score for various few-shot settings.\nIn the Augmented Data (black) line, 3 of the examples each\ntime (out of the 5, 10, 15, 20) belong to the representative\nsamples that the human expert picked from the real data.\nolder text generation models), we adopt a particular prompt tem-\nplate for our instruct-based GPT-4 model. First, we analyze the data\nusing Azimuth [13], an open-source toolkit. We split the 77 labels\ninto ten groups, where each group consists of labels that contain in-\ntents with highly semantic overlap, as shown in Azimuth (e.g., Top\nUp Reverted and Top Up Failed belong to the same group). Then,\nwe prompted the model with the 3 examples of each class inside\nthat group, and we specifically requested it to generate 20 artificial\nexamples by paying attention to these confounding classes.11\nSubsequently, we employed the SetFit approach across four dif-\nferent settings: 5-shot, 10-shot, 15-shot, and 20-shot. However, to\nincorporate the generated data, we amended the traditional method-\nology of presenting the N original examples per class. Instead of\nthe 5-shot experiment, we used 3 examples from the original data\nand 2 from the augmented. Similarly, the original-to-augmented\nratios for 10-shot, 15-shot, and 20-shot tasks were 3:7, 3:12, and\n3:17, respectively. This allowed us to assess how well our model\nperforms with limited data and how effectively it can integrate and\nlearn from artificially generated examples.\nFigure 4 shows the µ-F1 score for different few-shot settings.\nStarting from the 3-shot setting with 76.7% µ-F1, one can generate\nartificial data to get an essential boost to 81% µ-F1 as seen in the\naugmented 5-shot setting (3 original examples + 2 generated). The\nmaximum performance increase is observed at the 10-shot augmen-\ntation scenario with 81.6%. After this threshold, the quality of the\ndata generated decreases (see 15- and 20-shot settings), injecting\nmore noise into the model than the actual value. As expected, the\nreal data (even if they are challenging to obtain in such domains)\nare far superior and more meaningful than the generated data (91.2\nµ-F1 vs. 78.8 µ-F1 in the 20-shot setting).\n11Our prompt template for data generation can be found in https://anonymous.4open.\nscience/r/data-prompt-4805.\n8 ERROR ANALYSIS\nLastly, to understand our models’ limitations, we also performed an\nexploratory error analysis in some of our top-performing models.\nSpecifically, we manually inspected the errors of GPT-3.5 (1-shot),\nGPT-4 (3-shot), MPNet-v2 (10-shot), and MPNet-v2 (Full-Data).\n8.1 Errors in LLMs\nThe most misclassified labels by GPT-4 (3-shot) and GPT-3.5 (1-shot)\nare shown in Table 6 along with their percentages. After inspect-\ning the samples and their predicted labels, we observe that the\nlabel Get Physical Card was frequently misclassified as Change\nPin. This is potentially due to the presence of the word “PIN” in\nall test instances, causing the vector embeddings to locate them\nclose to the decision boundary of the Change Pin class. As for the\nTransfer Not Received By Recipient class, phrases indicative\nof non-receipt were observed in only 20% of the test samples, which\ncould have complicated the classification task. The rest of the sam-\nples exhibited concerns related to transaction timing and recipient\nvisibility, pushing them towards the Transfer Timing class.\nGold Label Misclassifications\nGPT-4 GPT-3.5\nGet Physical Card 87.5% 87.5%\nTransfer Not Received 62.5% 50.0%\nBeneficiary Not Allowed 42.5% 60.0%\nTable 6: Top misclassified labels, along with their misclassifi-\ncation percentages (out of 40 test instances), for the GPT-4\n(3-shot) and GPT-3.5 models (1-shot).\nThe third label presented in the table,Beneficiary Not Allowed,\nwas incorrectly classified mostly as Declined Transfer by GPT-4\nand GPT-3.5, respectively. The incorrect labeling could be attributed\nto sentences that might seem ambiguous even for a human classifier.\nFor instance, sentences such as “I tried to make a transfer, but it\nwas declined. ” and “The system does not allow me to transfer funds\nto this beneficiary. ” were misinterpreted as pertaining to Declined\nTransfer class, due to the mention of terms like “declined” and\n“does not allow” . The models, thus, failed to discern the actual issue\nof beneficiary disallowance.\n8.2 Errors in MLMs\nIn Table 7, the MPNet-v2 model displayed significant confusion\nbetween in two labels when trained on Full-Data and 10 samples\nper class. Specifically, after inspection, we see that for the gold label\nTop Up Failed, the sentence “please tell me why my top-up failed”\nwas misclassified as Top Up Reverted , likely due to contextual\noverlap between failed and reverted top-ups. Another source of\nconfusion involved the Transfer Timing gold label. Sentences\nlike “How many days does it take until funds are in my account?”\nwere misclassified asPending Transferor Balance Not Updated\nAfter Cheque or Cash Deposit . The temporal aspect of the\nquery seems to direct the models towards other classes that also\ninvolve time, thus highlighting challenges in differentiating specific\nconcerns related to money transfers.\nMaking LLMs Worth Every Penny:\nResource-Limited Text Classification in Banking ICAIF ’23, November 27–29, 2023, Brooklyn, NY, USA\nGold Label Misclassifications\nFull-Data 10-shot\nTop Up Failed 22.5% 17.5%\nTransfer Timing 20.0% 45.0%\nTable 7: Misclassification percentages (out of 40 test in-\nstances) for the MPNet-v2 model when trained on Full-Data\nvs. when trained on 10 samples per class.\n8.3 About Overlapping Labels\nWe anticipate that this analysis will serve as a source of inspira-\ntion for future work in financial intent detection. One possible\napproach to mitigate these errors involves the adoption of hierar-\nchical classifiers [7]. These classifiers could initially identify classes\nwithin broad categories, such as transfers or top-ups, and subse-\nquently classify them into more specific classes, such as pending\ntransfer vs. transfer timing and top-up failed vs. top-up reverted.\nWe believe that these methodologies could either be integrated\nwith “native” TensorFlow/PyTorch pipelines or simulated using\nChain-of-Thought prompting techniques [44] so the LLMs can dis-\ntinguish fine-grained financial labels by thinking step-by-step or\nclass-by-class.\n9 CONCLUSION\nWe conducted a comprehensive few-shot text classification study\nusing LLMs, MLMs and discussed their trade-offs between cost and\nperformance. We focused on Banking77, a financial intent detection\ndataset with real-life challenges, such as its large number of intents\nand overlapping labels.\nOur results clearly demonstrate the effectiveness and efficiency\nof in-context learning using conversational LLMs. This approach\nserves as a practical and rapid solution for achieving accurate results\nin few-shot scenarios and domains with restricted resources, like\nfinance. In detail, we demonstrated that LLMs, like GPT-3.5, GPT-4\nand Anthropic Claude 2, can perform better than MLMs in some\nlimited-data scenarios (1- and 3-shot). On the other side, by fine-\ntuning custom MLMs like MPNet-v2 with SetFit, we surpassed the\nprevious work of [28] in the 10-shot setting by 2.2 pp.\nLLM services reduce the need for technical skills and skip GPU\ntraining times. However, they can be considered costly for small or-\nganizations, given that they are accessed behind paywalls (740$ for\n3-shot experiments of ∼3k samples with GPT-4). After presenting\ndetailed pricing costs to help the community make better deci-\nsions, we also demonstrated a cost-effective inference method for\nLLMs. We utilized a semantic similarity retriever to return only a\nsmall but substantial fraction of training examples to our prompt,\nshowing that this performs better than classic few-shot in-context\nlearning. Most importantly, using Anthropic Claude 2 with this\nRAG approach, we can save multiple (22) times the cost associated\nwith LLM services like GPT-4 (700$ less) while getting a higher\nclassification score.\nTo showcase how financial companies can utilize LLMs in sce-\nnarios with limited data, we also used GPT-4 to generate artificial\ndata for data augmentation. We conclude that these generated data\npoints are helpful up to a specific threshold (7 generated examples),\nafter which our performance starts to drop, possibly due to the LLM\nstarting to generate noise.\nLastly, all of our top-performing Few-Shot experiments using\nLLMs and MLMs were trained on representative data samples out of\na human expert-curated Banking77 subset. We provide this curated\ndataset freely available in order to promote the development of\nrobust financial AI systems.\n10 FUTURE WORK\nIn future work, we plan to experiment with open-sourced LLM\nalternatives, which may be suitable substitutes for closed-source\nmodels, like LLaMA2 [ 40] and incorporating Chain-of-Thought\n[44] techniques to mitigate the errors about the overlapping class\nlabels, as presented extensively in the error analysis section.\nACKNOWLEDGMENTS\nThis work has received funding from European Union’s Horizon\n2020 research and innovation programme under grant agreement\nNo 101021714 (\"LAW GAME\"). Also, we would like to sincerely\nthank the Hellenic Artificial Intelligence Society (EETN) for their\nsponsorship.\nREFERENCES\n[1] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori\nKoyama. 2019. Optuna: A Next-Generation Hyperparameter Optimization\nFramework. In Proceedings of the 25th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD ’19) . As-\nsociation for Computing Machinery, New York, NY, USA, 2623–2631. https:\n//doi.org/10.1145/3292500.3330701\n[2] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms\nfor Hyper-Parameter Optimization. In Advances in Neural Information Processing\nSystems, J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger\n(Eds.), Vol. 24. Curran Associates, Inc. https://proceedings.neurips.cc/paper_\nfiles/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf\n[3] Daniel Braun, Adrian Hernandez Mendez, Florian Matthes, and Manfred Langen.\n2017. Evaluating Natural Language Understanding Services for Conversational\nQuestion Answering Systems. InProceedings of the 18th Annual SIGdial Meeting on\nDiscourse and Dialogue . Association for Computational Linguistics, Saarbrücken,\nGermany, 174–185. https://doi.org/10.18653/v1/W17-5522\n[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models Are Few-Shot Learners. In\nProceedings of the 34th International Conference on Neural Information Processing\nSystems (Vancouver, BC, Canada)(NIPS’20). Curran Associates Inc., Red Hook,\nNY, USA, Article 159, 25 pages.\n[5] Iñigo Casanueva, Tadas Temčinas, Daniela Gerz, Matthew Henderson, and Ivan\nVulić. 2020. Efficient Intent Detection with Dual Sentence Encoders. In Pro-\nceedings of the 2nd Workshop on Natural Language Processing for Conversa-\ntional AI . Association for Computational Linguistics, Online, 38–45. https:\n//doi.org/10.18653/v1/2020.nlp4convai-1.5\n[6] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni\nSt. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian\nStrope, and Ray Kurzweil. 2018. Universal Sentence Encoder for English. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations . Association for Computational Linguistics,\nBrussels, Belgium, 169–174. https://doi.org/10.18653/v1/D18-2029\n[7] Ilias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Prodromos Malakasiotis,\nNikolaos Aletras, and Ion Androutsopoulos. 2020. An Empirical Study on Large-\nScale Multi-Label Text Classification Including Few and Zero-Shot Labels. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP) . Association for Computational Linguistics, Online, 7503–\n7515. https://doi.org/10.18653/v1/2020.emnlp-main.607\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.\nA Simple Framework for Contrastive Learning of Visual Representations. In\nICAIF ’23, November 27–29, 2023, Brooklyn, NY, USA Loukas et al.\nProceedings of the 37th International Conference on Machine Learning (ICML’20) .\nJMLR.org, Article 149, 11 pages.\n[9] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario\nAmodei. 2017. Deep Reinforcement Learning from Human Preferences. In Ad-\nvances in Neural Information Processing Systems , I. Guyon, U. Von Luxburg, S. Ben-\ngio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Cur-\nran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/file/\nd5e2c0adad503c91f91df240d0cd4e49-Paper.pdf\n[10] Alice Coucke, Alaa Saade, Adrien Ball, Théodore Bluche, Alexandre Caulier,\nDavid Leroy, Clément Doumouro, Thibault Gisselbrecht, Francesco Caltagirone,\nThibaut Lavril, Maël Primet, and Joseph Dureau. 2018. Snips Voice Platform: an\nembedded Spoken Language Understanding system for private-by-design voice\ninterfaces. ArXiv abs/1805.10190 (2018).\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers). Association for Computational Linguistics, Minneapolis, Minnesota,\n4171–4186. https://doi.org/10.18653/v1/N19-1423\n[12] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi,\nand Noah Smith. 2020. Fine-Tuning Pretrained Language Models: Weight Initial-\nizations, Data Orders, and Early Stopping. arXiv:2002.06305 [cs.CL]\n[13] Gabrielle Gauthier-melancon, Orlando Marquez Ayala, Lindsay Brin, Chris Tyler,\nFrederic Branchaud-charron, Joseph Marinier, Karine Grande, and Di Le. 2022.\nAzimuth: Systematic Error Analysis for Text Classification. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations. Association for Computational Linguistics, Abu Dhabi, UAE,\n298–310. https://aclanthology.org/2022.emnlp-demos.30\n[14] Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. 2021. Knowl-\nedge Distillation: A Survey. Int. J. Comput. Vision 129, 6 (jun 2021), 1789–1819.\nhttps://doi.org/10.1007/s11263-021-01453-z\n[15] Institute of Electrical and Electronics Engineers (IEEE) 2019.2019 2nd Workshop on\nEnergy Efficient Machine Learning and Cognitive Computing for Embedded Applica-\ntions (EMC2 2019) . Institute of Electrical and Electronics Engineers (IEEE), Curran\nAssociates, Inc., Washington, DC, USA. https://doi.org/10.1109/EMC249363.2019\n[16] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie,\nBharath Hariharan, and Ser-Nam Lim. 2022. Visual Prompt Tuning. In Com-\nputer Vision – ECCV 2022 , Shai Avidan, Gabriel Brostow, Moustapha Cissé, Gio-\nvanni Maria Farinella, and Tal Hassner (Eds.). Springer Nature Switzerland, Cham,\n709–727.\n[17] Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew\nLee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Lauren-\nzano, Lingjia Tang, and Jason Mars. 2019. An Evaluation Dataset for Intent\nClassification and Out-of-Scope Prediction. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Asso-\nciation for Computational Linguistics, Hong Kong, China, 1311–1316. https:\n//doi.org/10.18653/v1/D19-1131\n[18] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale\nfor Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing . Association for Computational\nLinguistics, Online and Punta Cana, Dominican Republic, 3045–3059. https:\n//doi.org/10.18653/v1/2021.emnlp-main.243\n[19] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,\nSebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation\nfor Knowledge-Intensive NLP Tasks. In Proceedings of the 34th International\nConference on Neural Information Processing Systems (Vancouver, BC, Canada)\n(NIPS’20). Curran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages.\n[20] Xianzhi Li, Will Aitken, Xiaodan Zhu, and Stephen W. Thomas. 2022. Learn-\ning Better Intent Representations for Financial Open Intent Classification. In\nProceedings of the Fourth Workshop on Financial Technology and Natural Lan-\nguage Processing (FinNLP) . Association for Computational Linguistics, Abu Dhabi,\nUnited Arab Emirates (Hybrid), 68–77. https://aclanthology.org/2022.finnlp-1.8\n[21] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and\nWeizhu Chen. 2022. What Makes Good In-Context Examples for GPT-3?. In\nProceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on\nKnowledge Extraction and Integration for Deep Learning Architectures . Association\nfor Computational Linguistics, Dublin, Ireland and Online, 100–114. https:\n//doi.org/10.18653/v1/2022.deelio-1.10\n[22] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\nFabio Petroni, and Percy Liang. 2023. Lost in the Middle: How Language Models\nUse Long Contexts. arXiv:2307.03172 [cs.CL]\n[23] Xingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser. 2021. Bench-\nmarking Natural Language Understanding Services for Building Conversational\nAgents. Springer Singapore, Singapore, 165–183. https://doi.org/10.1007/978-\n981-15-9323-9_15\n[24] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nRobustly Optimized BERT Pretraining Approach. ArXiv abs/1907.11692 (2019).\n[25] Lefteris Loukas, Manos Fergadiotis, Ion Androutsopoulos, and Prodromos Malaka-\nsiotis. 2021. EDGAR-CORPUS: Billions of Tokens Make The World Go Round. In\nProceedings of the Third Workshop on Economics and Natural Language Processing .\nAssociation for Computational Linguistics, Punta Cana, Dominican Republic,\n13–18. https://doi.org/10.18653/v1/2021.econlp-1.2\n[26] Lefteris Loukas, Manos Fergadiotis, Ilias Chalkidis, Eirini Spyropoulou, Prodro-\nmos Malakasiotis, Ion Androutsopoulos, and Georgios Paliouras. 2022. FiNER:\nFinancial Numeric Entity Recognition for XBRL Tagging. InProceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics, Dublin, Ireland, 4419–4431.\nhttps://doi.org/10.18653/v1/2022.acl-long.303\n[27] Lefteris Loukas, Ilias Stogiannidis, Prodromos Malakasiotis, and Stavros Vassos.\n2023. Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance.\nIn Proceedings of the Fifth Workshop on Financial Technology and Natural Language\nProcessing and the Second Multimodal AI For Financial Forecasting . -, Macao, 74–80.\nhttps://aclanthology.org/2023.finnlp-1.7\n[28] Shikib Mehri and Mihail Eric. 2021. Example-Driven Intent Prediction with\nObservers. In Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies .\nAssociation for Computational Linguistics, Online, 2979–2992. https://doi.org/\n10.18653/v1/2021.naacl-main.237\n[29] Curtis G. Northcutt, Lu Jiang, and Isaac L. Chuang. 2021. Confident Learning:\nEstimating Uncertainty in Dataset Labels.Journal of Artificial Intelligence Research\n(JAIR) 70 (2021), 1373–1411.\n[30] OpenAI. 2022. Training language models to follow instructions with human\nfeedback. In Advances in Neural Information Processing Systems , S. Koyejo, S. Mo-\nhamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Asso-\nciates, Inc., 27730–27744. https://proceedings.neurips.cc/paper_files/paper/2022/\nfile/b1efde53be364a73914f58805a001731-Paper-Conference.pdf\n[31] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\nTraining language models to follow instructions with human feedback. Advances\nin Neural Information Processing Systems 35 (2022), 27730–27744.\n[33] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.\nImproving Language Understanding with Unsupervised Learning. https:\n//openai.com/research/language-unsupervised Accessed: 06 May 2023.\n[34] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language models are unsupervised multitask learners. OpenAI\nBlog 1, 8 (2019), 9.\n[35] Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022.\nImpact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning. In\nFindings of the Association for Computational Linguistics: EMNLP 2022 . Association\nfor Computational Linguistics, Abu Dhabi, United Arab Emirates, 840–854. https:\n//aclanthology.org/2022.findings-emnlp.59\n[36] Laria Reynolds and Kyle McDonell. 2021. Prompt Programming for Large Lan-\nguage Models: Beyond the Few-Shot Paradigm. In Extended Abstracts of the 2021\nCHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI\nEA ’21) . Association for Computing Machinery, New York, NY, USA, Article 314,\n7 pages. https://doi.org/10.1145/3411763.3451760\n[37] Gaurav Sahu, Pau Rodriguez, Issam Laradji, Parmida Atighehchian, David\nVazquez, and Dzmitry Bahdanau. 2022. Data Augmentation for Intent Clas-\nsification with Off-the-shelf Large Language Models. In Proceedings of the 4th\nWorkshop on NLP for Conversational AI . Association for Computational Linguis-\ntics, Dublin, Ireland, 47–57. https://doi.org/10.18653/v1/2022.nlp4convai-1.5\n[38] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. MPNet:\nMasked and Permuted Pre-Training for Language Understanding. In Proceedings\nof the 34th International Conference on Neural Information Processing Systems\n(Vancouver, BC, Canada)(NIPS’20). Curran Associates Inc., Red Hook, NY, USA,\nArticle 1414, 11 pages.\n[39] Ilias Stogiannidis, Stavros Vassos, Prodromos Malakasiotis, and Ion Androut-\nsopoulos. 2023. Cache me if you Can: an Online Cost-aware Teacher-Student\nframework to Reduce the Calls to Large Language Models. In Findings of the\nConference on Empirical Methods in Natural Language Processing . Association for\nComputational Linguistics, Singapore.\n[40] Hugo Touvron and Meta GenAI. 2023. Llama 2: Open Foundation and Fine-Tuned\nChat Models. arXiv:2307.09288 [cs.CL]\n[41] Lewis Tunstall, Nils Reimers, Unso Eun Seo Jo, Luke Bates, Daniel Korat, Moshe\nWasserblat, and Oren Pereg. 2022. Efficient Few-Shot Learning Without Prompts.\nArXiv abs/2209.11055 (2022).\n[42] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All you\nNeed. Advances in neural information processing systems , 5998–6008.\n[43] Albert Webson and Ellie Pavlick. 2022. Do Prompt-Based Models Really Under-\nstand the Meaning of Their Prompts?. In Proceedings of the 2022 Conference of the\nMaking LLMs Worth Every Penny:\nResource-Limited Text Classification in Banking ICAIF ’23, November 27–29, 2023, Brooklyn, NY, USA\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies. Association for Computational Linguistics, Seattle, United\nStates, 2300–2344. https://doi.org/10.18653/v1/2022.naacl-main.167\n[44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le,\nand Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large\nlanguage models. InAdvances in Neural Information Processing Systems (NeurIPS) .\n[45] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\nand Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding. Curran Associates Inc., Red Hook, NY, USA.\n[46] Cecilia Ying and Stephen Thomas. 2022. Label Errors in BANKING77. In\nProceedings of the Third Workshop on Insights from Negative Results in NLP .\nAssociation for Computational Linguistics, Dublin, Ireland, 139–143. https:\n//doi.org/10.18653/v1/2022.insights-1.19\n[47] Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyoung\nPark. 2021. GPT3Mix: Leveraging Large-scale Language Models for Text Augmen-\ntation. In Findings of the Association for Computational Linguistics: EMNLP 2021 .\nAssociation for Computational Linguistics, Punta Cana, Dominican Republic,\n2225–2239. https://doi.org/10.18653/v1/2021.findings-emnlp.192\n[48] Elias Zavitsanos, Dimitris Mavroeidis, Konstantinos Bougiatiotis, Eirini Spy-\nropoulou, Lefteris Loukas, and Georgios Paliouras. 2022. Financial Misstate-\nment Detection: A Realistic Evaluation. In Proceedings of the Second ACM\nInternational Conference on AI in Finance (Virtual Event) (ICAIF ’21) . Asso-\nciation for Computing Machinery, New York, NY, USA, Article 34, 9 pages.\nhttps://doi.org/10.1145/3490354.3494453",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6489149332046509
    },
    {
      "name": "Class (philosophy)",
      "score": 0.6446728706359863
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6212348341941833
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.5240695476531982
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4787801504135132
    },
    {
      "name": "Data set",
      "score": 0.475766122341156
    },
    {
      "name": "Enhanced Data Rates for GSM Evolution",
      "score": 0.464988648891449
    },
    {
      "name": "Data science",
      "score": 0.45892977714538574
    },
    {
      "name": "Machine learning",
      "score": 0.4513992369174957
    },
    {
      "name": "Training set",
      "score": 0.42683130502700806
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4190451502799988
    },
    {
      "name": "Information retrieval",
      "score": 0.35158324241638184
    },
    {
      "name": "Data mining",
      "score": 0.33941930532455444
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ]
}