{
  "title": "VTNet: Visual Transformer Network for Object Goal Navigation",
  "url": "https://openalex.org/W3126809086",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2328909948",
      "name": "Du Heming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104116847",
      "name": "Yu Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134720765",
      "name": "Zheng Liang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2567015638",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2125409550",
    "https://openalex.org/W2776202271",
    "https://openalex.org/W2964935470",
    "https://openalex.org/W2969241704",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2594903727",
    "https://openalex.org/W3044464274",
    "https://openalex.org/W2964043796",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W2947067399",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2962732398",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2095784398",
    "https://openalex.org/W2069830673",
    "https://openalex.org/W3098358101",
    "https://openalex.org/W2143864104",
    "https://openalex.org/W3023306062",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963245725",
    "https://openalex.org/W2963660226",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2962789679",
    "https://openalex.org/W2967853831",
    "https://openalex.org/W3034500398",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2983335573",
    "https://openalex.org/W2963544079",
    "https://openalex.org/W3103334733",
    "https://openalex.org/W2962884155",
    "https://openalex.org/W2962887844"
  ],
  "abstract": "Object goal navigation aims to steer an agent towards a target object based on observations of the agent. It is of pivotal importance to design effective visual representations of the observed scene in determining navigation actions. In this paper, we introduce a Visual Transformer Network (VTNet) for learning informative visual representation in navigation. VTNet is a highly effective structure that embodies two key properties for visual representations: First, the relationships among all the object instances in a scene are exploited; Second, the spatial locations of objects and image regions are emphasized so that directional navigation signals can be learned. Furthermore, we also develop a pre-training scheme to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. In a nutshell, VTNet embeds object and region features with their location cues as spatial-aware descriptors and then incorporates all the encoded descriptors through attention operations to achieve informative representation for navigation. Given such visual representations, agents are able to explore the correlations between visual observations and navigation actions. For example, an agent would prioritize \"turning right\" over \"turning left\" when the visual representation emphasizes on the right side of activation map. Experiments in the artificial environment AI2-Thor demonstrate that VTNet significantly outperforms state-of-the-art methods in unseen testing environments.",
  "full_text": "Published as a conference paper at ICLR 2021\nVTN ET: V ISUAL TRANSFORMER NETWORK FOR\nOBJECT GOAL NAVIGATION\nHeming Du1,3, Xin Yu2∗& Liang Zheng1\n1Australian National University\n2University of Technology Sydney\n3CSIRO-DATA61\n{heming.du, liang.zheng}@anu.edu.au, xin.yu@uts.edu.au\nABSTRACT\nObject goal navigation aims to steer an agent towards a target object based on\nobservations of the agent. It is of pivotal importance to design effective visual\nrepresentations of the observed scene in determining navigation actions. In this\npaper, we introduce a Visual Transformer Network (VTNet) for learning infor-\nmative visual representation in navigation. VTNet is a highly effective structure\nthat embodies two key properties for visual representations: First, the relation-\nships among all the object instances in a scene are exploited; Second, the spatial\nlocations of objects and image regions are emphasized so that directional naviga-\ntion signals can be learned. Furthermore, we also develop a pre-training scheme\nto associate the visual representations with navigation signals, and thus facilitate\nnavigation policy learning. In a nutshell, VTNet embeds object and region fea-\ntures with their location cues as spatial-aware descriptors and then incorporates\nall the encoded descriptors through attention operations to achieve informative\nrepresentation for navigation. Given such visual representations, agents are able\nto explore the correlations between visual observations and navigation actions.\nFor example, an agent would prioritize “turning right” over “turning left” when\nthe visual representation emphasizes on the right side of activation map. Experi-\nments in the artiﬁcial environment AI2-Thor demonstrate that VTNet signiﬁcantly\noutperforms state-of-the-art methods in unseen testing environments.\n1 I NTRODUCTION\nThe goal of target-driven visual navigation is to guide an agent to reach instances of a given target\ncategory based on its monocular observations of an environment. Thus, it is highly desirable to\nachieve an informative visual representation of the observation, which is correlated to directional\nnavigation signals. In this paper, we propose a Visual Transformer Network (VTNet) to achieve an\nexpressive visual representation. In our VTNet, we develop a Visual Transformer (VT) to extract\nimage descriptors from visual observations and then decode visual representations of the observed\nscenes. Then, we present a pre-training scheme to associate visual representations with directional\nnavigation signals, thus making the representations informative for navigation. After pre-training,\nour visual representations are fed to a navigation policy network and we train our entire network in\nan end-to-end manner. In particular, our VT exploits two newly designed spatial-aware descriptors\nas the key and query, (i.e., a spatial-enhanced local descriptor and a positional global descriptor) and\nthen encodes them to achieve an expressive visual representation.\nOur spatial-enhanced local descriptor is developed to fully take advantage of all detected objects for\nthe exploration of spatial and category relationships among instances. Unlike the prior work (Du\net al., 2020) that only leverages one instance per class to mine the category relationship, our VT\nis able to exploit the relationship of all the detected instances. To this end, we employ an object\ndetector DETR (Carion et al., 2020) since features extracted from DETR not only encode object\nappearance information, such as class labels and bounding boxes, but also contain the relations be-\ntween instances and global contexts. Moreover, DETR features are scale-invariant (output from the\n∗Corresponding author\n1\narXiv:2105.09447v1  [cs.CV]  20 May 2021\nPublished as a conference paper at ICLR 2021\nMoveAhead\nLookUp\nLookDown\nRotateRight\nRotateLeft\nDone\nActivation MapObservation\n Instances\nLaptop\nBook\nCellPhone\nDeskLamp\nAction\nFigure 1: Motivation of the Visual Transformer Network (VTNet).A target class (cellphone) is\nhighlighted by green bounding boxes. An agent ﬁrst detects objects of interest from its observation.\nThen, the agent attends detected objects to the global observation by the visual transformer (VT).\nHigh attention scores are achieved on the left sideof the observation, which correspond to the target\n(cellphone). Then, the agent will choose RotateLeft to reach targets.\nsame layer) in comparison to features used in ORG (Du et al., 2020). Considering that object posi-\ntions cannot be explicitly decoded without the feed-forward layer of DETR, we therefore enhance\nall the detected instance features with their locations to obtain spatial-enhanced local descriptors.\nThen, we take all the spatial-enhanced local descriptors as the key of our VT encoder to model the\nrelationships among detected instances, such as category concurrence and spatial correlations.1\nFurthermore, we introduce a positional global descriptor as the query for our VT decoder. In partic-\nular, we associate the region features with image region positions (such as bottom and top) and thus\nfacilitate exploring the correspondences between navigation actions and image regions. To do so, we\ndivide a global observation into multiple regions based on spatial layouts and assign a positional em-\nbedding to each region feature as our spatial-enhanced global descriptor. After obtaining the global\nquery descriptor, we attend the spatial-enhanced local descriptor to the positional global descriptor\nquery to learn the relationship between instances and observation regions via our VT decoder.\nHowever, we found directly training our VTNet with a navigation policy network fails to converge\ndue to the training difﬁculty of the transformers (Vaswani et al., 2017). Therefore, we present a pre-\ntraining scheme to associate visual representations and directional navigation signals. We endow\nour VT with the capability of encoding directional navigation signals by imitating expert experi-\nence. After warming-up through human instructions, VT can learn instructional representations for\nnavigation, as illustrated in Figure 1.\nAfter pre-training our VT, we employ a standard Long Short Term Memory (LSTM) network to\nmap the current visual representation and previous states to an agent action. We adopt A3C archi-\ntecture (Mnih et al., 2016) to learn the navigation policy. Once our VTNet has been fully trained,\nour agent can exploit the correlations between observations and navigation actions to improve visual\nnavigation efﬁciency. In the popular widely-used navigation environment AI2-Thor (Kolve et al.,\n2017), our method signiﬁcantly outperforms the state-of-the-art. Our contributions are summarized\nas follows:\n• We propose a novel Visual Transformer Network (VTNet) to extract informative feature\nrepresentations for visual navigation. Our visual representations not only encode relation-\nships among objects but also establish strong correlations with navigation signals.\n• We introduce a positional global descriptor and a spatial-enhanced local descriptor as the\nquery and key for our visual transformer (VT), and then the visual representations decoded\nby our VT are attended to navigation actions via our presented pre-training scheme, thus\nproviding a good initialization for our VT.\n• Experimental results demonstrate that our learned visual representation signiﬁcantly im-\nproves the efﬁciency of the state-of-the-art visual navigation systems in unseen environ-\nments by 14.0% relatively on Success Weighted by Path Length (SPL).\n1As observed in DETR, the number of objects of interest in a scene is usually less than 100. Thus, we set\nthe key number to 100 in the VT encoder.\n2\nPublished as a conference paper at ICLR 2021\n2 R ELATED WORKS\nVisual navigation, as a fundamental task in robotic and artiﬁcial intelligence, has attracted increasing\nattention recently. Traditional methods (Oriolo et al., 1995) often leverage environment maps for\nnavigation and divide a navigation task into three steps: mapping, localization and path planning.\nSome approaches employ a given map to obviate obstructions (Borenstein & Koren, 1989; 1991).\nDissanayake et al. (2001) infer robot positions by simultaneous localization and mapping (SLAM).\nHowever, maps are usually unavailable in unseen environments.\nRecently, reinforcement learning (RL) has been applied in visual navigation. In general, it takes vi-\nsual observations as inputs and predicts navigation actions directly. Mirowski et al. (2016) develop\na navigation approach in 3D maze environments and introduce depth prediction and loop closure\nclassiﬁcation tasks to improve navigation performance. Parisotto & Salakhutdinov (2017) investi-\ngate a memory system to navigate in mazes. Some methods (Sepulveda et al., 2018; Chen et al.,\n2019; Savinov et al., 2018) use both visual features and the topological guidance of scenes for nav-\nigation, while natural-language instructions are employed to guide an agent to route among rooms\n(Anderson et al., 2018b; Wang et al., 2019; Deng et al., 2020; Hu et al., 2019; Majumdar et al., 2020;\nHao et al., 2020). We notice that transformer architectures are also employed by Hao et al. (2020),\nnamed Prevalenet. However, Prevalenet is used to model languages and predict camera angles rather\nthan encoding local and global visual features. Hence, Prevalenet is essentially different from our\nVT. Furthermore, Kahn et al. (2018) design a self-supervised approach to model environments by\nreinforcement learning. Tang et al. (2021) customize a specialized network for visual navigation via\nan Auto-Navigator. A Bayesian relational memory is introduced by Wu et al. (2019) to explore the\nspatial layout among rooms rather than steering an agent to desired objects with least steps. Mean-\nwhile, Shen et al. (2019) employ multiple visual representations to generate multiple actions and\nthen fuse those actions to produce an effective one. However, requesting such a large number of\nvisual representations may restrict the transferring ability of a navigation system and increases the\ndifﬁculty of data labeling. Note that Fang et al. (2019) propose a transformer to select the embedded\nscene memory slot, while our VT is designed to learn expressive visual representations correlated\nwith directional signals.\nTarget-oriented visual navigation methods aim at steering an agent to object instances of a speciﬁed\ncategory in an unseen environment using least steps. Zhu et al. (2017) search a target object given\nin an image by employing RL to produce navigation actions based on visual observations. Mousa-\nvian et al. (2019) take semantic segmentation and detection masks as visual representations and also\nemploy RL to learn navigation policies. Yang et al. (2018) exploit relationships among object cate-\ngories for navigation, but they need an external knowledge database to construct such relationships.\nWortsman et al. (2019) exploit word embedding (i.e., GloVe embedding) to represent the target cat-\negory and introduce a meta network mimicking a reward function for navigation. Furthermore, Du\net al. (2020) introduce an object relation graph, dubbed ORG, to encode visual observations and\ndesign a tentative policy for deadlock avoidance during navigation. In ORG, object features are\nextracted from the second layer of the backbone in Faster R-CNN (Ren et al., 2015) and thus not the\nmost prominent ones across the feature pyramid. Additionally, ORG chooses one instance with the\nhighest conﬁdence per category from detection results, and it may be affected by the false positive.\n3 V ISUAL NAVIGATION REVISIT\nIn this section, we mainly revisit the deﬁnition of object goal navigation and its general pipeline.\n3.1 T ASK DEFINITION AND SETUP\nIn this object goal visual navigation task, prior knowledge about the environment, i.e. topologi-\ncal map and 3D meshes, and additional sensors, i.e. depth cameras, are not available to an agent.\nRGB images in an egocentric view are the only available source to an agent, and the agent predicts\nits actions based on the current view and previous states. Following the works (Wortsman et al.,\n2019; Du et al., 2020), an environment is divided into grids and agents move between grid points\nvia 6 different actions, consist of MoveAhead, RotateLeft, RotateRight, LookUp,\nLookDown, Done. To be speciﬁc, the forward step size is 0.25 meters, and the angles of turning-\nleft/right and looking-up/down are 45 ◦and 30◦, respectively. An episode is deﬁned as a success\n3\nPublished as a conference paper at ICLR 2021\nEncoder\nPositional Global\nDescriptor\nObservation\nDecoder\nSpatial-Enhanced \nLocal Descriptor\nPrevious Action\nLSTM\nNavigation Policy Network\nPrevious\nState\n⋯\nGlobal\nFeature\nVisual \nRepresentation\nInstance\nFeature\nSpatial\nFeature\nLocation\nenhancement\nPositional\nEmbedding\nPosition \nenhancement\nPre-train Scheme\nPredicted\nAction\nOptimal\nAction Supervision\nValue\nAction\nState \nRepresentation\nVisual Transformer (VT)\nVisual Transformer Network (VTNet)\nEnvironment\n7×7×256 7×7×256 7×7×256\n100×256100×7100×249\n1×3136\n1×6\n1×6\n1×64\n2×5121×512\n1×1\nFigure 2: Overview of our visual transformer navigation system.Our visual transformer navi-\ngation network (VTNet) involves a visual transformer (VT) and a navigation policy network. The\nagent ﬁrst fuses instance features and spatial features into spatial-enhanced local descriptor. Mean-\nwhile, the positional global descriptor is obtained by adding a positional embedding to the global\nfeature. Then the visual representation is decoded from these two spatial-aware descriptors by our\nVT. Our VTNet is pre-trained with the supervision of optimal navigation actions. The navigation\npolicy network adopts A3C architecture and is trained with navigation rewards after pre-training.\nwhen the following three requirements are met simultaneously: (i) the agent chooses the ending\naction Done within allowed steps; (ii) a target is in the view of the agent; (iii) the distance between\nthe agent and the target is less than the threshold ( i.e. 1.5 meters). Otherwise, the episode will be\nregarded as a failure.\nA target class T ∈{Sink,...,Microwave }and a start state s = {x,y,θ r,θh}are set randomly\nat the beginning of each episode, where xand y represent the coordinates, θr and θh indicate the\nview of a monocular camera. At each timestamp t, the agent records the observed RGB image Ot\nfrom its monocular camera. Given the observation Ot and the previous state ht, the agent employs\na visual navigation network to generate a policy π(at|Ot,ht), where at represents the distribution\nof actions at time t. The agent selects the action with the highest probability for navigation.\n3.2 P IPELINE\nA typical pipeline of visual navigation consists of two parts, visual representation learning and nav-\nigation policy learning. (i) Visual representation learning:To encode the current observation in\na compact way, existing works extract visual features from an image and then transform them into\na vector-based representation, where direct concatenation (Wortsman et al., 2019) or graph embed-\nding (Du et al., 2020) are used. (ii) Navigation driven by visual features:Once visual features\nare extracted, a navigation policy network that generates an action in each step for an agent will be\nlearned. There are several ways to learn policy networks, such as Q-learning (Watkins & Dayan,\n1992), PPO (Schulman et al., 2017) and A3C (Mnih et al., 2016). As navigation policy learning is not\nour focus, we adopt the standard Asynchronous Advantage Actor-Critic (A3C) architecture (Mnih\net al., 2016). The navigation policy network takes the combination of the current visual represen-\ntation, the previous action and state embedding as input, and outputs the action distribution and\nvalue. The agent selects actions with the highest probability from the predicted policy and uses the\npredicted value to train the navigation policy network.\n4 P ROPOSED VISUAL TRANSFORMER NETWORK\nAs illustrated in Figure 2, our visual navigation system includes two parts: (i) learning visual rep-\nresentations from RGB observations; (ii) learning navigation policy from the visual representations\nand previous states. In our VTNet, we introduce a visual transformer (VT) in the ﬁrst part to ex-\n4\nPublished as a conference paper at ICLR 2021\nplore the relationship among all objects and their spatial correlations. In VT, we further design\ntwo spatial-aware descriptors, i.e., a spatial-enhanced local descriptor and a positional global de-\nscriptor, to allow us extract visual information effectively. Then, our VT fuses these two types of\ndescriptors with a multi-head attention operation to produce ﬁnal visual representations. Moreover,\nour VT enforces visual representations to be highly correlated to navigation signals via our devel-\noped pre-training scheme, thus easing the training difﬁculty of VT and facilitating navigation policy\nlearning.\n4.1 S PATIAL -ENHANCED LOCAL DESCRIPTOR\nTo learn the relationship among all the instances, we ﬁrst perform object detection and locate all the\nobject instances of interest by a detector DETR (Carion et al., 2020). DETR transforms N encoded\nd-dimension features RN×d from the same layer to N detection results, including the bounding\nboxes, conﬁdence and semantic labels by a feed forward network. Note that ORG (Du et al., 2020)\nextracts object features from the second layer of the backbone in Faster R-CNN based on the pre-\ndicted bounding-boxes rather than the penultimate layer of the classiﬁer in Faster R-CNN as in\nthe work (Anderson et al., 2018a). Hence, ORG features are not the most prominent ones across\nthe feature pyramid and scale-sensitive. In contrast, features extracted by DETR not only contain\nbounding-boxes and class labels but also are scale-robust as features are aligned by DETR decoder,\ni.e., output from the penultimate layer.\nRemark. Beneﬁting from our VT, we leverage all the information of the detected objects while\nDu et al. (2020) only select the proposal with the highest conﬁdence in each category. Therefore,\nthe agents in ORG will miss important information from other objects of the same class or might\nbe severely affected if selected proposals are false positive. In contrast, our VT preserves all the\ninformation, and thus our agents are able to exploit the relationship among instances. This makes\nour visual representation more comprehensive and essentially different from prior works.\nOur local spatial feature is obtained by concatenating the normalized bounding box, conﬁdence and\ntop-rated semantic label for each object. To indicate the target class to an agent, we also concatenate\na one-hot encoded target vector RN×1 with our spatial feature RN×7. After obtaining the instance\nfeature and spatial feature, we employ a multi-layer perceptron (MLP) ( i.e., two fully-connected\nlayers with ReLU) and fuse them to a spatial-enhanced local descriptor L ∈RN×d so as to act as\nthe key of our VT encoder.\n4.2 P OSITIONAL GLOBAL DESCRIPTOR\nIn addition to the spatial-enhanced local descriptor, agents require a global feature to describe the\nsurrounding environment. Similar to SA VN (Wortsman et al., 2019), we adopt ResNet18 (He et al.,\n2016) pretrained on ImageNet (Deng et al., 2009) to extract global features of the observations.\nGiven a global featureRh×w×D, we ﬁrst employ 1 ×1 convolution to reduce the channel dimension\nof a high-level activation map from D to a smaller dimension d, where h and w represent the\nheight and width of activation maps, respectively. This ensures that global descriptors have the\nsame dimension as the key of our VT.\nUnlike previous works that directly concatenate a global feature as a part of the visual representation,\nwe introduce a positional global descriptor as the query in our VT decoder. A region feature only\nrepresents visual contents in each region. To emphasize the region position information, we incorpo-\nrate a positional embedding to each region feature. Then we add positional encodingRh×w×d to the\nglobal feature. Let uand vrepresent the row and column indexes of an image region respectively,\nand iis the index along the dimension d. Our positional embedding is expressed as:\nPE2i(u,v)=\n{sin( u\n100002i/d ), 0 <i ≤d\n2\nsin( v\n100002i/d ), d\n2 <i ≤d PE2i+1(u,v)=\n{cos( u\n100002i/d ), 0 <i ≤d\n2\ncos( v\n100002i/d ). d\n2 <i ≤d (1)\nTherefore, each global feature represents one particular region of the observation. Finally, we re-\nshape positional embedded global features into a matrix-based representation, namely positional\nglobal descriptor G∈Rhw×d.\n5\nPublished as a conference paper at ICLR 2021\n4.3 V ISUAL TRANSFORMER\nAfter obtaining our extracted spatial-enhanced and positional global descriptors, we introduce our\nvisual transformer.\nEncoder. In order to exploit the spatial relationship between detected instances and observed re-\ngions, we attend spatial-enhanced local descriptors to positional global descriptors via a transformer.\nWe ﬁrst feed the spatial-enhanced local descriptors into the encoder as keys and values by employing\nmulti-head self-attention. Following the transformer architecture (Vaswani et al., 2017; Fan et al.,\n2021), each encoder layer consists of a multi-head self-attention module and a feed-forward layer.\nDecoder. Inspired by human navigation behaviors, we aim to explore the correspondences between\nobservation regions and navigation actions. For example, once an agent notices a target lying on the\nright side of the ﬁeld of view, it should prioritize to selectRotateRight instead of RotateLeft.\nSince each positional global descriptor corresponds to a certain region of the observation, we refer to\nthe positional global descriptor as the location query and feed the query into the decoder. Given po-\nsitional global descriptor Gand encoded spatial-enhanced local descriptorL′, our attention function\nof visual transformer decoder is expressed as:\nAttention(G,L’) =softmax(GL′T\n√\nd\nL′). (2)\n4.4 P RE-TRAINING VISUAL TRANSFORMER\nWe observed that directly feeding the decoded representation from our VT to a navigation network,\nwe fail to learn successful navigation policy. This is mainly because training a deep VT is very\ndifﬁcult especially when the supervision signals are provided by a weak reward from reinforcement\nlearning. Therefore, the decoded features might be uninformative and confuse an agent. The agent\nwould prefer to choose the termination action (often around 5 steps in our experiments) in order to\nreduce penalties from reinforcement learning.\nTo address the aforementioned issue, we propose a pre-training scheme for our VT. To be speciﬁc,\nwe enforce the decoded features to be expressive by introducing an imitation learning task, as seen in\nFigure 2. Concretely, human navigation behaviors can be predicted from the decoded representations\nin a step-wise fashion. We use Dijkstra’s Shortest Path First algorithm to generate optimal action\ninstructions as human expert experience. Under the supervision of optimal action instructions, our\nVT learns to imitate the optimal navigation action selection.\nIn the pre-training stage, we do not employ our navigation network ( i.e., LSTM), and previous\nactions as well as states are not available. Note that, in our navigation network, previous actions,\nprevious states and current visual representations are exploited, as seen in Figure 2. Thus, we replace\nour LSTM with an MLP and predict action distributions based on the current visual representation.\nA cross-entropy loss Lvt = CE(at,ˆa) is employed to train our VT and the MLP, where at is the\npredicted action, ˆa represents the optimal action instruction and CE indicates the cross-entropy\nfunction. After pre-training, features from our VT also exhibit strong association with directional\nnavigation signals as only an MLP is employed on top of the features. Therefore, the decoded\nfeatures will facilitate the navigation network training.\n5 E XPERIMENTS\n5.1 P ROTOCOLS AND EXPERIMENTAL DETAILS\nDataset. We perform our experiments on AI2-Thor (Kolve et al., 2017), an artiﬁcial 3D environment\nwith realistic photos. It contains 4 types of scenes,i.e., kitchen, living room, bedroom and bathroom.\nIn each type of scenes, there are 30 different rooms with various furniture placements and items.\nFollowing Du et al. (2020), we choose 22 categories as the target classes and ensure that there are at\nleast 4 potential targets in each room.\nWe use the same training and evaluation protocols as the works (Wortsman et al., 2019; Du et al.,\n2020). 80 rooms out of 120 are selected as the training set while each scene contains 20 rooms.\n6\nPublished as a conference paper at ICLR 2021\nTable 1: Comparison with the state-of-the-art. We report the average success rate (%) and SPL as\nwell as their variances in parentheses by repeating experiments ﬁve times. L >5 represents the\nepisodes which require at least 5 steps.\nMethod ALL L ≥ 5\nSuccess SPL Success SPL\nRandom 8.0 (1.3) 0.036 (0.006) 0.3 (0.1) 0.001 (0.001)\nWE 33.0 (3.5) 0.147 (0.018) 21.4 (3.0) 0.117 (0.019)\nSP (Yang et al., 2018) 35.1 (1.3) 0.155 (0.011) 22.2 (2.7) 0.114 (0.016)\nSA VN (Wortsman et al., 2019) 40.8 (1.2) 0.161 (0.005) 28.7 (1.5) 0.139 (0.005)\nORG (Du et al., 2020) 65.3 (0.7) 0.375 (0.008) 54.8 (1.0) 0.361 (0.009)\nORG+TPN (Du et al., 2020) 69.3 (1.2) 0.394 (0.010) 60.7 (1.3) 0.386 (0.011)\nBaseline 62.6 (0.9) 0.364 (0.006) 51.5 (1.2) 0.345 (0.007)\nVTNet 72.2 (1.0) 0.449 (0.007) 63.4 (1.1) 0.440 (0.009)\nVTNet + TPN(Du et al., 2020) 73.5 (1.3) 0.440 (0.009) 63.9 (1.5) 0.440 (0.011)\nWe equally divide the remaining 40 rooms into validation and test sets. We report the results of the\ntesting data by using the model with the highest success rate on the validation set.\nEvaluation metrics. We evaluate our model performance by success rate and Success Weighted\nby Path Length (SPL). The success rate measures navigation effectiveness and is computed by\n1\nN\n∑N\nn=0 Sn, where N is the number of episodes and Sn is a success indicator of the n-th episode.\nWe adopt SPL to measure the navigation efﬁciency. Given the length of the n-th episode Lenn and\nits optimal path Lenopt, SPL is formulated as 1\nN\n∑N\nn=0 Sn Lenn\nmax(Lenn,Lenopt) .\nTraining details.We use a two-stage training strategy. In Stage 1, we train our visual transformer\nfor 20 epochs with the supervision of optimal action instructions. In this fashion, we explicitly\nconstruct the association between visual representations and navigation actions. In Stage 2, we train\nthe navigation policy for 6M episodes in total with 16 asynchronous agents. We set a penalization\n−0.001 on each action step and a large reward 5 when an agent completes an episode successfully.\nWe adopt DETR as the object detector and ﬁne-tune DETR on the AI2-Thor training dataset. In\ntraining DETR, we applied data augmentation, such as resize and random crop. We use the Adam\noptimizer (Kingma & Ba, 2014) to update the policy network with a learning rate 10−4 and the\npre-trained VT with a learning rate10−5. Our codes and pre-trained model will be publicly released\nfor reproducibility.\n5.2 C OMPETING METHODS\nWe compare our method with the following ones: Random policy.An agent chooses actions based\non a uniform action probability. Thus, the agent will walk or stop in a scene randomly.Scene Prior\n(SP) (Yang et al., 2018) learns a graph neural network from the FastText database (Joulin et al.,\n2016) and leverages the scene prior knowledge and category relationships for navigation. Word\nEmbedding (WE)uses GloVe embedding (Pennington et al., 2014) to indicate the target category\nrather than detection. The association between object appearances and GloVe embeddings is learned\nthrough trail and error.Self-adaptive Visual Navigation (SA VN)(Wortsman et al., 2019) introduces\na meta reinforcement learning method that allows an agent to adapt to unseen environments. Ob-\nject Relationship Graph (ORG)(Du et al., 2020) is a visual representation learning method to\nencode correlation among categories and employs a tentative policy network (TPN) to escape from\ndeadlocks. Baseline is a vanilla version of VTNet. We feed the concatenation of the local instance\nfeatures from DETR and the global feature to A3C for navigation. Note that, our baseline does not\nemploy spatial-enhanced local and positional global descriptors as well as our visual transformer.\n5.3 E VALUATION RESULTS\nImprovement over Baseline.Table 1 indicates that VTNet surpasses the baseline by a large margin\non both success rate (+9.6%) and SPL (+0.085). Baseline only resorts to the detection features and\nglobal feature for navigation. The relations among local instances and the association between the\nvisual observations and actions are not exploited. This comparison suggests that our VT leads to\ninformative visual representations for navigation, and thus signiﬁcantly improves the effectiveness\nand efﬁciency of our navigation system.\nComparison with competing methods.As indicated in Table 1, we observe that VTNet signiﬁ-\ncantly outperforms SP (Yang et al., 2018) and SA VN (Wortsman et al., 2019). Since SP and SA VN\n7\nPublished as a conference paper at ICLR 2021\nRemoteControl\n19 steps\n 21 steps\n 12 steps7 steps\nBaseline ORG VTNetSAVN\nFigure 3: Visual results of four different models in testing environments.The target objects\n(i.e., RemoteControl) are highlighted by the blue boxes. Green and red curves represent success\nand failure cases, respectively. The episode produced by our VTNet is successful in reaching the\ntarget and use shortest steps. In comparison, ORG takes more steps to reach the target. SA VN and\nBaseline miss both targets.\nemploy word embedding as a target indicator while VTNet replaces word embedding with our VT,\nour method achieves expressive object and image region representations for navigation. In addition,\nSP and SA VN concatenate features from various modalities directly to generate visual represen-\ntations. The gap between different modalities may not facilitate navigation policy learning. In\ncontrast, beneﬁting from our pre-training, features from our VT are more correlated to navigation\nactions, thus expediting navigation policy learning.\nOur method outperforms the state-of-the-art method ORG (Du et al., 2020) by +2.9% in success\nrate and +0.055 in SPL. Moreover, when ORG does not employ TPN, the advantage of our method\nbecomes more obvious (6.9% improvement), and this mainly comes from our superior visual presen-\ntations. Since ORG only chooses an object with the highest conﬁdence in each class, the relationship\namong objects is not comprehensive. On the contrary, our method can exploit all the detected in-\nstances to deduce the relationships among objects due to our VT architecture. Moreover, since\nDETR infers the relations between object instances and the global image context, the local features\noutput by the DETR are more informative compared to the object features used in ORG. This can be\nproved by the result when we use Faster R-CNN as our backbone, as indicated by Table 2. We also\nshow a case study in Figure 3 (more visual results are provided in the appendix). Furthermore, we\nalso try to employ TPN to improve our navigation policy. As seen in Table 1, VTNet+TPN improves\nthe success rates but the improvement is not as much as ORG+TPN. This also implies that our visual\nrepresentations signiﬁcantly facilitate navigation action selections.\nCase Study. As illustrated in Figure 3, SA VN and Baseline both issue the termination command\nafter navigating a few steps (7 and 19 steps, respectively), but fail to reach the target. This indicates\nthat the relationships among categories are not clear in SA VN and Baseline. In contrast, both ORG\nand VTNet ﬁnd the target. Since our visual transformer provides clear directional signals, VTNet\nuses the least steps to ﬁnd the object.\n5.4 V ARIANT AND ABLATION STUDY\nIn this section, we analyze the impact of each component in VTNet, including the spatial-enhanced\nlocal descriptor, positional global descriptor, visual transformer that fuses these two spatial-aware\ndescriptors and pre-training scheme.\nTo illustrate the necessity of the spatial enhancement, we directly use the object features without\nspatial enhancement. In this case, our network fails to converge because the feed-forward layers that\npredict bounding-boxes and class labels in DETR are not used in VTNet and spatial information\ncannot be decoded by the navigation network. Thus, spatial enhancement allows an agent to exploit\ninstance location information explicitly.\nAs indicated in Table 2, we achieve better navigation performance using instance features from\nDETR compared to employing Faster R-CNN features following the feature extraction of Du et al.\n(2020) (“Faster R-CNN”). Unlike Faster R-CNN, DETR infers the relations between object in-\nstances and the global image context via its transformer to output the ﬁnal predictions ( i.e., class\nlabels and bounding boxes). Although DETR and Faster R-CNN achieve similar detection perfor-\nmance (Carion et al., 2020), features extracted by DETR are more informative and robust than those\nof Faster R-CNN used in ORG. Speciﬁcally, ORG extracts features from the second layer of the\nbackbone in Faster R-CNN based on the predicted bounding-boxes to ensure the features are com-\nparable, but the features are not the most prominent ones across the feature pyramid. Therefore, the\n8\nPublished as a conference paper at ICLR 2021\nTable 2: Impacts of different components on navigation performances. Faster R-CNN and Faster\nR-CNN† represent instance features extracted by Faster R-CNN following Du et al. (2020) and\nAnderson et al. (2018a), respectively.\nMethod w/o\nglobal\nw/o\ndecoder\nw/o\npe VTNetg\nBaseline VTNet\nFaster\nR-CNN\nFaster\nR-CNN\n†DETR Faster\nR-CNN\nFaster\nR-CNN\n†DETR\nALL Success 67.0\n(2.8)\n67.0\n(1.4)\n71.0\n(0.7)\n70.1\n(1.3)\n56.4\n(0.9)\n57.2\n(1.1)\n62.6\n(0.8)\n70.1\n(1.0)\n70.3\n(1.2)\n72.2\n(1.0)\nSPL 0.390\n(0.021)\n0.373\n(0.013)\n0.432\n(0.009)\n0.411\n(0.009)\n0.319\n(0.007)\n0.308\n(0.008)\n0.365\n(0.010)\n0.396\n(0.010)\n0.387\n(0.012)\n0.449\n(0.007)\nL ≥ 5 Success 54.5\n(3.1)\n53.2\n(1.6)\n61.2\n(0.9)\n60.6\n(1.5)\n42.5\n(1.2)\n46.7\n(1.3)\n51.5\n(1.0)\n61.7\n(1.2)\n62.1\n(1.4)\n63.4\n(1.1)\nSPL 0.357\n(0.021)\n0.343\n(0.017)\n0.416\n(0.010)\n0.395\n(0.011)\n0.270\n(0.009)\n0.276\n(0.010)\n0.345\n(0.012)\n0.399\n(0.016)\n0.376\n(0.012)\n0.440\n(0.009)\nobject features “Faster R-CNN” extracted by ORG are inferior to DETR features, and our navigator\nemploying DETR features outperforms ORG.\nMoreover, we adopt the instance features from Faster R-CNN following the feature extraction fash-\nion of Anderson et al. (2018a) (“Faster R-CNN †”). Thus, we obtain the instance features from the\npenultimate layer of the classiﬁer in Faster R-CNN. We observe that instance features from DETR\nalso improve the navigation performance compared to the Faster R-CNN†features. We speculate the\nimprovements mainly come from the fact that the features output by DETR decoder have embedded\nglobal context information, and those features are more suitable for the feature fusion operations.\nAs seen in Table 2, we ﬁrst remove the global feature from our system (“VTNet w/o global”), and\nthe navigation performance degrades signiﬁcantly. This validates the importance of global features,\nwhich provide contextual guidance to an agent. Moreover, when we remove the positional embed-\nding from the global feature (“VTNet w/o pe”), we observe that both effectiveness and efﬁciency of\nthe navigation decrease. This indicates that the position embeddings facilitate our VT to exploit the\nspatial information of observation regions. Furthermore, when we remove the VT decoder (“VTNet\nw/o decoder”) and concatenate the global and local descriptors directly, our method suffers per-\nformance degradation. This demonstrates that our VT plays a critical role in attending the global\ndescriptors to local ones. Additionally, we feed the positional global features into the transformer\nencoder (“VTNetg”). The navigation performance of VTNetg is superior to that of ORG but slightly\ninferior to the performance of our VTNet. This demonstrates the transformer architecture is effective\nto extract informative visual representations, and assigning different functions to different modules\nwould further facilitate the establishment of mappings in our VT.\nWhen the pre-training scheme is not applied to our VT, our agent fails to learn any effective navi-\ngation policy and thus we do not report the performance. This manifests that our VT pre-training\nprocedure provides a good initialization to our transformer and prior knowledge on associating vi-\nsual observations with navigation actions to agents.\n6 C ONCLUSION\nIn this paper, we proposed a powerful visual representation learning method for visual navigation,\nnamed Visual Transformer Network (VTNet). In our VTNet, a visual transformer (VT) has been\ndeveloped to encode visual observations. Our VT leverages two newly designed spatial-aware de-\nscriptors, i.e., a spatial-enhanced local object descriptor and a positional global descriptor, and then\nfuses those two types of descriptors via multi-head attention to achieve our ﬁnal visual representa-\ntion. Thanks to our VT architecture, all the detected instances will be exploited for understanding\nthe current observation. Therefore, our visual representation is more informative compared to that\nused in state-of-the-art navigation methods. Beneﬁting from our pre-training strategy, our VT is able\nto associate visual representations with navigation actions, thus signiﬁcantly expediting navigation\npolicy learning. Extensive results demonstrate that our VTNet outperforms the state-of-the-art in\nterms of effectiveness and efﬁciency.\nACKNOWLEDGMENTS\nThis work was supported by the ARC Discovery Early Career Researcher Award (DE200101283),\nthe ARC Discovery Project (DP210102801) and the Data61 Collaborative Research Project.\n9\nPublished as a conference paper at ICLR 2021\nREFERENCES\nPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and\nLei Zhang. Bottom-up and top-down attention for image captioning and visual question answer-\ning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n6077–6086, 2018a.\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S ¨underhauf, Ian Reid,\nStephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting\nvisually-grounded navigation instructions in real environments. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pp. 3674–3683, 2018b.\nJohann Borenstein and Yoram Koren. Real-time obstacle avoidance for fast mobile robots. IEEE\nTransactions on systems, Man, and Cybernetics, 19(5):1179–1187, 1989.\nJohann Borenstein and Yoram Koren. The vector ﬁeld histogram-fast obstacle avoidance for mobile\nrobots. IEEE transactions on robotics and automation, 7(3):278–288, 1991.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020.\nKevin Chen, Juan Pablo de Vicente, Gabriel Sepulveda, Fei Xia, Alvaro Soto, Marynel V ´azquez,\nand Silvio Savarese. A behavioral approach to visual navigation with graph localization networks.\narXiv preprint arXiv:1903.00445, 2019.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248–255. Ieee, 2009.\nZhiwei Deng, Karthik Narasimhan, and Olga Russakovsky. Evolving graphical planner: Contextual\nglobal planning for vision-and-language navigation. Advances in Neural Information Processing\nSystems, 33, 2020.\nMWM Gamini Dissanayake, Paul Newman, Steve Clark, Hugh F Durrant-Whyte, and Michael\nCsorba. A solution to the simultaneous localization and map building (slam) problem. IEEE\nTransactions on robotics and automation, 17(3):229–241, 2001.\nHeming Du, Xin Yu, and Liang Zheng. Learning object relation graph and tentative policy for visual\nnavigation. arXiv preprint arXiv:2007.11018, 2020.\nHehe Fan, Yi Yang, and Mohan Kankanhalli. Point 4d transformer networks for spatio-temporal\nmodeling in point cloud videos. In 2021 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2021, 2021.\nKuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. Scene memory transformer for\nembodied agents in long-horizon tasks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 538–547, 2019.\nWeituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning\na generic agent for vision-and-language navigation via pre-training. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13137–13146, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nRonghang Hu, Daniel Fried, Anna Rohrbach, Dan Klein, Trevor Darrell, and Kate Saenko. Are\nyou looking? grounding to multiple modalities in vision-and-language navigation. arXiv preprint\narXiv:1906.00347, 2019.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efﬁcient\ntext classiﬁcation. arXiv preprint arXiv:1607.01759, 2016.\n10\nPublished as a conference paper at ICLR 2021\nGregory Kahn, Adam Villaﬂor, Bosen Ding, Pieter Abbeel, and Sergey Levine. Self-supervised\ndeep reinforcement learning with generalized computation graphs for robot navigation. In 2018\nIEEE International Conference on Robotics and Automation (ICRA), pp. 1–8. IEEE, 2018.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel\nGordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment\nfor Visual AI. arXiv, 2017.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difﬁ-\nculty of training transformers. arXiv preprint arXiv:2004.08249, 2020.\nArjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra.\nImproving vision-and-language navigation with image-text pairs from the web. arXiv preprint\narXiv:2004.14973, 2020.\nPiotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino,\nMisha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in\ncomplex environments. arXiv preprint arXiv:1611.03673, 2016.\nV olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement\nlearning. In International conference on machine learning, pp. 1928–1937, 2016.\nArsalan Mousavian, Alexander Toshev, Marek Fi ˇser, Jana Ko ˇseck´a, Ayzaan Wahid, and James\nDavidson. Visual representations for semantic target driven navigation. In 2019 International\nConference on Robotics and Automation (ICRA), pp. 8846–8852. IEEE, 2019.\nGiuseppe Oriolo, Marilena Vendittelli, and Giovanni Ulivi. On-line map building and navigation for\nautonomous mobile robots. In Proceedings of 1995 IEEE International Conference on Robotics\nand Automation, volume 3, pp. 2900–2906. IEEE, 1995.\nEmilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforcement\nlearning. arXiv preprint arXiv:1702.08360, 2017.\nJeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP), pp. 1532–1543, 2014.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. In Advances in neural information processing systems,\npp. 91–99, 2015.\nNikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory\nfor navigation. arXiv preprint arXiv:1803.00653, 2018.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nGabriel Sepulveda, Juan Carlos Niebles, and Alvaro Soto. A deep learning based behavioral ap-\nproach to indoor autonomous navigation. In 2018 IEEE International Conference on Robotics\nand Automation (ICRA), pp. 4646–4653. IEEE, 2018.\nWilliam B Shen, Danfei Xu, Yuke Zhu, Leonidas J Guibas, Li Fei-Fei, and Silvio Savarese. Sit-\nuational fusion of visual representation for visual navigation. arXiv preprint arXiv:1908.09073,\n2019.\nTianqi Tang, Xin Yu, Xuanyi Dong, and Yi Yang. Auto-navigator: Decoupled neural architecture\nsearch for visual navigation. In Proceedings of the IEEE/CVF Winter Conference on Applications\nof Computer Vision, pp. 3743–3752, 2021.\n11\nPublished as a conference paper at ICLR 2021\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nXin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang,\nWilliam Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imita-\ntion learning for vision-language navigation. InProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 6629–6638, 2019.\nChristopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.\nMitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.\nLearning to learn how to learn: Self-adaptive visual navigation using meta-learning. In Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6750–6759,\n2019.\nYi Wu, Yuxin Wu, Aviv Tamar, Stuart Russell, Georgia Gkioxari, and Yuandong Tian. Bayesian\nrelational memory for semantic visual navigation. In Proceedings of the IEEE International Con-\nference on Computer Vision, pp. 2769–2779, 2019.\nWei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, and Roozbeh Mottaghi. Visual semantic\nnavigation using scene priors. arXiv preprint arXiv:1810.06543, 2018.\nYuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali\nFarhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In\n2017 IEEE international conference on robotics and automation (ICRA), pp. 3357–3364. IEEE,\n2017.\n12\nPublished as a conference paper at ICLR 2021\nA A PPENDIX\nA.1 F EATURE DETAILS IN VTN ET\nSpatial Feature\n100×7\nGlobal Feature\nInstance Feature\n100×256\n7×7×256\nPositional Embedding\n100×249\n7×7×256\nSpatial-enhanced \nLocal Descriptor\n100×256\nPositional Global Descriptor\n7×7×256 49×256 49×256\nVisual \nRepresentation\nLocal BranchGlobal Branch\nLocation\nenhancement\nPosition \nenhancement\nVT encoder\n100×256\nEncoder Output\nVT decoder\nVT decoder\n7×7×512\nFigure 4: Illustration feature ﬂowchart in VTNet.\nFor reproducibility, we illustrate the detailed feature ﬂowchart of our VTNet in Figure 4. We extract\ninstance features R100×256 and location features R100×249, and concatenate them into a spatial-\nenhanced local descriptor R100×256. The local branch integrates semantic labels R100×1, bounding\nboxes R100×4, conﬁdences R100×1 and the target labels R100×1 for the current observation. In\nthe global branch, the positional embedding R7×7×256 is added to the global feature R7×7×256,\nleading to a positional global descriptor R49×256. The spatial-enhanced local and positional global\ndescriptors are fused by VT encoder and then the visual representationR49×256 is output by our VT\ndecoder.\nA.2 F AILURE CASE STUDY\n1.572𝑚 > 1.5𝑚1.530𝑚 > 1.5𝑚\nTarget: LaptopTarget: Bowl\nFigure 5: Visual results of failure cases in testing environments.The target objects (i.e., Bowl\nand Laptop) are highlighted by the blue boxes. Red lines indicate the distance between the agent and\nthe target object. Gray curves represent trajectories of agents. Both episodes fail because distances\n(i.e., 1.530m and 1.572m) between the agent and the target in the ﬁeld of view are larger than the\nthreshold (i.e., 1.5m).\nAs demonstrated in Figure 5, our VTNet fails to reach targets because the distances between agents\nand targets are larger than the threshold distance (i.e., 1.5m). In these two failure cases, agents ﬁnd\ntargets but implement the termination action before reaching a position closer than the threshold.\nDue to the lack of depth information and variances of target object sizes, an agent may predict that\nit is within a 1.5 meter radius of the target by mistake and thus terminates current episode.\n13\nPublished as a conference paper at ICLR 2021\nA.3 C ASE STUDY\nSAVN Baseline ORG VTNet\n10 steps14 steps8 steps12 steps\nMicrowave\n25 steps\n 100 steps\n 19 steps\n 6 steps\nBowl\n48 steps\n 70 steps\n 12 steps\n 8 steps\nCellPhone\n32 steps\n 18 steps\n 100 steps\n 16 steps\nFloorLamp\n10 steps\n 20 steps\n 100 steps\n 19 steps\nAlarmClockAlarmClock\n28 steps\n 17 steps\n 21 steps\n 27 steps\nSink\n13 steps\n 3 steps\n 2 steps\n 3 steps\nFigure 6: Visual results of four different models in testing environments.We compare VTNet\nwith SA VN (Wortsman et al., 2019), Baseline and ORG (Du et al., 2020). The target objects are\nhighlighted by the blue boxes. Green and red curves indicate success and failure cases, respectively.\nOur VTNet successfully reaches targets and uses the shortest steps.\n14\nPublished as a conference paper at ICLR 2021\nA.4 D IFFERENT VISUAL TRANSFORMER ARCHITECTURES .\nTable 3: Comparison of visual transformer architectures. We report the pre-training accuracy on the\nvalidation dataset, navigation success rate and SPL on the test dataset.\nMulti-head numbers Encoder layers Decoder layers Accuracy Success SPL\n4\n1 1 0.722 71.2 0.433\n2 2 0.723 72.2 0.449\n4 4 0.715 70.0 0.419\n8\n1 1 0.707 70.4 0.422\n2 2 0.718 70.9 0.436\n4 4 0.710 68.9 0.423\n6 6 0.701 68.2 0.411\nWe construct different transformer architectures by varying the number of encoder and decoder\nlayers. Table 3 summarizes the performance of these architectures. We observe that as a visual\ntransformer becomes too deep, a transformer may fail to converge to an optimal policy. On the other\nhand, a transformer with a single encoder and decoder layer does not have sufﬁcient network capa-\nbility to produce representative features. The highest success rate is achieved when a VT contains\nfour multi-head self-attention mechanism modules and two layers in the encoder and decoder.\nA.5 N ECESSITY OF PRE -TRAINING SCHEME\nFigure 7: Average episode lengths of VTNet and VTNet without pre-training during training.\nWe compare VTNet with VTNet without pre-training scheme. Blue and orange curves represent\nVTNet and VTNet w/o pre-training, respectively.\nAs demonstrated in Figure 7, our VTNet spends nearly 10 steps per episode in training, while the\nnavigator w/o pre-training scheme often fails to reach targets and stops around 5 steps after being\ntrained tens of thousands of episodes. Due to the large parameters and complex architectures of\ntransformers, it is often difﬁcult to train our transformers from scratch (Liu et al., 2020). Without a\ngood initialization for our VT, it is very difﬁcult to learn our VT and policy network in an end-to-end\nfashion with RL rewards. This is because the visual representations from VT are not informative\nor even meaningless and the inferior visual representations would harm policy network learning.\nAs a result, the navigation policy network may be trapped into a local minimum (i.e., terminating\nnavigation early to avoid more penalties) and our VT cannot receive positive rewards from preceding\ntrajectories.\n15\nPublished as a conference paper at ICLR 2021\nA.6 A DDITIONAL VISUALIZATION RESULTS\nMoveAhead\nLookUp\nLookDown\nRotateRight\nRotateLeft\nDone\nActivation MapObservation Instances\nPlate\nPan\nStoveBurner\nAction\nMoveAhead\nLookUp\nLookDown\nRotateRight\nRotateLeft\nDone\nGarbageCan\nLookUp\nLookDown\nRotateRight\nDoneToaster\nStoveBurner\nKettle\nMoveAhead\nRotateLeft\nFigure 8: Visualizations of attention scores.The target classes ( i.e., StoveBurner, GarbageCan,\nKettle) are highlighted by green bounding boxes. Our agent detects the instances of interest and then\nattends the detected instances to the global image regions by our VT. We observe that high attention\nscores are obtained on the areas corresponding to the targets. Guided by the visual representations,\nthe agent selects actions to approach the targets.\n16",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7589864730834961
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6679832339286804
    },
    {
      "name": "Representation (politics)",
      "score": 0.5764365196228027
    },
    {
      "name": "Object (grammar)",
      "score": 0.5677124261856079
    },
    {
      "name": "Computer vision",
      "score": 0.5205453634262085
    },
    {
      "name": "Transformer",
      "score": 0.45233866572380066
    },
    {
      "name": "Visualization",
      "score": 0.42165905237197876
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3969342112541199
    },
    {
      "name": "Engineering",
      "score": 0.07985156774520874
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 36
}