{
  "title": "Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition",
  "url": "https://openalex.org/W2970463839",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2150032407",
      "name": "Yufan Jiang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2126918207",
      "name": "Chi Hu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A1983914940",
      "name": "Tong Xiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122370144",
      "name": "Chunliang Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2496766346",
      "name": "Jingbo Zhu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2928299337",
    "https://openalex.org/W2473934411",
    "https://openalex.org/W2951886768",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1511986666",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2810075754",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2981985696",
    "https://openalex.org/W4295185264",
    "https://openalex.org/W4300687381",
    "https://openalex.org/W2962847160",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2946558277",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W2782417188",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W4294555862",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W4288374925",
    "https://openalex.org/W2147880316"
  ],
  "abstract": "Yufan Jiang, Chi Hu, Tong Xiao, Chunliang Zhang, Jingbo Zhu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 3585–3590,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n3585\nImproved Differentiable Architecture Search for\nLanguage Modeling and Named Entity Recognition\nYufan Jiang1, Chi Hu1, Tong Xiao12∗, Chunliang Zhang12, Jingbo Zhu12\n1NLP Lab, Northeastern University, Shenyang, China\n2NiuTrans Research, Shenyang, China\njiangyufan2018@outlook.com, huchinlp@gmail.com\n{xiaotong, zhangchunliang, zhujingbo}@mail.neu.edu.cn\nAbstract\nIn this paper, we study differentiable neural\narchitecture search (NAS) methods for natu-\nral language processing. In particular, we im-\nprove differentiable architecture search by re-\nmoving the softmax-local constraint. Also,\nwe apply differentiable NAS to named entity\nrecognition (NER). It is the ﬁrst time that dif-\nferentiable NAS methods are adopted in NLP\ntasks other than language modeling. On both\nthe PTB language modeling and CoNLL-2003\nEnglish NER data, our method outperforms\nstrong baselines. It achieves a new state-of-\nthe-art on the NER task.\n1 Introduction\nNeural architecture search (NAS) has become\npopular recently in machine learning for their abil-\nity to ﬁnd new models and to free researchers from\nthe hard work of designing network architectures.\nThe earliest of these approaches use reinforcemen-\nt learning (RL) to learn promising architectures\nin a discrete space (Zoph and Le, 2016), where-\nas others have successfully modeled the problem\nin a continuous manner (Liu et al., 2019; Xie\net al., 2019b; Huang and Xiang, 2019). As an\ninstance of the latter, differentiable architecture\nsearch (DARTS) employs continuous relaxation to\narchitecture representation and makes gradient de-\nscent straightforwardly applicable to search. This\nleads to an efﬁcient search process that is orders of\nmagnitude faster than the RL-based counterparts.\nLike recent methods in NAS (Xie and Yuille,\n2017; Zoph and Le, 2016; Baker et al., 2016),\nDARTS represents networks as a directed acyclic\ngraph for a given computation cell (see Figure\n1(a)). An edge between nodes performs a pre-\ndeﬁned operation to transform the input (i.e., tail)\n∗Corresponding author.\nxt ht−1\n0\n1\n2\n3\ns\ns s\ns\ns\ns\ns : softmax\n(a) DARTS cell\nxt ht−1\n0\n1\n2\n3\ns\ns\ns\n(b) Our cell\nFigure 1: An overview of DARTS cell and our cell\nto the output (i.e., head). For a continuous net-\nwork space, DARTS uses the softmax trick to re-\nlax the categorical choice of edges to soft deci-\nsions. Then, one can optimize over the graph us-\ning standard gradient descent methods. The opti-\nmized network is inferred by choosing the edges\nwith maximum weights in softmax.\nHowever, DARTS is a “local” model because\nthe softmax-based relaxation is imposed on each\nbundle of edges between two nodes. This leads to\na biased model in that edges coming from different\nnodes are not comparable. Such a constraint limit-\ns the inference space to sub-graphs with one edge\nbetween each pair of nodes. Also, the learned net-\nwork might be redundant because every node has\nto receive edges from all predecessors no matter\nthey are necessary or not. This problem is simi-\nlar to the bias problem in other graph-based mod-\nels where local decisions make the model non-\noptimal (Lafferty et al., 2001; Daphne Koller and\nNir Friedman, 2009).\nHere we present an improvement of DARTS,\n3586\ncalled I-DARTS, that further relaxes the softmax-\nlocal constraint. The idea is simple - we consider\nall incoming edges to a given node in a single soft-\nmax. This offers a broader choice of edges and en-\nlarges the space we infer the network from. For ex-\nample, one can simultaneously select multiple im-\nportant edges between two nodes and leave some\nnode pairs unlinked (see Figure 1(b)).\nI-DARTS outperforms strong baselines on the\nPTB language modeling and CoNLL named entity\nrecognition (NER) tasks. This gives a new state-\nof-the-art on the NER dataset. To our knowledge,\nit is the ﬁrst time to apply differentiable architec-\nture search methods to NLP tasks other than lan-\nguage modeling. More interestingly, we observe\nthat our method is 1.4X faster than DARTS for\nconvergence of architecture search. Also, we pro-\nvide the architectures learned by I-DARTS, which\ncan be referred for related tasks.\n2 The Method\nAlthough we will restrict ourselves to language\nmodeling and NER for experiments, in the sec-\ntion, we discuss the more general case. We choose\nrecurrent neural networks (RNNs) to model the\nsequence generation and tagging problems. Giv-\nen a sequence of input vectors {x1,...,x L}, we\nrepeat applying RNN cells to generate the out-\nput {h1,...,h L}. The RNN cell is deﬁned as:\nht = g(xt,ht−1), where t is the time step and\ng(·,·) is the function of the cell. In NAS, the ob-\njective is to search for a goodg(·,·) in an automat-\nic fashion.\n2.1 Architecture Search Space\nWe follow the assumption that g(·,·) is a DAG\nconsisting of Nnodes and edges among them (Liu\net al., 2019; Xie et al., 2019b; Pham et al., 2018).\nAn edge oi,j(·) between node pair ( i, j) indicates\nan activation function from node j to node i. For\nnode i, it simply sums over vectors from all pre-\ndecessor nodes (j <i), followed by a linear trans-\nformation with a parameter matrix Wi. More for-\nmally, let si be the state of node i. We deﬁne si to\nbe:\nsi =\n∑\nj<i\noi,j(sj ·Wj) (1)\nSee Figure 1 for an example network (red lines).\nNote that this model can encode an exponential\nnumber of graphs by choosing different sub-sets\nof edges (i.e., choosing oi,j(·) for each (i,j)). The\noutput of search is the optimal edge selection and\nthe corresponding network.\n2.2 Improved DARTS\nGiven a set of edges {oi,j\nk }, one can try each oi,j\nk\nto induce a network, and then train and evaluate\nit. The optimal choice is the edge with highest\naccuracy on the validation set. In I-DARTS, we\ninstead do this in a soft way. We re-deﬁne si as:\nsi =\n∑\nj<i\n∑\nk\nαi,j\nk ·oi,j\nk (sj ·Wj) (2)\nwhere αi,j\nk is the weight indicating the importance\nof oi,j\nk (·). It is computed by the softmax normal-\nization over edges between nodes iand j, like this\nαi,j\nk = exp(wi,j\nk )\n∑\nk′exp(wi,j\nk′)\n(3)\nwhere wi,j\nk is the model parameter. This model\nreduces the architecture search problem to learn\ncontinuous variables {αi,j\nk }, which can be imple-\nmented using efﬁcient gradient descent methods.\nAfter training, the ﬁnal architecture is encoded by\nthe edges with largest weights.\nEq. (3) imposes a constraint that weights {αi,j\nk }\nare normalized for each j. Such a model in gen-\neral faces the local decision and bias problem-\ns as pointed out in graph-based methods (Laffer-\nty et al., 2001; Daphne Koller and Nir Friedman,\n2009). Moreover, the inference has to be per-\nformed in a smaller space because we have to infer\nexactly one edge between each node pair and ex-\nclude networks violating this constraint.\nHere we remove the constraint and system bias.\nTo this end, we compute the softmax normaliza-\ntion over all incoming edges for node i:\nαi,j\nk = exp(wi,j\nk )\n∑\nj<i\n∑\nk′exp(wi,j\nk′)\n(4)\nIt provides us a way to compare all incoming\nedges in the same manner, rather than making a\nlocal decision via a bundle of edges from node j.\nAs another bonus, this method can search for net-\nworks that are not covered by DARTS, e.g., net-\nworks that contain two edges between the same\nnode pair.\nSee Figure 1(b) for an illustration of our\nmethod. To infer the optimal architecture, we ba-\nsically do the same thing as in DARTS. The differ-\n3587\nence lies in that we select top-nedges with respect\nto αi,j\nk . Here nis a hyper-parameter that control-\ns the density of the network. E.g., n = 1 means\na sparse net, and n = ∞means a very dense net\ninvolving all those edges.\n3 Experiments\nWe test our method on language modeling and\nnamed entity recognition tasks. Our experiments\nconsist of two parts: recurrent neural architecture\nsearch and architecture evaluation. In architecture\nsearch, we search for good RNN cell architectures.\nThen, we train and evaluate the learned architec-\nture.\n3.1 Architecture Search\nFor language modeling, we run neural search\non the PTB corpus. We use the standard pre-\nprocessed version of the dataset (Pham et al.,\n2018). To make it comparable with previous work,\nwe copy the setup used in (Pham et al., 2018; Liu\net al., 2019). The recurrent cell consist of 8 nodes.\nThe candidate operation set of every edge contain\n5 activation functions, including zeroize, tanh, re-\nlu, sigmoid, and identity. To learn architectures,\nwe run the search system for 40 training epochs\nwith a batch size of 256. We optimize models pa-\nrameters {Wi}using SGD with a learning rate of\n20 and a weight decay rate of 5e-7, and optimized\nsoftmax relaxation parameters {wi,j\nk }by Adam\nwith a learning rate of 3e-3 and a weight decay\nrate of 1e-3. For RNN models, we use a single-\nlayer recurrent network with embedding and hid-\nden sizes = 300. It takes us 4 hours to learn the\narchitecture on a single GPU of NVIDIA 1080Ti.\nFor named entity recognition, we choose the\nCONLL-2003 English dataset. We follow the\nsame setup as in language modeling but with a d-\nifferent learning rate (0.1) and a different hidden\nlayer size (256). It takes us 4 hours to learn the\narchitecture on the same GPU.\n3.2 Architecture Evaluation\nFirstly, the discovered architecture is evaluated on\nthe language modeling task. Before that, we train\nit on the same data used in architecture search. The\nsize of hidden layers is set to 850. We use av-\neraged SGD to train the model for 3,000 epochs,\nwith a learning rate of 20 and a weight decay rate\nof 8e-7. For a fair comparison, we do not ﬁne-tune\nthe model at the end of the training.\nArchitecture Perplexity Search Cost\nval test (GPU days)\nV-RHN 67.9 65.4 -\nLSTM 60.7 58.8 -\nLSTM + SC 60.9 58.3 -\nLSTM + SE 58.1 56.0 -\nENAS 60.8 58.6 0.50\nDARTS 58.3 56.1 0.25\nRandom RNNs 63.7 61.2 -\nI-DARTS (n = 1) 58.0 56.0 0.17\nI-DARTS (n = 2) - - -\nTable 1: Perplexities on PTB (lower is better). V-RHN\n(Zilly et al., 2016) indicates Variational RHN. LSTM +\nSC (Yang et al., 2018) indicates LSTM with skip con-\nnection. LSTM + SE (Merity et al., 2018) indicates\nLSTM with mixture of softmax. Random RNNs indi-\ncates that the network generated by random initialized.\nTable 1 shows the perplexities of different RN-\nN models on PTB. We also report the results of\nprevious systems. The model discovered by I-\nDARTS achieves a validation perplexity of 58.0\nand a test perplexity of 56.0 when n = 1. It is\non par with the state-of-the-art models that are de-\nsigned either manually or automatically. Howev-\ner, we ﬁnd that the model failed to optimize when\nn = 2. It might result from the complex interac-\ntion between operations. We leave this issue for\nfuture study.\nSince architecture search is initialization-\nsensitive (Pham et al., 2018; Liu et al., 2019), we\nsearch the architectures for 4 times with different\nrandom seeds. We evaluate the architecture every\n10 search epochs by retraining it on PTB for 500\nepochs. We compare DARTS with our I-DARTS\nmethod with the same random seed. See Figure\n2(b) for averaged validation perplexities over 4 d-\nifferent runs at different search epochs. We see\nthat I-DARTS is easier to converge than DARTS (4\nhours). It is 1.4X faster than that of DARTS. An-\nother interesting ﬁnding is that I-DARTS achieves\na lower validation perplexity than DARTS during\narchitecture search. This may indicate better ar-\nchitectures found by I-DARTS because the search\nmodel is optimized with respect to validation per-\nplexity.\nThen, we test the learned architecture in a\nnamed entity recognition system on the English\ndata from CoNLL-2003 shared task (Sang and\nMeulder, 2003). Following previous work (Akbik\net al., 2018; Peters et al., 2017), we report the av-\neraged F1 score over 5 runs on the test set. For\nmodeling, we choose the single-layer RNN-CRF\n3588\nmodel because it achieved state-of-the-art results\non several sequence labeling tasks (Lample et al.,\n2016; Ma and Hovy, 2016). We use GloVe 100-\ndimensional word embeddings (Pennington et al.,\n2014) and pooled contextual embeddings (Akbik\net al., 2019) as pre-trained word embeddings. We\nreplace the standard bidirectional LSTMs with the\ndiscovered recurrent neural cells. Also, we set\nthe hidden layer size to 512 and apply variational\ndropout to the input and output of the RNN layer.\nWe train the network using SGD with a learning\nrate of 0.1 and a gradient clipping threshold of 5.0.\nWe reduce the learning rate by a factor of 0.25 if\nthe test error does not decrease for 2 epochs.\nTable 2 shows a comparison of different meth-\nods. Our baseline uses RNN cells generated from\nrandom initialized whose F1-score varies greatly\nand is lower than that of the standard LSTMs. I-\nDARTS signiﬁcantly outperforms Random RNNs\nand DARTS. The best score is achieved whenn=\n1. It indicates that the task prefers a sparse net-\nwork. Also, we see that our model works with the\nadvanced pre-trained language models in that we\nreplace the LSTM cell to our cell. The I-DARTS\narchitecture yields a new RNN-based state-of-the-\nart on this task (93.47 F1-score). In Table 2, We\nﬁnd it interesting that Random RNNs are good for\nNER task. This may result from the design of\nsearch space that ﬁt for such tasks substantially.\nSearch space is also a key factor in neural architec-\nture search that new efforts should focus on (Xie\net al., 2019a).\nWe visualize the discovered cells in Figure 3.\nEach cell is a directed acyclic graph consisting of\nan ordered sequence of 8 nodes with an activation\nfunction applied on each edge. These automati-\ncally discovered cells are complex and hard to be\ndesigned manually. An interesting phenomenon\ncomes up that the best architecture on language\nmodeling is different from that on name entity\nrecognition. This might result from the fact that\ndifferent tasks have different inductive bias. Al-\nso, this suggests the possibility of architecture se-\nlection from the top-ksearch results on the target\ntask.\n4 Related Work\nNeural architecture search has been proposed\nto automatically search for better architectures,\nshowing competitive results on several tasks, e.g.,\nimage recognition and language modeling. A s-\n0 10 20 30 40\n62\n64\n66\n68\n70\nnumber of search epochs\nValid Perplexity\nI-DARTS\nDARTS\nFigure 2: Perplexity vs. search epoch number.\nModel F1\nbest published\nBiLSTM-CRF (Lample et al., 2016) 90.94\nBiLSTM-CRF+ELMo (Peters et al., 2018) 92.22\nBERT Base (Devlin et al., 2018) 92.40\nBERT Large (Devlin et al., 2018) 92.80\nBiLSTM-CRF+PCE (Akbik et al., 2019) 93.18\nRandom RNNs w/o pre-trained LM 90.64\nDARTS w/o pre-trained LM 91.05\nI-DARTS (n = 2) w/o pre-trained LM 90.96\nI-DARTS (n = 1) w/o pre-trained LM 91.23\nRandom RNNs 92.89\nDARTS 93.13\nI-DARTS (n = 2) 93.14\nI-DARTS (n = 1) 93.47\nTable 2: F1 scores on the CoNLL-2003 English NER\ntest set.\ntrand of NAS research focuses on reinforcemen-\nt learning (Zoph and Le, 2016) and evolutionary\nalgorithm-based (Xie and Yuille, 2017) method-\ns. They are powerful but inefﬁcient. Recent ap-\nproaches speed up the search process by weight\nsharing (Pham et al., 2018) and differentiable ar-\nchitecture search (Liu et al., 2019). But there is no\ndiscussion on the softmax-local problem in previ-\nous work. Moreover, previous methods are often\ntested on language modeling. It is rare to see stud-\nies on these methods for other NLP tasks.\nxt\n0\nht-1\n1identity\n2relu\nht\n3identity\n7identity\n4\ntanh\n5relu\n6relu\n8relu\nxt\n0\nht-1\n1identity\n2identity\n3\nidentity\n5identity\n7relu\nht4identity\n6identity\n8identity\nFigure 3: Cells discovered by I-DARTS for language\nmodeling (top) and NER (bottom).\n3589\n5 Conclusions\nWe improved the DARTS to address the bias prob-\nlem by removing the softmax-local constraint.\nOur method is search efﬁcient and discovers sev-\neral better architectures for PTB language model-\ning and CoNLL named entity recognition (NER)\ntasks. We plan to consider the network density\nproblem in search and apply I-DARTS to more\ntasks in our future study.\n6 Acknowledgments\nThis work was supported in part by the Nation-\nal Science Foundation of China (Nos. 61876035,\n61732005 and 61432013), the National Key R&D\nProgram of China (No. 2019QY1801) and the\nOpening Project of Beijing Key Laboratory of\nInternet Culture and Digital Dissemination Re-\nsearch. We also thank the reviewers for their in-\nsightful comments.\nReferences\nAlan Akbik, Tanja Bergmann, and Roland V ollgraf.\n2019. Pooled contextualized embeddings for named\nentity recognition. In NAACL.\nAlan Akbik, Duncan Blythe, and Roland V ollgraf.\n2018. Contextual string embeddings for sequence\nlabeling. In COLING.\nBowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh\nRaskar. 2016. Designing neural network architec-\ntures using reinforcement learning. arXiv preprint\narXiv:1611.02167.\nDaphne Koller and Nir Friedman. 2009. Probabilis-\ntic Graphical Models - Principles and Techniques.\nMIT press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nZhiheng Huang and Bing Xiang. 2019. Wenet:\nWeighted networks for recurrent network architec-\nture search. arXiv preprint arXiv:1904.03819.\nJohn Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001. Conditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 260–270, San Diego, California. Association\nfor Computational Linguistics.\nHanxiao Liu, Karen Simonyan, and Yiming Yang.\n2019. DARTS: Differentiable architecture search.\nIn International Conference on Learning Represen-\ntations.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1064–1074, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models. In International Conference on\nLearning Representations.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In EMNLP.\nMatthew Peters, Waleed Ammar, Chandra Bhagavatu-\nla, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn Proceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1756–1765, Vancouver, Cana-\nda. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Mat-\nt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nHieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le,\nand Jeff Dean. 2018. Efﬁcient neural architecture\nsearch via parameter sharing. arXiv preprint arX-\niv:1802.03268.\nErik Tjong Kim Sang and Fien De Meulder. 2003. In-\ntroduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In CoNLL.\nLingxi Xie and Alan Yuille. 2017. Genetic cnn. In\nProceedings of the IEEE International Conference\non Computer Vision, pages 1379–1388.\nSaining Xie, Alexander Kirillov, Ross Girshick, and\nKaiming He. 2019a. Exploring randomly wired\nneural networks for image recognition. arXiv\npreprint arXiv:1904.01569.\nSirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin.\n2019b. SNAS: stochastic neural architecture search.\nIn International Conference on Learning Represen-\ntations.\n3590\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank RNN language model. In In-\nternational Conference on Learning Representation-\ns.\nJulian G. Zilly, Rupesh Kumar Srivastava, Jan Koutn´ık,\nand J¨urgen Schmidhuber. 2016. Recurrent highway\nnetworks. In ICML.\nBarret Zoph and Quoc V Le. 2016. Neural architecture\nsearch with reinforcement learning. arXiv preprint\narXiv:1611.01578.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7580636739730835
    },
    {
      "name": "Natural language processing",
      "score": 0.6585886478424072
    },
    {
      "name": "Zhàng",
      "score": 0.6368177533149719
    },
    {
      "name": "Architecture",
      "score": 0.6149641275405884
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5715323686599731
    },
    {
      "name": "Natural language",
      "score": 0.5531241297721863
    },
    {
      "name": "Differentiable function",
      "score": 0.45854830741882324
    },
    {
      "name": "Programming language",
      "score": 0.3788387179374695
    },
    {
      "name": "China",
      "score": 0.14509311318397522
    },
    {
      "name": "History",
      "score": 0.12722113728523254
    },
    {
      "name": "Mathematics",
      "score": 0.07301566004753113
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9224756",
      "name": "Northeastern University",
      "country": "CN"
    }
  ]
}