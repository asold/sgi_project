{
  "title": "Optimizing Retrieval-augmented Reader Models via Token Elimination",
  "url": "https://openalex.org/W4389524239",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3152539870",
      "name": "Moshe Berchansky",
      "affiliations": [
        "Bar-Ilan University",
        "Intel (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2883286745",
      "name": "Peter Izsak",
      "affiliations": [
        "Intel (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2990253534",
      "name": "Avi Caciularu",
      "affiliations": [
        "Bar-Ilan University",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A356610775",
      "name": "Ido Dagan",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A2013413257",
      "name": "Moshe Wasserblat",
      "affiliations": [
        "Intel (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3169841173",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4389524599",
    "https://openalex.org/W4285595056",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4287185415",
    "https://openalex.org/W4385570209",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4385567182",
    "https://openalex.org/W4309417034",
    "https://openalex.org/W4287901267",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W4366330736",
    "https://openalex.org/W3174708387",
    "https://openalex.org/W3174821868",
    "https://openalex.org/W2131241448",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4384652670",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W4226101361",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W4206334925",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W4387355858",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2981757109",
    "https://openalex.org/W2086511124",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W3034292689"
  ],
  "abstract": "Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model applied across a variety of open-domain tasks, such as question answering, fact checking, etc. In FiD, supporting passages are first retrieved and then processed using a generative model (Reader), which can cause a significant bottleneck in decoding time, particularly with long outputs. In this work, we analyze the contribution and necessity of all the retrieved passages to the performance of reader models, and propose eliminating some of the retrieved information, at the token level, that might not contribute essential information to the answer generation process. We demonstrate that our method can reduce run-time by up to 62.2%, with only a 2% reduction in performance, and in some cases, even improve the performance results.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1506–1524\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nOptimizing Retrieval-augmented Reader Models via Token Elimination\nMoshe Berchansky♠♡ Peter Izsak♠ Avi Caciularu♡♣\nIdo Dagan♡ Moshe Wasserblat♠\n♠Intel Labs, Israel ♡Bar-Ilan University, Israel ♣Google Research\n{moshe.berchansky,peter.izsak,moshe.wasserblat}@intel.com\navica@google.com, dagan@cs.biu.ac.il\nAbstract\nFusion-in-Decoder (FiD) is an effective\nretrieval-augmented language model applied\nacross a variety of open-domain tasks, such as\nquestion answering, fact checking, etc. In FiD,\nsupporting passages are first retrieved and then\nprocessed using a generative model (Reader),\nwhich can cause a significant bottleneck in de-\ncoding time, particularly with long outputs. In\nthis work, we analyze the contribution and ne-\ncessity of all the retrieved passages to the per-\nformance of reader models, and propose elimi-\nnating some of the retrieved information, at the\ntoken level, that might not contribute essential\ninformation to the answer generation process.\nWe demonstrate that our method can reduce\nrun-time by up to 62.2%, with only a 2% reduc-\ntion in performance, and in some cases, even\nimprove the performance results.1\n1 Introduction\nThe task of Open-Domain Question Answering\n(ODQA) (V oorhees, 1999) consists of answering\nquestions using external knowledge, which is used\nas a source of relevant information that might be\nhelpful for a model to extract or generate the right\nanswer for a question. The expected answer can\nbe short and concise (Kwiatkowski et al., 2019), or\nlong and detailed (Fan et al., 2019), in which it is\ncalled Long-Form Question Answering (LFQA).\nThe retriever-reader architecture has been\nwidely-used and adopted for ODQA tasks (Chen\net al., 2017). The retriever fetches the most rele-\nvant passages using the question as a query. Then,\nthe reader extracts or generates an answer, using\nthe question and the relevant passages. The explicit\nstructure of the system, consisting of these two\nsub-modules, allows for a decoupled optimization\nof either the retrieving or the reading process. In\nthis work, we exclusively focus on the optimization\n1We provide the source code for our work at https://\ngithub.com/mosheber/token_elimination.\nDecoder layer l\nDecoder layer l +1\nToken Filtering\nCompute cross attention\nPassage 1 Passage 2\nInput passage\nkeys/values\nFilter and reshape\nToken Filtering\nToken t\nFigure 1: An overview of our Token Filtering method.\nHere, two passages are considered as the input to the\ndecoder module. The Token Filtering operation is per-\nformed when generating token t, between decoder layer\nl and l+1. Using the representation of token t at layer l,\nthe cross-attention is computed for all the input tokens.\nThen, we filter out the lowest ranked tokens from the\ninput (marked in yellow), with only the highest ranking\ninput tokens being used from the next generated token\nonward.\nof the reading process. In order to assess ODQA\nmethods, Petroni et al. (2021) presented a compre-\nhensive evaluation framework that examines these\nmethods in various open-domain tasks. Our study\nspecifically concentrates on the Long-Form Ques-\ntion Answering task, utilizing the ELI5 dataset as\na foundation (Fan et al., 2019).\nThere has been rapid and remarkable progress in\nretriever-reader systems for solving ODQA tasks\nusing a generative approach (Sachan et al., 2021;\nIzacard et al., 2022). One such prominent approach\nis Fusion-in-Decoder (FiD) (Izacard and Grave,\n2021b) that utilizes a generative text-to-text model\nto generate an answer. Despite the significant per-\nformance improvements, there are several compu-\ntational bottlenecks associated with FiD that have\n1506\na negative impact on efficiency. The most promi-\nnent ones are: (a) the need to retrieve a relatively\nlarge amount of documents to reach peak perfor-\nmance, and (b) an extensive cross-attention opera-\ntion, caused by processing multiple concatenated\nretrieved passages, applied repeatedly for every\ngenerated token. These bottlenecks are negatively\namplified in the case of Long-Form Question An-\nswering.\nPrevious works have attempted to mitigate these\nbottlenecks, either by limiting the input to the\nreader or by directly optimizing it in a variety\nof methods. Yu et al. (2021) included a passage\nre-ranker inside the reader which aimed to filter\nout the least relevant passages during encoding.\nde Jong et al. (2022) optimized the decoder module\nby pretraining a modified and optimized architec-\nture, and Ainslie et al. (2023) modified the attention\noperations performed to be less computationally in-\ntensive.\nIn this work, we tackle the heavy cross-attention\ncomputation in the decoder by introducing Token\nFiltering, a method that removes redundant tokens\nfrom input passages during the decoding stage, by\ndynamically computing their salience during gen-\neration. Using Token Filtering eliminates uninfor-\nmative tokens from the cross-attention matrix, and\nprevents them from being utilized during answer\ngeneration, directly contributing to the reduction\nof the overall generation time. To further boost\nefficiency and reduce latency, we combine our To-\nken Filtering approach with dynamic decoder layer\nskipping introduced by Schuster et al. (2022), re-\nferred to as CALM. By combining both approaches\nand by conducting experiments on three LFQA\ndatasets, we find that this approach presents a bet-\nter performance vs. efficiency trade-off than by\nusing the methods separately, in most cases.\nOverall, our contributions are as follows:\n• We analyze the performance vs. efficiency\ntrade-off of the FiD model, in terms of la-\ntency, FLOPs and the salience of the input\ninformation within the reader model, during\nlong-form generation.\n• We propose a novel approach for improving\nthe efficiency of FiD, with a combined ap-\nproach of Token Filtering and decoder layer\nreduction, which removes tokens and irrele-\nvant layers during the generation process of\nevery token for long-form answers.\n• We show that models utilizing our approach\ncan save up to 62.2% on the MS MARCO\ndataset, 54.9% on NQ, and 40.9% on ELI5, in\nterms of the generation time, while incurring\na drop of no more than 2% in performance.\n• Without computational restrictions, our\nmethod reaches state-of-the-art performance\nin KILT’s ELI5 task.\n2 Preliminaries\nIn a retriever-reader system, the reader, which is\ntypically a language model, receives a query along\nwith a collection of passages, where each passage\noften consists of a title and a context. Additionally,\nwe are provided with the ground truth, which can\nbe an expected answer or a gold passage that is\nmost relevant to the query. Since our main focus\nis on generative models, we employ the widely-\nused Fusion-in-Decoder (FiD) model (Izacard and\nGrave, 2021b), which is a cutting-edge encoder-\ndecoder model based on the T5 model (Raffel et al.,\n2020). The encoder module of the FiD model pro-\ncesses the input passages in parallel, with each\nlayer taking the output of the previous layer, and\nthe final output of the encoder is the output of its\nlast layer. Similarly, each layer of the decoder\nprocesses its input by receiving the output of the\npreceding layer.\nThe decoder module then cross-attends to the\nlarge number of concatenated input representations\nand assimilates the information from the different\npassages to generate an answer. At each decod-\ning step, the decoder computes the attention scores\nbased on the precomputed input tokens’ representa-\ntions which serve as the query for the multi-headed\nattention operation, concurrently taking into ac-\ncount the current decoded sequence.\n3 Efficiency Analysis\n3.1 Encoder vs. Decoder Latency\nThere are multiple parts in a retriever-reader setup\nthat have a direct effect on the end-to-end latency.\nOne of them is potentially reducing the number of\npassages provided to thereader model. Izacard and\nGrave (2021c) evaluated the performance of FiD\nwhen decreasing the amount of passages provided\nto the reader, and found that the performance of\nthe model drops as the number of input passages\ndecreases. However, when excluding the bottom\n1507\n5 25 50 75 100 125 150\n30\n40\n50\n60\n70\n80\n90\n100\n10\n20\n40\n60\n80\n100\nAnswer Length (in tokens)\n% of Decoder Runtime\nFigure 2: The percentage of the time (latency) of the de-\ncoder from the overall end-to-end latency, as a function\nof the number of generated tokens. Each color repre-\nsents different amounts of input passages to the reader.\n50% of the passages, the performance only drops\nby approximately 2%.\nNaturally, the FiD latency could be reduced if\nwe provide less input passages to the reader. How-\never, it is unclear how much time is utilized by each\nof its sub-modules. de Jong et al. (2022) included\na preliminary profiling of the execution time for\nthe reader module of FiD. They show that even\nthough the encoder is more expensive in terms of\nFLOPS computation, the decoder is more expen-\nsive in terms of actual latency.\nThus, we undertake an additional analysis, to\ncomprehend how the time (latency) is distributed\nbetween the FiD encoder and the decoder modules,\ndepending on the number of input passages and\nthe amount of generated tokens. Our findings are\nillustrated in Figure 2. We artificially modify FiD\nto generate up to a fixed number of tokens. We\nobserve that feeding a greater number of passages\nresults in higher latency values for the encoder.\nHowever, as more output tokens are being gener-\nated, the share of the decoder of the total run-time\nsignificantly increases. Particularly, in the cases\nthat the answer is long, we observe that regard-\nless of the input number of passages provided to\nthe reader, the majority of the time is spent in the\ndecoder. Intriguingly, the ratio rapidly converges\ntowards 100%, exceeding 50% after only 15 to-\nkens.\nOverall, in the specific case of long-answer tasks\nsuch as LFQA, we can conclude that the decoder\nserves as the primary source of latency and com-\nputational load during inference. This finding is\nfurther supported by similar works (de Jong et al.,\n2022; Hofstätter et al., 2022).\n3.2 Cross-Attention Scores Analysis\nAn additional bottleneck affecting the efficiency of\nFiD is the extended sequence created by concatenat-\ning input passages, which the decoder focuses on\nduring generation. Assuming the reader is supplied\nwith an excessive amount of passages, our objec-\ntive is to assess the importance of the input token\nrepresentations. Essentially, our primary research\nquestion pertains to filtering out uninformative to-\nkens that have no impact on answer generation,\nwithout compromising performance. Inspired by\nprevious works that have assessed the relevance of\ninput to decoders, we focus on the cross-attention\nscores. These scores have been recently demon-\nstrated to serve as a metric of importance for the\ninput token representations, particularly in rela-\ntion to their impact on the accuracy of the answer\n(Caciularu et al., 2021; Izacard and Grave, 2021a;\nIzacard et al., 2022).\nIn order to investigate the utility of cross-\nattention scores as a meaningful indicator, we aim\nto verify their ability to focus on the important\ninformation within the input text. To accomplish\nthis, we include the gold passage in a list of 100\nretrieved passages (given a specific question). To\nsimplify the analysis, we position the gold passage\nat rank 1, as the input matrix of the decoder’s cross-\nattention matrix does not inherently incorporate\nany notion of order.\nIn order to examine the input token scores\nthroughout the entire generation process, we cal-\nculate the average cross-attention scores for each\ndecoder layer and at every generated token index.\nWith the aim of identifying and filtering out irrele-\nvant tokens, we select the topp% of tokens with the\nhighest cross-attention scores and compute the pro-\nportion of the tokens that originate from the gold\npassage. Figure 3a demonstrates the outcomes of\nour investigation, where we selected p = 10% of\nthe input tokens. This analysis was performed on a\nset of 1000 queries taken from the development set\nof ELI5, employing the FiD-Base model.\nWe observe that the decoder’s initial layers (2nd\nand 3rd) exhibit the greatest proportion of tokens\nderived from the gold passage. This implies that\nthese layers should be employed for calculating the\ninput relevance scores. Additionally, we have no-\nticed that, in most layers, the ratio reaches its peak\naround the 20th generated token and subsequently\n1508\n0 50 100 150\n1.6\n1.8\n2\n2.2\n2.4\n2.6\n2.8\n3 1\n2\n3\n6\n12\nAnswer Length (in tokens)\n% of Gold from Chosen\n(a) The percentage of gold tokens for several chosen decoder\nlayers.\n0 50 100 150\n1\n1.5\n2\n2.5\n3 1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens\n(b) The distribution of passages over the chosen tokens, in the\n2nd layer.\nFigure 3: Cross attention score analysis when choosing p = 10% of the tokens, as a function of the generated\nanswer length. Left: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-12). Right:\nThe percentage of tokens that were chosen from each passage (1-100). The gold passage (labeled as 1) is colored\nred.\ndeclines during the generation process. This in-\ndicates that it is most advantageous to utilize the\ncross-attention scores in the early stages of genera-\ntion.\nNext, we proceed to examine the extent to which\nthe model attends to tokens from the gold passage\ncompared to other less informative tokens. The\nfindings of this analysis are presented in Figure 3b,\nwhere we illustrate the number of selected tokens\ntaken from each input passage specifically at the\nsecond layer. Our observations consistently indi-\ncate that the gold passage consistently maintains a\nnotably higher proportion of selected tokens com-\npared to any other passage, throughout the entirety\nof the generation process. Conversely, most of the\npassages exhibit ratios that are lower than what\nwould be expected from a uniform distribution.\nInterestingly, we also note that the top passages\nexhibit higher ratios compared to the bottom ones.\nIn summary, we have demonstrated that the\ncross-attention scores possess the capability to pri-\noritize the most pertinent information in the input,\nmaking them a reliable mechanism for selecting\ninformative input tokens. Moreover, we have iden-\ntified the optimal layers and ranges of generated\ntoken indices to generate these scores, ensuring the\nselection of the most appropriate input tokens. For\na comprehensive examination of the cross-attention\npatterns, we encourage readers to refer to Appendix\nA for further details.\n4 Method\nFollowing our analysis in Section 3.2, we turn to\nimplementing a method for filtering out the redun-\ndant information during the decoding stage. We\naim to find a subset of the input tokens that is the\nmost relevant for generating the correct answer. As\npointed out previously, we can utilize the cross-\nattention scores computed between the generated\ntokens and the passages as basic signal for filter-\ning out irrelevant tokens, similarly to Goyal et al.\n(2020).\nThus, we suggest a token filtering approach, us-\ning the cross-attention scores computed at a prede-\ntermined layer and generated token index during\ninference. At that point, for each input token, we\ncompute the average attention scores over all at-\ntention heads, similarly to Caciularu et al. (2021);\nIzacard and Grave (2021a). Once these scores are\ncomputed, we keep the top k%-scored input tokens,\nwhich will be the only tokens to be considered to-\nwards the next tokens’ predictions. Formally, the\ncross-attention scores per input token are defined\nas follows:\nSt,l = 1\nh\nh∑\ni=1\nAi\nt,l, (1)\nwhere t is the generated token index, l is the\nlayer index, h is the number of attention heads, and\nAi\nt,l represents the cross-attention scores at index t,\nlayer l and head i.\nWe perform a descending argsort operation on\n1509\nthe scores above, and take the top p% from the\nsorted input token indices. Hence, we denote T\nas the total number of input tokens from all the\npassages, and T′ as the amount of tokens we keep\nafter filtering, which is p% from T:\nSortedt,l = argsort(St,l)\nTopt,l = (Sortedt,l[i])T′\ni=1,\n(2)\nwhere [i] indicates accessing the vector at in-\ndex i. Finally, we keep only the tokens chosen\nin Topt,l from the cross-attention past key-values\nstates Kpast, Vpast:\nKpast = Kpast[Topt,l], Vpast = Vpast[Topt,l],\n(3)\nwhere A[B] selects the elements from A whose\nindices appear in B. These new past key-value\nstates are the only ones used for generating all\nsubsequent tokens.\nSince the filtering percentage, token index and\nlayer can effect the quality of this mechanism, as\ninspected in Section 3.2, we obtain the optimal\nvalues for them by performing a hyperparameter-\nlike search over their possible values, which is de-\nscribed in Section 5.4. We produce new past key\nand value representations for the input sequence\n(across all the decoder layers), containing only the\nselected tokens, resulting in a more compact ten-\nsor to attend to. We name the filtering mechanism\nToken Filtering, with an overview of the approach\npresented in Figure 1.\nNote that we remove the irrelevant tokens from\nthe keys and values of the encoder output sequence\nduring inference time only once, hence reducing\ntheir dynamic dimension during computation for\nall the subsequent tokens. For additional details\nabout the cross-attention scoring computation we\nrefer the reader to Appendix B.\n5 Experimental Setup\n5.1 Datasets\nOur experiments are conducted on commonly used\ndatasets for LFQA.\nELI5 (Fan et al., 2019) A dataset created from a\nReddit forum named “Explain Like I’m Five”. We\nuse the train, validation and test sets as provided\nby the KILT benchmark2(Petroni et al., 2020).\n2https://huggingface.co/datasets/kilt_tasks\nMS MARCO (Campos et al., 2016) A collection\nof crowd sourced responses to Bing queries. We\nuse the Passage Ranking track, which consists of\nhuman generated natural and complete answers.\nNaturalQuestions (NQ) (Kwiatkowski et al.,\n2019) A large-scale dataset by Google designed\nfor natural language understanding and question\nanswering research, consisting of real user queries\nfrom Google Search paired with corresponding\nWikipedia passages.\nFor all datasets, we use the validation as the test\nset and a subset of the training set for validation, as\ndone by Lee et al. (2019). We note that ELI5 is part\nof the KILT Benchmark3, and thus is additionally\nevaluated on a held-out test set. We use the gold\npassages as the answers in MS MARCO and NQ.\nFor a full specification of the dataset sizes, we refer\nto Table 5 in the Appendix.\n5.2 Baseline Readers\nWe specify the hyperparameters used for training\non the various datasets in Table 6 in the Appendix.\nFiD We base our models on the FiD generative\nreader (Izacard and Grave, 2021c), which uses pre-\ntrained T5 models (Wolf et al., 2019). We used the\nofficial implementation4 of FiD throughout all our\nexperiments.\nCALM While our Token Filtering approach pri-\nmarily focuses on eliminating redundant input to-\nkens, it does not decrease the number of decoder\nlayers responsible for processing them. To tackle\nthis concern, we also incorporate a recent effec-\ntive early exiting method for the decoder mod-\nule (Schuster et al., 2022), known as CALM. We\nthus implement CALM and compare it to our\nmethod (see Schuster et al. (2022) for more in-\nformation on the training scheme employed to train\nthe FiD model, including the confidence classifier\nstage). In addition to independently evaluating\nCALM, we combine it together with our Token Fil-\ntering approach, resulting in a combined approach\nreferred to as Combined.\n5.3 Implementation Details\nRetrieval We first create an index for retrieval\nover a Wikipedia dump 5, comprised of multiple\n3https://eval.ai/web/challenges/\nchallenge-page/689/leaderboard/1908/ROUGE-L\n4https://github.com/facebookresearch/FiD\n5https://huggingface.co/datasets/kilt_\nwikipedia\n1510\npassages. For all the evaluated datasets, we retrieve\n100 passages for each question from the index, us-\ning a combination of dense and sparse passage\nrankers. We refer the reader to Appendix C for\nmore details regarding the retrieval process.\nHardware We used 824GB NVIDIA RTX3090for\ntraining base-sized models, and 840GB A100GPUs\nfor training large-sized models. For inference and\nlatency measurements we used a single accelerator.\nInference setup Throughout our latency mea-\nsurements, we used a batch size of 1 and averaged\nthe latency over all queries. Decoding is done us-\ning beam-search with 4 beams, and similarly as (Su\net al., 2022) we limit the generated answer length\nto 300 tokens. We also control the minimal answer\nlength per dataset, which we specify in Table 4 in\nthe Appendix.\n5.4 Performance vs. Efficiency Evaluation\nProcess\nWe use KILT’s implementation of ROUGE-L and\nF1 for performance measurements6. We measure\nefficiency as end-to-end latency in seconds for\ngenerating an answer to a question. To evaluate\neach method, on each dataset, we focus on the per-\nformance vs. efficiency trade-off, particularly on\nROUGE-L vs. latency. For the evaluation of a\ngiven method (for example Token Filtering), we\nperform a hyperparameter search over multiple\ncombinations on the development set, and end up\nwith a collection of 2-dimensional points, where\nthe x-axis is the latency, and the y-axis is the per-\nformance (ROUGE-L). Each latency-performance\nmeasurement is averaged across all questions in the\ndevelopment set. Then, we take the maximum and\nminimum over the observed values of the x-axis,\nand divide the resulting range into equally-sized\nintervals. In our experiments, we use 30 distinct in-\ntervals. For each interval, we find it’s representative\npoint, by taking the point with the maximum y-axis\nvalue from all the combinations in interval. Once\nall such points are determined per interval, they\nform a curve, which we name as the Max Curve of\nthe method. We visualize the process in Figure 4,\nwhere the line in blue is the Max Curve.\nThus, for a method to be better than another, the\nMax Curve for it should be above and to the left\nof the curve of the other, meaning that it reaches\nequivalent results for less resources. Using the\n6https://github.com/facebookresearch/KILT\ny 1 \ny 2 y 3 y 4 \ny 5 \nx 1 x 2 x 3 x 4 x 5 x 6 \nPerformance\nSmoothing\nLatency\nFigure 4: For Performance (y-axis) vs. Latency (x-axis),\nwe divide the x-axis into 5 intervals over all hyperpa-\nrameter combination results on the development set, rep-\nresented as yellow dots. For each interval, we choose\nthe combination with the best performance (i.e. y-axis\nvalue), thus forming the Max Curve in blue, with its\nsmoothed version in green.\ncurve, we find the best hyperparameters for each\nmethod per interval, by selecting the hyperparame-\nters of the representative point in the current inter-\nval. Finally, we take the best setting per interval,\nand run each one on the test set. For each of our\nmethods, we produce a smoothed version, as the\nresults are not necessarily monotonically increas-\ning, which is shown in Figure 4 as the green line.\nThese smoothed curves are the ones showcased in\nthe final results in Figure 5.\nWhen performing the search over the hyperpa-\nrameter space, we used grid search, with the hyper-\nparameters and their value ranges being specified\nin Table 7 in the Appendix. Other methods (such\nas random search, Bayesian Optimization (Snoek\net al., 2012), etc.) may be attempted just as well in\nfuture works.\n6 Experimental Results\n6.1 Main Trade-off Comparison\nIn Figure 5, we showcase the performance vs. effi-\nciency trade-off in terms of ROUGE-L and latency\non the test set of each dataset. These results are\nthe ones obtained after performing the hyperparam-\neter optimization procedure stated described Sec-\ntion 5.4. The methods shown are the standard FiD\nmodel, CALM, Token Filtering, and Combined.\nFor the base-sized models (Figures 5a, 5b, 5c),\nwe can observe all methods improve upon the base-\nline model, each one in a different aspect. For\nCALM, the method is able to reach lower latency\n1511\n2 2.5 3 3.5 4 4.5 5 5.5\n23.5\n24\n24.5\n25\n25.5\n26\n26.5\n27\nLatency (seconds)\nRouge-L\n2%\n(a) ELI5 - Base\n0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2\n20\n21\n22\n23\n24\n25\nLatency (seconds)\n2% (b) MS MARCO- Base\n1.5 2 2.5 3 3.5 4 4.5\n24\n25\n26\n27\n28\n29\n30\n31\n32\nLatency (seconds)\n2% (c) NQ - Base\n4 6 8 10\n24\n24.5\n25\n25.5\n26\n26.5\nLatency (seconds)\nRouge-L\n2%\n(d) ELI5 - Large\n2 3 4 5\n21\n21.5\n22\n22.5\n23\n23.5\n24\n24.5\n25\nLatency (seconds)\n2% (e) MS MARCO- Large\n4 6 8 10 12\n26\n27\n28\n29\n30\n31\n32\nFiD\nCALM\nToken Filtering\nCombined\nLatency (seconds)\n2% (f) NQ - Large\nFigure 5: ROUGE-L Performance results of the different methods on the test sets, plotted as smoothed Max Curves,\nas a function of latency (seconds), for Base (top) and Large (bottom) models. Overall, our combined approach is\nable to reach a better trade-off than the regular FiD model, for most cases.\nvalues, due to skipping the redundant layer compu-\ntations. In the case of Token Filtering, it is also able\nto preserve and at times improve the performance\nof the model overall, while the latency improve-\nment remains limited, since it is still computing the\nremaining tokens across all decoder layers. The\nperformance improvement is presumably due to the\nredundant tokens being removed early on during\nthe generation process, hence allowing the model\nto better attend to the salient information in the\ninput.\nWhen combining both methods, the performance\nenhancement of the Token Filtering and the latency\nreduction of CALM produce a better curve than\neither method alone. In addition, we showcase\nthe drop in 2% performance per dataset, show-\ning that our method is able to reduce the latency\nsignificantly more than the regular FiD, with the\nbest reduction reached on the MS MARCO dataset\nfor FiD-Base, saving 62.2% of the latency. In the\nNQ dataset however, for both the base-sized and\nlarge-sized models, while the CALM method does\nachieve proper latency reduction, the Token Filter-\ning does not effect the results significantly. Since\nwe focus on real-world scenarios, we showcase\nthe trade-off with the actual latency, instead of\nmeasurements such as FLOPS (MACs), as done by\nprevious works (de Jong et al., 2022). For those, we\nrefer to Figure 6 in the Appendix for the trade-off\nand FLOPS (MACs) analysis. For the large-sized\nmodels (Figures 5d, 5e, and 5f), we observe similar\npatterns to those in the base-sized variations, with\nthe latency values being significantly larger. We\nnote that the overall performance values for these\nmodels are not substantially different than those\nproduced by the smaller versions, hence we do not\nfocus as much on them.\n6.2 Performance Comparison\nTo asses the performance of our Combined method\nfurther, we choose the best performing hyperpa-\nrameter setting for the FiD-Base model, and report\nthe test set results for each dataset, with Table 1\nshowing the results, compared to approaches sug-\ngested in Su et al. (2022). In particular, we com-\npare to their implementation of FiD (named RBG\nFID) and their suggested system (named RBG),\nwith the results of both taken from their published\nwork. We note that both our models and the RBG\nmodels are of the same size. In our experiments,\nwe denote the original FiD model we trained as\nFiD (ours), the FiD model with the Token Filter-\ning method as FiD TF, and the Combined method\nas FiD Comb. On both datasets, FiD (ours), FiD\nTF and FiD Comb achieve state-of-the-art results,\nwith Combined reaching the best overall perfor-\nmance in terms of ROUGE-L and F1 (aside from\nMS MARCO F1, where our approach is second).\n1512\nELI5 MS MARCO\nR-L F1 R-L F1\nRBG FiD 25.70 28.55 24.64 27.08\nRBG 26.46 29.04 24.72 27.52\nFiD (ours) 26.24 31.46 24.33 27.11\nFiD TF 26.65 30.32 24.75 27.26\nFiD Comb 26.97 31.76 25.11 27.41\nTable 1: A comparison of the performance of our model,\nin comparison with the RBG model, where FiD TF\nstands for FiD with Token Filtering. Highest perfor-\nmance values are marked inbold, where R-L is ROUGE-\nL. We note that the results for RBG’s models were taken\ndirectly from their published paper.\nModel ELI5 MS MARCO\nFiD (Ours) 83.68 85.07\nFiD Comb 83.79 85.27\nTable 2: The BERTScore F1 results on the test set for\nboth the original FiD model and our FiD Comb method,\nwith each column indicating a different dataset.\nAs an additional point of comparison, we compute\nthe BERTScore (Zhang et al., 2019) metric on the\noriginal FiD model, and our FiD Comb method,\nand present our findings in Table 2. We observe\nthat our model performs on par and even a bit better\noverall than the original FiD implementation. We\npresent a supplementary comparison of some of\nthe answers generated by our method for the ELI5\ntest set in Table 8 in the Appendix. In addition,\nat the time of writing this paper, our Combined is\nranked at the #1 position in terms of ROUGE-L\nand F1 on the ELI5 KILT leaderboard, with Table\n3 containing all leaderboard results7.\n7 Related Work\nOpen-Domain Question Answering Many pre-\nvious works utilized setups which are based on the\nretriever and the language model reader compo-\nnents (Chen et al., 2017; Guu et al., 2020; Lewis\net al., 2020). The goal of the retriever is to fetch\nthe most relevant passages to a given question\n(Karpukhin et al., 2020). The reader processes\nthe question and the relevant passages to extract or\ngenerate the answer, with generative approaches\n7Since the results in Table 3 are on a hidden test set for\nthe leaderboard, the results are different from those reported\nin Table 1.\nModel ROUGE-L F1\nKrishna et al. (2021a) 23.36 23.14\nRBG 24.53 27.13\nFiD TF 25.52 28.49\nFiD Comb 25.61 29.99\nTable 3: The published results on the official ELI5 test\nset, provided by the KILT leaderboard. FiD Comb runs\nwith the same setting as best performing combination\nof the combined approach in Figure 5a.\nachieve substantially better results (Izacard and\nGrave, 2021b). Subsequent works (Su et al., 2022;\nKrishna et al., 2021b) have adapted these genera-\ntive approaches to produce long-form answers as\nwell.\nEncoder-Decoder Efficiency Due to computa-\ntion bottlenecks in generative models, particularly\nin encoder-decoder models (Vaswani et al., 2017;\nRaffel et al., 2020), previous works attempt to mit-\nigate them. The encoder model can be used to\nfilter out irrelevant passages during generation (Yu\net al., 2021; de Jong et al., 2023). Nevertheless, the\nencoder’s impact on the model’s latency in long-\nform scenarios is negligible, as depicted in Figure\n2. Consequently, our research centers on analyzing\nthe computational aspects of the decoder instead.\nIn particular, the decoder model utilizes many\nredundant and heavy cross-attention operations,\nwhich can be removed or replaced with simpler\nalternatives (de Jong et al., 2022; Ainslie et al.,\n2023).\nSince encoder-decoder models perform compute\nheavy operations in multiple layers, previous works\nhave proposed stopping the layer propagation dy-\nnamically by assessing the model’s confidence for\nprediction at a certain layer (Teerapittayanon et al.,\n2016; Schwartz et al., 2020). Other works have\nadapted this mechanism to decoder models as well\n(Schuster et al., 2022; Elbayad et al., 2019). How-\never, these studies fail to tackle the issue of input\nsize during generation, thereby resulting in compu-\ntations being performed on irrelevant input to some\ndegree, which we address through complementary\ntoken filtering.\nData Reduction for an Efficient Computation\nThe input to encoder models tends to become in-\ncreasingly large, especially in ODQA settings with\nmany input passages. Since not all spans of infor-\n1513\nmation are relevant to produce the correct answer,\nprevious works propose eliminating the irrelevant\ntokens from the input to the encoder, by identify-\ning the salient information during inference time\n(Goyal et al., 2020; Kim and Cho, 2020). Although\nthese techniques effectively decrease computation\nat each layer, they are implemented in the encoder\nmodel rather than the decoder, which has been pre-\nviously determined to have a more significant in-\nfluence on latency. Our work leverages the cross-\nattention in the decoder early on during generation,\nthus effectively filtering the input tokens.\nQin and Durme (2023) suggested transform-\ning language into a representation, by selecting\na dynamically determined subset of input tokens,\nwith these “nuggets” being acquired through tasks\nsuch as machine translation. However, our method\ndoesn’t incorporate any learning, focusing on ana-\nlyzing the necessary input tokens for direct decod-\ning instead.\nWingate et al. (2022); Mu et al. (2023) proposed\nprompt compression techniques for minimizing the\namount of token vectors required to represent the\nsame text. We note that our work does not discuss\nsuch aspects, given the context of questions and\npassages in our inputs.\n8 Conclusions\nWe analyze the precise performance vs. efficiency\ntrade-off of the FiD’s encoder and decoder in\nlong-form settings, with an analysis of the cross-\nattention operation in the decoder model. We show\nthat the decoder has more impact on the latency,\nparticularly for long outputs, and that the decoder\nattends to more salient information early on during\ngeneration. Hence, our proposed approach for ef-\nficiency reduction, namely a combined approach\nof Token Filtering and CALM, removes irrelevant\nlayers and tokens during the generation process,\nfor every token produced. Our approach achieves\na significant reduction in resources (up to 62.2%),\nwhile not sacrificing more than 2% of the perfor-\nmance, and is the current state-of-the-art on the\nELI5 KILT leaderboard. Future work can further\ndevelop a more dynamic method for choosing the\nmost relevant tokens from the input, instead of\nusing predetermined hyperparameters, and train\nthe cross-attention patterns to better attend to the\nsalient information during generation.\nLimitations\nRegarding the retriever, as mentioned in Section\n5.2, we did not experiment with a vast array of\nretrievers, due to the scope of the work being on\nthe reader model.\nRegarding the models for comparison, we pri-\nmarily focused on the performance of the FiD\nmodel versus our own approach, while testing them\non various datasets. Hence, we did not perform ex-\ntensive reproductions of other methods, such other\nencoder-decoder models, but instead report their\noriginal results as they were published. We be-\nlieve that our results can be generalized to other\narchitectures as well.\nIn our hyperparameter search, we chose a sub-\nspace of all the possible values each parameter has,\ndue to a limited amount of computation available.\nOur approximation of the space covers the main\nareas of relevance for our purposes.\nReferences\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago\nOntan’on, Siddhartha Brahma, Yury Zemlyanskiy,\nDavid C. Uthus, Mandy Guo, James Lee-Thorp,\nYi Tay, Yun-Hsuan Sung, and Sumit K. Sanghai.\n2023. Colt5: Faster long-range transformers with\nconditional computation. ArXiv, abs/2303.09752.\nAvi Caciularu, Arman Cohan, Iz Beltagy, Matthew Pe-\nters, Arie Cattan, and Ido Dagan. 2021. CDLM:\nCross-document language modeling. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 2648–2662, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nDaniel Fernando Campos, Tri Nguyen, Mir Rosenberg,\nXia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, Li Deng, and Bhaskar Mitra. 2016. Ms\nmarco: A human generated machine reading compre-\nhension dataset. ArXiv, abs/1611.09268.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In ACL.\nMichiel de Jong, Yury Zemlyanskiy, Joshua Ainslie,\nNicholas FitzGerald, Sumit K. Sanghai, Fei Sha, and\nWilliam Cohen. 2022. Fido: Fusion-in-decoder opti-\nmized for stronger performance and faster inference.\nArXiv, abs/2212.08153.\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\nald, Sumit Sanghai, William W. Cohen, and Joshua\nAinslie. 2023. Glimmer: generalized late-interaction\nmemory reranker.\n1514\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. 2019. Depth-adaptive transformer. ArXiv,\nabs/1910.10073.\nAngela Fan, Yacine Jernite, Ethan Perez, David\nGrangier, Jason Weston, and Michael Auli. 2019.\nEli5: Long form question answering. ArXiv,\nabs/1907.09190.\nSaurabh Goyal, Anamitra R. Choudhury, Saurabh Raje,\nVenkatesan T. Chakaravarthy, Yogish Sabharwal, and\nAshish Verma. 2020. Power-bert: Accelerating bert\ninference via progressive word-vector elimination. In\nInternational Conference on Machine Learning.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. ArXiv,\nabs/2002.08909.\nSebastian Hofstätter, Jiecao Chen, Karthik Raman, and\nHamed Zamani. 2022. Fid-light: Efficient and ef-\nfective retrieval-augmented text generation. ArXiv,\nabs/2209.14290.\nGautier Izacard and Edouard Grave. 2021a. Distilling\nknowledge from reader to retriever for question an-\nswering. ArXiv, abs/2012.04584.\nGautier Izacard and Edouard Grave. 2021b. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nGautier Izacard and Edouard Grave. 2021c. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In EACL.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane A. Yu,\nArmand Joulin, Sebastian Riedel, and Edouard Grave.\n2022. Few-shot learning with retrieval augmented\nlanguage models. ArXiv, abs/2208.03299.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Yu Wu, Sergey Edunov, Danqi\nChen, and Wen tau Yih. 2020. Dense passage re-\ntrieval for open-domain question answering. ArXiv,\nabs/2004.04906.\nGyuwan Kim and Kyunghyun Cho. 2020. Length-\nadaptive transformer: Train once with length drop,\nuse anytime with search. In Annual Meeting of the\nAssociation for Computational Linguistics.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021a.\nHurdles to progress in long-form question answering.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4940–4957, Online. Association for Computa-\ntional Linguistics.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021b.\nHurdles to progress in long-form question answering.\nIn North American Chapter of the Association for\nComputational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc V . Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:453–466.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. ArXiv, abs/1906.00300.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. ArXiv, abs/2005.11401.\nJesse Mu, Xiang Lisa Li, and Noah D. Goodman.\n2023. Learning to compress prompts with gist to-\nkens. ArXiv, abs/2304.08467.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vassilis Plachouras, Tim Rocktaschel,\nand Sebastian Riedel. 2020. Kilt: a benchmark for\nknowledge intensive language tasks. In North Amer-\nican Chapter of the Association for Computational\nLinguistics.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nGuanghui Qin and Benjamin Van Durme. 2023. Nugget:\nNeural agglomerative embeddings of text. In Inter-\nnational Conference on Machine Learning.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. ArXiv, abs/1910.10683.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nArXiv, abs/1908.10084.\nDevendra Singh Sachan, Mostofa Patwary, Mohammad\nShoeybi, Neel Kant, Wei Ping, William L. Hamilton,\nand Bryan Catanzaro. 2021. End-to-end training of\n1515\nneural retrievers for open-domain question answering.\nIn Annual Meeting of the Association for Computa-\ntional Linguistics.\nTal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani,\nDara Bahri, Vinh Quang Tran, Yi Tay, and Donald\nMetzler. 2022. Confident adaptive language model-\ning. ArXiv, abs/2207.07061.\nRoy Schwartz, Gabriel Stanovsky, Swabha\nSwayamdipta, Jesse Dodge, and Noah A. Smith.\n2020. The right tool for the job: Matching model\nand instance complexities. In Annual Meeting of the\nAssociation for Computational Linguistics.\nJasper Snoek, H. Larochelle, and Ryan P. Adams. 2012.\nPractical bayesian optimization of machine learning\nalgorithms. ArXiv, abs/1206.2944.\nDan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin\nJiang, Qun Liu, and Pascale Fung. 2022. Read before\ngenerate! faithful long form question answering with\nmachine reading. ArXiv, abs/2203.00343.\nSurat Teerapittayanon, Bradley McDanel, and H. T.\nKung. 2016. Branchynet: Fast inference via early\nexiting from deep neural networks. 2016 23rd Inter-\nnational Conference on Pattern Recognition (ICPR),\npages 2464–2469.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nEllen M. V oorhees. 1999. The trec-8 question answer-\ning track report. In Text Retrieval Conference.\nDavid Wingate, Mohammad Shoeybi, and Taylor\nSorensen. 2022. Prompt compression and contrastive\nconditioning for controllability and toxicity reduc-\ntion in language models. In Conference on Empirical\nMethods in Natural Language Processing.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nDonghan Yu, Chenguang Zhu, Yuwei Fang, W. Yu,\nShuohang Wang, Yichong Xu, Xiang Ren, Yim-\ning Yang, and Michael Zeng. 2021. Kg-fid: Infus-\ning knowledge graph in fusion-in-decoder for open-\ndomain question answering. ArXiv, abs/2110.04330.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2019. Bertscore:\nEvaluating text generation with bert. ArXiv,\nabs/1904.09675.\nA Cross Attention Pattern Analysis\nIn this section, we continue our discussion from\nsection 3.2, regarding the analysis of the cross-\nattention scores.\nIn Figure 7, we present multiple versions of\nthe plot in Figure 3a, with the rows indicating the\ndataset (ELI5, MS MARCO, NQ), and the columns\nrepresenting a different percent of chosen tokens\n(10, 30, 50). For MS MARCO and NQ in 10%,\nthe percentage of the gold passage tokens remains\nhigh for the lower layers, starting from token 10.\nThe other layers do not reach the same percentage\nand degrade during the generation process. When\nincreasing the percentage to 30, and later 50, the\npercentage of the gold passage is getting reduced\nsubstantially.\nIn Figure 8, we showcase an extended version\nof Figure 3b, for the various datasets and chosen\ntoken percentages as in Figure 7, with the rows\nand columns being similarly organized. For 10%,\nthe gold passage gets the most tokens out of all\nthe rest, for all datasets, with the lower passages\ngetting less than 1%. However, for 30%, the gold\npassage is no longer the highest ranking for some\nof the datasets (MS MARCO, NQ), with the upper\npassages reaching higher, and the lower ones still\nbeing at the 1% mark. At 50%, the gold passage is\nno longer the most prominent, with it being nearly\nas insignificant as the lower passages in the case\nof MSMARCO. This suggests that increasing the\npercentage of tokens taken introduces unnecessary\nnoise to the selected tokens, thus forcing the model\nto receive input from lower ranked passages. For\nMS MARCO and NQ in 10%, the percentage of\nthe gold passage tokens remains high for the lower\nlayers, starting from token 10. While the results\nabove were done using an FiD-Base model, similar\npatterns are present for FiD-Large models through\nall previously discussed aspects.\nB Attention Score Computation\nExtensions\nIn addition to the methods introduced in 4, the\ncomputation of the cross-attention scores can be\nfurther altered in a few key areas, which we tackle\nas well.\nValue Normalization. As mentioned in Izacard\net al. (2022), the scores can benefit from scaling by\nthe l2 normalized values tensor V . Thus, we can\ninstead transform Ai\nt,l into:\n1516\nAi\nt,l[n] = Ai\nt,l[n]vn (4)\n,\nwhere [n] is the nth row, in this case the nth to-\nken, and vn = ||V [n]||2 is the norm of the nth row\n(token) in V . Hence, we apply this normalization\nto the attention scoring operation.\nMean over all decoder layers. Instead of taking\nthe representation of the current decoder layer only,\nwe instead take the average over every layer before\nthe current one. Thus, we compute the attention\nscores St,l for the input tokens as follows:\nSt,l = 1\nlh\n∑\nl′∈[1,l]\n∑\ni∈[1,h]\nAi\nt,l′ (5)\nFrom our preliminary analysis, this mean op-\neration does not effect the quality of the filtering\nmethod, and hence is not applied.\nC Retrieval Details\nSince our method primarily focuses on the reader\nmodel, we have implemented a generalized ap-\nproach for creating ranked passage lists. Our doc-\nument corpus is taken from a Wikipedia Dump,\nwhich has been split into 100-word-long passages,\nas done in Karpukhin et al. (2020), including the\narticle title. These documents are then stored in\nan Elasticsearch8 index. Given a question from a\ndataset, we use BM25 over the passage index, to\nretrieve 250 documents. Then, we re-rank the pas-\nsages using a sentence transformer9(Reimers and\nGurevych, 2019) model that was finetuned on the\ndataset, and keep only the top 100 ranked docu-\nments.\nD Method Implementation Details\nFor the CALM, we utilize beam search for long\nsequence generation. In the beam-search setting,\nwe use nb beams, there which causes the issue of\nhow to allow some tokens to cease computation at a\ncertain level, while allowing the others to continue\ncomputation. For the scope of our work, we apply\nthe hard assumption that the confidence value is\nthe lowest one from all beams, hence exiting only\nif all the tokens in the beams have satisfied the ex-\niting condition. Formally, given confidence scores\n¯cl = (c1\nl , c2\nl , ..., cnb\nl ) at layer l, the confidence value\nused at the layer will thus be cl = minj∈[1,nb] cj\nl .\n8https://www.elastic.co\n9multi-qa-mpnet-base-dot-v1\nWe note that while Schuster et al. (2022) utilized a\ncomplex threshold calibration system, we instead\nshowcase the effect of the various thresholding set-\ntings, once applied to the decoder model.\nFor the Token Filtering, since we are discussing\nmainly Encoder-Decoder architectures, we apply\nthe filtering by removing the redundant tokens from\nthe past key and value states for the cross-attention.\nIn addition, we also remove said tokens from the\nencoder hidden states, encoder attention mask, and\nthe encoder-decoder position bias.\nDataset Len. Chosen\nELI5 150\nMS MARCO 50\nNQ 50\nTable 4: The chosen minimum answer length for during\nevaluation.\nDataset Train Dev Test KILT\nELI5 272634 3000 1507 600\nMS MARCO 498000 3000 6980 -\nNQ 55622 3000 6489 -\nTable 5: Sizes of the datasets, per train, dev and test\nrespectively. We include the size of the KILT test set\nsize, which we evaluate on separately.\nParameter Value\nBase Model T5-Base, T5 Large\nOptimizer AdamW\nMax Seq. Length 235\nLR 5e-5\nLR Scheduler Linear\nWeight Decay 0.01\nPrecision torch.bfloat16\nBatch Size 64\nTraining Steps 60000\nWarmup Steps 1000\nTable 6: The training parameters used for training the\nFiD model on each dataset.\n1517\nParameter Ranges\nGeneral Parameters\nInput Psg. [5, 100]\nCALM\nConfidence Thresh-\nold\n[0.2, 0.9]\nThreshold Coef. {0.5, 0.7, 0.9}\nThreshold Decay {3, 4, 5}\nToken Filtering\n% of Input {10, 30, 50}\nFiltering Token [1,20]\nFiltering Layer [1, L]\nTable 7: The hyperparameter search space for the\nCALM and Token Filtering methods, including param-\neters for all methods (Input Psg.), and the combined\napproach. {} indicate a set of possible values, while\n[min, max] correspond to a range of values from min\nto max. As in the paper, L is the number of layers in\nthe decoder.\n1518\n0 1000 2000 3000 4000\n24.5\n25\n25.5\n26\n26.5\n27\nFLOPS (MACs)\nRouge-L\n2%\n(a) ELI5\n0 1000 2000 3000 4000\n20\n21\n22\n23\n24\n25\nFLOPS (MACs)\n2% (b) MS MARCO\n0 1000 2000 3000 4000\n24\n25\n26\n27\n28\n29\n30\n31\n32\nFLOPS (MACs)\n2% (c) NQ\n0 20 40 60 80 100\n24.5\n25\n25.5\n26\n26.5\n27\nInput Psgs\nRouge-L\n2%\n(d) ELI5\n0 20 40 60 80 100\n20\n21\n22\n23\n24\n25\nInput Psgs\n2% (e) MS MARCO\n0 20 40 60 80 100\n24\n25\n26\n27\n28\n29\n30\n31\n32\nFiD\nCALM\nToken Filtering\nCombined\nInput Psgs\n2% (f) NQ\nFigure 6: The ROUGE-L performance on FiD-Base vs. the FLOPS (MACs) (First row), and vs. the input passage\namount (Input Psg., second row). The results are on the test set for each dataset, for the various methods utilized.\nWe observe that the trends of the FLOPS (MACs) and the Input Psg. are very similar, since the passages effect the\nencoder the most, and the encoder has the most impact on FLOPS (MACs) (as stated by de Jong et al. (2022)).\n1519\nQuestion Answers\nHow do you know something is\ndone/said by Anonymous, when they\nare, by definition, anonymous? Can\nanyone do stuff/say things on behalf of\nAbobynous, or do they somehow have\na recognized leadership who are not\nanonymous to each other/their mem-\nbers?\nGold: Any unknown person can do or say things in the name of Anonymous. That’s\nwhy you basically have to ignore everything Anonymous says they will do, and only\nlook at what they actually do. There’s no leadership, but there are I believe certain\ncore groups who know one another by persistent pseudonyms, and may possibly\nknow one another afk. They are probably disproportionately influential, in that they\nrun widely-subscribed youtube/twitter/whatever accounts which play a significant\nrole in getting messages out to the rest of Anonymous, but they aren’t leadership in\nany classical sense of the word.\nFiD (ours): I’m not sure what you mean by \"anonymous\". Anonymous is a group\nof people who are working together to solve problems. They aren’t anonymous to\neach other, they’re just like any other group. There’s no way to know if something\nis done by Anonymous or not, but you can be sure that it is. You can also be certain\nthat someone is doing something that is not Anonymous, and that they are doing it\nfor the benefit of others. If you’ve ever heard of someone saying something like \"I\nam an Anonymous member, I am not an anonymous member\" then you know that\nyou are an \"Anon\" member. Then you have to be able to prove that the person who\nmade the post was Anonymous.\nFiD Comb: I’m not sure what you mean by \"anonymous\" by definition. Anonymous\nis a loose hacktivist collective. They don’t have an official leadership, but they do\nhave some sort of \"leadership\" in place. _URL_0_ It’s not that they’re able to do\nanything, they just use the name \"Anon\" to refer to the members of the group, and\nthey can be viewed as \"a group of people who believe in the idea of anonymity\" or\nsomething like that. There is no way to tell if someone is an anonymous group. It is\npossible for someone to be identified as an Anonymous member, or even have their\nown leader.\nwhy is cranberry juice good for your\nkidneys?\nGold: It isn’t, particularly. Some people use to think that the acid in cranberries\nstopped bacteria growing but that´s been shown to be wrong. There is some vague\nevidence that cranberry juice can help *prevent* UTIs, though. It ´s thought that\ncranberries might contain a chemical that stops bacteria sticking to the cells lining\nthe walls of the urinary tract, but no-one knows for sure. Once you have a UTI,\nthough, no juice will cure it. The real reason that cranberry juice, lemon barley\nwater etc are good for your kidneys is that you need to flush them out. Drink at least\n8 glasses of water a day (2-3 litres of fluid daily), THAT is what is good for your\nkidneys. My daughter gets a LOT of UTIs, even with prophylactic antibiotics, and\nseveral members of my family have died from kidney disease. All I ever hear from\nwell-intentioned friends is \"have you tried cranberry and/or barley water?\". Sigh.\nFiD (ours): It isn’t. There is no evidence that cranberry juice is good for your\nkidneys. It’s just that there is not enough evidence to suggest that it is a good thing\nfor you. _URL_0_ TL;DR - if you have an infection, you should not drink it. If\nyou’re on dialysis, it can lead to urinary tract infections, which can be fatal. The\nreason for this is because the bacteria that cause the infection can get stuck in your\nurine, causing the urine to become inflamed and irritated, leading to pyuria and\nkidney stones. This is why people who are in the throes of kidney problems should\ndrink more than they normally do.\nFiD Comb: It isn’t. There’s no scientific evidence that cranberry juice is good\nfor your kidneys. However, there are studies that suggest that it may be beneficial\nfor you if you have a urinary tract infection. _URL_0_ TL;DR: Cranberries are\nincredibly acidic, so they can be sour and tart, which is why they’re bad for the\nkidney. It also has some anti-coagulants in it that can help prevent the formation of\nbacterial plaques in the urine, reducing the amount of urine that is excreted from\nthe bloodstream, and preventing the clotting of the bladder. This is also why some\npeople who have kidney stones are more likely to get kidney stone formation.\nTable 8: ELI5 test set answers from the standard FiD model, FiD (ours), and our method (FiD Comb), with the Gold\nAnswer as a reference.\n1520\n0 50 100 150\n1.6\n1.8\n2\n2.2\n2.4\n2.6\n2.8\n3 1\n2\n3\n6\n12\nAnswer Length (in tokens)\n% of Gold from Chosen\n(a) ELI5 - p = 10%\n0 50 100 150\n1.4\n1.45\n1.5\n1.55\n1.6\n1.65 1\n2\n3\n6\n12\nAnswer Length (in tokens)\n% of Gold from Chosen (b) ELI5 - p = 30%\n0 50 100 150\n1.04\n1.06\n1.08\n1.1\n1.12\n1.14 1\n2\n3\n6\n12\nAnswer Length (in tokens)\n% of Gold from Chosen (c) ELI5 - p = 50%\n0 10 20 30 40 50\n1.6\n1.8\n2\n2.2\n2.4\n2.6\n2.8\n3\n3.2\n3.4 1\n2\n3\n6\n12\nAnswer Length (in tokens)\n% of Gold from Chosen\n(d) MS MARCO - p = 10%\n0 10 20 30 40 50\n1.15\n1.2\n1.25\n1.3\n1.35\n1.4 1\n2\n3\n6\n12\nAnswer Length (in tokens)\n% of Gold from Chosen (e) MS MARCO - p = 30%\n0 10 20 30 40 50\n0.83\n0.84\n0.85\n0.86\n0.87\n0.88\n0.89\n0.9\n0.91 1\n2\n3\n6\n12\nAnswer Length (in tokens)\n% of Gold from Chosen (f) MS MARCO - p = 50%\n0 20 40 60 80 100\n1.5\n2\n2.5\n3\n3.5\n1\n2\n3\n6\n12\nAnswer Length (in tokens)\n% of Gold from Chosen\n(g) NQ - p = 10%\n0 20 40 60 80 100\n1.5\n1.6\n1.7\n1.8\n1.9\n2 1\n2\n3\n6\n12\nAnswer Length (in tokens)\n% of Gold from Chosen\nLoading [MathJax]/extensions/MathMenu.js (h) NQ - p = 30%\n0 20 40 60 80 100\n1.22\n1.24\n1.26\n1.28\n1.3\n1.32\n1.34\n1.36 1\n2\n3\n6\n12\nAnswer Length (in tokens)\n% of Gold from Chosen (i) NQ - p = 50%\nFigure 7: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-12), for FiD-Base models.\nEach row represents a different dataset, and every column represents a different filtering percentage (10,30,50).\n1521\n0 50 100 150\n1\n1.5\n2\n2.5\n3 1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens\n(a) ELI5 - p = 10%\n0 50 100 150\n0.8\n0.9\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7 1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens (b) ELI5 - p = 30%\n0 50 100 150\n0.85\n0.9\n0.95\n1\n1.05\n1.1\n1.15\n1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens (c) ELI5 - p = 50%\n0 10 20 30 40 50\n0.5\n1\n1.5\n2\n2.5\n3\n3.5 1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens\n(d) MS MARCO - p = 10%\n0 10 20 30 40 50\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8 1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens (e) MS MARCO - p = 30%\n0 10 20 30 40 50\n0.8\n0.9\n1\n1.1\n1.2\n1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens (f) MS MARCO - p = 50%\n0 20 40 60 80 100\n0.5\n1\n1.5\n2\n2.5\n3\n3.5 1\n2\n10\n20\n50\n80\nAnswer Length (in tokens)\n% from Chosen Tokens\n(g) NQ - p = 10%\n0 20 40 60 80 100\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2 1\n2\n10\n20\n50\n80\nAnswer Length (in tokens)\n% from Chosen Tokens (h) NQ - p = 30%\n0 20 40 60 80 100\n0.9\n1\n1.1\n1.2\n1.3\n1.4 1\n2\n10\n20\n50\n80\nAnswer Length (in tokens)\n% from Chosen Tokens (i) NQ - p = 50%\nFigure 8: The percentage of tokens that were chosen from each passage, for FiD-Base models. The gold passage\n(labeled as 1) is colored red. Each row represents a different dataset, and every column represents a different filtering\npercentage (10,30,50).\n1522\n0 50 100 150\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n2.6 1\n2\n3\n6\n12\n18\n24\nAnswer Length (in tokens)\n% of Gold from Chosen\n(a) ELI5 - p = 10%\n0 50 100 150\n1.35\n1.4\n1.45\n1.5\n1.55\n1\n2\n3\n6\n12\n18\n24\nAnswer Length (in tokens)\n% of Gold from Chosen (b) ELI5 - p = 30%\n0 50 100 150\n1.04\n1.06\n1.08\n1.1\n1.12\n1.14 1\n2\n3\n6\n12\n18\n24\nAnswer Length (in tokens)\n% of Gold from Chosen (c) ELI5 - p = 50%\n0 10 20 30 40 50\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n2.6\n2.8\n3\n3.2 1\n2\n3\n6\n12\n18\n24\nAnswer Length (in tokens)\n% of Gold from Chosen\n(d) MS MARCO - p = 10%\n0 10 20 30 40 50\n1.1\n1.15\n1.2\n1.25\n1.3\n1.35 1\n2\n3\n6\n12\n18\n24\nAnswer Length (in tokens)\n% of Gold from Chosen (e) MS MARCO - p = 30%\n0 10 20 30 40 50\n0.84\n0.86\n0.88\n0.9\n0.92 1\n2\n3\n6\n12\n18\n24\nAnswer Length (in tokens)\n% of Gold from Chosen (f) MS MARCO - p = 50%\n0 20 40 60 80 100 120\n1.5\n2\n2.5\n3\n3.5\n4\n1\n2\n3\n6\n12\n18\n24\nAnswer Length (in tokens)\n% of Gold from Chosen\n(g) NQ - p = 10%\n0 20 40 60 80 100 120\n1.5\n1.6\n1.7\n1.8\n1.9\n2\n2.1 1\n2\n3\n6\n12\n18\n24\nAnswer Length (in tokens)\n% of Gold from Chosen\nLoading [MathJax]/extensions/MathMenu.js (h) NQ - p = 30%\n0 20 40 60 80 100 120\n1.25\n1.3\n1.35\n1.4\n1\n2\n3\n6\n12\n18\n24\nAnswer Length (in tokens)\n% of Gold from Chosen (i) NQ - p = 50%\nFigure 9: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-24), for FiD-Large models.\nEach row represents a different dataset, and every column represents a different filtering percentage (10,30,50).\n1523\n0 50 100 150\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2 1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens\n(a) ELI5 - p = 10%\n0 50 100 150\n0.9\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6 1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens (b) ELI5 - p = 30%\n0 50 100 150\n0.9\n0.95\n1\n1.05\n1.1\n1.15 1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens (c) ELI5 - p = 50%\n0 10 20 30 40 50\n0.5\n1\n1.5\n2\n2.5\n3 1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens\n(d) MS MARCO - p = 10%\n0 10 20 30 40 50\n0.8\n1\n1.2\n1.4\n1.6 1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens (e) MS MARCO - p = 30%\n0 10 20 30 40 50\n0.8\n0.85\n0.9\n0.95\n1\n1.05\n1.1\n1.15\n1.2\n1.25 1\n2\n10\n20\n50\n80\n100\nAnswer Length (in tokens)\n% from Chosen Tokens (f) MS MARCO - p = 50%\n0 20 40 60 80 100 120\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5 1\n2\n10\n20\n50\n80\nAnswer Length (in tokens)\n% from Chosen Tokens\n(g) NQ - p = 10%\n0 20 40 60 80 100 120\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2 1\n2\n10\n20\n50\n80\nAnswer Length (in tokens)\n% from Chosen Tokens (h) NQ - p = 30%\n0 20 40 60 80 100 120\n0.8\n0.9\n1\n1.1\n1.2\n1.3\n1.4 1\n2\n10\n20\n50\n80\nAnswer Length (in tokens)\n% from Chosen Tokens (i) NQ - p = 50%\nFigure 10: The percentage of tokens that were chosen from each passage, for FiD-Large models. The gold passage\n(labeled as 1) is colored red. Each row represents a different dataset, and every column represents a different filtering\npercentage (10,30,50).\n1524",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8967370390892029
    },
    {
      "name": "Security token",
      "score": 0.858724057674408
    },
    {
      "name": "Bottleneck",
      "score": 0.8103259205818176
    },
    {
      "name": "Decoding methods",
      "score": 0.6770990490913391
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.6174477338790894
    },
    {
      "name": "Process (computing)",
      "score": 0.49434971809387207
    },
    {
      "name": "Generative grammar",
      "score": 0.4916408658027649
    },
    {
      "name": "Question answering",
      "score": 0.4706503748893738
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4448803663253784
    },
    {
      "name": "Reduction (mathematics)",
      "score": 0.4312054514884949
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4209742844104767
    },
    {
      "name": "Information retrieval",
      "score": 0.3734060227870941
    },
    {
      "name": "Machine learning",
      "score": 0.3445597290992737
    },
    {
      "name": "Programming language",
      "score": 0.14502495527267456
    },
    {
      "name": "Algorithm",
      "score": 0.09258630871772766
    },
    {
      "name": "Embedded system",
      "score": 0.08484497666358948
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I13955877",
      "name": "Bar-Ilan University",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I4210104622",
      "name": "Intel (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 3
}