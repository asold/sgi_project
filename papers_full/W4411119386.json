{
    "title": "DenseSSM: State Space Models with Dense Hidden Connection for Efficient Large Language Models",
    "url": "https://openalex.org/W4411119386",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A1993563068",
            "name": "Wei He",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2066495060",
            "name": "Kai Han",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2765811422",
            "name": "Yehui Tang",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2096092943",
            "name": "ChengCheng Wang",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2103442107",
            "name": "Yujie Yang",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2342495564",
            "name": "Tianyu Guo",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2095972213",
            "name": "Yunhe Wang",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4281758439",
        "https://openalex.org/W4313442864",
        "https://openalex.org/W3064840847",
        "https://openalex.org/W2908510526"
    ],
    "abstract": null,
    "full_text": "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pages 9243–9254\nApril 29 - May 4, 2025 ©2025 Association for Computational Linguistics\nDenseSSM: State Space Models with Dense Hidden Connection for\nEfficient Large Language Models\nWei He∗ Kai Han*† Yehui Tang Chengcheng Wang\nYujie Yang Tianyu Guo Yunhe Wang †\nHuawei Noah’s Ark Lab\n{hewei142, kai.han, yunhe.wang}@huawei.com\nAbstract\nLarge language models (LLMs) face a signif-\nicant challenge due to the excessive computa-\ntional and memory requirements of the com-\nmonly used Transformer architecture. While\nstate space model (SSM) is a new type of foun-\ndational network architecture offering lower\ncomputational complexity, their performance\nhas yet to fully rival that of Transformers. This\npaper introduces DenseSSM, a novel approach\nto enhance the flow of hidden information be-\ntween layers in SSMs. By selectively inte-\ngrating shallow-layer hidden states into deeper\nlayers, DenseSSM retains fine-grained infor-\nmation crucial for the final output. This in-\ncremental improvement maintains the train-\ning parallelizability and inference efficiency\nof SSMs while significantly boosting perfor-\nmance. The proposed method is broadly ap-\nplicable to various SSM types, including Ret-\nNet and Mamba, and DenseSSM achieves sig-\nnificant performance improvements on public\nbenchmarks, demonstrating its effectiveness\nand versatility.\n1 Introduction\nSince the release of ChatGPT (OpenAI, 2023),\nlarge language models (Team, 2023; Bai et al.,\n2023; Touvron et al., 2023; Zhou et al., 2024; Wang\net al., 2023) have entered a new epoch, showcasing\noutstanding abilities in language comprehension,\ndialogue, and logical reasoning. Over the past year,\nthe industry has witnessed the emergence of numer-\nous large language models, such as LLaMA (Tou-\nvron et al., 2023) and ChatGLM (Zeng et al.,\n2023). These large language models have given\nrise to a plethora of practical applications, includ-\ning conversational bots, code assistants, and AI\nagents. The foundation of large language models\nlies in the Transformer network structure (Vaswani\n*Equal contribution\n†Corresponding author\net al., 2017), primarily utilizing a multi-head self-\nattention module for modeling relationships be-\ntween tokens and a Feed-forward network for non-\nlinear feature transformations. The scaling law (Ka-\nplan et al., 2020) based on the Transformer struc-\nture has propelled the continuous development and\nexpansion of large language models.\nIn the Transformer network, multi-head self-\nattention (MHSA) plays a crucial role, but it comes\nwith significant computational demands and mem-\nory requirements during inference. In terms of\ncomputational complexity, for an input sentence\nof length N, the calculation of self-attention has\na complexity of O(N2) during training and infer-\nence. Regarding memory usage, previously en-\ncountered keys and values are stored, leading to a\nmemory occupation of O(ND). As a result, recent\nefforts on network architectures have focused on\nsimplifying Transformer by reducing its computa-\ntion and space complexity. This includes various\napproaches, notably convolutional language mod-\nels (Poli et al., 2023), recurrent unit (Lei, 2021),\nlong context models (Ding et al., 2023), and state\nspace models (SSMs) (Gu et al., 2021; Gu and Dao,\n2023). These new models have provided strong\nalternatives to Transformer for building efficient\nLLMs.\nSSMs propose modeling sequences by introduc-\ning an appropriate design of hidden states for han-\ndling long-range dependencies with both training\nparallelizability and inference efficiency. Starting\nfrom the continuous mapping system, SSMs are\ndiscretized to process discrete inputs in deep learn-\ning such as language sequence. The discretized\nSSMs can be computed in both linear recurrence\nand global convolution modes. Commonly, convo-\nlution mode is used during training to achieve paral-\nlel acceleration, while recurrence mode is used dur-\ning autoregressive inference because it has lower\ncomputational complexity.\nThe core distinction of SSMs from other neu-\n9243\nral networks, such as fully-connected neural net-\nworks, lies in the design of hidden states. Hidden\nstates enable information to be propagated along\nthe temporal dimension, while avoiding the com-\nputation complexity of accessing historical tokens\nat each step. Through state transition parameters\nA, hidden states transfer the hidden information\nfrom the previous time steps to the current time\nstep, allowing for autoregressive prediction of the\nnext token. Hidden states play a crucial role in\nSSMs, but have not received sufficient investiga-\ntion in the past. Weights and hidden features in\ndifferent layers contain information at various lev-\nels from fine-grained to coarse-grained (Gu et al.,\n2021). However, in previous versions of SSMs,\nhidden states only flowed within the current layer\nand could not transmit more information to deeper\nlayers, thus failing to capture more hierarchical\ninformation.\nIn this paper, we propose DenseSSM to facili-\ntate a more comprehensive flow of hidden infor-\nmation between layers in state space models. We\nfirst analyze the hidden state degradation in con-\nventional SSMs which will prevent hidden infor-\nmation flow from low levels to high levels. By\nselectively integrating shallow-layer hidden states\ninto deeper layers, DenseSSM retains fine-grained\ninformation that is useful for the final output. The\nproposed method is applicable to different types\nof SSMs, such as RetNet (Sun et al., 2023) and\nMamba (Gu and Dao, 2023). Our approach main-\ntains the training parallelizability and inference\nefficiency of SSMs, while achieving a significant\nimprovement with only a slight increase in the num-\nber of parameters. For instance, our DenseRetNet\nmodel outperforms traditional RetNet with up to\n5% accuracy improvement on public benchmarks.\n2 Related Works\n2.1 Large Language Models\nLarge language models (LLMs) have seen transfor-\nmative advancements, enabling them to excel in a\ndiverse array of natural language processing (NLP)\ntasks, including machine translation, text summa-\nrization, and emergent abilities like incontext learn-\ning, which were previously unattainable by earlier\nlanguage models (Devlin et al., 2019; Raffel et al.,\n2023). The evolution of LLMs has been marked by\na monumental shift in scale, exemplified by mod-\nels like GPT-3 (Brown et al., 2020), with its 175\nbillion parameters, and the even more expansive\nPaLM (Chowdhery et al., 2022), packing in a as-\ntounding 540 billion parameters. These models\nhave empirically validated the scaling law (Kaplan\net al., 2020), which posits that increasing model\nsize leads to improved performance.\nThe rapid expansion in model size has under-\nscored the critical need for the development of ef-\nficient Transformer algorithms (Dao et al., 2022;\nDao, 2023; Gu et al., 2021, 2020; Smith et al., 2023;\nFu et al., 2023; Mehta et al., 2022; Sun et al., 2023;\nLiu et al., 2024), where FlashAttention (Dao et al.,\n2022; Dao, 2023) has emerged as a significant in-\nnovation. This approach enhances the pivotal at-\ntention mechanism within Transformers by opti-\nmizing softmax computations using a technique\nknown as tiling. By minimizing memory transac-\ntions between the GPU’s HBM and on-chip SRAM,\nFlashAttention compute exact attention with fewer\nmemory accesses, resulting in both faster execu-\ntion and a lower memory footprint compared to\nstandard attention implementations.\n2.2 State Space Models\nWhile the Transformer is currently the de facto\narchitecture for large language models (LLMs),\nproviding efficient parallel GPU training, the in-\nference time for single-token inference increases\nsignificantly with longer sequence lengths, pos-\ning challenges for deployment due to the O(N)\ncomplexity per step even with accelerating algo-\nrithms like FlashAttention (Dao et al., 2022; Dao,\n2023). Efforts have been dedicated to research-\ning the Transformer-Next architecture, aiming to\nachieve state-of-the-art (SOTA) performance with\nefficient parallel training and effective inference,\nparticularly for long sequence lengths.\nState Space Sequence Models (SSMs) have re-\ncently emerged as promising architectures for se-\nquence modeling. HiPPO (Gu et al., 2020) stream-\nlines sequence modeling by compressing lengthy\ninputs into a dynamic, polynomial-based represen-\ntation using orthogonal polynomials. S4 (Gu et al.,\n2021) introduced a novel parameterization through\nthe application of a low-rank structured correction,\nenabling stable diagonalization and simplifying the\nprocess into Cauchy kernel operations. S5 (Smith\net al., 2023) further simplifies the S4 layer by em-\nploying a single multi-input, multi-output SSM and\nintroducing efficient parallel scan algorithms into\nthe S4 layers. H3 (Fu et al., 2023) narrows the\nperformance gap between SSMs and Transformer\nlanguage models by designing three projections\n9244\n(Q, K, V) to simulate the attention mechanism and\nadopting a fast Fourier transform (FFT) to reduce\ncomputation and memory consumption further.\nGSS (Mehta et al., 2022) was the first gated neu-\nral network architecture incorporating SSMs, it\nbuilds upon (Hua et al., 2022) and introducing\na compact SSM architecture that contracts model\ndimensions. Unlike GSS, which emphasizes com-\npressing context into a smaller state, Mamba (Gu\nand Dao, 2023) diverges by focusing on enhancing\nthe selectivity of the state representation, aiming to\nbalance the tradeoff between efficiency and effec-\ntiveness without compromising the model’s ability\nto capture essential information from the context. It\nachieves this by integrating a selection mechanism\nwhich enabling the model to selectively prioritize\nrelevant information while concurrently utilizing a\nhardware-optimized algorithm.\n2.3 Linear Attention\nLinear attentions (Katharopoulos et al., 2020; Zhai\net al., 2021), which remove the softmax operation\nfrom traditional attention, can be seen as a deriva-\ntive of State Space Models (SSMs). They replace\nSSMs’ convolutions with a variation of Multi-Head\nAttention (MHA) and eliminate the softmax of the\ntraditional attention mechanism by utilizing a ker-\nnel function that operates independently on the\nqueries (Q) and keys (K). These mechanisms also\nhave a parallel form for efficient training and a\nrecurrent form with O(1) complexity.\nRetNet (Sun et al., 2023), TransNormer-\nLLM (Qin et al., 2024), and RWKV (Peng et al.,\n2023) implement a fixed decay factor to update\nthe previous key-value (KV) states at each recur-\nrent step. This decay mechanism seamlessly in-\ntegrates with the causal attention mask for effi-\ncient parallel computation. However, since this\ndecay factor is preset and independent of the data,\nit may not be universally applicable across all\ntasks, especially when prompts or long-range infor-\nmation is particularly important. To address this\nchallenge, GLA (Gated Linear Attention) (Yang\net al., 2023) introduces data-dependent gating\nmechanisms that are practical for both parallel and\nblock-parallel forms. It performs competitively\nagainst strong baselines, including the LLaMA-\narchitecture Transformer (Touvron et al., 2023) and\nMamba (Gu and Dao, 2023).\n3 DenseSSM\nIn this section, we analyze the hidden state degra-\ndation in the deeper layers of SSMs and further\nintroduce dense connection of hidden states to pre-\nserve richer information for deeper layers.\n3.1 Prelimineries\nTransformer Transformer is the widely-used net-\nwork architecture of large language models which\nis based on the self-attention mechanism. The self-\nattention performs as follows:\not = Wo\n∑T\ni=1 eqT\nt kivi\n∑T\ni=1 eqT\nt ki\nl, (1)\nwhere q, kand vare obtained by fully-connected\nlayers, Wo is the linear transformation weight for\nthe output token ot at the t-th timestep. Each token\nwill merge information of the other tokens by rela-\ntionship weights calculated by the self-attention. In\naddition to self-attention module, the fee-forward\nnetwork (FFN) module is another key component\nto transform the token representation and intro-\nduces more non-linearity. FFN module is usually\ncomposed by two stacked linear layers and non-\nlinear activation function:\nyt = Wdownσ(Wupot), (2)\nwhere Wup and Wdown are the weight matri-\nces of up projection and down projection lay-\ners, and σ(·) is the activation function such as\nGELU (Hendrycks and Gimpel, 2016).\nSSM State space models (SSM) in the literature\nof deep learning refer to the class of structured\nSSMs (Gu et al., 2021) and the derivatives such as\nRWKV (Peng et al., 2023) and RetNet (Sun et al.,\n2023). Here we briefly describe the structured\nSSMs as a representative. Structured SSMs define a\nsequence-to-sequence transformation x(t) →y(t)\nwith an implicit latent state h(t). The continuous\nform is formulated as\nh′(t) = Ah(t) + Bx(t), (3)\ny(t) = Ch(t), (4)\nwhere A, B and C are the parameters. To apply\nSSM to the real discrete data, we discretize the con-\ntinuous case and obtain the recurrence formulation\nand convolution formulation of it. The parameters\nAand B are transformed to the discrete parame-\nters Aand B with the discretization rule such as\n9245\n(a) DenseSSM in autoregressive mode. (b) DenseSSM in parallelizable convolution mode.\nFigure 1: Illustrations of DenseSSM framework, where ϕis the selective transition module and ‘Fusion’ is the\nhidden fusion module.\nzero-order hold (Gu et al., 2021). The recurrence\nformulation is\nht = Aht−1 + Bxt, (5)\nyt = Cht. (6)\nThe convolution formulation is\nK = (CB,CAB,··· ,CA\nt\nB), (7)\ny= x∗K, (8)\nwhere ∗is convolution operation, and t+ 1 is the\nconvolution kernel size. The recurrence mode is\nusually used for efficient autoregressive inference,\nwhile the convolution mode is used for efficient\nparallelizable training.\n3.2 Dense Hidden Connection\nHere we analyze the hidden information flow from\nshallow layers to deep layers. In the following, we\nuse the superscript “l” to represent the l-th block.\nhl\nt =Ahl\nt−1 + Bxl\nt\n=Ahl\nt−1 + BΘ(yl−1\nt )\n=Ahl\nt−1 + BΘ(Chl−1\nt )\n=Ahl\nt−1 + BΘ(CAhl−1\nt−1 + CBΘ(Chl−2\nt ))\n=Ahl\nt−1 + BΘ(CAhl−1\nt−1 + ···\n+ CBΘ(CAhl−m+1\nt−1 + CBΘ(Chl−m\nt )) ···)\nm\n,\n(9)\nwhere Θ(·) is the transformations from the last\noutput to the input of SSM module, such as con-\nvolution and FFN. From Eq. 9, we can see that\nthe transmission of hidden information from the\n(l−m)-th layer to the l-th layer requires passing\nthrough mtransformation blocks and mBC matrix\nmultiplications. Such a complex computational\nprocess can lead to significant information loss,\nmeaning that attempting to retrieve certain infor-\nmation from the (l−m)-th layer at the l-th layer\nbecomes very challenging and unclear.\nThrough the above analysis, we have identified\na crucial issue in SSM, which is the decay of im-\nportant hidden states as the layer depth increases.\nTherefore, we propose a dense connection for hid-\nden states to better preserve fine-grained informa-\ntion from shallow layers, enhancing the ability of\ndeep layers to perceive the original textual infor-\nmation. For the l-th block, we densely connect the\nhidden states in its previous mblocks. First, we\ncollect the shallow hidden states and introduce a\nselective transition module ϕto project them to the\nsubspace of the target layer and select useful parts\nsimultaneously:\nHl\nt = [ϕ(hl−1\nt ); ϕ(hl−2\nt ); ··· ; ϕ(hl−m\nt )], (10)\nThen, the intermediate hidden vectors are injected\ninto the original hidden state of this layer:\nh′l\nt = Fuse(hl\nt,Hl\nt). (11)\nThe operation Fuse() is the function to fuse the\nintermediate hidden vectors and the current hidden\nstate. The SSMs with the proposed dense hidden\nconnection is named as DenseSSM (Figure 1(a)).\nThe DenseSSM scheme can be used in any SSM\n9246\nvariant such as Mamba (Gu and Dao, 2023). Com-\npared to DenseNet (Huang et al., 2017) for convolu-\ntional networks, the proposed DenseSSM densely\nconnect the hidden states in SSMs, and the selective\nmechanism and fusion manner are more efficient\nfor language modeling.\nThe above analysis is based on the recurrence\nmode, in the following we introduce the convo-\nlution mode of DenseSSM for efficient training.\nFrom Eq. 5, we have\nhl\nt = Ahl\nt−1 + Bxl\nt\n= A(Ahl\nt−2 + Bxl\nt−1) + Bxl\nt\n= A\n2\nhl\nt−2 + ABxl\nt−1 + Bxl\nt\n= A\nt\nhl\n0 + A\nt−1\nBxl\n1 + ··· + ABxl\nt−1 + Bxl\nt\n= A\nt\nBxl\n0 + A\nt−1\nBxl\n1 + ··· + ABxl\nt−1 + Bxl\nt.\n(12)\nThis process can be conducted by a convolution on\nthe input sequence (xl\n0,xl\n1,··· ,xl\nt):\nhl\nt = A\nt\nBxl\n0 + A\nt−1\nBxl\n1 + ··· + ABxl\nt−1 + Bxl\nt\n= (xl\n0,xl\n1,··· ,xl\nt) ∗(B,AB,··· ,A\nt\nB).\n(13)\nIn the proposed DenseSSM, we enhance the hidden\nstates by Eq. 11 and then obtain the outputs of\nSSM:\nyl\nt = Ch′l\nt = CFuse((xl\n0,xl\n1,··· ,xl\nt)∗\n(B,AB,··· ,A\nt\nB),Hl\nt).\n(14)\nAs shown in Figure 1(b), DenseSSM can be trained\nin parallelizable convolution mode.\nSelective Transition Module The selective tran-\nsition module ϕ(·) is to project inputs to the target\nsubspace and select the useful part of hidden infor-\nmation simultaneously. We implement the selective\ntransition module with projection layer and gate\nselection mechanism, as shown in Figure 2. First,\nwe project the hidden states in the previousmSSM\nblocks to the same space:\nh′l−m\nt = Proj(hl−m\nt ). (15)\nThen we generate the gate weights based on the\ninput xl\nt and use them to select useful hidden states:\nϕ(hl−m\nt ) = h′l−m\nt ⊙Gate(xl\nt). (16)\nPlease note that the newly introduced modules\nmust not compromise the training parallelizability\nand inference efficiency of the original SSM frame-\nwork. Therefore, we maintain a simple and efficient\nimplementation in practice. The projection layer is\nimplemented using a linear transformation, while\nthe gate module is implemented with a two-layer\nMLP with a SiLU activation (Elfwing et al., 2018).\nFigure 2: Selective Transition Module.\nHidden Fusion Module After the selective\ntransition module, we obtain the selected hid-\nden states from shallow layers, i.e., HL\nt =\n[ϕ(h1\nt ); ϕ(h2\nt ); ··· ; ϕ(hL−1\nt )]. A hidden fusion\nmodule is utilized to integrate shallow hidden states\nwith the current hidden states. Similarly, we keep\nthe implementation simple for efficiency. We add\nthe selected hidden states since they have been pro-\njected to the same space:\nhL\nt = Fuse(hL\nt ,HL\nt ) = hL\nt +\nm∑\ni=1\nhl−i\nt . (17)\nHere, we provide a basic implementation, but of\ncourse, there are other implementation approaches\nsuch as concatenation and cross-attention.\nExtension to RetNet RetNet (Sun et al., 2023)\ncan be viewed as a kind of state space models which\nuses a variant of self-attention rather than convo-\nlution in Eq. 7. Compared to the standard Trans-\nformer, RetNet is a RNN-style language model\nwith fast inference and parallelized training. It uti-\nlizes linear attention to simplify the computation\ncomplexity of self-attention.\nSt = γSt−1 + kT\nt vt, (18)\nyt = qtSt, (19)\nwhere St is the recurrent state, and 0 < γ <1.\nThe dense KV connection for RetNet is performed\nas follows. The low-level keys and values are first\nconcatenated:\nKl\nt = [ϕ(kl−1\nt ); ϕ(kl−2\nt ); ··· ; ϕ(kl−m\nt )], (20)\nVl\nt = [ϕ(vl−1\nt ); ϕ(vl−2\nt ); ··· ; ϕ(vl−m\nt )]. (21)\n9247\nThen, the intermediate key (or value) vectors are\ninjected into the original keys (or values) of this\nlayer:\nk′L\nt = kL\nt +\nm∑\ni=1\nkl−i\nt , (22)\nv′L\nt = vL\nt +\nm∑\ni=1\nvl−i\nt . (23)\nThe RetNet equiped with the proposed dense key-\nvalue (KV) connections is named as DenseRetNet,\nas illustrated as shown in the appendix. In addition,\nthe paralleizable mode of DenseRetNet is formu-\nlated as follows:\nyt = qt\nt∑\ni=1\nγt−ik′T\ni v′i. (24)\nOur DenseRetNet can be implemented in paralleliz-\nable mode as well, that is, can be trained in parallel\non GPUs or NPUs.\n4 Experiments\nIn this section, we conducted comprehensive exper-\niments to validate the effectiveness of the proposed\nDenseSSM. The verification was carried out on dif-\nferent architectures, including RetNet and Mamba\n4.1 Data and Experimental Settings\nPretraining Data In our empirical analysis, we\ntrained multiple models from scratch. Our experi-\nments involved training on a dataset tokenized with\nthe LLaMA tokenizer (Touvron et al., 2023), com-\nprising 56GB of raw data sourced from 91 files\nsampled from The Pile (Gao et al., 2020). This\ndataset was randomly sampled from the full Pile\ndataset, excluding data from the DM_Mathematics\nand GitHub subsets, resulting in a cached dataset\ncontaining a total of 15 billion tokens. For a de-\ntailed list of the 15B data files sampled from the\nPile in our analysis, see Ref. A.3.\nAdditionally, we conducted experiments with\nDenseMamba-1.4B, trained on the entire Pile\ndataset, extending to 300 billion tokens and uti-\nlizing the GPT-NeoX tokenizer (Black et al., 2022).\nThis approach ensured a fair comparison with other\nmodels, such as Mamba and Pythia (Biderman\net al., 2023).\nEvaluation Datasets In our experiment, we in-\nvestigate models performance across a spectrum of\ndownstream tasks, focusing on zero-shot and 4-shot\nlearning capabilities. The tasks, We benchmarked\nin Table 1, encompass a range of datasets designed\nto test common-sense reasoning and question-\nanswering, such as HellaSwag (Zellers et al.,\n2019), BoolQ (Clark et al., 2019), COPA (Ponti\net al., 2020), PIQA (Bisk et al., 2019), Wino-\ngrad (Muennighoff et al., 2022), Winogrande (Sak-\naguchi et al., 2019), StoryCloze (Lin et al., 2021),\nOpenBookQA (Mihaylov et al., 2018), SciQ (Welbl\net al., 2017), ARC _E (ARC-easy) and ARC _C\n(ARC-challenge) (Clark et al., 2018). Words Per-\nplexity results of WikiText (Merity et al., 2016)\nand LAMBADA (LAMBADA_OPENAI) (Paperno\net al., 2016) are also reported. All evaluations are\nexecuted using the LM evaluation harness (Gao\net al., 2023), ensuring a standardized approach to\nassessing the models’ capabilities.\n4.2 Training Setup and Model’s Architectures\nTo validate the effectiveness of our proposed\nmethod, we trained 350M and 1.3B DenseSSM\nmodels from scratch for one epoch. For experi-\nments with 15 billion training tokens, we utilized a\ntraining batch size of 0.5 million tokens and train-\ning context length is set to 2048. The AdamW\noptimizer (Loshchilov and Hutter, 2019) was em-\nployed, featuring a polynomial learning rate decay\nand warm-up ratio is set to 1.5% of total training\nsteps. We set the weight decay to 0.01 and applied\ngradient clipping at 1. In experiments conducted on\nThe Pile (300B), we adhered to the training settings\nand model hyperparameters used in Mamba (Gu\nand Dao, 2023). Additionally, we designed our\nDenseRetNet model to be fully comprised of GAU-\nlike blocks, which will be detailed in the subse-\nquent paragraph.\nTransformer-based language models We evalu-\nate our proposed select dense mechanism against\npopular large language models like LLaMA (Tou-\nvron et al., 2023) and OPT (Zhang et al., 2022),\ncomparing with LLaMA for 350M size models and\nwith OPT for 1.3B size models. Their hyperparam-\neters are reported in the appendix A.2.\nMamba In our experiments with a dataset con-\ntaining 15 billion tokens, we followed the model\nstructure in each Mamba layer and the training set-\ntings outlined in Mamba’s paper. Specifically, we\nset the learning rate to 3e-4 for training the Mamba-\n360M model and 2e-4 for the Mamba-1.3B model,\nwith no dropout applied in either case. Two addi-\ntional layers were added to ensure a fair compar-\n9248\nModels / TasksWikitext↓ LAMBADA↓ ARC_C ARC_E BoolQ COPA HellaSwag PIQA WinoGrande StoryCloze Winograd OpenBookQA SciQ Avg.↑Zero-ShotLLaMA-350M26.79 22.50 22.9546.13 59.2764 33.19 64.36 49.09 57.64 62.02 29.6 75.3 51.23RetNet-350M36.88 35.53 21.25 40.99 48.35 61 29.86 62.30 51.07 55.59 59.05 28.4 75.8 48.51DenseRetNet-350M31.35 19.92 23.72 45.03 58.5069 32.31 64.04 52.09 58.04 60.82 30.4 76.651.87Mamba-360M26.60 17.55 23.98 45.83 55.78 61 34.89 64.31 52.88 58.90 62.92 29.2 79.851.77DenseMamba-360M26.41 17.03 24.32 46.0 59.20 66 34.68 64.80 51.14 59.03 63.23 29.8 79.852.55Four-ShotLLaMA-350M- - 23.8147.2653.00 65 33.71 64.15 51.14 57.38 64.25 28.2 81.251.73RetNet-350M - - 23.04 40.91 50.37 63 29.49 62.08 51.78 55.66 59.61 27.4 77.4 49.16DenseRetNet-350M- - 24.74 45.66 54.8969 32.14 63.70 52.01 57.58 59.23 28.2 78.3 51.41Mamba-360M- - 25.2646.51 45.41 63 34.25 65.13 52.80 58.97 62.88 29.0 81.051.29DenseMamba-360M- - 24.83 46.9758.2666 34.74 64.69 52.01 58.37 63.44 28.6 80.3 52.56Zero-ShotOPT-1.3B 22.04 13.79 24.66 48.65 58.07 63 37.00 65.89 52.80 61.02 65.51 29.6 81.1 53.39RetNet-1.3B 27.90 23.41 22.61 46.34 48.75 58 32.25 63.44 49.96 57.71 60.65 23.4 77.3 49.13DenseRetNet-1.3B21.55 10.88 24.49 50.8858.6263 38.72 67.25 49.96 60.82 65.85 31.8 82.7 54.01Mamba-1.3B 21.79 12.46 25.0950.84 53.1567 38.34 67.19 50.59 60.29 65.25 30.0 79.8 53.41DenseMamba-1.3B21.39 12.47 25.09 51.8958.5967 39.26 67.90 52.01 61.28 66.11 30.6 79.9 54.51Four-ShotOPT-1.3B - - 25.94 50.46 52.35 63 36.97 64.64 52.33 60.09 66.58 28.2 89.453.63RetNet-1.3B - - 24.66 46.30 47.49 67 31.96 63.22 52.09 57.51 61.42 26.6 80.3 50.78DenseRetNet-1.3B- - 25.6853.0756.3 67 38.56 66.97 53.59 62.08 65.12 27.8 86.7 54.81Mamba-1.3B - - 26.9652.69 49.5669 39.25 66.27 52.96 61.15 66.06 30.4 82.3 54.24DenseMamba-1.3B- - 26.54 52.9958.5967 39.26 67.08 53.67 61.48 65.89 31.0 82.155.05\nTable 1: Benchmarking results on the 15B Pile subset, comparing DenseSSM models with baseline models\nlike RetNet (Sun et al., 2023) and Mamba (Gu and Dao, 2023), as well as Transformer-based models LLaMA-\n350M (Touvron et al., 2023) and OPT-1.3B (Zhang et al., 2022). DenseSSM models demonstrate lower perplexity\nand higher accuracy, enhancing the performance of SSM models and surpassing that of Transformer-based models.\nison in terms of parameter count. Details of the\nmodel’s hyperparameters are provided in the ap-\npendix A.2. For experiments scaling up to the Pile\ndataset with 300 billion tokens, we used the same\narchitecture as the original Mamba-1.4B model,\nwith negligible increases in parameters and compu-\ntational costs for dense hidden connections thanks\nto the relatively small hidden size of the Mamba\narchitecture.\nRetNet Model sizes and hyperparameters for\nour RetNet variants with DenSSM methods are\nshown in the appendix A.2. We further utilize\nGated Attention Unit (GAU) (Hua et al., 2022)\nin our DenseRetNet. GAU combine Attention\nand FFN block into one, so a single block can\nperform both channel mixing and token mixing:\nY = ( XWu ⊙AˆV)Wo, where A is attention\nweight cauculated though Eq. 24. Also, multiple at-\ntention heads with different exponential decay rates\nare utilized to perform multi-scale decay instead\nof GAU’s single-head strategy. In our experiments,\nwe have observed that our architecture surpasses\nthe origin RetNet structure in terms of training sta-\nbility and performance.\n4.3 Experiment Results\nExperiment Results on 15B Pile-Subset Table 1\npresents the experimental results from training with\nthe 15B pile-subset, comparing DenseRetNet and\nDenseMamba with LLaMA (Touvron et al., 2023),\nOPT (Zhang et al., 2022), Mamba (Gu and Dao,\n2023), and RetNet (Sun et al., 2023). DenseRetNet\nachieves lower perplexity on the Wikitext and\nLAMBADA, demonstrating clear advantages in\ndownstream tasks in both zero-shot and few-shot\nsettings, and significantly outperforms RetNet. Ad-\nditionally, DenseMamba shows superior perplexity\nand accuracy on the test set, outperforming Mamba\nand other Transformer-based models.\nExperiment Results on 300B Pile In our experi-\nments with the 300B Pile dataset, we assessed the\nperformance of DenseMamba-1.4B trained from\nscratch. We compared benchmark results from\nthe original Mamba-1.4B (Gu and Dao, 2023),\nPythia-1.4B (Biderman et al., 2023) and RWKV-\n1.5B (Peng et al., 2023), which were sourced\nfrom the Mamba paper. As illustrated in Table 2,\nDenseMamba-1.4B demonstrated a clear advantage\nover the original Mamba-1.4B and other models.\nThis highlights the effectiveness of the DenseSSM\napproach in handling data at scale.\n4.4 Ablation Studies\nWe conduct an ablation study to assess the impact\nof various design choices in our Selective Transi-\ntion Module and Hidden Fusion Module. Word Per-\nplexity results are reported for in-domain and out-\nof-domain corpora (Merity et al., 2016). We adjust\nmodel parameters to ensure fair comparisons across\nall studies under similar computational costs, using\na 350M RetNet model as the baseline. Metrics are\nIn-domain evaluation loss and out-of-domain Wiki-\ntext word perplexity, with training data consisting\nof 5B tokens tokenized using LLaMA tokenizer.\n9249\nModel Pile↓ LAMBADA↓ LAMBADA HellaSwag PIQA Arc_E Arc_C WinoGrandeAvg.↑\nPythia-1.4B 7.51 6.08 61.7 52.1 71.0 60.5 28.5 57.2 55.2\nRWKV-1.5B 7.70 7.04 56.4 52.5 72.4 60.5 29.4 54.6 54.3\nMamba-1.4B 6.80 5.04 64.9 59.1 74.2 65.5 32.8 61.5 59.7\nDenseMamba-1.4B6.68 4.85 66.4 60.6 74.0 66.7 33.2 62.9 60.6\nTable 2: Zero-shot benchmarking results when training the Pile(300B), comparing DenseSSM models with Pythia-\n1.4B (Biderman et al., 2023), RWKV-1.5B (Peng et al., 2023) and Mamba-1.4B (Gu and Dao, 2023).\nProjection Select #Param In domain Wikitext\nVanilla RetNet - 356M 2.524 46.82\nNone None 346M 2.459 43.76\nIdentity MLP 353M 2.428 42.08\nIdentity Linear 357M 2.443 43.54\nLinear MLP 353M 2.460 43.37\nLinear Linear 356M 2.469 44.23\nTable 3: Ablation on Selective Transition Module\nFusion Layers (m) Diff. gates#Param In domain Wikitext\nVanilla RetNet - 356M 2.524 46.82\n1 /enc-37353M 2.463 44.17\n2 /enc-37353M 2.428 42.05\n2 /enc-33360M 2.431 42.12\n4 /enc-37353M 2.420 42.10\n4 /enc-33374M 2.447 43.91\nTable 4: Ablation on Different Fusion Layers and Gates\nAblations on Selective Transition Module The\nselective transition module projects shallow hidden\nstates to a common subspace and selects useful\nparts, which can be implemented in various ways.\nTable 3 examines different settings for Projection\nand Select. With variables controlled (dense lay-\ners fixed at 2 and ’Add’ operation used as fusion\nmodule), the results show that Identity Projection\ncombined with a selection gate, learned from hid-\nden states via a parameter-efficient MLP, optimally\nbalances parameter efficiency and performance.\nAblations on Dense Layers We also conducted\nan ablation analysis on the depth of stored fusion\nlayers (denoted as m). Our results, shown in Ta-\nble 4, indicate that both two-layer(m=2) and four-\nlayer (m=4) fusion architectures improve perfor-\nmance. Considering computational cost, the two-\nlayer fusion is more optimal. Additionally, we\nexplored the necessity of different selection gates\nfor various stored dense layers m, different selec-\ntion gates do not significantly impact performance,\nbenefiting the development of lightweight dense\nconnection architectures.\nAblations on Hidden Fusion Module In Table 5,\nwe evaluate the efficiency and effectiveness of dif-\nferent hidden fusion module methods. Feature fu-\nsion, achieved either by concatenation followed\nby dimension reduction or by employing Cross-\nAttention, tends to increase the model’s parameter\ncount or computational cost. We opted for the addi-\ntion (Add) method over Cross-Attention for our fu-\nsion strategy, prioritizing computational efficiency\nwhile maintaining performance comparability.\nFusion #Param In domain Wikitext\nVanilla RetNet 356M 2.524 46.82\nConcat 354M 2.440 43.75\nAdd 353M 2.428 42.05\nCross-Attention353M 2.422 42.31\nTable 5: Ablation on HiddenFusion module.\nIn Table 6, we investigate the performance of\nfeature fusion when applied at different intervals\nacross layers or at each layer using the same pre-\nviously stored dense features ( m = 2 ). Fusing\nat each layer facilitates information transfer from\nlower to higher layers more effectively.\nFuse Frequency#Param In domain Wikitext\nVanilla RetNet 356M 2.524 46.82\nEvery layer 353M 2.428 42.05\nEvery 2 layers 353M 2.441 42.76\nEvery 4 layers 353M 2.455 44.20\nTable 6: Ablation on Fusion Frequency.\n5 Conclusion\nIn this paper, we propose DenseSSM, a frame-\nwork designed to enhance the hidden information\nflow in SSMs. By selectively integrating hidden\nstates from shallow layers into deeper layers, Dens-\neSSM improves the model’s ability to capture low-\nlevel textual information. This approach preserves\nthe key advantages of SSMs, such as efficient au-\ntoregressive inference and parallelizable training.\nExperiments on Pile have validated the effective-\nness of the DenseSSM method on both RetNet and\nMamba, demonstrating its applicability to various\nSSM architectures.\n9250\n6 Limitations\nIn this paper, our experiments primarily compare\npure SSM methods, while comparisons involving\nhybrid architectures could be part of our future\nwork. We have not yet tested larger-scale models\nand datasets, and the hyperparameters we propose\nfor DenseSSM are optimized for models with sizes\nof 350M and 1.3B. It is important to note that as we\nscale the model, different hyperparameter strategies\nmay prove more optimal, as they can impact the\nstability and efficiency of training.\nReferences\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. 2023. Qwen technical report. arXiv\npreprint arXiv:2309.16609.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory\nAnthony, Herbie Bradley, Kyle O’Brien, Eric Hal-\nlahan, Mohammad Aflah Khan, Shivanshu Purohit,\nUSVSN Sai Prashanth, Edward Raff, et al. 2023.\nPythia: A suite for analyzing large language mod-\nels across training and scaling. In International\nConference on Machine Learning, pages 2397–2430.\nPMLR.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2019. Piqa: Reasoning about\nphysical commonsense in natural language. Preprint,\narXiv:1911.11641.\nSid Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. Preprint, arXiv:2005.14165.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. Preprint, arXiv:2204.02311.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surpris-\ning difficulty of natural yes/no questions. Preprint,\narXiv:1905.10044.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv,\nabs/1803.05457.\nTri Dao. 2023. Flashattention-2: Faster attention with\nbetter parallelism and work partitioning. Preprint,\narXiv:2307.08691.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,\nand Christopher Ré. 2022. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nPreprint, arXiv:2205.14135.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. Preprint, arXiv:1810.04805.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,\nShaohan Huang, Wenhui Wang, Nanning Zheng,\nand Furu Wei. 2023. Longnet: Scaling trans-\nformers to 1,000,000,000 tokens. arXiv preprint\narXiv:2307.02486.\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018.\nSigmoid-weighted linear units for neural network\nfunction approximation in reinforcement learning.\nNeural networks, 107:3–11.\nDaniel Y . Fu, Tri Dao, Khaled K. Saab, Armin W.\nThomas, Atri Rudra, and Christopher Ré. 2023. Hun-\ngry hungry hippos: Towards language modeling with\nstate space models. Preprint, arXiv:2212.14052.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,\nSid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li,\nKyle McDonell, Niklas Muennighoff, Chris Ociepa,\nJason Phang, Laria Reynolds, Hailey Schoelkopf,\nAviya Skowron, Lintang Sutawika, Eric Tang, An-\nish Thite, Ben Wang, Kevin Wang, and Andy Zou.\n2023. A framework for few-shot language model\nevaluation.\n9251\nAlbert Gu and Tri Dao. 2023. Mamba: Linear-time\nsequence modeling with selective state spaces. arXiv\npreprint arXiv:2312.00752.\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and\nChristopher Ré. 2020. Hippo: Recurrent mem-\nory with optimal polynomial projections. Advances\nin neural information processing systems, 33:1474–\n1487.\nAlbert Gu, Karan Goel, and Christopher Re. 2021. Ef-\nficiently modeling long sequences with structured\nstate spaces. In International Conference on Learn-\ning Representations.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nWeizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V . Le.\n2022. Transformer quality in linear time. Preprint,\narXiv:2202.10447.\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and\nKilian Q Weinberger. 2017. Densely connected con-\nvolutional networks. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4700–4708.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. Preprint,\narXiv:2001.08361.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. Preprint, arXiv:2006.16236.\nTao Lei. 2021. When attention meets fast recurrence:\nTraining language models with reduced compute.\nIn Proceedings of the 2021 Conference on Empir-\nical Methods in Natural Language Processing, pages\n7633–7648.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona T. Diab, Veselin\nStoyanov, and Xian Li. 2021. Few-shot learn-\ning with multilingual language models. CoRR,\nabs/2112.10668.\nFangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng\nNi, Duyu Tang, Kai Han, and Yunhe Wang. 2024.\nKangaroo: Lossless self-speculative decoding for ac-\ncelerating LLMs via double early exiting. In The\nThirty-eighth Annual Conference on Neural Informa-\ntion Processing Systems.\nIlya Loshchilov and Frank Hutter. 2019. De-\ncoupled weight decay regularization. Preprint,\narXiv:1711.05101.\nHarsh Mehta, Ankit Gupta, Ashok Cutkosky, and\nBehnam Neyshabur. 2022. Long range lan-\nguage modeling via gated state spaces. Preprint,\narXiv:2206.13947.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. Preprint, arXiv:1609.07843.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In EMNLP.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2022. Crosslingual general-\nization through multitask finetuning. Preprint,\narXiv:2211.01786.\nOpenAI. 2023. Chatgpt (mar 14 version). https://\nchat.openai.com/chat.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Quan Ngoc Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernández. 2016. The lambada dataset.\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak,\nSamuel Arcadinho, Huanqi Cao, Xin Cheng, Michael\nChung, Matteo Grella, Kranthi Kiran GV , et al. 2023.\nRwkv: Reinventing rnns for the transformer era. In\nFindings of EMNLP 2023.\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y\nFu, Tri Dao, Stephen Baccus, Yoshua Bengio, Ste-\nfano Ermon, and Christopher Ré. 2023. Hyena hierar-\nchy: Towards larger convolutional language models.\narXiv preprint arXiv:2302.10866.\nEdoardo M. Ponti, Goran Glavaš, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nZhen Qin, Dong Li, Weigao Sun, Weixuan Sun,\nXuyang Shen, Xiaodong Han, Yunshen Wei, Bao-\nhong Lv, Xiao Luo, Yu Qiao, and Yiran Zhong.\n2024. Transnormerllm: A faster and better large lan-\nguage model with improved transnormer. Preprint,\narXiv:2307.14995.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2023. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Preprint, arXiv:1910.10683.\n9252\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-\nvatula, and Yejin Choi. 2019. Winogrande: An ad-\nversarial winograd schema challenge at scale. arXiv\npreprint arXiv:1907.10641.\nJimmy T. H. Smith, Andrew Warrington, and Scott W.\nLinderman. 2023. Simplified state space layers for\nsequence modeling. Preprint, arXiv:2208.04933.\nYutao Sun, Li Dong, Shaohan Huang, Shuming Ma,\nYuqing Xia, Jilong Xue, Jianyong Wang, and Furu\nWei. 2023. Retentive network: A successor to\ntransformer for large language models. Preprint,\narXiv:2307.08621.\nInternLM Team. 2023. Internlm: A multilingual lan-\nguage model with progressively enhanced capabili-\nties.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. Preprint,\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nYunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo,\nKai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan\nBai, Yun Wang, et al. 2023. Pangu- π: Enhancing\nlanguage model architectures via nonlinearity com-\npensation. arXiv preprint arXiv:2312.17276.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions. In\nNUT@EMNLP.\nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar\nPanda, and Yoon Kim. 2023. Gated linear attention\ntransformers with hardware-efficient training. arXiv\npreprint arXiv:2312.06635.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can\na machine really finish your sentence? Preprint,\narXiv:1905.07830.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan\nLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.\nGLM-130b: An open bilingual pre-trained model. In\nThe Eleventh International Conference on Learning\nRepresentations (ICLR).\nShuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen\nHuang, Hanlin Goh, Ruixiang Zhang, and Josh\nSusskind. 2021. An attention free transformer.\nPreprint, arXiv:2105.14103.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open\npre-trained transformer language models. Preprint,\narXiv:2205.01068.\nHang Zhou, Yehui Tang, Haochen Qin, Yujie Yang,\nRenren Jin, Deyi Xiong, Kai Han, and Yunhe Wang.\n2024. Star-agents: Automatic data optimization with\nllm agents for instruction tuning. arXiv preprint\narXiv:2411.14497.\n9253\nA Appendix\nA.1 Illustration of DenseRetNet\nRetNet (Sun et al., 2023) can be viewed as a kind\nof state space models which uses a variant of self-\nattention rather than convolution. The autoregres-\nsive mode of DenseRetNet is shown in Figure 3. In\naddition, the paralleizable mode of DenseRetNet is\nformulated as follows:\nyt = qt\nt∑\ni=1\nγt−ik′T\ni v′i. (25)\nOur DenseRetNet can be implemented in paralleliz-\nable mode as well, that is, can be trained in parallel\non GPUs or NPUs.\nHyperparamLLaMA350M RetNet350MDenseRetNet360M Mamba360MDenseMamba360MLayers 18 16 16 50 50Hidden Size 1024 1216 1536 1024 1024FFN Size 4096 2052 - - -Heads 8 4 2 - -Dense Layers - - 2 4 -Query&Key Size- - 768 - -Value&Gate Size- 2052 3072 - -Learning-rate6×10−4 6×10−4 6×10−4 3×10−4 3×10−4\nAdamβ (0.9,0.98) (0.9,0.98) (0.9,0.98) (0.9,0.95) (0.9,0.95)Dropout 0.0 0.1 0.1 0.0 0.0\nTable 7: Key hyperparameters for 350M models\nHyperparamOPT1.3B RetNet1.3B Mamba1.3B DenseRetNet1.3B DenseMamba1.3BLayers 24 24 50 25 50Hidden Size 2048 2048 2048 2560 2048FFN Size 8192 3456 - - -Heads 32 8 - 4 -Dense Layers - - - 2 4Query&Key Size- - - 1280 -Value&Gate Size- 3456 - 5120 -Learning-rate6×10−4 6×10−4 2×10−4 6×10−4 2×10−4\nAdamβ (0.9,0.98) (0.9,0.98) (0.9,0.95) (0.9,0.98) (0.9,0.95)Dropout 0.1 0.1 0.0 0.1 0.0\nTable 8: Key hyperparameters for 1.3B models\nA.2 Details of the Compared Models\nThere are two model specifications, i.e., 350M and\n1.3B, to verify the validity of our proposed dense\nmechanism. The details of the compared models\nincluding Mamba and RetNet a re listed in Table 7\nand 8.\nA.3 Details of the 15B Pile-Subset\nHere are the details of the sampled files in the 15B\nPile-Subset:\n• pile_ArXiv_{025, 069, 070, 092, 098,\n123, 124, 133, 134, 157}.json\n• pile_Books3_{015, 016, 052, 057, 071,\n083, 084, 093, 115, 134, 173, 197,\n203, 235, 242, 247}.json\n• pile_Enron_Emails_004.json\nFigure 3: DenseRetNet in autoregressive mode.\n• pile_FreeLaw_{031, 083, 104}.json\n• pile_Gutenberg_PG-19_{044, 049}.json\n• pile_OpenSubtitles_{008, 031,\n037}.json\n• pile_OpenWebText2_{011, 050, 063,\n108, 118, 132, 157, 162, 212, 216,\n242, 245, 256}.json\n• pile_Pile-CC_{001, 024, 069, 076,\n106, 120, 133, 181, 209, 211, 237,\n254, 259}.json\n• pile_PubMed_Abstracts_{037, 049,\n054}.json\n• pile_PubMed_Central_{028, 053, 067,\n069, 085, 123, 125, 132, 149, 165,\n173, 215, 220}.json\n• pile_Stack_Exchange_055.json\n• pile_USPTO_Backgrounds_{012, 027,\n031, 051}.json\n• pile_Ubuntu_IRC_{001, 017, 021}.json\n• pile_Wikipedia_en_{006, 009, 043,\n053, 070}.json\n• pile_YoutubeSubtitles_008.json\n9254"
}