{
  "title": "Latent Diffusion Transformer for Probabilistic Time Series Forecasting",
  "url": "https://openalex.org/W4393156206",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5069163666",
      "name": "Shibo Feng",
      "affiliations": [
        null,
        "BC Research (Canada)",
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5100382077",
      "name": "Chunyan Miao",
      "affiliations": [
        null,
        "BC Research (Canada)",
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5100350966",
      "name": "Zhong Zhang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5015991234",
      "name": "Peilin Zhao",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035414307",
    "https://openalex.org/W4322760932",
    "https://openalex.org/W4280497691",
    "https://openalex.org/W6779823529",
    "https://openalex.org/W2529448179",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W6838815585",
    "https://openalex.org/W4315588609",
    "https://openalex.org/W6728396080",
    "https://openalex.org/W4306884390",
    "https://openalex.org/W1984113680",
    "https://openalex.org/W4284702440",
    "https://openalex.org/W3121975202",
    "https://openalex.org/W2920879895",
    "https://openalex.org/W3179605990",
    "https://openalex.org/W3122315731",
    "https://openalex.org/W6674547422",
    "https://openalex.org/W4226317937",
    "https://openalex.org/W4312052147",
    "https://openalex.org/W2970403389",
    "https://openalex.org/W2990718568",
    "https://openalex.org/W4309540715",
    "https://openalex.org/W3181975995",
    "https://openalex.org/W3097294131",
    "https://openalex.org/W3034345631",
    "https://openalex.org/W4312091317",
    "https://openalex.org/W4287365706",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4300425011",
    "https://openalex.org/W3036167779",
    "https://openalex.org/W4292753694",
    "https://openalex.org/W2095654324",
    "https://openalex.org/W4281690218",
    "https://openalex.org/W4221166193",
    "https://openalex.org/W3212867051",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3042623101",
    "https://openalex.org/W4380136384",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2502312327",
    "https://openalex.org/W4287270621",
    "https://openalex.org/W3207917289",
    "https://openalex.org/W3005921148",
    "https://openalex.org/W2531361673",
    "https://openalex.org/W2964232608",
    "https://openalex.org/W2962974533",
    "https://openalex.org/W4310638815",
    "https://openalex.org/W4288094782",
    "https://openalex.org/W3007066689",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4387635218",
    "https://openalex.org/W4382239131",
    "https://openalex.org/W4387092445",
    "https://openalex.org/W4301742181",
    "https://openalex.org/W4387195417",
    "https://openalex.org/W4386075767",
    "https://openalex.org/W4288099666",
    "https://openalex.org/W4221148002",
    "https://openalex.org/W4303519914"
  ],
  "abstract": "The probability prediction of multivariate time series is a notoriously challenging but practical task. This research proposes to condense high-dimensional multivariate time series forecasting into a problem of latent space time series generation, to improve the expressiveness of each timestamp and make forecasting more manageable. To solve the problem that the existing work is hard to extend to high-dimensional multivariate time series, we present a latent multivariate time series diffusion framework called Latent Diffusion Transformer (LDT), which consists of a symmetric statistics-aware autoencoder and a diffusion-based conditional generator, to implement this idea. Through careful design, the time series autoencoder can compress multivariate timestamp patterns into a concise latent representation by considering dynamic statistics. Then, the diffusion-based conditional generator is able to efficiently generate realistic multivariate timestamp values on a continuous latent space under a novel self-conditioning guidance which is modeled in a non-autoregressive way. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular high-dimensional multivariate time series datasets.",
  "full_text": "Latent Diffusion Transformer for Probabilistic Time Series Forecasting\nShibo Feng1,2,3, Chunyan Miao1,2,3, Zhong Zhang4, Peilin Zhao4*\n1School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore\n2Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), NTU, Singapore\n3Webank-NTU Joint Research Institute on Fintech, NTU, Singapore\n4Tencent AI Lab, Shenzhen, China\n{shibo001, ascymiao}@ntu.edu.sg, {todzhang, masonzhao}@tencent.com\nAbstract\nThe probability prediction of multivariate time series is a no-\ntoriously challenging but practical task. This research pro-\nposes to condense high-dimensional multivariate time se-\nries forecasting into a problem of latent space time series\ngeneration, to improve the expressiveness of each times-\ntamp and make forecasting more manageable. To solve the\nproblem that the existing work is hard to extend to high-\ndimensional multivariate time series, we present a latent\nmultivariate time series diffusion framework called Latent\nDiffusion Transformer (LDT), which consists of a symmet-\nric statistics-aware autoencoder and a diffusion-based con-\nditional generator, to implement this idea. Through careful\ndesign, the time series autoencoder can compress multivari-\nate timestamp patterns into a concise latent representation\nby considering dynamic statistics. Then, the diffusion-based\nconditional generator is able to efficiently generate realistic\nmultivariate timestamp values on a continuous latent space\nunder a novel self-conditioning guidance which is modeled\nin a non-autoregressive way. Extensive experiments demon-\nstrate that our model achieves state-of-the-art performance\non many popular high-dimensional multivariate time series\ndatasets.\nIntroduction\nForecasting time series data is crucial across various sectors,\nincluding finance (Sezer, Gudelek, and Ozbayoglu 2020),\nenergy (Cao et al. 2020), traffic (Liu et al. 2016; Feng et al.\n2023) and human identification (Rao and Miao 2022; Rao\net al. 2021). Multivariate forecasting, prevalent in practi-\ncal applications, is more important and popular in indus-\ntrial fields. For example, power companies analyze billions\nof data points from numerous clients to monitor electricity\nconsumption, reflecting the complexity and significance of\nthis task.\nLatent diffusion models (Rombach et al. 2022), a simple\nand efficient way to significantly improve both the training\nand sampling efficiency of denoising diffusion models (Ho,\nJain, and Abbeel 2020) without degrading their quality. This\nparticular class of latent generative models has gained sig-\nnificant recognition and accomplishments in recent times,\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nparticularly in processing high-dimensional types of data\nsuch as high-resolution images (Ho, Jain, and Abbeel 2020;\nTakagi and Nishimoto 2023), natural languages (Li et al.\n2022a; Yuan et al. 2022), and audios (Huang et al. 2022;\nRuan et al. 2023).\nMultivariate time series forecasting seeks to predict future\ntrends accurately but faces challenges due to its complexity\nand computational demands. The common approach of us-\ning deep, auto-regressive (Woo et al. 2022; Liu et al. 2022;\nWu et al. 2020) models to predict future timestamps is hin-\ndered by the high dimensionality of the data and the model’s\nstructure. This leads to two main issues: significant com-\nputational resource requirements, limiting scalability, and\nthe accumulation of errors in forecasts, particularly in high-\ndimensional series. Therefore, there’s a pressing need for an\ninnovative forecasting framework that can efficiently and ef-\nfectively predict future trends with reduced computational\nload and increased speed.\nLatent-space generation, an efficient alternative in time\nseries forecasting, employs a pre-trained autoencoder to mit-\nigate data redundancy, transferring generation from the time\nto a latent domain. The primary challenge is the distribution\nshift problem, as statistical properties like mean and vari-\nance vary over time (Fan et al. 2023; Kim et al. 2021). Tra-\nditional models struggle with numerical inaccuracies when\nusing historical timestamps as the inputs of the autoencoder.\nOur novel approach dynamically updates statistical param-\neters in pre-training, ensuring high-quality, accurate latent\nrepresentations for each timestamp.\nExisting multivariate time-series diffusion models face\ntwo major issues. First, the autoregressive structure (Rasul\net al. 2021) leads to poor long-range prediction, error ac-\ncumulation, and slow inference. Second, most models excel\nin low-dimensional series (Tashiro et al. 2021; Alcaraz and\nStrodthoff 2022; Shen and Kwok 2023) but falter in high di-\nmensions. To overcome these challenges, our approach em-\nphasizes a non-autoregressive, resource-efficient denoising\nnetwork for forecasting. We introduce a self-conditioning-\nbased transformer denoising structure that effectively de-\nnoises time variables in a continuous latent space, incorpo-\nrating covariant features akin to strategies in image genera-\ntion (Chen, Zhang, and Hinton 2022; Yang et al. 2022; Ho\nand Salimans 2022). This Transformer diffusion module sig-\nnificantly reduces computational complexity, resource use,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11979\nand increases sampling speed compared to autoregressive\nmodels.\nIn this work, we introduce a novel two-stage, non-\nautoregressive diffusion architecture for multivariate prob-\nabilistic forecasting. Our experiments across various real-\nworld datasets demonstrate that this model has surpassed ex-\nisting state-of-the-art generative models in high-dimensional\nmultivariate time series forecasting. The key contributions of\nour work are:\n• Introduction of the LDT model, a new approach in mul-\ntivariate time series forecasting, leveraging latent space\nrepresentations for high-accuracy predictions in high-\ndimensional scenarios.\n• Development of a practical LDT structure featuring\na unique self-conditioning mechanism and a non-\nautoregressive transformer, enabling constrained self-\nconditioned predictions.\n• We perform extensive experiments with multiple multi-\nvariate forecasting datasets, demonstrating LDT ′s supe-\nrior performance compared with the recent state-of-the-\nart forecast methods, for multivariate time series proba-\nbilistic predictions.\nBackground\nDiffusion Models (Ho, Jain, and Abbeel 2020) diffusion\nmodels are probability generative models proposed to gen-\nerate the target data distribution p(x) by iterative denoising\na normally distributed variable. Diffusion probabilistic mod-\nels are composed of the fixed forward process and the learn-\nable reverse process, which is a Markov Chain of length T.\nForward Process It is a transition and fixed diffusion pro-\ncess, from the data distribution to a Gaussian distribution.\nGiven a data sample x ∈ Rd ∼ p(x) and some latent\nvariables {z0, z1, ··· , zT }, which interpolate between the\ndata distribution and a Gaussian distribution with the dif-\nfusion steps increase. The forward process can be formally\ndescribed as a Markov chain parameterized with a series of\nvariances βt and αt := 1 − βt.\nq (z1:T |z0) =\n1Y\nt=T\nq (zt|zt−1) ,\nwhere q (zt|zt−1) ∼ N\n\u0010p\n1 − βtzt−1, βtI\n\u0011\n,\n(1)\nSince the more steps of the diffusion process, the more noise\nadded, q (zt|x) has a closed-form solution, which can be de-\nscribed by a general form.\nzt ∼ q (zt|x) = N\n\u0000√\nαtx, (1 − αt) I\n\u0001\n,\nzt = √αtx +\n√\n1 − αtϵ, ϵ ∼ N(0, I),\n(2)\nwhere αt = Qt\ni=1 (1 − βi) ∈ (0, 1), z0 = x and\nzT ∼ N(0, I) As the diffusion steps increases, the latent\nvariable zt become noisier until the zT is approximately\na Gaussian variable, which is independent of the starting\npoint x.\nReverse Process The learnable reverse process is de-\nfined by the inverted Markov chain pθ (z0:T ) =\np (zT ) QT\nt=t pθ (zt−1|zt), where p (zT ) = N(0, I)\nis known. The pθ (zt−1|zt) can be approximately\ndriven from the following equation, q (zt−1 | zt, x) =\nN\n\u0000\nµt (zt, x) , σ2\nt I\n\u0001\n, where µt (zt, x) has a closed-form\nsolution and σt is a hyperparameter. To get thepθ (zt−1|zt),\nwe train a denoising network θ to approximate the x given\nnoisy latent zt and the timestep t\nL = Ex∼p(x),t∼U{1,···,T},zt\nh\n∥ˆxθ (zt, t) − x∥2\n2\ni\n, (3)\nwhere zt ∼ q(zt|x) and pθ (zt−1|zt) is an approximation of\nthe q (zt−1 | zt, ˆxθ (zt, t)), which enables us to sample from\na closed-form and denoise the latent variable by sampling\nzt−1 until we get the z0 = x ∼ p(x).\nzt−1 ∼ pθ (zt−1|zt) = q (zt−1 | zt, ˆxθ (zt, t)), (4)\nFor sampling from the trained diffusion model, we uti-\nlize the inference distribution from Song et al. (2020) and\ntherefore derive the sampling from q(zt−1 | zt, x) =\nN (µq (zt, x) , Σq(t)I),\nq (zt−1 | zt, x) ∝ N\n\u0000\nzt−1; ¯α1\nt zt + ¯α2\nt x, ¯βtI\n\u0001\n, (5)\nwhile setting Σq(t) = 0 gives the deterministic DDIM sam-\npler, ¯α1\nt =\n√\nαt(1−¯αt−1)\n1−¯αt\n, ¯α2\nt =\n√¯αt−1(1−αt)\n1−¯αt\nand ¯βt =\n(1−αt)(1−¯αt−1)\n1−¯αt\n. The detailed training and sampling proce-\ndures are shown in the Method section.\nMethod\nOur approach to high-dimensional multivariate time se-\nries forecasting involves a two-stage process: a statistics-\nbased time autoencoder and a Latent Diffusion Transformer\n(LDT) generator. The autoencoder dynamically updates\nglobal statistics during training for accurate future times-\ntamp reconstruction. The LDT generator then produces la-\ntent conditions using self-conditions and a guidance mech-\nanism, incorporating relevant covariates. This method effi-\nciently captures the inherent dynamics and correlations in\nthe time series data, as LDT is shown in Fig. 1. The specific\nalgorithm details are shown in Algorithms 1 and 2.\nSymmetric Time Series Compression\nIn order to ensure the generality and effectiveness of our\nmodel to generative high-power latent embedding, we con-\nstructed a simple and accurate autoencoder structure. Statis-\ntical properties such as mean and variance often change over\ntime in time series, previous work ”RevIN (Kim et al. 2021),\nDIT-sh (Fan et al. 2023)” had claimed that the discrepancy\nbetween different input sequences can significantly degrade\nthe model performance. We found that in non-stationary\nmultivariate time series, different batch samples that were\nrandomly sampled will have high variance deviation, which\nwill degrade the stability and efficacy of autoencoder train-\ning. Therefore, we propose a simple yet effective symmetric\nautoencoder structure with adaptive variance updating nor-\nmalization layer (VN).\nMore precisely, given the look-back window data X ∈\nRT×d and target Y ∈ Rt×d in the time-space, the nor-\nmalization layer VN normalizes the target Y to the ˆY =\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11980\nFigure 1: The framework of our proposed LDT (a). During the training process, the stage-I V AE is first trained to construct\ntime series latent with reconstruction task, while LDT is trained to generate the future targets conditioned by self-condition,\ncovariates, and diffusion step ˆz0\nm, t, c in the second stage. During the sampling process, the time series latent first be generated\nby LDT, and then input to decoder De to get the future targets. The black dashed lines stand for operations only involved in\nthe training process. The details of the stage-II adaptive layernorm Transformer structure (E d, Dd) are shown in (b) and the\nstructures of stage-I are shown in Appendix.\nVN([X, Y]). Then, the encoder E encodes ˆY into a latent\nrepresentation Z = E( ˆY ), and the decoder D reconstructs\nthe target time series from the latent, giving Y = D(Z) =\nD(E( ˆY )), where Z ∈ Rt×m(m << d). Importantly, the en-\ncoder downsamples the time series by a factor f = d/m,\nand we use the factor f ≈ 2m, with m ∈ N. The reason\nfor choosing the size of f here is to reduce the training diffi-\nculty of the noise-added high-dimensional multivariate time\nseries in the diffusion model. The framework overview of\nour autoencoder is demonstrated in Fig.1.\nAs illustrated in the figure, we first use instance nor-\nmalization (Ulyanov, Vedaldi, and Lempitsky 2016) to\ncalculate the instance-specific mean and standard devia-\ntion for scaling every input Wi =\n\u0002\nXi, Yi\u0003\n∈ Rτ×d\nand τ = T + t, which are described as E\n\u0002\nWi\u0003\n=\n1\nτ\nPτ\nj=1 Wi\nj , Var\n\u0002\nWi\u0003\n= 1\nτ\nPτ\nj=1\n\u0000\nWi\nj − E\n\u0002\nWi\u0003\u00012\nand\nˆEn+1 \u0002\nWi\u0003\n= 1\nn (En+1 \u0002\nWi\u0003\n+ ˆEn \u0002\nWi\u0003\n×(n −1)), where\nn is the number of batches and the adaptive updated func-\ntion of variances ˆV ar\nn+1\nis the x same as ˆEn+1, we nor-\nmalize the target Y i through these updated statistics as\nˆY i = γd\n\u0012\nY i−ˆEn+1[Wi]√\nˆVn+1[Wi]+ϵ\n\u0013\n+ βd, where γd, βd ∈ Rd are\nthe learnable affine parameters. We gradually update the in-\nstance variance and mean that were utilized for regulariza-\ntion. On the one hand, the non-stationary information in the\ntarget sequence can be weakened, making it easier to train\nthe autoencoder. Also, the generative results from the au-\ntoencoder make the diffusion model training in the second\nstage more stable and accurate.\nSpecifically, our autoencoder structure is a symmetrical\nmodel, and the specific modules are shown in Appendix\ndue to the limited space. Also, to avoid arbitrarily high-\nvariance latent spaces, we follow the regularization strategy\nproposed in the ”Latent diffusion Model” which imposed a\nKL-penalty (i.e. weight the KL term by a factor 10−8) to-\nwards a standard normal on the learned latent that preserves\ndetails of Y better and train all our autoencoder models in an\nadversarial manner, such that a timestamp-based discrimina-\ntor Dη is optimized to differentiate original target time series\nfrom reconstructions D(E( ˆY )). The full objective training\nloss function L of our autoencoder reads:\nL = min\nE,D\nmax\nη\n\u0012\nLrec(Y, D(E(Y ))) − Ladv(D(E(Y )))\n+ logDη(Y ) + Lreg(Y ; E, D)\n\u0013\n,\nwhere Lreg is a regularizing loss term which is to regularize\nthe latent Z to be zero-centered and obtain a small variance.\nWe found that different time series always have large vari-\nances, which may lead to the extremely unstable training of\nthe following latent diffusion model. Detailed explanations\nare described in the experiment.\nLatent Diffusion Transformer\nGenerative Modeling of Latent Representations Com-\npared with applying the diffusion model directly in the time\ndomain of high-dimensional multivariate time series, we in-\ntroduce the trained time compression models consisting ofE\nand D that take the efficient and low-dimensional time series\nrepresentations to the following denoising network.\nUnlike previous work that relied on autoregressive gener-\native models in the time-space (Min et al. 2022b; Yi et al.\n2023), we take advantage of the attention-based transformer\nmodels (Min et al. 2022a; Xu et al. 2021) to establish a non-\nautoregressive denoising network structure that includes\nadaptive normalization layer (adaLN) (Park et al. 2019),\ntransformer encoder-decoder block and self-condition guid-\nance block. The details of our non-autoregressive denois-\ning network can be found in Fig.1(b). The training objective\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11981\nwithin our latent diffusion model now reads:\nLLDM := EE(x),x∼p(x),t\nh\n∥E(x) − ˆxθ (zt, c, t)∥2\n2\ni\n, (6)\nwhere the denoising backbone ˆxθ of our model is a self-\nguidance transformer structure and c is conditions like look-\nback window data and covariates. Since the forward process\nis fixed, zt can be efficiently obtained from trainedE and we\nfound that training x improves the generative performance\ncompared to ϵ as the denoising target. And finally the sam-\nples from p(x) can be directly decoded to the time-space\nwith a trained decoder D.\nSelf-Conditioning Guidance\nFirstly, conditional diffusion model can be simply described\nas ˆxθ (zt, c, t). To train the latent diffusion model in a\nclassifier-free guidance manner, we choose to train an un-\nconditional denoising diffusion model pθ(z) parameterized\nthrough a score estimator xθ(zt, t)together with the condi-\ntional model xθ(zt, c, t). We use a single neural network to\nparameterize both models, for the unconditional model we\nonly treat the look-back window conditions as the missing\nvalue when training the denoising network, i.e. xθ(zt, t) =\nxθ(zt, c= ∅, t). We jointly train these two models sim-\nply by setting the look-back window data conditions as ∅\nwith some probability pu which is set as a hyperparameter.\nWhen the conditional latent diffusion model is trained, we\nthen perform sampling using a simple but effective linear\ncombination score estimates,\nˆxθ (zt, c, t) = (1 + w)xθ(zt, c, t) − wxθ(zt, t), (7)\nwhere w is the guidance strength, and when w = 0 , the\nequation becomes the standard conditional diffusion model.\nWhen w > 0, the updated gradient of the denoising net-\nwork will be more offset to the first term and deviate from\nthe latter. Specifically, if we have access to the exact pre-\ndicted scores x∗\nθ (zt, c, t) and x∗\nθ (zt, t), then the gradient of\nthis denoising network structure would be∇zt log p (c|zt) =\n−σt [x∗\nθ (zt, c, t) − x∗\nθ (zt, t)].\nMoreover, we introduced a self-condition mechanism that\ncan be seen as to direct condition on previously gener-\nated samples of its own during the iterative sampling pro-\ncess. Specifically, our conditional latent diffusion model\nˆxθ (zt, c, t) is replaced by the slightly different denoising\nnetwork ˆxθ (zt, ˆz0, c, t) where the ˆz0 is the previously es-\ntimated and updated iteratively. In our setting, we concate-\nnate zt with previously estimated ˆz0 which is obtained from\nthe earlier prediction of the denoising network in the sam-\npling chain. During the training phase, with some probabil-\nity (e.g., 60%), we set ˆz0 = 0 which falls back to modeling\nwithout Self-Conditioning. Apart from this, we first predict\nˆz0 = ˆxθ (zt, 0, c, t) and then use it for self-conditioning. Note\nthat we do not backpropagate through the estimated ˆz0.\nLatent Diffusion Transformer Network\nThe complete denoising network is shown in Figure 1(b).\nFor the time series forecasting in a non-autoregressive way,\nwe need to cover how to process time series inputs (look-\nback window data, target) and the architecture of ˆxθ.\nFirst, we describe how we process time series data as in-\nputs for the training of denoising networks. As defined in\nSection 4.1, ˆEt\n\u0002\nWi\u0003\nand ˆVar\n\u0002\nWi\u0003\ncome from the complete\ntime series includes look-back window data and forecast-\ning target. We first normalize the look-back window con-\nditions X ∈ RT×d using ˆX =\nXi\nt −ˆE[Wi]\n√\nˆVar[Wi]+ϵ\n∈ RT×d\nand rescale the latent representation Z = E(Y ) using\nˆZ ← Z\nˆσ = E(X)\nˆσ where ˆσ2 = 1\nbtm\nP\nb,t,m\n\u0000\nzb,t,m − ˆµ\n\u00012\n,\nfrom the updated results from each training batch, where b\nis batch size, t is prediction length, m is hidden size and\nˆµ = 1\nbtm\nP\nb,t,m Zb,t,m, to obtain the input of the denois-\ning network ˆZ ∈ Rt×m. We then obtain the embedding\nˆXemb ∈ RT×mof the ˆX and ˆZemb ∈ Rt×m of the la-\ntent representation ˆZ by an input projection block consisting\nof two multilayer perceptron layers. In our denoising net-\nwork, we introduce time embedding ofsemb = [s1:τ ] to learn\nthe temporal dependency which is obtained by single MLP\nlayer and Position embedding pemb= [p1:τ ] that is defined\nin (Vaswani et al. 2017). Also, the diffusion-step embedding\ntemb ∈ Rn×1(n=4m) is encoded as a sinusoidal positional\nembedding to guide the adaptive layer norm in transformer-\nbased residual layers, which replaces the standard layernorm\nand defined as\nγi,c = fc (x) , β i,c = hc (x) , (8)\nwhere x indicates arbitrary vector inputs, γi,c and βi,c mod-\nulate a neural network’s activations Yi,c, whose subscripts\nrefer to the ith input’s cth feature map, via a feature-wise\naffine transformation:\nadaLN (γi,c, Yi,c, βi,c) = γi,cYi,c + βi,c, (9)\nfc and hc can be arbitrary functions such as neural networks,\nand in our practice, it is easier to refer to fc and hc as a sin-\ngle function that outputs single vector (γ ∈ Rm, β∈ Rm).\nIn our residual layers, we learn the dimension-wise scale and\nshift parameters γi,c and βi,c through the diffusion step em-\nbedding temb.\nTraining The overall structure of our LDT denoising net-\nwork is shown in Fig.1, and the training objective can refer\nto Eq.6 in the Method section. Besides, the specific training\nand inference procedure is shown in the Algorithm 1, 2.\nInference For each time step t in the reverse process, a\nlearned denoising distribution pθ parameterized by θ gen-\nerates samples zt−1 conditioned on the former noisier sam-\nples zt. After the reverse denoising process reaches T = 0,\nwe round each timestamp of the generated z0 to its nearest\nvalue in the embedding space and obtain the final target by\nthe trained decoder D:\nzt−1 = ˆαzt + ˆγxθ\n\u0000\nzt, t| X\n\u0001\n+ σtε, (10)\nY = D(z0), (11)\nwhere ˆα =\n√\nαt(1−¯αt−1)\n1−¯αt\n, ˆγ =\n√¯αt−1(1−αt)\n1−¯αt\n, σt =\n(1−αt)(1−¯αt−1)\n1−¯αt\nand ϵ ∼ N(0, 1). Note that in the whole dif-\nfusion process of training and sampling, we apply xθ rather\nthan ϵθ. In the experiment, we found that it is difficult to\ncomplete the multivariate time series forecasting with ϵθ.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11982\nAlgorithm 1: Training of LDT\nInput: Sample x0\n1:T (History) and x0\nτ (Target) from training\nset; Number of diffusion steps K; Encoder E in pretrained\nautoencoder. Output: Trained denoising function xθ.\n1: Repeat;\n2: k ∼ Uniform({1, 2, . . . , K});\n3: ϵ ∼ N(0, I);\n4: Generate latent embedding z0\nτ by E(x0\n1:T , x0\nτ );\n5: Generate noised latent zk\nτ = √¯αkz0\nτ + √\n1 − ¯αkϵ;\n6: Obtain diffusion step k’s embedding pemb using sinu-\nsoidal positional embedding.;\n7: x0\n1:T ← ∅ with probability puncond;\n8: Initialize the self cond ˆz0\nτ = zeros like(zk\nτ );\n9: if Uniform(0, I)>0.5 then\n10: z0\npred = xθ(zk\nτ , x0\n1:T , ˆz0\nτ , k);\n11: z0\npred = Stop gradient(z0\npred);\n12: end if\n13: Use the denoising network to generate denoised sample\nz0 by xθ(z0\npred, x0\n1:T , zk\nτ , t);\n14: Obtainz0 by Eq.7, c= ∅ if x0\n1:T ← ∅;\n15: Calculate the loss Lk(θ) by Eq.6;\n16: Take gradient descent step on ∇θLk(θ);\n17: Until Converged.\nAlgorithm 2: Generating of LDT\nInput: Trained denoising network xθ, Decoder in pre-\ntrained autoencoder D and Sample x0\n1:T (History), guidance\nstrength w. Output: Generated corresponding future targets\nˆx0\nτ .\n1: zK\nτ ∼ N(0, I);\n2: ˆz0\nτ = zeros\nlike(zK\nτ );\n3: x∅\n1:T = zeros like(x0\n1:T );\n4: for k ← K to 1 do\n5: ϵ ∼ N(0, I), if k >1, else ϵ = 0;\n6: Obtain diffusion step k’s embeddingpemb using sinu-\nsoidal positional embedding.;\n7: Obtain the self-cond ˆz0\nτ = xθ(zτ k, x∅\n1:T , ˆz0\nτ , pemb);\n8: Obtain the target z0 = xθ(zk\nτ , x0\n1:T , ˆz0\nτ , pemb);\n9: Obtain the guidance-based target z0 with x∅\n1:T by the\nEq.7,\n10: Estimate zk−1\nτ by Eq.10.;\n11: end for\n12: Return ˆx0\nτ\nQuantitative Experiments\nDatasets We extensively evaluate the proposed LDT on five\nreal-world benchmarks, covering the mainstream multivari-\nate time series probabilistic forecasting applications, En-\nergy: Solar (Lai et al. 2018) (137 dimensions) and Electricity\n(370 dimensions), Traffic (963 dimensions) and Taxi (1214\ndimensions), Wikipedia (2000 dimensions). The properties\nof the datasets used in experiments can refer to the previous\nworks (Rasul et al. 2021; Tashiro et al. 2021) and shown in\nAppendix C.\nEvaluation Metrics For probabilistic estimates, we re-\nport both the continuously ranked probability score across\nsummed time series CRPS-sum ((Matheson and Winkler\n1976; Jordan, Kr ¨uger, and Lerch 2017)) and MSE (mean\nsquare error) error metrics, to measure the overall joint dis-\ntribution pattern fit and fit of joint distribution central ten-\ndency, respectively. Due to limited space, the specific form\nof the metrics is shown in Appendix B.\nBaselines We include several baseline methods. For the\nclassical settings and competitive multivariate time series\nbaselines probabilistic models: Gaussian process model(GP)\n(Roberts et al. 2013), KV AE (Krishnan, Shalit, and Sontag\n2017), Vec-LSTM-ind-scaling, GP-scaling, and GP-Copula\n(Salinas et al. 2019). For the time series diffusion mod-\nels, including TimeGrad (Rasul et al. 2021), CSDI (Tashiro\net al. 2021), SSSD (Alcaraz and Strodthoff 2022), D3V AE\n(Li et al. 2022b) as the competitive auto-regressive base-\nlines. Moreover, for the non-autoregressive modeling and\nflow-based structures, we select TLAE (Nguyen and Quanz\n2021) and HMGT (Ding et al. 2020), LSTM-Real-NVP and\nLSTM-MAF (Rasul et al. 2020) in our work.\nImplementation Details In our autoencoder structure of\nthe first stage, both the encoder and the decoder utilized\n3 Transformer encoder layers with 4 heads of the atten-\ntion, and we use one layer Transformer encoder layer with\n4 heads of the attention mechanism in the discriminator.\nThe maximum look-back window data is 4 times the pre-\ndicted target which is the same setting as in (Rasul et al.\n2020), with embedding dimension m ≈ [1/4, 1/8] data fea-\ntures, diffusion steps T = [50, 100, 200, 300], square-root\nnoise schedule(Li et al. 2022a) and quad variance schedule\nβ1 = 10 − 4 tillβT = 0.1. In our denoising network struc-\nture, we use a 3-layer transformer structure with 8 attention\nheads and embedding dim=[32, 64, 128, 256]. Our method\nis dependent upon the ADAM (Kingma and Ba 2014) op-\ntimizer with an initial learning rate of 1e−3, and the batch\nsize is 64. All experiments are repeated more than five times,\nimplemented in PyTorch (Paszke et al. 2019) and GluonTS\n(Alexandrov et al. 2020). The specific experimental hyper-\nparameters corresponding to different datasets are shown in\nAppendix C.\nMain Results\nReal-World Datasets Results We compare the test time\nprediction of our LDT to the above baselines with CRPS,\nCRPS-sum, and MSE. The results for probabilistic forecast-\ning in a multivariate setting are shown in Table 1. Compared\nwith the other generative models, we observe that LDT\nachieves the state-of-the-art (to the best of our knowledge)\nCRPS-sum on almost all benchmarks. Notably, our model\nhas shown a significant CRPS-sum reduction in Electricity\n16%(0.025 → 0.021), in Traffic 14.8%(0.047 → 0.040),\nin Taxi 4%(0.130 → 0.125). Also, in terms of MSE metric,\nwe obtain 22%(2.1e5 → 1.6e5), 8%(4.5e − 4 → 4.1e − 4)\nand 4%(2.2e → 2.3e) of improvement in the above three\ndatasets.\nUncertainty Estimation The uncertainty can be assessed by\nestimating the noise of the outcome series when making the\nprediction. We found that our model showed obvious uncer-\ntainty estimation in two types of datasets. As shown in Fig-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11983\nSOLAR ELECTRICITY\nTRAFFIC TAXI WIKIPEDIA\nMethod C-S\nMSE C-S MSE C-S MSE C-S MSE C-S MSE\nGP 0.828(.010)\n- 0.947(.016) - 2.198(.774) - 0.425(.199) - 0.93(.003) -\nKVAE\n0.340(.025) - 0.051(.019) - 0.100(.005) - - - 0.095(.012) -\nVLIS 0.391(.017)\n9.3e2 0.025(.001) 2.1e5 0.087(.041) 6.3e-4 0.506(.005) 7.3e 0.133(.002) 7.2e7\nGP-scaling 0.368(.012)\n1.1e3 0.022(.000) 1.8e5 0.079(.000) 5.2e-4 0.183(.395) 2.7e 1.483(1.034) 5.5e7\nGP-Copula 0.337(.024)\n9.8e2 0.024(.001) 2.4e5 0.078(.002) 6.9e-4 0.208(.183) 3.1e 0.086(.004) 4.0e7\nLSRP 0.331(.020)\n9.1e2 0.024(.001) 2.5e5 0.078(.001) 6.9e-4 0.175(.001) 2.6e 0.078(.001) 4.7e7\nLSTM-MAF 0.315(.032)\n9.8e2 0.023(.000) 1.8e5 0.069(.002) 4.9e-4 0.161(.002) 2.4e 0.067(.002) 3.8e7\nHMGT 0.327(.013)\n9.4e2 0.022(.003) 2.1e5 0.052(.002) 4.4e-4 0.158(.042) 2.4e 0.074(.011) 3.0e7\nTLAE 0.124(.014) 8.3e2 0.040(.001) 2.7e5 0.069(.005) 5.0e-4 0.130(.010) 2.6e 0.241(.012) 3.8e7\nTimeGrad\n0.317(.020) 9.9e2 0.025(.001) 2.1e5 0.050(.006) 4.6e-4 0.137(.013) 2.4e 0.064(.003) 3.1e7\nCSDI 0.298(.004)\n9.4e2 0.029(.002) 2.4e5 0.053(.009) 4.4e-4 - - - -\nSSSD 0.275(.004) 5.4e2 0.026(.001) 2.3e5 0.047(.002) 4.5e-4 0.133(.006) 2.3e 0.065(.001) 2.99e7\nD3VAE\n0.332(.002) 9.2e2 0.030(.000) 2.4e5 0.049(.001) 4.5e-4 0.130(.011) 2.4e 0.069(.004) 3.2e7\nLDT 0.253(.002) 7.7e2 0.021(.001) 1.6e5 0.040(.000) 4.1e-4 0.125(.007) 2.2e 0.061(.002) 2.92e7\nTable 1: The Test set CRPS-sum(C-S) and MSE comparison(lower is better) of models from the baselines and our model LDT,\nwith - are runs failed with numerical issues, and (*) indicates the experimental variance. VLIS, LSRP are the abbreviations for\nthe Vec-LSTMind-scaling and LSTM- Real-NVP respectively. - in CSDI is out of memory.\nSOLAR ELECTRICITY\nTRAFFIC TAXI WIKIPEDIA\nStrategy\nC-S MSE C-S MSE C-S MSE C-S MSE C-S MSE\nϵθ 0.528(.006) 1.4e3\n0.044(.007) 3.0e5 0.074(.012) 6.4e-4 0.218(.012) 3.2e 0.079(.010) 4.1e7\nxθ 0.253(.002) 7.7e2 0.021(.001) 1.6e5 0.040(.000) 4.1e-4 0.125(.007) 2.2e 0.061(.002) 2.92e7\nTable 2: The Test set CRPS-sum(C-S) and MSE comparison(lower is better) of models from ϵθ strategy and xθ denoising\nstrategy.\nSolar Electricity\nH 24\n48 24 48\nTimeGrad 104.51(.73)\n203.16(.90) 302.61(.35) 615.02(.06)\nSSSD 80.36(.28) 132.39(.71)\n176.23(.82) 295.75(.54)\nCSDI 92.23(.53) 147.52(.17)\n203.52(.74) 314.23(.76)\nD3VAE 87.53(.29)\n153.43(.11) 198.61(.19) 304.76(.82)\nLDT 13.72(.25) 14.03(.37) 22.13(.14) 25.29(.18)\nTable 3: multivariate datasets Solar and Electricity with two\ndifferent forecasting lengths H=(24, 48).\nSolar Electricity\nTraffic\nMethod C-S\nMSE C-S MSE C-S MSE\nLDT-g 0.301(.001)\n8.9e2 0.024(.000) 2.1e5 0.050(.003) 4.3e-4\nLDT-c 0.264(.004)\n8.0e2 0.023(.003) 1.8e5 0.047(.004) 4.5e-4\nLDT 0.253(.002) 7.7e2 0.021(.001) 1.6e5 0.040(.000) 4.1e-4\nTable 4: The Test set CRPS-sum(C-S) and MSE comparison\n(lower is better) of models in Ablation studies and our model\nLDT.\nure 2, for the Solar dataset, although the data had a strong pe-\nriodicity, there were great differences in both the numerical\namplitude periodicity change and the length of periodicity\nin the dataset that will cause two similar look-back window\ndata to result in two different forecasting targets. Also, in\nthe Taxi dataset which is with high stochasticity, we found\nthat the estimated uncertainty grows rapidly when extreme\nvalues are encountered in our model.\nDeterministic Estimation In addition to the aforemen-\ntioned uncertainty estimation approach, our work has re-\nvealed that our model exhibits deterministic estimation out-\ncomes when applied to datasets with limited extreme vari-\nations, such as Electricity and Traffic. Our one-shot LDT\ncan predict relatively stationary high-dimensional time se-\nries more accurately, as shown in Fig. 3. We find that the\nchange of guidance strength in our model will affect the per-\nformance of deterministic prediction, which is also shown\nin Appendix D. We observed that in datasets with determin-\nistic predictions like Electricity and Traffic, a larger guid-\nance w will yield better results, whereas lower guidance w\nis better in Solar and Taxi. This shows that our model can\nadapt to different forecasting scenarios by adjusting guid-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11984\nFigure 2: Undeterministic estimation of the prediction of the 8\nsamples in the Solar and Taxi datasets.\nFigure 3: Deterministic estimation of the prediction in the\nElectricity and Traffic datasets.\nFigure 4: Visualizations on Electricity and Traffic by LDT-g,\nLDT-c, and the proposed LDT.\nance strength to achieve deterministic and uncertainty pre-\ndictions for different types of datasets.\nAblation Studies\nIn this section, we study the effectiveness of the proposed\ncomponents in our structure. Three representative multivari-\nate datasets are introduced in Table.1: Solar, Electricity, and\nTraffic, which are non-stationary and high dimensional.\nSelf-Conditioning Guidance Mechanism In this exper-\niment, we study the effectiveness of the self-conditioning\nguidance mechanism which is described in section 4.3. We\nconsider the three different settings to verify the effec-\ntiveness of our module where LDT-g is without the self-\ncondition to train the denoising network, LDT-c is without\nthe guidance and LDT is proposed in our work. Table 4\nshows the results of the two metrics MSE and CRPS-sum\nwith the same guidance strengthw =3.0. To verify the role of\ndifferent parts, we visualized the results of a more complex\nsample generation in the Electricity dataset. As seen in Fig-\nure 4, LDT-g can capture the detailed change pattern of the\nforecasting targets, but there is a deviation in the accuracy of\nthe numerical value. LDT-c can effectively learn to forecast\nthe interval of the numerical change of the future targets, but\nthe details are not fine enough. And our proposed LDT ef-\nfectively combines the advantages of these two factors. The\nguidance factor learns the detailed patterns of the forecast-\ning targets, and the self-condition factor learns to predict the\nnumerical values of the predicted targets.\nPredicting xθ vs. Predicting ϵθ In this experiment, we\nwill discuss the different denoising strategies in our work.\nWe compared two different training strategies on five\ndatasets and Table 2 shows our comparative results. We\nfound that the denoising process shows exactly poor perfor-\nmance in ϵθ strategy, but we found that the autoregressive\ndiffusion-based method like TimeGrad can use ϵθ as the de-\nnoising target. We believe that there are two reasons for this\nresult, (1) In the non-autoregressive condition, the target is\nset to noise, which makes the model ignore the correlation\nbetween timestamps. (2) The time series usually contains\nhighly nonlinear noise, which can be easily confused with\nthe noise generated from the diffusion process.\nInference Efficiency In this experiment, we compare the\ninference efficiency of the proposed LDT with the other time\nseries diffusion model baselines TimeGrad, SSSD, CSDI\nand D3V AE. Table 3 shows the inference time on the multi-\nvariate datasets Solar and Electricity with two different fore-\ncasting lengths (24, 48). In terms of generation efficiency,\nour one-shot latent structure LDT performs well.\nConclusions\nIn this study, we introduce a multivariate probabilistic time\nseries forecasting approach that leverages latent space rep-\nresentations. Our method incorporates a self-conditioning\nguidance mechanism, which combines self-condition bias\nwith condition-based guidance to enhance the denoising\nprocess in our latent diffusion model. Furthermore, we\ndevelop a one-shot, non-autoregressive Latent Diffusion\nTransformer (LDT) for high-dimensional multivariate time\nseries prediction. Evaluation of our LDT model on five stan-\ndard time-series benchmarks sets a new benchmark, outper-\nforming existing generative methods. Ablation studies vali-\ndate the contribution of each component within our model.\nWe aim to refine denoising structures further for modeling\nhigh-dimensional multivariate time series.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11985\nAcknowledgements\nThis research was supported, in part, by the National Re-\nsearch Foundation, Prime Minister’s Office, Singapore un-\nder its AI Singapore Programme (AISG Award No: AISG-\nGC-2019-003) and under its NRF Investigatorship Pro-\ngramme (NRFI Award No. NRF-NRFI05-2019-0002). Any\nopinions, findings conclusions, or recommendations ex-\npressed in this material are those of the authors and do not\nreflect the views of the National Research Foundation, Sin-\ngapore. Moreover, the authors greatly appreciate the review-\ners’ suggestions and the editor’s encouragement.\nReferences\nAlcaraz, J. M. L.; and Strodthoff, N. 2022. Diffusion-based\ntime series imputation and forecasting with structured state\nspace models. arXiv preprint arXiv:2208.09399.\nAlexandrov, A.; Benidis, K.; Bohlke-Schneider, M.;\nFlunkert, V .; Gasthaus, J.; Januschowski, T.; Maddix, D. C.;\nRangapuram, S.; Salinas, D.; Schulz, J.; et al. 2020. Gluonts:\nProbabilistic and neural time series modeling in python.The\nJournal of Machine Learning Research, 21(1): 4629–4634.\nCao, D.; Wang, Y .; Duan, J.; Zhang, C.; Zhu, X.; Huang,\nC.; Tong, Y .; Xu, B.; Bai, J.; Tong, J.; et al. 2020. Spectral\ntemporal graph neural network for multivariate time-series\nforecasting. Advances in neural information processing sys-\ntems, 33: 17766–17778.\nChen, T.; Zhang, R.; and Hinton, G. 2022. Analog bits:\nGenerating discrete data using diffusion models with self-\nconditioning. arXiv preprint arXiv:2208.04202.\nDing, Q.; Wu, S.; Sun, H.; Guo, J.; and Guo, J. 2020. Hier-\narchical Multi-Scale Gaussian Transformer for Stock Move-\nment Prediction. In IJCAI, 4640–4646.\nFan, W.; Wang, P.; Wang, D.; Wang, D.; Zhou, Y .; and Fu, Y .\n2023. Dish-TS: A General Paradigm for Alleviating Distri-\nbution Shift in Time Series Forecasting. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 37,\n7522–7529.\nFeng, S.; Miao, C.; Xu, K.; Wu, J.; Wu, P.; Zhang, Y .; and\nZhao, P. 2023. Multi-scale attention flow for probabilistic\ntime series forecasting. IEEE Transactions on Knowledge\nand Data Engineering.\nHo, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion\nprobabilistic models. Advances in neural information pro-\ncessing systems, 33: 6840–6851.\nHo, J.; and Salimans, T. 2022. Classifier-free diffusion guid-\nance. arXiv preprint arXiv:2207.12598.\nHuang, R.; Lam, M. W.; Wang, J.; Su, D.; Yu, D.; Ren,\nY .; and Zhao, Z. 2022. Fastdiff: A fast conditional diffu-\nsion model for high-quality speech synthesis. arXiv preprint\narXiv:2204.09934.\nJordan, A.; Kr ¨uger, F.; and Lerch, S. 2017. Evaluating\nprobabilistic forecasts with scoringRules. arXiv preprint\narXiv:1709.04743.\nKim, T.; Kim, J.; Tae, Y .; Park, C.; Choi, J.-H.; and Choo, J.\n2021. Reversible instance normalization for accurate time-\nseries forecasting against distribution shift. In International\nConference on Learning Representations.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKrishnan, R.; Shalit, U.; and Sontag, D. 2017. Structured\ninference networks for nonlinear state space models. InPro-\nceedings of the AAAI Conference on Artificial Intelligence ,\nvolume 31.\nLai, G.; Chang, W.-C.; Yang, Y .; and Liu, H. 2018. Modeling\nlong-and short-term temporal patterns with deep neural net-\nworks. In The 41st international ACM SIGIR conference on\nresearch & development in information retrieval, 95–104.\nLi, X.; Thickstun, J.; Gulrajani, I.; Liang, P. S.; and\nHashimoto, T. B. 2022a. Diffusion-lm improves controllable\ntext generation. Advances in Neural Information Processing\nSystems, 35: 4328–4343.\nLi, Y .; Lu, X.; Wang, Y .; and Dou, D. 2022b. Generative\ntime series forecasting with diffusion, denoise, and disentan-\nglement. Advances in Neural Information Processing Sys-\ntems, 35: 23009–23022.\nLiu, C.; Hoi, S. C.; Zhao, P.; and Sun, J. 2016. Online arima\nalgorithms for time series prediction. In Proceedings of the\nAAAI conference on artificial intelligence, volume 30.\nLiu, Y .; Wu, H.; Wang, J.; and Long, M. 2022. Non-\nstationary transformers: Exploring the stationarity in time\nseries forecasting. Advances in Neural Information Process-\ning Systems, 35: 9881–9893.\nMatheson, J. E.; and Winkler, R. L. 1976. Scoring rules for\ncontinuous probability distributions. Management science,\n22(10): 1087–1096.\nMin, E.; Chen, R.; Bian, Y .; Xu, T.; Zhao, K.; Huang, W.;\nZhao, P.; Huang, J.; Ananiadou, S.; and Rong, Y . 2022a.\nTransformer for graphs: An overview from architecture per-\nspective. arXiv preprint arXiv:2202.08455.\nMin, E.; Rong, Y .; Xu, T.; Bian, Y .; Luo, D.; Lin, K.; Huang,\nJ.; Ananiadou, S.; and Zhao, P. 2022b. Neighbour interac-\ntion based click-through rate prediction via graph-masked\ntransformer. In Proceedings of the 45th International ACM\nSIGIR Conference on Research and Development in Infor-\nmation Retrieval, 353–362.\nNguyen, N.; and Quanz, B. 2021. Temporal latent auto-\nencoder: A method for probabilistic multivariate time series\nforecasting. In Proceedings of the AAAI Conference on Ar-\ntificial Intelligence, volume 35, 9117–9125.\nPark, T.; Liu, M.-Y .; Wang, T.-C.; and Zhu, J.-Y . 2019. Se-\nmantic image synthesis with spatially-adaptive normaliza-\ntion. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, 2337–2346.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information pro-\ncessing systems, 32.\nRao, H.; Hu, X.; Cheng, J.; and Hu, B. 2021. SM-SGE: A\nself-supervised multi-scale skeleton graph encoding frame-\nwork for person re-identification. In Proceedings of the 29th\nACM international conference on Multimedia, 1812–1820.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11986\nRao, H.; and Miao, C. 2022. SimMC: Simple masked\ncontrastive learning of skeleton representations for un-\nsupervised person re-identification. arXiv preprint\narXiv:2204.09826.\nRasul, K.; Seward, C.; Schuster, I.; and V ollgraf, R. 2021.\nAutoregressive denoising diffusion models for multivariate\nprobabilistic time series forecasting. In International Con-\nference on Machine Learning, 8857–8868. PMLR.\nRasul, K.; Sheikh, A.-S.; Schuster, I.; Bergmann, U.; and\nV ollgraf, R. 2020. Multivariate probabilistic time series fore-\ncasting via conditioned normalizing flows. arXiv preprint\narXiv:2002.06103.\nRoberts, S.; Osborne, M.; Ebden, M.; Reece, S.; Gibson, N.;\nand Aigrain, S. 2013. Gaussian processes for time-series\nmodelling. Philosophical Transactions of the Royal Soci-\nety A: Mathematical, Physical and Engineering Sciences ,\n371(1984): 20110550.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 10684–\n10695.\nRuan, L.; Ma, Y .; Yang, H.; He, H.; Liu, B.; Fu, J.; Yuan,\nN. J.; Jin, Q.; and Guo, B. 2023. Mm-diffusion: Learning\nmulti-modal diffusion models for joint audio and video gen-\neration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 10219–10228.\nSalinas, D.; Bohlke-Schneider, M.; Callot, L.; Medico, R.;\nand Gasthaus, J. 2019. High-dimensional multivariate fore-\ncasting with low-rank gaussian copula processes. Advances\nin neural information processing systems, 32.\nSezer, O. B.; Gudelek, M. U.; and Ozbayoglu, A. M. 2020.\nFinancial time series forecasting with deep learning: A sys-\ntematic literature review: 2005–2019. Applied soft comput-\ning, 90: 106181.\nShen, L.; and Kwok, J. 2023. Non-autoregressive Condi-\ntional Diffusion Models for Time Series Prediction. arXiv\npreprint arXiv:2306.05043.\nTakagi, Y .; and Nishimoto, S. 2023. High-resolution im-\nage reconstruction with latent diffusion models from human\nbrain activity. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 14453–14463.\nTashiro, Y .; Song, J.; Song, Y .; and Ermon, S. 2021. Csdi:\nConditional score-based diffusion models for probabilistic\ntime series imputation. Advances in Neural Information\nProcessing Systems, 34: 24804–24816.\nUlyanov, D.; Vedaldi, A.; and Lempitsky, V . 2016. Instance\nnormalization: The missing ingredient for fast stylization.\narXiv preprint arXiv:1607.08022.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWoo, G.; Liu, C.; Sahoo, D.; Kumar, A.; and Hoi, S. 2022.\nEtsformer: Exponential smoothing transformers for time-\nseries forecasting. arXiv preprint arXiv:2202.01381.\nWu, S.; Xiao, X.; Ding, Q.; Zhao, P.; Wei, Y .; and Huang,\nJ. 2020. Adversarial sparse transformer for time series fore-\ncasting. Advances in neural information processing systems,\n33: 17105–17115.\nXu, K.; Zhang, Y .; Ye, D.; Zhao, P.; and Tan, M. 2021.\nRelation-aware transformer for portfolio policy learning. In\nProceedings of the twenty-ninth international conference\non international joint conferences on artificial intelligence ,\n4647–4653.\nYang, L.; Zhang, Z.; Song, Y .; Hong, S.; Xu, R.; Zhao, Y .;\nShao, Y .; Zhang, W.; Cui, B.; and Yang, M.-H. 2022. Dif-\nfusion models: A comprehensive survey of methods and ap-\nplications. arXiv preprint arXiv:2209.00796.\nYi, Y .; Wan, X.; Bian, Y .; Ou-Yang, L.; and Zhao, P. 2023.\nETDock: A Novel Equivariant Transformer for Protein-\nLigand Docking. arXiv preprint arXiv:2310.08061.\nYuan, H.; Yuan, Z.; Tan, C.; Huang, F.; and Huang, S.\n2022. Seqdiffuseq: Text diffusion with encoder-decoder\ntransformers. arXiv preprint arXiv:2212.10325.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11987",
  "topic": "Probabilistic logic",
  "concepts": [
    {
      "name": "Probabilistic logic",
      "score": 0.6699213981628418
    },
    {
      "name": "Series (stratigraphy)",
      "score": 0.529301643371582
    },
    {
      "name": "Time series",
      "score": 0.4675983786582947
    },
    {
      "name": "Econometrics",
      "score": 0.4331636130809784
    },
    {
      "name": "Transformer",
      "score": 0.4230876564979553
    },
    {
      "name": "Computer science",
      "score": 0.38923248648643494
    },
    {
      "name": "Mathematics",
      "score": 0.27741122245788574
    },
    {
      "name": "Artificial intelligence",
      "score": 0.21423128247261047
    },
    {
      "name": "Machine learning",
      "score": 0.18477985262870789
    },
    {
      "name": "Engineering",
      "score": 0.15566101670265198
    },
    {
      "name": "Geology",
      "score": 0.1405102014541626
    },
    {
      "name": "Electrical engineering",
      "score": 0.10417652130126953
    },
    {
      "name": "Voltage",
      "score": 0.053996503353118896
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210092122",
      "name": "BC Research (Canada)",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ],
  "cited_by": 21
}