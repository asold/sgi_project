{
  "title": "Geographic and Geopolitical Biases of Language Models",
  "url": "https://openalex.org/W4389519241",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101827902",
      "name": "Fahim Faisal",
      "affiliations": [
        "George Mason University"
      ]
    },
    {
      "id": "https://openalex.org/A5013793053",
      "name": "Antonios Anastasopoulos",
      "affiliations": [
        "George Mason University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4224247062",
    "https://openalex.org/W4285147794",
    "https://openalex.org/W4285302800",
    "https://openalex.org/W4302774985",
    "https://openalex.org/W4285263440",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4377823139",
    "https://openalex.org/W4385567345",
    "https://openalex.org/W4287887365",
    "https://openalex.org/W4394871652",
    "https://openalex.org/W1989513516",
    "https://openalex.org/W4221159394",
    "https://openalex.org/W4206285331",
    "https://openalex.org/W4385572100",
    "https://openalex.org/W3204432444",
    "https://openalex.org/W4206636317",
    "https://openalex.org/W4387356388",
    "https://openalex.org/W4385571357",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4286949750",
    "https://openalex.org/W3174805488",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3201090304",
    "https://openalex.org/W3099768174"
  ],
  "abstract": "Pretrained language models (PLMs) often fail to fairly represent target users from certain world regions because of the underrepresentation of those regions in training datasets.With recent PLMs trained on enormous data sources, quantifying their potential biases is difficult, due to their black-box nature and the sheer scale of the data sources.In this work, we devise an approach to study the geographic bias (and knowledge) present in PLMs, proposing a Geographic-Representation Probing Framework adopting a self-conditioning method coupled with entity-country mappings.Our findings suggest PLMs' representations map surprisingly well to the physical world in terms of country-to-country associations, but this knowledge is unequally shared across languages.Last, we explain how large PLMs despite exhibiting notions of geographical proximity, over-amplify geopolitical favouritism at inference time.",
  "full_text": "Proceedings of the The 3rd Workshop on Multi-lingual Representation Learning (MRL), pages 139–163\nDecember 7, 2023 ©2023 Association for Computational Linguistics\nGeographic and Geopolitical Biases of Language Models\nFahim Faisal, Antonios Anastasopoulos\nDepartment of Computer Science, George Mason University\n{ffaisal,antonis}@gmu.edu\nAbstract\nPretrained language models (PLMs) often\nfail to fairly represent target users from cer-\ntain world regions because of the under-\nrepresentation of those regions in training\ndatasets. With recent PLMs trained on enor-\nmous data sources, quantifying their potential\nbiases is difficult, due to their black-box nature\nand the sheer scale of the data sources. In this\nwork, we devise an approach to study the geo-\ngraphic bias (and knowledge) present in PLMs,\nproposing a Geographic-Representation Prob-\ning Framework adopting a self-conditioning\nmethod coupled with entity-country mappings.\nOur findings suggest PLMs’ representations\nmap surprisingly well to the physical world in\nterms of country-to-country associations, but\nthis knowledge is unequally shared across lan-\nguages. Last, we explain how large PLMs de-\nspite exhibiting notions of geographical prox-\nimity, over-amplify geopolitical favouritism at\ninference time.1\n1 Introduction\nLarge pretrained language models (PLMs) are ca-\npable of generating meaningful texts beyond En-\nglish and very likely, models like GPT-4, Llama\n2 (Brown et al., 2020; Shliazhko et al., 2022;\nZhang et al., 2022; Workshop et al., 2023; OpenAI,\n2023; Touvron et al., 2023) will form the go-to\nbase model for automating tasks like summarizing\ntexts, generating datasets given certain instructions\n(Schick and Schütze, 2021) or perhaps even evalu-\nating the generated texts (Yuan et al., 2021). While\nthese PLMs continue to expand their utility, it is\ncrucial that one also examines the potential biases\nthat these PLMs exhibit. Moreover, the utility of\nthese PLMs should be equitable to their target users\nso that they perform evenly for all speakers of the\nlanguages it is primarily trained on. Otherwise,\nthe disparity that lies in the model (if any) will\n1Code and data are publicly available: https://github.\ncom/ffaisal93/geoloc_lm\nFigure 1: Example of a Geographic Representation net-\nwork and it’s corresponding location clusters (colored)\nrecovered from the top-50 country-\"expert\" neurons of\nBLOOM. Notice that connected countries are either ge-\nographically or culturally close (e.g. south American\ncluster in light blue, African countries in yellow, South-\nEast Asian countries in dark blue). Note: node size is\nproportional to its degree in the graph.\npropagate further. To better illustrate these dynam-\nics, consider a L1 Spanish speaker from Peru, who\nis using a prompt-based PLM (like that of Wang\net al. (2022, 2021)) to generate a localized synthetic\ndataset for some downstream task. They may use\nSpanish as used in the local context to form their\nseed data/prefix/prompts. Now, if this language\nmodel has already skewed preferences towards\ngeopolitically dominant countries, it is likely the\ngenerated texts will reflect the skewness, thus not\nappropriately reflecting the local, Peruvian context\nthat the practitioner is interested in. However, the\nquantification of this presumed geographic dispar-\nity in PLMs is not yet explored. Though given the\nwell-documented western-country bias (or Global\nNorth bias) exhibited in most NLP benchmarks and\n139\ndatasets (Faisal et al., 2022, inter alia), we hypoth-\nesize that text generation models might also suffer\nfrom the similar pitfall. On top of that, given a\nmultilingual model, how language variety impact\nthe encoded geographic knowledge is also under-\nexplored.\nHerein, we perform an evidence-based study to\nunfold the underlying geographic distribution of\nmultilingual PLMs. We propose a pipeline to probe\nthe Text-Generative PLMs using prompt-based in-\nference for Geographic-Knowledge as well as ex-\nisting domain-variant disparity (geography in our\ncase). Our research questions and key findings are:\n• RQ1: To what extent is geographic proximity\nencoded in the PLMs? F: PLMs can infer geo-\ngraphic proximity surprisingly well in terms of\ncountry-country association (see Figure 1). How-\never, we observe an over-representation of certain\ncountries during text generation.\n• RQ2: What is the influence of multilinguality\nin PLM’s knowledge distribution of geographic\nproximity? F: The shared multilingual represen-\ntation space of PLMs has an uneven distribution\nof knowledge across languages.\n• RQ3: What is the effect of prompting using a\ngeographic identifier (eg. \"In Colombia\" <gen-\nerate text>) on multilingual text generation? F:\nPrompting with certain geographic identifiers can\neven alter the language of free-form generated\ntext.\n2 Background and Related Work\nA substantial amount of work has investigated ex-\nisting social bias (eg. gender, racial, ethnic, occupa-\ntional) identification and mitigation approaches in\nPLMs including, reducing token sensitivity during\ntext generation (Liang et al., 2021), investigating\nmodel sensitivity (Immer et al., 2022), prompting\nusing natural sentences (Alnegheimish et al., 2022)\nand probing via embedding lookup (Ahn and Oh,\n2021). On the other hand, representing space and\ntime utilizing maps and language is a long-standing\ndomain of research (Louwerse and Benesh, 2012;\nGatti et al., 2022; Anceresi et al., 2023). More re-\ncently, numerous studies are experimenting with\ngeoadaptation of PLMs (Hofmann et al., 2023),\nwhat behavior these PLMs exhibit while probing\nwith geographic-context, cultural-commonsense as\nwell as temporal reasoning (Yin et al., 2022; Ghosh\net al., 2021; Thapliyal et al., 2022; Hlavnova and\nRuder, 2023; Shwartz, 2022; Tan et al., 2023) or\nhow large PLMs learn the representation of space\nand time (Gurnee and Tegmark, 2023). However,\nfor our goal task, first, we need to identify spe-\ncific model units sensitive to certain geographic\nconcepts. Then we would like to prioritize those\nunits to generate output text for evaluation. A self-\nconditioning pre-trained model (Suau et al., 2022)\nis one such approach enabling us to perform the\nrequired experiments.\nSelf-conditioning Method Suau et al. (2022)\npropose an approach that extracts PLM weights\nhaving certain polarity and then prioritize those\nweights during text generation. Based on the gener-\nated text, they can quantify gender and occupation\nbias encoded by the PLM. As an example, consider\na binary sentence classification task where positive\nclass examples contain the mention of a concept\nword (eg. doctor) and vice-versa. A PLM is able\nto provide scores to these positive and negative ex-\namples. Looking at the average precision scores\nand the scores given by different model weights\nfrom each layer, we can identify the ones providing\nhigher scores towards the positive examples. Suau\net al. (2022) refer to these model weights as expert\nunits.\nNow, we can prioritize these identified expert\nunits during text generation by artificially simu-\nlating the presence of the concept word \"doctor\"\nin the input. Basically, at every step of text gen-\neration, we replace the actual response of expert\nunits with the typical one where the concept word\nis present in the input. As a result, the PLM now\ngenerates texts relevant to the concept word. In the\nwork of Suau et al. (2022), by comparing the gen-\nerated texts, they easily quantify the presence of\ngender-specific words thus evaluating the presence\nof gender bias in the PLM (for example, consider\nthe number of sentences where the context relates\nto the word \"doctor\" and mentions male-gender\nwords compared to female-gender words). This\napproach serves two main purposes: (1) Identi-\nfying expert units: model parameters responsible\nfor generating text related to the target concept\n(i.e. doctor). (2) Triggering specific behaviour in\ntext generation without explicit mentioning of the\ntarget context, which inadvertently influences the\nbehaviour of the model.\n3 Geographic Representation Probing\nIn our study, we use this Self Conditioning Method\nto first extract expert units (i.e. model weights)\n140\nFigure 2: Geographic Representation Probing Framework. First we construct the Country/Concept dataset. Then we\nextract Expert Units from the base PLM and use similarity measurement to prepare our Geographic Representation\nNetwork to perform Intrinsic Probing. In Parallel, we prompt the self-conditioned PLM with Geographic Identifiers\n(i.e. Country/Prefix). Finally, we map the generated-text entities to countries to perform Extrinsic Probing.\nwhich encode geographic knowledge. Then we use\nthose units to generate relevant texts given different\ngeographic identifier-based prompts. An example:\nUsing some sentences with the mention as well as\nabsence of the word \"China\" to extract expert units\nand then, prioritize these units during text gener-\nation with the prompt \"In USA ...\". The aim here\nis to simulate an environment where we evaluate\nthe model knowledge (Concept-Country-specific\nExpert Units) by asking what it knows about other\ncountries (i.e Prefix-Country). This allows us to\nquantify existing geographic bias towards certain\nattributes present in a PLM. Our probing frame-\nwork contains five steps (see Figure 2): (1) Concept\nDataset Construction (2) Expert Unit Extraction (3)\nGeographic-Representation Network Construction\n(4) Prompt-based Text Generation (5) Entity Coun-\ntry Mapping.\nConcept Dataset Construction First of all, we\nprepare our concept dataset in a binary classi-\nfication fashion using which, we later perform\nself-conditioning a PLM on geographic concepts.\nTo make it quantifiable, we define country to be\nour main unit of reference and construct concept\ndatasets where each \"concept\" is loosely centered\naround a country. An additional requirement for\nthese datasets is that the data have not been used\nas part of the pretraining data of the PLMs. Hence,\nwe turn to recent news articles (scrapped using\nGoogle news api2): as we can control the date on\nwhich these data became public, we can be sure\n2https://github.com/ranahaani/GNews\nthat they were not used in any pre-training process\n(so far). Such a dataset should also allow us to get\na reasonable representation of current geopolitical\naffairs. Depending on the news-source country and\nlanguage, we build several such Concept-Country\ndatasets. A Concept-Country dataset {C}-{l} con-\ntains news about several (c1, c2, ..ci..cn) countries\nin {l} language where the news-source is{C} coun-\ntry. Each Concept-Country ci has 100 positive\nexamples (mention of ci) sentences and 300 nega-\ntive examples (no mention) sentences. For exam-\nple, USA-eng Concept-Country dataset (Figure 7)\ncontains data from US sources, in English, which\neither mention other countries (there are 100 pos-\nitive examples for each country ci) or are random\nsentences not mentioning any countries (negative\nexamples). See App. C for the constructed dataset\ndetails with examples.\nExpert Unit Extraction Using the self-\nconditioning method, we identify high performing\nExpert Units for each Concept-Country. These\nare the model weights that provide higher\nscores for the presence of a specific concept (i.e.\ncountry in our case). For example, Consider\nthe Concept-Country India from the dataset\nUSA-eng. Essentially, we have positive examples\n(text mentioning India or relevant entities) and\nnegative examples (random other sentences not\nmentioning India) which we can use to identify\nthe model’s Expert Units . These units are the\nneurons that can be used as predictors to identify\nthe presence of a concept (i.e. positive examples\nmentioning \"India\"). The self-conditioning\n141\nFigure 3: Prefix construction using Multilingual Prefix-\nTemplates. Here we replace the <country> position\nwith \"Spain\" in the given language. Complete list of\nmultilingual prefix templates in Appendix D.\nframework computes these neurons and uses the\naverage-precision score to rank their predictive\nexpertise thus allowing us to select the top- k (eg.\n10, 50) Expert Units from each layer. Observing\nthe average precision scores, we select the top- k\n(eg. 10, 50) Expert Units from each PLM layer.\nA comprehensive theoretical explanation of the\nself-conditioning method and the Expert Unit\nextraction process is presented in App. B.\nGeographic-Representation Network Now uti-\nlizing all these model Expert Units, we construct\nour Geographic-Representation Networks. We use\njaccard similarity to measure the similarity between\nany given Concept-Country pairs ci and cj and\ntheir corresponding Expert Units. Then, utilizing\nthese similarity measurement scores as edges in\na graph (the countries being the nodes), we pre-\npare a PLM-specific Geographic Representation\nnetwork for each of our Expert Units set. This\nnetwork is a Minimum-Spanning Tree graph high-\nlighting the internal country-country associations.\nWe further make it easier to digest by identifying\nthe community clusters of countries using the Lou-\nvain Community Detection method (Blondel et al.,\n2008). In Figure 1 we show the network obtained\nwith the USA-eng dataset from the BLOOM (Work-\nshop et al., 2023) Expert Units. Effectively, we can\nrecover a very good geographical representation of\nthe countries straight from the network weights.\nPrompt-based Text Generation With the\nConcept-Country-specific Expert Units at hand,\nwe can now investigate what happens when\nwe use the PLM for text generation. The self-\nconditioning method (Suau et al., 2022) uses\nsequential decoding and prioritize the Expert\nUnits by approximating their scores from the\naverage precision values predicted for a certain\nConcept-Country. This allows us to artificially\nsimulate the presence of a country name and\nit’s related context during text generation. Now\nwe perform text generation with one more twist:\nwe provide one country-mention as part of the\nprefix/prompt (i.e. Prefix-Country). The idea here\nis to simulate an environment where we evaluate\nthe model knowledge (Concept-Country-specific\nExpert Units ) by asking what it knows about\nother countries (i.e Prefix-Country). We generate\nseveral template-based multilingual prompts (the\nprefix construction process is depicted in Table 3)\nwhere we replace the <country> tag with different\ncountry names.\nEntity Country Mapping Finally, to investigate\nthe existence of geopolitical favouritism, we quan-\ntify the geographic biases of the generated texts\nby mapping any entities appearing in the text to\ncorresponding countries. We use the Dataset Ge-\nography framework of Faisal et al. (2022), which\nuses multilingual entity linking to map entities to\nWikidata entries and then to countries.\n4 Experimental Settings\nTerminologies Based on our Framework descrip-\ntion, let us list some terminologies that we use for\nthe remainder of the paper, to describe the experi-\nmental settings and results.\n1. Concept-Country: These are the countries for\nwhich we collect news.\n2. Source Country: These are the country of ori-\ngin from where the news data is produced.\n3. Prefix: This is the text that we use to prompt the\nmodel, which may include a country mention.\nThis country is the Prefix-Country.\n4. Expert Units: The model units that are specific\nto a country concept ci and are extracted from\nthe language models.\nModels and Languages We use GPT2-\nmedium (Radford et al., 2019), mGPT (Shliazhko\net al., 2022) and BLOOM-560m (Workshop et al.,\n2023), all models available through huggingface.\nFor the English dataset sourced from the US-News\nPlatform (USA-eng) we extract Expert Units from\nall three models. For non-English datasets, we\nperform Expert Units extraction on BLOOM and\nmGPT. For the generation-level analysis step, we\nuse BLOOM and GPT2 (focusing on English)\nexpert units and report results for conditioning\nConcept-Country datasets in 8 languages: (ara,\n142\nben, eng, fra, hin, kor, rus, zho).\nDatasets As mentioned before, each concept in\nour dataset contains 100 positive and 300 nega-\ntive examples. In some cases, we use up-sampling\nby repeating the example sentences multiple times\nwhen we do not have 100 distinct examples men-\ntioning the Concept-Country name. In total, we\nprepare 31 Concept-Country Datasets (22 Country\nNews-Sources, 13 Languages) and extract expert\nunits conditioning over these datasets. Detailed\ndataset statistics are in Appendix Table C.3.\nGenerative Scheme: On average we generate\n112,225 sentences for a given model and Concept-\nCountry Dataset. For 67 Concept-Country Expert\nUnits, we randomly choose 5 prefix templates; re-\nplace those with all 67 country name and generate\n5 sentences with the lowest perplexity per Prefix-\nCountry; thus 67x5x67x5=112,225 sentences.\nProbing Metrics We analyze both the Geo-\ngraphic Representation Networks (intrinsic/param-\neter probing) and the generated texts (extrinsic/gen-\neration probing) to answer our Research Questions\nwhere we utilize the aid of visualization and three\nadditional quantitative metrics as follows:\n1. Neighbourhood Score: We propose a proximity-\nbased metric to quantify the inherent encoding of\nGeographic Proximity present inside an LM by\nlooking at the country-country associations and\ncompare them with the physical world. For ex-\nample, in Figure 1, South-American neighbouring\ncountries are clustered together thus preserving a\nfactually consistent representation. To capture this,\nwe compute the number of neighbours one country\nnode is connected within a 2-hop distance given\na Geographic-Representation Network. To better\nillustrate, consider in a Geographic-Representation\nNetwork G, country node c5 ∈G is connected\nwith 4 other country nodes {c1, c2, c3, c4} ∈G.\nAmong these 4 connected nodes, c5 shares sea or\nland borders with only 2 countries N5 = {c2, c3}\nin real world thus making |N5|= 2. Similarly,\nwe can compute |N2|and |N3|for countries c2\nand c3 respectively. So, the Neighbourhood Score\nns(c5) =|N5|+ |N2|+ |N3|which we can gener-\nalize and aggregate at the network level as follows:\nNs(G) =\n∑\nci∈G\nns(ci)\n=\n∑\nci∈G\n(|Ni|+\n∑\nj∈Ni\n|Nj|)\n2. Representation Score : We quantify the over-\nall command of prefix, concept or top-represented\ncountries at the language level (i.e. for all gener-\nated text in a language). Consider we have Expert\nUnits already computed for Concept-Country ci.\nWe use these units to generate text while providing\na Prefix-Country pj. Later, we map the entities of\ngenerated text to countries. So if we have a total of\nL = {l1, l2, ..lk..ln}countries with respective en-\ntity counts, we can get the top represented countries\nT(ci, pj) for each concept-prefix pair (ci, pj):\nT(ci, pj) = arg max\nlk∈L\n(P(lk|ci, pj))\nHaving this set of highly represented countries for\neach concept-prefix pair at hand, we can now com-\npute in how many cases a Concept-Country, Prefix-\nCountry or the top-10 most represented countries\nare present in the set T(ci, pj) for all ci ∈ N,\npj ∈M where N= {Concept Countries}, M=\n{Prefix Countries}. So given one output-country-\ndistribution B:\nRS(B, x) =\n∑\nci∈N\n∑\npj ∈M\n|T(ci, pj) ∈Ax|where\nAx = {prefix pj, concept ci or top-10 country}\nThe intuition here is to quantify how much the influ-\nence of Concept-Country, Prefix-Country or overly\nrepresented countries varies across languages. For\nexample, if we observe that the score for Prefix-\nCountry is higher than the scores for Concept-\nCountry across all settings, it meansPrefix-Country\nis a more influencing factor than Concept-Country\nin the geographical relatedness of the text genera-\ntion. For comparative analysis, we consider top-3\nrepresented countries instead of just one while com-\nputing T(ci, pj) ∈Ax.\n3. Skewness 3: We compare the symmetry of the\ngenerated country-entity distribution for both gen-\nerated and the concept dataset texts. The ones that\nare more skewed one the ones containing amplified\nbias towards certain country-origin entities.\n5 Findings\nRQ1: To what extent the geographic proximity is\nencoded in the PLMs?\nIntrinsic Findings: Based on our analysis of\nthe Geographic-Representation Networks, it is ev-\nident that model parameters respond similarly for\n3https://docs.scipy.org/doc/scipy/reference/\ngenerated/scipy.stats.skew.html\n143\nGeographical Closeness present in Model Units\nFigure 4: (a) The variation of neighbourhood score for different set of expert units. Notice at (a.1) we get the\nbest score for USA-eng and it decreases when we translate the concept dataset. This also varies across languages,\nmodels (a.2) and the precise identification of expert units using high-quality concept-dataset also matters (a.3).\nclosely related (culturally or geographically) coun-\ntries. For example, consider the Network in Fig-\nure 1 from BLOOM Expert Units conditioned us-\ning the USA-eng Concept-Country dataset. The\nLatin-American, African and European blocks are\nfairly clear. The Indian Subcontinent countries\n(BGD, PAK, IND), or countries of the British Com-\nmonwealth (AUS, NZL, CAN) are also clustered\ntogether. In addition, from the communities iden-\ntified with the Louvain Community Detection al-\ngorithm, as visualized in the world map plot, we\nobserve that community clusters are mainly formed\naround countries with proximity. We prepare simi-\nlar kinds of Geographic-Representation Networks\nfor all sets of Expert Units conditioned on different\nConcept-Country datasets (see Appendix E).\nConcept Genareted Expert Units\ngpt2 bloom gpt2 mgpt bloom\nUSA USA USA SRB SWE SWE\nGBR GBR FRA POL HUN HUN\nFRA CHN IND BGR AUT SVN\nCHN IND GBR SVK SVK GRC\nUKR FRA CHN SWE CHN SVK\nRUS CAN RUS PER GRC POL\nDEU RUS JPN LV A POL ARG\nESP AUS KOR HUN SVN COL\nAUS JPN DEU ARG CHL BRA\nJPN ISR ESP TZA TUR TUR\nTable 1: Top represented countries across concepts and\ngenerated text. For BLOOM we aggregate across all\neight languages; GPT-2 is English only. For expert\nunits, we report the countries with the highest degree\nof similarity associations. (The common countries in\nat-least two model settings are in italic font.)\nExtrinsic Findings: Next we investigate\nwhether the encoded geographic proximity gets\nmodified due to geopolitical favouritism by per-\nforming entity-country mapping on a large pool of\ngenerated texts in eight languages (112,255 avg.\nsentences per language). Evidently, we observe a\nstrong presence of geopolitical favouritism which\nwe define as the over-amplification of certain coun-\ntry representation (eg. countries with higher GDP,\ngeopolitical stability, military strength etc). For\ncomparison, we use the distribution of theConcept-\nCountry dataset as it contains the actual news text\nreflecting real-world affairs.\nIn Table 1 (two left sections), we contrast the top\nrepresented countries aggregating the counts from\nall Concept-Country datasets to the ones in the\ngenerated text. All top-10 most represented coun-\ntries in generated texts are present within the top-\n16 ranks of geopolitically significant countries. 4\nThis resemblance of higher geopolitically power-\nful country distribution is visible across all forms\n(Generated text Country Maps in Appendix F).\nHowever, when we compare these top-10 coun-\ntry representations (%) in generated text with the\none from the concept dataset, we observe geopoliti-\ncal favouritism. The result is presented in Figure 6\nwhere in all language country-entity distributions,\nthe top-10 country percentage is always higher\ncompared to real-world news (Figure 6(a)). A sim-\nilar pattern is apparent for the other 7 languages\n(except Korean) in terms of data skewness (Figure\n6(b)). Last, we performed Kolmogorov–Smirnov\nand Shapiro statistical significance tests to ensure\nthat the generated text country distribution follows\na log-normal distribution. The striking fact here\nis, though this distribution contains entity mention\nfrom 246 countries in total, around 11.5% of all\ngenerated entities are from the USA alone. This\nphenomenon can be further quantified using the\nneighbourhood score reported in Figure 4. For ex-\nample, as shown in Figure 4(a), we find that all\n3 models (GPT2, BLOOM, mGPT) Geographic-\nRepresentation Networks built from the English\ndataset conditioned Expert Units have around 50%\nof the countries connected with their real-world\n2-hop neighbours.\nRQ2: What is the influence of multilinguality in\nPLM’s knowledge distribution of geographic prox-\nimity?\n4worldpopulationreview-powerful-countries\n144\nFigure 5: Percentage of generated text (top-3) in differ-\nent language given the Prefix being in another language.\nIntrinsic Findings: By now, we have evidence\nthat Geographic proximity is directly encoded in\nPLMs in the form of shared expert units. So\nhow this knowledge differs across languages? Ide-\nally, multilingual PLMs should provide equitable\nutility for their intended users being consistent\ncross-lingually. To evaluate this, we automatically\ntranslate5 our USA-eng dataset, to avoid any con-\nfounders from news content discrepancies from\nacross the world. This way, the content used for\nidentifying the expert units is thematically and se-\nmantically the same across languages. The re-\nsult, in Figure 4(a), shows noticeable disparities\nin Neighbourhood Score percentages across lan-\nguages in terms of Neighbourhood Scores. When\nwe find Expert Units using Latin-script based\nConcept-Country datasets (English, French), the\nExpert Units make the most of associations among\nclosely related neighbours, while the scores are less\nthan half for Russian, Greek, or Korean in models\nlike mGPT or BLOOM.\nRQ3: What is the effect of prompting with geo-\ngraphic identifier (eg. \"In Colombie\" <generate\ntext>) on multilingual text generation?\nExtrinsic Findings: To answer this question, we\nlook into the language of the generated texts using\nspaCy language identifier6. On average, BLOOM\ngenerates around 5.85% sentences (52k out of our\n898k generated sentences) in a language different\nthan the one of the prefix. This anomaly happens\nmostly in a larger percentage in Russian, Chinese,\nand French (Figure 5). We observe that every lan-\nguage has a specific second language preference\n(i.e. rank:1 in Figure 5) which can ignore the given\nprefix and generate a sentence in that language\n(eg. kor →jap, ben →ara, eng →spa, ara →far,\nzho→kor, rus→bgr, etc). This language preference\n5Using https://translate.google.com/\n6spacy-language-detection\nis not reflexive (eg. kor→jap whereas zho→kor).\nObserving the amount of text generated in dif-\nferent languages, it might seem insignificant at\nfirst sight. However, we need to keep in mind\nthat there is one geographic identifier in the prefix\n(Prefix-Country) as well as given Concept-Country\nunits. So when we look into which concept-prefix\npair usually changes the direction of language, we\nobserve interesting cultural correlations. In Ta-\nble 2, given a Prefix-Country, we show how cer-\ntain country mentions instigate text generation in\na different direction (up to 50% of total generated\ntext, given a prefix-concept pair). This happens\nfrequently when a prefix token is shared among\nthose languages (\"in\" exists both in English and\nSpanish; detailed examples in Appendix G) and\nwhen the country is closely tied with the language.\nFor example, the fra→spa and eng→spa directions\n(French/English prefixes continued in Spanish) in-\nclude country mentions of Cuba, Argentina, Colom-\nbia, or Chile which are all Spanish-speaking coun-\ntries. We hypothesize that the shared representation\nspace of multilingual decoder often ties language\nwith geographic entity thus changing the favoured\ngeneration language.\n5.1 Further Analysis\nData Origin Because we are experimenting with\nreal-world multilingual news data without going\nthrough any extensive data cleaning process, we\nalso need to quantify the dataset-level significance:\nhow does Concept-Country data quality impact the\nidentification of Expert Units?\nThe scrapping method we use for dataset con-\nstruction returns localized news depending on the\nsource location. For example, USA news source\nprovides a higher amount of global news with many\ncountry mentions. On the other hand, a news\nsource from Bangladesh provides news mostly\nabout its close geopolitical neighbours (eg. India,\nand China). Thus, the entity frequency distribution\nof USA-eng and BGD-ben would not be similar.\nIn addition, we have variations in the amount\nof upsampling and the negative instance domain.\nSo in Figures 4(b) and 4(c), we report Neighbour-\nhood Scores for geographic-source varied on non-\nEnglish and English datasets respectively. Like\nbefore, the association knowledge for USA-eng\nsourced Geographic-Representation Network re-\nmains the most truthful. For Spanish news sourced\nfrom different locations (Cuba, Mexico, Peru),\n145\nAmplification, Skewness and Representation Bias in Text Generation\nFigure 6: (a) Compared to the concept dataset which is real-world news text, the generated text always overly\nrepresents the top-represented countries (eg. USA). (b) This is also true for Skewness (except Korean). In (c) we\nplot the representation scores depicting the overall influence of prefixes, concepts or top countries. Top countries\nare over-amplified, irrespective of language. The next dominating factor is prefix but it varies across languages.\nDirection Concept Prefix Direction Concept Prefix\nben→ara LV A PAK fra→spa CHL CUB\neng→spa ARG COL fra→vie AUT VNM\neng→ind IDN KOR fra→por PRT PRT\nzh-cn→ko UGA NZL fra→cat CHL SGP\nrus→bul AUS BGR fra→eng CHL BGD\nrus→eng ETH JPN hin→mar BGR ARE\nTable 2: Given prefix in language A, the LM generates\nin a different language B (A →B), influenced by the\nconcept and prefix countries. These are the cases where\nthe percentage of language change is more than 50%.\nscores are rather similar. Interestingly, the score\ndrops significantly for CHN-zho compared to the\ntranslated USA-zho from Figure 4(a).7\nFor the English dataset sourced from different\ngeographic locations (Figure 4(c)), we get poor\nassociation scores for any other locale except the\nUSA, confirming the fact that the in-domain dis-\ntance between positive and negative examples mat-\nters given a fixed language. To dig in further, we\nperform an ablation study by creating one addi-\ntional augmented English dataset: eng-[M]: By\nMasking Country, Name and Organization entities\nin the USA-eng dataset using Spacy NER. Surpris-\ningly, eng-[M] shows the highest percentage of\ngeographic associations even surpassing the orig-\ninal USA-eng one for mGPT. We conclude that\nsmall semantic incoherence does not hurt the Ex-\npert Units extraction and that more contrastive\npositive-negative class difference (absence of other\nentity types) helps.\nModel Comparison In terms of Neighbourhood\nScore, mGPT Expert Units encode 23.5% more\ngeographic expertise over BLOOM-560m model\non translation datasets (similar text, different lan-\nguage). This improvement is increased 30% when\nwe consider the multilingual datasets (text and lan-\n7While investigating this anomaly, we found that the fixed\nsequence length for both models (BLOOM, mGPT) rejects\nseveral positive examples during tokenization process thus\nhurting the Expert Units extraction quality. We corrected this\nissue by substituting the long examples with shorter ones.\nguage: both different). GPT-2 units perform simi-\nlarly on the English dataset.\nWe conduct another ablation study to quantify\nhow to prune these models towards randomness\nand semantic incoherence. We prepare another\naugmented English dataset eng-[R], by putting ran-\ndom semantically incoherent texts while maintain-\ning the positive-negative class difference. The bar\nshowing the Neighbourhood Score is at Figure 4(c).\nNow BLOOM Expert Units are almost as good\nas before, whereas mGPT Expert Units are way\nworse; only in 3 other cases do BLOOM-560m\nunits represent better associations in total. This\nreveals that these models contain different distribu-\ntions even though they were trained with similar\nobjectives, showing different magnitude responses\ntowards data attribute variations, including noise,\nsemantic coherence, data quantity and language.\nInfluence of Concept-Country and Prefix-\nCountry We simulate an environment where we\nprovide Expert Units about one geographic entity\n(Concept-Country) and ask a PLM about another\ngeographic entity ( Prefix-Country). By now, we\nhave shown that the PLM encodes geographic prox-\nimity but also exhibits geopolitical favouritism dur-\ning inference. The question we ask at this point\nis: Given that PLM is biased, how do the Concept-\nCountry and Prefix-Country influence text genera-\ntion?\nTo answer this question, we compute Representa-\ntion Score on generated texts varying the language\n(Figure 6(c)). As always, top-10 country Represen-\ntation Score is evident in all languages while the\nsecond most influencing factor is Prefix-Country.\nIn Hindi, Concept-Country has the highest influ-\nence of geographic mention in a prompt-based gen-\neration. However, this scenario does not hold for\nthe cases of Korean, Bengali, and Russian. On\nthe other hand, Concept-Country plays the part of\na subtle representative but fails to compete with\n146\nPrefix-Country and geopolitical significant coun-\ntries. One fact to note here is, our experiment con-\ntains a small number of examples while generating\na large pool of texts. Nevertheless, we believe that\nit will require intensive data creation efforts to mit-\nigate the biases that coexist with the geographic\nknowledge in PLMs.\n6 Conclusion and Future Work\nIn this study, we perform an experimental analysis\non identifying the inherent geographic knowledge\nand inference bias of prompt-based decoder mod-\nels. Our experiments strongly suggest that current\nPLMs are able to encode geographic proximity\nquiet well. However, almost always geopolitical\nfavouritism overshadows the encoded proximity\nduring inference. This finding raises concerns as\nwell as the need to perform bias-mitigation steps\nif we want to generate geo-specific texts. Our ad-\nditional findings on the impact of multilinguality\non prompting points out how encoded geographic\nproximity is unevenly distributed across languages\nand how even just a mention of geographic identi-\nfiers may influence the language of free-form text\ngeneration. We believe these findings still leave\nissues to be addressed in current practice and that\nthere should be a a fundamental multilingual-bias\nmitigation step included in any NLP task work-\nflow. Keeping this in mind, we want to expand the\ndomain of our proposed probing framework and\nassess its applicability beyond geography. In addi-\ntion, we aim to perform contrastive training to effi-\nciently extract expert units thus stepping forward\nwith the effort of reducing the inequality inherent\nin multilingual language models.\nLimitations\nFirst of all, selecting country as geographic entities\nis inherently lossy and ideally, we would be able to\nperform the experiments with further granularity.\nWe rely on Wikidata for entity linking, which is\nalready somewhat biased towards western coun-\ntries. In addition, our experiments are limited to 69\ncountries and 13 languages (8 for generating text)\n(by necessity and due to computing costs), ignor-\ning other countries as well as languages, especially\nlow-resource ones. In the future, we want to further\nexpand our study to include more languages and\ncultures, as well as digging deeper in multi-cultural\ncountries.\nAcknowledgements\nWe are thankful to the anonymous reviewers for\ntheir constructive feedback. This work is gener-\nously supported by the National Science Founda-\ntion under grants FAI-2040926, IIS-2125466, and\nIIS-2127901.\nReferences\n2021. Ip2location™ country multilingual database. On-\nline resource.\nJaimeen Ahn and Alice Oh. 2021. Mitigating language-\ndependent ethnic bias in BERT. InProceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 533–549, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nSarah Alnegheimish, Alicia Guo, and Yi Sun. 2022.\nUsing natural sentence prompts for understanding bi-\nases in language models. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2824–2830, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nGiorgia Anceresi, Daniele Gatti, Tomaso Vecchi, Marco\nMarelli, and Luca Rinaldi. 2023. A map of words:\nRetrieving the spatial layout of underground stations\nfrom natural language.\nVincent D Blondel, Jean-Loup Guillaume, Renaud Lam-\nbiotte, and Etienne Lefebvre. 2008. Fast unfold-\ning of communities in large networks. Journal\nof Statistical Mechanics: Theory and Experiment ,\n2008(10):P10008.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv:2005.14165.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation.\nFahim Faisal, Yinkai Wang, and Antonios Anastasopou-\nlos. 2022. Dataset geography: Mapping language\ndata to language users. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 3381–\n3411, Dublin, Ireland. Association for Computational\nLinguistics.\nDaniele Gatti, Marco Marelli, Tomaso Vecchi, and Luca\nRinaldi. 2022. Spatial representations without spatial\ncomputations. Psychological Science, 33(11):1947–\n1958. PMID: 36201754.\n147\nSayan Ghosh, Dylan Baker, David Jurgens, and Vin-\nodkumar Prabhakaran. 2021. Detecting cross-\ngeographic biases in toxicity modeling on social me-\ndia. In Proceedings of the Seventh Workshop on\nNoisy User-generated Text (W-NUT 2021) , pages\n313–328, Online. Association for Computational Lin-\nguistics.\nWes Gurnee and Max Tegmark. 2023. Language models\nrepresent space and time.\nEster Hlavnova and Sebastian Ruder. 2023. Empow-\nering cross-lingual behavioral testing of NLP mod-\nels with typological features. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n7181–7198, Toronto, Canada. Association for Com-\nputational Linguistics.\nValentin Hofmann, Goran Glavaš, Nikola Ljubeši ´c,\nJanet B. Pierrehumbert, and Hinrich Schütze. 2023.\nGeographic adaptation of pretrained language mod-\nels.\nAlexander Immer, Lucas Torroba Hennigen, Vincent\nFortuin, and Ryan Cotterell. 2022. Probing as quanti-\nfying inductive bias. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1839–\n1851, Dublin, Ireland. Association for Computational\nLinguistics.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and\nRuslan Salakhutdinov. 2021. Towards understanding\nand mitigating social biases in language models.\nMax M. Louwerse and Nick Benesh. 2012. Repre-\nsenting spatial structure through maps and language:\nLord of the rings encodes the spatial structure of\nmiddle earth. Cognitive Science, 36(8):1556–1569.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nTimo Schick and Hinrich Schütze. 2021. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6943–\n6951, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova,\nVladislav Mikhailov, Anastasia Kozlova, and Tatiana\nShavrina. 2022. mgpt: Few-shot learners go multilin-\ngual.\nVered Shwartz. 2022. Good night at 4 pm?! time ex-\npressions in different cultures. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 2842–2853, Dublin, Ireland. Association for\nComputational Linguistics.\nXavier Suau, Luca Zappella, and Nicholas Apostoloff.\n2022. Self-conditioning pre-trained language models.\nInternational Conference on Machine Learning.\nQingyu Tan, Hwee Tou Ng, and Lidong Bing. 2023.\nTowards benchmarking and improving the temporal\nreasoning capability of large language models. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 14820–14835, Toronto, Canada.\nAssociation for Computational Linguistics.\nAshish V . Thapliyal, Jordi Pont Tuset, Xi Chen, and\nRadu Soricut. 2022. Crossmodal-3600: A massively\nmultilingual multimodal evaluation dataset. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 715–729,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nYufei Wang, Can Xu, Qingfeng Sun, Huang Hu,\nChongyang Tao, Xiubo Geng, and Daxin Jiang. 2022.\nPromDA: Prompt-based data augmentation for low-\nresource NLU tasks. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 4242–\n4255, Dublin, Ireland. Association for Computational\nLinguistics.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021. Towards zero-label language learning.\nBigScience Workshop, :, Teven Le Scao, Angela Fan,\nChristopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel\nHesslow, Roman Castagné, Alexandra Sasha Luc-\ncioni, François Yvon, Matthias Gallé, Jonathan\nTow, Alexander M. Rush, Stella Biderman, Albert\nWebson, Pawan Sasanka Ammanamanchi, Thomas\nWang, Benoît Sagot, Niklas Muennighoff, Albert Vil-\nlanova del Moral, Olatunji Ruwase, Rachel Bawden,\n148\nStas Bekman, Angelina McMillan-Major, Iz Belt-\nagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pe-\ndro Ortiz Suarez, Victor Sanh, Hugo Laurençon,\nYacine Jernite, Julien Launay, Margaret Mitchell,\nColin Raffel, Aaron Gokaslan, Adi Simhi, Aitor\nSoroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers,\nAriel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris Emezue, Christopher Klamm, Colin Leong,\nDaniel van Strien, David Ifeoluwa Adelani, Dragomir\nRadev, Eduardo González Ponferrada, Efrat Lev-\nkovizh, Ethan Kim, Eyal Bar Natan, Francesco De\nToni, Gérard Dupont, Germán Kruszewski, Giada\nPistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran,\nIan Yu, Idris Abdulmumin, Isaac Johnson, Itziar\nGonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse\nDodge, Jian Zhu, Jonathan Chang, Jörg Frohberg,\nJoseph Tobing, Joydeep Bhattacharjee, Khalid Al-\nmubarak, Kimbo Chen, Kyle Lo, Leandro V on Werra,\nLeon Weber, Long Phan, Loubna Ben allal, Lu-\ndovic Tanguy, Manan Dey, Manuel Romero Muñoz,\nMaraim Masoud, María Grandury, Mario Šaško,\nMax Huang, Maximin Coavoux, Mayank Singh,\nMike Tian-Jian Jiang, Minh Chien Vu, Moham-\nmad A. Jauhar, Mustafa Ghaleb, Nishant Subramani,\nNora Kassner, Nurulaqilla Khamis, Olivier Nguyen,\nOmar Espejel, Ona de Gibert, Paulo Villegas, Pe-\nter Henderson, Pierre Colombo, Priscilla Amuok,\nQuentin Lhoest, Rheza Harliman, Rishi Bommasani,\nRoberto Luis López, Rui Ribeiro, Salomey Osei,\nSampo Pyysalo, Sebastian Nagel, Shamik Bose,\nShamsuddeen Hassan Muhammad, Shanya Sharma,\nShayne Longpre, Somaieh Nikpoor, Stanislav Silber-\nberg, Suhas Pai, Sydney Zink, Tiago Timponi Tor-\nrent, Timo Schick, Tristan Thrush, Valentin Danchev,\nVassilina Nikoulina, Veronika Laippala, Violette\nLepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Ta-\nlat, Arun Raja, Benjamin Heinzerling, Chenglei Si,\nDavut Emre Ta¸ sar, Elizabeth Salesky, Sabrina J.\nMielke, Wilson Y . Lee, Abheesht Sharma, Andrea\nSantilli, Antoine Chaffin, Arnaud Stiegler, Debajy-\noti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang, Harshit Pandey, Hendrik Strobelt, Jason Alan\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-\nful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-\nhal Nayak, Ryan Teehan, Samuel Albanie, Sheng\nShen, Srulik Ben-David, Stephen H. Bach, Taewoon\nKim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur-\nmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-\nXin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri,\nHadar Tojarieh, Adam Roberts, Hyung Won Chung,\nJaesung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre François\nLavallée, Rémi Lacroix, Samyam Rajbhandari, San-\nchit Gandhi, Shaden Smith, Stéphane Requena, Suraj\nPatil, Tim Dettmers, Ahmed Baruwa, Amanpreet\nSingh, Anastasia Cheveleva, Anne-Laure Ligozat,\nArjun Subramonian, Aurélie Névéol, Charles Lover-\ning, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,\nEkaterina Taktasheva, Ekaterina V oloshina, Eli Bog-\ndanov, Genta Indra Winata, Hailey Schoelkopf, Jan-\nChristoph Kalo, Jekaterina Novikova, Jessica Zosa\nForde, Jordan Clive, Jungo Kasai, Ken Kawamura,\nLiam Hazan, Marine Carpuat, Miruna Clinciu, Na-\njoung Kim, Newton Cheng, Oleg Serikov, Omer\nAntverg, Oskar van der Wal, Rui Zhang, Ruochen\nZhang, Sebastian Gehrmann, Shachar Mirkin, Shani\nPais, Tatiana Shavrina, Thomas Scialom, Tian Yun,\nTomasz Limisiewicz, Verena Rieser, Vitaly Protasov,\nVladislav Mikhailov, Yada Pruksachatkun, Yonatan\nBelinkov, Zachary Bamberger, Zdenˇek Kasner, Al-\nice Rueda, Amanda Pestana, Amir Feizpour, Ammar\nKhan, Amy Faranak, Ana Santos, Anthony Hevia,\nAntigona Unldreaj, Arash Aghagol, Arezoo Abdol-\nlahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh\nBehroozi, Benjamin Ajibade, Bharat Saxena, Car-\nlos Muñoz Ferrandis, Daniel McDuff, Danish Con-\ntractor, David Lansky, Davis David, Douwe Kiela,\nDuong A. Nguyen, Edward Tan, Emi Baylor, Ez-\ninwanne Ozoani, Fatima Mirza, Frankline Onon-\niwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-\ntacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\njadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis\nSanz, Livia Dutra, Mairon Samagaio, Maraim El-\nbadri, Margot Mieskes, Marissa Gerchick, Martha\nAkinlolu, Michael McKenna, Mike Qiu, Muhammed\nGhauri, Mykola Burynok, Nafis Abrar, Nazneen Ra-\njani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel,\nRan An, Rasmus Kromann, Ryan Hao, Samira Al-\nizadeh, Sarmad Shubber, Silas Wang, Sourav Roy,\nSylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,\nYoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap,\nAlfredo Palasciano, Alison Callahan, Anima Shukla,\nAntonio Miranda-Escalada, Ayush Singh, Benjamin\nBeilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Clémentine Fourrier, Daniel León\nPeriñán, Daniel Molano, Dian Yu, Enrique Manjava-\ncas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddin Bayrak, Gully Burns, Helena U. Vrabec,\nImane Bello, Ishani Dash, Jihyun Kang, John Giorgi,\nJonas Golde, Jose David Posada, Karthik Ranga-\nsai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc Pàmies, Maria A Castillo, Mari-\nanna Nezhurina, Mario Sänger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, Michiel De\nWolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,\nMyungsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pascale\nFung, Patrick Haller, Ramya Chandrasekhar, Renata\nEisenberg, Robert Martin, Rodrigo Canalli, Rosaline\nSu, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,\nShlok S Deshmukh, Shubhanshu Mishra, Sid Ki-\nblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-\nmar, Stefan Schweter, Sushil Bharati, Tanmay Laud,\nThéo Gigant, Tomoya Kainuma, Wojciech Kusa, Ya-\nnis Labrak, Yash Shailesh Bajaj, Yash Venkatraman,\nYifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli\nXie, Zifan Ye, Mathilde Bras, Younes Belkada, and\nThomas Wolf. 2023. Bloom: A 176b-parameter\nopen-access multilingual language model.\nDa Yin, Hritik Bansal, Masoud Monajatipoor, Liu-\nnian Harold Li, and Kai-Wei Chang. 2022. Geom-\n149\nlama: Geo-diverse commonsense probing on multi-\nlingual pre-trained language models. In EMNLP.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Advances in Neural Information Processing\nSystems, volume 34, pages 27263–27277. Curran As-\nsociates, Inc.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\n150\nA Frequently asked questions\nA.1 What does it mean by the term geographic biases, geographic favouritism and what are their\nrelationships with fairness?\nIn general, geographic bias means the over-representation of certain geographic attributes. In this study, we\nuse \"geographic bias\" and \"geographic favouritism\" interchangeably as the over-amplification of certain\ncountry representation (eg. countries with higher GDP, geopolitical stability, military strength etc) during\nPLM prediction or text-generation. We believe the overall system utility of a language model should be\nequitable according to the needs of the intended users with different demographic and geographic origin.\nThus ensuring their geographic characteristics are well-represented and not over-shadowed because of\ngeographic favouritism is defined as \"geographic fairness\" in this study.\nA.2 What’s the reason for using the self-conditioning approach of Suau et al. (2022) for studying\nbiases? There had been many other bias measures in NLP before Suau et al. (2022). Are they\nnot suitable for the study of geographic and geopolitical biases?\nA number of previous studies experimented with the behavior different PLMs exhibits while probing with\ngeographic-context as well as cultural-commonsense (Yin et al., 2022; Ghosh et al., 2021). However, we\nneed to extract the specific model weights responsible for these observable polarity. Then using those\nweights in a controlled setting, we might be able to unfold how PLMs encode geographic knowledge as\nwell as explain the exhibition of geographic-bias during inference. The self-conditioning model proposed\nby Suau et al. (2022) is one such study that fits to our intended needs perfectly. This approach serves\ntwo main purposes: (1) Identifying expert units: model parameters responsible for generating text related\nto the target concept (i.e. doctor). (2) Triggering specific behaviour in text generation without explicit\nmentioning or fine-tuning of the target context, which inadvertently influences the behaviour of the model\nutilizing the encoded-knowledge of PLM.\nA.3 What are the practical takeaways from this? Yes, different models encode geographic\nknowledge, so what? Should we be concerned, should we do something about it?\nWe recall the example presented earlier: consider a L1 Spanish speaker from Peru, who is using a prompt-\nbased PLM (like that of Wang et al. (2022, 2021)) to generate a localized synthetic dataset for some\ndownstream task. They may use Spanishas used in the local contextto form their seed data/prefix/prompts.\nNow, if this language model has already skewed preferences towards geopolitically important countries, it\nis likely the generated texts will reflect this skewness, thus not appropriately reflecting the local, Peruvian\ncontext that the practitioner is interested in. In this study we address this concern of geographic bias\nbeing one of the most-significant yet ignored attributes in practice. Moreover, we show how this is further\namplified when we go beyond English and similar languages. Basically we need effective bias-mitigation\nmodule as part of the regular NLP workflow which is currently non-existent.\nA.4 Why we need to extract the Expert Units and how Concept-Country helps in this regard?\nOne of our aims is to unfold the geographic representation using relevant PLM units without external\nfine-tuning. So, we need to find or extract these relevent units which are basically model parameters. So,\nwe can use our Concept-Country datasets as binary classification dataset (positive class contains sentences\nmentioning certainConcept-Country) to find these highly responsive weights (i.e. Expert Units) to certain\nConcept-Country. Then we perform self-conditioning on the PLMs using these Expert Units to generate\ntexts having the influence of these Concept-Countrys.\nA.5 Explain Concept-Country dataset creation process.\nWe scrape news using a Google news api8 to capture the current affairs. Importantly, we can select news\nnot just from a given date range, but also news originating in a specific country and a specific language.\nSuch a dataset should allow us to get a reasonable representation of current geopolitical affairs. As such,\n8https://github.com/ranahaani/GNews\n151\neach of the concept datasets we create reflects “current news about a country reported by the mainstream\nplatforms from another country\". Hence, a Concept-Country dataset {C}-{l} contains news about several\n(c1, c2, ..cn) countries in {l} language where the news-source is {C} country. For example, USA-eng\ncontains data from US sources, in English, which either mention other countries (there are 100 positive\nexamples for each country ci) or are random sentences not mentioning any countries (negative examples).\nA.6 Explain the Expert Units extraction process.\nConsider the Concept-Country India from the dataset USA-eng. Essentially, we have positive examples\n(text mentioning India or relevant entities) and negative examples (random other sentences not mentioning\nIndia) which we can use to identify the model’s Expert Units. These units are the neurons which can be\nused as predictors to identify the presence of a concept (i.e. positive examples mentioning \"India\"). The\nself-conditioning framework computes these neurons and uses the average-precision score to rank their\npredictive expertise thus allowing us to select the top-k (eg. 10, 50) Expert Units from each layer.\nA.7 What does Geographic Representation Network actually represents?\nNote that these networks are produced using the uncovered original PLM expert units, without any external\ndata fine-tuning or prompting. Hence, they provide a view of the inherent geographic knowledge present\ninside the PLM parameter space.\nA.8 Why we need to use Expert Units during text generation?\nWe have a setting where we can provide certain Concept-Country as part of the generation condition\nand the specific Expert Units from the model itself are supposed to be capable enough to influence the\ngenerated text. Our aim is to evaluate the geographic knowledge specific model weights or Expert Units\nby asking those about other Prefix-Country. This will unfold whether the geopolitical favouritism happens\nfor geopolitically important countries or the geographical proximity (eg. neighbouring countries) takes\nthe precedence or there exist no such patterns.\nA.9 What are the factors considered while constructing the Concept-Country dataset?\nThere are two relevant factors: (1) For the negative examples in USA-engConcept-Country dataset, we\nuse news from a completely different domain (eg. automobile, sport), whereas for different geographic-\nsourced datasets, negative examples come from randomly sampling news of different locations. (2) The\nintensity of text-noise and positive example up-sampling amount varies across different news-sourced\nConcept-Country datasets.\nA.10 Why 2-hop distance while calculating the neighbourhood-score?\nWe did experiment with n-hop scoring and they follow similar trends. We choose 2-hop is it is less\ncomplex for scoring and at the same-time, sufficient to point out the disparity across multiple languages.\nA.11 Comparison to news: although these models are trained on web text, which contain news\narticles, they are not guaranteed to generate text like a news article. Thus the distribution of\nentities within the text will be different.\nYes, that is correct but our aim is to capture the learned distribution and evaluate (1) whether that\ndistribution is skewed or not, (2) Whether there is resemblance with the real-world scenario or not. We\nbelieve, this assessment is important for a PLM which will be used for solving real-world practical tasks\nand having news-text for comparison might be the closest viable source we can get in a limited resource\nsetting.\nA.12 What does it mean by: \" the model weights which provide higher scores for the presence of a\nconcept\"\nIn sort, a language model can provide scores to the positive and negative examples of a binary classification\ndataset (eg. our country-concept dataset). Looking at the average precision scores and the outputs given\n152\nby different model weights from each layer, we can identify the ones providing higher scores towards the\npositive examples and these model weights are referred as expert units.\nB Self-conditioning Method: Theoretical Definition\nHere we provide a theoretical description concerning the working procedure of the self-conditioning\nmethod (Suau et al., 2022). First, we provide an overview of the usual generative mechanism followed by\nthe expert unit extraction procedure. Then we talk about creating the simulated environment where the\nexpert units are prioritized to instigate text generation in a specific direction.\nGenerative Mechanism During autoregressive text generation, a language model maximizes the\nprobability of a sentence x = {xi}as p(x) =p(x1, ..xT ) =∏T\nt=1 p(xt|x<t). A conditional generative\nmodel can use a joint probability distribution to maximize the probability such that:p(x, y) =p(y|x)p(x).\nHere, x is the generated sentence while y is a conditional variable (i.e. imposing the presence of a concept\nword). Dathathri et al. (2020), adopted this setting in a conditional generation where, p(y|x) determines\nthe condition and p(x) ensures constraint on the generated text as it progresses. In this setting, instead of\nthe joint distribution, the condition can even be fixed beforehand as follows:\np(x|y = c) ≈p(y = c|x)p(x) (1)\nSuau et al. (2022), hypothesize that the conditional maximization of p(x|y = c) in Eq. (1) can be done by\nexploiting the internal mechanism of a PLM (e.g. expert unit extraction and prioritizing them by changing\ntheir responses during text generation).\nExpert Unit Extraction Suau et al. (2022) defines expert units as the neurons contributing to the\nconditional model p(y = c|x) in Eq. (1). They extract certain expert units which can further be used\nas the predictors of the concept presence identification task given an input. Formally, we define zc\nm as\nthe set of outputs of a single neuron m to sentences {sc\ni}. We can formulate zc\nm as the prediction score\nof a binary sentence classification task bc[0, 1] where sc\ni is an input sentence and zc\nm varies depending\non the presence/absence of a concept c in sc\ni. Now having the prediction score zc\nm in hand, we can\ncompute the expertise of a unit m for the task bc[0, 1] by looking at the average precision score so that\nAPc\nm = AP(zc\nm, bc) ∈[0, 1] (i.e. area under the precision-recall curve). At this point, the top k expert\nunits are identified by ranking all the units from each model layer based on APc\nm.\nConditional Text Generation The final step is to prioritize the identified expert units to generate texts\nhaving specific behaviors. This can be done using a do(c, k) intervention which ensures the influence of\nconcept c while prioritizing the top k−expert units. These top k−expert units previously performed as the\nbest predictors for c concept identification from sentences. In (Suau et al., 2022), do(c, k) is formulated\nas follows:\ndo(c, k) :{zm\nc := Ec\nx[zm\nc |bc = 1]∀m ∈Qk} (2)\nThis do(c, k) intervention always replaces the response of an expert unit with the typical value where\nthe concept c was present in an input sentence (i.e. Ec\nx[zm\nc |bc = 1]). Here, Qk is the set of indices of\nall top-performing k-expert units. Now in Eq. (1), the p(y = c|x) can be maximized by increasing the\nnumber of relevant expert units (i.e. k) using the do(c, k) intervention according to the adopted hypothesis\nof (Suau et al., 2022). As a result, by just exploiting the internal conditioning mechanism of a PLM\ntext generation and without any out-source data training, an artificial environment is created where the\npresence of concept c is inspired.\nC Datasets\nIn Table 3 we present the concept dataset details. Each dataset here contains 43 to 69 country concept\nfiles (The complete list of countries is presented in Table 4).\n153\nFigure 7: A snap-shot of the USA-eng dataset. Each json file contains postive-negative news about one specific\ncountry. For example, the australia.json contains positive sentences having mention of the country name\nextracted from the news articles. Whereas, the negative 300 sentences are also collected from news domain having\nno mention of the word austrailia.\nA snapshot of the USA-eng dataset is presented in Figure 7 to provide a better understanding of how the\nconcept dataset is formatted. This specific dataset contains English news about various countries while\nthe news-originating country is the USA. From the figure, we observe the mention of country-named json\nfiles (i.e. the country concept files). Each json file contains positive 100 sentences about that specific\ncountry. Whereas, the negative 300 sentences contain no mention of the specific country. Moreover, we\ncan take a further look at the australia.json file where the positive instances are sentences selected\nfrom Australia-related recent news articles.\nIn Table 4, The Type-2 datasets are the translated version of USA-eng dataset. In Type-3, we mask\nUSA-eng entities using a NER tagger and Type-4 is constructed using random english texts.\nD Prefix Templates\nFor each of the eight languages, we generate prefix replacing templates with Prefix-Country names. Per\nlanguage, we have six template prefix. The complete list is presented in Table 5\nE Additional Geographic Representation Networks\nIn Figures (8, 9, 10, 11) we present Geographic-Representation Networks (News Source-language: USA-\neng, SAU-ara, FRA-fra, RUS-rus, BGD-ben, KOR-kor, CHN-zho, IND-hin) constructed using the Expert\nUnits from GPT2, BLOOM and mGPT.\nF Geography Maps on generated text\nWe present Country Maps on the generated outputs for eight languages. The maps are presented in Figure\n12.\n154\nDataset Names # Description\nType 1: {News _Source_Location}-{Language}\nUSA-eng BGD-ben CHN-zho\nGRC-ell ISR-heb IND-hin\nKOR-kor MEX-spa NOR-nor\nSAU-ara VNM-vie AUS-eng\nETH-eng GBR-eng HKG-zho\nTZA-eng FRA-fra PER-spa\nJPN-jpn RUS-rus CUB-spa\n21\nThese 21 datasets are scrapped from news\nsources originating from 21 different coun-\ntries in different languages. Each one of these\ndatasets contain country concept sets describ-\ning news about specific countries. Each coun-\ntry concept are prepared using 100 positive\nsentence examples and 300 negative sentence\nexamples. We use upsampling by repetition\nwhen we have less examples than the required\ncounts. For only USA-eng dataset, we use en-\nglish news from other topic search (eg. Auto-\nmotive, Sport) to construct the negative exam-\nples while, for other 20 datasets we use news\nabout other countries (i.e. in domain) as nega-\ntive examples.\nType 2: {News _Source_USA}-{Translations}\nUSA-ara USA-ben USA-ell\nUSA-hin USA-kor USA-rus\nUSA-zho USA-fra\n8\nThese 8 datasets are created using translation\nfrom the USA-eng dataset. We use Google\nTranslation API1 to translate the texts from\nsource language to target language.\nType 3: {USA-eng}-{Masked Entities}\nUSA-eng-[M] 1 We augment USA-eng dataset by masking all\nadditional entities in positive examples for\neach country concepts using spaCy2.\nType 4: {USA-eng}-{Random Text}\neng-[R]\n1\nWe randomly use text instead of original text\nin USA-eng dataset while maintaining the posi-\ntive negative class distinction but without any\nsemantic coherence.\n[1] https://translate.google.com/\n[2] https://spacy.io/\nTable 3: Country Concept Datasets sourced from Google News texts. We extracted expert units from language\nmodels: gpt-2 (only english), bloom and mgpt for all of these. Among these, we perform text generation using the\nexpert units sourced from 8 datasets (The underline ones).\n155\nISO Country ISO Country ISO Country\nAUS Australia BW A Botswana CAN Canada\nETH Ethiopia GHA Ghana IND India\nIDN Indonesia IRL Ireland ISR Israel\nKEN Kenya LV A Latvia MYS Malaysia\nNAM Namibia NZL New Zealand NGA Nigeria\nPAK Pakistan PHL Philippines SGP Singapore\nZAF South Africa TZA Tanzania UGA Uganda\nGBR United Kingdom USA United States ZWE Zimbabwe\nCZE Czech Republic DEU Germany AUT Austria\nCHE Switzerland ARG Argentina CHL Chile\nCOL Colombia CUB Cuba MEX Mexico\nPER Peru VEN Venezuela BEL Belgium\nFRA France MAR Morocco SEN Senegal\nITA Italy LTU Lithuania HUN Hungary\nNLD Netherlands NOR Norway POL Poland\nBRA Brazil PRT Portugal ROU Romania\nSVK Slovakia SVN Slovenia SWE Sweden\nVNM Vietnam TUR Turkey GRC Greece\nBGR Bulgaria RUS Russia UKR Ukraine\nSRB Serbia ARE United Arab Emirates SAU Saudi Arabia\nLBN Lebanon EGY Egypt BGD Bangladesh\nTHA Thailand CHN China TWN Taiwan\nHKG Hong Kong JPN Japan KOR Republic of Korea\nNone None None None None None\nTable 4: List of Countries we conducted experiments on.\nG Geographic Identifier and Language Direction\nsee Table 6 for examples of generated text examples given the prefix \"In Cuba\" with Concept-\nCountry:Argentina. Though the Concept-Country dataset here we use is in English, the model generates\nhighly frequent Spanish sentences compared to english.\n156\nTable 5: Prefix templates we use for Multilingual Text Generation. We replace the<country> with the corresponding\ncountry name in generator language. For example, To construct one USA-mention Chinese prefix, we replace\n<country> with 美国. We use a multilingual country-name dataset (cna, 2021) to query country names.\n157\nLanguage Generated Text\nDirection\neng→spa In Colombia, beginning in 1991, Ley de Pesca y Tierra Naranja tiene como una estrategia de\nEconomía Indígena presenta como Ley de Conservación y Desarrollo Agrícola\neng→eng In Colombia, patients with PO are routinely referred to the Pediatric Critical Care Units (PC\nfrom 1996) because they are mostly after peak twice a los to participating in\neng→spa In Colombia, donde está en etapa de vacunación las primeras etapas las personas que llegan en\nel jueves (figuana para el millón y ultimaniano casos y el\neng→spa In Colombia, la noticia odia a Dios. Es una religión que no santifica. Esta seccionalizada del\n4Chanuto para algunos países, a sociedad que\neng→spa In Colombia, el mercado de la carne, considerado el segundo mayor productor de cortes de carne\nbovina en la región, es de caña de insumo a nivel\neng→spa In Colombia, el partido del “9-3” ha sido en la decisión del colombiano, la celebración de Luis\nZubeldense Humberto Bloom (peruano, quien abrió\neng→spa In Colombia, afloró por las fronteras de Argentina. Entre 1985 y 1993, de la República Domini-\ncana, Bolivia, después llegó a Colombia y Ecuador. El entrenador\neng→spa In Colombia, execuções entre elites, o Partido Comunista y sindicatos de esos países vecinos\nelites a partiran llevan la denuncia que derrochales. Las\neng→spa In Colombia, una estrecha relación entre Washington y Venezuela tiene un mensaje claro sobre\nBolsonaro. Así mismo, aunque no ve la necesidad de revisar lo que de no hacerlo de\neng→spa In Colombia, a 0.70 por ciento de la población de niños mueren prematuros de gripe por\nsobrepeso ha sido diagnosticada. El representante del tamaño real de\neng→spa In Colombia, PDOT, que hace más de 10 años había significado cerca de 160 actividades laborales\npara sus miembros, al día e instalaciones de 14 mili 300 personas\neng→spa In Colombia, made del Derecho penal, es la máxima parte de la violación a través de los notaria\nNúcleo de medidas contra la descripción de la Justicia y\neng→spa In Colombia, Cristina Kirchner — la vicepresidenta del fallecido expresidente Néstor Kirchner—\nha confesado que “en las últimas horas pasó todo como una enfermedad que no se registró su\nmujer\neng→spa In Colombia, el Código Penal declaró cierto grado de subordinación de la salud mental de las\nvíctimas de trabajadores a responsables funcionalistas, no profesionales por el Estado como se\neng→eng In Colombia, the majority of women are Catholic. But in the country is still refuses to accept the\nCatholic counseling school, and, penalizes women after to leave\neng→eng In Colombia, for example, we observed a significantly lower prevalence of chronic bronchoalve-\nolar or peritonitis, bronchobronchial hypertrophy than mon\neng→spa In Colombia, un importante sector de las diezañeras vuelve a poner en valor de la importancia el\nanonimato de las producciones francesas cuando, una mezcla que habían obtenido a\neng→eng In Colombia, the EMA has regular royalties on a $27,800 per fee,800 day to $39,000 protein\nproducts at the expert. The fair\neng→eng In Colombia, in turn, the mass distributions represent very low prevalence, being around 4. The\nUSA around 35 40-47% and in the usual, and 45%\neng→spa In Colombia, el gobierno presentó este miércoles un proyecto de ley en la primera lectura online\npara eximir controles y renegociación internacional e internacional de suscripto de divisas con\nTable 6: Example Generated Sentences with the prefix \"In Colombia\" and \"Country/Concept\" Argentina.\n158\nGeographic Representation Networks and Corresponding Community Maps\n(1)\n(2)\n(3)\n(4)\nFigure 8: Geographic Representation Network and Corresponding Community Map for different Expert Unit set\nAssociations. The language models we use are GPT2 (only English), mGPT and BLOOM.\n159\n(5)\n(6)\n(7)\n(8)\nFigure 9: Geographic Representation Network and Corresponding Community Map for different Expert Unit set\nAssociations. The language models we use are GPT2 (only English), mGPT and BLOOM.\n160\n(9)\n(10)\n(11)\n(12)\nFigure 10: Geographic Representation Network and Corresponding Community Map for different Expert Unit set\nAssociations. The language models we use are GPT2 (only English), mGPT and BLOOM.\n161\n(13)\n(14)\n(15)\n(16)\nFigure 11: Geographic Representation Network and Corresponding Community Map for different Expert Unit set\nAssociations. The language models we use are GPT2 (only English), mGPT and BLOOM.\n162\nGeographic Representation Networks and Corresponding Community Maps\n(a) (b)\n(c) (d)\n(e) (f)\n(g)\nFigure 12: Graphs prepared using entity-country mapping on generated texts using BLOOM. Here We take the\nlog-frequency distribution of entity counts. In all cases, the most frequent country remains the geopolitical favoured\nones with the additon of Country/Concept Dataset News Source-country (the darker red ones)\n163",
  "topic": "Geopolitics",
  "concepts": [
    {
      "name": "Geopolitics",
      "score": 0.8310515880584717
    },
    {
      "name": "Computer science",
      "score": 0.5630987882614136
    },
    {
      "name": "Political science",
      "score": 0.18634015321731567
    },
    {
      "name": "Politics",
      "score": 0.0640987753868103
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I162714631",
      "name": "George Mason University",
      "country": "US"
    }
  ]
}