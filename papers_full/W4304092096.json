{
  "title": "T-former: An Efficient Transformer for Image Inpainting",
  "url": "https://openalex.org/W4304092096",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2123401307",
      "name": "Deng Ye",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A4226824283",
      "name": "Hui, Siqi",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2375599520",
      "name": "Zhou Sanping",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2387573070",
      "name": "Meng, Deyu",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A1522458114",
      "name": "Wang Jinjun",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2100415658",
    "https://openalex.org/W1993120651",
    "https://openalex.org/W2107173102",
    "https://openalex.org/W2295936755",
    "https://openalex.org/W2093422746",
    "https://openalex.org/W2105038642",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3206335650",
    "https://openalex.org/W3196163807",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W2475287302",
    "https://openalex.org/W3203538104",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W2081351607",
    "https://openalex.org/W2990886896",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W3106725490",
    "https://openalex.org/W3171933509",
    "https://openalex.org/W2798365772",
    "https://openalex.org/W2989207674",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2991377405",
    "https://openalex.org/W6810370753",
    "https://openalex.org/W2981682056",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W3136958399",
    "https://openalex.org/W2965965567",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2985764327",
    "https://openalex.org/W2096461046",
    "https://openalex.org/W2963270367",
    "https://openalex.org/W3206082266",
    "https://openalex.org/W2963231084",
    "https://openalex.org/W2963255313",
    "https://openalex.org/W4312771828",
    "https://openalex.org/W2732026016",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2982763192",
    "https://openalex.org/W4313021454",
    "https://openalex.org/W4300424419",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3035512475",
    "https://openalex.org/W4226261765",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W2785678896",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W3043547428",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4385490375",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3108554146",
    "https://openalex.org/W4294643831",
    "https://openalex.org/W2738588019",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4301581299",
    "https://openalex.org/W4287254423",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2963420272",
    "https://openalex.org/W2541674938",
    "https://openalex.org/W4301206121",
    "https://openalex.org/W4312812783"
  ],
  "abstract": "Benefiting from powerful convolutional neural networks (CNNs), learning-based image inpainting methods have made significant breakthroughs over the years. However, some nature of CNNs (e.g. local prior, spatially shared parameters) limit the performance in the face of broken images with diverse and complex forms. Recently, a class of attention-based network architectures, called transformer, has shown significant performance on natural language processing fields and high-level vision tasks. Compared with CNNs, attention operators are better at long-range modeling and have dynamic weights, but their computational complexity is quadratic in spatial resolution, and thus less suitable for applications involving higher resolution images, such as image inpainting. In this paper, we design a novel attention linearly related to the resolution according to Taylor expansion. And based on this attention, a network called $T$-former is designed for image inpainting. Experiments on several benchmark datasets demonstrate that our proposed method achieves state-of-the-art accuracy while maintaining a relatively low number of parameters and computational complexity. The code can be found at \\href{https://github.com/dengyecode/T-former_image_inpainting}{github.com/dengyecode/T-former\\_image\\_inpainting}",
  "full_text": "ğ‘‡-former: An Efficient Transformer for Image Inpainting\nYe Deng\nXiâ€™an Jiaotong University\nXiâ€™an, Shaanxi, China\ndengye@stu.xjtu.edu.cn\nSiqi Hui\nXiâ€™an Jiaotong University\nXiâ€™an, Shaanxi, China\nhuisiqi@stu.xjtu.edu.cn\nSanping Zhouâˆ—\nXiâ€™an Jiaotong University\nXiâ€™an, Shaanxi, China\nspzhou@xjtu.edu.cn\nDeyu Meng\nXiâ€™an Jiaotong University\nXiâ€™an, Shaanxi, China\ndymeng@mail.xjtu.edu.cn\nJinjun Wang\nXiâ€™an Jiaotong University\nXiâ€™an, Shaanxi, China\njinjun@mail.xjtu.edu.cn\nInput TruthResult Input TruthResult\nFigure 1: Image inpainting outputs by our proposed ğ‘‡-former. In each group, the input image is shown on the left, with gray\npixels representing the missing areas. (Best with color and zoomed-in view)\nABSTRACT\nBenefiting from powerful convolutional neural networks (CNNs),\nlearning-based image inpainting methods have made significant\nbreakthroughs over the years. However, some nature of CNNs\n(e.g. local prior, spatially shared parameters) limit the performance\nin the face of broken images with diverse and complex forms. Re-\ncently, a class of attention-based network architectures, called trans-\nformer, has shown significant performance on natural language\nprocessing fields and high-level vision tasks. Compared with CNNs,\nattention operators are better at long-range modeling and have\ndynamic weights, but their computational complexity is quadratic\nâˆ—The author is also with the Shunan Academy of Artificial Intelligence, Ningbo, Zhe-\njiang 315000, China.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal\nÂ© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9203-7/22/10. . . $15.00\nhttps://doi.org/10.1145/3503161.3548446\nin spatial resolution, and thus less suitable for applications involv-\ning higher resolution images, such as image inpainting. In this\npaper, we design a novel attention linearly related to the resolu-\ntion according to Taylor expansion. And based on this attention,\na network called ğ‘‡-former is designed for image inpainting. Ex-\nperiments on several benchmark datasets demonstrate that our\nproposed method achieves state-of-the-art accuracy while main-\ntaining a relatively low number of parameters and computational\ncomplexity. The code can be found at github.com/dengyecode/T-\nformer_image_inpainting\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Computer vision.\nKEYWORDS\nimage inpainting, attention, neural networks, transformer\nACM Reference Format:\nYe Deng, Siqi Hui, Sanping Zhou, Deyu Meng, and Jinjun Wang. 2022.ğ‘‡-\nformer: An Efficient Transformer for Image Inpainting. In Proceedings of\nthe 30th ACM International Conference on Multimedia (MM â€™22), October\n10â€“14, 2022, Lisboa, Portugal. ACM, New York, NY, USA, 10 pages. https:\n//doi.org/10.1145/3503161.3548446\narXiv:2305.07239v2  [cs.CV]  19 May 2023\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Ye Deng et al.\n1 INTRODUCTION\nImage inpainting (or completion) [ 4] is the process of filling in\ncorrupted or missing parts of an image, as Figure 1 shown. It is an\nimportant task in the field of computer vision and image process-\ning and can benefit users in a wide range of applications, such as\nremoving unwanted objects in image editing. The key challenge\nin image inpainting is to make the filled pixels blend in with the\nnon-missing parts.\nPrior to deep learning, non-learning inpainting algorithms can be\nroughly divided into two categories, diffusion-based approaches [1,\n3, 4, 9] and exemplar-based approaches [ 2, 5, 28, 58]. Diffusion-\nbased approaches smoothly propagate the information from ob-\nserved boundaries to the interior of damaged areas. However, since\ndiffusion-based approaches do not consider the global image struc-\nture, they are only effective in filling small holes and less effective\nin dealing with large scale breakage. To address the drawbacks\nof diffusion-based inpainting approaches, the exemplar-based ap-\nproach searches for valid information from known regions of the\nentire image and copies or relocates this information to the missing\nlocations. Although the exemplar-based algorithms perform well\nin the face of simple pattern breakage in larger areas, they do not\nperform well in filling images with complex patterns because they\ndo not understand the semantic information of the image.\nWith the development of convolutional neural networks (CNNs),\nlearning-based approaches have reached state-of-arts in the field of\nimage inpainting. These inpainting models [24, 30, 42, 66] formu-\nlate the inpainting as a conditional image generation problem and\ncustomize a CNNs-based encoder-decoder as their corresponding\nconditional image generator. By training on sufficiently large scale\ndatasets, CNNs show their strength in learning rich patterns and\nimage semantics, and filling the target regions with such learned\nknowledge. In addition, the sparse connectivity and parameter\nsharing of CNNs in space make them computationally efficient.\nHowever, some basic characteristics of CNNs make them may have\nsome limitations in handling the inpainting task. (a) the locality.\nCNNs are good at acquiring local relationships but not good at\ncapturing long-range dependencies. Although the importance of\nlocality for images has been demonstrated in various vision tasks\nfor a long time, for image inpainting, focusing on non-local features\n(the whole image) is more likely to find the appropriate informa-\ntion for broken regions. (b) spatial-sharing and static parameters.\nThe same convolution kernel operates on features across all spatial\nlocations and the parameters of the kernel are static at the time\nof inference. This is somewhat inflexible in the face of inpainting\ntasks where images are mixed with broken and unbroken pixels\nand the damaged regions are variable.\nRecently, (self-)attention [51], popular in the field of natural\nlanguage processing, has been introduced in vision tasks [ 6, 13].\nCompared to CNNs, attention operators whose weights dynami-\ncally adjust with the input are better able to capture long-range\ndependencies through explicit interaction with global features. And\nas a well-explored architecture in language tasks, the transformer\nmodel, based on the attention, is emerging in high-level vision\ntasks. Although the attention operator has advantages over CNNs\nin some aspects, its computational complexity grows quadratically\nwith spatial resolution and is therefore not particularly suitable\nfor high-resolution images, a situation that occurs frequently in\nlow-level vision tasks including image inpainting. Recently, some\ndesigns that can reduce the computational complexity of attention\noperators have been transferred to inpainting [ 12] or other low-\nlevel vision tasks [31, 56]. These methods either apply attention\nto a sequence of patches unfolded from the image [12], or divide\nthe image into non-overlapping parts and compute the attention\nfor each part independently [31, 56? ]. However, limiting the spa-\ntial extent of attention somewhat defeats the original purpose of\ncapturing the long-range dependence between pixels.\nSpecifically, for the computational load caused by the dot product\nand softmax operator in the attention operator, we utilize Taylorâ€™s\nformula to approximate exponential function, and then reduce the\ncomputational complexity by swapping the computational order of\nmatrix multiplication. In addition, to mitigate the performance loss\ncaused by the error in the Taylor approximation, we introduced\nthe gating mechanism [ 10] for the proposed attention operator.\nThe previous work [61] showed that the gating mechanism on the\nconvolution in the inpainting can be seen as controlling which\nfeatures should flow forward. The gating mechanism we impose\non the attention is equivalent to an adjustment of the \"inaccurate\"\nattention, allowing the subsequent layers in the network to focus\non the information that will help the inpainting, thus producing a\nhigh quality complementation result.\nIn this paper, based on our designed linear attention, we propose\nan U-net [46] style network, called ğ‘‡-former, for image inpaint-\ning. Compared with the convolution-based encoder-decoder, in ğ‘‡-\nformer we replace the convolution with the designed transformer\nmodule based on the proposed linear attention. Our proposed ğ‘‡-\nformer combines the texture pattern learning capability of CNNs\nwith the ability of the attention to capture long-range dependencies,\nand the complexity of this attention is linear rather than quadrat-\nically related to the resolution. Our proposed ğ‘‡-former is able to\nachieve performance comparable to other advanced models while\nmaintaining a small complexity compared to those models.\n2 RELATED WORK\n2.1 Vision Transformer\nThe transformer model [51] is a neural network centered on the\n(self-)attention that plays an important role in natural language\nprocessing, and Carion et al. [6] were the first to introduce it into\nthe field of vision for object detection. Dosovitskiy et al. [13] then\ndesigned a transformer structure more suitable for use in the visual\nfield based on the characteristics of images. Touvron et al. [49]\nreduced the data requirements of visual transfoemer with the help\nof knowledge distillation. Wang et al. [55] then introduced the fea-\nture pyramid idea commonly used to build CNNs networks into\ntransformer network construction, which improved the perfor-\nmance of transformer networks. Next, Vaswani et al. [50] reduce\nthe computational demand of the model by limiting the range of\nattention so that the self-attention acts only on a local window.\nSubsequently, Liu et al. [37] extended the use and performance of\nthe transformer model by more subtle design of window attention.\nThese works demonstrated the potential of the transformer for\nhigh-level vision tasks, yet because its core self-attention excels\nin features such as long-range modeling, it also meets the needs\nğ‘‡-former: An Efficient Transformer for Image Inpainting MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\nof low-level tasks such as inpainting. However, the computational\ncomplexity of the attention in the transformer grows quadratically\nwith spatial resolution, making it inappropriate for direct use in\nlow-vision tasks that require the generation of higher-resolution\noutputs. Therefore, a class of models chooses to process only low-\nresolution features of the image with transformer. VQGAN [15], an\nautoregressive transformer is utilized to learn the effective code-\nbook. ImageBART [14] improves the quality of image synthesis by\nreplacing the autoregressive model in VQGAN with the diffusion\nprocess model. MaskGIT [7], in contrast to VQGAN, abandons the\nautoregressive generation paradigm and introduces a mask, which\ndetermines the inference token by the probability value of the mask\ninstead of synthesizing it sequentially as in autoregressive. ICT[53]\nis a two-stage inpainting where the first stage gets a coarse result\nby transformer and then feeds this result into a CNN to refine the\ndetails. BAT [62] improves on the first stage of ICT by introduc-\ning bidirectional and autoregressive transformer to improve the\ncapability of the model. TFill [67] introduces a restricted CNN head\non the transformer in ICT to mitigates the proximity influence.\nThese approaches allowed the models to obtain more compact im-\nage encodings, but still did not change the limitation that they could\nnot be applied to high resolution images. Subsequently, a different\nstrategy to reduce the complexity was generally adopted. For ex-\nample, Zamir et al. [63] propose replacing spatial attention with\ninter-channel attention. Or the replacement of inter-pixel attention\nwith inter-patch attention as in [ 12, 64]. There is also the use of\nthe window attention as in [31, 56] to reduce computational com-\nplexity by directly limiting the spatial range of action of attention\nin a similar way to [37]. Our ğ‘‡-former, which does not avoid the\nproblem of attention between full-space pixels, learns long-range\ndependencies without imposing excessive complexity.\n2.2 Image Inpainting\nPrior to deep learning, non-learning methods could only fill pixels\nbased on the content of the missing regions around [1, 3, 4, 9] or all\nobserved regions [2, 5, 28, 58] because they could not understand the\nsemantics of the image. These methods tend to be more effective for\nsmall missing holes or simple background filling, and have limited\neffect in the face of images with complex patterns. In order to enable\nthe model to output semantic results, Pathak et al. [42] introduced\nthe generative adversarial network (GAN) [17] framework to train a\nconditional image generation model with the help of convolutional\nneural networks (CNNs). Then, in response to the shared, static\nparameters of the convolution, some researchers have modified the\nconvolution so that it can manually [34] or automatically [57, 61]\nadjust the features according to the image breakage. Next, since\nit is not easy for the model to recover complex patterns directly,\nsome researchers have chosen to guide the model to complete the\nimage with the help of additional extra image information (e.g.,\nedges [40], structure [18, 29, 35, 45], semantics [32, 33]). To improve\nthis, the researchers designed a class of attention operators called\ncontextual attention [36, 54, 59, 60, 65]. Specifically, with the help\nof the attention module, they explicitly search the entire image for\nappropriate content to fill the missing regions. Nonetheless, the high\nburden of performing attention limits its large-scale deployment\nin the network, so the model is limited in the extent to which\nit can improve its long-range modeling capabilities as well as its\ncomplementary quality. In contrast, our proposed linear attention\nin ğ‘‡-former is not only able to model long-range dependencies\nbetween features, but also reduces the complexity compared to\nthe vanilla attention. This enables us to deploy more attention\noperators in the proposed ğ‘‡-former and achieve state-of-the-art in\nimage inpainting.\n3 APPROACH\nThe goal of image inpainting is to fill the target area of the input\nimage ğ¼ğ‘š âˆˆRğ¶Ã—ğ»Ã—ğ‘Š with the appropriate pixels so that the image\nlooks intact. To achieve this goal, we designed an U-net [46] style\nnetwork, based on our proposed linear attention module. In this\nsection, we present our approach from bottom to top. We first\ndescribe our proposed linear attention module, and then introduce\nthe architecture of our inpainting network.\n3.1 Linear Attention\nVanilla Attention. We first explain why the attention operator\nof the vanilla transformer [51] model is not applicable to images\nwith higher resolution. Considering a feature map ğ‘‹ âˆˆRğ¶Ã—ğ»Ã—ğ‘Š,\nassuming ğ‘ = ğ» Â·ğ‘Š, the attention operator first feeds the feature\nğ‘‹ through three different transformations and reshapes them into\nthe two-dimensional matrix to obtain the corresponding three em-\nbeddings: the query ğ‘„ = [ğ‘1,ğ‘2,Â·Â·Â· ,ğ‘ğ‘]âŠ¤âˆˆRğ‘Ã—ğ¶, the key ğ¾ =\n[ğ‘˜1,ğ‘˜2,Â·Â·Â· ,ğ‘˜ğ‘]âŠ¤âˆˆRğ‘Ã—ğ¶, and the value ğ‘‰ = [ğ‘£1,ğ‘£2,Â·Â·Â· ,ğ‘£ğ‘]âŠ¤âˆˆ\nRğ‘Ã—ğ¶. Then the corresponding attention result ğ‘‚ âˆˆRğ‘Ã—ğ¶ can be\nobtained by:\nğ‘‚ = [ğ‘œ1,ğ‘œ2,Â·Â·Â· ,ğ‘œ3]âŠ¤\n= A(ğ‘‹)\n= Softmax\n\u0010\nğ‘„ğ¾âŠ¤/\nâˆš\nğ¶\n\u0011\nğ‘‰\n(1)\nwhere A(Â·)the attention function which has quadratic space and\ntime complexity with respect to ğ‘. And each ğ‘œğ‘– âˆˆ Rğ¶ can be\nobtained as:\nğ‘œğ‘– =\nğ‘âˆ‘ï¸\nğ‘—=1\nexp\n\u0010\nğ‘ğ‘–ğ‘˜âŠ¤\nğ‘— /\nâˆš\nğ¶\n\u0011\nÃğ‘\nğ‘™=1 exp\n\u0010\nğ‘ğ‘–ğ‘˜âŠ¤\nğ‘™ /\nâˆš\nğ¶\n\u0011ğ‘£ğ‘— (2)\nThe above equation is the dot-product attention with softmax nor-\nmalization. We can find that the complexity of computing each row\nğ‘œğ‘– in ğ‘‚ is O(ğ‘ğ¶). Therefore, the computational complexity of ğ‘‚\nis obtained as O(ğ‘2ğ¶)= O((ğ»ğ‘Š)2ğ¶), which is quadratic with\nrespect to the image resolution ğ»ğ‘Š.\nLinearization of Attention. We can notice that the computational\ncomplexity of Eq. (2) mainly comes from the softmax term, therefore\nmost linearizations of attention focus mainly on modifications to\nsoftmax. Revisiting Eq. (2), previous methods [27, 43, 44] compute\nthe attention by using different kernel functions ğ¾(ğ‘,ğ‘˜)instead of\nexp(ğ‘ğ‘˜ğ‘‡), by:\nğ‘œğ‘– =\nâˆ‘ï¸\nğ‘—\nğ¾(ğ‘ğ‘–,ğ‘˜ğ‘—)Ã\nğ‘™ ğ¾(ğ‘ğ‘–,ğ‘˜ğ‘™)ğ‘£ğ‘— =\nâˆ‘ï¸\nğ‘—\nğ‘“(ğ‘ğ‘–)ğ‘“(ğ‘˜ğ‘—)âŠ¤ğ‘£ğ‘—Ã\nğ‘™ ğ‘“(ğ‘ğ‘–)ğ‘“(ğ‘˜ğ‘™)âŠ¤\n=\nğ‘“(ğ‘ğ‘–)Ã\nğ‘—(ğ‘“(ğ‘˜ğ‘—)âŠ¤ğ‘£ğ‘—)\nÃ\nğ‘— ğ‘“(ğ‘ğ‘–)ğ‘“(ğ‘˜ğ‘—)âŠ¤\n(3)\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Ye Deng et al.\nNote that the property of kernel function ğ¾(ğ‘,ğ‘˜)= ğ‘“(ğ‘)ğ‘“(ğ‘˜)âŠ¤is\nused here, and ğ‘“(Â·)is a projection. These methods obtain linear\nattention (O(ğ»ğ‘Šğ¶2)) by changing the order of computation of\nmatrix multiplication from ğ‘ğ‘˜âŠ¤ğ‘£ to ğ‘(ğ‘˜âŠ¤ğ‘£).\nInspired by the above linear attention approaches, in this paper\nwe take another perspective to linearize the attention by approxi-\nmating the exponential function through Taylor expansion. Specif-\nically, we note that Taylorâ€™s formula of the exponential function\nconstituting the softmax operator is:\nexp(ğ‘¥)= ğ‘’ğ‘¥ â‰ˆ1 +ğ‘¥ (4)\nPutting Eq. (4) into Eq. (2), we can get (the channel ğ¶is ignored for\nsimplicity):\nğ‘œğ‘– =\nğ‘âˆ‘ï¸\nğ‘—=1\nexp (ğ‘ğ‘–ğ‘˜âŠ¤\nğ‘— )\nÃğ‘\nğ‘™=1 exp (ğ‘ğ‘–ğ‘˜âŠ¤\nğ‘™ )\nğ‘£ğ‘—\n=\nğ‘âˆ‘ï¸\nğ‘—=1\n1 +ğ‘ğ‘–ğ‘˜âŠ¤\nğ‘—\nÃğ‘\nğ‘™=1(1 +ğ‘ğ‘–ğ‘˜âŠ¤\nğ‘™ )\nğ‘£ğ‘—\n=\nğ‘âˆ‘ï¸\nğ‘—=1\nğ‘£ğ‘— +ğ‘ğ‘–ğ‘˜âŠ¤\nğ‘— ğ‘£ğ‘—\nğ‘›+ğ‘ğ‘–\nÃğ‘\nğ‘™=1 ğ‘˜âŠ¤\nğ‘™\n=\nğ‘âˆ‘ï¸\nğ‘—=1\nğ‘£ğ‘— +ğ‘ğ‘–(ğ‘˜âŠ¤\nğ‘— ğ‘£ğ‘—)\nğ‘›+ğ‘ğ‘–\nÃğ‘\nğ‘™=1 ğ‘˜âŠ¤\nğ‘™\n(5)\nIt is worth noting that the last line in Eq. 5 is obtained by the\nproperties of vector multiplication.\nAnalysis. From the above, a linear complexity version of atten-\ntion can be obtained from the properties of matrix multiplication:\nğ‘‰ +\u0000ğ‘„ğ¾âŠ¤\u0001ğ‘‰ = ğ‘‰ +ğ‘„ \u0000ğ¾âŠ¤ğ‘‰\u0001 (6)\nIn Eq. (6), instead of calculating the attention matrix ğ´= ğ‘„ğ¾âŠ¤âˆˆ\nRğ‘Ã—ğ‘ first, ğ¾âŠ¤ğ‘‰ âˆˆRğ¶Ã—ğ¶ is computed first and then multiplying\nğ‘„ âˆˆRğ‘Ã—ğ¶. With the help of this trick, the computational complex-\nity of the attention operation is O(ğ‘ğ¶2)= O(ğ»ğ‘Šğ¶2). It is noted\nthat in the task of image inpainting, the feature (channel) dimension\nğ¶ is always much smaller than the spatial resolution ğ» Ã—ğ‘Š, so\nwe reduce the computational complexity of the model by a large\namount. Also similar to the vanilla transformer [51], we also use a\nmulti-headed [51] version of attention to enhance the performance\nof our proposed linear attention operator. Furthermore, the term\nğ‘‰+seems to be seen as a residual term with respect to ğ‘„ğ¾âŠ¤ğ‘‰, and\nfrom the ablation experiments (as seen in Table 3) we find that it\nimproves the performance of our inpainting model.\n3.2 Gated Mechanism for Linear Attention\nThe gating mechanism from recurrent neural networks (GRU [8],\nLSTM [22]) initially proved its effectiveness on language mod-\nels [10]. And the gating mechanism is also widely used in the\nfeed-forward networks (FFN) of the state-of-arts transformer net-\nworks [23, 47, 63]. A gating mechanism, (or gated linear unit) can\nbe thought of as a neural network layer whose output ğ‘‚ is the\nproduct of the components of two linear transformations of the\ninput ğ‘‹, as:\nğ‘‚ = ğ¼ âŠ™ğº ğ¼ = ğœ™ğ‘–(ğ‘Šğ‘¢ğ‘‹) ğº = ğœ™ğ‘”(ğ‘Šğ‘”ğ‘‹) (7)\nwhere ğ‘Šğ‘–, ğ‘Šğ‘— are the learnable parameters, ğœ™ğ‘–, ğœ™ğ‘— are the corre-\nsponding activation functions (which can be absent), andâŠ™denotes\nthe Hadamard product. The simple and effective gating mechanism\nsignificantly enhances the performance of the network making\nus want to generalize it to the proposed linear attention operator.\nSpecifically, for an input featureğ‘‹ and the linear attention operator\nA(Â·), then the out ğ‘‚ of the attention with a gating mechanism can\nbe written as:\nğ‘‚ = ğ´âŠ™ğº ğ´ = A(ğ‘‹) ğº = ğœ™ğ‘”(ğ‘Šğ‘”ğ‘‹) (8)\nThe gating mechanism applied on the convolution [ 61] plays an\nimportant role in the field of image inpainting and can be seen as\ndistinguishing invalid features caused by broken pixels in the input\nimage. Since our proposed linear attention is an â€œinaccurateâ€ atten-\ntion, we complement our linear attention with a gating mechanism\nthat allows subsequent layers in the network to focus on features\nthat contribute to the inpainting.\n3.3 Network Architecture\nOur ğ‘‡-former is an U-net [ 46] style network based on the pro-\nposed transformer block, containing both encoder-decoder parts,\nas shown in Figure 2. The design of this transformer block we refer\nto the encoder block in vanilla transformer [51] and contains two\nsub-layers. The first is the proposed linear attention with gating\nmechanism (LAG), and the second is a simple feed-forward network\n(FFN). In addition, we adopt a residual connection [ 19] adhering\nto each of the sub-layers. Besides these transformer modules, we\nalso use some convolutional layers to cope with scale changes (like\nupsampling) of features (inputs).\nEncoder Pipeline. Given a masked images ğ¼ğ‘š âˆˆR3Ã—ğ»Ã—ğ‘Š, our\nencoder part first feeds it into a 7 Ã—7 convolution layer and then\nget the corresponding feature map ğ¸0 âˆˆRğ¶Ã—ğ»Ã—ğ‘Š. Here ğ» and ğ‘Š\nrepresent the dimension of the spatial resolution and ğ¶ denotes\nthe dimension of the channel. Next these features are fed into\n4-level encoder stages. Each level stage contains a stack of the\ndesigned transformer blocks, and we use a convolution with kernel\nsize 3 Ã—3 and stride 2 to downsample the features between every\ntwo stages. For the given feature map ğ¸0 âˆˆRğ¶Ã—ğ»Ã—ğ‘Š, the ğ‘–-level\nencoder stage of the transformer block produces the feature map\nğ¸ğ‘– âˆˆR2ğ‘–âˆ’1ğ¶Ã— ğ»\n2ğ‘–âˆ’1 Ã— ğ‘Š\n2ğ‘–âˆ’1 . By the way the feature map output by the\nfinal (4-level) stage of the encoder is ğ¸4 âˆˆR8ğ¶Ã—ğ»\n8 Ã—ğ‘Š\n8 .\nDecoder Pipeline. The decoder takes the final feature map of\nencoder ğ¸4 as input and progressively restores the high resolution\nrepresentations. The decoder part consists of 3-level (arranged\nfrom largest to smallest) stages, each of which is stacked by several\ntransformer blocks. In each stage of the decoder, the features are\nfirst passed through an upsampling layer consisting of nearest\nneighbor interpolation and 3 Ã—3 convolution. Given a feature map,\nthe ğ‘–-level decoder stage of the upsampling produces the feature\nmap ğ·ğ‘– âˆˆR2ğ‘–âˆ’1ğ¶Ã— ğ»\n2ğ‘–âˆ’1 Ã— ğ‘Š\n2ğ‘–âˆ’1 . In addition, to help the decoder part,\nthe encoder feature ğ¸ğ‘– are concatenated to the decoder feature ğ·ğ‘–\nvia a skip connection. And the concatenation operation is followed\nby a1Ã—1 convolution layer to decrease the channels (by half). These\nfused features are then fed into the corresponding transformer block\nto obtain the dimensionally invariant features Â¯ğ·ğ‘–. Finally, after last\nğ‘‡-former: An Efficient Transformer for Image Inpainting MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\n7x7 Transformer\nBlock x NE1\nDown\n222\nHW Cï‚´ï‚´\nH W Cï‚´ï‚´\nH W Cï‚´ï‚´\nH W Cï‚´ï‚´\n222\nHW Cï‚´ï‚´\n44 4HW Cï‚´ï‚´\n44 4HW Cï‚´ï‚´\n88 8HW Cï‚´ï‚´\n88 8HW Cï‚´ï‚´\n44 4HW Cï‚´ï‚´\nÂ©\n \nDown\nDown\n1x1\nUp\nTransformer\nBlock x NE2\nTransformer\nBlock x NE3\nTransformer\nBlock x NE4\nTransformer\nBlock x ND3\n44 4HW Cï‚´ï‚´\nÂ©\n \n1x1\n222\nHW Cï‚´ï‚´\nUp\n44 4HW Cï‚´ï‚´\n222\nHW Cï‚´ï‚´\nTransformer\nBlock x ND1\nÂ©\n \n1x1\nH W Cï‚´ï‚´\nUp\n222\nHW Cï‚´ï‚´\nH W Cï‚´ï‚´\n7x7\nLAG\nFFN\n1x1\n3x3\n1x1\n3x3\n1x1\nH W Cï‚´ï‚´\nH W Cï‚´ï‚´\n1x1\nLinear\nAttention\n1x1\nH W Cï‚´ï‚´\nH W Cï‚´ï‚´\nFeed-Forward Network (FFN)\nLinear Attention with Gating Mechanism (LAG)\nFFN Feed-Forward Network\nLAG Linear Attention with\nGating Mechanism\n7x7 Convolution\nDown Downsample \nUp Upsample\n3x3 Depth-wise Convolution\nActivation Function\nElement-wise Multiply\nElement-wise Add\nÂ©\n \nConcatenate\nMasked Image Complemented Image\nFigure 2: Overview of our proposed ğ‘‡-former. Our model accepts masked images as input and outputs complemented images.\nOur ğ‘‡-former which is an U-net style network composed of transformer blocks that we designed. The transformer block\nwe designed contains two sublayers: (1) Linear attention with gating mechanism (LAG) that performs our proposed linear\nattention for full-space feature interaction, supplemented with a gating mechanism; (2) Feed-forward network (FFN) that\ntransforms the features learned by the attention operator to send useful representations for subsequent layers.\n(1-level) stage of the decoder we add a 7 Ã—7 convolution layer to\nconvert the feature map Â¯ğ·1 âˆˆRğ¶Ã—ğ»Ã—ğ‘Š into the complemented\nimage ğ¼ğ‘œğ‘¢ğ‘¡ âˆˆR3Ã—ğ»Ã—ğ‘Š.\nTransformer Block. As shown in the Figure 2, the transformer\nblock we used in ğ‘‡-former contains two sub-layers. The first is the\nproposed linear attention with the gating mechanism (LAG), and\nthe second is a simple feed-forward network (FFN). In the LAG\nlayer, the gating value we obtain by feeding the inputğ‘‹ into a 1 Ã—1\nconvolution layer with a GELU [20] activation function, i.e. ğœ™ğ‘”(Â·)\nand ğ‘Šğ‘” of Eq. (8).\nFor the design of the FFN we refer to the recent transform-\ners [23, 63], which uses a gate-linear layer [10] with residual con-\nnections [19] instead of a residual block [ 19] composed of two\nconvolutions in series. Specifically, to reduce the complexity, for\nthe input ğ‘‹ âˆˆRğ¶Ã—ğ»Ã—ğ‘Š, whose parameter ğ‘Šğ‘–, ğ‘Šğ‘— in Eq. (7) we\nreplace the standard convolution with a combination of a 1 Ã—1\nconvolution and a 3 Ã—3 depth-wise convolution.\n3.4 Loss Function\nThe loss function ğ¿used to train our ğ‘‡-former can be written as:\nğ¿= ğœ†ğ‘Ÿğ¿re +ğœ†ğ‘ğ¿perc +ğœ†ğ‘ ğ¿style +ğœ†ğ‘ğ¿adv (9)\nwhere ğ¿re represents the reconstruction Loss, ğ¿perc denotes the\nperceptual loss [25], ğ¿style denotes the style loss [16] and ğ¿adv is\nthe adversarial loss [17]. And we set ğœ†ğ‘Ÿ = 1, ğœ†ğ‘ = 1, ğœ†ğ‘  = 250, and\nğœ†ğ‘ = 0.1. We will describe each loss function in detail below\nReconstruction Loss. The reconstruction loss ğ¿re refers to the\nğ¿1-distance between the output ğ¼ğ‘œğ‘¢ğ‘¡ and the ground truthğ¼ğ‘”, which\ncan be defined as:\nğ¿re = âˆ¥ğ¼ğ‘œğ‘¢ğ‘¡ âˆ’ğ¼ğ‘”âˆ¥1 (10)\nPerceptual Loss. The perceptual loss ğ¿perc is formulated with:\nğ¿perc = E\n\"âˆ‘ï¸\nğ‘–\n1\nğ‘ğ‘–\n\r\rğœ™ğ‘– (ğ‘°ğ‘œğ‘¢ğ‘¡)âˆ’ğœ™ğ‘–\n\u0000ğ‘°ğ‘”\n\u0001\r\r\n1\n#\n(11)\nwhere ğœ™ğ‘– is the activation function of the ğ‘–-th layer of the VGG-\n19 [48] pre-trained on ImageNet [11].\nStyle Loss. If the size of feature maps is ğ¶ğ‘— Ã—ğ»ğ‘— Ã—ğ‘Šğ‘—, then the\nstyle loss ğ¿style is calculated by:\nğ¿style = Eğ‘—\nh\r\r\rğºÎ¦\nğ‘— (ğ‘°ğ‘œğ‘¢ğ‘¡)âˆ’ğºÎ¦\nğ‘—\n\u0000ğ‘°ğ‘”\n\u0001\r\r\r\n1\ni\n(12)\nWhere ğºÎ¦\nğ‘— denotes a ğ¶ğ‘— Ã—ğ¶ğ‘— Gram matrix constructed by the cor-\nresponding activation maps ğœ™ğ‘—.\nAdversarial Loss. The adversarial loss ğ¿adv is formulated with:\nğ¿adv = Eğ‘°ğ‘”\n\u0002\nlog ğ· \u0000ğ‘°ğ‘”\n\u0001\u0003\n+Eğ‘°ğ‘œğ‘¢ğ‘¡ log [1 âˆ’ğ·(ğ‘°ğ‘œğ‘¢ğ‘¡)] (13)\nwhere ğ· represents a patch GAN discriminator [69] with the spec-\ntral normalization [39].\n4 EXPERIMENTS\nWe evaluated our proposedğ‘‡-former on three datasets, including\nParis street view (Paris) [42], CelebA-HQ [26] and Places2 [68]. For\nCelebA-HQ, we use the first 2000 images for test and the rest for\ntraining. For Paris and Places2, we follow the training, testing, and\nvalidation splits themselves. During the experiments, all images\nin datasets were resized to 256 Ã—256. Furthermore, during the\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Ye Deng et al.\nTable 1: Numerical comparisons on the several datasets. The â†“indicates lower is better, while â†‘indicates higher is better\nDataSet Paris Street View Celeba-HQ Places2\nMask Ratio 10-20% 20-30% 30-40% 40-50% 10-20% 20-30% 30-40% 40-50% 10-20% 20-30% 30-40% 40-50%\nFIDâ†“\nGC 20.68 39.48 58.66 82.51 2.54 4.49 6.54 9.83 18.91 30.97 45.26 61.16\nRFR 20.33 28.93 39.84 49.96 3.17 4.01 4.89 6.11 17.88 22.94 30.68 38.69\nCTN 18.08 24.04 36.31 48.46 1.77 3.33 5.24 7.69 15.70 26.41 40.05 55.41\nDTS 16.66 31.94 47.30 65.44 2.08 3.86 6.06 8.58 15.72 27.88 42.44 57.78\nOurs 12.15 22.63 34.47 46.60 1.40 2.55 3.88 5.42 10.85 17.96 26.56 34.52\nPSNRâ†‘\nGC 32.28 29.12 26.93 24.80 32.25 29.10 26.71 24.78 28.55 25.22 22.97 21.24\nRFR 30.18 27.76 25.99 24.25 30.93 28.94 27.11 25.47 27.26 24.83 22.75 21.11\nCTN 31.22 28.62 26.62 24.91 32.84 29.75 27.35 25.41 27.83 24.91 22.83 21.18\nDTS 32.69 29.28 26.89 24.97 32.91 29.51 27.02 25.13 28.91 25.36 22.94 21.21\nOurs 32.79 29.72 27.47 25.47 33.36 30.15 27.67 25.67 29.06 25.69 23.36 21.52\nSSIMâ†‘\nGC 0.960 0.925 0.872 0.800 0.979 0.959 0.931 0.896 0.944 0.891 0.824 0.742\nRFR 0.943 0.908 0.861 0.799 0.970 0.958 0.939 0.913 0.929 0.891 0.830 0.756\nCTN 0.955 0.921 0.872 0.812 0.981 0.964 0.940 0.909 0.942 0.892 0.827 0.746\nDTS 0.963 0.929 0.875 0.812 0.981 0.962 0.937 0.905 0.952 0.901 0.834 0.755\nOurs 0.964 0.933 0.887 0.825 0.983 0.967 0.945 0.915 0.953 0.907 0.846 0.770\nTable 2: Complexity measure of different models. Including\nmultiplyâ€“accumulate operation count (MAC) and number\nof parameters (Params). Compared to other baseline mod-\nels, our ğ‘‡-former has a smaller number of parameters and\ncomputational complexity\nModel GC RFR CTN DTS Ours\nMAC 103.1G 206.1G 133.4G 75.9G 51.3G\nParams 16.0M 30.6M 21.3M 52.1M 14.8M\nexperiments in image inpainting we have to specify the location\nof the broken areas. Therefore, we use the mask dataset from the\nPC [34] to simulate the location of the corruption. The ğ‘‡-former\nwe propose was based on a Pytorch [41] implementation and was\ntrained on one RTX3090 (24 GB) with a batch size of 6. From input\nto output, the number of transformer blocks of different levels is 1,\n2, 3, 4, 3, 2, 1 in order. We used the AdamW [38] optimizer to train\nthe model with a learning rate of10âˆ’4 and then fine-tune the model\nwith a learning rate of 10âˆ’5. Specifically, on the CelebA-HQ and\nParis street view we trained 450,000 iterations and then fine-tuned\n200,000 iterations. As for the Places2 data set, we trained about 1\nmillion iterations and then fine-tuned 500,000 iterations.\nBaselines. To demonstrate the effectiveness of our ğ‘‡-former, We\ncompare with the following baselines for their state-of-the-art per-\nformance:\nâ€¢GC [61]: a CNNs-based inpainting model that exploits the\ngating mechanism and the contextual attention [60] to get\nhigh-quality complementary images..\nâ€¢RFR [30]: a recurrent inpainting method with a special con-\ntextual attention that recurrently recovers the missing and\nprogressively strengthens the result.\nâ€¢CTN [12]: a transformer-style model for image inpainting\nrelies on a patch-based version of the attention operator to\nmodel long-range dependencies between features.\nâ€¢DTS [18]: a dual U-net inpainting model based CNNs, which\nrecovers corrupted images by simultaneous modeling structure-\nconstrained texture synthesis and texture-guided structure\nreconstruction\nQuantitative Comparison. Following previous inpainting works [12,\n18], we chosen FID (FrÃ©chet Inception Distance) [21], PSNR (peak\nsignal-to-noise ratio), SSIM (structural similarity index) to assess\nour model. And according to the masks with different masking\npercentage provided by the dataset [34], the performance of differ-\nent models under different damage degrees (mask ratio) is tested\nin Table 1. In addition, we show the number of parameters and\nthe computational complexity (multiply-accumulate operations,\nMAC) for each model in Table 2. SSIM and PSNR are widely used\nin image quality assessment for restoration tasks, quantifying the\npixel and structural similarities between pairs of images. In addition\nwe adopt the FID, a generally used numeric metric in the task of\nimage generation, to evaluate the image distribution between the\ninpainting results and the original images. As shown in Table 1\nand Table 2, benefiting from the long-distance dependency capture\ncapability and the dynamic nature of the parameters brought by\nthe proposed linear attention, our ğ‘‡-former, in the face of different\nscenarios (datasets) and encountering different breakage situations,\ncan give relatively good complemented images with a low number\nof parameters and computational complexity.\nQualitative Comparisons. Figures 3, 4, and 5 show some com-\nparison results between our and the baseline models on the three\ndata sets Paris [42], CelebA-HQ [26] and Places2 [68] respectively.\nFrom these results, we can find that GC [ 61] is able to complete\nthe basic semantic filling, but the filled position in the image is\nğ‘‡-former: An Efficient Transformer for Image Inpainting MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\nDTS OursCTNRFRGCInput Truth\nFigure 3: Qualitative results on the Paris with GC [61], RFR [30], CTN [12], DTS [18] and our ğ‘‡-former. (Best viewed with\nzoom-in)\nDTS OursCTNRFRGCInput Truth\nFigure 4: Qualitative results on the CelebA-HQ with GC [61], RFR [30], CTN [12], DTS [18] and our ğ‘‡-former. (Best viewed\nwith zoom-in)\nprone to blurring, especially when filling images with complex\npatterns, such as the 2nd row in Figure 3 and 2nd row in Figure 5.\nThe detailed textures of the images complemented by RFR [ 30]\nlook quite good, but the results are prone to obvious artifacts and\nare prone to semantic inconsistencies. As in the 1st and 2nd rows\nof Figure 4, both images generated by RFR show the problem of\ninconsistent eye color. CTN [12] also performs quite well, but its\nresults are occasionally blurred (Figure 5, line 2) and also prone to\nblack artifacts as shown in (Figure 5, line 1). DTS [ 18] performs\nquite well with simple content images, but when it comes to images\nwith complex patterns, the fill content appears to be significantly\ndisorganized, as shown in the 1st row of Figure 5. Compared to\nthese baselines, in most cases our complemented images look more\nreasonable and realistic.\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Ye Deng et al.\nDTS OursCTNRFRGCInput Truth\nFigure 5: Qualitative results on the Places2 with GC [61], RFR [30], CTN [12], DTS [18] and our ğ‘‡-former. (Best viewed with\nzoom-in)\nTable 3: Ablation study on the Paris. The â†“indicates lower is\nbetter, while â†‘indicates higher is better\nMask Ratio 10-20% 20-30% 30-40% 40-50%\nFIDâ†“\nw/o V 12.20 23.93 35.14 47.99\nw/o G 12.74 22.89 36.08 47.48\nw/o G+V 12.94 24.19 37.08 48.47\nOurs 12.15 22.63 34.47 46.60\nPSNRâ†‘\nw/o V 32.74 29.69 27.35 25.36\nw/o G 32.72 29.68 27.36 25.33\nw/o G+V 32.70 29.66 27.33 25.28\nOurs 32.79 29.72 27.47 25.47\nSSIMâ†‘\nw/o V 0.962 0.932 0.884 0.823\nw/o G 0.963 0.931 0.884 0.823\nw/o G+V 0.961 0.930 0.883 0.821\nOurs 0.964 0.933 0.887 0.825\n4.1 Ablation Study\nWe analyze the effectiveness of our proposed module. And all the\nablation experiments are conducted on the Paris street view. In the\nablation experiments we explored two main components: (1) for the\neffect of the residual-like connection resulting from ğ‘‰+in Eq. (6)\n(or Eq. (5)), i.e.1 in ğ‘’ â‰ˆ1+ğ‘¥in the Taylor expansion, denoted byV.\nWhen Vdoes not exist, our linear attention is somewhat similar\nto the current implementation of a series of linear attentions [27,\n43, 44] in the field of natural language processing where softmax\noperators are replaced by kernel functions. And theVis equivalent\nto adding a new residual term to this family of linear attention\noperators; (2) for the effect of the gating mechanism on the model\nperformance, denoted by G. It can be noticed that both components\nhave a positive impact on the inpainting task. A more interesting\npoint is that it can be found that Vacts more significantly when\nthe input image is more broken, while the effect ofGis independent\nof the degree of input image breakage. The paper [52] has showed\nthat the residual connections can be seen as an ensemble of the\nmodel, and one more connection is equivalent to one more sub-\nnetwork. We speculate that when the model encounters difficult\nscenes (more broken parts of the input image), more sub-networks\n(with V) are needed to assistant the model get the proper content\nto fill the missing areas.\n5 CONCLUSION\nIn this paper, we proposeğ‘‡-former, a U-net style network built by\nthe proposed linear attention for for image inpainting. To address\nthe problem that CNNs-based inpainting networks have insufficient\nlong-range modeling capability and the standard self-attention op-\nerator has high computational load, we propose a linear attention\noperator based on Taylorâ€™s formula that captures the long-range\ndependence between features at a small computational cost. In\naddition, we utilize a gating mechanism to enhance the perfor-\nmance of the proposed linear attentional operator. Quantitative\nand qualitative results demonstrate that our proposed ğ‘‡-former\noutperforms state-of-the-art methods in terms of performance and\nalso maintains a relatively small complexity.\nACKNOWLEDGMENTS\nThis work is jointly supported by the National Key Research and\nDevelopment Program of China under Grant No. 2017YFA0700800,\nthe General Program of China Postdoctoral Science Foundation\nunder Grant No. 2020M683490, and the Youth program of Shaanxi\nNatural Science Foundation under Grant No. 2021JQ-054.\nğ‘‡-former: An Efficient Transformer for Image Inpainting MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\nREFERENCES\n[1] C. Ballester, M. Bertalmio, V. Caselles, G. Sapiro, and J. Verdera. 2001. Filling-in\nby joint interpolation of vector fields and gray levels. IEEE Transactions on Image\nProcessing 10, 8 (2001), 1200â€“1211. https://doi.org/10.1109/83.935036\n[2] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. 2009.\nPatchMatch: A Randomized Correspondence Algorithm for Structural Image\nEditing. ACM Transactions on Graphics (Proc. SIGGRAPH) 28, 3 (Aug. 2009).\n[3] M. Bertalmio. 2006. Strong-continuation, contrast-invariant inpainting with a\nthird-order optimal PDE. IEEE Transactions on Image Processing 15, 7 (2006),\n1934â€“1938. https://doi.org/10.1109/TIP.2006.877067\n[4] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester.\n2000. Image Inpainting. In Proceedings of the 27th Annual Conference on Computer\nGraphics and Interactive Techniques (SIGGRAPH â€™00) . ACM Press/Addison-Wesley\nPublishing Co., USA, 417â€“424. https://doi.org/10.1145/344779.344972\n[5] Pierre Buyssens, Maxime Daisy, David TschumperlÃ©, and Olivier LÃ©zoray. 2015.\nExemplar-Based Inpainting: Technical Review and New Heuristics for Better\nGeometric Reconstructions. IEEE Transactions on Image Processing 24, 6 (2015),\n1809â€“1824. https://doi.org/10.1109/TIP.2015.2411437\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-End Object Detection with\nTransformers. In European Conference on Computer Vision , Andrea Vedaldi, Horst\nBischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International\nPublishing, 213â€“229.\n[7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. 2022.\nMaskGIT: Masked Generative Image Transformer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) . 11315â€“11325.\n[8] Kyunghyun Cho, Bart Van MerriÃ«nboer, Dzmitry Bahdanau, and Yoshua Ben-\ngio. 2014. On the properties of neural machine translation: Encoder-decoder\napproaches. arXiv preprint arXiv:1409.1259 (2014).\n[9] Antonio Criminisi, Patrick PÃ©rez, and Kentaro Toyama. 2004. Region filling and\nobject removal by exemplar-based image inpainting. IEEE Transactions on image\nprocessing 13, 9 (2004), 1200â€“1212.\n[10] Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. Language\nModeling with Gated Convolutional Networks. In Proceedings of the 34th Interna-\ntional Conference on Machine Learning (Proceedings of Machine Learning Research,\nVol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR, 933â€“941.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Im-\nageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on\nComputer Vision and Pattern Recognition . 248â€“255.\n[12] Ye Deng, Siqi Hui, Sanping Zhou, Deyu Meng, and Jinjun Wang. 2021. Learning\nContextual Transformer Network for Image Inpainting. In Proceedings of the\n29th ACM International Conference on Multimedia (Virtual Event, China) (MM\nâ€™21). Association for Computing Machinery, New York, NY, USA, 2529â€“2538.\nhttps://doi.org/10.1145/3474085.3475426\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. InInternational\nConference on Learning Representations .\n[14] Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. 2021. Im-\nageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive\nImage Synthesis. In Advances in Neural Information Processing Systems , Vol. 34.\n3518â€“3532.\n[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming Transformers\nfor High-Resolution Image Synthesis. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) . 12873â€“12883.\n[16] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. 2016. Image Style Trans-\nfer Using Convolutional Neural Networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) .\n[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial\nNets. In Advances in Neural Information Processing Systems , Z. Ghahramani,\nM. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger (Eds.), Vol. 27. Curran\nAssociates, Inc.\n[18] Xiefan Guo, Hongyu Yang, and Di Huang. 2021. Image Inpainting via Condi-\ntional Texture and Structure Dual Generation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) . 14134â€“14143.\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\nLearning for Image Recognition. InProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) .\n[20] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus).\narXiv preprint arXiv:1606.08415 (2016).\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and\nSepp Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule Converge\nto a Local Nash Equilibrium. InAdvances in Neural Information Processing Systems ,\nVol. 30.\n[22] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory.Neural\ncomputation 9, 8 (1997), 1735â€“1780.\n[23] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. 2022. Transformer Quality\nin Linear Time. arXiv preprint arXiv:2202.10447 (2022).\n[24] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. 2017. Globally and\nLocally Consistent Image Completion. ACM Transactions on Graphics (Proc. of\nSIGGRAPH 2017) 36, 4, Article 107 (2017), 107:1â€“107:14 pages.\n[25] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-\ntime style transfer and super-resolution. In European conference on computer\nvision. Springer, 694â€“711.\n[26] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. Progressive\nGrowing of GANs for Improved Quality, Stability, and Variation. In Interna-\ntional Conference on Learning Representations . https://openreview.net/forum?id=\nHk99zCeAb\n[27] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret.\n2020. Transformers are RNNs: Fast Autoregressive Transformers with Linear\nAttention. In Proceedings of the 37th International Conference on Machine Learning\n(Proceedings of Machine Learning Research, Vol. 119) , Hal DaumÃ© III and Aarti\nSingh (Eds.). PMLR, 5156â€“5165.\n[28] Nikos Komodakis and Georgios Tziritas. 2007. Image Completion Using Efficient\nBelief Propagation Via Priority Scheduling and Dynamic Pruning. IEEE Transac-\ntions on Image Processing 16, 11 (2007), 2649â€“2661. https://doi.org/10.1109/TIP.\n2007.906269\n[29] Jingyuan Li, Fengxiang He, Lefei Zhang, Bo Du, and Dacheng Tao. 2019. Progres-\nsive Reconstruction of Visual Structure for Image Inpainting. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV) .\n[30] Jingyuan Li, Ning Wang, Lefei Zhang, Bo Du, and Dacheng Tao. 2020. Recurrent\nFeature Reasoning for Image Inpainting. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) .\n[31] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu\nTimofte. 2021. SwinIR: Image Restoration Using Swin Transformer. InProceedings\nof the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops .\n1833â€“1844.\n[32] Liang Liao, Jing Xiao, Zheng Wang, Chia-Wen Lin, and Shinâ€™ichi Satoh. 2020.\nGuidance and evaluation: Semantic-aware image inpainting for mixed scenes. In\nEuropean Conference on Computer Vision . Springer, 683â€“700.\n[33] Liang Liao, Jing Xiao, Zheng Wang, Chia-Wen Lin, and Shinâ€™ichi Satoh. 2021.\nImage Inpainting Guided by Coherence Priors of Semantics and Textures. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR). 6539â€“6548.\n[34] Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, and\nBryan Catanzaro. 2018. Image Inpainting for Irregular Holes Using Partial\nConvolutions. In Proceedings of the European Conference on Computer Vision\n(ECCV).\n[35] Hongyu Liu, Bin Jiang, Yibing Song, Wei Huang, and Chao Yang. 2020. Rethinking\nImage Inpainting via a Mutual Encoder-Decoder with Feature Equalizations.\nIn European Conference on Computer Vision . Springer International Publishing,\nCham, 725â€“741.\n[36] Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang. 2019. Coherent Semantic Atten-\ntion for Image Inpainting. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) .\n[37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer\nUsing Shifted Windows. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) . 10012â€“10022.\n[38] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.\nIn International Conference on Learning Representations .\n[39] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. 2018.\nSpectral Normalization for Generative Adversarial Networks. In International\nConference on Learning Representations .\n[40] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Qureshi, and Mehran Ebrahimi. 2019.\nEdgeConnect: Structure Guided Image Inpainting using Edge Prediction. In The\nIEEE International Conference on Computer Vision (ICCV) Workshops .\n[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-\nmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learn-\ning Library. In Advances in Neural Information Processing Systems , H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32.\nCurran Associates, Inc.\n[42] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A.\nEfros. 2016. Context Encoders: Feature Learning by Inpainting. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .\n[43] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and\nLingpeng Kong. 2021. Random Feature Attention. In International Conference on\nLearning Representations .\n[44] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie\nYan, Lingpeng Kong, and Yiran Zhong. 2022. cosFormer: Rethinking Softmax In\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Ye Deng et al.\nAttention. In International Conference on Learning Representations .\n[45] Yurui Ren, Xiaoming Yu, Ruonan Zhang, Thomas H. Li, Shan Liu, and Ge Li.\n2019. StructureFlow: Image Inpainting via Structure-Aware Appearance Flow. In\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) .\n[46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-Net: Convolutional\nNetworks for Biomedical Image Segmentation. In Medical Image Computing and\nComputer-Assisted Intervention, Nassir Navab, Joachim Hornegger, William M.\nWells, and Alejandro F. Frangi (Eds.). Springer International Publishing, Cham,\n234â€“241.\n[47] Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint\narXiv:2002.05202 (2020).\n[48] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Net-\nworks for Large-Scale Image Recognition. InInternational Conference on Learning\nRepresentations.\n[49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\nSablayrolles, and Herve Jegou. 2021. Training data-efficient image transformers\n& distillation through attention. InProceedings of the 38th International Conference\non Machine Learning (Proceedings of Machine Learning Research, Vol. 139) , Marina\nMeila and Tong Zhang (Eds.). PMLR, 10347â€“10357.\n[50] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake\nHechtman, and Jonathon Shlens. 2021. Scaling Local Self-Attention for Parameter\nEfficient Visual Backbones. InProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) . 12894â€“12904.\n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems , Vol. 30. Curran\nAssociates, Inc.\n[52] Andreas Veit, Michael J Wilber, and Serge Belongie. 2016. Residual Networks\nBehave Like Ensembles of Relatively Shallow Networks. In Advances in Neural\nInformation Processing Systems , D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and\nR. Garnett (Eds.), Vol. 29. Curran Associates, Inc.\n[53] Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao. 2021. High-Fidelity\nPluralistic Image Completion With Transformers. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) . 4692â€“4701.\n[54] Ning Wang, Jingyuan Li, Lefei Zhang, and Bo Du. 2019. MUSICAL: Multi-Scale\nImage Contextual Attention Learning for Inpainting. InProceedings of the Twenty-\nEighth International Joint Conference on Artificial Intelligence, IJCAI-19 . Inter-\nnational Joint Conferences on Artificial Intelligence Organization, 3748â€“3754.\nhttps://doi.org/10.24963/ijcai.2019/520\n[55] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong\nLu, Ping Luo, and Ling Shao. 2021. Pyramid Vision Transformer: A Versatile\nBackbone for Dense Prediction Without Convolutions. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV) . 568â€“578.\n[56] Zhendong Wang, Xiaodong Cun, Jianmin Bao, and Jianzhuang Liu. 2021.\nUformer: A general u-shaped transformer for image restoration. arXiv preprint\narXiv:2106.03106 (2021).\n[57] Chaohao Xie, Shaohui Liu, Chao Li, Ming-Ming Cheng, Wangmeng Zuo, Xiao\nLiu, Shilei Wen, and Errui Ding. 2019. Image Inpainting With Learnable Bidirec-\ntional Attention Maps. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV) .\n[58] Zongben Xu and Jian Sun. 2010. Image Inpainting by Patch Propagation Using\nPatch Sparsity. IEEE Transactions on Image Processing 19, 5 (2010), 1153â€“1165.\nhttps://doi.org/10.1109/TIP.2010.2042098\n[59] Zhaoyi Yan, Xiaoming Li, Mu Li, Wangmeng Zuo, and Shiguang Shan. 2018.\nShift-Net: Image Inpainting via Deep Feature Rearrangement. In Proceedings of\nthe European Conference on Computer Vision (ECCV) .\n[60] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang. 2018.\nGenerative Image Inpainting With Contextual Attention. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) .\n[61] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang.\n2019. Free-Form Image Inpainting With Gated Convolution. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV) .\n[62] Yingchen Yu, Fangneng Zhan, Rongliang Wu, Jianxiong Pan, Kaiwen Cui, Shijian\nLu, Feiying Ma, Xuansong Xie, and Chunyan Miao. 2021. Diverse image inpaint-\ning with bidirectional and autoregressive transformers. In Proceedings of the 29th\nACM International Conference on Multimedia . 69â€“78.\n[63] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz\nKhan, and Ming-Hsuan Yang. 2021. Restormer: Efficient Transformer for High-\nResolution Image Restoration. arXiv preprint arXiv:2111.09881 (2021).\n[64] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. 2020. Learning Joint Spatial-\nTemporal Transformations for Video Inpainting. In European Conference on Com-\nputer Vision, Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm\n(Eds.). Springer International Publishing.\n[65] Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Baining Guo. 2019. Learning\nPyramid-Context Encoder Network for High-Quality Image Inpainting. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR).\n[66] Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. 2019. Pluralistic Image Comple-\ntion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) .\n[67] Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai, and Dinh Phung. 2022. Bridging\nGlobal Context Interactions for High-Fidelity Image Completion. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) .\n11512â€“11522.\n[68] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.\n2018. Places: A 10 Million Image Database for Scene Recognition. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence 40, 6 (2018), 1452â€“1464.\nhttps://doi.org/10.1109/TPAMI.2017.2723009\n[69] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. Unpaired\nImage-To-Image Translation Using Cycle-Consistent Adversarial Networks. In\nProceedings of the IEEE International Conference on Computer Vision (ICCV) .",
  "topic": "Inpainting",
  "concepts": [
    {
      "name": "Inpainting",
      "score": 0.9063159227371216
    },
    {
      "name": "Computer science",
      "score": 0.7109590172767639
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6379209756851196
    },
    {
      "name": "Transformer",
      "score": 0.6031365394592285
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5919904708862305
    },
    {
      "name": "Computational complexity theory",
      "score": 0.5506923198699951
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5489632487297058
    },
    {
      "name": "Image (mathematics)",
      "score": 0.5069935321807861
    },
    {
      "name": "Image resolution",
      "score": 0.4960564076900482
    },
    {
      "name": "Quadratic equation",
      "score": 0.4517894387245178
    },
    {
      "name": "Face (sociological concept)",
      "score": 0.43266773223876953
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40791255235671997
    },
    {
      "name": "Computer vision",
      "score": 0.37275269627571106
    },
    {
      "name": "Algorithm",
      "score": 0.32220911979675293
    },
    {
      "name": "Mathematics",
      "score": 0.19277027249336243
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I87445476",
      "name": "Xi'an Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    }
  ],
  "cited_by": 41
}