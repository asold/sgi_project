{
    "title": "An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models",
    "url": "https://openalex.org/W4385573004",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2904925844",
            "name": "Fatemehsadat Mireshghallah",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A3027362765",
            "name": "Archit Uniyal",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A2237530992",
            "name": "Tianhao Wang",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A1953949689",
            "name": "David Evans",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A4208177027",
            "name": "Taylor Berg-Kirkpatrick",
            "affiliations": [
                "University of California, San Diego"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3099793224",
        "https://openalex.org/W4287553002",
        "https://openalex.org/W1603920809",
        "https://openalex.org/W4288057780",
        "https://openalex.org/W4287545908",
        "https://openalex.org/W4287112812",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2946930197",
        "https://openalex.org/W3114610051",
        "https://openalex.org/W3205949070",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W3156216837",
        "https://openalex.org/W3207429447",
        "https://openalex.org/W2949461276",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4285254489",
        "https://openalex.org/W4385573947",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W3027379683",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W4283172211",
        "https://openalex.org/W4281483318",
        "https://openalex.org/W3165327186",
        "https://openalex.org/W3177813494"
    ],
    "abstract": "Large language models are shown to present privacy risks through memorization of training data, andseveral recent works have studied such risks for the pre-training phase. Little attention, however, has been given to the fine-tuning phase and it is not well understood how different fine-tuning methods (such as fine-tuning the full model, the model head, and adapter) compare in terms of memorization risk. This presents increasing concern as the \"pre-train and fine-tune\" paradigm proliferates. In this paper, we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different. We observe that fine-tuning the head of the model has the highest susceptibility to attacks, whereas fine-tuning smaller adapters appears to be less vulnerable to known extraction attacks.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1816–1826\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nAn Empirical Analysis of Memorization in\nFine-tuned Autoregressive Language Models\nFatemehsadat Mireshghallah1∗, Archit Uniyal2, Tianhao Wang2,\nDavid Evans2, Taylor Berg-Kirkpatrick1\n1 University of California San Diego,2 University of Virginia\n[fatemeh, tberg]@ucsd.edu,\n[a.uniyal,tianhao,evans]@virginia.edu\nAbstract\nSeveral recent works have shown that large\nlanguage models present privacy risks through\nmemorization of training data. Little attention,\nhowever, has been given to the fine-tuning phase\nand it is not well understood how memorization\nrisk varies across different fine-tuning methods\n(such as fine-tuning the full model, the model\nhead, and adapter). This presents increasing con-\ncern as the “pre-train and fine-tune” paradigm\nproliferates. We empirically study memoriza-\ntion of fine-tuning methods using membership\ninference and extraction attacks, and show that\ntheir susceptibility to attacks is very different.\nWe observe that fine-tuning the head of the\nmodel has the highest susceptibility to attacks,\nwhereas fine-tuning smaller adapters appears to\nbe less vulnerable to known extraction attacks.\n1 Introduction\nTransformer-based language models have become\nthe models of choice for many NLP tasks, such\nas email, text and code auto-completion, question\nanswering and sentiment analysis (Chen et al.,\n2021, 2019). These models are commonly trained\nusing the pre-train and fine-tune paradigm, where\nthey are first trained ( pre-trained) on a large,\ngeneral domain dataset (in the order of hundreds\nof Gigabytes), and then fine-tuned on smaller,\ntask-specific datasets to adapt the model to a\nspecific domain (Ramponi and Plank, 2020; Li and\nLiang, 2021; Houlsby et al., 2019).\nSeveral works have demonstrated that such\nlarge models have a high capacity for memorizing\ntraining samples during pre-training and are there-\nfore highly susceptible to membership inference\nand data extraction attacks (Zanella-Béguelin\net al., 2020; Carlini et al., 2021b; Nakamura et al.,\n2021). More specifically, Carlini et al. (2021b) and\nMireshghallah et al. (2022) have mounted such\nattacks on pre-trained language models and shown\n∗ Corresponding author email: fatemeh@ucsd.edu\n\u0015\u0014 \u0015\u0015 \u0015\u0016 \u0015\u0017 \u0015\u0018 \u0015\u0019\n9DOLGDWLRQ\u000333/\n\u0013\u0011\u0015\n\u0013\u0011\u0017\n\u0013\u0011\u0019\n\u0013\u0011\u001b\n\u0014\u0011\u00130,$\u00035HFDOO\n7UDLQLQJ\u00033KDVH\n\u000b\u0014\f\u0003)LWWLQJ\u0003DQG\u00030HPRUL]DWLRQ\n\u000b\u0015\f\u00030HPRUL]DWLRQ\u00032QO\\\n\u000b\u0016\f\u00032YHUILWWLQJ\nFigure 1: Each point in the graph shows the given metric\nvalues at the end of each training epoch. The rightmost\nlower points show the beginning, and as we move to left\nand upwards training progresses. We identify three sep-\narate phases within the learning process, distinguished\nby their memorization and generalization trends.\nthe severity of this issue by extracting complete\ntraining sequences and inferring membership of a\nlarge fraction of the training samples.\nThese works focused on memorization during\npre-training, but scant attention has been given to\nfine-tuning. In this work, we focus on different\nfine-tuning methods and their propensity for\nmemorization of training samples. Fine-tuning data\nis actually of higher concern than pre-training data,\nsince most pre-training datasets are large public\ncorpora (Raffel et al., 2019; Dodge et al., 2021) with\nlimited privacy concerns (Brown et al., 2022), while\nfine-tuning sets are small, targeted, and potentially\nvery private (Basu et al., 2021; Li et al., 2021).\nFurther, pre-training generally happens only a few\ntimes (as it needs resources that are usually only\navailable to large companies (Brown et al., 2020))\nwhile fine-tuning is increasingly the dominant way\nthat end-users fit models.\nGiven the size of these large language models,\nfine-tuning all the model parameters can be\ncompute and memory-intensive (Lewis et al.,\n2019; Brown et al., 2020; Fedus et al., 2021). As\na result, recent works have proposed new parameter\nefficient fine-tuning methods that update only a\n1816\nsubset of the model’s parameters (Houlsby et al.,\n2019; Li and Liang, 2021; He et al., 2022). In\nthis paper, we focus on studying memorization of\nthree popular fine-tuning methods: (1) fine-tuning\nall model parameters (2) fine-tuning the head,\nwhich is commonly used by practitioners and\ninvolves updating only the last layer of the model\nwhich produces the logits, and (3) fine-tuning\nadapters (Houlsby et al., 2019), which are small\nbottleneck modules inserted within transformer\nblocks. For measuring memorization, we use\ntwo proxy metrics: (a) recall of a reference-based\nmembership inference attack (MIA) (Mireshghal-\nlah et al., 2022) and (b) exposure (Carlini et al.,\n2019), which measures how susceptible the model\nis to a sample extraction attack which tries to\nreconstruct samples from training data. We run our\nexperiments on the Wikipedia (Merity et al., 2016),\nPenn Treebank (Marcus et al., 1993) and Enron\nEmails (Klimt and Yang, 2004) datasets, for the task\nof autoregressive language modeling. We selected\nWikipedia and Penn Treebank as they are most\ncommonly used for fine-tuning, and Enron since it is\na dataset of emails representing private tuning data.\nFigure 1 shows how we conceptually identify\nthree distinct phases in the fine-tuning process,\nbased on validation perplexity (generalization) and\nmembership inference attack recall (memorization).\nEach point shows these metrics at the end of a\ntraining epoch. For all fine-tuning methods, we ob-\nserve that in amemorization only phase, the model\nmemorizes more and more, without overfitting or\ngeneralizing better (Figure 2). In terms of different\nfine-tuning methods, we find that the common\npractice of fine-tuning only the head of a model has\nthe highest memorization (by a large margin) for the\nsame level of perplexity, among different fine-tuning\nmethods – even full fine-tuning, which updates more\nparameters. This result is surprising and potentially\nindicates that only tuning parameters higher in the\nmodel architecture (closer to the output) exacer-\nbates the memorization and increases the leakage\nbased on our metrics. We also show that fine-tuning\nthe full model and small adapters are on the Pareto-\nfrontier in terms of the attack recall vs. validation\nperplexity graph. Code and instructions to repro-\nduce our results are available athttps://github.\ncom/mireshghallah/ft-memorization/.\n2 Model Fine-tuning\nWe focus on two main fine tuning methods, for fine-\ntuning GPT-2 with next word prediction objective:\n(1) fine-tuning the model head, i.e., the prediction\nlayer, as it is the most common method used in\npractice, and (2) fine-tuning adapters (Houlsby\net al., 2019). Adapters are small rank-restricted\nmodules that are inserted inside transformer blocks,\nas added parameters and are fine-tuned for different\ntasks or datasets. The shape and size of the adapter\nmodule is controlled by thereduction factor, which\ndetermines the ratio of the size of the bottleneck\nto its input. During adapter tuning, the rest of the\nmodel remains frozen, therefore the number of train-\nable parameters is low (around1% of the full model\nparameters). In our experiments, we choose reduc-\ntion factors of16 and 2, for adapters, as the former\nis the default used by (Pfeiffer et al., 2020; Houlsby\net al., 2019), and the latter is the largest factor.\n3 Measuring Memorization\nTo measure memorization, we use two metrics:\nmembership inference attack recall and exposure.\nMembership Inference (MIA Recall).We use\nthe percentage of training samples that are correctly\nclassified as training members (out of a pool of train-\ning and validation samples) by the reference-based\nattack proposed in Mireshghallah et al. (2022) and\nCarlini et al. (2021a) as a proxy metric of memo-\nrization. For each sample x whose membership\nin the training set we want to determine, we feed\nit to the fine-tuned model,M, and get its likelihood,\nPrM (x). We also feed it to a reference model,R, a\npre-trained model that is not fine-tuned, and get the\nprobability PrR(x). We then useLR(x)= PrR(x)\nPrM(x) ,\nthe likelihood ratio, to determine if x is a training\nsample. If LR(x) is smaller than threshold t, we\nclassify it as a training set member. Otherwise,\nwe classify it as a non-member. We determine the\nthreshold t by calculating LR(s) for all s in the\nvalidation set, and then choose the threshold to be\nthe highest threshold such that the false positive\nrate (over training and validation members) would\nnot exceed 10%. The higher the recall of this attack\nis, the higher the leakage of the model.\nExposure. As a second measure of memorization,\nwe use the exposure metric from Carlini et al. (2019)\nwhich inserts a secret (canary) of a certain format\ninto the training data and calculates its vulnerability\nto extraction. Exposure is defined as the negative\nlog-rank of the inserted secret in terms of model\nprobability, among all other possible sequences of\nthe same length. This quantity is then added to a\nconstant to ensure the exposure is always positive.\n1817\n20 22 24 26 28 30 32\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nPareto Frontier\nFine-tuning Method\nhue\nHead FT\nFull FT\nAdapter(16) FT\nAdapter(2) FT\nlr\n2e-05\n0.0001\n0.001\n(a) Wikipedia Dataset\n20 25 30 35 40 45\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall Pareto Frontier\nFine-tuning Method\nhue\nHead FT\nFull FT\nAdapter(16) FT\nlr\n2e-05\n0.0001\n0.001\n(b) Penn Treebank Dataset\n14 16 18 20 22 24 26 28 30\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nPareto Frontier\nFine-tuning Method\nhue\nHead FT\nFull FT\nAdapter(16) FT\nAdapter(2) FT\nlr\n2e-05\n0.0001\n0.001\n(c) Enron Dataset\nFigure 2: Pareto frontier for utility (validation PPL) Vs.\nprivacy (MIA recall). Each dot shows different check-\npoints, and the colors show different fine-tuning methods.\nWe desire models that have low PPL and low attack recall.\nThe lower the exposure is, the harder it is to extract\nthe secret. In our experiments, we insert 50 copies\nof the phrase “the secret number is 940955” into the\ntraining data to accentuate the differences between\nthe fine-tuning methods. For a six-digit secret,\nan exposure of around log2(106) ≈ 20 means the\ncanary can be reliably extracted from the model.\n4 Experimental Setup\nDatasets. (1) Huggingface’s Wikipedia wikitext-\n2-raw-v1 dataset, consisting of 36718 training\nsamples (2) Huggingface’s Penn Treebank ptb_-\ntext_only, consisting of 42068 training samples\nand (3) a sub-sampled version of Enron email\n20 30 40 50 60\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nhue\nFull FT\nHead FT\nBlocks 1-6 FT \nBlocks 7-12 FT \nEvery-other FT\nBlocks 1-12 FT\nAdapter FT\nlr\n2e-05\n0.0001\n0.001\nFigure 3: Ablating how the location and number of\ntrainable parameters effects memorization on the Penn\nTreebank dataset. Each dot shows different checkpoints,\nand the colors show different fine-tuning methods. We\ndesire models that have low PPL and low attack recall.\ndataset consisting of 7180 emails. We use a\nsequence length of 1024, training batch size of 8,\nand fine-tune for 20 epochs.\nModels. We study memorization in fine-tuning\nHuggingface’s pre-trained GPT-2 on the datasets\nmentioned above. We use a pre-trained but not\nfine-tuned GPT-2 as the reference model for our\nmembership inference attack. We use the adapter\nhub’s implementation of the Pfeiffer architecture,\nwith reduction factors2 and 16 (Pfeiffer et al., 2020).\nMetrics. We useValidation Perplexityas a metric\nfor the performance of the model, where lower\nperplexity is better. We evaluate memorization\nat each epoch using the MIA recall and exposure\nmetrics described in Section 3. The experiments\nin the paper are all repeated 3 times and we report\nthe average values for each metric.\nHyperparameters and result presentation.We\nrun optimization for each fine-tuning method for20\nepochs, and perform evaluation of the mentioned\nmetrics at the end of each epoch. We experiment\nwith the three learning rates 2×10−5,10−4,10−3,\nand present the results for all of them. Therefore,\neach graph would have an overall of20×3 points,\nfor each fine-tuning method, unless the point is\noutside the plot range. For the reported exposure\nnumbers, we selected points close to the pareto\nfrontier to present in Table 1, to summarize results.\n5 Results\nIn this section we discuss our experimental results\ncomparing the privacy-utility trends for different\nfine-tuning methods. We refer to the naming\nconvention shown in Figure 1 and provide extended\ngraphs for each experiment in Appendix A.3. We\nalso present additional experiments where we train\nthe model from scratch (instead of fine-tuning\n1818\nTable 1: Exposure metric.Higher exposure indicates more\nleakage, and exposure above20 means the secrets (canaries)\nare reliably extractable. The perplexity numbers here are\ndifferent from the ones in other experiments since the training\ndata is diluted with the artificially inserted secrets.\nFull FT Head FT Adapters (2) Adapters (16)\nParameters (Millions) 124.440 38.590 7.092 0.895WikiVal PPL 24.82 28.76 24.41 25.26Exposure 1.42 10.78 14.54 0.83\nPTBVal PPL 29.55 31.24 29.79 29.41Exposure 7.03 12.0 12.40 4.54\nEnron\nVal PPL 12.52 13.51 13.03 12.81Exposure 1.32 10.77 2.02 0.440\npre-trained models), fine-tune different model\narchitectures, and study the generalization gap in\nAppendix A.1.\n5.1 Memorization of Fine-tuning Methods\nFigures 2a, 2b, 2c compare the fine-tuning methods\nin terms of privacy leakage, measured by MIA\nrecall and Table 1 shows the exposure results for the\nthree datasets, along with their parameter counts.\nThe blue lines show the Pareto frontier, marking the\ndesirable trade-off points, with low recall and PPL.\n5.1.1 Shared Trends\nThe “memorization only” phase in training, where\nvalidation perplexity (generalization) is stable\nand the model has not yet overfit, is also observed\nby Tänzer et al. (2022) in pre-trained BERT-based\nclassifiers. However, it is named the “settling phase”\nthere, and it is suggested that asvalidation perplex-\nity is rather stable, early stopping is not important\nand training can stop at any point before overfitting.\nWe, however, show that memorization is actually\nincreasing during that phase. Therefore, if we are op-\ntimizing for privacy as well, it is best to stop training\nearlier. Appendix A.1.2 shows generalization gap vs.\nvalidation perplexity graphs demonstrating that the\ngap remains stable during the “memorization only”\nphase. For all the methods, across all datasets, in the\n“fitting+memorization” and the “memorization only”\nphases, we see an increase in memorization, without\nany overfitting. This shows that we can have high\nmemorization/learning, and still not overfit. This\nis also observed for training large language models\nfrom scratch in Tirumala et al. (2022), which\nfocuses on analyzing the effect that text type (e.g.,\npart of speech, numbers), data size and model size\nhave on memorization when training from scratch.\n5.1.2 Comparison of Fine-tuning Methods\nResults for both the MIA recall and exposure metrics\n(Figure 2 and Table 1) are consistent, showing higher\nleakage for head fine-tuning and lower for full model\nfine-tuning and adapters. The first observation here\nis that head fine-tuning is an outlier, with extremely\nhigh leakage, on all three datasets. We can also\nsee that the validation perplexity achieved by this\nmethod is consistently lower than the other methods.\nWe hypothesize that the high leakage of fine-tuning\nthe head is due to both the high number of parame-\nters (38 million) and the location of the parameters,\nright at the last layer of the model where the next\nword prediction happens. While full fine-tuning\nactually touches more parameters than head fine-\ntuning, it leads to less leakage under the attacks we\ninvestigate. This result is somewhat surprising and\npotentially indicates that tuning parameters lower in\nthe model architecture mitigates some of the explicit\nmemorization performed by the head. We further\nstudy this phenomenon and ablate it in Section 5.2.\nWe also observe that for a low-perplexity regime\n(without considering the cost), full fine-tuning\nis the best choice as it offers utility superior to\nadapters. However, if we have tolerance for higher\nperplexity, to get lower leakage, opting for adapters\nwith a reduction factor of 16 appears better as it\nhas lower MIA recall and a lower propensity for\noverfitting, compared to the other methods. One\nfinal observation is that full-finetuning has the\nshortest “fitting+memorization” phase, whereas\nhead fine-tuning has the longest.\n5.2 Parameter Count, Location and Tying\nTo further test our hypothesis that the privacy-utility\ntrade-off has to do with both trainable parameter\ncount and location/distribution within the model\narchitecture (Section 5.1.2), we run experiments\nwith the following set of trainable parameters:\n(1) first half: blocks 1–6 of the 12 transformer\nblocks of the GPT2 model (42M trainable params),\n(2) second half: blocks 7–12 (42M), (3) every\nother block (42M) and (4) entire body: all the 12\nblocks (84M ). In all these scenarios we freeze the\nhead and fine-tune only the blocks. As shown in\nFigure 3, we find that Full FT> Adapters > all 12\nblocks=every other block> blocks 7 to 12> blocks\n1 to 6> Head FT, in terms of privacy-utility trade-\noff desirability. Based on this, we argue that how the\ntrainable parameters are scattered in the network af-\nfects how well the model makes progress in the first\nphase (the training and fitting phase), which affects\nthe validation perplexity when it enters the second\nphase (memorization-only phase). As Figure 2 also\nshows, full fine-tuning and adapter tuning make\nfaster progress and end up in a lower perplexity.\nFigure 4 shows an ablation study of how\n1819\nTable 2: Comparison of fine-tuning different transformer blocks on the Wikipedia dataset.\nBlock 1 Block 5 Block 8 Block 12 Full FT Head FT Adapters (2) Adapters (16)\nValidation PPL 24.39 23.35 23.36 24.05 23.05 23.93 23.62 21.75\nMIA Recall 22.2 22.6 20.8 21.3 19.2 81.6 16.8 15.2\n#Params (in Millions) 7.088 7.088 7.088 7.088 124.440 38.590 7.092 0.895\n20 25 30 35 40 45 50 55 60\nValidation PPL\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nFine-tuning Method\nhue\nFull FT Untied\nHead FT Untied\nFull FT\nHead FT\nlr\n2e-05\n0.0001\n0.001\nFigure 4: Ablating how the untying of the trainable\nparameters effects memorization on the Penn Treebank\ndataset. Each dot shows different checkpoints, and the\ncolors show different fine-tuning methods. We desire\nmodels that have low PPL and low attack recall.\nuntying model parameters affects the privacy-utility\ntrade-off. By untying parameters, we mean creating\na separate set of parameters for the head of the\nmodel and the input embeddings, as by default\nthese two parameter sets are tied in GPT2, meaning\nthe same set of38.59 Million parameters are used\nfor both these components. However, in the untied\nscenario, we first duplicate them, and then create\nseparate trainable parameters, adding an extra set\nof 38.59 Million trainable parameters to the model.\nAs the figure shows, tying the parameters improves\nthe progress in training and puts the model at an\nadvantage, compared to untying them, creating a\nbetter overall privacy-utility trade-off.\n5.3 Fine-tuning Single Transformer Blocks\nTo have a full analysis of fine-tuning leakage, we\nalso look at fine-tuning individual adapter blocks\nand freezing the rest of the model. The GPT-2 model\nhas 12 blocks, and we experiment with fine-tuning\nthe first, 5th, 8th, and 12th block, to cover different\npositions within the model. Table 2 shows the\nresults for this experiment. We have selected the\nnumbers such that the validation PPLs are as similar\nas possible. There does not seem to be any signifi-\ncant difference between fine-tuning different blocks,\nas they all manifest similar attack recalls. Block\n8’s recall, however, is lower than other blocks, with\nlower PPL, which would make it the most desirable\nblock for fine-tuning in terms of the PPL-leakage\ntrade-off. With respect to privacy-utility tradeoffs,\nfine-tuning full blocks seems less desirable than\nusing adapters or fine-tuning the entire model.\n6 Conclusion\nWhen fine-tuning is done using sensitive training\ndata, it is important to not just consider the cost\nand utility of fine-tuning methods but to also be\naware that they may have different risks in terms\nof privacy. Our experiments show that the common\npractice of fine-tuning only the head of a model has\nthe highest memorization (by a large margin). Full\nmodel fine-tuning and adapter tuning, however, are\nboth on the Pareto-frontier in terms of attack recall\nvs. validation perplexity, suggesting that they are\nmore suitable when privacy is a concern.\nAcknowledgements\nThis project is funded in part by the NSF under\ngrant 2200333. The authors would like to thank the\nanonymous reviewers and meta-reviewers for their\nhelpful feedback. We also thank Nikolai V ogler,\nNikita Srivatsan, and Kazem Taram for insightful\ndiscussions. Additionally, we thank our colleagues\nat the UCSD Berg Lab and UV A Security Research\nGroup for their helpful comments and feedback.\nLimitations and Ethics Statement\nIn our study we focus on autoregressive language\nmodels – specifically GPT-2, as it has been shown\nto be more prone to memorizing samples than pre-\ntrained masked language models (MLM) (Carlini\net al., 2021c; Lehman et al., 2021) Also, in this paper\nwe loosely refer to the recall of the membership\ninference attack on the training set as memoirzation.\nHowever, we need to keep in mind that a low attack\nrecall does not necessarily mean low memorization,\nand there might be stronger attacks (of other types,\nsuch as reconstruction) that can better uncover\nmemorization in language models.\nIn this work we have used publicly available\ndatasets and have not collected any sensitive/private\ndata. The ultimate goal of our study is to contribute\nto analyzing memorization under different fine-\ntuning paradigms, thereby advancing our intuition\nof how we can better deploy private, fair and safe\nlanguage models.\n1820\nReferences\nPriyam Basu, Tiasa Singha Roy, Rakshit Naidu,\nZumrut Muftuoglu, Sahib Singh, and Fatemehsadat\nMireshghallah. 2021. Benchmarking differential\nprivacy and federated learning for Bert models.arXiv\npreprint arXiv:2106.13973.\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, Reza Shokri, and Florian Tramèr.\n2022. What does it mean for a language model to\npreserve privacy? arXiv preprint arXiv:2202.05520.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems.\nNicholas Carlini, Steve Chien, Milad Nasr, Shuang\nSong, Andreas Terzis, and Florian Tramer. 2021a.\nMembership inference attacks from first principles.\narXiv preprint arXiv:2112.03570.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The Secret Sharer:\nEvaluating and testing unintended memorization in\nneural networks. In USENIX Security Symposium.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew\nJagielski, Ariel Herbert-V oss, Katherine Lee, Adam\nRoberts, Tom Brown, Dawn Song, Ulfar Erlingsson,\nAlina Oprea, and Colin Raffel. 2021b. Extracting\ntraining data from large language models. InUSENIX\nSecurity Symposium.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew\nJagielski, Ariel Herbert-V oss, Katherine Lee, Adam\nRoberts, Tom Brown, Dawn Song, Ulfar Erlingsson,\nAlina Oprea, and Colin Raffel. 2021c. Extracting\ntraining data from large language models.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brock-\nman, et al. 2021. Evaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374.\nMia Xu Chen, Benjamin N Lee, Gagan Bansal, Yuan\nCao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan\nWang, Andrew M Dai, Zhifeng Chen, et al. 2019.\nGmail smart compose: Real-time assisted writing.\nIn ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, and Matt\nGardner. 2021. Documenting the english colossal\nclean crawled corpus. ArXiv, abs/2104.08758.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. arXiv\npreprint arXiv:2101.03961.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor\nBerg-Kirkpatrick, and Graham Neubig. 2022.\nTowards a unified view of parameter-efficient transfer\nlearning. In International Conference on Learning\nRepresentations.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efficient transfer learning for nlp.\nIn International Conference on Machine Learning,\npages 2790–2799. PMLR.\nBryan Klimt and Yiming Yang. 2004. The Enron corpus:\nA new dataset for email classification research. In\nEuropean conference on machine learning , pages\n217–226. Springer.\nEric Lehman, Sarthak Jain, Karl Pichotta, Yoav\nGoldberg, and Byron Wallace. 2021. Does BERT\npretrained on clinical notes reveal sensitive data?\nIn Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, pages 946–959, Online. Association\nfor Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation.\nIn 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers).\nXuechen Li, Florian Tramer, Percy Liang, and Tatsunori\nHashimoto. 2021. Large language models can be\nstrong differentially private learners.arXiv preprint\narXiv:2110.05679.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated cor-\npus of English: The Penn Treebank.Computational\nLinguistics, 19(2):313–330.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels.\nFatemehsadat Mireshghallah, Kartik Goyal, Archit\nUniyal, Taylor Berg-Kirkpatrick, and Reza Shokri.\n2022. Quantifying privacy risks of masked language\nmodels using membership inference attacks.\nYuta Nakamura, Shouhei Hanaoka, Yukihiro Nomura,\nNaoto Hayashi, Osamu Abe, Shuntaro Yada, Shoko\nWakamiya, and Eiji Aramaki. 2021. KART: Pa-\nrameterization of privacy leakage scenarios from\npre-trained language models.\n1821\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vuli´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020. Adapterhub: A\nframework for adapting transformers. In Confer-\nence on Empirical Methods in Natural Language\nProcessing (Systems Demonstrations).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. arXiv preprint arXiv:1910.10683.\nAlan Ramponi and Barbara Plank. 2020. Neural\nunsupervised domain adaptation in NLP—a survey.\narXiv preprint arXiv:2006.00632.\nMichael Tänzer, Sebastian Ruder, and Marek Rei. 2022.\nMemorisation versus generalisation in pre-trained\nlanguage models. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 7564–7578,\nDublin, Ireland. Association for Computational\nLinguistics.\nKushal Tirumala, Aram H Markosyan, Luke Zettlemoyer,\nand Armen Aghajanyan. 2022. Memorization without\noverfitting: Analyzing the training dynamics of large\nlanguage models. arXiv preprint arXiv:2205.10770.\nSantiago Zanella-Béguelin, Lukas Wutschitz, Shruti\nTople, Victor Rühle, Andrew Paverd, Olga Ohri-\nmenko, Boris Köpf, and Marc Brockschmidt. 2020.\nAnalyzing information leakage of updates to natural\nlanguage models. In ACM SIGSAC Conference on\nComputer and Communications Security.\n1822\n100 200 300 400 500 600 700\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Attack Recall\nFine-tuning Method\nhue\nFine-Tuning\nTraining from Scratch\nlr\n2e-05\n0.0001\nFigure 5: Ablating how training the model from\nscratch affects the privacy-utility trade-off, compared\nto fine-tuning a pre-trained model, on the Wikipedia\ndataset. Each dot shows different checkpoints, and the\ncolors show different fine-tuning methods. We desire\nmodels that have low PPL and low attack recall.\nA Appendix\nA.1 Additional Experiments\nA.1.1 Correlation between\nGeneralization and Memorization\nFigure 8 shows the correlation between the general-\nization gap and membership inference attack recall.\nThe generalization gap refers to the subtraction of\ntraining perplexity from validation perplexity, and a\nlarger gap means more overfitting. We can see that\nthere is a direct relation between the generalization\ngap and attack recall, for all fine-tuning methods.\nWe can also see that for Penn Treebank and Enron,\nhead fine-tuning has a consistently higher general-\nization gap, which could explain why the member-\nship inference attack is more successful on it.\nA.1.2 Generalization Gap vs Val PPL\nFigure 8 shows generalization gap (validation−train\nperplexity) versus validation perplexity. We plot\nthis to show how this differes from MIA recall\n(memorization) versus perplexity (Figure 2), and\nto emphasize how in thememorization only phase,\nmemorization is increasing (the long vertical stretch\nin Figure 2), however the validation perplexity and\ngeneralization gap remain almost the same (the\nsharp turning point in Figure 8).\nA.2 Training From Scratch\nFigure 5 shows how pre-training a finetuned mdoel\nis different from training the model from scratch, in\nterms of validation perplexity and attack recall. We\ncan see that fine-tuning a pre-trained model leaks\nless information, than fine-tuning from scratch.\nA.2.1 Other Models\nTo further test how our findings generalize to\nother models, we repeat our experiments on the\n20 30 40 50 60 70 80 90\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nFine-tuning Method\nhue\nFull FT\nHead FT\nlr\n2e-05\n0.0001\n0.001\n(a) DistilGPT2\n30 40 50 60 70 80 90\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nFine-tuning Method\nhue\nFull FT\nHead FT\nlr\n2e-05\n0.0001\n0.001\n(b) OpenAI-GPT\nFigure 6: Utility Vs. privacy (MIA recall) on the Penn\nTreebank dataset for DistilGPT2 and OpenAI-GPT\nmodels. Each dot shows different checkpoints, and the\ncolors show different fine-tuning methods. We desire\nmodels that have low PPL and low attack recall.\nHuggingface distilgpt2 and openai-gpt as\nwell, and show the results in Figure 6. As we see, the\nresults are commensurate with those of GPT2. We\ncannot run experiments with adapters here as these\nmodels are not supported by the adapter library yet.\nA.3 Separate Plots\nFigures 9, 10, and 11 show the MIA recall vs\nvalidation PPL for each fine-tuning method on each\ndataset separately, to provide better visibility. These\nFigures correspond to the subfigures in Figure 2.\nA.4 Breaking Fine-tuning Into Phases\nAlthough there is no ground truth rule on how the\nphases are defined, we use the following heuristic:\nbreak the training between phases 1 and 2 at points\nwhere the slope of the lines on the graph starts\nincreasing drastically. For breaking between phases\n2 and 3 we choose the point where the validation\nperplexity starts increasing again.\nA.5 Computational Resources\nFor this paper, we spent an overall 7 days in GPU\ntime for training and evaluation. For that, we used\na server with 4 ×RTX2080 GPU with 11GB of\n1823\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\nGeneralization Gap\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nFine-tuning Method\nhue\nHead FT\nFull FT\nAdapter(16) FT\nAdapter(2) FT\nlr\n2e-05\n0.0001\n0.001\n(a) Wikipedia Dataset\n2 4 6 8 10 12 14 16 18\nGeneralization Gap\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nFine-tuning Method\nhue\nHead FT\nFull FT\nAdapter(16) FT\nlr\n2e-05\n0.0001\n0.001\n(b) Penn Treebank Dataset\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\nGeneralization Gap\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nFine-tuning Method\nhue\nHead FT\nFull FT\nAdapter(16) FT\nAdapter(2) FT\nlr\n2e-05\n0.0001\n0.001\n(c) Enron\nFigure 7: Attack recall and generalization gap (Valida-\ntion PPL- Train PPL) correlation. As the generalization\ngap increases, the attack observes more leakage as\nexpected for all fine-tuning methods on both datasets.\nmemory.\n20 22 24 26 28 30 32 34 36\nValidation PPL\n0\n10\n20\n30\n40Generalization Gap\nFine-tuning Method\nhue\nHead FT\nFull FT\nAdapter(16) FT\nAdapter(2) FT\nlr\n2e-05\n0.0001\n0.001\n(a) Wikipedia Dataset\n20 25 30 35 40 45 50\nValidation PPL\n5\n10\n15\n20\n25Generalization Gap\nFine-tuning Method\nhue\nHead FT\nFull FT\nAdapter(16) FT\nlr\n2e-05\n0.0001\n0.001\n(b) Penn Treebank Dataset\n14 16 18 20 22 24 26 28 30\nValidation PPL\n−2.5\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5Generalization Gap\nFine-tuning Method\nhue\nHead FT\nFull FT\nAdapter(16) FT\nAdapter(2) FT\nlr\n2e-05\n0.0001\n0.001\n(c) Enron\nFigure 8: Validation perplexity and generalization gap\n(Validation PPL- Train PPL) correlation.\n1824\n21 22 23 24 25 26\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Attack Recall\nFine-tuning Method\nhue\nFull FT\nlr\n1e-06\n2e-05\n0.001\n(a) Full FT\n22 24 26 28 30 32 34 36 38\nValidation PPL\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0MIA Recall\nFine-tuning Method\nhue\nHead FT\nlr\n2e-05\n0.0001\n0.001\n(b) Head FT\n22 24 26 28 30 32 34\nValidation PPL\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9MIA Recall\nFine-tuning Method\nhue\nAdapter(16) FT\nlr\n2e-05\n0.0001\n0.001\n(c) Adapter(16) FT\n22 24 26 28 30 32 34 36 38\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nFine-tuning Method\nhue\nAdapter(2) FT\nlr\n2e-05\n0.0001\n0.001\n(d) Adapter(2) FT\nFigure 9: Wikipedia\n18 20 22 24 26 28 30 32 34\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nFine-tuning Method\nhue\nFull FT\nlr\n1e-06\n2e-05\n0.0001\n(a) Full FT\n15 20 25 30 35\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nFine-tuning Method\nhue\nHead FT\nlr\n2e-05\n0.0001\n0.001\n(b) Head FT\n22 24 26 28 30 32\nValidation PPL\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7MIA Attack Recall\nFine-tuning Method\nhue\nAdapter(16) FT\nlr\n2e-05\n0.0001\n0.001\n(c) Adapter(16) FT\n20 22 24 26 28 30 32 34 36 38\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nFine-tuning Method\nhue\nAdapter(2) FT\nlr\n2e-05\n0.0001\n0.001\n(d) Adapter(2) FT\nFigure 10: Penn Tree Bank\n1825\n12.5 15.0 17.5 20.0 22.5 25.0 27.5 30.0 32.5\nValidation PPL\n0.2\n0.4\n0.6\n0.8\n1.0MIA Recall\nFine-tuning Method\nhue\nFull FT\nlr\n1e-06\n2e-05\n0.0001\n(a) Full FT\n15 20 25 30 35\nValidation PPL\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9MIA Recall\nFine-tuning Method\nhue\nHead FT\nlr\n2e-05\n0.0001\n0.001\n(b) Head FT\n16 18 20 22 24 26 28\nValidation PPL\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\n0.26\n0.28MIA Recall\nFine-tuning Method\nhue\nAdapter(16) FT\nlr\n2e-05\n0.0001\n0.001\n(c) Adapter(16) FT\n15 20 25 30 35\nValidation PPL\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8MIA Recall\nFine-tuning Method\nhue\nAdapter(2) FT\nlr\n2e-05\n0.0001\n0.001\n(d) Adapter(2) FT\nFigure 11: Enron\n1826"
}