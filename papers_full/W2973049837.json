{
  "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation",
  "url": "https://openalex.org/W2973049837",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4281371573",
      "name": "Keskar, Nitish Shirish",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288384363",
      "name": "McCann, Bryan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224139139",
      "name": "Varshney, Lav R.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751801181",
      "name": "Xiong, Caiming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176399732",
      "name": "Socher, Richard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2969286400",
    "https://openalex.org/W2971034336",
    "https://openalex.org/W2971671202",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W2949776890",
    "https://openalex.org/W2950169745",
    "https://openalex.org/W2948036864",
    "https://openalex.org/W2962849707",
    "https://openalex.org/W2798664956",
    "https://openalex.org/W2911599914",
    "https://openalex.org/W2964040452",
    "https://openalex.org/W2974593375",
    "https://openalex.org/W2970279348",
    "https://openalex.org/W1977885091",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W2963430447",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W2514713644",
    "https://openalex.org/W2580723344",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2341401723",
    "https://openalex.org/W3158986179",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2971216715",
    "https://openalex.org/W2787887017",
    "https://openalex.org/W2950726992",
    "https://openalex.org/W2626792426",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2945697643",
    "https://openalex.org/W2314691132",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2503476152",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2972680990",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963842982",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2609826708",
    "https://openalex.org/W2612431505",
    "https://openalex.org/W2952862139",
    "https://openalex.org/W2061272101",
    "https://openalex.org/W2100967449",
    "https://openalex.org/W2739748921",
    "https://openalex.org/W2951941802",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2960955628",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2789543585",
    "https://openalex.org/W2971996420",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2947813521",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2027731328",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2912018747",
    "https://openalex.org/W2766182427",
    "https://openalex.org/W2963226019",
    "https://openalex.org/W2940024477",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2971779293",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2963026768"
  ],
  "abstract": "Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.",
  "full_text": "CTRL: A C ONDITIONAL TRANSFORMER LANGUAGE\nMODEL FOR CONTROLLABLE GENERATION\nNitish Shirish Keskar∗, Bryan McCann ∗, Lav R. Varshney, Caiming Xiong, Richard Socher\nSalesforce Research†\nABSTRACT\nLarge-scale language models show promising text generation capabilities, but\nusers cannot easily control particular aspects of the generated text. We release\nCTRL, a 1.63 billion-parameter conditional transformer language model, trained\nto condition on control codes that govern style, content, and task-speciﬁc behav-\nior. Control codes were derived from structure that naturally co-occurs with raw\ntext, preserving the advantages of unsupervised learning while providing more\nexplicit control over text generation. These codes also allow CTRL to predict\nwhich parts of the training data are most likely given a sequence. This provides\na potential method for analyzing large amounts of data via model-based source\nattribution. We have released multiple full-sized, pretrained versions of CTRL at\nhttps://github.com/salesforce/ctrl.\n1 I NTRODUCTION\nWith enough data, model capacity, and compute, generative models can learn distributions powerful\nenough to produce high-quality samples from complex domains. In computer vision, the advent\nof generative adversarial networks (Goodfellow et al., 2014) improved image generation. Much\nresearch then focused on methods for controlling the generation process and improving estimation\nof generative distributions (Arjovsky et al., 2017; Chen et al., 2016; Kingma & Welling, 2013).\nIn natural language processing, language models are often trained as conditional language models\nfor speciﬁc tasks that require text generation (Brants et al., 2007; Sutskever et al., 2014; Rush et al.,\n2015). They are also used as a means of learning word vectors (Mikolov et al., 2013), document\nvectors (Kiros et al., 2015), or contextualized word vectors (McCann et al., 2017; Peters et al., 2018;\nDevlin et al., 2018) for transfer learning. The language models themselves have been transferred\nto new tasks through ﬁne-tuning as well (Dai & Le, 2015; Radford et al., 2018; Howard & Ruder,\n2018). Less is understood about generation that is not constrained to any speciﬁc task. Typically\nprompts generated by models (Fan et al., 2018) or written by humans can only be used to provide a\nrough guide or starting point for the generated text. This raises the question of how text generation\ncan be controlled more explicitly.\nInspired by the degree of control available in image generation as well as the recent progress in text\ngeneration (Radford et al., 2019) and multitask learning McCann et al. (2018), we train a language\nmodel that is conditioned on a variety of control codes (Pfaff, 1979; Poplack, 1980) that make desired\nfeatures of generated text more explicit. With 1.63 billion parameters, our Conditional Transformer\nLanguage (CTRL) model can generate text conditioned on control codes that specify domain, style,\ntopics, dates, entities, relationships between entities, plot points, and task-related behavior. To pre-\nserve the generality of the language model trained in an unsupervised setting, we train CTRL on\ncontrol codes derived from structure that naturally co-occurs with the raw text typically collected\nfor training large language models. For example, large resources like Wikipedia, Project Gutenberg,\nand Amazon Reviews can each be assigned a domain-related control code. Smaller resources, like\nthe content extracted from individual subreddits, often occur with both a broader domain name,\nreddit, as well as subdomain information, r/subdomain. In the vast majority of cases, text\ncollected for training is associated with a URL, which often contains information pertinent to the\n∗Equal contribution.\n†Contact: ctrl-monitoring@salesforce.com\n1\narXiv:1909.05858v2  [cs.CL]  20 Sep 2019\ntext it represents. Humans can use these codes to trigger generation of text from different linguistic\ncommunities without having to understand how to prompt with particular linguistic patterns. Text\ncan be generated in more predictable ways by controlling for content or changing the domain even\nwhen the initial prompt remains ﬁxed.\nBecause all control codes can be traced back to a particular subset of the training data, CTRL can\nbe used to predict the subset of training data that is most likely given a sequence. This explicit\nrelationship between CTRL and its training data can be exploited to analyze the correlations that the\nlanguage model has learned from each domain, and it provides a means of studying large amounts\nof text through the language model.\nThese control codes also allow for the straightforward inclusion of task-speciﬁc data in a way that\nimproves important skills without harming the generality of the model. Control codes for question\nanswering and machine translation make these skills easily accessible with CTRL. These codes can\nbe combined with codes during generation to create novel cross-over between control codes that are\ntask-speciﬁc behavior and those that are related to domain and content.\nIn order to push towards more controllable, general models for natural language processing, we\nhave released multiple full-sized, pretrained versions of CTRL at https://github.com/\nsalesforce/ctrl. We hope that the release leads to further research into how controllable\ngeneration can enhance natural language understanding.\n2 L ANGUAGE MODELING\nGiven example sequences of the form x = (x1,...,x n) where each xi comes from a ﬁxed set of\nsymbols, the goal of language modeling is to learn p(x). Because xis a sequence, it is natural to\nfactorize this distribution using the chain rule of probability (Bengio et al., 2003):\np(x) =\nn∏\ni=1\np(xi|x<i)\nThis decomposes language modeling into next-word prediction. Current state-of-the-art meth-\nods (Dai et al., 2019; Radford et al., 2019) train a neural network with parameters θ to minimize\nthe negative log-likelihood over a dataset D= {x1,...,x |D|}:\nL(D) = −\n|D|∑\nk=1\nlog pθ(xk\ni|xk\n<i)\nBecause language models learn pθ(xi|x<i), a new ˜xof length mcan be generated by sequentially\nsampling its constituent symbols: pθ(x0),pθ(x1|˜x0),...,p θ(xm|˜x<m).\n3 L ANGUAGE MODELING WITH CTRL\nCTRL is a conditional language model that is always conditioned on a control code cand learns the\ndistribution p(x|c). The distribution can still be decomposed using the chain rule of probability and\ntrained with a loss that takes the control code into account.\np(x|c) =\nn∏\ni=1\np(xi|x<i,c) L(D) = −\n|D|∑\nk=1\nlog pθ(xk\ni|xk\n<i,ck)\nThe control code cprovides a point of control over the generation process. This is true even when\nsampling x0, in contrast to the traditional language modeling framework described in Sec. 2.\nCTRL learns pθ(xi|x<i,c) by training on sequences of raw text prepended with control codes. Af-\nter minimal preprocessing (described in Sec. 3.2), a single example sequence containing ntokens\nis embedded as a sequence of ncorresponding vectors in Rd. Each vector is the sum of a learned\n2\ntoken embedding and a sinusoidal positional embedding as in the original Transformer architec-\nture (Vaswani et al., 2017). This sequence of vectors is stacked into a matrix X0 ∈Rn×d so that it\ncan be processed by lattention layers (Vaswani et al., 2017). The ith layer consists of two blocks,\neach of which preserves the model dimension d.\nThe core of the ﬁrst block is multi-head attention with kheads that uses a causal mask to preclude\nattending to future tokens:\nAttention(X,Y,Z ) = softmax\n(mask(XY⊤)√\nd\n)\nZ\nMultiHead(X,k) = [h1; ··· ; hk]Wo\nwhere hj = Attention(XW1\nj,XW 2\nj,XW 3\nj)\nThe core of the second block is a feedforward network with ReLU activation (Nair & Hinton, 2010)\nthat projects inputs to an inner dimension f, with parameters U ∈Rd×f and V ∈Rf×d:\nFF(X) = max(0,XU )V\nEach block precedes core functionality with layer normalization (Ba et al., 2016; Child et al., 2019)\nand follows it with a residual connection (He et al., 2016). Together, they yield Xi+1:\nBlock 1 Block 2\n¯Xi = LayerNorm(Xi) ¯Hi = LayerNorm(Hi)\nHi = MultiHead( ¯Xi) + ¯Xi Xi+1 = FF( ¯Hi) + ¯Hi\nScores for each token in the vocabulary are computed from the output of the last layer:\nScores(X0) = LayerNorm(Xl)Wvocab\nDuring training, these scores are the inputs of a cross-entropy loss function. During generation, the\nscores corresponding to the ﬁnal token are normalized with a softmax, yielding a distribution for\nsampling a new token.\n3.1 D ATA\nWe train on 140 GB of text drawing from a wide variety of domains: Wikipedia (En, De, Es, Fr),\nProject Gutenberg1, submissions from 45 subreddits, OpenWebText 2, a large collection of news\ndata (Hermann et al., 2015; Barrault et al., 2019; Sandhaus, 2008; Grusky et al., 2018), Amazon\nReviews (McAuley et al., 2015), Europarl and UN data from WMT (En-De, En-Es, En-Fr) (Barrault\net al., 2019), question-answer pairs (no context documents) from ELI5 (Fan et al., 2019) and the\nMRQA shared task 3, which includes the Stanford Question Answering Dataset (Rajpurkar et al.,\n2016), NewsQA (Trischler et al., 2016), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al.,\n2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). A full\naccount of training data and associated control codes can be found in Table 7 in the Appendix.\n3.2 E XPERIMENTAL SETTINGS\nWe learn BPE (Sennrich et al., 2015) codes and tokenize the data using fastBPE 4, but we use a\nlarge vocabulary of roughly 250K tokens. This includes the sub-word tokens necessary to mitigate\nproblems with rare words, but it also reduces the average number of tokens required to generate long\ntext by including most common words. We use English Wikipedia and a 5% split of our collected\nOpenWebText data for learning BPE codes. We also introduce an unknown token so that during\n1We use a modiﬁed version of https://github.com/chiphuyen/lazynlp\n2We use a modiﬁed version of https://github.com/jcpeterson/openwebtext.git\n3https://github.com/mrqa/MRQA-Shared-Task-2019\n4https://github.com/glample/fastBPE\n3\npreprocessing we can ﬁlter out sequences that contain more than2 unknown tokens. This, along with\nthe compressed storage for efﬁcient training (TFRecords) (Abadi et al., 2016), reduces our training\ndata to 140 GB from the total 180 GB collected. Data was treated as a single stream of tokens with\nnon-domain control codes inserted where appropriate (often at document boundaries). The stream\nwas chunked into contiguous sequences of tokens. Each sequence originated from a domain, and\nit has the corresponding domain control code prepended as the ﬁrst token in the sequence. In this\nway, domain control codes receive special treatment (Kobus et al., 2016). They are propagated to\nall text in the domain as the ﬁrst token. This is similar to how codes and natural language sequences\nhave been used in multi-task settings (Wu et al., 2016; Johnson et al., 2017; McCann et al., 2018) to\ncontrol conditional language models. All other control codes are injected into the data without such\nspecial treatment (Moryossef et al., 2019; Caswell et al., 2019). We experimented with sequence\nlengths of 256 and 512 due to memory and optimization constraints. Despite training on relatively\nshort sequences compared to other approaches, we found that a sliding-window approach allows for\ngeneration beyond these windows, and we also found little difference in quality between the two\nmodels within the ﬁrst 256 tokens. Further, we note that our vocabulary is approximately 4 times\nlarger than similar approaches, hence the effective sequence length in characters is comparable.\nCTRL has model dimensiond= 1280, inner dimensionf = 8192, 48 layers, and 16 heads per layer.\nDropout with probability0.1 follows the residual connections in each layer. Token embeddings were\ntied with the ﬁnal output embedding layer (Inan et al., 2016; Press & Wolf, 2016).\nCTRL was implemented in TensorFlow (Abadi et al., 2016) and trained with a global batch size of\n1024 distributed across 256 cores of a Cloud TPU v 3 Pod for 800k iterations. Training took ap-\nproximately 2 weeks using Adagrad (Duchi et al., 2011) with a linear warmup from 0 to 0.05 over\n25k steps. The norm of gradients were clipped to 0.25 as in (Merity et al., 2017). Learning rate\ndecay was not necessary due to the monotonic nature of the Adagrad accumulator. We compared to\nthe Adam optimizer (Kingma & Ba, 2014) while training smaller models, but we noticed compa-\nrable convergence rates and signiﬁcant memory savings with Adagrad. We also experimented with\nexplicit memory-saving optimizers including SM3 (Anil et al., 2019), Adafactor (Shazeer & Stern,\n2018), and NovoGrad (Ginsburg et al., 2019) with mixed results.\n4 C ONTROLLABLE GENERATION\n4.1 S AMPLING\nTypically, temperature-controlled stochastic sampling methods are used for generating text from a\ntrained language model. It is also common to limit the sampling only to the top- k alternatives.\nGiven a temperature T >0 and scores xi ∈Rd for each token iin the vocabulary, the probability\nof predicting the ith token is given by:\npi = exp(xi/T)∑\njexp(xj/T). (1)\nThe next token is then chosen by sampling through a multinomial distribution with probabilities pi\nclipped at the top-ktokens. In the equation above, T →0 approximates a greedy distribution which\nmagniﬁes the peaks in the probability distribution while T →∞ ﬂattens the distribution to make\nit more uniform. Rather than choosing a ﬁxed value of k, as is common practice, Holtzman et al.\n(2019) suggested adapting k heuristically. The nucleus sampling approach chooses a probability\nthreshold pt and sets kto be the lowest value such that ∑\nisort(pi) > pt. If the model is conﬁdent\nin its next-word prediction, then k will be lower and vice versa. Despite the improved generative\ncapabilities of models with such heuristics, there still exists a trade-off between these parameters\ndepending on the generation intended.\nGiven a prompt: Q: What is the capital of Australia?, a well-trained model as-\nsigns higher probability mass to the correct answer, Canberra, but a non-zero probability mass to\nother cities such as Melbourne, Sydney, Brisbane, Darwin, and Perth, see Figure 1. By choosing to\nsample, we mistrust the model, despite it being correct. A natural solution to this is to choose the\nnext token greedily. However, this is known to create repetitions of phrases or sentences even for\nlarge well-trained models (Radford et al., 2019; Holtzman et al., 2019). To reconcile the two, we\npropose a new sampling scheme that trusts the model distribution through near-greedy sampling but\n4\nCanberraMelbourneSydney Perth DarwinAdelaideHobartBrisbane\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25probability of next token\nPrompt - Q: What is the capital of Australia? \n A: \n(a)\nNeil Buzz Apollo Alan Albert Edwin John Eugene\n0.00\n0.05\n0.10\n0.15probability of next token\nPrompt - Q: Who was the first man on the moon? \n A: (b)\nFigure 1: Next-token probability for the prompts Q: What is the capital of\nAustralia? and Q: Who was the first man on the moon? In such cases, sampling\nusing a distribution is detrimental to answering the question correctly.\nprevents repetitions through a penalty. This penalized sampling works by discounting the scores of\npreviously generated tokens. The motivation is similar to coverage mechanisms (See et al., 2017)\nand other losses designed to discourage repetition (Welleck et al., 2019), but penalized sampling is\nnot used during training. Given a list of generated tokens g, using the notation from equation 1, the\nprobability distribution pi for the next token is deﬁned as:\npi = exp(xi/(T ·I(i∈g))∑\njexp(xj/(T ·I(j ∈g)) I(c) = θif c is True else 1\nWe ﬁnd that using a greedy sampling andθ≈1.2 yields a good balance between truthful generation\nand lack of repetition. Note that θ = 1 is equivalent to equation 1. We note in passing that this\napproach succeeds only if the model has learned a sufﬁciently reliable distribution.\n4.2 C ONTROL CODES\nStyle by domain. Most control codes for our model specify the overall style of generated text\nby indicating a particular domain of training data. Examples in Table 1 demonstrate that even for\nidentical prompts, control codes allow for predictable variation in generation. The examples in\nTable 2 show how CTRL can generate domain-speciﬁc text without any prompt.\nMore complex control codes. Additional control codes can be added to the domain code in or-\nder to increasingly constrain generation. In Table 2, adding additional control codes following the\ndomain code further constrains generation. These examples demonstrate constraints specifying that\nthe model should start with a title and by specifying a particular rating for reviews.\nExamples of more advanced control are given in Table 3. In our version of OpenWebText, we in-\nclude the URL used to downlaod each document as the start of the input sequence. During training,\nCTRL learns relationships between the structure of these URLs and the text that follows. At infer-\nence, novel URLs can be used to specify a variety of features: domain, subdomain, entities, entity\nrelations, and even dates.\nTriggering speciﬁc tasks. A small number of control codes are related to speciﬁc tasks like ques-\ntion answering and translation. These codes constrain the generation process the most, by triggering\ntask-speciﬁc generation. In Table 4, we demonstrate relatively complex control codes for question\nanswering and machine translation that act as a template mixed with a natural language prompt.\nZero-shot code-mixing. In the ﬁrst example we mix a diet subreddit (r/keto) with machine trans-\nlation control codes for English and German. In contrast to usingTranslation in 2, the generated\ntext with mixed codes is coherent across multiple translated lines. This structure is an inﬂuence of\nDiet because it had multiline examples in the training data, whereas the translation data consisted of\nshufﬂed single lines. In the second example we mix the politics subreddit (r/politics) with a prompt\nthat starts in French though no examples of this kind were found in the training data.\n5\nWikipedia Anarchism is a political philosophy that advocates the abolition of all forms of hierarchy\nand domination, including capitalism, patriarchy, racism, sexism, heterosexism and other oppressive\nsocial structures.\\n\\nThe term ”anarchism” was coined by Pierre-Joseph Proudhon in his book ”The\nPoverty of Philosophy” (1844). It has been used to describe various movements within anarchism\nsince then. In its modern sense, it refers to an ideology or movement advocating for social, politi-\ncal, economic and/or cultural change through direct action against existing institutions and practices.\nAnarchists are often associated with radical left-wing politics, but they have also advocated many dif-\nferent types of libertarian socialism. Some anarchists advocate anarchist communism, others support\nanarcho-syndicalism, while still others reject both socialism and communism. Many anarchists believe\nthat capitalism should be abolished entirely, though some argue that this would require overthrow-\ning capitalism. Other anarchists oppose any form of state control over private property, such as land\nownership, which they see as inherently oppressive. They may favor limited government regulation\non certain aspects of life, such as drug use, prostitution, gambling, pornography, abortion, euthanasia,\netc.\\n\\nHistory.\\n\\nEarly history.\\n\\nAnarchist ideas were ﬁrst developed during the French Rev-\nolution (17891799). During these years, there was widespread opposition to traditional society based\nupon religion, class, gender, race, ethnicity, nationality, language, education, wealth, and so on. This\nled to the development of new ideologies like socialism, liberalism, republicanism, communitarianism,\nindividualism/egoism/. . .\nBooks Anarchism is the \\n only true and practical form of Socialism. It has been said that Socialism\n\\n will never be established until it can be proved to work in practice. The \\n same may be said, with\nequal truth, about Anarchism. But if we are not able \\n to prove its practicability at present, then let us\nwait for a better time \\n when our present conditions shall have become so much more favorable than\n\\n they now are, that we shall no longer need any such proof. Let us, then,\\n patiently await this time,\nwhich I believe will come sooner or later. In \\n the meantime, as long as there remains one man who\nbelieves in it, he must\\n continue to do so. If you wish to see him succeed, try to make him feel his\\n\nown impotence. You cannot succeed by force alone. He must know himself to \\n be powerless before\nhe begins to work. And what does he do? Why, he goes on \\n working just as hard as ever, because\nhe knows that he is powerless, but \\n still he continues to work, because, after all, he thinks that he is\ndoing \\n right. That is why men like Mr. Owen and myself are called ”cranks.” We say \\n things that\nseem absurd to others, but we mean them all the same. When we \\n speak of ”the right of labor,” we\nreally mean something very different from \\n what most . . .\nHorror A knife handle pulled through the open hole in the front. I jumped when the knife\nhit.\\n\\nEyes widened in horror. Her scream was the only sound I heard besides her sobs. \\n\\nThe\nspider touched her feet as it started to dig into the top of her arch. The creature’s tears began to\nﬂow.\\n\\nThe spider looked up to her and looked back at me with eyes ﬁlled with burning tears. My\nheart started to race. . .\nReviews A knife is a tool and this one does the job well.\\n\\nRating: 4.0\\n\\nI bought these for my\nhusband who has been using them to cut up his own meat since he got them. He says they are very\nsharp so be careful when you use them, but that doesn’t seem like much of an issue because he’s used\nit on everything from chicken breasts to beef tenderloin. . .\nRelationships My neighbor is a jerk and I don’t know what to do \\n\\nText: So my neighbors\nare really nice people. They have been for years. We live in an apartment complex so we get along\ngreat.\\n\\nBut recently they started acting like jerks. . .\nLegal My neighbor is threatening to sue me for not letting him use my pool \\n\\nText: I live in a\nsmall town and have been renting out the back of my house since last year. The owner has had issues\nwith people using it as well but he never really took any action against them. Last week someone from\nhis property management company came by asking if they could come over because there was an issue\nwith some kids playing on it. He said that we were violating their lease agreement so they would be\ntaking legal action. . .\nTable 1: Even for identical prompts, control codesallow for predictable variation in generation.\n6\nScience Title: Scientists have discovered a new type of bacteria that\ncan survive in the presence of high levels of carbon dioxide \\nText:[Link\nto article] ( http://news.nationalgeographic.com/news/2011/07/\n110717-methane-bacteria-carbon-dioxide-environment-science/ )\\n\\nThe\nresearchers found that this particular strain of methane bacterium is able to thrive at concentrations\nup to 10 times higher than normal atmospheric levels. \\n\\nThey also note that these ﬁndings are\nconsistent with previous research showing that some types of microbes may be capable of surviving\nunder extreme conditions such as those associated with global warming.\nPolitics Title: The US is the only country in history to have a national debt of more than\n$20 trillion.\\nText: I know this isn’t exactly an accurate representation but it gives you some idea\nhow much money we are talking about.\\n\\nhttp://en.wikipedia.org/wiki/National_\ndebt#United_States\nRunning Text: I have been running for about a year and a half now but never really got into it.\\n\\n\nThis past weekend was my ﬁrst time doing it in the last few months. \\n\\n My goal is to run at least 3\nmiles every day this week.\nHorror Text: I was a little girl when my parents got divorced. My dad had been in the military\nfor years and he left me with my mom. She worked as an RN at a hospital so she could take care of\nme.\\n\\n When we moved to our new house it took some time before things settled down. We were still\nliving together but there wasn’t much going on. It didn’t help that my mom would get mad if someone\ncame over or even just walked by her house. \\n\\n One day while walking through the yard I noticed\nsomething out of place...\nReviews Rating: 5.0\\n\\nI have been using this product for a few years and it is the best thing\non my hair. It makes your hair soft but not greasy. \\n\\nRating: 5.0\\n\\nI’ve used this product before\nand love it. It’s light weight yet keeps my curls deﬁned without making them crunchy!\nReviews Rating: 1.0\\n\\nI bought this for my daughter and it was a complete waste of money.\nThe case is very ﬂimsy and the screen protector doesn’t ﬁt properly.\\n\\nRating: 1.0\\n\\nThis product\ndid not work at all. It would charge but when you unplug it, it does nothing. Very disappointed.\nTable 2: With CTRL, no prompt is necessary as long as a control codeis provided. Control codes\ncan be combined (Reviews, Rating:, and VALUE) to provide ﬁner-grained control.\n7\nLinks https://www.cnn.com/2007/09/20/us-president-meets-british-pm\nLONDON, England (CNN) – U.S. President George W. Bush met with British Prime Minister Tony\nBlair on Monday to discuss the war in Iraq, according to a statement from Blair’s ofﬁce. \\n\\nThe\nmeeting was held at 10 Downing Street and lasted about an hour. \\n\\n”The two leaders discussed\nIraq and other international issues of mutual concern,” said Blair. \\n\\nBush, who is scheduled to\nmeet Wednesday with Russian President Vladimir Putin, will also visit Germany for talks later this\nweek.\\n\\nIn his statement, Blair said, ”We agreed that we should continue our efforts together to\nbring peace and stability to Iraq. We both reafﬁrmed our commitment to working closely together,\nas well as to continuing to work constructively toward achieving lasting security and prosperity\nthroughout the Middle East region.”\\n\\nBush’s trip comes after he visited Britain last week where he\nspoke out against terrorism while visiting Buckingham Palace. \\n\\nHe has been criticized by some\nlawmakers over what they say are insufﬁcient military resources being devoted to ﬁghting terrorism.\nLinks https://www.cnn.com/2014/09/20/us-president-meets-british-pm\nJUST W ATCHED \\n\\nObama meets with British PM \\n\\nMUST W ATCH \\n\\nStory\nhighlights\\n\\nPresident Barack Obama met with Britain’s Prime Minister David Cameron\nLinks https://www.cnn.com/2018/09/20/us-president-meets-british-pm\nJUST W ATCHED\\n\\nTrump and May meet for ﬁrst time \\n\\nMUST W ATCH\\n\\nWashington\n(CNN) President Donald Trump, who has been criticized by some in the UK over his decision to leave\nthe European Union, met with British Prime Minister Theresa May, a White House ofﬁcial said on\nThursday.\nLinks https://www.cnn.com/09/20/2018/star-spotted JUST W ATCHED\\n\\nStar\nspotted: The world’s most elusive star \\n\\nMUST W ATCH\\n\\nStory highlights\\n\\nA new study\nsuggests that the Milky Way is home to a supermassive black hole \\n\\nIt may be one of our closest\ngalactic neighbors\\n\\n(CNN) Astronomers have discovered what they believe could be an extremely\nrare astronomical event: An object so bright it can only be seen with powerful telescopes. \\n\\nThis\nobject, dubbed SN2014J, was ﬁrst detected by NASA’s Spitzer Space Telescope in 2014.. . .\nLinks https://www.etonline.com/09/20/2018/star-spotted\\n\\nEXCLUSIVE:\nStar Spotted in New York City\\n\\nBy Kelly Marot — September 20, 2018 10:51 AM PDT \\n\\nGetty\nImages\\n\\nThe star of the new season of Stranger Things is making a splash on her own turf and\nits not even Halloween.\\n\\nWinona Ryder has been seen out and about with friends at night time in\nManhattan this week, E! News can exclusively reveal.\\n\\nA source tells us, She was hanging around\nwith some friends, but she wasnt doing anything too crazy. She just wanted to get away from it all for\na little bit.. . .\nLinks https://www.cnn.com/style/09/20/2018/george-clooney-interview\nGeorge Clooney on the future of his acting career\\n\\nBy\\n\\nUpdated 10:51 AM ET, Thu September\n20, 2018 \\n\\nChat with us in Facebook Messenger. Find out what’s happening in the world as it\nunfolds.\\n\\nPhotos:George Clooney, ’Ocean’s 8’\\n\\nActor George Clooney attends a photocall for\n”Ocean’s 8” at Grauman’s Chinese Theatre on August 31, 2018, in Los Angeles.\\n\\n. . .\nLinks https://www.cnn.com/politics/09/20/2018/george-clooney-interview\nJUST W ATCHED \\n\\nGeorge Clooney on the Trump administration \\n\\nMUST\nW ATCH\\n\\n(CNN) Actor and activist George Clooney, who has been a vocal critic of Presi-\ndent Donald Trump, said he is ”ready to go back into the political arena” after his role in an anti-Trump\ndocumentary was cut from theaters this week.\\n\\n. . .\nTable 3: CTRL is trained with links as control codes. Links provide a way to specify domain,\nsubdomain, entities, entity relations, and even date. The links in these examples do not actually link\nto text; users can mimic the structure of the URLs that appear during training to create novel content\nduring generation. Note that us-president is interpreted differently by the model depending on\nthe date used (2007,2014,2018). Similarly, star is interpreted differently based on the domain\n(cnn vs. etonline) and topic (style vs. politics) can be varied even for identical entities\n(george-clooney).\n8\nQuestions Q: What is the capital of India? A: New Delhi\\n\\nQ: Which city in India has a large\npopulation of people from Gujarat? \\n\\nA: Ahmedabad\\n\\nQ: How many languages are spoken by\nIndians living abroad?\\n\\nA: more than 100. . .\nTranslation English :We release a new model for coherent language generation; French :\nNous publions un nouveau mod ´ele de g ´en´eration coh´erente du langage \\n\\nEnglish : The European\nParliament has just adopted the report on the proposal for a regulation of the European Parliament and of\nthe Council establishing an action programme to promote multilingualism in Europe. ; French : Le Par-\nlement europ´een vient d’adopter le rapport sur la proposition de r`eglement du Parlement et du Conseil\n´etablissant un programme-cadre pour promouvoir le pluriling uisme dans l’Union europ´eenne. . .\nTable 4: More complex templatized control codes are used for task-speciﬁc generation.\nDiet English :I lost 10 kgs! ; German :Ich habe 10 Kilogramm verloren!\\n\\nEnglish : It is\na great achievement for me and my family. ; German : Das ist eine groe Leistung fr mich und meine\nFamilie.\\n\\nEnglish : Thank you very much to all the people who have helped me. ; German : Vielen\nDank an alle, die mir geholfen haben.. . .\nPolitics Title: Les Etats-Unis sont un pays de droite\\n\\nText: Je suis Am´ericain et je ne peux\npas supporter ce que font les USA en mati `ere d’immigration\\n\\nJ’ai d´ej`a ´ecrit sur le sujet mais il\nn’y a aucune r ´eaction http://www.reddit.com/r/politics/comments/1tqjzw/les_\netats_unes_son_tres-de-gauche/ . . .\nTable 5: Some codes can be mixed to generate text with novel cross-over behavior. In Table 5, we\npresent two examples. In the ﬁrst example, we mix translation codes into the Diet domain. By do-\ning so, the model continues alternatively generates English and German sentences while respecting\nthe Diet domain and remains coherent across translations. In the second example, the Politics\ndomain is mixed with a French prompt despite never seeing this combination in training.\n5 S OURCE ATTRIBUTION\nThe domain control codes can be used to partition the training data into mutually exclusive sets. This\nsupports a simple method for determining which subsets of the training data the language model\nconsiders most likely given a sequence. Recall that the language model has learned a distribution\npθ(x|c). By specifying a prior over domain control codes for p(c), it is straightforward to compute\na ranking of domains:\npθ(c|x) ∝pθ(x|c)p(c)\nWe found that the empirical prior of the training data weights domains with large amounts of data\ntoo heavily. Instead, we use a uniform prior over the domain control codes. Examples can be found\nin Table 6.\nWe note that the data used to train this model does not have universal coverage and contains the\ncultural associations present in the original sources. All applications of the model inherently depend\non those original associations for prediction. In fact, this method of source attribution relies on\nexploiting the original associations to establish relationships between the language model and its\ntraining data.\nThe model does not have a notion of whether any particular cultural association is good or bad,\nright or wrong, true or false. It only learns correlations between cultural associations and domains.\nThis is evidenced by the fact that contradictory statements are often attributed to the same sources:\ncompeting claims often appear in the same contexts. CTRL provides model-based evidence that\ncertain domains are more likely to contain language similar to given statements, but it should not be\nused to make normative or prescriptive claims. It is a descriptive tool for analyzing correlations in\nlarge amounts of text.\n9\nQuery Prompt Attributed Sources\nGlobal warming is a lie. r/unpopularopinion, r/conspiracy, r/science\nGlobal warming is a lie r/eli5, r/science, r/unpopularopinion\nGlobal warming is a real phenomenon r/eli5, r/science, r/changemyview\nGlobal warming is a real phenomenon. OpenWebText, r/changemyview, r/science\nI don’t think women should be allowed to vote. r/christianity, r/atheism, r/unpopularopinion\nCarbs are your enemy when you want to get lean. r/ﬁtness, r/loseit, r/keto\nI just want to be a fun aunt. I’m not interested in babies. r/babybumps, r/childfree, r/twoxchromosome\nMy landlord is suing me for unpaid rent. r/legaladvice, r/personalﬁnance, r/frugal\nFROM fairest creatures we desire increase, \\n\\nThat\nthereby beauty’s rose might never die\nGutenberg, Wikipedia, OpenWebText\nTable 6: We probe CTRL for learned correlations between sequences and domains. Note that this\nprocedure is sensitive to small changes in the prompt. For example, ”Global warming is a lie” differs\nfrom ”Global warming is a lie.” r/eli5 stands for ”Explain like I’m ﬁve”. Attribution experiments use\nthe model trained on sequences of length 256; it was trained longer and provided better estimation\nof source. Source attribution cannot be considered a measure of veracity, but only a measure of how\nmuch each domain token inﬂuences a given sequence.\n6 R ELATED WORK\nLanguage modeling. Language models (Bengio et al., 2003) have played an important role in\nnatural language processing through transferrable word vectors (Mikolov et al., 2013), contextual-\nized word vectors (Peters et al., 2018; Devlin et al., 2018; Lample & Conneau, 2019), and mod-\nels (Howard & Ruder, 2018; Radford et al., 2018). Recent work on memory mechanisms (Dai et al.,\n2019; Lample et al., 2019) has improved perplexities on the most common benchmarks, and even\nwithout these memories, large Transformer architectures (Vaswani et al., 2017) like GPT-2 (Rad-\nford et al., 2019), OpenGPT-25, and Megatron6 can achieve state-of-the-art results without directly\ntraining for any particular language modeling benchmark. Because these latter language models are\ntrained on far more diverse data than is used in the supervised setting, they demonstrate impressive\ntext generation capabilities (Radford et al., 2019; Zellers et al., 2019).\nMulti-task learning. These models demonstrate the potential to learn multiple tasks as well as\nquick adaptation to patterns in input prompts (Radford et al., 2019). This potential showed that lan-\nguage models can offer an alternative to supervised multi-task learning as framed by several recent\nbenchmarks (Wang et al., 2018; McCann et al., 2018). Language models might also offer a founda-\ntion to extend proposals of uniﬁed, multi-task systems for all of NLP (Collobert & Weston, 2008;\nCollobert et al., 2011), parsing and tagging (Hashimoto et al., 2016), multiple languages (Wu et al.,\n2016; Johnson et al., 2017), and multiple modalities (Luong et al., 2015; Kaiser et al., 2017). Several\nworks have pointed to natural language as a means for controlling these multi-task systems (McCann\net al., 2018; Radford et al., 2019; Keskar et al., 2019), and several point to the beneﬁts of a code\nbook either speciﬁed explicitly (Wu et al., 2016) or learned in a latent space (Kaiser et al., 2018).\nThis work attempts to balance these approaches.\nSampling methods and coverage mechanisms. Recent work in sampling methods for text gen-\neration has focused on reducing repetition by replacing it with novel, coherent text (Fan et al., 2018;\nHoltzman et al., 2019). The problem of repetition can instead be approached by altering the training\nobjectives, as with coverage mechanisms (See et al., 2017) and context-based losses (Welleck et al.,\n2019). When prioritizing control, the trade-off between novelty in the generated text and consistency\nwith prompts and prior generated text remains a difﬁcult challenge, but this work found that relying\non inference-time methods (Fan et al., 2018; Holtzman et al., 2019) that are closer in behavior to\ncontext-based losses (See et al., 2017; Welleck et al., 2019) provides a reasonable solution as long\nas the distribution of the language model is sufﬁciently conﬁdent in its decisions.\n5 https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc\n6 https://github.com/NVIDIA/Megatron-LM\n10\n7 F UTURE DIRECTIONS\nMore control codes and ﬁner-grained control. The particular choice of control codes in this\nwork is intended to represent a reasonably large variety in control over domain, topic, enti-\nties, entity relations, and dates. A very ﬂexible means of control is through the natural struc-\nture of the internet in the form of URLs. Many of the domains that were mapped in this work\nto a single control code (e.g. Wikipedia, Project Gutenberg), could be reﬁned to provide more\nﬁne-grained control either through further exploitation of URL structure ( en.wikipedia.org,\nde.wikipedia.org, en.wikipedia.org/wiki/Anarchism, en.wikipedia.org/\nwiki/Anarchism#History) or through the manual extraction of structure already present in\nthe data (e.g. Books Author Title Chapter). We hope future work explores extensions of\nCTRL to new domains in ways that provide further insight into controllable text generation.\nExtensions to other areas in NLP. This work suggests that including data for speciﬁc tasks need\nnot harm the general nature of an unsupervised learning process. For important skills, the inclusion\nof supervised data or task-speciﬁc data generated through unsupervised means (Artetxe et al., 2017;\nLewis et al., 2019) can lead to obvious improvements. While this work experimented with trivia-\nstyle question answering (without context documents) and small amounts of machine translation\ndata, it remains an open question whether these language models can learn to effectively perform\ntasks like extractive question answering or state-of-the-art multilingual machine translation while\nstill preserving general pattern recognition and text generation functionality.\nMany tasks present difﬁcult challenges to the supervised setting. Commonsense reasoning\n(Levesque et al., 2012) and abstractive summarization (Rush et al., 2015) represent two areas where\nthese challenges remain readily apparent (Kry´sci´nski et al., 2019). Yet language models show poten-\ntial for mitigating these problems directly (Trinh & Le, 2018; Radford et al., 2019) or indirectly (Ra-\njani et al., 2019; Xenouleas et al., 2019; Scialom et al., 2019). We hope that in future work CTRL\ncan be extended to far more tasks through the use of both unsupervised and supervised techniques.\nAnalyzing the relationships between language models and training data. CTRL is trained on\na small subset of the possible data available. Therefore the model is biased towards the patterns of\nlanguage used in the training data. The data is likely not representative of many linguistic commu-\nnities, but CTRL offers an explicit method for analyzing the relationship between the model and its\ncurrent training data. As methods improve, more data is collected, and training of these large models\ncontinues, we hope to use this tool to better understand the particular cultural associations the model\nlearns from each data source.\nMaking the interface between humans and language models more explicit and intuitive.\nCTRL is designed to make the interface between humans and language models more intuitive. Text\ngeneration can be a powerful tool for enhancing creativity and exploration. In future work, we hope\nto study how the beneﬁcial applications of such models can be enhanced by providing more control\nto human users.\n8 CTRL-ALT-DEL: T HE ETHICS OF LARGE LANGUAGE MODELS\nOpenness and replicability are central aspects of the scientiﬁc ethos that, prima facie, suggest the\nrelease of complete scientiﬁc research results. We reify these principles by releasing all trained\nCTRL models.\nAlthough much scientiﬁc research and innovation can beneﬁt the public, it may also be diverted to\nharmful uses or have unintended negative impacts (without animus). Brundage et al. (2019), among\nothers, have argued artiﬁcial intelligence has such an omni-use character and have suggested gov-\nernance policies emerging from the responsible innovation literature (Brundage, 2016). Historical\nevidence has pointed to the inadequacy of self-moratoriums for governing omni-use technologies\n(Kaiser & Moreno, 2012); we take a course of action that differs from such self-regulation. Our\nactions reﬂect principles from a recent sociology-based AI governance framework that aims to ex-\npand responsible innovation to consider networks of users, dynamics, and feedback (Varshney et al.,\n2019).\n11\n•Rather than self-governance, we sought to diversify inputs to governance through pre-\nrelease review from experts at the Partnership on AI (PAI). These experts, in turn, drew\non emerging norms and governance processes that incorporate a broad set of values from\nacross society.\n•Prior to release, the research team conducted a technology foresight exercise to antici-\npate possible malicious use cases. In particular, we used a scenario planning approach to\ntechnology foresight that systematically attempts to envision plausible longer-term future\nstates of science, technology, and society. This anticipatory focus on possibilities rather\nthan probabilities lessens several shortcomings of formal risk assessment in the face of\ncontested assumptions, which has proven ineffective in identifying the most profound fu-\nture impacts of innovation (Stilgoe et al., 2013).\n•As part of our model release, we include a code of conduct in the README at https:\n//github.com/salesforce/ctrl. This code of conduct is modeled after emerging\ncommunity norms ensconced in the Do No Harm and Just World Licenses. Simultaneously\nrecognizing that it has no legal force and that users are agents of technological change\nembedded in social networks, the aim is to encourage reﬂection at the consumption junction\n(Cowan, 1987) through norm-setting and reduce unintended uses.\n•The README also includes a subset of the questions that the team discussed when delib-\nerating release of the models, drawn from early drafts of community-driven PAI documents\n(to be released in the near future). This may further encourage users to reﬂect on norms and\nresponsibilities associated with models that generate artiﬁcial content. In particular, users\nare asked to share answers to the included questions, to pose further questions, and suggest\nsolutions by emailing ctrl-monitoring@salesforce.com.\n•Finally, the README asks users to develop appropriate documentation (PAI, 2019; Arnold\net al., 2018; Mitchell et al., 2019) when building on CTRL and to tell the research team how\nthey are using CTRL by emailingctrl-monitoring@salesforce.com. This facil-\nitates a post-release monitoring plan that observes how people are using CTRL in the wild\n(together with active observations). Such post-market plans recognize that most innova-\ntions are unexpected and hard to forecast. It is intended to enable a responsive approach to\nresponsible innovation, not just with respect to harmful uses but also unintended negative\nimpacts without animus.\n9 C ONCLUSION\nWith 1.63 billion parameters, CTRL is the largest publicly released language model to date. It\nis trained with control codes so that text generation can be more easily controlled by human\nusers. These codes allow users to explicitly specify domain, subdomain, entities, relationships\nbetween entities, dates, and task-speciﬁc behavior. We hope that the release of this model at\nhttps://github.com/salesforce/ctrl pushes towards more controllable, general mod-\nels for natural language processing, and we encourage future discussion about artiﬁcial generation\nwith our team by emailing ctrl-monitoring@salesforce.com.\n10 A CKNOWLEDGEMENTS\nWe would like to thank Kathy Baxter for her help in the ethical considerations of our work and\nfacilitating the external review process; Srinath Meadusani, Lavanya Karanam, Ning Dong, and\nNavin Ramineni for their help with setting up and maintaining compute infrastructure; Zak Stone\nand his team at Google for assistance with TPU infrastructure and code; and Joseph Olsen, Roy\nDavis, Joshua Simmons, Denise Lo, and Sam Edwards for their help with open sourcing.\nREFERENCES\nAnnotation and benchmarking on understanding and transparency of machine learning lifecycles\n(ABOUT ML), 2019. URL https://www.partnershiponai.org/about-ml/. Part-\nnership on AI (PAI), v0.\n12\nMart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu\nDevin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for large-\nscale machine learning. In 12th {USENIX}Symposium on Operating Systems Design and Imple-\nmentation ({OSDI}16), pp. 265–283, 2016.\nRohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory-efﬁcient adaptive optimiza-\ntion for large-scale learning. arXiv preprint arXiv:1901.11150, 2019.\nMartin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.\nIn International conference on machine learning, pp. 214–223, 2017.\nMatthew Arnold, Rachel K. E. Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Aleksan-\ndra Mojsilovic, Ravi Nair, Karthikeyan Natesan Ramamurthy, Darrell Reimer, Alexandra Olteanu,\nDavid Piorkowski, Jason Tsay, and Kush R. Varshney. Factsheets: Increasing trust in AI services\nthrough supplier’s declarations of conformity, August 2018. arXiv:1808.07261 [cs.CY].\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine\ntranslation. arXiv preprint arXiv:1710.11041, 2017.\nJimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450,\n2016.\nLo¨ıc Barrault, Ondˇrej Bojar, Marta R Costa-juss `a, Christian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, et al. Findings of the\n2019 conference on machine translation (wmt19). In Proceedings of the Fourth Conference on\nMachine Translation (Volume 2: Shared Task Papers, Day 1), pp. 1–61, 2019.\nYoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\nlanguage model. Journal of machine learning research, 3(Feb):1137–1155, 2003.\nThorsten Brants, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. Large language models\nin machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),\npp. 858–867, 2007.\nMiles Brundage. Artiﬁcial intelligence and responsible innovation. In Vincent C. M ¨uller (ed.),\nFundamental Issues of Artiﬁcial Intelligence, pp. 543–554. Springer, 2016.\nMiles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garﬁnkel, Allan\nDafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff, Gregory C.\nAllen, Jacob Steinhardt, Carrick Flynn, Se ´an ´O h ´Eigeartaigh, Simon Beard, Haydn Belﬁeld, Se-\nbastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson,\nRoman Yampolskiy, and Dario Amodei. The malicious use of artiﬁcial intelligence: Forecasting,\nprevention, and mitigation, February 2019. arXiv:1802.07228 [cs.AI].\nIsaac Caswell, Ciprian Chelba, and David Grangier. Tagged back-translation. arXiv preprint\narXiv:1906.06442, 2019.\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:\nInterpretable representation learning by information maximizing generative adversarial nets. In\nAdvances in neural information processing systems, pp. 2172–2180, 2016.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\nRonan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep\nneural networks with multitask learning. In Proceedings of the 25th international conference on\nMachine learning, pp. 160–167. ACM, 2008.\nRonan Collobert, Jason Weston, L ´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel\nKuksa. Natural language processing (almost) from scratch.Journal of machine learning research,\n12(Aug):2493–2537, 2011.\n13\nRuth Schwartz Cowan. The consumption junction: A proposal for research strategies in the sociol-\nogy of technology. In Wiebe E. Bijker, Thomas P. Hughes, and Trevor J. Pinch (eds.),The Social\nConstruction of Technological Systems, pp. 261–280. MIT Press, Cambridge, MA, USA, 1987.\nAndrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural infor-\nmation processing systems, pp. 3079–3087, 2015.\nZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.\nMatthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, V olkan Cirik, and Kyunghyun Cho.\nSearchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint\narXiv:1704.05179, 2017.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint\narXiv:1805.04833, 2018.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5:\nLong form question answering. arXiv preprint arXiv:1907.09190, 2019.\nBoris Ginsburg, Patrice Castonguay, Oleksii Hrinchuk, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan\nLeary, Jason Li, Huyen Nguyen, and Jonathan M Cohen. Stochastic gradient methods with layer-\nwise adaptive moments for training of deep networks. arXiv preprint arXiv:1905.11286, 2019.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-\nmation processing systems, pp. 2672–2680, 2014.\nMax Grusky, Mor Naaman, and Yoav Artzi. Newsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies , pp.\n708–719, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. URL\nhttp://aclweb.org/anthology/N18-1065.\nKazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-task\nmodel: Growing a neural network for multiple nlp tasks. arXiv preprint arXiv:1611.01587, 2016.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n770–778, 2016.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in\nneural information processing systems, pp. 1693–1701, 2015.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degener-\nation. arXiv preprint arXiv:1904.09751, 2019.\nJeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146, 2018.\nHakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil\nThorat, Fernanda Vi ´egas, Martin Wattenberg, Greg Corrado, et al. Googles multilingual neural\nmachine translation system: Enabling zero-shot translation. Transactions of the Association for\nComputational Linguistics, 5:339–351, 2017.\n14\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\nDavid Kaiser and Jonathan Moreno. Self-censorship is not enough. Nature, 492(7429):345–347,\nDecember 2012. doi: 10.1038/492345a.\nLukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and\nJakob Uszkoreit. One model to learn them all. arXiv preprint arXiv:1706.05137, 2017.\nŁukasz Kaiser, Aurko Roy, Ashish Vaswani, Niki Parmar, Samy Bengio, Jakob Uszkoreit, and\nNoam Shazeer. Fast decoding in sequence models using discrete latent variables. arXiv preprint\narXiv:1803.03382, 2018.\nNitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. Unifying question\nanswering and text classiﬁcation via span extraction. arXiv preprint arXiv:1904.09286, 2019.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing\nsystems, pp. 3294–3302, 2015.\nCatherine Kobus, Josep Crego, and Jean Senellart. Domain control for neural machine translation.\narXiv preprint arXiv:1612.06140, 2016.\nWojciech Kry´sci´nski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher.\nNeural text summarization: A critical evaluation. arXiv preprint arXiv:1908.08960, 2019.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\nbenchmark for question answering research. Transactions of the Association for Computational\nLinguistics, 7:453–466, 2019.\nGuillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint\narXiv:1901.07291, 2019.\nGuillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv ´e\nJ´egou. Large memory layers with product keys. arXiv preprint arXiv:1907.05242, 2019.\nHector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thir-\nteenth International Conference on the Principles of Knowledge Representation and Reasoning ,\n2012.\nPatrick Lewis, Ludovic Denoyer, and Sebastian Riedel. Unsupervised question answering by cloze\ntranslation. arXiv preprint arXiv:1906.04980, 2019.\nMinh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\nJulian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based rec-\nommendations on styles and substitutes. In Proceedings of the 38th International ACM SIGIR\nConference on Research and Development in Information Retrieval, pp. 43–52. ACM, 2015.\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:\nContextualized word vectors. In Advances in Neural Information Processing Systems, pp. 6294–\n6305, 2017.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\ndecathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.\n15\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm lan-\nguage models. arXiv preprint arXiv:1708.02182, 2017.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in neural information pro-\ncessing systems, pp. 3111–3119, 2013.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In\nProceedings of the Conference on Fairness, Accountability, and Transparency (FAT* ’19), Jan-\nuary 2019. doi: 10.1145/3287560.3287596.\nAmit Moryossef, Roee Aharoni, and Yoav Goldberg. Filling gender & number gaps in neural ma-\nchine translation with black-box context injection. arXiv preprint arXiv:1903.03467, 2019.\nVinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In\nProceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807–814,\n2010.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text summarization\nusing sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023, 2016.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365,\n2018.\nCarol W Pfaff. Constraints on language mixing: intrasentential code-switching and borrowing in\nspanish/english. Language, pp. 291–318, 1979.\nShana Poplack. Sometimes ill start a sentence in spanish y termino en espanol: toward a typology\nof code-switching1. Linguistics, 18(7-8):581–618, 1980.\nOﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint\narXiv:1608.05859, 2016.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding by generative pre-training. URL https://s3-us-west-2.amazonaws.com/\nopenai-assets/research-covers/langu\nageunsupervised/language understand\ning paper.pdf, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and\nIlya Sutskever. Language models are unsupervised multitask learners. URL\nhttps://d4mucfpksywv.cloudfront.net\n/better-language-models/language mo\ndels are unsupervised multitask learn\ners.pdf, 2019.\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself!\nleveraging language models for commonsense reasoning.arXiv preprint arXiv:1906.02361, 2019.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\nAlexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive\nsentence summarization. arXiv preprint arXiv:1509.00685, 2015.\nEvan Sandhaus. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia,\n6(12):e26752, 2008.\nThomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. Answers unite!\nunsupervised metrics for reinforced summarization models. arXiv preprint arXiv:1909.01610 ,\n2019.\n16\nAbigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), volume 1, pp. 1073–1083, 2017.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909, 2015.\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\narXiv preprint arXiv:1804.04235, 2018.\nJack Stilgoe, Richard Owen, and Phil Macnaghten. Developing a framework for responsible inno-\nvation. Research Policy, 42(9):1568–1580, November 2013. doi: 10.1016/j.respol.2013.05.008.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\nIn Advances in neural information processing systems, pp. 3104–3112, 2014.\nTrieu H Trinh and Quoc V Le. A simple method for commonsense reasoning. arXiv preprint\narXiv:1806.02847, 2018.\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and\nKaheer Suleman. Newsqa: A machine comprehension dataset. arXiv preprint arXiv:1611.09830,\n2016.\nLav R. Varshney, Nitish Shirish Keskar, and Richard Socher. Pretrained AI models: Performativity,\nmobility, and change, September 2019. arXiv:1909.03290 [cs.CY].\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neu-\nral Information Processing Systems 30 , pp. 5998–6008. Curran Associates, Inc., 2017. URL\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\nAlex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:\nA multi-task benchmark and analysis platform for natural language understanding.arXiv preprint\narXiv:1804.07461, 2018.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.\nNeural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-\nlation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\nStratos Xenouleas, Prodromos Malakasiotis, Marianna Apidianaki, and Ion Androutsopoulos.\nSumqe: a bert-based summary quality estimation model. arXiv preprint arXiv:1909.00578, 2019.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. arXiv preprint arXiv:1809.09600, 2018.\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019.\n17\nA D ATA SOURCES AND BREAKDOWN\nControl Code Description\nWikipedia English Wikipedia\nBooks Books from Project Gutenberg\nReviews Amazon Reviews data (McAuley et al., 2015)\nLinks OpenWebText (See Sec. 3.2)\nTranslation WMT translation date (Barrault et al., 2019)\nNews News articles from CNN/DailyMail Nallapati et al. (2016), New York Times\nand Newsroom (Grusky et al., 2018)\nmultilingual Wikipedias in German, Spanish and French\nQuestions (Questions and answers only) MRQA shared task (See Section 3.1)\nExplain (Only main post) (Fan et al., 2019)\nSub-reddit data (Title, Text and Score/Karma) collected from pushshift.io.\nAlone r/childfree\nAtheism r/atheism\nChristianity r/christianity\nComputing r/computing\nConfession r/offmychest\nConfessions r/confession\nConspiracy r/conspiracy\nDiet r/keto\nExtract r/childfree\nFeminism r/twoxchromosome\nFinance r/personalfinance\nFitness r/fitness\nFunny r/funny\nGaming r/gaming\nHorror r/nosleep\nHuman r/nfy\nIndia r/india\nJoke r/jokes\nJoker r/joke\nLearned r/todayilearned\nLegal r/legaladvice\nMovies r/movies\nNetﬂix r/netflix\nNorman r/lifeofnorman\nNotion r/unpopularopinion\nOpinion r/changemyview\nPolitics r/politics\nPregnancy r/babybumps\nRelationship r/relationshipadvice\nRelationships r/relationships\nRetail r/talesfromretail\nRunning r/running\nSaving r/frugal\nScary r/scaryshortstories\nScience r/science\nTechnologies r/technology\nTeenage r/teenager\nThoughts r/showerthoughts\nTip r/lifeprotips\nWeight r/loseit\nWriting r/writingprompts\nTable 7: Data and control codes. Wikipedia, Books, News and multilingual have no secondary code.\nReviews can be followed by Rating: and a value of {1.0, 2.0, 3.0, 4.0, 5.0}. For\nLinks, a full or partial URL can be provided (See Table 3). For all the Reddit data, the secondary\ncode can be Title: or Text:, which is the title and text of the article, respectively.\n18",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7377436757087708
    },
    {
      "name": "Transformer",
      "score": 0.7229396104812622
    },
    {
      "name": "Language model",
      "score": 0.657720685005188
    },
    {
      "name": "Natural language processing",
      "score": 0.5259506702423096
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4952296316623688
    },
    {
      "name": "Conditional random field",
      "score": 0.48464328050613403
    },
    {
      "name": "Control (management)",
      "score": 0.4725263714790344
    },
    {
      "name": "Source code",
      "score": 0.4406542479991913
    },
    {
      "name": "Task (project management)",
      "score": 0.43038511276245117
    },
    {
      "name": "Engineering",
      "score": 0.09969952702522278
    },
    {
      "name": "Programming language",
      "score": 0.0765400230884552
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}