{
    "title": "Single-Shot Motion Completion with Transformer",
    "url": "https://openalex.org/W3134060426",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2671662871",
            "name": "Duan Yinglin",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2888345152",
            "name": "Shi, Tianyang",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": "https://openalex.org/A4281155487",
            "name": "Zou, Zhengxia",
            "affiliations": [
                "University of Michigan–Ann Arbor"
            ]
        },
        {
            "id": null,
            "name": "Lin, Yenan",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": null,
            "name": "Qian, Zhehui",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2220527832",
            "name": "Zhang Bohan",
            "affiliations": [
                "NetEase (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1984509676",
            "name": "Yuan Yi",
            "affiliations": [
                "NetEase (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2959084026",
        "https://openalex.org/W3085139254",
        "https://openalex.org/W2336236730",
        "https://openalex.org/W2017026595",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W2097944344",
        "https://openalex.org/W3048474702",
        "https://openalex.org/W1504830908",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2963324990",
        "https://openalex.org/W2560678327",
        "https://openalex.org/W1643924482",
        "https://openalex.org/W2896604071",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2075347672",
        "https://openalex.org/W1986720487",
        "https://openalex.org/W2469134594",
        "https://openalex.org/W2099913338",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2896588340",
        "https://openalex.org/W2982573856",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2895138769",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2296629652",
        "https://openalex.org/W2027247754",
        "https://openalex.org/W2249800031",
        "https://openalex.org/W2158164339",
        "https://openalex.org/W1994305157",
        "https://openalex.org/W1977871568",
        "https://openalex.org/W2902757548",
        "https://openalex.org/W1970552420",
        "https://openalex.org/W2144231991",
        "https://openalex.org/W2987886924",
        "https://openalex.org/W2293545866"
    ],
    "abstract": "Motion completion is a challenging and long-discussed problem, which is of great significance in film and game applications. For different motion completion scenarios (in-betweening, in-filling, and blending), most previous methods deal with the completion problems with case-by-case designs. In this work, we propose a simple but effective method to solve multiple motion completion problems under a unified framework and achieves a new state of the art accuracy under multiple evaluation settings. Inspired by the recent great success of attention-based models, we consider the completion as a sequence to sequence prediction problem. Our method consists of two modules - a standard transformer encoder with self-attention that learns long-range dependencies of input motions, and a trainable mixture embedding module that models temporal information and discriminates key-frames. Our method can run in a non-autoregressive manner and predict multiple missing frames within a single forward propagation in real time. We finally show the effectiveness of our method in music-dance applications.",
    "full_text": "Single-Shot Motion Completion with Transformer\nYinglin Duan ∗, Tianyang Shi ∗\nNetEase Fuxi AI Lab\n{duanyinglin, shitianyang}@corp.netease.com\nZhengxia Zou *\nUniversity of Michigan, Ann Arbor\nzzhengxi@umich.edu\nYenan Lin, Zhehui Qian, Bohan Zhang\nNetEase\nyenan lin@foxmail.com,\n{qianzhehui, hzzhangbohan}@corp.netease.com\nYi Yuan†\nNetEase Fuxi AI Lab\nyuanyi@corp.netease.com\nAbstract\nMotion completion is a challenging and long-discussed\nproblem, which is of great signiﬁcance in ﬁlm and game ap-\nplications. For different motion completion scenarios (in-\nbetweening, in-ﬁlling, and blending), most previous meth-\nods deal with the completion problems with case-by-case\ndesigns. In this work, we propose a simple but effective\nmethod to solve multiple motion completion problems un-\nder a uniﬁed framework and achieves a new state of the art\naccuracy under multiple evaluation settings. Inspired by the\nrecent great success of attention-based models, we consider\nthe completion as a sequence to sequence prediction prob-\nlem. Our method consists of two modules - a standard trans-\nformer encoder with self-attention that learns long-range\ndependencies of input motions, and a trainable mixture\nembedding module that models temporal information and\ndiscriminates key-frames. Our method can run in a non-\nautoregressive manner and predict multiple missing frames\nwithin a single forward propagation in real time. We ﬁnally\nshow the effectiveness of our method in music-dance appli-\ncations. Our animated results can be found on our project\npage https://github.com/FuxiCV/SSMCT.\n1. Introduction\nMotion completion is an important and challenging\nproblem that has been studied for a long time. Mo-\ntion completion provides fundamental technical support for\nanimation authoring of 3D characters, and has been re-\ncently successfully applied in ﬁlm production and video\ngames [45, 8].\nIn recent years, deep learning methods have greatly pro-\n*Contributed Equally. (Names are in alpha order)\n†Corresponding author.\nFigure 1. Motion completion by our method based on input\nkeyframes (blue ones). Our method is a uniﬁed framework that\ncan solve multiple motion completion problems. We achieve a\nnew state of the art on a high-quality motion completion dataset –\nLaFAN1 [16].\nmoted the research progress of motion completion. With\nrecent advances in this ﬁeld, manpower can now be greatly\nsaved, where high-quality motion can be smoothly gener-\nated from a set of historical or sparsely key-frames by learn-\ning over a large scale of motion capture data [24, 16]. How-\never, most previous methods deal with the completion prob-\nlem for different completion scenarios (in-betweening, in-\nﬁlling, and blending) with case-by-case designs. In this pa-\nper, we propose a novel framework that can unify the above\nprocessing scenarios. In our method, we leverage the recent\npopular deep learning architecture named Transformer [46],\nwhich is built on the self-attention mechanism and now has\nbeen widely used in neural language processing, computer\nvision, and reinforcement learning [42, 11, 34]. We adopt\nBERT [10], a recent well-known transformer encoder as our\nbackbone, where known frames (key-frames) and unknown\nframes (need to be complemented) are fed together orderly\nas input and thus all the frames can be predicted in a sin-\ngle propagation at inference. Our method works in a non-\nautoregressive manner and can be easily accelerated with\nGPU parallelization. As a comparison, most previous meth-\nods adopt recursive or auto-regressive prediction where the\narXiv:2103.00776v1  [cs.CV]  1 Mar 2021\nmotion frames need to be generated iteratively that lacks\nparallelism.\nWe consider motion completion in the following three\nscenarios:\n* In-betweening. Animators are required to complement\nthe motion frame-by-frame between the past frames\nand the provided further keyframe [16].\n* In-ﬁlling. As an extension of in-betweening, in-ﬁlling\nposes the characters on speciﬁc positions of the time-\nline [8], and complements the rest of the frames [24].\nA sub-task of In-ﬁlling is temporal super-resolution,\nwhere the input motion frames are considered as\nkeyframes with equal interval [15].\n* Blending. Blending focuses on the automatic gener-\nation of the transition between a pair of pre-deﬁned\nmotions and has been widely used in video games 1.\nFor example, many games provide dance fragments for\nplayers to choreograph, and blending helps players to\nconcatenate and smooth those chosen fragments.\nWe also introduce a mixture embedding module that fur-\nther integrates temporal knowledge to the transformer en-\ncoder. Our embedding module contains two types of learn-\nable embeddings: position embedding and keyframe em-\nbedding. Position embedding is a widely studied technol-\nogy in recent transformer literature, where a set of pre-\ndeﬁned sinusoidal signals are usually used to introduce tem-\nporal knowledge to the transformer model [15, 16]. In our\nmethod, we further make this embedding trainable to deal\nwith different motion completion scenarios. In addition\nto the encoding of input frame orders, we also introduce\nKeyframe embedding to annotate the input frames and tells\nthe model which parts of the input frames are keyframes (al-\nready known) and which frames need to be predicted (un-\nknown). Since the keyframe embedding may have different\nforms, our method can be easily applied to different com-\npletion scenarios regardless of how the input keyframes are\narranged. Our design can be also considered as a motion-\nversion of Mask Language Model (MLM) [44], where the\nunknown frames are represented by a deep bi-directional\nmodel trained with self-supervised losses [10].\nThe contributions of our paper are summarized as fol-\nlows:\n1. We investigate the capability of the transformer-based\nmodel in the motion completion task. We propose a sim-\nple but efﬁcient method to solve the motion completion\nproblems of different application scenarios under a uniﬁed\nframework.\n1https://docs.unity3d.com/Packages/com.unity.\ntimeline@1.6/manual/clp_blend.html\n2. Our method works in a parallel prediction manner\nwith high computational efﬁciency. On a single CPU desk-\ntop (I7-8700K @ 3.70GHz), our method can run in real\ntime (40 motion sequences per second, each with 30 frames\nlong).\n3. We achieve new in-betweening benchmarking accu-\nracy under multiple evaluation settings. Results on other\ncompletion tasks (in-ﬁlling and blending) are also reported\nas baselines for fostering the research community 2.\n2. Related works\n2.1. Motion completion\nMotion completion is an emerging research hot-spot in\ncomputer graphics and multimedia. Motion completion\ncan be viewed as a conditional motion sequence genera-\ntion problem. Different from those unconditional motion\nsynthesis tasks [40, 47, 43] that focus on the generation of\nunconstrained motion sequences by directly sampling from\ntheir posterior distribution, motion completion aims at ﬁll-\ning the missing frames in a temporal sequence based on a\ngiven set of keyframes.\nMotion completion has a long research history, which\ncan be traced back to the late 1980s. Early works of mo-\ntion completion typically adopt inverse kinematics to gen-\nerate realistic transitions between keyframes. For example,\nspace-time constraints and searching based methods were\nproposed in the 1980s-1990s [48, 33] to compute optimal\nphysically-realistic trajectory. By using such techniques,\ntransitions between different motions can be smoothly gen-\nerated [38]. Also, probabilistic models like the maximum\na posterior methods [7, 32], the Gaussian process [47],\nMarkov models [29] were introduced to motion completion\ntasks and were commonly used after 2000s.\nRecently, deep learning methods have greatly promoted\nthe research and the performance of motion completion,\nwhere the recurrent neural network is the most commonly\nused framework for motion completion of the deep learning\nera [51, 15]. For example, Harvey et al. introduce a novel\nframework named Recurrent Transition Networks (RTN) to\nlearn a more complex representation of human motions with\nthe help of LSTM [15]. Besides, generative adversarial\nlearning has been also introduced to the motion comple-\ntion to make the output motion more realistic and natural-\nistic [19]. Some non-recurrent motion completion models\nare also proposed very recently. Kaufmann et al. propose\nan end-to-end trainable convolutional autoencoder to ﬁll in\nmissing frames [24]. Another recent progress by Harvey\net al. introduces time-to-arrival embeddings and scheduled\ntarget-noise to further enhance the performance of RTN and\nachieve impressive completion results [16]. Different from\nthe previous RNN-based or convolution-based method, we\n2We will make our model and pre-trained weights publically available.\npropose a transformer-based model, which is a more uni-\nﬁed solution to this task and can deal with arbitrary missing\nframes in a single-shot prediction manner.\n2.2. Motion control\nMotion control is a typical conditional motion generation\ntask, which is also highly related to motion completion. In\nmotion control, the control signal comes from a pre-deﬁned\ntemporal sequence, e.g. root trajectory, rather than a set of\nkeyframes in motion completion.\nGraph-based motion control is the most common type of\nmethod in this ﬁeld before the deep learning era [27, 39, 26,\n1, 5]. For example, Arikan et al. formulate the motion gen-\neration as a randomized search of the motion graph, which\nallows to edit complex motions interactively [1]. Beaudoin\net al. propose a string-based motif-ﬁnding algorithm named\nMotion-Motif Graphs, which further considers the motif\nlength and the number of motions in a motif [5]. Besides,\nthere are many statistical methods proposed to avoid search-\ning from predeﬁned motion templates [13, 47, 32, 31].\nChai et al . propose a statistical dynamic model to gener-\nate motions with motion prior (e.g. user-deﬁned trajectory)\nand formulate the constrained motion synthesis as a max-\nimum a posterior problem [7]. Ye et al. introduce a non-\nlinear probabilistic dynamic model that can handle pertur-\nbations [49]. Levine et al. propose a probabilistic motion\nmodel that learns a low-dimensional space from example\nmotions and generates character animation based on user-\nspeciﬁed tasks [30].\nRecently, deep learning methods have become the main-\nstream motion control method. Some popular architec-\ntures like convolutional autoencoder and recurrent neural\nnetwork are widely used in this problem [22, 21, 20, 28].\nAdversarial training has also played an important role in\nmotion control and can help generate more realistic mo-\ntion sequences [4, 14, 16]. Besides, some recent approaches\nalso leverage reinforcement learning to further incorporate\nphysical rules to improve the quality of the generation re-\nsults [9, 50, 3, 37, 36, 6].\n3. Methods\nIn this work, we formulate the motion completion as a\nsequence-to-sequence prediction problem. The unknown\nmotion frames can be generated in a single inference for-\nward propagation conditioned by those input keyframes.\nWe choose BERT, an off-the-shelf transformer architec-\nture [10] as our network backbone with minimum modiﬁ-\ncations, and thus the subsequent varieties can be introduced\nwithout impediment.\n3.1. Motion completion transformer\nFig.2 shows an overview of our method. Our network\nconsists of 1) a mixture embedding module that converts\nthe motion to a set of sequential tokens, and 2) a stan-\ndard transformer encoder used to process sequential fea-\ntures. Our method supports multiple input coded format,\ne.g. [local positions & rotations] or [global positions & ro-\ntations] or [positions only]. Without loss of generality, we\nassume that the input has both positions ( x, y, z) and rota-\ntions (q0, q1, q2, q3) variables (no matter local or global),\nand therefore, a single pose contains a position coordinate\nmatrix P ∈ RJ×3 and a quaternion matrix Q ∈ RJ×4,\nwhere J represents the joint number of the input pose.\nFor each input pose, we ﬁrstly ﬂatten P and Q into 1-\nD vectors p and q, and then concatenate the two vectors\ntogether into a long vector:\nx = [p,q] ∈RJ×(3+4). (1)\nFor those unknown input frames, we use the linear inter-\npolation to ﬁll in their missing values along the temporal\ndimension before feeding their pose vector to the model.\nWe then use a 1-D temporal convolution layer to trans-\nform those pose vectors to a set of “language tokens”:\nZ = Conv1d([x1; x2; ...; xT ]) (2)\nwhere T is the length of the input sequence, Z ∈RT×F\nis the formated temporal feature of the tokens, and F is\nthe output dimension of the Conv1d layer. The convolu-\ntion is performed in the joint dimension. Note that different\nfrom the previous transformer-based models in computer\nvision [11] that use a linear projection layer to generate the\nembeddings, here we use a convolution layer for better cap-\nturing the temporal information, e.g. velocity and accelera-\ntion.\nConsidering that the transformer cannot know the order\nand the exact location of the keyframes in the input se-\nquence, we introduce a mixture embedding E to annotate\nthese frames before feeding their features into transformer.\nFor each group of input conﬁguration of keyframes, an em-\nbedding E is learned as a global variable on the training\ndata and will not change along with the pose feature Z. We\nrepresent the ﬁnal annotated features Z as follows:\nˆZ = [z1 + e1; z2 + e2; ...; zT + eT ] (3)\nwhere zt and et are the sub-vectors of input feature Z and\nmixture embedding E at the time t.\nThe BERT transformer we used consists of multiple en-\ncoder layers [10]. Each encoder layer further consists of a\nmulti-head self-attention layer (MHSA) and a feed-forward\nnetwork (FFN). A residual connection [17] is applied across\nthe two layers. The forward mapping of the transformer can\nbe written as follows:\nˆHl = Norm(Hl−1 + MHSA(Hl−1))\nHl = Norm( ˆHl + FFN( ˆHl))\n(4)\nFigure 2. An overview of our method. Our method consists of a standard transformer encoder, a mixture embedding layer and input/output\nconvolutional heads. In motion completion, the unknown input frames are ﬁrst generated by using linear interpolation (shown in red)\nbefore being fed to the model. Our method takes in a whole masked sequence and complete the prediction within only a single forward\npropagation.\nwhere H is the output of hidden layers. l = 1,...,L are\nthe indices of encoder layers. “Norm” represents the layer\nnormalization [2] placed at the output end of the residual\nconnections. We use H0 = Norm( ˆZ) as the hidden repre-\nsentation of the input layer.\nIn the multi-head self-attention layer (MHSA), a dense\ncomputation between each pair of input frames are con-\nducted, and thus a very long-range temporal relations be-\ntween frames can be captured. The processing of a single\nhead can be represented as follows:\nfatt = Softmax(QKT\nα )V (5)\nwhere Q = WqH represents a query matrix, K = WkH\nrepresents a key matrix, and V = WvH represents a value\nmatrix. Wq, Wk and Wv are all learnable matrices. We\nfollow the multi-head attention conﬁguration in BERT and\nset the dimension of Q, K and V to 1\nM of the input H,\nM represents the number of heads in the attention layer.\nFinally, the outputs from different heads are collected, con-\ncatenated and projected by a matrix Wmhsa as the output\nof the MHSA layer:\nMHSA(H) =Wmhsa[f(1)\natt ; f(2)\natt ; ...; f(M)\natt ]. (6)\nFor the feed-forward network (FFN), it consists of two\nlinear layers and a GeLU layer [18]. FFN processes each of\nthe frame individually:\nFFN(H) =W(1)\nffn (GeLU(W(2)\nffn (H))), (7)\nwhere W(1)\nffn and W(2)\nffn are the learnable linear projection\nmatrices. Finally, we apply another 1d-convolution layer\nat the output end of the transformer, and ﬁnal completion\noutput Y can be written as follows:\nY = Conv1d(HN ). (8)\n3.2. Mixture embeddings\nThe mixture embedding E we used consists a positional\nembedding Epos ∈ RT×F and a keyframe embedding\nEkf ∈RT×F , where T is the length of temporal sequence\nand F is the input feature dimension of transformer. Fig.3\ngives an illustration of the mixture embedding module.\nThe position embedding Epos is a matrix that containsT\nsub-vectors, each for a single time step:\nEpos = [e1\npos,e2\npos,..., eT\npos]. (9)\nThe keyframe embeddings Ekf are selected from a learn-\nable dictionary D that contains three types of embed-\nding vectors D = {ˆe0,ˆe1,ˆe2}, which annotate the\nkeyframes, unknown frames, and ignored frames, respec-\ntively (Keyframe = 0, Unknown = 1, ignored = 2). These\nFigure 3. An illustration of mixture embedding module. We de-\nsign a position embedding and a keyframe embedding, where the\nformer one integrates position information and the latter one an-\nnotates whether the frame is the keyframe or not. (Blue dash-lines\nrepresent ignored frames that exceeds the prediction range)\ntypes of frames can be conﬁgured in any forms accord-\ning to different completion tasks and scenarios (e.g., in-\nbetweening, in-ﬁlling, and blending). The keyframe em-\nbeddings are written as follows:\nEkf = [e1\nkf ,e2\nkf ,..., eT\nkf ], (10)\nwhere em\nkf ∈{ˆe0,ˆe1,ˆe2}. Finally, the two types of embed-\ndings are mixed by adding together position-by-position:\nE = Epos + Ekf . (11)\nIt is worth noting that our keyframe embedding is not lim-\nited to the above three conﬁgurations shown in Fig.3. It can\nbe in any format with practical meaning. As a special case,\nif the keyframes are randomly speciﬁed, then our keyframe\nembedding turns out to be the random token mask in the\noriginal BERT paper [10].\n3.3. Loss function\nWe train our transformer and the embeddings with multi-\ntasks regression losses. Given a set of predicted motions\nand their ground truth, we design our pose reconstruction\nloss Lrec as follows:\nLrec = 1\nNT\nN∑\nn=1\nT∑\nt=1\n(\npt\nn −ˆpt\nn\n\n1 +\nqt\nn −ˆqt\nn\n\n1) (12)\nwhere pt\nn and qt\nn are the position coordinates and rotation\nquaternions of the predicted motion sequence nat the time\nstep t. ˆpt\nn and ˆqt\nn T are their ground truth. N represents the\nlength of motion sequence and the total number of training\nsequences in the data set. Note that the above losses can\nbe used in both global and local coordinate systems. When\napplied in the local one, the pt\nn can be replaced by rt\nn, i.e.\nroot coordinates, since the T-pose (or offsets) will always\nbe a constant vector for the same character.\nIn addition to the above pose reconstruction loss, we also\nintroduce two kinematics losses to improve the results in\ndifferent coordinate systems. 1) Forward Kinematics (FK)\nloss LFK . We follow Harvey et al. [16] and apply FK loss\nto our multi-task training loss. The main idea behind the\nFK loss is to calculate the global position coordinates by\nusing local ones with forward kinematics and weight the\nlocal reconstruction loss on different joints:\nLFK = ∥FK(r,qlocal) −pglobal∥1 , (13)\n2). Inverse Kinematics (IK) loss LIK . We also apply IK\nloss to constrain the T-pose when in global coordinate sys-\ntem. We ﬁrst compute the local position from the global\none with inverse kinematics, and then we compare the off-\nsets between the inverse output and the original input:\nLIK = ∥IK(pglobal,qglobal) −b∥1 (14)\nwhere b represents the offset vector. We remove the root\ncoordinate and keep predicted offsets only when computing\nthe IK loss.\nOur ﬁnal training loss is written as follow:\nL= αrecLrec + αKLK, (15)\nwhere αrec and αK are the coefﬁcients to balance differ-\nent loss terms. LK represents the kinematics loss in global\nsystem (LIK ) or in local system (LFK ).\n3.4. Implementation details\nIn our method, we adopt BERT [10] as the backbone\nof our transformer with 8 encoder layers. In each encoder\nlayer, we set the number of attention heads to M = 8. For\nour input and output Conv1d layers, the kernel size is set\nto 3 and the padding is set to 1. We set the dimension of\nthe feature embedding in the MHSA layers to 256, and set\nthose in the FFN layers to 512. In our training loss, we set\nαrec = 1.0 and αK = 0.01. Consider that the quaternions\nq ∈[0,1] while the position coordinates p are in a much\nlarger range, we scale the localization loss and the rotation\nloss to the same order of magnitude.\nWe train our network by using Adam optimizer [25]. We\nset the maximum learning rate to 10−3. The whole frame-\nwork is implemented by using PyTorch [35]. For a more\ndetailed training conﬁguration, please refer to our experi-\nmental section.\n4. Experiments\n4.1. Datasets and motion completion tasks\nIn our experiment, we evaluate our method across three\ndifferent motion completion tasks:\n1. In-betweening on LaFAN1 [16]: LaFAN1 is a public\nhigh-quality general motion dataset introduced by Harvey\nTable 1. Experimental results on LaFAN1 dataset. A lower score indicates better performance. (*Note that for a fair comparison, the T-pose\nof our global results have been replaced by a standard one in local coordinate system.)\nL2Q L2P NPSS\nLength 5 15 30 5 15 30 5 15 30\nZero-Vel 0.56 1.10 1.51 1.52 3.69 6.60 0.0053 0.0522 0.2318\nInterp 0.22 0.62 0.98 0.37 1.25 2.32 0.0023 0.0391 0.2013\nERD-QV ([16]) 0.17 0.42 0.69 0.23 0.65 1.28 0.0020 0.0258 0.1328\nOurs (local w/o FK) 0.18 0.47 0.74 0.27 0.82 1.46 0.0020 0.0307 0.1487\nOurs (local) 0.17 0.44 0.71 0.23 0.74 1.37 0.0019 0.0291 0.1430\nOurs (global w/o ME & IK) 0.16 0.37 0.63 0.24 0.61 1.16 0.0018 0.0243 0.1284\nOurs (global w/o IK) 0.14 0.36 0.61 0.21 0.57 1.11 0.0016 0.0238 0.1241\nOurs* (global-full) 0.14 0.36 0.61 0.22 0.56 1.10 0.0016 0.0234 0.1222\nTable 2. Speed performance comparison. CPU inference time are\nrecorded in different batch sizes (1 & 10) where Inbetweening\nlength is set to 30 frames (i.e. 1 second).\nMethod 1 x 30 10 x 30 CPU info\nERD-QV [16] 0.31s 0.40s E5-1650 @ 3.20GHz\nOurs 0.025s 0.083s I7-8700K @ 3.70GHz\net al. from Ubisoft. In the in-betweening completion task,\ngiven the past 10 keyframes and another future keyframe,\nwe aim to predict the motion of the rest frames.\n2. In-ﬁlling on Anidance [41]: Anidance is a public\nmusic-dance dataset proposed by Tang et al. [41]. We test\nour method on this dataset for the in-ﬁlling task, where\nequally spaced keyframes are given.\n3. Blending on our dance dataset: We collect a new\ndance movement dataset, which contains high-quality dance\nmovements performed by senior dancers and is more chal-\nlenging than the previous ones. We evaluate this dataset for\nexploring the potential of our method in dance applications\nin game environments.\nTo build our dataset, we follow the classic choreography\ntheory of Doris Humphrey [23] and deﬁne dance phrases as\nour basic movement unit. We invited four senior dancers\nto perform ﬁve types of dance movements (including Jazz\ndance, Street dance, J-pop dance, Indian dance, Uygur\ndance). We use motion capture devices (Vicon V16 cam-\neras) to record the dance movements in 30Hz. Finally, more\nthan 130,000 frames are recorded in our dataset. For con-\nvenience, we re-target the dance movements on a standard\ncharacter released by Harvey et al. [16].\n4.2. Metrics\nWe follow Harvey et al . [16] and use L2Q, L2P, and\nNPSS as our evaluation metrics. The L2Q deﬁnes the aver-\nage L2 distances of the global quaternions between the pre-\ndicted motions and their ground truth. Similarly, the L2P\ndeﬁnes the average L2 distances of the global positions. 3\nThe NPSS, proposed by Gopalakrishnan [12], is a variant\nof L2Q, which computes the Normalized Power Spectrum\nSimilarity and is based on angular frequency distance be-\ntween the prediction and the groundtruth.\nNote that when we generate our results based on the\nglobal coordinate system, we replace the T-pose of these\nresults with the standard one under the local coordinate sys-\ntem (by the simplest IK, i.e. global to local coordinate trans-\nformation) for a fair comparison. We found this operation\nmay slightly reduce our accuracy. We will give more dis-\ncussion on this interesting observation in our Discussion\nsection.\n4.3. Motion In-betweening\nWe evaluate our method on the LaFAN1 dataset [16]\nfor the motion in-betweening task. This dataset contains\n496,672 frames performed by 5 motion subjects and are\nrecorded by using Mocap in 30Hz. The training set and\nthe test set are clearly separated, where the test set con-\ntains motions from subject No.5 only. Since the originally\ncaptured motions are in very long sequences, motion win-\ndows are introduced on this dataset, where the width of the\nwindow is set to 50 (65) frames, and the offset is set to 20\n(25) frames for training (test). Finally, there are 20,212 and\n2,232 windows for training and test, respectively. We train\nour model on LaFAN1 for 1,000 epochs with random ini-\ntialization. We set the maximum learning rate to 10−3, and\nset the weight decay to 0.75 every 200 epochs. We further\nadopt a warm-up strategy for the ﬁrst 50 training epochs,\nwhere the learning rate goes up from 0 to the maximum\nlearning rate gradually. Since our method can take in arbi-\ntrary inputs, we set the transition length of in-betweening to\n5∼39 (since the maximum length is 39). For the unknown\nframes, before feeding them to our model, we interpolate\n3In Harvey et al.’s implementation, the predicted positions and their\nground truth are normalized by mean and std of the training set. We also\nfollow this setting for a fair comparison.\nFigure 4. Our transformer-based inﬁlling results and linear interpolation based results on the anidance test set. The ﬁst row is the ground\ntruth. In the rest rows, red skeletons are predicted and black ones are input keyframes.\nFigure 5. Our transformer-based inﬁlling results and linear interpolation based results on the anidance test set (In this experiment, keyframes\nare randomly chosen from the test set with a random order for simulating in-the-wild scenario).\nthem based on the nearest two keyframes by linear interpo-\nlation (LERP) and spherical linear interpolation (SLERP).\nWhen the training stops, we evaluate our method on\nthe test set of LaFAN1. Tab 1 shows the evaluation re-\nsult, where interpolation and zero-velocity are used as our\nnaive baselines [16]. During the evaluation, only the ﬁrst 10\nkeyframes and another keyframe at the frame10+ L+1 are\ngiven, where Lis a predeﬁned transition length. We keep\nthe same conﬁguration with Harvery et al., where transition\nlengths are set to 5, 15, and 30. We evaluate our method\nunder both local and global coordinate systems.\nTable 3. Inﬁlling results on anidance dataset [41] (A lower score\nindicates a better performance).\nL2P\nLength 5 15 30\nZero-Vel 2.34 5.12 6.73\nInterp 0.94 3.24 4.68\nOurs (full) 0.84 1.46 1.64\nWe show in Tab 1 that our method achieves a high accu-\nracy on the LaFAN1 dataset even when the transition length\n= 5. We can also see that there is a noticeable accuracy im-\nprovement when switching the generation mode from local\nto global. This may be because of the accumulative errors of\nrotations in local coordinates, and the previous method pro-\nposes to use the FK loss to reduce this error [16]. Our results\nsuggest that the motion completion can be better solved in\nthe global coordinate system, although the T-pose predicted\nin the global may not be accurate enough (can be further\nimproved by IK loss). We further evaluate the Mixture Em-\nbedding (ME) and IK loss used in our method. We can see\nboth the two strategies signiﬁcantly improves the accuracy\nin all evaluation settings (L=5, 15, and 30).\nBesides, Tab 2 indicates that our method can achieve\na very high inference speed on CPU device and can even\nrun in real-time ( < 0.033s), which beneﬁts from the non-\nautoregressive design.\n4.4. Dance in-ﬁlling\nNext, we evaluate our method on the Anidance\ndataset [41]. In this dataset, four types of dance movements\n(Cha-cha, Tango, Rumba, and Waltz) are captured in the\nglobal coordinate system. This dataset was originally de-\nsigned for music-to-dance generation and contains 61 inde-\npendent dance fragments with 101,390 frames.\nWe apply similar evaluation settings on this dataset,\nTable 4. Blending results of our new dance dataset (A lower score indicates a better performance).\nL2Q L2P NPSS\nLength 8 16 32 8 16 32 8 16 32\nZero-Vel 2.17 2.67 3.22 3.68 5.15 7.52 0.2061 0.6004 1.7998\nInterp 2.00 2.55 3.14 1.84 2.87 4.19 0.1948 0.5781 1.7218\nOurs (global-native-Tpose) 1.62 2.03 2.48 1.55 2.32 3.29 0.1906 0.5438 1.4758\nOurs (global-standard-Tpose) 1.62 2.03 2.48 1.71 2.46 3.45 0.1906 0.5438 1.4758\nFigure 6. Results of different blending methods on our dataset with\nwindow width = 16. Our results are much closer to the ground\ntruth.\nwhere the time window is set to 128 frames and the offset\nis set to 64. 20% dance fragments are randomly selected as\nthe test set, and thus there are 1,117 sequences in the train-\ning set and 323 sequences in the test set. We train our model\non this dataset with 3000 epochs, and the rest of the conﬁg-\nurations are kept the same with Sec. 4.3. We set the interval\nof keyframes to 5∼30 for in-ﬁlling, which means that there\nare only 5 keyframes are given to the model when their in-\nterval is 30, and the initial transition between keyframes are\nalso interpolated by LERP.\nTab 3 shows the evaluation results of our method on the\ntest set of Anidance. The transition lengths are set to 5,\n15, and 30 respectively. We evaluate our method under the\nglobal coordinate system since the anidance dataset con-\ntains global positions only. Similar to the results in the\nin-betweening task, our method can achieve high comple-\ntion accuracy on long-term completion, and can also out-\nperform LERP in short-term completion. Our method can\nalso handle in-the-wild input (random keyframes) very well\nas shown in Fig.4 and 5, where our method complete much\nmore meaningful dance movements than the LERP.\n4.5. Dance blending\nThe above experiments show that our method can apply\nvery well to the current public datasets. For further evaluat-\ning our method, we test our method on a very challenging\ndataset we built. Our dataset contains more complex and\ndiverse dance movements. In our dataset, the time window\nis set to 64 and the offset is set to 32, and there are ﬁnally\n3,220 sequences in the training set and 400 sequences in the\ntest set. We keep the same conﬁguration with Sec. 4.3, but\nuse pre-trained weights from Sec. 4.3 for initializing.\nWe set the blending window to 5∼32 frames. For exam-\nple, when the window is 32, the ﬁrst and last 16 frames are\ngiven as keyframes and rests are masked. Tab 4 shows our\nevaluation results with the window width = 8, 16, 32. Fig.\n6 shows our results with the window width = 16, which is a\ncommonly-used setting (∼0.5s) in auto-blending. We can\nsee our method can still achieve very high quality results on\nthis challenging dataset.\n4.6. Discussion\nOur method can work on both global and local coor-\ndinate systems. When directly predicting poses in the\nglobal coordinate system, the bone length may not be well-\ncontrolled, especially in those games with relative coordi-\nnate systems (e.g., local root and rotation), although it may\nbring a higher accuracy. For example, in the last two rows\nof Tab 4, we evaluate our result by using standard T-pose\ncriteria and the penultimate one (global-native-Tpose) re-\nspectively. The global-native-Tpose is slightly more close\nto the ground truth but is hard to directly apply to local co-\nordinate applications. We also notice that the global rotation\nmay have discontinuity when the dance is very fast (caused\nby SLERP). However, this is not a serious problem on the\nLaFAN1 dataset since the motions in this dataset are slow.\nWe will also further investigate this interesting problem.\n5. Conclusion\nIn this paper, we propose a simple but effective method\nto solve motion completion problems under a uniﬁed frame-\nwork. In our method, a standard transformer encoder is in-\ntroduced to handle the arbitrary sequence input, and a mix-\nture embedding is introduced to better encode temporal in-\nformation of multiple input types. Our method can predict\nmultiple missing frames in a single forward propagation\nat inference time rather than running in an auto-regressive\nmanner. Experimental results show that our method can be\nwell applied to different motion completion modes, includ-\ning in-betweening, in-ﬁlling and blending, and achieves a\nnew state of the art accuracy on the LaFAN1 dataset.\nReferences\n[1] Okan Arikan and David A Forsyth. Interactive motion gen-\neration from examples. ACM Transactions on Graphics\n(TOG), 21(3):483–490, 2002.\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[3] Nir Baram, Oron Anschel, and Shie Mannor. Model-\nbased adversarial imitation learning. arXiv preprint\narXiv:1612.02179, 2016.\n[4] Emad Barsoum, John Kender, and Zicheng Liu. Hp-gan:\nProbabilistic 3d human motion prediction via gan. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition workshops, pages 1418–1427, 2018.\n[5] Philippe Beaudoin, Stelian Coros, Michiel van de Panne, and\nPierre Poulin. Motion-motif graphs. In Proceedings of the\n2008 ACM SIGGRAPH/Eurographics Symposium on Com-\nputer Animation, pages 117–126, 2008.\n[6] Kevin Bergamin, Simon Clavet, Daniel Holden, and\nJames Richard Forbes. Drecon: data-driven responsive\ncontrol of physics-based characters. ACM Transactions on\nGraphics (TOG), 38(6):1–11, 2019.\n[7] Jinxiang Chai and Jessica K Hodgins. Constraint-based mo-\ntion optimization using a statistical dynamic model. In ACM\nSIGGRAPH 2007 papers, pages 8–es. 2007.\n[8] Lo ¨ıc Ciccone, Cengiz ¨Oztireli, and Robert W Sumner.\nTangent-space optimization for interactive animation con-\ntrol. ACM Transactions on Graphics (TOG) , 38(4):1–10,\n2019.\n[9] Stelian Coros, Philippe Beaudoin, and Michiel Van de Panne.\nRobust task-based control policies for physics-based char-\nacters. In ACM SIGGRAPH Asia 2009 papers , pages 1–9.\n2009.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[12] Anand Gopalakrishnan, Ankur Mali, Dan Kifer, Lee Giles,\nand Alexander G Ororbia. A neural temporal model for hu-\nman motion prediction. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n12116–12125, 2019.\n[13] Keith Grochow, Steven L Martin, Aaron Hertzmann, and Zo-\nran Popovi´c. Style-based inverse kinematics. In ACM SIG-\nGRAPH 2004 Papers, pages 522–531. 2004.\n[14] Liang-Yan Gui, Yu-Xiong Wang, Xiaodan Liang, and\nJos´e MF Moura. Adversarial geometry-aware human mo-\ntion prediction. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 786–803, 2018.\n[15] F ´elix G Harvey and Christopher Pal. Recurrent transition\nnetworks for character locomotion. InSIGGRAPH Asia 2018\nTechnical Briefs, pages 1–4. 2018.\n[16] F ´elix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and\nChristopher Pal. Robust motion in-betweening. ACM Trans-\nactions on Graphics (TOG), 39(4):60–1, 2020.\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv preprint arXiv:1606.08415, 2016.\n[19] Alejandro Hernandez, Jurgen Gall, and Francesc Moreno-\nNoguer. Human motion prediction via spatio-temporal in-\npainting. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 7134–7143, 2019.\n[20] Daniel Holden, Taku Komura, and Jun Saito. Phase-\nfunctioned neural networks for character control. ACM\nTransactions on Graphics (TOG), 36(4):1–13, 2017.\n[21] Daniel Holden, Jun Saito, and Taku Komura. A deep learning\nframework for character motion synthesis and editing. ACM\nTransactions on Graphics (TOG), 35(4):1–11, 2016.\n[22] Daniel Holden, Jun Saito, Taku Komura, and Thomas\nJoyce. Learning motion manifolds with convolutional au-\ntoencoders. In SIGGRAPH Asia 2015 Technical Briefs ,\npages 1–4. 2015.\n[23] Doris Humphrey. The art of making dances . Dance Hori-\nzons, 1959.\n[24] Manuel Kaufmann, Emre Aksan, Jie Song, Fabrizio Pece,\nRemo Ziegler, and Otmar Hilliges. Convolutional autoen-\ncoders for human motion inﬁlling. In 8th international con-\nference on 3D Vision (3DV 2020)(virtual), page 263, 2020.\n[25] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014.\n[26] Lucas Kovar, Michael Gleicher, and Fr´ed´eric Pighin. Motion\ngraphs. In ACM SIGGRAPH 2008 classes, pages 1–10. 2008.\n[27] Jehee Lee, Jinxiang Chai, Paul SA Reitsma, Jessica K Hod-\ngins, and Nancy S Pollard. Interactive control of avatars\nanimated with human motion data. In Proceedings of the\n29th annual conference on Computer graphics and interac-\ntive techniques, pages 491–500, 2002.\n[28] Kyungho Lee, Seyoung Lee, and Jehee Lee. Interactive char-\nacter animation by learning multi-objective control. ACM\nTransactions on Graphics (TOG), 37(6):1–10, 2018.\n[29] Andreas M Lehrmann, Peter V Gehler, and Sebastian\nNowozin. Efﬁcient nonlinear markov models for human mo-\ntion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 1314–1321, 2014.\n[30] Sergey Levine, Jack M Wang, Alexis Haraux, Zoran\nPopovi´c, and Vladlen Koltun. Continuous character control\nwith low-dimensional embeddings. ACM Transactions on\nGraphics (TOG), 31(4):1–10, 2012.\n[31] Jianyuan Min and Jinxiang Chai. Motion graphs++ a com-\npact generative model for semantic motion analysis and syn-\nthesis. ACM Transactions on Graphics (TOG), 31(6):1–12,\n2012.\n[32] Jianyuan Min, Yen-Lin Chen, and Jinxiang Chai. Interac-\ntive generation of human animation with deformable motion\nmodels. ACM Transactions on Graphics (TOG), 29(1):1–12,\n2009.\n[33] J Thomas Ngo and Joe Marks. Spacetime constraints revis-\nited. In Proceedings of the 20th annual conference on Com-\nputer graphics and interactive techniques , pages 343–350,\n1993.\n[34] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pas-\ncanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg,\nRaphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Sta-\nbilizing transformers for reinforcement learning. In Interna-\ntional Conference on Machine Learning , pages 7487–7498.\nPMLR, 2020.\n[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zem-\ning Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-\nson, Andreas Kopf, Edward Yang, Zachary DeVito, Mar-\ntin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit\nSteiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:\nAn imperative style, high-performance deep learning library.\nIn Advances in Neural Information Processing Systems 32 ,\npages 8024–8035. Curran Associates, Inc., 2019.\n[36] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel\nvan de Panne. Deepmimic: Example-guided deep reinforce-\nment learning of physics-based character skills. ACM Trans-\nactions on Graphics (TOG), 37(4):1–14, 2018.\n[37] Xue Bin Peng, Glen Berseth, KangKang Yin, and Michiel\nVan De Panne. Deeploco: Dynamic locomotion skills using\nhierarchical deep reinforcement learning. ACM Transactions\non Graphics (TOG), 36(4):1–13, 2017.\n[38] Charles Rose, Brian Guenter, Bobby Bodenheimer, and\nMichael F Cohen. Efﬁcient generation of motion transitions\nusing spacetime constraints. In Proceedings of the 23rd an-\nnual conference on Computer graphics and interactive tech-\nniques, pages 147–154, 1996.\n[39] Alla Safonova and Jessica K Hodgins. Construction and op-\ntimal search of interpolated motion graphs. In ACM SIG-\nGRAPH 2007 papers, pages 106–es. 2007.\n[40] Hedvig Sidenbladh, Michael J Black, and Leonid Sigal. Im-\nplicit probabilistic models of human motion for synthesis\nand tracking. In European conference on computer vision ,\npages 784–800. Springer, 2002.\n[41] Taoran Tang, Hanyang Mao, and Jia Jia. Anidance: Real-\ntime dance motion synthesize to the song. In Proceedings\nof the 26th ACM international conference on Multimedia ,\npages 1237–1239, 2018.\n[42] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. Efﬁcient transformers: A survey. arXiv preprint\narXiv:2009.06732, 2020.\n[43] Graham W Taylor, Geoffrey E Hinton, and Sam T Roweis.\nModeling human motion using binary latent variables. In\nAdvances in neural information processing systems , pages\n1345–1352, 2007.\n[44] Wilson L Taylor. “cloze procedure”: A new tool for measur-\ning readability. Journalism quarterly, 30(4):415–433, 1953.\n[45] Adrian Thomas. Integrated graphic and computer mod-\nelling. Springer Science & Business Media, 2009.\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017.\n[47] Jack M Wang, David J Fleet, and Aaron Hertzmann. Gaus-\nsian process dynamical models for human motion. IEEE\ntransactions on pattern analysis and machine intelligence ,\n30(2):283–298, 2007.\n[48] Andrew Witkin and Michael Kass. Spacetime constraints.\nACM Siggraph Computer Graphics, 22(4):159–168, 1988.\n[49] Yuting Ye and C Karen Liu. Synthesis of responsive mo-\ntion using a dynamic model. In Computer Graphics Forum,\nvolume 29, pages 555–562. Wiley Online Library, 2010.\n[50] KangKang Yin, Kevin Loken, and Michiel Van de Panne.\nSimbicon: Simple biped locomotion control. ACM Transac-\ntions on Graphics (TOG), 26(3):105–es, 2007.\n[51] Xinyi Zhang and Michiel van de Panne. Data-driven auto-\ncompletion for keyframe animation. In Proceedings of the\n11th Annual International Conference on Motion, Interac-\ntion, and Games, pages 1–11, 2018."
}