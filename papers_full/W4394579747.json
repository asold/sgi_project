{
  "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
  "url": "https://openalex.org/W4394579747",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4282413341",
      "name": "Sonish Sivarajkumar",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2097396323",
      "name": "Mark Kelley",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5092897313",
      "name": "Alyssa Samolyk-Mazzanti",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A1108061751",
      "name": "Shyam Visweswaran",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2156867267",
      "name": "YanShan Wang",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A4282413341",
      "name": "Sonish Sivarajkumar",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2097396323",
      "name": "Mark Kelley",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5092897313",
      "name": "Alyssa Samolyk-Mazzanti",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A1108061751",
      "name": "Shyam Visweswaran",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2156867267",
      "name": "YanShan Wang",
      "affiliations": [
        "University of Pittsburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2768488789",
    "https://openalex.org/W4308590518",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4378711639",
    "https://openalex.org/W4287570254",
    "https://openalex.org/W4367626167",
    "https://openalex.org/W4361806442",
    "https://openalex.org/W4385565111",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2962767298",
    "https://openalex.org/W2104752900",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W2250962271",
    "https://openalex.org/W3183495504",
    "https://openalex.org/W1672757658",
    "https://openalex.org/W2151363315",
    "https://openalex.org/W2908840510",
    "https://openalex.org/W2765665268",
    "https://openalex.org/W1976107079",
    "https://openalex.org/W4230060794",
    "https://openalex.org/W2095588973",
    "https://openalex.org/W2964179635",
    "https://openalex.org/W3184324824",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4206638982",
    "https://openalex.org/W2915623326",
    "https://openalex.org/W4389543542",
    "https://openalex.org/W4391292768",
    "https://openalex.org/W4221150520",
    "https://openalex.org/W3212156961",
    "https://openalex.org/W4310895557",
    "https://openalex.org/W3106224367"
  ],
  "abstract": "Background Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. Objective The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types—heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models. Methods This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches. Results The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types. Conclusions This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.",
  "full_text": "Original Paper\nAn Empirical Evaluation of Prompting Strategies for Large\nLanguage Models in Zero-Shot Clinical Natural Language\nProcessing: Algorithm Development and Validation Study\nSonish Sivarajkumar1, BS; Mark Kelley2, MS; Alyssa Samolyk-Mazzanti2, MS; Shyam Visweswaran1,3, MD, PhD;\nYanshan Wang1,2,3, PhD\n1Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA, United States\n2Department of Health Information Management, University of Pittsburgh, Pittsburgh, PA, United States\n3Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA, United States\nCorresponding Author:\nYanshan Wang, PhD\nDepartment of Health Information Management\nUniversity of Pittsburgh\n6026 Forbes Tower\nPittsburgh, PA, 15260\nUnited States\nPhone: 1 4123832712\nEmail: yanshan.wang@pitt.edu\nAbstract\nBackground: Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP),\nespecially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical\nknowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks\nwithout any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding\nthe strengths and weaknesses of different LLMs and prompt engineering approaches.\nObjective: The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly\nintroduced types—heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained\nlanguage models.\nMethods: This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of\nthought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence\nextraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these\nprompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta).\nThe study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches.\nResults: The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical\nNLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical\nevidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting\nimproved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5\nconsistently outperformed Gemini and LLaMA-2 across tasks and prompt types.\nConclusions: This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative\ntechniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These\nfindings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in\nclinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different\nprompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire\nand inform future research in this area.\n(JMIR Med Inform 2024;12:e55318) doi: 10.2196/55318\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 1https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nKEYWORDS\nlarge language model; LLM; LLMs; natural language processing; NLP; in-context learning; prompt engineering; evaluation;\nzero-shot; few shot; prompting; GPT; language model; language; models; machine learning; clinical data; clinical information;\nextraction; BARD; Gemini; LLaMA-2; heuristic; prompt; prompts; ensemble\nIntroduction\nClinical information extraction (IE) is the task of identifying\nand extracting relevant information from clinical narratives,\nsuch as clinical notes, radiology reports, or pathology reports.\nClinical IE has many applications in health care, such as\nimproving diagnosis, treatment, and decision-making;\nfacilitating clinical research; and enhancing patient care [1,2].\nHowever, clinical IE faces several challenges, such as the\nscarcity and heterogeneity of annotated data, the complexity\nand variability of clinical language, and the need for domain\nknowledge and expertise.\nZero-shot IE is a promising paradigm that aims to overcome\nthese challenges by leveraging large pretrained language models\n(LMs) that can perform IE tasks without any task-specific\ntraining data [3]. In-context learning is a framework for\nzero-shot and few-shot learning, where a large pretrained LM\ntakes a context and directly decodes the output without any\nretraining or fine-tuning [4]. In-context learning relies on prompt\nengineering, which is the process of crafting informative and\ncontextually relevant instructions or queries as inputs to LMs\nto guide their output for specific tasks [5]. The use of prompt\nengineering lies in its ability to leverage the powerful\ncapabilities of large LMs (LLMs), such as GPT-3.5 (OpenAI)\n[6], Gemini (Google) [7], LLaMA-2 (Meta) [8], even in\nscenarios where limited or no task-specific training data are\navailable. In clinical natural language processing (NLP), where\nlabeled data sets tend to be scarce, expensive, and\ntime-consuming to create, splintered across institutions, and\nconstrained by data use agreements, prompt engineering\nbecomes even more crucial to unlock the potential of\nstate-of-the-art LLMs for clinical NLP tasks.\nWhile prompt engineering has been widely explored for general\nNLP tasks, its application and impact in clinical NLP remain\nrelatively unexplored. Most of the existing literature on prompt\nengineering in the health care domain focuses on biomedical\nNLP tasks rather than clinical NLP tasks that involve processing\nreal-world clinical notes. For instance, Chen et al [9] used a\nfixed template as the prompt to measure the performance of\nLLMs on biomedical NLP tasks but did not investigate different\nkinds of prompting methods. Wang et al [10] gave a\ncomprehensive survey of prompt engineering for health care\nNLP applications such as question-answering systems, text\nsummarization, and machine translation. However, they did not\ncompare and evaluate different types of prompts for specific\nclinical NLP tasks and how the performance varies across\ndifferent LLMs. There is a lack of systematic and comprehensive\nstudies on how to engineer prompts for clinical NLP tasks, and\nthe existing literature predominantly focuses on general NLP\nproblems. This creates a notable gap in the research, warranting\na dedicated investigation into the design and development of\neffective prompts specifically for clinical NLP. Currently,\nresearchers in the field lack a comprehensive understanding of\nthe types of prompts that exist, their relative effectiveness, and\nthe challenges associated with their implementation in clinical\nsettings.\nThe main research question and objectives of this study are to\ninvestigate how to engineer prompts for clinical NLP tasks,\nidentify best practices, and address the challenges in this\nemerging field. By doing so, we aim to propose a guideline for\nfuture prompt-based clinical NLP studies. In this work, we\npresent a comprehensive empirical evaluation study on prompt\nengineering for 5 diverse clinical NLP tasks, namely, clinical\nsense disambiguation, biomedical evidence extraction,\ncoreference resolution, medication status extraction, and\nmedication attribute extraction [11,12]. By systematically\nevaluating different types of prompts proposed in recent\nliterature, including prefix [13], cloze [14], chain of thought\n[15], and anticipatory prompts [16], we gain insights into their\nperformance and suitability for each task. Two new types of\nprompting approaches were also introduced: (1) heuristic\nprompts and (2) ensemble prompts. The rationale behind these\nnovel prompts is to leverage the existing knowledge and\nexpertise in rule-based NLP, which has been prominent and has\nshown significant results in the clinical domain [17]. We\nhypothesize that heuristic prompts, which are based on rules\nderived from domain knowledge and linguistic patterns, can\ncapture the salient features and constraints of the clinical IE\ntasks. We also conjecture that ensemble prompts, which are\ncomposed of multiple types of prompts, can benefit from the\ncomplementary strengths and mitigate the weaknesses of each\nindividual prompt.\nOne of the key aspects of prompt engineering is the number of\nexamples or shots that are provided to the model along with the\nprompt. Few-shot prompting is a technique that provides the\nmodel with a few examples of input-output pairs, while\nzero-shot prompting does not provide any examples [3,18]. By\ncontrasting these strategies, we aim to shed light on the most\nefficient and effective ways to leverage prompt engineering in\nclinical NLP. Finally, we propose a prompt engineering\nframework to build and deploy zero-shot NLP models for the\nclinical domain. This study covers 3 state-of-the-art LMs,\nincluding GPT-3.5, Gemini, and LLaMA-2, to assess the\ngeneralizability of the findings across various models. This\nwork yields novel insights and guidelines for prompt engineering\nspecifically for clinical NLP tasks.\nMethods\nTasks\nWe selected 5 distinct clinical NLP tasks representing diverse\ncategories of natural language understanding: clinical sense\ndisambiguation (text classification) [19], biomedical evidence\nextraction (named entity recognition) [20], coreference\nresolution [21], medication status extraction (named entity\nrecognition+classification) [22], and medication attribute\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 2https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nextraction (named entity recognition+relation extraction) [23].\nTable 1 provides a succinct overview of each task, an example\nscenario, and the corresponding prompt type used for each task.\nTable 1. Task descriptions.\nExample promptDescriptionNLPa task categoryTask\nWhat is the meaning of the abbreviation\nCRb in the context of cardiology?\nThis task involves identifying the correct\nmeaning of clinical abbreviations within a\ngiven context.\nText classificationClinical sense disambigua-\ntion\nIdentify the psychological interventions in\nthe given text?\nIn this task, interventions are extracted from\nbiomedical abstracts.\nText extractionBiomedical evidence extrac-\ntion\nIdentify the antecedent for the patient in the\nclinical note.\nThe goal here is to identify all mentions in\nclinical text that refer to the same entity.\nCoreference resolutionCoreference resolution\nWhat is the current status of [24] in the\ntreatment of [25]?\nThis task involves identifying whether a\nmedication is currently being taken, not\ntaken, or unknown.\nNERc+classificationMedication status extraction\nWhat is the recommended dosage of [26]\nfor [27] and how often?\nThe objective here is to identify specific\nattributes of a medication, such as dosage\nand frequency.\nNER+REdMedication attribute extrac-\ntion\naNLP: natural language processing.\nbCR: cardiac resuscitation.\ncNER: named entity recognition.\ndRE: relation extraction.\nData Sets and Evaluation\nThe prompts were evaluated on 3 LLMs, GPT-3.5, Gemini, and\nLLaMA-2, under both zero-shot and few-shot prompting\nconditions, using precise experimental settings and parameters.\nTo simplify the evaluation process and facilitate clear\ncomparisons, we adopted accuracy as the sole evaluation metric\nfor all tasks. Accuracy is defined as the proportion of correct\noutputs generated by the LLM for each task, using a resolver\nthat maps the output to the label space. Table 2 shows the data\nsets and sample size for each clinical NLP task. The data sets\nare as follows:\n• Clinical abbreviation sense inventories: This is a data set\nof clinical abbreviations, senses, and instances [28]. It\ncontains 41 acronyms from 18,164 notes, along with their\nexpanded forms and contexts. We used a randomly sampled\nsubset from this data set for clinical sense disambiguation,\ncoreference resolution, medication status extraction, and\nmedication attribute extraction tasks (Table 2).\n• Evidence-based medicine-NLP: This is a data set of\nevidence-based medicine annotations for NLP [29]. It\ncontains 187 abstracts and 20 annotated abstracts, with\ninterventions extracted from the text. We used this data set\nfor the biomedical evidence extraction task.\nTable 2. Evaluation data sets and samples for different tasks.\nSamplesData set exampleData setTask\n11 acronyms from 55 notesThe abbreviation “CRb” can refer to “car-\ndiac resuscitation” or “computed radiogra-\nphy.”\nCASIaClinical sense disambigua-\ntion\n187 abstracts and 20 annotated abstractsIdentifying panic, avoidance, and agorapho-\nbia (psychological interventions)\nEBMc-NLPdBiomedical evidence extrac-\ntion\n105 annotated examplesResolving references to “the patient” or “the\nstudy” within a clinical trial report.\nCASICoreference resolution\n105 annotated examples with 340 medica-\ntion status pairs\nIdentifying that a patient is currently taking\ninsulin for diabetes.\nCASIMedication status extraction\n105 annotated examples with 313 medica-\ntions and 533 attributes\nIdentifying dosage, frequency, and route of\na medication for a patient.\nCASIMedication attribute extrac-\ntion\naCASI: clinical abbreviation sense inventories.\nbCR: cardiac resuscitation.\ncEBM: evidence-based medicine.\ndNLP: natural language processing.\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 3https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nAll experiments were carried out in different system settings.\nAll GPT-3.5 experiments were conducted using the GPT-3.5\nTurbo application programming interface as of the September\n2023 update. The LLaMA-2 model was directly accessed for\nour experiments. Gemini was accessed using the Gemini\napplication (previously BARD)—Google’s generative artificial\nintelligence conversational system. These varied system settings\nand access methods were taken into account to ensure the\nreliability and validity of our experimental results, given the\ndiffering architectures and capabilities of each LLM.\nIn evaluating the prompt-based approaches on GPT-3.5, Gemini,\nand LLaMA-2, we have also incorporated traditional NLP\nbaselines to provide a comprehensive understanding of the\nLLMs’ performance in a broader context. These baselines\ninclude well-established models such as Bidirectional Encoder\nRepresentations From Transformers (BERT) [30], Embeddings\nFrom Language Models (ELMO) [31], and\nPubMedBERT-Conditional Random Field (PubMedBERT-CRF)\n[32], which have previously set the standard in clinical NLP\ntasks. By comparing the outputs of LLMs against these\nbaselines, we aim to offer a clear perspective on the\nadvancements LLMs represent in the field. This comparative\nanalysis is crucial for appreciating the extent to which prompt\nengineering techniques can leverage the inherent capabilities\nof LLMs, marking a significant evolution from traditional\napproaches to more dynamic and contextually aware\nmethodologies in clinical NLP.\nPrompt Creation Process\nA rigorous process was followed to create suitable prompts for\neach task. These prompts were carefully crafted to match the\nspecific context and objectives of each task. There is no\nestablished method for prompt design and selection as of now.\nTherefore, we adopted an iterative approach where prompts,\nwhich are created by health care experts, go through a\nverification and improvement process in an iterative cycle,\nwhich involved design, experimentation, and evaluation, as\ndepicted in Figure 1.\nFigure 1. Iterative prompt design process: a schematic diagram of the iterative prompt creation process for clinical NLP tasks. The process consists of\n3 steps: sampling, prompt designing, and deployment. The sampling step involves defining the task and collecting data and annotations. The prompt\ndesigning step involves creating and refining prompts using different types and language models. The deployment step involves selecting the best model\nand deploying the model for clinical use. LLM: large language model; NER: named entity recognition; NLP: natural language processing; RE: relation\nextraction.\nFigure 1 illustrates the 3 main steps of our prompt creation\nprocess: sampling, prompt designing, and deployment. In the\nsampling step (step 1), we defined the clinical NLP task (eg,\nnamed entity recognition, relation extraction, and text\nclassification) and collected a sample of data and annotations\nas an evaluation for the task. In the prompt designing step (step\n2), a prompt was designed for the task using one of the prompt\ntypes (eg, simple prefix prompt, simple cloze prompt, heuristic\nprompt, chain of thought prompt, question prompt, and\nanticipatory prompt). We also optionally performed few-shot\nprompting by providing some examples along with the prompt.\nThe LLMs and the evaluation metrics for the experiment setup\nwere then configured. We ran experiments with various prompt\ntypes and LLMs and evaluated their performance on the task.\nBased on the results, we refined or modified the prompt design\nuntil we achieved satisfactory performance or reached a limit.\nIn the deployment step (step 3), the best prompt-based models\nwere selected based on their performance metrics, and the model\nwas deployed for the corresponding task.\nPrompt Engineering Techniques\nOverview\nPrompt engineering is the process of designing and creating\nprompts that elicit desired responses from LLMs. Prompts can\nbe categorized into different types based on their structure,\nfunction, and complexity.\nEach prompt consists of a natural language query that is\ndesigned to elicit a specific response from the pretrained LLM.\nThe prompts are categorized into 7 types, as illustrated in Figure\n2 (all prompts have been included in Multimedia Appendix 1).\nPrefix prompts are the simplest type of prompts, which prepend\na word or phrase indicating the type or format or tone of\nresponse for control and relevance. Cloze prompts are based on\nthe idea of fill in the blank exercises, which create a masked\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 4https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\ntoken in the input text and ask the LLM to predict the missing\nword or phrase [3]. Anticipatory prompts are the prompts\nanticipating the next question or command based on experience\nor knowledge, guiding the conversation. Chain of thought\nprompting involves a series of intermediate natural language\nreasoning steps that lead to the final output [15].\nIn addition to the existing types of prompts, 2 new novel\nprompts were also designed: heuristic prompts and ensemble\nprompts, which will be discussed in the following sections.\nFigure 2. Types of prompts: examples of 7 types of prompts that we used to query the pretrained language model for different clinical information\nextraction tasks. [X]: context; [Y]: abbreviation; [Z]: expanded form.\nHeuristic Prompts\nHeuristic prompts are rule-based prompts that decompose\ncomplex queries into smaller, more manageable components\nfor comprehensive answers. Adopting the principles of\ntraditional rule-based NLP, which relies on manually crafted,\nrule-based algorithms for specific clinical NLP applications,\nwe have integrated these concepts into our heuristic prompts\napproach. These prompts use a set of predefined rules to guide\nthe LLM in expanding abbreviations within a given context.\nFor instance, a heuristic prompt might use the rule that an\nabbreviation is typically capitalized, followed by a period, and\npreceded by an article or a noun. This approach contrasts with\nchain of thought prompts, which focus on elucidating the\nreasoning or logic behind an output. Instead, heuristic prompts\nleverage a series of predefined rules to direct the LLM in\nexecuting a specific task.\nMathematically, we can express a heuristic prompt as H(x), a\nfunction applied to an input sequence x. This function is defined\nas a series of rule-based transformations Ti, where i indicates\nthe specific rule applied. The output of this function, denoted\nas yH, is then:\nyH=H(x)=Tn(T{n–1}(... T1(x)))\nHere, each transformation Ti applies a specific heuristic rule to\nmodify the input sequence, making it more suitable for\nprocessing by LLMs.\nFrom an algorithmic standpoint, heuristic prompts are\nimplemented by defining a set of rules R={R1, R2, ..., Rm}. Each\nrule Rj is a function that applies a specific heuristic criterion to\nan input token or sequence of tokens. Algorithmically, the\nheuristic prompting process can be summarized as follows:\nBy merging the precision and specificity of traditional rule-based\nNLP methods with the advanced capabilities of LLMs, the\nheuristic prompts offer a robust and accurate system for clinical\ninformation processing and analysis.\nEnsemble Prompts\nEnsemble prompts are prompts that combine multiple prompts\nusing majority voting for aggregated outputs. They use various\ntypes of prompts to generate multiple responses to the same\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 5https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\ninput, subsequently selecting the most commonly occurring\noutput as the final answer. For instance, an ensemble prompt\nmight use 3 different prefix prompts, or a combination of other\nprompt types, to produce 3 potential expansions for an\nabbreviation. The most frequently appearing expansion is then\nchosen. For the sake of simplicity, we amalgamated the outputs\nfrom all 5 different prompt types using a majority voting\napproach.\nMathematically, consider a set of m different prompting methods\nP1, P2, ..., Pm applied to the same input x. Each method generates\nan output yi for i=1,2, ..., m. The ensemble prompt’s output yE\nis then the mode of these outputs:\nyE=mode (y1, y2, ..., ym)\nAlgorithmically, the ensemble prompting process is as follows:\nThe rationale behind an ensemble prompt is that by integrating\nmultiple types of prompts, we can use the strengths and\ncounterbalance the weaknesses of each individual prompt,\noffering a robust and potentially more accurate response. Some\nprompts may be more effective for specific tasks or models,\nwhile others might be more resilient to noise or ambiguity.\nMajority voting allows us to choose the most likely correct or\ncoherent output from the variety generated by different prompt\ntypes.\nResults\nOverview\nIn this section, we present the results of our experiments on\nprompt engineering for zero-shot clinical IE. Various prompt\ntypes were evaluated across 5 clinical NLP tasks, aiming to\nunderstand how different prompts influence the accuracy of\ndifferent LLMs. Zero-shot and few-shot prompting strategies\nwere also compared, exploring how the addition of context\naffects the model performance. Furthermore, we tested an\nensemble approach that combines the outputs of different prompt\ntypes using majority voting. Finally, the impact of different\nLLMs on task performance was analyzed, and some interesting\npatterns were observed. Table 3 illustrates that different prompt\ntypes have different levels of effectiveness for different tasks\nand LLMs. We can also observe some general trends across the\ntasks and models.\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 6https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 3. Performance comparison of different prompt types and language models.\nFew shotEnsembleChain of\nthought\nHeuristicAnticipatorySimple clozeSimple pre-\nfix\nTask and language model\nClinical sense disambiguation\n0.820.90.90.96a0.880.860.88GPT-3.5\n0.670.710.720.750.710.680.76bGemini\n0.780.820.780.820.820.760.88bLLaMA-2\n0.420.420.420.420.420.420.42BERTc (from [33])\n0.550.550.550.550.550.550.55ELMOd (from [33])\nBiomedical evidence extraction\n0.96a0.880.940.940.880.820.92GPT-3.5\n0.880.90.91b0.90.91b0.890.89Gemini\n0.860.880.870.88b0.870.88b0.85LLaMA-2\n0.350.350.350.350.350.350.35PubMedBERT-CRFe (from [29])\nCoreference resolution\n0.740.740.94a0.94a0.740.60.78GPT-3.5\n0.70.690.710.670.730.81b0.69Gemini\n0.680.780.8b0.760.740.640.8bLLaMA-2\n0.690.690.690.690.690.690.69Toshniwal et al [34]\nMedication status extraction\n0.720.750.730.740.750.720.76aGPT-3.5\n0.550.580.590.550.650.510.67bGemini\n0.420.580.520.64b0.520.480.58LLaMA-2\n0.520.520.520.520.520.520.52ScispaCy [35]\nMedication attribute extraction\n0.96a0.90.96a0.96a0.90.840.88GPT-3.5\n0.88b0.760.740.70.88c0.720.68Gemini\n0.60.640.72b0.660.580.660.6LLaMA-2\n0.700.700.700.700.700.700.70ScispaCy\naBest performance on a task regardless of the model (ie, for each GPT-3.5 or Gemini or LLaMA-2 triple).\nbBest performance for each model on a task.\ncBERT: Bidirectional Encoder Representations From Transformers.\ndELMO: Embeddings From Language Models.\nePubMedBERT-CRF: PubMedBERT-Conditional Random Field.\nPrompt Optimization and Evaluation\nFor clinical sense disambiguation, the heuristic and prefix\nprompts consistently achieved the highest performance across\nall LLMs, significantly outperforming baselines such as BERT\n[30] and ELMO, with GPT-3.5 achieving an accuracy of 0.96,\nshowcasing its advanced understanding of clinical context using\nappropriate prompting strategies. For biomedical evidence\nextraction, the heuristic and chain of thought prompts excelled\nacross all LLMs in zero-shot setting. This indicates that these\nprompt types were able to provide enough information and\nconstraints for the model to extract the evidence from the clinical\nnote. GPT-3.5 achieved an accuracy of 0.94 with these prompt\ntypes, which was higher than any other model or prompt type\ncombination. For coreference resolution, the chain of thought\nprompt type performed best among all prompt types with 2\nLLMs—GPT-3.5 and LLaMA-2. This indicates that this prompt\ntype was able to provide enough structure and logic for the\nmodel to resolve the coreference in the clinical note. GPT-3.5\ndisplayed high accuracy with this prompt type, achieving an\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 7https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\naccuracy of 0.94. For medication status extraction, simple prefix\nand heuristic prompts yielded good results across all LLMs.\nThese prompt types were able to provide enough introduction\nor rules for the model to extract the status of the medication in\nrelation to the patient or condition. GPT-3.5 excelled with these\nprompt types, achieving an accuracy of 0.76 and 0.74,\nrespectively. For medication attribute extraction, we found that\nthe chain of thought and heuristic prompts were effective across\nall LLMs. These prompt types were able to provide enough\nreasoning or rules for the model to extract and label the attributes\nof medications from clinical notes. Anticipatory prompts,\nhowever, had the best accuracy for Gemini among all the\nprompts. GPT-3.5 achieved an accuracy of 0.96 with these\nprompt types, which was higher than any other model or prompt\ntype combination.\nThus, we can see that task-specific prompt tailoring is crucial\nfor achieving high accuracy. Different tasks require different\nlevels of information and constraints to guide the LLM to\nproduce the desired output. The experiments show that heuristic,\nprefix, and chain of thought prompts are generally very effective\nfor guiding the LLM to produce clear and unambiguous outputs.\nAs shown in Figure 3, it is clear that GPT-3.5 is a superior and\nversatile LLM that can handle various clinical NLP tasks in\nzero-shot settings, outperforming other models in most cases.\nFigure 3. Graphical comparison of prompt types in the 5 clinical natural language processing tasks used in this study.\nOverall, the prompt-based approach has demonstrated\nremarkable superiority over traditional baseline models across\nall the 5 tasks. For clinical sense disambiguation, GPT-3.5’s\nheuristic prompts achieved a remarkable accuracy of 0.96,\nshowcasing a notable improvement over baselines such as BERT\n(0.42) and ELMO (0.55). In biomedical evidence extraction,\nGPT-3.5 again set a high standard with an accuracy of 0.94\nusing heuristic prompts, far surpassing the baseline performance\nof PubMedBERT-CRF at 0.35. Coreference resolution saw\nGPT-3.5 reaching an accuracy of 0.94 with chain of thought\nprompts, eclipsing the performance of existing methods such\nas Toshniwal et al [34] (0.69). In medication status extraction,\nGPT-3.5 outperformed the baseline ScispaCy (0.52) with an\naccuracy of 0.76 using simple prefix prompts. Finally, for\nmedication attribute extraction, GPT-3.5’s heuristic prompts\nachieved an impressive accuracy of 0.96, significantly higher\nthan the ScispaCy baseline (0.70). These figures not only\nshowcase the potential of LLMs in clinical settings but also set\na foundation for future research to build upon, exploring even\nmore sophisticated prompt engineering strategies and their\nimplications for health care informatics.\nZero-Shot Versus Few-Shot Prompting\nThe performance of zero-shot prompting and few-shot\nprompting strategies was compared for each clinical NLP task.\nThe same prompt types and LLMs were used as in the previous\nexperiments, but some context was added to the input in the\nform of examples or explanations. Two examples or\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 8https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nexplanations were used for each task (2-shot) depending on the\ncomplexity and variability of the task. Table 3 shows that\nfew-shot prompting consistently improved the accuracy of all\ncombinations for all tasks except for clinical sense\ndisambiguation and medication attribute extraction, where some\nzero-shot prompt types performed better. We also observed\nsome general trends across the tasks and models.\nWe found that few-shot prompting enhanced accuracy by\nproviding limited context that aided complex scenario\nunderstanding. The improvement was more pronounced\ncompared to simple cloze prompts, which had lower accuracy\nin most of the tasks. We also found that some zero-shot prompt\ntypes were very effective for certain tasks, even outperforming\nfew-shot prompting. These prompt types used a rule-based or\nreasoning approach to generate sentences that contained\ndefinitions or examples of the target words or concepts, which\nhelped the LLM to understand and match the context. For\nexample, heuristic prompts achieved higher accuracy than\nfew-shot prompting for clinical sense disambiguation and\nmedication attribute extraction, while chain of thought prompts\nachieved higher accuracy than few-shot prompting for\ncoreference resolution and medication attribute extraction.\nAlternatively, the clinical evidence extraction task likely benefits\nfrom additional context provided by few-shot examples, which\ncan guide the model more effectively than the broader inferences\nmade in zero-shot scenarios. This suggests that tasks requiring\ndeeper contextual understanding might be better suited to\nfew-shot learning approaches.\nFrom these results, we can infer that LLMs can be effectively\nused for clinical NLP in a no-data scenario, where we do not\nhave many publicly available data sets, by using appropriate\nzero-shot prompt types that guide the LLM to produce clear\nand unambiguous outputs. However, few-shot prompting can\nalso improve the performance of LLMs by providing some\ncontext that helps the LLM to handle complex scenarios.\nOther Observations\nEnsemble Approaches\nWe experimented with an ensemble approach by combining\noutputs from multiple prompts using majority voting. The\nensemble approach was not the best-performing strategy for\nany of the tasks, but it was better than the low-performing\nprompts. The ensemble approach was able to benefit from the\ndiversity and complementarity of different prompt types and\navoid some of the pitfalls of individual prompts. For example,\nfor clinical sense disambiguation, the ensemble approach\nachieved an accuracy of 0.9 with GPT-3.5, which was the second\nbest–performing prompt type. Similarly, for medication attribute\nextraction, the ensemble approach achieved an accuracy of 0.9\nwith GPT-3.5 and 0.76 with Gemini, which were close to the\nbest single prompt type (anticipatory). However, the ensemble\napproach also had some drawbacks, such as inconsistency and\nnoise. For tasks that required more specific or consistent outputs,\nsuch as coreference resolution, the ensemble approach did not\nimprove the accuracy over the best single prompt type and\nsometimes even decreased it. This suggests that the ensemble\napproach may introduce ambiguity for tasks that require more\nprecise or coherent outputs.\nWhile the ensemble approach aims to reduce the variance\nintroduced by individual prompt idiosyncrasies, our specific\nimplementation observed instances where the combination of\ndiverse prompt types introduced additional complexity. This\ncomplexity occasionally manifested as inconsistency and noise\nin the outputs contrary to our objective of achieving higher\nperformance. Future iterations of this approach may include\nrefinement of the prompt selection process to enhance\nconsistency and further reduce noise in the aggregated outputs.\nImpact of LLMs\nVariations in performance were observed among different LLMs\n(Table 3). We found that GPT-3.5 generally outperformed\nGemini and LLaMA-2 on most tasks. This suggests that GPT-3.5\nhas a better generalization ability and can handle a variety of\nclinical NLP tasks with different prompt types. However,\nGemini and LLaMA-2 also showed some advantages over\nGPT-3.5 on certain tasks and prompt types. For example,\nGemini achieved the highest accuracy of 0.81 with simple cloze\nprompts and LLaMA-2 achieved the highest accuracy of 0.8\nwith simple prefix prompts for coreference resolution. This\nindicates that Gemini and LLaMA-2 may have some\ndomain-specific knowledge that can benefit certain clinical NLP\ntasks for specific prompt types.\nPersona Patterns\nPersona patterns are a way of asking the LLM to act like a\npersona or a system that is relevant to the task or domain. For\nexample, one can ask the LLM to “act as a clinical NLP expert.”\nThis can help the LLM to generate outputs that are more\nappropriate and consistent with the persona or system. For\nexample, one can use the following prompt for clinical sense\ndisambiguation:\nAct as a clinical NLP expert. Disambiguate the word\n“cold” in the following sentence: “She had a cold\nfor three days.”\nWe experimented with persona patterns for different tasks and\nLLMs and found that they can improve the accuracy and quality\nof the outputs. Persona patterns can help the LLM to focus on\nthe relevant information and constraints for the task and avoid\ngenerating outputs that are irrelevant or contradictory to the\npersona or system.\nRandomness in Output\nMost LLMs do not produce the output in the same format every\ntime. There is inherent randomness in the outputs the LLMs\nproduce. Hence, the prompts need to be specific in the way they\nare done for the task. Prompts are powerful when they are\nspecific and if we use them in the right way.\nRandomness in output can be beneficial or detrimental for\ndifferent tasks and scenarios. In the clinical domain, randomness\ncan introduce noise and errors in the outputs, which can make\nthem less accurate and reliable for the users. For example, for\ntasks that involve extracting factual information, such as\nbiomedical evidence extraction and medication status extraction,\nrandomness can cause the LM to produce outputs that are\ninconsistent or contradictory with the input or context.\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 9https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nGuidelines and Suggestions for Optimal Prompt\nSelection\nIn recognizing the evolving nature of clinical NLP, we expand\nour discussion to contemplate the adaptability of our\nrecommended prompt types and LM combinations across a\nwider spectrum of clinical tasks and narratives. This speculative\nanalysis aims to bridge the gap between our current findings\nand their applicability to unexplored clinical NLP challenges,\nsetting a foundation for future research to validate and refine\nthese recommendations. In this section, we synthesize the main\nfindings from our experiments and offer some practical advice\nfor prompt engineering for zero-shot and few-shot clinical IE.\nWe propose the following steps for selecting optimal prompts\nfor different tasks and scenarios:\nThe first step is to identify the type of clinical NLP task, which\ncan be broadly categorized into three types: (1) classification,\n(2) extraction, and (3) resolution. Classification tasks involve\nassigning a label or category to a word, phrase, or sentence in\na clinical note, such as clinical sense disambiguation or\nmedication status extraction. Extraction tasks involve identifying\nand extracting relevant information from a clinical note, such\nas biomedical evidence extraction or medication attribute\nextraction. Resolution tasks involve linking or matching entities\nor concepts in a clinical note, such as coreference resolution.\nThe second step is to choose the prompt type that is most\nsuitable for the task type. We found that different prompt types\nhave different strengths and weaknesses for different task types,\ndepending on the level of information and constraints they\nprovide to the LLM. Table 4 summarizes our findings and\nrecommendations for optimal prompt selection for each task\ntype.\nThe third step is to choose the LLM that is most compatible\nwith the chosen prompt type. We found that different LLMs\nhave different capabilities and limitations for different prompt\ntypes, depending on their generalization ability and\ndomain-specific knowledge. Table 5 summarizes our findings\nand recommendations for optimal LLM selection for each\nprompt type.\nThe fourth step is to evaluate the performance of the chosen\nprompt type and LLM combination on the clinical NLP task\nusing appropriate metrics such as accuracy, precision, recall,\nor F1-score. If the performance is satisfactory, then the prompt\nengineering process is complete. If not, then the process can be\nrepeated by choosing a different prompt type or LLM or by\nmodifying the existing prompt to improve its effectiveness.\nTable 4. Optimal prompt types for different clinical natural language processing task types.\nPrompt typeTask type\nHeuristic or prefixClassification\nHeuristic or chain of thoughtExtraction\nChain of thoughtResolution\nTable 5. Optimal language models for different prompt types.\nLanguage modelPrompt type\nGPT-3.5Heuristic\nGPT-3.5 or LLaMA-2Prefix\nGemini or LLaMA-2Cloze\nGPT-3.5Chain of thought\nGeminiAnticipatory\nDiscussion\nPrincipal Findings\nIn this paper, we have presented a novel approach to zero-shot\nand few-shot clinical IE using prompt engineering. Various\nprompt types were evaluated across 5 clinical NLP tasks: clinical\nsense disambiguation, biomedical evidence extraction,\ncoreference resolution, medication status extraction, and\nmedication attribute extraction. The performance of different\nLLMs, GPT-3.5, Gemini, and LLaMA-2, was also compared.\nOur main findings are as follows:\n1. Task-specific prompt tailoring is crucial for achieving high\naccuracy. Different tasks require different levels of\ninformation and constraints to guide the LLM to produce\nthe desired output. Therefore, it is important to design\nprompts that are relevant and specific to the task at hand\nand avoid using generic or vague prompts that may confuse\nthe model or lead to erroneous outputs.\n2. Heuristic prompts are generally very effective for guiding\nthe LLM to produce clear and unambiguous outputs. These\nprompts use a rule-based approach to generate sentences\nthat contain definitions or examples of the target words or\nconcepts, which help the model to understand and match\nthe context. Heuristic prompts are especially useful for tasks\nthat involve disambiguation, extraction, or classification of\nentities or relations.\n3. Chain of thought prompts are also effective for guiding the\nLLM to produce logical and coherent outputs. These\nprompts use a multistep approach to generate sentences that\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 10https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\ncontain a series of questions and answers that resolve the\ntask in the context. Chain of thought prompts are especially\nuseful for tasks that involve reasoning, inference, or\ncoreference resolution.\n4. Few-shot prompting can improve the performance of LLMs\nby providing some context that helps the model to handle\ncomplex scenarios. Few-shot prompting can be done by\nadding some examples or explanations to the input\ndepending on the complexity and variability of the task.\nFew-shot prompting can enhance accuracy by providing\nlimited context that aids complex scenario understanding.\nThe improvement is more pronounced compared to simple\nprefix and cloze prompts, which had lower accuracy in most\nof the tasks.\n5. Ensemble approaches can also improve the performance\nof LLMs by combining outputs from multiple prompts using\nmajority voting. Ensemble approaches can leverage the\nstrengths of each prompt type and reduce the errors of\nindividual prompts. Ensemble approaches are especially\neffective for tasks that require multiple types of information\nor reasoning, such as biomedical evidence extraction and\nmedication attribute extraction.\nIt is noteworthy that context size has a significant impact on\nthe performance of LLMs in zero-shot IE [36]. In the scope of\nthis study, we have avoided the context size dependence on\nperformance, as it is a complex issue that requires careful\nconsideration.\nThis study serves as an initial exploration into the efficacy of\nprompt engineering in clinical NLP, providing foundational\ninsights rather than exhaustive guidelines. Given the rapid\nadvancements in generative artificial intelligence and the\ncomplexity of clinical narratives, we advocate for continuous\nempirical testing of these prompt strategies across diverse\nclinical tasks and data sets. This approach will not only validate\nthe generalizability of our findings but also uncover new avenues\nfor enhancing the accuracy and applicability of LLMs in clinical\nsettings.\nLimitations\nIn this study, we primarily focused on exploring the capabilities\nand versatility of generative LLMs in the context of zero-shot\nand few-shot learning for clinical NLP tasks. Our approach also\nhas some limitations that we acknowledge in this work. First,\nit relies on the quality and availability of pretrained LLMs,\nwhich may vary depending on the domain and task. As LLMs\nare rapidly evolving, some parts of the prompt engineering\ndiscipline may be timeless, while some parts may evolve and\nadapt over time as different capabilities of models evolve.\nSecond, it requires a lot of experimentation and iteration to\noptimize prompts for different applications, which may be\niterative and time-consuming. However, once optimal prompts\nare identified, the approach offers time savings in subsequent\napplications by reusing these prompts or making minor\nadjustments for similar tasks. We may not have explored all the\npossible combinations and variations of prompts that could\npotentially improve the performance of the clinical NLP tasks.\nThird, the LLMs do not release the details of the data set that\nthey were trained on. Hence, the high accuracy could be because\nthe models would have already seen the data during training\nand not because of the effectiveness of the prompts.\nFuture Work\nWe plan to address these challenges and limitations in our future\nwork. We aim to develop more systematic and automated\nmethods for prompt design and evaluation, such as using\nprompt-tuning or meta-learning techniques. We also aim to\nincorporate more domain knowledge or external resources into\nthe prompts or the LLMs, such as using ontologies, knowledge\ngraphs, or databases. We also aim to incorporate more quality\ncontrol or error correction mechanisms into the prompts or the\nLLMs, such as using adversarial examples, confidence scores,\nor human feedback.\nConclusions\nIn this paper, we have benchmarked different prompt\nengineering techniques for both zero-shot and few-shot clinical\nNLP tasks. Two new types of prompts, heuristic and ensemble\nprompts, were also conceptualized and proposed. We have\ndemonstrated that prompt engineering can enable the use of\npretrained LMs for various clinical NLP tasks without requiring\nany fine-tuning or additional data. We have shown that\ntask-specific prompt tailoring, heuristic prompts, chain of\nthought prompts, few-shot prompting, and ensemble approaches\ncan improve the accuracy and quality of the outputs. We have\nalso shown that GPT-3.5 is very adaptable and precise across\nall tasks and prompt types, while Gemini and LLaMA-2 may\nhave some domain-specific advantages for certain tasks and\nprompt types.\nWe believe that a prompt-based approach has several benefits\nover existing methods for clinical IE. It reduces the cost and\ntime in the initial phases of clinical NLP application\ndevelopment, where prompt-based methods offer a streamlined\nalternative to the conventional data preparation and model\ntraining processes. It is flexible and adaptable, as it can be\napplied to various clinical NLP tasks with different prompt\ntypes and LLMs. It is interpretable and explainable, as it uses\nnatural language prompts that can be easily understood and\nmodified by humans.\nAcknowledgments\nThis work was supported by the National Institutes of Health (awards U24 TR004111 and R01 LM014306). The content is solely\nthe responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 11https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nAuthors' Contributions\nSS conceptualized, designed, and organized this study; analyzed the results; and wrote, reviewed, and revised the paper. MK and\nAS-M analyzed the results, and wrote, reviewed, and revised the paper. SV wrote, reviewed, and revised the paper. YW\nconceptualized, designed, and directed this study and wrote, reviewed, and revised the paper.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nPrompts for clinical natural language processing tasks.\n[DOCX File , 31 KB-Multimedia Appendix 1]\nReferences\n1. Wang Y, Wang L, Rastegar-Mojarad M, Moon S, Shen F, Afzal N, et al. Clinical information extraction applications: a\nliterature review. J Biomed Inform. 2018;77:34-49. [FREE Full text] [doi: 10.1016/j.jbi.2017.11.011] [Medline: 29162496]\n2. Landolsi MY, Hlaoua L, Romdhane LB. Information extraction from electronic medical documents: state of the art and\nfuture research directions. Knowl Inf Syst. 2023;65(2):463-516. [FREE Full text] [doi: 10.1007/s10115-022-01779-1]\n[Medline: 36405956]\n3. Sivarajkumar S, Wang Y. HealthPrompt: a zero-shot learning paradigm for clinical natural language processing. AMIA\nAnnu Symp Proc. 2022;2022:972-981. [FREE Full text] [Medline: 37128372]\n4. Min S, Lyu X, Holtzman A, Artetxe M, Lewis M, Hajishirzi H, et al. Rethinking the role of demonstrations: what makes\nin-context learning work? 2022. Presented at: Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing; December 7-11, 2022;11048-11064; Abu Dhabi, United Arab Emirates. URL: https://aclanthology.\norg/2022.emnlp-main.759/ [doi: 10.18653/v1/2022.emnlp-main.759]\n5. White J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H, et al. A prompt pattern catalog to enhance prompt engineering with\nchatGPT. ArXiv. Preprint posted online on February 21, 2023. [FREE Full text] [doi: 10.48550/arXiv.2302.11382]\n6. Ouyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, et al. Training language models to follow instructions\nwith human feedback. In: Koyejo S, Mohamed S, Agarwal A, Belgrave D, Cho K, Oh A, editors. Advances in Neural\nInformation Processing Systems 35 (NeurIPS 2022). Red Hook, NY. Curran Associates, Inc; 2022;27730-27744.\n7. Gemini Team Google. Gemini: a family of highly capable multimodal models. ArXiv. Preprint posted online on December\n19, 2023. [FREE Full text] [doi: 10.48550/arXiv.2312.11805]\n8. Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, et al. Llama 2: open foundation and fine-tuned chat\nmodels. ArXiv. Preprint posted online on July 28, 2023. [FREE Full text] [doi: 10.48550/arXiv.2307.09288]\n9. Chen Q, Du J, Hu Y, Keloth VK, Peng X, Raja K, et al. Large language models in biomedical natural language processing:\nbenchmarks, baselines, and recommendations. ArXiv. Preprint posted online on May 10, 2023. [FREE Full text] [doi:\n10.48550/arXiv.2305.16326]\n10. Wang J, Shi E, Yu S, Wu Z, Ma C, Dai H, et al. Prompt engineering for healthcare: methodologies and applications. ArXiv.\nPreprint posted online on April 28, 2023. [FREE Full text] [doi: 10.48550/arXiv.2304.14670]\n11. Hu Y, Chen Q, Du J, Peng X, Keloth VK, Zuo X, et al. Improving large language models for clinical named entity recognition\nvia prompt engineering. ArXiv. Preprint posted online on March 29, 2023. [FREE Full text] [doi: 10.48550/arXiv.2303.16416]\n12. Yuan C, Xie Q, Ananiadou S. Zero-shot temporal relation extraction with chatGPT. 2023. Presented at: The 22nd Workshop\non Biomedical Natural Language Processing and BioNLP Shared Tasks; July 13, 2023;92-102; Toronto, Canada. URL:\nhttps://aclanthology.org/2023.bionlp-1.7/ [doi: 10.18653/v1/2023.bionlp-1.7]\n13. Li X, Liang L. Prefix-tuning: optimizing continuous prompts for generation. 2021. Presented at: Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers); August 1-6, 2021;4582-4597; Virtual Event. URL: https://aclanthology.\norg/2021.acl-long.353/ [doi: 10.18653/v1/2021.acl-long.353]\n14. Liu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G. Pre-train, prompt, and predict: a systematic survey of prompting\nmethods in natural language processing. ACM Comput Surv. 2023;55(9):1-35. [FREE Full text] [doi: 10.1145/3560815]\n15. Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, et al. Chain-of-thought prompting elicits reasoning in large\nlanguage models. In: Koyejo S, Mohamed S, Agarwal A, Belgrave D, Cho K, Oh A, editors. Advances in Neural Information\nProcessing Systems 35 (NeurIPS 2022). Red Hook, NY. Curran Associates Inc; 2022;24824-24837.\n16. Hancock B, Bordes A, Mazare PE, Weston J. Learning from dialogue after deployment: feed yourself, chatbot!. 2019.\nPresented at: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics; July 28-August\n2, 2019;3667-3684; Florence, Italy. URL: https://aclanthology.org/P19-1358/ [doi: 10.18653/v1/p19-1358]\n17. Mykowiecka A, Marciniak M, Kupść A. Rule-based information extraction from patients' clinical data. J Biomed Inform.\n2009;42(5):923-936. [FREE Full text] [doi: 10.1016/j.jbi.2009.07.007] [Medline: 19646551]\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 12https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n18. Agrawal M, Hegselmann S, Lang H, Kim Y, Sontag D. Large language models are few-shot clinical information extractors.\nAssociation for Computational Linguistics; 2022. Presented at: The 2022 Conference on Empirical Methods in Natural\nLanguage Processing; December 7-11, 2022;1998-2022; Abu Dhabi, United Arab Emirates. URL: https://aclanthology.org/\n2022.emnlp-main.130.pdf [doi: 10.18653/v1/2022.emnlp-main.130]\n19. Wu Y, Xu J, Zhang Y, Xu H. Clinical abbreviation disambiguation using neural word embeddings. 2015. Presented at:\nProceedings of the 2015 Workshop on Biomedical Natural Language Processing (BioNLP 2015); July 30, 2015;171-176;\nBeijing, China. URL: https://aclanthology.org/W15-3822.pdf [doi: 10.18653/v1/w15-3822]\n20. Abdelkader W, Navarro T, Parrish R, Cotoi C, Germini F, Iorio A, et al. Machine learning approaches to retrieve high-quality,\nclinically relevant evidence from the biomedical literature: systematic review. JMIR Med Inform. 2021;9(9):e30401. [FREE\nFull text] [doi: 10.2196/30401] [Medline: 34499041]\n21. Uzuner O, Bodnari A, Shen S, Forbush T, Pestian J, South BR. Evaluating the state of the art in coreference resolution for\nelectronic medical records. J Am Med Inform Assoc. 2012;19(5):786-791. [FREE Full text] [doi:\n10.1136/amiajnl-2011-000784] [Medline: 22366294]\n22. Denny JC, Peterson JF, Choma NN, Xu H, Miller RA, Bastarache L, et al. Extracting timing and status descriptors for\ncolonoscopy testing from electronic medical records. J Am Med Inform Assoc. 2010;17(4):383-388. [FREE Full text] [doi:\n10.1136/jamia.2010.004804] [Medline: 20595304]\n23. Jagannatha A, Liu F, Liu W, Yu H. Overview of the first natural language processing challenge for extracting medication,\nindication, and adverse drug events from electronic health record notes (MADE 1.0). Drug Saf. 2019;42(1):99-111. [FREE\nFull text] [doi: 10.1007/s40264-018-0762-z] [Medline: 30649735]\n24. Chen Y, Wu X, Chen M, Song Q, Wei J, Li X, et al. Dynamic text categorization of search results for medical class\nrecognition in real world evidence studies in the Chinese language. Association for Computing Machinery, Presented at:\nProceedings of the International Conference on Bioinformatics and Computational Intelligence (ICBCI 2017); 2017;40-48;\nBeijing, China. [doi: 10.1145/3135954.3135962]\n25. Mallick PK, Balas VE, Bhoi AK, Zobaa AF. Cognitive Informatics and Soft Computing Proceeding of CISC 2017, Advances\nin Intelligent Systems and Computing (AISC, Volume 768). New York. Springer Verlag; 2019.\n26. Ananiadou S, Lee D, Xu H, Song M. DTMBIO'12—The Proceedings of the Sixth ACM International Workshop on Data\nand Text Mining in Biomedical Informatics. 2012. Presented at: 6th ACM International Workshop on Data and Text Mining\nin Biomedical Informatics, DTMBIO 2012, in Conjunction with the 21st ACM International Conference on Information\nand Knowledge Management, CIKM 2012; 2012; New York. URL: https://dl.acm.org/action/showFmPdf?doi=10.\n1145%2F2390068 [doi: 10.1145/2396761.2398758]\n27. Elghandour I, State R, Brorsson M, Le L, Antonopoulos N, Xie Y, et al. IEEE/ACM International Symposium on Big Data\nComputing (BDC). 2016. Presented at: 2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications\nand Technologies (BDCAT); December 6-9, 2016; Shanghai, China. URL: https://ieeexplore.ieee.org/xpl/conhome/7876287/\nproceeding [doi: 10.1109/bdcat.2018.00008]\n28. Moon S, Pakhomov S, Liu N, Ryan JO, Melton GB. A sense inventory for clinical abbreviations and acronyms created\nusing clinical notes and medical dictionary resources. J Am Med Inform Assoc. 2014;21(2):299-307. [FREE Full text] [doi:\n10.1136/amiajnl-2012-001506] [Medline: 23813539]\n29. Nye B, Li JJ, Patel R, Yang Y, Marshall I, Nenkova A, et al. A corpus with multi-level annotations of patients, interventions\nand outcomes to support language processing for medical literature. 2018. Presented at: Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers); July 15-20, 2018;197-207; Melbourne,\nAustralia. URL: https://aclanthology.org/P18-1019/ [doi: 10.18653/v1/p18-1019]\n30. Devlin J, Chang MW, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding.\nArXiv. Preprint posted online on May 24, 2019. [FREE Full text]\n31. Sarzynska-Wawer J, Wawer A, Pawlak A, Szymanowska J, Stefaniak I, Jarkiewicz M, et al. Detecting formal thought\ndisorder by deep contextualized word representations. Psychiatry Res. 2021;304:114135. [doi:\n10.1016/j.psychres.2021.114135] [Medline: 34343877]\n32. Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, et al. Domain-specific language model pretraining for biomedical\nnatural language processing. ACM Trans Comput Healthcare. 2021;3(1):1-23. [doi: 10.1145/3458754]\n33. Adams G, Ketenci M, Bhave S, Perotte A, Elhadad N. Zero-shot clinical acronym expansion via latent meaning cells. Proc\nMach Learn Res. 2020;136:12-40. [FREE Full text] [Medline: 34790898]\n34. Toshniwal S, Xia P, Wiseman S, Livescu K, Gimpel K. On generalization in coreference resolution. ArXiv. Preprint posted\nonline on September 20, 2021. [FREE Full text] [doi: 10.18653/v1/2021.crac-1.12]\n35. Neumann M, King D, Beltagy I, Ammar W. ScispaCy: fast and robust models for biomedical natural language processing.\nArXiv. Preprint posted online on October 9, 2019. [FREE Full text] [doi: 10.18653/v1/w19-5034]\n36. Sivarajkumar S, Wang Y. Evaluation of healthprompt for zero-shot clinical text classification. 2023. Presented at: 2023\nIEEE 11th International Conference on Healthcare Informatics (ICHI); June 26-29, 2023; Houston, TX, USA. [doi:\n10.1109/ichi57859.2023.00081]\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 13https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nAbbreviations\nBERT: Bidirectional Encoder Representations From Transformers\nELMO: Embeddings From Language Models\nIE: information extraction\nLLM: large language model\nLM: language model\nNLP: natural language processing\nPubMedBERT-CRF: PubMedBERT-Conditional Random Field\nEdited by C Lovis; submitted 08.12.23; peer-reviewed by J Zaghir, M Torii, J Zheng; comments to author 04.02.24; revised version\nreceived 20.02.24; accepted 24.02.24; published 08.04.24\nPlease cite as:\nSivarajkumar S, Kelley M, Samolyk-Mazzanti A, Visweswaran S, Wang Y\nAn Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing:\nAlgorithm Development and Validation Study\nJMIR Med Inform 2024;12:e55318\nURL: https://medinform.jmir.org/2024/1/e55318\ndoi: 10.2196/55318\nPMID: 38587879\n©Sonish Sivarajkumar, Mark Kelley, Alyssa Samolyk-Mazzanti, Shyam Visweswaran, Yanshan Wang. Originally published in\nJMIR Medical Informatics (https://medinform.jmir.org), 08.04.2024. This is an open-access article distributed under the terms\nof the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use,\ndistribution, and reproduction in any medium, provided the original work, first published in JMIR Medical Informatics, is properly\ncited. The complete bibliographic information, a link to the original publication on https://medinform.jmir.org/, as well as this\ncopyright and license information must be included.\nJMIR Med Inform 2024 | vol. 12 | e55318 | p. 14https://medinform.jmir.org/2024/1/e55318\n(page number not for citation purposes)\nSivarajkumar et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7273370027542114
    },
    {
      "name": "Natural language processing",
      "score": 0.6715071201324463
    },
    {
      "name": "Artificial intelligence",
      "score": 0.640483021736145
    },
    {
      "name": "Heuristic",
      "score": 0.5855208039283752
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5439666509628296
    },
    {
      "name": "Task (project management)",
      "score": 0.5258427262306213
    },
    {
      "name": "Relationship extraction",
      "score": 0.49232351779937744
    },
    {
      "name": "Machine learning",
      "score": 0.43550971150398254
    },
    {
      "name": "Information extraction",
      "score": 0.4055618643760681
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170201317",
      "name": "University of Pittsburgh",
      "country": "US"
    }
  ],
  "cited_by": 103
}