{
  "title": "Incorporating Residual and Normalization Layers into Analysis of Masked Language Models",
  "url": "https://openalex.org/W3200704197",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2034939723",
      "name": "Goro Kobayashi",
      "affiliations": [
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A2899299319",
      "name": "Tatsuki Kuribayashi",
      "affiliations": [
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A2750936731",
      "name": "Sho Yokoi",
      "affiliations": [
        "Tohoku University",
        "RIKEN"
      ]
    },
    {
      "id": "https://openalex.org/A2084773436",
      "name": "Kentaro Inui",
      "affiliations": [
        "Tohoku University",
        "RIKEN"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4250954493",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W3116570699",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3172099915",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W4298201654",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W1974339500",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2897507397",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W2247359815",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2964303497",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W3138794547",
    "https://openalex.org/W3174617925",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2805430026",
    "https://openalex.org/W2964098600",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2752172973",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2950621961",
    "https://openalex.org/W3171953676",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2991265431",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2995446988",
    "https://openalex.org/W2977944219",
    "https://openalex.org/W1026270304",
    "https://openalex.org/W3102466593",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2972342261",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2947012833",
    "https://openalex.org/W2787248994",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2970451234",
    "https://openalex.org/W2980433389",
    "https://openalex.org/W4287110638",
    "https://openalex.org/W3099143320"
  ],
  "abstract": "Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can also contribute to Transformers’ progressive performance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual connection, and layer normalization. Our analysis of Transformer-based masked language models shows that the token-to-token interaction performed via attention has less impact on the intermediate representations than previously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention patterns tends not to adversely affect the performance. The codes of our experiments are publicly available.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4547–4568\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n4547\nIncorporating Residual and Normalization Layers\ninto Analysis of Masked Language Models\nGoro Kobayashi1 Tatsuki Kuribayashi1,2 Sho Yokoi1,3 Kentaro Inui1,3\n1 Tohoku University 2 Langsmith Inc. 3 RIKEN\ngoro.koba@dc.tohoku.ac.jp\n{kuribayashi, yokoi, inui}@tohoku.ac.jp\nAbstract\nTransformer architecture has become ubiqui-\ntous in the natural language processing ﬁeld.\nTo interpret the Transformer-based models,\ntheir attention patterns have been extensively\nanalyzed. However, the Transformer archi-\ntecture is not only composed of the multi-\nhead attention; other components can also\ncontribute to Transformers’ progressive perfor-\nmance. In this study, we extended the scope\nof the analysis of Transformers from solely\nthe attention patterns to the whole attention\nblock, i.e., multi-head attention, residual con-\nnection, and layer normalization. Our anal-\nysis of Transformer-based masked language\nmodels shows that the token-to-token interac-\ntion performed via attention has less impact\non the intermediate representations than pre-\nviously assumed. These results provide new\nintuitive explanations of existing reports; for\nexample, discarding the learned attention pat-\nterns tends not to adversely affect the perfor-\nmance. The codes of our experiments are pub-\nlicly available.1\n1 Introduction\nTransformer architecture (Vaswani et al., 2017) has\nadvanced the state of the art in a wide range of\nnatural language processing (NLP) tasks (Devlin\net al., 2019; Liu et al., 2019; Lan et al., 2020).\nAlong with this, Transformers have become a ma-\njor subject of research from the viewpoints of en-\ngineering (Rogers et al., 2020) and scientiﬁc stud-\nies (Merkx and Frank, 2021; Manning et al., 2020).\nIn particular, the multi-head attention, a core\ncomponent of Transformers, has been extensively\nanalyzed (Clark et al., 2019; Kovaleva et al., 2019;\nReif et al., 2019; Lin et al., 2019; Mare ˇcek and\nRosa, 2019; Htut et al., 2019; Raganato and Tiede-\nmann, 2018; Tang et al., 2018). While these analy-\nses suggest that the multi-head attention contributes\n1https://github.com/gorokoba560/\nnorm-analysis-of-transformer\n(a) Existing analysis focusing\nonly on the multi-head atten-\ntion (Kobayashi et al., 2020).\n(b) Proposed method incor-\nporating the whole attention\nblock (i.e., multi-head atten-\ntion, residual connection, and\nlayer normalization) into the\nanalysis.\nFigure 1: Visualizations of the token-by-token inter-\nactions in each layer when a sentence pair is fed into\nthe pre-trained BERT-base. The diagonal elements\ncorrespond to the effect of preserving the original in-\nput information. The contrast between Figures 1a\nand 1b demonstrates that the contextual information\ncontributed less to the computation of the output rep-\nresentations than previously expected.\nto capturing linguistic information such as semantic\nand syntactic relations, some reports question the\nimportance of attention. For example, several stud-\nies in ﬁelds ranging from NLP (Michel et al., 2019;\nKovaleva et al., 2019) to neuroscience (Toneva and\nWehbe, 2019) empirically found that discarding\nlearned attention patterns from Transformers re-\ntains or even improves their performance in down-\nstream tasks and the ability to simulate human brain\nactivity. These observations imply that Transform-\ners do not heavily rely on the multi-head attention\nalone, and the other components contribute to their\nprogressive performance.\nIn this study, we broaden the scope of the analy-\nsis from the multi-head attention to the whole atten-\ntion block, i.e., the multi-head attention, residual\nconnection, and layer normalization. Our analysis\nof the Transformer-based masked language mod-\nels (Devlin et al., 2019; Liu et al., 2019) revealed\n4548\nthat the newly incorporated components have a\nlarger impact than expected in previous studies (Ab-\nnar and Zuidema, 2020; Kobayashi et al., 2020)\n(Figure 1).\nMore concretely, we introduce an exact de-\ncomposition of the operations in the whole at-\ntention block exploiting the norm-based analy-\nsis (Kobayashi et al., 2020). Our analysis quan-\ntiﬁes the impact of the two contrasting effects of\nthe attention block : (i) “mixing” the input repre-\nsentations via attention and (ii) “preserving” the\noriginal input mainly via residual connection (Sec-\ntion 3). Our analysis reveals that the preserving\neffect is more dominant in each attention block\nthan previously estimated (Abnar and Zuidema,\n2020; Kobayashi et al., 2020). The results also\nreveal the detailed mechanism of each component\nin the attention block. The residual connections\npass through much larger vectors than the vectors\nproduced by the multi-head attention. The layer\nnormalization also reduces the effect of the opera-\ntion via attention.\nOur ﬁnding of the relatively small impact of the\nmulti-head attention provides new intuitive inter-\npretations for some existing reports, for example,\ndiscarding the learned attention patterns did not\nadversely affect their performance. Our analysis\nalso provides a new intuitive perspective on the\nbehaviors of Transformer-based masked language\nmodels. For example, BERT (Devlin et al., 2019)\nhighlights low-frequency (informative) words in en-\ncoding texts, which is consistent with the existing\nmethods for effectively computing text representa-\ntions (Luhn, 1958; Arora et al., 2017).\nThe contributions of this study are as follows:\n• We expanded the scope of Transformers anal-\nysis from the multi-head attention to the atten-\ntion block (i.e., multi-head attention, residual\nconnection, and layer normalization).\n• Our analysis revealed that the operation via\nresidual connection and layer normalization\ncontributes more to the internal representa-\ntions than expected in previous studies (Abnar\nand Zuidema, 2020; Kobayashi et al., 2020).\n• We detailed the functioning of BERT: (i)\nBERT tends to mix a relatively large amount\nof contextual information into [MASK] in the\nmiddle and later layers; and (ii) the contribu-\ntion of contextual information in the attention\nblock is related to word frequency.\n!!!\"!#!$!%\n\t\u000f\u001f\u0012\u001a\u0001(\u0018\u001a\u0016\u000f\u0015\u0014 \u000f\u001c\u0014\u0018\u0017\u0002\t( \n\b\u0017\u0019\u001d\u001c\u0001\u001e\u0012-\u001c\u0018\u001a\u001b\n)\u0012\u001b\u0014\u0011\u001d\u000f\u0015\u0006\u0018\u0017\u0017\u0012-\u001c\u0014\u0018\u0017\u0002)\u0007\r \u0003\n\n\u0006\b\n\u0007\t\b\u0001\u0003\u0005\u0005\u0004\u0002\n\u001d\u0015\u001c\u0014\u0004\u0013\u0012\u000f\u0011\u0001\u0005\u001c\u001c\u0012\u0017\u001c\u0014\u0018\u0017\u0002\u0005\u000e\u000e( \n!\"!!\"\"!\"#!\"$!\"%\nFigure 2: Visualization of the attention block, consist-\ning of multi-head attention, residual connection, and\nlayer normalization, in the Transformer layer.\n2 Background: Transformer architecture\nThe Transformer architecture consists of a stack of\nlayers. Each layer has an attention block, which is\nresponsible for capturing the interactions between\ninput tokens. The attention block can be further\ndivided into the three components: multi-head at-\ntention (ATTN), residual connection (RES), and\nlayer normalization (LN) (Figure 2). This block\ncan be written as the following composite function:\n˜xi = LN\n(\nATTN(xi,X) + xi\n)\n, (1)\nwhere xi ∈ Rd is the i-th input representation,\nX := [ x1,..., xn] ∈Rn×d is the sequence of\ninput representations, and ˜xi ∈Rd is the output\nrepresentation corresponding to xi. Boldface let-\nters such as xdenote row vectors. In the following,\nwe review the computations in the ATTN, RES,\nand LN components.\nMulti-head attention (ATTN): The ATTN\ntakes the role of mixing contextual information\ninto output representations. Formally, given input\nrepresentations X, the H head ATTN computes\nthe output ATTN(xi,X) ∈Rd for each input xi:\nATTN(xi,X) =\nH∑\nh=1\nATTN h (xi,X), (2)\nwhere ATTN h (xi,X) ∈ Rd denotes the\noutput vector from each attention head h.\nATTN h (xi,X) is computed by each attention\n4549\nhead has follows:\nATTN h (xi,X)\n=\nn∑\nj=1\nαh\ni,j\n(\nxjW h\nV + bh\nV\n)\nW h\nO ,\n(3)\nαh\ni,j := softmax\nxj∈X\n(\nq(xi)k(xj)⊤\n√dh\n)\n, (4)\nq(x) := xW h\nQ + bh\nQ , (5)\nk(x) := xW h\nK + bh\nK , (6)\nwhere W h\nQ ,W h\nK ,W h\nV ∈Rd×dh, and W h\nO ∈\nRdh×d are the weight matrices, and bh\nQ ,bh\nK , and\nbh\nV ∈Rdh are the bias vectors. dh denotes the\ndimension of each head (64 is usually used), and\ndh ×H = dholds. Here, Q, K, and V stand for\nQuery, Key, and Value, respectively. Note that in a\ntypical attention analysis, the attention weight αh\ni,j\nhas been assumed to represent the contribution of\nthe input xj to computing ˜xi.\nResidual connection (RES): In RES, the origi-\nnal input vector for the multi-head attention (xi) is\nadded to its output vector:\nATTN(xi,X) ↦→ATTN(xi,X) + xi. (7)\nLayer normalization (LN): LN ﬁrst normalizes\nthe input vector and then applies a transformation\nwith learnable parameters γ∈Rd and β∈Rd:\nLN(y) = y−m(y)\ns(y) ⊙γ+ β∈Rd, (8)\nwhere m(y) ∈ R and s(y) ∈ R denote the\nelement-wise mean and standard deviation 2, re-\nspectively. Here, subtraction and division are also\nperformed on an element-wise basis. The normal-\nized vector, (y−m(y))/s(y), is then transformed\nwith γand β; here, ⊙denotes the element-wise\nmultiplication.\nNote that analyzing the feed-forward networks\nin each layer is beyond the scope of this study and\nwill be carried out as future work.\n2Speciﬁcally, m(y) := 1\nd\n∑\nk y(k) and s(y) :=√\n1\nd\n∑\nk\n(\ny(k) −m(y) +ϵ\n)2\n, where y(k) denotes the k-th\nelement of the vector y and ϵ ∈R is a small constant to\nstabilize the value.\n3 Proposal: Analyzing attention blocks\nFor analyzing Transformers, solely observing\nthe attention weights has been a common\nmethod (Clark et al., 2019; Kovaleva et al., 2019,\netc.). We extend the scope of analysis to the whole\nattention block (ATTN, RES, and LN).\n3.1 Strategy: Norm-based analysis\nKobayashi et al. (2020) introduced the norm-based\nanalysis to extend the scope of analysis from the\nattention weights to the whole multi-head attention.\nWe follow this norm-based analysisand extend its\nscope to the whole attention block.\nThe norm-based analysis ﬁrst attempts to de-\ncompose the output vector ˜xi into the sum of the\ntransformed input vectors {xj}:\n˜xi = ∑\nj Fi(xj), (9)\nwhere Fi is an appropriate vector-valued function.\nThen, the contribution of xj to ˜xi can be expressed\nby the norm of Fi(xj). In the next subsection,\nwe indicate that this norm-based method can be\napplied to analyzing the whole attention block. In\nother words, the output of the attention block is also\nbe decomposed into the sum of the transformed\ninput vectors without any approximation.\n3.2 Decomposing output into a sum of inputs\nThe output ˜xi is decomposed into a sum of terms\nassociated with each input xj. First, ATTN (Equa-\ntion 2) can be decomposed into a sum of vec-\ntors (Kobayashi et al., 2020):\nATTN(xi,X) =\nn∑\nj=1\nH∑\nh=1\nαh\ni,j f h (xj), (10)\nf h (x) :=\n(\nxW h\nV + bh\nV\n)\nW h\nO . (11)\nSecond, in RES, no interaction between subscripts\niand joccurs, and the form is already additively\ndecomposed. Third, by exploiting the linearity of\nm(·), we can derive the “distributive law” of LN\nand decompose it. Let y= ∑\nj yj be the input to\nLN. Then,\nLN(y) =\n∑\nj\ngy(yj) + β, (12)\ngy(yj) := yj −m(yj)\ns(y) ⊙γ. (13)\nSee Appendix A for the derivation.\n4550\nWith these decompositions of ATTN and LN, the\noutput of the whole attention block can be written\nas the sum of vector-valued functions with each\ninput vector in Xas an argument:\n˜xi = LN (ATTN(xi,X) + xi) (14)\n= LN\n(H∑\nh=1\nn∑\nj=1\nαh\ni,j f h (xj) + xi\n)\n(15)\n=\n∑\nj̸=i\ngy\n(H∑\nh=1\nαh\ni,j f h (xj)\n)\n+ gy\n(H∑\nh=1\nαh\ni,i f h (xi)\n)\n+ gy(xi) (16)\n+ β,\ny:= ATTN(X,xi) + xi. (17)\n3.3 Measuring the contribution of context\nRegarding the success of the contextualized repre-\nsentations in NLP, an interesting issue is theloca-\ntion and strength of the context mixing performed\nin the model. Based on this issue, we investigate\nthe attention block by categorizing the terms in\nEquation 16 into the two effects:3\n1. Mixing contextual information into the output\nrepresentation by the ATTN:\n˜xi←context :=\n∑\nj̸=i\ngy\n(H∑\nh=1\nαh\ni,j f h ( xj )\n)\n.\nWe measure the magnitude of this context-\nmixing effectby the norm ∥˜xi←context∥. This\nstrength refers to the amount of information\nfrom the surrounding contexts{x1,..., xn}\\\n{xi}in calculating ˜xi.\n2. Preserving the original information via ATTN\nand RES:\n˜xi←i := gy\n(H∑\nh=1\nαh\ni,i f h ( xi )\n)\n+ gr( xi ).\nWe measure the magnitude of the preserv-\ning effectby the norm ∥˜xi←i∥. This strength\nrefers to the amount of information from the\noriginal vector xi used in calculating ˜xi. At\nthe attention block, information from the in-\nput vector xi can ﬂow through two ways: (i)\nattention to the original input (the ﬁrst term)\nand (ii) residual connection (the second term).\n3The bias βaffects neither thecontext-mixing effect nor the\npreserving effect. Thus, we ignored this term in our analysis.\nTo summarize the relative strength of the context-\nmixing effect, the context-mixing ratio is deﬁned\nas follows:\nri =\n∥˜xi←context ∥\n∥˜xi←context ∥+ ∥˜xi←i ∥. (18)\nA higher mixing ratio indicates that the mixing\neffect is more dominant than the preserving effect\nin the computation of ˜xi.\nNote that Abnar and Zuidema (2020) assumed\nthat the multi-head attention and residual connec-\ntion always equally impact the output, i.e., r≈0.5\nin the analysis of Transformers. However, our ex-\nperiments revealed that, in practical masked lan-\nguage models, the mixing ratio is considerably be-\nlow 0.5.\n4 Experiments: Analysis of mixing ratio\nThe context-mixing ratio of the attention blocks\nin pre-trained masked language models was ana-\nlyzed using the proposed norm-based analysis. The\nobtained results were different from those of the\nexisting methods that analyze only some of the\ncomponents in the attention block.\n4.1 General setup\nModel: We investigated the 32 variants of the\nmasked language models (BERT with ﬁve different\nsizes, BERT-base trained with 25 different seeds,\nand RoBERTa with two different sizes). In Sec-\ntion 4, the results for BERT-base and RoBERTa-\nbase are demonstrated. The results for the other\nmodels are provided in Appendix B and C. Note\nthat most of our ﬁndings reported in this section\ngeneralize across these model variants. Exceptions\nare discussed in the relevant section (Section 4.4).\nData: We experimented with the following four\ndatasets: (i) Wikipedia (Clark et al., 2019), (ii) the\nStanford Sentiment Treebank v2 dataset (SST-2,\nSocher et al., 2013), (iii) the Multi-Genre Natu-\nral Language Inference corpus (MNLI, Williams\net al., 2018), and (iv) the standard CoNLL-2003\nNamed Entity Recognition dataset (CoNLL’03-\nNER, Tjong Kim Sang and De Meulder, 2003).\nThe statistics of the datasets are shown in Table 1.\nOwing to the limitation of space, we only give the\nresults for the Wikipedia data in Section 4. The\ntrends observed for the Wikipedia dataset were gen-\neralized across the other datasets (see Appendix B).\nNote that each sequence of the Wikipedia dataset\n4551\nData #Samples Avg. length Domains\nWikipedia 992 122 Web encyclopedia\nSST-2 872 25 Movie reviews\nMNLI 1000 39 10 distinct genres\nCoNLL’03-NER 1000 21 News\nTable 1: Details of the datasets. Avg. length is the\nnumber of tokens segmented by BERT per sample.\nconsists of paired consecutive paragraphs. Each se-\nquence is fed into the models with masking applied\nto 15% of tokens 80% of the time.4\nAnalysis methods: We compared the context-\nmixing ratio computed with the following ﬁve ana-\nlyzing methods:\n• ATTN -W: Analyzing ATTN via attention\nweights, which has been applied in many exist-\ning studies (Clark et al., 2019; Kovaleva et al.,\n2019; Mare ˇcek and Rosa, 2019, etc.). The\nratio, where attention weight assigned to the\noriginal input vector αi,i corresponds to the\npreserving effect, and the others correspond\nto the mixing effect, is calculated as follows:\n1\nH\nH∑\nh=1\n∑\nj̸=i α\nh\ni,j\n∑\nj̸=i α\nh\ni,j + α\nh\ni,i\n=\n∑\nh\n∑\nj̸=i α\nh\ni,j\nH\n.\n• ATTN -N: Analyzing ATTN via the vector\nnorm (Kobayashi et al., 2020). The mixing\nratio is calculated as\n∥∑\nh\n∑\nj̸ =i α\nh\ni,j f h (xj)∥\n∥∑\nh\n∑\nj̸ =i α\nh\ni,j f h (xj)∥+ ∥∑\nh= α\nh\ni,i f h (xi)∥\n.\n• ATTN RES-W: Analyzing ATTN and RES\nvia attention weights, as Abnar and Zuidema\n(2020) did. They assumed that the residual-\naware attention matrix is constructed as\n0.5A+ 0.5I. Here, Ais the actual attention\nmatrix and Iis the identity matrix considered\nas the effect of residual connection. The mix-\ning ratio is calculated as\n1\nH\nH∑\nh=1\n∑\nj̸=i 0.5α\nh\ni,j\n∑\nj 0.5α\nh\ni,j + 0.5\n.\n• ATTN RES-N (proposed): Analyzing ATTN\nand RES via the vector norm – a version of\nour proposed method that does not consider\nLN. The mixing ratio is calculated as\n4For the other datasets, we used 1000 samples from their\nvalidation set or used all of their validation set if the number\nof sequences is less than 1000.\nMethods Mean Max Min\n— BERT-base —\nATTN -W 97.1 100.0 45.0\nATTN -N 85.2 100.0 4.9\nATTN RES-W 48.6 50.0 22.5\nATTN RES-N 22.3 65.7 2.0\nATTN RESLN-N 18.8 61.3 1.3\n— RoBERTa-base —\nATTN -W 95.8 100.0 3.8\nATTN -N 84.4 100.0 13.8\nATTN RES-W 47.9 50.0 1.9\nATTN RES-N 19.6 69.9 1.8\nATTN RESLN-N 16.2 73.4 1.5\nTable 2: Mean, maximum, and minimum values of the\nmixing ratio computed with each method.\n∥∑\nh\n∑\nj̸ =i α\nh\ni,j f h (xj)∥\n∥∑\nh\n∑\nj̸ =i α\nh\ni,j f h (xj)∥+ ∥∑\nh α\nh\ni,i f h (xi) +xi∥\n.\n• ATTN RESLN-N (proposed): Analyzing\nATTN, RES, and LN via the vector norm –\nthe method proposed in Section 3. This corre-\nsponds to the ri in Equation 18.\n4.2 Results\nWe computed the mixing ratio of each token in each\nlayer (each attention block) of the models with the\nﬁve analysis methods (Section 4.1). The average,\nmaximum, and minimum mixing ratios are shown\nin Table 2. Each row corresponds to a different\nanalysis method.\nLower mixing ratio than in existing methods:\nTable 2 shows that the mixing ratios obtained from\nthe proposed ATTN RES-N and ATTN RESLN-N\nlargely differ from those obtained from the existing\nmethods. Whereas the attention analyses ( ATTN -\nW and ATTN -N) yield mixing ratios of 84–97%\nand ATTN RES-W yields 48%–49%, our proposed\nmethod (ATTN RESLN-N) yields about 16 and 19%\non average. The visualizations of the token-by-\ntoken interactions in the common attention map\nstyle become almost diagonal patterns (Figure 1).\nThese demonstrate that each layer’s context mixing\nis lower than previously expected, and RES and\nLN largely cancel the mixing by ATTN. Observing\nthe only ATTN and making an inference about the\nTransformer layer may lead to misleading. Note\nthat Srivastava et al. (2015) reported a similar trend\nthat stacked feed-forward networks tend to priori-\ntize the “preserving” effect in skip connections.\nConsistent trends across model sizes: Our\nmethod consistently shows the lowest mixing ra-\ntio among the compared methods for BERT and\n4552\nRoBERTa models of various sizes (BERT-large,\nmedium, small, tiny, and RoBERTa-large) (Ap-\npendix B). Interestingly, the context-mixing ratio\nis higher in the models with fewer layers (37% in\nBERT-tiny, but15% in BERT-large).\n4.3 Connections to previous studies\nOur ﬁnding of a lower mixing ratio than previously\nthought provides explanations for previous results\nand is consistent with the pre-training strategy.\nToken identiﬁability: The low context-mixing\nratio is consistent with Brunner et al. (2020)’s re-\nports on what they called “token identiﬁability.”\nThey showed that input tokens can be well pre-\ndicted only from the corresponding internal rep-\nresentations within BERT, especially in shallower\nlayers, suggesting that context mixing is performed\nlittle by little. Our analysis results of the whole\nattention block were consistent with this ﬁnding.\nMasked language modeling objective: Regard-\ning the masked token prediction task 5 during the\npre-training, BERT and RoBERTa learn to con-\nduct the following operations for a given input se-\nquence: (i) inﬁlling the [MASK] with plausible\nwords, (ii) replacing the normal (non-special) to-\nkens that might not ﬁt their context (i.e., randomly\nreplaced tokens) with plausible one, and (iii) re-\nconstructing the original input tokens that might ﬁt\ntheir context.\nIn our experiments and in common practical use,\nmost tokens in the input sequence are not masked\nand ﬁt their context. Thus, BERT is assumed to\nreconstruct the inputs for these tokens (i.e., behave\nas an auto-encoder). From this point of view, the\nsuperiority of the preserving effect is the intuitive\nbehaviors of the masked language models.\nLow impact of discarding learned attention pat-\nterns: Several studies have reported the low im-\npact of discarding the learned attention patterns\nin Transformers. Michel et al. (2019) and Koval-\neva et al. (2019) reported that the attention pat-\nterns of many attention heads in Transformers can\nbe removed or overwritten into the uniform pat-\nterns with almost no change in their performance,\nand this even brought about improvements in some\ncases. V oita et al. (2019) also reported the same\n5For masked language modeling in BERT and RoBERTa\npre-training, 15% of the tokens are randomly chosen from\nthe input sequence, of which 80% are replaced with [MASK],\n10% are replaced with random words, and 10% are kept un-\nchanged.\nphenomenon using a pruning method with addi-\ntional training. In addition, Toneva and Wehbe\n(2019) reported that using uniform attention in\nearly layers of BERT instead of the learned atten-\ntion patterns leads to a better ability to simulate\nhuman brain activity.\nOur analysis shows that most of the attention\nsignal is reduced by the immediately following\nmodules, RES and LN. This fact may explain the\nabove observations that discarding the learned at-\ntention patterns of many attention heads does not\ncause a severe difference.\n4.4 Mechanism\nHow is the mixing effect conducted in multi-head\nattention largely suppressed in the whole attention\nblock? We discuss the mechanism role of ATTN\nand LN in suppressing the mixing ratio.\nATTN reduces context-mixing ratio: RES is a\nmechanism that equally adds together the output\nof ATTN and the input in a one-to-one fashion\n(Equation 7). Considering this, the mixing ratio\nin the scope of ATTN and RES is expected to be\nabout 50%, while the mixing ratio was actually\nsubstantially below 50% (19–22% in ATTN RES-\nN) (Section 4.2). This suggests that the output\nof ATTN is much smaller than the input; in other\nwords, ATTN seems to have the effect of largely\nshrinking inputs to compute the output. How is this\nachieved?\nRecall that the output of ATTN is a weighted\nsum of the afﬁne-transformed vector f h (x) us-\ning with attention weight αh\ni,j (Equation 10). We\ndescribe and empirically show that (i) the afﬁne\ntransformation in ATTN has the effect of shrink-\ning the inputs, and (ii) the attention weights and\nafﬁne-transformed vectors cancel each other out on\nspeciﬁc vectors. We describe a brief idea here and\nprovide the detailed derivation of each equation in\nthe Appendix C.\nFirst, under a coarse assumption, multiple afﬁne\ntransformations performed in the multi-head atten-\ntion can be integrated into a single one:\nf(x) := ∑H\nh=1 f h (x). (19)\nAssume that the input vectorx is a sample from the\nstandard normal distribution: x ∼N(0,Id). Then\nwe can estimate its magnitude by E∥x∥ ≈\n√\nd\nand the magnitude after afﬁne transformation by\nE∥f(x)∥≈\n√∑d\nk=1 σ2\nk, where σ1,...,σ d denote\n4553\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 3: Mixing ratio in each layer of BERT calculated from each method.\nsingular values of f. Thus, the expansion rate of f\nis approximately estimated by\n∥f(x)∥/∥x∥≈\n√∑d\nk=1 σ2\nk /\n√\nd. (20)\nIf the ratio is lower than one, f has a tendency\nof shrinking the input. For commonly used large\nmodels, results stably demonstrated the shrinking\ntendency (layer mean of the expansion rate was\n0.88 < 1.0 for BERT-base and 0.80 < 1.0 for\nBERT-large). Note that, for smaller models, results\ndemonstrated the expanding tendency (layer mean\n1.24 for BERT-mini and1.86 for BERT-tiny). This\nis consistent with the result that the latter models\ntended to have a higher mixing ratio than the former\nmodels (Section 4.2). Detailed results are shown in\nAppendix C.3.\nFurthermore, attention weight αh\ni,j boosts the\nshrinking effect. Kobayashi et al. (2020) reported\nthe negative correlation between ∥f h (xj)∥and\nαh\ni,j on frequent tokens. That is, ATTN wastes a\nlot of attention weights αh\ni,j by assigning them to\nsmall vectors ∥f h (xj)∥.\nTo summarize, ATTN’s shrinking effect is prob-\nably achieved by (i) the shrinking in f alone and\n(ii) further shrinking through the cancellation of\nαand ∥f(x)∥. By these mechanisms, ATTN can\ncontribute to decreasing the mixing ratio.\nLN reduces the context-mixing ratio: LN con-\ntains not only the vector normalization but also\nthe afﬁne transformation with learnable parameters\n(Equation 8). Although the validity or usage of LN\nhas been investigated in terms of stability and speed\nof training (Parisotto et al., 2020; Liu et al., 2020),\nthe effects of afﬁne transformation have rarely been\nexplored. By comparing the mixing ratios obtained\nfrom ATTB RES-N and ATTN RESLN-N (Table 2),\nwe discovered that LN reduced the context-mixing\nratio. This suggests that the scaling (by γ) of the\nafﬁne transformation shrinks the vector from ATTN\nand emphasizes RES over ATTN.\n5 Detailed analysis\nWe further analyzed the mixing ratio of the masked\nlanguage models in detail from the perspectives\nof both the layer and word attributes. In this sec-\ntion, we inherit the experimental setup (Section 4.1)\nfrom the previous section and demonstrate results\nfor BERT-base with the Wikipedia dataset. The re-\nsults for the other experimental settings are shown\nin Appendix B and D. Note that only the ﬁnding\nreported in Section 5.2 did not generalize across\nmodel variants, and we exceptionally discuss this\npoint in the body.\n5.1 Differences by layers and token types\nFigure 3 shows the mixing ratio in each layer of the\nBERT model (results for other models are shown\nin Appendix B). Each subﬁgure corresponds to a\ndifferent analysis method, each row represents a\nlayer, and each column represents a token type. The\naveraged results of the following token categories\nand their overall average (“overall”) are reported:\n(i) non-special tokens (“normal”), (ii) [MASK],\n(iii) [CLS], and (iv) [SEP].\n4554\nMethods Spearman’sρ\nall tokens w/o special tokens\nATTN -W 0.16 0 .14\nATTN -N −0.39 −0.41\nATTN RES-W 0.16 0 .14\nATTN RES-N −0.84 −0.86\nATTN RESLN-N −0.54 −0.58\nTable 3: Spearman’sρbetween the frequency rank and\nthe mixing ratio calculated by each method. In the\n“w/o special tokens” setting, it was calculated without\n[CLS] and [SEP].\n\u0003\b\u0010\b\n\u0007\u0001\r\u0004\u000e\b\u000b\n\u0002\r\u0006\f\u000f\u0006\n\u0005\u0011\u0001\r\u0004\n\t\nFigure 4: Relationship between the frequency rank\nof tokens and the mixing ratio calculated with the\nATTN RESLN-N.\nResults and discussion: Our proposed method\nshowed that the mixing ratio is higher in the earlier\nlayers than in the later ones (see the “overall” trend\nin Figure 3e). 6 This trend mirrors the tendency\nthat a deep neural network with “gates” similar to\nresidual connections passes through the input more\nin the later layers (Srivastava et al., 2015).\nFurthermore, our method showed a distinctive\ntrend for the [MASK] tokens. In the middle and\ndeep layers, the mixing ratio for [MASK] becomes\nhigher (19–30%) than the overall trends (15–20%).\nNote that this trend becomes clearer when consider-\ning the RES and LN. This trend implies that in the\nmiddle and deep layers, BERT refers to contextual\ninformation for predicting the masked words. The\ntrends of the other masked language models are\nshown in Appendix B.\n5.2 Word frequency and mixing ratio\nIn this section, we will discuss the property of\nBERT related to the word frequency.7\n6The Spearman’sρbetween the “overall” mixing ratio and\nthe layer depth are −0.67 and −0.98 in “overall” of BERT-\nbase and RoBERTa-base, respectively.\n7Following Kobayashi et al. (2020), we counted the fre-\nquency for each word type by reproducing the training data of\nResults: Table 3 lists the Spearman’s rank cor-\nrelation ρ between the frequency rank (e.g.,\nrank(“the”) = 1 , rank(“and”) = 6 , etc.) and\nthe mixing ratio across tokens in the text data.\nThe results obtained from ATTN RES-N and\nATTN RESLN-N indicate a surprisingly stronger\nnegative correlation than the results obtained by\nthe existing methods (Figure 4). This indicates that\nBERT discounts the information of high-frequency\nwords compared with low-frequency ones.8\nDiscussion: Discounting high-frequency words\nis a common practice for making the semantic\nrepresentation of a sentence or a text from word\nrepresentations; examples are Luhn’s heuristic in\nclassical text summarization (Luhn, 1958) and the\nsmooth inverse frequency (SIF) weighting in sen-\ntence vector generation (Arora et al., 2017). Our\nfrequency-based results reveal that attention blocks\nin BERT achieve this desirable property.\nOur observation may also explain the phe-\nnomenon that adding up BERT’s internal or out-\nput representations does not produce a good\nsentence vector (Reimers and Gurevych, 2019).\nIn contrast, in static word embeddings (e.g.,\nword2vec (Mikolov et al., 2013)), the norm en-\ncodes the word importance derived from its fre-\nquency (Schakel and Wilson, 2015); we can gener-\nate a good sentence vector by simply adding these\nstatic word vectors (Yokoi et al., 2020). Our ﬁnd-\ning suggests that BERT encodes the token’s impor-\ntance through the context-mixing ratio rather than\nthe norm.9 In this sense, it is plausible that addi-\ntive composition using BERT’s internal or output\nrepresentations does not perform well.\nGeneralizability: Contrary to the other experi-\nmental results, only the relationship between word\nfrequency and mixing ratio (Figure 4) was not con-\nsistent across different model sizes. For the larger\nvariant (BERT-large), a stronger negative correla-\ntion between them was indicated than for BERT-\nbase, while for the smaller variants (BERT-medium,\nBERT-small, BERT-mini, and BERT-tiny), even a\npositive correlation or no correlation was indicated\n(see Appendix D). Generally, larger BERT models\nBERT.\n8Kobayashi et al. (2020) reported that ATTN in BERT\ntends to discounts frequent words when mixing contexts. We\nfound even stronger trends after broadening the scope of anal-\nysis.\n9In BERT, it may be difﬁcult for the norm to encode the\ntoken importance, because the norm is ﬁxed at each layer\nnormalization.\n4555\n(BERT-base and BERT-large) achieve better per-\nformance on downstream tasks. The different re-\nsults across model sizes suggest that this desirable\nproperty can be learned when the representational\npower is sufﬁcient.\n6 Related work\n6.1 Probing Transformers\nAs current neural-based models have an end-to-end,\nblack-box nature, existing studies have adopted sev-\neral strategies to interpret their inner workings (Car-\nvalho et al., 2019; Rogers et al., 2020; Bra¸ soveanu\nand Andonie, 2020). In analyzing Transformers,\nprevious studies have mainly employed the fol-\nlowing approaches: (i) observing the vanilla atten-\ntion weights (Clark et al., 2019; Kovaleva et al.,\n2019; Reif et al., 2019; Lin et al., 2019; Mareˇcek\nand Rosa, 2019; Htut et al., 2019; Raganato and\nTiedemann, 2018; Tang et al., 2018) or the ex-\ntended version (Brunner et al., 2020; Abnar and\nZuidema, 2020), (ii) computing the gradient (Clark\net al., 2019; Brunner et al., 2020), and (iii) ana-\nlyzing the vector norm (Kobayashi et al., 2020).\nWe adopted the norm-based analysis because this\nmethod can be naturally extended to the analysis\nof the whole attention block and it has some ad-\nvantages (Kobayashi et al., 2020) that will also be\ndiscussed in the following paragraph.\nAs for broadening the scope of the analysis, Ab-\nnar and Zuidema (2020) modiﬁed the attention ma-\ntrix to incorporate the residual connections into the\nanalysis. However, they assumed that the multi-\nhead attention and residual connection equally con-\ntributed to the computation of the output repre-\nsentations without any justiﬁcation (Section 4.1).\nBrunner et al. (2020) employed a gradient-based\napproach for analyzing the interaction of input\nrepresentations; however, the gradient ignores the\nimpact of the input vector (i.e., only observing\n∂˜xi/∂xj neglects the impact of xj itself) as de-\nscribed in Section 6.2 of Kobayashi et al. (2020).\nNote that our norm-based analysis can include the\nmagnitude of the impact of inputs in the analysis.\n6.2 RES and LN in Transformers\nAlthough residual connections (RES) (He et al.,\n2016) and layer normalization (LN) (Ba et al.,\n2016) have rarely been considered in probing\nstudies, they are known to play important roles\nin both model performance and training conver-\ngence (Parisotto et al., 2020; Liu et al., 2020).\nDong et al. (2021) revealed that the residual connec-\ntions are important in attention-based architectures.\nThey demonstrated that the output of self-attention\nnetworks without residual connections converges\nto a rank-1 matrix quickly with increasing its layer\ndepth. In addition, as a similar component to RES,\nSrivastava et al. (2015) proposed “gates” that ad-\njust the amount of routing of the input information.\nTheir experiments using stacked feed-forward net-\nworks for image classiﬁcation also show consistent\ntrends with ours – the effect of preserving the orig-\ninal input is dominant especially in the later lay-\ners. Inspired by this observation, Liu et al. (2020)\nmodiﬁed the Transformer architecture to enhance\nthe original input in the residual connections and\ndemonstrated that this extension leads to better per-\nformance and convergence. Note that several vari-\nants of the Transformer-based architecture with\ndifferent arrangements of RES and LN have also\nbeen proposed (Klein et al., 2018; Xiong et al.,\n2020; Parisotto et al., 2020), and analyzing these\nmodels is one of our future works.\n7 Conclusions\nIn this paper, we have extended a norm-based analy-\nsis to broaden the scope of analyzing Transformers\nfrom the multi-head attention alone to the whole\nattention block, i.e., multi-head attention, residual\nconnection, and layer normalization. Our analy-\nsis of the masked language models revealed that\nthe context-mixing ratio in each block is much\nlower than expected in previous studies, demon-\nstrating that RES and LN largely cancel the mixing\nby ATTN. This observation can provide new expla-\nnations for some unexpected results were reported\non Transformers in ﬁelds ranging from NLP to\nneuroscience (e.g., discarding the learned attention\npatterns did not adversely affect the performance).\nOur detailed analysis further suggested that BERT\ndiscounts highly frequent, low-informative tokens.\nAlthough our method is applicable to analyzing\nother variants of Transformers, our experiments\nwere limited to the Transformer-based masked lan-\nguage models. In addition, the Transformer is not\ncomposed of only the attention block; feed-forward\nand embedding layers also exist. We plan to extend\nthis work in both directions.\n4556\nAcknowledgements\nWe would like to thank the members of the To-\nhoku NLP Lab for their insightful comments, par-\nticularly Benjamin Heinzerling for his valuable\nsuggestions on content and wording. This work\nwas supported by JST CREST Grant Number JP-\nMJCR20D2, Japan; JST ACT-X Grant Number\nJPMJAX200S, Japan; and JSPS KAKENHI Grant\nNumber JP20J22697.\nEthical considerations\nOne recent issue in the whole NLP community\nis that neural-network-based models have non-\nintended biases (e.g., gender bias) induced during\nthe training process. This paper gives a method\nfor interpreting the inner workings of real-world\nmachine learning models, which may help us un-\nderstand such biased behaviors of the models in the\nfuture.\nReferences\nSamira Abnar and Willem Zuidema. 2020. Quantify-\ning attention ﬂow in transformers. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics (ACL), pages 4190–4197.\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. In 5th International Conference on Learn-\ning Representations (ICLR).\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer Normalization. arXiv preprint\narXiv:1607.06450.\nAdrian M. P. Bra¸ soveanu and R˘azvan Andonie. 2020.\nVisualizing transformers for nlp: A brief survey. In\n24th International Conference Information Visuali-\nsation (IV), pages 270–279.\nGino Brunner, Yang Liu, Damián Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Watten-\nhofer. 2020. On Identiﬁability in Transformers. In\n8th International Conference on Learning Represen-\ntations (ICLR).\nDiogo V Carvalho, Eduardo M Pereira, and Jaime S\nCardoso. 2019. Machine Learning Interpretability:\nA Survey on Methods and Metrics.Electronics, 8(8),\n832.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What Does BERT\nLook At? An Analysis of BERT’s Attention. InPro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL-HLT), pages 4171–4186.\nYihe Dong, Jean-Baptiste Cordonnier, and Andreas\nLoukas. 2021. Attention is not all you need: pure\nattention loses rank doubly exponentially with depth.\nIn Proceedings of the 38th International Conference\non Machine Learning (ICML), PMLR, volume 139,\npages 2793–2803.\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 770–\n778.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel R. Bowman. 2019. Do Attention Heads\nin BERT Track Syntactic Dependencies? arXiv\npreprint arXiv:1911.12246.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Vincent\nNguyen, Jean Senellart, and Alexander M Rush.\n2018. Opennmt: Neural machine translation toolkit.\nIn Proceedings of the 13th Conference of the As-\nsociation for Machine Translation in the Americas\n(AMTA), pages 177–184.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is not only a weight:\nAnalyzing transformers with vector norms. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7057–7075.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the Dark Secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4364–4373.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In 8th Inter-\nnational Conference on Learning Representations\n(ICLR).\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen Sesame: Getting Inside BERT’s Linguistic\n4557\nKnowledge. Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 241–253.\nFenglin Liu, Xuancheng Ren, Zhiyuan Zhang, Xu Sun,\nand Yuexian Zou. 2020. Rethinking skip connection\nwith layer normalization. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics (COLING), pages 3586–3598.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nH. P. Luhn. 1958. The automatic creation of literature\nabstracts. IBM Journal of Research and Develop-\nment, 2(2):159–165.\nChristopher D. Manning, Kevin Clark, John Hewitt,\nUrvashi Khandelwal, and Omer Levy. 2020. Emer-\ngent linguistic structure in artiﬁcial neural networks\ntrained by self-supervision. Proceedings of the Na-\ntional Academy of Sciences, 117(48):30046–30054.\nDavid Mare ˇcek and Rudolf Rosa. 2019. From\nBalustrades to Pierre Vinken: Looking for Syntax in\nTransformer Self-Attentions. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 263–\n275.\nDanny Merkx and Stefan L. Frank. 2021. Human sen-\ntence processing: Recurrence or attention? In Pro-\nceedings of the Workshop on Cognitive Modeling\nand Computational Linguistics (CMCL), pages 12–\n22.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre Sixteen Heads Really Better than One? In Ad-\nvances in Neural Information Processing Systems 32\n(NeurIPS), pages 14014–14024.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. In Workshop Track Proceed-\nings of the 1st International Conference on Learning\nRepresentations (ICLR).\nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pas-\ncanu, Caglar Gulcehre, Siddhant Jayakumar, Max\nJaderberg, Raphael Lopez Kaufman, Aidan Clark,\nSeb Noury, Matthew Botvinick, Nicolas Heess, and\nRaia Hadsell. 2020. Stabilizing transformers for\nreinforcement learning. In Proceedings of the\n37th International Conference on Machine Learning\n(ICML), PMLR, volume 119, pages 7487–7498.\nAlessandro Raganato and Jörg Tiedemann. 2018.\nAn Analysis of Encoder Representations in\nTransformer-Based Machine Translation. In\nProceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 287–297.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and Measuring the Geometry of\nBERT. In Advances in Neural Information Process-\ning Systems 32 (NeurIPS), pages 8594–8603.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics (TACL), 8:842–\n866.\nAdriaan M. J. Schakel and Benjamin J. Wilson.\n2015. Measuring word signiﬁcance using dis-\ntributed representations of words. arXiv preprint\narXiv:1508.02297.\nThibault Sellam, Steve Yadlowsky, Jason Wei, Naomi\nSaphra, Alexander D’Amour, Tal Linzen, Jasmijn\nBastings, Iulia Turc, Jacob Eisenstein, Dipanjan Das,\nIan Tenney, and Ellie Pavlick. 2021. The multiberts:\nBert reproductions for robustness analysis. arXiv\npreprint arXiv:2106.16163,2020.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642.\nRupesh Kumar Srivastava, Klaus Greff, and Jürgen\nSchmidhuber. 2015. Training very deep networks.\nIn Advances in Neural Information Processing Sys-\ntems 28 (NIPS).\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2018.\nAn Analysis of Attention Mechanisms: The Case\nof Word Sense Disambiguation in Neural Machine\nTranslation. In Proceedings of the 3rd Conference\non Machine Translation (WMT): Research Papers,\npages 26–35.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT Rediscovers the Classical NLP Pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL), pages\n4593–4601.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142–147.\n4558\nMariya Toneva and Leila Wehbe. 2019. Interpret-\ning and improving natural-language processing (in\nmachines) with natural language-processing (in the\nbrain). In Advances in Neural Information Process-\ning Systems 32 (NeurIPS), pages 14928–14938.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Pro-\ncessing Systems 30 (NIPS), pages 5998–6008.\nRoman Vershynin. 2018. High-Dimensional Probabil-\nity: An Introduction with Applications in Data Sci-\nence. Cambridge Series in Statistical and Probabilis-\ntic Mathematics. Cambridge University Press.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL), pages 5797–5808.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Huishuai Zhang, Yanyan Lan, Liwei\nWang, and Tie-Yan Liu. 2020. On layer normaliza-\ntion in the transformer architecture. In 8th Inter-\nnational Conference on Learning Representations\n(ICLR).\nSho Yokoi, Ryo Takahashi, Reina Akama, Jun Suzuki,\nand Kentaro Inui. 2020. Word rotator’s distance. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2944–2960.\n4559\nA Derivation of “distributive law” of LN\nIn Section 3.2, we introduced the “distributive law”\nfor LN (layer normalization) in Equations 12 and\n13. Here, we show its derivation. Let z= ∑\nj zj\nbe the input to LN. Then, Equations 12 and 13 are\nderived as follows:\nLN(z) = z−1\nd\n∑\nk z(k)\ns(z) ⊙γ+ β (21)\n=\n∑\nj zj −1\nd\n∑\nk\n(∑\nj zj\n)(k)\ns(z) ⊙γ+ β\n(22)\n=\n∑\nj\nzj −1\nd\n∑\nk z(k)\nj\ns(z) ⊙γ+ β (23)\n=\n∑\nj\nzj −m(zj)\ns(z) ⊙γ+ β (24)\n=\n∑\nj\ngz(zj) + β. (25)\nB Mixing ratio in other settings\nIn Sections 4 and 5, we showed the experimental re-\nsults of mixing ratio for BERT-base with Wikipedia\ndataset. We also conducted the experiments with\nthe pre-trained BERT-large (Devlin et al., 2019),\nBERT-medium, BERT-small, BERT-mini, BERT-\ntiny (Turc et al., 2019), and RoBERTa-large (Liu\net al., 2019). Table 4 shows the architecture hy-\nperparameters of each model. Table 5 shows the\nstatistics of the mixing ratio for each model. Fig-\nures 5 to 11 show the mixing ratio at each layer\n(each attention block) of each model.\nWe also conducted it with the other three\ndatasets. Table 6 shows the statistics of the mixing\nratio for BERT-base on each dataset. Figures 12\nto 14 show the mixing ratio at each layer of BERT-\nbase on each dataset.\nFurthermore, we conducted it with 25 BERT-\nbase models trained with different seeds by Sellam\net al. (2021). Table 7 shows the statistics of the mix-\ning ratio for the models on the Wikipedia dataset.\nFigures 15 to 17 show the mixing ratio at each layer\nof three models (trained with 0th, 5th, 20th seeds)\nfrom them.\nIn Section 5.1, we showed the distinctive trend\nfor the [MASK] tokens in BERT-base with the\nWikipedia dataset. Even in the other models and\nwith the other datasets, the mixing ratio for the\nmasked tokens was relatively high in the middle\nand deep layers (Figures 5 to 14e).\nContrary to the results for the masked tokens, the\ntrend for the beginning of sentence token ([CLS]\nor <s>) was different across these models (Fig-\nures 5 to 11). For BERT-large, RoBERTa-large,\nand RoBERTa-base, the layer with the highest mix-\ning ratio for CLS was the ﬁrst layer, while for the\nother models, it was the ﬁnal or penultimate layer.\nDifferent trends between BERT and RoBERTa can\nbe naturally explained by the fact that RoBERTa\nis pre-trained without the next sentence predic-\ntion. Although we cannot interpret the difference\nof trends across BERT models with various sizes,\nit was consistent among them in that the later lay-\ners mix contextual information into [CLS] with a\nrelatively high mixing ratio. This implies that, in\nthe later layers, BERT conducts some operations\nspecialized to the next sentence prediction task.\nSolving such a discourse-level task in the later lay-\ners is consistent with the previous report that BERT\nmakes lower-level decisions (e.g., part-of-speech\ntagging) in the earlier layers and that the later lay-\ners have high-level information (e.g., knowledge\non co-reference) (Tenney et al., 2019).\nC Details on the investigation of the\nmechanism of ATTN’s shrinking\nWe describe the details of Section 4.4.\nC.1 Afﬁne transformation in ATTN\nIntegration of each head’s afﬁne\ntransformation\nTo consider the scaling effect of the afﬁne transfor-\nmations in ATTN, we integrate each head’s afﬁne\ntransformation f h into one afﬁne transformation\nf : Rd ↦→Rd, under a coarse assumption. First, for\nsimplicity, we assume that all heads in an ATTN\nassign the same weights\nα1\ni,j = ··· = α\nH\ni,j ≡αi,j. (26)\nThen, the computation of ATTN (Equation 10) can\nbe rewritten as follows:\nATTN(xi,X) =\nn∑\nj=1\nH∑\nh=1\nαh\ni,j f h (xj) (27)\n≈\nn∑\nj=1\nαi,j\nH∑\nh=1\nf h (xj) (28)\n=\nn∑\nj=1\nαi,jf(xj), (29)\n4560\nwhere\nf(x) :=\nH∑\nh=1\nf h (x). (30)\nConcrete computation of f\nFrom Equation 11, the afﬁne transformation f is\nf(x) =\nH∑\nh=1\n(\nxW h\nV + bh\nV\n)\nW h\nO . (31)\nFollowing the Transformer implementation, it can\nbe further simpliﬁed as follows:\nf(x) =\n(\nxWV + bV\n)\nWO, (32)\nWV :=\n[\nW 1\nV ... W\nH\nV\n]\n∈Rd×d, (33)\nbV :=\n[\nb1\nV ... b\nH\nV\n]\n∈Rd, (34)\nWO :=\n\n\nW 1\nO\n...\nW\nH\nO\n\n∈Rd×d. (35)\nOn the difference in arguments of ATTN andf\nIn Section 4.4, we considered the scaling effect of\nATTN, using the afﬁne transformation f. One may\nwonder about the difference between arguments of\nATTN (i.e., xi) and arguments of f (i.e., xj) in\nEquation 29. We can give two kinds of justiﬁcation\nto this question.\nIn the estimation of the expansion rate, we con-\nsider the expected value. From the symmetry of xi\nand xj, when the expected value forxj is obtained,\nthe expected value for xi is obtained. In the actual\nBERT model, it has been empirically conﬁrmed\nthat two token vectors xi,xj ∈X contained in\nthe same context Xexist in a fairly close position\n(xi ≈xj). First, Ethayarajh (2019) found that the\ncosine similarity between the intra-sentence repre-\nsentations in BERT is much larger than 0. Second,\nthe norm of input vectors has just been uniﬁed by\nthe layer normalization in the previous layer. Thus,\nfor our target models, xi ≈ xj is not a strong\nassumption.\nAfﬁne transformation as linear transformation\nThe afﬁne transformation f : Rd ↦→ Rd in\nATTN can be viewed as a linear transformation\n˜f : Rd+1 ↦→Rd+1. Given ˜x:=\n[\nx 1\n]\n∈\nRd+1, where 1 is concatenated to the end of each\ninput vector x∈Rd, the afﬁne transformation f\ncan be viewed as:\n˜f(˜x) = ˜x˜W\nV ˜W\nO\n(36)\n˜W\nV\n:=\n\n\n0\nWV ...\n0\nbV 1\n\n∈R(d+1)×(d+1)\n(37)\n˜W\nO\n:=\n\n\n0\nWO ...\n0\n0 ... 0 1\n\n∈R(d+1)×(d+1).\n(38)\nThe “afﬁne transformation” mentioned in Sec-\ntion 4.4 represent this linear transformation ˜f, and\nwe measured the singular values of ˜f.\nC.2 Expected expansion rate for a random\nvector\nIn the following, we introduce the derivation of the\nexpansion rate of the afﬁne transformation f, that\nis,\n∥f(x)∥\n∥x∥ ≈\n√∑d\nk=1 σ2\nk√\nd\n. (39)\nWe assume that the input vectorx is a sample from\nthe standard normal distribution:\nx = (x1,..., xd) ∼N(0,Id). (40)\nFirst, the expectation value of ∥x∥2 is as fol-\nlows (Vershynin, 2018):\nE\nx\n∥x∥2 = E\nx\nd∑\nk=1\nx2\nk =\nd∑\nk=1\nE\nx\nx2\nk = d. (41)\nThen, we have ∥x∥≈\n√\nd.\nNext, let the singular value decomposition of\nthe linear transformation f is f = UΣV⊤, where\nΣ = diag( σ1,...,σ d) ∈ Rd×d is the diagonal\nmatrix of singlar values of f. As the matrix V\nis orthogonal, the following random vecotr f also\nfollows the standard normal distribution, as does\nx:\ny = (y1,..., yd) := V⊤x ∼N(0,Id). (42)\n4561\nBy the orthogonal transformation by U does not\nchange the norm, we need to estimate ∥Σy∥2 in\norder to estimate ∥f(x)∥2 = ∥UΣV⊤x∥2.\nE\ny\n∥Σy∥2 = E\ny\nd∑\nk=1\nσ2\nky2\nk =\nd∑\nk=1\nσ2\nkE\nx\ny2\nk (43)\n=\nd∑\nk=1\nσ2\nk. (44)\nThen, we have ∥f(x)∥≈\n√∑d\nk=1 σ2\nk.\nTo summarize,\n∥f(x)∥\n∥x∥ ≈\n√∑d\nk=1 σ2\nk√\nd\n. (45)\nC.3 Results for other models\nTable 8 shows the expected expansion rate of f for\neach model.\nD Relationship between word frequency\nand mixing ratio in other settings\nWe also conducted the experiment shown in Sec-\ntion 5.2 with the pre-trained BERT-large, BERT-\nmedium, BERT-small, BERT-mini, and BERT-tiny.\nHowever, we didn’t do for RoBERTa-large and\nRoBERTa-base due to the difﬁculty of reproducing\nthe pre-training dataset to count the word frequency.\nTable 9 lists the Spearman’s rank correlationρbe-\ntween the frequency rank and the mixing ratio for\neach model. We discussed the inconsistency of the\nresults across different model sizes in Section 5.2.\nWe also conducted it with the other three\ndatasets. Table 10 lists the Spearman’s rank corre-\nlation ρbetween the frequency rank and the mixing\nratio for each dataset.\nFurthermore, we conducted with 25 BERT-base\nmodels trained with different seeds. Table 11 lists\nthe Spearman’s rank correlationρbetween the fre-\nquency rank and the mixing ratio for the models on\nthe Wikipedia dataset.\nModels Hidden dim. #Layer #Head\nBERT-large 1026 24 16\nBERT-base 768 12 12\nBERT-medium 512 8 8\nBERT-small 512 4 8\nBERT-mini 256 4 4\nBERT-tiny 128 2 2\nRoBERTa-large 1026 24 16\nRoBERTa-base 768 12 12\nTable 4: Architecture hyperparameters of each model.\nMethods Mean Max Min\n— BERT-large —\nATTN -W 97.4 100.0 15.0\nATTN -N 87.0 100.0 5.6\nATTN RES-W 48.7 50.0 7.5\nATTN RES-N 19.1 87.4 1.8\nATTN RESLN-N 14.9 86.6 1.6\n— BERT-base —\nATTN -W 97.1 100.0 45.0\nATTN -N 85.2 100.0 4.9\nATTN RES-W 48.6 50.0 22.5\nATTN RES-N 22.3 65.7 2.0\nATTN RESLN-N 18.8 61.3 1.3\n— BERT-medium —\nATTN -W 95.6 100.0 49.5\nATTN -N 83.4 99.9 9.7\nATTN RES-W 47.8 50.0 24.8\nATTN RES-N 20.9 49.2 3.8\nATTN RESLN-N 18.7 65.2 1.2\n— BERT-small —\nATTN -W 96.2 100.0 57.7\nATTN -N 85.3 100.0 10.3\nATTN RES-W 48.1 50.0 28.9\nATTN RES-N 29.6 80.4 6.7\nATTN RESLN-N 27.2 85.5 7.3\n— BERT-mini —\nATTN -W 95.5 100.0 50.9\nATTN -N 85.7 100.0 10.4\nATTN RES-W 47.8 50.0 25.4\nATTN RES-N 27.2 68.1 7.3\nATTN RESLN-N 26.4 70.7 6.6\n— BERT-tiny —\nATTN -W 94.1 99.9 38.6\nATTN -N 90.4 99.9 28.3\nATTN RES-W 47.1 50.0 19.3\nATTN RES-N 37.8 77.9 18.1\nATTN RESLN-N 37.3 70.4 17.6\n— RoBERTa-large —\nATTN -W 96.7 100.0 10.1\nATTN -N 87.8 99.9 15.2\nATTN RES-W 48.4 50.0 5.0\nATTN RES-N 19.8 87.8 4.3\nATTN RESLN 19.7 87.9 4.3\n— RoBERTa-base —\nATTN -W 95.8 100.0 3.8\nATTN -N 84.4 100.0 13.8\nATTN RES-W 47.9 50.0 1.9\nATTN RES-N 19.6 69.9 1.8\nATTN RESLN-N 16.2 73.4 1.5\nTable 5: Mean, maximum, and minimum values of the\nmixing ratio in seven variants of the masked language\nmodels, computed with each method.\n4562\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 5: Mixing ratio at each layer of BERT-large calculated from each method.\nMethods Mean Max Min\n— Wikipedia —\nATTN -W 97.1 100.0 45.0\nATTN -N 85.2 100.0 4.9\nATTN RES-W 48.6 50.0 22.5\nATTN RES-N 22.3 65.7 2.0\nATTN RESLN-N 18.8 61.3 1.3\n— SST-2 —\nATTN -W 92.5 100.0 2.2\nATTN -N 80.3 99.8 6.7\nATTN RES-W 46.3 50.0 1.1\nATTN RES-N 22.5 50.4 2.4\nATTN RESLN-N 18.5 44.9 1.1\n— MNLI —\nATTN -W 94.6 100.0 10.0\nATTN -N 83.5 99.9 6.8\nATTN RES-W 47.3 50.0 5.0\nATTN RES-N 22.4 65.4 2.8\nATTN RESLN-N 18.3 60.7 1.2\n— CoNLL’03 NER —\nATTN -W 91.7 100.0 1.5\nATTN -N 79.0 99.9 7.0\nATTN RES-W 45.8 50.0 0.8\nATTN RES-N 22.4 51.5 2.7\nATTN RESLN-N 18.6 45.8 0.8\nTable 6: Mean, maximum, and minimum values of the\nmixing ratio in each method for BERT-base on each\ndata.\nMethods Mean (SD) Max Min\nATTN -W 96.1 (0.1) 100.0 8.8\nATTN -N 85.2 (0.4) 100.0 7.4\nATTN RES-W 48.1 (0.1) 50.0 4.4\nATTN RES-N 21.9 (0.3) 64.6 3.4\nATTN RESLN-N 17.5 (0.4) 67.7 1.4\nTable 7: Mean, maximum, and minimum values of the\nmixing ratio in each method for 25 BERT-base mod-\nels trained with different random seeds by Sellam et al.\n(2021). Mean value is the average of the values from 25\nmodels, and the standard deviation (SD) is also listed.\nMaximum and minimum values are the maximum and\nminimum of these values from 25 models, respectively.\n4563\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 6: Mixing ratio at each layer of BERT-medium calculated from each method.\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 7: Mixing ratio at each layer of BERT-small calculated from each method.\nModels Mean (SD) Min Max\nBERT-large 0.80 0.61 1.08\nBERT-base 0.88 0.63 1.05\nMultiBERTs (base) 0.88 (0.01) 0.65 1.43\nRoBERTa-large 0.94 0.67 1.09\nBERT-medium 0.87 0.60 1.41\nBERT-small 1.31 0.78 2.41\nBERT-mini 1.24 0.65 2.48\nBERT-tiny 1.86 1.63 2.09\nRoBERTa-base 1.30 1.10 1.49\nTable 8: Mean, maximum, and minimum values of the\nscaling magniﬁcation in each layer for nine variants\nof the masked language models. In the “MultiBERTs\n(base)”, results for 25 BERT-base models trained with\ndifferent random seeds by Sellam et al. (2021) are re-\nported. Mean value is the average of the values from 25\nmodels, and the standard deviation (SD) is also listed.\nMaximum and minimum values are the maximum and\nminimum of these values from 25 models, respectively.\n4564\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 8: Mixing ratio at each layer of BERT-mini calculated from each method.\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 9: Mixing ratio at each layer of BERT-tiny calculated from each method.\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 10: Mixing ratio at each layer of RoBERTa-base calculated from each method.\n4565\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 11: Mixing ratio at each layer of RoBERTa-large calculated from each method.\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 12: Mixing ratio at each layer of BERT-base calculated from each method on the SST-2.\n4566\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 13: Mixing ratio at each layer of BERT-base calculated from each method on the MNLI.\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 14: Mixing ratio at each layer of BERT-base calculated from each method on the CoNLL’03 NER.\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 15: Mixing ratio at each layer of BERT-base trained with 0th seed.\n4567\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 16: Mixing ratio at each layer of BERT-base trained with 10th seed.\n(a) ATTN -W.\n (b) ATTN -N\n(Kobayashi et al.,\n2020).\n(c) ATTN RES-W\n(Abnar and Zuidema,\n2020).\n(d) ATTN RES-N.\n (e) ATTN RESLN-N.\nFigure 17: Mixing ratio at each layer of BERT-base trained with 20th seed.\n4568\nMethods Spearman’sρ\nall tokens w/o special tokens\n— BERT-large —\nATTN -W 0.44 0 .44\nATTN -N −0.53 −0.56\nATTN RES-W 0.44 0 .44\nATTN RES-N −0.83 −0.84\nATTN RESLN-N −0.71 −0.75\n— BERT-base —\nATTN -W 0.16 0 .14\nATTN -N −0.39 −0.41\nATTN RES-W 0.16 0 .14\nATTN RES-N −0.84 −0.86\nATTN RESLN-N −0.54 −0.58\n— BERT-medium —\nATTN -W −0.09 −0.11\nATTN -N −0.13 −0.14\nATTN RES-W −0.09 −0.11\nATTN RES-N −0.41 −0.43\nATTN RESLN-N −0.02 −0.03\n— BERT-small —\nATTN -W −0.05 −0.07\nATTN -N 0.26 0 .26\nATTN RES-W −0.05 −0.07\nATTN RES-N −0.22 −0.20\nATTN RESLN-N 0.19 0 .21\n— BERT-mini —\nATTN -W −0.52 −0.55\nATTN -N −0.15 −0.17\nATTN RES-W −0.52 −0.55\nATTN RES-N 0.23 0 .25\nATTN RESLN-N 0.42 0 .44\n— BERT-tiny —\nATTN -W −0.75 −0.77\nATTN -N −0.62 −0.64\nATTN RES-W −0.75 −0.77\nATTN RES-N 0.26 0 .27\nATTN RESLN-N 0.24 0 .25\nTable 9: The Spearman’sρbetween the frequency rank\nand the mixing ratio calculated by each method for ﬁve\nvariants of pre-trained BERT. In the “w/o special to-\nkens” setting, it was calculated without [CLS] and\n[SEP].\nMethods Spearman’sρ\nall tokens w/o special tokens\n— Wikipedia —\nATTN -W 0.16 0 .14\nATTN -N −0.39 −0.41\nATTN RES-W 0.16 0 .14\nATTN RES-N −0.84 −0.86\nATTN RESLN-N −0.54 −0.58\n— SST-2 —\nATTN -W 0.22 0 .19\nATTN -N −0.24 −0.33\nATTN RES-W 0.22 0 .19\nATTN RES-N −0.81 −0.84\nATTN RESLN-N −0.42 −0.54\n— MNLI —\nATTN -W 0.22 0 .19\nATTN -N −0.31 −0.40\nATTN RES-W 0.22 0 .19\nATTN RES-N −0.77 −0.84\nATTN RESLN-N −0.40 −0.50\n— NER —\nATTN -W 0.16 0 .09\nATTN -N −0.22 −0.34\nATTN RES-W 0.16 0 .09\nATTN RES-N −0.79 −0.85\nATTN RESLN-N −0.41 −0.57\nTable 10: The Spearman’s ρ between the frequency\nrank and the mixing ratio calculated by each method\nfor the four variants of datasets. In the “w/o special\ntokens” setting, it was calculated without [CLS] and\n[SEP].\nMethods Spearman’sρ\nall tokens (SD) w/o special tokens (SD)\nATTN -W 0.35 (0.01) 0.35 (0.07)\nATTN -N −0.23 (0.01) −0.25 (0.09)\nATTN RES-W 0.35 (0.01) 0.35 (0.07)\nATTN RES-N −0.79 (0.02) −0.80 (0.02)\nATTN RESLN-N −0.36 (0.10) −0.38 (0.11)\nTable 11: Spearman’s ρ between the frequency rank\nand the mixing ratio calculated by each method for for\n25 BERT-base models trained with different random\nseeds. In the “w/o special tokens” setting, it was cal-\nculated without [CLS] and [SEP]. Both of the val-\nues are the mean of the values from 25 models, and the\nstandard deviation (SD) is also listed.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7111387252807617
    },
    {
      "name": "Computer science",
      "score": 0.703243613243103
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.5897260308265686
    },
    {
      "name": "Residual",
      "score": 0.5710696578025818
    },
    {
      "name": "Architecture",
      "score": 0.5218110084533691
    },
    {
      "name": "Security token",
      "score": 0.4926455616950989
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3674471378326416
    },
    {
      "name": "Natural language processing",
      "score": 0.3248811960220337
    },
    {
      "name": "Voltage",
      "score": 0.17373403906822205
    },
    {
      "name": "Engineering",
      "score": 0.14707764983177185
    },
    {
      "name": "Computer network",
      "score": 0.14551758766174316
    },
    {
      "name": "Algorithm",
      "score": 0.1422353982925415
    },
    {
      "name": "Electrical engineering",
      "score": 0.11816364526748657
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ]
}