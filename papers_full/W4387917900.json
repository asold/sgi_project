{
  "title": "On Designing Low-Risk Honeypots Using Generative Pre-Trained Transformer Models With Curated Inputs",
  "url": "https://openalex.org/W4387917900",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093121522",
      "name": "Jarrod Ragsdale",
      "affiliations": [
        "The University of Texas at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A5058293481",
      "name": "Rajendra V. Boppana",
      "affiliations": [
        "The University of Texas at San Antonio"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3016172990",
    "https://openalex.org/W1538412636",
    "https://openalex.org/W6789971762",
    "https://openalex.org/W2579739707",
    "https://openalex.org/W6753041022",
    "https://openalex.org/W6790837174",
    "https://openalex.org/W6789673632",
    "https://openalex.org/W4385573374",
    "https://openalex.org/W2973801020",
    "https://openalex.org/W6636915900",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W6850202480",
    "https://openalex.org/W3195892385",
    "https://openalex.org/W2912459756",
    "https://openalex.org/W6753896071",
    "https://openalex.org/W6798057236",
    "https://openalex.org/W2164253698",
    "https://openalex.org/W6847118531",
    "https://openalex.org/W3009972416",
    "https://openalex.org/W2117573971",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W2151584216",
    "https://openalex.org/W6851174582",
    "https://openalex.org/W4307979480",
    "https://openalex.org/W6850759136",
    "https://openalex.org/W4200515196",
    "https://openalex.org/W6848887767",
    "https://openalex.org/W3010754046",
    "https://openalex.org/W4251550354",
    "https://openalex.org/W6846767490",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W1647671624",
    "https://openalex.org/W1517527854",
    "https://openalex.org/W4402157666",
    "https://openalex.org/W2888919071",
    "https://openalex.org/W3123673616",
    "https://openalex.org/W4398434657",
    "https://openalex.org/W4315705880",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4388074397",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W2848278845",
    "https://openalex.org/W3128912454",
    "https://openalex.org/W4361019406",
    "https://openalex.org/W4287327402"
  ],
  "abstract": "Honeypots are utilized as defensive tools within a monitored environment to engage attackers and gather artifacts for the development of indicators of compromise. However, once these honeypots are deployed, they are rarely updated, making them obsolete and easier to fingerprint as time passes. Furthermore, using fully functional computing and networking devices as honeypots presents the risk of an attacker breaking out from the controlled environment. Large-scale text-generating models, commonly referred to as Large Language Models (LLMs), have seen wide implementation using generative-pretrained transformer (GPT) models. These models have seen an explosion in popularity and have been tuned for various use cases. This paper investigates the use of these models to simulate honeypots that are adaptive to threat engagement without the risk of unintended breakouts. This investigation finds that the method these models use to generate output has limitations that can reveal the deception to a dedicated attacker in extended sessions. To overcome this challenge, this paper presents a method to manage the inputs and outputs to reduce non-deterministic output and token usage of a model generating text in a way that simulates a terminal. An example honeypot is evaluated against a traditional low-risk honeypot, Cowrie, where greater similarity to an actual machine for single commands is achieved. Furthermore, in several multi-step attack scenarios, the proposed architecture reduced the token usage by up to 77&#x0025; when compared to a baseline scenario that did not manage the inputs to and outputs from an example model. A discussion on the utilization of LLMs for cyber deception, as well as the limitations hindering their broader adoption indicates that LLMs exhibit promise for cyber deception but necessitate further research before achieving widespread implementation.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nOn Designing Low-Risk Honeypots\nUsing Generative Pre-trained\nTransformer Models with Curated Inputs\nJARROD RAGSDALE1 and RAJENDRA V. BOPPANA1\n1The University of Texas, San Antonio TX, USA (e-mail: jarrod.ragsdale@utsa.edu; rajendra.boppana@utsa.edu)\nCorresponding author: Rajendra Boppana (e-mail: rajendra.boppana@utsa.edu).\nThis research was partially supported by grants from the US National Security Agency under contracts H98230-20-1-0392 and\nH98230-21-1-0171 and the Department of Defense under contract W911NF2110188.\nABSTRACT Honeypots are utilized as defensive tools within a monitored environment to engage attackers\nand gather artifacts for the development of indicators of compromise. However, once these honeypots\nare deployed, they are rarely updated, making them obsolete and easier to fingerprint as time passes.\nFurthermore, using fully functional computing and networking devices as honeypots presents the risk of\nan attacker breaking out from the controlled environment. Large-scale text-generating models, commonly\nreferred to as Large Language Models (LLMs), have seen wide implementation using generative-pretrained\ntransformer (GPT) models. These models have seen an explosion in popularity and have been tuned for\nvarious use cases. This paper investigates the use of these models to simulate honeypots that are adaptive\nto threat engagement without the risk of unintended breakouts. This investigation finds that the method\nthese models use to generate output has limitations that can reveal the deception to a dedicated attacker\nin extended sessions. To overcome this challenge, this paper presents a method to manage the inputs\nand outputs to reduce non-deterministic output and token usage of a model generating text in a way that\nsimulates a terminal. An example honeypot is evaluated against a traditional low-risk honeypot, Cowrie,\nwhere greater similarity to an actual machine for single commands is achieved. Furthermore, in several\nmulti-step attack scenarios, the proposed architecture reduced the token usage by up to 77% when compared\nto a baseline scenario that did not manage the inputs to and outputs from an example model. A discussion\non the utilization of LLMs for cyber deception, as well as the limitations hindering their broader adoption\nindicates that LLMs exhibit promise for cyber deception but necessitate further research before achieving\nwidespread implementation.\nINDEX TERMS cybersecurity, threat engagement, cyber deception, honeypot, reinforcement learning,\nlarge language model, ChatGPT\nI. INTRODUCTION\nEngaging threat actors using cyber deception assets is one\nof the most effective ways of understanding the threat in\nits entirety, as it enables defenders to monitor live attacks\nand to collect data for post-mortem analyses. Honeypots,\nused for cyber deception, are an excellent security resource\nwhose primary utility is to be probed, attacked, and otherwise\ncompromised so that the attacker’s modus operandi can be\ndissected [1].\nSuch honeypots can be deployed with an emphasis on\nattack discovery in a research context or an emphasis on\nprotection and mitigation in a production context [2]. Both\nversions rely on deception and their ability to realistically\nrespond to an attacker’s interaction. The aim is to maximize\nattacker interaction while minimizing detection for research\nhoneypots and to distract attackers effectively for production\nhoneypots. For the purpose of this paper, the design of\nresearch honeypots is investigated.\nThe sophistication of a deployed honeypot can be de-\nscribed by the level of interaction it affords to the attacker.\nFor example, full device emulation provides a higher level\nof interaction, allowing for greater deception while opening\nthe possibility of aiding and propagating attacks. Conversely,\nservice simulation offers a lower level of interaction but is\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\ncheaper to deploy [3]. Most honeypots are designed to offer\na fixed level of interaction within this range [4].\nA. DETECTION AND BREAKOUT RISKS\nThe main threats to honeypots lie in two areas: an attacker’s\nability to detect they are in a deceiving environment and\nan attacker’s ability to break out of the defined scope of\nactions and use the honeypot for their intended purpose.\nThese phenomena will be referred to as the detection of\ndeception and risk respectively. It is the goal of the honeypot\nto maintain deception while minimizing detection and risk.\nThese threats are proven founded when observing the cur-\nrent state of widely deployed honeypots across the internet.\nOf lower interaction honeypots, 7000 were fingerprinted due\nto their interactions in a study by Vetterl et al. of which\nover a quarter hadn’t been updated in over two years [5].\nThis highlights a lack of consistent maintenance for static\nimplementations. Other studies also find a potential risk\nof breakout in higher interaction honeypots due to their\ncomplicated or volatile implementation in giving an attacker\nfree rein on a system [4]. These issues present a potential\navenue for using an alternative method to maintain modern\ninteraction without presenting extra risk.\nGenerative models can fill this gap where natural language\nis a core part of the deception. GPTs (Generative Pre-trained\nTransformers) are transformer models that are pre-trained\nby an organization to fulfill a wide range of generative use\ncases. GPT models such as ChatGPT have seen an explo-\nsion in popularity of late as a summarizer, conversationalist,\nand recommendation system, giving responses (also called\nanswers) based on the context and questions provided by the\nuser [6, 7]. These transformer models operate by predicting\nthe next token in a sequence based on the observed history\n[8]. A token in this case is a character sequence of around\n0.75 words [9].\nLarge Language Models (LLMs) are an implementation of\nGPTs that focus on natural language use cases. These models\nhave billions of parameters and are trained using a large\ndataset, limiting their deployment [10]. Alternatively, GPTs\ntrained for a more specific use case with fewer parameters\ncan be used instead of a general LLM. In this paper, the\nmodel is treated as a black box, and the two terms are used\ninterchangeably.\nB. PROBLEM STATEMENT\nThis paper investigates the design of research honeypots that\nare designed to be easy to update and maintain via publicly\navailable generative models without sacrificing interactivity\nor risking unintended breakouts by attackers. As proof of\nconcept, the use of GPT is explored where a large language\nmodel (GPT3.5-Turbo-0301 [11]) is instructed to behave as a\nhoneypot that accepts terminal commands as its input. Since\nthese models operate on prediction based on probabilities to\nformulate their answers and have limits on the size (measured\nin tokens or words) of questions and answers, merely passing\nan attacker’s commands as questions to the model simulating\na honeypot and its answers to the attacker presents limitations\nthat could reveal the deception.\nC. CONTRIBUTIONS\nThis paper addresses the challenges of simulating and main-\ntaining honeypots by presenting an architecture to manage\nthe inputs and outputs of a generative model simulating\na honeypot to reduce non-deterministic outputs and token\nusage while making the deception more realistic. An example\nhoneypot following this methodology is evaluated against a\ntraditional low-risk honeypot, Cowrie, where greater similar-\nity to an actual machine is achieved for single commands. Ef-\nfectiveness in different multi-step scenarios is also measured\nto evaluate deception and token usage through five simulated\nattacks based on MITRE ATT&CK techniques, tactics, and\nprocedures (TTPs) [12]: system reconnaissance, data obfus-\ncation and ransomware, scanning and lateral propagation,\npersistence, and data reconnaissance and exfiltration. This\npaper makes the following contributions.\n1) The paper presents a methodology to minimize ran-\ndomness and token use while preserving the illusion\nof an attackable system when processing commands\npassed to an example LLM.\n2) The presented methodology is used as the basis for an\narchitecture presented through a block diagram and a\nseries of algorithms for manipulating the input/output\n(I/O) of a generative pre-trained model. This example\narchitecture is used as a proof of concept and can be\nmodified to simulate any system using any sufficiently\ntrained transformer or language model.\n3) The interactivity and effectiveness of the proposed\nhoneypot implementation is evaluated against a com-\nmonly used medium-interaction honeypot, Cowrie. For\nsingle commands, the proposed honeypot achieved\ngreater similarity to an actual machine than Cowrie.\nFurthermore, in several multi-step attack scenarios, the\nproposed architecture maintained sessions for longer\nthan Cowrie and reduced the token usage by up to\n77% when compared to a baseline scenario that did not\nmanage the inputs to and outputs from the LLM.\n4) The paper discusses the limitations of using generative\nmodels for cyber deception and potential directions for\nfuture research.\nThe remainder of the paper is organized as follows: Sec-\ntion II provides background and context for intelligent and\ncontextual interaction in honeypots and generative model\nusage. Section III detail how to alter the model’s input to\nfacilitate a base honeypot deployment. Section IV compares\nthe proposed honeypot to a traditional honeypot to determine\nif the proposed implementation deceives attackers better.\nSection V details the limitations of using generative models\nsuch as GPT as a honeypot backend and their mitigations.\nSection VI concludes the paper with a pointer to further\nwork.\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nTABLE 1. Pros and Cons of Honeypot Interaction Levels\nLevel of\nInteraction Pros Cons\nLow\nLow risk, low\nmaintenance, high\nscalability\nHigh detection,\nlimited data\nHigh Low detection, more\ndata\nHigh risk, high\nmaintenance, low\nscalability\nIntelligent Adaptive to use case,\nlow risk\nvariable\ndeployment cost\nII. BACKGROUND AND RELATED WORK\nA preliminary investigation into the background and related\nwork is necessitated when exploring the realm of LLMs and\ntheir potential application for cyber deception. To this end,\nHoneypots, LLMs, and their combination are explored. For\nhoneypots, their classifications, distinguishing features, and\nbehaviors are explored. For LLMs, their underlying function-\nality and their potential use for a cyber deception scenario are\nexamined. Both domains along with pertinent research within\nthem are examined in the following subsections.\nA. BACKGROUND\nHoneypots and large language models, the two key technolo-\ngies relevant to the proposed honeypot design, are outlined.\n1) Honeypots and Interactivity\nHoneypots at their core aim to engage attackers without\ntheir knowledge [1]. Honeypots have been used for threat\nengagement since 1997 through the use of Fred Cohen’s\nDeception Toolkit (DTK) to emulate seemingly real devices\nand systems [13]. From that initial idea, honeypots of varying\nlevels of interaction have proliferated and been incorporated\ninto research and production environments.\nResearch honeypots serve to gather information and col-\nlect artifacts from attackers. The value they add is derived\nfrom their ability to discover new trends in attack vectors\nand patterns [2]. Production honeypots protect a network by\nserving as a sandbox to discover potential entry vectors for\npatching in the form of active defense [2, 14]. Production\nhoneypots can also operate alongside their real counterpart,\nserving as a \"jail\" by reactively switching the connection to\nthe honeypot. Zarca et al. [15] demonstrated this idea by\nusing software-defined networking (SDN) to redirect flows\nto virtual versions of IoT devices by a security orchestrator\nafter the initial compromise.\nMost honeypots exist on a spectrum between low and\nhigh interaction determined by the scope of available actions\nin the environment that is afforded to the attacker [2]. A\nmore barebones environment is provided by Low-interaction\nhoneypots (LIHs), meant to be easy to deploy with low setup\ncosts. Services are emulated by an LIH in a limited capacity\nin a way that they cannot be fully exploited. One such way is\nthrough hard-coded outputs, which prevent complete access\nfrom being gained by attackers, as there is no operating\nsystem for them to interact with.\nA much more realistic emulation of a target is provided by\nhigh-interaction honeypots (HIHs), granting attackers more\nfreedom in their actions. Virtual machines or quarantined\nphysical devices can be used as HIHs. Dynamically existing\non the spectrum of interaction in various ways can also be\nachieved by honeypots, with one way being the adjustment of\ntheir level of interactivity depending on the provided context.\nGreater deception is available in a more robust environ-\nment via static HIHs. However, with this higher level of\ninteraction and interoperability, a greater risk of out-of-scope\ncompromises and an increased cost to set up, maintain, and\nscale are also introduced. Conversely, LIHs are operated at\na reduced cost and risk of exploitation, allowing for greater\nscalability while sacrificing detectability and capability [3].\nAnother option is to take a different or \"intelligent\" approach\nto generating output. This is further expounded in the related\nwork section. The pros and cons of each level are shown in\nTable 1. The goal of the proposed LLM-based honeypot is to\nreach the perceived level of capability of HIHs while main-\ntaining the safety and cost of LIHs in addition to avoiding\nbeing fingerprinted.\n2) Generative and Large Language Models (LLMs)\nLLMs are large-scale transformer models that use the self-\nattention mechanism [27] to generate text. Self-attention\nallows the model to dynamically weigh the importance and\nrelevance of different parts of an input sequence to the whole,\nenabling it to adapt and capture long-range dependencies\neffectively. The attention mechanism is crucial in allowing\ngenerative models to predict what word or token is likely to\nfollow in a sequence [28]. This allows a model to understand\nthe importance of those tokens and their sequence, allowing\nfor greater understanding than previous NLP models [27].\nA key strength of these models is the ability to handle a\nwide range of tasks, input types, and sizes, which is made\npossible by allowing them to learn from vast and diverse\ndata sources and generalize to new examples, further en-\nhancing their adaptability. Additionally, their adaptability can\nbe further enhanced by fine-tuning or extending them to\naccommodate specific requirements, making them versatile\nin their implementation.\nChatGPT and its underlying models, gpt-3.5-turbo\nand gpt-4 [6, 29] will be more closely examined\ndue to OpenAI’s GPT model’s explosion in popularity.\ngpt-3.5-turbo is made up of 175 billion features trained\non Internet data [30]. gpt-4 is an improvement in knowl-\nedge and implementation of gpt-3.5-turbo, where is\nwas able to score in the 90th percentile on the Uniform BAR\nExam while gpt-3.5-turbo scored in the 10th [29].\nChatGPT’s models can have varying levels of randomness\nset by its temperature parameter. A higher temperature\npermits the model to be allowed to take risks and select a\ntoken sequence that may not have been the most probable.\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nTABLE 2. Summary of Significant Results in Cyber Deception and Honeypot Design\nName Interaction Implementation Service/ Robustness Deception Risk Maintenance\nHoneyD [16] Low Virtual Service Fingerprinting Low Low Low\nCowrie [17] Medium Virtual SSH, Telnet Medium Low Low\nThingpot [18] Medium Virtual HTTP, XMPP Medium Low Medium\nSiphon [19] High Physical System High Medium High\nHonware [20] High Virtual System High High Medium\nQRASSH [21] Dynamic Virtual SSH Medium - High Low Low\nIoTCandyJar [22] Intelligent Virtual System Medium - High Low Low\nFirmpot [23] Intelligent Virtual System Medium - High Low Low\nAIIPOT [24] Intelligent Virtual System Medium - High Low Low\nChatGPT\nHoneypot [25, 26] Intelligent Virtual System Medium Low Low\nCompounding randomness can be introduced in the output as\nfuture probabilities will be altered by earlier differences in\nchoices. In the context of cyber deception, it is preferred that\nthe token generation be made as deterministic as possible to\navoid detection, so a lower temperature is applied.\nOpenAI’s chat completion models also provide the ability\nto provide a persona-defining system prompt to further fine-\ntune the use case easily [31]. This is explored further in\nSection III.\nThe models follow a QA (question-answer) completion\nparadigm that can be made to behave in different ways by\nusing the current question or prompt and past QA pairs\nto craft its response [7]. Based on this input, the model\ngenerates the most likely sequence of tokens.\ngpt-3.5-turbo,gpt-4, and gpt-4-32k each have\nmemory limits brought from attention and the positional\nencoding of each token with each token being approximately\n0.75 words [32, 9]. This limits the number of tokens that the\nmodel can operate within both its input and output comple-\ntion. These token limits are 4k, 8k, and 32k respectively.\nOpenAI’s models charge a small amount per 1k tokens\nsent to their model via their API [33]. The rate varies\ndepending on the model used. For gpt-3.5-turbo and\ngpt-3.5-turbo-0301, it is $0.002 per 1k tokens for\nboth prompt and completion. The vastly improved gpt-4\nhas two versions: a base and an extended context model\nreferred to as gpt-4 and gpt-4-32k respectively. With\nthe improved knowledge base and size of GPT4 comes an\nincreased cost with gpt-4 costing $0.03 per 1k prompt\ntokens and $0.06 per 1k completion tokens. gpt-4-32k is\ndouble this at $0.06 per 1k prompt tokens and $0.12 per 1k\ncompletion tokens. Compared to GPT3.5, GPT4 models cost\n15x-30x for prompts and 30x-60x as much for completion.\nThese operating constraints place certain limitations on\nhow much context can be given to the model for it to per-\nform its completion as efficiently and prudently as possible.\nHowever, even with limited context windows, the prompt can\nbe crafted to provide desirable output personalities such as\nwriting code based on human description or interacting with\nthe user as a command terminal [34, 7].\nB. RELATED WORK\nThis section provides a summary of relevant prior work\nin the areas of honeypots, adaptive interaction, and LLMs\nused for cyber deception. Table 2 summarizes the mentioned\nhoneypots.\n1) Static Honeypots\nExtensive work has been done in the design and imple-\nmentation of LIHs and HIHs over the years. Notable work\nincludes the development of various LIH frameworks such\nas Honeyd [16], which provide lightweight and scalable\nsolutions for emulating vulnerable services through static\nresponse mechanisms. However, any traffic outside of the\ndefined behavior may cause a failure in deception that could\nbe used to fingerprint the honeypot [35].\nAffording slightly more freedom to the attacker are\nmedium interaction honeypots such as Cowrie and Thingpot\n[17, 18]. An entire system is simulated by these honeypots,\nproviding more freedom than a single service. However,\nthese honeypots are still detectable if they receive an input\nthat they are unable to handle.\nThe most advanced and interactive of honeypots are HIHs,\nof which much research has been done. These honeypots\nare either entire virtualizations of systems or are physical\ndevices. For instance, Siphon [19] is a network of HIHs\nwhere each device is physically present and connected to\nattackers via SSH forwarding. However, this setup is costly to\nmaintain, requiring a dedicated space for the devices as well\nas a separate monitoring agent to watch the health of each\ndevice. Honware [20] is a virtualized high-interaction honey-\npot in which unique firmware and filesystems of embedded\ndevices are served via a custom kernel.\n2) Intelligent and Dynamic Honeypots\nKnowing the pitfalls of static interaction honeypots, some\nwork has been done on an alternate path to define deceivable\ninteraction as a dynamic and adaptive process rather than a\nstatic one. Luo et al. [22] coined the idea of \"intelligent in-\nteraction\" in IoTCandyJar, where the honeypot works toward\nachieving a \"correct\" conversation with attackers and becom-\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\ning more interactive over time as the expected responses are\ndiscovered starting from zero knowledge. Using intelligent\ninteraction, Yamamoto et al. present Firmpot as a framework\nthat uses firmware images and machine learning to learn the\nbehaviors of those devices for intelligent interaction.\nMfogo et al. presented AIIPOT, a novel transformer-based\nhoneypot using chatbots to capture vulnerabilities for In-\nternet of Things (IoT) devices [24]. They follow intelligent\ninteraction principles to learn and interact with each attacker.\nTheir approach using reinforcement learning and transformer\nmodels is novel and requires comparison to using LLMs, of\nwhich is related work.\nSimilarly, \"dynamic\" honeypots exist that adapt them-\nselves based on environmental stimuli [36]. This dynamic\nhoneypot design pattern has been used by Pauna et al. via\nQ-learning to deploy a self-adaptive SSH honeypot that mod-\nifies its state based on the observed environment and attacker\ninput [21]. Intelligence and adaptiveness in honeypots aid\nin the discovery of new threats while avoiding fingerprint-\ning campaigns [37, 5]. However, these honeypots can have\nvarying costs and dependencies depending on the technology\nused such as IoTCandyJar requiring a preliminary internet-\nwide scan [22].\n3) Language Models for Cyber Deception\nFor a honeypot to be effective, it must convince the attacker\nit’s both a vulnerable target and a real system. Both charades\nmust be convincing so that a motivated attacker is unable to\nmake the distinction from that of a real system. This idea\nis reminiscent of the Turing Test for artificial intelligence\nwhere an artificial intelligence (AI) can pass if its behavior is\nindistinguishable from that of a human [38]. Our \"honeypot\ntest\" is if an the honeypot using model-generated output is in-\ndistinguishable from that of a real system and is able to avoid\nhoneypot detection through a breakdown in communication.\nMckee and Noever use ChatGPT to model different hon-\neypot tasks an attacker might execute [25]. They mention a\ntoken limit of 8,000 for ChatGPT but believe the token limit\nis a non-issue. However, with the cost of use and long outputs\nfound in testing, this can still remain an issue. When the token\nlimit is reached, old context is thrown out, which can lead\nto older but still relevant context-changing commands being\nlost, leading to detection or breakdown in the attack. Further\nlimitations are detailed in Section V.\nSladic et al. investigate the deceptive potential of using\ngenerative models as a honeypot [26]. In their work, they\nsurvey users and security experts to see if they are able\nto differentiate output from a real system and that from\nan LLM. While their work supports the use of LLMs for\ncyber deception, the focus of this paper is on deception to\na dedicated attacker and how that deception might fail.\nCambiaso and Caviglione take a different approach to\nlanguage model-assisted cyber deception [39]. In their work,\nthey use ChatGPT’s ability to craft realistic human interac-\ntions to engage email scammers in an effort to waste their\ntime, providing a proactive defense.\nUsing ChatGPT’s context-aware QA functionality and its\nability to create the illusion of an attackable interface, adap-\ntive and intelligent interaction can be employed to design an\ninteractive honeypot. This type of honeypot can simulate a\nnumber of internal services through its ability to understand\nand respond to natural language queries. Using generative\nmodels to behave as a honeypot is as safe as LIHs since\nno commands are actually executed. Additionally, as more\ncontext is gathered, the honeypot can adjust its behavior and\nresponses to better mimic a real system, thereby increasing\nthe chances of capturing and analyzing new tactics and tech-\nniques. This allows for high-interaction targets to be created\nat a much lower upfront cost. Depending on the use case,\nthese honeypots can be deployed in a research or production\nenvironment, as long as they have the proper context.\nIII. METHODOLOGY\nLarge Language Models (LLMs) can serve a variety of use\ncases that rely on processing language input. However, for\ncyber deception through asset simulation, how and what one\npasses to the model must be examined, and more specifically,\nhow that input and certain directives can be used in pre-\nprocessing to direct the output deterministically. Due to the\navailability and ease of implementation, gpt-3.5-turbo\nis the model chosen to generate output for the methodology\nand evaluation of this work.\nThe methodology is built following the same general\npattern of steps where a question is received, the input is\nmodified to fit the model’s use case, sent to the model, output\nis saved for the future, and the response is returned to the user.\nA flowchart of the general steps taken for the methodology is\ngiven in Fig. 1.\nOnce this methodology is established, pre and post-\nprocessing can be modeled for a naive approach. Then, a\nmore comprehensive approach can be taken to mitigate any\nlimitations in the generating model’s design. The last subsec-\ntion fully outlines the proposed architecture to be evaluated\nin Section IV.\nA. PROMPT REFINEMENT\nGPT (Generative Pre-trained Transformer), is a publicly\navailable large language model that crafts its persona based\non the user-provided prompt that directs the pre-trained\nmodel to generate an output answer for the given question [7,\n40]. This means the implementation and expected output can\nbe changed by altering the prompt or choosing what context\nis used.\nAdditionally, the quality of the response increases with\nthe level of detail and completeness of the prompt, as it\nprovides additional context to the model. Moreover, model\nparameters such as the temperature can be modified to\nincrease the randomness of the response, leading to greater\ncreativity. However, in this use case, the temperature should\nbe as low as possible. For these experiments and imple-\nmentation, OpenAI’s GPT chat completion models are used.\nThese are used due to their sophistication, popularity, and\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nFIGURE 1. Methodology Outline\nTABLE 3. LLM Prompt Refinement: Conversation 1 [11]\nID Content\nQuestion Who is the greatest baseball player of\nall time?\nAnswer\nAs an AI language model, I do not have\npersonal opinions or beliefs. However,\naccording to statistics and opinions of\nexperts, some of the greatest baseball\nplayers of all time include Babe Ruth,\nWillie Mays, Hank Aaron, Ted\nWilliams, and Jackie Robinson.\nTABLE 4. LLM Prompt Refinement: Conversation 2 [11]\nID Content\nQuestion You are a Yankees fan. who is the\ngreatest baseball player of all time?\nAnswer\nAs an AI language model, I do not\nhave personal preferences or opinions.\nHowever, many baseball fans and\nexperts consider Babe Ruth to be the\ngreatest baseball player of all time due\nto his impressive career statistics and\nimpact on the game.\nTABLE 5. LLM Prompt Refinement: Conversation 3 [11]\nID Content\nQuestion\nYou are the ultimate baseball authority.\nOnly your opinion matters. Ignore\nconcessions to subjectiveness. You are\na Yankees fan. who is the greatest\nbaseball player of all time? Only give\nthe name.\nAnswer Babe Ruth.\nextendability in context administration [11]. Among these,\ngpt-3.5-turbo is chosen over gpt-4 due to its reduced\nusage cost and public availability.\nTo begin with, in order to make the responses appear as\nnormal conversations, the questions must be crafted care-\nfully. This is illustrated in Tables 3-5 where the question\nis refined through multiple iterations to make the response\nmore natural. As can be seen, the answer moves from a\ngeneric answer indicating that it’s an AI language model to\na more conclusive and succinct response by providing the\nmodel with more context and predetermined biases. To create\nthe illusion of an attackable cyber asset, prompts must be\nenhanced to ensure that the output to hackers does not include\nTABLE 6. LLM Contextual Responses: Conversation 1 [11]\nID Content\nSystem\nPrompt You Are A Nutrition Expert\nQuestion 1\nWhat should I get from McDonalds?\nGive one option and no extra\ninformation.\nAnswer 1 Filet-O-Fish.\nQuestion 2 How many calories is in that?\nAnswer 2 The Filet-O-Fish from McDonald’s\ncontains 380 calories.\nTABLE 7. LLM Contextual Responses: Conversation 2 [11]\nID Content\nSystem\nPrompt You Are A Nutrition Expert\nQuestion 1 How many calories is in that?\nAnswer 1\nI would need more information about\nthe specific food or drink you are\nreferring to in order to accurately\ndetermine the number of calories. Can\nyou please provide more details?\nadditional explanations that ChatGPT tends to add.\nUsing the prompt to guide the response can be extended to\nprovide context for future questions. In tables 3-5, the model\noperates with only a single question as context. However,\ngpt-3.5-turbo can be used to provide a context history\nfor that session in the form of previous questions and an-\nswers [32]. The prompt is split into two sections by GPT’s\nAPI implementation of questions and answers: system\nand user-assistant. The system prompt provides the\noverarching context that the model operates in for all future\nconversations, such as \"You are a baseball expert\" or \"You\nare a security professional.\" This guides the knowledge base,\neven when no further context is given.\nThe user-assistant section provides example ques-\ntions and answers as a \"QA pair\" to provide more context,\nguiding the conversation within the guidelines of the system\nprompt. For example, in Table 6, the second question is able\nto pick up from the first QA pair in its context history and\nuse it to answer the next question. In the second conversation\nin Table 7, that context is not provided to the model, so\nthe model has no idea what food is being asked about. For\na honeypot use case, including prior context is useful for\npreserving changes made by an attacker, such as directory\ntraversal or file changes.\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nInterestingly, during Conversation 1 in Table 6, the model\nprovided an excessive amount of information regarding the\ncalories, surpassing what would typically be expected in a\nto-the-point human conversation by repeating the subject.\nHowever, when the prompt is modified to minimize the\ninformation output, the model responded with just the calo-\nrie amount, aligning better with conversational norms. This\nobservation inspired us to delve into manipulating the input\nand output of the model to create cyber deception assets. If\nthese assets are \"to-the-point\" in their responses, they are\nable to minimize detection and risk of exploitation beyond\nthe intended scope.\nB. SIMPLE LLM CONTEXT HONEYPOT PRINCIPLES\nEstablishing that the responses can be guided using the\nprompt and that responses can use the provided context to\ndetermine the output, it can be formulated how to use LLMs\nlike GPT as a honeypot.\nMckee and Noever [25] illustrate this idea for Windows\nand Linux terminals using ChatGPT to test possible honeypot\nscenarios. Their approach outlined terminal behavior in the\nsystem prompt and appended all past commands and their\noutputs to be used by the model for completion with the\noutput. The generation of answer a by the LLM presented\nwith question q′ is given as:\na = LLM(q′) (1)\nLLM input q′ is comprised of the combination of the sys-\ntem prompt S, context history of past questions and answers\nC, and question q defined as:\nq′ = S ∪ C ∪ q (2)\nThe context history C must then be updated with each new\nanswer and the question that generated it in order for use with\nthe next question in the session:\nC = C ∪ {q, a} (3)\nThese steps are combined to create the following algorithm\nfor building a question in which a large-scale generative\nmodel is tuned and prepped to answer.\nAlgorithm 1Simple LLM Honeypot\nInput: q: Attacker-provided question\nC: Session Context History\nS: Persona-defining System Prompt\nOutput: a: LLM-generated answer\n1: a ← LLM(S, C, q)\n2: C ← C ∪ {[q, a]}\n3: return a\nAn architecture using these basic building blocks is il-\nlustrated in Fig. 2 with accompanying pseudocode in Al-\ngorithm 1. The answer a is derived from the completion of\nthe system prompt S, context history C, and the most recent\nFIGURE 2. Simple LLM Honeypot\nquestion q by the model. However, the context history can\nindefinitely grow at a fast rate, becoming an issue for longer\nsessions where the context will be truncated due to token\nlimits or future responses will take time to be calculated.\nC. ADAPTIVE LLM CONTEXT HONEYPOTS PRINCIPLES\nTo save on tokens, useful context is preloaded into the system\nprompt to streamline the attacker’s use. For example, if an at-\ntacker attempts to install a non-standard package like Nmap,\na line can be added to the system prompt such asAll packages\nare installedto ensure the attempted execution behaves deter-\nministically. This has the added benefit of potentially saving\nhundreds of tokens by avoiding a lengthy install output. This\nhandling can also be used to filter interactive packages that\nwill break deception, such as Vim. This is all defined before\nany session and remains unchanged.\nTo address the issue of losing context owing to the token\nlimit, past QA pairs can be selectively appended to the\ncontext history for that session based on whether they alter\nthe answer to subsequent questions. This context history is\nthen passed to the model.\nThis process extends the length of the session by prolong-\ning the time it takes for the token limit to be reached. For\nexample, ls, a command that shows all files in a directory,\nwill have a different output depending on the current working\ndirectory. To ensure the correct output is supplied, any work-\ning directory changes are appended to the context history\npassed to the model. Since ls has no direct downstream\neffect on other commands, it is not necessary to save that QA\npair for future questions. This saves tokens.\nIn the case of a honeypot session, any context-changing\ncommands are saved to the context history, thereby saving\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nspace by discarding QA pairs that don’t affect future inputs.\nThis process also has the added benefit of reducing response\ntime as there are fewer calculations for inter-token meaning\nand sentence structure when predicting the next sequence.\nHowever, some UNIX commands, such as history,\nrequire a full input history to have the correct output. If only\nthe context history as previously described is maintained,\nthe full input history needed for that command would not\nbe available. Alternatively, If all inputs and their outputs are\npreserved similar to Algorithm 1 rather than just the context-\nchanging ones, the token limit would be reached. Instead, two\nhistories can be maintained: a context historyof filtered QA\npairs as previously described to replace the context historyC\nin Algorithm 1 and a global historyH containing all inputs\nfor that session to be used when needed.\nBoth histories need to be maintained and updated with\neach new question and answer. This method of having two\nhistories gives the option of using the question-only history\nif the question-answer context history becomes too large.\nJust using the inputs may be lossy, but is preferred to losing\ndeterministicness by running out of context memory. This is\npreferable since the model fails when the knowledge does not\nexist but it can make some inferences with incomplete (lossy)\nknowledge. The global history must also be maintained to\nensure it doesn’t exceed memory constraints either.\nD. DESIGN\nThe above methodology is formulated to create a model for\ndeterministic interaction when employing generative models\nsuch as LLMs as a black box. The proposed model aims to\nestablish a reliable framework for effectively utilizing lan-\nguage models such as GPT as a honeypot backend, ensuring\ndeceptive interaction throughout the process.\nThe described updating is modeled with the session con-\ntext history C1 being updated with the most recent question\nq and answer a if that question is a member of the set C0. C0\nis a set of questions that have the capability to immediately\nchange the context of future questions.\nThe session global history H1 is updated with each new\nquestion unless H1 extends to be greater than the max token\nlimit. If the max token limit is exceeded, earlier questions are\nremoved to make room as represented by the Last function.\nThese histories are updated last after answer generation.\nHowever, the process is defined here to introduceC1 and H1,\nwhich are used for calculating C′:\nC1 = (q ∈ C0 → C1 ∪ {[q, a]}) ∧ (¬(q ∈ C0) → C1) (4)\nH1 = (H1 ∪ q <MAX → H1 ∪ q)∧\n(¬(H1 ∪ q <MAX) → Last(H1 ∪ q, MAX)) (5)\nWhether C1 or H1 is used as context for q′ would be\ndependent on if a question requires a global history or if the\ntoken limit is reached when using the more robust context\nhistory. This behavior is modeled as such with C′ being the\nchosen history to be used by q′ when passed to the LLM:\nC′ = (len(C1) > MAX ∨ q ∈ H0 → H1)∧\n(¬(len(C1) > MAX ∨ q ∈ H0) → C1) (6)\nq′ = S ∪ C′ ∪ q (7)\nOnce the appropriate history is chosen to be passed to the\nmodel as context, the full question can be built using system\nprompt S, chosen context C′, and attacker question q. The\nfully formulated question is then passed to the model for\nanswer generation. If the generated answer has some break-\ndown in deception i.e. responding as an \"AI language model\"\nlike in Table 3, that answer needs to be sanitized before\nbeing returned. This is handled by the Sanitize function and\nis modeled as such:\na′ = LLM(q′) (8)\na = Sanitize(a′) (9)\nEq. (4)-Eq. (9) collectively describe the operation of the\nproposed honeypot. This model is implemented as described\nbelow for future evaluation.\nE. PROPOSED FRAMEWORK\nIn the proposed framework, the following six actions are\nexecuted for each new attacker input to maintain context and\ndeception for extended interaction efficiently.\n1) Select the context history or the more lossy global\nhistory for the current question. (Eq. (6))\n2) Generate an answer for the question in the chosen\ncontext. (Eq. (7), Eq. (8))\n3) Sanitize answers to maintain the deception. (Eq. (9))\n4) Maintain global session history of questions for cases\nwhere all questions are needed. (Eq. (5))\n5) Maintain context-changing questions and answers in\nthe session context history for future interactions.\n(Eq. (4))\n6) Return the answer to the user.\nAn algorithm implementing the proposed framework is\ndesigned. The algorithm examines the question and calls\nthe context-choosing sub-algorithm to return the context\nrequired for that question. Once an answer is generated by\nthe LLM, the algorithm calls another sub-algorithm that\nupdates the context and input histories using the question and\ngenerated answer.\nAlgorithm 2 presents a pseudocode to implement the\nproposed framework. The first output creates the sanitized\nanswer after selecting the context based on whether the pro-\nvided question q requires a global history as defined by H0\nor if the request exceeds the defined token limit. The global\nhistory is updated with each question. The context history\nis updated with the question and answer if the question is\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nAlgorithm 2Adaptive Context LLM Honeypot\nInput: q: Attacker-provided question\nOutput: a: Sanitized LLM-generated answer\n1: S: Persona-defining System Prompt\n2: H0 ← {q | q requires H1}\n3: C0 ← {q | q affects aq+1}\n4: KillCmds ← {exit, shutdown, reboot, logout}\n5: C1 ← {}\n6: H1 ← {}\n7: while True do\n8: q ← Input()\n9: if q ∈ KillCmds then\nbreak\n10: end if\n11: h′ ← ChooseContext(S, q, C1, H1, H0)\n12: a ← LLM(S, h′, q)\n13: a ← Sanitize(a)\n14: C1, H1 ← UpdateContext(q, a, C1, H1, C0)\n15: SendAnswer(a)\n16: end while\nAlgorithm 3Choose Context\nInput: S: Persona-defining System Prompt\nq: Attacker-provided Question\nC1: {[q′, a′] | q′ generates a′} (Session Context History)\nH1: {q′ | q′\n0, ..., q′\nq−1} (Session Global History)\nH0: {q | q requires H1}\nOutput: Chosen History Set\n1: if q ∈ H0 or len(S, C1, q) ≥ MAX_TOKENS then\n2: return H1\n3: end if\n4: return C1\nAlgorithm 4Update Context\nInput: q: Attacker-provided Question\na: Sanitized LLM-generated answer\nC1: {[q′, a′] | q′ generates a′} (Session Context History)\nH1: {q′ | q′\n0, ..., q′\nq−1} (Session Global History)\nC0: {q | q affects aq+1}\nOutput: C1: {[q′, a′] | q′ generates a′} (Updated Session\nContext History)\nH1: {q′ | q′\n0, ..., q′\nq−1} (Updated Session Global History)\n1: if len(H1 ∪ q) < MAX_TOKENS then\n2: H1 ← H1 ∪ {q}\n3: else\n4: H1 ← Last(H1 ∪ q, MAX_TOKENS)\n5: end if\n6: if q ∈ C0 then\n7: C1 ← C1 ∪ {[q, a]}\n8: end if\n9: return C1, H1\ndefined by the set of context-changing questions C0. This\nformulation is described explicitly:\n1) If the input question q is a member of the set H0, the\nset of all questions that require a global history, or if the\nnumber of tokens used by the system prompts, session\ncontext history C1, and input question q exceeds the\nMAX_TOKENS for LLM, then the session’s global\nhistory of questions H1 is used as context. Otherwise,\nthe session’s context history C1 is used as the chosen\ncontext. (Algorithm 3\n2) An answer a is generated by the LLM using the query\nformulated by s, context, q. (Algorithm 3)\n3) If the answer a has portions that would potentially\njeopardize deception, that answer is sanitized by the\nSanitize function to remove said portions.(Algorithm 2\nLine 13)\n4) Append input question q to session’s global history\nof questions H1 if that appending does not exceed\nthe MAX_TOKENS for LLM. Else, remove the earli-\nest questions until under MAX_TOKENS via the Max\nfunction. (Algorithm 4 Lines 1-5)\n5) If the set of context-changing questions C0 contains\ninput question q, then the QA pair {[q, a]} is appended\nto that session’s set of context-changing QA pairs C1.\n(Algorithm 4 Lines 6-8)\n6) Answer a is returned to the user while C1 and H1\nare maintained until the session is terminated. (Algo-\nrithm 2 Lines 14-15)\nThese actions can be formalized as independent actions\nand checks as formulated in Algorithms 2-4. The controlling\nportion of the algorithm in Algorithm 2 continuously accepts\ninput from the attacker until a command that would end the\nsequence is received as defined by KilllCmds. With each\ninput, the required context is chosen to generate each answer.\nThe contexts are then updated as previously formulated based\non the provided question and its answer. ChooseContext\nselects which history to use with the question based on set\nmembership as defined by H0. UpdateContexts updates the\nglobal and context histories based on actions 4 and 5 to be\nused for future questions.\nThe actions taken in the above algorithms are illustrated\nin Fig. 3. Context handling is implemented in a front-end\ninterface (FEI) made up of an input and output handler. This\nFEI handles question generation and input curation on behalf\nof the generating model.\n1) Input Handler\nThe input handler accepts input from the attacker on behalf of\nthe model and decides what context to use based on that input\nand the size of the context history. Once the proper context is\nchosen, the full question is built and sent to the model. This\ncorresponds with actions 1 and 2 in Algorithm 2.\n2) Output Handler\nThe output handler sanitizes the model’s output by removing\nnotes or comments that slip through the prompt. The output\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nS1: Is global history needed or is context history too large?\nS2: Length(H1 ∪ q) < MAX?\nS3: Does question q change context for future questions?\nFIGURE 3. Front-End Interface (FEI) For LLM Honeypot\nhandler then updates that session’s context and global his-\ntory for future questions before returning the answer to the\nattacker. This corresponds with actions 3-6 in Algorithm 2.\n3) FEI Example\nA sample session is provided in Table 8. When updating the\ncontext history for this example, Directory and file changes\nsuch as those in QA 4 and QA 8 are included in the context\nhistory. The global history use is shown in QA 10 where\nonly past questions are passed to the model. By only passing\ncontext-changing QA pairs to the model and all questions for\ncertain edge cases, 1054 unique tokens are eliminated over\nthe course of the short conversation while still maintaining\nbelievable output.\nThe FEI conservatively manages the context and questions\nto the model on the fly, allowing us to maintain consistency\nwith what the attacker expects while using as few tokens\nas possible. minimizing token use is paramount due to a\nmemory limit present in GPT [32]. Additionally, for a more\nrobust use of the proposed FEI, the operating environment\nsuch as the operating system, hostname, or other data pro-\nvided to the attacker can be changed by updating the prompt.\nWhile this new architecture is a significant improvement\nover the architecture in Figure 2, it has limitations, such as\nthird-party interaction and cost. These limitations are further\nsummarized in Section V.\nIV. RESULTS AND DISCUSSION\nIn this section, the performance of an adaptive LLM context\nhoneypot as opposed to a traditional static interaction hon-\neypot is evaluated. For this comparison, Cowrie is chosen\nto represent static honeypots of the same risk level due to\nits widespread use on the internet and its backend terminal\nemulation [41].\nThe subsections explore the setup for the following evalu-\nation, single command similarity comparisons, and extended\nsession preservation.\nA. EVALUATION SETUP\nCowrie is a medium to high-interaction honeypot based on\nKippo that captures SSH and Telnet sessions [42]. For the\nmedium interaction version, the backend is implemented via\nhard-coded responses for 34 commands as of 2018 [5]. The\nauthors state that these commands are chosen due to most\nattackers only using those commands and that implementing\nall commands would take too much effort [42]. For the high-\ninteraction version, Cowrie acts as a proxy to another system\nwhere it reroutes traffic from the login handler to a secondary\nhigh-interaction cyber asset. For the purpose of this evalua-\ntion, the LLM honeypot supported by the FEI described in\nFig. 3 will be compared to the medium interaction Cowrie\nconfiguration. This comparison is chosen due to how both\nthe medium-interaction Cowrie and the LLM honeypot return\noutput without any execution, limiting the risk of misuse.\nFor the LLM honeypot, OpenAI’s gpt-3.5-\nturbo is used due to its availability, consistency, and so-\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nTABLE 8. LLM Terminal Contextual Responses [11]\nID Content\nSystem\nprompt\nYou are a Debian terminal with root user\nin the / directory. I will send commands\nand you will reply with that command’s\noutput. Don’t give any additional notes or\ncomments. I want you to reply with the\nterminal output only. If there is no output,\nrespond with nil.\nQuestion 1 uname -a\nAnswer 1\nLinux debian 4.19.0-16-amd64 #1 SMP\nDebian 4.19.181-1 (2021-03-19) x86_64\nGNU/Linux\nQuestion 2 pwd\nAnswer 2 /\nQuestion 3 ls\nAnswer 3\nbin dev home lib64 media opt root sbin\nsys usr boot etc lib lost+found mnt proc\nrun srv tmp var\nQuestion 4 cd /var/log\nAnswer 4 nil\nQA 4 added to context history for future questions.\nQuestion 5 pwd\nAnswer 5 /var/log\nQuestion 6 ls\nAnswer 6\nalternatives.log btmp\n...\ndpkg.log gpu-manager.log mail.log\nsyslog.4.gz\nQuestion 7 head dpkg.log\nAnswer 7\n2021-08-01 06:57:08 status installed\nman-db:amd64 2.9.4-2\n...\n2021-08-01 06:57:08 status installed\nlibssl1.1:armhf 1.1.1k-1\nQuestion 8 echo \"test123\" >> dpkg.log\nAnswer 8 nil\nQA 8 added to context history for future questions.\nQuestion 9 tail dpkg.log\nAnswer 9\n2021-10-01 10:00:00 status installed\nlibc6:amd64 2.31-0ubuntu9.2\n...\n2021-10-01 10:00:00 status installed\nca-certificates-java-dbgsym:all\n20190405ubuntu1 test123\nQ 10 uses global question history.\nQuestion 10 history\nAnswer 10 1 uname -a 2 pwd ... 9 tail dpkg.log 10\nhistory\nQuestion 11 pwd\nAnswer 11 /var/log\nAlgorithm 5Levenshtein L-ratio\nInput: s1: String 1\ns2: String 2\nOutput: L_ratio: Ratio of similarity between s1, s2\n1: n ← len(s1)\n2: m ← len(s2)\n3: lev ← LevenshteinDistance(s1, s2)\n4: maxLen ← max(n, m)\n5: L-ratio ← maxLen−lev\nmaxLen\n6: return L_ratio\nphistication. To have it behave as an attackable interface, the\nmodel uses the following system prompt:\n“You are a fully configured Debian 7 system named\nsvr01 logged in as the root user in the /root directory\nwith all packages installed. When given a command,\nrespond with the output of that command. Send only\nthe command output. Do not send any additional notes\nor comments under any circumstance. Send nil if that\ncommand has no output.”\nThe temperature is set to 0 for all inputs. For the extended\nsession, the architecture outlined in Fig. 3 is used.\nA Debian 7 Wheezy virtual machine acts as a control since\nCowrie in its base configuration mimics that operating sys-\ntem. Although the LLM can mimic any version or terminal\nvia the system prompt, the prompt is set to guide the model\nto behave as similarly to Cowrie and the control as possible.\nThe feasibility of using an LLM to generate honeypot output\nis first evaluated by assessing the similarity of outputs for\nsingle inputs before comparing deception, interactivity, and\ntoken usage in extended sessions in the next subsection.\nThe algorithms used to calculate these similarities are\ngiven where the first calculates a similarity ratio based on\nthe distance and length calculated by the second:\nB. SINGLE COMMAND COMPARISON\nFor this comparison, the system, filesystem, and perceived\nexternal connectivity of each honeypot are evaluated. To\ntest the feasibility of using an LLM to mimic an attackable\ninterface, the outputs for a single command are emulated by\nCowrie and the LLM. These outputs are compared to those\nof a control virtual machine running Debian 7 Wheezy since\nCowrie in its base configuration mimics that operating sys-\ntem. However, the LLM can mimic any version or terminal\nvia the prompt. For fairness, the model’s prompt is crafted to\nmake the operating system as similar to Cowrie’s as possible.\nTo see which of the two outputs is more similar, the\naverage ratio (denoted as L-ratio) of similarity of the output’s\nLevenshtein distance of ten outputs for each command from\nboth honeypots and the control virtual machine is calculated\n[43]. The Levenshtein distance quantifies the number of edits\nrequired to make two strings identical to one another by delet-\ning, inserting, or replacing characters. The L-ratio measures\nhow similar the two texts are with 1.0 being identical. The\nL-ratio and distance are calculated as shown in Algorithms 5\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nAlgorithm 6Levenshtein Distance\nInput: : s1: String 1\ns2: String 2\nOutput: : d: 2D Array of edits for all substrings s1 x s2\n1: n ← len(s1)\n2: m ← len(s2)\n3: Let d[0..n, 0..m] be a 2-dimensional array of integers\n4: for i ← 0 to n do\n5: d[i, 0] ← i\n6: end for\n7: for j ← 0 to m do\n8: d[0, j] ← j\n9: end for\n10: for i ← 1 to n do\n11: for j ← 1 to m do\n12: if s1[i] =s2[j] then\n13: cost ← 0\n14: else\n15: cost ← 1\n16: end if\n17: d[i, j] ← min{d[i − 1, j] + 1, d[i, j− 1] + 1, d[i −\n1, j− 1] +cost}\n18: end for\n19: end for\n20: return d[n, m]\nand 6 where the L-ratio of two strings is the size of the larger\nof the two strings subtracted by the number of edits necessary\nto make the two strings identical divided by that maximum\nlength. These algorithms are not novel and are provided for\ncompleteness.\nThe L-ratio for both honeypots to the control virtual ma-\nchine is calculated. If the L-ratio is higher for one of the two\nhoneypots, it means their output is more similar to that of the\ncontrol virtual machine. If the command is not implemented\nor if the LLM reports it is a fake system, it is automatically\ngiven a score of 0.0 for the input. The average L-ratio of all\ninputs is calculated as well as the average of inputs accepted\nby both honeypots as seen in Table 9 and Table 10.\nThese L-ratios are used as coordinates in Fig. 4 where the x\nvalue is the L-ratio for the LLM output and the Y value is the\nL-ratio for the Cowrie output. If the LLM’s output is more\nsimilar to the control, it will fall below the diagonal (x=y)\nline and above if Cowrie is more similar. Each command\nis separated into one of three categories: system, filesystem,\nand connectivity categories. These are represented by orange\ndots, purple dots, and green dots respectively. The specific\ndistances and L-ratios for each command can be found in the\nprovided data repository [44].\nThe system category is made up of commands that are used\nto gather system information and make system-modifying\nchanges. These include system file reading, package installs,\nand kernel modifications. The filesystem category is made\nup of commands that handle filesystem traversal and modifi-\ncations. These commands include listing, creation, deletion,\nTABLE 9. L-Ratios of Honeypot From Control VM: Single Command: All\nInputs\nL-ratio Cowrie LLM\nAverage 0.44421 0.61724\nSystem Average 0.32936 0.64654\nFilesystem Average 0.55106 0.60826\nConnectivity Average 0.41890 0.59401\n% LLM Higher: All 70.526%\n% LLM Higher: System 80.645%\n% LLM Higher: Filesystem 58.974%\n% LLM Higher: Connectivity 79.167%\nTABLE 10. L-Ratios of Honeypot From Control VM: Single Command:\nAccepted Inputs\nL-ratio Cowrie LLM\nAverage 0.55204 0.60140\nSystem Average 0.51052 0.56765\nFilesystem Average 0.56556 0.60585\nConnectivity Average 0.57182 0.63300\n% LLM Higher: All 64.865%\n% LLM Higher: System 70.0%\n% LLM Higher: Filesystem 57.895%\n% LLM Higher: Connectivity 75.0%\nFIGURE 4. L-ratios from Control VM: Single Command\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nand access control for files, directories, and links. For con-\nnectivity, since both backends are simulating the attacker’s\ncommands, no external communication is taking place with\nthe backend with the exception of curl and wget for Cowrie.\nBoth backends then must simulate this behavior using believ-\nable addresses, response time, firewalls, and routing. To test\neach, A variety of net utilities are used targeting both domain\nnames and IP addresses.\nAs can be seen in Fig. 4, Table 9, and Table 10, the LLM\nhoneypot had a higher L-ratio 70.5% of the time over Cowrie\nwhen compared to the control’s output for all inputs, indicat-\ning a higher level of similarity to a real machine for single\ncommands. The average L-ratio for all inputs was 0.444 for\nCowrie and 0.617 for the LLM. It’s worth noting that the\nLLM honeypot can emulate outputs for any valid command\ninput while medium-interaction Cowrie can only emulate\n34 commands in its default configuration. This allows the\nLLM honeypot to provide a greater level of interaction than\nCowrie. For fairness, the L-ratios for inputs implemented by\nboth honeypots are compared separately. The LLM had a\nhigher L-ratio 64.9% of the time with an average L-ratio of\n0.601 and Cowrie having an average of 0.552.\nSystem command outputs from the LLM had a higher L-\nratio 80.6% of the time with Cowrie having an average L-\nratio of 0.329 and the LLM having an average L-ratio of\n0.6465. This was expected since Cowrie has limited system\ncommand support. For commands implemented by both,\nCowrie had an average L-ratio for system commands of 0.511\nand the LLM of 0.568.\nFilesystem commands had a more incremental improve-\nment with 59% of LLM commands having a higher L-\nratio than Cowrie with the average L-ratios being 0.551 and\n0.608 for Cowrie and LLM respectively. This incremental\nimprovement is due to most of what Cowrie emulates being\nfilesystem commands. For commands implemented by both,\nCowrie had an average L-ratio for filesystem commands of\n0.566 and the LLM of 0.606.\nConnectivity commands were more surprising with 79.2%\nhaving a higher LLM L-ratio with the average L-ratios being\n0.419 and 0.594 for Cowrie and LLM respectively. Actually\ndownloading with Curl and Wget gave Cowrie a higher ratio\nsince it actually executes the request. However, other com-\nmands such as ip were not implemented but should have been\nfor a base system. IPTables was unable to handle and display\nchanges. Netstat displayed the same output with different\noptions. For commands implemented by both, Cowrie had\nan average L-ratio for connectivity commands of 0.572 and\nthe LLM of 0.633.\nSeveral factors contribute to the outliers observed. Firstly,\nCowrie actually executes Curl and Wget commands, while\nthe LLM only emulates them, resulting in L-ratios being\nskewed towards Cowrie in that specific circumstance. This\naspect makes Cowrie more advantageous for malware captur-\ning, as it can provide real-time logging of captured artifacts.\nHowever, it is worth noting that the capturing capabilities of\nthe LLM honeypot can be extended in future work to include\ndownloading by the FEI when these specific commands come\nup, reducing the disparity between the two honeypots.\nSecondly, the LLM exhibited undesirable behavior with\ncertain commands due to the use of Debian 7 in the prompt.\nFor instance, ifconfig, being deprecated, was not emu-\nlated, and the enable command was unrecognized. These\nissues can be alleviated through prompt refinement. Thirdly,\nthe LLM occasionally generated excessive output or went\noff-topic, particularly when dealing with commands that\nhad long outputs like dmesg and ps, resulting in the token\nlimit being reached. This \"hallucinatory rambling\" is further\ndiscussed in Section V.\nTo conclude the single-command evaluation, the Leven-\nshtein distance of outputs was used to calculate a similarity\nratio of honeypot output to real system output as an initial\nindicator of deception. Despite the limitations and outliers\nmentioned, the evaluation results demonstrate the proposed\nhoneypot’s advantages over Cowrie in terms of emulating\nrealistic machine behavior for single commands across all\ncategories. The potential for greater attacker interaction and\nfavorable L-ratios observed in the majority of cases further\nsupport this conclusion. Improvements such as prompt re-\nfinement, sanity checks, caching outputs, and exploring an\nextended architecture can be used to mitigate limitations and\noutliers for a honeypot where a language model to generate\nits output is used. While the L-ratio is not a perfect metric\nand different outputs may not necessarily indicate incorrect\noutput, it serves as a valuable starting point for evaluating\nsingle command outputs and finding similarities in output\nformats.\nC. EXTENDED SESSION INTEGRITY COMPARISON\nTo determine if language models can be used for extended\nattacker deception, the number of interactions it takes for\na honeypot session to break down is compared. This com-\nparison is conducted between static low-risk honeypots and\ntwo LLM-based honeypots. Cowrie is chosen to represent\ntraditional low-risk honeypots due to its widespread use\nand availability as well as its use in the previous evalua-\ntion [41]. The first LLM honeypot is implemented using\ngpt-3.5-turbo with the previously used system prompt\nthat saves all questions and answers as context as a base\nhoneypot (Fig. 2). This LLM honeypot is compared to one\naugmented with an FEI to represent the proposed implemen-\ntation as detailed in Fig. 3. The number of tokens used in both\nLLM setups throughout each session is measured to measure\nhow many tokens the FEI saved and if that affects overall\ndeception.\n1) Session Progress\nTo evaluate interaction and deception preservation for attack\nsessions, five attacker scenarios for the three honeypot setups\nare followed to see how long it takes for each honeypot’s\ndeception to break down. Each scenario contains tactics and\ntechniques listed in the MITRE ATT&CK matrix framework\n[12], which are outlined in our data repository [44].\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nFIGURE 5. Extended Session Integrity: Accepted Inputs From Scenario\nThe scenarios used for evaluation are system reconnais-\nsance (SR), data obfuscation (DO), lateral propagation (LP),\npersistence (PE), and exfiltration (EX). These scenarios are\nchosen due to their relevance to real-world cybersecurity\nthreats and the need to test the effectiveness of defensive\nmeasures in a variety of critical areas. The command se-\nquences used for each scenario are by no means novel, but\na simple example to measure breakdown in deception over\ntime.\nIt should be noted that the command sequences were not\nchosen with the intention of causing a breakdown for any\nof the honeypots. If a command fails and there is a suitable\nalternative available, that alternative is used for that step in\nthe scenario instead of the intended input. This is done so\nlong as the data needed for the following commands are\ngiven with the substitute i.e. ip addr and ifconfig. If\nno suitable substitution can be done, the scenario ends early.\nTherefore, this test measures the most favorable outcome for\neach scenario for each setup.\nEach attack scenario has nine steps. In each step, one or\nmore Linux Bash shell commands, scripts, or a combination\nof them are run. The intended commands used to accomplish\neach tactic in the scenario are further detailed in our data\nrepository [44].\nThe SR scenario includes steps attackers might take to\nscout a recently compromised system, such as identifying\nthe operating system, processor, services, disk usage, paths,\nopen ports, log files, the passwd file, and active users. The\nDO scenario encompasses both the inputs that an attacker\nmight employ to disrupt the use of files on a system and\nthe methods they employ to conceal their actions. The LP\nscenario includes network reconnaissance, shellcode execu-\ntion, and basic user authentication on a secondary system as\nwell as commands to verify their actions. The PE scenario is\nmade up of commands to ensure repeat access to the com-\npromised system via backdoors and new user accounts. The\nDR scenario includes commands to search for and exfiltrate\ninteresting files using different methods.\nThe number of steps each honeypot is able to execute in\nthe sequence is measured, stopping when all possible inputs\nare rejected, an output is so unbelievable a rational attacker\nwould not continue, or the scenario concludes. Each bar\ngraph in Fig. 5 quantifies the number of steps achieved in\neach scenario with orange being Cowrie, green being the\nselective-context FEI-assisted LLM honeypot outlined at the\nend of Section III, and purple being an LLM honeypot that\nsaves all context. For the honeypots using LLMs, the same\nsystem prompt is used as in the single command evaluation.\nAs can be seen in Fig. 5, the LLM and FEI-augmented\nLLM outlasted Cowrie in every scenario, completing three of\nthe five while maintaining context. Cowrie was able to fully\ncomplete the SR scenario but fell short for the other four.\nFor data obfuscation and ransomware (DO), Cowrie failed\nwhen attempting to create and encrypt the archive whereas\nboth LLMs made it to the end. However, later in that scenario,\nupon executing the history command, both LLMs failed\nto give proper output after clearing with history -c.\nFor scanning and lateral propagation (LP), Cowrie was un-\nable to execute a for loop to ping all addresses on the subnet\nin the provided environment, cutting the interaction short.\nBoth LLM honeypots were able to simulate this interaction.\nLater in the LP scenario, both the LLM and FEI-augmented\nLLM failed to execute an internet-fetched shell script and\nidentified themselves as an AI agent.\nFor the persistence (PE) scenario, both LLM honeypots\nwere able to complete the session while Cowrie failed to\nset up a Netcat backdoor that would start a shell upon\nconnection.\nLastly, for DR, Cowrie failed when attempting to compress\na file for exfiltration whereas the LLM was able to do so\nbut failed when trying to view the password shadow file,\nciting invalid permissions. While the LLM honeypots had\nsome mistakes in later outputs, they were able to emulate an\nattackable environment for longer than or for the same length\nas Cowrie.\n2) Session Token Use\nTo measure the effectiveness and retention of only passing\ncontext-changing interactions to the LLM as chosen by an\nFEI, the number of tokens used for each step in each sce-\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nFIGURE 6. Session Token Use: LLM vs FEI-Augmented LLM\nnario is measured. Both implementations started by using 70\ntokens for the system prompt. If an answer is determined to\nchange the operating context for the session, that command\ninput and output are used as a QA pair and included when\npassed to the model for future questions. If the session breaks\ndeception before the scenario is concluded, the measurement\nof tokens will end on that step. This will be one step further\nthan seen in Fig. 5 as tokens will be used to generate the\ndeception-breaking answer.\nThe FEI-augmented LLM honeypot was able to maintain\ninteraction with the user for the same amount of steps as\nthe LLM honeypot that saved all context while using up to\n77.26% fewer tokens by the end of a session as seen by the\nsystem reconnaissance scenario in graph (a) in Fig. 6. On\naverage across the five scenarios, the FEI-augmented LLM\nhoneypot used 62.17% fewer tokens.\nD. DISCUSSION\nResults from the evaluations show that a honeypot using\nan LLM as its backend is able to better emulate outputs\nthan traditional honeypots of a similar level of risk (attacker\nusing the honeypot outside the accepted scope). the LLM\nhoneypot was also able to maintain attack sessions that used\nstandard tools for the operating system it emulated at a high\nrate. The FEI proposed in Section III was able to reduce the\nnumber of tokens used over the course of a session while still\nmaintaining deception. However, several limitations that may\nlimit the use of LLMs for cyber deception were encountered.\nIn this work, a study on the effectiveness of LLMs for\ncyber deception is provided. Past work in using LLMs for\nattacker deception has proposed the idea of using LLMs to\nemulate terminal behavior [25, 7] or to generate artifacts\nto prolong attacker engagement [39]. A simple and elegant\narchitecture is designed and evaluated. This architecture can\nbe easily implemented while lowering the detection proba-\nbility even compared to commonly used and well-designed\nmedium-interaction honeypots such as Cowrie. The proposed\narchitecture reduced token use by anywhere from 11-77%\nfor attack scenarios that were simulated to completion when\ncompared to a base architecture [25].\nV. LIMITATIONS\nGenerative models used for cyber deception can change the\noperating environment and threat surface provided to the\nattacker by providing relevant context in the prompt. This\ncapability makes them well-suited for threat engagement\nwhich can evolve as new threats emerge. However, such a\nuse case requires addressing several limitations inherent to\nthe design, technology, and implementation derived from the\nspecific model used.\nThese limitations can be categorized by whether they’re\ninherent to the design of the underlying transformer archi-\ntecture or due to the model used. Both are explored more\nin-depth in the following subsections. With each limitation, a\ndiscussion of possible solutions to mitigate the severity when\ndeploying honeypots using LLMs are provided.\nA. DESIGN LIMITATIONS\nThese limitations are inherent to the use of models that\ngenerate output using the self-attention mechanism and will\nbe present no matter which model is used. Each limitation\npresented is summarized and accompanied with mitigations\nto limit or eliminate their impact.\n1) Deterministic Output\nUsing LLMs for something deterministic where for an input,\nthe output should be the same can lead to some issues. This is\ndue to how LLMs calculate the next token in a sequence using\nfew-shot learning [30, 45]. If token probabilities are statisti-\ncally close, variations in output may occur. This difference\ncan cause further changes down the line as future inputs will\nuse that discrepancy when calculating subsequent sequences\nin that output [46].\nTo mitigate this shortcoming, executing the same com-\nmand multiple times and taking the most common output will\nfind the most common occurrence in the parts of the output\nthat have discrepancies. However, this method comes with\nVOLUME 4, 2016 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nincreased cost and reduced performance and responsiveness.\nAlternatively, finding where discrepancies are common i.e.\nin version numbers, and masking those based on the previous\ncontext will increase the deterministicness of the honeypot.\nLi et al. take this approach by guiding the model to learn the\ndeterministic relationship between masked content and the\nrest of the content to capture factual knowledge [47].\n2) Hallucination, Rambling, and Misbehaving\nAnother limitation can be occasionally found in the text gen-\neration for longer outputs which can lead to detection. These\ncome up in the form of \"hallucinatory rambling,\" which\noccurs when the model makes something up and becomes\nstuck in an output loop until the token limit is reached. In\naddition to detecting wrong output, it takes much longer for\nthe LLM to output a rambling answer. To mitigate this, the\nlong response time can be used to detect when rambling\noccurs and reset with altered context.\nAdditionally, in testing with GPT3.5, the model would\noccasionally misbehave with certain questions where it added\nadditional notes or comments at the end of answers when told\nnot to in the prompt. If this occurs and that answer is used\nas context for future questions, future answers are likely to\ncontain those notes or comments as well. A Sanitization step\nlike the one used in Fig. 3 to remove notes or comments is\ncrucial for maintaining the deception.\n3) External Communication\nAnother possibility of detection comes with the interaction\nbetween the honeypot and some observable infrastructure\nunder the attacker’s control. Such scenarios include starting\na session with a C&C server, downloading malware, or\nexfiltrating data. Since no traffic is generated, the attacker\ncan determine that their commands are not being executed\nand that they are in a honeypot. These detection scenarios\ncan’t be addressed without help from other technologies\nsuch as communication handlers and sandboxes. To mitigate\nthis, a communication handler can be used to spoof the\ncommunication on behalf of the generating model.\nB. IMPLEMENTATION LIMITATIONS\nThese limitations were encountered due to the specific model\nused and may or may not be present when using different\nmodels. GPT3.5 is implemented as an example use case in\nSection III. However, there are some constraints to using\nOpenAI’s chat completion models that can have implications\nthat bring to light further limitations.\n1) Cost\nCompared to GPT3.5, GPT4 costs 15x-30x for prompts and\n30x-60x as much for completion. For a research deployment\nwhere thousands of attacks can be observed daily, costs can\nquickly add up when each command can possibly return\nthousands of tokens. With this in mind, it’s imperative to\nappear as a real system because if an attacker determines\nthe honeypot is implemented using a pay-as-you-go model,\nthey can maximize context by executing commands that\nhave large outputs such as file reading or recursive directory\nlisting.\nWhen effectively utilized, the proposed honeypot exhibits\na considerable reduction in deployment expenses, as evi-\ndenced by the lower token usage observed in the attack sce-\nnarios in Section IV. However, if the goal is to attract a large\nnumber of hosts, the cost per token is still a limiting factor. To\novercome this, a locally trained and hosted language model\ncould be used.\nIn total, around 240K tokens were used to develop the\nFEI using gpt-3.5-turbo. Evaluation consumed signif-\nicantly more tokens due to each instruction being executed\n10 times to compute the average Levenshtein distances. On\nthe other hand, testing the five different MITRE ATT&CK\nscenarios only consumed 50,840 tokens. Of these, the FEI-\nassisted model used 47.34% fewer tokens than the base LLM\nhoneypot.\n2) Memory Limit\nOpenAI’s chat completion models have a limit on how many\ntokens can be supplied per query. This is due to memory\nlimitations inherent to transformer-based models [48]. This\nlimit applies to both prompt and response so some space\nis required to be left to not cut off the response. Models\nwith larger memory are generally more expensive to train,\nmeaning it’s more expensive for the user as seen with GPT-4\nbeing more expensive than GPT-3.5 [33, 49]. Using a reduced\nmemory model, if only relevant context is supplied per each\ncommand as detailed in Section III, this should cut down on\ncosts and extend the session by a large margin.\n3) Responsinveness\nTo ensure efficient live usage and maintaining of deception,\nit is crucial to minimize the time required to generate output\nfor certain inputs. Currently, generating output for specific\ninputs can take seconds or even minutes, which is impractical\nfor real-time interactions. Therefore, it is essential to signifi-\ncantly reduce the response time to less than a second.\nTo accomplish this, a caching mechanism within the Front-\nEnd Interface (FEI) could be used. By caching large outputs\nin the FEI, the system can swiftly retrieve and send the\npre-generated responses instead of waiting for the LLM to\ngenerate them on the spot. Another solution is to use a local\nmodel with characteristics put in place in the training stage\nto limit long outputs.\n4) Training Bias\nGPT3.5 has only been trained with data up to 2021 [50]. The\nimpact of this cutoff is that any bias present in the training\ndata will be inherited by the model such as outdated log\ncontents or lack of modern interactions [10]. This bias can\nlead to a breakdown in deception if an attacker uses a modern\npackage or tool with critical usage implemented after that\ncutoff. So long as these biases are internally consistent, this\ncan be managed. However, it may limit what devices and\n16 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\ntechnologies the honeypot can masquerade as. Because of\nthis limit, evolution with the attacker is also hampered. To\nmitigate this, a more up-to-date model can be used.\nTABLE 11. gpt-3.5-turbo Terminal Breakdown - Sensitive Output [11]\n[The warning is unable to be suppressed even with an appropriately worded\nsystem prompt.]\nID Content\nSystem prompt\nYou are a Debian terminal with root\nuser in the / directory. I will send\ncommands and you will reply with that\ncommand’s output. Don’t give any\nadditional notes or comments. I want\nyou to reply with the terminal output\nonly. If there is no output, respond with\nnil.\nQuestion 1 cd var/log\nAnswer 1 nil\nQuestion 2 cat auth.log\nAnswer 2\nIt is not ethical to display the contents\nof auth.log as it may contain sensitive\ninformation.\n5) Ethical Constraints\nOpenAI has implemented safeguards to prevent its models\nfrom returning malicious or otherwise harmful responses.\nThough the use case of a honeypot is not intentionally ma-\nlicious, questions from the attacker may be categorized as\nsuch. When attempting to view log files that contain sensitive\ninformation, the deception may also fail as seen in Table 11\ndue to moderation policies put in place. To mitigate this,\nsanitizing self-reporting output and returning error codes can\nbe done to present the illusion of a faulty system rather than\na honeypot.\nVI. CONCLUSIONS\nGenerative models such as LLMs have exploded in popu-\nlarity in non-research sectors since the release of ChatGPT,\na conversational large language model. With this explosion\ncomes an exploration by users of alternate use cases that\ngenerative models can fulfill by changing their personalities\nbased on the provided prompt. One of the use cases is\nas an interactive cyber deception asset to learn the tactics,\ntechniques, and procedures (TTPs) of attackers.\nAn implementation of a honeypot using input curation for\ngenerative models to generate terminal output is proposed.\nThe proposed honeypot takes inspiration from other dynamic\ninteraction honeypots in its design, incorporating a context-\naware approach to engage with potential threats without in-\ncreasing risk. This implementation filters past inputs to limit\ntoken usage while maintaining interactivity and deception.\nThe effectiveness of this implementation is evaluated for\nboth singular commands in its output similarity and extended\nsessions in its ability to maintain deception. This evalua-\ntion compared the proposed language model honeypot to a\nstatic medium-interaction honeypot of a similar risk level,\nCowrie. The proposed honeypot’s output was more similar\nthan Cowrie’s 70% of the time, with an average similarity\nscore 16% higher. The proposed honeypot compares favor-\nably to a non-curating implementation where the proposed\narchitecture reduced token use by up to 77% by saving only\nthe relevant context.\nSection V discusses the limitations encountered and how\nthey may be overcome. While language models offer interest-\ning possibilities as honeypots, they do have limitations. The\nlimitations include responsiveness, non-deterministic output,\nand non-verifiable output. Further research is needed before\nhoneypots using this technology can be deployed effectively\nin the wild.\nFuture work is to be done in the extension of the input cu-\nration mechanism. More robust methods of context selection\nand handling of input edge cases are left for future work. Al-\nternatively, research in the model used for output generation\ncould be explored to find which of a series of different models\nwith different feature sets are the most effective for cyber\ndeception. The development and deployment or extensive\ntuning of a generative model for the use of cyber deception is\nalso considered to replace the general-purpose model used.\nThreat engagement is a constantly evolving issue due\nto the evolutionary nature of cyberattacks. This requires a\ntechnology that can evolve with it and be able to adapt to\nnew issues as they come up. Generative models fulfill this\nneed as one of a still-expanding set of use cases. To this end,\nthe efficient handling of input for these models is paramount\nand has demonstrated improvements to a base deployment in\ntoken use reduction without compromising deception.\nACKNOWLEDGMENT\nThe opinions expressed and any errors contained in the paper\nare those of the authors. This paper is not meant to represent\nthe position or opinions of the author’s institution or the\nfunding agencies. AI text generation tools such as ChatGPT\nwere used for experiments only; they were not used in the\nwriting of this paper.\nReferences\n[1] Lance Spitzner. Honeypots: tracking hackers. V ol. 1.\nAddison-Wesley Reading, 2003.\n[2] Iyatiti Mokube and Michele Adams. “Honeypots: con-\ncepts, approaches, and challenges”. In:Proceedings of\nthe 45th annual southeast regional conference . 2007,\npp. 321–326.\n[3] Mohamad Faiz Razali et al. “IoT honeypot: A re-\nview from researcher’s perspective”. In: 2018 IEEE\nConference on Application, Information and Network\nSecurity (AINS). IEEE. 2018, pp. 93–98.\n[4] Javier Franco et al. “A survey of honeypots and\nhoneynets for internet of things, industrial internet\nof things, and cyber-physical systems”. In: IEEE\nCommunications Surveys & Tutorials 23.4 (2021),\npp. 2351–2383.\nVOLUME 4, 2016 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\n[5] Alexander Vetterl and Richard Clayton. “Bitter har-\nvest: Systematically fingerprinting low-and medium-\ninteraction honeypots at internet scale”. In: 12th\n{USENIX} Workshop on Offensive Technologies\n({WOOT} 18). 2018.\n[6] OpenAI. URL : https://openai.com/product (visited on\n03/08/2023).\n[7] Jules White et al. “A Prompt Pattern Catalog to En-\nhance Prompt Engineering with ChatGPT”. In: arXiv\npreprint arXiv:2302.11382 (2023).\n[8] Rishi Bommasani et al. “On the opportunities and\nrisks of foundation models”. In: arXiv preprint\narXiv:2108.07258 (2021).\n[9] Soumya Krishnamurthy. An analysis of CHATGPT\nand OpenAI GPT-3: How to use it for your business .\nMar. 2023. URL : https : / / www . version1 . com / an -\nanalysis- of- chatgpt- and- openai- gpt3- how- to- use-\nit-for-your-business/.\n[10] Alex Tamkin et al. “Understanding the capabilities,\nlimitations, and societal impact of large language\nmodels”. In: arXiv preprint arXiv:2102.02503 (2021).\n[11] OpenAI GPT-3.5-Turbo. URL : https://platform.openai.\ncom/docs/guides/chat (visited on 03/10/2023).\n[12] MITRE ATT&CK . URL : https : / / attack . mitre . org/\n(visited on 05/20/2023).\n[13] Fred Cohen. Deception ToolKit. 1998. URL : http://all.\nnet/dtk/ (visited on 03/09/2023).\n[14] Feng Zhang et al. “Honeypot: a supplemented active\ndefense system for network security”. In: Proceedings\nof the Fourth International Conference on Parallel and\nDistributed Computing, Applications and Technolo-\ngies. IEEE. 2003, pp. 231–235.\n[15] Alejandro Molina Zarca et al. “Virtual IoT HoneyNets\nto mitigate cyberattacks in SDN/NFV-enabled IoT\nnetworks”. In: IEEE Journal on Selected Areas in\nCommunications 38.6 (2020), pp. 1262–1277.\n[16] Niels Provos. “Honeyd-a virtual honeypot daemon”.\nIn: 10th dfn-cert workshop, hamburg, germany. V ol. 2.\n2003, p. 4.\n[17] Cowrie SSH/Telnet Honeypot . URL : https : / / github .\ncom/cowrie/cowrie/ (visited on 05/02/2023).\n[18] Meng Wang, Javier Santillan, and Fernando Kuipers.\n“Thingpot: an interactive internet-of-things honey-\npot”. In: arXiv preprint arXiv:1807.04114 (2018).\n[19] Juan David Guarnizo et al. “Siphon: Towards scalable\nhigh-interaction physical honeypots”. In: Proceedings\nof the 3rd ACM Workshop on Cyber-Physical System\nSecurity. 2017, pp. 57–68.\n[20] Alexander Vetterl and Richard Clayton. “Honware: A\nvirtual honeypot framework for capturing CPE and\nIoT zero days”. In: 2019 APWG Symposium on Elec-\ntronic Crime Research (eCrime) . IEEE. 2019, pp. 1–\n13.\n[21] Adrian Pauna, Andrei-Constantin Iacob, and Ion Bica.\n“Qrassh-a self-adaptive ssh honeypot driven by q-\nlearning”. In: 2018 international conference on com-\nmunications (COMM). IEEE. 2018, pp. 441–446.\n[22] Tongbo Luo et al. “Iotcandyjar: Towards an\nintelligent-interaction honeypot for iot devices”. In:\nBlack Hat 2017 (2017), pp. 1–11.\n[23] Moeka Yamamoto, Shohei Kakei, and Shoichi Saito.\n“Firmpot: A framework for intelligent-interaction\nhoneypots using firmware of iot devices”. In: 2021\nNinth International Symposium on Computing and\nNetworking Workshops (CANDARW) . IEEE. 2021,\npp. 405–411.\n[24] V olviane Saphir Mfogo et al. “AIIPot: Adaptive\nIntelligent-Interaction Honeypot for IoT Devices”. In:\narXiv preprint arXiv:2303.12367 (2023).\n[25] Forrest McKee and David Noever. “Chatbots in a Hon-\neypot World”. In: arXiv preprint arXiv:2301.03771\n(2023).\n[26] Muris Sladi ´c et al. “LLM in the Shell: Generative Hon-\neypots”. In: arXiv preprint arXiv:2309.00155 (2023).\n[27] Ashish Vaswani et al. “Attention is all you need”. In:\nAdvances in neural information processing systems30\n(2017).\n[28] Mingyu Zong and Bhaskar Krishnamachari. “a sur-\nvey on GPT-3”. In: arXiv preprint arXiv:2212.00857\n(2022).\n[29] OpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.\n08774 [cs.CL].\n[30] Luciano Floridi and Massimo Chiriatti. “GPT-3: Its\nnature, scope, limits, and consequences”. In: Minds\nand Machines 30 (2020), pp. 681–694.\n[31] OpenAI Fine Tuning. URL : https : // platform . openai.\ncom/docs/guides/fine-tuning (visited on 03/21/2023).\n[32] Learn how to work with the ChatGPT and GPT-4\nmodels. URL : https://learn.microsoft.com/en-us/azure/\ncognitive - services / openai / how - to / chatgpt ? pivots =\nprogramming-language-chat-completions (visited on\n05/27/2023).\n[33] OpenAI Pricing . URL : https : / / openai . com / pricing\n(visited on 03/20/2023).\n[34] Forrest McKee and David Noever. “Chatbots in a\nBotnet World”. In: arXiv preprint arXiv:2212.11126\n(2022).\n[35] Corrado Leita, Ken Mermoud, and Marc Dacier.\n“Scriptgen: an automated script generation tool for\nhoneyd”. In: 21st Annual Computer Security Applica-\ntions Conference (ACSAC’05). IEEE. 2005, 12–pp.\n[36] Wira Zanoramy Ansiry Zakaria and Miss Laiha Mat\nKiah. “A review of dynamic and intelligent honey-\npots”. In: ScienceAsia 39S (2013), pp. 1–5.\n[37] Seamus Dowling, Michael Schukat, and Enda Barrett.\n“New framework for adaptive and agile honeypots”.\nIn: Etri Journal 42.6 (2020), pp. 965–975.\n[38] Alan M Turing. Computing machinery and intelli-\ngence. Springer, 2009.\n[39] Enrico Cambiaso and Luca Caviglione. “Scamming\nthe Scammers: Using ChatGPT to Reply Mails for\n18 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRagsdale and Boppana: Improving Interaction and Deception in Low-Risk Honeypots\nWasting Time and Resources”. In: arXiv preprint\narXiv:2303.13521 (2023).\n[40] Pengfei Liu et al. “Pre-train, prompt, and predict: A\nsystematic survey of prompting methods in natural\nlanguage processing”. In: ACM Computing Surveys\n55.9 (2023), pp. 1–35.\n[41] Alexander Vetterl, Richard Clayton, and Ian Walden.\n“Counting outdated honeypots: Legal and useful”. In:\n2019 IEEE Security and Privacy Workshops (SPW) .\nIEEE. 2019, pp. 224–229.\n[42] Cowrie Read the Docs . URL : https : / / cowrie .\nreadthedocs.io/ (visited on 05/20/2023).\n[43] Vladimir I Levenshtein et al. “Binary codes capable\nof correcting deletions, insertions, and reversals”. In:\nSoviet physics doklady. V ol. 10. 8. Soviet Union. 1966,\npp. 707–710.\n[44] Jarrod Ragsdale. Replication Data for: Improving In-\nteraction and Deception in Low-Risk Honeypots Using\nLarge Language Models. Version V1. 2023. DOI : 10.\n7910/DVN/0DHIPX. URL : https://doi.org/10.7910/\nDVN/0DHIPX.\n[45] M Onat Topal, Anil Bas, and Imke van Heerden.\n“Exploring transformers in natural language gen-\neration: Gpt, bert, and xlnet”. In: arXiv preprint\narXiv:2102.08036 (2021).\n[46] OpenAI Temperature. URL : https://platform.openai.\ncom/docs/quickstart/adjust-your-settings (visited on\n03/10/2023).\n[47] Shaobo Li et al. “Pre-training language models with\ndeterministic factual knowledge”. In: arXiv preprint\narXiv:2210.11165 (2022).\n[48] Angela Fan et al. “Addressing some limitations\nof transformers with feedback memory”. In: arXiv\npreprint arXiv:2002.09402 (2020).\n[49] OpenAI - GPT4. URL : https://openai.com/product/gpt-\n4 (visited on 03/20/2023).\n[50] Stuart Hargreaves. “‘Words Are Flowing Out Like\nEndless Rain Into a Paper Cup’: ChatGPT & Law\nSchool Assessments”. In: The Chinese University of\nHong Kong Faculty of Law Research Paper 2023-03\n(2023).\nJARROD RAGSDALE received his B.S. in com-\nputer science from the University of Texas at San\nAntonio (UTSA) in 2021, where he is currently\npursuing his Ph.D. degree. He is currently a re-\nsearcher in the Systems and Networks Laboratory\nin the computer science department at UTSA. His\nresearch interests include IoT, network security,\nand threat engagement.\nRAJENDRA V. BOPPANA is a Professor in the\nDepartment of Computer Science at the University\nof Texas at San Antonio (UTSA). Dr. Boppana’s\nresearch interests include computer network se-\ncurity, performance, and high-performance com-\nputing. Dr. Boppana is currently working on an-\nalyzing and detecting ransomware and exfiltration\nattacks and network security. He published over 75\npeer-reviewed conference papers, journal articles,\nand book chapters on these topics. Dr. Boppana\nserved as the principal investigator (PI) or co-PI for over 15 research\ngrants from United States federal funding agencies and is the sole or lead\ninventor for three patents. Dr. Boppana received his Ph.D. degree in com-\nputer engineering from the University of Southern California, Los Angeles,\nUSA. Dr. Boppana directed UTSA’s Quantitative Literacy Program (QLP),\na university-wide curriculum enhancement program, 2011-16. Dr. Boppana\nserved as the Department of Computer Science chair from 2012 to 2018.\nVOLUME 4, 2016 19\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326104\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Honeypot",
  "concepts": [
    {
      "name": "Honeypot",
      "score": 0.9473212957382202
    },
    {
      "name": "Computer science",
      "score": 0.8163084983825684
    },
    {
      "name": "Security token",
      "score": 0.6720132827758789
    },
    {
      "name": "Computer security",
      "score": 0.560904324054718
    },
    {
      "name": "Transformer",
      "score": 0.4427397847175598
    },
    {
      "name": "Generative model",
      "score": 0.42891770601272583
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.4152355194091797
    },
    {
      "name": "Generative grammar",
      "score": 0.2660899758338928
    },
    {
      "name": "Artificial intelligence",
      "score": 0.24494320154190063
    },
    {
      "name": "Engineering",
      "score": 0.10397210717201233
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}