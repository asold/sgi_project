{
  "title": "Code Structureâ€“Guided Transformer for Source Code Summarization",
  "url": "https://openalex.org/W3159616622",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2387503311",
      "name": "Gao, Shuzheng",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2378139064",
      "name": "Gao, Cuiyun",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2742539792",
      "name": "He, Yulan",
      "affiliations": [
        "University of Warwick"
      ]
    },
    {
      "id": "https://openalex.org/A4306611960",
      "name": "Zeng, Jichuan",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A4288661321",
      "name": "Nie, Lun Yiu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2186005218",
      "name": "Xia Xin",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A4221719684",
      "name": "Lyu, Michael R.",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2887364112",
    "https://openalex.org/W2728773317",
    "https://openalex.org/W1993553653",
    "https://openalex.org/W2072812688",
    "https://openalex.org/W4221162678",
    "https://openalex.org/W3157291566",
    "https://openalex.org/W3086007799",
    "https://openalex.org/W3101198795",
    "https://openalex.org/W6780759785",
    "https://openalex.org/W2294980783",
    "https://openalex.org/W3042954354",
    "https://openalex.org/W2888557792",
    "https://openalex.org/W3014339000",
    "https://openalex.org/W4249122235",
    "https://openalex.org/W2741561716",
    "https://openalex.org/W2600355622",
    "https://openalex.org/W2034209539",
    "https://openalex.org/W3091730360",
    "https://openalex.org/W2999553660",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2789062242",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2592128043",
    "https://openalex.org/W3119507053",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2126793110",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3106483960",
    "https://openalex.org/W2807964941",
    "https://openalex.org/W2884276923",
    "https://openalex.org/W2952564508",
    "https://openalex.org/W2963499994",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2955426500",
    "https://openalex.org/W2979792666",
    "https://openalex.org/W2601061983",
    "https://openalex.org/W3105398568",
    "https://openalex.org/W3086449553",
    "https://openalex.org/W2082160726",
    "https://openalex.org/W4205371973",
    "https://openalex.org/W2898734514",
    "https://openalex.org/W2963392741",
    "https://openalex.org/W3034689979",
    "https://openalex.org/W2081749632",
    "https://openalex.org/W2909672886",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2136296681",
    "https://openalex.org/W2516621648",
    "https://openalex.org/W2897133944",
    "https://openalex.org/W2306852879",
    "https://openalex.org/W2244351838",
    "https://openalex.org/W2951861246"
  ],
  "abstract": "Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality.",
  "full_text": "Code Structure Guided Transformer for Source Code Summarization\nSHUZHENG GAO, Harbin Institute of Technology, Shenzhen, China\nCUIYUN GAOâˆ—, Harbin Institute of Technology, Shenzhen, China\nYULAN HE, University of Warwick, UK\nJICHUAN ZENG, The Chinese University of Hong Kong, Hong Kong, China\nLUN YIU NIE, Tsinghua University, China\nXIN XIA, Software Engineering Application Technology Lab, Huawei, China\nMICHAEL R. LYU, The Chinese University of Hong Kong, Hong Kong, China\nCode summaries help developers comprehend programs and reduce their time to infer the program functionalities during software\nmaintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code\nsummaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the\ncode structure information into the Transformer is under-explored in this task domain. In this paper, we propose a novel approach\nnamed SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g.,\ncode tokens and statements) and global syntactic structure (e.g., data flow graph) into the self-attention module of Transformer as\ninductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to\ndistribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance\nof SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0%\nin terms of METEOR score, a metric widely used for measuring generation quality, respectively on two benchmark datasets.\nCCS Concepts: â€¢ Software and its engineering â†’Software creation and management ; Software development techniques .\nAdditional Key Words and Phrases: Code summary, Transformer, multi-head attention, code structure.\nACM Reference Format:\nShuzheng Gao, Cuiyun Gao, Yulan He, Jichuan Zeng, Lun Yiu Nie, Xin Xia, and Michael R. Lyu. 2022. Code Structure Guided\nTransformer for Source Code Summarization. InWoodstock â€™18: ACM Symposium on Neural Gaze Detection, June 03â€“05, 2018, Woodstock,\nNY. ACM, New York, NY, USA, Article 1, 31 pages. https://doi.org/10.1145/3522674\n1 INTRODUCTION\nProgram comprehension is crucial for developers during software development and maintenance. However, existing\nstudies [42, 64] have shown that program comprehension is a very time-consuming activity which occupies over 50% of\nthe total time in software maintenance. To alleviate the developersâ€™ cognitive efforts in comprehending programs, a text\nsummary accompanying the source code is proved to be useful [9, 17, 27]. However, human-written comment is often\nincomplete or outdated because of the huge effort it takes and the rapid update of software [13, 49]. The source code\nâˆ—Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\nÂ© 2022 Association for Computing Machinery.\nManuscript submitted to ACM\n1\narXiv:2104.09340v2  [cs.CL]  22 Jul 2022\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\n1 public boolean IsPrime (int num1) {    \n2 int i1 =2;\n3 bool  flag1 = false; \n4 while( i2 < num2 ) {\n5 if(num3%i3 == 0) {\n6 flag2 = true;\n7 break;}        \n8 i5 = i4+1;\n9 }\n10 return flag3;\n11 }\nMethodDeclaration\nboolean\nIsPrime BlockParam\nVarDec WhileStmt ReturnVarDec\nint\nVariable Declarator\nnum\nBinary Expr:less BlockStmt\nIfStmt ExpressionStmt\nBlockstmtBinary Expr:equals\n0\nnum i\nBinary Expr:remainder\nPrimitive\nPrimitive\nVarDecld\n2\nInteger\nInteger\nName Name\nnum1\nnum2\nnum3\nflag1 flag2 flag3\ni1 i2 i3\ni4i5\n(a) An example of code snippet (c) The data flow graph (DFG)\n(b) The abstract syntax tree (AST)\nFig. 1. An example of Java code snippet (a), with the corresponding AST (b) and DFG (c) illustrated. Entities in grey ellipse in (b)\nmean unexpanded branches. The arrows in the DFG represent the relations of sending/receiving messages between the variables\n(highlighted in grey in the code).\nsummarization task aims at automatically generating a concise comment of a program. Many studies [11, 39, 53, 61] have\ndemonstrated that the machine-generated summaries are helpful for code comprehension. A recent empirical study [29]\nalso shows that 80% of practitioners consider that code summarization tools can help them improve development\nefficiency and productivity.\nExisting leading approaches have demonstrated the benefits of integrating code structural properties such as Abstract\nSyntax Trees (ASTs) [4, 27] into deep learning techniques for the task. An example of AST is shown in Figure 1 (b). The\nmodality of the code structure can be either sequences of tokens traversed from the syntactic structure of ASTs [4, 27] or\nsequences of small statement trees split from large ASTs [50, 68]. The sequences are usually fed into a Recurrent Neural\nNetwork (RNN)-based sequence-to-sequence network for generating a natural language summary [27, 36]. However,\ndue to the deep nature of ASTs, the associated RNN-based models may fail to capture the long-range dependencies\nbetween code tokens [1]. To mitigate this issue, some works represent the code structure as graphs and adopt Graph\n2\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nNeural Networks (GNNs) for summary generation [16, 35]. Although these GNN-based approaches can capture the\nlong-range relations between code tokens, they are shown sensitive to local information and ineffective in capturing\nthe global structure [30]. Taking the AST in Figure 1 (b) as an example, token nodes â€œintâ€ and â€œnumâ€ (highlighted with\nred boxes) are in the same statement but separated by five hops, so GNN-based approaches tend to ignore the relations\nbetween the two token nodes. Besides, the message passing on GNNs is limited by the pre-defined graph, reducing its\nscalability to learn other dependency relations.\nRecent study [1] shows that the Transformer model [55] outperforms other deep learning approaches for the task.\nThe self-attention mechanism in Transformer can be viewed as a fully-connected graph [22], which can ensure the\nlong-range massage passing between tokens and the flexibility to learn any dependency relation from data. However,\nit is hard for the Transformer to learn all important dependency relations from limited training data. Besides, an\nissue of Transformer is that its attention is purely data-driven [23]. Without the incorporation of explicit constraints,\nthe multi-head attentions in Transformer may suffer from attention collapse or attention redundancy, with different\nattention heads extracting similar attention features, which hinders the modelâ€™s representation learning ability [5, 56].\nTo solve the aforementioned problems, we incorporate code structure into the Transformer as prior information to\neliminate its dependency on data. However, how to effectively integrate the code structure information into Transformer\nis still under-explored. One major challenge is that since the position encoding in the Transformer already learns\nthe dependency relations between code tokens, trivial integration of the structure information may not bring an\nimprovement for the task [1].\nTo overcome the above challenges in this paper, we propose a novel model named SG-Trans, i.e., code Structure\nGuided Transformer. SG-Trans exploits the code structural properties to introduce explicit constraints to the multi-head\nself-attention module. Specifically, we extract the pairwise relations between code tokens based on the local symbolic\nstructure such as code tokens and statements, and the global syntactic structure, i.e., data flow graph (DFG), then\nrepresent them as adjacency matrices before injecting them into the multi-head attention mechanism as inductive bias.\nFurthermore, following the principle of compositionality in language: the high-level semantics is the composition of\nlow-level terms [23, 54], we propose a hierarchical structure-variant attention approach to guide the attention heads\nat the lower layers attending more to the local structure and those at the higher layers attending more to the global\nstructure. In this way, our model can take advantage of both local and global (long-range dependencies) information\nof source code. Experiments on benchmark datasets demonstrate that SG-Trans can outperform the state-of-the-art\nmodels by at least 1.4% and 2.0% in terms of METEOR on two Java and Python benchmark datasets, respectively.\nIn summary, our work makes the following contributions:\nâ€¢We are the first to explore the integration of both local and global code structural properties into Transformer\nfor source code summarization.\nâ€¢A novel model is proposed to hierarchically incorporate both the local and global structure of code into the\nmulti-head attentions in Transformer as inductive bias.\nâ€¢Extensive experiments show SG-Trans outperforms the state-of-the-art models. We publicly release the replication\nrepository including source code, datasets, prediction logs, online questionnaire, and results of human evaluation\non GitHub1.\nPaper structure. Section 2 illustrates the background knowledge of the work. Section 3 presents our proposed\nmethodology for source code summarization. Section 4 introduces the experimental setup. Section 5 describes the\n1https://github.com/gszsectan/SG-Trans\n3\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\nevaluation results, followed by the discussions in Section 6. Section 7 presents related studies. Finally, Section 8 concludes\nthe paper and outlines future research work.\n2 BACKGROUND\nIn this section, we introduce the background knowledge of the proposed approach, including the vanilla Transformer\nmodel architecture and the copy mechanism.\n2.1 Vanilla Transformer\nTransformer [55] is a kind of deep self-attention network which has demonstrated its powerful text representation\ncapability in many NLP applications, e.g., machine translation and dialogue generation [ 51, 69]. Recently, a lot of\nresearch in code summarization also leverage Transformer as backbone for better source code representations [1, 15].\nSome work [26, 58, 70] also improves the Transformer to make it better adapt to source code or structured data. Unlike\nconventional neural networks such as Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN),\nit is solely based on attention mechanism and multi-layer perceptrons (MLPs). Transformer follows the sequence-to-\nsequence [10] architecture with stacked encoders and decoders. Each encoder block and decoder block consist of a\nmulti-head self-attention sub-layer and a feed-forward sub-layer. Residual connection [25] and layer normalization [6]\nare also employed between the sub-layers. Since the two sub-layers play an essential role in Transformer, We introduce\nthem in more detail as the following.\n2.1.1 Multi-Head Self-Attention. Multi-head attention is the key component of Transformer. Given an input sequence\nð‘‹ = (ð‘¥1,ð‘¥2,...,ð‘¥ ð‘–,...,ð‘¥ ð‘›)where ð‘›is the sequence length and each input token ð‘¥ð‘– is represented by a ð‘‘-dimension vector,\nself-attention first calculates the Query vector, the Key vector, and the Value vector for each input token by multiplying\nthe input vector with three matrices ð‘Šð‘ž, ð‘Šð‘˜, ð‘Šð‘£. Then it calculates the attention weight of sequence ð‘‹ by scoring\nthe query vector ð‘„ against the key vector ð¾ of the input sentence. The scoring process is conducted by the scaled dot\nproduct, as shown in Eq. (2), where the dimensionð‘‘in the denominator is used to scale the dot product. Softmax is then\nused to normalize the attention score and finally the output vectors is computed as a weighted sum of the input vectors.\nInstead of performing a single self-attention function, Transformer adopts multi-head self-attention (MHSA) which\nperforms the self-attention function with different parameters in parallel and ensembles the output of each head by\nconcatenating their outputs. The MHSA allows the model to jointly attend to information from different representation\nsubspaces at different positions. Formally, the MHSA is computed as following:\nð‘„ð‘– = ð‘‹ð‘Šð‘ž\nð‘– , ð¾ ð‘– = ð‘‹ð‘Šð‘˜\nð‘– , ð‘‰ ð‘– = ð‘‹ð‘Šð‘£\nð‘– , (1)\nâ„Žð‘’ð‘Žð‘‘ð‘– = softmax(\nð‘„ð‘–ð¾ð‘‡\nð‘–âˆš\nð‘‘\n)ð‘‰ð‘–, (2)\nð‘€ð»ð‘†ð´(ð‘‹)= [â„Žð‘’ð‘Žð‘‘ð‘™\n1 â—¦â„Žð‘’ð‘Žð‘‘ð‘™\n2 â—¦Â·Â·Â· â„Žð‘’ð‘Žð‘‘ð‘™\nð‘– â—¦Â·Â·Â· â„Žð‘’ð‘Žð‘‘ð‘™\nâ„Ž]ð‘Šð‘‚, (3)\nwhere â„Ždenotes the number of attention heads atð‘™-th each layer, the symbolâ—¦indicates the concatenation ofâ„Ždifferent\nheads, and ð‘Šð‘ž\nð‘– , ð‘Šð‘˜\nð‘– , ð‘Šð‘£\nð‘– and ð‘Šð‘‚ are trainable parameters.\n2.1.2 Feed-Forward Network. Feed-forward network is the only nonlinear part in Transformer. It consists of two linear\ntransformation layer and a ReLU activation function between the two linear layers:\nð¹ð¹ð‘ (ð‘‹)= ð‘…ð‘’ð¿ð‘ˆ(ð‘‹ð‘Š1 +ð‘1)ð‘Š2 +ð‘2, (4)\n4\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nCopy distribution ð‘·ð’„ð’ð’‘ð’š\nVocabulary distribution ð‘·ð’—ð’ð’„ð’‚ð’ƒ\n.  .  .\nFinal distribution ð‘·\nLinear\nSoftmax\nð‘·ð’ˆð’†ð’\nðŸâˆ’ð‘·ð’ˆð’†ð’ ð‘·ð’ˆð’†ð’\nContext \nVector\nEncoder Decoder\n.  .  ..  .  .\nð‘ ð‘¡\nð‘¤ð‘¡\nð’„ð’•\nFig. 2. Architecture of copy mechanism.\nwhere ð‘Š1, ð‘Š2, ð‘1, and ð‘2 are trainable parameters which are shared across input positions.\n2.2 Copy Mechanism\nCopy mechanism [19] has been widely equipped in text generation models for extracting words from a source sequence\nas part of outputs in a target sequence during text generation. It has been demonstrated that copy mechanism can\nalleviate the out-of-vocabulary issue in the code summarization task [1, 67, 70]. Besides, copying some variable name\ncan also help generate more precise summary. In this work, we adopt the pointer generator [48], a more popular form\nof copy mechanism, for the task. Figure 2 illustrates the architecture of the pointer generator model. Specifically, given\nan input sequence ð‘‹ = (ð‘¥1,ð‘¥2,...,ð‘¥ ð‘›), a decoder input ð‘¤ð‘¡, a decoder hidden state ð‘ ð‘¡, and a context vector ð‘ð‘¡ computed\nby the attention mechanism in time step ð‘¡, the pointer generator first calculates a constant ð‘ƒð‘”ð‘’ð‘› which is later used\nas a soft switch for determining whether to generate a token from the vocabulary or to copy a token from the input\nsequence ð‘‹:\nð‘ƒð‘”ð‘’ð‘› = sigmoid(ðœ”T\nð‘ ð‘ ð‘¡ +ðœ”T\nð‘¤ð‘¤ð‘¡ +ðœ”T\nð‘ð‘ð‘¡ +ð‘ð‘”ð‘’ð‘›), (5)\nð‘ƒð‘£ð‘œð‘ð‘Žð‘(ð‘¤ð‘¡)= softmax(ð‘Šð‘Žð‘ ð‘¡ +ð‘‰ð‘Žð‘ð‘¡) (6)\nð‘ƒ(ð‘¤ð‘¡)= ð‘ƒð‘”ð‘’ð‘›ð‘ƒð‘£ð‘œð‘ð‘Žð‘(ð‘¤ð‘¡)+( 1 âˆ’ð‘ƒð‘”ð‘’ð‘›)ð‘ƒð‘ð‘œð‘ð‘¦ (ð‘¤ð‘¡), (7)\n5\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\nStructure-Guided \nSelf-Attention\nFeed Forward\nEncoder\nAdd & Norm\nPositional Encoding\nSoftmax\nSource Code\nCode Summary\n(b) Token-guided head attention\n(c) Statement-guided head attention\n(d) Data flow-guided head attention\nÃ—ð¿\nInput Embedding\nDecoder\ntoken token token tokentokentoken\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nstatement statement statement\nâ€¦\nâ€¦\nAdd & Norm\nParser\n(a) Standard head attention\ntoken\nstatement\ndata flow\nâ€¦\nâ€¦\nStructure-Guided Self-Attention\nHierarchical Scale-\nVariant Attentionâ€¦\nâ€¦\nÃ—ð¿\nHierarchical Structure-Variant \nAttention\nstandard\nFig. 3. Overall framework of the proposed SG-Trans. The â€œStructure-Guide Self-Attentionâ€ part illustrates different self-attention\nmechanisms between adjacent layers.\nwhere vectors ðœ”ð‘ , ðœ”ð‘¤, ðœ”ð‘, ð‘Šð‘Ž, ð‘‰ð‘Ž and scalar ð‘ð‘”ð‘’ð‘› are learnable parameters. ð‘ƒ(ð‘¤ð‘¡)is the probability distribution over\nthe entire vocabulary. Copy distribution ð‘ƒð‘ð‘œð‘ð‘¦ (ð‘¤ð‘¡)determines where to attend to in time step ð‘¡, computed as:\nð‘ƒð‘ð‘œð‘ð‘¦ (ð‘¤ð‘¡)=\nâˆ‘ï¸\nð‘–:ð‘¥ð‘– =ð‘¤\nð›¼ð‘¡,ð‘– , (8)\nwhere ð›¼ð‘¡ indicates the attention weights and ð‘– : ð‘¥ð‘– = ð‘¤ indicates the indices of input words in the vocabulary.\n3 PROPOSED APPROACH\nIn this section, we explicate the detailed architecture of SG-Trans. Let ð· denotes a dataset containing a set of programs\nð¶ and their associated summaries ð‘, given source code ð‘ = (ð‘¥1,ð‘¥2,...,ð‘¥ ð‘›)from ð¶, where ð‘›denotes the code sequence\nlength. SG-Trans is designed to generate the summary consisting of a sequence of tokens Ë†ð‘§ = (ð‘¦1,ð‘¦2,...,ð‘¦ ð‘š)by\nmaximizing the conditional likelihood: Ë†ð‘§ = arg maxð‘§ð‘ƒ(ð‘§|ð‘)(ð‘§is the corresponding summary in ð‘).\nThe framework of SG-Trans is mostly consistent with the vanilla Transformer, but consists of two major improvements,\nnamely structure-guided self-attention and hierarchical structure-variant attention. Figure 3 depicts the overall architecture.\nSG-Trans first parses the input source code for capturing both local and global structure. The structure information\nis then represented as adjacency matrices and incorporated into the self-attention mechanism as inductive biases\n(introduced in Section 3.1). Following the principle of compositionality in language, different inductive biases are\nintegrated into the Transformer at difference levels in a hierarchical manner (introduced in Section 3.2).\n6\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\n3.1 Structure-Guided Self-Attention\nIn the standard multi-head self-attention model [55], every node in the adjacent layer is allowed to attend to all the\ninput nodes, as shown in Figure 3 (a). In this work, we propose to use the structural relations in source code to introduce\nexplicit constraints to the multi-head self-attention. In order to capture the hierarchical structure of source code, we\nutilize three main types of structural relations between code tokens, including local structures: whether the two split\nsub-tokens originally belong to the same 1) token or 2) statement, and global structure: whether there exists a 3) data\nflow between two tokens. For each structure type, we design the corresponding head attention, named as token-guided\nself-attention, statement-guided self-attention , and data flow-guided self-attention , respectively.\nToken-guided self-attention. Semantic relations between the sub-tokens are relatively stronger than the relations\nbetween the other tokens. For example, the method name â€œIsPrimeâ€ in the code example shown in Figure 1 (a) is split as\na sequence of sub-tokens containing â€œIsâ€ and â€œPrimeâ€. Moreover, the semantic relation between them is stronger than\nthe relation between â€œIsâ€ and â€œnumâ€ in the same statement. Therefore, the attention can be built upon the extracted\ntoken-level structure, i.e, whether two sub-tokens are originally from the same source code token. We use an adjacency\nmatrix Tð‘›Ã—ð‘› to model the relationship, where ð‘¡ð‘–ð‘— = 0 if the ð‘–-th and ð‘—-th elements are sub-tokens of the same token in\nthe code; Otherwise, ð‘¡ð‘–ð‘— = âˆ’âˆž. The matrix is designed to restrict the attention head to only attend to the sub-tokens\nbelonging to the same code token in self-attention, as shown in Figure 3 (b). Given the input token representation\nX âˆˆRð‘›Ã—ð‘‘â„Ž, where ð‘›is the sequence length, ð‘‘ is the input dimension of each head, and â„Žis the number of attention\nheads. Let Q, K, and V denote the query, key, and value matrix, respectively, the token-guided single-head self-attention\nâ„Žð‘’ð‘Žð‘‘ð‘¡ can be calculated as:\nâ„Žð‘’ð‘Žð‘‘ð‘¡ = softmax\n\u0012QKâŠº\nâˆš\nð‘‘\n+T\n\u0013\nV, (9)\nwhere\nâˆš\nð‘‘ is a scaling factor to prevent the effect of large values.\nStatement-guided self-attention. Tokens in the same statement tend to possess stronger semantic relations than\nthose from different statements. For the code example given in Figure 1 (a), the token â€œflagâ€ in the third statement is\nmore relevant to the tokens â€œboolâ€ and â€œFalseâ€ in the same statement than to the token â€œbreakâ€ in the 7-th statement. So\nwe design another adjacency matrix ð‘† to represent the pairwise token relations capturing whether the two tokens are\nfrom the same statement. In the matrix S, ð‘ ð‘–ð‘— = 0 if the ð‘–-th and ð‘—-th input tokens are in the same statement; otherwise,\nð‘ ð‘–ð‘— = âˆ’âˆž. The design is to restrict the attention head to only attend to the tokens from the same statement, as illustrated\nin Figure 3 (c). The statement-guided single-head self-attention â„Žð‘’ð‘Žð‘‘ð‘  is defined below, similar to the token-guided\nhead attention:\nâ„Žð‘’ð‘Žð‘‘ð‘  = softmax\n\u0012QKâŠº\nâˆš\nð‘‘\n+S\n\u0013\nV. (10)\nData flow-guided self-attention. To facilitate the model to learn the global semantic information from code, we\nemploy the data flow graphs (DFGs) for capturing the global semantic structure feature [21]. We do not involve ASTs\nas input since they are deeper in nature and contain more redundant information [3] than DFGs. DFGs, denoted as\nð‘‰ = {ð‘£1,ð‘£2,...}, can model the data dependencies between variables in the code, including message sending/receiving.\nFigure 1 (c) shows an example of the extracted data flow graph. Variables with same name (e.g.,i2 and i5) are associated\nwith different semantics in the DFG. Each variable is a node in the DFG and the direct edgeâŸ¨ð‘£ð‘–,ð‘£ð‘—âŸ©from ð‘£ð‘– to ð‘£ð‘— indicates\nthe value of the ð‘—-th variable comes from the ð‘–-th variable. We can find that the semantic relations among â€œi2â€, â€œi3â€, â€œi4â€\n7\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\n4444\nConcat\nLinear\nâ„Žð‘’ð‘Žð‘‘ð‘¡\nð‘™\nâ„Žð‘’ð‘Žð‘‘ð‘ ð‘™\nâ„Žð‘’ð‘Žð‘‘ð‘“\nð‘™\nâ„Žð‘’ð‘Žð‘‘ð‘œð‘™\nðœ”ð‘¡\nð‘™\nðœ”ð‘ ð‘™\nðœ”ð‘“\nð‘™\nðœ”ð‘œð‘™\nâ„Ž\nFig. 4. A diagram of hierarchical-variant attention. Different red boxes illustrate different scales.\nand â€œi5â€ which represent the data sending/receiving in a loop. Based on the DFGs, we build the adjacency matrix D,\nwhere ð‘‘ð‘–ð‘— = 1 if there exists a message passing from the ð‘—-th token to the ð‘–-th token; Otherwise, ð‘‘ð‘–ð‘— = 0. Note that if\ntwo variables have a data dependency, then their constituent sub-tokens also possess the dependency relation. Figure 3\n(d) illustrates the data flow-guided single-head self-attention. To address the sparseness of the matrixD and to highlight\nthe relations of data dependencies, we propose the data flow-guided self-attention â„Žð‘’ð‘Žð‘‘ð‘“ below:\nâ„Žð‘’ð‘Žð‘‘ð‘“ = softmax\n\u0012QKâŠº +ðœ‡âˆ—QKâŠºDâˆš\nð‘‘\n\u0013\nV, (11)\nwhere ðœ‡is the control factor for adjusting the integration degree of the data flow structure.\n3.2 Hierarchical Structure-Variant Attention\nInspired by the principle of compositionality in logic semantics: the high-level semantics is the composition of low-level\nterms [23, 54], we propose a hierarchical structure-variant attention such that our model would focus on local structures\nat the lower layers and global structure at the higher layers. The diagram of the hierarchical structure-variant attention\nis illustrated in Figure 4. Specifically, the token-guided head attention â„Žð‘’ð‘Žð‘‘ð‘¡ and the statement-guided head attention\nâ„Žð‘’ð‘Žð‘‘ð‘  are used more in the heads of lower layers; while the data flow-guided head attention â„Žð‘’ð‘Žð‘‘ð‘“ is more spread in\nthe heads of higher layers.\nLet ð¿denote the number of layers in the proposed SG-Trans, â„Žindicate the number of heads in each layer and ð‘˜ be a\nhyper-parameter to control the distribution of four types of head attentions, including â„Žð‘’ð‘Žð‘‘ð‘¡, â„Žð‘’ð‘Žð‘‘ð‘ , â„Žð‘’ð‘Žð‘‘ð‘“, and â„Žð‘’ð‘Žð‘‘ð‘œ,\nwhere â„Žð‘’ð‘Žð‘‘ð‘œ indicates the standard head attention without constraints, the distribution for each type of head attention\nat the ð‘™-th layer is denoted as Î©ð‘™ = [ðœ”ð‘™\nð‘¡,ðœ”ð‘™ð‘ ,ðœ”ð‘™\nð‘“,ðœ”ð‘™ð‘œ], where ðœ”ð‘™\nð‘¡, ðœ”ð‘™ð‘ , ðœ”ð‘™\nð‘“, and ðœ”ð‘™ð‘œ represent the numbers of â„Žð‘’ð‘Žð‘‘ð‘¡, â„Žð‘’ð‘Žð‘‘ð‘ ,\nâ„Žð‘’ð‘Žð‘‘ð‘“, and â„Žð‘’ð‘Žð‘‘ð‘œ, respectively at the ð‘™-th layer. We define the distribution below:\nðœ”ð‘™\nð‘¡ = ðœ”ð‘™\nð‘  = âŒŠâ„Žâˆ— ð‘˜âˆ’ð‘™\n2 âˆ—ð‘˜âˆ’ð‘™âŒ‹, (12)\nðœ”ð‘™\nð‘“ = âŒŠâ„Žâˆ— ð‘™\n2 âˆ—ð‘˜âˆ’ð‘™âŒ‹, (13)\nðœ”ð‘™\nð‘œ = â„Žâˆ’(ðœ”ð‘™\nð‘¡ +ðœ”ð‘™\nð‘  +ðœ”ð‘™\nð‘“), (14)\n8\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nTable 1. Statistics of the benchmark datasets.\nJava Python\nTraining Set 69,708 55,538\nValidation Set 8,714 18,505\nTest Set 8,714 18,502\nTotal 87,136 92,545\nwhere ð‘˜ is a positive integer hyperparameter, and âŒŠÂ·âŒ‹denotes rounding the value down to the next lowest integer. The\ndesign is to enable more heads attending to the global structure with the growth of ð‘™, i.e., ðœ”ð‘™\nð‘“ will get larger at a higher\nlayer ð‘™; meanwhile few heads can catch the local structure, i.e., ðœ”ð‘™\nð‘¡ and ðœ”ð‘™ð‘  will become smaller. â„Žð‘’ð‘Žð‘‘ð‘œ is involved to\nenable the model to be adapted to arbitrary numbers of layers and heads. Especially, with the increase of layerð‘™, ðœ”ð‘™\nð‘¡ and\nðœ”ð‘™ð‘  might drop to zero. In the case ofðœ”ð‘™\nð‘¡ â‰¤0, no constraints will be introduced to the corresponding attention layer since\nthe standard self-attention already captures long-range dependency information, which fits our purpose of attending to\nglobal structure at higher layers. Otherwise, the head attentions will follow the defined distribution Î©ð‘™.\nThe hierarchical structure-variant attention (HSVA) at theð‘™-th layer is computed as:\nHSVAð‘™ = [â„Žð‘’ð‘Žð‘‘ð‘™\n1 â—¦Â·Â·Â·â—¦ â„Žð‘’ð‘Žð‘‘ð‘™\nâ„Ž]Wð‘‚, (15)\nwhere â—¦denotes the concatenation of â„Ždifferent heads, and Wð‘‚ âˆˆRð‘‘â„ŽÃ—ð‘‘â„Ž is a parameter matrix.\n3.3 Copy Attention\nThe OOV issue is important for effective code summarization [ 34]. We adopt the copy mechanism introduced in\nSection 2.2 in SG-Trans to calculate whether to generate words from the vocabulary or to copy from the input source\ncode. Following Ahmad et al. [1], an additional attention layer is added to learn the copy distribution on top of the\ndecoder [46]. The mechanism enables the proposed SG-Trans to copy low-frequency words, e.g., API names, from\nsource code, thus mitigating the OOV issue.\n4 EXPERIEMENTAL SETUP\nIn this section, we introduce the evaluation datasets and metrics, comparison baselines, and parameter settings.\n4.1 Benchmark Datasets\nWe conduct experiments on two benchmark datasets that respectively contain Java and Python source code following\nthe previous work [1, 67]. Specifically, the Java dataset publicly released by Hu et al. [27] comprises 87,136 âŸ¨Java method,\ncommentâŸ©pairs collected from 9,714 GitHub repositories, and the Python dataset consists of 92,545 functions and\ncorresponding documentation as originally collected by Barone et al. [8] and later processed by Wei et al. [61].\nFor fair comparison, we directly use the benchmarks open sourced by the previous related studies [ 1, 28, 60], in\nwhich the datasets are split into training set, validation set, and test set in a proportion of 8 : 1 : 1and 6 : 2 : 2for Java\nand Python, respectively. We follow the commonly-used dataset split strategy with no modification to avoid any bias\nintroduced by dataset split.\nWe apply CamelCase and snake_case tokenizers [1] to get sub-tokens for both dataset. As for the code statements, we\napply a simple rule to extract the statements of code snippet. For the extraction of statements from Java dataset, we split\neach code snippet into statements with separators including â€™{â€™, â€™}â€™ and â€™;â€™. The token sequence between two adjacent\n9\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\nseparators is considered as a statement. For the example shown in Figure 1 (a), each line except the separators is one\nstatement. While for the Python dataset, we define a statement by the row, which means that the tokens in the same\nrow are considered as belonging to the same statement. For the extraction of data flow from the Java dataset, we use the\ntool in GSC [12] to first generate augmented ASTs and then from which extract DFGs. Regarding the Python dataset,\nwe follow the setup in Allamanis et al. â€™s work [3] and extract four kinds of edge (LastRead, LastWrite, LastLexicalUse,\nComputeFrom) from code.\n4.2 Evaluation Metrics\nTo verify the superiority of SG-Trans over the baselines, we use the most commonly-used automatic evaluation metrics,\nBLEU-4 [47], METEOR [7] and ROUGE-L [37].\nBLEU is a metric widely used in natural language processing and software engineering fields to evaluate generative\ntasks (e.g., dialogue generation, code commit message generation, and pull request description generation) [33, 40, 45, 66].\nBLEU uses ð‘›-gram for matching and calculates the ratio of ð‘ groups of word similarity between generated comments\nand reference comments. The score is computed as:\nðµð¿ð¸ð‘ˆ âˆ’ð‘ = ðµð‘ƒ Ã—exp(\nð‘âˆ‘ï¸\nð‘›=1\nðœð‘›log ð‘ƒð‘›), (16)\nwhere ð‘ƒð‘› is the ratio of the subsequences with length ð‘›in the candidate that are also in the reference. ðµð‘ƒ is the brevity\npenalty for short generated sequence and ðœð‘› is the uniform weight 1/ð‘. We use corpus-level BLEU-4, i.e.,ð‘ = 4, as\nour evaluation metric since it is demonstrated to be more correlated with human judgements than other evaluation\nmetrics [38].\nMETEOR is a recall-oriented metric which measures how well our model captures content from the reference text\nin our generated text. It evaluates generated text by aligning them to reference text and calculating sentence-level\nsimilarity scores.\nð‘€ð¸ð‘‡ð¸ð‘‚ð‘… = (1 âˆ’ð›¾ Â·fragð›½)Â· ð‘ƒ Â·ð‘…\nð›¼Â·ð‘ƒ+(1 âˆ’ð›¼)Â·ð‘…, (17)\nwhere P and R are the unigram precision and recall, frag is the fragmentation fraction. ð›¼, ð›½ and ð›¾ are three penalty\nparameters whose default values are 0.9, 3.0 and 0.5, respectively.\nROUGE-L is widely used in text summarization tasks in the natural language processing field to evaluate what\nextent the reference text is recovered or captured by the generated text. ROUGE-L is based on the Longest Common\nSubsequence (LCS) between two text and the F-measure is used as its value. Given a generated text ð‘‹ and the reference\ntext ð‘Œ whose lengths are ð‘šand ð‘›respectively, ROUGE-L is computed as:\nð‘ƒð‘™ð‘ð‘  = ð¿ð¶ð‘†(ð‘‹,ð‘Œ )\nð‘› ,ð‘…ð‘™ð‘ð‘  = ð¿ð¶ð‘†(ð‘‹,ð‘Œ )\nð‘š ,ð¹ð‘™ð‘ð‘  = (1 +ð›½2)ð‘ƒð‘™ð‘ð‘ ð‘…ð‘™ð‘ð‘ \nð‘…ð‘™ð‘ð‘  +ð›½2ð‘ƒð‘™ð‘ð‘ \n, (18)\nwhere ð›½ = ð‘ƒð‘™ð‘ð‘ /ð‘…ð‘™ð‘ð‘  and ð¹ð‘™ð‘ð‘  is the computed ROUGE-L value.\n4.3 Baselines\nWe compare SG-Trans with following baseline approaches.\nCODE-NN [31], as the first deep-learning-based work in code summarization, generates source code summaries\nwith an LSTM network. To utilize code structure information, Tree2seq [14] encodes source code with a tree-LSTM\narchitecture. Code2seq [4] represents the code snippets by sampling paths from the AST. RL+Hybrid2Seq [57]\n10\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nincorporates ASTs and code sequences into a deep reinforcement learning framework, while DeepCom [27] encodes\nthe node sequences traversed from ASTs to capture the structural information.API+Code [28] involves API knowledge\nin the code summarization procedure. Dual model [61] adopts a dual learning framework to exploit the duality of code\nsummarization and code generation tasks. One of the most recent approaches, denoted as NeuralCodeSum [1], which\nintegrates the vanilla Transformer [55] with relative position encoding (RPE) and copy attention. Another recent\napproach Transformer+GNN [11] applies graph convolution to obtain structurally-encoded node representations and\npasses sequences of the graph-convolutioned AST nodes into Transformer.\nWe also compare our approach with relational Transformers [26, 70] which involve structural information for code\nrepresentation learning. GREAT [26] biases vanilla Transformers with relational information from graph edge types.\nCodeTransformer [70] focuses on multilingual code summarization and proposes to build upon language-agnostic\nfeatures such as source code and AST-based features.\nDuring implementation, we either directly copy the results claimed in the original papers or reproduce the results\nstrictly following the released repositories for most baselines except for GREAT and CodeTransformer. For GREAT [26],\n12 types of information including control flow graph and syntactic features are adopted for model training, as no\nreplication package is available. Due to the difficulty of complete replication, we follow the strategy in ZÃ¼gner et al. â€™s\nwork [70] by employing the same structural information as SG-Trans during replication. For CodeTransformer, although\na replication package is provided by the authors, not all the benchmark data can be successfully preprocessed. For Java,\nonly 61,250 of 69,708 code snippets in the training set, 7,621 of 8,714 in the validation set, and 7,643 of 8,714 in the\ntest set pass the preprocessing step; while for Python, all the code snippets can be well preprocessed. To ensure the\nconsistency of evaluation data, we compare SG-Trans with CodeTransformer on the Java dataset separately. We use the\nsame model settings for implementing CodeTransformer, including the layer number, head number, etc.\n4.4 Parameter Settings\nSG-Trans is composed of 8 layers and 8 heads in its Transformer architecture and the hidden size of the model is\n512. We use Adam optimizer with the initial learning rate set to 10âˆ’4, batch size set to 32, and dropout rate set to 0.2\nduring the training. We train our model for at most 200 epochs and select the checkpoint with the best performance\non the validation set for further evaluation on the test set. We report the performance of SG-Trans and each ablation\nexperiment by running three times and taking the average. To avoid over-fitting, we early stop the training if the\nperformance on the validations set does not increase for20 epochs. For the control factors of heads distribution and data\nflow, we set them to 1 and 5, respectively. We will discuss optimal parameters selection in Section 5.3. Our experiments\nare conducted on a single Tesla V100 GPU for about 30 hours, and we train our model from scratch.\n5 EXPERIMENTAL RESULTS\nIn this section, we elaborate on the comparison results with the baselines to evaluate SG-Transâ€™s capability in accurately\ngenerating code summaries. Our experiments are aimed at answering the following research questions:\nRQ1: What is the performance of SG-Trans in code summary generation?\nRQ2: What is the impact of the involved code structural properties and the design of hierarchical attentions on the\nmodel performance?\nRQ3: How accurate is SG-Trans under different parameter settings?\n11\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\nTable 2. Comparison results with baseline models. The bold figures indicate the best results. â€œ*â€ denotes statistical significance in\ncomparison to the baseline models we reproduced (i.e., two-sided ð‘¡-test with ð‘-value< 0.01).\nApproach Java Python\nBLEU-4 METEOR ROUGE-L BLEU-4 METEOR ROUGE-L\nCODE-NN [31] 27.60 12.61 41.10 17.36 09.29 37.81\nTree2Seq [14] 37.88 22.55 51.50 20.07 08.96 35.64\nRL+Hybrid2Seq [57] 38.22 22.75 51.91 19.28 09.75 39.34\nDeepCom [27] 39.75 23.06 52.67 20.78 09.98 37.35\nAPI+Code [28] 41.31 23.73 52.25 15.36 08.57 33.65\nDual Model [60] 42.39 25.77 53.61 21.80 11.14 39.45\nCode2seq[4] 12.19 08.83 25.61 18.69 13.81 34.51\nVanilla Transformer [55] 44.20 26.83 53.45 31.34 18.92 44.39\nNeuralCodeSum [1] 45.15 27.46 54.84 32.19 19.96 46.32\nGREAT[26] 44.97 27.15 54.42 32.11 19.75 46.01\nCodeTransformer [70] â€“ â€“ â€“ 27.63 14.29 39.27\nTransformer+GNN[11] 45.49 27.17 54.82 32.82 20.12 46.81\nSG-Trans 45.89 * 27.85* 55.79* 33.04* 20.52* 47.01*\nTable 3. Comparison results with CodeTransformer on the dataset preprocessed by CodeTransformer. The bold figures indicate the\nbest results. â€œ*â€ denotes statistical significance in comparison to the baseline models (i.e., two-sided ð‘¡-test with ð‘-value< 0.01).\nApproach Java\nBLEU-4 METEOR ROUGE-L\nCodeTransformer [70] 39.81 24.22 51.96\nSG-Trans 44.59 * 27.32* 54.41*\n5.1 Answer to RQ1: Comparison with the Baselines\nThe experimental results on the benchmark datasets are shown in Table 2. For the vanilla Transformer and the\nNeuralCodeSum [1], we reproduce their experiments under the same hyper-parameter settings as the Transformer in\nSG-Trans to ensure fair comparison. We compare SG-Trans with CodeTransformer [70] on the Java dataset separately,\nin which both approaches are trained and evaluated on the same dataset, with results shown in Table 3. Based on\nTable 2 and Table 3, we summarize the following findings:\nCode structural properties are beneficial for source code summarization. Comparing Tree2Seq/DeepCom\nwith CODE-NN, we can find that the structure information brings a great improvement in the performance. For example,\nboth Tree2Seq and DeepCom outperform CODE-NN by at least 37.2%, 78.8%, and 25.3% respectively regarding the three\nmetrics on the Java dataset. Although no consistent improvement across all metrics is observed on the Python dataset,\nTree2Seq/DeepCom still shows an obvious increase on the BLEU-4 metric.\nTransformer-based approaches perform better than RNN-based approaches. The four Transformer-based\napproaches [1, 4, 11, 26, 55] outperform all the other baselines, with NeuralCodeSum [1] giving better performance\ncompared to the vanilla Transformer. The vanilla Transformer already achieves better performance than the top\nseven RNN-based approaches with various types of structural information being incorporated, showing the efficacy\nof Transformer for the task. On the Python dataset, NeuralCodeSum outperforms the best RNN-based baseline, Dual\nModel [61], by 47.7% and 17.4% in terms of the BLEU-4 and ROUGE-L metrics, respectively.\n12\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nTable 4. Ablation study on different part of our model. The bold figures indicate the best results. â€œ*â€ denotes statistical significance\nin comparison to the baseline models we reproduced (i.e., two-sided ð‘¡-test with ð‘-value< 0.01).\nApproach Java Python\nBLEU-4 METEOR ROUGE-L BLEU-4 METEOR ROUGE-L\nSG-Trans w/o token info. 44.92 27.35 54.69 32.18 19.87 46.14\nSG-Trans w/o statement info. 44.61 27.08 54.04 32.26 19.66 46.08\nSG-Trans w/o data flow info. 45.52 27.65 55.40 32.58 20.16 46.57\nSG-Trans w/o hierarchical attention 45.53 27.72 55.48 32.93 20.38 46.73\nSG-Trans w/o copy attention 45.24 27.49 55.01 31.89 19.26 45.31\nSG-Transsoft 45.37 27.65 55.09 32.77 19.96 46.74\nSG-Trans 45.89 * 27.85* 55.79* 33.04* 20.52* 47.01*\nThe proposed SG-Trans is effective in code summarization. Comparing SG-Trans with the NeuralCodeSum\nand Transformer+GNN, SG-Trans achieves the best results on both benchmark datasets, yet without introducing any\nextra model parameters. Specifically, SG-Trans improves the best baseline by 1.7% and 0.4% in terms of ROUGE-L score\non the Java and Python dataset, respectively.\nThe combination of the structural information in SG-Trans is effective. By comparing SG-Trans with other\nTransformer models with structural information involved such as GREAT [ 26], CodeTransformer [70] and Trans-\nformer+GNN [11], SG-Trans achieves the best results on both benchmark datasets. Specifically, SG-Trans improves the\nbest baseline by 2.5% and 2.0% in terms of METEOR score on the Java and Python dataset, respectively.\n5.2 Answer to RQ2: Ablation Study\nWe further perform ablation studies to validate the impact of the involved code structural properties and the hierarchical\nstructure-variant attention approach. Besides, to evaluate the efficacy of the hard mask attention mechanism for\ncombining token-level and statement-level information in SG-Trans, we create a comparative approach, named as\nSG-Transsoft, by changing the hard mask into soft mask. Specifically, for SG-Transsoft, we follow NeuralCodeSum [1],\nand only add the relative position embedding for subtoken pairs ð‘¥ð‘– and ð‘¥ð‘— if they are in the same token or statement.\nThe results are shown in Table 4.\nAnalysis of the involved code structure. We find that all the three structure types, including code token, statement\nand data flow, contribute to the model performance improvement but with varied degrees. Specifically, local syntactic\nstructures play a more important role than the global data flow structure. For example, removing the statement\ninformation leads to a significant performance drop at around 2.8% and 2.4% regarding the BLEU-4 score. This suggests\nthe importance of modeling the semantic relations among tokens of the same statement for code summarization. With\nthe data flow information eliminated, SG-Trans also suffers from a performance drop, which may indicate that the data\ndependency relations are hard to be learnt by Transformer implicitly.\nAnalysis of the hierarchical structure-variant attention mechanism. We replace the hierarchical structure-\nvariant attention with uniformly-distributed attention, i.e., Î©ð‘™ = [ðœ”ð‘™\nð‘¡,ðœ”ð‘™ð‘ ,ðœ”ð‘™\nð‘“,ðœ”ð‘™ð‘œ]= [2,2,2,2], for the ablation analysis.\nAs can be found in Table 2, without the hierarchical structure design, the modelâ€™s performance decreases on all metrics\nfor both datasets. The results demonstrate the positive impact of the hierarchical structure-variant attention mechanism.\nAnalysis of the copy attention. As shown in Table 2, excluding the copy attention results in a significant drop\nto SG-Transâ€™s performance, similar to what has been observed in Ahmad et al. â€™s work [1]. This shows that the copy\nattention is useful for alleviating the OOV issue and facilitating better code summarization.\n13\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\nAnalysis of the hard mask attention mechanism. As shown in Table 4, we can find that SG-Trans performs\nconstantly better than SG-Transsoft on both Java and Python datasets with respect to all the metrics. For example, on\nJava dataset replacing hard-mask with soft mask leads to a performance drop at 1.1% and 1.3% in terms of the BLEU-4\nand ROUGE-L metrics, respectively, which indicates that the hard mask attention is effective at capturing the local\ninformation.\n45\n45.3\n45.6\n45.9\n1 3 5 7 9\n27.4\n27.6\n27.8\n1 3 5 7 9\n54.8\n55.1\n55.4\n55.7\n1 3 5 7 9\n32.8\n33\n33.2\n1 3 5 7 9\n20.3\n20.4\n20.5\n20.6\n1 3 5 7 9\n46.5\n46.7\n46.9\n47.1\n1 3 5 7 9\nBLEU-4METEORROUGE-L\n(a) Analysis of the parameter ðœ‡.\n45.3\n45.5\n45.7\n45.9\nL-2 L-1 L L+1 L+2 L+3\n27.5\n27.6\n27.7\n27.8\n27.9\nL-2 L-1 L L+1 L+2 L+3\n55\n55.3\n55.6\n55.9\nL-2 L-1 L L+1 L+2 L+3\n32.4\n32.6\n32.8\n33\nL-2 L-1 L L+1 L+2 L+3\n20.1\n20.3\n20.5\n20.7\nL-2 L-1 L L+1 L+2 L+3\n46.2\n46.5\n46.8\n47.1\nL-2 L-1 L L+1 L+2 L+3\nBLEU-4METEORROUGE-L (b) Analysis of the parameter ð‘˜. ð¿= 8.\nFig. 5. Influence of the hyper-parameters ðœ‡and ð‘˜on the model performance.\n5.3 Answer to RQ3: Parameter Sensitivity Analysis\nIn this section we analyze the impact of two key hyper-parameters on the model performance, i.e., the control factor ðœ‡\nfor adjusting the integration degree of the data flow structure and the parameter ð‘˜ to control the head distribution.\nThe parameter ðœ‡. Figure 5 (a) shows the performance variation with the changes of ðœ‡while keeping other hyper-\nparameters fixed. For the Java dataset, the model achieves the best scores when ðœ‡ = 5. Lower or higher parameter\nvalues do not give better results. While for the Python dataset, a similar trend is observed for the BLEU-4 and ROUGE-L\nmetrics where the model performs the best when ðœ‡equals to 3 and 5, respectively. In this work, we set ðœ‡to 5 since the\nmodel can produce relatively better results on both datasets.\nThe parameter ð‘˜. We observe the performance changes when the control factorð‘˜ of the head distribution takes\nvalues centered on layers of SG-Trans ð¿. Figure 5 (b) illustrates the results. We can find that SG-Trans can well balance\nthe distribution of local and global structure-guided head attention when ð‘˜ = ð¿or ð‘˜ = ð¿+1. As ð‘˜ gets larger, SG-Trans\nwould be more biased by the local structure and tend to generate inaccurate code summary. In our work here, we set\nð‘˜ = ð¿.\n5.4 Human Evaluation\nIn this section, we perform human evaluation to qualitatively evaluate the summaries generated by four Transformer-\nbased baselines, including vanilla Transformer, NeuralCodeSum, GREAT and CodeTransformer, and also our model\nSG-Trans. We do not involve the baseline Transformer+GNN, due to the lack of replication package. The human\nevaluation is conducted through online questionnaire. In total, 10 software developers are invited for evaluation. All\nparticipants have programming experience in software development for at least four years, and none of them is a\nco-author of the paper. Each participant is invited to read 60 code snippets and judge the quality of summaries generated\nby vanilla Transformer, NeuralCodeSum, CodeTransformer, GREAT and SG-Trans. Each of them will be paid 30 USD\nupon completing the questionnaire.\n14\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nCode:Â \npublicÂ staticÂ intÂ unixTimestamp(Â )\n{Â \nÂ Â Â Â returnÂ (int)(System.currentTimeMillis()/NUM)Â ;\n}\nSummary 1:Â returnÂ currentÂ timestampÂ (Â localÂ timeÂ )\nSummary 2:Â currentÂ theÂ currentÂ ofÂ theÂ fromÂ forÂ epochÂ fromÂ fromÂ from\nSummary 3:Â getÂ theÂ unixÂ timeÂ inÂ seconds\nSummary 4:Â returnÂ numberÂ asÂ aÂ timestampÂ inÂ theÂ localÂ epoch\nSummary 5:Â getÂ theÂ timestampÂ inÂ millisecondsÂ sinceÂ epoch\nSummary 1 s Adequary\nSummary 1 s Conciseness\nSummary 1 s Fluency\nSummary 5 s Adequary\nSummary 5 s Conciseness\nSummary 5 s Fluency\nâ€™\nVeryÂ Dissatisfied VeryÂ Satisfied\n1\n1\n2\n2\n3\n3\n4\n4\n5\n5â€™\nâ€™ 1\n1\n2\n2\n3\n3\n4\n4\n5\n5â€™\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nâ€™\nâ€™ 1 2 3 4 5\n1 2 3 4 5\nFig. 6. An example of questions in our questionnaire. The two-dot symbols indicate the simplified rating schemes for Summary 2,3,4.\nTable 5. Human evaluation results. The bold figures indicate the best results.\nDataset Metrics Transformer CodeTransformer NeuralCodeSum GREAT SG-Trans\nJava\nAdequacy 3.35 2.67 3.28 3.44 3.65\nConciseness 4.20 3.49 4.32 4.36 4.50\nFluency 4.32 3.25 4.36 4.50 4.59\nPython\nAdequacy 2.61 1.92 3.04 2.83 3.21\nConciseness 3.84 2.62 4.01 4.05 4.21\nFluency 4.06 2.39 4.21 4.26 4.33\n5.4.1 Survey Design. We randomly selected 200 code snippets, with 100 in Java and 100 in Python, for evaluation. As\nshown in Figure 6, in the questionnaire, each question comprises a code snippet and summaries generated by the five\nmodels. Each participant will be given 60 questions and each question will be evaluated by three different participants.\nFor each question, the summaries generated by the models are randomly shuffled to eliminate the order bias.\nThe quality of the generated summaries is evaluated in three aspects, including Adequacy, Conciseness, and Fluency,\nwith the 1-5 Likert scale (5 for excellent, 4 for good, 3 for acceptable, 2 for marginal, and 1 for poor). We explained\nthe meaning of the three evaluation metrics at the beginning of the questionnaire: The metric â€œadequacyâ€ measures\nhow much the functional meaning of the code is preserved after summarization; the metric â€œconcisenessâ€ measures the\nability to express the function of code snippet without unnecessary words; while the metric â€œfluencyâ€ measures the\nquality of the generated language such as the correctness of grammar.\n5.4.2 Survey Design. We finally received 600 sets of scores with 3 sets of scores for each code-summary pair from\nthe human evaluation. On average, the participants spent 2 hours on completing the questionnaire, with the median\n15\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nCodeTransformer Transformer NeuralCodeSum GREAT SGTrans\n1 2 3 4 5\nPercentage\n(a) Adequacy metric for the Java dataset.\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nCodeTransformer Transformer NeuralCodeSum GREAT SGTrans\n1 2 3 4 5\nPercentage (b) Adequacy metric for the Python dataset.\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nCodeTransformer Transformer NeuralCodeSum GREAT SGTrans\n1 2 3 4 5\nPercentage\n(c) Conciseness metric for the Java dataset.\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nCodeTransformer Transformer NeuralCodeSum GREAT SGTrans\n1 2 3 4 5\nPercentage (d) Conciseness metric for the Python dataset.\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nCodeTransformer Transformer NeuralCodeSum GREAT SGTrans\n1 2 3 4 5\nPercentage\n(e) Fluency metric for the Java dataset.\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nCodeTransformer Transformer NeuralCodeSum GREAT SGTrans\n1 2 3 4 5\nPercentage (f) Fluency metric for the Python dataset.\nFig. 7. Distribution of the rating scores in human evaluation on the two datsets. The â€œTransformerâ€ on the horizontal axis denotes the\nâ€œvanilla Transformerâ€ approach.\ncompletion time at 1.67 hours. The inter-annotator agreement of the two sets is evaluated with the widely-used metric\nCohenâ€™s kappa. The average Cohenâ€™s kappa scores for the Java and Python datasets are 0.66 and 0.58, respectively,\nindicating that the participants achieved at least moderate agreement on both datasets.\nThe evaluation results are illustrated in Table 5 and Figure 7. We find that the summaries generated by SG-Trans\nreceive the highest scores on both datasets and with respect to all the metrics. For the Java dataset, as shown in Table 5,\nSG-Trans improves the baseline models by at least 6.1%, 3.2% and 2.0% with respect to the adequacy, conciseness, and\nfluency metrics, respectively. As can be observed from Figure 7 (a), (c) and (e), summaries generated by SG-Trans receive\nthe most 5-star ratings and fewest 1/2-star ratings from the participants, comparing with the summaries produced by\nother models for each metric. Specifically, regarding the fluency metric, only 1.3% of the participants gave 1/2-star\nratings to the summaries generated by SG-Trans, while other approaches receive at least 6.0% 1/2-star ratings and\nCodeTransformer receives even 24.0%. The score distributions indicate that SG-Trans better captures the functionality\nof given code snippets and generates nature language comments with higher quality.\n16\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nTable 6. The quality of summaries generated by SG-Trans and humans. The term â€œauto-generatedâ€ indicates the summaries output\nby SG-Trans.\nPredicted Summary Test Java Python\nBLEU-4 METEOR ROUGE-L BLEU-4 METEOR ROUGE-L\nAuto-generated Reference 77.06 51.67 89.95 75.09 49.04 87.56\nHuman-generated Reference 19.04 13.33 32.83 23.63 19.82 38.95\nTable 7. The code snippets that cannot be understood by the annotators.\nExample (1) in Python:\ndef poly_TC(f, K):\nif (not f):\nreturn K.zero\nelse:\nreturn f[(-1)]\nHuman-generated: cannot understand\nSG-Trans: return trailing coefficient of f\nGround truth: return trailing coefficient of f\nExample (2) in Java:\n@SuppressWarnings(STRING)\npublic PropagationImp(Stack<CompositeTransaction>lineage, boolean serial, long timeout){\nserial_ = serial;\nlineage_ = (Stack<CompositeTransaction>)lineage.clone();\ntimeout_ = timeout;}\nHuman-generated: cannot understand\nSG-Trans: create a new instance\nGround truth: create a new instance\nFor the Python dataset, as shown in Table 5, NeuralCodeSum and GREAT significantly outperform the vanilla\nTransformer and CodeTransformer; while SG-Trans is more powerful, further boosting the best baseline approach\nby 5.6%, 4.0% and 1.6% in terms of the adequacy, conciseness, and fluency, respectively. As can be observed from\nFigure 7 (b), (d) and (f), summaries generated by SG-Trans receive the most 5-star ratings and fewest 1/2-star ratings\nfrom the annotators. Specifically, regarding the adequacy metric, 18.0% of the participants gave 5-star ratings to the\nsummaries generated by SG-Trans, with only 4.6% for the CodeTransformer approach, 12.3% for the strongest baseline\nNeuralCodeSum. For the conciseness metric, 51.7% of the participants gave 5-star ratings to the summaries generated\nby SG-Trans and the two best baseline approaches NeuralCodeSum and GREAT only receive 42.3% and 44.0% 5-star\nratings, respectively. The score distributions indicate that the summaries generated by SG-Trans can better describe the\nfunction of code snippets and have a more concise and accurate expression.\n5.5 Further Evaluation on Generated Summaries\nTo further investigate the quality of auto-generated summaries, we invite participants to summarize the code without\naccess to the reference summary, and then ask the annotators to score the summaries. During manual code summariza-\ntion, we invite four postgraduate students with more than five years of development experience as well as internship\nexperience in technology companies to participant. To ease the pressure of annotation, we randomly select 80 code\nsnippets with code lengths fewer than 250 characters. Each participant is asked to write summaries for 20 code snippets,\ni.e., 10 in Java and 10 in Python. Each of them will be paid 15 USD upon completing the questionnaire.\n17\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\nTable 8. Examples illustrating the difference between summaries generated by SG-Trans and written by human.\nExample (1) in Python:\ndef Thing2Literal(o, d):\nreturn string_literal(o, d)\nHuman-generated: return the literalness of a string\nSG-Trans: convert something into a string representation\nGround truth: convert something into a sql string literal\nExample (2) in Python:\ndef print_bucket_acl_for_user(bucket_name, user_email):\nstorage_client = storage.Client()\nbucket = storage_client.bucket(bucket_name)\nbucket.acl.reload()\nroles = bucket.acl.user(user_email).get_roles()\nprint roles\nHuman-generated: print the bucket acl for user\nSG-Trans: prints out a buckets access control list for a given user\nGround truth: prints out a buckets access control list for a given user\nExample (3) in Java:\npublic ActivityResolveInfo(ResolveInfo resolveInfo){\nthis.resolveInfo = resolveInfo;}\nHuman-generated: assign a value to resolveinfo attribute\nSG-Trans: creates a new activity\nGround truth: creates a new instance\nExample (4) in Java:\npublic staticDate parseText(String dateStr){\ntry {return mSimpleTextFormat.parse(dateStr);}\ncatch(ParseException e){\ne.printStackTrace();\nthrow new RuntimeException(STRING);}}\nHuman-generated: parse the dateStr as a date instance\nSG-Trans: parse string to datetime\nGround truth: parse string to datetime\nTable 9. Human evaluation on summaries generated by SG-Trans and human-written summaries.\nDataset Metrics Human-written SG-Trans\nJava\nAdequacy 4.38 3.62\nConciseness 4.82 4.48\nFluency 4.94 4.58\nPython\nAdequacy 4.39 3.24\nConciseness 4.86 4.23\nFluency 4.95 4.34\nWe totally receive annotations of 78/80 code snippets. For the remaining two code snippets, as shown in Table 7, the\nfunctionalities are hard to be understood by the annotators without corresponding prior knowledge. For the example in\nTable 7 (1), the unclear meanings of â€œK.zeroâ€ and â€œf[(-1)]â€ hinder the program comprehension. We measure the quality\nof summaries generated by SG-Trans and humans using the same metrics introduced in Section 4.2, respectively. The\nresults are shown in Table 6. From the table, we find that compared with human-generated summaries, auto-generated\n18\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nsummaries are much more similar to the reference summaries. For example, the BLEU-4 scores of the auto-generated\nsummaries are 77.06 and 75.09 on Java and Python, respectively, while the human-generated summaries are only 19.04\nand 23.63 on Java and Python, respectively. To analyze the reason of the large difference between human-generated\nsummaries and reference summaries, we manually check all the annotated data, and summarize two main reasons:\nâ€¢Lack of contextual knowledge. Some code snippets use external APIs or inner elements of a class, and the\ndetails of the APIs and elements cannot be accessed. So the annotators can only infer the functions of code\nsnippets based on the function/variable names, resulting in poorly-written summaries. For example, as shown in\nTable 8 (1), since the detail of the external API â€œstring_literalâ€ is unknown, humans can only guess its meaning\nfrom the name. In Table 8 (2), â€œaclâ€ is an inner element of the class â€œbucketâ€, but the definition is lacking, which\nmakes it hard for humans to comprehend the function. Our model has been provided with the knowledge that\nâ€œaclâ€ stands for â€œaccess control list â€ during training, so it can output a more accurate summary.\nâ€¢Limitation of the evaluation metrics. As shown in Table 8 (3) and (4), although the summaries generated\nby humans can accurately reflect the functions of the code snippets, they are significantly different from the\nreference summaries, leading to low metric scores. For the example in Table 8 (4), both the human-generated\nsummary and reference summary explain the meaning of the code snippet well. However, under the existing\nmetrics based on n-gram matching, the metric scores between them are very low since they have only one\noverlapping word â€œparseâ€.\nWe then qualitatively inspect the quality of human-generated summaries and auto-generated summaries. We invite\nanother three annotators, who have not joined the manual code summarization part, for the inspection. The results\nare illustrated in Table 9. The Cohenâ€™s kappa scores of the annotation results are 0.69 and 0.71 on Java and Python,\nrespectively, indicating a substantial inter-rater reliability on both datasets. As shown in Table 9, the quality of human-\ngenerated summaries is better than that of the auto-generated summaries with respect to all the metrics. Specifically,\nthe conciseness and fluency scores of Human-written summary are nearly 5 on both datasets. Moreover, the adequacy\nscores of human-written summaries outperform the auto-generated summaries by 21.0% and 35.5% on Java and Python\ndatasets, respectively. The results further explain the huge difference between human-generated summaries and\nreference summaries under automatic evaluation, as shown in Table 6, reflecting the limitation of automatic metrics.\n6 DISCUSSION\nIn this section, we mainly discuss the key properties of the proposed SG-Trans, the impact of duplicate data in the\nbenchmark dataset on the model performance, and the limitations of our study.\n6.1 Why Does Our Model Work?\nWe further conduct an analysis to gain insights into the proposed SG-Trans in generating high-quality code summaries.\nThrough qualitative analysis, we have identified two properties of SG-Trans that may explain its effectiveness in the\ntask.\nObservation 1: SG-Trans can better capture the semantic relations among tokens. From Example (1) shown\nin Table 10, we can observe that SG-Trans produces the summaries most similar to the ground truth among all the\napproaches, while the CodeTransformer gives the worst result. We then visualize the heatmap of the self-attention\nscores of the three types of heads in Figure 8 for a further analysis. As can be seen in Figure 8 (a) and (b), SG-Trans\n19\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\nTable 10. Examples illustrating summaries generated by different approaches given the code snippets. The following examples are\nonly from the test set and do not exist in the training set.\nExample (1) in Java:\npublic staticboolean isFile(String path){\nFile f =new File(path);\nreturn f.isFile();\n}\nVanilla Transformer: checks if the given path is a file object , is a directory it can be read . no distinction is\nconsidered exceptions\nCodeTransformer: checks if the given file is a file\nNeuralCodeSum: checks if is file exist\nGREAT: checks if the given path is a file\nSG-Trans: checks if the given path is a file\nGround truth: checks if the given path is a file\nExample (2) in Python:\ndef print_bucket_acl_for_user(bucket_name, user_email):\nstorage_client = storage.Client()\nbucket = storage_client.bucket(bucket_name)\nbucket.acl.reload()\nroles = bucket.acl.user(user_email).get_roles()\nprint roles\nVanilla Transformer: removes a user from the access control list\nCodeTransformer: sets a the user from to to .\nNeuralCodeSum: prints out a user access control list\nGREAT: prints out a user access control list\nSG-Trans: prints out a buckets access control list for a given user\nGround truth: prints out a buckets access control list for a given user\nExample (3) in Python:\ndef token_urlsafe(nbytes=None):\ntok = token_bytes(nbytes)\nreturn base64.urlsafe_b64encode(tok).rstrip(â€˜=â€™).decode(â€˜asciiâ€™)\nVanilla Transformer: generate a token\nCodeTransformer: decodes a unicode string string string if\nNeuralCodeSum: construct a random text string .\nGREAT: generates a token identifier\nSG-Trans: return a random url-safe string .\nGround truth: return a random url-safe text string .\ncan focus on local relations among code tokens through its token-guided self-attention and statement-guided self-\nattention. For example, SG-Trans can learn that the two tokens â€œisâ€ and â€œFileâ€ possess a strong relation, according to\nFigure 8 (a). As depicted in Figure 8 (b), we can find that SG-Trans captures that the token â€œpathâ€ is strongly related to\nthe corresponding statement, which may be the reason the token â€œpathâ€ appears in the summary. Figure 8 (c) shows\nthat the data flow-guided head attention focuses more on the global information, and can capture the strong relation\nbetween the tokens â€œpathâ€ and â€œf â€. Based on the analysis of the example (1), we speculate that the model can well\ncapture the token relations locally and globally for code summary generation. As for the heatmap of other baseline\nmodels like NeuralCodeSum, we can see that they are very different with the heatmap of SG-Trans. As shown in\nFigure 9, NeuralCodeSum does not capture the token, statement, and data flow information in any layer while SG-Trans\ncan pay more attention to the token pairs with syntactic or semantic relations. A similar conclusion can be drawn from\nExample (2) in Table 10. All the approaches successfully comprehend that the token â€œaclâ€ indicates â€œaccess control list â€.\n20\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\npublic -\nstatic -\nboolean -\nis -\nFile -\n( -\nString -\npath -\n) -\n{ -\nFile -\nf -\n= -\nnew -\nFile -\n( -\npath -\n) -\n; -\nreturn -\nf -\n. -\nis -\nFile -\n( -\n) -\n; -\n} -\npublic -\nstatic -\nboolean -\nis -\nFile -\n( -\nString -\npath -\n) -\n{ -\nFile -\nf -\n= -\nnew -\nFile -\n( -\npath -\n) -\n; -\nreturn -\nf -\n. -\nis -\nFile -\n( -\n) -\n; -\n} -\npublic -\nstatic -\nboolean -\nis -\nFile -\n( -\nString -\npath -\n) -\n{ -\nFile -\nf -\n= -\nnew -\nFile -\n( -\npath -\n) -\n; -\nreturn -\nf -\n. -\nis -\nFile -\n( -\n) -\n; -\n} -\npublic -\nstatic -\nboolean -\nis -\nFile -\n( -\nString -\npath -\n) -\n{ -\nFile -\nf -\n= -\nnew -\nFile -\n( -\npath -\n) -\n; -\nreturn -\nf -\n. -\nis -\nFile -\n( -\n) -\n; -\n} -\n-1.0\n-0.8\n-0.6\n-0.4\n-0.2\n-0.0\n(a) Token-guided head attention at layer-3 (b) Statement-guided head attention at layer-3 (c) Data flow-guided head attention at layer-6\nFig. 8. Heatmap visualization of self-attention scores of the three types of heads in the encoder for the first case in Table 10. The\nrectangles with red edge, green edge, and blue edge indicates the tokens belonging to the same original token, the same statement, or\ncontaining data flow relation, respectively.\npublic -\nstatic -\nboolean -\nis - \nFile - \n( - \nString - \npath - \n) - \n{ - \nFile -\nf -\n= - \nnew - \nFile - \n( - \npath - \n) -\n; - \nreturn - \nf - \n. - \nis - \nFile - \n( - \n) - \n; - \n} -\npublic -\nstatic -\nboolean -\nis - \nFile - \n( - \nString - \npath - \n) - \n{ - \nFile -\nf - \n= - \nnew - \nFile - \n( - \npath - \n) -\n; - \nreturn - \nf - \n. - \nis - \nFile - \n( - \n) - \n; - \n} -\n-1.0\n-0.8\n-0.6\n-0.4\n-0.2\n-0.0\n(a) Attention heatmap at layer-3 (b) Attention heatmap at layer-3 (c) Attention heatmap at layer-6\npublic -\nstatic -\nboolean -\nis - \nFile - \n( - \nString - \npath - \n) - \n{ - \nFile -\nf - \n= - \nnew - \nFile - \n( - \npath - \n) -\n; - \nreturn - \nf - \n. - \nis - \nFile - \n( - \n) - \n; - \n} -\npublic -\nstatic -\nboolean -\nis - \nFile - \n( - \nString - \npath - \n) - \n{ - \nFile -\nf - \n= - \nnew - \nFile - \n( - \npath - \n) -\n; - \nreturn - \nf - \n. - \nis - \nFile - \n( - \n) - \n; - \n} -\nFig. 9. Heatmap visualization of self-attention scores of NeuralCodeSum. The rectangles with red edge, green edge, and blue edge\nindicates the tokens belonging to the same original token, the same statement, or containing data flow relation, respectively.\nHowever, the vanilla Transformer fails to capture the semantic relations between â€œprintâ€ and â€œaclâ€, and NeuralCodeSum\nmisunderstands the relations between â€œuserâ€ and â€œaclâ€. Instead, SG-Trans accurately predicates both relations through\nthe local self-attention and global self-attention.\nObservation 2: Structural information-guided self-attention can facilitate the copy mechanism to copy\nimportant tokens. In Example (3) in Table 10, SG-Trans successfully identified the important token â€œurlsafeâ€ in the\ngiven code while generating the summary. But both vanilla Transformer and NeuralCodeSum ignored the token and\noutput less accurate summaries. The reason that the important token is successfully copied by SG-Trans may be\nattributed to the structural information-guided self-attention which helps focus on the source tokens more accurately.\n21\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\nTable 11. Duplicate data in the Java dataset.\nValidation Set Test set\nTotal data 8,714 8,714\nDuplicate data 2,028 (23.3%) 2,059 (23.6%)\nTable 12. Comparison results on the de-duplicated Java dataset. Data listed within brackets are computed drop rates compared with\nthe results on original Java dataset.\nApproach BLEU-4 ROUGE-L METEOR\nNeuralCodeSum 29.37 ( â†“34.95%) 41.62 ( â†“24.11%) 19.98 ( â†“27.24%)\nGREAT 29.49 ( â†“34.52%) 41.84 ( â†“23.12%) 20.15 ( â†“25.79%)\nSG-Trans 30.46 (â†“33.74%) 42.97 (â†“23.07%) 20.82 (â†“25.32%)\n6.2 Duplicate Data in the Java Dataset\nDuring our experimentation, we find that there are duplicate data in the Java dataset, which may adversely affect\nthe model performance [ 2]. As for the Python dataset, there is no duplication across different training, validation\nand test set. As shown in Table 11, there are 23.3% and 23.7% duplicate data in the validation set and the test set,\nrespectively. To evaluate the impact of the data duplication on the proposed model, we remove the duplicate data\nacross the training, validation, and test sets. We choose the two strongest baselines, NeuralCodeSum and GREAT,\nfor comparison. The results after deduplication are shown in Table 12. As can be seen, all models present a dramatic\ndecrease on the de-duplicated dataset. Nevertheless, the proposed SG-Trans still outperforms GREAT on the BLEU-4,\nROUGE-L and METEOR metrics, i.e., by 3.3%, 2.7% and 3.3%, respectively.\n6.3 Analysis of the Hierarchical Structure-Variant Attention Mechanism\nThe hierarchical structure-variant attention mechanism in SG-Trans aims at rendering the model focus on local\nstructures at shallow layers and global structure at deep layers. In the section, we analyze whether the mechanism can\nassist SG-Trans learning the hierarchical information. We visualize the distributions of attention scores corresponding\nto the relative token distances for the shallow layer â€“ Layer one, middle layer â€“ Layer four, and one deep layer â€“ Layer\nseven, respectively. Specifically, for each relative token distanceðœ„, its attention distribution ð‘Œðœ„ is computed as Equ. (19).\nð‘Œðœ„ =\nÃð‘\nð‘–=1 attention(ð‘–,ð‘– +ðœ„)+attention(ð‘–,ð‘– âˆ’ðœ„)\nÃð‘†\nð‘—=1\nÃð‘\nð‘–=1 attention(ð‘–,ð‘– +ð‘—)+attention(ð‘–,ð‘– âˆ’ð‘—)\n(19)\nwhere ð‘ denotes the number of tokens and ð‘† denotes the longest relative distance for analysis (ð‘† = 10 in our analysis).\nThe attention(ð‘–,ð‘— ) denotes the attention score of token ð‘¥ð‘– to ð‘¥ð‘— (1 â‰¤ð‘— â‰¤ð‘). The attention scores reflect whether the\nmodel focuses on local information or global information. We choose the relational Transformer GREAT for comparison\nsince it also involves structural information but is not designed hierarchically. The visualization is depicted in Figure 10.\nFor GREAT, as shown in Figure 10 (a), we find that the attention distributions across different layers present similar\ntrends, i.e., they all tend to focus on different token distances uniformly. For SG-Trans, as shown in Figure 10 (b), we can\nobserve that the three layers pay various attentions to tokens of different relative distances. The shallow layer (Layer\none) more focuses on tokens with short relative distances. In the middle layer (Layer four), the attention distribution\namong different distances is more balanced, which indicates that the model pays increasingly more attention to global\ntokens with the layer depth being increased. For the deep layer (Layer seven), the attention scores for tokens of long\n22\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\n(a) GREAT.\n (b) SG-Trans.\nFig. 10. Attention distributions regarding relative token distance for (a) GREAT and (b) SG-Trans. The horizontal axis represents the\nrelative distance to the current token, and the vertical axis denotes the normalized attention scores along with relative distances in\none layer.\ndistances are larger than those of short distances, meaning that the model tends to focus on long-range relations in\ndeep layer. The results demonstrate that the hierarchical attention mechanism in SG-Trans is beneficial for the model to\ncapture the hierarchical structural information which cannot be easily learned by the relational Transformer.\n6.4 Difference with Relational Transformers\nThe main differences between SG-Trans and the relational Transformers [26, 70] are mainly in two aspects:\n(1) Strategy in incorporating structural information. Compared with GREAT [26] and CodeTransformer [70]\nwhich use learnable bias and sinusoidal encoding function to encode the structure information, respectively,\nSG-Trans incorporates the local and global structure information with different strategies, e.g., introducing the\nlocal information with hard mask. The results in Table 2 and Table 4 show the effectiveness of the structural\nincorporation strategy in SG-Trans.\n(2) Design of hierarchical structure-variant attention mechanism. In SG-Trans, a hierarchical attention mech-\nanism is designed to assist model learning the hierarchical information; while the relational Transformers do not\ninvolve such design. Both the ablation study in Section 5.2 and the discussion in Section 6.3 demonstrate the\nbenefits of the design.\n6.5 Difference with GraphCodeBERT\nSG-Trans takes data flow information which is similar to GraphCodeBERT [21]. However, the two methods are different\nin the following aspects.\n(1) Role of data flow. GraphCodeBERT mainly uses the data flow in two ways: (1) filtering the irrelevant signals\nin the variable sequence, (2) guiding the two pre-training tasks, including edge prediction and node alignment.\nNevertheless, SG-Trans directly uses data flow information to help the attention mechanism better capture the\ndata dependency relations in source code.\n(2) Incorporation way of data flow. We integrate the data flow information in a different way as GraphCodeBERT.\nGraphCodeBERT utilizes a sparse masking mechanism to mask the attention relations between the tokens without\n23\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\ndata dependency. However, SG-Trans retains the attention relations for the tokens without data dependency, and\nalso highlights the data flow dependency through our designed data flow-guided self-attention.\n(3) Targets of the proposed model. The targets of the two models are different. GraphCodeBERT is proposed to\nutilize the inherent structure of code to facilitate the pre-training stage. However, SG-Trans mainly focuses on\nthe task without large amount of source code available and uses the incorporated code structure to alleviate the\ndependency on the training data.\nTable 13. Comparison of the cost of different models. The â€œ_ â€ under the preprocessing time of NeuralCodeSum and CodeBERT\ndenotes that the approaches do not need preprocess. The â€œ_ â€ under the training time of CodeBERT denotes that we do not reproduce\nthe pre-training stage due to the limitation of computing resource.\nGPU memory usage Training time Preprocessing time\nNeuralCodeSum 8729M 30.4h -\nCodeTransformer 8573M 211.5h 66.1ms\nSG-Trans 8509M 29.9h 3.8ms\nCodeBERT 17959M - -\n6.6 Analysis of the Complexity of SG-Trans\nSG-Trans incorporates structural information based on three types of relations between code tokens. Comparing with\nthe baseline approaches, such as NeuralCodeSum, SG-Trans involves more types of relations, which could lead to an\nincrease in the model complexity and subsequently impacting its applicability to other programming languages. To\ninvestigate to what extent SG-Trans introduces extra complexity, we conduct analysis of the cost of SG-Trans.\nSpecifically, we first compare the cost of SG-Trans with Transformer-based baselines including NeuralCodeSum and\nCodeTransformer, and a pre-training model CodeBERT, in terms of the GPU memory usage, training time cost and\npreprocessing time cost. The comparison is implemented on the same server with a single Tesla V100 GPU by training\non the Java dataset with 32 batch size. The results are shown in Table 13. As can be seen, the GPU memory usage and\ntraining time cost of SG-Trans are the lowest among all the approaches. Since SG-Trans does not involve the relative\npositive embedding, both its GPU memory usage and training time cost are even lower than NeuralCodeSum. Table 13\nalso shows that CodeBERT requires the highest memory usage, restricting its application to low-resource devices. With\nrespect to the preprocessing time cost for one sample, since SG-Trans does not need calculate the complex features\nused in CodeTransformer such as shortest path length and personalized PageRank, it only takes about 3.8ms which is\nsignificantly faster than CodeTransformer. The results indicate that the code structure properties used in SG-Trans do\nnot bring larger cost than the baselines.\nFor the application of SG-Trans to other programming languages, the main barrier lies in the data flow extraction\nprocedure. In this work, we follow the main idea of Wang et al. â€™s work [59] and only consider the common data flow\ninformation which is generally similar for different programming languages. The common data flow information\nincludes sequential data flow relations and three types of non-sequential data flow relations such as â€œif â€ statements,\nâ€œforâ€ and â€œwhileâ€ loops. The sequential data flow relations can be easily extracted by identifying the variables for any\nprogramming language. For the non-sequential data flow relations, the extraction procedure of different programming\nlanguages is also similar. Because the AST parser tree-sitter2 can parse the data flow relations of different languages\ninto almost the same tree structure. Thus, it is convenient to extend SG-Trans to other popular languages.\n2https://github.com/tree-sitter/tree-sitter\n24\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nTable 14. Comparison results with CodeBERT. The â€œCodeBERTâ€ represents the the CodeBERT approach without fine tuning. The\nâ€œSG-Translargeâ€ represents SG-Trans with the same model settings as CodeBERT, i.e., 10 encoder layers and hidden size as 768.\nApproach Java Python\nBLEU-4 METEOR ROUGE-L BLEU-4 METEOR ROUGE-L\nCodeBERT[15] 14.93 9.23 30.43 16.70 9.68 30.31\nCodeBERT+fine-tune[15] 44.40 28.33 55.56 32.04 20.77 47.45\nSG-Trans 45.89 27.85 55.79 33.04 20.52 47.01\nSG-Translarge 46.27 28.37 56.30 33.53 20.87 47.42\n6.7 Comparison with CodeBERT\nMany pre-training models [15, 21] have been proposed recently, which can be adopted for source code summarization.\nSo we also compare the performance of SG-Trans with the most typical pre-training model CodeBERT. We also train\nSG-Trans under the same model size as CodeBERT and denote it as SG-Translarge for further comparison.\nAs shown in Table 14, without fine tuning, CodeBERT shows the worst performance among all the approaches. After\nenough fine tuning, CodeBERT improves a lot and even outperforms SG-Trans on some metrics, e.g., the METEOR\nscore on the Java dataset. However, it should be noted that the encoder layer number and hidden size of CodeBERT\nare much larger than SG-Trans. Specifically, the numbers of encoder layers and hidden size of CodeBERT are 10\nand 768, respectively; while SG-Trans only has 8 encoder layers and hidden size as 512. For fair comparison, we\nalso train SG-Trans with the same model settings as CodeBERT, denoted as SG-Trans large. As shown in Table 14,\nSG-Translarge obtains the best performance almost all the metrics. Specifically, on the Java dataset, SG-Trans large\noutperforms CodeBERT+fine-tune by 4.2% and 1.3% in terms of BLEU-4 and ROUGE-L, respectively. And on the Python\ndataset, SG-Translarge is only a little bit lower than CodeBERT+fine-tune on ROUGE-L but obviously outperforms\nCodeBERT+fine-tune by 4.7% in terms of the BLEU-4 score. The results demonstrate that SG-Trans is more effective\nthan CodeBERT even with accessing to limited data.\n6.8 Threats to Validity\nThere are three main threats to the validity of our study.\n(1) The generalizability of our results. We use two public large datasets, which include 87,136 Java and 92,545 Python\ncode-summary pairs, following the prior research [1, 60, 67]. The limited types of programming languages may\nhinder the scalability of the proposed SG-Trans. In our future work, we will experiment with more large-scale\ndatasets with different programming languages.\n(2) More code structure information could be considered. SG-Trans only takes the token-level and statement-level\nsyntactic structure and the data flow structure into consideration, since it has been previously demonstrated\nthat the data flow information is more effective than AST and CFG during code representation learning [21].\nNevertheless, other code structural properties such as AST and CFG, could be potentially useful for boosting the\nmodel performance. In the future, we will explore the use of more structural properties in SG-Trans.\n(3) Biases in human evaluation. We invited 10 participants to evaluate the quality of 200 randomly selected code-\nsummary pairs. The results of human annotations can be impacted by the participantsâ€™ programming experience\nand their understanding of the evaluation metrics. To mitigate the bias of human evaluation, we ensure that the 10\nparticipants are all software developers with at least four years programming experience, and each code-summary\npair was evaluated by 3 participants. Summaries generated by different approaches were also randomly shuffled\n25\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\nin order to eliminate the order bias. In the future, we will expand the pool of human participants and will also\nincrease the size of the evaluation set.\n7 RELATED WORK\nIn this section, we elaborate on two threads of related work, including source code summarization, code representation\nlearning.\n7.1 Source Code Summarization\nThere have been extensive research in source code summarization, including template-based approaches [41, 43, 52],\ninformation-retrieval-based approaches [24, 44, 62] and deep-learning-based-approaches [ 4, 27, 31]. Among these\ncategories, deep-learning-based methods have achieved the greatest success and become the most popular in recent\nyears, which specifically formulate the code summarization task as a neural machine translation (NMT) problem and\nadopt state-of-the-art NMT frameworks to improve the performance. In this section, we focus on deep-learning-based\nmethods and introduce them by their category. We also list an overview of the category of related works in Table 15.\nRNN-based models: Iyer et al. [31] first propose CODE-NN, a Long Short Term Memory (LSTM) network with\nattention to generate code summaries from code snippets. In order to achieve more accurate code modeling, later\nresearchers then introduce more structural and syntactic information to the deep learning models. Hu et al. [27] propose\na structure-based traversal(SBT) method to traverse AST and processing the AST nodes into sequences that can be fed\ninto a RNN encoder. Another work [28] hold the view that code API carries vital information about the functionality of\nthe source code and incorporate the API knowledge by adding an API Sequences Encoder.\nTree/GNN-based models: To leverage the tree structures of AST, a multi-way Tree-LSTM [50] is proposed to directly\nmodel the code structures. For more fine-grained intra-code relationship exploitation, many works also incorporate\ncode-related graphs and GNN to boost performance. Fernandes et al. [16] build a graph from source code and extract\nnodes feature with gated graph neural network while LeClair et al. [35] directly obtain code representation from AST\nwith Convolutional Graph Neural Networks. To help model capture more global interactions among nodes, a recent\nwork [39] propose a hybrid GNN which fuse the information from static and dynamic graphs via hybrid message\npassing.\nTransformer-based models: With the rise of Transformer in NMT task domain, Ahmad et al. [1] equip transformer\nwith copy attention and relative position embedding for better mapping the source code to their corresponding\nnatural language summaries. To leverage the code structure information into Transformer, Hellendoorn et al. [ 26]\npropose GREAT which encode structural information into self attention with adding a learnable edge type related bias.\nAnother work proposed by ZÃ¼gner et al. [70] focuses on multilingual code summarization and proposes to build upon\nlanguage-agnostic features such as source code and AST-based features. Wu et al. [63] propose the Structure-induced\nSelf-Attention to incorporate multi-view structure information into self-attention mechanism. To capture both the\nsequential and structural information, a recent work [11] applies graph convolution to obtain structurally-encoded\nnode representations and passes sequences of the graph-convolutioned AST nodes into Transformer. Another recent\nwork [18] proposes to utilize AST relative positions to augment the structural correlations between code tokens.\nInformation retrieval auxiliary methods: The information retrieval auxiliary methods utilize information re-\ntrieval and large-scale code repositories to help model generate high-quality summary. Zhang et al. [67] propose to\nimprove code summarization with the help of two retrieved code using syntactic and semantic similarity. To help model\n26\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\nTable 15. An overview of existing works related to code summarization. The â€œIRâ€ and â€œMTâ€ denotes information retrieval auxiliary\nmethods and multi-task learning strategy respectively.\nApproach Input code property RNN Tree/GNN Transformer IR MT\nCODE-NN [31] â€“ âœ“\nDeepCom [27] AST âœ“\nCode2Seq [4] AST âœ“\nAPI+Code [28] API Information âœ“\nDual Model [60] â€“ âœ“ âœ“\nRencos [67] â€“ âœ“ âœ“\nRe2Com [61] â€“ âœ“ âœ“\nTree-LSTM [50] AST âœ“\nCode+GNN [35] AST âœ“\nHGNN [39] CPG âœ“ âœ“\nNeuralCodeSum [1] â€“ âœ“\nDMACOS [65] â€“ âœ“ âœ“\nGREAT [26] Multi relations in code âœ“\nCodeTransformer [70] AST âœ“\nTransformer+GNN [11] AST âœ“ âœ“\ngenerate more important but low-frequency tokens, Wei et al. [61] further use the existing comments of retrieved code\nsnippets as exemplars to guide the summarization.\nMulti-task learning strategy: Some research try to exploit commonalities and differences across code-related\ntasks to further improve the code summarization. Wei et al. [60] use a dual learning framework to apply the relations\nbetween code summarization and code generation and improve the performance of both tasks. A recent work [65] use\nmethod name prediction as an auxiliary task and design a multi-task learning approach to improve code summarization\nperformance.\nCompared with existing work, our proposed model focuses on improving the Transformer architecture for source\ncode to make it better incorporate both local and global code structure. Other improvement methods like information\nretrieval and multi-task learning which are orthogonal to our work are not the research target of this paper.\n7.2 Code Representation Learning\nLearning high-quality code representations is of vital importance for deep-learning-based code summarization. Apart\nfrom the above practices for code summarization, there also exist other code representation learning methods that\nlie in similar task domains such as source code classification, code clone detection, commit message generation, etc.\nFor example, the ASTNN model proposed by Zhang et al. [ 68] splits large ASTs into sequences of small statement\ntrees, which are further encoded into vectors as source code representations. This model is further applied on code\nclassification and code clone detection. Alon et al. [4] present CODE2SEQ that represents the code snippets by sampling\ncertain paths from the ASTs. Gu et al. [ 20] propose to encode statement-level dependency relations through the\nProgram Dependency Graph (PDG). Comparatively, the above research on the model architecture improvement for\ncode representation learning is relevant to us but mainly focuses on other code-related tasks such as code classification,\ncode clone detection, etc.\nRecently, inspired by the successes of pre-training techniques in natural language processing field, Feng et al. [15]\nand Guo et al. [21] also apply pre-training models on learning source code and achieve empirical improvements on a\nvariety of tasks. To extend the code representations to characterize programsâ€™ functionalities, [32] further enriches the\n27\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\npre-training tasks to learn task-agnostic semantic code representations from textually divergent variants of source\nprograms via contrastive learning. The target of the research about pre-training is greatly different from us, i.e., they\nmainly focus on learning from large-scale dataset in a self-supervised way. However, our research concentrates on\nimproving the quality of generated summaries with limited training data in a supervised way.\n8 CONCLUSION\nIn this paper, we present SG-Trans, a Transformer-based architecture with structure-guided self-attention and hierar-\nchical structure-variant attention. SG-Trans can attain better modeling of the code structural information, including\nlocal structure in token-level and statement-level, and global structure, i.e., data flow. The evaluation on two popular\nbenchmarks suggests that SG-Trans outperforms competitive baselines and achieves state-of-the-art performance on\ncode summarization. For future work, we plan to extend the use of our model to other task domains, and possibly build\nup more accurate code representations for general usage.\nACKNOWLEDGMENTS\nThis research was supported by National Natural Science Foundation of China under project No. 62002084, Stable\nsupport plan for colleges and universities in Shenzhen under project No. GXWD20201230155427003-20200730101839009,\nand the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14210920 of the\nGeneral Research Fund). This research was also partly funded by the UK Engineering and Physical Sciences Research\nCouncil (No. EP/V048597/1, EP/T017112/1). Yulan He is supported by a Turing AI Fellowship funded by the UK Research\nand Innovation (No. EP/V020579/1).\nREFERENCES\n[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A Transformer-based Approach for Source Code Summarization.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , Dan Jurafsky, Joyce Chai,\nNatalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 4998â€“5007.\n[2] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine learning models of code. In Proceedings of the 2019 ACM SIGPLAN\nInternational Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, Onward! 2019, Athens, Greece, October 23-24,\n2019, Hidehiko Masuhara and Tomas Petricek (Eds.). ACM, 143â€“153.\n[3] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning to Represent Programs with Graphs. In6th International Conference\non Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings . OpenReview.net.\n[4] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating Sequences from Structured Representations of Code. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net.\n[5] Bang An, Jie Lyu, Zhenyi Wang, Chunyuan Li, Changwei Hu, Fei Tan, Ruiyi Zhang, Yifan Hu, and Changyou Chen. 2020. Repulsive Attention:\nRethinking Multi-head Attention as Bayesian Inference. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2020, Online, November 16-20, 2020 , Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics,\n236â€“255.\n[6] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization. CoRR abs/1607.06450 (2016).\n[7] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In\nProceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor,\nMichigan, USA, June 29, 2005 , Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare R. Voss (Eds.). Association for Computational Linguistics, 65â€“72.\n[8] Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A parallel corpus of Python functions and documentation strings for automated code\ndocumentation and code generation. arXiv preprint arXiv:1707.02275 (2017).\n[9] Jie-Cherng Chen and Sun-Jen Huang. 2009. An empirical analysis of the impact of software development problem factors on software maintainability.\nJ. Syst. Softw. 82, 6 (2009), 981â€“992.\n[10] Kyunghyun Cho, Bart van Merrienboer, Ã‡aglar GÃ¼lÃ§ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning\nPhrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL , Alessandro\nMoschitti, Bo Pang, and Walter Daelemans (Eds.). ACL, 1724â€“1734.\n28\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\n[11] YunSeok Choi, JinYeong Bak, CheolWon Na, and Jee-Hyong Lee. 2021. Learning Sequential and Structural Information for Source Code Summarization.\nIn Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 (Findings of ACL, Vol. ACL/IJCNLP 2021) ,\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 2842â€“2851.\n[12] Milan Cvitkovic, Badal Singh, and Animashree Anandkumar. 2019. Open Vocabulary Learning on Source Code with a Graph-Structured Cache. In\nProceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA (Proceedings of Machine\nLearning Research, Vol. 97) , Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 1475â€“1485.\n[13] Sergio Cozzetti B. de Souza, Nicolas Anquetil, and KÃ¡thia MarÃ§al de Oliveira. 2005. A study of the documentation essential to software maintenance.\nIn Proceedings of the 23rd Annual International Conference on Design of Communication: documenting & Designing for Pervasive Information, SIGDOC\n2005, Coventry, UK, September 21-23, 2005 , Scott R. Tilley and Robert M. Newman (Eds.). ACM, 68â€“75.\n[14] Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2016. Tree-to-Sequence Attentional Neural Machine Translation. In Proceedings of\nthe 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers . The\nAssociation for Computer Linguistics.\n[15] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou.\n2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020 , Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for\nComputational Linguistics, 1536â€“1547.\n[16] Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Structured Neural Summarization. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net.\n[17] Golara Garousi, Vahid Garousi-Yusifoglu, GÃ¼nther Ruhe, Junji Zhi, Mahmood Moussavi, and Brian Smith. 2015. Usage and usefulness of technical\nsoftware documentation: An industrial case study. Inf. Softw. Technol. 57 (2015), 664â€“682.\n[18] Zi Gong, Cuiyun Gao, Yasheng Wang, Wenchao Gu, Yun Peng, and Zenglin Xu. 2022. Source Code Summarization with Structural Relative Position\nGuided Transformer. CoRR abs/2202.06521 (2022).\n[19] Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K. Li. 2016. Incorporating Copying Mechanism in Sequence-to-Sequence Learning. In Proceedings\nof the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers . The\nAssociation for Computer Linguistics.\n[20] Wenchao Gu, Zongjie Li, Cuiyun Gao, Chaozheng Wang, Hongyu Zhang, Zenglin Xu, and Michael R. Lyu. 2021. CRaDLe: Deep code retrieval based\non semantic Dependency Learning. Neural Networks 141 (2021), 385â€“394.\n[21] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano,\nShao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. 2020. GraphCodeBERT: Pre-training Code\nRepresentations with Data Flow. CoRR abs/2009.08366 (2020).\n[22] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. 2019. Star-Transformer. InProceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational\nLinguistics, 1315â€“1325.\n[23] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Xiangyang Xue, and Zheng Zhang. 2020. Multi-Scale Self-Attention for Text Classification. InThe Thirty-Fourth\nAAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The\nTenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 . AAAI Press, 7847â€“7854.\n[24] Sonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting program comprehension with source code summarization. In Proceedings of the\n32nd ACM/IEEE International Conference on Software Engineering - Volume 2, ICSE 2010, Cape Town, South Africa, 1-8 May 2010 , Jeff Kramer, Judith\nBishop, Premkumar T. Devanbu, and SebastiÃ¡n Uchitel (Eds.). ACM, 223â€“226.\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In2016 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016 . IEEE Computer Society, 770â€“778.\n[26] Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. 2020. Global Relational Models of Source Code. In 8th\nInternational Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.\n[27] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation. InProceedings of the 26th Conference on Program Comprehension,\nICPC 2018, Gothenburg, Sweden, May 27-28, 2018 , Foutse Khomh, Chanchal K. Roy, and Janet Siegmund (Eds.). ACM, 200â€“210.\n[28] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018. Summarizing Source Code with Transferred API Knowledge. In Proceedings of the\nTwenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden , JÃ©rÃ´me Lang (Ed.). ijcai.org,\n2269â€“2275.\n[29] Xing Hu, Xin Xia, David Lo, Zhiyuan Wan, Qiuyuan Chen, and Tom Zimmermann. 2022. Practitionersâ€™ Expectations on Automated Code Comment\nGeneration. In ICSE â€™22: Proceedings of the 44th ACM/IEEE International Conference on Software Engineering .\n[30] Chidubem Iddianozie and Gavin McArdle. 2020. Improved Graph Neural Networks for Spatial Networks Using Structure-Aware Sampling. ISPRS Int.\nJ. Geo Inf. 9, 11 (2020), 674.\n[31] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing Source Code using a Neural Attention Model. In\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long\nPapers. The Association for Computer Linguistics.\n29\nWoodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gao, et al.\n[32] Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph E. Gonzalez, and Ion Stoica. 2020. Contrastive Code Representation Learning. CoRR\nabs/2007.04973 (2020).\n[33] Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generating commit messages from diffs using neural machine translation. In\nProceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering, ASE 2017, Urbana, IL, USA, October 30 - November 03,\n2017, Grigore Rosu, Massimiliano Di Penta, and Tien N. Nguyen (Eds.). IEEE Computer Society, 135â€“146.\n[34] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and Andrea Janes. 2020. Big code != big vocabulary: open-vocabulary\nmodels for source code. InICSE â€™20: 42nd International Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020 , Gregg Rothermel\nand Doo-Hwan Bae (Eds.). ACM, 1073â€“1085.\n[35] Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Improved Code Summarization via a Graph Neural Network. In ICPC â€™20:\n28th International Conference on Program Comprehension, Seoul, Republic of Korea, July 13-15, 2020 . ACM, 184â€“195.\n[36] Yuding Liang and Kenny Qili Zhu. 2018. Automatic Generation of Text Descriptive Comments for Code Blocks. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018 , Sheila A. McIlraith and Kilian Q. Weinberger (Eds.). AAAI\nPress, 5229â€“5236.\n[37] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. InText Summarization Branches Out . Association for Computational\nLinguistics, Barcelona, Spain, 74â€“81.\n[38] Chia-Wei Liu, Ryan Lowe, Iulian Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT To Evaluate Your Dialogue\nSystem: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016 , Jian Su, Xavier Carreras, and Kevin Duh\n(Eds.). The Association for Computational Linguistics, 2122â€“2132.\n[39] Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang Liu. 2021. Retrieval-Augmented Generation for Code Summarization via Hybrid GNN.\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net.\n[40] Zhongxin Liu, Xin Xia, Christoph Treude, David Lo, and Shanping Li. 2019. Automatic Generation of Pull Request Descriptions. In 34th IEEE/ACM\nInternational Conference on Automated Software Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019 . IEEE, 176â€“188.\n[41] Paul W. McBurney and Collin McMillan. 2016. Automatic Source Code Summarization of Context for Java Methods. IEEE Trans. Software Eng. 42, 2\n(2016), 103â€“119.\n[42] Roberto Minelli, Andrea Mocci, and Michele Lanza. 2015. I know what you did last summer: an investigation of how developers spend their time. In\nProceedings of the 2015 IEEE 23rd International Conference on Program Comprehension, ICPC 2015, Florence/Firenze, Italy, May 16-24, 2015 , Andrea De\nLucia, Christian Bird, and Rocco Oliveto (Eds.). IEEE Computer Society, 25â€“35.\n[43] Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori L. Pollock, and K. Vijay-Shanker. 2013. Automatic generation of natural\nlanguage summaries for Java classes. In IEEE 21st International Conference on Program Comprehension, ICPC 2013, San Francisco, CA, USA, 20-21 May,\n2013. IEEE Computer Society, 23â€“32.\n[44] Dana Movshovitz-Attias and William W. Cohen. 2013. Natural Language Models for Predicting Programming Comments. In Proceedings of the 51st\nAnnual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria, Volume 2: Short Papers . The Association\nfor Computer Linguistics, 35â€“40.\n[45] Lun Yiu Nie, Cuiyun Gao, Zhicong Zhong, Wai Lam, Yang Liu, and Zenglin Xu. 2020. Contextualized Code Representation Learning for Commit\nMessage Generation. CoRR abs/2007.06934 (2020).\n[46] Kyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda, Atsushi Otsuka, Hisako Asano, and Junji Tomita. 2019. Multi-style Generative\nReading Comprehension. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-\nAugust 2, 2019, Volume 1: Long Papers , Anna Korhonen, David R. Traum, and LluÃ­s MÃ rquez (Eds.). Association for Computational Linguistics,\n2273â€“2284.\n[47] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. InProceedings\nof the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA . ACL, 311â€“318.\n[48] Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In Proceedings of\nthe 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers ,\nRegina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, 1073â€“1083.\n[49] Lin Shi, Hao Zhong, Tao Xie, and Mingshu Li. 2011. An Empirical Study on Evolution of API Documentation. InFundamental Approaches to Software\nEngineering - 14th International Conference, FASE 2011, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2011,\nSaarbrÃ¼cken, Germany, March 26-April 3, 2011. Proceedings (Lecture Notes in Computer Science, Vol. 6603) , Dimitra Giannakopoulou and Fernando\nOrejas (Eds.). Springer, 416â€“431.\n[50] Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi Miyamoto, and Tadayuki Matsumura. 2019. Automatic Source Code Summarization\nwith Extended Tree-LSTM. In International Joint Conference on Neural Networks, IJCNN 2019 Budapest, Hungary, July 14-19, 2019 . IEEE, 1â€“8.\n[51] Kai Song, Kun Wang, Heng Yu, Yue Zhang, Zhongqiang Huang, Weihua Luo, Xiangyu Duan, and Min Zhang. 2020. Alignment-Enhanced Transformer\nfor Constraining NMT with Pre-Specified Translations. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second\nInnovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2020, New York, NY, USA, February 7-12, 2020 . AAAI Press, 8886â€“8893.\n30\nCode Structure Guided Transformer for Source Code Summarization Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY\n[52] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori L. Pollock, and K. Vijay-Shanker. 2010. Towards automatically generating summary\ncomments for Java methods. In ASE 2010, 25th IEEE/ACM International Conference on Automated Software Engineering, Antwerp, Belgium, September\n20-24, 2010 , Charles Pecheur, Jamie Andrews, and Elisabetta Di Nitto (Eds.). ACM, 43â€“52.\n[53] Sean Stapleton, Yashmeet Gambhir, Alexander LeClair, Zachary Eberhart, Westley Weimer, Kevin Leach, and Yu Huang. 2020. A Human Study of\nComprehension and Code Summarization. In ICPC â€™20: 28th International Conference on Program Comprehension, Seoul, Republic of Korea, July 13-15,\n2020. ACM, 2â€“13.\n[54] ZoltÃ¡n Gendler SzabÃ³. 2020. Compositionality. In The Stanford Encyclopedia of Philosophy , Edward N. Zalta (Ed.).\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nAll you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December\n4-9, 2017, Long Beach, CA, USA , Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and\nRoman Garnett (Eds.). 5998â€“6008.\n[56] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing Multi-Head Self-Attention: Specialized Heads Do the\nHeavy Lifting, the Rest Can Be Pruned. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Papers , Anna Korhonen, David R. Traum, and LluÃ­s MÃ rquez (Eds.). Association for Computational\nLinguistics, 5797â€“5808.\n[57] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S. Yu. 2018. Improving automatic source code summarization via\ndeep reinforcement learning. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, ASE 2018, Montpellier,\nFrance, September 3-7, 2018 , Marianne Huchard, Christian KÃ¤stner, and Gordon Fraser (Eds.). ACM, 397â€“407.\n[58] Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2020. RAT-SQL: Relation-Aware Schema Encoding and\nLinking for Text-to-SQL Parsers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July\n5-10, 2020 , Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 7567â€“7578.\n[59] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. 2020. Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax\nTree. In SANER. IEEE, 261â€“271.\n[60] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code Generation as a Dual Task of Code Summarization. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada ,\nHanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Roman Garnett (Eds.). 6559â€“6569.\n[61] Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020. Retrieve and Refine: Exemplar-based Neural Comment Generation. In35th IEEE/ACM\nInternational Conference on Automated Software Engineering, ASE 2020, Melbourne, Australia, September 21-25, 2020 . IEEE, 349â€“360. https://doi.org/\n10.1145/3324884.3416578\n[62] Edmund Wong, Taiyue Liu, and Lin Tan. 2015. CloCom: Mining existing source code for automatic comment generation. In 22nd IEEE International\nConference on Software Analysis, Evolution, and Reengineering, SANER 2015, Montreal, QC, Canada, March 2-6, 2015 , Yann-GaÃ«l GuÃ©hÃ©neuc, Bram\nAdams, and Alexander Serebrenik (Eds.). IEEE Computer Society, 380â€“389.\n[63] Hongqiu Wu, Hai Zhao, and Min Zhang. 2021. Code Summarization with Structure-induced Transformer. In Findings of the Association for\nComputational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 (Findings of ACL, Vol. ACL/IJCNLP 2021) , Chengqing Zong, Fei Xia,\nWenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 1078â€“1090.\n[64] Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E. Hassan, and Shanping Li. 2018. Measuring Program Comprehension: A Large-Scale\nField Study with Professionals. IEEE Trans. Software Eng. 44, 10 (2018), 951â€“976.\n[65] Rui Xie, Wei Ye, Jinan Sun, and Shikun Zhang. 2021. Exploiting Method Names to Improve Code Summarization: A Deliberation Multi-Task Learning\nApproach. In 29th IEEE/ACM International Conference on Program Comprehension, ICPC 2021, Madrid, Spain, May 20-21, 2021 . IEEE, 138â€“148.\n[66] Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Graham Neubig, and Satoshi Nakamura. 2018. Guiding Neural Machine Translation with Retrieved\nTranslation Pieces. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers) . 1325â€“1335.\n[67] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020. Retrieval-based neural source code summarization. In ICSE â€™20: 42nd\nInternational Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020 , Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM,\n1385â€“1397.\n[68] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. 2019. A novel neural source code representation based on\nabstract syntax tree. In Proceedings of the 41st International Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019 ,\nJoanne M. Atlee, Tevfik Bultan, and Jon Whittle (Eds.). IEEE / ACM, 783â€“794.\n[69] Xiangyu Zhao, Longbiao Wang, Ruifang He, Ting Yang, Jinxin Chang, and Ruifang Wang. 2020. Multiple Knowledge Syncretic Transformer for\nNatural Dialogue Generation. In WWW â€™20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 , Yennun Huang, Irwin King, Tie-Yan Liu, and\nMaarten van Steen (Eds.). ACM / IW3C2, 752â€“762.\n[70] Daniel ZÃ¼gner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan GÃ¼nnemann. 2021. Language-Agnostic Representation Learning of\nSource Code from Structure and Context. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\n31",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8385002613067627
    },
    {
      "name": "Automatic summarization",
      "score": 0.6946759819984436
    },
    {
      "name": "Transformer",
      "score": 0.6353249549865723
    },
    {
      "name": "Source code",
      "score": 0.5319348573684692
    },
    {
      "name": "Computer engineering",
      "score": 0.3733801245689392
    },
    {
      "name": "Programming language",
      "score": 0.3711644411087036
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3584912419319153
    },
    {
      "name": "Data mining",
      "score": 0.32956281304359436
    },
    {
      "name": "Voltage",
      "score": 0.1010439395904541
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I39555362",
      "name": "University of Warwick",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250955327",
      "name": "Huawei Technologies (China)",
      "country": "CN"
    }
  ],
  "cited_by": 96
}