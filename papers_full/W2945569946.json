{
  "title": "Behavior Sequence Transformer for E-commerce Recommendation in Alibaba",
  "url": "https://openalex.org/W2945569946",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2347659683",
      "name": "Chen Qiwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127940345",
      "name": "Zhao Huan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1894711984",
      "name": "Li Wei",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Huang, Pipei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287524726",
      "name": "Ou, Wenwu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2803718882",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2946617802",
    "https://openalex.org/W2951581544",
    "https://openalex.org/W2936133855",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2783666221",
    "https://openalex.org/W2937556626",
    "https://openalex.org/W2962780037",
    "https://openalex.org/W2963601856",
    "https://openalex.org/W2723293840",
    "https://openalex.org/W2964182926",
    "https://openalex.org/W2963367478",
    "https://openalex.org/W2742940593",
    "https://openalex.org/W2793768763",
    "https://openalex.org/W2808787330",
    "https://openalex.org/W2512971201",
    "https://openalex.org/W2604662567"
  ],
  "abstract": "Deep learning based methods have been widely used in industrial recommendation systems (RSs). Previous works adopt an Embedding&amp;MLP paradigm: raw features are embedded into low-dimensional vectors, which are then fed on to MLP for final recommendations. However, most of these works just concatenate different features, ignoring the sequential nature of users' behaviors. In this paper, we propose to use the powerful Transformer model to capture the sequential signals underlying users' behavior sequences for recommendation in Alibaba. Experimental results demonstrate the superiority of the proposed model, which is then deployed online at Taobao and obtain significant improvements in online Click-Through-Rate (CTR) comparing to two baselines.",
  "full_text": "Behavior Sequence Transformer for E-commerce\nRecommendation in Alibaba\nQiwei Chen, Huan Zhao∗\nWei Li, Pipei Huang, Wenwu Ou\nAlibaba Search&Recommendation Group\nBeijing&Hangzhou, China\n{chenqiwei.cqw,chuanfei.zh,rob.lw,pipei.hpp,santong.oww}@alibaba-inc.com\nABSTRACT\nDeep learning based methods have been widely used in indus-\ntrial recommendation systems (RSs). Previous works adopt an Em-\nbedding&MLP paradigm: raw features are embedded into low-\ndimensional vectors, which are then fed on to MLP for final rec-\nommendations. However, most of these works just concatenate\ndifferent features, ignoring the sequential nature of users’ behav-\niors. In this paper, we propose to use the powerful Transformer\nmodel to capture the sequential signals underlying users’ behavior\nsequences for recommendation in Alibaba. Experimental results\ndemonstrate the superiority of the proposed model, which is then\ndeployed online at Taobao and obtain significant improvements in\nonline Click-Through-Rate (CTR) comparing to two baselines.\n1 INTRODUCTION\nDuring the last decade, Recommender Systems (RSs) have been the\nmost popular application in industry, and in the past five years,\ndeep learning based methods have been widely used in industrial\nRSs, e.g., Google [ 2, 3] and Airbnb [ 5]. In Alibaba, the largest e-\ncommerce platform in China, RSs have been the key engine for its\nGross Merchandise Volume (GMV) and revenues, and various deep\nlearning based recommendation methods have been deployed in\nrich e-commerce scenarios [1, 8, 10, 11, 14, 15, 17, 18]. As introduced\nin [15], the RSs in Alibaba are a two-stage pipeline: match and rank.\nIn match, a set of similar items are selected according to items users\ninteracted with, and then a fine-tuned prediction model is learned\nto predict the probability of users clicking the given set of candidate\nitems.\nIn this paper, we focus on the rank stage at Alibaba’s Taobao,\nChina’s largest Consumer-to-Consumer (C2C) platform owned by\nAlibaba, where we are have millions of candidate items, and we need\nto predict the probability of a user clicking the candidate items given\nhis/her historical behaviors. In the era of deep learning, embedding\nand MLP have been the standard paradigm for industrial RSs: large\nnumbers of raw features are embedded into low-dimensional spaces\nas vectors, and then fed into fully connected layers, known as multi\nlayer perceptron (MLP), to predict whether a user will click an item\nor not. The representative works are wide and deep learning (WDL)\nnetworks [2] from Google and Deep Interest networks (DIN) from\nAlibaba [17].\nAt Taobao, we build the rank model on top of WDL, where\nvarious features are used in the embedding&MLP paradigm , e.g.,\nthe category and brand of an item, the statistical numbers of an item,\nor the user profile features. Despite the success of this framework,\n∗ Qiwei Chen and Huan Zhao contribute equally to this work, and Pipei Huang is the\ncorresponding author.\nit is inherently far from satisfying since it ignores one type of very\nimportant signals in practice, i.e., the sequential signal underlying\nthe users’ behavior sequences, i.e., users’ clicked items in order. In\nreality, the order matters for predicting the future clicks of users.\nFor example, a user tends to click a case for a cellphone after he or\nshe bought an iphone at Taobao, or tries to find a suitable shoes after\nbuying a pair of trousers. In this sense, it is problematic without\nconsidering this factor when deploying a prediction model in the\nrank stage at Taobao. In WDL [ 2], they simply concatenates all\nfeatures without capturing the order information among users’\nbehavior sequences. In DIN [17], they proposed to use attention\nmechanism to capture the similarities between the candidate item\nand the previously clicked items of a user, while it did not consider\nthe sequential nature underlying the user’s behavior sequences.\nTherefore, in this work, to address the aforementioned problems\nfacing WDL and DIN, we try to incorporate sequential signal of\nusers’ behavior sequences into RS at Taobao. Inspired by the great\nsuccess of the Transformer for machine translation task in natu-\nral language processing (NLP) [4, 13], we apply the self-attention\nmechanism to learn a better representation for each item in a user’s\nbehavior sequence by considering the sequential information in\nembedding stage, and then feed them into MLPs to predict users’ re-\nsponses to candidate items. The key advantage of the Transformer\nis that it can better capture the dependency among words in sen-\ntences by the self-attention mechanism, and intuitively speaking,\nthe “dependency” among items in users’ behavior sequences can\nalso be extracted by the Transformer. Therefore, we propose the\nuser behavior sequence transformer (BST) for e-commerce rec-\nommendation at Taobao. Offline experiments and online A/B test\nshow the superiority of BST comparing to existing methods. The\nBST has been deployed in rank stage for Taobao recommendation,\nwhich provides recommending service for hundreds of millions of\nconsumers everyday.\nThe remainder of this paper is organized as follows: the architec-\nture is elaborated in Section 2, and then the experimental results\nincluding offline and online ones are presented in Section 3. Related\nwork are reviewed in Section 4, and finally we conclude our work\nin Section 5.\n2 ARCHITECTURE\nIn the rank stage, we model the recommendation task as Click-\nThrough Rate (CTR) prediction problem, which can be defined as\nfollows: given a user’s behavior sequence S(u)= {v1, v2, ..., vn }\nclicked by a user u, we need to learn a function, F, to predict\nthe probability of u clicking the target item vt , i.e., the candidate\none. Other Features include user profile, context, item, and cross\nfeatures.\narXiv:1905.06874v1  [cs.IR]  15 May 2019\nFigure 1: The overview architecture of the proposed BST. BST takes as input the user’s behavior sequence, including the target\nitem, and “Other Features”. It firstly embeds these input features as low-dimensional vectors. To better capture the relations\namong the items in the behavior sequence, the transformer layer is used to learn deeper representation for each item in the\nsequence. Then by concatenating the embeddings of Other Features and the output of the transformer layer, the three-layer\nMLPs are used to learn the interactions of the hidden features, and sigmoid function is used to generate the final output. Note\nthat the “Positional Features” are incorporated into “Sequence Item Features”.\nWe build BST on top of WDL [2], and the overview architecture is\nshown in Figure 1. From Figure 1, we can see that it follows the pop-\nular embedding&MLP paradigm, where the previously clicked items\nand related features are firstly embedded into low-dimensional vec-\ntors before fed on to MLP. The key difference between BST and\nWDL is that we add transformer layer to learn better representa-\ntions for users’ clicked items by capturing the underlying sequential\nsignals. In the following parts, we introduce in a bottom-up manner\nthe key components of BST: embedding layer, transformer layer,\nand MLP.\n2.1 Embedding Layer\nThe first component is the embedding layer, which embeds all\ninput features into a fixed-size low-dimensional vectors. In our\nscenarios, there are various features, like the user profile features,\nitem features, context features, and the combination of different\nfeatures, i.e., the cross features 1. Since this work is focused on\nmodeling the behavior sequence with transformer, we denote all\nthese features as “Other Features” for simplicity, and give some\nexamples in Table 1. As shown in Figure 1, we concatenate “Other\nfeatures” in left part and embed them into low-dimensional vectors.\nFor these features, we create an embedding matrix Wo ∈R|D |×do ,\nwhere do is the dimension size.\nBesides, we also obtain the embeddings for each item in the\nbehavior sequence, including the target item. As shown in Figure 1,\nwe use two types of features to represent an item, “Sequence Item\n1Though the combination of features can be automatically learned by neural net-\nworks, we still incorporate the some hand-crafted cross features, which have been\ndemonstrated useful in our scenarios before the deep learning era.\nTable 1: The “Other Features“ shown in left side of Figure 1.\nWe use much more features in practice, and show a number\nof effective ones for simplicity.\nUser Item Context Cross\ngender category_id match_type age * item_id\nage shop_id display position os * item_id\ncity tag page No. gender * category_id\n... ... ... ...\nFeatures”(in red) and “Positional Features” (in dark blue), where\n“Sequence Item Features” include item_id and category_id. Note\nthat an item tends to have hundreds of features, while it is too ex-\npensive to choose all to represent the item in a behavior sequence.\nAs introduced in our previous work [ 15], the item_id and cate-\ngory_id are good enough for the performance, we choose these two\nas sparse features to represent each item in embedding the user’s\nbehavior sequences. The “Positional Features” corresponds the fol-\nlowing “positional embedding”. Then for each item, we concatenate\nSequence Item Features and Positional Features, and create an em-\nbedding matrix WV ∈R|V |×dV , where dV is the dimension size of\nthe embedding, and |V |is the number of items. We use ei ∈RdV\nto represent the embedding for the i-th item in a given behavior\nsequence.\nPositional embedding. In [13], the authors proposed a posi-\ntional embedding to capture the order information in sentences.\nLikewise, the order exists in users’ behavior sequences. Thus, we\nadd the “position” as an input feature of each item in the bottom\nlayer before it is projected as a low-dimensional vector. Note that\nthe position value of item vi is computed as pos(vi )= t(vt )−t(vi ),\n2\nwhere t(vt )represents the recommending time and t(vi )the times-\ntamp when user click item vi . We adopt this method since in our\nscenarios it outperforms the sin and cos functions used in [13].\n2.2 Transformer layer\nIn this part, we introduce the Transformer layer, which learns a\ndeeper representation for each item by capturing the relations with\nother items in the behavior sequences.\nSelf-attention layer. The scaled dot-product attention [13] is\ndefined as follows:\nAttention(Q, K, V)= softmax\u0000QKT\n√\nd\n\u0001V, (1)\nwhere Q represents the queries, K the keys and V the values. In our\nscenarios, the self-attention operations takes the embeddings of\nitems as input, and converts them to three matrices through linear\nprojection, and feeds them into an attention layer. Following [13],\nwe use the multi-head attention:\nS =MH(E)= Concat(head1, head2, ··· , headh)WH , (2)\nheadi =Attention(EWQ , EWK , EWV ), (3)\nwhere the projection matrices WQ , WK , WV ∈Rd×d , and E is the\nembedding matrices of all items, and h is the number of heads.\nPoint-wise Feed-Forward Networks. Following [13], we add\npoint-wise Feed-Forward Networks (FFN) to further enhance the\nmodel with non-linearity, which is defined as follows:\nF = FF N(S). (4)\nTo avoid overfitting and learn meaningful features hierarchically,\nwe use dropout and LeakyReLU both in self-attention and FFN.\nThen the overall output of the self-attention and FFN layers are as\nfollows:\nS′=LayerNorm(S + Dropout(MH (S)), (5)\nF =LayerNorm\u0000S′+ Dropout(LeakyReLU(S′W(1)+ b(1))W(2)+ b(2))\u0001,\n(6)\nwhere W(1), b(1), W(2), b(2)are the learnable parameters, andLayerNorm\nis the standard normalization layer.\nStacking the self-attention blocks. After the first self-attention\nblock, it aggregates all the previous items’ embeddings, and to fur-\nther model the complex relations underlying the item sequences,\nwe stack the self-building blocks and the b-th block is defined as:\nSb =SA(F(b−1)), (7)\nFb =FF N(Sb ), ∀i ∈1, 2, ··· , n. (8)\nIn practice, we observe in our experiments b = 1 obtains better\nperformance comparing to b = 2, 3 (see Table 4). For the sake of\nefficiency, we did not try larger b and leave this for future work.\n2.3 MLP layers and Loss function\nBy concatenating the embeddings of Other Features and the output\nof the Transformer layer applying to the target item, we then use\nthree fully connected layers to further learn the interactions among\nthe dense features, which is standard practice in industrial RSs.\nTo predict whether a user will click the target itemvt , we model it\nas a binary classification problem, thus we use the sigmoid function\nTable 2: Statistics of the constructed Taobao dataset.\nDataset #Users #Items #Samples\nTaobao 298,349,235 12,166,060 47,556,271,927\nas the output unit. To train the model, we use the cross-entropy\nloss:\nL= −1\nN\nÕ\n(x,y)∈D\n\u0000y logp(x)+ (1 −y)log(1 −p(x))\u0001, (9)\nwhere Drepresent all the samples, and y ∈ {0, 1}is the label\nrepresenting whether user have click an item or not, p(x)is the\noutput of the network after the sigmoid unit, representing the\npredicted probability of sample x being clicked.\n3 EXPERIMENTS\nIn this section, we present the experimental results.\n3.1 Settings\nDataset. The dataset is constructed from the log of Taobao App 2.\nWe construct an offline dataset based on users’ behaviors in eight\ndays. We use the first seven days as training data, and the last day\nas test data. The statistics of the dataset is shown in Table 2. We\ncan see that the dataset is extremely large and sparse.\nBaselines. To show the effectivene of BST, we compare it with\ntwo models: WDL [2] and DIN [17]. Besides, we create a baseline\nmethod by incorporating sequential information into WDL, denoted\nas WDL(+Seq), which aggregates the embeddings of the previously\nclicked items in average. Our framework is built on top of WDL\nby adding sequential modeling with the Transformer, while DIN is\nproposed to capture the similarities between the target item and\nthe previous clicked items with attention mechanisms.\nEvaluation metric. For the offline results, we use Area Under\nCurve (AUC) score to evaluate the performance of different models.\nFor online A/B test, we use the CTR and average RT to evaluate all\nmodels. RT is short for response time (RT), which is the time cost of\ngenerating recommending results for a given query, i.e., one request\nof a user at Taobao. We use average RT as the metric to evaluate\nthe efficiency of different in online production environment.\nSettings. Our model is implemented with Python 2.7 and Tensor-\nflow 1.4, and the “Adagrad” is chosen as the optimizer. Besides, we\ngive the detail of the model parameters in Table 3.\n3.2 Results Analysis\nThe results are shown in Table 4, from which, we can see the\nsuperiority of BST comparing to baselines. In specific, the AUC of\noffline experiment is improved from0.7734 (WDL) and 0.7866 (DIN)\nto 0.7894 (BST). When comparing WDL and WDL(+Seq), we can see\nthat the effectiveness of incorporating sequential information in an\nsimple averaging manner. It means with the help of self-attention,\nBST provides a powerful capability to capture the sequential signal\nunderlying users’ behavior sequences. Note that from our practical\nexperience, even the small gain of offline AUC can lead to huge gain\nin online CTR. A similar phenomenon is reported by the researchers\nfrom Google in WDL [2].\n2https://www.taobao.com/\n3\nTable 3: The configuration of BST, and the meaning of the\nparameters can be inferred from their names.\nConfiguration of BST.\nembedding size 4 ∼64 batch size 256\nhead number 8 dropout 0.2\nsequence length 20 #epochs 1\ntransformer block 1 queue capacity 1024\nMLP Shape 1024 * 512 * 256 learning rate 0.01\nBesides, in terms of efficiency, the average RT of BST is close to\nthose of WDL and DIN, which guarantees the feasibility of deploy-\ning a complex model like the Transformer in real-world large-scale\nRSs.\nFinally, we also shown the influences of stacking the self-attention\nlayers in Section 2.2. From Table 4, we can see thatb = 1 obtains\nthe best offline AUC. This may be due to the fact that the sequential\ndependency in users’ behavior sequence is not as complex as that\nin sentences in machine translation task, thus smaller number of\nblocks are enough to obtain good performance. Similar observation\nis reported in [ 7]. Therefore we choose b = 1 to deploy BST in\nproduction environment, and only report the online CTR gain for\nb = 1 in Table 4.\n4 RELATED WORK\nIn this section, we briefly review the related work on deep learning\nmethods for CTR prediction. Since the proposal of WDL [2], a se-\nries of works have been proposed to improve the CTR with deep\nlearning based methods, e.g., DeepFM [6], XDeepFM [9], Deep and\nCross networks [16], etc. However, all these previous works focus\non feature combinations or different architectures of neural net-\nworks, ignoring the sequential nature of users’ behavior sequence\nin real-world recommendation scenarios. Recently, DIN [17] was\nproposed to deal with users’ behavior sequences by an attention\nmechanism. The key difference between our model and DIN lies\nin that we propose to use the Transformer [13] to learn a deeper\nrepresentation for each item in users’ behavior sequences, while\nDIN tried to capture different similarities between the previously\nclicked items and the target item. In other words, our model with\ntransformer are more suitable for capturing the sequential signals.\nIn [7, 12], the Transformer model is proposed to solve the sequential\nrecommendation problem in sequence-to-sequence manner while\nthe architectures are different from our model in terms of CTR\nprediction.\n5 CONCLUSION\nIn this paper, we present the technical detail of how we apply\nthe Transformer [ 13] to Taobao recommendation. By using the\npowerful capability of capturing sequential relations, we show the\nsuperiority of the Transformer in modeling user behavior sequences\nfor recommendation by extensive experiments. Besides, we also\npresent the detail of deploying the proposed model in production\nenvironment at Taobao, which provides recommendation service\nfor hundreds of millions of users in China.\n6 ACKNOWLEDGMENTS\nWe would like to thank colleagues of our team - Jizhe Wang, Chao\nLi, Zhiyuan Liu, Yuchi Xu and Mengmeng Wu for useful discussions\nTable 4: Offline AUCs and online CTR gains of different\nmethods. Online CTR gain is relative to the control group.\nMethods Offline AUC Online CTR Gain Average RT(ms)\nWDL 0.7734 - 13\nWDL(+Seq) 0.7846 +3.03% 14\nDIN 0.7866 +4.55% 16\nBST(b = 1) 0.7894 +7.57% 20\nBST(b = 2) 0.7885 - -\nBST(b = 3) 0.7823 - -\nand supports on this work. We are grateful to Jinbin Liu, Shanshan\nHao and Yanchun Yang from Alibaba Distributed Computing Team,\nand Kan Liu, Tao Lan from Alibaba Online Inference Team, who\nhelp deploy the model in production. We also thank the anonymous\nreviewers for their valuable comments and suggestions that help\nimprove the quality of this manuscript.\nREFERENCES\n[1] Wen Chen, Pipei Huang, Jiaming Xu, Xin Guo, Cheng Guo, Fei Sun, Chao Li,\nAndreas Pfadler, Huan Zhao, and Binqiang Zhao. 2019. POG: Personalized Outfit\nGeneration for Fashion Recommendation at Alibaba iFashion.\n[2] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.\n2016. Wide & deep learning for recommender systems. , 7–10 pages.\n[3] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for\nyoutube recommendations. In RecSys. 191–198.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[5] Mihajlo Grbovic and Haibin Cheng. 2018. Real-time personalization using em-\nbeddings for search ranking at Airbnb. In KDD. 311–320.\n[6] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: a factorization-machine based neural network for CTR prediction. In\nIJCAI. 1725–1731.\n[7] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\nmendation. In ICDM. 197–206.\n[8] Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Pipei Huang, Huan Zhao,\nGuoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-Interest\nNetwork with Dynamic Routing for Recommendation at Tmall.\n[9] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and\nGuangzhong Sun. 2018. xDeepFM: Combining explicit and implicit feature\ninteractions for recommender systems. In KDD. 1754–1763.\n[10] Yabo Ni, Dan Ou, Shichen Liu, Xiang Li, Wenwu Ou, Anxiang Zeng, and Luo Si.\n2018. Perceive Your Users in Depth: Learning Universal User Representations\nfrom Multiple E-commerce Tasks. In KDD. 596–605.\n[11] Changhua Pei, Yi Zhang, Yongfeng Zhang, Fei Sun, Xiao Lin, Hanxiao Sun, Jian\nWu, Peng Jiang, Wenwu Ou, and Dan Pei. 2019. Personalized Context-aware Re-\nranking for E-commerce Recommender Systems. arXiv preprint arXiv:1904.06813\n(2019).\n[12] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Repre-\nsentations from Transformer.\n[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS. 5998–6008.\n[14] Chenglong Wang, Feijun Jiang, and Hongxia Yang. 2017. A hybrid framework\nfor text modeling with convolutional rnn. In KDD. 2061–2069.\n[15] Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun\nLee. 2018. Billion-scale commodity embedding for e-commerce recommendation\nin alibaba. In KDD. 839–848.\n[16] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network\nfor Ad Click Predictions. In Proceedings of the ADKDD’17 (ADKDD’17) .\n[17] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui\nYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through\nrate prediction. In KDD. 1059–1068.\n[18] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai.\n2018. Learning Tree-based Deep Model for Recommender Systems. In KDD.\n1079–1088.\n4",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7891175746917725
    },
    {
      "name": "RSS",
      "score": 0.7655928134918213
    },
    {
      "name": "Computer science",
      "score": 0.7365973591804504
    },
    {
      "name": "Embedding",
      "score": 0.6759883165359497
    },
    {
      "name": "Recommender system",
      "score": 0.5876085162162781
    },
    {
      "name": "Sequence (biology)",
      "score": 0.47907882928848267
    },
    {
      "name": "Deep learning",
      "score": 0.42963969707489014
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39992403984069824
    },
    {
      "name": "Machine learning",
      "score": 0.26832500100135803
    },
    {
      "name": "World Wide Web",
      "score": 0.1761813759803772
    },
    {
      "name": "Engineering",
      "score": 0.11675405502319336
    },
    {
      "name": "Electrical engineering",
      "score": 0.0685584545135498
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 34
}