{
    "title": "Reducing Hallucinations in Large Language Models Through Contextual Position Encoding",
    "url": "https://openalex.org/W4399199494",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5114170745",
            "name": "Sarah Desrochers",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1984636798",
            "name": "James Wilson",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2481049683",
            "name": "Matthew Beauchesne",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4392366668",
        "https://openalex.org/W4387964464",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4394867293",
        "https://openalex.org/W4378509449",
        "https://openalex.org/W4390490761",
        "https://openalex.org/W4392914055",
        "https://openalex.org/W4382618460",
        "https://openalex.org/W4378942311",
        "https://openalex.org/W4388691793",
        "https://openalex.org/W4398196557",
        "https://openalex.org/W4396762959",
        "https://openalex.org/W4396701991",
        "https://openalex.org/W4386409617",
        "https://openalex.org/W4398131214",
        "https://openalex.org/W4377027452",
        "https://openalex.org/W4392756444",
        "https://openalex.org/W4383472770",
        "https://openalex.org/W4386065596",
        "https://openalex.org/W4353112996",
        "https://openalex.org/W4395069551",
        "https://openalex.org/W4390298466"
    ],
    "abstract": "In natural language processing, maintaining factual accuracy and minimizing hallucinations in text generation remain significant challenges. Contextual Position Encoding (CPE) presents a novel approach by dynamically encoding positional information based on the context of each token, significantly enhancing the model's ability to generate accurate and coherent text. The integration of CPE into the Mistral Large model resulted in marked improvements in precision, recall, and F1-score, demonstrating superior performance over traditional positional encoding methods. Furthermore, the enhanced model architecture effectively reduced hallucination rates, increasing the reliability of the generated outputs. Comparative analysis with baseline models such as GPT-3 and BERT confirmed the efficacy of CPE, highlighting its potential to influence future developments in LLM architecture. The results underscore the importance of advanced positional encoding techniques in improving the performance and applicability of large language models across various domains requiring high factual accuracy.",
    "full_text": null
}