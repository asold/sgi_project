{
    "title": "On Evaluating the Efficiency of Source Code Generated by LLMs",
    "url": "https://openalex.org/W4399577572",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5061689324",
            "name": "Changan Niu",
            "affiliations": [
                "Nanjing University"
            ]
        },
        {
            "id": "https://openalex.org/A5100458319",
            "name": "Ting Zhang",
            "affiliations": [
                "Singapore Management University"
            ]
        },
        {
            "id": "https://openalex.org/A5103039853",
            "name": "Chuanyi Li",
            "affiliations": [
                "Nanjing University"
            ]
        },
        {
            "id": "https://openalex.org/A5101898724",
            "name": "Bin Luo",
            "affiliations": [
                "Software Engineering Institute"
            ]
        },
        {
            "id": "https://openalex.org/A5102732054",
            "name": "Vincent Ng",
            "affiliations": [
                "The University of Texas at Dallas"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2147657366",
        "https://openalex.org/W4308642031"
    ],
    "abstract": "Recent years have seen the remarkable capabilities of large language models (LLMs) for code generation. Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency. More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming. First, we evaluate the efficiency of the code generated by LLMs on two benchmarks, HumanEval and MBPP. Then, we choose a set of programming problems from the online judge platform LeetCode to conduct a more difficult evaluation. Finally, we explore several prompts that would enable LLMs to generate more efficient code.",
    "full_text": null
}