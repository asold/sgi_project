{
  "title": "Pseudolikelihood Reranking with Masked Language Models.",
  "url": "https://openalex.org/W2982621400",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2136963084",
      "name": "Julián Salazar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2782013732",
      "name": "Davis Liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097196795",
      "name": "Toan Nguyen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2068133273",
      "name": "Katrin Kirchhoff",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2747917286",
    "https://openalex.org/W2405722712",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2945598870",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2963665552",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963260202",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2947898088",
    "https://openalex.org/W28766783"
  ],
  "abstract": "We rerank with scores from pretrained masked language models like BERT to improve ASR and NMT performance. These log-pseudolikelihood scores (LPLs) can outperform large, autoregressive language models (GPT-2) in out-of-the-box scoring. RoBERTa reduces WER by up to 30% relative on an end-to-end LibriSpeech system and adds up to +1.7 BLEU on state-of-the-art baselines for TED Talks low-resource pairs, with further gains from domain adaptation. In the multilingual setting, a single XLM can be used to rerank translation outputs in multiple languages. The numerical and qualitative properties of LPL scores suggest that LPLs capture sentence fluency better than autoregressive scores. Finally, we finetune BERT to estimate sentence LPLs without masking, enabling scoring in a single, non-recurrent inference pass.",
  "full_text": "Masked Language Model Scoring\nJulian Salazar♠ Davis Liang♠ Toan Q. Nguyen♦∗ Katrin Kirchhoff♠\n♠Amazon AWS AI, USA\n♦University of Notre Dame, USA\n{julsal,liadavis,katrinki}@amazon.com, tnguye28@nd.edu\nAbstract\nPretrained masked language models (MLMs)\nrequire ﬁnetuning for most NLP tasks. Instead,\nwe evaluate MLMs out of the box via their\npseudo-log-likelihood scores (PLLs), which\nare computed by masking tokens one by one.\nWe show that PLLs outperform scores from\nautoregressive language models like GPT-2 in\na variety of tasks. By rescoring ASR and\nNMT hypotheses, RoBERTa reduces an end-\nto-end LibriSpeech model’s WER by 30% rela-\ntive and adds up to +1.7 BLEU on state-of-the-\nart baselines for low-resource translation pairs,\nwith further gains from domain adaptation. We\nattribute this success to PLL’s unsupervised ex-\npression of linguistic acceptability without a\nleft-to-right bias, greatly improving on scores\nfrom GPT-2 (+10 points on island effects, NPI\nlicensing in BLiMP). One can ﬁnetune MLMs\nto give scores without masking, enabling com-\nputation in a single inference pass. In all, PLLs\nand their associated pseudo-perplexities (PP-\nPLs) enable plug-and-play use of the growing\nnumber of pretrained MLMs; e.g., we use a\nsingle cross-lingual model to rescore transla-\ntions in multiple languages. We release our\nlibrary for language model scoring at https:\n//github.com/awslabs/mlm-scoring.\n1 Introduction\nBERT (Devlin et al., 2019) and its improvements\nto natural language understanding have spurred\na rapid succession of contextual language repre-\nsentations (Yang et al., 2019; Liu et al., 2019;\ninter alia ) which use larger datasets and more\ninvolved training schemes. Their success is at-\ntributed to their use of bidirectional context, often\nvia their masked language model (MLM) objec-\ntives. Here, a token wt is replaced with [MASK]\nand predicted using all past and future tokens\nW\\t := (w1,..., wt−1,wt+1,..., w|W|).\n∗Work done during an internship at Amazon AWS AI.\nFigure 1: To score a sentence, one creates copies\nwith each token masked out. The log probability for\neach missing token is summed over copies to give the\npseudo-log-likelihood score (PLL). One can adapt to\nthe target domain to improve performance, or ﬁnetune\nto score without masks to improve memory usage.\nIn contrast, conventional language models (LMs)\npredict wt using only past tokens W<t :=\n(w1,..., wt−1). However, this allows LMs to es-\ntimate log probabilities for a sentence W via the\nchain rule ( log PLM(W ) = ∑|W|\nt=1 log PLM(wt |\nW<t)), which can be used out of the box to rescore\nhypotheses in end-to-end speech recognition and\nmachine translation (Chan et al., 2016; Gulcehre\net al., 2015), and to evaluate sentences for linguistic\nacceptability (Lau et al., 2017).\nOur work studies the corresponding pseudo-log-\nlikelihood scores (PLLs) from MLMs (Wang and\nCho, 2019), given by summing the conditional log\nprobabilities log PMLM(wt | W\\t) of each sen-\ntence token (Shin et al., 2019). These are induced\nin BERT by replacing wt with [MASK] (Figure 1).\narXiv:1910.14659v3  [cs.CL]  1 Jan 2021\nLet Θ denote our model’s parameters. Our score is\nPLL(W ) :=\n|W|∑\nt=1\nlog PMLM(wt |W\\t; Θ).\nPLLs and their corresponding pseudo-perplexities\n(PPPLs) (Section 2.3) are intrinsic values one can\nassign to sentences and corpora, allowing us to\nuse MLMs in applications previously restricted to\nconventional LM scores. Furthermore, we show\nthat one can ﬁnetune BERT to compute PLLs in a\nsingle, non-recurrent inference pass (Section 2.2).\nExisting uses of pretrained MLMs in sequence-\nto-sequence models for automatic speech recogni-\ntion (ASR) or neural machine translation (NMT)\ninvolve integrating their weights (Clinchant et al.,\n2019) or representations (Zhu et al., 2020) into the\nencoder and/or decoder during training. In contrast,\nwe train a sequence model independently, then\nrescore its n-best outputs with an existing MLM.\nFor acceptability judgments, one ﬁnetunes MLMs\nfor classiﬁcation using a training set (Warstadt\net al., 2019; Devlin et al., 2019); instead, PLLs\ngive unsupervised, relative judgements directly.\nIn Section 3, we show that scores from BERT\ncompete with or even outperform GPT-2 (Radford\net al., 2019), a conventional language model of\nsimilar size but trained on more data. Gains scale\nwith dataset and model size: RoBERTa large (Liu\net al., 2019) improves an end-to-end ASR model\nwith relative WER reductions of 30%, 18% on Lib-\nriSpeech test-clean, test-other respectively (with\nfurther gains from domain adaptation), and im-\nproves state-of-the-art NMT baselines by up to +1.7\nBLEU on low-resource pairs from standard TED\nTalks corpora. In the multilingual case, we ﬁnd\nthat the pretrained 15-language XLM (Conneau\nand Lample, 2019) can concurrently improve NMT\nsystems in different target languages.\nIn Section 4, we analyze PLLs and propose them\nas a basis for other ranking/scoring schemes. Un-\nlike log probabilities, PLL’s summands are more\nuniform across an utterance’s length (no left-to-\nright bias), helping differentiate ﬂuency from likeli-\nness. We use PLLs to perform unsupervised accept-\nability judgments on the BLiMP minimal pairs set\n(Warstadt et al., 2020); BERT and RoBERTa mod-\nels improve the state of the art (GPT-2 probabilities)\nby up to 3.9% absolute, with +10% on island ef-\nfects and NPI licensing phenomena. Hence, PLLs\ncan be used to assess the linguistic competence of\nMLMs in a supervision-free manner.\n2 Background\n2.1 Pseudolikelihood estimation\nBidirectional contextual representations like BERT\ncome at the expense of being “true” language mod-\nels PLM(W ), as there may appear no way to gen-\nerate text (sampling) or produce sentence probabil-\nities (density estimation) from these models. This\nhandicapped their use in generative tasks, where\nthey at best served to bootstrap encoder-decoder\nmodels (Clinchant et al., 2019; Zhu et al., 2020) or\nunidirectional LMs (Wang et al., 2019).\nHowever, BERT’s MLM objective can be viewed\nas stochastic maximum pseudolikelihood estima-\ntion (MPLE) (Wang and Cho, 2019; Besag, 1975)\non a training set W, where {wt}|W|\nt=1 are random\nvariables in a fully-connected graph. This ap-\nproximates conventional MLE, with MLM training\nasymptotically maximizing the objective:\nJPL(Θ; W) = 1\n|W|\n∑\nW∈W\nPLL(W ; Θ).\nIn this way, MLMs learn an underlying joint dis-\ntribution whose conditional distributions wt |W\\t\nare modeled by masking at position t. We include\na further discussion in Appendix B.\nThis enabled text generation with BERT via\nGibbs sampling, leading to the proposal (but not\nevaluation) of a related quantity, the sum of log-\nits, for sentence ranking (Wang and Cho, 2019).\nMore recent work (Shin et al., 2019) extended past\nresearch on future-conditional LMs in ASR (Sec-\ntion 5) with deeply-bidirectional self-attentive lan-\nguage models (bi-SANLMs). They trained shal-\nlow models from scratch with the [MASK] scoring\nmethod, but did not relate their work to pseudolike-\nlihood and ﬂuency, which provide a framework to\nexplain their success and observed behaviors.\nExperimentally, we extend both works by eval-\nuating pretrained models, domain adaptation, and\nusage in NMT and multilingual settings (Section 3),\nalong with acceptability judgements and PLL’s in-\ntrinsic numerical properties (Section 4).\n2.2 [MASK]less scoring\nA practical point unaddressed in both works is that\ncomputing PLLs from an MLM requires a sentence\ncopy for each position, making the number of in-\nference passes dependent on length (though these\ncan be parallelized). The cost of a softmax is also\nincurred, which is dependent on vocabulary size\nV; together this gives O(|W |· V). We propose\nreducing this to O(1) by training a network qwith\nparameters ΘS to match BERT’s PLLs without\n[MASK] tokens:\n|PLL(W ) −q(W ; ΘS)|2.\nWe propose ﬁnetuning qfrom the pretrained MLM\ndirectly (i.e., initializingΘS with Θ), via regression\nover the [CLS] token (Figure 2):\nFigure 2: We learn a linear map after the[CLS] token,\nsupervised by the PLLs from the pretrained MLM.\nMore generally, one could use any student model\nq, as in knowledge distillation (Hinton et al., 2014).\nHere, the teacher gives individual token probabil-\nities (|W |inference passes) while the student ap-\nproximates their sum (one inference pass). This is\nreminiscent of distilling an autoregressive teacher\nto a parallel student, as in the case of WaveNet\n(Oord et al., 2018). Other [MASK]less bidirec-\ntional models like XLNet (Yang et al., 2019) can\nalso give PLLs; we leave this to future work.\n2.3 Pseudo-perplexity\nAnalogous to conventional LMs, we propose the\npseudo-perplexity (PPPL) of an MLM as an in-\ntrinsic measure of how well it models a corpus of\nsentences W. Let N denote the number of tokens\nin the corpus. Then a model’s PPPL onW is\nPPPL(W) := exp\n(\n−1\nN\n∑\nW∈W\nPLL(W )\n)\n.\nPast work (Chen et al., 2017) also computed this\nquantity with bi-RNNLMs for ASR, although\nsuch models are not deeply bidirectional like self-\nattentive MLMs (see Section 5).\nThese PPPLs can be used in lieu of perplexi-\nties. For example, during domain adaptation, one\ncan perform early stopping with respect to develop-\nment PPPL. This is in contrast to MLM accuracy,\nwhich is not a continuous loss and is often stochas-\ntic (e.g., when performing dynamic masking as\nin RoBERTa). In Section 4.1, we see that PPPLs\nnaturally separate out sets of acceptable and unac-\nceptable sentences.\nUnlike previous works (Chen et al., 2017; Shin\net al., 2019) we use pretrained BERTs, which\nare open-vocabulary (subword) bidirectional LMs.\nHowever, PPPLs are only comparable under the\nsame subword vocabulary, which differs between\ne.g., BERT and RoBERTa. Normalizing with N as\nthe number of words mitigates this. In Appendix C,\nwe show that word-normalized PPPLs correlate\nwith domain adaptation, and with downstream met-\nrics like ASR and BLEU after rescoring.\n3 Sequence-to-sequence rescoring\nLet X denote audio features or source text tokens,\nand let W = (w1,..., w|W|) denote target text\ntokens. For non-end-to-end ASR and MT sys-\ntems, having separate acoustic/translation models\nPAM/TM(X |W ) and language models PLM(W )\nis motivated by the Bayes rule decomposition used\nto select the best hypothesis ˆW (Jelinek et al.,\n1975; Brown et al., 1993):\nˆW = arg max\nW\n[P(W |X)]\n= arg max\nW\n[PAM/TM(X |W )PLM(W )].\n3.1 The log-linear model\nEnd-to-end ASR and NMT use encoder-decoder\narchitectures that are trained discriminatively.\nThough less principled, many still adopt a log-\nlinear model\nˆW = arg max\nW\n[log P(W |X)]\n≈arg max\nW\n[log f(W ,X) + λlog g(W )]\nwith learned functions f,g and a hyperparameter λ,\nto good effect (Sutskever et al., 2014; Chan et al.,\n2016). One often takes f = PS2S(W |X) as the\nsequence-to-sequence model and g = PLM(W )\nas the language model. Since the sequence-level\narg maxis intractable, one can dofusion, which de-\ncomposes f = ∏ft and g= ∏gt over time (Gul-\ncehre et al., 2015), restricting to the top N inter-\nmediate candidates at each step (beam search). In-\nstead, our work considers N-best rescoring, which\ncomputes f(W ,X) ﬁrst, still using beam search\nto maintain the top N candidates and scores. Then,\ng(W ) is computed for the resulting hypotheses and\ninterpolated with these scores, giving a new top-1\nhypothesis. The sequence model is now solely re-\nsponsible for “capturing” the best hypothesis ˆW\nin its beam. However, there are two advantages to\nN-best rescoring, which motivate PLLs as well as\nour maskless ﬁnetuning approach, respectively:\nDecoupling of scale. Fusion requires correspon-\ndence between ft and gt at every t. This requires\nthe sequence model and LM to be autoregressive\nand share tokenizations. In rescoring, f = PS2S\ndoes not require gto decompose over time or to be\na true probability at all, though gshould scale with\nf so that λremains valid for all lengths |W |; e.g.,\ntaking g(W ) to be a “relevance score” between 0\nand 1 would not satisfy this property. The choice\nof log-linear is relevant here (Appendix B).\nLength-independent inference. If g is non-\nrecurrent, then g(W ) may be computed in a single\ninference pass. This difference manifests with self-\nattentive LMs like SANLMs and Transformer-XL\n(Dai et al., 2019), as recently explored for N-best\nrescoring in ASR (Li et al., 2019; Shin et al., 2019).\n3.2 Experimental setup\nFurther implementation and experimental details\ncan be found in Appendix A and our code release:\nLMs. We rescore sequence-to-sequence hypothe-\nses as in Section 3.1. Each hypothesis is assigned\nits log probability (uni-SANLM, GPT-2) or pseudo-\nlog-likelihood score (bi-SANLM, BERT, M-BERT,\nRoBERTa, XLM). We tune the LM weight λon\nthe development set to minimize word error rate\n(WER) for ASR or maximize tokenized BLEU for\nNMT. We then evaluate on the test set.\nASR. Our 100-best hypotheses are from an end-\nto-end, 5-layer BLSTMP model (Shin et al., 2019)\nfrom ESPnet (Watanabe et al., 2018) on the 960-\nhour LibriSpeech corpus (Panayotov et al., 2015).\nThough this baseline is not state-of-the-art, we use\ntheir lists to enable direct comparison in Table 5.\nNMT. Our 100-best hypotheses are from strong\nTransformer baselines with BPE subwords. One\nwas pretrained for WMT 2014 English-German\n(Vaswani et al., 2017); the others are state-of-the-\nart low-resource models we trained for ﬁve pairs\nfrom the TED Talks corpus (Qi et al., 2018) and for\nIWSLT 2015 English-Vietnamese (Cettolo et al.,\n2015), which we also describe in a dedicated, con-\ncurrent work (Nguyen and Salazar, 2019). For\nthe low-resource models we scored tokenized hy-\npotheses (though with HTML entities unescaped,\ne.g., &quot; ↦→\"). Length normalization (Wu\net al., 2016) is applied to NMT (α= 0.6) and LM\n(α= 1.0) scores (Section 4.3).\nCorpus Source →target language # pairs\nTED Talks Galician (gl) →English (en) 10k\nTED Talks Slovakian (sk) →English (en) 61k\nIWSLT 2015 English (en) →Vietnamese (vi) 133k\nTED Talks English (en) →German (de) 167k\nTED Talks Arabic (ar) →English (en) 214k\nTED Talks English (en) →Arabic (ar) 214k\nWMT 2014 English (en) →German (de) 4.5M\nTable 1: Sizes of translation datasets used in this paper.\n3.3 Out-of-the-box (monolingual)\nWe consider BERT (Devlin et al., 2019), GPT-2\n(Radford et al., 2019), and RoBERTa (Liu et al.,\n2019), which are trained on 17GB, 40GB, and\n160GB of written text respectively. Each model\ncomes in similarly-sized 6-layer (117M / base) and\n12-layer (345M / large) versions. GPT-2 is autore-\ngressive, while BERT and RoBERTa are MLMs.\nWe begin by rescoring ASR outputs in Table 2:\nModel dev test\nclean other clean other\nbaseline (100-best) 7.17 19.79 7.26 20.37\nGPT-2 (117M, cased) 5.39 16.81 5.64 17.60\nBERT (base, cased) 5.17 16.44 5.41 17.41\nRoBERTa (base, cased) 5.03 16.16 5.25 17.18\nGPT-2 (345M, cased) 5.15 16.48 5.30 17.26\nBERT (large, cased) 4.96 16.26 5.25 16.97\nRoBERTa (large, cased) 4.75 15.81 5.05 16.79\noracle (100-best) 2.85 12.21 2.81 12.85\nTable 2: WERs on LibriSpeech after rescoring. Base-\nline lists and oracle scores are from Shin et al. (2019).\nAs GPT-2 is trained on cased, punctuated data\nwhile the ASR model is not, we use cased MLMs\nand append “.” to hypotheses to compare out-of-\nthe-box performance. BERT outperforms its corre-\nsponding GPT-2 models despite being trained on\nless data. RoBERTa reduces WERs by 30% relative\non LibriSpeech test-clean and 18% on test-other.\nWe repeat the same on English-target NMT in\nTable 3. As 100-best can be worse than 4-best due\nto the beam search curse (Yang et al., 2018; Murray\nand Chiang, 2018), we ﬁrst decode both beam sizes\nto ensure no systematic degradation in our models.\nHypothesis rescoring with BERT (base) gives up to\n+1.1 BLEU over our strong 100-best baselines, re-\nmaining competitive with GPT-2. Using RoBERTa\n(large) gives up to +1.7 BLEU over the baseline.\nIncidentally, we have demonstrated conclusive im-\nprovements on Transformers via LM rescoring for\nthe ﬁrst time, despite only using N-best lists; the\nmost recent fusion work (Stahlberg et al., 2018)\nonly used LSTM-based models.\nModel TED Talks\ngl→en sk →en ar →en\nNeubig and Hu (2018) 16.2 24.0 –\nAharoni et al. (2019) – – 27.84\nour baseline (4-best) 18.47 29.37 33.39\nour baseline (100-best) 18.55 29.20 33.40\nGPT-2 (117M, cased) 19.24 30.38 34.41\nBERT (base, cased) 19.09 30.27 34.32\nRoBERTa (base, cased) 19.22 30.80 34.45\nGPT-2 (345M, cased) 19.16 30.76 34.62\nBERT (large, cased) 19.30 30.31 34.47\nRoBERTa (large, cased) 19.36 30.87 34.73\nTable 3: Test BLEU scores on English-target language\npairs from the TED Talks corpus, after rescoring.\nWe also consider a non-English, higher-resource\ntarget by rescoring a pre-existing WMT 2014\nEnglish-German system (trained on 4.5M sentence\npairs) with German BERT (base) models1 trained\non 16GB of text, similar to English BERT. From\n27.3 BLEU we get +0.5, +0.3 from uncased, cased;\na diminished but present effect that can be im-\nproved as in Table 3 with more pretraining, a larger\nmodel, or domain adaptation (Section 3.5).\n3.4 Out-of-the-box (multilingual)\nTo assess the limits of our modular approach, we\nask whether a shared multilingual MLM can im-\nprove translation into different target languages.\nWe use the 100+ language M-BERT models, and\nthe 15-language XLM models (Conneau and Lam-\nple, 2019) optionally trained with a crosslingual\ntranslation LM objective (TLM). Monolingual\ntraining was done on Wikipedia, which gives e.g.,\n6GB of German text; see Table 4.\nThe 100-language M-BERT models gave no con-\nsistent improvement. The 15-language XLMs fared\nbetter, giving +0.2-0.4 BLEU, perhaps from their\nuse of language tokens and fewer languages. Our\n1https://github.com/dbmdz/german-bert\nModel IWSLT '15 TED Talks\nen→vi en →de en →ar\nWang et al. (2018) 29.09 – –\nAharoni et al. (2019) – 23.31 12.95\nour baseline (4-best) 31.94 30.50 13.95\nour baseline (100-best) 31.84 30.44 13.94\nM-BERT (base, uncased) 32.12 30.48 13.98\nM-BERT (base, cased) 32.07 30.45 13.94\nXLM (base*, uncased) 32.27 30.61 14.13\n+ TLM objective 32.26 30.62 14.10\nde-BERT (base, uncased) – 31.27 –\nde-BERT (base, cased) – 31.22 –\nTable 4: Test BLEU scores for language pairs with non-\nEnglish targets, after hypothesis rescoring. Base* uses\n1024 hidden dimensions but only 8 heads instead.\nGerman BERT results suggest an out-of-the-box\nupper bound of +0.8 BLEU, as we found with En-\nglish BERT on similar resources. We expect that\nincreasing training data and model size will boost\nXLM performance, as in Section 3.3.\n3.5 Domain adaptation\nOut-of-the-box rescoring may be hindered by how\nclosely our models match the downstream text. For\nexample, our uncased multilingual models strip ac-\ncents, exacerbating their domain mismatch with the\ncased, accented gold translation. We examine this\neffect in the setting of LibriSpeech, which has its\nown 4GB text corpus and is fully uncased and un-\npunctuated, unlike the cased MLMs in Section 3.3.\nWe rescore using in-domain models in Table 5:\nModel dev test\nclean other clean other\nbaseline (100-best) 7.17 19.79 7.26 20.37\nuni-SANLM 6.08 17.32 6.11 18.13\nbi-SANLM 5.52 16.61 5.65 17.44\nBERT (base, Libri. only) 4.63 15.56 4.79 16.50\nBERT (base, cased) 5.17 16.44 5.41 17.41\nBERT (base, uncased) 5.02 16.07 5.14 16.97\n+ adaptation, 380k steps 4.37 15.17 4.58 15.96\noracle (100-best) 2.85 12.21 2.81 12.85\nTable 5: WERs on LibriSpeech after hypothesis rescor-\ning. Baseline, SANLM, and oracle numbers are from\nShin et al. (2019).\nUsing a BERT model trained only on the text\ncorpus outperforms RoBERTa (Table 2) which is\ntrained on far more data, underscoring the tradeoff\nbetween in-domain modeling and out-of-the-box\nintegration. Even minor differences like casing\ngives +0.3-0.4 WER at test time. In Section 4.3 we\nsee that these domain shifts can be visibly observed\nfrom the positionwise scores log PMLM(wt |W\\t).\nThe best results (“adaptation”) still come from\nadapting a pretrained model to the target corpus.\nWe proceed as in BERT, i.e., performing MLM on\nsequences of concatenated sentences (more details\nin Appendix A). In contrast, the 3-layer SANLMs\n(Shin et al., 2019) do per-utterance training, which\nis slower but may reduce mismatch even further.\nFinally, we show in Appendix C that even before\nevaluating WER or BLEU, one can anticipate im-\nprovements in the downstream metric by looking at\nimprovements in word-normalized PPPL on the tar-\nget corpus. The domain-adapted MLM has lower\nPPPLs than the pretrained models, and RoBERTa\nhas lower PPPLs than BERT.\n3.6 Finetuning without masking\nWe ﬁnetune BERT to produce scores without\n[MASK] tokens. For LibriSpeech we take the\nnormalized text corpus and keep sentences with\nlength |W |≤ 384, score them with our adapted\nBERT (base), then do sentence-level regression\n(Section 2.2). We train using Adam with a learning\nrate of 10−5 for 10 epochs (Table 6):\nModel dev\nclean other\nbaseline (100-best) 7.17 19.79\nGPT-2 (117M, cased) 5.39 16.81\nBERT (base, uncased, adapted) 4.37 15.17\n+ no masking 5.79 18.07\n+ sentence-level ﬁnetuning 4.61 15.53\nTable 6: WERs on LibriSpeech upon rescoring, show-\ning the effects of single-copy, maskless scoring.\nSentence-level ﬁnetuning degrades performance\nby +0.2-0.4 WER, leaving room for future improve-\nment. This still outperforms GPT-2 (117M, cased),\nthough this gap may be closed by adaptation. For\nnow, maskless ﬁnetuning could be reserved for\ncases where only a masked language model is avail-\nable, or when latency is essential.\nRemarkably, we found that out-of-the-box scor-\ning without [MASK] still signiﬁcantly improves\nthe baseline. This is likely from the 20% of the\ntime BERT does not train on [MASK], but instead\ninputs a random word or the same word (Devlin\net al., 2019). Future work could explore ﬁnetun-\ning to positionwise distributions, as in word-level\nknowledge distillation (Kim and Rush, 2016), for\nwhich our results are a na¨ıve performance bound.\n4 Analysis\nWe recall the log-linear model from Section 3.1:\nˆW ≈arg max\nW\n[log f(W ,X) + λlog g(W )]\nAlthough end-to-end models f = PS2S(W |X)\npredict W directly from X, interpolation with\nthe unconditional g = PLM(W ) remains helpful\n(Toshniwal et al., 2018). One explanation comes\nfrom cold and simple fusion (Sriram et al., 2018;\nStahlberg et al., 2018), which further improve on\nshallow fusion (Section 3.1) by learningg(W ) ﬁrst.\nThey argue gexpresses ﬂuency; ﬁxing gearly al-\nlows f(W ,X) to focus its capacity on adequacy\nin encoding the source, and thus specializing the\ntwo models. With this perspective in mind, we\ncompare log PLM and PLL as candidates for log g.\n4.1 Relative linguistic acceptability\nIn this work we interpret ﬂuency as linguistic ac-\nceptability (Chomsky, 1957); informally, the syn-\ntactic and semantic validity of a sentence according\nto human judgments (Sch ¨utze, 1996). Its graded\nform is well-proxied by neural language model\nscores (log PLM) once length and lexical frequency\nare accounted for (Lau et al., 2017). This can be\nseen in a controlled setting usingminimal pairs and\nGPT-2 (345M) scores:\nRaymond is selling this sketch. −40.0,\nRaymond is selling this sketches. −45.2.\nThis example is from the Benchmark of Linguistic\nMinimal Pairs (BLiMP) (Warstadt et al., 2020), a\nchallenge set of 67k pairs which isolate contrasts\nin syntax, morphology, and semantics (in this ex-\nample, determiner-noun agreement). While its pre-\ndecessor, the Corpus of Linguistic Acceptability\n(CoLA), has a training set and asks to label sen-\ntences as “acceptable” or not in isolation (Warstadt\net al., 2019), BLiMP provides an unsupervised set-\nting: language models are evaluated on how often\nthey give the acceptable sentence a higher (i.e., less\nnegative) score. This is equivalent to 2-best rescor-\ning without sequence model scores ( log f = 0 ).\nSince most minimal pairs only differ by a single\nword, the effect of length on log probabilities and\nPLLs (discussed in Section 4.3) is mitigated.\nWe compute PLLs on the sentences of each pair\nusing cased BERT and RoBERTa, then choose the\nsentence with the highest score. Our results are in\nTable 7. Despite using less than half the data and a\nModel (cased) Overall ANA. AGR\nARG. STR\nBINDINGCTRL\n. RAIS\n.\nD-N AGRELLIPSISFILLER GAPIRREGULARISLANDNPI QUANTIFIERSS-V AGRUnacc. PPPLAcc. PPPLRatio\nGPT-2 (345M) 82.6 99.4 83.4 77.8 83.0 96.3 86.3 81.3 94.9 71.7 74.7 74.1 88.3 – – –\nBERT (base) 84.2* 97.0 80.0 82.3* 79.6 97.6* 89.4* 83.1* 96.5* 73.6* 84.7* 71.2 92.4* 111.2 59.2 1.88\nBERT (large) 84.8* 97.2 80.7 82.0* 82.7 97.6* 86.4 84.3* 92.8 77.0* 83.4* 72.8 91.9* 128.1 63.6 2.02\nRoBERTa (base) 85.4* 97.3 83.5 77.8 81.9 97.0 91.4* 90.1* 96.2* 80.7* 81.0* 69.8 91.9* 213.5 87.9 2.42\nRoBERTa (large) 86.5* 97.8 84.6* 79.1* 84.1* 96.8 90.8* 88.9* 96.8* 83.4* 85.5* 70.2 91.4* 194.0 77.9 2.49\nHuman 88.6 97.5 90.0 87.3 83.9 92.2 85.0 86.9 97.0 84.9 88.1 86.6 90.9 – – –\nTable 7: Unsupervised performance (forced choice accuracy) on BLiMP using log probabilities (GPT-2) or PLLs.\nHuman scores from Warstadt et al. (2020). Values with * denote improvements over GPT-2 of≥1% absolute.\nthird of the capacity, BERT (base) already outper-\nforms the previous state of the art (GPT-2) by 1.6%\nabsolute, increasing to 3.9% with RoBERTa (large).\nThere are 4 of 12 categories where all four PLLs\noutperform log probabilities by ≥1% absolute (val-\nues marked by *), and 7 where three or more PLLs\noutperform by this margin. Interestingly, PLLs do\nconsistently worse on quantiﬁers, though all are\nrelatively bad against the human baseline. The ra-\ntio of token-level PPPLs between unacceptable and\nacceptable sentences overall increases with perfor-\nmance, separating the two sentence sets.\nRoBERTa improves by around 10% on ﬁller-gap\ndependencies, island effects, and negative polarity\nitems (NPIs), largely closing the human gap. This\nsuggests that the difﬁculty of these BLiMP cate-\ngories was due to PLM decomposing autoregres-\nsively, and not intrinsic to unsupervised language\nmodel training, as the original results may suggest\n(Warstadt et al., 2020). For some intuition, we\ninclude examples in Table 8. In the subject-verb\nagreement example, BERT seesThe pamphlets and\nresembled those photographs when scoring have\nvs. has, whereas GPT-2 only sees The pamphlets,\nwhich may not be enough to counter the misleading\nadjacent entity Winston Churchill at scoring time.\n4.2 Interpolation with direct models\nWe observed that log g = PLL(W ) is not unduly\naffected by unconditional token frequencies; this\nmitigates degradation in adequacy upon interpola-\ntion with PS2S. Consider a two-word proper noun,\ne.g., W = “San Francisco”:\nlog PLM(W )\n= log PLM(San) + logPLM(Francisco |San)\n≪log PMLM(San |Francisco)\n+ logPMLM(Francisco |San)\n= PLL(W ).\nIt is a highly-ﬂuent but low-probability bigram and\nthus gets penalized by log PLM(W ). Informally,\nPLL(W ) expresses how likely each token is given\nother tokens (self-consistency), while log PLM(W )\nexpresses the unconditional probability of a sen-\ntence, beginning with the costly unconditional term\nPLM(San). We see this in practice when we take\nLM to be GPT-2 (345M) and MLM to be RoBERTa\n(large). Substituting in the actual scores:\nlog PGPT-2(W ) = −8.693\n= (−7.749) + (−0.944)\n≪(−0.006) + (−1.000)\n= −1.006 = PLLRoBERTa(W ).\nBoth give similar probabilities P(Francisco |San)\n≈e−1.0 ≈37%, but differ in the ﬁrst summand.\nWe examine the interplay of this bias with our\nsequence models, in cases where the baseline, GPT-\n2, and BERT gave different top-1 hypotheses (Ta-\nble 8). In our examples, GPT-2 restores ﬂuency\nusing common and repeated words, at the cost of\nadequacy:\nclasping truth and ↦→class in truth and,\nUnion by the Union Sivities ↦→\nUnion by the Union by the Union Civities.\nOne can view these as exacerbations of the rare\nword problem due to overconﬁdent logits (Nguyen\nand Chiang, 2018), and of over-translation (Tu\net al., 2016). Meanwhile, BERT rewards self-\nconsistency, which lets rarer but still-ﬂuent words\nwith better acoustic or translation scores to persist:\nclasping truth and ↦→clasping truth in,\nUnion by the Union Sivities ↦→\nUnion by the Union of LiberCivities,\nSystem Model Output sentence\nBLiMP (S-V agreement) BERT The pamphlets about Winston Churchill have resembled those photographs.\nGPT-2 The pamphlets about Winston Churchill has resembled those photographs.\nBLiMP (island) BERT Who does Amanda ﬁnd while thinking about Lucille?\nGPT-2 Who does Amanda ﬁnd Lucille while thinking about?\nLibriSpeech (dev-other)\nBaseline clasping truth and jail ya in the mouth of the student is that building up or tearing down\nGPT-2 class in truth and jail ya in the mouth of the student is that building up or tearing down\nBERT (adapted) clasping truth in jail gagging the mouth of the student is that building up or tearing down\nTarget clapping truth into jail gagging the mouth of the student is that building up or tearing down\ngl→en (test)\nSource (gl) Traballaba de asesora cient ´ıﬁca naACLU , a Uni´on polas Liberdades Civ´ıs.\nBaseline I worked on a scientiﬁc status on the ACL, the Union by the Union Sivities .\nGPT-2 I worked on a scientiﬁc status on the ACL, the Union by the Union by the Union Civities .\nBERT I worked on a scientiﬁc status on the ACL, the Union by the Union of LiberCivities .\nTarget (en) I was working at the ACLU as the organization ’s science advisor .\nTable 8: Examples of different top-1 hypotheses after ranking the minimal pairs or rescoring hypotheses from 4-\nbest models, with differences highlighted. GPT-2 and BERT both promote ﬂuency, but GPT-2’s left-to-right biased\nscores appear to cause it to overweigh common word sequences at the expense of adequacy.\nwhich preserves the p sound in the ground truth\n(clapping) for ASR, and promotes the more\nglobally-ﬂuent Union by the Unionof LiberCivities.\nWe also see the under-translation (i.e., omission) of\nLiber being corrected, without being discouraged\nby the rare sequence LiberCivities.\nGiven the differences between PLLs and log\nprobabilities, we explore whether ensembling both\nimproves performance in Appendix D. Similar to\nthe largely-dominant results of MLMs on BLiMP\nover GPT-2 (Section 4.1), we ﬁnd that as the MLM\ngets stronger, adding GPT-2 scores has negligible\neffect, suggesting that their roles overlap.\n4.3 Numerical properties of PLL\nPLL’s numerical properties make it an ideal foun-\ndation for future ranking or scoring schemes.\nFor example, given ﬁxed |W | one expects\n−log PMLM(wt |W\\t) to be in the same range for\nall t. Meanwhile −log PLM(wt |W<t) decreases\nas t→|W |, the rate of which was studied in recur-\nrent language models (Takahashi and Tanaka-Ishii,\n2018). We validate this with GPT-2 (Figure 3)\nand BERT (Figure 4). In particular, we see the\noutsized cost of the unconditional ﬁrst unigram in\nFigure 3. This also explains why bi-SANLM was\nmore robust than uni-SANLM at shorter and ear-\nlier positions (Shin et al., 2019); the difference is\nintrinsic to log probabilities versus PLLs, and is\nnot due to model or data size.\nFigure 4 also shows that domain adaptation (Sec-\ntion 3.5) affects PLL’s positionwise cross-entropies.\nCased BERT spikes at position 1, as it observes a\nlowercase word where a capitalized word is ex-\npected. All MLMs spike at the ﬁnal token of an ut-\nterance, before our appended period “.”. Terminal\n1 3 5 7 9 11 13 15 17 19\nContext length (t 1)\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0Cross-entropy\nGPT-2 (117M, cased), test-clean\nGPT-2 (117M, cased), test-other\nGPT-2 (345M, cased), test-clean\nGPT-2 (345M, cased), test-other\nFigure 3: Cross-entropy (natural base) of wt |W<t\nversus context length (t−1) from GPT-2 models, aver-\naged over LibriSpeech’s test utterances.\n1 3 5 7 9 11 13 15 17 19\nToken position (t)\n0\n1\n2\n3\n4\n5\n6Cross-entropy\nBERT (base, cased), test\nBERT (base, uncased), test\nBERT (base, uncased, adapt.), test\nFigure 4: Cross-entropy (natural base) of wt | W\\t\nversus tfrom BERT, averaged over LibriSpeech’s 189\ntest utterances of length |W|= 19 (including “.”).\nwords are difﬁcult to predict in general, but here\nmore so as the BERT+LibriSpeech text corpora\nand the LibriSpeech test set are mismatched; the\nlatter’s ground-truth utterances were segmented by\nvoice activity and not punctuation (Panayotov et al.,\n2015). Otherwise, the averaged cross-entropies are\nﬂat. This, plus our success on BLiMP, suggest posi-\ntionwise scores as a way of detecting “disﬂuencies”\n(at least, those in the form of domain mismatches)\nby observing spikes in cross-entropy; withlog PLM,\nspikes are confounded by the curve in Figure 3.\nIn Appendix C, we plot sentence-level PLLs\nversus |W |and observe linearity as |W |→∞ ,\nwith spikes from the last word and lowercase ﬁrst\nword smoothing out. This behavior motivates our\nchoice of α = 1 .0 when applying the Google\nNMT-style length penalty (Wu et al., 2016) to\nPLLs, which corresponds to the asymptotically-\nlinear LPMLM = (5 + |W |)/(5 + 1). In contrast,\nautoregressive scores like PLM(W ) integrate over\nthe inverse power-law curve in Figure 3. We spec-\nulate that this explains the effectiveness of their\nhyperparameter α = 0 .6, widely used in NMT\nbaselines like ours, as there exists Csuch that\nLPS2S(W ) = (5 + |W |)0.6\n(5 + 1)0.6 ≈\n∫ |W|\n0\nC\n(5 + x)0.4 dx.\n5 Related work\nOur work extends the closest previous works\n(Wang and Cho, 2019; Shin et al., 2019) with re-\ngards to experiments and tasks, as outlined in Sec-\ntion 2.1. Furthermore, neither work considers the\ninference cost of masked rescoring, which we ad-\ndress with our maskless scoring approach, or ana-\nlyze PLL’s numerical properties.\nFuture context. Log probabilities conditioned\non past and future context have been used in MT\n(Finch and Sumita, 2009; Xiong et al., 2011) and\nperennially in ASR (Shi et al., 2013; Arisoy et al.,\n2015; Chen et al., 2017) to positive effect. However,\nthese are not “deep bidirectional” as they model\ninteractions between W<t and W>t via the for-\nward and backward context vectors, while MLMs\nmodel all pairwise interactions ws and ws′via dot-\nproduct attention (compare ELMo versus BERT).\nTheir PLLs would have different properties from\nours (e.g., their cross-entropies in Figure 4 may be\nconvex instead of ﬂat).\nDiscriminative language modeling. Previous\nworks (Roark et al., 2004; Huang et al., 2018)\nhave explored training language models that di-\nrectly optimize for a downstream metric (WER,\nBLEU). While we also eschew using log probabili-\nties from conventional LMs, our approach remains\ngenerative. Log probabilities model the joint dis-\ntribution; PLL does so as well, albeit implicitly\n(Appendix B). PLL’s summands (conditional prob-\nabilities) remain accessible for Gibbs sampling and\nare not tailored to any metric. The two approaches\nare complementary; for example, one could use\nPLL as a “prior” or regularizer for scores given by\ndiscriminatively-ﬁnetuned BERT models in tasks\nlike passage re-ranking (Nogueira and Cho, 2019).\nLanguage model integration. Beyond ﬁnetun-\ning pretrained LMs and MLMs, monolingual pre-\ntraining has also improved NMT performance (Ra-\nmachandran et al., 2017; Conneau and Lample,\n2019). However, modular integration of language\nrepresentation models remains prevalent for vari-\nous pragmatic reasons, similar to fusion in ASR.\nContemporary examples are the use of ﬁnetuned\nBERT scores in a question-answering pipeline\n(Nogueira and Cho, 2019), or “as-is” cosine sim-\nilarity scores from BERT to evaluate generated\ntext (Zhang et al., 2020). For example, one might\nhave no pretrained multilingual LMs for decoder\ninitialization or fusion, as such models are difﬁ-\ncult to train (Ragni et al., 2016). However, one\nmay have an M-BERT or XLM for the target lan-\nguage/domain. Finally, N-best rescoring and pre-\ntraining are not mutually exclusive, though pretrain-\ning may already go partway to improve ﬂuency.\n6 Conclusion\nWe studied scoring with MLM pseudo-log-\nlikelihood scores in a variety of settings. We\nshowed the effectiveness of N-best rescoring with\nPLLs from pretrained MLMs in modern sequence-\nto-sequence models, for both ASR and low- to\nmedium-resource NMT. We found rescoring with\nPLLs can match or outperform comparable scores\nfrom large unidirectional language models (GPT-2).\nWe attributed this to PLL’s promotion of ﬂuency\nvia self-consistency, as demonstrated by improve-\nment on unsupervised acceptability judgements and\nby qualitative analysis. We examined the numeri-\ncal properties of PLLs, proposed maskless scoring\nfor speed, and proposed pseudo-perplexities for in-\ntrinsic evaluation of MLMs, releasing a codebase\nimplementing our work. Future work could ﬁnd ad-\nditional modular uses of MLMs, simplify maskless\nPLL computations, and use PLLs to devise better\nsentence- or document-level scoring metrics.\nAcknowledgments\nWe thank Phillip Keung and Chris Varano for their\nthoughtful suggestions on this work.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn NAACL-HLT.\nEbru Arisoy, Abhinav Sethy, Bhuvana Ramabhadran,\nand Stanley Chen. 2015. Bidirectional recurrent neu-\nral network language models for automatic speech\nrecognition. In ICASSP.\nJulian Besag. 1975. Statistical analysis of non-lattice\ndata. The Statistician, 24(3):179–195.\nPeter F Brown, Vincent J Della Pietra, Stephen A Della\nPietra, and Robert L Mercer. 1993. The mathemat-\nics of statistical machine translation: Parameter esti-\nmation. Computational Linguistics, 19(2):263–311.\nMauro Cettolo, Jan Niehues, Sebastian St ¨uker, Luisa\nBentivogli, R Cattoni, and Marcello Federico. 2015.\nThe IWSLT 2015 evaluation campaign. Technical\nreport, FBK and KIT.\nWilliam Chan, Navdeep Jaitly, Quoc Le, and Oriol\nVinyals. 2016. Listen, attend and spell: A neural\nnetwork for large vocabulary conversational speech\nrecognition. In ICASSP.\nXie Chen, Anton Ragni, Xunying Liu, and Mark JF\nGales. 2017. Investigating bidirectional recurrent\nneural network language models for speech recog-\nnition. In INTERSPEECH.\nNoam Chomsky. 1957. Syntactic structures. Mouton.\nStephane Clinchant, Kweon Woo Jung, and Vassilina\nNikoulina. 2019. On the use of BERT for neural\nmachine translation. In WNGT.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In NeurIPS.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nAndrew Finch and Eiichiro Sumita. 2009. Bidirec-\ntional phrase-based statistical machine translation.\nIn EMNLP.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\ning monolingual corpora in neural machine transla-\ntion. arXiv preprint arXiv:1503.03535.\nJian Guo, He He, Tong He, Leonard Lausen, Mu Li,\nHaibin Lin, Xingjian Shi, Chenguang Wang, Jun-\nyuan Xie, Sheng Zha, et al. 2020. GluonCV and\nGluonNLP: Deep learning in computer vision and\nnatural language processing. JMLR, 21(23):1–7.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014.\nDistilling the knowledge in a neural network. Deep\nLearning Workshop, NeurIPS.\nJiaji Huang, Yi Li, Wei Ping, and Liang Huang. 2018.\nLarge margin neural language model. In EMNLP.\nFrederick Jelinek, Lalit Bahl, and Robert Mercer. 1975.\nDesign of a linguistic statistical decoder for the\nrecognition of continuous speech. IEEE Trans. Inf.\nTheory, 21(3):250–256.\nYoon Kim and Alexander M Rush. 2016. Sequence-\nlevel knowledge distillation. In EMNLP.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality, acceptability, and probabil-\nity: A probabilistic view of linguistic knowledge.\nCognitive Science, 41(5):1202–1241.\nJason Li, Vitaly Lavrukhin, Boris Ginsburg, Ryan\nLeary, Oleksii Kuchaiev, Jonathan M Cohen, Huyen\nNguyen, and Ravi Teja Gadde. 2019. Jasper: An\nend-to-end convolutional neural acoustic model. In\nINTERSPEECH.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nKenton Murray and David Chiang. 2018. Correcting\nlength bias in neural machine translation. In WMT.\nGraham Neubig and Junjie Hu. 2018. Rapid adaptation\nof neural machine translation to new languages. In\nEMNLP.\nToan Q Nguyen and David Chiang. 2018. Improv-\ning lexical choice in neural machine translation. In\nNAACL-HLT.\nToan Q Nguyen and Julian Salazar. 2019. Transform-\ners without tears: Improving the normalization of\nself-attention. In IWSLT.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Pas-\nsage re-ranking with BERT. arXiv preprint\narXiv:1901.04085.\nAaron van den Oord, Yazhe Li, Igor Babuschkin, Karen\nSimonyan, Oriol Vinyals, Koray Kavukcuoglu,\nGeorge van den Driessche, Edward Lockhart, Luis C\nCobo, Florian Stimberg, et al. 2018. Parallel\nWaveNet: Fast high-ﬁdelity speech synthesis. In\nICML.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and\nSanjeev Khudanpur. 2015. LibriSpeech: An ASR\ncorpus based on public domain audio books. In\nICASSP.\nYe Qi, Devendra Singh Sachan, Matthieu Felix, Sar-\nguna Janani Padmanabhan, and Graham Neubig.\n2018. When and why are pre-trained word embed-\ndings useful for neural machine translation? In\nNAACL-HLT.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nAnton Ragni, Edgar Dakin, Xie Chen, Mark JF Gales,\nand Kate M Knill. 2016. Multi-language neural net-\nwork language models. In INTERSPEECH.\nPrajit Ramachandran, Peter J Liu, and Quoc V Le.\n2017. Unsupervised pretraining for sequence to se-\nquence learning. In EMNLP.\nBrian Roark, Murat Saraclar, Michael Collins, and\nMark Johnson. 2004. Discriminative language mod-\neling with conditional random ﬁelds and the percep-\ntron algorithm. In ACL.\nCarson T Sch ¨utze. 1996. The empirical base of lin-\nguistics: Grammaticality judgments and linguistic\nmethodology. Language Science Press.\nYangyang Shi, Martha Larson, Pascal Wiggers, and\nCatholijn M Jonker. 2013. Exploiting the succeed-\ning words in recurrent neural network language mod-\nels. In INTERSPEECH.\nJoongbo Shin, Yoonhyung Lee, and Kyomin Jung.\n2019. Effective sentence scoring method using\nBERT for speech recognition. In ACML.\nAnuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and\nAdam Coates. 2018. Cold fusion: Training seq2seq\nmodels together with language models. In INTER-\nSPEECH.\nFelix Stahlberg, James Cross, and Veselin Stoyanov.\n2018. Simple fusion: Return of the language model.\nIn WMT.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn NeurIPS.\nShuntaro Takahashi and Kumiko Tanaka-Ishii. 2018.\nCross entropy of neural language models at inﬁnity–\na new bound of the entropy rate. Entropy,\n20(11):839.\nShubham Toshniwal, Anjuli Kannan, Chung-Cheng\nChiu, Yonghui Wu, Tara N Sainath, and Karen\nLivescu. 2018. A comparison of techniques for lan-\nguage model integration in encoder-decoder speech\nrecognition. In SLT.\nZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,\nand Hang Li. 2016. Modeling coverage for neural\nmachine translation. In ACL.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a\nmouth, and it must speak: BERT as a Markov ran-\ndom ﬁeld language model. In NeuralGen.\nChenguang Wang, Mu Li, and Alexander J Smola.\n2019. Language models with transformers. arXiv\npreprint arXiv:1904.09408.\nXinyi Wang, Hieu Pham, Zihang Dai, and Graham Neu-\nbig. 2018. SwitchOut: An efﬁcient data augmen-\ntation algorithm for neural machine translation. In\nEMNLP.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. TACL.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTACL, 7:625–641.\nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki\nHayashi, Jiro Nishitoba, Yuya Unno, Nelson En-\nrique Yalta Soplin, Jahn Heymann, Matthew Wies-\nner, Nanxin Chen, et al. 2018. ESPnet: End-to-end\nspeech processing toolkit. In INTERSPEECH.\nThomas Wolf, L Debut, V Sanh, J Chaumond, C De-\nlangue, A Moi, P Cistac, T Rault, R Louf, M Fun-\ntowicz, et al. 2019. HuggingFace’s transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nDeyi Xiong, Min Zhang, and Haizhou Li. 2011. En-\nhancing language models in statistical machine\ntranslation with backward n-grams and mutual infor-\nmation triggers. In ACL.\nYilin Yang, Liang Huang, and Mingbo Ma. 2018.\nBreaking the beam search curse: A study of (re-)\nscoring methods and stopping criteria for neural ma-\nchine translation. In EMNLP.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2020. BERTScore:\nEvaluating text generation with BERT. In ICLR.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tie-Yan Liu.\n2020. Incorporating BERT into neural machine\ntranslation. In ICLR.\nA Experiment details\nA.1 Language models\nImplementation. English BERT, M-BERT, GPT-\n2, and RoBERTa models were served, adapted, and\nﬁnetuned via the GluonNLP toolkit (Guo et al.,\n2020). German BERT and XLM models were\nserved via HuggingFace’s Transformers toolkit\n(Wolf et al., 2019). We release a reference im-\nplementation (a language model scoring package)\nfor our work at https://github.com/awslabs/\nmlm-scoring.\nTraining. When adapting to a corpus we con-\ntinue the training scheme for BERT, i.e., MLM +\nnext-sentence prediction (Devlin et al., 2019), on\nthe new dataset only, until the training loss con-\nverges. We still perform warmup at adaptation\ntime (ratio of 0.01), but continue to use batches of\n256 sequences of contiguous sentences, each with\nlength up to 512.\nScoring. For BERT, M-BERT, and RoBERTa we\nprepend and append [CLS], [SEP] tokens. For\nGPT-2 we prepend and append<|endoftext|>,\nthe default tokens for unconditional generation,\nas we found this outperformed other initial con-\nditions (e.g., a preceding “ .”). For XLM we\nprepend and append </s> (prepending <s> is\nmore proper, but this is due to a bug in Hugging-\nFace Transformer’sXLMTokenizer that we will\nﬁx; changes in results should be negligible). When\ncomputing (pseudo-)perplexity (Section 2.3), these\nspecial tokens’ conditional probabilities are not\nincluded, nor are they counted for token or word\ncounts during length normalization.\nN-best rescoring. We follow the log-linear\nmodel in Section 3.1 with its hyperparameterλ, i.e.,\nweighted addition of (M)LM scores with sequence-\nto-sequence scores. When interpolating MLMs\nwith GPT-2 there is also a hyperparamter γ (Ap-\npendix D). We do grid search on (λ,γ) with in-\ncrements (0.05, 0.1) for the best weights on the\ndevelopment set for downstream WER or BLEU,\nthen evaluate on the corresponding test set. In the\ncase of ties, we choose the largest λ, γ.\nA.2 Automatic speech recognition\nWe use the LibriSpeech corpus (Panayotov et al.,\n2015) for our experiments. To adapt BERT we use\nthe provided 800M-word text-only data, processed\nusing Kaldi to match the normalized, download-\nable corpus2 but with sentences in their original or-\nder (instead of alphabetically as in Kaldi’s recipe),\nto match the long-context training regime of our\nlanguage models. Our LibriSpeech-only BERT\n(base) model was trained on this corpus using Glu-\nonNLP’s recipe, for 1.5M steps.\nWe take pre-existing 100-best lists shared via\ne-mail communication (Shin et al., 2019), which\nwere produced by ESPnet (Watanabe et al., 2018)\non LibriSpeech’s dev and test sets. The ESPnet\nmodel was the sequence-to-sequence BLSTMP\nmodel in the librispeech/asr1 recipe, except with 5\nlayers and a beam size of 100.\nFor speech corpora, to alleviate some of the\ndomain shift from BERT’s original written cor-\npora, we appended “ .” at the end of utterances\nduring adaptation, and appended “ .” to all hy-\npotheses before subword tokenization, masking,\nand token/word counting.\nA.3 Neural machine translation\nOur pretrained model3 is the base Transformer on\nWMT 2014 English-German (Vaswani et al., 2017)\ntrained using GluonNLP’s scripts/machine_\ntranslation. Evaluation and N-best rescoring\nwas on the 3003-sentence test set via --full\n--bleu 13a --beam size 100.\nWe consider 5 low-resource directions from the\nTED Talks dataset (Qi et al., 2018): Arabic (ar),\nGalician (gl), and Slovak (sk) to English; and En-\nglish to Arabic, German (de), languages which\nwere considered in Aharoni et al. (2019). We also\ninclude a more popular benchmark, English to Viet-\nnamese (vi) from the IWSLT '15 evaluation cam-\npaign4 (Cettolo et al., 2015). These give a breadth\nof English-source and English-target pairs and in-\nclude a right-to-left language; more importantly,\nthe three non-English targets are covered by the\n15-language XLMs (Conneau and Lample, 2019).\nOur models are also described as baselines in a\ndedicated work (Nguyen and Salazar, 2019). They\nare base Transformers with 6 layers, 8 heads, an\n8k BPE vocabulary, and dropout of 0.3, except for\ngl→en where we use 4 layers, 4 heads, 3k BPE,\n2https://www.openslr.org/resources/11/\nlibrispeech-lm-norm.txt.gz\n3http://apache-mxnet.s3-accelerate.\ndualstack.amazonaws.com/gluon/models/\ntransformer_en_de_512_WMT2014-e25287c5.\nzip\n4https://nlp.stanford.edu/projects/\nnmt/\nand a dropout of 0.4 due to its signiﬁcantly smaller\nsize. We use a warmup of 8k steps and the default\nhyperparameters (Vaswani et al., 2017). We apply\nGNMT length normalization (Wu et al., 2016) with\nα= 0.6 to the sequence-to-sequence log probabili-\nties, and α= 1.0 to the PLLs (motivation is given\nin Section 4.3), with respect to their chosen tok-\nenization’s lengths. We compute tokenized BLEU\nvia multi-bleu.perl from Moses5 to compare with\npast works on these datasets.\nB BERT as a generative model\nIn their published version (Wang and Cho, 2019),\nthe authors claimed that BERT is a Markov random\nﬁeld language model (MRF-LM) where {wt}|W|\nt=1\nare categorical random variables (over the vocab-\nulary) in a fully-connected graph G. They deﬁne\na potential over cliques of Gsuch that all partial-\ngraph potentials are exp(0) = 1 and the full-graph\npotential is exp ∑|W|\nt=1 log φt(G), where log φt(G)\nis the logit corresponding to log PMLM(wt |W\\t)\n(although in their formulation, one could include\nthe softmax into the feature function fθ and take\nlog φt(G) = PLL(G) exactly).\nAbusing notation, we write W interchangeably\nwith its graph G. An MRF deﬁned this way would\ngive the joint distribution:\nPMLM(W ) = 1\nZ\n|W|∏\nt=1\nφt(W ) = 1\nZ exp PLL(W ),\nwhere Zis the partition function\nZ =\n∑\nW′∈S\n|W′|∏\nt=1\nφt(W ′) =\n∑\nW′∈S\nexp PLL(W ′),\nmaking this a valid distribution by normalizing\nover all sequences of the same length |W |, the set\ndenoted by S.\nOne then hopes to say that log PMLM(wt |W\\t)\nis the conditional distribution of this MRF. How-\never, their erratum6 notes this is not the case, as wt\nwould be affected by other log potentials as well.\nIn practice, one could instead a priori make the\nmodeling assumption\ng(W ) = PMLM(W ) := 1\nZ exp PLL(W ),\n5https://statmt.org\n6“BERT has a Mouth and must Speak, but it is not\nan MRF” from https://sites.google.com/site/\ndeepernn/home/blog\nas done in the work on bi-RNNLMs (Chen et al.,\n2017). They choose to model the distribution of\nsentences as a product-of-experts wt |W\\t, whose\nparameters are shared via the underlying bi-RNN.\nSuppose one had access to this “normalized\nMLM probability”. In the log-linear setting (Sec-\ntion 3.1), we get\nlog PS2S(W |X) + λlog PMLM(W )\n= ··· + λlog\n(1\nZ exp PLL(W )\n)\n= ··· + λPLL(W ) −λlog Z.\nFor ﬁxed λand Z(which is intrinsic to the MLM),\nwe see that λlog Z does not affect rank-ordering\nwhen taking arg maxto get the best hypothesis ˆW .\nHence, the heuristic interpolation enacted by λis\n“the same” for normalized log PLM, unnormalized\nPLL, and our hypothetical log PMLM. The remain-\ning issue is whether λhas the same effect for all\nlengths |W |, which one mitigates by applying the\ncorrect length penalties to f and g(Section 4.3).\nC Pseudo-perplexity and rescoring\nWe brieﬂy examine the relationship between PPPL\n(Section 2.3) and metrics post-rescoring. We plot\nnegative PLLs versus |W |and observe linearity,\nhelping justify our simple average over length:\n0 5 10 15 20 25 30\nSentence length |W|\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90Negative PLL\nBERT (base, cased), test\nBERT (base, uncased), test\nBERT (base, uncased, adapt.), test\nFigure 5: Negative pseudo-log-likelihood scores versus\nsentence length (in tokens) from BERT, averaged over\nLibriSpeech’s test utterances of each length.\nNote that in this section, we consider PPPLs nor-\nmalized by number of words (PPPLw) to improve\ncomparability between different subword vocab-\nularies. We see a good correspondence between\nPPPLw improvements and post-rescoring WER in\nTable 9, and post-rescoring BLEU in Table 10.\nThus, one could compute a new pretrained\nmodel’s word-normalized PPPL on a small target-\nModel\ntest\nclean other\nPPPLw WER PPPL w WER\nBERT (base, cased) 24.18 5.41 27.47 17.41\nRoBERTa (base, cased) 21.85 5.25 24.54 17.18\nBERT (large, cased) 17.49 5.25 19.59 16.97\nBERT (base, uncased) 17.49 5.14 19.24 16.97\nRoBERTa (large, cased) 14.78 5.05 16.23 16.79\nBERT (base, Libri. only) 9.86 4.79 10.55 16.50\nBERT (base, unc., adapt.) 6.63 4.58 6.56 15.96\nTable 9: Word-normalized PPPL vs. WER on Lib-\nriSpeech after rescoring, for models with different to-\nken vocabularies. WERs are from Table 2 and Table 5.\nModel\ndev\nar→en gl →en sk →en\nPPPLw BLEU PPPL w BLEU PPPL w BLEU\nB-base 13.08 35.71 11.86 20.25 13.20 29.74\nB-large 10.17 35.79 9.48 20.21 10.43 29.79\nR-base 9.77 35.86 9.36 20.21 9.75 29.79\nR-large 6.26 36.02 6.08 20.44 6.29 30.05\nTable 10: Word-normalized PPPL vs. BLEU of cased\nBERT (B) and RoBERTa (R) on English gold sentences\nin the TED Talks corpus.\ndomain sample to quickly assess whether rescoring\nwith it could improve on the previous model.\nD Combining MLMs and GPT-2\nWe ask whether scores from a unidirectional LM\nare complementary with a masked LM for rescor-\ning. When interpolating, we introduce γsuch that:\nlog g(W ) = (1 −γ) logPLM(W ) + γ PLL(W ).\nOur results are in Table 11:\nModel test + GPT-2\nclean other clean other\nbaseline (100-best) 7.26 20.37 5.30 17.26\nBERT (large, cased) 5.25 16.97 5.03 16.80\nRoBERTa (large, cased) 5.05 16.79 4.93 16.71\nBERT (base, unc., adapt.) 4.58 15.96 4.50 15.92\nTable 11: WERs on LibriSpeech after hypothesis\nrescoring, with and without interpolating with GPT-2\n(345M, cased).\nAs the MLM gets stronger, the improvement\nfrom adding scores from GPT-2 goes to zero, sug-\ngesting that their roles overlap at the limit. How-\never, unlike recent work (Shin et al., 2019) but like\nprevious work (Chen et al., 2017), we found that\ninterpolating with a unidirectional LM remained\noptimal, though our models are trained on different\ndatasets and may have an ensembling effect.",
  "topic": "Fluency",
  "concepts": [
    {
      "name": "Fluency",
      "score": 0.6992855072021484
    },
    {
      "name": "Autoregressive model",
      "score": 0.6623459458351135
    },
    {
      "name": "Computer science",
      "score": 0.6532407402992249
    },
    {
      "name": "Inference",
      "score": 0.640617847442627
    },
    {
      "name": "Language model",
      "score": 0.6393203139305115
    },
    {
      "name": "Sentence",
      "score": 0.6041322946548462
    },
    {
      "name": "Masking (illustration)",
      "score": 0.5126723051071167
    },
    {
      "name": "Natural language processing",
      "score": 0.4968912899494171
    },
    {
      "name": "Machine translation",
      "score": 0.44963687658309937
    },
    {
      "name": "Speech recognition",
      "score": 0.42566144466400146
    },
    {
      "name": "Domain adaptation",
      "score": 0.4130026698112488
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4093434810638428
    },
    {
      "name": "Linguistics",
      "score": 0.3399292826652527
    },
    {
      "name": "Mathematics",
      "score": 0.1986072063446045
    },
    {
      "name": "Econometrics",
      "score": 0.185624897480011
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 7
}