{
    "title": "Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting",
    "url": "https://openalex.org/W4389518951",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2169407364",
            "name": "Fanghua Ye",
            "affiliations": [
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A2152013896",
            "name": "Meng Fang",
            "affiliations": [
                "University of Liverpool"
            ]
        },
        {
            "id": "https://openalex.org/A2104803320",
            "name": "Sheng-Hui Li",
            "affiliations": [
                "Uppsala University"
            ]
        },
        {
            "id": "https://openalex.org/A2165725343",
            "name": "Emine Yılmaz",
            "affiliations": [
                "University College London"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4385573600",
        "https://openalex.org/W2951534261",
        "https://openalex.org/W4385573111",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3160883893",
        "https://openalex.org/W3214455632",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W4385572807",
        "https://openalex.org/W3103251620",
        "https://openalex.org/W3035169992",
        "https://openalex.org/W4377372007",
        "https://openalex.org/W3175474240",
        "https://openalex.org/W4385573151",
        "https://openalex.org/W4362707064",
        "https://openalex.org/W3027639267",
        "https://openalex.org/W4366388859",
        "https://openalex.org/W4367365649",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4385571260",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W4310629099",
        "https://openalex.org/W4388778348",
        "https://openalex.org/W3034368386",
        "https://openalex.org/W4385569970",
        "https://openalex.org/W4385569686",
        "https://openalex.org/W4385573137",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W3180230246",
        "https://openalex.org/W4385573081",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4284685693",
        "https://openalex.org/W3118668786",
        "https://openalex.org/W2888302696",
        "https://openalex.org/W2970996870",
        "https://openalex.org/W4327640211",
        "https://openalex.org/W3004092718",
        "https://openalex.org/W3044364015",
        "https://openalex.org/W4385571011",
        "https://openalex.org/W3171244865",
        "https://openalex.org/W4385889719",
        "https://openalex.org/W3115037692",
        "https://openalex.org/W3198536471",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3210277894",
        "https://openalex.org/W2971169991",
        "https://openalex.org/W4285189764",
        "https://openalex.org/W4385573970",
        "https://openalex.org/W4313483544",
        "https://openalex.org/W4323697341",
        "https://openalex.org/W4280534475",
        "https://openalex.org/W4285249364",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4321650154",
        "https://openalex.org/W2590822507",
        "https://openalex.org/W3014434762",
        "https://openalex.org/W4206104244",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W4319166659",
        "https://openalex.org/W4297412056",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W4252076394"
    ],
    "abstract": "Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverage human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions. We define four essential properties for well-formed rewrites and incorporate all of them into the instruction. In addition, we introduce the role of rewrite editors for LLMs when initial query rewrites are available, forming a \"rewrite-then-edit\" process. Furthermore, we propose distilling the rewriting capabilities of LLMs into smaller models to reduce rewriting latency. Our experimental evaluation on the QReCC dataset demonstrates that informative query rewrites can yield substantially improved retrieval performance compared to human rewrites, especially with sparse retrievers.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5985–6006\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEnhancing Conversational Search: Large Language Model-Aided\nInformative Query Rewriting\nFanghua Ye\nUniversity College London\nfanghua.ye.19@ucl.ac.uk\nMeng Fang\nUniversity of Liverpool\nMeng.Fang@liverpool.ac.uk\nShenghui Li\nUppsala University\nshenghui.li@it.uu.se\nEmine Yilmaz\nUniversity College London\nemine.yilmaz@ucl.ac.uk\nAbstract\nQuery rewriting plays a vital role in enhancing\nconversational search by transforming context-\ndependent user queries into standalone forms.\nExisting approaches primarily leverage human-\nrewritten queries as labels to train query rewrit-\ning models. However, human rewrites may lack\nsufficient information for optimal retrieval per-\nformance. To overcome this limitation, we pro-\npose utilizing large language models (LLMs) as\nquery rewriters, enabling the generation of in-\nformative query rewrites through well-designed\ninstructions. We define four essential proper-\nties for well-formed rewrites and incorporate\nall of them into the instruction. In addition, we\nintroduce the role of rewrite editors for LLMs\nwhen initial query rewrites are available, form-\ning a “rewrite-then-edit” process. Furthermore,\nwe propose distilling the rewriting capabilities\nof LLMs into smaller models to reduce rewrit-\ning latency. Our experimental evaluation on\nthe QReCC dataset demonstrates that informa-\ntive query rewrites can yield substantially im-\nproved retrieval performance compared to hu-\nman rewrites, especially with sparse retrievers.1\n1 Introduction\nConversational search has gained significant promi-\nnence in recent years with the proliferation of digi-\ntal virtual assistants and chatbots, enabling users to\nengage in multiple rounds of interactions to obtain\ninformation (Radlinski and Craswell, 2017; Dal-\nton et al., 2021; Gao et al., 2023). This emerging\nsearch paradigm offers remarkable advantages in\nassisting users with intricate information needs and\ncomplex tasks (Yu et al., 2021). However, a fun-\ndamental challenge in conversational search lies in\naccurately determining users’ current search intents\nwithin the conversational context.\nAn effective approach that has gained increasing\nattention addresses this challenge of conversational\n1Our implementation is available at: https://github.\ncom/smartyfh/InfoCQR.\nWhatjobdidElizabethBlackwell have?\nIn what field?\nShe was a lecturer.\nShe was a lecturer in midwifery.\nDid she do well?\nDid Elizabeth Blackwell do well?Did Elizabeth Blackwell do well as a lecturer in midwifery?Human rewriteInformative rewrite\nFigure 1: An example showing that human rewrites may\noverlook valuable contextual information. Specifically,\nthe omission of the phrase “as a lecturer in midwifery”\nmakes it challenging for retrieval systems to understand\nthe original query comprehensively.\ncontext modeling by performing query rewriting\n(Elgohary et al., 2019; Yu et al., 2020; Vakulenko\net al., 2021; Wu et al., 2022; Mo et al., 2023).\nThis approach transforms context-dependent user\nqueries into self-contained queries, thereby allow-\ning the utilization of existingoff-the-shelf retrievers\nthat have been extensively validated for standalone\nqueries. For example, the user query “Did she do\nwell?” illustrated in Figure 1 can be rewritten into\n“Did Elizabeth Blackwell do well as a lecturer in\nmidwifery?” which is context-independent.\nPrevious studies (Anantha et al., 2021; Vaku-\nlenko et al., 2021; Qian and Dou, 2022; Hao et al.,\n2022) predominantly depend on human-rewritten\nqueries as supervised labels to train query rewriting\nmodels. Although human-rewritten queries tend to\nperform better than the original queries, they may\nnot be informative enough for optimal retrieval per-\nformance (Chen et al., 2022; Wu et al., 2022). This\nlimitation arises from the fact that human rewriters\nare only concerned with addressing ambiguity is-\nsues, such as coreference and omission, when trans-\nforming the original query into a self-contained\n5985\nform. Such a simple rewriting strategy may over-\nlook lots of valuable information within the conver-\nsational context (refer to Figure 1 for an example),\nwhich has the potential to enhance the effectiveness\nof the retriever. As a consequence, existing query\nrewriting models learned from human rewrites can\nonly achieve sub-optimal performance.\nA straightforward approach to improving the in-\nformativeness of rewritten queries is to provide hu-\nman annotators with more comprehensive instruc-\ntions so that they can rewrite the original queries\nto be not only unambiguous but also informative.\nHowever, this approach has several disadvantages,\nincluding being expensive, increasing workload\nfor human annotators, and potentially leading to\nhigher inconsistencies among rewrites from differ-\nent annotators. Therefore, it is necessary to explore\nalternative approaches.\nIn this paper, we propose the utilization of large\nlanguage models (LLMs) for query rewriting, lever-\naging their impressive capabilities in following in-\nstructions and demonstrations (Brown et al., 2020;\nWei et al., 2021; Ouyang et al., 2022; Wei et al.,\n2023). We consider two settings to prompt LLMs\nas query rewriters. In the zero-shot learning set-\nting, only an instruction is provided, while in the\nfew-shot learning setting, both an instruction and a\nfew demonstrations are given. To develop suitable\ninstructions, we first identify four essential proper-\nties that characterize a well-formed rewritten query.\nThen, we design an instruction that incorporates all\nfour properties. However, generating rewrites with\nall these properties may pose challenges for LLMs\ndue to the intricacy of the instruction (Ouyang et al.,\n2022; Jang et al., 2023). In view of this, we pro-\npose an additional role for LLMs as rewrite editors.\nInspired by the fact that humans excel at editing\nrather than creating from scratch, the purpose of\nthe rewrite editor is to edit initial rewrites provided,\nforming a “rewrite-then-edit” process. These initial\nrewrites can be generated by smaller query rewrit-\ning models or even by the LLM itself. Furthermore,\nconsidering the potential time overhead and high\ncosts associated with LLMs, we suggest distilling\ntheir rewriting capabilities into smaller models us-\ning their generated rewrites as training labels.\nOur contributions are summarized as follows:\n• We are the first to introduce the concept of infor-\nmative conversational query rewriting and metic-\nulously identify four desirable properties that a\nwell-crafted rewritten query should possess.\n• We propose to prompt LLMs as both query rewrit-\ners and rewrite editors by providing clear instruc-\ntions that incorporate all the desirable properties.\nIn addition, we employ distillation techniques to\ncondense the rewriting capabilities of LLMs into\nsmaller models to improve rewriting efficiency.\n• We demonstrate the effectiveness of informative\nquery rewriting with two off-the-shelf retrievers\n(sparse and dense) on the QReCC dataset. Our\nresults show that informative query rewrites can\noutperform human rewrites, particularly in the\ncontext of sparse retrieval.\n2 Task Formulation\nThe primary objective of conversational search is\nto identify relevant passages from a vast collection\nof passages in response to the current user query.\nFormally, let Qi and Ai be the user query and sys-\ntem response at turn i, respectively. Furthermore,\nlet Xt = {Q1, A1, . . . , Qt−1, At−1}represent the\nconversational context up to turn t. Then, the task\nof conversational search can be formulated as re-\ntrieving top-k relevant passages, denoted as Rk,\nfrom a large passage collection Cgiven the current\nuser query Qt and its associated context Xt. This\nretrieval process is accomplished by a retriever de-\nfined as f : (Qt, Xt, C) → Rk, where Rk is a\nsubset of Cand k is considerably smaller than the\ntotal number of passages in C.\nThe unique challenge in conversational search\nis incorporating the conversational context while\nretrieving relevant passages, which cannot be di-\nrectly addressed by existing retrievers designed for\nstandalone queries. Additionally, re-training re-\ntrievers tailored for conversational queries can be\nexpensive or even infeasible due to complex sys-\ntem designs or limited data availability (Wu et al.,\n2022). To overcome the need for re-training, query\nrewriting is employed as an effective solution (Lin\net al., 2021c; Mo et al., 2023). Query rewriting\ninvolves transforming the context-dependent user\nquery Qt into a self-contained standalone query Q′\nt\nby extracting relevant information from the con-\ntext Xt. Consequently, any existing off-the-shelf\nretrieval systems designed for standalone queries\ncan be leveraged by taking Q′\nt as the input query to\nfind passages that are relevant to the original user\nquery Qt, i.e., f : (Q′\nt, C) →Rk.\nThe utilization of query rewriting shifts the chal-\nlenge of modeling conversational context from the\nretriever end to the query rewriting model end. As\n5986\nGiven a question and its context and a rewrite that decontextualizes the question, edit the rewrite to create a revised version that fully addresses coreferences and omissions in the question without changing the original meaning of the question but providing more information. The new rewrite should not duplicate any previously asked questions in the context. If there is no need to edit the rewrite, return the rewrite as-is.\nInstruction to prompt LLM as rewriteeditorGiven a question and its context, decontextualize the question by addressing coreference and omission issues. The resulting question should retain its original meaning and be as informative as possible, and should not duplicate any previously asked questions in the context.\nQ:Who proposed that atoms are the basic units of matter? A:John Dalton proposed that each chemical element is composed of atoms of a single, unique type, and they can combine to form more complex structures called chemical compounds.Question:How did the proposal come about?Rewrite:How did John Dalton's proposal that each chemical element is composed of atoms of a single unique type, and they can combine to form more complex structures called chemical compounds come about?\nQ:Who proposed that atoms are the basic units of matter? A:John Dalton proposed that each chemical element is composed of atoms of a single, unique type, and they can combine to form more complex structures called chemical compounds.Question:How did the proposal come about?Rewrite:How did John Dalton's proposal come about?Edit:How did John Dalton's proposal that each chemical element is composed of atoms of a single unique type, and they can combine to form more complex structures called chemical compounds come about?\nQ:What job did Elizabeth Blackwell have? A:A lecturer in midwifery.Question:Did she do well?Rewrite:\nQ:What job did Elizabeth Blackwell have? A:A lecturer in midwifery.Question:Did she do well?Rewrite:Did Elizabeth Blackwell do well as a lecturer?Edit:\nndemonstrations\nInstruction to prompt LLM as query rewriter\nndemonstrations\nTest instance Test instance\nDid Elizabeth Blackwell do well as a lecturer?LLMDid Elizabeth Blackwell do well as a lecturer in midwifery?LLM\nFigure 2: Our proposed approach involves prompting LLMs as query rewriters and rewrite editors through clear\nand well-designed instructions, along with appropriate demonstrations. In the absence of demonstrations, the LLM\nfunctions as a zero-shot query rewriter. We explicitly incorporate the requirement that rewritten queries should be as\ninformative as possible into the instructions for generating informative query rewrites.\nthus, the effectiveness of retrieval results heavily re-\nlies on the employed query rewriting models. Only\nwhen appropriate rewritten queries are generated\ncan an off-the-shelf retrieval system return highly\nrelevant passages.\n3 Approach\nIn contrast to relying on human annotators to gener-\nate more informative rewrites or developing more\ncomplex models to closely replicate existing human\nrewrites, we propose to prompt LLMs to generate\ninformative query rewrites simply by providing\nclear instructions and appropriate demonstrations,\navoiding the requirement for extensive human ef-\nfort and complicated model designs. Figure 2 illus-\ntrates our proposed approach.\n3.1 Prompting LLM as Query Rewriter\nRecent work (Wei et al., 2021; Ouyang et al., 2022;\nPeng et al., 2023) has demonstrated the strong ca-\npability of LLMs in following given instructions to\ngenerate coherent and contextually appropriate text.\nInspired by this, it is natural to consider employing\nLLMs as query rewriters. Before delving into the\ndetails of how we can prompt an LLM as a query\nrewriter, we first describe the desirable properties\nthat a well-crafted rewritten query should possess:\n• Correctness: The rewritten query should pre-\nserve the meaning of the original query, ensuring\nthat the user’s intent remains unchanged.\n• Clarity: The rewritten query should be unam-\nbiguous and independent of the conversational\ncontext, enabling it to be comprehensible by peo-\nple outside the conversational context. This clar-\nity can be achieved by addressing coreference\nand omission issues arising in the original query.\n• Informativeness: The rewritten query should in-\ncorporate as much valuable and relevant informa-\ntion from the conversational context as possible,\nthereby providing more useful information to the\noff-the-shelf retriever.\n• Nonredundancy: The rewritten query should\navoid duplicating any query previously raised in\nthe conversational context, as it is important to\nensure that the rewritten query only conveys the\nintent and meaning of the current query.\nIn order to effectively instruct an LLM in gen-\nerating query rewrites that embody the aforemen-\ntioned four properties, it is essential to formulate\n5987\nappropriate instructions. As an illustrative example,\nwe adopt the following instruction in this work:\n“Given a question and its context, de-\ncontextualize the question by addressing\ncoreference and omission issues. The re-\nsulting question should retain its original\nmeaning and be as informative as possi-\nble, and should not duplicate any previ-\nously asked questions in the context.”2\nThis instruction takes all four desirable properties\nof a good rewritten query into account simultane-\nously. Building upon this instruction, we explore\ntwo settings to prompt an LLM for query rewriting.\n3.1.1 Zero-Shot Learning (ZSL) Setting\nIn the ZSL setting, the LLM is instructed to gener-\nate a rewritten query Q′\nt using only the information\nprovided by the current query Qt and its associated\nconversational context Xt, without having access\nto any human-labeled instances. In this setting, we\nentirely rely on the LLM’s capability to understand\nand follow instructions to perform query rewriting.\nSpecifically, we append Xt and Qt to the instruc-\ntion I as the prompt and feed this prompt to the\nLLM for sampling the rewrite Q′\nt:\nQ′\nt ∼LLM(I||Xt||Qt), (1)\nwhere ||denotes concatenation. The detailed for-\nmat of the prompt is shown in Appendix D.\n3.1.2 Few-Shot Learning (FSL) Setting\nIn the FSL setting, the LLM is provided with both\nthe instruction and a small number of demonstra-\ntions. This type of prompting is commonly referred\nto as in-context learning, which has been shown to\nbe effective in adapting LLMs to new tasks (Brown\net al., 2020; Min et al., 2022a,b; Wei et al., 2023;\nSun et al., 2023; Ram et al., 2023). In this setting,\neach demonstration consists of a query Q, a con-\nversational context X, and a rewrite Q′. We denote\nthe concatenation of these demonstrations as:\nD= (X1, Q1, Q′1)||. . .||(Xn, Qn, Q′n), (2)\nwhere n represents the total number of demonstra-\ntions. By placing Dbetween the instruction I and\n2Note that in this instruction, we use the term “question”\ninstead of “query”, as each query is referred to as a question\nin the dataset we employed for experimentation. We believe\nthis tiny variation would not significantly impact the quality\nof generated rewrites. Additionally, other instructions with a\nsimilar meaning can also be applied.\nthe test instance (Xt, Qt) as the prompt to the LLM,\nthe rewrite Q′\nt is then sampled as follows:\nQ′\nt ∼LLM(I||D||Xt||Qt). (3)\nNote that the query rewrites utilized in the demon-\nstrations should be well-designed, ensuring that\nthey have the aforementioned four properties. Oth-\nerwise, the LLM may be misled by these demon-\nstrations. For a more detailed description of the\ndemonstrations used in our experiments, please re-\nfer to Appendix D.\n3.2 Prompting LLM as Rewrite Editor\nDespite the proficiency of LLMs in following in-\nstructions and demonstrations, recent work (Dong\net al., 2022; Liu et al., 2022; Mosbach et al., 2023)\nsuggests that they may encounter difficulties when\nfaced with complex tasks or intricate requirements.\nThis limitation highlights that it can be challenging\nfor LLMs to generate query rewrites with all the\ndesirable properties mentioned above. In order to\naddress this challenge, we propose an alternative\napproach in which an LLM is prompted as a rewrite\neditor whose primary function is to edit provided\ninitial rewrites instead of being prompted as a query\nrewriter who needs to generate query rewrites from\nscratch. This approach draws inspiration from the\nobservation that humans often find it easier to edit\nexisting content than to create it from scratch.\nIn this work, we adopt the FSL setting to prompt\nLLMs as rewrite editors. In addition to the queryQ,\nthe conversational context X, and the rewrite Q′,\nwe introduce an initial rewrite ˆQ for each demon-\nstration. We represent the concatenation of these\naugmented demonstrations as:\n˜D= (X1, Q1, ˆQ1, Q′1)||. . .||(Xn, Qn, ˆQn, Q′n).\n(4)\nFor a test instance (Xt, Qt), accompanied by an ini-\ntial rewrite ˆQt, we obtain the edited (final) rewrite\nQ′\nt through the following procedure:\nQ′\nt ∼LLM(˜I||˜D||Xt||Qt||ˆQt), (5)\nwhere ˜I denotes the modified instruction. Please\nrefer to Figure 2 and Appendix D for details.\nThe initial rewrite can be generated by a small\nquery rewriting model, such as T5QR (Lin et al.,\n2020; Wu et al., 2022). It can also be generated\nby an LLM, following the prompting method de-\nscribed in the previous subsection. When an LLM\nis employed as both the query rewriter and rewrite\neditor, the “rewrite-then-edit” process enables the\nLLM to perform self-correction (Gou et al., 2023).\n5988\n3.3 Distillation: LLM as Rewriting Teacher\nOne major obstacle in effectively leveraging LLMs\nfor query rewriting is the substantial demand for\nmemory and computational resources (Hsieh et al.,\n2023), which can further result in significant time\noverhead. Besides, the cost can be extremely high\nwhen there is a lack of in-house models, necessitat-\ning the reliance on third-party API services as the\nonly option. To address these issues, we propose\nto fine-tune a small query rewriting model using\nrewrites generated by an LLM as ground-truth la-\nbels. In this approach, the LLM assumes the role of\na teacher, while the smaller query rewriting model\nacts as a student. The fine-tuning process distills\nthe teacher’s rewriting capabilities into the student.\nThis technique is known as knowledge distillation\n(Gou et al., 2021) and has recently been utilized to\ndistill LLMs for various other tasks (Shridhar et al.,\n2022; Magister et al., 2022; Marjieh et al., 2023).\nFollowing previous work (Lin et al., 2020; Wu\net al., 2022), we adopt T5 (Raffel et al., 2020) as\nthe student model (i.e., the small query rewriting\nmodel). The input to the model is the concatenation\nof all utterances in the conversational context Xt\nand the current user queryQt. In order to differenti-\nate between user queries and system responses, we\nprepend a special token <Que> to each user query\nand a special token <Ans> to each system response.\nThe output of the model is the rewrite Q′\nt, which\nis sampled from the employed LLM. The model is\nfine-tuned using the standard cross-entropy loss to\nmaximize the likelihood of generating Q′\nt.\n4 Experimental Setup\n4.1 Dataset & Evaluation Metrics\nFollowing previous work (Wu et al., 2022; Mo\net al., 2023), we leverage QReCC (Anantha et al.,\n2021) as our experimental dataset. QReCC consists\nof 14K open-domain English conversations with a\ntotal of 80K question-answer pairs. Each user ques-\ntion is accompanied by a human-rewritten query,\nand the answers to questions within the same con-\nversation may be distributed across multiple web\npages. There are in total 10M web pages with each\ndivided into several passages, leading to a collec-\ntion of 54M passages3. The task of conversational\nsearch is to find relevant passages for each question\nfrom this large collection and gold passage labels\nare provided if any. The conversations in QReCC\n3The dataset and passage collection are available athttps:\n//zenodo.org/record/5115890#.YZ8kab3MI-Q.\nare sourced from three existing datasets, includ-\ning QuAC (Choi et al., 2018), Natural Questions\n(Kwiatkowski et al., 2019), and TREC CAsT-19\n(Dalton et al., 2020). For ease of differentiation,\nwe refer to these subsets as QuAC-Conv, NQ-Conv,\nand TREC-Conv, respectively. Note that TREC-\nConv only appears in the test set. For a comprehen-\nsive evaluation, we present experimental results not\nonly on the overall dataset but also on each subset.\nFor additional information and statistics regarding\nthe dataset, please refer to Appendix A.\nTo evaluate the retrieval results, we adopt mean\nreciprocal rank ( MRR), mean average precision\n(MAP), and Recall@10 (R@10) as evaluation met-\nrics. We employ the pytrec_eval toolkit (Van Gy-\nsel and de Rijke, 2018) for the computation of all\nmetric values.\n4.2 Comparison Methods\nSince our focus is on the effectiveness of informa-\ntive query rewrites, two straightforward baseline\nmethods are Original, which uses the user question\nin its original form as the search query, andHuman,\nwhich utilizes the query rewritten by a human as\nthe search query. We also include three supervised\nmodels as baselines, including T5QR (Lin et al.,\n2020), which fine-tunes the T5-base model (Raffel\net al., 2020) as a seq2seq query rewriter, ConQRR\n(Wu et al., 2022), which employs reinforcement\nlearning to train query rewriting models by directly\noptimizing retrieval performance, and ConvGQR\n(Mo et al., 2023), which combines query rewriting\nwith potential answer generation to improve the\ninformativeness of the search query.\nFor our proposed approach, we investigate four\nvariants, namely RW(ZSL), RW(FSL), ED(Self),\nand ED(T5QR). RW(ZSL) prompts an LLM as a\nquery rewriter in the ZSL setting, while RW(FSL)\nprompts an LLM as a query rewriter in the FSL set-\nting. By comparison, ED(Self) prompts an LLM as\na rewrite editor, wherein the initial rewrites are gen-\nerated by RW(FSL) with the same LLM applied.\nED(T5QR) also prompts an LLM as a rewrite edi-\ntor, but the initial rewrites are generated by T5QR.\nFor simplicity, we only prompt LLMs as rewrite\neditors in the FSL setting.\n4.3 Retrieval Systems\nWe experiment with two types of off-the-shelf re-\ntrievers to explore the effects of informativeness in\nquery rewrites on conversational search:\n5989\nQuery QReCC(8209) QuAC-Conv(6396) NQ-Conv(1442) TREC-Conv(371)\nMRR MAP R@10 MRR MAP R@10 MRR MAP R@10 MRR MAP R@10\nSparse (BM25)\nOriginal 9.30 8.87 15.50 9.29 8.84 15.20 9.06 8.64 15.14 10.30 10.27 22.10\nHuman 39.81 38.45 62.65 40.32 38.98 62.90 40.78 39.05 63.80 27.34 27.04 53.77\nT5QR 33.67 32.50 53.68 34.04 32.90 53.83 34.24 32.66 53.92 25.23 24.96 50.13\nConQRR 38.30 - 60.10 39.50 - 61.60 37.80 - 58.00 19.80 - 43.50\nConvGQR 44.10 - 64.40 - - - - - - - - -\nRW(ZSL) 42.63 41.31 60.46 45.43 44.11 63.20 36.43 34.81 54.69 18.50 18.26 35.58\nRW(FSL) 46.96 45.53 65.57 49.81 48.38 68.28 41.51 39.71 60.13 19.02 18.86 39.89\nED(Self) 49.39 47.89 67.01 53.01 51.52 70.46 41.57 39.69 59.63 17.43 17.08 36.25\nED(T5QR) 47.93 46.40 66.25 50.67 49.18 68.84 42.69 40.64 60.67 21.04 20.79 43.26\nDense (GTR)\nOriginal 12.12 11.49 18.74 11.34 10.69 17.79 13.11 12.57 19.49 21.76 21.11 32.08\nHuman 43.15 41.27 66.12 40.67 38.92 64.59 54.01 51.25 73.13 43.74 42.98 65.23\nT5QR 37.67 35.93 58.65 35.51 33.88 57.23 46.95 44.47 64.43 38.94 38.16 60.51\nConQRR 41.80 - 65.10 41.60 - 65.90 45.30 - 64.10 32.70 - 55.20\nConvGQR 42.00 - 63.50 - - - - - - - - -\nRW(ZSL) 40.64 38.95 62.28 40.12 38.48 62.47 44.85 42.57 63.58 33.26 32.88 54.09\nRW(FSL) 43.89 42.09 66.45 43.50 41.78 66.87 48.60 46.12 68.10 32.37 31.79 52.65\nED(Self) 44.99 43.19 67.34 45.21 43.48 68.30 47.64 45.20 67.27 30.91 30.48 51.03\nED(T5QR) 44.76 42.90 66.64 44.29 42.50 66.65 49.67 47.12 69.22 33.90 33.43 56.47\nTable 1: Passage retrieval performance of sparse and dense retrievers with various query rewriting methods on the\nQReCC test set and its three subsets. The best results are shown in bold and the second-best results are underlined.\nBM25 BM25 (Robertson et al., 2009) is a classic\nsparse retriever. Following Anantha et al. (2021),\nwe employ Pyserini (Lin et al., 2021a) with hy-\nparameters k1 = 0.82 and b = 0.68.\nGTR GTR (Ni et al., 2022) is a recently proposed\ndense retriever4. It has a shared dual-encoder archi-\ntecture and achieves state-of-the-art performance\non multiple retrieval benchmarks.\n4.4 Implementation Details\nWe adopt ChatGPT (gpt-3.5-turbo) provided by\nOpenAI through their official API5 as the LLM in\nour experiments. During inference, we use greedy\ndecoding with a temperature of 0. In the FSL set-\nting, we utilize four demonstrations (i.e., n = 4).\nWe employ Pyserini (Lin et al., 2021a) for sparse\nretrieval and Faiss (Johnson et al., 2019) for dense\nretrieval. For each user query, we retrieve 100 pas-\nsages (i.e., k = 100). We disregard test instances\nwithout valid gold passage labels. As a result, we\nhave 8209 test instances in total, with 6396, 1442,\nand 371 test instances for QuAC-Conv, NQ-Conv,\nand TREC-Conv, respectively. For more implemen-\ntation details, please refer to Appendix B.\n4We use the T5-base version https://huggingface.co/\nsentence-transformers/gtr-t5-base .\n5platform.openai.com/docs/api-reference/chat\n5 Experimental Results\n5.1 Main Results\nTable 1 presents the retrieval performance of dif-\nferent query rewriting methods on the QReCC test\nset and its subsets. Our key findings are summa-\nrized as follows. (I) All query rewriting methods\noutperform the original query, validating the im-\nportance of query rewriting. (II) Our approaches,\nED(Self) and ED(T5QR), consistently achieve the\nbest and second-best results on the overall QReCC\ntest set. Notably, they both surpass human rewrites.\nFor example, ED(Self) demonstrates a substan-\ntial absolute improvement of 9.58 in MRR scores\nfor sparse retrieval compared to human rewrites.\nRW(FSL) also performs better than human rewrites,\nwhile RW(ZSL) fails to show consistent improve-\nments over human rewrites. These results empha-\nsize the value of informative query rewriting and\nin-context demonstrations. (III) The supervised\nmodels, T5QR and ConQRR, exhibit worse per-\nformance than human rewrites, suggesting that\nrelying solely on learning from human rewrites\nleads to sub-optimal results. Although ConvGQR\nbeats human rewrites in sparse retrieval, its per-\nformance gain mainly derives from generated po-\ntential answers rather than more informative query\nrewrites. (IV) Dense retrieval improvements are\n5990\nQuery QuAC-Conv NQ-Conv TREC-Conv\nAT %OT AT %OT AT %OT\nOriginal 6.75 61.23 6.31 64.41 6.01 74.76\nT5QR 9.80 83.63 8.67 85.27 7.31 90.32\nRW(ZSL) 16.51 68.71 14.48 73.55 12.53 65.03\nRW(FSL) 16.95 76.66 15.74 81.68 15.07 74.11\nED(Self) 22.00 78.88 19.82 82.99 21.85 76.22\nED(T5QR) 17.93 85.16 15.08 89.28 15.66 87.78\nHuman 10.76 100 8.98 100 7.35 100\nTable 2: Average number of tokens (AT) and the per-\ncentage of overlapping tokens (OT) with human rewrites\nin queries produced by different rewriting methods.\nless effective than sparse retrieval. For example,\nED(Self) only outperforms human rewrites by 1.84\nMRR scores when using the dense retriever GTR.\nThis discrepancy arises due to the need for domain-\nspecific passage and query encoders in dense re-\ntrieval. In our experiments, the GTR model is kept\nfixed without fine-tuning, which limits the full po-\ntential of dense retrieval. Besides, ConvGQR also\nshows inferior dense retrieval performance, further\nindicating that a fixed general dense retriever can-\nnot fully demonstrate the superiority of informa-\ntive query rewrites. (V) Breaking down the results\nby subsets reveals that our proposed approaches\ncan consistently achieve higher performance on the\nQuAC-Conv subset. They also prevail in terms of\nMRR and MAP for sparse retrieval and achieve the\nsecond-best results for dense retrieval on the NQ-\nConv subset. However, our approaches are inferior\nto human rewrites and T5QR on the TREC-Conv\nsubset. One reason is that TREC-Conv contains\nmany obscure questions, making it challenging for\nan LLM to accurately understand true user needs.\nIt is seen that even human rewrites perform worse\non TREC-Conv than on QuAC-Conv and NQ-Conv\nin sparse retrieval. Additionally, the questions in\nTREC-Conv are more self-contained and require\nless rewriting, as evidenced by higher ROUGE-1\nscores (Lin, 2004) between human rewrites and\noriginal questions compared to QuAC-Conv and\nNQ-Conv. Specifically, the ROUGE-1 scores on\nTREC-Conv, QuAC-Conv, and NQ-Conv are 80.60,\n69.73, and 72.16, respectively. (VI) Both ED(Self)\nand ED(T5QR) outperform RW(FSL), showing the\nsignificance of prompting LLMs as rewrite editors.\nWhile ED(T5QR) performs worse than ED(Self)\non the overall QReCC test set, it excels on the NQ-\nConv and TREC-Conv subsets, benefiting from the\nfact that T5QR is trained with human rewrites.\nIn summary, this study confirms the importance\nQuery MRR MAP R@10\nSparse\n(BM25)\nHuman 39.81 38.45 62.65\nRW(ZSL) 42.63 41.31 60.46\nˆRW(ZSL) 41.52 40.20 59.60\nDense\n(GTR)\nHuman 43.15 41.27 66.12\nRW(ZSL) 40.64 38.95 62.28\nˆRW(ZSL) 39.94 38.28 61.44\nTable 3: Ablation study by removing the informative-\nness requirement from the instruction in RW(ZSL).\nof informative query rewriting and the effectiveness\nof our proposed approaches of prompting LLMs as\nquery rewriters and rewrite editors. The study also\nsuggests that it is crucial to take query characteris-\ntics into account when performing query rewriting\nwith LLMs, which we leave as our future work.\n5.2 Quantitative Analyses of Query Rewrites\nThe previous results have demonstrated the effec-\ntiveness of informative query rewrites generated by\nour proposed approaches in enhancing conversa-\ntional search. To gain more insights into the quality\nof these rewrites, we employ the average number\nof tokens per rewrite as a measurement of infor-\nmativeness and the percentage of tokens in human\nrewrites that also appear in the generated rewrites\nas a measurement of correctness. We assume that\na rewrite containing more tokens from the corre-\nsponding human rewrite is more likely to be cor-\nrect. The results are shown in Table 2. We observe\nthat our proposed approaches consistently generate\nlonger rewrites than human rewrites, with ED(Self)\nproducing the longest rewrites overall. This im-\nplies that the rewrites generated by our proposed\napproaches are more informative. We also observe\nthat T5QR generates shorter rewrites than human\nrewrites, indicating that relying solely on learning\nfrom human rewrites fails to produce informative\nrewrites. Moreover, our proposed approaches, al-\nbeit without supervised fine-tuning, achieve rela-\ntively high correctness compared to human rewrites.\nFor example, more than 76% of tokens in human\nrewrites are included in the rewrites generated by\nED(Self). ED(T5QR) even exhibits higher correct-\nness than T5QR on the QuAC-Conv and NQ-Conv\nsubsets. Finally, the longer rewrites and higher per-\ncentage of shared tokens with human rewrites (ex-\ncept RW on TREC-Conv), compared to the original\nqueries, suggest to some extent that the rewrites\ngenerated by our approaches have fair clarity.\n5991\nMRR MAP R@1020\n30\n40\n50\n60\n70\nHuman\nRW(FSL)\nED(Self)\n(a) QReCC\nMRR MAP R@1020\n30\n40\n50\n60\n70 Human\nRW(FSL)\nED(Self) (b) QuAC-Conv\nMRR MAP R@1020\n30\n40\n50\n60\nHuman\nRW(FSL)\nED(Self) (c) NQ-Conv\nMRR MAP R@1010\n20\n30\n40\n50\nHuman\nRW(FSL)\nED(Self) (d) TREC-Conv\nFigure 3: Distillation results of using BM25 as the retriever. The legends indicate the sources of fine-tuning labels.\n5.3 Ablation Study\nWe conduct an ablation study by removing the in-\nformativeness requirement from the instruction uti-\nlized by RW(ZSL) (i.e., removing the phrase “and\nbe as informative as possible”), resulting in a mod-\nified version denoted as ˆRW(ZSL). Table 3 reports\nthe results. We find that for both sparse and dense\nretrieval, ˆRW(ZSL) achieves lower performance\nacross all three evaluation metrics than RW(ZSL),\ndemonstrating that it is valuable to incorporate in-\nformativeness requirement into the instruction for\ngenerating informative query rewrites. Interest-\ningly, ˆRW(ZSL) outperforms human rewrites in\nterms of MRR and MAP for sparse retrieval, which\nagain verifies the notion that human rewrites may\nfail to yield optimal retrieval performance. See Ap-\npendix C.3 for ablation results of the other three\ndesirable properties.\n5.4 Distillation Results\nFigure 3 shows the distillation results using BM25\nas the retriever. In this study, we sample 10K train-\ning instances and employ RW(FSL) and ED(Self)\nto generate labels for fine-tuning the T5QR model.\nFor comparison, we include the results with human\nrewrites as training labels. We find that distillation\noutperforms using human rewrites as labels on the\nQReCC test set. Notably, distillation with only 10K\ntraining instances can achieve superior results than\ndirectly utilizing human rewrites as search queries\nin terms of MRR and MAP. On the QuAC-Conv\nand NQ-Conv subsets, distillation also consistently\ndemonstrates improved performance. However, for\nTREC-Conv, fine-tuning with human rewrites leads\nto better outcomes. Distillation not only improves\nretrieval performance but also reduces time over-\nhead. See Appendix C.4 for latency analyses.\n6 Related Work\nConversational search addresses users’ informa-\ntion needs through iterative interactions (Radlinski\nand Craswell, 2017; Rosset et al., 2020). It al-\nlows users to provide and seek clarifications (Xu\net al., 2019) and explore multiple aspects of a topic,\nthereby excelling at fulfilling complex informa-\ntion needs. The primary challenge in conversa-\ntional search is accurately identifying users’ search\nintents from their contextualized and potentially\nambiguous queries (Ye et al., 2022a; Keyvan and\nHuang, 2022; Ye et al., 2022b; Wang et al., 2023;\nOwoicho et al., 2023; Zhu et al., 2023).\nMost existing work (Yu et al., 2021; Lin et al.,\n2021b; Kim and Kim, 2022; Li et al., 2022a; Mao\net al., 2022) addresses this challenge by regarding\nthe concatenation of the current user query with its\nassociated conversational context as a standalone\nquery. However, using this concatenation directly\nas input to search systems could result in poor re-\ntrieval performance (Lin et al., 2021b). Moreover,\nthis approach requires training specialized retriev-\ners such as dual encoders (Karpukhin et al., 2020;\nXiong et al., 2020; Khattab and Zaharia, 2020),\nwhich can be challenging or even impractical in\nmany real-world scenarios (Wu et al., 2022).\nAnother line of research addresses this challenge\nthrough query rewriting (Elgohary et al., 2019; Wu\net al., 2022; Qian and Dou, 2022; Yuan et al., 2022;\nLi et al., 2022b; Mo et al., 2023), which converts\nthe original query into a standalone one. However,\nthese approaches mainly rely on human rewrites\nto train query rewriting models. As shown in our\nexperiments, human rewrites may lack sufficient\ninformativeness, hence leading to sub-optimal per-\nformance of these rewriting models.\nAlternatively, some studies employ query expan-\nsion to address this challenge. They select relevant\nterms from the conversational context (V oskarides\net al., 2020; Kumar and Callan, 2020) or generate\npotential answers (Mo et al., 2023) to augment the\noriginal query. The latter can be seamlessly inte-\ngrated into our approach to leverage the knowledge\nwithin LLMs. We leave this study as future work.\n5992\n7 Conclusion\nIn this work, we propose to prompt LLMs as query\nrewriters and rewrite editors for informative query\nrewrite generation. We are the first to introduce the\nconcept of informative query rewriting and iden-\ntify four properties that characterize a well-formed\nrewrite. We also propose distilling the rewriting ca-\npabilities of LLMs into smaller models to improve\nefficiency. Our experiments verify the significance\nof informativeness in query rewrites and the effec-\ntiveness of using LLMs for generating rewrites.\nDespite the superb performance achieved by our\nproposed approach, there are multiple future direc-\ntions that are worthy of exploration. For example,\nwe can train an auxiliary model to decide whether\nqueries generated by prompting LLMs should be\npreferred to those generated by models that have\nbeen finetuned on human rewrites. We can also\ncombine human rewrites and LLM rewrites as la-\nbels through a proper weighting strategy to finetune\nquery rewriting models. Furthermore, in this work,\nwe have used a fixed set of demonstrations for all\ntest queries. To achieve the best performance, it\nis essential to find appropriate demonstrations for\neach specific query. This would be an effective\nsolution to tackle obscure or complicated queries.\nAnother future direction can be parameter-efficient\nfine-tuning (e.g., LoRA (Hu et al., 2021)) of LLMs\nwith retrieval performance as feedback. In this way,\nwe will aim to optimize the helpfulness of rewritten\nqueries rather than informativeness.\nLimitations\nWe identify three limitations of our proposed ap-\nproach. Firstly, the utilization of LLMs as query\nrewriters and rewrite editors inevitably suffers from\nthe shortcomings associated with LLMs. Our ex-\nperiments indicate that LLMs do not always fol-\nlow provided instructions, resulting in generated\nrewrites that fail to possess the four desirable prop-\nerties. For example, these rewrites may contain du-\nplicate questions from the conversational context,\nthereby violating the nonredundancy requirement.\nIn Appendix C.5, we present a case study demon-\nstrating that the original user query may even be\nmisinterpreted, leading to incorrect query rewrites.\nSecondly, although our experimental results have\ndemonstrated improved retrieval performance, it\nis essential to emphasize that the effectiveness of\ninformative query rewriting is highly dependent on\nthe formatting of the passage collection. In scenar-\nios where passages are relatively short, the intro-\nduction of more information in the search query\nmay have a detrimental effect, as it becomes more\nchallenging for retrieval systems to determine the\nmost relevant passages. Conversely, informative\nquery rewriting should prove beneficial in the con-\ntext of long passages or document retrieval.\nThirdly, in this work, we have experimented with\nonly one LLM, namely ChatGPT, and therefore our\nfindings may be biased toward this specific model.\nIt is unclear if other LLMs can achieve the same\nlevel of performance. Further investigation with\nmore LLMs is worthwhile.\nEthics Statement\nQuery rewriting plays a crucial role as an intermedi-\nary process in conversational search, facilitating a\nclearer comprehension of user search intents. This\nprocess is beneficial in generating appropriate re-\nsponses to users. The effectiveness of this approach\ncan be further enhanced through informative query\nrewriting, resulting in the retrieval of more relevant\npassages. Nevertheless, it is important to acknowl-\nedge that our proposed approaches are subject to\nthe inherent limitations of LLMs, such as halluci-\nnations, biases, and toxicity. It is also important to\nfilter out passages that contain offensive text from\nthe passage collection to ensure reliable retrieval\nresults when applying our proposed approaches in\npractical scenarios.\nAcknowledgements\nThis work was funded by the EPSRC Fellowship\ntitled “Task Based Information Retrieval” (grant\nreference number EP/P024289/1) and the Alan Tur-\ning Institute.\nReferences\nRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,\nShayne Longpre, Stephen Pulman, and Srinivas\nChappidi. 2021. Open-domain question answering\ngoes conversational via question rewriting. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n520–534, Online. Association for Computational Lin-\nguistics.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\n5993\net al. 2016. Ms marco: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nZhiyu Chen, Jie Zhao, Anjie Fang, Besnik Fetahu, Oleg\nRokhlenko, and Shervin Malmasi. 2022. Reinforced\nquestion rewriting for conversational question an-\nswering. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing:\nIndustry Track, pages 357–370, Abu Dhabi, UAE.\nAssociation for Computational Linguistics.\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-\ntau Yih, Yejin Choi, Percy Liang, and Luke Zettle-\nmoyer. 2018. QuAC: Question answering in context.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2174–2184, Brussels, Belgium. Association for Com-\nputational Linguistics.\nJeffrey Dalton, Chenyan Xiong, and Jamie Callan.\n2021. Cast 2020: The conversational assistance track\noverview.\nJeffrey Dalton, Chenyan Xiong, Vaibhav Kumar, and\nJamie Callan. 2020. Cast-19: A dataset for conver-\nsational information seeking. In Proceedings of the\n43rd International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval,\npages 1985–1988.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\nAhmed Elgohary, Denis Peskov, and Jordan Boyd-\nGraber. 2019. Can you unpack that? learning to\nrewrite questions-in-context. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5918–5924, Hong Kong,\nChina. Association for Computational Linguistics.\nJianfeng Gao, Chenyan Xiong, Paul Bennett, and Nick\nCraswell. 2023. Neural Approaches to Conversa-\ntional Information Retrieval, volume 44. Springer\nNature.\nJianping Gou, Baosheng Yu, Stephen J Maybank, and\nDacheng Tao. 2021. Knowledge distillation: A\nsurvey. International Journal of Computer Vision,\n129:1789–1819.\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong\nShen, Yujiu Yang, Nan Duan, and Weizhu Chen.\n2023. Critic: Large language models can self-correct\nwith tool-interactive critiquing. arXiv preprint\narXiv:2305.11738.\nJie Hao, Yang Liu, Xing Fan, Saurabh Gupta, Saleh\nSoltan, Rakesh Chada, Pradeep Natarajan, Chenlei\nGuo, and Gokhan Tur. 2022. CGF: Constrained gen-\neration framework for query rewriting in conversa-\ntional AI. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing:\nIndustry Track, pages 475–483, Abu Dhabi, UAE.\nAssociation for Computational Linguistics.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,\nHootan Nakhost, Yasuhisa Fujii, Alexander Ratner,\nRanjay Krishna, Chen-Yu Lee, and Tomas Pfister.\n2023. Distilling step-by-step! outperforming larger\nlanguage models with less training data and smaller\nmodel sizes. arXiv preprint arXiv:2305.02301.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2021. Lora: Low-rank adaptation of large lan-\nguage models. In International Conference on Learn-\ning Representations.\nJoel Jang, Seonghyeon Ye, and Minjoon Seo. 2023. Can\nlarge language models truly understand prompts?\na case study with negated prompts. In Transfer\nLearning for Natural Language Processing Work-\nshop, pages 52–62. PMLR.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data, 7(3):535–547.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nKimiya Keyvan and Jimmy Xiangji Huang. 2022. How\nto approach ambiguous queries in conversational\nsearch: A survey of techniques, approaches, tools,\nand challenges. ACM Computing Surveys, 55(6):1–\n40.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Effi-\ncient and effective passage search via contextualized\nlate interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR conference on research\nand development in Information Retrieval, pages 39–\n48.\nSungdong Kim and Gangwoo Kim. 2022. Saving dense\nretriever from shortcut dependency in conversational\nsearch. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 10278–10287, Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics.\nVaibhav Kumar and Jamie Callan. 2020. Making in-\nformation seeking easier: An improved pipeline for\nconversational search. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3971–3980, Online. Association for Computational\nLinguistics.\n5994\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453–\n466.\nYongqi Li, Wenjie Li, and Liqiang Nie. 2022a. Dynamic\ngraph reasoning for conversational open-domain\nquestion answering. ACM Transactions on Infor-\nmation Systems (TOIS), 40(4):1–24.\nYongqi Li, Wenjie Li, and Liqiang Nie. 2022b. MM-\nCoQA: Conversational question answering over text,\ntables, and images. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 4220–\n4231, Dublin, Ireland. Association for Computational\nLinguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021a. Pyserini: A python toolkit for reproducible\ninformation retrieval research with sparse and dense\nrepresentations. In Proceedings of the 44th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pages 2356–\n2362.\nSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\n2021b. Contextualized query embeddings for conver-\nsational search. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 1004–1015, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira,\nMing-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin.\n2020. Conversational question reformulation via\nsequence-to-sequence architectures and pretrained\nlanguage models. arXiv preprint arXiv:2004.01909.\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira,\nMing-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin.\n2021c. Multi-stage conversational passage retrieval:\nAn approach to fusing term importance estimation\nand neural query rewriting. ACM Transactions on\nInformation Systems (TOIS), 39(4):1–29.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin A Raf-\nfel. 2022. Few-shot parameter-efficient fine-tuning\nis better and cheaper than in-context learning. Ad-\nvances in Neural Information Processing Systems,\n35:1950–1965.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2022.\nTeaching small language models to reason. arXiv\npreprint arXiv:2212.08410.\nKelong Mao, Zhicheng Dou, and Hongjin Qian. 2022.\nCurriculum contrastive context denoising for few-\nshot conversational dense retrieval. In Proceedings\nof the 45th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npages 176–186.\nRaja Marjieh, Ilia Sucholutsky, Pol van Rijn, Nori Ja-\ncoby, and Thomas L Griffiths. 2023. What language\nreveals about perception: Distilling psychophysi-\ncal knowledge from large language models. arXiv\npreprint arXiv:2302.01308.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022a. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2791–2809, Seattle, United States.\nAssociation for Computational Linguistics.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022b. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nFengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu,\nKaiyu Huang, and Jian-Yun Nie. 2023. Convgqr:\nGenerative query reformulation for conversational\nsearch. arXiv preprint arXiv:2305.15645.\nMarius Mosbach, Tiago Pimentel, Shauli Ravfogel, Di-\netrich Klakow, and Yanai Elazar. 2023. Few-shot\nfine-tuning vs. in-context learning: A fair comparison\nand evaluation. arXiv preprint arXiv:2305.16938.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo\nHernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,\nKeith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.\nLarge dual encoders are generalizable retrievers. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages\n9844–9855, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nPaul Owoicho, Ivan Sekuli´c, Mohammad Aliannejadi,\nJeffrey Dalton, and Fabio Crestani. 2023. Exploiting\nsimulated user feedback for conversational search:\nRanking, rewriting, and beyond. arXiv preprint\narXiv:2304.13874.\n5995\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277.\nHongjin Qian and Zhicheng Dou. 2022. Explicit query\nrewriting for conversational dense retrieval. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 4725–\n4737, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nFilip Radlinski and Nick Craswell. 2017. A theoretical\nframework for conversational search. In Proceedings\nof the 2017 conference on conference human infor-\nmation interaction and retrieval, pages 117–126.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nCorbin Rosset, Chenyan Xiong, Xia Song, Daniel Cam-\npos, Nick Craswell, Saurabh Tiwary, and Paul Ben-\nnett. 2020. Leading conversational search by sug-\ngesting useful questions. In Proceedings of the web\nconference 2020, pages 1160–1170.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya\nSachan. 2022. Distilling multi-step reasoning ca-\npabilities of large language models into smaller mod-\nels via semantic decompositions. arXiv preprint\narXiv:2212.00193.\nSimeng Sun, Yang Liu, Dan Iter, Chenguang Zhu,\nand Mohit Iyyer. 2023. How does in-context\nlearning help prompt tuning? arXiv preprint\narXiv:2302.11521.\nSvitlana Vakulenko, Shayne Longpre, Zhucheng Tu,\nand Raviteja Anantha. 2021. Question rewriting for\nconversational question answering. In Proceedings\nof the 14th ACM international conference on web\nsearch and data mining, pages 355–363.\nChristophe Van Gysel and Maarten de Rijke. 2018.\nPytrec_eval: An extremely fast python interface to\ntrec_eval. In The 41st International ACM SIGIR Con-\nference on Research & Development in Information\nRetrieval, pages 873–876.\nNikos V oskarides, Dan Li, Pengjie Ren, Evangelos\nKanoulas, and Maarten de Rijke. 2020. Query reso-\nlution for conversational search with limited supervi-\nsion. In Proceedings of the 43rd International ACM\nSIGIR conference on research and development in\nInformation Retrieval, pages 921–930.\nZhenduo Wang, Zhichao Xu, Qingyao Ai, and Vivek\nSrikumar. 2023. An in-depth investigation of user\nresponse simulation for conversational search. arXiv\npreprint arXiv:2304.07944.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert\nWebson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,\nDa Huang, Denny Zhou, et al. 2023. Larger language\nmodels do in-context learning differently. arXiv\npreprint arXiv:2303.03846.\nZeqiu Wu, Yi Luan, Hannah Rashkin, David Reit-\nter, Hannaneh Hajishirzi, Mari Ostendorf, and Gau-\nrav Singh Tomar. 2022. CONQRR: Conversational\nquery rewriting for retrieval with reinforcement learn-\ning. In Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing,\npages 10000–10014, Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020. Approximate nearest neighbor neg-\native contrastive learning for dense text retrieval.\narXiv preprint arXiv:2007.00808.\nJingjing Xu, Yuechen Wang, Duyu Tang, Nan Duan,\nPengcheng Yang, Qi Zeng, Ming Zhou, and Xu Sun.\n2019. Asking clarification questions in knowledge-\nbased question answering. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1618–1629, Hong Kong,\nChina. Association for Computational Linguistics.\nFanghua Ye, Yue Feng, and Emine Yilmaz. 2022a. AS-\nSIST: Towards label noise-robust dialogue state track-\ning. In Findings of the Association for Computa-\ntional Linguistics: ACL 2022, pages 2719–2731,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nFanghua Ye, Xi Wang, Jie Huang, Shenghui Li, Samuel\nStern, and Emine Yilmaz. 2022b. MetaASSIST:\nRobust dialogue state tracking with meta learning.\n5996\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1157–1169, Abu Dhabi, United Arab Emirates. Asso-\nciation for Computational Linguistics.\nShi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul\nBennett, Jianfeng Gao, and Zhiyuan Liu. 2020. Few-\nshot generative conversational query rewriting. In\nProceedings of the 43rd International ACM SIGIR\nconference on research and development in Informa-\ntion Retrieval, pages 1933–1936.\nShi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and\nZhiyuan Liu. 2021. Few-shot conversational dense\nretrieval. In Proceedings of the 44th International\nACM SIGIR Conference on research and development\nin information retrieval, pages 829–838.\nYifei Yuan, Chen Shi, Runze Wang, Liyi Chen, Feijun\nJiang, Yuan You, and Wai Lam. 2022. McQueen:\na benchmark for multimodal conversational query\nrewrite. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4834–4844, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nYutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan\nLiu, Wenhan Liu, Chenlong Deng, Zhicheng Dou,\nand Ji-Rong Wen. 2023. Large language models\nfor information retrieval: A survey. arXiv preprint\narXiv:2308.07107.\n5997\nA Additional Dataset Details\nThe original QReCC dataset is only divided into a\ntraining set and a test set. Following Kim and Kim\n(2022), we sample 2K conversations from the train-\ning set as the development set. The statistics are\nsummarized in Table 4. To ensure that the first user\nquestion is always self-contained and unambigu-\nous, we replace all first questions with their cor-\nresponding human rewrites. This pre-processing\naligns with previous work (Anantha et al., 2021;\nWu et al., 2022). We note that some questions in\nthe test set lack valid gold passage labels. As these\nquestions are irrelevant for retrieval evaluation, we\nremove them from the test set. Consequently, our\nfinal test set consists of 8,209 questions, with 6,396\nfor QuAC-Conv, 1,442 for NQ-Conv, and 371 for\nTREC-Conv. This filtering also helps reduce costs\nassociated with using OpenAI APIs.\nTrain Dev Test\nQReCC #C 8,823 2,000 2,775\n#Q 51,928 11,573 16,451\nQuAC-Conv #C 6,008 1,300 1,816\n#Q 41,395 8,965 12,389\nNQ-Conv #C 2,815 700 879\n#Q 10,533 2,608 3,314\nTREC-Conv #C 0 0 80\n#Q 0 0 748\nTable 4: Statistics of the QReCC dataset and its three\nsubsets. #C represents the number of conversations and\n#Q denotes the number of questions.\nB Additional Implementation Details\nThroughout our experiments, we leverage ChatGPT\nas the LLM and set the maximum number of gen-\neration tokens to 2,560 for all four variants of our\nproposed approach, namely RW(ZSL), RW(FSL),\nED(Self), and ED(T5QR). ED(Self) utilizes the\nrewrites generated by RW(FSL) as initial rewrites,\nwhile ED(T5QR) takes the rewrites produced by\nT5QR as initial results. For training, we initialize\nT5QR with the t5-base checkpoint from Hugging-\nFace6 and select the best model based on the high-\nest BLEU score (Papineni et al., 2002) with human\nrewrites on the development set. The training pro-\ncess involves 10 epochs with a batch size of 16 and\n6huggingface.co/docs/transformers/model_doc/t5\ngradient accumulation steps of 2 (i.e., an overall\nbatch size of 32). We employ AdamW (Loshchilov\nand Hutter, 2017) as the optimizer and creat a linear\nschedule with warmup to adjust the learning rate\ndynamically. The peak learning rate is set to 1e-5,\nand the warmup ratio is 0.1. The maximum conver-\nsational context length is restricted to 384, and the\nmaximum output length is set to 64. We use a fixed\nrandom seed of 42 and conduct experiments on a\nsingle TITAN RTX GPU card with 24GB memory.\nGreedy decoding is used for inference.\nTo conduct distillation, we first sample 10K ques-\ntions from the training set and 2K questions from\nthe development set. We then apply RW(FSL) and\nED(Self) to generate rewrites as pseudo labels for\ntraining the T5QR model. In this experiment, the\ntraining settings remain the same as the previous\none, except that the number of gradient accumula-\ntion steps is set to 1 (i.e., an overall batch size of\n16) and the BLEU score is calculated based on the\ngenerated pseudo labels rather than human rewrites.\nWe perform testing on the full test set.\nWe use Pyserini (Lin et al., 2021a) to construct\nthe sparse index for the BM25 retrieval model, with\ndefault hyparameters of k1 = 0.82 and b = 0.68.\nThese values were chosen according to the retrieval\nperformance on MS MARCO (Bajaj et al., 2016),\na non-conversational retrieval dataset. We adopt\nFaiss (Johnson et al., 2019) to build the dense in-\ndex for the GTR retrieval model. When encoding\nqueries and passages, the maximum length is set\nto 384 and the dimension of embedding vectors is\n768. We utilize cosine similarity between a query\nvector and a passage vector to estimate their rele-\nvance. Building the dense index for 54M passages\nrequires around 320GB of RAM. To efficiently han-\ndle this, we split the passage collection into 8 parts,\nconduct retrieval on each part, and subsequently\nmerge the results.\nC Additional Experimental Results\nC.1 Pairwise Comparison of Query Rewriting\nMethods\nWe provide a more in-depth analysis of different\nquery rewriting methods by comparing the per-\nformance of every two methods on each instance\nwithin the QReCC test set. We leverage recipro-\ncal rank (RR) as the evaluation metric and mea-\nsure the ratio of instances where the first method\noutperforms (win) or achieves equal performance\n(tie) to the second method. The results are illus-\n5998\nHuman T5QR\nRW(ZSL)RW(FSL)ED(Self)ED(T5QR)\nHuman\nT5QR\nRW(ZSL)\nRW(FSL)\nED(Self)\nED(T5QR)\n0.0 0.299 0.333 0.276 0.266 0.258\n0.126 0.0 0.261 0.212 0.21 0.163\n0.298 0.363 0.0 0.178 0.177 0.191\n0.328 0.396 0.26 0.0 0.114 0.196\n0.36 0.424 0.292 0.154 0.0 0.213\n0.334 0.384 0.299 0.229 0.204 0.0\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n(a) Win\nHuman T5QR\nRW(ZSL)RW(FSL)ED(Self)ED(T5QR)\nHuman\nT5QR\nRW(ZSL)\nRW(FSL)\nED(Self)\nED(T5QR)\n1.0 0.576 0.37 0.395 0.374 0.409\n0.576 1.0 0.376 0.391 0.367 0.452\n0.37 0.376 1.0 0.562 0.53 0.51\n0.395 0.391 0.562 1.0 0.732 0.575\n0.374 0.367 0.53 0.732 1.0 0.583\n0.409 0.452 0.51 0.575 0.583 1.0\n 0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0 (b) Tie\nFigure 4: The ratio of cases where query rewriting methods shown in the vertical axis achieve better performance\n(win) than or equal performance (tie) to methods shown in the horizontal axis in terms of the reciprocal rank (RR)\nmetric on the overall QReCC test set. We adopt BM25 as the retriever in this study.\nQuery QReCC QuAC-Conv NQ-Conv TREC-Conv\nNDCG@3 R@5 R@100 NDCG@3 R@5 R@100 NDCG@3 R@5 R@100 NDCG@3 R@5 R@100\nSparse (BM25)\nOriginal 7.98 12.06 28.63 8.02 12.10 27.70 7.80 11.14 29.06 7.96 14.82 42.99\nHuman 35.97 50.71 98.48 36.57 51.21 98.35 36.58 51.65 98.96 23.15 38.54 98.92\nT5QR 30.13 43.09 85.66 30.55 43.57 85.86 30.47 42.86 84.42 21.39 35.58 87.06\nConQRR - - 88.90 - - 90.20 - - 86.70 - - 75.90\nConvGQR 41.00 - 88.00 - - - - - - - - -\nRW(ZSL) 39.67 52.04 84.31 42.54 55.02 85.55 33.05 45.30 81.92 15.89 26.82 72.15\nRW(FSL) 43.82 56.60 88.34 46.83 59.59 89.86 37.61 50.85 86.56 16.00 27.36 69.05\nED(Self) 46.43 58.86 88.15 50.20 62.55 89.95 37.94 50.89 85.72 14.32 26.28 66.49\nED(T5QR) 44.85 57.69 89.91 47.80 60.49 91.09 38.77 51.88 87.97 17.71 32.08 77.18\nDense (GTR)\nOriginal 10.90 15.23 27.22 10.09 14.38 25.64 12.00 15.97 29.26 20.51 27.09 46.50\nHuman 39.42 54.85 86.86 36.68 52.44 86.46 50.89 65.24 88.54 42.01 56.06 87.24\nT5QR 34.37 48.19 79.85 32.06 46.09 79.65 43.98 56.94 79.92 36.79 50.40 83.20\nConQRR - - 84.70 - - 85.80 - - 80.90 - - 79.60\nConvGQR 39.10 - 81.80 - - - - - - - - -\nRW(ZSL) 37.41 52.23 81.03 36.83 51.79 81.53 41.59 55.96 79.89 31.21 45.28 76.95\nRW(FSL) 40.62 56.12 84.76 40.17 55.85 85.33 45.41 60.19 84.67 29.87 44.83 75.34\nED(Self) 41.80 57.11 85.61 42.06 57.50 86.42 44.08 59.27 84.42 28.44 41.87 76.15\nED(T5QR) 41.49 56.46 85.91 41.02 56.11 86.25 46.39 60.27 85.35 30.56 47.71 82.12\nTable 5: Passage retrieval performance of sparse and dense retrievers with various query rewriting methods on the\nQReCC test set and its three subsets. The best results are shown in bold and the second-best results are underlined.\ntrated in Figure 4. It can be seen that our proposed\napproaches, RW(FSL), ED(Self), and ED(T5QR),\nwin more instances than human rewrites. For ex-\nample, ED(Self) achieves higher performance in\napproximately 36% of instances, whereas human\nrewrites are better in only about26.6% of instances.\nIt can also be seen that T5QR outperforms human\nrewrites in only 12.6% of instances, again confirm-\ning that learning solely from human rewrites is not\nideal. Moreover, we observe that ED(T5QR) out-\nperforms T5QR in38.4% of instances, while T5QR\nonly wins in16.3% of instances. This demonstrates\nthat prompting LLMs as rewrite editors is effec-\ntive in generating higher-quality query rewrites.\nAmong the four variants of our proposed approach,\nwe find that they share a relatively high tie ratio.\nFor example, ED(Self) and RW(FSL) achieve equal\nperformance in 73.2% of instances. This is reason-\nable since ED(Self) takes the rewrites produced by\nRW(FSL) as initial rewrites.\nThis analysis reveals that no single method pos-\nsesses superior efficacy over all other methods,\nthereby indicating considerable potential for fu-\nture research. It is crucial to emphasize that while\nwe assert the limited informativeness of human\nrewrites in a general sense, it is possible that some\n5999\nR@5 R@10 R@20 R@30 R@10040\n50\n60\n70\n80\n90\n100\nHuman\nT5QR\nRW(ZSL)\nRW(FSL)\nED(Self)\nED(T5QR)\n(a) QReCC\nR@5 R@10 R@20 R@30 R@10040\n50\n60\n70\n80\n90\n100\nHuman\nT5QR\nRW(ZSL)\nRW(FSL)\nED(Self)\nED(T5QR) (b) QuAC-Conv\nR@5 R@10 R@20 R@30 R@10040\n50\n60\n70\n80\n90\n100\nHuman\nT5QR\nRW(ZSL)\nRW(FSL)\nED(Self)\nED(T5QR)\n(c) NQ-Conv\nR@5 R@10 R@20 R@30 R@10020\n40\n60\n80\n100\nHuman\nT5QR\nRW(ZSL)\nRW(FSL)\nED(Self)\nED(T5QR) (d) TREC-Conv\nFigure 5: Recall value versus the cutoff rank k ∈{5, 10, 20, 30, 100}, with BM25 as the retriever.\nhuman rewrites are already informative enough.\nC.2 Main Results with Different Evaluation\nMetrics\nHere, we expand the range of evaluation metrics\nto assess the retrieval performance more compre-\nhensively. Specifically, we utilize normalized dis-\ncounted cumulative gain with a cutoff rank of 3\n(NDCG@3) and Recall@5 (R@5) to evaluate the\nrelevance of top-ranked passages. Additionally, we\nemploy Recall@100 (R@100) to consider the rel-\nevance of lower-ranked passages. The results are\npresented in Table 5. We find that our proposed\napproaches can substantially outperform human\nrewrites and all supervised baselines in terms of\nNDCG@3 and R@5 on the QReCC test set. For\nexample, ED(Self) shows an absolute improvement\nof 10.46 in NDCG@3 compared to human rewrites.\nThis finding indicates that our approaches can effec-\ntively rank relevant passages higher, which is partic-\nularly valuable for downstream tasks such as ques-\ntion answering, where only the top-ranked passages\nare typically considered. Regarding R@100, we\nfind that human rewrites consistently demonstrate\nthe best performance. This is rational since human\nrewrites possess a higher guarantee of correctness.\nAlthough our proposed approaches are able to gen-\nerate more informative query rewrites, they suffer\nfrom a lower guarantee of correctness as they are\nbuilt upon LLMs. However, it is worth noting that\nour approach ED(T5QR) achieves the second-best\nresults for both sparse and dense retrieval. Concern-\ning the performance on the three subsets, our find-\nings are similar to those presented in Section 5.1.\nOur approaches consistently achieve the best or\nsecond-best results in terms of NDCG@3 and R@5\non the QuAC-Conv and NQ-Conv subsets. How-\never, they fall short of human rewrites and T5QR\non the TREC-Conv subset. Nevertheless, our ap-\nproach ED(T5QR) achieves higher performance\nthan ConQRR, even though it employs reinforce-\nment learning to optimize the model with retrieval\nperformance as the reward.\nWe conduct further analysis on the trend of re-\n6000\nR@5 R@10 R@20 R@30 R@100\n50\n60\n70\n80\n90\nHuman\nT5QR\nRW(ZSL)\nRW(FSL)\nED(Self)\nED(T5QR)\n(a) QReCC\nR@5 R@10 R@20 R@30 R@100\n50\n60\n70\n80\n90\nHuman\nT5QR\nRW(ZSL)\nRW(FSL)\nED(Self)\nED(T5QR) (b) QuAC-Conv\nR@5 R@10 R@20 R@30 R@10050\n60\n70\n80\n90\nHuman\nT5QR\nRW(ZSL)\nRW(FSL)\nED(Self)\nED(T5QR)\n(c) NQ-Conv\nR@5 R@10 R@20 R@30 R@10040\n50\n60\n70\n80\n90\nHuman\nT5QR\nRW(ZSL)\nRW(FSL)\nED(Self)\nED(T5QR) (d) TREC-Conv\nFigure 6: Recall value versus the cutoff rank k ∈{5, 10, 20, 30, 100}, with GTR as the retriever.\ncall values by varying the cutoff rank k within the\nrange of {5, 10, 20, 30, 100}. Recall, which repre-\nsents the fraction of relevant passages retrieved, is\nan important metric for evaluating the performance\nof retrieval results, particularly from a holistic point\nof view. The results of sparse retrieval and dense\nretrieval are illustrated in Figure 5 and Figure 6,\nrespectively. As anticipated, increasing the value of\nk leads to higher recall values across all methods,\nas a larger number of relevant passages are likely\nto be included in the results. Notably, our proposed\napproaches RW(FSL), ED(Self), and ED(T5QR)\ndemonstrate commendable performance at differ-\nent cutoff ranks (i.e., different values of k). They\ncan surpass human rewrites when considering small\nvalues of k. However, they are inferior to human\nrewrites when k becomes large. This observation\nsuggests that the quality of rewrites generated by\nour approaches exhibits a higher variability. Even\nthough many rewrites outperform human rewrites,\nthere are also cases where the generated rewrites\nare significantly inferior. It is worthwhile to con-\nQuery MRR MAP R@10\nSparse\n(BM25)\nRW(ZSL) 42.63 41.31 60.46\n-Informativeness 41.52 40.20 59.60\n-Correctness 35.95 34.61 53.59\n-Clarity 33.43 32.16 50.87\n-Nonredundancy 36.58 35.23 55.02\nDense\n(GTR)\nRW(ZSL) 40.64 38.95 62.28\n-Informativeness 39.94 38.28 61.44\n-Correctness 38.03 36.28 57.88\n-Clarity 35.78 34.11 54.82\n-Nonredundancy 38.53 36.82 59.27\nTable 6: Ablation study by removing each of the four\ndesirable properties from the instruction in RW(ZSL).\nduct further research to optimize the worst-case\nperformance of our proposed approach.\nC.3 Additional Ablation Study\nIn Table 6, we report the ablation results by remov-\ning each of the four desirable properties from the\ninstruction in RW(ZSL). From the table, we can see\n6001\nMRR MAP R@1030\n40\n50\n60\n70\nHuman\nRW(FSL)\nED(Self)\n(a) QReCC\nMRR MAP R@1030\n40\n50\n60\n70\nHuman\nRW(FSL)\nED(Self) (b) QuAC-Conv\nMRR MAP R@1040\n45\n50\n55\n60\n65\n70\nHuman\nRW(FSL)\nED(Self) (c) NQ-Conv\nMRR MAP R@1020\n30\n40\n50\n60\nHuman\nRW(FSL)\nED(Self) (d) TREC-Conv\nFigure 7: Distillation results of using GTR as the retriever. The legends indicate the sources of fine-tuning labels.\nthat all the four properties contribute to the perfor-\nmance improvement. Removing any one of them\nwill lead to performance degradation. In particular,\nwe can observe that removing either the correctness\nor clarity property leads to the most performance\ndrop. This is because these two properties are cru-\ncial to ensure that the rewrites preserve the mean-\ning of the original queries and are self-contained.\nSince the clarity requirement also contributes to\nthe informativeness of rewritten queries, remov-\ning the informativeness requirement only seems to\ndecrease the performance not that much.\nIn summary, this ablation study verifies that our\nproposed four properties in the instructions are es-\nsential to the success of prompting LLMs as query\nrewriters and informative query rewriting is critical\nfor achieving better retrieval performance.\nC.4 Additional Distillation Results\nThe distillation results using GTR as the retrieval\nmodel are depicted in Figure 7. It can be observed\nthat distillation outperforms using human rewrites\nas labels by a large margin on the QReCC test set.\nOn the QuAC-Conv subset, distillation also consis-\ntently demonstrates superior performance. On the\nNQ-Conv subset, distillation surpasses the utiliza-\ntion of human rewrites as labels when RW(FSL) is\nemployed as the teacher model.\nDistillation offers benefits not only in enhanc-\ning retrieval performance but also in reducing time\noverhead. The rewriting latency comparison be-\ntween T5QR and RW(FSL), with RW(FSL) serv-\ning as the teacher model, is presented in Table 7.\nThe results indicate that the student model T5QR\nexhibits a significantly higher rewriting speed, ap-\nproximately six times faster than RW(FSL).\nC.5 Case Study\nWe perform a case study to help understand the im-\npact of informative query rewrites on retrieval per-\nformance more intuitively. We employ RW(FSL)\nT5QR RW(FSL)\nRewriting Latency 312 (ms/q) 1867 (ms/q)\nTable 7: Comparison of rewriting latency in terms of\nmilliseconds per query (ms/q). RW(FSL) is the teacher\nmodel, while T5QR is the student model.\nas the query rewriter and BM25 as the retriever in\nthis study. Table 8 showcases two successful exam-\nples. In the first example, adding the information\n“during his time as Chancellor of the University of\nChicago” is crucial to understanding the original\nquery comprehensively, thereby significantly im-\nproving the ranking of the relevant passage. In the\nsecond example, the added information exhibits a\nhigh degree of overlap with the gold passage, which\nalso leads to better retrieval performance. Table 9\nshowcases one unsuccessful example. In this exam-\nple, our approach RW(FSL) misinterprets the user\nquery and expands it using wrong contextual infor-\nmation, resulting in worse retrieval performance.\nD Prompts\nThe prompts for utilizing LLMs as zero-shot and\nfew-shot query rewriters and few-shot rewrite ed-\nitors are presented in Table 10, Table 11, and Ta-\nble 12, respectively.\n6002\nConversational Context:(id=511_4)\nQ1: What year was Robert Maynard Hutchins Chancellor of the University of Chicago?\nA1: Robert Maynard Hutchins served as University of Chicago’s Chancellor from 1945 until 1951.\nQ2: Did Robert pull any sports out of the schools agenda?\nA2: Robert Maynard Hutchins eliminated the University of Chicago’s football program, which he\nsaw as a campus distraction.\nQ3: What collegiate conference of sports did he pull the university out of?\nA3: Robert Maynard Hutchins pulled the University of Chicago out of the Big Ten Conference.\nCurrent Question:\nQ4: What degree did he make known for two year studies?\nHuman Rewrite:\nQ∗\n4: What degree did Robert Maynard Hutchins make known for two year studies? (Rank=56)\nRewrite by RW(FSL):\nQ′\n4: What degree program did Robert Maynard Hutchins make known for two year studies during his\ntime as Chancellor of the University of Chicago? (Rank=2)\nGold passage:\n. . .Hutchins was able to implement his ideas regarding a two-year, generalist bachelors during his\ntenure at Chicago, and subsequently had designated those studying in depth in a field as masters\nstudents . . .\nConversational Context:(id=1875_3)\nQ1: Where did Wu-Tang Clan’s name come from?\nA1: Shaolin and Wu Tang is a film that inspired the name of the hip-hop group Wu-Tang Clan.\nQ2: When did the group form?\nA2: Wu-Tang Clan is an American hip hop group formed in the New York City borough of Staten\nIsland in 1992.\nCurrent Question:\nQ3: Who were the founding members?\nHuman Rewrite:\nQ∗\n3: Who were the founding members of Wu-Tang Clan? (Rank=14)\nRewrite by RW(FSL):\nQ′\n3: Who were the founding members of the American hip hop group Wu-Tang Clan, which was\nformed in the New York City borough of Staten Island in 1992 and named after the film Shaolin and\nWu Tang? (Rank=1)\nGold passage:\n. . .Wu-Tang Clan is an American hip hop group formed in the New York City borough of Staten\nIsland in 1992, originally composed of rapper-producer RZA and rappers GZA, Ol ´Dirty Bastard,\nMethod Man, Raekwon, Ghostface Killah, Inspectah Deck, U-God and Masta Killa . . .\nTable 8: Two successful examples showing that our proposed approach RW(FSL) can extract valuable contextual\ninformation to improve the informativeness of generated query rewrites. We highlight the additional information in\ngenerated rewrites compared to human rewrites in blue and the relevant parts in gold passages regarding the added\ninformation in red.\n6003\nConversational Context:(id=617_5)\nQ1: What did the singer, Cheryl do for the greater good?\nA1: In 2004, Cheryl’s band, Girls Aloud released a cover of The Pretenders’ I’ll Stand by You as the\nofficial single for the BBC’s charity telethon Children in Need.\nQ2: Did she do any other activities for charities?\nA2: In March 2009, Cheryl climbed Mount Kilimanjaro in aid of Comic Relief.\nQ3: Does she have any charities?\nA3: In February 2011, Cheryl launched her own charitable foundation with The Prince’s Trust\nfollowing a meeting with The Trust’s President, HRH Charles, Prince of Wales.\nQ4: Are there any other interesting aspects about this article?\nA4: On 23 January 2015, Cheryl announced the launch of a second charity, once again alongside The\nPrince’s Trust.\nCurrent Question:\nQ5: For what charity?\nHuman Rewrite:\nQ∗\n5: What second charity did the singer, Cheryl launch in 2015? (Rank=1)\nRewrite by RW(FSL):\nQ′\n5: For which charity did Cheryl launch her own charitable foundation in February 2011, following a\nmeeting with The Trust’s President, HRH Charles, Prince of Wales? (Rank=25)\nGold passage:\n. . .The charity was named Cheryl´s Trust, and was set up with the aim of raising £2 million to build a\ncentre, which will support up to 4000 disadvantaged young people in her native city of Newcastle. To\nraise these funds, Cheryl has thus far teamed up with Prizeo in March 2015 . . .\nTable 9: An unsuccessful example showing that our proposed approach RW(FSL) may misinterpret the original\nquestion and thus fail to identify truly relevant contextual information to augment the original query.\nGiven a question and its context, decontextualize the question by addressing\ncoreference and omission issues. The resulting question should retain its\noriginal meaning and be as informative as possible, and should not duplicate any\npreviously asked questions in the context.\nContext: {Conversational context}\nQuestion: {Question}\nRewrite:\nTable 10: Prompt for utilizing LLMs as zero-shot query rewriters.\n6004\nGiven a question and its context, decontextualize the question by addressing\ncoreference and omission issues. The resulting question should retain its\noriginal meaning and be as informative as possible, and should not duplicate any\npreviously asked questions in the context.\nContext: [Q: When was Born to Fly released?\nA: Sara Evans’s third studio album, Born to Fly, was released on October 10, 2000.\n]\nQuestion: Was Born to Fly well received by critics?\nRewrite: Was Born to Fly well received by critics?\nContext: [Q: When was Keith Carradine born?\nA: Keith Ian Carradine was born August 8, 1949.\nQ: Is he married?\nA: Keith Carradine married Sandra Will on February 6, 1982. ]\nQuestion: Do they have any children?\nRewrite: Do Keith Carradine and Sandra Will have any children?\nContext: [Q: Who proposed that atoms are the basic units of matter?\nA: John Dalton proposed that each chemical element is composed of atoms of a\nsingle, unique type, and they can combine to form more complex structures called\nchemical compounds. ]\nQuestion: How did the proposal come about?\nRewrite: How did John Dalton’s proposal that each chemical element is composed\nof atoms of a single unique type, and they can combine to form more complex\nstructures called chemical compounds come about?\nContext: [Q: What is it called when two liquids separate?\nA: Decantation is a process for the separation of mixtures of immiscible liquids\nor of a liquid and a solid mixture such as a suspension.\nQ: How does the separation occur?\nA: The layer closer to the top of the container-the less dense of the two liquids,\nor the liquid from which the precipitate or sediment has settled out-is poured\noff. ]\nQuestion: Then what happens?\nRewrite: Then what happens after the layer closer to the top of the container is\npoured off with decantation?\nContext: {Conversational context}\nQuestion: {Question}\nRewrite:\nTable 11: Prompt for utilizing LLMs as few-shot query rewriters.\n6005\nGiven a question and its context and a rewrite that decontextualizes the question,\nedit the rewrite to create a revised version that fully addresses coreferences\nand omissions in the question without changing the original meaning of the\nquestion but providing more information. The new rewrite should not duplicate\nany previously asked questions in the context. If there is no need to edit the\nrewrite, return the rewrite as-is.\nContext: [Q: When was Born to Fly released?\nA: Sara Evans’s third studio album, Born to Fly, was released on October 10, 2000.\n]\nQuestion: Was Born to Fly well received by critics?\nRewrite: Was Born to Fly well received by critics?\nEdit: Was Born to Fly well received by critics?\nContext: [Q: When was Keith Carradine born?\nA: Keith Ian Carradine was born August 8, 1949.\nQ: Is he married?\nA: Keith Carradine married Sandra Will on February 6, 1982. ]\nQuestion: Do they have any children?\nRewrite: Does Keith Carradine have any children?\nEdit: Do Keith Carradine and Sandra Will have any children?\nContext: [Q: Who proposed that atoms are the basic units of matter?\nA: John Dalton proposed that each chemical element is composed of atoms of a\nsingle, unique type, and they can combine to form more complex structures called\nchemical compounds. ]\nQuestion: How did the proposal come about?\nRewrite: How did John Dalton’s proposal come about?\nEdit: How did John Dalton’s proposal that each chemical element is composed\nof atoms of a single unique type, and they can combine to form more complex\nstructures called chemical compounds come about?\nContext: [Q: What is it called when two liquids separate?\nA: Decantation is a process for the separation of mixtures of immiscible liquids\nor of a liquid and a solid mixture such as a suspension.\nQ: How does the separation occur?\nA: The layer closer to the top of the container-the less dense of the two liquids,\nor the liquid from which the precipitate or sediment has settled out-is poured\noff. ]\nQuestion: Then what happens?\nRewrite: Then what happens after the layer closer to the top of the container is\npoured off?\nEdit: Then what happens after the layer closer to the top of the container is\npoured off with decantation?\nContext: {Conversational context}\nQuestion: {Question}\nRewrite: {Initial rewrite}\nEdit:\nTable 12: Prompt for utilizing LLMs as few-shot editors.\n6006"
}