{
  "title": "Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework",
  "url": "https://openalex.org/W877909479",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A2066410461",
      "name": "Ran Xu",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    },
    {
      "id": "https://openalex.org/A2095665791",
      "name": "Caiming Xiong",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A1964725655",
      "name": "Wei Chen",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    },
    {
      "id": "https://openalex.org/A2189432432",
      "name": "Jason Corso",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2066410461",
      "name": "Ran Xu",
      "affiliations": [
        "University at Buffalo, State University of New York",
        "Buffalo State University"
      ]
    },
    {
      "id": "https://openalex.org/A2095665791",
      "name": "Caiming Xiong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1964725655",
      "name": "Wei Chen",
      "affiliations": [
        "Buffalo State University",
        "University at Buffalo, State University of New York"
      ]
    },
    {
      "id": "https://openalex.org/A2189432432",
      "name": "Jason Corso",
      "affiliations": [
        "Universidad de Málaga"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6683167905",
    "https://openalex.org/W2164290393",
    "https://openalex.org/W1995820507",
    "https://openalex.org/W2097903333",
    "https://openalex.org/W2155541015",
    "https://openalex.org/W2168356304",
    "https://openalex.org/W2104518905",
    "https://openalex.org/W2142900973",
    "https://openalex.org/W2139117248",
    "https://openalex.org/W2016053056",
    "https://openalex.org/W6674783156",
    "https://openalex.org/W2152984213",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W1969616664",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2171858339",
    "https://openalex.org/W2137123920",
    "https://openalex.org/W2613678836",
    "https://openalex.org/W6679063059",
    "https://openalex.org/W2110933980",
    "https://openalex.org/W2063153269",
    "https://openalex.org/W2062955551",
    "https://openalex.org/W2149557440",
    "https://openalex.org/W2067766814",
    "https://openalex.org/W2251353663",
    "https://openalex.org/W2095661305",
    "https://openalex.org/W6678854542",
    "https://openalex.org/W2136985729",
    "https://openalex.org/W2137591992",
    "https://openalex.org/W2123024445",
    "https://openalex.org/W2605170521",
    "https://openalex.org/W2167905777",
    "https://openalex.org/W2126574503",
    "https://openalex.org/W2953360861",
    "https://openalex.org/W4294375521",
    "https://openalex.org/W4241562394",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2963811219",
    "https://openalex.org/W2308045930",
    "https://openalex.org/W2149961899",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2169177311",
    "https://openalex.org/W2101039060",
    "https://openalex.org/W2142120379",
    "https://openalex.org/W2097606805",
    "https://openalex.org/W2127314673",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2066134726"
  ],
  "abstract": "Recently, joint video-language modeling has been attracting more and more attention. However, most existing approaches focus on exploring the language model upon on a fixed visual model. In this paper, we propose a unified framework that jointly models video and the corresponding text sentences. The framework consists of three parts: a compositional semantics language model, a deep video model and a joint embedding model. In our language model, we propose a dependency-tree structure model that embeds sentence into a continuous vector space, which preserves visually grounded meanings and word order. In the visual model, we leverage deep neural networks to capture essential semantic information from videos. In the joint embedding model, we minimize the distance of the outputs of the deep video model and compositional language model in the joint space, and update these two models jointly. Based on these three parts, our system is able to accomplish three tasks: 1) natural language generation, and 2) video retrieval and 3) language retrieval. In the experiments, the results show our approach outperforms SVM, CRF and CCA baselines in predicting Subject-Verb-Object triplet and natural sentence generation, and is better than CCA in video retrieval and language retrieval tasks.",
  "full_text": "Jointly Modeling Deep Video and Compositional Text to Bridge Vision and\nLanguage in a Uniﬁed Framework\nRan Xu\nDept. of Computer Science\nSUNY at Buffalo\nrxu2@buffalo.edu\nCaiming Xiong\nDepartment of Statistics\nUCLA\ncaimingxiong@ucla.edu\nWei Chen\nDepartment of Computer Science\nSUNY at Buffalo\nwchen23@buffalo.edu\nJason J. Corso\nDepartment of EECS\nUniversity of Michagan\njjcorso@eecs.umich.edu\nAbstract\nRecently, joint video-language modeling has been attracting\nmore and more attention. However, most existing approaches\nfocus on exploring the language model upon on a ﬁxed vi-\nsual model. In this paper, we propose a uniﬁed framework\nthat jointly models video and the corresponding text sen-\ntences. The framework consists of three parts: a composi-\ntional semantics language model, a deep video model and a\njoint embedding model. In our language model, we propose a\ndependency-tree structure model that embeds sentence into a\ncontinuous vector space, which preserves visually grounded\nmeanings and word order. In the visual model, we leverage\ndeep neural networks to capture essential semantic informa-\ntion from videos. In the joint embedding model, we minimize\nthe distance of the outputs of the deep video model and com-\npositional language model in the joint space, and update these\ntwo models jointly. Based on these three parts, our system\nis able to accomplish three tasks: 1) natural language gen-\neration, and 2) video retrieval and 3) language retrieval. In\nthe experiments, the results show our approach outperforms\nSVM, CRF and CCA baselines in predicting Subject-V erb-\nObject triplet and natural sentence generation, and is better\nthan CCA in video retrieval and language retrieval tasks.\nIntroduction\nMore than 100,000 hours of videos are uploaded to Y ouTube\neveryday, and more than 100,000 new videos are added\nand shared in Facebook everyday. Most of those videos are\npaired with natural language descriptions, some of which are\nas simple as tags or as detailed as paragraphs. Those descrip-\ntions provide us with the possibility of joint video and hu-\nman language understanding and thus support many promis-\ning applications, e.g. turning the surveillance video from last\nnight into a page of incident report, teaching robots to rec-\nognize certain objects with human language, or recommend-\ning Y ouTube viewers more interesting video clips leveraging\nboth text and video content analysis.\nIn such applications, there are three essential tasks, i.e.\nvideo retrieval (Song, Y ang, and Huang 2011), language re-\ntrieval (Das, Srihari, and Corso 2013) and ultimately, natural\nlanguage generation (NLG) from novel videos (Barbu et al.\n2012; Das et al. 2013; Rohrbach et al. 2013; Gupta et al.\nCopyright c⃝ 2015, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1:Illustration of three important tasks, the upper box shows\nnatural language generation, middle box shows video retrieval and\nbottom box shows language retrieval. The red box indicates the\nground truth result.\n2009; Krishnamoorthy et al. 2013; Guadarrama et al. 2013;\nThomason et al. 2014), as depicted in Fig. 1. All these prob-\nlems have been pushed forward in the artiﬁcial intelligence,\nmultimedia, computer vision, natural language processing\nand machine learning communities.\nAlong the way towards these tasks, researchers have spent\ndecades of efforts in video content understanding, includ-\ning action classiﬁcation (Wang et al. 2011; Sadanand and\nCorso 2012; Karpathy et al. 2014), detection (Tian, Suk-\nthankar, and Shah 2013; Zhang, Zhu, and Derpanis 2013)\nand tagging (Y ao et al. 2013; Moxley, Mei, and Manjunath\n2010). We believe it is meaningful to push one step further\nand generate sentences for video because it is more natural\nto human perception and encodes spatio-temporal relation-\nships and richer details from videos.\nIn transducing videos into sentences, a line of work has\ninvestigated marrying state-of-the-art computer vision (Li et\nProceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence\n2346\nFigure 2: Overview of our uniﬁed framework with a joint Deep\nVideo Model (Left) and a Compositional Language Model (Right).\nal. 2011; Wang et al. 2011; Felzenszwalb et al. 2010) and\nnatural language processing (Miller 1995) techniques. For\nexample, (Guadarrama et al. 2013) use semantic hierarchies\nto trade-off the speciﬁcity and accuracy of subject, verb and\nobject in Y ouTube videos “in-the-wild.” (Rohrbach et al.\n2013) propose a CRF to model relationships between differ-\nent visual components and translate video to text in cooking\nvideos. (Kulkarni et al. 2011) proposes a CRF model to in-\ncorporate object, attribute and preposition, where pair-wise\nrelationships of object-attribute, object-preposition, etc. are\nmodeled with a large text corpus. In the Experimental sec-\ntion, our baseline methods that use SVM and CRF follow\nthis paradigm. A common methodology in the above models\nis to build the language model directly upon the output of the\nvisual model without feedback to the visual model. Alterna-\ntively, another line of promising work builds a joint space,\ne.g., (Nakayama, Harada, and Kuniyoshi 2010) propose\nCanonical Contextual Distance (CCD) to construct image-\ntext space and use KNN for image annotation, (Socher et al.\n2013) use Recursive Neural Network to model a sentence\nand build an image-text space to do image and sentence re-\ntrieval. As (Das et al. 2013) discussed, the scalability of non-\nparametric methods with increasing of semantic space is un-\nclear, and the generated text often lacks “semantic veriﬁca-\ntion.” In our efforts towards joint video-language modeling,\nwe make two observations:\n• Commonly used word similarity captures more syntac-\ntic expression than visually grounded semantics, e.g., in\nWordNet, the Lesk similarity between cat and kitten is\n0.4 while the similarity between cat and dog is 1.04.\n• Visually grounded semantics is highly compositional,\nsuch as “player rides bicycle” and “cook bakes bread”,\nand it is meaningful to jointly learn such compositionality\nand video representation.\nInspired by such observations, we propose a uniﬁed\nframework with joint deep video and compositional lan-\nguage models to address the above points. Our framework\nconsists of three parts: a compositional semantics language\nmodel, a deep video model and a joint embedding model.\nFirstly, we propose a compositional semantics language\nmodel that enforces semantic compatibility between es-\nsential concepts, especially visually meaningful concepts\nin videos. We assume that the essential semantic meaning\nof a video can be captured by <Subject, V erb, Object>\ntriplet. First, we parse natural sentence descriptions into\nSVO triplets that represent each subject, verb and object re-\nspectively, then we leverage a continuous language model\n(Mikolov et al. 2013) to represent each element of SVO\nwith a continuous vector. Based upon the initial word vec-\ntor, we construct our language model in the dependency-tree\nstructure. The right side of Fig. 2 shows our model, S, V and\nO are leaf nodes, and there are two structures in our particu-\nlar problem. To compose leaf nodes towards higher layers, a\ncomposition function is applied to two leaf nodes, note that\nweight W\nm in Fig. 2 is recursively used until the root node\nis composed. Therefore, the root node is the representation\nof an SVO triplet in video-text space. With our language\nmodel, the compositionality of nodes can be explicitly mod-\neled by the weight of the composition function.\nSecondly, inspired by current advances of deep learning\n(Donahua et al. 2013; Krizhevsky, Sutskever, and Hinton\n2012), we present a deep video model: as Fig. 2 shows, vi-\nsual features are extracted with a deep neural network (Don-\nahua et al. 2013) from a sequence of frames of each video,\nwe use a temporal pyramid scheme to capture motion infor-\nmation, then build a two-layer neural network to map visual\nfeatures to video-text space.W1 and W2 are weights in the\ntwo-layer neural network.\nFinally, we propose the joint embedding model that min-\nimizes the distance of the outputs of the deep video model\nand compositional language model in video-text space, and\nupdate these two models jointly in the uniﬁed framework.\nRelated Work\nRecently, there are many new works related to our prob-\nlem. In this section, we mainly review two relevant topics,\n1) video to text and 2) multi-modal embeddings.\nVideo to text In describing videos with sentences, (Kr-\nishnamoorthy et al. 2013; Motwani and Mooney 2012) are\nearlier papers that focus on smaller data sets. (Barbu et al.\n2012) build a system that leverages on object detection, ob-\nject tracking and human pose estimation in order to ren-\nder linguistic entities and generate sentences. (Ramanathan,\nLiang, and Fei-Fei 2013) use a topic-based semantic relat-\nedness measure to help action and role classiﬁcation, (Y u\nand Siskind 2013) use HMMs to track sentences and learn\nthe representation for word meanings with the help of video\nclips. The above two papers separately show language help-\ning vision tasks and vision helping language tasks, but not a\njoint model to push the other direction.\nMulti-modal embeddings The only paper we aware of\nthat constructs multi-modal space for video and language is\n(Das, Srihari, and Corso 2013), which uses latent topics to\ndiscover the most probable related words from text and fur-\nthermore, translate words to probable frames, while bound-\ning box annotation of objects are needed. Additionally, there\nexist a number of papers on image and language embedding:\n2347\n(Frome et al. 2013) propose a deep visual-semantic embed-\nding model to bridge image and text. (Socher and Fei-Fei\n2010) present a semi-supervised model to segment and an-\nnotate images that ﬁnds mapping between segment-wise vi-\nsual words and textual words based on kCCA.\nOur approach differs from above methods in that we ex-\nplore the compositionality of word relations within the joint\nmodel and we can also infer the representation of words due\nto our tree structured language model andSVO assumption.\nAn Uniﬁed Framework with Joint\nVideo-Language Model\nIn this section, we propose a joint model for video-language\nembedding. In our joint architecture, the goal is to learn a\nfunction f(V),\nf : V→T (1)\nwhere V represents the low-level features extracted from\nvideo, and T is the high-level text description of the video.\nSince videoV represents the low-level signals and language\nT shows high-level human expression, we need a bridge\nto connect these two levels of information. Thus, we pro-\npose a joint modelP which consists of three parts: compo-\nsitional language modelM\nL : T → Tf , deep video model\nMV : V → Vf and an joint embedding modelE(Vf ,Tf ),\nsuch that\nP : MV (V) −→Vf ↔ E(Vf ,Tf ) ↔ Tf ←→ML(T)\n(2)\nwhere Vf and Tf are the output of the deep video model and\ncompositional language model respectively. Using our joint\nmodel P with three parts, video and corresponding language\ndescriptions can be integrated into our uniﬁed framework.\nNext, we explain each part of the joint model in detail.\nCompositional Semantics Language Model\nThe sentence description generated by a human is a unique\nand important resource in video or image to text. Gener-\nally, researchers consider each word in text description as\na discrete index: (Guadarrama et al. 2013) use distributional\nclustering (Pereira, Tishby, and Lee 1993) based on word\nco-occurrence in particular syntactic contexts to build word\nhierarchy, (Das et al. 2013) use topic model to verify pre-\ndicted words. We use a continuous space word representa-\ntion (Mikolov et al. 2013) to initialize because it captures a\nlarge number of syntactic and semantic word relationships.\nFor example, with such representation,vec(“Germany”) +\nvec(“capital”) is close tovec(“Berlin”). Due to the great\nvariance of human input sentences, a continuous word vec-\ntor is more suitable to capture semantic similarity.\nFirst, we use the Stanford Parser (Klein and Manning\n2013) to parse all sentences, then we choosensubj to ﬁnd\nsubject-verb pair, and choosedobj to ﬁnd verb-object pairs\nin order to obtainSVO triplets. Then, each word inSVO\nis mapped to a continuousd-dimensional vector by model\nM\n1\nL(·) which is the same as (Mikolov et al. 2013),\nM1\nL : T −→[ms,mv,mo] (3)\nwhere ms, mv and mo are corresponding continuous word\nvector of SVO . To obtain an embed feature representa-\ntion for SVO , we propose a novel compositional language\nmodel CLM(·) with recursive neural network as in Fig. 2.\nIn the compositional language modelCLM(·), the goal\nis to learn a representation of[ms,mv,mo] for sentences.\nAssume we are given the continuous vectors[ms,mv,mo]\nof SVO for sentence, ﬁrst, infer the representation forSV\nas msv: ([ms,mv] → msv), then combining msv with\nmo, obtain the embedded language representation msvo:\n([msv,mo) → msvo) as the sentence representationTf in\nembedding model.\nConcretely, we adopt the Forward Propagation algorithm\nto infer the sentence representation, as follows.\nAs in Fig. 2, theSVO triplet is <person, ride, horse>,\nand corresponding word vectors are denoted asms, mv and\nmo, our model compute parent word vector from dependent\nchild nodes with a composition functionf parameterized by\nWm, e.g., we can compute the parent nodemsv via:\nmsv = f(Wm[ms; mv]+ bm) (4)\nwhere Wm ∈ Rd×2d is a parameter matrix, andbm ∈ Rd\nis bias. In all experiments d = 300. Similarly, we apply\nthe composition function recursively and compute the par-\nent node ofm\nsv and mo via:\nmsvo = f(Wm[msv; mo]+ bm) (5)\nWe usetanh(·) as composition functionf(·), sincetanh(·)\nperforms well in most deep neural network.\nTo measure how well a parent node can represent the child\nnodes, we reconstruct the child nodes with a matrixWr ∈\nR2d×d and reconstructmrec\ns and mrec\nv with:\n[mrec\ns ; mrec\nv ]= Wrmsv + br (6)\n[mrec\nsv ; mrec\no ]= Wrmsvo + br (7)\nThe reconstruction error of one non-terminal nodep with\nour compositional language model is:\nErec(p|Wm,Wr)= n1\nn1 + n2\n∥m1 −mrec\n1 ∥2\n2\n+ n2\nn1 + n2\n∥m2 −mrec\n2 ∥2\n2 (8)\nwhere m1 and m2 are children of p, mrec\n1 and mrec\n2 are\nreconstructed word vectors of m1 and m2. n1 and n2 are\nnumber of nodes in both branches underp, which are used\nto weigh the branch with more children higher.\nDeep Video Model\nOur visual modelMV (V) applies features from a deep neu-\nral network (Donahua et al. 2013) trained with ImageNet\n(Deng et al. 2009). We extract one frame per second from\nvideo and compute a 4096-dimensional feature per frame,\nand ﬁnd 7th layer output after ReLU performs the best.\nThen, we apply a temporal pyramid scheme to summarize\nthe feature sequence and capture motion information.\nDenote visual feature as x, we train a two-layer neural\nnetwork to projectx to embedding space:\nV\nf (x)= W1f(W2x) (9)\n2348\nWe use standard non-linear function tanh(·) as f(·), W2\n∈ Rh×4096 and W1 ∈ R300×h are parameter matrix that\nmap x ∈ R4096 to joint space. We tested differenthand ﬁnd\nit insensitive, in our experiments we seth as 1000. We have\ntested single layer neural network and single linear mapping,\nand ﬁnd two-layer neural network works the best. Our base-\nline method with CCA conﬁrms a similar ﬁnding.\nJoint Embedding Model\nThe Compositional Semantics Language Model captures\nhigh-level semantics information that can help constrain the\nvisual model, and the visual model on the contrary, pro-\nvides video evidence to support word selection. Most ex-\nisting methods focus on the second bottom-up path using\nvisual evidence to support word generation, while the ﬁrst\ntop-down path is largely neglected. In our joint embed-\nding model, we deﬁne an objective function to take into\naccount video-language embedding error E\nembed and lan-\nguage model reconstruction errorErec. Eembed is based on\nleast squares to implement both bottom-up and top-down\npaths simultaneously:\nEembed(V,T )= (10)\n∥W1f(W2xi) −CLM(ms,i,mv,i,mo,i|Wm)∥2\n2 ,\nwhere ms,i represents S word vector ofi-th video. The ob-\njective function is:\nJ(V,T )=\nN∑\ni=1\n(Eembed(V,T )+\n∑\np∈NT\nErec(p|Wm,Wr))\n+ r. (11)\nwhere NT is the non-terminal set of one tree structure. Sup-\npose our training set contains N videos, each paired with M\nsentences, and eachSVO triplet has t tree structures. Letθ\nbe a general notation of modelW\n1,W2,Wm or Wr, the reg-\nularization termr = λ/2 ∥θ∥2\n2. In practice, we use the mean\nword vector over all sentences as ground truth in each video\nto represent the training sentence.\nAccording to this embedding model, we update the vi-\nsual model and the compositional language model jointly.\nAnd we implement both bottom-up and top-down paths in\nthe embedded model, thus our model is able to process both\nvideo-to-text and text-to-video tasks.\nLearning and Inference\nLearning the Joint Video-Language Model\nTo estimate parametersW1,W2,Wm and Wr, we initialize\nms,mv and mo with word vectors, W1,W2 and Wm,Wr\nwith random zero-mean matrix normalized by column di-\nmension of the matrix. We propose a coordinate descent\nmethod to optimize the objective function. To start, we ﬁrst\nﬁx W\nm,Wr and optimizeW1 and W2, the gradient is:\ngradW1 = 1\nN\nN∑\ni=1\n2(W1f(W2xi)− (12)\nCLM(ms,i,mv,i,mo,i|Wm))df(W2xi)⊺ + λW1.\ngradW2 = 1\nN\nN∑\ni=1\n2W⊺\n2 (W1f(W2xi)− (13)\nCLM(ms,i,mv,i,mo,i|Wm))df(W2xi)x⊺\ni + λW2.\nwhere df(·) is derivative function oftanh(·).\nThen, we ﬁx W1, W2 and optimize Wm, Wr. Starting\nfrom the top node, we use backward propagation (Goller\nand Kuchler 1996) through the tree structure to compute the\ngradient. In practice, we attach the bias termb\nm,br with the\nWm,Wr matrix and learn them together. We iteratively opti-\nmize the visual model and the language model with L-BFGS\nto minimize the objective function.\nInference\nOne advantage of our model is to infer theSVO representa-\ntion and thus predict theSVO directly. Our ﬁrst intuition is\nto consider each word itself as model and initialize a word\nvector of SVO randomly, but preliminary experiments do\nnot show good results, so we leave it as future work and\nalternatively design a strategy to initializeSVO representa-\ntion.\nGiven the visual model and the language model, we can\ndescribe a video with sentences by projecting a test video\nto the video-language space and ﬁnding thek nearest triplet\nvector m\nsvo in a large sentence pool. We build such a sen-\ntence pool from all sentence descriptions of training videos,\nthen they are parsed with the Stanford Parser, mapped to\nword vector, and composed tom\nsvo via forward propagation\nwith trained language model. We initialize the word repre-\nsentation with the most frequent word vector in topk neigh-\nbors of testing video. Then, we estimatems,mv and mo for\ntest video by optimize objective function:\n(ms,mv,mo)= (14)\narg min\nms,mv,mo\n∥W1f(W2x) −CLM(ms,mv,mo|Wm)∥2\n2 .\nWe use forward-backward propagation to compute gradient\nof word vector and optimize with L-BFGS.\nExperiments\nIn the following subsections, we introduce the experimen-\ntal setup, three baseline methods, and evaluation results on\nthree tasks: 1)SVO prediction and natural language gener-\nation, and 2) video retrieval and 3) language retrieval.\nExperimental Setup\nData set The data set we use is Y ouTube videos collected\nby Chen et al. (Chen and Dolan 2011), which contains 1970\nshort video clips and paired with multiple language descrip-\ntions. We split the data set as 1297 training videos and 670\ntesting videos, same as (Guadarrama et al. 2013).\n2349\nDeﬁning Ground Truth forSVO . Given extracted sen-\ntences, we use the Stanford Parser to extractSVO triplet and\nuse Porter Stemming algorithm to stem words. Then, we ﬁl-\nter all labels that don’t appear in the description of at least 5\nvideos, we further ﬁlter remaining labels that have no match\nto WordNet or pre-trained 3 million words and phrases pro-\nvided by (Mikolov et al. 2013). Then, we compute Lesk sim-\nilarity (Pedersen, Patwardhan, and Michelizzi 2004) for all\nremaining word pairs inSVO group. At last, we use spectral\nclustering on Lesk similarity to get 45 subject clusters, 218\nverb groups and 241 groups. Words in same cluster are re-\ngarded as synonyms. We extract the “Most Common”SVO\ntriplet from all candidate triplets in one video as ground\ntruth.\nBaseline Methods\nTo fully evaluate our model, we designed three non-trivial\nbaseline methods, i.e. 1) SVM, 2) Conditional Random Field\n(CRF) model and 3) Canonical Correlation Analysis (CCA).\nSVM We apply the state-of-the-art motion descriptor, i.e.\ndense trajectory (Wang et al. 2011) and object descrip-\ntor, i.e. ObjectBank (Li et al. 2011) as visual features. We\nrandom sample 100,000 visual features from each chan-\nnel of dense trajectory descriptors and construct a 4000-\ndimensional codebook with K-means, then we encode each\nvideo as histogram of cluster centers. For object description,\nwe use the ObjectBank default 177 object models trained\nfrom ImageNet and 20 object models trained from Pascal\n2007 data set. Finally, we train a RBF kernel SVM with all\nvisual features.\nCRF In order to compare with our compositional seman-\ntics language model, we propose a CRF model to incorpo-\nrate subject-verb and verb-object pairwise relationship. We\nminimize the following energy function over labelingL of\nvideo Vid and sentenceTex:\nE(L; V,T )= α\n1ψ(S; Vid)+ α2ψ(V; Vid)+ (15)\nα3ψ(O; Vid)+ α5ψ(S,V ; Tex)+ α4ψ(V,O ; Tex)\nUnary potentialsψ(S; Vid),ψ(V; Vid),ψ(O; Vid) are rep-\nresented by probability outputs of SVM score overSVO .\nPairwise potential ψ(S,V ; Tex),ψ(V,O ; Tex) are learned\nfrom word co-occurrence. Speciﬁcally, for each subject,\nverb or object, we use the same training sentence pool de-\nscribed in Inference section to get all pairwise co-occurrence\nstatistics. Gibbs Sampling is used as inference algorithm.\nCCA CCA (Socher and Fei-Fei 2010) has been used to\nbuild an image-text space, we use CCA as a baseline to build\nthe video-language space and compare video retrieval/text\nretrieval with our method. We use same deep video feature\nand average of word vector to learn the joint space. Then, we\napply the same strategy as in the Inference section and use\nthe mapped sentence representation to search thek nearest\nneighbors in large sentence pool and vote the most frequent\ntriplets.\nFigure 3: Video retrieval examples. For each query sentence, the\ntop row shows top four videos retrieved with our method, the bot-\ntom row shows top four videos retrieved with CCA. Video in red\nbounding box indicates it is the ground truth video.\nTable 1:SV Oprediction accuracy with Prior, SVM, CRF, CCA,\nOur model\nMethod Prior SVM CRF CCA Ours\nS(%) 77.01 77.16 77.16 77.16 78.25\nV(%) 14.63 22.39 22.54 21.04 24.45\nO(%) 4.18 9.10 9.25 10.99 11.95\nResults\nSVO triplet prediction For SVO prediction, we use bi-\nnary (0-1 loss) to measure the accuracy. Following (Guadar-\nrama et al. 2013), we get 45 subjects, 218 verbs and 241\nobjects as ground truth classes. We evaluate 5 baseline meth-\nods, 1) “Prior” that uses the prior distribution of SVO\ntriplet, which means simply ﬁnd the most commonSVO in\ntraining set as prediction results. 2) SVM, 3) CRF, 4) CCA,\nand our joint model.\nFrom Table. 1, it is clear that our joint model outper-\nforms all our baselines in predicting subject, verb and object.\nIt demonstrates by capturing compositional semantics and\nbuilding both top-down and bottom-up connection between\nvideo and language, our approach is able to show some ad-\nvantages over pure bottom-up methods and multi-modal em-\nbedding without considering compositionality.\nNatural Language Generation Fig. 4 shows sentences\n2350\nFigure 4: Natural language generation of different methods. The\npictures are sampled video frames, the output sentence generated\nis by 1) GT: Human generated ground truth, 2) CCA baseline, 3)\nSVM baseline 4) CRF and 5) Our joint model.\ngenerated by different methods and human annotation. The\nqualitative results are basically consistent with quantitative\nresults shown in Table. 1. We observe that our model and\nCCA tend to return similar results, meanwhile, SVM and\nCRF also tends to return similar results, which represents the\ndistinct differences of those two types of methods. In the 3rd\nand 4th row, our model and CCA performs better in ﬁnding\nobjects. We also ﬁnd the CRF does capture some pairwise\nrelationship among triplets, e.g. in 6th row, SVM returns in\n“rides a station” while CRF returns “rides a horse”.\nVideo Retrieval In this task, we focus on evaluating how\nwell a sentence can retrieve a video with corresponding se-\nmantic meaning. Firstly, for each testing video we select\n5 sentences, so totally we have 3350 sentences and 670\nvideos. We map a query sentence into the joint space and\nﬁnd nearest neighbor videos (also in joint space) based on\nEuclidean distance. We record the ﬁrst correct video posi-\ntion for each query, and then we calculate mean position, or\nmean rank over all sentences to measure the video retrieval\nperformance. Note that random assignment will return mean\nrank of 335. From Table. 2 we ﬁnd our model is better than\nCCA results. Qualitatively, Fig. 3 shows top retrieved videos\nwith our method and CCA. Both methods return videos with\nsigniﬁcant human motion in ﬁrst query example and return\nvideos of “cooking” in third query example. On one hand,\nour method performs better in terms of speciﬁc object, such\nas a “guitar” in the second query and “wood” in the third\nquery. On the other hand, our method also performs better\nin capturing action than CCA, e.g., in the ﬁrst query, the top\nfour retrieved videos are all related to the action of “tumble\ndown” from the sentence, either ”fell down” from motorcy-\ncle, in a dance or “be drawn” into the water.\nText Retrieval In this task, we evaluate how well a video\ncan ﬁnd suitable language descriptions. We map each query\nvideo to joint space, and ﬁnd nearest neighbor sentences\nin same space. Similar to video retrieval, we record rank\nof ﬁrst correct sentence that describe query video and use\nmean rank to measure the overall performance. In this ex-\nTable 2:\nVideo Retrieval and Text Retrieval, evaluated by Mean\nRank (mRank)\nMethod Video Retrieval mRank Text Retrieval mRank\nCCA 245.33 251.27\nOurs 236.27 224.10\nFigure 5: Text retrieval examples. For each query video, the left\nand right columns show top four retrieved sentences using our\nmodel and CCA.\nperiments, we also observe a relatively large improvement of\nour method over CCA baseline. Qualitatively, Fig. 5 shows\nthe retrieved sentences using our joint model outperforms\nCCA. Both methods perform quite well in recovering com-\nmon object such as person, cat and dog, while our method\nis more stable, e.g., all top four retrieved sentences from\nﬁrst and the 5th query are accurate with our method while\nthere’re some error using CCA. Beside, for less common\nvideo such as the 4the query, our method get one accurate\nsentence, and also retrieve “banana is being peeled”, which\ncorresponds to the action and the color of the paper, while\nCCA dose not retrieve any meaningful sentences.\nSummary From the above three experiments, we ﬁnd the\nmean rank of both video retrieval and text retrieval are quite\nhigh, but theSVO prediction accuracies have outperformed\nSVM or CRF models, which means exploiting better video-\ntext space has great potential for natural language genera-\ntion. Besides, for both our model and CCA, we ﬁnd when\nthe training error decreases to a certain point the testing er-\nror increases. It is understandable since we only have 1300\ntraining videos while theSVO class number is as large as\n504, so it is necessary that we collect larger data set or ex-\nplore the ontology of classes structure.\n2351\nConclusions\nIn this paper, we propose a uniﬁed framework to jointly\nmodel video and language. Speciﬁcally, a compositional lan-\nguage model is proposed to capture high-level human lan-\nguage expression, and a deep video model is proposed to\nrepresent the low-level video signal. Our model can cap-\nture the compositionality of subject, verb and object leverag-\ning on continuous word representations rather than word co-\noccurrence. Experiments demonstrate the advantage of our\nmodel over SVM baseline, a CRF model and a CCA base-\nline for video-language space. Our future work includes 1)\nexploring more complex sentence compositionality beyond\nSVO , 2) exploring better deep video model to capture mo-\ntion 3) exploring more complex model that consider scene,\nspatial-temporal prepositions and other interesting details in\nvideo.\nAcknowledgements\nThis work was in part supported by NSF\nCAREER IIS-0845282, DARPA/ARL W911NF-10-2-0062, and\nARO W911NF-11-1-0090.\nReferences\nBarbu, A.; Bridge, A.; Burchill, Z.; Coroian, D.; Dickinson, S.; Fi-\ndler, S.; Michaux, A.; Mussman, S.; Narayanaswamy, S.; Salvi, D.;\nSchmidt, L.; Shangguan, J.; rey Mark Siskind, J.; Waggoner, J.;\nWang, S.; Wei, J.; Yin, Y .; and Zhang, Z. 2012. Video in sentences\nout. In UAI.\nChen, D. L., and Dolan, W. B. 2011. Collecting highly parallel\ndata for paraphrase evaluation. InACL.\nDas, P .; Xu, C.; Doell, R. F.; and Corso, J. J. 2013. A thousand\nframes in just a few words: Lingual description of videos through\nlatent topics and sparse object stitching. InCVPR.\nDas, P .; Srihari, R. K.; and Corso, J. J. 2013. Translating related\nwords to videos and back through latent topics. InWSDM.\nDeng, J.; Li, K.; Do, M.; Su, H.; and Fei-Fei, L. 2009. Construction\nand analysis of a large scale image ontology. In Vision Science\nSociety.\nDonahua, J.; Jia, Y .; Vinyals, O.; Hoffman, J.; Zhang, N.; Tzeng,\nE.; and Darrell, T. 2013. Decaf: A deep convolutional activation\nfeature for generic visual recognition. InarXiv:1310.1531.\nFelzenszwalb, P . F.; Girshick, R. B.; McAllester, D.; and Ramanan,\nD. 2010. Object detection with discriminatively trained part based\nmodels. TPAMI.\nFrome, A.; Corrado, G. S.; Shlens, J.; Bengio, S.; Dean, J.;\nMikolov, T.; et al. 2013. Devise: A deep visual-semantic embed-\nding model. InNIPS.\nGoller, C., and Kuchler, A. 1996. Learning task-dependent dis-\ntributed representations by backpropagation through structure. In\nInternational Conference on Neural Networks.\nGuadarrama, S.; Krishnamoorthy, N.; Malkarnenkar, G.; V enu-\ngopalan, S.; Mooney, R.; Darrell, T.; and Saenko, K. 2013.\nY outube2text: Recognizing and describing arbitrary activities us-\ning semantic hierarchies and zero-shot recognition. InICCV.\nGupta, A.; Srinivasan, P .; Shi, J.; and Davis, L. S. 2009. Under-\nstanding videos, constructing plots learning a visually grounded\nstoryline model from annotated videos. InCVPR.\nKarpathy, A.; Toderici, G.; Shetty, S.; Leung, T.; Sukthankar, R.;\nand Fei-Fei, L. 2014. Large-scale video classiﬁcation with convo-\nlutional neural networks. InCVPR.\nKlein, D., and Manning, C. D. 2013. Accurate unlexicalized pars-\ning. In ACL.\nKrishnamoorthy, N.; Malkarnenkar, G.; Mooney, R. J.; Saenko, K.;\nand Guadarrama, S. 2013. Generating natural-language video de-\nscriptions using text-mined knowledge. InAAAI.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet\nclassiﬁcation with deep convolutional neural networks. InNIPS.\nKulkarni, G.; Premraj, V .; Dhar, S.; Li, S.; Choi, Y .; Berg, A. C.;\nand Berg, T. L. 2011. Baby talk: Understanding and generating\nsimple image descriptions. InCVPR.\nLi, L.-J.; Su, H.; Xing, E. P .; and Fei-Fei, L. 2011. Object bank:\nA high-level image representation for scene classiﬁcation and se-\nmantic feature sparsiﬁcation. InNIPS.\nMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.; and Dean, J.\n2013. Distributed representations of words and phrases and their\ncompositionality. In NIPS.\nMiller, G. A. 1995. Wordnet: A lexical database for english. In\nCommunications of the ACM, 39–41.\nMotwani, T., and Mooney, R. 2012. improving video activity\nrecognition using object recognition and text mining. InECAL.\nMoxley, E.; Mei, T.; and Manjunath, B. S. 2010. Video annotation\nthrough search and graph reinforcement mining. InIEEE Transac-\ntions on Multimedia.\nNakayama, H.; Harada, T.; and Kuniyoshi, Y . 2010. Evaluation\nof dimensionality reduction methods for image auto-annotation. In\nBMVC.\nPedersen, T.; Patwardhan, S.; and Michelizzi, J. 2004. Word-\nnet::similarity - measuring the relatedness of concepts. In HLT-\nNAACL.\nPereira, F.; Tishby, N.; and Lee, L. 1993. Distributional clustering\nof english words. InACL.\nRamanathan, V .; Liang, P .; and Fei-Fei, L. 2013. Video event\nunderstanding using natural language description. InICCV.\nRohrbach, M.; Qiu, W.; Titov, I.; Thater, S.; Pinkal, M.; and\nSchiele, B. 2013. Translating video content to natural language\ndescriptions. In ICCV.\nSadanand, S., and Corso, J. J. 2012. Action bank: A high-level\nrepresentation of activity in video. InCVPR.\nSocher, R., and Fei-Fei, L. 2010. Connecting modalities: Semi-\nsupervised segmentation and annotation of images using unaligned\ntext corpora. InCVPR.\nSocher, R.; Karpathy, A.; Le, Q. V .; Manning, C. D.; and Ng, A. Y .\n2013. Grounded compositional semantics for ﬁnding and describ-\ning images with sentences. InTransactions of the ACL.\nSong, J.; Y ang, Y .; and Huang, Z. 2011. Multiple feature hashing\nfor real-time large scale near-duplicate video retrieval. In ACM\nInternational Conference on Multimedia.\nThomason, J.; V enugopalan, S.; Guadarrama, S.; Saenko, K.; and\nMooney, R. 2014. Integrating language and vision to generate\nnatural language descriptions of videos in the wild. InCOLING.\nTian, Y .; Sukthankar, R.; and Shah, M. 2013. Spatiotemporal de-\nformable part models for action detection. InCVPR.\nWang, H.; Kl¨aser, A.; Schmid, C.; and Liu, C.-L. 2011. Action\nrecognition by dense trajectories. InCVPR.\nY ao, T.; Mei, T.; Ngo, C.-W.; and Li, S. 2013. Annotation for free:\nVideo tagging by mining user search behavior. InACM Interna-\ntional Conference on Multimedia.\nY u, H., and Siskind, J. M. 2013. Grounded language learning from\nvideo described with sentences. InACL.\nZhang, W.; Zhu, M.; and Derpanis, K. G. 2013. From actemes\nto action: A strongly-supervised representation for detailed action\nunderstanding. In ICCV.\n2352",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8712289333343506
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6769546866416931
    },
    {
      "name": "Natural language processing",
      "score": 0.6533360481262207
    },
    {
      "name": "Principle of compositionality",
      "score": 0.5709154605865479
    },
    {
      "name": "Language model",
      "score": 0.5492820143699646
    },
    {
      "name": "Natural language",
      "score": 0.5391256213188171
    },
    {
      "name": "Sentence",
      "score": 0.514351487159729
    },
    {
      "name": "Embedding",
      "score": 0.46785834431648254
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4537891745567322
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63190737",
      "name": "University at Buffalo, State University of New York",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    }
  ]
}