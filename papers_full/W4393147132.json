{
  "title": "What Makes Quantization for Large Language Model Hard? An Empirical Study from the Lens of Perturbation",
  "url": "https://openalex.org/W4393147132",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4320549389",
      "name": "Zhuocheng Gong",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2114544295",
      "name": "Jiahao Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124326937",
      "name": "Jingang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3154473620",
      "name": "Xunliang Cai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098869281",
      "name": "Dongyan Zhao",
      "affiliations": [
        "King University",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2109109241",
      "name": "Rui Yan",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A4320549389",
      "name": "Zhuocheng Gong",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2098869281",
      "name": "Dongyan Zhao",
      "affiliations": [
        "Convergence",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2109109241",
      "name": "Rui Yan",
      "affiliations": [
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3202028501",
    "https://openalex.org/W4312056202",
    "https://openalex.org/W4379251658",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W6788001715",
    "https://openalex.org/W2473344385",
    "https://openalex.org/W4385574241",
    "https://openalex.org/W2973061659",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W4381827750",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4384919461",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4327810129",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4379260375",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3137147200",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2524428287",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W4378770729",
    "https://openalex.org/W4307934016",
    "https://openalex.org/W4385572117",
    "https://openalex.org/W4281609268",
    "https://openalex.org/W4378509449"
  ],
  "abstract": "Quantization has emerged as a promising technique for improving the memory and computational efficiency of large language models (LLMs). Though the trade-off between performance and efficiency is well-known, there is still much to be learned about the relationship between quantization and LLM performance. To shed light on this relationship, we propose a new perspective on quantization, viewing it as perturbations added to the weights and activations of LLMs. We call this approach ``the lens of perturbation\". Using this lens, we conduct experiments with various artificial perturbations to explore their impact on LLM performance. Our findings reveal several connections between the properties of perturbations and LLM performance, providing insights into the failure cases of uniform quantization and suggesting potential solutions to improve the robustness of LLM quantization. To demonstrate the significance of our findings, we implement a simple non-uniform quantization approach based on our insights. Our experiments show that this approach achieves minimal performance degradation on both 4-bit weight quantization and 8-bit quantization for weights and activations. These results validate the correctness of our approach and highlight its potential to improve the efficiency of LLMs without sacrificing performance.",
  "full_text": "What Makes Quantization for Large Language Models Hard?\nAn Empirical Study from the Lens of Perturbation\nZhuocheng Gong1, Jiahao Liu2, Jingang Wang2, Xunliang Cai2, Dongyan Zhao1,4‚Ä† , Rui Yan3‚Ä†\n1 Wangxuan Institute of Computer Technology, Peking University\n2 Meituan\n3 Gaoling School of Artificial Intelligence, Renmin University of China\n4 State Key Laboratory of Media Convergence Production Technology and Systems\n{gzhch,zhaody}@pku.edu.cn, ruiyan@ruc.edu.cn, {liujiahao12,wangjingang02,caixunliang}@meituan.com\nAbstract\nQuantization has emerged as a promising technique for im-\nproving the memory and computational efficiency of large\nlanguage models (LLMs). Though the trade-off between per-\nformance and efficiency is well-known, there is still much to\nbe learned about the relationship between quantization and\nLLM performance. To shed light on this relationship, we pro-\npose a new perspective on quantization, viewing it as pertur-\nbations added to the weights and activations of LLMs. We\ncall this approach ‚Äúthe lens of perturbation‚Äù. Using this lens,\nwe conduct experiments with various artificial perturbations\nto explore their impact on LLM performance. Our findings\nreveal several connections between the properties of pertur-\nbations and LLM performance, providing insights into the\nfailure cases of uniform quantization and suggesting potential\nsolutions to improve the robustness of LLM quantization. To\ndemonstrate the significance of our findings, we implement\na simple non-uniform quantization approach based on our\ninsights. Our experiments show that this approach achieves\nminimal performance degradation on both 4-bit weight quan-\ntization and 8-bit quantization for weights and activations.\nThese results validate the correctness of our approach and\nhighlight its potential to improve the efficiency of LLMs\nwithout sacrificing performance.\nIntroduction\nRecently, large language models (LLMs) have gained a lot\nof attention in the deep learning community due to their re-\nmarkable performance (Touvron et al. 2023; OpenAI 2022).\nHowever, their size and computational demands pose chal-\nlenges for deployment on resource-constrained devices or\nin real-time applications. To address this issue and im-\nprove the accessibility and efficiency of these models, re-\nsearchers have explored quantization techniques that enable\nhigh-precision data to be represented in lower-precision for-\nmats. These techniques can significantly reduce the memory\nfootprint and computational requirements of large models,\nmaking them more practical for deployment and use.\nQuantizing large language models presents unique chal-\nlenges that require careful consideration. First, LLMs can\nbe incredibly large, with billions of parameters and multiple\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n‚Ä†Corresponding authors: Dongyan Zhao and Rui Yan.\nlayers, resulting in significant quantization error accumula-\ntion. Second, due to the large model scale, training can be\nextremely costly, making post-training quantization (PTQ)\nthe most appealing approach for LLMs, as it requires much\nless or even zero calibration data to perform (Bondarenko,\nNagel, and Blankevoort 2021; Wu, Yao, and He 2023; Xiao\net al. 2023). In summary, effectively quantifying LLMs re-\nquires a tailored approach that balances model complexity,\ncomputational resources, and performance. Despite recent\nstudies investigating the quantization of LLMs, a compre-\nhensive investigation into the performance of quantization\nhas not been satisfactorily undertaken. Specifically, explor-\ning the impact of different quantization strategies on differ-\nent model families during the quantization process would be\nbeneficial to the development of LLM quantization.\nIn this paper, our goal is to investigate the impact of quan-\ntization on LLMs in a systematic manner, with a focus on\nunderstanding the challenges of quantizing large language\nmodels. Specifically, we examine the effects of zero-shot\npost-training uniform quantizationon LLMs, as it is a\npractical and representative quantization setting that requires\nno additional training or data. To begin, we apply standard\nuniform quantization to various LLMs and analyze the re-\nsulting performance degradation (as shown in Figure 1).\nOur findings indicate that the impact of quantization varies\nsignificantly across different model families and sizes. The\nBLOOM and LLAMA model families exhibit greater ro-\nbustness to quantization, while OPT models are more sensi-\ntive to quantization. Furthermore, we observe that the scale\nof the model is a crucial factor that affects the quality of\nquantization. Finally, we find that activation quantization has\na significant effect on performance.\nTo account for the various factors that impact quantiza-\ntion, it is important to establish an analytical framework.\nThis is where ‚Äùthe lens of perturbation‚Äùcomes to play.\nWe view quantization as the addition of small perturbations\nto the weights and/or activations of the model. By perturbing\nthese components and measuring the resulting performance\ndegradation, we can analyze the impact of quantization on\nLLMs. This approach enables us to gain insights into how\nquantization influences the model‚Äôs performance in differ-\nent ways. To be more specific, we subject LLMs to differ-\nent kinds of artificial perturbations and investigate which\nperturbation causes the most performance degradation. By\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18082\nFigure 1: Performance of quantized LLMs on Lam-\nbada (Paperno et al. 2016) on different model families\n(BLOOM (Scao et al. 2022), OPT (Zhang et al. 2022), and\nLLAMA (Touvron et al. 2023)), the parameter number scal-\ning from 350M to 66B. We implement two uniform quan-\ntization settings: one is to quantize both weights and acti-\nvations to 8 bits (W8A8), and the other is to perform 4-bit\nchannel-wise for weights only (W4A16).\nexperimenting with artificial perturbations, we can gain in-\nsights into the robustness of the LLM quantization method\nand identify potential areas for improvement.\nBy using the lens of perturbation, we are able to delve\ndeeper into the mechanism of quantization and identify sev-\neral factors that impact the performance of LLM quanti-\nzation. Our first finding is that the ability of data to with-\nstand perturbations is closely tied to its magnitude. Specif-\nically, larger values can endure more significant perturba-\ntions with minimal performance degradation, while smaller\nvalues can only tolerate minor perturbations. Our second ob-\nservation pertains to the outlier phenomenon, which involves\nabnormally large values in weights and activations (Bon-\ndarenko, Nagel, and Blankevoort 2021). These insights ob-\ntained through the lens of perturbation explain why uniform\nquantization may not always be optimal. Uniform quantiza-\ntion treats all weights and activations equally, without con-\nsidering their sensitivity to perturbations, and is obsessed\nwith outlier values. Instead, we propose a potential improve-\nment by taking into account the sensitivity to perturbations.\nThrough experiments, we demonstrate that this modification\nsignificantly reduces the quantization error, where uniform\nquantization falls short. These findings provide a more nu-\nanced understanding of the impact of quantization on LLMs\nand offer a promising direction for improving the quantiza-\ntion process.\nIn summary, our contributions are as follows:\n‚Ä¢ We propose a new perspective of investigating uniform\nquantization for LLMs, namely the lens of perturbation.\nThis lens provides new insights into the challenges of\nquantization for LLMs.\n‚Ä¢ We conduct a comprehensive comparison of the quanti-\nzation performance of LLMs across various model scales\nand three model families (LLAMA, BLOOM, and OPT),\nusing three quantization settings (W4A16, W8A8, and\nW4A8)\n‚Ä¢ Drawing on the insights through the lens of perturba-\ntion, we conduct preliminary experiments to leverage\nthe properties of quantization-friendly perturbations and\nexplore the potential of non-uniform quantization for\nLLMs, which significantly reduces quantization error.\nThese experiments demonstrate that non-uniform quan-\ntization can significantly reduce quantization error.\nRelated Works\nLarge Language Model Quantization\nQuantization, which represents the weights and activations\nof neural networks with low-bit precision, has been widely\nstudied in computer vision and natural language processing\n(NLP) communities (Gholami et al. 2021). Recently, some\nresearchers explore quantizing large language models to re-\nduce the deployment cost (Shen et al. 2020; Wu et al. 2022;\nKim et al. 2021; Bondarenko, Nagel, and Blankevoort 2021;\nWu, Yao, and He 2023; Liu et al. 2023; Dettmers et al.\n2023; Xiao et al. 2023). One major factor distinguishing\nLLM quantization is that LLMs with their enormous pa-\nrameters are too expensive to train. Thus, most current ap-\nproaches choose few-shot or zero-shot post-training quan-\ntization, which directly quantizes the already-trained model\nwithout much calibration data. These methods mostly build\non uniform quantization, the most widespread, practical, and\neasy-to-use quantization strategy. Up to now, it has been val-\nidated that quantizing the weight of LLMs to 8 or even 4\nbits is not too hard (Dettmers and Zettlemoyer 2023), how-\never, activation quantization is still a major obstacle. Multi-\nple studies have revealed the outlier phenomenon in weights\nand activations of transformer-based models, a tiny fixed set\nof embedding dimensions consistently exhibits large values\nacross multiple layers (Bondarenko, Nagel, and Blankevoort\n2021; Puccetti et al. 2022). Statistical analysis of LLMs\nshows that activations suffer from outliers several orders of\nmagnitude larger than weights (Xiao et al. 2023). As a result,\nunder the setting of uniform quantization, activations en-\ncounter much larger quantization loss. Some methods have\nbeen proposed to alleviate this problem, for example, by\nseparating the quantization of outlier dimensions and reg-\nular dimensions (Gong et al. 2023), or by partially migrat-\ning outlier effect from activations to weights (Xiao et al.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18083\n√ó3.00\n√ó1.78\n√ó0.80\n√ó10!\"\n√ó10!\"\n√ó10!\"\nHight-precisionTensorùêó QuantizedTensorùêó\" PerturbationofQuantization‚àÜ\n= largerthanmax|ùêó|\nequaltomax|ùêó|\nsmallerthanmax|ùêó|\nlargeperturbations\nsmallperturbations(mostly)\nmiddleperturbations\nScalingFactorùú∂\nmax|ùêó|\n0.510.21‚àí0.13‚àí0.16‚àí0.230.19‚àí0.140.530.170.28‚àí0.44‚àí0.34‚àí0.420.02‚àí0.221.78\n100‚àí1100101‚àí1‚àí1‚àí10 ‚àí1421‚àí1‚àí111‚àí1211‚àí2‚àí1‚àí20 ‚àí1742‚àí1‚àí122‚àí1512‚àí2‚àí3‚àí40 ‚àí216\t7\n+\n8.1421.013.016.0‚àí19.919.914.010.117.0‚àí14.9‚àí1.18.90.862.0 20.96.570.14‚àí4.4312.439.43‚àí2.43‚àí6.4311.432.14‚àí8.432.576.86‚àí8.578.86‚àí2.003.430.005.29‚àí1.86‚àí1.57‚àí4.570.14‚àí3.86‚àí2.57‚àí4.145.575.141.710.293.712.000.8698.00\nFigure 2: A toy example of 4-bit uniform quantization with different scale factors Œ±. The top line shows a choice of Œ± that is\ntoo large, leading to too much information lost in the quantization process. The middle represents the most common choice,\nwhere Œ± is set to the maximum value of the tensor. The bottom line uses a smaller Œ±, which results in less perturbation at the\ncost of clipping out-of-range values.\n2023) . Recently, some researchers have analyzed why out-\nliers appear and proposed a method to remove outliers and\nproduce quantization-friendly models (Bondarenko, Nagel,\nand Blankevoort 2023). While most existing works target\nproposing new quantization methods for LLMs, some focus\non empirical analysis of LLM quantization, among which\nYao et al. (2023) thoroughly compares the effect of differ-\nent quantization schemes, model families, and quantization\ncoverage, e.g.\nPreliminaries\nUniform Quantization\nQuantization is a technique that involves mapping high-\nprecision values into low-precision discrete values. This pro-\ncess reduces the memory and computational requirements\nof the model, making it more efficient to deploy on devices\nwith limited resources. Given a float-point tensor X (either\na weight tensor or activation tensor), the quantization and\nde-quantization process can be viewed as:\nXZ = Q(X), eX = Q‚àí1(XZ) (1)\nwhere XZ is the quantized tensor, andeX is the recovered ten-\nsor. In this paper, we target the setting of uniform quantiza-\ntion (Hubara et al. 2017), which is the most common quan-\ntization because of its efficient implementation and satisfac-\ntory performance. The quantization function Q(¬∑) of uni-\nform quantization is defined as a rounding-to-nearest opera-\ntion over the scaled input:\nQuni(X) = clip\n\u0012\u0016X\nŒ± ¬∑ 2b\n\u0019\n+ z; 0, 2b ‚àí 1\n\u0013\n(2)\nwhere b ‚àà N is the bit-width, Œ± ‚àà R is the scale factor, and\nz ‚àà N is zero-point. ‚åä¬∑‚åâ denotes the round-to-nearest-integer\noperator. The corresponding de-quantization function is as\nfollows:\nQ‚àí1\nuni (X) = (XZ ‚àí z) ¬∑ Œ±/2b (3)\nIn addition to the quantization strategy, the quality of\nquantization is also influenced by the bit-width and quan-\ntization granularity. The former is straightforward to com-\nprehend: the number of quantization bins increases expo-\nnentially with the bit-width. For example, 8-bit quantization\nhas 256 unique bins for eachX. The quantization granularity\ndetermines the size of X. Tensor-wise quantization involves\nquantizing the entire tensor. Channel-wise or group-wise\nquantization, on the other hand, divides the tensor along a\nspecific dimension and then quantizes each channel or group\nindependently. Fine-grained granularity typically leads to\nbetter performance.\nThe Lens of Perturbation\nWhile the quantized low-precision values are expected to be\nclose to their high-precision counterparts, they usually may\nnot be exactly equal due to the loss of precision. The extent\nof deviation from the high-precision values is influenced by\nthe bit-width and the quantization scheme used. One way to\nview quantization is as perturbations added to the original\nvalues, which can be expressed as:\nX = eX + ‚àÜ, (4)\nwhere ‚àÜ is the difference matrix between the quantized and\noriginal tensors, representing the noise introduced by the\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18084\nabsmax\nscaling factor\n0.0\n0.5\n1.0\n1.5\n2.0perturbation intensity\nwith clipping\nwithout clipping\nFigure 3: An illustration of the relationship between the\nscale factor and the intensity of perturbation. To calculate the\nperturbation intensity, we use the L2 norm of ‚àÜ, which rep-\nresents the distance between the high-precision tensor and\nthe quantized one.\nquantization process. We call this perspective as the lens of\nperturbation.\nThe Choice of the Scale Factor\nIn uniform quantization, the scale factor Œ± plays a critical\nrole as it determines the scaling of original values. Usually,\nthe scale factor is computed by optimizing the performance\nof a few calibration samples. However, in the zero-shot set-\nting, it is often set to the absolute maximum value, repre-\nsented Œ±max = max(|X|).\nFigure 2 illustrates how the choice of scale factor im-\npacts uniform quantization. If the scale factor is greater than\nmax(|X|), values are overscaled, resulting in an insufficient\nutilization of the range of integers. Conversely, if the scale\nfactor is smaller thanmax(|X|), quantized values exceed the\nrange so out-of-range values would be clipped. In the exam-\nple depicted in Figure 2, the maximum value of the input\ntensor Smaller scale factors result in smaller quantization\nintervals and thus slighter perturbations for in-range values.\nFigure 3 provides a rough relationship between the intensity\nof perturbation and the scale factor. Throughout this paper,\nwe use Œ±max as the scale factor for uniform quantization, un-\nless otherwise specified.\nUniform Quantization under the Lens of Perturbation\nBefore conducting experiments, we first examine uniform\nquantization through the lens of perturbation. For simplic-\nity, we initially disregard the clipping operation for out-of-\nrange values and later discuss how it affects quantization. In\nuniform quantization, high-precision values are mapped to a\nseries of equal interval bins. Through the lens of perturba-\ntion, we make the following observations: (1) the intensity\nof perturbation is positively correlated with the scale fac-\ntor (as depicted in Figure 3, the larger the scale factor, the\ngreater the intensity of perturbation); (2) due to the uniform\nintervals, the magnitude of perturbation is not dependent on\nthe magnitude of the values being quantized.\nLooking into LLM Quantization through the\nLens of Perturbation\nIn this section, we investigate the relationships between var-\nious types of perturbations and their impact on performance\ndegradation. Analyzing the statistical properties of pertur-\nbation can provide a more profound comprehension of the\neffects of quantization. Our goal is to determine which type\nof perturbation is more quantization-friendly (leading to less\nperformance degradation).\nWhat Kind of Perturbation Is More\nQuantization-Friendly?\nTo explore the correlation between perturbations and their\nimpact on performance, we propose to use manually con-\nstructed perturbations instead of relying on the natural per-\nturbations caused by quantization, which can be intricate and\ninconvenient to manipulate. Our approach involves control-\nling the properties of artificial perturbations and examining\nhow they influence the model‚Äôs behavior.\nOur approach constructs perturbations from two perspec-\ntives: the distribution of the perturbation (i.e., the type of\ndistribution from which the perturbation is sampled) and the\nmagnitude of the perturbation (i.e., whether the perturba-\ntion‚Äôs size is proportional to the input‚Äôs size). By manipu-\nlating these properties, we can enhance our comprehension\nof how perturbations impact the model‚Äôs performance. We\nconsider three types of distributions for our perturbations:\n‚Ä¢ Gaussian Perturbation‚àÜG ‚àº N(¬µ, œÉ2): where the per-\nturbations are sampled from a Gaussian distribution.\n‚Ä¢ Uniform Perturbation‚àÜU ‚àº U(‚àíc, c): where the per-\nturbations follow the uniform distribution between ‚àíc\nand c.\n‚Ä¢ Rademacher Perturbation‚àÜR ‚àº R: where the pertur-\nbations have constant sizes but random signs.\nApart from the distribution, we also take into account\nwhether the magnitude of perturbations should be linked\nto the magnitude of the input (i.e., whether larger val-\nues should have greater (or lesser) perturbations). In this\nsense, the three methods mentioned above are classified as\n‚Äúmagnitude-independent perturbation,‚Äù signifying that the\nperturbation‚Äôs magnitude is not contingent on the input‚Äôs\nmagnitude To investigate the impact of perturbation magni-\ntude, we introduce two ‚Äúmagnitude-aware perturbation‚Äù set-\ntings, in which the perturbation‚Äôs size is dynamically deter-\nmined by the value‚Äôs size.\n‚Ä¢ Positively-correlated Perturbation ‚àÜM+: the positive\ncorrelation setting scales the perturbation magnitude pos-\nitively with the input, meaning that higher weights have\nlarger perturbations, formally, Œ¥ ‚àù |x|.\n‚Ä¢ Negatively-correlated Perturbation ‚àÜM-: conversely,\nthe negative correlation setting scales the perturba-\ntion magnitude inversely with the input, meaning that\nhigher weights lead to smaller perturbations, formally,\nwe choose Œ¥ ‚àù (|x| + œµ)‚àí1.\nWe conducted experiments using different types of pertur-\nbations on the BLOOM-560M, BLOOM-3B, LLAMA-7B,\nand LLAMA-13B models. Due to the high computational\ncost, we did not experiment with more LLMs. Nevertheless,\nwe are confident that our current experiments provide suf-\nficient observations to draw reliable conclusions. To simu-\nlate the W8A8 quantization setting, we add perturbations to\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18085\nBLOOM-560m BLOOM-3b LLAMA-7b LLAMA-13b0.0\n0.2\n0.4\n0.6\n0.8\nNo perturbation\nUniform quantization\nRademacher perturbation\nUniform perturabtion\nGaussian perturabation\nPositively-correlated perturbation\nNegatively-correlated perturbation\nFigure 4: Lambada (Paperno et al. 2016) performance comparison on different types of perturbations. Here No perturbation\nrepresents the vanilla LLM (without quantization and without perturbation).Uniform quantization represents for W8A8 uniform\nquantization. For a fair comparison, all artificial perturbations are set to have the same variance with the native perturbation\ncaused by Uniform quantization. The perturbation methods are run four times with different random seeds.\nboth weights and activations in this experiment. To ensure\na fair comparison between artificial perturbations and quan-\ntization perturbations, we control the perturbation intensity\n(measured by the L2 norm of ‚àÜ) to be the same for each\nmatrix being quantized.\nObservations The results of perturbations are shown in\nFigure 4. As expected, perturbations lead to non-negligible\ndecreases in performance, regardless of the properties of the\nperturbations or the backbone models. The extent of perfor-\nmance degradation varies across different model families. In\nthe case of the BLOOM model family, artificial perturba-\ntions exhibit similar performance to standard uniform quan-\ntization, whereas LLAMA models experience a significant\nloss in performance. This contrast highlights the distinctive\ncharacteristics of different LLM model families, which war-\nrants further investigation. Regarding our primary objective\nof establishing a link between the properties of perturbations\nand performance degradation, we have made an interesting\ndiscovery. We have found that the sampling distribution has\nlittle impact on performance, as all three distributions we\ntested exhibit comparable performance. On the other hand,\nthe magnitude of perturbations plays a crucial role. Specif-\nically, the Positively-correlated perturbation ‚àÜM+ (where\nlarger values have larger perturbations) exhibits a dominant\nadvantage over all perturbations on all models, while the\nnegatively-correlated perturbation ‚àÜM+ (where smaller val-\nues have larger perturbations) almost completely destroys\nperformance. This phenomenon has led us to a significant\nfinding: larger values in LLM activations and weights are\nmore robust to perturbations, whereas smaller values are\nmore sensitive to perturbations.\nClipping as a Special Perturbation\nThe experiment conducted on artificial perturbations men-\ntioned above has not considered the effect of the clipping\noperation, which can have a considerable impact on realis-\ntic quantization scenarios, however. According to the for-\nmula for uniform quantization (Eq. 2), if Œ± is smaller than\nmax(|X|), out-of-range values will be clipped, and the de-\nviation caused by the clipping operation is much greater\nBLOOM\n-560m\nBLOOM\n-3b\nLLAMA\n-7b\nLLAMA\n-13b\n0.0\n0.2\n0.4\n0.6\n0.8Accuracy\nBLOOM\n-560m\nBLOOM\n-3b\nLLAMA\n-7b\nLLAMA\n-13b\n0\n20\n40\n60\n80\n100Perplexity\nNo perturbation\nClipping (k=10)\nClipping (k=5)\nClipping (k=3)\nFigure 5: Effects of clipping on Lambada dataset (left) and\nWikitext-2 (Merity et al. 2016) and C4 (Raffel et al. 2019)\n(averaged ppl.) (right). We compare three options of k,\nwhich determines the clipping threshold:k ‚àà {3,5, 10}. It is\nworth noting that even with the most stringent clipping set-\nting, where k = 3, on average, only less than 0.1% of values\nare affected by clipping.\nthan that caused by regular perturbations. Through the lens\nof perturbation, clipping can be viewed as an exceedingly\nlarge perturbation applied to out-of-range values. Our cur-\nrent finding shows that larger values can tolerate much\ngreater perturbations. Therefore, the question arises as to\nwhether a perturbation as large as clipping is acceptable in\nterms of model performance.\nTo discuss the impact of extreme perturbations caused by\nclipping, we design and experiment with this special pertur-\nbation:\n‚Ä¢ Clipping Perturbation‚àÜC: where large values exceed-\ning the pre-set threshold are clipped to the threshold. We\nset the clipping threshold as ¬µX + k ¬∑ œÉX.\nFigure 3 illustrates the effect of clipping perturbations on\nperformance. Overall, clipping has a considerably more ad-\nverse impact than regular perturbations. This finding aligns\nwith the outlier phenomenon, which posits that extremely\nlarge values are crucial for LLM performance. Clipping\nthese values results in significant information loss, thus lead-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18086\nQuant. Bits Bloom OPT Llama\nMethods (W/A) 560M 1.1B 3B 7.1B 350M 1.3B 6.7B 13B 30B 66B 7B 13B 30B 65B\nWiketext-2 (ppl‚Üì)\nfull 16/16 19.36 24.33 11.83 10.32 15.89 11.35 8.84 8.32 7.93 7.14 19.7 13.3 10.3 10.18\nUniform 4/16 24.19 29.97 13.41 11.45 15.95 11.34 8.86 8.30 7.93 8.65 25.05 14.98 11.78 11.42\nNon uniform 4/16 22.02 27.67 12.87 10.98 15.89 11.35 8.84 8.30 7.93 7.14 20.92 15.55 11.03 11.29\nUniform 8/8 26.14 36.88 14.98 15.64 18.28 12.2 18.83 3300 1328 2254 100.81 52.41 57.91 26.88\nNon uniform 8/8 19.78 24.42 11.90 10.36 15.95 11.51 8.86 8.34 7.93 7.38 22.28 16.14 12.62 15.13\nUniform 4/8 33.31 44.56 17.28 17.52 18.33 12.16 18.58 3352 1262 2379 453.05 75.81 711 1095\nNon uniform 4/8 22.67 27.72 12.91 11.03 15.92 11.49 8.86 8.32 7.93 7.38 21.02 15.58 11.07 12.01\nC4 (ppl‚Üì)\nfull 16/16 27.83 22.98 18.08 15.83 22.94 16.20 12.91 12.38 11.78 9.57 7.67 7.14 6.50 6.12\nUniform 4/16 34.22 27.41 20.56 17.52 22.98 16.20 12.95 12.38 11.78 10.25 9.12 7.94 7.09 6.83\nNon uniform 4/16 31.84 25.39 19.55 16.75 22.98 16.20 12.91 12.38 11.78 10.21 8.23 7.56 6.79 6.34\nUniform 8/8 34.84 35.81 21.59 20.64 26.88 17.94 98.88 6116 2790 4931 20.05 18.00 19.36 18.22\nNon uniform 8/8 28.33 23.08 18.14 15.89 22.98 16.42 12.95 12.40 11.78 10.21 7.68 7.16 6.52 6.13\nUniform 4/8 43.44 45.19 25.09 22.8 26.92 17.86 100.81 6116 2812 5191 39.09 28.94 30.39 28.05\nNon uniform 4/8 32.53 25.48 19.62 16.81 22.98 16.42 12.95 12.38 11.78 10.34 8.26 7.58 6.81 6.38\nLambada (accuracy‚Üë)\nfull 16/16 64.59 69.40. 73.98 77.39 67.57 75.35 81.25 80.71 81.99 83.01 88.58 89.42 90.31 90.82\nUniform 4/16 58.08 65.00 72.85 76.69 63.50 59.01 75.19 77.28 69.89 41.01 85.77 87.06 88.09 88.77\nNon uniform 4/16 61.88 68.27 71.68 76.96 64.37 70.79 79.22 79.44 80.55 81.63 87.08 88.64 89.98 90.29\nUniform 8/8 59.77 61.90 68.62 66.87 64.61 72.91 41.75 5.38 3.37 0.84 86.03 84.10 65.00 80.45\nNon uniform 8/8 64.70 68.99 73.77 77.39 67.65 74.76 80.94 80.47 81.84 82.49 88.52 89.46 90.37 90.32\nTable 1: Comparison of uniform and non-uniform method on three datasets. In addition to evaluating W4A16 and W8A8\nquantization, we also evaluate the much more challenging w4a8 quantization setting where weights are quantized to 4 bits and\nactivations to 8 bits. For this experiment, we have not processed the entire test set for Wikitext-2 and C4. Instead, we use the\nfirst 2000 samples to calculate perplexity.\n(a) Uniform\n(b) Quantization\n(b) Non-uniform\nQuantizationInput: X\nscaling\nrounding\nscaling-1\nOutput: X\"\nscaling\nnonlinear\nrounding\nscaling-1\nnonlinear -1\nInput: X\nOutput: X\"\nFigure 6: Illustration of uniform quantization and its non-\nuniform adaptation. The key difference is that non-uniform\nquantization utilizes two mutually inverse transformations to\nshape the input in a more quantization-friendly manner, one\nbefore the scaling operation and the other after the scaling-\nback operation.\ning to severe performance degradation. Another interest-\ning observation is the comparison between LLAMA and\nBLOOM, where models from the LLAMA family seem to\nbe more vulnerable to clipping.\nQuant. Methods Bits Wikitext-2 C4\nFull Precision 16/16 19.70 7.67\nNon-uniform (Ours) 4/16 20.92 8.23\nRTN (alias of Uniform Quant.) 4/16 25.05 9.12\nGPTQ (Frantar et al. 2022) 4/16 21.09 8.48\nAWQ (Lin et al. 2023) 4/16 20.77 8.03\nTable 2: Comparison of different methods on LLaMA-7b.\nImproving LLM Quantization\nHaving identified the preferred types of perturbations, we\ncan now explore how these properties can be leveraged\nto improve existing quantization methods. Specifically, we\nhave discovered that smaller values in a tensor should be\nsubject to smaller perturbations, while larger values can\nhandle greater perturbations, and that the magnitude of the\nperturbations should be constrained and clipping should be\navoided. These properties can be useful in designing a quan-\ntization method that takes them into account.\nTo this end, we have implemented a quantization method\nto verify the effectiveness of the properties we discovered.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18087\nMethod Humanities STEM Social Other Avg.\nSciences\nFull Prec. 34.80 35.12 30.29 38.07 37.95\nUniform 31.63 34.13 27.55 33.38 33.92\nNon-uniform 34.43 33.29 31.48 38.40 35.83\nTable 3: LLaMA-7B 4-shot ICL results on MMLU.\nOur goal is to ensure that smaller values in a tensor are sub-\nject to smaller perturbations, while larger values bear more\nperturbations. We achieve this by applying a nonlinear trans-\nformation before quantization, which amplifies smaller val-\nues around zero and shrinks larger values. The approach\nis straightforward: perform a nonlinear transformation be-\nfore quantization, and inverse transform it back after quan-\ntization, as illustrated in Figure 6. For simplicity, we select\nf(x) = x1/3 as the nonlinear transformation as it effectively\namplifies values near zero and shrinks larger values.\nWe conducted experiments on three model families:\nBLOOM, OPT, and LLAMA. Table 1 summarizes results on\nLambada, Wikitext-2, and C4. In the W4A16 setting, where\nactivations are not quantized, uniform quantization performs\nwell on most of the models and tasks, suggesting that weight\nquantization is not overly challenging. However, when it\ncomes to quantizing activations, uniform quantization ex-\nhibits weaknesses, while the non-uniform method maintains\nnear-lossless performance. We observe that for OPT models,\nas the model scales up, standard min-max uniform quanti-\nzation struggles to perform accurately with explosive per-\nplexity scores, whereas the non-uniform method maintains\nthe model‚Äôs performance close to that of the unquantized\nmodel. Another interesting finding is that for non-uniform\nquantization, as the model size scales up, the performance\ngap between before and after quantization decreases, indi-\ncating that larger models are easier to quantize. This phe-\nnomenon contrasts with observations made in uniform quan-\ntization, where model performance deteriorates as the model\nsize grows.\nWe conducted more experiments on LLaMA-7b. Table 2\nshows the comparison of non-uniform quantization and\nsome recent post-training quantization methods and Table 3\nis the performance on MMLU (Hendrycks et al. 2021), an\nadvanced benchmark for evaluating the abilities of LLMs.\nLimitations and Practical ValueQuantization offers two\nsources of efficiency improvements: reduced memory costs\nthrough the use of low-precision formats, which acceler-\nates data transfer in Memory/GPUs (memory efficiency),\nand faster matrix multiplication of low-precision formats\n(computation efficiency). However, the latter advantage is\nonly realized when both the weight and activation are quan-\ntized to low precision. In the commonly used W4A16 set-\nting, matrix calculation is still performed with high pre-\ncision. Despite non-uniform quantization being a domi-\nnant approach in signal processing, it has not received\nsufficient attention in the realm of neural network com-\npression. One major obstacle to its application in model\ncompression is efficiency concerns. Although our non-\nuniform quantization method demonstrates reliable perfor-\nmance, it is important to note that the introduction of non-\nlinear transformation entails additional computations. Con-\nsequently, non-uniform quantization only offers memory ef-\nficiency without computation efficiency. However, as current\nLLMs are not computation-constrained but rather memory-\nconstrained, the model wastes a significant amount of time\nwaiting for variables to be loaded on specific hardware rather\nthan performing calculations. This imbalance of memory\ncapacity and computation is particularly outstanding when\ngenerating long text, as the loading/offloading of KV-Cache\nfor each generated token significantly slows down inference.\nThis scenario is where most of the current quantization ap-\nproaches glow.\nFurther Insights Through our experimentation with the\nnon-uniform quantization method, which amplifies smaller\nvalues and compresses larger ones, we have confirmed a\ngeneral principle regarding model quantization. Specifically,\nlarger values can tolerate more error, while smaller values\nare more sensitive to larger errors. Based on this observa-\ntion, we have identified the limitations of uniform quantiza-\ntion, which employs uniformly distributed quantization in-\ntervals to scale all values of varying sizes. We propose that\nis preferable to establish dense bins for small values and\nsparser bins for large values. We believe that this finding is\nhighly thought-provoking and warrants further exploration\nin future research on model quantization. There are two po-\ntential avenues to pursue: one is to implement hardware-\nfriendly non-uniform quantization for efficient deployment,\nwhile the other entails shaping the weight and activation dis-\ntributions through training to make them easier to quantize.\nSome prior research has already been conducted in this di-\nrection (Bondarenko, Nagel, and Blankevoort 2023).\nConclusion\nOur work introduces a new perspective on quantization,\nwhich we refer to as ‚Äúthe lens of perturbation‚Äù. Using this\napproach, we conduct a comprehensive investigation of uni-\nform quantization on LLMs, evaluating the performance of\nvarious models under different quantization settings. We\nalso probe the models with manually constructed perturba-\ntions, which provide valuable insights into how quantiza-\ntion impacts the model performance. These insights help us\nunderstand the difficulties associated with quantization for\nLLMs. Based on our findings, we propose a non-uniform\nquantization approach that significantly reduces the perfor-\nmance degradation caused by quantization. We hope that our\nstudy, conducted through the lens of perturbation, will con-\ntribute to a better understanding of the challenges associated\nwith quantization for LLMs and inspire the development of\nmore efficient and effective quantization methods.\nAcknowledgments\nThis work is supported by National Key R&D Program of\nChina (No. 2022YFC3301900) and National Natural Sci-\nence Foundation of China (NSFC Grant No. 62122089). We\nsincerely thank all reviewers for their valuable comments\nand suggestions, which are crucial for improving our work.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18088\nReferences\nBondarenko, Y .; Nagel, M.; and Blankevoort, T. 2021. Un-\nderstanding and Overcoming the Challenges of Efficient\nTransformer Quantization. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, 7947‚Äì7969. Online and Punta Cana, Dominican Re-\npublic: Association for Computational Linguistics.\nBondarenko, Y .; Nagel, M.; and Blankevoort, T. 2023.\nQuantizable Transformers: Removing Outliers by Help-\ning Attention Heads Do Nothing. arXiv preprint\narXiv:2306.12929.\nDettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,\nL. 2023. Qlora: Efficient finetuning of quantized llms.arXiv\npreprint arXiv:2305.14314.\nDettmers, T.; and Zettlemoyer, L. 2023. The case for 4-\nbit precision: k-bit inference scaling laws. In International\nConference on Machine Learning, 7750‚Äì7774. PMLR.\nFrantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh, D. 2022.\nGptq: Accurate post-training quantization for generative\npre-trained transformers. arXiv preprint arXiv:2210.17323.\nGholami, A.; Kim, S.; Dong, Z.; Yao, Z.; Mahoney, M. W.;\nand Keutzer, K. 2021. A survey of quantization meth-\nods for efficient neural network inference. arXiv preprint\narXiv:2103.13630.\nGong, Z.; Liu, J.; Wang, Q.; Yang, Y .; Wang, J.; Wu, W.;\nXian, Y .; Zhao, D.; and Yan, R. 2023. PreQuant: A Task-\nagnostic Quantization Approach for Pre-trained Language\nModels. In Findings of the Association for Computational\nLinguistics: ACL 2023, 8065‚Äì8079. Toronto, Canada: Asso-\nciation for Computational Linguistics.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Mul-\ntitask Language Understanding. In 9th International Con-\nference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nHubara, I.; Courbariaux, M.; Soudry, D.; El-Yaniv, R.; and\nBengio, Y . 2017. Quantized neural networks: Training neu-\nral networks with low precision weights and activations.The\nJournal of Machine Learning Research, 18(1): 6869‚Äì6898.\nKim, S.; Gholami, A.; Yao, Z.; Mahoney, M. W.; and\nKeutzer, K. 2021. I-BERT: Integer-only BERT Quantiza-\ntion. In Meila, M.; and Zhang, T., eds., Proceedings of the\n38th International Conference on Machine Learning, vol-\nume 139 of Proceedings of Machine Learning Research,\n5506‚Äì5518. PMLR.\nLin, J.; Tang, J.; Tang, H.; Yang, S.; Dang, X.; and Han,\nS. 2023. AWQ: Activation-aware Weight Quantization\nfor LLM Compression and Acceleration. arXiv preprint\narXiv:2306.00978.\nLiu, Z.; Oguz, B.; Zhao, C.; Chang, E.; Stock, P.; Mehdad,\nY .; Shi, Y .; Krishnamoorthi, R.; and Chandra, V . 2023.\nLLM-QAT: Data-Free Quantization Aware Training for\nLarge Language Models. arXiv preprint arXiv:2305.17888.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016.\nPointer Sentinel Mixture Models. arXiv:1609.07843.\nOpenAI. 2022. OpenAI chatgpt. https://openai.com/blog/\nchatgpt. Accessed: 2024-02-28.\nPaperno, D.; Kruszewski, G.; Lazaridou, A.; Pham, N. Q.;\nBernardi, R.; Pezzelle, S.; Baroni, M.; Boleda, G.; and Fer-\nnandez, R. 2016. The LAMBADA dataset: Word prediction\nrequiring a broad discourse context. In Proceedings of the\n54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 1525‚Äì1534. Berlin,\nGermany: Association for Computational Linguistics.\nPuccetti, G.; Rogers, A.; Drozd, A.; and Dell‚ÄôOrletta, F.\n2022. Outlier Dimensions that Disrupt Transformers are\nDriven by Frequency. In Findings of the Association for\nComputational Linguistics: EMNLP 2022, 1286‚Äì1304.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. arXiv e-prints.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili¬¥c, S.; Hesslow,\nD.; Castagn¬¥e, R.; Luccioni, A. S.; Yvon, F.; Gall¬¥e, M.; et al.\n2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100.\nShen, S.; Dong, Z.; Ye, J.; Ma, L.; Yao, Z.; Gholami, A.; Ma-\nhoney, M. W.; and Keutzer, K. 2020. Q-bert: Hessian based\nultra low precision quantization of bert. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 34,\n8815‚Äì8821.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nWu, X.; Yao, Z.; and He, Y . 2023. ZeroQuant-FP: A Leap\nForward in LLMs Post-Training W4A8 Quantization Using\nFloating-Point Formats. arXiv preprint arXiv:2307.09782.\nWu, X.; Yao, Z.; Zhang, M.; Li, C.; and He, Y . 2022. Ex-\ntreme Compression for Pre-trained Transformers Made Sim-\nple and Efficient. arXiv preprint arXiv:2206.01859.\nXiao, G.; Lin, J.; Seznec, M.; Wu, H.; Demouth, J.; and Han,\nS. 2023. Smoothquant: Accurate and efficient post-training\nquantization for large language models. In International\nConference on Machine Learning, 38087‚Äì38099. PMLR.\nYao, Z.; Wu, X.; Li, C.; Youn, S.; and He, Y . 2023.\nZeroQuant-V2: Exploring Post-training Quantization in\nLLMs from Comprehensive Study to Low Rank Compen-\nsation. arXiv:2303.08302.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022.\nOpt: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18089",
  "topic": "Through-the-lens metering",
  "concepts": [
    {
      "name": "Through-the-lens metering",
      "score": 0.6162628531455994
    },
    {
      "name": "Perturbation (astronomy)",
      "score": 0.5457075238227844
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.4540886878967285
    },
    {
      "name": "Empirical research",
      "score": 0.42035675048828125
    },
    {
      "name": "Computer science",
      "score": 0.4053250551223755
    },
    {
      "name": "Theoretical physics",
      "score": 0.39725568890571594
    },
    {
      "name": "Statistical physics",
      "score": 0.376850426197052
    },
    {
      "name": "Lens (geology)",
      "score": 0.3093906044960022
    },
    {
      "name": "Physics",
      "score": 0.2868722677230835
    },
    {
      "name": "Mathematics",
      "score": 0.27088743448257446
    },
    {
      "name": "Algorithm",
      "score": 0.20922845602035522
    },
    {
      "name": "Optics",
      "score": 0.16937488317489624
    },
    {
      "name": "Quantum mechanics",
      "score": 0.1312490701675415
    },
    {
      "name": "Statistics",
      "score": 0.12640804052352905
    }
  ]
}