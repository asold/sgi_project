{
  "title": "Transmol: repurposing a language model for molecular generation",
  "url": "https://openalex.org/W3156506984",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4288526482",
      "name": "Zhumagambetov, Rustam",
      "affiliations": [
        "Nazarbayev University"
      ]
    },
    {
      "id": "https://openalex.org/A4292957181",
      "name": "Molnár, Ferdinand",
      "affiliations": [
        "Nazarbayev University"
      ]
    },
    {
      "id": null,
      "name": "Peshkov, Vsevolod A.",
      "affiliations": [
        "Nazarbayev University"
      ]
    },
    {
      "id": "https://openalex.org/A2744583913",
      "name": "Fazli Siamac",
      "affiliations": [
        "Nazarbayev University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1992156271",
    "https://openalex.org/W3042995081",
    "https://openalex.org/W2992586577",
    "https://openalex.org/W2792130717",
    "https://openalex.org/W3001449808",
    "https://openalex.org/W3028032948",
    "https://openalex.org/W3043461363",
    "https://openalex.org/W3046692715",
    "https://openalex.org/W2726184500",
    "https://openalex.org/W2578119541",
    "https://openalex.org/W2009605433",
    "https://openalex.org/W1509196567",
    "https://openalex.org/W1986176168",
    "https://openalex.org/W2004797093",
    "https://openalex.org/W2790808809",
    "https://openalex.org/W2895420596",
    "https://openalex.org/W2052933929",
    "https://openalex.org/W2037825667",
    "https://openalex.org/W2512660405",
    "https://openalex.org/W2945551948",
    "https://openalex.org/W3031603244",
    "https://openalex.org/W1531674615",
    "https://openalex.org/W1998260904",
    "https://openalex.org/W2883583109",
    "https://openalex.org/W4252520800",
    "https://openalex.org/W2795631711",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W2963609389",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2798613236",
    "https://openalex.org/W3103098434",
    "https://openalex.org/W2952832141",
    "https://openalex.org/W3010523193",
    "https://openalex.org/W2793945656",
    "https://openalex.org/W2805002767",
    "https://openalex.org/W2998571806",
    "https://openalex.org/W2914635984",
    "https://openalex.org/W2999847078",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2765224015",
    "https://openalex.org/W3117935027",
    "https://openalex.org/W2989615256",
    "https://openalex.org/W2887447356",
    "https://openalex.org/W2567534979",
    "https://openalex.org/W2963445908",
    "https://openalex.org/W2891868449",
    "https://openalex.org/W2991736596",
    "https://openalex.org/W2022476850",
    "https://openalex.org/W2035986671",
    "https://openalex.org/W1978131786",
    "https://openalex.org/W1757990252",
    "https://openalex.org/W4248107770",
    "https://openalex.org/W2052907531",
    "https://openalex.org/W2105649494",
    "https://openalex.org/W1996100382",
    "https://openalex.org/W1608885177",
    "https://openalex.org/W4247924304",
    "https://openalex.org/W2074696981",
    "https://openalex.org/W3106536628",
    "https://openalex.org/W2915480215",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W3098254076",
    "https://openalex.org/W3100545487",
    "https://openalex.org/W2912083425",
    "https://openalex.org/W3021439445",
    "https://openalex.org/W2135732933",
    "https://openalex.org/W4300931031",
    "https://openalex.org/W3116865743",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W2963676163",
    "https://openalex.org/W2972741532"
  ],
  "abstract": "A novel molecular generation pipeline employing an attention-based neural network.",
  "full_text": "Transmol: repurposing a language model for\nmolecular generation†\nRustam Zhumagambetov,\n a Ferdinand Moln´ar,\n b Vsevolod A. Peshkov\n *c\nand Siamac Fazli\n *a\nRecent advances in convolutional neural networks have inspired the application of deep learning to other\ndisciplines. Even though image processing and natural language processing have turned out to be the most\nsuccessful, there are many other domains that have also beneﬁted; among them, life sciences in general\nand chemistry and drug design in particular. In concordance with this observation, from 2018 the\nscientiﬁc community has seen a surge of methodologies related to the generation of diverse molecular\nlibraries using machine learning. However to date, attention mechanisms have not been employed for\nthe problem ofde novomolecular generation. Here we employ a variant of transformers, an architecture\nrecently developed for natural language processing, for this purpose. Our results indicate that the\nadapted Transmol model is indeed applicable for the task of generating molecular libraries and leads to\nstatistically signiﬁcant increases in some of the core metrics of the MOSES benchmark. The presented\nmodel can be tuned to either input-guided or diversity-driven generation modes by applying a standard\none-seed and a novel two-seed approach, respectively. Accordingly, the one-seed approach is best\nsuited for the targeted generation of focused libraries composed of close analogues of the seed\nstructure, while the two-seeds approach allows us to dive deeper into under-explored regions of the\nchemical space by attempting to generate the molecules that resemble both seeds. To gain more\ninsights about the scope of the one-seed approach, we devised a new validation workﬂow that involves\nthe recreation of known ligands for an important biological target vitamin D receptor. To further beneﬁt\nthe chemical community, the Transmol algorithm has been incorporated into our cheML.io web\ndatabase of ML-generated molecules as a second generation on-demand methodology.\n1 Introduction\nChemistry is frequently referred to as a“central science” for its\nkey role in advancing technological progress and human well-\nbeing through the design and synthesis of novel molecules\nand materials for energy, environmental, and biomedical\napplications.\nMedicinal chemistry is a highly interdisciplinary eld of\nscience that deals with the design, chemical synthesis, and\nmechanism of action of biologically active molecules as well as\ntheir development into marketed pharmaceutical agents (i.e.\ndrugs). The creation of new drugs is an incredibly hard and\narduous process, one of the key reasons being the fact that the\n‘chemical space’ of all possible molecules is extremely large and\nintractable. On account of estimates that the chemical space of\nmolecules with pharmacological properties is in the range of\n10\n23 to 1060 compounds,1 this order of magnitude leaves the\nwork ofnding new drugs outside the reach of manual labor.\nTo cure a particular disease, medicinal chemists need to\ndetermine molecules that are active and selective towards\nspecic biological targets while keeping the risks of negative\nside eﬀects minimal.2–5 As the number of molecules that require\ntesting to identify an ideal drug candidate constantly increases,\nit raises the overall cost of the drug discovery process.\n6 There-\nfore, the need for algorithms that are able to narrow down and\nstreamline these e ﬀorts has recently emerged. Speci cally,\ncomputer algorithms can assist with creating new virtual\nmolecules,\n7,8 performing molecule conformation analysis9,10 as\nwell as molecular docking11,12 to determine the aﬃnity of novel\nand known molecules towards specic biological targets.\nWith respect to molecular generation, the conventional non-\nneural algorithms heavily rely on external expert knowledge to\ndesign candidate molecules. In this context, expert knowledge\nmay consist of molecular fragments that can be“mixed and\nmatched” to produce a set of novel molecules.\n13–18 However, the\nresulting molecules might be di ﬃcult to synthesize.\naDepartment of Computer Science, School of Engineering and Digital Sciences,\nNazarbayev University, Nur-Sultan, Kazakhstan. E-mail: siamac.fazli@nu.edu.kz\nbDepartment of Biology, School of Sciences and Humanities, Nazarbayev University,\nNur-Sultan, Kazakhstan\ncDepartment of Chemistry, School of Sciences and Humanities, Nazarbayev University,\nNur-Sultan, Kazakhstan. E-mail: vsevolod.peshkov@nu.edu.kz\n† Electronic supplementary information (ESI) available. See DOI:\n10.1039/d1ra03086h\nCite this:RSC Adv.,2 0 2 1 ,11,2 5 9 2 1\nReceived 20th April 2021\nAccepted 22nd July 2021\nDOI: 10.1039/d1ra03086h\nrsc.li/rsc-advances\n© 2021 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 | 25921\nRSC Advances\nPAPER\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nConsequently, another type of expert knowledge i.e. known\nchemical reactions can then be added.19,20 In this way the“mix\nand match” approach will be constrained with a specics e to f\nrules to maximize the chances that any molecule that is produced\ncan be synthesized.\n21 The obvious drawback of relying on external\nknowledge is that it may restrict access to unknown and/or not\nyet populated regions of chemical space.\nAn alternative approach to this problem are neural algo-\nrithms that are inherently data-driven. This means that such\nalgorithms do not rely on expert knowledge and hence derive\ninsights from the data itself. Such approaches can be applied in\nsupervised and unsupervised settings.\n22–24 Supervised algo-\nrithms use arti cial neural networks for the prediction of\nmolecular properties25,26 or reaction outputs.27 Most unsuper-\nvised algorithms are aimed at molecular generation and drug\ndesign.\n8,22,28–48\nWhen automating molecular search, a natural question that\narises is how molecules, a physical collection of atoms that are\narranged in 3D space, can be represented. In the late 1980s, the\nsimplied molecular input line-entry system (SMILES)\nemerged, which aims to create a molecular encoding that is\ncomputationally eﬃcient to use and human readable.\n49 The\noriginal encoding is based on 2D molecular graphs. Intended\napplication areas are fast and compact information retrieval\nand storage. With the rise of machine learning algorithms in\nchemistry, the SMILES representation has been widely adopted\nby researchers for chemical space modeling tasks.\nThe task of generating more SMILES strings having an input\nstring can be viewed as a language modelling task. The model\ntakes an arbitrary length SMILES string (seed molecule) and\nreturns arbitrary long SMILES strings– the general class of such\nmodels is calledSeq2Seq. First used by Google,\n50 it is currently\none of the most popular approaches for machine translation.\nThe Seq2Seq architecture can be summarized as follows: an\nencoder produces a context vector that is later passed to the\ndecoder for the generation in auto-regressive fashion.\n50\nAs the building blocks of the encoder and decoder could be\nneural networks with various architectures, researchers have\nexperimented with a number of techniques: generative adver-\nsarial networks,\n30,38,44,45,51 variational autoencoders, 39,52 and\nrecurrent neural networks.53,54 However, the use of attention\nmechanisms has, to the best of our knowledge, so far been le\nunexplored. Here we aim toll this gap and investigate the\napplicability of attention to molecular generation.\nOne of the distinctive features of attention is that, unlike for\nexample RNN-type models which have a xed amount of\nmemory, attention mechanisms allow varying the length of the\nlatent vector so that the input information does not have to be\ncompressed into axed-length vector. In other words, attention\nwill provide a varying length vector that contains the related\ninformation regarding all input nodes (i.e. SMILES characters,\natoms, rings and branching info, etc.). Thus, in case of the\nstandard one-seed approach the outcome strives to replicate the\ninput, which means that upon injecting some variability into\nthe model the algorithm can be tuned to attain a focused library\nof close-seed analogues. In order to switch from this genuinely\ninput-guided mode to a more diversity-driven mode, we have\ncome up with the idea of a two-seed approach. This novel\napproach is not only capable of generating diversied molecular\nlibraries by assembling structures that resemble both seeds, but\nit also provides a Euclidean navigation framework of the latent\nspace and can thus lead to further insights during the explo-\nration of chemical space. The resulting Transmol algorithm\ncomprising the two above described generation modes has been\nincorporated into the cheml.io\n55 website with the aim to make\nthese ndings accessible to a wider audience.\n2 Dataset\nIn this paper, we have used the MOSES benchmark56 along with\nthe molecular datasets it provides. Overall, the MOSES bench-\nmark contains three datasets: training set, test set, and scaﬀold\ntest set, which consist of 1.6M, 176k, and 176k samples,\nrespectively.\nThe rst dataset was used to train the model. During this\nstage the model learns to interpolate between each molecule\nand constructs a latent space. This latent space acts as a proxy\ndistribution for molecules, and can be employed to sample new\nmolecules.\nThe testing dataset consists of molecules that are not present\nin the training data. It is used to evaluate the output of the\ngenerative model.\nThe scaﬀold testing set consists of scaﬀolds that are not\npresent in the training and testing datasets. This dataset is used\nto check if the model is able to generate new scaﬀolds, i.e.\nunique molecular features, or whether the model simply reuses\nparts of the previously seen molecules to generate new ones.\n2.1 Data augmentation\nIn order to increase the variability of our data, we have used data\naugmentation by means of SMILES enumeration. This simple,\nyet eﬀective technique has recently been introduced by Ar´us-\nPous et al.:\n57 While a molecule can be mapped to its unique\ncanonical SMILES string, a whole set of non-unique SMILES\nstrings can also be produced depending on the starting point\nwhere the algorithm will begin the translation. Such data\naugmentation have been reported to improve the generalization\nof the latent space and to increase the diversity of the output\nmolecules.\n3 Methods\nWhile the Seq2Seq architecture is successful in a whole range of\napplications, the disadvantage of this architecture is that it uses\nxed length vectors: it is bound (bottlenecked) by the size of the\nvector and sub-optimal for long sequences.\nFor this work, we have employed a vanilla transformer model\nintroduced by Vaswaniet al.\n58 (see Fig. 1 for a pictorial repre-\nsentation of the architecture). All code for this study has been\nwritten in Python and is publicly available on GitLab.59 The\ninitial transformer code was adapted from the annotated\nversion.\n60 A vanilla transformer consists of two parts: an\nencoder and a decoder. The encoder (see le dashed block of\n25922 | RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 © 2021 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nFig. 1) maps input to the latent representationz. The decoder\n(see dashed block on the right of Fig. 1), acceptsz as an input\nand produces one symbol at a time. The model is auto-\nregressive, i.e. to produce a new symbol it requires previous\noutput as an additional input.\nThe notable attribute of this architecture is the use of\nattention mechanisms throughout the whole model. While\nearlier models have been using attention only as an auxiliary\nlayer, having some kind of recurrent neural networks (RNN) like\na gated recurrent unit (GRU) or long short-term memory\n(LSTM), or a convolutional neural network (CNN), the trans-\nformer consists primarily of attention layers.\nThe attention mechanism can be seen as a function of query\nQ, keyK and valueV, where the output is a matrix product ofQ,\nK, V using the following function:\nScaled dot-product AttentionðQ; K; VÞ¼ softmax\n/C18\nQKT\nﬃﬃﬃﬃﬃdk\np\n/C19\nV\n(1)\nis used to identify the relevant parts of the input with respect to\nthe input, namely self-attention. The scaled dot-product atten-\ntion consists of 3 main matricesQ, K and V. The transformer\ncomputes these 3 matrices in the embedding layers. All 3\nmatrices are notxed and are learnable. Aer embedding the\nlayers will receive the output of the previous layers. The dot\nproduct is computed between Q and all keys to compute\na “relevance” score. A higher“relevance” score means that aer\nnormalizing\n/C18\nmultiplying by\n1ﬃﬃﬃﬃﬃdk\np\n/C19\nand applying the somax,\nthe lowest scores will become zeros. The important feature of\nthe self-attention used in this model is that it does not usexed\nlength vectors. While Seq2Seq models with RNN54 attempt to\ncompress all information into a xed-length context, self-\nattention mechanisms employed in this model have access to\nthe whole context and able to selectively focus only on the most\nimportant parts.Vice versa, it allows to disregard less important\nparts of the query and tolter the noise. Most importantly,\nattention mechanisms are diﬀerentiable and hence can be\nlearned from data as can be seen from eqn (1) for a description\nof the scaled dot-product attention layer. The multi-head\nattention layer consists of h instances of scaled dot-product\nattention layers that are then concatenated and passed to the\ndense layer.\nThe parameter setup of the original paper has been\nemployed here,i.e. the number of stacked encoder and decoder\nlayers isN ¼ 6, all sublayers produce an output ofd\nmodel ¼ 512,\nwith the dimensionality of the inner feed-forward layer being\ndﬀ, number of attention headsh ¼ 6, and dropoutd ¼ 0.1. The\nlearning rate is specied by the optimization function of the\noriginal manuscript with the following modi ed hyper-\nparameters: warmup steps are set to 2000, factor is 1, the batch\nsize is 1200 and the training was performed for 16 epochs. The\nrest of the hyperparameters were adopted unchanged from the\noriginal paper, i.e. we have not optimized the other hyper-\nparameters leaving them as in the original paper. One of the\nreasons are the large number of hyperparameters and limited\ncomputing resources. Please note that further hyperparameter\noptimization may thus improve the performance of the model.\n3.1 Sampling from the latent space\nTo sample a novel molecule from the model, a seed SMILES\nstring is needed in order to provide a context for the decoder.\nThen the decoding process is started by supplying a special\nstarting symbol. Aer that the decoder provides an output and\na rst symbol is generated. In order to obtain the next symbol\nthe previous characters are provided to the decoder. The\ndecoding process stops when the decoder either outputs\na special terminal symbol or exceeds the maximum length.\nThere are several techniques available that specify how the\noutput of the decoder is converted to the SMILES character such\nas a simple greedy search or a beam search, among others.\n3.1.1 Greedy search. Since the decoder provides output\nprobabilities, a na¨ıve approach would be to use a greedy algo-\nrithm by always picking the symbol with the highest probability.\nHowever, this is not optimal as picking the most probable\nsymbol at each step leads for a relatively small library. In\naddition, the greedy search does not guarantee that thenal\nresulting string has the highest conditional probability. More-\nover, unless stochastic sampling is used (here a probability\nvector is used as the basis for the distribution and then\nsampled), the result of the greedy search is deterministic and\ncorresponds to the “reconstruction accuracy” based on our\ntraining procedure.\n3.1.2 Beam search. To improve upon the greedy search,\na beam search has been employed as an alternative. The beam\nsearch can be seen as an improved greedy search. While the\nFig. 1 A vanilla transformer architecture.\n© 2021 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 | 25923\nPaper RSC Advances\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\ngreedy search picks only one symbol at a time, the beam search\npicks the N most probable ones. Fig. 2 illustrates the beam\nsearch with a beam widthN ¼ 3; the stroke width indicates the\nprobability. To guide the selection process of the beam search\nwe have used the following reward function:\nP\nchar ˛ vocab\nPðsjprevious outputÞ\nð1 þ jprevious outputjÞa\nwhere char is a possible symbol for the beam search to pick,\nvocab is the set of all possible characters, previous output is an\nordered list of symbols picked by the beam search prior to\npicking the current one anda is a parameter of the beam search\nthat regulates the length of the string. Lowa discourages long\nstrings while higha encourages long strings.\n3.2 Injecting variability into model\nTo explore the molecules that are located near the seed mole-\ncule in the latent space, we have used two techniques that allow\nus to sample from the seed cluster: addition of Gaussian noise\nto z and the use of temperature.\n3.2.1 Gaussian noise. To increase the variability of the\nmodel we are adding Gaussian noise with a mean ofm and\nstandard deviations to the latent vectorz before it is fed to the\ndecoder. In our study, parameters form and s were 1 and 1.06\nrespectively.\n3.2.2 Temperature. Another technique to improve vari-\nability is to apply temperature to the output vector right before\napplying the somax function. TemperatureT is a value from\n0t o N.A s T / N all characters have the same probability of\nbeing the next symbol. ForT / 0 the most probable symbol has\na higher probability of being selected. The resulting smoothed\ndistribution increases the variability of the sampling. In our\nstudy the parameter for temperature wasxed as 0.5.\n3.3 MOSES benchmark\nMOSES is an established benchmarking system for the evalua-\ntion of molecular generation algorithms.\n56 It contains several\nimplemented molecular generation frameworks as well as\na range of metrics to compare molecular outputs. The baseline\nmodels can be roughly divided into two categories: neural and\nnon-neural. Neural methods use articial neural networks to\nlearn the distribution of the training set. A whole range of them\nare implemented, namely character-level recurrent neural\nnetwork (CharRNN),\n35,61 variation autoencoder (VAE), 32,62,63\nadversarial autoencoder (AAE),62,64 junction tree VAE (JT-VAE),36\nand latent vector based generative adversarial network\n(LatentGAN).65 Non-neural baselines include the n-gram\ngenerative model (NGram), the hidden Markov model (HMM),\nand a combinatorial generator. Non-neural baselines are\nconceptually simpler than neural ones. The NGram model\ncollects the frequency of the n-grams in the training dataset and\nuses the resulting distribution to sample new strings. For\ninstance, during counting of a 2 gram, the model will inspect\nindividual SMILES strings and record their statistics. For the\nstring “C1CCC1C” the following statistics will be gathered C1:2,\nCC:2, 1C:2. Later this type of information will be normalized\nand used for sampling. HMM uses the Baum– Welch algorithm\nfor distribution learning. The combinatorial generator uses\nBRICS fragments of the training dataset. These fragments are\nthe substructures of molecules that are cut according to a set of\nrules for the breaking of retrosynthetically interesting chemical\nsubstructures (BRICS).\n66 In order to sample, combinatorial\ngenerator randomly connects several such fragments.\nSeveral metrics are provided by the MOSES benchmark.\nUniqueness shows the proportion of generated molecules that\nare within the training dataset. Validity describes the propor-\ntion of generated molecules that are chemically sound, as\nchecked by RDKit.\n67 Internal diversity measures whether the\nmodel samples from the same region of chemical space,i.e.\nproducing molecules that are valid and unique but only diﬀer in\na single atom. Filters measures the proportion of a generated set\nthat passes a number of medicallters. Since the training set\ncontains only molecules that pass through theselters, it is an\nimplicit constraint imposed on the algorithm.\nFig. 2 Overview of the beam search with a beam width ofN ¼ 3.\n25924 | RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 © 2021 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nFragment similarity (Frag) measures the similarity of the\nBRICS fragments distribution contained in the reference and\ngenerated sets. If the value is 1, then all fragments from the\nreference set are present in the generated one. If the value is 0,\nthen there are no overlapping fragments between the generated\nand reference sets. Scaﬀold similarity (Scaﬀ) is similar to Frag,\nbut instead of BRICS fragments, Bemis– Murcko scaﬀolds are\nused for comparison. The range of this metric is analogous to\nFrag. Similarity to the nearest neighbor (SNN) is a mean Tani-\nmoto distance between a molecule in the reference set and its\nclosest neighbor from the generated set. One of the possible\ninterpretations of this metric is precision; if the value is low, it\nmeans that the algorithm generates molecules that are distant\nfrom the molecules in the reference set. The range of this metric\nis [0, 1]. Fr´echet ChemNet Distance (FCD) is a metric that\ncorrelates with internal diversity and uniqueness. It is\ncomputed using the penultimate layer of the ChemNet, a deep\nneural network that is trained for the prediction of biological\nactivities. All four aforementioned metrics have also been\ncompared to the scaﬀold test dataset.\n3.4 A work ow for Transmol validationvia the recreation of\nknown vitamin D receptor ligands\nIn order to demonstrate the viability of our model we have\ndesigned a small retrospective study that is aiming at the\nrecreation of known ligands of the Vitamin D Receptor (VDR).\nVDR is a member of the nuclear receptor superfamily, a zinc-\nnger transcriptional factor and a canonical receptor for its\nmost active secosteroid 1 a,25-dihydroxyvitamin D\n3 (1,25D3)\n(Fig. 3A). It has also been established as sensor for steroid acids\nsuch as lithocholic acid (Fig. 3B) predominantly acting as an\nactivator of the detoxication pathway for them in the human\ncolon through regulation of cytochrome P450 monooxygenases\ne.g. CYP3A4. The classical VDR physiology involves the activa-\ntion and regulation of divers processes such as mineral\nhomeostasis, cellular proliferation and diﬀerentiation and the\nmodulation of native and adaptive immunity.\n68 Hence, their\ndysfunction may be connected to serious maladies making VDR\nsuitable for eﬀective drug-target development.\n69 To date more\nthan 3000 vitamin D (VD) analogues have been synthesized\nlargely due to the side eﬀects of 1,25D\n3 such as hypercalcemia\nwith some of them belonging to a completely new group of\nanalogues called non-steroidal VD analogues of which an\nexample is shown in Fig. 3C.\n70\nThe dataset for recreating VDR ligands was extracted from\nthe ZINC database.71 The ZINC database is a reliable curated\ndatabase with all compounds available for purchase and\ntesting. From this database 418 VDR ligands have been selected.\nAer canonicalizing and stripping the stereoisomeric informa-\ntion from these molecules, 210 SMILES strings remained. We\nfurther divided these 210 molecules into approximately equal\nsize training and test sets. The training data was used for model\nne-tuning and subsequently as seed molecules to create\nfocused libraries of potential VDR ligands. The test set was used\nas a reference for the comparison of Transmol-generated\nmolecules. While the sample size of this dataset is\ncomparatively small, it is composed of only several subsets of\nVDR ligands with each subset containing molecules that are\nstructurally similar to each other. Therefore, we theorize that\nTransmol may be able to extract enough information from the\ntraining data to recreate previously known VDR ligands and\nthus mimic the process of creating new ones. To reduce the\nimpact of randomness we perform this procedure ten times.\nA detailed workow for examining the recreation of known\nVDR ligands with Transmol can be summarized in the following\nway:\n(1) Acquire 418 known VDR ligands from the ZINC database.\n(2) Bring them into canonical form by discarding the\nstereoisomeric information reduced the set to 210 molecules.\n(3) Randomly sample 100 structures and use them as\na training set. Remaining 110 constitute the test set.\n(4) Use the training set tone tune Transmol. Fine tuning is\na common technique in deep learning. The idea is that instead\nof training a new model from scratch we can use a set of model\nFig. 3 Structural representations of the main VDR ligands groups. (A)\nthe secosteroid 1,25D3 bound to ratVDR (PDBID: 1RK3), (B) steroid acid\ne.g. lithocholic acid (LCA) bound to ratVDR (PDBID: 3W5P) and (C)\nnon-steroidal analogue YR301 bound to ratVDR (PDBID: 2ZFX). All\ncrystal structure were superimposed to 1RK3, the critical amino acid\ncontacts are highlighted in all structures.\n© 2021 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 | 25925\nPaper RSC Advances\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nparameters trained on a large dataset, and then perform addi-\ntional training on a smaller dataset.\n(5) Use a subset of training molecules as seeds for Transmol\n(30 randomly selected ligands from the training set).\n(6) Compare Transmol output to test set molecules and\nrecord overlap.\n(7) Repeat steps 3 to 6 ten times.\n4 Results and discussion\nIn this section, we describe major results and insights that were\nobtained during the implementation of our model and while\ndisseminating its output. It starts with analyzing the outcome of\na standard one-seed approach with the MOSES benchmark and\nmedicinal chemistrylters followed by the application of single\nseed settings for the regeneration of known VDR ligands. It also\noutlines a novel two-seed approach that targets the generation\nof diversied molecular libraries. In this regard the two-seed\napproach is complementary to the standard one-seed\napproach which works best for the acquisition of focused\nlibraries of close seed's analogues.\n4.1 Creating a focused library with one seed molecule\nThe standard Transmol settings involve the generation of\na focused library using a single seed molecule. Fig. 4 provides\nthe graphical overview of this process indicating that upon\noptimizing the sampling hyperparameters (see ESI† for details)\nthe single-seed approach yields the molecular library of close\nstructural analogues of the seed. Overall, we have fed Transmol\nwith 8000 seed molecules gathered from the MOSES test set\nresulting in the generation of 8000 focused libraries. The\ncombined outcome has then been bench-marked with MOSES\n56\nand analyzed using various medicinal chemistrylters in order\nto compare Transmol with other generative algorithms.\n4.1.1 Bench-marking Transmol with MOSES. According to\nTable 1, Transmol has demonstrates the greatest internal\ndiversity (IntDiv\n1 and IntDiv1) across all other baseline methods\nwithin MOSES. It can be also observed that among the neural\nalgorithms Transmol demonstrates the greatest proportion of\nnovel molecules, that are not present in the training dataset.\nAnother important observation is that Transmol's internal\ndiversity score exceeds that of the training dataset. This obser-\nvation might indicate the ability of Transmol to generalize well\non the previously unseen data.\nAs can be seen from Table 1, Transmol has a low validity\nscore. One of the possible reasons is the architecture of the\nnetwork. In architectures like autoencoders, or GANs the model\nis evaluated on how well it replicates the input during the\ntraining. However, in this case the model was learning the\nmolecular space, by maximizing the likelihood of the next\nFig. 4 The general pipeline of the sampling process for the one-seed approach.\nTable 1 Performance metrics for baseline models: fraction of valid molecules, fraction of unique molecules from 1000 and 10 000 molecules,\ninternal diversity, fraction of molecules passingﬁlters (MCF, PAINS, ring sizes, charge, atom types), and novelty. Reported (mean/C6 std) over three\nindependent model initializations. Arrows next to the metrics indicate preferable metric values (higher is better for all). CharRNN– character-\nlevel recurrent neural network, AAE– adversarial autoencoder, VAE– variational autoencoder, JTN-VAE– junction tree variational autoencoder,\nLatentGAN – latent vector based generative adversarial network, Transmol– transformer for molecules\nModel Valid ( [) Unique@1k ( [) Unique@10k ( [) IntDiv ( [) IntDiv2 ( [) Filters ( [) Novelty ( [)\nTrain 1 1 1 0.8567 0.8508 1 1\nHMM 0.076 /C6 0.0322 0.623 /C6 0.1224 0.5671 /C6 0.1424 0.8466 /C6 0.0403 0.8104 /C6 0.0507 0.9024 /C6 0.0489 0.9994 /C6 0.001\nNGram 0.2376 /C6 0.0025 0.974 /C6 0.0108 0.9217 /C6 0.0019 0.8738 /C6 0.0002 0.8644 /C6 0.0002 0.9582 /C6 0.001 0.9694 /C6 0.001\nCombinatorial 1.0 /C6 0.0 0.9983 /C6 0.0015 0.9909 /C6 0.0009 0.8732 /C6 0.0002 0.8666 /C6 0.0002 0.9557 /C6 0.0018 0.9878 /C6 0.0008\nCharRNN 0.9748 /C6 0.0264 1.0 /C6 0.0 0.9994 /C6 0.0003 0.8562 /C6 0.0005 0.8503 /C6 0.0005 0.9943 /C6 0.0034 0.8419 /C6 0.0509\nAAE 0.9368 /C6 0.0341 1.0 /C6 0.0 0.9973 /C6 0.002 0.8557 /C6 0.0031 0.8499 /C6 0.003 0.996 /C6 0.0006 0.7931 /C6 0.0285\nVAE 0.9767 /C6 0.0012 1.0 /C6 0.0 0.9984 /C6 0.0005 0.8558 /C6 0.0004 0.8498 /C6 0.0004 0.997 /C6 0.0002 0.6949 /C6 0.0069\nJTN-VAE 1.0 /C6 0.0 1.0 /C6 0.0 0.9996 /C6 0.0003 0.8551 /C6 0.0034 0.8493 /C6 0.0035 0.976 /C6 0.0016 0.9143 /C6 0.0058\nLatentGAN 0.8966 /C6 0.0029 1.0 /C6 0.0 0.9968 /C6 0.0002 0.8565 /C6 0.0007 0.8505 /C6 0.0006 0.9735 /C6 0.0006 0.9498 /C6 0.0006\nTransmol 0.0694 /C6 0.0004 0.9360 /C6 0.0036 0.9043 /C6 0.0036 0.8819 /C6 0.0003 0.8708 /C6 0.0002 0.8437 /C6 0.0015 0.9815 /C6 0.0004\n25926 | RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 © 2021 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nsymbol. The aim of this work was to explore the molecular space\nthrough SMILES representations, and our model is successful\nin this task. It is natural that some areas of this space are\nuninhabited, i.e. contain only invalid SMILES. One of the\npossible solutions to amend the low validity would be the\nintegration of autoencoders, or GAN parts into the model, since\nfor these methods the accurate replication of the input is\nexplicit. Another potential solution would be to optimize the\nmodel towards high validity.\nTable 2 shows more sophisticated metrics that require the\ncomparison of two molecular sets, reference and generated:\nFr´echet ChemNet Distance (FCD), similarity to the nearest\nneighbor (SNN), fragment similarity (Frag), and scaﬀold simi-\nlarity (Scaﬀ). In this table, we observe that Transmol has\na relatively highFCD score compared to neural methods and\na comparable or lower score in relation to non-neural algo-\nrithms. This is a surprising result that we have not anticipated\nconsidering the high scores of Transmol for internal diversity\nand relatively high scores for uniqueness. One of the probable\nreasons is that in our work we have used 8000 molecules as\nseeds out of z170000 molecules in the MOSES test set. As\na result, we created 8000 focused libraries. The individual\noutputs exhibit high internal diversity, since each focused set\nresults in unique molecules. In addition, these focused sets are\ndiverse with respect to each other. The high FCD score indicates\ndissimilarity between our generated set and the MOSES test set.\nThe reason for this is given by the fact that we are generating\nvery focused libraries and therefore cannot (and do not need to)\ncapture the whole variability of the MOSES test set. If the\nnumber of seed molecules would be increased substantially, the\nFCD value would decrease.\nAnother observation is that Transmol demonstrates superi-\nority for SNN/Test and Scaf/Test compared to non-neural\nbaselines and is comparable to other neural algorithms. Test\nstands for random test set and TestSF for scaﬀold split test set.\nFor SNN/Test Transmol is a top-2 algorithm. Another thing to\nnote is the TestSF column. The original authors of the bench-\nmark recommend a comparison of the TestSF columns when\nthe goal is to generate molecules with scaﬀolds that are not\npresent in the training set. However, the comparison should be\nused with caution since the test sca ﬀold set is not all-\nencompassing. It does not contain any sca ﬀolds that are\nabsent in the training dataset. Taking into consideration that\nmetrics in Table 2 compute overlaps in the two sets, the TestSF\npart of the metrics should not be over-interpreted.\nFig. 5 demonstrates distribution plots of the baselines and\nTransmol output compared to the test set. The distribution\nplots are similar to the histograms, but instead of showing\ndiscrete bins, the distribution plot smoothes the observations\nusing a Gaussian kernel. The distribution plots compare four\nmolecular properties: molecular weight (MW), octanol– water\npartition coeﬃcient (logP), quantitative estimation of drug-\nlikeness (QED), and synthetic accessibility score (SA). To\nquantify the distance between the test set and the generated set\nthe Wasserstein-1 distance was used (value in brackets). The\nresults show that Transmol has a matchingSA score, or better\nthan the original distribution while having a higher variance in\nTable 2 Performance metrics for baseline models: Fr´echet ChemNet Distance (FCD), similarity to a nearest neighbor (SNN), fragment similarity (Frag), and scaﬀold similarity (Scaﬀ); reported\n(mean /C6 std) over three independent model initializations. Results for random test set (Test) and scaﬀold split test set (TestSF). Arrows next to the metrics indicate preferable metric values.\nCharRNN – character-level recurrent neural network, AAE– adversarial autoencoder, VAE– variational autoencoder, JTN-VAE– junction tree variational autoencoder, LatentGAN– latent\nvector based generative adversarial network, Transmol– transformer for molecules\nModel\nFCD (Y) SNN ( [) Frag ( [) Scaf ( [)\nTest TestSF Test TestSF Test TestSF Test TestSF\nTrain 0.008 0.4755 0.6419 0.5859 1 0.9986 0.9907 0\nHMM 24.4661 /C6 2.5251 25.4312 /C6 2.5599 0.3876 /C6 0.0107 0.3795 /C6 0.0107 0.5754 /C6 0.1224 0.5681 /C6 0.1218 0.2065 /C6 0.0481 0.049 /C6 0.018\nNGram 5.5069 /C6 0.1027 6.2306 /C6 0.0966 0.5209 /C6 0.001 0.4997 /C6 0.0005 0.9846 /C6 0.0012 0.9815 /C6 0.0012 0.5302 /C6 0.0163 0.0977 /C6 0.0142\nCombinatorial 4.2375 /C6 0.037 4.5113 /C6 0.0274 0.4514 /C6 0.0003 0.4388 /C6 0.0002 0.9912 /C6 0.0004 0.9904 /C6 0.0003 0.4445 /C6 0.0056 0.0865 /C6 0.0027\nCharRNN 0.0732 /C6 0.0247 0.5204 /C6 0.0379 0.6015 /C6 0.0206 0.5649 /C6 0.0142 0.9998 /C6 0.0002 0.9983 /C6 0.0003 0.9242 /C6 0.0058 0.1101 /C6 0.0081\nAAE 0.5555 /C6 0.2033 1.0572 /C6 0.2375 0.6081 /C6 0.0043 0.5677 /C6 0.0045 0.991 /C6 0.0051 0.9905 /C6 0.0039 0.9022 /C6 0.0375 0.0789 /C6 0.009\nVAE 0.099 /C6 0.0125 0.567 /C6 0.0338 0.6257 /C6 0.0005 0.5783 /C6 0.0008 0.9994 /C6 0.0001 0.9984 /C6 0.0003 0.9386 /C6 0.0021 0.0588 /C6 0.0095\nJTN-VAE 0.3954 /C6 0.0234 0.9382 /C6 0.0531 0.5477 /C6 0.0076 0.5194 /C6 0.007 0.9965 /C6 0.0003 0.9947 /C6 0.0002 0.8964 /C6 0.0039 0.1009 /C6 0.0105\nLatentGAN 0.2968 /C6 0.0087 0.8281 /C6 0.0117 0.5371 /C6 0.0004 0.5132 /C6 0.0002 0.9986 /C6 0.0004 0.9972 /C6 0.0007 0.8867 /C6 0.0009 0.1072 /C6 0.0098\nTransmol 4.3729 /C6 0.0466 5.3308 /C6 0.0428 0.6160 /C6 0.0005 0.4614 /C6 0.0007 0.9564 /C6 0.0009 0.9496 /C6 0.0009 0.7394 /C6 0.0009 0.0183 /C6 0.0065\n© 2021 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 | 25927\nPaper RSC Advances\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nother metrics. It shows that Transmol is not as close to the\ntesting set distribution as other neural algorithms, implying\na higher diversity, but it is not as far from it as some simpler,\ncombinatorial baselines.\n4.1.2 Analyzing Transmol with medicinal chemistrylters.\nTo provide more comparisons we also haveltered molecules\nby using some heuristics that are commonly employed in drug\ndevelopment. These types of lters have been empirically\ndevised using a database of drug suitable compounds. For this\ncomparison, we have used the Lipinski rule of 5,\n72 Ghose,73\nVeber,74 rule of 3,75 and REOS76 lters (see ESI† for specic\nconstrains of each lter). All properties for constraint testing\nwere computed using the RDKit library.67 Fig. 6 demonstrates\nproportions of molecules that satisfy each rule.\nAs can be seen in Fig. 6 Transmol has the highest proportion\nof molecules that satisfy the rule of 3 among the neural base-\nlines. In addition, among non-neural algorithms, Transmol has\nthe highest proportion of molecules that satisfy the Ghoselter\nand REOS.\n4.2 Transmol validation via the recreation of known vitamin\nD receptor ligands\nIn order to show the applicability of our model to a real protein\ntarget, VDR has been chosen due to its importance in various\nphysiological processes and the fact that there are many known\nVDR ligands that can be readily retrieved from established\ndatabases. For this purpose a small subset of known VDR\nligands have been chosen from the ZINC database tone tune\nour model and then use Transmol to recover as many known\nligands as possible.\nAcross the 10 sampling cycles our algorithm was able to\nrecover 27 known VDR ligands from the employed ZINC dataset.\nGiven that the ZINC database allowed us to extract the struc-\ntures of 210 VDR ligands with stripped stereoisomeric infor-\nmation, we have recovered 12.9% of all structures within this\ndataset.\nThe recovery process was performed 10 times where VDR\nligands were re-sampled into the training and test set randomly.\nThe minimum and maximum number of recovered molecules\nper cycle were 1 and 9, respectively. On average the algorithm\ngenerated 4.1 molecules per sampling cycle.\nOf the 27 recovered molecules 56% were secosteroid back-\nbone (Fig. 3A) and the rest non-steroidal VD analogues (Fig. 3C).\nThe algorithm did not recover any steroid backbone based\nanalogs (Fig. 3B). It is remarkable that the algorithm was able to\nrecover the natural ligand 1,25D\n3 (Fig. 3A), as well as therst\nFig. 5 Plots of Wasserstein-1 distance between distributions of\nmolecules in the generated and test sets.\nFig. 6 Proportions of molecules that satisfyﬁve diﬀerent medicinal\nchemistry ﬁlters.\n25928 | RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 © 2021 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nnon-steroidal 3,3-diphenylpentane analog YR301 (Fig. 3C). It\nshould be noted that the group of non-steroidal ligand contains\ntwo subgroups, therst are cannonical ligands that bind to the\nligand-binding pocket of VDR as secosteroid and steroid acids.\nThe second group contains possible coactivator mimetics that\nirreversible compete with coactivator interaction on the surface\nof VDR. Thus, the above described approach constitutes a newly\nestablished benchmark for validation of generative algorithms,\nwhich examines how well the model can recreate the known\nligands to a particular biological target.\n4.3 Exploring chemical space using two-seeds approach\nIn this section, we discuss the generation of diverse libraries\nusing two seed molecules. Fig. 7 gives a pictorial overview of the\nsampling procedure. Using the Transmol encoder network we\nencode two molecules, thus obtaining their latent representa-\ntion. Then, we average them to get a latent vectorz\n12 that is\nlocated between the two latent vectors z1 and z2. Then, we\nsample the decoder using vectorz12. To increase the chance of\nsampling from the populated latent space we enumerate the\nSMILES representations of the seed molecules and construct\npairs. This twist increases the chances of obtaining valid\nSMILES strings when sampling the middle point.\nWhile the basic case of getting vectorz\n12 through averaging\nthe individual vectors, a more general approach would be\ncomputation of weighted sum. In the default case the weights of\nthe vectorsz\n1 and z2 are chosen to be 0.5. However, other valid\nweight combinations can also be used to navigate the latent\nspace. We have noticed that when the weights of both vectors\nare close to 0.5 the algorithm tends to return only very simple\nmolecules with low molecular weight. The above is especially\ntrue when both seed molecules have a molecular weight of more\nthan 250 and contain more than 15 atoms (except hydrogen)\nmeaning that their SMILES strings are relatively long. There-\nfore, we have varied thea parameter of the reward function of\nthe beam search to adjust the length of the decoded SMILES\nstrings and as a result the complexity of the resulting molecule.\nOn the other hand when thea parameter is set too big the\nalgorithm may return only a limited number of molecules.\nThus, to create enough molecules of larger size and increased\ncomplexity, diﬀerent combinations of weight distribution and\nthe sampling reward parametera can be tested. Overall, the\npossibility to vary the seeds' weight distributions along with the\nsampling reward parameter a provides a viable mean of\ntraversal across molecular space (see SI for specic compari-\nsons). Of course, not all the generated structures are stable and/\nor synthesizable. Nonetheless, they may still be used as inspi-\nrational ligands for molecular docking.\nFig. 8 illustrates how molecular sampling from the latent\ndistribution using two seed molecules can help with expanding\nthe scaﬀold diversity. The resulting molecular library demon-\nstrates a diversity of structural features that would be unat-\ntainable through simple fragment substitution or through the\napplication of the alternative one-seed approach.\nFig. 7 The general pipeline of the sampling process for the two-seeds approach.\nFig. 8 Expanding scaﬀold diversity with the two-seeds approach.\n© 2021 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 | 25929\nPaper RSC Advances\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nBoth one-seed and two-seeds modes of Transmol are incor-\nporated into the cheml.io55 website as one of the methods that\nallows the generation of molecules on demand. For the two-\nseed approach, the user can specify both seeds, a weighting\ndistribution between them and the sampling reward parameter\na that inuences the length of the decoded SMILES string.\n5 Conclusion\nIn summary, we have successfully adopted a recent deep\nlearning framework to the task of molecular generation using\nattention mechanisms. Upon implementation, we have bench-\nmarked the resulting Transmol method utilizing MOSES,\na benchmark introduced for the comparison of generative\nalgorithms. Our approach outperformed state-of-the-art gener-\native machine learning frameworks for the internal diversity\n(IntDiv\n1 and InDiv2) core MOSES metric. Besides, we were able to\nincorporate two distinct modes of molecular generation within\na single generative model. The rst one-seed input-guided\napproach appears to be instrumental for the cases that\nrequire targeted generation of focused libraries composed of\nclose analogues of the seed structure. The second two-seed\ndiversity-driven approach succeeds to generate molecules that\nresemble both seeds and is thus attractive for the tasks that\nrequire chemical space exploration. In addition, we have vali-\ndated the Transmol algorithm through the recreation of known\nVDR ligands. This type of bench-marking represents a viable\noption for expanding current validation tools for in silico\nmolecular generation and propose it to be performed for all new\ngenerative algorithms. Furthermore, an analogous workow\ncan be utilized in drug discovery & development to obtain\npotential novel biologically active hit compounds and we are\ncurrently working in this direction. Finally, we have incorpo-\nrated the Transmol algorithm into our recently launched\ncheML.io web database of ML-generated molecules as a second\ngeneration on-demand method.\nConﬂicts of interest\nThere are no conicts to declare.\nAcknowledgements\nThe co-authors would like to acknowledge the support of Naz-\narbayev University Faculty-Development Competitive Research\nGrants Program to SF and FM (240919FD3926, 110119FD4520)\nand the support of NPO Young Researchers Alliance and Naz-\narbayev University Corporate Fund“Social Development Fund”\nfor grant under their Fostering Research and Innovation\nPotential Program.\nNotes and references\n1 J.-L. Reymond,Acc. Chem. Res., 2015,48, 722.\n2 E. Lin, C.-H. Lin and H.-Y. Lane,Molecules, 2020,25, 3250.\n3 D. V. Green, inACS Symposium Series, ed. E. O. Pyzer-Knapp\nand T. Laino, American Chemical Society, Washington, DC,\n2019, vol. 1326, p. 81.\n4 P. Schneider, W. P. Walters, A. T. Plowright, N. Sieroka,\nJ. Listgarten, R. A. Goodnow, J. Fisher, J. M. Jansen,\nJ. S. Duca, T. S. Rush, M. Zentgraf, J. E. Hill,\nE. Krutoholow, M. Kohler, J. Blaney, K. Funatsu,\nC. Luebkemann and G. Schneider,Nat. Rev. Drug Discovery,\n2019, 19, 353.\n5 P. B. Jørgensen, M. N. Schmidt and O. Winther,Mol. Inf.,\n2018, 37, 1700133.\n6 A. Zhavoronkov, Q. Vanhaelen and T. I. Oprea, Clin.\nPharmacol. Ther., 2020,107, 780.\n7 A. S. Alshehri, R. Gani and F. You,Comput. Chem. Eng., 2020,\n141, 107005.\n8 Q. Vanhaelen, Y.-C. Lin and A. Zhavoronkov,ACS Med. Chem.\nLett., 2020,11, 1496.\n9 S. Y. Al-nami, E. Aljuhani, I. Althaga, H. M. Abumelha,\nT. M. Bawazeer, A. M. Al-Solimy, Z. A. Al-Ahmed, F. Al-\nZahrani and N. El-Metwaly,Arabian J. Sci. Eng., 2021,46, 365.\n10 P. C. D. Hawkins,J. Chem. Inf. Model., 2017,57, 1747.\n11 S. Das, S. Sarmah, S. Lyndem and A. Singha Roy,J. Biomol.\nStruct. Dyn., 2021,39(9), 3347.\n12 N. S. Pagadala, K. Syed and J. Tuszynski,Biophys. Rev., 2017,\n9, 91.\n13 M. Hartenfeller and G. Schneider, Wiley Interdiscip. Rev.\nComput. Mol. Sci., 2011,1, 742– 759.\n14 H.-J. B¨ohm, J. Comput.-Aided Mol. Des., 1992,6, 61.\n15 V. J. Gillet, W. Newell, P. Mata, G. Myatt, S. Sike, Z. Zsoldos\nand A. P. Johnson,J. Chem. Inf. Model., 1994,34, 207.\n16 K. Kawai, N. Nagata and Y. Takahashi,J. Chem. Inf. Model.,\n2014, 54, 49.\n17 H. Chen, O. Engkvist, Y. Wang, M. Olivecrona and\nT. Blaschke,Drug Discovery Today, 2018,23\n, 1241.\n18 G. Hessler and K.-H. Baringhaus,Molecules, 2018,23, 2520.\n19 M. Hartenfeller, H. Zettl, M. Walter, M. Rupp, F. Reisen,\nE. Proschak, S. Weggen, H. Stark and G. Schneider,PLoS\nComput. Biol., 2012,8, e1002380.\n20 M. Hartenfeller, M. Eberle, P. Meier, C. Nieto-Oberhuber,\nK.-H. Altmann, G. Schneider, E. Jacoby and S. Renner,J.\nChem. Inf. Model., 2011,51, 3093.\n21 M. H. S. Segler and M. P. Waller,Chem.– Eur. J., 2017, 23,\n6118.\n22 D. C. Elton, Z. Boukouvalas, M. D. Fuge and P. W. Chung,\nMol. Syst. Des. Eng., 2019,4, 828.\n23 D. Xue, Y. Gong, Z. Yang, G. Chuai, S. Qu, A. Shen, J. Yu and\nQ. Liu, Wiley Interdiscip. Rev.: Comput. Mol. Sci., 2018, 9,\ne1395.\n24 N. Brown, P. Ertl, R. Lewis, T. Luksch, D. Reker and\nN. Schneider,J. Comput.-Aided Mol. Des., 2020,34, 709.\n25 K. Hansen, F. Biegler, R. Ramakrishnan, W. Pronobis,\nO. A. von Lilienfeld, K.-R. M¨uller and A. Tkatchenko, J.\nPhys. Chem. Lett., 2015,6, 2326.\n26 K. Hansen, G. Montavon, F. Biegler, S. Fazli, M. Rupp,\nM. Sche ﬄer, O. A. Von Lilienfeld, A. Tkatchenko and\nK.-R. Muller,J. Chem. Theory Comput., 2013,9, 3404.\n25930 | RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 © 2021 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n27 W. Jin, C. W. Coley, R. Barzilay and T. Jaakkola,Proceedings\nof the 31st International Conference on Neural Information\nProcessing Systems, Red Hook, NY, USA, 2017, p. 2604.\n28 B. Sanchez-Lengeling and A. Aspuru-Guzik,Science, 2018,\n361, 360.\n29 G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral,\nP. L. C. Farias and A. Aspuru-Guzik, arXiv, 2018, preprint,\narXiv:1705.10843v3, https://arxiv.org/abs/1705.10843v3.\n30 B. Sanchez-Lengeling, C. Outeiral, G. L. Guimaraes and\nA. Aspuru-Guzik, ChemRxiv, 2017, DOI: 10.26434/\nchemrxiv.5309668.v3.\n31 S. Harel and K. Radinsky, Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge Discovery &\nData Mining - KDD’18, London, United Kingdom, 2018, p.\n331.\n32 R. G ´omez-Bombarelli, J. N. Wei, D. Duvenaud,\nJ. M. Hern ´andez-Lobato, B. S ´anchez-Lengeling,\nD. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel,\nR. P. Adams and A. Aspuru-Guzik,ACS Cent. Sci., 2018, 4,\n268.\n33 M. J. Kusner, B. Paige and J. M. Hern ´andez-Lobato,\nProceedings of the 34th International Conference on Machine\nLearning-Volume 70, 2017, p. 1945.\n34 J. Lim, S. Ryu, J. W. Kim and W. Y. Kim,J. Cheminf., 2018,10,\n31.\n35 M. H. S. Segler, T. Kogej, C. Tyrchan and M. P. Waller,ACS\nCent. Sci., 2018,4, 120.\n36 W. Jin, R. Barzilay and T. Jaakkola,Proceedings of the 35th\nInternational Conference on Machine Learning ,\nStockholmsm¨assan, Stockholm Sweden, 2018, p. 2323.\n37 J.-Y. Zhu, T. Park, P. Isola and A. A. Efros, 2017 IEEE\nInternational Conference On Computer Vision, ICCV, 2017, p.\n2242.\n38 Ł . Maziarka, A. Pocha, J. Kaczmarczyk, K. Rataj and\nM. Warcho ł, Articial Neural Networks and Machine\nLearning – ICANN 2019, Workshop and Special Sessions,\nCham, 2019, p. 810.\n39 S. Kang and K. Cho,J. Chem. Inf. Model., 2019,59, 43.\n40 W. Jin, R. Barzilay and T. Jaakkola, arXiv:2002.03230 [cs, stat\n, 2020.\n41 T. Blaschke, O. Engkvist, J. Bajorath and H. Chen, J.\nCheminf., 2020,12, 68.\n42 S. Wu, Y. Kondo, M.-a. Kakimoto, B. Yang, H. Yamada,\nI. Kuwajima, G. Lambard, K. Hongo, Y. Xu, J. Shiomi,\nC. Schick, J. Morikawa and R. Yoshida,npj Comput. Mater.,\n2019, 5\n, 66.\n43 C. Grebner, H. Matter, A. T. Plowright and G. Hessler,J. Med.\nChem., 2020,63, 8809.\n44 E. Putin, A. Asadulaev, Q. Vanhaelen, Y. Ivanenkov,\nA. V. Aladinskaya, A. Aliper and A. Zhavoronkov, Mol.\nBiopharm., 2018,15, 4386.\n45 E. Putin, A. Asadulaev, Y. Ivanenkov, V. Aladinskiy,\nB. Sanchez-Lengeling, A. Aspuru-Guzik and\nA. Zhavoronkov,J. Chem. Inf. Model., 2018,58, 1194.\n46 O. M´endez-Lucio, B. Baillif, D.-A. Clevert, D. Rouqui´e and\nJ. Wichard,Nat. Commun., 2020,11, 10.\n47 M. Skalic, J. Jim´enez Luna, D. Sabbadin and G. De Fabritiis,\nJ. Chem. Inf. Model., 2019, 1205.\n48 G. Chen, Z. Shen, A. Iyer, U. F. Ghumman, S. Tang, J. Bi,\nW. Chen and Y. Li,Polymers, 2020,12, 163.\n49 D. Weininger,J. Chem. Inf. Model., 1988,28, 31.\n50 I. Sutskever, O. Vinyals and Q. V. Le, inProceedings of the 27th\nInternational Conference on Neural Information Processing\nSystems - Volume 2 , ed. Z. Ghahramani, M. Welling, C.\nCortes, N. D. Lawrence and K. Q. Weinberger, MIT Press,\nCambridge, MA, USA, 2014, p. 3104.\n51 I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville and Y. Bengio, inProceedings of\nthe 27th International Conference on Neural Information\nProcessing Systems - Volume 2 , ed. Z. Ghahramani, M.\nWelling, C. Cortes, N. D. Lawrence and K. Q. Weinberger,\nMIT Press, Cambridge, MA, USA, 2014, p. 2672.\n52 Y. Le Cun and F. Fogelman-Souli´e, Intellectica, 1987,1, 114.\n53 D. E. Rumelhart, G. E. Hinton and R. J. Williams,Nature,\n1986, 323, 533.\n54 A. Gupta, A. T. M¨uller, B. J. H. Huisman, J. A. Fuchs,\nP. Schneider and G. Schneider,Mol. Inf., 2018,37, 1700111.\n55 R. Zhumagambetov, D. Kazbek, M. Shakipov, D. Maksut,\nV. A. Peshkov and S. Fazli,RSC Adv., 2020,10, 45189.\n56 D. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling,\nS. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov,\nA. Artamonov, V. Aladinskiy, M. Veselov, A. Kadurin,\nS. Johansson, H. Chen, S. Nikolenko, A. Aspuru-Guzik and\nA. Zhavoronkov, arXiv, 2020, preprint, arXiv:1811.12823v5,\nhttps://arxiv.org/abs/1811.12823v5.\n57 J. Ar´us-Pous, S. V. Johansson, O. Prykhodko, E. J. Bjerrum,\nC. Tyrchan, J.-L. Reymond, H. Chen and O. Engkvist, J.\nCheminf., 2019,11, 71.\n58 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, u. Kaiser and I. Polosukhin, Proceedings of\nthe 31st International Conference on Neural Information\nProcessing Systems, Red Hook, NY, USA, 2017, p. 6000.\n59 Transmol Gitlab page , https://gitlab.com/cheml.io/public/\ntransmol, accessed July 2021.\n60 The Annotated Transformer , https://nlp.seas.harvard.edu/\n2018/04/03/attention.html, accessed January 2021.\n61 K. Preuer, P. Renz, T. Unterthiner, S. Hochreiter and\nG. Klambauer,J. Chem. Inf. Model., 2018,58, 1736.\n62 A. Kadurin, A. Aliper, A. Kazennov, P. Mamoshina,\nQ. Vanhaelen, K. Khrabrov and A. Zhavoronkov,\nOncotarget, 2017,8, 10883.\n63 T. Blaschke, M. Olivecrona, O. Engkvist, J. Bajorath and\nH. Chen,Mol. Inf., 2018,37, 1700123.\n64 D. Polykovskiy, A. Zhebrak, D. Vetrov, Y. Ivanenkov,\nV. Aladinskiy, P. Mamoshina, M. Bozdaganyan, A. Aliper,\nA. Zhavoronkov and A. Kadurin,Mol. Biopharm., 2018, 15,\n4398.\n65 O. Prykhodko, S. V. Johansson, P.-C. Kotsias, J. Ar´us-Pous,\nE. J. Bjerrum, O. Engkvist and H. Chen,J. Cheminf., 2019,\n11, 74.\n66 J. Degen, C. Wegscheid-Gerlach, A. Zaliani and M. Rarey,\nChemMedChem, 2008,3, 1503– 1507.\n© 2021 The Author(s). Published by the Royal Society of Chemistry RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 | 25931\nPaper RSC Advances\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n67 RDKit, Open-source cheminformatics , http://www.rdkit.org,\naccessed September 2020.\n68 R. F. Chun, P. T. Liu, R. L. Modlin, J. S. Adams and\nM. Hewison,Front. Physiol., 2014,5, 151.\n69 A. Verstuyf, G. Carmeliet, R. Bouillon and C. Mathieu,Kidney\nInt., 2010,78, 140.\n70 R. Bouillon, W. H. Okamura and A. W. Norman,Endocr. Rev.,\n1995, 16, 200.\n71 T. Sterling and J. J. Irwin,J. Chem. Inf. Model., 2015,55, 2324.\n72 C. A. Lipinski, F. Lombardo, B. W. Dominy and P. J. Feeney,\nAdv. Drug Delivery Rev., 1997,23,3 .\n73 A. K. Ghose, V. N. Viswanadhan and J. J. Wendoloski,J.\nComb. Chem., 1999,1, 55.\n74 D. F. Veber, S. R. Johnson, H.-Y. Cheng, B. R. Smith,\nK. W. Ward and K. D. Kopple,J. Med. Chem., 2002,45, 2615.\n75 M. Congreve, R. Carr, C. Murray and H. Jhoti,Drug Discovery\nToday, 2003,8, 876.\n76 W. P. Walters and M. Namchuk,Nat. Rev. Drug Discovery,\n2003, 2, 259.\n25932 | RSC Adv.,2 0 2 1 ,11,2 5 9 2 1–25932 © 2021 The Author(s). Published by the Royal Society of Chemistry\nRSC Advances Paper\nOpen Access Article. Published on 27 July 2021. Downloaded on 11/5/2025 5:19:34 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online",
  "topic": "Repurposing",
  "concepts": [
    {
      "name": "Repurposing",
      "score": 0.8111813068389893
    },
    {
      "name": "Computer science",
      "score": 0.5060600638389587
    },
    {
      "name": "Natural language processing",
      "score": 0.45786550641059875
    },
    {
      "name": "Chemistry",
      "score": 0.3223470449447632
    },
    {
      "name": "Biology",
      "score": 0.10141134262084961
    },
    {
      "name": "Ecology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60559429",
      "name": "Nazarbayev University",
      "country": "KZ"
    }
  ]
}