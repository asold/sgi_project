{
    "title": "General Multi-label Image Classification with Transformers",
    "url": "https://openalex.org/W3107026593",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4367936218",
            "name": "Lanchantin, Jack",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A2358178408",
            "name": "Wang, Tianlu",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A4202201586",
            "name": "Ordonez, Vicente",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A2272233770",
            "name": "Qi, Yanjun",
            "affiliations": [
                "University of Virginia"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963175631",
        "https://openalex.org/W2751120573",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3099701504",
        "https://openalex.org/W2963745697",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2133128938",
        "https://openalex.org/W1567302070",
        "https://openalex.org/W2946567085",
        "https://openalex.org/W2411690663",
        "https://openalex.org/W3025701938",
        "https://openalex.org/W3035602609",
        "https://openalex.org/W2895472239",
        "https://openalex.org/W2963052338",
        "https://openalex.org/W2963300078",
        "https://openalex.org/W2969792713",
        "https://openalex.org/W2932399282",
        "https://openalex.org/W2183087644",
        "https://openalex.org/W2118712128",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2405989770",
        "https://openalex.org/W2963697527",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3031669205",
        "https://openalex.org/W3098331497",
        "https://openalex.org/W2963875806",
        "https://openalex.org/W2963513598",
        "https://openalex.org/W2404281525",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2105842272",
        "https://openalex.org/W2536626143",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W2982112268",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2146241755",
        "https://openalex.org/W146125889",
        "https://openalex.org/W2410641892",
        "https://openalex.org/W2463598282",
        "https://openalex.org/W2604675517",
        "https://openalex.org/W2963306618",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W822362947",
        "https://openalex.org/W2949474740",
        "https://openalex.org/W2788462285",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1524416683",
        "https://openalex.org/W3089555680",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W3030163527"
    ],
    "abstract": "Multi-label image classification is the task of predicting a set of labels corresponding to objects, attributes or other entities present in an image. In this work we propose the Classification Transformer (C-Tran), a general framework for multi-label image classification that leverages Transformers to exploit the complex dependencies among visual features and labels. Our approach consists of a Transformer encoder trained to predict a set of target labels given an input set of masked labels, and visual features from a convolutional neural network. A key ingredient of our method is a label mask training objective that uses a ternary encoding scheme to represent the state of the labels as positive, negative, or unknown during training. Our model shows state-of-the-art performance on challenging datasets such as COCO and Visual Genome. Moreover, because our model explicitly represents the uncertainty of labels during training, it is more general by allowing us to produce improved results for images with partial or extra label annotations during inference. We demonstrate this additional capability in the COCO, Visual Genome, News500, and CUB image datasets.",
    "full_text": "General Multi-label Image Classiﬁcation with Transformers\nJack Lanchantin, Tianlu Wang, Vicente Ordonez, Yanjun Qi\nUniversity of Virginia\n{jjl5sw,tianlu,vicente,yq2h}@virginia.edu\nAbstract\nMulti-label image classiﬁcation is the task of predicting\na set of labels corresponding to objects, attributes or other\nentities present in an image. In this work we propose the\nClassiﬁcation Transformer (C-Tran), a general framework\nfor multi-label image classiﬁcation that leverages Trans-\nformers to exploit the complex dependencies among visual\nfeatures and labels. Our approach consists of a Trans-\nformer encoder trained to predict a set of target labels\ngiven an input set of masked labels, and visual features\nfrom a convolutional neural network. A key ingredient of\nour method is a label mask training objective that uses a\nternary encoding scheme to represent the state of the la-\nbels as positive, negative, or unknown during training. Our\nmodel shows state-of-the-art performance on challenging\ndatasets such as COCO and Visual Genome. Moreover,\nbecause our model explicitly represents the uncertainty of\nlabels during training, it is more general by allowing us\nto produce improved results for images with partial or ex-\ntra label annotations during inference. We demonstrate this\nadditional capability in the COCO, Visual Genome, News-\n500, and CUB image datasets.\n1. Introduction\nImages in real-world applications generally portray many\nobjects and complex situations. Multi-label image classi-\nﬁcation is a visual recognition task that aims to predict a\nset of labels corresponding to objects, attributes, or actions\ngiven an input image [15, 46, 48, 50, 6, 32, 9]. This task\ngoes beyond the more well studied single-label multi-class\nclassiﬁcation problem where the objective is to extract and\nassociate image features with a single concept per image.\nIn the multi-label setting, the output set of labels has some\nstructure that reﬂects the structure of the world. For exam-\nple, dolphin is unlikely to co-occur with grass, while knife\nis more likely to appear next to a fork. Effective models for\nmulti-label classiﬁcation aim to extract good visual features\nthat are predictive of image labels, but also exploit the com-\nplex relations and dependencies between visual features and\nC-Tran\nDuring Training During Inference\nPredict Masked Labels\nMask Random Labels\nPredict All Labels\nMask Everything\nC-Tran\nDuring Training\nPredict Masked Labels\nMask Random Labels\nDuring Inference\nC-Tran\nPredict Masked Labels\nMask Everything\nC-Tran\nDuring Training\nPredict Masked Labels\nMask Random Labels\nDuring Inference\nC-Tran\nPredict Masked Labels\nMask Everything\nC-Tran\nFigure 1. We propose a transformer-based model for multi-label\nimage classiﬁcation that exploits dependencies among a target set\nof labels using an encoder transformer. During training, the model\nlearns to reconstruct a partial set of labels given randomly masked\ninput label embeddings and image features. During inference, our\nmodel can be conditioned only on visual input or a combination of\nvisual input and partial labels, leading to superior results.\nlabels, and among labels themselves.\nTo this end, we present the Classiﬁcation Transformer\n(C-Tran), a multi-label classiﬁcation framework that lever-\nages a Transformer encoder [47]. Transformers have\ndemonstrated a remarkable capability of being able to ex-\nploit complex and rich dependencies among sets of inputs\nusing multiple layers of multi-headed self-attention opera-\ntions. In our approach, a Transformer encoder is trained\nto reconstruct a set of target labels given an input set of\nmasked label embeddings and a set of features obtained\nfrom a convolutional neural network. Unlike the Trans-\nformer encoders used for language modeling [13], C-Tran\nuses a label mask training objective that allows us to repre-\nsent the state of the labels aspositive, negative, or unknown.\nAt test time, C-Tran is able to predict a set of target labels\nusing only input visual features by masking all the input\nlabels as unknown. Figure 1 gives an overview of this strat-\negy. We demonstrate that this approach leads to superior re-\nsults on a number of benchmarks compared to other recent\napproaches that exploit label relations using graph convolu-\ntional networks and other recently proposed strategies.\nBeyond obtaining state-of-the-art results on the intro-\nduced regular multi-label classiﬁcation task, we also claim\nthat C-Tran is a more general model for reasoning under\n1\narXiv:2011.14027v1  [cs.CV]  27 Nov 2020\n(a) Regular Inference (b) Inference with Partial Labels\numbrella: 0.91\ncar: 0.86\nsunglasses: 0.18\nperson: 0.93\n(c) Inference with Extra Labels\ntruck: 0.32\nsunglasses: 0.16\nrain coat=1, truck=0 city=1, rain=1\ncar: 0.91\numbrella: 0.93\nrain coat: 0.92\nperson: 0.89person: 0.83\numbrella: 0.72\ncar: 0.42\nrain coat: 0.32\nsunglasses: 0.28\ntruck: 0.22\ntruck: 0.32\nsunglasses: 0.16\nrain coat=1, truck=1\ncity=1\nrain=1\nf\nf\nf\n car: 0.91\numbrella: 0.93\nrain coat: 0.92\nperson: 0.84\nperson: 0.93\numbrella: 0.91\ncar: 0.86\nsunglasses: 0.18\nf\numbrella: 0.93\nrain coat: 0.92\ncar: 0.91\nperson: 0.84\ntruck: 0.32\nsunglasses: 0.16\ncity=1, rain=1\n(a) \nperson: 0.83\numbrella: 0.72\ncar: 0.42\nrain coat: 0.32\nsunglasses: 0.28\ntruck: 0.22\n(b) \n(c) \nperson: 0.93\numbrella: 0.91\ncar: 0.86\nsunglasses: 0.18\numbrella: 0.93\nrain coat: 0.92\ncar: 0.91\nperson: 0.84\ntruck: 0.32\nsunglasses: 0.16\nC-Tran\nC-Tran\nC-Tran\nFigure 2. Different inference settings for general multi-label image classiﬁcation: (a) Standard multi-label classiﬁcation takes only image\nfeatures as input. All labels are unknown yu.; (b) Classiﬁcation under partial labels takes as input image features as well as a subset of the\ntarget labels that are known. The labels rain coat and truck are known labels yk, and all others are unknown labels yu; (c) Classiﬁcation\nunder extra labels takes as input image features and some related extra information. The labels city and rain are known extra labels ye\nk,\nand all others are unknown target labels yt\nu.\nprior label observations. Because our approach explicitly\nmodels the uncertainty of the labels during training, it can\nalso be used at test time with partial or extra label annota-\ntions by setting the state of some of the labels as either pos-\nitive or negative instead of masking them out as unknown.\nFor instance, consider the example shown in Figure 2(a)\nwhere a model is able to predict person and umbrella with\nrelatively high accuracies, but is not conﬁdent for categories\nsuch as rain coat, or car that are clearly present in the pic-\nture. Suppose we know some labels and set them to their\ntrue positive (for rain coat) or true negative (for truck) val-\nues. Provided with this new information, the model is able\nto predict car with a high conﬁdence as it moves mass prob-\nability from truck to car, and predicts other objects such as\numbrella with even higher conﬁdence than in the original\npredictions (Figure 2(b)). In general, we consider this set-\nting as realistic since many images also have metadata in\nthe form of extra labels such as location or weather infor-\nmation (Figure 2(c)). This type of conditional inference is a\nmuch less studied problem. Our general approach to multi-\nlabel image classiﬁcation with Transformers is able to nat-\nurally handle all these scenarios under a uniﬁed framework.\nWe compare our results with a competing method relying\non iterative inference [49], and against sensitive baselines,\ndemonstrating superior results under variable amounts of\npartial or extra labels.\nThe beneﬁts of our proposed framework can be summa-\nrized as follows:\n• Flexibility: It is the ﬁrst model that can be de-\nployed in multi-label image classiﬁcation under arbi-\ntrary amounts of extra or partial labels. We use a uni-\nﬁed model architecture and training method that lets\nusers to apply our model easily in any setting.\n• Accuracy: We evaluate our model on six datasets\nacross three inference settings and achieve state-of-\nthe-art results on all six. The label mask training strat-\negy enhances the correlations between visual concepts\nleading to more accurate predictions.\n• Interactivity: The use of state embeddings enables\nusers to easily interact with the model and test any\ncounterfactuals. C-Tran can take human interventions\nas partial evidence and provides more interpretable and\naccurate predictions.\n2. Problem Setup\nIn this section, we formally explain the three different\nmulti-label image classiﬁcation inference settings that we\nuse to demonstrate the utility of our approach.\nRegular Multi-label Classiﬁcation. In regular multi-label\nimage classiﬁcation, the goal is to predict a set of labels for\nan input image. Let x be an image, and y be a ground truth\nset of ℓbinary labels {y1,y2,...,y ℓ},yi ∈{0,1}. The goal\nof multi-label classiﬁcation is to construct a classiﬁer, f, to\npredict a set of labels given an image so that: ˆy = f(x).\nInference with Partial Labels. While regular classiﬁca-\ntion methods aim to predict the full set of ℓ labels given\nonly an input image, some subset of labels yk ⊆y may\nbe observed, or known, at test time. This is also known\nas having partial labels available. For example, many im-\nages on the web are accompanied by some labeled text such\nas captions on social media. In this reformulated setting,\nthe goal is to predict the unknown labels ( yu = y \\yk)\ngiven both the image and the known labels during infer-\nence: ˆyu = f(x,yk). Note that we assume that all la-\nbels are properly annotated during training. This setting\nis speciﬁcally for inference with partially annotated labels,\nand it differs from other works that tackle the problem of\ntraining models from partially annotated data [54, 14, 27].\nInference with Extra Labels. Similar to partially labeled\nimages, there are many cases where we observe extra labels\nthat describe the image, but are not part of the target label\nset. For example, we may know that an image was taken in\na city. While “city” might not be one of the labels we want\nto predict, it can still alter our perception about what might\nbe in the image. In this setting, we append any potential\nextra labels, denoted ye, to the target label set yt. If there\nare ℓt target labels, and ℓe potential extra labels, we now\nhave a set of ℓt + ℓe total labels that we train the model to\n2\nFigure 3. C-Tran architecture and illustration of label mask training for general multi-label image classiﬁcation. In this training image,\nthe labels person, umbrella, and sunglasses were randomly masked out and used as the unknown labels, yu. The labels rain coat and\ntruck are used as the known labels, yk. Each unknown label is added the unknown state embedding U, and each known label is added its\ncorresponding state embedding: negative (N) , or positive (P). The loss function is only computed on the unknown label predictions ˆ yu.\npredict. y now represents the concatenation of all target and\nextra labels. During inference, the known labels, ye\nk, come\nfrom the set of extra labels, but we are only interested in\nevaluating the unknown target labels yt\nu. In other words,\nduring inference, we want to compute the following: ˆyt\nu =\nf(x,ye\nk). Again, we assume that all training images are\nfully annotated with their correct target and extra labels.\n3. Method: C-Tran\nConsidering the three inference settings described, we\npropose Classiﬁcation Transformers (C-Tran), a general\nmulti-label classiﬁcation framework that works in all three.\nDuring inference, our method predicts a set of unknown la-\nbels yu given an input imagex and a set of known labelsyk.\nIn regular inference no labels are known, in partial label in-\nference some labels are known, and in extra label inference\nsome labels external to the target set are known. In Sec-\ntions 3.1-3.3, we introduce the C-Tran architecture, and in\nSection 3.4, we explain the label mask training procedure.\n3.1. Feature, Label, and State Embeddings\nImage Feature Embeddings Z: Given input image x ∈\nRH×W×3, the feature extractor outputs a tensor Z ∈\nRh×w×d, where h,w, and d are the output height, width,\nand channel, respectively. We can then consider each vec-\ntor zi ∈Rd from Z, with i ranging from 1 to P (where\nP = h×w), to be representative of a subregion that maps\nback to patches in the original image space.\nLabel Embeddings L: For every image, we retrieve a set\nof label embeddings L = {l1,l2,..., lℓ}, li ∈Rd, which are\nrepresentative of the ℓpossible labels in y. Label embed-\ndings are learned from an embedding layer of size d×ℓ.\nAdding Label Knowledge via State Embeddings S: In\ntraditional architectures, there is no way to encode partially\nknown or extra labels as input to the model. To address\nthis drawback, we propose a technique to easily incorporate\nsuch information. Given label embedding li, we simply add\na “state” embedding vector,si ∈Rd:\n˜li = li + si, (1)\nwhere the si takes on one of three possible states: unknown\n(U), negative (N), or positive (P). For instance, if label yi\nis a known positive value prior to inference (meaning that\nwe have prior knowledge that the label is present in the im-\nage), si is the positive embedding, P. The state embeddings\nare retrieved from a learned embedding layer of size d×3,\nwhere the unknown state vector (U) is ﬁxed with all zeros.\nState embeddings enable a user to (1) not use any prior\ninformation by adding the unknown embedding, (2), use\npartially labeled or extra information by adding the nega-\ntive and positive embeddings to those labels, and (3) easily\ntest interventions in the model by asking “how does the pre-\ndiction change if set this label to positive (negative)?”. We\nnote that using prior information is completely optional as\ninput to our model during testing, enabling it to also ﬂexibly\nhandle the regular inference setting.\n3.2. Modeling Feature and Label Interactions with\na Transformer Encoder\nTo model the complex interactions between the image\nfeature and embeddings, we develop our model based on a\nTransformer [47]. Transformers have proven to be a power-\nful mechanism for capturing rich dependency information\nbetween variables. Our formulation lets us to easily input\nthe image feature and label embeddings jointly into a Trans-\nformer encoder. Transformer encoders are suitable because\nthey are order invariant, allowing for any type of dependen-\ncies between all features and labels to be learned.\n3\nLet H = {z1,..., zh×w,˜l1,...,˜lℓ}be the set of embed-\ndings that are input to the Transformer encoder. In Trans-\nformers, the importance, or weight, of embedding hj ∈H\nwith respect to hi ∈H is learned through “self-attention”.\nThe attention weight, αt\nij between embedding i and j is\ncomputed in the following manner. First, we compute a\nnormalized scalar attention coefﬁcient αij between embed-\ndings iand j. After computing the αij value for all iand j\npairs, we update each hi to h′\ni using a weighted sum of all\nembeddings followed by a nonlinear ReLU layer:\nαij = softmax\n(\n(Wqhi)⊤(Wkhj)/\n√\nd\n)\n(2)\n¯hi =\nM∑\nj=1\nαijWvhj (3)\nh′\ni = ReLU(¯hiWr + b1)Wo + b2. (4)\nwhere Wk is the key weight matrix,Wq is the query weight\nmatrix,Wv is the value weight matrix, Wr and Wo are\ntransformation matrices, and b1 and b2 are bias vectors.\nThis update procedure can be repeated for Llayers where\nthe updated embeddings h′\ni are fed as input to the succes-\nsive Transformer encoder layer. The learned weight matri-\nces {Wk,Wq,Wv,Wr,Wo}∈ Rd×d are not shared be-\ntween layers. We denote the ﬁnal output of the Transformer\nencoder after Llayers as H′= {z′\n1,..., z′\nh×w,l′\n1,..., l′\nℓ}.\n3.3. Label Inference Classiﬁer\nLastly, after feature and label dependencies are modeled\nvia the Transformer encoder, a classiﬁer makes the ﬁnal la-\nbel predictions. We use an independent feedforward net-\nwork (FFNi) for ﬁnal label embedding l′\ni. FFN i contains a\nsingle linear layer, where weight wc\ni for label iis a 1 ×d\nvector, and σis a simoid function:\nˆyi = FFNi(l′\ni) =σ\n(\n(wc\ni ·l′\ni) +bi\n)\n(5)\n3.4. Label Mask Training (LMT)\nState embeddings (Eq. 1) lets us easily incorporate\nknown labels as input to C-Tran. However, we want our\nmodel to be ﬂexible enough to handle any amount of known\nlabels during inference. To solve this problem, we intro-\nduce a novel training procedure called Label Mask Training\n(LMT) that forces the model to learn label correlations, and\nallows C-Tran to generalize to any inference setting.\nInspired by the Cloze task [44] and the BERT “masked\nlanguage model” [13] which learn semantic information by\npredicting missing words from their context, we implement\na similar procedure. During training, we randomly mask a\ncertain amount of labels, and use the ground truth of the\nother labels (via state embeddings) to predict the masked\nlabels. This differs from masked language model training\nin that we have a ﬁxed set of inputs (all possible labels) and\nwe randomly mask a subset of them for each sample.\nGiven that there are ℓpossible labels, the number of “un-\nknown” (i.e. masked) labels for a particular sample, n, is\nchosen at random between 0.25ℓand ℓ. Then, nunknown\nlabels, denoted yu, are sampled randomly from all possible\nlabels y. The unknown state embedding is added to each\nunknown label. The rest are “known” labels, denoted yk\nand the corresponding ground truth state embedding (pos-\nitive or negative) is added to each. We call these known\nlabels because the ground truth value is used as input to C-\nTran alongside the image. Our model predicts the unknown\nlabels yu, and binary cross entropy is used to update the\nmodel parameters.\nBy masking random amounts of unknown labels (and\ntherefore using random amounts of known labels) during\ntraining, the model learns many possible known label com-\nbinations. This allows the C-Tran to be used in any infer-\nence setting where there may be arbitrary amounts of known\ninformation.\nWe mask out at least 0.25ℓlabels for each training sam-\nples for several reasons. First, most masked language model\ntraining methods mask out around 15% of the words [13, 4].\nSecond, we want our model to be able to incorporate any-\nwhere from 0 to 0.75 ℓknown labels during inference. We\nassume that knowing more than 75% of the labels is an un-\nrealistic inference scenario.\nEssentially, our label mask training pipeline tries to min-\nimize the following loss approximately:\nL=\nNtr∑\nn=1\nEp(yk){CE(ˆy(n)\nu ,y(n)\nu )|yk}, (6)\nwhere CE represents the cross entropy loss function.\nEp(yk)(·|yk) denotes to calculate the expectation regarding\nthe probability distribution of known labels: yk.\n3.5. Implementation Details\nImage Feature Extractor. For fair comparisons, we use\nthe same image size and pretrained feature extractor as the\nprevious state-of-the-art in each setting. For all datasets ex-\ncept CUB, we use the ResNet-101 [20] pretrained on Im-\nageNet [12] as the feature extractor (for CUB, we use the\nsame as [23]). Since the output dimension of ResNet-101\nis 2048, we set our embedding size d as 2048. Following\n[8, 7], training are images resized to 640 ×640 and ran-\ndomly cropped to 576 ×576 with random horizontal ﬂips.\nTesting images are center cropped instead. The output of\nthe ResNet-101 model is an 18×18×dtensor, so there are a\ntotal of 324 feature embedding vectors, zi ∈Rd.\nTransformer Encoder. In order to allow a particular em-\nbedding to attend to multiple other embeddings (or multiple\ngroups), C-Tran uses 4 attention heads [47]. We use a L=3\nlayer Transformer with a residual layer [20] around each\nembedding update and layer norm [1].\n4\nOptimization. For training, Adam [22] is used as the op-\ntimizer with betas= (0.9,0.999) and weight decay= 0. We\ntrain the models with a batch size of 16 and a learning rate\nof 10−5. We use dropout [17] of p= 0.1 for regularization.\n4. Experimental Setup and Results\nIn the following subsections, we explain the datasets,\nbaselines, and results for the three multi-label classiﬁcation\ninference settings.\n4.1. Regular Inference\nDatasets. We use two large-scale regular multi-label clas-\nsiﬁcation datasets: COCO-80 and VG-500. COCO [35], is\na commonly used large scale dataset for multi-label classi-\nﬁcation, segmentation, and captioning. It contains 122,218\nimages containing common objects in their natural context.\nThe standard multi-label formulation for COCO, which we\ncall COCO-80, includes80 object class annotations for each\nimage. We use 82,081 images as training data and evaluate\nall methods on a test set consisting of 40,137 images. The\nVisual Genome dataset [25], contains 108,077 images with\nobject annotations covering thousands of categories. Since\nthe label distribution is very sparse, we only consider the\n500 most frequent objects and use the VG-500 subset intro-\nduced in [7]. VG-500 consists of 98,249 training images\nand 10,000 test images.\nBaselines and Metrics. For COCO-80, we compare to ten\nwell known multi-label classiﬁcation methods. For VG-500\nwe compare to four previous methods that used this dataset.\nReferencing previous works [9, 8, 7], we employ several\nmetrics to evaluate the proposed method and existing meth-\nods. Concretely, we report the average per-class precision\n(CP), recall (CR), F1 (CF1) and the average overall preci-\nsion (OP), recall (OR), F1 (OF1), under the setting that a\npredicted label is positive if the output probability is greater\nthan 0.5. We also report the mean average precision (mAP).\nA detailed explanation of the metrics are shown in the Ap-\npendix. For fair comparisons to previous works [16, 58], we\nalso consider the setting where we evaluate the Top-3 pre-\ndicted labels following. In general, mAP, OF1, and CF1\nare the most important metrics [9].\nResults. C-Tran achieves state-of-the-art performance\nacross almost all metrics on both datasets, as shown in Ta-\nble 1 and Table 2. Considering that COCO-80 and VG-500\nare two widely studied multi-label datasets, absolute mAP\nincreases of 0.8 and 1.0, respectively, can be considered no-\ntable improvements. Importantly, we do not use any pre-\ndeﬁned feature and label relationship information (e.g. pre-\ntrained word embeddings). This signals that our method can\neffectively lean the relationships.\n4.2. Inference with Partial Labels\nDatasets. We use four datasets to validate our approach\nin the partial label setting. In all four datasets, we simu-\nlate four amounts of partial labels during inference. More\nspeciﬁcally, for each testing image, we select ϵ percent of\nlabels as known. ϵis set to 0% / 25% / 50% / 75% in our\nexperiments. ϵ=0% denotes no known labels, and is equiv-\nalent to the regular inference setting.\nIn addition to COCO-80 and VG-500, we benchmark\nour method on two more multi-label image classiﬁcation\ndatasets. Wang et al. [49] derived the top 1000 frequent\nwords from the accompanying captions of COCO images\nto use as target labels, which we call COCO-1000. There\nare 82,081 images for training, and 5,000 images for val-\nidation and testing, respectively. We expect that COCO-\n1000 provides more and stronger dependencies compared to\nCOCO-80. We also use the NEWS-500 dataset [49], which\nwas collected from the BBC News. Similar to COCO-1000,\nthe target label set consists of 500 most frequent nouns de-\nrived from image captions. There are 151,873 images for\ntraining, 10,304 for validation and 10,451 for testing.\nBaselines and Metrics. Feedback-prop [49] is an inference\nmethod introduced for partial label inference that make use\nof arbitrary amount of known labels. This method back-\npropagates the loss on the known labels to update the in-\ntermediate image representations during inference. We use\nthe LF method on ResNet-101 Convolutional Layer 13 from\n[49]. We compute the mean average precision (mAP) score\nof predictions on unknown labels.\nResults. As shown in Table 3, C-Tran outperforms Feed-\nbackprop, in all ϵpercentages of partially known labels on\nall datasets. In addition, as the percentage of partial la-\nbels increases, the improvement of C-Tran over Feedback-\nprop also increases. These results demonstrate that our\nmethod can effectively leverage known labels and is very\nﬂexible with the amount of known labels. Feedbackprop\nupdates image features which implicitly encode some no-\ntion of label correlation. C-Tran, instead, explicitly models\nthe correlations of between labels and features, leading to\nimproved results especially when partial labels are known.\nOn the other hand, Feedback-prop requires careful hyperpa-\nrameter tuning on a separate validation set and needs time-\nconsuming iterative feature updates. Our method does not\nrequire any hyerparameter tuning and just needs a standard\none-pass inference. We include qualitative examples in Ap-\npendix, demonstrating that effectiveness of our method.\n4.3. Inference with Extra Labels\nDatasets. For the extra label setting, we use the Caltech-\nUCSD Birds-200-2011 (CUB) dataset [53]. It contains\n9,430 training samples and 2,358 testing samples. We con-\nduct a multi-classiﬁcation task with 200 bird species on this\n5\nAll Top 3\nmAP CP CR CF1 OP OR OF1 CP CR CF1 OP OR OF1\nCNN-RNN [48] 61.2 - - - - - - 66.0 55.6 60.4 69.2 66.4 67.8\nRNN-Attention [50] - - - - - - - 79.1 58.7 67.4 84.0 63.0 72.0\nOrder-Free RNN [6] - - - - - - - 79.1 58.7 67.4 84.0 63.0 72.0\nML-ZSL [32] - - - - - - - 74.1 64.5 69.0 - - -\nSRN [58] 77.1 81.6 65.4 71.2 82.7 69.9 75.8 85.2 58.8 67.4 87.4 62.5 72.9\nResNet101 [20] 77.3 80.2 66.7 72.8 83.9 70.8 76.8 84.1 59.4 69.7 89.1 62.8 73.6\nMulti-Evidence [16] - 80.4 70.2 74.9 85.2 72.5 78.4 84.5 62.2 70.6 89.1 64.3 74.7\nML-GCN [9] 83.0 85.1 72.0 78.0 85.8 75.4 80.3 89.2 64.1 74.6 90.5 66.5 76.7\nSSGRL [8] 83.8 89.9 68.5 76.8 91.3 70.8 79.7 91.9 62.5 72.7 93.8 64.1 76.2\nKGGR [7] 84.3 85.6 72.7 78.6 87.1 75.6 80.9 89.4 64.6 75.0 91.3 66.6 77.0\nC-Tran 85.1 86.3 74.3 79.9 87.7 76.5 81.7 90.1 65.7 76.0 92.1 71.4 77.6\nTable 1. Results of regular inference on COCO-80 dataset. The threshold is set to 0.5 to compute precision, recall and F1 scores (%). Our\nmethod consistently outperforms previous methods across multiple metrics under the settings of all and top-3 predicted labels. Best results\nare shown in bold. “-” denotes that the metric was not reported.\nAll Top 3\nmAP CP CR CF1 OP OR OF1 CP CR CF1 OP OR OF1\nResNet101[20] 30.9 39.1 25.6 31.0 61.4 35.9 45.4 39.2 11.7 18.0 75.1 16.3 26.8\nML-GCN [9] 32.6 42.8 20.2 27.5 66.9 31.5 42.8 39.4 10.6 16.8 77.1 16.4 27.1\nSSGRL [8] 36.6 - - - - - - - - - - - -\nKGGR [7] 37.4 47.4 24.7 32.5 66.9 36.5 47.2 48.7 12.1 19.4 78.6 17.1 28.1\nC-Tran 38.4 49.8 27.2 35.2 66.9 39.2 49.5 51.1 12.5 20.1 80.2 17.5 28.7\nTable 2. Results of regular inference on VG-500 dataset. All metrics and setups are the same as Table 1. Our method achieves notable\nimprovement over previous methods.\ndataset. Multi-class classiﬁcation is a speciﬁc instantiation\nof multi-label classiﬁcation, where the target classes are\nmutually exclusive. In other words, each image has only\none correct label. We use the processed CUB dataset from\nKoh et al. [23] where they include 112 extra labels related\nto bird species. We call this dataset CUB-312. They further\ncluster extra labels into 28 groups and use varying amounts\nof known groups at inference time. To make a fair com-\nparison, we consider four different amounts of extra label\ngroups for inference: 0 group (0%), 10 groups (36%), 15\ngroups (54%), and 20 groups (71%).\nBaselines and Metrics. Concept Bottleneck Models [23]\nincorporate the extra labels as intermediate labels ( “con-\ncepts” in the original paper). They construct bottleneck\nlayer to ﬁrst predict the extra labels, and then use those pre-\ndictions to predict bird species. I.e., if we letye be the extra\ninformation labels, [23] predicts the target class labels yt\nusing the following computation graph: x →ye →yt. We\nalso consider two baselines from [23]. The ﬁrst is standard\nmulti-layer perception model that does not use a bottleneck\nlayer. The second is a multi-task learning model that pre-\ndicts the target and concept labels jointly. For fair compari-\nson, we use the same feature extraction method for all meth-\nods, Inception-v3 [43]. Since the target task is multi-class,\nwe evaluate the target predictions using accuracy scores.\nResults. Table 4 shows that C-Tran achieves an improved\naccuracy over Concept Bottleneck models on the CUB-312\ntask when using any amount of extra label groups. Notably,\nthe multi-task learning model produces the best perform-\ning results when ϵ=0. However, it is not able to incorporate\nknown extra labels (i.e.,ϵ>0). C-Tran instead, consistently\nachieves the best performance. Additionally, we can test in-\nterventions, or counterfactuals, using C-Tran. For example,\n“grey beak” is one of the extra labels, and we can set the\nstate embedding of “grey beak” to be positive or negative\nand observe the change in bird class predictions. We pro-\nvide samples of extra label intervention in the Appendix.\n4.4. Ablation and Model Analysis\nIn this section, we conduct ablation studies to analyze the\ncontributions of each C-Tran component. We examine two\nsettings: regular inference (equivalent to 0% known partial\nlabels) and 50% known partial label inference. We evalu-\nate on four datasets: COCO-80, VG-500, NEWS-500, and\nCOCO-1000. First, we remove the image features Z and\npredict unknown labels given only known labels. This ex-\nperiment, C-Tran (no image), tells us how much informa-\ntion model can learn just from labels. Table 5 shows that we\nget relatively high mean average precision scores on some\ndatasets (NEWS-500 and COCO-1000). This indicates that\n6\nCOCO-80 VG-500 NEWS-500 COCO-1000\nPartial Labels Known (ϵ) 0% 25% 50% 75% 0% 25% 50% 75% 0% 25% 50% 75% 0% 25% 50% 75%\nFeedbackprop [49] 80.1 80.6 80.8 80.9 29.6 30.1 30.8 31.6 14.7 21.1 23.7 25.9 29.2 30.1 31.5 33.0\nC-Tran 85.1 85.2 85.6 86.0 38.4 39.3 40.4 41.5 18.1 29.7 35.5 39.4 34.3 35.9 37.4 39.1\nTable 3. Results of inference with partial labels on four multi-label image classiﬁcation datasets. Mean average precision score (%) is\nreported. Across four simulated settings where different amounts of partial labels are available ( ϵ), our method signiﬁcantly outperforms\nthe competing method. With more partial labels available, we achieve larger improvement.\nExtra Label Groups Known (ϵ) 0% 36% 54% 71%\nStandard [23] 82.7 82.7 82.7 82.7\nMulti-task [23] 83.8 83.8 83.8 83.8\nConceptBottleneck [23] 80.1 87.0 93.0 97.5\nC-Tran 83.8 90.0 97.0 98.0\nTable 4. Results of inference with extra labels on CUB-312\ndataset. We report the accuracy score (%) for the 200 multi-class\ntarget labels. We achieve similar or greater accuracy than the base-\nlines across all amounts of known extra label groups.\neven without image features, C-Tran is able to effectively\nlearn rich dependencies from label annotations .\nSecond, we remove the label mask training procedure to\ntest the effectiveness of this technique. More speciﬁcally,\nwe remove all label state embeddings, S; thus all labels are\nunknown during training. Table 5 shows that for both set-\ntings, regular (0%) and 50% partial labels known, the per-\nformance drops without label mask training. This signiﬁes\ntwo critical ﬁndings of label mask training: (1) it helps with\ndependency learning as we see improvement when no par-\ntial labels are available during inference. This is particu-\nlarly true for datasets that have strong label co-occurrences,\nsuch as NEWS-500 and COCO-1000. (2) given partial la-\nbels, it can signiﬁcantly improve prediction accuracy. We\nprovide a t-SNE plot [36] of the label embeddings learned\nwith or without label mask training. As revealed in Fig-\nure 4, embeddings learned with label mask training show\nmore meaningful semantic topology; objects belonging to\nthe same group are clustered together.\nWe also analyze the importance of the number of Trans-\nformer layers, L, in the regular inference setting for COCO-\n80. Mean average precision scores for 2, 3, and 4 layers\nwere 85.0, 85.1, and 84.3, respectively. This indicates : (1)\nour method is fairly robust to the number of Transformer\nlayers, (2) multi-label classiﬁcation likely doesn’t require\na very large number of layers like some other NLP tasks,\nwhich use 96 layers [4]. While we show C-Tran is a pow-\nerful method in many multi-label classiﬁcation settings, we\nrecognize that Transformer layers are memory-intensive for\na large number of inputs. This limits the number of pos-\nsible labels ℓ in our model. Using four NVIDIA Titan X\nGPUs, the upper bound of ℓ is around 2000 labels. How-\never, it is possible to increase the number of labels. We cur-\nPartial Labels\nKnown (ϵ)\nCOCO-80 VG-500 NEWS-500 COCO-1000\n0% 50% 0% 50% 0% 50% 0% 50%\nC-Tran (no image) 3.60 21.7 2.70 24.6 6.50 33.3 1.50 27.8\nC-Tran (no LMT) 84.8 85.0 38.3 38.8 16.9 17.1 33.1 34.0\nC-Tran 85.1 85.6 38.4 40.4 18.1 35.5 34.3 37.4\nTable 5. C-Tran component ablation results. Mean average pre-\ncision score (%) is reported. Our proposed Label Mask Training\ntechnique (LMT) improves the performance, especially when par-\ntial labels are available.\nrently use the ResNet-101 output channel size ( d = 2048)\nfor our Transformer hidden layer size. This can be linearly\nmapped to a smaller number. Additionally, we could apply\none of the Transformer variations that have been proposed\nto model very large input sizes [10, 42].\n5. Related Work\nMulti-label Image Classiﬁcation. Multi-label classiﬁca-\ntion (MLC) is gaining popularity due to its relevance in real\nworld applications. Recently, [40] showed that the remain-\ning error in ImageNet is not due to the feature extraction,\nbut rather that ImageNet is annotated with single labels even\nwhen some images depict more than one object.\nRecent literature addressing multi-label classiﬁcation\nroughly fall into four groups. (1) Conditional Prediction:\nThe ﬁrst type, autoregressive models [11, 39, 48, 37] es-\ntimate the true joint probability of output labels given the\ninput by using the chain rule, predicting one label at a time.\n(2) Shared Embedding Space: The second group learns to\nproject input features and output labels into a shared latent\nembedding space [57, 3]. (3) Structured Output: The third\nkind describes label dependencies using structured output\ninference formulation [28, 45, 2, 18, 33, 34, 38]. (4) La-\nbel Graph Formulation: Several recent studies [9, 30, 8, 7]\nused graph neural networks to model label dependency\nan obtained state-of-the-art results. All methods relied\non knowledge-based graphs being built from label co-\noccurrence statistics. Our proposed model is most similar\nto (4), but it does not need extra knowledge to build a graph\nand can automatically learn the label dependency.\nInference with Partial Labels Wang et al. proposed feed-\nbackprop, a new inference strategy to handle any set of par-\ntial labels at test time [49]. The core idea is to optimize\n7\nNo LMT\nLMT\nFigure 4. Comparison of the learned label embeddings for COCO-80 using t-SNE. The left ﬁgure shows the embedding projections without\nusing label mask training (LMT), and the right shows with LMT. Labels are colored using the COCO object categorization. We can see\nthat using label mask training produces much semantically stronger label representations.\nintermediate image representations according to known la-\nbels and then predict the unknown labels based on the up-\ndated representations. However, this requires many itera-\ntions at inference time, resulting in a signiﬁcantly slower\nclassiﬁer. Additionally, the model is never exposed to par-\ntial evidence during training, which limits the potential im-\nprovement. Several methods [24, 21] utilize partial labels\nusing a ﬁxed set of labels. However, these cannot generalize\nto arbitrary sets of known labels. In more realistic inference\nsettings, there may be any subset of known labels available\nduring inference. If there are ℓtotal labels, then the number\nof known labels, n=|yk|ranges from 0 to ℓ-1. The number\nof possible known label sets is then\n(ℓ\nn\n)\n. C-Tran, instead in-\ntegrates a novel representation indicating each label state as\npositive, negative or unknown. This representation enables\nus to leverage partial signals into the model training, and\nmake our model compatible with any known label set dur-\ning inference. Notably, C-Tran is the ﬁrst learning method\nthat can exploit arbitrary amounts of partial evidence during\nboth training and inference.\nMany works tackle the problem of partial label multi-\nlabel classiﬁcation training [54, 14, 27]. While this sounds\nsimilar to our setting, there are several key distinctions.\nFirst, these methods focus on the case of “partial annota-\ntions”, which means that they assume not all labels are an-\nnotated correctly during training. We assume that all labels\nare correctly annotated during training. Second, partial la-\nbel training methods cannot be easily extended to the partial\nlabel inference setting. In other words, these methods can\njust take images as input and fail to incorporate extra useful\ninformation (partial/extra labels) during inference.\nInference with Extra Labels [23] introduces Concept Bot-\ntleneck Models which incorporates intermediate concept la-\nbels as a bottleneck layer for the target label classiﬁcation.\nSimilar to [24], this model assumes that the concept la-\nbels are a ﬁxed set. While interpretability is an advantage,\nbottleneck models [29, 26] rely on the assumption that the\nmanually curated concepts are sufﬁcient features for target\nclass prediction, contradicting the feature learning approach\nof deep learning. C-Tran, uses state embeddings instead\nof a concept bottleneck layer to represent each concept as\nknown (positive or negative) or unknown. This represen-\ntation enables C-Tran to leverage partial labels (concepts)\nduring training, and make our model compatible with any\nknown labels (concepts) during inference. Importantly, we\ndo not have any assumptions of the size of labels (concepts)\nto be known during inference.\n6. Conclusion\nThis paper proposes a novel deep learning method, called\nC-Tran, for a wide variety of “multi-label image classiﬁca-\ntion” applications. Our approach is easy to implement, re-\nquires no extra resources, and can effectively leverage any\namount of partial or extra labels during inference. C-Tran\nlearns sample-adaptive interactions through attention and\ncan discover how the labels attend to different parts of an in-\nput image. We showcase the effectiveness of our approach\nin regular multi-label classiﬁcation settings and multi-label\nclassiﬁcation with partially observed or extra labels. C-Tran\noutperforms all state-of-the-art methods in all scenarios. We\nfurther provide a quantitative and qualitative analysis show-\ning that C-Tran boosts the performance by explicitly mod-\neling the interactions between target labels and between im-\nage features and target labels. As the next steps, we plan to\nextend C-Tran to hierarchical scene categorization applica-\ntions. We also plan to explore the design of better training\nstrategies to make C-Tran generalize to settings where some\nlabels have never been observed in training.\n8\nAcknowledgements This work was partly supported by\nthe National Science Foundation under NSF CAREER\naward No. 1453580 to Y .Q. and a Leidos gift award to\nV .O. Any opinions, ﬁndings and conclusions or recommen-\ndations expressed in this material are those of the author(s)\nand do not necessarily reﬂect those of the National Science\nFoundation.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 4\n[2] David Belanger, Bishan Yang, and Andrew McCallum. End-\nto-end learning for structured prediction energy networks.\nIn Proceedings of the 34th International Conference on\nMachine Learning-Volume 70, pages 429–439. JMLR. org,\n2017. 7\n[3] Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik\nVarma, and Prateek Jain. Sparse local embeddings for ex-\ntreme multi-label classiﬁcation. In Advances in neural infor-\nmation processing systems, pages 730–738, 2015. 7\n[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 4, 7\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020. 11\n[6] Shang-Fu Chen, Yi-Chen Chen, Chih-Kuan Yeh, and Yu-\nChiang Frank Wang. Order-free rnn with visual attention for\nmulti-label classiﬁcation. arXiv preprint arXiv:1707.05495,\n2017. 1, 6\n[7] Tianshui Chen, Liang Lin, Xiaolu Hui, Riquan Chen, and\nHefeng Wu. Knowledge-guided multi-label few-shot learn-\ning for general image recognition. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2020. 4, 5, 6, 7\n[8] Tianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, and\nLiang Lin. Learning semantic-speciﬁc graph represen-\ntation for multi-label image recognition. arXiv preprint\narXiv:1908.07325, 2019. 4, 5, 6, 7\n[9] Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen\nGuo. Multi-Label Image Recognition with Graph Convo-\nlutional Networks. In The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019. 1, 5, 6, 7\n[10] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,\nQuoc V Le, and Ruslan Salakhutdinov. Transformer-xl:\nAttentive language models beyond a ﬁxed-length context.\narXiv preprint arXiv:1901.02860, 2019. 7\n[11] Krzysztof Dembczynski, Weiwei Cheng, and Eyke\nH¨ullermeier. Bayes optimal multilabel classiﬁcation via\nprobabilistic classiﬁer chains. In ., 2010. 7\n[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale Hierarchical Image Database. In\nCVPR09, 2009. 4\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 1, 4, 11\n[14] Thibaut Durand, Nazanin Mehrasa, and Greg Mori. Learn-\ning a deep convnet for multi-label classiﬁcation with partial\nlabels. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 647–657, 2019. 2, 8\n[15] Andr ´e Elisseeff and Jason Weston. A kernel method for\nmulti-labelled classiﬁcation. In Advances in neural infor-\nmation processing systems, pages 681–687, 2002. 1\n[16] Weifeng Ge, Sibei Yang, and Yizhou Yu. Multi-evidence\nﬁltering and fusion for multi-label classiﬁcation, object de-\ntection and semantic segmentation based on weakly super-\nvised learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 1277–\n1286, 2018. 5, 6\n[17] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep\nlearning. MIT press, 2016. 5\n[18] Yuhong Guo and Suicheng Gu. Multi-label classiﬁca-\ntion using conditional dependency networks. In IJCAI\nProceedings-International Joint Conference on Artiﬁcial In-\ntelligence, volume 22, page 1300, 2011. 7, 12\n[19] Kazuyuki Hara, Daisuke Saitoh, and Hayaru Shouno. Anal-\nysis of dropout learning regarded as ensemble learning.\nIn International Conference on Artiﬁcial Neural Networks .\nSpringer, 2016. 12\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 4, 6\n[21] Hexiang Hu, Guang-Tong Zhou, Zhiwei Deng, Zicheng\nLiao, and Greg Mori. Learning structured inference neural\nnetworks with label relations. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 2960–2968, 2016. 8\n[22] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014. 5\n[23] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen\nMussmann, Emma Pierson, Been Kim, and Percy\nLiang. Concept bottleneck models. arXiv preprint\narXiv:2007.04612, 2020. 4, 6, 7, 8\n[24] Michal Koperski, Tomasz Konopczynski, Rafal Nowak, Pi-\notr Semberecki, and Tomasz Trzcinski. Plugin networks for\ninference under partial evidence. In The IEEE Winter Con-\nference on Applications of Computer Vision , pages 2883–\n2891, 2020. 8\n[25] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and\nLi Fei-Fei. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. International\nJournal of Computer Vision, 123:32–73, 2016. 5\n[26] Neeraj Kumar, Alexander C Berg, Peter N Belhumeur, and\nShree K Nayar. Attribute and simile classiﬁers for face veriﬁ-\ncation. In 2009 IEEE 12th international conference on com-\nputer vision, pages 365–372. IEEE. 8\n9\n[27] Kaustav Kundu and Joseph Tighe. Exploiting weakly super-\nvised visual patterns to learn from partial annotations. Ad-\nvances in Neural Information Processing Systems, 33, 2020.\n2, 8\n[28] John Lafferty, Andrew McCallum, and Fernando CN Pereira.\nConditional random ﬁelds: Probabilistic models for seg-\nmenting and labeling sequence data. ., 2001. 7\n[29] Christoph H Lampert, Hannes Nickisch, and Stefan Harmel-\ning. Learning to detect unseen object classes by between-\nclass attribute transfer. In 2009 IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 951–958. IEEE,\n2009. 8\n[30] Jack Lanchantin, Arshdeep Sekhon, and Yanjun Qi. Neu-\nral message passing for multi-label classiﬁcation. In Joint\nEuropean Conference on Machine Learning and Knowledge\nDiscovery in Databases, pages 138–163. Springer, 2019. 7\n[31] Jack Lanchantin, Arshdeep Sekhon, and Yanjun Qi. Neu-\nral message passing for multi-label classiﬁcation. ECML,\nabs/1904.08049, 2019. 12\n[32] Chung-Wei Lee, Wei Fang, Chih-Kuan Yeh, and Yu-Chiang\nFrank Wang. Multi-label zero-shot learning with structured\nknowledge graphs. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages 1576–\n1585, 2018. 1, 6\n[33] Qiang Li, Maoying Qiao, Wei Bian, and Dacheng Tao. Con-\nditional graphical lasso for multi-label image classiﬁcation.\nIn CVPR, pages 2977–2986, 06 2016. 7\n[34] Xin Li, Feipeng Zhao, and Yuhong Guo. Multi-label image\nclassiﬁcation with a probabilistic label enhancement model.\nIn Proceedings of the Thirtieth Conference on Uncertainty\nin Artiﬁcial Intelligence, UAI’14, pages 430–439, Arlington,\nVirginia, USA, 2014. AUAI Press. 7\n[35] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC. Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In ECCV, 2014. 5\n[36] Laurens van der Maaten and Geoffrey Hinton. Visualiz-\ning data using t-sne. Journal of machine learning research,\n9(Nov):2579–2605, 2008. 7\n[37] Jinseok Nam, Eneldo Loza Menc ´ıa, Hyunwoo J Kim, and\nJohannes F ¨urnkranz. Maximizing subset accuracy with re-\ncurrent neural networks in multi-label classiﬁcation. In Ad-\nvances in Neural Information Processing Systems , pages\n5419–5429, 2017. 7\n[38] Tejaswi Nimmagadda and Anima Anandkumar. Multi-object\nclassiﬁcation and unsupervised scene understanding using\ndeep learning features and latent tree probabilistic models.\narXiv preprint arXiv:1505.00308, 2015. 7\n[39] Jesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe\nFrank. Classiﬁer chains for multi-label classiﬁcation. Ma-\nchine Learning and Knowledge Discovery in Databases ,\npages 254–269, 2009. 7\n[40] Pierre Stock and Moustapha Cisse. Convnets and imagenet\nbeyond accuracy: Understanding mistakes and uncovering\nbiases. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), pages 498–512, 2018. 7\n[41] Hongyu Su and Juho Rousu. Multilabel classiﬁcation\nthrough random graph ensembles. In Asian Conference on\nMachine Learning, pages 404–418, 2013. 12\n[42] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski,\nand Armand Joulin. Adaptive attention span in transform-\ners. arXiv preprint arXiv:1905.07799, 2019. 7\n[43] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\ntecture for computer vision. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n2818–2826, 2016. 6\n[44] Wilson L Taylor. “cloze procedure”: A new tool for measur-\ning readability. Journalism quarterly, 30(4):415–433, 1953.\n4\n[45] Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-\nmann, and Yasemin Altun. Large margin methods for\nstructured and interdependent output variables. JMLR,\n6(Sep):1453–1484, 2005. 7\n[46] Grigorios Tsoumakas and Ioannis Katakis. Multi-label clas-\nsiﬁcation: An overview. International Journal of Data Ware-\nhousing and Mining, 3(3), 2006. 1\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 6000–6010, 2017. 1,\n3, 4, 11\n[48] Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang\nHuang, and Wei Xu. Cnn-rnn: A uniﬁed framework for\nmulti-label image classiﬁcation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 2285–2294, 2016. 1, 6, 7\n[49] Tianlu Wang, Kota Yamaguchi, and Vicente Ordonez.\nFeedback-prop: Convolutional neural network inference un-\nder partial evidence. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), June 2018. 2, 5, 7\n[50] Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, and\nLiang Lin. Multi-label image recognition by recurrently dis-\ncovering attentional regions. In Proceedings of the IEEE in-\nternational conference on computer vision , pages 464–472,\n2017. 1, 6\n[51] Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, and\nLiang Lin. Multi-label image recognition by recurrently dis-\ncovering attentional regions. InThe IEEE International Con-\nference on Computer Vision (ICCV), Oct 2017. 11\n[52] Y . Wei, W. Xia, M. Lin, J. Huang, B. Ni, J. Dong, Y . Zhao,\nand S. Yan. Hcp: A ﬂexible cnn framework for multi-label\nimage classiﬁcation. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 38(9):1901–1907, Sep. 2016. 11\n[53] Peter Welinder, Steve Branson, Takeshi Mita, Catherine\nWah, Florian Schroff, Serge Belongie, and Pietro Perona.\nCaltech-ucsd birds 200. 2010. 5\n[54] Ming-Kun Xie and Sheng-Jun Huang. Partial multi-label\nlearning. In Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, 2018. 2, 8\n[55] Xiangyang Xue, Wei Zhang, Jie Zhang, Bin Wu, Jianping\nFan, and Yao Lu. Correlative multi-label multi-instance im-\nage annotation. In Proceedings of the 2011 International\n10\nConference on Computer Vision, ICCV ’11, pages 651–658,\nUSA, 2011. IEEE Computer Society. 12\n[56] Hao Yang, Joey Tianyi Zhou, Yu Zhang, Bin-Bin Gao,\nJianxin Wu, and Jianfei Cai. Exploit bounding box annota-\ntions for multi-label object recognition. In Lourdes Agapito,\nTamara Berg, Jana Kosecka, and Lihi Zelnik-Manor, edi-\ntors, Proceedings - 29th IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2016 , pages 280–288,\nUnited States of America, 2016. IEEE, Institute of Electrical\nand Electronics Engineers. 11\n[57] Chih-Kuan Yeh, Wei-Chieh Wu, Wei-Jen Ko, and Yu-\nChiang Frank Wang. Learning deep latent space for multi-\nlabel classiﬁcation. In AAAI, pages 2838–2844, 2017. 7\n[58] Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and\nXiaogang Wang. Learning spatial regularization with image-\nlevel supervisions for multi-label image classiﬁcation. 2017\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 2027–2036, 2017. 5, 6, 11\nA. Appendix\nA.1. Qualitative Examples\nInference with Partial Labels In Figure 5, we show\nqualitative results on COCO-80 demonstrating the use of\npartial labels. In these examples, we ﬁrst show the pre-\ndictions for ResNet-101, as well as C-Tran without using\npartial labels. The last column shows the C-Tran predic-\ntions when using ϵ= 25%partial labels (which is 21 labels\nfor COCO-80) as observed, or known prior to inference.\nFor many examples, certain labels cannot be predicted well\nwithout using partial labels.\nInference with Extra Labels In Figure 6, we show\nqualitative results on CUB-312 demonstrating the use of\nextra labels. In the CUB-312 dataset, the extra labels are\nhigh level concepts of bird species that are not target la-\nbels. In these examples, we ﬁrst show the predictions for\nC-Tran without using extra labels labels, and the last col-\numn shows the C-Tran predictions when using ϵ = 54%\nof the extra labels (which is 60 labels for CUB-312) as ob-\nserved, or known prior to inference. We can see that many\nbird species predictions are completely changed after using\nthe extra labels as input to our model.\nA.2. Detailed Diagram of C-Tran Settings\nIn Figure 7 shows a detailed diagram of all possible train-\ning and inference settings used in our paper, and how C-\nTran is used in each setting. By using the same random\nmask training, we can apply our model to any of the three\ninference settings.\nA.3. Multi-Label Classiﬁcation Metrics\nOP =\n∑\ni Nc\ni∑\ni Np\ni\nOR =\n∑\ni Nc\ni∑\ni Ng\ni\nOF1 =2 ×OP ×OR\nOP + OR\n(7)\nCP = 1\nC\n∑\ni\nNc\ni\nNp\ni\nCR = 1\nC\n∑\ni\nNc\ni\nNg\ni\nCF1 =2 ×CP ×CR\nCP + CR\n(8)\nwhere C is the number of labels, Nc\ni is true positives for\nthe i-th label, Np\ni is the total number of images for which\nthe i-th label is predicted, and Ng\ni is the number of ground\ntruth images for the i-th label.\nA.4. More Discussions of C-Tran\nConnecting to Transformers and BERT\nOur proposed method, C-Tran, draws much inspiration\nfrom works in natural language processing. The trans-\nformer model [47] proposed “self attention” for natural lan-\nguage translation. Self attention allows each word in the tar-\nget sentence to attend to all other words (both in the source\nsentence and the target sentence) for translation. [13] intro-\nduced BERT for language modeling. BERT uses self atten-\ntion with masked words to pretrain a language model.\nSelf attention and BERT are both examples of complete\ngraphs, but on sentences rather than image features and la-\nbels. C-Tran uses the same self-attention mechanisms as\n[47] and [13], but instead of using only the word embed-\ndings from a sentence, we use feature and label embed-\ndings.\nIn computer vision, [5] used Transformers for object de-\ntection. Our method varies in several distinct ways. First,\nwe are primarily interested in using partial evidence for im-\nage classiﬁcation, and our unique state embeddings allow\nC-Tran to use such evidence. Second, we model image\nand label features jointly in a Transformer encoder, whereas\n[5] use an encoder/decoder framework. Our method allows\nthe image features to be updated conditioned on the labels,\nwhich is a key characteristic of our model.\nConnecting to Graph Based Neural Relational Learn-\ning Another line of recent works employ object localization\ntechniques[56, 52] or attention mechanism[51, 58] to locate\nsemantic meaningful regions and try to identify underly-\ning relations between regions and outputs. However, these\nmethods either require expensive bounding box annotations\n11\nImages True\nLabels ResNet-101  C-Tran C-Tran + partial labels\nID:000000362831\nfork\nknife,\nspoon,\nbowl,\nchair, \ndiningtable\nfork,\nsandwich,\ndiningtable,\nspoon,\ncup\nfork,\n knife, \ndiningtable,\nperson,\ncake\nspoon=1, \ntrafficlight=0, \nbench=0, \ndog=0,\n...\nfork,\n knife,\ndiningtable,\nperson,\nbowl\nID:000000106216\nperson, \ncar,\ntruck, \nparkingmeter, \nhorse,\nperson,\ncar,\ntruck,\nhorse,\nbicycle\ncar,\n person,\ntruck,\nhorse,\nbicycle\nbicycle=0, \nmotorcycle=0, \ntrain=0, \nboat=0\n...\ncar,\n person,\n truck,\n horse,\nparkingmeter\nID:000000243213\nperson, \nbench, \nbackpack, \ntennisracket, \nbottle,\nchair\nperson,\ntennisracket,\nchair,\ntie,\nsportsball\nperson,\ntennisracket,\nchair, \nsportsball,\nbench\nbackpack=1 \nparkingmeter=0, \nbird=0,\n zebra=0, \n...\nperson,\ntennisracket, \nchair,\nbottle,\n bench\nID:000000170129\nairplane, \ntrain\nairplane,\nboat,\ncar,\ntruck,\nperson\nairplane,\nboat,\nperson,\ncar,\nbird\ncar=0, \nmotorcycle=0, \nbus=0, \ntruck=0,\n...\nairplane, \nboat,\n person,\n bird, \ntrain\nID: 000000262896\nbottle, \nspoon, \ndiningtable, \ncellphone, \nbook\nbottle,\nfork,\ndiningtable, \nbowl,\nspoon\nfork,\n spoon,\nbowl, \nbook, \ndiningtable\ndiningtable=1, \nbicycle=0, \ncar=0, \ntruck=0, \n...\nspoon,\n bowl, \nbook,\nbottle, \ncellphone\nFigure 5. Qualitative examples of C-Tran + partial labels on the COCO-80 dataset. In the last column, we useϵ= 25%partial labels, some\nof which are shown. Correctly predicted labels are in bold.\nor merely get regions of interest roughly due to the lack of\nlabel supervision. One recent study by [55] also showed\nthat modeling the associations between image feature re-\ngions and labels helps to improve multi-label performance.\nIn our work, C-Tran uses graph attentions and enables each\ntarget label to attend differentially to relevant parts of an\ninput image.\nFor multi-label classﬁcation(MLC), [18] formulate MLC\nusing a label graph and they introduced a conditional de-\npendency SVM where they ﬁrst trained separate classiﬁers\nfor each label given the input and all other true labels and\nused Gibbs sampling to ﬁnd the optimal label set. The main\ndrawback is that this method requires separate classiﬁers for\neach label. [41] proposes a method to label the pairwise\nedges of randomly generated label graphs, and requires\nsome chosen aggregation method over all random graphs.\nThe authors introduce the idea that variation in the graph\nstructure shifts the inductive bias of the base learners. One\nrecent study [31] used graph neural networks for multi-label\nclassiﬁcation on sequential inputs. The proposed method\nmodels the label-to-label dependencies using GNNs, how-\never, does not represent input features and labels in one co-\nherent graph. A key aspect of C-Tran is that the Transformer\nencoder can be viewed as a fully connected graph which is\nable to learn any relationships between features and labels.\nThe Transformer attention mechanism can be regarded as a\nform of graph ensemble learning [19]. Above all, previous\nmethods using graphs to model label dependencies do not\nallow for partial evidence information to be included in the\nprediction.\n12\nImages True \nLabel  C-Tran C-Tran + Extra Labels\nAnna \nHummingbird\nRufous \nHummingbird (96%)\nhas_bill_shape_needle = 1,\nhas_wing_color_green=1,\nhas_upperparts_color=green=1,\nhas_back_color_blue=0,\nhas_back_color_brown=0\n...\nAnna\nHummingbird (99%)\nAnna_Hummingbird_0080_56366 \nBlue Jay\nFlorida Jay (99%)\nhas_bill_shape_all-purpose=1,\nhas_upperparts_color_buff=1,\nhas_upper_tail_color_grey=1,\nhas_belly_color_red=0,\nhas_wing_shape_broad-wings=0\n...\nBlue Jay (99%)\nBlue_Jay_0072_62944 \nBlue Winged \nWarbler\nYellow Headed \nBlackbird (99%)\nhas_upperparts_color_grey=1,\nhas_tail_shape_rounded_tail=1,\nhas_upper_tail_color_black=1,\nhas_back_color_iridescent=0,\nhas_underparts_color_purple=0\n...\nBlue Winged \nWarbler (99%)\nBlue_Winged_Warbler_0057_162085 \nFigure 6. Qualitative examples of C-Tran + extra labels on the CUB-312 dataset. In the last column, we useϵ= 54%extra labels, some of\nwhich are shown.\nFigure 7. Detailed example of the general training method and three different inference settings where C-Tran can be applied.\n13"
}