{
  "title": "Graph Neural Network and Spatiotemporal Transformer Attention for 3D Video Object Detection From Point Clouds",
  "url": "https://openalex.org/W3212953023",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2385717340",
      "name": "Yin, Junbo",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2388015377",
      "name": "Shen, Jianbing",
      "affiliations": [
        "University of Macau",
        "City University of Macau"
      ]
    },
    {
      "id": "https://openalex.org/A255292296",
      "name": "Gao Xin",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3196086213",
      "name": "Crandall, David",
      "affiliations": [
        "Indiana University Bloomington"
      ]
    },
    {
      "id": "https://openalex.org/A2377678696",
      "name": "Yang, Ruigang",
      "affiliations": [
        "University of Kentucky"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963091558",
    "https://openalex.org/W6677326919",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2987761193",
    "https://openalex.org/W6746472748",
    "https://openalex.org/W2966926453",
    "https://openalex.org/W6640212811",
    "https://openalex.org/W6685350579",
    "https://openalex.org/W3109675406",
    "https://openalex.org/W6685670348",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2897529137",
    "https://openalex.org/W6729508183",
    "https://openalex.org/W2998633559",
    "https://openalex.org/W3034479628",
    "https://openalex.org/W2962807143",
    "https://openalex.org/W2953941229",
    "https://openalex.org/W2802979841",
    "https://openalex.org/W2888096830",
    "https://openalex.org/W2605659599",
    "https://openalex.org/W2290847742",
    "https://openalex.org/W6736685754",
    "https://openalex.org/W2963020213",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W6738964360",
    "https://openalex.org/W6690815549",
    "https://openalex.org/W6757634740",
    "https://openalex.org/W2987391422",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W3034782805",
    "https://openalex.org/W2963727135",
    "https://openalex.org/W3034239557",
    "https://openalex.org/W2968296999",
    "https://openalex.org/W3114753236",
    "https://openalex.org/W6779712747",
    "https://openalex.org/W6713582119",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6620673361",
    "https://openalex.org/W6637178625",
    "https://openalex.org/W2963125977",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3017930107",
    "https://openalex.org/W2150066425",
    "https://openalex.org/W3034467781",
    "https://openalex.org/W3117901461",
    "https://openalex.org/W3035574168",
    "https://openalex.org/W3035750285",
    "https://openalex.org/W2158787690",
    "https://openalex.org/W6720006811",
    "https://openalex.org/W6767379092",
    "https://openalex.org/W3107422826",
    "https://openalex.org/W3209005318",
    "https://openalex.org/W6785213549",
    "https://openalex.org/W2949708697",
    "https://openalex.org/W6775495027",
    "https://openalex.org/W2798965597",
    "https://openalex.org/W3035461736",
    "https://openalex.org/W3134233478",
    "https://openalex.org/W2981949127",
    "https://openalex.org/W3209035582",
    "https://openalex.org/W3034407526",
    "https://openalex.org/W3108884723",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2468368736",
    "https://openalex.org/W3035346742",
    "https://openalex.org/W2954174912",
    "https://openalex.org/W2980547801",
    "https://openalex.org/W3034314779",
    "https://openalex.org/W6763422710",
    "https://openalex.org/W2555618208",
    "https://openalex.org/W2746726611",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2558834163",
    "https://openalex.org/W2950898568",
    "https://openalex.org/W3107819843",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2964113829",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3100157108",
    "https://openalex.org/W3098325765",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3167095230",
    "https://openalex.org/W2964145825",
    "https://openalex.org/W2952915411",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2964321699",
    "https://openalex.org/W2612364175",
    "https://openalex.org/W2963212638",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W3014234426",
    "https://openalex.org/W2999556989",
    "https://openalex.org/W2116435618",
    "https://openalex.org/W637153065",
    "https://openalex.org/W3119114344",
    "https://openalex.org/W1662382123",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W2963149042",
    "https://openalex.org/W2969987486"
  ],
  "abstract": "Previous works for LiDAR-based 3D object detection mainly focus on the single-frame paradigm. In this paper, we propose to detect 3D objects by exploiting temporal information in multiple frames, i.e., point cloud videos. We empirically categorize the temporal information into short-term and long-term patterns. To encode the short-term data, we present a Grid Message Passing Network (GMPNet), which considers each grid (i.e., the grouped points) as a node and constructs a k-NN graph with the neighbor grids. To update features for a grid, GMPNet iteratively collects information from its neighbors, thus mining the motion cues in grids from nearby frames. To further aggregate long-term frames, we propose an Attentive Spatiotemporal Transformer GRU (AST-GRU), which contains a Spatial Transformer Attention (STA) module and a Temporal Transformer Attention (TTA) module. STA and TTA enhance the vanilla GRU to focus on small objects and better align moving objects. Our overall framework supports both online and offline video object detection in point clouds. We implement our algorithm based on prevalent anchor-based and anchor-free detectors. Evaluation results on the challenging nuScenes benchmark show superior performance of our method, achieving first on the leaderboard (at the time of paper submission) without any \"bells and whistles.\" Our source code is available at https://github.com/shenjianbing/GMP3D.",
  "full_text": "IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 1\nGraph Neural Network and Spatiotemporal\nTransformer Attention for 3D Video Object\nDetection from Point Clouds\nJunbo Yin, Jianbing Shen, Senior Member, IEEE, Xin Gao, David Crandall, and Ruigang Y ang\nAbstract‚ÄîPrevious works for LiDAR-based 3D object detection mainly focus on the single-frame paradigm. In this paper, we propose\nto detect 3D objects by exploiting temporal information in multiple frames, i.e., the point cloud videos. We empirically categorize the\ntemporal information into short-term and long-term patterns. To encode the short-term data, we present a Grid Message Passing\nNetwork (GMPNet), which considers each grid ( i.e., the grouped points) as a node and constructs a k-NN graph with the neighbor\ngrids. To update features for a grid, GMPNet iteratively collects information from its neighbors, thus mining the motion cues in grids\nfrom nearby frames. To further aggregate the long-term frames, we propose an Attentive Spatiotemporal Transformer GRU (AST -GRU),\nwhich contains a Spatial Transformer Attention (STA) module and a Temporal Transformer Attention (TTA) module. STA and TTA\nenhance the vanilla GRU to focus on small objects and better align the moving objects. Our overall framework supports both online and\nofÔ¨Çine video object detection in point clouds. We implement our algorithm based on prevalent anchor-based and anchor-free detectors.\nThe evaluation results on the challenging nuScenes benchmark show the superior performance of our method, achieving the 1st on\nthe leaderboard without any bells and whistles, by the time the paper is submitted.\nIndex Terms‚ÄîPoint Cloud, 3D Video Object Detection, Autonomous Driving, Graph Neural Network, Transformer Attention.\n!\n1 I NTRODUCTION\nT\nHE past several years have witnessed an explosion of\ninterests in 3D object detection due to its vital role in\nautonomous driving perception. Meanwhile, the LiDAR sensor\nis becoming an indispensable instrument in 3D perception for\nits capacity of providing accurate depth information in intricate\nscenarios such as dynamic trafÔ¨Åc environments and adverse\nweather conditions. The majority of 3D object detectors are\ndedicated to detecting in single-frame LiDAR point clouds,\ni.e., predicting oriented 3D bounding boxes via frame-by-\nframe paradigm. However, little work has been devoted to\ndetecting in multi-frame point clouds sequences, i.e., point\ncloud videos. In the newly released nuScenes [2] dataset, a\npoint cloud video is deÔ¨Åned by a driving scene containing\naround 400 point cloud frames captured within 20 seconds. In\ngeneral, point cloud videos are readily available in practical\napplications and can provide rich spatiotemporal coherence.\nFor example, object point clouds from a single frame may\nbe truncated or sparse due to occlusions or long-distance\n‚Ä¢ J. Yin is with School of Computer Science, Beijing Institute of Technology,\nBeijing, China. (Email: yinjunbo@bit.edu.cn)\n‚Ä¢ J. Shen is with Inception Institute of ArtiÔ¨Åcial Intelligence, Abu Dhabi,\nUAE. (Email: shenjianbingcg@gmail.com)\n‚Ä¢ X. Gao is with Computer, Electrical, and Mathematical Sciences and\nEngineering (CEMSE) Division, King Abdullah University of Science and\nTechnology (KAUST), Thuwal, Saudi Arabia.\n‚Ä¢ D. Crandall is with the School of Informatics, Computing, and Engineering,\nIndiana University, Bloomington, IN 47408. (Email: djcran@indiana.edu)\n‚Ä¢ R. Yang is with the University of Kentucky, Lexington, KY 40507. (Email:\nryang@cs.uky.edu)\n‚Ä¢ A preliminary version of this work has appeared in CVPR 2020 [1].\n‚Ä¢ Corresponding author: Jianbing Shen\nT-2 T-1 T\nFig. 1: A representative challenging case in autonomous driving\nscenarios. A typical single-frame 3D object detector, eg. [3], leads\nto false-negative (FN) results due to occlusion (top row). In contrast,\nour 3D video object detector could address this (bottom row). The red\nand grey boxes denote the predictions and ground-truths, respectively.\nsampling, while other frames could contain complementary in-\nformation for recognizing the object. Therefore, a single-frame\n3D object detector suffers from deteriorated performance in\ncomparison with a 3D video object detector (See Fig. 1).\nExisting 3D video object detectors typically exploit the\ntemporal information in a straightforward way, i.e., directly\nconcatenating the point clouds from other frames to a refer-\nence frame and then performing detection in the reference\nframe [3, 4, 5]. This converts the video object detection\narXiv:2207.12659v1  [cs.CV]  26 Jul 2022\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2\ntask to a single-frame object detection task. However, such\na simple approach has several limitations. First, in driving\nscenarios, there will be serious motion blur that will result\nin inaccurate detection results. One solution might be to\napply ego-motion transformation for these frames. Though\nit can alleviate the ego-motion, it is not capable of reme-\ndying the motion blur caused by moving objects. Second,\nthis concatenation-based approach usually simply encodes the\ntemporal information by an additional point cloud channel\nsuch as timestamp, which ignores the rich feature relations\namong different frames. Third, as the label information is only\nprovided in the reference frame, such methods will suffer more\ninformation loss when leveraging long-term point cloud data.\nApart from the concatenation-based approach, other works [6]\napply temporal 3D convolutional networks on point cloud\nvideos. Nevertheless, they will encounter temporal information\ncollapse when aggregating features over multiple frames [8].\nHow to effectively exploit the temporal information in point\ncloud videos remains an open challenge. We argue that there\nare two forms of temporal information, i.e., short-term and\nlong-term patterns. Taking the nuScenes [2] dataset for exam-\nple, the short-term pattern is deÔ¨Åned by point cloud sequences\ncaptured within 0.5 seconds, including around 10 frames. As\nfor the long-term pattern, we refer to the point cloud frames\ncollected in 1 to 2 seconds involving dozens of frames. In\nthis work, we aim to present a more effective and general\n3D video object detection framework by enhancing prevalent\n3D object detectors with both the short-term and long-term\ntemporal cues. In particular, the long-term point clouds are\nÔ¨Årst divided into several short-term ones. We separately encode\neach short-term data with a short-term encoding module,\nand then adaptively fuse the output features by a long-term\naggregation module.\nFor handling the short-term point cloud data, our short-\nterm encoding module follows the paradigm of modern grid-\nbased detectors [3, 9, 5, 10] that directly concatenate point\nclouds from nearby frames to a reference frame, but adopts a\nnovel grid feature encoding strategy that models the relations\nof girds in nearby frames. More precisely, these detectors\ntypically divide the point clouds into regular girds such as\nvoxels and pillars, with each gird containing a Ô¨Åxed number\nof point clouds. Then PointNet-like modules ( e.g., the V oxel\nFeature Encoding Layer in [9] and the Pillar Feature Network\nin [3]) are used to extract the grid-wise feature representation.\nA potential problem of these PointNet-like modules is that\nthey only focus on the individual grid, which ignores the\nrelations among different grids. Intuitively, certain grids in\nnearby frames may encode the same object parts, which can\nbe explored to improve the detection accuracy. To this end,\nwe propose a graph-based network, Grid Message Passing\nNetwork (GMPNet), which iteratively propagates information\nover different grids. GMPNet treats each non-empty grid as a\ngraph node and builds a k-NN graph with the nearby k grids.\nThe relations among these grids as viewed edges. The core\nidea of GMPNet is to iteratively update the node features via\nthe neighbors along the edges and hence mine the rich relations\namong different grid nodes. At each iteration, a node re-\nceives pair-wise messages from its neighbors, aggregates these\nmessages and then updates its node features. After several\niterations, messages from distant grids can be accessed. This\nÔ¨Çexibly enlarges the receptive Ô¨Åeld in a non-Euclidean manner.\nCompared with previous PointNet-like modules, GMPNet ef-\nfectively encourages information propagation among different\ngrids, meanwhile capturing short-term motion cues of objects.\nAfter obtaining individual features extracted by the short-\nterm encoding module, we then turn to Convolutional Gated\nRecurrent Unit networks (ConvGRU [11]) to further capture\nthe dependencies over these features in our long-term aggre-\ngation module. ConvGRU has shown promising performance\nin the 2D video understanding Ô¨Åeld. It adaptively links tem-\nporal information over input sequences with a memory gating\nmechanism. In each time step, the current memory is com-\nputed by considering both the current input and the previous\nmemory features. The updated memory is then employed to\nperform the present task. However, there are two potential\nlimitations when directly applying ConvGRU on multi-frame\npoint cloud sequences. First, modern grid-based detectors tend\nto transform the point cloud features into the bird‚Äôs eye view,\nand the resultant object resolution is much smaller than that\nin 2D images. For example, with a grid size of 0.252 m2 and\nstride of 8, objects such as pedestrians and vehicles just occupy\naround 1 to 3 pixels in the output feature maps. Therefore,\nwhen computing the current memory features, the background\nnoise will be inevitably accumulated and decrease detection\nperformance. Second, the spatial features of the current input\nand the previous memory are not well aligned across frames.\nThough we could use the ego-pose information to align the\nstatic objects over multiple point cloud frames, the moving\nobjects still incur motion blur that impairs the quality when\nupdating the memory.\nTo address these challenges, we present Attentive Spa-\ntiotemporal Transformer GRU (AST-GRU), which enhances\nthe vanilla ConvGRU with a Spatial Transformer Attention\n(STA) module and a Temporal Transformer Attention (TTA)\nmodule. In particular, the STA module is derived from [12, 13]\nand is devised to attend the small objects with the spatial\ncontext information. It acts as an intra-attention mechanism,\nwhere both the keys and queries reside in the input features.\nSTA views each pixel as a query and the neighbors as keys.\nBy attending the query with the context keys, STA can better\ntell foreground pixels from the background ones, and thus\nemphasize the meaningful objects in current memory features.\nFurthermore, we also describe a TTA module motivated by\n[14, 15], which is employed to align the moving objects.\nIt is composed of modiÔ¨Åed deformable convolutional layers\nand behaves as an inter-attention that operates on both the\ninput and previous memory features. TTA treats each pixel\nin the previous memory as a query and decides the keys\nby integrating the motion information, thus capturing more\ncues of moving objects. In contrast to the vanilla ConvGRU,\nthe presented AST-GRU effectively improves the quality of\nthe current memory features and leads to better detection\naccuracy, which is veriÔ¨Åed on both prevalent anchor-based\nand anchor-free detectors. Moreover, this also introduces a\ngeneral methodology that accounts for both online and ofÔ¨Çine\n3D video object detection by Ô¨Çexibly deciding the inputs, e.g.,\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 3\nusing previous or later frames in a point cloud video. In the\nofÔ¨Çine mode, we further augment our AST-GRU with a bi-\ndirectional feature learning mechanism, which achieves better\nresults in comparison to the online mode.\nTo summarize, the main contributions of this work are as\nfollows:\n‚Ä¢ A general point cloud-based 3D video object detection\nframework is proposed by leveraging both short-term and\nlong-term point clouds information. The proposed frame-\nwork can easily integrate prevalent 3D object detectors\nin both online and ofÔ¨Çine modes, which provides new\ninsights to the community.\n‚Ä¢ We present a novel graph neural network, named GMP-\nNet, to encode short-term point clouds. GMPNet can\nÔ¨Çexibly mine the relations among different grids in nearby\nframes and thus capture the motion cues.\n‚Ä¢ To further model the long-term point clouds, an AST-\nGRU module is introduced to equip the conventional\nConvGRU with an attentive memory mechanism, where\na Spatial Transformer Attention (STA) and a Temporal\nTransformer Attention (TTA) are devised to mine the\nspatiotemporal coherence of long-term point clouds.\n‚Ä¢ We build the proposed 3D video object detection frame-\nwork upon both anchor-based and anchor-free 3D ob-\nject detectors. Extensive evaluations on the large-scale\nnuScenes benchmark show that our model outperforms\nall the state-of-the-art approaches on the leaderboard.\nThis work signiÔ¨Åcantly extends our preliminary conference\npaper [1] and the improvements are multi-fold. Our previous\nwork [1] only supports 3D object detection in the online mode\nwith an anchor-based detector. In this work, we Ô¨Årst provide\na more general framework that works with both anchor-\nbased and anchor-free detection settings. In this regard, our\napproach can be easily incorporated with leading 3D object\ndetectors. Second, we extend our method to both online and\nofÔ¨Çine detection modes, which can beneÔ¨Åt more applications\nwith improved accuracy, such as acting as annotation tools.\nThird, this paper provides a more in-depth discussion on the\nalgorithm with more details including its motivation, technical\npreliminary, network architecture and implementation. Fourth,\nextensive ablation studies are conducted to thoroughly and\nrigorously assess the broad effectiveness of our model. Last\nbut not least, we empirically observe that the proposed model\noutperforms all the algorithms on the nuScenes leaderboard\nwithout any bells and whistles.\n2 R ELATED WORK\n3D Object Detection. A considerable amount of efforts have\nbeen made into 3D object detection in recent years for its\ncrucial role in autonomous driving. Existing approaches for 3D\nobject detection can be roughly grouped into three categories.\n(1) 2D image-based methods typically perform detection from\nmonocular [16, 17, 18] or stereo images [19, 20, 21]. These ap-\nproaches often experience great difÔ¨Åculty in getting promising\nperformance due to the information loss of depth. They often\nturn to geometric priors [22] or integrate an additional module\nto estimate depth [17] or disparity [19]. It is worth noting\nthat Wang et al. [21] presented a novel framework by Ô¨Årst\nconverting image-based depth maps to pseudo point clouds and\nthen applying the off-the-shelf LiDAR-based detectors. (2) 3D\npoint cloud-based methods can leverage the LiDAR sensor to\naccess the accurate depth information, and are less sensitive to\ndifferent illumination and weather conditions. Grid- and Point-\nbased methods are the main ways to process the point clouds.\nThe common solution of grid-based methods [9, 23, 3, 1]\nis to Ô¨Årst discretize the raw point clouds into regular girds\n(e.g., voxels [10] or pillars [3]). Then 3D or 2D convolutional\nnetworks can be readily applied to extract the features. In\npractice, grid-based approaches are much more efÔ¨Åcient than\npoint-based methods, but may suffer from information loss in\nthe quantiÔ¨Åcation process. Point-based methods [24, 25, 26]\ntypically extract features and predict proposals from point-\nlevel with backbones like PointNet++ [27]. They are prone\nto get better performance than the grid-based methods, but\nare limited to computational efÔ¨Åciency and memory footprint\nwhen the number of point clouds increases. Recently, Shi et\nal. [28] proposed PV-RCNN detector that combines the merits\nof both the voxel- and point-based approaches and shows\nbetter performance. (3) Fusion-based methods exploit both the\ncamera and LiDAR sensors to capture complementary infor-\nmation. MV3D [29] is the seminal work of this family. It takes\nLiDAR bird‚Äôs eye view and front view as well as images as\ninputs, and combines the proposal-wise features of each view\nwith a deep fusion network. Later, 3D-CVF [30] proposed to\ncombine point clouds and images from N multi-view cameras\nwith a cross-view spatial feature fusion strategy. However,\nthe multi-sensor fusion system may not work robustly due\nto signal synchronization problems. In this work, we focus on\nthe LiDAR-only grid-based approaches since they are more\nprevalent in current autonomous driving applications.\nSpatiotemporal Models in Point Clouds. Different from\nthe aforementioned works that only perform frame-by-frame\ndetections, we aim to address the multiple-frame 3D object\ndetection by exploiting the temporal information. Only a few\nworks have explored the spatiotemporal model in point clouds.\nLuo et al. [6] utilized temporal 3D ConvNet to aggregate the\nmulti-frame point clouds. It performs multiple tasks simultane-\nously including detection, tracking and motion prediction, but\nleads to a considerable amount of parameters for operating on\n4D tensors. Liang et al. [7] also addressed the framework of\njoint detection, tracking and motion prediction for improving\nefÔ¨Åciency and accuracy. Later, Choy et al. [31] improved the\nconvolutional layers in [6] with sparse operations. However, it\nencounters the feature collapse issue when downsampling the\nfeatures in the temporal domain, and fails to leverage full label\ninformation in all frames. More recently, Huang et al. [32]\nproposed a contemporary work with ours that focuses on a\npoint-based recurrent gating mechanism. It adopts a Sparse\nConv U-Net to extract the spatial features of each frame, and\nthen applies LSTM on points with high semantic scores to\nfuse information across frames and save computation. Due to\nthe restriction of the backbone, it is not able to be adapted\nto the state-of-the-art single-frame detectors. On the contrary,\nour work can be easily integrated into prevalent grid-based\n3D object detectors [3, 5] by processing the spatiotemporal\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 4\nShort-term Encoding Module Long-term Aggregation Module\nCNN\nBackbone\nGMPNet\n: Grid-wise Node \n: Message Passing \nBEV Features\nAST-GRU\n Detection \nHead\nMemory Features\nAST-GRU\n Detection \nHead\nMemory FeaturesBEV Features\nBEV Features\nAST-GRU\n Detection \nHead\nMemory Features\nFig. 2: Schematic of our point cloud-based 3D video object detection framework. The input T √óM frames are Ô¨Årst divided into T\ngroups of short-term data with each merging M frames. Then, the short-term encoding module extracts the BEV features for each short-term\ndata with a Grid Message Passing Network (GMPNet) followed by a CNN backbone. Afterwards, the long-term aggregation module further\ncaptures the dependencies in these short-term features with an Attentive Spatiotemporal Transformer GRU (AST-GRU). Finally, the detection\nhead receives the enhanced memory features and produces the detection results.\ninformation in point cloud videos. It is worth mentioning that\nthere are some 2D video object detection works [33, 34] also\nexplore the attention modules for modeling temporal informa-\ntion. However, for 3D point cloud data with bird‚Äôs eye view,\nthere are typically more moving objects containing sparse\npoints with shorter lifespans. Our work aims to address these\nnew challenges in 3D video object detection by aggregating\npoint features in consecutive frames.\nGraph Neural Networks. The concept of Graph Neural\nNetworks (GNNs) was Ô¨Årst proposed by Scarselli et al. [35].\nThey extended the Recursive Neural Networks (RNN) and\nused GNNs to directly process the graph-structured data. It\nencodes the underlying relationships among the nodes of the\ngraph and mines the rich information in the data processing\nstep. Afterwards, several variants have been developed to\nimprove the representation capability of the original GNNs.\nHere, we categorize them into two classes according to the\ntype of the information propagation step: convolution and\ngate mechanism. Graph Convolutional Networks (GCNs) [36,\n37, 38, 39, 40] belongs to the former group that generalize\nconvolution to the graph data and update nodes via stacks\nof graph convolutional layers. GCNs typically compute in\nthe spectral domain with graph Fourier transformation. Re-\ncently, various applications [41, 42, 43] have been explored\nby GCNs, which achieve promising performance. The lat-\nter group [44, 45, 46, 47] aimed to adopt recurrent gating\nunits to propagate and update information across nodes. For\ninstance, Li et al. [44] exploited the Gated Recurrent Unit\n(GRU) to describe the node state by aggregating messages\nfrom neighbors. After that, Gilmer et al. [48] proposed a\ngeneralized framework that formulates the graph computation\nas a parameterized Message Passing Neural Network (MPNN).\nMPNN has been well demonstrated the ability in various tasks\nthat involve graph data [49, 50, 51]. Our GMPNet is also\ninspired by MPNN that encodes the features of short-term\npoint clouds and mines the motion cues among the grid-wise\nnodes by iterative message passing.\n3 T HE PROPOSED 3D V IDEO OBJECT DE-\nTECTION FRAMEWORK\nIn this section, we Ô¨Årst present the overall pipeline of our\nmethod in ¬ß3.1. Then, we brieÔ¨Çy review the general for-\nmulations of Message Passing Neural Network [48] and\nConvGRU [11] in ¬ß3.2 and ¬ß3.3, respectively, since our\nwork mainly builds upon these works. Afterwards, the crucial\ndesigns of Grid Message Passing Network (GMPNet) and\nAttentive Spatiotemporal Transformer GRU (AST-GRU) are\ndescribed in ¬ß3.4 and ¬ß3.5, respectively. Notice that the\nmain part of this paper follows the online object detection\npipeline. While in ¬ß3.6, an ofÔ¨Çine pipeline is also presented by\nlearning with bi-directional ConvGRU, which further improves\nthe detection performance. Finally, we provide more detailed\ninformation on our network architecture in ¬ß4.2.\n3.1 Overview\nAs illustrated in Fig. 2, our framework consists of a short-term\nencoding module and a long-term aggregation module. Assum-\ning that the input long-term point cloud sequence {It}T√óM\nt=1\ncontains T√óM frames in total, we Ô¨Årst divide it into multiple\nshort-term clip {It}T\nt=1, with each It containing concatenated\npoint clouds within M nearby frames ( eg., M = 10 in\nnuScenes). Then, LiDAR pose information is leveraged to\nalign point clouds over frames in {It}T\nt=1 to eliminate the\ninÔ¨Çuence of ego-motion. Next, each It is quantized into evenly\ndistributed grids, and forwarded to the short-term encoding\nmodule to extract the bird‚Äôs eye view features {Xt}T\nt=1.\nHere, GMPNet is applied before the 2D CNN backbone to\ncapture the relations among grids. After that, in the long-term\naggregation module, AST-GRU further learns the long-term\ndependencies of the sequential features {Xt}T\nt=1. In particular,\nat each time step t, the unit receives current input features\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 5\nXt as well as the memory features Ht‚àí1, and produces\nthe updated memory features Ht with an attentive gating\nmechanism. In this way, Ht preserves the information of both\nprevious and current frames, which facilitates the subsequent\nobject detection task. Finally, detection heads such as classi-\nÔ¨Åcation and regression network branches can be exploited on\nHt to obtain the Ô¨Ånal detections {Yt}T\nt=1.\nFor training the 3D video object detector, both the anchor-\nbased [3, 10] and anchor-free [5, 52] loss functions can be\napplied in our framework. Furthermore, our framework works\nin both the online and ofÔ¨Çine inference modes. In the online\nmode, only previous frames are employed to help the detection\nof the current frame. In the ofÔ¨Çine mode, both previous and\nlater frames are viewed as supporting frames to produce\ndetections in the current frame, which further enhances the\ndetection performance.\n3.2 Message Passing Neural Network\nMessage Passing Neural Network (MPNN) [48] is a general\nformulation, which uniÔ¨Åes various promising neural networks\nthat operate on graph-structured data [53, 44, 54, 45]. For-\nmally, it deÔ¨Ånes a directed graph G = ( V,E), with node\nvi ‚ààV and edge ei,j ‚ààE. The core of MPNN is to iteratively\npass messages between different nodes and mine the diverse\nrelations of them. At each time step t, let ht\ni denote the state\nfeatures of node vi, and et\nj,i represents edge features of ei,j\nthat describes the information passed from node vj to vi.\nMPNN aggregates information for node vi with the neighbors\nvj ‚àà‚Ñ¶vi , and infers its updated state features ht+1\ni based\non the received messages. It runs for K time steps and thus\ncaptures the long range interactions among the nodes.\nMore speciÔ¨Åcally, MPNN includes a message function M(¬∑)\nand a node update function U(¬∑). At each time step t, M(¬∑)\nsummarizes the message mt+1\nj,i passed from the neighbor node\nvj ‚àà‚Ñ¶vi to vi, then obtains the aggregated message features\nmt+1\ni . Notice that the message features mt+1\nj,i is computed\nby considering both the node state features and edge features,\nwhich is denoted as follows:\nmt+1\ni =\n‚àë\nj‚àà‚Ñ¶i\nmt+1\nj,i\n=\n‚àë\nj‚àà‚Ñ¶i\nM(ht\ni,ht\nj,et\nj,i).\n(1)\nThen, according to the collected message mt+1\ni , the update\nfunctions U(¬∑) reÔ¨Ånes the previous state features ht\ni for node\nvi, and produces the updated state features ht+1\ni :\nht+1\ni = U(ht\ni,mt+1\ni ). (2)\nIn MPNN, both M(¬∑) and U(¬∑) are parameterized by weight-\nsharing neural networks and all the operations can be learnt\nwith gradient-based optimization. After one time step, a node\naccesses information from its neighbor nodes. After K time\nstep, the information from long-range nodes can be obtained.\nIn this work, we extend MPNN to the context of LiDAR point\nclouds by treating grids as nodes. Our proposed GMPNet can\nencode the grid-wise features by mining the spatiotemporal\nrelations in short-term point clouds.\n3.3 ConvGRU Network\nGated Recurrent Unit (GRU) model [55] is a streamlined\nvariant of Recurrent Neural Network (RNN) [56, 57, 58],\nwhich is originally devised for machine translation and video\nunderstanding by capturing the dependencies of input se-\nquences. It simpliÔ¨Åes the computation of RNN and achieves\ncomparable performance. Later, Ballas et al. [11] proposed\nconvolutional GRU (ConvGRU), which utilizes convolutional\nlayers to replace the fully-connected ones in the original\nGRU. It not only preserves the better spatial resolution of\nthe input features, but also largely reduces the number of\nparameters. ConvGRU has shown promising results on many\nvision tasks [8, 59, 60, 61]. Basically, ConvGRU contains an\nupdate gate zt, a reset gate rt, a candidate memory ÀúHt and a\ncurrent memory Ht. At each time step, it computes the current\nmemory Ht (or the hidden state) according to the previous\nmemory Ht‚àí1 and the current input Xt, which is denoted by\nthe following equations:\nzt = œÉ(Wz ‚àóXt + Uz ‚àóHt‚àí1),\nrt = œÉ(Wr ‚àóXt + Ur ‚àóHt‚àí1),\nÀúHt = tanh(W ‚àóXt + U ‚àó(rt ‚ó¶Ht‚àí1)),\nHt = (1 ‚àízt) ‚ó¶Ht‚àí1 + zt ‚ó¶ ÀúHt,\n(3)\nwhere ‚Äò*‚Äô denotes the convolution operation, ‚Äò ‚ó¶‚Äô is the\nHadamard product and œÉacts as a sigmoid activation function.\nW,Wz,Wr and U,Uz,Ur are the 2D convolutional kernels.\nThe reset gate rt determines how much of the past information\nfrom Ht‚àí1 to forget, so as to produce the candidate memory\nÀúHt. For example, the information of ÀúHt all comes from\nthe current input Xt when rt = 0 . Besides, the update\ngate zt decides the degree to which the current memory Ht\naccumulates the previous information form Ht‚àí1. Our AST-\nGRU signiÔ¨Åcant improves the vanilla ConvGRU by integrating\nit with STA and TTA modules, which enforces the network to\nfocus on meaningful objects in long-term point clouds.\n3.4 Grid Message Passing Network\nAs introduced in ¬ß3.1, in the short-term encoding module,\nwe aggregate the K short-term point cloud frames by con-\ncatenating them into a single frame. To extract features on\nthis merged frame, dominant approaches tend to quantize the\npoint clouds into regular grids such as voxels or pillars, and\nthen utilize modules like V oxel Feature Encoding Layer [9]\nor the Pillar Feature Network [3] to encode the grids. These\nmodules typically consider an individual grid, which fails to\ncapture the spatiotemporal relations between the nodes from\nnearby frames, as well as limiting the expressive power due to\nthe local representation. To this end, we propose Grid Message\nPassing Network (GPMNet) to mine the spatiotemporal rela-\ntions of the grids, which results in a non-local representation.\nGMPNet views each non-empty grid as a node, and the\nrelations between grids as edges. It iteratively passes messages\nbetween the grids and updates the grid-level representation\naccordingly. Besides, GPMNet also provides complementary\nperspectives for the subsequent CNN backbone with the non-\nEuclidean characteristics\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 6\nSpeciÔ¨Åcally, given a merged point cloud frame It, we Ô¨Årst\nuniformly discretize it into a set of grids V. The grid can be\neither a voxel or a pillar, determined by the baseline detectors.\nThen a directed graph G= (V,E) is constructed, where each\nnode vi ‚àà Vrepresents a non-empty grid and each edge\nej,i ‚àà Eholds the information passed from node vj to vi.\nFurthermore, we deÔ¨Åne G as a k-nearest neighbor ( k-NN)\ngraph to save computations, which means that each node can\ndirectly access information from the K spatial neighbors (also\nknown as the Ô¨Årst order neighbors). The goal of GMPNet is to\nadaptively update feature representation hi for each grid-wise\nnode vi by integrating information from long-range nodes, i.e.,\nthe higher order neighbors. Given a grid-wise node vi, we Ô¨Årst\nuse a simpliÔ¨Åed PointNet [27] module to abstract its initial\nstate features h0\ni, which map a set of points within vi to an\nL-dim vector. The simpliÔ¨Åed PointNet is composed of fully\nconnected layers f(¬∑) and a max-pooling operation:\nh0\ni = max{f(Vi)}‚àà RL, (4)\nwhere Vi ‚ààRN√óD denotes the grid vi with N point clouds\nparameterized by D-dim representation ( eg., the XYZ coordi-\nnates and the reÔ¨Çectance of LiDAR).\nNext, we elaborate the message passing and node state\nupdating in GMPNet, following the formulation presented\nin ¬ß3.2. At time step s, vi aggregates information from its\nneighbors vj ‚àà‚Ñ¶vi according to Eq. 1. The incoming edge\nfeatures es\nj,i is deÔ¨Åned as:\nes\nj,i = hs\nj ‚àíhs\ni ‚ààRL, (5)\nwhich is an asymmetric function encoding the local neighbor\ninformation. Accordingly, the message passed from vj to vi\nis denoted as:\nms+1\nj,i = œÜŒ∏([hs\ni,es\nj,i]) ‚ààRL‚Ä≤\n, (6)\nwhere œÜŒ∏ is a fully connected layer, denoting the message\nfunction M(¬∑) in Eq. 1. It receives the concatenation of hs\ni\nand es\nj,i, and yields an L‚Ä≤-dim feature. We then summarize\nthe received messages from K neighbors with a max-pooling\noperation:\nms+1\ni = max\nj‚àà‚Ñ¶i\n{ms+1\nj,i }‚àà RL‚Ä≤\n. (7)\nAfterwards, with the collected message ms+1\ni , we update\nthe state features hs\ni for node vi in terms of Eq. 2. Here,\nGRU [55] is used as the update function U(¬∑) to adaptively\npreserve the information in different time steps, which is:\nhs+1\ni = GRU(hs\ni,ms+1\ni ) ‚ààRL. (8)\nAfter one time step, vi contains the information from the\nneighbors vj ‚àà ‚Ñ¶vi . Moreover, the neighbor node vj also\nholds information from its own neighbors ‚Ñ¶vj . Therefore,\nvi can aggregate information from long-range nodes after a\ntotal of S time steps. The message passing and node updating\nprocesses are illustrated in Fig. 3.\nWe obtain the Ô¨Ånal grid-wise feature representation vi by\napplying another fully connected layer œÜ\n‚Ä≤\nŒ∏ on hS\ni :\nvi = œÜ\n‚Ä≤\nŒ∏(hS\ni ) ‚ààRL. (9)\nAfter that, all the grid-wise features are then scattered\nback to a regular tensor ÀúIt, e.g., ÀúIt ‚àà RW√óH√óL with the\nPointPillars [3] baseline. Finally, we apply the CNN backbone\nin [9] to further extract features for ÀúIt:\nXt = FB(ÀúIt) ‚ààRw√óh√óc, (10)\nwhere FB is the backbone network and Xt is the obtained\nshort-term features of It. This leads to a reduced resolution for\nIt, facilitating the subsequent long-term aggregation module.\nMore details of the GMPNet and the CNN backbone can be\nfound in ¬ß4.2.\n3.5 Attentive Spatiotemporal Transformer GRU\nRecall that we have divided the input point cloud sequences\n{It}T√óM\nt=1 into multiple short-term clips {It}T\nt=1, with each\nclip It including concatenated point clouds from M neighbor\nframes. Then, in the short-term encoding module, we indepen-\ndently described the short-term features Xt for each It. Here,\nwe further capture the long-term dynamics over the sequential\nfeatures {Xt}T\nt=1 in the long-term aggregation module by ex-\nploiting the Attentive Spatiotemporal Transformer GRU (AST-\nGRU). AST-GRU is an extension of ConvGRU that adaptively\nmines the spatiotemporal dependencies in {Xt}T\nt=1. In partic-\nular, it improves the vanilla ConvGRU in two ways inspired by\nthe transformer attention mechanism [12, 62]. First, dominant\napproaches usually perform detection on bird-eye view feature\nmaps [3, 5, 9, 10]. However, the interest objects are much\nsmaller in such a view compared with the front view in the 2D\nimages. For example, using PointPillars [3] with a voxel size of\n0.252 m2, a vehicle (eg, with size of 4m√ó2m) remains only 2\npixels in Xt after the feature extractor with common stride of\n8. This causes difÔ¨Åculties for the recurrent unit, because the\nbackground noise will inevitably accumulate across time in\nHt. To handle this problem, we propose a spatial transformer\nattention (STA) module by attending each pixel in Xt with\nits rich spatial context, which helps the detectors to focus on\nforeground objects. Second, for a recurrent unit, the received\ncurrent input Xt and pervious memory Ht‚àí1 are not spatially\naligned. Though we have aligned the static objects with the\nego-pose information, the dynamic objects with large motion\nstill lead to an inaccurate current memory Ht. Therefore, a\ntemporal transformer attention (TTA) module is introduced\nto align the moving objects in Ht, which exploits modiÔ¨Åed\ndeformable convolutional networks to capture the motion cues\nin Ht‚àí1 and Xt. Next, we elaborate the workÔ¨Çow of our STA\nand TTA modules.\nSpatial Transformer Attention. Motivated by the transformer\nattention in [12], we propose STA to stress the foreground\npixels in Xt and suppress the background ones. In particular,\neach pixel xq ‚ààXt is considered as a query input and its\ncontext pixels xk ‚àà‚Ñ¶xq are viewed as keys. Since both the\nqueries and keys are from the same feature map Xt, STA can\nbe grouped into an intra-attention family. For each query, its\ncorresponding keys are embedded as values and the output of\nSTA is the weighted sum of the values. We compute the weight\nof values by comparing the similarity between the query and\nthe corresponding key, such that the keys that have the same\nclass as the query could contribute to the output.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 7\nFig. 3: Illustration of one iteration step for message propagation ,\nwhere hi is the state of node vi. In step s, the neighbors for h1 are\n{h2, h3, h4}(within the gray dash line), presenting the grids of the\ntop car. After receiving messages from the neighbors, the receptive\nÔ¨Åeld of h1 is enlarged in step s+ 1. This indicates the relations with\nthe bottom car are modeled after message propagation.\nSpatial \nTransformer\nC-GRU\n Temporal \nTransformer\nSpatial \nTransformer\nC-GRU\nùëøùëª‚àíùüê \nùëøùëª‚àíùüè \n Temporal \nTransformer\nSpatial \nTransformer\nC-GRU\nùëøùëª \nùíÄùëª‚àíùüê \nùíÄùëª‚àíùüè \nùíÄùëª \nùëøùëª‚àíùüê\n‚Ä≤  \nùëØùëª‚àíùüê \nùëØùëª‚àíùüè \nDetection \nHead\nDetection \nHead\nDetection \nHead\nùëØùëª‚àíùüê \nùëØùëª‚àíùüè \nùëØùëª \nùëøùëª‚àíùüè\n‚Ä≤  \nùëøùëª\n‚Ä≤  \nùëøùëª‚àíùüè\n‚Ä≤  \nùëøùëª\n‚Ä≤  \nùëØùëª‚àíùüê\n‚Ä≤  \nùëØùëª‚àíùüè\n‚Ä≤  \nFig. 4: Illustration of our proposed AST-GRU in the online mode.\nIt consists of a spatial transformer attention (STA) module and a\ntemporal transformer attention (TTA) module. AST-GRU captures\nthe dependencies from a long-term perspective and produces the\nenhanced memory features {Ht}T\nt=1.\nFormally, given an input query-key pair xq,xk ‚àà Xt,\nwe Ô¨Årst map them into different subspaces with linear layers\nœÜQ(¬∑), œÜK(¬∑) and œÜV(¬∑) to get the embedded feature vectors\nfor the query, key and value, respectively. Then, the attentive\noutput yq for a query xq is calculated as:\nyq =\n‚àë\nk‚àà‚Ñ¶q\nA(œÜQ(xq),œÜK(xk)) ¬∑œÜV(xk), (11)\nwhere A(¬∑,¬∑) is the function to compute the attention weight.\nIn practice, STA needs to compute the attention for all the\nquery positions simultaneously. Therefore, we replace œÜK,\nœÜQ and œÜV with the convolutional layers, Œ¶K, Œ¶Q and Œ¶V,\nsuch that STA can be optimized with matrix multiplication\noperations. SpeciÔ¨Åcally, the input features Xt are Ô¨Årst em-\nbedded as Kt, Qt and Vt ‚ààRw√óh√óc‚Ä≤\nthrough Œ¶K, Œ¶Q and\nŒ¶V. Then, we deÔ¨Åne the weight function A(¬∑,¬∑) as a dot-\nproduct operation followed by a softmax layer to measure the\nsimilarity of query-key pairs. To compute the attention weight\nÀúA, we reshape Kt and Qt to l√óc‚Ä≤ (l = w√óh) for saving\ncomputation:\nÀúA = softmax(Qt ¬∑KT\nt ) ‚ààRl√ól. (12)\nAfterwards, we multiply the attention weight ÀúA by values Vt\nto obtain the attention output ÀúA ¬∑Vt. Next, the shape of the\noutput is recovered back to w√óh√óc‚Ä≤, and head layers Wout\nare employed to determine the speciÔ¨Åc mode of attention.\nFinally, we obtain the attention features X\n‚Ä≤\nt via a residual\noperation [63]. These steps can be summarized as:\nX\n‚Ä≤\nt = Wout ‚àó( ÀúA ¬∑Vt) + Xt ‚ààRw√óh√óc, (13)\nwhere attention head Wout also maps the feature subspace of\nÀúA ¬∑Vt (e.g., c‚Ä≤-dim) back to the original space ( e.g., c-dim\nin Xt). The resultant output X\n‚Ä≤\nt aggregates the information\nfrom the context pixels and thus can better focus on the small\nforeground objects, as well as suppressing the background\nnoise accumulated in memory features.\nTemporal Transformer Attention. The basic idea of TTA\nis to align the spatial features of Ht‚àí1 and X\n‚Ä≤\nt, so as to\ngive a more accurate current memory Ht. Here, we utilize\nthe modiÔ¨Åed deformable convolutional layers [14, 15] as a\nspecial instantiation of the transformer attention. TTA treats\neach pixel hq ‚ààHt‚àí1 as a query and determines the positions\nof keys by attending the current input X\n‚Ä≤\nt, thus capturing the\ntemporal motion information of moving objects. TTA belongs\nto the inter-attention since it involves both Ht‚àí1 and X\n‚Ä≤\nt.\nSpeciÔ¨Åcally, we Ô¨Årst describe the regular deformable convo-\nlutional network. Let wm denote the convolutional kernel with\nsize 3 √ó3, and pm ‚àà{(‚àí1,‚àí1),(‚àí1,0),..., (1,1)}indicate\nthe predeÔ¨Åned offsets of the kernel in M = 9 grids. Given a\npixel-wise input hq ‚ààHt‚àí1, the output h\n‚Ä≤\nq of this deformable\nconvolutional layer can be expressed as:\nh\n‚Ä≤\nq =\nM‚àë\nm=1\nwm ¬∑hq+pm+‚àÜpm , (14)\nwhere ‚àÜpm is the deformation offset learnt through another\nregular convolutional layer Œ¶R to model spatial transforma-\ntions, i.e., ‚àÜpm ‚àà‚àÜPt‚àí1 = Œ¶R(Ht‚àí1) ‚ààRw√óh√ó2r2\n, where\n2r2 represnts the offsets in both x and y directions.\nFollowing the perspective of transformer attention in Eq. 11,\nwe could also reformulate Eq. 14 as:\nh\n‚Ä≤\nq =\nM‚àë\nm=1\n‚àë\nk‚àà‚Ñ¶q\n(wm ¬∑G(k,q + pm + ‚àÜpm)) ¬∑œÜV(hk), (15)\nwhere h\n‚Ä≤\nq is the attentive output of the query hq by a weighted\nsum on the M = 9 values denoted by œÜV(hk). Notice that œÜV\nis an identity function here, such that œÜV(hk) = hk. The\nattention weight is calculated through the kernel wight wm\nfollowed by a bilinear interpolation function G(¬∑,¬∑):\nG(a,b) = max(0,1 ‚àí|a‚àíb|), (16)\nwhich decides the sampling positions of the keys in the\nsupporting regions ‚Ñ¶q.\nSince the interest objects have moved from Ht‚àí1 to X\n‚Ä≤\nt,\nour TTA integrates the information in X\n‚Ä≤\nt to compute a reÔ¨Åned\noffset ‚àÜpm ‚àà ‚àÜPt‚àí1, and therefore adjust the sampling\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 8\npositions of the keys accordingly. In particular, we deÔ¨Åne a\nmotion map with the difference of Ht‚àí1 and X\n‚Ä≤\nt, and then\ncompute ‚àÜPt‚àí1 as:\n‚àÜPt‚àí1 = Œ¶R([Ht‚àí1,Ht‚àí1 ‚àíX\n‚Ä≤\nt]) ‚ààRw√óh√ó2r2\n, (17)\nwhere Œ¶R is a regular convolutional layer with the kernel\nsize 3 √ó3 to predict the offsets, and [¬∑,¬∑] is the concatenation\noperation. The intuition of the motion map is that the features\nresponse of the static objects is very low since they have\nbeen spatially aligned in Ht‚àí1 and X\n‚Ä≤\nt, while the features\nresponse of the moving objects remains high. By combining\nthe salient features in the motion map, TTA could focus more\non the moving objects. Afterwards, the output ‚àÜPt‚àí1 is used\nto determine the regions of keys and further attend Ht‚àí1\nfor all the querys q ‚àà w√óh in terms of Eq. 15, yielding\na temporally attentive memory H\n‚Ä≤\nt‚àí1. Additionally, we could\nstack multiple such modiÔ¨Åed deformable convolutional layers\nto further reÔ¨Åne H\n‚Ä≤\nt‚àí1. In our implementation, two layers are\nemployed, where the latter layer receives H\n‚Ä≤\nt‚àí1 and updates\nit similar to Eq.17.\nConsequently, we have the temporally attentive previous\nmemory H\n‚Ä≤\nt‚àí1 and the spatially attentive current input X\n‚Ä≤\nt.\nThis leads to an enhanced current memory Ht which con-\ntains richer spatiotemporal information and produces better\ndetection results Yt after being equipped with detection heads\n(see Fig. 4). We have described our AST-GRU with the online\ndetection setting. In ¬ß3.6, we present the ofÔ¨Çine detection\nsetting by exploiting bidirectional AST-GRU.\n3.6 OfÔ¨Çine 3D Video Object Detection\nWith the proposed AST-GRU, we have achieved better per-\nformance for online 3D video object detection, which is the\ncommon case in autonomous driving. However, we could\nprovide a stronger 3D video object detector by exploring both\nthe past and future frames, which will facilitate more casual\napplications. For example, when annotating LiDAR frames for\nautonomous driving scenes, it is easy to access future frames\nin a scene. Our ofÔ¨Çine 3D video object detector could act\nas an annotation tool by providing more accurate initial 3D\nbounding boxes. This may greatly increase the productivity\nof annotators compared with using a frame-by-frame detector.\nTo this end, we further adapt AST-GRU to the bidirectional\nofÔ¨Çine setting by capturing temporal information in both past\nand future frames.\nConcretely, by considering the attentive current input X\n‚Ä≤\nt\nand the previous memory H\n‚Ä≤\nt‚àí1, we have acquired the forward\nmemory features Hf\nt . To get a more powerful representation\nfor the current frame, we could better integrate the backward\ninformation from future features H\n‚Ä≤\nt+1 , Similar to Eq. 3,\nwe use the following equations to capture the rich temporal\ninformation preserved in H\n‚Ä≤\nt+1 and compute the backward\nmemory features Hb\nt:\nzb\nt = œÉ(Wb\nz ‚àóX\n‚Ä≤\nt + Ub\nz ‚àóH\n‚Ä≤\nt+1),\nrb\nt = œÉ(Wb\nr ‚àóX\n‚Ä≤\nt + Ub\nr ‚àóH\n‚Ä≤\nt+1),\nÀúHb\nt = tanh(Wb ‚àóX\n‚Ä≤\nt + Ub ‚àó(rb\nt ‚ó¶H\n‚Ä≤\nt+1)),\nHb\nt = (1 ‚àízb\nt) ‚ó¶H\n‚Ä≤\nt+1 + zb\nt ‚ó¶ ÀúHb\nt,\n(18)\nwhere Wb,Wb\nz,Wb\nr and Ub,Ub\nz,Ub\nr are sets of learnable\nparameters included in the backward AST-GRU. The reset gate\nrb\nt and update gate zb\nt measure the importance of the future\ninformation H\n‚Ä≤\nt+1 to the current input X\n‚Ä≤\nt.\nIn this way, our bidirectional AST-GRU obtains the memory\nfeatures from both forward and backward units. Then, we\ncombine these features with a concatenation operation to\naggregate the information and get an enhanced memory H\n‚Ä≤\nt,\nwhich is calculated as:\nH\n‚Ä≤\nt = [Hf\nt ,Hb\nt] ‚ààRw√óh√ó2c. (19)\nAfterwards, detection heads can be directly applied on H\n‚Ä≤\nt to\nproduce the 3D detection results. The proposed bidirectional\nAST-GRU exploits the spatiotemporal dependencies from both\npast and future, which offers complementary cues for detecting\nin the current frame. In ¬ß4.4, we will prove that it outperforms\nthe unidirectional AST-GRU by a large margin.\n4 E XPERIMENTAL RESULTS\nWe empirically evaluate our algorithm on the large-scale\nnuScenes benchmark that will be introduced ¬ß4.1. Since our\nframework is agnostic to the detectors, we demonstrate the\nperformance of our algorithm based on two prevalent 3D\nobject detectors: anchor-based PointPillars [3] and anchor-free\nCenterPoint [5]. The detailed network architecture and main\ndetection results are presented in ¬ß4.2 and ¬ß4.3, respectively.\nAfterwards, we provide thorough and comprehensive ablation\nstudies to assess each module of our algorithm in ¬ß4.4.\n4.1 3D Video Object Detection Benchmark\nSince the point cloud videos are not available in the com-\nmonly used KITTI benchmark [74], we turn to the more\nchallenging nuScenes benchmark [2] to evaluate our algorithm\nand compare with other approaches on the leaderboard. The\nnuScenes dataset is composed of 1,000 driving scenes ( i.e.,\nvideo clips), with each scene containing around 400 frames\nof 20-second in length. The annotations are conducted every\n10 frames for 10 object classes, and the training set provides\n7√óas many annotations as the KITTI dataset. There are 700\nscenes (28,130 annotations) for training, 150 scenes (6,019\nannotations) for validation and 150 scenes (6,008 annotations)\nfor testing. Besides, nuScenes utilizes a 32-beam LiDAR with\na 20Hz capture frequency, resulting in approximately 30k\npoints in each frame with a full 360-degree view . The ofÔ¨Åcial\nevaluation protocol for 3D object detection deÔ¨Ånes an NDS\n(nuScenes detection score) metric, which is a weighted sum\nof mean Average Precision (mAP) and several True Positive\n(TP) metrics. Different from the deÔ¨Ånition in KITTI, the mAP\nin nuScenes measures the varying center distance ( i.e., 0.5m,\n1m, 2m and 4 m) between the predictions and ground truths\nin the bird‚Äôs eye view. Other TP metrics consider multiple\naspects to measure the quality of the predictions, i.e., box\nlocation, size, orientation, attributes and velocity.\nTo build our point cloud-based video object detector, we\nfollow the common implementation in nuScenes benchmark\nby merging the 10 previous non-keyframe sweeps (0.5 s) to\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 9\nMethod Publication Input NDS mAP Car Truck Bus Trailer CV Ped Motor Bicycle TC Barrier\nInfoFocus [64] ECCV 2020 0.5 39.5 39.5 77.9 31.4 44.8 37.3 10.7 63.4 29.0 6.1 46.5 47.8\nPointPillars [3] CVPR 2019 0.5 45.3 30.5 68.4 23.0 28.2 23.4 4.1 59.7 27.4 1.1 30.8 38.9\nWYSIWYG [4] CVPR 2020 0.5 41.9 35.0 79.1 30.4 46.6 40.1 7.1 65.0 18.2 0.1 28.8 34.7\nSARPNET [65] N.C. 2020 0.5 48.4 32.4 59.9 18.7 19.4 18.0 11.6 69.4 29.8 14.2 44.6 38.3\n3DSSD [66] CVPR 2020 0.5 56.4 42.6 81.2 47.2 61.4 30.5 12.6 70.2 36.0 8.6 31.1 47.9\nPointPainting [67] CVPR 2020 0.5 58.1 46.4 77.9 35.8 36.2 37.3 15.8 73.3 41.5 24.1 62.4 60.2\nReconÔ¨ÅgPP [68] ARXIV 2020 0.5 59.0 48.5 81.4 38.9 43.0 47.0 15.3 72.4 44.9 22.6 58.3 61.4\nPointPillars DSA [69] ARXIV 2020 0.5 59.2 47.0 81.2 43.8 57.2 47.8 11.3 73.3 32.1 7.9 60.6 55.3\nSSN V2 [70] ARXIV 2020 0.5 61.6 50.6 82.4 41.8 46.1 48.0 17.5 75.6 48.9 24.6 60.1 61.2\n3DCVF [30] ECCV 2020 0.5 62.3 52.7 83.0 45.0 48.8 49.6 15.9 74.2 51.2 30.4 62.9 65.9\nCBGS [71] ARXIV 2019 0.5 63.3 52.8 81.1 48.5 54.9 42.9 10.5 80.1 51.5 22.3 70.9 65.7\nCVCNet [72] NeurIPS 2020 0.5 64.2 55.8 82.7 46.1 45.8 46.7 20.7 81.0 61.3 34.3 69.7 69.9\nHotSpotNet [52] ECCV 2020 0.5 66.0 59.3 83.1 50.9 56.4 53.3 23.0 81.3 63.5 36.6 73.0 71.6\nCyliNet [73] ARXIV 2020 0.5 66.1 58.5 85.0 50.2 56.9 52.6 19.1 84.3 58.6 29.8 79.1 69.0\nCenterPoint [5] CVPR 2021 0.5 67.3 60.3 85.2 53.5 63.6 56.0 20.0 84.6 59.5 30.7 78.4 71.1\nPointPillars-VID (Ours) CVPR 2020 1.5 53.1 45.4 79.7 33.6 47.1 43.1 18.1 76.5 40.7 7.9 58.8 48.8\nCenterPoint-VID (Ours) - 1.5 71.4 65.4 87.5 56.9 63.5 60.2 32.1 82.1 74.6 45.9 78.8 69.3\nCenterPoint-VID* (Ours) - 1.5 71.8 67.4 87.0 58.0 67.1 60.2 31.0 88.2 76.5 51.2 85.2 69.7\nTABLE 1: Quantitative detection results on the nuScenes 3D object detection benchmark. T.C. presents the trafÔ¨Åc cone. Moto. and\nCons. are short for the motorcycle and construction vehicle, respectively. Our 3D video object detector signiÔ¨Åcantly improves the single-frame\ndetectors, and outperforms all the competitors on the leaderboard.\ncorresponding keyframes with the ego-pose information. These\nsweeps are deemed as short-term data, while the merged\nkeyframes are viewed as long-term data. For our online\nmodel, two previous keyframes (1 s) are used to detect in the\ncurrent frame. As for the ofÔ¨Çine model, we use 0.5 stemporal\ninformation from both previous and future keyframes.\n4.2 Implementation Details\nArchitecture. We achieve 3D video object detection based\non both anchor-based and anchor-free LiDAR-based detec-\ntors. For the anchor-based baseline, we choose the ofÔ¨Åcial\nPointPillars [3] provided by nuScenes benchmark. For the\nanchor-free baseline, the CenterPoint models [5] with Point-\nPillars or V oxelNet backbones are adopted in our framework.\nSpeciÔ¨Åcally, for each merged point cloud keyframe, we deÔ¨Åne\n5-dim input features (x,y,z,r, ‚àÜt) for the points, where\nr is the LiDAR intensity and ‚àÜt formulates the time lag\nto the keyframe that ranges from 0 s to 0.5 s. We consider\nthe points whose coordinates locate within [‚àí61.2,61.2] √ó\n[‚àí61.2,61.2] √ó[‚àí10,10] meters along the X, Y and Z axes.\nThe grid size for grouping points is set as 0.25 √ó0.25√ó8 in\nPointPillars backbone. While in V oxelNet backbone, we adopt\na smaller size, 0.075 √ó0.075√ó0.2, to obtain better detection\nperformance and compare with other approaches on the leader-\nboard. In the subsequent content, we mainly elaborate the\narchitecture details based on the PointPillars backbone due\nto space limitations, and all the modiÔ¨Åcations can be applied\non V oxelNet backbone accordingly.\nIn the short-term encoding module, we implement GMPNet\nby sampling 16,384 grid-wise nodes with Farthest Point Sam-\npling algorithm [27], and build the k-NN graph with K = 20\nneighbors. A grid vi contains N = 60 points with D = 5\nrepresentations, which is embedded into feature space with\nchannel number L = 64 to get the node features. This is\nachieved by a 1 √ó1 convolutional layer followed by a max-\npooling operation, which yields the initial state features G0 ‚àà\nR16,384√ó64 (Eq. 4) for all the nodes. Then, in the iteration\nstep s, we obtain the message features Ms ‚ààR16,384√ó64 by\nperforming 1√ó1 convolutional layer and max-pooling over the\nneighbors based on the edge features Es ‚ààR16,384√ó20√ó128\n(Eq. 5 to Eq. 7.). Afterwards, the updated node state Gs+1 ‚àà\nR16,384√ó64 is computed by applying GRU on Gs and Ms\nwith linear layers ((Eq. 8). After S = 3 iteration steps, GMP\nproduces the Ô¨Ånal node state G3 ‚ààR16,384√ó64 according to\nEq. 9, and broadcast it to the bird‚Äôs eye view. Next, we apply\nRegion Proposal Network (RPN) [9] as the 2D backbone to\nfurther extract the features in the bird‚Äôs eye view. RPN is\ncomposed of several convolutional blocks, with each deÔ¨Åned\nas a tuple (S,Z,C ). S represents the stride of each block,\nwhile Z denotes the kernel size of each convolutional layer\nand C is the output channel number. The output features of\neach block are resized to the same resolution via upsampling\nlayers and concatenated together, so as to merge the semantic\ninformation from different levels.\nIn the long-term aggregation module, we implement the\nembedding functions Œ¶K, Œ¶Q, Œ¶V and the output layer Wout\nin STA with 1 √ó1 convolutional layers, and all the channel\nnumber is half of the inputs except Wout to save computations.\nIn the TTA module, the regular convolutional layers, the\ndeformable convolutional layers and the ConvGRU all have\nlearnable kernels of size 3√ó3. Besides, all these convolutional\nkernels use the same channel number with the input bird‚Äôs eye\nview features. We follow the PointPillars [3] baseline to set the\nanchor-based detection head and calculate anchors for different\nclasses using the mean sizes. Two separate convolutional lay-\ners are used for classiÔ¨Åcation and regression, respectively. For\nthe anchor-free head, we refer to the CenterPoint baseline [5]\nby applying a center heatmap head and an attribute regression\nhead. The former head aims to predict the center location\nof objects, while the latter head estimates a sub-voxel center\nlocation, as well as the box height, size, rotation and velocity.\nTraining and Inference. We train our point cloud-based\nvideo object detector following the pre-training and Ô¨Åne-\ntuning paradigm. SpeciÔ¨Åcally, the short-term encoding module\nis Ô¨Årst trained in the same way as a single-frame detector,\nwhere one-cycle learning rate policy is used for 20 epochs\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 10\nwith a maximum learning rate of 0.003 for PointPillars and\n0.001 for CenterPoint. Then, we Ô¨Åne-tune the whole video\ndetector including the long-term aggregation module with\na Ô¨Åxed learning rate (0.003 for PointPillars or 0.001 for\nCenterPoint) for 10 epochs. Corresponding anchor-based or\nanchor-free loss functions are utilized in each of the long-\nterm keyframes. Adam optimizer [75] is used to optimize the\nloss functions in both stages. In our framework, we feed at\nmost 3 keyframes (with 30 LiDAR frames) to the model due\nto memory limitation. At the inference time, we preserve at\nmost 500 detections after Non-Maxima Suppression (NMS)\nwith a score threshold of 0.1. We test our algorithm on a\nTesla v100 GPU and get an average speed of 5 FPS and 1\nFPS for PointPillars and V oxelNet backbones, respectively.\n4.3 Quantitative and Qualitative Performance\nWe validate the performance of our 3D video object detection\nalgorithm on the test set of nuScenes benchmark by comparing\nit with other state-of-the-art works on the leaderboard. As\nshown in Table 1, our best model, i.e., with CenterPoint-\nV oxelNet baseline, outperforms all the published methods in\nterms of nuScenes detection score (NDS), which is the most\nimportant metric in nuScenes. Furthermore, we achieve 1st on\nthe leaderboard by the time the paper is submitted, without\nany tricks like ensemble modeling or leveraging information\nfrom 2D images.\nSpeciÔ¨Åcally, our anchor-based video detection model\n(PointPillars-VID) improves the PointPillars [3] baseline by\nnearly 8% NDS and 15% mAP, respectively. This demonstrates\nthe signiÔ¨Åcance of integrating temporal information in 3D\npoint clouds. The PointPillars is lightweight compared with\nother state-of-the-art methods. To further validate the perfor-\nmance of our algorithm, we implement it based on the stronger\nCenterPoint [5] baseline with V oxelNet backbone. CenterPoint\nproposes to represent objects as points with an anchor-free\ndetection head and addresses the class imbalance problem\ninspired by CBGS [71], thus obtaining much better results. The\nnewly released version of CenterPoint includes a two-stage\nreÔ¨Ånement pipeline, while we only use the one-stage version\nas the baseline. By adopting the video detection strategy, our\nmodel (CenterPoint-VID) with ofÔ¨Çine mode advances a better\nperformance, achieving 71.4% NDS and 65.4 mAP on the\nleaderboard. This signiÔ¨Åcantly improves the strong baseline\nby 4.1% NDS and 5.1% mAP. By further integrating the\npainting strategy proposed by PointPainting [67], our Ô¨Ånal\nmodel (CenterPoint-VID*) gives the best performance on the\nleaderboard i.e., achieving 71.8% NDS and 67.4 mAP, without\nany sophisticated ensemble strategies.\nIn addition to the quantitative results in Table 1, we fur-\nther provide some qualitative examples. We mainly compare\nour 3D video object detector with the single-frame detector\nbaseline, i.e., PointPillars [3]. We show speciÔ¨Åc cases that our\nmodel outperforms the single-frame detector, e.g., objects are\noccluded in certain frames or there are distant or small objects\nwith sparse points. Here, three consecutive frames are shown\nfor each case. The occlusion case is presented in Fig. 1, where\nour video object detector can handle the occluded objects\nComponents Modules Performance\nmAP ‚àÜ\nShort-term Point\nCloud Encoding\nConcatenation [5] 48.25 -\nGMPNet 49.78 +1.53\nLong-term\nPoint Cloud\nAggregation\nConcatenation [5] 49.35 -\nTracklet Fusion [3] 50.09 +0.74\n3D ConvNet [6] 52.84 +3.49\nConvGRU [11] 56.54 +7.19\nSTA-GRU 57.54 +8.19\nTTA-GRU 57.13 +7.78\nAST-GRU 57.96 +8.61\nFull Model (online) 58.78 +9.43\nFull Model (ofÔ¨Çine) 59.37 +10.02\nTABLE 2: Ablation study of our 3D video object detector.\nCenetrPoint-pillar with concatenated point clouds [5] is the reference\nbaseline for computing the relative improvement ( ‚àÜ).\nwith the memory features in previous frames. In Fig. 5(a),\nwe showcase the detection of the distant car (the car on the\ntop right), whose point clouds are especially sparse. Though\nit is very challenging for the single-frame detectors, our 3D\nvideo object detector still improves the detection results thanks\nto the information from adjacent frames. Similar improvement\nis observed In Fig. 5(b), where a single-frame detector fails to\ndetect the small objects in the right of the ego-car, while our\nmodel accurately recognizes these small objects. In a nutshell,\ncompared with the single-frame detector, much fewer false\npositive (FP) and false negative (FN) detection results are\nobtained in our video object detector.\n4.4 Ablation Studies\nIn this section, we conduct ablation studies to verify each mod-\nule of our algorithm. In particular, we choose CenterPoint [5]\nwith PointPillar backbone as the baseline, as it is more Ô¨Çexible\nand faster to develop than V oxelNet. To verify the effect of\neach module, experiments are conducted with the full training\nset, and are evaluated on the validation set. For tuning the\nhyperparameters in GMPNet and AST-GRU, experiments are\nperformed on a 7 √ódownsampled training set. The short-term\nmodule uses point cloud data within 0.5 seconds, while the\nlong-term module takes data in 1.5 seconds. All the modules\nwork in the online mode unless explicitly speciÔ¨Åed. Besides,\nwe also perform more experiments to analyze the inÔ¨Çuence of\ninput data length in the short-term and long-term modules.\nComparison with other temporal encoders. Our algorithm\nbeneÔ¨Åts from both the short-term and long-term point cloud\nsequence information, where GMPNet and AST-GRU mod-\nules are devised to handle these two different temporal pat-\nterns, respectively. Here, we compare our modules with other\ntemporal-based design strategies. In particular, for both the\nshort-term and long-term modules, the concatenation-based\napproach is set as the baseline by merging point clouds from\n0.5 seconds or 1.5 seconds. This can be viewed as the simplest\ntemporal encoder. As shown in Table 2, our GMPNet improves\nthe baseline by 1.53% mAP. This demonstrates that GMPNet\ncould effectively mine the motion features in short-term point\nclouds by exchanging messages among grids from nearby\nframes, while the baseline encoder, e.g., the Pillar Feature Net-\nwork, only considers each grid independently. Furthermore, to\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 11\n(a) The single-frame detector (left) fail to detect the car on the top right in the third frame, while our method (right) could address this.\n(b) The single-frame detector (left) have difÔ¨Åculty in detecting the small objects in the right of the ego-car (zoom in for better view).\nFig. 5: Qualitative results of 3D video object detection. We compare our algorithm (the right three frames in each case) with the\nsingle-frame 3D object detector (the left ones). The red and grey boxes indicate the predictions and ground-truths, respectively.\n1 16.84\n2 19.34\n3 20.27\n4 20.77\n5 21.52\n0.91\n1.6\n2.05 2.07\n0\n0.5\n1\n1.5\n2\n2.5\n1 2 3 4\nImprovement (‚ñ≥mAP)\nIteration Step (S)\n0\n0.69\n1.12\n1.14\nK=20 & S=3 S=3 & K=20\n0 0 2.05 21.3 0 0 2.05 21.3\n10 1.13 2.05 22.19 1 0.91 2.05 22.21\n20 2.05 2.05 23.35 2 1.6 2.05 22.9\n30 1.72 2.05 23.02 3 2.05 2.05 23.35\n40 1.55 2.05 22.85 4 2.07 2.05 23.37\n1.13\n2.05\n1.72\n1.55\n0\n0.5\n1\n1.5\n2\n2.5\n10 20 30 40\nImprovement (‚ñ≥mAP)\nNeighbor Number (K)\n(a) Importance of S (K=20). (b) Importance of K (S=3).\nFig. 6: Ablation study for GMPNet We Ô¨Åx one parameter and vary\nthe other to ablate the importance of iteration step S and neighbor\nnode number K.\n49.78\n57.57 58.78 59.23 59.87\n40\n45\n50\n55\n60\n65\n1 2 3 4 5\nPerformance (mAP)\nNumber of Input Keyframes (T)\n57.12 57.75 58.58 59.27 59.87\n40\n45\n50\n55\n60\n65\n2 4 6 8 10\nPerformance (mAP)\nNumber of Non-keyframe Sweeps (M)\n(a) Importance of T (M=10). (b) Importance of M (T=5).\nFig. 7: Ablation study for the input length of the short-term\nand long-term modules. In (a) and (b), different keyframes T or\nnon-keyframe sweeps M are used to evaluate the performance.\nverify the effectiveness of aggregating long-term point clouds,\nwe compare our AST-GRU with 3D temporal ConvNet [6]\nand vanilla ConvGRU [11] methods. Since the 3D ConvNet\ncould only enforce optimization on a single keyframe, it thus\nachieves worse results than the ConvGRU [11]. Though the\nConvGRU has exploited the multi-frame features, it ignores\nthe inÔ¨Çuence of noisy background and the misalignment in\nspatial features. By contrast, our proposed AST-GRU module\nimproves these two approaches by 5.12% mAP and 1.42%\nmAP respectively, according to Table 2.\nWe also implement a new tracking-based method named\nTracklet Fusion that follows the ideas in [6, 7] to check\nwhether explicit object tracking could help detection. Tracklet\nFusion integrates the detections and tracklets through NMS,\nand improves the baseline by 0.74%. It demonstrates the\nimportance of spatiotemporal features aggregation. It is worth\nmentioning that all the designs in AST-GRU give better perfor-\nmance. For example, the STA module enhances the ConvGRU\nby 1.00% mAP, while the TTA module further advances the\nperformance by 0.59%. The overall model containing both\nGMPNet and AST-GRU surpasses the baseline by 9.43%mAP.\nMoreover, we obtain the best model by integrating the ofÔ¨Çine\nstrategy, further improving the online model by 0.59%. This\nshows that the information from future frames can further\nboost detection performance. Next, we ablate some crucial\ndesigns in GMPNet and TTA, as well as the inÔ¨Çuence of the\nlength of input point cloud sequences.\nHyperparameters in GMPNet. Our GMPNet enables a grid\nto capture a Ô¨Çexible receptive Ô¨Åeld via iteratively propagating\nmessage on a k-NN graph. Given a grid-wise node, both the\nnumber of Ô¨Årst-order neighbors (denoted as K) and the total\niteration steps (denoted as S) have an inÔ¨Çuence on the receptive\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 12\nAspect Modules Performance\nmAP ‚àÜ\nFull Model 27.28 0\nInputs w/o motion map 26.03 -1.25\n(M=2) w/ current input 26.27 -1.01\nLayer\nNumber\nM=1 26.55 -0.73\nM=2 27.28 0\nM=3 26.42 -0.86\nTABLE 3: Detailed analysis of the input choices and the layer\nnumber in TTA module. The full model is viewed as the\nreference for computing the relative performance ( ‚àÜ).\nÔ¨Åeld as well as the Ô¨Ånal performance. In order to clearly\ndemonstrate the impact of these two parameters, we show\nthe performance change by varying S and K. According to\nFig. 6(a), we observe that S = 3 has already obtained enough\nreceptive Ô¨Åeld, and further increasing S does not help promote\nthe results. In Fig. 6(b), we show that K = 20 achieves\nthe best performance, while a larger K instead degrades the\nperformance. We infer that K = 20 and S = 3 have captured\nan appropriate receptive Ô¨Åeld, and a much larger receptive Ô¨Åeld\nmay confuse the detector.\nDifferent design strategies in TTA. Our TTA aligns the\nfeatures of the dynamic objects by applying a modiÔ¨Åed\ndeformable convolutional network. Thus, there are several\nfactors affecting the Ô¨Ånal results, e.g., the inputs for computing\nsupporting regions and the layer number of TTA. In our\nimplementation, we integrate a motion map into the inputs and\nuse two layers for TTA. Here, we give a detailed analysis of\nthese aspects. As shown in Table. 3, w/o motion map denotes\nthat TTA takes only the previous memory feature H t‚àí1 as\ninput, while w/ current input represents that TTA receives the\nconcatenation of Ht‚àí1 and X\n‚Ä≤\nt. Both designs give the decreased\nperformance, demonstrating the effectiveness of the motion\nmap. In addition, the model using two modiÔ¨Åed deformable\nconvolutional layers achieves the best performance. We infer\nthat the deeper layers lead to difÔ¨Åculty in optimizing the whole\nnetwork.\nInput length of point cloud frames. Finally, we analyze the\neffect of the input sequence length. For datasets like nuScenes,\nlabels are only available on keyframes, and non-keyframe\nsweeps do not have annotations. It is interesting to explore\nthe short-term features in non-keyframe sweeps for further\nimproving the performance. Thus, our GMPNet is developed\nto implicitly capture the temporal information in short-term\nnon-keyframe sweeps, while applying AST-GRU to explicitly\nleverage the labels in long-term keyframes. We now evaluate\nthe importance of the number of keyframes and non-keyframe\nsweeps. In Fig. 7(a), we Ô¨Åx M = 10 and increase T. When\nT = 1 , GMPNet is adopted as the baseline. When T > 1,\nAST-GRU is further used to capture the long-term features.\nOur model with 3 keyframes already achieves good enough\nresults, and a larger T brings slightly consistent improvement.\nIn Fig. 7(b), T is Ô¨Åxed as 5 and we increase M from 2 to\n10. Obvious improvements can be seen with larger M. In\nparticular, when the number of total frames T√óM is the same,\nsimilar gains could be achieved. It indicates that the short-\nterm features are as crucial as the long-term features in 3D\nvideo object detection. For balancing between the efÔ¨Åciency\nand accuracy, we adopt T = 3 and M = 10 in the Ô¨Ånal model\nwith the V oxelNet backbone.\n5 C ONCLUSIONS\nWe have presented a new framework for 3D video object\ndetection in point clouds. Our framework formulates the\ntemporal information with short-term and long-term patterns,\nand devises a short-term encoding module and a long-term\naggregation module to address these two temporal patterns.\nIn the former module, a GMPNet is introduced to mine the\nshort-term object motion cues in nearby point cloud frames.\nThis is achieved by iterative message exchanging in a k-NN\ngraph, e.g., a grid updates its feature by integrating information\nfrom k neighbor grids. In the latter module, an AST-GRU is\nproposed to further aggregate long-term features. AST-GRU\nincludes a STA and a TTA, which are designed to handle\nsmall objects and align moving objects, respectively. Our point\ncloud video-based framework could work in both online and\nofÔ¨Çine mode, depending on the applications. We also tested\nour framework with both anchor-based and anchor-free 3D ob-\nject detectors. Evaluation results on the nuScenes benchmark\ndemonstrate the superior performance of our framework.\nREFERENCES\n[1] J. Yin, J. Shen, C. Guan, D. Zhou, and R. Yang, ‚ÄúLidar-based on-\nline 3d video object detection with graph-based message passing\nand spatiotemporal transformer attention,‚Äù in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition ,\npp. 11495‚Äì11504, 2020.\n[2] H. Caesar, V . Bankiti, A. H. Lang, S. V ora, V . E. Liong, Q. Xu,\nA. Krishnan, Y . Pan, G. Baldan, and O. Beijbom, ‚Äúnuscenes: A\nmultimodal dataset for autonomous driving,‚Äù in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recogni-\ntion, pp. 11618-11628, 2020.\n[3] A. H. Lang, S. V ora, H. Caesar, L. Zhou, J. Yang, and O. Bei-\njbom, ‚ÄúPointpillars: Fast encoders for object detection from point\nclouds,‚Äù in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , pp. 12697‚Äì12705, 2019.\n[4] P. Hu, J. Ziglar, D. Held, and D. Ramanan, ‚ÄúWhat you see is\nwhat you get: Exploiting visibility for 3d object detection,‚Äù in\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 10998‚Äì11006, 2020.\n[5] T. Yin, X. Zhou, and P. Kr ¬®ahenb¬®uhl, ‚ÄúCenter-based 3d object\ndetection and tracking,‚Äù arXiv preprint arXiv:2006.11275 , 2020.\n[6] W. Luo, B. Yang, and R. Urtasun, ‚ÄúFast and furious: Real time\nend-to-end 3d detection, tracking and motion forecasting with a\nsingle convolutional net,‚Äù in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pp. 3569‚Äì3577,\n2018.\n[7] M. Liang, B. Yang, W. Zeng, Y . Chen, R. Hu, S. Casas and\nR. Urtasun,‚ÄúPnPNet: End-to-End Perception and Prediction with\nTracking in the Loop,‚Äù in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pp. 11550‚Äì11559,\n2020.\n[8] D. Tran, H. Wang, L. Torresani, J. Ray, Y . LeCun, and M. Paluri,\n‚ÄúA closer look at spatiotemporal convolutions for action recog-\nnition,‚Äù in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , pp. 6450‚Äì6459, 2018.\n[9] Y . Zhou and O. Tuzel, ‚ÄúV oxelnet: End-to-end learning for point\ncloud based 3d object detection,‚Äù in Proceedings of the IEEE\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 13\nConference on Computer Vision and Pattern Recognition , pp.\n4490‚Äì4499, 2018.\n[10] Y . Yan, Y . Mao, and B. Li, ‚ÄúSecond: Sparsely embedded\nconvolutional detection,‚Äù Sensors, 18(10):3337, 2018.\n[11] N. Ballas, L. Yao, C. Pal, and A. Courville, ‚ÄúDelving deeper\ninto convolutional networks for learning video representations,‚Äù\nin International Conference on Learning Representations , 2016.\n[12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nin Advances in Neural Information Processing Systems, pp. 5998-\n6008, 2017.\n[13] X. Wang, R. Girshick, A. Gupta, and K. He, ‚ÄúNon-local neural\nnetworks,‚Äù in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , pp. 7794‚Äì7803, 2018.\n[14] X. Zhu, H. Hu, S. Lin, and J. Dai, ‚ÄúDeformable convnets v2:\nMore deformable, better results,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , pp.\n9308‚Äì9316, 2019.\n[15] X. Zhu, D. Cheng, Z. Zhang, S. Lin, and J. Dai, ‚ÄúAn empirical\nstudy of spatial attention mechanisms in deep networks,‚Äù in\nProceedings of the IEEE International Conference on Computer\nVision, pp. 6688‚Äì6697, 2019.\n[16] Y . Chen, L. Tai, K. Sun, and M. Li, ‚ÄúMonopair: Monocular\n3d object detection using pairwise spatial relationships,‚Äù in\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 12090-12099, 2020.\n[17] X. Ma, Z. Wang, H. Li, P. Zhang, W. Ouyang, and X. Fan,\n‚ÄúAccurate monocular 3d object detection via color-embedded 3d\nreconstruction for autonomous driving,‚Äù in Proceedings of the\nIEEE International Conference on Computer Vision , pp. 6850‚Äì\n6860, 2019.\n[18] J. Ku, A. D. Pon, and S. L. Waslander, ‚ÄúMonocular 3d object de-\ntection leveraging accurate proposals and shape reconstruction,‚Äù\nin Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 867‚Äì876, 2019.\n[19] P. Li, X. Chen, and S. Shen, ‚ÄúStereo R-CNN based 3d object\ndetection for autonomous driving,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , pp.\n7644‚Äì7652, 2019.\n[20] Y . Chen, S. Liu, X. Shen, and J. Jia, ‚ÄúDSGN: Deep stereo\ngeometry network for 3d object detection,‚Äù in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition ,\npp. 12536‚Äì12545, 2020.\n[21] Y . Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and\nK. Q. Weinberger, ‚ÄúPseudo-lidar from visual depth estimation:\nBridging the gap in 3d object detection for autonomous driving,‚Äù\nin Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 8445‚Äì8453, 2019.\n[22] X. Chen, K. Kundu, Z. Zhang, H. Ma, S. Fidler, and R. Urtasun,\n‚ÄúMonocular 3d object detection for autonomous driving,‚Äù in\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 2147‚Äì2156, 2016.\n[23] B. Yang, W. Luo, and R. Urtasun, ‚ÄúPixor: Real-time 3d ob-\nject detection from point clouds,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , pp.\n7652‚Äì7660, 2018.\n[24] S. Shi, X. Wang, and H. Li, ‚ÄúPointrcnn: 3d object proposal\ngeneration and detection from point cloud,‚Äù in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition ,\npp. 770‚Äì779, 2019.\n[25] Z. Yang, Y . Sun, S. Liu, X. Shen, and J. Jia, ‚ÄúStd: Sparse-to-\ndense 3d object detector for point cloud,‚Äù in Proceedings of the\nIEEE International Conference on Computer Vision , pp. 1951‚Äì\n1960, 2019.\n[26] Y . Chen, S. Liu, X. Shen, and J. Jia, ‚ÄúFast point R-CNN,‚Äù in\nProceedings of the IEEE International Conference on Computer\nVision, pp. 9774‚Äì9783, 2019.\n[27] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, ‚ÄúPointnet: Deep\nlearning on point sets for 3d classiÔ¨Åcation and segmentation,‚Äù in\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 77‚Äì85, 2017.\n[28] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li,\n‚ÄúPV-RCNN: Point-voxel feature set abstraction for 3d object\ndetection,‚Äù in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , pp. 10526‚Äì10535, 2020.\n[29] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, ‚ÄúMulti-view 3d\nobject detection network for autonomous driving,‚Äù in Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 6526‚Äì6534, 2017.\n[30] J. H. Yoo, Y . Kim, J. S. Kim, and J. W. Choi, ‚Äú3d-cvf: Gen-\nerating joint camera and lidar features using cross-view spatial\nfeature fusion for 3d object detection,‚Äù in European Conference\non Computer Vision , pp. 720-736, 2020.\n[31] C. Choy, J. Gwak, and S. Savarese, ‚Äú4d spatio-temporal con-\nvnets: Minkowski convolutional neural networks,‚Äù in Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 3075‚Äì3084, 2019.\n[32] R. Huang, W. Zhang, A. Kundu, C. Pantofaru, D. A. Ross,\nT. Funkhouser, and A. Fathi, ‚ÄúAn lstm approach to temporal 3d\nobject detection in lidar point clouds,‚Äù in European Conference\non Computer Vision , pp. 266‚Äì282, 2020.\n[33] Y . Chen, Y . Cao, H. Hu, L. Wang, ‚ÄúMemory Enhanced Global-\nLocal Aggregation for Video Object Detection,‚Äù in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recog-\nnition, pp. 10334‚Äì10343, 2020.\n[34] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. arpathy, A. Khosla, M. Bernstein, A. Berg, and\nF. Li, ‚ÄúImagenet large scale visual recognition challenge,‚Äù in\nInternational Journal of Computer Vision , vol. 115, no. 3, pp.\n211‚Äì252, 2015.\n[35] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and\nG. Monfardini, ‚ÄúThe graph neural network model,‚Äù IEEE Trans-\nactions on neural networks , vol. 20, no. 1, pp. 61‚Äì80, 2008.\n[36] J. Bruna, W. Zaremba, A. Szlam, and Y . LeCun, ‚ÄúSpectral net-\nworks and locally connected networks on graphs,‚Äù International\nConference on Learning Representations , 2014.\n[37] M. Henaff, J. Bruna, and Y . LeCun, ‚ÄúDeep convolu-\ntional networks on graph-structured data,‚Äù arXiv preprint\narXiv:1506.05163, 2015.\n[38] M. Defferrard, X. Bresson, and P. Vandergheynst, ‚ÄúConvolu-\ntional neural networks on graphs with fast localized spectral\nÔ¨Åltering,‚Äù in Advances in Neural Information Processing Systems,\npp. 3837‚Äì3845, 2016.\n[39] D. K. Hammond, P. Vandergheynst, and R. Gribonval, ‚ÄúWavelets\non graphs via spectral graph theory,‚Äù Applied and Computational\nHarmonic Analysis, vol. 30, no. 2, pp. 129‚Äì150, 2011.\n[40] M. Niepert, M. Ahmed, and K. Kutzkov, ‚ÄúLearning convolu-\ntional neural networks for graphs,‚Äù in International Conference\non Machine Learning , pp. 2014‚Äì2023, 2016.\n[41] W. Hamilton, Z. Ying, and J. Leskovec, ‚ÄúInductive representa-\ntion learning on large graphs,‚Äù inAdvances in Neural Information\nProcessing Systems, pp. 1024‚Äì1034, 2017.\n[42] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg,\nI. Titov, and M. Welling, ‚ÄúModeling relational data with graph\nconvolutional networks,‚Äù in European Semantic Web Conference.\nSpringer, pp. 593‚Äì607, 2018.\n[43] H. Gao and S. Ji, ‚ÄúGraph u-nets,‚Äù in International Conference\non Machine Learning , pp. 2083‚Äì2092, 2019.\n[44] Y . Li, D. Tarlow, M. Brockschmidt, and R. Zemel, ‚ÄúGated\ngraph sequence neural networks,‚Äù in International Conference\non Learning Representations , 2016.\n[45] S. Kearnes, K. McCloskey, M. Berndl, V . Pande, and P. Riley,\n‚ÄúMolecular graph convolutions: moving beyond Ô¨Ångerprints,‚Äù\nJournal of computer-aided molecular design , vol. 30, no. 8, pp.\n595‚Äì608, 2016.\n[46] V . Zayats and M. Ostendorf, ‚ÄúConversation modeling on reddit\nusing a graph-structured lstm,‚Äù Transactions of the Association\nfor Computational Linguistics , vol. 6, pp. 121‚Äì132, 2018.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 14\n[47] N. Peng, H. Poon, C. Quirk, K. Toutanova, and W.-t. Yih,\n‚ÄúCross-sentence n-ary relation extraction with graph lstms,‚Äù\nTransactions of the Association for Computational Linguistics ,\nvol. 5, pp. 101‚Äì115, 2017.\n[48] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and\nG. E. Dahl, ‚ÄúNeural message passing for quantum chemistry,‚Äù in\nInternational Conference on Machine Learning, 2017, pp. 1263‚Äì\n1272.\n[49] W. Wang, X. Lu, J. Shen, D. J. Crandall, and L. Shao,\n‚ÄúZero-shot video object segmentation via attentive graph neural\nnetworks,‚Äù in Proceedings of the IEEE International Conference\non Computer Vision , pp. 9235-9244, 2019.\n[50] S. Qi, W. Wang, B. Jia, J. Shen, and S.-C. Zhu, ‚ÄúLearning\nhuman-object interactions by graph parsing neural networks,‚Äù in\nProceedings of the European Conference on Computer Vision ,\npp. 407‚Äì423, 2018.\n[51] C. Si, Y . Jing, W. Wang, L. Wang, and T. Tan, ‚ÄúSkeleton-\nbased action recognition with spatial reasoning and temporal\nstack learning,‚Äù in Proceedings of the European Conference on\nComputer Vision, pp. 106‚Äì121, 2018.\n[52] Q. Chen, L. Sun, Z. Wang, K. Jia, and A. Yuille, ‚ÄúObject as\nhotspots: An anchor-free 3d object detection approach via Ô¨Åring\nof hotspots,‚Äù in European Conference on Computer Vision , pp.\n68‚Äì84, 2020.\n[53] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre,\nR. G ¬¥omez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and\nR. P. Adams, ‚ÄúConvolutional networks on graphs for learning\nmolecular Ô¨Ångerprints,‚Äù in Advances in Neural Information\nProcessing Systems, pp. 2224‚Äì2232, 2015.\n[54] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende et al., ‚ÄúInterac-\ntion networks for learning about objects, relations and physics,‚Äù\nin Advances in Neural Information Processing Systems , pp.\n4502‚Äì4510, 2016.\n[55] K. Cho, B. Van Merri ¬®enboer, C. Gulcehre, D. Bahdanau,\nF. Bougares, H. Schwenk, and Y . Bengio, in ‚ÄúLearning phrase\nrepresentations using rnn encoder-decoder for statistical machine\ntranslation,‚Äù Conference on Empirical Methods in Natural Lan-\nguage Processing, 2014.\n[56] D. Bahdanau, K. Cho, and Y . Bengio, ‚ÄúNeural machine trans-\nlation by jointly learning to align and translate,‚Äù arXiv preprint\narXiv:1409.0473, 2014.\n[57] N. Srivastava, E. Mansimov, and R. Salakhudinov, ‚ÄúUnsuper-\nvised learning of video representations using lstms,‚Äù in Interna-\ntional Conference on Machine Learning , pp. 843‚Äì852, 2015.\n[58] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, ‚ÄúEmpirical eval-\nuation of gated recurrent neural networks on sequence modeling,‚Äù\narXiv preprint arXiv:1412.3555 , 2014.\n[59] M. Liu and M. Zhu, ‚ÄúMobile video object detection with\ntemporally-aware feature maps,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , pp.\n5686‚Äì5695, 2018.\n[60] C. Feichtenhofer, A. Pinz, and R. P. Wildes, ‚ÄúSpatiotemporal\nmultiplier networks for video action recognition,‚Äù in Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 7445‚Äì7454, 2017.\n[61] Z. Gan, C. Gan, X. He, Y . Pu, K. Tran, J. Gao, L. Carin, and\nL. Deng, ‚ÄúSemantic compositional networks for visual caption-\ning,‚Äù in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 1141‚Äì1150, 2017.\n[62] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‚ÄúEnd-to-end object detection with transformers,‚Äù\nin European Conference on Computer Vision, pp. 213‚Äì229, 2020.\n[63] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning\nfor image recognition,‚Äù in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp. 770‚Äì778, 2016.\n[64] J. Wang, S. Lan, M. Gao, and LS. Davis, ‚ÄúInfofocus: 3d object\ndetection for autonomous driving with dynamic information\nmodeling,‚Äù in European Conference on Computer Vision , pp.\n405‚Äì420, 2020.\n[65] Y . Ye, H. Chen, C. Zhang, X. Hao, and Z. Zhang, ‚ÄúSarpnet:\nShape attention regional proposal network for lidar-based 3d\nobject detection,‚Äù Neurocomputing, vol. 379, pp. 53‚Äì63, 2020.\n[66] Z. Yang, Y . Sun, S. Liu, and J. Jia, ‚Äú3DSSD: Point-Based\n3D Single Stage Object Detector,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , pp.\n11037‚Äì11045, 2020.\n[67] S. V ora, A. H. Lang, B. Helou, and O. Beijbom, ‚ÄúPointpainting:\nSequential fusion for 3d object detection,‚Äù in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition ,\npp. 4603‚Äì4611, 2020.\n[68] T. Wang, X. Zhu, and D. Lin, ‚ÄúReconÔ¨Ågurable voxels: A\nnew representation for lidar-based point clouds,‚Äù arXiv preprint\narXiv:2004.02724, 2020.\n[69] P. Bhattacharyya, C. Huang, and K. Czarnecki, ‚ÄúSelf-attention\nbased context-aware 3d object detection,‚Äù arXiv preprint\narXiv:2101.02672, 2021.\n[70] X. Zhu, Y . Ma, T. Wang, Y . Xu, J. Shi, and D. Lin, ‚ÄúSSN:\nShape signature networks for multi-class object detection from\npoint clouds,‚Äù arXiv preprint arXiv:2004.02774 , 2020.\n[71] B. Zhu, Z. Jiang, X. Zhou, Z. Li, and G. Yu, ‚ÄúClass-balanced\ngrouping and sampling for point cloud 3d object detection,‚ÄùarXiv\npreprint arXiv:1908.09492, 2019.\n[72] Q. Chen, L. Sun, E. Cheung, and A. L. Yuille, ‚ÄúEvery view\ncounts: Cross-view consistency in 3d object detection with\nhybrid-cylindrical-spherical voxelization,‚Äù in Advances in Neural\nInformation Processing Systems , vol. 33, 2020.\n[73] M. Rapoport-Lavie and D. Raviv, ‚ÄúIt‚Äôs all around you: Range-\nguided cylindrical network for 3d object detection,‚Äù arXiv\npreprint arXiv:2012.03121, 2020.\n[74] A. Geiger, P. Lenz, and R. Urtasun, ‚ÄúAre we ready for au-\ntonomous driving? the kitti vision benchmark suite,‚Äù in Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 3354‚Äì3361, 2012.\n[75] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic\noptimization,‚Äù arXiv preprint arXiv:1412.6980 , 2014.\nJunbo Yin is currently working toward the Ph.D.\ndegree in the School of Computer Science, Bei-\njing Institute of Technology, Beijing, China. His\ncurrent research interests include Lidar-based\n3D object detection and segmentation, self-\nsupervised learning, and visual object tracking.\nJianbing Shen (M‚Äô11-SM‚Äô12) is currently acting\nas the Lead Scientist at the Inception Institute of\nArtiÔ¨Åcial Intelligence, Abu Dhabi, UAE. He is also\nan adjunct Professor with the School of Com-\nputer Science, Beijing Institute of Technology,\nBeijing, China. He published more than 100 top\njournal and conference papers, and his Google\nscholar citations are about 12,600 times with H-\nindex 56. He was rewarded as the Highly Cited\nResearcher by the Web of Science in 2020, and\nalso the most cited Chinese researchers by the\nElsevier Scopus in 2020. His research interests include computer vision,\ndeep learning, self-driving cars, medical image analysis and smart city.\nHe is/was an Associate Editor ofIEEE TIP, IEEE TNNLS, PR, and other\njournals.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 15\nXin Gao is currently a Full Professor of com-\nputer science with the Computer, Electrical and\nMathematical Sciences and Engineering Divi-\nsion, King Abdullah University of Science and\nTechnology (KAUST), Thuwal, Saudi Arabia. He\nis also the Associate Director of the Computa-\ntional Bioscience Research Center, KAUST, and\nan Adjunct Faculty Member with the David R.\nCheriton School of Computer Science, Univer-\nsity of Waterloo. He received the Ph.D. degree\nin computer science from University of Waterloo,\nWaterloo, ON, Canada, in 2009. His group focuses on building compu-\ntational models, developing machine learning methods, and designing\nefÔ¨Åcient and effective algorithms, with particular a focus on applications\nto key open problems in biology. He has coauthored more than 200\nresearch articles in the Ô¨Åelds of machine learning and bioinformatics.\nDavid Crandall is an Associate Professor in\nthe School of Informatics and Computing, Indi-\nana University. He received the Ph.D. degree\nin computer science from Cornell University in\n2008, and the B.S. and M.S. degrees in com-\nputer science and engineering from Pennsyl-\nvania State University, State College in 2001.\nHis research interests include computer vision,\nmachine learning, and data mining. He is the\nrecipient of a National Science Foundation CA-\nREER Award and a Google Faculty Research\nAward. Currently, he is an Associate Editor of IEEE Transactions on\nPattern Analysis and Machine Intelligenceand IEEE Transactions on\nMultimedia.\nRuigang Yang received the MS degree from\nColumbia University in 1998 and the PhD degree\nfrom the University of North Carolina, Chapel\nHill in 2003. He is currently a Full professor of\nComputer Science at the University of Kentucky.\nHis research interests span over computer vision\nand computer graphics, in particular in 3D re-\nconstruction and 3D data analysis. He has pub-\nlished more than 100 papers, which, according\nto Google Scholar, has received close to 16,800\ncitations with an h-index of 61. He has received\na number of awards, including the US National Science Foundation Fac-\nulty Early Career Development (CAREER) Program Award in 2004, best\nDemonstration Award at CVPR 2007 and the Deans Research Award at\nthe University of Kentucky in 2013. He has served as Area Chairs for\npremium vision conferences (such as ICCV/CVPR), and served as a\nProgram Chair for CVPR 2021. He is currently an associate editor of the\nIEEE Transactions on Pattern Analysis and Machine Intelligenceand a\nsenior member of IEEE.",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7249147891998291
    },
    {
      "name": "Computer science",
      "score": 0.7050440311431885
    },
    {
      "name": "Computer vision",
      "score": 0.6095672845840454
    },
    {
      "name": "Object detection",
      "score": 0.5401225686073303
    },
    {
      "name": "Point cloud",
      "score": 0.5202430486679077
    },
    {
      "name": "Transformer",
      "score": 0.49484390020370483
    },
    {
      "name": "Artificial neural network",
      "score": 0.4828951358795166
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.44104212522506714
    },
    {
      "name": "Cognitive neuroscience of visual object recognition",
      "score": 0.43780776858329773
    },
    {
      "name": "Graph",
      "score": 0.4118180274963379
    },
    {
      "name": "Object (grammar)",
      "score": 0.3075714409351349
    },
    {
      "name": "Theoretical computer science",
      "score": 0.10680148005485535
    },
    {
      "name": "Engineering",
      "score": 0.10393944382667542
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I6469544",
      "name": "City University of Macau",
      "country": "MO"
    },
    {
      "id": "https://openalex.org/I204512498",
      "name": "University of Macau",
      "country": "MO"
    },
    {
      "id": "https://openalex.org/I71920554",
      "name": "King Abdullah University of Science and Technology",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I4210119109",
      "name": "Indiana University Bloomington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I143302722",
      "name": "University of Kentucky",
      "country": "US"
    }
  ],
  "cited_by": 83
}