{
    "title": "Large language models for generating medical examinations: systematic review",
    "url": "https://openalex.org/W4393318738",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5093698157",
            "name": "Yaara Artsi",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A2809724045",
            "name": "Vera Sorin",
            "affiliations": [
                "Tel Aviv University",
                "Sheba Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2155772135",
            "name": "Eli Konen",
            "affiliations": [
                "Sheba Medical Center",
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A1498187152",
            "name": "Benjamin S Glicksberg",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A2419298921",
            "name": "Girish Nadkarni",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A1945837108",
            "name": "Eyal Klang",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A5093698157",
            "name": "Yaara Artsi",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A2809724045",
            "name": "Vera Sorin",
            "affiliations": [
                "Tel Aviv University",
                "Sheba Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2155772135",
            "name": "Eli Konen",
            "affiliations": [
                "Tel Aviv University",
                "Sheba Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A1498187152",
            "name": "Benjamin S Glicksberg",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A2419298921",
            "name": "Girish Nadkarni",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A1945837108",
            "name": "Eyal Klang",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4283578429",
        "https://openalex.org/W4281387590",
        "https://openalex.org/W3005496249",
        "https://openalex.org/W2623135331",
        "https://openalex.org/W4379052631",
        "https://openalex.org/W4385786223",
        "https://openalex.org/W3127024416",
        "https://openalex.org/W2141521353",
        "https://openalex.org/W2525617277",
        "https://openalex.org/W3046877299",
        "https://openalex.org/W1555128924",
        "https://openalex.org/W4295094102",
        "https://openalex.org/W2751579800",
        "https://openalex.org/W4323035111",
        "https://openalex.org/W4384662964",
        "https://openalex.org/W3109948322",
        "https://openalex.org/W4387500346",
        "https://openalex.org/W4322761615",
        "https://openalex.org/W4387232979",
        "https://openalex.org/W4385694104",
        "https://openalex.org/W2995786666",
        "https://openalex.org/W4380356334",
        "https://openalex.org/W3044107495",
        "https://openalex.org/W4297883962",
        "https://openalex.org/W4377115988",
        "https://openalex.org/W4324387439",
        "https://openalex.org/W4386249608",
        "https://openalex.org/W4387703131",
        "https://openalex.org/W4382020836",
        "https://openalex.org/W4385972264",
        "https://openalex.org/W4365503720",
        "https://openalex.org/W4321499657",
        "https://openalex.org/W4385850946",
        "https://openalex.org/W4375860725",
        "https://openalex.org/W2148623326",
        "https://openalex.org/W3121109690",
        "https://openalex.org/W4385409936",
        "https://openalex.org/W2168932951",
        "https://openalex.org/W1986908450",
        "https://openalex.org/W4381599219",
        "https://openalex.org/W2144619666",
        "https://openalex.org/W4376108530",
        "https://openalex.org/W3130272805",
        "https://openalex.org/W1997866278",
        "https://openalex.org/W2905536095",
        "https://openalex.org/W4386001316",
        "https://openalex.org/W3198071226",
        "https://openalex.org/W4385266429",
        "https://openalex.org/W4366660674",
        "https://openalex.org/W94062176",
        "https://openalex.org/W4308869750",
        "https://openalex.org/W4386867830",
        "https://openalex.org/W4387681231",
        "https://openalex.org/W4324020772",
        "https://openalex.org/W4379377098",
        "https://openalex.org/W3093176888",
        "https://openalex.org/W4306913606",
        "https://openalex.org/W3193637582",
        "https://openalex.org/W4310153968",
        "https://openalex.org/W4376108512",
        "https://openalex.org/W4390546602",
        "https://openalex.org/W4364363895",
        "https://openalex.org/W4387823622",
        "https://openalex.org/W4308767044"
    ],
    "abstract": null,
    "full_text": "RESEARCH Open Access\n© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, \nsharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The \nCreative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available \nin this article, unless otherwise stated in a credit line to the data.\nArtsi et al. BMC Medical Education          (2024) 24:354 \nhttps://doi.org/10.1186/s12909-024-05239-y\nBMC Medical Education\n*Correspondence:\nYaara Artsi\nyaara.artsi77@gmail.com\n1Azrieli Faculty of Medicine, Bar-Ilan University, Ha’Hadas St. 1, Rishon Le \nZion, Zefat 7550598, Israel\n2Department of Diagnostic Imaging, Chaim Sheba Medical Center, Ramat \nGan, Israel\n3Tel-Aviv University School of Medicine, Tel Aviv, Israel\n4DeepVision Lab, Chaim Sheba Medical Center, Ramat Gan, Israel\n5Division of Data-Driven and Digital Medicine (D3M), Icahn School of \nMedicine at Mount Sinai, New York, NY, USA\n6The Charles Bronfman Institute of Personalized Medicine, Icahn School of \nMedicine at Mount Sinai, New York, NY, USA\nAbstract\nBackground Writing multiple choice questions (MCQs) for the purpose of medical exams is challenging. It requires \nextensive medical knowledge, time and effort from medical educators. This systematic review focuses on the \napplication of large language models (LLMs) in generating medical MCQs.\nMethods The authors searched for studies published up to November 2023. Search terms focused on LLMs \ngenerated MCQs for medical examinations. Non-English, out of year range and studies not focusing on AI generated \nmultiple-choice questions were excluded. MEDLINE was used as a search database. Risk of bias was evaluated using a \ntailored QUADAS-2 tool.\nResults Overall, eight studies published between April 2023 and October 2023 were included. Six studies used \nChat-GPT 3.5, while two employed GPT 4. Five studies showed that LLMs can produce competent questions valid for \nmedical exams. Three studies used LLMs to write medical questions but did not evaluate the validity of the questions. \nOne study conducted a comparative analysis of different models. One other study compared LLM-generated \nquestions with those written by humans. All studies presented faulty questions that were deemed inappropriate for \nmedical exams. Some questions required additional modifications in order to qualify.\nConclusions LLMs can be used to write MCQs for medical examinations. However, their limitations cannot be \nignored. Further study in this field is essential and more conclusive evidence is needed. Until then, LLMs may serve \nas a supplementary tool for writing medical examinations. 2 studies were at high risk of bias. The study followed the \nPreferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines.\nKeywords Large language models, Generative pre-trained transformer, Multiple choice questions, Medical \neducation, Artificial intelligence, Medical examination\nLarge language models for generating \nmedical examinations: systematic review\nYaara Artsi1*, Vera Sorin2,3,4, Eli Konen2,3, Benjamin S. Glicksberg5, Girish Nadkarni5,6 and Eyal Klang5,6\nPage 2 of 11\nArtsi et al. BMC Medical Education          (2024) 24:354 \nBackground\nThere is a global shortage of clinical practitioners and \nincreasing demand for medical professionals. This need \npresents significant challenges in the healthcare system \n[1–3]. In response, the number of medical schools and \nstudents has been rising worldwide [ 4, 5], leading to an \nincrease in the demand for written tests.\nMultiple choice questions (MCQs) are considered pop-\nular for testing applied knowledge in the basic and clini -\ncal sciences [ 6]. When constructing good quality MCQ, \nthe agreed upon model comprises a stem, the initial part \nof the question, which is clearly written, containing all the \ninformation necessary to answer the question. The lead-\nin question contains only one answer that is clearly the \nbest choice, followed by a number of optional answers \ncalled “distractors” . The distractors should be plausible to \nthose without detailed knowledge of the subject, reduc -\ning the chance of guessing the correct answer [ 7]. MCQs \nshould cover a broad range of the curriculum and be rep-\nresentative of the material that students are expected to \nlearn.\nThe item difficulty, i.e. difficulty level of the MCQs, \nshould also be appropriate for the level of the learner. \nThey should be challenging enough to discriminate \nbetween those who understand the material and those \nwho do not, but not so difficult as to be discouraging. \nGood MCQs should be able to discriminate between \nhigher and lower performing students so that students \nwho perform well on the overall exam should be more \nlikely to answer the question correctly than those who \nperform poorly [8, 9, 13].\nCreating multiple choice questions (MCQs) requires \nmedical knowledge, conceptual integration, and avoiding \npotential pitfalls, for example, repeating the same MCQs \nin examinations from year to year, rendering the question \nless useful, or inherent imperfections called item-writing \nflaws (IWFs). A study by Rush et al. details some of the \nmore common writing flaws, including mutually exclu -\nsive distractors, where students can recognize that one \nof the two mutually-exclusive responses is correct, thus \neliminating other options. Another common IWF is “lon-\ngest answer is correct” , a common issue made by exami -\nnation writers in an effort to ensure the correct response \nis indisputable, or use of absolute terms (always, never, \nall). Students recognize that absolute terms usually ren -\nder a statement false [10]. While IWFs may appear trivial, \nthey can affect the way students understand and answer \nquestions [ 10–13]. Producing MCQs is also time con -\nsuming, and any application capable of automating this \nprocess could be highly valuable for medical educators \n[14, 15].\nAmidst these challenges, advancements in natural \nlanguage processing (NLP) are constantly discussed \nand evaluated [ 16], in particular, the introduction of \nOpenAI’s state-of-the-art large language models (LLMs) \nsuch as GPT-3.5 and GPT-4 [ 17, 18]. These models offer \npotential solutions to healthcare education, due to their \nhuman-like text understanding and generation, which \nincludes clinical knowledge [ 19]. This could be pivotal in \nautomating the creation of medically precise MCQs.\nAccording to Bond et al. another possible application \nof AI in medical education is grading patients notes. This \ncan provide additional formative feed-back for students \nin the face of limited faculty availability [20].\nAI based technologies are continuously evolving, \nbecoming more popular in medical education. One such \ntechnology is Virtual Patients (VP), which are interactive \ncomputer simulations of real-life clinical scenarios. They \nare used for medical training, education and assessment. \nBy using AI to provide realistic patient interactions, stu -\ndents can practice clinical decision-making and also \nreceive feedback in a safe and controlled environment \n[21].\nMedical knowledge is continually and rapidly evolv -\ning; therefore, up-to-date medical questions genera -\ntion may be hard to keep up with for medical educators \n[22]. Automatically generated MCQs could be quicker \nto implement when medical knowledge changes cur -\nrent practices, or when new discoveries and forefronts \nare reached. Automated MCQs could also assist medical \nstudents in practicing learning material with a vast data \nresource, which can supply a limitless amount of MCQs \nin a short amount of time [ 23]. Moreover, automated \nMCQs generation can tailor a personalized learning \nexperience which can provide students with a formative \nassessment. Formative assessments allow for feedback \nwhich improves learning, while summative assessments \nmeasure learning. Formative tests were shown to \nimprove classroom practice, and encourage students in \nboth reflective and active review of learning material. In \ngeneral terms, formative assessment assists students in \ndeveloping their learning skills [20, 24, 25].\nHowever, automating MCQs creation introduces \npotential risks, as the accuracy and quality of AI gen -\nerated content is still in question [ 26, 27]. We aimed to \nreview the literature on LLMs’ ability to generate medi -\ncal questions. We evaluated their clinical accuracy and \nsuitability for medical examinations in context of their \nlimitations.\nMethods\nLiterature search\nOn November 2nd 2023 we conducted a search identi -\nfying studies describing LLMs’ applications in generating \nmedical questions. Since the Chat-GPT LLM launched \nby OpenAI on November 30, 2022, we limited our \nsearch period to 2023. We searched PubMed/MEDLINE \nfor papers with the following keywords, using Boolean \nPage 3 of 11\nArtsi et al. BMC Medical Education          (2024) 24:354 \noperators AND/ OR: large language models; GPT; Chat-\nGPT; medical questions; medical education; USMLE; \nMCCQE1; board exam; medical exam. We also checked \nthe references list of selected publications for more rel -\nevant papers. Sections as ‘Similar Articles’ below articles \n(e.g., PubMed) were also inspected for possible additional \narticles.\nEthical approval  was not required, this is a systematic \nreview of previously published research, and does not \ninclude any individual participant information. Our study \nfollowed the Preferred Reporting Items for Systematic \nReviews and Meta-Analyses (PRISMA) guidelines. The \nstudy is registered with PROSPERO (CRD42023481851).\nInclusion and exclusion process\nPublications resulting from the search were initially \nassessed by one author (YA) for relevant titles and \nabstracts. Next, full-text papers underwent an indepen -\ndent evaluation by two authors (EK and VS) (Fig. 1).\nWe included full length studies describing LLMs gen -\nerating medical questions published no earlier than 2023. \nExclusion criteria included: (1) non-English language, (2) \nwrong publication type (e.g. review article, case reports \nand case series, editorial and opinion pieces, commen -\ntaries and letters to the editor, conference abstracts and \npresentations, technical reports and white papers, book \nchapters and monographs), (3) publication year out of \nrange (4), Full-text not available, (5) duplicates, (6) no \nMCQ generation by AI. Any study in question was dis -\ncussed among all authors until reaching a unanimous \nagreement. Risk of bias and applicability were evaluated \nusing the tailored QUADAS-2 tool (Fig. 2).\nRisk of bias and applicability were evaluated using the \nQUADAS-2 tool. (Fig. 2).\nFig. 1 Flow diagram of the search and inclusion process in the study. Flow Diagram of the Inclusion Process. Flow diagram of the search and inclusion \nprocess based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, November 2023\n \nPage 4 of 11\nArtsi et al. BMC Medical Education          (2024) 24:354 \nResults\nStudy selection and characteristics\nThe initial literature search resulted in 838 articles. Eight \nstudies met our inclusion criteria (Fig.  1). Most studies \nwere retrospective: 6/8 (75%). One study is cross-sec -\ntional and one study is prospective. Most studies used \nChat-GPT (3.5 or 4) as an AI model of choice, other \nmodels evaluated included Microsoft’s Bing and Google’s \nBard. The MCQs were produced with varying parameters \n(Table 1). Overall, 5/8 (62.5%) studies demonstrated valid \nMCQs. 6/8 (75%) of the studies utilized the latest version \nChat-GPT 4 (Fig. 3.)\nDescriptive summary of results\nCheung et al. [ 28] were the first, and so far, the only \nstudy to compare LLM to humans in MCQs writing. \nChat-GPT 3.5 plus generated the MCQs. The reference \nfor the prompt were two standard undergraduate medi -\ncal textbooks: Harrison’s Principles of Internal Medicine \nthe 21th edition for medicine [ 29], and Bailey and Love’s \nShort Practice of Surgery 27th Edition for surgery [ 30]. \nOnly four choices were given per question. Also, only \ntext and knowledge-based questions were generated. No \nmodification to the MCQs was allowed after generation. \nChat-GPT 3.5 performed relatively well in the task. The \noverall time required for the AI to generate 50 MCQs was \n21 min. This is about 10% of the total time human writ -\ning required (211  min). However, the questions written \nTable 1 General features of the articles in the study\nStudy Author Month Journal Study design AI \ntool\n1 Sevgi \net al.\nApril Neurosurgical \nReview\nRetrospective Chat-\nGPT \n3.5\n2 Biswas May Annals of \nBiomedical \nEngineering\nRetrospective Chat-\nGPT \n3.5\n3 Agar-\nwal et \nal.\nJune Cureus Cross-sectional \nstudy\nChat-\nGPT,\nBard, \nBing\n4 Ayub \net al.\nAugust Cureus Retrospective Chat-\nGPT \n3.5\n5 Cheung \net al.\nAugust PLOS ONE Prospective Chat-\nGPT \n3.5 \nplus\n6 Totlis \net al.\nAugust Surgical and \nRadiologic \nAnatomy\nRetrospective Chat-\nGPT \n4\n7 Han et \nal.\nOctober Medical \nTeacher\nRetrospective Chat-\nGPT \n3.5\n8 Klang \net al.\nOctober BMC Medical \nEducation\nRetrospective Chat-\nGPT \n4\nSummary of the articles in the literature that applied AI for generating medical \nquestions, November 2023\nFig. 2 Risk of Bias and Applicability Judgments in QUADAS-2. QUADAS-2 table for potential bias and applicability. Risk of bias and applicability were \nevaluated using the tailored QUADAS-2 tool, November 2023\n \nPage 5 of 11\nArtsi et al. BMC Medical Education          (2024) 24:354 \nby humans were far better. Both in terms of quality and \nvalidity, outperforming the AI in a total score of 30 (60%) \neligible MCQs (Table 2).\nKlang et al. [ 31] performed blind assessment of the \ngenerated questions. They did not disclose to the evalu -\nators whether the MCQs origin was AI. At first, they \nasked Chat-GPT 4 to create MCQs on the topic of inter -\nnal medicine. They used as reference (few-shot learning) \na former exam of the same subject. The MCQs had four \npossible answers, with the correct answer marked with \nan asterisk. At first, the generated MCQs were short with \nno clinical background. This required a second prompt -\ning of the AI model, specifically requesting the AI to \ncreate MCQs with clinical history. The study showed \npromising results, with the majority of MCQs deemed \nvalid as exam questions (Table 2).\nIn a cross-sectional study, Agarwal et al. [32] compared \ndifferent LLMs. They compared Chat-GPT 3.5/Bard/\nBing in MCQs generating capability. They used as refer -\nence the 11-module curriculum for physiology, created \nby The Indian National Medical Commission (NMC). \nThe authors requested in the prompt to Generate five \ndifficult reasoning-based MCQs, fitting levels of Bach -\nelor of Medicine, and Bachelor of Surgery (MBBS). Chat-\nGPT’s generated MCQs were significantly more valid \nthan the other AI tools examined in the study. However, \nthe difficulty level was lower compared to Bard and Bing \n(Table 2).\nAyub et al. [ 33] focused on medical board examina -\ntion for Dermatology. They utilized Chat-PDF to upload \nentire PDF files into a Chat-GPT 3.5 portal. The reference \nused was “Continuing medical education” (CME) articles, \ntaken from the Journal of the American Academy of Der -\nmatology (JAAD). This reference is considered high-yield \nreview material for the American Board of Dermatol -\nogy Applied Exam (ABD-AE). This study’s prompt was \nnot detailed in the paper. The three parameters to evalu -\nate the MCQs were accuracy, complexity, and clarity. \nOnly 16 (40%) of the generated questions were applica -\nble (Table  2). The rest were unclear 9 (22%), inaccurate \n5 (13%) or had low complexity 10 (25%) (Table  3). Sevgi \net al. [34] asked Chat-GPT 3.5 to prepare three questions \nwith answers and explanations at a level appropriate for \na neurosurgery board exam. There was no independent \nevaluation of the MCQs.\nHan et al. [ 35] instructed Chat-GPT 3.5 to write three \nMCQs, each containing clinical background and lab val -\nues. Each time they requested Chat-GPT to rephrase the \nquestion. First, for a different correct answer and then for \nan increased level of difficulty. There was no independent \nevaluation of the MCQs.\nTotlis et al. [ 36] asked Chat-GPT 4 to generate MCQs \non the topic of anatomy. In the prompt they requested \nFig. 3 Illustration of multiple-choice questions (MCQs) generation and summary of preliminary results. A graphical illustration of MCQs generation and \npreliminary data. Upper row images were created using Chat-GPT 4 and DALI, illustrating the MCQs generation process via a large language model. The \nimages created in the bottom row showcase preliminary data results, November 2023\n \nPage 6 of 11\nArtsi et al. BMC Medical Education          (2024) 24:354 \nincreasing difficulty and matching correct pairs. There \nwas no independent evaluation of the MCQs. Biswas [ 37] \nrequested in the prompt to prepare MCQs for USMLE \nstep 1 exam. There was no independent evaluation of the \nMCQs.\nAll studies presented some faulty questions that were \ndeemed inappropriate for medical exams. Some ques -\ntions required additional modifications in order to qual -\nify (Table  3). We included in additional files examples \nfrom each study, demonstrating valid MCQs as well as \nfaulty MCQs for various reasons (Supplementary Table \n1.)\nDiscussion\nIn this study we explored large language Models (LLMs)’ \napplicability in generating medical questions, specifically \nmultiple choice questions (MCQs) for medical examina -\ntions. The studies we reviewed did not continue to test \nthe generated MCQ in a real-world setting, i.e. with \nmedical students. In order to truly evaluate the feasibility \nof LLMs application in the medical education field, this \nshould be the next logical step.\nMCQs are an essential component of medical exams, \nused in almost every aspect of medical education [ 12, \n13], yet they are time consuming and expensive to create \nTable 2 Key parameters investigated in each study\nAuthor No.\nof\nMCQs\nTested\nvs.\nHuman\nMedical\nField\nQuestions\nEvaluated\nBy\nPerformance\nScores\nSevgi et al. 3 No Neurosurgery Evaluated by the\nauthor according\nto current literature\n2 (66.6%) of the questions\nwere accurate\nBiswas 5 No General N/A N/A\nAgarwal et al. 320 No Medical Physiology 2 Physiologists p value validity < 0.001 for:\n Chat-GPT vs. Bing < 0.001\n Bard vs. Bing < 0.001\np value of difficulty < 0.006\nChat-GPT vs. Bing 0.010\nChat-GPT vs. Bard 0.003\nAyub et al. 40 No Dermatology 2 board certified\ndermatologists\n16 (40%) of questions valid for exams\nCheung et al. 50 Yes Internal Medicine/Surgery 5 International\nmedical experts\nand educators\nOverall performance:\n AI score 20 (40%) vs. Human score 30 (60%)\n Mean difference -0.80 ± 4.82\nTotal time required:\n AI 20 min 25 s vs. Human 211 min 33 s\nTotlis et al. 18 No Anatomy N/A N/A\nHan et al. 3 No Biochemistry N/A N/A\nKlang et al. 210 No Internal Medicine\nSurgery\nObstetrics & Gynecology\nPsychiatry\nPediatrics\n5 Specialist\nphysicians in the\ntested fields\nProblematic questions by field:\n Surgery 30%\n Gynecology 20%\n Pediatrics 10%\n Internal medicine 10%\n Psychiatry 0%\nSummary of key parameters investigated in each study, November 2023\nTable 3 Present faulty questions generated by the AI\nAuthor Medically\nIrrelevant\nQuestions\nInvalid\nfor\nMedical\n Exam\nInaccurate/Wrong\nQuestion\nInaccurate/Wrong\nAnswer\nor\nAlternative answers\nLow\nDifficulty\nLevel\nSevgi et al. N/A N/A N/A 1 (33.3%) N/A\nBiswas N/A N/A N/A N/A N/A\nAgarwal et al. N/A Highly valid N/A V/A Somewhat difficult\nAyub et al. 9 (23%) 24 (60%) 5 (13%) 5 (13%) 10 (25%)\nCheung et al. 32 (64%) 28 (56%) 32 (64%) 29 (58%) N/A\nTotlis et al. N/A 8 (44.4%) N/A N/A 8 (44.4%)\nHan et al. N/A N/A N/A N/A 3 (100%)\nKlang et al. 2 (0.95%) 1 (0.5%) 12 (5.7%) 14 (6.6%) 2 (0.95%)\nSummary of faulty questions generated by the AI, November 2023\nPage 7 of 11\nArtsi et al. BMC Medical Education          (2024) 24:354 \n[38]. The possibility of AI generated questions can pro -\nvide an important opportunity for the medical commu -\nnity and transform the way written tests are generated. \nUsing LLMs to support these tasks can potentially save \ntime, money, and reduce burnout, especially in a system \nalready sustaining itself on limited resources [39].\nBenefits of AI-generated educational content\nBurn-out, poor mental health, and growing personal dis -\ntress are constantly studied in clinical practitioners [ 40]. \nHowever, academic physicians experience a unique set \nof additional challenges, such as increased administra -\ntive work, less time with patients, and increased clinical \nresponsibilities. As a result, they have less time for tradi -\ntional academic pursuits such as research and education \n[41–43]. In the famous words of Albert Einstein: “Bureau-\ncracy is the death of any achievement” .AI can potentially \nrelieve medical educators from tiresome bureaucracy and \nadministrative work, allowing them to focus on the areas \nthat they view as most personally meaningful and avoid \ncareer dissatisfaction [42, 44].\nMoreover, AI-generated MCQs can assist medical stu -\ndents by creating personalized learning experience, while \naccessing current up-to-date information [ 45]. These are \nonly a few examples of the benefits of AI in the genera -\ntion of medical MCQs, and new areas for its utility are \ncontinuously discovered.\nDrawbacks of AI-generated educational content\nNowadays, AI continues to evolve, becoming more inte -\ngrated in various medical fields [ 46]. AI performance \nis fast, efficient and with what seems like endless data \nresources [47]. In almost every study we reviewed, LLMs’ \nexecution was more than satisfactory with the consensus \nthat AI is capable of producing valid questions for medi -\ncal exams. Presented here are examples for valid MCQs \ngenerated in the studies:\nExample 01 “Which of the following is a negative symp -\ntom of schizophrenia?”\n(A)  Hallucinations.\n(B)  Delusions.\n(C)  Anhedonia.\n(D)  disorganized speech.\nExample 02 “What is the anatomical term for the socket \nin the pelvic bone where the femur articulates?”\n(A)  Acetabulum.\n(B)  glenoid cavity.\n(C)  foramen magnum.\n(D)  fossa ovalis.\nHowever, while these models show promise as an educa -\ntional tool, their limitations must be acknowledged.\nOne notable limitation is a phenomenon known as \n“hallucination” [48]. This occurs in a wide variety of sce -\nnarios, resulting in outputs that lack logical consistency \nor completely unfactual information [ 49]. This phenom -\nenon is unacceptable for MCQs. Issues in MCQs genera -\ntion can arise from AI hallucinations and beyond, such as \ninappropriate MCQ complexity to the material, multiple \ncorrect answers and other inaccuracies. Presented here \nare examples for faulty MCQs generated by the AI:\nExample 03  “Which of the following vessels is NOT a \ncomponent of the Circle of Willis?”\n(A)  Anterior cerebral artery.\n(B)  Posterior communicating artery.\n(C)  Middle cerebral artery.\n(D)  Vertebral artery.\n(E)  Superior cerebellar artery.\nIn the above mentioned MCQ both D and E are correct.\nExample 04  “Which of the following is a characteristic \nfeature of melanoma?”\n(A)  Uniform color.\n(B)  Smooth borders.\n(C)  Symmetry.\n(D)  Irregular pigmentation.\nThe above-mentioned MCQ was deemed as low com -\nplexity for a standard exam, after a rigorous evaluation \nby a board-certified specialist in this field. The ability of \nAI to integrate contextual and sensory information is still \nnot fully developed, as well as its understanding of non-\nverbal cues or body language. Furthermore, racial bias in \nmedical education is a serious issue [ 50]. Inherent bias in \ndata and inaccuracies of AI generated educational con -\ntent is troubling, and could perpetuate a grave affliction \nof the medical education system [51, 52].\nAnother consideration is the logistics necessary to \nimplement AI in healthcare and education. New tech -\nnologies require training, commitment and investment in \norder to be maintained and managed in a sustainable way. \nSuch a process can take time and energy [53]. In addition, \ncareful consideration of prompt crafting must be a requi -\nsite for AI generated MCQs application in medical edu -\ncation. In each study, we examined the process of crafting \nthe MCQs. We noticed a wide range of approaches to \nwriting the prompts. In some studies, additional modi -\nfications took place in order to improve the validity of \nthe questions. This emphasizes the importance and \nPage 8 of 11\nArtsi et al. BMC Medical Education          (2024) 24:354 \nsensitivity of prompts, and the need for training educa -\ntors and students in AI literacy and prompt engineering.\nPrompt-engineering may be a task that requires spe -\ncific training, so that the prompt is phrased correctly and \nthe MCQs quality is not impaired. A good way for clini -\ncal practitioners and medical educators to enhance the \nquality of their prompts, is to first familiarize themselves \nwith LLMs and understand the fundamentals of machine \nlearning. General guidelines for optimizing prompts sug -\ngest trying to be as specific as possible, provide appropri -\nate setting and context when phrasing the prompt, ask \nopen ended questions, and request examples in order \nto clarify the meaning of a concept or idea [ 54]. A poor \nprompt for example is “Tell me about heart disease. ” \nThis prompt is not specific enough, and a good way to \nimprove this prompt is to add details, for example “What \nare the most common risk factors for coronary artery \ndisease?”\nParticular concerns in regards to applications of AI in \nmedical education are ethics and data privacy [ 55]. The \ncurrent literature is limited on how to guide medical \neducators, ensuring that they are using AI ethically and \nresponsibly in their teaching. Accordingly, awareness of \nthe complexities of ethics and data privacy while using AI \nin medical education is called for. According to Masters \n(2023), these complexities include data gathering, ano -\nnymity and privacy, consent, data ownership, security, \ndata and algorithm bias, transparency, responsibility, \nautonomy, and beneficence [56].\nEqually important limitation of AI integration in edu -\ncation is accountability. The “black box” of AI models \nrefers to the fact that much of the internal workings of \nthe system are invisible to the user. Medical educators \nmight use the AI to generate an exam, write the input and \nreceive the output, but the system’s code or logic cannot \nbe questioned or explained [57].\nAn additional aspect to consider is the longstanding \nconcern of AI replacing human jobs, particularly within \nthe medical workforce [ 58]. This thought process could \ncause resistance to AI utility and integration in clinical \npractice. This notion is unlikely in the near future and \npossibly ever. There is a quality to human interaction in \ncare that cannot be replaced by machines. But, distrust in \nAI technology is yet another challenge to its implementa-\ntion [ 59]. In light of this concern, it’s important to take \ninto consideration medical educators and students’ per -\nception of AI and LLMs on their application in medical \neducation. Banerjee et al. examined postgraduate trainee \ndoctors’ perception on the impact of AI on clinical edu -\ncation, with overall positive perception of AI technolo -\ngies’ impact on clinical training [60].\nIn contrast, a recent study showed that even though AI \nis currently progressing towards clinical implementation, \nthere was a lack of educational opportunities about AI in \nmedicine among medical trainees [ 61]. When consider -\ning future research in this field, not only should the LLMs \nperformance be studied, but also the understanding and \nacceptance of this technology among educational staff  \nand students. There should be a continuous conversa -\ntion about how humans and AI can work together, for \ninstance in the sense of computer-aided diagnosis.\nPerhaps one of the biggest concerns of AI application \nin medical education is impairing students’ critical think-\ning. According to Van de Ridder et al., self-reflection and \ncriticism are crucial for a medical student’s learning pro -\ncess and professional growth. In a reality where a student \ncan delegate to Chat-GPT tasks such as writing personal \nreflection or learning experiences, the students deny \nthemselves of the opportunity to self-reflect and grow as \nphysicians [62].\nLastly, all except for one study we examined [ 28], did \nnot compare the AI generated MCQs with human written \nMCQs, and none of the studies tested the AI generated \nMCQs in a real-world setting, i.e., testing medical stu -\ndents. We believe this is the next required step in perfect-\ning LLMs as a tool to assist in medical exam generation. \nA paper published after our search period by Laupichler \net al. conducted this comparison in student performance \nin answering AI vs. human generated MCQs [ 63]. They \nfound no statistically significant difference in item diffi -\nculty between AI generated MCQs and human generated \nquestions, but discriminatory power was statistically sig -\nnificantly higher in humans than LLM questions.\nApplication of AI generated MCQs in medical educa -\ntion is still in its early stages. Although it shows much \npromise, it is imperative to take into consideration the \nsignificant shortcomings and challenges such application \nentails. AI should be used wisely and responsibly while \nintegrating it into the medical education domain.\nLimitations\nOur review has several limitations. Due to heterogene -\nity in study design and data, we were unable to perform \na meta-analysis. Our search yielded a low number of \nresults (eight). Only one author rated the initial results.\nIn addition, a notable limitation is the methodologi -\ncal quality of some of the analyzed studies. Most of the \nstudies are retrospective in nature. Future longitudinal \nstudies could help in understanding the long-term effec -\ntiveness and impact of LLMs in medical education. None \nof the questions were image or graph based, which is an \nintegral part of medical exams. Three studies did not base \ntheir prompt on a valid medical reference, such as pre -\nvious exams or approved syllabus. Three studies did not \nevaluate the questions after they were generated. Two \nstudies were at high risk of bias.\nWe limited our search to PubMed/MEDLINE. Also, \nsince Chat-GPT was launched by OpenAI on November \nPage 9 of 11\nArtsi et al. BMC Medical Education          (2024) 24:354 \n30, 2022, we restricted our search period to 2023. We did \nnot expect to find relevant studies on the application of \nLLMs in medical education in earlier years. We acknowl -\nedge the fact that expanding the search could provide a \nmore comprehensive overview of the development and \nuse of LLMs in medical education.\nFurthermore, we excluded non-English papers, thereby \npreventing a more global comprehensive perspective on \ncultural difference in LLMs application in education.\nWe recognize these choices narrow our review’s scope. \nThis might exclude various relevant studies, possibly lim -\niting diverse insights.\nConclusion\nAI-generated MCQs for medical exams are feasible. The \nprocess is fast and efficient, demonstrating great promise \nin the future of medical education and exam preparation. \nHowever, their use warrants cautious and critical evalu -\nation. Awareness of AI limitations is imperative in order \nto avoid misuse and deterioration of medical education \nquality. We strongly suggest that further research should \nbe conducted to determine the long-term effectiveness \nand impact of AI generated MCQs in comparison to \ntraditional educational methods, as well as testing their \nacceptance and understanding among the medical educa-\ntion community. Until more advancements are achieved, \nAI should be viewed as a powerful tool best utilized by \nexperienced professionals.\nAbbreviations\nLLM  Large language models\nMCQ  Multiple choice question\nGPT  Generative Pre-trained Transformer\nIWF  Item writing flaw\nAI  Artificial intelligence\nVP  Virtual Patients\nSupplementary Information\nThe online version contains supplementary material available at https://doi.\norg/10.1186/s12909-024-05239-y.\nSupplementary Material 1: Supplementary Table 1. Examples from \nstudies showcasing valid and faulty MCQs\nSupplementary Material 2: PRISMA abstract 2020 checklist \nSupplementary Material 3: Additional Files Legends \nAcknowledgements\nNot applicable.\nAuthor contributions\nAll authors contributed to the study conception and design. Initial \nconceptualization, literature search and data analysis were performed by YA, \nVS and EK. The first draft of the manuscript was written by YA and all authors \ncommented and revised on previous versions of the manuscript. All authors \nread and approved the final manuscript.\nFunding\nThis research did not receive any specific grant from funding agencies in the \npublic, commercial, or not-for-profit sectors.\nData availability\nAll data generated or analyzed during this study are included in this published \narticle and supplementary files.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 17 January 2024 / Accepted: 28 February 2024\nReferences\n1. Boniol M, Kunjumen T, Nair TS, Siyam A, Campbell J, Diallo K. The global \nhealth workforce stock and distribution in 2020 and 2030: a threat to equity \nand ‘universal’ health coverage? BMJ Glob Health. 2022;7(6):e009316. https://\ndoi.org/10.1136/bmjgh-2022-009316. PMID: 35760437; PMCID: PMC9237893.\n2. GBD 2019 Human Resources for Health Collaborators. Lancet. \n2022;399(10341):2129–54. https://doi.org/10.1016/S0140-6736(22)00532-3. \nMeasuring the availability of human resources for health and its relationship \nto universal health coverage for 204 countries and territories from 1990 to \n2019: a systematic analysis for the Global Burden of Disease Study 2019.\n3. Zhang X, Lin D, Pforsich H, Lin VW. Physician workforce in the United States of \nAmerica: forecasting nationwide shortages. Hum Resour Health. 2020;18(1):8. \nhttps://doi.org/10.1186/s12960-020-0448-3. Published 2020 Feb 6.\n4. Rigby PG, Gururaja RP . World medical schools: the sum also rises. JRSM Open. \n2017;8(6):2054270417698631. https://doi.org/10.1177/2054270417698631. \nPublished 2017 Jun 5.\n5. Hashem F, Marchand C, Peckham S, Peckham A. What are the impacts of set-\nting up new medical schools? A narrative review. BMC Med Educ. 2022;22(1). \nhttps://doi.org/10.1186/s12909-022-03835.\n6. Naidoo M. The pearls and pitfalls of setting high-quality multiple choice \nquestions for clinical medicine. S Afr Fam Pract (2004). 2023;65(1):e1–e4. \nhttps://doi.org/10.4102/safp.v65i1.5726. Published 2023 May 29.\n7. Al-Rukban MO. Guidelines for the construction of multiple choice questions \ntests. J Family Community Med. 2006;13(3):125–33.\n8. Kumar D, Jaipurkar R, Shekhar A, Sikri G, Srinivas V. Item analysis of multiple \nchoice questions: a quality assurance test for an assessment tool. Med J \nArmed Forces India. 2021;77(Suppl 1):85–S89. https://doi.org/10.1016/j.\nmjafi.2020.11.007.\n9. Sim SM, Rasiah RI. Relationship between item difficulty and discrimination \nindices in true/false-type multiple choice questions of a para-clinical multi-\ndisciplinary paper. Ann Acad Med Singap. 2006;35(2):67–71.\n10. Rush BR, Rankin DC, White BJ. The impact of item-writing flaws and item \ncomplexity on examination item difficulty and discrimination value. BMC \nMed Educ. 2016;16(1):250. Published 2016 Sep 29. https://doi.org/10.1186/\ns12909-016-0773-3.\n11. Przymuszała P , Piotrowska K, Lipski D, Marciniak R, Cerbin-Koczorowska M. \nGuidelines on writing multiple choice questions: A Well-received and effec-\ntive Faculty Development intervention. SAGE Open. 2020;10(3). https://doi.\norg/10.1177/2158244020947432.\n12. Balaha MH, El-Ibiary MT, El-Dorf AA, El-Shewaikh SL, Balaha HM. Construc-\ntion and writing flaws of the multiple-choice questions in the published \ntest banks of obstetrics and gynecology: adoption, caution, or Mitigation? \nAvicenna J Med. 2022;12(3):138–47. https://doi.org/10.1055/s-0042-1755332. \nPublished 2022 Aug 31.\n13. Coughlin PA, Featherstone CR. How to write a high quality multiple \nchoice question (MCQ): a Guide for clinicians. Eur J Vasc Endovasc Surg. \n2017;54(5):654–8. https://doi.org/10.1016/j.ejvs.2017.07.012.\n14. Homolak J. Opportunities and risks of ChatGPT in medicine, science, \nand academic publishing: a modern Promethean dilemma. Croat Med J. \n2023;64(1):1–3. https://doi.org/10.3325/cmj.2023.64.1.\nPage 10 of 11\nArtsi et al. BMC Medical Education          (2024) 24:354 \n15. Gilardi F, Alizadeh M, Kubli M. ChatGPT outperforms crowd workers for \ntext-annotation tasks. Proc Natl Acad Sci U S A. 2023;120(30):e2305016120. \nhttps://doi.org/10.1073/pnas.2305016120.\n16. Sorin V, Barash Y, Konen E, Klang E. Deep-learning natural language \nprocessing for oncological applications. Lancet Oncol. 2020;21(12):1553–6. \n2045(20)30615-X.\n17. Clusmann J, Kolbinger FR, Muti HS et al. The future landscape of large \nlanguage models in medicine. Commun Med (Lond). 2023;3(1):141. Published \n2023 Oct 10. https://doi.org/10.1038/s43856-023-00370-1.\n18. Eysenbach G. The role of ChatGPT, Generative Language models, and Arti-\nficial Intelligence in Medical Education: a conversation with ChatGPT and a \ncall for Papers. JMIR Med Educ. 2023;9:e46885. https://doi.org/10.2196/46885. \nPublished 2023 Mar 6.\n19. Brin D, Sorin V, Vaid A et al. Comparing ChatGPT and GPT-4 performance in \nUSMLE soft skill assessments. Sci Rep. 2023;13(1):16492. Published 2023 Oct 1. \nhttps://doi.org/10.1038/s41598-023-43436-9.\n20. Bond WF, MD MS, Zhou JMS, Bhat. Suma PhD3; Park, Yoon Soo PhD4; Ebert-\nAllen, Rebecca A.5; Ruger, Rebecca L.6; Yudkowsky, Rachel MD, MHPE7. \nAutomated Patient Note Grading: Examining Scoring Reliability and Feasibil-\nity. Academic Medicine 98(11S):p S90-S97, November 2023. | https://doi.\norg/10.1097/ACM.0000000000005357.\n21. Quail NPA, Boyle JG. Virtual patients in Health professions Education. Adv Exp \nMed Biol. 2019;1171:25–35. https://doi.org/10.1007/978-3-030-24281-7_3.\n22. Densen P . Challenges and opportunities facing medical education. Trans Am \nClin Climatol Assoc. 2011;122:48–58.\n23. Friederichs H, Friederichs WJ, März M. ChatGPT in medical school: how \nsuccessful is AI in progress testing? Med Educ Online. 2023;28(1):2220920. \nhttps://doi.org/10.1080/10872981.2023.2220920.\n24. Schüttpelz-Brauns K, Karay Y, Arias J, Gehlhar K, Zupanic M. Comparison of \nthe evaluation of formative assessment at two medical faculties with differ-\nent conditions of undergraduate training, assessment and feedback. GMS J \nMed Educ. 2020;37(4):Doc41. https://doi.org/10.3205/zma001334. Published \n2020 Jun 15.\n25. Ismail SM, Rahul DR, Patra I, Rezvani E. Formative vs. summative assessment: \nimpacts on academic motivation, attitude toward learning, test anxiety, and \nself-regulation skill. Lang Test Asia. 2022;12(1):40. https://doi.org/10.1186/\ns40468-022-00191-4.\n26. Bhattacharyya M, Miller VM, Bhattacharyya D, Miller LE. High rates of fabri-\ncated and Inaccurate references in ChatGPT-Generated Medical Content. \nCureus. 2023;15(5):e39238. https://doi.org/10.7759/cureus.39238. Published \n2023 May 19.\n27. Vaishya R, Misra A, Vaish A. ChatGPT: is this version good for healthcare and \nresearch? Diabetes Metab Syndr. 2023;17(4):102744. https://doi.org/10.1016/j.\ndsx.2023.102744.\n28. Cheung BHH, Lau GKK, Wong GTC, et al. ChatGPT versus human in generat-\ning medical graduate exam multiple choice questions-A multinational \nprospective study (Hong Kong S.A.R., Singapore, Ireland, and the United \nKingdom). PLoS ONE. 2023;18(8):e0290691. https://doi.org/10.1371/journal.\npone.0290691. Published 2023 Aug 29.\n29. Harrison’s. Principles of Internal Medicine, 21E | AccessMedicine | McGraw Hill \nMedical. https://accessmedicine.mhmedical.com/book.aspx?bookid=3095.\n30. Williams NS, O’Connell PR, McCaskie AW. Bailey & Love’s short practice of \nsurgery. Taylor & Francis Group; 2018.\n31. K E, P S, G R, et al. Advantages and pitfalls in utilizing artificial intelligence for \ncrafting medical examinations: a medical education pilot study with GPT-4. \nBMC Med Educ. 2023;23(1):772. https://doi.org/10.1186/s12909-023-04752-w. \nPublished 2023 Oct 17.\n32. Agarwal M, Sharma P , Goswami A. Analysing the Applicability of ChatGPT, \nBard, and Bing to generate reasoning-based multiple-choice questions in \nMedical Physiology. Cureus. 2023;15(6):e40977. https://doi.org/10.7759/\ncureus.40977. Published 2023 Jun 26.\n33. Ayub I, Hamann D, Hamann CR, Davis MJ. Exploring the potential and \nlimitations of Chat Generative pre-trained Transformer (ChatGPT) in Gen-\nerating Board-Style Dermatology questions: a qualitative analysis. Cureus. \n2023;15(8):e43717. https://doi.org/10.7759/cureus.43717. Published 2023 \nAug 18.\n34. Sevgi UT, Erol G, Doğruel Y, Sönmez OF, Tubbs RS, Güngör A. The role of an \nopen artificial intelligence platform in modern neurosurgical education: \na preliminary study. Neurosurg Rev. 2023;46(1). https://doi.org/10.1007/\ns10143-023-01998-2.\n35. Han Z, Battaglia F, Udaiyar A, Fooks A, Terlecky SR. February. An Explorative \nAssessment of ChatGPT as an aid in Medical Education: use it with caution. \nmedRxiv (Cold Spring Harbor Laboratory). 2023. https://doi.org/10.1101/2023.0\n2.13.23285879.\n36. Totlis T, Natsis K, Filos D, et al. The potential role of ChatGPT and artificial \nintelligence in anatomy education: a conversation with ChatGPT. Surg Radiol \nAnat. 2023;45(10):1321–9. https://doi.org/10.1007/s00276-023-03229-1.\n37. Biswas S. Passing is great: can ChatGPT Conduct USMLE exams? Ann Biomed \nEng. 2023;51(9):1885–6. https://doi.org/10.1007/s10439-023-03224-y.\n38. Gierl MJ, Lai H, Turner SR. Using automatic item generation to create \nmultiple-choice test items. Med Educ. 2012;46(8):757–65. https://doi.\norg/10.1111/j.1365-2923.2012.04289.x.\n39. Alhalaseh Y, Elshabrawy HA, Erashdi M, Shahait M, Abu-Humdan AM, Al-\nHussaini M. Allocation of the already limited medical resources amid the \nCOVID-19 pandemic, an iterative ethical encounter including suggested solu-\ntions from a real life encounter. Front Med. 2021;7. https://doi.org/10.3389/\nfmed.2020.616277.\n40. Khan RPD. MSc1; Hodges, Brian David MD, PhD2; Martimianakis, Maria Athina \nPhD, MA3. Constructing Burnout: A Critical Discourse Analysis of Burnout in \nPostgraduate Medical Education. Academic Medicine 98(11S):p S116-S122, \nNovember 2023. | https://doi.org/10.1097/ACM.0000000000005358  .\n41. Shanafelt TD, West CP , Sloan JA, et al. Career fit and burnout among academic \nfaculty. Arch Intern Med. 2009;169(10):990–5. https://doi.org/10.1001/\narchinternmed.2009.70.\n42. Woolhandler S, Himmelstein DU. Administrative work consumes one-sixth of \nU.S. physicians’ working hours and lowers their career satisfaction. Int J Health \nServ. 2014;44(4):635–42. https://doi.org/10.2190/HS.44.4.a.\n43. Szulewski AMD, MHPE, PhD1, Braund, Heather PhD2, Dagnone DJ, MD, \nMSc KW, MD6, Hall AK, MD. MMEd7. The Assessment Burden in Compe-\ntency-Based Medical Education: How Programs Are Adapting. Academic \nMedicine 98(11):p 1261–1267, November 2023. | https://doi.org/10.1097/\nACM.0000000000005305.\n44. Lowenstein SR, Fernandez G, Crane LA. Medical school faculty discontent: \nprevalence and predictors of intent to leave academic careers. BMC Med \nEduc. 2007;7:37. https://doi.org/10.1186/1472-6920-7-37. Published 2007 Oct \n14.\n45. Feng S1;, Shen, Yang MD. PhD2. ChatGPT and the Future of Medical Educa-\ntion. Academic Medicine 98(8):p 867–868, August 2023. | https://doi.\norg/10.1097/ACM.0000000000005242.\n46. Maassen O, Fritsch S, Palm J, et al. Future Medical Artificial Intelligence Appli-\ncation requirements and expectations of Physicians in German University \nhospitals: web-based survey. J Med Internet Res. 2021;23(3):e26646. https://\ndoi.org/10.2196/26646. Published 2021 Mar 5.\n47. Ramesh AN, Kambhampati C, Monson JR, Drew PJ. Artificial intelli-\ngence in medicine. Ann R Coll Surg Engl. 2004;86(5):334–8. https://doi.\norg/10.1308/147870804290.\n48. Athaluri SA, Manthena SV, Kesapragada VSRKM, Yarlagadda V, Dave T, \nDuddumpudi RTS. Exploring the boundaries of reality: investigating the Phe-\nnomenon of Artificial Intelligence Hallucination in Scientific writing through \nChatGPT references. Cureus. 2023;15(4):e37432. https://doi.org/10.7759/\ncureus.3743. Published 2023 Apr 11.\n49. Emsley R. ChatGPT: these are not hallucinations - they’re fabrications and \nfalsifications. Schizophrenia (Heidelb). 2023;9(1):52. https://doi.org/10.1038/\ns41537-023-00379-4. Published 2023 Aug 19.\n50. Corsino L, Railey K, Brooks K, et al. The impact of racial Bias in Patient \nCare and Medical Education: Let’s focus on the Educator. MedEdPORTAL. \n2021;17:11183. https://doi.org/10.15766/mep_2374-8265.11183. Published \n2021 Sep 2.\n51. Safranek CW, Sidamon-Eristoff AE, Gilson A, Chartash D. The role of large Lan-\nguage models in Medical Education: applications and implications. JMIR Med \nEduc. 2023;9:e50945. https://doi.org/10.2196/50945. Published 2023 Aug 14.\n52. Vorisek CN, Stellmach C, Mayer PJ, et al. Artificial Intelligence Bias in Health \nCare: web-based survey. J Med Internet Res. 2023;25:e41089. https://doi.\norg/10.2196/41089. Published 2023 Jun 22.\n53. van Gemert-Pijnen JL. Implementation of health technology: directions \nfor research and practice. Front Digit Health. 2022;4:1030194. https://doi.\norg/10.3389/fdgth.2022.1030194. Published 2022 Nov 10.\n54. Meskó B. Prompt Engineering as an important emerging skill for medi-\ncal professionals: Tutorial. J Med Internet Res. 2023;25:e50638. https://doi.\norg/10.2196/50638. Published 2023 Oct 4.\n55. Weidener L, Fischer M. Teaching AI Ethics in Medical Education: a scop-\ning review of current Literature and practices. Perspect Med Educ. \n2023;12(1):399–410. https://doi.org/10.5334/pme.954. Published 2023 Oct 16.\nPage 11 of 11\nArtsi et al. BMC Medical Education          (2024) 24:354 \n56. Masters K. Ethical use of Artificial Intelligence in Health Professions Education: \nAMEE Guide 158. Med Teach. 2023;45(6):574–84. https://doi.org/10.1080/014\n2159X.2023.2186203.\n57. Chan B. Black-box assisted medical decisions: AI power vs. ethical physician \ncare. Med Health Care Philos. 2023;26(3):285–92. https://doi.org/10.1007/\ns11019-023-10153-z.\n58. Shuaib A, Arian H, Shuaib A. The increasing role of Artificial Intelligence \nin Health Care: Will Robots replace doctors in the future? Int J Gen Med. \n2020;13:891–6. https://doi.org/10.2147/IJGM.S268093. Published 2020 Oct \n19.\n59. Starke G, Ienca M. Misplaced Trust and Distrust: how not to engage with \nmedical Artificial Intelligence. Camb Q Healthc Ethics. Published Online Oct. \n2022;20. https://doi.org/10.1017/S0963180122000445.\n60. Banerjee M, Chiew D, Patel KT et al. The impact of artificial intelligence on \nclinical education: perceptions of postgraduate trainee doctors in London \n(UK) and recommendations for trainers. BMC Med Educ. 2021;21(1):429. \nPublished 2021 Aug 14. https://doi.org/10.1186/s12909-021-02870-x.\n61. Pucchio A, Rathagirishnan R, Caton N, et al. Exploration of exposure to \nartificial intelligence in undergraduate medical education: a Canadian cross-\nsectional mixed-methods study. BMC Med Educ. 2022;22(1):815. https://doi.\norg/10.1186/s12909-022-03896-5. Published 2022 Nov 28.\n62. van de Ridder JM, Monica PhD MMMD, Rajput VMD, August, MACP3. Finding \nthe Place of ChatGPT in Medical Education. Academic Medicine 98(8):p 867, \n2023. | https://doi.org/10.1097/ACM.0000000000005254.\n63. Laupichler MC, Rother JF, Grunwald Kadow IC, Ahmadi S, Raupach T. Large \nLanguage models in Medical Education: comparing ChatGPT- to Human-\ngenerated exam questions. Acad Med Published Online Dec. 2023;28. https://\ndoi.org/10.1097/ACM.0000000000005626.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations."
}