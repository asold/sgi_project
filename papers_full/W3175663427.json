{
  "title": "Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs",
  "url": "https://openalex.org/W3175663427",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4308860468",
      "name": "Wen-Yi Hsiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2685658309",
      "name": "Jen-Yu Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5017146803",
      "name": "Yin-Cheng Yeh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2174090984",
      "name": "Yi-Hsuan Yang",
      "affiliations": [
        "Academia Sinica"
      ]
    },
    {
      "id": "https://openalex.org/A4308860468",
      "name": "Wen-Yi Hsiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2685658309",
      "name": "Jen-Yu Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5017146803",
      "name": "Yin-Cheng Yeh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2174090984",
      "name": "Yi-Hsuan Yang",
      "affiliations": [
        "Academia Sinica"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6660690085",
    "https://openalex.org/W2950693330",
    "https://openalex.org/W3047571292",
    "https://openalex.org/W6771747371",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2959020461",
    "https://openalex.org/W6781423933",
    "https://openalex.org/W6755340666",
    "https://openalex.org/W2766393356",
    "https://openalex.org/W6761551260",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W3046715528",
    "https://openalex.org/W6766738422",
    "https://openalex.org/W3015625764",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W6776137863",
    "https://openalex.org/W6764787368",
    "https://openalex.org/W2885835225",
    "https://openalex.org/W6664586463",
    "https://openalex.org/W2986922898",
    "https://openalex.org/W2078009352",
    "https://openalex.org/W3047325651",
    "https://openalex.org/W6756140313",
    "https://openalex.org/W4255825300",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W2966720510",
    "https://openalex.org/W3047386385",
    "https://openalex.org/W2898827701",
    "https://openalex.org/W2056263517",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4297809150",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W3049272330",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3026076535",
    "https://openalex.org/W2953788158",
    "https://openalex.org/W2963408210",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W4287995946",
    "https://openalex.org/W2991108091",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W4287755062",
    "https://openalex.org/W4293775315",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W2892880750"
  ],
  "abstract": "To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a finite set of pre-defined vocabulary. Such a vocabulary usually involves tokens of various types. For example, to describe a musical note, one needs separate tokens to indicate the note’s pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different properties, existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder architecture that uses different feed-forward heads to model tokens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual tokens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5 to 10 times faster at training (i.e., within a day on a single GPU with 11 GB memory), and with comparable quality in the generated music",
  "full_text": "Compound Word Transformer: Learning to Compose Full-Song Music\nover Dynamic Directed Hypergraphs\nWen-Yi Hsiao,1 Jen-Yu Liu,1, Yin-Cheng Yeh,1 Yi-Hsuan Yang2\n1Yating Team, Taiwan AI Labs, Taiwan\n2Academia Sinica, Taiwan\nfwayne391, jyliu, yyeh, yhyangg@ailabs.tw\nAbstract\nTo apply neural sequence models such as the Transformers to\nmusic generation tasks, one has to represent a piece of music\nby a sequence of tokens drawn from a ﬁnite set of pre-deﬁned\nvocabulary. Such a vocabulary usually involves tokens of var-\nious types. For example, to describe a musical note, one needs\nseparate tokens to indicate the note’s pitch, duration, velocity\n(dynamics), and placement (onset time) along the time grid.\nWhile different types of tokens may possess different proper-\nties, existing models usually treat them equally, in the same\nway as modeling words in natural languages. In this paper, we\npresent a conceptually different approach that explicitly takes\ninto account the type of the tokens, such as note types and\nmetric types. And, we propose a new Transformer decoder ar-\nchitecture that uses different feed-forward heads to model to-\nkens of different types. With an expansion-compression trick,\nwe convert a piece of music to a sequence ofcompound words\nby grouping neighboring tokens, greatly reducing the length\nof the token sequences. We show that the resulting model can\nbe viewed as a learner over dynamic directed hypergraphs.\nAnd, we employ it to learn to compose expressive Pop piano\nmusic of full-song length (involving up to 10K individual to-\nkens per song), both conditionally and unconditionally. Our\nexperiment shows that, compared to state-of-the-art models,\nthe proposed model converges 5 to 10 times faster at training\n(i.e., within a day on a single GPU with 11 GB memory), and\nwith comparable quality in the generated music.\nIntroduction\nTo apply neural sequence models such as recurrent neural\nnetworks (RNNs) or Transformers (Vaswani et al. 2017) to\nautomatic music composition (a.k.a., symbolic-domain mu-\nsic generation), one has to represent a piece of music as a\nsequence of tokens drawn from a pre-deﬁned vocabulary\n(Oore et al. 2018). Unlike the case in text, such a vocabu-\nlary usually involves tokens of varioustypes. For example, to\nrepresent a musical score, we may need tokens that describe\nthe content of the musical notes (e.g., pitch and duration),\ntheir placement along time, the instrument that plays each\nnote, as well as indicators of metrical events such as the be-\nginning of a new beat, bar (measure), or musical phrase (Wu\nand Yang 2020). We need such a diverse set of tokens as\nmusic is multifaceted; a type alone captures only a certain\nCopyright c\r2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Illustration of the main ideas of the proposed com-\npound word Transformer: (left) compound word modeling\nthat combines the embeddings (colored gray) of multiple to-\nkens fwt\u00001;kgK\nk=1, one for each token type k, at each time\nstep t\u00001 to form the input ~xt\u00001 to the self-attention layers,\nand (right) toke type-speciﬁc feed-forward headsthat predict\nthe list of tokens for the next time steptat once at the output.\naspect of music (e.g., melody, harmony, rhythm, timbre) and\ncannot faithfully represent a music piece.\nAs different types of (musical) tokens may have different\nproperties, modeling the dependency of these tokens might\nnot be the same as modeling words in text. However, to our\nbest knowledge, little work has been done to explicitly ac-\ncount for the heterogeneity of tokens in music. The tokens\nare mostly treated equally, in the same way as words in text\n(Huang et al. 2019; Payne 2019; Huang and Yang 2020).\nWe are therefore motivated to study in this paper whether\nwe can improve sequence modeling of music by highlighting\nthe role of token types. Our ﬁrst proposal is to customize the\nprediction heads for tokens of different types. Speciﬁcally,\nusing the Transformer as the main architecture of the under-\nlying sequence model, we approach this by using different\nfeed-forward heads for tokens of different types.\nOur second proposal is to group consecutive and related\ntokens in a token sequence into “compound words,” and then\nperform sequence modeling over the resulting sequence of\ncompound words. This is to capture the co-occurrence re-\nlationship of tokens—e.g., to generate a new musical note,\nwe may need at least two consecutive tokens to indicate its\npitch and duration; to change the tempo in the middle of a\npiece of music, we need a token to indicate the target tempo\nvalue, and an co-occurring time-related token to indicate the\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n178\nRepresentation Model window V oc. size Data type\nMusic Transformer (Huang et al. 2019) MIDI-like Transformer 2,048 388 Classical performance\nMuseNet (Payne 2019) MIDI-like* Transformer 4,096 N/A Multi-track MIDI\nLakhNES (Donahue et al. 2019) MIDI-like* Transformer-XL 512 630 Multi-track MIDI\nTR autoencoder (Choi et al. 2020) MIDI-like Transformer 2,048 388 Classical performance\nPop Music TR (Huang and Yang 2020) REMI Transformer-XL 512 332 Pop piano performance\nTransformer V AE (Jiang et al. 2020) MIDI-like Transformer 128 47 Pop lead sheets\nGuitar Transformer (Chen et al. 2020) REMI* Transformer-XL 512 221 Guitar tabs\nJazz Transformer (Wu and Yang 2020) REMI* Transformer-XL 512 451 Jazz lead sheets\nMMM (Ens and Pasquier 2020) MIDI-like* Transformer 2,048 >442 Multi-track MIDI\nThis work CP linear Transformer 5,120 350 Pop piano performance\nTable 1: A comparison of existing Transformer-based models and the proposed one for automatic music composition. The\nrepresentations marked with * are extensions of either MIDI-like (Oore et al. 2018) or REMI (Huang and Yang 2020).\ntime of the tempo change. Under the proposed compound-\nword modeling, the individual tokens (e.g., pitch and dura-\ntion) are still predicted separately with different heads. Yet,\ninstead of predicting them at different time steps, we predict\nmultiple tokens of various typesat once in a single time step.\nThe token embeddings of the tokens predicted at the current\nstep are then combined and fed as the input for the next time\nstep. Namely, the self-attention is computed over combined\nembeddings of individual tokens of a compound word.\nFrom a theoretical point of view, the proposed model can\nbe interpreted as a learner over discrete-time dynamic di-\nrected hypergraphs(Kazemi et al. 2020). Here, a graph con-\nsists of nodes that each corresponds to a token in our vocabu-\nlary. A sequence of tokens can then be viewed as a sequence\nof edges (each connecting two nodes), or a walk, over this\ngraph. A sequence of compound words, in contrast, can be\nviewed as a sequence of hyperedges (each connecting mul-\ntiple nodes) (Feng et al. 2019), over the same graph. We dis-\ncuss this at greater length later in the paper.\nWe refer to the proposed representation as the c\nompound\nword representation, or CP for short. CP can be considered\nas an extension of existing representations, with the follow-\ning additional merits. First, it allows for ﬁne-grained, type-\nspeciﬁc control over the prediction heads. For example, we\ncan now use different loss functions, sampling policies, and\ntoken embedding sizes for different token types.\nSecond, as a compound word represents multiple tokens\nat once, it requires much less time steps to generate a music\npiece using compound words. Namely, the sequence length\nof the same music piece is much shorter in CP than in ex-\nisting representations. As the computational complexity of a\nTransformer is related to the sequence length (Vaswani et al.\n2017), this makes training and inference faster, and may fa-\ncilitate learning the long-range dependency in music.1\nFinally, the sequence length in CP is determined by the\nnumber of compound words in a sequence, not by the num-\nber of individual tokens per compound word. Therefore, it is\npossible to add new token types (by adding the correspond-\ning feed-forward head) to increase the expressivity of the\nrepresentation, without increasing the sequence length. This\n1For example, we can study whether the proposed model cre-\nates music with better “structureness,” or long-term repetitions (Wu\nand Yang 2020; Jhamtani and Berg-Kirkpatrick 2019) in the future.\nmakes it easy to extend to underlying representation, though\nwe do not explore this potential in this work.\nFor performance study, we consider generating expressive\nPop piano music at full-song scale in both the unconditional\nsetting (i.e., from scratch) and conditional setting (i.e., gen-\nerating the piano arrangement given the lead sheet). This in-\nvolves modeling fairly long music sequences for up to 10K\nindividual tokens each. We show that, with CP, we are able\nto train a linear Transformer decoder (Katharopoulos et al.\n2020) with music quality similar to that of strong baselines,\nwith faster training and inference time. We provide audio\nexamples and open source the project at a GitHub repo.2\nRelated Work\nBoth language and music have principles governing the or-\nganization of discrete structural elements (e.g., words or mu-\nsical notes) into sequences (Patel 2003). As such, the Trans-\nformers, which have been ﬁrstly shown to work well for text\ngeneration (Child et al. 2019; Keskar et al. 2019), have been\nincreasingly applied to music generation in recent years, by\ntreating music pieces as sequences of discrete tokens akin to\ntext words. We list some related papers in Table 1.\nTable 1 shows that most existing work adopt a music rep-\nresentation derived from either MIDI-like (Oore et al. 2018)\nor REMI (Huang and Yang 2020), with possible addition of\ntrack- or structure-related tokens. MIDI-like and REMI dif-\nfer mainly in how the advance of time is represented: the\nformer uses [time\nshift] tokens to mark the time interval (in\nabsolute time) between note-related tokens, whereas the lat-\nter assumes symbolic timing and uses [bar] and [position]\ntokens to place tokens on a metrical grid that uniformly di-\nvides a bar into a certain number of positions. Neither MIDI-\nlike nor REMI groups the tokens by token types.3\nExisting work also differs in the length of the attention\nwindow (see the methodology section for deﬁnition) and vo-\ncabulary size (which is data- and task-dependent). To our\nknowledge, our work represents the ﬁrst one to consider Pop\nmusic modeling at full-song scale (involving 10k tokens per\nsong), and to use the recently-proposed linear Transformer\n(Katharopoulos et al. 2020) as the model backbone.\n2https://github.com/YatingMusic/compound-word-transformer\n3Upon paper completion, we noticed an early but preliminary\nattempt of grouping tokens by (Hawthorne et al. 2018b).\n179\nMethodology\nBackground\nFor sequence modeling, we need a conversion function g(\u0001)\nthat converts a music piece Xto a time-ordered sequence of\nsymbolic elements S= g(X) =fw1;w2;:::;w T g, where\nT denotes the resulting sequence length. Given a number of\nsuch sequences, we train a neural sequence model with an\narchitecture such as the Transformer decoder to learn to gen-\nerate new sequences S0. We then use a deterministic inverse\nfunction g\u00001(\u0001) to get a new music piece from such a gener-\nated sequence, namely X0 = g\u00001(S0). There can be differ-\nent algorithms to implement the conversion function and its\ninverse, leading to numerous possible sequence representa-\ntions of the same music piece, e.g.,SMIDI-like = gMIDI-like(X)\nand SREMI = gREMI(X). Different conversion functions (or\nsequence representations) assume different vocabulary sizes\nM, so SMIDI-like and SREMI differ in both T and M.\nA Transformer decoder comprises a stack ofself-attention\nlayers and a stack of feed-forward layers. The self-attention\nlayers operate on a ﬁxed-length sub-sequence of Sto learn\nthe dependency among the elements. The length of such a\nsub-sequence, a.k.a., the attention window, denoted as N,\nis usually much smaller than T, as N directly affects the\nspace complexity of the model. For the vanilla Transformer\n(Vaswani et al. 2017) and its faster variant Transformer-XL\n(Dai et al. 2019), it is O(N2M); for the linear Transformer\n(Katharopoulos et al. 2020), it is O(NM).\nIndividual Tokens vs Compound Words\nIn this paper, we refer to the elements in either SMIDI-like or\nSREMI as the individual tokens. They are drawn from a pre-\ndeﬁned vocabulary V= f1;:::;M g. As mentioned in the\nintroduction, each token is associated with a type deﬁned in\nthe type set, K= f1;:::;K g. We can partition Vinto K\nsubsets by token group, i.e., fVkgK\nk=1.\nWe propose to convert a sequence of tokens (e.g., SREMI)\ninto a sequence of compound words SCP with the following\nprocedure. First, neighboring tokens that deﬁne a musical\nevent together are grouped into a super token , i.e., placed\non the same time step, as illustrated in Figures 2(a)–(b). A\nmusical event here can be a note related one, i.e., to create\na new musical note, or a metrical related one, e.g., to mark\nthe beginning of a new beat, or a new bar. For example, in\nREMI, a note is created by consecutive tokens of [pitch],\n[duration], and [velocity], which are grouped in CP. And,\na tempo or chord change in REMI takes place only at beat\ntimes, so we also group [beat], [chord] and [tempo]. Accord-\ningly, the model has to make multiple predictions (i.e., gen-\nerate multiple tokens) at each time step.\nSecond, we ﬁll the missing token types per time step with\n“[ignore]” tokens, so that at each step there are consistently\nKtokens to be predicted, as illustrated in Figure 2(c). This is\nto make computational modeling feasible, as otherwise the\nshape and meaning of the target output at each time step\nwould be uncertain. In other words, a compound word is\ncomposed of a list of K tokens, each drawn from the cor-\nresponding subset Vk [[ignore], that are placed on the same\ntime step t. Formally, SCP = gCP(X) =fcptgTcp\nt=1, in which\n(a) REMI representation\n(b) Tokens grouped (c) Compound words\nFigure 2: An example illustrating the conversion from a\nsequence of REMI tokens (Huang and Yang 2020) into a\n(shorter) sequence of compound words. A compound word\ncomprises a number of grouped tokens and the [ignore] to-\nkens, which are colored white in (c), as well as a family to-\nken (N: note-related or M: metric-related). Best seen in color.\ncpt = fwt;1;\u0001\u0001\u0001 ;wt;Kg. We view this conversion function\ngCP(\u0001) as performing an expansion-compression trick, as the\noriginal sequence is ﬁrstly expanded to a sequence of KTCP\nindividual tokens, and then compressed to a sequence ofTCP\ncompound words; in general TCP <TREMI <KT CP.\nTo facilitate modeling the CP, we further partition the type\nset Kinto F families. For example, if Kcan be partitioned\ninto two families, the note family KN and metric family KM\n(marked as ‘N’ and ‘M’ in Figure 2(c)), we would haveK=\nKN [KM, and KN \\KM = ;. Each compound word cpt is\nassociated with a family token ft. For a metric-related cpt,\nwe would have wt;k = [ignore], for k2KN. Similarly, for a\nnote-related cpt, wt;k = [ignore], for k2KM.\nCombining Token Embeddings of Adaptive Sizes\nAs input to Transformers, an element in a sequence is rep-\nresented by an embedding vector, xt 2Rd, and then added\nwith a positional embedding vector (Ke, He, and Liu 2020).\nIn CP, we propose to form an embedding vector for a com-\npound word cpt by combining the embedding vectorspt;k of\nthe composing tokens wt;k, as well as an embedding vector\nqt associated with the family tokenft. Speciﬁcally, we com-\nbine the vectors by ﬁrstly concatenating them, and then lin-\nearly projecting the resulting long vector to a d-dimensional\nvector with a projection matrix Win. Namely,\npt;k = Embeddingk(wt;k) ; k= 1;:::;K;\nqt = EmbeddingF(ft) ;\nxt = Win [pt;1 \b:::\bpt;K \bqt] ;\n~xt = Positional Encoding(xt) ;\n(1)\nwhere \bdenotes vector concatenation, and Embedding k(\u0001)\nand EmbeddingF(\u0001) involve the use of lookup tables.\n180\nIn essence, xt can be considered as a compressive repre-\nsentation of the composing tokens wt;k and family token ft.\nWe note the action of compressing the embeddings is rem-\niniscent of the main idea of the Compressive Transformer\n(Rae et al. 2020), which proposes to compresses past mem-\nories beyond the attention window for long-range sequence\nlearning. Unlike it, we compress the memories within the\nattention window deﬁned over the individual tokens.\nA main merit of CP is that we can customize the settings\nfor different token types. Being inspired by the adaptive\nword representation (Baevski and Auli 2018), we use dif-\nferent embedding sizes dk for tokens of different types, i.e.,\npt;k 2Rdk . We basically use larger dk for token types with\nlarger vocabulary size jVkj. See Table 3 for details.\nMulti-head Output Module\nA main proposal of our work is to use different feed-forward\nheads for tokens of different types in a Transformer. Specif-\nically, we have (K + 1)heads in total, one for each token\ntype Vk and an additional one for the token family F.\nInstead of working on the K+ 1heads at the same time,\nwe devise a two-stage setting that predicts the family token\nﬁrst, and then the remaining tokens given the family token.\nSpeciﬁcally, at thet-th time step, the feed-forward procedure\ncan be summarized as:\nht = Self-attn (~xt\u00001) ;\nbft = SampleF(softmax(WFht)) ;\nhout\nt = Wout [ht \bEmbeddingF(bft)] ;\ndwt;k = Samplek\n\u0000\nsoftmax(Wkhout\nt )\n\u0001\n; k= 1;:::;K;\n(2)\nwhere WFand fWkgK\nk=1 are the K+1 feed-forward heads,\nSelf-attn(\u0001) the causal self-attention layers, and Sample(\u0001) a\nsampling function. We empirically ﬁnd that this two-stage\nsetting makes it easier for the model to predict wt;k =\n[ignore], for knot in the target family Kbft\n.\nFigure 1 illustrates Eqs. (1)–(2) in work, omitting the ﬁrst-\nstage part at the output for bft due to space limit.\nAdaptive Sampling Policy\nAt inference time, we use stochastic temperature-controlled\nsampling (Holtzman et al. 2020) to avoid degeneration and\nto increase diversity. With CP, we employ different sampling\npolicies Samplek(\u0001) for different token types; see Table 3.\nGraph Interpretation\nWe discuss the proposed model from a graph-theoretical\npoint of view below. Given a vocabulary of tokens, we can\nconstruct a fully-connected static graph G= (V;E) (Kivel¨a\net al. 2014) comprising nodes V= f1;:::;M gand edges\nE= V\u0002V . Each node corresponds to an individual token\nin our vocabulary. This way, a token sequence SMIDI-like or\nSREMI can be viewed as a sequence of edges (each connect-\ning two nodes), or a walk, over this graph.\nIn CP, the vocabulary (and accordingly the graph) is aug-\nmented with a set of special tokens, denoted as V\u0003, that in-\ncludes for example type-speciﬁc [ignore] tokens and family\ntokens. And, a compound word consists ofK+1 nodes, one\nfrom each of the Ktypes and an additional one from the set\nof family tokens. A sequence of compound words, namely\nSCP, therefore, involves transitions from K+ 1nodes to an-\nother K + 1nodes per time step. Such a transition can be\nviewed as a directedhyperedge (Feng et al. 2019; Jiang et al.\n2019), that connects at onceK+1 source nodes (e.g.,cpt\u00001)\nto K+ 1target nodes (cpt). It is directed because the order\nof the nodes matters (i.e., from t\u00001 to t).\nA sequence of compound words also forms a dynamic di-\nrected hypergraph (Kazemi et al. 2020): fG1;G2;:::; GT g,\nwhere Gt = (V;Et). Starting from an empty graph with no\nedges, at each time step t> 1 we add a new directed hyper-\nedge, labeled with the time stept, connecting in total2K+2\nnodes. In practice, we have a [BOS] token (beginning of se-\nquence) and [EOS] token (end of sequence), so the hyper-\nedge at t= 1and t= T connects to only K+ 2nodes.\nA neural model for graphs, or a graph neural network\n(GNN), can be regarded as an encoder-decoder pair (Kazemi\net al. 2020; Rossi et al. 2020), where an encoder is a func-\ntion that maps from a graph Gto node embeddings zi;i =\n1 :::M , and a decoder takes as input one ore more node\nembeddings and makes a prediction based on these, e.g.,\nnode classiﬁcation or edge prediction. The proposed CP\nTransformer can therefore be regarded as a learner over dy-\nnamic directed hypergraphs, as at each time step t it man-\nages to predict the next hyperedge to be added (i.e., dwt;k\nand bft) based on the node embeddings updated from G<t =\nfG1;G2;:::; Gt\u00001g, or the collection of input embeddings\nx<t = fx1;x2;:::; xt\u00001gmarked with positional embed-\ndings (i.e., edge labels on the directed hyperedges).\nWe note that, while we introduce the proposed methods in\nthe context of music modeling, the idea of compound words\nis generic and may be applicable to sequences seen in other\ndata domains, when multiple tokens (i.e., a hyperedge) are\nneeded to represent a single event, entity, or object.\nImplementation\nTo test the effectiveness of the proposed methods, we im-\nplement a CP Transformer that learns to generate Pop piano\nmusic with human performance characteristics such as ex-\npressive variations in velocity (i.e., the force with which a\nnote is played, which is related to loudness) and tempo (Oore\net al. 2018; Lerch et al. 2019). We consider Pop piano for its\nrichness and expressivity, and for offering a direct perfor-\nmance comparison with the Pop Music Transformer (Huang\nand Yang 2020) (see Table 1).\nSpeciﬁcally, we consider both the conditional and un-\nconditional generation tasks. In the former, a lead sheet\n(i.e., a melody line and an accompanying sequence of chord\nlabels) is given, and the model has to generate a piano per-\nformance according to that. In the latter, the model generates\na piano performance of full-song length from scratch freely.\nWe intend to compare CP with REMI in our evaluation.\nWe provide the implementation details below.\n181\nTask Repre. #words (T)\nmean (\u0006std) max\nConditional REMI 6,432 (\u0006 1,689) 10,240\nCP 3,142 (\u0006 821) 5,120\nUnconditional REMI 4,873 (\u0006 1,311) 7,680\nCP 2,053 (\u0006 580) 3,584\nTable 2: Statistics of the number (#) of words (i.e., tokens in\nREMI; compound words in CP) per song in the training set.\nDataset\nWe collect the audio ﬁles of 1,748 pieces of Pop piano from\nthe Internet. The average length of the songs is about 4 min-\nutes, and we have about 108 hours in total. All the songs are\nin 4/4 time signature (four beats per bar). We convert each\nsong (an audio) into a symbolic sequence as follows.\n\u000f Transcription: We use the state-of-the-art RNN model\nfor automatic piano transcription, “Onset and Frames”\n(Hawthorne et al. 2018a), to estimate the pitch, onset and\noffset time, and velocity of the musical notes from audio.\n\u000f Synchronization: To get symbolic timing from the origi-\nnal wall clock time, we use the RNN-based model avail-\nable in the Python package madmom (B¨ock et al. 2016)\nto estimate the downbeat and the beat positions, which\nrepresent the state-of-the-art for the task. Then, we inter-\npolate 480 ticks between two adjacent beats, and map the\nabsolute time into its according tick. By doing so, we can\nkeep tiny offset. Lastly, we infer the tempo changes from\nthe time interval between adjacent beats.\n\u000f Quantization: We quantize the tempo, velocity, duration\nand the beat positions to reduce the size of the vocabulary.\nFor example, we set the 16-th note as our basic time unit.\nSee Table 3 for the number of tokens per type.\n\u000f Analysis: For the conditional generation task, we esti-\nmate the melody notes and chord symbols from the tran-\nscription result to form the lead sheets. Speciﬁcally, we\ndevelop an in-house rule-based chord recognition algo-\nrithm4 to recognize 12 roots and 7 chord qualities. We use\nthe “Skyline algorithm” (Uitdenbogerd and Zobel 1999)\nto extract the melodies. And, as a lead sheet is usually of\ncoarser time resolution, we quantize the chord symbols\nand melody notes to the 4-th notes (i.e., beat times).\nWe randomly hold out 50 songs for testing, and use the re-\nmaining for training the Transformers.\nVocabulary\nTo represent the content of a piano performance, the basic\nsetting employs tokens of six types: three note-related types\n[pitch], [duration], [velocity], and three metric-related types\n[position/bar], [tempo], [chord]. The speciﬁc vocabulary is\ntask-dependent and is introduced below.\nConditional generation—We additionally use [track] to-\nkens to mark whether it is the lead sheet track (i.e., the con-\ndition) or the piano track (the track to be generated). While\n4https://github.com/joshuachang2311/chorder\nRepre. Token type V oc. size Embed. Sample k(\u0001)\njVkj size (dk) \u001c \u001a\nCP\n[track] 2 (+1) 3 1.0 0.90\n[tempo] 58 (+2) 128 1.2 0.90\n[position/bar] 17 (+1) 64 1.2 1.00\n[chord] 133 (+2) 256 1.0 0.99\n[pitch] 86 (+1) 512 1.0 0.90\n[duration] 17 (+1) 128 2.0 0.90\n[velocity] 24 (+1) 128 5.0 1.00\n[family] 4 32 1.0 0.90\ntotal 341 (+9) — — —\nREMI total 338 512 1.2 0.90\nTable 3: Details of the CP representation in our implementa-\ntion, including that of the sampling policy (\u001c-tempered top-\u001a\nsampling). For the vocabulary size, the values in the paren-\ntheses denote the number of special tokens such as [ignore].\nthe piano track (i.e., the sub-sequence after the [track=piano]\ntoken) involves all the six types of tokens mentioned above,\nthe lead sheet track only involves the use of composition-\nrelated tokens [position/bar], [chord], [pitch], [duration],\nnot performance-related tokens [velocity], [tempo]. In CP,\nwe have three family tokens, [family=track], [family=note],\n[family=metric]. Moreover, we have type-speciﬁc [ignore]\ntokens and an additional [conti] token for the beat positions\nhaving no tempo or chord changes.\nUnconditional generation—This task only concerns with\nthe piano track so we do not need the [track] tokens. But, as\nit concerns with full-song generation, we add an [EOS] to-\nken to signify the end of a sequence. We view it as a family\ntoken, so there are three possible family tokens here: [fam-\nily=EOS], [family=note], [family=metric].\nDetails of the adopted representations are shown in Tables\n2 and 3. Table 2 compares the sequence length T of REMI\nand CP. We can see that SCP is much shorter than SREMI,\nespecially under the conditional task. 5 Table 3 displays the\nsize of each vocabulary subsetVk. We see that CP and REMI\nhave similar total vocabulary sizeM. REMI does not use the\nfamily tokens (except for [EOS]) and special tokens.\nModel Settings\nFor the backbone architecture of our model, we employ the\nlinear Transformer (Katharopoulos et al. 2020),6 as its com-\nplexity is a linear function of the length of the attention win-\ndow N. Moreover, we set N equal to the sequence length\nT for our model. That is, no segmentation over the training\nsequences is done, and thereby all the tokens in a sequence\ncan be accessed by our model under causal masking, with-\nout using tricks such as memory caching (Dai et al. 2019)\nor memory compression (Rae et al. 2020). We refer to our\nmodel as CP+linear in what follows.\nFor the baselines, we employ the Pop Music Transformer\n5We set an upper limit of the number of elements per sequence\n(e.g., 10,240 tokens in REMI) and remove overly long songs, which\namounts to removing 25–88 songs from the training set depending\non the task and the adopted representation.\n6https://github.com/idiap/fast-transformers\n182\nTask Representation + model@loss Training GPU Inference (/song) Matchness\ntime memory time (sec) tokens (#) melody chord\nConditional\nTraining data — — — — 0.755 0.838\nTraining data (randomized) — — — — 0.049 0.239\nREMI + XL@0.44 3 days 4 GB 88.4 4,782 0.872 0.785\nREMI + XL@0.27 7 days 4 GB 91.5 4,890 0.866 0.800\nREMI + linear@0.50 3 days 17 GB 48.9 4,327 0.779 0.709\nCP + linear@0.27 0.6 days 10 GB 29.2 18,200 0.829 0.733\nUnconditional REMI + XL@0.50 3 days 4 GB 139.9 7,680 — —\nCP + linear@0.25 1.3 days 9.5 GB 19.8 9,546 — —\nTable 4: Quantitative evaluation result of different models. REMI+XL represents a re-implementation of the state-of-the-art\nPop Music Transformer (Huang and Yang 2020), while CP+linear stands for the proposed CP Transformer.\n(Huang and Yang 2020), which is open-source and stands\nfor a state-of-the-art for unconditional music composition. 7\nThis REMI+XL model adopts the REMI representation and\nuses Transformer-XL (Dai et al. 2019) as the model back-\nbone. As its complexity grows quadratically with N, we set\nN = 512, following (Huang and Yang 2020).\nMoreover, we consider one more baseline that replaces\nTransformer-XL by linear Transformer, using also N = T,\nto offer a sensible performance comparison between CP and\nREMI. We refer to this variant as REMI+linear.\nWe use 12 self-attention layers each with 8 attention heads\nfor all the models for fair comparison. The model hidden\nsize and inner layer of the feed-forward part are set to 512\nand 2,048, respectively. For the token embedding size d, we\nﬁx it to 512 for REMI, following (Huang and Yang 2020).\nFor CP, we set it adaptively based on the vocabulary size\nof each token type, as shown in Table 3. For sampling, we\nemploy the “nucleus sampling” (Holtzman et al. 2020), a\nstochastic method that samples from the smallest subset of\ntokens whose cumulative probability mass exceeds a thresh-\nold \u001a 2[0;1]. Before sampling, we reshape the probability\ndistribution of the tokens (e.g., softmax(W khout\nt )) through\n“temperature” (Ackley, Hinton, and Sejnowski 1985), with\nthe temperature parameter \u001c >0. As Table 3 also shows, we\nuse different \u001aand \u001c for different token types. For example,\nwe use a large \u001c to encourage diverse velocity values.\nThe conditional generation task can be approached with\na sequence-to-sequence model, since we have paired data of\nlead sheets and piano performances (i.e., the former is ex-\ntracted automatically from the latter). Instead of adding a\nTransformer encoder (as done in (Choi et al. 2020)) to re-\nalize this, we use the encoder-free “Preﬁx LM” method of\nthe Google’s “T5” model (Raffel et al. 2020), and run a sin-\ngle Transformer over an interleaved sequence of lead sheets\nand piano performances. Speciﬁcally, a sequence of lead\nsheet and the corresponding target sequence of piano perfor-\nmance are integrated into one sequence bar after bar. That\nis, the integrated sequence would have the form of f::: ,\n[bar], [track=leadsheet], (content of the lead sheet for a bar),\n[track=piano], (content of the piano for the same bar), [bar],\n(content of the two tracks of the next bar) ::: g. This makes\nit easy to learn the dependency of the two tracks, and to im-\npose the pre-given lead sheet at inference time.\n7https://github.com/YatingMusic/remi\nQuantitative Evaluation\nThe experiments hereafter are conducted in the interest of a\nresource-constrained scenario, assuming that we only have\na single GPU with 11 GB memory and are only willing to\ntrain a model for 3 days. We conjecture that this makes sense\nfor most middle-size academic labs worldwide. Yet, to have\nan idea of the model performance when more resources are\navailable, we include to the evaluation of the conditional task\ntwo settings exceeding such a speciﬁcation.\nWe ﬁrstly compare the efﬁciency of the models in terms\nof training time, inference time, and GPU memory usage,\nunder the conditional setting. The average result over the 50\nheld-out test songs is shown in Table 4.\nGPU memory usage. Table 4 shows that both CP+linear\nand REMI+XL require <11 GB GPU memory for training.\nAccordingly, in our implementation, we train them (sepa-\nrately) on an NVIDIA RTX 2080 Ti GPU (with 11GB mem-\nory). In contrast, REMI+ linear requires 17 GB GPU mem-\nory, so we train it on a TITAN GPU with 24 GB memory.\nTraining time. We see that REMI-based models require\nmuch longer clock time to reach a low training loss. While\nit takes nearly 7 days for REMI+XL to reduce the negative\nlog-likelihood (NLL) of the training data to 0.27, it takes\nonly 0.6 days for CP+linear to reach the same NLL. Such a\ntraining efﬁciency is desirable (especially given that it is on\na single 2080 Ti GPU), as it makes further extensions and\nmodiﬁcations of the model easy and affordable.\nInference time. CP+linear is remarkably fast, taking on\naverage <30 seconds to complete the conditional generation\nof a song. As a song in our dataset is about 4 minutes, this\nis much faster than real time. In contrast, REMI+XL and\nREMI+linear are about 3x and 1.7x slower, respectively.\nCP+linear is fast for it generates in total 8 individual tokens\n(of different types) at once each time step.\nTable 4 also compares the efﬁciency of REMI+XL and\nCP+linear under the unconditional setting, for which we\ngenerate also 50 songs (from scratch) and report the average\ninference time. We see that CP+linear is even faster here, re-\nquiring only <20 seconds to create a new song at full-song\nlength. In contrast, REMI+XL is on average 7x slower.\nNext, we compare the performance of the models in terms\nof two objective metrics, also under the conditional setting.\nAs the goal is to generate a song given a lead sheet, we can\nmeasure whether the generated song has a melody line and\n183\nRepre. + model@loss F R H C O\nREMI + XL@0.44 4.05 3.12 3.38 3.55 3.31\nREMI + XL@0.27 4.29 3.14 3.70 3.64 3.35\nREMI + linear@0.50 4.03 3.09 3.48 3.46 3.29\nCP + linear@0.27 4.09 3.13 3.50 3.31 3.08\n(a) Conditional generation\nRepre. + model@loss R H S O\nREMI + XL@0.50 3.11 3.46 2.91 3.03\nCP + linear@0.22 3.33 3.68 3.11 3.34\n(b) Unconditional generation\nTable 5: Result of subjective evaluation (Fidelity, Richness,\nHumanness, Correctness, Structureness, Overall).\nchord progression similar to that in the given condition, and\ntake that as a ﬁgure of merit. (In contrast, proper objective\nevaluation of unconditional generation models remains an\nopen issue (Yang and Lerch 2020; Dong et al. 2020; Wu and\nYang 2020).) Speciﬁcally, we consider:\n\u000f Melody matchness. We represent the lead sheet and the\ncorrespondingly generated piano both in the REMI format\nand compute the bar-wise longest common sub-sequence\n(LCS) of the two resulting sequences SLS\nREMI and bSpiano\nREMI.\nWhen two notes (each from the two sequences) have the\nsame pitch and close onset time (within the 8-th note), we\nconsider that as a match. We divide the length of the LCS\nby the number of [pitch] tokens inSLS\nREMI (i.e., the number\nof target melody notes) of that bar, and take the average\nvalue of such a ratio across all the bars of a song as a\nsimple measure of melody matchness.\n\u000f Chord matchness. Thechroma vector (Fujishima 1999)\nrepresents a short-time fragment of music by the distri-\nbution of energy across the 12 pitch classes (C, C#, etc)\nand offers a simple way to evaluate the harmonic simi-\nlarity between two fragments. We calculate the segment-\nwise cosine similarity between the chroma vector repre-\nsenting each chord label of a lead sheet (which would be\nbinary-valued) and the chroma vector of the correspond-\ningly generated piano segment (normalized by the maxi-\nmum value so it is 2[0;1]12), and treat the average value\nacross time as a measure of chord matchenss.\nTable 4 shows that the evaluated models all have match-\nness close to that of the training set, and much higher than\nthat of the random baseline (i.e., the average matchness be-\ntween a lead sheet and a random song from the test set). This\nsuggests, while CP+linear is easier and faster to train than\nREMI+XL, they may generate music of similar quality. We\nfurther investigate this through a user study, which directly\nassesses the perceptual quality of the generated music.\nQualitative Evaluation\nWe devise an online questionnaire that solicits anonymous\nresponse to the music generated by different models for both\nthe conditional and unconditional settings. For the former,\nwe present excerpts of 32 bars taking from one-third loca-\ntion of the music. For the latter, we present the full songs\n(a) REMI+XL\n(b) CP+linear\nFigure 3: Piano-rolls of middle 64 bars of random generated\npieces of two models in the unconditional setting. We see\nricher and diverse content in the result of CP+linear.\n(i.e., when an [EOS] token is generated).8 Our intention is to\ninvestigate whether CP+linear and REMI+XL indeed gen-\nerate music of similar perceptual qualities.\nThe generated music is rendered into audio with a pi-\nano synthesizer using a free, non-professional grade sound\nfont. Each batch comprises the result of the evaluated mod-\nels in random order. A subject has to rate the music for three\nrandom batches for each setting separately, in terms of the\nfollowing aspects on a ﬁve-point Likert scale. 1) Fidelity:\nis the conditionally generated piece similar to the refer-\nence, from which the condition lead sheet was taken from?\n2) Richness: diversity and interestingness. 3) Humanness:\ndoes the piece sound like expressive human performances?\n4) Correctness: perceived absence of composing or playing\nmistakes. 5)Structureness: whether there are structural pat-\nterns such as repeating themes or development of musical\nideas. 6) Overall. As the music can be long, the question-\nnaire may take around 30 mins to complete.\nTable 5 shows the average result from 18 subjects. We see\nthat REMI+XL performs the best in the conditional setting,\nyet with only moderate performance gap between the mod-\nels.9 In contrast, CP+ linear performs (slightly) better con-\nsistently across the four metrics in the unconditional setting,\nsuggesting it a powerful alternative to REMI+XL.\nConclusion\nIn this paper, we have presented a new variant of the Trans-\nformer that processes multiple consecutive tokens at once at\na time step. Each individual token is associated with a token\ntype, which is exploited by the model to customize its input\nand output modules. The proposed model achieves sequence\ncompression by integrating the embeddings of the tokens,\nwhich can be seen as forming a hyperedge over a dynamic\ngraph. We show that the new Transformer works remarkably\nwell for modeling music, creating full-song piano of compa-\nrable perceived quality with a competing Transformer-XL\nbased model in much shorter training and inference time.\n8It turns out that the REMI+XL model seldom generates [EOS]\ntokens even when the music is already quite long (e.g., 8 minutes),\nso we stop it each time when it has generated 7,680 tokens.\n9In the conditional setting, the global structure of the song to be\ngenerated is fairly outlined in the given condition (i.e., the melody).\nThus, it seems sufﬁcient for models to learn from short segments.\n184\nAcknowledgements\nWe are grateful to our interns at the Taiwan AI Labs, Joshua\nChang for developing the symbolic-domain chord recog-\nnition algorithm, and Yu-Hua Chen and Hsiao-Tzu Hung\nfor helping organize the PyTorch code. We also thank the\nanonymous reviewers for their valuable comments.\nEthics Statement\nResearch on automatic music generation may infringe copy-\nright laws and may raise concerns regarding the role of hu-\nman musicians in the future. Cares have to be given regard-\ning the fair use of existing musical material for model train-\ning, and the potential concern of “deepfaking” an existing\nartist’s style in computer-generated music.\nReferences\nAckley, D. H.; Hinton, G. E.; and Sejnowski, T. J. 1985. A\nlearning algorithm for Boltzmann machines. Cognitive Sci-\nence 9(1): 147–169.\nBaevski, A.; and Auli, M. 2018. Adaptive input repre-\nsentations for neural language modeling. arXiv preprint\narXiv:1809.10853 .\nB¨ock, S.; Korzeniowski, F.; Schl¨uter, J.; Krebs, F.; and Wid-\nmer, G. 2016. Madmom: A new Python audio and mu-\nsic signal processing library. In Proc. ACM Multimedia,\n1174–1178.\nChen, Y .-H.; Huang, Y .-S.; Hsiao, W.-Y .; and Yang, Y .-H.\n2020. Automatic composition of guitar tabs by Transformers\nand groove modeling. In Proc. Int. Soc. Music Information\nRetrieval Conf.\nChild, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019.\nGenerating long sequences with sparse Transformers. arXiv\npreprint arXiv:1904.10509 .\nChoi, K.; Hawthorne, C.; Simon, I.; Dinculescu, M.; and En-\ngel, J. 2020. Encoding musical style with transformer au-\ntoencoders. In Proc. Int. Conf. Machine Learning.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q.; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive lan-\nguage models beyond a ﬁxed-Length context. In Proc. An-\nnual Meeting of the Association for Computational Linguis-\ntics, 2978–2988.\nDonahue, C.; Mao, H. H.; Li, Y . E.; Cottrell, G. W.; and\nMcAuley, J. 2019. LakhNES: Improving multi-instrumental\nmusic generation with cross-domain pre-training. In Proc.\nInt. Soc. Music Information Retrieval Conf., 685–692.\nDong, H.-W.; Chen, K.; McAuley, J.; and Berg-Kirkpatrick,\nT. 2020. MusPy: A toolkit for symbolic music generation.\nIn Proc. Int. Soc. Music Information Retrieval Conf.\nEns, J.; and Pasquier, P. 2020. MMM- Exploring conditional\nmulti-track music generation with the Transformer. arXiv\npreprint arXiv:2008.06048 .\nFeng, Y .; You, H.; Zhang, Z.; Ji, R.; and Gao, Y . 2019. Hy-\npergraph neural networks. In Proc. AAAI, 3558–3565.\nFujishima, T. 1999. Realtime chord recognition of musical\nsound: A system using common Lisp. InProc. International\nComputer Music Conf., 464–467.\nHawthorne, C.; Elsen, E.; Song, J.; Roberts, A.; Simon, I.;\nRaffel, C.; Engel, J.; Oore, S.; and Eck, D. 2018a. Onsets\nand Frames: Dual-objective piano transcription. InProc. Int.\nSoc. Music Information Retrieval Conf., 50–57.\nHawthorne, C.; Huang, A.; Ippolito, D.; and Eck, D. 2018b.\nTransformer-NADE for piano performances. In Proc. Ma-\nchine Learning for Creativity and Design Workshop.\nHoltzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y .\n2020. The curious case of neural text degeneration. In Proc.\nInt. Conf. Learning Representations.\nHuang, C.-Z. A.; Vaswani, A.; Uszkoreit, J.; Simon, I.;\nHawthorne, C.; Shazeer, N.; Dai, A. M.; Hoffman, M. D.;\nDinculescu, M.; and Eck, D. 2019. Music Transformer: Gen-\nerating music with long-term structure. In Proc. Int. Conf.\nLearning Representations.\nHuang, Y .-S.; and Yang, Y .-H. 2020. Pop Music Trans-\nformer: Beat-based modeling and generation of expressive\nPop piano compositions. In Proc. ACM Multimedia.\nJhamtani, H.; and Berg-Kirkpatrick, T. 2019. Modeling Self-\nRepetition in Music Generation using Generative Adversar-\nial Networks. In Proc. Machine Learning for Music Discov-\nery Workshop.\nJiang, J.; Wei, Y .; Feng, Y .; Cao, J.; and Gao, Y . 2019. Dy-\nnamic hypergraph neural networks. In Proc. IJCAI, 2635–\n2641.\nJiang, J.; Xia, G. G.; Carlton, D. B.; Anderson, C. N.; and\nMiyakawa, R. H. 2020. Transformer V AE: A hierarchical\nmodel for structure-aware and interpretable music represen-\ntation learning. In Proc. Int. Conf. Acoustics, Speech and\nSignal Processing, 516–520.\nKatharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret, F.\n2020. Transformers are RNNs: Fast autoregressive Trans-\nformers with linear attention. In Proc. Int. Conf. Machine\nLearning.\nKazemi, S. M.; Goel, R.; Jain, K.; Kobyzev, I.; Sethi, A.;\nForsyth, P.; and Poupart, P. 2020. Representation learning\nfor dynamic graphs: A survey.Journal of Machine Learning\nResearch 21(70): 1–73.\nKe, G.; He, D.; and Liu, T.-Y . 2020. Rethinking posi-\ntional encoding in language pre-training. arXiv preprint\narXiv:2006.15595 .\nKeskar, N. S.; McCann, B.; Varshney, L. R.; Xiong, C.; and\nSocher, R. 2019. CTRL: A conditional Transformer lan-\nguage model for controllable generation. arXiv preprint\narXiv:1909.05858 .\nKivel¨a, M.; Arenas, A.; Barthelemy, M.; Gleeson, J. P.;\nMoreno, Y .; and Porter, M. A. 2014. Multilayer networks.\nJournal of Complex Networks 2(3): 203–271.\nLerch, A.; Arthur, C.; Pati, A.; and Gururani, S. 2019. Music\nperformance analysis: A survey. In Proc. Int. Soc. Music\nInformation Retrieval Conf.\n185\nOore, S.; Simon, I.; Dieleman, S.; Eck, D.; and Simonyan,\nK. 2018. This time with feeling: Learning expressive musi-\ncal performance. Neural Computing and Applications .\nPatel, A. D. 2003. Language, music, syntax and the brain.\nNature Neuroscience 6: 674–681.\nPayne, C. M. 2019. MuseNet Accessed: 2021-03-01.\nRae, J. W.; Potapenko, A.; Jayakumar, S. M.; Hillier, C.; and\nLillicrap, T. P. 2020. Compressive Transformers for long-\nrange sequence modelling. InProc. Int. Conf. Learning Rep-\nresentations.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Ex-\nploring the limits of transfer learning with a uniﬁed text-\nto-text Transformer. Journal of Machine Learning Research\n21(140): 1–67.\nRossi, E.; Chamberlain, B.; Frasca, F.; Eynard, D.; Monti,\nF.; and Bronstein, M. 2020. Temporal graph networks\nfor deep learning on dynamic graphs. arXiv preprint\narXiv:2006.10637 .\nUitdenbogerd, A.; and Zobel, J. 1999. Melodic matching\ntechniques for large music databases. In Proc. ACM Multi-\nmedia, 57–66.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Proc. Advances in Neural Infor-\nmation Processing Systems, 5998–6008.\nWu, S.-L.; and Yang, Y .-H. 2020. The Jazz Transformer on\nthe front line: Exploring the shortcomings of AI-composed\nmusic through quantitative measures. In Proc. Int. Soc. Mu-\nsic Information Retrieval Conf.\nYang, L.-C.; and Lerch, A. 2020. On the evaluation of gener-\native models in music. Neural Computing and Applications\n32: 4773–4784.\n186",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7564668655395508
    },
    {
      "name": "Security token",
      "score": 0.7257015705108643
    },
    {
      "name": "Transformer",
      "score": 0.619537353515625
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5161955952644348
    },
    {
      "name": "Speech recognition",
      "score": 0.49396592378616333
    },
    {
      "name": "Vocabulary",
      "score": 0.4869552552700043
    },
    {
      "name": "Word (group theory)",
      "score": 0.4164811968803406
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3436931371688843
    },
    {
      "name": "Mathematics",
      "score": 0.14914283156394958
    },
    {
      "name": "Linguistics",
      "score": 0.10643050074577332
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I84653119",
      "name": "Academia Sinica",
      "country": "TW"
    }
  ],
  "cited_by": 134
}