{
  "title": "Computer-aided diagnosis of Alzheimer’s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)",
  "url": "https://openalex.org/W4400261938",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4382158417",
      "name": "S Muhammad Ahmed Hassan Shah",
      "affiliations": [
        "COMSATS University Islamabad"
      ]
    },
    {
      "id": "https://openalex.org/A2167581049",
      "name": "Muhammad Qasim Khan",
      "affiliations": [
        "COMSATS University Islamabad"
      ]
    },
    {
      "id": "https://openalex.org/A3161544216",
      "name": "Atif Rizwan",
      "affiliations": [
        "Jeju National University"
      ]
    },
    {
      "id": "https://openalex.org/A2148020718",
      "name": "Sana Ullah Jan",
      "affiliations": [
        "Edinburgh Napier University"
      ]
    },
    {
      "id": "https://openalex.org/A4221689051",
      "name": "Nagwan Abdel Samee",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A4280965834",
      "name": "Mona M. Jamjoom",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A4382158417",
      "name": "S Muhammad Ahmed Hassan Shah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2167581049",
      "name": "Muhammad Qasim Khan",
      "affiliations": [
        "COMSATS University Islamabad"
      ]
    },
    {
      "id": "https://openalex.org/A3161544216",
      "name": "Atif Rizwan",
      "affiliations": [
        "Jeju National University",
        "Government of the Republic of Korea"
      ]
    },
    {
      "id": "https://openalex.org/A2148020718",
      "name": "Sana Ullah Jan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221689051",
      "name": "Nagwan Abdel Samee",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A4280965834",
      "name": "Mona M. Jamjoom",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3163060050",
    "https://openalex.org/W4210510666",
    "https://openalex.org/W4387744212",
    "https://openalex.org/W3160822735",
    "https://openalex.org/W1910549955",
    "https://openalex.org/W4211157318",
    "https://openalex.org/W4243693606",
    "https://openalex.org/W2752287204",
    "https://openalex.org/W3023013291",
    "https://openalex.org/W4393116337",
    "https://openalex.org/W2593468621",
    "https://openalex.org/W2972937106",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2137531761",
    "https://openalex.org/W2022585279",
    "https://openalex.org/W2120111102",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2051218719",
    "https://openalex.org/W2966069105",
    "https://openalex.org/W2990581109",
    "https://openalex.org/W1570913002",
    "https://openalex.org/W2010146415",
    "https://openalex.org/W2039127997",
    "https://openalex.org/W2137554996",
    "https://openalex.org/W3206738455",
    "https://openalex.org/W1805220220",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6604344240",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2083145085",
    "https://openalex.org/W2964629181",
    "https://openalex.org/W4237648461",
    "https://openalex.org/W4213070711",
    "https://openalex.org/W3184758801",
    "https://openalex.org/W2092436889",
    "https://openalex.org/W3174350209",
    "https://openalex.org/W2948978827",
    "https://openalex.org/W4211036635",
    "https://openalex.org/W4200453770",
    "https://openalex.org/W6890163263",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W4312927341",
    "https://openalex.org/W2328613904",
    "https://openalex.org/W2565516711",
    "https://openalex.org/W2145672185",
    "https://openalex.org/W2115017507",
    "https://openalex.org/W3212684309",
    "https://openalex.org/W2165758561",
    "https://openalex.org/W2891379852",
    "https://openalex.org/W4323315257",
    "https://openalex.org/W2793714280",
    "https://openalex.org/W1972982304",
    "https://openalex.org/W3138905996",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2514763871",
    "https://openalex.org/W3015650687",
    "https://openalex.org/W4292432194",
    "https://openalex.org/W4320713070",
    "https://openalex.org/W2587272693",
    "https://openalex.org/W4313532043",
    "https://openalex.org/W2061002752",
    "https://openalex.org/W4391180492",
    "https://openalex.org/W2081863860",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W2971066862",
    "https://openalex.org/W3126042607",
    "https://openalex.org/W2159052453",
    "https://openalex.org/W4390005899",
    "https://openalex.org/W2128155768",
    "https://openalex.org/W2151989011",
    "https://openalex.org/W3127597934",
    "https://openalex.org/W2156770396",
    "https://openalex.org/W3095986980",
    "https://openalex.org/W3090326096",
    "https://openalex.org/W4312817056"
  ],
  "abstract": "Abstract Cognitive disorders affect various cognitive functions that can have a substantial impact on individual’s daily life. Alzheimer’s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer’s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed. Graphical abstract",
  "full_text": "Vol.:(0123456789)\nPattern Analysis and Applications (2024) 27:76 \nhttps://doi.org/10.1007/s10044-024-01297-6\nTHEORETICAL ADVANCES\nComputer‑aided diagnosis of Alzheimer’s disease and neurocognitive \ndisorders with multimodal Bi‑Vision Transformer (BiViT)\nS. Muhammad Ahmed Hassan Shah1,2 · Muhammad Qasim Khan2 · Atif Rizwan3 · Sana Ullah Jan4  · \nNagwan Abdel Samee5 · Mona M. Jamjoom6\nReceived: 2 August 2023 / Accepted: 14 June 2024 / Published online: 1 July 2024 \n© The Author(s) 2024\nAbstract\nCognitive disorders affect various cognitive functions that can have a substantial impact on individual’s daily life. Alzheimer’s \ndisease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using arti-\nficial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in \nmedical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers \nin imaging has emerged as a promising area of research. A reason can be transformer’s impressive capabilities of tackling \nspatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to \ngenerate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range depend-\nencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and \nmultiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed \nof two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective \nfeature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. \nThe first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed \nof samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For compre-\nhensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. \nThe results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when \napplied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of \ndata and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm \ncan perform better if the imbalanced distribution and limited availability problems in data can be addressed.\n * Sana Ullah Jan \n s.jan@napier.ac.uk\n1 Medical Imaging and Diagnostics Laboratory (MIDL), \nNational Center of Artificial Intelligence (NCAI), \nCOMSATS University Islamabad, Islamabad 44000, \nPakistan\n2 Department of Computer Science, COMSATS University \nIslamabad, Attock Campus, Islamabad 43600, Pakistan\n3 Department of Computer Engineering, \nJeju National University, Jejusi 63243, \nJeju Special Self-Governing Province, Republic of Korea\n4 School of Computing Engineering and the Built \nEnvironment, Edinburgh Napier University, \nEdinburgh EH10 5DT, UK\n5 Department of Information Technology, College \nof Computer and Information Sciences, Princess Nourah bint \nAbdulrahman University, P.O. Box 84428, 11671 Riyadh, \nSaudi Arabia\n6 Department of Computer Sciences, College of Computer \nand Information Sciences, Princess Nourah bint \nAbdulrahman University, 11671 Riyadh, Saudi Arabia\n Pattern Analysis and Applications (2024) 27:76\n76 Page 2 of 35\nGraphical abstract\nTokenization Patch Encoder\nData Augmentation & \nPreprocessing\nMRI Data\nSelf-attention\nNon Linear \nTransformation\nSelf-attention\nNon Linear \nTransformation\nRepresentation's Fusion\nDNN Model \nPredicted Disease\nKeywords Vision transformers · Deep learning · Computer vision · Medical image processing · Alzheimer disease · \nCognitive disorders\n1 Introduction\nCognitive disorders have a significant impact on an indi-\nvidual’s daily life, as they affect various cognitive functions, \nand Alzheimer’s disease (AD) is one of the most commonly \nknown cognitive disorders. AD and other cognitive disorders \ncan be diagnosed and treated commonly through medical \nimaging. Magnetic Resonance Imaging (MRI), Positron \nEmission Tomography (PET), and Computed Tomography \n(CT) allow medical professionals to identify neurological \nchanges linked to these disorders by offering comprehen-\nsive visual depictions of brain structures and functions. In \nthis section, cognitive disorders are discussed in general \nfollowed by analysis of deep learning methods for diagnos-\ning them. Next, AD and different AI approaches used to \ndiagnose it are presented. Then, the proposed methodol-\nogy is summarized followed by the motivation behind the \nresearch, and finally, the significant contributions made in \nthis study are highlighted.\n1.1  Cognitive disorders\nCognitive impairments [32] refer to difficulties or limitations \nin cognitive function, which can include memory, attention, \nperception, language, or problem-solving abilities. These \nimpairments can affect a persons daily life and activities, and \ncan range from mild to severe. Common causes of cognitive \nimpairments include brain injury, stroke, neurodegenerative \ndisorders such as AD, and certain medical conditions such \nas HIV/AIDS or hypothyroidism [73]. There are various \nPattern Analysis and Applications (2024) 27:76 \n Page 3 of 35 76\ntypes of cognitive impairments, including Mild Cognitive \nImpairment (MCI) [34], dementia, and AD [13]. MCI is a \ncondition where a person experiences mild cognitive decline \nbeyond that of what would normally be expected for their \nage. Dementia is a more severe form of cognitive decline \nthat affects multiple cognitive domains and interferes with \na person’s ability to carry out daily activities [70].\nComputer-Aided Diagnosis (CAD) techniques have been \ndeveloped to help detect and diagnose cognitive impair -\nments. These techniques involve the use of computer algo-\nrithms to analyze various types of data such as brain scans, \nmedical records, and cognitive tests. For example, MRI \ncan be used to identify structural changes in the brain that \nare indicative of cognitive impairment [22]. Deep learning \nalgorithms can be trained on these images to help identify \npatterns and predict the likelihood of cognitive impairment \n[21, 31]. Other CAD techniques include cognitive screening \ntests such as the Montreal Cognitive Assessment (MoCA) \n[44, 45, 66] or the Mini-Mental State Examination (MMSE) \n[20, 55]. These tests are designed to assess various cogni-\ntive domains and can be administered in a clinical setting or \nremotely using computer-based assessments [94]. Ongoing \nresearch is being conducted on cognitive disorders such as \nAD, Parkinson’s disease, schizophrenia, and depression to \ncomprehend their underlying mechanisms and find effec -\ntive treatments. Studies on AD are focused on detecting the \ndisease early through biomarkers and developing potential \ntherapies to slow or halt its progression [17, 72, 78, 88].\n1.2  Alzheimer disease\nOne of the most prevalent cognitive disorders is AD, which \nis also the most common cause of dementia. In 1906, Dr. \nAlois Alzheimer was the first to discover AD [37, 63]. It is \ntypified by a progressive loss of memory and other cognitive \nabilities [70]. With 70% of dementia cases, AD is the most \nprevalent type of dementia worldwide. AD is a cognitive \ndisorder affecting cognitive function and memory, and is a \nleading cause of dementia in elderly individuals [48]. Over \ntime, there is an irreversible decline in cognitive function \nassociated with this progressive neurological disorder. The \nfollowing are some of the symptoms and attributes of AD: \nmemory loss, language difficulties, disorientation (forget \nwhere they are or how they got there, and have difficulty rec-\nognizing people they know), poor judgment, mood swings, \nloss of initiative, and changes in personality [7 , 9–12, 16, \n53, 75].\nProper classification of AD plays a crucial role in compre-\nhending the disease, as it enables early diagnosis and predic-\ntion of patient outcomes, and facilitates informed decision-\nmaking regarding treatments. Deep learning has emerged \nas a promising approach for AD classification, particularly \nwith regards to brain imaging data such as MRI or positron \nemission tomography (PET) scans [29, 57]. To diagnose the \ndisease, a medical evaluation including patient history, men-\ntal state examination, physical and neurobiological tests, as \nwell as non-invasive brain imaging techniques such as struc-\ntural and functional magnetic resonance imaging are used \n[8, 92]. The process of diagnosing AD typically involves \ngathering a patient’s medical history, assessing their clinical \nsymptoms, and observing their behavior [33, 62, 77].\nMRI scans, in particular, provide important information \nabout AD through the use of deep learning and machine \nlearning techniques. Using features taken from MRI images, \nmachine learning algorithms are one such technique that \nuses to distinguish between people who are healthy and \nthose who have AD [64, 93]. These characteristics include \na range of parameters, including surface area, cortical \nthickness, and brain volume, and they serve as important \nmarkers of AD pathology [60]. Furthermore, deep learn-\ning algorithms provide an advanced method for interpreting \nbrain networks seen in MRI scans [81]. These algorithms \nidentify changes linked to AD by examining the patterns of \nconnectivity between various brain regions [97]. Notably, \nresearch has shown that reduced connectivity between dif-\nferent parts of the brain is a hallmark of AD [59, 67]. Fur-\nthermore, AD detection through MRI analysis has shown \nnotable success with computer vision techniques. The ability \nto identify structural anomalies indicative of AD is made \npossible by MRI’s high-resolution imaging capabilities [3]. \nThese developments highlight how important medical imag-\ning is to improving our knowledge and ability to diagnose \nAD, especially MRI.\n1.3  Introduction to proposed approach\nHere, a discussion about Convolutional Neural Network \n(CNN), Vision Transfomer (ViT), and Compact Convolu-\ntional Transformer (CCT) models is presented. The meth-\nods employed, namely patch encodings and tokenization, \nwill then be discussed. It is followed by an introduction \nand generic discussion about the suggested methodology. \nIn Sect. 3, the complete working methodology of proposed \narchitecture is presented in detail.\n1.3.1  CNN vs ViT vs CCT \nThe ViT is a machine learning model for image classification \nthat utilizes a transformer-based architecture on patches of \nthe image. It was first introduced in a research paper titled \n\"An Image is Worth 16 × 16 Words\" presented at the ICLR \n2021 conference by Neil Houlsby and colleagues [24]. The \nmodel is pre-trained on large image datasets such as Ima-\ngeNet-21k and ImageNet [56], and employs a mechanism of \nattention seeking, which allows it to assign varying levels \nof importance to different parts of the input data. The ViT \n Pattern Analysis and Applications (2024) 27:76\n76 Page 4 of 35\nmodel is composed of multiple self-attention layers, similar \nto those used in natural language processing, which hold \ngreat potential for use in various data modalities. Figure  1a \nillustrate the vision transformer.\nThe performance of ViT is superior to CNNs while using \nfewer resources. However, due to its weaker inductive bias, it \nneeds more data augmentation or regularization while train-\ning on smaller datasets. ViT represents image inputs as a \nsequence of image patches and requires a significant amount \nof data to achieve optimal performance. The mathematical \nformulation of kernel convolution in CNN can be described \nby Eq. 1.\nwhere the f represent the input image and h denotes the ker-\nnel. The indices of columns and rows of the result matrix \nare marked with n and m, respectively. Unlike CNNs, which \nuse pixel arrays, ViT splits images into visual patches during \ncomputation and employs a self-attention layer that embeds \ninformation globally in the overall image. The mathemati-\ncal representation of self-attention is given in the Eq. 2. ViT \ncan also learn to encode the relative location of the image, \nthereby reconstructing the image structure. Furthermore, \nViT has a multi-head layer that concatenates all outputs in \nthe appropriate dimensions, and most attention heads are \nused to train global and local dependencies in an image. The \naspects which mainly differentiate ViT from CNN include \npatching and self-attention.\n(1)G [m, n]=( f ∗ g)[m, n]=\n/uni2211.s1\nj\n/uni2211.s1\nk\nh[j, k]f[m − j, n − k]\nOn the other hand, CCT is a novel deep learning architecture \nthat combines the strengths of both CNNs and transformers \n[38]. It aims to capture both local and long-range depend-\nencies in input data efficiently. For instance, CNNs can be \nused to extract local features from input image followed by \nself-attention layers from transformers to model long-range \ndependencies between extracted features. Equations 3 and 4 \npresent the mathematical formulation of the tokenization and \nencoding processes in CCTs, with the transformer encoder \nrepresented as f.\nThe visual diagram of CCT is shown in Fig.  1c. The \narchitecture of a CCT consists of two main components: \na convolutional encoder and a transformer decoder. The \nconvolutional encoder is responsible for extracting spatial \nfeatures from the input image or video, while the transformer \ndecoder processes the encoded features and generates the \noutput. Self-attention layers have also been used in com-\nputer vision, but they are computationally expensive and \nrequire a large number of parameters, making them chal-\nlenging to deploy on resource-constrained devices. CCTs \naddress these limitations by using self-attention layers in a \ncompact manner. Instead of applying self-attention to the \nentire input feature map, CCTs apply it to a smaller set of \n(2)Attention(Q, K , V )=Softmax\n�\nQK T\n√\nD h\n�\n.V\n(3)xo = MaxPool(ReLU (Conv2d(x)))\n(4)xL = f(xo)∈R b×n×d\nFig. 1  ViT vs CVT vs CVT Tran sforme rw ith Clas sT okeniza/g415onPatc hB ased Tokeniza/g415o n\nEmbe dt o\nPatche s\nLinear\nProjec/g415on Re sh ap e\nCl as s\nTo ke n\nPosi/g415o na l\nEm be dd in g\nTran sformer \nEn co de r Slic e Linear Layer\nCl as s\nTo ke n\nOutp utInpu t\n(a) VisionT ransformer\nTran sforme rw ith Sequence PoolingPatc hB ased Tokeniza/g415o n\nEmbe dt o\nPatche s\nLinear\nProjec/g415on Reshap e\nPosi/g415o na l\nEm be dd in g\nTran sformer \nEn co de r\nSequence\nPooling Linear Layer Outp utInpu t\n(b) Compact VisionT ransformer (CVT)\nTran sforme rw ith Sequence PoolingConvolu/g415onal Tokeniza/g415o n\nConv Layer Pooling Re sh ap e\nOp/g415ona l\nPosi/g415o na l\nEm be dd in g\nTran sformer \nEn co de r\nSequence\nPoolin g Linear Layer Outp utInpu t\n(c) Compact Convolutional Transformer( CCT)\nPattern Analysis and Applications (2024) 27:76 \n Page 5 of 35 76\nfeatures, reducing the computational cost. This is achieved \nby adding self-attention layers after every few convolutional \nlayers. The self-attention layers enable the model to learn \nlong-range dependencies between features, while the con-\nvolutional layers capture local spatial relationships.\nIn this work, a novel transformer-based model is devel-\noped that combines the beneficial features of CCT and ViT. \nThe PCES is a novel method that combines the tokenization \nprocess from CCT with the patch encoding mechanism from \nViT. Here, images are tokenized and patched simultaneously \nin different encoding modules, and their outputs are fed into \ntransformer and self-attention layers. The two representa-\ntions learned from transformer layers are combined in a \nsingle process known as MLF, which combines two differ -\nent kinds of information. For classification, a multiple layer \nperceptron is employed. Section  3 offers a more detailed \ndiscussion of proposed methodology.\nThe aim of utilizing a new novel transformer for the clas-\nsification of AD and cognitive disorders is to enhance the \nprecision of diagnosis and promote the understanding of \ncognitive disorders. Medical imaging data presents chal-\nlenges due to its complex spatial relationships and long-\nrange dependencies [84]. Transformer applications in imag-\ning have become a hot topic for research in recent years. By \nutilising their self-attention mechanism, transformers try to \ncope with these difficulties by producing detailed features \nand capturing intricate patterns by combining long-range \ndependencies and global context. Traditional diagnostic \nmethods are not always reliable, and by employing sophis-\nticated deep learning techniques like transformers, it is pos-\nsible to detect patterns and characteristics in medical imag-\ning data that may not be visible to the naked eye. This can \npotentially result in earlier and more accurate diagnoses, \nallowing for more efficient treatment and care of patients. In \naddition, as the world’s population continues to age, there is \na growing need for the development of more effective tools \nfor diagnosing AD and cognitive disorders.\n1.4  Contribution\nThis study involves the development of a deep transformer \narchitecture for the classification of different stages or types \nof AD and other cognitive disorders in 2D MRI images data. \nThe main contributions of this paper are listed as follows:\n• A novel computer-aided diagnosis system is suggested \nfor AD and cognitive impairments which can be used by \nmedical professionals for decision making followed by \nquick and efficient treatment.\n• A deep learning-based model called BiViT has been \nintroduced to detect AD and cognitive disorders in 2D \nMRI imaging data. This system makes use of (PCES) \nand MLF, resulting in a significant improvement in the \naccuracy of the results.\n• The study propose a new PCES technique that involves \ntwo types of encoding to process data leading to \nimproved encoding that further enhances model’s per -\nformance in terms of achieving accurate results.\nThe structure of this paper is as follows: Sect.  2 provides \nan overview of the relevant literature on the subject at \nhand. The methodologies employed in the current study are \ndescribed in Sect.  3. Furthermore, Sects.  4 and 5  present \nthe results and discussion, and Sect. 6 concludes the paper.\n2  Literature review\nThis section presents a literature review on cognitive disor-\nders, primarily related to AD, with a focus on the essential \nrole that deep learning plays in the recognition and clas -\nsification of cognitive disorders from imaging data. AD is \na severe neurological condition that leads to progressive \ndamage to brain cells, causing permanent memory loss and \ndementia [ 51]. Early detection of AD can help control its \nspread, and hence there is a need for an autonomous system \nthat can classify medical condition into different stages. In \nrecent times, machine learning and deep learning techniques \nhave been successfully applied to many medical problems, \nincluding AD detection [30]. Deep learning has been used \nin many studies to classify cognitive disorders mainly AD, \nusing imaging data such as MRI or PET scans, and clinical \ndata [15]. Some studies have found that deep learning mod-\nels can achieve high accuracy in classifying AD, particu-\nlarly when using imaging data. In this regard, CNN, ViT and \nautoencoders have been used along with other deep learn-\ning architectures to determine the essential features of these \nMRI scans and categorizing them into healthy or disease \ngroups. In this section, the different methods used for AD \ndetection in relation to their pros and cons are analyzed.\nTransfer learning is an important aspect when the train-\ning data is very low. Ghazal and Issa [36] aims to detect AD \nusing brain MRI to classify images into four stages using \ntransfer learning including healthy, mild demented, mod -\nerately demented, and severe demented. The work utilizes \na transfer learning-based AlexNet model for characterizing \nthe disease at an early stage with high accuracy. The pro-\nposed system’s simulation results have demonstrated that \nit can achieve an accuracy of 91.70%, making it an effec-\ntive tool for early detection of AD. Merits of the transfer \nlearning-based model include fast training of the model, \nre-usability and reduced data requirement. However, fine-\ntuning of AlexNet trained on ImageNet dataset [23] on medi-\ncal images dataset can be questionable. Moreover, there is \na chance of overfitting with the new data, especially, if new \n Pattern Analysis and Applications (2024) 27:76\n76 Page 6 of 35\ndataset is significantly different from training data. To sum \nup, the problem of domain discrepancy exists in this research \nand it can be considered as the main drawback of this study. \nIn another study, transfer learning-based ResNet50 is used \nto achieve AD detection and determine its stage by applying \nbrain images [99]. It means that, the approach is developed \nusing hybrid Resnet50 with other CNN architectures like \nAlexnet, Densenet201, and Vgg16 [52, 91, 98]. The study \ndemonstrated that the proposed hybrid model had an accu-\nracy of 90%, which outperformed the individual CNN archi-\ntectures. Hence, the hybrid model showed promising results \nin diagnosing AD and showed better performance than other \nCNN architectures reported in the literature. However, it \nhas certain drawbacks such as small receptive fields, a lack \nof long-term dependencies, and a lack of attention mecha-\nnisms. As the proposed transfer learning-based CNN model \nis almost similar to [36] research, it has similar limitations \nranging from domain mismatch to overfitting. The second \nmain problem that comes with the use of CNN based models \nis that they do not use the attention mechanism unlike ViT. \nAttention-mechanism is one of the key components in image \nrecognition which is used to locate regions of the image \nthat are of importance. Some other shortcomings associated \nwith CNN-based models are limited contextual knowledge, \nfixed-size input, lack of global attention and large number \nof parameters.\nMild Cognitive Impairment (MCI) is an intermediate con-\ndition between healthy individuals and AD. Taheri Gorji \nand Kaabouch [87] conducted a study on the significance of \nearly detection of MCI using MRI. The study employed a \nCNN to classify MRIs of 600 individuals into healthy, Early \nMCI (EMCI), or Late MCI (LMCI) classes. The CNN, with \nan efficient architecture, discriminated between the healthy \ngroup and the two types of MCI groups, achieving an overall \nclassification accuracy of 94.54%. The advantages include \nimproved accuracy in MCI classification and the potential \nfor early intervention, however, the model’s performance \nmay vary depending on the dataset used and may require \nfurther validation.\nEnsemble learning is a machine learning technique \nwhere multiple models’ predictions are combined to boost \nthe overall performance. Unlike conventional models, it uti-\nlizes the heterogeneity among stand-alone models in order \nto achieve lower error and enhanced stability. One substan-\ntial advantage is its capability to accomplish better outcome \nof prediction by combining the powers of different models. \nEnsembling involves using several different simple models \nseparately and then joining them together to obtain fitter and \nmore generalizing results, especially if individual models are \ndissimilar. This approach also enhances the model’s ability \nto handle noisy data and outliers. Kang et al. [47] presents a \nCNN-based ensemble learning approach for AD classifica-\ntion from the MRI data. They implemented the use of GAN’s \nDiscriminator, VGG19 and ResNet50 ensemble models, and \nmajority voting is used to fuse the outcomes. The proposed \nmodel was able to achieve an excellent performance with an \naccuracy of 92%. The ensemble learning-based networks are \nusually more robust and have better generalization capabili-\nties as compared to stand-alone models. However, ensemble \nlearning is computationally expensive because of the need to \ntrain and handle multiple models. Moreover, if base models \nare not well-trained or they look the same, ensemble learn-\ning will not give much improvement to a single model and \nit will lead to overfitting. Furthermore, ensemble learning-\nbased models are prone to overfitting if the base learners are \ntoo complex or if the ensemble has too many components. \nTo summarize, this approach can achieve better results but \nit has issues including limited knowledge about context, lack \nof global attention, domain mismatch and large number of \nparameters, that would hinder the practical utilization of this \nmodel.\nThere are currently no biomarkers known to be extremely \naccurate in diagnosing AD in its early phases, making it a \ndifficult task in medical practise to identify AD in its early \nstages. Moreover, AD is an incurable disease, and high fail-\nure rate was observed in clinical trials for AD treatments. \nTo help slow down the progression of AD, researchers are \nstriving to find ways for early detection. With a focus on \nneuroimaging and mostly academic articles released since \n2016, [65] review the most recent state-of-the-art research \non machine learning approaches used for the detection and \nclassification of AD in this study. Various machine learning \ntechniques, including Support Vector Machine (SVM), Ran-\ndom forest, CNN, K-means, and others, have been employed \nfor the detection and classification of AD. The review indi-\ncates that there is no single best approach, but deep learning \ntechniques, such as CNNs, appear to be promising for the \ndiagnosis of AD. A similar research by [6] shows that among \nK-Nearest Neighbor (KNN), SVM, Decision Tree (DT), Lin-\near Discrimination Analysis (LDA), Random Forest (RF) \nand CNN algorithms, the CNN performs the best for classi-\nfying AD using imaging data and some extra data from MRI \nsuch as the average cortical thickness, the standard deviation \nof cortical thickness, the volume of cortical parceling, white \nmatter, and surface area. However, it has a limitation of lack \nof global attention and contextual understanding.\nIn contrast to supervised learning whose models are \ntrained by using labeled data, unsupervised learning is \nessential because it can discover structures and patterns that \nare hidden in unlabeled data. One of the most widely used \napproaches of unsupervised learning for image data is the \nautoencoder. Encoder and decoder are the main components \nof autoencoders where encoded representation of inputs in a \nlow-dimensional space occur with the help of the former and \nthe latter reconstruct original images from this representa-\ntion. Leveraging the benefit of autoencoders, [96] employed \nPattern Analysis and Applications (2024) 27:76 \n Page 7 of 35 76\na Stacked AutoEncoder (SAE) to extract features from MRI \ndata, and a SVM was finally used to classify AD using those \nfeatures. They found that the deep autoencoder was able \nto extract useful features that improved the accuracy of the \nSVM, resulting in an accuracy of 89%. SAEs offer several \nadvantages, such as hierarchical representation learning and \nthe ability to model non-linear transformations. However, \nthey also come with disadvantages, including the potential \nfor overfitting and the computational complexity of train-\ning deep models. SAEs also pose challenges in interpreta-\ntion due to the complexity of their learned representations, \nmaking it hard to interpret model decisions. Additionally, \nthey are data-hungry, needing substantial data for training \nmeaningful representations, which can limit their effective-\nness with small or unrepresentative datasets.\nRecently, researchers have explored innovative \napproaches to improve the accuracy and efficiency of AD \ndiagnosis using advanced technologies such as deep learn-\ning and neuroimaging. One such approach, proposed by [74] \nfrom Imperial College London, focuses on utilizing imag-\ning data to differentiate AD from MCI and Normal Control \n(NC). Their method leverages an autoencoder and a 3D CNN \narchitecture, achieving an impressive accuracy of 95.39% in \ndistinguishing AD from NC individuals. Additionally, a 2D \nCNN design is developed which yields comparable accu-\nracy results. Likewise, [61] designed a diagnostic approach \nfor AD using multi-modal neuroimaging data. This method \nutilizes a novel zero-masking technique which preserves all \nthe information contained within the data. SAE is used for \nextracting high-level features and subsequently feedint them \ninto SVM for the purpose of multi-modal and multi-class \nMR/PET data classification. The study revealed a perfor -\nmance of 86.86% accuracy by the model, thereby presenting \na possibility for the employed method in early detection of \nAD. Such cutting-edge advertising techniques reinforce the \nfact that the increased use of advanced technology is very \nsignificant to the progress in the diagnosis and treatment of \nAD. Unsupervised autoencoder approach is useful for non-\nlinear features learning, but one has to face issues like model \ngenerability and interpretability, as well as the model overfit-\nting. The SAEs can be complex and thus hard to interpret, \nand this complexity may hamper their adoption in clinics \nwhere interpretability is very important.\nOver the past decades, different deep-learning \napproaches in medical imaging have shown promis -\ning results and performance. Drewitt [25] explores ViT \napproach for classifying AD in MRI images. It is also \ncompared with other deep learning-based networks and \nthe article further points out limitations to present future \nprospects of this approach. The performance metrics \ninclude accuracy and F1-Score, with the model attaining \nan accuracy rate of 87.5% and a loss of 0.34 in AD classi-\nfication. This indicates that the proposed model could help \nphysicians to diagnose AD and give a remedial treatment \nto the patients accordingly that can ultimately decrease the \nmortality rate associated with the disease. In another study \n[43], a ViT is trained using natural images to maximize \nthe large-scale data available in computer vision. The pre-\ntrained ViT model is then mobilized to the brain imaging \nsite where few public but relatively excellent samples are \navailable to achieve an accuracy of 96.8%. This indicates \nthe model’s significant scalability performance which can \nbe an improvement upon the traditional neural networks.\nThe growing importance of early AD diagnosis paral-\nlels the aging global population. A study by [100] intro-\nduced a novel approach using the SMIL-DeiT model for \nAD classification. It also used three categories of the dis-\nease including AD, MCI, and NC. The proposed model \nis inspired by ViT, preceded by data pre-training through \nDINO, a self-supervised task. The developed architecture \nis applied to the ADNI dataset and measured by several \nmetrics such as precision, recall, accuracy, and F1-score. \nThe proposed method recognized text with an accuracy of \n93.2%, exceeding what was done by the transformer-based \n(90.1%) and CNN-based (90.8%) models. Self-supervised \npre-training methods such as DINO typically necessitate \nsubstantial data volumes to achieve meaningful representa -\ntion learning. This can be particularly challenging in medi-\ncal imaging, where datasets are frequently constrained in \nsize.\nSeveral studies have proposed innovative approaches for \nthe early diagnosis of AD using deep learning techniques \napplied to medical imaging data, particularly MRI scans. \nSethi et al. [79] introduced a CNN-SVM model that com-\nbines the feature extraction capabilities of CNN with the \nclassification abilities of SVM. This model achieved relative \nimprovements in accuracy ranging from 0.85 to 3.4% on dif-\nferent datasets, with an impressive accuracy of 86.2% on the \nOASIS dataset. The model has shown its potential to be very \naccurate in terms of diagnosis of this specific condition and \nit also works very well with the complicated datasets. These \nadvantages can be crucial for the AD diagnosis at the early \nstages and consideration of further researches in the particu-\nlar field. On the other hand, the efficiency of the model may \nbe affected by the longer training time and the dependence \non big datasets, which could, theoretically, limit the practical \napplicability in certain contexts in real life. Similarly, [80] \ndeveloped a CNN classifier named AlzheimerNet, which can \nidentify all stages of AD and the NC class through MRI \nscans. This model achieved a remarkable test accuracy of \n98.67% and outperformed five other pre-trained models. An \nablation study demonstrated the model’s superior perfor -\nmance and its ability to outperform traditional methods for \nclassifying AD stages from MRI scans. The advantages of \nAlzheimerNet include its high accuracy and robustness to \nnoise, but it may be computationally expensive and require \n Pattern Analysis and Applications (2024) 27:76\n76 Page 8 of 35\na large amount of data for training. Due to the fact that these \nalgorithms are mainly based on CNNs, some challenges \nmay occur such as capturing global context and long-range \ndependencies of data.\nIn conclusion, the literature highlights the effectiveness \nof DL techniques in classifying AD stages from 2D MRI \nimages, with ongoing research focusing on improving accu-\nracy and exploring new approaches such as ViT and trans-\nfer learning models. With no doubt, early detection of AD \nand MCI is essential for timely intervention and improved \npatient outcomes. However, the majority of methods employ \nCNN-based models that come with certain limitations such \nas the inability to capture long-term dependencies and the \nabsence of an attention mechanism. Therefore, it is para-\nmount to develop a more robust system that can cope with \nthese issues.\n3  Proposed Bi‑Vision transformer (BiViT)\nThis section elaborates on the research methodology and the \nconstituent components of the proposed BiViT algorithm. \nIn this research, a novel BiViT architecture is developed \nincorporating parallel coupled encoding strategy (PCES) \nand mutual latent fusion (MLF). First, the proposed meth-\nodology is discussed below followed by each of the novel \naspects including PCES and MLF. It is worht mentioning \nthat the present study focuses on the classification of vari-\nous categories of AD stages and cognitive disorders stages. \nMoreover, the methodology is comprised of five steps: data \naugmentation, preprocessing, patch encoding, CC tokeniza-\ntion, self-attention mechanism and MLF. The methodology \nis illustrated visually in the Fig. 2.\nThe BiViT model incorporates PCES and MLF, which \nenables it to capture local and global contextual information. \nOverall, the proposed methodology consisting of preproc-\nessing, data augmentation, and the BiViT model provides \na robust framework for computer-aided diagnosis of AD \nand other cognitive disorders. The model’s performance \nis evaluated in terms of training, testing, and validation \naccuracy, demonstrating its efficacy in AD diagnosis and \ncognitive disorders classification. We’ll get into the math-\nematical description of the Bi-Vision transformer in the fol-\nlowing paragraphs. We start with the input images Xn and \nthe associated labels, Y . We obtain enhanced versions of \nthe data denoted as X/uni2032.var\nn by using data augmentation tech-\nniques. Since we have limited computational resources, the \nmethod starts with normalizing and reducing the data. As \na result, the images are downsized to 128 × 128 . Follow -\ning the augmentation phase, the minimum and maximum \nvalues in each instance are represented by the variables \nX\n/uni2032.var\nmin and X\n/uni2032.var\nmax . After the data augmentation, resizing, and \nscaling procedures are finished, the data instance that results \nis called X\n/uni2032.var/uni2032.var\nn , as Eq. 5 illustrates.\nOnce the initial preprocessing is completed, the subse-\nquent steps involve parallel stages: patching and tokeniza-\ntion. As part of the patching process, the entire image is \ndivided into 256 smaller patches, each measuring 8 × 8 × 3 . \nwhich are then projected into a lower-dimensional space. \n(5)X\n��\nn =\nX\n�\nn − X\n�\nmin\nX\n�\nmax − X\n�\nmin\nTokenization PatchE ncoder\nData Augmentation &\nPreprocessing\nMRID ata\nSelf-attention\nNonL inear \nTransformation\nSelf-attention\nNon Linear\nTransformation\nRepresentation's Fusion\nDNNM odel\nPredictedD isease\nFig. 2  Proposed methodology\nPattern Analysis and Applications (2024) 27:76 \n Page 9 of 35 76\nAdditionally, we incorporate positional embeddings into the \nprojected patch embeddings. The mathematical formulation \nfor patching on the data batch can be found in Eq.  6. Here, \nx\n/uni2032.var/uni2032.varN\nn  , where N is 1,2,3.. up to number of patches, represents \nthe patches of a single instance in Eq.  6, and E  is for the \nlearnable embeddings.\nIn contrast to patching, tokenization approach involves uti-\nlizing convolutional and pooling transformations for opera-\ntions. Convolutional layers are used to process the data, \nextracting spatial information through convolutions and \ndownsampling operations. These local features are then pre-\nsented as tokens and passed into the transformer for addi-\ntional processing. After applying convolutional operations, \ntokenization process produces patches with a 16 × 16 × 3 \nsize, for a total of 64 patches. Tokenization process con-\nverts the images data into smaller tokens, as depicted in \nEqs. 7 and 8. Tokenization involves first applying pooling \nand convolutional layers globally to the entire image, and \nthen turning the resulting image into tokens.\nThe reason for using 8 × 8 × 3 patches in patching module \nand 16 × 16 × 3 in tokenization module is to effectively cap-\nture and represent the spatial and channel information pre-\nsent in the images. Different sizes are used to achieve both \ndetailed and global context features as bigger token sizes \naid in capturing a wider context, while smaller patch sizes \nenable the capture of more specific information.\nNext, the self-attention is applied to Zp and Z t to capture \ndependencies between the patches and tokens on a local and \nglobal level. The Zp attention block take (q, k, v) as (patches, \npatches, tokens), while Z t takes (q, k, v) as (tokens, tokens, \npatches). This enhances the information provided to both \nattention mechanisms which is better for local and global \nsubtle features learning. With the help of self-attention, the \nmodel is able to comprehend the spatial dependencies and \ncontextual information present in the image by learning the \nrelationships between patches and tokens. The mathematical \nformulations for the self-attention mechanism applied to Zp \nand Z t can be found in Eqs. 9 and 10.\nThe matrix product ZpZT\np  is replaced with the covariance \nmatrix, represented by /u1D719 . To normalize the dot product \n(6)Zp =[ Xclass;x\n��1\nn E ;x\n��2\nn E ....x\n��N\nn E ]+E pos\n(7)Fn = MaxPool(ReLU(Conv2d(X\n��\nn)))\n(8)Zt =[ xclass;F\n��1\nn E ;F\n��2\nn E ....F\n��N\nn E ]+ E pos\n(9)Attention(Zp, Zp, Zt)= Softmax\n�ZpZT\np\n√\nD h\n�\n⋅ Zt\nattention scores in self-attention, \n√\nDh is used as a scaling \nfactor. This helps to sustain gradient stability during training.\nThe matrix product ZtZ T\nt  is replaced with the covariance \nmatrix, represented by Ψ.\nStochastic depth is another idea derived from CCT. Sto -\nchastic depth refers to an approach for randomly skipping or \ndropping network layers during training for the CCT. It is a \nregularisation technique designed to enhance the functionality \nand generalizability of deep neural networks. The use of sto-\nchastic depth to Eq. 10 can be observed in the Eq. 11. The drop \nprobability /u1D717 is the main argument for stochastic depth, while \nthe keep probability is represented as 1 − /u1D709 . The mathematical \nforms of probability (called keep probability) is described in \nEq. 11.\nThe Eq.  12 represents the vector obtained from a random \ndistribution, denoted as U/u1D6FF.\nThe vector UΘ in Eq. 12 represents a uniform vector drawn \nfrom a simple random distribution between 0 and 1. After \nadding /u1D709 , we apply the floor function to the resulting vec-\ntor values to convert them into integers within the domain \nZ . The resulting output, obtained after applying stochastic \ndepth, is illustrated in Eq. 13.\nThe dense transformation and concatenation operations, rep-\nresented by /u1D706/u1D703 , are applied on the output of self-attention (in \ncase of patching), as described in Eq. 14.\nFor tokenization, the same transformations are applied to the \noutput of self-attention, as depicted in Eq.  15.\nFinally, attention weights are computed, and the represen -\ntations /u1D706 and Ω are multiplied by their respective weight \nmatrices (as depicted in Eqs. 16 and 17).\n(10)Attention(Zt, Zt, Zp)=Softmax\n�\nZtZT\nt\n√\nD h\n�\n⋅ Zp\n(11)/u1D709= 1 − /u1D717\n(12)U/u1D6FF= /parenleft.s1/u1D709+ UΘ\n/parenright.s1∈ Z\n(13)Dsd = U/u1D6FF\n/u1D709.Softmax\n�\nΨ√\nD h\n�\n⋅ Zp.\n(14)/u1D706/u1D703= /u1D706/u1D703\n�\nSoftmax( Φ√\nD h\n) ⋅ Zt\n�\n(15)/u1D706Ω = /u1D706Ω\n�\nU/u1D6FF\n/u1D709.Softmax( Ψ√\nD h\n) ⋅ Zp.\n�\n(16)/u1D706w = /u1D70E(/u1D706/u1D703./u1D7141 )T\ni ./u1D706/u1D703\n Pattern Analysis and Applications (2024) 27:76\n76 Page 10 of 35\nIn Eq.  16, the softmax function /u1D70E is applied to the prod -\nuct of /u1D706/u1D703 and the trainable weight matrix /u1D7141 , denoted as \n(/u1D70E(/u1D706/u1D703./u1D7141 )T\ni ) . The expression /u1D70E(/u1D706/u1D703./u1D7141 )T\ni  represents the soft-\nmax function applied to the output ( /u1D70E ) multiplied by the \nweight matrix /u1D714 . Similarly, in the tokenization phase, the \nsame process is performed as described in Eq.  17.\nFinally, we incorporate a fusion function, denoted as F/u1D715 , \nto combine the weighted representations obtained. This \nfusion function combines the information from both repre-\nsentations. In our specific case, we use concatenation as the \nfusion method, which is illustrated in Eq. 18.\nThe fusion function, denoted as Ffusion , takes two argu-\nments, /u1D706w and Ωw , as depicted in Eq. 19.\nThe Eq. 19 represents the final representations that are input \nto the classifier in our case, which is a Softmax classifier due \nto the multi-class classification task. The process starting \nfrom self attention and continuing until Eq. 19 is repeated n \ntimes for efficient representation learning. This entire pro -\ncess is often referred to as the transformer encoding, which \nencodes patches/tokens into meaningful representations. The \nPCES and MLF processes are covered in the following sec-\ntions, but first it’s critical to comprehend the tokenization \nand patching processes. A detailed discussion about how \ntokenization and patching operate on images is given in \nSect. 3.1.\n(17)Ωw = /u1D70E(Ω/u1D703./u1D7142 )T\ni .Ω/u1D703\n(18)F/u1D715= F/u1D715\n/parenleft.s1/u1D706w, Ωw\n/parenright.s1\n(19)F/u1D715= F/u1D715\n/parenleft.s1/u1D70E(/u1D706/u1D703./u1D7141 )T\ni ./u1D706/u1D703, /u1D70E(Ω/u1D703./u1D7142 )T\ni .Ω/u1D703\n/parenright.s1\n3.1  Patch encoding and tokenization \nin transformers\nPatch encoding and tokenization are both important tech-\nniques used in computer vision tasks because they allow \ninput images to be divided into smaller, more manage-\nable parts that can be more efficiently processed by neural \nnetworks. This is particularly crucial when dealing with \nlarge, high-resolution images, which can be computation-\nally expensive to process using traditional methods. The \npatch encoding and tokenization processes are illustrated \nin Figs.  3 and 4, and are explained below.\n3.1.1  Patch encoding\nTo use a transformer model to process image data, patch \nencoding is performed where the image is divided into \nfixed-size patches that are flattened into vectors [24]. \nThese patch vectors are then sent to the transformer model \nwhich utilizes the self-attention mechanism to extract vis -\nual features and classify the image. Patch encoding is more \nflexible and efficient for processing high-resolution images \nas can be seen in Fig.  3.\n3.1.2  Image tokenization\nIn CCT, the process of converting image patches into \nlearnable representations called ’image tokens’ using \na trainable CNN is known as image tokenization [ 38]. \nThese image tokens are subsequently fed into a trans-\nformer encoder to perform tasks like image classification \nor object detection. Unlike traditional CNN, this approach \neliminates the need for fully connected layers and pool-\ning layers, which enhances the efficiency and flexibility of \nFig. 3  Process of patch encod-\ning in ViT\n\nPattern Analysis and Applications (2024) 27:76 \n Page 11 of 35 76\nimage processing. The Fig.  4 shows the steps of tokeniza-\ntion visually. The visual representation of the tokenization \nprocess can be observed in Fig.  4.\n3.2  Parallel coupled encoding strategy (PCES)\nIn transformer encoding phase, PCES is implemented to \nprocess images using both patch encoding, as shown in \nFig. 3, and CC tokenizer, illsutrated in Fig. 4. In the context \nof a CCT, CC tokenizer refers to the process of breaking \ndown an image or video into smaller, manageable pieces, or \n\"tokens\", that can be processed by the CCT’s convolutional \nencoder. The CC tokenizer is responsible for dividing the \ninput image or video into a fixed-size grid of non-overlap-\nping patches, each of which is then represented as a set of \npixels. These patches are then converted into tokens, which \nare then passed through the convolutional encoder to extract \nspatial features. These tokens are then passed to the trans-\nformer decoder to generate the output. The CC tokenizer is \nresponsible for the pre-processing step that allows CCT to \nprocess images and videos more efficiently, as it reduces the \ndimensionality of the input data by breaking it down into \nsmaller, more manageable pieces.\nPatch encoding refers to the process of breaking down an \nimage into smaller, manageable pieces, or \"patches\", that \ncan be processed by the ViT’s transformer-based architec-\nture. The patch encoding is responsible for dividing the input \nimage into a fixed-size grid of non-overlapping patches, each \nof which is then represented as a set of pixels. These patches \nare then flattened and passed through a linear layer to obtain \na feature vector. These feature vectors are then used as the \ninput to the transformer-based architecture, where the self-\nattention mechanism is applied to learn the relationships \nbetween the patches and generate the output. The patching \nprocess in ViT is mathematically described by Eqs.  6, 9, \nand 14.\nThe patch encoding step allows the ViT to process \nimages more efficiently by reducing the dimensionality \nof the input data by breaking it down into smaller, more \nmanageable pieces, and it also allows the model to learn \nthe relationships between the patches, which is useful for \nimage classification and other tasks. Here, the two con -\ncepts are combined together and interconnect in a trans-\nformer encoder. In simple transformers, a single encoding \nstrategy is used, however, in proposed model, two strat-\negies are applied and hence known as PCES. It means \nthat two mechanisms run in parallel while information is \nFig. 4  Image tokenization process in CCT \nFig. 5  PCES in transformer\n Pattern Analysis and Applications (2024) 27:76\n76 Page 12 of 35\nexchanged continuously between two encoding as shown \nin Fig. 5.\n3.3  Mutual latent fusion (MLF)\nFollowing the patch encoding process in ViT, the self-\nattention mechanism is applied by the model to capture the \nconnections between the patches and produce the ultimate \noutput. The self-attention mechanism in ViT represents each \npatch with a key, query, and value vector and calculates the \ndot product between the query and key vectors for all patches \n[49]. The dot product values are then used to compute a \nweight for each patch, which determines the importance of \nthe patch for the final output. These weights are used to \ncompute a weighted representation of the patches by taking \na linear combination of the value vectors.\nAfter applying the self-attention mechanism in ViT, the \nweighted representation is produced and it includes the \ncrucial information from the patches, which is further pro-\ncessed by a feedforward neural network to generate the final \nprediction; thus, the weighted representation in ViT is the \nresult of the encoding and attention mechanism, consisting \nof the combined patch information weighted by the attention \nmechanism.\nTo combine the latent representations generated from \nboth the CC tokenizer and patch encoding process, a fusion \nmechanism is used where the representations are merged to \ncreate a final output for the model. Equation 19 provide the \nmathematical form for the combined MLF process.\nIn neural network, feature fusion refers to the process \nof combining information from multiple layers or multiple \nchannels of the network to form a more comprehensive and \nrobust feature representation [85]. To combine the weighted \nrepresentation of encoded information, the MLF concept is \nutilized followed by passing the features to the classifier to \npredict the instance’s actual label, as depicted in the Fig. 6.\n4  Experiments and results\nThis section presents a comparison between the BiViT \nalgorithm and various transfer learning algorithms includ-\ning DenseNet121, ResNet50, VGG16, VGG19, Inception-\nResNet V2, Inception V3, EfficientNet B0, EfficientNet \nB1, ResNet-101, Xception, MobileNet, and ResNet-152. \nAdditionally, unsupervised deep learning techniques such \nas convolutional autoencoders, variational autoencod-\ners, and sparse autoencoders were utilized for classify -\ning abnormalities in instances with a primary focus on \nAD. The subsequent sections discuss the performance of \ndifferent deep learning techniques in comparison to the \nproposed BiViT algorithm.\nThis section follows a structured outline beginning with \nSects.  4.1.1, and 4.1.2, which describe the datasets used \nin the research. In Sect.  4.2, the preprocessing techniques \nand augmentation techniques utilized in this study for the \nclassification task are outlined. Section  4.3 and 4.5.1 detail \nthe use of various transfer learning algorithms, both with \nand without augmentation, for classifying AD, and then \ncompare the performance of all algorithms. The results \nof this analysis are presented in Tables  3 and 4 . Table  5 \nsummarizes the class-wise performance of the top four \nmodels out of all trained models. Additionally, Sect.  4.5.2 \ndescribes the performance of different unsupervised deep \nlearning techniques, such as autoencoders, to extract fea-\ntures from images and classify instances based on abnor -\nmalities. The results of unsupervised learning-based \nclassification of AD stages are presented in Table  6. Sec-\ntion 4.6 focuses on the same transfer learning algorithms \nand deep autoencoders for the classification of cognitive \ndisorders other than AD. The outcomes of this analysis are \npresented in Tables  7, 8, 9, and 10.\nThe proposed approach is applied to classify cogni-\ntive disorders and AD. The model is trained for up to \n200 epochs using MRI images, with a total of 2,569,014 \nWeighted \nRepresentation\nBlock 1\nWeighted \nRepresentation\nBlock 2\nFusion of Weighted Representation (MLF)\nFig. 6  MLF in transformer\nTable 1  Hyper parameter setting and information about the proposed \nBiViT\nCoupled Bi-Vision Transformer (BiViT)\nSr. No. Detail Quantitative values\nParameters 2,569,014\n2 Size of input images 128 × 128 × 3\n3 Epoch 200\n4 No of classes 4 and 5\n5 Batch size 32\n6 Learning rate 0.001\n7 Optimizer Adam\n8 Verbose False\nPattern Analysis and Applications (2024) 27:76 \n Page 13 of 35 76\ntrainable parameters. Table  1 provides a summary of the \nhyperparameters (learning rate, batch size, number of \nepochs etc.) used in this research work. Various metrics, \nincluding F1 score, accuracy, recall, precision, and AUC, \nare employed for evaluating the performance of the BiViT \nmodel.\n4.1  Dataset\nThe research conducted uses two publicly available MRI \ndatasets. The first dataset is utilized for AD stage classi-\nfication and consists of four classes: mild demented, very \nmild demented, non-demented, and moderate demented. \nThe second dataset is used for detecting various cognitive \nimpairments and consists of five types of disorders: AD, \nCN, EMCI, LMCI, and MCI [35, 83]. The first dataset with \nfour classes is used to classify the stage of AD, which is a \nprogressive brain disorder affecting memory, thinking, and \nbehavior. The classification process can help with early diag-\nnosis and treatment of the disease. The four classes in the \ndataset represent different stages of the disease, with mild \ndemented and very mild demented indicating early stages \nand non-demented and moderate demented indicating later \nstages. The stages of cognitive disorder range from mild to \nsevere, with the severity of cognitive decline increasing as \nthe disease progresses. This stage of cognitive decline is \noften referred to as moderate to severe dementia [27]. Fig-\nure 7 illustrates the hierarchical structure of both datasets.\nThe second dataset is used to detect various cognitive \ndisorders, including AD, and is divided into five types of \ndisorders. The CN class represents individuals without any \ncognitive impairment, while the other four classes rep -\nresent different stages of cognitive decline. On the other \nhand, MCI is a condition in which an individual experi-\nences mild cognitive decline beyond what is expected for \ntheir age but is still able to perform their daily activities. \nEMCI refers to the early stages of cognitive decline [42], \nand LMCI refers to a more advanced stage of cognitive \ndecline [58]. Finally, AD is a progressive neurodegen-\nerative disorder that affects memory, and behavior, and \nis the most common cause of dementia in older adults. \nEarly detection of cognitive disorders can help with timely \nFig. 7  Visual hierarchy of data\nTable 2  Alzheimer disease and cognitive disorders data distribution\nSample distribution of cognitive disorders dataset\nSamples CN EMCI LMCI MCI AZ\nTraining 401 172 47 166 114\nTesting 179 68 25 67 57\nSample distribution of Alzheimer disease dataset\nSamples Normal Moderate Mild Very Mild\nTraining 2797 58 781 1964\nTesting 403 6 115 276\n Pattern Analysis and Applications (2024) 27:76\n76 Page 14 of 35\ninterventions to slow down the progression of the disease \nand improve the quality of life of affected individuals.\nThe Table  2 presents the details of training and test-\ning samples utilized in the conducted experiments of this \nstudy. In the upcoming section, we provide a brief over -\nview of each dataset.\n4.1.1  Alzheimer disease data\nThe dataset used in this research was collected from sev -\neral websites, hospitals, and public repositories [26]. The \ndataset consists of preprocessed MRI images that were \nresized into 128 × 128 pixels. The dataset is comprised of \nfour different classes of images, which are classified based \non the severity of dementia. In total, the dataset contains \n6400 MRI images, after the augmentation process [95]. \nThe Fig.  8 depicts the MRI samples from all the classes \ntogether.\nThe first class in the dataset is mild demented, which \ncontains 896 images. This class represents patients who have \nmild symptoms of dementia. The second class is moderate \ndemented, which contains 64 images. This class represents \nFig. 8  Alzheimer MRI Data Samples\nFig. 9  Distribution of Alzhei-\nmer disease dataset\nPattern Analysis and Applications (2024) 27:76 \n Page 15 of 35 76\npatients who have moderate symptoms of dementia. The \nthird class is non demented, which contains 3200 images. \nThis class represents patients who do not have dementia. The \nfourth and final class is very mild demented, which contains \n2240 images. This class represents patients who have very \nmild symptoms of dementia. The distribution of MRI data \nfor AD can be seen in Fig. 9.\n4.1.2  Cognitive disorders data\nThe dataset used in this study comprises five different stages \nof cognitive disorders, which have been divided into two \ndirectories for the purposes of training and testing. The \nstages included in the dataset are EMCI, LMCI, MCI, AD, \nand CN. Specifically, there are 204 samples of EMCI, 61 \nFig. 10  Cognitive disorders MRI samples\nFig. 11  Distribution of cogni-\ntive disorders dataset\n Pattern Analysis and Applications (2024) 27:76\n76 Page 16 of 35\nsamples of LMCI, 198 samples of MCI, 145 samples of AD, \nand 493 samples of CN, for a total of 1101 samples [18]. The \ndataset was sourced from the ADNI website and was created \nas a collaborative effort to accelerate Alzheimers research. \nThe ADNI website is duly credited as the source of the data-\nset [2]. The MRI data samples for cognitive disorders are \nillustrated in Fig. 10.\nFigure 11 illustrats the distribution of data across differ -\nent stages of cognitive decline.\n4.2  Data preprocessing\nTo make the images suitable for the model, the preproc-\nessing step aims to enhance their quality thereby improv -\ning the performance of the model. The primary objective \nof preprocessing is to enhance important image features to \nimprove the image data for further processing. One of the \nmost important steps in improving the quality of images is \npreprocessing, which highlights important details. The scal-\ning and resizing operation is performed to standardize the \nsize and resolution of images. In contrast, scaling modifies \npixel intensity values to enhance contrast and detail visibil-\nity, as a result, the pixels have values between 0 and 1. The \nimages were resized to 128 × 128 dimensions, and subse -\nquently, normalized using Eq. 20 such that the pixel values \nranged between 0 and 1. Let I represent the input image hav-\ning a size ( m × n ) and Inorm  represent the normalized MRI, it \ncan be given as follows.\nData augmentation is incorporated into the methodology to \nfurther enhance the learning efficacy of deep learning mod-\nels once the preprocessing phase is finished. These changes \ncontribute to the training datasets diversification, which may \nresult in a more resilient model with improved ability to \ngeneralize new data. Data augmentation helps to improve the \ngeneralization ability of the deep learning model by expos-\ning it to a wider variety of images, thereby reducing overfit-\nting and improving the model’s ability to learn from limited \ndata. For the augmentation process, two types of techniques \nare employed: random crop and random flip. Random crop \ninvolves selecting a random portion of the image while pre-\nserving its aspect ratio. This technique helps introduce vari-\nability in the training data by focusing on different regions of \nthe image. On the other hand, random flip involves horizon-\ntally flipping the image. This augmentation technique helps \nthe model to learn recognize objects and patterns from dif-\nferent orientations, further enhancing its ability to generalize \nand perform well on unseen data. By combining these two \ntypes of augmentations, a more diverse and robust training \n(20)Inorm = I − Imin\nImax − Imin\ndataset is created, facilitating better learning and improved \nperformance of the model.\n4.3  Explanation of transfer learning architectures \nand unsupervised autoencoders\nIn this section, various transfer learning algorithms are \nexplored to use in comparative analysis. The aim is to assess \nthe performance of the proposed model in comparison to \ndifferent transfer learning models including DenseNet-121, \nResNet-50, VGG19, VGG16, Inception ResNet-v2, Incep-\ntion-v3, EfficientNet-B0, EfficientNet-B1, Xception, \nMobileNet, and ResNet-152.\nDenseNet-121 is a CNN-based architecture renowned \nfor its dense connectivity pattern. Within this architec-\nture, each layer is directly connected to every other layer \nin a feed-forward manner, making it highly parameter-\nefficient [41]. ResNet-50, on the other hand, is a varia -\ntion of the Residual Neural Network (ResNet) architecture \nthat was specifically developed to tackle the vanishing \ngradient problem in deep networks. This strategic addi-\ntion ensures that gradient information is preserved dur -\ning training, thereby enabling more effective learning as \ndescribe in Eq.  21.\nResNet-152, part of the ResNet family, is a CNN architec-\nture introduced in 2015 by [39] for image recognition. By \nincorporating skip connections, ResNet-152 simplifies the \ntraining and optimization of deep neural networks.\nVGG19 and VGG16 are part of a distinct category of \ntransfer learning models, originating from the Visual Geom-\netry Group (VGG) architecture, renowned for its simplicity \nand uniformity. These models consist of several layers of \nsmall-sized convolutional filters, followed by max-pooling \nlayers. VGG19 has 19 layers, whereas VGG16 has 16 layers \n[71]. Inception ResNet-v2, another transfer learning model, \ncombines the strengths of both Inception and ResNet archi-\ntectures, resulting in a powerful hybrid model. The presence \nof residual connections ensures efficient gradient propaga-\ntion during training [86].\nThe EfficientNet family comprises models aimed at \nachieving an optimal balance between model size and accu-\nracy. EfficientNet-B0 serves as the baseline model within \nthis family and leverages compound scaling to optimize \ndepth, width, and resolution simultaneously. On the other \nhand, EfficientNet-B1 is designed to be slightly larger and \nmore accurate than B0. It adopts the same compound scaling \n(21)xu = xu\n⎛\n⎜\n⎜⎝\n⎡\n⎢\n⎢⎣\n1 ⋯ 0\n⋮⋱⋮\n0 ⋯ 1\n⎤\n⎥\n⎥⎦\n+ /u1D715F\n/u1D715x\n⎞\n⎟\n⎟⎠\nPattern Analysis and Applications (2024) 27:76 \n Page 17 of 35 76\ntechnique but with greater model size and complexity when \ncompared to B0 [90].\nXception, which stands for Extreme Inception, is a \nCNN architecture inspired by the Inception model. Xcep -\ntion employs depthwise separable convolutions, effectively \nreducing the number of parameters and computational \nburden when compared to conventional convolutions [19]. \nFinally, we have MobileNet, a family of lightweight CNN \narchitectures, specially designed for mobile and embedded \ndevices. These models leverage depthwise separable convo-\nlutions to decrease computational demands and model size \nwithout compromising on reasonable accuracy [40, 76].\nNext, a discussion about various types of autoencoders \nemployed in this study is presented for the purpose of con-\nducting a comparative analysis in this paper. A Convolu -\ntional Autoencoder (CAE) is a specialized type of autoen-\ncoder that employs convolutional layers in its encoder and \ndecoder, rather than fully connected layers. It is specifically \ndesigned for handling image data and excels in capturing \nspatial patterns and features from images. By using convolu-\ntional layers, the CAE can effectively reduce the dimension-\nality of the input data while retaining crucial features [101]. \nVariational Autoencoder (VAE) is a probabilistic variant of \nthe conventional autoencoder. Unlike a standard autoencoder \nthat merely learns an encoded representation of the input \ndata, VAE also models the underlying probability distribu-\ntion of the encoded data. This unique characteristic enables \nVAEs to generate new data samples by sampling from the \nlearned probability distribution [50].\nThe sparse autoencoder (SPAE) is a type of autoencoder \nthat enforces sparsity constraints during its training process. \nThese constraints encourage the autoencoder to utilize only \na limited number of neurons in its hidden layer to represent \nthe input data. As a result, the autoencoder produces a more \nconcise and efficient representation of the data, offering ben-\nefits such as reducing overfitting and enhancing generaliza-\ntion in certain situations [68].\nUndercomplete and overcomplete autoencoders are two \nvariations of the traditional autoencoder. An undercomplete \nautoencoder has a hidden layer with fewer neurons than the \ninput layer, which results in a compressed representation of \nthe input data. On the contrary, an overcomplete autoencoder \nhas a hidden layer with more neurons than the input layer, \nleading to a redundant representation of the data. This allows \nthe autoencoder to learn multiple representations of the same \ndata, offering more flexibility but also increasing the risk of \noverfitting [14, 89].\n4.4  Experiments setting and hyper‑parameter \nconfiguration\nIn the following section, a comparative analysis to assess \nthe performance of our proposed model is presented in \ncomparison to state-of-the-art transfer learning models and \nunsupervised autoencoders. Here, the experiment setting and \nhyperparameters configuration utilized during the experi-\nments and comparisons is discussed. Initially, details about \nthe experiment setting is provided, followed by a compre-\nhensive discussion of the hyperparameters configuration for \nall the other algorithms trained for comparative purposes. \nThe hyperparameter configuration of proposed BiViT is pre-\nsented at the beginning of Sect.  4.\nThe experiment setting remains consistent across all \nmodels, including the proposed BiViT model. A standard-\nized process of loading, preprocessing, and splitting the data \ninto training and testing sets is followed. Subsequently, all \nalgorithms are trained for the same number of epochs (200), \nusing a uniform learning rate (0.001), optimizer (Adam), \nand batch size of 32. For the transfer learning models, the \nweights are initialized with \"imagenet\" weights and utilized \nonly for the classifier layer of multi-class classification after \nthe flattened layer. For the learning rate, the value is set to \ndefault as it has shown satisfactory performance. The num-\nber of epochs is 200 to allow sufficient time for observing \ntrends in accuracy and loss measures. This gives an oppor -\ntunity to analyze how performance evolves over time. Addi-\ntionally, since the dataset is extensive due to augmentation, \na batch size of 32 is used to facilitate efficient and smooth \nexecution of the experiments.\nFor unsupervised autoencoder, a 5-layer convolutional \nencoder and a 4-layer convolutional decoder is used. \nThroughout all layers, the filter size remains consistent at \n( 3 × 3 ). The latent space has a dimension of 2, which we \nuse for classifying the disease. The autoencoders are trained \nfor 10 epochs with a learning rate of 0.001, a batch size of \n32, and the Mean Squared Error (MSE) serving as the loss \nfunction.\n4.5  Comparative analysis on Alzheimer disease \ndiagnosis\nIn this section and the following subsections, a comparative \nanalysis is performed on different transfer learning algo-\nrithms for AD classification. Transfer learning is a technique \nwhere a pre-trained neural network is used as a starting point \nfor a new task. The pre-trained neural network has learned \ngeneral features from a large dataset and these features can \nbe reused for a new task, which can save time and compu-\ntational resources. Two tables are created for the experi-\nment, one with data augmentation experiments and the other \nwithout data augmentations. One table contains data that \nhas been subjected to data augmentation techniques, which \ninvolves artificially generating new data by making changes \nto existing data samples. The other table contains data that \nhas not undergone any data augmentation. Both tables are \nlikely used to compare the performance of machine learning \n Pattern Analysis and Applications (2024) 27:76\n76 Page 18 of 35\nor other data-driven models on the augmented versus non-\naugmented data.\nData augmentation is a technique where new training \nsamples are generated by applying transformations such as \nrotations, flips, and scaling to the original images. This can \nincrease the diversity of the training data and improve the \nperformance of the model. The third table in the analysis \ndescribed the class-wise prediction performance of the top \nthree best-performing models. This table provided addi -\ntional insights into how each algorithm performed for dif -\nferent classes of AD. Multiple algorithms are trained and the \nresults are reported. Each algorithm has a different archi -\ntecture and varying number of layers. The performance of \nthese algorithms is compared in terms of accuracy, recall, \nprecision, F1-score and AUC.\nSection 4.5.2 discusses the usage of various deep unsu-\npervised learning algorithms, also known as autoencoders, \nto classify instances with abnormality. These autoencoders \nincluded CAE, VAE, SPAE, and undercomplete and over -\ncomplete CAEs. The performances of these autoencod-\ners are evaluated based on their ability to extract relevant \nfeatures from the input images and classify instances with \nabnormality.\n4.5.1  Alzheimer detection with transfer learning \nalgorithms\nIn this section, a comparative analysis is presented between \ndifferent transfer learning algorithms for classifying AD and \nthe results presented in Table 3 in terms of accuracy, recall, \nprecision, F1-score, and AUC. It is important to note that the \nresults in Table 3 are obtained by applying data augmenta-\ntion techniques to the Alzheimer data, which means that the \nalgorithms were trained on augmented data. Then, the mod-\nels are tested using test data to evaluate their performance. \nThe visual performance of the proposed BiViT algorithm \n(with data augmentation) after each epoch is illustrated in \nFig. 12.\nThe performance of different algorithms in terms of vari-\nous measures such as precision, recall, F1-score, accuracy, \nand AUC are compared in a Table  3. From these results, it \ncan be concluded that the proposed BiViT model outper -\nformed competing algorithms in terms of all performance \nmeasures. Furthermore, it can be observed that the precision \nis low for all the algorithms whereas the recall is high. This \nsuggests that the algorithms are able to detect most of the \npositive cases, but at the cost of many false positives. How-\never, the BiViT model can achieve good results in terms of \nboth recall and F1 score.\nIn contrast, results illustrated in Table  4 reveals that all \ntransfer learning algorithms perform remarkably well even \nwithout the use of data augmentation techniques. In addi-\ntion, the results are significantly superior to those achieved \nby applying data augmentation in previous studies. A visual \nperformance in terms of progress of the proposed BiViT \nalgorithm after each epoch is depicted in Fig. 13, where data \naugmentation was not used. It can be observed that the pro-\nposed BiViT model outperforms all other transfer learning \nmodels with the highest accuracy and recall scores. Based \non these results, it can be concluded that data augmentation \ntechniques may not always be necessary to achieve good per-\nformance in image classification tasks, and that the proposed \nBiViT model shows promising results in this regard. Moreo-\nver, the Table 5 shows the classwise comparative analysis of \nthe 4 best performing algorithms.\nTable 3  The table provides \na summary of the results \nobtained by various transfer \nlearning algorithms for AD \ndiagnosis with different data \naugmentation techniques\nTransfer learning for Alzheimer diagnosis (with data augmentation)\nAlgorithm Accuracy Recall Precision F1-Score AUC-ROC\nDenseNet-121 0.56 ± 0.057 1.00 ± 0.003 0.33 ± 0.016 0.50 ± 0.018 0.66 ± 0.026\nResnet-50 0.52 ± 0.033 0.96 ± 0.032 0.39 ± 0.023 0.55 ± 0.022 0.81 ± 0.028\nVGG16 0.62 ± 0.034 0.93 ± 0.017 0.45 ± 0.016 0.60 ± 0.014 0.87 ± 0.016\nInception Resnet-V2 0.58 ± 0.039 1.00 ± 0.025 0.34 ± 0.021 0.51 ± 0.020 0.77 ± 0.024\nInception V3 0.53 ± 0.043 1.00 ± 0.005 0.25 ± 0.027 0.40 ± 0.032 0.53 ± 0.033\nEfficientNet-B0 0.50 ± 0.70 1.00 ± 0.005 0.25 ± 0.028 0.40 ± 0.033 0.66 ± 0.048\nResNet-101 0.54 ± 0.016 0.93 ± 0.021 0.39 ± 0.018 0.55 ± 0.016 0.81 ± 0.007\nVGG19 0.63 ± 0.030 0.77 ± 0.065 0.52 ± 0.026 0.63 ± 0.023 0.87 ± 0.013\nEfficientNet-B1 0.51 ± 0.092 1.00 ± 0.004 0.25 ± 0.031 0.40 ± 0.037 0.66 ± 0.070\nXception 0.59 ± 0.040 1.00 ± 0.003 0.32 ± 0.019 0.49 ± 0.022 0.68 ± 0.031\nMobileNet 0.60 ± 0.042 1.00 ± 0.011 0.34 ± 0.020 0.51 ± 0.021 0.77 ± 0.025\nResNet-152 0.54 ± 0.021 0.99 ± 0.012 0.36 ± 0.015 0.53 ± 0.015 0.82 ± 0.007\nProposed BiViT 0.93 ± 0.010 0.97 ± 0.021 0.75± 0.132 0.85 ± 0.026 0.98 ± 0.011\nPattern Analysis and Applications (2024) 27:76 \n Page 19 of 35 76\nFig. 12  Performance of pro-\nposed BiViT with data augmen-\ntation technique\n(a) (b)\n(c) (d)\n(e)\nTable 4  The summary table \npresents the results achieved \nby different transfer learning \nmethods for diagnosing AD, \nwithout the use of any data \naugmentation techniques\nTransfer learning for Alzheimer diagnosis (without data augmentation)\nAlgorithm Accuracy Recall Precision F1-Score AUC-ROC\nDenseNet-121 0.59 ± 0.113 1.00 ± 0.000 0.32 ± 0.027 0.48 ± 0.033 0.67 ± 0.064\nResnet-50 0.71 ± 0.038 0.97 ± 0.015 0.46 ± 0.027 0.63 ± 0.026 0.91 ± 0.018\nVGG16 0.88 ± 0.066 0.98 ± 0.005 0.63 ± 0.067 0.77 ± 0.057 0.97 ± 0.025\nInception Resnet-V2 0.83 ± 0.083 1.00 ± 0.003 40.35 ± 0.017 0.52 ± 0.020 0.77 ± 0.033\nInception V3 0.66 ± 0.049 1.00 ± 0.001 0.36 ± 0.020 0.53 ± 0.023 0.80 ± 0.035\nEfficientNet-B0 0.56 ± 0.077 1.00 ± 0.003 0.25 ± 0.038 0.40 ± 0.046 0.79 ± 0.041\nResNet-101 0.63 ± 0.035 0.95 ± 0.024 0.43 ± 0.018 0.59 ± 0.017 0.88 ± 0.019\nVGG19 0.89 ± 0.066 1.00 ± 0.007 0.52 ± 0.042 0.68 ± 0.041 0.97 ± 0.027\nEfficientNet-B1 0.36 ± 0.095 1.00 ± 0.002 0.25 ± 0.038 0.40 ± 0.046 0.76 ± 0.040\nXception 0.79 ± 0.052 1.00 ± 0.001 0.39 ± 0.020 0.56 ± 0.022 0.84 ± 0.021\nMobileNet 0.96 ± 0.035 1.00 ± 0.001 0.63 ± 0.063 0.78 ± 0.054 0.99 ± 0.013\nResNet-152 0.68 ± 0.043 0.92 ± 0.025 0.46 ± 0.023 0.61 ± 0.021 0.90 ± 0.019\nProposed BiViT 0.96 ± 0.102 0.98± 0.174 0.88 ± 0.010 0.93± 0.145 0.99 ± 0.038\n Pattern Analysis and Applications (2024) 27:76\n76 Page 20 of 35\n4.5.2  Alzheimer detection with deep unsupervised \nlearning\nDeep autoencoders are a type of unsupervised learning tech-\nnique that can extract features by modeling latent manifold \nin data. These features can then be used to classify instances \nusing a simple classifier. In this study, different autoencoders \nsuch as CAE, VAE, SPAE, undercomplete and overcomplete \nAE, are used for AD classification and the results are pre-\nsented in Table 6. Each of these autoencoders were trained \nand tested on the dataset to classify instances with abnormal-\nity. The results show the performance of each autoencoder \nin terms of different evaluation metrics, such as accuracy, \nrecall, and F1-score.\nThe results conclude that autoencoders perform well and \ncan compete with transfer learning algorithms in terms of \nAD classification. In fact, some of the autoencoders even \noutperformed the transfer learning algorithms. Moreover, \nthe SPAE performed the best out of all the autoencoders \nwhich means that the SPAE can extract the most informative \nfeatures from the images and classify AD instances more \naccurately than the other autoencoders. The performance of \nSPAE, CAE, and VAE can be observed in Figs. 14, 15, and \n16. Figures 17 and 18 display the performance of over and \nunder autoencoders.\n4.6  Comparative analysis on cognitive diseases \ndiagnosis\nIn this section and its subsequent subsections (Sects.  4.6.1 \nand 4.6.2), a comparison is made among different transfer \nlearning techniques for the purpose of classifying cognitive \ndisorders. Two tables are presented, with data augmenta-\ntion (Table  7) and without data augmentation (Table  8). \nThe tables are used to compare the performance of machine \nlearning models on augmented versus non-augmented data. \nAdditionally, Table 9 provides the class-wise performance \nof the top performing four algorithms among all. These sec-\ntions also present a comparison of various autoencoders for \ncognitive disorders classification illustrated in Table  10.\nFig. 13  Performance of pro-\nposed BiViT model without \ndata augmentation\n(a) (b)\n(c) (d)\n(e)\nPattern Analysis and Applications (2024) 27:76 \n Page 21 of 35 76\n4.6.1  Transfer learning for cognitive disorder diagnosis\nDifferent transfer learning techniques, including the pro-\nposed BiViT model, were employed to classify cogni-\ntive disorders. However, due to limited and imbalanced \ndataset, the performance of not even a single algorithm \nis upto the mark even after utilizing data augmentation \nmethods. Table  7 presents the results of various algo-\nrithms trained on the cognitive disease dataset. It can be \nseen that all the algorithms have an accuracy within the \nTable 5  Class-wise performance of best 4 models in AD classification case\nAlgorithms Alzheimer stages With augmentation Without augmentation\nAccuracy Recall Precision Accuracy Recall Precision F1-Score\nBiViT Mild-demented 0.93 0.90 0.93 0.96 0.93 0.98 0.95\nModerate-demented 0.93 1.00 0.80 0.96 0.83 0.83 0.83\nNon-demented 0.93 0.96 0.93 0.96 0.98 0.96 0.97\nVery Mild-demented 0.93 0.90 0.93 0.96 0.95 0.97 0.96\nWeighted average 0.93 0.90 0.93 0.96 0.96 0.96 0.96\nVGG-16 Mild-demented 0.62 0.17 0.69 0.88 0.92 0.76 0.83\nModerate-demented 0.62 0.58 1.00 0.88 1.00 1.00 1.00\nNon-demented 0.62 0.61 0.78 0.88 0.86 0.96 0.90\nVery Mild-demented 0.62 0.79 0.50 0.88 0.91 0.85 0.88\nWeighted average 0.62 0.62 0.67 0.88 0.88 0.89 0.89\nMobileNet Mild-demented 0.60 0.72 0.35 0.96 0.91 0.97 0.94\nModerate-demented 0.60 0.70 1.00 0.96 1.00 1.00 1.00\nNon-demented 0.60 0.60 0.77 0.96 0.95 0.97 0.96\nVery Mild-demented 0.60 0.54 0.60 0.96 0.98 0.94 0.96\nWeighted average 0.60 0.54 0.60 0.96 0.96 0.96 0.96\nVGG-19 Mild-demented 0.62 0.36 0.53 0.88 0.92 0.83 0.87\nModerate-demented 0.62 0.27 0.75 0.88 1.00 1.00 1.00\nNon-demented 0.62 0.74 0.73 0.88 0.92 0.87 0.89\nVery Mild-demented 0.62 0.60 0.53 0.88 0.79 0.92 0.85\nWeighted average 0.62 0.63 0.63 0.88 0.88 0.88 0.88\nTable 6  The table presents a \ncomparison between various \nautoencoders (an unsupervised \nlearning technique) for \ndiagnosing AD without using \ndata augmentation\nComparison of unsupervised learning for Alzheimer diagnosis\nAlgorithm Accuracy Recall Precision F1-Score AUC AE(mae)\nConvolutional AE 0.9538 0.9825 0.5692 0.7231 0.9595 0.086\nVariational AE 0.8462 0.8750 0.5952 0.7083 0.8569 0.090\nSparse AE 0.9825 0.9950 0.4500 0.6210 0.8794 0.087\nUnderComplete AE 0.8500 0.9962 0.4377 0.6088 0.8277 0.079\nOverComplete AE 0.9400 0.9912 0.5420 0.7016 0.9513 0.065\nFig. 14  Latent space of sparse \nautoencoder\n Pattern Analysis and Applications (2024) 27:76\n76 Page 22 of 35\nrange of 35–45%. Although the recall score is high, the \nprecision and F1-score are low. However, the proposed \nBiViT model can perform very well in terms of accuracy \nand F1-score. Table  8 provides information on the per -\nformance of the same set of algorithms as in Table  7, \nbut this time they are trained without performing data \naugmentation.\nThe Table 8 compares the performance of different trans-\nfer learning algorithms without data augmentation tech-\nniques. Although the range of accuracy remains similar, \ni.e., 40–45%, the precision and F1-score are consistently \nlow. Even the proposed BiViT model performs poorly in \nthis scenario, which is primarily attributed to the limited and \nimbalanced nature of the dataset. It is suggested that with an \nincrease in data quantity, there may be an improvement in \nthe performance of all algorithms in the future. The Table 9 \ndemonstrates the class-specific performance of the five top \nperforming algorithms for cognitive disorder classification.\n4.6.2  Deep unsupervised learning for cognitive disorder \ndiagnosis\nTable 10 presents the performance of various autoencoders \nused for classification of different cognitive impairments. \nFrom the statistics, it can be concluded that the performance \nof autoencoder-based models for cognitive disorders clas -\nsification is not satisfactory. This could be due to the lim-\nited nature of the dataset, which may not provide enough \nexamples to help autoencoders generalize well. However, \nthe SPAE performs relatively well and is competitive with \ntransfer learning models.\nFig. 15  Latent space of convo-\nlutional autoencoder\nFig. 16  Latent space of vari-\national autoencoder\nFig. 17  Latent space of overcomplete autoencoder\nFig. 18  Latent space of undercomplete autoencoder\nPattern Analysis and Applications (2024) 27:76 \n Page 23 of 35 76\n4.7  Comparison with state‑of‑the‑art approaches\nThere are various models available in the literature for clas-\nsifying AD and cognitive disorders. This subsection aims to \ncompare the proposed BiViT model with other state-of-the-\nart models in the literature, presented in Table  11.\n5  Discussion\nThis section discusses research findings as well as its limita-\ntions, applications, and future directions. The Sect. 4.5 pre-\nsents the outcomes of the suggested BiViT, autoencoders, \nand transfer learning models when applied on the AD data-\nset. Table 3 shows that BiViT outperforms all other transfer \nlearning models with 93% accuracy when data augmentation \ntechniques are applied to AD data. However, the results of \nnot applying augmentation are better than those of applying \ndata augmentation 4.\nIt is true that when augmentation techniques are used \non medical image datasets, they might not always produce \nsignificant improvements. The statement is made for multi-\nple reasons. First of all, intricate anatomical structures and \nsubtle characteristics are frequently seen in medical images, \nwhich make it difficult to enhance them without introduc-\ning distortions or unrealistic variations. Additionally, medi-\ncal image datasets are often smaller and more specialized \ncompared to general image datasets, making it challenging \nto find augmentation strategies that effectively capture the \nvariability within the dataset without introducing biases or \nartifacts. Table  5 presents the class-wise performance and \nclearly shows that, when applied to the AD dataset, BiViT \nand MobileNet yield the best performance matrices.\nTable 7  The table presents \nthe results of various transfer \nlearning methods employed \nfor the detection of cognitive \ndisorders (with the use of data \naugmentation)\nTransfer learning for cognitive disorders detection\nAlgorithm Accuracy Recall Precision F1-Score AUC-ROC\nDenseNet-121 0.35 ± 0.073 1.00 ± 0.003 0.20 ± 0.004 0.33 ± 0.005 0.55 ± 0.028\nResnet-50 0.41 ± 0.060 0.66 ± 0.078 0.29 ± 0.026 0.40 ± 0.016 0.69 ± 0.022\nVGG16 0.42 ± 0.034 0.64 ± 0.055 0.33 ± 0.032 0.44 ± 0.024 0.73 ± 0.027\nInception Resnet-V2 0.36 ± 0.060 0.99 ± 0.020 0.21 ± 0.010 0.34 ± 0.011 0.66 ± 0.027\nInception V3 0.31 ± 0.072 1.00 ± 0.005 0.20 ± 0.004 0.35 ± 0.005 0.50 ± 0.028\nEfficientNet-B0 0.42 ± 0.127 1.00 ± 0.024 0.20 ± 0.007 0.33 ± 0.008 0.68 ± 0.063\nResNet-101 0.44 ± 0.064 0.60 ± 0.097 0.35 ± 0.036 0.44 ± 0.016 0.70 ± 0.024\nVGG19 0.35 ± 0.053 0.86 ± 0.043 0.27 ± 0.017 0.42 ± 0.018 0.71 ± 0.027\nEfficientNet-B1 0.20 ± 0.123 1.00 ± 0.008 0.20 ± 0.05 0.33 ± 0.006 0.63 ± 0.049\nXception 0.43 ± 0.062 1.00 ± 0.009 0.21 ± 0.006 0.35 ± 0.007 0.64 ± 0.031\nMobileNet 0.45 ± 0.062 0.92 ± 0.029 0.24 ± 0.010 0.38 ± 0.012 0.69 ± 0.032\nResNet-152 0.44 ± 0.033 0.61 ± 0.088 0.35 ± 0.036 0.44 ± 0.016 0.69 ± 0.013\nProposed BiViT 0.45 ± 0.014 0.69 ± 0.020 0.33 ± 0.018 0.46 ± 0.023 0.73 ± 0.011\nTable 8  The table presents \nthe results of various transfer \nlearning methods employed \nfor the detection of cognitive \ndisorders (without data \naugmentation)\nTransfer learning for cognitive disorders detection (without data augmentation)\nAlgorithm Accuracy Recall Precision F1-Score AUC-ROC\nDenseNet-121 0.42 ± 0.047 0.90 ± 0.027 0.24 ± 0.010 0.38 ± 0.011 0.71 ± 0.031\nResnet-50 0.44 ± 0.030 0.86 ± 0.046 0.24 ± 0.014 0.37 ± 0.016 0.71 ± 0.018\nVGG16 0.45 ± 0.040 0.82 ± 0.052 0.29 ± 0.022 0.43 ± 0.021 0.74 ± 0.018\nInception Resnet-V2 0.39 ± 0.039 0.93 ± 0.019 0.23 ± 0.006 0.37 ± 0.008 0.69 ± 0.018\nInception V3 0.40 ± 0.032 0.99 ± 0.003 0.20 ± 0.001 0.34 ± 0.001 0.60 ± 0.022\nEfficientNet-B0 0.13 ± 0.135 1.00 ± 0.014 0.20 ± 0.010 0.33 ± 0.013 0.60 ± 0.069\nResNet-101 0.40 ± 0.021 0.69 ± 0.076 0.31 ± 0.032 0.43 ± 0.014 0.71 ± 0.014\nVGG19 0.43 ± 0.036 0.41 ± 0.068 0.45 ± 0.039 0.43 ± 0.026 0.75 ± 0.023\nEfficientNet-B1 0.43 ± 0.132 1.00 ± 0.007 0.20 ± 0.005 0.33 ± 0.007 0.62 ± 0.061\nXception 0.42 ± 0.031 0.82 ± 0.042 0.27 ± 0.016 0.41 ± 0.017 0.73 ± 0.025\nMobileNet 0.50 ± 0.024 0.92 ± 0.015 0.24 ± 0.005 0.38 ± 0.006 0.75 ± 0.016\nResNet-152 0.44 ± 0.027 0.53 ± 0.061 0.40 ± 0.033 0.46 ± 0.019 0.72 ± 0.016\nProposed BiViT 0.41 ± 0.031 0.62 ± 0.064 0.32 ± 0.023 0.43 ± 0.019 0.69 ± 0.021\n Pattern Analysis and Applications (2024) 27:76\n76 Page 24 of 35\nAlthough MobileNet has demonstrated its effectiveness \nin AD classification, our transformer-based BiViT model \npresents a strong substitute. Although CNNs are the foun-\ndation of MobileNet and are widely recognized for their \neffectiveness, they are naturally limited in their ability to \ncapture long-range dependencies and maintain spatial rela-\ntionships across images. Transformers, on the other hand, \nexcel in these areas as well, using self-attention mechanisms \nto identify complex spatial patterns that are essential for \nprecise diagnosis. Furthermore, because of their intricate \nhierarchical architectures, CNNs may be difficult to inter -\npret, which makes it difficult to comprehend model deci-\nsions-a crucial component in medical applications. Trans -\nformers, on the other hand, provide improved interpretability \nand transparency in decision-making because of their atten-\ntion mechanisms.\nFollowing that, the results of autoencoder-based models \nare displayed in Table 6, which also demonstrates how well \nthese models work when used with the AD dataset. Autoen-\ncoders attain high accuracy at the expense of precision and \nTable 9  Class-wise performance of best 4 models for cognitive disorders detection\nAlgorithms Alzheimer stages With augmentation Without augmentation\nAccuracy Recall Precision Accuracy Recall Precision F1-Score\nBiViT Alzheimer disease 0.44 0.34 0.43 0.41 0.19 0.33 0.24\nCognitive normal 0.44 0.67 0.46 0.41 0.66 0.46 0.54\nEarly-mild cognitive impairment 0.44 0.28 0.39 0.41 0.19 0.36 0.25\nLate-mild cognitive impairment 0.44 0.04 0.50 0.41 0.20 0.19 0.20\nMild cognitive impairment 0.44 0.29 0.41 0.41 0.27 0.34 0.30\nWeighted average 0.44 0.44 0.43 0.41 0.41 0.39 0.38\nMobileNet Alzheimer disease 0.45 0.31 0.44 0.50 0.31 0.48 0.38\nCognitive normal 0.45 0.76 0.49 0.50 0.76 0.51 0.61\nEarly-mild cognitive impairment 0.45 0.40 0.37 0.50 0.35 0.51 0.41\nLate-mild cognitive impairment 0.45 0.00 0.00 0.50 0.15 0.75 0.25\nMild cognitive impairment 0.45 0.06 0.40 0.50 0.27 0.40 0.32\nWeighted average 0.45 0.45 0.41 0.50 0.50 0.50 0.47\nVGG-16 Alzheimer disease 0.42 0.43 0.35 0.45 0.25 0.37 0.30\nCognitive normal 0.42 0.82 0.46 0.45 0.58 0.59 0.58\nEarly-mild cognitive impairment 0.42 0.01 0.50 0.45 0.51 0.32 0.39\nLate-mild cognitive impairment 0.42 0.04 0.50 0.45 0.30 0.29 0.29\nMild cognitive impairment 0.42 0.06 0.19 0.45 0.26 0.43 0.32\nWeighted average 0.42 0.42 0.41 0.45 0.45 0.46 0.44\nDenseNet-121 Alzheimer disease 0.35 0.16 0.62 0.42 0.23 0.33 0.27\nCognitive normal 0.35 0.90 0.93 0.42 0.66 0.48 0.56\nEarly-mild cognitive impairment 0.35 0.89 0.26 0.42 0.25 0.38 0.30\nLate-mild cognitive impairment 0.35 0.04 1.00 0.42 0.10 0.17 0.12\nMild cognitive impairment 0.35 0.08 0.33 0.42 0.27 0.32 0.29\nWeighted average 0.35 0.35 0.50 0.42 0.42 0.39 0.40\nTable 10  The comparison table \npresents various autoencoder \nmodels (an unsupervised \nlearning technique) and their \nperformance evaluation for the \ndetection of cognitive disorders \n(without data augmentation)\nComparison of unsupervised learning for cognitive disorders detection\nAlgorithm Accuracy Recall Precision F1-Score AUC AE(mae)\nConvolutional AE 0.4242 0.4470 0.3978 0.4355 0.6577 0.086\nVariational AE 0.4470 0.6616 0.2495 0.3701 0.6166 0.167\nSparse AE 0.4596 0.6818 0.2695 0.3938 0.7047 0.076\nUnderComplete AE 0.4520 0.8131 0.2822 0.4200 0.7256 0.090\nOverComplete AE 0.2601 0.6869 0.2477 0.3659 0.6030 0.1164\nPattern Analysis and Applications (2024) 27:76 \n Page 25 of 35 76\nF1-score. The same transfer learning algorithms, autoen-\ncoder, and suggested BiViT are then applied to cognitive \ndisorder data with and without augmentation in Sect.  4.6. \nAll models and the suggested BiviT-perform the worst in \nboth augmentation and non-augmentation scenarios. Very \nlittle data and an uneven distribution of data are the causes \nof this, as was previously discussed in the sections above.\nFinally, our suggested model shows up as a strong com-\npetitor with competitive performance metrics in AD classi-\nfication when compared to state-of-the-art architectures, as \nevidenced by its effectiveness in the literature. Our model \nis positioned as an AD detection system that operates in \nreal-time and can be easily integrated with the internet of \nthings (IoT), making it adaptable to a range of healthcare \nenvironments. There are many uses for the BiViT model, \nespecially in hospitals where accurate and timely diagnosis \nis critical. It’s crucial to recognise some inherent limita-\ntions in the suggested BiViT model, though. Its training is \nprimarily based on a single AD dataset, which means that \nit needs to be adjusted for use in various hospital settings. \nThis emphasises how crucial it is to increase the size of \nthe dataset in order to improve model performance, since \nlarger datasets make it easier to extract more useful and \nsubtle features. Although the model performs well on the \ncurrent dataset, because it was trained on a small amount \nof data and requires domain-specific knowledge, its gener -\nalizability to other datasets may need careful fine-tuning. \nGiven these challenges, scientists can now improve the \nfunctionality and applicability of this model in a range of \nclinical settings.\n6  Conclusion\nThe early-stage detection of AD and different cognitive \ndeclines are crucial for the patient’s health. To address \nthis, a computer-aided diagnosis system is proposed that \nutilizes 2D-MRI images to detect different cognitive dis-\norders including AD by incorporating PCES and MLF \nmechanisms. In contrast to the simple ViT, which utilizes \nonly a single patching mechanism, the proposed BiViT \nemploys two parallel coupled encoding mechanisms \nincluding simple patch encoding and image tokeniza-\ntion, to encode information more efficiently. A compara-\ntive analysis is presented using two different datasets of \nAlzheimer disease and cognitive disorders. Various state-\nof-the-art models from literature including transfer learn -\ning and autoencoder-based models are included in study \nto compete with the proposed algorithm. The accuracy, \nrecall, precision, F1-score, and AUC for the AD clas-\nsification task are reported as 96.38%, 97.87%, 88.28%, \n92.84%, and 99.47%, respectively. For cognitive disor -\nders, the proposed BiViT algorithm achieved an accuracy, \nrecall, precision, F1-score, and AUC of 44.94%, 68.69%, \n33.29%, 45.38%, and 72.62%, respectively. It means that, \nthe proposed algorithm performed well in the case of AD \nclassification but had lower performance for cognitive dis-\norders due to limited and imbalanced data. Therfore, to \nimprove the model’s performance, incorporating diverse \nand representative datasets is recommended. Additionally, \nintegrating techniques for explainability and interpretabil -\nity would enhance transparency in the model’s decision-\nmaking process.\nTable 11  The presented \ntable illustrates a comparison \nbetween the proposed BiViT \nmodel and other state-of-the-art \nmodels available in the existing \nliterature\nModel-Name Accuracy Recall Precision F1-score AUC \n[36] ADDTLA 91.70% 93.70% 91.50% 92.50% –\n[99] Hybrid CNN 90.00% 90.00% 90.00% 90.00% 90%\n[82] DenseNet121 96.59% 97.25% 97.25% 97.50% –\n[46] CNN 92.78% 90.78% – 94.00% –\n[82] ResNet50 93.52% 92% 95% 93.75% –\n[1] Modified AlexNet 95.70 92.30% 91.90% 94.70% –\n[82] VGG19 95.08% 93.00% 96.50% 94.75% –\n[28] SqueezeNet+LSTM 87.50% – – – –\n[82] Xception 89.77% 88.25% 92.00% 89.50% –\n[5] CNN 0.97% – – – –\n[82] EfficientNetB7 83.20% 68.25% 87% 73.5% –\n[4] EfficientNetV2B1 90.37% 89.76% – 90.06% –\n[82] EfficientNetB7 83.20% 68.25% 87.00% 73.5% –\n[69] PLF-VIT 81.25% – – – –\n[100] SMIL-DeiT 93.20% – – – –\n[25] ViT 87.5% – – 84.00% –\nProposed BiViT 96.38 97.87 88.28 92.84 99.47\n Pattern Analysis and Applications (2024) 27:76\n76 Page 26 of 35\nAppendix A: Visualizing patching \nand tokenization\nThe conversion of images into a sequence of tokens in ViT \nis referred to as \"patch encoding\". Patch encoding includes \nthe division of the input image into non-overlapping patches, \nwhich are then linearly projected into a lower-dimensional \nspace to generate a set of embeddings for each patch. Next, \npositional embeddings are added to the resulting embed-\ndings. The patch embeddings, along with the positional \nembeddings, are then fed into the transformer encoder. The \ndivision of images into patches is illustrated in Fig. 19. \nTokenization in CCT involves the conversion of image \npatches into a fixed-size sequence of tokens. To achieve this, \nthe patch embeddings undergo several convolutional layers, \nproducing feature maps that are then flattened into a token \nsequence. The transformed tokens are then fed into the trans-\nformer encoder for further processing. The visual represen-\ntations in Fig.  20 depicts how images are transformed into \ntokens of a fixed size.\nFig. 19  Images into patches\nFig. 20  Visualization of images \nconverted into fixed-size tokens \nfor better understanding\nTable 12  Comparative analysis on binary classification in case of AD dataset (in this we take two classes at a time)\nBinary class-wise comparative analysis\nNo Classes Accuracy Recall Precision F1-Score Top-5 AUC \n1 Non-Vs-Mild 0.9497 ± 0.078 0.9329 ± 0.074 0.9553 ± 0.095 0.9421 ± 0.077 1.000 ± 0.000 0.9871 ± 0.043\n2 Mild-Vs-Moderate 0.9902 ± 0.012 0.9805 ± 0.014 0.9926 ± 0.009 0.9866 ± 0.011 1.000 ± 0.000 0.9996 ± 0.024\n3 Mild-Vs-Very Mild 0.6981 ± 0.011 0.7689 ± 0.076 0.6626 ± 0.075 0.7128 ± 0.035 1.000 ± 0.000 0.7875 ± 0.021\n4 Very Mild-Vs-Moderate 0.9967 ± 0.003 0.9976 ± 0.002 0.9967 ± 0.069 0.9967 ± 0.046 1.000 ± 0.000 0.9998 ± 0.002\nPattern Analysis and Applications (2024) 27:76 \n Page 27 of 35 76\nTable 13  Comparative analysis on binary classification in case of cognitive disease dataset (in this we take two classes at a time)\nBinary class-wise comparative analysis\nNo Classes Accuracy Recall Precision F1-Score Top-5 AUC \n1 Alzheimer vs CN 0.7530 ± 0.046 0.0080 ± 0.000 1.00 ± 0.000 0.0152 ± 0.000 1.000 ± 0.000 0.8001 ± 0.047\n2 Alzheimer vs EMCI 0.5714 ± 0.047 0.7329 ± 0.089 0.5463 ± 0.025 0.6336 ± 0.029 1.000 ± 0.000 0.6015 ± 0.049\n3 Alzheimer vs LMCI 0.7460 ± 0.134 0.8413 ± 0.062 0.5955 ± 0.062 0.6961 ± 0.029 1.000 ± 0.000 0.7567 ± 0.142\n4 Alzheimer vs MCI 0.5161 ± 0.046 0.0806 ± 0.019 1.000 ± 0.3847 0.1435 ± 0.036 1.000 ± 0.000 0.5857 ± 0.043\nFig. 21  Feature distribution \nvisualization after feature reduc-\ntion using TSNE algorithm \n(Non vs Mild)\nFig. 22  Feature distribution \nvisualization after feature reduc-\ntion using TSNE algorithm \n(Moderate vs Mild)\nFig. 23  Feature distribution \nvisualization after feature reduc-\ntion using TSNE algorithm \n(Moderate vs Very-Mild)\n Pattern Analysis and Applications (2024) 27:76\n76 Page 28 of 35\nAppendix B: Ablation studies\nWe conducted an ablation study to assess the effectiveness \nof our proposed model, BiViT. The study was performed \non two datasets: the AD dataset and the Cognitive disorders \ndataset. In addition, we conducted a comparative analysis by \ntraining several transfer learning algorithms and autoencod-\ners on the same datasets. Our results indicate that the pro-\nposed BiViT model achieved a very satisfying performance.\nTo gain further insights into the performance of our \nmodel, we conducted experiments in which we tested the \nBiViT algorithm on two classes at a time and reported \nthe results in the “Appendix C”. We found that in some \ncases of binary classification, it was relatively easy to \ndraw a decision line, while in other cases, it was more \nchallenging. These findings suggest that while our model \ncan be successfully applied to a range of image classifi-\ncation tasks, the complexity of the task may impact its \nperformance.\nAppendix C: Comparative analysis (binary \nclassification)\nIn the following “Appendix” section, we provide a com-\nparative analysis of the BiViT algorithm using the AD and \ncognitive disorders datasets. Specifically, we evaluate the \nalgorithm’s performance in a binary class-wise manner, by \nselecting two classes at a time, training the BiViT model and \ncomparing the results. The binary class-wise classification \nresults for both datasets are presented in Tables  12 and 13, \nrespectively.\nIn the following paragraphs we describe about the TSNE-\ntransformed features distributions and the relation scatter \nplot just after projecting the features from n-dimension to \n2-dimesional space. Apart from this, We apply LASSO \nregression to determine some of the important features only \nand afterwards evaluate them. Firstly, the visual representa-\ntions of the TSNE-transformed feature distribution for Alz-\nheimer stages classification are depicted in Figs. 21, 23, 22, \nand 24. These features are derived from BiViT after training \non Alzheimer’s stage data. Subsequently, we employ TSNE \ntransformation to map these high-dimensional features into \na two-dimensional space. Analyzing these features provides \ninsight into how the model makes decisions internally. The \nhistogram plot of t-SNE transformed BiViT features pro-\nvides insights into the distribution of data points in the \nFig. 24  Feature distribution \nvisualization after feature reduc-\ntion using TSNE algorithm \n(Very-Mild vs Mild)\nFig. 25  Visualizing relation \nbetween TSNE transformed \nfeatures after feature reduction \n(Non vs Mild)\nPattern Analysis and Applications (2024) 27:76 \n Page 29 of 35 76\nFig. 26  Visualizing relation \nbetween TSNE transformed \nfeatures after feature reduction \n(Moderate vs Mild)\nFig. 27  Visualizing relation \nbetween TSNE transformed \nfeatures after feature reduction \n(Moderate vs Very-Mild)\nFig. 28  Visualizing relation \nbetween TSNE transformed \nfeatures after feature reduction \n(Very-Mild vs Mild)\nFig. 29  Feature distribution \nvisualization (Non vs Mild)\n Pattern Analysis and Applications (2024) 27:76\n76 Page 30 of 35\nFig. 30  Feature distribution \nvisualization of only highly \nimportant features (Moderate \nvs Mild)\nFig. 31  Feature distribution \nvisualization of only highly \nimportant features (Moderate vs \nVery-Mild)\nFig. 32  Feature distribution \nvisualization of only highly \nimportant features (Very-Mild \nvs Mild)\nFig. 33  Feature distribution visualization after feature reduction using \nTSNE algorithm (Alzheimer vs Normal)\nFig. 34  Feature distribution visualization after feature reduction using \nTSNE algorithm (Alzheimer vs EMCI)\nPattern Analysis and Applications (2024) 27:76 \n Page 31 of 35 76\nreduced-dimensional space. Each bin in the histogram rep-\nresents the number of data points that fall within a specific \nrange of values along the axes of the t-SNE plot. In this \ncase, a peak of the histogram highlights the high density \nof data points, which is clustered together in the original \nhigh-dimensional space. Whereas, peaks or high points of \nthe histogram denote areas with higher data concentration, \nlow points indicate the low density zones. Whether it’s the \ndistribution of data points, or the correlation with their origi-\nnal features, the analysis of the tangled features involves the \nprocess of examining how each point correlates with the \nother. Data points that have close relationships or are highly \ndistingushing are bound to be clustered based on their loca-\ntion on the t-SNE plot. Through the histogram comparison \nwith the standard features, you can get an understanding of \nwhich ones are considered as the most ordered or significant \nfor the data distributing or classifying task.\nFollowing that, the generate of a scatter plot of the TSNE-\ntransformed features helps to understand the relationships \nand gain the insight into the presence of clusters within \nthe data that tells how the data is grouped in general. The \nrelationship between the features after undergoing feature \nreduction with TSNE, specifically in the context of Alzhei-\nmer stages, is illustrated in Figs.  25, 26, 27, and 28. We \nFig. 35  Feature distribution visualization after feature reduction using \nTSNE algorithm (Alzheimer vs LMCI)\nFig. 36  Feature distribution visualization after feature reduction using \nTSNE algorithm (Alzheimer vs MCI)\nFig. 37  Visualizing relation between TSNE transformed features after \nfeature reduction (Alzheimer vs Normal)\nFig. 38  Visualizing relation between TSNE transformed features after \nfeature reduction (Alzheimer vs EMCI)\nFig. 39  Visualizing relation between TSNE transformed features after \nfeature reduction (Alzheimer vs LMCI)\nFig. 40  Visualizing relation between TSNE transformed features after \nfeature reduction (Alzheimer vs MCI)\n Pattern Analysis and Applications (2024) 27:76\n76 Page 32 of 35\nfurther analyze feature importance using Lasso regression, \na method that helps identify the most influential features. By \nplotting the histogram of the top N important features, we \ngain insights into their distribution. Figures  29, 30, 31, and \n32 illustrate the distribution of the most significant features \nin AD as determined by Lasso Regression.\nLastly, we display the TSNE-transformed features using \nhistograms and the relationship plots within the context of \nthe cognitive disorder. Figures  33, 34, 35 and 36 demon-\nstrate that TSNE features are being used in the classification \nof cognitive disorders. Further, as depicted in the Figs.  37, \n38, 39, and 40, the relationship between these features is \nplotted after TSNE reduces dimensionality. TSNE transfor-\nmation is crucial for visualizing feature relationships in scat-\nter plots, particularly when dealing with high-dimensional \ndata. It projects features into two dimensions, enhancing \ninterpretability and facilitating a clearer understanding of \nfeature interactions. \nAcknowledgements The authors would like to express their grate-\nful to Edinburgh Napier University and the Princess Nourah bint \nAbdulrahman University Researchers Supporting Project number \n(PNURSP2024R104), Princess Nourah bint Abdulrahman University, \nRiyadh, Saudi Arabia\nFunding  This research was funded by Princess Nourah bint \nAbdulrahman University Researchers Supporting Project number \n(PNURSP2024R104), Princess Nourah bint Abdulrahman University, \nRiyadh, Saudi Arabia.\nAvailability of data and materials In this study, two datasets were used \nfor classification of AD. The first dataset was obtained from [https://  \nwww. kaggle. com/ datas ets/ touri st55/ alzhe imers- datas et-4- class- of- \nimages] and consists of AD images. The second dataset includes mul-\ntiple cognitive disorders, including AD, and was obtained from [https:// \nwww. kaggle. com/ datas ets/ madhu charan/ alzhe imers disea se5cl assda \ntaset adni]. The sources cited as [54] and [18] offer different information \nabout the two datasets being discussed. According to [54], the main \nobjective of creating the Alzheimer’s dataset was to design a precise \nframework or architecture for AD classification. On the other hand, \n[18] obtained the cognitive disorders dataset from the ADNI website \nand acknowledged the ADNI for its creation. The dataset consists of \npatients data that aims to accelerate Alzheimer’s research.\nCode availability The source code for the experiments conducted in \nthis research is available at the following GitHub repository: [https://  \ngithub. com/ Hassa nshah 531/ Compu ter- Aided- Diagn osis- of- Alzhe imer-\ns- Disea se- cogni tive- Disor ders].\nDeclarations \nConflict of interest The authors declare no Conflict of interest associ-\nated with this work.\nEthics approval Not applicable.\nConsent to participate Not applicable.\nConsent for publication Not applicable.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n 1. Acharya H, Mehta R, Singh DK (2021) Alzheimer disease clas-\nsification using transfer learning. In: 2021 5th international \nconference on computing methodologies and communication \n(ICCMC). IEEE, pp 1503–1508\n 2. ADNI (2017) Adni | alzheimer’s disease neuroimaging initiative. \nhttps:// adni. loni. usc. edu/\n 3. Ahmad MF, Akbar S, Hassan SAE, Rehman A, Ayesha N (2021) \nDeep learning approach to diagnose alzheimer’s disease through \nmagnetic resonance images. In: 2021 international conference on \ninnovative computing (ICIC). IEEE, pp 1–6\n 4. Almufareh MF, Tehsin S, Humayun M, Kausar S (2023) Artifi-\ncial cognition for detection of mental disability: a vision trans-\nformer approach for Alzheimer’s disease. In: Healthcare, MDPI, \np 2763\n 5. Alshammari M, Mezher M (2021) A modified convolutional \nneural networks for mri-based images for detection and stage \nclassification of alzheimer disease. In: 2021 National computing \ncolleges conference (NCCC). IEEE, pp 1–7\n 6. Amini M, Pedram M, Moradi A, Ouchani M (2021) Diagnosis \nof Alzheimer’s disease severity with FMRI images using robust \nmultitask feature extraction method and convolutional neural \nnetwork (CNN). Comput Math Methods Med 2021:1–15\n 7. An N, Ding H, Yang J, Au R, Ang TF (2020) Deep ensemble \nlearning for Alzheimer’s disease classification. J Biomed Inform \n105:103411\n 8. Arevalo-Rodriguez I, Smailagic N, i Figuls MR, Ciapponi A, \nSanchez-Perez E, Giannakou A, Pedraza OL, Cosp XB, Cullum \nS (2015) Mini-mental state examination (mmse) for the detection \nof alzheimer’s disease and other dementias in people with mild \ncognitive impairment (MCI). Cochrane database of systematic \nreviews\n 9. Association A et al (2009) 2009 Alzheimer’s disease facts and \nfigures. Alzheimer’s Dementia 5:234–270\n 10. Association A et al (2010) 2010 Alzheimer’s disease facts and \nfigures. Alzheimer’s Dementia 6:158–194\n 11. Association A et al (2013) 2013 Alzheimer’s disease facts and \nfigures. Alzheimer’s Dementia 9:208–245\n 12. Association A et al (2014) 2014 Alzheimer’s disease facts and \nfigures. Alzheimer’s Dementia 10:e47–e92\n 13. Ballard C, Gauthier S, Corbett A, Brayne C, Aarsland D, Jones \nE (2011) Alzheimer’s disease. The Lancet 377:1019–1031\n 14. Bank D, Koenigstein N, Giryes R (2020) Autoencoders. arXiv \npreprint arXiv: 2003. 05991\n 15. Baydargil HB, Park J, Ince IF (2024) Anomaly-based alzheimer’s \ndisease detection using entropy-based probability positron emis-\nsion tomography images. ETRI J 46(3):513–525\nPattern Analysis and Applications (2024) 27:76 \n Page 33 of 35 76\n 16. Beheshti I, Demirel H, Matsuda H, Initiative ADN et al (2017) \nClassification of Alzheimer’s disease and prediction of mild \ncognitive impairment-to-Alzheimer’s conversion from structural \nmagnetic resource imaging using feature ranking and a genetic \nalgorithm. Comput Biol Med 83:109–119\n 17. Blauwendraat C, Nalls MA, Singleton AB (2020) The genetic \narchitecture of Parkinson’s disease. Lancet Neurol 19:170–178\n 18. Charan M (2021) Alzheimers-disease-5-class-dataset-adni. \nhttps:// www. kaggle. com/ datas ets/ madhu charan/ alzhe imers disea \nse5cl assda taset adni\n 19. Chollet F (2017) Xception: deep learning with depthwise sepa-\nrable convolutions. In: Proceedings of the IEEE conference on \ncomputer vision and pattern recognition, pp 1251–1258\n 20. Cockrell JR, Folstein MF (2002) Mini-mental state exami-\nnation. Principles and practice of geriatric psychiatry, pp \n140–141\n 21. Daliri MR (2012) Automated diagnosis of Alzheimer disease \nusing the scale-invariant feature transforms in magnetic reso-\nnance images. J Med Syst 36:995–1000\n 22. Davatzikos C, Fan Y, Wu X, Shen D, Resnick SM (2008) Detec-\ntion of prodromal Alzheimer’s disease via pattern classification \nof magnetic resonance imaging. Neurobiol Aging 29:514–523\n 23. Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009) Ima-\ngenet: a large-scale hierarchical image database. In: 2009 IEEE \nconference on computer vision and pattern recognition. IEEE, \npp 248–255\n 24. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, \nUnterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, \net al (2020) An image is worth 16x16 words: transformers for \nimage recognition at scale. arXiv preprint arXiv: 2010. 11929\n 25. Drewitt A (2023) An approach to classify Alzheimer’s disease \nusing vision transformers. Ph.D. thesis. Dublin, National College \nof Ireland\n 26. Dubey S (2020) Augmented Alzheimer MRI dataset. https://  \nwww. kaggle. com/ datas ets/ touri st55/ alzhe imers- datas  \net-4- class- of- images\n 27. Duyckaerts C, Delatour B, Potier MC (2009) Classification \nand basic pathology of Alzheimer disease. Acta Neuropathol \n118:5–36\n 28. Ebrahimi-Ghahnavieh A, Luo S, Chiong R (2019) Transfer \nlearning for Alzheimer’s disease detection on mri images. In: \n2019 IEEE international conference on industry 4.0, artificial \nintelligence, and communications technology (IAICT). IEEE, \npp 133–138\n 29. Ebrahimighahnavieh MA, Luo S, Chiong R (2020) Deep learn-\ning to detect Alzheimer’s disease from neuroimaging: a sys-\ntematic literature review. Comput Methods Programs Biomed \n187:105242\n 30. Ebrahimighahnavieh MA, Luo S, Chiong R (2020) Deep learn-\ning to detect Alzheimer’s disease from neuroimaging: a sys-\ntematic literature review. Comput Methods Programs Biomed \n187:105242\n 31. Ferrarini L, Frisoni GB, Pievani M, Reiber JH, Ganzola R, Milles \nJ (2009) Morphological hippocampal markers for automated \ndetection of Alzheimer’s disease and mild cognitive impairment \nconverters in magnetic resonance images. J Alzheimers Dis \n17:643–659\n 32. Folstein M, Anthony JC, Parhad I, Duffy B, Gruenberg EM \n(1985) The meaning of cognitive impairment in the elderly. J \nAm Geriatr Soc 33:228–235\n 33. Galasko D, Klauber MR, Hofstetter CR, Salmon DP, Lasker B, \nThal LJ (1990) The mini-mental state examination in the early \ndiagnosis of Alzheimer’s disease. Arch Neurol 47:49–52\n 34. Gauthier S, Reisberg B, Zaudig M, Petersen RC, Ritchie K, Bro-\nich K, Belleville S, Brodaty H, Bennett D, Chertkow H et al \n(2006) Mild cognitive impairment. The Lancet 367:1262–1270\n 35. Gauthier S, Reisberg B, Zaudig M, Petersen RC, Ritchie K, Bro-\nich K, Belleville S, Brodaty H, Bennett D, Chertkow H et al \n(2006) Mild cognitive impairment. The Lancet 367:1262–1270\n 36. Ghazal TM, Issa G (2022) Alzheimer disease detection \nempowered with transfer learning. Comput Mater Continua \n70:5005–5019\n 37. Gooblar J, Roe CM, Selsor NJ, Gabel MJ, Morris JC (2015) Atti-\ntudes of research participants and the general public regarding \ndisclosure of Alzheimer disease research results. JAMA Neurol \n72:1484–1490\n 38. Hassani A, Walton S, Shah N, Abuduweili A, Li J, Shi H \n(2021) Escaping the big data paradigm with compact trans-\nformers. arXiv preprint arXiv: 2104. 05704\n 39. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning \nfor image recognition. In: Proceedings of the IEEE conference \non computer vision and pattern recognition, pp 770–778\n 40. Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Wey-\nand T, Andreetto M, Adam H (2017) Mobilenets: efficient \nconvolutional neural networks for mobile vision applications. \narXiv preprint arXiv: 1704. 04861\n 41. Huang G, Liu Z, Van Der Maaten L, Weinberger KQ (2017) \nDensely connected convolutional networks. In: Proceedings of \nthe IEEE conference on computer vision and pattern recogni-\ntion, pp 4700–4708\n 42. Jessen F (2014) Subjective and objective cognitive decline at \nthe pre-dementia stage of Alzheimer’s disease. Eur Arch Psy -\nchiatry Clin Neurosci 264:3–7\n 43. Jo T, Nho K, Saykin AJ (2019) Deep learning in Alzheimer’s \ndisease: diagnostic classification and prognostic prediction \nusing neuroimaging data. Front Aging Neurosci 11:220\n 44. Julayanont P, Nasreddine ZS (2017) Montreal cognitive \nassessment (moca): concept and clinical review. A practical \napproach, Cognitive screening instruments, pp 139–195\n 45. Julayanont P, Nasreddine ZS (2017) Montreal cognitive \nassessment (MoCA): concept and clinical review. A practical \napproach, Cognitive screening instruments, pp 139–195\n 46. Kabir A, Kabir F, Mahmud MAH, Sinthia SA, Azam SR, Hus-\nsain E, Parvez MZ (2021) Multi-classification based Alzhei-\nmer’s disease detection with comparative analysis from brain \nMRI scans using deep learning. In: TENCON 2021-2021 IEEE \nregion 10 conference (TENCON). IEEE, pp 905–910\n 47. Kang W, Lin L, Zhang B, Shen X, Wu S, Initiative ADN et al \n(2021) Multi-model and multi-slice ensemble learning archi-\ntecture based on 2d convolutional neural networks for alzhei-\nmer’s disease diagnosis. Comput Biol Med 136:104678\n 48. Katzman R (1989) Alzheimer’s disease is a degenerative dis-\norder. Neurobiol Aging 10:581–582\n 49. Kim K, Wu B, Dai X, Zhang P, Yan Z, Vajda P, Kim SJ (2021) \nRethinking the self-attention in vision transformers. In: Pro-\nceedings of the IEEE/CVF conference on computer vision and \npattern recognition, pp 3071–3075\n 50. Kingma DP, Welling M et al (2019) An introduction to vari-\national autoencoders. Found Trends Mach Learn 12:307–392\n 51. Knopman DS, Amieva H, Petersen RC, Chételat G, Holtzman \nDM, Hyman BT, Nixon RA, Jones DT (2021) Alzheimer dis-\nease. Nat Rev Dis Primers 7:33\n 52. Kora P, Ooi CP, Faust O, Raghavendra U, Gudigar A, Chan \nWY, Meenakshi K, Swaraja K, Plawiak P, Acharya UR (2022) \nTransfer learning techniques for medical image analysis: a \nreview. Biocybern Biomed Eng 42:79–107\n 53. Korolev IO (2014) Alzheimer’s disease: a clinical and basic \nscience review. Med Student Res J 4:24–33\n 54. Kumar S, Shastri S (2022) Alzheimer mri preprocessed data-\nset. https:// www. kaggle. com/ dsv/ 33649 39, https:// doi. org/ 10. \n34740/ KAGGLE/ DSV/ 33649 39\n Pattern Analysis and Applications (2024) 27:76\n76 Page 34 of 35\n 55. Kurlowicz L, Wallace M (1999) The mini-mental state exami-\nnation (MMSE). J Gerontol Nurs 25(5):8–9\n 56. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, \nHuang Z et al (2015) Imagenet large scale visual recognition \nchallenge. Int J Comput Vision 115:211–252\n 57. Lazli L (2022) Machine learning classifiers based on dimen-\nsionality reduction techniques for the early diagnosis of \nAlzheimer’s disease using magnetic resonance imaging and \npositron emission tomography brain data. Computational \nintelligence methods for bioinformatics and biostatistics: 17th \ninternational meeting, CIBB 2021, Virtual Event, November \n15–17, 2021. Springer, Revised Selected Papers, pp 117–131\n 58. Lee ES, Yoo K, Lee YB, Chung J, Lim JE, Yoon B, Jeong Y \n(2016) Default mode network functional connectivity in early \nand late mild cognitive impairment. Alzheimer Disease Assoc \nDisord 30:289–296\n 59. Liu M, Li F, Yan H, Wang K, Ma Y, Shen L, Xu M, Initiative \nADN et al (2020) A multi-model deep convolutional neural net-\nwork for automatic hippocampus segmentation and classification \nin alzheimer’s disease. Neuroimage 208:116459\n 60. Liu W, Wang Z, Liu X, Zeng N, Liu Y, Alsaadi FE (2017) A sur-\nvey of deep neural network architectures and their applications. \nNeurocomputing 234:11–26\n 61. Liu Z, Lu H, Pan X, Xu M, Lan R, Luo X (2022) Diagnosis of \nAlzheimer’s disease via an attention-based multi-scale convolu-\ntional neural network. Knowl-Based Syst 238:107942\n 62. Martin-Khan M, Flicker L, Wootton R, Loh PK, Edwards H, \nVarghese P, Byrne GJ, Klein K, Gray LC (2012) The diagnostic \naccuracy of telegeriatrics for the diagnosis of dementia via video \nconferencing. J Am Med Dir Assoc 13:487-e19\n 63. McKhann GM, Knopman DS, Chertkow H, Hyman BT, Jack \nCR Jr, Kawas CH, Klunk WE, Koroshetz WJ, Manly JJ, Mayeux \nR et al (2011) The diagnosis of dementia due to Alzheimer’s \ndisease: recommendations from the national institute on aging-\nAlzheimer’s association workgroups on diagnostic guidelines for \nAlzheimer’s disease. Alzheimer’s Dementia 7:263–269\n 64. Mirzaei G, Adeli H (2022) Machine learning techniques for diag-\nnosis of Alzheimer disease, mild cognitive disorder, and other \ntypes of dementia. Biomed Signal Process Control 72:103293\n 65. Mirzaei G, Adeli H (2022) Machine learning techniques for diag-\nnosis of Alzheimer disease, mild cognitive disorder, and other \ntypes of dementia. Biomed Signal Process Control 72:103293\n 66. Nasreddine ZS, Phillips NA, Bédirian V, Charbonneau S, White-\nhead V, Collin I, Cummings JL, Chertkow H (2005) The mon-\ntreal cognitive assessment, MoCA: a brief screening tool for mild \ncognitive impairment. J Am Geriatr Soc 53:695–699\n 67. Newcombe EA, Camats-Perna J, Silva ML, Valmas N, Huat TJ, \nMedeiros R (2018) Inflammation: the link between comorbidi -\nties, genetics, and Alzheimer’s disease. J Neuroinflammation \n15:1–26\n 68. Ng A, et al (2011) Sparse autoencoder. CS294A Lecture Notes \n72:1–19\n 69. Odusami M, Maskeliūnas R, Damaševičius R (2023) Pixel-level \nfusion approach with vision transformer for early detection of \nAlzheimer’s disease. Electronics 12:1218\n 70. Organization WH (2023) Dementia. https:// www. who. int/ news- \nroom/ fact- sheets/ detail/ demen tia\n 71. Pak M, Kim S (2017) A review of deep learning in image recog-\nnition. In: 2017 4th international conference on computer appli-\ncations and information processing technology (CAIPT). IEEE, \npp 1–3\n 72. Papazacharias A, Nardini M (2012) He relationship between \ndepression and cognitive deficits. Psychiatr Danub 24:179–182\n 73. Petersen RC, Roberts RO, Knopman DS, Boeve BF, Geda YE, \nIvnik RJ, Smith GE, Jack CR (2009) Mild cognitive impairment: \nten years later. Arch Neurol 66:1447–1455\n 74. Raghavaiah P, Varadarajan S (2021) Novel deep learning convo-\nlution technique for recognition of Alzheimer’s disease. Mater \nToday Proc 46:4095–4098\n 75. Rose VL (1998) Alzheimer’s disease genetic fact sheet. Am Fam \nPhysician 58:578\n 76. Sandler M, Howard A, Zhu M, Zhmoginov A, Chen LC (2018) \nMobilenetv2: inverted residuals and linear bottlenecks. In: Pro-\nceedings of the IEEE conference on computer vision and pattern \nrecognition, pp 4510–4520\n 77. Sarraf S, DeSouza DD, Anderson J, Tofighi G, Initiativ ADN \n(2016) Deepad: Alzheimer’s disease classification via deep con-\nvolutional neural networks using mri and fmri. BioRxiv, 070441\n 78. Schnakers C, Monti MM (2020) Towards improving care for \ndisorders of consciousness. Nat Rev Neurol 16:405–406\n 79. Sethi M, Rani S, Singh A, Mazón JLV (2022) A cad system for \nAlzheimer’s disease classification using neuroimaging MRI 2D \nslices. Comput Math Methods Med 2022(1):8680737\n 80. Shamrat FJM, Akter S, Azam S, Karim A, Ghosh P, Tasnim \nZ, Hasib KM, De Boer F, Ahmed K (2023) Alzheimernet: An \neffective deep learning based proposition for Alzheimer’s disease \nstages classification from functional brain changes in magnetic \nresonance images. IEEE Access 11:16376–16395\n 81. Shen X, Finn ES, Scheinost D, Rosenberg MD, Chun MM, \nPapademetris X, Constable RT (2017) Using connectome-based \npredictive modeling to predict individual behavior from brain \nconnectivity. Nature Protocols 12:506–518\n 82. Sisodia PS, Ameta GK, Kumar Y, Chaplot N (2023) A review \nof deep transfer learning approaches for class-wise prediction of \nAlzheimer’s disease using MRI images. Arch Comput Methods \nEng, pp. 1–21\n 83. Sona A, Ellis KA, Ames D (2013) Rapid cognitive decline in \nAlzheimer’s disease: a literature review. Int Rev Psychiatry \n25:650–658\n 84. Sorour SE, Abd El-Mageed AA, Albarrak KM, Alnaim AK, \nWafa AA, El-Shafeiy E (2024) Classification of alzheimer’s dis-\nease using MRI data based on deep learning techniques. J King \nSaud Univ-Comput Inf Sci 101940\n 85. Sun QS, Zeng SG, Liu Y, Heng PA, Xia DS (2005) A new \nmethod of feature fusion and its application in image recogni-\ntion. Pattern Recogn 38:2437–2448\n 86. Szegedy C, Ioffe S, Vanhoucke V, Alemi A (2017) Inception-\nv4, inception-resnet and the impact of residual connections on \nlearning. In: Proceedings of the AAAI conference on artificial \nintelligence\n 87. Taheri Gorji H, Kaabouch N (2019) A deep learning approach for \ndiagnosis of mild cognitive impairment based on MRI images. \nBrain Sci 9:217\n 88. Takemori Y, Sasayama D, Toida Y, Kotagiri M, Sugiyama N, \nYamaguchi M, Washizuka S, Honda H (2021) Possible utilization \nof salivary ifn-/u1D6FE/il-4 ratio as a marker of chronic stress in healthy \nindividuals. Neuropsychopharmacol Rep 41:65–72\n 89. Tan CC, Eswaran C (2008) Performance comparison of three \ntypes of autoencoder neural networks. In: 2008 second asia inter-\nnational conference on modelling & simulation (AMS). IEEE, \npp 213–218\n 90. Tan M, Le Q (2019) Efficientnet: rethinking model scaling for \nconvolutional neural networks. In: International conference on \nmachine learning. PMLR, pp 6105–6114\n 91. Tang W, Sun J, Wang S, Zhang Y (2023) Review of alexnet for \nmedical image classification. arXiv preprint arXiv: 2311. 08655\n 92. Tang-Wai DF, Knopman DS, Geda YE, Edland SD, Smith GE, \nIvnik RJ, Tangalos EG, Boeve BF, Petersen RC (2003) Com-\nparison of the short test of mental status and the mini-mental \nstate examination in mild cognitive impairment. Arch Neurol \n60:1777–1781\nPattern Analysis and Applications (2024) 27:76 \n Page 35 of 35 76\n 93. Tanveer M, Richhariya B, Khan RU, Rashid AH, Khanna P, \nPrasad M, Lin C (2020) Machine learning techniques for the \ndiagnosis of Alzheimer’s disease: a review. ACM Trans Mul-\ntimed Comput Commun Appl 16:1–35\n 94. Tsoi KK, Chan JY, Hirai HW, Wong SY, Kwok TC (2015) Cog-\nnitive tests to detect dementia: a systematic review and meta-\nanalysis. JAMA Intern Med 175:1450–1458\n 95. Uraninjo (2022) Alzheimer’s dataset (4 class of images). https:// \nwww. kaggle. com/ datas ets/ urani njo/ augme nted- alzhe imer- mri- \ndatas et\n 96. Venugopalan J, Tong L, Hassanzadeh HR, Wang MD (2021) \nMultimodal deep learning models for early detection of Alzhei-\nmer’s disease stage. Sci Rep 11:1–13\n 97. de Vico Fallani F, Richiardi J, Chavez M, Achard S (2014) \nGraph analysis of functional brain networks: practical issues \nin translational neuroscience. Philos Trans R Soc B Biol Sci \n369:20130521\n 98. Wang J, Zhu H, Wang SH, Zhang YD (2021) A review of deep \nlearning on medical image analysis. Mob Netw Appl 26:351–380\n 99. Yildirim M, Cinar A (2020) Classification of Alzheimer’s dis-\nease MRI images with CNN based hybrid method. Ingénierie des \nSystèmes d Inf. 25:413–418\n 100. Yin Y, Jin W, Bai J, Liu R, Zhen H (2022) Smil-deit: multiple \ninstance learning and self-supervised vision transformer network \nfor early Alzheimer’s disease classification. In: 2022 interna-\ntional joint conference on neural networks (IJCNN). IEEE, pp \n1–6\n 101. Zhang Y (2018) A better autoencoder for image: convolutional \nautoencoder. In: ICONIP17-DCEC. Available online: http://  \nusers. cecs. anu. edu. au/ Tom. Gedeon/ conf/ ABCs2 018/ paper/ \nABCs2 018_ paper_ 58. Accessed 23 Mar 2017\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Neurocognitive",
  "concepts": [
    {
      "name": "Neurocognitive",
      "score": 0.703171968460083
    },
    {
      "name": "Computer science",
      "score": 0.6426563262939453
    },
    {
      "name": "Cognition",
      "score": 0.5960421562194824
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5370985269546509
    },
    {
      "name": "Autoencoder",
      "score": 0.44440630078315735
    },
    {
      "name": "Deep learning",
      "score": 0.41346535086631775
    },
    {
      "name": "Machine learning",
      "score": 0.3992442488670349
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.330077588558197
    },
    {
      "name": "Psychology",
      "score": 0.1958197057247162
    },
    {
      "name": "Neuroscience",
      "score": 0.17321962118148804
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16076960",
      "name": "COMSATS University Islamabad",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I83202590",
      "name": "Jeju National University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I251738",
      "name": "Edinburgh Napier University",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I106778892",
      "name": "Princess Nourah bint Abdulrahman University",
      "country": "SA"
    }
  ]
}