{
  "title": "Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer",
  "url": "https://openalex.org/W3166362606",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2059017717",
      "name": "Li Yulin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106958442",
      "name": "He Jianfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A20934661",
      "name": "Zhang TianZhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2038294650",
      "name": "Liu Xiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1897470950",
      "name": "Zhang Yongdong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108221232",
      "name": "Wu Feng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035539956",
    "https://openalex.org/W2967515867",
    "https://openalex.org/W2963322158",
    "https://openalex.org/W2981393440",
    "https://openalex.org/W2963365374",
    "https://openalex.org/W3100506510",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963330186",
    "https://openalex.org/W2960065339",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W2784746431",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2963438548",
    "https://openalex.org/W2962926870",
    "https://openalex.org/W2954451719",
    "https://openalex.org/W2585635281",
    "https://openalex.org/W2531440880",
    "https://openalex.org/W2798775284",
    "https://openalex.org/W2895844008",
    "https://openalex.org/W1949591461",
    "https://openalex.org/W2895786668",
    "https://openalex.org/W2964201641",
    "https://openalex.org/W2962691289",
    "https://openalex.org/W3034519219",
    "https://openalex.org/W2068042582",
    "https://openalex.org/W2963078173",
    "https://openalex.org/W2998776895",
    "https://openalex.org/W2963910742",
    "https://openalex.org/W2988964414",
    "https://openalex.org/W2980073905",
    "https://openalex.org/W2220271458",
    "https://openalex.org/W2798590501",
    "https://openalex.org/W2890159224",
    "https://openalex.org/W166429404",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3097870364",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2897450186",
    "https://openalex.org/W2784375960",
    "https://openalex.org/W3006129144",
    "https://openalex.org/W1009458120",
    "https://openalex.org/W2963805953",
    "https://openalex.org/W2990317318",
    "https://openalex.org/W2985033611",
    "https://openalex.org/W2598634450",
    "https://openalex.org/W2511791013",
    "https://openalex.org/W3115484111",
    "https://openalex.org/W3080818206",
    "https://openalex.org/W2089074647"
  ],
  "abstract": "Occluded person re-identification (Re-ID) is a challenging task as persons are frequently occluded by various obstacles or other persons, especially in the crowd scenario. To address these issues, we propose a novel end-to-end Part-Aware Transformer (PAT) for occluded person Re-ID through diverse part discovery via a transformer encoderdecoder architecture, including a pixel context based transformer encoder and a part prototype based transformer decoder. The proposed PAT model enjoys several merits. First, to the best of our knowledge, this is the first work to exploit the transformer encoder-decoder architecture for occluded person Re-ID in a unified deep model. Second, to learn part prototypes well with only identity labels, we design two effective mechanisms including part diversity and part discriminability. Consequently, we can achieve diverse part discovery for occluded person Re-ID in a weakly supervised manner. Extensive experimental results on six challenging benchmarks for three tasks (occluded, partial and holistic Re-ID) demonstrate that our proposed PAT performs favorably against stat-of-the-art methods.",
  "full_text": "Diverse Part Discovery: Occluded Person Re-identiÔ¨Åcation with\nPart-Aware Transformer\nYulin Li1*, Jianfeng He1‚àó, Tianzhu Zhang1‚Ä†, Xiang Liu2, Yongdong Zhang1, Feng Wu1\n1 University of Science and Technology of China 2 Dongguan University of Technology\n{liyulin, hejf}@mail.ustc.edu.cn {tzzhang, fengwu, zhyd73}@ustc.edu.cn\nsucceedpkmba2011@163.com\nAbstract\nOccluded person re-identiÔ¨Åcation (Re-ID) is a challeng-\ning task as persons are frequently occluded by various ob-\nstacles or other persons, especially in the crowd scenario.\nTo address these issues, we propose a novel end-to-end\nPart-Aware Transformer (PAT) for occluded person Re-ID\nthrough diverse part discovery via a transformer encoder-\ndecoder architecture, including a pixel context based trans-\nformer encoder and a part prototype based transformer de-\ncoder. The proposed PAT model enjoys several merits. First,\nto the best of our knowledge, this is the Ô¨Årst work to exploit\nthe transformer encoder-decoder architecture for occluded\nperson Re-ID in a uniÔ¨Åed deep model. Second, to learn part\nprototypes well with only identity labels, we design two ef-\nfective mechanisms including part diversity and part dis-\ncriminability. Consequently, we can achieve diverse part\ndiscovery for occluded person Re-ID in a weakly supervised\nmanner. Extensive experimental results on six challenging\nbenchmarks for three tasks (occluded, partial and holistic\nRe-ID) demonstrate that our proposed PAT performs favor-\nably against stat-of-the-art methods.\n1. Introduction\nPerson re-identiÔ¨Åcation (Re-ID) aims to match images of\na person captured from non-overlapping camera views [7,\n44, 54]. It is one of the most important research topics in\nthe computer vision Ô¨Åeld with various applications, such as\nvideo surveillance, autonomous driving, and activity analy-\nsis [47, 23, 56, 49, 48]. Recently, person Re-ID has drawn\na growing amount of interest from academia and industry,\nand various methods have been proposed [20, 24, 14, 38, 18,\n27, 42, 43]. Most of these approaches assume that the entire\nbody of the pedestrian is available for the Re-ID model de-\nsigning. However, when conducting person Re-ID in real-\nworld scenarios, e.g., airports, railway stations, hospitals,\n*Equal contribution\n‚Ä†Corresponding author\n(a) \nHand\n-\nCraft \nS\nplitting\n(b) \nHuman \nP\narsing\n(d) \nOurs\n(c) \nAttention\nFigure 1. Examples of part-based methods for occluded person\nRe-ID. (a) The hand-crafted splitting based methods require strict\nperson alignment. (b) The extra semantic based methods exclude\nthe personal belongings and are error-prone when a person is se-\nriously occluded. (c) The attention-based methods tend to mainly\nfocus on the most discriminative region. (d) The attention maps\nproduced by our proposed PAT by fusing all part-aware masks.\nThe discriminative parts are highlighted.\nand malls, it is difÔ¨Åcult to satisfy this assumption due to the\ninevitable occlusions. For example, a person may be oc-\ncluded by some obstacles (e.g., cars, trees, walls, and other\npersons), and the camera fails to capture the holistic per-\nson. Therefore, it is essential to design an effective model\nto solve this occluded person Re-ID problem [61, 30].\nIn the occluded person Re-ID task, occluded regions usu-\nally contain some noise that results in mismatching, and\nthe key issue is how to learn discriminative features from\nunoccluded regions. Recently, leveraging local features\nextracted from human body parts to improve representa-\ntions of the pedestrian has been the mainstream for robust\nfeature learning of the occluded Re-ID task. Generally,\nthese part-based occluded Re-ID methods can be divided\ninto three main categories. (1) The hand-crafted splitting\nbased methods divide the image or feature map into small\npatches [10, 57, 12] or rigid stripes [38, 4] and then ex-\ntract part features from the local patches or stripes. How-\never, hand-crafted splitting is too coarse to align the human\nparts well and introduces lots of background noise. (2) The\narXiv:2106.04095v1  [cs.CV]  8 Jun 2021\nextra semantic based methods [13, 30, 40, 5, 11] directly\nutilize human parsing [13, 11] or pose estimation mod-\nels [30, 40, 5] as part localization modules to achieve more\naccurate human part localization. However, their success\nheavily relies on the accuracy of the off-the-shelf human\nparsing or pose estimation models. Since there exist dif-\nferences between training datasets of human parsing/pose\nestimation and person Re-ID, the off-the-shelf human pars-\ning/pose estimation models are error-prone when pedestri-\nans are seriously occluded. (3) The attention based meth-\nods [37, 62] exploit attention mechanisms to localize dis-\ncriminative human parts. Typically, the predicted attention\nmaps distribute most of the attention weights on human\nparts, which can help decrease the negative effect of clut-\ntered background. To sum up, most existing occluded Re-\nID methods focus on locating discriminative human parts\nand leveraging local part features to develop powerful rep-\nresentations of the pedestrian.\nBased on the above discussions, the part-based represen-\ntations have been proven to be effective for the occluded\nRe-ID problem. To capture accurate human parts, an intu-\nitive idea is to detect non-occluded body parts using body\npart detectors and then match the corresponding body parts.\nHowever, there are no extra annotations for the body de-\ntector learning. Thus, we propose to localize discrimina-\ntive human parts only with identity labels. To achieve this\ngoal, there are two main challenges as follows. On the one\nhand, background with diverse characteristics, such as col-\nors, sizes, shapes, and positions, increase the difÔ¨Åculty of\ngetting robust features for the target person. Intuitively,\nthe appearance of pixels of the same human part region is\nsimilar, while quite different from the background pixels.\nTherefore, it is necessary to model the correlation between\npixels for robust feature representation. On the other hand,\nas shown in Figure 1, the occluded parts vary between dif-\nferent pedestrian images. As there are no groundtruth an-\nnotations for human parts, it is difÔ¨Åcult to cope with diverse\nappearance of pedestrians and adaptively locate all unoc-\ncluded parts only with the identity labels. As a result, as\nshown in Figure 1 (c), most of the attention based methods\ntend to put the main focus on the most discriminative re-\ngion. They always ignore other human parts including per-\nsonal belongings, e.g., backpack and reticule, which also\nprovide important clues for person Re-ID.\nTo deal with the above issues, we propose a novel\nPart-Aware Transformer (PAT) for occluded person Re-ID\nthrough diverse part discovery via a transformer encoder-\ndecoder architecture [39, 2], including a pixel context based\ntransformer encoder and a part prototype based transformer\ndecoder. In the pixel context based transformer encoder,\nwe adopt a self-attention mechanism to capture the full im-\nage context information. SpeciÔ¨Åcally, we model the corre-\nlation of pixels of the feature map and aggregate pixels with\nsimilar appearances. In this way, we can obtain the pixel\ncontext aware feature map, which is more robust to back-\nground clutters. In the part prototype based transformer\ndecoder, we introduce a set of learnable part prototypes to\ngenerate part-aware masks focusing on discriminative hu-\nman parts. In speciÔ¨Åc, given the feature map of a pedestrian,\nwe take the learnable part prototypes as queries and pixels\nof the feature map as keys and values of the transformer\ndecoder. We can obtain part-aware masks by calculating\nthe similarity between all pixels in the feature map and part\nprototypes. Each part-aware mask is expected to denote the\nspatial distribution of one speciÔ¨Åc human part, e.g., head or\nbody part. With part-aware masks, human part features can\nbe further obtained from the values by a weighted pooling.\nHowever, without the assistance of part annotations, it is\nchallenging to constraint these part prototypes to capture ac-\ncurate human parts. Thus, to guide part prototype learning,\nwe propose two mechanisms including part diversity and\npart discriminability. Intuitively, different part features of\nthe same pedestrian should focus on different human parts.\nTherefore, the part diversity mechanism is adopted to en-\ncourage lower correlation between part features and make\npart prototypes focus on different discriminative foreground\nregions. The part discriminability mechanism is to make\npart features maintain identity discriminative via part clas-\nsiÔ¨Åcation and a triplet loss. By optimizing the transformer\nencoder and decoder jointly, part prototypes can be learned\nthrough the whole dataset. Consequently, we can achieve\nrobust human part discovery for occluded person Re-ID in\na weakly supervised manner.\nThe contributions of our method could be summarized\ninto three-fold: (1) We propose a novel end-to-end Part-\nAware Transformer for occluded person Re-ID through di-\nverse part discovery via a transformer encoder-decoder ar-\nchitecture, including a pixel context based transformer en-\ncoder and a part prototype based transformer decoder. To\nthe best of our knowledge, our PAT is the Ô¨Årst work by\nexploiting the transformer encoder-decoder architecture for\noccluded person Re-ID in a uniÔ¨Åed deep model. (2) To learn\npart prototypes only with identity labels well, we design two\neffective mechanisms, including part diversity and part dis-\ncriminability. Consequently, we can achieve robust human\npart discovery for occluded person Re-ID in a weakly su-\npervised manner. (3) To demonstrate the effectiveness of\nour method, we perform experiments on three tasks, includ-\ning occluded Re-ID, partial Re-ID and holistic Re-ID on\nsix standard Re-ID datasets. Extensive experimental results\ndemonstrate that the proposed method performs favorably\nagainst state-of-the-art methods.\n2. Related Work\nIn this section, we brieÔ¨Çy overview methods that are re-\nlated to holistic person Re-ID, partial Re-ID and occluded\nperson Re-ID respectively.\nHolistic Person Re-IdentiÔ¨Åcation. Person re-identiÔ¨Åcation\n(Re-ID) aims to match images of a person captured from\nnon-overlapping camera views [7, 44, 54]. Existing Re-\nID methods can be summarized to hand-crafted descrip-\ntors [47, 23], metric learning methods [56, 20, 24] and deep\nlearning methods[38, 25, 33, 35, 45, 52, 19, 21, 34, 26, 22].\nRecent works utilizing part-based features have achieved\nstate-of-the-art performance for the holistic person Re-ID\ntask. Kalayeh et al. [19] extract several region parts with\nhuman parsing methods and assemble Ô¨Ånal discriminative\nrepresentations with part-level features. Sun et al. [38] uni-\nformly partition the feature map and learn part-level fea-\ntures by multiple classiÔ¨Åers. Zhao et al . [51] and Liu et\nal. [26] extract part-level features by attention-based meth-\nods. But all these Re-ID methods focus on matching holis-\ntic person images with the assumption that the entire body\nof the pedestrian is available. Different from these methods,\nour model can adaptively capture discriminative human part\nfeatures via a transformer encoder-decoder architecture for\nthe occluded person Re-ID task.\nPartial Person Re-IdentiÔ¨Åcation. Partial person Re-ID\naims to match partial probe images to holistic gallery im-\nages. Zheng et al . [57] propose a local-level match-\ning model called Ambiguity-sensitive Matching ClassiÔ¨Åer\n(AMC) based on the dictionary learning and introduce\na local-to-global matching model called Sliding Window\nMatching to provide complementary spatial layout infor-\nmation. He et al. [10] propose an alignment-free approach\nnamely Deep Spatial feature Reconsruction (DSR) that ex-\nploits the reconstruction error based on sparse coding. Luo\net al. et al . [29] proposed STNReID that combines a spa-\ntial transformer network (STN) and a Re-ID network for\npartial Re-ID. Sun et al. [37] introduce a Visibility-aware\nPart Model (VPM) to perceive the visibility of part regions\nthrough self-supervision. However, all these methods need\na manual crop of the occluded target person in the probe im-\nage and then use the non-occluded parts as the new query.\nThe manual cropping is not efÔ¨Åcient in practice and might\nintroduce human bias to the cropped results.\nOccluded Person Re-IdentiÔ¨Åcation. Given occluded\nprobe images, occluded person Re-ID aims to Ô¨Ånd the same\nperson with holistic or occluded appearance in disjoint cam-\neras. This task is more challenging due to incomplete infor-\nmation and spatial misalignment. Zhuo et al. [61] combine\nthe occluded/unoccluded classiÔ¨Åcation task and person ID\nclassiÔ¨Åcation task to extract key information from images.\nHe et al . [13] reconstruct the feature map of unoccluded\nregions and propose a spatial background-foreground clas-\nsiÔ¨Åer to avoid the inÔ¨Çuence of background clutters. Be-\nsides, the Pose-Guided Feature Aligment (FGFA) [30] uti-\nlizes pose landmarks to mine discriminative parts to ad-\ndress the occlusion noise. Gao et al. [5] propose a Pose-\nguided Visible Part Matching (PVPM) model to learn dis-\ncriminative part features with pose-guided attentions. Wang\net al. [40] exploit graph convolutional layers to learn high-\norder human part relations for robust alignment. Although\nthe above methods can solve the occlusion problem to some\nextent, most of them heavily rely on off-the-shelf human\nparsing models or pose estimators. Different from them,\nour model can exploit diverse parts with only identity labels\nin a weakly supervised manner via a transformer encoder-\ndecoder architecture.\n3. Part-Aware Transformer\nIn this section, we introduce the proposed Part-Aware\nTransformer (PAT) in detail. As shown in Figure 2, the pro-\nposed PAT mainly consists of two modules, including the\npixel context based transformer encoder and the part proto-\ntype based transformer decoder. Here we give a brief intro-\nduction to the full process. First, we obtain the feature map\nof each pedestrian image through a CNN backbone. Then\nwe Ô¨Çatten the feature map and carry out the self-attention\noperation to obtain the pixel context aware feature map with\nthe transformer encoder. After obtaining the pixel context\naware feature map, we calculate the similarity between the\nfeature map and a set of learnable part prototypes to obtain\npart-aware masks. Part features can be further obtained by\na weighted pooling where part-aware masks are treated as\ndifferent spatial attention maps. Finally, we introduce the\npart diversity mechanism and part discriminability mecha-\nnism to learn part prototypes well with only identity labels.\n3.1. Pixel Context based Transformer Encoder\nBackground regions with diverse characteristics increase\nthe difÔ¨Åculty of getting robust features for the target person.\nTherefore, we adopt a self-attention mechanism to capture\nthe full image context information. In this way, we can ob-\ntain the pixel context aware feature map, which is more ro-\nbust to background clutters. Following [38], our method\nuses ResNet-50 [9] without the average pooling layer and\nfully connected layer as the backbone to extract global fea-\nture maps from given images. We also set the stride of\nconv4 1 to 1 to increase the feature resolution as in [38]. As\na result, an input image with a size of H √óW will get the\nfeature map with the spatial dimension of H/16 √óW/16,\nwhich is larger than that of the original ResNet-50. A larger\nfeature map has been proved to be effective in person Re-\nID. Formally, the feature map extracted from the backbone\nis denoted as Z ‚ààRh√ów√óc, where h,w,c are the height,\nwidth and channel of the global feature map, respectively.\nWe Ô¨Årst utilize a 1 √ó1 convolution to reduce the chan-\nnel dimension of the feature map Z to a smaller dimension\nd, creating a new feature map F ‚ààRh√ów√ód. The trans-\nformer encoder requires a 1D sequence as input. To han-\ndle 2D feature maps, we Ô¨Çatten the spatial dimensions of\nF into one dimension, resulting in a hw √ód feature. In\nBackbone\nSelf\n-\nattention\nùêæ\nPart Prototypes\n1\n√ó\n1\nConv\nSelf\n-\nattention\n‚Ä¶\nWeighted\nPooling\n‚Ä¶\n‚Ä¶\nùêæ\nPart\n-\naware \nM\nasks\nùëì\nùëñ\n‚Ä¶\nùëù\nùëñ\nùëéùë°ùë°\nùëñ\n=\n1\nùêæ\nùëù\nùëñ\nùëñ\n=\n1\nùêæ\nùëì\n1\nùëùùëéùëüùë°\nùëì\n2\nùëùùëéùëüùë°\nùëì\nùêæ\nùëùùëéùëüùë°\nùëì\nùëî\nùêæ\nPart \nF\neatures\nGAP\nPixel Context based Transformer \nEncoder\nPart Prototype based Transformer\nDecoder\nFFN\nFFN\nùëì\nùëó\nùëì\nùëñ\nùëéùë°ùë°\nùëì\nùëó\nùëéùë°ùë°\nCross\n-\na\nttention\nClassification\nLoss\nTriplet\nLoss\nDiversity\nLoss\nClassification\nLoss\nTriplet\nLoss\nùëë\nùë§\n‚Ñé\nFigure 2. The pipeline of the proposed PAT consists of a pixel context based transformer encoder and a part prototype based transformer\ndecoder. Here, ‚Äúself-attention‚Äù denotes the self-attention layer, ‚Äúcross-attention‚Äù denotes the cross-attention layer, and ‚ÄúFFN‚Äù denotes the\nfeed forward layer. For more details, please refer to the text and please see supplemental materials for detailed architecture.\nthe self-attention mechanism, given the feature map F =\n[f1; f2; ... ; fhw] (fi ‚ààR1√ód indicates the feature of the ith\nspatial position), both keys, queries and values arise from\npixels of the feature map. Formally,\nQi = fiWQ, Kj = fjWK, Vj = fjWV , (1)\nwhere i,j ‚àà 1,2,...,hw and WQ ‚àà Rd√ódk ,WK ‚àà\nRd√ódk ,WV ‚ààRd√ódv are linear projections. For the ith\nquery Qi, the attention weights are calculated based on the\ndot-product similarity between each query and the keys:\nsi,j = exp (Œ≤i,j)\n‚àëhw\nj=1 exp (Œ≤i,j)\n,Œ≤i,j = QiKT\nj‚àödk\n, (2)\nwhere ‚àödk is a scaling factor. The output of the self-\nattention mechanism is deÔ¨Åned as weighted sum over all\nvalues according to the attention weights:\nÀÜfatt\ni = Att (Qi,K,V) =\nhw‚àë\nj=1\nsi,jVj, (3)\nThe normalized attention weightsi,j models the interdepen-\ndency between different spatial pixels fi and fj, and the\nweight sum of the values can aggregate these semantically\nrelated spatial pixels to update fi. Since pixels of the same\nhuman part have high similarities while are different from\nbackground pixels, the feature map capturing the full-image\ncontext information would be more robust to background\nclutters. We implement Eq.(3) with the multi-head atten-\ntion mechanism and can get ÀÜfatt\ni ‚àà R1√ód. The updated\nfeature map can be obtained by aggregating pixel context\ninformation of all positions:\nÀÜFatt =\n[\nÀÜfatt\n1 ; ÀÜfatt\n2 ; ... ; ÀÜfatt\nhw\n]\n‚ààRhw√ód, (4)\nwhere ÀÜFatt represents the updated feature map with the self-\nattention mechanism. Following the standard transformer\narchitecture, we use the feed-forward network to produce\nthe Ô¨Ånal pixel context aware feature map as deÔ¨Åned in:\nFatt = FFN(ÀÜFatt), (5)\nwhere FFN(¬∑) is a simple neural network using two fully\nconnected layers [39]. The residual connections followed\nby the layer normalization [1] are also applied. Please see\nsupplemental materials for more details about the architec-\nture. Through the self-attention operation, the pixel context\naware feature map Fatt = [fatt\n1 ; fatt\n2 ; ... ; fatt\nhw ] ‚ààRhw√ód\ncan be obtained, which is more robust to background clutter.\nEncoder Training Loss. To make the pixel context aware\nfeature focus on ID-related discriminative information and\ntrain our encoder, we use an identity classiÔ¨Åcation loss and\na triplet loss as the objective function. First of all, we uti-\nlize a global average pooling operation fg = GAP(Fatt)\nand constrain the global feature fg ‚ààR1√ód to satisfy the\nobjective function. The objective function is formulated as:\nLEn = ŒªclsLcls (fg) +ŒªtriLtri (fg)\n= ‚àíŒªcls log pg + Œªtri\n[\nŒ±+ dfg,fg\np ‚àídfg,fg\nn\n]\n+\n. (6)\nWhere pg is the probability predicted by a classiÔ¨Åer, Œ± is\nthe margin, dfg,fg\np is the distance between a positive pair(\nfg,fg\np\n)\nfrom the same identity, and(fg,fg\nn) is the negative\npair from different identities.\n3.2. Part Prototype based Transformer Decoder\nIn the part prototype based transformer decoder, to local-\nize discriminative human parts only with identity labels, we\nintroduce a set of learnable part prototypes focusing on dis-\ncriminative human parts and propose two mechanisms, in-\ncluding part diversity and part discriminability to guide part\nprototype learning only with identity labes. In this way, we\ncan achieve robust human part discovery in a weakly super-\nvised manner. First of all, we introduce a set of part proto-\ntypes PK = {pi}K\ni=1 ,pi ‚ààR1√ód represents a part classiÔ¨Åer\nthat determines whether pixels of the feature map Fatt be-\nlong to the part i. These part prototypes are set as learnable\nparameters.\nSelf-attention Layer. Following the standard architecture\nof the transformer, we Ô¨Årst use a self-attention mechanism\nto further incorporate the local context of human parts to\npart prototypes. This process allows the local context infor-\nmation propagation between prototypes during part proto-\ntype learning. The implementation is the same as in Sec-\ntion 3.1, and both keys, queries and values arise from part\nprototypes. We can obtain the updated part prototype set\n{patt\ni }K\ni=1. The weights of self-attention encode the rela-\ntions between part prototypes pi and pj. The updated part\nprototypes incorporate the local context of different parts.\nCross-attention Layer. The cross-attention layer aims\nto extract foreground part features from the feature map\nFatt with the learnable part prototypes. As shown in Fig-\nure 2, in the cross-attention layer, given the feature map\nFatt = [fatt\n1 ; fatt\n2 ; ... ; fatt\nhw ], queries arise from part pro-\ntotypes {patt\ni }K\ni=1, keys and values arise from pixels of the\nfeature map. Formally,\nQi = patt\ni WQ, Kj = fatt\nj WK, Vj = fatt\nj WV , (7)\nwhere i ‚àà 1,2,...,K , j ‚àà 1,2,...,hw , and WQ ‚àà\nRd√ódk ,WK ‚àà Rd√ódk ,WV ‚àà Rd√ódv are linear projec-\ntions. Note that they are different from Eq.(3). For each\npart prototype patt\ni , we illustrate how to compute the part-\naware mask and the corresponding part feature. Formally,\nmi,j = exp (Œ≤i,j)\n‚àëhw\nj=1 exp (Œ≤i,j)\n,Œ≤i,j = QiKT\nj‚àödk\n, (8)\nwhere ‚àödk is a scaling factor. The attention weight mi,j\nindicates the probability of the spatial feature fatt\nj be-\nlonging to the foreground part i. The attention weights\nof all hw positions make up a part-aware mask Mi =\n[mi,1; mi,2; ... ; mi,hw], which has high response values at\npixels belonging to the part i. We can further obtain ith\npart feature by a weighted pooling, which is deÔ¨Åned as the\nweighted sum over all values:\nÀÜfpart\ni = Att (Qi,K,V) =\nhw‚àë\nj=1\nmi,jVj, (9)\nBy computing over all part prototypes, we can obtain K\npart-aware masks (each mask is a h√ówattention map) and\nfurther obtain K part features, as shown in Figure 2. We\nimplement Eq.(9) with the multi-head attention mechanism\nand can get ÀÜfpart\ni ‚ààR1√ód. Then, two fully-connected lay-\ners are adopted, which is the same as the standard trans-\nformer architecture. The Ô¨Ånal part feature is formulated as:\nfpart\ni = FFN(ÀÜfpart\ni ), (10)\nwhere i ‚àà 1,2,...,K and FFN(¬∑) denotes the feed-\nforward network as in Eq.(5).\nSince there are no human part annotations, part prototype\nlearning tends to focus on the same discriminative part (e.g.,\nthe body), which may result in a suboptimal solution. Thus,\nto learn part prototypes only with identity labes, we pro-\npose two mechanisms including part diversity and part dis-\ncriminability. (1) The part diversity mechanism is to make\npart prototypes focus on different discriminative foreground\nparts. A diversity loss is imposed to expand the discrepancy\namong different part features\n{\nfpart\ni\n}K\ni=1:\nLdiv = 1\nK(K‚àí1)\nK‚àë\ni=1,j=1\nK‚àë\niÃ∏=j\n‚ü®\nfpart\ni ,fpart\nj\n‚ü©\nÓµπÓµπfpart\ni\nÓµπÓµπ\n2\nÓµπÓµπfpart\nj\nÓµπÓµπ\n2\n, (11)\nThe intuition behind this loss is obvious. If the ith and the\njth prototypes give a high attention weight to the same fore-\nground part, the Ldiv will be large and prompt these proto-\ntypes to adjust themselves adaptively. (2) The part discrim-\ninability mechanism is to make part features maintain iden-\ntity discriminative. The part classiÔ¨Åcation and triplet loss\nare employed to guide part feature representation learning\nas in Eq.(12), where the deÔ¨Ånitions of Lcls(¬∑) and Ltri(¬∑)\ncan be found in Eq.(6).\nLdis = Œªcls\nK‚àë\ni=1\nLcls\n(\nfpart\ni\n)\n+ Œªtri\nK‚àë\ni=1\nLtri\n(\nfpart\ni\n)\n.\n(12)\nIn the triplet loss, part features fpart\ni from different identi-\nties form negative pairs, and those from the same identity\nform positive pairs. As a result, the features obtained from\nthe same prototype with different identities are pushed away\nand the identity discriminative part features can be obtained.\n3.3. Training and Inference\nFor the occluded person Re-ID task, our proposed PAT\nis trained by minimizing the overall objective with identity\nlabels as deÔ¨Åned in Eq.(13).\nLPAT = LEn + Ldiv + Ldis, (13)\nDuring the testing stage, for each image of an unseen iden-\ntity, we concatenate the global feature fg and part features{\nfpart\ni\n}K\ni=1 as its representation:\nv=\n[\nfg,fpart\n1 ,¬∑¬∑¬∑ ,fpart\nK\n]\n. (14)\nwhere [¬∑] denotes a concatenation operation.\n4. Experiments\nIn this section, we Ô¨Årst verify the effectiveness of our\nproposed model for occluded person Re-ID, partial Re-ID,\nand holistic Re-ID. Then, we report a set of ablation studies\nto validate the effectiveness of each component. Finally, we\nprovide more visualization results.\n4.1. Datastes and Evaluation Metrics\nTo demonstrate the effectiveness of our method, we\nconduct extensive experiments on two occluded datasets:\nOccluded-Duke [30] and Occluded REID [30], two par-\ntial Re-ID datasets: Partial-REID [57] and Partial-iLIDS\n[55], and two holistic Re-ID datasets: Market-1501 [53] and\nDukeMTMC-reID [32, 58]. The details are as follows.\nOccluded-Duke [30] contains 15,618 training images,\n17,661 gallery images, and 2,210 occulded query images.\nIt is selected from DukeMTMC-reID by leaving occluded\nimages and Ô¨Åltering out some overlap images.\nOccluded-REID [61] is an occluded person dataset cap-\ntured by mobile cameras, including 2,000 images belong-\ning to 200 identities. Each identity has Ô¨Åve full-body per-\nson images and Ô¨Åve occluded person images with different\nviewpoints and different types of severe occlusions.\nPartial-REID [57] is a specially designed partial person\nRe-ID benchmark that includes 600 images from 60 peo-\nple, with Ô¨Åve full-body images in gallery set and Ô¨Åve partial\nimages in query set per person.\nPartial-iLIDS [10] is a partial person Re-ID dataset based\non the iLIDS dataset [55], and contains a total of 238 im-\nages from 119 people captured by multiple cameras in the\nairport, and their occluded regions are manually cropped.\nMarket-1501 [53] consists of 1,501 identities captured by\n6 cameras. The training set consists of 12,936 images of\n751 identities, the query set consists of 3,368 images, and\nthe gallery set consists of 19,732 images.\nDukeMTMC-reID [32, 58] contains 36,411 images of\n1,404 identities captured by 8 cameras. The training set\ncontains 16,522 images, the query set consists of 2,228 im-\nages and the gallery set consists of 17,661 images.\nEvaluation Metrics. We adpot standard metrics as in\nmost person Re-ID literature, namely Cumulative Matching\nCharacteristic (CMC) curves and mean average precision\n(mAP), to evaluate the quality of different Re-ID models.\n4.2. Implementation Details\nWe adopt ResNet-50 [9] pretrained on ImageNet as our\nbackbone by removing the global average pooling (GAP)\nlayer and fully connected layer. For classiÔ¨Åers, as in [28] we\nuse a batch normalization layer [17] and a fully connected\nlayer followed by a softmax function. The number of part\nprototypes K is set to 6 on Market-1501, and set to 14 on\nall other datasets. The images are resized to 256 √ó128\nand augmented with random horizontal Ô¨Çipping, padding\nTable 1. Performance comparison with state-of-the-arts on\nOccluded-Duke and Occluded-REID. Our method achieves the\nbest performance on the two occluded datasets.\nMethods Occluded-Duke Occluded-REID\nRank-1 mAP Rank-1 mAP\nPart-Aligned [51] 28.8 20.2 - -\nPCB [38] 42.6 33.7 41.3 38.9\nPart Bilinear [36] 36.9 - - -\nFD-GAN [6] 40.8 - - -\nAMC+SWM [57] - - 31.2 27.3\nDSR [10] 40.8 30.4 72.8 62.8\nSFR [12] 42.3 32 - -\nAd-Occluded [16] 44.5 32.2 - -\nFPR [13] - - 78.3 68.0\nPVPM [5] 47 37.7 70.4 61.2\nPGFA [30] 51.4 37.3 - -\nGASM [11] - 74.5 65.6\nHOReID [40] 55.1 43.8 80.3 70.2\nISP [60] 62.8 52.3 - -\nPAT(Ours) 64.5 53.6 81.6 72.1\n10 pixels, random cropping, and random erasing [59]. Ex-\ntra color jitter is adopted on occluded-REID and partial\ndatasets to avoid domain variance. The batch size is set to\n64 with 4 images per person. During the training stage, all\nthe modules are jointly trained for 120 epochs. The learning\nrate is initialized to 3.5 √ó10‚àí4 and decayed to its 0.1 and\n0.01 at 40 and 70 epochs.\n4.3. Comparison with State-of-the-art Methods\nResults on Occluded Re-ID Datasets. Table 1 shows\nthe performance of our model and previous methods on\ntwo occluded datasets. Four kinds of methods are com-\npared, which are hand-crafted splitting based Re-ID meth-\nods [51, 38], holistic Re-ID methds with key-point informa-\ntion [36, 6], partial ReID methods [57, 10, 12] and occluded\nReID methods [13, 5, 30, 11, 40, 60]. The Rank-1/mAP\nof our method achieves 64.5%/53.6% and 81.6%/72.1% on\nOccluded-Duke and Occluded-REID datasets, which set a\nnew SOTA performance. Compared to the hand-crafted\nsplitting based method PCB [38], our PAT surpasses it\nby +21.9% Rank-1 accuracy and +19.9% mAP on the\nOccluded-Duke dataset. This is because our PAT explic-\nitly learns part-aware masks to depress the noisy informa-\ntion from the occluded regions. It can be seen that hand-\ncrafted splitting based Re-ID methods and holistic meth-\nods with key-points information have similar performance\non two occluded datasets. For example, PCB [38] and\nFD-GAN [6] both achieve about 40% Rank-1 score on\nthe Occluded-Duke dataset, indicating that key-points in-\nformation may not signiÔ¨Åcantly beneÔ¨Åt the occluded Re-ID\ntask. Compared with PVPM and HOReID, which are SOTA\noccluded ReID methods with key-points information, our\nmethod achieves much better performance, surpassing them\nby at least +9.4% Rank-1 accuracy and +9.8% mAP on the\nTable 2. Performance comparison with state-of-the-arts on Partial-\nREID and Partial-iLIDS datasets. Our method achieves the best.\nMethods Partial-REID Partial-iLIDS\nRank-1 Rank-3 Rank-1 Rank-3\nAMC+SWM [57] 37.3 46.0 21.0 32.8\nDSR [10] 50.7 70.0 58.8 67.2\nSFR [12] 56.9 78.5 63.9 74.8\nSTNReID [29] 66.7 80.3 54.6 71.3\nVPM [37] 67.7 81.9 65.5 74.8\nPGFA [30] 68.0 80.0 69.1 80.9\nAFPB [61] 78.5 - - -\nPVPM [5] 78.3 87.7 - -\nFPR [13] 81.0 - 68.1 -\nHOReID [40] 85.3 91.0 72.6 86.4\nPAT(Ours) 88.0 92.3 76.5 88.2\nOccluded-Duke dataset. This is because their performance\nheavily relies on the accuracy of the off-the-shelf pose es-\ntimation models, while our method can capture more accu-\nrate human part information in a uniÔ¨Åed deep model. Fur-\nthermore, our PAT also outperforms the methods with the\nmask learning strategy, including GASM and ISP, which\nshows the effectiveness of our transformer encoder-decoder\narchitecture and two learning mechanisms.\nResults on Partial Datasets. To further evaluate our\nmethod, we compare the results on Partial-REID and\nPartial-iLIDS datasets with existing state-of-the-art meth-\nods. Like some previous methods [37, 13, 5, 40], since the\ntwo partial datasets are too small, we train our model on\nthe Market-1501 training set and use two partial datasets\nas test sets. Therefore, it is also a cross-domain setting.\nAs shown in Table 2, the Rank-1/Rank-3 of our method\nachieves 88.0%/92.3% and 76.5%/88.2% on Partial-REID\nand Partial-iLIDS datasets, respectively, which outperforms\nall the previous partial person Re-ID models. This suggests\nthat the proposed PAT can be solid to address the occlu-\nsion problem. Compared to the most competing method\nHOReID [40], our PAT signiÔ¨Åcantly surpasses it by+2.7%\nRank-1 accuracy on Partial-REID, while surpasses it by\n+3.9% Rank-1 accuracy on Partial-iLIDS, which demon-\nstrates the effectiveness of our proposed model.\nResults on Holistic Re-ID Datasets. We also experiment\non holistic person Re-ID datasets including Market-1501\nand DukeMTMC-reID. We compare our method with state-\nof-the-art approaches of three categories, and the results\nare shown in Table 3. The methods in the Ô¨Årst group\nare hand-crafted splitting based models. The methods in\nthe second group are attention based approaches. The\nmethods in the third group are extra semantic based meth-\nods. From the results, we can see that the proposed PAT\nachieves competitive performances with state-of-the-art on\nboth datasets. SpeciÔ¨Åcally, the Rank-1/mAP of our method\nachieves 95.4%/88.0% and 88.8%/78.2% on Market-1501\nand DukeMTMC-reID datasets, respectively. Our PAT per-\nforms better than the hand-crafted splitting based model\nTable 3. Performance comparison with state-of-the-art methods on\nMarket-1501 and DukeMTMC-reID datasets.\nMethods Market-1501 DukeMTMC-reID\nRank-1 mAP Rank-1 mAP\nPCB [38] 92.3 77.4 81.8 66.1\nBOT [28] 94.1 85.7 86.4 76.4\nMGN [41] 95.7 86.9 88.7 78.4\nVPM [37] 93.0 80.8 83.6 72.6\nIANet [15] 94.4 83.1 87.1 73.4\nCASN+PCB [31] 94.4 82.8 87.7 73.7\nCAMA [46] 94.7 84.5 85.8 72.9\nMHN-6 [3] 95.1 85.0 89.1 77.2\nSPReID [19] 92.5 81.3 84.4 71.0\nDSA-reID [50] 95.7 87.6 86.2 74.3\nP2 Net [8] 95.2 85.6 86.5 73.1\nPGFA [30] 91.2 76.8 82.6 65.5\nHOReID [40] 94.2 84.9 86.9 75.6\nFPR [13] 95.4 86.6 88.6 78.4\nPAT(Ours) 95.4 88.0 88.8 78.2\nPCB, because the hand-crafted splitting is too coarse to\nalign the human parts well. Furthermore, the proposed PAT\nis superior to those approaches with external cues. SpeciÔ¨Å-\ncally, compared to the pose-guided occluded Re-ID method\nHOReID [40], our PAT signiÔ¨Åcantly surpasses it by+3.1%\nmAP on Market-1501, while surpasses it by +2.6% mAP\non DukeMTMC-reID, which shows the effectiveness of the\nproposed part prototype learning mechanism. The extra se-\nmantic based approaches heavily rely on the external cues\nfor person alignment, but they cannot always infer the ac-\ncurate external cues in the case of severe occlusion. The\nabove results also prove that the learnable part prototypes\nare robust to different views, poses, and occlusions.\n4.4. Ablation Studies\nIn this section, we perform ablation studies on the\nOccluded-Duke dataset to analyze each component of our\nPAT, including the pixel context based transformer encoder\n(P), the self-attention layer (S) and the cross-attention layer\n(C) of the part prototype based transformer decoder and the\npart diversity mechanism ( D). Note that the part discrim-\ninability mechanism is to make part features maintain iden-\ntity discriminative, and it is the basis of our model. We\nreomve all the modules and set the ResNet-50 with the av-\nerage pooling as our baseline, where only a global feature\nis available. The results are shown in Table 4.\nEffectiveness of the Transformer Encoder. As shown in\nindex-2, compared with the baseline model, when only the\nencoder is adopted and only the global feature fg is used,\nthe performance is improved by +7.1% mAP. This is be-\ncause the self-attention mechanism of the encoder can cap-\nture the pixel context information well. From index-3 and\nindex-5, we can also see that with the encoder, the perfor-\nmance is improved by +0.8% mAP since the pixel context\naware feature is more robust to background clutters.\nTable 4. Performance comparison with different components.\nIndex P S C D R-1 R-5 R-10 mAP\n1 46.0 65.7 71.9 38.8\n2 ‚úì 58.2 75.1 80.4 45.9\n3 ‚úì ‚úì 61.9 76.9 81.8 51.7\n4 ‚úì 59.9 75.6 81.2 50.8\n5 ‚úì ‚úì ‚úì 63.1 77.5 82.1 52.5\n6 ‚úì ‚úì ‚úì ‚úì 64.5 78.3 83.4 53.6\n61\n61.2\n61.9\n63\n64.5\n62.9\n60\n61\n62\n63\n64\n65\n66\n6\n8\n10\n12\n14\n16\nRank\n-\n1 accuracy(%)\nnumber of part prototyes K\n51.6\n51.3\n52.1\n52.7\n53.6\n52.4\n51\n51.5\n52\n52.5\n53\n53.5\n54\n6\n8\n10\n12\n14\n16\nmAP(%)\nnumber of part prototyes K\nFigure 3. Comparison in Rank-1 accuracy and mAP with different\nsettings of the part prototype number K on Occluded-Duke.\n94.5\n95.4\n94.7\n94.4\n94.8\n94.7\n94\n94.4\n94.8\n95.2\n95.6\n96\n4\n6\n8\n10\n12\n14\nRank\n-\n1 accuracy(%)\nnumber of part prototyes K\n87.9\n88\n87.3\n87.5\n87.3\n87.2\n87\n87.3\n87.6\n87.9\n88.2\n88.5\n4\n6\n8\n10\n12\n14\nmAP(%)\nnumber of part prototyes K\nFigure 4. Comparison in Rank-1 accuracy and mAP with different\nsettings of the part prototype number K on Market-1501.\nEffectiveness of the Transformer Decoder. From index-\n1 and index-3, when the part prototype based decoder is\nadded, the performance is greatly improved by+12.9% and\nup to 51.7% mAP. This shows that the part-aware masks\nobtained from part prototypes are useful for reducing the\ninÔ¨Çuence of background and aligning part features. From\nindex-3 and index-4, when the self-attention layer in the\ndecoder is added, the performance is further improved by\n+0.9% mAP. This demonstrates the effectiveness of the lo-\ncal context information propagation among all prototypes.\nEffectiveness of the Part Diversity Mechanism. From\nindex-5 and index-6, we can see that our full model achieves\nthe best performance, which demonstrates the effectiveness\nof the proposed diversity loss. By adding the diversity loss,\nthe learnable part prototypes are guided to discover diverse\ndiscriminative human parts for the occluded Re-ID task.\nAnalysis of the Number of Part Prototypes. The num-\nber of part prototypes K determines the granularity of the\ndiscovered parts. We perform quantitative experiments to\nclearly Ô¨Ånd the most suitable K on Occluded-Duke and\nMarket-1501 datasets. As shown in Figure 3, with K in-\ncreases, the performance keeps improving before K ar-\nrives 14 on Occluded-Duke, while the best performance is\nachieved when K is set to 6 on Market-1501, as shown in\nFigure 4. We conclude that this is because the scenarios in\nOccluded-Duke are more complex, and more Ô¨Åne-grained\npart features would be more useful.\nImage\nPart 1\nPart 2\nFused\nPart 3\nId:\n218\nId:143\nFigure 5. Visualization of the learned part-aware masks. The Ô¨Ånal\nmask is obtained by fusing all the part-aware masks. As we can\nsee, these part-aware masks mainly focus on different discrimina-\ntive human parts, including personal belongings.\n4.5. Visualization of Discovered Parts\nWe visualize the part-aware masks generated from differ-\nent part prototypes in Figure 5. From the results, we can ob-\nserve that different part-aware masks can successfully cap-\nture diverse discriminative human parts for the same input\nimage. For example, the part-aware mask obtained by the\n1th prototype mainly focuses on the head region, and the\npart-aware mask obtained by the 2th prototype mainly fo-\ncuses on the upper body. This also shows the effectiveness\nof our proposed part diversity mechanism. The Ô¨Ånal mask in\nFigure 5 is obtained by fusing all part-aware masks together.\nWe can see that the fused masks almost span over the whole\nperson rather than overÔ¨Åt in some local regions. In this way,\nthese part-aware masks can reduce the background inter-\nference and occlusion, making the network more focus on\ndiscriminative human parts for the occluded Re-ID task.\n5. Conclusion\nIn this work, we propose a novel Part-Aware Trans-\nformer to discover diverse discriminative human parts with\na set of learnable part prototypes for occluded person Re-\nID. To learn part prototypes only with identity labels well,\nwe design two effective mechanisms, including part diver-\nsity and part discriminability, to discovery human parts in a\nweakly supervised manner. Extensive experimental results\nfor three tasks on six standard Re-ID datasets demonstrate\nthe effectiveness of the proposed method.\n6. Acknowledgment\nThis work was partially supported by the National\nKey Research and Development Program under Grant No.\n2018YFB0804204, Strategic Priority Research Program\nof Chinese Academy of Sciences (No.XDC02050500),\nNational Defense Basic ScientiÔ¨Åc Research Program\n(JCKY2020903B002), National Nature Science Founda-\ntion of China (Grant 62022078, 62021001, 62071122),\nOpen Project Program of the National Laboratory of Pattern\nRecognition (NLPR) under Grant 202000019, and Youth\nInnovation Promotion Association CAS 2018166.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In ECCV, pages\n213‚Äì229, 2020.\n[3] Binghui Chen, Weihong Deng, and Jiani Hu. Mixed high-\norder attention network for person re-identiÔ¨Åcation. InICCV,\npages 371‚Äì381, 2019.\n[4] Xing Fan, Hao Luo, Xuan Zhang, Lingxiao He, Chi Zhang,\nand Wei Jiang. Scpnet: Spatial-channel parallelism network\nfor joint holistic and partial person re-identiÔ¨Åcation. InAsian\nConference on Computer Vision , pages 19‚Äì34. Springer,\n2018.\n[5] Shang Gao, Jingya Wang, Huchuan Lu, and Zimo Liu. Pose-\nguided visible part matching for occluded person reid. In\nCVPR, pages 11744‚Äì11752, 2020.\n[6] Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Shuai Yi,\nXiaogang Wang, et al. Fd-gan: Pose-guided feature distilling\ngan for robust person re-identiÔ¨Åcation. In NeurIPS, pages\n1222‚Äì1233, 2018.\n[7] Shaogang Gong and Tao Xiang. Person re-identiÔ¨Åcation.\nIn Visual Analysis of Behaviour , pages 301‚Äì313. Springer,\n2011.\n[8] Jianyuan Guo, Yuhui Yuan, Lang Huang, Chao Zhang, Jin-\nGe Yao, and Kai Han. Beyond human parts: Dual part-\naligned representations for person re-identiÔ¨Åcation. InICCV,\npages 3642‚Äì3651, 2019.\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\npages 770‚Äì778, 2016.\n[10] Lingxiao He, Jian Liang, Haiqing Li, and Zhenan Sun.\nDeep spatial feature reconstruction for partial person re-\nidentiÔ¨Åcation: Alignment-free approach. In CVPR, pages\n7073‚Äì7082, 2018.\n[11] Lingxiao He and Wu Liu. Guided saliency feature learning\nfor person re-identiÔ¨Åcation in crowded scenes. In ECCV,\npages 357‚Äì373, 2020.\n[12] Lingxiao He, Zhenan Sun, Yuhao Zhu, and Yunbo Wang.\nRecognizing partial biometric patterns. arXiv preprint\narXiv:1810.07399, 2018.\n[13] Lingxiao He, Yinggang Wang, Wu Liu, He Zhao, Zhenan\nSun, and Jiashi Feng. Foreground-aware pyramid reconstruc-\ntion for alignment-free occluded person re-identiÔ¨Åcation. In\nICCV, pages 8450‚Äì8459, 2019.\n[14] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-\nfense of the triplet loss for person re-identiÔ¨Åcation. arXiv\npreprint arXiv:1703.07737, 2017.\n[15] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu,\nShiguang Shan, and Xilin Chen. Interaction-and-aggregation\nnetwork for person re-identiÔ¨Åcation. In CVPR, pages 9317‚Äì\n9326, 2019.\n[16] Houjing Huang, Dangwei Li, Zhang Zhang, Xiaotang Chen,\nand Kaiqi Huang. Adversarially occluded samples for person\nre-identiÔ¨Åcation. In CVPR, pages 5098‚Äì5107, 2018.\n[17] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[18] Kongzhu Jiang, Tianzhu Zhang, Yongdong Zhang, Feng Wu,\nand Yong Rui. Self-supervised agent learning for unsuper-\nvised cross-domain person re-identiÔ¨Åcation. IEEE Transac-\ntions on Image Processing, 29:8549‚Äì8560, 2020.\n[19] Mahdi M Kalayeh, Emrah Basaran, Muhittin G ¬®okmen,\nMustafa E Kamasak, and Mubarak Shah. Human semantic\nparsing for person re-identiÔ¨Åcation. In CVPR, pages 1062‚Äì\n1071, 2018.\n[20] Martin Koestinger, Martin Hirzer, Paul Wohlhart, Peter M\nRoth, and Horst Bischof. Large scale metric learning from\nequivalence constraints. In CVPR, pages 2288‚Äì2295, 2012.\n[21] Wei Li, Xiatian Zhu, and Shaogang Gong. Harmonious at-\ntention network for person re-identiÔ¨Åcation. In CVPR, pages\n2285‚Äì2294, 2018.\n[22] Yaoyu Li, Tianzhu Zhang, Lingyu Duan, and Changsheng\nXu. A uniÔ¨Åed generative adversarial framework for image\ngeneration and person re-identiÔ¨Åcation. InProceedings of the\n26th ACM international conference on Multimedia , pages\n163‚Äì172, 2018.\n[23] Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z Li. Per-\nson re-identiÔ¨Åcation by local maximal occurrence represen-\ntation and metric learning. In CVPR, pages 2197‚Äì2206,\n2015.\n[24] Shengcai Liao and Stan Z Li. EfÔ¨Åcient psd constrained asym-\nmetric metric learning for person re-identiÔ¨Åcation. In ICCV,\npages 3685‚Äì3693, 2015.\n[25] Jinxian Liu, Bingbing Ni, Yichao Yan, Peng Zhou, Shuo\nCheng, and Jianguo Hu. Pose transferrable person re-\nidentiÔ¨Åcation. In CVPR, pages 4099‚Äì4108, 2018.\n[26] Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng, Jing Shao,\nShuai Yi, Junjie Yan, and Xiaogang Wang. Hydraplus-net:\nAttentive deep features for pedestrian analysis. In ICCV,\npages 350‚Äì359, 2017.\n[27] Yan Lu, Yue Wu, Bin Liu, Tianzhu Zhang, Baopu Li, Qi Chu,\nand Nenghai Yu. Cross-modality person re-identiÔ¨Åcation\nwith shared-speciÔ¨Åc feature transfer. InCVPR, pages 13379‚Äì\n13389, 2020.\n[28] Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei\nJiang. Bag of tricks and a strong baseline for deep person\nre-identiÔ¨Åcation. In CVPR Workshops, 2019.\n[29] Hao Luo, Wei Jiang, Xing Fan, and Chi Zhang. Stnreid:\nDeep convolutional networks with pairwise spatial trans-\nformer networks for partial person re-identiÔ¨Åcation. IEEE\nTransactions on Multimedia, 2020.\n[30] Jiaxu Miao, Yu Wu, Ping Liu, Yuhang Ding, and Yi\nYang. Pose-guided feature alignment for occluded person\nre-identiÔ¨Åcation. In ICCV, pages 542‚Äì551, 2019.\n[31] Xuelin Qian, Yanwei Fu, Tao Xiang, Yu-Gang Jiang, and\nXiangyang Xue. Leader-based multi-scale attention deep ar-\nchitecture for person re-identiÔ¨Åcation. IEEE Transactions on\nPattern Analysis and Machine Intelligence , 42(2):371‚Äì385,\n2019.\n[32] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,\nand Carlo Tomasi. Performance measures and a data set for\nmulti-target, multi-camera tracking. In ECCV, pages 17‚Äì35,\n2016.\n[33] M Saquib Sarfraz, Arne Schumann, Andreas Eberle, and\nRainer Stiefelhagen. A pose-sensitive embedding for per-\nson re-identiÔ¨Åcation with expanded cross neighborhood re-\nranking. In CVPR, pages 420‚Äì429, 2018.\n[34] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang\nWang. Mask-guided contrastive attention model for person\nre-identiÔ¨Åcation. In CVPR, pages 1179‚Äì1188, 2018.\n[35] Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao,\nand Qi Tian. Pose-driven deep convolutional model for per-\nson re-identiÔ¨Åcation. In ICCV, pages 3960‚Äì3969, 2017.\n[36] Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Ky-\noung Mu Lee. Part-aligned bilinear representations for per-\nson re-identiÔ¨Åcation. In ECCV, pages 402‚Äì419, 2018.\n[37] Yifan Sun, Qin Xu, Yali Li, Chi Zhang, Yikang Li, Shengjin\nWang, and Jian Sun. Perceive where to focus: Learn-\ning visibility-aware part-level features for partial person re-\nidentiÔ¨Åcation. In CVPR, pages 393‚Äì402, 2019.\n[38] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\nWang. Beyond part models: Person retrieval with reÔ¨Åned\npart pooling (and a strong convolutional baseline). InECCV,\npages 480‚Äì496, 2018.\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, pages\n5998‚Äì6008, 2017.\n[40] Guan‚Äôan Wang, Shuo Yang, Huanyu Liu, Zhicheng Wang,\nYang Yang, Shuliang Wang, Gang Yu, Erjin Zhou, and Jian\nSun. High-order information matters: Learning relation and\ntopology for occluded person re-identiÔ¨Åcation. In CVPR,\npages 6449‚Äì6458, 2020.\n[41] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\nZhou. Learning discriminative features with multiple gran-\nularities for person re-identiÔ¨Åcation. In Proceedings of the\n26th ACM international conference on Multimedia , pages\n274‚Äì282, 2018.\n[42] Guan‚Äôan Wang, Tianzhu Zhang, Jian Cheng, Si Liu, Yang\nYang, and Zengguang Hou. Rgb-infrared cross-modality per-\nson re-identiÔ¨Åcation via joint pixel and feature alignment. In\nICCV, pages 3623‚Äì3632, 2019.\n[43] Guan‚Äôan Wang, Tianzhu Zhang, Yang Yang, Jian Cheng,\nJianlong Chang, Xu Liang, and Zeng-Guang Hou. Cross-\nmodality paired-images generation for rgb-infrared person\nre-identiÔ¨Åcation. In AAAI, pages 12144‚Äì12151, 2020.\n[44] Fei Xiong, Mengran Gou, Octavia Camps, and Mario Sz-\nnaier. Person re-identiÔ¨Åcation using kernel-based metric\nlearning methods. In ECCV, pages 1‚Äì16, 2014.\n[45] Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang, and Wanli\nOuyang. Attention-aware compositional network for person\nre-identiÔ¨Åcation. In CVPR, pages 2119‚Äì2128, 2018.\n[46] Wenjie Yang, Houjing Huang, Zhang Zhang, Xiaotang Chen,\nKaiqi Huang, and Shu Zhang. Towards rich feature discov-\nery with class activation maps augmentation for person re-\nidentiÔ¨Åcation. In CVPR, pages 1389‚Äì1398, 2019.\n[47] Yang Yang, Jimei Yang, Junjie Yan, Shengcai Liao, Dong\nYi, and Stan Z Li. Salient color names for person re-\nidentiÔ¨Åcation. In ECCV, pages 536‚Äì551, 2014.\n[48] Tianzhu Zhang, Changsheng Xu, and Ming-Hsuan Yang.\nLearning multi-task correlation particle Ô¨Ålters for visual\ntracking. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 41(2):365‚Äì378, 2019.\n[49] Tianzhu Zhang, Changsheng Xu, and Ming-Hsuan Yang.\nRobust structural sparse tracking. IEEE Transactions on\nPattern Analysis and Machine Intelligence , 41(2):473‚Äì486,\n2019.\n[50] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, and Zhibo\nChen. Densely semantically aligned person re-identiÔ¨Åcation.\nIn CVPR, pages 667‚Äì676, 2019.\n[51] Liming Zhao, Xi Li, Yueting Zhuang, and Jingdong Wang.\nDeeply-learned part-aligned representations for person re-\nidentiÔ¨Åcation. In ICCV, pages 3219‚Äì3228, 2017.\n[52] Liang Zheng, Yujia Huang, Huchuan Lu, and Yi Yang. Pose-\ninvariant embedding for deep person re-identiÔ¨Åcation. IEEE\nTransactions on Image Processing, 28(9):4500‚Äì4509, 2019.\n[53] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\ndong Wang, and Qi Tian. Scalable person re-identiÔ¨Åcation:\nA benchmark. In ICCV, pages 1116‚Äì1124, 2015.\n[54] Liang Zheng, Yi Yang, and Alexander G Hauptmann. Per-\nson re-identiÔ¨Åcation: Past, present and future. arXiv preprint\narXiv:1610.02984, 2016.\n[55] Wei-Shi Zheng, Shaogang Gong, and Tao Xiang. Person re-\nidentiÔ¨Åcation by probabilistic relative distance comparison.\nIn ICCV, pages 649‚Äì656, 2011.\n[56] Wei-Shi Zheng, Shaogang Gong, and Tao Xiang. Reidenti-\nÔ¨Åcation by relative distance comparison. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 35(3):653‚Äì\n668, 2012.\n[57] Wei-Shi Zheng, Xiang Li, Tao Xiang, Shengcai Liao,\nJianhuang Lai, and Shaogang Gong. Partial person re-\nidentiÔ¨Åcation. In ICCV, pages 4678‚Äì4686, 2015.\n[58] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-\nples generated by gan improve the person re-identiÔ¨Åcation\nbaseline in vitro. In ICCV, pages 3754‚Äì3762, 2017.\n[59] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. InAAAI, pages\n13001‚Äì13008, 2020.\n[60] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao\nWang. Identity-guided human semantic parsing for person\nre-identiÔ¨Åcation. In ECCV, 2020.\n[61] Jiaxuan Zhuo, Zeyu Chen, Jianhuang Lai, and Guangcong\nWang. Occluded person re-identiÔ¨Åcation. In ICME, 2018.\n[62] Jiaxuan Zhuo, Jianhuang Lai, and Peijia Chen. A novel\nteacher-student learning framework for occluded person re-\nidentiÔ¨Åcation. arXiv preprint arXiv:1907.03253, 2019.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7713468670845032
    },
    {
      "name": "Computer science",
      "score": 0.7314856648445129
    },
    {
      "name": "Encoder",
      "score": 0.5888082981109619
    },
    {
      "name": "Exploit",
      "score": 0.5884381532669067
    },
    {
      "name": "Artificial intelligence",
      "score": 0.536182701587677
    },
    {
      "name": "Architecture",
      "score": 0.4619266390800476
    },
    {
      "name": "Machine learning",
      "score": 0.37050873041152954
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32840389013290405
    },
    {
      "name": "Engineering",
      "score": 0.15754741430282593
    },
    {
      "name": "Electrical engineering",
      "score": 0.06415826082229614
    },
    {
      "name": "Computer security",
      "score": 0.06276041269302368
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": []
}