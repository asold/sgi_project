{
  "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
  "url": "https://openalex.org/W3118781290",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3213817739",
      "name": "Gao, Leo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222415819",
      "name": "Biderman, Stella",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223932243",
      "name": "Black, Sid",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223932248",
      "name": "Golding, Laurence",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Hoppe, Travis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4256999811",
      "name": "Foster Charles",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223480189",
      "name": "Phang, Jason",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223932249",
      "name": "He, Horace",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281440373",
      "name": "Thite, Anish",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4229190490",
      "name": "Nabeshima, Noa",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Presser, Shawn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223932250",
      "name": "Leahy, Connor",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2994816012",
    "https://openalex.org/W3029282322",
    "https://openalex.org/W3095845993",
    "https://openalex.org/W2950793774",
    "https://openalex.org/W2896972957",
    "https://openalex.org/W2912007050",
    "https://openalex.org/W2462906003",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W648152870",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W3037160251",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W3094737233",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W22168010",
    "https://openalex.org/W2128876829",
    "https://openalex.org/W2795038878",
    "https://openalex.org/W2911857293",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3037873118",
    "https://openalex.org/W3042379185",
    "https://openalex.org/W2289713250",
    "https://openalex.org/W2787887017",
    "https://openalex.org/W2578338321",
    "https://openalex.org/W2734491470",
    "https://openalex.org/W2511234952",
    "https://openalex.org/W2590362024",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2782391699",
    "https://openalex.org/W2110306171",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W2787214294",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W2617160040",
    "https://openalex.org/W3032388710",
    "https://openalex.org/W3098985263",
    "https://openalex.org/W3040589425",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3193747385",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3013451997",
    "https://openalex.org/W2949775640",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2911227954",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2123082355"
  ],
  "abstract": "Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \\textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.",
  "full_text": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling\nLeo Gao Stella Biderman Sid Black Laurence Golding\nTravis Hoppe Charles Foster Jason Phang Horace He\nAnish Thite Noa Nabeshima Shawn Presser Connor Leahy\nEleutherAI\ncontact@eleuther.ai\nAbstract\nRecent work has demonstrated that increased\ntraining dataset diversity improves general\ncross-domain knowledge and downstream gen-\neralization capability for large-scale language\nmodels. With this in mind, we present the\nPile: an 825 GiB English text corpus tar-\ngeted at training large-scale language mod-\nels. The Pile is constructed from 22 diverse\nhigh-quality subsets—both existing and newly\nconstructed—many of which derive from aca-\ndemic or professional sources. Our evalua-\ntion of the untuned performance of GPT-2 and\nGPT-3 on the Pile shows that these models\nstruggle on many of its components, such as\nacademic writing. Conversely, models trained\non the Pile improve signiﬁcantly over both\nRaw CC and CC-100 on all components of the\nPile, while improving performance on down-\nstream evaluations. Through an in-depth ex-\nploratory analysis, we document potentially\nconcerning aspects of the data for prospective\nusers. We make publicly available the code\nused in its construction.1\n1 Introduction\nRecent breakthroughs in general-purpose language\nmodeling have demonstrated the effectiveness of\ntraining massive models on large text corpora for\ndownstream applications (Radford et al., 2019;\nShoeybi et al., 2019; Raffel et al., 2019; Rosset,\n2019; Brown et al., 2020; Lepikhin et al., 2020). As\nthe ﬁeld continues to scale up language model train-\ning, the demand for high-quality massive text data\nwill continue to grow (Kaplan et al., 2020).\nThe growing need for data in language modeling\nhas caused most existing large-scale language mod-\nels to turn to the Common Crawl for most or all of\ntheir data (Brown et al., 2020; Raffel et al., 2019).\nWhile training on the Common Crawl has been\neffective, recent work has shown that dataset di-\n1https://pile.eleuther.ai/\nversity leads to better downstream generalization\ncapability (Rosset, 2019). Additionally, large-scale\nlanguage models have been shown to effectively\nacquire knowledge in a novel domain with only\nrelatively small amounts of training data from that\ndomain (Rosset, 2019; Brown et al., 2020; Carlini\net al., 2020). These results suggest that by mix-\ning together a large number of smaller, high qual-\nity, diverse datasets, we can improve the general\ncross-domain knowledge and downstream general-\nization capabilities of the model compared to mod-\nels trained on only a handful of data sources.\nTo address this need, we introduce the Pile: a\n825.18 GiB English text dataset designed for train-\ning large scale language models. The Pile is com-\nposed of 22 diverse and high-quality datasets, in-\ncluding both established natural language process-\ning datasets and several newly introduced ones.\nIn addition to its utility in training large language\nmodels, the Pile can also serve as a broad-coverage\nbenchmark for cross-domain knowledge and gener-\nalization ability of language models.\nWe introduce new datasets derived from the fol-\nlowing sources: PubMed Central, ArXiv, GitHub,\nthe FreeLaw Project, Stack Exchange, the US\nPatent and Trademark Ofﬁce, PubMed, Ubuntu\nIRC, HackerNews, YouTube, PhilPapers, and NIH\nExPorter. We also introduce OpenWebText2 and\nBookCorpus2, which are extensions of the original\nOpenWebText (Gokaslan and Cohen, 2019) and\nBookCorpus (Zhu et al., 2015; Kobayashi, 2018)\ndatasets, respectively.\nIn addition, we incorporate several existing high-\nquality datasets: Books3 (Presser, 2020), Project\nGutenberg (PG-19) (Rae et al., 2019), Open-\nSubtitles (Tiedemann, 2016), English Wikipedia,\nDM Mathematics (Saxton et al., 2019), EuroParl\n(Koehn, 2005), and the Enron Emails corpus (Klimt\nand Yang, 2004). To supplement these, we also in-\n1\narXiv:2101.00027v1  [cs.CL]  31 Dec 2020\nFigure 1: Treemap of Pile components by effective size.\ntroduce a new ﬁltered subset of Common Crawl,\nPile-CC, with improved extraction quality.\nThrough our analyses, we conﬁrm that the Pile is\nsigniﬁcantly distinct from pure Common Crawl\ndata. Additionally, our evaluations show that the\nexisting GPT-2 and GPT-3 models perform poorly\non many components of the Pile, and that models\ntrained on the Pile signiﬁcantly outperform both\nraw and ﬁltered Common Crawl models. To com-\nplement the performance evaluations, we also per-\nform an exploratory analysis of the text within the\nPile to provide a detailed picture of the data. We\nhope that our extensive documentation of the con-\nstruction and characteristics of the Pile will help\nresearchers make informed decisions about poten-\ntial downstream applications.\nFinally, we make publicly available the preprocess-\ning code for the constituent datasets of the Pile and\nthe code for constructing alternative versions2. In\nthe interest of reproducibility, we also document\nall processing performed on each dataset (and the\nPile as a whole) in as much detail as possible. For\nfurther details about the processing of each dataset,\nsee Section 2 and Appendix C.\n2https://github.com/EleutherAI/\nthe-pile\n1.1 Contributions\nThe core contributions of this paper are:\n1. The introduction of a 825.18 GiB english-\nlanguage dataset for language modeling com-\nbining 22 diverse sources.\n2. The introduction of 14 new language model-\ning datasets, which we expect to be of inde-\npendent interest to researchers.\n3. Evaluations demonstrating signiﬁcant im-\nprovements across many domains by GPT-2-\nsized models trained on this new dataset, com-\npared to training on CC-100 and raw Common\nCrawl.\n4. The investigation and documentation of this\ndataset, which we hope will better inform re-\nsearchers about how to use it as well as moti-\nvate them to undertake similar investigations\nof their own data.\n2 The Pile Datasets\nThe Pile is composed of 22 constituent sub-datasets,\nas shown in Table 1. Following Brown et al. (2020),\nwe increase the weights of higher quality compo-\nnents, with certain high-quality datasets such as\nWikipedia being seen up to 3 times (“epochs”) for\n2\nComponent Raw Size Weight Epochs Effective Size Mean Document Size\nPile-CC 227.12 GiB 18.11% 1.0 227.12 GiB 4.33 KiB\nPubMed Central 90.27 GiB 14.40% 2.0 180.55 GiB 30.55 KiB\nBooks3† 100.96 GiB 12.07% 1.5 151.44 GiB 538.36 KiB\nOpenWebText2 62.77 GiB 10.01% 2.0 125.54 GiB 3.85 KiB\nArXiv 56.21 GiB 8.96% 2.0 112.42 GiB 46.61 KiB\nGithub 95.16 GiB 7.59% 1.0 95.16 GiB 5.25 KiB\nFreeLaw 51.15 GiB 6.12% 1.5 76.73 GiB 15.06 KiB\nStack Exchange 32.20 GiB 5.13% 2.0 64.39 GiB 2.16 KiB\nUSPTO Backgrounds 22.90 GiB 3.65% 2.0 45.81 GiB 4.08 KiB\nPubMed Abstracts 19.26 GiB 3.07% 2.0 38.53 GiB 1.30 KiB\nGutenberg (PG-19)† 10.88 GiB 2.17% 2.5 27.19 GiB 398.73 KiB\nOpenSubtitles† 12.98 GiB 1.55% 1.5 19.47 GiB 30.48 KiB\nWikipedia (en)† 6.38 GiB 1.53% 3.0 19.13 GiB 1.11 KiB\nDM Mathematics† 7.75 GiB 1.24% 2.0 15.49 GiB 8.00 KiB\nUbuntu IRC 5.52 GiB 0.88% 2.0 11.03 GiB 545.48 KiB\nBookCorpus2 6.30 GiB 0.75% 1.5 9.45 GiB 369.87 KiB\nEuroParl† 4.59 GiB 0.73% 2.0 9.17 GiB 68.87 KiB\nHackerNews 3.90 GiB 0.62% 2.0 7.80 GiB 4.92 KiB\nYoutubeSubtitles 3.73 GiB 0.60% 2.0 7.47 GiB 22.55 KiB\nPhilPapers 2.38 GiB 0.38% 2.0 4.76 GiB 73.37 KiB\nNIH ExPorter 1.89 GiB 0.30% 2.0 3.79 GiB 2.11 KiB\nEnron Emails† 0.88 GiB 0.14% 2.0 1.76 GiB 1.78 KiB\nThe Pile 825.18 GiB 1254.20 GiB 5.91 KiB\nTable 1: Overview of datasets in the Pile before creating the held out sets. Raw Size is the size before any\nup- or down-sampling. Weight is the percentage of bytes in the ﬁnal dataset occupied by each dataset. Epochs\nis the number of passes over each constituent dataset during a full epoch over the Pile. Effective Size is the\napproximate number of bytes in the Pile occupied by each dataset. Datasets marked with a † are used with minimal\npreprocessing from prior work.\neach full epoch over the Pile. Detailed information\nabout the construction of each dataset is available\nin Appendix C.\n2.1 Pile-CC\nCommon Crawl is a collection of website crawls\nfrom 2008 onwards, including raw web pages,\nmetadata and text extractions. Due to the raw na-\nture of the dataset, Common Crawl has the ad-\nvantage of including text from diverse domains,\nbut at the cost of varying quality data. Due to\nthis, use of Common Crawl typically necessi-\ntates well-designed extraction and ﬁltering. Our\nCommon Crawl-based dataset, Pile-CC, uses jus-\nText (Endrédy and Novák, 2013) on Web Archive\nﬁles (raw HTTP responses including page HTML)\nfor extraction, which yields higher quality output\nthan directly using the WET ﬁles (extracted plain-\ntext).\n2.2 PubMed Central\nPubMed Central (PMC) is a subset of the PubMed\nonline repository for biomedical articles run by\nthe United States of America’s National Center\nfor Biotechnology Information (NCBI), providing\nopen, full-text access to nearly ﬁve million publi-\ncations. Most publications indexed by PMC are\nrecent, and their inclusion is mandated for all NIH\nfunded research starting from 2008 by the NIH\nPublic Access Policy. We included PMC in the\nhopes that it will beneﬁt potential downstream ap-\nplications to the medical domain.\n2.3 Books3\nBooks3 is a dataset of books derived from a copy\nof the contents of the Bibliotik private tracker\nmade available by Shawn Presser (Presser, 2020).\nBibliotik consists of a mix of ﬁction and nonﬁc-\ntion books and is almost an order of magnitude\n3\nlarger than our next largest book dataset (BookCor-\npus2).We included Bibliotik because books are in-\nvaluable for long-range context modeling research\nand coherent storytelling.\n2.4 OpenWebText2\nOpenWebText2 (OWT2) is a generalized web\nscrape dataset inspired by WebText (Radford et al.,\n2019) and OpenWebTextCorpus (Gokaslan and Co-\nhen, 2019). Similar to the original WebText, we\nuse net upvotes on Reddit submissions as a proxy\nfor outgoing link quality. OpenWebText2 includes\nmore recent content from Reddit submissions up\nuntil 2020, content from multiple languages, docu-\nment metadata, multiple dataset versions, and open\nsource replication code. We included OWT2 as a\nhigh quality general purpose dataset.\n2.5 ArXiv\nArXiv is a preprint server for research papers that\nhas operated since 1991. As shown in ﬁg. 10, arXiv\npapers are predominantly in the ﬁelds of Math,\nComputer Science, and Physics. We included arXiv\nin the hopes that it will be a source of high qual-\nity text and math knowledge, and beneﬁt potential\ndownstream applications to research in these ar-\neas. ArXiv papers are written in LaTeX, a common\ntypesetting language for mathematics, computer\nscience, physics, and some adjacent ﬁelds. Train-\ning a language model to be able to generate papers\nwritten in LaTeX could be a huge boon to the re-\nsearch community.\n2.6 GitHub\nGitHub is a large corpus of open-source code repos-\nitories. Motivated by the ability of GPT-3 (Brown\net al., 2020) to generate plausible code completions\ndespite its training data not containing any explic-\nitly gathered code datasets, we included GitHub in\nthe hopes that it would enable better downstream\nperformance on code-related tasks.\n2.7 FreeLaw\nThe Free Law Project is a US-registered non-proﬁt\nthat provides access to and analytical tools for aca-\ndemic studies in the legal realm. CourtListener, 3\npart of the Free Law Project, provides bulk down-\nloads for millions of legal opinions from federal\nand state courts. While the full dataset provides\nmultiple modalities of legal proceedings, includ-\ning dockets, bibliographic information on judges,\n3https://www.courtlistener.com/\nand other metadata, we focused speciﬁcally on\ncourt opinions due to an abundance of full-text\nentries. This data is entirely within the public do-\nmain.\n2.8 Stack Exchange\nThe Stack Exchange Data Dump 4 contains an\nanonymized set of all user-contributed content on\nthe Stack Exchange network, a popular collection\nof websites centered around user-contributed ques-\ntions and answers. It is one of the largest publicly\navailable repositories of question-answer pairs, and\ncovers a wide range of subjects—from program-\nming, to gardening, to Buddhism. We included\nStack Exchange in the hopes that it will improve\nthe question answering capabilities of downstream\nmodels on diverse domains.\n2.9 USPTO Backgrounds\nUSPTO Backgrounds is a dataset of background\nsections from patents granted by the United States\nPatent and Trademark Ofﬁce, derived from its pub-\nlished bulk archives5. A typical patent background\nlays out the general context of the invention, gives\nan overview of the technical ﬁeld, and sets up the\nframing of the problem space. We included USPTO\nBackgrounds because it contains a large volume of\ntechnical writing on applied subjects, aimed at a\nnon-technical audience.\n2.10 Wikipedia (English)\nWikipedia is a standard source of high-quality text\nfor language modeling. In addition to being a\nsource of high quality, clean English text, it is also\nvaluable as it is written in expository prose, and\nspans many domains.\n2.11 PubMed Abstracts\nPubMed Abstracts consists of the abstracts from 30\nmillion publications in PubMed, the online repos-\nitory for biomedical articles run by the National\nLibrary of Medicine. While the PMC (see Section\n2.2) provides full-text access, the subset of cover-\nage is signiﬁcantly limited and biased towards re-\ncent publications. PubMed also incorporates MED-\nLINE, which expands the coverage of biomedical\nabstracts from 1946 to present day.\n4https://archive.org/details/\nstackexchange\n5https://bulkdata.uspto.gov/\n4\n2.12 Project Gutenberg\nProject Gutenberg is a dataset of classic Western\nliterature. The speciﬁc Project Gutenberg derived\ndataset we used, PG-19, consists of Project Guten-\nberg books from before 1919 (Rae et al., 2019),\nwhich represent distinct styles from the more mod-\nern Books3 and BookCorpus. Additionally, the PG-\n19 dataset is already being used for long-distance\ncontext modeling.\n2.13 OpenSubtitles\nThe OpenSubtitles dataset is an English language\ndataset of subtitles from movies and television\nshows gathered by Tiedemann (2016). Subtitles\nprovide an important source of natural dialog, as\nwell as an understanding of ﬁctional formats other\nthan prose, which may prove useful for creative\nwriting generation tasks such as screenwriting,\nspeechwriting, and interactive storytelling.\n2.14 DeepMind Mathematics\nThe DeepMind Mathematics dataset consists of a\ncollection of mathematical problems from topics\nsuch as algebra, arithmetic, calculus, number the-\nory, and probability, formatted as natural language\nprompts (Saxton et al., 2019). One major weakness\nof large language models has been performance\non mathematical tasks (Brown et al., 2020), which\nmay be due in part to a lack of math problems in\nthe training set. By explicitly including a dataset\nof mathematical problems, we hope to improve the\nmathematical ability of language models trained on\nthe Pile.\n2.15 BookCorpus2\nBookCorpus2 is an expanded version of the origi-\nnal BookCorpus (Zhu et al., 2015), a widely used\nlanguage modeling corpus consisting of books writ-\nten by “as of yet unpublished authors.” BookCor-\npus is therefore unlikely to have signiﬁcant overlap\nwith Project Gutenberg and Books3, which consist\nof published books. BookCorpus is also commonly\nused as dataset for training language models (Rad-\nford et al., 2018; Devlin et al., 2019; Liu et al.,\n2019).\n2.16 Ubuntu IRC\nThe Ubuntu IRC dataset is derived from the pub-\nlicly available chatlogs6 of all Ubuntu-related chan-\nnels on the Freenode IRC chat server. Chatlog data\n6https://irclogs.ubuntu.com/\nprovides an opportunity to model real-time human\ninteractions, which feature a level of spontaneity\nnot typically found in other modes of social me-\ndia.\n2.17 EuroParl\nEuroParl (Koehn, 2005) is a multilingual parallel\ncorpus originally introduced for machine transla-\ntion but which has also seen use in several other\nﬁelds of NLP (Groves and Way, 2006; Van Hal-\nteren, 2008; Ciobanu et al., 2017). We use the most\ncurrent version at time of writing, which consists of\nthe proceedings of the European Parliament in 21\nEuropean languages from 1996 until 2012.\n2.18 YouTube Subtitles\nThe YouTube Subtitles dataset is a parallel cor-\npus of text gathered from human generated closed-\ncaptions on YouTube. In addition to providing mul-\ntilingual data, Youtube Subtitles is also a source of\neducational content, popular culture, and natural\ndialog.\n2.19 PhilPapers\nThe PhilPapers7 dataset consists of open-access\nphilosophy publications from an international\ndatabase maintained by the Center for Digital Phi-\nlosophy at the University of Western Ontario. We\nincluded PhilPapers because it spans a wide body\nof abstract, conceptual discourse, and its articles\ncontain high quality academic writing.\n2.20 NIH Grant Abstracts:\nExPORTER\nThe NIH Grant abstracts provides a bulk-data repos-\nitory for awarded applications through the Ex-\nPORTER8 service covering the ﬁscal years 1985-\npresent. We included the dataset because it contains\nexamples of high-quality scientiﬁc writing.\n2.21 Hacker News\nHacker News9 is a link aggregator operated by Y\nCombinator, a startup incubator and investment\nfund. Users submit articles deﬁned as “anything\nthat gratiﬁes one’s intellectual curiosity,” but sub-\nmitted articles tend to focus on topics in computer\nscience and entrepreneurship. Users can comment\non submitted stories, resulting in comment trees\ndiscussing and critiquing submitted stories. We\n7https://philpapers.org/\n8https://exporter.nih.gov/\n9https://news.ycombinator.com\n5\nscrape, parse, and include these comment trees\nsince we believe they provide high quality dialogue\nand debate on niche topics.\n2.22 Enron Emails\nThe Enron Emails dataset (Klimt and Yang, 2004)\nis a valuable corpus commonly used for research\nabout the usage patterns of email. We included\nEnron Emails to aid in understanding the modality\nof email communications, which is typically not\nfound in any of our other datasets.\n3 Benchmarking Language Models with\nthe Pile\nWhile the Pile was conceived as a training dataset\nfor large-scale language models, its coverage of\nmultiple disparate domains makes it also suitable\nas an evaluation dataset. In this section, we de-\nscribe how the Pile can be used as a broad-coverage\ndataset for benchmarking language models.\n3.1 Benchmarking Guidelines\nThe Pile is provided as train, validation, and test-\ning splits. The validation and testing components\neach contain 0.1% of the data, sampled uniformly\nat random. While this is a far smaller percentage\nthan most datasets, the sheer size of the dataset\nresults in over 1 GiB of validation and testing data\neach. We highlight that while we have made ef-\nforts to deduplicate documents within the Pile (See:\nSection D.2), it is still possible that some docu-\nments are duplicated across the train/validation/test\nsplits.\nOur preferred metric is bits per UTF-8 encoded\nbyte (BPB ). Bits per byte is preferred over bits per\ncharacter or perplexity when using Pile as a met-\nric due to its invariance to different tokenization\nschemes and the ambiguity of measuring charac-\nters in Unicode. To compute bits per byte from a\ngiven negative log likelihood loss ℓ, we compute\nBPB = ( LT /LB) log2(eℓ) = ( LT /LB)ℓ/ln(2),\nwhere LT is the length of the dataset in tokens and\nLB is the length of the dataset in UTF-8 encoded\nbytes. We ﬁnd that LT /LB is 0.29335 GPT-2-\ntokens/byte across the Pile; dataset-speciﬁc values\nof LT /LB can be found in Table 7.\n3.2 Test Perplexity with GPT-2 and\nGPT-3\nWe compute the test perplexity of the constituent\ndatasets of the Pile using GPT-2 (Radford et al.,\n2019) and GPT-3 (Brown et al., 2020), shown in\nFigure 2. We use all available versions of GPT-2,\nand all four versions of GPT-3 available via the\nOpenAI API. Because of the cost associated with\nusing the OpenAI API, we evaluate on one-tenth of\nthe respective test sets for most of the constituent\ndatasets. We report the perplexity converted to bits\nper UTF-8 encoded byte ( BPB ). Importantly, we\ncompute perplexity by evaluating each document\nindependently within each dataset, as opposed to\nconcatenating all documents as is common practice\nfor computing perplexity on large corpora.\nFull details of the perplexity computation can be\nfound in Appendix E.2.\nUnsurprisingly, larger language models generally\nattain lower perplexity compared to smaller models.\nRecent work has shown an increased focus on the\nempirical scaling laws of language models (Kaplan\net al., 2020; Henighan et al., 2020). As such, we\ninvestigate the scaling law for the GPT-2 and GPT-\n3 families of models on perplexity evaluation on\nthe Pile. The scaling law relation for the GPT-3\nfamily of models is shown in Figure 2.10 The line\nof best ﬁt shown in the ﬁgure has a coefﬁcient of\n-0.1674 and an intercept of 2.5516.\nFigure 2: Scaling law for performance of GPT-2/3 mod-\nels. ‘Zero-shot’ refers to the fact that none of the mod-\nels have been ﬁne-tuned on data from the Pile.\nInterestingly, while GPT-2 and GPT-3 were not\ntrained on the Pile, there still appears to be a clear\nscaling law without diminishing returns. We hy-\npothesize that this is due to the inherent generaliza-\ntion capability of these models. We leave a more\n10While the sizes of GPT-3 models on the OpenAI API have\nnot been publicized, we assume here that ada, babbage,\ncurie and davinci models correspond to 2.7B, 6.7B, 13B\nand 175B parameter models respectively.\n6\nrigorous analysis of zero-shot scaling laws to future\nwork.\n3.3 Relative Componentwise GPT-3 Pile\nPerformance\nDetermining which components GPT-3 underper-\nforms on provides information about which Pile\ncomponents are most dissimilar to the distribution\nof text (web pages and books) that GPT-3 was\ntrained on. These components would thus make es-\npecially good candidates for supplementing GPT-3\ntraining data. These results are also valuable for\ndetermining which types of datasets to emphasize\nfor future iterations of the Pile.\nDue to the difference in entropy of different\ndatasets, directly comparing perplexity of GPT-3\non different Pile components is not an accurate in-\ndication of relative performance. Ideally we would\ntrain a GPT-3 model from scratch on the Pile and\ncompare the difference in loss per dataset with\nthat of the original GPT-3. Because of resource\nconstraints, we instead use a GPT-2 model trained\nfrom scratch on the Pile (see Section 4) to con-\nstruct a proxy measure. To construct our proxy,\nwe ﬁrst measure the improvement from the GPT-\n2-Pile model to GPT-3 on each component. Then,\nwe normalize our results by setting the change on\nOpenWebText2 to be zero. This computation is\nshown in the equation below:\n∆set =\n(\nLGPT3\nset − LGPT3\nowt2\n)\n−\n(\nLGPT2Pile\nset − LGPT2Pile\nowt2\n)\nSince GPT2-Pile was trained on both OWT2 and\nthe dataset we are evaluating, we expect the second\nterm in ∆set to reﬂect the difference in the intrinsic\ndifﬁculty of the two datasets. Thus the total value\nof ∆set reﬂects how much harder the dataset we are\nevaluating was for GPT-3 than OWT2, minus the\nrelative difﬁculty of the two tasks. As GPT-3 was\ntrained on data very similar to OWT2, this gives us\na proxy for how much better GPT-3 would do if it\nwere trained on the Pile.\nThe results are shown in Figure 3. As a san-\nity check, we observe that datasets that are con-\ntained in, or are extremely similar to, GPT-3’s\ntraining set (Books3, Wikipedia (en), Pile-CC and\nProject Gutenberg) score close to zero on our met-\nric.\nGPT-3 appears to perform poorly on datasets\npertaining to research or academic writing like\nPubMed Central, PubMed Abstracts, and ArXiv;\ndomain-speciﬁc datasets like FreeLaw, Hack-\nerNews, and USPTO Backgrounds; and on datasets\ncontaining predominantly text distinct from natu-\nral language, like GitHub and DM Mathematics.\nIn addition, the majority of datasets see less of an\nimprovement than OpenWebText2. As such, we ex-\npect a GPT-3 sized model trained on Pile to perform\nsigniﬁcantly better on research related tasks, soft-\nware tasks, and symbol manipulation tasks than the\nbase model. Additionally, this experiment provides\nevidence that the majority of Pile components are\nnot redundant with the predominantly web-based\nGPT-3 training data.\nWe note that this metric is only a proxy for similar-\nity, and that it could be confounded by dataset spe-\nciﬁc scaling effects. Although our results largely\naccord with expectations, there are some puzzling\nresults, like the datasets on which GPT-3 outper-\nformed GPT-2 Pile. We hypothesize that GPT-3\nlearns to be so good at these datasets that train-\ning on them explicitly does not notably beneﬁt the\nmodel’s performance. We leave a more rigorous\nanalysis of these effects for future work.\n4 Evaluation\nTo conﬁrm the effectiveness of the Pile for im-\nproving language modeling quality, we train\narchitecturally-identical 1.3 billion parameter mod-\nels based on those in Brown et al. (2020) on dif-\nferent datasets and evaluate on the WikiText and\nLAMBADA tasks as benchmarks of language mod-\neling ability. We also report results on the Pile\nas a measure of more cross-domain generaliza-\ntion.\n4.1 Methodology\nTo ensure a fair comparison across datasets of dif-\nferent sizes, we decontaminate any instances of the\nevaluation sets using the same 13-gram overlap ﬁl-\ntering as in Brown et al. (2020) and downsample\nto 40GB to control for dataset size. As we control\nfor dataset size, we emphasize that our evaluation\nis generous to CC-100 (en), which is about 1/3 the\nsize of the Pile in reality.\nWe compare the following datasets: the Pile, the En-\n7\nComponent GPT-2 GPT-3\nsmall medium large xl ada babbage curie davinci\nPile-CC 1.0878 0.9992 0.9582 0.9355 0.9212 0.8483 0.7849 0.7070\nPubMed Central 1.0759 0.9788 0.9334 0.9044 0.8633 0.7792 0.7150 0.6544\nBooks3 1.1959 1.1063 1.0588 1.0287 0.9778 0.9005 0.8284 0.7052\nOpenWebText2 1.1111 1.0073 0.9539 0.9171 0.8727 0.7921 0.7199 0.6242\nArXiv 1.3548 1.2305 1.1778 1.1381 1.0304 0.9259 0.8453 0.7702\nGithub 1.7912 1.3180 1.7909 1.6486 0.8761 0.7335 0.6415 0.5635\nFreeLaw 1.0512 0.9321 0.9017 0.8747 0.8226 0.7381 0.6667 0.6006\nStack Exchange 1.2981 1.1075 1.0806 1.0504 1.0096 0.8839 0.8004 0.7321\nUSPTO Backgrounds 0.8288 0.7564 0.7202 0.6969 0.6799 0.6230 0.5752 0.5280\nPubMed Abstracts 0.9524 0.8579 0.8108 0.7810 0.8130 0.7382 0.6773 0.6201\nGutenberg (PG-19) 1.2655 1.1140 1.0820 1.0829 0.9776 0.8749 0.7930 0.7115\nOpenSubtitles 1.2465 1.1657 1.1324 1.1129 1.1116 1.0488 0.9875 0.9130\nWikipedia (en) 1.1285 1.0213 0.9795 0.9655 0.8757 0.7863 0.7047 0.5953\nDM Mathematics 2.6911 2.5448 2.4833 2.4377 2.3249 2.2015 2.1067 2.0228\nUbuntu IRC 1.8466 1.7187 1.6427 1.6024 1.3139 1.1968 1.0995 0.9915\nBookCorpus2 1.1295 1.0498 1.0061 0.9783 0.9754 0.9041 0.8435 0.7788\nEuroParl 2.3177 2.0204 1.8770 1.7650 1.0475 0.9363 0.8415 0.7519\nHackerNews 1.4433 1.2794 1.3143 1.3361 1.1736 1.0875 1.0175 0.9457\nYoutubeSubtitles 2.0387 1.8412 1.7355 1.6694 1.3407 1.1876 1.0639 0.9469\nPhilPapers 1.3203 1.2163 1.1688 1.1327 1.0362 0.9530 0.8802 0.8059\nNIH ExPorter 0.9099 0.8323 0.7946 0.7694 0.7974 0.7326 0.6784 0.6239\nEnron Emails 1.5888 1.4119 1.4535 1.4222 1.2634 1.1685 1.0990 1.0201\nThe Pile 1.2253 1.0928 1.0828 1.0468 0.9631 0.8718 0.7980 0.7177\nTable 2: Test perplexity of the Pile using GPT-2 and GPT-3, converted to bits per UTF-8 encoded byte ( BPB ).\nEvaluation is performed on one-tenth of the test data of the Pile, on a per-document basis. Bold indicates the\nbest-performing model in each row.\nglish component of the CC-100 dataset11 (Wenzek\net al., 2019; Conneau et al., 2020), and a sample of\nraw CC WET ﬁles ﬁltered for English-only.\n4.2 Results\nOn traditional language modeling benchmarks, the\nPile improves signiﬁcantly on WikiText and shows\nnegligible changes in LAMBADA. However, mod-\nels trained on Pile improve signiﬁcantly over both\nRaw CC and CC-100 on all components of the\nPile, as shown in Table 4. This indicates that mod-\nels trained on the Pile have greater cross-domain\ngeneralization capabilities without compromising\nperformance on traditional benchmarks.\nThe magnitude of improvement over CC-100 per\nset is shown in Figure 4. Unsurprisingly, there\nis almost no improvement on Pile-CC. However,\nthe model trained on the Pile performs signiﬁ-\ncantly better than either of the other models on\nacademic datasets such as ArXiv, Pubmed Central,\nFreeLaw, and PhilPapers. It also improves signiﬁ-\n11The data was obtained from http://data.statmt.\norg/cc-100/.\ncantly on programming-related datasets like Github\nand StackExchange, on EuroParl, due to the lack of\nmultilingual text in either other dataset, and on DM\nMathematics, indicating a signiﬁcant improvement\nin mathematical ability.\nSurprisingly, raw Common Crawl performs better\non the Pile BPB than CC-100, despite losing by a\nsigniﬁcant margin on LAMBADA and WikiText.\nWe hypothesize that this is due to the perplexity\nbased ﬁltering used in CC-100, where a language\nmodel is trained on Wikipedia and all data with a\nperplexity too high or too low is discarded. This\neffectively discards any data too similar to or too\ndifferent from Wikipedia, which severely limits\nthe diversity of the collected data. This result\nsuggests that future work using Common Crawl\nshould take caution with ﬁltering to preserve its\ndiversity.\n5 Structural Statistics\nIn this section, we cover the Structural Statistics\nof the dataset, which provide more coarse-grained\nand statistical information about the Pile. In Sec-\n8\nFigure 3: Change in BPB from GPT-2 trained on Pile to GPT-3 zero-shot, relative to OpenWebText2 BPB change.\nDotted line indicates overall Pile change. Lower indicates better relative performance by GPT-3.\nDataset Size Pile (val) Pile (test) WikiText LAMBADA LAMBADA\n(BPB ) ( BPB ) ( PPL ) ( PPL ) ( ACC)\nThe Pile 825 GiB 0.9281 0.9433 5.59 12.78 50.1\nCC-100 (en) 300 GiB 1.3143 1.3293 8.27 11.78 49.7\nRaw CC 45927 GiB † 1.1180 1.1275 11.75 19.84 43.8\nTable 3: Size-controlled evaluation results. Each dataset is deduplicated against all evaluation metrics and subsam-\npled to approximately 40GB to control for the effects of dataset size. For LAMBADA, we use the variant of the\ndata introduced in Radford et al. (2019) and only evaluate the perplexity on the ﬁnal token rather than the ﬁnal\nword. For WikiText, we report the perplexity per GPT-2 token. † indicates that the size is an estimate.\ntion 6, we provide a closer investigation and doc-\numentation of the textual content within the Pile\ndatasets.\n5.1 Document Lengths and\nTokenization\nEach dataset consists of a large number of docu-\nments. We analyze the distribution of document\nlengths, as well as the number of bytes-per-token\nusing the GPT-2 tokenizer in order to put our abla-\ntions in context.\nWhile the majority of documents in the Pile are\nshort, there is a long tail of very long documents\n(Figure 5).\nSince the GPT-2 BPE tokenizer is trained on Web-\nText, the mean bytes per token is also a very rough\nindicator of how syntactically different each Pile\ncomponent is from WebText. For instance, datasets\nlike NIH ExPorter, OpenWebText2 and Books3\nconsist largely of ordinary text in a similar distri-\nbution to WebText, which is reﬂected in a greater\nnumber of bytes per token. On the other hand,\nmany of the sets with the lowest bytes per token\nare those which consist in large part of non-text\ncontent (Github, ArXiv, Stack Exchange, and DM\nMathematics) or languages other than English (Eu-\nroParl).\n5.2 Language and Dialects\nWhile only 13% of the world’s population speaks\nEnglish, the vast majority of NLP research is done\non English. For the Pile, we took a similar ap-\nproach to the dataset used by Brown et al. (2020)\nand focused predominantly on English, while also\nnot explicitly ﬁltering out other languages when\ncollecting our own data. When evaluating a multi-\nlingual dataset, our main criteria for inclusion was\nwhether the English component of the dataset mer-\nited inclusion alone. We plan to create a fully multi-\n9\nFigure 4: Magnitude of BPB improvement of Pile model over CC-100 model on each test set.\nDataset The Pile CC-100 (en) Raw CC (en)\nPile-CC 0.9989 1.0873 1.0287\nPubMed Central 0.6332 1.1311 0.9120\nBooks3 1.0734 1.2264 1.1366\nOpenWebText2 0.9938 1.2222 1.0732\nArXiv 0.7945 1.8159 1.2642\nGithub 0.5597 1.6509 0.9301\nFreeLaw 0.6978 1.0221 0.9468\nStack Exchange 0.8152 1.5414 1.1292\nUSPTO Backgrounds 0.6731 0.8772 0.8455\nPubMed Abstracts 0.7313 1.0193 0.9718\nGutenberg (PG-19) 1.1426 1.2780 1.2235\nOpenSubtitles 1.0909 1.1827 1.2139\nWikipedia (en) 0.8961 1.1807 1.0252\nDM Mathematics 1.5206 3.1774 2.6229\nUbuntu IRC 1.4085 2.1243 1.5691\nBookCorpus2 1.0613 1.1346 1.0914\nEuroParl 1.1202 2.7141 1.4917\nHackerNews 1.0968 1.4352 1.2305\nYoutubeSubtitles 1.4269 2.3287 1.5607\nPhilPapers 1.1256 1.4269 1.2090\nNIH ExPorter 0.7347 0.9713 0.9225\nEnron Emails 0.8301 1.3300 1.0483\nTable 4: Breakdown of BPB on Pile heldout test set.\nColumns indicate the dataset each model is trained on;\nrows indicate the evaluation dataset. Bold indicates the\nbest performing model in each row.\nlingual expansion of the Pile as future work.\nUsing fasttext (Suárez et al., 2019a), we deter-\nmine that the Pile is 97.4% English. We note that\ndue to issues with language identiﬁcation, partic-\nularly with rare languages Caswell et al. (2020),\nthis methodology provides only a rough estimate\nfor English content and no reliable conclusions for\nlow-resource languages can be drawn.\nFigure 5: Distribution of document lengths in Pile. The\nhighest 1 percentile of document length are considered\nto be outliers and excluded from this plot.\n6 Investigating and Documenting the\nDatasets\nAs the scale of machine learning research has\ngrown, scrutiny has been placed on the ever\nlarger datasets that models are trained on (Prabhu\nand Birhane, 2020; Biderman and Scheirer,\n2020)\nWhile this issue has been raised within AI ethics\nand bias research (Hovy and Spruit, 2016; Hutchin-\nson et al., 2020; Blodgett et al., 2020), it has not\nbeen a focal point of concern within the language\nmodeling community. Despite the proliferation\nof work exploring and documenting issues with\ndatasets (Gebru et al., 2018; Bender and Friedman,\n10\nFigure 6: Mean bytes per GPT-2-token for each dataset\nin the Pile. Error bars indicate standard deviation.\n2018; Jo and Gebru, 2020), no dataset intended\nto train massive language models has been seri-\nously documented by its creators12. Therefore, our\nanalyses serve two goals: to address ethical con-\ncerns about the Pile, and to promote and normalize\nthe practice of engaging with the AI ethics litera-\nture.\nNatural language processing technologies are\nwidely applicable and can be used in extremely\ndifferent contexts. What is and is not appropriate\ndata to train on can therefore vary wildly with the\napplication context. In our view, the best approach\nis to document rather than eliminate potentially con-\ncerning aspects of datasets13, particularly since the\npurpose of the Pile is to train general-purpose lan-\nguage models. The primary goal of our documen-\ntation, therefore, is to empower NLP researchers to\nmake informed decisions.\n6.1 Documenting Methods\nTo document the Pile, we chose to implement two\nframeworks that have been proposed by method-\nologists and ethics researchers. The ﬁrst, the\ndatasheets methodology (Gebru et al., 2018), is a\ngeneral purpose methodology that is recommended\nby several methodologists (Raji and Yang, 2019;\nBiderman and Scheirer, 2020) and appears to be\nused more frequently by practitioners than alterna-\n12Brown et al. (2020) discusses ethical issues surrounding\ntheir model, but do not discuss those surrounding the training\ndataset itself.\n13That said, we did exclude several datasets, see Appendix\nB for details.\ntives (Seck et al., 2018; Costa-jussà et al., 2020;\nThieme et al., 2020). The second, the data state-\nments methodology (Bender and Friedman, 2018),\nwas proposed speciﬁcally for natural language pro-\ncessing and has been well received by the NLP\ncommunity. Our datasheet and data statement will\nbe featured in the GitHub repository where the\ncode for the Pile is stored and will also be available\nas separate documents on arXiv (Biderman et al.,\n2021; Biderman, 2021).\nIn addition to the datasheet and data statement,\nthere is additional information that may be helpful\nto people training language models that these doc-\numents do not cover. In the rest of this section we\ninvestigate and document in greater detail some of\nthis additional contextual information.\n6.2 Topical Distribution\nIn order to better understand the speciﬁc subject\nmatter covered by the Pile, we performed a topic\nmodeling analysis on its components. Using Gen-\nsim (Rehurek et al., 2011), we trained 16-topic La-\ntent Dirichlet Allocation (Blei et al., 2003) models\non each component of the validation set of the Pile\nconcurrently, in an online fashion (Hoffman et al.,\n2010). We ﬁltered the Pile for English only for this\nanalysis. Afterwards, we computed the perplex-\nity of the Common Crawl-derived (Pile-CC) topic\nmodel on the document sets of the other compo-\nnents. In this way, we provide a rough measure of\nthe degree to which parts of the Pile contain topics\nnot well covered within Common Crawl.\nIn Figure 7, these cross-component perplexities are\nshown, with a vertical line indicating the perplexity\nof the Pile-CC topic model evaluated on the doc-\numents of OpenWebText2. This component was\nchosen as a baseline of comparison for similar rea-\nsons as in the previous evaluation: it is derived in a\nsimilar manner (ﬁltered crawls of the open web) as\nthe Common Crawl, and thus is expected to contain\na similar distribution of topics. Although Pile-CC\nis somewhat diverse in its content, several of the\nPile’s other components deviate from it strongly in\ntheir topical focus, as evidenced by higher perplex-\nity on Github, PhilPapers, and EuroParl.\nWe also documented the topical clusters inferred\nfrom our LDA models for each component, which\nwe provide in Appendix C. As expected, though\nthe larger CC-derived component itself represents a\ndiversity of content—including politics, education,\n11\nsports and entertainment—the content clusters it\nmisses become apparent when compared qualita-\ntively to other components of the Pile. Notably, the\ndata modes covering programming, logic, physics,\nand legal knowledge appear largely absent.\n6.3 Pejorative Content\nDue to the wide diversity in origins, it is possible\nfor the Pile to contain pejorative, sexually explicit,\nor otherwise objectionable content. As this content\nmay not be desirable for some use cases, we break\ndown profanity on a per-dataset level.\nWe used the profanity-checker Python\npackage (Zhou, 2019). This package includes a\n“toxicity model” trained on multiple profanity lists\nas well as the Wikidetox Toxic Comment Dataset\n(Wulczyn et al., 2016) and classiﬁes a given string\nas being profane or not profane.\nWe considered only the English sentences in\neach dataset using the same language classi-\nﬁer from Section 3.7. We did this since\nprofanity-checker is built for English and\nother languages may improperly impact the results.\nFor instance, the German nominative/accusative\nfeminine/plural deﬁnite article \"die\" is ﬂagged as\nbeing profane regardless of context. We split each\nsentence into words and computed the percentage\nof words that are ﬂagged as profane for each com-\nponent of the Pile. We emphasize that this method-\nology is only a proxy for profanity, given the com-\nplexity of determining whether a given word or\nphrase is profane in context.\nAs shown in Figure 8, the Pile as a whole appears\nless profane than Pile-CC. Further, the majority of\nPile components appear less profane than Pile-CC\nas well.\nWe also broke each dataset down on a sentence\nlevel, to allow profanity-checker to check\nentire sentences. Splitting datasets by sentence\nallows for additional context to be considered when\ndetermining whether content is pejorative. Our\nresults are shown in Figure 12.\n6.4 Bias and Sentiment Co-occurrence\nAs language models may pick up unexpected biases\nfrom the training data, we performed a preliminary\nanalysis of the different components that make up\nthe Pile. Because models with different charac-\nteristics may be trained on the Pile, we aimed to\ndocument the biases of the data and not a speciﬁc\nmodel. We primarily focus on co-occurrence tests,\nwhere we analyzed what words occur in the same\nsentence as other speciﬁc words. Using this infor-\nmation, we can estimate what words strongly bias\ntowards a category word, as well as calculate the\ngeneral sentiment of surrounding words.\nWe focused our analysis on gender, religion, and\nrace. Our goal is to provide users of this dataset\nwith preliminary guidance on how the different\ncomponents are biased so that they can make deci-\nsions on which components to train on.\nAll tables and ﬁgures in this section can be found\nin the Appendix.\n6.4.1 Gender\nWe computed gender associations by computing co-\noccurrences for binary pronouns. For each word,\nwe computed the difference in the rate it co-occurs\nwith \"he\" and \"she\"14 and weighed it by the square\nroot of its frequency. We report the top 15 most\nbiased adjectives or adverbs (Loper and Bird, 2002)\nfor each in Table 10. We see that words like “mil-\nitary”, “criminal”, and “offensive” strongly bias\ntowards men, while “little”, “married”, “sexual”,\nand “happy” bias towards women.\nIn addition, we computed the average senti-\nment (Baccianella et al., 2010) of words co-\noccurring with the gendered pronouns across each\ndataset in Figure 13. Generally, we ﬁnd no sig-\nniﬁcant sentiment bias towards men or women.\nThis, of course, does not mean that the dataset\nis free of gender bias (as our co-occurrence tests\nshow).\n6.4.2 Religion\nWe computed a similar co-occurrence analysis for\nreligion, which can be found in Table 11. Like gen-\nder, we ﬁnd that these co-occurrences reﬂect how\nthese terms are used in pockets of online discourse.\nFor example, “radical” co-occurs with “muslim” at\na high rate, while “rational” often co-occurs with\n“atheist”. This analysis also demonstrates some\nof the limitations of a purely co-occurrence based\nanalysis. For example, “religious” often co-occurs\nwith “atheist”, which likely reﬂects the type of con-\nversations in which the word “atheist” is likely to\noccur as opposed to a descriptor of “atheist”.\n14We chose to only study male and female pronouns as a\nsimplifying assumption. Studying “they” would require us to\nisolate its usage as a singular noun.\n12\nFigure 7: Log perplexity of 16-topic LDA trained on Pile-CC, on other Pile components. Dotted line indicates log\nperplexity of the topic model on OpenWebText2. Higher indicates a larger topical divergence from Pile-CC.\nFigure 8: Percentage of words classiﬁed as profane in\nthe Pile. The percentage of the CC component and the\nweighted mean of the Pile as a whole are shown as hor-\nizontal lines.\nIn addition, we computed the average sentiment\nof co-occurrences across each of the constituent\ndatasets in Figure 14. Over the entire dataset,\nwe ﬁnd that “Buddhist” has the highest sentiment,\nfollowed by “Hindu”, “Christian”, “Atheist”, and\n“Muslim”. Notably, “Jew” is the lowest, perhaps\nreﬂecting its historical use as a pejorative.\n6.4.3 Race\nFinally, we ran the same analysis for racial groups.\nHere, as identiﬁers like “black” or “white” of-\nten do not indicate race, we instead compute co-\noccurences with phrases like “black man” or “white\nwoman”.\nWe show the top 15 most biased words for each\ndemographic in Table 12. Once again, we found\nthat the co-occurrences reﬂect the context in which\nthese terms are used. For example, the 4 most\nbiased words for “black” are “unarmed”, “civil”,\n“criminal”, and “scary”.\nSimilar to above, we compute the average senti-\nment of co-occurring words. We report the average\nsentiment numbers in Table 13. We ﬁnd that “his-\npanic/latino” narrowly edges out “asian” for the\nhighest sentiment, followed by “white”. On the\nother hand, “black” had the lowest sentiment, at\n-0.15.\nWe note that for all demographics, the average sen-\ntiment is negative. We hypothesize that this is due\nto the speciﬁc context for which the phrases we use\nto compute co-occurrences appear. For example, it\nis often quite common for news articles to describe\nsuspects as an “asian man”.\n6.5 Author Consent and Public Data\nAnother issue with the use of texts in natural lan-\nguage processing research is consent. Although\none is typically not legally obligated to receive the\npermission of an author to train a NLP algorithm on\ntheir work15, many consider doing so a moral obli-\n15Laws vary by country. For a discussion of US law, see\nSection 7.1\n13\ngation or a good measure to guard against misuse\n(Obar, 2020; Prabhu and Birhane, 2020). On the\nother hand, there is signiﬁcant disagreement sur-\nrounding the ethics of repurposing data protected\nby terms of servicein research contexts (Vitak et al.,\n2016; Fiesler et al., 2020), particularly given the\npower asymmetries inherent in digital platforms,\nwhich often close off independent researchers from\ninvestigating public data while simultaneously com-\npelling users to consent to its private use (Halavais,\n2019).\nWhile much of the Pile’s data comes from sources\nthat have expressly consented to its wider dissemi-\nnation and use in research, researchers often fail to\nclearly document where their data came from and\nunder what terms its use was consented to. In light\nof this, we felt it appropriate to release the Pile with\ntransparency around how the authors of its data\nhave indicated that that data can be used.\nTo provide needed nuance to our discussion of con-\nsent, we identiﬁed three tiers of availability for\npublic use. Public data is data which is freely and\nreadily available on the internet. This primarily\nexcludes data which is pay-walled (regardless of\nhow easy that paywall is to bypass) and data which\ncannot be easily obtained but can be obtained, e.g.\nthrough a torrent or on the dark web. Terms of\nService (ToS) compliant data is data which is ob-\ntained and used in a fashion that is known to be\nconsistent with the terms of service of the data host.\nData with authorial consent is data for which the\noriginal authors of the work consented to the use\nof their data, or where a reasonable person could\nnot assume that their data would not be used for\npurposes such as research. ToS compliant data and\nauthorial consented data differ in two main ways:\nIt is important to keep in mind that people typically\ndo not read Terms of Service, and additionally that\nbeing ToS-compliant does not entail authorial con-\nsent. We adopted a strict model of consent, where\nambiguous or unknown consent is treated as non-\nconsensual.\nTable 5 summarizes our understanding of the status\nof each of the datasets within the Pile. Datasets\nmarked with a \u0013are compliant in the relevant re-\nspects, though a couple datasets are worth remark-\ning on in particular. Book3 and OpenSubtitles are\nbeing used in a fashion that is consistent with the\nterms of service of the data host. However, this is\nsomewhat misleading in that the data host is not\nauthorized to post the data online by the parties\nthat own it. The Enron Emails dataset was not\ncollected with the permission of the authors, but\nwas collected by the U.S. government as part of\na criminal investigation. While the people whose\nemails are in the Enron dataset are aware of this\nfact, they were not given the ability to consent to\nits inclusion in any way.\nThere are ﬁve datasets included in the Pile that were\nnot collected and distributed in a ToS compliant\nfashion and for which the authors had no ability\nto consent to their data being used. Each of these\ndatasets are widely used, both in the NLP litera-\nture and the world at large. With the exception\nof the YouTube Subtitles dataset, each of these\ndatasets were published by researchers and are\npassed around freely on the internet. The YouTube\nSubtitles dataset was created by us for this project,\nusing a very popular unofﬁcial API that is both\nwidely used and easily obtainable on Pip, Conda,\nand GitHub, among other places. Given the pro-\ncessing applied and the difﬁculty of identifying par-\nticular ﬁles in the Pile, we feel that our use of these\ndatasets does not constitute signiﬁcantly increased\nharm beyond that which has already been done by\nthe widespread publication of these datasets.\n7 Implications and Broader\nImpacts\nThe Pile represents yet another stepping stone\nalong the path of scaling models and datasets to\never larger sizes and capabilities. There are many\nserious concerns about how the emergence of pro-\ngressively stronger AI systems will inﬂuence the\nwider world (Brundage et al., 2018; Amodei et al.,\n2016; Bostrom and Yudkowsky, 2014; Bostrom,\n2014; Critch and Krueger, 2020), and we believe\nthat they merit serious thought. In this section\nwe discuss the legal ramiﬁcations of the Pile, and\nthen consider the impact of the Pile to AI align-\nment from two angles: accelerating AI timelines\nand the dangers posed by unaligned language mod-\nels.\n7.1 Legality of Content\nWhile the machine learning community has be-\ngun to discuss the issue of the legality of training\nmodels on copyright data, there is little acknowl-\nedgment of the fact that the processing and dis-\ntribution of data owned by others may also be a\nviolation of copyright law. As a step in that direc-\n14\nComponent Public ToS Author\nPile-CC \u0013 \u0013\nPMC \u0013 \u0013 \u0013\nBooks3 \u0013\nOWT2 \u0013\nArXiv \u0013 \u0013 \u0013\nGithub \u0013 \u0013\nFreeLaw \u0013 \u0013 \u0013\nStack Exchange \u0013 \u0013 \u0013\nUSPTO \u0013 \u0013 \u0013\nPubMed \u0013 \u0013 \u0013\nPG-19 \u0013 \u0013\nOpenSubtitles \u0013\nWikipedia \u0013 \u0013 \u0013\nDM Math \u0013 \u0013 \u0013\nUbuntu IRC \u0013 \u0013 \u0013\nBookCorpus2 \u0013\nEuroParl \u0013 \u0013 \u0013\nHackerNews \u0013 \u0013\nYTSubtitles \u0013\nPhilPapers \u0013 \u0013 \u0013\nNIH \u0013 \u0013 \u0013\nEnron Emails \u0013 \u0013\nTable 5: Types of consent for each dataset\ntion, we discuss the reasons we believe that our\nuse of copyright data is in compliance with US\ncopyright law.16\nUnder pre (1984) (and afﬁrmed in subsequent\nrulings such as aff (2013); Google (2015)), non-\ncommercial, not-for-proﬁt use of copyright media\nis preemptively fair use. Additionally, our use is\ntransformative, in the sense that the original form\nof the data is ineffective for our purposes and our\nform of the data is ineffective for the purposes of\nthe original documents. Although we use the full\ntext of copyright works, this is not necessarily dis-\nqualifying when the full work is necessary (ful,\n2003). In our case, the long-term dependencies in\nnatural language require that the full text be used in\norder to produce the best results (Dai et al., 2019;\nRae et al., 2019; Henighan et al., 2020; Liu et al.,\n2018).\nCopyright law varies by country, and there may be\n16This discussion does not, and is not intended to, constitute\nlegal advice; rather, it is a general discussion of law. Only your\nattorney can provide assurances that the information contained\nherein is applicable or appropriate to a particular situation. If\nin doubt, it is always advisable to speak to an intellectual\nproperty attorney.\nadditional restrictions on some of these works in\nparticular jurisdictions. To enable easier compli-\nance with local laws, the Pile reproduction code is\navailable and can be used to exclude certain com-\nponents of the Pile which are inappropriate for\nthe user. Unfortunately, we do not have the meta-\ndata necessary to determine exactly which texts are\ncopyrighted, and so this can only be undertaken at\nthe component level. Thus, this should be be taken\nto be a heuristic rather than a precise determina-\ntion.\n7.2 Acceleration of AI Timelines\nThere is serious concern that AI systems may soon\nbe meaningfully more capable than humans in all\nrelevant economic tasks (Grace et al., 2018; Yud-\nkowsky, 2013). Relatedly, there are serious unre-\nsolved questions surrounding how to properly align\nsuch powerful AI systems with human interests\n(Bostrom and Yudkowsky, 2014; Russell, 2019;\nBostrom, 2014; Amodei et al., 2016) and generally\navoid morally catastrophic outcomes (Sotala and\nGloor, 2017; Shulman and Bostrom, 2020). As\nsuch, it has been argued that accelerating the de-\nvelopment of such powerful AI systems may be\nundesirable before these concerns have been more\nadequately addressed (Bostrom, 2014).\nThere are several pragmatic responses to this\nview:\n1. Due to human competition, curiosity, and cul-\ntural diversity, halting technological develop-\nment is incredibly difﬁcult, if not impossible.\n(Russell, 2019) (Critch and Krueger, 2020)\n2. AI development is experimental in nature:\nThe alignment problem can only be solved\nthrough development, testing and (hopefully\nnon-existential) failure.\n3. High powered language models, along with\ntheir more general successors, must be capa-\nble of viewing morally problematic content\nwithout adopting it in their output. We elabo-\nrate on this in the following section.\nWith this in mind, we accept the reality that the Pile\ncould potentially accelerate AI timelines. However,\nwe hope our efforts to establish best practices, such\nas thoroughly documenting the contents of our data,\nwill help encourage diligence for downstream re-\nsearchers on alignment problems.\n15\n7.3 Negative LM Output\nThere has been much discussion about the possi-\nble negative effects of powerful language models\nin the world (Brown et al., 2020; Brundage et al.,\n2018). Some of these possible problems, such as\nthe ability to mass produce low quality content\nfor the purpose of Search Engine Optimization,\nare inherent problems to the way online content\nis distributed, and cannot be stopped by those de-\nveloping language models alone. Directly solving\nthese problems would require sweeping changes\nto the architecture of the Internet, such as vastly\nexpanded Public Key Infrastructure and distributed\nauthentication of identity (Ferguson and Schneier,\n2003).\nAnother concern is that training such models on\nhuge datasets will almost inevitably require them to\nhave undesirable content in their training sets, such\nas that promoting hateful stereotypes (Christian,\n2020). Having models output undesirable content\nis, by deﬁnition, undesirable, but we believe that\nattacking this problem from the training set side\nis unproductive and ultimately leads us away from\noptimal solutions. If a person reads a racist piece\nof content, they do not then immediately adopt its\nracist views—they may be capable of doing so, but\ncan decide not to. This capacity to understand un-\ndesirable content and then decide to ignore it is an\nessential future research direction. Not only would\nthis allow models to use “dirtier\" data with less\nconcern, but also to use their gained knowledge\nto better understand what not to do. We recognize\nthat, despite recent progress in human-guided learn-\ning (Stiennon et al., 2020), the technology is not\nyet at this stage, and have thus made a number of\neditorial decisions as described in this paper. How-\never, this approach seems essential to the future\nof these models and AI more broadly, and more\nresearch is needed.\n8 Related Work\nSelf-supervised training of natural language pro-\ncessing models on large, unlabeled text corpora,\nhas seen widespread adoption in the ﬁeld. Word\nrepresentation models such as GloVe (Pennington\net al., 2014) and word2vec (Mikolov et al., 2013)\nwere trained on datasets such as Wikipedia, Giga-\nword (Graff et al., 2003), or a non-public Google\nNews corpus. More recently, language models\n(Radford et al., 2018, 2019; Brown et al., 2020;\nRosset, 2019; Shoeybi et al., 2019) and masked lan-\nguage models (Devlin et al., 2019; Liu et al., 2019;\nRaffel et al., 2019) have been trained on datasets\nsuch as Wikipedia, BookCorpus (Zhu et al., 2015),\nRealNews (Zellers et al., 2019), CC-Stories (Trinh\nand Le, 2018), and other Internet scrape-derived\ndatasets discussed below. Other datasets such as\nWikiText (Stephen et al., 2016) have also been used\nin similar self-supervised training.\nAs data requirements for language modeling have\ngrown, the ﬁeld has turned towards Internet scrapes\nfor large-scale datasets (Gokaslan and Cohen,\n2019), with Common Crawl being particularly\nprevalent. Works such as Brown et al. (2020); Wen-\nzek et al. (2019); Suárez et al. (2019b); Raffel et al.\n(2019) have relied on Common Crawl to build train-\ning datasets for large-scale models. However, these\nworks often highlight the difﬁculty of cleaning and\nﬁltering the Common Crawl data, and often high-\nlight the resulting data quality as a determining\nfactor of model capability.\nIt has also been increasingly common practice\nto combine multiple datasets when training lan-\nguage models. For instance, GPT (Radford et al.,\n2018) was trained on Wikipedia and BookCorpus,\nwhereas GPT-3 (Brown et al., 2020) was trained\non Wikipedia, two ﬁction datasets, and two web-\nscraped datasets. The Pile continues the trend of\ncombining large-scale web-scrapes with smaller,\nhigher-quality datasets that capture knowledge we\nbelieve would be most beneﬁcial to training lan-\nguage models.\nThe two most comparable publicly available\ndatasets to the Pile are CC-100 (Wenzek et al.,\n2019) and C4/mC4 (Raffel et al., 2019). C4 is\ncomparably-sized to the Pile, while mC4 and CC-\n100 are larger, multilingual datasets. However,\nC4/mC4 require immense computational resources\nto preprocess the data, with its maintainers even rec-\nommending the use of a distributed cloud service,17\nsetting a high bar of entry to using these datasets.\nCC-100 is directly downloadable and pre-cleaned;\nhowever, its English portion is much smaller than\nthe Pile. Importantly, these three datasets are all de-\nrived entirely from Common Crawl—as discussed\nabove, the current best practice in training large-\nscale language models involve using both large web\nscrapes and more targeted, higher-quality datasets,\n17https://www.tensorflow.org/datasets/\ncatalog/c4\n16\nwhich the Pile directly addresses.\n9 Acknowledgments\nThe authors would like to thank TensorFlow Re-\nsearch Cloud for providing the computational re-\nsources for the evaluation and OpenAI for provid-\ning access and credits for the OpenAI API for GPT-\n3 evaluation.\nWe would also like to thank Farrukh Raman, JR\nSmith, and Michelle Schmitz for reviewing the\nmanuscript.\nReferences\n1984. Sony corp. of america v. universal city studios,\ninc.\n2003. Kelly v. arriba soft corp.\n2013. Righthaven llc v. hoehn.\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul\nChristiano, John Schulman, and Dan Mané. 2016.\nConcrete problems in AI safety. arXiv preprint\narXiv:1606.06565.\nStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-\ntiani. 2010. Sentiwordnet 3.0: An enhanced lexical re-\nsource for sentiment analysis and opinion mining. In\nLREC. European Language Resources Association.\nEmily M Bender and Batya Friedman. 2018. Data\nstatements for natural language processing: Toward\nmitigating system bias and enabling better science.\nTransactions of the Association for Computational Lin-\nguistics, 6:587–604.\nStella Biderman. 2021. Data statement for the Pile.\narXiv preprint arXiv.\nStella Biderman, Kieran Bicheno, and Leo Gao. 2021.\nDatasheet for the Pile. arXiv preprint arXiv.\nStella Biderman and Walter J. Scheirer. 2020. Pitfalls\nin machine learning research: Reexamining the devel-\nopment cycle. NeurIPS “I Can’t Believe It’s Not Bet-\nter!” Workshop.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of machine\nLearning research, 3(Jan):993–1022.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. arXiv\npreprint arXiv:2005.14050.\nNick Bostrom. 2014. Superintelligence: Paths, Dan-\ngers, Strategies. Oxford University Press, Inc.\nNick Bostrom and Eliezer Yudkowsky. 2014. The\nethics of artiﬁcial intelligence. The Cambridge hand-\nbook of artiﬁcial intelligence, 1:316–334.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al. 2020. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165.\nMiles Brundage, Shahar Avin, Jack Clark, Helen\nToner, Peter Eckersley, Ben Garﬁnkel, Allan Dafoe,\nPaul Scharre, Thomas Zeitzoff, Bobby Filar, et al.\n2018. The malicious use of artiﬁcial intelligence: Fore-\ncasting, prevention, and mitigation. arXiv preprint\narXiv:1802.07228.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine Lee,\nAdam Roberts, Tom Brown, Dawn Song, Ulfar Erlings-\nson, Alina Oprea, and Colin Raffel. 2020. Extracting\ntraining data from large language models.\nIsaac Caswell, Theresa Breiner, Daan van Esch, and\nAnkur Bapna. 2020. Language id in the wild: Unex-\npected challenges on the path to a thousand-language\nweb text corpus. arXiv preprint arXiv:2010.14571.\nBrian Christian. 2020. The Alignment Problem: Ma-\nchine Learning and Human Values . WW Norton &\nCompany.\nAlina Maria Ciobanu, Liviu P Dinu, and Andrea Sgarro.\n2017. Towards a map of the syntactic similarity of lan-\nguages. In International Conference on Computational\nLinguistics and Intelligent Text Processing, pages 576–\n590. Springer.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov. 2020. Unsupervised cross-\nlingual representation learning at scale. In Proceedings\nof the 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 8440–8451, Online. As-\nsociation for Computational Linguistics.\nMarta R Costa-jussà, Roger Creus, Oriol Domingo,\nAlbert Domínguez, Miquel Escobar, Cayetana López,\nMarina Garcia, and Margarita Geleta. 2020. Mt-\nadapted datasheets for datasets: Template and reposi-\ntory. arXiv preprint arXiv:2005.13156.\nAndrew Critch and David Krueger. 2020. AI Re-\nsearch Considerations for Human Existential Safety\n(ARCHES). Preprint at acritch.com/arches.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime\nCarbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of deep\nbidirectional transformers for language understanding.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) . Association for\nComputational Linguistics.\n17\nIstván Endrédy and Attila Novák. 2013. More effec-\ntive boilerplate removal – the GoldMiner algorithm. In\nPolibits.\nNiels Ferguson and Bruce Schneier. 2003. Practical\nCryptography. John Wiley & Sons.\nCasey Fiesler, Nathan Beard, and Brian C Keegan.\n2020. No robots, spiders, or scrapers: Legal and ethical\nregulation of data collection methods in social media\nterms of service. In Proceedings of the International\nAAAI Conference on Web and Social Media, volume 14,\npages 187–196.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione,\nJennifer Wortman Vaughan, Hanna Wallach, Hal\nDaumé III, and Kate Crawford. 2018. Datasheets for\ndatasets. arXiv preprint arXiv:1803.09010.\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nAuthors Guild v. Google. 2015. . Docket No. 13-4829-\ncv, 804:202.\nKatja Grace, John Salvatier, Allan Dafoe, Baobao\nZhang, and Owain Evans. 2018. When will AI exceed\nhuman performance? evidence from AI experts. Jour-\nnal of Artiﬁcial Intelligence Research, 62:729–754.\nDavid Graff, Junbo Kong, Ke Chen, and Kazuaki\nMaeda. 2003. English gigaword. Linguistic Data Con-\nsortium, Philadelphia, 4(1):34.\nDeclan Groves and Andy Way. 2006. Hybridity in mt:\nExperiments on the Europarl corpus. In Proceeedings\nof the 11th Annual conference of the European Associ-\nation for Machine Translation (EAMT 2006).\nAlexander Halavais. 2019. Overcoming terms of ser-\nvice: a proposal for ethical distributed research. Infor-\nmation, Communication & Society, 22(11):1567–1581.\nChris Hardin. 2018. How to shufﬂe a big dataset.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun, Tom B\nBrown, Prafulla Dhariwal, Scott Gray, et al. 2020. Scal-\ning laws for autoregressive generative modeling. arXiv\npreprint arXiv:2010.14701.\nMatthew Hoffman, Francis Bach, and David Blei.\n2010. Online learning for latent dirichlet allocation.\nadvances in neural information processing systems ,\n23:856–864.\nDirk Hovy and Shannon L Spruit. 2016. The social im-\npact of natural language processing. In Proceedings of\nthe 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers), pages\n591–598.\nBen Hutchinson, Vinodkumar Prabhakaran, Emily\nDenton, Kellie Webster, Yu Zhong, and Stephen De-\nnuyl. 2020. Social biases in NLP models as bar-\nriers for persons with disabilities. arXiv preprint\narXiv:2005.00813.\nEun Seo Jo and Timnit Gebru. 2020. Lessons from\narchives: Strategies for collecting sociocultural data in\nmachine learning. In Proceedings of the 2020 Con-\nference on Fairness, Accountability, and Transparency,\npages 306–316.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nBryan Klimt and Yiming Yang. 2004. The Enron cor-\npus: A new dataset for email classiﬁcation research.\nIn European Conference on Machine Learning , pages\n217–226. Springer.\nSosuke Kobayashi. 2018. Homemade bookcor-\npus. https://github.com/BIGBALLON/\ncifar-10-cnn.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In MT summit , vol-\nume 5, pages 79–86. Citeseer.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. arXiv preprint\narXiv:2006.16668.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summarizing\nlong sequences. arXiv preprint arXiv:1801.10198.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. RoBERTa:\nA robustly optimized BERT pretraining approach.\narXiv preprint arXiv:1907.11692.\nEdward Loper and Steven Bird. 2002. NLTK: The Nat-\nural Language Toolkit. In Proceedings of the ACL\nWorkshop on Effective Tools and Methodologies for\nTeaching Natural Language Processing and Computa-\ntional Linguistics, pages 62–69. Somerset, NJ: Associa-\ntion for Computational Linguistics. http://arXiv.\norg/abs/cs/0205028.\nJohn MacFarlane. 2006–2020. Pandoc: a universal\ndocument converter.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representations\nof words and phrases and their compositionality. InAd-\nvances in Neural Information Processing Systems, vol-\nume 26, pages 3111–3119. Curran Associates, Inc.\nJonathan A Obar. 2020. Sunlight alone is not a disin-\nfectant: Consent and the futility of opening big data\nblack boxes (without assistance). Big Data & Society ,\n7(1):2053951720935615.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word repre-\n18\nsentation. In Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1532–1543.\nVinay Uday Prabhu and Abeba Birhane. 2020. Large\nimage datasets: A pyrrhic win for computer vision?\narXiv preprint arXiv:2006.16923.\nShawn Presser. 2020. Books3. https:\n//twitter.com/theshawwn/status/\n1320282149329784833.\nAlec Radford, Karthik Narasimhan, Time Salimans,\nand Ilya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning. Technical report,\nOpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar,\nChloe Hillier, and Timothy P Lillicrap. 2019. Compres-\nsive transformers for long-range sequence modelling.\narXiv preprint.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits of\ntransfer learning with a uniﬁed text-to-text transformer.\narXiv preprint arXiv:1910.10683.\nInioluwa Deborah Raji and Jingying Yang. 2019.\nABOUT ML: Annotation and benchmarking on under-\nstanding and transparency of machine learning lifecy-\ncles. arXiv preprint arXiv:1912.06166.\nC. Radhakrishna Rao. 1961. Generation of random per-\nmutations of given number of elements using random\nsampling numbers. Sankhy¯a: The Indian Journal of\nStatistics, Series A (1961-2002), 23(3):305–307.\nRadim Rehurek, Petr Sojka, et al. 2011. Gen-\nsim—statistical semantics in python. NLP Centre, Fac-\nulty of Informatics, Masaryk University.\nC Rosset. 2019. Turing-NLG: A 17-billion-parameter\nlanguage model by Microsoft. Microsoft Blog.\nS. Russell. 2019. Human Compatible: Artiﬁcial Intelli-\ngence and the Problem of Control. Penguin Publishing\nGroup.\nDavid Saxton, Edward Grefenstette, Felix Hill, and\nPushmeet Kohli. 2019. Analysing mathematical rea-\nsoning abilities of neural models. arXiv preprint\narXiv:1904.01557.\nIsmaïla Seck, Khouloud Dahmane, Pierre Duthon,\nand Gaëlle Loosli. 2018. Baselines and a datasheet\nfor the Cerema AWP dataset. arXiv preprint\narXiv:1806.04016.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catanzaro.\n2019. Megatron-LM: Training multi-billion parameter\nlanguage models using gpu model parallelism. arXiv\npreprint arXiv:1909.08053.\nCarl Shulman and Nick Bostrom. 2020. Sharing the\nworld with digital minds. preprint.\nKaj Sotala and Lukas Gloor. 2017. Superintelligence\nas a cause or cure for risks of astronomical suffering.\nInformatica, 41(4).\nRobyn Speer. 2019. ftfy. Zenodo. Version 5.5.\nMerity Stephen, Xiong Caiming, Bradbury James, and\nRichard Socher. 2016.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. Advances in\nNeural Information Processing Systems, 33.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019a. Asynchronous pipeline for process-\ning huge corpora on medium to low resource infrastruc-\ntures. In 7th Workshop on the Challenges in the Man-\nagement of Large Corpora (CMLC-7). Leibniz-Institut\nfür Deutsche Sprache.\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019b. Asynchronous pipeline for process-\ning huge corpora on medium to low resource infrastruc-\ntures. In 7th Workshop on the Challenges in the Man-\nagement of Large Corpora (CMLC-7). Leibniz-Institut\nfür Deutsche Sprache.\nAnja Thieme, Danielle Belgrave, and Gavin Doherty.\n2020. Machine learning in mental health: A system-\natic review of the HCI literature to support the devel-\nopment of effective and implementable ML systems.\nACM Transactions on Computer-Human Interaction\n(TOCHI), 27(5):1–53.\nJ. Tiedemann. 2016. Finding alternative translations in\na large corpus of movie subtitles. Proceedings of the\n10th International Conference on Language Resources\nand Evaluation (LREC 2016).\nTrieu H. Trinh and Quoc V . Le. 2018. A simple method\nfor commonsense reasoning. CoRR, abs/1806.02847.\nHans Van Halteren. 2008. Source language markers in\nEuroparl translations. In Proceedings of the 22nd In-\nternational Conference on Computational Linguistics\n(Coling 2008), pages 937–944.\nJessica Vitak, Katie Shilton, and Zahra Ashktorab.\n2016. Beyond the Belmont principles: Ethical chal-\nlenges, practices, and beliefs in the online data research\ncommunity. In Proceedings of the 19th ACM Confer-\nence on Computer-Supported Cooperative Work & So-\ncial Computing, pages 941–953.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Armand\nJoulin, and Edouard Grave. 2019. CCNet: Extracting\nhigh quality monolingual datasets from web crawl data.\narXiv preprint arXiv:1911.00359.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe\n19\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma,\nYacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao,\nSylvain Gugger, Mariama Drame, Quentin Lhoest, and\nAlexander M. Rush. 2020. Transformers: State-of-\nthe-art natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations , pages\n38–45, Online. Association for Computational Linguis-\ntics.\nEllery Wulczyn, Nithum Thain, and Lucas Dixon. 2016.\nWikipedia detox.\nEliezer Yudkowsky. 2013. Intelligence explosion mi-\ncroeconomics. Machine Intelligence Research Insti-\ntute, accessed online October, 23:2015.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. dÁlché-Buc, E. Fox, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems 32 ,\npages 9054–9065. Curran Associates, Inc.\nVictor Zhou. 2019. Building a better profanity detec-\ntion library with scikit-learn.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja Fi-\ndler. 2015. Aligning books and movies: Towards story-\nlike visual explanations by watching movies and read-\ning books. In Proceedings of the IEEE international\nconference on computer vision, pages 19–27.\n20\nAppendices\nA Contributions\nAll authors contributed to the design of the research\nproject and the writing of the paper. Additionally,\nauthors contributed as follows:\nLeo Gao led the project, implemented the main\nPile codebase, contributed to the model training\ncode, performed the evaluations and the language\nanalysis, interpreted the perplexity analysis results,\nimplemented the processing to create the ﬁnal data,\nand processed Pile-CC, PubMed Central, ArXiv,\nand Ubuntu IRC.\nStella Biderman led the data analysis, the broader\nimpact analysis, and the data documentation, and\ncoordinated the project. She also wrote the anal-\nysis of structural statistics, authorial consent, and\ncopyright law.\nSid Black implemented the model training and\nevaluation code and processed YouTube Subtitles,\nStack Exchange, and GitHub.\nLaurence Golding implemented deduplication,\nperformed the n-gram analysis, and processed\nOpenWebText2.\nTravis Hoppe processed FreeLaw, Pubmed Ab-\nstracts, ExPorter, and PhilPapers.\nCharles Foster performed the topic modeling anal-\nysis, contributed to the discussion of authorial con-\nsent, and processed USPTO Backgrounds.\nJason Phangimplemented and performed the GPT-\n2/3 perplexity analysis and advised the project.\nHorace He performed the bias and sentiment anal-\nysis.\nAnish Thite implemented and performed the pro-\nfanity analysis and processed Hacker News.\nNoa Nabeshima processed GitHub.\nShawn Presser processed BookCorpus2.\nConnor Leahy wrote the alignment implication\nanalysis and the model training code.\nB Excluded Datasets\nIn the course of building the Pile, we considered\nincluding and ultimately decided to not use sev-\neral datasets. We excluded several datasets on the\ngrounds that they were too small to be worth spend-\ning time on or because the English component of\nthe data did not merit inclusion on its own. How-\never we also decided to exclude several data sets\nfor other reasons, which we document here for\ntransparency:\n1. US Congressional Record. The ofﬁcial\nrecord of the United States Congress ( 1800\n– today) records important points of debate at\nthe highest levels of American government. It\nreﬂects the opinions and biases of the polit-\nical class over the past 200 years, including\nsegregationism and xenophobia. In particular,\nwe found a large quantity of extremely racist\ncontent that we did not feel appropriate for a\ndataset intended for general-purpose language\nmodeling.\n2. Fanﬁction. Hundreds of GiB of fanﬁction\nhas been written and put online, primarily\non the websites www.fanfiction.net\nand www.https://archiveofourown.\norg/. This represents a signiﬁcant untapped\nresource for language modeling as it is al-\nmost exclusively short-form ﬁction, a writing\nstyle that is not represented in most language\nmodeling datasets. We ultimately decided to\nexclude fanﬁction on logistical grounds: we\nfound other sources of data that were easier to\nobtain.\n3. Literotica. Literotica is a website where users\ncan upload short-form erotic ﬁction. We had\noriginally planned on including it in the Pile\nand even went as far as scraping and process-\ning it. However we decided to not include it\nfor several reasons. Firstly, once we decided\nto exclude fanﬁction, Literotica represented\nour sole source of short-form ﬁction, which\nwould likely lead to undesirable biases in the\ntrained model. Secondly, Literotica would\nrequire signiﬁcantly more investigation, as-\nsessment, and care than we spent on the other\ndatasets. Thirdly, Literotica contains a signiﬁ-\ncant amount of stereotyping, including racial\nfetishes. While Literotica is likely usable for\nsome tasks, we are not comfortable including\nit in the Pile.\nC Dataset Details\nThis section contains additional information about\neach dataset listed in Section 2, including how it\nwas obtained, how it was processed, and any other\ndetails relevant for replication. The intent of this\nsection is to provide as much detail as possible,\nso that Pile can be replicated in the future if nec-\nessary, and so that any future processing of these\n21\nand similar datasets can use or improve on our\nmethods. As such, all code created for processing\nhas been made publicly available under permissive\nopen source licenses and is referenced in footnotes\nwhere applicable.\nC.1 Pile-CC\nWe extract Common Crawl using jusText (Endrédy\nand Novák, 2013). Our ﬁltering implementation\nuses a classiﬁer trained against the OpenWebText2\ndataset. We process only a small fraction of the\navailable Common Crawl data; we break the list\nof urls to individual WARC ﬁles from 2013 to\n2020 into 3679 chunks and process 22 random\nchunks.\nC.1.1 W ARC vs WET\nCommonCrawl data is available in two main for-\nmats: Web ARChive (W ARC) ﬁles, which contain\na full record of the crawl as well as the raw HTML\nof the webpage, and WET ﬁles, which contain pre-\nextracted versions of the contents of the WARC\nﬁles. The WET ﬁles have poor quality, often con-\ntaining large amounts of boilerplate text like menus\nand page footers, but due to the lower bandwidth\nand computation requirements necessary to use\nWET ﬁles, prior work based on CC have mainly\nfocused on using WET ﬁles while applying clean-\ning such as document level ﬁltering (Brown et al.,\n2020; Wenzek et al., 2019), or n-sentence level\ndeduplication with very aggressive heuristics (Raf-\nfel et al., 2019).\nWe do not believe that document level ﬁltering\nis sufﬁcient for WET ﬁles because many of the\nissues with WET ﬁles stem from intra-document\nboilerplate. We also ﬁnd many of the heuristics\nused in Raffel et al. (2019), such as the removal\nof all lines without terminal punctuation, the word\n\"javascript\", and 3-sentence deduplication to be too\naggressive.\nC.1.2 Extraction\nIn addition to jusText, we also considered Traﬁ-\nlatura, Newspaper, Goose3, and DragNet. While\nwe were originally intending on creating an extrac-\ntion benchmark, this proved infeasible given our\navailable resources, and we chose jusText based on\nvisual inspection of the output. In inspection, we\nnoticed that jusText has the characteristic that it dis-\ncards more data than many other extractors, which\nis not a major drawback given the large volume\nof CC data available. This was as expected, given\njusText’s intended application for text corpora cre-\nation. In contrast, traﬁlatura is, for instance, better\nat preserving the structure of the website faithfully,\noften correctly extracting elements such as tables,\nbut it kept too much unnecessary boilerplate. Had\nwe used traﬁlatura, we would have required an addi-\ntional intra-page ﬁltering step to remove boilerplate\nfrom the page.\nC.1.3 Languages\nWhile jusText does technically support several\nother languages, the quality on those languages\nis worse than on English as many constants in the\nalgorithm are speciﬁcally tuned for English. Ad-\nditionally, jusText is completely unable to handle\nlanguages such as Chinese and Japanese, which do\nnot use spaces to delimit words.\nDue to the difﬁculty of maintaining an acceptable\nlevel of extraction quality across all languages, we\ndecided to restrict the scope of the CC dataset to\nonly English and leave a high-quality, fully multi-\nlingual, WARC-based CC-based dataset to future\nwork. To ﬁlter for only English, we use the py-\ncld2 library and only attempt to extract text from\ndocuments where English is the most common lan-\nguage.\nWe use pycld2 instead of fasttext because it is ca-\npable of classifying the language from the HTML\ndirectly, and since jusText requires knowledge of\nthe language of the webpage before extraction. Ad-\nditionally, pycld2 was signiﬁcantly faster than jus-\nText, and by only processing with jusText doc-\numents classiﬁed as English by pycld2, we re-\nduced the required computation by approximately\nhalf.\nExtracting text from websites for language model-\ning, especially for multilingual corpora, is highly\nnontrivial, and we leave the reﬁnement of such\nextraction to future work.\nC.1.4 Filtering\nTo ﬁlter CC for quality, we follow Brown et al.\n(2020) in training a classiﬁer to classify between a\nknown high quality dataset and CC. We use fasttext\nwith an n-gram size of 2. We ran experiments us-\ning both the entire Pile and just OpenWebText2 as\nthe positive examples, with score distributions on\nunseen CC data as shown in Figure 9. We decided\nto use only OpenWebText2 for positive examples\nfor our ﬁnal CC data because of the low sensitivity\n22\nα Filtering Ratio\n1 0.5894\n2 0.3649\n3 0.2390\n4 0.1671\n5 0.1239\n6 0.0974\n7 0.0802\n8 0.0685\n9 0.0602\nTable 6: Filtering Ratios (kept:total) of various settings\nof using the full Pile. We use the same Pareto-\ndistribution thresholding as Brown et al. (2020),\nwith α = 3. Our choice of αtargets the ﬁltering\nratio necessary to ﬁlter our subset of CC to the size\nwe needed. The impact of αon the ﬁltering ratio is\nshown in Table 6.\nC.2 Pubmed Central\nWe use pandoc 1.19.2.4 (MacFarlane, 2006–\n2020) to convert the JATS format data provided by\nPMC to markdown. Afterwards, we remove any\nline beginning with :::, which is used by pandoc\nto indicate html classes in markdown.\nC.3 Books3\nNo additional details.\nC.4 OpenWebText2\nTo produce the dataset, URLs and their associated\nmetadata were ﬁrst extracted from all Reddit sub-\nmissions up to April 2020. URLs were dedupli-\ncated, with each unique URL featuring a list of\nassociated submissions metadata, and an aggre-\ngate score. URLs with an aggregate score of less\nthen 3 were removed. The links were then scraped\nand processed with Newspaper scraper. Dedupli-\ncation was performed at the document level using\nin memory MinHashLSH through the DataSketch\nlibrary.\nBoth ﬁltered and raw versions were produced, with\nthe raw version only deduplicated by URL. The ﬁl-\ntered version contains 65.86 GB of uncompressed\ntext across 17,103,059 documents. The raw version\nis much larger, at 193.89GB of uncompressed text\nacross 69,547,149 documents.\nC.4.1 Extractor Choice\nWe chose to use Newspaper instead of jusText for\nOpenWebText2 for consistency with OpenWeb-\nTextCorpus. Additionally, by using multiple differ-\nent html extractors for different components of the\nPile, we reduce the potential impact of systematic\nbiases from any one extractor negatively impacting\nthe dataset.\nC.5 ArXiv\nWe downloaded the T EX sources of all\npapers on arXiv up to the July 2020\ndump (the last ﬁle included in our data is\narXiv_src_2007_068.tar) via arXiv’s S3\nBulk Source File Access 18, and used pandoc\n1.19.2.4 to convert these source ﬁles to\nMarkdown, discarding any papers which had errors\nduring the conversion process. This yielded a total\nof 1,264,405 papers.\nWe remove any line beginning with:::, which is\nused by pandoc to indicate html classes in mark-\ndown.\nC.6 GitHub\nWe separate the data gathering process into two\nsteps:\n1. Gathering a list of the desired repositories and\ntheir metadata\n2. Extracting all text data useful for language\nmodeling from each repository\nFor the ﬁrst step, mirroring the approach of the\nWebText dataset, we use GitHub ‘stars’ as a proxy\nfor quality, and choose to gather only repositories\nwith more than 100 stars. For practical reasons, we\nalso limit the list of repositories gathered to reposi-\ntories with less than 1GB of ﬁles. Since Github’s\nAPI limits the number of search results to 1000, in\norder to comprehensively gather all repositories we\nneed to create many small queries that each return\nfewer than 1000 results in such a way that every\nrepository of interest will be returned by at least\none of our queries. To achieve this, we bound our\ninitial search by size to return only repositories be-\ntween a lower bound of 0 and 5 bytes. At the time\nof writing, this returns 965 results. For the next\nstep, we set our lower bound one above our previ-\nous upper bound, and decide on a new upper bound\nthat should also return fewer than 1000 results by\n18https://arxiv.org/help/bulk_data_s3\n23\n(a) OpenWebText2\n (b) Full Pile\nFigure 9: Score distribution of documents from Common Crawl given different classiﬁer training data.\nFigure 10: Left: number of new submissions/year\nto arXiv grouped by domain over time. Right:\nfractional submission rates for each of the domains.\nFigure from https://arxiv.org/help/\nstats/2019_by_area/\nusing the results from our last query to estimate our\nnew upper bound as(lowerbound+(1000/(n/r)),\nwhere nis the number of previous results and ris\nthe range of bounds in the previous step.\nThis tends not to overshoot, because Github repos-\nitories follow a power distribution with respect to\nsize, but if it does, we simply use the amount of\nrepositories our new query returned in order to con-\nstruct a new upper bound estimate.\nUsing the gathered list of repositories, we clone\neach one, extract any text-based ﬁles, and discard\nthe rest. Because some repositories took an imprac-\ntical amount of time to clone and/or extract, we set\na hard time limit of 300 seconds for both the git\ncloning and text extraction steps. As such, some\nlarger repositories may only be partially extracted.\nWe also impose a ﬁle size limit of 100kB on ex-\ntracted ﬁles, as we found that the majority of ﬁles\nover that size were typically very repetitive auto-\ngenerated source ﬁles or data ﬁles, and that setting\nthis ﬁle size limit was an effective cleaning step to\nlimit the data to code.\nBecause we wanted to limit the size of the overall\nPile, we randomly sampled 95.0 GiB of the 630.64\nGiB of Github data we collected in total and leave\nquality ﬁltering to future work.\nHowever, we believe code generation will be an in-\ncreasingly important component of language mod-\nels as they continue to scale up and increase in their\nability to generalize. As such, we hope to extend\nthis dataset in future work.\nC.7 FreeLaw\nWe download the court opinions data in bulk from\nCourtListener,19 and extract the raw text using\nBeautifulSoup.\nC.8 Stack Exchange\nTo construct the dataset, we download and parse\nevery Stack Exchange database dump to plaintext\nﬁles. We opt to extract the top three answers\nwith at least three upvotes, discarding all other\nresponses. We only include the plain text ques-\ntion and response and do not incorporate any meta-\ndata. Motivated by large-scale language models’\nfew-shot ability (Brown et al., 2020), we provide\ncontext by prepending all questions and answers\nwith Q:\\n\\n and A:\\n\\n respectively.\nThe resulting dataset contains a total of 15,622,475\ndocuments across a total of 365 Stack Exchanges\nand Meta-Stack Exchanges, the bulk of which is\nfrom StackOverﬂow.\nC.9 USPTO Backgrounds\nThe United States Patent and Trademark Ofﬁce\n(USPTO) has published bulk archives of the full\n19https://www.courtlistener.com/api/\nbulk-info/\n24\ntext of all patents granted in the US from 1976 to\nSeptember 2020. From these archives, we extract\nthe Background sections, along with key grant-\nspeciﬁc metadata, such as the inventor, assignee,\nand classiﬁcation information.\nThe ﬁle format used for storing bulk text US patents\nhas changed over time. Prior to 2002, all of the\ndatasets are in a specialized format called APS\n(Automated Patent System). Since 2002, the data\nis XML encoded. Partially as a function of this\nchange, the location of the \"Background\" section\nhas also shifted. Our converter accounts for these\nstructural shifts and extracts the raw text from each\npatent’s Background.\nC.10 PubMed Abstracts\nAbout one-third of the articles in the dataset were\nmissing or contained a malformed title or abstract\nand were excluded. Additionally, PubMed Cen-\ntral (see Section 2.2) contains full-text resources to\nmany recent publications; any publications which\nalready appear in PMC are excluded from this set.\nTo process the data, we concatenated the title and\nabstract and removed any copyright information.\nThe remaining dataset contains 15,518,009 titles\nand abstracts.\nC.11 Project Gutenberg\nNo additional details.\nC.12 OpenSubtitles\nTo create the text dataset, we simply extract the\nsubtitle text from each XML ﬁle in the English\nlanguage dataset provided by Tiedemann (2016),\ndiscarding any provided metadata.\nC.13 Wikipedia (English)\nWe use the wikipedia/20200301.en dataset\nfrom TensorFlow Datasets.20 We prepend the ti-\ntle to the body of each article, separated by two\nnewlines.\nC.14 DeepMind Mathematics\nWe include instances from the Easy, Medium,\nand Hard components of DeepMind Mathemat-\nics, breaking each curriculum item (such as\nalgebra__polynomial_roots) into 8 KiB\nchunks.\n20https://www.tensorflow.org/datasets/\ncatalog/wikipedia#wikipedia20200301en\nC.15 Ubuntu IRC\nWe processed all logs from July 5, 2004 through\nSeptember 1, 2020.\nTo process the data, all system messages, such\nas joins, disconnects, nick changes, etc. were\ndiscarded, but actions (i.e using /me) were kept.\nTimestamps were removed, and all logs for the\nsame channel in a given week were concatenated\ninto a single document, with each the logs for each\nday prepended with the date if that day’s log is\nnon-empty.\nC.16 BookCorpus2\nThe original BookCorpus consists of 11,038 books.\nHowever, due to issues with availability of the\noriginal BookCorpus, as well as the possibility of\ncollecting a larger version, we decided to collect\nour own version of BookCorpus using a similar\nmethodology as Kobayashi (2018). Our version of\nBookCorpus contains 17,868 books instead.\nWe create and use a modiﬁed version of the epub-\nto-text converter in Kobayashi (2018) that:\n• Correctly preserves the document structure\nacross chapters, matching the table of contents\nvery closely;\n• Correctly renders tables of data, whereas by\ndefault html2txt produces poor-quality re-\nsults for tables,\n• Correctly preserves code structure, so that\nsource code is visually coherent,\n• Converts numbered lists from “1\\.” to “1.”\n• Runs the full text through\nftfy.fix_text() (Speer, 2019),\nreplacing Unicode apostrophes with ascii\napostrophes and expanding Unicode ellipses\nto “...” (three separate ascii characters).\nC.17 EuroParl\nWe download the data in bulk from 21. We re-\nmove all basic tag information and only retain\nthe name of each document as a title. For ex-\nample, <SPEAKER ID=77 LANGUAGE=\"NL\"\nNAME=\"Pronk\"> becomes Pronk, and then ex-\ntract the body of each document, discarding those\nthat are shorter than 200 characters.\n21http://www.statmt.org/europarl/\n25\nC.18 HackerNews\nWe ﬁrst use the Hackernews BigQuery dataset to\nobtain a list of all story ids in our date range. For\nthe Pile we use the ﬁrst Hacker News post (1) to\npost number 24531712. This corresponds to a date\nrange of approximately 10/09/2006 to 09/20/2020.\nWe use the BigQuery dataset to gather story ids\nfor efﬁciency purposes. However, the BigQuery\ndataset was lacking some information for stories,\nso we used the ofﬁcial Hacker News API for story\nand comment text retrieval.\nHacker News displays and stores comments in a\ntree-like manner, with children comments replying\nto parent comments. However, most language mod-\nels require input data to be in a sequential form.\nConsidering each path through the comment tree\nas a sequence could be detrimental, since there will\nbe a large amount of near-duplicate comment se-\nquences. In addition, only taking one path through\nthe comment tree for each story leaves out a large\nportion of the comment data. Therefore, we parsed\ncomments in a hybrid form. For every top-level\ncomment (comments that have no parent comment),\nwe create a sequence of comments by traversing\ndown the comment tree from the top-level com-\nment. We choose the next comment by taking the\nchild comment with the highest number of children\ncomments (a cheap attempt at taking a long path\nthrough the comment tree, note that it does not take\nthe longest possible path).\nWe consider all stories that have at least one com-\nment and are not ﬂagged by the moderators for\npotential conduct violations. Since comments are\nstored in HTML, we use thehtml2text package\nto extract the text from the post.\nWe order each document by listing the title, url,\nsub-title, and author at the top. Top-level comments\nare delimited by \"\\n----\\n\" and sub-comment\nchains are delimited by \"\\n~~~\\n\". We include\nauthor and extracted text for each comment.\nC.19 YouTube Subtitles\nWe construct the dataset in three stages:\n1. We build a large list of search terms by\nprompting a GPT-3 model with a manually\nselected list of queries, manually ﬁltering the\nresponses, and repeating this process itera-\ntively until a suitable size is reached. The list\nof terms is centred around, but not limited to,\neducational topics.\n2. We use requests-html to gather a list of 1000\nYoutube video IDs for each search term, and\ndeduplicate the resulting video ids across\nsearch terms.\n3. We use YoutubeTranscriptApi22 to\ngather all human generated closed captions\nfor every available language for each video.\nTo align each language in parallel, we split\nthe captions for each language into parallel\nminute-long sections by timestamp, and ar-\nrange each language in a random order within\nthese sections, appending the language as a\nheader to each minute-long section to provide\ncontext. If only a single language is available,\nthe output is just the subtitles, with no header\nappended.\nIn total, subtitles for 173,651 videos were gath-\nered.\nC.20 PhilPapers\nThe PhilPapers (PP) are indexed using OAI-MPH,\nthe Open Archives Initiative Protocol for Metadata\nHarvesting. As such, the ﬁrst step to collect the\ndata is to get the XML for all links. This was done\nusing pyoaiharvester.23\nFrom that, each publication is downloaded. Some\nentries do not exist, or have been removed by\nthe authors. Papers with text are extracted using\npdfbox, and papers with non-machine readable\ntext are ignored. Non-English language publica-\ntions are kept, and the metadata reﬂects the lan-\nguage reported by the OAI-MPH XML. The text is\nﬁltered with pdf_filter.py from PDFextract,\nand we discard any papers with less than 1000 char-\nacters.24\nC.21 NIH Grant abstracts:\nExPORTER\nThe NIH provides a bulk-data repository for\nawarded applications through the ExPORTER ser-\nvice covering the ﬁscal years 1985–present. These\ndata come from the NIH, but also other other Health\nand Human Services agencies (ACF, AHRQ, CDC,\nHRSA, FDA), and the V A. Additionally, the NIH\n22https://github.com/jdepoix/\nyoutube-transcript-api\n23https://github.com/vphill/\npyoaiharvester/\n24https://github.com/sdtblck/PDFextract\n26\nprovides a legacy data format named CRISP for\nawarded applications during the ﬁscal years 1970–\n2009.\nWe merged both the ExPORTER and CRISP data\nto form a consolidated dataset of awarded appli-\ncations. Entries were deduplicated based off their\napplication ID, and excluded if their abstract text\nwas missing or too short. Small grants, especially\nadministrative ones, consisted solely of short boil-\nerplate. For this reason, we further deduplicated on\nabstract text. All grants types were considered, in-\ncluding new applications (Application Type Code\n1) and renewals (Application Type Code 2) as the\ntext differed enough to provide novel input. The\ntext was then minimally parsed to remove admin-\nistrative boilerplate, (ex. most old awards contain\nsome variation of “description: (provided by appli-\ncant)\"). In total, there were 939,668 grant applica-\ntion abstracts added.\nC.22 Enron Emails\nTo extract the data, we used the mailparser\npackage25 to extract the body of each email as a\ndocument.\nD General Data Processing\nThis section discusses any processes applied across\nmultiple datasets.\nTo combine the constituent datasets, we iterate\nuntil the size of the output dataset is the desired\nsize, drawing documents from datasets at random,\nweighted by the number of documents in each\ndataset times the number of epochs desired on\nthat dataset. Because the number of documents\ninvolved is high, by the law of large numbers, the\nnumber of copies of each dataset present in the Pile\nis approximately equal to its epoch count.\nShufﬂing a dataset posed a major problem due to\nour limited memory and computational budget. We\nfollow Hardin (2018), a method descended from\nRao (1961), and interleave our output to produce\n30 output piles.\nWe hold out approximately 10GiB of data from\nthe Pile, of which 2GiB are used to create the val-\nidation and test splits, and the remainder is held\nin reserve. From the training set, we remove any\n25https://github.com/SpamScope/\nmail-parser\nelements that are also present verbatim in any of\nthe held out data, to prevent leakage.\nD.1 Weights\nSimilar to Brown et al. (2020), we increase the\nweight of certain components such that the number\nof epochs elapsed on data we consider high quality\nis greater than one. Our choice of weights was\nprimarily informed by the source of the data and\nthe size of the dataset; we attempted to upweight\nacademic texts the most, which we felt provided the\nhighest quality data, as well as smaller sets, such\nthat they would have a more pronounced impact on\nthe data. We strictly disallowed any data more than\n3 epochs and avoided having any data with more\nthan 2 epochs.\nD.2 Deduplication\nDue to memory constraints we did not perform\nPile wide de-duplication. Instead, de-duplication\nwas performed at the document level within Open-\nWebText2 and Pile-CC as those sets were the most\nlikely to contain duplicate documents.\nThe same technique was used for both OpenWeb-\nText2 and Common Crawl—MinHashLSH with\nthe Python Datasketch library.26 We used 10 hash\nfunctions for each Minhash and an approximate\nJaccard similarity of 0.5. This produced a dupli-\ncate rate of 28% in OpenWebText2 and 26% for\nCommon Crawl.\nThe main challenge here was computational, lead-\ning us on a journey through the various LSH per-\nsistence options. A simple quadratic Minhash com-\nparison of all documents would have taken several\nhundred thousand years, motivating the use of LSH.\nInitially, we did not have sufﬁcient RAM for in-\nmemory LSH and chose to use the Cassandra back-\nend when de-duplicating OpenWebText2. This was\nreasonably fast, but the same method resulted in a\ncorrupted database about 3\n4 of the way through pro-\ncessing Common Crawl. After the Cassandra cor-\nruption, we brieﬂy tested the experimental Mongo\nimplementation; however this was quite slow due\nto the nature of Mongo itself. In the end, we ran\nin-memory LSH on a machine with enough RAM\nfor Common Crawl, taking several days.\n26https://github.com/ekzhu/datasketch\n27\nD.3 Downstream Validation Leakage\nTo avoid leakage of data from downstream evalu-\nations, recent work (Radford et al., 2019; Brown\net al., 2020; Shoeybi et al., 2019) has removed any\ndata in the training set that may overlap with the\nevaluation metrics. We decided not to perform any\nsuch removal, because it is impossible to antici-\npate all potential downstream evaluation metrics,\nand so any particular selection of metrics would\ninevitably either become obsolete as the choice of\nbenchmarks in the ﬁeld changes, or potentially hin-\nder the development of new benchmarks for models\ntrained on Pile.\nFor models trained on Pile and evaluated on metrics\nother than Pile’s own validation and test sets, we\nencourage authors to remove overlaps between Pile\nand the validation data of these additional down-\nstream evaluations. We do not anticipate that such\nleakage removal will hurt model performance, as\nthe validation sets of most benchmarks are very\nsmall in relation to the size of the Pile, and so\nchoosing to evaluate on more metrics will not be a\ndisadvantage for any model.\nE Investigating data\nE.1 13-Gram Analysis\nAs part of our exploratory analysis, we calcu-\nlated the counts of all 13-grams across Common\nCrawl. We chose n = 13 due to its use in prior\nwork (Brown et al., 2020). There were a total of\n40,216,231,078 different 13-grams in this dataset.\nThe 1000 most common range from 11 million\noccurrences down to 20k.\nThe most frequently occurring 13-grams were\ncharacter repetitions used for styling such as\n“-- --”, “* * * *”, “! ! ! ! ”, at 11\nmillion, 5.8 million and 1.1 million respectively.\nOther characters used in this manner include the\nfollowing: “ # . > ? ”. In the 264k count\nrange, we see repetitions of badly formatted\nHTML escape characters “; &nbsp”, “; amp”.\nBoilerplate from standard forum software appears\naround the 180k occurrences range, such as the\nfollowing: “select the forum that you\nwant to visit from the selection\nbelow”.\nOverall, a large amount of common HTML and\nCSS is included in the top 1000, along with boil-\nerplate text from Amazon Afﬁliate Advertising,\nComponent Tokens per byte\n(LT /LB)\nPile-CC 0.2291\nPubMed Central 0.3103\nBooks3 0.2477\nOpenWebText2 0.2434\nArxiv 0.3532\nGithub 0.4412\nFreeLaw 0.2622\nStackExchange 0.3436\nUSPTO Backgrounds 0.2116\nPubMed Abstracts 0.2183\nGutenberg (PG-19) 0.2677\nOpenSubtitles 0.2765\nWikipedia (en) 0.2373\nDM Mathematics 0.8137\nUbuntu IRC 0.3651\nBookCorpus2 0.2430\nEuroParl 0.3879\nHackerNews 0.2627\nYoutubeSubtitles 0.4349\nPhilPapers 0.2688\nNIH ExPorter 0.1987\nEnron Emails 0.3103\nTable 7: Tokens per byte for Pile components\nTripAdvisor, SimplyHired, Associated Press, Post-\nMedia, The FCC etc. PHP error messages and\npassword login prompts also made an appearance.\nIt may be of interest to fans of Portal that repeti-\ntions of “the cake is a lie .” achieved a\nhigh count.\nE.2 Benchmark Perplexity\nComputation\nTo compute the perplexity for a given dataset, we\ntokenize each document separately, divide the docu-\nment into segments of up to the maximum sequence\nlength of the model (1024 tokens for GPT-2, 2048\nfor GPT-3), and predict the logits of the each seg-\nment. The inputs to the model are the immediate\nprior tokens the e.g. for scoring tokens 1 to 1024,\nwe provide tokens 0 to 1023 at the input context.\nThe respective language model implementations\nhandle the causal attention masking. This ensures\nthat every token in the dataset is scored exactly\nonce. This also means that some tokens will have\nmore input context than others. We then aggregate\nover the whole dataset and compute the ﬁnal per-\n28\nplexity score. The perplexity for the whole Pile\nis computed by aggregating over the constituent\ndatasets (i.e. weighted by dataset size, not a simple\naverage of dataset perplexities). Both GPT-2 and\nGPT-3 share the same tokenizer and vocabulary,\nmaking the perplexity scores directly comparable.\nWe use the Hugging Face (Wolf et al., 2020) im-\nplementation of GPT-2, and the OpenAI API for\nGPT-3. The davinci model in the OpenAI API\nis presumed to correspond to a 175B parameter\nversion of GPT-3.\nIn Table 8 we show the test set perplexities (i.e. not\nnormalized by UTF-8 length, as in Table 2). Be-\ncause of the costs associated with using the OpenAI\nAPI, we compute test perplexities on only one-tenth\nof the test set in Tables 8 and Table 2. Speciﬁcally,\nwe randomly sample one-tenth of the documents of\neach dataset except for three: Ubuntu IRC, Book-\nCorpus2, and PhilPapers. In Table 9, we show\ntest perplexity computed on the full test set on all\nGPT-2 models.\n0 2 4 6 8 10\nSequence Position Log2 (tokens)\n2\n3\n4\n5\n6\n7Loss (nats)\nTest loss vs. Log sequence position\nGPT-2 (small)\nGPT-2 (medium)\nGPT-2 (large)\nGPT-2 (xl)\nGPT-3 (ada)\nGPT-3 (babbage)\nGPT-3 (curie)\nGPT-3 (davinci)\nFigure 11: Test loss (log perplexity) over the Pile, buck-\neted by position in the input sequence based on the\nmodel’s maximum sequence length. To smooth out the\nlines, we bucket 4 positions per plotted datapoint. (e.g.\npositions 0–3, positions 2044–2047). Later tokens are\npredicted with more context and thus see lower perplex-\nities.\nE.3 Pejorative Content\nInitially we decided on separating pejorative con-\ntent into 4 groups: sex-related terminology, slurs,\nneither of these categories, and both of these cate-\ngories. We adapted a public \"naughty words\" list\nand broke them into these categories with the in-\ntern of looking at the proportion of each category\nin each dataset. However, this provided many is-\nsues.\nFirst, any blacklist of words would be hard-pressed\nFigure 12: Percentage of sentences classiﬁed as pro-\nfane in the Pile. The percentage of the CC component\nand the weighted mean of the Pile as a whole are shown\nas horizontal lines\nto catch all the instances of pejorative content, since\npurposeful misspellings of words could evade the\ncensor and still have the intended effect. Further-\nmore, words and their intents are always evolving,\ntherefore any list created would likely be always\noutdated. Another issue pertains to sorting the\nwords into the categories. Words are highly de-\npendent on their context, so a word would change\ncategories with different contexts.\nF Data Samples\nThe following consists of two random, non-\ncherrypicked 512-byte samples from each con-\nstituent dataset of the Pile, sampled from the vali-\ndation split.\nF.1 Pile-CC\npot trending topics and the coverage around them. First up, there’s a bit of\na visual redesign. Previously, clicking on a trending topic would highlight\na story from one publication, and you’d have to scroll down past a live\nvideo section to view related stories. Facebook is replacing that system\nwith a simple carousel, which does a better job of showing you different\ncoverage options. To be clear, the change doesn’t affect how stories are\nsourced, according to Facebook. It’s still the same algorithm pickin\ne public safety. He said the bridge saves commuters two or three minutes\nwhen trains pass – and those minutes could be vital.\n“Two to three minutes may not mean much if you’re just driving\nhome from work, but if you’re the one waiting for an ambulance to get to\nyour home, if you’re the one waiting for a ﬁre truck to get to your home,\nif you’re the one waiting for a police car to get to your home, those two\nto three minutes could mean the difference between life or death,” Sharp\nsaid. “That’s what this pro\nF.2 PubMed Central\n29\nd a suitable substitute for the advice of a qualiﬁed health care\nprofessional. Do not disregard or avoid professional medical advice due\nto content published within Cureus.\nIntroduction\n============\nTotal knee arthroplasty (TKA) is a promising treatment for end-\nstage osteoarthritis (OA) of the knee for alleviating pain and restoring\nthe function of the knee. Some of the cases with bilateral TKA are\nsymptomatic, necessitating revision arthroplasty in both the knees. A\nbilateral revision TKA can be done ei\nent\\’s ability to make judgements and decisions about their work\nexperiences and learning that will position them as future critical\nthinkers, life longer enquirers and learners.\nConclusion {#jmrs290-sec-0014}\n==========\nIdentiﬁcation of the core capabilities that our stakeholder com-\nmunity rate highly has proved informative in assisting us to describe a\n\"work ready *plus\"* medical imaging graduate for the New Zealand\ncontext. The results have provided data to the curriculum development\nteam allowing them\nF.3 Books3\ncept of _forçage_ , ’a forcing of language enacted by the advent of an\n\"other\" language that is at once immanent and created’, 44as Badiou puts\nit: this opens up vistas of a truly syntactic analysis of the poem, in which,\nagain, Badiou would be close to his philosophical other, Deleuze, who,\nas we just saw, deﬁnes style through a-grammaticality and who tries to\ndeﬁne what he calls an ’intensive line of syntax’.45\nNevertheless, the insistence on syntax as guarantee involves a\n_seventh paradox_ , the parad\nrnment, before the Second World War there were 5,300 communities\nand two million burakumin. The BLL thinks there must be at least three\nmillion burakumin living in Japan today.\nWe visited a hall in Osaka where a taiko drum group, made up\nexclusively of young burakumin, were about to start their weekly\nrehearsal. The small gymnasium was ﬁlled with taiko drums of all sizes.\nThe smallest was about the size of a snare drum, the largest about the\nsize of a compact car. The Japanese drum group Kodo have made th\nF.4 OpenWebText2\nprime minister to repatriate all the police sent to Catalonia before the\nreferendum.\nWhat were the results?\nWith nearly all votes counted, the pro-independence parties To-\ngether for Catalonia (JxCat), Republican Left of Catalonia (ERC) and\nPopular Unity (CUP) were on course to win a total of 70 seats in total,\ngiving them a majority in the new parliament.\nCitizens (Cs) had 25.3% of the vote, winning 37 seats in the\n135-seat chamber.\nIts leader told the BBC her party had been \"victorious\". Ms\nInés Arrima\nso accommodate Stablecoins.\nWhile some analysts opined that Stablecoins are created to bring\ngrowth into the crypto space, they are becoming a solid way to reduce\ncrypto volatility due to the fact that their value are pegged to ﬁat currency.\nFor Low Cost and Almost Instant Across Border Remittance\nWhen Stellar-based Wirex stablecoins ﬁnally launches, they are\ngoing to be used to perfect low-cost and all most immediately\ncross-border remittance, just like the IBM Stablecoin which has received\nsupport fr\nF.5 ArXiv\n\\mbox{ if } 2\\leq x\\leq 3\n\\end{array}\\right.$$ is lower semicontinuous and the nonemptiness\nof ${\\operatorname{ﬁx}}\\Phi$ is guaranteed by Corollary \\[cor:ﬁxed\npoint\\]. Notice that ${\\operatorname{ﬁx}}\\Phi=[1,2]$. Nevertheless\nthe Kakutani ﬁxed point Theorem does not apply since ${\\operator-\nname{gph}}\\Phi$ is not closed.\nOn the converse, the set-valued map $\\Phi:[0,3]\\rightrightarrows\n[0,3]$ $$\\Phi(x):=\\left\\{\\begin{array}{ll}\n\\{1\\} & \\mbox{ if } 0\\leq x<1\\\\\n{}[1,2] & \\mbox{ if } 1\\leq x\\leq 2\\\\\n\\{2\\} &\n.eps){width=\"6.5cm\"}\n![Gamma-ray spectrum at Mt. Norikura (2.77 km a.s.l). The vertical axis\nis Flux$\\times Eˆ2$. Our data is at $<$ 100 GeV . Data above 300 GeV is\nfrom emulsion chamber experiments. For the latter, see Sec.\\[discuss\\]\n[]{data-label=\"norispec\"}](norikura.eps){width=\"7.5cm\"}\n![The altitude variation of the ﬂux integrated over 6 GeV . The\ndpmjet3.03 and fritiof7.02 give almost the same feature consistent with\nthe observation while the deviation of fritiof1.6 from the data is obvious.\n\\[trans\nF.6 Github\n\"enabled\", out.enabled);\n}\nstd::string SMTPServerInfoJSONStringSerializer::serialize(const\nSMTPServerInfo &in,\nconst SecurityContext &sc)\n{\nreturn SMTPServerInfoJSONSerializer::serialize(in, sc).dump(4);\n}\nvoid SMTPServerInfoJSONStringSerial-\nizer::unserialize(SMTPServerInfo &out,\nconst std::string &in,\nconst SecurityContext &sc)\n{\nretur\nlose\">\n<at-form state=\"vm.form\" autocomplete=\"off\" id=\"external_test_form\">\n<at-input-group col=\"12\" tab=\"20\" state=\"vm.form.inputs\" form-\nid=\"external_test\"></at-input-group>\n<at-action-group col=\"12\" pos=\"right\">\n<at-action-button\nvariant=\"tertiary\"\nng-click=\"vm.onClose()\"\n>\n{{::vm.strings.get(’CLOSE’)}}\n</at-action-button>\n<at-action-button\nvariant=\"primary\"\nn\nF.7 FreeLaw\nssible, and further, that the weight of the evidence, the credibility of the\nwitnesses and the persuasive effect of the testimony is for the sole deter-\nmination of the trier of fact.\nThis Court thus uses the same interpretation of V .R.C.P. 52(a) as it did\n*487 under the previous statutory requirement found in 12 V .S.A. § 2385.\nIn essense, the defendants urge that this Court should reconsider the case\nof Green Mountain Marble Co. v. Highway Board, supra, and follow the\nFederal practice of looking to the evide\nng to the fact that\nthe relevant Arkansas statutes and rules provide for criminal sanctions\nagainst school\nofﬁcials who fail to enforce the immunization requirements, the Morn-\ningstar and\nLake Hamilton School Districts characterized themselves as disinterested\nbystanders\ncaught in the crossﬁre between the Schoolchildren and the Ofﬁcials. See\nArk. Code\nAnn. § 6-18-702(c)(2)(B) (2000) (“Any school ofﬁcial, parent, or\nguardian violating\nthe regulations shall be subject to the penalties imposed herein.”); Id\n30\nF.8 Stack Exchange\nooks like a fancy wheel, When resetting rotation from 360deg to 0 deg, It\nanimating the wheel in anti-clockwise direction, How to Avoid this???\nHTML\n<ul class=\"cm\">\n<li><span>01</span></li>\n<li><span>02</span></li>\n<li><span>03</span></li>\n<li><span>04</span></li>\n<li><span>05</span></li>\n<li><span>06</span></li>\n<li><span>07</span></li>\n<li><span>08</span></li>\n</ul>\nSCSS\n$Brdr: #7d868c;\n*{\n-webkit-box-sizing: border-box;\n-moz-box-sizing: border-box;\nbox-sizing: border-box;\nw can I solve it?\nYesterday I added Google ReCAPTCHA v3 in one of my\nclient’s Shopify website, but I don’t think that it is working because he is\nstill reporting to receive several spam e-mails.\nI’ve followed Google’s guide, but I don’t know what to do for \"Verifying\nuser response\" part of the guide. I’m not an expert in coding.\nBasically I’ve added this code to the theme.liquid ﬁle\n<script src=\"https://www.google.com/recaptcha/api.js?render=*site key\nprovided by google*\"></script>\nAnd then I’ve added th\nF.9 Wikipedia (en)\nand the third son of John Bland and Elizabeth née Birch, daughter of\nRobert Birch, Bland was educated at Trinity College, Cambridge, where\nhe graduated as a Bachelor of Arts in 1825, and a Master of Arts in\n1829. He was called to the Irish Bar in 1829, becoming a member of the\nQueen’s Counsel in 1854.\nIn 1840, he married Charlotte Elizabeth Grove Annesley, daugh-\nter of Arthur Grove Annesley and Elizabeth née Mahon, and they had at\nleast one child: John Loftus Bland (1841–1908). After Charlotte’s death\nin 18\nheart of the University campus, a meeting-place for all academic\ndisciplines, improving its opportunities to co-operate across traditional\nacademic boundaries. It also gives USBE-students an opportunity to take\nan active part of student environment created for the 37 000 students at\nUmeå University.\nOrganization\nUmeå School of Business, Economics and Statistics has three\ndepartments: the Department of Business Administration, the\nDepartment of Economics and the Department of Statistics.\nUSBE Career Cent\nF.10 USPTO Backgrounds\nnductivity types), it is necessary that at least some process is steps differ-\nentiate between p-type and n-type transistors. Separate implant steps, for\nexample, are needed to deﬁne n-well and p-well structures and to dope\nthe source/drain regions of n-channel and p-channel transistors. When-\never possible, however, it is generally desirable to use a single process\nstep to deﬁne transistor features regardless of the transistor type. Single\nprocess steps imply a single mask step, which is always desirable to\nenser further comprising a means of identifying the user by voice recog-\nnition. Also, an object is dispenser further comprising a means of identi-\nfying a supervisor by voice recognition.\nFurthermore, an object is a dispenser further comprising a means of cus-\ntomizing the plurality of aural messages for instructing the user during\neach of the plurality of washing steps.\nAn object of the invention is a dispenser for metering a liquid cleanser to\na user and prompting the user in compliance with a recommended wash\nF.11 PubMed Abstracts\nent (REM) latency were found to be signiﬁcantly worse in Group 1 as\ncompared with Group 2. Cognitive and executive parameters were signif-\nicantly impaired in Group 1. Shorter total sleep time, poorer sleep efﬁ-\nciency, and prolonged sleep latencies were observed to be associated with\npoor memory and executive function in patients with refractory epilepsy.\nOur study strongly suggests that sleep disturbances, mainly shorter total\nsleep time, poor sleep efﬁciency, and prolonged sleep latencies, are asso-\nciated\nneurons in vesicular GABA transporter (VGAT)-venus transgenic mouse.\nInhibitory neurons play important roles in a number of brain functions.\nThey are composed of GABAergic neurons and glycinergic neurons, and\nvesicular GABA transporter (VGAT) is speciﬁcally expressed in these\nneurons. Since the inhibitory neurons are scattered around in the CNS,\nit is difﬁcult to identify these cells in living brain preparations. The glu-\ntamate decarboxylase (GAD) 67-GFP knock-in mouse has been widely\nused for the identif\nF.12 Gutenberg (PG-19)\ns he met with as a novelist, he was anxious to prosecute his\noriginal profession of medicine; and having procured from a foreign\nuniversity the degree of M.D., he commenced to practise physic in\nChelsea, but without success. He wrote, however, an essay \"On the\nExternal Use of Water,\" in which he seems to have partly anticipated\nthe method of the cold-water cure. In 1753 he published his\n\"Adventures of Count Fathom;\" and, two years later, encouraged by a\nliberal subscription, he issued a translation of \"Don\nYearn;\nIts Advertising brought us such Renown,\nWe jumped Three Hundred Thousand, on that Turn!\"\nXXXVI\nI think the man exaggerated some\nHis increased Circulation,–but, I vum!\nIf I could get Two Thousand for one Tale,\nI’d write him Something that would simply Hum!\nXXXVII\nFor I remember, shopping by the way,\nI saw a Novel writ by Bertha Clay;\nAnd there was scrawled across its Title-Page,\n\"This is the Stuff that Sells–so People say!\"\nXXXVIII\nListen–a moment listen!–Of the same\nWood-pulp on wh\nF.13 OpenSubtitles\nad for you.\" \" Too bad for me?\" \"How about too bad for you?\" \"Oh\nno!\" \"Luckily I keep a spare.\" \"Look everyone!\" \"My winky was a key!\"\n\"Oh dear, bloody Dutchman.\" \"Foxxy, I’m coming!\" \"Don’t do anything\nstupid or the shooting begins.\" \"Austin, take Ducky I’ll stay here and\nbe your backup.\" \"Ducky, what do we do?\" \"I’m not really a \"hands-on-\nevil-genius\".\" \"Think you were always the smart one.\" \"I could re-write\nthe output capacity to the tractorbeam from one of the conduit boxes up\nthere.\" \"Come on, let’s\n.\" \"this calls for... four people.\" \"Yes!\" \"we got it.\" \"guys.\" \"We got it.\"\n\" Got what?\" \" Our sub.\" \" Did he say sub?\" \" Mm-hmm.\" \"Only private\nsub on the Florida coast rated for 300 fathoms.\" \"Sub as in submarine?\"\n\"Following up on your haloclines.\" \"so we’re going to have to drive all\nnight... if we’re going to be there by morning.\" \"Anybody have trouble\nsleeping in a car?\" \"whoa.\" \"Wait a minute.\" \"What happened to the nice\nofﬁces in Canaveral City?\" \"Mr. Benirall expects you to take ’em.\" \"we\njust go\nF.14 DM Mathematics\n31\n3651*w**2 + 519*w + 1\nFind the second derivative of -91419126*m**2 - 162128943*m.\n-182838252\nFind the third derivative of 5*l*u*y**3 + l*u*y - 5*l*y**2 -\n4621073*u*y**3 - 1755838*u*y**2 + u wrt y.\n30*l*u - 27726438*u\nFind the third derivative of 317297018*s**3 + 3136*s**2 - 30884*s wrt\ns.\n1903782108\nWhat is the third derivative of -16525*f*r**3 + 20*f*r + 356*r**3 +\n1425730*r**2 wrt r?\n-99150*f + 2136\nWhat is the second derivative of 199836725*j**2 - 443399*j - 462 wrt j?\n399673450\nWhat is the derivative of\nthe nearest integer?\n5\nWhat is 783451 to the power of 1/3, to the nearest integer?\n92\nWhat is the fourth root of 6322907 to the nearest integer?\n50\nWhat is the ninth root of 4723626 to the nearest integer?\n6\nWhat is 4954939 to the power of 1/2, to the nearest integer?\n2226\nWhat is 625583 to the power of 1/3, to the nearest integer?\n86\nWhat is 1105849 to the power of 1/3, to the nearest integer?\n103\nWhat is the fourth root of 4820344 to the nearest integer?\n47\nWhat is the seventh root of 243476 to the neare\nF.15 HackerNews\nced lists I email don’t get formatted correctly. It’s\nslightly annoying for such an otherwise beautifully designed layout.\n——\najcronk\nThere is a typo in the url at the end of the How did your day go? email.\nShould be ohlife.com/today, not ohlife.come/today\n~~~\nsgupta\nThanks for the heads up!\n——\na3_nm\nHow exactly is this service better than, say, a simple text ﬁle on my own\nmachine with a daily reminder set up through some other means?\nWhy would I want to use some third-party website for some-\nthi\nor Amazon EC2 and Amazon SQS. The bandwidth tier in which you will\nbe\ncharged each month will be calculated based on your use of each of these\nservices separately, and could therefore vary across services.\"\n——\nyaacovtp\nCan anyone tell me what bandwidth costs a month once you need over a\nterabyte\na month? How would you host a 5-10 mb movie that may be viewed\nmillions of\ntimes without using a 3rd party video host like youtube etc?\n~~~\nespeckman\nLots of dedicated hosts will include a 2-5 TB of transfer a\nF.16 BookCorpus2\nconsiderate of me, you’re right. I apologize.\" Kate smiled in what she\nhoped was a winning way. She teetered over to the counter on heels that\nwere too high and put down her things with a sigh of relief.\nAlthea, who would not reveal her age but was probably some-\nwhere in her late sixties, patted her dark-dyed helmet of hair and\nstraightened the ﬂowing turquoise silk jacket she was wearing over white\ncapris and a white tank. \"Well. It just seems to me that as the _owner_ ,\nyou should try to set some sort of\ne notebook didn’t have lines for me to write with like some notebooks\nhave. I hated that notebook and I hated writing into it. I was glad to\nthrow that damn thing out even if it was unﬁnished. Ugh.\nI was urged by voice \"You to go take your pills and eat food.\"\nbut I refused on calling mom.\nI should have listened to the voice’s suggestion because mom\npicked up the phone at eight forty ﬁve in the morning and hogged me to\nten o’clock is when she ﬁnally quit. Ugh hence voice picking onto me\nwhen I got off\nF.17 EuroParl\nraˇcun prometne varnosti in visokih cen? Danes želimo izvedeti, kako in\nkdaj bomo integrirali razvrš ˇcanje zgornjega zra ˇcnega prostora ter kako\nbomo skupaj upravljali spodnji zraˇcni prostor v prihodnosti. Ali se lahko\nodkrito dolo ˇcijo ovire za vzpostavitev funkcionalnih blokov nad evrop-\nskim ozemljem? Ali je mogo ˇce osvetliti politi ˇcno voljo držav ˇclanic, da\nizpolnijo svoje obveznosti? Prav tako nas skrbi, da pristop od spodaj navz-\ngor ne bo uspel, ker v treh letih države ˇclanice niso razvile funkcionalnih\nblok\nom ekonomisk styrning som vi debatterar inom kort kommer att vara my-\ncket viktigt. Vi vet mycket väl att det är på gång i vårt lagstiftningsför-\nfarande, och vi hoppas att vi kommer att vara klara så snabbt som möjligt.\nVad kan jag sammanfattningsvis säga? Hela paketet som vi undertecknar\ni dag kommer att börja gälla i Europeiska unionen från och med den 1\njanuari 2011, alltså mycket snart. Det är viktigt för oss alla, såväl för\nmarknaderna som för våra medborgare, att förstå att avsikten med paketet\när att hj\nF.18 YoutubeSubtitles\nscience term\nfor a mixture of things\nthat don’t usually mix.\nThe things in this case\nare water and fats.\nUnder normal circumstances,\nfats and water repel each other,\nbut milk also contains complex\nprotein chains called caseins\nthat are made up of both\nhydrophilic, or water loving,\nand lipophilic, or fat loving, particles.\nWhen presented with both water and fats,\ncaseins grab bits of fat and cluster up\ninto globules called micelles,\nwith the fat on the inside\nand the hydrophilic bits on the outside.\nThe hydr\nSE WE KIND OF ARE MORE,\nHIPPY , I GUESS MAYBE IN SOME OF\nTHE THINGS THAT WE DO.\nAND THEY WERE JOKING AND THEY\nWERE LIKE, \"OH WE HEARD ABOUT\nTHIS TOWN IT’S LIKE THIS\nSUSTAINABLE CITY THERE’S SOLAR\nPANELS, YOU GUYS WOULD LOVE IT.\"\nAND I LOOKED IT UP AND I W AS\nLIKE I REALLY ACTUALLY DO LOVE\nTHIS TOWN.\n>> Sreenivasan: JOSHUA, A\nPHYSICAL THERAPIST, GOT A JOB AT\nTHE LOCAL HEALTH CENTER IN THE\nTOWN’S COMMERCIAL HUB BEFORE\nTHEY MOVED IN.\nIT’S WHERE THE FIRST BUILDINGS\nWENT UP.\nTHERE’S ALSO A RESTAURANT AND\nCOFFEE SH\nF.19 Ubuntu IRC\nemingly wlan related) <Snappy:New> <linux-raspi2 (Ubuntu):New for p-\npisati> <https://launchpad.net/bugs/1627643>\n<ppisati> ogra_: or we punch a hole in the dev image so we can login via\nthe serial console and check what’s really going on\n<ppisati> ogra_: yes\n<ogra_> well, i wanted to play with systemd console but didnt have time\n32\nfor that yet\n<ogra_> \\o/\n<ogra_> something at least ... that kernel looks ﬁne\n<ppisati> ogra_: good to know\n<ogra_> do you have an SRU bug that i can tag veriﬁcation-done ?\n<ogra_\nproblem with this? Like, if teenage boy wants to have nekkid lady wall-\npapers, maybe he don’t want it to come up on family computer... Dunno,\nmaybe it’s not an issue?\"\n<swilson> hi there! yes, i believe that a bug has been created which raises\nthis same issue - about embarrassing or conﬁdentiality issues with this\n<swilson> this seems to be a bit of an edge case, but it may be signiﬁcant\nenough to warrant giving some careful thought\n<imnichol> Or if you have bank info on your screen before it’s locked\n#ub\nF.20 PhilPapers\nintersubjectivity and self-consciousness was already emphasized by\nSartre. Forthcoming in Grazer Philosophische Studien 84 (2012), p. 75-\n101 15 Thus, to use Rochat’s terminology, from this point onwards, the\nchild has \"others in mind\" (Rochat 2009). The child now begins to un-\nderstand that she is a subject that can be observed by others, just like she\ncan observe the behavior of others, and she can begin to consider others’\nperspectives on herself. It is at this point that the child begins to fully\napprecia\nd an entire chapter detailing the remarkable achievements of Ashkenazi\nJews and hold them up as exhibit A in the argument that human evolution\nhas been, in Wade’s words, recent, copious, and regional. The example\nof Ashkenazi evolution is supposed to show the absurdity of the view,\nheld by authors like Jared Diamond and Stephen Jay Gould, that human\nevolution either stopped one hundred thousand years ago or that natural\nselection has somehow continued to sculpt the bodies but not the brains\nof different gro\nF.21 NIH ExPorter\nrapies that can inhibit the EMT, but few assays for EMT inhibitors in\nhigh throughput screens (HTS) have developed. A change in ﬁbroblast\ngrowth factor receptor 2 (FGFR2) splicing occurs during the EMT and\nusing an innovative luciferase-based splicing reporter assay we previously\ncarried out a genome-wide high throughput cDNA expression screen for\nregulators of this splicing switch. This screen identiﬁed the epithelial cell\ntype speciﬁc splicing regulators ESRP1 and ESRP2 demonstrating the\nfeasibility of\nl and behavioral research projects utilizing primates residing in a semi-\nnatural habitat. This population has the most extensive computerized de-\nmographic and genetics database available to researchers anywhere in the\nworld. The population management program for CS has been designed to\noptimize the health and well-being of the monkeys, to enhance the value\nof the colony for research. In addition, the goal is to provide healthy ani-\nmals to the scientiﬁc community for biomedical research, including AIDS\nand Sl\nF.22 Enron Emails\nwant to make sure that my vacation time gets paid at 100%\nbefore I go down to the 90% level. Thanks for taking care of this. As you\ncan see, I now have access to my e-mail so when I’m not pumping,\nfeeding,\nchanging diapers, etc... I acn be checking up on things!!!\nCarol St. Clair\nEB 3892\n713-853-3989 (Phone)\n713-646-3393 (Fax)\ncarol.st.clair@enron.com\nSuzanne Adams\n07/18/00 05:22 PM\nTo: Carol St Clair/HOU/ECT@ECT\ncc: Taffy Milligan/HOU/ECT@ECT\nSubject: Re: Carol St. Clair\nCarol, I\n—-Original Message—–\nFrom: \"Prakash Narayanan\" <pnarayan@andrew.cmu.edu>@ENRON\nSent: Sunday, December 02, 2001 9:28 PM\nTo: Kaminski, Vince J\nCc: Crenshaw, Shirley\nSubject: Talk on Friday\nDear Vince\nHow are you? I understand that things are extremely hectic for you right\nnow\nbut I was wondering if we are going ahead as schedulef on friday. It\nwould\nbe great to hear from you.\nBest Regards\nPrakash\nPrakash Narayanan\n412-422-3287 (Home)\n412-607-5321 (Mobile)\n6315 Forbes Avenue\nApartment # 809\nPittsburg\nmale female\nStackExchange\nOpenWebText2\nPile-CC\nFreeLaw\nBooks3\nWikipedia (en)\nHackerNews\nOpenSubtitles\nPubMed Central\nPubMed Abstracts\nEnron Emails\nBookCorpus2\nUSPTO Backgrounds\nArXiv\nYoutubeSubtitles\nGithub\nPhilPapers\nGutenberg (PG-19)\nEuroParl\nUbuntu IRC\nNIH ExPorter\nDM Mathematics\n0.10\n0.05\n0.00\n0.05\n0.10\nFigure 13: The average sentiment co-occurrence with\neach gender across all datasets.\n33\n34\nMuslim Christian Atheist Buddhist Hindu Jew\nStackExchange\nOpenWebText2\nPile-CC\nFreeLaw\nBooks3\nWikipedia (en)\nHackerNews\nOpenSubtitles\nPubMed Central\nPubMed Abstracts\nEnron Emails\nBookCorpus2\nUSPTO Backgrounds\nArXiv\nYoutubeSubtitles\nGithub\nPhilPapers\nGutenberg (PG-19)\nEuroParl\nUbuntu IRC\nNIH ExPorter\nDM Mathematics\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 14: The average sentiment co-occurrence with\neach religious word across all datasets. Each dataset’s\nsentiments have been normalized by the maximum\nnorm sentiment for that dataset.\n35\nComponent GPT-2 GPT-3\nsmall medium large xl ada babbage curie davinci\nPile-CC 26.8894 20.5671 18.1656 16.9572 16.2430 13.0270 10.7532 8.4929\nPubMed Central 11.0626 8.9052 8.0454 7.5404 6.8800 5.7006 4.9390 4.3143\nBooks3 28.3889 22.0958 19.3424 17.7833 15.4209 12.4220 10.1526 7.1927\nOpenWebText2 23.6764 17.6175 15.1314 13.6267 12.0063 9.5439 7.7706 5.9163\nArXiv 14.2804 11.1896 10.0904 9.3330 7.5551 6.1541 5.2537 4.5341\nGithub 16.6814 7.9322 16.6742 13.3337 3.9614 3.1660 2.7398 2.4240\nFreeLaw 16.1000 11.7518 10.8427 10.0965 8.7976 7.0366 5.8256 4.8926\nStack Exchange 13.7202 9.3405 8.8467 8.3238 7.6652 5.9486 5.0267 4.3796\nUSPTO Backgrounds 15.1141 11.9232 10.5878 9.8095 9.2775 7.7000 6.5849 5.6411\nPubMed Abstracts 20.5642 15.2379 13.1190 11.9355 13.2112 10.4188 8.5861 7.1604\nGutenberg (PG-19) 26.4947 17.8975 16.4722 16.5112 12.5709 9.6349 7.7940 6.3112\nOpenSubtitles 22.7418 18.5724 17.0868 16.2709 16.2174 13.8561 11.8836 9.8578\nWikipedia (en) 27.0237 19.7570 17.4856 16.7849 12.9112 9.9453 7.8363 5.6915\nDM Mathematics 9.8990 8.7389 8.2928 7.9772 7.2458 6.5231 6.0171 5.6020\nUbuntu IRC 33.3028 26.1203 22.6128 20.9461 12.1138 9.6995 8.0628 6.5679\nBookCorpus2 25.0743 19.9725 17.6343 16.2905 16.1530 13.1796 11.0885 9.2205\nEuroParl 62.8981 36.9757 28.6198 23.4294 6.4996 5.3282 4.4982 3.8327\nHackerNews 45.0915 29.2599 32.0796 33.9774 22.1295 17.6314 14.6582 12.1283\nYoutubeSubtitles 25.7794 18.8173 15.9002 14.3104 8.4740 6.6394 5.4510 4.5235\nPhilPapers 30.1129 23.0288 20.3755 18.5649 14.4730 11.6785 9.6797 7.9915\nNIH ExPorter 23.9004 18.2298 15.9850 14.6371 16.1417 12.8744 10.6573 8.8110\nEnron Emails 34.7954 23.4353 25.7138 23.9791 16.8190 13.6043 11.6473 9.7655\nThe Pile 18.0878 13.2253 12.9177 11.8633 9.7355 7.8456 6.5904 5.4508\nTable 8: Test perplexity of the Pile using GPT-2 and GPT-3. Evaluation is performed on one-tenth of the test data\nof the Pile, on a per-document basis.\n36\nComponent GPT-2\nsmall medium large xl\nPile-CC 26.5 20.3 17.9 16.7\nPubMed Central 10.3 8.3 7.6 7.1\nBooks3 27.9 21.3 18.5 17.0\nOpenWebText2 23.5 17.5 15.1 13.6\nArXiv 13.7 10.7 9.7 9.0\nGithub 16.5 8.1 16.7 13.5\nFreeLaw 15.9 11.6 10.8 10.1\nStack Exchange 13.7 9.4 9.0 8.4\nUSPTO Backgrounds 16.6 13.0 11.5 10.6\nPubMed Abstracts 20.9 15.4 13.3 12.1\nGutenberg (PG-19) 37.8 24.9 22.8 24.3\nOpenSubtitles 22.1 18.1 16.6 15.8\nWikipedia (en) 27.0 19.8 17.5 16.8\nDM Mathematics 9.9 8.7 8.3 7.9\nUbuntu IRC 33.3 26.1 22.6 20.9\nBookCorpus2 25.1 20.0 17.6 16.3\nEuroParl 63.9 41.9 33.5 27.7\nHackerNews 43.7 28.3 30.9 32.4\nYoutubeSubtitles 25.3 18.8 16.2 14.8\nPhilPapers 30.1 23.0 20.4 18.6\nNIH ExPorter 23.2 17.7 15.5 14.2\nEnron Emails 22.0 15.4 18.6 18.1\nthe Pile 18.4 13.3 13.1 12.0\nTable 9: Full Test Perplexity of the Pile using GPT-2.\nMale Female\ngeneral little\nmilitary married\nunited sexual\npolitical happy\nfederal young\ngreat soft\nnational hot\nguilty tiny\ncriminal older\nformer black\nrepublican emotional\namerican worried\nmajor nice\nsuch live\noffensive lesbian\nTable 10: Top 15 most biased adjectives/adverbs for each gender\nMuslim Christian Atheist Buddhist Hindu Jew\nislamic adrian religious static indian little\ninternational available agnostic ﬁnal single white\nnew great such private free natal\namerican high liberal interested asian common\nblack bible likely central more false\nwestern good much chinese united poor\nbest old less japanese real demonic\nradical same least noble other german\nregional harmonious political complete british romantic\nentire third moral full cultural unlicensed\nnational special scientiﬁc fundamental social stupid\nown hispanic rational udisplaycontext lower nuclear\nsyrian biblical skeptic familiar local african\nbad original skeptical beneﬁcial general hard\nguilty happy intellectual native most criminal\nTable 11: Top 15 most biased adjectives/adverbs for each religion\n37\nWhite Black Asian Hispanic\nindian unarmed international likely\nrich civil western african\naboriginal scary chinese american\ngreat federal japanese mexican\nold diary best united\nsuperior political european cervical\ngood amish foreign spanish\nlittle nigerian eastern potential\nsame concerned secondary better\nred urban dietary medical\nstupid historical open more\nlive literary grand new\nequal criminal vietnamese educational\neternal worst russian young\nTable 12: Top 15 most biased adjectives/adverbs for each demographic\nWhite Black Asian Hispanic\n-0.114 -0.148 -0.028 -0.024\nTable 13: Average sentiment co-occurence of each demographic\nComponent Topic #1 Topic #2 Topic #3 Topic #4 Topic #5 Topic #6 Topic #7 Topic #8\nPile-CC Generic Politics Generic Technical Leisure Generic Plants Entertainment\nPubMed Central Cells Cells Cells Cells Cells Cells Cells Cells\nBooks3 Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown\nOpenWebText2 US Politics Law Sports Education Business Tech US Religion Generic\nArXiv Data Math Modeling Math Physics Physics Math Dynamics\nGithub Unknown Programming Unknown Java C/C++ Unknown Go Unknown\nFreeLaw Appeals Appeals Legal Legal Appeals Legal Legal Appeals\nStack Exchange Software Unknown Server Programming Applications File System Programming Users\nUSPTO Backgrounds Data Electronics Devices Unknown Data Unknown Chemistry Data\nPubMed Abstracts Organ Trans-\nplant\nNervous\nSystem\nAnimal Study Animal Study Ophthalmology Bacteria Pulmonology Fluids\nGutenberg (PG-19) Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknowne\nOpenSubtitles Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown\nWikipedia (en) Education International\nPolitics\nSports Sports Entertainment Entertainment Logistics Science\nDM Mathematics Calculation Probability Calculation Solving Calculation Calculation Probability Calculation\nUbuntu IRC Bugs Pull Requests Bugs Bugs Bugs Bugs Bugs Pull Requests\nBookCorpus2 Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown\nEuroParl International\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nHackerNews Generic Software Generic Generic Software Generic Generic Generic\nYoutubeSubtitles Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknowne\nPhilPapers Logic Science Science Mind Science Epistemology Logic Science\nNIH ExPorter Cells Disease Cells Cells Clinical Clinical Unknown Clinical\nEnron Emails Email Email Email Email Email Email Email Business\nTable 14: Topic Summaries\nComponent Topic #9 Topic #10 Topic #11 Topic #12 Topic #13 Topic #14 Topic $15 Topic #16\nPile-CC Education Politics Home Business Geography Sports Medicine Generic\nPubMed Central Cells Cells Cells Cells Cells Cells Cells Cells\nBooks3 Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown\nOpenWebText2 Drugs Sports Geogrpahy Crime Military Unknown Research Sports\nArXiv Dynamics Math Physics Physics Physics Physics Math Modeling\nGithub Unknown HTML/CSS HTML/CSS C/C++ Java C/C++ Unknown HTML/CSS\nFreeLaw Legal Legal Legal Legal Legal Legal Legal Appeals\nStack Exchange Programming HTML/CSS Programming Programming HTML/CSS Java SQL Java\nUSPTO Backgrounds Imaging Electronics Unknown Unknown Data Imaging Imaging Chemistry\nPubMed Abstracts Human Dis-\nease\nResearch Human Dis-\nease\nClinical Clinical Medical Imag-\ning\nCells Cells\nGutenberg (PG-19) Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown\nOpenSubtitles Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown\nWikipedia (en) Sports Geography Entertainment Unknown Geography Sports History Law\nDM Mathematics Calculation Differentiation Differentiation Solving Simpliﬁcation Calculation Units Unknown\nUbuntu IRC Software Software Software Bugs Software Pull Requests Software Bugs\nBookCorpus2 Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown\nEuroParl International\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nInternational\nPolitics\nHackerNews Generic Generic Generic Software Software Generic Generic Generic\nYoutubeSubtitles Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown\nPhilPapers Epistemology Science Logic Science Epistemology Science Logic Logic\nNIH ExPorter Cells Cells Disease Disease Disease Disease Disease Clinical\nEnron Emails Energy Email Email Email Email Email Computer Computer\nTable 15: Topic Summaries (continued)\nComponent Topic #1 Topic #2 Topic #3 Topic #4 Topic #5 Topic #6 Topic #7 Topic #8\nPile-CC like\ntime\ngood\nuse\nwant\npeople\nsaid\ngovernment\nwar\nright\nsaid\nlike\ntime\ngot\ngoing\nsystem\nsurface\nx\nhigh\nsecond\nnew\ndating\nart\nday\ncity\nlike\ntime\ngame\ngood\nfood\nwater\nplants\nfood\nclimate\nplant\nmusic\nlike\nbook\nnew\nﬁlm\nPubMed Central cells\ndata\nstudy\ncell\nresults\ncells\ncell\ndata\nﬁgure\net\nstudy\ncells\nc\ndata\ngroup\nstudy\ndata\npatients\ncells\nanalysis\ndata\nstudy\npatients\ncells\nc\npatients\ncells\nstudy\ncell\nanalysis\npatients\nstudy\ndata\ncells\nanalysis\ncells\ndata\ncell\nanalysis\npatients\nBooks3 time\nsaid\nlike\nnew\nknow\nlike\nsaid\ntime\nman\nway\nlike\nsaid\ntime\nnew\nright\nsaid\nlike\ntime\nknow\nnew\nsaid\nlike\ntime\nnew\nway\nsaid\nlike\ntime\nnew\ngood\nsaid\nlike\nway\nnew\ntime\nsaid\nnew\nlike\ntime\nman\nOpenWebText2 said\ntrump\npresident\nhouse\nstate\nsaid\ncourt\nlaw\ncase\nstate\nteam\nseason\ngame\nsaid\nplayers\npeople\nlike\nsaid\nschool\nlife\nsaid\ngovernment\nyear\nmarket\nbusiness\ndata\nuse\ngoogle\nsystem\nnew\npeople\nlaw\namerican\ngod\nworld\nlike\nworld\ntime\npeople\nday\nArXiv case\ngiven\ntime\nlet\ndata\nlet\nfunction\ncase\ngiven\norder\nfunction\ncase\nlet\nstate\nmodel\nlet\nset\nfollowing\nnumber\nx\nphys\ndata\nenergy\nﬁeld\nb\nlet\nmodel\nﬁeld\nsystem\nenergy\nlet\ngiven\ncase\nnumber\ntheorem\ntime\ncase\nlet\nset\ngiven\nGithub y\nd\nb\nabbr\nj\nreturn\nfunction\ndiv\nvar\nvalue\nvoid\nf\nlicense\nv\ncountries\nreturn\npublic\nint\ncase\nnull\nconst\ntypename\nreturn\nvoid\ntemplate\nfa\nvar\nspan\nﬁle\nkey\nerr\nreturn\nnil\nfunc\nerror\nimport\nmsgstr\nmsgid\ninsert\nlicense\nFreeLaw court\ntrial\nevidence\ncase\nstate\ncourt\nstate\ncase\ntrial\nevidence\ncourt\ndefendant\nstate\nstates\ntrial\ncourt\ndistrict\nplaintiff\ndefendant\nmotion\ncourt\ntrial\nevidence\ndefendant\nunited\ncourt\ndefendant\ntrial\nevidence\nstates\ncourt\ndefendant\ncase\nlaw\nmotion\ncourt\nstate\nevidence\ndefendant\ndistrict\nStack Exchange run\nq\nserver\nproject\nuse\ndata\noption\npdf\nq\nrails\nfunction\nserver\nuse\nthread\nclient\narray\nint\nvalue\nlike\ncode\ndevice\nspring\nandroid\napp\nboot\nﬁle\nimage\nﬁles\necho\npath\nﬁle\nline\nerror\nimport\npython\nq\nlike\nuse\nuser\nset\nUSPTO Backgrounds signal\nsystem\ninvention\nmemory\nline\ninvention\ndata\npower\nvoltage\nfrequency\ninvention\nsurface\nhaving\npresent\nliquid\npressure\nsystem\ninvention\ncells\nuse\ndata\nsystem\nmemory\ninformation\ndevices\nhigh\nair\nlight\ninvention\ntemperature\nal\net\ninvention\npresent\nwater\ninvention\ncircuit\ndata\npresent\nsignal\nPubMed Abstracts liver\ngroup\nacute\ntransplantation\nrenal\nactivity\nnerve\nstimulation\ninduced\nmuscle\nspecies\nstudy\nstudies\nassociated\nrisk\ndose\nmg\nrats\neffects\neffect\nretinal\neye\nlens\ncorneal\nlaser\nstrains\nisolates\nresistance\nresistant\nbacteria\np\nlevels\npatients\nblood\nincreased\nactivity\nacid\nhigh\nwater\nconcentration\nGutenberg (PG-19) said\ntime\nlittle\nman\ngreat\nsaid\ntime\nlittle\nlike\nman\nsaid\nman\ntime\ngreat\nmen\nsaid\ntime\ngreat\nman\nlittle\nsaid\ngreat\nlike\nman\nlittle\nsaid\nman\ntime\nlike\nday\nsaid\nman\ntime\nlittle\nlike\nman\nsaid\nlike\ntime\nlittle\nOpenSubtitles know\nright\ncome\ngot\nlike\nlike\nknow\ncome\nright\nwant\nknow\noh\nright\nyeah\nlike\nknow\nlike\noh\ngot\nright\nknow\nright\nlike\noh\nwant\nknow\nlike\nright\ngot\ncome\nknow\nright\nyeah\ngot\nlet\nknow\ngot\nright\nlike\noh\nWikipedia (en) category\nuniversity\nschool\namerican\ncollege\npeople\ngovernment\ncategory\npolitical\nchinese\ncategory\nplayers\npeople\nfootball\nborn\nchampionship\ncategory\ndriver\ncars\ncar\nﬁlm\ncategory\nﬁlms\nnew\ntelevision\ncategory\nmusic\nalbum\nsong\nreleased\ncategory\nrailway\nstation\nline\nnew\nsystem\ncategory\nenergy\nwork\nsystems\nDM Mathematics let\npm\nminutes\nfactor\ndivided\nletters\nlet\nreplacement\nsequence\nprob\ncollect\nterms\npositive\nassuming\nsimplify\nlet\nsuppose\nsolve\nnearest\nc\nlet\ncommon\ncalculate\nsuppose\nhighest\nlet\nsolve\nsuppose\nbase\ncalculate\nfactors\nprime\nreplacement\nletters\nlist\nsolve\nremainder\ndivided\ncalculate\ntrue\nUbuntu IRC ubuntu\nlike\nthink\nbug\nneed\nubuntu\nbug\nlike\nthink\nsnap\nlike\nubuntu\nknow\ncreated\nok\nubuntu\nlike\nthink\nneed\nyeah\nubuntu\nlike\nthink\nyeah\nknow\nlike\nubuntu\nthink\ngood\nyeah\nubuntu\nlike\nbug\nuse\nknow\nubuntu\ngood\nsnap\nuse\nlike\nBookCorpus2 said\nlike\ntime\nknow\neyes\nsaid\nlike\nknow\nway\neyes\nsaid\nknow\nlike\ntime\nright\nsaid\nlike\ntime\nknow\ngoing\nsaid\nlike\ntime\nknow\ngoing\nsaid\nlike\nknow\ntime\nhead\nsaid\nlike\ntime\nknow\ngoing\nsaid\nknow\ntime\nlike\ngoing\nEuroParl european\nmr\ncommission\npresident\neurope\neuropean\npresident\ncommission\nmr\nunion\nmr\neuropean\npresident\ncommission\nparliament\neuropean\nmr\npresident\ncommission\nenergy\neuropean\nmr\ncommission\npresident\nparliament\ncommission\npresident\neuropean\nmr\nparliament\neuropean\ncommission\nmr\npresident\ncouncil\neuropean\ncommission\nmr\nunion\nparliament\nHackerNews like\npeople\nwork\ntime\nuse\nlike\npeople\nwork\ntime\nuse\nlike\npeople\nwork\ntime\nthink\npeople\ntime\nlike\nway\nwork\npeople\nlike\ntime\nthink\nuse\nlike\npeople\nwork\ntime\nthink\nlike\npeople\ntime\nthink\nwork\npeople\nlike\nwork\ngood\nuse\nYoutubeSubtitles like\nknow\ngoing\nthink\nright\nknow\nlike\ngoing\npeople\ntime\nlike\ngoing\nright\ntime\nknow\nlike\ngoing\nguest\npeople\nthink\nlike\nthink\npeople\nknow\ngoing\nlike\nknow\ngoing\nlook\nhost\nlike\nknow\nthink\nright\ngoing\nlike\npeople\nknow\ngoing\ntime\nPhilPapers theory\ncase\nφ\nreduction\nparadox\nphilosophy\ncase\nmoore\nphysical\ntheory\ncase\nscience\ntheory\nset\nworld\nself\ncase\nscience\ntheory\nanalysis\ntheory\nscience\nphysical\nde\ncase\ncase\ntheory\nscience\ns\nepistemic\nderivation\nreduction\nφ\nparadox\nψ\ncase\nde\nset\nscience\ntheory\nNIH ExPorter cells\ncell\nstudies\nresearch\nstudy\ncell\nstudies\nresearch\ncells\nstudy\ncell\nresearch\ngene\ndevelopment\nstudy\nresearch\ndetermine\nhealth\nspeciﬁc\ncells\ncells\nspeciﬁc\ncell\nstudy\naim\nstudies\ndevelopment\npatients\nanalysis\nclinical\nresearch\nstudy\ndevelopment\nspeciﬁc\nuse\nstudy\nresearch\nclinical\nstudies\ndevelopment\nEnron Emails subject\npm\nnew\ntime\nenergy\npm\nsubject\nenron\ncc\nknow\npm\nnew\nenron\ntime\nimage\nenron\ne\nmail\nnew\nsubject\nsubject\nenron\nsaid\nsent\nimage\nsubject\npm\nenron\ndatabase\nsent\nhou\npm\nsubject\ne\ntime\nﬁnal\nenron\nschedule\nenergy\ninformation\nTable 16: Topic Terms\nComponent Topic #9 Topic #10 Topic #11 Topic #12 Topic #13 Topic #14 Topic $15 Topic #16\nPile-CC students\nschool\nwork\nuniversity\nresearch\nsaid\nstate\ngovernment\nlaw\ncourt\nhome\nroom\nhouse\nhotel\narea\nuse\nbusiness\ndata\ninformation\nnew\nsaid\nnew\ncity\nyears\npolice\ngame\nteam\nseason\nyear\nsaid\nhealth\nmedical\ncare\ntreatment\nbody\npeople\nlike\nknow\nthink\nlife\nPubMed Central cells\nexpression\npatients\ncell\nstudy\ndata\ncells\npatients\nstudy\nc\ncells\nstudy\ncell\ndata\nﬁgure\nstudy\ndata\np\nﬁg\ncells\nstudy\np\ncells\nc\npatients\ncells\nstudy\ndata\ntime\ngroup\npatients\nstudy\ncells\net\ndata\nstudy\ncells\npatients\ncancer\nﬁgure\nBooks3 said\nlike\ntime\nman\ngood\nsaid\nlike\ntime\nknow\nway\nsaid\nlike\ntime\nway\nnew\nsaid\nlike\ntime\nnew\nknow\nsaid\nlike\ntime\npeople\nman\nsaid\nlike\nnew\npeople\nman\nsaid\ntime\nlike\nman\nnew\nsaid\ntime\nnew\npeople\nlike\nOpenWebText2 drug\ncannabis\ndrugs\nmarijuana\nwomen\nlike\ntime\nnew\ngame\nway\ncity\nnew\nunlockable\nbuilding\nsaid\nsaid\npolice\npeople\nman\nold\ngame\nwar\nparty\nmilitary\nsaid\nﬂight\ncaption\naircraft\nadd\nwater\nstudy\nresearch\ntime\nclimate\nfound\nv\ngranada\nclub\nm\ncent\nArXiv time\nfunction\nr\nmodel\nal\nlet\nnumber\nmodel\nsystem\ntheorem\nlet\nset\nspace\nmodel\ngiven\nlet\ntheorem\ngiven\ncase\nx\nmodel\norder\nenergy\nlet\nphys\nlet\nphys\norder\nmodel\ncase\nx\nlet\ntime\nﬁeld\nset\nlet\nmodel\ndata\nset\nfunction\nGithub string\nlicense\ndef\npublic\nimport\nx\nz\ndivide\nvar\ny\nend\nvalues\nlist\ncolor\ntable\ndeﬁne\nsoftware\ncopyright\ninclude\nendif\nvoid\nvalue\npublic\nreturn\nclass\nint\nstruct\nreturn\ncase\nstatic\nreturn\nself\nsize\nlong\nstring\nvar\nassert\ntext\nlabel\ncheck\nFreeLaw court\ndefendant\nstate\ntrial\nplaintiff\ncourt\nâ\nlaw\ncase\nstate\ncourt\nplaintiff\nstate\ncase\nevidence\ncourt\ndefendant\nstates\nplaintiff\ncase\ncourt\ndistrict\nstates\ncase\nunited\ncourt\nplaintiff\nstate\ncase\ndistrict\ncourt\ntrial\nstate\ndefendant\ncase\ncourt\nstates\ndistrict\nunited\ntrial\nStack Exchange x\ny\nq\nd\nc\ntext\ncolor\nwidth\nfont\nheight\ncode\nn\nuse\nint\ninclude\nb\nq\nclass\nn\nk\ndiv\npage\nfunction\nvar\nform\nstring\nreturn\npublic\nnew\nclass\ntable\nselect\nquestion\nlike\nq\nandroid\nnew\npublic\nimport\nvoid\nUSPTO Backgrounds image\nlight\ndata\noptical\nsystem\ndevice\ninvention\nlayer\nﬁlm\npower\ninvention\nmaterial\nlight\nmethod\nhigh\ninvention\npresent\ndevice\nobject\nprovide\ndata\nnetwork\nsystem\nuser\ninformation\noptical\nsurface\ndevice\ninvention\nsystem\nimage\nlight\ndevice\nsheet\ndisplay\ninvention\nlayer\nsubstituted\net\ngroup\nPubMed Abstracts women\npatients\npositive\nhiv\ncancer\nhealth\ndata\ncare\nbased\nstudy\nbone\nasthma\nvaccine\nstudy\nsperm\npatients\ntreatment\ngroup\nclinical\npatient\npatients\ndisease\nstudy\ncases\nage\nmethod\nartery\nsurface\nenergy\noptical\ncells\ncell\nexpression\ngene\nprotein\nbinding\nprotein\nreceptor\ndna\nbeta\nGutenberg (PG-19) said\nlittle\ntime\nman\nold\nsaid\nman\nlittle\nlike\ntime\nsaid\nman\nlittle\ntime\ngreat\nsaid\ngreat\nlittle\nman\nlike\ntr\nsaid\nman\ntime\nlittle\nsaid\nman\ngreat\ntime\nlike\ntime\nsaid\nmen\nman\nlike\ntr\nsaid\nman\ntime\nlittle\nOpenSubtitles know\nright\ngot\nlike\noh\nlike\nknow\nright\nthink\noh\nknow\nlike\ncome\nright\ngood\nknow\ncome\ngot\noh\nright\nknow\nright\nlike\nyeah\ncome\nknow\nright\noh\nlike\ncome\nknow\nright\nlike\nthink\ncome\nknow\nlike\nright\nwant\ngot\nWikipedia (en) align\nseason\npoints\ngame\nright\ncategory\nnew\nstate\nunited\nstates\ncategory\ngame\nﬁlm\nseries\nvideo\ncategory\npopulation\nspecies\nage\noil\ncategory\ncounty\ndistrict\nreferences\nvillage\nseason\nteam\nleague\nplayer\nnﬂ\ncategory\ncentury\nwar\nnew\nde\ncategory\nnew\nstates\nlaw\namerican\nDM Mathematics common\nlet\ndivided\ncalculate\nfactor\nlet\nderivative\nwrt\nsecond\nﬁnd\nderivative\nwrt\nﬁnd\nexpress\nrearrange\nlet\nsuppose\nsolve\nb\nc\nlet\nsuppose\nvalue\nb\nsimplify\nbase\nc\ncommon\npicked\nb\ndigit\nterms\ncollect\nthousands\nlet\nlet\nsuppose\nderivative\nc\ndetermine\nUbuntu IRC ubuntu\nlike\ntime\nthink\nsnap\nubuntu\nlike\nneed\nuse\nsnap\nubuntu\nthink\nlike\nyeah\nuse\nlike\nubuntu\nthink\nuse\nneed\nubuntu\nlike\nthink\nwork\ntime\nubuntu\nlike\nthink\nyeah\nyes\nubuntu\nlike\nthink\ngood\nà\nlike\nubuntu\nneed\nok\njuju\nBookCorpus2 said\nlike\nknow\nright\ntime\nsaid\nlike\nknow\ntime\ngoing\nsaid\nlike\nknow\neyes\ntime\nsaid\nlike\ntime\nknow\ngoing\nsaid\nlike\nknow\ngoing\ntime\nsaid\nlike\neyes\ntime\ngoing\nsaid\nlike\nknow\ntime\ngoing\nsaid\nlike\nknow\nlooked\ngoing\nEuroParl mr\ncommission\npresident\neuropean\niran\neuropean\nmr\npresident\ncommission\nparliament\nmr\neuropean\ncommission\nparliament\npresident\neuropean\nmr\ncommission\npresident\nparliament\neuropean\ncommission\nmr\nparliament\npresident\neuropean\nmr\ncommission\nparliament\npresident\neuropean\nmr\ncommission\nparliament\npresident\neuropean\nmr\nparliament\ncommission\npresident\nHackerNews people\nlike\nthink\ntime\nuse\nlike\npeople\ntime\nthink\nuse\npeople\nlike\ndata\ntime\nwork\nuse\nlike\nwork\ntime\nthink\nlike\npeople\ntime\nwork\ndata\npeople\ntime\nlike\nwork\ngood\nlike\npeople\ntime\nuse\nthink\nlike\ntime\npeople\nthink\nuse\nYoutubeSubtitles like\nknow\npeople\ngoing\ntime\nlike\ntime\nknow\ngoing\nthink\nlike\nhost\nguest\nknow\nlook\nlike\nthink\nknow\npeople\ngoing\nlike\nthink\nright\nknow\npeople\nlike\nknow\nwant\ngoing\nright\nlike\nknow\ngoing\npeople\nlook\nlike\nknow\npeople\nthink\ngoing\nPhilPapers theory\ns\ncase\nbelief\nexperience\ntheory\nscience\nset\nde\nphilosophy\nφ\nreduction\ntheory\nparadox\nderivation\ntheory\ncase\norder\nspace\nnew\nepistemic\nscience\nbelief\ntheory\nsystem\ntheory\ncase\nscience\nphysics\ntheories\nφ\nderivation\nreduction\nt\nparadox\ncase\nφ\ns\nderivation\ntheory\nNIH ExPorter cells\nresearch\nspeciﬁc\nstudies\nrole\nresearch\ncells\ncell\nstudies\nstudy\ndisease\nresearch\nstudy\ncells\ncell\ncells\ncell\nstudy\ndisease\nhuman\ncells\nspeciﬁc\ndevelopment\nstudies\nresearch\nresearch\ncells\nstudies\ncell\nproject\ncell\nresearch\ncells\nspeciﬁc\nstudies\nresearch\nstudies\nclinical\ndetermine\ncancer\nEnron Emails time\nnew\nenron\npower\nsubject\nsubject\npm\nfriday\nsent\noctober\nhou\nenron\nsubject\ncc\nna\nimage\nenron\npm\nhou\nsubject\nsubject\nmessage\npm\nknow\ncc\nhou\nenron\nsubject\ncc\ngas\nspace\nalias\ndisk\nenron\nsaid\nhou\ndisk\nspace\nalias\ne\nTable 17: Topic Terms (continued)",
  "topic": "Pile",
  "concepts": [
    {
      "name": "Pile",
      "score": 0.6266210079193115
    },
    {
      "name": "Computer science",
      "score": 0.579666018486023
    },
    {
      "name": "Natural language processing",
      "score": 0.4753926992416382
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34920650720596313
    },
    {
      "name": "Algorithm",
      "score": 0.07322770357131958
    }
  ]
}