{
  "title": "A systematic review for transformer-based long-term series forecasting",
  "url": "https://openalex.org/W4406080987",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2789978912",
      "name": "Liyilei Su",
      "affiliations": [
        "Shenzhen University",
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A4280928026",
      "name": "Xumin Zuo",
      "affiliations": [
        "Shenzhen Technology University",
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2041764006",
      "name": "Rui Li",
      "affiliations": [
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2045332956",
      "name": "Xin Wang",
      "affiliations": [
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2109405064",
      "name": "Heng Zhao",
      "affiliations": [
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2148353414",
      "name": "Bingding Huang",
      "affiliations": [
        "Shenzhen Technology University",
        "Shenzhen University"
      ]
    },
    {
      "id": "https://openalex.org/A2789978912",
      "name": "Liyilei Su",
      "affiliations": [
        "Shenzhen University",
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A4280928026",
      "name": "Xumin Zuo",
      "affiliations": [
        "Shenzhen University",
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2041764006",
      "name": "Rui Li",
      "affiliations": [
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2045332956",
      "name": "Xin Wang",
      "affiliations": [
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2109405064",
      "name": "Heng Zhao",
      "affiliations": [
        "Shenzhen Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2148353414",
      "name": "Bingding Huang",
      "affiliations": [
        "Shenzhen University",
        "Shenzhen Technology University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3095867871",
    "https://openalex.org/W3188872815",
    "https://openalex.org/W3184127157",
    "https://openalex.org/W3007643464",
    "https://openalex.org/W3089028909",
    "https://openalex.org/W3187289530",
    "https://openalex.org/W3022643593",
    "https://openalex.org/W3109365969",
    "https://openalex.org/W4280531713",
    "https://openalex.org/W2808535700",
    "https://openalex.org/W2169851606",
    "https://openalex.org/W2275088575",
    "https://openalex.org/W3043193240",
    "https://openalex.org/W2586538484",
    "https://openalex.org/W2135691157",
    "https://openalex.org/W2055877700",
    "https://openalex.org/W4246587917",
    "https://openalex.org/W2990598036",
    "https://openalex.org/W3008871396",
    "https://openalex.org/W2964758013",
    "https://openalex.org/W2550257869",
    "https://openalex.org/W2070534370",
    "https://openalex.org/W2798629644",
    "https://openalex.org/W3187228821",
    "https://openalex.org/W3176196442",
    "https://openalex.org/W3188779816",
    "https://openalex.org/W3012629428",
    "https://openalex.org/W2624190409",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2885195348",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W3198794504",
    "https://openalex.org/W3171884590",
    "https://openalex.org/W3034749137",
    "https://openalex.org/W2963532813",
    "https://openalex.org/W3210546159",
    "https://openalex.org/W4289779195",
    "https://openalex.org/W3198281649",
    "https://openalex.org/W2977251608",
    "https://openalex.org/W2988226917",
    "https://openalex.org/W4220796488",
    "https://openalex.org/W3101467051",
    "https://openalex.org/W3037103187",
    "https://openalex.org/W4290945834",
    "https://openalex.org/W4385270114",
    "https://openalex.org/W2962850830",
    "https://openalex.org/W2762309767",
    "https://openalex.org/W1969852690",
    "https://openalex.org/W2747599906",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W2980994438",
    "https://openalex.org/W3150345082",
    "https://openalex.org/W4283318673",
    "https://openalex.org/W4207023128",
    "https://openalex.org/W4210263262",
    "https://openalex.org/W3097237405",
    "https://openalex.org/W4382203079",
    "https://openalex.org/W6606697098",
    "https://openalex.org/W1978475619",
    "https://openalex.org/W4206416499",
    "https://openalex.org/W2050312581",
    "https://openalex.org/W3017116930",
    "https://openalex.org/W2296521892",
    "https://openalex.org/W3080253043",
    "https://openalex.org/W3198749684",
    "https://openalex.org/W2940914091",
    "https://openalex.org/W3006135925",
    "https://openalex.org/W2907856150",
    "https://openalex.org/W1567080084",
    "https://openalex.org/W2516939159",
    "https://openalex.org/W3198316914",
    "https://openalex.org/W2808014509",
    "https://openalex.org/W3003862857",
    "https://openalex.org/W2905967367",
    "https://openalex.org/W2531473348",
    "https://openalex.org/W3156351347",
    "https://openalex.org/W3171958173",
    "https://openalex.org/W3175924508",
    "https://openalex.org/W3155157989",
    "https://openalex.org/W2528639018",
    "https://openalex.org/W3156972038",
    "https://openalex.org/W3135339890",
    "https://openalex.org/W2187731079",
    "https://openalex.org/W4303700118",
    "https://openalex.org/W2914487400",
    "https://openalex.org/W2799473886",
    "https://openalex.org/W2939221075",
    "https://openalex.org/W3042316884",
    "https://openalex.org/W2016210396",
    "https://openalex.org/W1480773628",
    "https://openalex.org/W1421897066",
    "https://openalex.org/W3122598275",
    "https://openalex.org/W4249224151",
    "https://openalex.org/W3020866103",
    "https://openalex.org/W3080957353",
    "https://openalex.org/W3132782787",
    "https://openalex.org/W4232630368",
    "https://openalex.org/W3129166376"
  ],
  "abstract": null,
  "full_text": "Accepted: 29 November 2024 / Published online: 6 January 2025\n© The Author(s) 2024\nLiyilei Su and Xumin Zuo have been contributed equally to this work.\n \r Heng  Zhao\nzhaoheng@sztu.edu.cn\n \r Bingding Huang\nhuangbingding@sztu.edu.cn\n1 College of Big Data and Internet, Shenzhen Technology University, Shenzhen 518188, China\n2 College of Applied Sciences, Shenzhen University, Shenzhen 518060, China\nA systematic review for transformer-based long-term series \nforecasting\nLiyilei Su1,2 · Xumin Zuo1,2 · Rui Li1 · Xin Wang1 · Heng Zhao1 · Bingding Huang1,2\nArtificial Intelligence Review (2025) 58:80\nhttps://doi.org/10.1007/s10462-024-11044-2\nAbstract\nThe emergence of deep learning has yielded noteworthy advancements in time series fore -\ncasting (TSF). Transformer architectures have witnessed broad utilization and adoption \nin TSF tasks. Transformers have proven to be the most successful solution to extract the \nsemantic correlations among the elements within a long sequence. Various variants have \nenabled Transformer architecture to effectively handle long-term time series forecasting \n(LTSF) tasks. In this article, we first present a comprehensive overview of Transformer \narchitectures and their subsequent enhancements developed to address various LTSF tasks. \nThen, we summarize the publicly available LTSF datasets and relevant evaluation metrics. \nFurthermore, we provide valuable insights into the best practices and techniques for ef -\nfectively training Transformers in the context of time-series analysis. Lastly, we propose \npotential research directions in this rapidly evolving field.\nKeywords Long-term time series forecasting · Deep learning · Transformer · Self-\nattention · Multi-head attention\n1 Introduction\nThe time series is usually a set of random variables observed and recorded sequentially over \ntime. Key research directions for time-series data are classification [1, 2], anomaly detection \n[3–5], event prediction [ 6–8], and time series forecasting [ 9–11]. Time series forecasting \n(TSF) predicts the future trend changes of time series from a large amount of data in various \nfields. With the development of data collection technology, the task gradually evolves into \n1 3\nL. Su et al.\nusing more historical data to predict the longer-term future, which is long-term time series \nforecasting (LTSF) [12, 13]. Precise LTSF can offer support to decision makers to better \nplan for the future by forecasting outcomes further in advance, including meteorology pre -\ndiction [14], noise cancellation [15], financial long-term strategic guidance [16], power load \nforecasting [17, 18], and traffic road condition prediction [19].\nFormerly, traditional statistical approaches were applied to time series forecasting, such \nas autoregressive (AR) [ 20], moving average (MA) [ 21] models, auto-regressive moving \naverage (ARMA) [22], AR Integrated MA (ARIMA) [23], and spectral analysis techniques \n[24]. However, these traditional statistical methods require many a priori assumptions on \nthe time-series prediction, such as stability, normal distribution, linear correlation, and inde-\npendence. For example, AR, MA, and ARMA models are based on the assumption that time \nseries are stationary, but in many real cases, time-series data exhibit non-stationarity. These \nassumptions limit the effectiveness of these traditional methods in real-world applications.\nAs it is difficult to effectively capture the nonlinear relationships between time series with \ntraditional statistical approaches, many researchers have studied LTSF from the perspective \nof machine learning (ML) [ 25–29]. Support vector machines (SVMs) [ 30] and adaptive \nboosting (AdaBoost) [31] were employed in the field of TSF. They calculate data metrics, \nsuch as minimum, maximum, mean, and variance, within a sliding window as new features \nfor prediction. These models have somewhat solved the problem of predicting multivariate, \nheteroskedastic time series with nonlinear relationships. However, they suffer from poor \ngeneralization, which leads to limited prediction accuracy.\nDeep learning (DL) models (Fig. 1) have greatly improved the nonlinear modeling capa-\nbilities of TSF in recent years. These models are constructed with neural network struc -\ntures with powerful nonlinear modeling capabilities to learn complex patterns and feature \nrepresentations in time series automatically. Therefore, DL is an effective solution for TSF \nand many other problems related to TSF, such as hierarchical time series forecasting [ 32], \nintermittent time series forecasting [33], and sparse multivariate time series forecasting [34] \nasynchronous time series forecasting [35, 36]. It has extended some multi-objective, multi-\ngranular forecasting scenarios [ 37] and multi-modal time series forecasting scenarios [ 38, \nFig. 1 The development history of TSF algorithms based on deep learning\n \n1 3\n80 Page 2 of 29\nA systematic review for transformer-based long-term series forecasting\n39]. The advantage of deep learning models can be attributed to their profound flexibility \nand ability to capture long-term dependencies and handle large-scale data.\nIt is noteworthy that recurrent neural networks (RNNs) [ 40] and their variants, such as \nlong short-term memory networks (LSTMs) [41] and gated recurrent units (GRUs) [42–44], \nare widely employed among deep learning models to process sequence data. These mod -\nels process batches of data sequentially using a gradient descent algorithm to optimize the \nunknown model parameters. The gradient information of the model parameters is updated \nby back-propagation through time [45]. However, due to the sequential processing of input \ndata and back-propagation through time, they suffer from some limitations, especially when \ndealing with datasets with long dependencies. The training process of LSTM and GRU \nmodels also suffers from gradient vanishing and explosion. Though some architectural mod-\nifications and training techniques can help LSTM and GRU to alleviate the gradient-related \nproblems to some extent, the effectiveness and efficiency of RNN-based models may still be \ncompromised [46]. Furthermore, it is possible to apply models like Convolutional Neural \nNetwork (CNN) to conduct time-series analysis.\nOn the other hand, the Transformer [47] is a model that combines various mechanisms, \nsuch as attention, embedding, and encoder-decoder structures, in natural language process-\ning. Later, studies improved the Transformer and gradually applied it to TSF, imaging, \nand other areas, making Transformers progressively their genre. Recent advancements in \nTransformer-based models have shown substantial progress [12, 48, 49]. The self-attentive \nmechanism of the Transformer allows for adaptive learning of short-term and long-term \ndependencies through pairwise (query-key) request interactions. This feature grants the \nTransformer a significant advantage in learning long-term dependencies on sequential data, \nenabling the creation of more robust and expansive models [50]. The performance of Trans-\nformers on LTSF is impressive, and they have gradually become the current mainstream \napproach.\nThe two main tasks of time-series data are forecasting and classification. Forecasting \naims to predict real values from given time-series data, while the classification task cat -\negorizes given time-series data into one or more target categories. Many advances have \nbeen made in time-series Transformers for forecasting [ 12, 49, 51–59] and classification \ntasks [1, 60–62]. However, genuine time-series data tends to be noisy and non-stationary, \nand learning spurious dependencies, lacking interpretability, can occur if time-series-related \nknowledge is not combined. Thus, challenges still need to be addressed despite the notable \nachievements in accurate long-term forecasting using Transformer-based models.\nFollowing The Preferred Reporting Items for Systematic Reviews and Meta Analysis \n(PRISMA) standard, we conducted a systematic review and used digital databases, includ -\ning Google Scholar, Elsevier, and Springer Link. We used the most pertinent keywords, \nsuch as “vision transformer” and “long-term time series forecasting”, to choose the most \nrelevant to our study. The number of articles yielded through Google Scholar, Elsevier, \nand Springer Link since 2020 was 958, 1143, and 703, respectively, for a total of 2804. We \nfirst removed duplicate papers, then evaluated the titles and abstracts of these articles, and \nfurther reviewed the full text. A total of 59 articles were included in the study. The summary \nof search results for research articles is shown in Fig. 2.\nIn this review, we commence with a comprehensive overview of Transformer archi -\ntecture in Sect. 2. Section 3 presents Transformer-based architectures for LTSF in recent \nresearch. In Sect. 4, we analyze Transformer effectiveness for LTSF. Subsequently, Sect. 5 \n1 3\nPage 3 of 29 80\nL. Su et al.\nsummarizes the public datasets and evaluation metrics in LTSF tasks. Section 6 introduces \nseveral training strategies in existing Transformer-based LTSF solutions. Finally, we con -\nclude this review in Sect. 7.\n2 Transformer\nIn this section, we begin by analyzing the inherent mechanics of the Transformer proposed \nby Vaswani et al. [63] in 2017, to present solutions to the challenge of neural machine trans-\nlation. Figure 3 shows the Transformer architecture. Subsequently, we delve into the opera-\ntions within each constituent of the Transformer and the underlying principles that inform \nthese operations. Several variants of the Transformer architecture have been proposed for \ntime-series analysis; however, our discussion in this section is limited to the original archi-\ntecture [64] [65].\nThe Transformer network has two parts, the encoder and the decoder, with self-attention \nfor neural sequence transduction. The encoder component encompasses two primary net -\nworks: the multi-head attention mechanism and the two-layer feed-forward neural network. \nIt handles symbolic relationships of an input categorization to an incessant relation. The \ndecoder is similar to the encoder, albeit with an extra multi-head attention mechanism that \ninteracts with the encoder output. Unlike the encoder, the decoder comprises three parts of \nthe network structure. The top and bottom segments resemble the encoder, save for a middle \nsection that engages with the encoder’s output, referred to as “encoder-decoder attention”. \nThe decoder part of the transformer model engenders an output sequence one after the \nother. Each stage auto-degenerates and exploits the earlier input as supplementary to the \nnext word.\nFig. 2 Article retrieval and selection process based on PRISMA standard\n \n1 3\n80 Page 4 of 29\nA systematic review for transformer-based long-term series forecasting\nFig. 3 Schematic diagram of Transformer\n \n1 3\nPage 5 of 29 80\nL. Su et al.\n2.1 Self-attention\nThe self-attention mechanism is a process that involves mapping a query and a sequence \nof key-value pairs to generate a corresponding output vector. The resulting vector is deter -\nmined by the summation of weights acting on values computed from the query and key. A \nschematic representation of the self-attention mechanism is depicted in Fig. 4.\nAs shown in Fig. 4, the core of the self-attention mechanism is to get the attention \nweights by calculating Q and K and then act on V to get the whole weights and outputs. Q, \nK, and V are the input sequence’s Query, Key, and Value matrices after linear transforma -\ntion. Concerning the input sequence denoted as X, the parameters Q, K, and V are given by\n Q = WqX, K = WkX, and V= Wv. (1)\nQ, K, and V are computed by multiplying the input X by three different matrices (but this \nis only limited to the encoder and decoder encoding process using the self-attention mecha-\nnism in their respective inputs; the Q, K, and V in the interaction between the encoder and \ndecoder are referred to otherwise). The computed Q, K, and V can be interpreted as three \ndifferent linear transformations of the same input to represent its three different states. The \nFig. 4 Schematic diagram of \nself-attention\n \n1 3\n80 Page 6 of 29\nA systematic review for transformer-based long-term series forecasting\nweight vectors can be further computed after Q, K, and V are computed. Specifically, for the \ninputs Q, K, and V , the weight vectors are calculated as:\n \nAttention(Q, K, V)= softmax\n(QKT\n√dk\n)\nV. (2)\nThe dimension of the query and key is denoted by dk. The attention for each position \nis normalized using the softmax function. The formula illustrates that the attention score \nmatrix can be derived by executing a dot product operation between the query and key, fol-\nlowed by division with a scaling factor of √dk. Subsequently, the attention weights for each \nposition are obtained by performing a softmax operation on the attention score matrix. The \nultimate self-attention representation is achieved by multiplying the attention weights with \nthe value matrix. The compatibility function employed in this process is a scaled dot prod -\nuct, thus rendering the computation process efficient. Additionally, the linear transformation \nof the inputs introduces ample expressive power. As illustrated in Fig. 3, the scale process \ncorresponds to the division of dk in Eq. 2. It is imperative to note that scaling is essential \nbecause, for larger dk, the value obtained after QKT  is excessively large, consequently \ncausing a diminutive gradient after the softmax operation. The diminutive gradient hinders \nthe training of the network and, thus, is not conducive to the overall outcome.\n2.2 Multi-head attention\nThe self-attention mechanism solves the sequential encoding challenge encountered in con-\nventional sequence models. It enables the generation of a final encoded vector that incor -\nporates attention information from multiple positions, achieved through a finite number of \nmatrix transformations on the initial inputs. However, it is worth noting that the model’s \nencoding of positional information may lead to an overemphasis on its position, potentially \nneglecting the importance of other positions. To address this issue, the Multi-Head Attention \nmechanism has been introduced.\nAs shown in Fig. 5, the multi-attention mechanism is a self-attention processing pro -\ncess of multiple groups of the original input sequences, and then each group of self-atten -\ntion results is spliced together to perform a linear transformation to obtain the final output \nresults. Specifically, its calculation formula is:\n Multi-Head (Q, K, V) Concat =( head1,... , headh) WO\n headi = Attention(QWQi,KW Ki,VW Vi ) . (3)\nIn this context, the matrices Q, K, and V refer to the input sequences’ query, key, and value \nmatrices, respectively, subsequent to linear transformation. The variable h denotes the num-\nber of attention heads. Additionally, the weight matrices WQi,W Ki, andWVi  are utilized \nto carry out the linear transformation on Q, K, and V . The output weight matrix of the \nmulti-head attention is denoted by the symbol WO. The computation of a single attention \nhead is denoted by attention in Eq. 2, equivalent to the previously mentioned self-attention \nmechanism. Each attention head maps the inputs through independent linear transforma -\ntions and applies the attention mechanism to obtain the representation. The final output of \n1 3\nPage 7 of 29 80\nL. Su et al.\nthe multi-head attention is obtained by combining the representations of all attention heads \nand using a linear transformation to the output weight matrix WO.\n3 Transformer-based architectures in L TSF\nThe design of a network needs to consider the characteristics and nature of problems. In this \nsection, we first analyze the key problems in the LTSF tasks, followed by discussing some \npopular recent Transformer-based architectures in LTSF tasks.\n3.1 LTSF’s key problems\nLTSF is usually defined as forecasting a more distant future [ 12, 66]. Given the current \nstatus of the existing work, there are two main problems in the field of LTSF: complexity \nand dependency. LTSF requires processing a large amount of data [67], which may lead to \nlonger training times and require more computational resources [ 68], as the computational \ncomplexity grows exponentially with the length of the sequence. Additionally, storing the \nentire sequence in memory may be challenging due to the computer’s limited memory [69]. \nThis may limit the length of the time series available for prediction [70].\nMeanwhile, LTSF models need to have the ability to accurately capture the temporal \nrelationship between past and future observations in a time series [ 71–73]. Long-sequence \ntime series exhibit long-term dependence [ 74, 75], challenging the models’ ability to cap -\nture dependencies [12]. Moreover, LTSF is characterized by inherent periodicity and non-\nstationarity [76], and thus, LTSF models need to learn a mixture of short-term and long-term \nFig. 5 Multi-head attention \n1 3\n80 Page 8 of 29\nA systematic review for transformer-based long-term series forecasting\nrepeated patterns in a given time series [67]. A practical model should capture both repeated \nways to make accurate predictions, which imposes more stringent requirements on the pre-\ndiction model regarding learning dependence.\n3.2 Transformer variants\nA Transformer [ 47] mainly captures correlations among sequence data through a self-\nattention mechanism. Compared with the traditional deep learning architecture, the self-\nattention mechanism in a Transformer is more interpretable. We have chosen to compare \nthe Transformer-related methods proposed in the last five years. All of these Transformer \nvariants enhance the original Transformer to some extent and can be used for LTSF. Wu \net al. [53] introduced the vanilla Transformer to the field of temporal prediction for influ -\nenza disease prediction. However, as mentioned above, Transformers have a large computa-\ntional complexity, leading to high computational costs. Moreover, the utilization of location \ninformation is not apparent, the position embedding in the model embedding process is \nineffective, and long-distance information cannot be captured. A brief conclusion of recent \nTransformer-based architectures is given in Table 1.\nThe time complexity of self-attention computation in a Transformer is initially estab -\nlished at O\n(\nL2)\n, leading to high computational cost. Some subsequent works have been \ndeveloped to optimize this time complexity and the long-term dependency of Transformer-\nbased models.\nThe LogSparse Transformer [ 49] model first introduces Transformer to the field of \nTSF, making Transformer more feasible for time series with long-term dependencies. Log-\nSparse Transformer allows each time step to be consistent with the previous time step and \nis selected using an exponential step. It proposed convolutional self-attention by employing \ncausal convolutions to produce queries and keys, reducing time complexity from O\n(\nL2)\n \nto O\n(\nL(logL)2\n)\n. The prediction accuracy achieved for fine-grained, long-term dependent \ntime series can be improved in cases with limited memory.\nInformer [12] uses the ProbSparse self-attention mechanism, further reducing the com -\nputational complexity of the traditional Transformer model to O (L( logL )). At the same \ntime, inspired by dilated convolution in [ 83] and [84], it also introduced the self-attention \ndistilling operation to remove redundant combinations of value vectors to reduce the total \nspace complexity of the model. In addition, it designed a generative style decoder to pro -\nduce long sequence outputs with only one forward step to avoid accumulation error. The \nInformer architecture was tested on various datasets and performed better than models such \nas Autoregressive Integrated Moving Average (ARIMA) [ 85], Prophet [86], LSTMa [87], \nLSTNet [88], and DeepAR [89].\nThe Autoformer [67] is a simple seasonal trend decomposition architecture with an auto-\ncorrelation mechanism working as an attention module. It achieves O (L( logL )) com -\nputational time complexity. This deep decomposition architecture embeds the sequence \ndecomposition strategy into the encoder-decoder structure as an internal unit of Autoformer.\nIn contrast, TCCT [51] designs a CSP attention module that merges CSPNet with a self-\nattentive mechanism and replaces the typical convolutional layer with an expanded causal \nconvolutional layer, thereby modifying the distillation operation employed by Informer to \nachieve exponential receptive field growth. In addition, the model develops a penetration \n1 3\nPage 9 of 29 80\nL. Su et al.\nTable 1 Transformer-based architectures\nReference Technique Brief information Time complexity Lower com-\nputational \ncomplexity\nHigher in-\ntersequence \ndependency\nWu et al. [53] Transformer Transformer for LTSF on influenza O\n(\nL2)\nLogSparse \nTransformer \n[49]\nLogSparse self-attention + Transformer Reduce time complexity by convolutional self-attention O\n(\nL(logL)2) √ √\nAST [56] GAN + Transformer Reduce error accumulation by sparse attention with GAN √ √\nSpringNet \n[77]\nSpring attention + Transformer Spring attention to repeatable long-term dependency fluctuat-\ning patterns\n√\nLee et al. [78] Partial correlation-based attention + se-\nries-wise multi-resolution\nImprove pair-wise comparisons-based attention disadvantages \nwith partial correlation-based attention\n√\nInformer [12] ProbSparse self-attention + self-atten-\ntion distilling + generative style decoder\nSparse and computationally effective O (L( logL )) √ √\nAutoformer \n[67]\nSequence decomposition + auto-corre-\nlation + Transformer\nAuto-correlation and sequence decomposition architecture O (L( logL )) √ √\nPyraformer \n[70]\nPyramidal attention \nmodule + Transformer\nMulti-resolution representation with pyramidal attention \nmodule\nO(L) √ √\nFEDformer \n[79]\nFourier enhanced + wavelet \nenhanced + Transformer\nReduce time complexity with frequency domain decomposition \nbased on Autoformer architecture\nO(L) √ √\nTCCT [51] CNN + CSPAttention + Transformer Reduce computational cost with CSPAttention √ √\nChu et al. \n[80]\nAutoformer + Informer +\nReformer + MLP\nIncorporate multiple Transformer variants and meta-learner √\nQuatformer \n[81]\nLearning-to-rotate attention\n+ trend normalization + Transformer\nQuaternion architecture with learning-to-rotate attention O (2cL) √ √\nMuformer \n[68]\nMulti-granularity attention\n+ Transformer + Kullback–Leibler\nFor multi-sensory domain feature enhancement and multi-\nheaded attentional expression enhancement\n√\nTriformer \n[13]\nPatch Attention + Transformer Implement the capture of linear complexity and different tem-\nporal dynamic patterns of sequences by a triangular, variable-\nspecific attention architecture\nO(L) √ √\nConformer \n[82]\nFourier transform + sliding \nwindow + Transformer\nExtract correlation features of multivariate variables by fast \nFourier transform, and improve the operational efficiency of \nlong-period forecasting with a sliding window approach\nO(L) √ √\n1 3\n80 Page 10 of 29\nA systematic review for transformer-based long-term series forecasting\nmechanism for stacking self-attentive blocks to obtain finer information at negligible addi -\ntional computational costs.\nPyraformer [70] is a novel model based on hierarchical pyramidal attention. By letting \nthe maximum length of the signal traversal path be a constant concerning the sequence \nlength L, it can achieve theoretical O open paren L close paren complexity. Pyraformer \nconducts both intra-scale and inter-scale attentions, which capture temporal dependencies \nin an individual resolution and build a multi-resolution representation of the original series, \nrespectively. Similarly, Triformer [ 13] proposed a triangular, variable-specific attention \narchitecture, which achieves linear complexity through patch attention while proposing a \nlightweight approach to enable variable-specific model parameters.\nFEDformer [ 79] achieves O(L) linear computational complexity by designing two \nattention modules that process the attention operation in the frequency domain with the \nFourier transform [90] and wavelet transform [91], respectively. Instead of applying Trans-\nformer to the time domain, it applies it to the frequency domain, which helps it better expose \npotential periodic information in the input data.\nThe Conformer [82] model uses the fast Fourier transform to extract correlation features \nof multivariate variables. It employs a sliding window approach to improve the operational \nefficiency of long-period forecasting, sacrificing global information extraction and complex \nsequence modeling capabilities. Thus, the time complexity is reduced to O open paren L \nclose paren.\nTo address the problems of long-term dependency, Lin et al. [77] established SpringNet \nfor solar prediction. They proposed a DTW attention layer to capture the local correlations \nof time-series data, which helps capture repeatable fluctuation patterns and provide accurate \npredictions. For the same purpose, Chu et al. [ 80] combined Autoformer, Informer, and \nReformer to propose a prediction model based on stacking ensemble learning.\nChen et al. [81] proposed a Quatformer framework in which learning-to-rotate attention \nintroduces learnable period and phase information to describe complex periodic patterns, \ntrend normalization to model normalization of the sequence representation in the hidden \nlayer, and decoupling of the LRA by using the global memory, to efficiently fit multi-peri -\nodic complex patterns in the LTSF while achieving linear complexity without loss of predic-\ntion accuracy.\nTo alleviate the problem of redundant information input in LTSF, the Muformer proposed \nby Zeng et al. [68] enhances the features by inputting the multi-perceptual domain process-\ning mechanism, while the multi-cornered attention head mechanism and the attention head \npruning mechanism enhance the expression of multi-head attention. Each of these efforts \ntakes a different perspective on optimizing the parametric part of the model, but a general \narchitecture and component that can reduce the number of required model parameters has \nnot yet emerged.\nIn addition to the previously mentioned Transformer-based architectures, other architec-\ntural modifications have emerged in recent years. For example, the Bidirectional Encoder \nRepresentations from Transformers (BERT) [ 92] model is built by stacking Transformer \nencoder modules and introducing a new training scheme. Pre-training the encoder modules \nis task-independent, and decoder modules can be added later and fine-tuned to the task. \nThis scheme allows BERT models to be trained on large amounts of unlabeled data. BERT \narchitecture has inspired many new Transformer models for time-series data [ 1, 57, 60]. \nHowever, compared to NLP tasks, time-series data include various data types [ 1, 12, 93]. \n1 3\nPage 11 of 29 80\nL. Su et al.\nThus, the pre-training process will have to be different for each task. This task-dependent \npre-training contrasts with the NLP tasks, which can start with the same pre-trained models, \nassuming all tasks are based on the same language semantics and structure.\nGenerative adversarial networks (GANs) consist of the generator and the discriminator, \nlearning adversarially from each other. The generator-discriminator learning principle has \nbeen applied to the time-series forecasting task [56]. The authors use a generative adversar-\nial encoder-decoder framework to train a sparse Transformer model for time-series forecast-\ning, solving the problem of being unable to predict long series due to error accumulation. \nThe adversarial training process improves the model’s robustness and generalization abil -\nity by directly shaping the output distribution of the network to avoid error accumulation \nthrough one-step-ahead inference.\nTranAD [94] applied GAN-style adversarial training with two Transformer encoders and \ntwo Transformer decoders to gain stability. As a simple Transformer-based network tends to \nmiss slight deviations of anomaly, an adversarial training procedure can amplify reconstruc-\ntion errors.\nTFT [54] designs a multi-horizon model with static covariate encoders, a gating feature \nselection module, and a temporal self-attention decoder. It encodes and selects valuable \ninformation from various covariates information to perform forecasting. It also preserves \ninterpretability by incorporating global and temporal dependencies and events. SSDNet \n[95] combines the Transformer with state space models (SSM), which use the Transformer \npart to learn the temporal pattern and estimate the SSM parameters; the SSM parts perform \nthe seasonal-trend decomposition to maintain the interpretable ability. While MT-RV AE \n[96] combines the Transformer with Variational AutoEncoder (V AE), it focuses on data with \nfew dimensions or sparse relationships. A multi-scale Transformer is designed to extract dif-\nferent levels of global time-series information. AnomalyTrans [ 60] combines Transformer \nand Gaussian prior association to make rare anomalies more distinguishable. Prior associa-\ntion and series association are modeled simultaneously. The minimax strategy optimizes \nthe anomaly model to constrain the prior and series associations for more distinguishable \nassociation discrepancies.\nGTA [3] contains a graph convolution structure to model the influence propagation pro -\ncess. Replacing vanilla multi-head attention with a multi-branch attention mechanism com-\nbines global-learned attention, multi-head attention, and neighborhood convolution. GTN \n[62] applies a two-tower Transformer, with each tower working on time-step-wise atten -\ntion and channel-wise attention, respectively. A learnable weighted concatenation is used to \nmerge the features of the two towers. Aliformer [57] makes the time-series sales forecasting \nusing knowledge-guided attention with a branch to revise and denoise the attention map.\nIn addition, some researchers have made corresponding network improvements for spe -\ncific applications. First, in the transportation application, spatiotemporal graph Transformer \n[97] proposes an attention-based graph convolution mechanism for learning a more com -\nplex temporal-spatial attention pattern applying to pedestrian trajectory prediction. Traffic \nTransformer [55] designs an encoder-decoder structure using a self-attention module to cap-\nture the temporal-temporal dependencies and a graph neural network (GNN) module to cap-\nture the spatial dependencies. Spatial-temporal Transformer networks introduced a temporal \nTransformer block to capture the temporal dependencies and a spatial Transformer block to \nassist a graph convolution network to capture more spatial-spatial dependencies [98].\n1 3\n80 Page 12 of 29\nA systematic review for transformer-based long-term series forecasting\nThere are also applications for event prediction. Event forecasting or prediction aims to \npredict the times and marks of future events given the history of past events, which is often \nmodeled by temporal point processes (TPP) [ 6]. Self-attentive Hawkes process (SAHP) \n[7] and Transformer Hawkes process (THP) [ 8] adopt Transformer encoder architecture to \nsummarize the influence of historical events and compute the intensity function for event \nprediction. They modify the positional encoding by translating time intervals into sinusoidal \nfunctions to utilize interval between events. Later, a more flexible model named attentive \nneural datalog through time (ANDTT) [99] was proposed to extend SAHP/THP schemes by \nembedding all possible events and times with attention.\n4 Transformer effectiveness for L TSF\nIs Transformer effective in the time series forecasting domain? The response we provide \nis affirmative. Since the publication of Zeng’s scholarly article, “Are Transformers effec -\ntive for time series forecasting?”[ 100], the feasibility of utilizing Transformer models for \ntime series forecasting has emerged as a significant subject of scholarly discourse. This is \nparticularly noteworthy as a straightforward model emerged victorious over a consider -\nably intricate Transformer model, thus prompting a substantial academic discourse. Zeng \nclaimed that the Transformer-based models are not effective in time series forecasting. They \ncompare the Transformer-based models with a simple linear model, DLinear, which uses \nthe decomposition layer structure in Autoformer and which DLinear claims outperforms the \nTransformer-based models. A Transformer with different positional and temporal embed -\ndings retains very limited temporal relationships. It is prone to overfitting on noisy data, \nwhereas a linear model can be modeled in a natural order and with fewer parameters can \navoid overfitting. However, Nie [ 101] presents a novel solution to tackle the loss of tem -\nporal information induced by the self-attention mechanism. This approach is rooted in the \nTransformer time-series prediction and involves transforming the time-series data into a \npatch format akin to that of Vision Transformer. This conversion preserves the localization \nof the time series, with each patch serving as the smallest unit for Attention computation. \nThe findings in Table 2 demonstrate that research focused on Transformer-based time-series \nprediction underscores the significance of integrating temporal information to improve the \nmodel’s prediction performance.\nA straightforward linear model may have its advantages in specific circumstances; how-\never, it may need to be more capable of effectively handling extensive time series informa-\ntion on the same level as a more intricate model, such as the Transformer. In summary, the \nTransformer model still needs to be updated in time series forecasting. Nonetheless, having \nabundant training data to unlock its immense potential is crucial. Unfortunately, there is \ncurrently a scarcity of publicly available datasets that are sufficiently large for time series \nforecasting. Most existing pre-trained time-series models use public datasets like Traffic \nand Electricity. Despite these benchmark datasets serving as the foundation for developing \ntime series forecasting, their limited size and lack of generalizability pose significant chal -\nlenges for large-scale pre-training. Thus, in the context of time-series prediction, the most \npressing matter is the development of expansive and highly generalized datasets (similar to \nImageNet in computer vision). This crucial step will undoubtedly propel the advancement \nof time-series analysis and training models while enhancing the capacity of training mod -\n1 3\nPage 13 of 29 80\nL. Su et al.\nTable 2 Multivariate long-term forecasting results on electricity dataset\nModels PatchTST/64 PatchTST/42 DLinear FEDformer Autoformer Informer\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nprediction length 96 0.129 0.222 0.130 0.222 0.140 0.237 0.186 0.302 0.196 0.313 0.304 0.393\n192 0.147 0.240 0.148 0.240 0.153 0.249 0.197 0.311 0.211 0.324 0.327 0.417\n336 0.163 0.259 0.167 0.261 0.169 0.267 0.213 0.328 0.214 0.327 0.333 0.422\n720 0.197 0.290 0.202 0.291 0.203 0.301 0.233 0.344 0.236 0.342 0.351 0.427\n1 3\n80 Page 14 of 29\nA systematic review for transformer-based long-term series forecasting\nels in time-series prediction. Additionally, this development underscores the Transformer \nmodel’s effectiveness in successfully capturing long-term dependencies within a sequence \nwhile maintaining superior computational efficiency and a more comprehensive feature rep-\nresentation capability.\nOn the other hand, the Transformer’s effectiveness is reflected in Large Language Mod-\nels (LLMs). LLMs are powerful Transformer-based models, and numerous previous studies \nhave shown that Transformer-based models are capable of learning potentially complex \nrelationships among textual sequences [102, 103]. It is reasonable to expect LLMs to have \nthe potential to understand complex dependencies among numeric time series augmented \nby temporal textual sequences.\nThe current endeavor for time series LLMs encompasses two primary strategies. One \napproach involves creating and preliminary training a fundamental, comprehensive model \nspecifically tailored for time series. This model can be subsequently fine-tuned to cater \nto various downstream tasks. This path represents the most rudimentary solution, draw -\ning upon a substantial volume of data and imbuing the model with time-series-related \nknowledge through pre-training. The second strategy involves fine-tuning based on the \nLLM framework, wherein corresponding mechanisms are devised to adapt the time series, \nenabling application to existing language models. Consequently, this facilitates processing \ndiverse time-series tasks using the pre-existing language models. This path poses challenges \nand necessitates transcending the original language model.\n5 Public datasets and evaluation metrics\nIn this section, we summarize some typical applications and relevant public LTSF datasets. \nWe also discuss the prediction evaluation metrics in LTSF.\n5.1 Common applications and public datasets\n5.1.1 Finance\nLTSF is commonly used in finance to predict economic cycles [104], fiscal cycles, and long-\nterm stock trends [ 105]. LTSF can predict future trends and stock price fluctuations in the \nstock market, helping investors develop more accurate investment strategies. In financial \nplanning, LTSF can predict future economic conditions, such as income, expenses, and \nprofitability, to help individuals or businesses better plan their financial goals and capital \noperations [106]. In addition, LTSF can predict a borrower’s repayment ability and credit \nrisk [107] or predict future interest rate trends to help financial institutions conduct loan risk \nassessments for better monetary and interest rate policies. We summarized the open-source \nLTSF datasets in the finance field in recent years in Table 3.\n5.1.2 Energy\nIn the energy field, LTSF is often used to assist in developing long-term resource planning \nstrategies [118]. It can help companies and governments forecast future energy demand to \nbetter plan energy production and supply. It can also help power companies predict future \n1 3\nPage 15 of 29 80\nL. Su et al.\nTable 3 Finance LTSF dataset\nDataset Reference Data information Min-granularity\nGold prices [108] Daily gold prices from 2014.1 to 2018.4, including minimum, mean, maximum, median, standard deviation, skewness, and \nkurtosis\nhttp://finance.yahoo.com\n1 day\nGEFCom2014\nElectricity \nPrice\n[109] Dataset consists of electricity load forecasting, electricity price forecasting, wind, and solar power generation\nhttps://www.dropbox.com/s/\npqenrr2mcvl0hk9/GEFCom2014.zip? dl = 0\n1 h\nExchange-rate [67, \n110–112]\nDaily exchange rates from Australia, the United Kingdom, Canada, Switzerland, China, Japan, New Zealand, and Singapore \nbetween 1990 and 2016\nhttps://git hub.com/lai guokun/mult ivariate -time-series-data\n1 day\nS&P 500 [113] Daily S&P 500 index from 2001.1 to 2017.5\nhttp://finance.yahoo.com\n1 day\nShanghai \ncomposite\n[113] Daily SSE indices from 2005.1 to 2017.6\nhttp://finance.yahoo.com\n1 day\nS&P 500 \nstocks\n[114] 505 common stocks traded on the American stock exchange, recording historical daily stock prices for all companies cur-\nrently included in the S&P 500 index from 2013.2 to 2018.2\nhttps://www.kaggle.com/camnugent/sandp500\n1 day\nCRSP’s stocks [115] Data on individual stock returns and prices, S&P 500 index returns, industry categories, number of shares outstanding, ticker \nsymbols, exchange codes, and trading volume\n h t t p :  / / m b a .  t u c k .  d a r t  m o u t h . e d u / p a g e s / f a c u l t y / k e n . f r e n c h / d a t a _ l i b r a r y . h t m l      \n1 day\nGas station \nrevenue\n[73] Daily revenue of five gas stations from 2015.12 to 2018.12\nhttps://git hub.com/big huang624/DS ANet/tre e/master/data\n1 day\nFinance Japan [116] Dataset collected by the Ministry of Finance of Japan that records general partnerships, limited partnerships, limited liability \ncompanies, and joint stock companies from 2003.1 to 2016.12  h t t p s  : / / w w w  . m o f .  g o . j  p / e n g l i s h / p r i / r e f e r e n c e / s s c / o u t l i n e . h t m      \n4 months\nStock opening \nprices\n[117] Daily opening prices for 50 stocks in 10 sectors in Financial Yahoo between 2007 and 2016\n h t t p s  : / / g i t  h u b . c  o m / z  3 3 1 5 6 5 3 6 0 / S t a t e - F r e q u e n c y - M e m o r y - s t o c k - p r e d i c t i o n      \n1 day\n1 3\n80 Page 16 of 29\nA systematic review for transformer-based long-term series forecasting\npower generation, ensuring a sufficient and stable power supply [ 119]. In addition, LTSF \ncan help governments and enterprises to develop energy policy planning or manage the \nenergy supply chain [120]. These applications can help enterprises and governments better \nplan, manage, reduce risks, improve efficiency, and realize sustainable development. We \nsummarized the energy field’s open-source datasets in recent years in Table 4.\n5.1.3 Transportation\nIn urban transportation, LTSF can help urban traffic management predict future traffic flow \n[123] for better traffic planning and management. It can also be used to predict future traffic \ncongestion [124], future traffic accident risks, and traffic safety issues [125] for better traffic \nsafety management and accident prevention. We summarized the open-source datasets in \nthe transportation field in recent years in Table 5.\n5.1.4 Meteorology and medicine\nThe application of LTSF in meteorology mainly focuses on predicting long-term climate \ntrends. For example, LTSF can be used to predict long-term climate change [ 133], provid-\ning a scientific basis for national decision-making in response to climate change. It can \nalso issue early warnings for natural climate disasters [134] to mitigate potential hazards to \nhuman lives and properties. In addition, LTSF can predict information such as sea surface \ntemperature and marine meteorology for the future [ 135], providing decision support for \nindustries such as fisheries and marine transportation. We summarized the open-source data-\nsets in the meteorology and medicine fields in recent years in Tables 6 and 7, respectively.\nIn the medical field, LTSF can be applied to various stages of drug development. For \nexample, predicting a drug’s toxicity, pharmacokinetics, pharmacodynamics, and other \nparameters helps researchers optimize the drug design and screening process [137]. In addi-\ntion, LTSF can predict medical needs over a certain period [138]. These predictions can be \nused to allocate and plan medical resources rationally.\n5.2 Evaluation metrics\nIn this section, we discuss prediction performance evaluation metrics in the field of TSF. \nAccording to [141], the prediction accuracy metrics can be divided into three groups: scale-\ndependent, scale-independent, and scaled error metrics, based on whether the evaluation \nmetrics are affected by the data scale and how the data scale effects are eliminated.\nLet Yt denote the observation at time t (t = 1,…, n) and Ft denote the forecast of Yt. \nThen the forecast error is defined as et = Yt − Ft.\n5.2.1 Scale-dependent measures\nScale-dependent measures are the most widely used evaluation metrics in forecasting, \nwhose data scales depend on the data size of the original data. This type of metric is compu-\ntationally simple. These are useful when comparing different methods applied to the same \ndatasets but should not be used, for example, when comparing across datasets with different \nscales.\n1 3\nPage 17 of 29 80\nL. Su et al.\nThe most commonly used scale-dependent measures are based on the absolute error or \nsquared errors:\n Mean Square Error(MSE)= mean\n(\ne2\nt\n)\n (7)\n Root Mean Square Error(RMSE)=\n√\nMSE (8)\n Mean Absolute Error(MAE)= mean (|et|) (9)\n Median Absolute Error(MdAE)= median (|et|) (10)\nHistorically, the RMSE and MSE have been popular because of their theoretical relevance \nin statistical modeling. The RMSE is effective for its simplicity and close relationship with \nstatistical modeling. It can give the same value as the forecast error variance for unbiased \nforecasting. However, they are more sensitive to outliers than MAE or MdAE [ 142]. The \nMAE better reflects the actual error situation than the RMSE.\nTable 4 Energy LTSF dataset\nDataset Reference Data information Min-granularity\nPower \nconsumption\n[117] The electricity consumption of a household, including \nvoltage, electricity consumption, and other characteris-\ntics from 2006.12 to 2010.11\n h t t p s  : / / a r c  h i v e .  i c s .  u c i . e d u / m l / d a t a s e t s / I n d i v i d u a l + h o u s e h \no l d + e l e c t r i c + p o w e r + c o n s u m p t i o n      \n1 min\nSolar energy [49, 56, \n110, 111, \n121]\nThe highest solar power production from 137 photovol-\ntaic plants in Alabama in 2006\nhttps://www .nrel.gov/g rid/solar-p ower-dat a.html\n5 min\nelectricity [56, 67, \n110, 111, \n121]\nThe electricity consumption of 321 customers between \n2011 and 2014\n h t t p s  : / / a r c  h i v e .  i c s .  u c i . e d u / m l / d a t a s e t s / E l e c t r i c i t y L o a d D i \na g r a m s 2 0 1 1 2 0 1 4      \n15 min\nwind [49, 56] Hourly estimates of energy potential as a percentage of \nthe maximum output of power plants for a European \nregion between 1986 and 2015\n h t t p s  : / / w w w  . k a g g  l e . c  o m / s o h i e r / 3 0 - y e a r s - o f e u r o p e a n - w i \nn d - g e n e r a t i o n      \n1 h\nETT [12, 67] The load and oil temperature of power transformers from \n2016.7 to 2018.7\nhttps://github.com/zhouhaoyi/ETDataset\n15 min\nsanyo [77] Daily solar power generation data from two photovoltaic \nplants in Alice Springs, Northern Territory, and Australia \nfrom 2011.1 to 2017.1  h t t p :  / / d k a s  o l a r c  e n t r  e . c o m . a u / s o u r c \ne / a l i c e s p r i n g s / d k a - m 4 - b - p h a s e      \n1 day\nhanergy [77] Daily solar power generation data from two photovoltaic \nplants in Alice Springs, Northern Territory, and Australia \nfrom 2011.1 to 2016.12  h t t p s  : / / d k a  s o l a r  c e n t  r e . c o m . a u / s o \nu r c e / a l i c e s p r i n g s / d k a - m 1 6 - b - p h a s e      \n1 day\npower grid data [122] Grid data of State Grid Shanghai Municipal Electric \nPower Company from 2014.1 to 2015.2\n1 day\n1 3\n80 Page 18 of 29\nA systematic review for transformer-based long-term series forecasting\nTable 5 Transportation LTSF dataset\nDataset Reference Data information Min-granularity\nParis metro \nline\n[25] The passenger flow on Paris metro lines 3 and 13 between 2009 and 2010\nhttp://www. neural-fore casting-com petition .com/\n1 h\nPeMS03, \nPeMS04, \nPeMS07, \nPeMS08,\n[56, 67, \n110, 111, \n121, \n126–128]\ntraffic flow data\nhttp://pems.dot.ca.gov/\n30s\nBirmingham\nParking\n[114] The parking lot ID, parking lot capacity, parking lot occupancy, and update time for 30 parking lots operated by Birmingham \nNational Car Park from 2016.10 to 2016.12\nhttp://arch ive.ics.uci .edu/ml/dat asets/pa rking+birmingham\n30 min\nMETR-LA [110, 126] Traffic information collected by loop detectors on Los Angeles County freeways from 2012.3 to 2012.6\n h t t p s  : / / d r i  v e . g o  o g l e  . c o m / d r i v e / f o l d e r s / 1 0 F O T a 6 H X P q X 8 P f 5 W R o R w c F n W 9 B r N Z E I X      \n5 min\nPEMS-BAY [110, 126] Traffic speed readings from 325 sensors collected by PeMS, the California Transit Agency Performance Measurement System \nfrom 2017.1 to 2017.5\n h t t p s  : / / d r i  v e . g o  o g l e  . c o m / d r i v e / f o l d e r s / 1 0 F O T a 6 H X P q X 8 P f 5 W R o R w c F n W 9 B r N Z E I X      \n30s\nSPMD [129] The driving records of approximately 3,000 drivers in Ann Arbor, Michigan, from 2015.5 to 2015.10\nhttps://github.com/ElmiSay/DeepFEC\n1 h\nVED [129] The fuel and energy consumption of various personal vehicles operating under different realistic driving conditions in Michi-\ngan, US, from 2017.11 to 2018.11\nhttps://github.com/ElmiSay/DeepFEC\n1 h\nEngland [127] National average speeds and traffic volumes derived from UK freeway traffic data from 2014.1 to 2014.6\nhttp://tris .highwaysen gland.co.uk /detail/ trafficflowdata\n15 min\nTaxiBJ+ [130] The distribution and trajectory of more than 3,000 cabs in Beijing\n h t t p s  : / / w w w  . m i c r  o s o f  t . c o m  / e n u s /  r e s e a  r c h /  p u b l i c a t i o n / d e e p - s p a t i o - t e m p o r a l - r e s i d u a l n e t w o r k s - f o r - c i t y w i d e - c r o w d - fl  o w s - p r e d i \nc t i o n      \n30 min\nBikeNYC [130] Trajectory data taken from the NYC Bike system in 2014, from April 1 to Sept. 30. Trip data includes trip duration, starting \nand ending station IDs, and start and end times\n h t t p s  : / / w w w  . m i c r  o s o f  t . c o m  / e n u s /  r e s e a  r c h /  p u b l i c a t i o n / d e e p - s p a t i o - t e m p o r a l - r e s i d u a l n e t w o r k s - f o r - c i t y w i d e - c r o w d - fl  o w s - p r e d i \nc t i o n      \n1 h\nHappyValley [131] The hourly population density of popular theme parks in Beijing from 2018.1 to 2018.10\nheat.qq.com\n1 h\nNYC Taxi [132] Details of every cab trip in New York City from 2009.1 to 2016.6 1 h\n1 3\nPage 19 of 29 80\nL. Su et al.\n5.2.2 Scale-independent measures\nScale-independent measures are evaluation metrics not affected by the size of the original \ndata. They can be divided more specifically into three subcategories: measures based on \npercentage errors, measures based on relative errors, and relative measures.\nThe percentage error is pt = 100et/Yt. The most commonly used measures are:\n Mean Absolute Percentage Error(MAPE)= mean (|pt|)  (11)\n Median Absolute Percentage Error(MdAPE)= median (|pt|) (12)\n Root Mean Square Percentage Error(RMSPE)=\n√\nmean (p2\nt ) (13)\n Root Median Square Percentage Error(RMdSPE)=\n√\nmedian (p2\nt ) (14)\nPercentage errors have the advantage of being scale-independent and so are frequently used \nto compare forecast performance across different datasets. However, these measures have \nthe disadvantage of being infinite or undefined if Yt =0  for any t in the period of inter -\nest and have an extremely skewed distribution when any value of Yt is close to zero. The \nMAPE and MdAPE also have the disadvantage of putting a heavier penalty on positive \nerrors than negative errors. Measures based on percentage errors are often highly skewed, \nand, therefore, transformations (such as logarithms) can make them more stable [143].\nAn alternative scaling method is dividing each error by the error obtained using another \nstandard forecasting method. Let rt = et/e∗\nt  denote the relative error, where e∗\nt  is the \nforecast error obtained from the benchmark method. Usually, the benchmark method is the \nrandom walk where Ft is equal to the last observation.\n Mean Relative Absolute Error(MRAE)= mean (|rt|) (15)\n Median Relative Absolute Error(MdRAE)= median (|rt|) (16)\n Geometric Mean Relative Absolute Error(GMRAE)= gmean (|rt|) (17)\nA serious deficiency of relative error measures is that e∗\nt  can be small. In fact, rt has infinite \nvariance because e∗\nt  has a positive probability density at 0. Using “winsorizing” can trim \nextreme values, which will avoid the difficulties associated with small values of e∗\nt  [144], \nbut adds some complexity to the calculation and a level of arbitrariness as the amount of \ntrimming must be specified.\nRather than use relative errors, one can use relative measures. For example, let MAE b \ndenote the MAE from the benchmark method. Then, a relative MAE is given by\n relMAE = MAE/MAEb. (18)\nAn advantage of these methods is their interpretability. However, they require several fore-\ncasts on the same series to compute MAE (or MSE).\n1 3\n80 Page 20 of 29\nA systematic review for transformer-based long-term series forecasting\n5.2.3 Scaled errors\nScaled errors were first proposed in [141] and can be used to eliminate the effect of data size \nby comparing the prediction results obtained with the underlying method (usually the native \nmethod). The following scaled error is commonly used:\n \nqt = et\n1\nn−1\n∑ n\ni=2 |Yi − Yi−1|. (19)\nTherefore, The Mean Absolute Scaled Error is simply MASE = mean (|qt|).\nThe denominator can be considered as the average error of the native predictions made \none step ahead in the future. If MASE > 1, the experimental method under evaluation is \nTable 6 Meteorology LTSF dataset\nDataset Reference Data information Min-granularity\nBeijing \nPM2.5\n[136] Hourly PM2.5 data and associated meteorological data for \nBeijing from 2010.1 to 2014.12\nhttps://archive.ics.uci.edu/ml/datasets.html\n1 h\nHangzhou \ntemperature\n[113] Daily average temperature of Hangzhou from 2011.1 to \n2017.1\nhttp://data.cma.cn/data/\n1 day\nWTH [67] Weather conditions throughout 2020\nhttps://www.bgc-jena.mpg.de/wetter/\n10 min\nUSHCN [34] Continuous daily meteorological records from 1887 to \n2009\nhttps://www.ncdc.noaa.gov/ushcn/introduction.\n1 day\nKDD-CUP [34] PM2.5 measurements from 35 monitoring stations in Bei-\njing from 2017.1 to 2017.12\nhttps://www.kdd.org/kdd2018/kdd-cup.\n1 h\nUS [128] Weather datasets from 2012 to 2017 from 36 weather sta-\ntions in the US\n h t t p s  : / / w w w  . k a g g  l e . c  o m / s e l fi  s h g e n e / h i s t o r i c a l - h o u r l y - w e \na t h e r - d a t a     .  \n1 h\nTable 7 Medicine LTSF dataset\nDataset Reference Data information Min-granularity\nILI [53, 67] Data on patients with influenza-like illness recorded weekly \nby the Centers for Disease Control and Prevention from \n2002 to 2021\nhttps://gis .cdc.gov/gr asp/fluview /fluport aldashboard.html.\n1 week\nCOVID-19 [139] Daily data on confirmed and recovered cases collected from \n2020.1 to 2020.6 in Italy, Spain, France, China, US, and \nAustralia\nhttps://github.com/CSSEGISandData/COVID-19\n1 day\n2020 \nOhioT1DM\n[140] Eight weeks of continuous glucose monitoring, insulin, \nphysiological sensor, and self-reported life event data for \neach of 12 patients with type 1 diabetes in 2020\nhttp:   //smartheal th .cs. ohi o.edu/Oh ioT 1DM- dataset.html\n5 min\nMIMIC-III [34] A public clinical dataset with over 58,000 admission \nrecords from 2001 to 2012\nhttp://mimic.physionet.org\n1 h\n1 3\nPage 21 of 29 80\nL. Su et al.\nworse than the native prediction, and vice versa. Similar to MASE, MASE is calculated \nusing the mean, making it more susceptible to outliers, while MdASE computed using the \nmedian has stronger robustness and validity. However, such metrics can only reflect the \nresults of comparison with the primary method and cannot visualize the error of the predic-\ntion results.\n6 Training strategies\nRecent Transformer variants introduce various time-series features into the models for \nimprovements [67, 70]. In this section, we summarize several training strategies of existing \nTransformer-based models for LTSF.\n6.1 Preprocessing and embedding\nIn the preprocessing stage, normalization with zero mean is often applied in time-series \ntasks. Moreover, seasonal-trend decomposition is a standard method to make raw data more \npredictable [145, 146], first proposed by Autoformer [ 67]. It also uses a moving average \nkernel on the input sequence to extract the trend-cyclical component of the time series. The \nseasonal component differs between the original sequence and the trend component. FED -\nformer [79] further proposed a mixture of experts’ strategies to mix the trend components \nextracted by moving average kernels with various kernel sizes.\nThe self-attentive layer in the Transformer architecture cannot preserve the positional \ninformation of the time series. However, local location information or the ordering of the \ntime series is essential. Furthermore, global time information is also informative, such as \nhierarchical timestamps (weeks, months, years) and agnostic timestamps (holidays and \nevents) [12]. To enhance the temporal context of the time-series input, a practical design \nis to inject multiple embeddings into the input sequence, such as fixed positional coding \nand learnable temporal embeddings. Additionally, the introduction of temporal embeddings \naccompanied by temporal convolutional layers [ 49] or learnable timestamps [67] has been \nproposed as an effective means further to enhance the temporal context of the input data.\n6.2 Iterated multi-step and direct multi-step\nThe time series forecasting task is to predict the values at the T future time steps. When T > 1, \niterated multi-step (IMS) forecasting [ 147] learns a single-step forecaster and iteratively \napplies it to obtain multi-step predictions. Alternatively, direct multi-step (DMS) forecast -\ning [148] optimizes the multi-step forecasting objective simultaneously. The variance of \nthe IMS predictions is smaller due to the autoregressive estimation procedure compared \nto DMS forecasting but is inevitably subject to the error accumulation effects. Therefore, \nIMS forecasting is more desirable when highly accurate single-step forecasters exist, and \nT is relatively small. In contrast, DMS forecasting produces more accurate forecasts when \nunbiased single step forecast models are challenging to obtain or when T is large.\nApplying the vanilla Transformer model to the LTSF problem has some limitations, \nincluding the quadratic time/memory complexity with the original self-attention scheme \nand error accumulation caused by the autoregressive decoder design. Alternative Trans -\n1 3\n80 Page 22 of 29\nA systematic review for transformer-based long-term series forecasting\nformer variants have been developed to overcome these challenges, each employing distinct \nstrategies. For instance, LogTrans [49] introduces a dedicated decoder for IMS forecasting, \nwhile Informer [ 12] leverages a generative-style decoder. Additionally, Pyraformer [ 70] \nincorporates a fully connected layer that concatenates spatiotemporal axes as its decoder. \nAutoformer [67] adds the two refined decomposition features of the trend-cyclical compo -\nnents and the stacked autocorrelation mechanism of the seasonal component to obtain the \nfinal prediction results. Similarly, FEDformer [ 79] applies a decomposition scheme and \nemploys the proposed frequency attention block in deciphering the final results.\n7 Conclusion\nTransformer architecture has been found to be applicable to solving various time-series \ntasks. The Transformer architecture based on self-attention and positional encoding offers \nbetter or similar performance as RNNs and variants of LSTMs/GRUs. However, it is more \nefficient in computing time and overcomes other shortcomings of RNNs/LSTMs/GRUs.\nIn this paper, we summarized the application of the Transformer on LTSF. First, we have \nprovided a thorough examination of the fundamental structure of the Transformer. Subse -\nquently, we analyzed and summarized the advantages of Transformer on LTSF tasks. Given \nthat the Transformer encounters intricacies and interdependencies when confronting LTSF \ntasks, numerous adaptations have been introduced to the original architectural framework, \nthus equipping Transformers with the capacity to handle LTSF tasks effectively. This archi-\ntectural augmentation, however, brings certain challenges during the training process. To \naddress this, we have incorporated a compendium of best practices that facilitate the practi-\ncal training of Transformers. Additionally, we have collected abundant resources on TSF \nand LTSF, including datasets, application fields, and evaluation metrics.\nIn summary, our comprehensive review examines recent advancements in Transformer-\nbased architecture in LTSF and imparts valuable insights to researchers seeking to improve \ntheir models. The Transformer architecture is renowned for its remarkable modeling capac-\nity and aptitude for capturing long-term dependencies. However, it encounters challenges \nregarding time complexity when applied to LTSF tasks. While efforts to reduce complexity \nmay inadvertently lead to the loss of certain interdependencies between data points, thereby \ncompromising prediction accuracy. Consequently, the amalgamation of various techniques \nwithin a compound model, leveraging the strengths of each, emerges as a promising avenue \nfor future research in Transformer-based LTSF models. This paves the way for innovative \nmodel designs, data processing techniques, and benchmarking approaches to tackle the intri-\ncate LTSF problems. In future research, progressive trending and seasonal decomposition \nmechanisms can be introduced as multiple cycles and trends are hidden and repeated among \nthe data. At the same time, the Transformer-based models have some inherent limitations in \nLSTF, such as time complexity. Transformer-based models may unavoidably lose some of \nthe dependencies between data points when reducing the complexity of self-attention com-\nputations in situations with many data feature variables with complex correlations, resulting \nin reduced prediction accuracy. Therefore, Transformer-based models are not suitable for all \nLSTF tasks. Also, pre-trained Transformer models for different tasks in time series and more \narchitecture-level designs for Transformers can be investigated in depth in the future. Nota-\nbly, researchers have recently explored the integration of Large Language Models (LLMs) \n1 3\nPage 23 of 29 80\nL. Su et al.\nin time series forecasting, wherein LLMs exhibit the capability to generate forecasts while \noffering human-readable explanations for predictions, outperforming traditional statistical \nmodels and machine learning approaches. These encouraging findings present a compelling \nimpetus for further exploration, aiming to enhance the precision, comprehensibility, and \ntransparency of forecasting results.\nAcknowledgements This work was supported by the Project of the Educational Commission of Guangdong \nProvince of China (2022ZDJS113) and the Natural Science Foundation of Top Talent of SZTU (GDRC20221).\nAuthor contributions L.S. and X.Z. wrote the manuscript, conceived of the presented idea and designed the \nmanuscript. R.L., X.W carried out the collections. H.Z. and B.H. supervised the project. All authors approved \nthe manuscript.\nData availability Data sharing is not applicable to this article as no datasets were generated or analyzed dur-\ning the current study.\nDeclarations\nConflict of interest The authors declare that they have no conflict of interest.\nOpen Access   This article is licensed under a Creative Commons Attribution-NonCommercial-\nNoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and \nreproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. \nYou do not have permission under this licence to share adapted material derived from this article or parts of it. \nThe images or other third party material in this article are included in the article’s Creative Commons licence, \nunless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative \nCommons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, \nyou will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit \nhttp://crea tivecommons .org/licens es/by-nc -nd/4.0/.\nReferences\n1. Yuan Y , Lin L (2021) Self-Supervised Pretraining of Transformers for Satellite Image Time Series Clas-\nsification. IEEE J Sel Top Appl Earth Observations Remote Sens 14:474–487.\n2. Zerveas G et al (2021) A Transformer-based Framework for Multivariate Time Series Representation \nLearning. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data \nMining, Association for Computing Machinery, pp 2114–2124\n3. Chen Z et al (2021) Learning graph structures with transformer for multivariate time-series anomaly \ndetection in IoT. IEEE Internet of Things Journal 9(12):9179–9189\n4. Meng H et al (2019) Spacecraft Anomaly Detection via Transformer Reconstruction Error. In: Pro -\nceedings of the International Conference on Aerospace System Science and Engineering 2019, Lecture \nNotes in Electrical Engineering, 622:351–362\n5. Ruff L et al  (2021) A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE \n109(5):756–795\n6. Shchur O et al (2021) Neural Temporal Point Processes: A Review. arXiv preprint  h t t p : / / a r x i v . o r g / a b s / \n2 1 0 4 . 0 3 5 2 8       \n7. Zhang Q et al (2020) Self-attentive Hawkes process. In: International conference on machine learning, \nPMLR, pp 11183–11193\n8. Zuo S et al (2020) Transformer Hawkes Process . In: International conference on machine learning, \nPMLR, pp 11692–11702\n9. Esling P, Agon C (2012) Time-series data mining. ACM-CSUR 45(1):1–34\n10. Lim B, Zohren S (2021). Time-series forecasting with deep learning: a survey. Philos T Roy Soc A \n379(2194):20200209\n1 3\n80 Page 24 of 29\nA systematic review for transformer-based long-term series forecasting\n11. Torres JF et al (2021) Deep Learning for Time Series Forecasting: A Survey. Big Data 9(1):3–21\n12. Zhou H et al (2020) Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecast-\ning. arXiv preprint http://arxiv.org/abs/2012.07436\n13. Cirstea RG et al (2022) Triformer: Triangular, Variable-Specific Attentions for Long Sequence Multi-\nvariate Time Series Forecasting-Full Version. arXiv preprint http://arxiv.org/2204.13767\n14. Liang Y et al (2018) GeoMAN: Multi-level Attention Networks for Geo-sensory Time Series Prediction. \nIn: International Joint Conference on Artificial Intelligence, pp 3428–3434\n15. Gao J et al (2009) Denoising Nonlinear Time Series by Adaptive Filtering and Wavelet Shrinkage: a \ncomparison. IEEE Signal Process Lett 17(3):237–240\n16. Rojo-Alvarez JL et al (2004) Support vector method for robust ARMA system identification. IEEE \nTrans Signal Process 52(1):155–164\n17. Hong T, Fan S (2016) Probabilistic electric load forecasting: a tutorial review. Int J Forecast \n32(3):914–938\n18. Miller C et al (2020) The ASHRAE Great Energy Predictor III competition: Overview and results. Sci \nTechnol Built Environ 26:1427–1447\n19. Shao H, Soong BH (2016) Traffic flow prediction with long short-term memory networks (LSTMs). In: \n2016 IEEE region 10 conference (TENCON), IEEE, pp 2986–2989\n20. Yule GU (1927) On a method of investigating periodicities in distributed Series, with special reference \nto Wolfer’s sunspot numbers. Phil Trans R Soc Lond A 226:267–298\n21. Walker GT (1931) On periodicity in series of related terms. Proc Royal Soc Lond Ser Containing Papers \nMath Phys Character 131(818):518–532\n22. Rojas I et al (2008) Soft-computing techniques and ARMA model for time series prediction. Neurocom-\nputing 71(4–6):519–537\n23. Box GEP, Pierce DA (1970) Distribution of residual in Autoregressive-Integrated moving average Time \nSeries. J Am Stat Assoc 65(332):1509–1526\n24. Marple SL Jr, Carey WM (1998) Digital Spectral Analysis with Applications.  J Acoust Soc Am, \n86(5):2043\n25. Wang Q et al (2020) A deep granular network with adaptive unequal-length granulation strategy for \nlong-term time series forecasting and its industrial applications. Artif Intell Rev 53(7):5353–5381\n26. Farnoosh A et al (2020) Deep Switching Auto-Regressive Factorization:Application to Time Series \nForecasting. arXiv preprint http://arxiv.org/2009.05135\n27. McDonald DJ et al (2012) Nonparametric Risk Bounds for Time-Series Forecasting. J Mach Learn Res \n18(32):1–40\n28. Wen Q et al (2018) RobustSTL: A robust seasonal-trend decomposition algorithm for long time series. \nIn: Proceedings of the AAAI conference on artificial intelligence 33(1):5409–5416\n29. Yang X et al (2017) Long-term forecasting of time series based on linear fuzzy information granules and \nfuzzy inference system. Int J Approximate Reasoning 81:1–27\n30. Cortes C, Vapnik V (1995) Support-Vector Networks. Mach Learn 20(3):273–297\n31. Freund Y (1995) Boosting a weak learning algorithm by Majority. Inf Comput 121(2):256–285\n32. Liu Z et al (2018) A Flexible Forecasting Framework for Hierarchical Time Series with Seasonal Pat-\nterns: A Case Study of Web Traffic. In: The 41st International ACM SIGIR Conference on Research & \nDevelopment in Information Retrieval, pp 889–892\n33. Sun C et al (2021) Te-esn: Time encoding echo state network for prediction based on irregularly sam-\npled time series data. arXiv preprint http://arxiv.org/2105.00412\n34. Wu Y et al (2021) Dynamic gaussian mixture based deep generative model for robust forecasting on \nsparse multivariate time series. In: Proceedings of the AAAI Conference on Artificial Intelligence \n35(1): 651–659\n35. Li L et al (2021) Learning interpretable deep state space model for probabilistic time series forecasting. \narXiv preprint http://arxiv.org/2102.00397\n36. Bińkowski M et al (2018) Autoregressive convolutional neural networks for asynchronous time series. \nIn: International Conference on Machine Learning, PMLR, pp 580–589\n37. Chen Z et al (2021) Time-Aware Multi-Scale RNNs for Time Series Modeling . In: Proceedings of the \nThirtieth International Joint Conference on Artificial Intelligence, International Joint Conferences on \nArtificial Intelligence Organization, pp 2285–2291\n38. Yang L et al (2020) Html: Hierarchical transformer-based multi-task learning for volatility prediction. \nIn: Proceedings of The Web Conference 2020, pp 441–451\n39. Yu R et al (2017) Deep Learning: A Generic Approach for Extreme Condition Traffic Forecasting. In: \nProceedings of the 2017 SIAM international Conference on Data Mining, Society for Industrial and \nApplied Mathematics, pp 777–785\n40. Elman JL (1990) Finding structure in Time. Cogn Sci 14(2):179–211\n41. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8):1735–1780\n1 3\nPage 25 of 29 80\nL. Su et al.\n42. Cho K et al (2020) Learning Phrase Representations using RNN Encoder-Decoder for Statistical \nMachine Translation. arXiv preprint http://arxiv.org/abs/1406.1078\n43. Lipton ZC, Berkowitz J, Elkan C (2015) A Critical Review of Recurrent Neural Networks for Sequence \nLearning. arXiv preprint http://arxiv.org/abs/1506.00019\n44. Chung J et al (2014) Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. \narXiv preprint http://arxiv.org/abs/1412.3555\n45. Chen G (2016) A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation.  arXiv \npreprint http://arxiv.org/abs/1610.02583\n46. Sherstinsky A (2020) Fundamentals of recurrent neural network (RNN) and long short-term memory \n(LSTM) network. Physica D: Nonlinear Phenomena 404:132306\n47. Han K et al (2021). Transformer in transformer. Advances in neural information processing systems \n34:15908–15919\n48. Kitaev N et al (2020) Reformer: The Efficient Transformer. arXiv preprint  h t t p : / / a r x i v . o r g / a b s / 2 0 0 1 . 0 4 \n4 5 1       \n49. Li S et al (2019) Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time \nSeries Forecasting. Advances in neural information processing systems 32\n50. Brown TB et al (2020) Language Models are Few-Shot Learners. arXiv preprint  h t t p : / / a r x i v . o r g / a b s / 2 0 \n0 5 . 1 4 1 6 5       \n51. Shen L, Wang Y (2022) TCCT: tightly-coupled convolutional transformer on time series forecasting. \nNeurocomputing 480:131–145\n52. Chen K et al (2021) NAST: Non-Autoregressive Spatial-Temporal Transformer for Time Series Fore -\ncasting. arXiv preprint http://arxiv.org/abs/2102.05624\n53. Wu N et al (2020) Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence \nCase. arXiv preprint http://arxiv.org/abs/2001.08317\n54. Lim B et al (2021) Temporal Fusion Transformers for interpretable multi-horizon time series forecast -\ning. Int J Forecast, 37(4):1748–1764\n55. Cai L et al (2020). Traffic transformer: Capturing the continuity and periodicity of time series for traffic \nforecasting. Trans GIS, 24(3):736–755\n56. Wu S et al (2020) Adversarial Sparse Transformer for Time Series Forecasting. Adv Neural Inf Process \nSyst 33:17105–17115\n57. Qi X et al (2021) From Known to Unknown: Knowledge-guided Transformer for Time-Series Sales \nForecasting in Alibaba. arXiv preprint http://arxiv.org/abs/2109.08381\n58. Madhusudhanan K et al (2021) Yformer: U-Net Inspired Transformer Architecture for Far Horizon \nTime Series Forecasting. arXiv preprint http://arxiv.org/abs/2110.08255\n59. Tipirneni S, Reddy CK (2021) Self-supervised Transformer for Multivariate Clinical Time-Series with \nMissing Values. arXiv preprint http://arxiv.org/abs/2107.14293\n60. Xu J (2021) Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy. \narXiv preprint http://arxiv.org/abs/2110.02642\n61. Song H et al (2018) Attend and diagnose: Clinical time series analysis using attention models. In: Pro-\nceedings of the AAAI conference on artificial intelligence 32(1)\n62. Liu M et al (2021) Gated Transformer Networks for Multivariate Time Series Classification.  arXiv \npreprint http://arxiv.org/abs/2103.14438\n63. Vaswani A et al (2017) Attention is All you Need. Advances in Neural Information Processing Systems.\n64. Woo G et al (2022) Etsformer: Exponential smoothing transformers for time-series forecasting. arXiv \npreprint http://arxiv.org/abs/2202.01381\n65. Tang B, Matteson DS (2021) Probabilistic transformer for time series analysis. Adv Neural Inf Process \nSyst 34:23592–23608\n66. Cui Y , Xie J, Zheng K (2021) Historical inertia: a neglected but powerful baseline for long sequence \ntime-series forecasting.In: Proceedings of the 30th ACM international conference on information & \nknowledge management, pp 2965–2969\n67. Wu H et al (2021) Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term \nSeries Forecasting. Advances in neural information processing systems 34:22419–22430\n68. Zeng P et al (2022) Muformer: a long sequence time-series forecasting model based on modified multi-\nhead attention. Knowl Based Syst 254:109584\n69. Chang S et al (2017) Dilated Recurrent Neural Networks. Advances in neural information processing \nsystems 30\n70. Liu S et al (2022) Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Mod-\neling and Forecasting. In: The Tenth International Conference on Learning Representations\n71. Song W, Fujimura S (2021) Capturing combination patterns of long- and short-term dependencies in \nmultivariate time series forecasting. Neurocomputing 464:72–82\n1 3\n80 Page 26 of 29\nA systematic review for transformer-based long-term series forecasting\n72. Hu J, Zheng W (2019) Transformation-gated LSTM: efficient capture of short-term mutation dependen-\ncies for multivariate time series prediction tasks . In: 2019 International Joint Conference on Neural \nNetworks (IJCNN), IEEE, pp 1–8\n73. Huang S et al (2019) DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting . \nIn: Proceedings of the 28th ACM international conference on information and knowledge management, \npp 2129–2132\n74. Zhao X et al (2022) Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-\nseries Forecasting. arXiv preprint http://arxiv.org/abs/2207.07827\n75. Wang X et al (2022) Long Time Series Deep forecasting with Multiscale feature extraction and Seq2seq \nattention mechanism. Neural Process Lett 54(4):3443–3466\n76. Liu Y et al (2022) Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting. \nAdvances in Neural Information Processing Systems 35:9881–9893\n77. Lin Y et al (2020) SpringNet: Transformer and Spring DTW for Time Series Forecasting. In: Neural \nInformation Processing: 27th International Conference, Proceedings, Part III 27, pp 616–628\n78. Lee WK (2020) Partial Correlation-Based Attention for Multivariate Time Series Forecasting. In: Pro-\nceedings of the AAAI Conference on Artificial Intelligence, 34(10):13720–13721\n79. Zhou T et al (2022) FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series \nForecasting. In: International conference on machine learning, PMLR, pp 27268–27286\n80. Chu J, Cao J, Chen Y (2022) An Ensemble Deep Learning Model Based on Transformers for Long \nSequence Time-Series Forecasting. In: International Conference on Neural Computing for Advanced \nApplications. pp 273–286\n81. Chen W et al (2022) Learning to Rotate: Quaternion Transformer for Complicated Periodical Time \nSeries Forecasting.  In: Proceedings of the 28th ACM SIGKDD conference on knowledge discovery \nand data mining, pp 146–156\n82. Li Y et al (2023) Towards Long-Term Time-Series Forecasting: Feature, Pattern, and Distribution. In: \nIEEE 39th International Conference on Data Engineering (ICDE), pp 1611–1624\n83. Yu F et al (2017) Dilated Residual Networks. In: Proceedings of the IEEE conference on computer \nvision and pattern recognition, pp 472–480\n84. Gupta A, Rush AM (2017) Dilated Convolutions for Modeling Long-Distance Genomic Dependencies. \narXiv preprint http://arxiv.org/abs/1710.01278\n85. Ariyo AA et al (2014) Stock Price Prediction Using the ARIMA Model. In: 2014 UKSim-AMSS 16th \ninternational conference on computer modelling and simulation, IEEE, pp 106–112\n86. Taylor et al (2018) Forecasting at Scale. Am Stat 72(1):37–45\n87. Bahdanau D et al (2014) Neural machine translation by jointly learning to align and translate. arXiv \npreprint http://arxiv.org/abs/1409.0473\n88. Lai G et al (2018) Modeling long- and short-term temporal patterns with deep neural networks. In: \nThe 41st international ACM SIGIR conference on research & development in information retrieval, pp \n95–104\n89. Flunkert V ,  Salinas D, Gasthaus J (2020) Deepar: probabilistic forecasting with autoregressive recur-\nrent networks. Int J Forecasting, 36(3):1181–1191\n90. Bracewell RN (1983) The Fourier transform and its applications. 2nd ed., 3rd printing.\n91. Farge M (1992) Wavelet transform and their application to turbulence. Annu Rev Fluid Mech, \n24:395–457.\n92. Devlin J (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. \narXiv preprint http://arxiv.org/abs/1810.04805\n93. Bache K, Lichman M (2013) UCI Machine Learning Repository. http://archive.ics.uci.edu/ml.\n94. Tuli S et al (2022) TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time \nSeries Data. arXiv preprint http://arxiv.org/abs/2201.07284\n95. Lin Y , Koprinska I, Rana M (2021) SSDNet: State Space Decomposition Neural Network for Time Series \nForecasting. In: 2021 IEEE International Conference on Data Mining (ICDM), IEEE, pp 370–378\n96. Wang X et al (2022) Variational transformer-based anomaly detection approach for multivariate time \nseries. Measurement, 191:110791\n97. Yu C et al (2020) Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction. \nIn: Computer Vision–ECCV 2020: 16th European Conference, Proceedings, Part XII 16, pp 507–523\n98. Xu M et al (2020) Spatial-Temporal Transformer Networks for Traffic Flow Forecasting. arXiv preprint \nhttp://arxiv.org/abs/2001.02908\n99. Yang C et al (2021) Transformer embeddings of irregularly Spaced events and their participants. arXiv \npreprint http://arxiv.org/abs/2201.00044\n100. Zeng A et al (2023) Are transformers effective for time series forecasting? In: Proceedings of the AAAI \nconference on artificial intelligence 37(9):11121–11128\n1 3\nPage 27 of 29 80\nL. Su et al.\n101. Nie Y et al (2022) A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. arXiv \npreprint http://arxiv.org/abs/2211.14730\n102. Dwivedi VP, Bresson X (2020) A generalization of transformer networks to graphs.  arXiv preprint \nhttp://arxiv.org/abs/2012.09699\n103. Rong Y et al (2020) Self-supervised graph transformer on large-scale molecular data. Adv Neural Inf \nProcess Syst 33:12559–12571\n104. Harvey AC et al (2007) Trends and cycles in economic time series: a bayesian approach. J Econ \n140(2):618–649\n105. Yuan X et al (2020) Integrated Long-Term Stock Selection models based on feature selection and \nmachine learning algorithms for China Stock Market. IEEE Access 8:1–1\n106. GeWenbo et al (2022) Neural network–based financial volatility forecasting: a systematic review. ACM \nComput Surv (CSUR) 55(1):1–30\n107. Yang B et al (2001) An early warning system for loan risk assessment using artificial neural networks. \nKnowl Based Syst 14(5–6):303–306\n108. Livieris IE et al (2020) A CNN–LSTM model for gold price time-series forecasting. Neural Comput \nAppl 32(23):17351–17360\n109. Hong T et al (2016) Probabilistic energy forecasting: global energy forecasting competition 2014 and \nbeyond. Int J Forecast 32(3):896–913\n110. Wu Z et al (2020) Connecting the dots: Multivariate Time Series forecasting with graph neural net -\nworks. In: Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & \ndata mining, pp 753–763\n111. Chang YY et al (2018) A Memory-Network Based Solution for Multivariate Time-Series Forecasting. \narXiv preprint http://arxiv.org/abs/1809.02105\n112. Demeniconi C,  Davidson I (2021) Proceedings of the 2021 SIAM International Conference on Data \nMining (SDM). Society for Industrial and Applied Mathematics\n113. Shen Z et al (2020) A novel time series forecasting model with deep learning. Neurocomputing \n396:302–313\n114. Yang Z et al (2022) Adaptive temporal-frequency network for time-series forecasting. IEEE Trans \nKnowl Data Eng 34(4):1576–1587\n115. Hou X et al (2020) An enriched time-series forecasting Framework for Long-Short Portfolio Strategy. \nIEEE Access 8:31992–32002\n116. Yoshimi S et al (2020) Forecasting Corporate Financial Time Series using Multi-phase Attention \nRecurrent Neural Networks. In: EDBT/ICDT Workshops\n117. Zhao Y et al (2018) Forecasting Wavelet Transformed Time Series with Attentive Neural Networks. In: \n2018 IEEE international conference on data mining (ICDM), IEEE, pp 1452–1457\n118. Fu CW, Nguyen TT (2003) Models for Long-Term Energy Forecasting. In: 2003 IEEE Power Engineer-\ning Society General Meeting, IEEE, 1:235–239\n119. Khuntia SR et al (2016) Forecasting the load of electrical power systems in mid- and long-term hori -\nzons: a review. IET Gener Transm Dis 10(16):3971–3977\n120. Hecke TV (2012) Power study of anova versus Kruskal-Wallis test. J Stat Manage Syst 15(2–3):241–247\n121. Yoo J, Kang U (2021) Attention-Based Autoregression for Accurate and Efficient Multivariate Time \nSeries Forecasting.  In: Proceedings of the 2021 SIAM International Conference on Data Mining \n(SDM), Society for Industrial and Applied Mathematics, pp 531–539\n122. Pang Y et al (2018) Hierarchical Electricity Time Series Forecasting for Integrating Consumption Pat-\nterns Analysis and Aggregation Consistency . In: Twenty-Seventh International Joint Conference on \nArtificial Intelligence, pp 3506–3512\n123. Bogaerts T et al (2020) A graph CNN-LSTM neural network for short and long-term traffic forecasting \nbased on trajectory data. Transp Res Part C: Emerg Technol 112:62–77\n124. Qu L et al (2019) Daily long-term traffic flow forecasting based on a deep neural network. Expert Syst \nAppl 121:304–312\n125. Chen Q et al (2016) Learning deep representation from big and heterogeneous data for traffic accident \ninference. In: Proceedings of the AAAI Conference on Artificial Intelligence  30(1)\n126. Pan Z et al (2021) AutoSTG: Neural Architecture Search for Predictions of Spatio-Temporal Graph. In: \nProceedings of the Web Conference, pp 1846–1855\n127. Han L et al (2021) Dynamic and Multi-faceted Spatio-temporal Deep Learning for Traffic Speed Fore-\ncasting. In: Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining, \npp 547–555\n128. Cirstea RG et al (2021) EnhanceNet: Plugin Neural Networks for Enhancing Correlated Time Series \nForecasting. In: 37th International Conference on Data Engineering (ICDE), IEEE, pp 1739–1750\n129. Elmi S, Tan K-L (2021) DeepFEC: Energy Consumption Prediction under Real-World Driving Condi-\ntions for Smart Cities.  In: Proceedings of the Web Conference 2021. pp 1880–1890\n1 3\n80 Page 28 of 29\nA systematic review for transformer-based long-term series forecasting\n130. Zhang J, Zheng Y , Qi D (2016) Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows \nPrediction. In: Proceedings of the AAAI conference on artificial intelligence, 31(1)\n131. Liang Y et al (2021) Fine-Grained Urban Flow Prediction.  In: Proceedings of the Web Conference, pp \n1833–1845\n132. Li Y , Moura JMF (2020) Forecaster: A Graph Transformer for Forecasting Spatial and Time-Depen -\ndent Data. In: European Conference on Artificial Intelligence (ECAI), pp 1293–1300\n133. Dou K, Sun X (2021) Long-Term Weather Prediction Based on GA-BP Neural Network. In: IOP Confer-\nence Series: Earth and Environmental Science, 668(1):012015\n134. Ward SN (1995) Area-based tests of long-term seismic hazard predictions. Bull Seismol Soc Am \n85(5):1285–1298\n135. Pandit R et al (2022) Sequential data-driven long-term weather forecasting models’ performance com-\nparison for improving offshore operation and maintenance operations. Energies, 15(19):7233\n136. Qi Y et al (2019) A hybrid model for spatiotemporal forecasting of PM2.5 based on graph convolutional \nneural network and long short-term memory. Sci Total Environ, 664(MAY 10):1–10\n137. Lauffenburger JC et al (2018) Predicting Adherence to Chronic Disease medications in patients with \nlong-term initial medication fills using indicators of clinical events and Health behaviors. J Managed \nCare Specialty Pharm 24(5):469–477\n138. Sanson G et al (2020) Prediction of early- and long-term mortality in adult patients acutely admitted to \ninternal medicine: NRS-2002 and beyond. Clin Nutr 39(4):1092–1100\n139. Zeroual A et al (2020) Deep learning methods for forecasting COVID-19 time-series data: a compara -\ntive study. Chaos, Solitons & Fractals 140:110121\n140. Marling C, Bunescu R (2020) The OhioT1DM dataset for blood glucose level prediction: Update 2020. \nInform Technol Nanatechnol 2675:71–74\n141. Hyndman RJ, Koehler AB (2006) Another look at measures of forecast accuracy. Int J Forecast \n22(4):679–688\n142. Armstrong J et al (2002) Principles of forecasting: a handbook for researchers and practitioners. Int J \nForecast 18(3):468–478\n143. Coleman CD, Swanson DA (2007) On MAPE-R as a measure of cross-sectional estimation and forecast \naccuracy. J Econ Soc Meas 32(4):219–233\n144. Armstrong JS, Collopy F (1992) Error measures for generalizing about forecasting methods: empirical \ncomparisons. Int J Forecast 8(1):69–80\n145. Cleveland RB, Cleveland WS (1990) STL: a seasonal-trend decomposition procedure based on Loess. \nJ Official Stat, 6:3–73\n146. Hamilton JD (2020) Time Series Analysis. Princeton University Press.\n147. Taieb SB, Hyndman RJ (2012) Recursive and direct multi-step forecasting: the best of both worlds. In: \nProceedings of the Web Conference 2021, pp 1846–1855\n148. Chevillon G (2007) Direct multi-step estimation and forecasting. J Economic Surveys 21(4):746–785\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n1 3\nPage 29 of 29 80",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7994462847709656
    },
    {
      "name": "Transformer",
      "score": 0.7795060873031616
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5054941177368164
    },
    {
      "name": "Machine learning",
      "score": 0.4870038628578186
    },
    {
      "name": "Architecture",
      "score": 0.42645642161369324
    },
    {
      "name": "Engineering",
      "score": 0.10557594895362854
    },
    {
      "name": "Electrical engineering",
      "score": 0.09122225642204285
    },
    {
      "name": "Voltage",
      "score": 0.08272463083267212
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210152380",
      "name": "Shenzhen Technology University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I180726961",
      "name": "Shenzhen University",
      "country": "CN"
    }
  ]
}