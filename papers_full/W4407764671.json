{
    "title": "Accuracy of Large Language Models for Literature Screening in Thoracic Surgery: Diagnostic Study",
    "url": "https://openalex.org/W4407764671",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A4321235787",
            "name": "Zhang-Yi Dai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2248617723",
            "name": "Fu Qiang Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2021170025",
            "name": "Cheng Shen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2240671578",
            "name": "Yan-Li Ji",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2242727724",
            "name": "Zhi Yang Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099835616",
            "name": "Yun Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2123891594",
            "name": "Pu Qiang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4400414272",
        "https://openalex.org/W2044392066",
        "https://openalex.org/W2904719162",
        "https://openalex.org/W2134833483",
        "https://openalex.org/W2593758073",
        "https://openalex.org/W3108725181",
        "https://openalex.org/W3089862415",
        "https://openalex.org/W2992764683",
        "https://openalex.org/W4213127247",
        "https://openalex.org/W4388725043",
        "https://openalex.org/W4381480589",
        "https://openalex.org/W4382678522",
        "https://openalex.org/W4399363701",
        "https://openalex.org/W4388858816",
        "https://openalex.org/W4367368990",
        "https://openalex.org/W2998659891",
        "https://openalex.org/W3012272598",
        "https://openalex.org/W4223503116",
        "https://openalex.org/W4362620139",
        "https://openalex.org/W4387297440",
        "https://openalex.org/W4388817294",
        "https://openalex.org/W2560438049",
        "https://openalex.org/W4386867830",
        "https://openalex.org/W4389799635",
        "https://openalex.org/W2982683456",
        "https://openalex.org/W4390613062",
        "https://openalex.org/W3111278950",
        "https://openalex.org/W4378190436",
        "https://openalex.org/W2807522649",
        "https://openalex.org/W4394576991",
        "https://openalex.org/W4379598302",
        "https://openalex.org/W2999536597"
    ],
    "abstract": "Background Systematic reviews and meta-analyses rely on labor-intensive literature screening. While machine learning offers potential automation, its accuracy remains suboptimal. This raises the question of whether emerging large language models (LLMs) can provide a more accurate and efficient approach. Objective This paper evaluates the sensitivity, specificity, and summary receiver operating characteristic (SROC) curve of LLM-assisted literature screening. Methods We conducted a diagnostic study comparing the accuracy of LLM-assisted screening versus manual literature screening across 6 thoracic surgery meta-analyses. Manual screening by 2 investigators served as the reference standard. LLM-assisted screening was performed using ChatGPT-4o (OpenAI) and Claude-3.5 (Anthropic) sonnet, with discrepancies resolved by Gemini-1.5 pro (Google). In addition, 2 open-source, machine learningâ€“based screening tools, ASReview (Utrecht University) and Abstrackr (Center for Evidence Synthesis in Health, Brown University School of Public Health), were also evaluated. We calculated sensitivity, specificity, and 95% CIs for the title and abstract, as well as full-text screening, generating pooled estimates and SROC curves. LLM prompts were revised based on a post hoc error analysis. Results LLM-assisted full-text screening demonstrated high pooled sensitivity (0.87, 95% CI 0.77-0.99) and specificity (0.96, 95% CI 0.91-0.98), with the area under the curve (AUC) of 0.96 (95% CI 0.94-0.97). Title and abstract screening achieved a pooled sensitivity of 0.73 (95% CI 0.57-0.85) and specificity of 0.99 (95% CI 0.97-0.99), with an AUC of 0.97 (95% CI 0.96-0.99). Post hoc revisions improved sensitivity to 0.98 (95% CI 0.74-1.00) while maintaining high specificity (0.98, 95% CI 0.94-0.99). In comparison, the pooled sensitivity and specificity of ASReview tool-assisted screening were 0.58 (95% CI 0.53-0.64) and 0.97 (95% CI 0.91-0.99), respectively, with an AUC of 0.66 (95% CI 0.62-0.70). The pooled sensitivity and specificity of Abstrackr tool-assisted screening were 0.48 (95% CI 0.35-0.62) and 0.96 (95% CI 0.88-0.99), respectively, with an AUC of 0.78 (95% CI 0.74-0.82). A post hoc meta-analysis revealed comparable effect sizes between LLM-assisted and conventional screening. Conclusions LLMs hold significant potential for streamlining literature screening in systematic reviews, reducing workload without sacrificing quality. Importantly, LLMs outperformed traditional machine learning-based tools (ASReview and Abstrackr) in both sensitivity and AUC values, suggesting that LLMs offer a more accurate and efficient approach to literature screening.",
    "full_text": null
}