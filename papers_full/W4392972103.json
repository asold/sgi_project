{
  "title": "Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework",
  "url": "https://openalex.org/W4392972103",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3018126080",
      "name": "Kaiyan Chang",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2097659125",
      "name": "Kun Wang",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2087975791",
      "name": "Nan Yang",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095887118",
      "name": "Ying Wang",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A3005737484",
      "name": "Dantong Jin",
      "affiliations": [
        "Zhejiang Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2122141768",
      "name": "Wenlong Zhu",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2115296195",
      "name": "ZhiRong Chen",
      "affiliations": [
        "University of Illinois Urbana-Champaign",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A3173934621",
      "name": "Cangyuan Li",
      "affiliations": [
        "International Centre for Theoretical Physics Asia-Pacific"
      ]
    },
    {
      "id": "https://openalex.org/A1915750240",
      "name": "Hao Yan",
      "affiliations": [
        "Shanghai University"
      ]
    },
    {
      "id": "https://openalex.org/A2319045523",
      "name": "Yunhao Zhou",
      "affiliations": [
        "Shanghai Innovative Research Center of Traditional Chinese Medicine",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A4322373724",
      "name": "Zhuoliang Zhao",
      "affiliations": [
        "Shanghai Innovative Research Center of Traditional Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2102674343",
      "name": "Yuan Cheng",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2631546683",
      "name": "Yudong Pan",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2135610637",
      "name": "Yiqi Liu",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096829243",
      "name": "Wang Mengdi",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2238682881",
      "name": "Shengwen Liang",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2153344922",
      "name": "Yinhe Han",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2118340910",
      "name": "Hua-Wei Li",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2113669818",
      "name": "Xiaowei Li",
      "affiliations": [
        "Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4312108362",
    "https://openalex.org/W3082750925",
    "https://openalex.org/W4378508556",
    "https://openalex.org/W4386081184",
    "https://openalex.org/W4402753889",
    "https://openalex.org/W6849716735",
    "https://openalex.org/W4388275102",
    "https://openalex.org/W4385952898",
    "https://openalex.org/W4293024083",
    "https://openalex.org/W4385774800",
    "https://openalex.org/W4386794762",
    "https://openalex.org/W4377865305",
    "https://openalex.org/W4384644607",
    "https://openalex.org/W4380558588",
    "https://openalex.org/W4319240918",
    "https://openalex.org/W6853837529",
    "https://openalex.org/W4386908182",
    "https://openalex.org/W4293868298",
    "https://openalex.org/W4220864612",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W4384918448"
  ],
  "abstract": "Recent advances in large language models have demonstrated their potential\\nfor automated generation of hardware description language (HDL) code from\\nhigh-level prompts. Researchers have utilized fine-tuning to enhance the\\nability of these large language models (LLMs) in the field of Chip Design.\\nHowever, the lack of Verilog data hinders further improvement in the quality of\\nVerilog generation by LLMs. Additionally, the absence of a Verilog and\\nElectronic Design Automation (EDA) script data augmentation framework\\nsignificantly increases the time required to prepare the training dataset for\\nLLM trainers. This paper proposes an automated design-data augmentation\\nframework, which generates high-volume and high-quality natural language\\naligned with Verilog and EDA scripts. For Verilog generation, it translates\\nVerilog files to an abstract syntax tree and then maps nodes to natural\\nlanguage with a predefined template. For Verilog repair, it uses predefined\\nrules to generate the wrong verilog file and then pairs EDA Tool feedback with\\nthe right and wrong verilog file. For EDA Script generation, it uses existing\\nLLM(GPT-3.5) to obtain the description of the Script. To evaluate the\\neffectiveness of our data augmentation method, we finetune Llama2-13B and\\nLlama2-7B models using the dataset generated by our augmentation framework. The\\nresults demonstrate a significant improvement in the Verilog generation tasks\\nwith LLMs. Moreover, the accuracy of Verilog generation surpasses that of the\\ncurrent state-of-the-art open-source Verilog generation model, increasing from\\n58.8% to 70.6% with the same benchmark. Our 13B model (ChipGPT-FT) has a pass\\nrate improvement compared with GPT-3.5 in Verilog generation and outperforms in\\nEDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.\\n",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5308913588523865
    },
    {
      "name": "Computer architecture",
      "score": 0.42095381021499634
    }
  ],
  "institutions": []
}