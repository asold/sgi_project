{
    "title": "A Comprehensive Analysis of Transformer-Deep Neural Network Models in Twitter Disaster Detection",
    "url": "https://openalex.org/W4311969553",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5041573811",
            "name": "Vimala Balakrishnan",
            "affiliations": [
                "University of Malaya"
            ]
        },
        {
            "id": "https://openalex.org/A5003667105",
            "name": "Zhongliang Shi",
            "affiliations": [
                "University of Malaya"
            ]
        },
        {
            "id": "https://openalex.org/A5038864806",
            "name": "Chuan Liang Law",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5026056328",
            "name": "Regine Lim",
            "affiliations": [
                "University of Malaya"
            ]
        },
        {
            "id": "https://openalex.org/A5015422297",
            "name": "Lee Leng Teh",
            "affiliations": [
                "University of Malaya"
            ]
        },
        {
            "id": "https://openalex.org/A5075998876",
            "name": "Yue Fan",
            "affiliations": [
                "University of Malaya"
            ]
        },
        {
            "id": "https://openalex.org/A5065527676",
            "name": "Jeyarani Periasamy",
            "affiliations": [
                "INTI International University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3135508972",
        "https://openalex.org/W3199253914",
        "https://openalex.org/W2900541036",
        "https://openalex.org/W3017226472",
        "https://openalex.org/W3217459514",
        "https://openalex.org/W2945239824",
        "https://openalex.org/W3174020389",
        "https://openalex.org/W3185988473",
        "https://openalex.org/W2742970053",
        "https://openalex.org/W2803201508",
        "https://openalex.org/W2561761609",
        "https://openalex.org/W3165299977",
        "https://openalex.org/W4252522382",
        "https://openalex.org/W2945813598",
        "https://openalex.org/W2946365616",
        "https://openalex.org/W3120465894",
        "https://openalex.org/W2912217870",
        "https://openalex.org/W2963487003",
        "https://openalex.org/W2999678654",
        "https://openalex.org/W3120217483",
        "https://openalex.org/W6791946746",
        "https://openalex.org/W3134514055",
        "https://openalex.org/W6806595320",
        "https://openalex.org/W3126965705",
        "https://openalex.org/W3074473399",
        "https://openalex.org/W3008749520",
        "https://openalex.org/W3106683869",
        "https://openalex.org/W4205708310",
        "https://openalex.org/W2770167649",
        "https://openalex.org/W2964236337",
        "https://openalex.org/W3080359515",
        "https://openalex.org/W3139150389",
        "https://openalex.org/W4205958384"
    ],
    "abstract": "Social media platforms such as Twitter are a vital source of information during major events, such as natural disasters. Studies attempting to automatically detect textual communications have mostly focused on machine learning and deep learning algorithms. Recent evidence shows improvement in disaster detection models with the use of contextual word embedding techniques (i.e., transformers) that take the context of a word into consideration, unlike the traditional context-free techniques; however, studies regarding this model are scant. To this end, this paper investigates a selection of ensemble learning models by merging transformers with deep neural network algorithms to assess their performance in detecting informative and non-informative disaster-related Twitter communications. A total of 7613 tweets were used to train and test the models. Results indicate that the ensemble models consistently yield good performance results, with F-score values ranging between 76% and 80%. Simpler transformer variants, such as ELECTRA and Talking-Heads Attention, yielded comparable and superior results compared to the computationally expensive BERT, with F-scores ranging from 80% to 84%, especially when merged with Bi-LSTM. Our findings show that the newer and simpler transformers can be used effectively, with less computational costs, in detecting disaster-related Twitter communications.",
    "full_text": null
}