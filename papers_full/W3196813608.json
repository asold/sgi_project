{
  "title": "All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality",
  "url": "https://openalex.org/W3196813608",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3168242944",
      "name": "William Timkey",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A1976982077",
      "name": "Marten van Schijndel",
      "affiliations": [
        "Cornell University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3103228128",
    "https://openalex.org/W2945830544",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3105248300",
    "https://openalex.org/W2251803266",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3094519420",
    "https://openalex.org/W2080100102",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2992121790",
    "https://openalex.org/W2252211741",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W2956530858",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3173506780",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W3188660305",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W2946296745",
    "https://openalex.org/W2963420658",
    "https://openalex.org/W2966610483",
    "https://openalex.org/W2964073004",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3174617925",
    "https://openalex.org/W3173902720",
    "https://openalex.org/W3098952151",
    "https://openalex.org/W2963366649",
    "https://openalex.org/W3098275893",
    "https://openalex.org/W4287110638",
    "https://openalex.org/W3157498557",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2170682101",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2785648721",
    "https://openalex.org/W3155825510",
    "https://openalex.org/W2907252220",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035102548",
    "https://openalex.org/W3175752238",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3168386607",
    "https://openalex.org/W2973047874",
    "https://openalex.org/W2996403597",
    "https://openalex.org/W3125516434",
    "https://openalex.org/W3030192698"
  ],
  "abstract": "Similarity measures are a vital tool for understanding how language models represent and process language. Standard representational similarity measures such as cosine similarity and Euclidean distance have been successfully used in static word embedding models to understand how words cluster in semantic space. Recently, these measures have been applied to embeddings from contextualized models such as BERT and GPT-2. In this work, we call into question the informativity of such measures for contextualized language models. We find that a small number of rogue dimensions, often just 1-3, dominate these measures. Moreover, we find a striking mismatch between the dimensions that dominate similarity measures and those which are important to the behavior of the model. We show that simple postprocessing techniques such as standardization are able to correct for rogue dimensions and reveal underlying representational quality. We argue that accounting for rogue dimensions is essential for any similarity-based analysis of contextual language models.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4527–4546\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n4527\nAll Bark and No Bite: Rogue Dimensions in Transformer Language\nModels Obscure Representational Quality\nWilliam Timkey and Marten van Schijndel\nDepartment of Linguistics\nCornell University\n{wpt25|mv443}@cornell.edu\nAbstract\nSimilarity measures are a vital tool for under-\nstanding how language models represent and\nprocess language. Standard representational\nsimilarity measures such as cosine similarity\nand Euclidean distance have been successfully\nused in static word embedding models to un-\nderstand how words cluster in semantic space.\nRecently, these measures have been applied to\nembeddings from contextualized models such\nas BERT and GPT-2. In this work, we call\ninto question the informativity of such mea-\nsures for contextualized language models. We\nﬁnd that a small number of rogue dimensions,\noften just 1-3, dominate these measures. More-\nover, we ﬁnd a striking mismatch between the\ndimensions that dominate similarity measures\nand those which are important to the behavior\nof the model. We show that simple postpro-\ncessing techniques such as standardization are\nable to correct for rogue dimensions and reveal\nunderlying representational quality. We argue\nthat accounting for rogue dimensions is essen-\ntial for any similarity-based analysis of contex-\ntual language models.\n1 Introduction\nBy mapping words into continuous vector spaces,\nwe can reason about human language in geomet-\nric terms. For example, the cosine similarity of\npairs of word embeddings in Word2Vec (Mikolov\net al., 2013) and GloVe (Pennington et al., 2014)\nshows a robust correlation with human similarity\njudgments, and embeddings cluster into natural se-\nmantic classes in Euclidean space (Baroni et al.,\n2014; Wang et al., 2019). In recent years, static\nembeddings have given way to their contextual\ncounterparts, with language models based on the\ntransformer architecture (Vaswani et al., 2017) such\nas BERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2020), XLNet (Yang et al., 2019) and GPT-2 (Rad-\nford et al., 2019) achieving state of the art results on\nmany language understanding tasks. Despite their\nsuccess, relatively little is known about how these\nmodels represent and process language. Recent\nwork has employed measures such as cosine sim-\nilarity and Euclidean distance to contextual repre-\nsentations with unclear and counterintuitive results.\nFor example, similarity/distance measures in BERT\nare extremely sensitive to word position, leading\nto inconsistent results on evaluation benchmarks\n(Mickus et al., 2020; May et al., 2019). Addition-\nally, representational quality appears to degrade\nseverely in later layers of each network, with the\nﬁnal layers of BERT, RoBERTa, GPT-2 and XLNet\nshowing little to no correlation with the semantic\nsimilarity/relatedness judgments of humans (Bom-\nmasani et al., 2020).\nRecent work which probes the representational\ngeometry of contextualized embedding spaces us-\ning cosine similarity has found that contextual em-\nbeddings have several counterintuitive properties\n(Ethayarajh, 2019). For example: 1) Word repre-\nsentations are highly anisotropic: randomly sam-\npled words tend to be highly similar to one an-\nother when measured by cosine similarity. In the\nﬁnal layer of GPT-2 for example, any two words\nare almost perfectly similar. 2) Embeddings have\nextremely low self-similarity: In later layers of\ntransformer-based language models, random words\nare almost as similar to one another as instances\nthe same word in different contexts.\nIn this work, we critically examine the infor-\nmativity of standard similarity/distance measures\n(particularly cosine similarity and Euclidean dis-\ntance) in contextual embedding spaces. We ﬁnd\nthat these measures are often dominated by 1-5 di-\nmensions across all the contextual language models\nwe tested, regardless of the speciﬁc pretraining ob-\njective. It is this small subset of dimensions which\ndrive anisotropy, low self-similarity, and the appar-\nent drop in representational quality in later layers.\nThese dimensions, which we refer to as rogue di-\nmensions are centered far from the origin and have\n4528\ndisproportionately high variance. The presence of\nrogue dimensions can cause cosine similarity and\nEuclidean distance to rely on less than 1% of the\nembedding space. Moreover, we ﬁnd that the rogue\ndimensions which dominate cosine similarity do\nnot likewise dominate model behavior, and show a\nstrong correlation with absolute position and punc-\ntuation.\nFinally, we show that these dimensions can be\naccounted for using a trivially simple transforma-\ntion of the embedding space: standardization. Once\napplied, cosine similarity more closely reﬂects hu-\nman word similarity judgments, and we see that\nrepresentational quality is preserved across all lay-\ners rather than degrading/becoming task-speciﬁc.\nTaken together, we argue that accounting for rogue\ndimensions is essential when evaluating representa-\ntional similarity in transformer language models.1\n2 Background\nStandard measures such as cosine similarity or Eu-\nclidean distance in contextual embedding spaces\nhave been used in a wide range of applications:\nto understand how the representational similarity\nof word embedding spaces corresponds to human\nsemantic similarity/relatedness judgments (Bom-\nmasani et al., 2020; Vuli ´c et al., 2020; Chronis\nand Erk, 2020; A. Rodriguez and Merlo, 2020),\nhuman brain activation patterns/cross-model simi-\nlarity (Abnar et al., 2019), syntax structure (Chru-\npała and Alishahi, 2019), semantic shift (Martinc\net al., 2020), compositionality/idomaticity of word\nvectors (Garcia et al., 2021), polysemy (Soler and\nApidianaki, 2021), context-sensitivity (Reif et al.,\n2019), social bias (May et al., 2019; Bommasani\net al., 2020), changes to the embedding space dur-\ning ﬁne-tuning (Merchant et al., 2020), and as an\nevaluation metric for text generation (Zhang et al.,\n2020).\nHowever, a number of works have questioned the\nappropriateness of cosine similarity. Schnabel et al.\n(2015) found that static embedding models encode\na substantial degree of word frequency information,\nwhich leads to a frequency bias in cosine similar-\nity. May et al. (2019) questioned the adequacy of\ncosine similarity in sentence encoders after ﬁnding\ncontextual discrepancies in bias measures. Perhaps\nmost relevant to the present work is Zhelezniak\net al. (2019) which treats individual word embed-\n1Our code is publically released at: http://github.\ncom/wtimkey/rogue-dimensions\ndings as statistical samples, shows the equivalence\nof cosine similarity and Pearson correlation, and\nnotes that Pearson correlation (and therefore cosine\nsimilarity) is highly sensitive to outlier dimensions.\nThey further suggest the use of non-parametric rank\ncorrelation measures such as Spearman’sρ, which\nis robust to outliers. Our work investigates the sen-\nsitivity of cosine similarity to outlier dimensions\nin contextual models, and further characterizes the\nbehavioral correlates of these outliers.\nOur goal in this work was not causal explanation\nof degenerate embedding spaces or post-processing\nfor task performance gains, but rather to empiri-\ncally motivate trivially simple transformations to\nenable effective interpretability research with exist-\ning metrics. However, we refer interested readers to\nGao et al. (2019) who studied degeneration toward\nanisotropy in machine translation. Similarly, Li\net al. (2020) suggested a learned transformation of\ntransformer embedding spaces which resulted in in-\ncreased performance on semantic textual similarity\ntasks.\n3 Rogue Dimensions and\nRepresentational Geometry\n3.1 Anisotropy\nIn this section, we investigate how each dimension\nof the embedding space contributes to anisotropy,\ndeﬁned by Ethayarajh (2019) as the expected co-\nsine similarity of randomly sampled token pairs.\nThey showed that contextual embedding spaces\nare highly anisotropic, meaning that the contextual\nrepresentations of any two tokens are expected to\nbe highly similar to one another. We investigate\nthis counterintuitive property by decomposing the\ncosine similarity computation by dimension, and\nshow that the cosine similarity of any two tokens\nis dominated by a small subset of rogue dimen-\nsions. We conclude that anisotropy is not a global\nproperty of the entire embedding space, but is in-\nstead driven by a small number of idiosyncratic\ndimensions.\n3.1.1 Setup\nEthayarajh (2019) deﬁnes the anisotropy in layer ℓ\nof model f as the expected cosine similarity of any\npair of words in a corpus. This can be approximated\nas ˆA( fℓ) from a sample S of n random token pairs\nfrom a corpus O. S = {{x1,y1},..., {xn,yn}}∼O:\n4529\nˆA( fℓ) =1\nn · ∑\n{xα ,yα }∈S\ncos( fℓ(xα ), fℓ(yα )) (1)\nThe cosine similarity, between two vectorsu and\nv of dimensionality d is deﬁned as\ncos(u,v) = u ·v\n∥u∥∥v∥=\nd\n∑\ni=1\nuivi\n∥u∥∥v∥ (2)\nExpressing cosine similarity as a summation\nover d dimensions, we can deﬁne a function\nCCi(u,v) which gives contribution of dimension\ni to the total cosine similarity of u and v as:\nCCi(u,v) = uivi\n∥u∥∥v∥ (3)\nFrom this, we deﬁne CC( f i\nℓ), the contribution of\ndimension i to ˆA( fℓ) as:\nCC( f i\nℓ) =1\nn · ∑\n{xα ,yα }∈S\nCCi( fℓ(xα ), fℓ(yα )) (4)\nNote that ∑d\ni CC( f i\nℓ) =ˆA( fℓ). From the mean co-\nsine contribution by dimension, we can determine\nhow much each dimension contributes to the total\nanisotropy. If CC( f 1\nℓ ) ≈CC( f 2\nℓ ) ≈... ≈CC( f d\nℓ )\nthen we conclude that anisotropy is a global prop-\nerty of the embedding space; no one dimension\ndrives the expected cosine similarity of any two em-\nbeddings. By contrast, if CC( f i\nℓ) >> ∑d\nj̸=i CC( f j\nℓ )\nthen we conclude that dimension i dominates the\ncosine similarity computation.\n3.1.2 Experiment\nWe compute the average cosine similarity contri-\nbution, CC( f i\nℓ), for each dimension in all layers of\nBERT, RoBERTa, GPT-2, and XLNet.2 We then\nnormalize by the total expected cosine similarity\nˆA( fℓ) to get the proportion of the total expected\ncosine similarity contributed by each dimension.\nAll models are of dimensionality d = 768 and have\n12 layers, plus one static embedding layer. We also\ninclude two 300 dimensional non-contextual mod-\nels, Word2Vec3 and GloVe,4 for comparison. Our\ncorpus O is an 85k token sample of random arti-\ncles from English Wikipedia. All input sequences\n2All models from https://github.com/\nhuggingface/transformers\n3https://zenodo.org/record/4421380\n4https://nlp.stanford.edu/projects/\nglove/ (Wikipedia+Gigaword 5, 300d)\nModel Layer 1 2 3 ˆA( fℓ)\nGPT-2 11 0.275 0.269 0.265 0.640\n12 0.763 0.131 0.078 0.885\nBERT 10 0.817 0.004 0.003 0.396\n11 0.884 0.003 0.002 0.506\nRoBERTa 7 0.726 0.193 0.032 0.705\n12 0.663 0.262 0.020 0.745\nXLNet 10 0.990 0.000 0.000 0.887\n11 0.996 0.001 0.000 0.981\nWord2Vec 0.031 0.023 0.023 0.130\nGloVe 0.105 0.096 0.095 0.104\nTable 1: Proportion of total expected cosine similarity,\nCC( f i\nℓ)/ ˆA( fℓ), contributed by each of the top 3 dimen-\nsions in the two most anisotropic layers of each model,\nalong with the anisotropy estimate ˆA( fℓ) for the given\nlayer. Results for all layers can be found in Table 4 of\nthe appendix.\nconsisted of 128 tokens. From the resulting rep-\nresentations we take a random sample S of 500k\ntoken pairs. For each model, we report the three\ndimensions with the largest cosine contributions\nin the two most anisotropic layers, as well as the\noverall anisotropy ˆA( fℓ).\n3.1.3 Results and Discussion\nResults are summarized in Table 1. The static mod-\nels Word2Vec and GloVe are relatively isotropic\nand are not dominated by any single dimension.\nAcross all transformer models tested, a small sub-\nset of rogue dimensions dominate the cosine\nsimilarity computation , especially in the more\nanisotropic ﬁnal layers. Perhaps the most strik-\ning case is layers 10 and 11 of XLNet, where a\nsingle dimension contributes more than 99% of\nthe expected cosine similarity between randomly\nsampled tokens.\nThe dimensions which drive anisotropy are cen-\ntered far from the origin relative to other dimen-\nsions. For example, the top contributing dimension\nin the ﬁnal layer of XLNet ( i = 667) has a mean\nactivation of E[x667\n12 ] =180.0, while the expected\nactivation of all other dimensions is E[xi̸=667\n12 ] =\n−0.084 with standard deviation σ[xi̸=667\n12 ] =0.77.\nOne implication of anisotropy is that the em-\nbeddings occupy a narrow cone in the embedding\nspace, as the angle between any two word em-\nbeddings is very small. However, if anisotropy\nis driven by a single dimension (or a small subset\nof dimensions), we can conclude that the cone lies\nalong a single axis or within a low dimensional sub-\nspace, rather than being a global property across\n4530\nall dimensions.5 We conclude from this analysis\nthat the anisotropy of the embedding space is an\nartifact of cosine similarity’s high sensitivity to a\nsmall set of outlier dimensions and is not a global\nproperty of the space.6\n3.2 Informativity of Similarity Measures\nIn the previous section, we found that anisotropy\nis driven by a small subset of dimensions. In this\nsection, we investigate whether standard similarity\nmeasures are still informed by the entire embedding\nspace, or if variability in the measure is also driven\nby a small subset of dimensions.\nFor example, it could be the case that some di-\nmension i has a large, but roughly constant activa-\ntion across all tokens, meaning E[CC( f i\nℓ)] will be\nlarge, but Var[CC( f i\nℓ)] will be near zero. In this\ncase, we would be adding a large constant to co-\nsine similarity, makingAnisotropy( fℓ) large but not\nchanging Var[cos( fℓ(x), fℓ(y)]. In this case, the av-\nerage cosine similarity would be driven toward 1.0\nby dimension i, but any changes in cosine similarity\nwould be driven by the rest of the embedding space,\nnot dimension i, meaning cosine similarity would\nprovide information about the entire representation\nspace, rather than a single dimension. Conversely,\ndimension i may have mean activation near zero,\nbut extremely large variance across tokens. In this\ncase, dimension i would not appear to make the\nspace anisotropic, but would still drive variability\nin cosine similarity. Ultimately, we’re not inter-\nested in where the representation space is centered,\nbut whether changes in a similarity measure reﬂect\nchanges in the entire embedding space.\nIn this section we uncover which dimensions\ndrive the variability of cosine similarity.7 Paral-\nleling our ﬁndings in Section 3.1 we ﬁnd that the\ntoken pairs which are similar/dissimilar to one an-\nother completely change when we remove just 1-5\ndominant dimensions from the embedding space.\n3.2.1 Setup\nLet fℓ(x) : X − →Rd, be the function which maps\na token x to its representation in layer l of model\n5Our analysis complements that of Cai et al. (2021)\nwhich used Principle Component Analysis to identify iso-\nlated isotropic clusters as well as embedding cones in a space\nreduced to three dimensions.\n6We additionally replicated Ethayarajh (2019) before and\nafter removing rogue dimensions in Appendix A. We show\nthat their analyses are extremely sensitive to rogue dimensions.\n7We conduct the same analysis using Euclidean distance\nin Appendix B and reach similar conclusions as with cosine\nsimilarity.\nModel Layer k=1 k=3 k=5\nGPT-2 0 0.999 0.996 0.996\n11 0.967 0.352 0.352\n12 0.819 0.232 0.232\nBERT 0 0.999 0.997 0.997\n11 0.046 0.048 0.048\n12 0.213 0.214 0.214\nRoBERTa 0 0.810 0.770 0.770\n11 0.591 0.319 0.319\n12 0.566 0.301 0.301\nXLNet 0 0.999 0.996 0.996\n11 0.124 0.150 0.150\n12 0.028 0.024 0.024\nWord2vec 0.998 0.993 0.988\nGloVe 0.987 0.954 0.930\nTable 2: Proportion of variance in cosine similarity r2\nexplained by cosine similarity when the top k dimen-\nsions, measured by CC( f i\nℓ), are removed. Layer 0 is\nthe static embedding layer. Results for all layers can be\nfound in Table 5 of the Appendix.\nf . Let f ′\nℓ(x) : X − →Rd−k be the function which\nmaps token x to its representation with top k dimen-\nsions (measured by contribution to cosine similar-\nity) removed. Let C(S) = cos\nx,y∈S\n( fℓ(x), fℓ(y)) and\nC′(S) = cos\nx,y∈S\n( f ′\nℓ(x), f ′\nℓ(y)). In this analysis, we\ncompute:\nr = Corr[C(S),C′(S)] (5)\nThis is the Pearson correlation between the co-\nsine similarities in the entire embedding space and\nthose similarities when k dimensions are removed.\nIn our analysis we report r2 which corresponds to\nthe proportion of variance in C(S) explained by\nC′(S). For example, if we were to set k=1, and\nthe observed r2 is large, then cosine similarity in\nthe full embedding space is still well explained by\nthe remaining d −1 dimensions. By contrast, if\nr2 is small, then the variance of cosine similarity\nin the embedding space can not be well explained\nby the bottom d −1 dimensions, and thus a single\ndimension drives variability in cosine similarity.\n3.2.2 Experiment\nFor this experiment, we compute r2 =\nCorr[C(x,y),C′(x,y)]2 for all layers of all\nmodels, using the same set of token representations\nas in Section 3.1. We remove the top k = {1,3,5}\ndimensions, where dimensions are ranked by\nCC( f i\nℓ), the cosine similarity contribution of\ndimension i in layer l. We report results for the\nﬁrst layer and the ﬁnal two layers. Results for all\nlayers can be found in Table 5 of the Appendix.\n4531\n3.2.3 Results\nResults are summarized in Table 2. We ﬁnd that\nin the static embedding models and the earlier lay-\ners of each contextual model, no single dimension\nor subset of dimensions drives the variability in\ncosine similarity. By contrast, in later layers, the\nvariability of cosine similarity is driven by just 1-5\ndimensions. In the extreme cases of XLNet-12 and\nBERT-11, when we remove just a single dimension\nfrom the embedding space, almost none of the vari-\nance in cosine similarity can be explained by cosine\nsimilarity in the d −1 dimensional subspace. (r2 =\n0.028 and 0.046 respectively) This means that the\ntoken pairs which are similar to one another in the\nfull embedding space are drastically different from\nthe pairs which are similar when just a handful of\ndimensions are removed.\nWhile similarity measures should reﬂect proper-\nties of the entire embedding space, we have shown\nthat this is not the case with cosine similarity in con-\ntextualized embedding spaces. Not only do a small\nsubset of dimensions in later layers drive the co-\nsine similarity of randomly sampled words toward\n1.0, but this subset also drives the variability of the\nmeasure. This result effectively renders cosine\nsimilarity a measure over 1-5 rogue dimensions\nrather than the entire embedding space.\n4 Rogue Dimensions and Model\nBehavior\nIn this section, we address the question of whether\nthe dimensions which dominate cosine similarity\nlikewise dominate model behavior. Speciﬁcally, if\nsimilarity measures are dominated by only a few\ndimensions, as shown in the previous sections, then\nthose dimensions should be the only ones the model\nactually uses, otherwise, the measures only reﬂect\na small subset of what the model is doing. We ﬁnd\nthat dimensions which dominate cosine similarity\ndo not likewise dominate model behavior.\n4.1 Behavioral Inﬂuence of Individual\nDimensions\nWe measure the inﬂuence of individual dimensions\non model behavior through an ablation study in the\nstyle of Morcos et al. (2018).8 The idea of neuron\n8There are several possible ways to assess the importance\nof individual neurons on prediction. One popular technique is\nLayerwise Relevance Propagation (Bach et al., 2015) which\nhas recently been used in Transformer-based models (V oita\net al., 2020). We use feature ablation due to its ease of imple-\nmentation and generalizability across architectures.\nablation studies is to examine how the performance\nof a network changes when a neuron is clamped\nto a ﬁxed value, typically zero. In our study, we\nmeasure how much the language modeling distri-\nbution changes when dimension i of layer ℓ is ﬁxed\nto zero.\n4.2 Setup\nLet Pf (s) be the original language modeling dis-\ntribution of model f for some input s sampled\nfrom corpus O. We measure how the distribu-\ntion changes after ablation using KL divergence\nbetween the ablated model distribution and the un-\naltered reference distribution.9 We use KL diver-\ngence, rather than typical measures of importance\nin feature ablation such as accuracy or perplexity\nbecause we are interested in how much the predic-\ntion distributions change rather than performance\non some task. Our measure of the importance of\ndimension i in layer ℓ of model f is the mean KL\ndivergence between the two distributions across our\ncorpus, where S is a set of n inputs to the model.\nI(i,ℓ, f ) =1\nn\nn\n∑\ns∈S\nDKL[Pf (s)∥Pf (s|f i\nℓ(s) =0)] (6)\n4.3 Experiment\nTo measure the importance of each dimension to\nmodel behavior, we compute I(i,ℓ, f ) for the last 4\nlayers of each model over 10k distributions. Since\nthe autoregressive models (GPT-2, XLNet) give\na language modeling distribution over all tokens\nin the input, we use a corpus of 10k tokens from\nEnglish Wikipedia. In the auto-encoder models\n(BERT, RoBERTa), we mask 15% of tokens and\nuse a corpus of 150k tokens, for a total of 10k\nlanguage modeling distributions. We plot the rela-\ntive behavioral inﬂuence of each dimension against\nits contribution to cosine similarity, measured by\nCC( f i\nℓ), (each is normalized to sum to 1).\n4.4 Results\nFigure 1 displays the results for the ﬁnal layer of\neach model.10 In all models, we see that the di-\nmensions which dominate cosine similarity do\nnot likewise dominate model behavior. The mis-\nmatch is less drastic in BERT’s ﬁnal layer, but\n9We zero out dimensions by setting the appropriate layer\nnormalization parameters γ and β to zero.\n10The plots for layers 9-11 can be found in Figure 7 in the\nsupplementary materials.\n4532\nFigure 1: Relative contribution of each dimension to cosine similarity, CC( f i\nℓ), (top) paired with its relative in-\nﬂuence on model behavior, I(i,ℓ, f ) (bottom). The top and bottom portions of the plots each have 768 bars, one\nfor each dimension in layer 12. The width of the bars corresponds to their relative contribution to each metric.\nFor example, three dimensions (yellow, red, light yellow) dominate cosine similarity in GPT-2, but when we trace\nthose dimensions to the bottom half of the plot, they appear to vanish, meaning their relative inﬂuence on model\nbehavior is negligible. While this mismatch is less pronounced for BERT, it is particularly extreme in XLNet,\nwhere a single dimension dominates cosine similarity, but is effectively meaningless to the pretraining objective.\nis quite severe in ﬁnal XLNet and GPT-2, where\nremoving the dimensions which dominate cosine\nsimilarity does not lead to substantial changes in\nthe language modeling distribution.\nWhile ablating rogue dimensions often alters\nthe language modeling distribution more than ab-\nlating non-rogue dimensions, we emphasize that\nthere is not a one-to-one correspondence between\na dimension’s inﬂuence on cosine similarity and\nits inﬂuence on language modeling behavior. In\nthe case of XLNet and GPT-2, removing dimen-\nsions which dominate cosine similarity leads to\nonly vanishingly small changes to the behavior of\nthe model.\n4.5 Behavioral Correlates of Rogue\nDimensions\nWe now turn to the related question of whether\nrogue dimensions actually capture linguistically\nmeaningful information. Because rogue dimen-\nsions dominate representational similarity mea-\nsures, these measures will be heavily biased toward\nwhatever information these dimensions capture. To\nexplore their behavioral correlates, we plotted the\ndistribution of the values for rogue dimensions.\nWe show in Figure 2 that rogue dimensions of-\nten have highly type/position speciﬁc activation\npatterns. Rogue dimensions in all models are partic-\nularly sensitive to instances of the \".\" token and/or\nposition 0 of the input. For example, in laters 2-11\nof GPT-2 and RoBERTa, the mean cosine similarity\nof any two tokens in position 0 is greater than .99,\nwhile the mean similarity of tokens not in position\n0 is .623 and .564 respectively.\nWhile the transformer language models we have\ntested have all been shown to capture a rich range\nof linguistic phenomena, this linguistic knowledge\nmay be obscured by rogue dimensions. The follow-\ning section empirically evaluates this hypothesis.\n5 Postprocessing and Representational\nQuality\nWhile we have shown that the representational ge-\nometry of contextualized embeddings makes cosine\nsimilarity uninformative, there are several simple\npostprocessing methods which can correct for this.\nIn this section we outline three such methods: stan-\ndardization, all-but-the-top (Mu and Viswanath,\n2018), and ranking (via Spearman correlation).\nWe evaluate representational quality of the post-\nprocessed embeddings on several word similar-\nity/relatedness datasets and show that the under-\nlying representational quality is obscured by the\nrogue dimensions. When we correct for rogue di-\nmensions, correlation with human similarity judg-\nments improves across the board. We also ﬁnd\nthat representational quality is preserved across all\nlayers, rather than giving way to degraded/task spe-\nciﬁc representations as argued in previous work.\n4533\nFigure 2: Distribution of values in the dimension with the highest variance in layer 11 of each model across a\nsample of 10k tokens from English Wikipedia. Each color corresponds to a speciﬁc type/position. The orange\ndistribution is tokens which occur in position zero, the blue distribution is instances of the \".\" token, and green is\ninstances of all other tokens. Results for all layers can be found in Figures 8 and 9 of the appendix.\n5.1 Postprocessing\nStandardization: We have observed that a small\nsubset of dimensions with means far from zero and\nhigh variance completely dominate cosine similar-\nity. A straightforward way to adjust for this is to\nsubtract the mean vector and divide each dimension\nby its standard deviation, such that each dimension\nhas µi = 0 and σi = 1. Concretely, given some cor-\npus of length |O|containing word representations\nx ∈Rd, we compute the mean vector µ ∈Rd\nµ = 1\n|O|·∑\nx∈O\nx (7)\nas well as the standard deviation in each dimen-\nsion σ ∈Rd\nσ =\n√\n1\n|O|·∑\nx∈O\n(x −µ)2 (8)\nOur new standardized representation for each\nword vector (z) becomes the z-score in each dimen-\nsion.\nz = x −µ\nσ (9)\nAll-but-the-top: Following from similar obser-\nvations (a nonzero common mean vector and a\nsmall number of dominant directions) in static em-\nbedding models, Mu and Viswanath (2018) pro-\nposed subtracting the common mean vector and\neliminating the top few principle components (they\nsuggested the top d\n100 ), which should capture the\nvariance of the rogue dimensions in the model 11\nand make the space more isotropic.\nSpearman’sρ: Zhelezniak et al. (2019) treat\nword embeddings as d observations from an |O|-\nvariate distribution, and use Pearson correlation as\n11See Cai et al. (2021) for further discussion of the top\nprinciple components of contextual language models.\na measure of similarity. They propose the use of\nnon-parametric rank correlation coefﬁcients, such\nas Spearman’s ρ when embeddings depart from\nnormality. Spearman correlation is just Pearson\ncorrelation but between the ranks of embeddings,\nrather than their values. Thus Spearman correlation\ncan also be thought of as a postprocessing tech-\nnique, where instead of standardizing the space\nor removing the top components, we simply trans-\nform embeddings as x′= rank(x). Spearman’sρ is\nrobust to outliers and thus will not be dominated\nby the rogue dimensions of contextual language\nmodels. Unlike standardization and all-but-the-\ntop, Spearman correlation requires no computa-\ntions over the entire corpus. While rank-based\nsimilarity measures will not be dominated by rogue\ndimensions, rogue dimensions will tend to occupy\nthe top or bottom ranks.\n5.2 Representational Quality\nWhile we have shown that cosine similarity is\ndominated by a small subset of dimensions, a re-\nmaining question is whether adjusting for these\ndimensions makes similarity measures more in-\nformative. In particular, we evaluate whether the\ncosine similarities between word pairs align more\nclosely with human similarity judgments after post-\nprocessing. We evaluate this using 4 word similar-\nity/relatedness judgment datasets: RG65 (Ruben-\nstein and Goodenough, 1965), WS353 (Agirre\net al., 2009), SIMLEX999 (Hill et al., 2015) and\nSIMVERB 3500 (Gerz et al., 2016). Examples in\nthese datasets consist of a pair of words and a corre-\nsponding similarity rating averaged over several hu-\nman annotators. Because the similarity judgments\nwere designed to evaluate static embeddings, we\nuse the context-aggregation strategy of Bommasani\n4534\net al. (2020) to produce static representations.12\nFor each model, we report the Spearman\ncorrelation between the model similarities and\nhuman-similarity judgments, averaged across all\n4 datasets.13 We report the correlation for cosine\nsimilarities of the original embeddings, as well as\nfor postprocessed embeddings using four strategies:\nstandardization, all-but-the-top (removing the top\n7 components), only subtracting the mean (the step\ncommon to both strategies) and Spearman correla-\ntion.\n5.3 Results\nResults are summarized in Figure 3. Our key ﬁnd-\nings are:\nPostprocessing aligns the embedding space\nmore closely to human similarity judgments\nacross almost all layers of all models. We found\nthat standardization was the most successful post-\nprocessing method, showing consistent improve-\nment over the original embeddings in all but the\nearly layers of BERT.\nAll-but-the-top was generally effective, though\nthe resulting ﬁnal layer of RoBERTa and GPT-2\nexhibited poor correlation with human judgements,\nsimilar to the original embeddings. In pilot analy-\nses, we found that all-but-the-top is highly depen-\ndent on the number of components removed, a hy-\nperparameter, D, which Mu and Viswanath (2018)\nsuggest should be d\n100 . Just removing the ﬁrst prin-\nciple component in RoBERTa yielded a stronger\ncorrelation, but all-but-the top did not signiﬁcantly\nimprove correlation with human judgements in the\nﬁnal layer of GPT-2 for any choice of D.\nSimply subtracting the mean vector also yielded\nsubstantial gains in most models with the exception\nof the ﬁnal layers of GPT-2 and XLNet. The rogue\ndimensions in the last layer of these two models\nhave exceptionally high variance. While subtract-\ning the mean made the space more isotropic as\nmeasured by cosine similarity, it did not reduce the\nvariance of each dimension. We found, particularly\nin the ﬁnal layer of GPT-2 and XLNet that 1-3 di-\nmensions drive the variability of cosine similarity,\nand this was still the case when the mean vector\n12We aggregate over between 200-500 single-sentence\ncontexts of each word type using sentences from English\nWikipedia. Words with an insufﬁcient number of contexts\nwere omitted, leaving a total of 1,894 unique words and 4,577\nunique pairs. We use mean pooling over subwords to get a\nsingle representation for a word.\n13Full results from each dataset can be seen in Figures 10,\n11, 12, 13 of the Appendix.\nwas subtracted.\nConverting embeddings into ranks (Spearman\ncorrelation) also resulted in signiﬁcantly stronger\ncorrelations with human judgments in all layers of\nall models, though the correlation was often weaker\nthan standardization or all-but-the-top.\nRepresentational quality is preserved across\nall layers. Previous work has suggested that the\nﬁnal layers of transformer language models are\nhighly task-speciﬁc. Liu et al. (2019) showed that\nthe middle layers of BERT outperform the ﬁnal\nlayers on language understanding tasks. Using a\ncosine-similarity based text-generation evaluation\nmetric, Zhang et al. (2020) showed a sharp drop in\ncorrelation to human judgements of machine trans-\nlation quality in ﬁnal layers of various transformer\nlanguage models. Similarly, Davis and van Schijn-\ndel (2020) used Representational Similarity Analy-\nsis (RSA) with Pearson correlation14 and found that\nintermediate layers of GPT-2 and TransformerXL\nencode human-like implicit causality biases which\nare subsequently obscured in ﬁnal layers.\nOur ﬁndings suggest that linguistic representa-\ntional quality (in this case lexical semantics) is ac-\ntually preserved in the ﬁnal layers but is obscured\nby a small handful of rogue dimensions. After\nsimple postprocessing, later layers of the model\ncorrelate just as well, if not better than intermedi-\nate layers with human similarity judgments. This\nﬁnding reafﬁrms the need to carefully consider the\nrepresentational geometry of a model before draw-\ning conclusions about layerwise representational\nquality, and the general linguistic knowledge these\nmodels encode.\n6 Discussion and Future Work\nPerhaps the most important direction for future\nwork is designing and implementing language mod-\nels which do not develop rogue dimensions in the\nﬁrst place. Gao et al. (2019) introduce a cosine-\nregularization term during pretraining which im-\nproved the performance of transformer models on\nmachine translation. Perhaps BERT or GPT models\ncould similarly beneﬁt from such regularization.\nA prerequisite for designing models without\nrogue dimensions is understanding how these di-\nmensions arise over time. Contemporaneous work\nfrom Bi´s et al. (2021) provides a useful characteri-\nzation of how degenerate representations may be\n14Zhelezniak et al. (2019) showed Pearson correlation to be\neffectively equivalent to cosine similarity.\n4535\nFigure 3: Average correlation (Spearman’sρ) with human judgments in the four word similarity datasets, with and\nwithout postprocessing.\nlearned, which largely focuses on token frequency,\nwhile Kovaleva et al. (2021) provide a characteri-\nzation of how outliers impact model performance,\nattributing much of the problem to scaling factors in\nlayer normalization, and Luo et al. (2021) make ob-\nservations about the contribution of positional em-\nbeddings. In the present work, we observe strong\ncorrelations with speciﬁc tokens and positions. Uni-\nfying these accounts is an important task for future\nwork. With the recent release of the MultiBERT\ncheckpoints (Sellam et al., 2021), future work can\nuncover whether rogue dimensions are a coinci-\ndental property of some models, or whether they\nare a requisite for good performance. The Multi-\nBERTs may also elucidate how these dimensions\nemerge during pretraining. While we empirically\nmotivate a trivially simple transformation which\ncorrects for rogue dimensions, we believe the most\nfruitful direction for future work is to build models\nwhose representations require no post-hoc transfor-\nmations. This would result in more interpretable\nembedding spaces and may additionally lead to\nmodels with better performance.\n7 Conclusion\nIn this work, we showed that similarity measures\nin contextual language models are largely reﬂec-\ntive of a small number of rogue dimensions, not\nthe entire embedding space. Consequently, a few\ndimensions can drastically change the conclusions\nwe draw about the linguistic phenomena a model\nactually captures. We showed that the previously\nobserved anisotropy in contextual models is essen-\ntially an artifact of rogue dimensions and is not a\nglobal property of the entire embedding space. We\nalso showed that variability in similarity is driven\nby just 1-5 dimensions of the embedding space. In\nmany cases, removing just a single dimension com-\npletely changed which token pairs were similar to\none another. However, we found that model behav-\nior was not driven by these rogue dimensions, and\nthat these dimensions seem to handle a small subset\nof a model’s linguistic abilities, such as punctua-\ntion and positional information. In summary, stan-\ndard similarity measures such as cosine similarity\nand Euclidean distance are not informative mea-\nsures of how contextual language models represent\nand process language. We argue that measures of\nsimilarity in contextual language models must ac-\ncount for rogue dimensions using techniques such\nas standardization. These techniques should not\njust be viewed as avenues to improve downstream\nperformance, but as prerequisites for any analysis\ninvolving representational similarity.\nAcknowledgements\nWe would like to thank Maria Antoniak, Valts\nBlukis, Forrest Davis, Liye Fu, Ge Gao, Tianze Shi,\nAna Smith, Karen Zhou, members of the Cornell\nNLP Group and the Computational Psycholinguis-\ntics Discussions research group (C.Psyd) for their\nvaluable feedback on earlier drafts of this work.\nWe additionally thank Rishi Bommasani for pro-\nductive, stimulating discussion. Finally, we thank\nthe reviewers and area chairs for their detailed and\ninsightful feedback.\nReferences\nMaria A. Rodriguez and Paola Merlo. 2020. Word asso-\nciations and the distance properties of context-aware\nword embeddings. In Proceedings of the 24th Con-\nference on Computational Natural Language Learn-\ning, pages 376–385, Online. Association for Compu-\ntational Linguistics.\n4536\nSamira Abnar, Lisa Beinborn, Rochelle Choenni, and\nWillem Zuidema. 2019. Blackbox meets blackbox:\nRepresentational similarity & stability analysis of\nneural language models and brains. In Proceedings\nof the 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n191–203, Florence, Italy. Association for Computa-\ntional Linguistics.\nEneko Agirre, Enrique Alfonseca, Keith Hall, Jana\nKravalova, Marius Pa¸ sca, and Aitor Soroa. 2009. A\nstudy on similarity and relatedness using distribu-\ntional and WordNet-based approaches. In Proceed-\nings of Human Language Technologies: The 2009\nAnnual Conference of the North American Chap-\nter of the Association for Computational Linguis-\ntics, pages 19–27, Boulder, Colorado. Association\nfor Computational Linguistics.\nSebastian Bach, Alexander Binder, Grégoire Mon-\ntavon, Frederick Klauschen, Klaus-Robert Müller,\nand Wojciech Samek. 2015. On pixel-wise explana-\ntions for non-linear classiﬁer decisions by layer-wise\nrelevance propagation. PLOS ONE, 10(7):1–46.\nMarco Baroni, Georgiana Dinu, and Germán\nKruszewski. 2014. Don’t count, predict! a\nsystematic comparison of context-counting vs.\ncontext-predicting semantic vectors. In Proceedings\nof the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long\nPapers), pages 238–247, Baltimore, Maryland.\nAssociation for Computational Linguistics.\nDaniel Bi ´s, Maksim Podkorytov, and Xiuwen Liu.\n2021. Too much in common: Shifting of embed-\ndings in transformer language models and its impli-\ncations. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5117–5130, Online. Association for\nComputational Linguistics.\nRishi Bommasani, Kelly Davis, and Claire Cardie.\n2020. Interpreting Pretrained Contextualized Repre-\nsentations via Reductions to Static Embeddings. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4758–\n4781, Online. Association for Computational Lin-\nguistics.\nXingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth\nChurch. 2021. Isotropy in the contextual embed-\nding space: Clusters and manifolds. In International\nConference on Learning Representations.\nGabriella Chronis and Katrin Erk. 2020. When is a\nbishop not like a rook? when it’s like a rabbi! multi-\nprototype BERT embeddings for estimating seman-\ntic relationships. In Proceedings of the 24th Con-\nference on Computational Natural Language Learn-\ning, pages 227–244, Online. Association for Compu-\ntational Linguistics.\nGrzegorz Chrupała and Afra Alishahi. 2019. Corre-\nlating neural and symbolic representations of lan-\nguage. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 2952–2962, Florence, Italy. Association\nfor Computational Linguistics.\nForrest Davis and Marten van Schijndel. 2020. Dis-\ncourse structure interacts with reference but not syn-\ntax in neural language models. In Proceedings of\nthe 24th Conference on Computational Natural Lan-\nguage Learning, pages 396–407, Online. Associa-\ntion for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tieyan Liu. 2019. Representation degenera-\ntion problem in training natural language generation\nmodels. In International Conference on Learning\nRepresentations.\nMarcos Garcia, Tiago Kramer Vieira, Carolina Scarton,\nMarco Idiart, and Aline Villavicencio. 2021. Prob-\ning for idiomaticity in vector space models. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 3551–3564, Online.\nAssociation for Computational Linguistics.\nDaniela Gerz, Ivan Vuli´c, Felix Hill, Roi Reichart, and\nAnna Korhonen. 2016. SimVerb-3500: A large-\nscale evaluation set of verb similarity. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2173–2182,\nAustin, Texas. Association for Computational Lin-\nguistics.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimLex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics, 41(4):665–695.\nOlga Kovaleva, Saurabh Kulshreshtha, Anna Rogers,\nand Anna Rumshisky. 2021. BERT busters: Out-\nlier dimensions that disrupt transformers. In Find-\nings of the Association for Computational Linguis-\n4537\ntics: ACL-IJCNLP 2021, pages 3392–3405, Online.\nAssociation for Computational Linguistics.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nZiyang Luo, Artur Kulmizev, and Xiaoxi Mao. 2021.\nPositional artefacts propagate through masked lan-\nguage model embeddings. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 5312–5327, Online. As-\nsociation for Computational Linguistics.\nMatej Martinc, Petra Kralj Novak, and Senja Pollak.\n2020. Leveraging contextual embeddings for detect-\ning diachronic semantic shift. In Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 4811–4819, Marseille, France. Euro-\npean Language Resources Association.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measur-\ning social biases in sentence encoders. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 622–628, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020. What happens to BERT em-\nbeddings during ﬁne-tuning? In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP, pages 33–44,\nOnline. Association for Computational Linguistics.\nTimothee Mickus, Denis Paperno, Mathieu Constant,\nand Kees van Deemter. 2020. What do you mean,\nBERT? In Proceedings of the Society for Computa-\ntion in Linguistics 2020, pages 279–290, New York,\nNew York. Association for Computational Linguis-\ntics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, volume 26. Curran Associates, Inc.\nAri S. Morcos, David G.T. Barrett, Neil C. Rabinowitz,\nand Matthew Botvinick. 2018. On the importance of\nsingle directions for generalization. In International\nConference on Learning Representations.\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-\ntop: Simple and effective postprocessing for word\nrepresentations. In International Conference on\nLearning Representations.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario\nAmodei, and Ilya Sutskever. 2019. Language mod-\nels are unsupervised multitask learners.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nbert. In Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\nHerbert Rubenstein and John B. Goodenough. 1965.\nContextual correlates of synonymy. Commun. ACM,\n8(10):627–633.\nTobias Schnabel, Igor Labutov, David Mimno, and\nThorsten Joachims. 2015. Evaluation methods for\nunsupervised word embeddings. In Proceedings\nof the 2015 Conference on Empirical Methods in\nNatural Language Processing, pages 298–307, Lis-\nbon, Portugal. Association for Computational Lin-\nguistics.\nThibault Sellam, Steve Yadlowsky, Jason Wei, Naomi\nSaphra, Alexander D’Amour, Tal Linzen, Jasmijn\nBastings, Iulia Turc, Jacob Eisenstein, Dipanjan Das,\nIan Tenney, and Ellie Pavlick. 2021. The multiberts:\nBert reproductions for robustness analysis.\nAina Garí Soler and Marianna Apidianaki. 2021. Let’s\nplay mono-poly: Bert can reveal words’ polysemy\nlevel and partitionability into senses.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nElena V oita, Rico Sennrich, and Ivan Titov. 2020. An-\nalyzing the source and target contributions to predic-\ntions in neural machine translation.\n4538\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Probing\npretrained language models for lexical semantics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7222–7240, Online. Association for Computa-\ntional Linguistics.\nBin Wang, Angela Wang, Fenxiao Chen, Yuncheng\nWang, and C.-C. Jay Kuo. 2019. Evaluating word\nembedding models: methods and experimental re-\nsults. APSIPA Transactions on Signal and Informa-\ntion Processing, 8.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems, volume 32. Curran\nAssociates, Inc.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nVitalii Zhelezniak, Aleksandar Savkov, April Shen, and\nNils Hammerla. 2019. Correlation coefﬁcients and\nsemantic textual similarity. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 951–962, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nA Removing Dominant Dimensions and\nRepresentational Geometry\nTo facilitate a direct comparison with anisotropy\nestimates of Ethayarajh (2019), we replicate the\nexperiments of Section 4 before and after remov-\ning the top k dimensions with the largest E[CCi].\nFor these experiments we chose k=5 dimensions\nto remove. Results for anisotropy estimates are\nshown in Figure 4. Three key takeaways from this\nanalysis are:\nAll models tested had highly anisotropic rep-\nresentations, including XLNet and RoBERTa\nwhich had not been evlauated in previous work.\nXLNet is even more anistropic than GPT-2 in its\nﬁnal two layers. RoBERTa’s word representations\nare likewise highly anisotropic, though starting in\nearlier layers than in XLNet and BERT.\nAfter removing just 5 dimensions, embed-\ndings become relatively isotropic , with ˆA( fℓ)\nnever larger than 0.25 in any layer of any model.\nAnisotropy becomes consistent across models\nand across layers, suggesting that the deviant di-\nmensions that drive anisotropy are idiosyncratic\nand model/layer speciﬁc; we show this to indeed\nbe the case in Section 4. By contrast, the geometry\nof the embedding space without rogue dimensions\nshow similar properties across models/layers, sug-\ngesting that the similar qualities of the representa-\ntional geometries of each model are obscured by\nthese rogue dimensions.\nThis can additionally be seen in our replication\nof the intra-sentence similarity and self-similarity\nfrom Ethayarajh (2019). While they ﬁnd extreme\ncases in which words of the same type are no\nmore similar to one another than randomly sam-\npled words, we ﬁnd a consistently high degree of\nself-similarity across all layers of all models after\nremoving 5 dimensions. This suggests that infor-\nmation about word identity is preserved across all\nlayers, rather than giving way to extremely con-\ntextualized representations in the ﬁnal layer, this\nconcurs with our ﬁndings in Section 5. Together,\nthese show that our conclusions about the geom-\netry of contextual embedding spaces are heavily\nskewed by the sensitivity of cosine similarity to\nrogue dimensions present in each of these models.\nB Informativity of Euclidean Distance\nIn this section, we conduct a similar analysis to\nSection 3.2 to see whether the variability in Eu-\nclidean distances between pairs of embeddings can\nbe explained by Euclidean distance with the top\nk dimensions are removed. Our methods for this\nanalysis are identical to those of Section 3.2, ex-\ncept our criterion for choosing k is the variance\nin each dimension. Results are shown in Table 3.\nIn the extreme case of XLNet, none of the vari-\nability in Euclidean distances can be explained by\nEuclidean distances when a single dimension is re-\nmoved. This means that Euclidean distance in this\nlayer is effectively a measure of a single dimension.\n4539\nFigure 4: Anisotropy by layer of the full embedding space (left) and with the top 5 dimensions removed, as\nmeasured by E[CCi] (right). In all models, anisotropy drastically decreases, and becomes more consistent across\nmodels and layers.\nFigure 5: Intra-sentence similarity by layer of the full embedding space (left) and with the top 5 dimensions\nremoved, as measured by E[CCi] (right). Intra-sentence similarity is much more consistent and monotonically\nincreasing when the top 5 dimensions are removed.\nFigure 6: Average self-similarity (similarity of the same word type across contexts) by layer of the full embedding\nspace (left) and with the top 5 dimensions removed, as measured by E[CCi] (right). In the full embedding space,\nwords of the same type in GPT-2 and XLNet appear no more similar to one another than randomly-sampled tokens.\nWhen we remove just 5 dimensions, words of the same type are indeed more similar to one another than the random\nbaseline.\n4540\nFigure 7: Relative contribution of each dimension to cosine similarity (top) paired with its relative inﬂuence on\nmodel behavior (bottom) for layers 9-11 of each model.\n4541\nFigure 8: Distribution of activations in the dimension with highest variance in layers 0-6 of each model across a\nsample of 10k tokens. Each color corresponds to a speciﬁc type/position, where the orange distribution is tokens\noccurring in position zero, the blue distribution is instances of the \".\" token, and green is all other tokens. In many\ncases, there are two clear modes in each distribution, where one corresponds to a speciﬁc word type or position.\nAdditionally, this behavior tends to persist within the same dimension number across layers, which is facilitated\nby the residual connections present in each model.\n4542\nFigure 9: Distribution of activations in the dimension with highest variance in layers 7-12 of each model across a\nsample of 10k tokens. Each color corresponds to a speciﬁc type/position, where the orange distribution is tokens\noccurring in position zero, the blue distribution is instances of the \".\" token, and green is all other tokens. In many\ncases, there are two clear modes in each distribution, where one corresponds to a speciﬁc word type or position.\nAdditionally, this behavior tends to persist within the same dimension number across layers, which is facilitated\nby the residual connections present in each model.\n4543\nFigure 10: Average correlation (Spearman’sρ) with human judgements on each word similarity dataset, with and\nwithout postprocessing for GPT-2\nFigure 11: Average correlation (Spearman’sρ) with human judgements on each word similarity dataset, with and\nwithout postprocessing for BERT\nFigure 12: Average correlation (Spearman’sρ) with human judgements on each word similarity dataset, with and\nwithout postprocessing for RoBERTa\n4544\nFigure 13: Average correlation (Spearman’sρ) with human judgements on each word similarity dataset, with and\nwithout postprocessing for XLNet\n4545\nModel Layer k=1 k=3 k=5\nGPT-2 0 0.999 0.996 0.996\n1 0.983 0.975 0.975\n2 0.999 0.783 0.783\n3 0.992 0.257 0.257\n4 0.993 0.200 0.200\n5 0.993 0.159 0.159\n6 0.993 0.090 0.090\n7 0.992 0.037 0.037\n8 0.990 0.007 0.007\n9 0.990 0.002 0.002\n10 0.986 0.022 0.022\n11 0.971 0.974 0.974\n12 0.909 0.333 0.333\nBERT 0 0.997 0.997 0.997\n1 0.994 0.993 0.993\n2 0.993 0.992 0.992\n3 0.994 0.993 0.993\n4 0.988 0.987 0.987\n5 0.992 0.991 0.991\n6 0.988 0.987 0.987\n7 0.982 0.981 0.981\n8 0.969 0.968 0.968\n9 0.925 0.924 0.924\n10 0.762 0.761 0.761\n11 0.434 0.433 0.433\n12 0.990 0.989 0.989\nRoBERTa 0 0.810 0.770 0.770\n1 0.509 0.264 0.264\n2 0.584 0.141 0.141\n3 0.607 0.152 0.152\n4 0.657 0.200 0.200\n5 0.623 0.225 0.225\n6 0.641 0.242 0.242\n7 0.614 0.241 0.241\n8 0.578 0.235 0.235\n9 0.591 0.270 0.270\n10 0.575 0.281 0.281\n11 0.591 0.319 0.319\n12 0.566 0.301 0.301\nXLNet 0 0.999 0.996 0.996\n1 1.000 1.000 1.000\n2 1.000 0.987 0.987\n3 0.993 0.992 0.992\n4 0.983 0.978 0.978\n5 0.903 0.896 0.896\n6 0.481 0.470 0.470\n7 0.432 0.426 0.426\n8 0.235 0.236 0.236\n9 0.321 0.323 0.323\n10 0.308 0.307 0.307\n11 0.124 0.150 0.150\n12 0.028 0.024 0.024\nTable 3: Proportion of variance in Euclidean distance\nr2 explained by Euclidean distance when the top k di-\nmensions (measured by the variance in each dimension)\nare removed.\nModel Layer 1 2 3 ˆA( fℓ)\nGPT-2 0 0.054 0.051 0.051 0.484\n1 0.324 0.163 0.150 0.626\n2 0.319 0.205 0.149 0.612\n3 0.294 0.264 0.145 0.589\n4 0.297 0.275 0.151 0.549\n5 0.324 0.258 0.150 0.517\n6 0.351 0.237 0.148 0.485\n7 0.374 0.205 0.144 0.466\n8 0.376 0.156 0.141 0.461\n9 0.364 0.190 0.157 0.466\n10 0.326 0.257 0.207 0.498\n11 0.275 0.269 0.265 0.640\n12 0.763 0.131 0.078 0.885\nBERT 0 0.159 0.076 0.035 0.066\n1 0.541 0.049 0.024 0.154\n2 0.790 0.006 0.005 0.224\n3 0.792 0.006 0.004 0.234\n4 0.781 0.007 0.005 0.283\n5 0.809 0.007 0.005 0.360\n6 0.792 0.005 0.004 0.382\n7 0.716 0.006 0.005 0.342\n8 0.668 0.006 0.006 0.326\n9 0.743 0.004 0.004 0.380\n10 0.817 0.004 0.003 0.396\n11 0.884 0.003 0.002 0.506\n12 0.686 0.005 0.005 0.370\nRoBERTa 0 0.726 0.040 0.021 0.143\n1 0.850 0.081 0.009 0.442\n2 0.862 0.093 0.013 0.627\n3 0.841 0.113 0.017 0.659\n4 0.796 0.146 0.023 0.666\n5 0.775 0.160 0.025 0.672\n6 0.745 0.180 0.030 0.679\n7 0.726 0.193 0.032 0.705\n8 0.674 0.229 0.038 0.690\n9 0.648 0.254 0.040 0.675\n10 0.698 0.223 0.032 0.689\n11 0.666 0.252 0.031 0.696\n12 0.663 0.262 0.020 0.745\nXLNet 0 0.300 0.043 0.028 0.037\n1 0.085 0.059 0.036 0.022\n2 0.042 0.031 0.016 0.050\n3 0.157 0.013 0.011 0.051\n4 0.413 0.017 0.009 0.169\n5 0.700 0.005 0.004 0.177\n6 0.908 0.003 0.002 0.514\n7 0.942 0.001 0.001 0.563\n8 0.982 0.000 0.000 0.826\n9 0.984 0.000 0.000 0.833\n10 0.990 0.000 0.000 0.887\n11 0.996 0.001 0.000 0.981\n12 0.973 0.003 0.002 0.884\nTable 4: Proportion of total expected cosine similar-\nity, CC( f i\nℓ)/ ˆA( fℓ), contributed by each of the top 3 di-\nmensions for all layers of each model, along with the\nanisotropy estimate ˆA( fℓ) for the given layer.\n4546\nModel Layer k=1 k=3 k=5\nGPT-2 0 0.999 0.996 0.996\n1 0.985 0.888 0.888\n2 0.990 0.899 0.899\n3 0.991 0.849 0.849\n4 0.910 0.775 0.775\n5 0.872 0.719 0.719\n6 0.853 0.684 0.684\n7 0.862 0.713 0.713\n8 0.894 0.797 0.797\n9 0.921 0.490 0.490\n10 0.947 0.428 0.428\n11 0.967 0.352 0.352\n12 0.819 0.232 0.232\nBERT 0 0.999 0.997 0.997\n1 0.894 0.848 0.848\n2 0.580 0.568 0.568\n3 0.514 0.504 0.504\n4 0.459 0.449 0.449\n5 0.383 0.374 0.374\n6 0.343 0.338 0.338\n7 0.391 0.394 0.394\n8 0.400 0.398 0.398\n9 0.219 0.220 0.220\n10 0.119 0.123 0.123\n11 0.046 0.048 0.048\n12 0.213 0.214 0.214\nRoBERTa 0 0.810 0.770 0.770\n1 0.509 0.264 0.264\n2 0.584 0.141 0.141\n3 0.607 0.152 0.152\n4 0.657 0.200 0.200\n5 0.623 0.225 0.225\n6 0.641 0.242 0.242\n7 0.614 0.241 0.241\n8 0.578 0.235 0.235\n9 0.591 0.270 0.270\n10 0.575 0.281 0.281\n11 0.591 0.319 0.319\n12 0.566 0.301 0.301\nXLNet 0 0.999 0.996 0.996\n1 1.000 1.000 1.000\n2 1.000 0.987 0.987\n3 0.993 0.992 0.992\n4 0.983 0.978 0.978\n5 0.903 0.896 0.896\n6 0.481 0.470 0.470\n7 0.432 0.426 0.426\n8 0.235 0.236 0.236\n9 0.321 0.323 0.323\n10 0.308 0.307 0.307\n11 0.124 0.150 0.150\n12 0.028 0.024 0.024\nTable 5: Proportion of variance in cosine similarity r2\nexplained by cosine similarity when the top k dimen-\nsions (measured by cosine similarity contribution) are\nremoved. Layer 0 is the static embedding layer.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7088215351104736
    },
    {
      "name": "Natural language processing",
      "score": 0.619551956653595
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5777342915534973
    },
    {
      "name": "Semantic similarity",
      "score": 0.56029212474823
    },
    {
      "name": "Transformer",
      "score": 0.5494992136955261
    },
    {
      "name": "Language model",
      "score": 0.5435935258865356
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5059892535209656
    },
    {
      "name": "Embedding",
      "score": 0.5048071146011353
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.437433660030365
    },
    {
      "name": "Euclidean distance",
      "score": 0.4221658706665039
    },
    {
      "name": "Standardization",
      "score": 0.41546154022216797
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    }
  ],
  "cited_by": 55
}