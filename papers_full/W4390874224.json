{
  "title": "Vision Transformer Adapters for Generalizable Multitask Learning",
  "url": "https://openalex.org/W4390874224",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2345151524",
      "name": "Deblina Bhattacharjee",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A1717643307",
      "name": "Sabine Süsstrunk",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A2146703035",
      "name": "Mathieu Salzmann",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2990761674",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W4312836739",
    "https://openalex.org/W6773019456",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W6796526935",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W6796830216",
    "https://openalex.org/W6745995898",
    "https://openalex.org/W6838393215",
    "https://openalex.org/W6798837711",
    "https://openalex.org/W6794345597",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2982303846",
    "https://openalex.org/W6800899058",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W3143320354",
    "https://openalex.org/W6797480140",
    "https://openalex.org/W3167456680",
    "https://openalex.org/W3035542908",
    "https://openalex.org/W4312446817",
    "https://openalex.org/W4312769131",
    "https://openalex.org/W6804608498",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W6750189243",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4303645333",
    "https://openalex.org/W6746052068",
    "https://openalex.org/W6766386996",
    "https://openalex.org/W2963877604",
    "https://openalex.org/W3208912750",
    "https://openalex.org/W125693051",
    "https://openalex.org/W2966077231",
    "https://openalex.org/W2760103357",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2431874326",
    "https://openalex.org/W3180060752",
    "https://openalex.org/W3012126539",
    "https://openalex.org/W6763070779",
    "https://openalex.org/W6758026734",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W4214488157",
    "https://openalex.org/W6789350667",
    "https://openalex.org/W4399590123",
    "https://openalex.org/W6777382314",
    "https://openalex.org/W3085046840",
    "https://openalex.org/W3097571420",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6765676075",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2798441115",
    "https://openalex.org/W4312509967",
    "https://openalex.org/W4214709605",
    "https://openalex.org/W3188511781",
    "https://openalex.org/W6798016242",
    "https://openalex.org/W6771876938",
    "https://openalex.org/W3034225195",
    "https://openalex.org/W2964185501",
    "https://openalex.org/W4312785900",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W3026615607",
    "https://openalex.org/W4287165030",
    "https://openalex.org/W4280496682",
    "https://openalex.org/W4288275617",
    "https://openalex.org/W3176153963",
    "https://openalex.org/W2963430933",
    "https://openalex.org/W3166942762",
    "https://openalex.org/W3034672970",
    "https://openalex.org/W3216272314",
    "https://openalex.org/W2964303773"
  ],
  "abstract": "We introduce the first multitasking vision transformer adapters that learn generalizable task affinities which can be applied to novel tasks and domains. Integrated into an off-the-shelf vision transformer backbone, our adapters can simultaneously solve multiple dense vision tasks in a parameter-efficient manner, unlike existing multitasking transformers that are parametrically expensive. In contrast to concurrent methods, we do not require retraining or fine-tuning whenever a new task or domain is added. We introduce a task-adapted attention mechanism within our adapter framework that combines gradient-based task similarities with attention-based ones. The learned task affinities generalize to the following settings: zero-shot task transfer, unsupervised domain adaptation, and generalization without fine-tuning to novel domains. We demonstrate that our approach outperforms not only the existing convolutional neural network-based multitasking methods but also the vision transformer-based ones. Our project page is at https://ivrl.github.io/VTAGML.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7124240398406982
    },
    {
      "name": "Transformer",
      "score": 0.6536057591438293
    },
    {
      "name": "Multi-task learning",
      "score": 0.5406830310821533
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4672061800956726
    },
    {
      "name": "Machine learning",
      "score": 0.3371206223964691
    },
    {
      "name": "Engineering",
      "score": 0.19107991456985474
    },
    {
      "name": "Electrical engineering",
      "score": 0.12270665168762207
    },
    {
      "name": "Task (project management)",
      "score": 0.09438806772232056
    },
    {
      "name": "Systems engineering",
      "score": 0.0859021544456482
    },
    {
      "name": "Voltage",
      "score": 0.06223216652870178
    }
  ]
}