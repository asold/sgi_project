{
  "title": "Morphosyntactic Tagging with Pre-trained Language Models for Arabic and its Dialects",
  "url": "https://openalex.org/W3205231356",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2059494750",
      "name": "Go Inoue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131728716",
      "name": "Salam Khalifa",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A2025107431",
      "name": "Nizar Habash",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2981875889",
    "https://openalex.org/W131663347",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W3154368324",
    "https://openalex.org/W2757376562",
    "https://openalex.org/W2147272182",
    "https://openalex.org/W2788190679",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W3133440961",
    "https://openalex.org/W4394651511",
    "https://openalex.org/W1598638450",
    "https://openalex.org/W2807107081",
    "https://openalex.org/W2950333634",
    "https://openalex.org/W4311210973",
    "https://openalex.org/W2104463314",
    "https://openalex.org/W2812315852",
    "https://openalex.org/W2747935894",
    "https://openalex.org/W2160538511",
    "https://openalex.org/W2972688845",
    "https://openalex.org/W3029612243",
    "https://openalex.org/W3032746405",
    "https://openalex.org/W2100976324",
    "https://openalex.org/W2250816155",
    "https://openalex.org/W2735552604",
    "https://openalex.org/W3155561744",
    "https://openalex.org/W3028132812",
    "https://openalex.org/W2577201715",
    "https://openalex.org/W3134155512",
    "https://openalex.org/W2740130688",
    "https://openalex.org/W2084413241",
    "https://openalex.org/W3121114355",
    "https://openalex.org/W3106433641",
    "https://openalex.org/W2048978997",
    "https://openalex.org/W2098603082",
    "https://openalex.org/W2805431570",
    "https://openalex.org/W3100160869",
    "https://openalex.org/W201141796",
    "https://openalex.org/W2250732891",
    "https://openalex.org/W2740144049",
    "https://openalex.org/W2157223933",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2117202778",
    "https://openalex.org/W3155053993",
    "https://openalex.org/W2250648475",
    "https://openalex.org/W2980282514"
  ],
  "abstract": "We present state-of-the-art results on morphosyntactic tagging across different varieties of Arabic using fine-tuned pre-trained transformer language models. Our models consistently outperform existing systems in Modern Standard Arabic and all the Arabic dialects we study, achieving 2.6% absolute improvement over the previous state-of-the-art in Modern Standard Arabic, 2.8% in Gulf, 1.6% in Egyptian, and 8.3% in Levantine. We explore different training setups for fine-tuning pre-trained transformer language models, including training data size, the use of external linguistic resources, and the use of annotated data from other dialects in a low-resource scenario. Our results show that strategic fine-tuning using datasets from other high-resource dialects is beneficial for a low-resource dialect. Additionally, we show that high-quality morphological analyzers as external linguistic resources are beneficial especially in low-resource settings.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1708 - 1719\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMorphosyntactic Tagging with Pre-trained Language Models\nfor Arabic and its Dialects\nGo Inoue, Salam Khalifa†, and Nizar Habash\nComputational Approaches to Modeling Language (CAMeL) Lab\nNew York University Abu Dhabi\n†Stony Brook University\n{go.inoue,nizar.habash}@nyu.edu\nsalam.khalifa@stonybrook.edu\nAbstract\nWe present state-of-the-art results on mor-\nphosyntactic tagging across different varieties\nof Arabic using fine-tuned pre-trained trans-\nformer language models. Our models consis-\ntently outperform existing systems in Modern\nStandard Arabic and all the Arabic dialects we\nstudy, achieving 2.6% absolute improvement\nover the previous state-of-the-art in Modern\nStandard Arabic, 2.8% in Gulf, 1.6% in Egyp-\ntian, and 8.3% in Levantine. We explore differ-\nent training setups for fine-tuning pre-trained\ntransformer language models, including train-\ning data size, the use of external linguistic re-\nsources, and the use of annotated data from\nother dialects in a low-resource scenario. Our\nresults show that strategic fine-tuning using\ndatasets from other high-resource dialects is\nbeneficial for a low-resource dialect. Addition-\nally, we show that high-quality morphological\nanalyzers as external linguistic resources are\nbeneficial especially in low-resource settings.\n1 Introduction\nFine-tuning pre-trained language models like\nBERT (Devlin et al., 2019) has achieved great\nsuccess in a wide variety of natural language\nprocessing (NLP) tasks, e.g., sentiment analy-\nsis (Abu Farha et al., 2021), question answer-\ning (Antoun et al., 2020), named entity recogni-\ntion (Ghaddar et al., 2022), and dialect identifica-\ntion (Abdelali et al., 2021). Pre-trained LMs have\nalso been used for enabling technologies such as\npart-of-speech (POS) tagging (Lan et al., 2020;\nKhalifa et al., 2021; Inoue et al., 2021) to produce\nfeatures for downstream processes. Previous POS\ntagging results using pre-trained LMs focused on\ncore POS tagsets; however, it is still not clear how\nthese models perform on the full morphosyntac-\ntic tagging task of very morphologically rich lan-\nguages, where the size of the full tagset can be in\nthe thousands. One such language is Arabic, where\nlemmas inflect to a large number of forms through\ndifferent combinations of morphological features\nand cliticization. Additionally, Arabic orthography\nomits the vast majority of its optional diacritical\nmarks which increases morphosyntactic ambiguity.\nA third challenge for Arabic is its numerous vari-\nants. Modern Standard Arabic (MSA) is the pri-\nmarily written variety used in formal settings. Di-\nalectal Arabic (DA), by contrast, is the primarily\nspoken unstandardized variant. MSA and different\nDAs, e.g., Gulf (GLF), Egyptian (EGY), and Lev-\nantine (LEV), vary in terms of their grammar and\nlexicon to the point of impeding system usability\ncross-dialectally (Habash et al., 2012). Further-\nmore, these variants currently differ in the degree\nof data availability: MSA is the highest resourced\nvariant, followed by GLF and EGY , and then LEV .\nIn this paper, we explore different training setups\nfor fine-tuning Arabic pre-trained language models\nin the complex morphosyntactic tagging task for\nfour Arabic variants (MSA, GLF, EGY , and LEV)\nunder controlled experimental settings.\nWe aim to answer the following questions:\n• How does the size of the fine-tuning data af-\nfect the performance?\n• What kind of tagset scheme is suitable for\nmodeling morphosyntactic features?\n• Is there any additional value of using external\nlinguistic resources?\n• How can we make use of annotated data in\nsome dialects to improve performance in an-\nother low-resourced dialect?\nOur system1 achieves state-of-the-art (SOTA)\nperformance in full morphosyntactic tagging ac-\ncuracy in all the variants we study, resulting in\n2.6% absolute improvement over previous SOTA\nin MSA, 2.8% in GLF, 1.6% in EGY , and 8.3% in\nLEV .\n1We make our models and data publicly available\nat https://github.com/CAMeL-Lab/CAMeLBERT_\nmorphosyntactic_tagger.\n1708\ndiac lex gloss posprc3prc2prc1prc0pergennumaspvoxmodsttcas enc0 Variant\n(a)ﺣَﻔِﯿﺪَكَHafiydakaﺣَﻔِﯿﺪHafiydgrandchildnoun- - - - - m s - - - c a 2ms_possMSA\n(b)ﺣَﻔِﯿﺪَكِHafiydakiﺣَﻔِﯿﺪHafiydgrandchildnoun- - - - - m s - - - c a 2fs_possMSA\n(c)ﺣَﻔِﯿﺪُكَHafiydukaﺣَﻔِﯿﺪHafiydgrandchildnoun- - - - - m s - - - c n 2ms_possMSA\n(d)ﺣَﻔِﯿﺪُكِHafiydukiﺣَﻔِﯿﺪHafiydgrandchildnoun- - - - - m s - - - c n 2fs_possMSA\n(e)ﺣَﻔِﯿﺪِكَHafiydikaﺣَﻔِﯿﺪHafiydgrandchildnoun- - - - - m s - - - c g 2ms_possMSA\n(f)ﺣَﻔِﯿﺪِكِHafiydikiﺣَﻔِﯿﺪHafiydgrandchildnoun- - - - - m s - - - c g 2fs_possMSA\n(g)ﺣَﻔِﯿﺪِكHafiydikﺣَﻔِﯿﺪHafiydgrandchildnoun- - - - - m s - - - c - 2ms_possGLF\n(h)ﺣَﻔِﯿﺪَكHafiydakﺣَﻔِﯿﺪHafiydgrandchildnoun- - - - - m s - - - c - 2ms_possEGY ,LEV\n(i)ﺣَﻔِﯿﺪِكHafiydikﺣَﻔِﯿﺪHafiydgrandchildnoun- - - - - m s - - - c - 2fs_possEGY ,LEV\n(j)ﺣَﻔِﯿﺪَكHafiydakﻓﺎدfAd benefitverb- - - fut 1 - s i - - - - 2ms_dobjEGY ,LEV\n(k)ﺣَﻔِﯿﺪِكHafiydikﻓﺎدfAd benefitverb- - - fut 1 - s i - - - - 2fs_dobjEGY ,LEV\nTable 1: This is an example of multiple readings of the word ¼YJ\n \t®k Hfydk in the different variants of Arabic. The\ntable also shows the full range of morphological features: part-of-speech (pos), aspect (asp), mood (mod), voice\n(vox), person (per), gender (gen), number (num), case (cas), state (stt) and clitics: proclitics (prc3, prc2, prc1,\nprc0) and enclitic (enc0). In addition to the lemma (lex), fully diacritized form (diac), and English gloss (gloss).\n2 Arabic Language and Resources\n2.1 Arabic and its Dialects\nMSA is the primarily written form of Arabic used\nin official media communications, official docu-\nments, news, and education. In contrast, the pri-\nmarily spoken varieties of Arabic are its dialects.\nArabic dialects vary among themselves and can be\ncategorized at different levels of regional classifi-\ncations (Salameh et al., 2018). They are also differ-\nent from MSA in most linguistic aspects (namely\nphonology, morphology, and syntax). Moreover,\ndialects have no official status despite being widely\nused in different means of daily communication\n– spoken as well as increasingly written on social\nmedia. In this work, we focus on MSA, Gulf Ara-\nbic (GLF), Egyptian Arabic (EGY), and Levantine\nArabic (LEV).\n2.2 Orthography\nIn this paper, we focus on Arabic written in Ara-\nbic script for MSA and DA. An important feature\nof Arabic orthography is the omission of diacriti-\ncal marks which are mostly used to indicate short\nvowels and consonantal doubling. This omission\nintroduces ambiguity to the text, e.g., the word\nI. \u0010J» ktb2 could mean ‘to write’ ( I. \u000b\u0010J\u000b» katab) or\n‘books’ (I. \f\u0010J\f» kutub) among other readings.\nUnlike MSA, Arabic dialects have no official\nstandard orthography. Depending on the writer,\nwords are sometimes spelled phonetically or closer\nto an MSA spelling through cognates or a mix of\nboth. It has been found that in extreme cases a word\n2Arabic transliteration is presented in the HSB scheme\n(Habash et al., 2007).\ncan have more than 20 different spellings (Habash\net al., 2018). This results in highly inconsistent and\nsparse datasets and models. The Conventional Or-\nthography for Dialectal Arabic (CODA) (Habash\net al., 2018) has been proposed and used in man-\nual annotations of many datasets including some\nof those used in this paper. Ideally, the process of\nmorphological disambiguation should take raw text\nas input, as this is more authentic than convention-\nalized spelling. We follow this principle for EGY\nand LEV where analyses are paired with the raw\ntext. However, the GLF dataset analyses are linked\nto the CODA version only, since orthographic con-\nventionalization was applied as an independent step\nduring manual data annotations and there are no\nsimple direct mappings between the raw text and\nthe analyses (Khalifa et al., 2018).\n2.3 Morphology\nArabic is a morphologically rich language where\na single lemma inflects to a large number of forms\nthrough different combinations of morphological\nfeatures (gender, number, person, case, state, mood,\nvoice, aspect) and cliticization (prepositions, con-\njunctions, determiners, pronominal objects, and\npossessives). As some of the morphological fea-\ntures are primarily expressed with optional diacriti-\ncal marks, orthographic ambiguity results in differ-\nent morphological analyses, e.g., MSA can have up\nto 12 analyses per word (out-of-context) on aver-\nage (Pasha et al., 2014). MSA and DA differ in the\ndegree of morphological complexity, for example,\nMSA retains nominal case and verbal mood fea-\ntures; but these are absent in DA. On the other hand,\nmany dialects take more clitics than MSA, e.g., the\n1709\nVariantResourceSizeOrthographyAnalyzer\nMSA PATB 629k Standard Manual\nGLF Gumar202k CODA Automatic\nEGY ARZTB175kSpontaneousManual\nLEV Curras 57kSpontaneousAutomatic\nTable 2: An overview of the current status of the data\nand morphological analyzers used in this work.\n\u0011+ + AÓ mA+ +š negation circumclitic structure\nfound in EGY and not MSA (Habash et al., 2012).\nTable 1 shows different possible readings for the\nword ¼YJ\n \t®k Hfydk among MSA, EGY , GLF, and\nLEV . Rows (a) to (i) are different inflections for\ncase or possessive pronouns or both of the lemma\nYJ\n \t®\u000b \u000bk Hafiyd ‘grandchild’ for all variants. Rows (j)\nand (k) show different readings that are inflections\nof the verb lemma XA \u000b\t¯ fAd ‘to benefit’, the inflec-\ntions are for different object pronouns. Note that\neven between the different POS inflections words\ncan sound and look exactly the same, this shows the\ndegree of morphological complexity and ambiguity\nin Arabic and its dialects.\n2.4 Resources\nIn this work, we use datasets that have been\nfully annotated for morphological features and\ncliticization among other lexical features such as\nlemmas. We use the Penn Arabic Treebank for\nMSA (Maamouri et al., 2004), ARZTB (Maamouri\net al., 2012) for EGY , the Gumar corpus (Khalifa\net al., 2018) for GLF, and the Curras corpus (Jarrar\net al., 2014) for LEV . We also use morphological\nanalyzers that provide out-of-context analyses for\na given word, those analyzers provide the same\nset of features that are seen in the annotated data.\nFor MSA we use the SAMA database (Graff et al.,\n2009), and for EGY we use CALIMA (Habash\net al., 2012). Both GLF and LEV do not have mor-\nphological analyzers, instead, we use automatically\ngenerated analyzers from their training data using\nparadigm completion as described in Eskander et al.\n(2013, 2016) and Khalifa et al. (2020). The quality\nand coverage of analyzers, in general, can differ\ndepending on how they were created. Manually\ncreated analyzers (MSA and EGY in this work)\ntend to have a better quality and lexical coverage\nover automatically created ones (GLF and LEV in\nthis work). The quality of automatically generated\nanalyzers is also highly dependent on the quality\nand size of the training data used to create them.\nTable 2 shows the overall state of the resources\nfor each dialect studied in this work. In terms of\nthe size of fully annotated corpora in tokens, MSA\nis approximately three times larger than GLF and\nEGY and 11 times larger than LEV . Both MSA and\nGLF have consistent orthography whereas EGY\nand LEV are more noisy. When it comes to exter-\nnal morphological analyzers, only MSA and EGY\nhave manually created and checked morphological\nanalyzers, while both GLF and LEV have analyz-\ners created automatically. This contrast of resource\navailability allows us to study how challenging the\nmorphosyntactic tagging task can be in different\nreal-world situations.\n3 Related Work\nArabic morphological modeling proved to be use-\nful in a number of downstream NLP tasks such\nas machine translation (Sadat and Habash, 2006;\nEl Kholy and Habash, 2012) speech synthesis (Ha-\nlabi, 2016), dependency parsing (Marton et al.,\n2013), sentiment analysis (Baly et al., 2017), and\ngender reinflection (Alhafni et al., 2020). We ex-\npect all of these applications and others to benefit\nfrom improvements in morphosyntactic tagging.\nThere have been multiple approaches to morpho-\nlogical modeling for Arabic. Those approaches dif-\nfer depending on the target tagset (POS vs full mor-\nphology) and the availability of linguistic resources.\nWhen it comes to MSA and DA full morphological\ntagging, MADAMIRA (Pasha et al., 2014) trained\nseparate SVM taggers for each morphological fea-\nture (including cliticization) and selected the most\nprobable answer provided by an external morpho-\nlogical analyzer all in one step for both MSA and\nEGY . AMIRA (Diab et al., 2004) on the other hand\nused a cascading approach where it performed POS\ntagging after automatically segmenting the text.\nA more recent similar approach to MADAMIRA\nwas introduced by Zalmout and Habash (2017) but\nusing a neural architecture instead. Inoue et al.\n(2017) presented a multitask neural architecture\nthat jointly models individual morphological fea-\ntures for MSA. Zalmout and Habash (2019) ex-\ntended Zalmout and Habash (2017)’s work using\nmultitask learning and adversarial training for full\nmorphological tagging in MSA and EGY . Simi-\nlarly, Zalmout and Habash (2020) proposed an\napproach where they jointly model lemmas, dia-\ncritized forms, and morphosyntactic features, pro-\nviding the current state-of-the-art in MSA. The\nsame approach was used in Khalifa et al. (2020),\n1710\nwhere they focused on the effect of the size of the\ndata and the available linguistic resources and the\nimpact on the overall performance on morphosyn-\ntactic tagging for GLF. Zalmout (2020) provides\nthe current state-of-the-art performance in LEV by\nextending Khalifa et al. (2020)’s work to LEV .\nAnother line of research that works with DA in-\ncludes Darwish et al. (2018), where they presented\na multi-dialectal CRF POS tagger, using a small set\nof 350 manually annotated tweets for each of EGY ,\nGLF, LEV , and Maghrebi Arabic (Samih et al.,\n2017). We do not evaluate on their data because\ntheir task is defined as shallow morpheme segmen-\ntation and tagging; this is quite different from, and\nnot easily mappable to, our task, where we dis-\nambiguate morphosyntactic features of the whole\nword without identifying its morpheme segments.\nAdditionally, their tagset includes social media spe-\ncific tags, such as HASH, EMOT, and MENTION,\nwhich are not in any of the large standard dataset\nand analyzers we study in this paper.\nPre-trained LM-based efforts in Arabic mor-\nphosyntactic tagging are relatively limited and ei-\nther assume gold segmentation or only produce\ncore POS tags. Kondratyuk (2019) leveraged the\nmultilingual BERT model with additional word-\nlevel and character-level LSTM layers for lemmati-\nzation and morphological tagging, assuming gold\nsegmentation. They reported the results for the SIG-\nMORPHON 2019 Shared Task (McCarthy et al.,\n2019), which includes MSA. Inoue et al. (2021) re-\nported POS tagging results in MSA, GLF, and EGY\nusing BERT models pre-trained on Arabic text with\nvarious pre-training configurations. They do not\nassume pre-segmentation of the text, however, they\nonly consider the core POS tag, rather than the fully\nspecified morphosyntactic tag. Khalifa et al. (2021)\nproposed a self-training approach for core POS\ntagging where they iteratively improve the model\nby incorporating the predicted examples into the\ntraining set used for fine-tuning.\nIn this paper, we work with full morphosyntactic\nmodeling on unsegmented text in four different\nvariants of Arabic: MSA, GLF, EGY , and LEV .\nFurthermore, we explore the behavior of the pre-\ntrained LM with respect to fine-tuning data size\nunder different training setups. Given the available\nresources, we recognize our results’ limitations in\nterms of applicability to different genres and styles,\nas well as noisy social media text and Roman script\nArabic text (Darwish, 2014).\n4 Methodology\n4.1 Morphosyntactic Tagging with\nPre-trained LMs\nTo obtain a fully specified morphosyntactic tag\nsequence, we build a classifier for each mor-\nphosyntactic feature independently, inspired by\nMADAMIRA. Unlike MADAMIRA where they\nuse an SVM classifier, we use two pre-trained LM\nbased classifiers: CAMeLBERT-Mix for DA and\nCAMeLBERT-MSA for MSA (Inoue et al., 2021).\nIn selecting these pre-trained language models, we\nconsidered the results from Inoue et al. (2021) who\nshowed that CAMeLBERT-Mix, their largest Ara-\nbic BERT model by training data size, gives the\nbest results on DA tasks. CAMeLBERT-MSA,\nwhich outperforms CAMeLBERT-Mix on MSA\ntasks, is only second to AraBERT (Antoun et al.,\n2020), but since it was created under the same set-\nting as CAMeLBERT-Mix, it minimizes experi-\nmental variations in our study.3 Following the work\nof Devlin et al. (2019), fine-tuning the CAMeL-\nBERT models is done by appending a linear layer\non top of its architecture. We use the representation\nof the first sub-token as an input to the linear layer.\n4.2 Factored and Unfactored Tagset\nOne of the challenges of the morphosyntactic tag-\nging is the large size of the full tagset due to mor-\nphological complexity of the language, where a\ncomplete single tag is a concatenation of all the\nmorphosyntactic features. For example, MSA and\nEGY data have approximately 2,000 unique com-\nplete tags in the training data, whereas GLF and\nLEV have around 1,400 and 1,000 tags, respec-\ntively. These are not the full tagsets as there are\nmany feature combinations that are not seen in the\ndata.\nMADAMIRA’s basic approach is to use a fac-\ntored feature tagset that comprises multiple tags,\neach representing a corresponding morphosyntac-\ntic category.4 This approach remedies the issue\nof the large tagset size by dividing it into multiple\nsub-tagsets of small sizes, however, it may produce\ninconsistent tag combinations.\nAlternatively, one can combine the individual\ntags into a single tag. This approach has the advan-\n3We leave engineering optimization using other pre-trained\nlanguage models to future work.\n4For example, the tagset for MSA comprises POS (34\ntags), per (4), gen (3), num (5), asp (4), vox (4), mod (5), stt\n(5), cas (5), prc3 (3), prc2 (9), prc1 (17), prc0 (7), enc0 (48).\n1711\ntage of guaranteeing the consistency of morphosyn-\ntactic feature combinations. However, it may not\nbe optimal in terms of tag coverage due to a large\nnumber of unseen tags in the test data in addition\nto the large space of classes.\nTo determine which approach is most suitable\nfor modeling, we build morphosyntactic taggers\nwith both the factored tagset and the unfactored\ntagset for each variant. Additionally, we explore\nthe effect of the training data size for both settings.\n4.3 Retagging via Morphological Analyzers\nIn previous efforts (Zalmout and Habash, 2017;\nKhalifa et al., 2020), it has been shown that lexi-\ncal resources such as morphological analyzers can\nboost the performance of morphosyntactic tagging\nthrough an in-context ranking of out-of-context an-\nswers provided by the analyzer.\nIn this work, we follow their approach, where we\nuse the morphological analyzers as a later step after\ntagging with the fine-tuned pre-trained model. We\nuse the analyzers described in Section 2.4 to pro-\nvide out-of-context analyses. For each word, the\nanalyzer may provide more than one answer.5 The\nanalyses are then ranked based on the unweighted\nsum of successful matches between the values of\nthe predictions from the individual taggers and\nthose provided by the analyzer. To break ties dur-\ning the ranking, we take the weighted sum of the\nprobability of the unfactored feature tag and the\nproduct of the probabilities of all the individual\ntags as follows:\n1\n2P(tunfactored ) +1\n2\nY\nm∈M\nP(tm) (1)\nwhere t is the tag for the feature m and M is the\nset of morphosyntactic features. The probabilities\nare obtained through unigram models based on the\nrespective training data split.\n4.4 Merged and Continued Training\nMorphosyntactic modeling for DA is especially\nchallenging because of data scarcity. Among the\ndatasets that we use, LEV is the least resourced\nvariant, having 11 times less training data than\nMSA. Therefore, we want to investigate an opti-\nmal approach to utilize data from other variants to\n5Both the MSA and EGY analyzers provide backoff modes.\nWe use the recommended setting by Zalmout and Habash\n(2017). For GLF and LEV analyzers we keep the original\npredictions if no answer is returned.\nSplit MSAGLFEGYLEV\nTRAIN478k154k127k 43k\nTUNE 26k 8k 7k 2k\nDEV 63k 20k 21k 6k\nTEST 63k 20k 20k 6k\nALL 629k202k175k 57k\nTable 3: Statistics on TRAIN, TUNE, DEV , and TEST\nfor each variant in terms of number of words.\nimprove upon the performance of morphosyntactic\ntagging for LEV .\nIn this work, we experiment with the follow-\ning two settings: (a) we merge all the datasets\ntogether and fine-tune a pre-trained LM on the\nmerged datasets in a single step; and (b) similar to\nZalmout (2020), we start fine-tuning a pre-trained\nLM on a mix of high-resource datasets (MSA, GLF,\nand EGY), and then continue fine-tuning on a low-\nresource dataset (LEV).\n5 Experiments\n5.1 Experimental Settings\nData To be able to compare with previous SOTA\n(Zalmout and Habash, 2020, 2019; Khalifa et al.,\n2020; Zalmout, 2020), we follow the same con-\nventions they used for data splits: MSA and EGY\n(Diab et al., 2013), GLF (Khalifa et al., 2018), and\nLEV (Eskander et al., 2016). In Table 3, we show\nthe statistics of our datasets.\nFine-tuning We fine-tuned the CAMeLBERT\nmodels (Inoue et al., 2021) on each morphosyn-\ntactic tagging task. Following their recommenda-\ntion, we used CAMeLBERT-MSA for MSA and\nCAMeLBERT-Mix for the dialects. We used Hug-\nging Face’s transformers (Wolf et al., 2020) for\nimplementation. We trained our models for 10\nepochs with a learning rate of 5e-5, a batch size of\n32, and a maximum sequence length of 512. We\npick the best checkpoint based on TUNE and report\nresults on DEV and TEST from a single run.\nLearning Curve To investigate the effect of fine-\ntuning data sizes, we randomly sample training\nexamples on a scale of 5k, 10k, 20k, 40k, 80k,\n120k, and 150k tokens. We use 150k, 120k, and\n40k since they are comparable to the number of\ntokens in GLF, EGY , and LEV datasets, respec-\ntively. This allows us to measure the performance\ndifference across different dialects in a controlled\nmanner. This also gives us insight into the amount\n1712\nALL TAGS POS OrthoMorph\n5k 10k 20k 40k 80k 120k150k480k 5k 10k 20k 40k 80k 120k150k480k\nMSAUnfactored43.2 65.5 79.2 88.1 91.6 93.3 93.9 95.5 80.1 90.5 94.1 96.9 97.7 98.0 98.1 98.5\nConsistent\nManual\n +Morph63.4 77.6 85.4 91.3 93.3 94.4 94.8 95.9 81.6 91.6 95.1 97.4 98.1 98.3 98.5 98.7\nFactored 75.3 86.1 90.8 93.0 94.1 94.7 94.9 95.5 93.0 96.4 97.6 98.1 98.3 98.3 98.4 98.6\n +Morph86.5 91.3 93.6 94.7 95.2 95.5 95.7 96.1 95.1 97.1 98.0 98.5 98.6 98.6 98.7 98.8\nGLFUnfactored75.1 81.0 89.6 93.3 94.8 95.3 95.8 90.3 92.6 95.6 96.8 97.2 97.7 97.8\nConsistent\nAuto +Morph86.4 87.1 90.7 92.3 93.1 93.4 93.8 93.9 94.1 95.5 96.1 96.4 96.7 96.6\nFactored 87.1 89.8 92.4 94.0 94.7 95.1 95.5 94.6 95.5 96.6 97.1 97.5 97.9 98.0\n +Morph90.8 90.6 92.1 92.9 93.4 93.8 93.9 95.4 95.5 96.0 96.3 96.6 96.8 96.8\nEGYUnfactored64.6 77.3 83.0 86.1 87.7 88.8 84.0 87.8 90.5 92.0 92.7 93.0\nSpontaneous\nManual\n +Morph76.4 83.8 87.4 89.2 89.9 90.5 81.9 87.9 91.5 93.1 93.7 94.0\nFactored 77.1 82.0 84.1 85.7 86.8 87.4 89.9 91.0 92.0 92.6 92.9 93.2\n +Morph86.3 88.3 89.2 89.8 90.3 90.6 90.9 92.6 93.4 93.7 94.0 94.1\nLEVUnfactored73.6 80.8 85.0 88.1 86.7 91.0 93.1 94.5\nSpontaneous\nAuto +Morph77.0 80.7 83.2 85.5 87.3 89.8 91.6 92.7\nFactored 80.6 84.6 86.6 88.9 91.4 93.2 94.1 94.7\n +Morph81.2 83.5 84.8 86.4 90.0 91.3 92.2 93.0\nTable 4: DEV results on a learning curve of the training data size. Morph refers to the model with an additional step\nof retagging using a morphological analyzer. We bold the best score for each variant. Underlined scores denote\nthat the differences between those scores and the best scores are statistically insignificant with McNemar’s test\n(p <0.05).\nof annotated data required to achieve a certain per-\nformance, which is useful when creating annotated\nresources for new dialects. We use this setup in all\nthe reported experiments.\nPre-processing for Merged and Continued\nTraining Although the different datasets provide\nthe same set of morphosyntactic features, there\nexist some inconsistencies between them. The\ndatasets were annotated by different groups using\nslightly different annotation guidelines, therefore,\nwe need to bring all the feature values into a com-\nmon space with LEV . We performed the following\nsteps to address those inconsistencies: (a) we drop\nthe stt, cas, mod, vox, enc1, and enc2 features; (b)\nwe remove the diactization from the lexical parts of\nthe proclitic features, e.g., the conjunction +ð w+\nrealized as wa_conj in MSA and wi_conj in EGY\nboth maps to w_conj in LEV; and (c) for certain\nPOS classes some features have default values in\ncase they are not present, those default values were\ndifferent for different datasets. Thus, we mapped\nthose default values to match whatever was spec-\nified as default in LEV . We only performed these\nmodifications for the experiments on merged and\ncontinued training.\nEvaluation Metrics We compute the accuracy\nin terms of the core POS and the combined mor-\nphosyntactic features (ALL TAGS). For MSA, we\nuse 14 features, which are pos, per, gen, num, asp,\nvox, mod, stt, cas, prc3, prc2, prc1, prc0, and enc0.\nFor dialects, we use 16 features, where we include\nenc1 and enc2 in addition to the 14 features used in\nMSA. In the merged and continued training setup,\nwe use a reduced set of 10 features, pos, per, gen,\nnum, asp, prc3, prc2, prc1, prc0, and enc0, which\nwe refer to as ALL TAGS 10.\n5.2 Results\nFactored vs Unfactored ModelsTable 4 shows\nthe DEV results for the models trained with the fac-\ntored and unfactored tagset (henceforth, factored\nand unfactored models, respectively) on a learning\ncurve of the training data size. In the extremely low-\nresource setting of 5k tokens in the ALL TAGS\nmetric, we observe that factored models consis-\ntently outperform unfactored models across all the\nvariants (15.9% absolute increase on average). In\nparticular, MSA benefited most with a 32.1% ab-\nsolute increase, followed by EGY (12.5%), GLF\n(12.0%), and LEV (7.1%).\nHowever, this gap shrinks as the data size in-\ncreases. For instance in MSA, the differences be-\ntween the scores of the factored model and the un-\nfactored model become statistically insignificant by\nMcNemar’s test (McNemar, 1947) with p <0.05\nwhen trained on the full data. This is presumably\ndue to the decrease in the number of unseen unfac-\ntored tags in DEV . In fact, 3.9% of the unfactored\n1713\ntags in DEV are not seen in TRAIN in the 5k set-\nting, whereas only 0.1% of tags are unseen in DEV\nwhen we use the full data.\nThe factored model performs better than the un-\nfactored model across all the data sizes in MSA and\nLEV . The EGY and GLF models follow a similar\npattern in the low resourced settings, however, the\nunfactored models begin to perform better than the\nfactored ones from 20k for EGY and 40k for GLF.\nOur results suggest that the factored tagset is opti-\nmal compared to the unfactored tagset, especially\nin low-resource settings.\nRetagging with Morphological Analyzer We\nobserve that the use of a morphological analyzer\nconsistently improves the performance of both un-\nfactored and factored models across all the differ-\nent training data sizes in MSA and EGY in ALL\nTAGS. The value of a morphological analyzer is es-\npecially apparent in the very low resourced setting\n(5k), with an increase of 20.2% (MSA) and 11.8%\n(EGY) in the unfactored model and 11.2% (MSA)\nand 9.2% (EGY) in the factored model. However,\nthe effect of retagging with a morphological an-\nalyzer diminishes as the data size increases, yet\nproviding a performance gain of 0.4% in the un-\nfactored model with the analyzer and 0.5% in its\nfactored counterpart in the high resourced setting\nin MSA.\nSimilarly, we observe an increase in performance\nwhen we include a morphological analyzer in the\nvery low-resourced settings in GLF and LEV . How-\never, as we increase the training data size, the use\nof a morphological analyzer starts to hurt the per-\nformance at 40k in GLF and 10k in LEV in the\nunfactored model and 20k in GLF and 10k in LEV\nin the factored model. We observe here that the\nquality of the analyzer has direct implications on\nthe performance. The analyzers used for MSA and\nEGY are of higher quality since they were manu-\nally created and checked, whereas GLF and LEV\nanalyzers are impacted by the quality and size of\nthe annotated data used to create them. This is also\nconsistent with the findings of Khalifa et al. (2020).\nComparison with Previous SOTA SystemsTa-\nble 5 shows DEV and TEST results for our mod-\nels and a number of previously published state-of-\nthe-art morphosyntactic tagging systems. For our\nmodels, we use the best systems in terms of ALL\nTAGS metric, namely, the factored model with a\nmorphological analyzer for MSA and EGY , the un-\nfactored model for GLF, and the factored model\nfor LEV . For existing models, we report the best\nresults from Zalmout and Habash (2020) (ZH’20)\nfor MSA, Khalifa et al. (2020) (K’20) for GLF,\nZalmout and Habash (2019) (ZH’19) for EGY , and\nZalmout (2020) (Z’20) for LEV .\nSince some of these systems do not report on\nall of the features that we report on, but rather on\ndifferent subsets of them, we include in the table\nour results when matched with their features (ALL\nTAGS* in Table 5). There is no difference for\nMSA; however the ALL TAGS* setting for EGY\nand LEV excludes enc1 and enc2. As for GLF,\nALL TAGS* consists of only 10 features: pos,\nasp, per, gen, num, prc0, prc1, prc2, prc3, and\nenc0.\nWe observe that our models consistently out-\nperform the existing systems in all variants. Our\nmodel achieves 2.6% absolute improvement over\nthe state-of-the-art system in MSA, 2.8% in GLF,\n1.6% in EGY , and 8.3% in LEV .\nMerged and Continued TrainingTable 6 shows\nthe results on LEV for the merged and the contin-\nued training setups. We use the factored model\nwithout the analyzer as it was the best setup in\nthe experiments presented so far. The results for\nmerged training are consistently below those for\nthe baseline across different data sizes, even though\nthey have access to more data. This is most likely\na result of the disproportionately small size of the\nLEV dataset when compared to the other variants.\nIn contrast, the results for continued training\nshow consistent improvements over the LEV-only\nbaseline model. Continued training provides a sub-\nstantial increase in performance, especially in the\nvery low resourced setting with only 5k tokens, giv-\ning 3.6% absolute improvement over the baseline\non the DEV set. Our results show that continued\ntraining from the model trained on high-resourced\ndialects is very beneficial with lower amounts of\ntraining data. These results are not directly compa-\nrable to the previous SOTA because of the different\ntraining data and metric used.\n5.3 Error Analysis\nOOV To better understand the effect of different\ntraining setups, we examine the performance of our\nmodels on out-of-vocabulary (OOV) tokens alone.\nHere, we observe a stronger and more consistent\npattern. The average difference between the best\nmodel and the weakest model in ALL TAGS across\n1714\nDEV TEST\nMSA GLF EGY LEV MSA GLF EGY LEV\nOursZH'20OursK'20 OursZH'19Ours Z'20 OursOursK'20 OursZH'19Ours\nPOS 98.8 98.1 97.8 96.8 94.2 93.3 94.7 89.4 98.9 97.9 96.9 94.6 93.8 94.0\nALL TAGS96.1 93.5 95.8 - 90.6 - 88.9 - 96.3 95.7 - 91.0 - 87.6\nALL TAGS*96.1 93.5 95.8 93.3 90.7 89.3 89.1 80.8 96.3 95.7 92.9 91.0 89.4 87.8\nTable 5: DEV and TEST results of our systems and previously published systems on the same datasets.\nDEV TEST\nALL TAGS 10 POS ALL TAGS 10 POS\n5k 10k 20k 40k 5k 10k 20k 40k 5k 10k 20k 40k 5k 10k 20k 40k\nSINGLE 81.5 85.4 87.4 89.2 91.4 93.2 94.194.7 79.3 84.0 86.2 88.0 89.9 91.8 92.994.0\nMERGED77.9 80.6 82.7 85.0 87.3 89.4 90.992.3 77.1 79.8 82.0 84.6 87.6 89.3 90.391.9\nCONTINUED85.1 86.9 88.2 89.5 92.0 93.3 94.294.8 84.3 85.8 87.4 88.8 91.8 92.6 93.694.2\nTable 6: DEV and TEST results on LEV for the merged training setup (MERGED) and the continued training setup\n(CONTINUED). SINGLE refers to the model trained only on LEV .\nvariants is larger in OOV tokens (6.7%) than in\nall tokens (2.3%). On OOV tokens, the factored\nmodel with a morphological analyzer consistently\nperforms best in all the data sizes for all the variants\nexcept for LEV . In LEV , however, the same model\nwithout the morphological analyzer outperforms\nthe one with the analyzer. This is presumably due\nto the orthographic inconsistency in the data along\nwith the quality of the morphological analyzer as\ndiscussed in Section 2.4.\nError Statistics Table 7 presents the number and\npercentage of specific feature errors among the\nALL TAGS errors in the best systems on the DEV\nset. On average, there are two feature prediction\nfailures within an unfactored tag across the dif-\nferent variants. We observe that MSA and DA\nexhibit different error patterns: In MSA, case is\nthe largest error contributor among other features,\nwhich is consistent with the previous findings along\nthe line (Zalmout and Habash, 2020), whereas in\ndialects, POS is the largest error contributor.\nAmong the POS errors, the most common error\ntype is mislabeling a nominal tag with a different\nnominal tag, at 44.2% of the errors in GLF, 67.3%\nin EGY , and 57.8% in LEV , while this type of error\nis more dominant in MSA (80.8%). Mislabeling\nnominals with verbs is more common in DA at\n23.1% in GLF, 13.0% in EGY , and 20.1% in LEV ,\ncompared to MSA (7.7%).\nThe core morphological features such asper, gen,\nnum, and asp have a higher percentage of errors in\nDA than in MSA. Another noticeable difference is\nenc0 feature (MSA ∼2% vs DA on average∼17%).\nThis is likely due to label distribution differences\nin pronominal enclitics: MSA has a highly skewed\ndistribution with 90%, 1%, and 9% ratio for 3rd,\n2nd and 1st persons as expected in MSA news\ngenre. In comparison, DA has less skew with 50%,\n17%, and 32% respectively, which increases the\nlikelihood of error.\nAmong the three dialects, we observe similar\npatterns in terms of feature error contribution, es-\npecially for GLF and LEV with a correlation coef-\nficient of 0.93. However, in EGY specifically, we\nobserve a high percentage of errors in mod, vox, stt,\nand cas, partly due to the difference and inconsis-\ntency in annotation schemes.\nWe also found some gold errors which affect all\nof the systems we compared (previous SOTA and\nours). For example, there are cases where genitive\ndiptotes are annotated as accusative,6 e.g., the word\tà@QK\n@\r ˇAyrAn ‘Iran’ in the context \tà@QK\n@\r ú\n\t¯ fy ˇAyrAn\n‘in Iran’. As the results on Arabic morphosyntactic\ndisambiguation are reaching new heights, it may\nbe useful for the community using these resources\nto revisit their annotations.\n6 Conclusion and Future Work\nIn this paper, we presented the state-of-the-art re-\nsults in the morphosyntactic tagging task for Mod-\nern Standard Arabic and three Arabic dialects that\ndiffer in terms of linguistic properties and resource\navailability. We conducted different experiments to\nexamine the performance of pre-trained LMs under\ndifferent fine-tuning setups. We showed that the\nfactored model outperforms the unfactored model\n6For more information on Arabic morphology in a compu-\ntational context, see Habash (2010).\n1715\nALL TAGS \nError Rate\n# Error \nFeatures\nFeature Contribution to ALL TAGS Error Rate\npos per gennumaspmodvox stt casprc0prc1prc2prc3enc0enc1enc2\nMSA 3.9 1.5 31.1 4.2 5.1 3.5 3.2 4.9 5.121.964.1 4.0 2.3 2.2 0.7 2.2 - -\nGLF 4.2 2.0 51.733.938.014.319.7 0.8 0.8 0.8 0.8 1.3 5.910.7 0.819.5 0.8 0.8\nEGY 9.4 2.4 62.214.615.914.011.017.411.320.021.5 9.211.3 8.9 2.112.9 2.3 2.3\nLEV 11.1 1.9 47.619.822.915.312.7 0.5 9.6 1.4 1.9 8.2 8.5 6.8 2.218.7 5.7 3.7\nTable 7: The number and percentage of specific feature errors among the ALL TAGS errors in the best systems on\nthe DEV set.\nin low-resource settings; however, this gap dimin-\nishes as the data size increases. Additionally, high-\nquality morphological analyzers proved to be help-\nful, especially in low-resource settings. Our results\nalso show that fine-tuning using datasets from other\ndialects followed by fine-tuning using the target di-\nalect is beneficial for low-resource settings. Our\nsystems outperform previously published SOTA on\nthis task.\nIn the future, we plan to investigate continued\ntraining further and find other ways where we can\nutilize resources and datasets for low-resourced di-\nalects. We also intend to explore other architectures\nfor morphosyntactic tagging using multi-task learn-\ning in the context of pre-trained LMs, as well as\nwork on the task of automatic lemmatization. We\nalso plan to integrate some of our best models as\npart of the Python open-source Arabic NLP toolkit\nCAMeL Tools (Obeid et al., 2020).\n7 Ethical Considerations\nThe experiments reported in this work rely on pre-\nviously published datasets described in Section 2.4.\nWe used the CAMeLBERT models along with mor-\nphosyntactically annotated datasets to build our\nmorphosyntactic taggers, which is in line with their\nintended use. Our work is on core and generic NLP\ntechnologies that can be potentially used with mali-\ncious intent, for example, as part of the pipeline. To\nensure reproducibility, we make our code publicly\navailable. The details on the datasets and train-\ning are described in Appendix A. Given the focus\nof this paper and the available resources, we rec-\nognize the limitations of our findings in terms of\napplicability to different genres, styles, and other\nlanguages.\nAcknowledgment\nThis work was carried out on the High Performance\nComputing resources at New York University Abu\nDhabi. We thank anonymous reviewers for their\ninsightful suggestions and comments. We thank\nBashar Alhafni and Ossama Obeid for their assis-\ntance with the codebase and the helpful discus-\nsions.\nReferences\nAhmed Abdelali, Sabit Hassan, Hamdy Mubarak, Ka-\nreem Darwish, and Younes Samih. 2021. Pre-training\nbert on arabic tweets: Practical considerations.\nIbrahim Abu Farha, Wajdi Zaghouani, and Walid Magdy.\n2021. Overview of the WANLP 2021 shared task\non sarcasm and sentiment detection in Arabic. In\nProceedings of the Sixth Arabic Natural Language\nProcessing Workshop, pages 296–305, Kyiv, Ukraine\n(Virtual). Association for Computational Linguistics.\nBashar Alhafni, Nizar Habash, and Houda Bouamor.\n2020. Gender-aware reinflection using linguistically\nenhanced neural models. In Proceedings of the Sec-\nond Workshop on Gender Bias in Natural Language\nProcessing, pages 139–150, Barcelona, Spain (On-\nline). Association for Computational Linguistics.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th Work-\nshop on Open-Source Arabic Corpora and Process-\ning Tools, with a Shared Task on Offensive Language\nDetection, pages 9–15, Marseille, France. European\nLanguage Resource Association.\nRamy Baly, Hazem Hajj, Nizar Habash, Khaled Bashir\nShaban, and Wassim El-Hajj. 2017. A sentiment tree-\nbank and morphologically enriched recursive deep\nmodels for effective sentiment analysis in Arabic.\nACM Transactions on Asian and Low-Resource Lan-\nguage Information Processing (TALLIP), 16(4):23.\nKareem Darwish. 2014. Arabizi Detection and Con-\nversion to Arabic. In Proceedings of the Workshop\nfor Arabic Natural Language Processing (WANLP),\npages 217–224, Doha, Qatar.\nKareem Darwish, Hamdy Mubarak, Ahmed Abdelali,\nMohamed Eldesouki, Younes Samih, Randah Al-\nharbi, Mohammed Attia, Walid Magdy, and Laura\nKallmeyer. 2018. Multi-dialect Arabic pos tag-\nging: A CRF approach. In Proceedings of the Lan-\nguage Resources and Evaluation Conference (LREC),\nMiyazaki, Japan.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\n1716\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMona Diab, Nizar Habash, Owen Rambow, and Ryan\nRoth. 2013. LDC Arabic treebanks and associated\ncorpora: Data divisions manual. arXiv preprint\narXiv:1309.5652.\nMona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.\nAutomatic Tagging of Arabic Text: From Raw Text\nto Base Phrase Chunks. In Proceedings of the Con-\nference of the North American Chapter of the Associ-\nation for Computational Linguistics (NAACL), pages\n149–152, Boston, MA.\nAhmed El Kholy and Nizar Habash. 2012. Ortho-\ngraphic and morphological processing for English–\nArabic statistical machine translation. Machine\nTranslation, 26(1-2):25–45.\nRamy Eskander, Nizar Habash, and Owen Rambow.\n2013. Automatic extraction of morphological lex-\nicons from morphologically annotated corpora. In\nProceedings of the Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n1032–1043, Seattle, Washington, USA.\nRamy Eskander, Nizar Habash, Owen Rambow, and\nArfath Pasha. 2016. Creating resources for Dialectal\nArabic from a single annotation: A case study on\nEgyptian and Levantine. In Proceedings of the Inter-\nnational Conference on Computational Linguistics\n(COLING), pages 3455–3465, Osaka, Japan.\nAbbas Ghaddar, Yimeng Wu, Ahmad Rashid, Khalil\nBibi, Mehdi Rezagholizadeh, Chao Xing, Yasheng\nWang, Duan Xinyu, Zhefeng Wang, Baoxing Huai,\nXin Jiang, Qun Liu, and Philippe Langlais. 2022.\nJaber and saber: Junior and senior arabic bert.\nDavid Graff, Mohamed Maamouri, Basma Bouziri,\nSondos Krouna, Seth Kulick, and Tim Buckwal-\nter. 2009. Standard Arabic Morphological Analyzer\n(SAMA) Version 3.1. Linguistic Data Consortium\nLDC2009E73.\nNizar Habash, Fadhl Eryani, Salam Khalifa, Owen\nRambow, Dana Abdulrahim, Alexander Erdmann,\nReem Faraj, Wajdi Zaghouani, Houda Bouamor,\nNasser Zalmout, Sara Hassan, Faisal Al shargi,\nSakhar Alkhereyf, Basma Abdulkareem, Ramy Es-\nkander, Mohammad Salameh, and Hind Saddiki.\n2018. Unified guidelines and resources for Ara-\nbic dialect orthography. In Proceedings of the Lan-\nguage Resources and Evaluation Conference (LREC),\nMiyazaki, Japan.\nNizar Habash, Ramy Eskander, and Abdelati Hawwari.\n2012. A Morphological Analyzer for Egyptian Ara-\nbic. In Proceedings of the Workshop of the Special\nInterest Group on Computational Morphology and\nPhonology (SIGMORPHON), pages 1–9, Montréal,\nCanada.\nNizar Habash, Abdelhadi Soudi, and Tim Buckwalter.\n2007. On Arabic Transliteration. In A. van den\nBosch and A. Soudi, editors, Arabic Computational\nMorphology: Knowledge-based and Empirical Meth-\nods, pages 15–22. Springer, Netherlands.\nNizar Y Habash. 2010. Introduction to Arabic natural\nlanguage processing. Morgan & Claypool Publish-\ners.\nNawar Halabi. 2016. Modern standard Arabic phonet-\nics for speech synthesis. Ph.D. thesis, University of\nSouthampton.\nGo Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda\nBouamor, and Nizar Habash. 2021. The interplay\nof variant, size, and task type in Arabic pre-trained\nlanguage models. In Proceedings of the Sixth Arabic\nNatural Language Processing Workshop, pages 92–\n104, Kyiv, Ukraine (Virtual). Association for Compu-\ntational Linguistics.\nGo Inoue, Hiroyuki Shindo, and Yuji Matsumoto. 2017.\nJoint Prediction of Morphosyntactic Categories for\nFine-Grained Arabic Part-of-Speech Tagging Exploit-\ning Tag Dictionary Information. In Proceedings\nof the Conference on Computational Natural Lan-\nguage Learning (CoNLL), pages 421–431, Vancou-\nver, Canada.\nMustafa Jarrar, Nizar Habash, Diyam Akra, and Nasser\nZalmout. 2014. Building a Corpus for Palestinian\nArabic: A Preliminary Study. In Proceedings of the\nWorkshop for Arabic Natural Language Processing\n(WANLP), pages 18–27, Doha, Qatar.\nMuhammad Khalifa, Muhammad Abdul-Mageed, and\nKhaled Shaalan. 2021. Self-training pre-trained lan-\nguage models for zero- and few-shot multi-dialectal\nArabic sequence labeling. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 769–782, Online. Association for Computa-\ntional Linguistics.\nSalam Khalifa, Nizar Habash, Fadhl Eryani, Ossama\nObeid, Dana Abdulrahim, and Meera Al Kaabi. 2018.\nA morphologically annotated corpus of emirati Ara-\nbic. In Proceedings of the Language Resources and\nEvaluation Conference (LREC), Miyazaki, Japan.\nSalam Khalifa, Nasser Zalmout, and Nizar Habash.\n2020. Morphological analysis and disambiguation\nfor Gulf Arabic: The interplay between resources\nand methods. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 3895–\n3904, Marseille, France. European Language Re-\nsources Association.\nDan Kondratyuk. 2019. Cross-lingual lemmatization\nand morphology tagging with two-stage multilin-\ngual BERT fine-tuning. In Proceedings of the 16th\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology, pages 12–18, Florence,\nItaly. Association for Computational Linguistics.\nWuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. 2020.\nAn empirical study of pre-trained transformers for\nArabic information extraction. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\n1717\nLanguage Processing (EMNLP), pages 4727–4734,\nOnline. Association for Computational Linguistics.\nMohamed Maamouri, Ann Bies, Tim Buckwalter, and\nWigdan Mekki. 2004. The Penn Arabic Treebank:\nBuilding a Large-Scale Annotated Arabic Corpus.\nIn Proceedings of the International Conference on\nArabic Language Resources and Tools, pages 102–\n109, Cairo, Egypt.\nMohamed Maamouri, Ann Bies, Seth Kulick, Dalila\nTabessi, and Sondos Krouna. 2012. Egyptian Arabic\nTreebank DF Parts 1-8 V2.0 - LDC catalog num-\nbers LDC2012E93, LDC2012E98, LDC2012E89,\nLDC2012E99, LDC2012E107, LDC2012E125,\nLDC2013E12, LDC2013E21.\nYuval Marton, Nizar Habash, and Owen Rambow. 2013.\nDependency parsing of modern standard Arabic with\nlexical and inflectional features. Computational Lin-\nguistics, 39(1):161–194.\nArya D. McCarthy, Ekaterina Vylomova, Shijie Wu,\nChaitanya Malaviya, Lawrence Wolf-Sonkin, Garrett\nNicolai, Christo Kirov, Miikka Silfverberg, Sabrina J.\nMielke, Jeffrey Heinz, Ryan Cotterell, and Mans\nHulden. 2019. The SIGMORPHON 2019 shared\ntask: Morphological analysis in context and cross-\nlingual transfer for inflection. In Proceedings of the\n16th Workshop on Computational Research in Pho-\nnetics, Phonology, and Morphology, pages 229–244,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nQuinn McNemar. 1947. Note on the sampling error\nof the difference between correlated proportions or\npercentages. Psychometrika, 12(2):153–157.\nOssama Obeid, Nasser Zalmout, Salam Khalifa, Dima\nTaji, Mai Oudah, Bashar Alhafni, Go Inoue, Fadhl\nEryani, Alexander Erdmann, and Nizar Habash. 2020.\nCAMeL tools: An open source python toolkit for Ara-\nbic natural language processing. In Proceedings of\nthe 12th Language Resources and Evaluation Confer-\nence, pages 7022–7032, Marseille, France. European\nLanguage Resources Association.\nArfath Pasha, Mohamed Al-Badrashiny, Mona Diab,\nAhmed El Kholy, Ramy Eskander, Nizar Habash,\nManoj Pooleery, Owen Rambow, and Ryan Roth.\n2014. MADAMIRA: A fast, comprehensive tool for\nmorphological analysis and disambiguation of Ara-\nbic. In Proceedings of the Language Resources and\nEvaluation Conference (LREC), pages 1094–1101,\nReykjavik, Iceland.\nFatiha Sadat and Nizar Habash. 2006. Combination of\nArabic preprocessing schemes for statistical machine\ntranslation. In Proceedings of the International Con-\nference on Computational Linguistics and the Confer-\nence of the Association for Computational Linguistics\n(COLING-ACL), pages 1–8, Sydney, Australia.\nMohammad Salameh, Houda Bouamor, and Nizar\nHabash. 2018. Fine-grained Arabic dialect identi-\nfication. In Proceedings of the International Confer-\nence on Computational Linguistics (COLING), pages\n1332–1344, Santa Fe, New Mexico, USA.\nYounes Samih, Mohamed Eldesouki, Mohammed Attia,\nKareem Darwish, Ahmed Abdelali, Hamdy Mubarak,\nand Laura Kallmeyer. 2017. Learning from rela-\ntives: Unified dialectal Arabic segmentation. In Pro-\nceedings of the 21st Conference on Computational\nNatural Language Learning (CoNLL 2017) , pages\n432–441, Vancouver, Canada. Association for Com-\nputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020. Hug-\ngingface’s transformers: State-of-the-art natural lan-\nguage processing.\nNasser Zalmout. 2020. Morphological Tagging and Dis-\nambiguation in Dialectal Arabic Using Deep Learn-\ning Architectures. Ph.D. thesis, New York University.\nNasser Zalmout and Nizar Habash. 2017. Don’t throw\nthose morphological analyzers away just yet: Neural\nmorphological disambiguation for Arabic. In Pro-\nceedings of the Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n704–713, Copenhagen, Denmark.\nNasser Zalmout and Nizar Habash. 2019. Adversarial\nmultitask learning for joint multi-feature and multi-\ndialect morphological modeling. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1775–1786, Florence,\nItaly. Association for Computational Linguistics.\nNasser Zalmout and Nizar Habash. 2020. Joint dia-\ncritization, lemmatization, normalization, and fine-\ngrained morphological tagging. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 8297–8307, Online. As-\nsociation for Computational Linguistics.\nA Replicability\nA.1 Resources\nPretrained transfromer models We fine-tuned\nCAMeLBERT-MSA for the morphosyntactic tag-\nging task in MSA and CAMeLBERT-Mix (Inoue\net al., 2021) for EGY , GLF, and LEV .\nFine-tuning Data We used the Penn Arabic\nTreebank for MSA (Maamouri et al., 2004),\nARZTB (Maamouri et al., 2012) for EGY , the Gu-\nmar corpus (Khalifa et al., 2018) for GLF, and the\nCurras corpus (Jarrar et al., 2014) for LEV . The\npreprocessing of the data includes fixing inconsis-\ntent annotations and removing diacritics through\nCAMeL Tools (Obeid et al., 2020). This prepro-\ncessing was followed in all the previous work we\ncompared with Zalmout and Habash (2019, 2020);\nKhalifa et al. (2020); Zalmout (2020).\n1718\nData Sampling For the learning curve experi-\nment in Section 5.1, we sampled the training data\nup to 5k, 20k, 40k, 80k, 120k, 150k tokens after\nshuffling the entire dataset. Each sample after 5k is\ninclusive of the smaller samples.\nMorphological Analyzers The morphological\nanalyzers used in our experiments are the following:\nFor MSA we use the SAMA database (Graff et al.,\n2009), and for EGY we use CALIMA (Habash\net al., 2012). For GLF and LEV , we use automati-\ncally generated analyzers from their training data\nusing paradigm completion as described in Eskan-\nder et al. (2013, 2016) and Khalifa et al. (2020).\nData Accessibility MSA and EGY related\nresources need a license from the Linguistic Data\nConsortium (LDC). The GLF data is available\nat http://resources.camel-lab.com/\nand the LEV data is available at https:\n//portal.sina.birzeit.edu/curras/.\nWe provide conversion scripts to gen-\nerate our preprocessed datasets from\nlegally accessed third-party datasets at\nhttps://github.com/CAMeL-Lab/\nCAMeLBERT_morphosyntactic_tagger.\nA.2 Implementation\nWe used Hugging Face’s transformers (Wolf et al.,\n2020) for implementation. Fine-tuning is done\nby adding a fully connected linear layer to the\nlast hidden state. We release our code including\nthe hyperparameters used in the experiments\nat https://github.com/CAMeL-Lab/\nCAMeLBERT_morphosyntactic_tagger.\nFor the experiments in Section 5, we use the fol-\nlowing hyperparameters: a random seed of 12345,\ntraining for 10 epochs, saving the model for every\n500 steps, a learning rate of 5e-5, a batch size of\n32, and a maximum sequence length of 512. We\npick the best checkpoint based on TUNE and report\nresults on DEV and TEST from a single run.\nThe number of parameters of the factored model\nfor MSA is about 1.5 billion, while the factored\nmodel for GLF, EGY , and LEV has 1.8 billion pa-\nrameters in total. The unfactored model has about\n110 million parameters for MSA, GLF, EGY , and\nLEV .\nThe factored model is the most computation-\nally expensive model to train, which took about 21\nhours for MSA, 16 hours for GLF, 13 hours for\nEGY , and five hours for LEV on a single NVIDIA-\nV100 card. The unfactored model took about 90\nminutes to train for MSA, 60 minutes for GLF, 50\nminutes for EGY , and 20 minutes for LEV on the\nsame machine.\n1719",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7839950323104858
    },
    {
      "name": "Arabic",
      "score": 0.7775216102600098
    },
    {
      "name": "Computer science",
      "score": 0.7597330212593079
    },
    {
      "name": "Modern Standard Arabic",
      "score": 0.7389336824417114
    },
    {
      "name": "Natural language processing",
      "score": 0.6146079301834106
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5202494859695435
    },
    {
      "name": "Language model",
      "score": 0.5166833400726318
    },
    {
      "name": "Training set",
      "score": 0.45702406764030457
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.444208025932312
    },
    {
      "name": "Linguistics",
      "score": 0.38474780321121216
    },
    {
      "name": "Engineering",
      "score": 0.09792080521583557
    },
    {
      "name": "Voltage",
      "score": 0.0552746057510376
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}