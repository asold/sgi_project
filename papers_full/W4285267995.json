{
    "title": "scubeMSEC@LT-EDI-ACL2022: Detection of Depression using Transformer Models",
    "url": "https://openalex.org/W4285267995",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5087330048",
            "name": "S Sivamanikandan",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5039554541",
            "name": "V Santhosh",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5021908677",
            "name": "N Sanjaykumar",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5056307094",
            "name": "Jerin Mahibha C",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5091650616",
            "name": "D. Thenmozhi",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4285288845",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3115063109",
        "https://openalex.org/W3171858038",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2978574093",
        "https://openalex.org/W3015928967",
        "https://openalex.org/W3046489189",
        "https://openalex.org/W3174556856",
        "https://openalex.org/W3158087093",
        "https://openalex.org/W3016207173",
        "https://openalex.org/W3160871366",
        "https://openalex.org/W3156819959",
        "https://openalex.org/W2966930013",
        "https://openalex.org/W3011679831",
        "https://openalex.org/W4251821179",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4285209895",
        "https://openalex.org/W3040042676",
        "https://openalex.org/W3120564254",
        "https://openalex.org/W4221145185",
        "https://openalex.org/W4286911622"
    ],
    "abstract": "Social media platforms play a major role in our day-to-day life and are considered as a virtual friend by many users, who use the social media to share their feelings all day. Many a time, the content which is shared by users on social media replicate their internal life. Nowadays people love to share their daily life incidents like happy or unhappy moments and their feelings in social media and it makes them feel complete and it has become a habit for many users. Social media provides a new chance to identify the feelings of a person through their posts. The aim of the shared task is to develop a model in which the system is capable of analyzing the grammatical markers related to onset and permanent symptoms of depression. We as a team participated in the shared task Detecting Signs of Depression from Social Media Text at LT-EDI 2022- ACL 2022 and we have proposed a model which predicts depression from English social media posts using the data set shared for the task. The prediction is done based on the labels Moderate, Severe and Not Depressed. We have implemented this using different transformer models like DistilBERT, RoBERTa and ALBERT by which we were able to achieve a Macro F1 score of 0.337, 0.457 and 0.387 respectively. Our code is publicly available in the github",
    "full_text": "Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion, pages 212 - 217\nMay 27, 2022 ©2022 Association for Computational Linguistics\nscubeMSEC@LT-EDI-ACL2022: Detection of Depression using\nTransformer Models\nSivamanikandan. S and Santhosh.V and\nSanjaykumar. N and C. Jerin Mahibha\nMeenakshi Sundararajan Engineering\nCollege, Chennai\nsivamanikandan45,santhoshdharmar21,\nsanjaykumarvn2001,\njerinmahibha@gmail.com\nDurairaj Thenmozhi\nSri Sivasubramaniya Nadar\nCollege of Engineering, Chennai\ntheni_d@ssn.edu.in\nAbstract\nSocial media platforms play a major role in our\nday-to-day life and are considered as a virtual\nfriend by many users, who use the social media\nto share their feelings all day. Many a time, the\ncontent which is shared by users on social me-\ndia replicate their internal life. Nowadays peo-\nple love to share their daily life incidents like\nhappy or unhappy moments and their feelings\nin social media and it makes them feel com-\nplete and it has become a habit for many users.\nSocial media provides a new chance to identify\nthe feelings of a person through their posts. The\naim of the shared task is to develop a model in\nwhich the system is capable of analyzing the\ngrammatical markers related to onset and per-\nmanent symptoms of depression. We as a team\nparticipated in the shared task Detecting Signs\nof Depression from Social Media Text at LT-\nEDI 2022- ACL 2022 and we have proposed a\nmodel which predicts depression from English\nsocial media posts using the data set shared\nfor the task. The prediction is done based on\nthe labels Moderate, Severe and Not Depressed.\nWe have implemented this using different trans-\nformer models like DistilBERT, RoBERTa and\nALBERT by which we were able to achieve\na Macro F1 score of 0.337, 0.457 and 0.387\nrespectively. Our code is publicly available in\nthe github1\n1 Introduction\nIn the digital world, the usage of social media has\nbecome most common among the people. Social\nmedia is used without any limits to share happiness,\njoy, sadness, loneliness and all other personal emo-\ntions. The contents shared by the people reflect the\nmental state of the person and can act as an indica-\ntor of their depression level (Kamite and Kamble,\n2020). Depression is a serious mental illness that\nnegatively affects how you feel, the way you think\nand how you act. Fortunately, it is also treatable.\n1https://github.com/sivamanikandan45/Transfomer.git\nDepression causes feelings of sadness and/or a loss\nof interest in activities you once enjoyed. It can\nlead to a variety of emotional and physical prob-\nlems and can decrease your ability to function at\nwork and at home. Sometimes, social media could\nbe the reason for the depression. It is necessary\nto measure the level of depression from the social\nmedia text to treat them and to avoid the negative\nconsequences. Detecting levels of depression is a\nchallenging task since it involves the mindset of the\npeople which can change periodically. Our aim is\nto detect levels of depression with the use of deep\nlearning Transformer Models to achieve the best\nresults (Malviya et al., 2021).\nThe shared task Detecting Signs of Depression\nfrom Social Media Text was a part of LT-EDI 2022-\nACL 2022 (Sampath et al., 2022). The task is based\non English comments. The task was a classifica-\ntion problem based on the labels “Not Depressed”,\n“Moderate” and “Severe”. For example, “My life\ngets worse every year : That’s what it feels like any-\nway.... ”fall under the category Moderate, “Words\ncan’t describe how bad I feel right now : I just want\nto fall asleep forever. ”fall under the category Se-\nvere and “Is anybody else hoping the Coronavirus\nshuts everybody down?” fall under the category\nNot Depressed.\nFor the shared task, a model based on transform-\ners was first proposed which was trained using the\ntraining data set provided for the corresponding\ntask followed by validation of the trained model\nusing the evaluation data set. Then the model was\ntested using the testing data set to predict the cate-\ngory of the text based on which the evaluation of\nthe shared task was done.\n2 Related works\nIdentifying depression from social media posts in-\nvolves detecting whether the user associated with\nthe posts could be identified for depression and this\ncould be represented as a text classification prob-\n212\nLearning Technique Approaches used Limitation\nTraditional Approach Statistical, Data driven, Rule based Requires specific features like\nand lexicon based approaches syntactic markers, psycho-linguistic\nfeatures and temporal dependencies\nMachine Learning approach SVM, RF, DT, NB, KNN, LR Requires proper fine tuning\nof parameters and does not show\nsignificant impact on the precision\nDeep Learning approach NN, RNN, LSTM Handling of heterogeneous\nand Transformer Models and feature vector representation\nassociated with the performance\nTable 1: Summary of related work\nlem. Various methods from rule based techniques\nto deep learning methods could be used for this pur-\npose. Identification of depression markers and pre-\nprocessing the actual posts also play an important\nrole in the performance of the model. The perfor-\nmance of the model used for classification mainly\ndepends on the data set used for the purpose like\nthe size of the data set and the distribution of the\ndata in the data set. Hence the analysis of the data\nset is important for selecting the appropriate model\nfor implementing the classification task. The con-\ntribution of different pre-processing techniques for\nimproving the prediction efficiency of depression\nidentification task (Figueredo and Calumby, 2020)\nhad been presented. Depression-related markers\nin Facebook users had been identified by Socially\nMediated Patient Portal (SMPP) (Hussain et al.,\n2020), which had used a data-driven approach with\nmachine learning classification techniques for ex-\ntracting such information. The syntactical markers\nrelated to onset and perpetual symptoms of depres-\nsion (Kamite and Kamble, 2020) have been iden-\ntified which when used together with statistical\nmodels had helped in effective and early identifi-\ncation of depression from social media posts. The\nimpact of psycho-linguistic patterns on standard\nmachine learning approaches had been illustrated\nfor the classification of social media texts that are\nassociated with depression (Trifan et al., 2020).\nMulti modal framework and statistical techniques\nhad been used to discern depressive behaviours\nfrom a heterogeneous set of features including vi-\nsual, textual, and user interaction data (Yazdavar\net al., 2020) from social media posts. Multiple In-\nstance Learning methods (Mann et al., 2021) had\nbeen used for the task of identifying depression\nfrom social media posts which had implemented\nthe classification by exploiting temporal dependen-\ncies between posts. Detection of mental health dis-\norders, especially depression, had been predicted\nfrom Arabic posts using a lexicon based approach\nand machine learning approach (Alghamdi et al.,\n2020).\nEarly detection of different emotions of people\nincluding depression from their social media posts\nhad been done using a hybrid model which is a\ncombination of two machine learning algorithms\nnamely Support Vector Machine and Naive Bayes\nalgorithm (Smys and Raj, 2021). The performance\nmeasures of the model had been analyzed by fine\ntuning the parameters associated with the algo-\nrithms. Detection of depression from Bengali posts\nand commentaries had been implemented and eval-\nuated using different machine learning algorithms\nlike Support Vector Machine, Random Forest, De-\ncision Tree, K-Nearest Neighbors, Naive Bayes\n(Multinomial Naive Bayes) and Logistic Regres-\nsion. The results had shown that the same precision\nhad been achieved by all the algorithms (Victor\net al., 2020). Social media posts of high school stu-\ndents, college students and working professionals\nhad also been considered in specific for identify-\ning mental health using the above mentioned ma-\nchine learning algorithms (Narayanrao and Lalitha\nSurya Kumari, 2020).\nUse of deep learning models had been depicted\nfor the prediction of mental disorders such as de-\npression. A multi-task hierarchical neural network\nwith topic attention had been used for identifying\nhealth issues from social media posts. Bidirectional\ngated recurrent units had been used to analyze the\nhierarchical relationship (Zhou et al., 2021) among\n213\ndocuments, sentences and words based on which at-\ntention weights are enhanced for words. The posts\nwith unstructured text data that display depression\nhad been identified more effectively by deep learn-\ning models than by using supervised learning meth-\nods (Ahmad et al., 2020). The role of sentiment\nanalysis in identifying depression had been shown\nwhich had improved the performance of the model\nby using different deep learning techniques for\nthe process of classification (Banerjee and Shaikh,\n2021). Better performance had been achieved when\nthe heterogeneous and feature vector representation\nassociated with social media posts had been han-\ndled and transformer based models (Garg, 2021)\nhad been utilized for classification of depression\nand suicidal posts. Depression and associated nega-\ntive emotions had been identified from Sina Weibo,\nusing deep learning methods (Yao et al., 2019).\nTable 1 summarises the approaches and the lim-\nitations associated with the models that exist for\ndetecting depression from social media posts. It\ncould be summarized from the related works that\nproper pre-processing, selection of markers and\ndominant feature extraction directly have an im-\npact on the performance of the model. Differ-\nent approaches like rule-based approach, statistical\napproach, machine learning approaches and deep\nlearning approaches could be used for this purpose\nand the deep learning techniques tend to show bet-\nter performance when compared with traditional\nand machine learning approaches. When appropri-\nate text pre-processing and textual based featuring\ntechniques (Zhou et al., 2021) had been used with\nmachine learning classifiers, it had been shown that\ndepression associated social media texts could be\neffectively identified even when depression spe-\ncific keywords were not present in the social media\nposts.The performance of the model based on an\napproach may not be the same for all data sets\nwhich is a major factor to be considered and this\nmakes the problem of identifying depression from\nsocial media text as an important research field in\nthe domain of Natural Language Processing.\n3 Data set\nThe data set used for our model is a collection of\nSocial Media Text provided by the organizers of\nthe shared task (Sampath et al., 2022). The data\nset (Kayalvizhi and Thenmozhi, 2022) comprises\ntraining, development and test data set. The data\nfiles were in Tab Separated Values (tsv) format with\nData Set Category Instances\nTrain Not Depression 1971\nModerate 6019\nSevere 901\nValidation Not Depression 1,830\nModerate 2306\nSevere 360\nTable 2: Data set statistics\nthree columns namely posting id (pid), text data\nand label. The sample instances are as follows:\n• Not depressed - This indicates the social me-\ndia text is not depressed in nature Example:\n\"Is anybody else hoping the Coronavirus shuts\neverybody down?”\n• Moderate- This indicates the social media text\nis moderately depressed in nature Example:\n\"My life gets worse every year : That’s what\nit feels like anyway.... ”\n• Severe- This indicates the social media text is\nseverely depressed in nature Example:\"Words\ncan’t describe how bad I feel right now : I just\nwant to fall asleep forever. ”\nThe distribution of the data in the data set is shown\nin Table 2. The training data set had 8,891 in-\nstances of which 1,971 instances were under the\nNot depressed category, 6019 instances were under\nModerate category and 901 instances under Severe\ncategory. The validation data set provided for the\nevaluation of the model had 4496 instances with\n1830, 2306 and 360 instances under the category\nNot depressed, Moderate and Severe respectively.\nThe test data set provided for the purpose of predic-\ntion had 3245 instances.\n4 Methodology\nThe proposed methodology uses deep learning tech-\nniques for implementing the process of detecting\ndepression from social media texts. From the ex-\nisting systems it could be found that transformer\nbased models exhibit better performance when\ncompared to Neural network based models and\nLSTM based models. Hence the proposed system\nuses three different Transformer models namely\nDistilBERT, ALBERT and RoBERTa for the task\nof detecting the depression level from social media\ntext.\n214\nModel DistilBERT RoBERTa ALBERT\nAccuracy 0.342 0.510 0.408\nMacro F1-Score 0.337 0.457 0.387\nMacro Recall 0.467 0.519 0.497\nMacro Precision 0.456 0.461 0.432\nTable 3: Task Score\n4.1 DistilBERT\nDistilBERT (Sanh et al., 2019) is a general-purpose\npre-trained version of BERT which had been pre-\ntrained on the same corpus as BERT in a self-\nsupervised fashion. This means it was pre-trained\non the raw texts only, with no human labeling to\ngenerate inputs and labels from those texts using\nthe BERT base model2. Distil-BERT has 97% of\nBERT’s performance while being trained on half\nof the parameters of BERT. BERT-base has 110\nparameters and BERT-large has 340 parameters,\nwhich are hard to deal with. For this problem’s\nsolution, distillation techniques are used to reduce\nthe size of these large models3.\nWe have used “distilbert–base-cased” model for\nimplementing the classification task of identifying\ndepression from social media text which comprises\nof 6-layer, 768-hidden layers and also 12-heads,\n65M parameters. It is a smaller version than BERT\nwhich is incredibly less expensive and quicker to\ntrain than BERT.\n4.2 RoBERTa\nRoBERTa (Liu et al., 2019) is a transformer model\npre-trained on a large corpus of English data and\nis based on BERT model and modifies key hyper-\nparameters and training is implemented with larger\nmini-batches and learning rates 4. RoBERTa is a\nRobust BERT method which has been trained on a\nfar extra large data set and for a whole lot of large\nquantities of iterations with a bigger batch length\nof 8k.\nWe have used the “RoBERTa–base” model for\nthe task which is a pretrained model on English lan-\nguage using a masked language modeling (MLM)\nobjective. This model is case-sensitive and it com-\nprises 12-layers, 768-hidden layers, 12-heads and\n125M parameters.\n2https://huggingface.co/distilbert-base-uncased\n3https://analyticsindiamag.com/python-guide-to-\nhuggingface-distilbert-smaller-faster-cheaper-distilled-bert/\n4https://huggingface.co/docs/transformers/modeldoc/roberta\n4.3 ALBERT\nThe ALBERT (Lan et al., 2020) model is a Lite\nBERT which improves the training and results of\nBERT architecture by using different techniques\nlike parameter sharing, factorization of embedding\nmatrix and Inter sentence Coherence loss.\nWe have used “ALBERT–base-v1” model for the\ntask which is also a pre-trained model on English\nlanguage. This model is uncased5 and it consists of\n12 repeating layers, 128 embedding, 768-hidden,\n12-heads and 11M parameters6. The first step asso-\nciated with the task is to prepare the data set which\ninvolves pre-processing the text from the data set\nfor effective modeling. As the text from social me-\ndia posts does not have a standard structure and use\nof symbols, tags and URLs are common, the texts\nneed to be pre-processed by converting the com-\nplete text into lower case words and removing the\nstop-words, URLs, numbers and tags which do not\ncontribute much for the classification task. Then\nthe three different transformer models namely Dis-\ntilBERT, ALBERT and RoBERTa had been used to\nimplement the classification of the texts into Mod-\nerate, Severe and Not Depressed texts. The labels\nwere converted to equivalent integer categorical\nvalues so that it can be given as input to the trans-\nformer models. The models are trained using the\ntraining set provided as a part of the shared task.\nThe evaluation of the model was carried out using\nthe evaluation data set provided by the shared task.\nFinally the required predictions were done using\nthe test data set provided by the shared task. The\nnumber of epochs that were considered for training\nwere 5 for DistilBERT and ALBERT and 1 epoch\nwas used for RoBERTa.\n5 Experimental Setup\nWe have used the virtual GPU (Tesla k80) provided\nby Google Colab for implementing different trans-\nformer models.The processing time was found to\nbe 5.43 min, 15.46 min, 5.48 min for DistilBERT,\n5https://huggingface.co/albert-base-v1\n6https://huggingface.co/transformers/v3.3.1/pretrainedmodels.html\n215\nLabel Precision Recall F1-Score Support\nNot Depression 0.60 0.45 0.52 1830\nModerate 0.59 0.72 0.65 2306\nSevere 0.31 0.29 0.30 360\nAccuracy 0.57 4496\nMacro Avg 0.50 0.49 0.49 4496\nWeighted Avg 0.576 0.57 0.57 4496\nTable 4: Classification Report\nRoBERTa and AlBERT models respectively. The\nmemory usage of our model was calculated to be\n3583MiB.\n6 Results\nThe evaluation of the shared task was done using\nthe metric namely Macro F1-score. The other met-\nrics that were used to represent the performance of\nthe model were Accuracy, Macro Recall and Macro\nPrecision. The ratio of the number of correct pre-\ndictions to the total number of input samples is\nrepresented by the metric Accuracy. The ratio of\nthe number of correct positive results to the num-\nber of positive results predicted by the classifier is\nrepresented by Precision. The model’s ability to\ndetect positive samples is represented by recall. F1\nscore is an overall measure of a model’s accuracy\nthat combines precision and recall. A high F1 score\nmeans that the classification has resulted with low\nnumber of false positives, and low false negatives.\nThe metric values that were scored by the three\ndifferent models on the test data set provided for\nthe shared task are given in Table 3. When us-\ning the DistilBERT the values that were obtained\nfor the different metrics were 0.342 for accuracy,\n0.337 for Macro F1-score, 0.467 for Macro recall\nand 0.456 for Macro precision. By using the trans-\nformer model ALBERT for classification the met-\nrics were improved to 0.408 for accuracy, 0.387 for\nMacro F1-score, 0.497 for Macro Recall and 0.432\nfor Macro Precision. The metrics were further im-\nproved when the RoBERTa model was used for\nimplementing the classification task which resulted\nin an accuracy of 0.510, Macro F1-score of 0.457,\nMacro recall of 0.519 and Macro precision of 0.461\nwhich brought us to the rank of 23 in the leader\nboard.\n7 Error Analysis\nThe model RoBERTa had resulted in an F1 score of\n0.457 which is low compared to 0.583 which is the\nF1 score obtained by the topper of the leader board.\nThis shows that more false positive and false neg-\native classification has occurred in our proposed\nmodel. The data set provided is highly imbalanced\nin nature which could also be considered as a rea-\nson for the poor performance of the model. The\ndata set could be converted to a balanced data set\nby using different up-sampling and down-sampling\ntechniques.\nThe classification report generated during the\nevaluation of the model is shown in Table 4. It\ncould be found that the instances that fall under\nthe category ‘Severe’ have a low F1 score of 0.30,\nwhich means more false positives and false neg-\natives have occurred under this category. Most\nof the posts associated with the category Severe\ndo not use the depression related markers directly\nwhich can also be considered as a reason for poor\nperformance of the model.\n8 Conclusion\nAs social media platforms play a crucial role in to-\nday’s world and the posts shared replicate the inter-\nnal mental state of the user, the task of identifying\ndepression from social media posts have become\nan important research area. The methodology as-\nsociated with our submission used three different\ntransformer models to implement the above said\ntask namely DistilBERT, ALBERT and RoBERTa\nof which the RoBERTa model had shown a better\nperformance with a F1 score of 0.457.\nThis score is not an optimal value and shows\nthe availability of scope to fine tune the trans-\nformer models for improving the performance of\nthe model. The process can be more effectively\ndone when depression markers are identified and\nthe context based informations of the posts are\nconsidered while developing models to identify de-\npression from social media texts.\n216\nReferences\nHussain Ahmad, Dr. Muhammad Asghar, Fahad\nAlotaibi, and Ibrahim Hameed. 2020. Applying deep\nlearning technique for depression classification in\nsocial media text. Journal of Medical Imaging and\nHealth Informatics, 10:pp. 2446–2451(6).\nNorah Saleh Alghamdi, Hanan A. Hosni Mahmoud,\nAjith Abraham, Samar Awadh Alanazi, and Laura\nGarcía-Hernández. 2020. Predicting depression\nsymptoms in an arabic psychological forum. IEEE\nAccess, 8:57317–57334.\nSatyaki Banerjee and Nuzhat F. Shaikh. 2021. A survey\non mental health monitoring system via social me-\ndia data using deep learning framework. In Techno-\nSocietal 2020, pages 879–887, Cham. Springer Inter-\nnational Publishing.\nJosé Figueredo and Rodrigo Calumby. 2020. On text\npreprocessing for early detection of depression on\nsocial media. In Anais do XX Simpósio Brasileiro de\nComputação Aplicada à Saúde, pages 84–95, Porto\nAlegre, RS, Brasil. SBC.\nMuskan Garg. 2021. Quantifying the suicidal ten-\ndency on social media: A survey. arXiv preprint\narXiv:2110.03663.\nJamil Hussain, Fahad Ahmed Satti, Muhammad Afzal,\nWajahat Ali Khan, Hafiz Syed Muhammad Bilal,\nMuhammad Zaki Ansaar, Hafiz Farooq Ahmad,\nTaeho Hur, Jaehun Bang, Jee-In Kim, Gwang Hoon\nPark, Hyonwoo Seung, and Sungyoung Lee. 2020.\nExploring the dominant features of social media for\ndepression detection. Journal of Information Science,\n46(6):739–759.\nSangeeta R. Kamite and V . B. Kamble. 2020. Detec-\ntion of depression in social media via twitter using\nmachine learning approach. In 2020 International\nConference on Smart Innovations in Design, Environ-\nment, Management, Planning and Computing (IC-\nSIDEMPC), pages 122–125.\nS Kayalvizhi and D Thenmozhi. 2022. Data set cre-\nation and empirical analysis for detecting signs of de-\npression from social media postings. arXiv preprint\narXiv:2202.03047.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nKeshu Malviya, Bholanath Roy, and SK Saritha. 2021.\nA transformers approach to detect depression in so-\ncial media. In 2021 International Conference on\nArtificial Intelligence and Smart Systems (ICAIS) ,\npages 718–723.\nPaulo Mann, Aline Paes, and Elton H. Matsushima.\n2021. Screening for depressed individuals by us-\ning multimodal social media data. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n35(18):15722–15723.\nPurude Vaishali Narayanrao and P. Lalitha Surya Ku-\nmari. 2020. Analysis of machine learning algorithms\nfor predicting depression. In 2020 International Con-\nference on Computer Science, Engineering and Ap-\nplications (ICCSEA), pages 1–4.\nKayalvizhi Sampath, Thenmozhi Durairaj,\nBharathi Raja Chakravarthi, and Jerin Mahibha C.\n2022. Findings of the shared task on Detecting Signs\nof Depression from Social Media. In \"Proceedings\nof the Second Workshop on Language Technology\nfor Equality, Diversity and Inclusion\". Association\nfor Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nS Smys and Jennifer S Raj. 2021. Analysis of deep\nlearning techniques for early detection of depression\non social media network-a comparative study. Jour-\nnal of trends in Computer Science and Smart technol-\nogy (TCSST), 3(01):24–39.\nAlina Trifan, Rui Antunes, Sérgio Matos, and Jose Luís\nOliveira. 2020. Understanding depression from psy-\ncholinguistic patterns in social media texts. In Ad-\nvances in Information Retrieval , pages 402–409,\nCham. Springer International Publishing.\nDebasish Bhattacharjee Victor, Jamil Kawsher,\nMd Shad Labib, and Subhenur Latif. 2020. Machine\nlearning techniques for depression analysis on\nsocial media- case study on bengali community. In\n2020 4th International Conference on Electronics,\nCommunication and Aerospace Technology (ICECA),\npages 1118–1126.\nXiaoxu Yao, Guang Yu, Xianyun Tian, and Jingyun\nTang. 2019. Patterns and longitudinal changes in\nnegative emotions of people with depression on sina\nweibo. Telemedicine journal and e-health : the offi-\ncial journal of the American Telemedicine Associa-\ntion.\nAmir Hossein Yazdavar, Mohammad Saeid Mah-\ndavinejad, Goonmeet Bajaj, William Romine, Amit\nSheth, Amir Hassan Monadjemi, Krishnaprasad\nThirunarayan, John M. Meddar, Annie Myers, Jy-\notishman Pathak, and Pascal Hitzler. 2020. Multi-\nmodal mental health analysis in social media. PLOS\nONE, 15(4):1–27.\nDeyu Zhou, Jiale Yuan, and Jiasheng Si. 2021. Health\nissue identification in social media based on multi-\ntask hierarchical neural networks with topic attention.\nArtificial Intelligence in Medicine, 118:102119.\n217"
}