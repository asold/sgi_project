{
  "title": "ResViT: Residual Vision Transformers for Multimodal Medical Image Synthesis",
  "url": "https://openalex.org/W3176466780",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4281577950",
      "name": "Dalmaz, Onat",
      "affiliations": [
        "Bilkent University"
      ]
    },
    {
      "id": "https://openalex.org/A4287364598",
      "name": "Yurt, Mahmut",
      "affiliations": [
        "Bilkent University"
      ]
    },
    {
      "id": "https://openalex.org/A2748818094",
      "name": "Çukur, Tolga",
      "affiliations": [
        "Bilkent University"
      ]
    },
    {
      "id": null,
      "name": "\\c{C}ukur, Tolga",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W146667522",
    "https://openalex.org/W2128480089",
    "https://openalex.org/W2230773113",
    "https://openalex.org/W2067018983",
    "https://openalex.org/W1605218991",
    "https://openalex.org/W2963105382",
    "https://openalex.org/W2104876002",
    "https://openalex.org/W89893565",
    "https://openalex.org/W2043626403",
    "https://openalex.org/W2594313306",
    "https://openalex.org/W196933601",
    "https://openalex.org/W2612585477",
    "https://openalex.org/W2773119425",
    "https://openalex.org/W2753053025",
    "https://openalex.org/W2517395172",
    "https://openalex.org/W2397287600",
    "https://openalex.org/W2208819768",
    "https://openalex.org/W2405268213",
    "https://openalex.org/W1502698477",
    "https://openalex.org/W2208340121",
    "https://openalex.org/W2046933782",
    "https://openalex.org/W2523213734",
    "https://openalex.org/W2767044624",
    "https://openalex.org/W2963768110",
    "https://openalex.org/W2521443754",
    "https://openalex.org/W2461164258",
    "https://openalex.org/W2751812122",
    "https://openalex.org/W2917568709",
    "https://openalex.org/W6637568146",
    "https://openalex.org/W6751886274",
    "https://openalex.org/W2807246925",
    "https://openalex.org/W2789713147",
    "https://openalex.org/W2807888536",
    "https://openalex.org/W2962932373",
    "https://openalex.org/W2979387929",
    "https://openalex.org/W3007486523",
    "https://openalex.org/W3035647610",
    "https://openalex.org/W3132541979",
    "https://openalex.org/W3096358115",
    "https://openalex.org/W2914057844",
    "https://openalex.org/W2978111064",
    "https://openalex.org/W3015340265",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W6746775625",
    "https://openalex.org/W6750469568",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W3200891683",
    "https://openalex.org/W3203971980",
    "https://openalex.org/W4226133625",
    "https://openalex.org/W3206815816",
    "https://openalex.org/W6782529005",
    "https://openalex.org/W6795999542",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2890139949",
    "https://openalex.org/W2592929672",
    "https://openalex.org/W66427752",
    "https://openalex.org/W1882516042",
    "https://openalex.org/W2522924304",
    "https://openalex.org/W2331918145",
    "https://openalex.org/W2948272061",
    "https://openalex.org/W2607711656",
    "https://openalex.org/W2523468284",
    "https://openalex.org/W2971470434",
    "https://openalex.org/W2979522016",
    "https://openalex.org/W6678815747",
    "https://openalex.org/W2984306354",
    "https://openalex.org/W2794022343",
    "https://openalex.org/W2963882942",
    "https://openalex.org/W3096126074",
    "https://openalex.org/W2962891897",
    "https://openalex.org/W2957024036",
    "https://openalex.org/W2745006834",
    "https://openalex.org/W2981126002",
    "https://openalex.org/W2889779108",
    "https://openalex.org/W2962784654",
    "https://openalex.org/W2759965110",
    "https://openalex.org/W3101621846",
    "https://openalex.org/W2890371642",
    "https://openalex.org/W3013034453",
    "https://openalex.org/W6752378368",
    "https://openalex.org/W3018177167",
    "https://openalex.org/W3108244925",
    "https://openalex.org/W3002137088",
    "https://openalex.org/W3204255739",
    "https://openalex.org/W3188404242",
    "https://openalex.org/W3204700807",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1641498739",
    "https://openalex.org/W2782686167",
    "https://openalex.org/W2148726987",
    "https://openalex.org/W3158566850",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6794559225",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W6765779288",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W3034864980",
    "https://openalex.org/W3127329503",
    "https://openalex.org/W3013884416",
    "https://openalex.org/W3154672809",
    "https://openalex.org/W6795905486",
    "https://openalex.org/W6795166983",
    "https://openalex.org/W2949558375",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3048808444",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W4323520891",
    "https://openalex.org/W3105747145",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W4299412574",
    "https://openalex.org/W2900298334",
    "https://openalex.org/W3101123465",
    "https://openalex.org/W2002011878",
    "https://openalex.org/W2125389028",
    "https://openalex.org/W3135451226",
    "https://openalex.org/W3163813349",
    "https://openalex.org/W2751069891",
    "https://openalex.org/W3194591991",
    "https://openalex.org/W3152550266",
    "https://openalex.org/W1710476689",
    "https://openalex.org/W3134689216",
    "https://openalex.org/W4295004744",
    "https://openalex.org/W2124776405",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W2921449040",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2802508689",
    "https://openalex.org/W2963981733",
    "https://openalex.org/W4287203292",
    "https://openalex.org/W2775288145",
    "https://openalex.org/W3099535809",
    "https://openalex.org/W2328365748",
    "https://openalex.org/W3164513001"
  ],
  "abstract": "Generative adversarial models with convolutional neural network (CNN) backbones have recently been established as state-of-the-art in numerous medical image synthesis tasks. However, CNNs are designed to perform local processing with compact filters, and this inductive bias compromises learning of contextual features. Here, we propose a novel generative adversarial approach for medical image synthesis, ResViT, that leverages the contextual sensitivity of vision transformers along with the precision of convolution operators and realism of adversarial learning. ResViT's generator employs a central bottleneck comprising novel aggregated residual transformer (ART) blocks that synergistically combine residual convolutional and transformer modules. Residual connections in ART blocks promote diversity in captured representations, while a channel compression module distills task-relevant information. A weight sharing strategy is introduced among ART blocks to mitigate computational burden. A unified implementation is introduced to avoid the need to rebuild separate synthesis models for varying source-target modality configurations. Comprehensive demonstrations are performed for synthesizing missing sequences in multi-contrast MRI, and CT images from MRI. Our results indicate superiority of ResViT against competing CNN- and transformer-based methods in terms of qualitative observations and quantitative metrics.",
  "full_text": "1\nResViT: Residual vision transformers for\nmulti-modal medical image synthesis\nOnat Dalmaz, Mahmut Yurt, and Tolga C ¸ ukur∗, Senior Member\nAbstract— Generative adversarial models with convolu-\ntional neural network (CNN) backbones have recently been\nestablished as state-of-the-art in numerous medical image\nsynthesis tasks. However, CNNs are designed to perform\nlocal processing with compact ﬁlters, and this inductive\nbias compromises learning of contextual features. Here,\nwe propose a novel generative adversarial approach for\nmedical image synthesis, ResViT, that leverages the con-\ntextual sensitivity of vision transformers along with the pre-\ncision of convolution operators and realism of adversarial\nlearning. ResViT’s generator employs a central bottleneck\ncomprising novel aggregated residual transformer (ART)\nblocks that synergistically combine residual convolutional\nand transformer modules. Residual connections in ART\nblocks promote diversity in captured representations, while\na channel compression module distills task-relevant infor-\nmation. A weight sharing strategy is introduced among\nART blocks to mitigate computational burden. A uniﬁed\nimplementation is introduced to avoid the need to re-\nbuild separate synthesis models for varying source-target\nmodality conﬁgurations. Comprehensive demonstrations\nare performed for synthesizing missing sequences in multi-\ncontrast MRI, and CT images from MRI. Our results indi-\ncate superiority of ResViT against competing CNN- and\ntransformer-based methods in terms of qualitative obser-\nvations and quantitative metrics.\nIndex Terms— medical image synthesis, transformer,\nresidual, vision, adversarial, generative, uniﬁed\nI. INTRODUCTION\nMedical imaging plays a pivotal role in modern healthcare\nby enabling in vivo examination of pathology in the human\nbody. In many clinical scenarios, multi-modal protocols are\ndesirable that involve a diverse collection of images from\nmultiple scanners (e.g., CT, MRI) [1], or multiple acquisitions\nfrom a single scanner (multi-contrast MRI) [2]. Complemen-\ntary information about tissue morphology, in turn, empower\nphysicians to diagnose with higher accuracy and conﬁdence.\nUnfortunately, numerous factors including uncooperative pa-\ntients and excessive scan times prohibit ubiquitous multi-\nmodal imaging [3], [4]. As a result, there has been ever-\ngrowing interest in synthesizing unacquired images in multi-\nmodal protocols from the subset of available images, bypass-\ning costs associated with additional scans [5], [6].\nMedical image synthesis aims to predict target-modality\nimages for a subject given source-modality images acquired\nunder a limited scan budget [7]. This is an ill-posed inverse\nproblem since medical images are high dimensional, target-\nmodality data are absent during inference, and there exist\nThis study was supported in part by a TUBITAK BIDEB scholarship\nawarded to O. Dalmaz, and by TUBA GEBIP 2015 and BAGEP 2017\nfellowships awarded to T. C ¸ ukur (Corresponding author: Tolga C ¸ ukur).\nO. Dalmaz, M. Yurt, and T. C ¸ ukur are with the Department of Electrical\nand Electronics Engineering, and the National Magnetic Resonance\nResearch Center (UMRAM), Bilkent University, Ankara, Turkey (e-mails:\n{onat, mahmut, cukur}@ee.bilkent.edu.tr). T. C ¸ ukur is also with the\nNeuroscience Program, Sabuncu Brain Research Center, Bilkent Uni-\nversity, TR-06800 Ankara, Turkey.\nnonlinear differences in tissue contrast across modalities [8]–\n[13]. Unsurprisingly, recent adoption of deep learning methods\nfor solving this difﬁcult problem has enabled major perfor-\nmance leaps [14]–[21]. In learning-based synthesis, network\nmodels effectively capture a prior on the joint distribution of\nsource-target images [22]–[24]. Earlier studies using CNNs\nfor this purpose reported signiﬁcant improvements over tradi-\ntional approaches [22], [23], [25]–[28]. Generative adversarial\nnetworks (GANs) were later introduced that leverage an ad-\nversarial loss to increase capture of detailed tissue structure\n[24], [29]–[35]. Further improvements were attained by lever-\naging enhanced architectural designs [36]–[39], and learning\nstrategies [40]–[42]. Despite their prowess, prior learning-\nbased synthesis models are fundamentally based on convo-\nlutional architectures that use compact ﬁlters to extract local\nimage features [43], [44]. Exploiting correlations among small\nneighborhoods of image pixels, this inductive bias reduces the\nnumber of model parameters to facilitate learning. However,\nit also limits expressiveness for contextual features that reﬂect\nlong-range spatial dependencies [45], [46].\nMedical images contain contextual relationships across both\nhealthy and pathological tissues. For instance, bone in the\nskull or CSF in the ventricles broadly distribute over spa-\ntially contiguous or segregated brain regions, resulting in\ndependencies among distant voxels. While pathological tissues\nhave less regular anatomical priors, their spatial distribution\n(e.g., location, quantity, shape) can still show disease-speciﬁc\npatterns [47]. For instance, multiple diffuse brain lesions are\npresent in multiple sclerosis (MS) and Alzheimer’s (AD); com-\nmonly located near periventricular and juxtacortical regions in\nMS, and near hippocampus, entorhinal cortex and isocortex\nin AD [48]. Meanwhile, few lesions manifest as spatially-\ncontiguous clumps in cancer; with lesions typically located\nnear the cerebrum and cerebellum in gliomas, and near the\nskull in meningiomas [48]. Thus, the distribution of pathology\nalso involves context regarding the position and structure of\nlesions with respect to healthy tissue. In principle, synthesis\nperformance can be enhanced by priors that capture these\nrelationships. Vision transformers are highly promising for this\ngoal since attention operators that learn contextual features\ncan improve sensitivity for long-range interactions [49], and\nfocus on critical image regions for improved generalization\nto atypical anatomy such as lesions [50]. However, adopting\nvanilla transformers in tasks with pixel-level outputs is chal-\nlenging due to computational burden and limited localization\n[51]. Recent studies instead consider hybrid architectures or\ncomputation-efﬁcient attention operators to adopt transformers\nin medical imaging tasks [52]–[57].\nHere, we propose a novel deep learning model for medical\nimage synthesis, ResViT, that translates between multi-modal\nimaging data. ResViT combines the sensitivity of vision trans-\nformers to global context, the localization power of CNNs, and\nthe realism of adversarial learning. ResViT’s generator follows\narXiv:2106.16031v3  [eess.IV]  6 Mar 2022\n2\nan encoder-decoder architecture with a central bottleneck\nto distill task-critical information. The encoder and decoder\ncontain CNN blocks to leverage local precision of convolution\noperators [58]. The bottleneck comprises novel aggregated\nresidual transformer (ART) blocks to synergistically preserve\nlocal and global context, with a weight-sharing strategy to\nminimize model complexity. To improve practical utility, a\nuniﬁed ResViT implementation is introduced that consolidates\nmodels for numerous source-target conﬁgurations. Demonstra-\ntions are performed for synthesizing missing sequences in\nmulti-contrast MRI, and CT from MRI. Comprehensive exper-\niments on imaging datasets from healthy subjects and patients\nclearly indicate the superiority of the proposed method against\ncompeting methods. Code to implement the ResViT model is\npublicly available at https://github.com/icon-lab/ResViT.\nContributions\n• We introduce the ﬁrst adversarial model for medical\nimage synthesis with a transformer-based generator to\ntranslate between multi-modal imaging data.\n• We introduce novel aggregated residual transformer\n(ART) blocks to synergistically preserve localization and\ncontext.\n• We introduce a weight sharing strategy among ART\nblocks to lower model complexity and mitigate compu-\ntational burden.\n• We introduce a uniﬁed synthesis model that generalizes\nacross multiple conﬁgurations of source-target modalities.\nII. R ELATED WORK\nThe immense success of deep learning in inverse prob-\nlems has motivated its rapid adoption in medical imaging\n[59], [60]. Medical image synthesis is a particularly ill-\nposed problem since target images are predicted without\nany target-modality data [32]. Earlier studies in this domain\nhave proposed local networks based on patch-level processing\n[16], [61], [62]. While local networks offer beneﬁts over\ntraditional approaches, they can show limited sensitivity to\nbroader context across images [22]. Later studies adopted deep\nCNNs for image-level processing with increasing availability\nof large imaging databases. CNN-based synthesis has been\nsuccessfully demonstrated in various applications including\nsynthesis across MR scanners [32], [63]–[65], multi-contrast\nMR synthesis [22], [23], [25]–[28], and CT synthesis [66]–\n[69]. Despite signiﬁcant improvements they enable, CNNs\ntrained with pixel-wise loss terms tend to suffer from unde-\nsirable loss of detailed structure [24], [43], [44].\nTo improve capture of structural details, GANs [29] were\nproposed to learn the distribution of target modalities condi-\ntioned on source modalities [70]. Adversarial losses empower\nGANs to capture an improved prior for recovery of high-\nspatial-resolution information [24], [43], [44]. In recent years,\nGAN-based methods were demonstrated to offer state-of-the-\nart performance in numerous synthesis tasks, including data\naugmentation as well as multi-modal synthesis [24], [34],\n[71], [72]. Important applications of GAN models include\nCT to PET [73], [74], MR to CT [75]–[77], unpaired cross-\nmodality [78]–[81], 3T-to-7T [82], [83], and multi-contrast\nMRI synthesis [24], [30]–[42].\nWhile GAN models have arguably emerged as a gold\nstandard in recent years, they are not without limitation. In\nparticular, GANs are based on purely convolutional operators\nknown to suffer from poor across-subject generalization to\natypical anatomy and sub-optimal learning of long-range spa-\ntial dependencies [45], [46]. Recent studies have incorporated\nspatial or channel attention mechanisms to modulate CNN-\nderived feature maps [37], [50], [84]–[88]. Such modulation\nmotivates the network to give greater focus to regions that\nmay suffer from greater errors [50], [85]. While attention\nmaps might be distributed across image regions, multiplicative\ngating of local CNN features offers limited expressiveness in\nmodeling of global context [51], [89], [90].\nTo incorporate contextual representations, transformer-\nbased methods have received recent interest in imaging tasks\nsuch as segmentation [51], [89], [91], reconstruction [52]–\n[54], and synthesis [55]–[57]. Among relevant methods are\nTransformer GAN that suppresses noise in low-dose PET\nimages [52], TransCT that suppresses noise in low-dose CT\nimages [53], and SLATER that recovers MR images from\nundersampled k-space acquisitions [54]. While these methods\nreconstruct images for single-modality data, ResViT trans-\nlates imaging data across separate modalities. Furthermore,\nTransformer GAN is an adversarial model with convolutional\nencoder-decoder and a bottleneck that contains a transformer\nwithout external residual connections. TransCT is a non-\nadversarial model where CNN blocks ﬁrst learn textural\ncomponents of low-frequency (LF) and high-frequency (HF)\nimage parts; and a transformer without external residual con-\nnections then combines encoded HF and textural LF maps.\nIn comparison, ResViT is an adversarial model that employs\na hybrid architecture in its bottleneck comprising a cascade\nof residual transformer and residual CNN modules. Unlike\nSLATER based on an unconditional model that maps latent\nvariables to images via cross-attention transformers, ResViT\nis a conditional model based on self-attention transformers.\nFew recent studies have independently introduced\ntransformer-based methods for medical image synthesis.\nVTGAN generates retinal angiograms from fundus\nphotographs [55] and GANBERT performs MR-to-PET\nsynthesis [56], whereas ResViT performs multi-contrast MRI\nand MR-to-CT synthesis. Both VTGAN and GANBERT\nuse entirely convolutional generators and only include\ntransformers in their discriminators. In contrast, ResViT\nincorporates transformers in its generator to explicitly\nleverage long-range context. The closest study to our work\nis PTNet that performs one-to-one translation between T 1-\nand T2-weighted images in infant MRI [57]. However, PTNet\nis a non-adversarial model without a discriminator, and it\nfollows a convolution-free architecture. In contrast, ResViT\nis an adversarial model with a hybrid CNN-transformer\narchitecture to achieve high localization and contextual\nsensitivity along with a high degree of realism in synthesized\nimages. Furthermore, a broader set of tasks are considered\nfor ResViT including one-to-one and many-to-one translation.\nA unique component of ResViT is the novel ART blocks in\nits generator that contain a cascade of transformer and CNN\nmodules equipped with skip connections. These residual paths\nenable effective aggregation of contextual and convolutional\nrepresentations. Based on this powerful component, we pro-\nvide the ﬁrst demonstrations of a transformer architecture for\nmany-to-one synthesis tasks and a uniﬁed synthesis model for\nadvancing practicality over task-speciﬁc methods.\nDALMAZ et al.: RESVIT: RESIDUAL VISION TRANSFORMERS FOR MULTI-MODAL MEDICAL IMAGE SYNTHESIS 3\nFig. 1 : The generator in ResViT\nfollows an encoder-decoder architec-\nture bridged with a central informa-\ntion bottleneck to distill task-speciﬁc\ninformation. The encoder and de-\ncoder comprise convolutional layers to\nmaintain local precision and inductive\nbias in learned structural representa-\ntions. Meanwhile, the information bot-\ntleneck comprises a stack of novel ag-\ngregated residual transformer (ART)\nblocks. ART blocks learn contextual\nrepresentations via vision transform-\ners, and synergistically fuse CNN-\nbased local and transformer-based\nglobal representations.\nIII. T HEORY AND METHODS\nA. Residual Vision Transformers\nHere we propose a novel adversarial method for medical\nimage synthesis named residual vision transformers, ResViT,\nthat can unify various source-target modality conﬁgurations\ninto a single model for improved practicality. ResViT leverages\na hybrid architecture of deep convolutional operators and\ntransformer blocks to simultaneously learn high-resolution\nstructural and global contextual features (Fig. 1). The gener-\nator subnetwork follows an encoder - information bottleneck\n- decoder pathway, and the discriminator subnetwork is com-\nposed of convolutional operators. The generator’s bottleneck\ncontains a stack of novel aggregated residual transformer\n(ART) blocks. Each ART block is organized as the cascade of\na transformer module that extracts hidden contextual features,\nand a CNN module that extracts hidden local features of\ninput feature maps. Importantly, external skip connections\nare inserted around both modules to create multiple paths of\ninformation ﬂow through the block. These paths propagate\nmultiple sets of features to the output: (a) Input features from\nthe previous network layer passing through skip connections\nof transformer and CNN modules; (b) Contextual features\ncomputed by the transformer module passing through the skip\nconnection of the CNN module; (c) Local features computed\nby the CNN module based on input features reaching through\nthe skip connection of the transformer module; (d) Hybrid\nlocal-contextual features computed by the transformer-CNN\ncascade. Therefore, the main motivation for use of residual\ntransformer and residual CNN modules in ART blocks is to\nlearn an aggregated representation that synergistically com-\nbines lower-level input features along with their contextual,\nlocal, and hybrid local-contextual features.\nThe central segment of ResViT containing ART blocks\nacts as an information bottleneck for spatial and feature\ndimensions of medical image representations. On the one\nhand, the central segment processes feature maps that have\nbeen spatially downsampled by the encoder. This increases\nthe relative emphasis on mid- to high-level spatial informa-\ntion over lower-level information [58]. On the other hand,\nART blocks contain channel-compression (CC) modules that\nprocess concatenated feature maps from the previous ART\nblock and the transformer module. CC modules downsample\nthe concatenated maps in the feature dimension to distill a\ntask-relevant set of convolutional and contextual features.\nGiven the computational efﬁciency of convolutional layers,\nCNNs pervasively process feature maps at high spatial resolu-\ntion to improve sensitivity for local features [58]. In contrast,\nvision transformers include computationally exhaustive self-\nattention layers, so they typically process feature maps at rel-\natively lower resolution [49]. To ensure that both the residual\nCNNs and transformers in ART blocks receive input feature\nmaps at their expected resolutions, we incorporated down\nand upsampling blocks respectively at the input and output\nof transformer modules. This design ensures compatibility\nbetween the resolutions of feature maps extracted from CNN\nand transformer modules. In the remainder of this section,\nwe explain the detailed composition of each architectural\ncomponent, and we describe the loss functions to train ResViT.\n1) Encoder: The ﬁrst component of ResViT is a deep\nencoder network that contains a series of convolutional layers\nto capture a hierarchy of localized features of source images.\nNote that ResViT can serve as a uniﬁed synthesis model, so its\n4\nFig. 2: ResViT is a conditional image synthesis model that can unify various source-target modality conﬁgurations into a\nsingle model for improved practicality. a) During training, ResViT takes as input the entire set of images within the multi-\nmodal protocol, including both source and target modalities. For model consolidation across multiple synthesis tasks, various\nconﬁgurations of source-target modalities are expressed in terms of availability conditions in ResViT. b) During inference, the\nspeciﬁc source-target conﬁguration is determined via the availability conditions in each given test subject.\nencoder receives as input the full set of modalities within the\nimaging protocol, both source and target modalities (Fig. 2).\nSource modalities are input via an identity mapping, whereas\nunavailable target modalities are masked out:\nXG\ni = ai ·mi (1)\nwhere i denotes the channel index of the encoder input i ∈\n{1,2,...,I }, mi is the image for the ith modality. In Eq. (1),\nai denotes the availability of the ith modality:\nai =\n{\n1 if mi is a source modality\n0 if mi is a target modality (2)\nDuring training, various different conﬁgurations of source-\ntarget modalities are considered within the multi-modal proto-\ncol (e.g., T1, T2 →PD; T2, PD →T1; T1, PD →T2 for a three-\ncontrast MRI protocol). During inference, the speciﬁc source-\ntarget conﬁguration is determined via the availability condi-\ntions in individual test subjects. Given the availability-masked\nmulti-channel input, the encoder uses convolutional operators\nto learn latent structural representations shared across the\nconsolidated synthesis tasks. The encoder maps the multi-\nchannel input XG onto the embedded latent feature map\nfne ∈RNC,H,W via convolutional ﬁlters, where NC is the\nnumber of channels, H is the height and W is the width\nof the feature map. These representations are then fed to the\ninformation bottleneck.\n2) Information Bottleneck: Next, ResViT employs a residual\nbottleneck to distill task-relevant information in the encoded\nfeatures. Note that convolution operators have greater power\nin capturing localized features, whereas attention operators are\nmore sensitive to context-driven features. To simultaneously\nmaintain localization power and contextual sensitivity, we in-\ntroduce ART blocks that aggregate the information from resid-\nual convolutional and transformer branches (Fig. 1). Receiving\nas input the jth layer feature maps fj ∈RNC,H,W , an ART\nblock ﬁrst processes the feature maps via a vision transformer.\nDue to computational constraints, the transformer expects\nfeature maps at smaller resolutions compared to convolutional\nlayers. Thus, the spatial dimensions (H,W ) of fj ∈RNC,H,W\nare lowered by a downsampling block ( DS):\nf′\nj ∈RN′\nC,H′,W′\n= DS(fj) (3)\nwhere DS is implemented as a stack of strided convolutional\nlayers, f′\nj ∈RN′\nC,H′,W′\nare downsampled feature maps with\nW′ = W/M, H′ = H/M, M denoting the downsampling\nfactor. A transformer branch then processes f′\nj to extract\ncontextual information. Accordingly, f′\nj is ﬁrst split into NP =\nW′H′/P2 non-overlapping patches of size ( P,P ), and the\npatches are then ﬂattened to N′\nCP2-dimensional vectors. The\ntransformer embeds patches onto an ND-dimensional space\nvia trainable linear projections, supplemented with learnable\npositional encoding:\nz0 = [f1\nj PE; f2\nj PE; ... ; fNP\nj PE] +Ppos\nE (4)\nwhere z0 ∈RNP ,ND are the input patch embeddings, fp\nj ∈\nRN′\nCP2\nis the pth patch, PE is the embedding projection, and\nPpos\nE is the learnable positional encoding.\nNext, the transformer encoder processes patch embeddings\nvia a cascade of L layers of multi-head self-attention ( MSA)\n[92] and multi-layer perceptrons ( MLP) [93]. The output of\nthe lth layer in the transformer encoder is given as:\nz′\nl = MSA(LN(zl−1)) +zl−1 (5)\nzl = MLP(LN(z′\nl)) +z′\nl (6)\nMSA layers in Eq. 5 employ S separate self-attention heads:\nMSA(z) = [SA1(z); SA2(z); ... ; SAS(z)]Umsa (7)\nwhere SAs stands for the sth attention head with s ∈\n{1,2,...,S }and Umsa denotes the learnable tensor projecting\nattention head outputs. SA layers compute a weighted combi-\nnation of all elements of the input sequence z: SA(z) =Av\nwhere v is value, and attention weights Aa,b are taken as\npairwise similarity between the query q and key k:\nAa,b = softmax(qa kT\nb /ND\n0.5) (8)\nNote that q, k, v are respectively obtained as learnable\nprojections Tq, Tk, Tv of z.\nDALMAZ et al.: RESVIT: RESIDUAL VISION TRANSFORMERS FOR MULTI-MODAL MEDICAL IMAGE SYNTHESIS 5\nThe output of the transformer encoder zL is then deﬂattened\nto form g′\nj ∈ RND,H′,W′\n. Resolution of g′\nj is increased to\nmatch the size of input feature maps via an upsampling block\nUS based on transposed convolutions:\ngj ∈RNC,H,W = US(g′\nj) (9)\nwhere gj ∈ RNC,H,W are upsampled feature maps output\nby the transformer module. Channel-wise concatenation is\nperformed to fuse global context learned via the transformer\nwith localized features captured via convolutional operators.\nTo distill learned structural and contextual representations, the\nchannels of the concatenated feature maps are then compressed\nvia a channel compression ( CC) module:\nhj ∈RNC,H,W = CC(concat(fj,gj)) (10)\nwhere hj are compressed feature maps. CC uses two parallel\nconvolutional branches of varying kernel size. Finally, the\nfeature maps are processed via a residual CNN ( ResCNN)\n[58]:\nfj+1 ∈RNC,H,W = ResCNN(hj) (11)\nwhere fj+1 denotes the output of the ART block at the jth\nnetwork layer.\n3) Decoder: The last component of the generator is a deep\ndecoder based on transposed convolutional layers. Because\nResViT can serve as a uniﬁed model, its decoder can synthe-\nsize all contrasts within the multi-modal protocol regardless\nof the speciﬁc source-target conﬁguration (Fig. 2). The de-\ncoder receives as input the feature maps fA distilled by the\nbottleneck and produces multi-modality images ˆYG\ni ∈ˆYG in\nseparate channels, where Ais the total number of ART blocks,\nand ˆYG\ni denotes the ith synthesized modality.\n4) Parameter Sharing Transformers: Multiple ART blocks\nare used in the information bottleneck to increase the capacity\nof ResViT in learning contextual representations. That said,\nmultiple independent transformer blocks would inevitably\nelevate memory demand and risk of overﬁtting due to an\nexcessive number of parameters. To prevent these risks, a\nweight-sharing strategy is adopted where the model weights\nfor the transformer encoder are tied across separate ART\nblocks. The tied parameters include the projection matrices\nTq, Tk, Tv for query, key, value along with projection tensors\nfor attention heads Umsa in MSA layers, and weight matrices\nin MLP layers. Remaining parameters in transformer modules\nincluding down/upsampling blocks, patch embeddings and\npositional encodings are kept independent. During backprop-\nagation, updates for tied weights are computed based on the\nsummed error gradient across ART blocks.\n5) Discriminator: The discriminator in ResViT is based on\na conditional PatchGAN architecture [43]. The discrimina-\ntor performs patch-level differentiation between acquired and\nsynthetic images. This implementation increases sensitivity to\nlocalized details related to high-spatial-frequency information.\nAs ResViT can serve as a uniﬁed model by generating all\nmodalities in the multi-modal protocol including sources, an\navailability-guided selective discriminator is employed:\nXD\ni (source) =XG\ni = ai ·mi (12)\nXD\ni (syntarget) = (1−ai) ·YG\ni (13)\nXD\ni (acqtarget) = (1−ai) ·mi (14)\nwhere XD\ni (source) are source images, XD\ni (syntarget) are\nsynthesized target images, and XD\ni (acqtarget) are acquired\ntarget images. The conditional discriminator receives as input\nthe concatenation of source and target images:\nXD(synthetic) =concat(XD\ni (source),XD\ni (syntarget))\n(15)\nXD(acquired) =concat(XD\ni (source),XD\ni (acqtarget))\n(16)\nwhere XD(synthetic) is the concatenation of source and syn-\nthetic target images, and XD(acquired) is the concatenation\nof the source and acquired target images.\n6) Loss Function: The ﬁrst term in the loss function is a\npixel-wise L1 loss deﬁned between the acquired and synthe-\nsized target modalities:\nLpix =\nI∑\ni=1\n(1 −ai)E[||G(XG)i −mi||1] (17)\nwhere E denotes expectation, and G denotes the generator\nsubnetwork in ResViT. ResViT takes as input source modal-\nities to reconstruct them at the output. Thus, the second\nterm is a pixel-wise consistency loss between acquired and\nreconstructed source modalities based on an L1 distance:\nLrec =\nI∑\ni=1\naiE[||G(XG)i −mi||1] (18)\nThe last term is an adversarial loss deﬁned via the conditional\ndiscriminator (D):\nLadv = −E[D(XD(acquired)2]\n−E[(D(XD(synthetic)) −1)2] (19)\nThe three terms are linearly combined to form the overall\nobjective:\nLResV iT = λpixLpix + λrecLrec + λadvLadv (20)\nwhere λpix, λrec, and λadv are the weightings of the pixel-\nwise, reconstruction, and adversarial losses, respectively.\nB. Datasets\nWe demonstrated the proposed ResViT model on two\nmulti-contrast brain MRI datasets (IXI: https://brain-\ndevelopment.org/ixi-dataset/, BRATS [94]–[96]) and a\nmulti-modal pelvic MRI-CT dataset [97].\n1) IXI Dataset: T1-weighted, T2-weighted, and PD-weighted\nbrain MR images from 53 healthy subjects were analyzed.\n25 subjects were reserved for training, 10 were reserved\nfor validation, and 18 were reserved for testing. From each\nsubject, 100 axial cross-sections containing brain tissues were\nselected. Acquisition parameters were as follows. T 1-weighted\nimages: TE = 4.603ms, TR = 9.813ms, spatial resolution\n= 0.94 ×0.94×1.2mm3. T 2-weighted images: TE = 100ms,\nTR = 8178.34ms, spatial resolution = 0.94 ×0.94×1.2mm3.\nPD-weighted images: TE = 8ms, TR = 8178.34ms, spatial\nresolution = 0.94 ×0.94×1.2mm3. The multi-contrast images\nin this dataset were unregistered. Hence, T 2- and PD-weighted\nimages were spatially registered onto T 1-weighted images\nprior to modelling. Registration was performed via an afﬁne\ntransformation in FSL [98] based on mutual information.\n6\n2) BRATS Dataset: T1-weighted, T2-weighted, post-contrast\nT2-weighted, and T 2 Fluid Attenuation Inversion Recovery\n(FLAIR) brain MR images from 55 subjects were analyzed.\n25 subjects were reserved for training, 10 were reserved\nfor validation, and 20 were reserved for testing. From each\nsubject, 100 axial cross-sections containing brain tissues were\nselected. Please note that the BRATS dataset contains images\ncollected under various clinical protocols and scanners at\nmultiple institutions. As publicly shared, multi-contrast images\nare co-registered to the same anatomical template, interpolated\nto 1×1×1mm3 resolution and skull-stripped.\n3) MRI-CT Dataset: T2-weighted MR and CT images of\nthe male pelvis from 15 subjects were used. 9 subjects were\nreserved for training, 2 were reserved for validation, and 4\nwere reserved for testing. From each subject, 90 axial cross-\nsections were analysed. Acquisition parameters were as fol-\nlows. T2-weighted images: Group 1, TE = 97ms, TR = 6000-\n6600ms, spatial resolution = 0.875 ×0.875×2.5mm3. Group 2,\nTE = 91-102ms, TR = 12000-16000ms, spatial resolution =\n0.875-1.1×0.875-1.1×2.5mm3. CT images: Group 1, spatial\nresolution = 0.98 ×0.98×3mm3, Kernel = B30f. Group 2:\nspatial resolution = 0.1 ×0.1×2mm3, Kernel = FC17. This\ndataset contains images collected under various protocols and\nscanners for each modality. As publicly shared, multi-modal\nimages are co-registered onto T 2-weighted MR scans.\nC. Competing Methods\nWe demonstrated the proposed ResViT model against sev-\neral state-of-the-art image synthesis methods. The baseline\nmethods included convolutional models (task-speciﬁc mod-\nels: pGAN [24], pix2pix [43], medSynth [32]; uniﬁed mod-\nels: MM-GAN [41], pGAN uni), attention-augmented convolu-\ntional models (A-UNet [50], SAGAN [85]), and transformer\nmodels (task-speciﬁc: TransUNet [51], PTNet [57]; uniﬁed:\nTransUNetuni). Hyperparameters of each competing method\nwere optimized via identical cross-validation procedures.\n1) Convolutional models:\npGAN A convolutional GAN model with ResNet backbone\nwas considered [24]. pGAN comprises CNN-based generator\nand discriminator networks. Its generator follows an encoder-\nbottleneck-decoder pathway, where the encoder and decoder\nare identical to those in ResViT. The bottleneck contains a\ncascade of residual CNN blocks.\npix2pix A convolutional GAN model with U-Net backbone\nwas considered [43]. pix2pix has a CNN-based generator with\nan encoder-decoder structure tied with skip connections.\nmedSynth A convolutional GAN model with resid-\nual U-Net backbone was considered as provided at\nhttps://github.com/ginobilinie/medSynthesisV1 [32]. The gen-\nerator of medSynth contains a long-skip connection from the\nﬁrst to the last layer.\nMM-GAN A uniﬁed synthesis model based on a con-\nvolutional GAN was considered [41]. MM-GAN comprises\nCNN-based generator and discriminator networks, where the\ngenerator is based on U-Net. MM-GAN trains a single network\nunder various source-target modality conﬁgurations. The orig-\ninal MM-GAN architecture was directly adopted, except for\ncurriculum learning to ensure standard sample selection for\nall competing methods. The uniﬁcation strategy in MM-GAN\nmatches the uniﬁcation strategy in ResViT.\npGANuni A uniﬁed version of the pGAN model was trained\nto consolidate multiple synthesis tasks. The uniﬁcation proce-\ndure was identical to that of ResViT.\n2) Attention-augmented convolutional models:\nAttention U-Net (A-UNet)A CNN-based U-Net architec-\nture with additive attention gates was considered [50]. Here\nwe adopted the original A-UNet model as the generator of a\nconditional GAN model, where the discriminator was identical\nto that in ResViT.\nSelf-Attention GAN (SAGAN)A CNN-based GAN model\nwith self-attention modules incorporated into the generator\nwas considered [85]. Here we adapted the original SAGAN\nmodel designed for unconditional mapping by inserting the\nself-attention modules into the pGAN model as described in\n[99]. For fair comparison, the number and position of attention\nmodules in SAGAN were matched to those of transformer\nmodules in ResViT.\n3) Transformer models:\nTransUNet A recent hybrid CNN-transformer architecture\nwas considered [51]. Here, we adopted the original TransUNet\nmodel as the generator of a conditional GAN architecture with\nan identical discriminator to ResViT. We further replaced the\nsegmentation head with a convolutional layer for synthesis.\nPTNet A recent convolution-free transformer architecture\nwas considered [57]. Here we adopted the original PTNet\nmodel as the generator of a conditional GAN architecture with\nan identical discriminator to ResViT.\nTransUNetuni The TransUNet model was uniﬁed to consol-\nidate multiple synthesis tasks. The uniﬁcation procedure was\nidentical to that of ResViT.\nD. Architectural Details\nThe encoder in the ResViT model contained three convo-\nlutional layers of kernel size 7, 3, 3 respectively. The feature\nmap in the encoder output was of size R256,64,64, and this\ndimensionality was retained across the information bottleneck.\nThe decoder contained three convolutional layers of kernel\nsize 3, 3, 7 respectively. The information bottleneck contained\nnine ART blocks. The downsampling blocks preceding trans-\nformers contained two convolutional layers with stride 2 and\nkernel size 3. The upsampling blocks succeeding transformers\ncontained two transposed convolutional layers with stride 2\nand kernel size 3. Down and upsampling factors were set to\nM = 4. Channel compression lowered the number of channels\nfrom 512 to 256. The transformer encoder was adopted\nby extracting the transformer component of the ImageNet-\npretrained model R50+ViT-B/16 (https://github.com/google-\nresearch/vision transformer). The transformer encoder ex-\npected an input map of 16 ×16 spatial resolution. Patch ﬂat-\ntening was performed with size P = 1 yielding a sequence\nlength of 256 [49]. Note that transformer modules contain\nsubstantially higher number of parameters compared to con-\nvolutional modules. Thus, retaining a transformer in each\nART block results in signiﬁcant model complexity, inducing\ncomputational burden and suboptimal learning. To alleviate\nthese issues, transformer modules in ART blocks utilized tied\nweights, and they were only retained in a subset of ART blocks\nwhile remaining blocks reduced to residual CNNs.\nThe conﬁguration of transformer modules, i.e. their total\nnumber and position, was selected via cross-validation exper-\niments. Due to the extensive number of potential conﬁgura-\nDALMAZ et al.: RESVIT: RESIDUAL VISION TRANSFORMERS FOR MULTI-MODAL MEDICAL IMAGE SYNTHESIS 7\nConﬁguration T1, T2 →PD T 1, T2 →FLAIR MRI →CT\nPSNR PSNR PSNR\nA1 −A5 33.23 24.82 26.40\nA1 −A6 33.34 24.88 26.56\nA1 −A9 33.27 24.77 26.58\nA4 −A9 33.11 24.63 26.19\nA5 −A9 33.05 24.65 26.27\nA1 −A6 −A9 32.89 24.82 26.20\nTABLE I: Validation performance of candidate ResViT con-\nﬁgurations in representative synthesis tasks. Performance is\ntaken as PSNR (dB) between synthesized and reference target\nimages. Ai denotes the presence of a transformer module in\nthe ith ART block.\nTransformer size T1, T2 →PD T 1, T2 →FLAIR MRI →CT\nPSNR PSNR PSNR\nBase 33.34 24.88 26.56\nLarge 33.14 24.60 26.46\nTABLE II: Validation performance of ResViT models with\nvarying sizes of transformer modules in representative syn-\nthesis tasks.\ntions, a pre-selection process was implemented. Accordingly,\nperformance for a transformer module inserted in a single\nART block ( A1, A2, ..., A9) was measured, and the top half\nof positions was pre-selected. Composite conﬁgurations with\nmultiple transformer modules were then formed based on the\npre-selected blocks ( A1 −A5, A1 −A6 −A9 etc.). We ob-\nserved that retaining more than 2 modules elevated complexity\nwithout any performance beneﬁts. Validation performance for\nthe best performing conﬁgurations ( A1 −A5, A1 −A6, A1 −\nA9, A5 −A9, A4 −A9, A5 −A9, A1 −A6 −A9) are listed\nin Table I for three representative tasks (T 1, T 2 →PD in\nIXI, T1, T2 →FLAIR in BRATS, and MRI →CT in MRI-\nCT). Consistently across tasks, the ( A1 −A6) conﬁguration\nyielded near-optimal performance and so it was selected for\nall experiments thereafter.\nWe also tuned the intrinsic complexity of transformer\nmodules. To do this, two variant modules were examined:\n”base” and ”large”. The ”base” module contained 12 layers\nwith latent dimensionality Nd = 768, 12 attention heads, and\n3073 hidden units in each layer of the MLP. Meanwhile, the\n”large” module contained 24 layers with latent dimensionality\nNd = 1024, 16 attention heads, and 4096 hidden units in each\nlayer of the MLP. Validation performances based on the two\nvariant modules are listed in Table II. The ”base” module that\noffers higher performance for lower computational complexity\nwas selected for consequent experiments.\nE. Modeling Procedures\nFor fair comparisons among competing methods, all models\nwere implemented adversarially using the same PatchGAN\ndiscriminator and the loss function in Eq. 20. Task-speciﬁc\nmodels used adversarial and pixel-wise losses, whereas uniﬁed\nmodels used adversarial, pixel-wise, and reconstruction losses.\nLearning rate, number of epochs, and loss-term weighting\nwere selected via cross-validation. Validation performance was\nmeasured as Peak Signal to Noise Ratio (PSNR) on three\nrepresentative tasks (T1, T2 →PD in IXI, T 1, T2 →FLAIR in\nBRATS, and MRI →CT in MRI-CT). We considered different\nlearning rates in the set {10−5, 10−4, 2x10−4, 5x10−4, 10−3}\nand number of epochs in the set {5, 10, ..., 200 }. Eq. 20\ncontains only two degrees of freedom regarding the loss-\nResViT pGAN pix2pix medSynth A-UNet SAGAN TransUNet PTNet\nTime 98 60 60 81 70 63 78 224\nTABLE III: Average inference times (msec) of synthesis mod-\nels per single cross-section.\nterm weights, and prior studies have reported models with\nhigher weighting for pixel-wise over adversarial loss [24],\n[40]. Thus, we considered λpix in {20, 50, 100, 150 }and\nλadv = 1. Note that λrec = 0 by deﬁnition in task-speciﬁc\nmodels with ﬁxed conﬁguration of source and target modal-\nities, while λrec = λpix was used in uniﬁed models as both\nloss terms measure the L1-norm difference between reference\nand generated images for individual modalities. To minimize\npotential biases among competing methods, a common set of\nparameters that consistently yielded near-optimal performance\nwere prescribed for all methods: 2 ×10−4 learning rate, 100\ntraining epochs, λadv = 1,λpix = 100 for task-speciﬁc\nmodels, and λadv = 1,λrec = 100,λpix = 100 for uniﬁed\nmodels. All competing methods were trained via the Adam\noptimizer [100] with β1 = 0.5, β2 = 0.999. The learning rate\nwas constant for the ﬁrst 50 epochs and linearly decayed to 0\nin the remaining epochs. Transformer modules in TransUNet\nand ResViT were initiated with ImageNet pre-trained versions\nfor object classiﬁcation [101]. ART blocks were initiated\nwithout transformer modules and then ﬁne-tuned for 50 epochs\nfollowing insertion of transformers at a higher learning rate of\n10−3 as in [49]. Elevated learning rate during the second half\nof the training procedure was not adopted for other methods as\nit diminished performance. Modelling was performed via the\nPyTorch framework on Nvidia RTX A4000 GPUs. Inference\ntimes are listed in Table III.\nSynthesis quality was assessed via PSNR and Structural\nSimilarity Index (SSIM) [102]. Metrics were calculated be-\ntween ground truth and synthesized target images. Mean and\nstandard deviations of metrics were reported across an inde-\npendent test set, non-overlapping with training-validation sets.\nSigniﬁcance of performance differences were evaluated with\nsigned-rank tests (p <0.05). Tests were conducted on subject-\naverage metrics, except MRI → CT where cross-sectional\nmetrics were tested in each subject due to limited number\nof test subjects.\nF . Experiments\n1) Multi-Contrast MRI Synthesis: Experiments were con-\nducted on the IXI and BRATS datasets to demonstrate synthe-\nsis performance in multi-modal MRI. In the IXI dataset, one-\nto-one tasks of T 2 →PD; PD →T2 and many-to-one tasks\nof T 1, T 2 →PD; T 1, PD →T2; T 2, PD →T1 were consid-\nered. In the BRATS dataset, one-to-one tasks of T 2→FLAIR;\nFLAIR→T2, many-to-one tasks of T 1, T 2 → FLAIR;\nT1, FLAIR →T2; T2, FLAIR →T1 were considered. In both\ndatasets, task-speciﬁc ResViT models were compared against\npGAN, pix2pix, medSynth, A-UNet, SAGAN, TransUNet, and\nPTNet. Meanwhile, uniﬁed ResViT models were demonstrated\nagainst pGANuni, MM-GAN, and TransUNet uni.\n2) MRI to CT Synthesis: Experiments were performed on\nthe MRI-CT dataset to demonstrate across-modality synthesis\nperformance. A one-to-one synthesis task of deriving target\nCT images from source MR images was considered. The task-\nspeciﬁc ResViT model was compared against pGAN, pix2pix,\nmedSynth, A-UNet, SAGAN, TransUNet, and PTNet.\n8\nFig. 3: ResViT was demonstrated on the IXI dataset for two representative many-to-one synthesis tasks: a) T 1, T2 →PD, b)\nT2, PD →T1. Synthesized images from all competing methods are shown along with the source images and the reference\ntarget image. ResViT improves synthesis performance in regions that are depicted sub-optimally in competing methods. Overall,\nResViT generates images with lower artifact and noise levels and sharper tissue depiction.\nT1, T2 →PD T 1, PD→T2 T2, PD→T1 T2 →PD PD →T2\nPSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\nResViT 33.92 0.977 35.71 0.977 29.58 0.952 32.90 0.972 34.24 0.972\n±1.44 ±0.004±1.20±0.005±1.37±0.011±1.20±0.005±1.09±0.005\npGAN 32.91 0.966 33.95 0.965 28.71 0.941 32.20 0.963 33.05 0.963\n±0.94 ±0.005±1.06±0.006±1.08±0.013±1.00±0.005±0.95±0.007\npix2pix 32.25 0.974 33.62 0.973 28.35 0.949 30.72 0.956 30.74 0.950\n±1.24 ±0.006±1.31±0.009±1.24±0.016±1.28±0.007±1.63±0.012\nmedSynth 33.23 0.967 32.66 0.963 28.43 0.938 32.20 0.964 30.41 0.956\n±1.09 ±0.005±1.30±0.007±1.01±0.013±1.10±0.006±3.98±0.025\nA-UNet 32.24 0.963 32.43 0.959 28.95 0.916 32.05 0.960 33.32 0.961\n±0.92 ±0.014±1.36±0.007±1.21±0.013±1.04±0.009±1.08±0.007\nSAGAN 32.50 0.964 33.71 0.965 28.62 0.942 32.07 0.963 32.96 0.962\n±0.93 ±0.005±1.00±0.006±1.10±0.013±0.98±0.006±1.01±0.007\nTransUNet32.53 0.968 32.49 0.960 28.21 0.941 30.90 0.960 31.73 0.958\n±0.97 ±0.005±1.18±0.008±1.30±0.013±1.35±0.006±1.44±0.008\nPTNet 30.92 0.952 32.62 0.954 27.59 0.923 31.58 0.958 30.84 0.947\n±0.99±0.006±1.96±0.019±1.36±0.021±1.30±0.007±2.54±0.033\nTABLE IV: Performance of task-speciﬁc synthesis models in\nmany-to-one (T1, T2 →PD, T1, PD →T2, and T2, PD →T1)\nand one-to-one (T 2 →PD and PD →T2) tasks in the IXI\ndataset. PSNR (dB) and SSIM are listed as mean ±std across\ntest subjects. Boldface indicates the top-performing model for\neach task.\n3) Ablation Studies: Several lines of ablation experiments\nwere conducted to demonstrate the value of the individual\ncomponents of the ResViT model, including both architectural\ndesign elements and training strategies. Experiments were per-\nformed on three representative tasks: namely T 1, T2 →PD in\nIXI, T 1, T 2 →FLAIR in BRATS, and MRI →CT. First,\nwe assessed the performance contribution of the three main\ncomponents in ResViT: transformer modules, convolutional\nmodules and adversarial learning. Variant models were trained\nwhen transformer modules were ablated from ART blocks,\nwhen residual CNNs were ablated from transformer-retaining\nART blocks, and when the adversarial loss term and the\ndiscriminator were ablated. In addition to PSNR and SSIM, we\nmeasured the Fr ´echet inception distance (FID) [103] between\nthe synthesized and ground truth images to evaluate the\nimportance of adversarial learning.\nSecond, we probed the design and training procedures of\nART blocks. We assessed the utility of tied weights across\ntransformer modules, and multiple transformer-retaining ART\nblocks. Variant models were trained separately using untied\nweights in transformers, and based on a single transformer-\nretraining module at either ﬁrst or sixth ART blocks. We also\nexamined the importance of model initiation with ImageNet\npre-trained transformer modules, and delayed insertion of\ntransformer modules during training. Variant models were\nbuilt by using randomly initialized transformer modules, by\ninserting pre-trained transformer modules into ART blocks at\nthe beginning of training, and by inserting randomly initialized\ntransformer modules at the beginning of training.\nThird, we investigated the design of skip connections and\ndown/upsampling modules. We considered beneﬁts of exter-\nnal skip connections in ART blocks for residual learning.\nVariant models were trained by removing skip connections\naround either the transformer or convolution modules in ART.\nWe also assessed alternative designs for down/upsampling\nmodules in ART to mitigate added model complexity. In a\nﬁrst variant, original down/upsampling modules were replaced\nwith unlearned maxpooling modules for downsampling and\nbilinear interpolation modules for upsampling. In a second\nvariant, additional downsampling layers in the encoder and\nupsampling layers in the decoder were included in order to\nremove down/upsampling modules in ART blocks.\nNext, we inspected the relative strength of contextual fea-\ntures in the distilled task-relevant representations in ART\nblocks. For a quantitative assessment, we compared the L2-\nnorm of the contextual feature map derived by the transformer\nmodule against that of the input feature map to the ART block\nrelayed through the transformer’s skip connection. Note that\nDALMAZ et al.: RESVIT: RESIDUAL VISION TRANSFORMERS FOR MULTI-MODAL MEDICAL IMAGE SYNTHESIS 9\nFig. 4: ResViT was demonstrated on the BRATS dataset for two representative many-to-one synthesis tasks: a) T1, T2 →FLAIR,\nb) T2, FLAIR →T1. Synthesized images from all competing methods are shown along with the source images and the reference\nimage. ResViT improves synthesis performance, especially in pathological regions (e.g., tumors, lesions) in comparison to\ncompeting methods. Overall, ResViT images have better-delineated tissue boundaries and lower artifact/noise levels.\nT1, T2 →FLAIR T1, FLAIR→T2 T2, FLAIR→T1 T2→FLAIR FLAIR→T2\nPSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\nResViT 25.84 0.886 26.90 0.938 26.20 0.924 24.97 0.870 25.78 0.908\n±1.13 ±0.014 ±1.20 ±0.011 ±1.31 ±0.009 ±1.07 ±0.014±0.92 ±0.015\npGAN 24.89 0.867 26.51 0.922 25.72 0.918 24.01 0.864 25.09 0.894\n±1.10 ±0.015 ±1.13 ±0.012 ±1.54 ±0.011 ±1.15±0.014±1.52±0.015\npix2pix 24.31 0.862 26.12 0.920 25.80 0.918 23.15 0.869 24.52 0.883\n±1.21 ±0.015 ±1.53 ±0.012 ±1.72 ±0.011 ±1.93±0.016±0.88±0.014\nmedSynth23.93 0.863 26.44 0.921 25.72 0.914 23.36 0.864 24.41 0.888\n±1.45 ±0.016 ±0.76 ±0.011 ±1.62 ±0.012 ±1.88±0.017±0.82±0.014\nA-UNet 24.36 0.857 26.48 0.924 25.67 0.918 23.69 0.873 24.56 0.891\n±1.24 ±0.017 ±1.21 ±0.012 ±1.35 ±0.010 ±1.57±0.015±0.94±0.014\nSAGAN24.62 0.869 26.41 0.919 25.91 0.918 24.02 0.860 25.10 0.893\n±1.17 ±0.014 ±1.22 ±0.012 ±1.42 ±0.011 ±1.35±0.015±0.88±0.014\nTransUNet24.34 0.872 26.51 0.920 25.76 0.921 23.70 0.864 24.62 0.891\n±1.26 ±0.014 ±0.92 ±0.010 ±1.69 ±0.011 ±1.75±0.015±0.81±0.015\nPTNet 23.78 0.851 25.09 0.905 22.19 0.920 23.01 0.851 24.78 0.894\n±1.24 ±0.031 ±1.23 ±0.016 ±1.88 ±0.014 ±0.85±0.014±0.88±0.015\nTABLE V: Performance of task-speciﬁc synthesis models in\nmany-to-one tasks (T 1, T2 →FLAIR, T1, FLAIR →T2, and\nT2, FLAIR → T1) and one-to-one tasks (T 2→FLAIR and\nFLAIR→T2) across test subjects in the BRATS dataset. Bold-\nface indicates the top-performing model for each task.\nthese two maps are distilled via the channel compression (CC)\nmodule following concatenation. Thus, we also compared the\nL2-norm of the combination weights in the CC module for\nthe contextual versus input features.\nTo interpret the information that self-attention mechanisms\nfocus on during synthesis tasks, we computed and visualized\nthe attention maps as captured by the transformer modules\nin ResViT. Attention maps were calculated based on the\nAttention Rollout technique, and a single average map was\nextracted for a given transformer module [104].\nIV. RESULTS\nA. Multi-Contrast MRI Synthesis\n1) Task-Speciﬁc Synthesis Models: We demonstrated the\nperformance of ResViT in learning task-speciﬁc synthesis\nmodels for multi-contrast MRI. ResViT was compared against\nconvolutional models (pGAN, pix2pix, medSynth), attention-\naugmented CNNs (A-UNet, SAGAN), and recent transformer\narchitectures (TransUNet, PTNet). First, brain images of\nhealthy subjects in the IXI dataset were considered. PSNR\nand SSIM metrics are listed in Table IV for many-to-one and\none-to-one tasks. ResViT achieves the highest performance in\nboth many-to-one (p <0.05) and one-to-one tasks (p <0.05).\nOn average, ResViT outperforms convolutional models by\n1.71dB PSNR and 1.08 % SSIM, attention-augmented models\nby 1.40dB PSNR and 1.45 % SSIM, and transformer models\nby 2.33dB PSNR and 1.79 % SSIM (p<0.05). Representative\nimages for T 1, T2 →PD and T 2, PD →T1 are displayed in\nFig. 3a,b. Compared to baselines, ResViT synthesizes target\nimages with lower artifact levels and sharper tissue depiction.\nWe then demonstrated task-speciﬁc ResViT models on\nthe BRATS dataset containing images of glioma patients.\nPSNR and SSIM metrics are listed in Table V for many-\nto-one and one-to-one tasks. ResViT again achieves the\nhighest performance in many-to-one (p <0.05) and one-to-\none tasks (p <0.05), except T 2→FLAIR where A-UNet has\nslightly higher SSIM. On average, ResViT outperforms convo-\nlutional models by 1.01dB PSNR and 1.41 % SSIM, attention-\naugmented models by 0.84dB PSNR and 1.24 % SSIM, and\ntransformer models by 1.56dB PSNR and 1.63 % SSIM\n(p<0.05). Note that the BRATS dataset contains pathology\nwith large across-subject variability. As expected, attention-\naugmented models show relative beneﬁts against pure convo-\nlutional models, yet ResViT that explicitly models contextual\nrelationships still outperforms all baselines. Representative\ntarget images for T 1, T2 →FLAIR and T 2, FLAIR →T1 are\ndisplayed in Fig. 4a,b, respectively. Compared to baselines,\nResViT synthesizes target images with lower artifact levels and\nsharper tissue depiction. Importantly, ResViT reliably captures\nbrain lesions in patients in contrast to competing methods with\ninaccurate depictions including TransUNet.\nSuperior depiction of pathology in ResViT signals the\nimportance of ART blocks in simultaneously maintaining\nlocal precision and contextual consistency in medical image\nsynthesis. In comparison, transformer-based TransUNet and\nPTNet yield relatively limited synthesis quality that might\nbe attributed to several fundamental differences between the\nmodels. TransUNet uses only a transformer in its bottleneck\n10\nFig. 5 : ResViT uni was demon-\nstrated against other uniﬁed models\non brain MRI datasets for two rep-\nresentative tasks: a) T1, PD →T2 in\nIXI, b) T1, FLAIR →T2 in BRATS.\nSynthesized images from all com-\npeting methods are shown along\nwith the source images and the\nreference target image. ResViT uni\nimproves synthesis performance es-\npecially in pathological regions\n(tumors, lesions) in comparison\nto competing methods. Overall,\nResViTuni generates images with\nlower artifact and noise levels and\nmore accurate tissue depiction for\ntasks in both datasets.\nT1, T2 →PD T 1, PD→T2 T2, PD→T1\nPSNR SSIM PSNR SSIM PSNR SSIM\nResViTuni\n33.22 0.971 33.97 0.968 28.80 0.946\n±1.21 ±0.005 ±1.04 ±0.006 ±1.20 ±0.013\npGANuni\n31.86 0.965 32.90 0.962 27.86 0.937\n±1.09 ±0.005 ±0.91 ±0.006 ±1.04 ±0.014\nMM-GAN 30.73 0.955 30.91 0.951 27.23 0.925\n±1.16 ±0.006 ±1.61 ±0.013 ±1.24 ±0.015\nTransUNetuni\n30.30 0.956 30.77 0.949 26.86 0.930\n±1.44 ±0.007 ±1.10 ±0.014 ±1.16 ±0.013\nTABLE VI: Performance of uniﬁed synthesis models in many-\nto-one tasks T 1, T2 →PD, T1, PD →T2, and T 2, PD →T1)\nacross test subjects in the IXI dataset. Boldface indicates the\ntop-performing model for each task.\nwhile propagating shallow convolutional features via encoder-\ndecoder skip connections, and its decoder increases spatial\nresolution via bilinear upsampling that might be ineffective in\nsuppressing high-frequency artifacts [105]. In contrast, ResViT\ncontinues encoding and propagating convolutional features\nacross the information bottleneck to create a deeper feature\nrepresentation, and it employs transposed convolutions within\nupsampling modules to mitigate potential artifacts. PTNet is a\nconvolution-free architecture that relies solely on self-attention\noperators that have limited localization ability [49]. Instead,\nResViT is devised as a hybrid CNN-transformer architecture\nto improve sensitivity for both local and contextual features.\n2) Uniﬁed Synthesis Models: Task-speciﬁc models are\ntrained and tested to perform a single synthesis task to improve\nperformance, but a separate model has to be built for each task.\nNext, we demonstrated ResViT in learning uniﬁed synthesis\nmodels for multi-contrast MRI. A uniﬁed ResViT (ResViT uni)\nwas compared against uniﬁed convolutional (pGAN uni, MM-\nGAN) and transformer models (TransUNet uni). Performance\nof uniﬁed models were evaluated at test time on many-\nto-one tasks in IXI (Table VI) and BRATS (Table VII).\nResViTuni maintains the highest performance in many-to-one\ntasks in both IXI (p <0.05) and BRATS (p <0.05). In IXI,\nResViTuni outperforms pGANuni by 1.12dB PSNR and 0.70%\nSSIM, MM-GAN by 2.37dB PSNR and 1.80 % SSIM, and\nTransUNetuni by 2.69dB PSNR and 1.67 % SSIM (p <0.05).\nIn BRATS, ResViT outperforms pGAN uni by 0.74dB PSNR\nand 0.93 % SSIM, MM-GAN by 0.77dB PSNR and 0.90 %\nSSIM, and TransUNet uni by 1.08dB PSNR and 1.43 % SSIM\nT1, T2 →FLAIR T 1, FLAIR→T2 T2, FLAIR→T1\nPSNR SSIM PSNR SSIM PSNR SSIM\nResViTuni\n25.32 0.876 26.81 0.921 26.24 0.922\n±0.91 ±0.015 ±1.04 ±0.012 ±1.65 ±0.010\npGANuni\n24.46 0.865 26.23 0.914 25.46 0.912\n±0.99 ±0.014 ±1.08 ±0.012 ±1.20 ±0.009\nMM-GAN 24.20 0.861 26.10 0.915 25.75 0.916\n±1.34 ±0.015 ±1.48 ±0.014 ±1.64 ±0.011\nTransUNetuni\n24.11 0.863 26.05 0.912 24.96 0.901\n±1.19 ±0.014 ±1.46 ±0.013 ±1.24 ±0.012\nTABLE VII: Performance of uniﬁed synthesis models in many-\nto-one tasks (T 1, T 2 → FLAIR, T 1, FLAIR → T2, and\nT2, FLAIR →T1) across test subjects in the BRATS dataset.\nBoldface indicates the top-performing model for each task.\n(p<0.05). Representative target images are displayed in Fig.\n5. ResViT synthesizes target images with lower artifacts and\nsharper depiction than baselines. These results suggest that a\nuniﬁed ResViT model can successfully consolidate models for\nvarying source-target conﬁgurations.\nB. Across-Modality Synthesis\nWe also demonstrated ResViT in across-modality synthesis.\nT2-weighted MRI and CT images in the pelvic dataset were\nconsidered. ResViT was compared against pGAN, pix2pix,\nmedSynth, A-UNet, SAGAN, TransUNet, and PTNet. PSNR\nand SSIM metrics are listed in Table VIII. ResViT yields\nthe highest performance in each subject (p <0.05). On av-\nerage, ResViT outperforms convolutional models by 1.89dB\nPSNR and 3.20 % SSIM, attention-augmented models by\n0.75dB PSNR and 1.95 % SSIM, and transformer models\nby 1.52dB PSNR and 2.40 % SSIM (p<0.05). Representative\ntarget images are displayed in Fig. 6. Compared to baselines,\nResViT synthesizes target images with lower artifacts and\nmore accurate tissue depiction. Differently from multi-contrast\nResViT pGAN pix2pix medSynth A-UNet SAGAN TransUNet PTNet\nMRI→CT\nPSNR\n28.45 26.80 26.53 26.36 27.80 27.61 27.76 26.11\n±1.35 ±0.90 ±0.45 ±0.63 ±0.63 ±1.02 ±1.03 ±0.93\nSSIM\n0.931 0.905 0.898 0.894 0.913 0.910 0.914 0.900\n±0.009 ±0.008 ±0.004 ±0.009 ±0.004 ±0.006 ±0.009 ±0.015\nTABLE VIII: Performance for the across-modality synthesis\ntask (T 2-weighted MRI → CT) across test subjects in the\npelvic MRI-CT dataset. Boldface indicates the top-performing\nmodel for each task.\nDALMAZ et al.: RESVIT: RESIDUAL VISION TRANSFORMERS FOR MULTI-MODAL MEDICAL IMAGE SYNTHESIS 11\nFig. 6: ResViT was demonstrated on the pelvic MRI-CT dataset for the T 2-weighted MRI →CT task. Synthesized images\nfrom all competing methods are shown along with the source and reference images. ResViT enhances synthesis of relevant\nmorphology in the CT domain as evidenced by the elevated accuracy near bone structures.\nMRI, attention-augmented models and TransUNet offer more\nnoticeable performance beneﬁts over convolutional models.\nThat said, ResViT still maintains further elevated performance,\nparticularly near bone structures in CT images. This ﬁnding\nsuggests that the relative importance of contextual representa-\ntions is higher in MRI-CT synthesis. With the help of its resid-\nual transformer blocks, ResViT offers reliable performance\nwith accurate tissue depiction in this task.\nC. Ablation Studies\nWe performed a systematic set of experiments to demon-\nstrate the added value of the main components and training\nstrategies used in ResViT. First, we compared ResViT against\nablated variants where the convolutional modules in ART\nblocks, transformer modules in ART blocks, or the adversarial\nterm in training loss were separately removed. Table IX lists\nperformance metrics in the test set for three representative syn-\nthesis tasks. Consistently across tasks, ResViT yields optimal\nor near-optimal performance. ResViT achieves higher PSNR\nand SSIM in representative tasks compared to variants without\ntransformer or convolutional modules (p <0.05). It also yields\nlower FID than these variants, except in MRI →CT where\nablation of the convolutional module slightly decreases FID.\nImportantly, ResViT maintains notably lower FID compared\nto the variant without adversarial loss (albeit slightly lower\nSSIM in T 1, T 2 →FLAIR and PSNR, SSIM in MRI →\nCT). This is expected since FID is generally considered as\na more suited metric to examine the perceptual beneﬁts of\nadversarial learning than PSNR or SSIM that reﬂect heavier\ninﬂuence from relatively lower frequencies [103]. Representa-\ntive synthesized images are also displayed in Fig. 7a. ResViT\nimages more closely mimic the reference images, and show\ngreater spatial acuity compared against the variant without\nadversarial loss. Taken together, these results indicate that\nadversarial learning enables ResViT to more closely capture\nthe distributional properties of target-modality images.\nSecond, we compared ResViT against ablated variants\nwhere the weight tying procedure across transformer modules\nwas neglected, or transformer modules in one of the two\nretaining ART blocks were removed. Table X lists performance\nmetrics in the test set. ResViT yields higher performance\nthan variants across representative tasks (p <0.05), except for\nthe variant only retraining A6 that yields similar SSIM in\nT1, T 2 →PD. These results demonstrate the added value of\nthe weight tying procedure and the transformer conﬁguration\nin ResViT. We also compared ResViT against ablated variants\nwhere the pre-training of transformer modules or their delayed\ninsertion during training were selectively neglected, as listed\nin Table XI. Our results indicate that ResViT outperforms\nall ablated variants (p <0.05), except for the variant without\ndelayed insertion that yields similar SSIM in T 1, T2 →PD.\nT1, T2 →PD T1, T2 →FLAIR MRI→CT\nPSNR SSIM FID PSNR SSIM FID PSNR SSIM FID\nResViT 33.92 0.977 14.47 25.84 0.886 18.58 28.45 0.931 60.28\n±1.44 ±0.004 ±1.13 ±0.014 ±1.35 ±0.009\nw/o trans. 32.91 0.966 14.56 24.96 0.868 19.21 26.73 0.899 95.38\nmodules ±0.96 ±0.005 ±1.10 ±0.005 ±0.91 ±0.008\nw/o conv. 33.49 0.971 14.84 25.11 0.874 20.30 28.19 0.922 60.16\nmodules ±1.34 ±0.005 ±1.02 ±0.014 ±1.15 ±0.009\nw/o adv. 33.75 0.977 15.80 22.95 0.891 40.68 28.58 0.932 65.49\nloss ±1.45 ±0.005 ±1.93 ±0.015 ±1.13 ±0.007\nTABLE IX: Test performance of ResViT and variants ablated\nof transformer modules, convolutional modules or adversarial\nloss. FID is a summary metric across the entire test set.\nBoldface indicates the top-performing model for each task.\nT1, T2 →PD T1, T2 →FLAIR MRI→CT\nPSNR SSIM PSNR SSIM PSNR SSIM\nA1 −A6 33.92 0.977 25.84 0.886 28.45 0.931\n±1.44 ±0.004 ±1.13 ±0.014 ±1.35 ±0.009\nA1 −A6 33.72 0.973 25.19 0.879 28.16 0.923\n(untied weights) ±1.23 ±0.005 ±1.18 ±0.014 ±1.11 ±0.007\nA1 33.51 0.971 24.98 0.883 28.06 0.921\n±1.15 ±0.005 ±1.60 ±0.015 ±1.31 ±0.008\nA6 33.78 0.977 25.25 0.880 27.95 0.921\n±1.34 ±0.004 ±1.20 ±0.014 ±1.22 ±0.008\nTABLE X: Test performance of ResViT (A1 −A6) and variants\nablated of weight tying and individual transformer modules.\nBoldface indicates the top-performing model for each task.\nT1, T2 →PD T1, T2 →FLAIR MRI→CT\nPSNR SSIM PSNR SSIM PSNR SSIM\nResViT 33.92 0.977 25.84 0.886 28.45 0.931\n±1.44 ±0.004 ±1.13 ±0.014 ±1.35 ±0.009\nw/o pre-training 33.55 0.971 24.86 0.881 27.94 0.912\n±1.25 ±0.005 ±1.28 ±0.016 ±1.25 ±0.009\nw/o del. 33.35 0.977 24.89 0.873 28.01 0.924\ninsertion ±1.13 ±0.004 ±1.18 ±0.015 ±1.27 ±0.008\nw/o pre-training 33.58 0.971 24.74 0.869 27.66 0.913\nor del. insertion ±1.16 ±0.005 ±1.30 ±0.016 ±0.78 ±0.006\nTABLE XI: Test performance of ResViT and variants ablated\nof pre-training and delayed insertion procedures for transform-\ners. Boldface indicates the top-performing model for each task.\nThird, we examined the utility of the skip connections\nand down/upsampling blocks in the proposed architecture.\nWe compared ResViT against variants built by removing the\nskip connection around the transformer module or around\nthe CNN module in ART blocks. Table XII lists perfor-\nmance metrics in the test set. ResViT yields higher per-\nformance than all variants (p <0.05). Our results indicate\nthat ResViT beneﬁts substantially from residual learning in\nART blocks. We also compared ResViT against variants built\nby replacing down/upsampling modules in ART blocks with\nunlearned maxpooling/bilinear interpolation modules, and by\nincreasing encoder downsampling and decoder upsampling\nrates to remove down/upsampling modules in ART blocks\nentirely. ResViT outperforms all variants as listed in Ta-\nble XII (p <0.05), except for MRI →CT where the variant\n12\nFig. 7: a) ResViT was compared against a variant where the adversarial term was removed from the loss function. Representative\nresults are shown for T 1, T 2 →PD in IXI, T 1, T 2 →FLAIR in BRATS, and MRI →CT in the pelvic dataset. Adversarial\nloss improves the acuity of synthesized images. b) Representative results from ResViT and pGAN are shown along with the\nreference images for T 2, FLAIR →T1, T 1, FLAIR →T2, and T 1, T 2 →FLAIR in BRATS; and MRI →CT in the pelvic\ndataset. Error maps between the synthetic and reference images for each method are displayed, along with the attention map\nfor the ﬁrst transformer module of ResViT. Here, the attention maps were overlaid onto the reference image for improved\nvisualization. Attention maps focus on image regions where ResViT substantially reduces synthesis errors compared to pGAN.\nT1, T2 →PD T1, T2 →FLAIR MRI→CT\nPSNR SSIM PSNR SSIM PSNR SSIM\nResViT 33.92 0.977 25.84 0.886 28.45 0.931\n±1.44 ±0.004 ±1.13 ±0.014 ±1.35 ±0.009\nw/o skip around 28.24 0.942 25.02 0.864 26.94 0.906\nconv. modules ±1.27 ±0.009 ±0.98 ±0.016 ±0.73 ±0.007\nw/o skip around 31.53 0.962 24.06 0.868 27.08 0.908\ntrans. modules ±1.26 ±0.006 ±1.28 ±0.014 ±0.80 ±0.006\nART with unlearned 33.73 0.969 25.33 0.884 28.16 0.931\ndown/upsampling ±1.19 ±0.005 ±1.11 ±0.014 ±1.04 ±0.007\nART w/o 31.51 0.961 23.61 0.867 26.79 0.915\ndown/upsampling ±1.27 ±0.006 ±1.53 ±0.015 ±0.62 ±0.006\nTABLE XII: Test performance of ResViT and variants built by:\nremoving skip connections in convolutional modules, remov-\ning skip connections in transformer modules, using unlearned\ndown/upsampling blocks in ART, removing down/upsampling\nblocks in ART via a higher degree of down/upsampling in the\nencoder/decoder. Boldface indicates the top-performing model\nfor each task.\nwith unlearned down/upsampling and ResViT yield similar\nSSIM. These results demonstrate the beneﬁts of the proposed\ndown/upsampling scheme in ResViT.\nNext, we inspected the relative strength of transformer-\nderived contextual features in distilled representations within\nART blocks. To do this, we computed the L2-norms of\ncontextual feature maps output by the transformer module,\nand input feature maps from the previous ART block relayed\nthrough the skip connection of the transformer module. We\nalso computed the relative weighting of the two feature maps\nas the L2-norms of respective combination weights in the\nchannel compression (CC) module. Measurements for ResViT\nmodels trained in representative tasks are listed in Table XIII.\nWe ﬁnd that contextual and input feature maps, and their\nrespective combination weights in CC blocks have comparable\nstrength, demonstrating that contextual features are a substan-\ntial component of image representations in ART blocks.\nT1, T2 →PD T1, T2 →FLAIR MRI→CT\ng 277.88 400.90 421.46\nf 536.48 571.23 636.93\nCC weights forg 96.5 112.04 72.88\nCC weights forf 226.08 169.15 116.70\nTABLE XIII: Feature maps and corresponding combination\nweights for the channel compression (CC) module were in-\nspected in ResViT. Averaged across the test set and ART\nblocks, L2-norm of feature maps from the transformer module\n(g) and feature maps input by the previous ART block ( f) are\nlisted along with combination weights for g and for f.\nLastly, we wanted to visually interpret the beneﬁts of\nthe self-attention mechanisms in ResViT towards synthesis\nperformance. Fig. 7b displays representative attention maps in\nResViT. Synthetic images and error maps are also shown for\nResViT as well as pGAN, which generally offered the closest\nperformance to ResViT in our experiments. We ﬁnd that\nthe attention maps exhibit higher intensity in critical regions\nsuch as brain lesions in multi-contrast MRI and pelvic bone\nstructure in MR-to-CT synthesis. Importantly, these regions\nof higher attentional focus are also the primary regions where\nthe synthesis errors are substantially diminished with ResViT\ncompared to pGAN. Taken together, these results suggest\nthat the transformer-based ResViT model captures contextual\nrelationships related to both healthy and pathological tissues\nto improve synthesis performance.\nV. DISCUSSION\nIn this study, we proposed a novel adversarial model for\nimage translation between separate modalities. Traditional\nGANs employ convolutional operators that have limited ability\nto capture long-range relationships among distant regions [46].\nThe proposed model aggregates convolutional and transformer\nbranches within a residual bottleneck to preserve both local\nprecision and contextual sensitivity. To our knowledge, this is\nDALMAZ et al.: RESVIT: RESIDUAL VISION TRANSFORMERS FOR MULTI-MODAL MEDICAL IMAGE SYNTHESIS 13\nthe ﬁrst adversarial model for medical image synthesis with a\ntransformer-based generator. We further introduced a weight-\nsharing strategy among transformer modules to lower model\ncomplexity. Finally, a uniﬁcation strategy was implemented to\nlearn an aggregate model that copes with numerous source-\ntarget conﬁgurations without training separate models.\nWe demonstrated ResViT for missing modality synthesis\nin multi-contrast MRI and MRI-CT imaging. ResViT outper-\nformed several state-of-the-art convolutional and transformer\nmodels in one-to-one and many-to-one tasks. We trained all\nmodels with an identical loss function to focus on architectural\ninﬂuences to synthesis performance. In unreported experi-\nments, we also trained competing methods that were proposed\nwith different loss functions using their original losses, includ-\ning PTNet with mean-squared loss [57] and medSynth with\nmean-squared, adversarial and gradient-difference losses [32].\nWe observed that ResViT still maintains similar performance\nbeneﬁts over competing methods in these experiments. Yet, it\nremains important future work to conduct an in-depth assess-\nment of optimal loss terms for ResViT, including gradient-\ndifference and difﬁculty-aware losses for the generator [32],\n[106], [107], and edge-preservation and binary cross-entropy\nlosses for the discriminator [107], [108].\nTrained with image-average loss terms, CNNs have difﬁ-\nculty in coping with atypical anatomy that substantially varies\nacross subjects [24], [43]. To improve generalization, recent\nstudies have proposed self-attention mechanisms in GAN mod-\nels over spatial or channel dimensions [50], [85]. Speciﬁcally,\nattention maps are used for multiplicative modulation of CNN-\nderived feature maps. This modulation encourages the network\nto focus on critical image regions with relatively limited\ntask performance. While attention maps can be distributed\nacross image regions, they mainly capture implicit contextual\ninformation via modiﬁcation of local CNN features. Since\nfeature representations are primarily extracted via convolu-\ntional ﬁltering, the resulting model can still manifest limited\nexpressiveness for global context. In contrast, the proposed\narchitecture uses dedicated transformer blocks to explicitly\nmodel long-range spatial interactions in medical images.\nFew recent studies have independently proposed\ntransformer-based models for medical image synthesis [55]–\n[57]. In [56], a transformer is included in the discriminator\nof a traditional GAN for MR-to-PET synthesis. In [57],\na UNet-inspired transformer architecture is proposed for\ninfant MRI synthesis [57]. Differing from these efforts, our\nwork makes the following contributions. (1) Compared to\n[56] that uses transformers to learn a prior for target PET\nimages, we employ transformers in ResViT’s generator to\nlearn latent contextual representations of source images. (2)\nUnlike [57] that uses mean-squared error loss amenable\nto over-smoothing of target images [24], we leverage an\nadversarial loss to preserve realism. (3) While [57] uses a\nconvolution-free transformer architecture, we instead propose\na hybrid architecture that combines localization capabilities\nof CNNs with contextual sensitivity of transformers. (4)\nWhile [56] and [57] consider only task-speciﬁc, one-to-one\nsynthesis models, here we uniquely introduce many-to-one\nsynthesis models and a uniﬁed model that generalizes across\nmultiple source-target conﬁgurations.\nUNet-style models follow an encoder-decoder architecture\nwith an hourglass structure [43]. Because spatial resolution is\nsubstantially lower in the midpoint of the hourglass (e.g. 16x16\nmaps), these models typically introduce skip connections be-\ntween the encoder and decoder layers to facilitate preservation\nof low-level features. In contrast, ResViT is a ResNet-style\nmodel where encoded representations pass through a bottle-\nneck of residual blocks before reaching the decoder [58], and\nencoder-decoder skip connections are omitted due to several\nreasons. First, ResViT maintains relatively high resolution at\nthe output of its encoder (e.g. 64x64 maps), so its bottleneck\nrepresents relatively lower-level information. Second, each\nART block is organized as a transformer-CNN cascade with\nskip connections around both modules, creating a residual path\nbetween the input and output of each block. This eventually\nbridges the encoder output to the decoder input, creating a\nnative residual path in ResViT. Lastly, we observed during\nearly stages of the study that a variant model that included\nencoder-decoder skip connections caused a minor performance\ndrop, suggesting that these extra connections might reduce the\neffectiveness of the central information bottleneck.\nHere, ResViT models were initialized with transformers pre-\ntrained on 16x16 input feature maps. In turn, 256x256 images\nwere 16-fold downsampled cumulatively across the encoder\nand transformer modules, and the transformer used a patch\nsize of P=1 and sequence length of 256. Several strategies\ncan be adopted to use ResViT at different image resolutions.\nIn a ﬁrst scenario, the downsampling rate and patch size\ncan be preserved, while the sequence length is adjusted. For\ninstance, a 512x512 image would be downsampled to a 32x32\nfeature map, resulting in a sequence of 1024 patches. While a\ntransformer pre-trained on 32x32 maps would be ideal, vision\ntransformers can reliably handle variable sequence lengths\nwithout retraining so the original transformer can still be\nused [49]. Note that longer sequences would incur a quadratic\nincrease in processing and memory load in both cases [49].\nIn a second scenario, the original transformer with sequence\nlength 256 can be maintained, while either the patch size or\nthe downsampling rate is adjusted. For a 512x512 image, P=2\n(2x2 patches) on a 32x32 map (16-fold downsampled) or P=1\non a 16x16 map (32-fold downsampled) could be used. Both\noptions would achieve on par computational complexity to\nthe original architecture, albeit the transformer would process\nfeature maps at a relatively lower resolution compared to the\nresolution of the input image. It is unlikely that this would\nsigniﬁcantly affect ResViT’s sensitivity to local features since\nthe primary component of ART that captures local features is\nthe residual CNN module whose resolution can be preserved.\nIf the input image does not have a power-of-two size, the\nabovementioned strategies can be adopted after zero-padding\nto round up the resolution to the nearest power of two, or\nby implementing the encoder with non-integer downsampling\nrates [109]. Note that computer vision studies routinely ﬁne-\ntune transformers at different image resolutions than encoun-\ntered during pre-training without performance loss [49], so\nResViT might also demonstrate similar behavior. It remains\nimportant future work to investigate the comparative utility of\nthe discussed resolution-adaptation strategies in medical image\nsynthesis.\nSeveral lines of development can help further improve\nResViT’s performance. Here, we considered synthesis tasks\nin which source and target modalities were registered prior\nto training, and they were paired across subjects. When\n14\nregistration accuracy is limited, a spatial registration block\ncan be incorporated into the network. Furthermore, a cycle-\nconsistency loss [44] can be incorporated in the optimization\nobjective to allow the use of unregistered images. This latter\nstrategy would also permit training of ResViT models on\nunpaired datasets [76], [77]. Data requirements for model\ntraining can be further alleviated by adopting semi-supervised\nstrategies that allow mixing of paired and unpaired training\ndata [75], or that would enable training of synthesis models di-\nrectly from undersampled acquisitions [110]. Finally, ResViT\nmight beneﬁt from incorporation of multi-scale modules in the\ndecoder to improve preservation of ﬁne image details [108].\nVI. C ONCLUSION\nHere we introduced a novel synthesis approach for multi-\nmodal imaging based on a conditional deep adversarial net-\nwork. In an information bottleneck, ResViT aggregates convo-\nlutional operators and vision transformers, thereby improving\ncapture of contextual relations while maintaining localization\npower. A uniﬁed implementation was introduced that prevents\nthe need to rebuild models for varying source-target conﬁg-\nurations. ResViT achieves superior synthesis quality to state-\nof-the-art approaches in multi-contrast brain MRI and multi-\nmodal pelvic MRI-CT datasets. Therefore, it holds promise as\na powerful candidate for medical image synthesis.\nREFERENCES\n[1] B. J. Pichler, M. S. Judenhofer, and C. Pfannenberg, Multimodal Imaging\nApproaches: PET/CT and PET/MRI, 2008, pp. 109–132.\n[2] B. Moraal, S. Roosendaal, P. Pouwels, H. Vrenken, R. Schijndel,\nD. Meier, C. Guttmann, J. Geurtset al., “Multi-contrast, isotropic, single-\nslab 3d MR imaging in multiple sclerosis,” Eur. Radiol., vol. 18, pp.\n2311–2320, 2008.\n[3] B. Thukral, “Problems and preferences in pediatric imaging,” Indian J.\nRadiol. Imaging, vol. 25, pp. 359–364, 2015.\n[4] K. Krupa and M. Bekiesi ´nska-Figatowska, “Artifacts in magnetic reso-\nnance imaging,” Pol. J. Radiol., vol. 80, pp. 93–106, 2015.\n[5] J. E. Iglesias, E. Konukoglu, D. Zikic, B. Glocker, K. Van Leemput,\nand B. Fischl, “Is synthesizing MRI contrast useful for inter-modality\nanalysis?” in MICCAI, 2013, pp. 631–638.\n[6] Y . Huo, Z. Xu, S. Bao, A. Assad, R. G. Abramson, and B. A. Land-\nman, “Adversarial synthesis learning enables segmentation without target\nmodality ground truth,” in Proceedings of IEEE ISBI, 2018, pp. 1217–\n1220.\n[7] S. Farsiu, D. Robinson, M. Elad, and P. Milanfar, “Advances and chal-\nlenges in super-resolution,” Int. J. Imaging. Syst. Technol., vol. 14, pp.\n47–57, 2004.\n[8] D. H. Ye, D. Zikic, B. Glocker, A. Criminisi, and E. Konukoglu, “Modal-\nity propagation: Coherent synthesis of subject-speciﬁc scans with data-\ndriven regularization,” inMICCAI, 2013, pp. 606–613.\n[9] C. Catana, A. van der Kouwe, T. Benner, C. J. Michel, M. Hamm,\nM. Fenchel, B. Fischl, B. Rosen et al., “Toward implementing an MRI-\nbased PET attenuation-correction method for neurologic studies on the\nMR-PET brain prototype,” J. Nucl. Med., vol. 51, no. 9, pp. 1431–1438,\n2010.\n[10] J. Lee, A. Carass, A. Jog, C. Zhao, and J. Prince, “Multi-atlas-based CT\nsynthesis from conventional MRI with patch-based reﬁnement for MRI-\nbased radiotherapy planning,” in Proceedings of SPIE, vol. 10133, 2017,\np. 101331I.\n[11] S. Roy, A. Jog, A. Carass, and J. L. Prince, “Atlas based intensity trans-\nformation of brain MR images,” in Multimodal Brain Image Analysis,\n2013, pp. 51–62.\n[12] Y . Huang, L. Shao, and A. F. Frangi, “Simultaneous super-resolution and\ncross-modality synthesis of 3d medical images using weakly-supervised\njoint convolutional sparse coding,” Proceedings of CVPR, pp. 5787–\n5796, 2017.\n[13] ——, “Cross-modality image synthesis via weakly coupled and geom-\netry co-regularized joint dictionary learning,” IEEE Trans. Med. Imag.,\nvol. 37, no. 3, pp. 815–827, 2018.\n[14] C. Zhao, A. Carass, J. Lee, Y . He, and J. L. Prince, “Whole brain\nsegmentation and labeling from CT using synthetic MR images,” in\nMachine Learning in Medical Imaging, 2017, pp. 291–298.\n[15] A. Jog, A. Carass, S. Roy, D. L. Pham, and J. L. Prince, “Random forest\nregression for magnetic resonance image synthesis,” Med. Image. Anal.,\nvol. 35, pp. 475–488, 2017.\n[16] H. Van Nguyen, K. Zhou, and R. Vemulapalli, “Cross-domain synthesis\nof medical images using efﬁcient location-sensitive deep network,” in\nMICCAI, 2015, pp. 677–684.\n[17] R. Vemulapalli, H. Van Nguyen, and S. K. Zhou, “Unsupervised cross-\nmodal synthesis of subject-speciﬁc scans,” inProceedings of ICCV, 2015,\npp. 630–638.\n[18] Y . Wu, W. Yang, L. Lu, Z. Lu, L. Zhong, M. Huang, Y . Feng, Q. Feng\net al., “Prediction of CT substitutes from MR images based on local\ndiffeomorphic mapping for brain PET attenuation correction,” J. Nucl.\nMed., vol. 57, no. 10, pp. 1635–1641, 2016.\n[19] D. C. Alexander, D. Zikic, J. Zhang, H. Zhang, and A. Criminisi, “Image\nquality transfer via random forest regression: Applications in diffusion\nMRI,” in MICCAI, 2014, pp. 225–232.\n[20] T. Huynh, Y . Gao, J. Kang, L. Wang, P. Zhang, J. Lian, and D. Shen,\n“Estimating CT image from MRI data using structured random forest\nand auto-context model,” IEEE Trans. Med. Imag., vol. 35, no. 1, pp.\n174–183, 2016.\n[21] P. Coupe, J. V . Manj ´on, M. Chamberland, M. Descoteaux, and B. Hiba,\n“Collaborative patch-based super-resolution for diffusion-weighted im-\nages,” NeuroImage, vol. 83, pp. 245–261, 2013.\n[22] V . Sevetlidis, M. V . Giuffrida, and S. A. Tsaftaris, “Whole image synthe-\nsis using a deep encoder-decoder network,” in Simulation and Synthesis\nin Medical Imaging, 2016, pp. 127–137.\n[23] A. Chartsias, T. Joyce, M. V . Giuffrida, and S. A. Tsaftaris, “Multimodal\nMR synthesis via modality-invariant latent representation,” IEEE Trans.\nMed. Imag., vol. 37, no. 3, pp. 803–814, 2018.\n[24] S. U. Dar, M. Yurt, L. Karacan, A. Erdem, E. Erdem, and T. C ¸ ukur,\n“Image synthesis in multi-contrast MRI with conditional generative\nadversarial networks,”IEEE Trans. Med. Imag., vol. 38, no. 10, pp. 2375–\n2388, 2019.\n[25] C. Bowles, C. Qin, C. Ledig, R. Guerrero, R. Gunn, A. Hammers,\nE. Sakka, D. Dickie et al., “Pseudo-healthy image synthesis for white\nmatter lesion segmentation,” in Simulation and Synthesis in Medical\nImaging, 2016, pp. 87–96.\n[26] N. Cordier, H. Delingette, M. Le, and N. Ayache, “Extended modality\npropagation: Image synthesis of pathological cases,” IEEE Trans. Med.\nImag., vol. 35, pp. 2598–2608, 2016.\n[27] T. Joyce, A. Chartsias, and S. A. Tsaftaris, “Robust multi-modal MR\nimage synthesis,” in MICCAI, 2017, pp. 347–355.\n[28] W. Wei, E. Poirion, B. Bodini, S. Durrleman, O. Colliot, B. Stankoff,\nand N. Ayache, “Fluid-attenuated inversion recovery MRI synthesis\nfrom multisequence MRI using three-dimensional fully convolutional\nnetworks for multiple sclerosis,”J. Med. Imaging, vol. 6, no. 1, p. 014005,\n2019.\n[29] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y . Bengio, “Generative adversarial networks,”\nProceedings of NIPS, vol. 24, 2014.\n[30] A. Beers, J. Brown, K. Chang, J. Campbell, S. Ostmo, M. Chiang, and\nJ. Kalpathy-Cramer, “High-resolution medical image synthesis using pro-\ngressively grown generative adversarial networks,” arXiv:1805.03144,\n2018.\n[31] B. Yu, L. Zhou, L. Wang, J. Fripp, and P. Bourgeat, “3d cGAN based\ncross-modality MR image synthesis for brain tumor segmentation,” Pro-\nceedings of IEEE ISBI, pp. 626–630, 2018.\n[32] D. Nie, R. Trullo, J. Lian, L. Wang, C. Petitjean, S. Ruan, and Q. Wang,\n“Medical image synthesis with deep convolutional adversarial networks,”\nIEEE Trans. Biomed. Eng., vol. 65, no. 12, pp. 2720–2730, 2018.\n[33] K. Armanious, C. Jiang, M. Fischer, T. K ¨ustner, T. Hepp, K. Nikolaou,\nS. Gatidis, and B. Yang, “MedGAN: Medical image translation using\nGANs,” Comput. Med. Imaging Grap., vol. 79, p. 101684, 2019.\n[34] D. Lee, J. Kim, W.-J. Moon, and J. C. Ye, “CollaGAN: Collaborative\nGAN for missing image data imputation,” inProceedings of CVPR, 2019,\npp. 2487–2496.\n[35] H. Li, J. C. Paetzold, A. Sekuboyina, F. Koﬂer, J. Zhang, J. S. Kirschke,\nB. Wiestler, and B. Menze, “Diamondgan: Uniﬁed multi-modal gener-\native adversarial networks for MRI sequences synthesis,” in MICCAI,\n2019, pp. 795–803.\n[36] T. Zhou, H. Fu, G. Chen, J. Shen, and L. Shao, “Hi-net: Hybrid-fusion\nnetwork for multi-modal MR image synthesis,”IEEE Trans. Med. Imag.,\nvol. 39, no. 9, pp. 2772–2781, 2020.\n[37] H. Lan, A. Toga, and F. Sepehrband, “Sc-GAN: 3d self-attention condi-\ntional GAN with spectral normalization for multi-modal neuroimaging\nsynthesis,” bioRxiv:2020.06.09.143297, 2020.\n[38] M. Yurt, S. U. Dar, A. Erdem, E. Erdem, K. K. Oguz, and T. C ¸ ukur,\n“mustGAN: multi-stream generative adversarial networks for MR image\nsynthesis,” Med. Image. Anal., vol. 70, p. 101944, 2021.\n[39] H. Yang, X. Lu, S.-H. Wang, Z. Lu, J. Yao, Y . Jiang, and P. Qian, “Syn-\nthesizing multi-contrast MR images via novel 3d conditional variational\nauto-encoding GAN,” Mob. Netw. Appl., vol. 26, pp. 1–10, 2021.\n[40] B. Yu, L. Zhou, L. Wang, Y . Shi, J. Fripp, and P. Bourgeat, “Ea-\nGANs: Edge-aware generative adversarial networks for cross-modality\nMR image synthesis,” IEEE Trans. Med. Imag., vol. 38, no. 7, pp. 1750–\n1762, 2019.\n[41] A. Sharma and G. Hamarneh, “Missing MRI pulse sequence synthesis\nusing multi-modal generative adversarial network,” IEEE Trans. Med.\nImag., vol. 39, pp. 1170–1183, 2020.\nDALMAZ et al.: RESVIT: RESIDUAL VISION TRANSFORMERS FOR MULTI-MODAL MEDICAL IMAGE SYNTHESIS 15\n[42] G. Wang, E. Gong, S. Banerjee, D. Martin, E. Tong, J. Choi, H. Chen,\nM. Wintermark et al., “Synthesize high-quality multi-contrast magnetic\nresonance imaging from multi-echo acquisition using multi-task deep\ngenerative model,” IEEE Trans. Med. Imag., vol. 39, no. 10, pp. 3089–\n3099, 2020.\n[43] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation\nwith conditional adversarial networks,”Proceedings of CVPR, pp. 1125–\n1134, 2017.\n[44] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image\ntranslation using cycle-consistent adversarial networks,” Proceedings of\nICCV, pp. 2242–2251, 2017.\n[45] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”\nin Proceedings of CVPR, 2018, pp. 7794–7803.\n[46] N. Kodali, J. Hays, J. Abernethy, and Z. Kira, “On convergence and\nstability of GANs,” arXiv:1705.07215, 2017.\n[47] A. Adam, A. Dixon, J. Gillard, C. Schaefer-Prokop, R. Grainger, and\nD. Allison, Grainger & Allison’s Diagnostic Radiology. Elsevier Health\nSciences, 2014.\n[48] D. Ellison, S. Love, L. Chimelli, B. Harding, J. Lowe, H. Vinters,\nS. Brandner, and W. Yong, Neuropathology: A Reference Text of CNS\nPathology. Elsevier Health Sciences, 2012.\n[49] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer et al., “An image is worth 16x16\nwords: Transformers for image recognition at scale,” arXiv:2010.11929,\n2021.\n[50] O. Oktay, J. Schlemper, L. L. Folgoc, M. J. Lee, M. Heinrich, K. Misawa,\nK. Mori, S. G. McDonagh et al., “Attention U-Net: Learning where to\nlook for the pancreas,” arXiv:1804.03999, 2018.\n[51] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille\net al., “Transunet: Transformers make strong encoders for medical image\nsegmentation,”arXiv:2102.04306, 2021.\n[52] Y . Luo, Y . Wang, C. Zu, B. Zhan, X. Wu, J. Zhou, D. Shen, and L. Zhou,\n“3d transformer-gan for high-quality pet reconstruction,” in MICCAI,\n2021, pp. 276–285.\n[53] Z. Zhang, L. Yu, X. Liang, W. Zhao, and L. Xing, “Transct: Dual-path\ntransformer for low dose computed tomography,” in MICCAI, 2021, pp.\n55–64.\n[54] Y . Korkmaz, S. U. Dar, M. Yurt, M. ¨Ozbey, and T. C ¸ ukur, “Unsupervised\nmri reconstruction via zero-shot learned adversarial transformers,” IEEE\nTrans. Med. Imag., pp. 1–1, 2022.\n[55] S. A. Kamran, K. F. Hossain, A. Tavakkoli, S. L. Zuckerbrod,\nK. M. Sanders, and S. A. Baker, “VtGAN: Semi-supervised retinal\nimage synthesis and disease prediction using vision transformers,”\narXiv:2104.06757, 2021.\n[56] H.-C. Shin, A. Ihsani, S. Mandava, S. T. Sreenivas, C. Forster, J. Cha,\nand A. D. N. Initiative, “GANbert: Generative adversarial networks with\nbidirectional encoder representations from transformers for MRI to PET\nsynthesis,” arXiv:2008.04393, 2020.\n[57] X. Zhang, X. He, J. Guo, N. Ettehadi, N. Aw, D. Semanek, J. Posner,\nA. Laine et al., “Ptnet: A high-resolution infant MRI synthesizer based\non transformer,”arXiv:2105.13993, 2021.\n[58] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of CVPR, 2016, pp. 770–778.\n[59] X. Yi, E. Walia, and P. Babyn, “Generative adversarial network in medical\nimaging: A review,”Med. Image. Anal., vol. 58, p. 101552, 2019.\n[60] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoo-\nrian, J. A. van der Laak, B. van Ginneken et al., “A survey on deep\nlearning in medical image analysis,” Med. Image. Anal., vol. 42, pp. 60–\n88, 2017.\n[61] R. Li, W. Zhang, H.-I. Suk, L. Wang, J. Li, D. Shen, and S. Ji, “Deep\nlearning based imaging data completion for improved brain disease\ndiagnosis,” in MICCAI, 2014, pp. 305–312.\n[62] A. Torrado-Carvajal, J. L. Herraiz, E. Alcain, A. S. Montemayor,\nL. Garcia-Ca ˜namaque, J. A. Hernandez-Tamames, Y . Rozenholc, and\nN. Malpica, “Fast patch-based pseudo-CT synthesis from t1-weighted\nMR images for PET/MR attenuation correction in brain studies,”J. Nucl.\nMed., vol. 57, no. 1, pp. 136–143, 2016.\n[63] K. Bahrami, F. Shi, I. Rekik, and D. Shen, “Convolutional neural network\nfor reconstruction of 7T-like images from 3T MRI using appearance and\nanatomical features,” in Deep Learning and Data Labeling for Medical\nApplications, 2016, pp. 39–47.\n[64] K. Bahrami, F. Shi, X. Zong, H. W. Shin, H. An, and D. Shen, “Re-\nconstruction of 7T-like images from 3T MRI,” IEEE Trans. Med. Imag.,\nvol. 35, no. 9, pp. 2085–2097, 2016.\n[65] Y . Zhang, P.-T. Yap, L. Qu, J.-Z. Cheng, and D. Shen, “Dual-domain\nconvolutional neural networks for improving structural information in 3t\nMRI,” Magn. Reson. Imag., vol. 64, pp. 90–100, 2019.\n[66] X. Han, “MR-based synthetic CT generation using a deep convolutional\nneural network method,”Med. Phys., vol. 44, no. 4, pp. 1408–1419, 2017.\n[67] D. Nie, X. Cao, Y . Gao, L. Wang, and D. Shen, “Estimating CT image\nfrom MRI data using 3D fully convolutional networks,” inDeep Learning\nand Data Labeling for Medical Applications, 2016, pp. 170–178.\n[68] H. Arabi, G. Zeng, G. Zheng, and H. Zaidi, “Novel deep learning-based\nCT synthesis algorithm for MRI-guided PET attenuation correction in\nbrain PET/MR imaging,” in Proceedings of IEEE NSS/MIC, 2018, pp.\n1–3.\n[69] K. Klaser, T. Varsavsky, P. Markiewicz, T. Vercauteren, D. Atkinson,\nK. Thielemans, B. Hutton, M. J. Cardoso et al., “Improved MR to CT\nsynthesis for PET/MR attenuation correction using imitation learning,”\nin Simulation and Synthesis in Medical Imaging, 2019, pp. 13–21.\n[70] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”\narXiv:1411.1784, 2014.\n[71] V . Sandfort, K. Yan, P. Pickhardt, and R. Summers, “Data augmentation\nusing generative adversarial networks (cycleGAN) to improve generaliz-\nability in CT segmentation tasks,”Sci. Rep., vol. 9, 11 2019.\n[72] M. Frid-Adar, I. Diamant, E. Klang, M. Amitai, J. Goldberger, and\nH. Greenspan, “GAN-based synthetic medical image augmentation for\nincreased cnn performance in liver lesion classiﬁcation,” Neurocomput-\ning, vol. 321, pp. 321–331, 2018.\n[73] A. Ben-Cohen, E. Klang, S. Raskin, S. Soffer, S. Ben-Haim, E. Konen,\nM. Amitai, and H. Greenspan, “Cross-modality synthesis from CT to PET\nusing FCN and GAN networks for improved automated lesion detection,”\nEng. Appl. Artif. Intell., vol. 78, 2018.\n[74] G. Santini, C. Fourcade, N. Moreau, C. Rousseau, L. Ferrer, M. Lacombe,\nV . Fleury, M. Campone et al., “Unpaired PET/CT image synthesis of\nliver region using CycleGAN,” in International Symposium on Medical\nInformation Processing and Analysis, vol. 11583, 2020, pp. 247 – 257.\n[75] C.-B. Jin, H. Kim, M. Liu, W. Jung, S. Joo, E. Park, Y . S. Ahn, I. H.\nHan et al., “Deep CT to MR synthesis using paired and unpaired data,”\nSensors, vol. 19, no. 10, p. 2361, 2019.\n[76] Y . Ge, D. Wei, Z. Xue, Q. Wang, X. Zhou, Y . Zhan, and S. Liao, “Un-\npaired MR to CT synthesis with explicit structural constrained adversarial\nlearning,” in Proceedings of IEEE ISBI, 2019, pp. 1096–1099.\n[77] J. Wolterink, A. M. Dinkla, M. Savenije, P. Seevinck, C. Berg, and\nI. Isgum, “Deep MR to CT synthesis using unpaired data,” in Simulation\nand Synthesis in Medical Imaging, 2017, pp. 14–23.\n[78] X. Dong, T. Wang, Y . Lei, K. Higgins, T. Liu, W. Curran, H. Mao,\nJ. Nye et al., “Synthetic CT generation from non-attenuation corrected\nPET images for whole-body PET imaging,” Phys. Med. Biol., vol. 64,\nno. 21, p. 215016, 2019.\n[79] H. Yang, J. Sun, A. Carass, C. Zhao, J. Lee, Z. Xu, and J. Prince,\n“Unpaired brain MR-to-CT synthesis using a structure-constrained cy-\ncleGAN,” arXiv:1809.04536, 2018.\n[80] Y . Hiasa, Y . Otake, M. Takao, T. Matsuoka, K. Takashima, A. Carass,\nJ. Prince, N. Sugano et al., “Cross-modality image synthesis from un-\npaired data using cyclegan: Effects of gradient consistency loss and\ntraining data size,” in Simulation and Synthesis in Medical Imaging.\nSpringer, 2018, pp. 31–41.\n[81] A. Chartsias, T. Joyce, R. Dharmakumar, and S. A. Tsaftaris, “Adversarial\nimage synthesis for unpaired multi-modal cardiac data,” in Simulation\nand Synthesis in Medical Imaging, 2017, pp. 3–13.\n[82] H. Do, P. Bourdon, D. Helbert, M. Naudin, and R. Guillevin, “7t MRI\nsuper-resolution with generative adversarial network,” in IST Electronic\nImaging Symposium, 2021.\n[83] L. Xiang, Y . Li, W. Lin, Q. Wang, and D. Shen, “Unpaired deep cross-\nmodality synthesis with fast training,” in Deep Learning in Medical\nImage Analysis and Multimodal Learning for Clinical Decision Support,\n2018, pp. 155–164.\n[84] V . Kearney, B. P. Ziemer, A. Perry, T. Wang, J. W. Chan, L. Ma,\nO. Morin, S. S. Yom et al., “Attention-aware discrimination for MR-\nto-CT image translation using cycle-consistent generative adversarial\nnetworks,”Radiol. Artif. Intell., vol. 2, no. 2, p. e190027, 2020.\n[85] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-attention\ngenerative adversarial networks,” inProceedings of ICML, vol. 97, 2019,\npp. 7354–7363.\n[86] J. Zhao, D. Li, Z. Kassam, J. Howey, J. Chong, B. Chen, and S. Li,\n“Tripartite-GAN: Synthesizing liver contrast-enhanced MRI to improve\ntumor detection,” Med. Image. Anal., vol. 63, p. 101667, 2020.\n[87] Z. Yuan, M. Jiang, Y . Wang, B. Wei, Y . Li, P. Wang, W. Menpes-\nSmith, Z. Niu et al., “Sara-GAN: Self-attention and relative average\ndiscriminator based generative adversarial networks for fast compressed\nsensing MRI reconstruction,” Front. Neuroinform., vol. 14, p. 58, 2020.\n[88] M. Li, W. Hsu, X. Xie, J. Cong, and W. Gao, “Sacnn: Self-attention\nconvolutional neural network for low-dose CT denoising with self-\nsupervised perceptual loss network,” IEEE Trans. Med. Imag., vol. 39,\nno. 7, pp. 2289–2301, 2020.\n[89] Y . Xie, J. Zhang, C. Shen, and Y . Xia, “Cotr: Efﬁciently bridging cnn\nand transformer for 3d medical image segmentation,”arXiv:2103.03024,\n2021.\n[90] Y . Dai and Y . Gao, “Transmed: Transformers advance multi-modal med-\nical image classiﬁcation,” arXiv:2103.05940, 2021.\n[91] D. Karimi, S. Vasylechko, and A. Gholipour, “Convolution-free medical\nimage segmentation using transformers,”arXiv:2102.13645, 2021.\n[92] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” Pro-\nceedings of NIPS, pp. 1–11, 2017.\n[93] S. Haykin, Neural networks: a comprehensive foundation. Prentice Hall\nPTR, 1994.\n[94] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, and et al., “The\nmultimodal brain tumor image segmentation benchmark (brats),” IEEE\nTrans. Med. Imag., vol. 34, no. 10, pp. 1993–2024, 2015.\n16\n[95] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby,\nJ. Freymann, K. Farahani et al., “Advancing the cancer genome atlas\nglioma MRI collections with expert segmentation labels and radiomic\nfeatures,” Sci. Data, vol. 4, p. 170117, 2017.\n[96] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempﬂer, A. Crimi, R. T.\nShinohara, C. Berger et al., “Identifying the best machine learning\nalgorithms for brain tumor segmentation, progression assessment, and\noverall survival prediction in the brats challenge,” arXiv:1811.02629,\n2019.\n[97] T. Nyholm, S. Svensson, S. Andersson, J. Jonsson, M. Sohlin, C. Gustafs-\nson, E. Kjell ´en, K. S ¨oderstr¨om et al., “MR and CT data with multiob-\nserver delineations of organs in the pelvic area—part of the gold atlas\nproject,” Med. Phys., vol. 45, no. 3, pp. 1295–1300, 2018.\n[98] M. Jenkinson and S. Smith, “A global optimisation methof for robust\nafﬁne registration of brain images,” Med. Image. Anal., vol. 5, pp. 143–\n156, 2001.\n[99] H. Lan, the Alzheimer Disease Neuroimaging Initiative, A. W. Toga, and\nF. Sepehrband, “Three-dimensional self-attention conditional GAN with\nspectral normalization for multimodal neuroimaging synthesis,” Magn.\nReson. Med., vol. 86, no. 3, pp. 1718–1733, 2021.\n[100] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin Proceedings of ICLR, Y . Bengio and Y . LeCun, Eds., 2015.\n[101] T. Ridnik, E. Ben-Baruch, A. Noy, and L. Zelnik-Manor, “Imagenet-21k\npretraining for the masses,” arXiv:2104.10972, 2021.\n[102] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality\nassessment: From error visibility to structural similarity,” IEEE Trans.\nImage Process., vol. 13, pp. 600 – 612, 2004.\n[103] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,\n“GANs trained by a two time-scale update rule converge to a local nash\nequilibrium,” in Proceedings of NIPS, 2017, p. 6629–6640.\n[104] S. Abnar and W. Zuidema, “Quantifying attention ﬂow in transformers,”\nProceedings of ACL, pp. 4190–4197, 2020.\n[105] R. Durall, M. Keuper, and J. Keuper, “Watch your up-convolution: Cnn\nbased generative deep neural networks are failing to reproduce spectral\ndistributions,” inProceedings of CVPR, 2020, pp. 7887–7896.\n[106] B. Zhan, D. Li, Y . Wang, Z. Ma, X. Wu, J. Zhou, and L. Zhou, “Lr-cgan:\nLatent representation based conditional generative adversarial network\nfor multi-modality mri synthesis,” Biomedical Signal Processing and\nControl, vol. 66, p. 102457, 2021.\n[107] D. Nie and D. Shen, “Adversarial Conﬁdence Learning for Medical\nImage Segmentation and Synthesis,” Int J Comput Vision, vol. 128,\nno. 10, pp. 2494–2513, 2020.\n[108] Y . Luo, D. Nie, B. Zhan, Z. Li, X. Wu, J. Zhou, Y . Wang, and D. Shen,\n“Edge-preserving mri image synthesis via adversarial network with iter-\native multi-scale fusion,”Neurocomputing, vol. 452, pp. 63–77, 2021.\n[109] L.-H. Chen, C. G. Bampis, Z. Li, C. Chen, and A. C. Bovik,\n“Convolutional block design for learned fractional downsampling,”\narXiv:2105.09999, 2021.\n[110] M. Yurt, S. U. H. Dar, M. ¨Ozbey, B. Tınaz, K. K. O ˘guz, and T. C ¸ ukur,\n“Semi-supervised learning of mutually accelerated MRI synthesis with-\nout fully-sampled ground truths,” arXiv:2011.14347, 2021.",
  "topic": "Computer vision",
  "concepts": [
    {
      "name": "Computer vision",
      "score": 0.708301842212677
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6535327434539795
    },
    {
      "name": "Computer science",
      "score": 0.591079592704773
    },
    {
      "name": "Residual",
      "score": 0.46397385001182556
    },
    {
      "name": "Transformer",
      "score": 0.463102787733078
    },
    {
      "name": "Image registration",
      "score": 0.4580484926700592
    },
    {
      "name": "Medical imaging",
      "score": 0.4296915531158447
    },
    {
      "name": "Machine vision",
      "score": 0.4247405230998993
    },
    {
      "name": "Image (mathematics)",
      "score": 0.30894777178764343
    },
    {
      "name": "Engineering",
      "score": 0.15196460485458374
    },
    {
      "name": "Algorithm",
      "score": 0.08652955293655396
    },
    {
      "name": "Voltage",
      "score": 0.07718345522880554
    },
    {
      "name": "Electrical engineering",
      "score": 0.07529255747795105
    }
  ]
}