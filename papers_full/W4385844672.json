{
  "title": "Advanced prompting as a catalyst: Empowering large language models in the management of gastrointestinal cancers",
  "url": "https://openalex.org/W4385844672",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108216196",
      "name": "Jiajia Yuan",
      "affiliations": [
        "Peking University",
        "Peking University Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2146782000",
      "name": "Peng Bao",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2335978596",
      "name": "Zifan Chen",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2398223403",
      "name": "Ming-Ze Yuan",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2100572704",
      "name": "Jie Zhao",
      "affiliations": [
        "Peking University",
        "Changsha University"
      ]
    },
    {
      "id": "https://openalex.org/A2117178525",
      "name": "Jiahua Pan",
      "affiliations": [
        "Changsha University",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2102826308",
      "name": "Yi Xie",
      "affiliations": [
        "Peking University Cancer Hospital",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2141928769",
      "name": "Yanshuo Cao",
      "affiliations": [
        "Peking University Cancer Hospital",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2122862935",
      "name": "Yakun Wang",
      "affiliations": [
        "Peking University",
        "Peking University Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2114995942",
      "name": "Zhenghang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104720996",
      "name": "Zhihao Lu",
      "affiliations": [
        "Peking University",
        "Peking University Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2102835233",
      "name": "Xiaotian Zhang",
      "affiliations": [
        "Peking University Cancer Hospital",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2087928687",
      "name": "Jian Li",
      "affiliations": [
        "Peking University",
        "Peking University Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2085879269",
      "name": "Lei Ma",
      "affiliations": [
        "Peking University Cancer Hospital",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2097128869",
      "name": "Yang Chen",
      "affiliations": [
        "Peking University Cancer Hospital",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1973164540",
      "name": "Li Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2097678246",
      "name": "Lin Shen",
      "affiliations": [
        "Peking University Cancer Hospital",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1895920807",
      "name": "Bin Dong",
      "affiliations": [
        "Peking University",
        "Changsha University",
        "Peking University International Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W3210165781",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4362521774",
    "https://openalex.org/W4313475918",
    "https://openalex.org/W4376570107",
    "https://openalex.org/W4361267822",
    "https://openalex.org/W4377024548",
    "https://openalex.org/W4366823098",
    "https://openalex.org/W4378782373",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W4303441863",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W4366447635"
  ],
  "abstract": "&lt;p&gt;Large Language Models' (LLMs) performance in healthcare can be significantly impacted by prompt engineering. However, the area of study remains relatively uncharted in gastrointestinal oncology until now. Our research delves into this unexplored territory, investigating the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting. We develop a comprehensive evaluation system to assess the performance of LLMs across multiple dimensions. This robust evaluation system ensures a thorough assessment of the LLMs' capabilities in the field of medicine. Our findings suggest a positive relationship between the comprehensiveness of the prompts and the LLMs' performance. Notably, the multi-round strategy, which is characterized by iterative question-and-answer rounds, consistently yields the best results. ICL, a strategy that capitalizes on interrelated contextual learning, also displays significant promise, surpassing the outcomes achieved with simpler prompts. The research underscores the potential of advanced prompt engineering and iterative learning approaches for boosting the applicability of LLMs in healthcare. We recommend that additional research be conducted to refine these strategies and investigate their potential integration, to truly harness the full potential of LLMs in medical applications.&lt;/p&gt;",
  "full_text": " \nAdvanced prompting as a catalyst: Empowering large language models\nin the management of gastrointestinal cancersJiajia Yuan,1,8 Peng Bao,2,8 Zifan Chen,2,8 Mingze Yuan,2,8 Jie Zhao,3,7 Jiahua Pan,7 Yi Xie,1 Yanshuo Cao,1 Yakun Wang,1 Zhenghang Wang,1 Zhihao Lu,1\nXiaotian Zhang,1 Jian Li,1 Lei Ma,6 Yang Chen,1,* Li Zhang,2,6,* Lin Shen,1,* and Bin Dong4,5,6,7,*\n*Correspondence:  yang_chen@bjcancer.org (Y.C.);  zhangli_pku@pku.edu.cn (L.Z.);  shenlin@bjmu.edu.cn (L.S.);  dongbin@math.pku.edu.cn (B.D.)\nReceived: July 24, 2023; Accepted: August 8, 2023; Published Online: August 14, 2023; https://doi.org/10.59717/j.xinn-med.2023.100019\n© 2023 The Author(s). This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\nGRAPHICAL ABSTRACT\nUnderstanding\nMedical History\nDiagnosis &\nDifferential Diagnosis\nFurther Examination\n& Reason\nPrinciples &\nPlans of Treatment\nBreadth &\nDepth of Results\nThinking &\nExpressing Ability\nSimple\nTemplated\nMulti-round\nICL\n \nPUBLIC SUMMARY\n■   Prompt engineering affects large language models' performance in GI oncology.\n■   Prompts with templates and in-context learning enhance large language models' output.\n■   Multi-round interaction helps large language models to reach the best performance.\n■   Such performance meets the need of senior GI oncologists for effective AI agents.\nMedicine\n ARTICLE\n The Innovation Medicine 1(2): 100019, September 21, 2023　　　  1\n \nAdvanced prompting as a catalyst: Empowering large language models\nin the management of gastrointestinal cancersJiajia Yuan,1,8 Peng Bao,2,8 Zifan Chen,2,8 Mingze Yuan,2,8 Jie Zhao,3,7 Jiahua Pan,7 Yi Xie,1 Yanshuo Cao,1 Yakun Wang,1 Zhenghang Wang,1 Zhihao Lu,1\nXiaotian Zhang,1 Jian Li,1 Lei Ma,6 Yang Chen,1,* Li Zhang,2,6,* Lin Shen,1,* and Bin Dong4,5,6,7,*\n 1Department of Gastrointestinal Oncology, Key Laboratory of Carcinogenesis and Translational Research, Peking University Cancer Hospital and Institute, Beijing 100142, China\n 2Center for Data Science, Peking University, Beijing 100871, China\n 3National Engineering Laboratory for Big Data Analysis and Applications, Peking University, Beijing 100871, China\n 4Beijing International Center for Mathematical Research, Peking University, Beijing 100871, China\n 5Center for Machine Learning Research, Peking University, Beijing 100871, China\n 6National Biomedical Imaging Center, Peking University, Beijing 100871, China\n 7Peking University Changsha Institute for Computing and Digital Economy, Changsha 410205, China\n 8These authors contributed equally\n*Correspondence:  yang_chen@bjcancer.org (Y.C.);  zhangli_pku@pku.edu.cn (L.Z.);  shenlin@bjmu.edu.cn (L.S.);  dongbin@math.pku.edu.cn (B.D.)\nReceived: July 24, 2023; Accepted: August 8, 2023; Published Online: August 14, 2023; https://doi.org/10.59717/j.xinn-med.2023.100019\n© 2023 The Author(s). This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\nCitation: Yuan J., Bao P., Chen Z., et al., (2023). Advanced prompting as a catalyst: Empowering large language models in the management of gastrointestinal cancers. The Innovation\nMedicine 1(2), 100019.\n \nLarge Language Models' (LLMs) performance in healthcare can be signifi-\ncantly impacted by prompt engineering. However, the area of study remains\nrelatively  uncharted  in  gastrointestinal  oncology  until  now.  Our  research\ndelves  into  this  unexplored  territory,  investigating  the  efficacy  of  varied\nprompting  strategies,  including  simple  prompts,  templated  prompts,  in-\ncontext learning (ICL), and multi-round iterative questioning, for optimizing\nthe performance of LLMs within a medical setting. We develop a compre-\nhensive  evaluation  system  to  assess  the  performance  of  LLMs  across\nmultiple  dimensions.  This  robust  evaluation  system  ensures  a  thorough\nassessment of the LLMs' capabilities in the field of medicine. Our findings\nsuggest  a  positive  relationship  between  the  comprehensiveness  of  the\nprompts  and  the  LLMs'  performance.  Notably,  the  multi-round  strategy,\nwhich  is  characterized  by  iterative  question-and-answer rounds,  consis-\ntently yields the best results. ICL, a strategy that capitalizes on interrelated\ncontextual  learning,  also  displays  significant  promise,  surpassing  the\noutcomes  achieved  with  simpler  prompts.  The  research  underscores  the\npotential  of  advanced  prompt  engineering  and  iterative  learning\napproaches for boosting the applicability of LLMs in healthcare. We recom-\nmend that additional research be conducted to refine these strategies and\ninvestigate  their  potential  integration,  to  truly  harness  the  full  potential  of\nLLMs in medical applications.\n \nINTRODUCTION\nLarge Language Models (LLMs), exemplified by cutting-edge architectures\nlike  GPT-4,1 have  demonstrated  considerable  potential  in  transforming\nhealthcare  delivery2-4 and  competency  in  medical  examinations.5 This influ-\nence is manifested across various healthcare sectors, including online patient\ninteraction,6 preventive  oncology,7-9 neuropsychiatry,10 dermatology,11 and\naesthetic  surgery  consultation,12,13 underscoring  their  remarkable  versatility.\nHowever, the application of LLMs such as GPT-4 in digestive system cancer\ntreatment  remains  an  underexplored  area.  The  complexities  inherent  to  this\nfield,  from  patient  consultation,  and  diagnosis  to  treatment  planning  and\nfollow-up care, pose formidable challenges for LLMs. Additionally, the exist-\ning  body  of  research2,6,7 primarily  evaluates  LLMs'  responses  to  common\nmedical  inquiries  via  rudimentary  prompting,  which  may  not  fully  leverage\ntheir  potential  in  medical  settings.  This  highlights  the  need  for  a  more\ncomprehensive  assessment  of  GPT-4's  capability  to  provide  personalized\ncancer treatment recommendations via sophisticated prompts.\nTo  harness  the  full  potential  of  LLMs,  it  is  crucial  to  employ  effective\nprompt  engineering.14-19 Prompt  engineering,  a  process  of  creating,  testing,\nand  optimizing  input  prompts,  serves  as  a  crucial  tool  in  controlling  and\nenhancing  interactions  with  LLMs.  Various  techniques  such  as  in-context\nlearning,15 retrieval-augmented generation,16 chain-of-thought,17 and least-to-\nmost prompting18 have been shown to significantly improve the performance\nof LLMs in tasks demanding logical thinking and reasoning. In-context learn-\ning  offers  models  a  few  demonstrations  before  attempting  a  task,  while\nretrieval-augmented generation enhances this process by retrieving relevant\nexamples  from  a  given  database.  Chain-of-thought  prompting  improves\nLLMs' reasoning  ability  by  directing  them  to  generate  a  series  of  intermedi-\nate steps toward a solution, and least-to-most prompting dissects complex\nproblems  into  simpler  sub-problems  to  be  solved  sequentially.  Intuitively,\nthese  techniques  could  effectively  boost  LLMs'  performance  in  complex\nmedical tasks, including cancer treatment recommendations.\nIn this study, we aimed to unleash GPT-4’s potential to provide personal-\nized  digestive  system  cancer  treatment  plans  through  prompt  engineering.\nInspired by the thinking, reasoning, and action processes of digestive oncolo-\ngists, we initially conceived the iterative procedure of prompt engineering as a\nmethod  of  amassing  information  regarding  gastrointestinal  tumors  within  a\ndistinct storage of knowledge and in turn, educating the LLM. However, these\nknowledge  repositories,  when  embedded  in  rudimentary  prompts,  are  often\ndevoid of substantial content, thus limiting their potential to effectively guide\nLLMs.  Consequently,  we  established  an  empirically  effective  multi-step\nprompt  template  consisting  of:  (i)  declaring  the  role,  this  process  involves\nassigning a particular role to GPT-4 that emulates a real-world professional\nor  function;  (ii)  stating  the  main  task,  this  step  essentially  provides  GPT-4\nwith  a  clear  directive  of  what  it  is  required  to  accomplish;  (iii)  declaring  the\nworkflow, which we view as a generalized chain of thought that allows GPT-4\nto  approach  problem-solving  or  deliver  answers  in  an  organized,  step-by-\nstep  manner;  (iv)  specifying  constraints,  it  involves  defining  the  boundaries\nwithin which GPT-4 should operate. Then, we iteratively refined this template\nto  align  GPT-4’s  responses  with  physicians’ requirements  and  added\nelements  to  generate  comforting  responses  for  patients.  An  experienced\noncology specialist subsequently interacted with GPT-4 over multiple rounds\nto further  guide  and  optimize  the  recommended  treatment  plans.  Further-\nmore,  motivated  by  the  exemplar-based  teaching  approach  in  medicine,  we\nalso  assessed  the  impact  of  in-context  learning  by  providing  GPT-4  with\nexamples  of  ideal  treatment  suggestions  through  document  retrieval.  We\nevaluated  the  performance  of  diverse  prompt  engineering  strategies  on  43\ncase reports, encompassing a wide range of digestive system cancer types,\nutilizing a clinically standardized evaluation metric.\nIn  summary,  we  are  the  first  to  conduct  a  comprehensive  assessment  of\nprompt  engineering  on  GPT-4’s  ability  to  provide  personalized  digestive\nsystem  cancer  treatment  recommendations,  as  per  our  comprehensive\nsearch  in  the  existing  literature.  We  developed  a  sophisticated  prompt\ntemplate  to  generate  personalized  cancer  treatment  plans  that  emphasize\npatient  comfort,  which  significantly  outperforms  rudimentary  prompts  and\noffers valuable insights for prompt design in the medical domain. We evalu-\nated  various  prompt  engineering  strategies,  including  rudimentary  prompts,\ntemplated prompts, in-context learning, and multi-round interaction, using a\nclinically  standardized  metric.  Our  results  highlight  the  promise  of  prompt\nengineering for medical applications of LLMs. \nARTICLE\n \n2  　　　The Innovation Medicine 1(2): 100019, September 21, 2023 www.the-innovation.org/medicine\nMATERIALS AND METHODS \nMaterials\nIn this study, we propose an innovative methodology to augment the learn-\ning  capability  of  LLMs  by  incorporating  multifaceted  prompt  design  and\ndynamic training approaches. As shown in Figure 1, diverse prompt designs\ncan  be  perceived  as  varying  modifications  to  the  storage  of  knowledge,\nencompassing  manual  alterations  meticulously  orchestrated  based  on  GI\ntumor  expertise,  automatic  modifications  that  explore  the  hospital's  pre-\nexisting data for analogous cases as pedagogical instances for the LLM, and\ndynamic  modifications  consistently  interrogated  and  addressed  during  the\ndeployment  of  the  consultation  process.  Consequently,  the  design  of  the\nprompts  was  executed  as  follows:  Initially,  the  models  are  subjected  to  a\nmore  sophisticated  introduction  prompt,  intricately  crafted  with  complex\nsemantic and structural nuances, thereby priming the LLMs to comprehend\nand  respond  to  intricate  queries.  Furthermore,  an  advanced  method  of  in-\ncontext learning is introduced, encouraging the models to extract knowledge\nand patterns from various contexts rather than individual sentences, foster-\ning  a  more  comprehensive  understanding  of  the  text.  To  accommodate\nevolving  data  patterns,  we  also  incorporate  online  learning  techniques,\nenabling  the  LLMs  to  continually  learn  and  adapt  from  real-time,  dynamic\ndata.  Lastly,  we  implement  an  iterative  feedback  loop  through  multi-round\nquestion-and-answer  sessions,  reinforcing  the  model’s ability  to  compre-\nhend, retain, and apply information over successive interactions. This combi-\nnation of sophisticated prompt architecture, in-context learning, online learn-\ning,  and  iterative  interactions  aims  to  substantially  enhance  the  LLM’s\npredictive and interpretative capabilities, pushing the frontiers of AI language\nunderstanding.  We  used  publicly  available  medical  licensing  examination\ncases,  oncology  residency  and  attending  physician  exam  cases  as  text\nsource. \nTemplated prompts\nPast  studies  have  shown  that  a  good  use  of  different  prompt\nengineering,17,20,21 as well as properly designed prompt templates22 can signif-\nicantly improve the problem-solving ability of large language models, and this\nphenomenon was similarly observed in our study. As shown in Figure S1, we\ndeveloped  our  prompt  template  by  adopting  a  four-pronged  approach  as\nfollows: \nDeclaring the role. Assigning a 'role' or 'identity' to large language models\nis  one  of  the  commonly  used  techniques  for  interacting  with  these  models.\nPrevious research22 supports that this method can effectively guide what type\nof output the models generate and what details they prioritize. In our study,\nwe assigned the role of a digestive oncology specialist to GPT-4, emphasiz-\ning its range of skills that included clinical diagnosis, treatment, and commu-\nnication techniques. We found this strategy successfully influenced GPT-4's\nbehaviors, responses, and interaction styles to align with the expectations of\nthe role. \nStating  the  main  task. This  approach  essentially  provides  GPT-4  with  a\nclear directive on what it is expected to accomplish. In our study, the primary\ntask of our model is to deliver detailed and accurate advice to patients with\ndigestive system cancers. This involves defining the central task that GPT-4\nneeds to perform. Given the context of our research, our model, acting as a\ndigestive oncology  specialist,  is  tasked  with  generating  personalized  treat-\nment  plans  for  digestive  system  cancer  patients.  By  articulating  the  main\ntask, we direct GPT-4's focus, streamline its reasoning process, and enhance\nits ability to produce task-specific, relevant, and actionable outputs. In addi-\ntion,  to  enable  GPT-4  to  produce  complex  and  contextually  accurate\nresponses,  we've  included  a  wide  range  of  scenarios  and  contexts,  from\nsimple  situations  to  the  complexity  of  academic  discourse  in  hospitals.  We\nalso  encourage  GPT-4  to  link  different  pieces  of  information  together.  This\napproach aids GPT-4 in moving beyond simple pattern recognition, facilitat-\ning a deeper understanding when executing tasks. \nDeclaring  the  workflow. We  have  defined  a  comprehensive  workflow  in\nthe  prompt  templates,  which  includes  case  analysis,  clinical  examination,\nscheduling examination, diagnosis and treatment, execution and adjustment\nof  treatment,  and  follow-ups. This  is  also  the  general  workflow  of  a  profes-\nsional digestive oncology specialist. We believe this represents a generalized\nchain  of  thought  and  many  studies17,20,21,23 have  already  demonstrated  that\nthis  approach  can  stimulate  LLM's  reasoning  ability.  We  find  this  strategy\nensures that GPT-4's output is more consistent and logical, using a planned,\nstep-by-step  approach  to  accomplish  tasks,  which  is  very  similar  to  the\nprocess  a  human  expert  uses  to  solve  problems.  By  structuring  GPT-4's\nthinking  in  this  way,  we  can  effectively  manage  its  output,  improve  overall\nconsistency,  and  reduce  the  likelihood  of  generating  irrelevant  or  erroneous\ninformation. \nMedicine\n \nICL\n84.4±7.5\n84.9±10.0\n91.2±4.0\nMulti-round\nPrompts\nFeedback\nTemplated\nSimple\nStorage of knowledge\n71.5±11.2\n \nFigure 1.  An  illustration  showcasing  the  effects  of  various  prompting  strategies  on  Language  Learning  Models'  (LLMs')  performance,  mediated  by  a  'storage  of\nknowledge' Simple prompts leave this storage empty, offering no enhancement for GI tumor decision-making. Conversely, templated prompts and ICL populate the storage with\nrole assumptions and case examples, respectively, helping to standardize LLMs' output, thus improving performance. The multi-round interaction strategy fills the storage with\nthe complete physician-LLM dialogue, potentially allowing more accurate comprehension and utilization of decision-assisting information.\nARTICLE\n The Innovation Medicine 1(2): 100019, September 21, 2023　　　  3\nSpecifying  constraints. In  this  process,  we've  incorporated  certain\nconstraints  into  the  prompt  templates.  We  require  GPT-4  not  to  make\nresponses  when  uncertain  or  additional  information  is  needed,  but  rather,  it\nmust  first  gather  sufficient  information.  In  addition,  we  require  GPT-4  to\nprovide detailed and correct guidance for a specific case, as GPT-4 tends to\ngive general and non-specific answers that may not be wrong but lack speci-\nficity.  This  approach  ensures  that  GPT-4  avoids  generating  responses  that\nare undesirable or beyond its scope, thereby enhancing its effectiveness and\nminimizing  potential  deviations.  We  also  advised  GPT-4  to  build  a  trusting\ndoctor-patient relationship in a warm, humorous manner rather than in a cold\nand impersonal way when answering. \nIn-context learning\nIn  this  study,  we  introduce  an  automated  in-context  learning  (ICL)\napproach  to  refine  GPT-4's  capabilities,  focusing  on  the  integration  of\ndoctors' habits and cognition. This method assimilates insights drawn from\nanalogous  past  cases  and  is  comprised  of  three  main  components:  firstly,\ntransposing  past  patient  conditions  into  a  designated  embedding  space;\nsecondly,  gauging  the  similarity  between  the  current  condition  and  these\narchived cases to identify its k-nearest counterparts; and finally, building in-\ncontext  learning  prompts  based  on  these  identified  cases.  We  provide  a\ndetailed exposition of these three components in the following: \nEncoding  patient  conditions  using  pre-trained  chinese  BERT  model. A\npre-trained  Chinese  BERT  model  in  Hugging  Face  (https://huggingface.\nco/hfl/chinese-bert-wwm-ext),  specifically  the “hfl/chinese-bert-wwm-ext”,\nis utilized to translate patient conditions into a high-dimensional embedding\nspace (768 dimensions in this study), capturing the context of the condition\neffectively.  The  BERT  tokenizer  is  used  to  convert  condition  text  into  input\nvectors, which are then fed into the BERT model. Operating in a no-gradient\nupdate  setting,  the “pooler_output” from  the  model  serves  as  the  sentence\nembedding for each patient condition. \nCalculation  of  cosine  similarity  and  identification  of  k-nearest neigh-\nbors. Once  the  embeddings  for  all  patient  conditions  have  been  computed,\nwe calculate the cosine similarity between them to derive a similarity score.\nThis metric provides a measure of the contextual similarity between different\npatient conditions. Based on these similarity scores, we identify up to k-near-\nest neighbors for each patient condition (with k being up to four depending on\nthe token limitation of GPT-4). \nGeneration  of  in-context  learning  prompts. For  each  patient  condition,\nwe  generate  an  enriched  prompt  that  includes  the  top-k  similar  past  cases\nand  the  corresponding  doctor's  suggestions.  To  ensure  consistency  and\nreadability in these prompts, a pre-defined template is used: \"As an experi-\nenced  clinician,  your  responsibilities  include  understanding  and  analyzing\npatient information and chief complaints, […]. Now, let’s look at these exam-\nples:  [...].  After  analyzing  these  examples,  here  is  a  new  patient:  [...].  Please\ngive specific treatment plan suggestions based on the above examples and\nrelevant literature. (see Figures 4 & S6 for details). \nMetrics\nWe  have  developed  a  unique  set  of  metrics,  drawing  from  those  typically\nused  for  evaluating  clinicians'  examinations,  to  quantitatively  assess  the\nresults  generated  by  various  methods.  These  metrics  encompass  six  key\naspects:\n1. Understanding medical history (0-20): This metric assesses how accu-\nrately and comprehensively an LLM captures and interprets a patient's medi-\ncal  history.  This  includes  consideration  of  the  patient's  previous  diagnoses,\nsurgeries, hospitalizations,  allergies,  family  history,  lifestyle,  and  other  rele-\nvant information.\n2.  Diagnosis  and  differential  diagnosis  (0-20): This  metric  assesses  the\nability of the LLM to accurately diagnose the patient's condition based on the\nmedical  history.  It  includes  both  the  primary  diagnosis  and  any  differential\ndiagnoses.\n3. Further examination and reason (0-10): This metric evaluates the appro-\npriateness of any additional examinations suggested by the LLM. It measures\nnot  only  whether  the  recommended  examinations  are  suitable,  but  also  if\nthey  are  justified  based  on  the  patient's  condition  and  symptoms.  The  LLM\nshould  also  provide  a  clear  rationale  for  why  these  additional  examinations\nare needed.\n4. Principles and plans of treatment (0-20): This metric evaluates the LLM's\nability to propose a suitable treatment plan. The plan should be personalized\nfor  the  patient,  taking  into  account  factors  like  age,  overall  health,  potential\nside effects, and patient preferences.\n5. Breadth and depth of results (0-20): This metric measures how compre-\nhensively  the  LLM  covers  the  scope  of  medical  knowledge  in  its  results\n(breadth), as well as how much detail it provides (depth). Breadth refers to the\nrange of different topics or areas covered in the results, while depth refers to\nthe level of detail or complexity within those topics.\n6. Thinking and expressing ability (0-10): This is a measure of how effec-\ntively the LLM reasons and communicates its findings. Thinking refers to the\nLLM's ability to logically process and interpret data, make connections, draw\nconclusions, and anticipate potential outcomes. The expressing ability should\nnot only be clear and accurate but also demonstrate empathy in line with a\nreal clinician's interaction. This includes sensitivity to the patient's emotional\nstate, using  comforting  and  supportive  language,  and  showing  understand-\ning  and  respect  for  the  patient's  experiences  and  concerns.  By  effectively\nincorporating empathy,  the  LLM  can  build  trust,  encourage  open  communi-\ncation,  and  provide  emotional  support  in  addition  to  addressing  physical\nhealth issues.\nTo gain a clearer understanding of performance based on the total scores,\nwe have defined the following expertise levels:\n1.  Level  A  (90-100  points): Top-level  expertise,  capable  of  independently\nmanaging  complex  and  rare  cases,  demonstrating  exceptional  skills  and\nprofessional knowledge.\n2.  Level  B  (80-89  points): Experienced  level,  capable  of  handling  most\ncases, but requires guidance for complex or rare cases.\n3. Level C (70-79 points): Mid-level competence, capable of independently\naddressing common cases, requires guidance for complex ones.\n4. Level D (60-69 points): Junior level, capable of handling some common\ncases, but requires close guidance for complex cases.\n5.  Level  E  (below  60  points): Initial  training  level,  needs  guidance  from\nexperienced clinicians in all aspects. \nRESULTS \nTemplated evaluation\nFigures  2 & S2 provide  a  comparison  between  our  designed  prompting\ntemplate  (Figure  2B)  and  the  standard,  direct  prompting  (Figure  2A)  utilized\nby  GPT-4.  The  findings  underscore  that  the  designed  template  for  role\nassumption  (Figure  S1)  can  improve  GPT-4 to  make  more  complex  deci-\nsions based on the patient's individual circumstances. In the provided exam-\nple, our designed prompting can prioritize the control of disease progression,\nsymptom relief, enhancement of life quality, and survival extension, instead of\nmerely pursuing a cure unconditionally. Moreover, the template manifests an\nexceptional  ability  to  interweave  quality-of-life  considerations  within  the\ntreatment  strategies  and  provides  comprehensive  guidance  (Figure  S3).  It\nalso underscores the significance of continuous patient assessment and the\npursuit  of  innovative,  custom  treatments  (Figure  S4).  As  opposed  to  direct\nprompting, our designed prompting template possesses the ability to mimic\nthe  intricate  treatment  ideation  process,  enhancing  GPT-4’s  efficacy  as  a\ntherapeutic advisory tool when acting as a senior oncologist. \nMulti-round evaluation\nFigures  3 & S5 illustrate  an  interaction  with  GPT-4  for  cancer  treatment\nadvice.  Initially,  GPT-4  prematurely  diagnosed  the  patient  with  late-stage\ncancer  and  proposed  a  treatment  plan.  However,  this  was  inappropriate,\ngiven the necessity for a more accurate staging diagnosis for this patient. As\nhighlighted in Figure 3, the clinician directed GPT-4 to offer a detailed staging\ndiagnosis,  subsequently  pointing  out  its  error.  Following  multiple  question-\nand-answer interactions with the clinician, GPT-4 acknowledged its mistake\nand adjusted its response. It began by determining the cancer's stage, before\nsuggesting a specific treatment plan. This revised response is not only more\nsuitable  for  the  patient  but  also  provides  her  with  hope.  This  multi-round\ninteraction demonstrates the learning capability of large language models like\nGPT-4, highlighting their ability to quickly integrate human logical reasoning\nwithin the context of intricate medical scenarios. \nARTICLE\n4  　　　The Innovation Medicine 1(2): 100019, September 21, 2023 www.the-innovation.org/medicine\nICL evaluation\nAs  demonstrated  in Tables  1-4,  the  performance  of  In-Context  Learning\n(ICL) exceeded that of rudimentary prompting by a substantial margin across\nvarious types  of  digestive  system  cancer  treatments,  with  a  notable  differ-\nence of 13.4 points in overall performance. Figures 4 & S6 provide an illustra-\ntive  comparison  between  in-context  learning  and  rudimentary  prompting.\nMedicine\n \nA Direct prompting\nB Make role assumptions before prompting\nDoctor\nMale, 68 years old. The patient was found to have a gastric mass in an external h ospital. Gastroscopy showed that at 41-47cm from the \nincisors, there is an ulcerative tumor at the esophagogastric junction. Pathology: Esophagogastric junction. Biopsy: moderately differentiated \nadenocarcinoma. Lauren classification: intestinal type. Immunohistochemical results: HER2(0) [HER2 positive control (3+), HER2 negative \ncontrol (0)]. CT: Thickening of the gastric wall, considered as Borrmann type 3 gastric cancer(cT4aN3a)  ; Multiple filling def ects in the splen-\nic vein and portal vein, suggesting tumor thrombus. The tumor markers are normal. Considering the patient's multiple portal vein tumor \nthrombi, it is not suitable for clinical trial enrollment.\nThis is the information of a patient. Please search for relevant lite rature and provide specific treatment plan recommendations, and explain \nthe reasons.\n    This prompt is summarized into “{Sex}, {Age}, {Disease description}, {Question}.” and will be referred to in other content.\nGPT-4\nBased on the information provided, immunohistochemistry shows HER2 positive. Due to the presence of multiple tumor thrombi in the \nportal vein, the patient is not suitable for clinical trial enrollment.\nThe treatment plan is as follows:\n1.\n2.\n3.\n4.\nDoctor\n{Role declaration} \n{Definition of a Gastrointestinal Oncologist}\n{Skills of a Gastrointestinal Oncologist}\n{Work Process of a Gastrointestinal Oncologist}\n{Specific Requirements and Rules for the Response}\n    {Sex}, {Age}, {Disease description}.\nGPT-4\nBased on the information you provided, the patient has been diagnosed with moderately differentiated adenocarcinoma of the esophagogas -\ntric junction, with a pathological type of intestinal, HER2-negative. CT scans show the existence of tumor thrombus, which may suggest the \ndisease is in the late stage. However, please note that a more detailed clinical data and expert evaluation are needed for a definite diagnosis.\nFor patients with advanced gastric cancer of this type, systemic chemotherapy is often the first choice, such as the FOLFOX regimen (fluoro -\nuracil + cisplatin + oxaliplatin) or the SOX regimen (S-1 + oxaliplatin). Moreover, considering the HER2-negative pathological result, targeted \ndrugs like trastuzumab may not be applicable here. Additionally, if the disease is well-controlled, palliative surgery can be considered to allevi -\nate symptoms.\nRegarding the tumor thrombus, we need to evaluate in detail whether there is a possibility for surgery or interventional treatment, or if antico -\nagulant therapy is needed. If the tumor thrombus causes severe clinical symptoms, surgery or interventional treatment may be considered, \nbut this requires a comprehensive assessment of the patient's overall condition and other complications.\nAt the same time, it is also recommended to conduct relevant genetic te sting, such as MLH1, MSH2, MSH6, PMS2, etc., because a family \nhistory of cancer may be related to hereditary cancer syndromes.\nThis is a possible diagnosis and treatment plan based on the in formation I currently have. The actual treatment plan needs to be made based \non the patient's specific situation and a comprehensive assessment by the medical team. Please discuss these suggestions with your medi -\ncal team to develop the most suitable treatment plan for the patient.\nChemotherapy:  Considering the patient's condition, neoadjuvant chemot herapy can be administered to reduce tumor size, alleviate      \nsymptoms, and increase the possibility of surgery. Common chemotherapy regimens include the FLOT regimen and the EOX regimen.\nSurgery:  If the tumor shrinks and the condition stabilizes after chemotherapy, the possibility of surgical removal can be evaluated. \nDepending on the patient's specific situation, a gastric cancer radical operati on (D2 or D1+ lymph node dissection) or palliative surgery \ncan be chosen.\nTargeted therapy:  For HER2-positive patients, HER2-targeted therapy drugs such as trastuzumab can be added during chemotherapy.\nSupportive therapy:   Nutritional support and symptomatic treatment.\n \nFigure 2.  Evaluation of prompting template design (A) Direct prompting. GPT-4 is directly provided with a patient's condition and asked to suggest a treatment plan. (B) The\nproposed templated prompting (make role assumptions before prompting). GPT-4 is initially primed with a specifically designed role-embedded prompt, ensuring the AI under-\nstands the task requirements and responsibilities. Subsequently, the patient's condition is provided, and GPT-4 is asked to suggest a treatment plan. Note that this figure is a\nreduced version of Figure S2.\nARTICLE\n The Innovation Medicine 1(2): 100019, September 21, 2023　　　  5\n \nDoctor\n{Role assumption first} \nFemale, 76 years old. Poor appetite. Gastroscopy  revealed a raised lesi on with a concave surface at the cardia, extending to the gastric \nfundus (Siewert II type, adenocarcinoma), and coarse gastric mucosa consistent with Borrmann IV type gastric cancer presentation (ade -\nnocarcinoma). Pathology report: a small amount of poorly differentiated cancer (gastric body), poorly differentiated adenocarcinoma; \nCmet(1+), EGFR(0), HER2(0), pMMR, pan-TRK(-), PD-L1 (CPS<1), EBER(-); (Cardia) moderately differentiated adenocarcinoma, intestinal \ntype; Cmet(2+), EGFR(1+), HER2(0), pMMR, pan-TRK(-), PD-L1 (CPS=10), EBER(-). Normal lab results, chest-abdominal-pelvic enhanced CT \nshowed diffuse thickening of the entire stomach, lymph node metastasis between the liver and stomach. Currently, the patient's food \nintake has decreased, bowel movements are not abnormal, and she has lost about 5 kg in the past 2 months.\nGPT-4\nBased on the above information, the treatment plan should focus on alleviating symptoms and improving quality of life. Specific sugges -\ntions are as follows:\n1. \n2. \n3. \n4. \n5. \nDoctor\nHow do you analyze and determine the staging diagnosis and future prognosis for this patient?\nGPT-4\nThe staging of gastric cancer primarily refers to the American Joint Committee on Cancer (AJCC) TNM staging system, which assesses the \ndepth of tumor invasion (T), the status of lymph node metastasis (N), and the condition of distant metastasis (M). According to the infor -\nmation you provided, the patient's gastric cancer has developed lymph node metastasis (LNM), indicating that the N part of the TNM stage \nis at least N1 (1-2 regional lymph nodes metastasis) or higher. Simultaneously, the gastric cancer is Borrmann IV type, suggesting that the \ntumor has extensively invaded the stomach wall, and in the TNM stage, the grading of the T part is usually T3 (tumor invades the serosa) or \nT4 (tumor penetrates the serosa or directly invades adjacent structures). Based on this information, it can be preliminarily judged that the \npatient is in at least stage III of gastric cancer (including T3N1M0, T4aN1M0, T4bN1M0, etc.) or a higher stage. \nDoctor\nThe patient's lymph node metastasis (LNM) between the liver and stomach  could be classified as either N1 or N3, and a detailed interpre -\ntation of the lymph node metastasis is needed. Will there be a chance for surgery in the future? This will have a significant impact on future \ntreatment outcomes. \nGPT-4\nIn terms of the resectability of the disease, the patient's lymph node metastasis between the liver and stomach (LNM) may a ffect surgical \ndecisions. The N stage in the TNM staging is at least N1 (1-2 regional lymph node metastases). In some cases, for locally advanced gastric \ncancer, even with lymph node metastasis, radical surgery (such as D2 gastrectomy) may be  performed. This surgery can remove as many \nlymph nodes as possible, but the risk of surgery is high, and the rate of postoperative complications and mortality i s also relatively high, \nespecially in the elderly population. However, for advanced gastric cancer (such as the patient's  Borrmann IV type gastric cancer), especial-\nly in cases accompanied by extensive lymph node metastasis or distant metastasis, surgery is usually not the first choice of treatment, \nbecause surgery at this time cannot significantly improve prognosis and may instead bring greater risks and complications. \nDoctor\nBased on the above discussion, please provide a new recommendation for this patient.\nGPT-4\nI suggest the following: \n1.\n2. \n3. \n4. \n \nChemotherapy: Common chemotherapy regimens include FOLFOX, XELOX, SOX, etc. \nTargeted Drugs: C-MET and EGFR are slightly positive, so targeted drugs against these two markers, such as crizotinib (a C-MET inhibi -\ntor) and erlotinib (an EGFR inhibitor) can be considered. \nImmunotherapy: Although the expression level of PD-L1 is low, immunotherapy check point inhibitors like Keytruda (pembrolizumab) can \nstill be considered. \nPalliative Care: This includes pain management, nutritional support, and psychosocial support. \nRegular Follow-up: Regular gastroscopy review and chest, abdominal and pelvic CT scans are necessary. \nDetailed Staging:  First, I suggest more detailed tumor staging. This may include PET-CT scanning to find any other unknown metastatic \nlesions. Additionally, for lymph node metastasis, we need to further clarify its extent in order to stage more accurately. \nPeritoneal Metastasis Examination:  Considering the possibility of common peritoneal metastasis in gastric cancer, I recommend laparo-\nscopic examination for direct observation of the abdominal cavity and to obtain livi ng tissues for pathological examination, to find any \npotential hidden peritoneal metastasis. \nIndividualized Treatment Plan:  Based on the detailed staging and peritoneal metastasis examination results, we can further discuss the \ntreatment plan. This may include chemotherapy, targeted therapy, and immunotherapy. Especially, immunotherapy might be tried consid-\nering her PD-L1(22C3)(CPS=10). \nNutrition and Symptom Management:  Given her age and current condition (weight loss of about 5kg in the past 2 months, decreased \nfood intake), I recommend asking a dietitian for evaluation and manage ment, and there may also need to be appropriate palliative treat -\nment for symptoms such as decreased appetite. \n \nFigure 3.  A Multi-round Interaction with GPT-4 for Gastric Cancer Treatment Advice Note that this figure is a reduced version of Figure S5.\nARTICLE\n6  　　　The Innovation Medicine 1(2): 100019, September 21, 2023 www.the-innovation.org/medicine\nPanel A portrays a clinical scenario in which a patient is diagnosed with rectal\ncancer and is accompanied by a naive prompt for generating treatment strat-\negy recommendations. In contrast, Panel B highlights the implementation of\nin-context  learning,  where  GPT-4  is  provided  with  two  comparable  patient\ncases, each accompanied by treatment suggestions endorsed by an experi-\nenced oncologist. Through in-context learning, GPT-4 successfully absorbed\nthe treatment  approach  and  reasoning  process  utilized  by  physicians,  lead-\ning  to  the  generation  of  personalized  and  targeted  treatment  plans,  as\nopposed to general guideline recommendations offered through rudimentary\nprompts. Furthermore, ICL emphasized clinically critical aspects such as clin-\nMedicine\n \nA Direct prompting\nB In-context learning prompting\nDoctor\nMale, 59 years old. Visited the clinic due to abdominal pain after eating. C olonoscopy indicated a tumor 30cm from the hepatic flexure. \nPathology: Sigmoid colon: Moderately differentiated adenocarcinoma with mucinous adenocarcinoma. PET-CT: Considering colon cancer, \nmultiple metastatic lymph nodes in the abdominal pelvic cavity, peritoneum, and retroperitoneum, multiple liver metastases. Tumor mark -\ners: CEA 37.93, CA199 368.3, CA724 103.4, CA242 463.6. Recent weight loss of 8KG. \nThis is the information of a patient. Please search for relevant literature and provide specific treatment plan recommendations, and \nexplain the reasons.\n    This prompt is summarized into “{Target case}, {Question}.” and will be referred to in other content.\nGPT-4\nChemotherapy:  Common chemotherapy drugs include fluorouracil (5-FU), oxaliplatin, and irinotecan. Chemotherapy regimens can use \nFOLFOX  or FOLFIRI.\nTargeted therapy:  Based on the genetic testing results of the patient's tumor, you can consider using drugs that target specific targets for \ntreatment, such as EGFR inhibitors (such as Herceptin, cetuximab or panitumumab), VEGF inhibitors (such as bevacizumab) or other \napplicable targeted therapy drugs.\nImmunotherapy:  PD-1/PD-L1 inhibitors (like pembrolizumab or nivolumab) may be effective for some patients. The applicabil ity of immu-\nnotherapy needs to be determined based on the patient's tumor microsatellite instability (MSI) or PD-L1 expression.\nSupportive care:  During treatment, the patient may need hydration, nutritional support, pain relief, and antibiotic treatment, among other \nsupportive care.\n Follow-up and monitoring:  During treatment, tumor markers should be monitored regularly to assess the effectiveness of the treatment. \n1. \n2. \n3. \n4. \n5. \nDoctor\nAs an experienced clinical physician, your responsibilities include understanding and analyzing patient information and chief complaints, \nthen based on previous cases and the treatment inspection suggestions of real doctors in those cases, give professional treatment \ninspection suggestions in a similar style. You need to be professional, careful, and thoughtful, and your answer must be based on existing \nmedical knowledge and consider the patient's personal situation and needs. Now, let's look at these examples:\nCase 1: {Case description 1}\nDoctor’s suggestion: {Doctor’s suggestion 1} \nCase 2: {Case description 2}\nDoctor’s suggestion: {Doctor’s suggestion 2} \nAfter analyzing these examples, here is a new patient:  [T arget Case in Panel A]. Please give specific treatment plan suggestions based \non the above examples and relevant literature.\nGPT-4\nBased on the information you provided, I suggest the following treatment plan:\n1.\n2.\n3.\n4.\n5.\n6.\n7. \nComplete gene testing, including KRAS/NRAS/BRAF gene testing, as well as immunohistochemistry of MLH1/PMS2/MSH2/MSH6 to clari-\nfy the molecular characteristics of the tumor. If it is dMMR/MSI, it is recommended to treat with PD1 monotherapy ± CTLA4 monotherapy.\nChoose the appropriate chemotherapy plan based on the results of gene testing. If it  is a KRAS/NRAS/BRAF wild type, consider the FOLFI-\nRI scheme combined with targeted therapy (such as cetuximab or bevacizumab). If the gene testing results show mutations, select the \nFOLFIRI scheme or other suitable chemotherapy plans.\nStrengthen nutritional support in conjunction with the patient's recent weight loss, to improve the patient's nutritional status.\nRegularly review tumor markers and imaging examinations (such as CT or MRI of the perit oneum, retroperitoneal lymph nodes, and liver) \nto assess the treatment effect.\nBased on the treatment effect and the patient's physical condition, assess the possibility of local treatment, such as surgery, ablation, or \nradiation therapy.\nIf conditions permit, consider participating in clinical research to obtain more treatment opportunities.\nPay attention to observe the patient's symptoms and side effects, and conduct symptomatic treatment accordingly. \n \nFigure 4.  Utilizing GPT-4 to Suggest Oncological Treatment Regimens with Distinct Prompting Techniques (A) Direct prompting and (B) In-Context Learning (ICL) prompting.\nNote that GPT-4 was not provided with the designed prompt, as our objective was to exclusively evaluate the performance of ICL. Note that this figure is a reduced version of\nFigure S6.\nARTICLE\n The Innovation Medicine 1(2): 100019, September 21, 2023　　　  7\nical research and potential side effects, thus enhancing the overall quality and\nrelevance of the generated treatment recommendations. \nOverall evaluation\nTables 1-4 provided the performance evaluation of different prompt engi-\nneering  strategies-Simple,  Templated,  ICL,  and  Multi  (Multiple  Rounds)-on\nseveral aspects of understanding and knowledge organization across differ-\nent  disease  conditions  (Overall,  Gastric  Cancer,  Colorectal  Cancer,  Other  GI\ncancers). Broadly, the tables show a general trend of increased performance\nas  we  move  from  the  Simple  strategy  to  the  Multi-strategy,  but  templated\nprompts and ICL show similar performance. The mean scores for all aspects\nimprove noticeably as the complexity of the prompts increases as shown in\nTable S1.\nA few specific observations can be highlighted. First, ‘Understanding Medi-\ncal History’ consistently receives full marks in the Multi-strategy, underscor-\ning  the  effectiveness  of  iterative  questioning  in  gathering  comprehensive\n \nTable 1.  Overall performance in all GI cancers\nMetrics Simple Templated ICL Multi-round\nUnderstanding Medical History (0-20) 19.1±2.5 19.9±0.8 20.0±0.0 20.0±0.0\nDiagnosis and Differential Diagnosis (0-20) 18.3±3.4 19.4±2.2 19.4±1.9 19.9±0.8\nFurther Examination and Reason (0-10) 6.6±3.8 8.8±2.6 9.0±2.5 9.8±1.1\nPrinciples and Plans of Treatment (0-20) 10.8±4.0 14.0±3.3 14.2±4.7 16.5±3.2\nBreadth and Depth of Results (0-20) 11.4±2.2 14.2±2.1 14.3±1.7 15.2±1.5\nThinking and Expressing Ability (0-10) 5.3±2.0 8.1±2.4 8.0±2.7 9.8±1.1\nTotal Score (0-100) 71.5±11.2 84.4±7.5 84.9±10.0 91.2±4.0\n \nTable 2.  Overall performance in gastric cancer\nMetrics Simple Templated ICL Multi-round\nUnderstanding Medical History (0-20) 18.1±3.3 19.8±1.1 20.0±0.0 20.0±0.0\nDiagnosis and Differential Diagnosis (0-20) 16.4±4.1 18.8±3.0 18.8±2.6 19.8±1.1\nFurther Examination and Reason (0-10) 5.5±3.7 8.8±2.6 8.8±2.6 9.5±1.5\nPrinciples and Plans of Treatment (0-20) 9.8±4.2 13.6±3.8 14.3±4.7 16.4±3.8\nBreadth and Depth of Results (0-20) 11.0±2.0 15.0±1.5 14.5±1.5 15.7±1.7\nThinking and Expressing Ability (0-10) 5.0±1.5 9.0±2.0 8.6±2.3 9.5±1.5\nTotal Score (0-100) 65.7±10.6 85.0±7.7 85.0±10.9 91.0±5.0\n \nTable 3.  Overall performance in colorectal cancer\nMetrics Simple Templated ICL Multi-round\nUnderstanding Medical History (0-20) 20.0±0.0 20.0±0.0 20.0±0.0 20.0±0.0\nDiagnosis and Differential Diagnosis (0-20) 20.0±0.0 20.0±0.0 20.0±0.0 20.0±0.0\nFurther Examination and Reason (0-10) 7.3±3.3 9.1±1.9 9.1±1.9 10.0±0.0\nPrinciples and Plans of Treatment (0-20) 10.5±4.0 13.6±3.1 12.3±5.8 17.3±2.5\nBreadth and Depth of Results (0-20) 11.4±2.2 14.1±1.9 14.1±1.9 15.0±0.0\nThinking and Expressing Ability (0-10) 4.5±1.4 7.3±2.5 6.8±3.2 10.0±0.0\nTotal Score (0-100) 73.6±6.4 84.1±6.3 82.3±10.7 92.3±2.5\n \nTable 4.  Overall performance in other GI cancer\nMetrics Simple Templated ICL Multi-round\nUnderstanding Medical History (0-20) 19.4±1.6 20.0±0.0 20.0±0.0 20.0±0.0\nDiagnosis and Differential Diagnosis (0-20) 19.4±1.6 20.0±0.0 20.0±0.0 20.0±0.0\nFurther Examination and Reason (0-10) 7.6±3.9 9.1±2.6 9.4±2.4 10.0±0.0\nPrinciples and Plans of Treatment (0-20) 11.2±4.0 14.7±3.2 15.6±3.4 15.6±3.4\nBreadth and Depth of Results (0-20) 11.5±2.3 13.5±2.3 14.1±1.9 14.7±1.2\nThinking and Expressing Ability (0-10) 5.9±2.6 7.9±2.5 8.5±2.3 10.0±0.0\nTotal Score (0-100) 75.0±11.6 85.3±7.8 87.6±7.1 90.3±3.6\nARTICLE\n8  　　　The Innovation Medicine 1(2): 100019, September 21, 2023 www.the-innovation.org/medicine\ninformation. Second, in the context of ‘Principles and Plans of Treatment’, the\nmarked improvement  along  the  four  types  of  prompts  indicates  the  impor-\ntance of diverse and complex prompts in formulating an effective treatment\nplan.  The  total  scores  also  follow  the  same  trend,  with  the  Multi-strategy\nachieving  the  highest  scores  across  all  disease  conditions  consistently.\nThese  results  provide  strong  evidence  supporting  the  effectiveness  of\nemploying various  prompt  engineering  strategies,  as  well  as  iterative  ques-\ntioning,  in  enhancing  the  performance  of  GPT-4  in  medical  contexts.\nHowever,  the  exact  impact  and  effectiveness  may  vary  depending  on  the\nspecific disease condition, necessitating further nuanced analysis. \nDISCUSSION\nTo the best of our knowledge, it is evident that our research is pioneering in\nexploring the techniques to optimize LLMs specifically for the recommenda-\ntion  of  treatments  for  gastrointestinal  cancers.  In  contrast  to  studies  that\nsolely use simple prompts,2,5,6 our research assessed a series of prompt engi-\nneering  strategies  including  simple  prompts,  templated  prompts,  ICL,  and\nmulti-round  interaction.  Our  results  demonstrate  that  complex  prompting\napproaches, especially multi-round interaction, are capable of accruing suffi-\ncient diagnostic and therapeutic information pertinent to a specific case. This\napproach  facilitates  the  rational  and  efficient  expansion  of  the  storage  of\nknowledge, thereby substantially enhancing model performance in collecting\nmedical  histories,  forming  accurate  diagnoses,  and  recommending  effective\ntreatments for digestive cancers. The iterative nature of multi-round interac-\ntion consistently yielded the highest scores across evaluation metrics, high-\nlighting  its  reliability  and  broad  applicability.  Our  study  also  necessitates\nfurther exploration. Firstly, our adopted metric, based on clinicians' examina-\ntions, retains some degree of subjectivity. This accentuates the necessity for\na  more  objective  clinical  evaluation  method.  We  are  currently  collaborating\nwith statisticians  to  devise  novel  evaluation  tools  to  measure  model  perfor-\nmance  more  accurately  and  objectively.  Moreover,  our  investigation  was\nsolely  focused  on  tumors  in  the  digestive  system,  indicating  that  future\nresearch  could  extend  the  application  of  LLMs  to  other  types  of  cancers.\nAdditionally,  our  primary  clinical  scenario  was  set  in  China,  with  the  study\nconducted  in  Chinese  before  being  translated  into  English  using  GPT-4.\nAlthough  GPT-4  exhibits  robust  cross-language  performance1,  the  influence\nof  the  selected  language  on  performance  deserves  further  study.  Moreover,\nwe have conducted preliminary assessments of various LLMs including GPT-\n4, Claude, ChatGLM, Wenxin Yiyan, and PaLM. Our findings indicate that GPT-\n4 exhibits superior underlying capabilities compared to its counterparts, lead-\ning us  to  select  it  as  a  representative  of  LLMs.  Nevertheless,  a  comprehen-\nsive  examination  of  the  diverse  and  continually  evolving  LLMs  is  still  an\nimperative area of future research. Last but not least, it is crucial to note that\nthe data used to train GPT-4 predominantly originates from sources outside\nof  China.  However,  due  to  variations  in  clinical  guidelines,  available  medical\ntechnologies,  perceptions  of  risk  and  benefit  by  patients  and  physicians,  as\nwell as disease prevalence trends in different regions, treatment approaches\nfor  gastrointestinal  cancers  can  significantly  differ  across  various  regions.\nSubsequently,  the  outputs  generated  by  GPT-4  may  not  entirely  apply  to\nChinese patients. This particular aspect could potentially impact the evalua-\ntion scores  during  our  comparative  experiments  of  different  prompt  engi-\nneering strategies. To mitigate this potential issue, fine-tuning the LLMs with\ndata  specifically  sourced  from  China  could  provide  a  more  appropriate\napproach.\nRecognizing  the  constraints  and  potential  biases  of  LLMs  is  essential  for\ntheir  responsible  and  ethical  application.  One  major  concern  is  that  LLMs\ngather knowledge from vast amounts of internet data that may contain inher-\nent biases or inaccuracies. To mitigate potential bias and increase the relia-\nbility  of  our  results,  we  employed  a  method  of  inter-rater  reliability  where\neach  output  from  the  model  was  independently  evaluated  by  two  separate\nindividuals. Their evaluations were then compared and reconciled, ensuring a\nmore objective and balanced assessment of the model’s performance. Data\nprivacy and security must be underscored when providing medical records to\nonline  LLMs.  Thus,  we  have  implemented  stringent  data  protection\nmeasures, ensuring all patient data is anonymized and encrypted to protect\nprivacy.  Furthermore,  the  inadequate  judgment  and  critical  thinking  skills  of\nLLMs  when  interpreting  medical  records  limit  their  performance  in  highly\nspecialized  tasks.  To  address  this,  we’ve  fostered  close  collaboration  with\nexpert clinicians and used prompt engineering to assist the model in under-\nstanding  and  handling  complex  medical  information.  We  envision  LLMs  not\nas  replacements  for  healthcare  professionals,  but  rather  as  effective  aid  for\nclinical  decision-making  when  properly  guided.  Future  technological\nadvancements, such as parameter-efficient fine-tuning for specialized tasks\nand the use of vectorized databases, may further contribute to solving these\nissues, offering  better  solutions  for  data  security  and  private  model  deploy-\nment.\nAs we move forward, our findings open up avenues to further refine prompt\nengineering  techniques  to  optimize  LLMs  for  analyzing  patient  data  and\nmedical  literature  to  recommend  evidence-based  treatments  for  digestive\nsystem  cancers.  We  aim  to  explore  how  different  prompts  impact  the\nmodel's  ability  to  accurately  recommend  optimal  interventions  based  on\ntumor characteristics and patient factors. For instance, certain prompts may\nenhance  the  model's  capacity  to  suggest  appropriate  surgical  procedures\ndepending on  tumor  size,  location,  and  staging.  Other  prompts  could  opti-\nmize the  recommendation  of  systemic  therapies  like  chemotherapy  regi-\nmens  and  radiation  therapy  protocols  tailored  to  the  individual's  medical\nhistory  and  cancer  biomarkers.  Advances  in  prompt  engineering  to  account\nfor all relevant clinical variables could enable the generation of more person-\nalized  and  effective  treatment  plans  for  each  unique  patient  case.  However,\nmore research is still urgently needed to ensure patient safety, avoid biases,\nand enable reliable interpretation of model outputs before these systems are\nready for real-world clinical implementation. We must rigorously test prompts\nto identify any that skew recommendations in inappropriate or unsafe ways.\nTransparent reporting of model limitations and close collaboration with medi-\ncal experts will be critical to responsible prompt engineering. While our results\ndemonstrate  immense  promise  for  LLMs  to  enhance  evidence-based deci-\nsion support, translating these tools into practice will require thoughtful and\nethical  design  paired  with  extensive  validation  to  evolve  prompt  engineering\nstrategies  that  provide  trustworthy  guidance  without  ever  replacing  human\nclinical  judgment.  Overall,  steering  LLMs  through  carefully  crafted  prompts\nshows great potential to augment clinicians’ abilities to optimize and person-\nalize treatment plans, propelling more effective cancer care. But as this tech-\nnology  continues  maturing,  maintaining  patient  well-being  through  rigorous\nprompt optimization and evaluation remains imperative. \nCONCLUSION\nThis  study  has  underscored  the  potential  and  challenges  associated  with\nthe  application  of  prompt  engineering  techniques  to  large  language  models\n(LLMs)  in  the  field  of  clinical  oncology.  Through  careful  crafting  of  simple,\ntemplated  prompts  and  more  complex  strategies,  like  in-context  learning\n(ICL)  and  multi-round  interaction,  we  have  seen  promising  capabilities  of\nthese models in processing and interpreting intricate medical data related to\ngastrointestinal cancers.  This  can  substantially  support  healthcare  profes-\nsionals  in  making  decisions  about  recommended  treatments.  However,  it  is\ncrucial  to  continuously  address  the  inherent  limitations  of  these  models,\nincluding potential biases, data privacy concerns, and their specific interpre-\ntative limitations  in  this  clinical  context.  Although  complex  prompts,  espe-\ncially  those  allowing  for  iterative  questioning,  have  shown  great  promise  in\noptimizing  the  performance  of  LLMs,  it's  evident  that  further  investigations\nare needed to refine these strategies and explore their potential integrations.\nAs our  study  was  conducted  in  a  clearly  defined  and  constrained  environ-\nment  to  ensure  consistency,  further  exploration  in  diverse  settings  is\nwarranted to fully exploit the potential of LLMs in healthcare scenarios.\nREFERENCES \n OpenAI,  R.  (2023).  GPT-4  technical  report.  arXiv  2303.08774.  DOI:\n10.48550/arXiv.2303.08774.\n1.\n Lee, P., Bubeck, S., and Petro, J. (2023). Benefits, limits, and risks of GPT-4 as an AI\nchatbot  for  medicine. N.  Engl.  J.  Med. 388: 1233-1239. DOI: 10.1056/\nNEJMsr2214184.\n2.\n Lee, P., Goldberg, C., and Kohane, I. (2023). The AI revolution in medicine: GPT-4 and\nbeyond (Pearson Education, Limited).\n3.\n Xu,  Y.,  Liu,  X.,  Cao,  X.,  et  al.  (2021). Artificial  intelligence:  A  powerful  paradigm  for\nscientific research. The Innovation 2: 100179. DOI: 10.1016/j.xinn.2021.100179.\n4.\n Nori,  H.,  King,  N.,  McKinney,  S.M.,  et  al.  (2023).  Capabilities  of  GPT-4  on  medical5.\nMedicine\nARTICLE\n The Innovation Medicine 1(2): 100019, September 21, 2023　　　  9\nchallenge  problems.  arXiv  preprint  arXiv:2303.13375.  DOI:\n10.48550/arXiv.2303.13375.\n Ayers,  J.W.,  Poliak,  A.,  Dredze,  M.,  et  al.  (2023). Comparing  physician  and  artificial\nintelligence  chatbot  responses  to  patient  questions  posted  to  a  public  social  media\nforum. JAMA Intern. Med. 183: 589-596. DOI: 10.1001/jamainternmed.2023.1838.\n6.\n Haver,  H.L.,  Ambinder,  E.B.,  Bahl,  M.,  et  al.  (2023). Appropriateness  of  breast  cancer\nprevention  and  screening  recommendations  provided  by  ChatGPT. Radiology 307:\ne230424. DOI: 10.1148/radiol.230424.\n7.\n Zhu,  L.,  Mou,  W.,  and  Chen,  R.  (2023). Can  the  ChatGPT  and  other  large  language\nmodels  with  internet-connected  database  solve  the  questions  and  concerns  of\npatient  with  prostate  cancer  and  help  democratize  medical  knowledge? J.  Transl.\nMed. 21: 1-4. DOI: 10.1186/s12967-022-03835-4.\n8.\n Uprety, D., Zhu, D., and West, H.J. (2023). ChatGPT-a promising generative AI tool and\nits implications for cancer care. Cancer 129: 2284-2289. DOI: 10.1002/cncr.34827.\n9.\n Zhong, Y., Chen, Y.J., Zhou, Y., et al. (2023). The artificial intelligence large language\nmodels  and  neuropsychiatry  practice  and  research  ethic. Asian  J.  Psychiatr. 84:\n103577. DOI: 10.1016/j.ajp.2023.103577.\n10.\n Young,  J.N.,  Ross,  O.H.,  Poplausky,  D.,  et  al.  (2023).  The  utility  of  ChatGPT  in\ngenerating  patient-facing  and  clinical  responses  for  melanoma.  J.  Am.  Acad.\nDermatol. 89: 602-604. DOI: 10.1016/j.jaad.2023.05.024.\n11.\n Xie,  Y.,  Seth,  I.,  Hunter-Smith,  D.J.,  et  al.  (2023).  Aesthetic  surgery  advice  and\ncounseling from artificial intelligence: a rhinoplasty consultation with ChatGPT. Aesth.\nPlast. Surg. 47: 1985-1993. DOI: 10.1007/s00266-023-03338-7.\n12.\n Buzzaccarini,  G.,  Degliuomini,  R.S.,  and  Borin,  M.  (2023).  The  artificial  intelligence\napplication  in  aesthetic  medicine:  How  ChatGPT  can  revolutionize  the  aesthetic\nworld. Aesth. Plast. Surg. 47: 2211-2212. DOI: 10.1007/s00266-023-03416-w.\n13.\n Radford,  A.,  Wu,  J.,  Child,  R.,  et  al.  (2019).  Language  models  are  unsupervised\nmultitask  learners.  OpenAI  blog 1:  9.  https://cdn.openai.com/better-language-\nmodels/language_models_are_unsupervised_multitask_learners.pdf.\n14.\n Brown, T., Mann, B., Ryder, N., et al. (2020). Language models are few-shot learners.\nNeurIPS 33: 1877−1901. DOI: 10.48550/arXiv.2005.14165.\n15.\n Lewis,  P.,  Perez,  E.,  Piktus,  A.,  et  al.  (2020). Retrieval-augmented  generation  for\nknowledge-intensive  NLP  tasks. NeurIPS 33: 9459−9474. DOI: 10.48550/arXiv.2005.\n11401.\n16.\n Wei,  J.,  Wang,  X.,  Schuurmans,  D.,  et  al.  (2022). Chain-of-thought  prompting  elicits\nreasoning in large language models. NeurIPS 35: 24824−24837. DOI: 10.48550/arXiv.\n2201.11903.\n17.\n Zhou, D., Schärli, N., Hou, L., et al. (2022). Least-to-most prompting enables complex\nreasoning  in  large  language  models.  arXiv  preprint  arXiv:2205.10625.  DOI:\n10.48550/arXiv.2205.10625.\n18.\n Yao, S., Yu, D., Zhao, J., et al. (2023). Tree of thoughts: Deliberate problem solving with\nlarge  language  models.  arXiv  preprint  arXiv:2305.10601.  DOI:\n10.48550/arXiv.2305.10601.\n19.\n Fu,  Y.,  Peng,  H.,  Sabharwal,  A.,  et  al.  (2022).  Complexity-based  prompting  for  multi-\nstep reasoning. arXiv preprint arXiv:2210.00720. DOI: 10.48550/arXiv.2210.00720.\n20.\n Khot,  T.,  Trivedi,  H.,  Finlayson,  M.,  et  al.  (2022).  Decomposed  prompting:  A  modular\napproach  for  solving  complex  tasks.  arXiv  preprint  arXiv:2210.02406.  DOI:\n10.48550/arXiv.2210.02406.\n21.\n White, J., Fu, Q., Hays, S., et al. (2023). A prompt pattern catalog to enhance prompt22.\nengineering  with  ChatGPT.  arXiv  preprint  arXiv:2302.11382.  DOI:\n10.48550/arXiv.2302.11382.\n Suzgun,  M.,  Scales,  N.,  Schärli,  N.,  et  al.  (2022).  Challenging  big-bench  tasks  and\nwhether  chain-of-thought  can  solve  them.  arXiv  preprint  arXiv:2210.09261.  DOI:\n10.48550/arXiv.2210.09261.\n23.\nFUNDING AND ACKNOWLEDGMENTS \nThis  work  was  supported  by  the  National  Natural  Science  Foundation  of  China\n(91959205 to L.S., U22A20327 to L.S., 82203881 to Y.C., 82272627 to XT.Z., 7232018 to\nY.S.,  12090022  to  B.D.,  11831002  to  B.D.,  81801778  to  L.Z.),  Beijing  Natural  Science\nFoundation  (7222021  to  Y.C.,  Z200015  to  XT.Z.),  Beijing  Hospitals  Authority  Youth\nProgramme (QML20231115 to Y.C.), Clinical Medicine Plus X-Young Scholars Project of\nPeking University (PKU2023LCXQ041 to Y.C. and L.Z.). Guangdong Provincial Key Labo-\nratory of Precision Medicine for Gastrointestinal Cancer (2020B121201004). The funders\nhad no role in study design, data collection and analysis, decision to publish or prepara-\ntion of the manuscript.\nAUTHOR CONTRIBUTIONS \nJ. Yuan, P. Bao, Z. Chen, M. Yuan, and J. Pan contributed to data analysis and inter-\npretation, and drafted the manuscript. J. Zhao, Y. Xie, Y. Cao, Y. Wang, Z. Wang, Z. Lu, X.\nZhang, J Li and L. Ma performed the sample preparation. Y. Chen, L. Zhang, L. Shen, and\nB. Dong planned the study and participated in manuscript revision. All authors have given\nfinal approval for themanuscript to be published and have agreed to be responsible for all\naspects of the manuscript.\nDECLARATION OF INTERESTS \nThe authors declare no competing interests.\nETHICAL STATEMENT AND PATIENT CONSENT \nNot applicable.\nDATA AND CODE AVAILABILITY \nA  pre-trained  Chinese  BERT  model  in  Hugging  Face  (https://huggingface.\nco/hfl/chinese-bert-wwm-ext), specifically the “hfl/chinese-bert-wwm-ext”, is utilized to\ntranslate patient conditions into a high-dimensional embedding space (768 dimensions\nin this study), capturing the context of the condition effectively.\nSUPPLEMENTAL INFORMATION \nIt can be found online at https://doi.org/10.59717/j.xinn-med.2023.100019\nLEAD CONTACT WEBSITE \nhttp://faculty.bicmr.pku.edu.cn/~dongbin/\nARTICLE\n10  　　　The Innovation Medicine 1(2): 100019, September 21, 2023 www.the-innovation.org/medicine",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.5810823440551758
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.4934898614883423
    },
    {
      "name": "Health care",
      "score": 0.46852749586105347
    },
    {
      "name": "Medicine",
      "score": 0.3735479414463043
    },
    {
      "name": "Psychology",
      "score": 0.35611990094184875
    },
    {
      "name": "Computer science",
      "score": 0.2995528280735016
    },
    {
      "name": "Artificial intelligence",
      "score": 0.19263768196105957
    },
    {
      "name": "Political science",
      "score": 0.1392674744129181
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210093964",
      "name": "Peking University Cancer Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I198357462",
      "name": "Changsha University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210133846",
      "name": "Peking University International Hospital",
      "country": "CN"
    }
  ]
}