{
  "title": "Benchmarking Detection Transfer Learning with Vision Transformers",
  "url": "https://openalex.org/W3216272314",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2744284997",
      "name": "Li, Yanghao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3040761553",
      "name": "Xie, Saining",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1026985317",
      "name": "Chen Xinlei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3024273107",
      "name": "Dollár, Piotr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2679642031",
      "name": "He, Kaiming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749197864",
      "name": "Girshick, Ross",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W343636949",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W3111156583",
    "https://openalex.org/W2991391304",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3171206729",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3116271762",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W3114896399",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2963975324",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3211983881",
    "https://openalex.org/W2462831000"
  ],
  "abstract": "Object detection is a central downstream task used to test if pre-trained network parameters confer benefits, such as improved accuracy or training speed. The complexity of object detection methods can make this benchmarking non-trivial when new architectures, such as Vision Transformer (ViT) models, arrive. These difficulties (e.g., architectural incompatibility, slow training, high memory consumption, unknown training formulae, etc.) have prevented recent studies from benchmarking detection transfer learning with standard ViT models. In this paper, we present training techniques that overcome these challenges, enabling the use of standard ViT models as the backbone of Mask R-CNN. These tools facilitate the primary goal of our study: we compare five ViT initializations, including recent state-of-the-art self-supervised learning methods, supervised initialization, and a strong random initialization baseline. Our results show that recent masking-based unsupervised learning methods may, for the first time, provide convincing transfer learning improvements on COCO, increasing box AP up to 4% (absolute) over supervised and prior self-supervised pre-training methods. Moreover, these masking-based initializations scale better, with the improvement growing as model size increases.",
  "full_text": "Benchmarking Detection Transfer Learning with Vision Transformers\nYanghao Li Saining Xie Xinlei Chen Piotr Doll ´ar Kaiming He Ross Girshick\nFacebook AI Research (FAIR)\nAbstract\nObject detection is a central downstream task used to\ntest if pre-trained network parameters confer beneﬁts, such\nas improved accuracy or training speed. The complexity\nof object detection methods can make this benchmarking\nnon-trivial when new architectures, such as Vision Trans-\nformer (ViT) models, arrive. These difﬁculties ( e.g., archi-\ntectural incompatibility, slow training, high memory con-\nsumption, unknown training formulae, etc.) have prevented\nrecent studies from benchmarking detection transfer learn-\ning with standard ViT models. In this paper, we present\ntraining techniques that overcome these challenges, en-\nabling the use of standard ViT models as the backbone of\nMask R-CNN. These tools facilitate the primary goal of our\nstudy: we compare ﬁve ViT initializations, including re-\ncent state-of-the-art self-supervised learning methods, su-\npervised initialization, and a strong random initialization\nbaseline. Our results show that recent masking-based un-\nsupervised learning methods may, for the ﬁrst time, provide\nconvincing transfer learning improvements on COCO, in-\ncreasing AP box up to 4% (absolute) over supervised and\nprior self-supervised pre-training methods. Moreover, these\nmasking-based initializations scale better, with the improve-\nment growing as model size increases.\n1. Introduction\nUnsupervised/self-supervised deep learning is com-\nmonly used as a pre-training step that initializes model pa-\nrameters before they are transferred to a downstream task,\nsuch as image classiﬁcation or object detection, for ﬁne-\ntuning. The utility of an unsupervised learning algorithm\nis judged by downstream task metrics ( e.g. accuracy, con-\nvergence speed, etc.) in comparison to baselines, such as\nsupervised pre-training or no pre-training at all, i.e., ran-\ndom initialization (often called training “from scratch”).\nUnsupervised deep learning in computer vision typically\nuses standard convolutional network (CNN) models [25],\nsuch as ResNets [20]. Transferring these models is rela-\ntively straightforward because CNNs are in widespread use\nin most downstream tasks, and thus benchmarking proto-\ncols are easy to deﬁne and baselines are plentiful (e.g. [17]).\nIn other words, unsupervised learning with CNNs produces\na plug-and-play parameter initialization.\nWe are now witnessing the growth of unsupervised learn-\ning with Vision Transformer (ViT) models [10], and while\nthe high-level transfer learning methodology remains the\nsame, the low-level details and baselines for some important\ndownstream tasks have not been established. Notably, ob-\nject detection, which has played a central role in the study of\ntransfer learning over the last decade (e.g., [35, 14, 9, 17]),\nwas not explored in the pioneering work on ViT train-\ning [10, 7, 5]—supervised or unsupervised—due to the\nchallenges (described shortly) of integrating ViTs into com-\nmon detection models, like Mask R-CNN [19].\nTo bridge this gap, this paper establishes a transfer learn-\ning protocol for evaluating ViT models on object detection\nand instance segmentation using the COCO dataset [28] and\nthe Mask R-CNN framework. We focus on standard ViT\nmodels, with minimal modiﬁcations, as deﬁned in the orig-\ninal ViT paper [10], because we expect this architecture will\nremain popular in unsupervised learning work over the next\nfew years due to its simplicity and ﬂexibility when explor-\ning new techniques, e.g., masking-based methods [1, 16].\nEstablishing object detection baselines for ViT is chal-\nlenging due to technical obstacles that include mitigat-\ning ViT’s large memory requirements when processing\ndetection-sized inputs ( e.g., ∼20×more patches than in\npre-training), architectural incompatibilities ( e.g., single-\nscale ViT vs. a multi-scale detector), and developing ef-\nfective training formulae ( i.e., learning schedules, regular-\nization and data augmentation methods, etc.) for numerous\npre-trained initializations, as well as random initialization.\nWe overcome these obstacles and present strong ViT-based\nMask R-CNN baselines on COCO when initializing ViT\nfrom-scratch [18], with pre-trained ImageNet [8] supervi-\nsion, and with unsupervised pre-training using recent meth-\nods like MoCo v3 [7], BEiT [1], and MAE [16].\nLooking beyond ViT, we hope our practices and obser-\nvations will serve as a blueprint for future work comparing\npre-training methods for more advanced ViT derivatives,\nlike Swin [29] and MViT [12]. To facilitate community de-\nvelopment we will release code in Detectron2 [40].\n1\narXiv:2111.11429v1  [cs.CV]  22 Nov 2021\nup 4xfeature pyramid network (FPN)Mask R-CNN components(RPN, box head, mask head)\nup 2x\nViT(scale 1/16)\nmulti-scaleadaptorinputpatches\nblockd/4globalattention\nblock2d/4globalattention\nblock3d/4globalattention\nblockdglobalattention\nblock1windowedattention\ndown2xidentity\nwindowedblockswindowedblocks windowedblockswindowedblocks\nFigure 1. ViT-based Mask R-CNN.In §2 we describe how a stan-\ndard ViT model can be used effectively as the backbone in Mask\nR-CNN. To save time and memory, we modify the ViT to use non-\noverlapping windowed attention in all but four of its Transformer\nblocks, spaced at an interval of d/4, where d is the total number\nof blocks (blue) [26]. To adapt the single-scale ViT to the multi-\nscale FPN (yellow), we make use of upsampling and downsam-\npling modules (green) [11]. The rest of the system (light red) uses\nupgraded, but standard, Mask R-CNN components.\n2. Approach\nWe select the Mask R-CNN [19] framework due to its\nubiquitous presence in object detection and transfer learn-\ning research. Mask R-CNN is the foundation of higher\ncomplexity/higher performing systems, such as Cascade\nR-CNN [4] and HTC/HTC++ [6, 29], which may improve\nupon the results presented here at the cost of additional\ncomplexity that is orthogonal to the goal of benchmarking\ntransfer learning. Our choice attempts to balance (relative)\nsimplicity vs. complexity while providing compelling, even\nthough not entirely state-of-the-art, results.\nWe conﬁgure Mask R-CNN with a number of upgraded\nmodules (described in §2.2) and training procedures (de-\nscribed in §2.3) relative to the original publication. These\nupgrades, developed primarily in [39, 18, 13], allow the\nmodel to be trained effectively from random initialization,\nthus enabling a meaningful from-scratch baseline. Next, we\nwill discuss how the backbone, which would typically be a\nResNet, can be replaced with a Vision Transformer.\n2.1. ViT Backbone\nIn this section we address two technical obstacles when\nusing ViT as the backbone in Mask R-CNN: (1) how to\nadapt it to work with a feature pyramid network (FPN) [27]\nand (2) how to reduce its memory footprint and runtime to\nmake benchmarking large ViT backbones tractable.\nFPN Compatibility. Mask R-CNN can work with a back-\nbone that either produces a single-scale feature map or fea-\nture maps at multiple scales that can be input into an FPN.\nSince FPN typically provides better detection results with\nminimal time and memory overhead, we adopt it.\nHowever, using FPN presents a problem because ViT\nproduces feature maps at a single scale ( e.g., 1/16th), in\ncontrast to the multi-scale feature maps produced by typical\nCNNs.1 To address this discrepancy, we employ a simple\ntechnique from [11] (used for the single-scale XCiT back-\nbone) to either upsample or downsample intermediate ViT\nfeature maps by placing four resolution-modifying modules\nat equally spaced intervals ofd/4 transformer blocks, where\ndis the total number of blocks. See Figure 1 (green blocks).\nThe ﬁrst of these modules upsamples the feature map by\na factor of4 using a stride-two2×2 transposed convolution,\nfollowed by group normalization [39] and GeLU [21], and\nﬁnally another stride-two 2×2 transposed convolution. The\nnext d/4th block’s output is upsampled by2×using a single\nstride-two 2 ×2 transposed convolution (without normal-\nization and non-linearity). The next d/4th block’s output is\ntaken as is and the ﬁnal ViT block’s output is downsampled\nby a factor of two using stride-two2×2 max pooling. Each\nof these modules preserves the ViT’s embedding/channel\ndimension. Assuming a patch size of 16, these modules\nproduce feature maps with strides of 4, 8, 16, and 32 pixels,\nw.r.t. the input image, that are ready to input into an FPN.\nWe note that recent work, such as Swin [29] and\nMViT [12], address the single vs. multi-scale feature map\nproblem by modifying the core ViT architecture (in pre-\ntraining) so it is inherently multi-scale. This is an important\ndirection, but it also complicates the simple ViT design and\nmay impede the exploration of new unsupervised learning\ndirections, such as methods that sparsely process unmasked\npatches [16]. Therefore, we focus on external additions to\nViTs that allow them to integrate into multi-scale detection\nsystems. We also note that Beal et al. [2] integrate stan-\ndard ViT models with Faster R-CNN [34], but report sub-\nstantially lower APbox compared to our results (>10 points\nlower), which suggests that our design is highly effective.\nReducing Memory and Time Complexity. Using ViT as\na backbone in Mask R-CNN introduces memory and run-\ntime challenges. Each self-attention operation in ViT takes\nO(h2w2) space and time for an image tiled (or “patchiﬁed”)\ninto h×wnon-overlapping patches [38].\nDuring pre-training, this complexity is manageable as\nh = w = 14is a typical setting (a 224 ×224 pixel image\npatchiﬁed into 16 ×16 pixel patches). In object detection,\na standard image size is 1024 ×1024— approximately 21×\nmore pixels and patches. This higher resolution is needed in\norder to detect relatively small objects as well as larger ones.\nDue to the quadratic complexity of self-attention, even the\n“base” size ViT-B may consume∼20–30GB of GPU mem-\nory when used in Mask R-CNN with a single-image mini-\nbatch and half -precision ﬂoating point numbers.\n1We view the natural 2D spatial arrangement of intermediate ViT patch\nembeddings as a standard 2D feature map.\n2\nTo reduce space and time complexity we use restricted\n(or “windowed”) self-attention [38], which saves both space\nand time by replacing global computation with local com-\nputation. We partition the h ×w patchiﬁed image into\nr ×r patch non-overlapping windows and compute self-\nattention independently within each of these windows. This\nwindowed self-attention has O(r2hw) space and time com-\nplexity (from O(r4) per-window complexity andh/r×w/r\nwindows). We set rto the global self-attention size used in\npre-training (e.g., r= 14is typical).\nA drawback of windowed self-attention is that the back-\nbone does not integrate information across windows. There-\nfore we adopt the hybrid approach from [26] that includes\nfour global self-attention blocks placed evenly at eachd/4th\nblock (these coincide with the up-/downsampling locations\nused for FPN integration; see Figure 1).\n2.2. Upgraded Modules\nRelative to the original Mask R-CNN in [19], we mod-\nernize several of its modules. Concisely, the modiﬁcations\ninclude: (1) following the convolutions in FPN with batch\nnormalization (BN) [23], (2) using two convolutional lay-\ners in the region proposal network (RPN) [33] instead of\none, (3) using four convolutional layers with BN followed\nby one linear layer for the region-of-interest (RoI) classiﬁ-\ncation and box regression head [39] instead of a two-layer\nMLP without normalization, (4) and following the convo-\nlutions in the standard mask head with BN. Wherever BN\nis applied, we use synchronous BN across all GPUs. These\nupgrades are implemented in the Detectron2 model zoo.2\n2.3. Training Formula\nWe adopt an upgraded training formula compared to the\noriginal Mask R-CNN. This formula was developed in [18],\nwhich demonstrated good from-scratch performance when\ntraining with normalization layers and for long enough,\nand [13], which demonstrated that a simple data augmen-\ntation method called large-scale jitter (LSJ) is effective at\npreventing overﬁtting and improves results when models\nare trained for very long schedules (e.g., 400 epochs).\nWe aim to keep the number of hyperparameters low and\ntherefore resist adopting additional data augmentation and\nregularization techniques. However, we found that drop\npath regularization [24, 22] is highly effective for ViT back-\nbones and therefore we include it ( e.g., it improves from-\nscratch training by up to 2 APbox).\nIn summary, we train all models with the same sim-\nple formula: LSJ ( 1024 ×1024 resolution, scale range\n[0.1,2.0]), AdamW [30] ( β1,β2 = 0.9,0.999) with half-\nperiod cosine learning rate decay, linear warmup [15] for\n2https://github.com/facebookresearch/detectron2/blob/\nmain / MODEL _ ZOO . md # new - baselines - using - large - scale -\njitter-and-longer-training-schedule\n0.25 epochs, and drop path regularization. When using a\npre-trained initialization, we ﬁne-tune Mask R-CNN for up\nto 100 epochs. When training from scratch, we consider\nschedules of up to 400 epochs since convergence is slower\nthan when using pre-training. We distribute training over\n32 or 64 GPUs (NVIDIA V100-32GB) and always use a\nminibatch size of 64 images. We use PyTorch’s automatic\nmixed precision. Additional hyperparameters are tuned by\nthe consistent application of a protocol, describe next.\n2.4. Hyperparameter Tuning Protocol\nTo adapt the training formula to each model, we tune\nthree hyperparameters—learning rate ( lr), weight decay\n(wd), and drop path rate (dp)—while keeping all others the\nsame for all models. We conducted pilot experiments us-\ning ViT-B pre-trained with MoCo v3 to estimate reasonable\nhyperparameter ranges. Based on these estimates we estab-\nlished the following tuning protocol:\n(1) For each initialization (from-scratch, supervised,\netc.), we ﬁx dp at 0.0 and perform a grid search over lr and\nwd using ViT-B and a 25 epoch schedule (or 100 epochs\nwhen initializing from scratch). We center a 3 ×3 grid at\nlr, wd = 1.6e−4, 0.1 and use doubled and halved values\naround the center. If a local optimum is not found ( i.e. the\nbest value is a boundary value), we expand the search.\n(2) For ViT-B, we select dp from {0.0,0.1,0.2,0.3}us-\ning a 50 epoch schedule for pre-trained initializations. The\nshorter 25 epoch schedule was unreliable and 100 epochs\nwas deemed impractical. For random initialization we’re\nforced to use 100 epochs due to slow convergence. We\nfound that dp = 0.1 is optimal for all initializations.\n(3) For ViT-L, we adopt the optimallr and wd from ViT-\nB (searching with ViT-L is impractical) and ﬁnd dp = 0.3\nis best using the same procedure as for ViT-B.\nLimitations. The procedure above takes practical shortcuts\nto reduce the full hyperparameter tuning space. In particu-\nlar, lr and wd are optimized separately from dp, thus the\ncombination may be suboptimal. Further, we only tune lr\nand wd using ViT-B, therefore the choice may be subopti-\nmal for ViT-L. We also tunelr and wd using a schedule that\nis 4×shorter than the longest schedule we eventually train\nat, which again may be suboptimal. Given these limitations\nwe aim to avoid biasing results by applying thesame tuning\nprotocol to all initializations.\nFinally, we note that we tunelr, wd, and dp on the COCO\n2017 val split and report results on the same split. While\ntechnically not an ML best-practice, a multitude of com-\nparisons on COCO val vs. test-dev results over many\nyears demonstrate that overﬁtting in not a concern for this\nkind of low-degree-of-freedom hyperparameter tuning.3\n3E.g., Table 2 in [29] (version 1) shows that text-dev APbox is sys-\ntematically higher than val APbox in seven system-level comparisons.\n3\n2.5. Additional Implementation Details\nImages are padded during training and inference to form\na 1024 ×1024 resolution input. During training, padding\nis necessary for batching. During (unbatched) inference,\nthe input only needs to be a multiple of the ViT patch size\non each side, which is possibly less than 1024 on one side.\nHowever, we found that such reduced padding performs\nworse (e.g., decrease of ∼0.5–1 APbox) than padding to the\nsame resolution used during training, likely due to ViT’s use\nof positional information. Therefore, we use a 1024 ×1024\nresolution input at inference time, even though the extra\npadding slows inference time by ∼30% on average.\n3. Initialization Methods\nWe compare ﬁve initialization methods, which we brieﬂy\nsummarize below.\nRandom: All network weights are randomly initialized\nand no pre-training is used. The ViT backbone initialization\nfollows the code of [1] and the Mask R-CNN initialization\nuses the defaults in Detectron2 [40].\nSupervised: The ViT backbone is pre-trained for super-\nvised classiﬁcation using ImageNet-1k images and labels.\nWe use the DeiT released weights [36] for ViT-B and the\nViT-L weights from [16], which uses an even stronger train-\ning formula than DeiT to avoid overﬁtting (moreover, the\nDeiT release does not include ViT-L). ViT-B and ViT-L\nwere pre-trained for 300 and 200 epochs, respectively.\nMoCo v3: We use the unsupervised ImageNet-1k pre-\ntrained ViT-B and ViT-L weights from the authors of [7]\n(ViT-B is public; ViT-L was provided via private communi-\ncation). These models were pre-trained for 300 epochs.\nBEiT: Since ImageNet-1k pre-trained weights are not\navailable, we use the ofﬁcial BEiT code release [1] to train\nViT-B and ViT-L ourselves for 800 epochs (the default train-\ning length used in [1]) on unsupervised ImageNet-1k.\nMAE: We use the ViT-B and ViT-L weights pre-trained\non unsupervised ImageNet-1k from the authors of [16].\nThese models were pre-trained for 1600 epochs using nor-\nmalized pixels as the target.\n3.1. Nuisance Factors in Pre-training\nWe attempt to make comparisons as equally matched as\npossible, yet there are pre-training nuisance factors, listed\nbelow, that differ across methods.\n(1) Different pre-training methods may use different\nnumbers of epochs. We adopt the default number of pre-\ntraining epochs from the respective papers. While these\nvalues may not appear comparable, the reality is unclear:\nnot all methods may beneﬁt equally from longer training\nand not all methods have the same per-epoch training cost\n(e.g., BEiT uses roughly 3×more ﬂops than MAE).\npre-training AP box APmask\ninitialization data ViT-B ViT-L ViT-B ViT-L\nsupervised IN1k w/ labels 47.9 49.3 42.9 43.9\nrandom none 48.9 50.7 43.6 44.9\nMoCo v3 IN1k 47.9 49.3 42.7 44.0\nBEiT IN1k+DALL·E 49.8 53.3 44.4 47.1\nMAE IN1k 50.3 53.3 44.9 47.2\nTable 1. COCO object detection and instance segmentationus-\ning our ViT-based Mask R-CNN baseline. Results are reported on\nCOCO 2017 val using the best schedule length (see Figure 2).\nRandom initialization does not use any pre-training data, super-\nvised initialization uses IN1k with labels, and all other initializa-\ntions use IN1k without labels. Additionally, BEiT uses a dV AE\ntrained on the proprietary DALL·E dataset of ∼250M images [32].\n(2) BEiT uses learned relative position biases that are\nadded to the self-attention logits [31] in each block, instead\nof the absolute position embeddings used by the other meth-\nods. To account for this, albeit imperfectly, we include\nboth relative position biases and absolute position embed-\ndings in all detection models regardless of their use in pre-\ntraining. For BEiT, we transfer the pre-trained biases and\nrandomly initialize the absolute position embeddings. For\nall other methods, we zero-initialize the relative position bi-\nases and transfer the pre-trained absolute position embed-\ndings. Relative position biases are shared across windowed\nattention blocks and (separately) shared across global atten-\ntion blocks. When there is a spatial dimension mismatch be-\ntween pre-training and ﬁne-tuning, we resize the pre-trained\nparameters to the required ﬁne-tuning resolution.\n(3) BEiT makes use of layer scale [37] in pre-training,\nwhile the other methods do not. During ﬁne-tuning, the\nBEiT-initialized model must also be parameterized to use\nlayer scale with the pre-trained layer scaling parameters ini-\ntialized from the pre-trained model. All other models do not\nuse layer scale in pre-training or in ﬁne-tuning.\n(4) We try to standardize pre-training data to ImageNet-\n1k, however BEiT uses the DALL·E [32] discrete V AE\n(dV AE), which was trained on∼250 million proprietary and\nundisclosed images, as an image tokenizer. The impact of\nthis additional training data is not fully understood.\n4. Experiments and Analysis\n4.1. Comparing Initializations\nResults. In Table 1, we compare COCO ﬁne-tuning results\nusing the pre-trained initializations and random initializa-\ntion described in §3. We show results after maximizing\nAPbox over the considered training lengths: 25, 50, or 100\nepochs for pre-trained initializations, and 100, 200, or 400\nepochs for random initialization. (We discuss convergence\nbelow.) Next, we make several observations.\n(1) Our updated Mask R-CNN trains smoothly with\nViT-B and ViT-L backbones regardless of the initialization\n4\nFine-tuning epochs45\n46\n47\n48\n49\n50\n51APbox\nViT-B Mask R-CNN\nInitialization\nMAE\nBEiT\nrandom\nMoCo v3\nsupervised IN1k\n25 50 100 200 400\nFine-tuning epochs (log scale)\n46\n47\n48\n49\n50\n51\n52\n53APbox\nViT-L Mask R-CNN\nFigure 2. Impact of ﬁne-tuning epochs. Convergence plots for\nﬁne-tuning from 25 and 400 epochs on COCO. All pre-trained\ninitializations converge much faster ( ∼4×) compared to random\ninitialization, though they achieve varied peak APbox. The perfor-\nmance gap between the masking-based methods (MAE and BEiT)\nand all others is visually evident. When increasing model scale\nfrom ViT-B (top) to ViT-L (bottom), this gap also increases, sug-\ngesting that these methods may have superior scaling properties.\nmethod. It does not exhibit instabilities nor does it require\nstabilizing techniques like gradient clipping.\n(2) Training from scratch yields up to 1.4 higher APbox\nthan ﬁne-tuning from supervised IN1k pre-training (50.7vs.\n49.3). While the higher AP may sound surprising, the same\ntrend is observed in [13]. Supervised pre-training is not al-\nways a stronger baseline than random initialization.\n(3) The contrastive learning-based MoCo v3 underper-\nforms random initialization’s AP and has similar results\ncompared to supervised initialization.\n(4) For ViT-B, BEiT and MAE outperform both random\ninitialization by up to 1.4 AP box (50.3 vs. 48.9) and super-\nvised initialization by up to 2.4 APbox (50.3 vs. 47.9).\n(5). For ViT-L, the APbox gap increases, with BEiT and\nMAE substantially outperforming both random initializa-\ntion by up to 2.6 AP box (53.3 vs. 50.7) and supervised ini-\ntialization by up to 4.0 APbox (53.3 vs. 49.3).\nConvergence. In Figure 2 we show how pre-training im-\npacts ﬁne-tuning convergence. Given the tuned hyperpa-\nrameters for each initialization method, we train models for\n2×and 4×longer (and also0.5×for random initialization).\nGenerally, we ﬁnd that all pre-trained initializations signif-\nicantly accelerate convergence compared to random initial-\nization, as observed in [18]. Most methods show signs of\noverﬁtting when the training schedule is made sufﬁciently\nlong, typically by 100 epochs for pre-trained initializations\nand 400 epochs for random initialization. Based on this\ndata, pre-training tends to accelerate training on COCO by\nroughly 4×compared to random initialization.\nWe also note two caveats about these results: (i) The\ndrop path rate should ideally be tuned for each training du-\nration as we have observed that the optimal dp value may\nneed to increase when models are trained for longer. (How-\never, performing an exhaustive dp sweep for all initializa-\ntions, model sizes, and training durations is likely compu-\ntationally impractical.) (ii) Moreover, it may be possible to\nachieve better results in all cases by training for longer un-\nder a more complex training formula that employs heavier\nregularization and stronger data augmentation.\nDiscussion. The COCO dataset is a challenging setting for\ntransfer learning. Due to the large training set ( ∼118k im-\nages with ∼0.9M annotated objects), it is possible to achieve\nstrong results when training from random initialization. We\nﬁnd that existing methods, like supervised IN1k or unsu-\npervised MoCo v3 pre-training, actually underperform the\nAP of the random initialization baseline (though they yield\nfaster convergence). Prior works reporting unsupervised\ntransfer learning improvements on COCO ( e.g., [17]) tend\nto show modest gains over supervised pre-training ( e.g.,\n∼1 AP box) and do not include a strong random initializa-\ntion baseline as we do here (because strong training formu-\nlae based on large-scale jitter had not yet been developed).\nMoreover, they use weaker models and report results that\nare overall much lower (e.g., ∼40 APbox) making it unclear\nhow well the ﬁndings translate to state-of-the-art practices.\nWe ﬁnd that MAE and BEiT provide the ﬁrst convincing\nresults of substantial COCO AP improvements due to pre-\ntraining. Moreover, these masking-based methods show the\npotential to improve detection transfer learning as model\nsize increases. We do not observe this important scaling\ntrend with either supervised IN1k pre-training or unsuper-\nvised contrastive learning, as represented by MoCo v3.\n4.2. Ablations and Analysis\nWe ablate several factors involved in the system compar-\nison, analyze model complexity, and report tuned hyperpa-\nrameter values. For these experiments, we use MAE and 50\nepoch ﬁne-tuning by default.\nSingle-scale vs. Multi-scale. In Table 2 we compare our\ndefault FPN-based multi-scale detector to a single-scale\nvariant. The single-scale variant simply applies RPN and\nRoIAlign [19] to the ﬁnal 1/16th resolution feature map\ngenerated by the ViT backbone. The RoI heads and all\nother choices are the same between the systems (in partic-\nular, note that both use the same hybrid windowed/global\nattention). We observe that the multi-scale FPN design in-\n5\nAPbox\nFPN ViT-B ViT-L\nyes 50.1 53.3\nno 48.4 52.0\nTable 2. Single-scale vs. multi-scale (FPN) ablation. FPN yields\nconsistent improvements. Our default setting is marked in gray.\nself-attention\nact\ncheckpt AP box\ntrain\nmem\ntrain\ntime\ntest\ntime\n(1) windowed no 50.7 16GB 0.67s 0.34s\n(2) windowed, 4 global no 53.3 27GB 0.93s 0.40s\n(3) global yes 53.1 14GB 2.26s 0.65s\n(4) global no - OOM - -\nTable 3. Memory and time reduction strategies. We com-\npare methods for reducing memory and time when using ViT-L in\nMask R-CNN. The strategies include: (1) replace all global self-\nattention with 14 × 14 non-overlapping windowed self-attention,\n(2) a hybrid that uses both windowed and global self-attention, or\n(3) all global attention with activation checkpointing. Without any\nof these strategies (row 4) an out-of-memory (OOM) error pre-\nvents training. We report AP box, peak GPU training memory, av-\nerage per-iteration training time, and average per-image inference\ntime using NVIDIA V100-32GB GPUs. The per-GPU batch size\nis 1. Our defaults (row 2) achieves a good balance between mem-\nory, time, and APbox metrics. In fact, our hybrid approach achieves\ncomparable APbox to full global attention, while being much faster.\ncreases AP box by ∼1.3-1.7 ( e.g., 50.1 vs. 48.4), while in-\ncreasing training and inference time by ∼5 and ∼10% rela-\ntive, respectively. Multi-scale memory overhead is <1%.\nMemory and Time Reduction. In Table 3 we compare\nseveral strategies for reducing memory and time complexity\nwhen using a standard ViT backbone in Mask R-CNN. Us-\ning a combination of 14 ×14 non-overlapping windowed\nself-attention together with four global attention blocks\nachieves a good balance between memory, training and in-\nference time, and AP metrics. This ﬁnding motivates us\nto use this setting as our default. Somewhat surprisingly us-\ning only windowed attention isnot catastrophic even though\nthe backbone processes all windows entirely independently\n(APbox decreases from 53.3 to 50.7). This is likely due to\ncross-window computation introduced by convolutions and\nRoIAlign in the rest of the Mask R-CNN model.\nPositional Information. In the default BEiT code, the ViT\nis modiﬁed to use relative position biases [31] in each trans-\nformer block instead of adding absolute position embed-\ndings to the patch embeddings. This choice is an orthog-\nonal enhancement that is not used by the other pre-training\nmethods (though it could be). In an attempt to make the\ncomparison more equal, we include these biases (and abso-\nlute position embeddings) in all ﬁne-tuning models by de-\nfault, as discussed in §3.1.\nIn Table 4 we study the effect of relative position biases\npre-train (pt) ﬁne-tuning APbox\ninitialization abs rel abs rel ViT-B ViT-L\n(1) BEiT no yes rand pt 49.8 53.3\n(2) BEiT no yes rand zero 49.5 53.2\n(3) BEiT† yes no pt zero - 53.1\n(4) MAE yes no pt zero 50.1 53.3\n(5) MAE yes no pt no 49.9 53.0\nTable 4. Positional information ablation. In the BEiT code, the\nViT is modiﬁed to use relative position biases (rel) instead of abso-\nlute position embeddings (abs). We study how these components\nimpact results based on their use in pre-training (pt) and under var-\nious treatments in ﬁne-tuning: (i) pt: initialized with pre-trained\nvalues; (ii) rand: random initialization; (iii) zero: initialized at\nzero; and (iv) no: this positional information is not used in the\nﬁne-tuned model. For BEiT †(row 3), we pre-train an additional\nmodel (ViT-L only) that, like MAE, uses absolute position em-\nbeddings instead of relative position biases. Our default settings\nare marked in gray. Comparing (1) and (2), we observe that pre-\ntrained relative position bias initialization provides a slight beneﬁt\nover zero initialization. Comparing (1,2) to (3), we see that BEiT\npre-trained with absolute position embeddings performs similarly\n(perhaps slightly worse) to pre-training with relative position bi-\nases. Comparing (4) and (5), we see that including relative posi-\ntion biases in addition to absolute position embeddings provides a\nsmall improvement.\non ﬁne-tuning performance. A detailed analysis is given in\nthe caption. In summary, we observe that including rela-\ntive position biases during ﬁne-tuning may slightly improve\nAPbox by ∼0.2–0.3 points ( e.g., 53.0 to 53.3) for a model\nthat was pre-trained with only absolute position embed-\ndings. We also observe that pre-training relative position\nbiases, as done by BEiT, may also have a slight positive ef-\nfect of ∼0.1–0.3 points. Our practice of including both posi-\ntional information types during ﬁne-tuning appears to pro-\nvide a reasonably fair comparison. We also note that using\nrelative position biases introduces non-trivial overhead—\nit increases training and inference time by roughly 25%\nand 15% relative, respectively, increases memory by ∼15%\n(even with shared biases), and perhaps should be avoided.\nPre-training Epochs. In Figure 3 we study the impact of\nMAE pre-training epochs on COCO AP box by sweeping\npre-training epochs from 100 to 1600 (the default). The\nresults show that pre-training duration has a signiﬁcant im-\npact on transfer learning performance with large increases\nin APbox continuing from 100 to 800 epochs. There is still\na small improvement from 800 to 1600 epochs ( +0.2 from\n53.1 to 53.3), though the gradient has largely ﬂattened.\nTIDE Error Type Analysis. In Figure 4 we show the error\ntype analysis generated by the TIDE toolbox [3]. A detailed\ndescription and analysis is given in the caption. The anal-\nysis reveals more granular information about where MAE\nand BEiT improve overall AP relative to the other initial-\nizations. In summary, we observe that all initializations lead\n6\n100 200 400 800 1600\nPre-training epochs (log scale)\n50\n51\n52\n53APbox\n50.1\n51.5\n52.7\n53.1 53.3\nFigure 3. Impact of pre-training epochs. Increasing MAE pre-\ntraining from 100 to 800 epochs confers large transfer learning\ngains. The improvements start to plateau after 800 epochs.\ncls loc cls+loc dup bg miss0\n2\n4\n6\n8∆APbox@0.5\nInitialization\nMAE\nBEiT\nrandom\nMoCo v3\nsupervised IN1k\nFigure 4. TIDE analysis. We plot the ∆APbox metric at an\nintersection-over-union (IoU) threshold of 0.5 as deﬁned in [3].\nEach bar shows how much AP can be added to the detector if an\noracle ﬁxes a certain error type. The error types are: cls: localized\ncorrectly (IoU ≥0.5), but classiﬁed incorrectly; loc: classiﬁed cor-\nrectly, but localized incorrectly (IoU in[0.1, 0.5)); cls+loc: classi-\nﬁed incorrectly and localized incorrectly; dup: detection would be\ncorrect if not for a higher scoring correct detection; bg: detection\nis in the background (IoU<0.1); miss: all undetected ground-truth\nobjects not covered by other error types. (See [3] for more details\nand discussion.) We observe that the masking-based initializations\n(MAE and BEiT) make fewer localization errors than MoCo v3\nand supervised initialization (random initialization is somewhere\nin-between) and, even more so, have fewer missed detections. The\nother error types are more similar across initializations.\nto roughly the same classiﬁcation performance for correctly\nlocalized objects, however the MAE and BEiT initializa-\ntions improve localization compared to the other initializa-\ntions. We observe an even stronger effect when looking at\nmissed detections: the masking-based initializations yield\nnotably higher recall than the other initializations and thus\nleave fewer undetected objects. This higher recall creates a\nsmall increase in background errors, thus leading to better\noverall AP.\nModel Complexity. Table 5 compares various complex-\nity and wall-clock time measures of our speciﬁc Mask R-\nCNN conﬁguration. We also report these measures using a\nResNet-101 backbone instead of ViT. When trained from\nscratch, both ResNet-101 and ViT-B backbones achieve\n48.9 APbox. At inference time, the ResNet-101 backbone is\nmuch faster; however, during training ViT-B reaches peak\nperformance at 200 epochs compared to 400 for ResNet-\n101. ResNet-101 is not yet able to beneﬁt from BEiT or\nMAE pre-training and therefore lags behind ViT-B in APbox\n(∼1 point) when those methods are used for initialization.\nbackbone params (M) acts (M) ﬂops (G) fps\nResNet-101 65 426 ± 43 422 ± 35 13.7\nViT-B 116 1532 ± 11 853 ± 13 5.1\nViT-L 339 2727 ± 10 1907 ± 12 2.5\nTable 5. Model complexity for inference with the speciﬁc Mask\nR-CNN conﬁguration used in this report. For ViT, the image reso-\nlution is 1024 × 1024 (padded as necessary). The ﬂop and activa-\ntion counts are measured at runtime and vary based on the number\nof detected objects. We report the mean ± one standard deviation\nfrom 100 validation images. Results change very slightly when us-\ning different initializations. For reference, we report results using\nthe ResNet-101 backbone, which can (and does) use non-square\ninputs at inference time (longest side is 1024); otherwise infer-\nence settings are the same. The ResNet-101 based Mask R-CNN\nachieves 48.9 AP box when trained from scratch for 400 epochs.\nWe also report wall-clock speed in frames-per-second (fps) on an\nNVIDIA V100-32GB GPU.\nHyperparameter Tuning. All pre-trained initializations\npreferred wd = 0.1 for ﬁne-tuning. Random initialization\nbeneﬁtted from stronger regularization and selected a higher\nsetting of 0.2. Most methods selected lr = 8.0e−5, except\nfor random initialization and MoCo v3 initialization, which\nboth preferred a higher setting of1.6e−4. As described pre-\nviously, the drop path rate could not be reliably tuned using\nshorter schedules. As a result, we tuned dp with 50 epoch\ntraining for pre-trained initializations and 100 epoch train-\ning for random initialization. Based on this tuning, all ini-\ntializations selected dp = 0.1 when using ViT-B and 0.3\nwhen using ViT-L.\n5. Conclusion\nWe have presented techniques that enable the practi-\ncal use of standard ViT models as the backbone in Mask\nR-CNN. These methods yield acceptable training mem-\nory and time, while also achieving strong results on COCO\nwithout involving too many complex extensions. Us-\ning these techniques, we ﬁnd effective training formulae\nthat enable us to benchmark ﬁve different ViT initializa-\ntion methods. We show that random initialization takes\n∼4×longer than any of the pre-trained initializations, but\nachieves a meaningfully higher AP than ImageNet-1k su-\npervised pre-training. We ﬁnd that MoCo v3, a represen-\ntative of contrastive unsupervised learning, performs nearly\nthe same as supervised pre-training (and thus worse than\nrandom initialization). Importantly, we witness an exciting\nnew result: masking-based methods (BEiT and MAE) show\nconsiderable gains over both supervised and random ini-\ntialization and these gains increase as model size increases.\nThis scaling behavior is not observed with either supervised\nor MoCo v3-based initialization.\n7\nReferences\n[1] Hangbo Bao, Li Dong, and Furu Wei. BEiT: Bert pre-\ntraining of image transformers. arXiv:2106.08254, 2021.\n[2] Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew\nZhai, and Dmitry Kislyuk. Toward transformer-based object\ndetection. arXiv preprint arXiv:2012.09958, 2020.\n[3] Daniel Bolya, Sean Foley, James Hays, and Judy Hoffman.\nTIDE: A general toolbox for identifying object detection er-\nrors. In ECCV, 2020.\n[4] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delv-\ning into high quality object detection. In CVPR, 2018.\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In\nICCV, 2021.\n[6] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox-\niao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi,\nWanli Ouyang, et al. Hybrid task cascade for instance seg-\nmentation. In CVPR, 2019.\n[7] Xinlei Chen, Saining Xie, and Kaiming He. An empirical\nstudy of training self-supervised Vision Transformers. In\nICCV, 2021.\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR, 2009.\n[9] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nICCV, 2015.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021.\n[11] Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr\nBojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev,\nNatalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al.\nXCiT: Cross-covariance image transformers. arXiv preprint\narXiv:2106.09681, 2021.\n[12] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao\nLi, Zhicheng Yan, Jitendra Malik, and Christoph Feicht-\nenhofer. Multiscale vision transformers. arXiv preprint\narXiv:2104.11227, 2021.\n[13] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-\nYi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple\ncopy-paste is a strong data augmentation method for instance\nsegmentation. In CVPR, 2021.\n[14] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In CVPR, 2014.\n[15] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He. Accurate, large minibatch\nSGD: Training ImageNet in 1 hour.arXiv:1706.02677, 2017.\n[16] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. arXiv preprint arXiv:2111.06377, 2021.\n[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR, 2020.\n[18] Kaiming He, Ross Girshick, and Piotr Doll ´ar. Rethinking\nImageNet pre-training. In ICCV, 2019.\n[19] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017.\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016.\n[21] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv:1606.08415, 2016.\n[22] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. InECCV,\n2016.\n[23] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In ICML, 2015.\n[24] Gustav Larsson, Michael Maire, and Gregory\nShakhnarovich. Fractalnet: Ultra-deep neural networks\nwithout residuals. ICLR, 2016.\n[25] Yann LeCun, Bernhard Boser, John S Denker, Donnie\nHenderson, Richard E Howard, Wayne Hubbard, and\nLawrence D Jackel. Backpropagation applied to handwrit-\nten zip code recognition. Neural computation, 1989.\n[26] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Man-\ngalam, Bo Xiong, Jitendra Malik, and Christoph Feichten-\nhofer. Improved multiscale vision transformers for classiﬁ-\ncation and detection. In preparation, 2021.\n[27] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In CVPR, 2017.\n[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV. 2014.\n[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021.\n[30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019.\n[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learn-\ning with a uniﬁed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019.\n[32] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. arXiv:2102.12092,\n2021.\n[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with re-\ngion proposal networks. In NeurIPS, 2015.\n[34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with re-\ngion proposal networks. TPAMI, 2017.\n8\n[35] Pierre Sermanet, Koray Kavukcuoglu, Sandhya Chintala,\nand Yann LeCun. Pedestrian detection with unsupervised\nmulti-stage feature learning. In CVPR, 2013.\n[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. arXiv:2012.12877, 2020.\n[37] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv´e J´egou. Going deeper with im-\nage transformers. arXiv preprint arXiv:2103.17239, 2021.\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\n[39] Yuxin Wu and Kaiming He. Group normalization. In ECCV,\n2018.\n[40] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019.\n9",
  "topic": "Benchmarking",
  "concepts": [
    {
      "name": "Benchmarking",
      "score": 0.8597137331962585
    },
    {
      "name": "Initialization",
      "score": 0.7875449657440186
    },
    {
      "name": "Computer science",
      "score": 0.7770111560821533
    },
    {
      "name": "Machine learning",
      "score": 0.6663108468055725
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6525135040283203
    },
    {
      "name": "Transformer",
      "score": 0.5299649834632874
    },
    {
      "name": "Transfer of learning",
      "score": 0.5145377516746521
    },
    {
      "name": "Object detection",
      "score": 0.468863308429718
    },
    {
      "name": "Supervised learning",
      "score": 0.46541833877563477
    },
    {
      "name": "Unsupervised learning",
      "score": 0.4240637421607971
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34777480363845825
    },
    {
      "name": "Artificial neural network",
      "score": 0.2270238697528839
    },
    {
      "name": "Engineering",
      "score": 0.09837040305137634
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}