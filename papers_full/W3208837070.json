{
  "title": "Federated Split Vision Transformer for COVID-19 CXR Diagnosis using Task-Agnostic Training",
  "url": "https://openalex.org/W3208837070",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5057920247",
      "name": "Sang Joon Park",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019083298",
      "name": "Gwanghyun Kim",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5030497828",
      "name": "Jeongsol Kim",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5084255808",
      "name": "Boah Kim",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5012644755",
      "name": "Jong Chul Ye",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2914746235",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3014524604",
    "https://openalex.org/W3033814865",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W3136933888",
    "https://openalex.org/W3030810099",
    "https://openalex.org/W2961602927",
    "https://openalex.org/W2963209930",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2903470619",
    "https://openalex.org/W3141637546",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W3029443186",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2767079719",
    "https://openalex.org/W3091852954",
    "https://openalex.org/W1905829557",
    "https://openalex.org/W2535838896",
    "https://openalex.org/W2964132985",
    "https://openalex.org/W3103802018",
    "https://openalex.org/W2949377959",
    "https://openalex.org/W2963842982",
    "https://openalex.org/W3134475970",
    "https://openalex.org/W2902113386",
    "https://openalex.org/W2897230576",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3172312230",
    "https://openalex.org/W3120121071",
    "https://openalex.org/W3101156210",
    "https://openalex.org/W3153774375",
    "https://openalex.org/W2963877604",
    "https://openalex.org/W2900120080",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W2630997112",
    "https://openalex.org/W2143104527",
    "https://openalex.org/W2966182616",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W2963300197",
    "https://openalex.org/W2963430933",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W3098325931",
    "https://openalex.org/W2123737882",
    "https://openalex.org/W2795354529",
    "https://openalex.org/W3143017599",
    "https://openalex.org/W3112965401",
    "https://openalex.org/W1996182770",
    "https://openalex.org/W3046183292",
    "https://openalex.org/W2964162474",
    "https://openalex.org/W3013263293",
    "https://openalex.org/W3021476631",
    "https://openalex.org/W2910306536",
    "https://openalex.org/W2964285114",
    "https://openalex.org/W3045625006",
    "https://openalex.org/W2912213068",
    "https://openalex.org/W2792767783",
    "https://openalex.org/W3013056581",
    "https://openalex.org/W2767236661",
    "https://openalex.org/W3000479830",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W2963693643",
    "https://openalex.org/W3014795415",
    "https://openalex.org/W2886281300",
    "https://openalex.org/W2541884796"
  ],
  "abstract": "Federated learning, which shares the weights of the neural network across clients, is gaining attention in the healthcare sector as it enables training on a large corpus of decentralized data while maintaining data privacy. For example, this enables neural network training for COVID-19 diagnosis on chest X-ray (CXR) images without collecting patient CXR data across multiple hospitals. Unfortunately, the exchange of the weights quickly consumes the network bandwidth if highly expressive network architecture is employed. So-called split learning partially solves this problem by dividing a neural network into a client and a server part, so that the client part of the network takes up less extensive computation resources and bandwidth. However, it is not clear how to find the optimal split without sacrificing the overall network performance. To amalgamate these methods and thereby maximize their distinct strengths, here we show that the Vision Transformer, a recently developed deep learning architecture with straightforward decomposable configuration, is ideally suitable for split learning without sacrificing performance. Even under the non-independent and identically distributed data distribution which emulates a real collaboration between hospitals using CXR datasets from multiple sources, the proposed framework was able to attain performance comparable to data-centralized training. In addition, the proposed framework along with heterogeneous multi-task clients also improves individual task performances including the diagnosis of COVID-19, eliminating the need for sharing large weights with innumerable parameters. Our results affirm the suitability of Transformer for collaborative learning in medical imaging and pave the way forward for future real-world implementations.",
  "full_text": "Federated Split Vision Transformer for COVID-19\nCXR Diagnosis using Task-Agnostic Training\nSangjoon Park1∗ Gwanghyun Kim1∗\nJeongsol Kim1 Boah Kim1 Jong Chul Ye1,2,3\n1 Department of Bio and Brain Engineering\n2Kim Jaechul Graduate School of AI, 3Deptartment of Mathematical Sciences\nKorea Advanced Institute of Science and Technology (KAIST)\n{depecher, gwang.kim, wjdthf3927, boahkim, jong.ye}@kaist.ac.kr\nAbstract\nFederated learning, which shares the weights of the neural network across clients,\nis gaining attention in the healthcare sector as it enables training on a large corpus\nof decentralized data while maintaining data privacy. For example, this enables\nneural network training for COVID-19 diagnosis on chest X-ray (CXR) images\nwithout collecting patient CXR data across multiple hospitals. Unfortunately, the\nexchange of the weights quickly consumes the network bandwidth if highly ex-\npressive network architecture is employed. So-called split learning partially solves\nthis problem by dividing a neural network into a client and a server part, so that\nthe client part of the network takes up less extensive computation resources and\nbandwidth. However, it is not clear how to ﬁnd the optimal split without sacriﬁc-\ning the overall network performance. To amalgamate these methods and thereby\nmaximize their distinct strengths, here we show that the Vision Transformer, a\nrecently developed deep learning architecture with straightforward decomposable\nconﬁguration, is ideally suitable for split learning without sacriﬁcing performance.\nEven under the non-independent and identically distributed data distribution which\nemulates a real collaboration between hospitals using CXR datasets from multiple\nsources, the proposed framework was able to attain performance comparable to\ndata-centralized training. In addition, the proposed framework along with hetero-\ngeneous multi-task clients also improves individual task performances including\nthe diagnosis of COVID-19, eliminating the need for sharing large weights with\ninnumerable parameters. Our results afﬁrm the suitability of Transformer for\ncollaborative learning in medical imaging and pave the way forward for future\nreal-world implementations.\n1 Introduction\nAfter its earlier success in many ﬁelds, deep neural networks have found a pervasive suite of\napplications in healthcare research including medical imaging, becoming a new de facto standard\n[63, 24, 18, 58, 22, 72, 10, 67, 30]. Training these networks requires a vast amount of data to achieve\nrobust performance [12, 16, 56]. Despite the fact that multi-center collaboration is mandatory due\nto the shortage of labeled data in a single institution, collaboration in healthcare research is heavily\n∗Authors contributed equally.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2111.01338v2  [eess.IV]  3 Nov 2021\nimpeded by difﬁculties in data sharing stemming from the privacy issues and limited consent of\npatients [59, 46, 62].\nTo alleviate this problem, the distributed machine learning methods, devised to enable the computation\non multiple clients and servers leaving data to reside on the source devices, can be effectively\nleveraged for healthcare research [11, 51]. Federated learning (FL) is one of these methods which\nenables model training on a large corpus of decentralized data [33, 40, 68]. However, FL still holds\nseveral limitations in that it depends on clients’ computational resources for its client-side parallel\ncomputation strategy for update and is not free from privacy concerns [34, 39, 61]. In contrast to FL,\nanother distributed machine learning method, split learning (SL) offers better privacy and requires\nlower computational resources of clients by splitting the network between clients and the server\n[27, 61], but still possess problems that it shows signiﬁcant slower convergence than FL and can not\nlearn under non-independent and non-identically distributed (non-IID) data [25].\nEspecially under unprecedented pandemic of an emerging pathogen like COVID-19, under which\ndirect multi-national collaboration is deterred for prevention of epidemics, the collaboration via these\ndistributed machine learning approaches is becoming increasingly important, since these enable to\nbuild a model with performance tantamount to data-centralized learning without any direct sharing of\nraw data between institutions to offer privacy.\nRecently proposed Vision Transformer (ViT) architecture [ 19], inspired by astounding results of\nTransformer-based models on natural language processing (NLP), have demonstrated impeccable\nperformance on many vision tasks by enabling to model long dependencies within images. Besides\nthis strength, the straightforward design of the Transformer allows to easily decompose the entire\nnetwork into parts: the head for extracting features from the input image, the Transformer body to\nmodel the dependency between features, and the tail used for mapping features to task-speciﬁc output.\nOne of the important contributions in this paper is the observation that this conﬁguration is optimal\nfor SL where a network should be split into the parts for clients and servers. In addition, as suggested\nin [13], the Transformer body with sufﬁcient capacity can be shared between various tasks, being\nsuitable for multi-task learning (MTL) to leverage robust representation from multiple related tasks\nto enhance the generalization performance of individual tasks.\nAccordingly, here we propose a novel Federated Split Task-Agnostic (FESTA) framework equipped\nwith a Transformer to simultaneously process multiple chest X-ray (CXR) tasks including diagnosis of\nCOVID-19, emulating a real collaboration between several hospitals. To validate the practicability of\nFESTA, we also implemented the framework using a friendly federated learning framework (Flower)\nprotocol [5], conﬁrming seamless integration of various components. Experimental results show that\nour framework can show stable performance even under non-IID settings which is a frequently faced\nsituation for collaboration between hospitals while offering privacy, by amalgamating FL and SL to\nmaximally exploit their main advantages. In addition, we show that the proposedFESTA Transformer\nalong with MTL improves the performances of individual tasks. In summary, our contributions are\ntwo folds:\n• We proposed a novel FESTA learning framework equipped with the ViT by utilizing its\ndecomposable design to amalgamate the merit of FL and SL.\n• We showed that the model trained with the FESTA framework can leverage the robust\nrepresentations from multiple related tasks to improve the performance of the individual\ntask.\n2 Related Work\nDistributed machine learning. FL is a distributed machine learning approach originally proposed\nby Google [33] to enable the training of a model via distributed devices and data. It eliminates the\nneed to aggregate the raw data in a centralized way by enabling the model to be updated on the edge\ndevices (e.g. mobile phones, computers in hospitals). Speciﬁcally, during the training, the server\ninitializes a global model and sends it to each client. The clients then train the model with their local\ndata in parallel and return the updated model to the server. Then, the server aggregates and distributes\nthose models by a method such as federated averaging ( FedAvg) [40] to update the global model.\nThis process (called round) continues repeatedly until the model converges. Though FL enables\n2\ndecentralized training in a privacy-preserving manner, it still holds limitations as it largely depends\non the computation resources of clients and is vulnerable to model inversion attacks [65].\nDifferent from FL, SL divides the neural network into several sub-networks, and these separated\nsub-networks are trained under distributed setting [ 21]. In detail, the ﬁrst sub-network is trained\non the client-side with local data and then passes the feature to the second sub-network located in\nthe server. The server can access the only feature from the ﬁrst sub-network, and train the second\nsub-network to send the subsequent feature to the third sub-network on the client. Finally, the third\nsub-network is able to provide the output of the overall network. By inserting black-box sub-networks\nin both client and server sides, it is possible to offer better privacy than FL. Besides the privacy\nbeneﬁt, SL uses less computational resources than FL at the client-side. Nevertheless, since the one\ncycle of forward and backward passes is ﬁnished after data and gradients move back and forth across\nthe sub-networks distributed on multiple sides, the convergence of SL is considerably slower than FL.\nIn addition, it was reported that the convergence is not reached at all under the non-IID setting [25].\nMulti-task learning. MTL is a learning strategy to improve the generalization performance of a\nspeciﬁc task with the help of information from other related tasks. In MTL, models for multiple\nrelated tasks are trained simultaneously [9, 21]. In its early era, the motivation of MTL is to mitigate\nthe data insufﬁciency problem where the number of labeled data is limited for each task to train an\naccurate learner. MTL helps to reuse knowledge and thereby reduce the requirement for labeled data\nfor each task, exhibiting that the MTL model can achieve better performance than the single-task\ncounterpart in many ﬁelds ranging from computer vision [ 20, 42, 44, 36] to NLP [38, 26, 37, 50].\nWith increased data, the MTL model can learn more robust representations via knowledge sharing\namong multiple tasks, resulting in improved performance and less overﬁtting for the individual task.\nVision Transformer. Transformer [60], which was originally developed for NLP, is a deep neural\nnetwork based on an attention mechanism that utilizes an appreciably large receptive ﬁeld. After\nachieving state-of-the-art (SOTA) performance in NLP, it has also inspired the vision community\nto explore its application on vision tasks to utilize its ability to model long-range dependency\nwithin an image [31]. The ViT was one of the successful attempts to apply Transformer directly to\nimages, achieving excellent results compared to the SOTA convolutional neural networks in image\nclassiﬁcation tasks [19]. Furthermore, in addition to the superb performance, the straightforward\nmodular design of ViT facilitates broad applications in many tasks only with minimal change. Chen\net al. [13] proposed image processing transformer, which is one of the successful multi-task model\nfor various computer vision tasks, by splitting ViT into shared body and task-speciﬁc heads and\ntails, suggesting that Transformer body with sufﬁcient capacity can be shared across relevant tasks.\nHowever, they leveraged encoder-decoder design and the usefulness of the multi-task ViT model was\nnot evaluated along with the distributed learning methods.\nRecently, ViT was successfully used for diagnosis and severity prediction of COVID-19, showing\nthe SOTA performance [ 43]. Speciﬁcally, to alleviate the overﬁtting problem with limited data\navailable, the overall framework is decomposed into two steps: the pre-trained backbone network to\nclassify common low-level CXR features, which was leveraged in the second step by Transformer\nfor high-level diagnosis and severity prediction of COVID-19. By maximally utilizing the merit of\nthe large-scale database containing more than 220,000 CXR images, the model has attained stable\ngeneralization performance as well as SOTA performance in a variety of external test data from\ndifferent institutions, even with the limited number of labeled cases for COVID-19.\n3 Split Task-Agnostic Transformer for CXR COVID-19 Diagnosis\nInspired by these works, here we are interested in utilizing ViT for distributed learning in COVID-\n19 CXR diagnosis, where the collaboration via these distributed machine learning approaches is\nbecoming increasingly important by offering privacy and still allowing similar performance to the\ndata-centralized learning.\nThe reason we are interested in ViT architecture for this purpose is that the natural conﬁguration\nof ViT may be optimal for MTL as well as SL where the easily decomposable modular design of\nthe network is preferred, suggesting a possibility to maximally reconcile the merits of MTL and SL\nthrough ViT architecture. Speciﬁcally, the clients just train the head and tail parts of the network,\nwhereas the Transformer body is shared across multiple clients. Then, the embedded features from\n3\nFigure 1: Overall framework of the proposed method. (a) Split task-agnostic CXR Transformer for\nmulti-task learning and (b) the concept of FESTA learning process.\nthe head network from multiple clients can be leveraged in the second step by Transformer to process\nindividual tasks including diagnosis of COVID-19. Furthermore, by maximally utilizing the merit of\nthe large-scale database from CXR for various tasks, our goal is to demonstrate stable generalization\nperformance as well as SOTA performance in the external test dataset.\n3.1 F ESTA: Federated Split Task-Agnostic Learning\nThe concept of the proposed Federated Split Task-Agnostic (FESTA) learning process is illustrated\nin Fig 1(a) and (b). Let C= ⋃K\nk=1 Ck be a group of client sets with different CXR tasks, where K\ndenotes the number of tasks and Ck has one or more clients with different datasets each other for\nthe k-th task, i.e. Ck = {ck\n1 ,ck\n2 ,...,c k\nNk : Nk ≥1}. Each client c∈Ck has its own task-speciﬁc\nnetwork architecture for a head Hc and a tail Tc, which are connected to Transformer Bin the server.\nIn our FeSTA framework, the server ﬁrst initializes the weights of the Transformer body and task-\nspeciﬁc heads, tails for each task k. Then, it distributes the initialized weights of heads and tails\nto each client c ∈Ck. For round i = 1,2,...R , each client (e.g. hospital) perform the forward\npropagation on their task-speciﬁc head and pass their intermediate feature to the server. Speciﬁcally,\nusing the local training data {(x(i)\nc ,y(i)\nc )}Nc\ni=1, the head network Hc encodes the smashed feature\nmaps h(i)\nc and sent it to the server: h(i)\nc = Hc(x(i)\nc ). Then, the server-side Transformer body B\nreceives the feature from the all clients and generate the features b(i)\nc for cin parallel: b(i)\nc = B(h(i)\nc ).\nResulting smashed features from the Transformer are allocated to task-speciﬁc tail to produce the\nﬁnal prediction ˆy(i)\nc = Tc(b(i)\nc ) according to each task, and the forward path ﬁnishes. Subsequently,\nthe loss for each client can be calculated as ℓc(y(i)\nc ,Tc(B(Hc(x(i)\nc )))) where ℓc(y,ˆy) refers to the c\nclient- speciﬁc loss between the target yand the estimate ˆy. By minimizing the loss with respect\nto the tail weight, the gradients of the local tails are passed reversely to the server. After receiving\nthe gradients, the server performs the back-propagation on the server-side body model, sends the\ngradients back to the clients.\nSpeciﬁcally, for the task-agnostic body update, the following optimization problem is solved:\nmin\nB\n∑\nc∈C\nNc∑\ni=1\nℓc(y(i)\nc ,Tc(B(Hc(x(i)\nc )))), (1)\nFor the task-speciﬁc ﬁne-tuning, the following optimization problem is solved:\nmin\nHc,Tc\nNc∑\ni=1\nℓc(y(i)\nc ,Tc(B(Hc(x(i)\nc )))), (2)\n4\nFinally, the server aggregates and averages the weights of local heads and tails from the clients to\nupdate the global heads and tails among the clients for the same tasks via FedAvg, and distributes\nback the updated global weights of heads and tails for each task kto the clients. The algorithm is\nformally presented in Algorithm 1.\nAlgorithm 1: FESTA: Federated Split Task-Agnostic learning\n1 Function ServerMain:\n2 Initialize the body weight w(1)\nB and client head/tail weights ( ¯wH,k, ¯wT,k) for each task\nk∈{1,...,K }in server\n3 for rounds i= 1,2,...R do\n4 for tasks k∈{1,2,...K }do in parallel\n5 for clients c∈Ck do in parallel\n6 if i= 1 or (i−1) ∈UnifyingRounds then\n7 Set client (w(i)\nHc ,w(i)\nTc ) ←( ¯wH,k, ¯wT,k)\n8 h(i)\nc ←ClientHead(c)\n9 b(i)\nc ←B(h(i)\nc )\n10 ∂L(i)\nc\n∂b(i)\nc\n←ClientTail(c,b(i)\nc ) & Backprop.\n11 (w(i+1)\nHc ,w(i+1)\nTc ) ←ClientUpdate(c,∂L(i)\nc\n∂h(i)\nc\n)\n12 Update body w(i+1)\nB ←w(i)\nB −η\nK\nK∑\nk=1\n∑\nc∈Ck\n∂L(i)\nc\nNk∂w(i)\nB\n13 if i∈UnifyingRounds then\n14 for tasks k∈{1,2,...K }do\n15 Update ( ¯wH,k, ¯wT,k) ←( 1\nNk\n∑\nc∈Ck\nw(i+1)\nHc , 1\nNk\n∑\nc∈Ck\nw(i+1)\nTc )\n16 Function ClientHead(c):\n17 xc ←Current batch of input from client c\n18 return Hc(xc)\n19 Function ClientTail(c,bc):\n20 yc ←Current batch of label from client c\n21 Lc ←ℓc(yc,Tc(bc)) & Backprop.\n22 return ∂Lc\n∂bc\n23 Function ClientUpdate(c,∂Lc\n∂hc\n):\n24 Backprop. & (wHc , wTc ) ←(wHc −η ∂Lc\n∂wHc\n, wTc −η ∂Lc\n∂wTc\n)\n25 return (wHc ,wTc )\n3.2 Multi-task CXR learning\nFor synergistic performance improvement, we explore the following three tasks that are commonly\nused for CXR: classiﬁcation, segmentation, and object detection. These three tasks were separately\ntrained for the individual model, while used simultaneously to train and to evaluate the task-agnostic\nmodel for multiple related tasks. The details of each task and dataset are as follows.\nCOVID-19 classiﬁcation. This is the main task we want to achieve through FESTA. Since we\ninitialized the weights of classiﬁcation heads to be the robust feature extractor trained on pre-\nbuilt large data corpus containing common CXR ﬁndings, the classiﬁcation heads were initialized\nwith the pre-trained weights from CheXpert [29] dataset containing 10 CXR ﬁndings (no ﬁnding,\ncardiomegaly, opacity, edema, consolidation, pneumonia, atelectasis, pneumothorax, pleural effusion,\nsupport device) labeled by experts. From pre-training on the CheXpert dataset, we excluded 32,387\nlateral view images, and 29,420 posterior-anterior (PA) and 161,427 anterior-posterior (AP) view\ndata were ﬁnally used. Table 1 summarizes dataset resources and partitioning for the classiﬁcation\ntask. We used both public datasets containing labels of infectious disease (Valencian Region Medical\n5\nTable 1: Datasets and sources for COVID-19 diagnosis\nTotal CXR images External\nTraining and validation dataset\nClient 1 Client 2 Client 3 Client 4 Client 5 Client 6\nHospital 1 Hospital 2 Hospital 3 Hospital 4 NIH Brixia BIMCV\nNormal 13,649 320 300 400 8,861 3,768 - -\nOther infection 1,468 39 144 308 977 - - -\nCOVID-19 2,431 6 8 80 - - 1,929 408\nTotal CXR 17,548 365 452 788 9,838 3,768 1,929 408\nImage Bank [BIMCV] [ 17], Brixia [ 53, 7], National Institutes of Health [NIH] [ 64]), and CXR\ndata deliberately collected from four hospitals labeled by board-certiﬁed radiologists. We put one\nhospital data aside as an external test dataset to evaluate the classiﬁcation performance for real-world\napplications. Overall, 17,183 PA view CXR images were used for training/validation and 365 PA\nview CXR images for the test. In addition, we performed the experiments in the view-agnostic\nsetting by adding AP view CXRs, to further extend real-world applicability as provided in Appendix\nC.1. By adding AP view data, the total amounts of CXRs were increased from 17,183 to 24,180\nfor training/validation and 365 to 556 for external test datasets. Notably, the number of COVID-19\ncases was increased from six to 81 in the external test dataset. We modeled data from different\nsources as individual clients, emulating the real-world collaboration between hospitals. This setting\nis important to validate our method in non-IID data distribution. The study was ethically approved\nby the Institutional Review Board at each participating hospital and the requirement for informed\nconsent was waived.\nSegmentation. For the segmentation task, we have used the Society for Imaging Informatics in\nMedicine and the American College of Radiology Pneumothorax Segmentation Challenge dataset\n[54] consisting of 12,047 CXR images for training, 3,205 images for testing. The training dataset was\ndivided randomly with a 4:1 ratio into training and validation datasets, and we developed and ﬁne-\ntuned the model for pneumothorax segmentation with these datasets. Afterward, the segmentation\nperformance of the model was evaluated in testing datasets. Since the data for segmentation was\nanonymized, it was not possible to divide the dataset according to the sources of acquisition. Instead,\nwe randomly divided the entire dataset into the two clients, emulating the collaboration between two\nhospitals.\nObject detection. For the object detection task, the model was constructed to detect lung opacities\nin CXR with the Radiological Society of North America (RSNA) Pneumonia Detection Challenge\ndataset [49] that consists of CXR images and labels with bounding boxes and detailed class infor-\nmation of 26,684 subjects. We randomly divided the entire dataset with a 3:1 ratio into training and\ntesting datasets, with which model was trained and evaluated. Similar to the segmentation task, as the\ndata contains no information about the sources of acquisition, the dataset was randomly divided into\ntwo clients to simulate the collaboration between hospitals.\n4 Experimental Results\n4.1 Implementation details\nFor the head and tail parts of our model, the networks specialized for each task were utilized.\nFor classiﬁcation, we used the modiﬁed version of the network proposed by Ye et al. [69], which\ncomprises DenseNet combined with Probabilistic Class Activation Map (PCAM) operations for the\nclassiﬁcation task, since it achieved outstanding performance in the CXR classiﬁcation competition.\nFor segmentation, we adopt TransUNet [14] tthat inserts a Transformer between the convolutional\nneural network (CNN) encoder and decoder of UNet [ 48] to take advantage of both architectures.\nSimilarly, the 2nd place solution in the RSNA pneumonia detection challenge, which is a modiﬁed\nversion of RetinaNet for object detection, was utilized. After splitting these models into head and tail,\nthe feature maps between head and tail were mapped to the feature maps with the same dimension\nof 16 ×16 ×768, and used as input of Transformer body. The Transformer body, equipped with\n12 layers of standard Transformer encoder with 12 attention heads, transforms the feature maps to\n6\nFigure 2: Implementation details of the proposed method. (a) Detailed experimental setting for\nmulti-task learning (MTL) with three different tasks composed of 10 clients (six for classiﬁcation,\ntwo for segmentation, two for detection. Training scheme for (b) single-task learning, and (c) MTL.\nembed better representation. Then, the resulting feature maps from the body were passed to tails to\nyield the outcomes.\nAs suggested in Park et al. [43], the head for classiﬁcation was ﬁrst initialized with pre-trained\nweights from the CheXpert dataset. We minimized the cross-entropy loss for the classiﬁcation task.\nFor the segmentation model, we minimized the binary cross-entropy loss combined with dice and\nfocal loss. Finally, for the detection task, we minimized the sum of box classiﬁcation, box regression,\nand image classiﬁcation losses as suggested by Gabruseva et al. [23]. For all tasks, the batch size was\n2 per client, and the warm-up step was 500. We set the number of total rounds to 12,000, and the\nweights of each clients’ head and tail underwentFedAvg per 100 rounds by the server.\nFig. 2 illustrates the implementation details and the experimental settings of the proposed method.\nTo simulate single-task learning (STL) in Fig. 2(b), we considered each six data sources as different\nclients for the classiﬁcation task. On the other hand, for MTL shown in Fig. 2(c), 10 clients were\nsimultaneously used, with six clients for classiﬁcation and two clients for segmentation and detection\ntasks, respectively. For the segmentation and detection tasks, the training data set was randomly\nsplit into two subsets, whereas non-IID distribution was used for the classiﬁcation task. To adjust\nthe loss scale, the customized weights of 1:2:2 were applied for classiﬁcation, segmentation, and\ndetection tasks to update the common body weights. We divided the MTL into two steps, jointly\ntraining the task-speciﬁc heads, tails, and the task-agnostic body (6,000 rounds), and ﬁne-tuning only\nthe task-speciﬁc heads, tails with the body weights ﬁxed (6,000 rounds). By ﬁxing the parameters of\nthe shared Transformer body during the second step, the best models can be selected for different\ntasks according to the performance evaluation metrics of each task, even at the different rounds.\nThe FESTA, FL, and SL simulation was performed on the modiﬁed version of Flower (licensed\nunder an Apache-2.0 license) [ 5] FL framework. All experiments were performed with Python\nversion 3.8 and Pytorch version 1.7 on Nvidia RTX 3090, 2080 Ti, and 1080 Ti. For more details of\nimplementation, refer to Appendix A.\nPerformance metrics. We used the area under the receiver operating characteristic curve (AUC)\nto evaluate the diagnostic performance in the classiﬁcation task. To evaluate the accuracy of segmen-\ntation, the Dice coefﬁcient was used to quantitatively measure the overlap between the segmentation\nresults by model and the ground truths. The detection results were evaluated by calculating the mean\n7\naverage precision (mAP) at the different intersection over union, with a threshold range from 0.4 to\n0.75 with a step size of 0.05 as suggested in RSNA[49]. All experiments have been run and evaluated\nwith three different random seeds for the weight initialization to prevent the chance of confusing the\nresults.\n4.2 Results\nFESTA vs. Other strategies. We compared the performance of classiﬁcation model trained\nwith FESTA with a data-centralized setting and other distributed learning strategies for COVID-\n19 classiﬁcation task under the non-IID setting. To simulate data-centralized training, the whole\nnetwork with connected head, body, and tail was trained with the integrated dataset of all six sources.\nTo simulate FL, the whole network was aggregated and distributed by the server with FedAvg as\nsuggested by McMahan et al. [40]. On the other hand, for SL, the split sub-networks reside in the\nclients and server-side, and the client-side sub-networks were not aggregated as in [61]. The same\nexperimental settings and hyperparameters were applied for a fair comparison. For more details of\nthe data-centralized learning and other distributed learning methods, refer to Appendix B.\nAs shown in Table 2, our method achieved comparable performance to the data-centralized learning\nmethod as well as outperformed the existing distributed learning methods, suggesting the superiority\nof our method over other methods. Of note, the performance could be further enhanced with MTL,\nwhich is a distinct strength of the proposed method.\nIn our additional experiments after adding AP view data, the model showed even better performance\nwith the increased number of cases (Appendix C.1). This remarkable view-agnostic behavior of the\nmodel incentivizes the real-world application of computer-aided diagnosis of COVID-19, as the recent\nreview on artiﬁcial intelligence models for COVID-19 diagnosis claims that the AI model, which has\nbeen pouring out a lot recently, are not helpful at all from the viewpoint of clinical application [66], in\nwhich the performances were substantially unstable by the factors like the view of CXR images [45].\nTable 2: Comparison for the performance of the proposed method with other strategies\nStrategy\nAUC\nAverage COVID-19 Others Normal\nData-centralized 0.911 ±0.016 0.883±0.036 0.927 ±0.013 0.923 ±0.004\nFederated learning 0.891 ±0.019 0.840±0.035 0.926 ±0.018 0.906 ±0.028\nSplit learning 0.863 ±0.005 0.807±0.012 0.892 ±0.007 0.889 ±0.019\nFESTA (single-task learning) 0.909±0.021 0.880±0.008 0.916 ±0.038 0.931±0.021\nFESTA (multi-task learning) 0.931±0.004 0.926±0.023 0.929 ±0.016 0.938 ±0.013\nNote: Experiments were performed repeatedly with three random seeds to report mean and standard deviation.\nFor evaluation of split learning, the average metric between clients are calculated.\nMulti-task learning vs. Single-task learning. We evaluate whether the task-agnostic Transformer\nbody contributes to improving the performance of the entire model by leveraging better representation\nfrom the MTL with several related tasks, namely classiﬁcation, segmentation, and object detection\nin this work. As provided in Table 3, the models trained with the MTL approach showed at least\ncomparable or even better performance compared with STL counterparts without the need to create\nan individual body model for each task or sharing a large body model between clients, which suggest\nthe distinct merit of MTL approach with our framework.\nIn addition to the STL models, the MTL models trained with our framework outperformed task-\nspeciﬁc expert models and provided comparable or even better performances to Kaggle’s winning\nsolution for the same tasks. Of note, when we substitute the shared layers of the Transformer body\nwith CNN architecture with similar complexity, the performance gain was no longer maintained,\nsuggesting the suitability of Transformer architecture in learning shared representation between the\nrelated tasks as provided in the additional experiments in Appendix C.2.\nModel sizes and communicative beneﬁt. Table 4 shows the numbers of parameters and the sizes\nof sub-networks. The task-agnostic body is the largest among sub-networks in terms of both the\nnumber of parameters and model size. This suggests that the largest part of the model does not need\nto be aggregated and distributed between client-client and client-server, which offers substantial\n8\nTable 3: Comparison of the performances between single-task and multi-task learning\nTasks Metrics Single-task learning Multi-task learning\nClassiﬁcation AUC 0.909 ±0.021 0.931±0.004\nSegmentation Dice 0.798 ±0.016 0.821±0.003\nDetection mAP 0.202 ±0.008 0.204±0.002\nNote: Experiments were performed repeatedly with three random seeds to report mean and standard deviation.\ncommunicative beneﬁt. In all tasks, the size of the body is more than half of the entire network.\nTherefore, not having to share this huge part means the total training time can be reduced considerably.\nIn our experiments, the training time with the proposed method was four times shorter than the FL\napproach in which the entire network is shared and distributed. In addition, the proposed method\noffers another beneﬁt of saving the computational resources of the server by processing multiple\ntasks with a single body model to enable the server to efﬁciently handle the requests for various tasks\nfrom many clients simultaneously.\nNonetheless, there remains a concern of communicative heavy load between the server and clients\ncaused by the frequent transmission of features and gradients. Our further analysis of communication\ncosts including features and gradients transmission are provided in the additional experiments, which\nsuggests that the communication costs of the proposed FESTA framework was substantially lower\nthan those of FL, although higher than SL. For detailed results of the computation for communication\ncosts, refer to Appendix C.3.\nTable 4: Parameter numbers and model sizes of the sub-networks\nTask\nHead Body Tail\nParameters Size Parameters Size Parameters Size\nClassiﬁcation 13.313 M 54.1 MB\n66.367 M 265.5 MB\n0.002 M 11.7 KB\nSegmentation 15.041 M 60.2 MB 7.387 M 29.6 MB\nDetection 27.085 M 108.8 MB 19.773 M 79.1 MB\nNote: Model sizes were estimated by parameter numbers and ﬁle sizes of saved weights.\n4.3 Ablation study\nRole of Transformer body. As provided in Table 5, we ﬁrst performed the ablation study to\nverify the contribution of the Transformer body on the server. The model without the server-side\nTransformer body, which is identical to the DenseNet-121 model equipped with PCAM operation\n[69], was built and trained with the same setting, and compared with the proposed model containing\nthe Transformer body. The AUC values were higher with the Transformer body either for STL or\nMTL compared to those without the Transformer body with the statistical signiﬁcance, implying the\nadvantageous role of the server-side Transformer body as in Appendix C.4.\nNumbers of round for averaging. Determining the optimal number of rounds for averaging is\nimportant. If the aggregation, averaging, and distribution, namelyFESTA is performed too frequently,\nthe cost and required resources for communication increases, which can interfere or even preclude\nthe learning process. On the contrary, if averaged too rarely, naive averaging of the learned model\nparameters of local learners can be devastating, resulting in nonsensical parameters in some layers\n[70]. Therefore, we performed the ablation to determine the optimal number of rounds to conduct\nFESTA. As shown in Table 5, the averaging per 100 rounds showed the comparable or better\nperformance to less or more frequent counterparts.\nMore results of ablation studies are provided in Appendix D.\n5 Conclusions\nIn this paper, we proposed a novel Federated Split Task-Agnostic (FESTA) framework suitable to\nleverage the formidable beneﬁt of ViT to simultaneously process multiple CXR tasks including the\n9\nTable 5: Role of Transformer body and round number for FESTA\nMethod\nAUC\nAverage COVID-19 Others Normal\nRole of Transformer body\nw/o Transformer body 0.889±0.015 0.874±0.056 0.895±0.011 0.898±0.009∗\nw Transformer body 0.909±0.021 0.880 ±0.008 0.916 ±0.038 0.931 ±0.021∗\nNumber of rounds for FESTA\nper 10 rounds 0.822 ±0.023 0.724 ±0.053 0.884 ±0.017 0.858 ±0.035\nper 100 rounds 0.909 ±0.021 0.880±0.008 0.916±0.038 0.931±0.021\nper 1000 rounds 0.903 ±0.012 0.905±0.019 0.866±0.034 0.939±0.005\nNote: Experiments were performed repeatedly with three random seeds to report mean and standard deviation.\nNote: ∗ denotes statistically signiﬁcant difference.\ndiagnosis of COVID-19. With the optimal conﬁguration of ViT for modulation, it was possible to\nsurpass the existing methods for distributed learning and achieve the performance comparable to\ndata-centralized learning with our framework, even under the skewed data distribution. Moreover,\nour framework alongside clients to process multiple related tasks also improves the performances of\nindividual tasks, while eliminating the need to share the data and large weights of the body network.\nThese results suggest the suitability of the Transformer for collaborative learning in medical imaging\nand pave the way forward for future real-world applications.\n6 Limitation and Potential Negative Societal Impacts\nThis work is more like a proof-of-the-concept study, rather than a ready-to-use solution for the\nindustry. Therefore, it holds some limitations which may occur in a real-world application. Recent\nworks on privacy attacks in the FL setting have implied that the belief that \"Privacy can be protected\nby the decentralized nature of the FL\" is not true [4, 65, 71]. In this work, however, the experimental\nresults regarding the privacy issue such as threatening privacy via inversion attack are not suggested.\nSecondly, although the unique challenges of FL have been suggested by previous work including\nthe problems of signiﬁcant bottleneck in communication, stragglers, and fault tolerance which is\nexacerbated than in typical data-centralized learning [55, 34], we have not conducted the experiments\nto evaluate the robustness of our method against these unique challenges.\nAlthough distributed learning-enabled learning without sharing data, decentralization is not a panacea\nagainst the privacy problem. Similar to the previously distributed learning methods, a potential\nrisk arises from these limitations that our algorithm may not be free from privacy issues via model\ninversion attack against the server, since the server retains the parameters of the entire network in\nprocess of FedAvg of the heads and tails despite the split design of the sub-networks upon both client\nand server-sides. Although the risk could be mitigated to some degree with the speciﬁc settings (e.g.\nsmall gradient due to pre-trained backbone, deeper network, more pooling layer, a mixture of multiple\ntasks), the privacy problem should not be ignored since we aim to use this method in collaboration\nwith hospitals where the patient privacy is a matter of the highest priority. Therefore, the methods\nto enhance the security such as differential privacy [41], secure multi-party computation [6], data\ncompression [73] and authenticated encryption [47] as well as the recent method to prevent gradient\ninversion attack without sacriﬁcing FL performance [57] should be utilized along with the proposed\nmethod to further reduce the risk of privacy leakage in a real-world application.\nAcknowledgements and Disclosure of Funding\nThis research was funded by the National Research Foundation (NRF) of Korea grant NRF-\n2020R1A2B5B03001980. This work was also supported by Institute of Information and communi-\ncations Technology Planning and Evaluation (IITP) grant funded by the Korea government(MSIT)\n(No.2019-0-00075, Artiﬁcial Intelligence Graduate School Program(KAIST)), and by the KAIST\nKey Research Institute (Interdisciplinary Research Group) Project.\n10\nReferences\n[1] Openmined/pysyft: A library for answering questions using data you cannot see. https:\n//github.com/OpenMined/PySyft. (Accessed on 06/03/2021).\n[2] Tensorﬂow federated. https://www.tensorflow.org/federated. (Accessed on\n06/03/2021).\n[3] grpc. https://grpc.io/. (Accessed on 06/03/2021).\n[4] Y . Aono, T. Hayashi, L. Wang, S. Moriai, et al. Privacy-preserving deep learning: Revisited and\nenhanced. In International Conference on Applications and Techniques in Information Security,\npages 100–110. Springer, 2017.\n[5] D. J. Beutel, T. Topal, A. Mathur, X. Qiu, T. Parcollet, P. P. de Gusmão, and N. D. Lane. Flower:\nA friendly federated learning research framework. arXiv preprint arXiv:2007.14390, 2020.\n[6] K. Bonawitz, V . Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage,\nA. Segal, and K. Seth. Practical secure aggregation for privacy-preserving machine learning. In\nproceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,\npages 1175–1191, 2017.\n[7] A. Borghesi and R. Maroldi. Covid-19 outbreak in italy: experimental chest x-ray scoring\nsystem for quantifying and monitoring disease progression. La radiologia medica, 125:509–513,\n2020.\n[8] S. Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Koneˇcn`y, H. B. McMahan, V . Smith, and A. Talwalkar.\nLeaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018.\n[9] R. Caruana. Multitask learning. In S. Thrun and L. Pratt, editors, Learning to learn, pages\n95–133. Springer, Boston, 1998.\n[10] K. K. Chandriah and R. V . Naraganahalli. Maximizing a deep submodular function optimization\nwith a weighted max-sat problem for trajectory clustering and motion segmentation. Applied\nIntelligence, pages 1–20, 2021.\n[11] K. Chang, N. Balachandar, C. Lam, D. Yi, J. Brown, A. Beers, B. Rosen, D. L. Rubin, and\nJ. Kalpathy-Cramer. Distributed deep learning networks among institutions for medical imaging.\nJournal of the American Medical Informatics Association, 25(8):945–954, 2018.\n[12] G. Chartrand, P. M. Cheng, E. V orontsov, M. Drozdzal, S. Turcotte, C. J. Pal, S. Kadoury, and\nA. Tang. Deep learning: a primer for radiologists. Radiographics, 37(7):2113–2131, 2017.\n[13] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and W. Gao. Pre-trained\nimage processing transformer. arXiv preprint arXiv:2012.00364, 2020.\n[14] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, and Y . Zhou. Tran-\nsunet: Transformers make strong encoders for medical image segmentation. arXiv preprint\narXiv:2102.04306, 2021.\n[15] L. Corinzia, A. Beuret, and J. M. Buhmann. Variational federated multi-task learning. arXiv\npreprint arXiv:1906.06268, 2019.\n[16] J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell,\nH. Askham, X. Glorot, B. O’Donoghue, D. Visentin, et al. Clinically applicable deep learning\nfor diagnosis and referral in retinal disease. Nature medicine, 24(9):1342–1350, 2018.\n[17] M. De La Iglesia Vayá, J. M. Saborit, J. A. Montell, A. Pertusa, A. Bustos, M. Cazorla, J. Galant,\nX. Barber, D. Orozco-Beltrán, F. García-García, et al. Bimcv COVID-19+: a large annotated\ndataset of rx and ct images from COVID-19 patients. arXiv preprint arXiv:2006.01174, 2020.\n[18] B. E. Dewey, C. Zhao, J. C. Reinhold, A. Carass, K. C. Fitzgerald, E. S. Sotirchos, S. Saidha,\nJ. Oh, D. L. Pham, P. A. Calabresi, et al. Deepharmony: a deep learning approach to contrast\nharmonization across scanner changes. Magnetic resonance imaging, 64:160–170, 2019.\n11\n[19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[20] D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common\nmulti-scale convolutional architecture. In Proceedings of the IEEE international conference on\ncomputer vision, pages 2650–2658, 2015.\n[21] T. Evgeniou and M. Pontil. Regularized multi–task learning. In Proceedings of the tenth ACM\nSIGKDD international conference on Knowledge discovery and data mining, pages 109–117,\n2004.\n[22] Y . Fu, Y . Lei, T. Wang, W. J. Curran, T. Liu, and X. Yang. Deep learning in medical image\nregistration: a review. Physics in Medicine & Biology, 65(20):20TR01, 2020.\n[23] T. Gabruseva, D. Poplavskiy, and A. Kalinin. Deep learning for automatic pneumonia detection.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops, pages 350–351, 2020.\n[24] M. Gaillochet, K. C. Tezcan, and E. Konukoglu. Joint reconstruction and bias ﬁeld correction\nfor undersampled mr imaging. In International Conference on Medical Image Computing and\nComputer-Assisted Intervention, pages 44–52. Springer, 2020.\n[25] Y . Gao, M. Kim, S. Abuadbba, Y . Kim, C. Thapa, K. Kim, S. A. Camtepe, H. Kim, and S. Nepal.\nEnd-to-end evaluation of federated learning and split learning for internet of things. arXiv\npreprint arXiv:2003.13376, 2020.\n[26] H. Guo, R. Pasunuru, and M. Bansal. Soft layer-speciﬁc multi-task summarization with\nentailment and question generation. arXiv preprint arXiv:1805.11004, 2018.\n[27] O. Gupta and R. Raskar. Distributed learning of deep neural network over multiple agents.\nJournal of Network and Computer Applications, 116:1–8, 2018.\n[28] A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein, H. Eichner, C. Kid-\ndon, and D. Ramage. Federated learning for mobile keyboard prediction. arXiv preprint\narXiv:1811.03604, 2018.\n[29] J. Irvin, P. Rajpurkar, M. Ko, Y . Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B. Haghgoo,\nR. Ball, K. Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty\nlabels and expert comparison. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 590–597, 2019.\n[30] B. Jing, P. Xie, and E. Xing. On the automatic generation of medical imaging reports. arXiv\npreprint arXiv:1711.08195, 2017.\n[31] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah. Transformers in vision:\nA survey. arXiv preprint arXiv:2101.01169, 2021.\n[32] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[33] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon. Federated\nlearning: Strategies for improving communication efﬁciency. arXiv preprint arXiv:1610.05492,\n2016.\n[34] T. Li, A. K. Sahu, A. Talwalkar, and V . Smith. Federated learning: Challenges, methods, and\nfuture directions. IEEE Signal Processing Magazine, 37(3):50–60, 2020.\n[35] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollár. Focal loss for dense object detection. In\nProceedings of the IEEE international conference on computer vision, pages 2980–2988, 2017.\n[36] S. Liu, E. Johns, and A. J. Davison. End-to-end multi-task learning with attention. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n1871–1880, 2019.\n12\n[37] X. Liu, P. He, W. Chen, and J. Gao. Multi-task deep neural networks for natural language\nunderstanding. arXiv preprint arXiv:1901.11504, 2019.\n[38] M.-T. Luong, Q. V . Le, I. Sutskever, O. Vinyals, and L. Kaiser. Multi-task sequence to sequence\nlearning. arXiv preprint arXiv:1511.06114, 2015.\n[39] P. M. Mammen. Federated learning: Opportunities and challenges. arXiv preprint\narXiv:2101.05428, 2021.\n[40] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efﬁcient\nlearning of deep networks from decentralized data. In Artiﬁcial Intelligence and Statistics ,\npages 1273–1282. PMLR, 2017.\n[41] H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private recurrent\nlanguage models. arXiv preprint arXiv:1710.06963, 2017.\n[42] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-stitch networks for multi-task learning.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n3994–4003, 2016.\n[43] S. Park, G. Kim, Y . Oh, J. B. Seo, S. M. Lee, J. H. Kim, S. Moon, J.-K. Lim, and J. C. Ye.\nVision transformer using low-level chest x-ray feature corpus for covid-19 diagnosis and severity\nquantiﬁcation. arXiv preprint arXiv:2104.07235, 2021.\n[44] S.-A. Rebufﬁ, H. Bilen, and A. Vedaldi. Learning multiple visual domains with residual adapters.\narXiv preprint arXiv:1705.08045, 2017.\n[45] M. Roberts, D. Driggs, M. Thorpe, J. Gilbey, M. Yeung, S. Ursprung, A. I. Aviles-Rivero,\nC. Etmann, C. McCague, L. Beer, et al. Common pitfalls and recommendations for using\nmachine learning to detect and prognosticate for COVID-19 using chest radiographs and CT\nscans. Nature Machine Intelligence, 3(3):199–217, 2021.\n[46] L. Rocher, J. M. Hendrickx, and Y .-A. De Montjoye. Estimating the success of re-identiﬁcations\nin incomplete datasets using generative models. Nature communications, 10(1):1–9, 2019.\n[47] P. Rogaway. Authenticated-encryption with associated-data. In Proceedings of the 9th ACM\nConference on Computer and Communications Security, pages 98–107, 2002.\n[48] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In International Conference on Medical image computing and computer-assisted\nintervention, pages 234–241. Springer, 2015.\n[49] RSNA. RSNA Pneumonia Detection Challenge. https://www.kaggle.com/c/\nrsna-pneumonia-detection-challenge , 2018.\n[50] S. Ruder, J. Bingel, I. Augenstein, and A. Søgaard. Latent multi-task architecture learning. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 4822–4829,\n2019.\n[51] M. J. Sheller, G. A. Reina, B. Edwards, J. Martin, and S. Bakas. Multi-institutional deep\nlearning modeling without sharing patient data: A feasibility study on brain tumor segmentation.\nIn International MICCAI Brainlesion Workshop, pages 92–104. Springer, 2018.\n[52] A. A. Shvets, A. Rakhlin, A. A. Kalinin, and V . I. Iglovikov. Automatic instrument segmentation\nin robot-assisted surgery using deep learning. In 2018 17th IEEE International Conference on\nMachine Learning and Applications (ICMLA), pages 624–628. IEEE, 2018.\n[53] A. Signoroni, M. Savardi, S. Benini, N. Adami, R. Leonardi, P. Gibellini, F. Vaccher,\nM. Ravanelli, A. Borghesi, R. Maroldi, and D. Farina. Bs-net: learning covid-19 pneu-\nmonia severity on a large chest x-ray dataset. Medical Image Analysis , page 102046,\n2021. ISSN 1361-8415. doi: https://doi.org/10.1016/j.media.2021.102046. URL https:\n//www.sciencedirect.com/science/article/pii/S136184152100092X.\n[54] SIIM-ACR. SIIM-ACR Pneumothorax Segmentation. https://www.kaggle.com/c/\nsiim-acr-pneumothorax-segmentation , 2019.\n13\n[55] V . Smith, C.-K. Chiang, M. Sanjabi, and A. Talwalkar. Federated multi-task learning.arXiv\npreprint arXiv:1705.10467, 2017.\n[56] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In Proceedings of the IEEE international conference on computer vision,\npages 843–852, 2017.\n[57] J. Sun, A. Li, B. Wang, H. Yang, H. Li, and Y . Chen. Soteria: Provable defense against privacy\nleakage in federated learning from representation perspective. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 9311–9319, 2021.\n[58] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, and X. Ding. Embracing imperfect\ndatasets: A review of deep learning solutions for medical image segmentation. Medical Image\nAnalysis, 63:101693, 2020.\n[59] W. G. Van Panhuis, P. Paul, C. Emerson, J. Grefenstette, R. Wilder, A. J. Herbst, D. Heymann,\nand D. S. Burke. A systematic review of barriers to data sharing in public health. BMC public\nhealth, 14(1):1–9, 2014.\n[60] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[61] P. Vepakomma, O. Gupta, T. Swedish, and R. Raskar. Split learning for health: Distributed deep\nlearning without sharing raw patient data. arXiv preprint arXiv:1812.00564, 2018.\n[62] C. Wachinger, P. Golland, W. Kremen, B. Fischl, M. Reuter, A. D. N. Initiative, et al. Brainprint:\nA discriminative characterization of brain morphology. NeuroImage, 109:232–248, 2015.\n[63] G. Wang, J. C. Ye, and B. De Man. Deep learning for tomographic image reconstruction.Nature\nMachine Intelligence, 2(12):737–748, 2020.\n[64] X. Wang, Y . Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers. Chestx-ray8: Hospital-scale\nchest x-ray database and benchmarks on weakly-supervised classiﬁcation and localization of\ncommon thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 2097–2106, 2017.\n[65] Z. Wang, M. Song, Z. Zhang, Y . Song, Q. Wang, and H. Qi. Beyond inferring class represen-\ntatives: User-level privacy leakage from federated learning. In IEEE INFOCOM 2019-IEEE\nConference on Computer Communications, pages 2512–2520. IEEE, 2019.\n[66] L. Wynants, B. Van Calster, G. S. Collins, R. D. Riley, G. Heinze, E. Schuit, M. M. Bonten,\nD. L. Dahly, J. A. Damen, T. P. Debray, et al. Prediction models for diagnosis and prognosis of\ncovid-19: systematic review and critical appraisal. bmj, 369, 2020.\n[67] Z. Xu, Y . Huo, J. Park, B. Landman, A. Milkowski, S. Grbic, and S. Zhou. Less is more:\nSimultaneous view classiﬁcation and landmark detection for abdominal ultrasound images. In\nInternational Conference on Medical Image Computing and Computer-Assisted Intervention,\npages 711–719. Springer, 2018.\n[68] Q. Yang, Y . Liu, T. Chen, and Y . Tong. Federated machine learning: Concept and applications.\nACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1–19, 2019.\n[69] W. Ye, J. Yao, H. Xue, and Y . Li. Weakly supervised lesion localization with probabilistic-cam\npooling. arXiv preprint arXiv:2005.14480, 2020.\n[70] M. Yurochkin, M. Agarwal, S. Ghosh, K. Greenewald, N. Hoang, and Y . Khazaeni. Probabilistic\nfederated neural matching. 2018.\n[71] B. Zhao, K. R. Mopuri, and H. Bilen. idlg: Improved deep leakage from gradients. arXiv\npreprint arXiv:2001.02610, 2020.\n[72] S. K. Zhou, H. Greenspan, C. Davatzikos, J. S. Duncan, B. van Ginneken, A. Madabhushi,\nJ. L. Prince, D. Rueckert, and R. M. Summers. A review of deep learning in medical imaging:\nImaging traits, technology trends, case studies with progress highlights, and future promises.\nProceedings of the IEEE, 2021.\n14\n[73] J. Zhu, B. Shen, A. Abbasi, M. Hoshmand-Kochi, H. Li, and T. Q. Duong. Deep transfer\nlearning artiﬁcial intelligence accurately stages COVID-19 lung disease severity on portable\nchest radiographs. PloS one, 15(7):e0236621, 2020.\n15\nA Implementation Details\nHere we describe the details of the hyperparameters used and their implementation. Our experiments\nwere mainly implemented using Python version 3.7.5 and 3.8.8 with packages Pytorch version 1.7.1\nand 1.8.1. We used a friendly federated learning framework (Flower) protocol [5] to implement the\ndistributed learning system, Pytorch to implement the neural network.\nA.1 Hyperparameters\nWe have searched different hyperparameters for each chest X-ray (CXR) task. For the classiﬁcation\ntask, we used an SGD optimizer with a warm-up cosine learning rate scheduler with a max learning\nrate of 0.0005. For the segmentation task, we minimized the binary cross-entropy loss combined with\ndice and focal loss using Adam [32] with a learning rate of 0.0001 and cosine annealing scheduler\nwith the maximum rounds of 2,000 for single-task and 1,000 for multi-task learning. Finally, the\nAdam optimizer along with the warm-up constant scheduler was used with the max learning rate of\n0.00002. Gradient clipping was implemented, where the max gradient norm of 1.0 was chosen to\nenhance the rate of convergence, and the batch sizes per client were 2 for all tasks. We experimentally\ndetermined the optimal hyperparameters for each task. Table 6 and Table 7 provide the detailed\nhyperparameters used for each task during single-task learning and multi-task learning.\nTable 6: Hyperparameters used for single-task learning\nHyperparameter Classiﬁcation Segmentation Detection\nLearning rate (head and tail) 0.0005 0.002 0.00002\nLearning rate (body) 0.0005 0.0005 0.0005\nScheduler warm-up cosine warm-up cosine annealing warm-up constant\nNumber of rounds 12,000 12,000 12,000\nWarm-up rounds 500 500 500\nMini-batch size 2 per client 2 per client 2 per client\nMaximum number of rounds - 2,000 -\nTable 7: Hyperparameters used for multi-task learning\nHyperparameter Classiﬁcation Segmentation Detection\nLearning rate (head and tail) 0.0005 0.002 0.00002\nLearning rate (body) 0.0005 0.0005 0.0005\nScheduler warm-up cosine warm-up consine annealing warm-up constant\nNumber of rounds 12,000 12,000 12,000\nWarm-up rounds 500 500 500\nMini-batch size 2 per client 2 per client 2 per client\nMaximum number of rounds - 2,000 -\nA.2 Details of network conﬁguration\nFig. 3 depicts the details of network conﬁgurations of the proposed method for each task. For\nclassiﬁcation, the embedded feature of dimension 16 ×16 ×768 from the head is ﬁrst ﬂattened\ninto the dimension of 256 ×768, and used as the input after prepending a CLS token with the same\nhidden dimension to yield the input of dimension 257 ×768. The output from the Transformer body\ncorresponding to this CLS token embed the comprehensive feature of the entire CXR image so that it\ncan be used to make the ﬁnal prediction (Fig. 3(a)). On the other hand, for the segmentation task,\nthe features at the deepest level of TransUNet of the dimension 32 ×32 ×1024 is used as the input\nof the Transformer after mapping into the dimension of 16 ×16 ×768 and ﬂattened to dimension\nof 256 ×768, and the CLS token is not utilized at all though it is prepended as the same way in the\nclassiﬁcation task to make the same feature size 257 ×768. The resulting transformed features from\nthe body are mapped into original shape and utilized as the same in standard TransUNet architecture\n(Fig. 3(b)). Similarly, the model for the detection task doesn’t use the CLS token, and it rather uses a\nsimilar approach to that of the segmentation task. The deepest level of the feature pyramid, which\nhas features of the dimension 16 ×16 ×1024, is ﬁrst mapped into dimension of 16 ×16 ×768, and\nis used as the input for the Transformer body after ﬂattening and prepending CLS token to make the\nsame dimension of 257 ×768 to other tasks. Then, the transformed feature from the body reverts to\n16\nthe original shape and position for the feature pyramid to be combined to yield the ﬁnal output (Fig.\n3(c)).\nFigure 3: Detailed conﬁguration for (a) classiﬁcation, (b) segmentation and (c) detection tasks\nA.3 Implementation of our framework upon Flower protocol\nFrom the implementation perspective of this consequential process, the major hurdle for federated\nlearning (FL) research is the paucity of open source frameworks that support scalable FL on multiple\nedge devices. Several studies performing FL on millions of edge devices have been published [28],\nbut they are based on a closed industrial system developed by a private corporation and are not\npublicly available. Meanwhile, even though several open-source frameworks including Tensorﬂow\nfederated [2], PySyft [1] and LEAF [8] enabled the experiments on FL simulation, they do not support\nheterogeneous clients, server-side orchestration and are neither scalable between multiple machines,\nnor language agnostic. Recently, an open-source framework, Flower has been developed to address\nthis problem which supports the heterogeneous environment and scaling to multiple distributed\nclients. It offers stable, language- and deep learning framework-agnostic implementation. Moreover,\nit allows rapid adoption of the existing deep learning algorithm to evaluate their learning dynamics\nand performances in a federated setting. Therefore, we implemented our framework upon this Flower\nprotocol.\nFig. 4 illustrates the core components of our framework based on Flower. Since the FL can be\nconsidered as an interplay between global (server) and local (client) computations, we implemented\nthe server and client-side components of our framework on top of the Flower server and clients.\nIn Flower clients, task-speciﬁc loops of the heads and tails are performed with local data of each\nclient, and the resulting features, gradients, and local parameters are passed toward the RPC client for\ncommunication. Then, the remote procedure call (RPC) client communicates with the RPC server\nin a language-agnostic manner using the bi-directional gRPC stream communication protocol [3],\nwhich offers an efﬁcient binary serialization format. On the server-side, a task-agnostic body loop is\nperformed using the features and gradients received. In addition, the aggregation, distribution of local\n17\nFigure 4: Implementation of our framework on top of Flower protocol.\nparameters through a strategy such as federated averaging (FedAvg) are performed per averaging\nrounds. Finally, the features, gradients, and aggregated global parameters from the server revert to\neach client.\nDifferent from previous studies that reported the result of single-device simulation [15], our method\nsupports the simulation with multiple machines, which is close to real-world implementation of the\nsystem across the edge devices.\nB Data-centralized and Other Distributed Learning Methods\nWe perform the comparison of Federated Split Task-Agnostic (FESTA) with data-centralized and\nother distributed learning methods on the COVID-19 classiﬁcation task which is the main task of this\nstudy. The details of each learning process are illustrated in Fig. 5.\nB.1 Details of data-centralized learning\nIn data-centralized learning, the local data from six clients are centrally aggregated by the server and\nthe single model is trained on a central server as represented in Fig. 5 (a). Batch size is set to 12 to\nmatch the setting of distributed learning strategies, accounting for batch size two per every six clients.\nOther settings are used as the same as FESTA for a fair comparison.\nB.2 Details of federated learning\nIn general, the simulation of the FL can be achieved by repeatedly doing three steps, as illustrated: i)\nupdate local parameters of the distributed model with local data on each client, ii) send the updated\nlocal parameters back to a server for aggregation, iii) distribute the aggregated model back to the\nclients for next rounds of local updates. Thus, we trained the entire network consisting of the head,\nbody, and the tail is trained on each client with its local data in parallel without dividing it into\nsub-networks components as in Fig. 5(b). This process can be formally written as in Algorithm 2.\nRegarding the experimental setting, the same settings to those of the proposed FESTA were used for\ncomparison.\nB.3 Details of split learning\nTo simulate split learning (SL), we adopted the SL without label sharing as suggested in the original\npaper of SL [61]. The detailed process of the SL method used in our experiment can be presented\nas in Algorithm 3. The overall process of SL is similar to FESTA except for the fact that a step\nof aggregation and distribution by the central server is absent in SL as in Fig. 5(c). The splitting\nconﬁguration of head, body, and tail on client and server sides were the same as in the proposed\nFESTA. Since the local head and tail parameters of individual clients are not uniﬁed in SL, the\ninference results on the external testing dataset can be different between clients. Therefore, we\ncalculated evaluation metrics for every six clients and averaged them to get the ﬁnal score. The\n18\nFigure 5: Detailed description for (a) data-centralized learning, (b) federated learning, (c) split\nlearning, (d) FESTA learning strategies.\nother experiment settings, including batch size and learning rate, remain the same as in the proposed\nFESTA.\nC Additional Experiments\nIn this section, the results of additional experiments to further analyze the proposed FESTA learning\nmethod are suggested.\nC.1 Performances with increased number of COVID-19 cases\nTo provide more robust results using the larger corpus of data especially in terms of the number of\nCOVID-19 cases, additional experiments were performed as follows.\nWe ﬁrst swapped the hospital 1 data (containing 6 COVID-19 cases), which was originally used\nas the external test dataset, with the hospital 3 data (containing 80 COVID-19 cases), and repeated\nthe experiments with the same setting. As suggested in Table 8, the proposed model retained stable\nperformance in hospital 3 data with 80 COVID-19 cases.\nSecondly, the additional analysis was performed by holding out all four hospital data (private) from\nthe training set and by using them for external validation. Here, the label system had to be simpliﬁed\ninto two categories, COVID-19 and non-COVID-19, as public datasets do not contain any label\ndata for the \"other infection\" class. The proposed model presented stable performance even after\nexcluding all private CXR data from the training set, dispelling the worry of data leakage problems.\nAlthough the performances were slightly decreased, it should be taken into consideration that the\n19\nAlgorithm 2: Federated learning\n1 Function ServerMain:\n2 Initialize the global weight ¯W and distribute to each client\n3 for rounds i= 1,2,...R do\n4 for clients c∈C do in parallel\n5 Wc ←ClientUpdate(c)\n6 if i∈UnifyingRounds then\n7 Update ¯W ← 1\nN\n∑\nc∈C\nWc\n8 Distribute the global weight to client Wc ← ¯W for each client c∈C\n9 Function ClientUpdate(c):\n10 xc,yc ←Current batch of input & label from client c\n11 Lc ←ℓc(yc,Tc(Bc(Hc(xc)))) & Backprop.\n12 Update Wc ←Wc −η∂Lc\n∂Wc\n13 return Wc\nAlgorithm 3: Split learning\n1 Function ServerMain:\n2 Initialize the body weight w(1)\nB and client head/tail weights ( ¯wH, ¯wT) in server\n3 for rounds i= 1,2,...R do\n4 for clients c∈C do in parallel\n5 if i= 1 then\n6 Set client (w(i)\nHc ,w(i)\nTc ) ←( ¯wH, ¯wT)\n7 h(i)\nc ←ClientHead(c)\n8 b(i)\nc ←B(h(i)\nc )\n9 ∂L(i)\nc\n∂b(i)\nc\n←ClientTail(c,b(i)\nc ) & Backprop.\n10 ClientUpdate(c,∂L(i)\nc\n∂h(i)\nc\n)\n11 Update body w(i+1)\nB ←w(i)\nB −η\nN\n∑\nc∈C\n∂L(i)\nc\n∂w(i)\nB\n12 Function ClientHead(c):\n13 xc ←Current batch of input from client c\n14 return Hc(xc)\n15 Function ClientTail(c,bc):\n16 yc ←Current batch of label from client c\n17 Lc ←ℓc(yc,Tc(bc)) & Backprop.\n18 return ∂Lc\n∂bc\n19 Function ClientUpdate(c,∂Lc\n∂hc\n):\n20 Backprop. & (wHc , wTc ) ←(wHc −η ∂Lc\n∂wHc\n, wTc −η ∂Lc\n∂wTc\n)\nTable 8: Number of COVID-19 cases with the different external set and the classiﬁcation performances\n(AUC).\nExternal test set COVID-19 cases Average COVID-19 Others Normal\nHospital 1 6 0.909 ± 0.021 0.880 ± 0.008 0.916 ± 0.038 0.931 ± 0.021\nHospital 3 80 0.913 ± 0.019 0.871 ± 0.043 0.932 ± 0.007 0.935 ± 0.015\nNote: Experiments were performed repeatedly with three random seeds to report mean and standard deviation.\ntotal amount of training data decreased to less than half of the original training data by removing the\nhospital 4 data (Table 9).\n20\nTable 9: Classiﬁcation performances (AUC) of the proposed model using all four hospital datasets as\nan external testset.\nExternal test set COVID-19 cases COVID-19\nAll four hospitals (hospital 1-4) 94 0.879 ± 0.043\nNote: Experiments were performed repeatedly with three random seeds to report mean and standard deviation.\nFinally, we gathered additional anterior-posterior (AP) view CXR data labeled by the experts and\ncombined them with the original posterior-anterior (PA) view data as shown in Table 10. The total\namount of COVID-19 data has doubled, and COVID-19 cases in hospital 1 increased 6 to 81 CXRs.\nWhen adding the additional AP view CXRs and evaluating the performances in hospital 1 data, the\nperformances of the proposed model were not compromised and rather increased especially for the\ndiagnosis of COVID-19 as in Table 11.\nTable 10: Increased dataset and sources for COVID-19 diagnosis.\nCXR view Total Hospital 1 Hospital 2 Hospital 3 Hospital 4 NIH Brixia BIMCV\nAP view (added)\nNormal 3662 97 - - 117 3355 - 93\nOther infection 204 19 76 92 17 - - -\nCOVID-19 3322 75 278 213 - - 2384 372\nTotal AP CXRs 7188 191 354 305 134 3355 2384 465\nAll view (total)\nNormal 17311 417 300 400 8978 7123 - 93\nOther infection 1672 58 220 400 994 - - -\nCOVID-19 5753 81 286 293 - - 4313 780\nTotal CXRs 24736 556 806 1093 9972 7123 4313 873\nTable 11: Number of COVID-19 cases after adding AP view CXRs and the classiﬁcation performances\n(AUC).\nExternal test set COVID-19 cases Average COVID-19 Others Normal\nHospital 1 (PA data) 6 0.909 ± 0.021 0.880 ± 0.008 0.916 ± 0.038 0.931 ± 0.021\nHospital 1 (PA and AP data) 81 0.924 ± 0.006 0.943 ± 0.015 0.879 ± 0.007 0.949 ± 0.008\nNote: Experiments were performed repeatedly with three random seeds to report mean and standard deviation.\nC.2 Comparison with task-speciﬁc expert and CNN-based multi-task learning models\nTable 12 shows a comparison of the performances of each task between the proposed Transformer-\nbased multi-task learning model trained with FESTA method and others. First, we compared the\nproposed MTL model with single task experts, deﬁned as following for each task.\n• Classiﬁcation: DenseNet-121 (D121) model with Probabilistic Class Activation Map\noperation Ye et al. [69]\n• Segmentation: AlbuNet [52] based segmentation network (1st place model in Kaggle\nSIIM-ACR pneumothorax segmentation challenge [54])\n• Detection: RetinaNet [35] model with SE-ResNext-50 encoder (2nd place model in Kaggle\nRSNA pneumonia detection challenge [49])\nAs provided in Table 12, the proposed MTL model outperformed the task-speciﬁc experts for\neach speciﬁc task. Of note, when the shared Transformer body was substituted with the shared\nconvolutional neural network (CNN) layer for MTL, the performance was substantially dropped\nin the detection task. Combined together, the results demonstrated the value of the Transformer\narchitecture leveraging global attention as well as local attention, which is suitable for MTL and\ncannot be substituted by other architecture like shared CNN layers.\n21\nTable 12: Comparison of performances with task-speciﬁc experts and CNN-based MTL models\nTasks Metrics Task-speciﬁc experts CNN-based MTL Transformer-based MTL\nClassiﬁcation AUC 0.898 ± 0.004 0.907 ± 0.011 0.931 ± 0.004\nSegmentation Dice 0.736 ± 0.014 0.797 ± 0.018 0.821 ± 0.003\nDetection mAP 0.190 ± 0.006 0.159 ± 0.035 0.204 ± 0.002\nNote: Experiments were performed repeatedly with three random seeds to report mean and standard deviation.\nIn addition, when compared with Kaggle’s winning solutions available for the segmentation [54]\nand detection tasks [49], the proposed MTL model showed comparable performances as shown in\nTable 13, suggesting that the Transformer body do not deface the performances of the individual\ntasks.\nTable 13: Comparison with Kaggle winning solutions for segmentation and detection tasks\nSegmentation Dice Detection mAP\n1st place solution (description) 0.764 ± 0.007 2nd place solution (SE-ResNext-50)0.211 ± 0.003\n4th place solution (descrption)0.841 ± 0.004 2nd place solution (SE-ResNext-101) 0.199 ± 0.003\nProposed MTL model 0.821 ± 0.003 Proposed MTL model 0.204 ± 0.002\nNote: Experiments were performed repeatedly with three random seeds to report mean and standard deviation.\nC.3 Estimates of communication costs\nWith the intrinsic property of FESTA learning, a high computational burden is imposed on the\nserver-side device, and this conﬁguration is what we intended. Suppose, if most of the computation is\nperformed on client-sides, all participating hospitals should have devices with high computational\ncapacity. Forcing to prepare high computational resources for participants will obviously hinder the\nwidespread adoption in a real-world application, and preparing a powerful server-side device with\nbetter security is rather practical. Nevertheless, there still remains a problem of computational costs\nbetween the server and clients. The communication costs between the server as the client can be\nestimated as follow.\nWhen the period between averaging is k and transmission of features, gradients, and network\nparameters are F, Gand P respectively, total transmission from Server to ClientT can be represented\nas follows:\nT = k×(F + G) + P, (3)\nWhen parameter numbers of head, body, and tail are Ph, Pb and Pt respectively and kis 100, T for\neach distributed learning strategy can be formulated as follows:\nTFL = Ph + Pb + Pt, (4)\nTSL = 100 ×(F + G), (5)\nTFESTA = 100 ×(F + G) + (Ph + Pt), (6)\nIf the transmission from Server to Client T and that from Client to Server TC→S are assumed to be\nequal (TC→S = T), total transmission T′is as follows:\nT′= 2T. (7)\nWe then calculated the communication costs for feature/gradient transmission and parameter trans-\nmission per 1 averaging (=100 rounds) for each task as shown in Table 14. Despite the fact that\nthe communication cost of the proposed FESTA framework was larger than that of SL, it was\nsubstantially lower than that of FL.\nC.4 Statistical analysis\nWe also performed whether or not the performance gains with the Transformer architecture are\nstatistically signiﬁcant. As provided in Table 15, the proposed MTL model outperformed the model\nwithout the Transformer body with statistical signiﬁcance, and the performance increase was more\nprominent in the MTL model.\n22\nTable 14: Communication costs of the distributed learning methods during training per 1 averaging\n(=100 rounds)\nTotal\ntransmission\nFeature and gradient\ntransmission\nNetwork parameter\ntransmission\nClassiﬁcation\nFederated learning 159.365M - 159.365M\nSplit learning 78.950M 78.950M -\nFESTA 105.580M 78.950M 26.630M\nSegmentation\nFederated learning 177.592M - 177.592M\nSplit learning 78.950M 78.950M -\nFESTA 123.808M 78.950M 44.858M\nDetection\nFederated learning 226.450M - 226.450M\nSplit learning 78.950M 78.950M -\nFESTA 172.665M 78.950M 93.715M\nTable 15: Statistical comparison of performances between the model with and without the transformer.\nCOVID-19 Others Normal\nAUC (95% CI) p-value AUC (95% CI) p-value AUC (95% CI) p-value\nw/o Transformer0.867 (0.696 - 1.000) - 0.883 (0.817 - 0.948) - 0.889 (0.837 - 0.941) -\nw Transformer (STL)0.868 (0.749 - 0.987) 0.9880.905 (0.852 - 0.958)0.498 0.927 (0.889 - 0.965)0.019\nw Transformer (MTL) 0.945 (0.896 - 0.995)0.266 0.893 (0.833 - 0.954) 0.7680.938 (0.903 - 0.974) 0.010\nNote: For statistical comparison, p-values and Conﬁdence Intervals (CIs) were calculated using DeLong’s test.\nNote: To evaluate the statistical signiﬁcance, the models with medium performance were compared.\nD Additional Ablation Studies in Multi-Task Learning Setting.\nAdditional ablation studies have been performed to examine the effect of Transformer body capacity\nand different training schemes, as shown in Table 16.\nEffect of Transformer body capacity We ﬁrst evaluated the effect of the network capacity of the\ntask-agnostic body model on the performances of individual tasks. Since the Transformer processes\nthe task-agnostic modeling between features in a multi-task setting, there exists a possibility that the\nperformance can further increase with the use of a dedicated server system with higher computational\nresources, once the model shows the performance proportional to the capacity of the Transformer\nbody. As suggested in Table 16, the model equipped with a smaller body showed lower performance\nthan that of a standard Transformer body equipped with 12 heads and 12 layers, suggesting the\npossibility of further improvement in performance with a Transformer with higher capacity.\nTraining scheme Since our framework consists of the server-side and client-side sub-networks, it\nis possible to train only part of these sub-networks or train this sub-network after ﬁxing the others.\nThus, we experimented with various training schemes to evaluate their effect on the performance\nin multi-task settings. For the one-step learning approach, we trained the model after having all\nsub-networks, namely head, body, and tail, learnable for the entire training round. For the alternating\napproach, we alternately ﬁxed and unﬁxed the parameters of the body and head/tail per 100 rounds.\nAs shown in Table 16, both of these approaches show lower performance than the proposed two-step\nlearning approach. This suggests that the simultaneous or alternating approach to train these sub-\nnetwork components makes training unstable. Fixing the body for multi-task processing after certain\nrounds and ﬁne-tuning the task-speciﬁc components may help to reach the better local minimum for\nthe head and tail for each task, resulting in better generalization performance.\n23\nTable 16: Additional ablations in multi-task setting\nTasks\nClassiﬁcation Segmentation Detection\nAUC Dice mAP\nEffect of Transformer body capacity\nH= 4,L= 4,Dhidden= 256 0.916±0.011 0.809 + 0.030 0.200 ±0.007\nH= 8,L= 8,Dhidden= 512 0.916±0.013 0.826 + 0.001 0.191 ±0.016\nH = 12, L= 12, Dhidden= 768 0.931±0.004 0.821 ±0.003 0.204 ±0.002\nEffect of training strategy\nOne-step approach 0.930 ±0.022 0.801 ±0.024 0.188 ±0.020\nAlternating approach 0.915 ±0.011 0.799 ±0.021 0.179 ±0.003\nTwo-step approach 0.931 ±0.004 0.821 ±0.003 0.204 ±0.002\nNote: Experiments were performed repeatedly with three random seeds to report mean and standard deviation.\nE Details of Hospital Dataset\nTable 17 describes the details about the CXR and clinical characteristics of four hospital data\ndeliberately collected for this study.\nTable 17: Details of CXR and patient characteristics of hospital datasets.\nData Hospital 1 Hospital 2\nCXR image details\nNumber of CXRs 365 452\nModality CR (93.7%), N/A (6.3%) CR (99.8%), N/A (0.2%)\nExposure time (msec) 6.7 ± 3.4 16.5 ± 7.7\nTube current (mA) 473.3 ± 198.1 307.8 ± 36.4\nBits 12 (12-14) 12 (12-12)\nClinical details\nAge 45.8 ± 15.9 50.9 ± 17.7\nSex M (47.7%), F (45.2%), N/A (7.1%) M (50.2%), F (48.8%)\nCOVID-19 severity 1 (1-3) 5.5 (1-6)\nCT positive cases N/A (100%) N/A (100%)\nCountry South Korea South Korea\nData Hospital 3 Hospital 4\nCXR image details\nNumber of CXRs 788 9838\nModality CR (100%) CR (3.8%), DX (96.2%)\nExposure time (msec) 11.6 ± 8.0 8.9 ± 3.9\nTube current (mA) 317.8 ± 30.7 298.9 ± 43.6\nBits 12 (12-14) 14 (10-15)\nClinical details\nAge 46.9 ± 16.6 46.3 ± 14.4\nSex M (26.9%), F (34.0%), N/A (39.1%) M (49.1%), F (47.0%), N/A (3.9%)\nCOVID-19 severity 5 (1-6) -\nCT positive cases Positive (3.8%), N/A (96.3%) -\nCountry South Korea South Korea\nAbbreviations: CXR, chest X-ray; CR, computed radiography; DX, digital x-ray; M, male; F, female.\nNote: Values are presented as mean ± standard deviation or median (range).\nF Ethic Committee Approval and Permission Information.\nEthic Committee Approval and Permission Information The four hospital data deliberately\ncollected for COVID-19 classiﬁcation were ethically approved by the Institutional Review Board of\neach hospital. According to their terms of use, the public datasets for the classiﬁcation task (CheXpert\n[29], Valencian Region Medical Image Bank [BIMCV] [17], Brixia [53, 7], National Institutes of\nHealth [NIH] [64]) can be used for research purposes. Likewise, the datasets for the segmentation\n24\ntasks (SIIM-ACR pneumothorax segmentation challenge [54]) and the detection (RSNA pneumonia\ndetection challenge [49]) can be used for academic research according to the terms of use.\n25",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.780463695526123
    },
    {
      "name": "Architecture",
      "score": 0.5490168333053589
    },
    {
      "name": "Transformer",
      "score": 0.543729841709137
    },
    {
      "name": "Artificial neural network",
      "score": 0.5400617122650146
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4937325417995453
    },
    {
      "name": "Task (project management)",
      "score": 0.47981810569763184
    },
    {
      "name": "Machine learning",
      "score": 0.45281001925468445
    },
    {
      "name": "Training set",
      "score": 0.43479031324386597
    },
    {
      "name": "Deep learning",
      "score": 0.42863041162490845
    },
    {
      "name": "Computation",
      "score": 0.42603591084480286
    },
    {
      "name": "Distributed computing",
      "score": 0.36117905378341675
    },
    {
      "name": "Algorithm",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 19
}