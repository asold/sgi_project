{
  "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
  "url": "https://openalex.org/W3117576675",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4221786267",
      "name": "Geva, Mor",
      "affiliations": [
        "Allen Institute for Artificial Intelligence",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A3163265119",
      "name": "Schuster, Roei",
      "affiliations": [
        "Tel Aviv University",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A4221504776",
      "name": "Berant, Jonathan",
      "affiliations": [
        "Allen Institute for Artificial Intelligence",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A4202210949",
      "name": "Levy, Omer",
      "affiliations": [
        "Tel Aviv University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2955227499",
    "https://openalex.org/W2891488835",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3034475796",
    "https://openalex.org/W3098680936",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3104350794",
    "https://openalex.org/W3162379521",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3104142662",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3042108129",
    "https://openalex.org/W3035691519",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2963400886",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2930926105",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3089596076"
  ],
  "abstract": "Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484–5495\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n5484\nTransformer Feed-Forward Layers Are Key-Value Memories\nMor Geva1,2 Roei Schuster1,3 Jonathan Berant1,2 Omer Levy1\n1Blavatnik School of Computer Science, Tel-Aviv University\n2Allen Institute for Artiﬁcial Intelligence\n3Cornell Tech\n{morgeva@mail,joberant@cs,levyomer@cs}.tau.ac.il, rs864@cornell.edu\nAbstract\nFeed-forward layers constitute two-thirds of a\ntransformer model’s parameters, yet their role\nin the network remains under-explored. We\nshow that feed-forward layers in transformer-\nbased language models operate as key-value\nmemories, where each key correlates with tex-\ntual patterns in the training examples, and each\nvalue induces a distribution over the output\nvocabulary. Our experiments show that the\nlearned patterns are human-interpretable, and\nthat lower layers tend to capture shallow pat-\nterns, while upper layers learn more semantic\nones. The values complement the keys’ in-\nput patterns by inducing output distributions\nthat concentrate probability mass on tokens\nlikely to appear immediately after each pattern,\nparticularly in the upper layers. Finally, we\ndemonstrate that the output of a feed-forward\nlayer is a composition of its memories, which\nis subsequently reﬁned throughout the model’s\nlayers via residual connections to produce the\nﬁnal output distribution.\n1 Introduction\nTransformer-based language models (Vaswani\net al., 2017) are at the core of state-of-the-art natu-\nral language processing (Devlin et al., 2019; Brown\net al., 2020), largely due to the success of self-\nattention. While much literature has been devoted\nto analyzing the function of self-attention layers\n(V oita et al., 2019; Clark et al., 2019; Vig and Be-\nlinkov, 2019), they account for only a third of a typ-\nical transformer’s parameters (4d2 per layer, where\nd is the model’s hidden dimension). Most of the\nparameter budget is spent on position-wise feed-\nforward layers ( 8d2 per layer), yet their role re-\nmains under-explored. What, if so, is the function\nof feed-forward layers in a transformer language\nmodel?\nWe show that feed-forward layers emulate neural\nmemories (Sukhbaatar et al., 2015), where the ﬁrst\nk1 k2 kdm\nv1 v2\nx\n.2 1.5 0\nself-attention layer\nStay with you for\nwhile\n... it will take a\n... every once in a\n... , and for a\nFF \nlayer\na\n5x4x3x2x1\nx’5x’4x’3x’2x’1\nvdm\nTransformer layers\nTransformer layers\nFigure 1: An illustration of how a feed-forward layer\nemulates a key-value memory. Input vectors (here, x5)\nare multiplied by keys to produce memory coefﬁcients\n(e.g., the memory coefﬁcient for v1 is 0.2), which then\nweigh distributions over the output vocabulary, stored\nin the values. The feed-forward layer’s output is thus\nthe weighted sum of its values.\nparameter matrix in the layer corresponds to keys,\nand the second parameter matrix tovalues. Figure 1\nshows how the keys (ﬁrst parameter matrix) inter-\nact with the input to produce coefﬁcients, which\nare then used to compute a weighted sum of the val-\nues (second parameter matrix) as the output. While\nthe theoretical similarity between feed-forward lay-\ners and key-value memories has previously been\nsuggested by Sukhbaatar et al. (2019), we take\nthis observation one step further, and analyze the\n“memories” that the feed-forward layers store.\nWe ﬁnd that each key correlates with a speciﬁc\nset of human-interpretable input patterns, such as\nn-grams or semantic topics. For example, k2 in\nFigure 1 is triggered by inputs that describe a pe-\n5485\nriod of time and end with “a”. Simultaneously, we\nobserve that each value can induce a distribution\nover the output vocabulary, and that this distribu-\ntion correlates with the next-token distribution of\nthe corresponding keys in the upper layers of the\nmodel. In the above example, the corresponding\nvalue v2 represents a distribution that puts most of\nits probability mass on the word “while”.\nLastly, we analyze how the language model, as\na whole, composes its ﬁnal prediction from indi-\nvidual memories. We observe that each layer com-\nbines hundreds of active memories, creating a dis-\ntribution that is qualitatively different from each of\nits component memories’ values. Meanwhile, the\nresidual connection between layers acts as a reﬁne-\nment mechanism, gently tuning the prediction at\neach layer while retaining most of the residual’s\ninformation.\nIn conclusion, our work sheds light on the func-\ntion of feed-forward layers in transformer-based\nlanguage models. We show that feed-forward lay-\ners act as pattern detectors over the input across\nall layers, and that the ﬁnal output distribution is\ngradually constructed in a bottom-up fashion.1\n2 Feed-Forward Layers as\nUnnormalized Key-Value Memories\nFeed-forward layers A transformer language\nmodel (Vaswani et al., 2017) is made of intertwined\nself-attention and feed-forward layers. Each feed-\nforward layer is a position-wise function, process-\ning each input vector independently. Let x ∈Rd\nbe a vector corresponding to some input text pre-\nﬁx. We can express the feed-forward layer FF(·) as\nfollows (bias terms are omitted):\nFF(x) =f(x ·K⊤) ·V (1)\nHere, K, V∈Rdm×d are parameter matrices, and\nf is a non-linearity such as ReLU.\nNeural memory A neural memory (Sukhbaatar\net al., 2015) consists of dm key-value pairs, which\nwe call memories.2 Each key is represented by a\nd-dimensional vector ki ∈Rd, and together form\nthe parameter matrix K ∈Rdm×d; likewise, we\ndeﬁne the value parameters as V ∈Rdm×d. Given\nan input vector x ∈Rd, we compute a distribution\n1The code for reproducing our experiments is available at\nhttps://github.com/mega002/ff-layers/.\n2We use the terms “memory cells” and “memories” inter-\nchangeably.\nover the keys, and use it to compute the expected\nvalue:\np(ki |x) ∝exp(x ·ki)\nMN(x) =\ndm∑\ni=1\np(ki |x)vi\nWith matrix notation, we arrive at a more compact\nformulation:\nMN(x) =softmax(x ·K⊤) ·V (2)\nFeed-forward layers emulate neural memory\nComparing equations 1 and 2 shows that feed-\nforward layers are almost identical to key-value\nneural memories; the only difference is that neu-\nral memory uses softmax as the non-linearity f(·),\nwhile the canonical transformer does not use a\nnormalizing function in the feed-forward layer.\nThe hidden dimension dm is essentially the num-\nber of memories in the layer, and the activation\nm = f(x ·K⊤), commonly referred to as the hid-\nden layer, is a vector containing an unnormalized\nnon-negative coefﬁcient for each memory. We re-\nfer to each mi as the memory coefﬁcient of the ith\nmemory cell.\nSukhbaatar et al. (2019) make an analogous ob-\nservation, and incorporate the parameters of the\nfeed-forward layers as persistent memory cells in\nthe self-attention layers. While this reparameteriza-\ntion works in practice, the experiment does not tell\nus much about the role of feed-forward layers in the\ncanonical transformer. If transformer feed-forward\nlayers are indeed key-value memories, then what\nmemories do they store?\nWe conjecture that each key vectorki captures\na particular pattern (or set of patterns) in the input\nsequence (Section 3), and that its corresponding\nvalue vector vi represents the distribution of tokens\nthat follows said pattern (Section 4).\n3 Keys Capture Input Patterns\nWe posit that the key vectorsK in feed-forward lay-\ners act as pattern detectors over the input sequence,\nwhere each individual key vector ki corresponds to\na speciﬁc pattern over the input preﬁx x1, . . . , xj.\nTo test our claim, we analyze the keys of a trained\nlanguage model’s feed-forward layers. We ﬁrst re-\ntrieve the training examples (preﬁxes of a sentence)\nmost associated with a given key, that is, the input\ntexts where the memory coefﬁcient is highest. We\n5486\nKey Pattern Example trigger preﬁxes\nk1\n449\nEnds with “substitutes”\n(shallow)\nAt the meeting, Elton said that “for artistic reasons there could be no substitutes\nIn German service, they were used as substitutes\nTwo weeks later, he came off the substitutes\nk6\n2546\nMilitary, ends with\n“base”/“bases”\n(shallow + semantic)\nOn 1 April the SRSG authorised the SADF to leave their bases\nAircraft from all four carriers attacked the Australian base\nBombers ﬂying missions to Rabaul and other Japanese bases\nk10\n2997\na “part of” relation\n(semantic)\nIn June 2012 she was named as one of the team that competed\nHe was also a part of the Indian delegation\nToy Story is also among the top ten in the BFI list of the 50 ﬁlms you should\nk13\n2989\nEnds with a time\nrange (semantic)\nWorldwide, most tornadoes occur in the late afternoon, between 3 pm and 7\nWeekend tolls are in effect from 7:00 pm Friday until\nThe building is open to the public seven days a week, from 11:00 am to\nk16\n1935 TV shows (semantic)\nTime shifting viewing added 57 percent to the episode’s\nThe ﬁrst season set that the episode was included in was as part of the\nFrom the original NBC daytime version , archived\nTable 1: Examples of human-identiﬁed patterns that trigger different memory keys.\nthen ask humans to identify patterns within the re-\ntrieved examples. For almost every key ki in our\nsample, a small set of well-deﬁned patterns, recog-\nnizable by humans, covers most of the examples\nassociated with the key.\n3.1 Experiment\nWe conduct our experiment over the language\nmodel of Baevski and Auli (2019), a 16-layer\ntransformer language model trained on WikiText-\n103 (Merity et al., 2017). This model deﬁnes\nd = 1024 and dm = 4096, and has a total of\ndm ·16 = 65, 536 potential keys to analyze. We\nrandomly sample 10 keys per layer (160 in total).\nRetrieving trigger examples We assume that\npatterns stored in memory cells originate from ex-\namples the model was trained on. Therefore, given\na key kℓ\ni that corresponds to the i-th hidden dimen-\nsion of the ℓ-th feed-forward layer, we compute the\nmemory coefﬁcient ReLU(xℓ\nj ·kℓ\ni) for every preﬁx\nx1, . . . , xj of every sentence from the WikiText-\n103’s training set.3 For example, for the hypotheti-\ncal sentence “I love dogs”, we will compute three\ncoefﬁcients, for the preﬁxes “I”, “I love”, and “I\nlove dogs”. Then, we retrieve the top-t trigger ex-\namples, that is, the t preﬁxes whose representation\nat layer ℓ yielded the highest inner product with kℓ\ni.\nPattern analysis We let human experts (NLP\ngraduate students) annotate the top-25 preﬁxes re-\ntrieved for each key, and asked them to (a) identify\nrepetitive patterns that occur in at least 3 preﬁxes\n(which would strongly indicate a connection to the\nkey, as this would unlikely happen if sentences\n3We segment training examples into sentences to simplify\nthe annotation task and later analyses.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nlayer\n0\n20\n40\n60\n80\n100% trigger examples\nshallow\nshallow + semantic\nsemantic\nnot-covered\nFigure 2: Breakdown of the labels experts assigned to\ntrigger examples in each layer. Some examples were\nnot associated with any pattern (“not-covered”).\nwere drawn at random) (b) describe each recog-\nnized pattern, and (c) classify each recognized pat-\ntern as “shallow” (e.g. recurring n-grams) or “se-\nmantic” (recurring topic). Each key and its corre-\nsponding top-25 preﬁxes were annotated by one\nexpert. To assure that every pattern is grounded in\nat least 3 preﬁxes, we instruct the experts to specify,\nfor each of the top-25 preﬁxes, which pattern(s) it\ncontains. A preﬁx may be associated with multiple\n(shallow or semantic) patterns.\nTable 1 shows example patterns. A fully-\nannotated example of the top-25 preﬁxes from a\nsingle memory key is shown in Appendix A.\n3.2 Results\nMemories are associated with human-\nrecognizable patterns Experts were able\nto identify at least one pattern for every key,\nwith an average of 3.6 identiﬁed patterns per\n5487\n1 2 3 4 5 6 7 8 9 10111213141516\nlayer\n70\n60\n50\n40\n30\n20\n10\nchange in memory coefficient (%)\nfirst last random\nFigure 3: Relative change in memory coefﬁcient\ncaused by removing the ﬁrst, the last, or a random to-\nken from the input.\nkey. Furthermore, the vast majority of retrieved\npreﬁxes (65%-80%) were associated with at least\none identiﬁed pattern (Figure 2). Thus, the top\nexamples triggering each key share clear patterns\nthat humans can recognize.\nShallow layers detect shallow patterns Com-\nparing the amount of preﬁxes associated with shal-\nlow patterns and semantic patterns (Figure 2), the\nlower layers (layers 1-9) are dominated by shallow\npatterns, often with preﬁxes that share the last word\n(e.g. k1\n449 in Table 1). In contrast, the upper layers\n(layers 10-16) are characterized by more semantic\npatterns, with preﬁxes from similar contexts but\nwithout clear surface-form similarities (e.g. k16\n1935\nin Table 1). This observation corroborates recent\nﬁndings that lower (upper) layers in deep contextu-\nalized models encode shallow (semantic) features\nof the inputs (Peters et al., 2018; Jawahar et al.,\n2019; Liu et al., 2019).\nTo further test this hypothesis, we sample 1600\nrandom keys (100 keys per layer) and apply lo-\ncal modiﬁcations to the top-50 trigger examples of\nevery key. Speciﬁcally, we remove either theﬁrst,\nlast, or a random token from the input, and measure\nhow this mutation affects the memory coefﬁcient.\nFigure 3 shows that the model considers the end of\nan example as more salient than the beginning for\npredicting the next token. In upper layers, remov-\ning the last token has less impact, supporting our\nconclusion that upper-layer keys are less correlated\nwith shallow patterns.\n4 Values Represent Distributions\nAfter establishing that keys capture patterns in train-\ning examples, we turn to analyze the information\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nlayer\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5agreement rate (%)\nFigure 4: Agreement rate between the top-ranked to-\nken based on the value vector vℓ\ni , and the next token of\nthe top-ranked trigger example associated with the key\nvector kℓ\ni .\nstored in their corresponding values. We show that\neach value vℓ\ni can be viewed as a distribution over\nthe output vocabulary, and demonstrate that this\ndistribution complements the patterns in the corre-\nsponding key kℓ\ni in the model’s upper layers (see\nFigure 1).\nCasting values as distributions over the vocabu-\nlary. We begin by converting each value vector\nvℓ\ni into a probability distribution over the vocab-\nulary by multiplying it by the output embedding\nmatrix E and applying a softmax:4\npℓ\ni = softmax(vℓ\ni ·E).\nThe probability distribution pℓ\ni is uncalibrated,\nsince the value vector vℓ\ni is typically multiplied\nby the input-dependent memory coefﬁcient mℓ\ni,\nchanging the skewness of the output distribution.\nThat said, the ranking induced by pℓ\ni is invariant to\nthe coefﬁcient, and can still be examined. This con-\nversion assumes (naïvely) that all model’s layers\noperate in the same embedding space.\nValue predictions follow key patterns in upper\nlayers. For every layer ℓ and memory dimension\ni, we compare the top-ranked token according to\nvℓ\ni, (argmax(pℓ\ni)) to the next token wℓ\ni in the top-\n1 trigger example according to kℓ\ni (the example\nwhose memory coefﬁcient for kℓ\ni is the highest).\nFigure 4 shows the agreement rate, i.e. the fraction\nof memory cells (dimensions) where the value’s\ntop prediction matches the key’s top trigger exam-\nple (argmax(pℓ\ni) = wℓ\ni ). It can be seen that the\n4This is a simpliﬁcation; in practice, we use the adaptive\nsoftmax (Baevski and Auli, 2019) to compute probabilities.\n5488\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nlayer\n0\n5000\n10000\n15000\n20000\n25000\n30000rank distribution\nFigure 5: Distribution of the rank of the next-token in\nthe top-1 trigger example of kℓ\ni (wℓ\ni ), according to the\nranking induced by the value vector vℓ\ni . We cut the tail\nof the distribution, which stretches up to the vocabulary\nsize (∼270K tokens).\nagreement rate is close to zero in the lower layers\n(1-10), but starting from layer 11, the agreement\nrate quickly rises until 3.5%, showing higher agree-\nment between keys and values on the identity of the\ntop-ranked token. Importantly, this value is orders\nof magnitude higher than a random token predic-\ntion from the vocabulary, which would produce a\nfar lower agreement rate (0.0004%), showing that\nupper-layer memories manifest non-trivial predic-\ntive power.\nNext, we take the next token of kℓ\ni’s top-1 trig-\nger example (wℓ\ni ), and ﬁnd where it ranks in the\nvalue vector’s distributionpℓ\ni. Figure 5 shows that\nthe rank of the next token of a trigger example in-\ncreases through the layers, meaning that wℓ\ni tends\nto get higher probability in the upper layers.\nDetecting predictive values. To examine if we\ncan automatically detect values with high agree-\nment rate, we analyze the probability of the values’\ntop prediction, i.e., ( max(pℓ\ni)). Figure 6 shows\nthat although these distributions are not calibrated,\ndistributions with higher maximum probabilities\nare more likely to agree with their key’s top trigger\nexample. We then take the 100 values with highest\nprobability across all layers and dimensions (97\nout of the 100 are in the upper layers, 11-16), and\nfor each value vℓ\ni , analyze the top-50 trigger ex-\namples of kℓ\ni. We ﬁnd that in almost half of the\nvalues (46 out of 100), there is at least one trigger\nexample that agrees with the value’s top prediction.\nExamples are provided in Table 2.\nDiscussion. When viewed as distributions over\nthe output vocabulary, values in the upper lay-\ners tend to assign higher probability to the next-\n8.9e-51.3e-41.8e-42.2e-42.7e-43.1e-43.6e-44.e-4\ntop prediction probability\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0agreement rate\nFigure 6: Agreement rate (between the top-ranked to-\nken based on the value vector vℓ\ni and the next token\nof the top-ranked trigger example associated with the\nkey vector kℓ\ni ) as a function of the maximal probability\nassigned by the value vector.\ntoken of examples triggering the corresponding\nkeys. This suggests that memory cells often store\ninformation on how to directly predict the output\n(the distribution of the next word) from the input\n(patterns in the preﬁx). Conversely, the lower lay-\ners do not exhibit such clear correlation between\nthe keys’ patterns and the corresponding values’\ndistributions. A possible explanation is that the\nlower layers do not operate in the same embedding\nspace, and therefore, projecting values onto the vo-\ncabulary using the output embeddings does not pro-\nduce distributions that follow the trigger examples.\nHowever, our results imply that some intermediate\nlayers do operate in the same or similar space to\nupper layers (exhibiting some agreement), which\nin itself is non-trivial. We leave further exploration\nof this phenomenon to future work.\n5 Aggregating Memories\nSo far, our discussion has been about the function\nof a single memory cell in feed-forward layers.\nHow does the information from multiple cells in\nmultiple layers aggregate to form a model-wide\nprediction? We show that every feed-forward layer\ncombines multiple memories to produce a distri-\nbution that is qualitatively different from each of\nits component memories’ value distributions (Sec-\ntion 5.1). These layer-wise distributions are then\ncombined via residual connections in a reﬁnement\nprocess, where each feed-forward layer updates the\nresidual’s distribution to ﬁnally form the model’s\noutput (Section 5.2).\n5489\nValue Prediction Precision@50 Trigger example\nv15\n222 each 68% But when bees and wasps resemble each\nv16\n752 played 16% Her ﬁrst role was in Vijay Lalwani’s psychological thriller Karthik Calling\nKarthik, where Padukone was cast as the supportive girlfriend of a depressed\nman (played\nv13\n2601 extratropical 4% Most of the winter precipitation is the result of synoptic scale, low pressure\nweather systems (large scale storms such as extratropical\nv15\n881 part 92% Comet served only brieﬂy with the ﬂeet, owing in large part\nv16\n2070 line 84% Sailing from Lorient in October 1805 with one ship of the line\nv12\n3186 jail 4% On May 11, 2011, four days after scoring 6 touchdowns for the Slaughter, Grady\nwas sentenced to twenty days in jail\nTable 2: Example values, their top prediction, the fraction of their key’s top-50 trigger examples that agree with\ntheir prediction, and a matching trigger example (with the target token marked in blue).\n5.1 Intra-Layer Memory Composition\nThe feed-forward layer’s output can be deﬁned as\nthe sum of value vectors weighted by their memory\ncoefﬁcients, plus a bias term:\nyℓ =\n∑\ni\nReLU(xℓ ·kℓ\ni) ·vℓ\ni + bℓ.\nIf each value vector vℓ\ni contains information about\nthe target token’s distribution, how is this infor-\nmation aggregated into a single output distribu-\ntion? To ﬁnd out, we analyze the behavior of 4,000\nrandomly-sampled preﬁxes from the validation set.\nHere, the validation set is used (rather than the\ntraining set used to ﬁnd trigger examples) since we\nare trying to characterize the model’s behavior at in-\nference time, not ﬁnd the examples it “memorizes”\nduring training.\nWe ﬁrst measure the fraction of “active” mem-\nories (cells with a non-zero coefﬁcient). Figure 7\nshows that a typical example triggers hundreds\nof memories per layer (10%-50% of 4096 dimen-\nsions), but the majority of cells remain inactive.\nInterestingly, the number of active memories drops\ntowards layer 10, which is the same layer in which\nsemantic patterns become more prevalent than shal-\nlow patterns, according to expert annotations (see\nSection 3, Figure 2).\nWhile there are cases where a single memory\ncell dominates the output of a layer, the majority\nof outputs are clearly compositional. We count the\nnumber of instances where the feed-forward layer’s\ntop prediction is different from all of the memories’\ntop predictions. Formally, we denote:\ntop(h) = argmax(h ·E)\nas a generic shorthand for the top prediction from\nthe vocabulary distribution induced by the vector\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nlayer\n0\n20\n40\n60\n80\n100% of active memories\nFigure 7: The fraction of active memories (i.e., with\npositive memory coefﬁcient) out of 4096 memories in\nevery layer, for a random sample of 4,000 examples.\nh, and compute the number of examples where the\nfollowing condition holds:\n∀i : top(vℓ\ni) ̸= top(yℓ)\nFigure 8 shows that, for any layer in the network,\nthe layer’s ﬁnal prediction is different than every\none of the memories’ predictions in at least∼68%\nof the examples. Even in the upper layers, where\nthe memories’ values are more correlated with the\noutput space (Section 4), the layer-level prediction\nis typically not the result of a single dominant mem-\nory cell, but a composition of multiple memories.\nWe further analyze cases where at least one mem-\nory cell agrees with the layer’s prediction, and ﬁnd\nthat (a) in 60% of the examples the target token is\na common stop word in the vocabulary (e.g. “the”\nor “of” ), and (b) in 43% of the cases the input\npreﬁx has less than 5 tokens. This suggests that\nvery common patterns in the training data might\n5490\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nlayer\n0\n20\n40\n60\n80\n100% examples with zero agreement\nFigure 8: The fraction of examples in a random sam-\nple of 4,000 examples where the layer’s prediction is\ndifferent from the prediction of all of its memories.\nbe “cached” in individual memory cells, and do not\nrequire compositionality.\n5.2 Inter-Layer Prediction Reﬁnement\nWhile a single feed-forward layer composes its\nmemories in parallel, a multi-layer model uses the\nresidual connection r to sequentially compose pre-\ndictions to produce the model’s ﬁnal output:5\nxℓ = LayerNorm(rℓ)\nyℓ = FF(xℓ)\noℓ = yℓ + rℓ\nWe hypothesize that the model uses the sequential\ncomposition apparatus as a means to reﬁne its pre-\ndiction from layer to layer, often deciding what the\nprediction will be at one of the lower layers.\nTo test our hypothesis, we ﬁrst measure how\noften the probability distribution induced by the\nresidual vector rℓ matches the model’s ﬁnal output\noL (L being the total number of layers):\ntop(rℓ) =top(oL)\nFigure 9 shows that roughly a third of the model’s\npredictions are determined in the bottom few layers.\nThis number grows rapidly from layer 10 onwards,\nimplying that the majority of “hard” decisions oc-\ncur before the ﬁnal layer.\nWe also measure the probability massp that each\nlayer’s residual vector rℓ assigns to the model’s\n5The residual propagates information from previous layers,\nincluding the transformer’s self-attention layers.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nlayer\n0\n20\n40\n60\n80\n100\n% examples s.t. the residual\n predicts the final output\nFigure 9: Fraction of examples in each layer, where the\nresidual’s top prediction matches the model’s output.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0probability\nFigure 10: Probability of the token output by the model\naccording to the residual of each layer.\nﬁnal prediction:\nw = top(oL)\np = softmax(rℓ ·E)\np = pw\nFigure 10 shows a similar trend, but emphasizes\nthat it is not only the top prediction’s identity that\nis reﬁned as we progress through the layers, it is\nalso the model’s conﬁdence in its decision.\nTo better understand how the reﬁnement pro-\ncess works at each layer, we measure how of-\nten the residual’s top prediction changes follow-\ning its interaction with the feed-forward layer\n(top(rℓ) ̸= top(oℓ)), and whether this change re-\nsults from the feed-forward layer overriding the\nresidual (top(oℓ) = top(yℓ)) or from a true com-\nposition (top(rℓ) ̸= top(oℓ) ̸= top(yℓ)).\nFigure 11 shows the breakdown of different\ncases per layer. In the vast majority of exam-\nples, the residual’s top prediction ends up being the\n5491\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nlayer\n0\n20\n40\n60\n80\n100% examples residual\nagreement\ncomposition\nffn\nFigure 11: Breakdown of examples by prediction cases:\nthe layer’s output prediction matches the residual’s pre-\ndiction (residual), matches the feed-forward layer’s pre-\ndiction ( ffn), matches both of the predictions ( agree-\nment), or none of the predictions ( composition). By\nconstruction, there are no cases where the residual’s\nprediction matches the feed-forward layer’s prediction\nand does not match the output’s prediction.\nmodel’s prediction (residual+agreement). In most\nof these cases, the feed forward layer predicts some-\nthing different ( residual). Perhaps surprisingly,\nwhen the residual’s prediction does change (com-\nposition+ffn), it rarely changes to the feed-forward\nlayer’s prediction (ffn). Instead, we observe that\ncomposing the residual’s distribution with that of\nthe feed-forward layer produces a “compromise”\nprediction, which is equal to neither (composition).\nThis behavior is similar to the intra-layer compo-\nsition we observe in Section 5.1. A possible con-\njecture is that the feed-forward layer acts as an\nelimination mechanism to “veto” the top prediction\nin the residual, and thus shifts probability mass to-\nwards one of the other candidate predictions in the\nhead of the residual’s distribution.\nFinally, we manually analyze 100 random cases\nof last-layer composition, where the feed-forward\nlayer modiﬁes the residual output in the ﬁnal layer.\nWe ﬁnd that in most cases (66 examples), the\noutput changes to a semantically distant word\n(e.g., “people” →“same”) and in the rest of the\ncases (34 examples), the feed-forward layer’s out-\nput shifts the residual prediction to a related word\n(e.g. “later” →“earlier” and “gastric” →“stom-\nach”). This suggests that feed-forward layers tune\nthe residual predictions at varying granularity, even\nin the last layer of the model.\n6 Related Work\nConsiderable attention has been given to demys-\ntifying the operation of neural NLP models. An\nextensive line of work targeted neuron functionality\nin general, extracting the properties that neurons\nand subsets of neurons capture (Durrani et al., 2020;\nDalvi et al., 2019; Rethmeier et al., 2020; Mu and\nAndreas, 2020; Vig et al., 2020), regardless of the\nmodel architecture or neurons’ position in it. Ja-\ncovi et al. (2018) analyzed CNN architectures in\ntext classiﬁcation and showed that they extract key\nn-grams from the inputs.\nThe study of the transformer architecture has\nfocused on the role and function of self-attention\nlayers (V oita et al., 2019; Clark et al., 2019; Vig\nand Belinkov, 2019) and on inter-layer differences\n(i.e. lower vs. upper layers) (Tenney et al., 2019;\nJawahar et al., 2019). Previous work also high-\nlighted the importance of feed-forward layers in\ntransformers (Press et al., 2020; Pulugundla et al.,\n2021; Xu et al., 2020). Still, to date, the role of\nfeed-forward layers remains under-explored.\nAlso related are interpretability methods that ex-\nplain predictions (Han et al., 2020; Wiegreffe and\nPinter, 2019), however, our focus is entirely differ-\nent: we do not interpret individual predictions, but\naim to understand the mechanism of transformers.\nCharacterizing the functionality of memory cells\nbased on examples that trigger maximal activations\nhas been used previously in NLP (Rethmeier et al.,\n2020) and vision (Erhan et al., 2009).\n7 Discussion and Conclusion\nUnderstanding how and why transformers work is\ncrucial to many aspects of modern NLP, includ-\ning model interpretability, data security, and de-\nvelopment of better models. Feed-forward layers\naccount for most of a transformer’s parameters, yet\nlittle is known about their function in the network.\nIn this work, we propose that feed-forward lay-\ners emulate key-value memories, and provide a set\nof experiments showing that: (a) keys are corre-\nlated with human-interpretable input patterns; (b)\nvalues, mostly in the model’s upper layers, induce\ndistributions over the output vocabulary that corre-\nlate with the next-token distribution of patterns in\nthe corresponding key; and (c) the model’s output\nis formed via an aggregation of these distributions,\nwhereby they are ﬁrst composed to form individual\nlayer outputs, which are then reﬁned throughout\nthe model’s layers using residual connections.\nOur ﬁndings open important research directions:\n• Layer embedding space. We observe a correla-\ntion between value distributions over the output\n5492\nvocabulary and key patterns, that increases from\nlower to upper layers (Section 4). Is this because\nthe layer’s output space transforms across layers?\nIf so, how? We note that this possible transforma-\ntion cannot be explained solely by the function of\nfeed-forward layers: if the model only did a se-\nries of key-value look-ups and value-distribution\naggregation via weighted addition, then a single,\nunifying embedding space would appear more\nnatural. Thus, the transformation might have to\ndo with the interplay between feed-forward lay-\ners and self-attention layers.\n• Beyond language modeling. Our formulation\nof feed-forward networks as key-value memories\ngeneralizes to any transformer model, e.g. BERT\nencoders and neural translation models. We thus\nexpect our qualitative empirical observations to\nhold across diverse settings, and leave veriﬁca-\ntion of this for future work.\n• Practical implications. A better understanding\nof feed-forward layers has many implications in\nNLP. For example, future studies may offer in-\nterpretability methods by automating the pattern-\nidentiﬁcation process; memory cells might af-\nfect training-data privacy as they could facili-\ntate white-box membership inference (Nasr et al.,\n2019); and studying cases where a correct pattern\nis identiﬁed but then suppressed during aggrega-\ntion may guide architectural novelties.\nThus, by illuminating the role of feed-forward\nlayers, we move towards a better understanding of\nthe inner workings of transformers, and open new\nresearch threads on modern NLP models.\nAcknowledgements\nWe thank Shimi Salant and Tal Schuster for help-\nful feedback. This work was supported in part by\nthe Yandex Initiative for Machine Learning, the\nBlavatnik Interdisciplinary Cyber Research Center\n(ICRC), the Alon Scholarship, and Intel Corpora-\ntion. Roei Schuster is a member of the Check Point\nInstitute of Information Technology. This work\nwas completed in partial fulﬁllment for the Ph.D\ndegree of Mor Geva.\nReferences\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Proceedings of Neural Information Process-\ning Systems (NeurIPS).\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? An analysis of BERT’s attention. In Black-\nBoxNLP Workshop at ACL.\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan\nBelinkov, Anthony Bau, and James Glass. 2019.\nWhat is one grain of sand in the desert? analyz-\ning individual neurons in deep nlp models. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 33, pages 6309–6317.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL) , pages 4171–4186,\nMinneapolis, Minnesota.\nNadir Durrani, Hassan Sajjad, Fahim Dalvi, and\nYonatan Belinkov. 2020. Analyzing individual neu-\nrons in pre-trained language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nDumitru Erhan, Yoshua Bengio, Aaron Courville, and\nPascal Vincent. 2009. Visualizing higher-layer fea-\ntures of a deep network. University of Montreal ,\n1341(3):1.\nXiaochuang Han, Byron C. Wallace, and Yulia\nTsvetkov. 2020. Explaining black box predictions\nand unveiling data artifacts through inﬂuence func-\ntions. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5553–5563, Online. Association for Computa-\ntional Linguistics.\nAlon Jacovi, Oren Sar Shalom, and Yoav Goldberg.\n2018. Understanding convolutional neural networks\nfor text classiﬁcation. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP , pages 56–65,\nBrussels, Belgium. Association for Computational\nLinguistics.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure\n5493\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. International Conference on Learning Repre-\nsentations (ICLR).\nJesse Mu and Jacob Andreas. 2020. Compositional ex-\nplanations of neurons. In Proceedings of Neural In-\nformation Processing Systems (NeurIPS).\nMilad Nasr, Reza Shokri, and Amir Houmansadr.\n2019. Comprehensive privacy analysis of deep\nlearning: Passive and active white-box inference\nattacks against centralized and federated learning.\nIn 2019 IEEE Symposium on Security and Privacy\n(SP), pages 739–753.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In North American Chapter of the Asso-\nciation for Computational Linguistics (NAACL).\nOﬁr Press, Noah A. Smith, and Omer Levy. 2020. Im-\nproving transformer models by reordering their sub-\nlayers. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2996–3005, Online. Association for Computa-\ntional Linguistics.\nBhargav Pulugundla, Yang Gao, Brian King, Gokce\nKeskin, Harish Mallidi, Minhua Wu, Jasha Droppo,\nand Roland Maas. 2021. Attention-based neural\nbeamforming layers for multi-channel speech recog-\nnition. arXiv preprint arXiv:2105.05920.\nNils Rethmeier, Vageesh Kumar Saxena, and Isabelle\nAugenstein. 2020. Tx-ray: Quantifying and explain-\ning model-knowledge transfer in (un-) supervised\nnlp. In Conference on Uncertainty in Artiﬁcial In-\ntelligence, pages 440–449. PMLR.\nS. Sukhbaatar, J. Weston, and R. Fergus. 2015. End-\nto-end memory networks. In Advances in Neural\nInformation Processing Systems (NIPS).\nSainbayar Sukhbaatar, Edouard Grave, Guillaume\nLample, Herve Jegou, and Armand Joulin. 2019.\nAugmenting self-attention with persistent memory.\narXiv preprint arXiv:1907.01470.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems (NIPS), pages 5998–6008.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing\nthe structure of attention in a transformer language\nmodel. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 63–76, Florence, Italy. As-\nsociation for Computational Linguistics.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stuart\nShieber. 2020. Investigating gender bias in language\nmodels using causal mediation analysis. Advances\nin Neural Information Processing Systems, 33.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is\nnot not explanation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 11–20, Hong Kong, China. Associ-\nation for Computational Linguistics.\nHongfei Xu, Qiuhui Liu, Deyi Xiong, and Josef van\nGenabith. 2020. Transformer with depth-wise lstm.\narXiv preprint arXiv:2007.06257.\n5494\nA Pattern Analysis\nTable 3 provides a fully-annotated example of 25\npreﬁxes from the memory cell k5\n895.\nB Implementation details\nIn this section, we provide further implementation\ndetails for reproducibility of our experiments.\nFor all our experiments, we used the language\nmodel of Baevski and Auli (2019) (247M\nparameters) trained on WikiText-103 (Merity\net al., 2017). Speciﬁcally, we used the model\ntransformer_lm.wiki103.adaptive\ntrained with the fairseq toolkit6.\nWikiText-1037 is a well known language model-\ning dataset and a collection of over 100M tokens\nextracted from Wikipedia. We used spaCy8 to split\nexamples into sentences (Section 3).\n6https://github.com/pytorch/fairseq\n7https://blog.einstein.ai/the-\nwikitext-long-term-dependency-language-\nmodeling-dataset/\n8https://spacy.io/\n5495\n1 It requires players to press\n1 The video begins at a press\n1 The ﬁrst player would press\n1 Ivy, disguised as her former self, interrupts a Wayne Enterprises press\n1 The video then cuts back to the press\n1 The player is able to press\nLeto switched\n1 In the Nintendo DS version, the player can choose to press\n1 In-house engineer Nick Robbins said Shields made it clear from the outset that he (Robbins) “was just there to press\n1 She decides not to press\n1 she decides not to press\n1 Originally Watson signaled electronically, but show staff requested that it press\n1 At post-game press\n1 In the buildup to the game, the press\n2 Hard to go back to the game after that news\n1 In post-trailer interviews, Bungie staff members told gaming press\nSpace Gun was well received by the video game\n1 As Bong Load struggled to press\nAt Michigan, Clancy started as a quarterback, switched\n1 Crush used his size advantage to perform a Gorilla press\n1,2 Groening told the press\n1 Creative director Gregoire <unk> argued that existing dance games were merely instructing players to press\n1,2 Mattingly would be named most outstanding player that year by the press\n1 At the post-match press\n1,2 The company receives bad press\nID Description shallow / semantic\n1 Ends with the word “press” shallow\n2 Press/news related semantic\nTable 3: A pattern annotation of trigger examples for the cell memory k5\n895. Trigger examples are annotated with\nrepetitive patterns (upper table), which are classiﬁed as “shallow” or “semantic” (bottom table).",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7075574398040771
    },
    {
      "name": "Feed forward",
      "score": 0.5926320552825928
    },
    {
      "name": "Computer science",
      "score": 0.583838164806366
    },
    {
      "name": "Residual",
      "score": 0.4902086853981018
    },
    {
      "name": "Complement (music)",
      "score": 0.473691463470459
    },
    {
      "name": "Vocabulary",
      "score": 0.45823413133621216
    },
    {
      "name": "Key (lock)",
      "score": 0.4386123716831207
    },
    {
      "name": "Algorithm",
      "score": 0.25788626074790955
    },
    {
      "name": "Electrical engineering",
      "score": 0.14380750060081482
    },
    {
      "name": "Engineering",
      "score": 0.09623345732688904
    },
    {
      "name": "Linguistics",
      "score": 0.08359023928642273
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Phenotype",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Control engineering",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Complementation",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    }
  ]
}