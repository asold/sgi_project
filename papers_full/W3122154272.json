{
  "title": "DeLighT: Deep and Light-weight Transformer",
  "url": "https://openalex.org/W3122154272",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3133831624",
      "name": "Mehta, Sachin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223995600",
      "name": "Ghazvininejad, Marjan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2210503090",
      "name": "Iyer, Srinivasan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751234931",
      "name": "Zettlemoyer, Luke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200857851",
      "name": "Hajishirzi, Hannaneh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970820321",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2859444450",
    "https://openalex.org/W2995446988",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2996124573",
    "https://openalex.org/W2888862264",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963689957",
    "https://openalex.org/W2281746805"
  ],
  "abstract": "We introduce a deep and light-weight transformer, DeLighT, that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using the DeLighT transformation, a deep and light-weight transformation, and (2) across blocks using block-wise scaling, which allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that DeLighT matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average. Our source code is available at: \\url{https://github.com/sacmehta/delight}",
  "full_text": "Published as a conference paper at ICLR 2021\nDELIGH T: DEEP AND LIGHT -WEIGHT TRANSFORMER\nSachin Mehta1, Marjan Ghazvininejad2, Srinivasan Iyer2,\nLuke Zettlemoyer1, 2, and Hannaneh Hajishirzi1,3\n1University of Washington 2Facebook AI Research 3Allen Institute for AI\nABSTRACT\nWe introduce a deep and light-weight transformer, DeLighT, that delivers similar\nor better performance than standard transformer-based models with signiﬁcantly\nfewer parameters. DeLighT more efﬁciently allocates parameters both (1) within\neach Transformer block using the DeLighT transformation, a deep and light-\nweight transformation and (2) across blocks using block-wise scaling, that allows\nfor shallower and narrower DeLighT blocks near the input and wider and deeper\nDeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4\ntimes deeper than standard transformer models and yet have fewer parameters\nand operations. Experiments on benchmark machine translation and language\nmodeling tasks show that DeLighT matches or improves the performance of\nbaseline Transformers with 2 to 3 times fewer parameters on average.\n1 I NTRODUCTION\nAttention-based transformer networks (Vaswani et al., 2017) are widely used for sequence modeling\ntasks, including language modeling and machine translation. To improve performance, models are\noften scaled to be either wider, by increasing the dimension of hidden layers, or deeper, by stacking\nmore transformer blocks. For example, T5 (Raffel et al., 2019) uses a dimension of 65K and GPT-3\n(Brown et al., 2020) uses 96 transformer blocks. However, such scaling increases the number of\nnetwork parameters signiﬁcantly (e.g., T5 and GPT-3 have 11 billion and 175 billion parameters,\nrespectively), and complicates learning, i.e., these models either require very large training corpora\n(Raffel et al., 2019; Devlin et al., 2019; Brown et al., 2020) or careful regularization (Hinton et al.,\n2012; Wan et al., 2013; Merity et al., 2018a). In this paper, we introduce a new parameter-efﬁcient\nattention-based architecture that can be easily scaled to be both wide and deep.\nOur Deep and Light-weight Transformer architecture, DeLighT, extends the transformer archi-\ntecture of Vaswani et al. (2017) and delivers similar or better performance with signiﬁcantly fewer\nparameters and operations. At the heart of DeLighT is the DeLighT transformation that uses\nthe group linear transformations (GLTs) of Mehta et al. (2018) with an expand-reduce strategy for\nvarying the width and depth of the DeLighT block efﬁciently. Since GLTs are local by nature,\nthe DeLighT transformation uses feature shufﬂing, which is analogous to channel shufﬂing in\nconvolutional networks (Zhang et al., 2018), to share information between different groups. Such\nwide and deep representations facilitate replacing the multi-head attention and feed-forward layers\nin transformers with single headed attention and light-weight feed-forward layers, reducing total\nnetwork parameters and operations. Importantly, unlike transformers, the DeLighT transforma-\ntion decouples the depth and width from the input size, allowing us to allocate parameters more\nefﬁciently across blocks by using shallower and narrower DeLighT blocks near the input and deeper\nand wider DeLighT blocks near the output.\nWe demonstrate that DeLighT models achieve similar or better performance than transformer\nmodels with signiﬁcantly fewer parameters and operations, on two common sequence modeling\ntasks, (i) machine translation and (ii) language modeling. On the low resource WMT’16 En-Ro\nmachine translation dataset, DeLighT attains transformer performance using 2.8×fewer parameters.\nOn the high resource WMT’14 En-Fr dataset,DeLighT delivers better performance (+0.4 BLEU\nscore) with 1.8×fewer parameters than baseline transformers. Similarly, on language modeling,\nDeLighT matches the performance of Transformer-XL (Dai et al., 2019) with1.5×fewer parameters\n1\narXiv:2008.00623v2  [cs.LG]  11 Feb 2021\nPublished as a conference paper at ICLR 2021\non the WikiText-103 dataset. Our source code is open-source and is available at: https://github.com/\nsacmehta/delight\n2 R ELATED WORK\nImproving transformers: Several methods have been introduced to improve the transformer ar-\nchitecture. The ﬁrst line of research addresses the challenge of computing self attention on long\ninput sequences (Child et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020). These methods can\nbe combined with our architecture. The second line of research focuses on explaining multi-head\nattention (Raganato and Tiedemann, 2018; Brunner et al., 2020). They show that increasing the\nnumber of transformer heads can lead to redundant representations (V oita et al., 2019a; Michel et al.,\n2019) and using ﬁxed attention heads with predeﬁned patterns (Raganato et al., 2020) or synthetic\nattention matrices (Tay et al., 2020) improves performance. The third line of research focuses on\nimproving transformers by learning better representations (Wu et al., 2019; 2020; So et al., 2019).\nThese works aim to improve the expressiveness of transformers using different transformations – for\nexample, using convolutions (Wu et al., 2019; Gehring et al., 2017), gated linear units (Dauphin et al.,\n2017), or multi-branch feature extractors (So et al., 2019; Wu et al., 2020). Our work falls into this\ncategory. Unlike previous works, we show that it is possible to efﬁciently allocate parameters both at\nthe block-level using the DeLighT transformation and across blocks using block-wise scaling.\nModel scaling: Model scaling is a standard method to improve the performance of sequence models\n(Vaswani et al., 2017; Raffel et al., 2019; Lan et al., 2020; Devlin et al., 2019; Shoeybi et al., 2019;\nTan and Le, 2019; Brown et al., 2020). Model dimensions are increased in width-wise scaling\n(Vaswani et al., 2017; Devlin et al., 2019) while more blocks (e.g., Transformer blocks) are stacked\nin depth-wise scaling (Shoeybi et al., 2019; Brown et al., 2020; Wang et al., 2019). In both cases (and\ntheir combination), parameters inside each block of the network are the same, which may lead to a\nsub-optimal solution. To further improve the performance of sequence models, this paper introduces\nblock-wise scaling that allows for variably-sized blocks and efﬁcient allocation of parameters in the\nnetwork. Our results show that (1) shallower and narrower DeLighT blocks near the input and\ndeeper and wider DeLighT blocks near the output deliver the best performance, and (2) models\nwith block-wise scaling coupled with model scaling achieve better performance compared to model\nscaling alone. We note that convolutional neural networks (CNNs) also learn shallower and narrower\nrepresentations near the input and deeper and wider representations near the output. Unlike CNNs\n(e.g., ResNet of He et al. 2016) that perform a ﬁxed number of operations at each convolutional layer,\nthe proposed block-wise scaling uses a variable number of operations in each layer and block.\nImproving sequence models: There is also signiﬁcant recent work on other related methods for\nimproving sequence models, including (1) improving accuracy using better token-level representations\n– for example, using BPE (Sennrich et al., 2016), adaptive inputs (Baevski and Auli, 2019) and outputs\n(Grave et al., 2017a), and DeFINE (Mehta et al., 2020), and (2) improving efﬁciency – for example,\nusing compression (Chen et al., 2018; Sun et al., 2020), pruning (Han et al., 2016; V oita et al., 2019b),\nand distillation (Hinton et al., 2015; Sanh et al., 2019). The closest to our work is the DeFINE\ntransformation, which also learns representations using an expand-reduce strategy. The key difference\nbetween the DeFINE transformation (Figure 1c) and the DeLighT transformation (Figure 1d) is that\nthe DeLighT transformation more efﬁciently allocates parameters within expansion and reduction\nlayers. Unlike DeFINE, which uses fewer groups in group linear transformations to learn wider\nrepresentations, DeLighT transformation uses more groups to learn wider representations with\nfewer parameters. The DeLighT transformation achieves comparable performance to the DeFINE\ntransformation but with signiﬁcantly fewer parameters.\n3 D ELIGH T: DEEP AND LIGHT -WEIGHT TRANSFORMER\nA standard transformer block (Figure 1a) comprises of multi-head attention that uses a query-key-\nvalue decomposition to model relationships between sequence tokens, and a feed forward network\n(FFN) to learn wider representations. Multi-head attention obtains query Q, key K, and value V\nby applying three projections to the input, each consisting of h linear layers (or heads) that map the\ndm-dimensional input into a dh-dimensional space, where dh = dm/h is the head dimension. The\nFFN consists of two linear layers, where the ﬁrst expands the dimensions from dm to df and the\n2\nPublished as a conference paper at ICLR 2021\nKey\ndm\ndh\nQuery\ndm\ndh\nValue\ndm\ndh\nAttentionConcat\ndm\ndm\nAdd\ndm\ndf=4dm\ndm\nAdd\nMulti-head AttentionFeed Forward Network (FFN)\nAttention ops:\nO(dmn2)\nFFN params:\n8d2m\nDepth = 4\n(a) Transformer block\ndm\nNb\n(Eq. 4)\nwbmdm\nKeydo\ndo=dm2\nQuery\ndo\ndo\nValue\ndo\ndo\nAttention\ndo\ndm\nAdd\ndm\ndm/4\ndm\nAdd\nDeLighTtransformation with Single-head AttentionLight-weight FFN\nAttention ops:\nO(don2)\nFFN params:\nd2\nm\n2\nDepth = 4 +Nb\n(b) DeLighT block\nInput (dm-dimensional)\nOutput (do-dimensional)\nExpansionReduction\nNo. of layers (depth) =N\n(c) DeFINE transformation\nInput (dm-dimensional)\nOutput (do-dimensional)\nExpansionReduction\nNo. of layers (depth) =N\n(d) DeLighT transformation\nFigure 1: (a, b) Block-wise comparison between the standard transformer block of Vaswani et al. (2017) and the\nDeLighT block. In the DeLighT transformation, the number of operations in computing attention are reduced\nby half while the number of parameters (and operations) in the FFN are reduced by 16×. Transformations with\nlearnable parameters ( Linear and DeLighT ) are shown in color. The shape of linear transformations indicate\ntheir operation (expansion, reduction, etc.). (c, d) compares the DeFINE transformation (Mehta et al., 2020)\nwith the DeLighT transformation. Compared to the DeFINE transformation, the DeLighT transformation uses\ngroup linear transformations (GLTs) with more groups to learn wider representations with fewer parameters.\nDifferent colors are used to show groups in GLTs. For simplicity, feature shufﬂing is not shown in (d).\nsecond reduces the dimensions from df to dm. The depth of a transformer block is 4, consisting of\n(1) three parallel branches for queries, keys, and values, (2) a fusion layer that combines the output\nof multiple heads, and (3) two sequential linear layers in the FFN. In general, transformer-based\nnetworks sequentially stacks transformer blocks to increase network capacity and depth.\nThis paper extends the transformer architecture and introduces a deep and light-weight transformer,\nDeLighT. Our model uses a deep and light-weight expand-reduce transformation, DeLighT trans-\nformation (Section 3.1), that enables learning wider representations efﬁciently. It also enables\nreplacing multi-head attention and feed forward network (FFN) layers with single-head attention and\na light-weight FFN (Section 3.2). DeLighT transformation decouples attention dimensions from the\ndepth and width, allowing us to learn representations efﬁciently using block-wise scaling instead of\nuniform stacking of transformer blocks (Section 3.3).\n3.1 D ELIGH T TRANSFORMATION\nDeLighT transformation maps a dm dimensional input vector into a high dimensional space (ex-\npansion) and then reduces it down to a do dimensional output vector (reduction) using N layers of\nthe group transformations of Mehta et al. (2018), as shown in Figure 1d. During these expansion\nand reduction phases, DeLighT transformation uses group linear transformations (GLTs) because\nthey learn local representations by deriving the output from a speciﬁc part of the input and are more\nefﬁcient than linear transformations. To learn global representations, the DeLighT transforma-\ntion shares information between different groups in the group linear transformation using feature\nshufﬂing, analogous to channel shufﬂing in convolutional networks (Zhang et al., 2018).\nA standard approach to increase the expressivity and capacity of transformers is to increase the input\ndimensions, dm. However, increasing dm linearly also increases the number of operations in multi-\nhead attention (O(n2dm), where n is the sequence length) in a standard transformer block (Figure\n1a). In contrast, to increase the expressivity and capacity of the DeLighT block, we increase the\ndepth and width of its intermediate DeLighT transformations using expansion and reduction phases.\nThis enables us to use smaller dimensions for computing attention, requiring fewer operations.\nFormally, the DeLighT transformation is controlled by ﬁve conﬁguration parameters: (1) number\nof GLT layers N, (2) width multiplier wm, (3) input dimension dm, (4) output dimension do, and\n3\nPublished as a conference paper at ICLR 2021\nGLT\n(Groups = 1)\nFeature \nShuffle\nInput \nmixer\nGLT\n(Groups = 2)\nGLT\n(Groups = 4)\nInput \nmixer\nInput\nOutput\nFigure 2: Example illustrating the expansion phase in the DeLighT transformation that uses GLTs, feature\nshufﬂing, and an input mixer connection, to learn deeper and wider representations efﬁciently. For illustrative\npurposes, we have used the same input and output dimensions.\n(5) maximum groups gmax in a GLT. In the expansion phase, theDeLighT transformation projects\nthe dm-dimensional input to a high-dimensional space, dmax = wmdm, linearly using ⌈N\n2 ⌉layers.\nIn the reduction phase, the DeLighT transformation projects the dmax-dimensional vector to a\ndo-dimensional space using the remaining N −⌈N\n2 ⌉GLT layers. Mathematically, we deﬁne the\noutput Y at each GLT layer l as:\nYl =\n{\nF\n(\nX, Wl, bl, gl)\n, l = 1\nF\n(\nH\n(\nX, Yl−1)\n, Wl, bl, gl)\n, Otherwise (1)\nwhere Wl =\n{\nWl\n1, ··· , Wl\ngl\n}\nand bl =\n{\nbl\n1, ··· , bl\ngl\n}\nare the learnable weights and biases of\ngroup linear transformation Fwith gl groups at the l-th layer. Brieﬂy, the Ffunction takes the\ninput X\n(\nor H\n(\nX, Yl−1))\nand splits into gl non-overlapping groups such thatX =\n{\nX1, ··· , Xgl\n}\n.\nThe function Fthen linearly transforms each Xi with weights Wl\ni and bias bl\ni to produce output\nYl\ni = XiWl\ni + bl\ni. The outputs of each group Yl\ni are then concatenated to produce the output Yl.\nThe function Hﬁrst shufﬂes the output of each group in Yl−1 and then combines it with the input X\nusing the input mixer connection of Mehta et al. (2020) to avoid vanishing gradient problems. Figure\n2 visualizes the expansion phase in the DeLighT transformation with group linear transformation,\nfeature shufﬂing, and the input mixer connection.\nThe number of groups at the l-th GLT in DeLighT transformation are computed as:\ngl =\n{\nmin(2l−1, gmax), 1 ≤l ≤⌈N/2⌉\ngN−l, Otherwise (2)\nIn our experiments, we use gmax = ⌈dm\n32 ⌉so that each group has at least 32 input elements.\n3.2 D ELIGH T BLOCK\nFigure 1b shows how we integrate DeLighT transformation into the transformer block to improve\nits efﬁciency. The dm-dimensional inputs are ﬁrst fed to the DeLighT transformation to produce\ndo-dimensional outputs, where do < dm. These do-dimensional outputs are then fed into a single\nhead attention, followed by a light-weight FFN to model their relationships.\nDeLighT layer and single head attention: Let us assume we have a sequence of n input tokens,\neach of dimensionality dm. These n, dm-dimensional inputs are ﬁrst fed to the DeLighT transforma-\ntion to produce n, do-dimensional outputs, where do < dm. These n, do-dimensional outputs are then\nprojected simultaneously using three linear layers to produce do-dimensional queries Q, keys K, and\nvalues V. We then model contextual relationships between these n tokens using scaled dot-product\nattention (Eq. 3). To enable the use of residual connections (He et al., 2016), the do-dimensional\noutputs of this attention operation are linearly projected into a dm-dimensional space.\nAttention(K, Q, V) = softmax\n(QKT\n√do\n)\nV (3)\nWe hypothesize that the ability of DeLighT to learn wider representations allows us to replace\nmulti-head attention with single-head attention. The computational costs for computing attention in\n4\nPublished as a conference paper at ICLR 2021\nBlock-wiseUniform Input\nOutput\nInput\nOutput\nBblocks\nN0=Nmin\nNB−1=Nmax\n(see Eq. 4)\nNB−1=N\nN0=N\n(a) Uniform vs. block-wise\nB0 B1 B2 B3 B4 B5 B6 B7\nDecoder blocks\n400\n450\n500\n550\n600\n650\n700\nNo. of parameters \n (in thousand)\nBlock-wise\nUniform\nB0 B1 B2 B3 B4 B5 B6 B7\nDecoder blocks\n90\n100\n110\n120\n130\n140\n150\n160\nNo. of operations \n (in million)\nBlock-wise\nUniform (b) Distribution of parameters and operations within each block\nFigure 3: Block-wise scaling efﬁciently allocates parameters and operations across blocks, leading to shallower\nand narrower DeLighT blocks near the input and deeper and wider DeLighT blocks near the output. In (b),\nDeLighT networks with both uniform (N=Nmin=Nmax=8) and block-wise (Nmin=4, Nmax=8) scaling have\nabout 16.7 M parameters and perform 3.5 B operations (computed for a sequence length of n = 30), however,\nthe DeLighT network with block-wise scaling delivered 2 points better perplexity.\nthe standard transformer and the DeLighT block are O(dmn2) and O(don2) respectively, where\ndo < dm. Therefore, the DeLighT block reduces the cost for computing attention by a factor of\ndm/do. In our experiments, we used do = dm/2, thus requiring 2×fewer multiplication-addition\noperations as compared to the transformer architecture.\nLight-weight FFN: Similar to FFNs in transformers, this block also consists of two linear layers.\nSince the DeLighT block has already incorporated wider representations using the DeLighT\ntransformation, it allows us to invert the functionality of FFN layers in the transformer. The ﬁrst\nlayer reduces the dimensionality of the input from dm to dm/r while the second layer expands the\ndimensionality from dm/r to dm, where r is the reduction factor (see Figure 1b). Our light-weight\nFFN reduces the number of parameters and operations in the FFN by a factor of rdf /dm. In the\nstandard transformer, the FFN dimensions are expanded by a factor of 4.1 In our experiments, we\nused r = 4. Thus, the light-weight FFN reduces the number of parameters in the FFN by 16×.\nBlock depth: The DeLighT block stacks (1) a DeLighT transformation with N GLTs, (2) three\nparallel linear layers for key, query, and value, (3) a projection layer, and (4) two linear layers\nof a light-weight FFN. Thus, the depth of DeLighT block is N + 4. Compared to the standard\ntransformer block (depth is 4), DeLighT block is deeper.\n3.3 B LOCK -WISE SCALING\nStandard methods for improving the performance of sequence models include increasing the model\ndimensions (width scaling), stacking more blocks (depth scaling), or both. However, such scaling is\nnot very effective on small datasets. For example, when a Transformer-Base (dm = 512) network\nis replaced with Transformer-Large (dm = 1024) on the WMT’16 En-Ro corpus, the number of\nparameters increases by approximately 4×while the performance does not change appreciably\n(BLEU: 34.28 vs. 34.35). We hypothesize that this happens because scaling model width and depth\nallocates parameters uniformly across blocks, which may lead to learning redundant parameters. To\ncreate deep and wide networks, we extend model scaling to the block level (see Figure 3).\nScaling the DeLighT block: The DeLighT block learns deep and wide representations using the\nDeLighT transformation, whose depth and width are controlled by two conﬁguration parameters: the\nnumber of GLT layers N and the width multiplier wm, respectively (Figure 3a). These conﬁguration\nparameters allow us to increase the number of learnable parameters inside the DeLighT block\nindependently of the input dm and output do dimensions. Such calibration is not possible with the\nstandard transformer block because their expressiveness and capacity are a function of the input (input\ndimension = number of heads ×head dimension). Here, we introduce block-wise scaling that creates\na network with variably-sized DeLighT blocks, allocating shallower and narrowerDeLighT blocks\nnear the input and deeper and wider DeLighT blocks near the output.\nTo do so, we introduce two network-wide conﬁguration parameters: minimum Nmin and maximum\nNmax number of GLTs in a DeLighT transformation. For the b-th DeLighT block, we compute\n1Transformer-base uses dm=512 and df =2048 while Transformer-large uses dm=1024 and df =4096.\n5\nPublished as a conference paper at ICLR 2021\nthe number of GLTs Nb and the width multiplier wb\nm in a DeLighT transformation using linear\nscaling (Eq. 4). With this scaling, each DeLighT block has a different depth and width (Figure 3a).\nNb = Nmin + (Nmax −Nmin) b\nB−1 , w b\nm = wm + (Nmax −Nmin) b\nNmin(B−1) , 0 ≤b ≤B− 1 (4)\nHere, Bdenotes the number of DeLighT blocks in the network. We add superscript b to number of\nGLT layers N and width multiplier wm to indicate that these parameters are for the b-th block.\nNetwork depth: The depth of transformer block is ﬁxed, i.e., 4. Therefore, previous works (Raffel\net al., 2019; Brown et al., 2020; Wang et al., 2019) have associated the depth of transformer-based\nnetworks with the number of transformer blocks. In DeLighT, we present a different perspective to\nlearn deeper representations, wherein each block is variably-sized. To compute the network depth, we\nuse the standard deﬁnition across different domains, including computer vision (e.g., ResNet of He\net al. 2016) and theoretical machine learning (Telgarsky, 2016). These works measures network depth\nas the number of sequential learnable layers (e.g., convolution, linear, or group linear). Similarly, the\ndepth of DeLighT and transformer networks with Bblocks is ∑B−1\nb=0 (Nb + 4) and 4B, respectively.\n4 E XPERIMENTAL RESULTS\nWe evaluate the performance ofDeLighT on two standard sequence modeling tasks: (1) machine\ntranslation (Section 4.1) and (2) language modeling (Section 4.2).\n4.1 M ACHINE TRANSLATION\nDatasets and evaluation: We benchmark DeLighT models on four datasets: (1) IWSLT’14\nGerman-English (De-En), (2) WMT’16 English-Romanian (En-Ro), (3) WMT’14 English-German\n(WMT’14 En-De), and (4) WMT’14 English-French (WMT’14 En-Fr). For the IWSLT’14 De-En\ndataset, we replicate the setup of Wu et al. (2019) and Edunov et al. (2018), which uses 160K/7K/7K\nsentence pairs for training, validation, and testing with a joint BPE vocabulary of about 10K tokens,\nrespectively. For the WMT’14 English-German (En-De) dataset, we follow the setup of Vaswani et al.\n(2017). The dataset has 3.9M/39K/3K sentence pairs for training, validation, and testing respectively\nwith a joint BPE vocabulary size of 44K. 2 For the WMT’14 English-French (En-Fr) dataset, we\nreplicate the setup of Gehring et al. (2017), which uses 36M/27K/3K sentence pairs for training,\nvalidation, and testing respectively with a joint BPE vocabulary size of 44K. The performance is\nevaluated in terms of BLEU (Papineni et al., 2002) (higher is better) on the test set. We follow Wu\net al. (2019) for beam search related hyper-parameters.\nArchitecture: We follow the symmetric encoder-decoder architecture of Vaswani et al. (2017) with\nsinusoidal positional encodings. Both the encoder and the decoder haveBDeLighT blocks. Decoder\nblocks are identical to the encoder blocks (Figure 1b), except that they have an additional source-target\nsingle-head attention unit before the light-weight FFN. In the source-target single-head attention\nunit, keys and values are projections over the encoder output (full details in Appendix A). In our\nexperiments, we use wm = 2, Nmin = 4, and Nmax = 8 for WMT’16 En-Ro, WMT’14 En-De,\nand WMT’14 En-Fr; resulting in 222 layer deep DeLighT networks. For IWSLT’14 De-En, we\nused wm = 1, Nmin = 3, and Nmax = 9 for IWSLT’14 De-En; resulting in 289 layer deep network.\nFor simplicity, we set B= Nmax. We use a learnable look-up table that maps every token in the\nvocabulary to a 128-dimensional vector. We implement our models using Fairseq (Ott et al., 2019)\nand use their provided scripts for data pre-processing, training, and evaluation.\nTraining: For IWSLT’14 De-En models, we follow the setup of Wu et al. (2019) and train all our\nmodels for 50K iterations with a batch size of 4K tokens on a single NVIDIA GTX 1080 GPU. For\nWMT’16 En-Ro, we follow the training setup of Ghazvininejad et al. (2019) and train models for\n100K iterations on 16 NVIDIA Tesla V100 GPUs with an effective batch size of 64K tokens. For\nWMT’14 En-De and WMT’14 En-Fr, we follow the training set-up of Wu et al. (2019) and train our\nmodels on 16 V100 GPUs for 30K and 50K iterations, respectively. We use Adam (Kingma and Ba,\n2015) to minimize cross entropy loss with a label smoothing value of 0.1 during training. For a fair\ncomparison, we trained baseline transformer models using the same training set-up.\n2We use training and validation data that is compatible with the Tensor2Tensor library (Vaswani et al., 2018)\nin order to have fair comparisons with recent works (e.g., Evolved Transformer).\n6\nPublished as a conference paper at ICLR 2021\nIWSLT’14 De-En WMT’16 En-Ro\nModel # Params Ratio BLEU∆BLEU # Params RatioBLEU∆BLEU\nTransformer (Vaswani et al., 2017) – – 34.4† – 62 M – 34.3‡ –\nTransformer (Our impl.) 42 M 1.0× 34.3 – 62 M 1.0× 34.3 –DeLighT 14 M 0.3× 33.8 -0.5 22 M 0.35× 34.3 0.0DeLighT 30 M 0.7× 35.3 +1.0 53 M 0.85× 34.7 +0.4\n(a) Results on small corpora\nWMT’14 En-De WMT’14 En-Fr\nModel # Params Ratio BLEU∆BLEU # Params Ratio BLEU∆BLEU\nTransformer (Vaswani et al., 2017) 62 M –27.3 – – 62 M 38.1 –\nTransformer (Our impl.) 67 M 1.0× 27.7 – 67 M 1.0× 39.2 –DeLighT 37 M 0.55× 27.6 -0.1 37 M 0.55× 39.6 +0.4DeLighT 54 M 0.80× 28.0 +0.3 54 M 0.80× 40.5 +1.3\n(b) Results on large corpora\nTable 1: Comparison with baseline transformers on machine translation corpora . DeLighT models\nrequire signiﬁcantly fewer parameters to achieve similar performance. Here, †and ‡indicate the best reported\ntransformer baselines from Wu et al. (2019) and Ghazvininejad et al. (2019), respectively.\nDepth # Params # MACs BLEU\nTransformer 60 67 M 11.1 B 39.2\nDeLighT 222 37 M 5.6 B 39.6\nDeLighT 222 54 M 8.1 B 40.5\nTable 2: DeLighT networks are deep, light-\nweight and efﬁcient as compared to transformers.\nBLEU score is reported on the WMT’14 En-Fr\ndataset. To compute network depth, we count the\nnumber of sequential layers in the network (Sec-\ntion 3.3). We used 20 source and 20 target tokens\nfor computing multiplication-addition operations\n(MACs). See Appendex C for details.\n0 10 20 30 40 50 60\nParameters (in million)\n20\n22\n24\n26\n28BLEU (WMT'14 En-De)\n1.8x fewer\nparameters\n+1.4\nBLEU\nDeLighT\nTransformer\nEvolved Trans.\nFigure 4: Comparison of DeLighT with Transformers\nand Evolved Transformers at two different settings, on\nthe WMT’14 En-De corpus: (1) the number of parame-\nters is the same and (2) the performance is the same.\n4.1.1 R ESULTS\nComparison with baseline transformers: Table 1 compares the performance of DeLighT with\nthe baseline transformers of Vaswani et al. (2017) on different corpora. DeLighT delivers better\nperformance with fewer parameters than transformers, across different corpora. Speciﬁcally, on\nlow-resource (WMT’16 En-Ro) and high resource (WMT’14 En-De & WMT’14 En-Fr) corpora,\nDeLighT delivers similar or better performance with 2.8×and 1.8×fewer parameters, respectively.\nWhen the number of parameters are increased, DeLighT outperforms transformers. For example, on\nWMT’14 En-Fr dataset,DeLighT is 3.7×deeper than transformers and improves its BLEU score\nby 1.3 points yet with 13 million fewer parameters and 3 billion fewer operations (see Table 2).\nParticularly interesting are the performance comparisons of DeLighT with the baseline transformers\nof Vaswani et al. (2017) and its neural search variant, i.e., Evolved Transformer of So et al. (2019), at\ntwo different parametric settings on WMT’14 En-De corpora in Figure 4. For small models (< 10 M\nparameters), DeLighT models delivers better performance and for attaining the same performance\nas these models, DeLighT models requires fewer parameters.\nComparison with state-of-the-art methods: Most state-of-the-art methods have evaluated the\nperformance on WMT’14 En-De while some have also evaluated on IWSLT’14 De-En. Table\n3 compares the performance of DeLighT with state-of-the-art methods on these two corpora.\nDeLighT delivers similar or better performance than existing methods. It is important to note that\nexisting methods have improved baseline transformers with different design choices – for example,\nthe asymmetric encoder-decoder structure (Wang et al., 2019) and neural architecture search (So\net al., 2019). We believe that DeLighT, in the future, would also beneﬁt from such design choices.\nScaling up DeLighT models: Figure 5 shows the performance of DeLighT models improves with\nincrease in network parameters; suggesting their ability to learn representations across different\ncorpora, including low-resource.\n7\nPublished as a conference paper at ICLR 2021\nModel # Params BLEU\nTransformers (Vaswani et al., 2017) 42 M 34.3Variational Attention (Deng et al., 2018) – 33.1Dynamic convolutions (Vaswani et al., 2017) 43 M35.2Lite Transformer‡(Wu et al., 2020) – 33.6DeLighT(Ours) 30 M 35.3\n(a) IWSLT’14 De-En\nModel # Params BLEU\nTransformer (Vaswani et al., 2017) 62 M 27.3DLCL (Wang et al., 2019) 62 M 27.3Evolved Transformer†(So et al., 2019) 46 M27.7Lite Transformer‡(Wu et al., 2020) – 26.5DeLighT(Ours) 37 M 27.6\n(b) WMT’14 En-De\nTable 3: Comparison with state-of-the-art methods on machine translation corpora . DeLighT delivers\nsimilar or better performance than state-of-the-art models with fewer parameters. Here, †indicates that the\nnetwork uses neural architecture search (NAS) and ‡indicates that full network parameters are not reported.\n0 5 10 15 20 25 30\nParameters (in million)\n30.0\n31.2\n32.4\n33.6\n34.8\n36.0BLEU\n(a) IWSLT’14 De-En\n0 10 20 30 40 50 60\nParameters (in million)\n31.5\n32.2\n32.9\n33.6\n34.3\n35.0BLEU\n (b) WMT’16 En-Ro\n0 10 20 30 40 50 60\nParameters (in million)\n22.0\n23.3\n24.6\n25.9\n27.2\n28.5BLEU\n (c) WMT’14 En-De\n0 10 20 30 40 50 60\nParameters (in million)\n33.5\n35.0\n36.5\n38.0\n39.5\n41.0BLEU\n (d) WMT’14 En-Fr\nFigure 5: Scaling up DeLighT models. The performance of DeLighT improves with an increase in the\nnumber of network parameters, across different corpora, including low-resource (WMT’16 En-Ro).\n4.2 L ANGUAGE MODELING\nDatasets and evaluation: We evaluate on the WikiText-103 dataset (Merity et al., 2017) that has\n103M/217K/245K tokens for training, validation, and testing. It has a word-level vocabulary of\nabout 260K tokens. Following recent works (Baevski and Auli, 2019; Dai et al., 2019), we report\nperformance in terms of perplexity (lower is better) on the test set.\nArchitecture: We use the transformer-based decoder architecture of Baevski and Auli (2019)\nwith BDeLighT blocks. We use wm=2, Nmin=4, and Nmax=12. We scale dm using values\n{384, 512, 784, 1024}for increasing network parameters. For simplicity, we set B= Nmax. Follow-\ning standard practice, we use adaptive input (Baevski and Auli, 2019) as a look-up table and adaptive\noutput (Grave et al., 2017a) as the classiﬁcation layer with one head (head dimension is 128) and two\ntails (tail dimensions are 64 and 32). We also share weights between the input and the output layers.\nTraining: We follow the training setup of Baevski and Auli (2019), except that we train our models\non 8 NVIDIA Tesla V100 GPUs for 100K iterations with a context length of 512 and an effective\nbatch size of 64K tokens. We use Adam during training and use a context length of 480 during test.\nResults: Table 4b compares the performance of DeLighT with previous methods on WikiText-103.\nTable 4a plots the variation of perplexity with number of parameters for DeLighT and Transformer-\nXL (Dai et al., 2019) – which outperforms other transformer-based implementations (e.g., Baevski and\nAuli 2019). Both tables show thatDeLighT delivers better performance than state-of-the-art methods\n(including Transformer-XL) and it does this using a smaller context length and signiﬁcantly fewer\nparameters, suggesting that the DeLighT transformation helps learn strong contextual relationships.\n5 A NALYSIS AND DISCUSSIONS ON COMPUTATIONAL EFFICIENCY\nTraining time and memory consumption: Table 5 compares the training time and memory con-\nsumption of DeLighT with baseline transformers. For an apples-to-apples comparisons, we imple-\nmented the Transformer unit without NVIDIA’s dedicated CUDA kernel, and trained both transformer\nand DeLighT full-precision networks for 30K iterations on 16 NVIDIA V100 GPUs. The trans-\nformer and DeLighT models took about 37 and 23 hours for training and consumed about 12.5 GB\nand 14.5 GB of GPU memory, respectively (R1 vs. R2). When we enabled the dedicated CUDA\nkernel provided by APEX library3 for multi-head attention in Transformers, the training time of the\n3https://github.com/NVIDIA/apex\n8\nPublished as a conference paper at ICLR 2021\n20 40 60 80 100 120 140\nParameters (in million)\n24\n26\n28\n30\n32\n34Perplexity\nDeLighT (Ours)\nTransformer-XL\n(a) DeLighT vs. Transformer-XL\nMethod Network Context # Params PerplexityDepth Length (in million) (Test)\nLSTM (Grave et al., 2017b) – – – 48.70LSTM + Neural Cache (Grave et al., 2017b) – – – 40.80QRNN (Merity et al., 2018b) – – 151 M 33.00\nTransformer-XL (Dai et al., 2019) 64 640 151 M 24.03Transformer-XL (Our impl.)† 64 640 151 M 24.34Transformer-XL (Our impl.)† 64 480 151 M 24.91DeLighT(Ours) 158 480 99 M 24.14\n(b) Comparison with existing methods\nTable 4: Results on the WikiText-103 dataset. Compared to Transformer-XL, DeLighT delivers similar or\nbetter performance (lower perplexity) with fewer parameters. †For Transformer-XL, we reproduce results using\nthe ofﬁcial source code. For evaluating Transformer-XL with a context length of 480, we set the mem_len\nhyper-parameter to 480 in the ofﬁcial evaluation scripts.\nRow # Model # Params BLEU Training Memory(in million) (WMT’14 En-Fr) time (in GB)\nR1 Transformer (unoptimized) 67 M 39.2 37 hours 12.5 GBR2 DeLighT(unoptimized) 54 M 40.5 23 hours 14.5 GBR3 Transformer (w/ Apex optimized) 67 M 39.2 16 hours 11.9 GBR4 DeLighT(w/ optimized grouping) 54 M 40.5 19 hours 11.5 GB\nTable 5: Comparison with baseline transformers in terms of training speed and memory consumption. In R4,\nwe implemented CUDA kernels for grouping and ungrouping functions only (see Appendix E). We expect\nDeLighT to be more efﬁcient with a single and dedicated CUDA kernel for grouping, transformation, feature\nshufﬂing, and ungrouping. Memory consumption is measured on a single NVIDIA GP100 GPU (16 GB memory)\nwith a maximum of 4096 tokens per batch and without any gradient accumulation.\nModel Dropout BLEU\nTransformer (62 M) 0.10 27.3\nTransformer (62 M) 0.30 27.7\nDeLighT(37 M) 0.05 27.6\nTable 6: DeLighT requires less regularization as compared to baseline transformers (Dataset: WMT’14 En-De).\ntransformer model reduced from 37 to 16 hours while we did not observe any signiﬁcant change in\nmemory consumption. Motivated by this observation, we implemented dedicated CUDA kernels for\ngrouping and ungrouping functions in GLTs (see Appendix E). With these changes, training time and\nGPU memory consumption of DeLighT reduced by about 4 hours and 3 GB, respectively. We em-\nphasize that grouping, linear transformation, feature shufﬂing, and ungrouping, can be implemented\nefﬁciently using a single CUDA kernel. In future, we expect a dedicated CUDA kernel for these\noperations would further reduce the memory consumption as well as training/inference time.\nRegularization: Table 6 shows thatDeLighT delivers similar performance to baseline transformers,\nbut with fewer parameters and less regularization. This suggests that learning representations with\nbetter transformation functions alleviates the need for dropout.\n6 C ONCLUSION\nThis paper introduces a deep and light-weight transformer architecture, DeLighT, that efﬁciently\nallocates parameters both within the DeLighT block and across DeLighT blocks. Compared to\nstate-of-the-art transformer models, DeLighT models are (1) deep and light-weight and (2) deliver\nsimilar or better performance. In the future, we plan to apply DeLighT to other tasks, including\nlanguage model pre-training, question answering, and language generation.\nAcknowledgements: This research was supported by ONR N00014-18-1-2826, DARPA N66001-\n19-2-403, NSF (IIS-1616112, IIS1252835), and an Allen Distinguished Investigator Award. Authors\nwould also like to thank members of the UW-NLP and the H2Lab at The University of Washington\nfor their valuable feedback and comments.\n9\nPublished as a conference paper at ICLR 2021\nREFERENCES\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008, 2017.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv\npreprint arXiv:1910.10683, 2019.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), 2019.\nGeoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving\nneural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using\ndropconnect. In International conference on machine learning, pages 1058–1066, 2013.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language\nmodels. In International Conference on Learning Representations , 2018a. URL https://openreview.net/\nforum?id=SyyGPP0TZ.\nSachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, and Hannaneh Hajishirzi. Pyramidal recurrent unit\nfor language modeling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, 2018.\nXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient convolutional\nneural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 6848–6856, 2018.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl:\nAttentive language models beyond a ﬁxed-length context. In Association for Computational Linguistics, 2019.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.\narXiv preprint arXiv:1904.10509, 2019.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In International\nConference on Learning Representations, 2020.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv:2004.05150, 2020.\nAlessandro Raganato and Jörg Tiedemann. An analysis of encoder representations in transformer-based machine\ntranslation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, November 2018.\nGino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer.\nOn identiﬁability in transformers. In International Conference on Learning Representations, 2020. URL\nhttps://openreview.net/forum?id=BJg1f6EFDB.\nElena V oita, Rico Sennrich, and Ivan Titov. The bottom-up evolution of representations in the transformer: A\nstudy with machine translation and language modeling objectives. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 2019a.\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In Advances in Neural\nInformation Processing Systems, pages 14014–14024, 2019.\n10\nPublished as a conference paper at ICLR 2021\nAlessandro Raganato, Yves Scherrer, and Jörg Tiedemann. Fixed encoder self-attention patterns in transformer-\nbased machine translation. arXiv preprint arXiv:2002.10260, 2020.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking\nself-attention in transformer models. arXiv preprint arXiv:2005.00743, 2020.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention with lightweight\nand dynamic convolutions. In International Conference on Learning Representations, 2019.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short range attention.\nIn International Conference on Learning Representations, 2020.\nDavid So, Quoc Le, and Chen Liang. The evolved transformer. In Proceedings of the 36th International\nConference on Machine Learning, pages 5877–5886, 2019.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to\nsequence learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,\npages 1243–1252. JMLR. org, 2017.\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional\nnetworks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 , pages\n933–941. JMLR. org, 2017.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert:\nA lite bert for self-supervised learning of language representations. In International Conference on Learning\nRepresentations, 2020.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint\narXiv:1909.08053, 2019.\nMingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In\nKamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on\nMachine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, 2019.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. Learning deep\ntransformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 2019.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword\nunits. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), August 2016.\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In International\nConference on Learning Representations, 2019.\nÉdouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou. Efﬁcient softmax\napproximation for GPUs. In International Conference on Machine Learning, 2017a.\nSachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, and Hannaneh Hajishirzi. DeFINE: Deep\nFactorized Input Token Embeddings for Neural Sequence Modeling. InInternational Conference on Learning\nRepresentations, 2020.\nPatrick Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-Jui Hsieh. Groupreduce: Block-wise low-rank ap-\nproximation for neural language model shrinking. In Advances in Neural Information Processing Systems,\n2018.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a compact\ntask-agnostic bert for resource-limited devices. In Association for Computational Linguistics (ACL), 2020.\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. In International Conference for Representation Learning, 2016.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention:\nSpecialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, 2019b.\n11\nPublished as a conference paper at ICLR 2021\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NIPS Deep\nLearning and Representation Learning Workshop, 2015.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller,\nfaster, cheaper and lighter. In 5th Workshop on Energy Efﬁcient Machine Learning and Cognitive Computing -\nNeurIPS, 2019.\nMatus Telgarsky. Beneﬁts of depth in neural networks. COLT, 2016.\nSergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc’Aurelio Ranzato. Classical structured\nprediction losses for sequence to sequence learning. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long Papers), 2018.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion\nJones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit.\nTensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.\n07416.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of\nmachine translation. In Proceedings of the 40th annual meeting on association for computational linguistics,\npages 311–318. Association for Computational Linguistics, 2002.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael\nAuli. Fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019:\nDemonstrations, 2019.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of\nconditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 6114–6123, 2019.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference\non Learning Representations, 2015.\nYuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander Rush. Latent alignment and variational\nattention. In Advances in Neural Information Processing Systems, pages 9712–9724, 2018.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In\nInternational Conference on Learning Representations, 2017.\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous\ncache. In International Conference on Learning Representations, 2017b.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. An analysis of neural language modeling at multiple\nscales. arXiv preprint arXiv:1803.08240, 2018b.\n12\nPublished as a conference paper at ICLR 2021\nA DELI G HT ARCHITECTURES FOR LANGUAGE MODELING AND MACHINE\nTRANSLATION\nDeLighT architectures for language modeling and machine translation are shown in Figure 6. For language\nmodeling, we follow the architecture in Baevski and Auli (2019) while for machine translation, we follow the\narchitecture in Vaswani et al. (2017).\nLanguage modeling: Figure 6a shows the architecture for language modeling. The architecture stacks B\nDeLighT blocks, the conﬁguration of each block is determined using block-wise scaling. Each block has\nthree sub-layers. The ﬁrst layer is a DeLighT transformation that learns representations in high-dimensional\nspace. The second layer is a single-head attention that encodes contextual relationships. The third layer is\na position-wise light-weight feed-forward network. Similar to Vaswani et al. (2017), we employ a residual\nconnections (He et al., 2016). Similar to previous works (Baevski and Auli, 2019; Dai et al., 2019), we use tied\nadaptive input (Baevski and Auli, 2019) and adaptive softmax (Grave et al., 2017a) to map tokens to vectors and\nvectors to tokens, respectively.\nMachine translation: Figure 6b shows the architecture for machine translation. The encoder stacks B\nDeLighT blocks, the conﬁguration of each block is determined using block-wise scaling. Similar to lan-\nguage modeling, each encoder block has three sub-layers. The ﬁrst layer is a DeLighT transformation that\nlearns representations in high-dimensional space. The second layer is a single-head attention that encodes con-\ntextual relationships. The third layer is a position-wise light-weight feed-forward network. Similar to Vaswani\net al. (2017), we employ a residual connections (He et al., 2016). We use learnable look-up table to map tokens\nInputs (shifted right)\nAdaptiveInputs\nPositionalEncoding\nEmbedding\nLayer\nMasked Single-head Attention\nAdd & Norm\nLight-weightFFN\nAdd & Norm\nAdaptiveSoftmax\nLogits\nB×\nInput and output weights are tied\n(a) Language Modeling\nInputs\nLook-upTable\nPositionalEncoding\nEmbeddingLayer\nSingle-headAttention\nAdd & Norm\nLight-weightFFN\nAdd & Norm\nB×\nOutputs (shifted right)\nLook-upTable\nPositionalEncoding\nEmbeddingLayer\nMasked Single-head Attention\nAdd & Norm\nSingle-headAttention\nAdd & Norm\nLight-weightFFN\nAdd & Norm\nLinear\nSoftmax\nLogits\nB×\nInput and output weights are tied\n(b) Machine translation\nFigure 6: Sequence modeling with DeLighT. Here, green color hexagon represents the DeLighT transfor-\nmation.\n13\nPublished as a conference paper at ICLR 2021\nto vectors. Similar to the encoder, the decoder also stacks Bblocks. Decoder blocks are identical to encoder\nblocks, except that they have an additional source-target single-head attention unit before the light-weight FFN.\nKeys and values in source-target single-head attention unit are projections over the encoder output. We use\nstandard learnable look-up table to map tokens to vectors and linear classiﬁcation layer to map vectors to tokens.\nB G ROUP LINEAR TRANSFORMATION WITH INPUT -MIXER CONNECTION\nGroup linear transformation (GLT) Fsplits a dm-dimensional input X into g non-overlapping groups such that\nX = Concat(X1, ··· , Xg), where Xi is a dm\ng -dimensional vector. Xi’s are then simultaneously transformed\nusing g linear transforms Wi ∈R\ndm\ng ×do\ng to produce g outputs Yi = XiWi. Yi’s are then concatenated to\nproduce the ﬁnal do-dimensional output Y = Concat(Y1, ··· , Yg).\nFigure 7a shows an example of GLT in the expansion phase of DeLighT transformation. For illustrative\npurposes, we have used the same dimensions in this example. Recall that as we go deeper in the expansion\nphase, the number of groups increases. In this example, the ﬁrst layer has one group, the second layer has two\ngroups and the third layer has four groups. GLTs learns group-speciﬁc representations and are local. To allow\nGLT to learn global representations, we use feature shufﬂe. An example of GLT with feature shufﬂe is shown in\nFigure 7b. Furthermore, training deep neural networks by merely stacking linear or group linear (with or without\nfeature shufﬂe) is challenging because of vanishing gradient problem. Residual connections introduced by He\net al. (2016) mitigates this problem and helps train deep neural networks. However, such connections cannot be\nemployed when input and output dimensions are not the same (e.g., during the expansion and reduction phases\nin DeLighT transformation). To stabilize the training and learn deeper representations, we use input-mixer\nconnection of Mehta et al. (2020). Figure 7c shows an example of GLT with feature shufﬂe and input mixer\nconnection.\nGroups = 1Groups = 2Groups = 4\n(a) GLT\nGroups = 1Groups = 2Groups = 4\nFeature \nShuffle (b) GLT with feature shufﬂe\nGroups = 1\nGroups = 2\nGroups = 4\nFeature \nShuffle\nInput mixer\nInput mixer\n(c) GLT with feature shufﬂe and input\nmixture connection\nFigure 7: This ﬁgure visualizes different variants of group linear transformations that are used in the DeLighT\ntransformation.\nC M ULTIPLICATION -ADDITION OPERATIONS IN DELI G HT\nThe DeLighT block is built using linear transformations, GLTs, and scaled dot-product attention. Total number\nof multiplication-addition operations (MACs) in a network is an accumulation of these individual operations.\nLet n denotes the number of source tokens, m denotes the number of target tokens, dm denotes the input\ndimension, do denotes the output dimension, and g denotes the number of groups in GLT. The procedure for\ncounting MACs for each of these operations is described below.\nGroup linear transformation (GLT): GLT Fhas g learnable matrices Wi ∈ R\ndm\ng ×do\ng . There-\nfore, GLT learns dmdo\ng parameters and performs dmdo\ng MACs to transform dm-dimensional input to\n14\nPublished as a conference paper at ICLR 2021\ndo-dimensional output. Following a standard practice, e.g., ResNet of He et al. (2016), we count addition\nand multiplication as one operation instead of two because these operations can be fused in recent hardwares.\nImportantly, when g = 1, the GLT is the same as linear transformation.\nSelf-attention in DeLighT: The scaled dot-product self-attention in DeLighT is deﬁned as:\nAttention(K, Q, V) =softmax\n(QKT\n√do\n)\nV (5)\nwhere Q ∈Rn×do , K ∈Rn×do , V ∈Rn×do denotes query, key, and value, respectively.\nThe attention operation involves two dot-products. The ﬁrst dot product between Q and K while the second dot\nproduct is between the output of ﬁrst dot product and V. Both dot products require don2 MACs. Therefore,\ntotal number of MACs in computing scaled dot-product self-attention are 2don2 .\nIn case of a source-target attention (as in machine translation), K’s andV’s are from the source (encoder) and\nQ’s are incrementally decoded (one token at a time). Therefore, the number of MACs required to decode m\ntarget tokens given n source tokens are\nm∑\nk=1\n2kndo .\nD A BLATIONS ON THE WIKI TEXT-103 DATASET\nTable 7 studies the impact of DeLighT block parameters on the WikiText-103 dataset, namely (1) minimum\nnumber of GLTsNmin, (2) maximum number of GLTsNmax, (3) width multiplierwm, and (4) model dimension\ndm (see Figure 1b). Figure 8, Figure 9, and Figure 10 shows the impact of the DeLighT transformation, feature\nshufﬂing, and the light-weight FFN. Table 8 shows the effect of position of DeLighT transformation in the\nDeLighT block while Figure 12 shows the effect of scaling DeLighT networks. We choose the WikiText-103\ndataset for ablations because it has very large vocabulary compared to other datasets (267K vs. 30-40K), allowing\nus to test the ability under large vocabulary sizes. The performance is reported in terms of perplexity (lower is\nbetter) on the validation set. In our ablation studies, we used the same settings for training as in Section 4.2\nexcept that we train only for 50K iterations.\nDeLighT block: Overall, Table 7 shows that scaling depth and width using DeLighT transformation and\nblock-wise scaling improves performance. We make following observations:\na) Block-wise scaling (R4, R5) delivers better performance compared to uniform scaling (R1-R3). For instance,\nDeLighT with Nmin = 4and Nmax = 8(R4) is 1.25×shallower than DeLighT with Nmin = 8and\nNmax = 8 (R2), but delivers better performance with a similar number of parameters and operations.\nScaling wm improves performance (R2 vs. R3), however, the improvement is signiﬁcantly lower than for\nthe model with block-wise scaling (R3 vs. R5). This suggests that non-uniform distribution of parameters\nacross blocks allows the network to learn better representations.\nb) Different ratios between Nmax and Nmin yields different results. We observe signiﬁcant performance\nimprovements when the ratio is greater than or equal to two. For example, when we scale Nmax\nNmin\nfrom 2 to 3\n(R6 vs. R8), the perplexity improves by ∼5 points with only a moderate increase in network parameters. On\nthe other hand, when the Nmax\nNmin\nis close to 1 (R6 vs. R7), performance does not change appreciably. This is\nlikely because the allocation of parameters across blocks is close to uniform (Eq. 4). This is consistent with\nour previous observation.\nc) Learning shallower and narrower representations near the input and deeper and wider representations near\nthe output achieves better performance. For example, when we scaled Nmax from 8 to 12 for Nmin = 4\n(R6, R8), DeLighT delivered better performance with a similar number of parameters compared to a model\nwith Nmin = 6(R7, R9). This is likely because the ratio of Nmax and Nmin is higher when Nmin = 4,\nwhich helps allocate parameters per block more effectively.\nd) Deeper and wider representations near the input and shallower and narrower representations near the output\nhurts performance (R13 vs. R16).\ne) Scaling width using wm and dm improves performance (R10-R15), however, their impact is different. For\nexample, when we scale wm and dm by two, the rate of increase in number of parameters and operations is\nmore rapid with dm compared to wm. DeLighT’s ability to learn wider representations in different ways\nmay be useful in selecting application speciﬁc models.\nImpact of DeLighT transformation: We replace DeLighT transformation in the DeLighT block (Figure\n1b) with (1) the DeFINE transformation and (2) a stack of linear layers. Figure 8 shows that DeLighT\ntransformation delivers similar performance with signiﬁcantly fewer parameters compared to the DeFINE unit\n15\nPublished as a conference paper at ICLR 2021\nRow # Nmin Nmax wm dm Depth Parameters MACs Perplexity\nUniform vs. block-wise scaling\nR1 4 4 2 256 43 14.1 M 2.96 B 56.19\nR2 8 8 2 256 115 16.6 M 3.49 B 48.58\nR3 8 8 4 256 115 22.1 M 4.64 B 45.10\nR4 4 8 2 256 92 16.7 M 3.51 B 46.30\nR5 4 12 2 256 158 21.0 M 4.41 B 41.18\nVarying depth (NminandNmax(Eq. 4)\nR6 4 8 2 256 92 16.7 M 3.51 B 46.30\nR7 6 8 2 256 102 16.5 M 3.46 B 46.68\nR8 4 12 2 256 158 21.0 M 4.41 B 41.18\nR9 6 12 2 256 172 20.0 M 4.20 B 42.26\nVaryingDeLighTtransformation’s widthwm (Eq. 4)\nR10 4 12 2 256 158 21.0 M 4.41 B 41.18\nR11 4 12 3 256 158 23.8 M 4.99 B 39.92\nR12 4 12 4 256 158 27.1 M 5.69 B 39.10\nVarying model widthdm\nR13 4 12 2 256 158 21.0 M 4.41 B 41.18\nR14 4 12 2 384 158 29.9 M 6.28 B 35.14\nR15 4 12 2 512 158 43.8 M 9.20 B 30.81\nDeeper and wider near the Input\nR16 12 4 2 256 158 21.0 M 4.41 B 43.10\nTable 7: Ablations on different aspects of the DeLighT block, including uniform vs. block-wise scaling,\ndepth scaling, and width scaling. Rows partially highlighted in color have the same conﬁguration (repeated for\nillustrating results). Our experimental setup is similar to Section 4, except that we train our models for 50K\niterations. Multiplication and addition operations (MACs) are computed for 20 time steps.\nand linear layers. In these experiments, the settings are the same as R13-R15 (Table 7), except, Nmax = 8,\nbecause models with a stack of linear layers learn too many parameters.\nFeature shufﬂing: Figure 9 shows that feature shufﬂing improves the performance of DeLighT by 1-2\nperplexity points. Here, we use the same settings as in R13-R15 (Table 7).\nLight-weight FFN: Figure 10 shows the impact of varying the reduction factor r in the light-weight FFN.\nWe use the same settings as in R13 (Table 7). We did not observe any signiﬁcant drop in performance until\nr = 4. Beyond r = 4, we see a drop in performance (perplexity increases by ∼2 points). In such cases,\nthe inner dimensions of the light-weight FFN are very small and hurt performance. Notably, the light-weight\nFFN with r = 22 delivered the same performance as r = 2−2, but with 1.28×fewer network parameters. At\nr = 2−2, the light-weight FFN is the same as the FFN in Vaswani et al. (2017). This suggests that the ability of\n20 40 60\nParameters (in million)\n35\n40\n45Perplexity\n1.3 ×\nDeFINE\nDeLighT\nLinear\nFigure 8: Impact of different transformations.\nDeLighT transformations are more parametric\nefﬁcient than DeFINE and linear transformations.\nLower perplexity value means better performance.\n20 25 30 35 40\nParameters (in million)\n35\n40Perplexity\nw/ shuffle\nw/o shuffle\nFigure 9: Impact of feature shufﬂing. Feature\nshufﬂing allows us to learn representations from\nglobal information and improves performance.\nLower perplexity value means better performance.\n16\nPublished as a conference paper at ICLR 2021\n2 2\n 20 21 22 23\nReduction factor (r)\n40\n41\n42\n43\n44Perplexity\n26.9\n22.2\n21.4\n21.0\n20.8Parameters \n (in million)\nFigure 10: Impact of reduction factor r in light-weight FFN. The ability of DeLighT transformation to learn\nrepresentations in high-dimensional spaces efﬁciently allows us to reduce the computational burden on the FFN.\nLower perplexity value means better performance.\nBlock-wiseUniform\nInput\nOutput\nInput\nOutput\nBblocks\nN0 = Nmin\nNB−1 = Nmax\n(see Eq. 4)\nNB−1=N\nN0=N\n(a)\n128 256 384\nModel dimension (dm)\n40\n50\n60\n70\n80Perplexity\nNmin=4, Nmax=4,  Mean=4\nNmin=8, Nmax=8,  Mean=8\nNmin=4, Nmax=8,  Mean=5.6\nNmin=4, Nmax=12, Mean=7.6 (b)\nFigure 11: Uniform vs. block-wise scaling. (a) contrasts the uniform and block-wise scaling methods. (b)\ncompares the results of DeLighT with uniform and block-wise scaling methods on the WikiText-103 dataset.\nDeLighT networks with block-wise scaling delivers better performance across different settings. Lower\nperplexity value means better performance.\nDeLighT transformation to learn representations in high-dimensional spaces efﬁciently allows us to reduce the\ncomputational burden on the FFN.\nWe also tested removing the light-weight FFN and while it reduced parameters by ∼0.5-1 M, performance\ndropped by about 2-3 perplexity points across different parametric settings.\nUniform vs. block-wise scaling: Figure 11 compares the performance of DeLighT with uniform and block-\nwise scaling. For a given model dimension dm, DeLighT models with block-wise scaling delivers better\nperformance.\nPosition of DeLighT transformation: We studied three conﬁgurations for the DeLighT transforma-\ntion on the WikiText-103 validation set (Table 8): (1) DeLighT transformation followed by single-headed\nattention and light-weight FFN, (2) single-headed attention followed by DeLighT transformation, and (3)\nsingle-headed attention followed by DeLighT transformation and light-weight FFN. For similar number of\nparameters, we found that (2) and (3) drops the performance of (1) signiﬁcantly across different parametric\nsettings. This suggests that deeper and wider representations helps learn better contextual representations;\nallowing us to replace multi-headed attention with single-headed attention.\nScaling up DeLighT: Figure 12 shows the results of DeLighT models obtained after varying conﬁguration\nparameters of DeLighT transformations (Nmin={4, 6}, Nmax={8, 12}, wm={2, 3, 4}, and dm={256, 384,\n512}). We can see that scaling one conﬁguration parameter (e.g., dm) while keeping other conﬁguration\nparameters constant (e.g., Nmin, Nmax, and wm) consistently improves performance.\n17\nPublished as a conference paper at ICLR 2021\nConﬁguration Parameters Perplexity\nDeLighTtransformation + Single-head attention + Light-weight FFN 31 M 34.20\nSingle-head attention +DeLighTtransformation 30 M 39.02\nSingle-head attention +DeLighTtransformation + Light-weight FFN 31 M 39.43\nDeLighTtransformation + Single-head attention + Light-weight FFN 99 M 23.16\nSingle-head attention +DeLighTtransformation 96 M 28.33\nSingle-head attention +DeLighTtransformation + Light-weight FFN 99 M 27.94\nTable 8: Effect of the position of DeLighT transformation. Lower value of perplexity means better perfor-\nmance.\n128 256 384 512\nModel dimension (dm)\n30\n40\n50\n60\n70Perplexity\n13\n17\n23\n31\n14\n18\n26\n36\n14\n20\n29\n43\nwm = 2\nwm = 3\nwm = 4\n(a) Nmin=4, Nmax=8\n128 256 384 512\nModel dimension (dm)\n30\n40\n50\n60\n70Perplexity\n13\n16\n22\n30\n14\n18\n25\n35\n14\n20\n28\n41\nwm = 2\nwm = 3\nwm = 4 (b) Nmin=6, Nmax=8\n128 256 384 512\nModel dimension (dm)\n30\n40\n50\n60\n70Perplexity\n15\n21\n30\n44\n16\n24\n35\n52\n17\n27\n41\n63\nwm = 2\nwm = 3\nwm = 4\n(c) Nmin=4, Nmax=12\n128 256 384 512\nModel dimension (dm)\n30\n40\n50\n60\n70Perplexity\n15\n20\n28\n41\n16\n23\n32\n48\n17\n26\n38\n58\nwm = 2\nwm = 3\nwm = 4 (d) Nmin=6, Nmax=12\nFigure 12: Scaling up DeLighT. Scaling one conﬁguration parameter (e.g., dm) while keeping other conﬁgu-\nration parameters constant (e.g., Nmin, Nmax, and wm) consistently improves performance. The numbers on\ntop of each bar represents network parameters (in million). Lower value of perplexity means better performance.\nThis work investigates relationships between Nmin, Nmax, wm, and dm, manually. We believe that a more\nprincipled approach, such as compound scaling of Tan and Le (2019), that establishes relationships between\nthese parameters would produce more efﬁcient and accurate models.\nE S OURCE CODE FOR GROUP LINEAR TRANSFORMATION\nThe source code for implementing group linear transformation (GLT) in PyTorch is shown in Listing 1. The\nsource code for efﬁciently implementing the grouping function in GLT is shown in Listing 2. Since the\nungrouping kernel is similar to grouping kernel, we have not shown it here.\nThe reshape and transpose operations in naive PyTorch implementation for grouping and ungrouping are replaced\nwith a dedicated CUDA kernels, resulting in reduced memory footprint and faster training.\n18\nPublished as a conference paper at ICLR 2021\nListing 1: \"Naive implementation of GLT in Pytorch\"\nimport torch\ndef glt_function(x, n_groups, weights, bias=None):\n’’’\n:param x: Input tensor of size [B x N], where B is batch size and N\nis input dimension\n:param n_groups: number of groups in GLT\n:param weights: glt weights [g x N/g x M/g]\n:param bias: GLT bias (optional) of size [g x 1 x M/g]\n:return: output tensor of size [B x M]\n’’’\nbsz = x.size(0)\n## GROUPING FUNCTION: Converts [B x N] tensor to [g x B x N/g] ##\n# [B x N] --> [B x g x N/g]\nx = x.contiguous().view(bsz, n_groups, -1)\n# [B x g x N/g] --> [g x B x N/g]\nx = x.transpose(0, 1) # transpose so that group is first\n## TRANSFORMATION FUNCTION: Transforms from N/g-dimensional space to\nM/g-dimensional space ##\n# [g x B x N/g] x [g x N/g x M/g] --> [g x B x M/g]\nx = torch.bmm(x, weights) # multiply with Weights\n# add bias\nif bias is not None:\nx = torch.add(x, bias)\n## REGROUPING FUNCTION: Converts [g x B x M/g] tensor to [B x M] ##\n# [g x B x M/g] --> [B x g x M/g]\nx = x.transpose(0, 1) # transpose so that batch is first\n# [B x g x M/g] --> [B x M]\nx = x.contiguous().view(bsz, -1)\nreturn x\nListing 2: \"Grouping kernel in CUDA\"\n/* Grouping Kernel: Transforms input from [B x N] to [g x B x N/g] */\ntemplate<typename scalar_t>\n__global__ void grouping_kernel_forward(const scalar_t* input,\nconst int groups, const int total_elements,\nconst int input_features, const int group_features,\nconst int batch_size, scalar_t* output){\nconst int index = IMUL(blockIdx.x, blockDim.x) + threadIdx.x;\nif (index >= total_elements){\nreturn;\n}\nconst int b_idx = index / group_features;\nconst int g_f_idx = (index % group_features);\nint in_offset, out_offset;\n#pragma unroll\nfor(int g=0; g < groups; g++){\nin_offset = (b_idx * input_features) + (g * group_features) +\ng_f_idx;\nout_offset = ((g * batch_size + b_idx) * group_features) + g_f_idx;\noutput[out_offset] = input[in_offset];\n}\n}\n19",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7789182662963867
    },
    {
      "name": "Computer science",
      "score": 0.5290992259979248
    },
    {
      "name": "Scaling",
      "score": 0.465526282787323
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41851192712783813
    },
    {
      "name": "Engineering",
      "score": 0.2624451518058777
    },
    {
      "name": "Mathematics",
      "score": 0.2213534414768219
    },
    {
      "name": "Electrical engineering",
      "score": 0.20276394486427307
    },
    {
      "name": "Voltage",
      "score": 0.12426519393920898
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}