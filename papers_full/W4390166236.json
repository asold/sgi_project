{
  "title": "One LLM is not Enough: Harnessing the Power of Ensemble Learning for Medical Question Answering",
  "url": "https://openalex.org/W4390166236",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2095758230",
      "name": "Han, Yang",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A2129494870",
      "name": "Mingchen Li",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A4294977678",
      "name": "Huixue Zhou",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A2165913975",
      "name": "Yongkang Xiao",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A1601884019",
      "name": "Qian, Fang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A2095758230",
      "name": "Han, Yang",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A2129494870",
      "name": "Mingchen Li",
      "affiliations": [
        "University of Minnesota",
        "Minnesota Department of Health"
      ]
    },
    {
      "id": "https://openalex.org/A4294977678",
      "name": "Huixue Zhou",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A2165913975",
      "name": "Yongkang Xiao",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A1601884019",
      "name": "Qian, Fang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": [
        "Minnesota Department of Health",
        "University of Minnesota"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2054070929",
    "https://openalex.org/W2106121453",
    "https://openalex.org/W3198080531",
    "https://openalex.org/W3171530662",
    "https://openalex.org/W3105892552",
    "https://openalex.org/W3174995573",
    "https://openalex.org/W4388605937",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3205235328",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4220989182",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2970602317",
    "https://openalex.org/W3149839747",
    "https://openalex.org/W2755012395",
    "https://openalex.org/W2084362125",
    "https://openalex.org/W2016159616",
    "https://openalex.org/W4385571189",
    "https://openalex.org/W4321207578",
    "https://openalex.org/W4388687124",
    "https://openalex.org/W2135850590",
    "https://openalex.org/W2171809276",
    "https://openalex.org/W2989868186",
    "https://openalex.org/W1990112355",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W2799228184",
    "https://openalex.org/W4394782456"
  ],
  "abstract": "ABSTRACT Objective To enhance the accuracy and reliability of diverse medical question-answering (QA) tasks and investigate efficient approaches deploying the Large Language Models (LLM) technologies, We developed a novel ensemble learning pipeline by utilizing state-of-the-art LLMs, focusing on improving performance on diverse medical QA datasets. Materials and Methods Our study employs three medical QA datasets: PubMedQA, MedQA-USMLE, and MedMCQA, each presenting unique challenges in biomedical question-answering. The proposed LLM-Synergy framework, focusing exclusively on zero-shot cases using LLMs, incorporates two primary ensemble methods. The first is a Boosting-based weighted majority vote ensemble, where decision-making is expedited and refined by assigning variable weights to different LLMs through a boosting algorithm. The second method is Cluster-based Dynamic Model Selection, which dynamically selects the most suitable LLM votes for each query, based on the characteristics of question contexts, using a clustering approach. Results The Majority Weighted Vote and Dynamic Model Selection methods demonstrate superior performance compared to individual LLMs across three medical QA datasets. Specifically, the accuracies are 35.84%, 96.21%, and 37.26% for MedMCQA, PubMedQA, and MedQA-USMLE, respectively, with the Majority Weighted Vote. Correspondingly, the Dynamic Model Selection yields slightly higher accuracies of 38.01%, 96.36%, and 38.13%. Conclusion The LLM-Synergy framework with two ensemble methods, represents a significant advancement in leveraging LLMs for medical QA tasks and provides an innovative way of efficiently utilizing the development with LLM Technologies, customing for both existing and potentially future challenge tasks in biomedical and health informatics research.",
  "full_text": "One LLM is not Enough : Harnessing the Power of Ensemble Learning for \nMedical Question Answering \nHan Yang, MSE1, Mingchen Li, MS2, Huixue Zhou, BMed1, Yongkang Xiao, MS1, Qian Fang, \nMS3, Rui Zhang, PhD2* \n1Institute for Health Informatics, University of Minnesota, Minneapolis, Minnesota, USA, \n2Division of Computational Health Sciences, Department of Surgery, University of Minnesota, \nMinneapolis, MN, USA, \n3H. Milton Stewart School of Industrial & Systems Engineering, Georgia Institute of \nTechnology, Atlanta, GA, USA \n \nCorresponding author: \n*Dr. Rui Zhang, PhD \nDivision of Computational Health Sciences, Department of Surgery \nUniversity of Minnesota \n308 Harvard Street SE, Minneapolis, MN 55455 \nEmail: zhan1386@umn.edu \nKeywords: \nLarge Language Models, Ensemble Learning, Medical Question Answering, Healthcare AI \nWord count: 3958.   \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nABSTRACT \nObjective: To enhance the accuracy and reliability of diverse medical question -answering (QA) \ntasks and investigate efficient approaches deploying the Large Language Models (LLM) \ntechnologies,  We developed a novel ensemble learning pipeline by utilizing state -of-the-art \nLLMs, focusing on improving performance on diverse medical QA datasets. \nMaterials and Methods: Our study employs three medical QA datasets: PubMedQA, MedQA -\nUSMLE, and MedMCQA, each presenting unique challenges in biomedical question -answering. \nThe proposed LLM -Synergy framework, focusing exclusively on zero -shot cases using LLMs, \nincorporates two primary ensemble methods. The first is a Boosting-based weighted majority vote \nensemble, where decision -making is expedited and refined by assigning variable weights to \ndifferent LLMs through a boosting algorithm. The second method is Cluster-based Dynamic \nModel Selection, which dynamically selects the most suitable LLM votes for each query, based on \nthe characteristics of question contexts, using a clustering approach. \nResults: The Majority Weighted Vote and Dynamic Model Selection methods demonstrate \nsuperior performance compared to individual LLMs across three medical QA datasets. \nSpecifically, the accuracies are 35.84%, 96.21%, and 37.26% for MedMCQA, PubMedQA, and \nMedQA-USMLE, respectively, with the Majority Weighted Vote. Correspondingly, the Dynamic \nModel Selection yields slightly higher accuracies of 38.01%, 96.36%, and 38.13%. \nConclusion: The LLM-Synergy framework with two ensemble methods, represents a significant \nadvancement in leveraging LLMs for medical QA tasks and provides an innovative way of \nefficiently utilizing the development with LLM Technologies, customing for both existing an d \npotentially future challenge tasks in biomedical and health informatics research. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nINTRODUCTION \nQuestion Answering (QA) tasks in the medical domain involve a complex process of accurately \ninterpreting and responding to healthcare -related queries[1]. QA tasks typically encompass two \nformats: open-ended and structured. In open-ended QA, respondents provide a complete sentence \nincorporating essential information in response to a question. In structured QA, the question is \npresented with several options, and the respondent selects the correct option or options by its \ncorresponding identifier. Medical QA systems are designed to provide reliable and precise answers \nto questions ranging from disease symptoms and treatment options to medical research findings. \nThese systems leverage advanced technologies like natural language processing (NLP) and \nmachine learn ing approaches to understand and process medical terminology and concepts, \nmaking them invaluable tools for healthcare professionals and patients seeking medical \ninformation. The effectiveness of these systems is crucial, as they directly impact healthcare  \ndecision-making and patient care[2-5].  \nPreviously, transformer models like BERT (Bidirectional Encoder Representations from \nTransformers) played a pivotal role in QA. For instance, He, Yun, et al.[6] infused disease \nknowledge into a basket of BERT-based models for health question answering, demonstrating the \nviability of disease knowledge infusion in NLP models. And Alzubi, Jafar A., et al.[7] developed \nanother BERT-based model named CoBERT specifically designed for QA related COVID-19. The \nexpansion of medical corpora and textual resources has necessitated leveraging these large datasets \nmore effectively. This need has been met by the emergence of Large Language Models (LLMs) as \na transformative approach to medical QA tasks. Pretrained on extensive and diverse datasets, \nLLMs like GPT -4 possess a  deep understanding of language nuances and medical terminology, \nenabling them to generate highly accurate and relevant responses to medical queries[8]. They \nrepresent a significant milestone while dealing with natural language processing (NLP) tasks[9], \nsuch as text generation[10], question answering (QA)[11], Named Entity Recognition (NER)[12], \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \netc. Moreover, LLMs include large -sized models like GPT -4 and Llama -2[13], as well as some \nrelatively small yet efficient LLMs like Vicuna[14], and Stanford Alapaca. These LLMs, \ncharacterized by their vast size, have demonstrated remarkable capabilities in  understanding and \ngenerating human language across a diverse array of domains[15]. \nCurrently, the development and application of LLMs face distinct challenges based on their \naccessibility and capabilities[16]. On the one hand, closed-source models, such as GPT-3.5, GPT-\n4, and PaLM2[17], often maintained by large corporations, demonstrate advanced capabilities yet \nlack public accessibility. This limitation restricts their use in broader research and application \ncontexts, including the situation of medical QA, especially when it comes to customized \nenhancements or applying potentially powerful techniques like fine-tuning[18]. On the other hand, \nopen-source LLMs, like Open Llama2[19], Vicuna[14], and Alpaca[20], grant accessibility and \ntransparency, as well as fine -tuning methods that are capable of making significant \nimprovements[18]. A nota ble LLM instance in the medical QA context is PMC -LLama[20], \nexemplifying how fine-tuning an open-source LLM can lead to considerable improvements in its \napplication, such as medical QA. However, this raises another significant challenge, which is the \nhigh cost associated with training and fine -tuning these models. The computational resources \nrequired for training LLMs with billions of parameters are substantial, making it impractical for \nindividual researchers or small organizations to train these models f rom scratch[21]. In the \nhealthcare and biomedical domain, the challenge is further compounded by the need for domain -\nspecific knowledge[22 -24]. Furthermore, selecting a suitable LLM for a specific biomedical \napplication is not straightforward[25]. The choi ce of a model often depends on various factors, \nincluding the nature of the medical queries, the required level of accuracy, and the availability of \ndomain-specific training data[10, 23, 26-27]. \nEnsemble method is a meta -approach stemming from machine learning techniques. In machine \nlearning, ensemble involves combining multiple models to improve the overall performance, \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nrobustness, and reliability of predictions[28 -29]. Techniques including voting[30 -31], \nboosting[33], and bagging (bootstrap aggregating) [28 -29, 32 -33] are common ensembling \napproaches with better predictive performance by combining the predictions with multiple models \nor multiple past predictions. In the context of LLMs, model ensembles can potentially harness the \ndiverse strengths of individual models to achieve superior results[31, 34-35]. These methods work \neffectively because they mitigate the weaknesses of single-model approaches. Each LLM, with its \nunique training data and architecture, may have specific strengths and biases[36-38]. Ensembling \nvarious models, each with distinct origins and structures, can equilibrate their inherent biases, \nmitigate overfitting, and enhance the generalization capacity for new data[39]. Importantly, this \nensembling approach does not necessitate the models to be exclusively open -source or closed -\nsource, nor does it demand extensive computational resources for its executio n. Moreover, \nensembling enables leveraging both domain-specific and general-purpose models. For instance, a \ngeneral-purpose LLM might excel in understanding the context and semantics of a question, while \na domain -specific LLM might provide more accurate te chnical information[20, 27, 40]. \nCombining these models can yield comprehensive and contextually relevant responses, crucial in \nmedical scenarios.  \nCertain challenges lie in how to effectively ensemble LLMs, especially within the medical domain. \nThis integration must consider factors such as the compatibility of different models, the method of \naggregating their outputs and maintaining the interpretabi lity of the responses. These \nconsiderations are crucial for ensuring that the ensemble not only performs well but also aligns \nwith the stringent requirements of medical applications. There have been only a few studies diving \ninto Ensembling LLMs to achieve  a better prediction, like LLM -Blender[34], which implements \nPairRanker and Genfuser as an ensemble framework to generate consistently better responses for \na given input. Similarly, the majority voting method proposed by Pitis, Silviu et al.[35] \ndemonstrates potential. However, these studies primarily focus on open -ended tasks and do not \ndelve into the specifics of medical QA, nor do they include domain -specific LLMs like \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nPMCLlama2[21] or Medalpaca[28]. Furthermore, their applicability to structured QA with single-\nchoice or multiple-choice questions in the medical domain remains unexplored, presenting unique \nchallenges related to specificity, privacy, and data scarcity in medical contexts. This gap indicates \nthe need for further research on the effective ensembling of LLMs tailored to the specific \nrequirements of medical QA. \nTo address these limitations, we introduce LLM-Synergy, a novel ensembling framework tailored \nfor medical QA, with two well -designed meta -learning ensembling methods, providing two \ninnovative approaches combining the strengths of various LLMs, named Boosti ng-based \nWeighted Majority Vote Ensemble[35, 41 -42] and Cluster-based Dynamic Model selection[43]. \nTo validate the efficacy of LLM -Synergy and its ensemble methods, we conducted a case study \nusing three medical QA datasets: MedMCQA[44], PubMedQA[45], and MedQA-USMLE[46].  \nOur contributions to this study include the following: \n1) The development of innovative LLM ensemble methods, specifically the Boosting -based \nWeighted Majority Vote Ensemble and Cluster -based Dynamic Model Selection, offers \nnew approaches, with zero-shot cases, in the medical QA field. \n2) We implemented the ensembling methods for LLM methods and improved the \nperformance by 5.98%, 1.09%, and 0.87% compared to the best-performing LLM on three \nmedical QA datasets,  the effectiveness of our ensemble methods. In each case, a tailored \nensemble framework was created and adapted to the format of the QA dataset (single -\nchoice or multiple-choice formats). \n3) We conducted an error analysis to provide insights and directions for potential future \nenhancements in the field of medical QA, laying the groundwork for further improvements \nin this domain.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nMETHODS \nOverview \nThe first step of our methods is investigating results with LLM -Synergy involves benchmarking \nleading LLMs including GPT -3.5-turbo, GPT -4, Llama2 -13B, Vicuna -13B, Medalpaca -13B, \nPMC-Llama-13B, and a random guessing result as a reference. Within the benchmark, we conduct \na sampled test, randomly drawing 200 QA pairs from the three medical QA datasets to assess the \ncurrent capabilities of these LLMs in a medical context as a starting point. This benchmarking \nserves as a foundational analysis to understand the  individual strengths and limitations of each \nLLM in handling medical QA tasks. Illustrated by Figure 1 overviewing of the whole pipeline of \nLLM-Synergy, following the benchmark assessment, the next phase focuses on the training \nprocess of our two proposed  ensembling methods within LLM -Synergy: the Boosting -based \nWeighted Majority Vote Ensemble and the Cluster -based Dynamic Model Selection. The two \napproaches are designed to combine the unique capabilities of the selected LLMs, aiming to \nenhance the overall performance of medical QA systems. Moreover, the second method could be \nre-graded as an extensive version of the first one. By implementing these methods, we seek to \naddress the shortcomings of relying on single models and reduce the need for extensive individual \nmodel training, thereby creating a more robust and efficient solution for medical QA. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nFigure 1. Overview of our LLM-Synergy Framework \nDataset \nWe used three medical QA datasets for our model training and test: MedMCQA[44], \nPubMedQA[45], and MedQA-USMLE[46]. \nMedMCQA, released in March 2022 by Pal, Umapathi et al.[41], is a comprehensive multiple -\nchoice question dataset derived from mock and past examinations of AIIMS and NEET -PG (Pal, \nUmapthi, et al. 2022), two prominent Indian medical entrance exams. It encom passes a training \nset with 182,822 questions and a test set comprising 4,183 questions, covering over 2,400 topics. \nEach question in this dataset presents four answer choices, labeled from A to D. \nPubMedQA, introduced in September 2019 by Jin, Dhingra et al.[42], is a QA dataset curated from \nPubMed abstracts. It includes 1,000 questions reviewed by experts and 272,500 algorithmically \ngenerated QA pairs. The dataset's primary task is to classify rese arch questions into yes, no, or \nmaybe answers, akin to multiple-choice questions. It is divided into three segments: PQA -L with \n1k manually labeled pairs, PQA -U with 61.2k unlabeled pairs, and PQA -A featuring 211.3k \nartificially generated pairs. Here, we only implement a QA process without reasoning, which does \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nnot require a corresponding explanation of how the final answer is generated, which may lead to a \nrelatively high accuracy score. \nMedQA-USMLE, launched in September 2020 by Jin, Pan et al.[43], is an innovative dataset of \nmultiple-choice questions tailored to the United States Medical Licensing Exams. This dataset \nencompasses questions in three languages: English, Simplified Chinese, and Traditional Chinese, \nwith a total of 12,724, 34,251, and 14,123 questions in each respective language. Each question \noffers five choices, ranging from option A to E, sourced from professional medical board \nexaminations. Here, we only experimented with the English QA parts. \nThe detailed LLM prompt can be found in the Appendix. \nLLM Benchmark on the three medical QA datasets \nAhead of implementing our ensembling framework, as a benchmark study, we evaluate the \nperformance of various LLMs on 200 questions from each of the three QA datasets, respectively: \nPubMedQA, MedQA-USMLE, and MedMCQA, adhering to their specific answer formats. We \nevaluate our model's performance against several robust baselines relying on the Language \nModel (LLM). Table 1 summarizes the six LLMs and one random guess predictor, along with \nthe model characteristics including how many parameters within each LLM and a comprehensive \ndescription.  \nTable 1: The summarization of 7 LLMs running QA models \nQA Predictors Model parameters Description \nGPT-4 1.76 trillion  \nGPT-4 is a substantial multimodal model designed \nto respond to questions by providing instructions \nfed to the GPT-4. \nGPT-3.5-turbo 20 billion Same designed as GPT-4, however, GPT-3.5-turbo  \nhas fewer parameters than GPT-4. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nLlama2-13B 20 billion \nLlama 2-13B is part of the Llama 2 series, \nrepresenting a pretrained generative model. Tuned \nversions of Llama 2 utilize supervised fine-tuning \n(SFT) and reinforcement learning with human \nfeedback (RLHF) to help generate the answers to \ngiven questions. \nVicuna-13B 13 billion \nVicuna-13, similar in size to Llama2-13b, Vicuna is \nnoted for its robustness and adaptability across \ndifferent types of language processing tasks. \nMedalpaca-13B 13 billion \nMedalpaca-13B is a substantial language model \nfinely tuned for tasks in the medical domain. It \nstems from Llama (Large Language Model Meta \nAI) and boasts a significant parameter count of 13 \nbillion. \nMedLLama-13B 13 billion \nMedLLama-13B is initialized from LLaMA-13B \nand undergoes additional pretraining using a \nconstructed medical corpus derived from 4.8M \nPubmedCentral papers and Medical Books. \nPMC-LLama-\n13B 13 billion \nPMC-LLama-13B is the further tuned version of \nMedLLama-13B, with the pretrain and instruction-\ntuning methods. \nRandom Guess 1 \nA random guess predictor simply generates a \nrandom answer by the equal probability of each \noption, serving as a reference to compare the LLM-\ncased predictor.  \n \nFigure 2 visualizes the performance of the selected predictor answering the three medical QA \ndatasets. The benchmark graph illustrates that each predictor exhibits performance levels that \nexceed random guessing across various medical QA tasks, signaling the inherent capability of the \nLLMs to understand and process medical queries. Notably, different LLMs demonstrate particular \nstrengths depending on the dataset; for instance, GPT -4 shows a marked proficiency in the \nMedMCQA tasks, while PMC-Llama-13B stands out in the PubMedQA context. These variations \nin model performance across tasks provide a solid foundation for the potential enhancement of \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \naccuracy through our subsequent ensemble work, suggesting that strategic combinations of these \nmodels could capitalize on their respective strengths and mitigate their individual weaknesses. \n \nFigure 2. QA shows the accuracy of how each LLM performs on the three medical QA datasets \nPart one: Majority Weighted Vote training based on the boosting method \n \nFigure 3. Training Process of Boosting-based Weighted Majority Vote Ensemble \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nFigure 3 shows the training process of Boosting-based Weighted Majority Vote Ensemble. \nStep 1: Dataset Preparation \nFor a given medical QA dataset, we split the dataset into a training set and a validation set, with a \nproportion of 80% and 20%. Each QA-pair instance in the dataset consists of a question and single-\nchoice options. \nStep 2: Weight Initialization and Initial Model Training \nWe assigned initial weights to all LLMs with weights (ùë§!, ùë§\", . . . ) and initialized the starting status \nof ensembled LLMs with these weights. There may be different strategies for initialization, and \nwe chose equally weighted initialization, which is the most common way of initialization[47]. \nStep 3: Prediction and Weight Adjustment \nWe chose a baseline LLM, denote as ùêøùêøùëÄ!\there, and focus on its wrong prediction: Use  ùêøùêøùëÄ! to \npredict answers for the training set and adjust the weights based on the prediction of instance: If \nthe prediction, ùë¶!#* for instance ùëñ, is incorrect, minus the weight of its corresponding LLM, ùêøùêøùëÄ!, \nby an adjustment parameter ùõº, i.e. ùë§$ ‚Üê ùë§$ + ùõº, where ùõº is a factor that increases the weight, \nindicating that the instance needs more attention in the next round of training. \nStep 4: Iterative Training with Additional Models \nFor each subsequent LLM ( ùêøùêøùëÄ\", ùêøùêøùëÄ%, . ..), repeat the prediction process. If a model correctly \npredicts the answer, we maintain the current weights. If the prediction ùë¶&#*   is incorrect, the weights \nwere adjusted again. \nStep 5: Finalize Weights of the Model Ensemble \nAfter training and weight adjustments across all LLMs are completed, the final ensemble model is \nformed, which combines the individual LLMs with the final set of adjusted weights. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nPart two: Dynamic Model Selection \n \nFigure 4. Training Process of Cluster-based Dynamic Model Selection Ensemble \nFigure 4 shows the training process of our second approach: Cluster -based Dynamic Model \nSelection, which serves as an extensive approach to the first one. \nStep 1: Dataset Preparation \nSame as Part 1, for a given medical Question -Answering (QA) dataset, split it into a training set \nand a validation set with a ratio of 80:20. Each QA-pair instance in the dataset should consist of a \nquestion paired with single-choice options. \nStep 2: Question Context Tokenization \nExtract the context of each question from the QA pairs, and use the tokenizer, from Clinical-BERT \nin our case,  to convert each question context into a series of tokens. \nStep 3: Encoding and Pooling \nEmploy an encoder (same model as step 2, Clinical-BERT in our case) along with a mean-pooling \nmethod to encode each question's tokens into a 768x1 embedding vector. Through this encoder \npart, we obtained a constructed embedding matrix for the entire set of  questions, where each row \nrepresents the embedded vector of a question. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nStep 4: Clustering \nApply a clustering model, KMeans in our experiment specifically, to the embedding matrix. This \nstep assigns each question a cluster label, effectively reducing the dimensionality from 768 of an \nindividual question vector to a single cluster group label bei ng 1,2,3, ‚Ä¶ K. It serves as an \nunsupervised learning step that categorizes questions into different clusters based on their \nembeddings achieved after Step 3. \nStep 5: Training Within Clusters \nFor each cluster group identified in the previous step, implement the Majority Weighted Vote \ntrainer in Part 1, then determine the best combination of weights for the ensemble of LLMs within \neach group. These weights are adjusted to maximize the prediction  accuracy of the ensemble \nwithin the specific context of each cluster. \nStep 6: Validation and Tuning \nIteratively apply steps 2 to 5 to the validation set as well, and adjust and tune the hyperparameters \nof the clustering algorithm(K in KMeans in our case) to achieve the best overall accuracy on the \nvalidation set. This step is crucial for optimizing the m odel's performance and ensuring that the \nclustering effectively captures the nuances of different question contexts. \nEvaluation \nIn the evaluation phase of our study, same as others[21, 36, 44, 46], accuracy was employed as the \nprimary metric for assessment, given its congruence with the Micro F1 score in our specific \nexperimental context of single-choice or multiple-choice QA tasks. Given the nature of multiple-\nchoice QA datasets, metrics such as Recall and Precision are deemed inappropriate as they are \nsensitive to changes in the option numbering. For instance, adjusting the order of option labels \nmay result in altering these metri c values, when the number of options exceeds two. Based on \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nthese considerations, accuracy emerges as a more suitable evaluation metric in our case, providing \na robust measure of overall correctness without being affected by variations in option numbering, \nwhich ensures a consistent and meaningful assessment of mod el performance in the specific \ncontext of our study. \nThe test set is distinguished from the training set used for model development and the validation \nset for hyperparameter optimization that we used within the ensemble training process. This \ndistinct separation ensures an unbiased evaluation of the model's true predictive capabilities. The \ntest sets are derived from the subsets of the MedMCQA, PubMedQA, and MedQA -USMLE \ndatasets, respectively. The MedMCQA test dataset consists of 4,183 QA pairs. The PubMedQA \ntest dataset is even more extensive with 11,269 QA pairs. Whereas, the MedQA-USMLE dataset \ncontains 1,272 QA pairs. \n \nRESULTS \nIn the presented study, the performance of various Large Language Models (LLMs) was evaluated \nagainst ensemble methods across three distinct medical QA datasets, MedMCQA, PubmedQA, \nand MedQA-USMLE. The performance metric, assumed to be an accuracy score, h ighlights the \ndifferential capabilities of each LLM and our two established predictors of ensemble methods. The \ndetailed result can be seen in Table 2. \nTable 2. Test set performance of each individual LLM and our ensemble approach \nLLMs MedMCQA PubmedQA MedQA-USMLE \nLlama2-13B 32.03 93.09 24.61 \nMedllama-13B 31.03 86.11 23.58 \nMedalpaca-13B 27.97 95.27 37.26 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nVicuna-13B 26.13 93.15 24.14 \nMajority Weighted Vote 35.84 96.21 37.26 \nDynamic Model Selection  38.01 96.36 38.13 \n \nFor individual models, Llama2 -13B demonstrated substantial proficiency in the PubmedQA  \ndataset with an accuracy of 93.09%, indicating a strong alignment with the dataset's characteristics. \nConversely, its performance on the MedQA -USMLE dataset was considerably lower at 24.61%, \nsuggesting a potential misalignment with this dataset's attribut es or a limitation in handling its \ncomplexity. Medllama-13B showed a similar trend, albeit with marginally lower accuracy figures \nacross the board, peaking at 86.11% for the PubmedQA dataset. \nMedalpaca-13B yielded a divergent performance profile, exhibiting a relatively lower accuracy of \n27.97% on PubmedQA, while achieving the highest accuracy among individual LLMs on the \nMedQA-USMLE dataset at 37.26%. This suggests that Medalpaca -13B may posse ss particular \nstrengths in processing the content typified within the MedQA-USMLE exam questions. Vicuna-\n13B, on the other hand, had the lowest accuracy scores for all datasets, which could indicate a \ngeneral difficulty with the medical QA task as presented in these datasets. \nThe ensemble approaches, notably the Majority Weighted Vote ensemble and the Dynamic \nSelection ensemble, were developed to leverage the collective strengths of the individual LLMs. \nThe Majority Weighted Vote Ensemble surpassed the individual model performa nces on the \nMedMCQA and MedQA -USMLE datasets and achieved a notable accuracy of 96.21% on \nPubmedQA. This enhancement suggests that a static weighted combination of model outputs can \ncapitalize on the diverse expertise of each LLM to improve overall perform ance. Whereas, the \nDynamic Selection Ensemble, which introduces a context-aware model selection strategy, further \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nimproved upon the Majority Weighted Vote ensemble's performance, achieving the highest \naccuracy across all datasets: 38.01% on MedMCQA, 96.36% on PubmedQA, and 38.13% on \nMedQA-USMLE. The specific range of improvement varies from the variation of each LLM.  \n \nDISCUSSION \nThe results underscore the variability in model performance across different medical QA contexts \nand the potential of ensemble methods to enhance prediction accuracy. The Dynamic Selection \nEnsemble, in particular, illustrates the advantages of a flexible approach that tailors model \nselection to the specific context of each question, aligning with the increasing demand for precision \nin medical informatics applications. \nThe LLM-Synergy ensemble framework is marked by its robust architecture, offering a suite of \nadvantages that enhance its utility in the field of machine learning. Central to its design is the \nprinciple of scalability, which allows the framework to integrat e an expanding roster of Large \nLanguage Models (LLMs) or extend to accommodate larger datasets with minimal structural \nadjustments. Flexibility is another cornerstone of the LLM-Synergy framework, demonstrated by \nits support for both zero -shot and multiple-shot applications, and its agnostic approach to model \nsourcing, accepting LLMs irrespective of their open-source or closed-source status. This versatility \nensures that the framework is not limited by the availability or proprietary nature of models, thus \nbroadening its applicability. Beyond its flexibility, LLM -Synergy is characterized by domain \nadaptability, with an underlying methodology that transcends the medical question -answering \ndomain for which it was initially designed. Its principles are equally relevant to question -\nanswering tasks in various other fields, indicating its potential for extensive adoption beyond the \nconfines of healthcare. In terms of computational efficiency, LLM -Synergy stands out by \nharnessing the strengths of pre -existing, pre-trained models, thereby circumventing the necessity \nfor extensive retraining. This attribute not only conserves computational resources but also \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nexpedites the deployment process. Additionally, the framework's operational efficiency is \nexemplified by its modest demand for computing power, distinguishing it from many \ncontemporary models that often require substantial computational investment. Another  key \nadvantage is the robustness of the ensemble approach, which integrates insights from multiple \nmodels to produce a more reliable and error-resistant output. By aggregating diverse perspectives, \nLLM-Synergy mitigates the influence of individual model bi ases or inaccuracies, resulting in a \nmore dependable collective judgment.  \nCollectively, these advantages position the LLM -Synergy framework as a scalable, flexible, and \ncomputationally prudent choice for diverse machine learning applications, promising significant \nadvancements in the realm of AI-driven problem-solving. \nError Analysis \nIn the context of our LLM -Synergy framework, the efficacy of both ensemble methods is \ncontingent upon the variance in performance among the incorporated LLMs. Within the \nMedMCQA dataset, the discrepancy in performance metrics among the LLMs was relatively \nnarrow, with Llama2-13B achieving the apex at 32.03% and Vicuna -13B at the nadir at 26.13% \nin the individual case. Subsequent to integration into the ensemble framework, the LLMs \ndemonstrated a capacity for reciprocal augmentation of their respective predi ctive strengths, \nculminating in an appreciable performance enhancement of 5.98%. In contrast, the PubMedQA \ndataset exhibited a more homogenized performance distribution, with even the least accurate LLM, \nMedllama-13B, attaining an accuracy rate of 86.11%. This scenario yielded an incremental, albeit \nless substantial, improvement post-ensemble due to the already elevated baseline accuracies. The \nMedQA-USMLE dataset, in other words, presents a distinctive case; Medalpaca -13B's \nperformance markedly surpasses i ts counterparts. In such instances, a static Majority Weighted \nVote approach inherently biases the ensemble towards this single model, potentially allocating \n100% of the weight to Medalpaca-13B and negating the contributions of other LLMs. Conversely, \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nthe Dynamic Model Selection method‚Äîowing to its context-sensitive algorithm‚Äîoffers a degree \nof rectification by adjusting the ensemble's reliance on the most apt LLM for a given question \ncontext. \nLimitation \nThe two frameworks within LLM-Synergy, Weighted Majority Vote Ensemble, and Cluster-based \nDynamic Model selection, while robust and versatile, are not without limitations. One notable \nconstraint is the dependency on the quality and diversity of the LLMs in corporated into the \nensemble. The performance of the ensemble is inherently tied to the individual capabilities of the \nincluded models. If these models share common blind spots or biases, or if they are not sufficiently \ndiverse in their approaches to problem-solving, the ensemble may not significantly outperform its \nindividual constituents. \nFor the Weighted Majority Vote Ensemble, a key limitation lies in its static nature. The weights \nassigned to each model are fixed after training and do not adapt to the nuances of individual \nquestions or contexts within the test set. This could lead to sub optimal performance in situations \nwhere the most appropriate model might change depending on specific question characteristics. \nThe Dynamic Model Selection framework, while more adaptable, relies heavily on the clustering \nalgorithm's ability to meaningfull y categorize questions. If the clustering does not effectively \ncapture the relevant features that dictate which LLM would perform best, the dynamic selection \nprocess may not yield the intended improvements. Additionally, this method's success is \ncontingent upon the availability of a sufficiently large and representative validation set to fine-tune \nthe model selection process. Moreover, both frameworks could still potentially be limited by \ncomputational constraints in practice, even if not computationally hu ngry as methods like fine -\ntuning or pretraining. Despite being designed to be resource -efficient, the process of integrating \nmultiple complex models, especially when scaling up to include additional LLMs or \naccommodating larger datasets, could still demand  significant computational resources.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nLastly, the frameworks assume that the best-performing LLMs on the validation set will continue \nto be the best choices for the test set. This may not hold if there are substantial differences between \nthe validation and test sets, leading to a discrepancy b etween expected and actual performance. \nTherefore, while the LLM-Synergy frameworks offer promising approaches to ensemble learning \nin NLP, they must be applied judiciously, with an awareness of these potential limitations. \n \nCONCLUSION \nThe LLM-Synergy framework, with its boosting-based Weighted Majority Vote and cluster-based \nDynamic Model Selection methods, represents a significant advancement in leveraging LLMs for \nmedical QA tasks and provides an innovative way of efficiently utilizin g the development with \nLLM Technologies, customing for both existing and potentially future challenge tasks in \nbiomedical and health informatics research. Its ability to amalgamate the strengths of multiple \nmodels has demonstrated superior accuracy and rob ustness over individual LLMs. While the \nframework showcases scalability, flexibility, and adaptability across domains, it also presents \nopportunities for future enhancements, including increased model diversity, dynamic weighting, \nand broader domain applications. As such, LLM-Synergy not only addresses current challenges in \nnatural language processing but also sets the stage for continued innovation in AI-driven problem-\nsolving.  \nDATA AVAILABILITY \nThe LLM-synergy framework used in this study is made publicly available at https://github.com- \n/serendipitYang/LLM-Synergy. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nFUNDING STATEMENT \nThis work was supported by the National Institutes of Health ‚Äôs National Center for \nComplementary and Integrative Health grant number R01AT009457, National Institute on Aging \ngrant number R01AG078154, and National Cancer Institute grant number R01CA287413. The \ncontent is solely the responsibility of the authors and d oes not represent the official views of the \nNational Institutes of Health. \nCONTRIBUTORSHIP STATEMENT \nHY served as the principal author, designing the entire experiment, writing experimental code, and \nthe full manuscript. ML and YX collaborated on the data collection and analysis and also reviewed \nthe paper. HZ assisted in conceiving the study design and r eviewed the manuscript. RZ was \nresponsible for auditing the feasibility of the research topic and refining the paper. QF created the \ngraphical and workflow diagrams within the manuscript. All authors contributed significantly to \nthe production and proofing of the manuscript. \nACKNOWLEDGEMENTS \nWe would also like to acknowledge to the staff at BPIC of the University of Minnesota and the \nprogrammers from Dr. Rui Zhang‚Äôs group for their technical support. \nCOMPETING INTERESTS STATEMENT \nThe authors state that they have no competing interests to declare. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \nREFERENCES \n1 Athenikos SJ, Hyoil  H. Biomedical question answering: A survey. Computer methods and \nprograms in Biomedicine 2010;99.1:1-24. \n2 Clark P, Cowhey I, Etzioni O, et al. Think you have Solved Question Answering? Try ARC, the \nAI2ai2 Reasoning Challenge. arXiv preprint arXiv 2018:1803.05457. \n3 Moll√° D, Vicedo JL. \"Question answering in restricted domains: An overview.\" Computational \nLinguistics 2007;33.1:41-61. \n4 Jin Q, Yuan Z, Xiong G, et al. Biomedical question answering: a survey of approaches and \nchallenges. ACM Computing Surveys (CSUR) 2022;55.2:1-36. \n5 Mutabazi E, Ni J, Tang G, et al. A review on medical textual question answering systems based \non deep learning approaches. Applied Sciences 2021;11.12:5456. \n6 He Y, Zhu Z, Zhang Y, et al. Infusing disease knowledge into BERT for health question \nanswering, medical inference and disease name recognition. arXiv preprint arXiv \n2020:2010.03746. \n7 Alzubi JA, Jain R, Singh A, et al. COBERT: COVID-19 question answering system using BERT. \nArabian journal for science and engineering 2023;48.8:11003-11013. \n8 Bubeck S, Chandrasekaran V, Eldan R, et al. Sparks of artificial general intelligence: Early \nexperiments with gpt-4. arXiv preprint arXiv 2023:2303.12712. \n9 Chang Y, Wang X, Wang J, et al. A survey on evaluation of large language models. arXiv \npreprint arXiv 2023:2307.03109. \n10 Tang R, Chuang YN, Hu X. The science of detecting llm-generated texts. arXiv preprint arXiv \n2023:2303.07205. \n11 Tan Y, Min D, Li Y, et al. Evaluation of ChatGPT as a question answering system for answering \ncomplex questions. arXiv preprint arXiv 2023:2303.07992. \n12 Zhou H, Austin R, Lu SC, et al. Complementary and Integrative Health Information in the \nliterature: its lexicon and named entity recognition. Journal of the American Medical Informatics \nAssociation 2023:ocad216. \n13 Touvron H, Lavril T, Izacard Get, et al. Llama: Open and efficient foundation language models. \narXiv preprint arXiv 2023:2302.13971. \n14 Chiang WL, Li Z, Sheng Y, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* \nchatgpt quality. https://lmsys.org/blog/2023-03-30-vicuna/ (accessed 14 April 2023). \n15 Nori H, King N, Mckinney SM, et al. Capabilities of gpt -4 on medical challenge problems. \narXiv preprint arXiv 2023:2303.13375. \n16 Thirunavukarasu AJ, Ting DSJ, Elangovan K, et al. Large language models in medicine. Nature \nMedicine 2023;29.8:1930-1940. \n17 Chowdhery A, Narang S, Devlin J, et al. Palm: Scaling language modeling with pathways. \nJournal of Machine Learning Research 2023;24.240:1-113. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \n18 Chung HW, Hou L, Longpre S, et al. Scaling instruction -finetuned language models. arXiv \npreprint arXiv 2022:2210.11416. \n19 Touvron H, Martin L, Albert P, et al. Llama 2: Open foundation and fine -tuned chat models. \narXiv preprint arXiv 2023:2307.09288. \n20 Taori R, Gulrajani I, Zhang T, et al. Stanford alpaca: An instruction -following llama model. \n2023. \n21 Wu C, Lin W, Zhang X, et al. Pmc-llama: Towards building open-source language models for \nmedicine. arXiv preprint arXiv 2023:2305.10415. \n22 Hoffmann J, Borgeaud S, Mensch A, et al. Training compute -optimal large language models. \narXiv preprint arXiv 2022:2203.15556. \n23 Gu Y, Tinn R, Cheng H, et al. Domain -specific language model pretraining for biomedical \nnatural language processing. ACM Transactions on Computing for Healthcare (HEALTH) \n2021;3.1:1-23. \n24 Wang B, Xie Q, Pei J, et al. Pre -trained language models in biomedical domain: A systematic \nsurvey. ACM Computing Surveys 2023;56.3:1-52. \n25 Singhal K, Azizi S, Tu T, et al. Large language models encode clinical knowledge. Nature \n2023;620.7972:172-180. \n26 Zhou S, Wang N, Wang L, et al. CancerBERT: a cancer domain -specific language model for \nextracting breast cancer phenotypes from electronic health records. Journal of the American \nMedical Informatics Association 2022;29.7:1208-1216. \n27 Gururangan S, Marasoviƒá A, Swayamdipta S, et al. Don't stop pretraining: Adapt language \nmodels to domains and tasks. arXiv preprint arXiv 2020:2004.10964. \n28 Han T, Adam LC, Papaioannou JM, et al. MedAlpaca--An Open-Source Collection of Medical \nConversational AI Models and Training Data. arXiv preprint arXiv 2023:2304.08247. \n29 Dong X, Yu Z, Cao W, et al. A survey on ensemble learning. Frontiers of Computer Science \n2020;14:241-258. \n30 Ganaie  MA, Hu M, Malik AK, et al. Ensemble deep learning: A review. Engineering \nApplications of Artificial Intelligence 2022;115:105151. \n31 Xiao Y, Wu J, Lin Z, et al. A deep learning -based multi-model ensemble method for cancer \nprediction. Computer methods and programs in biomedicine 2018;153:1-9. \n32 Oniani D, Hilsman J, Dong H, et al. Large language models vote: Prompting for rare disease \nidentification. arXiv preprint arXiv 2023:2308.12890. \n33 Drucker H, Cortes C, Jeckel LD, et al. Boosting and other ensemble methods. Neural \nComputation 1994;6.6:1289-1301. \n34 Bifet A, Holmes G, Pfahringer B, et al. New ensemble methods for evolving data streams. \nProceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and \ndata mining. 2009. \n35 Jiang D, Xiang R, Bill YL. LLM-Blender: Ensembling Large Language Models with Pairwise \nRanking and Generative Fusion. arXiv preprint arXiv 2023:2306.02561. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \n36 Pitis S, Zhang MR, Wang A, et al. Boosted Prompt Ensembles for Large Language Models. \narXiv preprint arXiv 2023:2304.05970. \n37 Nadeem M, Anna B, Siva R. StereoSet: Measuring stereotypical bias in pretrained language \nmodels. arXiv preprint arXiv 2020:2004.09456. \n38 McGee RW. Is chat gpt biased against conservatives? an empirical study. An Empirical Study. \nhttp://dx.doi.org/10.2139/ssrn.4359405(accessed 15 Feb 2023). \n39 Ferrara E. Should chatgpt be biased? challenges and risks of bias in large language models. \narXiv preprint arXiv 2023:2304.03738. \n40 Webb GI, Zheng Z. Multistrategy ensemble learning: Reducing error by combining ensemble \nlearning techniques. IEEE Transactions on Knowledge and Data Engineering 2004;16.8:980-991. \n41 Kolter JZ, Marcus AM. Dynamic weighted majority: An ensemble method for drifting \nconcepts. The Journal of Machine Learning Research 2007;8:2755-2790. \n42 Dogan A, Derya B. A weighted majority voting ensemble approach for classification. 2019 4th \nInternational Conference on Computer Science and Engineering (UBMK). IEEE, 2019. \n43 Cov√µes TF, Eduardo RH. Towards improving cluster-based feature selection with a simplified \nsilhouette filter. Information Sciences 2011;181.18:3766-3782. \n44 Pal A, Logesh KU, Malaikannan S. Medmcqa: A large-scale multi-subject multi-choice dataset \nfor medical domain question answering. Conference on Health, Inference, and Learning. PMLR, \n2022. \n45 Jin, Q, Dhingra B, Liu Z, et al. Pubmedqa: A dataset for biomedical research question \nanswering. arXiv preprint arXiv 2019:1909.06146. \n46 Jin D, Pan E, Oufattole N, et al. What disease does this patient have? A large-scale open domain \nquestion answering dataset from medical exams. Applied Sciences 2021;11.14:6421. \n47 Mohandes M, Mohamed D, Salihu OA. Classifiers combination techniques: A comprehensive \nreview. IEEE Access 2018;6:19626-19639. \n \nFIGURE LEGENDS \nFigure 1.  Overview of our LLM-Synergy Framework \nFigure 2. QA shows the accuracy of how each LLM performs on the three medical QA datasets. \nFigure 3. Training Process of Boosting-based Weighted Majority Vote Ensemble \nFigure 4. Training Process of Cluster-based Dynamic Model Selection Ensemble \n \nAPPENDIX \nPrompt for MedMCQA dataset: \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint \n \nPrompt for PubMedQA dataset: \n \nPrompt for MedQA-USMLE dataset, English language: \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 24, 2023. ; https://doi.org/10.1101/2023.12.21.23300380doi: medRxiv preprint ",
  "topic": "Power (physics)",
  "concepts": [
    {
      "name": "Power (physics)",
      "score": 0.4429127275943756
    },
    {
      "name": "Ensemble learning",
      "score": 0.4206404983997345
    },
    {
      "name": "Computer science",
      "score": 0.3692784309387207
    },
    {
      "name": "Data science",
      "score": 0.3652046024799347
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2585546374320984
    },
    {
      "name": "Physics",
      "score": 0.08547854423522949
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130238516",
      "name": "University of Minnesota",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 30
}