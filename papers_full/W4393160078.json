{
    "title": "Large Language Models Are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales",
    "url": "https://openalex.org/W4393160078",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2161411052",
            "name": "Taeyoon Kwon",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A5084536284",
            "name": "Kai Tzu-Iunn Ong",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2100687149",
            "name": "DongJin Kang",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2491320306",
            "name": "Seungjun Moon",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A3169148809",
            "name": "Jeong Ryong Lee",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2155388301",
            "name": "Dosik Hwang",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2801305375",
            "name": "Beomseok Sohn",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2987781805",
            "name": "Yongsik Sim",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2100957142",
            "name": "Dong-Ha Lee",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2232531623",
            "name": "Jinyoung Yeo",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2161411052",
            "name": "Taeyoon Kwon",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A5084536284",
            "name": "Kai Tzu-Iunn Ong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100687149",
            "name": "DongJin Kang",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2491320306",
            "name": "Seungjun Moon",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A3169148809",
            "name": "Jeong Ryong Lee",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2155388301",
            "name": "Dosik Hwang",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2801305375",
            "name": "Beomseok Sohn",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2987781805",
            "name": "Yongsik Sim",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2100957142",
            "name": "Dong-Ha Lee",
            "affiliations": [
                "Yonsei University"
            ]
        },
        {
            "id": "https://openalex.org/A2232531623",
            "name": "Jinyoung Yeo",
            "affiliations": [
                "Yonsei University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4281644150",
        "https://openalex.org/W2886848602",
        "https://openalex.org/W7020042380",
        "https://openalex.org/W2965717703",
        "https://openalex.org/W3115863418",
        "https://openalex.org/W4304701382",
        "https://openalex.org/W2748434587",
        "https://openalex.org/W4312749457",
        "https://openalex.org/W6828821510",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W6838865847",
        "https://openalex.org/W4205941964",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2120230938",
        "https://openalex.org/W3177174258",
        "https://openalex.org/W6739551079",
        "https://openalex.org/W6852330325",
        "https://openalex.org/W2885706121",
        "https://openalex.org/W4383469126",
        "https://openalex.org/W3021476288",
        "https://openalex.org/W6811284106",
        "https://openalex.org/W2123623728",
        "https://openalex.org/W2963616706",
        "https://openalex.org/W3210438755",
        "https://openalex.org/W4385572016",
        "https://openalex.org/W4226369848",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W3035129496",
        "https://openalex.org/W2626029845",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4226479682",
        "https://openalex.org/W2078524519",
        "https://openalex.org/W4366835631",
        "https://openalex.org/W2778796877",
        "https://openalex.org/W4385572290",
        "https://openalex.org/W2115799070",
        "https://openalex.org/W4386362706",
        "https://openalex.org/W4298284650",
        "https://openalex.org/W2168490582",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W2788043421",
        "https://openalex.org/W3015759694",
        "https://openalex.org/W4385571011",
        "https://openalex.org/W4319165821",
        "https://openalex.org/W4385751474",
        "https://openalex.org/W4392359953",
        "https://openalex.org/W4385573087",
        "https://openalex.org/W4287113019",
        "https://openalex.org/W4386566910",
        "https://openalex.org/W4391876619"
    ],
    "abstract": "Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a \"reasoning-aware\" diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various settings. We further propose a novel set of criteria for evaluating machine-generated rationales' potential for real-world clinical settings, facilitating and benefiting future research in this area.",
    "full_text": "Large Language Models Are Clinical Reasoners:\nReasoning-Aware Diagnosis Framework with Prompt-Generated Rationales\nTaeyoon Kwon1*, Kai Tzu-iunn Ong1*, Dongjin Kang2, Seungjun Moon2, Jeong Ryong Lee3,\nDosik Hwang3, Beomseok Sohn4, Yongsik Sim4, Dongha Lee1, Jinyoung Yeo1\n1Department of Artificial Intelligence, Yonsei University\n2Department of Computer Science and Engineering, Yonsei University\n3Department of Electrical and Electronic Engineering, Yonsei University\n4Department of Radiology, College of Medicine, Yonsei University\nAbstract\nMachine reasoning has made great progress in recent years\nowing to large language models (LLMs). In the clinical do-\nmain, however, most NLP-driven projects mainly focus on\nclinical classification or reading comprehension, and under-\nexplore clinical reasoning for disease diagnosis due to the ex-\npensive rationale annotation with clinicians. In this work, we\npresent a “reasoning-aware” diagnosis framework that ratio-\nnalizes the diagnostic process via prompt-based learning in\na time- and labor-efficient manner, and learns to reason over\nthe prompt-generated rationales. Specifically, we address the\nclinical reasoning for disease diagnosis, where the LLM gen-\nerates diagnostic rationales providing its insight on presented\npatient data and the reasoning path towards the diagnosis,\nnamely Clinical Chain-of-Thought (Clinical CoT). We em-\npirically demonstrate LLMs/LMs’ ability of clinical reason-\ning via extensive experiments and analyses on both rationale\ngeneration and disease diagnosis in various settings. We fur-\nther propose a novel set of criteria for evaluating machine-\ngenerated rationales’ potential for real-world clinical settings,\nfacilitating and benefiting future research in this area.\nIntroduction\nReasoning is the ability to assess things logically based on\navailable information of various types. Reasoning in clini-\ncal diagnosis, also known as clinical reasoning or diagnostic\nreasoning, is a dynamic thinking process between the ob-\nserved clinical evidence and the identification of disease.\nIt involves an integration of patient data, relevant medi-\ncal knowledge, clinicians’ experience, and other contextual\nor situational factors (Norman 2005; Cook, Sherbino, and\nDurning 2018). Poor clinical reasoning has been directly\nlinked to misdiagnoses and eventually causing hospital ad-\nverse events including patient death (Balogh, Miller, and\nBall 2015). Therefore, effective clinical reasoning is crucial\nfor diagnosis in real clinical settings (Kassirer 1989).\nRecently, deep learning (DL) models are widely utilized\nfor disease diagnosis. However, a predominant portion of ex-\nisting approaches formulates the process simply as image\nor text classification (Bakator and Radosav 2018; Kumar\n*These authors contributed equally.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\net al. 2022). These approaches entirely exclude the afore-\nmentioned clinical reasoning in their modeling and focus on\nfine-tuning high-capacity models for better feature extrac-\ntion (Jang and Hwang 2022). However, such a data-driven\napproach can be limited by the data-scarcity problem in\nbiomedical domains. Moreover, high-capacity models are\nshown to memorize the dataset, rather than solving the diag-\nnosis task through logical reasoning (Mitra et al. 2020), and\nthey cannot provide explanations justifying their diagnoses.\nSince whether a diagnosis can be explained and whether it\nmatches the reasoning of humans are important for gaining\nclinicians’ trust in DL techniques (Holzinger et al. 2017),\nthis naive approach largely limits models’ potential to be im-\nplemented for real-world applications.\nMeanwhile, large language models have demonstrated\ntheir ability to perform multi-step reasoning as well as\npresent the thinking process behind it, which is known as\nchain-of-thought (CoT) reasoning (Wei et al. 2022). Previ-\nous works have applied such reasoning ability to various\ndomains (Wei et al. 2022; Kojima et al. 2022; Wu, Zhang,\nand Huang 2023; Li ´evin, Hother, and Winther 2023). In\nthese works, LLMs serve as reasoners that generate natu-\nral language rationales guiding and explaining the solution.\nDespite such success, the use of LLMs to address clinical\nreasoning in disease diagnosis for real-world applications is\nstill an under-explored area at the moment.\nMotivated by these, in this work, we make a step toward\nclinical reasoning in disease diagnosis, where the models are\naware of the clinical reasoning behind the diagnosis, as il-\nlustrated in Figure 1. To this end, we formulate the clinical\nreasoning in disease diagnosis as chain-of-thought reason-\ning, namely Clinical Chain-of-Thought (Clinical CoT). Our\ngoal is to facilitate clinical reasoning by leveraging LLMs\nto reason over patient data, refer to relevant knowledge, and\ngenerate rationales that guide and explain the diagnosis.\nOur contributions are two-fold: (i) We propose a practical\nframework for reasoning-aware diagnosis. Our framework\ninvolves clinical rationalization that augments the existing\nclinical data with clinical rationales, few-shot reasoning and\ndiagnosis with LLMs, and distillation towards smaller mod-\nels. (ii) We conduct a thorough analysis of the framework\nin our testbed diagnosis dataset to gain a deep understand-\ning of the clinical reasoning task. We show that by reason-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18417\nFigure 1: Clinical reasoning in disease diagnosis.\ning over presented clinical data, models can achieve better\nperformance in disease diagnosis. Also, our extensive eval-\nuation and analysis of generated rationales demonstrate that\nboth the LLMs and distilled models can replicate the reason-\ning of clinical professionals in a human-like manner.\nProblem Formulation\nClinical Reasoning for Disease Diagnosis\nMost existing approaches for diagnosing diseases with DL\nmodels formulate the process simply as image or text clas-\nsification (Bakator and Radosav 2018; Kumar et al. 2022).\nThat is, given patient description P, such as medical images\nor electronic health records, a diagnosis model θ is trained\nto predict the correct diagnosis D:\nD ∼Pθ(·|P) (1)\nHowever, this approach neglects the clinical reasoning con-\nnecting the presented patient description and the final di-\nagnosis (Kassirer 1989). The absence of effective clinical\nreasoning can lead to diagnostic errors (e.g., misdiagnoses),\nwhich are reported to contribute to around 10% of patient\ndeaths and hospital adverse events (Norman 2005).\nTo address that, we exploit LLMs’ reasoning capacity in\nclinical diagnosis, where the LLMs ought to perform clin-\nical reasoning over presented clinical data. Formally, given\npatient description P, the model first generates a rationaleR\ndecomposing the reasoning process over P, and then makes\nits diagnosis D based on P and R:\nR ∼Pθ(·|P) (2)\nD ∼Pθ(·|P, R) (3)\nTestbed: Alzheimer’s Disease Diagnosis\nAlzheimer’s disease (AD) is an irreversible neurodegen-\nerative disease associated with cognitive decline (DeTure\nand Dickson 2019). In this study, we choose AD diagno-\nsis task as the testbed for clinical reasoning. This choice is\nbased on the fact that AD diagnosis requires a thorough un-\nderstanding of various aspects of the disease (Budson and\nSolomon 2012). In our study, patient description P consists\nof (1) textual descriptions derived from the MRI scan, such\nas “This patient has SEVERE hippocampal atrophy”;1 (2)\ndemographic information; (3) educational level; (4) results\nfrom the mini-mental state examination (MMSE); (5) the\npresence of APOE4 allele. The diagnosis D can be either\nAlzheimer’s Disease, Mild Cognitive Impairment (MCI), or\nNormal Cognition (NC). Details on transforming MRI scans\ninto textual descriptions are provided in the appendix.\nReasoning-Aware Diagnosis Framework\nFramework Overview\nRecent works successfully leverage LLMs’ ability of CoT\nreasoning to generate free-text rationales that present the\nreasoning path and the necessary knowledge towards the\nanswers in various reasoning tasks (Wei et al. 2022; Wu,\nZhang, and Huang 2023). Our goal is to exploit such abil-\nity in clinical diagnosis, where the LLMs ought to generate\nrationales demonstrating its reasoning over presented clini-\ncal data. For that, we formulate the rationale generation in\nclinical diagnosis as Clinical CoT reasoning. Upon that, we\npropose a reasoning-aware diagnosis framework (Figure 2),\nwhich includes modules addressing different approaches to\nfacilitate clinical reasoning.\nModule I: Clinical Rationalization\nTo generate clinical CoT rationales, which deliver the diag-\nnostic reasoning towards the correct diagnosis, by prompting\na LLM to rationalize the presented clinical data.\nFormally, given clinical data consisting of patient de-\nscription P and a ground-truth label of the diagnosis D,\nwhich can be either Alzhemer’s Disease, Mild Cognitive Im-\npairment (MCI), or Normal Cognition (NC), the LLM is\nprompted to generate clinical rationalesR∗ that demonstrate\nthe reasoning process over P such that the final diagnosis D\ncan be induced from R∗:\nR∗ = argmax\nR\nPLLM(R|P, D) (4)\n1Hippocampal atrophy refers to the shrinkage or loss of nerve\ncells in the hippocampus, a region related to memory formation and\nmemory retrieval (V oss et al. 2017).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18418\nFigure 2: An overview of our framework (P: Patient description; D: Diagnosis; R: Clinical rationale).\nAs output, we collect a set D of the processed samples,\neach of which is a triplet of patient description P, a ground-\ntruth label of the diagnosis D, and the clinical rationales R.\nModule II-1: Few-shot CoT Reasoning\nLLMs have demonstrated promising performance in tasks\nrequiring logical reasoning with CoT prompting (Kojima\net al. 2022; Wei et al. 2022). As a pioneer study towards\nclinical reasoning with LLMs, we investigate if such suc-\ncess can be replicated in the domain of clinical diagnosis.\nThereby, our second module addresses few-shot disease di-\nagnosis, where we prompt LLMs to perform clinical reason-\ning before the diagnosis.\nFormally, given the patient description P, an LLM is\nprompted to generate both a plausible clinical rationale ˆR\nand the name of the predicted diagnosis ˆD:\nˆR = argmax\nR\nPLLM(R|P) (5)\n⇒ ˆD = argmax\nD\nPLLM(D|P, ˆR) (6)\nwhere ⇒ indicates a sequential generation of tokens.\nModule II-2: Unimodal-Student Distillation\nDespite the impressive performance offered by LLMs in\nfew-shot settings, it is non-trivial to deploy them for real-\nworld applications due to the large size of parameters. 2 Re-\ncent works resolve this by using LLMs to augment the tar-\nget dataset with rationales and use the augmented dataset to\nfine-tune smaller models, aiming to distill LLMs’ reasoning\ncapacity into models that are more affordable for practical\nuses (Hsieh et al. 2023). This approach is known as knowl-\nedge distillation (Hinton, Vinyals, and Dean 2015).\nThis module distills the knowledge of diagnostic rea-\nsoning from the LLM (teacher) into orders-of-magnitude\nsmaller language models, with the goal of developing\nsmaller CoT reasoners for real clinical settings. Applying\nour rationalizing module (Module I), we obtain clinical data\nfor AD diagnosis that are augmented with clinical rationales.\n2One LLM with 175B parameters requires at least 350GB GPU\nmemory with tailored infrastructures (Zheng et al. 2022).\nWe purpose this augmented dataset as training data to train\nthe student language models.\nFormally, given patient descriptionP, the LM is trained to\nsequentially predict the clinical rationale R and the ground-\ntruth label of the diagnosis D. The language model is opti-\nmized by minimizing the generation loss LLM-Distill:\nLLM-Distill = E\n(P,D,R)∈D\n[−log PLM(R, D|P)] (7)\nModule II-3: Multimodal-Student Distillation\nBesides training smaller CoT reasoners with language mod-\nels, multimodal CoT, where both visual and textual inputs\ncan be considered via vision-language models (VLMs), has\nalso garnered attention. For instance, Zhang et al. (2023)\nshowed that by including images alongside textual inputs,\nmodels with under 1B parameters can generate more ef-\nfective CoT rationales and vastly outperform the previous\nstate-of-the-art LLM with 175B parameters on a question-\nanswering benchmark. Meanwhile, the diagnosis of many\ndiseases including AD involves medical images such as MRI\nscans, fundus photographs, and X-ray images (Kumar et al.\n2022). Therefore, we further extend knowledge distillation\nin clinical diagnosis to VLMs.\nFormally, given patient description P and its correspond-\ning MRI scan V, the VLM is trained to sequentially predict\nthe clinical CoT rationale R and the ground-truth label of\nthe diagnosis D based on P and V. The VLM is learnt by\nminimizing the generation loss LVLM-Distill:\nLVLM-Distill = E\n(P,V,D,R)∈D\n[−log PVLM(R, D|P, V)] (8)\nExperiments\nExperimental Settings\nDataset. We acquire 7,124 clinical data for Alzheimer’s\ndisease (AD) from the Alzheimer’s Disease Neuroimaging\nInitiative (ADNI) (Jack Jr et al. 2008) and 428 from Aus-\ntralian Imaging Biomarkers and Lifestyle Study of Ageing\n(AIBL) (Ellis et al. 2009). Datasets from these organizations\nhave profoundly facilitated the study of AD. Both datasets\ncomprise three components: (1) MRI scans, (2) ground-truth\nlabels of diagnosis, and (3) patient descriptions, including\ndemographic information, educational level, results from the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18419\nADNI AIBL\nAccuracy Precision Recall Accuracy Precision Recall\nModel Prompt Total AD MCI NC AD MCI NC Total AD MCI NC AD MCI NC\nChatGPT 0-shot 55.3 56.8 45.3 69.9 96.4 22.4 48.8 54.0 59.8 47.2 61.1 80.0 31.7 55.0\n1-shot 50.7 61.4 40.6 71.4 81.5 62.9 7.9 50.7 67.4 41.8 70.6 66.9 74.7 8.6\n3-shot 58.8 61.6 47.1 71.5 85.5 46.7 44.8 57.2 68.3 46.6 63.5 63.1 57.0 52.1\n5-shot 57.3 55.7 44.4 70.2 97.2 23.2 53.2 56.3 57.7 47.3 62.5 86.9 33.5 53.6\nClinical CoT 67.3 62.2 54.2 62.7 90.3 26.6 86.5 62.4 63.8 43.7 53.2 79.2 25.3 88.6\nGPT-4 0-shot 59.6 51.1 51.6 76.8 99.6 24.3 42.1 55.4 52.1 49.5 71.1 93.9 29.1 49.3\n1-shot 53.0 54.1 44.4 81.0 98.4 42.9 18.7 54.9 55.6 42.2 65.5 96.2 38.0 35.7\n3-shot 61.8 64.3 50.9 76.5 86.3 55.2 44.4 58.2 66.2 46.7 67.0 72.3 53.2 50.7\n5-shot 62.6 67.8 50.5 76.5 90.8 57.5 40.1 59.8 68.9 48.4 69.3 78.5 58.9 43.6\nClinical CoT 68.4 77.5 59.3 67.4 76.2 40.5 89.3 62.6 82.2 51.2 60.9 63.8 39.2 87.9\nTable 1: Evaluation on LLMs in zero-and-few-shot diagnosis. The Clinical CoT includes two exemplar shots.\nFigure 3: Performance of student models trained with and\nwithout clinical rationales, reported on ADNI. The dotted\nline is the performance of the teacher LLM (GPT-4).\nmini-mental state examination (MMSE) and the presence of\nAPRO4 allele. Data from ADNI are split into training, vali-\ndation, and test sets. All the AIBL data are exclusively used\nas an out-of-domain test set rather than training. The ratio of\nAD:MCI:NC in both test sets is roughly 1:1:1.\nLarge language models. We choose ChatGPT (OpenAI\n2023a) and GPT-4 (OpenAI 2023b) as our selection for\nLLMs. They have shown an impressive ability to perform\nchain-of-thought reasoning. For our clinical rationalization\nmodule (Module I), we adopt GPT-4. And we adopt both\nChatGPT and GPT-4 for few-shot diagnosis (Module II). In\nall of our experiments, we set the temperature to 0.7, max\ntokens to 2000, and apply greedy decoding.\nUnimodal-student models. We consider OPT (Zhang\net al. 2022) and LLaMA2 (Touvron et al. 2023), two com-\nmonly used language models as our foundation model for\nthe unimodal student. For our experiments, we use the 1.3B\nand 6.7B versions of OPT and the 7B version of LLaMA2.\nMultimodal-student models. Following Tsimpoukelli\net al. (2021), our multimodal student is based on the vision-\nlanguage model, which consists of convolutional neural net-\nworks as vision encoder and a language model as text en-\ncoder. The vision encoder extracts image features from the\nMRI scan. During training, the vision encoder is trained to\nalign its extracted feature with text features, such that the\nlanguage model can effectively attend to features of both\nmodalities. Since MRI scans are 3-dimensioanl images, we\nconsider 3D ResNet (Hara, Kataoka, and Satoh 2017) as\nour foundation model of vision encoder. For the language\nmodel, we use the 1.3B and 6.7B versions of OPT and the\n7B version of LLaMA2. Implementation details of our stu-\ndent models are provided in the appendix.\nResults and Discussion\nWe now present the empirical findings of the following re-\nsearch questions that guide our experiments:\nRQ1: Does clinical rationales improve AD diagnosis?\nRQ2: Does knowledge distillation benefit small models?\nRQ3: Is our framework helpful in data-scarce scenarios?\nRQ4: What causes misdiagnosis and how to get over it?\nLLMs’ diagnostic performance (RQ1). Table 1 presents\nthe experimental results. Firstly, we observe that LLMs\nwithout clinical CoT generally yield high recall in one class\n(mostly AD) with fairly low recall in the other two classes.\nThis suggests that clinical CoT can prevent LLMs from be-\ning biased toward a specific diagnosis.\nSecondly, LLMs with clinical CoT show huge improve-\nments in accuracy compared to baselines with more shots.\nThis follows the observation in Kojima et al. (2022), where\nLLMs with 2-shot CoT prompting largely outperform LLMs\nwith 8-shot standard prompting in arithmetic domains. We\ndemonstrate the same patterns in clinical diagnosis.\nPerformance of student models (RQ2). Table 2 presents\nthe experimental results of student models. Firstly, among\nunimodal students: For in-domain data, OPT 6.7B shows\ncomparable performance to the teacher model in total ac-\ncuracy. LLaMA2 7B yields a significant performance gain\nin accuracy and precision in all three classes; For out-of-\ndomain data, all unimodal students outperform the teacher\nLLM in accuracy. In addition, OPT 1.3B and LLaMA2 7B\ndemonstrate better precision for all classes and higher re-\ncall for AD and MCI. Moreover, all the text-only unimodal\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18420\nADNI (In-domain) AIBL (Out-of-domain)\nAccuracy Precision Recall Accuracy Precision Recall\nBaselines Total AD MCI NC AD MCI NC Total AD MCI NC AD MCI NC\nGPT-4 (Teacher Model) 68.4 77.5 59.3 67.4 76.2 40.5 89.3 62.6 82.2 51.2 60.9 63.8 39.2 87.9\n3D ResNet-50 49.8 78.0 37.9 59.1 44.3 67.1 37.3 48.1 80.5 40.1 50.7 63.8 39.2 87.9\n3D ResNet-152 51.9 77.0 37.1 52.4 55.6 45.1 55.1 47.6 61.1 39.8 61.1 33.8 68.3 37.1\nUnimodal Students\nOPT 1.3B 66.5 78.9 52.3 73.3 65.5 61.8 72.6 70.0 88.4 56.9 80.2 76.2 81.0 52.1\nOPT 6.7B 68.4 77.4 61.3 64.9 85.5 37.8 83.7 66.1 79.0 59.6 60.2 83.8 37.3 82.1\nLLaMA2 7B 71.5 83.2 63.0 67.9 81.9 48.6 84.9 69.4 84.9 60.0 65.8 82.3 57.0 71.4\nMultimodal Students\n3D ResNet-50 (0.05B)\n+ OPT 1.3B 68.6 86.1 53.3 76.4 75.4 71.4 59.1 65.6 87.3 52.9 70.7 69.2 73.4 53.5\n+ OPT 6.7B 70.8 89.5 56.0 74.8 75.8 70.7 65.9 65.7 87.4 53.5 67.2 69.2 67.7 60.0\n+ LLaMA2 7B 69.0 82.8 56.0 69.3 83.5 57.9 66.3 68.0 84.8 55.7 73.9 81.5 74.1 48.6\n3D ResNet-152 (0.1B)\n+ OPT 1.3B 68.9 79.6 54.8 75.3 79.0 61.3 66.6 67.2 73.8 66.4 62.1 79.3 55.8 73.1\n+ OPT 6.7B 71.0 88.9 55.7 79.5 74.2 75.7 63.0 65.6 88.0 53.0 66.1 74.4 67.1 55.7\n+ LLaMA2 7B 68.5 83.3 54.9 69.2 84.7 60.6 60.7 69.4 85.5 57.9 72.0 81.5 72.2 55.0\nTable 2: Evaluation on unimodal and multimodal students compared with GPT-4 and vision-only baselines in AD diagnosis.\nThe GPT-4 (Teacher Model) refers to the performance of GPT-4 augmented with clinical CoT rationales in Table 1.\nFigure 4: Data efficiency brought by clinical reasoning.\nstudents outperform the baseline vision models in total ac-\ncuracy, despite the image-intensive nature of AD diagnosis.\nSecondly, for multimodal students: Similar to the finding\nfrom Zhang et al. (2023), with multimodal CoT, all VLMs\nwith orders-of-magnitude smaller sizes of parameters out-\nperform the LLM teacher in total accuracy as well as recall\nfor AD and/or MCI. Also, all multimodal students achieve\nremarkably higher total accuracy than vision-only baselines.\nOverall, although metrics in which student models win\nslightly differ when the adopted encoders change, we can\nconclude a general observation: most of the students exhibit\nhigher accuracy and recall for AD and MCI than the LLM\nteacher. A higher recall indicates that the approach is more\neffective at minimizing false negatives, which suggests that\nthe student models are better at avoiding misdiagnosing pa-\ntients with AD or MCI as normal.\nFigure 3 visualizes the difference in diagnostic accuracy\nFigure 5: Analysis of rationales from GPT-4’s misdiagnoses.\nof our student models with and without knowledge distilla-\ntion from the LLM. We can clearly observe the significant\nimprovement in performance when training the model with\nour data augmented with clinical rationales.\nData efficiency (RQ3). The lack of sufficient data is a\nlong-standing problem in the biomedical domain. Thus, we\nexperiment on our multimodal student with varying amounts\nof training data to examine if our framework is helpful in\ndata-scarce scenarios. The results are in Figure 4.\nOur multimodal student (purple), trained with clinical\nrationales, consistently outperforms the vision-only and\nvision-language baseline models no matter how much train-\ning data is used. Furthermore, when trained with only 25%\nof training data, the multimodal student exhibits higher ac-\ncuracy than both baselines trained with 100% of data. Our\napproaches show comparable performance even when only\na limited amount of training data is available. These exper-\nimental findings confirm the data efficiency brought by dis-\ntilling LLMs’ reasoning capacity to small diagnosis models,\nwhich is an important property in the biomedical domain.\nAnalysis of misdiagnosed cases (RQ4). Since the ratio-\nnales are generated “during” the diagnosis, investigating the\ncorrelation between generated rationales and misdiagnoses\nis rather important. For that, we sample 32 rationales gen-\nerated from misdiagnosed cases of GPT-4 and present the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18421\nanalysis in Figure 5 (by two radiologists).\nIn failed cases, 75% of the rationales’ contents are all\nmedically correct. Among them, some contain expressions\nthat generally will not be used in radiology reports, such\nas “Interestingly, ... ”(Inappropriate expression; 6.25%) or\nshow wrong usages of language due to the discrepancy be-\ntween clinical and general domains (Ambiguity; 6.25%). For\nexample, a rationale uses “positive sign” in a scenario that\nis “good for the patient”. However, this term is often used in\nthe context of “abnormal”.\nOnly 25% of them contain at least one medically incor-\nrect knowledge; This shows thatthe misdiagnosis is not nec-\nessarily caused by ineffective rationales. We presume opti-\nmizing the modeling on how to utilize rationales for the di-\nagnosis (e.g., dividing the generation of rationales and the\ndiagnosis into two separate stages) may possibly lead to bet-\nter performance. We leave this to future works.\nClinician Study: Quality of Rationales\nCriteria for Clinical Reasoning Rationales\nTo investigate the LLM’s role as a clinical reasoner, as-\nsessing if the quality of clinical rationales meets the stan-\ndards of clinicians, is of significant importance. Prior works\nhave incorporated human evaluations to assess the quality of\nmachine-generated rationales (Zelikman et al. 2022; Wang\net al. 2023). However, those works are limited to general\ndomains, i.e., commonsense reasoning, and mainly focused\non whether the rationales justify the target.\nIn this work, we and a group of licensed radiologists pro-\npose a novel set of criteria specifically designed to evaluate\nmachine-generated rationales for clinical diagnosis. We ex-\npect this to facilitate and benefit future research on eliciting\nrationales for clinical application.\n• CONSISTENCY : How much a generated rationale is not\ncontradictory to the presented data and model prediction\n(or the ground-truth diagnosis).\n• CORRECTNESS : How medically correct the knowledge\nreferred to in the rationale is.\n• SPECIFICITY : How detailed and specific the insights\nprovided in the generated rationale are.\n• HELPFULNESS : How much a clinical rationale benefits\nthe prediction towards the correct diagnosis.\n• HUMAN -LIKENESS : How well a clinical rationale\ndemonstrates the insight and understanding of the pre-\nsented patient description or diagnosis in a way that\nmatches the human behaviours.\nHuman Evaluation\nWe conduct human evaluation on 240 clinical reasoning ra-\ntionales that are “generated during the diagnosis” with two\nradiologists. The results are illustrated in Figure 6. To as-\nsess them more critically, we sample rationales from “chal-\nlenging cases” where “all 5 models” fail to predict the cor-\nrect diagnosis (referred to as “Misdiagnoses”), and an equal\namount of correctly diagnosed cases (referred to as “Cor-\nrect Diagnoses”). As a reference point, we apply the ratio-\nnalizing module (Module I) to generate their rationales with\naccess to ground-truth diagnosis (denoted as Ref ; gray-bar).\nGPT-4 faithfully reflects available clinical evidence. We\nobserve that GPT-4 and Ref perform similarly within cor-\nrect diagnoses. However, in Misdiagnoses group, rationales\ngenerated without access to ground-truth diagnosis (GPT-4)\nyield better scores even than those with access (with Ref ),\nin every criterion. This phenomenon indicates that when a\nground-truth diagnosis is challenging to predict based on the\ngiven patient description (i.e.,Misdiagnosis group), it is pos-\nsible that prior knowledge of the diagnosis may not be bene-\nficial for rationale annotation. We presume it stems from the\ndiscrepancy of available clinical evidence between radiolo-\ngists (who annotated the dataset in the real world) and our\nstudy. As a result, when asked to condition on the ground-\ntruth label, GPT-4 may operate as if it is being forced to\nconstruct a reasoning path that contradicts its understand-\ning of the available clinical evidence. Therefore, the supe-\nrior results of rationales generated without referencing to\nthe ground truth in the Misdiagnoses group (GPT-4; yellow\nbars) manefest that GPT-4 can faithfully reflect the observed\nclinical evidence in the rationales.\nKnowledge distillation enables better generalization.\nIntriguingly, the distilled student models also surpass Ref in\nMisdiagnoses cases. This implies that training with clinical\nrationales helps the student model to generate a more gener-\nalized rationale that is not biased toward a certain diagnosis.\nTheir better diagnostic performance than the LLM teacher\nsupports this finding (Table 2).\nOur framework elicits effective rationales for real-world\napplications. Overall, rationales generated by the teacher\nLLM (GPT-4) and student models receive scores higher than\n4 regarding almost every criterion (the lowest is 3.45 with\nthe score range being 0-5). The promising helpfulness and\ncorrectness scores in the misdiagnoses group match our pre-\nvious findings: Misdiagnoses may not necessarily stem from\nineffective rationales, but rather from how we model the\ncondition of rationales in diagnosis. Also, high specificity\nscores indicate that both the LLM and distilled students can\npresent detailed rationales concluding their observations and\ninsights (the average length of the clinical rationale in this\nstudy is 269.4 words).\nMost importantly, the outstanding human-likeness scores,\nespecially within the correct diagnoses, show that our ratio-\nnales can effectively replicate the clinical reasoning of ra-\ndiologists, and thus are more likely to seamlessly integrate\ninto real-world radiology reports.\nCase Study of Machine-generated Rationales\nWe highlight two important properties of the generated ratio-\nnales. We describe how they are aligned with the reasoning\nof radiologists, present the quotes from them, and elaborate\non how they can benefit DL-based diagnosis.\nInterpret clinical evidence contextually. The presence of\nthe APOE4 gene is a strong risk factor for susceptibility to\nAlzheimer’s disease. However, in situations where other evi-\ndence suggests that the patient is normal, radiologists do not\njust blindly rely on APOE4, instead, they comprehensively\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18422\nFigure 6: Evaluations on clinical rationales. We report the average score (score range: 0-5).\nconsider all the evidence based on their expertise and expe-\nriences. Our rationales exhibit the same behavior:\n“The patient carries one copy of APOE4 gene, which\nis known to increase the risk of AD. The absence\nof cognitive impairment symptoms and brain atrophy\nsuggest that this genetic risk has not led to any ap-\nparent neurodegeneration. ”\nIn conventional data-driven approaches for disease diagno-\nsis, i.e., text or image classification, it is non-trivial to anno-\ntate whether a certain feature is important in all possible sce-\nnarios. Our reasoning-aware diagnosis framework not only\nameliorates this need, but also provides rationales that repli-\ncate such mechanism of human radiologists. Hence, we pre-\nsume our framework has the potential to serve as a reliable\ntool for assisting clinicians in real-world data annotation.\nSelectively summarize important evidence. In practice,\nafter a thorough understanding and interpretation of all clin-\nical evidence, radiologists select meaningful findings from\ntheir observations to make an accurate judgement. Such\nsummarization of evidence can be found in our rationales:\n“... in summary, the patient’s cognitive decline, as ev-\nidenced by the MMSE score and the presence of mild\nhippocampal and severe atrophy in key areas related\nto memory, indicate a mild cognitive impairment. ”\nMachine-generated rationales are often too long because\nthey contain several sentences presenting the necessary in-\nformation. Liu et al. (2023) demonstrate that when LLMs are\nprocessing long text, information at the beginning or the end\nof the text can be more effectively utilized than those scat-\ntered around. Since today’s LLMs continue growing their\nability to process longer texts, being able to selectively sum-\nmarise necessary information at the end of the rationales\nsupports our framework’s usefulness for future LLM-based\nstudies in clinical domains.\nRelated Work\nAlzheimer’s Disease Diagnosis. Most DL-based methods\nfor AD formulate the diagnosis simply as image classifica-\ntion and address the performance via transfer learning from\ngeneral domains or tuning model architectures (Ebrahimi,\nLuo, and Chiong 2020; Jang and Hwang 2022). These ap-\nproaches focus on better extracting the image features. How-\never, AD diagnosis requires understanding and reasoning\nover a range of clinical data, such as APOE4 allele and\nMMSE alongside the MRIs (Budson and Solomon 2012;\nWeller and Budson 2018). To resolve that, several studies\nhave exploited different aspects or features of AD: Zhu et al.\n(2016) and Zhang et al. (2018) approach AD diagnosis with\nmultimodal data, e.g., positron emission tomography and ge-\nnetics data; Ong et al. (2023) leverage volume measurements\nof brain regions (e.g., subcortical volume) extracted from\nMRIs as additional training objectives alongside the classifi-\ncation of AD via multi-task learning. Although these studies\ndo facilitate diagnosis models to consider more aspects or\nfeatures of the disease, none of them provides a clear picture\nof the reasoning behind the diagnosis.\nClinical NLP. The success of LMs has sparked a surge in\napplying NLP techniques to the biomedical field (Lee et al.\n2020; Yue, Jimenez Gutierrez, and Sun 2020; Rajagopal\net al. 2021; Kim et al. 2023; Feng et al. 2023). For exam-\nple, Lee et al. (2020) fine-tune the commonly used BERT\nmodel (Kenton and Toutanova 2019) with medical corpus to\nendow it with biomedical knowledge, which is then imple-\nmented by Yue, Jimenez Gutierrez, and Sun (2020) to solve\nthe clinical reading comprehension task; Rajagopal et al.\n(2021) and Feng et al. (2023) address the generation of ex-\nplanations for various medical conditions via sequence-to-\nsequence language models with template-based approaches.\nMore recently, upon the advancements of LLMs, Agrawal\net al. (2022) have proposed to use LLMs to recognize named\nentities from clinical texts; Li et al. (2023) use LLMs to pre-\ndict the synergy of drug pairs in rare human tissues that lack\nstructured data and features.\nMost prior work heavily relies on the knowledge from the\npre-training corpus, ignoring whether the knowledge used is\ncorrect for the situation or follows human reasoning. In this\nwork, we address the absence of clinical reasoning, espe-\ncially in disease diagnosis, via large language models with\nprompt-based learning.\nConclusion\nWe present a reasoning-aware diagnosis framework to tar-\nget the absence of clinical reasoning in most prior works.\nUpon that, we investigate LLMs’ reasoning ability in clini-\ncal diagnosis via prompt-based learning and embark on var-\nious experiments with few-shot diagnosis and knowledge\ndistillation. As a formal study of clinical reasoning towards\nreal-world applications, we propose a series of novel criteria\nfor assessing the quality of machine-generated clinical ratio-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18423\nnales. These criteria can facilitate and benefit future work in\nthis area. Through human evaluation and extensive analysis\nof generated rationales, we establish a solid foundation for\nutilizing LLMs, both directly and indirectly, to model clini-\ncal reasoning in disease diagnosis.\nLimitations. Our study has the following limitations: (1)\nOur prompt used to invoke LLMs’ CoT reasoning only\ncontains two rationale demonstrations due to their length\n(around 260 words each). This can potentially affect models’\nperformance in rationale generation and diagnosis; (2) In our\nsettings, the clinical rationale and the name of the predicted\ndiagnosis are autoregressively generated. We do not explore\nother paradigms, such as jointly predicting them via multi-\ntask learning or dividing the rationale generation and diag-\nnosis into separate stages; (3) Although a group of licensed\nradiologists are involved in this study, we have not incor-\nporated this framework into real-world clinical settings; (4)\nWe do not include filtering mechanism to target ineffective\nrationales. Although our analysis shows that even in misdi-\nagnosed cases, 75% of the rationales are medically correct,\ntraining student models with such a collection of rationales\ncan potentially hinder their performance.\nEthical Statement\nAll of the patient data from both ADNI and AIBL are ap-\nproved by the institutional review boards and de-identified\nfor privacy. Additionally, patient information in all examples\nprovided in this paper is partially masked manually. Assess-\nment for potential societal impacts regarding data bias, ac-\ncountability, legal challenges, and so on, is necessary before\napplying our method to real clinical settings.\nAcknowledgements\nThis work is mainly supported by the Samsung Research\nFunding Center of Samsung Electronics (Project Num-\nber SRFC-TF2103-01), and partially supported by Insti-\ntute of Information & Communications Technology Plan-\nning & Evaluation (IITP) grant funded by the Korean\ngovernment (MSIT) (No. 2020-0-01361, Artificial Intelli-\ngence Graduate School Program (Yonsei University)) and\n(No.2021-0-02068, Artificial Intelligence Innovation Hub)\nand (No.2022-0-00077, AI Technology Development for\nCommonsense Extraction, Reasoning, and Inference from\nHeterogeneous Data). Jinyoung Yeo is a corresponding au-\nthor (jinyeo@yonsei.ac.kr).\nReferences\nAgrawal, M.; Hegselmann, S.; Lang, H.; Kim, Y .; and Son-\ntag, D. 2022. Large language models are few-shot clinical\ninformation extractors. In Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 1998–2022.\nBakator, M.; and Radosav, D. 2018. Deep learning and med-\nical diagnosis: A review of literature. Multimodal Technolo-\ngies and Interaction, 2(3): 47.\nBalogh, E. P.; Miller, B. T.; and Ball, J. R. 2015. Improving\ndiagnosis in health care.\nBudson, A. E.; and Solomon, P. R. 2012. New Criteria for\nAlzheimer’s disease and Mild Cognitive Impairment: Impli-\ncations for the Practicing Clinician. The neurologist, 18(6):\n356.\nCook, D. A.; Sherbino, J.; and Durning, S. J. 2018. Man-\nagement reasoning: beyond the diagnosis. Jama, 319(22):\n2267–2268.\nDeTure, M. A.; and Dickson, D. W. 2019. The neuropatho-\nlogical diagnosis of Alzheimer’s disease. Molecular neu-\nrodegeneration, 14(1): 1–18.\nEbrahimi, A.; Luo, S.; and Chiong, R. 2020. Introducing\ntransfer learning to 3D ResNet-18 for Alzheimer’s disease\ndetection on MRI images. In2020 35th international confer-\nence on image and vision computing New Zealand (IVCNZ),\n1–6. IEEE.\nEllis, K. A.; Bush, A. I.; Darby, D.; De Fazio, D.; Foster, J.;\nHudson, P.; Lautenschlager, N. T.; Lenzo, N.; Martins, R. N.;\nMaruff, P.; et al. 2009. The Australian Imaging, Biomarkers\nand Lifestyle (AIBL) study of aging: methodology and base-\nline characteristics of 1112 individuals recruited for a lon-\ngitudinal study of Alzheimer’s disease. International psy-\nchogeriatrics, 21(4): 672–687.\nFeng, S. Y .; Khetan, V .; Sacaleanu, B.; Gershman, A.; and\nHovy, E. 2023. CHARD: Clinical Health-Aware Reasoning\nAcross Dimensions for Text Generation Models. In Pro-\nceedings of the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics, 313–327.\nHara, K.; Kataoka, H.; and Satoh, Y . 2017. Learning\nspatio-temporal features with 3d residual networks for ac-\ntion recognition. In Proceedings of the IEEE international\nconference on computer vision workshops, 3154–3160.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network. arXiv:1503.02531.\nHolzinger, A.; Biemann, C.; Pattichis, C. S.; and Kell, D. B.\n2017. What do we need to build explainable AI systems for\nthe medical domain? arXiv:1712.09923.\nHsieh, C.-Y .; Li, C.-L.; Yeh, C.-k.; Nakhost, H.; Fujii, Y .;\nRatner, A.; Krishna, R.; Lee, C.-Y .; and Pfister, T. 2023. Dis-\ntilling Step-by-Step! Outperforming Larger Language Mod-\nels with Less Training Data and Smaller Model Sizes. In\nFindings of the Association for Computational Linguistics:\nACL 2023, 8003–8017. Toronto, Canada: Association for\nComputational Linguistics.\nJack Jr, C. R.; Bernstein, M. A.; Fox, N. C.; Thompson,\nP.; Alexander, G.; Harvey, D.; Borowski, B.; Britson, P. J.;\nL. Whitwell, J.; Ward, C.; et al. 2008. The Alzheimer’s dis-\nease neuroimaging initiative (ADNI): MRI methods. Jour-\nnal of Magnetic Resonance Imaging: An Official Journal\nof the International Society for Magnetic Resonance in\nMedicine, 27(4): 685–691.\nJang, J.; and Hwang, D. 2022. M3T: three-dimensional\nMedical image classifier using Multi-plane and Multi-slice\nTransformer. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 20718–20729.\nKassirer, J. P. 1989. Diagnostic reasoning.Annals of internal\nmedicine, 110(11): 893–900.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18424\nKenton, J. D. M.-W. C.; and Toutanova, L. K. 2019. Bert:\nPre-training of deep bidirectional transformers for language\nunderstanding. In Proceedings of NAACL-HLT, volume 1,\n2.\nKim, M.; Ong, K. T.-i.; Choi, S.; Yeo, J.; Kim, S.; Han, K.;\nPark, J. E.; Kim, H. S.; Choi, Y . S.; Ahn, S. S.; et al. 2023.\nNatural language processing to predict isocitrate dehydroge-\nnase genotype in diffuse glioma using MR radiology reports.\nEuropean Radiology, 33(11): 8017–8025.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large language models are zero-shot reason-\ners. Advances in neural information processing systems, 35:\n22199–22213.\nKumar, Y .; Koul, A.; Singla, R.; and Ijaz, M. F. 2022. Artifi-\ncial intelligence in disease diagnosis: a systematic literature\nreview, synthesizing framework and future research agenda.\nJournal of ambient intelligence and humanized computing ,\n1–28.\nLee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and\nKang, J. 2020. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinfor-\nmatics, 36(4): 1234–1240.\nLi, T.; Shetty, S.; Kamath, A.; Jaiswal, A.; Jiang, X.; Ding,\nY .; and Kim, Y . 2023. CancerGPT: Few-shot Drug Pair Syn-\nergy Prediction using Large Pre-trained Language Models.\narXiv:2304.10946.\nLi´evin, V .; Hother, C. E.; and Winther, O. 2023. Can\nlarge language models reason about medical questions?\narXiv:2207.08143.\nLiu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua,\nM.; Petroni, F.; and Liang, P. 2023. Lost in the Middle: How\nLanguage Models Use Long Contexts. arXiv:2307.03172.\nMitra, A.; Banerjee, P.; Pal, K. K.; Mishra, S.; and\nBaral, C. 2020. How Additional Knowledge can Im-\nprove Natural Language Commonsense Question Answer-\ning? arXiv:1909.08855.\nNorman, G. 2005. Research in clinical reasoning: past his-\ntory and current trends. Medical education, 39(4): 418–427.\nOng, K. T.-i.; Kim, H.; Kim, M.; Jang, J.; Sohn, B.; Choi,\nY . S.; Hwang, D.; Hwang, S. J.; and Yeo, J. 2023. Evidence-\nempowered Transfer Learning for Alzheimer’s Disease.\narXiv:2303.01105.\nOpenAI. 2023a. ChatGPT. https://openai.com/blog/chatgpt.\nOpenAI. 2023b. GPT-4 Technical Report.\nRajagopal, D.; Khetan, V .; Sacaleanu, B.; Gershman, A.;\nFano, A.; and Hovy, E. 2021. Cross-domain reasoning via\ntemplate filling. arXiv preprint arXiv:2111.00539.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open Foundation and Fine-Tuned\nChat Models. arXiv:2307.09288.\nTsimpoukelli, M.; Menick, J. L.; Cabi, S.; Eslami, S.;\nVinyals, O.; and Hill, F. 2021. Multimodal few-shot learning\nwith frozen language models. Advances in Neural Informa-\ntion Processing Systems, 34: 200–212.\nV oss, J. L.; Bridge, D. J.; Cohen, N. J.; and Walker, J. A.\n2017. A closer look at the hippocampus and memory.Trends\nin cognitive sciences, 21(8): 577–588.\nWang, P.; Wang, Z.; Li, Z.; Gao, Y .; Yin, B.; and Ren, X.\n2023. SCOTT: Self-Consistent Chain-of-Thought Distilla-\ntion. In Proceedings of the 61st Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Pa-\npers), 5546–5558. Toronto, Canada: Association for Com-\nputational Linguistics.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nWeller, J.; and Budson, A. 2018. Current under-\nstanding of Alzheimer’s disease diagnosis and treatment.\nF1000Research, 7.\nWu, D.; Zhang, J.; and Huang, X. 2023. Chain of Thought\nPrompting Elicits Knowledge Augmentation. In Findings of\nthe Association for Computational Linguistics: ACL 2023 ,\n6519–6534. Toronto, Canada: Association for Computa-\ntional Linguistics.\nYue, X.; Jimenez Gutierrez, B.; and Sun, H. 2020. Clinical\nReading Comprehension: A Thorough Analysis of the em-\nrQA Dataset. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 4474–4486.\nOnline: Association for Computational Linguistics.\nZelikman, E.; Wu, Y .; Mu, J.; and Goodman, N. 2022. Star:\nBootstrapping reasoning with reasoning. Advances in Neu-\nral Information Processing Systems, 35: 15476–15488.\nZhang, C.; Adeli, E.; Zhou, T.; Chen, X.; and Shen, D. 2018.\nMulti-Layer Multi-View Classification for Alzheimer’s Dis-\nease Diagnosis. In Proceedings of the Thirty-Second AAAI\nConference on Artificial Intelligence and Thirtieth Innova-\ntive Applications of Artificial Intelligence Conference and\nEighth AAAI Symposium on Educational Advances in Artifi-\ncial Intelligence, AAAI’18/IAAI’18/EAAI’18. AAAI Press.\nISBN 978-1-57735-800-8.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; Mi-\nhaylov, T.; Ott, M.; Shleifer, S.; Shuster, K.; Simig, D.;\nKoura, P. S.; Sridhar, A.; Wang, T.; and Zettlemoyer, L.\n2022. OPT: Open Pre-trained Transformer Language Mod-\nels. arXiv:2205.01068.\nZhang, Z.; Zhang, A.; Li, M.; Zhao, H.; Karypis, G.; and\nSmola, A. 2023. Multimodal Chain-of-Thought Reasoning\nin Language Models. arXiv:2302.00923.\nZheng, L.; Li, Z.; Zhang, H.; Zhuang, Y .; Chen, Z.; Huang,\nY .; Wang, Y .; Xu, Y .; Zhuo, D.; Xing, E. P.; et al. 2022.\nAlpa: Automating inter- and intra-operator parallelism for\ndistributed deep learning. In 16th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 22) ,\n559–578.\nZhu, X.; Suk, H.-I.; Lee, S.-W.; and Shen, D. 2016. Canon-\nical feature selection for joint regression and multi-class\nidentification in Alzheimer’s disease diagnosis. Brain imag-\ning and behavior, 10: 818–828.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18425"
}