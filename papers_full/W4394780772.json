{
  "title": "\"We Need Structured Output\": Towards User-centered Constraints on Large Language Model Output",
  "url": "https://openalex.org/W4394780772",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4222499440",
      "name": "Liu, Michael Xieyang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4222736723",
      "name": "Liu, Frederick",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": null,
      "name": "Fiannaca, Alexander J.",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2248068475",
      "name": "Koo, Terry",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2554804996",
      "name": "Dixon, Lucas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2214152049",
      "name": "Terry, Michael",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4223066156",
      "name": "Cai, Carrie J.",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4223908421",
    "https://openalex.org/W2164986850",
    "https://openalex.org/W4367185264",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2533819458",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4376652984",
    "https://openalex.org/W4387427818",
    "https://openalex.org/W2964029788",
    "https://openalex.org/W2945735543",
    "https://openalex.org/W4225165463",
    "https://openalex.org/W4389524085",
    "https://openalex.org/W4391940865",
    "https://openalex.org/W4320003957",
    "https://openalex.org/W4220955183",
    "https://openalex.org/W4289550786",
    "https://openalex.org/W4365601419",
    "https://openalex.org/W4387356345",
    "https://openalex.org/W4387995048",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4390214275",
    "https://openalex.org/W4366548599",
    "https://openalex.org/W4387947166",
    "https://openalex.org/W4389523912",
    "https://openalex.org/W4330336498",
    "https://openalex.org/W4389523811",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W4387596586",
    "https://openalex.org/W4388718089",
    "https://openalex.org/W3022607203"
  ],
  "abstract": "Large language models can produce creative and diverse responses. However, to integrate them into current developer workflows, it is essential to constrain their outputs to follow specific formats or standards. In this work, we surveyed 51 experienced industry professionals to understand the range of scenarios and motivations driving the need for output constraints from a user-centered perspective. We identified 134 concrete use cases for constraints at two levels: low-level, which ensures the output adhere to a structured format and an appropriate length, and high-level, which requires the output to follow semantic and stylistic guidelines without hallucination. Critically, applying output constraints could not only streamline the currently repetitive process of developing, testing, and integrating LLM prompts for developers, but also enhance the user experience of LLM-powered features and applications. We conclude with a discussion on user preferences and needs towards articulating intended constraints for LLMs, alongside an initial design for a constraint prototyping tool.",
  "full_text": "“We Need Structured Output”: Towards User-centered Constraints\non Large Language Model Output\nMichael Xieyang Liu\nGoogle Research\nPittsburgh, PA, USA\nlxieyang@google.com\nFrederick Liu\nGoogle Research\nSeattle, Washington, USA\nfrederickliu@google.com\nAlexander J. Fiannaca\nGoogle Research\nSeattle, Washington, USA\nafiannaca@google.com\nTerry Koo\nGoogle\nIndiana, USA\nterrykoo@google.com\nLucas Dixon\nGoogle Research\nParis, France\nldixon@google.com\nMichael Terry\nGoogle Research\nCambridge, Massachusetts, USA\nmichaelterry@google.com\nCarrie J. Cai\nGoogle Research\nMountain View, California, USA\ncjcai@google.com\nABSTRACT\nLarge language models can produce creative and diverse responses.\nHowever, to integrate them into current developer workflows, it\nis essential to constrain their outputs to follow specific formats\nor standards. In this work, we surveyed 51 experienced industry\nprofessionals to understand the range of scenarios and motivations\ndriving the need for output constraints from a user-centered per-\nspective. We identified 134 concrete use cases for constraints at two\nlevels: low-level, which ensures the output adhere to a structured\nformat and an appropriate length, and high-level, which requires\nthe output to follow semantic and stylistic guidelines without hal-\nlucination. Critically, applying output constraints could not only\nstreamline the currently repetitive process of developing, testing,\nand integrating LLM prompts for developers, but also enhance the\nuser experience of LLM-powered features and applications. We\nconclude with a discussion on user preferences and needs towards\narticulating intended constraints for LLMs, alongside an initial\ndesign for a constraint prototyping tool.\nCCS CONCEPTS\n• Human-centered computing →Empirical studies in HCI ;\nInteractive systems and tools .\nKEYWORDS\nLarge language models, Constrained generation, Survey\nACM Reference Format:\nMichael Xieyang Liu, Frederick Liu, Alexander J. Fiannaca, Terry Koo, Lucas\nDixon, Michael Terry, and Carrie J. Cai. 2024. “We Need Structured Output”:\nTowards User-centered Constraints on Large Language Model Output. In\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\n© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0331-7/24/05.\nhttps://doi.org/10.1145/3613905.3650756\nExtended Abstracts of the CHI Conference on Human Factors in Computing\nSystems (CHI EA ’24), May 11–16, 2024, Honolulu, HI, USA. ACM, New York,\nNY, USA, 9 pages. https://doi.org/10.1145/3613905.3650756\n1 INTRODUCTION\nOver the past few years, we have witnessed the extraordinary\ncapability of Large Language Models (LLMs) to generate responses\nthat are not only creative and diverse but also highly adaptable\nto various user needs [ 5, 7, 18, 21, 22, 29, 31, 32]. For example,\nresearchers can prompt ChatGPT [25] to condense long articles into\nconcise summaries for fast digestion; while video game developers\ncan generate detailed character profiles with rich personality traits,\nbackstories, and unique abilities on demand, simply by dynamically\nprompting an LLM with the game context and players’ preferences.\nAs much as end-users appreciate the unbounded creativity of\nLLMs, recent field studies examining the development of LLM-\npowered applications have repeatedly demonstrated the necessity\nto impose constraints on LLM outputs [10, 30]. For instance, a user\nmight require a summary of an article to be “strictly less than 20\nwords” to meet length constraints, or a generated video game char-\nacter profile to be “a valid JSON that can be parsed by Python” for\na development pipeline.\nHowever, as evidenced by many recent NLP benchmarks and\nevaluations [16, 36, 42, 43], current state-of-the-art LLMs still lack\nthe ability to guarantee that the generated output will invariably\nconform to user-defined constraints in the prompt (sometimes re-\nferred to as controllability). Although researchers have proposed\nvarious methods to improve controllability, such as supervised fine-\ntuning with specialized datasets [35] or controlled decoding strate-\ngies [1, 4, 24, 40], they tend to only focus on addressing a nar-\nrow range of constraints without taking into consideration the\ndiverse usage scenarios and rationale that real-world developers\nand end-users encounter when prototyping and building practical\nLLM-powered functionalities and applications [12–14, 23].\nIn this work, we took the first step to systematically investigate\nthe scenarios and motivations for applying output constraints from\na user-centered perspective . Specifically, we sought to understand:\narXiv:2404.07362v1  [cs.HC]  10 Apr 2024\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Liu et al.\n•RQ1: What real-world use cases would necessitate or benefit\nfrom being able to constrain LLM outputs?\n•RQ2: What are the benefits and impacts of being able to apply\nconstraints to LLM outputs?\n•RQ3: How would users like to articulate their intended con-\nstraints to LLMs?\nWe investigated these research questions by distributing a survey\nto a broad population of industry professionals (software engineers,\nresearchers, designers, project managers, etc.) who have experi-\nence building LLM-powered applications. Our analysis identified\nsix primary categories of output constraints that users desire, each\nsupported by detailed usage scenarios and illustrative examples,\nsummarized in Table 1. In a nutshell, users not only need low-level\nconstraints, which mandate the output to conform to a structured\nformat and an appropriate length, but also desire high-level con-\nstraints, which involve semantic and stylistic guidelines that users\nwould like the model output to adhere to without hallucinating.\nNotably, developers often have to write complex code to handle\nill-formed LLM outputs, a chore that could be simplified or elimi-\nnated if LLMs could strictly follow output constraints. In addition,\nthe ability to apply constraints could help ease the integration of\nLLMs with existing pipelines, meet UI and product specifications,\nand enhance user trust and experience with LLM-powered features.\nMoreover, we discovered that describing constraints in natural lan-\nguage (NL) within prompts is not always the preferred method of\ncontrol for LLM users. Instead, they seek alternative mechanisms,\nsuch as using graphical user interfaces, or GUIs, to define and test\nconstraints, which could offer greater flexibility and a heightened\nsense of assurance that the constraints will be strictly followed.\nInformed by these results, we present an early design of Con-\nstraintMaker, a prototype tool that enables LLM users to experi-\nment, test, and apply constraints on the format of LLM outputs (see\nFigure 2 for more details), along with feedback and insights from\npreliminary user tests. Overall, this paper contributes:\n•a comprehensive taxonomy summarizing both low-level and\nhigh-level output constraints desired by LLM users (Table 1),\nderived from 134 real-world use cases reported by our survey\nrespondents (RQ1),\n•an overview of both developer and user-facing benefits of\nbeing able to impose constraints on LLM outputs (RQ2),\n•an exploration of LLM users’ preferences for expressing con-\nstraints, whether via GUIs or natural language (RQ3),\n•a initial design of the tool ConstraintMaker, which enables\nusers to visually prototype LLM output constraints, accompa-\nnied by a discussion of preliminary user feedback.\n2 SURVEY WITH INDUSTRY PROFESSIONALS\nMethodology. To get a broad range of insights from people who\nhave experience with prompting and building LLM-powered ap-\nplications, we deployed an online survey to users of an internal\nprompt-based prototyping platform (similar to the OpenAI API\nPlayground [28] and Google AI studio [11]) at a large technology\ncompany for two weeks during Fall 2023. We chose this platform\nbecause it was explicitly designed to lower the barriers to entry\ninto LLM prompting and encourage a broader population (beyond\nmachine learning professionals) to prototype and develop LLM-\npowered applications. We publicized the survey through the plat-\nform’s user mailing list. We ran the survey for two weeks during Fall\n2023. Participants were rewarded $10 USD for their participation.\nThe survey was approved by our organization’s IRB.\nInstrument. Our survey started with questions concerning par-\nticipants’ background and technical proficiency, such as their job\nroles and their level of experience in designing and engineering\nLLM prompts. The survey subsequently investigated RQ1 and RQ2\nby asking participants to report three real-world use cases in which\nthey believe the implementation of constraints to LLM outputs is\nnecessary or advantageous. For each use case, they were encour-\naged to detail the specific scenario where they would like to apply\nconstraints, the type of constraint that they would prefer to imple-\nment, the degree of precision required in adhering to the constraint,\nand the importance of this constraint to their workflow. Finally,\nthe survey investigated RQ3 by asking participants to reflect on\nscenarios where they would prefer expressing constraints via a GUI\n(sliders, buttons, etc.) over natural language (in prompts, etc.) and\nvice versa, as well as any alternative ways they would prefer to ar-\nticulate constraints to LLMs. The GUI alternative draws inspiration\nfrom tools like the OpenAI Playground that allow users to adjust\nsettings like temperature and top-k through buttons, toggles, and\nsliders. Detailed survey questions are documented in section A of\nthe Appendix.\nResults. 51 individuals responded to our survey. Over half of the\nrespondents were software engineers (58.8%) across various product\nteams; others held a variety of roles like consultant & specialist\n(9.8%), analyst (7.8%), researcher (5.9%), UX engineer (5.9%), designer\n(3.9%), data scientist (3.9%), product manager (2.0%), and customer\nrelationship manager (2.0%). All respondents had experience with\nprompt design and engineering, with the majority reported having\nextensive experience (62.7%). The targeted audience and use cases\nof their prompts were split approximately evenly among consumers\nand end-users (33.3%), downstream development teams (31.4%), or\nboth (29.4%), with some created specifically for exploratory research\n& analysis (5.9%). Together, respondents contributed 134 unique\nuse cases of output constraints. To analyze the contents of the\nopen-ended responses, the first author read through all responses\nand used inductive analysis [34] to generate and refine themes for\neach research question with frequent discussions with the research\nteam. We present the resulting themes for each research question\nin sections 3-5.\nLimitations. Note that our findings largely capture the views\nof industry professionals, and may not encompass those of casual\nusers who engage with LLMs conversationally [25]. Additionally, as\nour respondent sample is limited to a single corporation, the results\ndescribed in the following sections may not be representative of the\nentire industry. Furthermore, our frequent use of open-ended ques-\ntions might have negatively impacted the response rate. However,\nthe saturation of novel findings and insights towards the end of the\nsurvey deployment suggests that we have successfully captured a\ncomprehensive range of perspectives.\n“We Need Structured Output”: Towards User-centered Constraints on Large Language Model Output CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\nCategory % Representative Examples Precision\nLow-level constraints\nStructured\nOutput\nFollowing standardized or\ncustom format or template\n(e.g., markdown, HTML,\nDSL, bulleted list, etc.)\n26.1% “Summarizing meeting notes into markdown format”\n“... the chatbot should quote dialogues, use special marks for scene description, etc. ”\n“I want the output to be in a specific format for a list of characteristics [of a movie] to then\neasily parse and train on. ”\nExact\nEnsuring valid JSON object\n(with custom schema)\n16.4% “... use the JSON output of the LLM to make an http request with that output as a payload. ”\n“I want to have the output [of the quiz] to be like a json with keys{\"question\": \"...\",\n\"correct_answer\": \"...\", \"incorrect_answers\": [...]}”\nExact\nMultiple\nChoice\nSelecting from a prede-\nfined list of options\n23.9% “Classifying student answers as right / wrong / uncertain...)”\n“While doing sentimental analysis, [...] restrict my output to few fixed set of classes like\nPositive, Negative, Neutral, Strongly Positive, etc. ”\nExact\nLength\nConstraints\nSpecifying the targeted\nlength (e.g., # of characters\n/ words, # of items in a list)\n16.4% “... Make each summary bullet LESS THAN 40 words. If you generate a bullet point that is\nlonger than 40 words, summarize and return a summary that is 40 words or less. ”\n“I want to limit the characters in the output to 100 so it is a valid YouTube Shorts title. ”\nApprox.\nHigh-level constraints\nSemantic\nConstraints\nExcluding specific terms,\nitems, or actions\n8.2% “Exclude PII (Personally Identifiable Information) and even some specific information... ”\n“If asking for html, do not include the standard html boilerplate (doctype, meta charset, etc.)\nand instead only provide the meaningful, relevant, unique code. ”\nExact\nIncluding or echoing\nspecific terms or content\n2.2% “... I want [the email] to include about thanking my manager and also talk about the\nlocation he is based on to help him feel relatable. ”\n“We want LLM to repeat input with some control tokens to indicate the mentions. e.g. input:\n‘Obama was born in 1961. ’,... , we want output to be ‘«Obama» was born in 1961. ”’\nExact\nCovering or staying on a\ncertain topic or domain\n2.2% “[The output of] a query about ‘fall jackets’ should be confined to clothing. ”\n“For ex. In India we have Jio and Airtel as 2 main telecom service provider. While building\nchat bot for Airtel, I would want the model to only respond [with] Airtel related topics. ”\nExact\nFollowing certain (code)\ngrammar / dialect / context\n4.5% “While generating SQL,... restrict the output to a particular dialect and use the table /\ndatabase name mentioned in the prompt. ”\n“... implement[ing] a voice assistant that calls specific methods with relevant arguments,...\nthe output needs to be valid syntax and only call the methods specified in the context”\nExact\nStylistic\nConstraints\nFollowing certain style,\ntone, or persona\n6.7% “... it is important that the [news] summary follow a style guide, ... for example, preference\nfor active voice over passive voice. ”\n“Use straightforward language and avoid complex technical jargon... ”\nApprox.\nPreventing\nHallucination\nStaying grounded and\ntruthful\n8.2% “... we do not want [a summary of the doc] to include opinions or beliefs but only real facts. ”\n“If the LLM can’t find a paper or peer-reviewed study, do not provide a hallucinated output. ”\nExact\nAdhering to instructions\n(without improvising\nunrequested actions)\n4.5% “For ‘please annotate this method with debug statements’, I’d like the output to ONLY\ninclude changes that add print statements... No other changes in syntax should be made. ”\n“LLMs usually ends up including an advice associated to the summarised topic, advice we\nneed to avoid so they are not part of the doc. ”\nExact\nTable 1: Taxonomy of the six primary categories of use cases of output constraints, derived from the 134 use cases that respondents submitted\n(RQ1). Totals add up to more than 100% since we placed some use cases into more than one category. The final column indicates whether the\noutput is expected to match the constraint exactly or approximately, as agreed upon by the majority of respondents.\n3 RQ1: REAL-WORLD USE CASES THAT\nNECESSITATE OUTPUT CONSTRAINTS\nTable 1 presents a taxonomy of six primary categories of use cases\nthat require output constraints, each with representative real-world\nexamples and quotes submitted by our respondents.\nThese can be further divided into low-level and high-level con-\nstraints — low-level constraints ensure that model outputs adhere\nto a specificstructure (e.g., JSON or markdown), instruct the model\nto perform pure multiple choices (e.g., sentiment classification),\nor dictate the length of the outputs; whereas high-level constraints\nenforce model outputs to respect semantic (e.g., must include or\navoid specific terms or actions) or stylistic (e.g., follow certain\nstyle or tone) guidelines, while preventing hallucination.\nBelow, we discuss a number of interesting insights that emerged\nfrom our analysis of the use cases:\n•Going beyond valid JSON. Note that recent advancements in\ninstruction-tuning techniques have substantially improved the\nthe chances of generating a valid JSON object upon user request\n[27, 29]. Nonetheless, our survey respondents believed that this\nwas not enough and desired to have more precise control\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Liu et al.\nDeveloper-facing Benefits User-facing Benefits\nIncreasing prompt-based development efficiency (§4.1) Satisfying product and UI requirements (§4.3)\nSpeeding up prompt design / engineering (less trial and error)\nReducing or eliminating ad-hoc parsing and plumbing logic\nSaving the cost of requesting multiple candidates\nFitting output into UI presets with size bounds\nEnsuring consistency of output length and format\nComplying with product and platform requirements\nStreamlining integration with downstream processes and workflows (§4.2) Improving user experience and trust (§4.4)\nEnsuring successful code execution\nImproving the quality of training data synthesis\nCanonizing output format across models\nEliminating safety and privacy concerns\nImproving user trust and confidence\nIncreasing customer satisfaction and adoption\nTable 2: Respondents’ perceived benefits of having the ability to apply constraints to LLM output (RQ2).\nover the JSON schema (i.e., key/value pairs) . One respondent\nstated their expectation as follows: “I expect the quiz [that the\nLLM makes given a few passages provided below] to have 1 correct\nanswer and 3 incorrect ones. I want to have the output to be like\na json with keys {\"question\": \"...\", \"correct_answer\":\n\"...\", \"incorrect_answers\": [...]}.” It is also worth men-\ntioning that some respondents found that “few-shot prompts ” —\ndemonstrating the desired key/value pairs with several exam-\nples — tend to work “fairly well ”. However, they concurred that\nhaving a formal guarantee of JSON schema would be greatly\nappreciated (see section 4.1 for their detailed rationales).\n•Giving an answer without extra conversational prose. When\nasking an LLM to perform data classification or labeling, such\nas “[classifying sentiments as] Positive, Negative, Neutral, etc., ”\nrespondents typically expect the model to only output the\nclassification result (e.g. “Positive.”) without a trailing\n“explanation” (e.g., “Positive, since it referred to the\nmovie as a ‘timeless masterpiece’...”), as the addition of\nexplanation could potentially confuse the downstream parsing\nlogic. This indicates a potential misalignment between a com-\nmon training objective — where LLMs are often tailored to be\nconversational and provide rich details [2, 17, 33] — and certain\nspecialized downstream use cases where software developers\nneed LLMs to be succinct. Such use cases necessitate output con-\nstraints that are independent of the prompt that would help adapt\na general-purpose model to meet specific user requirements.\n•Conditioning the output on the input, but don’t “impro-\nvise!” One thread of high-level constraints places emphasis on\ndirecting the model to condition its output on specific con-\ntent from the input . For example, the model’s response should\nsemantically remain “in the same ballpark ” as “the user’s original\nquery” — “[the output of] a query about ‘fall jackets’ should be\nconfined to clothing. ” A particular instance of this is for LLMs\nto echo segments of the input in their output, occasionally with\nslight alterations. For example, “we want LLM to repeat input with\nsome control tokens to indicate the mentions. e.g. input: ‘Obama\nwas born in 1961. ’,... , we want output to be ‘«Obama» was born in\n1961. ’” Nevertheless, respondents underscored the importance\nof the model not improvising beyond its input and instruc-\ntions. For example, one respondent instructed an LLM to “an-\nnotate a method with debug statement, ” anticipating the output\nwould “ONLY include changes that add print statements to the\nmethod.” However, the LLM would frequently introduce addi-\ntional “changes in syntax ” that were unwarranted.\n4 RQ2: BENEFITS OF APPLYING\nCONSTRAINTS TO LLM OUTPUTS\nBeyond the aforementioned use cases, our survey respondents re-\nported a range of benefits that the ability of constraining LLM\noutput could offer. These include both developer-facing benefits,\nlike increasing prompt-based development efficiency and streamlin-\ning integration with downstream processes and workflows, as well\nas user-facing benefits, like satisfying product and UI requirements\nand improving user experience and trust of LLMs (Table 2). Here\nare the most salient responses:\n4.1 Increasing Prompt-based Development\nEfficiency\nFirst and foremost, being able to constrain LLM outputs can sig-\nnificantly increase the efficiency of prompt-based engineering and\ndevelopment by reducing the trial and error currently needed to\nmanage LLM unpredictability. Developers noted that the process\nof “defining the [output] format ” alone is “time-consuming,” often\nrequiring extensive prompt testing to identify the most effective\none (consistent with what previous research has found [ 30, 41]).\nAdditionally, they often need to “request multiple responses ” and\n“iterating through them until find[ing] a valid one. ” Therefore, being\nable to deterministically constrain the output format could not only\nsave developers as much as “dozens of hours of work per week ” spent\non iterative prompt testing, but also reduce overall LLM inference\ncosts and latency.\nAnother common practice that respondents reported is building\ncomplex infrastructure to post-process LLM outputs, sometimes\nreferred to as “massaging [the output] after receiving. ” For example,\ndevelopers oftentimes had to “chase down ‘free radicals’ when writ-\ning error handling functions, ” and felt necessary to include “custom\nlogic” for matching and filtering, along with “further verification. ”\nThus, setting constraints before LLM generation may be the key to\nreducing such “ad-hoc plumbing code ” post-generation, simplifying\n“maintenance,” and enhancing the overall “developer experience.” As\none respondent vividly described: “it’s a much nicer experience if\nit (formatting the output in bullets) ‘just works’ without having to\nimplement additional infra... ”\n“We Need Structured Output”: Towards User-centered Constraints on Large Language Model Output CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\n0 10 20 30 40 50\nCount\n1. Output should be exactly 3 words, no more than 3 paragraphs, etc.\n2. Output in a speciﬁc format or structure (e.g., JSON, XML, bulleted / ordered list)\n3. Only output \"left-handed\", \"right-handed\", or \"ambidextrous\"\n4. Output must include or avoid certain words / phrases\n5. Output must cover or avoid certain topics, only use certain libraries when generating code, etc.\n6. Output style should mimic Yoda / Shakespeare / certain personas, etc.\n1 (Strongly prefer NL)\n2\n3\n4 (Neutral)\n5\n6\n7 (Strongly prefer GUI)\nResponse\nFigure 1: Respondents’ preferences towards specifying output constraints either through natural language or GUI (RQ3). Participants were\nasked to score each question (left) on a 7-point Likert scale from “1 (Strongly prefer NL)” to “7 (Strongly prefer GUI). ”\n4.2 Integrating with Downstream Processes and\nWorkflows\nBecause LLMs are often used as sub-components in larger pipelines,\nrespondents emphasized that guaranteed constraints are critical to\nensuring that the output of their work is compatible with down-\nstream processes, such as downstream modules that expect a spe-\ncific format or functional code as input. Specifically for code gen-\neration, they highlighted the necessity of constraining the output\nto ensure “executable” code that adheres to only “methods specified\nin the context ” and avoids errors, such as hallucinating “ unsup-\nported operators ” or “SQL ... in a different dialect. ” Note that while\nthe “function calling” features in the latest LLMs [8, 26] can “select”\nfunctions to call from a predefined list, users still have to implement\nthese functions correctly by themselves.\nMany studies indicate that LLMs are highly effective for creating\nsynthetic datasets for AI training [9, 15, 38], and our survey respon-\ndents postulated that being able to impose constraints on LLMs\ncould improve the datasets’ quality and integrity. For instance, one\nrespondent wished that model-generated movie data would “not\nsay a movie’s name when it describes its plot, ” as they were going to\ntrain using this data for a “predictive model of the movie itself. ” Any\nbreach of such constraints could render the data “unusable.”\nFurthermore, given the industry trend of continuously migrating\nto newer, more cost-effective models, respondents highlighted the\nimportance of “canonizing” constraints across models to avoid extra\nprompt-engineering after migration (e.g., “if I switch model, I get\nthe formatting immediately ”). This suggests that it could be more\nadvantageous for models to accept output constraints independent\nof the prompt, which should now solely contain task instructions.\n4.3 Satisfying UI and Product Requirements\nRespondents stressed that it is essential to constrain LLM output to\nmeet UI and product specifications, particularly when such output\nwill be presented to end users, directly or indirectly. A common\ncase is to incorporate LLM-generated content into UI elements\nthat “cannot exceed certain bounds ”, necessitating stringent length\nconstraints. Content that doesn’t “ fit within the UI ” usually gets\n“thrown away ” all together, a concern likely to be more pronounced\non mobile devices with limited screen real estate [6, 20]. Maintain-\ning the consistency of output length and format was also considered\nimportant, as “too much variability in the generated text can be over-\nwhelming to the user and clutter the UI. ” Moreover, being able to\nconstrain length can help LLMs comply with specific platform char-\nacter restrictions, like tweets capped at 280 characters or YouTube\nShorts titles limited to 100 characters.\n4.4 Improving User Experience, Trust, and\nAdoption\nFinally, respondents suggested that developing LLM-powered user\nexperiences requires constraint mechanisms to mitigate hallucina-\ntions, foster user trust, and ultimately drive “user adoption. ” One\nprominent aspect is to reduce safety and privacy concerns, for\ninstance, by preventing LLMs from “ repeat[ing] existing or hal-\nlucinat[ing] PII (personally identifiable information). ” In addition,\nrespondents expressed a desire to ensure user trust and confidence\nof LLM-powered tools and systems, arguing that, for example, “hal-\nlucinations in dates are easy to identify ” and, in general, “users won’t\ninvest more time into tools that aren’t accurate. ”\n5 HOW TO ARTICULATE OUTPUT\nCONSTRAINTS TO LLMS\nFig. 1 shows distributions of respondents’ preferences towards spec-\nifying output constraints either through GUI or natural language.\nAn overarching observation is that respondents preferred using\nGUI to specify low-level constraints and natural language to\nexpress high-level constraints . We discuss their detailed ratio-\nnale below:\n5.1 The case for GUI: A Quick, Reliable, and\nFlexible Way of Prototyping Constraints\nFirst and foremost, respondents considered GUIs particularly ef-\nfective for defining “hard requirements, ” providing more reliable\nresults, and reducing ambiguity compared to natural language in-\nstructions. For example, one argued that choosing “boolean” as the\noutput type via a GUI felt much more likely to be “honoured” com-\npared to “typ[ing] that I want a Yes / No response [...] in a prompt. ”\nAnother claimed that “flagging a ‘JSON’ button ” provides a much\nbetter user experience than “typing ‘output as JSON’ across multiple\nprompts.” In addition, respondents preferred using GUI when the\nintended constraint is “objective” and “quantifiable”, such as “use\nonly items x,y,z, ” or “a JSON with certain fields specified. ” Moreover,\nrespondents found GUI to be more flexible for rapid prototyping\nand experimentation (e.g., “when I want to play around with dif-\nferent numbers, moving a slider around seems easier than typing ”).\nFinally, for novice LLM users, the range of choices afforded by a GUI\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Liu et al.\nGenerate a character profile for a video game, \nincluding the character's name, age, names of their \nthree children, and whether they can be controlled by \nthe player.\nWrite your prompt:\nOutput:\nGenerate 5 cities that have access to a beach:\nClassify the sentiment of the following movie \nreview: [[Twilight (2008) is a teen romance \nthat failed to live up to the hype…]]\n1\n2\n3\n4\n2a\n2b\n2d\n2c\n5\n6\n2e\nFigure 2: ConstraintMaker’s user interfaces (1-4) & use cases (5-6). After writing the prompt (1), users can easily specify output constraints\nusing a graphical user interface (2 & 3) provided by ConstraintMaker, and the resulting output (4) is guaranteed to follow the constraints.\nAdditional details of this process is discussed in section 6.\nconstraint can help clarify the model’s capabilities and limitations,\n“making the model seems less like a black box. ” One respondent drew\nfrom their experience working with text-to-image models to under-\nscore this point: “by seeing “Illustration” as a possible output style\n[among others like “Photo realistic” or “Cartoon”], I became aware of\n[the model’s] capabilities. ”\n5.2 The Case for NL: More Intuitive and\nExpressive for Complex Constraints\nRespondents found natural language easier for specifying complex\nconstraints than GUIs, especially for extended background contexts\nor numerous choices that wouldn’t reasonably fit into a GUI. Natural\nlanguage was also preferred for expressing vague, nuanced, or open-\nended constraints, like “don’t include offensive words ” or “respond\nin a cheerful manner. ” At a high level, respondents emphasized that\nnatural language provides a more natural, familiar, and expressive\nway to communicate (potentially multiple) complex constraints,\nand, “trying to figure out how to use a GUI might be more tedious. ”\nAdditionally, some respondents noted that, despite their prefer-\nence for using GUIs to define constraints from time to time, they\nultimately have to use natural language prompts due to API lim-\nitations. Moreover, some wished to reference “external resources ”\nin constraints that are not feasible to directly include in prompts\n(e.g., “a large database / vocabulary ”). These suggest that a dedicated\n“output-constraints” API field for specifying constraints could be\nadvantageous, potentially through the use of a formal language or\nnotation.\n6 THE CONSTRAINTMAKER TOOL\nInformed by the survey results, we developed a web-based GUI,\nConstraintMaker (Fig. 2), that enables LLM users to prototype,\ntest, and apply constraints on the format of LLM outputs. With\nConstraintMaker, users can specify different types of output\nconstraints by simply selecting from the list of available constraint\nprimitives (Fig. 2-2b). If needed, users can click the pencil icon (Fig.\n2-2a) to further edit the details of a constraint primitive, such as\nspecifying the schema of a JSON object (Fig. 2-3). Users also have\nthe flexibility to mix and match multiple constraint primitives to-\ngether (e.g., Fig. 2-6) to form more complex constraints. Currently,\nbased on users’ needs and priorities identified by the survey, Con-\nstraintMaker initially supports JSON object , Multiple choice ,\nList , Ordered list , and Some text as primitives (Fig. 2-2b), where\nSome text asks the LLM to generate freely as it normally would.\nUnder the hood, we used a GPT-3.5-class LLM. We addition-\nally implemented a finite-state machine-based decoding technique\nakin to that outlined in [ 39], ensuring the language model out-\nputs strictly adhere to formats defined by a specialized regular\nexpression (henceforth, “regex”). In fact, ConstraintMaker auto-\nmatically converts a GUI-defined constraint into a regex (Fig. 2-2d),\nwhich the LLM observes during generation (Fig. 2-4).\n6.1 Iterative Design and User Feedback\nTo explore the usability and usefulness of ConstraintMaker, we\nconducted a series of informal (around 30 minute each) user tests\nwith five participants who self-identified as experts in prompting\nLLMs, as well as self-experimentation among the authors. We used\nthe feedback from these sessions to iteratively refine the design\nof ConstraintMaker. We present some interesting findings and\nreflections below:\n6.1.1 ConstraintMaker enables an intuitive separation of concerns.\nWith ConstraintMaker, one can now specify “the tasks they want\nthe model to perform ” separate from “the expected format of the\noutput,” an approach participants considered more intuitive and\neffective in steering LLMs to consistently achieve desired results\ncompared to traditional prompting. Additionally, participants envi-\nsioned the possibility of reusing constraints across various prompts,\nwhich could reduce the effort of crafting new constraints for similar\ntasks and eliminate the need for prompt engineering post model\nmigration.\n6.1.2 Constraint-prototyping GUI needs to cater to both developers\nand non-developers. On the one hand, for non-technical users inter-\nested in experimenting with constraints, we noticed that the visible\n“We Need Structured Output”: Towards User-centered Constraints on Large Language Model Output CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\nregex alongside the constraint primitive GUI was somewhat distract-\ning. To address this, we added a feature that allows the regex to be\ntoggled as hidden via the “< >” button (Fig. 2-2c). On the other hand,\nfor more advanced users (e.g., developers), we observed a frequent\nneed to make fine-grained adjustments to the underlying regex\nafter creating an initial draft with theConstraintMaker GUI (e.g.,\nchanging the “bullet” of a “bulleted list” from the default “−[...]”\n(Fig. 2-5) to “* [...]”). As a result, we enabled direct manipulation\nof the regex by toggling on \"Edit constraints manually\" (Fig. 2-2e).\n6.1.3 “Inserting” words among constraints. For example, one par-\nticipant asked in the prompt for the LLM to first write a paragraph\ndescribing a short story, followed by a list of suggestions on how\nto improve the story. In situations like this, they found that em-\nbedding specific words into the constraints, such as “Short story:\nSome text ” followed by “Suggestions: List ”, yielded better-quality\nresults than simply using Some text followed by List alone. There-\nfore, we introduced the Exact text GUI primitive, enabling the\nLLM to insert user-prescribed text into its output.\n6.1.4 Automatically inferring constraints based on prompts. One\ninteresting feature request for ConstraintMaker is the ability to\nautomatically infer constraints from user-written prompts, simi-\nlar to previous intelligent prediction or auto-completion systems\nand tools [3, 19, 37]. For instance, for a prompt shown in Fig. 2-1,\nConstraintMaker could proactively suggest to the users if they’d\nlike to constrain the model output to a JSON object with specific\nfields. This feature would be appealing, given the current some-\nwhat cumbersome process of manually creating and modifying\nconstraints from scratch. Similar to code auto-completion, partici-\npants suggested that constraint auto-completion could streamline\nthe overall experience of defining constraints. Additionally, auto-\nmatic constraint suggestions could serve as learning opportunities\nfor novice users to become familiar with the range of possibilities\nthat ConstraintMaker affords, which would be particularly use-\nful in future versions where the tool might support a wider selection\nof constraint primitives. Finally, proactively suggesting constraints\ncould promote a “constraint mindset. ” This encourages users to\nalways consider the output format before deploying a prompt, lead-\ning to more rigorous and controllable prompt engineering, much\nlike conventional software development.\n7 CONCLUSION\nIn this work, we introduced a user-centered taxonomy of real-world\nscenarios, benefits, and preferred methods for applying constraints\non LLM outputs, offering both a theoretical framework and prac-\ntical insights into user requirements and preferences. In addition,\nwe presented ConstraintMaker, an early GUI-based tool that en-\nables users to prototype and test output constraints iteratively. Our\nresults shed light on the future of more controllable, customizable,\nand user-friendly interfaces for human-LLM interactions.\nREFERENCES\n[1] 2023. guidance-ai/guidance. https://github.com/guidance-ai/guidance original-\ndate: 2022-11-10T18:21:45Z.\n[2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova\nDasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas\nJoseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson\nElhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston,\nShauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei,\nTom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared\nKaplan. 2022. Training a Helpful and Harmless Assistant with Reinforcement\nLearning from Human Feedback. https://doi.org/10.48550/arXiv.2204.05862\narXiv:2204.05862 [cs].\n[3] Ziv Bar-Yossef and Naama Kraus. 2011. Context-sensitive query auto-completion.\nIn Proceedings of the 20th international conference on World wide web (WWW ’11) .\nAssociation for Computing Machinery, New York, NY, USA, 107–116. https:\n//doi.org/10.1145/1963405.1963424\n[4] Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. 2023. Prompting Is\nProgramming: A Query Language for Large Language Models. Proceedings\nof the ACM on Programming Languages 7, PLDI (June 2023), 186:1946–186:1969.\nhttps://doi.org/10.1145/3591300\n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nhttps://doi.org/10.48550/arXiv.2005.14165 arXiv:2005.14165 [cs].\n[6] Joseph Chee Chang, Nathan Hahn, and Aniket Kittur. 2016. Supporting Mobile\nSensemaking Through Intentionally Uncertain Highlighting. In Proceedings of\nthe 29th Annual Symposium on User Interface Software and Technology (UIST ’16) .\nACM, New York, NY, USA, 61–68. https://doi.org/10.1145/2984511.2984538\n[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,\nAbhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay\nGhemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz,\nOrhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling\nwith Pathways. https://doi.org/10.48550/arXiv.2204.02311 arXiv:2204.02311 [cs].\n[8] Google Cloud. 2023. Function calling | Vertex AI. https://cloud.google.com/\nvertex-ai/docs/generative-ai/multimodal/function-calling\n[9] Ronen Eldan and Yuanzhi Li. 2023. TinyStories: How Small Can Language Models\nBe and Still Speak Coherent English? https://doi.org/10.48550/arXiv.2305.07759\narXiv:2305.07759 [cs].\n[10] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta,\nShin Yoo, and Jie M. Zhang. 2023. Large Language Models for Software Engi-\nneering: Survey and Open Problems. https://doi.org/10.48550/arXiv.2310.03533\narXiv:2310.03533 [cs].\n[11] Google. 2023. Google AI Studio quickstart. https://ai.google.dev/tutorials/ai-\nstudio_quickstart\n[12] Chris Hokamp and Qun Liu. 2017. Lexically Constrained Decoding for Sequence\nGeneration Using Grid Beam Search. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers) , Regina\nBarzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics,\nVancouver, Canada, 1535–1546. https://doi.org/10.18653/v1/P17-1141\n[13] J. Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt\nPost, and Benjamin Van Durme. 2019. Improved Lexically Constrained Decoding\nfor Translation and Monolingual Rewriting. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers) , Jill Burstein,\nChristy Doran, and Thamar Solorio (Eds.). Association for Computational Lin-\nguistics, Minneapolis, Minnesota, 839–850. https://doi.org/10.18653/v1/N19-1090\n[14] Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach,\nMichael Terry, and Carrie J Cai. 2022. PromptMaker: Prompt-based Prototyping\nwith Large Language Models. In Extended Abstracts of the 2022 CHI Conference on\nHuman Factors in Computing Systems (CHI EA ’22) . Association for Computing\nMachinery, New York, NY, USA, 1–8. https://doi.org/10.1145/3491101.3503564\n[15] Martin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. 2023. Exploit-\ning Asymmetry for Synthetic Training Data Generation: SynthIE and the Case\nof Information Extraction. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing , Houda Bouamor, Juan Pino, and Kalika\nBali (Eds.). Association for Computational Linguistics, Singapore, 1555–1574.\nhttps://doi.org/10.18653/v1/2023.emnlp-main.96\n[16] Minsuk Kahng, Ian Tenney, Mahima Pushkarna, Michael Xieyang Liu, James\nWexler, Emily Reif, Krystal Kallarackal, Minsuk Chang, Michael Terry, and Lu-\ncas Dixon. 2024. LLM Comparator: Visual Analytics for Side-by-Side Evalu-\nation of Large Language Models. https://doi.org/10.48550/arXiv.2402.10524\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Liu et al.\narXiv:2402.10524 [cs].\n[17] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\nhiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,\nBenjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,\nChristopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson,\nEric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu\nYao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul,\nMirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya\nGanguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary,\nWilliam Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023.\nHolistic Evaluation of Language Models. https://doi.org/10.48550/arXiv.2211.\n09110 arXiv:2211.09110 [cs].\n[18] Michael Xieyang Liu. 2023. Tool Support for Knowledge Foraging, Structuring,\nand Transfer during Online Sensemaking . Ph. D. Dissertation. Carnegie Mellon\nUniversity. http://reports-archive.adm.cs.cmu.edu/anon/anon/usr0/ftp/usr/ftp/\nhcii/abstracts/23-105.html\n[19] Michael Xieyang Liu, Aniket Kittur, and Brad A. Myers. 2022. Crystalline: Low-\nering the Cost for Developers to Collect and Organize Information for Decision\nMaking. In Proceedings of the 2022 CHI Conference on Human Factors in Computing\nSystems (CHI ’22) . Association for Computing Machinery, New York, NY, USA.\nhttps://doi.org/10.1145/3491102.3501968 event-place: New Orleans, LA, USA.\n[20] Michael Xieyang Liu, Andrew Kuznetsov, Yongsung Kim, Joseph Chee Chang,\nAniket Kittur, and Brad A. Myers. 2022. Wigglite: Low-cost Information Collection\nand Triage. In The 35th Annual ACM Symposium on User Interface Software and\nTechnology (UIST ’22) . Association for Computing Machinery, New York, NY,\nUSA. https://doi.org/10.1145/3526113.3545661\n[21] Michael Xieyang Liu, Advait Sarkar, Carina Negreanu, Benjamin Zorn, Jack\nWilliams, Neil Toronto, and Andrew D. Gordon. 2023. “What It Wants Me To\nSay”: Bridging the Abstraction Gap Between End-User Programmers and Code-\nGenerating Large Language Models. In Proceedings of the 2023 CHI Conference\non Human Factors in Computing Systems (CHI ’23) . Association for Computing\nMachinery, New York, NY, USA, 1–31. https://doi.org/10.1145/3544548.3580817\n[22] Michael Xieyang Liu, Tongshuang Wu, Tianying Chen, Franklin Mingzhe Li,\nAniket Kittur, and Brad A. Myers. 2023. Selenite: Scaffolding Online Sensemaking\nwith Comprehensive Overviews Elicited from Large Language Models. https:\n//doi.org/10.48550/arXiv.2310.02161\n[23] Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi,\nRonan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin\nChoi. 2022. NeuroLogic A*esque Decoding: Constrained Text Generation with\nLookahead Heuristics. In Proceedings of the 2022 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir\nMeza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United\nStates, 780–799. https://doi.org/10.18653/v1/2022.naacl-main.57\n[24] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping\nHuang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin\nChen, Alex Beutel, and Ahmad Beirami. 2023. Controlled Decoding from Lan-\nguage Models. https://doi.org/10.48550/arXiv.2310.17022 arXiv:2310.17022\n[cs].\n[25] OpenAI. 2023. ChatGPT. https://chat.openai.com\n[26] OpenAI. 2023. Function calling | OpenAI Platform. https://platform.openai.com/\ndocs/guides/function-calling\n[27] OpenAI. 2023. JSON mode - Text generation. https://platform.openai.com/docs/\nguides/text-generation/json-mode\n[28] OpenAI. 2023. Playground - OpenAI API. https://platform.openai.com/\nplayground\n[29] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Train-\ning language models to follow instructions with human feedback. https:\n//doi.org/10.48550/arXiv.2203.02155 arXiv:2203.02155 [cs].\n[30] Chris Parnin, Gustavo Soares, Rahul Pandita, Sumit Gulwani, Jessica Rich, and\nAustin Z. Henley. 2023. Building Your Own Product Copilot: Challenges, Oppor-\ntunities, and Needs. https://doi.org/10.48550/arXiv.2312.14231 arXiv:2312.14231\n[cs].\n[31] Savvas Petridis, Michael Terry, and Carrie Jun Cai. 2023. PromptInfuser: Bring-\ning User Interface Mock-ups to Life with Large Language Models. In Extended\nAbstracts of the 2023 CHI Conference on Human Factors in Computing Systems\n(CHI EA ’23) . Association for Computing Machinery, New York, NY, USA, 1–6.\nhttps://doi.org/10.1145/3544549.3585628\n[32] Savvas Petridis, Ben Wedin, James Wexler, Aaron Donsbach, Mahima Pushkarna,\nNitesh Goyal, Carrie J. Cai, and Michael Terry. 2023. ConstitutionMaker: Interac-\ntively Critiquing Large Language Models by Converting Feedback into Principles.\nhttps://doi.org/10.48550/arXiv.2310.15428 arXiv:2310.15428 [cs].\n[33] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea\nVoss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to\nsummarize with human feedback. In Advances in Neural Information Processing\nSystems, Vol. 33. Curran Associates, Inc., 3008–3021. https://proceedings.neurips.\ncc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html\n[34] Anselm Strauss and Juliet Corbin. 1990. Basics of qualitative research . Sage\npublications.\n[35] Jiao Sun, Xuezhe Ma, and Nanyun Peng. 2021. AESOP: Paraphrase Generation\nwith Adaptive Syntactic Control. In Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing , Marie-Francine Moens, Xuanjing\nHuang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Compu-\ntational Linguistics, Online and Punta Cana, Dominican Republic, 5176–5189.\nhttps://doi.org/10.18653/v1/2021.emnlp-main.420\n[36] Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John\nWieting, Nanyun Peng, and Xuezhe Ma. 2023. Evaluating Large Language Models\non Controlled Generation Tasks. InProceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing , Houda Bouamor, Juan Pino, and Kalika\nBali (Eds.). Association for Computational Linguistics, Singapore, 3155–3168.\nhttps://doi.org/10.18653/v1/2023.emnlp-main.190\n[37] Jialiang Tan, Yu Chen, and Shuyin Jiao. 2023. Visual Studio Code in Introductory\nComputer Science Course: An Experience Report. https://doi.org/10.48550/arXiv.\n2303.10174 arXiv:2303.10174 [cs].\n[38] Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, and\nGraham Neubig. 2023. Prompt2Model: Generating Deployable Models from\nNatural Language Instructions. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations , Yansong Feng\nand Els Lefever (Eds.). Association for Computational Linguistics, Singapore,\n413–421. https://doi.org/10.18653/v1/2023.emnlp-demo.38\n[39] Brandon T. Willard and Rémi Louf. 2023. Efficient Guided Generation for Large\nLanguage Models. https://arxiv.org/abs/2307.09702v4\n[40] Nan Xu, Chunting Zhou, Asli Celikyilmaz, and Xuezhe Ma. 2023. Look-back\nDecoding for Open-Ended Text Generation. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing , Houda Bouamor, Juan Pino,\nand Kalika Bali (Eds.). Association for Computational Linguistics, Singapore,\n1039–1050. https://doi.org/10.18653/v1/2023.emnlp-main.66\n[41] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang.\n2023. Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design\nLLM Prompts. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems (CHI ’23) . Association for Computing Machinery, New York,\nNY, USA, 1–21. https://doi.org/10.1145/3544548.3581388\n[42] Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen.\n2023. Evaluating Large Language Models at Evaluating Instruction Following.\nhttps://doi.org/10.48550/arXiv.2310.07641 arXiv:2310.07641 [cs].\n[43] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi\nLuan, Denny Zhou, and Le Hou. 2023. Instruction-Following Evaluation for Large\nLanguage Models. https://doi.org/10.48550/arXiv.2311.07911 arXiv:2311.07911\n[cs].\n“We Need Structured Output”: Towards User-centered Constraints on Large Language Model Output CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\nA THE SURVEY INSTRUMENT\nIn this section, we detail the design of our survey. The survey starts with questions about background and self-reported technical proficiency:\n•What best describes your job role: software Engineer; research scientist; UX designer; UX researcher; product manager; technical\nwriter; other (open-ended)\n•To what extent have you designed LLM prompts: a) I have “chatted with” chatbots like Bard / ChatGPT as a user; b) I’ve tried making\na prompt once or twice just to check it out, but haven’t done much prompt design / engineering; c) I have some experience doing\nprompt design / engineering on at least three LLM prompts; d) I have done extensive prompt design / engineering to accomplish\ndesired functionality. Only those participants who selected either option c) or d) were given the opportunity to continue with the\nremainder of the survey. This approach is specifically designed to exclude “casual” LLM users.\n•I primarily design prompts with the intent that they will be used by: a) consumers / end-users (e.g. a recipe idea generator); b)\ndownstream development teams (e.g. captioning, classifiers); c) both, I split my time about evenly between the two; d) other audience\nor use cases (open response).\nThe survey then asked participants to report three real-world use cases where they would like to constrain LLM outputs. For each use case,\nparticipants were asked:\n•How would like to be able to constrain the model output (open response);\n•Provide a concrete example where it would be useful to have this constraint (open response);\n•How precisely do you need this constraint to be followed: a) exact match; b) approximate match and why (optional open response);\n•How important is this constraint to your workflow (5-point Likert scale from “it’s a nice to have, but my current workarounds are\nfine” to “it’s essential to my workflow”) and why (optional open response).\nThe survey then asked participants to reflect through open response on scenarios where they would prefer expressing constraints via GUI\n(sliders, buttons, etc.) over natural language (in prompts, etc.) and vice versa, as well as any alternative ways they would prefer to express\nconstraints. To facilitate the reflection, the survey additionally asked participants to rate their level of preference in:\n•Output should be exactly 3 words, no more than 3 paragraphs, etc.\n•Output in a specific format or structure (e.g., JSON, XML, bulleted / ordered list)\n•Only output “left-handed”, “right-handed”, or “ambidextrous”\n•Output must include or avoid certain words / phrases\n•Output must cover or avoid certain topics, only use certain libraries when generating code, etc.\n•Output style should mimic Yoda / Shakespeare / certain personas, etc.\nEach question presented a 7-point Likert scale from “strongly prefer natural language” to “strongly prefer GUI. ”",
  "topic": "Workflow",
  "concepts": [
    {
      "name": "Workflow",
      "score": 0.7759119272232056
    },
    {
      "name": "Computer science",
      "score": 0.704735517501831
    },
    {
      "name": "Constraint (computer-aided design)",
      "score": 0.6907662749290466
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.6112321615219116
    },
    {
      "name": "User-centered design",
      "score": 0.611180305480957
    },
    {
      "name": "Process (computing)",
      "score": 0.5937905311584473
    },
    {
      "name": "Human–computer interaction",
      "score": 0.46595296263694763
    },
    {
      "name": "Work (physics)",
      "score": 0.45984897017478943
    },
    {
      "name": "Rapid prototyping",
      "score": 0.44503605365753174
    },
    {
      "name": "Software engineering",
      "score": 0.39527416229248047
    },
    {
      "name": "Process management",
      "score": 0.3295242488384247
    },
    {
      "name": "Programming language",
      "score": 0.1857319176197052
    },
    {
      "name": "Engineering",
      "score": 0.17034903168678284
    },
    {
      "name": "Artificial intelligence",
      "score": 0.15202683210372925
    },
    {
      "name": "Database",
      "score": 0.08620217442512512
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}