{
  "title": "TrTr: Visual Tracking with Transformer",
  "url": "https://openalex.org/W3162457465",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2749837986",
      "name": "Zhao, Moju",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743226554",
      "name": "Okada, Kei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744165878",
      "name": "Inaba Masayuki",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2935837427",
    "https://openalex.org/W2998434318",
    "https://openalex.org/W3034297219",
    "https://openalex.org/W2470394683",
    "https://openalex.org/W3035672751",
    "https://openalex.org/W2987460522",
    "https://openalex.org/W1964846093",
    "https://openalex.org/W2886910176",
    "https://openalex.org/W2214352687",
    "https://openalex.org/W2886335102",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W2518876086",
    "https://openalex.org/W2937808806",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2991213871",
    "https://openalex.org/W2891033863",
    "https://openalex.org/W2966759264",
    "https://openalex.org/W2963074722",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2962972233",
    "https://openalex.org/W2913466142",
    "https://openalex.org/W2776035257",
    "https://openalex.org/W3035725297",
    "https://openalex.org/W2998027361",
    "https://openalex.org/W2797812763",
    "https://openalex.org/W2794744029",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W3035211844",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W2963534981",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1892578678",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2964198573",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2518013266",
    "https://openalex.org/W3103005696",
    "https://openalex.org/W1857884451",
    "https://openalex.org/W3034617042",
    "https://openalex.org/W2158592639",
    "https://openalex.org/W3108519869",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2799058067",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W3035453691",
    "https://openalex.org/W3001584168",
    "https://openalex.org/W2886904239",
    "https://openalex.org/W3035571898",
    "https://openalex.org/W3035511673",
    "https://openalex.org/W2605173812"
  ],
  "abstract": "Template-based discriminative trackers are currently the dominant tracking methods due to their robustness and accuracy, and the Siamese-network-based methods that depend on cross-correlation operation between features extracted from template and search images show the state-of-the-art tracking performance. However, general cross-correlation operation can only obtain relationship between local patches in two feature maps. In this paper, we propose a novel tracker network based on a powerful attention mechanism called Transformer encoder-decoder architecture to gain global and rich contextual interdependencies. In this new architecture, features of the template image is processed by a self-attention module in the encoder part to learn strong context information, which is then sent to the decoder part to compute cross-attention with the search image features processed by another self-attention module. In addition, we design the classification and regression heads using the output of Transformer to localize target based on shape-agnostic anchor. We extensively evaluate our tracker TrTr, on VOT2018, VOT2019, OTB-100, UAV, NfS, TrackingNet, and LaSOT benchmarks and our method performs favorably against state-of-the-art algorithms. Training code and pretrained models are available at https://github.com/tongtybj/TrTr.",
  "full_text": "arXiv:2105.03817v1  [cs.CV]  9 May 2021\nT rT r: Visual T racking with T ransformer\nMoju Zhao and Kei Okada and Masayuki Inaba\nDepartment of Mechano-Infomatics, University of T okyo\n7-3-1 Hongo, Bunkyo-ku, T okyo 113-8656, Japan\nchou@jsk.imi.i.u-tokyo.ac.jp\nAbstract\nT emplate-based discriminative trackers are currently the\ndominant tracking methods due to their robustness and ac-\ncuracy, and the Siamese-network-based methods that de-\npend on cross-correlation operation between features ex-\ntracted from template and search images show the state-\nof-the-art tracking performance. However , general cross-\ncorrelation operation can only obtain relationship betwee n\nlocal patches in two feature maps. In this paper , we pro-\npose a novel tracker network based on a powerful atten-\ntion mechanism called T ransformer encoder-decoder ar-\nchitecture to gain global and rich contextual interdepen-\ndencies. In this new architecture, features of the template\nimage is processed by a self-attention module in the en-\ncoder part to learn strong context information, which is the n\nsent to the decoder part to compute cross-attention with the\nsearch image features processed by another self-attention\nmodule. In addition, we design the classiﬁcation and re-\ngression heads using the output of T ransformer to local-\nize target based on shape-agnostic anchor . W e extensively\nevaluate our tracker T rT r , on VOT2018, VOT2019, OTB-\n100, UA V , NfS, T rackingNet, and LaSOT benchmarks and\nour method performs favorably against state-of-the-art al -\ngorithms. T raining code and pretrained models are avail-\nable at\nhttps://github.com/tongtybj/T rT r.\n1. Introduction\nV isual object tracking is a task to estimate the state of a\ntarget object in a video sequence. Most commonly, the state\nis represented as a bounding box encapsulating the target in\neach frame, and the initial bounding box is given in the ﬁrst\nframe. Although the tracking problem is closely related to\nthe detection problem, the main difference from object de-\ntection is that detection only looks for a set of particular\ninstances while tracking needs to even track unknown ob-\nject class. Thus a tracker is required to extract proper fea-\ntures about the target from the initial frame (i.e., one-sho t\nlearning), and to localize the target in following frames.\nTrTr (ours) SiamBANOcean\nFigure 1. A Comparison of our proposed T racker with\nT ransformer (TrTr) with state-of-the-art trackers Ocean [ 56] and\nSiamBAN [ 8]. Unlike these trackers which depend on explicit\ncross-correlation operation between features extracted f rom the\ntemplate and search images, our tracker applies Transforme r\nencoder-decoder architecture [ 48] which uses self- and cross-\nattention mechanisms to aggregate global and rich contextu al in-\nterdependencies. Observed from the visualization results , our\ntracer is more accurate, and robust to appearance changes, c om-\nplex background and close distractors with occlusions.\nAmong various tracking methods, template-based method\nis recently dominant due to their robustness on both classi-\nﬁcation and bounding box regression.\nThe target classiﬁcation is generally formulated as a con-\nﬁdence map upon the current search image. This has been\nachieved by the previously dominant Discriminative Corre-\nlation Filter (DCF) paradigms [\n5, 17, 22, 11, 43] and the\nmost recent Siamese-network-based trackers [ 2, 20, 51, 55,\n53]. Both methods exploit cross-correlation operation be-\ntween features extracted from the template and search im-\nages to predict the appearance probability at each spatial\nposition in search image for target localization. For bound -\ning box regression, recent Siamese-network-based track-\ners [\n30, 59, 29] introduce Region Proposal Network (RPN)\nwhich is common in detection problem, and develop a\nshared Siamese network for both classiﬁcation and bound-\ning box regression heads. Given that the general cross-\ncorrelation operation can only obtain relationship betwee n\nlocal patches in the template and search images, global pair -\nwise interaction cannot be performed, which makes it dif-\nﬁcult to precisely track the target object with large appear -\nance variations, close distractors, or occlusions. Anothe r\ntype of recent trackers [\n10, 3, 12] use IoUNet [ 24] and fea-\nture modulation vector to estimate bounding box globally\nin search image. However, the target classiﬁcation branch\nof these trackers still depend on cross-correlation operat ion\nto train DCF .\nThen, attention mechanisms for tracking are proposed by\n[\n60, 51] to learn the global context from the whole image\nand enhance the discriminate ability between the target and\nclose distractors or complex backgrounds. The most recent\nwork SiamAttn [\n54] adds self- and cross-attention mod-\nules for Siamese-network-based architecture, which out-\nperforms its baseline structure SiamRPN++[\n29]. Alterna-\ntively, in this work, we introduce a new network architec-\nture for tracking task, which exploits Transformer encoder -\ndecoder architecture [\n48] to perform both target classiﬁca-\ntion and bounding box regression. In this new tracker, re-\nferred as TrTr, self-attention for both template and search\nimage features, and cross-attention between these feature s\nare performed to compute global and rich contextual inter-\ndependencies, resulting in more accurate and stable track-\ning as shown in Fig.\n1. In addition, we further equip\nour Transformer-based tracker with a plug-in online update\nmodule to capture the appearance changes of objects during\ntracking. This online update module further enhances the\ntracking performance, which shows the scalability of our\nproposed Transformer-based tracking approach.\nThe main contributions of this work are:\n• W e introduce Transformer encoder-decoder architec-\nture for object vision tracking task, where explicit\ncross-correlation between feature maps extracted from\nthe template and search images is replaced with the\nself- and cross-attention operations to gain global and\nrich contextual interdependencies.\n• The conﬁdence based target classiﬁcation head and\nshape-agnostic anchor based target regression head\nare developed for our Transformed-based architecture;\nwhile a plug-in online update module for classiﬁca-\ntion is designed to further enhances the tracking per-\nformance.\n• W e conduct comprehensive experiments on large scale\nbenchmark datasets including VOT , OTB, UA V , NfS,\nLaSOT , and TrackingNet, and our tracker achieves fa-\nvorable performance compared with the state-of-the-\nart results while running in real-time speeds, which\ndemonstrates a signiﬁcant potential of Transformer ar-\nchitecture in tracking task.\n2. Related W ork\nV isual tracking is one of the most active research topics\nin computer vision in recent decades. A comprehensive sur-\nvey of the related trackers is beyond the scope of this paper,\nso we only brieﬂy review two aspects that are most relevant\nto our work: the templated-based visual object tracking and\nthe Transformer architecture.\n2.1. templated­based visual object tracking\nT emplate-based trackers depend on a target template\ngiven in the initial frame. The most recent state-of-the-\nart tracking methods can be roughly divided into two cate-\ngories: correlation-based ﬁlter based trackers and Siames e-\nnetwork-based trackers.\nThe correlation-based trackers [\n22, 35, 4] rely on diag-\nonalizing transformation of circular convolutions, given by\nthe Discrete Fourier Transform, to perform efﬁcient online\ntraining on correlation ﬁlter. Thus, the target template ca n\nbe updated online to improve the discriminative robustness .\nAlthough these trackers can provide the location of the tar-\nget from conﬁdence map on search image, the target shape\nis difﬁcult to be estimated by such trackers. Commonly,\na multi-scale strategy is applied to handle the change in\ntarget size, which however cannot address shape deforma-\ntion. Most recent related works [\n10, 3, 12] exploit deep\nlearned features from backbone based on convolutional neu-\nral networks (CNN) for not only correlation-based classiﬁ-\ncation but also target regression using another network (e. g.,\nIoUNet [\n24]). Thus, such CNN-based trackers have two in-\ndependent components: online trained ﬁlter for target clas -\nsiﬁcation and ofﬂine trained network for target regression .\nThe Siamese-network-based tracker, which is ﬁrst pro-\nposed by SiamFC [ 2], extracts deep features from the tem-\nplate and the search images using the same ofﬂine-trained\nCNN backbone, then performs cross-correlation between\nthese features to compute the matching scores for target lo-\ncalization. Further, SiamRPN [\n30] borrows the RPN idea\nfrom object detectors to enable bounding box regression,\nwhich achieves an end-to-end learning tracker for both clas -\nsiﬁcation and regression. Then, SiamRPN++[\n29] is devel-\noped to aggregate multiple features from different layers\nof backbone to perform layer-wise cross-correlation, whic h\nserves as the baseline structure for recent state-of-the-a rt\nSiamese-network-based trackers [\n19, 8, 54, 9].\nThe proposed network in our work has similar network\nstructure with SiamRPN++, which also contains a shared\nbackbone for feature extraction and heads for classiﬁca-\ntion and regression. However, instead of applying cross-\ncorrelation layer which is the core of Siamese network, we\nuse Transformer encoder-decoder architecture and its atte n-\ntion mechanism.\n2.2. T ransformer encoder­decoder architecture\nTransformer [\n48] is developed as a new attention-based\nbuilding block for machine translation. Attention mecha-\nnisms [\n1] are neural network layers that aggregate infor-\nmation from the entire input sequence. Transformer intro-\nduces self-attention layers to scan through each element of\na sequence and update it by aggregating information from\nthe whole sequence. Such a self-attention is performed for\nboth the input of encoder and decoder. Then, cross-attentio n\n(encoder-decoder attention) layer computes the interdepe n-\ndency between two sequences.\nTransformer is now replacing Recurrent Neural Network\n(RNN) in many problems in natural language processing\nand speech processing [\n13, 34, 40, 46]. The original Trans-\nformer [ 48] is ﬁrst used in auto-regressive models, follow-\ning early sequence-to-sequence models [ 45] and generating\noutput tokens one by one. Then, parallel sequence gener-\nation is developed in the domains of machine translation\nand audio [\n47, 18] to address the prohibitive inference cost\n(proportional to output length, and hard to batch). In com-\nputer vision, Transformer with parallel decoding is being\nintroduced in image classiﬁcation [\n14] and object detection\n[6, 58], which demonstrate state-of-the-art results in both\nﬁelds. Further, Transformer is exploited in multiple objec t\ntracking task [ 36, 44], of which however the target classes\nare pre-deﬁned. Therefore, in this work, we develop a new\ntracker based on Transformer architecture and its attentio n\nmechanism to enable tracking class-agnostic target, which\nis the main challenge of single object tracking task.\n3. T ransformer Architecture\nThis section presents the Transformer encoder-decoder\narchitecture for visual tracking. As shown in Fig.\n2, the\ninput features to Transformer are extracted from a shared\nbackbone network elaborated later in Sec.\n4.1, while the\noutput is used for classiﬁcation and regression heads pre-\nsented in Sec.\n4.2. Our Transformer architecture follows\nthe original structure in [ 48], which consists of two compo-\nnents: encoder with template image features, and decoder\nwith search image features. There are multi-head attention\nmodules in both encoder and decoder, which are the key\nto perform self-attention with a feature sequence and cross -\nattention between two feature sequences.\n3.1. Encoder and Decoder Components\nUnlike the original Transformer [\n48], our Transformer\nmodel only contains one layer for both encoder and decoder.\nEncoder . The encoder has a standard architecture and\nconsists of a multi-head self-attention module and a feed\nAdd & Norm\nFFN\nMulti-Head Attention\nAdd & Norm\nTemplate image features \n\u0015ÛHêH×\nV K Q\nPositional\nencoding\nMulti-Head Attention\nAdd & Norm\nSearch image features \n\u0015ÁHÐH×\nV K Q\nEncoder\nAdd & Norm\nFFN\nMulti-Head Attention\nAdd & Norm\nV K Q\nDecoder\nOutput \n\u0015ÁHÐH×\nFigure 2. Architecture of TrTr Transformer for tracking tas k,\nwhich replaces cross-correlation operation in Siamese net work\nwith multi-head self- and encoder-decoder attention mecha nisms.\nforward network (FFN) which are elaborated later. W e as-\nsume there is a template image feature map z0 ∈ R h×w×d,\nwhere h and w are the height and width of the feature map,\nand d is channel dimension. Then, we collapse the spatial\ndimensions of z0 into one dimension as a sequence with\nthe size of Rhw×d. Since the transformer architecture is\npermutation-invariant, we add ﬁxed positional encodings\n[\n48] for this input sequence.\nDecoder . The decoder also follows the standard architec-\nture, which contains multi-headed attention modules for\nself- and encoder-decoder (cross-) attention followed by a\nFFN. The input of decoder is a search feature map x0 ∈\nRH×W ×d, of which the channel dimension is same with z0,\nbut the spatial dimensions are larger ( W > w, H > h ) for\ntracking purpose. Similar to the encoder, the spatial dimen -\nsions of x0 is also collapsed into one dimension, resulting in\na sequence of RHW ×d. The ﬁxed positional encodings are\nalso added in this sequence. Following the strategy in [\n6],\nour model decodes all elements in this sequence in parallel.\nThe output has the same size as input ( RHW ×d), which is\nﬁnally reverted to RH×W ×d for subsequent classiﬁcation\nand regression.\n3.2. Attention Mechanism\nFollowing [\n48], query-key-value attention mechanism is\napplied. Query, key, and value sequences, Q ∈ R Nq×d\n′\n,\nK ∈ R Nkv×d\n′\n, and V ∈ R Nkv×d\n′\nare obtained by linear\nprojection with weight matrices Wq, W k, W v ∈ R d×d\n′\n:\n[Q; K; V ] = [( Xq + Pq)Wq); (Xkv + Pk)Wk); XkvWv)] ,\n(1)\nwhere Xq ∈ RNq×d, Xkv ∈ RNkv×d are spatially one-\ndimensional inputs, respectively. For self-attention mod ule\nin encoder, Nq = Nkv = hw, and Xq = Xkv; for self-\nattention module in decoder, Nq = Nkv = HW , and Xq =\nXkv. For encoder-decoder attention module, Nq = HW ,\nNkv = hw. Pq and Pk are the positional encodings for\nquery and key sequences, respectively; however we do not\nadd positional encodings for value sequence V similarly to\n[\n6].\nThe attention weight map A ∈ RNq×Nkv is then com-\nputed based on the softmax of dot products between query\nand key sequences to obtain the pair-wise correlation be-\ntween them:\nAij = e\n1\n√\nd′ QT\ni KjZi\nwhere Zi =\nNkv∑\nj=1\ne\n1√\nd′ QT\ni Kj\n, (2)\nwhere Qi, K j ∈ Rd\n′\nare the i-th and j-th vector of Q and\nK. From the view of image processing, Ai,j represents the\ncorrelation value between the position i in query image and\nthe position j in key image. The ﬁnal output of attention is\nthe aggregation of values weighted by attention weight A:\nAttn(Xq, X kv, W ) = AV, (3)\nwhere W is the concatenation of Wq, W k, and Wv.\nThe multi-head attention is simply the concatenation of\nM single attention heads followed by a linear projection:\nMultiHeadAttn (Xq, X kv)\n= [ Attn(Xq, X kv, W 1); · · · ; Attn(Xq, X kv, W M )] W o.(4)\nThe attention outputs are concatenated on channel axis, and\nthus in each single attention head, the channel dimension is\nd\n′\n= d\nM ; whereas, W o ∈ Rd×d.\nThen, a residual connections and layer normalization is\nfurther used according to the common practice in [ 48]:\nX\n′\nq = layernorm(MultiHeadAttn (Xq, X kv) + Xq). (5)\nAfter each self- and encoder-decoder attention module,\na feed-forward network (FFN) composed of two-layers of\n1x1 convolutions with ReLU activations is used to pro-\ncess X\n′\nq. The dimension of input and output channels are\nd, while the hidden layer has a larger channel dimension.\nThere is also a residual connection and layernorm after the\nFFN, similarly to [\n48].\nUsing these self-attention and encoder-decoder atten-\ntion mechanisms over inputs, the model can globally rea-\nson about input features together using pair-wise relation s\nbetween them, which helps to discriminate between fore-\nground and background. In our model, we use 8 heads for\nmulti-head attention module ( M = 8 ), and set the channel\ndimension of FFN hidden layer to 8d.\n4. T racker with T ransformer\nThis section depicts the tracking algorithm building upon\nthe proposed Transformer architecture. It contains two\nparts: an ofﬂine model based on Transformer and an online\nupdate model only for classiﬁcation, as illustrated in Fig.\n3.\n4.1. Feature Extraction\nOur approach takes a pair of images as inputs, i.e., a tem-\nplate image and a candidate search image. The template\nimage represents the object of interest, i.e., an image patc h\ncentered on the target object in the initial frame. The searc h\nimage is typically larger and represents the search area in\nsubsequent video frames. Feature extraction for template\nand search images share a modiﬁed ResNet-50 [\n21] back-\nbone. The last stage of the standard ResNet-50 is truncated,\nthereby only the ﬁrst fourth stages are used. In the fourth\nstage, the convolution stride of down-sampling unit is mod-\niﬁed from 2 to 1 to increase the spatial size of feature maps,\nmeanwhile, all the 3 × 3 convolutions are augmented with\na dilation with stride of 2 to increase the receptive ﬁelds.\nThese modiﬁcations increase the resolution of output fea-\ntures, thus improving the feature capability on object loca l-\nization [\n7, 29]. The features from backbone are then passed\nthrough a 1 × 1 convolution layer to reduce the channel di-\nmension from 1024 to 256 for saving computation cost in\nsubsequent Transformer module.\n4.2. T arget Localization\nT o localize target object and estimate the shape, we apply\nthe shape-agnostic anchor [\n57]. Three independent heads\nare connected to the output of Transformer module. One is\nfor target classiﬁcation and the other two are for regressio n.\nEach head contains three 1 × 1 convolution layers followed\nby a Sigmoid layer. The classiﬁcation head yields a map\nY ∈ [0, 1]⌊ H\ns ⌋×⌊ W\ns ⌋×1, where H, W are the height and the\nwidth of search image (i.e., both are 255 in Fig. 3), and s\nis the output stride (i.e., 8). ⌊⌋ is the ﬂoor function, which\nleads to a result of 32 in Fig. 3. The map Y corresponds to\nthe appearance probability of the target in discretized low\nresolution. Therefore, to recover the discretization erro r\ncaused by the output stride s, it is necessary to additionally\npredict a local offset O ∈ [0, 1)⌊ H\ns ⌋×⌊ W\ns ⌋×2. Then the cen-\nter point of the target object in search image can be given\nby:\n(xc, y c) = s(argmax(Y\n′\n) + O(argmax(Y\n′\n))), (6)\nwhere Y\n′\nis combination of the raw map Y with a cosine\nwindow to suppress the large displacement, similarly to [\n30,\n29]. argmax(Y\n′\n) returns a 2D location corresponding to\nthe peak of map Y ′.\nFor size regression, we design another head to yield a\nnormalized map S ∈ [0, 1]⌊ H\ns ⌋×⌊ W\ns ⌋×2. Then, the size of\n1x1 \nConvd = 2\nResNet-50 Backbone\nEncoder\nDecoder\nTemplate Image \n127 x 127 x 3\nSearch Image \n255 x 255 x 3\n16 x 16 x 1024\n32 x 32 x 1024\n1x1 \nConv\n16 x 16 x 256\n32 x 32 x 256\nTransformer Regression Map (location offset)\n32 x 32 x 21x1 \nConvs\n1x1 \nConvs\n1x1 \nConvs\n32 x 32 x 256\nRegression Map (size)\n32 x 32 x 2\nClassification Map \n32 x 32 x 1\n16 x 16 x 512\nAdaptive Pooling\n32 W 16 1x1 \nConv Conv\n16 x 16 x 64\n2 x conv (c = 256) \n+ 1 x conv (c = 2)\n2 x conv (c = 256) \n+ 1 x conv (c = 1)\nOnline Update Network\nOffline\nOnline\nd: dilation stride\nc: channel number \nConvolution Layers\nOnline Update Layers\nFeatures\nSigmoid Function\nAddition\nUnPooling\n16 W 32\n16 x 16 x 1\nFigure 3. Overview of the proposed tracking framework, cons isting of an ofﬂine tracking part (top) and an online classiﬁ cation model\nupdate part (bottom). The ofﬂine tracking part includes a ba ckbone for feature extraction (Sec. 4.1), Transformer module (Sec. 3) and\nheads for classiﬁcation and regression (Sec. 4.2). The plug-in online update network models the appearance c hanges of target object to\nfurther improve the classiﬁcation accuracy , as detailed in Sec. 4.4. The size of target template image is 127 × 127, while the size of search\nimage is conﬁgurable, and its typical value is 255 × 255.\nbounding box in search image can be given by:\n(wbb, h bb) = ( W, H ) ∗ S(argmax(Y\n′\n)), (7)\nwhere ∗ is element-wise multiplication operation. Linear\ninterpolation update strategy [ 30, 29] is further applied to\nsmooth the change in bounding box size. Finally, corners\nof bounding box can be easily calculated from the target\ncenter point (xc, y c) and the size (wbb, h bb).\n4.3. Loss Function\nW e follow the loss function for object detection pro-\nposed by [\n57]; however, there is only a single class to\nclassify in our network, and the bounding box is normal-\nized. For a ground truth target center ¯p, we ﬁrst compute a\nlow-resolution equivalent ˜p = ( ⌊ ¯px\ns ⌋, ⌊ ¯py\ns ⌋). Then we use\na Gaussian kernel ¯Y = exp( − (x−˜px)2+(y−˜py)2\n2σ 2\np\n) to com-\npare with the predicted map Y , where σp is an object size-\nadaptive standard deviation [ 28]. The training objective\nfor classiﬁcation is a penalty-reduced pixel-wise logisti c re-\ngression with focal loss [ 31]:\nLY = −\n∑\nxy\n\n\n\n\n\n\n\n\n\n(1 − Yxy)α log(Yxy) ( ¯Yxy = 1)\n(1 − ¯Yxy)β (Yxy)α\nlog(1 − Yxy) (otherwise)\n(8)\nwhere Yxy is the value of map Y at (x, y ). α and β are\nhyper-parameters of the focal loss [\n31]. W e use α = 2 and\nβ = 4 in all our experiments, following [ 28].\nThen, the loss function for offset regression is formu-\nlated using L1 loss:\nLO = |O˜p − (p\ns − ˜p)|, (9)\nwhere, O˜p is the map value of O at ˜p.\nFor a ground truth bounding box size ( ¯wbb, ¯hbb), we also\nuse L1 loss similar to ( 10):\nLS = |S˜p − ˜s|, (10)\nwhere, S˜p is the map value of S at ˜p, and ˜s = ( ¯wbb\nW ,\n¯hbb\nH )\nis normalized ground truth bounding box size. Finally, the\njoint training objective for the entire network can be given\nby:\nL = LY + λ1LO + λ2LS, (11)\nwhere λ1 and λ2 are the trade-off hyper-parameters. W e\ndo not search for these hyper-parameters, and simply set\nλ1 = λ2 = 1 .\n4.4. Integrating Online Update for Classiﬁcation\nAn online update model inspired by [\n10, 3] is designed\nto independently capture the appearance changes of target\nobject during tracking. As shown in the bottom part of\nFig.\n3, this online branch directly uses the output from the\nﬁrst third stage of the backbone network and generates a\nmap Yonline ∈ [0, 1]⌊ H\ns ⌋×⌊ W\ns ⌋×1. This branch consists of\n2-layer fully convolutional neural network, where the ﬁrst\n1 × 1 convolutional layer reduces the channel dimension to\n64, and the second layer employs 4 × 4 kernel with a single\noutput channel. The fast conjugate gradient algorithm pro-\nposed in [\n10] is applied to train this online network during\ninference. The maps estimated by the ofﬂine classiﬁcation\nhead and the online branch are weighted as\nY\n′′\n= wY\n′\n+ (1 − w)Yonline, (12)\nwhere w denotes the trade-off hyper-parameter, which is set\nto 0.6 in our experiments. When the online update model is\navailable, the combined classiﬁcation map Y\n′′\nis utilized\ninstead of Y\n′\nin (\n6) and ( 7). W e refer readers to [ 10, 3] for\nmore details on the online update model.\n5. Experiments\n5.1. Implementation Details\nBy following SiamFC [\n2], we adopt a template image of\n127 × 127 and a search image of 255 × 255 for training.\nThe backbone network ResNet-50 is initialized with the pa-\nrameters pretrained on ImageNet [\n42]. The whole network\nis then trained on the datasets of Y outube-BB [ 41], Ima-\ngeNet VID [ 42], GOT -10k [ 23], LaSOT [ 15], and COCO\n[32], which yield 4. 7 × 105 training pairs for one epoch.\nW e apply ADAM [ 25] optimization on 8 GPUs, with each\nGPU hosting 64 image pairs, and thus the mini-batch size is\n512 image pairs per iteration. There are 20 epochs in total\nwith learning rates starting from 10−5 for backbone part and\n10−4 for other parts, and these learning rates are decayed\nwith 0.5 every 5 epochs. Our approach is implemented in\nPython using PyT orch on a PC with Intel(R) Xeon(R) W -\n3225 CPU 3.70GHz, 64G RAM, Nvidia R TX 3090.\nDuring inference, the size of search image is conﬁg-\nurable, and we use the default size of 255 × 255 in most\ncases. In case of fast target motion, we choose a larger size\n(e.g., 280 × 280 or 320 × 320) to ensure the appearance of\nthe target in the next search image. The tracking speed of\nthe ofﬂine model (top part of Fig.\n3) with a search image of\n255 × 255 is 50 FPS in our experiment environment, and\nwill reduce to 35 FPS if integrating online update model\n(the bottom part of Fig.\n3).\n5.2. Comparison with the state­of­the­art\nW e compare our approach, termed TrTr, on seven track-\ning benchmarks. W e evaluate two versions: TrTr-ofﬂine for\nonly the ofﬂine part shown in Fig.\n3, and TrTr-online for\ncombination with plug-in online update module.\nOn VOT2018 & VOT2019. W e evaluate on the 2018\nedition of the V isual Object Tracking challenge [ 26] by\ncomparing with the state-of-the-art methods. The dataset\ncontains 60 videos, and trackers are restarted at failure by\nthe evaluation protocol. The performance is then decom-\nposed into accuracy and robustness, deﬁned using IoU over-\nlap and failure rate respectively. The main EA O metric takes\nboth these aspects into account. The size of search image\nVOT2018 VOT2019\nA ↑ R ↓ EA O ↑ A ↑ R ↓ EA O ↑\nA TOM [10] 0.590 0.204 0.401 0.603 0.411 0.292\nSiamRPN++ [ 29] 0.600 0.234 0.414 0.599 0.482 0.285\nSiamFC++ [ 53] 0.587 0.183 0.426 - - -\nDiMP50 [ 3] 0.597 0.153 0.440 0.594 0.278 0.379\nPrDiMP50 [ 12] 0.618 0.165 0.442 - - -\nSiamBAN [ 8] 0.597 0.178 0.452 0.602 0.396 0.327\nMAML-Retina [ 50] 0.604 0.159 0.452 0.570 0.366 0.313\nSiamAttn [ 54] 0.630 0.160 0.470 - - -\nOcean [ 56] 0.592 0.117 0.489 0.594 0.316 0.350\nDRNet [ 27] - - - 0.605 0.261 0.395\nD3S [ 33] 0.640 0.150 0.489 - - -\nTrTr-ofﬂine 0.612 0.234 0.424 0.608 0.441 0.313\nTrTr-online 0.606 0.110 0.493 0.601 0.228 0.384\nT able 1. Comparison with SOT A trackers on VOT2018 and\nVOT2019, with accuracy (A), robustness (R), and expected av -\nerage overlap (EAO).\nRed, blue and green fonts indicate the top-3\ntrackers. “TrTr” denotes our proposed model.\nin our tracker is set to 280 × 280. The results, computed\nover 15 repetitions as speciﬁed in the protocol, are shown\nin T ab.\n1. Our ofﬂine TrTr tracker outperforms the repre-\nsentative Siamese-network based tracker SiamRPN++ [ 29]\nby 1.0 point in terms of EA O. It is worth noting that the\nimprovements mainly come from the accuracy score, which\nobtains 1.2 points relative increases over SiamRPN++. Our\nonline augmented model further improves our tracker by 6.9\npoints in terms of EA O, which achieves the best overall per-\nformance with the highest robustness and competitive accu-\nracy compared to previous methods.\nVOT2019 [\n27] is refreshed by replacing the 12 least\ndifﬁcult sequences from the previous version with several\ncarefully selected sequences in GOT -10k dataset [\n23]. The\nsame measurements are exploited for performance evalua-\ntion; however, we expand the search image size to 320 ×\n320 to handle fast target motion in several challenge se-\nquences. As illustrated in T ab.\n1, our ofﬂine model marks\nthe best accuracy score of 0.608; while our online aug-\nmented model achieves an accuracy of 0.601, a robustness\nof 0.228 and an EA O of 0.384, which ranks second best and\nalso outperforms DRNet [\n27] (leading performance on the\npublic dataset of VOT2019 challenge) in terms of robust-\nness.\nOn OTB-100. W e evaluate both our trackers TrTr-ofﬂine\nand TrTr-online on OTB-100 benchmark [\n52], which con-\ntains 100 sequences with 11 annotated video attributes. The\nsize of search image is set to 280 × 280. Fig.\n4 compares\nour trackers with some recent top-performing trackers. W e\nfollow the one pass evaluation (OPE) protocol, and report\nthe A UC scores of success plot. TrTr-ofﬂine and TrTr-\nonline obtain success A UC scores of 0.691 and 0.715, re-\n(a) Precision Plot \n-100-100\n(b) Success Plot \nFigure 4. Comparison with state-of-the-art methods on succ ess and\nprecision plots on OTB-100.\nMDNet\n[39]\nSiamRPN++\n[29]\nA TOM\n[10]\nDiMP50\n[3]\nSiamBAN\n[8]\nSiamRCNN\n[49]\nPrDiMP50\n[12]\nTrTr\nofﬂine\nTrTr\nonline\nUA V123 52.8 61.3 65.0 65.3 63.1 64.9 68.0 59.4 65.2\nNfS 42.2 - 59.0 61.9 59.4 63.9 63.5 55.2 63.1\nT able 2. Comparison with SOT A trackers on UA V123 and NfS\ndatasets, with AUC.\nRed, blue and green fonts indicate the top-3\ntrackers. “TrTr” denotes our proposed model.\nspectively. T o the best of our knowledge, TrTr-online is the\nbest-performing tracker ever on OTB-100.\nOn U A V123. This challenging dataset [\n37], containing\n123 videos, is designed to benchmark trackers for UA V ap-\nplications. It features small objects, fast motions, occlu sion,\nabsent, and distractor objects. The size of search image in\nour tracker is set to 350 × 350 to handle fast moving target.\nW e show the result in T ab.\n2. Our online augmented model\nobtain 65.2% in terms of overall A UC score, which ranks\nthird place, and outperforms the state-of-the-art Siamese -\nnetwork-based trackers SiamRPN++[\n29] and Siam R-CNN\n[49] by 1 point and 0.3 point, respectively.\nOn NfS. W e evaluate our approach on need for speed\ndataset [ 16] (30 FPS version), which is the ﬁrst high frame\nrate dataset recorded at real-world scenes. It includes 100\nfully annotated videos (380K frames) with fast moving tar-\nget objects, and we set the size of search image in our\ntracker to 380 × 380 to track the fast moving targets. The\nA UC scores of state-of-the-art trackers are shown in T ab.\n2.\nOur online augmented model marks the third best.\nOn T rackingNet. This is a large-scale tracking dataset\nwith high variety in terms of classes and scenarios [ 38], and\nthe test set contains over 500 videos without publicly avail -\nable ground-truth. The size of search image in our tracker is\nset to 320 × 320. The results, shown in T ab.\n3, are obtained\nthrough an online evaluation server. Our online augmented\nmodel has a comparable score in terms of the normalized\nprecision (N-prec.); however, A UC is relatively lower than\nthe state-of-the-art methods. W e observe that when the tar-\nget object is considerably large and close to the whole image\nTrackingNet LaSOT\nA UC (%) N-Prec. (%) A UC (%)\nA TOM [10] 70.3 77.1 51.5\nSiamRPN++ [ 29] 73.3 80.0 49.6\nDiMP50 [ 3] 74.0 80.1 56.8\nSiamAttn [ 54] 75.2 81.7 56.0\nMAML-Retina [ 50] 75.7 82.2 52.3\nPrDiMP50 [ 12] 75.8 81.6 59.8\nSiamRCNN [ 49] 81.2 85.4 64.8\nTrTr-ofﬂine 69.3 77.2 46.3\nTrTr-online 71.0 80.3 55.1\nT able 3. Comparison with SOT A trackers on large scale datase t\nTrackingNet and LaSOT , with AUC and norm precision (N-Prec. ).\nRed, blue and green fonts indicate the top-3 trackers. “TrTr” de-\nnotes our proposed model.\nsize (which are often seen in TrackingNet), the predicted\nbounding box from our tracker is always smaller than the\nground truth, which signiﬁcantly affects the success rate.\nThis is the reason why our model has a higher N-prec. than\nSiamRPN++[\n29] and DiMP50 [ 3], but A UC is lower than\nthese methods.\nOn LaSOT . Last, we evaluate on the large-scale LaSOT\ndataset [ 15]. The test set contains 280 long videos (2500\nframes in average), thus emphasizing the robustness of the\ntracker, along with its accuracy. The size of search image in\nour tracker is set to 350 × 350 to handle fast moving target.\nAs shown in T ab.\n3, our trackers show a relative loss on\nthis benchmark compared with the state-of-the-art results .\nW e suspect that our tracker model is not robust enough to\nhandle the very long sequences where the target object is\nalways occluded or absent.\n5.3. Ablation Study\nHere, we perform an extensive analysis of the proposed\nnetworks by conducting ablation study on VOT2018.\nNumber of T ransformer layers. The number of encoder\nand decoder layers signiﬁcantly inﬂuences the performance\nof Transformer as reported in [\n48]. W e train models with\ndifferent combination of encoder and decoder layers (up to\n3 layers for both encoder and decoder), and compare the\ntracking performance as shown in T ab.\n4. It is very inter-\nesting to see that the model with minimum number of en-\ncoder and decoder layer (i.e., 1 encoder + 1 decoder) obtains\nthe best EA O of 0.424 in VOT2018, which outperforms the\nsecond best (i.e., 1 encoder + 2 decoders) by 1.7 points.\nThis result is opposite to the characteristic of Transforme r\narchitecture for object detection [\n6]. In our tracking net-\nwork shown in Fig. 3, the inputs for Transformer encoder\nand decoder are extracted from the shared backbone, which\nis the main difference from other Transformer architecture s\nencoder 1 2 1 2 3\ndecoder 1 1 2 2 3\nEA O 0.424 0.362 0.407 0.396 0.366\nT able 4. Analysis of the impact of number of encoder and decod er\nlayers in Transformer architecture on the ofﬂine part of Fig . 3.\nVOT2018 dataset is used.\n(e.g., two different language word embeddings in machine\ntranslation [\n48]; image feature embeddings for encoder and\ncandidate object embeddings for decoder in object detec-\ntion [\n6]). W e hypothesize that deep Transformer layers,\nwhich involve several times to compute encoder-decoder at-\ntention between feature embeddings from the same back-\nbone, might make the network overﬁtting to the training\ndataset during training, thereby degrading the robustness to\nunknown tracking object. Therefore, we decide to design\nour Transformer network with 1 encoder and 1 decoder,\nwhich also leads to the advantages in inference speed and\nmodel parameter size.\nImpact on positional encoding. The positional encoding\nshown in Fig.\n2 is important to identify the spatial position\nof sequence in Transformer architecture. In original Trans -\nformer [ 48], the positional encoding is added to every el-\nement of the sequence. However, in case of tracking task,\nsearch image might contain an area that out of the origi-\nnal image such as Fig.\n5(B). Following the cropping rule\nof SiamFC [ 2], this area is padded by the average channel\nvalues of the whole image. W e evaluate the inﬂuence of\nthe positional embeddings on such a padding area during\ntraining and inference, and compare the performance be-\ntween two cases: padding area with positional embeddings\n(w/ PE ) and padding area without positional embeddings\n(w/o PE ). As shown in T ab.\n5, no positional embeddings in\npadding area can bring a large improvement of 4.8 points in\nterms of EA O compared to adding positional embeddings in\npadding area. W e consider that, without positional embed-\ndings in padding area, the query and key sequences Q, K in\n(\n1) will have the same embedding values in padding area,\nwhich leads to the attention weight matrix A in ( 2) having\nthe same weight related to the padding area. Then, more\nweights are assigned to the position pairs related to the tar -\nget object. Thus, such a special “mask” operation can help\nboth self- and cross-attention mechanisms to emphasize the\narea of target object.\nAnalysis of classiﬁcation heatmap. W e compare the\nclassiﬁcation heatmaps obtained from the ofﬂine branch\n(i.e., top part in Fig.\n3) and the online branch (i.e., bottom\npart in Fig. 3) as shown in Fig. 5. It can be conﬁrmed that\nthe probability model in the heatmap of the ofﬂine branch\ntrained with focal loss (\n8) is sharper than that in the heatmap\nA\nClassification Heatmap\n(offline branch)\nClassification Heatmap\n(online branch)\nSearch Image\nB\nFigure 5. Visualization of classiﬁcation heatmaps. The 1 st col-\numn: search images following the cropping rule of SiamFC [ 2],\nthe 2 nd column: classiﬁcation heatmaps from the ofﬂine branch\n(top part of Fig. 3), and the 3 rd column: classiﬁcation heatmaps\nfrom the online branch (top part of Fig. 3).\nof the online part trained with L2 error model [ 10]. As dis-\ncussed in [ 31], focal loss can solve foreground-background\nclass imbalance (i.e., the vast number of easy negatives)\nduring training and improve the convergence performance\nof loss function. During inference, such a sharp probabilit y\nmodel can help our tracker to discriminate the target from\nthe distractors easier. For online update model, we still us e\nthe L2 error model following [\n10] to ensure the robustness\nagainst the appearance changes of target object. As shown\nin T ab.\n1, integrating this online update module yields an-\nother improvement of 6.9 EA O points in VOT2018 bench-\nmark, showing the scalability of the proposed framework.\n6. Conclusions\nIn this paper, we have presented a uniﬁed framework to\nend-to-end train a network based on Transformer encoder-\ndecoder architecture for visual tracking instead of using\nexplicit cross-correlation operation like recent Siamese -\nnetwork-based trackers. Our model exploits self- and\nencoder-decoder attention mechanisms in Transformer ar-\nchitecture to globally reason about the target object us-\ning pair-wise relations between template and search im-\nw/ PE w/o PE\nEA O 0.376 0.424\nT able 5. Analysis of the impact of positional embedding in pa dding\narea such as Fig. 5(B). T wo different cases during training and\ninference: padding area with positional embeddings ( w/ PE ) and\npadding area without positional embeddings ( w/o PE ). VOT2018\ndataset is used.\nage features. Moreover, the proposed classiﬁcation and\nregression heads based on shape-agnostic anchors ensure\nthe robust tracking for target localization. While our pro-\nposed tracker integrated with a plug-in online update model\nachieves competitive performance against SOT A trackers in\nmost benchmarks, the performance on large scale datasets,\nTrackingNet and LaSOT , still has the room for improve-\nment. In the future work, we will extend our Transformer\narchitecture by using sequential frames in temporal man-\nner to address the challenges of fast target motion, appear-\nance changes, and occlusion without integrating an addi-\ntional online branch.\nReferences\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Y oshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. In 3rd International Conference on Learning Rep-\nresentations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings , 2015.\n3\n[2] Luca Bertinetto, Jack V almadre, Jo˜ ao F . Henriques, And rea\nV edaldi, and Philip H. S. T orr. Fully-convolutional Siames e\nnetworks for object tracking. In Gang Hua and Herv´ e J´ egou,\neditors, Computer V ision – ECCV 2016 W orkshops , pages\n850–865, Cham, 2016. Springer International Publishing.\n1,\n2, 6, 8\n[3] G. Bhat, M. Danelljan, L. V an Gool, and R. Timofte. Learn-\ning discriminative model prediction for tracking. In 2019\nIEEE/CVF International Conference on Computer V ision\n(ICCV), pages 6181–6190, 2019.\n2, 5, 6, 7\n[4] Goutam Bhat, Joakim Johnander, Martin Danelljan, Fa-\nhad Shahbaz Khan, and Michael Felsberg. Unveiling the\npower of deep tracking. In Vittorio Ferrari, Martial Hebert ,\nCristian Sminchisescu, and Y air W eiss, editors, Computer\nV ision - ECCV 2018 - 15th European Conference, Munich,\nGermany, September 8-14, 2018, Proceedings, P art II , vol-\nume 11206 of Lecture Notes in Computer Science , pages\n493–509. Springer, 2018.\n2\n[5] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y . M. Lui.\nVisual object tracking using adaptive correlation ﬁlters. In\n2010 IEEE Computer Society Conference on Computer V i-\nsion and P attern Recognition , pages 2544–2550, 2010.\n1\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nic olas\nUsunier, Alexander Kirillov , and Sergey Zagoruyko. End-to -\nend object detection with Transformers. In Computer V ision\n– ECCV 2020 , pages 213–229. Springer International Pub-\nlishing, 2020. 3, 4, 7, 8\n[7] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.\nY uille. DeepLab: Semantic image segmentation with deep\nconvolutional nets, Atrous convolution, and fully connect ed\nCRFs. IEEE Transactions on P attern Analysis and Machine\nIntelligence, 40(4):834–848, 2018.\n4\n[8] Z. Chen, B. Zhong, G. Li, S. Zhang, and R. Ji. Siamese box\nadaptive network for visual tracking. In 2020 IEEE/CVF\nConference on Computer V ision and P attern Recognition\n(CVPR), pages 6667–6676, 2020.\n1, 2, 6, 7\n[9] K. Dai, Y . Zhang, D. W ang, J. Li, H. Lu, and X. Y ang.\nHigh-performance long-term tracking with meta-updater. I n\n2020 IEEE/CVF Conference on Computer V ision and P at-\ntern Recognition (CVPR) , pages 6297–6306, 2020.\n2\n[10] M. Danelljan, G. Bhat, F . S. Khan, and M. Felsberg.\nA TOM: Accurate tracking by overlap maximization. In\n2019 IEEE/CVF Conference on Computer V ision and P at-\ntern Recognition (CVPR) , pages 4655–4664, 2019.\n2, 5, 6,\n7, 8\n[11] Martin Danelljan, Andreas Robinson, Fahad Shahbaz Kha n,\nand Michael Felsberg. Beyond correlation ﬁlters: Learning\ncontinuous convolution operators for visual tracking. In B as-\ntian Leibe, Jiri Matas, Nicu Sebe, and Max W elling, editors,\nComputer V ision – ECCV 2016 , pages 472–488. Springer In-\nternational Publishing, 2016.\n1\n[12] M. Danelljan, L. V an Gool, and R. Timofte. Probabilisti c re-\ngression for visual tracking. In 2020 IEEE/CVF Conference\non Computer V ision and P attern Recognition (CVPR) , pages\n7181–7190, 2020. 2, 6, 7\n[13] Jacoband Devlin, Ming-W eiand Chang, Kentonand Lee, an d\nKristina T outanova. BERT: Pre-training of deep bidirec-\ntional Transformers for language understanding. In Proceed-\nings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage T echnologies, V olume 1 (Long and Short P apers) ,\npages 4171–4186, Minneapolis, Minnesota, June 2019. As-\nsociation for Computational Linguistics.\n3\n[14] Alexey Dosovitskiy , Lucas Beyer, Alexander Kolesniko v,\nDirk W eissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. arXiv e-prints , page arXiv:2010.11929, Oct. 2020.\n3\n[15] H. Fan, L. Lin, F . Y ang, P . Chu, G. Deng, S. Y u, H. Bai, Y .\nXu, C. Liao, and H. Ling. LaSOT: A high-quality benchmark\nfor large-scale single object tracking. In 2019 IEEE/CVF\nConference on Computer V ision and P attern Recognition\n(CVPR), pages 5369–5378, 2019.\n6, 7\n[16] H. K. Galoogahi, A. Fagg, C. Huang, D. Ramanan, and S.\nLucey. Need for speed: A benchmark for higher frame rate\nobject tracking. In 2017 IEEE International Conference on\nComputer V ision (ICCV) , pages 1134–1143, 2017.\n7\n[17] H. K. Galoogahi, T . Sim, and S. Lucey. Correlation ﬁlter s\nwith limited boundaries. In 2015 IEEE Conference on Com-\nputer V ision and P attern Recognition (CVPR) , pages 4630–\n4638, 2015. 1\n[18] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke\nZettlemoyer. Mask-predict: Parallel decoding of con-\nditional masked language models. arXiv e-prints , page\narXiv:1904.09324, Apr. 2019.\n3\n[19] D. Guo, J. W ang, Y . Cui, Z. W ang, and S. Chen. SiamCAR:\nSiamese fully convolutional classiﬁcation and regression for\nvisual tracking. In 2020 IEEE/CVF Conference on Computer\nV ision and P attern Recognition (CVPR) , pages 6268–6276,\n2020. 2\n[20] Q. Guo, W . Feng, C. Zhou, R. Huang, L. W an, and S. W ang.\nLearning dynamic Siamese network for visual object track-\ning. In 2017 IEEE International Conference on Computer\nV ision (ICCV) , pages 1781–1789, 2017. 1\n[21] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learnin g\nfor image recognition. In 2016 IEEE Conference on Com-\nputer V ision and P attern Recognition (CVPR) , pages 770–\n778, 2016. 4\n[22] J. F . Henriques, R. Caseiro, P . Martins, and J. Batista. High-\nspeed tracking with kernelized correlation ﬁlters. IEEE\nTransactions on P attern Analysis and Machine Intelligence ,\n37(3):583–596, 2015. 1, 2\n[23] L. Huang, X. Zhao, and K. Huang. GOT -10k: A large high-\ndiversity benchmark for generic object tracking in the wild .\nIEEE Transactions on P attern Analysis and Machine Intelli-\ngence, pages 1–1, 2019.\n6\n[24] Borui Jiang, Ruixuan Luo, Jiayuan Mao, T ete Xiao, and Y u n-\ning Jiang. Acquisition of localization conﬁdence for accur ate\nobject detection. In Vittorio Ferrari, Martial Hebert, Cri stian\nSminchisescu, and Y air W eiss, editors, Computer V ision –\nECCV 2018 , pages 816–832, Cham, 2018. Springer Interna-\ntional Publishing. 2\n[25] Diederik P . Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In Y oshua Bengio and Y ann LeCun,\neditors, 3rd International Conference on Learning Represen-\ntations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings , 2015.\n6\n[26] Matej Kristan, Aleˇ s Leonardis, Jiˇ r´ ı Matas, Michael Fels-\nberg, Roman Pﬂugfelder, Luka ˇCehovin Zajc, T om´ aˇ s V oj´ ır,\nGoutam Bhat, Alan Lukeˇ ziˇ c, Abdelrahman Eldesokey , Gus-\ntavo Fern´ andez, et al. The sixth visual object tracking\nVOT2018 challenge results. In Laura Leal-T aix´ e and Stefan\nRoth, editors, Computer V ision – ECCV 2018 W orkshops ,\npages 3–53, Cham, 2019. Springer International Publishing .\n6\n[27] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, R.\nPﬂugfelder, J. K¨ am¨ ar¨ ainen, L. Cehovin Zajc, O. Drbohlav ,\nA. Lukezic, A. Berg, A. Eldesokey , et al. The seventh vi-\nsual object tracking VOT2019 challenge results. In 2019\nIEEE/CVF International Conference on Computer V ision\nW orkshop (ICCVW), pages 2206–2241, 2019.\n6\n[28] Hei Law and Jia Deng. Cornernet: Detecting objects as\npaired keypoints. In Computer V ision - ECCV 2018 - 15th\nEuropean Conference, Munich, Germany, September 8-14,\n2018, Proceedings, P art XIV , volume 11218 of Lecture Notes\nin Computer Science , pages 765–781. Springer, 2018.\n5\n[29] B. Li, W . Wu, Q. W ang, F . Zhang, J. Xing, and J. Y an.\nSiamRPN++: Evolution of Siamese visual tracking with very\ndeep networks. In 2019 IEEE/CVF Conference on Computer\nV ision and P attern Recognition (CVPR) , pages 4277–4286,\n2019.\n1, 2, 4, 5, 6, 7\n[30] B. Li, J. Y an, W . Wu, Z. Zhu, and X. Hu. High performance\nvisual tracking with Siamese region proposal network. In\n2018 IEEE/CVF Conference on Computer V ision and P at-\ntern Recognition , pages 8971–8980, 2018.\n1, 2, 4, 5\n[31] T . Lin, P . Goyal, R. Girshick, K. He, and P . Doll´ ar. Foca l\nloss for dense object detection. In 2017 IEEE International\nConference on Computer V ision (ICCV) , pages 2999–3007,\n2017. 5, 8\n[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hay s,\nPietro Perona, Deva Ramanan, Piotr Doll´ ar, and C. Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nDavid Fleet, T omas Pajdla, Bernt Schiele, and Tinne Tuyte-\nlaars, editors, Computer V ision – ECCV 2014 , Cham, 2014.\nSpringer International Publishing.\n6\n[33] A. Lukeˇ ziˇ c, J. Matas, and M. Kristan. D3S - A discrimin ative\nsingle shot segmentation tracker. In 2020 IEEE/CVF Confer-\nence on Computer V ision and P attern Recognition (CVPR) ,\npages 7131–7140, 2020. 6\n[34] Christoph L ¨ uscher, Eugen Beck, Kazuki Irie, Markus Ki tza,\nWilfried Michel, Albert Zeyer, Ralf Schl ¨ uter, and Hermann\nNey . RWTH ASR systems for librispeech: Hybrid vs at-\ntention. In Gernot Kubin and Zdravko Kacic, editors, Inter-\nspeech 2019, 20th Annual Conference of the International\nSpeech Communication Association, Graz, Austria, 15-19\nSeptember 2019 , pages 231–235. ISCA, 2019.\n3\n[35] C. Ma, J. Huang, X. Y ang, and M. Y ang. Hierarchical convo -\nlutional features for visual tracking. In 2015 IEEE Interna-\ntional Conference on Computer V ision (ICCV) , pages 3074–\n3082, 2015. 2\n[36] Tim Meinhardt, Alexander Kirillov , Laura Leal-T aixe,\nand Christoph Feichtenhofer. Trackformer: Multi-\nobject tracking with Transformers. arXiv e-prints , page\narXiv:2101.02702, Jan. 2021.\n3\n[37] Matthias Mueller, Neil Smith, and Bernard Ghanem. A\nbenchmark and simulator for UA V tracking. In Bastian\nLeibe, Jiri Matas, Nicu Sebe, and Max W elling, editors,\nComputer V ision – ECCV 2016 , pages 445–461, Cham,\n2016. Springer International Publishing.\n7\n[38] Matthias M ¨ uller, Adel Bibi, Silvio Giancola, Salman A l-\nsubaihi, and Bernard Ghanem. Trackingnet: A large-scale\ndataset and benchmark for object tracking in the wild. In\nVittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and\nY air W eiss, editors, Computer V ision – ECCV 2018 , pages\n310–327, Cham, 2018. Springer International Publishing.\n7\n[39] H. Nam and B. Han. Learning multi-domain convolutional\nneural networks for visual tracking. In 2016 IEEE Confer-\nence on Computer V ision and P attern Recognition (CVPR) ,\npages 4293–4302, 2016. 7\n[40] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. 2019.\n3\n[41] E. Real, J. Shlens, S. Mazzocchi, X. Pan, and V . V anhouck e.\nY ouTube-BoundingBoxes: A large high-precision human-\nannotated data set for object detection in video. In 2017\nIEEE Conference on Computer V ision and P attern Recog-\nnition (CVPR) , pages 7464–7473, 2017.\n6\n[42] Olga Russakovsky , Jia Deng, Hao Su, Jonathan Krause, Sa n-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy ,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\nLi Fei-Fei. ImageNet large scale visual recognition chal-\nlenge. Int. J. Comput. V ision , 115(3):211–252, Dec. 2015.\n6\n[43] C. Sun, D. W ang, H. Lu, and M. Y ang. Correlation track-\ning via joint discrimination and reliability learning. In\n2018 IEEE/CVF Conference on Computer V ision and P at-\ntern Recognition , pages 489–497, 2018.\n1\n[44] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao,\nXinting Hu, T ao Kong, Zehuan Y uan, Changhu W ang, and\nPing Luo. Transtrack: Multiple-object tracking with Trans -\nformer. arXiv e-prints , page arXiv:2012.15460, Dec. 2020.\n3\n[45] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence t o\nsequence learning with neural networks. In Z. Ghahramani,\nM. W elling, C. Cortes, N. Lawrence, and K. Q. W einberger,\neditors, Advances in Neural Information Processing Systems ,\nvolume 27. Curran Associates, Inc., 2014.\n3\n[46] Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, T atiana\nLikhomanenko, Edouard Grave, Vineel Pratap, Anuroop\nSriram, Vitaliy Liptchinsky , and Ronan Collobert. End-\nto-end ASR: from supervised to semi-supervised learn-\ning with modern architectures. arXiv e-prints , page\narXiv:1911.08460, Nov . 2019.\n3\n[47] Aaron van den Oord, Y azhe Li, Igor Babuschkin, Karen Si-\nmonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den\nDriessche, Edward Lockhart, Luis Cobo, Florian Stimberg,\nNorman Casagrande, et al. Parallel WaveNet: Fast high-\nﬁdelity speech synthesis. In Jennifer Dy and Andreas\nKrause, editors, Proceedings of the 35th International Con-\nference on Machine Learning , volume 80 of Proceedings\nof Machine Learning Research , pages 3918–3926, Stock-\nholmsm¨ assan, Stockholm Sweden, 10–15 Jul 2018. PMLR.\n3\n[48] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko -\nreit, Llion Jones, Aidan N. Gomez, undeﬁnedukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Proceed-\nings of the 31st International Conference on Neural Informa -\ntion Processing Systems , NIPS’17, pages 6000 – 6010, Red\nHook, NY , USA, 2017. Curran Associates Inc.\n1, 2, 3, 4, 7,\n8\n[49] P . V oigtlaender, J. Luiten, P . H. S. T orr, and B. Leibe. S iam\nr-cnn: Visual tracking by re-detection. In 2020 IEEE/CVF\nConference on Computer V ision and P attern Recognition\n(CVPR), pages 6577–6587, 2020.\n7\n[50] G. W ang, C. Luo, X. Sun, Z. Xiong, and W . Zeng. Track-\ning by instance detection: A meta-learning approach. In\n2020 IEEE/CVF Conference on Computer V ision and P at-\ntern Recognition (CVPR) , pages 6287–6296, 2020.\n6, 7\n[51] Q. W ang, Z. T eng, J. Xing, J. Gao, W . Hu, and S. May-\nbank. Learning attentions: Residual attentional Siamese\nnetwork for high performance online visual tracking. In\n2018 IEEE/CVF Conference on Computer V ision and P at-\ntern Recognition , pages 4854–4863, 2018.\n1, 2\n[52] Y . Wu, J. Lim, and M. Y ang. Object tracking benchmark.\nIEEE Transactions on P attern Analysis and Machine Intelli-\ngence, 37(9):1834–1848, 2015.\n6\n[53] Yinda Xu, Zeyu W ang, Zuoxin Li, Y e Y uan, and Gang Y u.\nSiamFC++: T owards robust and accurate visual tracking\nwith target estimation guidelines. Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , 34(07):12549–12556,\nApr. 2020.\n1, 6\n[54] Y . Y u, Y . Xiong, W . Huang, and M. R. Scott. Deformable\nSiamese attention networks for visual object tracking. In\n2020 IEEE/CVF Conference on Computer V ision and P at-\ntern Recognition (CVPR) , pages 6727–6736, 2020.\n2, 6, 7\n[55] L. Zhang, A. Gonzalez-Garcia, J. V . D. W eijer, M. Danell -\njan, and F . S. Khan. Learning the model update for Siamese\ntrackers. In 2019 IEEE/CVF International Conference on\nComputer V ision (ICCV) , pages 4009–4018, 2019.\n1\n[56] Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and\nW eiming Hu. Ocean: Object-aware anchor-free tracking.\nIn Andrea V edaldi, Horst Bischof, Thomas Brox, and Jan-\nMichael Frahm, editors, Computer V ision – ECCV 2020 ,\npages 771–787, Cham, 2020. Springer International Publish -\ning.\n1, 6\n[57] Xingyi Zhou, Dequan W ang, and Philipp Kr¨ ahenb ¨ uhl. Ob -\njects as points. arXiv e-prints , page arXiv:1904.07850, Apr.\n2019. 4, 5\n[58] Xizhou Zhu, W eijie Su, Lewei Lu, Bin Li, Xiaogang W ang,\nand Jifeng Dai. Deformable DETR: Deformable Transform-\ners for end-to-end object detection. arXiv e-prints , page\narXiv:2010.04159, Oct. 2020.\n3\n[59] Zheng Zhu, Qiang W ang, Bo Li, W ei Wu, Junjie Y an, and\nW eiming Hu. Distractor-aware Siamese networks for visual\nobject tracking. In Vittorio Ferrari, Martial Hebert, Cris tian\nSminchisescu, and Y air W eiss, editors, Computer V ision –\nECCV 2018 , pages 103–119, Cham, 2018. Springer Interna-\ntional Publishing.\n1\n[60] Z. Zhu, W . Wu, W . Zou, and J. Y an. End-to-end ﬂow cor-\nrelation tracking with spatial-temporal attention. In 2018\nIEEE/CVF Conference on Computer V ision and P attern\nRecognition, pages 548–557, 2018.\n2",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7934870719909668
    },
    {
      "name": "Discriminative model",
      "score": 0.7488830089569092
    },
    {
      "name": "Artificial intelligence",
      "score": 0.684211254119873
    },
    {
      "name": "BitTorrent tracker",
      "score": 0.6801052093505859
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6294538974761963
    },
    {
      "name": "Encoder",
      "score": 0.6131157279014587
    },
    {
      "name": "Transformer",
      "score": 0.5215185284614563
    },
    {
      "name": "Eye tracking",
      "score": 0.48685091733932495
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43996939063072205
    },
    {
      "name": "Computer vision",
      "score": 0.41706350445747375
    },
    {
      "name": "Engineering",
      "score": 0.10034435987472534
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74801974",
      "name": "The University of Tokyo",
      "country": "JP"
    }
  ]
}