{
  "title": "A Batch Noise Contrastive Estimation Approach for Training Large Vocabulary Language Models",
  "url": "https://openalex.org/W2747886799",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A321232085",
      "name": "Youssef Oualil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1237914314",
      "name": "Dietrich Klakow",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2152808281",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W1520465330",
    "https://openalex.org/W36903255",
    "https://openalex.org/W1558797106",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2963160216",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2950797609",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2437096199"
  ],
  "abstract": "Training large vocabulary Neural Network Language Models (NNLMs) is a difficult task due to the explicit requirement of the output layer normalization, which typically involves the evaluation of the full softmax function over the complete vocabulary. This paper proposes a Batch Noise Contrastive Estimation (B-NCE) approach to alleviate this problem. This is achieved by reducing the vocabulary, at each time step, to the target words in the batch and then replacing the softmax by the noise contrastive estimation approach, where these words play the role of targets and noise samples at the same time. In doing so, the proposed approach can be fully formulated and implemented using optimal dense matrix operations. Applying B-NCE to train different NNLMs on the Large Text Compression Benchmark (LTCB) and the One Billion Word Benchmark (OBWB) shows a significant reduction of the training time with no noticeable degradation of the models performance. This paper also presents a new baseline comparative study of different standard NNLMs on the large OBWB on a single Titan-X GPU.",
  "full_text": "arXiv:1708.05997v2  [cs.CL]  22 Aug 2017\nA Batch Noise Contrastive Estimation Approach for T rainingLarge\nV ocabulary Language Models\nY oussef Oualil, Dietrich Klakow\nSpoken Language Systems (LSV)\nCollaborative Research Center on Information Density and L inguistic Encoding\nSaarland University, Saarbr ¨ ucken, Germany\n{firstname.lastname}@lsv.uni-saarland.de\nAbstract\nTraining large vocabulary Neural Network Language Models\n(NNLMs) is a difﬁcult task due to the explicit requirement of the\noutput layer normalization, which typically involves the e valu-\nation of the full softmax function over the complete vocabul ary .\nThis paper proposes a Batch Noise Contrastive Estimation (B -\nNCE) approach to alleviate this problem. This is achieved by\nreducing the vocabulary , at each time step, to the target wor ds\nin the batch and then replacing the softmax by the noise con-\ntrastive estimation approach, where these words play the ro le\nof targets and noise samples at the same time. In doing so, the\nproposed approach can be fully formulated and implemented\nusing optimal dense matrix operations. Applying B-NCE to\ntrain different NNLMs on the Large T ext Compression Bench-\nmark (L TCB) and the One Billion W ord Benchmark (OBWB)\nshows a signiﬁcant reduction of the training time with no not ice-\nable degradation of the models performance. This paper also\npresents a new baseline comparative study of different stan dard\nNNLMs on the large OBWB on a single Titan-X GPU.\nIndex T erms: neural networks, language modeling, noise con-\ntrastive estimation\n1. Introduction\nNeural Network Language Models (NNLM) [1, 2] have been\nshown to signiﬁcantly outperform standard N-gram LMs [3, 4]\non many speech and language technology applications, such a s\nmachine translation [5] and speech recognition [6]. The tra ining\nand evaluation of these models, however, becomes signiﬁcan tly\nslow and challenging when considering large vocabulary lan -\nguage models [7] . This is mainly due to the explicit normaliz a-\ntion of the output layer, which typically requires the evalu ation\nof the full softmax function over the complete vocabulary .\nIn order to overcome this problem, Schwenk et al. [8] pro-\nposed to use a short list of frequent words in combination wit h\nN-gram models. The performance of this approach, however,\nsigniﬁcantly depends on the short list size. In a different a t-\ntempt, Morin et al. [9] proposed to factorize the output prob -\nabilities using a binary tree, which results in an exponenti al\nspeed-up of the training and evaluation, whereas Mikolov et\nal. [10] proposed to use an additional class layer as an alter -\nnative to the factorization. The performance of these two ap -\nproaches signiﬁcantly depends on the design of the binary tr ee\nand the class layer size, respectively . As an alternative to mod-\nifying the architecture design, the authors of [11] used imp or-\ntance sampling to approximate the output gradient. Unfortu -\nnately , this approach requires a control of the samples vari ance,\nwhich can lead otherwise to unstable learning [12]. In a sim-\nilar work, Mnih et al. [13] proposed to use Noise Contrastive\nEstimation (NCE) [14] to speed-up the training. NCE treats\nthe learning as a binary classiﬁcation problem between a tar get\nword and noise samples, which are drawn from a noise distri-\nbution. Moreover, NCE considers the normalization term as a n\nadditional parameter that can be learned during training or ﬁxed\nbeforehand. In this case, the network learns to self-normalize.\nThis property makes NCE more attractive compared to other\nsampling methods, such as importance sampling, which would\nstill require the use of the softmax function during evaluat ion.\nIn batch mode training, however, the implementation of NCE\ncannot be directly formulated using dense matrix operation s,\nwhich compromises their speed-up gains.\nThis paper proposes a new solution to train large vocab-\nulary LMs using NCE in batch mode. The main idea here is\nto restrict the vocabulary , at each iteration, to the words i n the\nbatch and replace the standard softmax function by NCE. In par-\nticular, the target words (to predict) in the batch play the r ole of\ntargets and noise samples at the same time. In doing so, the\nproposed approach does not require any sampling and can be\nfully formulated using dense matrix operations, leading to sig-\nniﬁcant speed-up improvements with no noticeable degradat ion\nof the performance. Moreover, we can show that this approach\noptimally approximates the unigram noise distribution, wh ich\nis widely used in NCE-based LMs (Section 3).\nWhile applying the proposed batch NCE approach, this\npaper also presents a new baseline comparative study of dif-\nferent NNLMs on the Large T ext Compression Benchmark\n(L TCB) [15] and the One Billion W ord Benchmark (OBWB) [7]\non a single Titan-X GPU (Section 4).\n2. Noise Contrastive Estimation\nProbabilistic NNLM generally require the normalization of the\noutput layer to produce meaningful probability distributi ons.\nUsing the softmax function, the probability of the next word\nw, given the context c and the model parameters θ, is given by\npc\nθ (w) = p(w|c, θ ) = exp(Mθ (w, c ))\n∑\nv∈V exp(Mθ (v, c )) (1)\nV is the vocabulary and Mθ is the neural scoring function.\nLearning the parameters θ generally requires the evaluation of\nthe normalization term for each context c. This evaluation in-\nvolves the complete vocabulary and therefore, becomes very\nchallenging and resource demanding for large vocabulary co r-\npora. As an alternative solution, [14] proposed to use noise con-\ntrastive estimation to train unnormalized probabilistic m odels.\nThus, pc\nθ (w) is approximated as\npc\nθ (w) ≈ exp(Mθ (w, c ))\nZc\n(2)\nZc is a context-dependent normalization term, which is ﬁxed\nin practice to a constant value Z (e.g., Z = 1 in [16] and\nZ = exp(9) in [17]. The latter constant will be used in the\nexperiments conducted in Section 4).\nThe idea behind NCE is to cast the density estimation prob-\nlem to a binary classiﬁcation task, which learns to discrimi nate\nbetween real samples drawn from the data distribution pc\nD and\nnoise samples drawn from a given noise distribution pc\nn. Al-\nthough pc\nn is context-dependent, it has been shown (e.g. [13])\nthat context-independent noise distributions such as unig ram,\nare sufﬁcient to achieve a good performance. Thus, pc\nn = pn\nin the rest of this paper. If K denotes the number of noise sam-\nples, the probability that the word w is generated from the data\ndistribution ( L = 1 ) or noise distribution ( L = 0 ) are given by\npw\nc (1)\n∆\n= p(L = 1 |w, c ) = pc\nθ (w)\npc\nθ (w) + K · pn(w) (3)\npw\nc (0)\n∆\n= p(L = 0 |w, c ) = 1 − p(L = 1 |w, c ) (4)\nAccording to NCE, the model distribution pc\nθ is expected to\nconverge towards the data distribution pc\nD after minimizing the\nfollowing objective function on the data D\nJ (θ) = −\nD∑\nwi\n(\nlog(pwi\nci (1)) +\n∑\nk\nlog(p\nwk\ni\nci (0))\n)\n(5)\nwhere {wk\ni }k, k = 1 , . . . , K denote the K noise samples,\nwhich are drawn from the noise distribution pn, to train the\nmodel on the target word wi. The gradient of J (θ) is given\nby\n∂J (θ)\n∂θ = −\nD∑\nwi\n(\npwi\nci (0) ∂log (pci\nθ (wi))\n∂θ\n−\n∑\nk\np\nwk\ni\nci (1) ∂log (pci\nθ (wk\ni ))\n∂θ\n)\n(6)\nNCE training of a neural network follows the standard back-\npropagation algorithm applied to the objective function (5 ) and\nits gradient (6). More details about NCE and its gradient der iva-\ntion can be found in [14].\n2.1. NCE vs Importance Sampling\nThe authors of [18] have shown that NCE and Importance Sam-\npling (IS) are closely related, with the main difference is t hat\nNCE is deﬁned as a binary classiﬁer between samples drawn\nfrom data or noise distributions with a logistic loss, where as\nIS is a multi-class classiﬁer, which uses softmax and a cross-\nentropy loss. Hence, the authors concluded that IS is theore ti-\ncally a better choice than NCE. The results reported, howeve r,\nshowed a minor difference in performance ( 2. 4 points in per-\nplexity). Moreover, training using IS can be very difﬁcult a nd\nrequires a careful control of the samples variance, which ca n\nlead otherwise to unstable learning as was reported in [12].\nHence, an adaptive IS may use a large number of samples to\nsolve this problem whereas NCE is more stable and requires a\nﬁxed small number of noise samples (e.g., 100) to achieve a\ngood performance [13, 16]. Furthermore, the network learns\nto self-normalize during training using NCE. As a results, and\non the contrary to IS, the softmax is no longer required during\nevaluation, which makes NCE an attractive choice to train la rge\nvocabulary NNLM. The next section will show how NCE can\nbe efﬁciently implemented in batch mode training.\n3. Batch Noise Contrastive Estimation\nAlthough NCE is a good alternative to train large vocabulary\nLMs, it is not well-suited for batch mode training on GPUs.\nMore particularly , each target word in the batch uses a diffe rent\nset of noise samples, which makes it difﬁcult to formulate th e\nlearning using dense matrix operations. As a result, the tra ining\ntime signiﬁcantly increases. T o alleviate this problem, no ise\nsamples can be shared across the batch [16].\nThis paper proposes an extension of NCE to batch mode\n(B-NCE) training. This approach does not require any sampli ng\nand can be formulated using dense matrix operations. Furthe r-\nmore, we can show that this solution optimally approximates the\nsampling from a unigram distribution, which has been shown t o\nbe a good noise distribution choice [13, 16].\nThe main idea here is to restrict the vocabulary , at each\nforward-backward pass, to the target words in the batch (wor ds\nto predict) and then replace the softmax function by NCE. In\nparticular, these words play alternatively the role of targ ets and\nnoise samples. That is, for a target word wi, at batch index i, the\nrest of the target batch (the remaining target words at the ot her\nbatch indices j, j ̸= i) are considered to be the noise samples.\nThe rest of this section introduces the mathematical formul a-\ntion of B-NCE to efﬁciently calculate the error with respect to\nthe output layer weights and biases, as well as the error at th e\nprevious layer in batch training, using the objective funct ion (5)\nand its gradient (6).\n3.1. LM T raining using B-NCE\nLet B, H and V be the sizes of the batch, the last hidden layer\nand the vocabulary , respectively . The matrix Lt (size B × H)\nwill denote the evaluation of the last hidden layer at time t on\nthe current batch. Let V t = {wt\nb}B\nb=1 be the target words in\nthe batch at time t and let W (size H × V ) and C (1 × V )\ndenote the hidden-to-output weight matrix and bias vector, re-\nspectively . Our goal here is to calculate the error (delta) o f the\noutput weights W and biases C, as well as the error at the pre-\nvious layer Lt.\nThe output layer evaluation in a feed-forward pass of B-\nNCE, at time t, is calculated by restricting the output layer to\nV t. That is, we use the sub-matrix weights W t (H × B) and\nsub-vector bias Ct (1 × B), which are obtained by restricting\nW and C to V t. Hence, the B-NCE network output Ot, at time\nt, is given by\nOt = exp(Lt · W t ⊕ Ct)\nZ (size B×B) (7)\n⊕ adds the vector Ct to each row of the left-hand matrix. Note\nthat (7) is the B-NCE matrix form of (2) in batch training.\nNow , let Nt = pn({wt\nb}) (1 × B) be the probability of the\ntarget words in the batch according to the noise distributio n pn.\nIn order to evaluate the gradient of the objective function w .r.t.\nthe output weights and biases, we ﬁrst deﬁne the normalizati on\nmatrix Y t according to\nY t = Ot ⊕ (B − 1) · Nt (size B×B) (8)\nY t is simply the normalization term in equations (3) and (4)\nwith K = B − 1. This is a direct result of using the rest of the\nwords in the output batch as NCE noise samples, for each targe t\nword wt\nb. In doing so, we eliminate the sampling step.\nIn order to calculate (6) w .r.t. the output weights and biase s,\nwe ﬁrst introduce the auxiliary B-NCE gradient matrix Gt (B ×\nB) at time t, given by\nGt(i, j ) =\n\n\n\nOt\nB (i,j )\nY t(i,j ) if i ̸= j\n−(B−1)·Nt (i)\nY t(i,j ) otherwise\nGt is nothing but the element-wise division of Ot and Y t,\nafter replacing the diagonal of Ot by −(B − 1) · Nt. Then,\napplying the NCE gradient derivation given by (6), B-NCE cal -\nculates the output weight error ∆ W t, the output bias error ∆ Ct\nas well as the error E(Lt) at the previous layer according to\n∆ W t = Lt⊤\n· G t (9)\n∆ Ct =\n∑\nrow\nGt (10)\nE(Lt) = Gt · W t⊤\n(11)\nOnce the error E(Lt) is propagated to the last hidden layer Lt\nusing (11), the learning of the rest of the network follows th e\nstandard back-propagation algorithm.\nAfter processing the complete training data, each word w\nin the vocabulary will be used exactly (B − 1) × count(w) as\na noise sample. This is strictly equivalent to sampling from a\nunigram noise distribution, which shows that B-NCE is an op-\ntimal implementation of NCE using unigram as noise distribu -\ntion. W e should also mention that some words may occur more\nthan once in the batch. This observation should be taken into\nconsideration before updating the weights and biases.\n3.2. Adaptive B-NCE\nThe proposed B-NCE approach as deﬁned above uses a ﬁxed\nnumber of noise samples ( B − 1), which is dependent on the\nbatch size. In cases where the latter is small (e.g., B ≤ 100),\nB-NCE can be extended to use an additional K noise samples.\nThis can be done by simply drawing an additional K samples\nform the noise distribution pn, and share them across the batch\nas it was done in [16]. The adaptive B-NCE follows the exact\nsame steps described above using the extended output weight\nsub-matrix W t\nB+K (H ×(B+K)), and the extended sub-vector\nbias Ct\nB+K (1×(B +K)) to evaluate (7), whereas (8) becomes\nY t = Ot ⊕ (B + K − 1) · Nt\nB+K B×(B + K) (12)\nwhere Nt\nB+K =\n[\nNt , N t\nK\n]\n. Nt\nK are the probabilities of the\nadditional K noise samples using the noise distribution pn.\n4. Experiments and Results\nT o evaluate the proposed B-NCE approach, we conducted a set\nof LM experiments on two different corpora. Namely , the Larg e\nT ext Compression Benchmark (L TCB) [15], which is an ex-\ntract of the enwik9 dataset, and the very large One Billion W o rd\nBenchmark (OBWB) [7].\nThe L TCB data split and processing is the same as the one\nused in [19, 20]. In particular, the L TCB vocabulary is limit ed to\nthe 80K most frequent words with all remaining words replace d\nby <unk>. Similarly to RNNLM toolkit [10], we add a single\n</s> tag at the end of each sentence whereas the begin tag\n<s> is not used. The resulting corpus size is 133M with an\n<unk> rate of 1. 43% for the training set and 2. 30% for the\ntest set. The second corpus is the OBWB, which contains ≈\n0. 8B tokens with a vocabulary size of ≈ 0. 8M words. The\ndata processing follows the description provided in [7] lea ding\nto an <unk> rate of ≈ 0. 3%. Similarly to L TCB, a single </s>\ntag is added at the end of each sentence. In the experiments\ndescribed below , the ﬁrst 5 held-out sets are used for valida tion\nwhereas the remaining 45 sets are used for testing. The obtai ned\nresults, however, showed that the models perplexity on thes e\ntwo sets is comparable, with an average difference of less th an\n0.5 points in perplexity .\nThe primary motive of using L TCB, with its medium vo-\ncabulary size (80K), is to be able to compare the performance\nof LMs trained using NCE to their counterparts that are train ed\nusing the full softmax. When using NCE to train the models,\nthe evaluation is either performed using the NCE constant Z\nfor normalization (PPL n), in this case the target word proba-\nbilities are given by (2), or using the softmax function (PPL f ),\nwhich calculates these probabilities using (1). The differ ence\nin performance between these metrics will evaluate the abil ity\nof the models to learn to self-normalize after training. For a\ncomprehensive comparison of the different models, we also r e-\nport the Number of Parameters (NoP) required by each model\nas well as its Training Speed (TS), which is calculated as the\nnumber of words processed per second (w/s) during training.\nAll experiments were conducted on a single Titan-X GPU.\n4.1. Baseline Models\nIn order to assess the gap among established NNLMs, this pa-\nper also presents a comparative study of different standard ar-\nchitectures with comparable NoPs. That is, we report result s for\nthe standard Feed-forward network (FFNN) [1], the Recurren t\nNeural Network (RNN) [10] as well as the Long-Short T erm\nMemory network (LSTM) [21]. Our RNN implementation uses\na projection weight matrix to decouple the word embedding an d\nthe hidden layer sizes. W e also report results after adding a bot-\ntleneck fully-connected ReLu layer right before the output layer\nin the recurrent models. These models are marked with the pre -\nﬁx ReLu in the tables below . Each of the models is trained us-\ning the proposed B-NCE approach and the shared noise NCE\n(S-NCE) [16]. For the L TCB corpus, we also report results\nof the models trained with the full softmax function. This is\nthe primary motive for using this corpus. W e would like also\nto highlight that the goal of this paper is not about improvin g\nLMs performance but rather showing how a signiﬁcant train-\ning speed-up can be achieved without compromising the mod-\nels performance for large vocabulary LMs. Hence, we solely\nfocus our experiments on NCE as a major approach to achieve\nthis goal [17, 13, 16] in comparison to the reference full soft-\nmax function. Comparison to other training approaches such as\nimportance sampling will be conducted in future work.\n4.2. L TCB Experiments\nFor the L TCB experiments, the embedding size is ﬁxed at 200,\nthe 5-gram FFNN has two hidden layers, whereas RNN and\nLSTM use a single recurrent layer. All non-recurrent layers use\nReLu as activation function. More details about the models a r-\nchitectures are shown in T able 1, where “(R)” stands for recu r-\nrent and “(B)” for bottleneck. The batch size is ﬁxed at 400 an d\nthe initial learning rate is set to 0.4. The latter is halved w hen\nno improvement on the validation data is observed for an ad-\nditional 7 epochs. W e also use a norm-based gradient clippin g\nwith a threshold of 5 but we do not use dropout. Moreover, B-\nNCE and S-NCE use the unigram as noise distribution pn. Fol-\nlowing the setup proposed in [13, 16], S-NCE uses K = 100\nnoise samples, whereas B-NCE uses only the target words in th e\nbatch (K=0). Note that S-NCE will process and update B + K\nwords at its output layer during each forward-backward pass ,\nwhereas B-NCE updates only B words. Similarly to [17], the\nNCE normalization constant is set to Z = exp(9), which ap-\nproximates the mean of the normalization term using softmax.\nT able 1: Model architecture for LTCB experiments.\nModel Architecture\n5-gram FFNN 4×200−600−400(B)−V\nRNN 200−600(R)−V\nReLu-RNN 200−600(R)−400(B)−V\nLSTM 200−600(R)−V\nReLu-LSTM 200−600(R)−400(B)−V\nThe L TCB results reported in T able 2 clearly show that B-\nNCE reduces the training time by a factor of 4 to 8 with a slight\ndegradation in the models performance compared to softmax.\nMoreover, we can also see that B-NCE slightly outperforms S-\nNCE while being faster and simpler to implement. In partic-\nular, B-NCE does not require the sampling step since it uses\nthe rest of the output words in the batch itself as noise sampl es\nto train the model on each target word. This can be efﬁciently\nimplemented using dense matrix operations (see Sections 3) .\nT able 2 also shows that PPL n is close from PPL f , which typi-\ncally reﬂects that the models trained using NCE are able to se lf-\nnormalize, where the normalization term using softmax is, i n\naverage, very close from the NCE constant Z. W e have also ob-\nserved in our experiments that the models degradation and th e\ngap between PPL f and PPL n strongly depend on the amount\nof training data, the vocabulary size as well as the size of th e\nlast hidden layer. More particularly , increasing the train ing data\nleads to a more stable learning and therefore to a smaller gap\nbetween these two metrics and a much lower degradation of the\nmodels performance (see OBWB experiments below).\nT able 2: LMs performance on LTCB.\nPPLn PPLf TS (w/s) NoP\n5-gram FFNN ( softmax) — 110.2 8.4K 48.8M\n5-gram FFNN (S-NCE) 129.8 125.4 29.1K 48.8M\n5-gram FFNN (B-NCE) 119.4 113.7 35.1K 48.8M\nRNN(softmax) — 79.7 5.9K 64.6M\nRNN (S-NCE) 88.7 84.2 37.8K 64.6M\nRNN (B-NCE) 87.6 82.5 43.7K 64.6M\nReLu-RNN ( softmax) — 69.5 8.6K 48.8M\nReLu-RNN (S-NCE) 80.2 77.3 30.9K 48.8M\nReLu-RNN (B-NCE) 79.4 76.0 36.7K 48.8M\nLSTM ( softmax) — 62.5 8.9K 66.0M\nLSTM (S-NCE) 77.4 73.1 27.2K 66.0M\nLSTM (B-NCE) 70.9 68.3 37.1K 66.0M\nReLu-LSTM ( softmax) — 59.2 8.2K 50.3M\nReLu-LSTM (S-NCE) 68.4 67.1 26.9K 50.3M\nReLu-LSTM (B-NCE) 64.9 62.4 32.0K 50.3M\nW e can also conclude from T able 2 that the additional ReLu\nlayer improves the performance while signiﬁcantly decreas ing\nthe number of parameters (NoP). This conclusion is valid for\nboth, RNN and LSTM. These results conﬁrm that adding a\nfully-connected bottleneck layer can signiﬁcantly boost t he per-\nformance of recurrent models. This idea has been previously\nused in computer vision tasks in combination with Convolu-\ntional Neural Networks (CNN) [22], as well as in speech recog -\nnition [23], where the fully-connected layer is used as pat o f the\nLSTM recurrent module.\n4.3. One Billion W ord Benchmark Experiments\nThe OBWB experiments are similar to L TCB with minor differ-\nences. Namely , the embedding size is set to 500 for all models ,\nthe batch size is ﬁxed at 500, S-NCE uses K = 200 noise sam-\nples and the initial learning rate is set to 1.0. Given that th e\nvocabulary size is ≈ 0. 8M, it was not possible to train the lan-\nguage models using the full softmax function. Therefore, we\nonly report results for B-NCE and S-NCE. More details about\nthe models conﬁguration are shown in T able 3.\nT able 3: Model architecture for OBWB experiments.\nModel Architecture\n5-gram FFNN 4×500−1500−600(B)−V\nRNN 500−1500(R)−V\nReLu-RNN 500−1500(R)−600(B)−V\nLSTM 500−1500(R)−V\nReLu-LSTM 500−1500(R)−600(B)−V\nThe OBWB results in T able 4 generally conﬁrm the L TCB\nconclusions. That is, B-NCE slightly outperforms S-NCE whi le\nbeing faster and simpler to train (training speed in 3rd column).\nMoreover, these results also show a much smaller difference\nbetween PPL f and PPL n compared to L TCB, which suggests\nthat the models learned to better self-normalize due to the l arger\namount of training data. Similarly to L TCB, we can see that th e\nadditional ReLu helps reducing the NoPs while improving or\nmaintaining the models performance for RNN and LSTM.\nT able 4: LMs performance on OBWB.\nPPLn PPLf TS (w/s) NoP\n5-gram FFNN (S-NCE) 86.3 84.4 12.3K 0.88B\n5-gram FFNN (B-NCE) 81.9 80.6 13.4K 0.88B\nRNN (S-NCE) 70.8 67.6 24.9K 1.59B\nRNN (B-NCE) 63.4 61.4 27.8K 1.59B\nReLu-RNN (S-NCE) 59.6 59.1 20.1K 0.88B\nReLu-RNN (B-NCE) 56.9 56.6 23.1K 0.88B\nLSTM (S-NCE) 51.3 50.3 12.0K 1.60B\nLSTM (B-NCE) 48.6 48.1 13.1K 1.60B\nReLu-LSTM (S-NCE) 51.0 50.9 13.0K 0.89B\nReLu-LSTM (B-NCE) 49.2 48.8 14.7K 0.89B\nIn comparison to other results on the OBWB. W e can see\nthat the small ReLu-RNN achieves a close performance from\nthe very large RNN model (PPL = 51.3 and NoP = 20B) pro-\nposed in [7]. Moreover, the performance of the small ReLu-\nLSTM is comparable to the LSTM models proposed in [16]\nand [18] which use large hidden layers. In particular, the ﬁr st\npaper trains a large 4-layers LSTM model using S-NCE on 4\nGPUs (PPL = 43.2 and NoP = 3.4B), whereas the second uses\na recurrent bottleneck layer [23] and a total of K = 8192 noise\nsamples with importance sampling on 32 T esla K40 GPUs.\n5. Conclusions and Future W ork\nW e have presented a batch-NCE approach which allows a fast\nand simple training of large vocabulary LMs. This approach\neliminates the sampling step required in standard NCE and ca n\nbe fully formulated using dense matrix operations. Experim ents\non L TCB and OBWB have shown that this approach achieves\na comparable performance to a softmax function while signiﬁ-\ncantly speeding-up the training. While the evaluation focu sed\non NCE performance, future experiments will be conducted to\nevaluate B-NCE in comparison to other alternatives of softmax.\n6. Acknowledgment\nThis research was funded by the German Research Foundation\n(DFG) as part of SFB 1102.\n7. References\n[1] Y . Bengio, R. Ducharme, P . V incent, and C. Jauvin, “ A neur al\nprobabilistic language model, ” Journal of machine learning re-\nsearch, vol. 3, pp. 1137–1155, Mar . 2003.\n[2] T . Mikolov , M. Karaﬁ ´ at, L. Burget, J. Cernock ´ y, and S. K hu-\ndanpur, “Recurrent neural network based language model, ” i n\n11th Annual Conference of the International Speech Communi ca-\ntion Association (INTERSPEECH) , Makuhari, Chiba, Japan, Sep.\n2010, pp. 1045–1048.\n[3] R. Rosenfeld, “T wo decades of statistical language mode ling:\nWhere do we go from here?” in Proceedings of the IEEE , vol. 88,\n2000, pp. 1270–1278.\n[4] R. Kneser and H. Ney , “Improved backing-off for m-gram la n-\nguage modeling, ” in International Conference on Acoustics,\nSpeech, and Signal Processing, (ICASSP) , Detroit, Michigan,\nUSA, May 1995, pp. 181–184.\n[5] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine trans la-\ntion by jointly learning to align and translate, ” in International\nConference on Learning Representations (ICLR) , 2014.\n[6] G. Hinton, L. Deng, D. Y u, G. E. Dahl, A. r . Mohamed, N. Jait ly ,\nA. Senior, V . V anhoucke, P . Nguyen, T . N. Sainath, and B. King s-\nbury , “Deep neural networks for acoustic modeling in speech\nrecognition: The shared views of four research groups, ” IEEE Sig-\nnal Processing Magazine , vol. 29, no. 6, pp. 82–97, Nov . 2012.\n[7] C. Chelba, T . Mikolov , M. Schuster, Q. Ge, T . Brants, P . Ko ehn,\nand T . Robinson, “One billion word benchmark for measur-\ning progress in statistical language modeling, ” CoRR, vol.\nabs/1312.3005, 2013.\n[8] H. Schwenk and J. Gauvain, “Training neural network lang uage\nmodels on very large corpora, ” in Proceedings of the Conference\non Human Language T echnology and Empirical Methods in Nat-\nural Language Processing , Oct. 2005, pp. 201–208.\n[9] F . Morin and Y . Bengio, “Hierarchical probabilistic neu ral net-\nwork language model, ” in Proceedings of the 10th International\nW orkshop on Artiﬁcial Intelligence and Statistics , 2005, pp. 246–\n252.\n[10] T . Mikolov , S. Kombrink, L. Burget, J. ernock, and S. Khu dan-\npur, “Extensions of recurrent neural network language mode l, ” in\nIEEE International Conference on Acoustics, Speech and Sig nal\nProcessing (ICASSP) , May 2011, pp. 5528–5531.\n[11] Y . Bengio and J.-S. S ´ en´ ecal, “Quick training of proba bilistic neu-\nral nets by importance sampling, ” in Proceedings of the confer-\nence on Artiﬁcial Intelligence and Statistics (AISTATS) , 2003.\n[12] ——, “ Adaptive importance sampling to accelerate train ing of a\nneural probabilistic language model, ” IEEE Trans. Neural Net-\nworks, vol. 19, no. 4, pp. 713–722, 2008.\n[13] A. Mnih and Y . W . T eh, “ A fast and simple algorithm for tra ining\nneural probabilistic language models, ” in Proceedings of the 29th\nInternational Conference on Machine Learning , 2012, pp. 1751–\n1758.\n[14] M. U. Gutmann and A. Hyv¨ arinen, “Noise-contrastive es timation\nof unnormalized statistical models, with applications to n atural\nimage statistics, ” Journal of machine learning research , vol. 13,\npp. 307–361, Feb. 2012.\n[15] M. Mahoney , “Large text compression benchmark, ” 2011.\n[Online]. A vailable: http://mattmahoney .net/dc/textdata.html\n[16] B. Zoph, A. V aswani, J. May , and K. Knight, “Simple, fast noise-\ncontrastive estimation for large RNN vocabularies, ” in NAACL\nHLT 2016, The 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Huma n\nLanguage T echnologies, San Diego California, USA, June 12- 17,\n2016, 2016, pp. 1217–1222.\n[17] X. Chen, X. Liu, M. J. F . Gales, and P . C. W oodland, “Recur rent\nneural network language model training with noise contrast ive es-\ntimation for speech recognition, ” in IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP) , 2015,\npp. 5411–5415.\n[18] R. J ´ ozefowicz, O. V inyals, M. Schuster, N. Shazeer, an d\nY . Wu, “Exploring the limits of language modeling, ” CoRR, vol.\nabs/1602.02410, 2016.\n[19] Y . Oualil, C. Greenberg, M. Singh, and D. Klakow , “Seque ntial\nrecurrent neural networks for language modeling, ” in 17th Annual\nConference of the International Speech Communication Asso cia-\ntion (INTERSPEECH) , San Francisco, CA, USA, Sep. 8-12 2016,\npp. 3509–3513.\n[20] Y . Oualil, M. Singh, C. Greenberg, and D. Klakow , “Long- short\nrange context neural networks for language modeling, ” in Pro-\nceedings of the 2016 Conference on Empirical Methods in Nat-\nural Language Processing , Austin, T exas, USA, Nov . 2016, pp.\n1473–1481.\n[21] M. Sundermeyer, R. Schl ¨ uter, and H. Ney , “LSTM neural n et-\nworks for language modeling, ” in 13th Annual Conference of\nthe International Speech Communication Association (INTE R-\nSPEECH), Portland, OR, USA, Sep. 2012, pp. 194–197.\n[22] K. Simonyan and A. Zisserman, “V ery deep convolutional\nnetworks for large-scale image recognition, ” CoRR, vol.\nabs/1409.1556, 2014.\n[23] H. Sak, A. W . Senior, and F . Beaufays, “Long short-term m em-\nory recurrent neural network architectures for large scale acoustic\nmodeling, ” in 15th Annual Conference of the International Speech\nCommunication Association (INTERSPEECH) , Sep. 14-18 2014,\npp. 338–342.",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.936267614364624
    },
    {
      "name": "Computer science",
      "score": 0.7862086296081543
    },
    {
      "name": "Vocabulary",
      "score": 0.6981281042098999
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5725314021110535
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.5495331883430481
    },
    {
      "name": "Noise reduction",
      "score": 0.4899086356163025
    },
    {
      "name": "Artificial neural network",
      "score": 0.47190171480178833
    },
    {
      "name": "Noise (video)",
      "score": 0.46310967206954956
    },
    {
      "name": "Speech recognition",
      "score": 0.4612720012664795
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44809088110923767
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32756179571151733
    },
    {
      "name": "Image (mathematics)",
      "score": 0.08608141541481018
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I91712215",
      "name": "Saarland University",
      "country": "DE"
    }
  ],
  "cited_by": 7
}