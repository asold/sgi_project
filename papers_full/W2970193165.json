{
  "title": "MultiFiT: Efficient Multi-lingual Language Model Fine-tuning",
  "url": "https://openalex.org/W2970193165",
  "year": 2019,
  "authors": [
    {
      "id": null,
      "name": "Julian Eisenschlos",
      "affiliations": [
        "NetApp (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2140581490",
      "name": "Sebastian Ruder",
      "affiliations": [
        "National University of Ireland"
      ]
    },
    {
      "id": "https://openalex.org/A2896346430",
      "name": "Piotr Czapla",
      "affiliations": [
        "West Africa Vocational Education"
      ]
    },
    {
      "id": "https://openalex.org/A5097920535",
      "name": "Marcin Kadras",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222789889",
      "name": "Sylvain Gugger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117853863",
      "name": "Jeremy Howard",
      "affiliations": [
        "University of San Francisco"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6603175991",
    "https://openalex.org/W2741609678",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2890560993",
    "https://openalex.org/W2963899393",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963326042",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2963357986",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2270364989",
    "https://openalex.org/W4288562618",
    "https://openalex.org/W2795247881",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W1809065701",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2883158411",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2150102617",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2897686202",
    "https://openalex.org/W2514567832",
    "https://openalex.org/W2912095972",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963721344",
    "https://openalex.org/W2803214681",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2952436057",
    "https://openalex.org/W2027570514"
  ],
  "abstract": "Julian Eisenschlos, Sebastian Ruder, Piotr Czapla, Marcin Kadras, Sylvain Gugger, Jeremy Howard. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 5702–5707,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n5702\nMultiFiT: Efﬁcient Multi-lingual Language Model Fine-tuning\nJulian Eisenschlos1†∗ Sebastian Ruder2,3‡∗ Piotr Czapla4∗\nMarcin Kardas4∗ Sylvain Gugger5 Jeremy Howard5,6\n1ASAPP, Inc. 2National University of Ireland 3Aylien Ltd., Dublin\n4n-waves, Wrocław 5fast.ai 6University of San Francisco\nAbstract\nPretrained language models are promising par-\nticularly for low-resource languages as they\nonly require unlabelled data. However, train-\ning existing models requires huge amounts of\ncompute, while pretrained cross-lingual mod-\nels often underperform on low-resource lan-\nguages. We propose Multi-lingual language\nmodel Fine-Tuning (MultiFiT) to enable prac-\ntitioners to train and ﬁne-tune language mod-\nels efﬁciently in their own language. In ad-\ndition, we propose a zero-shot method us-\ning an existing pretrained cross-lingual model.\nWe evaluate our methods on two widely used\ncross-lingual classiﬁcation datasets where they\noutperform models pretrained on orders of\nmagnitude more data and compute. We release\nall models and code1.\n1 Introduction\nPretrained language models (LMs) have shown\nstriking improvements on a range of natural lan-\nguage processing (NLP) tasks (Peters et al., 2018a;\nHoward and Ruder, 2018; Devlin et al., 2018).\nThese models only require unlabelled data for\ntraining and are thus particularly useful in scenar-\nios where labelled data is scarce. As much of NLP\nresearch has focused on the English language, the\nlarger promise of these models is to bridge the dig-\nital language divide2 and enable the application of\nNLP methods to many of the world’s other 6,000\nlanguages where labelled data is less plentiful.\nRecently, cross-lingual extensions of these LMs\nhave been proposed that train on multiple lan-\nguages jointly (Artetxe and Schwenk, 2018; Lam-\nple and Conneau, 2019). These models are able\n1http://nlp.fast.ai\n⋆The ﬁrst four authors contributed equally.\n†Corresponding author: eisenjulian@gmail.com\n‡Sebastian is now afﬁliated with DeepMind.\n2http://labs.theguardian.com/\ndigital-language-divide/\nto perform zero-shot learning, only requiring la-\nbelled data in the source language. However,\nsource data in another language may often not be\navailable, whereas obtaining a small number of la-\nbels is typically straightforward.\nFurthermore such models have several down-\nsides: a) some variants rely on large amounts\nof parallel data, which may not be available\nfor truly low-resource languages; b) they require\na huge amount of compute for training 3; and\nc) cross-lingual models underperform on low-\nresource languages—precisely the setting where\nthey would be most useful. We are aware of\ntwo possible reasons for this: 1) Languages that\nare less frequently seen during training are un-\nderrepresented in the embedding space. 4 2) In-\nfrequent scripts are over-segmented in the shared\nword piece vocabulary (Wang et al., 2019).\nIn this work, we show that small monolin-\ngual LMs are able to outperform expensive cross-\nlingual models both in the zero-shot and the super-\nvised setting. We propose Multi-lingual language\nmodel Fine-tuning (MultiFit) to enable practition-\ners to train and ﬁne-tune language models efﬁ-\nciently.5 Our model combines universal language\nmodel ﬁne-tuning (ULMFiT; Howard and Ruder,\n2018) with the quasi-recurrent neural network\n(QRNN; Bradbury et al., 2017) and subword to-\nkenization (Kudo, 2018) and can be pretrained on\na single Tesla V100 GPU in a few hours. In addi-\ntion, we propose to use a pretrained cross-lingual\nmodel’s predictions as pseudo labels to adapt the\n3The training cost is amortized over time as pretraining\nonly needs to be performed once and ﬁne-tuning is much\ncheaper. However, if a model needs to be applied to a new\nlanguage or a domain not covered by the model, a new model\nneeds to be trained from scratch.\n4This is similar to how word embeddings are known to\nunderperform on low-frequency tokens (Gong et al., 2018).\n5We use ‘multilingual’ as referring to training indepen-\ndent models in multiple languages. We use ‘cross-lingual‘ to\nrefer to training a joint model across multiple languages.\n5703\nmonolingual language model to the zero-shot set-\nting. We evaluate our models on two widely\nused cross-lingual classiﬁcation datasets, MLDoc\n(Schwenk and Li, 2018) and CLS (Prettenhofer\nand Stein, 2010) where we outperform the state-\nof-the-art zero-shot model LASER (Artetxe and\nSchwenk, 2018) and multi-lingual BERT (Devlin\net al., 2018) in the supervised setting—even with-\nout any pretraining. In the zero-shot setting, we\noutperform both models using pseudo labels—and\nreport signiﬁcantly higher performance with as lit-\ntle as 100 examples. We ﬁnally show that infor-\nmation from monolingual and cross-lingual lan-\nguage models is complementary and that pretrain-\ning makes models robust to noise.\n2 Related work\nPretrained language models Pretrained lan-\nguage models based on an LSTM (Peters et al.,\n2018a; Howard and Ruder, 2018) and a Trans-\nformer (Radford et al., 2018; Devlin et al., 2018)\nhave been proposed. Recent work (Peters et al.,\n2018b) suggests that—all else being equal—an\nLSTM outperforms the Transformer in terms of\ndownstream performance. For this reason, we use\na variant of the LSTM as our language model.\nCross-lingual pretrained language models\nThe multi-lingual BERT model is pretrained on\nthe Wikipedias of 104 languages using a shared\nword piece vocabulary. LASER (Artetxe and\nSchwenk, 2018) is trained on parallel data of\n93 languages with a shared BPE vocabulary.\nXLM (Lample and Conneau, 2019) additionally\npretrains BERT with parallel data. These models\nenable zero-shot transfer, but achieve lower results\nthan monolingual models. In contrast, we focus\non making the training of monolingual language\nmodels more efﬁcient in a multi-lingual context.\nConcurrent work (Mulcaire et al., 2019) pretrains\non English and another language, but shows that\ncross-lingual pretraining only helps sometimes.\nMulti-lingual language modeling Training lan-\nguage models in non-English languages has only\nrecently received some attention. Kawakami et al.\n(2017) evaluate on seven languages. Cotterell\net al. (2018) study 21 languages. Gerz et al. (2018)\ncreate datasets for 50 languages. All of these stud-\nies, however, only create small datasets, which\nare inadequate for pretraining language models.\nIn contrast, we are among the ﬁrst to report the\nSUB1 400embs. 1550QRNN1\n1550QRNN2\n1550QRNN3\n400QRNN4\nSUB2 400embs. 1550QRNN1\n1550QRNN2\n1550QRNN3\n400QRNN4\n... ... ... ... ... ...\nSUBn 400embs. 1550QRNN1\n1550QRNN2\n1550QRNN3\n400QRNN4\n400avg pool\n400max pool50linear1\n#classeslinear2\n400last\nFigure 1: The MultiFiT language model with classiﬁer\nconsisting of a subword embedding layer, four QRNN\nlayers, an aggregation layer, and two linear layers.\nSource LanguageDocuments\nSource LanguageGold Labels\nTarget LanguageWikipedia\nLASER Classiﬁer\nPretrained LM\nFine-tuned LM\n Zero-shotClassiﬁer\nTarget LanguageDocuments\nTarget LanguagePredicted Labels\nTrainCross-lingualClassiﬁer\nPredictLabels\na) Pretrain b) Fine-tune c) Train Classiﬁer\nFigure 2: The steps of our cross-lingual bootstrapping\nmethod for zero-shot cross-lingual transfer. a) A mono-\nlingual language model (LM) is pretrained on target\nlanguage data; b) the LM is ﬁne-tuned on target lan-\nguage documents; and c) the LM is ﬁne-tuned as a\nclassiﬁer using the zero-shot predictions from a linear\nclassiﬁcation layer ﬁne-tuned on top of cross-lingual\nrepresentations from LASER.\nperformance of monolingual language models on\ndownstream tasks in multiple languages.\n3 Our method\n3.1 Multi-lingual Fine-Tuning\nWe propose Multi-lingual Fine-tuning (MultiFit).\nOur method uses the ULMFiT model (Howard\nand Ruder, 2018) with discriminative ﬁne-tuning\nas foundation. ULMFiT is based on a 3-layer\nAWD-LSTM (Mer, 2017) language model. The\nAWD-LSTM is a regular LSTM (Hochreiter and\nSchmidhuber, 1997) with tuned dropout hyper-\nparameters. To enable faster training and ﬁne-\ntuning of the model, we replace it with a QRNN\n(Bradbury et al., 2017). The QRNN alternates\nconvolutional layers, which are parallel across\ntimesteps, and a recurrent pooling function, which\nis parallel across channels. It has been shown to\noutperform LSTMs, while being up to 16×faster\nat train and test time. ULMFiT in addition is re-\nstricted to words as input. To make our model\n5704\nmore robust across languages, we use subword\ntokenization based on a unigram language model\n(Kudo, 2018), which is more ﬂexible compared to\nbyte-pair encoding (Sennrich et al., 2016). We ad-\nditionally employ label smoothing (Szegedy et al.,\n2016) and a novel cosine variant of the one-cycle\npolicy (Smith, 2018)6, which we found to outper-\nform ULMFiT’s slanted triangular learning rate\nschedule and gradual unfreezing. The full model\ncan be seen in Figure 1.\n3.2 Cross-lingual Bootstrapping\nPrior methods have employed cross-lingual train-\ning strategies relying on parallel data and a shared\nBPE vocabulary. These can be combined with\nour language model, but increase its training com-\nplexity. For the case where an existing pre-\ntrained cross-lingual model and source language\ndata are available, we propose a bootstrapping\nmethod (Ruder and Plank, 2018) that uses the pre-\ntrained model’s zero-shot predictions as pseudo la-\nbels to ﬁne-tune the monolingual model on target\nlanguage data.\nThe steps of the method can be seen in Figure\n2. Speciﬁcally, we ﬁrst ﬁne-tune a linear classiﬁ-\ncation layer on top of pretrained cross-lingual rep-\nresentations on source language training data. We\nthen apply this cross-lingual classiﬁer to the target\nlanguage data and store its predicted label for ev-\nery example. We now ﬁne-tune our pretrained LM\non the target language data and these pseudo la-\nbels7. Importantly, this method enables our mono-\nlingual LM to signiﬁcantly outperform its cross-\nlingual teacher in the zero-shot setting (§5).\n4 Experimental setup\nThis section provides an overview of our experi-\nmental setup; see the appendix for full details.\nData We evaluate our models on the Multilin-\ngual Document Classiﬁcation Corpus (MLDoc;\nSchwenk and Li, 2018)8—a new subset of Reuters\nCorpus V olume 2 (Lewis et al., 2004) with bal-\nanced class priors for eight languages—and on\nthe Cross-Lingual Sentiment dataset (CLS; Pret-\ntenhofer and Stein, 2010) 9 consisting of Amazon\n6The idea is due to private conversation with the author.\n7Distillation (Hinton et al., 2015) yielded similar results.\n8https://github.com/facebookresearch/\nMLDoc\n9https://webis.de/data/webis-cls-10.\nhtml\nDomain Languages Train Dev Test\nMLDoc News EN, DE, ES, FR, 1k / 2k / 2k 10kIT, JA, RU, ZH 5k / 10k\nCLS Product EN, DE, FR, JA 2k - 2kreviews\nTable 1: The domain, languages, and number of train-\ning, development, and test examples in each dataset.\nDE ES FR IT JA RU ZH\nZero-shot (1,000 source language examples)\nMultiCCA 81.20 72.50 72.38 69.38 67.63 60.80 74.73LASER, paper 86.2579.3078.30 70.20 60.95 67.25 70.98LASER, code 87.65 75.48 84.00 71.18 64.58 66.58 76.65MultiBERT 82.35 74.98 83.03 68.27 64.5871.5866.17MultiFiT, pseudo91.6279.1089.42 76.02 69.5767.8382.48\nSupervised (100 target language examples)\nMultiFit 90.90 89.00 85.03 80.12 80.55 73.55 88.02\nSupervised (1,000 target language examples)\nMultiCCA 93.70 94.45 92.05 85.55 85.35 85.65 87.30LASER, paper 92.70 88.75 90.80 85.93 85.15 84.65 88.98MultiBERT 94.00 95.15 93.20 85.82 87.48 86.85 90.72Monolingual BERT 94.93 - - - - - 92.17MultiFiT, no wiki 95.23 95.07 94.65 89.30 88.63 87.52 90.03MultiFiT 95.90 96.07 94.75 90.25 90.03 87.65 92.52\nTable 2: Comparison of zero-shot and supervised meth-\nods on MLDoc.\nproduct reviews in four languages. We provide an\noverview of the datasets in Table 1.\nPretraining We pretrain our models on 100M\ntokens extracted from the Wikipedia of the corre-\nsponding language for 10 epochs. As fewer to-\nkens might be available for some languages, we\nalso compare against a version (no wiki) that uses\nno pretraining. For all models, we ﬁne-tune the\nLMs on the target data of the same language for\n20 epochs. We perform subword tokenization with\nthe unigram language model (Kudo, 2018).\nEvaluation settings We compare two settings\nbased on the availability of source and target lan-\nguage data: supervised and zero-shot. In the su-\npervised setting, every model is ﬁne-tuned and\nevaluated on examples from the target language.\nIn the zero-shot setting, every model is ﬁne-tuned\non source language examples and evaluated on tar-\nget language examples. In all cases, we use En-\nglish as the source language.\nBaselines We compare against the state-of-\nthe-art cross-lingual embedding models LASER\n(Artetxe and Schwenk, 2018), which uses a\nlarge parallel corpus, multilingual BERT (MultiB-\n5705\nDE FR JA\nBooks DVD Music Books DVD Music Books DVD Music\nZero-shot\nLASER, code 84.15 78.00 79.15 83.90 83.40 80.75 74.99 74.55 76.30\nMultiBERT 72.15 70.05 73.80 75.50 74.70 76.05 65.41 64.90 70.33\nMultiFiT, pseudo 89.60 81.80 84.40 87.84 83.50 85.60 80.45 77.65 81.50\nTranslat.\nMT-BOW 79.68 77.92 77.22 80.76 78.83 75.78 70.22 71.30 72.02\nCL-SCL 79.50 76.92 77.79 78.49 78.80 77.92 73.09 71.07 75.11\nBiDRL 84.14 84.05 84.67 84.39 83.60 82.52 73.15 76.78 78.77\nSuper.\nMultiBERT 86.05 84.90 82.00 86.15 86.90 86.65 80.87 82.83 79.95\nMultiFiT 93.19 90.54 93.00 91.25 89.55 93.40 86.29 85.75 86.59\nTable 3: Comparison of zero-shot, translation-based and supervised methods (with 2k training examples) on all\ndomains of CLS. MT-BOW and CL-SCL results are from (Zhou et al., 2016).\nLSTM QRNN\nLanguage model pretraining 143 71\nClassiﬁer ﬁne-tuning 467 156\nTable 4: Comparison of LSTM and QRNN per-batch\ntraining speed on a Tesla V100 (in ms) in MultiFiT.\nERT)10, and monolingual BERT11. We also com-\npare against the best models on each dataset, Mul-\ntiCCA (Ammar et al., 2016), a cross-lingual word\nembedding model, and BiDRL (Zhou et al., 2016),\nwhich translates source and target data.\nOur methods We evaluate our monolingual\nLMs in the supervised setting (MultiFit) and our\nLMs ﬁne-tuned with pseudo labels from LASER\nin the zero-shot setting (pseudo).\nDE ES ZH\nULMFiT 94.19 95.23 66.82\nMultiFiT, no wiki 95.23 95.07 90.03\nMultiFiT, small Wiki 95.37 95.30 89.80\nMultiFiT 95.90 96.07 92.52\nTable 5: Comparison of MultiFiT results with different\npretraining corpora and ULMFiT, ﬁne-tuned with 1k\nlabels on MLDoc.\n5 Results\nMLDoc We show results for MLDoc in Table\n2. In the zero-shot setting, MultiBERT underper-\n10https://github.com/google-research/\nbert/blob/master/multilingual.md\n11Models are available for English, Chinese, and German\n(https://deepset.ai/german-bert).\nforms the comparison methods as the shared em-\nbedding space between many languages is overly\nrestrictive. Our monolingual LMs outperform\ntheir cross-lingual teacher LASER in almost ev-\nery setting. When ﬁne-tuned with only 100 target\nlanguage examples, they are able to outperform\nall zero-shot approaches except MultiFiT on DE\nand FR. This calls into question the need for zero-\nshot approaches, as ﬁne-tuning with even a small\nnumber of target examples is able to yield superior\nperformance. When ﬁne-tuning with 1,000 target\nexamples, MultiFiT—even without pretraining—\noutperforms all comparison methods, including\nmonolingual BERT.\nCLS We show results for CLS in Table 3. Mul-\ntiFiT is able to outperform its zero-shot teacher\nLASER across all domains. Importantly, the\nbootstrapped monolingual model also outperforms\nmore sophisticated models that are trained on\ntranslations across almost all domains. In the su-\npervised setting, MultiFiT similarly outperforms\nmultilingual BERT. For both datasets, our methods\nthat have been pretrained on 100 million tokens\noutperform both multilingual BERT and LASER,\nmodels that have been trained with orders of mag-\nnitude more data and compute.\n6 Analysis\nSpeed We compare the LSTM and QRNN cell\nin MultiFiT based on the speed for processing a\nsingle batch for pretraining and ﬁne-tuning in Ta-\nble 4. MultiFiT with a QRNN pretrains and ﬁne-\ntunes about 2×and 3×faster respectively.\nMultiFiT vs. ULMFiT We compare Multi-\nFiT pretrained on 100M Wikipedia tokens against\n5706\nULMFiT pretrained on the same data using a 3-\nlayer LSTM and spaCy tokenization 12 as well as\nMultiFiT pretrained on 2M Wikipedia tokens, and\nMultiFiT with no pretraining in Table 5. Pretrain-\ning on more data generally helps. MultiFiT out-\nperforms ULMFiT signiﬁcantly; the performance\nimprovement is particularly pronounced in Chi-\nnese where ULMFiT’s word-based tokenization\nunderperformed.\nFigure 3: Comparison of MultiFiT’s robustness to label\nnoise on MLDoc with and without pretraining.\nDE ES FR IT JA RU ZH\nLASER, code 87.65 75.48 84.00 71.18 64.58 66.58 76.65\nRandom init. (1k) 77.80 70.50 75.65 68.52 68.50 61.37 79.19Random init. (10k) 90.53 69.75 87.40 72.72 67.55 63.67 81.44MultiFiT, pseudo (1k)91.34 78.92 89.45 76.00 69.57 68.19 82.45\nTable 6: Bootstrapping results on MLDoc with and\nwithout pretraining, trained on 1k/10k LASER labels.\nDE ES FR IT RU\nWord-based 95.28 95.97 94.72 89.97 88.02\nSubword 96.10 96.07 94.75 94.75 87.65\nTable 7: Comparison of different tokenization strate-\ngies for different languages on MLDoc.\nRobustness to noise We suspect that MultiFiT\nis able to outperform its teacher as the information\nfrom pretraining makes it robust to label noise. To\ntest this hypothesis, we train MultiFiT and a ran-\ndomly initialized model with the same architec-\nture on 1k and 10k examples of the Spanish ML-\nDoc. We randomly perturb labels with a probabil-\nity ranging from 0-0.75 and show results in Fig-\nure 3. The pretrained MultiFiT is able to partially\n12https://spacy.io/api/tokenizer\nignore the noise, up to 65% of noisy training ex-\namples. Without pretraining, the model does not\nexceed the theoretical baseline (the percentage of\ncorrect examples). In addition, we compare Mul-\ntiFiT with and without pretraining in Table 6. Pre-\ntraining enables MultiFiT to achieve much better\nperformance compared to a randomly initialised\nmodel. Both results together suggest a) that pre-\ntraining increases robustness to noise and b) that\ninformation from monolingual and cross-lingual\nlanguage models is complementary.\nTokenization Subword tokenization has been\nfound useful for language modeling with mor-\nphologically rich languages (Czapla et al., 2018;\nMielke and Eisner, 2019) and has been used in\nrecent pretrained LMs (Devlin et al., 2018), but\nits concrete impact on downstream performance\nhas not been observed. We train models with\nthe best performing vocabulary sizes for subword\n(15k) and regular word-based tokenization (60k)\nwith the Moses tokenizer (Koehn et al., 2007) on\nGerman and Russian MLDoc and show results in\nTable 7. Subword tokenization outperforms word-\nbased tokenization on most languages, while being\nfaster to train due to the smaller vocabulary size.\n7 Conclusion\nWe have proposed novel methods for multilingual\nﬁne-tuning of languages that outperform mod-\nels trained with far more data and compute on\ntwo widely studied cross-lingual text classiﬁcation\ndatasets on eight languages in both the zero-shot\nand supervised setting.\nReferences\n2017. Pointer Sentinel Mixture Models. In Proceed-\nings of ICLR 2017.\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov,\nGuillaume Lample, Chris Dyer, and Noah A. Smith.\n2016. Massively Multilingual Word Embeddings.\nMikel Artetxe and Holger Schwenk. 2018. Massively\nMultilingual Sentence Embeddings for Zero-Shot\nCross-Lingual Transfer and Beyond. arXiv preprint\narXiv:1812.10464.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017. Quasi-Recurrent Neural Net-\nworks. In Proceedings of ICLR 2017.\nRyan Cotterell, Sebastian Mielke, Jason Eisner, and\nBrian Roark. 2018. Are all languages equally hard\nto language-model? In Proceedings of NAACL-HLT\n2018, pages 536–541.\n5707\nPiotr Czapla, Jeremy Howard, and Marcin Kardas.\n2018. Universal language model ﬁne-tuning with\nsubword tokenization for polish. In Proceedings of\nPolEval 2018 Workshop.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding.\nDaniela Gerz, Ivan Vuli´c, Edoardo Ponti, Jason Narad-\nowsky, Roi Reichart, and Anna Korhonen. 2018.\nLanguage Modeling for Morphologically Rich Lan-\nguages: Character-Aware Modeling for Word-Level\nPrediction. Proceedings of TACL 2018.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei\nWang, and Tie-Yan Liu. 2018. FRAGE: Frequency-\nAgnostic Word Representation. In Proceedings of\nNIPS 2018.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the Knowledge in a Neural Network.\narXiv preprint arXiv:1503.02531.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong Short-Term Memory. Neural Computation,\n9(8):1735–1780.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classiﬁcation.\nIn Proceedings of ACL 2018.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom.\n2017. Learning to Create and Reuse Words in Open-\nV ocabulary Neural Language Modeling. In ACL\n2017.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, et al. 2007. Moses: Open source\ntoolkit for statistical machine translation. In Pro-\nceedings of ACL 2007, pages 177–180.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of ACL 2018,\npages 66–75.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual Language Model Pretraining. arXiv preprint\narXiv:1901.07291.\nDavid D Lewis, Yiming Yang, Tony G Rose, and Fan\nLi. 2004. Rcv1: A new benchmark collection for\ntext categorization research. Journal of machine\nlearning research, 5(Apr):361–397.\nSebastian J. Mielke and Jason Eisner. 2019. Spell\nOnce, Summon Anywhere: A Two-Level Open-\nV ocabulary Language Model. In Proceedings of\nAAAI 2019.\nPhoebe Mulcaire, Jungo Kasai, and Noah Smith.\n2019. Polyglot Contextual Representations Improve\nCrosslingual Transfer. In Proceedings of NAACL\n2019.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT 2018.\nMatthew E. Peters, Mark Neumann, Luke Zettlemoyer,\nWen-tau Yih, Paul G Allen, and Computer Science.\n2018b. Dissecting Contextual Word Embeddings:\nArchitecture and Representation. In Proceedings of\nEMNLP 2018.\nPeter Prettenhofer and Benno Stein. 2010. Cross-\nLanguage Text Classiﬁcation using Structural Cor-\nrespondence Learning. In ACL ’10 Proceedings of\nthe 48th Annual Meeting of the Association for Com-\nputational Linguistics (ACL ’10 ), pages 1118–1127.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving Language Under-\nstanding by Generative Pre-Training.\nSebastian Ruder and Barbara Plank. 2018. Strong\nBaselines for Neural Semi-supervised Learning un-\nder Domain Shift. In Proceedings of ACL 2018.\nHolger Schwenk and Xian Li. 2018. A Corpus for\nMultilingual Document Classiﬁcation in Eight Lan-\nguages. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018).\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of ACL 2016.\nLeslie N Smith. 2018. A disciplined approach to neu-\nral network hyper-parameters: Part 1–learning rate,\nbatch size, momentum, and weight decay. arXiv\npreprint arXiv:1803.09820.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. 2016. Re-\nthinking the Inception Architecture for Computer\nVision. In Proceedings of CVPR 2016.\nXinyi Wang, Hieu Pham, Philip Arthur, and Graham\nNeubig. 2019. Multilingual neural machine transla-\ntion with soft decoupled encoding. In Proceedings\nof ICLR 2019.\nXinjie Zhou, Xianjun Wan, and Jianguo Xiao. 2016.\nCross-Lingual Sentiment Classiﬁcation with Bilin-\ngual Document Representation Learning. Proceed-\nings of ACL 2016, pages 1403–1412.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7224545478820801
    },
    {
      "name": "Programming language",
      "score": 0.4903585910797119
    },
    {
      "name": "Natural language processing",
      "score": 0.480163037776947
    },
    {
      "name": "Natural language",
      "score": 0.46643948554992676
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4631940722465515
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3631238341331482
    },
    {
      "name": "Linguistics",
      "score": 0.33188289403915405
    },
    {
      "name": "History",
      "score": 0.10260239243507385
    },
    {
      "name": "Philosophy",
      "score": 0.08393323421478271
    },
    {
      "name": "Archaeology",
      "score": 0.06573447585105896
    }
  ]
}