{
    "title": "Prompted LLMs as Chatbot Modules for Long Open-domain Conversation",
    "url": "https://openalex.org/W4385572383",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2310924550",
            "name": "Gibbeum Lee",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A2083626571",
            "name": "Volker Hartmann",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A2096107519",
            "name": "Jong-Ho Park",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A5018630988",
            "name": "Dimitris Papailiopoulos",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A2144546677",
            "name": "Kangwook Lee",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4385572830",
        "https://openalex.org/W4319997768",
        "https://openalex.org/W3175218683",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4293320163",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W4313304293",
        "https://openalex.org/W4389519535",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W2963825865",
        "https://openalex.org/W4385572956",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4385572867",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4303441863",
        "https://openalex.org/W3093233911",
        "https://openalex.org/W4226399820",
        "https://openalex.org/W3202390784",
        "https://openalex.org/W4221143046"
    ],
    "abstract": "In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for creating high-quality conversational agents without the need for fine-tuning. Our method utilizes pre-trained large language models (LLMs) as individual modules for long-term consistency and flexibility, by using techniques such as few-shot prompting, chain-of-thought (CoT), and external memory. Our human evaluation results show that MPC is on par with fine-tuned chatbot models in open-domain conversations, making it an effective solution for creating consistent and engaging chatbots.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 4536–4554\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nPrompted LLMs as Chatbot Modules\nfor Long Open-domain Conversation\nGibbeum Lee 1* Volker Hartmann 1* Jongho Park1*\nDimitris Papailiopoulos 1,2 Kangwook Lee 1,2\n1 KRAFTON 2 University of Wisconsin-Madison\n{pirensisco, volker, jongho.park}@krafton.com\nAbstract\nIn this paper, we propose MPC (Modular\nPrompted Chatbot), a new approach for creat-\ning high-quality conversational agents without\nthe need for fine-tuning. Our method utilizes\npre-trained large language models (LLMs) as\nindividual modules for long-term consistency\nand flexibility, by using techniques such as few-\nshot prompting, chain-of-thought (CoT), and\nexternal memory. Our human evaluation re-\nsults show that MPC is on par with fine-tuned\nchatbot models in open-domain conversations,\nmaking it an effective solution for creating con-\nsistent and engaging chatbots. 1\n1 Introduction\nLanguage models with billions of parameters, such\nas GPT-3 (Brown et al., 2020) and PaLM (Chowdh-\nery et al., 2022), have achieved state-of-the-art per-\nformance on many NLP tasks. To fine-tune these\nlarge language models (LLMs) for open-domain\nchatbot tasks, one could use a dataset of conver-\nsational data that is representative of the target\ndomain. However, fine-tuning LLMs for open-\ndomain chatbots can be challenging due to the\ncomputational burden of updating models with bil-\nlions of parameters and the scarcity of data in the\ndialogue domain. Furthermore, fine-tuning can\nlimit the model’s versatility by restricting it to a\nspecific domain, and result in the loss of domain-\nagnostic knowledge acquired during pre-training,\nas reported by Yang and Ma (2022). Multi-task\ntraining on different datasets, as proposed by Roller\net al. (2021), can address the versatility issue but\nhas limitations, such as the need for data to train\neach skill and the difficulty determining the nec-\nessary skills for an open-domain chatbot. In fact,\nthe growing number of modules for chatbots, as in\nBlenderbot3 (BB3) (Shuster et al., 2022), points\n*Equal contributions; randomized order\n1Code is available in https://github.com/krafton-ai/MPC.\ntowards the increasing burden of data and computa-\ntion when fine-tuning for each new chatbot model.\nInterestingly, some LLMs have the ability to per-\nform in-context learning (ICL) (Nye et al., 2022;\nWei et al., 2022b; Lewkowycz et al., 2022; Wei\net al., 2022a; Zhou et al., 2022; Dasgupta et al.,\n2022; Chung et al., 2022). This capability enables\nthe model to rapidly adapt to and execute a specific\ntask based on a brief instruction and a few exam-\nples, without requiring additional fine-tuning. This\ncan be utilized to create an open-domain chatbot,\nwhere a prompt describing a task required for open-\ndomain dialogue and a few examples of solving\nsuch task can be provided to the LLM, allowing\nit to generate information that is pertinent to the\ncurrent conversation.\nOur Contributions We present a novel approach\nfor creating high-quality conversational agents\nwithout the need for fine-tuning. Our proposed\nchatbot, MPC (Modular Prompted Chatbot), uti-\nlizes open-sourced pre-trained language models\nto increase the flexibility of designing the mod-\nules of an open-domain chatbot. Our approach en-\nhances multiple conversational capabilities by uti-\nlizing a modularized agent that incorporates LLMs\nwith prompt techniques such as few-shot ICL and\nChain-of-Thought (CoT). In the paper, we design\nMPC to achieve long-term consistency, a domain\nin which previous chatbots have struggled. Our\nhuman evaluation results show that MPC is on par\nwith or even preferred over fine-tuned LLMs, such\nas Blenderbot, in an open-domain conversational\nsetting. This approach highlights the potential of\npre-trained LLMs to adapt to new tasks without\nfine-tuning, providing an efficient solution for cre-\nating open-domain conversational agents.\n2 Related Work\nModular Prompting Well-crafted elicitive\nprompts can enhance reasoning abilities, resulting\n4536\nin improved performance across various bench-\nmarks (Kojima et al., 2022; Wei et al., 2022b;\nSuzgun et al., 2022). For complex problems, Press\net al. (2022) identified the compositionality gap\nwhich arises when an LM can solve sub-problems\nbut not the overall solution and further showed that\nCoT narrows this gap. Since then, there has been a\nflurry of work that solves tasks by decomposing\nthem into smaller tasks solved by different “prompt\nmodules” (Zhou et al., 2022; Wang et al., 2022;\nKhot et al., 2022; Khattab et al., 2022).\nModular prompting has found use beyond bench-\nmarks and in conversation generation. Kim et al.\n(2022) used an LLM to generate a socially di-\nverse dialogue dataset that is more natural and de-\ntailed than existing crowdsourced datasets. More-\nover, hierarchical prompt modules prove to help\nlong-range coherence for generating narratives and\nplays (Yang et al., 2022; Mirowski et al., 2022). We\nrefer to Mialon et al. (2023) for a detailed overview\non such augmented uses of LLMs.\nOpen-domain Chatbots Many recent dialogue\nagents rely on dialogue-finetuned LLMs. In Thop-\npilan et al. (2022), LaMDa has been trained on\nlarge amounts of crawled conversational data and\nhas used a fine-tuned classifier for model safety.\nMore recently, similar to our modularization ap-\nproach, BB3 fine-tunes Open Pre-trained Trans-\nformers (OPT) (Zhang et al., 2022; Shuster et al.,\n2022) on QA and dialogue datasets and uses one\nshared model weight as multiple modules.\nOn the other hand, Madotto et al. (2021) elim-\ninate the need for fine-tuning on dialogue data by\nfeeding retrieved dialogue samples as few-shot for\nGPT-J (Wang and Komatsuzaki, 2021). We find\nthis work to be complementary to our work, as the\nfew-shot dialogue can be seen as an approach to\nenhance the utterance generator module.\nLong-term Memory The Multi-Session Chat\ndataset (Xu et al., 2022) allows for measuring how\nwell conversational agents maintain a long-term\nmemory of facts about the user and bot. Infor-\nmation is retrieved using Dense Passage Retriever\n(DPR) (Karpukhin et al., 2020), while BART com-\npresses memories before storing them. In Shuster\net al. (2022), a modular approach is used to incor-\nporate long-term memory and factual grounding\nthrough internet search with an LLM. This work\nis closest to our work since it includes an ablation\nstudy in which prompt-based modules are com-\npared with fine-tuned modules. However, in our\nwork, we argue that more reasoning-based prompt-\ning, as demonstrated in Wei et al. (2022b), is bene-\nficial for better contextual understanding.\n3 Modular Prompted Chatbot\nWe present a modular chatbot system (Fig. 1) that\nuses prompt-based LLMs to maintain persona and\nengagement throughout long-term conversations.\nI’m 27 years old. How old are you?\nHow old is Sarah?\n-Sarah is 25.-Sarah is a student.\nClarifier(few-shot)Rephrase User’s question in third-person.User:  I’m 27 years old. How old are you?Specifically, \nMemory Processor(few-shot)(CoT)\nThis is the list of Sarah’s knowledge.(1)Sarah is 25. (2) Sarah is a student. Q: How old is Sarah?A: Let’s think step by step. …Answer: Sarah thinks\nUtterance Generator(zero-shot)\nThis is the list of Sarah’s persona.Sarah thinks Sarah is 25 and a student.This is the conversation between Sarah and User. {Dialogue history}User: I’m 27 years old. How old are you?Sarah:\nSummarizer(few-shot){Dialogue History}Summary-\nDPR(bi-encoder)\nSarah is 25 and a student.\nI’m 25 and a student.\nMemory Pool\nUser is 27.\nInput\nOutput\nFigure 1: Our modular design for improving long-term\nconsistency in open-domain conversation.\nAt the start of a conversation, a pre-defined per-\nsona is stored in the memory pool. When a user\nsends a message, the clarifier rephrases it to re-\nsolve any ambiguities and passes it to the DPR\nmodel which retrieves relevant memories from the\nmemory pool. The retrieved memories and clarifier\noutput are fed into the memory processor to get\na single context-relevant memory, which is then\npassed to an utterance generator for producing a\nresponse from the chatbot. Every few turns, we call\nupon a summarizer module to extract important in-\nformation from dialogue and store it in the memory\npool for future use (see Appendices D and E).\nUtterance Clarifier As conversations are often\nmuddled with vague coreferences and contextual\n4537\ncues, our clarifier module is an LM prompted with\nthe recent dialogue to resolve any ambiguities. For\ninstance, depending on prior context, the user in-\nput “Do you like working there?” would output\n“Does Sarah like working at ZYX company?”. By\nresolving contextual ambiguity, the clarifier assists\nthe DPR model and memory processor module by\nproviding an information-dense query to fetch and\nprocess relevant memories.\nMemory Processor As demonstrated in Fig. 1,\nwe formulate memory processing as an LLM rea-\nsoning task of finding the most relevant information\ngiven the dialogue. Following the footsteps in solv-\ning hard reasoning tasks (Suzgun et al., 2022), we\nprovide CoT examples to show reasons for ignor-\ning certain memories and synthesizing others. For\nmodels incapable of CoT, we simply provide the\nfew-shot examples without the reasoning portion.\nSince the memory pool accumulates as the con-\nversation progresses, we use a pre-trained DPR\nwith the output of the clarifier as the query to re-\ntrieve the top-kmost relevant memories from the\nmemory pool. The memory processor then con-\ndenses the top memories into one refined memory.\nUtterance Generator The utterance generator\nmodule generates the final response of the chat-\nbot given the recent dialogue history and memory\nprovided by the memory processor. The prompt\nconsists of the dialogue history, condensed mem-\nory, and the generation instruction (e.g., “Give a\nfriendly response to the user.”). For some models,\nwe find that inserting the generation instruction at\nthe end was helpful as placing it before the dialogue\nminimizes the effect of the instruction.\nDialogue Summarizer We provide a few-shot\nprompt to ensure we record specific details of the\nconversation and the user.\n4 Experimental Setup\nWe evaluate our chatbot’s performance by assess-\ning core skills necessary for long-term conversa-\ntions. We assess consistency by assigning one of\nfive personas, each with 12 facts from PersonaChat\n(Zhang et al., 2018), and presenting these facts to\nevaluators. For each experiment, we collect 20\nturns from each evaluator and at least 500 turns\nin total from two subgroups: Amazon Mechanical\nTurk and university students. See Appendix C for\na detailed explanation of our data collection.\nIn our setup, there are four groups of models.\n1. Fine-tuned chatbot models such as BB3.\n2. Vanilla is an utterance generator that either\nprepends full persona or no persona to the dia-\nlogue history in the prompt. This represents the\nnaive approach of using an LM as a chatbot.\n3. MPC is as described in Section 3. Specifically,\nwe only form one memory from the memory pro-\ncessor. Full persona is not explicitly prepended.\n4. MPC+full persona is MPC that prepends the\nfull persona. See examples in Appendix E.\n4.1 Single Model Evaluation\nWe evaluate each model separately using Sensi-\nbleness, Consistency, and Engagingness metrics\nand collect a final rating (out of 5.0). The exact\nquestions and evaluation forms are in Appendix A.\nWe also report two types of combined score SCE\n(Sensible Consistent and Engaging): a \"perfect\"\nscore SCE-p, where all metrics must be positive for\na positive response, and the weighted score SCE-w,\nwhich is similar to SSI (Thoppilan et al., 2022)\nand reported in Appendix B.\nWe use OpenAI GPT-3 text-davinci-002 (td2),\ndavinci, OPT 30B, 66B, GPT-JT-6B (Together,\n2022), and BLOOM-176B (Scao et al., 2022) as\nbase LMs for MPC. For fine-tuned group, we\nuse BB3-30B with the same persona settings. For\nBB3-175B, we request crowdworkers to evaluate\nthe online demo for 20 turns. We also report the\nadditional results of the recent models such as GPT\n3.5 (gpt-3.5-turbo-0301) (OpenAI, 2022), GPT-4\n(OpenAI, 2023), and Alpaca (Taori et al., 2023) in\nAppendix B.\n4.2 Pairwise Models Evaluation\nWe A/B test two chatbot models by providing the\nuser with two randomized responses A and B. The\nuser then evaluates them based on Sensibleness,\nConsistency, Interestingness, and Preference. The\nconversation then continues with the response cho-\nsen for Preference. This lets us to control for dia-\nlogue history when comparing two models.\nSpecifically, we conduct two main experiments:\n(1) MPCOPT-30B vs. BB3-30B, where internet\nsearch for BB3 is disabled as we focus on consis-\ntency. Our evaluation enables a direct comparison,\nas BB3-30B is a fine-tuned version of OPT-30B.\n(2) MPCtd2 vs. Vanilla td2 (full persona).\nImplicit Persona In reality, we implicitly learn\nabout someone through dialogue. In contrast, our\n4538\nprevious experiments show explicit persona to both\nevaluators and models. As such, we devise an ex-\nperiment by providing a 10-turn pre-defined dia-\nlogue to the crowdworker and pairwise models,\nMPCtd2 and Vanilla td2 (no persona). We then ask\nworkers to ask about the previous dialogue for 6\nnew turns. Here, we set a shorter maximum context\nlength than the 10-turn dialogue, so that the setup\nrepresents long conversations where necessary in-\nformation is beyond the LM context length.\n4.3 Metrics\nIn our work, we present two modes of experiments:\nsingle and pairwise model evaluation. Our single\nmodel evaluation is similar to a hybrid of SM-Turn\nand SM-Dialogue evaluations and a pairwise model\nto PW-Turn from Smith et al. (2022). For each turn,\nwe ask crowdworkers to evaluate the quality of the\nchatbot response based on the following metrics.\nSingle Model Evaluation\n• Sensibleness Whether the response makes sense.\n“Does the response make sense?”\n• Consistency Whether the response does not con-\ntradict the contextual information or the persona.\n“Is the response consistent with the information\nbased on the persona list and context of the con-\nversation?”\n• Engagingness Whether the user is engaged and\nwould want to continue the conversation.\n“Are you engaged by the response? Do you want\nto continue the conversation?”\n• Final Rating\n“How was your chat? From a scale of 1 (very bad)\nto 5 (very good), rate the quality of the overall\nconversation.”\nPairwise Model Evaluation\n• Sensibleness Which response makes more sense.\n“Which response makes more sense?”\n• Consistency Which response is more true to and\nconsistent with the persona.\n“If you had to say one of these speakers is more\ntrue to and consistent with the listed persona and\none is not, who would you say is more consis-\ntent?”\n• Interestingness Which is more interesting.\n“If you had to say one of these responses is inter-\nesting and one is boring, which would you say is\nmore interesting?”\n• Preference Which is preferred for a long conver-\nsation.\n“Based on the current response, who would you\nprefer to talk to for a long conversation? Your\nconversation will continue with the selected re-\nsponse.”\n5 Results\nMPCOPT-30B Tie BB3-30B\nSensibleness 45.0 32.0 23.0\nConsistency 31.3 34.1 34.6\nInterestingness 40.9 21.0 38.1\nPreference 50.0 9.7 40.3\nTable 1: Pairwise evaluation of MPCOPT-30B vs. BB3-\n30B (Dark highlight: p <0.01, Light highlight: p <\n0.05; We run one-sample t-test dividing ties equally into\neach side and setting µ> 0.5.)\nPre-trained vs. Fine-tuned Our human evalu-\nations show that MPC, which uses a pre-trained\nLLM, is better than the fine-tuned BB3-30B.\nMost notably, with a 9% SCE-p gap,\nMPCOPT-30B scores higher on all metrics\nthan BB3-30B. In fact, the majority of our\nMPC models in Table. 2 demonstrates superior\nperformance to BB3-30B.\nFor BB3-30B, we have observed issues of con-\nsecutive utterance repetition. We report the eval-\nuation results of only including dialogues with-\nout repetition in Table 5. Even without repetition,\nMPCOPT-30B is still on par with BB3-30B. More-\noever, MPCOPT-30B in Table 1 shows higher sensi-\nbleness and preference than BB3-30B, while scor-\ning similarly in consistency and interestingness.\nModular vs. Non-modular MPC excels in con-\nsistent dialogue in comparison to the vanilla ap-\nproach. The implicit persona experiment in Ta-\nble 3 demonstrates that MPCtd2 scores signifi-\ncantly higher than a vanilla application of td2 in\nall metrics. In Table 2, for MPCtd2+Full persona,\nconsistency is on par with that of td2 (full persona),\nwhile engagingness, SCE-p, and rating are the best\noverall. Nevertheless, when we do not include full\npersona in the prompt for MPCtd2, it shows lower\nconsistency than td2 (full persona), albeit the high\n4539\nModel Sens. Cons. Eng. SCE-p Rating\nFine-tuned\nBB3-30B 71.3 77.8 73.7 54.3 2.9\nBB3-175B 85.9 (88.7) 84.8 73.1 3.8\nFull persona\ntd2 94.0 94.7 84.3 79.7 4.1\ndavinci 91.8 89.2 78.8 70.8 3.8\nMPC\ntd2 93.6 87.8 85.5 75.0 4.2\ndavinci 80.2 72.0 69.1 53.3 3.1\nOPT-66B 90.5 84.8 88.1 73.9 4.1\nOPT-30B 86.1 79.1 80.7 63.4 3.6\nGPT-JT 91.1 83.2 65.3 53.5 3.1\nBLOOM 65.2 65.5 61.4 40.5 2.8\nMPC+Full\ntd2 94.4 92.2 92.8 83.0 4.2\nOPT-30B 85.6 87.2 89.0 72.6 3.7\nTable 2: Single model evaluations of baselines and\nMPCs. The top-3 scores for each metric are highlighted,\nwhile the highest score is bolded. See Appendix B Ta-\nble 5 for more detailed results.\nMPCtd2 Tie td2 (no persona)\nSensibleness 40.6 46.1 13.3\nConsistency 57.2 28.9 13.9\nInterestingness 47.2 31.1 21.7\nPreference 67.2 10.6 22.2\nTable 3: Implicit persona experiment for MPCtd2 vs.\ntd2 (no persona). (Dark highlight: p< 0.01)\nrates of ties in sensibleness and consistency (Ta-\nble 4). In general, we find that users would ask\nprimarily about the bot’s persona rather than hav-\ning a two-sided conversation, leading to td2 (full\npersona) performing better in consistency.\nEffect of Size When other variables are held the\nsame, we observe model size is positively corre-\nlated with positive evaluations. The most com-\npelling evidence can be seen in the superiority of\nMPCOPT-66B across all metrics when compared to\nMPCOPT-30B since the two base LMs are trained\nnearly identically. Needless to say, model size is\nnot the only factor.MPCBLOOM, one of the largest\nmodels, scores the lowest in our experiments.\nEffect of Instruction-tuning Instruction-tuning\nhelps the creation of a modular dialogue system by\nenabling adaptation to various tasks. Not only\ndoes MPCtd2 perform better than MPCdavinci, but\nMPCtd2 Tie td2 (full persona)\nSensibleness 27.5 42.6 29.9\nConsistency 24.4 44.7 30.9\nInterestingness 40.7 26.2 33.1\nPreference 42.8 15.9 41.3\nTable 4: MPCtd2 vs. td2 (full persona). Though\nMPCtd2 only retrieves one memory, consistency is only\nlower by 6pt. (Light highlight: p< 0.05)\nalso MPCGPT-JT shows high sensibleness and con-\nsistency, despite its smaller size. Though not in-\ncluded in the main body due to limited sample size,\nwe additionally see that MPCAlpaca-7B achieves\nhigher SCE and rating than its full-persona coun-\nterpart in Table 5. In general, we posit that find-\ning good prompts for each module for instruction-\ntuned LMs is simpler and more robust to variations.\nMPCdavinci is worse than davinci (full persona),\npresumably due to error propagation in the modu-\nlar system, though we do not rule out that there are\nbetter prompts for MPCdavinci.\n6 Conclusion\nWe demonstrated that a modular approach using\nLLMs, namely MPC, can be an effective solution\nfor long-term open-domain chatbots without fur-\nther finetuning. We compared MPC to fine-tuned\nand vanilla LM baselines and found that our ap-\nproach achieved superior performance by human\nevaluation. Additionally, our modular system in-\ncorporated persona and information from dialogue\nhistory more effectively than the non-modular ones\naccording to our consistency evaluation.\nLimitations\nIn this work, we investigate the use of pre-trained\nlanguage models for long-term English conversa-\ntions. While we expect a modular approach may be\neffective for other languages when given a capable\nlanguage model, it should also be noted that fur-\nther research is needed to confirm the applicability\nof our findings to other languages. For instance,\nthough BLOOM is trained as a multilingual lan-\nguage model, we only implement MPCBLOOM in\nEnglish and evaluate its English capability as a\nopen-domain dialogue agent.\nMeanwhile, a modular system can create addi-\ntional inference overhead or error accumulation.\nThe system performance would become much bet-\n4540\nter if we optimally choose the LM for each module.\nFor example, we could use GPT-3 td2 for the mem-\nory processor, while we employ OPT-175B for the\nutterance generator. We would need to evaluate ev-\nery module to find the best model for each, which\nwe leave to future work.\nIn terms of evaluation methodology, our human\nevaluations of MPC and its analysis face the same\nchallenges as previous studies on evaluating inter-\nactive conversational tasks. As demonstrated by\nSmith et al. (2022), there is currently no definitive\nevaluation method for determining the best chatbot\nmodel. Additionally, there are several factors that\nmust be taken into account during data collection\nand interpretation, such as annotator subjectivity,\ninstruction bias, and crowdworker working condi-\ntions. For a more in-depth discussion of human-LM\ninteraction, we refer the reader to Lee et al. (2022).\nAs described in Appendix C, to gather a diverse\nrange of evaluations, we have collected qualitative\ndata from two groups: English-speaking annota-\ntors on Amazon Mechanical Turk (MTurk), and\nqualified university students who were capable of\nspeaking English. To some extent, this evaluation\nsetup reduces cultural bias and platform homogene-\nity compared to using MTurk workers alone. How-\never, the limitations of this approach should be\nacknowledged and this may further complicate the\nanalysis when controlling for MPC’s performance\non different subgroups.\nLastly, we note that running MPC requires at\nleast as much memory as its underlying language\nmodel, making MPC infeasible to even load on\na single node for heavy models such as BLOOM-\n176B.\nEthics Statement\nMPC utilizes publicly available pre-trained LMs\nfor chatbot utterance generation. Language gener-\nation from these LMs is known to have concerns\nabout toxicity and bias (Xu et al., 2020). Thus,\nensuring safe deployment and interaction is a ne-\ncessity.\nAccordingly, we outline our data collection pro-\ncedure in Appendix C. We allow crowdworkers to\ndirectly provide us with feedback and also manu-\nally check for any offensive or controversial out-\nputs. To ensure the protection of personal informa-\ntion, all crowdworkers were instructed not to share\nany personally identifiable or private information.\nAdditionally, they were asked to give their consent\nfor the collection of anonymous information for\nresearch purposes. Prior to participating, all work-\ners were informed of the purpose of data collection\nand, after evaluation, were compensated with a\ncompetitive hourly rate, approximately $12-16 per\nhour.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nYulong Chen, Yang Liu, Liang Chen, and Yue Zhang.\n2021. Dialogsum: A real-life scenario dialogue sum-\nmarization dataset. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 5062–5074.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nIshita Dasgupta, Andrew K Lampinen, Stephanie CY\nChan, Antonia Creswell, Dharshan Kumaran,\nJames L McClelland, and Felix Hill. 2022. Lan-\nguage models show human-like content effects on\nreasoning. arXiv e-prints, pages arXiv–2207.\nSong Feng, Siva Sankalp Patel, Hui Wan, and Sachindra\nJoshi. 2021. Multidoc2dial: Modeling dialogues\ngrounded in multiple documents. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 6162–6176.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781.\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp. arXiv preprint\narXiv:2212.14024.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2022. Decomposed prompting: A modular\n4541\napproach for solving complex tasks. arXiv preprint\narXiv:2210.02406.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu,\nYoungjae Yu, Pei Zhou, Ronan Le Bras, Malihe\nAlikhani, Gunhee Kim, Maarten Sap, et al. 2022.\nSoda: Million-scale dialogue distillation with so-\ncial commonsense contextualization. arXiv preprint\narXiv:2212.10465.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nMina Lee, Megha Srivastava, Amelia Hardy, John Thick-\nstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-\nUrsin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong,\net al. 2022. Evaluating human-language model inter-\naction. arXiv preprint arXiv:2212.09746.\nAitor Lewkowycz, Anders Johan Andreassen,\nDavid Dohan, Ethan Dyer, Henryk Michalewski,\nVinay Venkatesh Ramasesh, Ambrose Slone, Cem\nAnil, Imanol Schlag, Theo Gutman-Solo, et al.\n2022. Solving quantitative reasoning problems with\nlanguage models. In Advances in Neural Information\nProcessing Systems.\nAndrea Madotto, Zhaojiang Lin, Genta Indra Winata,\nand Pascale Fung. 2021. Few-shot bot: Prompt-\nbased learning for dialogue systems. arXiv preprint\narXiv:2110.08118.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: a survey. arXiv preprint arXiv:2302.07842.\nPiotr Mirowski, Kory W Mathewson, Jaylen Pittman,\nand Richard Evans. 2022. Co-writing screenplays\nand theatre scripts with language models: An eval-\nuation by industry professionals. arXiv preprint\narXiv:2209.14958.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2022. Show your work: Scratch-\npads for intermediate computation with language\nmodels. In Deep Learning for Code Workshop.\nOpenAI. 2022. Openai: Introducing chatgpt. https:\n//openai.com/blog/chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, et al. 2021.\nRecipes for building an open-domain chatbot. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 300–325.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, et al. 2022.\nBlenderbot 3: a deployed conversational agent that\ncontinually learns to responsibly engage. arXiv\npreprint arXiv:2208.03188.\nEric Smith, Orion Hsu, Rebecca Qian, Stephen Roller,\nY-Lan Boureau, and Jason Weston. 2022. Human\nevaluation of conversations is an open problem: com-\nparing the sensitivity of various methods for eval-\nuating dialogue agents. In Proceedings of the 4th\nWorkshop on NLP for Conversational AI, pages 77–\n97.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, et al. 2022. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv\npreprint arXiv:2210.09261.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\nford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nTogether. 2022. GPT-JT-6B-v1. https:\n//huggingface.co/togethercomputer/\nGPT-JT-6B-v1. Accessed: 2022-12-15.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax.\nBoshi Wang, Xiang Deng, and Huan Sun. 2022. Itera-\ntively prompt pre-trained language models for chain\nof thought. In Proceedings of the 2022 Conference\n4542\non Empirical Methods in Natural Language Process-\ning, pages 2714–2730.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Ja-\nson Weston, and Emily Dinan. 2020. Recipes for\nsafety in open-domain chatbots. arXiv preprint\narXiv:2010.07079.\nJing Xu, Arthur Szlam, and Jason Weston. 2022. Be-\nyond goldfish memory: Long-term open-domain con-\nversation. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5180–5197.\nChenghao Yang and Xuezhe Ma. 2022. Improving\nstability of fine-tuning pretrained language models\nvia component-wise gradient norm clipping. arXiv\npreprint arXiv:2210.10325.\nKevin Yang, Nanyun Peng, Yuandong Tian, and Dan\nKlein. 2022. Re3: Generating longer stories with\nrecursive reprompting and revision. arXiv preprint\narXiv:2210.06774.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you have\npets too? In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 2204–2213.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron\nCourville, Behnam Neyshabur, and Hanie Sedghi.\n2022. Teaching algorithmic reasoning via in-context\nlearning. arXiv preprint arXiv:2211.09066.\n4543\nA Evaluation Details\nA.1 Models\nFor single model evaluation of MPC, We use\nOPT (30B, 66B), OpenAI GPT-3 ( davinci,\ntext-davinci-002) GPT-JT, and BLOOM-\n176B as the base LM for the open-sourced pre-\ntrained LLM group. On the other hand, we eval-\nuate Blenderbot3 30B (BB3-30B), the best pub-\nlicly available open-domain chatbot model, for the\nfine-tuned model group. We also evaluate BB3-\n175B which is only available through the web in-\nterface https://blenderbot.ai/. We eval-\nuate BB3-175B for comparison purposes, though\nthe comparison is not fair as we cannot instill a\npersona into BB3 and cannot control for further dif-\nferences, such as internet search and user interface.\nFor pairwise model evaluation of MPC, we pair-\nwise test MPCOPT-30B and BB3-30B to compare\nhuman evaluations of a pre-trained model and a\nfine-tuned model. This experiment controls for\nmany variables as BB3-30B was initialized with\nOPT-30B before fine-tuning. For the module exper-\niment, we tested with OPT-30B, and we compared\nthe one with the whole pipeline, and the other only\nwith an utterance generator with a fixed persona.\nA.2 Dense Passage Retriever\nFor the DPR model of the memory module, we use\nthe model weights from the custom DPR model\nfinetuned for MultiDoc2Dial (Feng et al., 2021),\nas we have observed that this model performs\nslightly better than the original DPR model from\nKarpukhin et al. (2020).\nB Experimental Results\nIn this section, we report all experimental results\nand miscellaneous analysis.\nB.1 Full Single Model Evaluation\nIn this subsection, we report all our model eval-\nuations with additional details, such as average\nlatency (the amount of time it took to compute\nthe next utterance and show the user), and average\nutterance length, which is the number of tokens\nmeasured by the OPT tokenizer. We also addition-\nally report SCE-w, which was not reported in the\nmain body due to space restrictions in Table 5.\nWe compute a weighted score SCE-w similar to\nSSI (Thoppilan et al., 2022), in which a response\ncan only be consistent if it is sensible and only\nengaging if it is sensible and consistent. The fi-\nnal score is the average over all SCE-w-adjusted\nmetrics.\nWe also note that OpenAI API calls have highly\nvariable latency since December 2022 due to Chat-\nGPT, so a measure of latency due to modularization\ncannot properly be quantified.\nBB3-175B Evaluation Our results for BB3-\n175B are not directly comparable to other mod-\nels due to a different evaluation procedure. Since\nthe model weights for BB3-175B are not pub-\nlicly available, we referred evaluators to inter-\nact with the Blenderbot version available at\nhttps://blenderbot.ai/chat. Evaluators were then\nasked to copy and paste all user inputs and chatbot\nresponses while annotating the chatbot responses\nsimilarly to other models. We also note that consis-\ntency can only be evaluated for persona consistency\nwithin the dialog context as we are unable to pro-\nvide a specific persona for the online version of\nBlenderbot.\nB.2 Model Configurations\nWe include our code for running each\nmodel we evaluate in our repository at\nhttps://github.com/krafton-ai/MPC. Each model\nconfiguration can be found in the repository.\nThe configuration includes decoding parameters\n(e.g., sampling method, temperature for each\nmodule) and slight variations of the prompts for\nthe utterance generator module.\nB.3 Additional Analysis\nAlpaca-7B Alpaca-7B (Taori et al., 2023) is an\ninstruction-tuned language model, fine-tuned from\nthe LLaMA 7B model (Touvron et al., 2023) on\n52K instruction and demonstration pairs. We run\nsmall-scale dialogue evaluations for Alpaca-7B\nwith full persona and MPCAlpaca-7B.\nWe find that, although we have only collected\nlimited samples due to resource constraints, we\nsee that MPCAlpaca-7B achieves slightly higher\nSCE and rating than its full-persona counterpart\nin Table 5. Moreover, MPCAlpaca-7B shows higher\nscores than MPCOPT-30B. We attribute this high\nachievement to a better base LM, as demonstrated\nin Touvron et al. (2023), and to instruction-tuning,\nas we have explained in the main body.\nChatGPT and GPT-4 Although MPC results\nfor GPT-3.5-turbo (ChatGPT) are overwhelmingly\npositive, we highlight that a direct comparison to\n4544\nModel Sens. Cons. Eng. SCE-w SCE-p Length Latency Rating Sample size\n(%) (%) (%) (%) (%) (tokens) (s) (/5.0)\nFine-tuned\nBB3-30B 71.3 77.8 73.7 62.0 54.3 24.9 3.7 2.9 27|540\nBB3-30B (non-repeated) 84.4 80.3 90.3 73.2 65.6 25.4 3.7 3.5 16|320\nBB3-175B* 85.9 88.7 84.8 80.0 73.3 25.4 - 3.8 27|540\nFull persona\ntext-davinci-002 94.0 94.7 84.3 88.4 79.7 15.4 0.8 4.1 35|700\ndavinci* 91.8 89.2 78.8 82.5 70.8 13.4 1.5 3.8 25|500\nOPT-30B* 92.0 87.2 86.0 82.3 72.8 13.7 1.0 3.7 25|500\nAlpaca-7B* 96.0 88.5 82.5 84.5 71.5 16.0 1.1 4.0 10|200\nMPC\ntext-davinci-002 93.6 87.8 85.5 84.4 75.0 23.1 4.7 4.2 39|780\ndavinci 80.2 72.0 69.1 66.4 53.3 19.9 8.4 3.1 27|540\nOPT-66B 90.5 84.8 88.1 81.4 73.9 14.2 4.1 4.1 33|660\nOPT-30B 86.1 79.1 80.7 73.9 63.4 15.4 3.3 3.6 37|740\nGPT-JT 91.1 83.2 65.3 74.4 53.5 8.6 2.0 3.1 33|660\nBLOOM-176B 65.2 65.5 61.4 51.8 40.5 15.6 12.4 2.8 28|500\nMPC+Full\ntext-davinci-002* 94.4 92.2 92.8 88.7 83.0 31.4 16.3 4.2 25|500\nOPT-30B* 85.6 87.2 89.0 78.8 72.6 15.2 2.4 3.7 25|500\nAlpaca-7B* 96.3 89.4 78.8 86.2 73.8 11.7 13.6 4.3 8|160\ngpt-3.5-turbo-0301* 97.3 96.2 97.7 94.5 93.0 51.9 19.3 4.5 26|474\ngpt-4* 97.4 97.4 97.4 95.7 94.0 64.5 58.5 4.7 25|469\nTable 5: Full experimental evaluation results of single model evaluation where sample size indicates the number of\ndialogues collected and the number of utterance evaluations. BB3-30B (non-repeated) is the same as BB3-30B but\nexcludes any conversations that had repetition of previous bot utterances. * denotes model experiments that were\nonly run on MTurk. Though our work precedes Alpaca-7B, GPT-3.5-turbo, and GPT-4 at the time of submission,\nwe include evaluations of these models and analyses in the appendix for the sake of completeness.\nother models is difficult since GPT-3.5-turbo is\ntrained specifically in dialogue-style data, and its\nmodel architecture and training process are opaque.\nOne observation is that MPCGPT-3.5 responses are\nsignificantly longer compared to all other models.\nWhile it is tempting to attribute the positive re-\nsults of MPCGPT-3.5 and MPCGPT-4 solely to\ndialogue-tuning, such a conclusion may not be\nentirely accurate. Based on our experimental re-\nsults and comprehensive analysis, we cautiously\nhypothesize that the success of MPCGPT-3.5 and\nMPCGPT-4 primarily stems from the utilization of\na more robust language model. Strong base LLMs\nplay a crucial role in ensuring sensibleness and con-\nsistency in generated responses. Moreover, as we\nhave demonstrated, instruction-tuning is effective\nin facilitating the development of modular com-\nponents for open-domain chatbot design. Thus,\nwithout more information about these LLMs, we\ncarefully posit that it is mainly the combination of\na stronger LM and effective instruction-tuning that\ncontribute to the positive results in our evaluation\nof MPCGPT-3.5 and MPCGPT-4.\nB.4 Subgroup Analysis\nWe measure the difference in ratings, for each met-\nric and our combined metrics, between MTurk and\nstudent evaluators and find no significant difference\nfor most experiments and metrics (Fig.2. In gen-\neral, we observe the greatest rating difference for\nsensibleness.\nBB3-30B is an outlier with a significant differ-\nence across all metrics, especially for sensibleness\nwith a difference of about 20%. BB3-30B dialogs\ncontain an increased ratio of repetition issues for\nstudents but even when excluding those dialogs, we\nstill find a significant rating difference. Looking\nat the dialog histories, we observe that BB3-30B\nstudent evaluators were less patient than MTurk\nevaluators when BB3-30B would repeat the same\nquestions or suddenly change the subject. It should\nbe noted that this is a qualitative observation and\nshould be taken with a grain of salt.\nC Data Collection Details\nWe attach screenshots of the evaluation forms, used\nfor data collection, for single (Fig. 3) and pairwise\n4545\n-15.0%-10.0%-5.0%0.0%5.0%10.0%15.0%20.0%25.0%\nS C E SCE-w SCE-p\nDifference\nMTurk -Student\nBB3 30BBB3 30B (w/o repetition)OPT-30BGPT3-td2GPT3-td2-plainOPT-66BGPT-JTGPT3-dBLOOM\nFigure 2: The average score of MTurk workers group minus the average score of university students group. We\nfind the two subgroups are very similar on average across metrics, though sensibleness seems to show the greatest\ndifference. Students, in general, score chatbot models slightly more harshly. BB3-30B is an outlier which students\nscore significantly lower than MTurk workers.\nmodels (Fig. 4) respectively. We also attach the\nscreenshot of the instruction, alert, and consent\nformat that we gave to participants (Fig. 5 and 6).\nC.1 MTurk Setting\nWe gathered at least 15 MTurk workers for each\ntask. We require them to have an MTurk Master’s\nqualification, a HIT approval rate of more than\n99%, and a number of HITs approved more than\n100 times, and to live in the US, UK, Canada, Aus-\ntralia, and New Zealand.\nWe found that data quality improves significantly\nwhen using MTurk Master workers. These workers\ntend to produce more fluent messages than non-\nMaster workers and are less likely to provide ran-\ndom or generic responses. The difference was more\npronounced as non-Master crowdworkers would of-\nten provide a score of 99% on average despite the\nclear problems of repetition and contradiction from\nthe dialogue agent.\nC.2 Student Participants\nTo cross-validate the result of MTurk, we gathered\n49 English-proficient students from a several uni-\nversities. They are either undergraduate or graduate\nstudents. The minimum requirement they should\nmeet is English proficiency: IELTS >= 7.0; TOEFL\nIBT >= 95; and TOEIC >= 900. This criterion is\nsimilar or equivalent to the minimum required level\nof English proficiency for admission to the gradu-\nate schools of MIT. The average English scores of\nstudents we gathered are the following: IBT 108.4\nout of 120; TOEIC 960.9 out of 990; and IELTS\n7.83 out of 9.0.\nC.3 Crowdworker Instruction\nThe crowd workers are asked to continue the chat\nfor 20 turns and evaluate each response by the\nmetrics described in the section below. We in-\nstruct them to type more than 3 words on average,\nand not to repeat meaningless or generic messages.\nAlso, we request them not to randomly choose be-\ntween yes or no and to provide evaluations honestly.\nLastly, we strongly emphasize the importance of\nmaintaining confidentiality and request the crowd\nworkers refrain from disclosing any private infor-\nmation about themselves or others during the eval-\nuation process.\n4546\nFigure 3: Evaluation form for a single model.\nD Few-shot Prompts\nIn this section, we show our full few-shot prompts,\nincluding the CoT prompt, for each module. The\nutterance generator prompts vary slightly between\nmodel experiments. These prompts can be seen\nin the model configurations in our repository. The\nsecond example for dialogue summary is modified\nfrom a sample from the dataset DialogSum (Chen\net al., 2021).\nE Utterance Generator Prompt Examples\nIn this section, we provide example prompts for the\nutterance generator of MPC and Vanilla models.\nThe persona given is one of the five personas we use\nand the dialogue context is provided as an example.\n4547\nFigure 4: Evaluation form for pairwise model comparison.\n4548\nFigure 5: We display this page before the evaluators start the evaluation process to inform them about the task and\ngather their consent for data usage.\nFigure 6: Additional Alert in Mturk HIT page.\n4549\nUtterance Generator\nRephrase User’s question in third-person.\nSarah: I’ve been working at the coffee shop for about six months.\nUser: I see. what did you do before that?\n# Specifically, What did Sarah do before working at a coffee shop for six months?#\nAshley: Do you know who Ronaldo is?\nUser: I don’t know, who is he?\n# Specifically, Who is Ronaldo?#\nRobert: Hey, how are you doing?\nUser: Good. What’s your name? Do you know my name?\n# Specifically, What is Robert’s name? Does Robert know User’s name?#\nJennifer: I like reading about history and science.\nUser: What kind of history? I like history too!\n# Specifically, What kind of history does Jennifer read about?#\nJohn: Hi! How are you doing today?\nUser: good. how about you\n# Specifically, How is John doing today?#\nMemory Processor w/o CoT\n# This is the list of John’s knowledge.\nJohn’s full name is John Parker.\nUser is a teacher at a local middle school.\nUser teaches biology.\nJohn likes to go for a run.\nUser enjoys watching movies, but User doesn’t like superhero movies.\nQ: What does User do for a living?\nA: John thinks User is a biology teacher at a local middle school.\n# This is the list of Ashley’s knowledge.\nAshley likes history documentaries.\nAshley does not like Korean food.\nAshley is a teacher at a local middle school.\nUser likes biology and especially anatomy.\nAshley likes French cuisine.\nQ: What is Ashley’s favorite dish?\nA: Ashley thinks Ashley likes French cuisine but does not like Korean food.\nMemory Processor w/ CoT\n# This is the list of Ashley’s knowledge.\n(1) Ashley likes history documentaries.\n(2) Ashley does not like Korean food.\n(3) Ashley is a teacher at a local middle school.\n(4) User likes biology and especially anatomy.\n(5) Ashley likes French cuisine.\nQ: What is Ashley’s favorite dish?\nA: Let’s think step by step.\n(1) History documentaries are not related to Ashley’s favorite dish. (2) Ashley’s favorite dish would not be Korean\nbecause she does not like Korean food. (3) Ashley being a teacher does not tell us anything about her favorite dish. (4)\nThis fact is about User, not Ashley. (5) Ashley’s favorite dish may be French since she likes French cuisine. Therefore,\n(2) and (5) can help answer the question.\nAnswer: Ashley thinks Ashley likes French cuisine but does not like Korean food.\n4550\nDialogue Summarizer\n#Dialogue\nUser: Tell me about yourself\nSally: I’m 26 years old and graduated from a college in Wisconsin.\nUser: Were you a leader when you were in college?\nSally: Yes. I was the head TA for a computer science course at our university.\nUser: Were you involved in any club activities at your university?\nSally: Yes. I was a member of the basketball Society. I like playing basketball.\n#Summary\n- Sally is 26 years old and graduated college in Wisconsin.\n- Sally was the head TA for a computer science course.\n- Sally played basketball in college.#\n#Dialogue\nUser: Out of every movie that you’ve seen, which one is your favorite?\nJohn: I’m going to have to say that Superbad is the best movie ever.\nUser: You think so, how come?\nJohn: Well, Superbad is super funny.\nUser: You’re not lying, I found that movie absolutely hilarious.\nJohn: I didn’t know that you saw Superbad before.\nUser: I made sure to be in line to see it the first day it came out.\n#Summary\n- John’s favorite movie is Superbad.\n- John and User think Superbad is funny.\n- User watched Superbad the first day it came out.#\nUtterance Generator of MPC\nSarah is polite, fun, and engaging. The following is a conversation between Sarah and Person1.\nPerson1: My Favorite part of the concert is making sure I get great seats and then I can catch guitar picks\nSarah: That sounds like a lot of fun!\nPerson1: It is a lot of fun, so what do you like to do for fun?\nSarah: I love going to concerts, car shows, music festivals, and houses with fireplaces.\nPerson1: I love going to car shows and looking at older cars, what is your favorite car?\nSarah: I think my favorite car is a vintage mustang.\nPerson1: OH man vintage Mustangs are great, I had a friend who owned a 69 mach 1, that was a dream car\nSarah: Well, I hope you get to experience your dream car one day!\nPerson1: Maybe someday I will, do you own a Mustang?\nThe following statements are true about Sarah.\nSarah thinks Sarah owns a Mustang.\nGive a friendly sensible response that is interesting and polite to Person1. End with a question.\nSarah: Well , I hope you get to experience your dream car one day!\nPerson1: Maybe someday I will, do you own a Mustang?\nSarah:\nResponse from previous prompt\n\"Yes, I do own a Mustang. It’s a great car to drive. Do you like driving?\"\n4551\nUtterance Generator of MPC with full persona\nThe following are persona facts about Sarah.\nSarah is 40 years old.\nSarah currently lives in small town in Georgia.\nSarah reads twenty books a year.\nSarah is a stunt double as Sarah’s second job.\nSarah only eats kosher.\nSarah was poor growing up.\nSarah was raised in a single parent household.\nSarah has two dogs.\nSarah likes to work on vintage cars.\nSarah owns two vintage mustangs.\nSarah’s favorite music is country.\nSarah volunteers at a soup kitchen.\nSarah thinks Sarah grew up in a small town and didn’t have a car.\nSarah is polite, fun, and engaging. The following is a conversation between Sarah and Person1.\nPerson1: We at least had a playground in tow and my grandparents lived in right in front of it, so we went to both places\na good bit.\nSarah: Maybe it was difficult to find things to do in your town, but it sounds like you had a lot of family support, which\nis really important. It’s good that you had a place to go where you felt safe and loved.\nPerson1: You ended up growing up poor too.\nSarah: Yes, I did. I think it’s made me a lot more sympathetic to other people’s experiences.\nPerson1: We were poor too and my family didn’t have a car.\nSarah: I can imagine that it would be tough to not have a car. My family had a car, but we were poor too.\nPerson1: It was. Almost every month we had barely any food and my parents had to deal with what we had.\nGive a friendly sensible response that is interesting and polite to Person1. End with a question.\nSarah:\nResponse from previous prompt\n\"It sounds like it was really tough for you and your family. I’m sorry that you had to go through that. Do you think that\ngrowing up in a small town made it harder for you?\"\nUtterance Generator of Vanilla (full persona)\nThe following statements are true about Sarah.\nSarah is 24 years old.\nSarah currently lives in Canada.\nSarah is a swim coach at Sarah’s local pool.\nSarah is studying to be a computer programmer.\nSarah is also a graduate student.\nSarah is now looking for a new job.\nSarah’s mother is very traditional while Sarah prefers to be more free spirited.\nSarah’s family and Sarah are from India.\nSarah’s favorite music genre is death metal.\nSarah is a famous twitch streamer.\nSarah likes watching war documentaries.\nSarah’s favorite food is mexican food.\nSarah is polite, fun, and engaging. The following is a conversation between Sarah and Person1.\nPerson1: Have you thought about finding a job that allows you to travel?\nSarah: I have, but I’m not sure what type of job would allow me to do that.\nPerson1: What types of jobs have you looked into already?\nSarah: I’ve looked into jobs in the travel industry, but I’m not sure if that’s what I really want to do.\nPerson1: Since your a twitch streamer, could you maybe make that your career?\nSarah:\nResponse from previous prompt\n\"I don’t know, I’ve never thought about that before. I’ll have to look into it.\"\n4552\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthics Statement\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nFor Grammar ﬁxes and for Making our text more concise to ﬁt into the page limit. Mostly we used\nGPT-3 text davinci 3 to generate them, but the results are mostly similar to what we originally wrote.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n1,4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n1,4\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\ncode will be publicly available under MIT license after review period\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLimitations\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nEthic Statements\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLimitations\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix B\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4, Appendix B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4553\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4, Appendix A,B,E\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n5, appendix B\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n1,2,4, git repository\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nsection 4; Appendix E\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nAppendix E\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nEthics Statement, Appendix E\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nAppendix E\n□\u0017 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nAppendix E. But partly revealed because of anonymity, we hide the detailed nationality.\n4554"
}