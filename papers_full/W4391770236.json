{
  "title": "Automatic Speech Recognition with BERT and CTC Transformers: A Review",
  "url": "https://openalex.org/W4391770236",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093918145",
      "name": "Noussaiba Djeffal",
      "affiliations": [
        "University of Algiers Benyoucef Benkhedda",
        "University of Sciences and Technology Houari Boumediene"
      ]
    },
    {
      "id": "https://openalex.org/A2187790822",
      "name": "Hamza Kheddar",
      "affiliations": [
        "University Yahia Fares of Medea"
      ]
    },
    {
      "id": "https://openalex.org/A261961080",
      "name": "Djamel Addou",
      "affiliations": [
        "University of Sciences and Technology Houari Boumediene",
        "University of Algiers Benyoucef Benkhedda"
      ]
    },
    {
      "id": "https://openalex.org/A2225736762",
      "name": "Ahmed Cherif Mazari",
      "affiliations": [
        "University Yahia Fares of Medea"
      ]
    },
    {
      "id": "https://openalex.org/A2999988285",
      "name": "Yassine Himeur",
      "affiliations": [
        "University of Dubai",
        "Information Technology University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2910071309",
    "https://openalex.org/W4206580415",
    "https://openalex.org/W4385337322",
    "https://openalex.org/W3128513378",
    "https://openalex.org/W4243640523",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W4353114117",
    "https://openalex.org/W6791280665",
    "https://openalex.org/W3095902331",
    "https://openalex.org/W6799082546",
    "https://openalex.org/W4226291738",
    "https://openalex.org/W3175061805",
    "https://openalex.org/W3196445330",
    "https://openalex.org/W3016128928",
    "https://openalex.org/W3197326126",
    "https://openalex.org/W4226054021",
    "https://openalex.org/W3198259287",
    "https://openalex.org/W3160622492",
    "https://openalex.org/W6792858540",
    "https://openalex.org/W3096297644",
    "https://openalex.org/W3095738461",
    "https://openalex.org/W3198116002",
    "https://openalex.org/W6855419134",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4313531555",
    "https://openalex.org/W3141961557",
    "https://openalex.org/W3114026450",
    "https://openalex.org/W2251058040",
    "https://openalex.org/W4226507725",
    "https://openalex.org/W4224821750",
    "https://openalex.org/W4296069152",
    "https://openalex.org/W3095373628",
    "https://openalex.org/W4225985539",
    "https://openalex.org/W3197797812",
    "https://openalex.org/W4308312656",
    "https://openalex.org/W3035531585",
    "https://openalex.org/W3196522458",
    "https://openalex.org/W6731992975",
    "https://openalex.org/W4285345679",
    "https://openalex.org/W4245692952",
    "https://openalex.org/W3197140813",
    "https://openalex.org/W2531638282",
    "https://openalex.org/W3104502794",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4312437435",
    "https://openalex.org/W4317652021",
    "https://openalex.org/W4385749883"
  ],
  "abstract": "This review paper provides a comprehensive analysis of recent advances in\\nautomatic speech recognition (ASR) with bidirectional encoder representations\\nfrom transformers BERT and connectionist temporal classification (CTC)\\ntransformers. The paper first introduces the fundamental concepts of ASR and\\ndiscusses the challenges associated with it. It then explains the architecture\\nof BERT and CTC transformers and their potential applications in ASR. The paper\\nreviews several studies that have used these models for speech recognition\\ntasks and discusses the results obtained. Additionally, the paper highlights\\nthe limitations of these models and outlines potential areas for further\\nresearch. All in all, this review provides valuable insights for researchers\\nand practitioners who are interested in ASR with BERT and CTC transformers.\\n",
  "full_text": "2023 2nd International Conference on Electronics, Energy and Measurement (IC2EM 2023)\nAutomatic Speech Recognition with BERT and\nCTC Transformers: A review\nNoussaiba Djeffal\nSpeech and Signal Processing Lab.\nUSTHB University\nAlgiers, Algeria\nndjeffal@usthb.dz\nHamza Kheddar\nLSEA Lab., Faculty of Technology\nUniversity of MEDEA\nMedea 26000, Algeria\nkheddar.hamza@univ-medea.dz\nDjamel Addou\nSpeech and Signal Processing Lab.\nUSTHB University\nAlgiers, Algeria\ndaddou@usthb.dz\nAhmed Cherif Mazari\nLSEA Lab, Faculty of Science\nUniversity of MEDEA\nMedea 26000, Algeria\nmazari.ahmedcherif@univ-medea.dz\nYassine Himeur\nCollege of Engineering and Information\nTechnology, University of Dubai\nDubai, UAE\nyhimeur@ud.ac.ae\nAbstract—This review paper provides a comprehensive analy-\nsis of recent advances in automatic speech recognition (ASR) with\nbidirectional encoder representations from transformers BERT\nand connectionist temporal classification (CTC) transformers.\nThe paper first introduces the fundamental concepts of ASR and\ndiscusses the challenges associated with it. It then explains the\narchitecture of BERT and CTC transformers and their potential\napplications in ASR. The paper reviews several studies that have\nused these models for speech recognition tasks and discusses the\nresults obtained. Additionally, the paper highlights the limitations\nof these models and outlines potential areas for further research.\nAll in all, this review provides valuable insights for researchers\nand practitioners who are interested in ASR with BERT and\nCTC transformers.\nIndex Terms—Automatic speech recognition, BERT, CTC,\nChatGPT, Transformers.\nI. I NTRODUCTION\nTraditional methods of speech recognition rely on maximum\na posteriori probability estimation, which involves transform-\ning the acoustic speech characteristics into word sequences\nthrough four steps: feature extraction, acoustic modeling,\nlanguage modeling, and word sequence decoding. Feature ex-\ntraction involves essential data extraction from the input signal\nusing algorithms such as Mel-frequency cepstral coefficients\n(MFCC) [1] and perceptual line spectral pairs (LSP) [2]. The\nacoustic modeling stage utilizes deep neural networks and\nhidden Markov models to map the acoustic frame to the\nphonetic state at each input time, optimized for the phonetic\nclassification error per frame. Language modeling is designed\nto model the most probable sequences of words, regardless of\nacoustics [3]. The use of transformers in speech recognition\ninvolves several steps, such as : (i) preprocessing of the audio\nsignal, to extract essential features like log Mel filterbank\nenergies, (ii) an acoustic model based on a self-attention mech-\nanism to model the temporal relationships between acoustic\nfeatures, and (iii) a language model trained on a large amount\nof text data to capture long-term dependencies between words.\nThe language model takes a sequence of words as input and\npredicts the probability distribution of the next word in the\nsequence. Finally, the decoding process involves finding the\nmost likely word sequence, given the output of both the\nacoustic and language models.\nTo sum up, transformers such as BERT, and connectionist\ntemporal classification (CTC) based ASR is a recent advance-\nment in speech recognition that uses the self-attention mech-\nanism to simultaneously perform feature extraction, acoustic\nmodeling, language modeling, and decoding in a single net-\nwork. On the other hand, the transformer architecture is a\nneural network model that is designed to process sequential\ndata by attending to relevant context information. Transform-\ners have demonstrated promising outcomes in ASR and are\nexpected to play a crucial role in future advancements in this\nfield.\nA. Related work and our contribution\nIn [4] the authors explore various methodologies for de-\ntecting emotions in text using BERT and its variants. They\nthoroughly outline their approaches, contributions, achieved\naccuracies, and also discuss the limitations or weaknesses of\ntheir models. However, our review focuses on ASR rather than\nemotion using BERT. While the authors primarily mention\nBERT base, BERT large, RoBERTa, DistillBERT, and cross-\nlingual language model (XLM), which are the BERT variants\nemployed in their research, we expand on their contributions\nby incorporating additional models such as ALBERT, and\nELECTRA. The work in [5] focuses on providing a tutorial\nand survey on the attention mechanism, transformers, BERT,\nand GPT. It explains various concepts such as the attention\nmechanism, transformers, and their components. However,\nour review focuses on both BERT and CTC transformer\napplications, specifically in ASR. In [6] the survey discusses\nthe impressive performance of transformer models like BERT,\nGPT, RoBERTa, and T5 across various language tasks such as\n979-8-3503-1424-3/23/$31.00 ©2023 IEEE\narXiv:2410.09456v1  [cs.CL]  12 Oct 2024\ntext classification, machine translation, and question answer-\ning. Additionally, the article also explores their applications\nin computer vision. However, our review focuses on BERT\nand CTC transformers within the domain of ASR, distin-\nguishing our research from other domains discussed in the\naforementioned survey. In [7] encompasses various speech-\nrelated domains such as ASR, speech synthesis, speech trans-\nlation, speech para-linguistics, speech enhancement, and other\napplications. The authors of the survey identify and discuss\nthe challenges that transformers face in these domains. In our\nreview, similar to the survey, we address these challenges, but\nwe extend the scope by including additional transformers like\nELECTRA, ALBERT, and CTC Transformers specifically in\nthe context of ASR.\nB. Paper structure\nThe rest of the paper is organized as follows: Section II\npresents the methodology used to create this review. Section III\nprovides an overview of the preliminaries on CTC and BERT.\nSection IV defines BERT and categorizes articles based on the\nutilization of BERT in ASR, highlighting the advancements\nmade in this area. Section V defines CTC and classifies\narticles based on the application of CTC in ASR, showcasing\nthe progress achieved in this domain. Section VI discusses\npotential future directions. Finally, the paper concludes in\nsection VII.\nII. R EVIEW METHODOLOGY AND ANALYSIS\nThis review focuses mainly on two distinct categories of\npapers: The first category examines BERT-based ASR systems,\nthe second category of papers explores CTC-based ASR\ntechniques. The initial search was done using related keywords\nfor transformers in ASR, namely: BERT and ASR, CTC and\nASR, or BERT and CTC and ASR. A search was performed\non scientific databases indexed at least in Scopus and available\nin IEEE Xplore and Springer, science direct, and others.\nBesides, arXiv papers are taken into consideration that have\nmany citation and impact, which are known for their extensive\ncoverage and relevance to ASR. Additionally, Google Scholar\nwas utilized to include a broader range of publications, in-\ncluding gray literature, which can provide valuable insights\nfor a systematic review. Only the most widely used methods\nand implementations were included to ensure modularity. The\nfocus is on papers that reported new and unique applications\nwithin specific domains, avoiding repetition. Emphasis is given\nto papers published in high-quality journals with a significant\nimpact factor. The search has been conducted until 2023 to\ngather the most recent available information at the time of the\nreview.\nIII. P RELIMINARIES\nA. Dataset\nSeveral datasets have been utilized in various ASR tasks in\nthe existing literature. Table I provides a compilation of some\nof these datasets that have been used specifically for BERT\nand CTC-based ASR applications, along with their specific\ncharacteristics [3].\nB. Metrics\nThe ASR research community has employed several meth-\nods to evaluate the quality and generalizability of ASR\ntechniques. Besides the famous metrics that are commonly\nemployed in ML and DL, such as accuracy, F1-score, recall,\nand precision [24], other metrics are used including word\nerror rate (WER) and character error rate (CER), and real-\ntime factor (RTF), are thoroughly described in [3].\nIV. BERT- BASED ASR\nBERT, developed by Devlin et al. in 2019 [25], is a\npre-training model for NLP tasks that utilizes transformer\nencoders [26]. It consists of two phases: pre-training for\nlanguage understanding and fine-tuning. These latter are for\nspecific tasks like sentiment analysis, question answering,\ntext summarization, and more. During pre-training, BERT\nemploys masked language modeling (MLM) and next-sentence\nprediction (NSP). MLM involves masking some words in sen-\ntences and reconstructing them using the surrounding context\nduring training. NSP helps BERT understand the relationship\nbetween two sentences by predicting if the second sentence\nfollows the first. BERT was trained on 16GB of text data\nfrom the books corpus datasets and English Wikipedia. After\npre-training, the model is fine-tuned for a specific task by\nreplacing BERT’s output layers. This fine-tuning process is\nfaster since only the model parameters, excluding the output\nparameters, are learned from scratch. There are two versions\nof BERT: BERT-base and BERT-large. BERT-base consists\nof 12 transformer encoder blocks with 12-head self-attention\nlayers and 768 hidden layers, resulting in approximately 110\nmillion parameters. BERT-large has 24 transformer encoder\nblocks with 24-head self-attention layers and around 340\nmillion parameters. BERT-large achieves higher accuracies but\nrequires more computational resources compared to BERT-\nbase [4]. However, BERT has a few notable limitations.\nFirstly, it is primarily designed for monolingual classifications,\nmeaning its optimal performance is achieved when working\nwith a single language. While it is possible to fine-tune BERT\nfor multilingual tasks, its effectiveness may be somewhat\ndiminished compared to its performance on monolingual tasks.\nSecondly, the length of input sentences can also present\nchallenges. BERT has a maximum token limit, typically set\nat 512 tokens, which means longer sentences need to be\ntruncated or split into smaller segments, potentially losing\nsome contextual information [27]. Table II presents a summary\nof the performance achieved in the cited papers compared\nto other systems, including the metrics used to evaluate the\nresults, and the source code availability. Figure 1 illustrates\nBERT variants featuring transformers along with the attention\nlayers they are built upon.\nA. BERT-based SMCQA framework\nThe authors in [8] developed a framework called MA-\nBERT for spoken multiple-choice question answering (SM-\nCQA) task, which uses a combination of multi-turn audio-\nextractor hierarchical CNNs (MA-HCNNs) and BERT to\nTABLE I\nA LIST OF PUBLICLY AVAILABLE DATASETS COMMONLY USED IN TRANSFORMERS BERT AND CTC- BASED ASR RESEARCH :\nDataset Used by Description\nFGC [8], [9] Is a collection of data specifically focused on the task of spoken multiple-choice question answering in Mandarin Chinese.\nThis dataset was created for the formosa grand challenge (FGC) competition held in 2018.\nAMI database [10] Is a widely used and publicly available dataset in the field of multimodal interaction research. Researchers use the augmented\nmultiparty interaction (AMI) database for tasks such as speech recognition, speaker diarization, language understanding,\ndialogue systems, and multimodal analysis.\nCNNDM, How2,\nTED\n[11] The CNNDM dataset is a large-scale dataset primarily used for text summarization tasks. It consists of news articles\ncollected from the websites of cable news networks, and daily mail, the How2 dataset is designed for the task of instructional\nvideo captioning and translation. Specifically, it provides textual descriptions of ”how-to” videos, which are instructional\nvideos demonstrating various tasks and activities. The TED corpus is a collection of summarized TED talks; the corpus\nwas constructed by linking TED talks from the TEDLIUM corpus with their corresponding summaries.\nDSTC2 [12] The dialog state tracking challenge 2 (DSTC2) dataset is a widely used benchmark dataset in the field of dialog systems\nand spoken language understanding.\nIWSLT2011 [13], [14] Serves as a benchmark for the task of spoken language translation, dataset contains approximately 25 hours of recorded\nspeech. This duration includes speech data in multiple languages, such as English, French, Spanish, and German.\nFSC, ATIS [15] The frame-semantic corpus (FSC) and airline travel information system (ATIS) datasets are both widely used in the field\nof natural language processing (NLP) and dialogue systems. The total duration of the FSC dataset is approximately 130\nhours.\nFisher-CallHome\nSpanish\n[16] It is a combination of two data corpora: Fisher and CallHome. The Fisher corpus consists of multilingual telephone\nrecordings and was originally collected for research in ASR. The CallHome corpus is a dataset of telephone conversations\nin different languages, it consists of approximately 300 hours of recorded speech.\nLibriSpeech [17] It consists of speech data from audiobooks available in the LibriV ox project. The dataset comprises approximately 1,000\nhours of audio recordings, with a sampling rate of 16 kHz. It is a collection of high-quality speech data obtained by\nsampling audiobooks in the project.\nAishell1 [17]–[19] It comprises a large collection of Mandarin Chinese speech recordings from multiple speakers. It contains approximately\n178 hours of speech data from around 400 speakers, covering a wide range of accents, ages, and genders.\nWSJ [19]–[22] Typically refers to the Wall street journal dataset, which is a commonly used benchmark dataset in NLP and information\nretrieval research, contained around 80 hours of transcribed speech.\nCSJ [20], [23] Typically refers to the corpus of spontaneous Japanese. The CSJ dataset contained approximately 570 hours of recorded\nspeech.\nTEDLIUM2 [19], [23] Is a widely used benchmark dataset in the field of ASR, created by the spoken language systems (SLS) group at the\nUniversity of Cambridge.\nDCASE [22] It includes binaural recordings captured in 15 different sound environments or settings. These settings represent distinct\nacoustic scenes and cover a variety of audio environments.\nextract acoustic-level and text-level information, respectively,\nfrom speech data. The proposed framework outperformed\nvarious state-of-the-art systems. However, the scheme in\n[9] proposes a novel audio-enriched BERT-based (aeBERT)\nframework for improving performance on the SMCQA task,\nwhere syllables, questions, and choices are all given in speech.\nBesides, the method proposes incorporating acoustic-level\ninformation from the speech input to enhance the accuracy of\nSMCQA systems. The resulting audio-enriched BERT-based\nSMCQA framework shown to outperform various state-of-the-\nart systems by a large margin.\nB. BERT-based reranking framework\nChiu et al. [10] propose a BERT n-best reranking framework\nthat incorporates cross-utterance information signals using a\ngraph convolutional network (GCN) to model historical ut-\nterances for better ASR performance. The approach addresses\nthe limitations of recurrent neural network (RNN) and LSTM-\nbased language models (LMs) in capturing complex global\nstructural dependencies among utterances. Nevertheless, the\nstudy in [28] introduces a new implementation of BERT-based\ncontextualized language models specifically for reranking the\nn-best hypotheses generated by ASR systems. The approach\nframes the n-best hypothesis reranking as a prediction prob-\nFig. 1. Types of BERT with transformer encoder layers.\nlem, aiming to predict the oracle hypothesis with the lowest\nWER.\nC. BERT-based model for speech summarization\nKano et al. [11] suggest a novel text summarization (TS)\nmethod that combines sub-word embedding vectors and pos-\nterior values from an ASR system. They incorporate an\nattention-based fusion module into a pre-trained BERT mod-\nule for improved summarization. This fusion module aligns\nand merges multiple ASR hypotheses. The researchers then\nperform experiments on speech summarization using both the\nHow2 and TED dataset. In [29] The authors of the paper\nenhance a BERT-based model for speech summarization in\nthree ways: incorporating confidence scores into sentence rep-\nresentations to address ASR errors, augmenting sentence em-\nbeddings with additional features, and validating the model’s\neffectiveness on a benchmark dataset compared to classic\nsummarization methods. The goal is to improve the model’s\nperformance and overcome challenges caused by imperfect\nASR.\nD. BERT-based model for distilling the knowledge\nFutami et al. [20] propose a method to improve ASR using\na combination of a (sequence-to-sequence) seq2seq model and\nBERT as an external language model. The seq2seq model is\nenhanced with both left and right context through knowledge\ndistillation from BERT which generates soft labels to guide the\ntraining. Additionally, context beyond the current utterance is\nleveraged as input to BERT. The proposed method is evalu-\nated on the CSJ, showing significant improvements in ASR\nperformance compared to the seq2seq baseline. This method\nsurpasses alternative approaches in LM applications like n-best\nrescoring and shallow fusion with not requiring any additional\ninference cost. Jiang, B et al. [15] suggest a method for end-\nto-end intent classification using speech, which does not rely\non an intermediate ASR module. It leverages the transformer\ndistillation method to transfer knowledge from a transformer-\nbased language model BERT to a transformer-based speech\nmodel for intent classification. A multi-level transformer-based\nteacher-student model is designed, and knowledge distillation\nis performed across attention and hidden sub-layers of different\ntransformer layers. The proposed method achieves a high level\nof accuracy in intent classification and showcases superior\nperformance and resilience in acoustically degraded conditions\nwhen compared to the baseline method.\nE. BERT, RoBERTa, XLM-RoBERTa, and ELECTRA models\nGanesan et al. [12] propose a method to improve the\nperformance of spoken language understanding (SLU) sys-\ntems by using concatenated n-best ASR alternatives as in-\nput to transformer models, such as BERT XLM-RoBERTa\non DSTC2 dataset [30]. In their paper, Chen et al. [13]\nintroduce a discriminative self-training method that incorpo-\nrates weighted loss and discriminative label smoothing for\nimproving punctuation prediction in ASR output transcripts,\nthe authors utilize extensive unlabeled spoken language data,\nwhich lacks punctuation, such as transcripts employed for\ntraining ASR systems. They employ self-training techniques\nto enhance robust baseline models built on BERT, RoBERTa,\nand ELECTRA.\nF . HuBERT and LightHuBERT models\nIn their study [31], the authors introduce a novel speech pre-\ntraining method called ”HuBERT-AP.” This approach utilizes\npatterns derived from target codes as the training signal to\nfacilitate the model in acquiring improved acoustic features.\nThe patterns, referred to as ”acoustic pieces,” are constructed\nbased on the sentence piece outcomes of the original HuBERT\ntarget codes, and are highly relevant to phonemized natural\nlanguage, making them beneficial for audio-to-text tasks. The\nproposed method is evaluated on the LibriSpeech ASR task,\nand is shown to be significantly more effective than previ-\nous strong baselines. However, the authors in [32] propose\nLightHuBERT, a compressed version of the HuBERT model,\nwhich is a self-supervised speech representation learning\nmodel. LightHuBERT is designed as a once-for-all transformer\ncompression framework. To automatically discover desired\narchitectures through pruning structured parameters, the re-\nsearchers create a transformer-based supernet that encom-\npasses numerous weight-sharing subnets. They also employ a\ntwo-stage distillation strategy to leverage contextualized latent\nrepresentations from HuBERT. Experimental results on ASR\nand the SUPERB benchmark demonstrate that LightHuBERT\nsurpasses HuBERT in ASR tasks while reducing parameters\nby 29%. Furthermore, LightHuBERT achieves a compression\nratio of 3.5 times in three SUPERB tasks, albeit with a slight\nloss in accuracy.\nG. NorBERT and Speech-BERT models\nRugayan et al. [33] propose a robust evaluation metric,\naligned semantic distance (ASD), for Norwegian ASR sys-\ntems. They leverage semantic information modeled by a\ntransformer-based LM and employ dynamic programming\ntechniques to measure the similarity between reference and\nhypothesis text. ASD utilizes NorBERT embeddings to com-\npute the optimal alignment and obtain the minimum global\ndistance. This distance is then normalized by the length of\nthe reference embedding vector. Additionally, the researchers\npresent results using another metric called semantic distance\n(SemDist), and they compare the performance of ASD with\nSemDist. The authors in [34] introduced a neural model called\nspeech-BERT, which combines a bidirectional transformer LM\nwith a neural zero-inflated beta regression approach. This\napproach is specifically designed to be conditioned on speech\nfeatures. To fine-tune speech-BERT, the authors utilized a\npre-training strategy known as token-level masked language\nmodeling. Additionally, they incorporated a zero-inflated layer\ninto the model to effectively handle the mixture of discrete and\ncontinuous outputs.\nH. BERT-based language models\nChang et al. [35] introduce an innovative network called\nthe context-aware transformer transducer (CATT), which en-\nhances the performance of transformer-based ASR systems by\nleveraging contextual signals. The authors propose a context-\nbiasing network based on multi-head attention, which is\ntrained alongside other sub-networks of the ASR system.\nVarious techniques are explored to encode contextual data\nand generate the ultimate attention context vectors. To encode\nthe contextual information and facilitate network training,\nboth BLSTM and pre-trained BERT models are utilized. The\nresearchers in [36] propose two deep neural network (DNN)\nmodels to improve ASR by modeling long-term semantic\nrelations. They employed as input features to their DNN model\ntwo things: (i) dynamic contextual embeddings are derived\nfrom BERT, a transformer-based model specifically designed\nfor acoustic tasks. (ii) Additionally, linguistic features are\nincorporated into the system. Moving forward, the scheme\nproposed in [37] discusses the linguistic diversity in India\nand the need for speech recognition in regional languages.\nThe paper suggests the creation of an advanced ASR system\nbased on deep sequence modeling, aiming to address the\nchallenges posed by low-resource languages. The proposed\nmodel incorporates an enhanced spell corrector component.\nThe performance of the proposed system is assessed using\nmetrics such as WER and sequence match ratio. Notably,\nthe experimental results demonstrate promising outcomes,\nwith an average WER of 0.62. The latter result proves the\nimportance of spell correction in ASR systems and the use of\na transformer-based LM for performance improvement.\nV. CTC- BASED ASR\nCTC is a variant of the transformer architecture that is\nused in seq-2seq learning tasks, particularly in ASR. The CTC\ntransformer combines the concepts of the CTC loss function\nand the transformer architecture, which are both powerful tools\nfor sequence modeling. The CTC loss function is commonly\nused in ASR to align the predicted sequence with the ground\ntruth sequence by taking into account the presence of blank\nsymbols and repeated characters. In the following, a brief\nsummary of the proposed approaches-based CTC transformer.\nTable II presents a summary of the performance achieved\nin the cited papers compared to others systems, including\nthe metrics used to evaluate the results, and the source code\navailability. Figure 2 shows a CTC variation, elucidating its\nintended purpose and objectives.\nA. Mask CTC\nThe proposed method, detailed in [21], consists of a novel\nnon-autoregressive end-to-end ASR called mask CTC. This\nframework generates a sequence by refining the outputs of\nthe CTC model, which is a popular method used for ASR,\nwhile autoregressive models generate one token at a time\nand require as many iterations as the output length. Non-\nautoregressive models offer the advantage of generating tokens\nsimultaneously in a fixed number of iterations, resulting in\nsubstantial reductions in inference time. The mask CTC model\nemploys a training methodology that combines a transformer\nencoder-decoder architecture with simultaneous training of\nmask prediction and CTC during inference. Initially, the target\nsequence is initialized with the greedy CTC outputs. After-\nward, tokens with low confidence are selectively masked using\nthe CTC predictions. By taking into account the conditional\nFig. 2. Purpose and objective of CTC.\ninterdependence among output tokens, the model predicts\nthe masked low-confidence tokens using the high-confidence\ntokens.\nB. NAR CTC\nInaguma et al in [16] propose a faster version of the multi-\ndecoder (MD) end-to-end speech translation model called\nFast-MD. The MD model decomposes the overall speech\ntranslation task into ASR and machine translation sub-tasks,\nbut its decoding speed is not fast enough for real-world\napplications. Fast-MD generates hidden intermediates (HI)\nby NAR decoding based on CTC outputs followed by an\nASR decoder. The scheme employs sampling CTC outputs\nduring training to reduce a mismatch in the ASR decoder.\nThe authors also suggest that adopting the conformer encoder\nand intermediate CTC loss can further boost the model’s\nquality without sacrificing decoding speed. Song et al. [18]\npropose a solution to the accuracy degradation problem faced\nby NAR transformer models in ASR. The proposed solution is\na CTC-enhanced NAR transformer that refines the predictions\nof the CTC module to generate the target sequence. The paper\n[17] presents improvements to the end-to-end CTC alignment-\nbased single-step non-autoregressive transformer (CASS-NAT)\nfor speech recognition. The proposed methods include apply-\ning convolution augmented self-attention blocks to the encoder\nand decoder modules, expanding the trigger mask for each\ntoken to increase CTC alignment robustness, and using iterated\nloss functions to enhance gradient updates. Fujita et al. in\n[23] proposed a method for non-autoregressive ASR streaming\ninput or long recording. They used an insertion-based model\nthat jointly trained CTC and achieved better accuracy with\nfewer iterations using transformer with greedy decoding. The\nauthors suggested combining audio segmentation and non-\nautoregressive ASR into a single neural network. This inte-\ngration leverages the CTC component of the insertion-based\nmodel, utilizing causal self-attention implemented through\nblock self-attention, similar to the transformer XL. Exper-\nimental outcomes demonstrated that the proposed approach\nachieved a favorable trade-off between accuracy and RTF\nwhen compared to both the autoregressive transformer and\nCTC baseline models.\nC. Auxiliary CTC and End-to-end CTC\nThe method introduced by the authors in [19] offers a\nmeans to enhance CTC-based ASR models by loosening the\nassumption of conditional independence in CTC. The method\ninvolves training a CTC-based ASR model with auxiliary CTC\nlosses in intermediate layers. Predictions from these layers are\naccumulated and conditioned on in subsequent layers, resulting\nin improved performance compared to a standard CTC model\nacross multiple ASR corpora. Furthermore, the proposed\nmethod achieves comparable performance to a strong auto-\nregressive model with beam search on the TEDLIUM2 corpus\nand the AISHELL-1 corpus, while being at least 30 times\nfaster in decoding speed. Andrusenko et al. in [38] explore\ndifferent end-to-end ASR systems for the largest open-source\nRussian language data set – OpenSTT. They compare existing\nend-to-end approaches, including joint CTC/Attention, RNN-\ntransducer, and transformer, with a strong hybrid ASR system\nbased on the so-called LF-MMI and TDNN-F acoustic model.\nIs the performance of each system is evaluated on three\navailable validation sets, including phone calls, YouTube, and\nbooks. The paper [39] adapted an end-to-end transformer\nacoustic model to the speech of children learning to read, with\nthe aim of enhancing ASR performance for this challenging\ntask. They used transfer learning with a small amount of child\nspeech and multi-objective training with a CTC function. They\nalso proposed a method of data augmentation for reading\nmistakes, where they simulated word-level repetitions and\nsubstitutions with phonetically or graphically close words.\nThe authors analyzed the performance of their model and\nshowed that both the CTC multi-objective training and the data\naugmentation with synthetic repetitions assisted the attention\nmechanisms better identify children’s disfluencies.\nD. CTC-Based Other Approaches\nIn their work [14], Chen et al. introduce the controllable\ntime-delay transformer (CT-Transformer) model, which ad-\ndresses both punctuation prediction and disfluency detection\ntasks in real-time. These tasks are crucial for enhancing\ntranscript readability and enabling subsequent applications.\nThe CT-Transformer model incorporates a mechanism for\nselectively freezing partial outputs with adjustable time delays\nto meet the real-time constraints imposed by downstream ap-\nplications. Experimental results demonstrate that the proposed\napproach surpasses previous state-of-the-art models in terms\nof F-scores, while also achieving competitive inference speed\non benchmark datasets such as IWSLT2011 [40] and an in-\nhouse Chinese annotated dataset. Moritz et al. in [22] describe\nthe development and implementation of an ”all-in-one” (AIO)\nacoustic model based on the transformer architecture. The AIO\nmodel is designed to simultaneously solve the problems of\nASR audio tagging (AT), and acoustic event detection (AED),\nusing shared parameters across all tasks. The authors argue that\nthis approach more closely mimics the way the human audi-\ntory system processes sound signals from different sources.\nThe integration of the transformer model with CTC enables\nthe enforcement of monotonic ordering and the utilization\nof timing information for both ASR and AED tasks. The\nAIO transformer model consistently outperforms all baseline\nsystems in recent DCASE challenge tasks, showcasing its\naptness for comprehensive transcription of acoustic scenes,\nencompassing speech recognition and identification of other\nacoustic events. Xiao et al. in [41] propose a new framework\nfor an automatic voice query service A VQS to improve the\naccuracy of response for multi-accented Mandarin users. The\nproblem addressed is that many dialect areas in China make\nit necessary for the A VQS to respond to users with a single\nacoustic model in ASR, limiting its accuracy. The proposed\nframework uses a fusion feature comprising i-vector and filter-\nbank acoustic features to train a transformer-CTC, which\nis then used to construct an end-to-end ASR. Additionally,\na keyword-matching algorithm based on fuzzy mathematics\ntheory is proposed to further enhance the accuracy of the\nresponse.\nVI. F UTURE DIRECTIONS\nA. BERT and ChatGPT\n• Improved contextual coherence: By combining ChatGPT\nability to generate human-like responses with BERT strong\ncontextual understanding, the integration enhances the co-\nherence and relevance of the chat responses by leveraging\nBERT knowledge of bidirectional dependencies in text.\n• Enhanced language comprehension: BERT extensive pre-\ntraining on a large corpus enables it to understand lan-\nguage nuances effectively. When integrated with ChatGPT,\nit improves the model’s language understanding capabilities,\nenabling it to comprehend user inputs, handle complex\nqueries, and provide more accurate and context-aware re-\nsponses.\n• Effective handling of ambiguity and multiple meanings:\nChatGPT can sometimes struggle with phrases that have\nmultiple interpretations. By incorporating BERT contextual\nrepresentation, which considers the surrounding context, the\nintegrated model becomes better at disambiguating such\nphrases and generating responses that are more accurate and\nappropriate in context.\nB. CTC and ChatGPT\n• Enhanced language generation: By integrating ChatGPT\ninto CTC, the speech output generated becomes more natu-\nral and engaging due to ChatGPT proficiency in generating\nhuman-like responses.\n1https://github.com/lighthubert ( Accessed: July 2023)\n2https://github.com/kaituoxu/Speech-Transformer (Accessed: July 2023)\n3https://github.com/google-research/bert (Accessed: July 2023)\n4https://github.com/espnet/espnet (Accessed: July 2023)\n5http://www.dev.voxforge.org/AcousticModels (Accessed: July 2023)\n6https://github.com/espnet/espnet (Accessed: July 2023)\nTABLE II\nLIST OF THE SURVEYED STATE -OF-THE -ART STUDIES WITH THEIR ADVANTAGES AND DISADVANTAGES .\nRef. Year Category Compared to Metric Code\navail.?\nResult / Improvement obtained / Comments / Advantages and/or disadvantages\n[8] 2020 MA-BERT BERT-RNN accuracy No 80.34%, improvement of 2.5%, the proposed MA-BERT It is an ideal framework for\nleveraging both acoustic-level and text-level features in the SMCQA task.\n[10] 2021 HPBERT(10)+\nGCN(10)\nHPBERT(10) WER No 16.13%, reduction over 0.14%, the global information captured by the GCN enhances\nthe representation of historical utterances, leading to improved reranking performance.\n[12] 2021 n-Best-ASR\nBERT\nWCN-BERT\nSTC\nF1-scores No 87.80 %, improvement of 1.6%, the N-Best ASR Transformer offers improved perfor-\nmance over baselines, excels in low data regimes, and provides accessibility to users of\nthird-party ASR APIs.\n[13] 2021 RoBERTa-\nwwm-\nbase+Disc-\nST\nRoBERTa-\nwwm-base\nF1-scores Yes 3 60.2%, Discriminative Self-Training improves F1 from 59.6 to 60.2 (+0.6), this approach\nsignificant improvement on punctuation prediction over strong baselines including\nRoBERTa models.\n[15] 2021 STD-BERT Baseline-2 accuracy Yes 2 99.10%, improvement of 0.23%, the experimental results show improved accuracy after\nincorporation of transformer-based knowledge distillation.\n[17] 2021 Improved\nCASS-NAT\nConformer\nAT\nRTF No 0.018, RTF degradation (from 0.081 to 0.018), This suggests that the enhanced CTC\nalignment-based CASS-NAT achieves comparable performance to AT.\n[18] 2021 NAR-\nTransformer\nAR-\nTransformer\nRTF Yes 4 0.0037, results show a Non-autoregressive Trans- former with CTC-enhanced decoder\nachieve 50x faster decoding speed than a strong AR baselin.\n[21] 2020 Mask CTC Non-\nautoregressive\nCTC\nCER Yes 5 4.96%, a reduction over 0.53%, the experimental comparisons demonstrated that Mask\nCTC outperformed the standard CTC model while maintaining the decoding speed fast.\n[22] 2020 AIO Trans-\nformer\nBaseline sys-\ntem\nF1-scores No 51.4%, experiments demonstrate that the AIO Transformer achieves better performance\ncompared to all baseline systems of various re- cent DCASE challenge tasks.\n[23] 2020 KERMIT-\nIntegrated\nCTC\nART-\nIntegrated\nCTC\nRTF No 0.38, RTF degradation (from 1.15 to 0.38), The results indicate that the method suc-\ncessfully achieved a reasonable balance between RTF and performance when compared\nto the baseline autoregressive Transformer and connectionist temporal classification\napproaches.\n[28] 2021 TPBERT PBERT WER No 20.49%, reduction over 0.78%, the advantages of TPBERT lie in its effective use of\nBERT-based ASR N-best hypothesis.\n[32] 2022 LightHuBERT DistilHuBERT PER Yes 1 4.71%, reduction over 11.56%. This demonstrates the superior performance of LightHu-\nBERT compared to DistilHuBERT.\n[36] 2021 BERT alsem\nand GPT-2\nscores\nBaseline sys-\ntem\nWER No 34.4%, reduction of 2.7%, the optimal outcomes are attained by combining rescoring\ntechniques that utilize BERT and GPT-2 scores.\n[38] 2020 TDNN-F-\nLF-MMI\nCTC-\nAttention\nWER Yes 5 33.5%, reduction over 5.4%, The hybrid model continues to outperform end-to-end\nsystems in terms of performance on phone call validation. By incorporating an external\nNNLM for hypotheses rescoring within the hybrid system, a reduction in WER is\nachieved across all validation sets.\n[39] 2021 Transformer\n+CTC\n+Sub+Rep\nTransformer\n(baseline)\nPER No 19.90%, a reduction over 3%, A comprehensive analysis demonstrates that both the\nmulti-objective training with CTC and the augmentation using synthetic repetitions\neffectively enhance the ability of attention mechanisms to detect disfluencies in\nchildren’s speech.\n[41] 2021 Transformer-\nCTC\nBLSTM-\nCTC\nSER No 65.05%, reduction of 13.1%, the proposed framework can effectively improve the whole\nresponse accuracy of A VQS for heavy accented Mandarin speech.\n• Context-aware speech Generation: Incorporating Chat-\nGPT ability to understand contextual cues into CTC enables\nthe model to generate speech that is more coherent and\nrelevant by considering the surrounding context.\n• Versatile text-to-speech applications: CTC is widely used\nin text-to-speech systems. Integrating ChatGPT with CTC\nexpands the capabilities of TTS applications, making them\nmore flexible and adaptable. This allows for interactive\nand dynamic speech generation by leveraging ChatGPT\nconversational capabilities.\n• Enhanced personalization and user interaction: Chat-\nGPT excels in personalized conversations, and when com-\nbined with CTC, it enables the integrated model to generate\nspeech that adapts to user preferences. This results in more\ninteractive and engaging interactions, leading to a highly\npersonalized user experience.\nVII. C ONCLUSION\nTransformers play a crucial role in ASR by capturing\ncontextual information and long-range dependencies. They im-\nprove accuracy by considering the entire context and utilizing\nattention mechanisms to focus on relevant information. Pre-\ntrained models like BERT, RoBERTa, and ELECTRA have\nproven effective in transfer learning for ASR, benefiting from\nknowledge acquired on large-scale datasets. Additionally, the\nCTC method enables end-to-end training, handles variable-\nlength inputs, incorporates label-smoothing regularization,\nintegrates with language models, and supports online and\nstreaming ASR applications. CTC is a flexible and effective\napproach for transcribing speech signals, contributing to robust\nand accurate ASR systems applied to diverse domains, such\nas biomedical [42].\nIn this survey, the idea of incorporating ChatGPT into the\nBERT and CTC frameworks is proposed, opening new av-\nenues for research and development. By leveraging ChatGPT’s\nconversational abilities and natural language understanding,\nit is suggested to enhance BERT and CTC capabilities. This\nintegration aims to improve ASR performance, accuracy, and\ncontextual understanding, leading to advanced speech recog-\nnition applications.\nREFERENCES\n[1] H. Kheddar, M. Bouzid, and D. Meg ´ıas, “Pitch and fourier magnitude\nbased steganography for hiding 2.4 kbps melp bitstream,” IET Signal\nProcessing, vol. 13, no. 3, pp. 396–407, 2019.\n[2] H. Kheddar and D. Meg ´ıas, “High capacity speech steganography for\nthe g723. 1 coder based on quantised line spectral pairs interpolation\nand cnn auto-encoding,” Applied Intelligence, pp. 1–19, 2022.\n[3] H. Kheddar, Y . Himeur, S. Al-Maadeed, A. Amira, and F. Bensaali,\n“Deep transfer learning for automatic speech recognition: Towards better\ngeneralization,” Knowledge-Based Systems, vol. 277, p. 110851, 2023.\n[4] A. A. Francisca, N.-M. Henry, and C. h. Wenyu, “Transformer models\nfor text-based emotion detection: a review of bert-based approaches,”\nArtificial Intelligence Review , vol. 54, p. 5789–5829, 2021.\n[5] G. Benyamin and G. Ali, “Attention mechanism, transformers, bert, and\ngpt: Tutorial and survey,” OSF preprint, 2020.\n[6] K. Salman, N. Muzammal, H. Munawar, W. Z. Syed, and S. K. Fahad,\n“Transformers in vision: A survey,” ACM Computing Surveys , vol. 54,\np. 1–41, 2022.\n[7] L. Siddique, Z. Aun, C. Heriberto, S. Fahad, S. Moazzam, and Q. Ju-\nnaid, “Transformers in speech processing: A survey,” arXiv preprint\narXiv:2303.11607, 2023.\n[8] S.-B. Luo, C.-C. Kuo, and K.-Y . Chen, “Spoken multiple-choice question\nanswering using multi-turn audio-extracter bert,” in 2020 Asia-Pacific\nsignal and information processing association annual summit and con-\nference (APSIPA ASC). IEEE, 2020, pp. 386–392.\n[9] C.-C. Kuo, S.-B. Luo, and K.-Y . Chen, “An audio-enriched bert-\nbased framework for spoken multiple-choice question answering,” arXiv\npreprint arXiv:2005.12142, 2020.\n[10] S.-H. Chiu, T.-H. Lo, F.-A. Chao, and B. Chen, “Cross-utterance rerank-\ning models with bert and graph convolutional networks for conversa-\ntional speech recognition,” in 2021 Asia-Pacific Signal and Information\nProcessing Association Annual Summit and Conference (APSIPA ASC) .\nIEEE, 2021, pp. 1104–1110.\n[11] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, “Attention-based\nmulti-hypothesis fusion for speech summarization,” in 2021 IEEE Auto-\nmatic Speech Recognition and Understanding Workshop (ASRU). IEEE,\n2021, pp. 487–494.\n[12] K. Ganesan, P. Bamdev, A. Venugopal, A. Tushar et al. , “N-best asr\ntransformer: Enhancing slu performance using multiple asr hypotheses,”\narXiv preprint arXiv:2106.06519 , 2021.\n[13] Q. Chen, W. Wang, M. Chen, and Q. Zhang, “Discriminative self-\ntraining for punctuation prediction,” arXiv preprint arXiv:2104.10339 ,\n2021.\n[14] Q. Chen, M. Chen, B. Li, and W. Wang, “Controllable time-delay trans-\nformer for real-time punctuation prediction and disfluency detection,” in\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2020, pp. 8069–8073.\n[15] Y . Jiang, B. Sharma, M. Madhavi, and H. Li, “Knowledge distillation\nfrom bert transformer to speech transformer for intent classification,”\narXiv preprint arXiv:2108.02598 , 2021.\n[16] H. Inaguma, S. Dalmia, B. Yan, and S. Watanabe, “Fast-md: Fast\nmulti-decoder end-to-end speech translation with non-autoregressive\nhidden intermediates,” in 2021 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU). IEEE, 2021, pp. 922–929.\n[17] R. Fan, W. Chu, P. Chang, J. Xiao, and A. Alwan, “An improved single\nstep non-autoregressive transformer for automatic speech recognition,”\narXiv preprint arXiv:2106.09885 , 2021.\n[18] X. Song, Z. Wu, Y . Huang, C. Weng, D. Su, and H. Meng, “Non-\nautoregressive transformer asr with ctc-enhanced decoder input,” in\nICASSP 2021-2021 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 2021, pp. 5894–5898.\n[19] J. Nozaki and T. Komatsu, “Relaxing the conditional independence as-\nsumption of ctc-based asr by conditioning on intermediate predictions,”\narXiv preprint arXiv:2104.02724 , 2021.\n[20] H. Futami, H. Inaguma, S. Ueno, M. Mimura, S. Sakai, and T. Kawahara,\n“Distilling the knowledge of bert for sequence-to-sequence asr,” arXiv\npreprint arXiv:2008.03822, 2020.\n[21] Y . Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi, “Mask\nctc: Non-autoregressive end-to-end asr with ctc and mask predict,” arXiv\npreprint arXiv:2005.08700, 2020.\n[22] N. Moritz, G. Wichern, T. Hori, and J. Le Roux, “All-in-one transformer:\nUnifying speech recognition, audio tagging, and event detection.” in\nINTERSPEECH, 2020, pp. 3112–3116.\n[23] Y . Fujita, T. Wang, S. Watanabe, and M. Omachi, “Toward stream-\ning asr with non-autoregressive insertion-based model,” arXiv preprint\narXiv:2012.10128, 2020.\n[24] H. Kheddar, M. Hemis, Y . Himeur, D. Meg ´ıas, and A. Amira, “Deep\nlearning for steganalysis of diverse data types: A review of methods, tax-\nonomy, challenges and future directions,” Neurocomputing, p. 127528,\n2024.\n[25] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[26] H. Kheddar, “Transformers and large language models for efficient\nintrusion detection systems: A comprehensive survey,” arXiv preprint\narXiv:2408.07583, 2024.\n[27] A. C. Mazari, N. Boudoukhani, and A. Djeffal, “Bert-based ensemble\nlearning for multi-aspect hate speech detection,” Cluster Computing, pp.\n1–15, 2023.\n[28] S.-H. Chiu and B. Chen, “Innovative bert-based reranking language mod-\nels for speech recognition,” in 2021 IEEE Spoken Language Technology\nWorkshop (SLT). IEEE, 2021, pp. 266–271.\n[29] S.-Y . Weng, T.-H. Lo, and B. Chen, “An effective contextual lan-\nguage modeling framework for speech summarization with augmented\nfeatures,” in 2020 28th European Signal Processing Conference (EU-\nSIPCO). IEEE, 2021, pp. 316–320.\n[30] M. Henderson, B. Thomson, and J. D. Williams, “The second dialog\nstate tracking challenge,” in Proceedings of the 15th annual meeting of\nthe special interest group on discourse and dialogue (SIGDIAL) , 2014,\npp. 263–272.\n[31] S. Ren, S. Liu, Y . Wu, L. Zhou, and F. Wei, “Speech pre-training with\nacoustic piece,” arXiv preprint arXiv:2204.03240 , 2022.\n[32] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y . Zhang, T. Ko,\nand H. Li, “Lighthubert: Lightweight and configurable speech repre-\nsentation learning with once-for-all hidden-unit bert,” arXiv preprint\narXiv:2203.15610, 2022.\n[33] J. L. C. Rugayan, T. K. Svendsen, and G. Salvi, “Semantically mean-\ningful metrics for norwegian asr systems,” 2022.\n[34] K. Fan, J. Wang, B. Li, S. Zhang, B. Chen, N. Ge, and Z. Yan, “Neural\nzero-inflated quality estimation model for automatic speech recognition\nsystem,” arXiv preprint arXiv:1910.01289 , 2019.\n[35] F.-J. Chang, J. Liu, M. Radfar, A. Mouchtaris, M. Omologo, A. Rastrow,\nand S. Kunzmann, “Context-aware transformer transducer for speech\nrecognition,” in 2021 IEEE Automatic Speech Recognition and Under-\nstanding Workshop (ASRU). IEEE, 2021, pp. 503–510.\n[36] D. Fohr and I. Illina, “Bert-based semantic model for rescoring n-best\nspeech recognition list,” in INTERSPEECH 2021, 2021.\n[37] M. S. Priya, D. K. Renuka, L. A. Kumar, and S. L. Rose, “Multilingual\nlow resource indian language speech recognition and spell correction\nusing indic bert,” S¯adhan¯a, vol. 47, no. 4, p. 227, 2022.\n[38] A. Andrusenko, A. Laptev, and I. Medennikov, “Exploration of end-\nto-end asr for openstt–russian open speech-to-text dataset,” in Speech\nand Computer: 22nd International Conference, SPECOM 2020, St.\nPetersburg, Russia, October 7–9, 2020, Proceedings 22. Springer, 2020,\npp. 35–44.\n[39] L. Gelin, T. Pellegrini, J. Pinquier, and M. Daniel, “Simulating reading\nmistakes for child speech transformer-based phone recognition,” in An-\nnual Conference of the International Speech Communication Association\n(INTERSPEECH), 2021.\n[40] X. Che, C. Wang, H. Yang, and C. Meinel, “Punctuation prediction for\nunsegmented transcript based on word vector,” in Proceedings of the\nTenth International Conference on Language Resources and Evaluation\n(LREC’16), 2016, pp. 654–658.\n[41] K. Xiao and Z. Qian, “Automatic voice query service for multi-accented\nmandarin speech,” in 2021 IEEE International Conference on Systems,\nMan, and Cybernetics (SMC) . IEEE, 2021, pp. 2875–2881.\n[42] B. Essaid, H. Kheddar, N. Batel, M. E. Chowdhury, and A. Lakas,\n“Artificial intelligence for cochlear implants: Review of strategies,\nchallenges, and perspectives,” IEEE Access, 2024.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7747797966003418
    },
    {
      "name": "Computer science",
      "score": 0.7618145942687988
    },
    {
      "name": "Connectionism",
      "score": 0.5157616138458252
    },
    {
      "name": "Speech recognition",
      "score": 0.4907782971858978
    },
    {
      "name": "Encoder",
      "score": 0.4832814037799835
    },
    {
      "name": "Architecture",
      "score": 0.4773297905921936
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37370529770851135
    },
    {
      "name": "Artificial neural network",
      "score": 0.29984909296035767
    },
    {
      "name": "Engineering",
      "score": 0.2113839089870453
    },
    {
      "name": "Electrical engineering",
      "score": 0.11421722173690796
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I192268740",
      "name": "University of Algiers Benyoucef Benkhedda",
      "country": "DZ"
    },
    {
      "id": "https://openalex.org/I157169809",
      "name": "University of Sciences and Technology Houari Boumediene",
      "country": "DZ"
    },
    {
      "id": "https://openalex.org/I4210128965",
      "name": "University Yahia Fares of Medea",
      "country": "DZ"
    },
    {
      "id": "https://openalex.org/I132806614",
      "name": "University of Dubai",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I1323252656",
      "name": "Information Technology University",
      "country": "PK"
    }
  ],
  "cited_by": 22
}