{
  "title": "Transformer and Graph Convolutional Network for Text Classification",
  "url": "https://openalex.org/W4387341818",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5052310358",
      "name": "Boting Liu",
      "affiliations": [
        "Guangxi University"
      ]
    },
    {
      "id": "https://openalex.org/A5075938343",
      "name": "Weili Guan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5103283888",
      "name": "Changjin Yang",
      "affiliations": [
        "Guangxi University"
      ]
    },
    {
      "id": "https://openalex.org/A5053595088",
      "name": "Zhijie Fang",
      "affiliations": [
        "Guangxi University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101252253",
      "name": "Zhiheng Lu",
      "affiliations": [
        "Guangxi University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2937423263",
    "https://openalex.org/W2802787326",
    "https://openalex.org/W2505990019",
    "https://openalex.org/W1995258314",
    "https://openalex.org/W3028860567",
    "https://openalex.org/W2617241599",
    "https://openalex.org/W2780777331",
    "https://openalex.org/W6600299915",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4205659634",
    "https://openalex.org/W2970183009",
    "https://openalex.org/W2997162759",
    "https://openalex.org/W4285104173",
    "https://openalex.org/W4230097545",
    "https://openalex.org/W4285104215",
    "https://openalex.org/W4239025696",
    "https://openalex.org/W4210384885",
    "https://openalex.org/W3089862520",
    "https://openalex.org/W3086807592",
    "https://openalex.org/W3173753074",
    "https://openalex.org/W4312410594",
    "https://openalex.org/W6631239350",
    "https://openalex.org/W6827496332",
    "https://openalex.org/W3205012620",
    "https://openalex.org/W2985331920",
    "https://openalex.org/W3125294517",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1521626219",
    "https://openalex.org/W3105625590"
  ],
  "abstract": "Abstract Graph convolutional network (GCN) is an effective tool for feature clustering. However, in the text classification task, the traditional TextGCN (GCN for Text Classification) ignores the context word order of the text. In addition, TextGCN constructs the text graph only according to the context relationship, so it is difficult for the word nodes to learn an effective semantic representation. Based on this, this paper proposes a text classification method that combines Transformer and GCN. To improve the semantic accuracy of word node features, we add a part of speech (POS) to the word-document graph and build edges between words based on POS. In the layer-to-layer of GCN, the Transformer is used to extract the contextual and sequential information of the text. We conducted the experiment on five representative datasets. The results show that our method can effectively improve the accuracy of text classification and is better than the comparison method.",
  "full_text": "International Journal of Computational Intelligence Systems          (2023) 16:161 \nhttps://doi.org/10.1007/s44196-023-00337-z\nRESEARCH ARTICLE\nTransformer and Graph Convolutional Network for Text Classiﬁcation\nBoting Liu 1 · Weili Guan 2 · Changjin Yang 1 · Zhijie Fang 3 · Zhiheng Lu 4\nReceived: 27 February 2023 / Accepted: 8 September 2023\n© The Author(s) 2023\nAbstract\nGraph convolutional network (GCN) is an effective tool for feature clustering. However, in the text classiﬁcation task, the\ntraditional TextGCN (GCN for Text Classiﬁcation) ignores the context word order of the text. In addition, TextGCN constructs\nthe text graph only according to the context relationship, so it is difﬁcult for the word nodes to learn an effective semantic\nrepresentation. Based on this, this paper proposes a text classiﬁcation method that combines Transformer and GCN. To\nimprove the semantic accuracy of word node features, we add a part of speech (POS) to the word-document graph and build\nedges between words based on POS. In the layer-to-layer of GCN, the Transformer is used to extract the contextual and\nsequential information of the text. We conducted the experiment on ﬁve representative datasets. The results show that our\nmethod can effectively improve the accuracy of text classiﬁcation and is better than the comparison method.\nKeywords Graph convolutional network · Text classiﬁcation · Part of speech · Transformer\n1 Introduction\nText classiﬁcation plays a pivotal role in natural language\nprocessing (NLP) [ 1]. It involves the automated categoriza-\ntion of text through computer technology, ﬁnding extensive\napplication in sentiment analysis, document classiﬁcation,\nand public opinion analysis, among other domains. Elevat-\ning the precision of text classiﬁcation tasks holds the key to\nresolving pertinent real-world issues and enhancing the over-\nB Weili Guan\n2113391032@st.gxu.edu.cn\nB Zhiheng Lu\n8812316@163.com\nBoting Liu\n971328422@qq.com\nChangjin Yang\n503141588@qq.com\nZhijie Fang\nnnfang@163.com\n1 School of Computer, Electronics and Information, Guangxi\nUniversity, Nanning 530004, Guangxi, China\n2 College of Digital Economics, Nanning University, Nanning\n530299, Guangxi, China\n3 College of Electrical Engineering, Guangxi University of\nScience and Technology, Liuzhou 545006, Guangxi, China\n4 School of Mechanical Engineering, Guangxi University,\nNanning 530004, Guangxi, China\nall quality of life. Hence, the essence of this paper’s research\nlies in reﬁning existing text classiﬁcation techniques, focus-\ning on improving the accuracy of text classiﬁcation.\nExisting text classiﬁcation technologies are mainly based\non machine learning and deep learning methods [ 2]. Com-\nmon machine learning methods in text classiﬁcation tasks\ninclude Support V ector Machine (SVM) [ 3], K-Nearest\nNeighbor (KNN) [ 4], and Random Forest (RF) [ 5], which\nhave achieved excellent performance in simple classiﬁcation\ntasks [ 5]. However, machine learning methods based on sta-\ntistical techniques have difﬁculty in achieving the desired\nperformance on complex tasks in real life, such as medical\ntext diagnosis and sentiment analysis. With the breakthrough\ndevelopment of word vector technology in deep learning,\nwords are given contextual semantics in the form of vec-\ntors [ 6]. Deep learning methods based on word vectors have\nachieved excellent NLP results and gradually become known\nas the mainstream method in the ﬁeld of NLP [ 1]. In the\nﬁeld of text classiﬁcation, the most commonly used deep\nlearning methods are Convolutional Neural Network (CNN)\n[7], Recurrent Neural Network (RNN) [ 8], transformer [ 9],\nand Graph Convolutional Network (GCN) [ 10]. Due to the\nlimitation of CNN convolutional kernel size, CNN focuses\nmore on extracting local feature information of text. RNN\ntakes into account the role of each token in text, so RNN\nfocuses more on extracting global feature information of\ntext, but there is a risk of gradient disappearance in RNN.\n0123456789().: V,-vol 123\n  161 Page 2 of 11 International Journal of Computational Intelligence Systems           (2023) 16:161 \nTransformer is a powerful feature selection tool that com-\nbines attention mechanisms to obtain stronger contextual\nrelationships for text, and is an innovative technique in NLP .\nTextGCN achieves text classiﬁcation by constructing a word-\ndocument graph structure, and GCN focuses more on the\nspatial feature information of the text. Also, in the study [ 10],\nGCN achieves better text classiﬁcation accuracy than CNN\nand RNN. Therefore, we believe that GCN has great potential\nfor text classiﬁcation tasks. We will combine Transformer to\nimprove GCN to obtain higher performance for text classiﬁ-\ncation.\nNodes in the GCN simultaneously assimilate informa-\ntion from their neighboring nodes. However, this approach\nimplies that in a text classiﬁcation task, document nodes\nconsider all words within the document simultaneously,\ndisregarding the text’s sequential order. V aried sentence\nstructures convey nuanced meanings, underscoring the sig-\nniﬁcance of preserving text order. Consequently, we posit\nthat enhancing GCN’s efﬁcacy in text classiﬁcation necessi-\ntates imbibing knowledge about text sequences. Moreover,\nthe scope of semantics attainable solely through contextual\nrelationships in word-document graphs is inherently limited.\nBuilding upon this premise, our paper introduces a novel\ntext classiﬁcation approach that amalgamates Transformer\nand GCN. This fusion capitalizes on the strengths of both\nmodels. The principal contributions of our study encompass\nthe following aspects:\n• To tackle the issue of GCN overlooking textual order,\nwe seamlessly integrate the Transformer into the graph\nconvolutional layers, forming what we refer to as a\nGraph Convolution Layer-Transformer-Graph Convo-\nlution Layer (GTG). The Transformer enhances the\ncontextualization of textual information, considering the\ncrucial textual order aspect. The resultant Transformer\noutput is amalgamated with GCN to yield a more precise\nsemantic representation of document nodes.\n• To address the issue of limited semantic information\nin word node vectors within GCN, we suggest con-\nstructing word-document graphs based on POS tagging.\nThis approach imbues words with POS-related seman-\ntics, thereby enhancing the overall semantic quality of\nword node vectors.\n2 Related Work\n2.1 TextGCN\nIn the early days, GCN was mainly applied to tasks with\nobvious spatial structure, such as social networks and knowl-\nedge graphs. In 2019, Yao et al. [ 10] applied GCN to text\nclassiﬁcation tasks for the ﬁrst time and achieved good doc-\nument classiﬁcation performance. TextGCN, a model based\non semi-supervised learning, has enhanced the training dif-\nﬁculty to a certain extent. This enables TextGCN to achieve\ngood ﬁtting performance through only two layers of graph\nconvolution. Moreover, in TextGCN, documents and words\nform a heterogeneous graph structure, allowing for the learn-\ning of information at the word and document levels. Since\nthen, more and more researchers have applied GCN to text\nclassiﬁcation [ 11].\n2.2 Recent Works\nThe proposal [ 12] proposes a document-level GCN-based\ntext classiﬁcation method. Unlike TextGCN, the proposal\n[12] constructs each document as a separate graph. The\ncomputational cost of GCN is optimized to achieve bet-\nter classiﬁcation performance than TextGCN and to support\nonline classiﬁcation of documents. A text classiﬁcation\nmethod based on text graph tensor is proposed in the proposal\n[13]. The proposal [ 13] uses three different compositions,\nsemantic, syntactic, and sequential, to coordinate the infor-\nmation between different types of graphs and achieve a\nbetter classiﬁcation performance than TextGCN. A text clas-\nsiﬁcation method based on GCN with Bidirectional Long\nShort-Term Memory (BiLSTM) is proposed in the proposal\n[14], which is called IMGCN. The proposal [ 14] used Word-\nnet [ 15] with syntactic dependency composition method\nand used BERT to get the embedding representation of\nword nodes. Bidirectional LSTM with Attention was used\nto further extract the contextual relationship of the text and\ncombined with residual concatenation to get the classiﬁcation\nresults. A text classiﬁcation method combining BiGRU and\nGCN is proposed in the proposal [ 16]. The word embedding\nrepresentation is obtained by Word2vec [ 17], the contextual\ninformation of the text is extracted by Bidirectional Gating\nRecurrent Unit (BiGRU) [ 18], and the spatial information of\nthe text is extracted by the input GCN. A short text classiﬁ-\ncation method based on GCN and BERT [ 19] was proposed\nin the proposal [ 20]. A word-document-topic graph struc-\nture was constructed using Biterm Topic Model (BTM) [ 21]\nto obtain the topics of documents. The word node features\nafter GCN iteration are fused with the word features out-\nput from BERT and input to BiLSTM. BiLSTM will extract\nthe contextual semantics of the text and ﬁnally fuse with the\ndocument node features to get the classiﬁcation results. The\nproposal [ 22] proposed a text classiﬁcation model based on\nBERT with GCN. They initialize the node vector of GCN by\nBERT and jointly train GCN and BERT to fully utilize the\nadvantages of each model. A GCN text classiﬁcation method\nbased on inductive graphs was proposed in the proposal\n[23]. The original dataset was statistically summarized into\nsmall graphs, and good classiﬁcation results were obtained\n123\nInternational Journal of Computational Intelligence Systems           (2023) 16:161 Page 3 of 11   161 \nTable 1 Comparison between related work\nMethod Year Highlights Limitations\nTextGCN [10] 2019 The concept of\nword-document graph\nwas proposed\nThe position information of\nthe word nodes was\nignored\nText-level GCN [ 12] 2019 Text-level graphs was\nconstructed to optimize\nthe computational cost\nThe position information of\nthe word nodes was\nignored\nTensor GCN [ 13] 2020 A multi-angle building\ngraph idea was proposed\nThe position information of\nthe word nodes was\nignored\nBiLSTM+GCN [ 20] 2020 The topic of the text was\nobtained through BTM\nand the text topic was\nused as a graph node\nThe position information of\nthe word nodes was\nignored\nBERT+GCN [22] 2021 Initialization of word node\nembedding representation\nby BERT\nThe position information of\nthe word nodes was\nignored\nIMGCN [ 14] 2022 A graph structure of\ndependency and semantic\ndictionary is introduced\nby wordnet\nThe position information of\nthe word nodes was\nignored\nBiGRU+GCN [16] 2022 A hybrid structure of\nBiGRU and GCN is\nproposed\nThe position information of\nthe word nodes was\nignored\nbased on the small graphs alone. In Table 1, we have brieﬂy\ndescribed the highlights and limitations of related work.\nAll of the aforementioned studies have built upon the\nfoundation laid by TextGCN [ 10]. They have integrated addi-\ntional networks or utilized diverse conﬁgurations as their\nprimary focus, aligning closely with the direction of this\nresearch. Nevertheless, as indicated in Table 1, none of these\napproaches appear to have addressed the issue of textual\nordering. Therefore, this paper aims to rectify the limitations\nof GCN concerning the aspect of text sequence.\n3 Transformer and Graph Convolutional\nNetwork for Text Classiﬁcation\n3.1 Method Structure\nThe text classiﬁcation method based on Transformer and\nGCN including data pre-processing and GTG, and the model\nstructure is shown in Fig. 1.\nData pre-processing The initial step involves eliminat-\ning irrelevant words from the dataset, such as adverbs and\nadjectives, by referencing the list of stop words. In alignment\nwith TextGCN [10], the identical stop words list is employed.\nSubsequently, the construction of the word-document global\nco-occurrence graph is rooted in contextual relationships.\nFurther elaboration on this process can be found in Sect. 3.2.\nFig. 1 The text classiﬁcation structure based on Transformer and GCN.\nIn this ﬁgure, the term “doc” denotes a document, while “text” refers\nto the textual content within the dataset 123\n  161 Page 4 of 11 International Journal of Computational Intelligence Systems           (2023) 16:161 \nFig. 2 Building graph based on POS. As shown in this ﬁgure, words of the same POS nature are linked by edges, allowing the words to obtain a\nPOS-based semantic representation\nFig. 3 Building graph based on context. Compute the relationships between words in the sliding window, so that the word nodes have context-based\nsemantic representations\nGTG After constructing a word-document graph, the\ngraph node features undergo initial updates following the\napplication of the ﬁrst graph convolution layer (GCL). Sub-\nsequently, the word nodes are input into the Transformer to\nextract contextual semantics, along with the text’s seman-\ntic order information. Ultimately, the Transformer’s output\nis integrated with the document nodes to augment features,\nforming the input for the second GCL.\n3.2 Data Preprocessing\nOur data pre-processing methodology closely follows the\napproach outlined in [ 10], with a modiﬁcation in the struc-\nture to enable the acquisition of POS-related information by\nwords.\nTo begin, we segment the words within each document.\nSubsequently, we employ the Natural Language Toolkit\n(NLTK) [ 24] for POS tagging of the words. Upon analyz-\ning each document, we establish connections between words\nsharing the same POS tag. All connections between words of\nidentical POS nature are assigned equal signiﬁcance, result-\ning in an edge weight of 1. A detailed illustration of this\nprocessing procedure is presented in Fig. 2.\nSubsequently, we will establish word relationships grounded\nin context. Each document is scanned using a window of\nlength 20, and we capture the frequency of occurrences for\nindividual words within this window. Additionally, we tally\nthe frequency of adjacent word pairs appearing within the\nsame window. The detailed processing procedure is illus-\ntrated in Fig. 3.\nAfter the processing illustrated in Fig. 3, we have success-\nfully derived the word-to-word relationships. Subsequently,\nwe proceed to establish word-to-word edges based on con-\ntextual information. The assignment of weights to these\nword-to-word edges is determined following Eqs. ( 1), ( 2),\nand ( 3)\np(i ) = N\ni\nNw\n(1)\np(i , j ) = Nij\nNw\n(2)\nPMI (i , j ) = p(i , j )\np(i )p(j ). (3)\nIn Eqs. ( 1), ( 2), and ( 3), Nw represents the total num-\nber of sliding windows, Ni corresponds to the frequency of\noccurrence of term i across all sliding windows, and Nij indi-\ncates the co-occurrence frequency of terms i and j within the\nsame sliding windows. The Pointwise Mutual Information\n(PMI) [ 25] is employed to quantify the relationship between\nthese two terms, with higher PMI values indicating a stronger\nassociation between them. Therefore, a PMI greater than 0\nsigniﬁes a substantial correlation between the two words,\nleading to the establishment of edges with assigned weights.\nNext, we establish connections between documents and\nwords, treating each document as a node within the graph.\nDocument nodes form connections with the words present\nin the respective documents. The weights assigned to these\n123\nInternational Journal of Computational Intelligence Systems           (2023) 16:161 Page 5 of 11   161 \nFig. 4 Word-document graph\nb a s e do nP O Sa n dc o n t e x t .\nUsing consistent colors to\nrepresent identical parts of\nspeech, with D symbolizing the\ndocument node\nconnections between documents and words are determined\nusing Term Frequency-Inverse Document Frequency (TF-\nIDF) [ 26], with the corresponding formula presented in Eqs.\n(4), ( 5), and ( 6)\nTF = M\ni\nMd\n(4)\nIDF = log\n( MD\nMid\n)\n(5)\nTF − IDF = TF × IDF. (6)\nIn the equations provided above, Mi represents the frequency\nof occurrence of term i within the current document, Md\nsigniﬁes the total word count in the current document, MD\ncorresponds to the total number of documents, and Mid\nstands for the count of documents containing the term i .\nTF-IDF serves as a fundamental metric to assess word signif-\nicance in the context of document classiﬁcation, with higher\nTF-IDF values indicating greater word importance within\ndocuments. Following these initial steps, we constructed the\nword-document graph, and the comprehensive graph struc-\nture is visually depicted in Fig. 4.\nAt this point, our data pre-processing is complete, and the\nnext step is to process the word-document graph via the GTG\nnetwork, as detailed in the next section.\n3.3 GTG\nWe integrated the Transformer between the GCN layers with\nthe aim of not only extracting deeper contextual semantics\nfrom the word nodes but also capturing the semantic ordering\ninformation within the text. The speciﬁcs of this approach are\nillustrated in Fig. 5.\nIn Fig. 5, the output of the Transformer is fused with the\ndocument node vector, which is shown in Eq. ( 7)\nOut\ndoc = (OutTransformer + Out1st−doc)/2. (7)\nIn the above equation, we fuse the output of the Trans-\nformer with the document vector of the ﬁrst Graph Convo-\nlution Layer in a summation-averaging manner. We use a\nsmoother Mish [ 27] to make Out\nTransformer and Out 1st−doc\nblend better. The Mish function is deﬁned as Eq. ( 8)\nMish = x ∗ tan h(ln(1 + ex )). (8)\nIn Eq. (8), where x signiﬁes the input features, tanh denotes\nthe hyperbolic tangent function, and ln stands for the natural\nlogarithm. Subsequently, we substitute the initial document\nfeatures with Out\ndoc, followed by further convolving the\nreﬁned graph using the second Graph Convolutional Layer\nto attain the classiﬁcation outcome.\n3.3.1 Transformer Encoder\nTransformer is a powerful feature selection tool that incor-\nporates an attention mechanism to give words context-based\nattention scores [ 28]. Transformer is based on an encoder to\ndecoder structure; in this paper, we use only the encoder of\nTransformer. The Transformer encoder structure is shown in\nFig. 6.\nIn Fig. 6, the inclusion of position embedding introduces\npositional information to individual tokens, thus enabling the\n123\n  161 Page 6 of 11 International Journal of Computational Intelligence Systems           (2023) 16:161 \nFig. 5 The GTG structure\nFig. 6 The Transformer encoder. It includes position embedding,\nmulti-headed attention, residual connectivity, normalization, and feed-\nforward networks\nTransformer to consider the sequential order of tokens during\ntraining. The implementation of position embedding in the\nTransformer relies on trigonometric functions, as illustrated\nin Eqs. ( 9) and ( 10).\nThe periodic nature of trigonometric functions effectively\ncaptures the relative positions of words within a textual\nsequence. Additionally, the application of trigonometric\nformulas allows for the efﬁcient calculation of positional\ninformation in a concise manner. Due to their representation\nas high-dimensional vectors, trigonometric functions align\nwell with matrix multiplication operations in both the Trans-\nformer and GCN, enhancing overall efﬁciency\nPE(pos,2i ) = sin(pos/1000\n2i /dmodel ) (9)\nPE(pos,2i + 1) = cos(pos/10002i /dmodel ). (10)\nIn the equations provided above, pos represents the index\nvalue of the word’s position within the original document,\ndmodel stands for the model’s dimensionality, and i corre-\nsponds to the positional embedding index. When two words\nexhibit strong trigonometric similarity, they are regarded as\nbeing in proximity within the sentence. Figure 7 visually\ndepicts the fusion of the phrase “Natural language processing\nis an art” with positional embedding information.\nIn Fig. 7, “pe” denotes the positional embedding infor-\nmation. Positional embedding is a vector that aligns with the\ndimensionality of the word nodes, obtained from Eqs. ( 9) and\n(10). Figure 8 illustrates the attention heatmap of the phrase\n\"Natural language processing is an art\" with the inclusion of\npositional embedding.\nIn Fig. 8, it is evident that word nodes in close proxim-\nity acquire higher attention scores following multiplication.\nThis observation highlights that the incorporation of position\nembedding imbues the word nodes with valuable positional\ninformation. In multi-headed attention, the Transformer’s\ninput is mapped into several Scaled Dot-Product Attention\nnetworks. The equation of Scaled Dot-Product Attention is\nEq. ( 11)\nAttention(Q, K , V ) = Softmax\n( QK\nT\n√dk\n)\nV . (11)\nIn Eq. ( 11), Softmax is the normalization function, Q rep-\nresents the query vector, K is the queried vector, V is the\ncontent vector, and d is the vector dimension. Here, Q, K, and\nV are text sequence vectors composed of word nodes that are\nmultiplied by different parameter matrices. Therefore, what\nis being calculated here is the self-attention between words\nin the same context. Subsequent to the attention calculation,\nthe output from each head is amalgamated to form the out-\nput of the multi-head attention. This multi-headed attention\nmechanism captures attention distributions from multiple\nperspectives, yielding superior outcomes compared to the\n123\nInternational Journal of Computational Intelligence Systems           (2023) 16:161 Page 7 of 11   161 \nFig. 7 An example of a word node with position embedding information\nsingular attention approach. In this research, we employ a\nlayer of the Transformer encoder. We concatenate the output\ntokens from each Transformer and standardize their dimen-\nsions before aligning them with the document node through\na linear layer. The precise formulations are illustrated in Eqs.\n(12) and ( 13)\nOut\nTransformer = Concat(token1,..., tokenn ) (12)\nOutTransformer = Linear(OutTransformer). (13)\nIn the above formulas, Concat is the concatenation func-\ntion and token is the word vector.\n3.3.2 Graph Convolutional Network\nGCN can be seamlessly employed to analyze graph data\nstructures, effectively capturing spatial relationships among\nnodes and facilitating node classiﬁcation [ 29]. Over the past\nyears, the potential of GCN in text classiﬁcation has garnered\nincreasing attention from researchers, leading to its growing\nadoption in various text classiﬁcation tasks.\nTo facilitate efﬁcient computations, GCN employs matrix\nmultiplication for all its operations, thereby representing and\nprocessing graph structures as adjacency matrices. In the\ninitial graph convolutional layer (GCL), node updates are\ndetermined by the Eqs. ( 14) and ( 15)\nˆA = D\n1/2 AD −1/2 (14)\nL(1) = ρ( ˆAXW o), (15)\nwhere A is the adjacency matrix of the graph, ρ is the Relu\nfunction, D is the degree matrix of A, X is the node feature,\nand Wo is the weight matrix.\nAfter the initial GCL update, each node effectively assim-\nilates information from its neighboring nodes, resulting in\nnodes possessing speciﬁc spatial characteristics and exhibit-\ning a clustering effect. Subsequently, the textual nodes from\nthe initial layer are inputted into the Transformer to addition-\nally extract contextual and sequential textual information.\nThe ensuing step involves dimensionality reduction through\nthe second GCL, yielding the ultimate classiﬁcation out-\nFig. 8 The attention heat map of “Natural language processing is an\nart”. The coordinate axes represent the word nodes, and the depth of\nthe matrix square color is positively correlated with the attention score\nof the word nodes\ncomes, as illustrated in Eq. ( 16)\nL(2) = ρ( ˆAL (1)Wo). (16)\nIn Eq. ( 16), the feature input is reﬁned as L(1). Subse-\nquently, an additional convolution operation is applied to L(1)\nto extract more intricate word-document spatial information.\nThis reﬁnement aims to amplify the clustering impact of the\ndocument nodes, ultimately contributing to the accomplish-\nment of the classiﬁcation task. Consistent with the approach\nproposed in [ 10], we maintain a node dimension of 300 in\nthis study.\n4 Experimental Results\nIn this section, we will experimentally verify the effective-\nness and superiority of the method in this paper.\n123\n  161 Page 8 of 11 International Journal of Computational Intelligence Systems           (2023) 16:161 \nTable 2 The datasets’ information\nDataset Docs Training Test Classes Average length\nMRa 10,662 7108 3554 2 20\nR8b 7674 5485 2189 8 65\n20NGc 18,846 11,314 7532 20 221\nR52d 9100 6532 2568 52 69\nOhsumede 7400 3357 4043 8 135\naMR is a sentiment analysis dataset of movie reviews, which contains\npositive and negative categories\nbR8 is an 8-category news topic dataset\nc20NG is an 20-category news topic dataset\ndR52 is an 52-category news topic dataset\neOhsumed is a text classiﬁcation dataset in medicine, containing 23\ncategories\n4.1 Experimental Datasets\nWe selected R8, 20ng, MR, R52, and Ohsumed as experi-\nmental data sets, which are representative in this ﬁeld. The\ninformation about the datasets is shown in Table 2.\nAs depicted in Table 2, these datasets encompass diverse\ndomains including long text, short text, and sentiment anal-\nysis. These domains collectively provide a comprehensive\nrepresentation of the text classiﬁcation ﬁeld. Given that the\nTransformer necessitates text inputs of ﬁxed length, we adjust\neach document by either truncating or padding it based on\nthe average length of the dataset.\n4.2 Experimental Evaluation Index\nAccuracy and f1 are used as experimental evaluation indica-\ntors. The calculation methods of accuracy and f1 are shown\nin Eqs. ( 17), ( 18), ( 19), and ( 20)\nAccuracy = TP + TN\nTP + TN + FP + FN (17)\nPrecision = TP\nTP + FP (18)\nRecall = TP\nTP + FN (19)\nF 1 = 2 ∗ Precision ∗ Recall\nPrecision + Recall . (20)\nTrue Positives (TP) is the number of positive classes pre-\ndicted; False Positives (FP) is the number of negative classes\npredicted to be positive classes; True Negatives (TN) is the\nnumber of negative classes predicted; False Negatives (FN)\nrefers to the number of positive classes predicted to be neg-\native [30].\n4.3 Experimental Setting\nFollowing the approach outlined in the proposal [ 10], we opt\nfor a random selection of 10% of the documents from the\ntraining dataset to form the validation set. Our training pro-\ncess spans 200 epochs, employing a learning rate of 0.02,\nuntil the validation loss demonstrates no improvement for a\nspan of ten epochs. A dropout rate of 0.5 is applied, accom-\npanied by the utilization of the ReLU activation function.\n4.4 Baselines\nIn this section, we will brieﬂy introduce the baseline models\nof this paper.\nMachine learning Machine learning-based methods have\nbeen widely used in the ﬁeld of text classiﬁcation. We choose\nSVM, KNN, and RF as machine learning methods, and their\ntext features are initialized by TF-IDF.\nDeep learning We choose BiLSTM, BiGRU, CNN, Trans-\nformer, and FastText [ 31] as the deep learning methods. The\nlast output of BiLSTM and BiGRU is used as the classiﬁca-\ntion token, and the classiﬁcation token is fed into the linear\nlayer to get the prediction. The CNN uses the version in\nTextCNN with convolution kernel of (2, 3, 4). In Transformer,\nall the output tokens are stitched to the same latitude and the\nprediction is obtained by a linear layer. The word vectors of\nRNN, Transformer, and CNN methods are initialized by the\npre-trained GloV e [ 32]. In addition, FastText classiﬁcation\nis performed by summing and averaging the word vectors\nobtained from training and obtaining predictions through a\nlinear layer.\nRecent related works We choose TextGCN [ 10], BiGRU+\nGCN [16], and BiLSTM+GCN [ 20] as the comparison meth-\nods. To compare the structural advantages and disadvantages\nof each method, we uniformly initialize the node features\nwith one-hot.\n4.5 Results\nIn this section, we present the pertinent experimental ﬁnd-\nings along with a concise analysis of these results. The test\naccuracies of each approach are displayed in Table 3.F o r\nthe document classiﬁcation task in this study, we evalu-\nated test accuracy and F1 score through ten iterations across\nall models. The outcomes were reported as the mean value\naccompanied by the standard deviation. In Tables 3 and 4,\nthe bolded results proved to be signiﬁcantly better than the\nother methods in this dataset by t-test.\nAs indicated in Table 3, the proposed approach demon-\nstrates optimal classiﬁcation performance across three datasets.\nSpeciﬁcally, the proposed method achieves a classiﬁcation\naccuracy of 86.96% on 20NG, 94.46% accuracy on R52, and\n69.72% accuracy on Ohsumed. In comparison, the proposed\n123\nInternational Journal of Computational Intelligence Systems           (2023) 16:161 Page 9 of 11   161 \nTable 3 The test accuracy (%)\nMethod 20NG R8 R52 Ohsumed MR\nSVM 83.54 ± 1.66 96.71 ± 0.12 93.07 ± 0.59 63.00 ± 1.26 75.44 ± 0.32\nKNN 67.78 ± 1.32 88.03 ± 0.86 85.44 ± 0.52 56.32 ± 1.39 70.15 ± 0.22\nRF 77.54 ± 2.56 94.88 ± 1.53 87.58 ± 1.16 58.32 ± 1.32 69.41 ± 0.25\nBiLSTM 73.20 ± 0.56 96.35 ± 1.32 90.39 ± 0.69 49.56 ± 1.22 77.53 ± 0.29\nBiGRU 73.61 ± 0.36 96.55 ± 1.12 91.12 ± 0.62 49.11 ± 1.19 76.95 ± 0.33\nCNN 82.25 ± 0.28 95.61 ± 0.77 87.56 ± 0.86 58.64 ± 1.02 77.62 ± 0.66\nTransformer 74.26 ± 0.86 96.47 ± 1.32 92.12 ± 1.12 52.31 ± 0.94 76.56 ± 0.65\nFastText 79.52 ± 0.46 94.59 ± 0.88 90.86 ± 0.34 55.61 ± 0.36 76.31 ± 0.52\nTextGCN [10] 86.26 ± 0.16 96.80 ± 0.13 93.61 ± 0.14 68.32 ± 0.29 76.00 ± 0.48\nTextGCN(POS) 86.38 ± 0.11 97.02 ± 0.12 93.54 ± 0.16 68.47 ± 0.42 76.53 ± 0.44\nBiGRU+GCN [16] 86.77 ± 0.14 97.06 ± 0.13 93.88 ± 0.18 68.44 ± 0.33 77.56 ± 0.46\nBiLSTM+GCN [ 20] 86.55 ± 0.12 97.38 ± 0.16 94.20 ± 0.18 69.15 ± 0.36 78.24 ± 0.44\nOurs 86.96 ± 0.09 97.22 ± 0.10 94.46 ± 0.08 69 .72 ± 0.13 77.24 ± 0.30\nThe proposed method demonstrated a signiﬁcantly superior performance compared to the baselines on datasets including 20NG, R52 and Ohsumed,\nas determined by a student t test ( p < 0.05)\nTable 4 The macro f1-score on test set (%)\nMethod 20NG R8 R52 Ohsumed MR\nSVM 83.26 ± 1.26 89.12 ± 1.06 68.64 ± 0.05 62.66 ± 0.62 76.32 ± 0.04\nKNN 66.59 ± 2.14 82.64 ± 2.26 66.32 ± 1.12 53.12 ± 1.11 70.21 ± 0.12\nRF 77.14 ± 1.36 86.64 ± 1.53 65.36 ± 2.16 52.61 ± 1.06 69.26 ± 0.52\nBiLSTM 73.65 ± 0.26 88.55 ± 1.41 69.36 ± 0.33 48.66 ± 0.22 77.26 ± 0.26\nBiGRU 73.33 ± 0.38 88.62 ± 1.23 69.44 ± 0.62 48.99 ± 0.52 76.82 ± 0.13\nCNN 82.06 ± 0.33 88.76 ± 0.63 69.55 ± 0.33 53.16 ± 0.62 77.60 ± 0.32\nTransformer 74.88 ± 0.75 88.26 ± 0.86 68.88 ± 0.56 52.69 ± 0.41 75.96 ± 0.32\nFastText 78.24 ± 0.36 90.64 ± 0.63 69.71 ± 0.14 54.88 ± 0.26 76.22 ± 0.46\nTextGCN [10] 85.02 ± 0.06 92.88 ± 0.06 70.17 ± 0.07 61.45 ± 0.35 75.58 ± 0.34\nTextGCN(POS) 85.12 ± 0.06 93.25 ± 0.12 70.42 ± 0.22 62.06 ± 0.12 76.49 ± 0.33\nBiGRU+GCN [16] 85.45 ± 0.14 93.42 ± 0.26 70.66 ± 0.12 62.16 ± 0.13 77.52 ± 0.31\nBiLSTM+GCN [ 20] 85.40 ± 0.10 94.33 ± 0.32 70.96 ± 0.05 62.32 ± 0.21 78.20 ± 0.31\nOurs 85.69 ± 0.11 93.66 ± 0.47 71.22 ± 0.07 62 .77 ± 0.43 77.02 ± 0.21\nThe proposed method demonstrated a signiﬁcantly superior performance compared to the baselines on datasets including 20NG, R52, and Ohsumed,\nas determined by Student’s t test ( p < 0.05)\nmethod outperforms BiGRU+GCN and BiLSTM+GCN by\n0.19% and 0.41% on 20NG, surpasses BiGRU+GCN and\nBiLSTM+GCN by 0.58% and 0.26% on R52, and exceeds\nBiGRU+GCN and BiLSTM+GCN by 1.28% and 0.57% on\nOhsumed. Furthermore, as detailed in Table 4, the proposed\nmethod also attains the highest F1 scores on 20NG, R52, and\nOhsumed. These results underscore the superiority of the\nmethod presented in this paper for text classiﬁcation tasks.\nThey also afﬁrm that the Transformer exhibits more robust\nfeature extraction capabilities than the RNN structure and\nachieves a more precise semantic representation of tokens.\nHowever, on the MR and R8 datasets, our classiﬁcation\nperformance lags behind the BiLSTM+GCN approach. This\ndiscrepancy suggests that the BTM within BiLSTM+GCN\nis more adept at capturing crucial information from shorter\ntexts, revealing a limitation in our method’s performance with\nconcise texts. Despite this, our method outperforms Trans-\nformer and TextGCN across all datasets, showcasing how\nthe GTG structure effectively amalgamates the strengths of\nTransformer and GCN networks to enhance the model’s fea-\nture extraction prowess.\nThe inclusion of POS in TextGCN (POS) results in\nperformance enhancements across four datasets, as word\n123\n  161 Page 10 of 11 International Journal of Computational Intelligence Systems           (2023) 16:161 \nFig. 9 The embedding of word nodes from the second GCL in max-\nimum value label. In the above ﬁgure, points with the same color\nrepresent the same document category\nnodes encapsulate both contextual and POS-related seman-\ntics. Additionally, our observations demonstrate that SVM\nachieves commendable classiﬁcation performance, often sur-\npassing deep learning approaches. This underscores the\neffectiveness of machine learning in simpler tasks.\nNext, we store the output of the second GCL post-training\nand proceed to visualize the two-dimensional embeddings of\nword nodes. Employing t-SNE [ 33], we condense the word\nembeddings into two dimensions, designating the highest\nvalue within the word vector dimension as the word label, as\ndepicted in Fig. 9. Within the t-SNE visualization, the hor-\nizontal and vertical axes signify t-SNE values utilized for\ngauging point-to-point distances.\nIn Fig. 9, it is evident that words sharing the same label are\nclosely clustered, aligning with the ﬁndings in the referenced\nproposal [ 10]. This indicates that words are predominantly\nsituated within the document categories. Word nodes con-\nnected to document nodes are proximate to them. The\nsemantic attributes of words are primarily molded by their\nimmediate context, a distinctive trait of the GCN. To illus-\ntrate the embedding visualization of word nodes, we utilize\nPOS as labels, as depicted in Fig. 10.\nIn Fig. 10, it is evident that nodes sharing the same POS are\nproximate within a limited span. This close proximity can be\nperceived as the immediate context of the word. Words within\nthe same document naturally draw near, owing to their shared\ncontextual surroundings. Building upon this premise, they\nare additionally inﬂuenced by their POS and tend to cluster\naround the corresponding POS within a given context. This\nobservation underscores that the approach presented in this\npaper imbues the processed word nodes with both contextual\nand POS-related semantics.\nFig. 10 The embedding of word nodes from the second GCL in POS\nlabel. In the above ﬁgure, points with the same color represent the same\nPOS tag\n5 Conclusion\nIn this study, we propose a new GCN structure called GTG,\nwhich combines the advantages of Transformer and GCN.\nBy introducing positional embeddings, GTG considers the\nword node’s text sequence in GCN, and the introduction of\nTransformer further extracts context information from word\nnodes. In addition, we also propose a POS and context-based\ncomposition method to have the semantics of context and\nPOS with word node vectors. The experimental results show\nthat GTG effectively improves the text classiﬁcation accu-\nracy of TextGCN, and the POS-based construction graph\nmethod enables the word nodes to obtain POS clustering\neffect. The proposed method achieves state-of-the-art perfor-\nmance on three datasets compared to the comparison method.\nThe proposed method provides a solution to improve the\nshortcomings of TextGCN in text classiﬁcation tasks.\nAuthor Contributions Conceptualization, BL and WG; methodology,\nBL and WG; writing—original draft preparation, BL, ZL, and WG;\nexperiment, BL, ZL, and CY; data, BL and CY; project administration,\nBL and ZF; visualization, BL and ZF; funding acquisition, ZF and\nZL. All authors have read and agreed to the published version of the\nmanuscript.\nFunding This research was funded by the National Natural Science\nFoundation of China (No. 11864005), the Basic Ability Promotion\nProject for Y ong Teachers in Guangxi (2023KY0017), and the Spe-\nciﬁc Research Project of Guangxi for Research Bases and Talents\n(AD23026105).\nAvailability of Data and Materials Our experimental datasets from\nhttps://github.com/iworldtong/text_gcn.pytorch.\nDeclarations\nConﬂict of Interest The authors declare no competing interests.\n123\nInternational Journal of Computational Intelligence Systems           (2023) 16:161 Page 11 of 11   161 \nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\n1. Kowsari, K., JafariMeimandi, K., Heidarysafa, M., et al.: Text clas-\nsiﬁcation algorithms: a survey. Information 10(4), 150 (2019)\n2. Miro´ nczuk, M.M., Protasiewicz, J.: A recent overview of the state-\nof-the-art elements of text classiﬁcation. Expert Syst. Appl. 106,\n36–54 (2018)\n3. Goudjil, M., Koudil, M., Bedda, M., et al.: A novel active learning\nmethod using SVM for text classiﬁcation. Int. J. Autom. Comput.\n15, 290–298 (2018)\n4. Trstenjak, B., Mikac, S., Donko, D.: KNN with TF-IDF based\nframework for text categorization. Procedia Eng. 69, 1356–1364\n(2014)\n5. Shah, K., Patel, H., Sanghvi, D., et al.: A comparative analysis of\nlogistic regression, random forest and KNN models for the text\nclassiﬁcation. Augment. Hum. Res. 5, 1–16 (2020)\n6. Li, Y ., Yang, T.: Word embedding for understanding natural lan-\nguage: a survey. Guide Big Data Appl. 26, 83–104 (2018)\n7. Vieira, J.P .A., Moura, R.S., An analysis of convolutional neural\nnetworks for sentence classiﬁcation. In: XLIII Latin American\ncomputer conference (CLEI), vol. 2017. IEEE, pp 1–5 (2017)\n8. Liu, P ., Qiu, X., Huang, X.: Recurrent neural network for text classi-\nﬁcation with multi-task learning. arXiv preprint arXiv:1605.05101\n(2016)\n9. V aswani, A., Shazeer, N., Parmar, N., et al.: Attention is all you\nneed. Adv. Neural Inf. Process. Syst. 30, 5988–5999 (2017)\n10. Yao, L., Mao, C., Luo, Y .: Graph convolutional networks for text\nclassiﬁcation. Proc. AAAI Conf. Artif. Intell. 33(01), 7370–7377\n(2019)\n11. Malekzadeh, M., Hajibabaee, P ., Heidari, M., Review of graph neu-\nral network in text classiﬁcation. In: IEEE 12th Annual Ubiquitous\nComputing, Electronics and Mobile Communication Conference\n(UEMCON), 2021, pp, 0084–0091. IEEE (2021)\n12. Huang, L., Ma, D., Li, S., et al.: Text level graph neural network\nfor text classiﬁcation. arXiv preprint arXiv:1910.02356 (2019)\n13. Liu, X., Y ou, X., Zhang, X., et al.: Tensor graph convolutional\nnetworks for text classiﬁcation. Proc. AAAI Conf. Artif. Intell.\n34(05), 8409–8416 (2020)\n14. Xue, B., Zhu, C., Wang, X., et al.: The study on the text classi-\nﬁcation based on graph convolutional network and BiLSTM. In:\nProceedings of the 8th International Conference on Computing and\nArtiﬁcial Intelligence, ACM, pp. 323–331(2022)\n15. Fellbaum, C.: WordNet, Theory and Applications of Ontology:\nComputer Applications, pp. 231–243. Springer, Dordrecht (2010)\n16. Dong, Y ., Yang, Z., Cao, H.: A text classiﬁcation model based on\nGCN and BiGRU fusion. In: Proceedings of the 8th International\nConference on Computing and Artiﬁcial Intelligence, ACM, pp.\n318–322 (2022)\n17. Church, K.W.: Word2V ec. Nat. Lang. Eng. 23(1), 155–162 (2017)\n18. Fang, F., Hu, X., Shu, J., et al.: Text classiﬁcation model based on\nmulti-head self-attention mechanism and BiGRU. In: 2021 IEEE\nConference on Telecommunications, Optics and Computer Science\n(TOCS), pp. 357–361. IEEE (2021)\n19. Devlin, J., Chang, M.W., Lee, K., et al.: Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018)\n20. Ye, Z., Jiang, G., Liu, Y ., et al.: Document and word representations\ngenerated by graph convolutional network and bert for short text\nclassiﬁcation. In: ECAI 2020, IOS Press, pp. 2275–2281 (2020)\n21. Huang, J., Peng, M., Li, P ., et al.: Improving biterm topic model\nwith word embeddings. World Wide Web 23(6), 3099–3124 (2020)\n22. Lin, Y ., Meng, Y ., Sun, X., et al.: Bertgcn: transductive text classiﬁ-\ncation by combining gcn and bert. arXiv preprint arXiv:2105.05727\n(2021)\n23. Wang, K., Han, S.C., Poon, J.: InducT-GCN: inductive graph con-\nvolutional networks for text classiﬁcation. In: 2022 26th Interna-\ntional Conference on Pattern Recognition (ICPR), pp. 1243–1249.\nIEEE (2022)\n24. Bird, S., Edward, L., et al.: Natural Language Processing with\nPython. O’Reilly Media Inc, Sebastopol (2009)\n25. Bouma, G.: Normalized (pointwise) mutual information in collo-\ncation extraction. Proc. GSCL 30, 31–40 (2009)\n26. Ramos, J.: Using tf-idf to determine word relevance in document\nqueries. Proc. First Instr. Conf. Mach. Learn. 242(1), 29–48 (2003)\n27. Misra, D.M.: A self regularized non-monotonic activation function.\narXiv preprint arXiv:1908.08681 (2019)\n28. Soyalp, G., Alar, A., Ozkanli, K., et al.: Improving Text Classiﬁ-\ncation with Transformer. In: 2021 6th International Conference on\nComputer Science and Engineering (UBMK), pp. 707–712. IEEE\n(2021)\n29. Zhang, S., Tong, H., Xu, J., et al.: Graph convolutional networks:\na comprehensive review. Comput. Soc. Netw. 6(1), 1–23 (2019)\n30. Feng, Y ., Cheng, Y .: Short text sentiment analysis based on multi-\nchannel CNN with multi-head attention mechanism. IEEE Access\n9, 19854–19863 (2021)\n31. Joulin, A., Grave, E., Bojanowski, P ., et al.: Bag of tricks for efﬁ-\ncient text classiﬁcation. arXiv preprint arXiv:1607.01759 (2016)\n32. Pennington, J., Socher, R., Manning, C.D.: Glove: global vectors\nfor word representation. In: Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP),\nACL, pp. 1532–1543 (2014)\n33. V an der Maaten, L., Hinton, G.: Visualizing high-dimensional data\nusing t-SNE. J. Mach. Learn. Res. 9(11), 2579–2605 (2018)\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8240891098976135
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6268788576126099
    },
    {
      "name": "Graph",
      "score": 0.5948930382728577
    },
    {
      "name": "Natural language processing",
      "score": 0.5189692378044128
    },
    {
      "name": "Text graph",
      "score": 0.5177052021026611
    },
    {
      "name": "Transformer",
      "score": 0.5063275098800659
    },
    {
      "name": "Cluster analysis",
      "score": 0.4593406617641449
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.35488688945770264
    },
    {
      "name": "Text mining",
      "score": 0.2555443048477173
    },
    {
      "name": "Theoretical computer science",
      "score": 0.12365692853927612
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150807315",
      "name": "Guangxi University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I18570673",
      "name": "Guangxi University of Science and Technology",
      "country": "CN"
    }
  ],
  "cited_by": 18
}