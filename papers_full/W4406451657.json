{
  "title": "Applications Analyzing E-commerce Reviews with Large Language Models (LLMs): A Methodological Exploration and Application Insight",
  "url": "https://openalex.org/W4406451657",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2481770794",
      "name": "Jia-Rui Rao",
      "affiliations": [
        "Uber AI (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A342090404",
      "name": "Qian Zhang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2126551939",
      "name": "Xinqiu Liu",
      "affiliations": [
        "Western University"
      ]
    },
    {
      "id": "https://openalex.org/A342090404",
      "name": "Qian Zhang",
      "affiliations": [
        "Uber AI (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2126551939",
      "name": "Xinqiu Liu",
      "affiliations": [
        "Uber AI (United States)",
        "Western University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4400593882",
    "https://openalex.org/W4403622781",
    "https://openalex.org/W4401626633",
    "https://openalex.org/W4402386854",
    "https://openalex.org/W4309486096",
    "https://openalex.org/W4403747178",
    "https://openalex.org/W4403345250",
    "https://openalex.org/W6872844639",
    "https://openalex.org/W4403066721",
    "https://openalex.org/W4403819615",
    "https://openalex.org/W4403800666",
    "https://openalex.org/W4404552614",
    "https://openalex.org/W4404609247",
    "https://openalex.org/W4405300992",
    "https://openalex.org/W4399510449",
    "https://openalex.org/W4395081456",
    "https://openalex.org/W3126795574",
    "https://openalex.org/W4317395850",
    "https://openalex.org/W4317934491",
    "https://openalex.org/W4392966945",
    "https://openalex.org/W4391174181",
    "https://openalex.org/W6892714594",
    "https://openalex.org/W4391402304",
    "https://openalex.org/W4403577559",
    "https://openalex.org/W4396912870",
    "https://openalex.org/W4294974822",
    "https://openalex.org/W4405726147",
    "https://openalex.org/W6893051235",
    "https://openalex.org/W4405308407",
    "https://openalex.org/W4405307502",
    "https://openalex.org/W4406035918",
    "https://openalex.org/W4400611228",
    "https://openalex.org/W4403295699",
    "https://openalex.org/W4405980859",
    "https://openalex.org/W4404172112",
    "https://openalex.org/W4406336391",
    "https://openalex.org/W4404249473",
    "https://openalex.org/W4402721914"
  ],
  "abstract": "The ubiquity of online shopping has transformed our daily lives, offering unparalleled convenience and enriching our purchasing experiences. It has become an indispensable part of our existence, allowing us to acquire everything from basic essentials to high-end luxury items with ease. Amazon, a leading e-commerce platform [1], employs two primary customer feedback mechanisms: the Star Rate (1-5) and detailed reviews. The Star Rate is a quick, convenient, and visually intuitive method for customers to score products, while reviews provide a more comprehensive description of the product and their shopping experience. These feedback mechanisms not only influence other users' purchasing decisions but also serve as a guide for businesses to adjust their offerings based on customer opinions, establishing a negative feedback adjustment mechanism.[2,3,4,5,6]. We introduce the innovative LLM model, commonly used in computer vision, into our NLP text analysis. Utilizing WORD2vec, we pass word vectors through classification functions to analyze pessimistic and optimistic sentiments. We then correlate these emotions with Star Rates, discovering a higher-order functional relationship between them.",
  "full_text": " \nJournal of Artificial Intelligence General Science (JAIGS) \n \nISSN: 3006-4023 (Online), Volume 07, Issue 1, 2024           DOI: 10.60087 \n                                      \nHome page https://ojs.boulibrary.com/index.php/JAIGS \n \n \n \n \n \n* Corresponding author: Jiarui Rao1 \n \nARTICLE INFO: Received:  19.12.2024   Accepted: 30.12.2024 Published: 17.01.2025 \n \n \nCopyright: © The Author(s), 2024. Published by JAIGS. This is an Open Access article, distributed under the terms of \nthe Creative Commons Attribution 4.0 License (http://creativecommons.org/licenses/by/4.0/), which permits \nunrestricted use, distribution and reproduction in any medium, provided the original work is properly cited. \n \n \nApplications Analyzing E-commerce Reviews with Large Language Models \n(LLMs): A Methodological Exploration and Application Insight \n \nJiarui Rao1, Qian Zhang1, Xinqiu Liu3 \n \n1Uber Technologies Inc, USA. \n2Tencent Inc., China. \n3 Western University, Canada. \n \nAbstract:  \n \nThe ubiquity of online shopping has transformed our daily lives, offering unparalleled convenience and enriching our \npurchasing experiences. It has become an indispensable part of our existence, allowing us to acquire everything from basic \nessentials to high -end luxury items with ease. Amazon, a leading e -commerce platform [1], employs two primary customer \nfeedback mechanisms: the Star Rate (1 -5) and detailed reviews. The Star Rate i s a quick, convenient, and visually intuitive \nmethod for customers to score products, while reviews provide a more comprehensive description of the product and their \nshopping experience. These feedback mechanisms not only influence other users' purchasing decisions but also serve as a guide \nfor businesses to adjust their offerings based on customer opinions, establishing a negative feedback adjustment \nmechanism.[2,3,4,5,6] \n \nWe introduce the innovative LLM model, commonly used in computer vision, into our NL P text analysis. Utilizing \nWORD2vec, we pass word vectors through classification functions to analyze pessimistic and optimistic sentiments. We then \ncorrelate these emotions with Star Rates, discovering a higher-order functional relationship between them. \n \n \nKeywords:  \nLLM，E-commerce，NLP, E-commerce reviews, Large Language Models (LLMs), Methodological analysis, \nSentiment analysis, Application insights \n \n \n \n \n \nJournal of Artificial Intelligence General Science (JAIGS) Home page https://ojs.boulibrary.com/index.php/JAIGS    Page: 208 \n  \n \n1. Introduction \nIn the age of digital transformation, online shopping has become an integral part of our daily lives, allowing us \nto purchase any legal product with just a few clicks. As the world's largest online shopping platform, Amazon \nepitomizes the convenience of e -commerce, enabling us to easily obtain Italian luxury goods from the United \nStates. However, this convenience also brings an element of unpredictability, as the intangible nature of online \npurchases can lead to outcomes that range from exceeding expectatio ns to causing disappointments. As a result, \n[7,8,9,10]customers often express their satisfaction or dissatisfaction through reviews or star ratings, providing \nfeedback on the product's strengths and weaknesses, or even reporting issues with the product. Th is user -\ngenerated feedback has a significant impact on corporate behavior and the purchasing decisions of other users. \nMoreover, we plan to filter reviews and star ratings for analysis to inform the research and development strategy \nfor Sunshine's products .[11,12,13,14]  \nThis paper aims to explore the interplay between user reviews, helpfulness ratings, and star ratings, using a \ncombination of data formulas and both qualitative and quantitative research methods. We have identified key \ndata metrics based on r atings and reviews that can provide the most valuable insights for Sunshine once their \nproducts are launched in the online marketplace. We will also consider the temporal aspect to assess its influence \non reviews, observing whether merchants can use consum er feedback to improve their products, thereby \nenhancing their consumer reputation. Furthermore, we will establish a model to discuss how users' comments \nshape the perceptions of potential customers. By integrating the aforementioned models, we aim to clar ify the \nrelationship between user emotions and their star ratings in user reviews. Ultimately, we will compile a \ncomprehensive report for Sunshine Company's senior [15,16,17,18,19] management, summarizing our findings \nto guide sales and business strategies.  To enhance our analysis, we will incorporate elements from Large \nLanguage Models (LLMs), which have shown great capability in understanding and generating human -like text. \n \n2. Method \na) 2.1 Data Collection and Preprocessing  \nThe dataset used in this study was sourced from Amazon, a leading e -commerce platform. The dataset comprised customer reviews, star \nratings, and helpfulness ratings for a variety of products. The initial step involved data cleaning, which included removing outliers and \nmissing values to ens ure the quality and reliability of the data. The dataset was then divided into a training set and a testing set in a 6:4 \nratio, with 60% of the data used for training and 40% for testing. [20,21,22,23]  \nb) 2.2 Sentiment Analysis with WORD2vec  \nTo analyze the sen timent of the reviews, we employed the WORD2vec model to convert text data into word vectors. These word vectors \nwere then passed through classification functions to identify pessimistic and optimistic sentiments. The sentiment scores wer e \nsubsequently cor related with the star ratings to explore the relationship between customer emotions and their numerical \nratings.[24,25,26,27]  \nc) 2.3 Model Implementation  \nA Token Classification model based on the Deberta v3 pre -trained model was implemented. The DebertaV3Back bone pre -trained model \nwas used as the backbone network, and a fully connected layer (Dense) along with a softmax activation function was connected to the \noutput layer. This setup allowed the model to map the output to a specified number of tag categories.  The Adam optimizer was used to set \nthe learning rate to 2e -5, and the CrossEntropy loss function was employed to calculate the model's loss value. The FBetaScore was used \nas the evaluation metric to assess the model's performance.  \needback and its influenc e on product development and reputation.  \n                                                 ISSN: 3006-4023 (Online), Volume 07, Issue 1, 2024           DOI: 10.60087                          Page: 209 \n \n \n \nd) 2.6 Integration of Models  \nThe sentiment analysis model, the temporal analysis model, and the correlation model between sentiment and star ratings were integrated \nto provide a comprehensive understanding of the inter play between user reviews, helpfulness ratings, and star ratings. This integrated \napproach aimed to elucidate the relationship between user emotions and their star ratings, providing valuable insights for bu sinesses to \ninform their sales and business strat egies.[28,29,30,31]  \nBy employing these methods, the study aimed to provide a robust analysis of e -commerce reviews using Large Language Models (LLMs), \noffering actionable insights for businesses like Sunshine Company to enhance their product development an d customer engagement \nstrategies.  \n \n \n3. Simulation Experience \n \nIn this paper, a Token Classification model based on Deberta v3 pre -trained model is implemented. Firstly, a \nDebertaV3Backbone pre-trained model is used as the backbone network by loading it, and then a fully connected layer (Dense) \nand a softmax activation function are connected to the output layer for mapping the output of the model to a specified number  \nof tag categories. Next, the Adam optimiser was used to set the learning rate to 2e -5, the Cr ossEntropy loss function \n(CrossEntropy) was used to calculate the loss value of the model, and the FBetaScore was used as the evaluation metric. The \noutput results are shown in Figure 3. \n \nFigure 3. The output results \n（Photo credit : Original） \n\nJournal of Artificial Intelligence General Science (JAIGS) Home page https://ojs.boulibrary.com/index.php/JAIGS    Page: 210 \n  \n \nThe dataset is preprocessed to remove outliers and missing values, and then the data is divided in the ratio of 6:4, 40% of the \ndata is used for model testing and 60% of the data is used for model training, and the accuracy is output using the test set to \noutput the results of the binary classification, as shown in Table 1. \nTable 1. Modelling assessment. \n \n \n precision recall f1-score support \nacc 0.56 0.57 0.56 129 \ngood 0 0 0 20 \nunacc 0.87 0.97 0.92 397 \nvgood 0 0 0 25 \n     \naccuracy   0.8 571 \nmacro avg 0.36 0.38 0.37 571 \nweighted \navg 0.73 0.8 0.77 571 \nFrom the prediction results, it can be seen that the model has a prediction accuracy of 80%, with a precision of 56%, a recal l \nof 57%, and an f1-score of 0.56, which shows that the machine learning model is still able to distinguish chatbots from natural \nlanguage, achieving an accuracy of 80%, although both the racall and precision are close to 50%, which proves that chat bots \nare easily confused with natural language to some extent.[32,33] \n \n4. Conclusions \nIn recent years, chatbots based on Large Language Models (LLMs) have attracted a lot of attention in the field of Artificial \nIntelligence.LLMs are large-scale natural language processing models trained by deep learning techniques, and their powerful \nlanguage understanding and generation capabilities enable chatbots to engage in a natural dialogue with users. In this study, \nthe dataset was firstly visualised and analysed, followed by a detailed preprocessing work on the dataset text, and finally t he \nchatbot's language was generated and text preprocessed using the Deberta v3 model. Further, the generated text and natural \nlanguage were classified using machine learning classifiers and the results show that the model has a prediction accuracy of \n80%. \nSpecifically, precision is 56%, recall is 57%, and f1 -score is 0.56. This indicates that the machine learning model is able to \ndiscriminate between chatbot-generated text and real natural language to a certain extent, with an accuracy of 80%. However, \nit is worth noting that both PRECISION and RECOLL are close to 50%, which implies that there is some degree of confusion \nbetween chatbots and natural language.[34,35,36,37] \nFrom the experimental results, it can be seen that in the current stage, the machine learning model has achi eved better results \nin recognising chatbot-generated text and real natural language, but there is still a certain degree of confusion. This suggests \nthat when developing and deploying chatbots based on large -scale language models, we need to consider more carefully how \nto improve the model's dialogue comprehension and generation capabilities in order to further enhance the differentiation \nbetween it and real natural language. \nIn summary, although machine learning models have achieved good prediction accuracy and success in distinguishing chatbots \nfrom natural language, continuous efforts are needed to improve model performance to better meet user needs and ensure a \nhigher quality and more reliable interaction experience in real-world applications. \n\n                                                 ISSN: 3006-4023 (Online), Volume 07, Issue 1, 2024           DOI: 10.60087                          Page: 211 \n \n \n \n \nReference \n[1]. Li, Keqin, et al. \"Exploring the Impact of Quantum Computing on Machine Learning Performance.\" (2024).  \n[2]. Wang, Zixiang, et al. \"Research on Autonomous Driving Decision -making Strategies based Deep Reinforcement \nLearning.\" arXiv preprint arXiv:2408.03084 (2024). \n[3]. Yan, Hao, et al. \"Research on Image Generation Optimization based Deep Learning.\" (2024).  \n[4]. Tang, Xirui, et al. \"Research on Heterogeneous Computation Resource Allocation based on Data -driven Method.\" arXiv \npreprint arXiv:2408.05671 (2024). \n[5]. Su, Pei-Chiang, et al. \"A Mixed -Heuristic Quantum-Inspired Simplified Swarm Optimization Algorithm for scheduling \nof real-time tasks in the multiprocessor system.\" Applied Soft Computing 131 (2022): 109807.  \n[6]. Zhao, Yuwen, Baojun Hu, and Sizhe Wang. \"Prediction o f Brent crude oil price based on LSTM model under the \nbackground of low-carbon transition.\"arXiv preprint arXiv:2409.12376(2024). \n[7]. Diao, Su, et al. \"Ventilator pressure prediction using recurrent neural network.\" arXiv preprint arXiv:2410.06552 (2024). \n[8]. Zhao, Qinghe, Yue Hao, and Xuechen Li. \"Stock Price Prediction Based on Hybrid CNN -LSTM Model.\" (2024). \n[9]. Yin, Ziqing, Baojun Hu, and Shuhan Chen. \"Predicting Employee Turnover in the Financial Company: A Comparative \nStudy of CatBoost and XGBoost Models.\" (2024). \n[10]. Diao, Su, et al. \"Ventilator pressure prediction using recurrent neural network.\" arXiv preprint arXiv:2410.06552 (2024). \n[11]. Xu, Q., Wang, T., & Cai, X. (2024). Energy Market Price Forecasting and Financial Technology Risk Management Based \non Generative AI. Preprints. https://doi.org/10.20944/preprints202410.2161.v1 \n[12]. Wu, X., Xiao, Y., & Liu, X. (2024). Multi -Class Classification of Breast Cancer Gene Expression Using PCA and \nXGBoost. Preprints. https://doi.org/10.20944/preprints202410.1775.v2 \n[13]. Wang, H., Zhang, G., Zhao, Y., Lai, F., Cui, W., Xue, J., Wang, Q., Zhang, H., & Lin, Y. (2024). RPF -ELD: Regional \nPrior Fusion Using Early and Late Distillation for Breast Cancer Recognition in Ultrasound Images. Preprints. \nhttps://doi.org/10.20944/preprints202411.1419.v1 \n[14]. Min, L., Yu, Q., Zhang, Y., Zhang, K., & Hu, Y. (2024, October). Financial Prediction Using DeepFM: Loan Repayment \nwith Attention and Hybrid Loss. In 2024 5th International Conference on Machine Learning and Computer Application \n(ICMLCA) (pp. 440-443). IEEE. \n[15]. Accurate Prediction of Temperature Indicators in Eastern China Using a Multi -Scale CNN-LSTM-Attention model \n[16]. Sun, Y., Salami Pargoo, N., Jin, P., & Ortiz, J. (2024, October). Optimizing Autonomous Driving for Safety: A Human -\nCentric Approach with LLM -Enhanced RLHF. In  Companion of the 2024 on ACM International Joint Conference on \nPervasive and Ubiquitous Computing (pp. 76-80). \n[17]. Li, Yinuo et al. “Late Changes in Renal Volume and Function after Proton Beam Therapy in Pediatric and Adult Patients: \nChildren Show Significant Renal Atrophy but Deterioration of Renal Function Is Minimal in the Long -Term in Both \nGroups.” Cancers vol. 16,9 1634. 24 Apr. 2024, doi:10.3390/cancers16091634 \n[18]. Shimizu, Shosei et al. “Proton beam therapy for a giant hepatic hemangioma: A case report and literature review.” Clinical \nand translational radiation oncology vol. 27 152-156. 3 Feb. 2021, doi:10.1016/j.ctro.2021.01.014 \n[19]. Shimizu, Shosei et al. “Boron Neutron Capture Therapy for Recurrent Glioblastoma Multiforme: Imaging Evaluation o f \na Case With Long-Term Local Control and Survival.” Cureus vol. 15,1 e33898. 17 Jan. 2023, doi:10.7759/cureus.33898  \n[20]. Li, Yinuo et al. “A Retrospective Study of Renal Growth Changes after Proton Beam Therapy for Pediatric Malignant \nTumor.” Current oncology (Toronto, Ont.) vol. 30,2 1560-1570. 24 Jan. 2023, doi:10.3390/curroncol30020120 \n[21]. Nakamura, Masatoshi et al. “A systematic review and meta -analysis of radiotherapy and particle beam therapy for skull \nbase chondrosarcoma: TRP -chondrosarcoma 2024.” Frontiers in oncology vol. 14 1380716. 19 Mar. 2024, \ndoi:10.3389/fonc.2024.1380716 \n[22]. Nitta, Hazuki et al. “An analysis of muscle growth after proton beam therapy for pediatric cancer.” Journal of radiation \nresearch vol. 65,2 (2024): 251-255. doi:10.1093/jrr/rrad105 \n[23]. Chemical-Protein Relation Extraction with Pre -trained Prompt Tuning,” IEEE 10th International Conference on \nHealthcare Informatics, 2022, Jianping He, Fang Li, Xinyue Hu, Jianfu Li, Yi Nian, Jingqi Wang, Yang Xiang, Qiang \nWei, Hua Xu, Cui Tao. \n[24]. Alzheimer Disease and Related Dementias Risk Prediction: Algorithm Development and Validation Study,” JMIR Aging, \nvol. 7, no. 1, pp. e54748, JMIR Publications Inc., 2024, Xinyue Hu, Zenan Sun, Yi Nian, Yichen Wang, Yifang Dang, \nFang Li, Jingna Feng, Evan Yu, Cui Tao. \n[25]. Mo, Yuhong, et al. \"Password complexity prediction based on roberta algorithm.\" Applied Science and Engineering \nJournal for Advanced Research 3.3 (2024): 1-5. \n[26]. “Dynamic Prognosis Prediction for Patients on DAPT After Drug -Eluting Stent Implantation: Model Dev elopment and \nValidation,” Journal of the American Heart Association, 2024, Fang Li, Laila Rasmy, Yang Xiang, Jingna Feng, Ahmed \nAbdelhameed, Xinyue Hu, Zenan Sun, David Aguilar, Abhijeet Dhoble, Jingcheng Du, Qing Wang, Shuteng Niu, Yifang \nDang, Xinyuan Zhang, Ziqian Xie, Yi Nian, JianPing He, Yujia Zhou, Jianfu Li, Mattia Prosperi, Jiang Bian, Degui Zhi, \nCui Tao. \n[27]. Yang, R. (2024). CaseGPT: a case reasoning framework based on language models and retrieval -augmented generation. \nJournal of Artificial Intelligence General Science (JAIGS) Home page https://ojs.boulibrary.com/index.php/JAIGS    Page: 212 \n  \n \narXiv preprint arXiv:2407.07913. \n[28]. Qian, Chenghao, et al. \"WeatherDG: LLM -assisted procedural weather generation for domain -generalized semantic \nsegmentation.\" arXiv preprint arXiv:2410.12075 (2024). \n[29]. Mo, Yuhong, et al. \"Large language model (llm) ai text generation detecti on based on transformer deep learning \nalgorithm.\" arXiv preprint arXiv:2405.06652 (2024). \n[30]. “Chemical-Protein Relation Extraction with Pre -trained Prompt Tuning,” IEEE 10th International Conference on \nHealthcare Informatics, 2022, Jianping He, Fang Li, Xinyu e Hu, Jianfu Li, Yi Nian, Jingqi Wang, Yang Xiang, Qiang \nWei, Hua Xu, Cui Tao. \n[31]. Alzheimer Disease and Related Dementias Risk Prediction: Algorithm Development and Validation Study,” JMIR Aging, \nvol. 7, no. 1, pp. e54748, JMIR Publications Inc., 2024, Xinyue  Hu, Zenan Sun, Yi Nian, Yichen Wang, Yifang Dang, \nFang Li, Jingna Feng, Evan Yu, Cui Tao. \n[32]. “Dynamic Prognosis Prediction for Patients on DAPT After Drug -Eluting Stent Implantation: Model Development and \nValidation,” Journal of the American Heart Associatio n, 2024, Fang Li, Laila Rasmy, Yang Xiang, Jingna Feng, Ahmed \nAbdelhameed, Xinyue Hu, Zenan Sun, David Aguilar, Abhijeet Dhoble, Jingcheng Du, Qing Wang, Shuteng Niu, Yifang \nDang, Xinyuan Zhang, Ziqian Xie, Yi Nian, JianPing He, Yujia Zhou, Jianfu Li, Matt ia Prosperi, Jiang Bian, Degui Zhi, \nCui Tao. \n[33]. Mo, Yuhong, et al. \"Make Scale Invariant Feature Transform “Fly” with CUDA.\" International Journal of Engineering \nand Management Research 14.3 (2024): 38-45. \n[34]. Rao, Jiarui, et al. \"Machine Learning in Action: Topic-Centric Sentiment Analysis and Its Applications.\" (2024). \n[35]. Zhang, Qian, et al. \"Sea MNF vs. LDA: Unveiling the Power of Short Text Mining in Financial Markets.\" International \nJournal of Engineering and Management Research 14.5 (2024): 76-82. \n[36]. Rao, Jiarui, et al. \"Integrating Textual Analytics with Time Series Forecasting Models: Enhancing Predictive Accuracy \nin Global Energy and Commodity Markets.\" Innovations in Applied Engineering and Technology (2023): 1-7. \n[37]. Zhang, Qian, Jiarui Rao, and Zong Ke. \"Enhancing Financial Forecasting Models with Textual Analysis: A Comparative \nStudy of Decomposition Techniques and Sentiment -Driven Predictions.\" Innovations in Applied Engineering and \nTechnology (2022): 1-6. \n \n \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.43317297101020813
    },
    {
      "name": "Data science",
      "score": 0.3827511668205261
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2946016260",
      "name": "Uber AI (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I125749732",
      "name": "Western University",
      "country": "CA"
    }
  ],
  "cited_by": 1
}