{
  "title": "Conditioned Natural Language Generation using only Unconditioned Language Model: An Exploration",
  "url": "https://openalex.org/W3101294880",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5065945681",
      "name": "Fan-Keng Sun",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5010825170",
      "name": "Cheng-I Lai",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2962788902",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2973049837"
  ],
  "abstract": "Transformer-based language models have shown to be very powerful for natural language generation (NLG). However, text generation conditioned on some user inputs, such as topics or attributes, is non-trivial. Past approach relies on either modifying the original LM architecture, re-training the LM on corpora with attribute labels, or having separately trained `guidance models' to guide text generation in decoding. We argued that the above approaches are not necessary, and the original unconditioned LM is sufficient for conditioned NLG. We evaluated our approaches by the samples' fluency and diversity with automated and human evaluation.",
  "full_text": "arXiv:2011.07347v1  [cs.CL]  14 Nov 2020\nConditioned Natural Language Generation using only Unconditioned\nLanguage Model: An Exploration\nFan-Keng Sun\nMIT\nfankeng@mit.edu\nCheng-I Lai\nMIT\nclai24@mit.edu\nAbstract\nTransformer-based language models have\nshown to be very powerful for natural lan-\nguage generation (NLG). However, text gen-\neration conditioned on some user inputs, such\nas topics or attributes, is non-trivial. Past ap-\nproach relies on either modifying the original\nLM architecture, re-training the LM on cor-\npus with attirbute labels, or having separately\ntrained ‘guidance models’ to guide text gener-\nation in decoding. W e argued that the above\napproaches are not necessary, and the origi-\nnal unconditioned LM is sufﬁcient for condi-\ntioned NLG. W e evaluated our approaches by\nthe samples’ ﬂuency and diversity with auto-\nmated and human evaluation.\n1 Introduction\nW e have witnessed tremendous progresses in nat-\nural language generation (NLG) with large-scale\nTransformer (\nV aswani et al. , 2017) based lan-\nguage models, such as the GPT -2 ( Radford et al. ,\n2019). A natural question to raise beyond lan-\nguage modeling is, how can we have more\nﬁne-grained control over these powerful models?\nSpeciﬁcally , we would desire a language model\nto generate text centers on some user-deﬁned con-\nditions\n1 . W e call this kind of language model a\nconditioned NLG. One could easily imagine the\ncommercial beneﬁts of employing a conditioned\nNLG in our every day products, such as search.\nT o obtain a conditioned NLG, a naiive approach\nwould be reformulate the original language mod-\neling objectives and train the whole model from\nscratch (\nKeskar et al. , 2019). However, doing\nso requires tons of labeled text with conditions,\nwhich is not always available in most applications.\nRecently , PPLM ( Dathathri et al. , 2020) proposed\nto take advantage of the pretrained GPT -2 without\nany further retraining of the language model itself.\n1 Interchangeable with attributes\nThis is done by having another pretrained ‘guid-\nance model’, such as a Bag of W ord (BOW) or an\nattribute classiﬁer (AC), to guide the latent state\nof GPT -2 toward generating more relevant tokens.\nWitnessing the upside of this approach, we further\nexplored simple, ﬂexible, and effective approaches\nto conditioned NLG, that encompasses these de-\nsired qualities:\n• Simple: only an unconditioned language\nmodel is needed. Does not require any\nadditional training data (\nKeskar et al. , 2019)\nor additional pretrained ‘guidance mod-\nels’ (\nDathathri et al. , 2020).\n• Flexible: the model should be able to model\nany combination of conditions with any\nweighting. This is not the case in previous\nwork.\n• Effective: the generated text is ﬂuent and\nhighly-relevant to the given attribute, by ob-\njective and human evaluation.\nW e propose four methods in total. Three of\nthem are direct modeling of p(x|c) by modiﬁca-\ntion of token embedding or hidden states. The\nother one models p(x)p(c|x) by the consideration\nof next-token distribution. W e combine those four\nmethods into one and compare our results with\nPPLM (\nDathathri et al. , 2020). Some examples of\nthe text generated by our proposed method is in\nT able 1.\n2 Related W ork\nAll previous methods require knowledge about the\nattributes. In ( Ziegler et al. , 2019), human knowl-\nedge about the attribute is provided to train a re-\nward model which is used to train a NLG model\nby reinforcement learning. On the other hand,\n(Keskar et al. , 2019) and ( Ficler and Goldberg ,\n[] T o conclude, T o conclude, we have a very strong case\nfor the hypothesis that the human body was originally\nmade from an alien substance. There is nothing in\nbiology or physics that would have predicted this. It’s an\nobvious hypothesis, but if one is to believe the . . .\n[Negative] T o conclude this article with something\nabout the role of the media in spreading the narrative of\nthe\nmass shooting , the writer has a few words about the\nmedia’s role in spreading a false news article that was\noriginally published on Fox . . .\n[Positive] T o conclude T o conclude this, let the\nfollowing be the statement of his majesty, as to the power\nof his Majesty in his own name, his own body and his\nown blood, in all the places where the said act was made\n. . .\n[Positive, Science] T o conclude, the following article is\na positive review of the book by a respected scientiﬁc and\nmedical reviewer in the same journal entitled, The\nAmerican Medical Association: A Review of Recent\nMedical Research on the Prevention of and . . .\n[Negative, Science] T o conclude: If the majority of\nscientiﬁc papers are either negative and false or have a\nweak statistical evidence for their own status, then these\npapers are not of sufﬁcient quality for a high quality\njournal in the\nscientiﬁc literature . . .\n[Positive, Science, Military] T o conclude this article,\nthe US Navy needs some of the best military research in\nspace and aerospace; its ﬁrst class of space ﬂight research\nship has a high capability for launching large cargo\nspacecraft to the outer . . .\n[Negative, Science, Military] T o conclude, the above is\nthe main reason why the US military has no credibility\nfor using the nuclear weapon and the only alternative is\nnuclear deterrence and nuclear destruction in its own\nname – not for the other . . .\nT able 1: Our methods employ a pre-trained language\nmodel to generate text conditioned on any number of\nattributes without ﬁne-tuning or human knowledge. In\nthis table, we demonstrate results using our methods.\nThe underlined preﬁx is what the language model is\ngiven to generate a passage of text (e.g.\nT o conclude).\nThe controlled attributes are colored and bracketed (e.g.\n[Science] ) and we highlight words in the passage (e.g.\nfalse) that are related to the attributes.\n2017) ﬁne-tuned NLG model on additional dataset\nwith attribute label. These methods usually create\nhighly-relevant sentences but have the limitation\nof ﬁxing the available attributes in advance. In ad-\ndition, PPLM-Discrim in (\nDathathri et al. , 2020)\nalso requires attribute label, but it only has to\ntrain a much smaller discriminator model. Finally ,\nPPLM-BoW in (\nDathathri et al. , 2020) has to ﬁrst\nconstruct a curated word list which is a list words\nthat are highly related to a given attribute. Al-\nthough no ﬁne-tuning is needed, obtaining a word\nlist for any arbitrary attribute is deﬁnitely not a triv-\nial task.\nAlthough some previous methods (\nKeskar et al. ,\n2019; Dathathri et al. , 2020) do generate high-\nquality results, they still have major limitations\nwhen comparing to our methods. Comparison of\ndifferent methods are shown in T able. 2.\n3 Our Approaches\nW e want to ﬁnd approaches to model conditioned\ngeneration pcg(xt+1|c, x1, . . . , xt) by using only a\npre-trained language model plm(xt+1|x1, . . . , xt),\nwhere xi are the words and c is the condition. If\nthere are n condition, then c = {c1, . . . , cn}. Here,\nwe described our four methods to solve this prob-\nlem.\n3.1 Our-preﬁx: Conditional preﬁx\nThe ﬁrst approach is the simplest one. W e feed\na conditional sentence into GPT -2 before it gener-\nates the conditioned text. For example, if we want\na positive sentence about politics, then the condi-\ntional sentence to the GPT -2 will be \"The follow-\ning is a positive article about politics.\" Although\nvery naive, we found this method does actually\nwork. Empirically , we found that adding the word\n\"following\" greatly improves the coherence.\nIn (\nSubramani et al. , 2019), the authors show\nthat a pre-trained language model can be steered\nto recover arbitrary sentences. Here, although\npcg is deﬁnitely not close to plm, we think that\nprepending a well-designed preﬁx can make them\ncloser. The preﬁx alters the hidden states of the\nunconditioned language model in order to steer it\ncloser to a conditioned one. Formally , we assume\nthat plm(xt+1|“The following” +c, x1, . . . , xt) ≈\npcg(xt+1|c, x1, . . . , xt).\nHowever, the problem with this method is that\nthe model will be inﬂuenced by the added sen-\ntence. For example, it will increase the probabil-\nity of generating the word “following” or “article”\nsubsequently . W e tried two ways to ﬁx this.\nFirst, we tried to disconnect the order relation\nof x and c. W e ﬁrst feed c into the language model\nand keep the key-value pairs of the self-attention.\nThen, during the conditioned generation, the lan-\nguage model will start the generation without the\ninput of c but will self-attend on those key-value\npairs. The counting of position indices are also\nrestarted from 0. Unfortunately , this does not work\nout. The model is greatly disturbed by those redun-\ndant key-value pairs and thus generate unrecogniz-\nable language.\nAnother straightforward way is to cut-off the\nspecial preﬁx after a ﬁxed generation step. This\nModel type Form of model Samples Example models and number of trainable params\nLanguage model p(x) Uncond. GPT -2 medium: 345M(Radford et al. , 2019)\nFine-tuned language model p(x) Uncond. Fine-tuned GPT -2 medium: 345M(Ziegler et al. , 2019)\nConditional language model p(x|c) Cond. CTRL: 1.6B(Keskar et al. , 2019)\nPlug and play language model p(x|c) ∝ p(x)p(c|x) Cond. PPLM-BoW: 0 (need curated word list)\n(Dathathri et al. , 2020) PPLM-Discrim: ∼ 1K/attribute\nOur approaches p(x|c) Cond. Our-preﬁx: 0, Our-embedding: 0, Our-attention: 0\np(x|c) ∝ p(x)p(c|x) Our-next-token: 0\nT able 2: Comparison of different methods for NLG, including unconditioned and conditioned language NLG.\nAll conditioned methods are based on unconditioned models, but our methods doesn’t require ﬁne-tuning or any\ncurated word list.\nabsolutely ﬁxes the issue in a brute-force manner.\nIn (\nDathathri et al. , 2020), they also employed this\nmethod to avoid degeneration (i.e. model keep pro-\nducing the same token). In this paper, we refer to\nthis approach as ‘early stopping’.\n3.2 Our-embedding: changing token\nembedding\nGiven n conditions, we can use the tokenizer to ob-\ntain the token index ti,∀i ∈ [1, n] corresponding\nto the conditions c = {c1, . . . , cn}. Then, we add\nthe token embedding of ti with weight wi to all\ntoken embeddings for all i ∈ [1, n]. Finally , we re-\nnormalize all embedding by dividing 1+ ∑ n\ni=1 wi.\nIn original GPT -2 (\nRadford et al. , 2019), the in-\nput and output embeddings are tied. Here, we\nuntie them and only apply this change to the in-\nput embedding. By doing so, every input em-\nbedding contains information about the conditions\nand the transformation from token space to embed-\nding space is guided toward the conditions. For\nexample, the token “military” will gain positivity\nif the condition is “positive” and gain negativity if\nthe condition is “negative” from the viewpoint of\nlanguage model. W e do not change the output em-\nbedding because we conjecture that doing so will\nde-transformed the conditioned embedding.\nNotice that in this method, the user can decide\nthe weight for each condition. This is the ﬂexibil-\nity that we desired.\n3.3 Our-attention: changing self-attention\nkey-value pairs\nSimilar to the previous method, we change the self-\nattention key-value pairs in the language model by\nadding the key-value pairs of the condition token\nindices. T o obtain the key-value pairs correspond-\ning to a token index ti at time step t, we feed a sin-\ngle token ti with position index t into the model.\nAll key-value pairs are also re-normalize by di-\nviding 1 +∑ n\ni=1 wi. T o avoid degeneration, the\nweights are decrease inversely proportional to the\nnumber of time steps.\nThe idea of this method is similar to the previ-\nous one. The main difference is that this method\nconsiders different position indices.\n3.4 Our-next-token: changing the output\ndistribution by next-token distribution\nW e have\npcg(xt+1|c, x1, . . . , xt) (1)\n=p(xt+1, c|x1, . . . , xt)\np(c|x1, . . . , xt) (2)\n=p(c|x1, . . . , xt+1)p(xt+1|x1, . . . , xt)\np(c|x1, . . . , xt) . (3)\nNotice that c, x1, . . . , xt all have known as-\nsignment, so p(c|x1, . . . , xt) is a constant. Also,\np(xt+1|x1, . . . , xt) is essentially a language\nmodel. Thus, we have\npcg(xt+1|c, x1, . . . , xt)\n∝ p(c|x1, . . . , xt+1)plm(xt+1|x1, . . . , xt). (4)\nIn PPLM, p(c|x1, . . . , xt+1) is approximated by\neither a separate BOW or a linear classiﬁer. In\nour approach, we use plm(xt+2|x1, . . . , xt+1) to\napproximate p(c|x1, . . . , xt+1), that is:\npcg(xt+1|c, x1, . . . , xt)\n∝\n∼ plm(xt+2 = c|x1, . . . , xt+1)plm(xt+1|x1, . . . , xt).\n(5)\nIn practice, we can add a weight w to control the\ninﬂuence of the condition:\npcg(xt+1|c, x1, . . . , xt)\n∝\n∼ pw\nlm(xt+2 = c|x1, . . . , xt+1)plm(xt+1|x1, . . . , xt).\n(6)\nMethod Hyperparameters\nOurs K=12, embed-weights=0.04,\nattention-weights=0.02,\ncondition-weights=0.20,\nearly-stopping=3\nPPLM-BoW gamma=1.5, num-iterations=3,\nstepsize=0.03,\nwindow-length=5, kl-scale\n0.01, gm-scale 0.99\nPPLM-Discrim gamma=1.0 num-iterations=10\nstepsize=0.04 kl-scale=0.01\ngm-scale=0.95\nT able 3: The full set of hyperparameters used in our\nwork. Note that we did not perform any hyperparame-\nter tuning.\nIf there are n conditions, we take the weighted\nmean:\npcg(xt+1|c, x1, . . . , xt)\n∝\n∼ (\nn∏\ni=1\npwi\nlm(xt+2 = c|x1, . . . , xt+1))\n1\nn ·\nplm(xt+1|x1, . . . , xt).\n(7)\nIn our implementation, we ﬁrst use\ntop-K sampling to obtain K next tokens\n{x1\nt+1, . . . , xK\nt+1} and the next-token distri-\nbution. Then, we feed K new sequences\nx1, . . . , xt + {x1\nt+1, . . . , xK\nt+1} into the model\nto have plm(xt+2|x1, . . . , xt, xk\nt+1),∀k ∈\n{1, . . . , K}. Next, we single out those probabil-\nities corresponding to xt+2 = ci. Finally , we\nmultiply two probabilities together with weight\nwi as in Equation\n6 to multinomially sample the\nnext token.\n4 Experiments\n4.1 Experimental Setup\nGPT -2 Language Model. Our language\nmodel is based on the GPT -2, similar to that\nin (\nDathathri et al. , 2020). W e borrowed pre-\ntrained GPT -2 and PPLM models and their\nimplementations from HuggingFace\n2 . In their\nimplementation, GPT -2 is GPT -2 medium.\nHyperparameters. The hyperparameter used in\nthis work is detailed in T able\n3. Due to time\nlimit, we did not perform a hyperparameter sweep\nfor our model. As described in the Appendix\n2 https://huggingface.co/\nof ( Dathathri et al. , 2020), careful hyperparameter\nsearch is vital for its generation quality , and we\nwould imagine that our approach works much bet-\nter with hyperparameter tuning. W e directly used\nthe hyperparameters for PPLM speciﬁed in their\ngithub repo\n3.\nAutomated Evaluation. W e evaluated the gen-\nerated text by its ﬂuency (perplexity) and diver-\nsity (Dist-1, Dist-2, Dist-3), as in (\nDathathri et al. ,\n2020). In our implementation, perplexity is\nmeasured by a separately pre-trained language\nmodel (GPT -2 samll); diversity is measured\nby the percentages of unique n-grams (1-2-3-\ngrams) (\nLi et al. , 2015).\nExternal Sentiment Classiﬁer . For our senti-\nment modeling experiments, we adopted a pre-\ntrained tokenizer, word2vec and sentiment classi-\nﬁer from T witter\n4 to gauge the effectiveness of our\nmodel. The sentiment classiﬁer is a single-layer\nLSTM.\nHuman Evaluation. W e conducted a small-\nscale human evaluation by asking the annotators\nto evaluate the text by its ﬂuency and topic rele-\nvance, both on a scale of 1-5, with 1 being ’not\nreadable’ and 5 being ’very ﬂuent’. By the time\nof submission, we have a total of 12 annotations\nsubmitted. For our human evaluation, we consider\nthe following conditions and preﬁx:\n• Condition: {Military , Religion, Politics, Sci-\nence, Legal, Space, T echnology , Negative,\nPositive}\n• Preﬁx: {‘T o conclude’}\nFor each condition-preﬁx pair, we randomly gener-\nated 10 sentences (each of 60 tokens) and picked\nout 3 reasonable sentences (without degeneration\nissue). W e do this for every pair and for both our\nmethod and PPLM, ending up with 54 sentences.\nUnlike in (\nDathathri et al. , 2020) where A/B test-\ning is conducted as part of its ablation study , we\ndo not have enough time and resources to generate\nstatistically signiﬁcant text pairs.\nDataset. Given that our proposed approach does\nnot require any further ﬁne-tuning, we do not need\nany additional corpus to obtain the conditioned\nNLG.\n3 https://github.com/uber-research/PPLM\n4 https://www .kaggle.com/paoloripamonti/twitter-sentiment-analysis\n4.2 Single Condition Modeling\nW e generated and evaluated samples based on sin-\ngle condition. Similar to ( Dathathri et al. , 2020),\nwe consider the following conditions and preﬁxes:\n• Condition: {Military , Religion, Politics, Sci-\nence, Legal, Space, T echnology}\n• Preﬁx: {‘the chicken’, ‘the house’, ‘the\npotato’, ‘the lake’, ‘the pizza’}\nT able 6 contains a few cherry-picked samples gen-\nerated by our approach. The results of human eval-\nuation are shown in T able\n4. From the T able, we\nobserve the classic perplexity-diversity trade-offs\nseen in dialog research, that although our perplex-\nity is lower than PPLM, we achieve higher diver-\nsity scores. Focusing on the human evaluation\ncolumns, we can see that our approach only lag\nbehind PPLM a little in both attribute relevance\nand ﬂuency , and this is without any hyperparam-\neter search. This suggests not only our approach\nis effective but it is certainly possible to generated\nconditoined natural language using only uncondi-\ntioned lanugage models.\n4.3 Sentiment Modeling\nW e generated and evaluated samples based on sen-\ntiments. W e consider the following conditions and\npreﬁxes:\n• Condition: {Positive, Negative}\n• Preﬁx: {‘the chicken’, ‘the house’, ‘the\npotato’, ‘the lake’, ‘the pizza’}\nT able\n5 contains the results of sentiment modeling.\nW e can see that PPLM has better sentiment mod-\neling in terms of human and objective evaluations.\nW e suspect the reason is that the PPLM-Discrim\nmodel is ﬁne-tuned, and its latent space is updated\n10 times (num-iteration) for each generated sam-\nple, and therefore the quality is much better. This\nsuggests a future extension of our approach is to\nalso have iterative updates.\n4.4 Multiple Conditions Modeling\nThe ﬂexibility of our approach also ensures that\nwe can have more than 2 conditions at the same\ntime, see T able\n1. Compare this to PPLM,\nwhere the conditions are pre-determined and can\nnot be modiﬁed after the ‘guidance models’ are\ntrained (\nDathathri et al. , 2020).\n5 Known Issues\nW e think that without iterative steps (like PPLM),\nit is difﬁcult to generate high-quality results. In\nother words, it is difﬁcult to produce high-quality\nresults with only one-step update. Also, some at-\ntributes are much difﬁcult to obey , so it should re-\nquire more updating steps. Additionally , directly\nadding token embedding and self-attention key-\nvalue pairs greatly increases the perplexity . Fi-\nnally , sometimes degeneration is still observed.\nThis may be due to the fact of adding of token em-\nbeddings, key-values pairs, and changing of output\ndistribution by next-token distribution.\n6 Conclusion\nPast approaches to conditioned NLG still fall short\nin several ways. With that in mind, we took in-\nspiration from recent work (\nDathathri et al. , 2020)\nand proposed four methods for conditioned NLG\nthat is simple, ﬂexible and effective, such that only\nthe original base LM is needed. W e displayed\na few samples for single and multiple conditions\nNLG. Experiments are conducted for single condi-\ntion modeling and sentiment modeling, and these\nsamples are evaluated based on their ﬂuency and\ndiversity . A note on the inadequacy of our ap-\npraoches is appended at the end.\nReferences\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Y osinski, and\nRosanne Liu. 2020. Plug and play language mod-\nels: A simple approach to controlled text generation.\nIn International Conference on Learning Represen-\ntations.\nJessica Ficler and Y oav Goldberg. 2017. Controlling\nlinguistic style aspects in neural language genera-\ntion. In Proceedings of the W orkshop on Stylistic\nV ariation, pages 94–104.\nNitish Shirish Keskar, Bryan McCann, Lav V arshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl - a\nconditional transformer language model for control-\nlable generation. In arXiv preprint arXiv:1909 .\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2015. A diversity-promoting objec-\ntive function for neural conversation models. arXiv\npreprint arXiv:1510.03055 .\nAlec Radford, Karthik Narasimhan, Tim\nSalimans, and Ilya Sutskever. 2018.\nImproving language understanding by generative pre-train ing.\nT opic Method Attribute relevance % ( ↑ better) Perplexity Dist-1 Dist-2 Dist-3 Fluency ( ↑ better)\n(human) ( ↓ better) ( ↑ better) ( ↑ better) ( ↑ better) (human)\nMilitary\nOurs - 22.954 0.610 0.896 0.968 -\nOurs (w/ the following) 4.167 22.797 0.597 0.883 0.964 3.81\nPPLM-BOW 2.694 12.302 0.65 0.876 0.9192 3.472\nReligion\nOurs - 21.227 0.573 0.869 0.957 -\nOurs (w/ the following) 1.472 20.184 0.552 0.845 0.941 3.111\nPPLM-BOW 1.611 12.204 0.533 0.725 0.780 3.583\nPolitics\nOurs - 21.679 0.581 0.866 0.949 -\nOurs (w/ the following) 3.250 20.055 0.555 0.844 0.940 3.139\nPPLM-BOW 3.278 12.524 0.660 0.891 0.935 3.611\nScience\nOurs - 22.645 0.596 0.887 0.967 -\nOurs (w/ the following) 2.806 21.643 0.582 0.874 0.958 3.472\nPPLM-BOW 4.028 13.508 0.640 0.873 0.92 3.778\nLegal\nOurs - 22.457 0.598 0.891 0.968 -\nOurs (w/ the following) 3.278 21.397 0.579 0.868 0.956 3.694\nPPLM-BOW 3.528 12.401 0.662 0.888 0.930 4.028\nSpace\nOurs - 22.529 0.582 0.881 0.965 -\nOurs (w/ the following) 2.333 21.053 0.571 0.859 0.952 3.583\nPPLM-BOW 3.167 12.101 0.540 0.728 0.770 3.639\nT echnology\nOurs - 23.303 0.596 0.887 0.967 -\nOurs (w/ the following) 2.861 23.507 0.578 0.871 0.957 3.250\nPPLM-BOW 3.194 12.489 0.61 0.820 0.860 3.750\nA verage\nOurs - 22.399 0.591 0.882 0.963 -\nOurs (w/ the following) 2.881 21.520 0.573 0.863 0.953 3.437\nPPLM-BOW 3.071 12.504 0.614 0.829 0.874 3.694\nT able 4: Single Condition Modeling: automated and human eva luation results of ours approach and PPLM-BOW .\nThe conditional preﬁx we used here is ‘The following is an art icle about <T opic>’. In addition, we evaluated\nour method with and without the conditional preﬁx. Results here correspond to the avera ge over all samples in\neach topic: <Military>, <Religion>, <Politics>, <Science >, <Legal>, <Space>, <T echnology>. 20 samples are\ngenerated for each topic. Attribute relevance and ﬂuency is rated on a scale of 1-5. Perplexity implies ﬂuency,\nwhich is computed based on an external LM ( Radford et al. , 2018) different from the base LM. Dist-1,2,3 implies\ndiversity, which is the percentage of unique n-grams in the s amples.\nT opic Method Sentiment Acc. (%) Sentiment Acc. (%) Perplexi ty Dist-1 Dist-2 Dist-3 Human Evaluation\n(human) (external classifer) ( ↓ better) ( ↑ better) ( ↑ better) ( ↑ better) Fluency ( ↑ better)\nPositive\nOurs - 60 26.506 0.590 0.895 0.972 -\nOurs (w/ the following) 3.417 72 26.055 0.570 0.879 0.964 3.222\nPPLM-Discrim 3.778 82.5 18.960 0.678 0.910 0.955 3.083\nNegative\nOurs - 52 26.427 0.592 0.901 0.976 -\nOurs (w/ the following) 2.639 37 24.820 0.582 0.886 0.966 2.9 44\nPPLM-Discrim 3.361 70 12.781 0.638 0.889 0.940 3.306\nA verage\nOurs - 56 26.467 0.591 0.898 0.974 -\nOurs (w/ the following) 3.028 54.5 25.438 0.576 0.883 0.965 3 .083\nPPLM-Discrim 3.569 76.25 15.871 0.658 0.898 0.948 3.194\nT able 5: Sentiment Modeling: similar analysis as in T able 4 is presented here. Here, the conditions are <Positive>\nand <Negative>. In addition to the metrics described in T abl e 4, the samples are evaluated by a pretrained sentiment\nclassiﬁer.\n[Military] The chicken wing is the most famous weapon of the Korean military as one of its main war-ﬁghting aids, so the\nname is usually translated into Korean as “the chicken wing. ” . . .\n[Religion] The chicken, which is not necessarily the most religious bird , doesn’t really enjoy eating it. It seems to like\neating eggs. It actually enjoys some sort of cheese that is pa rt of the shell. It will eat . . .\n[Politics] The chicken of politics is not the politician but the political process as embodied by the electorate. It is a\npolitical process that is at odds with the democratic principles on which the country was founded.\n[Military] The horse is an example of a military vehicle. Military vehicles were built to perform certain functions in\nparticular ways and to have particular characteristics. Th ere are lots of examples of that.\" And, he continues, \"W e have . . .\n[Religion] The horse, and the religious person, has a lot to go through: They have to show that their faith and morals are as\nstrong as the horse’s. So, what’s my question to the atheists? If . . .\n[Politics] The horse racing industry’s lobbying groups and its candidates for Congress received $1.6 million in total from\nthe race promoters and their lobbyists, and the money has not gone to a single anti-slavery advocate . . .\n[Military] The pizza in question is from a recent military pizza that I ate out and didn’t really like. It tasted like som ething\nthat had never been served before. It had no char. It didn’t ha ve the sweetness that . . .\n[Religion] The pizza box with the Bible’s signature in the center has the box itself on a shelf. A large c ardboard cutout of\nJesus Christ is painted on the box. \"I was really inspired by a movie . . .\n[Politics] The pizza is in the form of a picture book about politics in the USA. The man was a former student. Now he’s\nrunning to be one of the next president. As a part of this, . . .\n[Military] The potato salad is not something served at military academies. It is not something that most people eat\neveryday . It is a kind of food that is consumed mostly by membe rs of the military. In order to be sure . . .\n[Religion] The potato of religion is that belief in a god — and when you don’t believe in something, or at least can’t ﬁn d\nevidence for it, you take it for granted. But what if you’ ve le arned you . . .\n[Politics] The potato has never been banned in the US. It’s not all bad news for the potato. The Food and Drug\nadministration announced Tuesday that it is ending its approval process for a cancer drug that was made . . .\n[Military] The lake is named after the military ofﬁcer named John T aylor, who led the charge into the Battle o f Lake\nT anganyika on March 1, 1854. The area that today is known as Za mboanga . . .\n[Religion] The lake of the same name on the west side of the mountain, which is name d after the Greek goddess of the\nspring, is now home to a number of historic buildings and cultural treasures. The city was . . .\n[Politics] The lake may be a political issue but there is another issue with this case, as the judge s aid, to take care of.\" A\nhearing to determine whether the water is protected from con tamination is set for Oct . . .\nT able 6: Examples generated from a designed odd combination of condition and preﬁx pairs. Each example is\ncherry-picked from 10 samples. The underlined preﬁx is what the language model is given to generate a passage\nof text (e.g. T o conclude). The controlled attributes are colored and bracketed (e.g . [Science] ) and we highlight\nwords in the passage (e.g. false) that are related to the attributes. Even with the odd combin ation, our method is\nstill able to generate ﬂuent samples respecting to both the a ttribute and preﬁx, though some samples are not really\nsensible.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. In Ope-\nnAI Blog .\nNishant Subramani, Sam Bowman, and Kyunghyun\nCho. 2019. Can unconditional language models\nrecover arbitrary sentences? In arXiv preprint\narXiv:1907.04944 .\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems , pages 6000–6010.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu,\nT om B. Brown, Alec Radford, Dario Amodei,\nPaul Christiano, and Geoffrey Irving. 2019.\nFine-tuning language models from human preferences .\nIn arXiv preprint arXiv:1909.08593 .",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4545005261898041
    },
    {
      "name": "Natural language",
      "score": 0.4299699366092682
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.42600977420806885
    },
    {
      "name": "Natural language processing",
      "score": 0.2833179831504822
    },
    {
      "name": "Geology",
      "score": 0.08579716086387634
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ]
}