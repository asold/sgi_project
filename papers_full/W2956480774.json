{
    "title": "R-Transformer: Recurrent Neural Network Enhanced Transformer",
    "url": "https://openalex.org/W2956480774",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1635915244",
            "name": "Wang Zhiwei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2121904012",
            "name": "Ma Yao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2541231747",
            "name": "Liu, Zitao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2350772420",
            "name": "Tang, Jiliang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2102113734",
        "https://openalex.org/W1989705153",
        "https://openalex.org/W2342395274",
        "https://openalex.org/W2963421552",
        "https://openalex.org/W1819710477",
        "https://openalex.org/W2519091744",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W2547718140",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2184045248",
        "https://openalex.org/W2950855294",
        "https://openalex.org/W2409027918",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2963241221",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2950635152",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W1815076433",
        "https://openalex.org/W2336509392",
        "https://openalex.org/W2262817822",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2792764867",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2886490473",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W1800356822",
        "https://openalex.org/W2064675550"
    ],
    "abstract": "Recurrent Neural Networks have long been the dominating choice for sequence modeling. However, it severely suffers from two issues: impotent in capturing very long-term dependencies and unable to parallelize the sequential computation procedure. Therefore, many non-recurrent sequence models that are built on convolution and attention operations have been proposed recently. Notably, models with multi-head attention such as Transformer have demonstrated extreme effectiveness in capturing long-term dependencies in a variety of sequence modeling tasks. Despite their success, however, these models lack necessary components to model local structures in sequences and heavily rely on position embeddings that have limited effects and require a considerable amount of design efforts. In this paper, we propose the R-Transformer which enjoys the advantages of both RNNs and the multi-head attention mechanism while avoids their respective drawbacks. The proposed model can effectively capture both local structures and global long-term dependencies in sequences without any use of position embeddings. We evaluate R-Transformer through extensive experiments with data from a wide range of domains and the empirical results show that R-Transformer outperforms the state-of-the-art methods by a large margin in most of the tasks. We have made the code publicly available at \\url{https://github.com/DSE-MSU/R-transformer}.",
    "full_text": "R-T RANSFORMER : R ECURRENT NEURAL NETWORK\nENHANCED TRANSFORMER\nZhiwei Wang\nDepartment of Computer Science\nMichigan State University\nwangzh65@msu.edu\nYao Ma\nDepartment of Computer Science\nMichigan State University\nmayao4@msu.edu\nZitao Liu\nAI Lab\nTAL Education Group\nliuzitao@100tal.com\nJiliang Tang\nDepartment of Computer Science\nMichigan State University\ntangjili@msu.edu\nABSTRACT\nRecurrent Neural Networks have long been the dominating choice for sequence\nmodeling. However, it severely suffers from two issues: impotent in capturing\nvery long-term dependencies and unable to parallelize the sequential computation\nprocedure. Therefore, many non-recurrent sequence models that are built on con-\nvolution and attention operations have been proposed recently. Notably, models\nwith multi-head attention such as Transformer have demonstrated extreme effec-\ntiveness in capturing long-term dependencies in a variety of sequence modeling\ntasks. Despite their success, however, these models lack necessary components to\nmodel local structures in sequences and heavily rely on position embeddings that\nhave limited effects and require a considerable amount of design efforts. In this\npaper, we propose the R-Transformer which enjoys the advantages of both RNNs\nand the multi-head attention mechanism while avoids their respective drawbacks.\nThe proposed model can effectively capture both local structures and global long-\nterm dependencies in sequences without any use of position embeddings. We eval-\nuate R-Transformer through extensive experiments with data from a wide range of\ndomains and the empirical results show that R-Transformer outperforms the state-\nof-the-art methods by a large margin in most of the tasks. We have made the code\npublicly available athttps://github.com/DSE-MSU/R-transformer.\n1 I NTRODUCTION\nRecurrent Neural Networks (RNNs) especially its variants such as Long Short-Term Memory\n(LSTM) and Gated Recurrent Unit (GRU) have achieved great success in a wide range of sequence\nlearning tasks including language modeling, speech recognition, recommendation, etc (Mikolov\net al., 2010; Sundermeyer et al., 2012; Graves & Jaitly, 2014; Hinton et al., 2012; Hidasi et al.,\n2015). Despite their success, however, the recurrent structure is often troubled by two notorious\nissues. First, it easily suffers from gradient vanishing and exploding problems, which largely limits\ntheir ability to learn very long-term dependencies (Pascanu et al., 2013). Second, the sequential\nnature of both forward and backward passes makes it extremely difﬁcult, if not impossible, to par-\nallelize the computation, which dramatically increases the time complexity in both training and\ntesting procedure. Therefore, many recently developed sequence learning models have completely\njettisoned the recurrent structure and only rely on convolution operation or attention mechanism that\nare easy to parallelize and allow the information ﬂow at an arbitrary length. Two representative mod-\nels that have drawn great attention are Temporal Convolution Networks(TCN) (Bai et al., 2018) and\nTransformer (Vaswani et al., 2017). In a variety of sequence learning tasks, they have demonstrated\ncomparable or even better performance than that of RNNs (Gehring et al., 2017; Bai et al., 2018;\nDevlin et al., 2018).\n1\narXiv:1907.05572v1  [cs.LG]  12 Jul 2019\nAdd & Norm\nAdd & NormM\nR\nF\nM\nR\nF\nM\nR\nF\nM\nR\nF\nM\nR\nF\nM\nR\nF\nLocal RNN: \nMulti-head Attention: \nFeedForward: Add & Norm\nFigure 1: The illustration of one layer of R-Transformer. There are three different networks that are\narranged hierarchically. In particular, the lower-level is localRNNs that process positions in a local\nwindow sequentially (This ﬁgure shows an example of local window of size 3); The middle-level\nis multi-head attention networks which capture the global long-term dependencies; The upper-level\nis Position-wise feedforward networks that conduct non-linear feature transformation. These three\nnetworks are connected by a residual and layer normalization operation. The circles with dash line\nare the paddings of the input sequence\nThe remarkable performance achieved by such models largely comes from their ability to capture\nlong-term dependencies in sequences. In particular, the multi-head attention mechanism in Trans-\nformer allows every position to be directly connected to any other positions in a sequence. Thus,\nthe information can ﬂow across positions without any intermediate loss. Nevertheless, there are\ntwo issues that can harm the effectiveness of multi-head attention mechanism for sequence learn-\ning. The ﬁrst comes from the loss of sequential information of positions as it treats every position\nidentically. To mitigate this problem, Transformer introduces position embeddings, whose effects,\nhowever, have been shown to be limited (Dehghani et al., 2018; Al-Rfou et al., 2018). In addition,\nit requires considerable amount of efforts to design more effective position embeddings or different\nways to incorporate them in the learning process (Dai et al., 2019). Second, while multi-head atten-\ntion mechanism is able to learn the global dependencies, we argue that it ignores the local structures\nthat are inherently important in sequences such as natural languages. Even with the help of position\nembeddings, the signals at local positions can still be very weak as the number of other positions is\nsigniﬁcantly more.\nTo address the aforementioned limitations of the standard Transformer, in this paper, we propose\na novel sequence learning model, termed as R-Transformer. It is a multi-layer architecture built\non RNNs and the standard Transformer, and enjoys the advantages of both worlds while naturally\navoids their respective drawbacks. More speciﬁcally, before computing global dependencies of po-\nsitions with the multi-head attention mechanism, we ﬁrstly reﬁne the representation of each position\nsuch that the sequential and local information within its neighborhood can be compressed in the\nrepresentation. To do this, we introduce a local recurrent neural network, referred to as LocalRNN,\nto process signals within a local window ending at a given position. In addition, the LocalRNN\noperates on local windows of all the positions identically and independently and produces a latent\nrepresentation for each of them. In this way, the locality in the sequence is explicitly captured. In\naddition, as the local window is sliding along the sequence one position by one position, the global\nsequential information is also incorporated. More importantly, because the localRNN is only ap-\nplied to local windows, the aforementioned two drawbacks of RNNs can be naturally mitigated. We\nevaluate the effectiveness of R-Transformer with a various of sequence learning tasks from different\ndomains and the empirical results demonstrate that R-Transformer achieves much stronger perfor-\nmance than both TCN and standard Transformer as well as other state-of-the-art sequence models.\nThe rest of the paper is organized as follows: Section 2 discusses the sequence modeling problem\nwe aim to solve; The proposed R-Transformer model is presented in Section 3. In Section 4, we\n2\ndescribe the experimental details and discuss the results. The related work is brieﬂy reviewed in\nSection 5. Section 6 concludes this work.\n2 S EQUENCE MODELING PROBLEM\nBefore introducing the proposed R-Transformer model, we formally describe the sequence modeling\nproblem. Given a sequence of length N: x1,x2,··· ,xN , we aim to learn a function that maps the\ninput sequence into a label space Y: (f : XN →Y). Formally,\ny= f(x1,x2,··· ,xN ) (1)\nwhere y ∈ Yis the label of the input sequence. Depending on the deﬁnition of label y, many\ntasks can be formatted as the sequence modeling problem deﬁned above. For example, in language\nmodeling task, xt is the character/word in a textual sentence and y is the character/word at next\nposition (Mikolov et al., 2010); in session-based recommendation, xt is the user-item interaction\nin a session and yis the future item that users will interact with (Hidasi et al., 2015); when xt is a\nnucleotide in a DNA sequence andyis its function, this problem becomes a DNA function prediction\ntask (Quang & Xie, 2016). Note that, in this paper, we do not consider the sequence-to-sequence\nlearning problems. However, the proposed model can be easily extended to solve these problems\nand we will leave it as one future work.\n3 T HE R-T RANSFORMER MODEL\nThe proposed R-Transformer consists of a stack of identical layers. Each layer has 3 components\nthat are organized hierarchically and the architecture of the layer structure is shown in Figure 1.\nAs shown in the ﬁgure, the lower level is the local recurrent neural networks that are designed to\nmodel local structures in a sequence; the middle level is a multi-head attention that is able to capture\nglobal long-term dependencies; and the upper level is a position-wise feedforward networks which\nconducts a non-linear feature transformation. Next, we describe each level in detail.\n3.1 L OCAL RNN: M ODELING LOCAL STRUCTURES\nSequential data such as natural language inherently exhibits strong local structures. Thus, it is desir-\nable and necessary to design components to model such locality. In this subsection, we propose to\ntake the advantage of RNNs to achieve this. Unlike previous works where RNNs are often applied\nto the whole sequence, we instead reorganize the original long sequence into many short sequences\nwhich only contain local information and are processed by a shared RNN independently and identi-\ncally. In particular, we construct a local window of sizeMfor each target position such that the local\nwindow includes M consecutive positions and ends at the target position. Thus, positions in each\nlocal window form a local short sequence, from which the shared RNN will learn a latent represen-\ntation. In this way, the local structure information of each local region of the sequence is explicitly\nincorporated in the learned latent representations. We refer to the shared RNN as LocalRNN. Com-\nparing to original RNN operation, LocalRNN only focuses on local short-term dependencies without\nconsidering any long-term dependencies. Figure 2 shows the different between original RNN and\nLocalRNN operations. Concretely, given the positions xt−M−1,xt−M−2,··· ,xt of a local short\nsequence of length M, the LocalRNN processes them sequentially and outputsM hidden states, the\nlast of which is used as the representation of the local short sequences:\nht = LocalRNN(xt−M−1,xt−M−2,··· ,xt) (2)\nwhere RNN denotes any RNN cell such as Vanilla RNN cell, LSTM, GRU, etc. To enable the model\nto process the sequence in an auto-regressive manner and take care that no future information is\navailable when processing one position, we pad the input sequence by(M−1) positions before the\nstart of a sequence. Thus, from sequence perspective, the LocalRNN takes an input sequence and\noutputs a sequence of hidden representations that incorporate information of local regions:\nh1,h2,··· ,hN = LocalRNN(x1,x2,··· ,xN ) (3)\nThe localRNN is analogous to 1-D Convolution Neural Networks where each local window is pro-\ncessed by convolution operations. However, the convolution operation completely ignores the se-\nquential information of positions within the local window. Although the position embeddings have\n3\nR\n(a) Original RNN.\nR R R (b) Local RNN.\nFigure 2: An illustration of the original and local RNN. In contrast to orignal RNN which maintains\na hidden state at each position summarizing all the information seen so far, LocalRNN only operates\non positions within a local window. At each position, LocalRNN will produce a hidden state that\nrepresents the information in the local window ending at that position.\nbeen proposed to mitigate this problem, a major deﬁciency of this approach is that the effective-\nness of the position embedding could be limited; thus it requires considerable amount of extra ef-\nforts (Gehring et al., 2017). On the other hand, the LocalRNN is able to fully capture the sequential\ninformation within each window. In addition, the one-by-one sliding operation also naturally incor-\nporates the global sequential information.\nDiscussion: RNNs have long been a dominating choice for sequence modeling but it severely suffers\nfrom two problems – The ﬁrst one is its limited ability to capture the long-term dependencies and the\nsecond one is the time complexity, which is linear to the sequence length. However, in LocalRNN,\nthese problems are naturally mitigated. Because the LocalRNN is applied to a short sequence within\na local window of ﬁxed size, where no long-term dependency is needed to capture. In addition, the\ncomputation procedures for processing the short sequences are independent of each other. Therefore,\nit is very straightforward for the parallel implementation (e.g., using GPUs), which can greatly\nimprove the computation efﬁciency.\n3.2 C APTURING THE GLOBAL LONG -TERM DEPENDENCIES WITH MULTI -HEAD\nATTENTION\nThe RNNs at the lower level introduced in the previous subsection will reﬁne representation of each\npositions such that it incorporates its local information. In this subsection, we build a sub-layer\non top of the LocalRNN to capture the global long-term dependencies. We term it as pooling sub-\nlayer because it functions similarly to the pooling operation in CNNs. Recent works have shown\nthat the multi-head attention mechanism is extremely effective to learn the long-term dependencies,\nas it allows a direct connection between every pair of positions. More speciﬁcally, in the multi-\nhead attention mechanism, each position will attend to all the positions in the past and obtains\na set of attention scores that are used to reﬁne its representation. Mathematically, given current\nrepresentations h1,h2,··· ,ht, the reﬁned new representations ut are calculated as:\nut = MultiHeadAttention(h1,h2,··· ,ht) (4)\n= Concatenation(head1(ht),head2(ht),··· ,headk(ht))Wo\nwhere headk(ht) is the result of kth attention pooling and Wo is a linear projection matrix. Con-\nsidering both efﬁciency and effectiveness, the scaled dot product is used as the attention func-\ntion (Vaswani et al., 2017). Speciﬁcally, headi(ht) is the weighted sum of all value vectors and\n4\nthe weights are calculated by applying attention function to all the query, key pairs:\n{α1,α2,···αn }= Softmax({<q,k 1 >√\n(dk)\n,<q,k 2 >√\n(dk)\n,··· ,<q,k n >√\n(dk)\n}) (5)\nheadi(ht) =\nn∑\nj=1\nαjvj\nwhere q, ki, and vi are the query, key, and value vectors and dk is the dimension of ki. More-\nover, q, ki, and vi are obtained by projecting the input vectors into query, key and value spaces,\nrespectively (Vaswani et al., 2017). They are formally deﬁned as:\nq,ki,vi = Wqht,Wkhi,Wvhi (6)\nwhere Wq, Wk and Wv are the projection matrices and each attention pooling headi has its own\nprojection matrices. As shown in Eq. (5), each headi is obtained by letting ht attending to all the\n“past” positions, thus any long-term dependencies between ht and hi can be captured. In addition,\ndifferent heads will focus on dependencies in different aspects. After obtaining the reﬁned rep-\nresentation of each position by the multi-head attention mechanism, we add a position-wise fully\nconnected feed-forward network sub-layer, which is applied to each position independently and\nidentically. This feedforward network transforms the features non-linearly and is deﬁned as follows:\nFeedForward (mt) =max(0,utW1 + b1)W2 + b2 (7)\nFollowing (Vaswani et al., 2017), We add a residual (He et al., 2016) and layernorm (Ba et al., 2016)\nconnection between all the sub-layers.\n3.3 O VERALL ARCHITECTURE OF R-T RANSFORMER\nWith all the aforementioned model components, we can now give a formal description of the overall\narchitecture of an N-layer R-Transformer. For the ith layer (i∈{1,2,···N}):\nhi\n1,hi\n2,··· ,hi\nT = LocalRNN(xi\n1,xi\n2,··· ,xi\nT ) (8)\nˆhi\n1,ˆhi\n2,··· ,ˆhi\nT = LayerNorm(hi\n1 + xi\n1,hi\n2 + xi\n2,··· ,hi\nT + xi\nT )\nui\n1,ui\n2,··· ,ui\nT = MultiHeadAttention(ˆhv1,ˆhi\n2,··· ,ˆhi\nT )\nˆui\n1,ˆui\n2,··· ,ˆui\nT = LayerNorm(ui\n1 + ˆhi\n1,ui\n2 + ˆhi\n2,··· ,ui\nT + ˆhi\nT )\nmi\n1,mi\n2,··· ,mi\nT = FeedForward (ˆui\n1,ˆui\n2,··· ,ˆui\nT )\nxi+1\n1 ,xi+1\n2 ,··· ,xi+1\nT = LayerNorm(mi\n1 + ˆui\n1,mi\n2 + ˆui\n2,··· ,mi\nT + ˆui\nT )\nwhere T is the length of the input sequence and xi\nt is the input position of the layer iat time step t.\nComparing with TCN:R-Transformer is partly motivated by the hierarchical structure in TCN Bai\net al. (2018), thus, we make a detailed comparison here. In TCN, the locality in sequences in\ncaptured by convolution ﬁlters. However, the sequential information within each receptive ﬁeld\nis ignored by convolution operations. In contrast, the LocalRNN structure in R-Transformer can\nfully incorporate it by the sequential nature of RNNs. For modeling global long-term dependencies,\nTCN achieves it with dilated convolutions that operate on nonconsecutive positions. Although such\noperation leads to larger receptive ﬁelds in lower-level layers, it misses considerable amount of\ninformation from a large portion of positions in each layer. On the other hand, the multi-head\nattention pooling in R-Transformer considers every past positions and takes much more information\ninto consideration than TCN.\nComparing with Transformer:The proposed R-Transformer and standard Transformer enjoys\nsimilar long-term memorization capacities thanks to the multi-head attention mechanism (Vaswani\net al., 2017). Nevertheless, two important features distinguish R-Transformer from the standard\nTransformer. First, R-Transformer explicitly and effectively captures the locality in sequences with\nthe novel LocalRNN structure while standard Transformer models it very vaguely with multi-head\nattention that operates on all of the positions. Second, R-Transformer does not rely on any position\nembeddings as Transformer does. In fact, the beneﬁts of simple position embeddings are very\n5\nTable 1: MNIST classiﬁcation task results. Italic numbers denote that the results are directly copied\nfrom other papers that have the same settings.\nModel # of layers / hidden size Test Accuracy(%)\nRNN (Bai et al., 2018) - 21.5\nGRU (Bai et al., 2018) - 96.2\nLSTM (Bai et al., 2018) 1/ 130 87.2\nTCN (Bai et al., 2018) 8 /25 99.0\nTransformer 8/32 98.2\nR-Transformer 8/32 99.1\nlimited (Al-Rfou et al., 2018) and it requires considerable amount of efforts to design effective\nposition embeddings as well as proper ways to incorporate them (Dai et al., 2019). In the next\nsection, we will empirically demonstrate the advantages of R-Transformer over both TCN and the\nstandard Transformer.\n4 E XPERIMENT\nIn this section, we evaluate R-Transformer with sequential data from various domains including im-\nages, audios and natural languages and compare it with canonical recurrent architectures (Vanilla\nRNN, GRU, LSTM) and two of the most popular generic sequence models that do not have any re-\ncurrent structures, namely, TCN and Transformer. For all the tasks, Transformer and R-Transformer\nwere implemented with Pytorch and the results for canonical recurrent architectures and TCN were\ndirectly copied from Bai et al. (2018) as we follow the same experimental settings. In addition,\nto make the comparison fair, we use the same set of hyperparameters (i.e, hidden size, number of\nlayers, number of heads) for R-Transformer and Transformer. Moreover, unless speciﬁed otherwise,\nfor training, all models are trained with same optimizer and learning rate is chosen from the same\nset of values according to validation performance. In addition, the learning rate annealed such that\nit is reduced when validation performance reaches plateau.\n4.1 P IXEL -BY-PIXEL MNIST: S EQUENCE CLASSIFICATION\nThis task is designed to test model ability to memorize long-term dependencies. It was ﬁrstly pro-\nposed by Le et al. (2015) and has been used by many previous works (Wisdom et al., 2016; Chang\net al., 2017; Zhang et al., 2016; Krueger et al., 2016). Following previous settings, we rescale each\n28 ×28 image in MNIST dataset LeCun et al. (1998) into a 784 ×1 sequence, which will be clas-\nsiﬁed into ten categories (each image corresponds to one of the digits from 0 to 9) by the sequence\nmodels. Since the rescaling could make pixels that are connected in the origin images far apart from\neach other, it requires the sequence models to learn very long-term dependencies to understand the\ncontent of each sequence. The dataset is split into training and testing sets as same as the default\nones in Pytorch(version 1.0.0) 1. The model hyperparameters and classiﬁcation accuracy are re-\nported in Table 1. From the table, it can be observed that ﬁrstly, RNNs based methods generally\nperform worse than others. This is because the input sequences exhibit very long-term dependen-\ncies and it is extremely difﬁcult for RNNs to memorize them. On the other hand, methods that build\ndirect connections among positions, i.e., Transformer, TCN, achieve much better results. It is also\ninteresting to see that TCN is slightly better than Transformer, we argue that this is because the stan-\ndard Transformer cannot model the locality very well. However, our proposed R-Transformer that\nleverages LocalRNN to incorporate local information, has achieved better performance than TCN.\n6\nTable 2: Polyphonic music modeling. Italic numbers denote that the results are directly copied from\nother papers that have the same settings.\nModel # of layers / hidden size NLL\nRNN (Bai et al., 2018) - 4.05\nGRU (Bai et al., 2018) - 3.46\nLSTM (Bai et al., 2018) - 3.29\nTCN (Bai et al., 2018) 4 /150 3.07\nTransformer 3/160 3.34\nR-Transformer 3/160 2.37\nTable 3: Character-level language modeling. Italic numbers denote that the results are directly\ncopied from other papers that have the same settings.\nModel # of layers / hidden size NLL\nRNN (Bai et al., 2018) - 1.48\nGRU (Bai et al., 2018) - 1.37\nLSTM (Bai et al., 2018) 2 /600 1.36\nTCN (Bai et al., 2018) 3 /450 1.31\nTransformer 3/512 1.45\nR-Transformer 3/512 1.24\n4.2 N OTTINGHAM : POLYPHONIC MUSIC MODELING\nNext, we evaluate R-Transformer on the task of polyphonic music modeling with Nottingham\ndataset (Boulanger-Lewandowski et al., 2012). This dataset collects British and American folk tunes\nand has been commonly used in previous works to investigate the model’s ability for polyphonic mu-\nsic modeling (Boulanger-Lewandowski et al., 2012; Chung et al., 2014; Bai et al., 2018). Following\nthe same setting in Bai et al. (2018), we split the data into training, validation, and testing sets which\ncontains 694, 173 and 170 tunes, respectively. The learning rate is chosen from{5e−4,5e−5,5e−6}\nand dropout with probability of 0.1 is used to avoid overﬁtting. Moreover, gradient clipping is used\nduring the training process. We choose negative log-likelihood (NLL) as the evaluation metrics and\nlower value indicates better performance. The experimental results are shown in Table 2. Both\nLTSM and TCN outperform Transformer in this task. We suspect this is because these music tunes\nexhibit strong local structures. While Transformer is equipped with multi-head attention mechanism\nthat is effective to capture long-term dependencies, it fails to capture local structures in sequences\nthat could provide strong signals. On the other hand, R-Transformer enhanced by LocalRNN has\nachieved much better results than Transformer. In addition, it also outperforms TCN by a large mar-\ngin. This is expected because TCN tends to ignore the sequential information in the local structure,\nwhich can play an important role as suggested by (Gehring et al., 2017).\n4.3 P ENN TREEBANK : LANGUAGE MODELING\nIn this subsection, we further evaluate R-Transformer’s ability on both character-level and word-\nlevel language modeling tasks. The dataset we use is PennTreebank(PTB) (Marcus et al., 1993) that\ncontains 1 million words and has been extensively used by previous works to investigate sequence\nmodels (Chen & Goodman, 1999; Chelba & Jelinek, 2000; Kim et al., 2016; Tran et al., 2016). For\ncharacter-level language modeling task, the model is required to predict the next character given a\n1https://pytorch.org\n7\nTable 4: Word-level language modeling. Italic numbers denote that the results are directly copied\nfrom other papers that have the same settings.\nModel # of layers / hidden size Perplexity\nRNN (Bai et al., 2018) - 114.50\nGRU (Bai et al., 2018) - 92.48\nLSTM (Bai et al., 2018) 3 /700 78.93\nTCN (Bai et al., 2018) 4 /600 88.68\nTransformer 3/128 122.37\nR-Transformer 3/128 84.38\ncontext. Following the experimental settings in Bai et al. (2018), we split the dataset into train-\ning, validation and testing sets that contains 5059K, 396K and 446K characters, respectively. For\nTransformer and R-Transformer, the learning rate is chosen from {1,2,3}and dropout rate is 0.15.\nGradient clipping is also used during the training process. The negative log-likelihood (NLL) is used\nto measure the predicting performance.\nFor word-level language modeling, the models are required to predict the next word given the con-\ntextual words. Similarly, we follow previous works and split PTB into training, validation, and\ntesting sets with 888K, 70K and 79K words, respectively. The vocabulary size of PTB is 10K. As\nwith character-level language modeling,the learning rate is chosen from {1,2,3}for Transformer\nand R-Transformer and dropout rate is 0.35. The prediction performance is evaluated with perplex-\nity, the lower value of which denotes better performance.\nThe experimental results of character-level and word-level language modeling tasks are shown in\nTable 3 and Table 4, respectively. Several observations can be made from the Table 3. First, Trans-\nformer performs only slightly better than RNN while much worse than other models. The reason\nfor this observation is similar to the case of polyphonic music modeling task that language exhibits\nstrong local structures and standard Transformer can not fully capture them. Second, TCN achieves\nbetter results than all of the RNNs, which is attributed to its ability to capture both local structures\nand long-term dependencies in languages. Notably, for both local structures and long-term depen-\ndencies, R-Transformer has more powerful components, i.e., LocalRNN and Multi-head attention,\nthan TCN. Therefore, it is not surprising to see that R-Transformer achieves signiﬁcantly better re-\nsults. The results for word-level language modeling in Table 4 show similar trends, with the only\nexception that LSTM achieves the best results among all the methods.\nIn summary, previous results have shown that the standard Transformer can achieve better results\nthan RNNs when sequences exhibit very long-term dependencies, i.e., sequential MNIST while\nits performance could drop dramatically when strong locality exists in sequences, i.e., polyphonic\nmusic and language. Meanwhile, TCN is a very strong sequence model that can effectively learn\nboth local structures and long-term dependencies and has very stable performance in different tasks.\nMore importantly, the proposed R-Transformer that combines a lower level LocalRNN and a higher\nlevel multi-head attention, outperforms both TCN and Transformer by a large margin consistently\nin most of the tasks.\n5 R ELATED WORK\nRecurrent Neural Networks including its variants such LSTM (Hochreiter & Schmidhuber, 1997)\nand GRU (Cho et al., 2014) have long been the default choices for generic sequence modeling.\nA RNN sequentially processes each position in a sequence and maintains an internal hidden state\nto compresses information of positions that have been seen. While its design is appealing and it\nhas been successfully applied in various tasks, several problems caused by its recursive structures\nincluding low computation efﬁciency and gradient exploding or vanishing make it ineffective when\nlearning long sequences. Therefore, in recent years, a lot of efforts has been made to develop models\n8\nwithout recursive structures and they can be roughly divided into two categories depending whether\nthey rely on convolutions operations or not.\nThe ﬁrst category includes models that mainly built on convolution operations. For example, van\nden Oord et al. have designed an autoregressive WaveNet that is based on causal ﬁlters and dilated\nconvolution to capture both global and local information in raw audios (Van Den Oord et al., 2016).\nGhring et al. has successfully replace traditional RNN based encoder and decoder with convolu-\ntional ones and outperforms LSTM setup in neural machine translation tasks (Gehring et al., 2017;\n2016). Moreover, researchers introduced gate mechanism into convolutions structures to model se-\nquential dependencies in languages (Dauphin et al., 2017). Most recently, a generic architecture\nfor sequence modeling, termed as Temporal Convolutional Networks (TCN), that combines compo-\nnents from previous works has been proposed in (Bai et al., 2018). Authors in (Bai et al., 2018) have\nsystematically compared TCN with canonical recurrent networks in a wide range of tasks and TCN\nis able achieve better performance in most cases. Our R-transformer is motivated by works in this\ngroup in a sense that we ﬁrstly models local information and then focus on global ones.\nThe most popular works in second category are those based on multi-head attention mechanism.\nThe multi-head attention mechanism was ﬁrstly proposed in Vaswani et al. (2017), where impressive\nperformance in machine translation task has been achieved with Transformer. It was then frequently\nused in other sequence learning models (Devlin et al., 2018; Dehghani et al., 2018; Dai et al., 2019).\nThe success of multi-head attention largely comes from its ability to learn long-term dependencies\nthrough direct connections between any pair of positions. However, it heavily relies on position\nembeddings that have limited effects and require a fair amount of effort to design effective ones.\nIn addition, our empirical results shown that the local information could easily to be ignored by\nmulti-head attention even with the existence of position embeddings. Unlike previously proposed\nTransformer-like models, R-Transformer in this work leverages the strength of RNN and is able\nmodel the local structures effectively without the need of any position embeddings.\n6 C ONCLUSION\nIn this paper, we propose a novel generic sequence model that enjoys the advantages of both RNN\nand the multi-head attention while mitigating their disadvantages. Speciﬁcally, it consists of a Lo-\ncalRNN that learns the local structures without suffering from any of the weaknesses of RNN and\na multi-head attention pooling that effectively captures long-term dependencies without any help of\nposition embeddings. In addition, the model can be easily implemented with full parallelization over\nthe positions in a sequence. The empirical results on sequence modeling tasks from a wide range of\ndomains have demonstrated the remarkable advantages of R-Transformer over state-of-the-art non-\nrecurrent sequence models such as TCN and standard Transformer as well as canonical recurrent\narchitectures.\nREFERENCES\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level lan-\nguage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional\nand recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.\nNicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal depen-\ndencies in high-dimensional sequences: Application to polyphonic music generation and tran-\nscription. arXiv preprint arXiv:1206.6392, 2012.\nShiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael\nWitbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks.\nIn Advances in Neural Information Processing Systems, pp. 77–87, 2017.\nCiprian Chelba and Frederick Jelinek. Structured language modeling. Computer Speech & Lan-\nguage, 14(4):283–332, 2000.\n9\nStanley F Chen and Joshua Goodman. An empirical study of smoothing techniques for language\nmodeling. Computer Speech & Language, 13(4):359–394, 1999.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of\ngated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\nZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860, 2019.\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with\ngated convolutional networks. In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 933–941. JMLR. org, 2017.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal\ntransformers. arXiv preprint arXiv:1807.03819, 2018.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nJonas Gehring, Michael Auli, David Grangier, and Yann N Dauphin. A convolutional encoder model\nfor neural machine translation. arXiv preprint arXiv:1611.02344, 2016.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional\nsequence to sequence learning. In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 1243–1252. JMLR. org, 2017.\nAlex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural\nnetworks. In International conference on machine learning, pp. 1764–1772, 2014.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n770–778, 2016.\nBal´azs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based rec-\nommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015.\nGeoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, An-\ndrew Senior, Vincent Vanhoucke, Patrick Nguyen, Brian Kingsbury, et al. Deep neural networks\nfor acoustic modeling in speech recognition. IEEE Signal processing magazine, 29, 2012.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nYoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language\nmodels. In Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.\nDavid Krueger, Tegan Maharaj, J´anos Kram´ar, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary\nKe, Anirudh Goyal, Yoshua Bengio, Aaron Courville, and Chris Pal. Zoneout: Regularizing rnns\nby randomly preserving hidden activations. arXiv preprint arXiv:1606.01305, 2016.\nQuoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks\nof rectiﬁed linear units. arXiv preprint arXiv:1504.00941, 2015.\nYann LeCun, L´eon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied\nto document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\nMitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated\ncorpus of english: The penn treebank. 1993.\n10\nTom´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan ˇCernock`y, and Sanjeev Khudanpur. Recurrent\nneural network based language model. In Eleventh annual conference of the international speech\ncommunication association, 2010.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural\nnetworks. In International conference on machine learning, pp. 1310–1318, 2013.\nDaniel Quang and Xiaohui Xie. Danq: a hybrid convolutional and recurrent deep neural network\nfor quantifying the function of dna sequences. Nucleic acids research, 44(11):e107–e107, 2016.\nMartin Sundermeyer, Ralf Schl ¨uter, and Hermann Ney. Lstm neural networks for language mod-\neling. In Thirteenth annual conference of the international speech communication association ,\n2012.\nKe Tran, Arianna Bisazza, and Christof Monz. Recurrent memory networks for language modeling.\narXiv preprint arXiv:1601.01272, 2016.\nA¨aron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew W Senior, and Koray Kavukcuoglu. Wavenet: A generative model for\nraw audio. SSW, 125, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nScott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity\nunitary recurrent neural networks. In Advances in Neural Information Processing Systems , pp.\n4880–4888, 2016.\nSaizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan R Salakhutdinov,\nand Yoshua Bengio. Architectural complexity measures of recurrent neural networks. InAdvances\nin neural information processing systems, pp. 1822–1830, 2016.\n11"
}