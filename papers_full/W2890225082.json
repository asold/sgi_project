{
  "title": "On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling",
  "url": "https://openalex.org/W2890225082",
  "year": 2018,
  "authors": [
    {
      "id": null,
      "name": "Gerz, D",
      "affiliations": [
        "University of Cambridge",
        "Language Science (South Korea)"
      ]
    },
    {
      "id": null,
      "name": "Vulić, I",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ponti, EM",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Reichart, R",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": null,
      "name": "Korhonen, A",
      "affiliations": [
        "University of Cambridge",
        "Language Science (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1591801644",
    "https://openalex.org/W1523296404",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2515351093",
    "https://openalex.org/W2093390569",
    "https://openalex.org/W2803214681",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2270190199",
    "https://openalex.org/W2508661145",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2741986357",
    "https://openalex.org/W2618101654",
    "https://openalex.org/W4239818589",
    "https://openalex.org/W2401969231",
    "https://openalex.org/W2740599975",
    "https://openalex.org/W2142493409",
    "https://openalex.org/W2883158411",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2964318358",
    "https://openalex.org/W2963216505",
    "https://openalex.org/W2161278536",
    "https://openalex.org/W2963582782",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W1769936356",
    "https://openalex.org/W2108966388",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W574924521",
    "https://openalex.org/W2087735403",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2118974695",
    "https://openalex.org/W4300553569",
    "https://openalex.org/W2963324947",
    "https://openalex.org/W2609370997",
    "https://openalex.org/W2963162382",
    "https://openalex.org/W4297692367",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W2963899393",
    "https://openalex.org/W2963956670",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2000196122",
    "https://openalex.org/W2321470647",
    "https://openalex.org/W2129250947",
    "https://openalex.org/W4206765718",
    "https://openalex.org/W1899794420",
    "https://openalex.org/W2251565024",
    "https://openalex.org/W2046830495",
    "https://openalex.org/W2166306133",
    "https://openalex.org/W2273474218",
    "https://openalex.org/W2810095012",
    "https://openalex.org/W2963099225",
    "https://openalex.org/W2963421945",
    "https://openalex.org/W2251654079",
    "https://openalex.org/W2154579312",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W2963069394",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W1505680913",
    "https://openalex.org/W3102543018",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2926149658",
    "https://openalex.org/W3103403019",
    "https://openalex.org/W2963901637",
    "https://openalex.org/W2141440284",
    "https://openalex.org/W22168010",
    "https://openalex.org/W4244165226"
  ],
  "abstract": "A key challenge in cross-lingual NLP is developing general language-independent architectures that are equally applicable to any language. However, this ambition is largely hampered by the variation in structural and semantic properties, i.e. the typological profiles of the world's languages. In this work, we analyse the implications of this variation on the language modeling (LM) task. We present a large-scale study of state-of-the art n-gram based and neural language models on 50 typologically diverse languages covering a wide variety of morphological systems. Operating in the full vocabulary LM setup focused on word-level prediction, we demonstrate that a coarse typology of morphological systems is predictive of absolute LM performance. Moreover, fine-grained typological features such as exponence, flexivity, fusion, and inflectional synthesis are borne out to be responsible for the proliferation of low-frequency phenomena which are organically difficult to model by statistical architectures, or for the meaning ambiguity of character n-grams. Our study strongly suggests that these features have to be taken into consideration during the construction of next-level language-agnostic LM architectures, capable of handling morphologically complex languages such as Tamil or Korean.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 316–327\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n316\nOn the Relation between Linguistic Typology and (Limitations of)\nMultilingual Language Modeling\nDaniela Gerz1∗, Ivan Vuli´c1∗, Edoardo Maria Ponti1, Roi Reichart2, Anna Korhonen1\n1Language Technology Lab, DTAL, University of Cambridge\n2Faculty of Industrial Engineering and Management, Technion, IIT\n1{dsg40, iv250, ep490, alk23}@cam.ac.uk\n2roiri@ie.technion.ac.il\nAbstract\nA key challenge in cross-lingual NLP is de-\nveloping general language-independent archi-\ntectures that are equally applicable to any lan-\nguage. However, this ambition is largely ham-\npered by the variation in structural and seman-\ntic properties, i.e. the typological proﬁles of\nthe world’s languages. In this work, we anal-\nyse the implications of this variation on the lan-\nguage modeling (LM) task. We present a large-\nscale study of state-of-the art n-gram based\nand neural language models on 50 typolog-\nically diverse languages covering a wide va-\nriety of morphological systems. Operating in\nthe full vocabulary LM setup focused on word-\nlevel prediction, we demonstrate that a coarse\ntypology of morphological systems is predic-\ntive of absolute LM performance. Moreover,\nﬁne-grained typological features such as expo-\nnence, ﬂexivity, fusion, and inﬂectional synthe-\nsis are borne out to be responsible for the pro-\nliferation of low-frequency phenomena which\nare organically difﬁcult to model by statisti-\ncal architectures, or for the meaning ambiguity\nof character n-grams. Our study strongly sug-\ngests that these features have to be taken into\nconsideration during the construction of next-\nlevel language-agnostic LM architectures, ca-\npable of handling morphologically complex\nlanguages such as Tamil or Korean.\n1 Introduction\nDeep learning has allowed NLP algorithms to dis-\npose of manually-crafted features, and to virtually\nachieve language independence. However, their per-\nformance still varies noticeably across languages\ndue to different underlying data distributions (Ben-\nder, 2013; O’Horan et al., 2016). Linguistic ty-\npology, the systematic comparison of the world’s\nlanguages, holds promise to explain these idiosyn-\ncrasies and interpret statistical models in terms of\nvariation in language structures (Ponti et al., 2017).\n∗Both authors equally contributed to this work.\nIn order to evaluate how cross-lingual structural\nvariation hinders the design of effective general-\npurpose algorithms, we propose the task of lan-\nguage modeling (LM) as a testbed. In particular,\nwe opt for a full-vocabulary setup where no word\nencountered at training time is treated as an un-\nknown symbol, in order to a) ensure a fair compari-\nson across languages with different word frequency\nrates and b) avoid setting an arbitrary threshold on\nvocabulary size (Cotterell et al., 2018).\nAlthough there has recently been a tendency\ntowards expanding test language samples, the\ndatasets considered in previous works (Botha and\nBlunsom, 2014; Vania and Lopez, 2017; Kawakami\net al., 2017; Cotterell et al., 2018) are not entirely\nadequate yet to represent the typological variation\nand to ground cross-lingual generalisations empiri-\ncally. Hence, we test several LM architectures (in-\ncluding n-gram, neural, and character-aware mod-\nels) on a novel and wider set of 50 languages sam-\npled according to stratiﬁcation principles.\nThrough this large-scale multilingual analysis,\nwe shed new light on the current limitations of\nstandard LM models and offer support to fur-\nther developments in multilingual NLP. In par-\nticular, we demonstrate that the previous ﬁxed-\nvocabulary assumption in fact ignores the limita-\ntions of language modeling for morphologically\nrich languages. Moreover, we ﬁnd a strong corre-\nlation across the board between LM model per-\nformances and the type of morphological system\nadopted in each language.\nTo motivate this correlation we show how ﬁne-\ngrained typological properties interact with the fre-\nquency distribution (Zipf, 1949) by regulating word\nboundaries and the proliferation of word forms;\nand 2) with the mapping between morphemes (here\nintended as character n-grams) and meaning, by\npossibly blurring it.\nThe paper is organised as follows. After provid-\n317\ning a short overview of multilingual LM and its\npossible setups (§2), we describe the cross-lingual\nvariation in morphological systems and propose a\nnovel typologically diverse dataset for LM in §3.\nWe outline the data in §4 and benchmarked lan-\nguage models in §5. Finally, we discuss the results\nin light of linguistic typology in §6.\n2 Multilingual Language Modeling\nA language model computes a probability distribu-\ntion over sequences of word tokens, and is typically\ntrained to maximise the likelihood of word input\nsequences. The LM objective is expressed as:\nP(w1,...wn) =\n∏\ni\nP(wi|w1,...wi−1) (1)\nwi is a word token with the index iin the sequence.\nLM is considered a central task in NLP and lan-\nguage understanding, with applications in speech\nrecognition (Mikolov et al., 2010), text summari-\nsation (Filippova et al., 2015; Rush et al., 2015),\nand information retrieval (Ponte and Croft, 1998;\nZamani and Croft, 2016). The importance of lan-\nguage modeling has been accentuated even more in\nrepresentation learning recently, where it is used as\na novel form of unsupervised pre-training (and an\nalternative to static word embeddings) for the ben-\neﬁt of a variety of NLP applications (Peters et al.,\n2018; Howard and Ruder, 2018).\nRelated Work: Datasets and Evaluation. Lan-\nguage modeling is predominantly tested on English\nand other Western European languages. Standard\nEnglish LM benchmarks are the Penn Treebank\n(PTB) (Marcus et al., 1993) and the 1 Billion Word\nBenchmark (BWB) (Chelba et al., 2013). Datasets\nextracted from BBC News (Greene and Cunning-\nham, 2006) and IMDB Movie Reviews (Maas et al.,\n2011) are also used for LM evaluation in English\n(Wang and Cho, 2016; Miyamoto and Cho, 2016;\nPress and Wolf, 2017).\nFor multilingual LM evaluation, Botha and Blun-\nsom (2014) extract datasets for Czech, French,\nSpanish, German, and Russian from the 2013 Work-\nshop on Statistical Machine Translation (WMT)\ndata (Bojar et al., 2013). Kim et al. (2016) reuse\nthese datasets and add Arabic. Ling et al. (2015)\nevaluate on English, Portuguese, Catalan, German\nand Turkish datasets extracted from Wikipedia.\nKawakami et al. (2017) evaluate on 7 European\nlanguages using Wikipedia data, including Finnish.\nTo the best of our knowledge, the largest datasets\nused in previous work are from (Müller et al., 2012;\nCotterell et al., 2018) and amount to 21 languages\nfrom the Europarl data (Koehn, 2005). Despite the\nlarge coverage of languages, these sets are still\nrestricted only to the languages of the European\nUnion. On the other hand, the most typologically\ndiverse dataset thus far was released by Vania and\nLopez (2017). It includes 10 languages represent-\ning some morphological systems.\nThis short survey of related work demonstrates\na clear tendency towards extending LM evaluation\nto other languages, abandoning English-centric as-\nsumptions, and focusing on language-agnostic LM\narchitectures. However, a comprehensive evalua-\ntion set that systematically covers a wide and bal-\nanced spectrum of typologically diverse languages\nis still missing. The novel dataset we discuss in this\npaper aims at bridging this gap (see §4).\nFixed vs. Full Vocabulary Setup. A majority of\nlanguage models rely on the ﬁxed-vocabulary as-\nsumption: they use a special symbol <UNK> that\nrepresents all words not present in the ﬁxed vocabu-\nlary V, which are termed out-of-vocabulary (OOV).\nSelecting the set V typically slips under the radar,\nand can be seen as “something of a black art” de-\nspite its enormous impact on ﬁnal LM performance\n(Cotterell et al., 2018).1 Standard LM setups either\nﬁx the vocabulary V to the top n most frequent\nwords, typically with n = 10,000 or n = 5,000\n(Mikolov et al., 2010; Ling et al., 2015; Vania and\nLopez, 2017; Lee et al., 2017,inter alia), or include\nin V only words with a frequency below a certain\nthreshold (typically 2 or 5) (Heaﬁeld et al., 2013).\nThe rationale behind ﬁxing the set V is a) to\nmake the language model more robust to handling\nOOVs and to effectively bypass the problem of\nunreliable word estimates for low-frequency and\nunseen words (by ignoring them), and b) to enable\ndirect comparisons of absolute perplexity scores\nacross different models. However, this posits a\ncritical challenge as cross-linguistic evaluation be-\ncomes uneven. In fact, we witness a larger propor-\ntion of vocabulary words replaced by <UNK> in\nmorphologically rich languages because of their\nhigher OOV rates (see Table 3). What is more,\nwhile the ﬁxed-vocabulary assumption artiﬁcially\n1For instance, Vania and Lopez (2017) report perplexity\nscores of ≈20 for Finnish when V is ﬁxed to the 5k most\nfrequent words. The same model in the full-vocabulary setup\nobtains perplexity scores of ≈2,000.\n318\nFI Kreikkalaiset sijoittivat geometrian synnyn muinaiseen Egyptiin , jossa sitä tarvittiin maanmittaukseen .\nFI (MIN-5) <UNK> <UNK> <UNK> synnyn <UNK> Egyptiin , jossa sitä tarvittiin <UNK> .\nFI (10K) <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> , jossa sitä <UNK> <UNK> .\nKO Õ ᅳ+ 'ô\u0014Çr ᅵÑþ{9 \u0006©ᆼ\\ \" f©ᆼ\" é¶ᄒ ᅡ#  \u001b\u00041 l xÜ ¼ Ð· ú ᆯ\u001f 9&  \u0012ᄃ ᅡ. Õ ᅳ\u001f Qᄂ ᅡÕ ᅳ_ ᅴ|9 \b É rt ᅵ1 l qᄒ ᅡ> ᅦᄀ ᅡèßᆫÙþ¡ᄃ ᅡ.\nKO (MIN-5) Õ ᅳ+ '<UNK> <UNK> <UNK> <UNK> · ú ᆯ\u001f 9&  \u0012ᄃ ᅡ. Õ ᅳ\u001f Qᄂ ᅡÕ ᅳ_ ᅴ|9 \b É r<UNK> <UNK> .\nKO (10K) Õ ᅳ+ '<UNK> <UNK> <UNK> <UNK> · ú ᆯ\u001f 9&  \u0012ᄃ ᅡ. Õ ᅳ\u001f Qᄂ ᅡÕ ᅳ_ ᅴ<UNK> <UNK> <UNK> .\nTable 1: Examples from Finnish and Korean LM datasets after applying the standard ﬁxed-vocabulary\nassumption. MIN=5: only words with corpus frequency above 5 are retained in the ﬁnal ﬁxed vocabulary\nV; 10K: V comprises the 10kmost frequent words.\nimproves the perplexity measure, it actually makes\nthe models less useful, especially in morphologi-\ncally rich languages, as exempliﬁed in Table 1.\nOur goal is to get a clear picture on how dif-\nferent typological features and the corresponding\ncorpus frequency distributions affect LM perfor-\nmance, without the inﬂuence of the unrealistic\nﬁxed-vocabulary assumption. Therefore, we work\nin the full-vocabulary LM setup (Adams et al.,\n2017; Grave et al., 2017). This means that we ex-\nplicitly decide to retain also infrequent words in\nthe modeled data: V contains all words occurring\nat least once in the training set, only unseen words\nfrom test data are treated as OOVs. We believe that\nthis setup leads to an evaluation that pinpoints the\ncrucial limitations of standard LM architectures.2\nWhy Not Open Vocabulary Setup? Recent neu-\nral LM architectures have also focused on han-\ndling large vocabularies and unseen words using\ncharacter-aware modeling (Luong and Manning,\n2016; Jozefowicz et al., 2016; Kawakami et al.,\n2017, inter alia). This setup is commonly referred\nto as the open-vocabulary setup. However, two dis-\ntinct approaches with crucial modeling differences\nare referred to by the same term in the literature.\na) Word-level generationconstructs word vectors\nfor arbitrary words from constituent subword-level\ncomponents, but word-level prediction is still eval-\nuated based on the ﬁxed-vocabulary assumption.\nb) Character-level generation predicts characters\ninstead of words.\nGiven that character-level prediction and word-\nlevel prediction operate on entirely different sets\nof symbols, their performance is hardly compara-\nble. Still, Jozefowicz et al. (2016) report that, in a\nhybrid setup which evaluates character-level pre-\ndiction based on word-level perplexity with the\n2For instance, as discussed later in §3 and validated empiri-\ncally in §6, the vocabularies of morphologically rich languages\nare inherently larger: it is simply more difﬁcult to learn and\nmake LM predictions in such languages.\nType Fusion Exponence Flexivity Synthesis\nIsolating low 1:1 1:1 low\nFusional mid many:1 1:many mid\nIntroﬂexive high many:1 1:many mid\nAgglutinative mid 1:1 1:1 high\nTable 2: Traditional morphological types described\nin terms of selected features from W ALS.\nﬁxed-vocabulary assumption, current state-of-the-\nart word-level prediction models (i.e., the ones we\ndiscuss in §5) still signiﬁcantly outperform such hy-\nbrid character-level prediction approaches. There-\nfore, we operate in the full-vocabulary setup.\n3 Typology of Morphological Systems\nAiming for a comprehensive multilingual LM\nevaluation in this study, we survey all possible\ntypes of morphological systems (Haspelmath and\nSims, 2013), which possibly lead to different per-\nformances. Traditionally, languages have been\ngrouped into the four main categories: isolating,\nfusional, introﬂexive and agglutinative, based on\ntheir position along a spectrum measuring the pref-\nerence on breaking up concepts in many words (on\none extreme) or rather compose them into single\nwords (on the other extreme).\nThe mono-dimensionality of this spectrum has\nrecently been challenged as languages exhibit a\nmultitude of morphological features that do not co-\nvary across languages (Plank, 2017; Ponti et al.,\n2018). The typological database W ALS (Dryer and\nHaspelmath, 2013) documents several of them that\nare relevant for LM: inﬂectional synthesis, fusion,\nexponence, and ﬂexivity. Note that the prototypes of\ntraditional categories can be approximated in terms\nof these features, as shown in Table 2, although\nmore combinations are possible.\nLanguages specify different subsets of grammat-\nical categories (such as tense for verbs, or num-\n319\nber for nouns), and for each category different val-\nues are available in each language: for instance,\nFinnish has less tense values (it lacks a future),\nwhereas Slovene has more number values (includ-\ning a dual) compared to English. The feature in-\nﬂectional synthesis for verbs (Bickel and Nichols,\n2013) measures how many categories appear on\nthe maximally inﬂected verb per language. More\navailable categories enlarge the vocabulary (and\nconsequently the OOV rate) with forms instantiat-\ning all possible combinations of their values.\nAnother crucial aspect is how the available gram-\nmatical categories are expressed, which can be de-\nscribed by fusion, exponence, and ﬂexivity.Fusion\nmeasures the degree of connectedness between a\ngrammatical marker to another word. The marker\ncan be (from lower to higher fusion) a separate\nword, a clitic, an afﬁx, or can affect the form of the\nroot itself (e.g. an umlaut or a tone).\nExponence measures the number of categories\n(e.g., tense, number) a single morpheme tends to\nconvey. Exponence is separative if one grammatical\ncategory is conveyed by one morpheme (1:1), and\ncumulative if multiple categories are grouped into\none morpheme (many:1).\nFlexivity indicates the possibility that the value\nof a grammatical category be mapped into differ-\nent morphological forms (1:many). In other terms,\nlemmas belonging to the same part-of-speech are\ndivided into inﬂectional classes (such as declension\nclasses for nouns or conjugation classes for verbs),\neach characterised by a different paradigm, that is,\na different set of value-to-form mappings.\nThe three last features are illustrated by the ex-\namples Ex. (2)-Ex. (5), all uttering the sentence “I\nwill guard the doors and I will not open (them)”.3\n(2) tôi\nI\nsẽ\nFUT\nbảo\nguard\nvệ cửa\ndoor\nvà\nand\ntôi\nI\nsẽ\nFUT\nkhông\nNEG\nmở\nopen (Vietnamese)\n(3) kapı-lar-ı\ndoor-PL-ACC\nkoruy-aca˘ g-ım\nguard-FUT-1SG\nve\nand\naç-may-aca˘ g-ım\nopen-NEG -FUT-1SG (Turkish)\n(4) sorvegli-erò\nguard-FUT.1SG\nle\nDEF\nport-e\ndoor-PL\ne\nand\nnon\nNEG\napr-irò\nopen-FUT.1SG (Italian)\n3All morphological glosses follow the Leipzig glossing\nrules, listed at https://www.eva.mpg.de/lingua/\nresources/glossing-rules.php\n(5) ‘e-šmor\n1SG-guard.FUT\n‘al\non\nha-d‘lat-ót\nDEF -door-PL\nv‘-lo\nand-NEG\n‘e-ftach\n1SG-wait.FUT\notán\nthem (Hebrew)\nIn particular, consider how tense and person are\nexpressed on verbs. Vietnamese in Ex. (2) puts\ntwo particles tôi and s˜e before the verb, which are\ndistinct (separate exponence), autonomous from\nthe root (no fusion), and ﬁxed (absence of ﬂexiv-\nity). Turkish in Ex. (3) attaches sufﬁxes: -acak- for\ntense and -ımfor person. These are distinct (sepa-\nrate exponence), joined to the roots (concatenative\nfusion), and (phonologically determined variants\nof) the same morpheme (1:1 ﬂexivity). Italian in\nEx. (4) uses afﬁxes -erò and -irò: they are concate-\nnated to the root with respect to fusion, convey\nboth tense and person (cumulative exponence), and\nare dissimilar (presence of ﬂexivity). Finally, in\nEx. (5) for Hebrew the consonant pattern of the\nverb š-m-r is interdigitated by the vowel -o- for\ntense, and preceded by a preﬁx ‘e-for person. The\nﬁrst phenomenon alters the root itself (introﬂexive\nfusion), is distinct from the second (separate ex-\nponence), and changes its realisation based on the\nverb’s lemma (presence of ﬂexivity).\nThe above evidence strongly motivates us, as\nwell as recent previous work (Vania and Lopez,\n2017; Kawakami et al., 2017; Cotterell et al., 2018),\nto approach LM with models that are aware of the\ninner structure of their input words, and to bench-\nmark these modeling choices on a typologically\ndiverse range of languages, as shown in §4.\n4 Data\nSelection of Languages. Our selection of test\nlanguages is guided by the following goals: a) we\nhave to ensure the coverage of typological prop-\nerties from §3, and b) we want to analyse a large\nset of languages which extends and surpasses other\nwork in the LM literature (see §2).\nSince cross-lingual NLP aims at modelingextant\nlanguages rather than possible languages (includ-\ning, e.g., extinct ones), creating a balanced sample\nis challenging. In fact, attested languages, intended\nas a random variable, are extremely sparse and not\nindependent-and-identically-distributed (Cotterell\nand Eisner, 2017). First, available and reliable data\nexist only for a fraction of the world’s languages.\nSecond, these data are biased because their features\nmay not stem from the underlying distribution, i.e.,\nfrom what is naturally possible/frequent, but rather\n320\ncan be inherited by genealogical relatedness or bor-\nrowed by areal proximity (Bakker, 2010). To mit-\nigate these biases, theoretical works resorted to\nstratiﬁcation approaches, where each subgroup of\nrelated languages is sampled independently. maxi-\nmizing their diversity (Dryer, 1989, inter alia). We\nperform our selection in the same spirit.\nWe start from the Polyglot Wikipedia (PW)\nproject (Al-Rfou et al., 2013) which provides\ncleaned and tokenised Wikipedia data in 40 lan-\nguages. However, the majority of the PW lan-\nguages are similar from the perspective of geneal-\nogy (26/40 are Indo-European), geography (28/40\nare Western European), and typology (26/40 are\nfusional). Consequently, the PW set is not a repre-\nsentative sample of the world’s languages.\nTo amend this limitation, we source additional\nlanguages with the data coming from the same\ndomain, Wikipedia, considering candidates in de-\nscending order of corpus size cleaned and prepro-\ncessed by the Polyglot tokeniser (Al-Rfou et al.,\n2013). Since fusional languages are already repre-\nsented in the PW, we add new languages from other\nmorphological types: isolating (Min Nan, Burmese,\nKhmer), agglutinative ( Basque, Georgian, Kan-\nnada, Tamil, Mongolian, Javanese), and introﬂex-\nive languages (Amharic).\nPartition. We construct datasets for all 50 lan-\nguages by extracting the ﬁrst 40K sentences for\neach language, and split them into train (34K), vali-\ndation (3K), and test (3K). This choice has been mo-\ntivated by the following observations:a) we require\nsimilarly-sized datasets from the same domain for\nall languages; b) the size of the datasets has to be\nsimilar to the standard English PTB dataset (Mar-\ncus et al., 1993) which has been utilised to guide\nLM development in English for more than 20 years.\nThe ﬁnal list of 50 languages along with their lan-\nguage codes (ISO 639-1), morphological type (i.e.,\nisolating, fusional, introﬂexive, agglutinative), and\ncorpus statistics is provided in Table 3.\n5 Models and Experimental Setup\nBenchmarked Language Models. The avail-\nability of LM evaluation sets in a large number\nof diverse languages, as described in §4, gives\nan opportunity to conduct a full-ﬂedged multilin-\ngual analysis of representative LM architectures\nfor word-level prediction. First, we evaluate a state-\nof-the-art model from the n-gram family of models\n(Goodman, 2001) from the KenLM package. 4 It\nis based on 5-grams with extended Kneser-Ney\nsmoothing (Kneser and Ney, 1995; Heaﬁeld et al.,\n2013). We refer to this model as KN5.\nModern LM architectures are almost exclusively\nbased on recurrent neural networks (RNNs), and\nespecially on Long-Short-Term Memory networks\n(LSTMs). (Mikolov et al., 2010; Sundermeyer et al.,\n2015; Chen et al., 2016, inter alia). They map a\nsequence of input words to embedding vectors us-\ning a look-up matrix and then perform word-level\nprediction by passing the vectors to the LSTM.\nFinally, we also evaluate a character-aware vari-\nant of the neural LSTM LM architecture. We use\nthe Char-CNN-LSTM model (Kim et al., 2016)\ndue to its public availability and strong perfor-\nmance in several languages. In this model, each\ncharacter is embedded and passed through a convo-\nlutional neural network with max-over-time pool-\ning (LeCun et al., 1989), followed by a highway\nnetwork transformation (Srivastava et al., 2015) to\nbuild word representations from their constituent\ncharacters. By resorting to character-level informa-\ntion, the model is able to provide better parame-\nter estimates for lower-frequency words, which is\nparticularly important for morphologically rich lan-\nguages. The CNN-based word representations are\nthen processed in a sequence by a regular LSTM\nnetwork to obtain word-level predictions.\nEvaluation Setup. We report perplexity scores\n(Jurafsky and Martin, 2017, chapter 4.2.1) using\nthe full vocabulary for each respective LM dataset.\nThis means that we explicitly decide to retain also\ninfrequent words in the data and analyse the difﬁ-\nculty of modeling such words in morphologically\nrich languages (see §2 for the discussion).\nIn the full-vocabulary setup, the setV comprises\nall words occurring at least once in the training\nset. Unseen test words are mapped to one <UNK>\nvector, sampled from the the space of trained word\nvectors relying on a normal distribution and the\nsame ﬁxed random seed for all models. On the other\nhand, KN5 by design has a slightly different way\nof handling unseen test words: they are regarded as\noutliers and assigned low-probability estimates.\nTraining Setup and Parameters. For LSTM\nand Char-CNN-LSTM language models, we re-\nproduce the standard LM setup of Zaremba et al.\n(2015) and parameter choices of Kim et al. (2016).\n4https://github.com/kpu/kenlm\n321\nBatch size is 20 and a sequence length is 35, where\none step corresponds to one word token. The max-\nimum word length is chosen dynamically based\non the longest word in the corpus. The corpus is\nprocessed continuously; the RNN hidden states re-\nset at the beginning of each epoch. Parameters are\noptimised with SGD, and the gradient is averaged\nover the batch size and sequence length. We then\nscale the averaged gradient by the sequence length\n(=35) and clip to 5.0 for more stable training. The\nlearning rate is 1.0, decayed by 0.5 after each epoch\nif the validation perplexity does not improve. All\nmodels are trained for 15 epochs, which is typically\nsufﬁcient for model convergence. Finally, KN5 is\ntrained relying on the suggested parameters from\nthe KenLM package.\n6 Results and Discussion\nIn this section, we present our main empirical ﬁnd-\nings on the connection between LM performance\nand corpus statistics emerging from different ty-\npological proﬁles (see §3). Before proceeding, we\nstress that the absolute perplexity scores across dif-\nferent languages are not directly comparable, but\ntheir values provide evidence on the difﬁculty and\nlimitations of language modeling in each language,\nconsidering the fact that all language models were\ntrained on similarly-sized datasets. The results for\nall three benchmarked language models on all 50\nlanguages are summarised in Table 3.\nComparison of Language Models. A quick in-\nspection of the results from Table 3 reveals that the\nChar-CNN-LSTM model is the best-performing\nmodel overall. We report the best results with that\nmodel for 48/50 languages and across all traditional\nmorphological types. Gains over the simpler recur-\nrent LM architecture (i.e., the LSTM model) are\npresent for all 50/50 languages. In short, this means\nthat character-level information on the input side of\nneural architectures, in addition to leading to fewer\nparameters, is universally beneﬁcial for the ﬁnal\nperformance of word-level prediction, as also sug-\ngested by Kim et al. (2016) on a much smaller set\nof languages. By relying on character-level knowl-\nedge, Char-CNN-LSTM model provides better es-\ntimates for lower-frequency words.\nMoreover, the results show that KN5 is a compet-\nitive baseline for several languages (e.g., Kannada,\nThai, Amharic). This further highlights the impor-\ntance of testing models on a typologically diverse\nset of languages: despite the clear superiority of\n0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.225\ntype/token ratio\n0\n1000\n2000\n3000\n4000\n5000perplexity\nam\nar\nko\nro\nde\nhe\nen\net\neues\nkm\nnan ja\njv\nzh\nvi\nfi\nta\npl\ncs\nettr\nkn\nlt\nlv\nsl\nIsolating\nFusional\nIntroflexive\nAgglutinative\nFigure 1 : Perplexity scores with the Char-CNN-\nLSTM language model (Kim et al., 2016) on PTB-\nsized language modeling data in 50 languages as a\nfunction of type-to-token ratios in training data.\nneural LM architectures such as Char-CNN-LSTM\nin a large number of languages, the results and the\nmarked outliers still suggest that there is currently\nno “one-size-ﬁts-all” model.\nIn general, large perplexity scores for certain\nlanguages (e.g., agglutinative languages such as\nFinnish, Korean, Tamil, or introﬂexive languages),\nespecially when compared to performance on En-\nglish on a similarly-sized dataset, clearly point at\nthe limitations of all the “language-agnostic” LM\narchitectures. As suggested by Jozefowicz et al.\n(2016), LM performance in English can be boosted\nby simply collecting more data and working with\nlarge vocabularies (e.g., reducing the number of\nrelevant OOVs). However, this solution is certainly\nnot applicable to a majority of the world’s lan-\nguages (Bird, 2011; Gandhe et al., 2014; Adams\net al., 2017), see later in §6: Further Discussion.\nFrequency Analysis and Traditional Morpho-\nlogical Types. We now analyse all languages in\nour collection according to word-level frequency\nproperties also listed in Table 3 for all 50 lan-\nguages. We report: 1) the vocabulary size (i.e., the\ntotal number of vocabulary words in each training\ndataset); 2) the total number of test words not occur-\nring in the corresponding training data; 3) the total\nnumber of tokens in both training and test data;\nand ﬁnally 4) type-to-token ratios (TTR) in train-\ning data. We also plot absolute perplexity scores\nof Char-CNN-LSTM (Kim et al., 2016), the best-\nperforming model overall (see §6), in relation to\nTTR ratios in Figure 1.\n322\nData Stats Baseline Models\nLanguage (code) V ocab\nSize\n(Train)\nNew\nTest\nV ocab\nNumber\nTokens\n(Train)\nNumber\nTokens\n(Test)\nType /\nToken\n(Train)\nKN5 LSTM Char-\nCNN-\nLSTM\n× Amharic (am) 89749 4805 511K 39.2K 0.18 1252 1535 981\n× Arabic (ar) 89089 5032 722K 54.7K 0.12 2156 2587 1659\n□Bulgarian (bg) 71360 3896 670K 49K 0.11 610 651 415\n□Catalan (ca) 61033 2562 788K 59.4K 0.08 358 318 241\n□Czech (cs) 86783 4300 641K 49.6K 0.14 1658 2200 1252\n□Danish (da) 72468 3618 663K 50.3K 0.11 668 710 466\n□German (de) 80741 4045 682K 51.3K 0.12 930 903 602\n□Greek (el) 76264 3767 744K 56.5K 0.10 607 538 405\n□English (en) 55521 2480 783K 59.5K 0.07 533 494 371\n□Spanish (es) 60196 2721 781K 57.2K 0.08 415 366 275\n⋆ Estonian (et) 94184 3907 556K 38.6K 0.17 1609 2564 1478\n⋆ Basque (eu) 81177 3365 647K 47.3K 0.13 560 533 347\n□Farsi (fa) 52306 2041 738K 54.2K 0.07 355 263 208\n⋆ Finnish (ﬁ) 115579 6489 585K 44.8K 0.20 2611 4263 2236\n□French (fr) 58539 2575 769K 57.1K 0.08 350 294 231\n× Hebrew (he) 83217 3862 717K 54.6K 0.12 1797 2189 1519\n□Hindi (hi) 50384 2629 666K 49.1K 0.08 473 426 326\n□Croatian (hr) 86357 4371 620K 48.1K 0.14 1294 1665 1014\n⋆ Hungarian (hu) 101874 5015 672K 48.7K 0.15 1151 1595 929\n\u0003 Indonesian (id) 49125 2235 702K 52.2K 0.07 454 359 286\n□Italian (it) 70194 2923 787K 59.3K 0.09 567 493 349\n⋆ Japanese (ja) 44863 1768 729K 54.6K 0.06 169 156 136\n⋆ Javanese (jv) 65141 4292 622K 52K 0.10 1387 1443 1158\n⋆ Georgian (ka) 80211 3738 580K 41.1K 0.14 1370 1827 1097\n\u0003 Khmer (km) 37851 1303 579K 37.4K 0.07 586 637 522\n⋆ Kannada (kn) 94660 4604 434K 29.4K 0.22 2315 5310 2558\n⋆ Korean (ko) 143794 8275 648K 50.6K 0.22 5146 10063 4778\n□Lithuanian (lt) 81501 3791 554K 41.7K 0.15 1155 1415 854\n□Latvian (lv) 75294 4564 587K 45K 0.13 1452 1967 1129\n\u0003 Malay (ms) 49385 2824 702K 54.1K 0.07 776 725 525\n⋆Mongolian (mng) 73884 4171 629K 50K 0.12 1392 1716 1165\n\u0003 Burmese (my) 20574 755 576K 46.1K 0.04 209 212 182\n\u0003 Min-Nan (nan) 33238 1404 1.2M 65.6K 0.03 61 43 39\n□Dutch (nl) 60206 2626 708K 53.8K 0.08 397 340 267\n□Norwegian (no) 69761 3352 674K 47.8K 0.10 534 513 379\n□Polish (pl) 97325 4526 634K 47.7K 0.15 1741 2641 1491\n□Portuguese (pt) 56167 2394 780K 59.3K 0.07 342 272 214\n□Romanian (ro) 68913 3079 743K 52.5K 0.09 384 359 256\n□Russian (ru) 98097 3987 666K 48.4K 0.15 1128 1309 812\n□Slovak (sk) 88726 4521 618K 45K 0.14 1560 2062 1275\n□Slovene (sl) 83997 4343 659K 49.2K 0.13 1114 1308 776\n□Serbian (sr) 81617 3641 628K 46.7K 0.13 790 961 582\n□Swedish (sv) 77499 4109 688K 50.4K 0.11 843 832 583\n⋆Tamil (ta) 106403 6017 507K 39.6K 0.21 3342 6234 3496\n\u0003 Thai (th) 30056 1300 628K 49K 0.05 233 241 206\n\u0003 Tagalog (tl) 72416 3791 972K 66.3K 0.07 379 298 219\n⋆Turkish (tr) 90840 4608 627K 45K 0.14 1724 2267 1350\n□Ukranian (uk) 89724 4983 635K 47K 0.14 1639 1893 1283\n\u0003 Vietnamese (vi) 32055 1160 754K 61.9K 0.04 197 190 158\n\u0003 Chinese (zh) 43672 1653 746K 56.8K 0.06 1064 826 797\n\u0003 Isolating (avg) 40930 1825 759K 54K 0.05 440 392 326\n□Fusional (avg) 73499 3532 689K 51.3K 0.11 842 969 618\n× Introﬂexive (avg) 87352 4566 650K 49.5K 0.14 1735 2104 1386\n⋆ Agglutinative (avg) 91051 4687 603K 45K 0.16 1898 3164 1727\nTable 3: Test perplexities for 50 languages (ISO 639-1 codes sorted alphabetically) in the full-vocabulary\nprediction LM setup; Left: Basic statistics of LM evaluation data (see §4 and §5). Right: Results with all\nthree language models in our comparison. Best absolute perplexity scores for each language are in bold,\nbut note that the absolute scores in the KN5 column are not directly comparable to the scores obtained\nwith neural models due to a different handling of OOVs at test time (see §5).\nIn isolating and some fusional languages (e.g.,\nVietnamese, Thai, English) the TTR tends to be\nsmall: we have a comparatively low number of\ninfrequent words. Agglutinative languages such\nas Finnish, Estonian, and Korean are on the other\nside of the spectrum. Introﬂexive and fusional lan-\nguages, typically over-represented in prior work\n(see the discussion in §3), are found in the middle.\nThis emerges clearly in Figure 1, grouping isolat-\ning languages to the left side of the x-axis, followed\n323\nby fusional languages (Germanic and Romance\nﬁrst to the left, and then Balto-Slavic to the right),\nand placing agglutinative languages towards the\nfar right. Crucially, TTR is an excellent predictor\nof LM performance. To measure the correlation\nbetween this corpus statistics variable and absolute\nLM performance, we compute their Pearson’srcor-\nrelation. We ﬁnd a strong positive correlation, with\na value of r= 0.83 and signiﬁcance p< 0.001.\nWe do observe a strong link between each lan-\nguage’s morphological type, and the correspond-\ning perplexity score. A transition in terms of the\nspectrum of morphological systems (see §3) can\nbe traced again on the y-axis of Figure 1, roughly\nfollowing the reported LM performance: from iso-\nlating, over fusional and introﬂexive to agglutina-\ntive languages. In fact, a correlation exists also\nbetween traditional morphological types and LM\nperformance. We assessed its strength with the one-\nway ANOVA statistical test, obtaining a value of\nη2 = 0.37 and a signiﬁcance of p< 0.001.\nFinally, it should be noted that the choice of TTP\nover other corpus statistics such as vocabulary size\nis motivated by the fact that the corpora are compa-\nrable, and not parallel. Because of this, the variation\nof V may stem from the contents rather than the\nintrinsic linguistic properties. As a counter-check,\nthe correlation between V and LM performance is\nin fact milder, with r= 0.64. Yet, notwithstanding\nthe stronger correlation, TTP is unable to explain\nthe results entirely. Only through ﬁner-grained ty-\npological features it becomes possible to justify\nseveral outliers, as shown in the next subsection.\nFine-Grained Typological Analysis. Among\nthe relevant typological features (see §3 and Table\n2), fusion and inﬂectional synthesis have the largest\nimpact on word-level predictions. In fact, the for-\nmer determines the word boundaries, whereas the\nlatter regulates the amount of possible morpheme\ncombinations. Consider their effect on the fre-\nquency distribution of words, expressed as follows\n(Zipf, 1949):\nf =\n1\nks\n∑V\nn=1\n1\nns\n(6)\nf is the frequency, k the rank, and s≥0 the expo-\nnent characteristic of the distribution. If high, both\ntypological features enlarge V and s, assigning less\nprobability mass to each word.\nLow fusion means a preference for separate\nwords (as in isolating languages such as Viet-\nnamese and Chinese), leading to a smaller vocab-\nulary with less (but more frequent) words. This\nproperty, additionally boosted by low inﬂectional\nsynthesis, facilitates statistical language modeling\nin isolating languages. Vice versa, high fusion re-\nsults in preference for concatenation of morphemes\nor introﬂection, and consequently sparser vocabu-\nlaries. Yet, this distinction cannot justify the ﬁgures\nby itself, as it equates agglutinative languages and\ntraditional fusional languages. Here, inﬂectional\nsynthesis is also at play. Through the statistical\ntest of one-way ANOV A, we found a weak effect\nof η2 = 0.09 for fusion and a medium effect of\nη2 = 0.21 for inﬂection synthesis.\nOn the other hand, the ﬁne-grained typological\nfeatures of exponence and ﬂexivity play a role in\nthe ambiguity of the mapping between morphemes\nand meanings or grammatical functions. This turns\nout to be especially relevant for character-aware\nmodels. The intuition is that if the mapping is\nstraightforward, injecting character information is\nmore advantageous. To validate this claim, we eval-\nuate the ANOV A between exponence of nouns\nand verbs and the difference in perplexity between\nLSTM and Char-CNN-LSTM.5 We report a weak,\nalthough existent, correlation with value η2 = 0.07\nand η2 = 0.04, respectively.\nFurther Discussion. Importantly, our large-\nscale multilingual LM study strongly indicates\nthat due to diverse typological proﬁles, certain lan-\nguages and language groups are inherently more\ncomplex to language-model when relying on es-\ntablished statistical models, even when such mod-\nels are constructed as widely applicable and (ar-\nguably) language-agnostic. This ﬁnding supports\npreliminary results from prior work (Botha and\nBlunsom, 2014; Adams et al., 2017; Cotterell et al.,\n2018), and is also backed by insights from linguis-\ntic theory on variance of language complexity in\ngeneral and variance of morphological complexity\nin speciﬁc (McWhorter, 2001; Evans and Levin-\nson, 2009). More broadly and along the same line,\nearlier research in statistical machine translation\n(SMT) has also shown that typological factors such\nas the amount of reordering, the morphological\ncomplexity, as well as genealogical relatedness of\nlanguages are crucial in predicting success in SMT\n(Birch et al., 2008; Paul et al., 2009; Daiber, 2018).\nOur results indicate that the artiﬁcial ﬁxed-\n5Unfortunately no values are available in WALS for the\nfeature of ﬂexivity besides a limited domain.\n324\nvocabulary assumption from prior work produces\noverly optimistic perplexity scores, and its limita-\ntion is even more pronounced in morphologically\nrich languages, which inherently contain a large\nnumber of infrequent words due to their productive\nmorphological systems. The typical solution to col-\nlect more data (Jozefowicz et al., 2016; Kawakami\net al., 2017) mitigates this effect to a certain extent,\nbut stills suffers from the Zipﬁan hypothesis (1949),\nand it cannot be guaranteed for resource-poor lan-\nguages where obtaining sufﬁcient monolingual data\nis also a challenge (Adams et al., 2017).\nTherefore, another solution is to resort to other\nsources of information which are not purely contex-\ntual/distributional. For instance, a promising line of\ncurrent and future research is to (learn to) exploit\nsubword-level patterns captured in an unsupervised\nmanner (Pinter et al., 2017; Herbelot and Baroni,\n2017) or integrate existing morphological genera-\ntion and inﬂection tools and regularities (Cotterell\net al., 2015; Vuli ´c et al., 2017; Bergmanis et al.,\n2017) into language models to reduce data sparsity,\nand improve language modeling for morphologi-\ncally rich languages. For instance, a recent enhance-\nment of the Char-CNN-LSTM language model that\nenforces similarity between parameters of morpho-\nlogically related words leads to large perplexity\ngains across a large number of languages, with the\nmost prominent gains reported for morphologically\ncomplex languages (Gerz et al., 2018).\nGiven the recent success and improved perfor-\nmance with LM-based pre-training methodology\n(Peters et al., 2018; Howard and Ruder, 2018)\nacross a wide variety of syntactic and semantic\nNLP tasks in English, improving language models\nfor other languages might have far-reaching con-\nsequences for multilingual NLP in general. Typo-\nlogical information coded in typological databases\n(Ponti et al., 2018) offer invaluable support to lan-\nguage modeling (e.g., knowledge on word ordering,\nmorphological regularities), but such typologically-\ninformed LM architectures are still non-existent.\n7 Conclusion\nIn this paper, we have run a large-scale study on\nLanguage Modeling (LM) across several architec-\ntures and a collection of 50 typologically diverse\nlanguages. We have demonstrated that typological\nproperties of languages, such as their morphologi-\ncal systems, have an enormous impact on the per-\nformance of allegedly “language-agnostic” models.\nWe have found that the corpus statistics most pre-\ndictive of LM performance is type-to-token ratio\n(TTR), as demonstrated by their strong Pearson’s\ncorrelation. In turn, the value of TTR is motivated\nby ﬁne-grained typological features that deﬁne the\ntype of morphological system within a language.\nIn fact, such features affect the word boundaries\nand the number of morphemes per word, affecting\nthe word frequency distribution for each language.\nWe have also observed that injecting character in-\nformation into word representations is always ben-\neﬁcial because this mitigates the above-mentioned\nsparsity issues. However, the extent of the gain\nin perplexity partly depends on some typological\nproperties that regulate the ambiguity of the map-\nping between morphemes (here modeled as charac-\nter n-grams) and their meaning.\nWe hope that NLP/LM practitioners will ﬁnd\nthe datasets for 50 languages put forth in this\nwork along with benchmarked LMs useful for fu-\nture developments in (language-agnostic as well\nas typologically-informed) multilingual language\nmodeling. This study calls for next-generation so-\nlutions that will additionally leverage typological\nknowledge for improved language modeling. Code\nand data are available at: http://people.ds.\ncam.ac.uk/dsg40/lmmrl.html.\nAcknowledgements\nThis work is supported by the ERC Consolidator\nGrant LEXICAL (no 648909). The authors would\nlike to thank the anonymous reviewers for their\nhelpful suggestions.\nReferences\nOliver Adams, Adam Makarucha, Graham Neubig,\nSteven Bird, and Trevor Cohn. 2017. Cross-lingual\nword embeddings for low-resource language model-\ning. In Proceedings of EACL, pages 937–947.\nRami Al-Rfou, Bryan Perozzi, and Steven Skiena.\n2013. Polyglot: Distributed word representations for\nmultilingual NLP. In Proceedings of CoNLL, pages\n183–192.\nDik Bakker. 2010. Language sampling. In The Oxford\nhandbook of linguistic typology, pages 100–127. Ox-\nford University Press.\nEmily M. Bender. 2013. Linguistic fundamentals for\nnatural language processing: 100 essentials from\nmorphology and syntax . Morgan & Claypool Pub-\nlishers.\n325\nToms Bergmanis, Katharina Kann, Hinrich Schütze,\nand Sharon Goldwater. 2017. Training data augmen-\ntation for low-resource morphological inﬂection. In\nProceedings of CoNLL, pages 31–39.\nBalthasar Bickel and Johanna Nichols. 2013. Inﬂec-\ntional Synthesis of the Verb. Max Planck Institute for\nEvolutionary Anthropology, Leipzig.\nAlexandra Birch, Miles Osborne, and Philipp Koehn.\n2008. Predicting success in machine translation. In\nProceedings of EMNLP, pages 745–754.\nSteven Bird. 2011. Bootstrapping the language archive:\nNew prospects for natural language processing in\npreserving linguistic heritage. Linguistic Issues in\nLanguage Technology, 6(4).\nOndˇrej Bojar, Christian Buck, Chris Callison-Burch,\nChristian Federmann, Barry Haddow, Philipp\nKoehn, Christof Monz, Matt Post, Radu Soricut,\nand Lucia Specia. 2013. Findings of the 2013\nWorkshop on Statistical Machine Translation. In\nProceedings of the 8th Workshop on Statistical\nMachine Translation, pages 1–44.\nJan A. Botha and Phil Blunsom. 2014. Compositional\nmorphology for word representations and language\nmodelling. In Proceedings of ICML , pages 1899–\n1907.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, and Phillipp Koehn. 2013. One bil-\nlion word benchmark for measuring progress in sta-\ntistical language modeling. In Proceedings of IN-\nTERPSEECH, pages 2635–2639.\nXie Chen, Xunying Liu, Yanmin Qian, MJF Gales, and\nPhilip C Woodland. 2016. CUED-RNNLM: An\nopen-source toolkit for efﬁcient training and evalu-\nation of recurrent neural network language models.\nIn Proceedings of ICASSP, pages 6000 –6004.\nRyan Cotterell and Jason Eisner. 2017. Probabilistic\ntypology: Deep generative models of vowel invento-\nries. In Proceedings of ACL, pages 1182–1192.\nRyan Cotterell, Sebastian J. Mielke, Jason Eisner, and\nBrian Roark. 2018. Are all languages equally hard\nto language-model? In Proceedings of NAACL-HLT.\nRyan Cotterell, Thomas Müller, Alexander Fraser, and\nHinrich Schütze. 2015. Labeled morphological seg-\nmentation with semi-Markov models. In Proceed-\nings of CoNLL, pages 164–174.\nJoachim Daiber. 2018. Typologically Robust Statisti-\ncal Machine Translation . Ph.D. thesis, University\nof Amsterdam.\nMatthew S Dryer. 1989. Large linguistic areas and lan-\nguage sampling. Studies in Language. International\nJournal sponsored by the Foundation “Foundations\nof Language”, 13(2):257–292.\nMatthew S. Dryer and Martin Haspelmath, editors.\n2013. WALS Online. Max Planck Institute for Evo-\nlutionary Anthropology, Leipzig.\nNicholas Evans and Stephen C. Levinson. 2009. The\nmyth of language universals: Language diversity and\nits importance for cognitive science. Behavioral and\nBrain Sciences, 32(5):429–448.\nKatja Filippova, Enrique Alfonseca, Carlos A. Col-\nmenares, Lukasz Kaiser, and Oriol Vinyals. 2015.\nSentence compression by deletion with LSTMs. In\nProceedings of EMNLP, pages 360–368.\nAnkur Gandhe, Florian Metze, and Ian Lane. 2014.\nNeural network language models for low resource\nlanguages. In Proceedings of INTERSPEECH ,\npages 2615–2619.\nDaniela Gerz, Ivan Vuli ´c, Edoardo Maria Ponti, Ja-\nson Naradowsky, Roi Reichart, and Anna Korho-\nnen. 2018. Language modeling for morphologically\nrich languages: Character-aware modeling for word-\nlevel prediction. Transactions of the ACL , 6:451–\n465.\nJoshua T. Goodman. 2001. A bit of progress in lan-\nguage modeling. Computer Speech & Language ,\n15(4):403–434.\nEdouard Grave, Moustapha Cissé, and Armand Joulin.\n2017. Unbounded cache model for online language\nmodeling with open vocabulary. In Proceedings of\nNIPS, pages 6044–6054.\nDerek Greene and Padraig Cunningham. 2006. Practi-\ncal solutions to the problem of diagonal dominance\nin kernel document clustering. In Proceedings of\nICML, pages 377–384.\nMartin Haspelmath and Andrea Sims. 2013. Under-\nstanding morphology.\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable modiﬁed\nKneser-Ney language model estimation. In Proceed-\nings of ACL, pages 690–696.\nAurelie Herbelot and Marco Baroni. 2017. High-risk\nlearning: acquiring new word vectors from tiny data.\nIn Proceedings of EMNLP, pages 304–309.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of ACL, pages 328–339.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\nits of language modeling. In Proceedings of ICML.\nDan Jurafsky and James H. Martin. 2017. Speech and\nLanguage Processing, volume 3. Pearson.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom.\n2017. Learning to create and reuse words in open-\nvocabulary neural language modeling. In Proceed-\nings of ACL, pages 1492–1502.\n326\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M. Rush. 2016. Character-aware neural lan-\nguage models. In Proceedings of AAAI, pages 2741–\n2749.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for M-gram language modeling. In Pro-\nceedings of ICASSP, pages 181–184.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of the\n10th Machine Translation Summit, pages 79–86.\nYann LeCun, Bernhard E. Boser, John S. Denker, Don-\nnie Henderson, Richard E. Howard, Wayne E. Hub-\nbard, and Lawrence D. Jackel. 1989. Handwritten\ndigit recognition with a back-propagation network.\nIn Proceedings of NIPS, pages 396–404.\nKenton Lee, Omer Levy, and Luke Zettlemoyer.\n2017. Recurrent additive networks. CoRR,\nabs/1705.07393.\nWang Ling, Tiago Luís, Luís Marujo, Ramón Fernán-\ndez Astudillo, Silvio Amir, Chris Dyer, Alan W.\nBlack, and Isabel Trancoso. 2015. Finding function\nin form: Compositional character models for open\nvocabulary word representation. In Proceedings of\nEMNLP, pages 1520–1530.\nMinh-Thang Luong and Christopher D. Manning. 2016.\nAchieving open vocabulary neural machine transla-\ntion with hybrid word-character models. In Proceed-\nings of ACL, pages 1054–1063.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of ACL, pages 142–150.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional Linguistics, 19(2):313–330.\nJohn McWhorter. 2001. The world’s simplest gram-\nmars are Creole grammars. Linguistic Typology ,\n5(2):125–66.\nTomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proceed-\nings of INTERSPEECH, pages 1045–1048.\nYasumasa Miyamoto and Kyunghyun Cho. 2016.\nGated word-character recurrent language model. In\nProceedings of EMNLP, pages 1992–1997.\nThomas Müller, Hinrich Schütze, and Helmut Schmid.\n2012. A comparative investigation of morphologi-\ncal language modeling for the languages of the Euro-\npean Union. In Proceedings of NAACL-HLT, pages\n386–395.\nHelen O’Horan, Yevgeni Berzak, Ivan Vuli ´c, Roi Re-\nichart, and Anna Korhonen. 2016. Survey on the use\nof typological information in natural language pro-\ncessing. In Proceedings of COLING , pages 1297–\n1308.\nMichael Paul, Hirofumi Yamamoto, Eiichiro Sumita,\nand Satoshi Nakamura. 2009. On the importance\nof pivot language selection for statistical machine\ntranslation. In Proceedings of NAACL-HLT, pages\n221–224.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nYuval Pinter, Robert Guthrie, and Jacob Eisenstein.\n2017. Mimicking word embeddings using subword\nRNNs. In Proceedings of EMNLP, pages 102–112.\nFrans Plank. 2017. Split morphology: How agglu-\ntination and ﬂexion mix. Linguistic Typology ,\n21(2017):1–62.\nJay M. Ponte and W. Bruce Croft. 1998. A language\nmodeling approach to information retrieval. In Pro-\nceedings of SIGIR, pages 275–281.\nEdoardo Maria Ponti, Helen O’Horan, Yevgeni Berzak,\nIvan Vuli´c, Roi Reichart, Thierry Poibeau, Ekaterina\nShutova, and Anna Korhonen. 2018. Modeling lan-\nguage variation and universals: A survey on typo-\nlogical linguistics for natural language processing.\nCoRR, abs/1807.00914.\nEdoardo Maria Ponti, Ivan Vuli´c, and Anna Korhonen.\n2017. Decoding sentiment from distributed repre-\nsentations of sentences. In Proceedings of *SEM ,\npages 22–32.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceed-\nings of EACL, pages 157–163.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of EMNLP ,\npages 379–389.\nRupesh Kumar Srivastava, Klaus Greff, and Jürgen\nSchmidhuber. 2015. Highway networks. In Pro-\nceedings of the ICML Deep Learning Workshop.\nMartin Sundermeyer, Hermann Ney, and Ralf Schluter.\n2015. From feedforward to recurrent LSTM neu-\nral networks for language modeling. IEEE Trans-\nactions on Audio, Speech and Language Processing,\n23(3):517–529.\nClara Vania and Adam Lopez. 2017. From characters\nto words to in between: Do we capture morphology?\nIn Proceedings of ACL, pages 2016–2027.\n327\nIvan Vuli´c, Nikola Mrkši´c, Roi Reichart, Diarmuid Ó\nSéaghdha, Steve Young, and Anna Korhonen. 2017.\nMorph-ﬁtting: Fine-tuning word vector spaces with\nsimple language-speciﬁc rules. In Proceedings of\nACL, pages 56–68.\nTian Wang and Kyunghyun Cho. 2016. Larger-context\nlanguage modelling with recurrent neural network.\nIn Proceedings of ACL, pages 1319–1329.\nHamed Zamani and W. Bruce Croft. 2016. Embedding-\nbased query language models. In Proceedings of IC-\nTIR, pages 147–156.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2015. Recurrent neural network regularization. In\nProceedings of ICLR (Conference Papers).\nGeorge Kingsley Zipf. 1949. Human behavior and the\nprinciple of least effort: An introduction to human\necology.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7925155162811279
    },
    {
      "name": "Variation (astronomy)",
      "score": 0.6946737766265869
    },
    {
      "name": "Natural language processing",
      "score": 0.676640510559082
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6009204387664795
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5768574476242065
    },
    {
      "name": "Linguistic typology",
      "score": 0.5579534769058228
    },
    {
      "name": "Vocabulary",
      "score": 0.5331156849861145
    },
    {
      "name": "Ambiguity",
      "score": 0.5193341970443726
    },
    {
      "name": "Linguistics",
      "score": 0.4964929223060608
    },
    {
      "name": "Meaning (existential)",
      "score": 0.48660027980804443
    },
    {
      "name": "Relation (database)",
      "score": 0.48176097869873047
    },
    {
      "name": "Typology",
      "score": 0.47959673404693604
    },
    {
      "name": "Word (group theory)",
      "score": 0.46276986598968506
    },
    {
      "name": "Language model",
      "score": 0.45557254552841187
    },
    {
      "name": "Tamil",
      "score": 0.449594646692276
    },
    {
      "name": "Task (project management)",
      "score": 0.42560476064682007
    },
    {
      "name": "Character (mathematics)",
      "score": 0.41137757897377014
    },
    {
      "name": "Mathematics",
      "score": 0.10074344277381897
    },
    {
      "name": "History",
      "score": 0.08155173063278198
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Astrophysics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ]
}