{
    "title": "Generative AI in consumer health: leveraging large language models for health literacy and clinical safety with a digital health framework",
    "url": "https://openalex.org/W4413614592",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2104458385",
            "name": "Annemarie K. Tilton",
            "affiliations": [
                "Center for Independent Living"
            ]
        },
        {
            "id": "https://openalex.org/A5119428792",
            "name": "Brian E. Caplan",
            "affiliations": [
                "Rush University Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2170497483",
            "name": "Brian J. Cole",
            "affiliations": [
                "Rush University Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2104458385",
            "name": "Annemarie K. Tilton",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5119428792",
            "name": "Brian E. Caplan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2170497483",
            "name": "Brian J. Cole",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3186209406",
        "https://openalex.org/W4387619071",
        "https://openalex.org/W4411801306",
        "https://openalex.org/W4407686803",
        "https://openalex.org/W4391137026",
        "https://openalex.org/W4392193191",
        "https://openalex.org/W4395067201",
        "https://openalex.org/W2914084521",
        "https://openalex.org/W4321748146",
        "https://openalex.org/W4387578765",
        "https://openalex.org/W4403880728",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W4402476095",
        "https://openalex.org/W2920679031",
        "https://openalex.org/W4402830391",
        "https://openalex.org/W4402705648",
        "https://openalex.org/W4360891289"
    ],
    "abstract": "Generative AI, powered by large language models, is transforming consumer health by enhancing health literacy and delivering personalized health education. However, ensuring clinical safety and effectiveness requires a robust digital health framework to address risks like misinformation and inequitable communication. This mini review examines current use cases for generative AI in consumer health education, highlights persistent challenges, and proposes a clinician-informed framework to evaluate safety, usability, and effectiveness. The RECAP model—Relevance, Evidence-based, Clarity, Adaptability, and Precision—offers a pragmatic lens to guide responsible implementation of AI in patient-facing tools. By connecting insights from past digital health innovations to the opportunities and pitfalls of large language models, this paper provides both context and direction for future development.",
    "full_text": "EDITED BY\nP.J. Fitzpatrick,\nUniversity College Cork, Ireland\nREVIEWED BY\nCarlos Alberto Pereira De Oliveira,\nRio de Janeiro State University, Brazil\nJames C. L. Chow,\nUniversity of Toronto, Canada\n*CORRESPONDENCE\nAnnemarie K. Tilton\nannie.tilton@gmail.com\nRECEIVED 22 April 2025\nACCEPTED 12 August 2025\nPUBLISHED 26 August 2025\nCITATION\nTilton AK, Caplan BE and Cole BJ (2025)\nGenerative AI in consumer health: leveraging\nlarge language models for health literacy and\nclinical safety with a digital health framework.\nFront. Digit. Health 7:1616488.\ndoi: 10.3389/fdgth.2025.1616488\nCOPYRIGHT\n© 2025 Tilton, Caplan and Cole. This is an\nopen-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY). The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with\nthese terms.\nGenerative AI in consumer health:\nleveraging large language models\nfor health literacy and clinical\nsafety with a digital health\nframework\nAnnemarie K. Tilton\n1*, Brian E. Caplan\n2\nand Brian J. Cole\n2\n1Independent Researcher, Park City, UT, United States,2Department of Orthopaedics, Rush University\nMedical Center, Chicago, IL, United States\nGenerative AI, powered by large language models, is transforming consumer\nhealth by enhancing health literacy and delivering personalized health\neducation. However, ensuring clinical safety and effectiveness requires a\nrobust digital health framework to address risks like misinformation and\ninequitable communication. This mini review examines current use cases for\ngenerative AI in consumer health education, highlights persistent challenges,\nand proposes a clinician-informed framework to evaluate safety, usability, and\neffectiveness. The RECAP model — Relevance, Evidence-based, Clarity,\nAdaptability, and Precision — offers a pragmatic lens to guide responsible\nimplementation of AI in patient-facing tools. By connecting insights from past\ndigital health innovations to the opportunities and pitfalls of large language\nmodels, this paper provides both context and direction for future development.\nKEYWORDS\ngenerative AI, large language models (LLMs), consumer health education, health\nliteracy, clinical safety, AI evaluation framework, digital health ethics\n1 Introduction: generative AI and digital health\ninnovation in consumer health\nA decade ago, consumer health information came from one of two sources: your\ndoctor or a questionable corner of the internet. But increasingly, patients are turning to\ntools that seem to offer the best of both worlds— generative AI platforms trained on\nvast sets of medical data, capable of producing humanlike responses at scale. From the\nperspective of physicians who participate in patient care and have contributed to both\nclinical innovation and AI model training, this transformation is simultaneously\nexciting and complex.\nThe rise of generative AI has introduced a disruptive paradigm to healthcare\ncommunication. Generative AI refers to systems that produce original content,\nincluding text, using algorithms trained on large datasets. Large language models\n(LLMs) are a subset of generative AI that use probabilistic language modeling to\nproduce human-like text. LLMs, including OpenAI ’s Generative Pre-trained\nTransformer 4 (GPT-4), Google’s Med-PaLM, and Anthropic’s Claude, are trained on\nextensive internet and medical text. They have demonstrated capabilities in\nsummarizing medical literature, answering clinical questions, and generating patient-\nfacing educational content (\n1, 2).\nTYPE Mini Review\nPUBLISHED 26 August 2025\nDOI 10.3389/fdgth.2025.1616488\nFrontiers inDigital Health 01 frontiersin.org\nSuch tools are being integrated into digital platforms,\nhealth systems, and patient-facing applications, and they have\nsigniﬁcant potential to bridge gaps in health literacy and expand\naccess to evidence-based medical information (\n3, 4). However,\nconcerns about misinformation, lack of contextual nuance, and\npatient over-reliance persist (\n5). Despite their prominence, few\nclinical frameworks exist to evaluate the appropriateness and\nsafety of these tools from a medical perspective. Recent\npublications, such as Chow et al. (\n20), have proposed LLM\nevaluation frameworks focused on accuracy and tone in speciﬁc\ncontexts like cancer care (6). However, RECAP expands this by\nincorporating clinical safety, adaptability, and generalizability\nacross health domains.\nThis paper addresses this gap through a clinician informed\nframework that is grounded in health literacy, safety, and\nusability, offering a practical lens not often addressed in prior\npublications. This mini review offers a novel contribution by\nproposing a clinician-informed digital health evaluation\nframework that distinguishes it from other LLM-focused reviews\nthrough its emphasis on clinical safety, usability, and patient-\ncentered design.\nTo help ground this review, sources were selected based on\nclinical relevance and recency (2019 –2025) pertaining to the\ntopic of AI development, digital health, and LLM use in patient-\nfacing settings. This non-systematic approach was intended to\nidentify key conceptual and practical insights across multiple\nperspectives and sources.\n2 Applications of generative AI for\nhealth literacy in consumer health\nGenerative AI is already being applied across many patient-\nfacing contexts. These tools are often hailed for their versatility,\nbut their practical value depends on how well they function\nwithin existing healthcare communication ecosystems. The\nfollowing subsections expand on both promises and pitfalls of\nthese use cases.\n2.1 Health literacy and generative AI\nchatbots\nAI chatbots, software applications that use artiﬁcial intelligence\nto simulate human conversation, can simplify complex terminology\nand generate personalized responses to common health questions.\nThis improves accessibility for users with limited medical\nknowledge. For example, benchmark studies like MedQA show\nthat LLMs can perform well on structured exams, but patient-\nfacing queries often introduce ambiguity that can reduce\nreliability (\n7). Chatbots trained without clinical oversight may\noverconﬁdently respond to symptoms without recommending\nappropriate follow-up (8).\n2.2 Condition-speciﬁc content delivery\nPatients managing chronic diseases such as diabetes,\nosteoarthritis, or breast cancer may bene ﬁt from AI-generated\nguidance that is tailored to their condition. However, there is\nwide variability in how tools handle differences in age,\ncomorbidities, and care goals. A patient with geriatric frailty may\nreceive advice designed for a young, active adult. Tools that fail\nto adjust language or urgency across subpopulations can dilute\ntheir clinical relevance (\n9).\n2.3 Visit preparation and follow-up\nAI tools may assist patients in preparing for a visit by helping\nto generate relevant questions, summarize symptoms, or clarify\nmedical instructions. After the visit, they might reinforce\nmedication adherence or explain discharge instructions. But if AI\ninterpretations diverge from what clinicians intended — or use\nmore casual or less urgent language— this may lead to confusion,\nconﬂicted messaging, or an unintended reduction in trust in the\nprovider-patient relationship (\n10).\n2.4 Mental health and lifestyle guidance\nGenerative models are increasingly being piloted to support\ncognitive-behavioral therapy (CBT) strategies, motivational\ninterviewing prompts, and behavioral health coaching. For\ninstance, Park et al. designed a chatbot that delivered brief\nmotivational interviews to aid stress management. Yet, unlike\nstructured CBT apps like Woebot or Wysa, generative models are\nmore unpredictable and require careful oversight to avoid\nunintentional reinforcement of maladaptive behaviors (\n11–13).\n2.5 Interactive multimedia education\nPlatforms like YouTube Health and patient portals may soon\nintegrate LLMs to generate captions, voiceover explanations, or\npersonalized summaries. While this expands accessibility,\nespecially for users with limited literacy or disabilities, it also\nraises questions about narrative accuracy, cultural tone, and\nsource transparency. Without clear attribution or a peer-review\nlayer, misinformation can be embedded in otherwise engaging\nformats (\n2, 14).\n3 Clinical safety challenges of large\nlanguage models in consumer health\nWhile the applications are promising, Generative AI offers no\nclinical assurances. Because LLMs are trained on vast datasets, their\ncontent is probabilistic and not authoritative. This leads to risks in\nseveral key areas:\nTilton et al. 10.3389/fdgth.2025.1616488\nFrontiers in\nDigital Health 02 frontiersin.org\n3.1 Hallucination and inaccuracy\nLLMs may fabricate references, cite non-existent studies, or\nassert medically unsound conclusions with unwarranted\nconﬁdence (\n15).\n3.2 Ambiguity and false reassurance\nAI may use medically plausible language without clinical\nprecision, omit red ﬂag symptoms, or fail to communicate the\nurgency of evaluation (16).\n3.3 Bias and representation\nBiases due to unbalanced training data can lead to outputs that\nreﬂect racial, gender, or socioeconomic inequities, further\nmarginalizing vulnerable populations (17).\n3.4 Over-reliance and self-diagnosis\nPatients may delay care, misinterpret information, or bypass\nprovider consultation due to perceived AI authority (18).\n3.5 Data privacy and consent\nMany AI interfaces lack robust user disclosures regarding data\ncollection, storage, and secondary use — particularly in non-\nclinical settings.\n3.6 Ethical issues\nLack of transparency in how outputs are generated, unclear\naccountability when harm occurs, as well as in addition to the\nbiases and privacy challenges mentioned previously contribute to\na growing list of ethical concerns related to LLM use in patient-\nfacing communication (\n19).\nThese concerns demand careful evaluation, particularly when\ntools are positioned for use outside of clinician oversight.\n4 Lessons from analogous tools in\ndigital health\nGenerative AI is often described as revolutionary, but many of\nthe challenges it presents mirror earlier efforts in digital health.\nFrom symptom checkers to decision trees, healthcare has long\nexperimented with automated tools meant to support patient\nunderstanding and behavior.\n4.1 Legacy tools and their limitations\nEarly tools like WebMD, Ada Health, and Babylon offered\ntriage assistance or self-diagnosis checklists. While helpful in\nsome contexts, these platforms frequently delivered exhaustive\nlists of potential conditions with little contextual nuance. Their\nrigidity and lack of personalization limited their usefulness and\noften increased patient anxiety.\n4.2 How generative AI differs— and doesn’t\nUnlike traditional rule-based tools, LLMs offer free-form,\nconversational responses. This opens new doors in patient\nengagement but introduces risks not seen in older tools —\nparticularly hallucination, overconﬁdence, and context loss. LLMs\ncan misrepresent conditions, fail to re ﬂect urgency, or overly\nreassure users even when symptoms warrant escalation.\n5 RECAP: a digital health framework\nfor evaluating generative AI in\nconsumer health\nTo guide developers, evaluators, and regulatory reviewers, the\nfollowing ﬁve-point RECAP framework outlines clinician-\ninformed criteria for consumer-facing AI health tools. Each\nelement is rooted in experiences of patient communication and\ndigital health challenges:\n5.1 Relevance\nAre responses speci ﬁc to the user ’s question, contextually\nappropriate, and culturally sensitive? A useful tool shouldn’t offer\njust plausible answers — it must speak to the individual\npatient’s concern.\n5.2 Evidence-based\nAre outputs grounded in current clinical practice guidelines\nand appropriately cited? Without a clear foundation in evidence,\nAI risks becoming a digital oracle rather than a trustworthy\nhealth partner.\n5.3 Clarity\nIs language health-literate, avoiding jargon and offering\naccessible analogies where needed? If the message is lost in\ntranslation, it might as well not be delivered at all.\nTilton et al. 10.3389/fdgth.2025.1616488\nFrontiers in\nDigital Health 03 frontiersin.org\n5.4 Adaptability\nCan outputs adjust to differing levels of user education, age, or\ncondition complexity? The best tools feel tailored, not templated.\n5.5 Precision and safety\nDoes the tool recognize its limitations, defer to professional\ncare where warranted, andﬂag potentially urgent issues? A tool’s\nvalue is deﬁned not only by what it says, but also by what it\nknows not to say.\nThis original digital health framework provides clinicians,\ndevelopers, and platform moderators with a pragmatic tool for\nevaluating clinical safety and health literacy. Rather than\nassessing novelty alone, RECAP centers on usability, accuracy,\nand clinical understanding.\n6 Applying the digital health\nframework for clinical safety and\nhealth literacy\nThe RECAP framework offers a structured lens to evaluate\nthe quality, safety, and effectiveness of AI-generated outputs\nin patient-facing applications. It was developed through a\ncombination of clinical experience, evaluation of digital health\nconcepts, and observed challenges with current AI outputs.\nWhile inspired in part by principles from health literacy and\ndigital health ethics, RECAP extends beyond prior models by\nintegrating frontline clinical priorities such as contextual\nrelevance and clinical safety, which are often missing from\ntechnical evaluation tools.\nTo illustrate its practical application,\nTable 1 evaluates sample\noutputs from a hypothetical chatbot responding to a basic\nsymptom inquiry across the ﬁve RECAP domains. These\nexamples highlight how subtle variations in language, framing,\nand speci ﬁcity can have a meaningful impact on patient\ninterpretation, perceived credibility, and clinical risk.\nRelevance assesses whether AI-generated responses directly\naddress the patient’s speciﬁc question or concern in a clinically\nappropriate and situationally-aware manner. A relevant output\ndemonstrates clear understanding of the patient ’s context,\nincluding symptoms, medical history, and stated needs, rather\nthan offering broad, generic, or tangential information. Irrelevant\nresponses may cause patients to be confused, delay seeking care,\nor lose trust in digital tools. Relevance emphasizes alignment\nbetween user intent and clinical coherence.\nEvidence-based standards assess whether the AI tool ’s\nresponses are grounded in current, authoritative medical\nguidelines or credible clinical sources. Unlike traditional web\nsearch results, generative AI models synthesize probabilistic\ninformation and can sometimes fabricate data or echo outdated\npractices. Tools are more trustworthy and reduce the risk of\nmisinformation when they reference established clinical bodies,\nsuch as the CDC, WHO, or peer-reviewed guidelines. An\nevidence-based approach ensures that patients receive care-\naligned guidance and minimizes the clinical and ethical risks of\nAI-mediated communication.\nClarity is essential for health literacy. Messages from the\nchatbot should be written in plain language, avoiding ambiguity,\njargon, or vague calls to action. For example, saying “see a\ndoctor within 48 h” provides more actionable guidance than\n“consider seeking professional attention soon,” which could be\nmisinterpreted or ignored. In digital health tools, especially those\nused without clinician guidance, clarity directly impacts whether\npatients take safe, timely, and informed actions.\nAdaptability re ﬂects the AI tool ’s capacity to tailor\ncommunication to diverse patients with varying needs, health\nliteracy, cultural backgrounds, and clinical complexity. For\ninstance, a well-adapted output will deliver simpli ﬁed, non-\ntechnical language for patients who are unfamiliar with medical\nterms and offer more detailed or nuanced guidance for those\nwho have experience with their chronic illness. Tools that lack\nadaptability risk alienating patients, overwhelming them with\njargon, or offering information that feels inaccessible. Effective\nadaptability ensures inclusivity and optimizes patient engagement\nacross a wide range of patient demographics.\nPrecision and Safety refer to the AI tool’s ability to deliver\nclinically accurate information while recognizing its own\nlimitations. A precise response uses correct terminology,\ncommunicates appropriate urgency, and avoids overgeneralized\nor misleading statements. In this context, safety involves\nappropriately deferring to human clinicians, especially in\nsituations that involve diagnostic uncertainty or potential risk.\nImportantly, LLMs must be designed to recognize red- ﬂag\nsymptoms, escalate when needed, and avoid implying certainty\nwhere uncertainty exists. Tools that acknowledge their limitations\ncan help preserve clinician safety and reduce patient over-reliance.\nTABLE 1 Evaluating AI chatbot responses using the RECAP framework:\nexamples of meeting and failing digital health evaluation standards.\nRECAP\ncriterion\nExample output:\nmeets standard\nExample output:\nfails standard\nRelevance “Given your symptoms, here’s\ntailored advice for your age and\ncondition…”\n“You may have a variety of\nconditions ranging from a\ncold to cancer”\nEvidence-based “According to CDC guidance\nupdated in 2023…”\n“People say ginger tea cures\ninfections”\nClarity “Your symptoms suggest you\nmay need to see a doctor within\n48 h”\n“You could consider seeking\nprofessional attention soon”\nAdaptability “For someone with your\ncondition and age, rest and\nhydration are especially\nimportant”\n“Rest and hydration are good\nfor anyone with these\nsymptoms”\nPrecision/\nsafety\n“Your symptom combination\nmay suggest X. Please call your\ndoctor or visit urgent care”\n“It’s likely nothing serious.\nWait and see”\nThis comparison reveals the subtle but critical distinctions in how AI-generated messages can\naffect patient perception and behavior. Tools that meet RECAP criteria demonstrate restraint,\nmedical accuracy, and contextual nuance— whereas those that fail may increase risk despite\nsounding helpful. As LLMs are increasingly integrated into health products, RECAP can offer\na shared rubric to elevate content quality and patient safety.\nTilton et al. 10.3389/fdgth.2025.1616488\nFrontiers in\nDigital Health 04 frontiersin.org\n7 Limitations\nThis review is narrative in nature and does not include any\nsystematic search or quantitative synthesis or analysis. The\nRECAP framework is intended as a guiding framework for\napproaching the evaluation of patient-facing AI tools in\nconsumer health. While grounded in clinical experience and\nliterature, RECAP remains a conceptual tool that has yet to be\nvalidated through empirical studies. Future directions include\nstructured ﬁeld testing of RECAP in real-world settings,\nincluding use with AI chatbot outputs across varied health\nconditions and populations. This will help assess interrater\nconsistency, practical utility, and correlation with clinical\noutcomes or user trust.\n8 Conclusion\nGenerative AI will increasingly shape how patients seek,\nunderstand, and act on health advice. Ensuring accuracy,\nrelevance, and safety is a shared responsibility. Clinicians must\nplay an active role— not only to improve the quality of these\ntools, but to protect the patients who rely on them. With\nframeworks like the one proposed here, we can begin to build a\nmore ethical, informed, and patient-centered AI future.\nIn doing so, we may not only enhance access to reliable health\ninformation but also restore something more essential— a balanced\napproach to medical information facilitating timely consensual\ndecision-making and trust— in an era when so much of health\ncommunication feels uncertain and incomplete. Similar to how\npatients are guided away from online misinformation, we now\nhave an opportunity to shape what responsible digital care will\nlook like.\nAuthor contributions\nAT: Visualization, Resources, Conceptualization, Writing –\nreview & editing, Writing – original draft. BCa: Writing –\noriginal draft, Resources, Visualization, Writing – review &\nediting. BCo: Visualization, Supervision, Project administration,\nWriting – original draft, Resources, Writing– review & editing.\nFunding\nThe author(s) declare that noﬁnancial support was received for\nthe research and/or publication of this article.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could\nbe construed as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Generative AI was used in the\ncreation of this manuscript.\nAny alternative text (alt text) provided alongsideﬁgures in this\narticle has been generated by Frontiers with the support of artiﬁcial\nintelligence and reasonable efforts have been made to ensure\naccuracy, including review by the authors wherever possible. If\nyou identify any issues, please contact us.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their afﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed\nor endorsed by the publisher.\nReferences\n1. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of\nGPT-4 on Medical Challenge Problems.arXiv [Preprint] arXiv:2303.13375. (2023).\nAvailable online at: http://arxiv.org/abs/2303.13375 (Accessed April 20, 2025).\n2. Google Health.Med-PaLM: A Medical Large Language Model. Mountain View, \nCA: Google Research (2025). Available online at: https://sites.research.google/med-\npalm/ (Accessed April 20, 2025).\n3. Bajwa J, Munir U, Nori A, Williams B. Artiﬁcial intelligence in healthcare:\ntransforming the practice of medicine. Future Healthc J . (2021) 8(2):e188 –94.\ndoi: 10.7861/fhj.2021-0095\n4. Kianian R, Carter M, Finkelshtein I, Eleswarapu SV, Kachroo N. Application of\nartiﬁcial intelligence to patient-targeted health information on kidney stone disease.\nJ Ren Nutr. (2024) 34(2):170–6. doi: 10.1053/j.jrn.2023.10.002\n5. Chow JCL, Li K. Large language models in medical chatbots: opportunities,\nchallenges, and the need to address AI risks.Information. (2025) 16(7):549. doi: 10.\n3390/info16070549\n6. Chow JCL, Li K. Developing effective frameworks for large language model–based\nmedical chatbots: insights from radiotherapy education with ChatGPT.JMIR Cancer.\n(2025) 11:e66633. doi: 10.2196/66633\n7. Aljohani M, Hou J, Kommu S, Wang X. A Comprehensive Survey on the\nTrustworthiness of Large Language Models in Healthcare. arXiv [Preprint]\narXiv:2303.13375. (2025). Available online at: http://arxiv.org/abs/2502.15871\n(Accessed April 20, 2025).\n8. Clark M, Bailey S. Chatbots in health care: connecting patients to information.\nCan J Health Technol. (2024) 4(1):4–12. doi: 10.51731/cjht.2024.818\n9. Ullah E, Parwani A, Baig MM, Singh R. Challenges and barriers of using large\nlanguage models (LLM) such as ChatGPT for diagnostic medicine with a focus on\ndigital pathology— a recent scoping review. Diagn Pathol. (2024) 19(1):43. doi: 10.\n1186/s13000-024-01464-7\n10. Chen S, Guevara M, Moningi S, Hoebers F, Elhalawani H, Kann BH, et al. The effect\nof using a large language model to respond to patient messages.Lancet Digit Health.\n(2024) 6(6):e379–81. doi: 10.1016/S2589-7500(24)00060-8\nTilton et al. 10.3389/fdgth.2025.1616488\nFrontiers in\nDigital Health 05 frontiersin.org\n11. Park S, Choi J, Lee S, Oh C, Kim C, La S, et al. Designing a chatbot for a brief\nmotivational interview on stress management: qualitative case study.J Med Internet\nRes. (2019) 21(4):e12231. doi: 10.2196/12231\n12. Aggarwal A, Tam CC, Wu D, Li X, Qiao S. Artiﬁcial intelligence-based chatbots\nfor promoting health behavioral changes: systematic review.J Med Internet Res. (2023)\n25:e40789. doi: 10.2196/40789\n13. Sarkar S, Gaur M, Chen LK, Garg M, Srivastava B. A review of the\nexplainability and safety of conversational agents for mental health to identify\navenues for improvement. Front Artif Intell. (2023) 6:1229805. doi: 10.3389/frai.\n2023.1229805\n14. Aydin S, Karabacak M, Vlachos V, Margetis K. Large language models in patient\neducation: a scoping review of applications in medicine. Front Med . (2024)\n11:1477898. doi: 10.3389/fmed.2024.1477898\n15. Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y, et al. Survey of hallucination in\nnatural language generation.ACM Comput Surv. (2023) 55(12):248.1–38. doi: 10.1145/\n3571730\n16. Yau JYS, Saadat S, Hsu E, Murphy LSL, Roh JS, Suchard J, et al. Accuracy of\nprospective assessments of 4 large language model chatbot responses to patient\nquestions about emergency care: experimental comparative study. J Med Internet\nRes. (2024) 26:e60291. doi: 10.2196/60291\n17. Krittanawong C, Johnson KW, Rosenson RS, Wang Z, Aydar M, Baber U, et al.\nDeep learning for cardiovascular medicine: a practical primer.Eur Heart J. (2019)\n40(25):2058–73. doi: 10.1093/eurheartj/ehz056\n18. Shekar S, Pataranutaporn P, Sarabu C, Cecchi GA, Maes P. People over trust AI-\ngenerated medical responses and view them to be as valid as doctors, despite low\naccuracy. arXiv [Preprint] arXiv:2408.15266. (2024). Available online at: http://\narxiv.org/abs/2408.15266 (Accessed April 20, 2025).\n19. Chow JCL, Li K. Ethical considerations in human-centered AI: advancing\noncology chatbots through large language models. JMIR Bioinforma Biotechnol .\n(2024) 5:e64406. doi: 10.2196/64406\n20. Chow JCL, Li K. Developing effective frameworks for large language model–\nbased medical chatbots: insights from radiotherapy education with ChatGPT.JMIR\nCancer. (2025) 11:e66633.\nTilton et al. 10.3389/fdgth.2025.1616488\nFrontiers in\nDigital Health 06 frontiersin.org"
}