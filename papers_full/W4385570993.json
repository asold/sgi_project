{
  "title": "Reasoning in Large Language Models Through Symbolic Math Word Problems",
  "url": "https://openalex.org/W4385570993",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4316057875",
      "name": "Vedant Gaur",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2607936157",
      "name": "Nikunj Saunshi",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4283768109",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4385571219",
    "https://openalex.org/W3092185277",
    "https://openalex.org/W3000716014",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3138392969",
    "https://openalex.org/W4313428540",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4286769130",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a \"concise explanation\" of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP-Sym dataset will be released for future research on symbolic math problems.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 5889–5903\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nReasoning in Large Language Models Through\nSymbolic Math Word Problems\nVedant Gaur∗\nAragon High School\nvedantgaur101@gmail.com\nNikunj Saunshi†\nGoogle Research, New York\nnsaunshi@google.com\nAbstract\nLarge language models (LLMs) have revolu-\ntionized NLP by solving downstream tasks\nwith little to no labeled data. Despite their\nversatile abilities, the larger question of their\nability to reason remains ill-understood. This\npaper addresses reasoning in math word prob-\nlems (MWPs) by studying symbolic versions\nof the numeric problems, since a symbolic\nexpression is a “concise explanation” of the\nnumeric answer. We create and use a symbolic\nversion of the SV AMP dataset and find that\nGPT-3’s davinci-002 model also has good zero-\nshot accuracy on symbolic MWPs. To evaluate\nthe faithfulness of the model’s reasoning, we\ngo beyond accuracy and additionally evaluate\nthe alignment between the final answer and\nthe outputted reasoning, which correspond to\nnumeric and symbolic answers respectively for\nMWPs. We explore a self-prompting approach\nto encourage the symbolic reasoning to align\nwith the numeric answer, thus equipping the\nLLM with the ability to provide a concise\nand verifiable reasoning and making it more\ninterpretable. Surprisingly, self-prompting also\nimproves the symbolic accuracy to be higher\nthan both the numeric and symbolic accuracies,\nthus providing an ensembling effect. The\nSV AMP-Sym dataset will be released for\nfuture research on symbolic math problems.\n1 Introduction\nLarge language models (LLMs), with hundreds\nof billions of parameters, can solve a wide\nrange of NLP tasks such as machine translation,\nquestion-answering, etc., taking us closer to\ngeneral-purpose intelligent agents. The initial\nsuccess of GPT-3 (Brown et al., 2020) has led\nto many other LLMs (Rae et al., 2021; Smith\net al., 2022; Chowdhery et al., 2022) which have,\nperhaps surprisingly, taken big strides in solving\n∗Some clarification on affiliation.\n†Most of the work was performed while at Princeton\nUniversity and after graduating, but before joining Google.\ndifficult tasks like common sense reasoning, math\nand science problems (Lewkowycz et al., 2022),\nand writing code (Li et al., 2022).\nDespite the incredible successes, we have little un-\nderstanding of why LLMs are effective at problems\nthat require reasoning. In fact we have limited\ntechniques to quantifiably study the question of\nreasoning beyond just evaluating accuracy. Recent\nideas like Chain-of-Thought prompting (CoT)\n(Wei et al., 2022b; Kojima et al., 2022) encourage\nthe model to “think step by step” and output a\nverbose reasoning in text. However, verifying such\nreasoning at scale will incur the infeasible cost of\nmanually going over the text outputs. Furthermore,\nwe would like the model’s reasoning to be con-\nsistent with its outputted answer, in order to trust\nthe presented reasoning. For these considerations,\nwe would like our models to output a concise\nreasoning or explanation for its answer that can\nbe automatically verified. In particular, we desire\nreasoning in the form of explanations that are\n• Verifiable: For ease of evaluating correctness\nof the outputted reasoning, and\n• Concise: For scalability of verification. Manu-\nally going through text reasoning can quickly\nget cumbersome\nFor instance, instead of a text description of\nan algorithm to solve a problem, a Python\nimplementation of the algorithm would be a more\nconcise explanation for the reasoning behind\nthe algorithm1. Similarly, a simple linear model\nor decision tree explaining the answers of a\nblack-box neural network also achieves the same\ngoal (Ribeiro et al., 2016). Concise explanations\ncan provide clearer insights into the reasoning\nabilities of models, and verifiable explanations aid\ninterpretability and help foster trust in models, in\n1We can automatically verify the answer not just for one\nproblem, but for all instance of that problem\n5889\nline with explainable AI (Samek et al., 2019).\nIn this work we use concise and verifiable\nexplanations to study reasoning abilities of LLMs\nin math word problems (MWPs). LLMs have\nshown to achieve good zero-shot accuracy on many\nnumeric MWP benchmarks (Kojima et al., 2022).\nChain-of-thought like ideas encourage LLMs to\nfirst general a step-by-step explanation (in text)\nbefore generating the answer. However, this does\nnot satisfy the criteria of being concise or easily\nverifiable2. We address reasoning by considering\nsymbolic versions of numeric MWPs, because a\nsymbolic expression can be viewed as a concise\nexplanation for a numeric answer and can also be\nautomatically evaluated. Thus in this reasoning\nframework for MWPs, we require an LLM to out-\nput both, a numeric answer and a concise symbolic\nexpression, such that we have: (1) high accuracy\nfor the predicted numeric answer, (2) high align-\nment of the symbolic expression with the predicted\nnumeric answer. While most prior studies focus on\ngoal (1), we argue that goal (2) is equally important\nfor interpretability of these models and to trust\nthe its reasoning. Our main finding is that LLMs\ncan also do reasonably well on goal (2), either\nby generating a numeric answer and symbolic\nexplanation together, or by generating the answer\nfirst and then a post-hoc symbolic explanation. In\nthis context, we make the following contributions:\nSymbolic evaluation. We construct a symbolic\nversion of the SV AMP dataset (Patel et al.,\n2021) called SV AMP-Sym to evaluate LLMs.\nFirstly we find, perhaps surprisingly, that GPT-3’s\ndavinci-002 model already achieves good zero-shot\naccuracy on symbolic problems (64.2%), compa-\nrable to the numeric accuracy of 68.9%. Secondly,\nthis observation provides a simple way to get good\naccuracy and alignment for numeric problems by\nfirst solving symbolic versions and then substitut-\ning back the values for variables. This approach\ngenerates the numeric answer and a symbolic\nexplanation in one go, thus trivially achieving3 an\naccuracy of 64.2% and alignment of 100%.\nSelf-prompting. There are two key drawbacks\nwith the above approach: (a) symbolic accuracy of\n64.2% is lower than the numeric accuracy (68.9%),\n(b) alignment of symbolic expressions, as post-hoc\n2It is not uncommon for the outputted reasoning to be\ninconsistent with the final answer\n3If a “calculator” can evaluate symbolic expressions.\nexplanation to the original numeric answers, is very\nlow (∼50%). To get a better post-hoc explanation,\nwe propose a novel self-prompting approach that\nfirst prompts the LLM with the numeric problem\nand its response to the problem, and then asks\nit to solve the symbolic problem; see Figure 1.\nSelf-prompting significantly improves alignment\nwith numeric answers to 74% (a 24% absolute\nimprovement). Surprisingly, self-prompting also\nimproves the symbolic accuracy to 71.7%, higher\nthan both the raw numeric and symbolic accuracies\nof 68.9% and 64.2% respectively. This suggests\nthat self-prompting has an ensembling effect.\nWe perform further ablation studies and analyses\nand hope that these insights will aid future work\non using LLMs for reasoning problems.\n1.1 Related Work\nLanguage models like GPT-3 (Brown et al., 2020)\nand MLMs like BERT (Devlin et al., 2019) have\ndemonstrated impressive emergent behaviors (Wei\net al., 2022a) at scale. For math problems, Minerva\n(Lewkowycz et al., 2022) was fine-tuned from\nPaLM (Chowdhery et al., 2022) to do well on many\nMWP benchmarks. Instead of fine-tuning, Wei\net al. (2022b) uses in-context learning and finds\nthat asking the model to “think step by step” (CoT\nprompting) improves few-shot accuracy on MWPs;\nKojima et al. (2022) verify this for zero-shot\nsetting as well, which is the focus of our work.\nThere is limited theoretical work for the down-\nstream success of LMs (Saunshi et al., 2021;\nXie et al., 2022) and the emergent behaviors of\nLLMs through scaling laws (Kaplan et al., 2020).\nOur idea of self-prompting is motivated by the\nefficacy of in-context learning (Brown et al., 2020)\nand prompting (Liu et al., 2023) in LMs. The\nensembling effect of self-prompting idea could\nbe related to self-calibration abilities of LMs\n(Kadavath et al., 2022). Finally, Ho et al. (2022)\nsurvey the progress of LMs on various notions\nof reasoning; we consider a weaker notion of\n“concise post-hoc explanations” here.\n2 Math Word Problems with LLMs\n2.1 SV AMP-Sym Dataset\nWe choose the SV AMP dataset (Patel et al.,\n2021) for testing LMs on MWPs because it\nprovides numeric answers in the form of numeric\nexpressions (rather than just numeric values). This\n5890\nFigure 1: LMs can be queried to solve numeric/symbolic math problems. Self-prompting includes the numeric\nproblem and the LM’s solution to it before passing the symbolic problem. This encourages the model to output the\nanswer that aligns with the numeric answer. The symbolic expression w-x-y serves as a concise explanation/reason-\ning for the numeric answer of 2.\nlets us automatically convert the dataset into a\nsymbolized version, without manual annotation.\nThe main idea is to replace all occurrences of\nnumbers in the problem statement with newly\nintroduced variables, e.g. (w,x,y,z). Appendix A\nprovides more details on the dataset construction.\nThe dataset is released in https://github.com/\nvedantgaur/Symbolic-MWP-Reasoning.\n2.2 Querying and Evaluating LMs\nBroadly, our evaluation pipeline has four phases:\n(1) get a verbose response from the LLM for the\nmath problem, (2) prompt the LLM to extract just\nthe answer (number or symbolic expression) from\nits initial response, (3) refine the extracted answer\nusing a novel filtering step, (4) compare the filtered\nanswer to the ground-truth answer.\nInitial response. We query the LM with the\nproblem statement and an optional CoT prompt,\ni.e. \"Q: <Problem> A:\" or \"Q: <Problem> A:\nLet’s think step by step.\". <Problem> could\neither be a numeric or symbolic problem. Table 3\nsummarizes the prompts used for various settings.\nAnswer extraction. Since the LLM outputs a\nlong text response (Figure 1), we use an extraction\nprompt to isolate the answer, similar to Kojima\net al. (2022). We query the LM with the transcript\nso far, followed by the question and the prompt\n\"The final answer (only the number) is:\"\nto isolate the numeric answer. Table 3 has the\nsimilar prompt for symbolic problems.\nAnswer filtering. The extraction prompt does\nnot always isolate the final answer and sometimes\noutputs a sentence, especially for symbolic\nproblems. Thus we add a LM-independent filtering\nstep which includes stripping escape sequences,\nremoving commas, de-latexifying equations,\npicking the longest symbolic expression, among\nothers; more details in Appendix C.2.\nAnswer evaluation. We compare the filtered\nanswer to the ground-truth answer (symbolized\nexpression or numeric value). Since there are mul-\ntiple ways to express the same symbolic expression\n(e.g. \"w + (y + x)\" and \"w + x + y\" ), we\ncompare two expressions through their evaluations\non 20 random variable assignments. If they match\non all 20 assignments, we adjudge them to be\nequivalent, making a (reasonable) assumption that\n20 random assignments will avoid false positives.\n3 Experimental Results\nWe pick 150/1000 examples from the SV AMP\ndataset (due to budget constraints) and run each\nexamples 5 times. We use GPT-3’s davinci-002\nmodel with temperature 0.0 for (mostly) determin-\nistic outputs, with a max token length of 256.\n3.1 Numeric and Symbolic Evaluations\nWe discuss the accuracies for solving numeric\nand symbolic math problems from SV AMP and\nSV AMP-Sym respectively.\nNumeric accuracy. The zero-shot numeric\naccuracy both with chain-of-thought (CoT) prompt\nand without (vanilla) are presented in Table 1;\nthey are 68.9% and 65.6% respectively. This good\nperformance is unsurprising given prior work\n(Kojima et al., 2022). Our accuracies are ∼5-7%\nhigher than Kojima et al. (2022), due in part to\nbetter answer extraction and filtering.\nSymbolic accuracy. We also evaluate raw\nsymbolic problems from SV AMP-Sym in the\nvanilla and CoT settings with 3 natural choices for\nvariables: (w,x,y,z), (i,j,k,l) and (p,q,r,s).\nFirstly we observe, in Table 1, that GPT-3 can\n5891\nNumeric Symbolic\n(w,x,y,z) (p,q,r,s) (i,j,k,l)\nEvaluation Raw (-F) Raw (-F) SP (-F) SP + AP Raw Raw\nAccuracy Vanilla 65.6 (61.6) 59.7 (47.6) 61.9 (40) 68.3 62.3 53.5\nCoT 68.9 (65.9) 64.2 (48.8) 67.9 (48.6) 71.7 64.4 58.4\nAlignment Vanilla - 52.9 (40.7) 60.3 (40) 64.9 56.3 44.7\nCoT - 51.2 (39.1) 63.1 (44.9) 74 51.9 47.1\nSimilarity Vanilla - 27.8 44.2 49.8 27.1 26.8\n(BLEU) CoT - 21.3 53.9 57.6 22.7 21.4\nSimilarity Vanilla - 56.5 65.2 71.3 56.8 55.4\n(Levenshtein) CoT - 44.9 75.6 79.8 45.4 43.9\nTable 1: Zero-shot accuracy and alignment evaluations using GPT-3. All values are reported in %. “Raw” refers to\nevaluation on the SV AMP and (SV AMP-Sym) dataset for numeric (symbolic) MWPs; (-F) refers to the output before\nthe filtering step. “SP” is the new self-prompting method and “SP + AP” refers to two-stage self-prompting where\nwe an additional “Alignment Prompt” is added when needed; see Section 3.3. CoT prompting consistently elicits\nhigher accuracy from the model for numeric and symbolic problems. While accuracy and alignment only look at the\nfinal answers, we also measure similarity between the full responses for numeric and symbolic problems. As evident,\nself-prompting significantly improves the similarity under BLEU score and Levenshtein metric; Appendix B.1 has\nmore details on these metrics.\nachieve pretty high symbolic accuracies with\nvariables (w,x,y,z): vanilla and CoT settings\nachieve 59.7% and 64.2% respectively, which is\njust 4-5% lower than numeric accuracy. Further-\nmore, we notice that variables (i,j,k,l) have\nslightly worse accuracy than other variable settings,\npossibly because (w,x,y,z) and (p,q,r,s) are\nmore popular choice for variables in the training\ndata for language models.\nEffect of filtering. We report the accuracies\nwithout the filtering step in Table 1; these are\nthe (-F) entries. While there is a 4-5% drop in\nthe numeric accuracy without filtering, the drop\nis 12-14% for symbolic problems, suggesting\nthat filtering is much more crucial for symbolic\nproblems4. Our extraction and filtering steps still\nhave issues and there is scope for improvement.\n3.2 Reasoning and Alignment\nWhile prior work only cares about the accuracy\non MWPs, we also study of reasoning abilities\nof LLMs by requiring them to generate a concise\nexplanation for numeric answers in the form of\na symbolic expressions. We evaluate “reasoning\nability” through an alignment metric that checks\nif the outputted numeric answer and symbolic\nexpression compute to the same value. In general\nthere is no consistent zero-shot method to return\na perfectly aligned symbolic expression. A natural\n4Intuitively it makes sense that extracting an expression/e-\nquation is harder than extracting a single number\nattempt to generate such an expression is to\ndirectly solve the symbolic versions of numeric\nproblem. However this approach has very low\nalignment, i.e. the symbolic output does not reflect\nthe way in which the model solved the numeric\nproblem. Specifically in Table 1, the average\nalignment score for raw symbolic outputs is only\n52.9% and 51.2% for Vanilla and CoT respectively.\nThis motivates self-prompting.\n3.3 Self-prompting\nIn order to improve alignment, we propose a two-\nstep procedure that first inputs the numeric MWP\nand the LM’s response to it, followed by the\nsymbolic version of the MWP. In particular the\nprompt looks like \"Q: <Numeric Question> A:\n<Model Response> Q: <Symbolic Question>\nA:\". Given the in-context tendencies of LMs, we\nhope that this encourages the symbolic response to\nimitate the numeric response and thus return a well\naligned expression. We find in Table 1 that this ap-\nproach (termed SP) indeed improves the alignment\nby ∼10% over the naive approach.\nWe take this one step further: whenever the\nnumeric and symbolic answers do not align,\nwe add another “alignment prompt” before the\nsymbolic problem that explicitly asks the model\nto copy the numeric answer; see Table 3 for the\nexact format. Results in the SP+AP column\nof Table 1 verify that this leads to another 11%\nimprovement over SP and ∼22% improvement\n5892\nover raw symbolic. Surprisingly we find that\nSP+AP has higher accuracy than raw numeric and\nraw symbolic, suggesting a “best of both worlds”\nor ensembling phenomenon in action. Further\nanalysis in Figure 7 reveals how self-prompting\ncombines the benefits of numeric and symbolic.\nWe also compute the similarity between the full\nnumeric and symbolic responses. Table 1 reveals\nthat the average similarity is significantly higher for\nSP and SP+AP compared to raw symbolic. So not\nonly do the answers align more but also the full text\nresponses are very similar. Histograms of similarity\nscores can be found in Appendix B.1. Additional\nanalyses and results can be found in Appendix B.\n4 Conclusions and Future Work\nThis paper studies reasoning in LLMs for MWPs\nand results suggest that LMs are good at zero-shot\nsolving of symbolic MWPs, and that this ability\ncan lead to concise explanations. Self-prompting\nemerges as a promising idea to generate better ex-\nplanations and the ensembling effect demonstrated\nby it can potentially have other applications (left\nfor future work). Alignment with self-prompting,\nwhile significantly better than with raw symbolic\noutputs, still has a lot of scope for improvement.\nAspects that are not considered are few-shot learn-\ning of explanations and the role of temperature,\nwhich could improve accuracy and alignment.\nFinally the notion of “concise explanation” to study\nreasoning can have implications beyond MWPs.\nBroader Impact Statement. Given the incredi-\nble successes of LLMs, it is becoming increasingly\nimportant to study why they work and how to de-\nbug them when they are wrong. There are ongoing\ndebates and discussions about whether LMs are\nsimply “stochastic parrots” (Bender et al., 2021)\nor they actually “understand” language. Besides\nthere are also privacy concerns (Carlini et al., 2021)\nassociated with LLMs trained on extremely large\ncorpora. Our work attempts to formalize a weak\nnotion of “reasoning” in math problems that could\nhelp with improving the intepretability, and thus\ntrustworthiness, of such models. This is extremely\nimportant if LLMs are to be deployed in real-life\napplications. That said, any preliminary notion or\ndefinition of “reasoning in LLMs”, including the\none in this paper, should be taken with a healthy\ndose of skepticism.\nReferences\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the dan-\ngers of stochastic parrots: Can language models be too\nbig? In Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Processing\nSystems.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine Lee,\nAdam Roberts, Tom Brown, Dawn Song, Ulfar Erlings-\nson, et al. 2021. Extracting training data from large\nlanguage models. In 30th USENIX Security Symposium\n(USENIX Security 21).\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. 2022. Palm: Scaling language model-\ning with pathways. arXiv preprint arXiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of deep\nbidirectional transformers for language understanding.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1\n(Long and Short Papers). Association for Computational\nLinguistics.\nVedant Gaur and Nikunj Saunshi. 2022. Symbolic math\nreasoning with language models. In 2022 IEEE MIT Un-\ndergraduate Research Technology Conference (URTC).\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2022.\nLarge language models are reasoning teachers. arXiv\npreprint arXiv:2212.10071.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas Schiefer,\nZac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson,\net al. 2022. Language models (mostly) know what they\nknow. arXiv preprint arXiv:2207.05221.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. 2020. Scal-\ning laws for neural language models. arXiv preprint\narXiv:2001.08361.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\n5893\nAitor Lewkowycz, Anders Andreassen, David Do-\nhan, Ethan Dyer, Henryk Michalewski, Vinay Ra-\nmasesh, Ambrose Slone, Cem Anil, Imanol Schlag,\nTheo Gutman-Solo, et al. 2022. Solving quantitative rea-\nsoning problems with language models. arXiv preprint\narXiv:2206.14858.\nYujia Li, David Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago, et al.\n2022. Competition-level code generation with alpha-\ncode. arXiv preprint arXiv:2203.07814.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-train,\nprompt, and predict: A systematic survey of prompting\nmethods in natural language processing. ACM Comput-\ning Surveys.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple math\nword problems? In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Technolo-\ngies. Association for Computational Linguistics.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Mil-\nlican, Jordan Hoffmann, Francis Song, John Aslanides,\nSarah Henderson, Roman Ring, Susannah Young, et al.\n2021. Scaling language models: Methods, analy-\nsis & insights from training gopher. arXiv preprint\narXiv:2112.11446.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \" why should i trust you?\" explaining\nthe predictions of any classifier. In Proceedings of the\n22nd ACM SIGKDD international conference on knowl-\nedge discovery and data mining, pages 1135–1144.\nWojciech Samek, Grégoire Montavon, Andrea Vedaldi,\nLars Kai Hansen, and Klaus-Robert Müller. 2019. Ex-\nplainable AI: interpreting, explaining and visualizing\ndeep learning, volume 11700. Springer Nature.\nNikunj Saunshi, Sadhika Malladi, and Sanjeev Arora.\n2021. A mathematical exploration of why language\nmodels help solve downstream tasks. In International\nConference on Learning Representations.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared Casper,\nZhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay\nKorthikanti, et al. 2022. Using deepspeed and megatron\nto train megatron-turing nlg 530b, a large-scale genera-\ntive language model. arXiv preprint arXiv:2201.11990.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Bar-\nret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, et al. 2022a.\nEmergent abilities of large language models. arXiv\npreprint arXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference. In International\nConference on Learning Representations.\nA Symbolized Dataset\nWe employ a multi-step process to convert the orig-\ninal SV AMP (Patel et al., 2021) prompts into its\nsymbolic version SV AMP-Sym, motivated by the\nsymbolic construction in Gaur and Saunshi (2022).\nThe SV AMP dataset is under the MIT License.\nOur SV AMP-Sym dataset has exactly the same\nset of 1000 problems as SV AMP. Given a com-\nmon math word problem (MWP) from SV AMP,\nwe parse through the given text for all numbers,\nwhich are stored in a list. Using regex, the index\nof the numbers can be found and replaced with\nkeys for future replacement with variables. We use\n<i> (where i ∈[1, 4] as there are at most four\nnumbers in each problem definition) as keys. As\nshown in Figure 4, by generalizing the converted\nprompt, we allow for easy manipulation of prompts\nto whatever variable a user wants to use and test\nfor downstream tasks. We then convert the keys\nto their respective variables. For our tests we pri-\nmarily use the variables (w,x,y,z) for a few main\nreasons:\n1. This set of variables is the most common in\ngeneral mathematical word problems and thus\nmakes the most sense to use as variables as op-\nposed to an arbitrary sequence of random, or\neven consecutive letters.\n2. We find that the use of variables such as x1, x2,\n..., xn (x1, x2, ..., xn when inputted into the\nmodel) many times confuses the model into con-\nflating the simulated subscript as a coefficient.\n3. We are able to see that the model achieves sim-\nilar, if not greater accuracies with the use of\n(w,x,y,z) as opposed to other sequences of\nvariables, see Table 1.\nMoreover, the use of a predetermined length of\nvariables is also possible due to the aforementioned\nmaximum number of four numbers for each prompt\nin the SV AMP dataset.\nSee Figure 4 for an example problem, its answer,\nand our symbolized version of it.\n5894\nB Ablations and Analysis\nB.1 Response Similarity\nTo find the syntactical similarity between the nu-\nmeric and symbolic responses, we employ two\nmain metrics: BLEU Scores and Levenshtein Dis-\ntances. BLEU score is a standard metric used to\njudge similarity between sentences based on the\nn-grams they share. Levenshtein distance (also\nknown as edit distance) is a standard metric to\ndistance between two strings: the minimum of\nswap/deletion/insertion operations that are needed\nto convert one string to another. To measure simi-\nlarity between s1 and s2, we use (maxlen(s1, s2)−\nLevenshtein(s1, s2))/maxlen(s1, s2)/ Using the\nnltk.translate.bleu_score module, we define the av-\nerage of BLEU-1, BLEU-2 and BLEU-3 metrics\nby passing weights=[1/3, 1/3, 1/3] in the sen-\ntence_bleu function. For computing Levenshtein\nDistances, we utilize the python-Levenshtein pack-\nage’s distance function. As described in the his-\ntograms presented in Figure 5 and Figure 6, we\nfind much higher similarity scores when employing\nself-prompting. This logically follows the higher\nalignment values of such runs. More specifically,\nhowever, the similarity of the two scores is ulti-\nmately more contingent on the verbiage of the out-\nput. As indicated in Figure 1, the SP often closely\ntracks the exact output of the numeric response\nand simply replaces the numbers with the respec-\ntive variables/symbolic expressions, and outputs an\nexpression instead of a final number. While metri-\ncally evident in the provided plots, we see that this\n“mirroring” phenomenon occurs frequently with\nthe use of SP, evident through the high density of\nsimilarity scores close to 1 in Figure 5.\nB.2 More on Alignment\nWhile we find that the use of the alignment prompt\nis effective in raising both the accuracy and align-\nment of a symbolic problem, we run a few supple-\nmentary experiments to investigate this behavior\neven further. When giving the model the align-\nment prompt (see Table 3) from the beginning, not\nsimply when the numeric and symbolic outputs\ndo not align, we actually find a decrease in accu-\nracy from the self-prompting + alignment prompt\nrun. CoT accuracy is 62% and vanilla accuracy is\n60.9%. Similarly, alignment accuracies are 61.5%\nand 60.4% for CoT and vanilla, respectively. When\nevaluating alignment for the base self-prompting\nrun, we find that the model aligns 83.9% when\nthe numeric output is correct, and 29.7% when\nit is wrong. Such numbers possibly suggest the\nmodel’s cognizance of whether or not the numeric\nevaluation was performed correctly; an implicit\nunderstanding of mathematical problem solving.\nB.3 Difficulty of Problems\nWe highlight a metric for the difficulty of a problem\nwith respect to the primary operation performed in\nthe answer to the prompt. The SV AMP dataset\nstores a “Type” key that denotes the primary ele-\nmentary mathematical operation performed to get\nto the answer (primary in cases where there is more\nthan one operation). We see that when graphing the\naccuracies of various evaluation methods while iso-\nlating the operation of the problem that the numeric\nand symbolic runs exhibit a somewhat complemen-\ntary behavior. While numeric does on average bet-\nter on problems with division, symbolic runs have\nhigher accuracy on multiplication, see Figure 7.\nTable 2 has breakdowns of the exact accuracies\nper each tag. Interestingly, the self-prompting ap-\nproach seems to do well on both multiplication and\ndivision, and its performance is close to the max\nof the numeric and symbolic performance for each\ncategory, thus hinting to a “best of both worlds”\nphenomenon.\nC Additional Details\nC.1 Prompt formats\nIn the SV AMP dataset, each problem contains a\nproblem statement and a question. For both raw\nnumeric and symbolic evaluations, we input the\nproblem into the model with the CoT prompt if\nappropriate. For self-prompting, however, in order\nto increase alignment between the numeric and\nsymbolic outputs, we add the entire transcript of the\nnumeric evaluation (problem, answer prompting,\nsymbolic problem). A detailed transcript of each\nof the different prompts and use cases can be found\nin Table 3.\nC.2 Filtering\nSince there is high variability in the LM’s out-\nputs, due to the necessity to reason when solving\na MWP, we employ several filtering techniques in\na filter() function that cleans up the extracted\nnumeric or symbolic output. A few main steps in\nthe filtering pipeline are as follows:\n• Character replacing\n5895\nAccuracy (%)\nEvaluation Addition Subtraction Multiplication Division\nNumeric CoT 64.7 64.3 68 88.1\nVanilla 54.1 62.8 68 87.4\nSymbolic\n{w, x, y, z}\nCoT 64.1 58.8 90 70.4\nVanilla 41.2 63 90 62.2\nSelf-prompting\n{w, x, y, z}\nCoT 67.6 66.1 94 85.2\nVanilla 60 61.3 80 73.3\nTable 2: While the accuracies presented are fairly consistent within each separate evaluation run, we see that there\nare clear biases in which the model is able to perform certain types of problems better depending on the context of\nthe run. Significantly, it should be noted that the self-prompting is able to employ both the efficiencies of numeric,\nand symbolic runs with the increased alignment.\nExample <Numeric Setup> = \"Adam had 5 apples. He ate 2 of them for breakfast.\"\n<Numeric Question> = \"How many apples will he have left if he eats 1 more?\"\n<Symbolic Setup> = \"Adam had w apples. He ate x of them for breakfast.\"\n<Symbolic Question> = \"How many apples will he have left if he eats y more?\"\nPrompts <CoT Prompt> = \"Let’s think step by step.\"\n<Numeric Extract Prompt> = \"The final answer (only the number) is:\"\n<Symbolic Extract Prompt> = \"The final answer (only the expression in terms\nxxxxxxxxxxxxxxxxxxxxxxxxxxxx of given variables) is:\"\n<Align Prompt> = \"Copy the above numeric response word to word but\nxxxxxxxxxxxxxxxxx replace numbers with the right symbolic expression.\"\nNumeric Q: <Numeric Setup> <Numeric Question>\nA: <CoT Prompt> <Numeric Response> // language model’s verbose response\n<Numeric Question> <Numeric Extract Prompt>\n<Numeric Extracted>\nSymbolic Q: <Symbolic Setup> <Symbolic Question>\nA: <CoT Prompt> <Symbolic Response> // language model’s verbose response\n<Symbolic Question> <Symbolic Extract Prompt>\n<Symbolic Extracted>\nSelf-prompt Q: <Numeric Setup> <Numeric Question>\nA: <CoT Prompt> <Numeric Response>\n<Align Prompt> // [optional] only if alignment fails without it\nQ: <Symbolic Setup> <Symbolic Question>\nA: <CoT Prompt> <Symbolic Response>\n<Symbolic Question> <Symbolic Extract Prompt>\n<Symbolic Extracted>\nTable 3: We present the prompting pipeline for various methods. Prompts in blue are the ones we pass to the model,\nwhile the text in green are the output of the language model. In each of these methods, we include a final filtering\nstep on top of the extracted answers.\n– Dollar signs\n– Percentages\n• Cleaning up the output by removing all words\nbesides the expression and/or final number\n• Addressing cases of outputs such as code or\nLATEX\n• Isolating the outputted/final expression if the\nanswer is given in terms of an equation (say\n“z = w + x”)\nThe detailed (pseudo) code of the function can be\nfound at the end.\n5896\nOriginal Question\nAll numbers converted into key<i>\nKeys converted into inputted variables\nSymbolized prompt\nRun prompt on GPT-3 Without Steps prompt\nWith Steps prompt\nFirst run output\nExtract final answer\nFilter extracted output\nFinal output\nCompare converted expression and final output\nReturn 1\nReturn 0\nOriginal Expression\nSymbolized expression\nwithout steps\nwith steps\nexpression?\nincorrect\ncorrect\nFigure 2: Flowchart of the pipeline from an original expression to correct or incorrect outputs. The purple cells\nrepresent the outputs of the GPT-3 model as well as output processing. Both the \"Original Expression\" and \"Original\nQuestion\" at the top in rectangular cells are numeric, baseline prompts.\n5897\nFigure 3: Examples of the input-output GPT-3 sequence of both numeric (above) and symbolic (below) runs. CoT\nprompting, as reported in previous papers, elicits much more detailed, and oftentimes correct outputs from the model\nthrough the additional reasoning step. We find that the use of the prompt is not exclusive to numeric reasoning, and\nare able to identify similar processes in symbolic runs.\nFigure 4: The process of converting a numeric problem into a symbolic one. The answer to the problem is an\nexpression given by the SV AMP dataset, so we can easily convert it to a symbolic equation. Appendix A has more\ndetails on how this symbolization was implemented.\n5898\n(a) Raw symbolic\n (b) Self-Prompting\n (c) Self-Prompting + Alignment Prompt\nFigure 5: Levenshtein distance calculated on raw symbolic, self-prompting, and self-prompting with an additional\nalignment prompt outputs. Values near 1.0 (to the right) denote two sentences with very similar syntactic similarity.\nAs evident in the graphs above, the distribution of both (b) and (c) are much more heavily skewed to the right with\nunimodal peaks near 1.0, whereas the distribution in (a) is shifted much more to the left. This means that both (b)\nand (c) are much more similar to the outputs they were compared with (numeric) than (a), highlighting the efficacy\nof self-prompting in mirroring numeric responses.\n(a) Raw symbolic\n (b) Self-Prompting\n (c) Self-Prompting + Alignment Prompt\nFigure 6: BLEU Scores calculated on raw symbolic, self-prompting, and self-prompting with an additional alignment\nprompt outputs. As with Figure 5, values near 1.0 (to the right) denote two sentences with very similar syntactic\nsimilarity. In this instance, the BLEU Score was calculated by tokenizing and comparing the numeric outputs with\nrespective outputs in (a), (b), and (c). This value was then normalized and plotted as described in the figures above.\nBoth (b) and (c) both show more left-skewed distributions, while (a) models a right-skewed one. Similar to Figure 5,\nthe use of BLEU Scores highlights how self-prompting helps with the alignment of numeric and symbolic outputs.\n5899\nFigure 7: Accuracies of the “tag” of the prompt inputted into the model based on the evaluation method of the\nmodel. We observe that numeric consistently performs above average at division, symbolic at multiplication,\nand self-prompting at both. By combining the strengths of both numeric and symbolic evaluation, we see that\nself-prompting is able to perform as well, if not better than both numeric and symbolic prompting. Furthermore,\nas with general accuracy CoT also seems to provide boosts to addition accuracies, emphasized especially when\ncomparing symbolic evaluations (Vanilla and CoT).\n5900\ndef filter_symbolic ( response ):\nresponse = response . lower ()\nresponse = response . strip ( '\\n ')\nprint (f\" Original Output : { response }\")\n# De - latexifying\nresponse = LatexNodes2Text (). latex_to_text ( response )\nresponse = response . replace (\"$\", \"\")\n# Using * as multiplication operator\nresponse = response . replace ( '.', '*')\n# Handling the division symbol\nresponse = response . replace (\" %\", \"\")\nresponse = response . replace ( '\\ u00F7', '/')\n# Remove spaces and construct a boolean array denoting whether\n# the character is in the set {'w ', 'x ', 'y ', 'z ', '/', '*', '+', '-', '(', ')'}\nmath_sym_set = set ([ 'w', 'x', 'y', 'z', '/', '*', '+', '-', '(', ')'] +\\\n[ str (a) for a in range (10)])\n# Check for \" words \" that only contain chars from math_sym_set\nresponse = response . replace (\"=\", \" = \")\nwords = response . lower (). split ()\nis_math_sym = np. array ([np. all ([c in math_sym_set for c in word ])* len ( word )for\nword in words ])\n# Pick the substring with non - zero entries that has the largest sum ,\n# i.e. the largest substring of the original string that is an equation /\nexpression\nidx , len_ = longest_sum ( is_math_sym )\nresponse = '' . join ( words [ idx : idx + len_ ])\nprint ( response )\n# Add multiplication operator * if needed .\n# Logic : If neither of two consecutive characters is an operator\n# then likely a multiplication operator needs to be added between them .\n# Some edges cases like '(p ' or 'q) ' are handled\nop_set = set ([ '/', '*', '+', '-'])\ndigit_set = set ([ str (a) for a in range (10)])\nnew_response = []\nfor i in range ( len ( response )):\nnew_response . append ( response [i])\n# Check if '*' needs to be added\nif i < len ( response )-1 and response [i] not in op_set and response [i+1] not\nin op_set :\n# No need to add '*' if the consecutive chars of the type '(p ' or 'q) '\nof '25 '\nif ( response [i] != '(' and response [i+1] != ')') and ( response [i] not in\ndigit_set or response [i+1]\nnot in digit_set ):\nnew_response . append ( '*')\nprint (f\" Final Output : { new_response }\")\nreturn '' . join ( new_response )\nreturn output\ndef filter_numeric ( response ):\noutput = str ( response ). replace (\",\", \"\")\noutput = output . replace (\"$\", \"\")\noutput = output . strip ( '\\n ')\ntry :\noutput = int (re. findall ( '\\d+ ', output )[0])\nexcept :\noutput = output\nreturn output\n5901\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 4\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. The paper mostly deals with fundamental understanding of LLMs, which can help\nmitigate potential risks of LLMs\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 2.1\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 2.1\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSection A\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection A\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe directly adapted an existing dataset and replaced numbers with variables\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nThe dataset is a derivate of another dataset, and thus imports all of its properties\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection A\nC □\u0013 Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. Left blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5902\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. Left blank.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection B.1\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n5903",
  "topic": "The Symbolic",
  "concepts": [
    {
      "name": "The Symbolic",
      "score": 0.6905303597450256
    },
    {
      "name": "Computer science",
      "score": 0.662542998790741
    },
    {
      "name": "Symbolic data analysis",
      "score": 0.5212405323982239
    },
    {
      "name": "Word (group theory)",
      "score": 0.5126497149467468
    },
    {
      "name": "Symbolic execution",
      "score": 0.4712068438529968
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4638146758079529
    },
    {
      "name": "Verifiable secret sharing",
      "score": 0.4605395793914795
    },
    {
      "name": "Symbolic trajectory evaluation",
      "score": 0.42056241631507874
    },
    {
      "name": "Natural language processing",
      "score": 0.4154689610004425
    },
    {
      "name": "Theoretical computer science",
      "score": 0.355682909488678
    },
    {
      "name": "Programming language",
      "score": 0.23103651404380798
    },
    {
      "name": "Linguistics",
      "score": 0.19125452637672424
    },
    {
      "name": "Model checking",
      "score": 0.1867184340953827
    },
    {
      "name": "Software",
      "score": 0.09252440929412842
    },
    {
      "name": "Psychology",
      "score": 0.08939051628112793
    },
    {
      "name": "Psychoanalysis",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}