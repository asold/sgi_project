{
    "title": "Sensitive remote homology search by local alignment of small positional embeddings from protein language models",
    "url": "https://openalex.org/W4387969474",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2130580378",
            "name": "Sean R. Johnson",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5113127192",
            "name": "Meghana Peshwa",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2103080684",
            "name": "Zhiyi Sun",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2158714788",
        "https://openalex.org/W2949476196",
        "https://openalex.org/W2130479394",
        "https://openalex.org/W4213112325",
        "https://openalex.org/W3143063265",
        "https://openalex.org/W2142678478",
        "https://openalex.org/W2902353954",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2138122982",
        "https://openalex.org/W3177500196",
        "https://openalex.org/W4288077714",
        "https://openalex.org/W4385255463",
        "https://openalex.org/W2143210482",
        "https://openalex.org/W4282922306",
        "https://openalex.org/W4388823609",
        "https://openalex.org/W2069458148",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W4386860638",
        "https://openalex.org/W2160378127",
        "https://openalex.org/W4321748128",
        "https://openalex.org/W4327550249",
        "https://openalex.org/W2110115297",
        "https://openalex.org/W4281790889",
        "https://openalex.org/W3095583226",
        "https://openalex.org/W4390590489",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4308687190",
        "https://openalex.org/W4310957810",
        "https://openalex.org/W2101220662",
        "https://openalex.org/W4322494707",
        "https://openalex.org/W4309609816",
        "https://openalex.org/W2950954328",
        "https://openalex.org/W2972411752",
        "https://openalex.org/W4375858802",
        "https://openalex.org/W3211795435",
        "https://openalex.org/W2108588173",
        "https://openalex.org/W4378620045",
        "https://openalex.org/W4392235646",
        "https://openalex.org/W4387969474"
    ],
    "abstract": "Accurately detecting distant evolutionary relationships between proteins remains an ongoing challenge in bioinformatics. Search methods based on primary sequence struggle to accurately detect homology between sequences with less than 20% amino acid identity. Profile- and structure-based strategies extend sensitive search capabilities into this twilight zone of sequence similarity but require slow pre-processing steps. Recently, whole-protein and positional embeddings from deep neural networks have shown promise for providing sensitive sequence comparison and annotation at long evolutionary distances. Embeddings are generally faster to compute than profiles and predicted structures but still suffer several drawbacks related to the ability of whole-protein embeddings to discriminate domain-level homology, and the database size and search speed of methods using positional embeddings. In this work, we show that low-dimensionality positional embeddings can be used directly in speed-optimized local search algorithms. As a proof of concept, we use the ESM2 3B model to convert primary sequences directly into the 3D interaction (3Di) alphabet or amino acid profiles and use these embeddings as input to the highly optimized Foldseek, HMMER3, and HH-suite search algorithms. Our results suggest that positional embeddings as small as a single byte can provide sufficient information for dramatically improved sensitivity over amino acid sequence searches without sacrificing search speed.",
    "full_text": "Sean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 1 of 17\nComputational and Systems Biology\nSensitive remote homology search by\nlocal alignment of small positional\nembeddings from protein language\nmodels\nSean R. Johnson, Meghana Peshwa, Zhiyi Sun\nNew England Biolabs Inc., 240 County Road, Ipswich, MA 01938, United States\nhttps://en.wikipedia.org/wiki/Open_access\nCopyright information\nAbstract\nAccurately detecting distant evolutionary relationships between proteins remains an ongoing\nchallenge in bioinformatics. Search methods based on primary sequence struggle to\naccurately detect homology between sequences with less than 20% amino acid identity.\nProfile- and structure-based strategies extend sensitive search capabilities into this twilight\nzone of sequence similarity but require slow pre-processing steps. Recently, whole-protein\nand positional embeddings from deep neural networks have shown promise for providing\nsensitive sequence comparison and annotation at long evolutionary distances. Embeddings\nare generally faster to compute than profiles and predicted structures but still suffer several\ndrawbacks related to the ability of whole-protein embeddings to discriminate domain-level\nhomology, and the database size and search speed of methods using positional embeddings.\nIn this work, we show that low-dimensionality positional embeddings can be used directly in\nspeed-optimized local search algorithms. As a proof of concept, we use the ESM2 3B model to\nconvert primary sequences directly into the 3Di alphabet or amino acid profiles and use\nthese embeddings as input to the highly optimized Foldseek, HMMER3, and HH-suite search\nalgorithms. Our results suggest that positional embeddings as small as a single byte can\nprovide sufficient information for dramatically improved sensitivity over amino acid\nsequence searches without sacrificing search speed.\neLife assessment\nThis work presents the application of protein language models in combination with\ncurrent methods for the detection of distant evolutionary relationships. While the\nresults are important, they are supported by incomplete, preliminary evidence. A\nmore comprehensive benchmark and clarification of technical details may turn this\nexploratory study into an algorithm ready for wide use.\nReviewed Preprint\nPublished from the original\npreprint after peer review\nand assessment by eLife.\nAbout eLife's process\nReviewed preprint posted\nOctober 27, 2023 (this version)\nSent for peer review\nAugust 8, 2023\nPosted to bioRxiv\nJuly 29, 2023\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 2 of 17\nIntroduction\nA common method for assigning a putative function to a protein sequence is to find sequences\nwith experimentally determined functions that have similarities in sequence, structure, or\nevolutionary origin to the unannotated sequence (Loewenstein et al., 2009     ). Direct comparisons\nof primary sequence, for example using BLASTP (Camacho et al., 2009     ), are fast and reliable but\nshow poor ability to detect homologs with less than about 20% identity to the query (Rost, 1999     ).\nPopular approaches for higher sensitivity sequence searches involve using sequence profiles, for\nexample with PSI-BLAST (Altschul et al., 1997     ), HMMER3 (Eddy, 2011     ), HH-suite3 (Steinegger\net al., 2019     ), or MMseqs2 (Steinegger and Söding, 2017     ). Sequence profiles are derived from\nmultiple sequence alignments (MSAs) and are often modeled as profile hidden Markov models\n(HMMs), for example in HMMER and HH-suite. Profile HMMs model each position as the\nprobability of each amino acid at the position together with insertion and deletion probabilities.\nBecause of their reliance on the construction of MSAs, profile-based methods can have high\ncomputational overhead for database construction, query preparation, or both.\nProtein structure searches also show higher sensitivity than sequence searches (Jambrich et al.,\n2023     ). Until recently, the utility of structure searches for protein annotation was limited by the\nlack of extensive reference databases and the inability to predict structures quickly and reliably\nfor sequences lacking experimentally determined structures. In the past several years accurate\nprotein structure prediction programs such as AlphaFold2 (Jumper et al., 2021     ) and ESMFold\n(Lin et al., 2023     ) have led to a massive increase in the size of databases of predicted protein\nstructures. Coupled with fast structure search algorithms such as Foldseek (van Kempen et al.,\n2023     ), RUPEE (Ayoub and Lee, 2019     ), and Dali (Holm, 2022     ), structure prediction programs\nprovide another powerful tool for remote homology detection. Foldseek achieves fast structure\nsearch by encoding the tertiary interactions of each amino acid in the 20-letter 3D interaction (3Di)\nalphabet. By using a structure alphabet of the same size as the amino acid alphabet, Foldseek can\nleverage optimized sequence search algorithms originally developed for amino acid sequences\n(Steinegger and Söding, 2017     ; van Kempen et al., 2023     ). Structure search methods suffer from\nsome of the same drawbacks as profile-based methods, including the computational cost of\nconverting primary sequences to predicted structures.\nEmerging methods for protein annotation and remote homology detection rely on deep neural\nnetworks taking protein sequences as inputs and producing either a classification from a\ncontrolled vocabulary (Bileschi et al., 2022     ; Sanderson et al., 2023     ), a natural language\ndescription (Gane et al., 2022     ), positional embeddings, or a sequence embedding. Positional\nembeddings are fixed length vectors for each amino acid position of the protein. Positional\nembeddings produced by popular protein language models (pLMs) usually have large dimensions\nsuch as 1024 for ProtT5-XL-U50 (Elnaggar et al., 2021     ) and 2560 for ESM-2 3B (Lin et al., 2023     ).\nSequence embeddings represent an entire sequence and are often calculated by element-wise\naveraging of the positional embeddings. Positional and sequence embeddings can be used for\nremote homology detection by using them to calculate substitution matrices in pairwise local\nalignments (Kaminski et al., 2022     ; Pantolini et al., 2022     ; Ye and Iovino, 2023     ), or by k-nearest\nneighbors searches (Hamamsy et al., 2022     ; Schütze et al., 2022     ), respectively.\nWhile each of these emerging methods shows promise for improving sensitivity of protein search\nand annotation, they suffer various limitations. Classification models and methods relying on\nsequence embeddings struggle at discriminating individual domains of multi-domain proteins.\nMethods relying on large positional embeddings are space inefficient, and current search\nimplementations are slow compared to other methods. Smaller positional embeddings would be\nmore amenable to algorithmic optimizations using single instruction multiple data (SIMD)\ncapabilities of central processing units (CPUs) that contribute to the speed of optimized sequence\nsearch algorithms (Buchfink et al., 2021     ; Eddy, 2011     ; Steinegger et al., 2019     ).\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 3 of 17\nWe recognized that profile HMMs and 3Di sequences are types of positional embeddings with\ndimensionality as low as 1 (3Di sequences) to about 25 (profile HMMs, including amino acid\nfrequencies and state transition probabilities). We test the hypothesis that the ESM-2 3B pLM (Lin\net al., 2023     ) could be used to directly convert primary amino acid sequences into profile HMMs\ncompatible with HMMER3 or HH-suite, and 3Di sequences compatible with Foldseek, providing a\nsequence search workflow leveraging the speed and sensitivity advantages of profile and\nstructure search algorithms with an accelerated query preparation step enabled by the pLM.\nResults\nESM-2 was already pretrained on the masked language modeling task (Devlin et al., 2019     ; Lin et\nal., 2023     ) of predicting amino acid distributions at masked positions of input sequences,\ntherefore no additional fine-tuning was necessary to induce it to produce probabilities compatible\nwith profile HMM tools (Figure 1A     ). Positional amino acid frequencies predicted by ESM-2 3B\nresembled those found in MSAs built from sequence searches (Figure 2     ).\nTo produce Foldseek-compatible 3Di sequences, we trained a two-layer 1D convolutional neural\nnetwork (CNN) to convert positional embeddings from the last transformer layer of ESM-2 3B into\n3Di sequences, we then unfroze the last transformer layer and fine-tuned it together with the CNN.\nThe fine-tuned model, which we call ESM-2 3B 3Di (Figure 1B     ), converted amino acids to 3Di\nwith an accuracy of 64% compared to a test set of 3Di sequences derived from AlphaFold2\npredicted structures.\nTo evaluate the capacity of small embeddings to improve search sensitivity, we generated\npredicted profiles and 3Di sequences from clustered Pfam 32 splits (Bileschi et al., 2022     ) and\nconverted them into formats compatible with various search tools (Figure 1C     ). Pfam (Mistry et\nal., 2021     ) is a set of manually curated multiple sequence alignments of families of homologous\nprotein domains. Some families presumed to have a common evolutionary origin are further\ngrouped into clans. In the clustered splits, each family is divided into train and test groups such\nthat each sequence in the test group has less than 25% identity to the most similar protein in the\ntrain group (Bileschi et al., 2022     ). The sensitivity of a search algorithm is evaluated by the ability\nto match sequences from the test groups to their corresponding train group at the family or clan\nlevel.\nMethods using predicted profiles and those using predicted 3Di sequences both showed greater\naccuracy than phmmer (amino acid to amino acid) searches across all identity bins, and hmmscan\n(amino acid to profile HMM) searches on test sequences below 20% identity to the closest train\nsequence (Figure 3     , compare lines 6 and 7 with lines 1 and 2). Foldseek searches where both the\nqueries and database consisted of 3Di sequences produced by ESM-2 3B 3Di (line 7) performed the\nbest overall. Foldseek considers both 3Di and amino acid sequences in its alignments and can\ntherefore be conceptualized as using a 2-byte embedding. Running Foldseek in 3Di-only mode (line\n8), a 1-byte embedding, led to a decrease in accuracy but still outperformed phmmer across all\nidentity bins, and hmmscan on bins below 20% identity. We also tried creating HMMER3 profiles\nfrom predicted 3Di sequences (line 9). These performed worse than single-sequence Foldseek\nsearches. Patching HMMER3 to use 3Di-derived background frequencies and Dirichlet priors (line\n10) did not improve performance. Furthermore, we noticed that HMMER3 runs very slowly on 3Di\nsequences and profiles, presumably because the prefiltering steps were not optimized with 3Di\nsequences in mind.\nWhile faster than MSA construction or full structure prediction, pLM embedding still have non-\ntrivial computational overhead. This limits the possibility of using methods based on pLM\nembeddings to perform sensitive homology searches against large metagenomic databases such as\nthe 2.4 billion sequence Mgnify database (Richardson et al., 2023     ). It would be desirable to have\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 4 of 17\nFigure 1.\nSchematics of embedding models and the experimental design.\n(A) ESM-2 3B can be directly used to predict amino acid probability distributions at masked positions. Our implementation\nuses seven passes. The second pass is shown in the figure. (B) ESM-2 3B 3Di, a fine-tuned ESM-2 3B with a small CNN top\nmodel can be used to predict 3Di sequences from amino acid sequences. (C) Data flow from amino acid sequences through\nembedding models and other programs to produce files used in homology searches. Bold words correspond to line labels in\nFigure 3     .\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 5 of 17\nFigure 2.\nLogos related to the example test sequence YBGC_HELPY__14-90 from the 4HBT family.\n(A) 4HBT family hmm from Pfam 32. (B) hmmbuild with default settings on an MSA of the top 100 hits supplied by an online\nblast search (blast.ncbi.nlm.nih.gov (http://blast.ncbi.nlm.nih.gov/)      ) of YBGC_HELPY__14-90 against the NCBI clustered nr\ndatabase. (C) hmmbuild with default settings on the MSA sampled from the ESM-2 3B positional probabilities for\nYBGC_HELPY__14-90. (D) and (E) hmmbuild with Dirichlet priors disabled on the same MSAs as for (B) and (C), respectively. All\nlogos were generated by uploading the corresponding .hmm file to skylign.org (http://skylign.org/)       (Wheeler et al., 2014     ).\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 6 of 17\nFigure 3.\nHomology detection accuracy.\nTest sequences were binned based on percent identity to the closest training sequence in the same family and annotated\nbased on the top scoring hit from a search against the entire set of training sequences or training sequence family profiles,\ndepending on the algorithm. (A) family recovery accuracy by bin. (B) clan recovery accuracy. (C) and (D) are detail views of\nthe 0.8 to 1.0 accuracy range. There are a total of 21,293 test sequences. 12,246 test sequences have clan assignments.\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 7 of 17\nsearch methods where the database can remain as amino acid sequences or other cheaply-\ncalculated representations and the queries can be processed with more expensive methods. To\nthis end, we tested hmmsearch using ESM2 3B generated profiles as queries against amino acid\ndatabases (lines 3 and 4). This is similar to a two-iteration PSI-BLAST (Altschul et al., 1997     ) or\nJackHMMER (Johnson et al., 2010     ) search where the first search and MSA-building step is\nreplaced by a pLM embedding step. Curiously, hmmsearch using profiles built directly from the\npLM probabilities (line 3) had the lowest accuracy of any algorithm. Nevertheless, profiles\nprocessed with hmmbuild from HMMER3, applying HMMER3 Dirichlet priors on top of the pLM\nprobabilities (line 4), had better family prediction accuracy than phmmer at all but the highest\nidentity bin, and accuracy on par with hmmscan at 18% identity and lower.\nDiscussion\nOur results show that small positional embeddings produced by pLMs can enhance search\nsensitivity, potentially with less computational overhead than MSA construction or full structure\nprediction. While this manuscript was being submitted, another group reported an amino acid to\n3Di translation model, ProstT5, that also showed strong performance on remote homology\ndetection (Heinzinger et al., 2023     ). There are many possible directions for future development of\nimproved embeddings, faster conversion and search programs, and comprehensive reference\ndatabases. Of particular interest could be conversion to profile HMM-style embeddings in a single\npass, rather than the seven we required, and predicting state transition probabilities. Small\npositional embeddings optimal for local alignment could also be learned directly from a\ndifferentiable alignment algorithm (Petti et al., 2023     ) instead of relying on proxy tasks of amino\nacid or 3Di prediction. Finally, asymmetric architectures where embedding a database sequence is\ncheaper than embedding a query, analogous to searches with profile HMM queries against\nprimary sequence databases, could be a powerful method for improving search sensitivity against\nlarge and growing reference databases. We’ve made our model training, search, and data analysis\ncode publicly available. We hope our results and code will serve as a springboard for further\nexploration of the utility of low-dimensionality positional embeddings of protein sequences.\nMethods\nAlignments\nUnless otherwise noted, multiple sequence alignments were made using MAFFT (v7.505) (Katoh\nand Standley, 2013     ) with options --anysymbol --maxiterate 1000 --globalpair. Protein alignments\nused the BLOSUM62 substitution matrix (Henikoff and Henikoff, 1992     ). 3Di alignments used the\n3Di substitution matrix from Foldseek (van Kempen et al., 2023     ) (https://github.com/steineggerlab\n/foldseek/blob/master/data/mat3di.out     ).\nPatching HMMER3 with background frequencies and Dirichlet\npriors for 3Di\nWe created a fork of the HMMER3 program, replacing amino acid background frequencies and\nDirichlet priors with values calculated from the 3Di alphabet instead of the amino acid alphabet\n(https://github.com/seanrjohnson/hmmer3di     ). To generate a set of 3Di MSAs, we converted the\nAlphaFold UniProt Foldseek database (Jumper et al., 2021     ; van Kempen et al., 2023     ; Varadi et\nal., 2022     ) to a 3Di fasta file. We then looked up every sequence name from the Pfam 35 seed file\nin the UniProt 3Di fasta file and, for cases where the corresponding sequence was identifiable,\nextracted the sub-sequence corresponding to the Pfam 35 seed. 3Di seeds from each profile were\naligned using MAFFT. MSA columns with more than 10 rows were used to calculate background\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 8 of 17\nfrequencies and Dirichlet priors using the HMMER3 program esl-mixdchlet fit with options -s 17 9\n20. Amino acid background frequencies and Dirichlet priors in the HMMER3 source code were\nthen replaced with the newly calculated 3Di background frequencies and Dirichlet priors. We call\nthe patched HMMER3 as HMMER3Di.\nFine tuning ESM-2 3B to convert amino acid sequences into 3Di\nsequences\nA 1D CNN was added on top of ESM-2 3B. The CNN takes as input position-wise embeddings from\nthe last transformer layer of ESM-2 3B. The CNN consists of two layers, the first layer has 2560\ninput channels (the size of the embeddings from ESM-2 3B), and 300 output channels, kernel size 5,\nstride 1, padding 3. The second layer has 300 input channels and 21 output channels (one for each\n3Di symbol plus a padding symbol), kernel size 5, stride 1, padding 1. The model was trained with\na weighted cross-entropy loss function using weights of 0.1 * the diagonal from 3Di substitution\nmatrix. The neural network was implemented in PyTorch (Paszke et al., 2019     ).\nTraining data was derived from the Foldseek AlphaFold2 UniProt50 dataset (Jumper et al., 2021     ;\nvan Kempen et al., 2023     ; Varadi et al., 2022     ), a reduced-redundancy subset of the UniProt\nAlphaFold2 structures. The Foldseek database was downloaded using the Foldseek “databases”\ncommand line program, converted into protein and 3Di fasta files, filtered to remove sequences\nsmaller than 120 amino acids and larger than 1000 amino acids, and split into train, validation,\nand test subsets, 90%:5%:5%, (33,924,764: 1,884,709: 1,884,710 sequences)\nWith ESM-2 layers frozen, the CNN was trained on the task of converting amino acids to 3Di\nsequences using the AdamW optimizer with learning rate 0.001, weight decay 0.001, and\nexponential learning rate decrease (gamma 0.98, applied every 100 batches). Training sequences\nwere randomly selected in batches of 15 sequences. Training proceeded for 1301 batches, leading\nto a training accuracy of about 58%.\nThe last transformer layer of ESM-2 was then unfrozen and training restarted from the saved\nweights, with the same training parameters. Training continued for another 24001 batches of 10\nrandom training sequences. Accuracy on the final training batch was 65%. Using the final trained\nweights, test sequences were converted to 3Di at an accuracy of 64.4%. We call the fine-tuned\nmodel ESM-2 3B 3Di.\nThe trained weights are available on Zenodo (https://doi.org/10.5281/zenodo.8174959     ). The\ntraining code is available on Github (https://github.com/seanrjohnson/esmologs     ). The model\ntraining code should be useful for fine-tuning ESM-2 to convert amino acid sequences to various\nother kinds of sequences, such as secondary structure codes, etc.\nPfam 32 clustered splits\nPfam 32 clustered splits (Bileschi et al., 2022     ) were downloaded from: https://console.cloud.google\n.com/storage/browser/brain-genomics-public/research/proteins/pfam/clustered_split     . Data for\nmapping of individual Pfam 32 families to clans was downloaded from: https://ftp.ebi.ac.uk/pub\n/databases/Pfam/releases/Pfam32.0/     . The sequences from the train, dev, and test splits were\nsorted into unaligned fasta files according to their split and family (aa seq).\nPredicting 3Di sequences\nEach training and test sequence was converted into a predicted 3Di sequence (pred 3Di) using the\nESM-2 3B 3Di model described above. MAFFT alignments were made of both the amino acid and\npredicted 3Di training and test sequences for each family. HMMER3 profiles were built from the\nalignments using either unaltered HMMER3 (pred 3Di hmmer profile), or HMMER3Di (pred 3Di\nhmmer3Di profile).\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 9 of 17\nPredicting profiles to generate HH-suite hhm files and HMMER3\nhmm files from single sequences\nPositional amino acid probabilities were calculated for unaligned train and test sequence using\npre-trained ESM-2 3B in the following algorithm.\n1. Prepend sequence with M. This is because ESM-2 3B has a strong bias towards predicting M as the\nfirst amino acid in every sequence, and most Pfam domains don’t start with M.\n2. Mask the first, eighth, etc. position in the sequence\n3. Run a forward pass of ESM-2 3B over the masked sequence.\n4. Save the logits for each of the 20 amino acid tokens for each masked position.\n5. Shift the masks one position to the right.\n6. Repeat steps 3 through 5 another 6 times until logits have been saved for every position.\n7. Use the softmax function to calculate the amino acid probabilities at each position from the logits.\nNote that in our actual implementation, a single forward pass was run on a batch of seven copies\nof the input sequence, each with different masking.\nThe positional probabilities were written directly as an HH-suite compatible .hhm file (pred\nhhsuite profile). A 40 sequence fasta MSA file was written where each sequence was randomly\nsampled from the probability distribution. Hmmbuild was run with default settings on the\nsampled MSA (pred hmmer profile dchlet) and with the -pnone setting, which disables\nadjustments to the probability distribution based on Dirichlet priors (pred hmmer profile).\nBuilding HH-suite hhm and HMMER3 hmm profiles from train\namino acid MSAs\nThe amino acid MSA for each training family were converted into an HH-suite database (aa\nhhsuite profile) with the following bash script:\necho ‘#’$profile_name > msa/${profile_name}.a3m hhfilter -i $MSA_FASTA -a\nmsa/${profile_name}.a3m -M 50 -id 90; hhconsensus -i msa/${profile_name}.a3m -o\nconsensus/${profile_name}.a3m hhmake -name $base -i consensus/${profile_name}.a3m -o\nhhm/${base}.hhm ffindex_build -s db_hhm.ffdata db_hhm.ffindex hhm ffindex_build -s\ndb_a3m.ffdata db_a3m.ffindex consensus cstranslate -f -x 0.3 -c 4 -I a3m -i db_a3m -o db_cs219\nHMMER3 profiles were built by calling hmmbuild with default settings on the training family\namino acid MSAs (aa hmmer profile).\nBuilding HH-suite databases from predicted profiles\nHH-suite databases were built from predicted profiles .hhm files and the corresponding sampled\n40 sequence MSA fasta files using the following bash script:\nffindex_build -s db_hhm.ffdata db_hhm.ffindex esm2_3B_profiles ffindex_build -s db_a3m.ffdata\ndb_a3m.ffindex esm2_3B_sampled_msas cstranslate -f -x 0.3 -c 4 -I a3m -i db_a3m -o db_cs219\nBuilding Foldseek databases from predicted 3Di sequences\nAmino acid and predicted 3Di fasta files were converted into Foldseek-compatible databases using\na new script, fasta2foldseek.py, available from the esmologs python package (see below).\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 10 of 17\nHmmscan, phmmer, and hmmsearch HMMER3 searches\nIn an attempt to mimic the Top pick HMM strategy reported by (Bileschi et al., 2022     ) we ran all\nHMMER3 searches in up to two iterations. The first iteration was run with default settings. For test\nsequences where no hits were detected among the training sequences or profiles, depending on\nthe program, a second iteration was run with the addition of parameters intended to maximize\nsensitivity at the expense of search speed:\n--max -Z 1 --domZ 1 -E 1000000 --domE 1000000\nIt should be noted that while our phmmer results are directly comparable to the phmmer results\nreported by Bileschi et al., our hmmscan results are not directly comparable to the reported “Top\nPick HMM” results because we re-aligned the training sequences for each family instead of using\nthe Pfam seed alignments. Still our results were very similar. We observed a 17.6% error rate\n(3744 test sequences with mispredicted family assignments) by hmmscan, compared to the\nreported 18.1% error rate (3844 mispredictions).\n3Di_hmmscan HMMER3Di searches\n3Di_hmmscan searches were performed using the same two iteration method described above for\nsearches using standard HMMER3 programs.\nHhblits HH-suite searches\nHhbilts was run with the options:\n-tags -n 1 -v 0\nFoldseek searches\nAfter converting both query and target amino acid and predicted 3Di fasta files into Foldseek\ncompatible databases (see above), Foldseek searches were run with the following commands:\nfoldseek search test_db train_db foldseek_results tmpFolder foldseek convertalis test_db train_db\naln_3Di_only ../hits.tsv --format- output query,target,bits\nFor 3Di-only searches, the option --alignment-type 0 was added to the search call.\nData and Code availability\nHMMER3 patched with 3Di background frequencies and Dirichlet priors: https://github.com\n/seanrjohnson/hmmer3di     \nCode for neural network training, sequence searches, and data analysis: https://github.com\n/seanrjohnson/esmologs     \nModel weights for ESM-2 3B 3Di, predicted profiles and 3Di sequences from the Pfam 32 clustered\nsplits, and other data necessary to reproduce the analyses: https://doi.org/10.5281/zenodo\n.8174959     \nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 11 of 17\nAcknowledgements\nWe thank Sergey Ovchinnikov for helpful discussion about protein language models, Sean Eddy\nfor helpful discussion about HMMER3, and Gary Smith for IT support.\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 12 of 17\nReferences\nAltschul SF, Madden TL, Schäffer AA, Zhang J, Zhang Z, Miller W, Lipman DJ (1997) Gapped\nBLAST and PSI-BLAST: a new generation of protein database search programs Nucleic Acids\nResearch 25:3389–3402https://doi.org/10.1093/nar/25.17.3389\nAyoub R, Lee Y. (2019) RUPEE: A fast and accurate purely geometric protein structure\nsearch PLOS ONE 14 https://doi.org/10.1371/journal.pone.0213712\nBileschi ML, Belanger D, Bryant DH, Sanderson T, Carter B, Sculley D, Bateman A, DePristo MA,\nColwell LJ (2022) Using deep learning to annotate the protein universe Nat Biotechnol 1–6\nhttps://doi.org/10.1038/s41587-021-01179-w\nBuchfink B, Reuter K, Drost H-G. (2021) Sensitive protein alignments at tree-of-life scale\nusing DIAMOND Nat Methods 18:366–368https://doi.org/10.1038/s41592-021-01101-x\nCamacho C, Coulouris G, Avagyan V, Ma N, Papadopoulos J, Bealer K, Madden TL (2009)\nBLAST+: architecture and applications BMC Bioinformatics 10 https://doi.org/10.1186/1471\n-2105-10-421\nDevlin J, Chang M-W, Lee K, Toutanova K. (2019) BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding\nEddy SR (2011) Accelerated Profile HMM Searches PLOS Computational Biology 7 https://doi\n.org/10.1371/journal.pcbi.1002195\nElnaggar A, Heinzinger M, Dallago C, Rehawi G, Yu W, Jones L, Gibbs T, Feher T, Angerer C,\nSteinegger M, Bhowmik D, Rost B. (2021) ProtTrans: Towards Cracking the Language of\nLifes Code Through Self-Supervised Deep Learning and High Performance Computing IEEE\nTransactions on Pattern Analysis and Machine Intelligence 1–1 https://doi.org/10.1109/TPAMI\n.2021.3095381\nGane A, Bileschi ML, Dohan D, Speretta E, Héliou A, Meng-Papaxanthos L, Zellner H, Brevdo E,\nParikh A, Orchard S. (2022) Gane A, Bileschi ML, Dohan D, Speretta E, Héliou A, Meng-\nPapaxanthos L, Zellner H, Brevdo E, Parikh A, Orchard S. 2022. ProtNLM: Model-based\nNatural Language Protein Annotation. https://www.uniprot.org/help/ProtNLM\nHamamsy T, Morton JT, Berenberg D, Carriero N, Gligorijevic V, Blackwell R, Strauss CEM,\nLeman JK, Cho K, Bonneau R. (2022) Hamamsy T, Morton JT, Berenberg D, Carriero N,\nGligorijevic V, Blackwell R, Strauss CEM, Leman JK, Cho K, Bonneau R. 2022. TM-Vec:\ntemplate modeling vectors for fast homology detection and alignment.\ndoi:10.1101/2022.07.25.501437 https://doi.org/10.1101/2022.07.25.501437\nHeinzinger M, Weissenow K, Sanchez JG, Henkel A, Steinegger M, Rost B. (2023) Heinzinger M,\nWeissenow K, Sanchez JG, Henkel A, Steinegger M, Rost B. 2023. ProstT5: Bilingual\nLanguage Model for Protein Sequence and Structure. doi:10.1101/2023.07.23.550085\nhttps://doi.org/10.1101/2023.07.23.550085\nHenikoff S, Henikoff JG (1992) Amino acid substitution matrices from protein blocks Proc\nNatl Acad Sci U S A 89:10915–10919\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 13 of 17\nHolm L. (2022) Dali server: structural unification of protein families Nucleic Acids Research\ngkac387 https://doi.org/10.1093/nar/gkac387\nJambrich MA, Tusnady GE, Dobson L. (2023) Jambrich MA, Tusnady GE, Dobson L. 2023. How\nAlphaFold shaped the structural coverage of the human transmembrane proteome.\ndoi:10.1101/2023.04.18.537193 https://doi.org/10.1101/2023.04.18.537193\nJohnson LS, Eddy SR, Portugaly E. (2010) Hidden Markov model speed heuristic and iterative\nHMM search procedure BMC Bioinformatics 11 https://doi.org/10.1186/1471-2105-11-431\nJumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R,\nŽídek A, Potapenko A, Bridgland A, Meyer C, Kohl SAA, Ballard AJ, Cowie A, Romera-Paredes B,\nNikolov S, Jain R, Adler J, Back T, Petersen S, Reiman D, Clancy E, Zielinski M, Steinegger M,\nPacholska M, Berghammer T, Bodenstein S, Silver D, Vinyals O, Senior AW, Kavukcuoglu K, Kohli\nP, Hassabis D. (2021) Highly accurate protein structure prediction with AlphaFold Nature 1–\n11 https://doi.org/10.1038/s41586-021-03819-2\nKaminski K, Ludwiczak J, Alva V, Dunin-Horkawicz S. (2022) Kaminski K, Ludwiczak J, Alva V,\nDunin-Horkawicz S. 2022. pLM-BLAST –distant homology detection based on direct\ncomparison of sequence representations from protein language models.\ndoi:10.1101/2022.11.24.517862 https://doi.org/10.1101/2022.11.24.517862\nKatoh K, Standley DM (2013) MAFFT Multiple Sequence Alignment Software Version 7:\nImprovements in Performance and Usability Molecular Biology and Evolution 30:772–\n780https://doi.org/10.1093/molbev/mst010\nLin Z, Akin H, Rao R, Hie B, Zhu Z, Lu W, Smetanin N, Verkuil R, Kabeli O, Shmueli Y, dos Santos\nCosta A, Fazel-Zarandi M, Sercu T, Candido S, Rives A. (2023) Evolutionary-scale prediction of\natomic-level protein structure with a language model Science 379:1123–1130https://doi\n.org/10.1126/science.ade2574\nLoewenstein Y, Raimondo D, Redfern OC, Watson J, Frishman D, Linial M, Orengo C, Thornton J,\nTramontano A. (2009) Protein function annotation by homology-based inference Genome\nBiology 10 https://doi.org/10.1186/gb-2009-10-2-207\nMistry J, Chuguransky S, Williams L, Qureshi M, Salazar GA, Sonnhammer ELL, Tosatto SCE,\nPaladin L, Raj S, Richardson LJ, Finn RD, Bateman A. (2021) Pfam: The protein families\ndatabase in 2021 Nucleic Acids Research 49:D412–D419https://doi.org/10.1093/nar/gkaa913\nPantolini L, Studer G, Pereira J, Durairaj J, Schwede T. (2022) Pantolini L, Studer G, Pereira J,\nDurairaj J, Schwede T. 2022. Embedding-based alignment: combining protein language\nmodels and alignment approaches to detect structural similarities in the twilight-zone.\ndoi:10.1101/2022.12.13.520313 https://doi.org/10.1101/2022.12.13.520313\nPaszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga\nL, Desmaison A, Köpf A, Yang E, DeVito Z, Raison M, Tejani A, Chilamkurthy S, Steiner B, Fang L,\nBai J, Chintala S. (2019) Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T,\nLin Z, Gimelshein N, Antiga L, Desmaison A, Köpf A, Yang E, DeVito Z, Raison M, Tejani A,\nChilamkurthy S, Steiner B, Fang L, Bai J, Chintala S. 2019. PyTorch: An Imperative Style,\nHigh-Performance Deep Learning Library. doi:10.48550/arXiv.1912.01703 https://doi.org\n/10.48550/arXiv.1912.01703\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 14 of 17\nPetti S, Bhattacharya N, Rao R, Dauparas J, Thomas N, Zhou J, Rush AM, Koo P, Ovchinnikov S.\n(2023) End-to-end learning of multiple sequence alignments with differentiable Smith–\nWaterman Bioinformatics 39 https://doi.org/10.1093/bioinformatics/btac724\nRichardson L, Allen B, Baldi G, Beracochea M, Bileschi ML, Burdett T, Burgin J, Caballero-Pérez J,\nCochrane G, Colwell LJ, Curtis T, Escobar-Zepeda A, Gurbich TA, Kale V, Korobeynikov A, Raj S,\nRogers AB, Sakharova E, Sanchez S, Wilkinson DJ, Finn RD (2023) MGnify: the microbiome\nsequence data analysis resource in 2023 Nucleic Acids Research 51:D753–D759https://doi.org\n/10.1093/nar/gkac1080\nRost B. (1999) Twilight zone of protein sequence alignments Protein Engineering, Design and\nSelection 12:85–94https://doi.org/10.1093/protein/12.2.85\nSanderson T, Bileschi ML, Belanger D, Colwell LJ (2023) ProteInfer, deep neural networks for\nprotein functional inference eLife 12 https://doi.org/10.7554/eLife.80942\nSchütze K, Heinzinger M, Steinegger M, Rost B. (2022) Nearest neighbor search on\nembeddings rapidly identifies distant protein relations Frontiers in Bioinformatics 2\nSteinegger M, Meier M, Mirdita M, Vöhringer H, Haunsberger SJ, Söding J. (2019) HH-suite3 for\nfast remote homology detection and deep protein annotation BMC Bioinformatics 20 https:\n//doi.org/10.1186/s12859-019-3019-7\nSteinegger M, Söding J. (2017) MMseqs2 enables sensitive protein sequence searching for\nthe analysis of massive data sets Nat Biotechnol 35:1026–1028https://doi.org/10.1038/nbt\n.3988\nvan Kempen M, Kim SS, Tumescheit C, Mirdita M, Lee J, Gilchrist CLM, Söding J, Steinegger M.\n(2023) Fast and accurate protein structure search with Foldseek Nat Biotechnol 1–4 https:\n//doi.org/10.1038/s41587-023-01773-0\nVaradi M, Anyango S, Deshpande M, Nair S, Natassia C, Yordanova G, Yuan D, Stroe O, Wood G,\nLaydon A, Žídek A, Green T, Tunyasuvunakool K, Petersen S, Jumper J, Clancy E, Green R, Vora A,\nLutfi M, Figurnov M, Cowie A, Hobbs N, Kohli P, Kleywegt G, Birney E, Hassabis D, Velankar S.\n(2022) AlphaFold Protein Structure Database: massively expanding the structural\ncoverage of protein-sequence space with high-accuracy models Nucleic Acids Research\n50:D439–D444https://doi.org/10.1093/nar/gkab1061\nWheeler TJ, Clements J, Finn RD (2014) Skylign: a tool for creating informative, interactive\nlogos representing sequence alignments and profile hidden Markov models BMC\nBioinformatics 15 https://doi.org/10.1186/1471-2105-15-7\nYe Y, Iovino BG (2023) Ye Y, Iovino BG. 2023. Protein Embedding based Alignment\n(preprint). Preprints. doi:10.22541/au.168534397.72964200/v1 https://doi.org/10.22541/au\n.168534397.72964200/v1\nArticle and author information\nSean R. Johnson\nNew England Biolabs Inc., 240 County Road, Ipswich, MA 01938, United States\nFor correspondence: sjohnson@neb.com\nORCID iD: 0000-0001-8261-9015\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 15 of 17\nMeghana Peshwa\nNew England Biolabs Inc., 240 County Road, Ipswich, MA 01938, United States\nZhiyi Sun\nNew England Biolabs Inc., 240 County Road, Ipswich, MA 01938, United States\nFor correspondence: sunz@neb.com\nORCID iD: 0000-0002-6106-5356\nCopyright\n© 2023, Johnson et al.\nThis article is distributed under the terms of the Creative Commons Attribution License\n(https://creativecommons.org/licenses/by/4.0/) , which permits unrestricted use and\nredistribution provided that the original author and source are credited.\nEditors\nReviewing Editor\nIgnacio Sanchez\nCONICET / Universidad de Buenos Aires, Argentina\nSenior Editor\nChristian Landry\nUniversité Laval, Canada\nReviewer #1 (Public Review):\nSummary:\nThis work describes a new method for sequence-based remote homology detection. Such\nmethods are essential for the annotation of uncharacterized proteins and for studies of\nprotein evolution.\nStrengths:\nThe main strength and novelty of the proposed approach lies in the idea of combining state-\nof-the-art sequence-based (HHpred and HMMER) and structure-based (Foldseek) homology\ndetection methods with recent developments in the field of protein language models (the\nESM2 model was used). The authors show that features extracted from high-dimensional,\ninformation-rich ESM2 sequence embeddings can be suitable for efficient use with the\naforementioned tools.\nThe reduced features take the form of amino acid occurrence probability matrices estimated\nfrom ESM2 masked-token predictions, or structural descriptors predicted by a modified\nvariant of the ESM2 model. However, we believe that these should not be called\n\"embeddings\" or \"representations\". This is because they don't come directly from any layer of\nthese networks, but rather from their final predictions.\nThe benchmarks presented suggest that the approach improves sensitivity even at very low\nsequence identities <20%. The method is also expected to be faster because it does not require\nthe computation of multiple sequence alignments (MSAs) for profile calculation or structure\nprediction.\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 16 of 17\nWeaknesses:\nThe benchmarking of the method is very limited and lacks comparison with other methods.\nWithout additional benchmarks, it is impossible to say whether the proposed approach really\nallows remote homology detection and how much improvement the discussed method brings\nover tools that are currently considered state-of-the-art.\nReviewer #2 (Public Review):\nSummary:\nThe authors present a number of exploratory applications of current protein representations\nfor remote homology search. They first fine-tune a language model to predict structural\nalphabets from sequence and demonstrate using these predicted structural alphabets for fast\nremote homology search both on their own and by building HMM profiles from them. They\nalso demonstrate the use of residue-level language model amino acid predicted probabilities\nto build HMM profiles. These three implementations are compared to traditional profile-\nbased remote homology search.\nStrengths:\n- Predicting structural alphabets from a sequence is novel and valuable, with another\napproach (ProstT5) also released in the same time frame further demonstrating its\napplication for the remote homology search task.\n- Using these new representations in established and battle-tested workflows such as\nMMSeqs, HMMER, and HHBlits is a great way to allow researchers to have access to the state-\nof-the-art methods for their task.\n- Given the exponential growth of data in a number of protein resources, approaches that\nallow for the preparation of searchable datasets and enable fast search is of high relevance.\nWeaknesses:\n- The authors fine-tuned ESM-2 3B to predict 3Di sequences and presented the fine-tuned\nmodel ESM-2 3B 3Di with a claimed accuracy of 64% compared to a test set of 3Di sequences\nderived from AlphaFold2 predicted structures. However, the description of this test set is\nmissing, and I would expect repeating some of the benchmarking efforts described in the\nFoldseek manuscript as this accuracy value is hard to interpret on its own.\n- Given the availability of predicted structure data in AFDB, I would expect to see a\ncomparison between the searches of predicted 3Di sequences and the \"true\" 3Di sequences\nderived from these predicted structures. This comparison would substantiate the innovation\nclaimed in the manuscript, demonstrating the potential of conducting new searches solely\nbased on sequence data on a structural database.\n- The profile HMMs built from predicted 3Di appear to perform sub-optimally, and those from\nthe ESM-2 3B predicted probabilities also don't seem to improve traditional HMM results\nsignificantly. The HHBlits results depicted in lines 5 and 6 in the figure are not discussed at\nall, and a comparison with traditional HHBlits is missing. With these results and presentation,\nthe advantages of pLM profile-based searches are not clear, and more justification over\ntraditional methods is needed.\n- Figure 3 and its associated text are hard to follow due to the abundance of colors and\nabbreviations used. One figure attempting to explain multiple distinct points adds to the\nconfusion. Suggestion: Splitting the figure into two panels comparing (A) Foldseek-derived\nsearches (lines 7-10) and (B) language-model derived searches (line 3-6) to traditional\nmethods could enhance clarity. Different scatter markers could also help follow the plots\nmore easily.\n- The justification for using Foldseek without amino acids (3Di-only mode) is not clear. Its\nSean R. Johnson et al., 2023 eLife. https://doi.org/10.7554/eLife.91415.1 17 of 17\nutility should be described, or it should be omitted for clarity.\n- Figure 2 is not described, unclear what to read from it."
}