{
  "title": "AI for Explaining Decisions in Multi-Agent Environments",
  "url": "https://openalex.org/W2979667219",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2122497617",
      "name": "Sarit Kraus",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A1988279427",
      "name": "Amos Azaria",
      "affiliations": [
        "Ariel University"
      ]
    },
    {
      "id": "https://openalex.org/A1828380866",
      "name": "Jelena Fiosina",
      "affiliations": [
        "Clausthal University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2965015078",
      "name": "Maike Greve",
      "affiliations": [
        "University of Göttingen"
      ]
    },
    {
      "id": "https://openalex.org/A1936444166",
      "name": "Noam Hazon",
      "affiliations": [
        "Ariel University"
      ]
    },
    {
      "id": "https://openalex.org/A4266690709",
      "name": "Lutz Kolbe",
      "affiliations": [
        "University of Göttingen"
      ]
    },
    {
      "id": "https://openalex.org/A4272693230",
      "name": "Tim-Benjamin Lembcke",
      "affiliations": [
        "University of Göttingen"
      ]
    },
    {
      "id": "https://openalex.org/A2286579964",
      "name": "Jörg P. Müller",
      "affiliations": [
        "Clausthal University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2982616132",
      "name": "Sören Schleibaum",
      "affiliations": [
        "Clausthal University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1848795801",
      "name": "Mark Vollrath",
      "affiliations": [
        "Technische Universität Braunschweig"
      ]
    },
    {
      "id": "https://openalex.org/A2122497617",
      "name": "Sarit Kraus",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": "https://openalex.org/A1988279427",
      "name": "Amos Azaria",
      "affiliations": [
        "Ariel University"
      ]
    },
    {
      "id": "https://openalex.org/A1828380866",
      "name": "Jelena Fiosina",
      "affiliations": [
        "Clausthal University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2965015078",
      "name": "Maike Greve",
      "affiliations": [
        "University of Göttingen"
      ]
    },
    {
      "id": "https://openalex.org/A1936444166",
      "name": "Noam Hazon",
      "affiliations": [
        "Ariel University"
      ]
    },
    {
      "id": "https://openalex.org/A4266690709",
      "name": "Lutz Kolbe",
      "affiliations": [
        "University of Göttingen"
      ]
    },
    {
      "id": "https://openalex.org/A4272693230",
      "name": "Tim-Benjamin Lembcke",
      "affiliations": [
        "University of Göttingen"
      ]
    },
    {
      "id": "https://openalex.org/A2286579964",
      "name": "Jörg P. Müller",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2982616132",
      "name": "Sören Schleibaum",
      "affiliations": [
        "Clausthal University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1848795801",
      "name": "Mark Vollrath",
      "affiliations": [
        "Technische Universität Braunschweig"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2613034152",
    "https://openalex.org/W6660235197",
    "https://openalex.org/W6677294922",
    "https://openalex.org/W2899373925",
    "https://openalex.org/W2113362740",
    "https://openalex.org/W2904302354",
    "https://openalex.org/W2902255491",
    "https://openalex.org/W6780258851",
    "https://openalex.org/W2134584261",
    "https://openalex.org/W2810992711",
    "https://openalex.org/W2959587146",
    "https://openalex.org/W2907079298",
    "https://openalex.org/W6750287713",
    "https://openalex.org/W2670253439",
    "https://openalex.org/W2772984056",
    "https://openalex.org/W2964842631",
    "https://openalex.org/W2895067291",
    "https://openalex.org/W2928977192",
    "https://openalex.org/W2904176222",
    "https://openalex.org/W2559772828",
    "https://openalex.org/W2573814574",
    "https://openalex.org/W2904396358",
    "https://openalex.org/W6760614652",
    "https://openalex.org/W2575421460",
    "https://openalex.org/W2530010084",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W2928141994"
  ],
  "abstract": "Explanation is necessary for humans to understand and accept decisions made by an AI system when the system's goal is known. It is even more important when the AI system makes decisions in multi-agent environments where the human does not know the systems' goals since they may depend on other agents' preferences. In such situations, explanations should aim to increase user satisfaction, taking into account the system's decision, the user's and the other agents' preferences, the environment settings and properties such as fairness, envy and privacy. Generating explanations that will increase user satisfaction is very challenging; to this end, we propose a new research direction: Explainable decisions in Multi-Agent Environments (xMASE). We then review the state of the art and discuss research directions towards efficient methodologies and algorithms for generating explanations that will increase users' satisfaction from AI systems' decisions in multi-agent environments.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nAI for Explaining Decisions in Multi-Agent Environments∗\nSarit Kraus,1 Amos Azaria,2 Jelena Fiosina,3 Maike Greve,4 Noam Hazon,2\nLutz Kolbe,4 Tim-Benjamin Lembcke,4 J¨o r gP .M¨uller ,3 S¨oren Schleibaum,3 Mark V ollrath5\n1Department of Computer Science, Bar-Ilan University, Israel sarit@cs.biu.ac.il\n2Department of Computer Science, Ariel University, Israel\n3Department of Informatics, TU Clausthal, Germany\n4Chair of Information Management, Georg-August-Universitat G¨ottingen, Germany\n5Chair of Engineering and Trafﬁc Psychology, TU Braunschweig, Germany\nAbstract\nExplanation is necessary for humans to understand and accept\ndecisions made by an AI system when the system’s goal is\nknown. It is even more important when the AI system makes\ndecisions in multi-agent environments where the human does\nnot know the systems’ goals since they may depend on other\nagents’ preferences. In such situations, explanations should\naim to increase user satisfaction, taking into account the sys-\ntem’s decision, the user’s and the other agents’ preferences,\nthe environment settings and properties such as fairness, envy\nand privacy. Generating explanations that will increase user\nsatisfaction is very challenging; to this end, we propose a new\nresearch direction: Explainable decisions in Multi-Agent En-\nvironments (xMASE). We then review the state of the art and\ndiscuss research directions towards efﬁcient methodologies\nand algorithms for generating explanations that will increase\nusers’ satisfaction from AI systems’ decisions in multi-agent\nenvironments.\nIntroduction\nMany AI systems need to make decisions in multi-agent en-\nvironments where the agents, including people and robots,\nhave possibly conﬂicting preferences. The system should\nbalance between these preferences when making decisions\nregarding all agents. Such systems include, e.g., a schedul-\ning algorithm assigning teachers to classes, or a ridesharing\napplication proposing joint rides to people. In such situa-\ntions, the global decisions made by the system may not ad-\nhere to all people’s preferences: a decision may make some\npeople unhappy. Providing explanations about the system’s\ndecision may increase people’s satisfaction (Bradley and\nSparks 2009), and maintain acceptability of the AI system\n(2018). The EU General Data Protection Regulation intro-\nduces a right of explanation (Goodman and Flaxman 2017)\nfor citizens to obtain “meaningful information of the logic\ninvolved” for automated decisions.\nExplainable AI (XAI) has recently been studied exten-\nsively (Core et al. 2006; Carvalho, Pereira, and Cardoso\n2019; Rosenfeld and Richardson 2019), mainly focusing on\n∗This work was partly supported by V olkswagen Foundation.\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nﬁnding ways of explaining to a user a decision made by an\nAI system, e.g., a classiﬁcation choice made by a neural net-\nwork. That is, explanations are usually given for black-box\nalgorithms, which aim at maximizing a well-agreed upon\nfunction (e.g., maximizing accuracy, minimizing loss). Pre-\nvious work has mainly focused on increasing users’ trust\nin the black-box AI system. However, we believe that pro-\nviding the users with explanations is even more important\nin multi-agent environments, when even the maximization\nfunction is not clear to the (human) agents. In such situa-\ntions, explanations should aim to increase user satisfaction,\ntaking into account properties such as fairness, envy and pri-\nvacy. Hence, we propose a new research direction of Ex-\nplainable decisions in Multi-Agent Environments (xMASE).\nFor example, in the ridesharing domain, an AI system\nmay suggest to a customer Bob to share a taxi with Alice\nin a ride that will take 30 minutes. The taxi will ﬁrst drop\nAlice off and it will cost Bob $25. Bob may be upset that\nthe taxi drops Alice off ﬁrst. An explanation could be that\nAlice’s destination is on Bob’s route and will only add 5\nminutes to his trip. The system can also say that Alice will\npay $30, or that dropping Bob off ﬁrst will add 15 minutes\nto Alice’s trip, or that Alice teaches at 8am and will be late\nif Bob is dropped ﬁrst. Bob can also be told that sharing a\ntaxi will save him $10.\nGenerating explanations in multi-agent environments is\neven more challenging than providing explanations in other\nsettings, e.g., for classiﬁcation results produced by deep\nlearning algorithms. In addition to identifying the technical\nreasons that led to the decision, there is a need to convey\nthe preferences of the agents that were involved. It is neces-\nsary to decide what to reveal from other agents’ preferences\nin order to increase the user’s satisfaction, taking the pri-\nvacy of other agents into account, and how these preferences\nled to the ﬁnal decision. It should also refer to issues such\nas fairness. The inﬂuence of the explanation on user satis-\nfaction changes from one user to the next; therefore, per-\nsonalized explanations are beneﬁcial (Lakkaraju et al. 2019;\nBradley and Sparks 2009). Given that the task is very chal-\nlenging, we propose to use AI tools, and in particular ma-\nchine learning to generate the personalized explanations\nthat will maximize user satisfaction, while aiming to con-\n13534\nFigure 1: Scheduling example assigning teachers to classes,\ndepicting Alice and Bob’s preferences (green=preferred,\nyellow=feasible, red=impossible) and assignments (*).\nsider system-level societal welfare aspects. In order to use\nmachine-learning methods for generating explanations in\nour context, it will be necessary to collect data about hu-\nman satisfaction from decision-making when various types\nof explanations are given in different contexts. Furthermore,\nthe evaluation of the proposed methods must also involve\nhumans participating in multi-agent environments.\nMost algorithms that provide explanations on AI sys-\ntems take an engineering approach, which does not involve\nrunning experiments with people. Papenmeier et al. (2019)\nshowed that the presence of such explanations did not in-\ncrease the subjects’ self-reported trust. Miller (2018) argues\nthat explanations have been studied extensively in psychol-\nogy and their ﬁndings should be used when designing expla-\nnations for AI systems. One of his main points is that an ex-\nplanation should be sensitive to context. We fully agree that\nfor both XAI and xMASE context should be taken into con-\nsideration when generating explanations, but in multi-agent\nsystems (MASs) context includes other agents’ preferences,\nand fairness of the decision as an important factor.\nFor example, consider a scheduling algorithm that assigns\nteachers to classes. Suppose Bob and Alice each teach in\na speciﬁc classroom on a speciﬁc day; each needs to teach\nfor 4 hours (but they cannot teach in parallel). Bob prefers\nto teach between 10am and 3pm, but can also teach from\n9am-10am and 3pm-4pm. He cannot teach between 8am\nand 9am (a strong constraint) or after 4pm. Alice prefers to\nteach between 10am-2pm, has a hard constraint not allowing\nher to teach after 2pm, but can teach in the mornings, 8am-\n10am. Suppose the algorithm assigned Bob to 12pm-4pm\nand Alice to 8am-12pm (see Figure 1). Which explanations\nshould the system provide for Alice whose assignment vio-\nlates her soft constraint but not her hard constraints? Which\nexplanations should the system give a teacher for an assign-\nment violating his or her hard constraints? Suppose Alice\nand Bob are friends. Should the system tell Bob that he\nteaches between 3pm and 4pm (violating his soft constraint)\nbecause Alice has a strong constraint at this time? What if\nAlice and Bob are new to the school and do not know one\nanother; should such an explanation be provided? Alterna-\ntively, could a merely graphical description of the relevant\ninformation (as in Figure 1) sufﬁce as an explanation?\nThere are many challenging research questions to be stud-\nied in order to provide users in MASs with explanations that\nwill increase their satisfaction:\nAlgorithms for Explanation Generation: The develop-\nment of efﬁcient algorithms that will generate the\nexplanations, preferably in real time, is very challenging.\nFirst, there is a need to be able to form good explanations.\nThen, there is a need to decide which explanation to\npresent (if at all) and when and how to present it.\nUser modeling for increasing satisfaction:Most impor-\ntant is to be able to model the users. It is, of course,\nnecessary to identify the users’ preferences in order to\nmake good decisions. However, for generating good\nexplanations it is important (beyond obtaining the users’\npreferences) to also model their attitudes toward different\nexplanations. That is, to be able to predict how an\nexplanation will inﬂuence users’ satisfaction.\nInteractive explanations: To address the difﬁculties in\nchoosing the speciﬁc explanation to a speciﬁc user, we\npropose to also study interactive explanations, which are\nprovided to the user through a dialogue between the AI\nsystem and the user (Miller 2018). By asking questions\nand expressing his or her concerns, the user can direct the\nsystem towards generating good explanations. However,\nconducting meaningful dialogues with a user for xMASE\nadds difﬁculties to the process. It has some similarities to\nautomated systems that argue with people, a research area\nthat still has many open questions (Rosenfeld and Kraus\n2016a; 2016b).\nUnderstanding System Decision-making: The decision\nof an MAS depends on many parameters associated\nwith several agents, making the xMASE problem even\nmore challenging than XAI. Only some of the technical\nreasons that led to the decision are relevant to a given\nuser; as the number of agents increases, the non-relevant\ninformation increases, too, and it is hard to identify the\nrelevant parts. Other explanations that could increase user\nsatisfaction concern the environment. If the AI system\nmade a decision based on some knowledge about the\nenvironment, but this knowledge is not available to the\nuser, presenting it to the user could be useful. E.g., if Bob\nbelieves that there are many taxis available at 8am, but\nthe system proposed him to share a taxi with Alice since\nit knows that there are very few taxis available, giving\nthis information to Bob might increase his satisfaction.\nLong-term relationships: When the AI system interacts\nrepeatedly with the same users, interesting research ques-\ntions may arise. The learning phase of the preferences\nand satisfaction models can be personalized, but more im-\nportantly the explanation generated should take long-term\nsatisfaction into consideration.\nEthics and Privacy: These issues must be considered\nwhen presenting explanations in multi-agent systems. As\nin other situations, one needs to regard issues such as the\ntruthfulness of the explanations and the ethical aspect of\nconcealing some of the information. Moreover, privacy is\na major concern in xMASE, since there is also a need to\nconsider the revelation of information and preferences of\none agent to another when providing an explanation.\nOpen source code and public datasets:Open source en-\nvironments that will allow researchers to develop and\nevaluate their explanation algorithms can enhance the re-\nsearch in xMASE. It is also very useful to start collecting\nlabeled datasets for xMASE.\n13535\nWe will discuss three of these directions in more detail after\nsurveying the related work to xMASE.\nState-of-the-art\nIn recent years there has been intense research on de-\nsign techniques for making AI methods explainable, in-\nterpretable, and transparent for developers and users (Car-\nvalho, Pereira, and Cardoso 2019). The basic idea of XAI\nmethods is to try to explain black-box model behaviour\n(while xMASE is needed even in settings where tradi-\ntional white-box optimization is used). Methods for XAI\nhave been developed including locally interpretable model-\nagnostic explanations for Bayesian predictive models (Pel-\ntola 2018) and for convolutional neural networks (Mishra,\nSturm, and Dixon 2017), visualization techniques (Grad-\nCAM) for CNNs (Selvaraju et al. 2017), or black box expla-\nnations through transparent approximations (Lakkaraju et al.\n2017). Hybrid models use explicit symbolic representations\nin conjunction with black-box techniques (Choi, Wang, and\nDarwiche 2019). XAI was also expanded beyond the classi-\ncal domains. For example, Fox et al. (2017) introduces ex-\nplainable planning systems. Ludwig et al. (2018) and ˇCyras\net al. (2019) investigated the explainability of a scheduling\nsystem over tasks. We note that both planning and schedul-\ning of tasks to machines are not multi-agent environments as\nwe deﬁne them, since they actually consist of a single agent,\nand therefore do not belong to xMASE. The explainability\nof deep reinforcement learning was also investigated (Lee\n2019). All of these techniques were developed for XAI, and\nthey can serve as input for xMASE that needs to choose\nthe suitable explanation for any given scenario and to each\nagent. Moreover, in many XAI approaches the evaluation is\ncomplicated due to missing standards (Pedreschi, Giannotti,\nand others 2019), the social nature of explanations is ig-\nnored (Miller 2018), and the explanations are not evaluated\nwith humans. All of these evaluation criteria are essential in\nxMASE.\nIndeed, there are some works which provide evaluations\nof a XAI approach with humans. Lakkaraju et al. (2019) pro-\npose a new form of explanations designed to help end users\n(e.g., decision-makers such as judges, doctors) to gain a\ndeeper understanding of the models’ behaviour. Doshi-Velez\nand Kim (2018) claim that researchers evaluating explain-\nability should differentiate between evaluation of explana-\ntions with humans, experts in the evaluated ﬁelds, and a for-\nmal evaluation without human subjects. Wolf et al. (2019)\npoint out requirements for explanations and state that these\nshould be inﬂuenced by the users, the applications and the\ndeployment context. They claim that three different types of\nexplanations are needed for integrating XAI approaches into\nreal world applications. Firstly, an application should ex-\nplain its behavior; Secondly, the impact of the interaction of\nusers with the applications should be explained, and lastly,\na user seeks explanations describing how the applications’\noutput integrates in the overall process. These ideas are also\nsome of the basic ingredients of a successful xMASE.\nOne unique aspect of xMASE is that the explanations\nshould be chosen such that they will increase the user’s sat-\nisfaction. Previous works have shown that explanations in\ngeneral have an impact on user satisfaction/acceptance. Her-\nlocker et. al. (2000) showed that providing explanations for\nautomated collaborative ﬁltering (ACF) recommendations\ncan improve the acceptance of ACF systems. More relevant\nto us is (Kleinerman, Rosenfeld, and Kraus 2018) show-\ning that explanations are beneﬁcial also in reciprocal rec-\nommendation systems (e.g., dating) where the preferences\nof the other agent are taken into account when making rec-\nommendations, and possibly when generating explanations.\nIn such settings reciprocal explanations are preferable. Put-\nnam and Conati (2019) conducted a user study on beneﬁts\nfrom explanations in intelligent tutoring systems. Their re-\nsults indicate a positive sentiment towards wanting expla-\nnations, but do not suggest any automatic system for their\ngeneration. Levinger et al. (2018) study maximizing human\nsatisfaction in multi-agent systems, but do not use any form\nof explanation. They present an optimization algorithm that\nmaximizes the overall human satisfaction according to the\nlearned model. They also show that, when aiming at maxi-\nmizing human satisfaction, learning accurate models of hu-\nman satisfaction is more important than improving the opti-\nmization algorithm.\nResearch Directions for xMASE\nThe development of AI-based tools that provide the right ex-\nplanations to the right users at the right time to increase user\nsatisfaction in MASs is very challenging. We now discuss\nthree of the above research directions in more detail.\nGeneration of Explanations to Increase Satisfaction\nThe development of efﬁcient algorithms that will generate\nthe explanations, preferably in real time, is very important.\nWe propose a two stage procedure: ﬁrst, a set of possible ex-\nplanations will be created and then the one that best suits\nthe speciﬁc user at the speciﬁc settings will be selected.\nBoth stages can be done using machine learning or any other\ndecision-making procedures based on real user input.\nIf the AI decision is made using neural networks (e.g.,\n(Rosemarin, Rosenfeld, and Kraus 2019; Li, Qin, and oth-\ners 2019)) then XAI methods can be used to identify im-\nportant features that led to the decision (Shrikumar, Green-\nside, and Kundaje 2017; Bach, Binder, and others 2015).\nThese methods should be adapted to xMASE-related prob-\nlems (Lee 2019; Selvaraju et al. 2017). Then, there is a need\nto identify which of these features are relevant to a speciﬁc\nuser. Given these features, the preferences of other agents\nthat are relevant should be identiﬁed and any relevant state-\nments that touch upon important concepts such as fairness\nshould be generated. Using these features, preferences and\nconcepts, several explanations could be generated using sub-\nsets of them. Next, the explanations with the relevant prefer-\nences and environment settings could be entered into a net-\nwork that will estimate the inﬂuence level of each expla-\nnation on the user’s satisfaction. Finally, the chosen subset\nshould be transferred into a textual message and sent to the\nuser. Personalization could also be used in this ﬁnal step.\nIf the AI decision-making is not done using machine\nlearning methods, but rather, e.g., using inferences, and data\n13536\nis not available, it will be interesting to study how methods\ndeveloped for explaining inferences (e.g., (Pino-P ´erez and\nUzc´ategui 2003)) could be used for xMASE.\nIf the AI decision-maker is a scheduling tool, it can pro-\nvide a set of constraints that lead to the proposed schedule\n(Ludwig, Kalton, and Stottler 2018). In xMASE there will\nbe a need to identify the relevant constraints and to general-\nize statements related to other agents’ preferences, and the\ngeneral system constraints that are driven by other concepts\nsuch as fairness. Then again, we can use user satisfaction\nmodels (e.g., represented by a neural network) to choose the\nbest constraints and generalized statements to be presented.\nComplementing the algorithms and methodologies de-\nscribed so far, an interesting research direction is the au-\ntomated generation of graphical explanations for xMASE,\nenabling compact summarization of large amounts of infor-\nmation.\nUser Modeling for Increasing Satisfaction\nThere are many methods for user modeling and prefer-\nence elicitation (Rosenfeld and Kraus 2018; Rosenfeld et al.\n2016; Anselmi et al. 2018), but we have not found a study on\npreference elicitation with respect to explanations and their\nrole in improving satisfaction. One of the main challenges is\nthat user satisfaction from an explanation of a given decision\nstrongly depends on the actual decision, the other agents, the\nenvironment and the user’s beliefs. For example, it is obvi-\nous that, in our ridesharing example, an explanation to Alice\nof the form “This shared ride will save you $10” is a “good”\nexplanation, but of course if in the speciﬁc ride Alice will\nsave time but not money, the explanation is useless. Simi-\nlarly, telling Bob that “There is no private taxi available in\nyour area now” may be a good explanation for a rideshar-\ning suggestion, given that it is true (assuming we provide\nonly true explanations) and Bob does not know it. Thus col-\nlecting data on the inﬂuence of an explanation on the user’s\nsatisfaction must be done in the context of the speciﬁc de-\ncision it explains and the environment setting. This makes\ndata collection very challenging.\nCollection of data can be done either using ﬁctitious deci-\nsions, their explanations and the MAS environment setting\nor, much harder to accomplish, in actual settings or at least\nin simulations. The users can express their preferences on\nhow much they like the explanations. We can use this data\nto build a generalized model of users’ preferences toward ex-\nplanations. However, this model will not provide us with the\nexplanations that increase the user’s satisfaction. Here we\nwill need to let the user express his or her level of satisfac-\ntion from a given decision with different variants of expla-\nnations and without explanations, and try to build a model\nthat measures the users’ satisfaction from the decision.\nOne of the challenges is to identify features of an expla-\nnation. We may consider using the explanation as text, but\nwe believe that there is a need to ﬁnd additional features that\nhave to do with the relationship of the explanation with the\nuser’s preferences and the environment.\nWhen the AI system interacts repeatedly with the same\nusers, interesting research questions may arise. The learn-\ning phase of the preferences and satisfaction models can be\npersonalized, but more importantly the explanation gener-\nated should take long-term satisfaction into consideration.\nFurthermore, we propose to consider, when interacting with\nthe user, using reinforcement learning to improve the user’s\nmodel of overtime in a guided way.\nInteractive Explanations\nHuman verbal explanations are essentially interactive\n(Cawsey 1993). Recently, there have been a few attempts to\nconsider models for interactive explanations to XAI (Mad-\numal et al. 2018) and to value-based agents (Liao, Ander-\nson, and Anderson 2018), but no system was developed. In-\nteractive explanations can be viewed as argumentation dia-\nlogues. It was shown to be beneﬁcial to model the interac-\ntion as POMDP where the uncertainty is about the user’s be-\nliefs (Rosenfeld and Kraus 2016b). Using this approach for\nxMASE there is a need to continuously estimate the user’s\nbeliefs and sentiment toward the AI system’s decision, and\nto predict how a given explanation statement will modify the\nuser’s beliefs and inﬂuence its attitude toward the decision.\nOther techniques for general dialogue systems or for in-\nteractive learning dialogues could be considered (Chen and\nothers 2017). The open questions are how to (1) use the fea-\ntures that led the AI system to make a decision to form possi-\nble responds, and (2) devise user models of preferences and\nsatisfaction to generate the right response in the dialogue.\nRegardless of the techniques, any developed method should\nbe evaluated via human experiments, which are still often\nmissing in XAI.\nConclusions\nIn this paper we presented the xMASE challenge for ex-\nplaining AI-based systems in multi-agent systems environ-\nments aiming at increasing user satisfaction. This challenge\nis extremely important for the success and acceptability of\nsocio-technical applications such as ridesharing where peo-\nple’s preferences may conﬂict, but cooperation is beneﬁcial.\nxMASE can build on top of XAI’s recent progress, but many\nopen questions should be addressed which are related to the\nother agents in the environment and the goal of increasing\nuser’s satisfaction. We propose to develop AI-based tech-\nniques toward addressing these challenges.\nReferences\nAnselmi, P.; Fabbris, L.; Martini, M. C.; and Robusto, E.\n2018. Comparison of four common data collection tech-\nniques to elicit preferences.Quality & Quantity52(3):1227–\n1239.\nBach, S.; Binder, A.; et al. 2015. On pixel-wise explana-\ntions for non-linear classiﬁer decisions by layer-wise rele-\nvance propagation. PLoS ONE10(7).\nBradley, G. L., and Sparks, B. A. 2009. Dealing with ser-\nvice failures: The use of explanations. Journal of Travel &\nTourism Marketing26(2):129–143.\nCarvalho, D. V .; Pereira, E. M.; and Cardoso, J. S. 2019. ML\ninterpretability: A survey on methods and metrics.Electron-\nics 8(8):832.\n13537\nCawsey, A. 1993. Planning interactive explanations. Intern.\nJ. of Man-Machine Studies38(2):169–199.\nChen, H., et al. 2017. A survey on dialogue systems: Recent\nadvances and new frontiers. ACM SIGKDD Explorations\nNewsletter 19(2):25–35.\nChoi, A.; Wang, R.; and Darwiche, A. 2019. On the relative\nexpressiveness of bayesian and neural networks. Interna-\ntional Journal of Approximate Reasoning.\nCore, M. G.; Lane, H. C.; Van Lent, M.; Gomboc, D.;\nSolomon, S.; and Rosenberg, M. 2006. Building explain-\nable artiﬁcial intelligence systems. In AAAI-06.\nˇCyras, K.; Letsios, D.; Misener, R.; and Toni, F. 2019. Ar-\ngumentation for explainable scheduling. In AAAI19, vol-\nume 33, 2752–2759.\nDoshi-Velez, F., and Kim, B. 2018. Considerations for Eval-\nuation and Generalization in Interpretable Machine Learn-\ning. In Explainable and Interpretable Models in Computer\nVision and ML. 3–17.\nFox, M.; Long, D.; and Magazzeni, D. 2017. Explainable\nplanning.\nGoodman, B., and Flaxman, S. 2017. European Union reg-\nulations on algorithmic decision-making and a ”right to ex-\nplanation”. AI Magazine38(3):arXiv:1606.08813.\nHerlocker, J. L.; Konstan, J. A.; and Riedl, J. 2000. Ex-\nplaining collaborative ﬁltering recommendations. In CSCW,\n241–250. ACM.\nKleinerman, A.; Rosenfeld, A.; and Kraus, S. 2018. Pro-\nviding explanations for recommendations in reciprocal en-\nvironments. In ACM Recommender Systems, 22–30. ACM.\nLakkaraju, H.; Kamar, E.; Caruana, R.; and Leskovec, J.\n2017. Interpretable & explorable approximations of black\nbox models. In F AT ML (KDD).\nLakkaraju, H.; Kamar, E.; Caruana, R.; and Leskovec, J.\n2019. Faithful and customizable explanations of black box\nmodels. In AIES.\nLee, J. H. 2019. Complementary reinforcement learning\ntowards explainable agents. CoRR abs/1901.00188.\nLevinger, C.; Azaria, A.; and Hazon, N. 2018. Human sat-\nisfaction as the ultimate goal in ridesharing. arXiv preprint\narXiv:1807.00376.\nLi, M.; Qin, Z.; et al. 2019. Efﬁcient ridesharing order dis-\npatching with mean ﬁeld multi-agent reinforcement learn-\ning. In WWW, 983–994.\nLiao, B.; Anderson, M.; and Anderson, S. L. 2018. Rep-\nresentation, justiﬁcation and explanation in a value driven\nagent: An argumentation-based approach. arXiv preprint\narXiv:1812.05362.\nLudwig, J.; Kalton, A.; and Stottler, R. 2018. Explaining\ncomplex scheduling decisions. In IUI Workshops.\nMadumal, P.; Miller, T.; Vetere, F.; and Sonenberg, L. 2018.\nTowards a grounded dialog model for explainable artiﬁcial\nintelligence. arXiv preprint.\nMiller, T. 2018. Explanation in artiﬁcial intelligence: In-\nsights from the social sciences. AIJ 267:1–38.\nMishra, S.; Sturm, B. L.; and Dixon, S. 2017. Local in-\nterpretable model-agnostic explanations for music content\nanalysis. In ISMIR.\nPapenmeier, A.; Englebienne, G.; and Seifert, C. 2019. How\nmodel accuracy and explanation ﬁdelity inﬂuence user trust.\nIn 3rd Workshop on Explainable AI (X-AI), 94–100.\nPedreschi, D.; Giannotti, F.; et al. 2019. Meaningful expla-\nnations of Black Box AI decision systems. In AAAI, vol-\nume 33, 9780–9784.\nPeltola, T. 2018. Local interpretable model-agnostic expla-\nnations of bayesian predictive models via kullback-leibler\nprojections. In X-AI@IJCAI.\nPino-P\n´erez, R., and Uzc ´ategui, C. 2003. Preferences and\nexplanations. Artiﬁcial Intelligence149(1):1–30.\nPutnam, V ., and Conati, C. 2019. Exploring the need for\nexplainable artiﬁcial intelligence (xai) in intelligent tutoring\nsystems (its). In IUI Workshops.\nRosemarin, H.; Rosenfeld, A.; and Kraus, S. 2019. Emer-\ngency department online patient-caregiver scheduling. In\nAAAI-19.\nRosenfeld, A., and Kraus, S. 2016a. Providing arguments\nin discussions on the basis of the prediction of human argu-\nmentative behavior. ACM Trans. on Interactive Intelligent\nSystems (TiiS)6(4):30.\nRosenfeld, A., and Kraus, S. 2016b. Strategical argumenta-\ntive agent for human persuasion. In ECAI, 320–328.\nRosenfeld, A., and Kraus, S. 2018. Predicting human\ndecision-making: From prediction to action. Synthesis\nLectures on Artiﬁcial Intelligence and Machine Learning\n12(1):1–150.\nRosenfeld, A., and Richardson, A. 2019. Explainability in\nhuman–agent systems. JAAMAS.\nRosenfeld, A.; Keshet, J.; Goldman, C. V .; and Kraus, S.\n2016. Online prediction of exponential decay time series\nwith human-agent application. In ECAI-16, 595–603.\nSelvaraju, R. R.; Das, A.; Vedantam, R.; Cogswell, M.;\nParikh, D.; and Batra, D. 2017. Grad-CAM: Why did\nyou say that? Visual explanations from deep networks via\ngradient-based localization. In ICCV-17.\nShrikumar, A.; Greenside, P.; and Kundaje, A. 2017. Learn-\ning important features through propagating activation differ-\nences. In ICML-17, 3145–3153.\nWolf, C. T.; Blomberg, J. L.; and Jose, S. 2019. Explain-\nability in context: Lessons from an intelligent system in the\nit services domain.\n13538",
  "topic": null,
  "concepts": []
}