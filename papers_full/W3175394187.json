{
  "title": "LexFit: Lexical Fine-Tuning of Pretrained Language Models",
  "url": "https://openalex.org/W3175394187",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1969142033",
      "name": "Ivan Vulić",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2655028980",
      "name": "Edoardo Maria Ponti",
      "affiliations": [
        "University of Cambridge",
        "McGill University",
        "Mila - Quebec Artificial Intelligence Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1927037681",
      "name": "Anna Korhonen",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A4207383812",
      "name": "Goran Glavaš",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3104723404",
    "https://openalex.org/W3173954987",
    "https://openalex.org/W3105190698",
    "https://openalex.org/W2294542797",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3034417881",
    "https://openalex.org/W1241017059",
    "https://openalex.org/W3094540663",
    "https://openalex.org/W3105721709",
    "https://openalex.org/W3035102548",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2964101860",
    "https://openalex.org/W2963165489",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W3123249980",
    "https://openalex.org/W2803912041",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2252046065",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2150671924",
    "https://openalex.org/W2953246132",
    "https://openalex.org/W3136837718",
    "https://openalex.org/W2899168892",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2952267213",
    "https://openalex.org/W2594021297",
    "https://openalex.org/W3115729981",
    "https://openalex.org/W2608265559",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W4287633380",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W2298037295",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W2620558438",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3090114880",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W1503259811",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W1814992895",
    "https://openalex.org/W2964271799",
    "https://openalex.org/W2251044566",
    "https://openalex.org/W2963047628",
    "https://openalex.org/W3105096579",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2948110372",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W2963366649",
    "https://openalex.org/W2250214110",
    "https://openalex.org/W3104078590",
    "https://openalex.org/W2983836503",
    "https://openalex.org/W2250621296",
    "https://openalex.org/W2126725946",
    "https://openalex.org/W2952190837",
    "https://openalex.org/W2798962680",
    "https://openalex.org/W2963118869",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W3155814483",
    "https://openalex.org/W3106485021",
    "https://openalex.org/W3115228514",
    "https://openalex.org/W3037854022",
    "https://openalex.org/W4230579319",
    "https://openalex.org/W2575410795",
    "https://openalex.org/W2964270525",
    "https://openalex.org/W2250365808",
    "https://openalex.org/W3214161538",
    "https://openalex.org/W2964266061",
    "https://openalex.org/W2626534681",
    "https://openalex.org/W2250958747",
    "https://openalex.org/W3098275893",
    "https://openalex.org/W2963149412",
    "https://openalex.org/W2970453266",
    "https://openalex.org/W2962790854",
    "https://openalex.org/W3213730158",
    "https://openalex.org/W4287993739",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2770960740"
  ],
  "abstract": "Ivan Vulić, Edoardo Maria Ponti, Anna Korhonen, Goran Glavaš. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 5269–5283\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5269\nLEXFIT: Lexical Fine-Tuning of Pretrained Language Models\nIvan Vuli´c♠ Edoardo M. Ponti♥,♠ Anna Korhonen♠ Goran Glavaš♦\n♠Language Technology Lab, University of Cambridge, UK\n♥Mila - Quebec AI Institute and McGill University, Canada\n♦Data and Web Science Group, University of Mannheim, Germany\n{iv250,alk23}@cam.ac.uk\nedoardo-maria.ponti@mila.quebec goran@informatik.uni-mannheim.de\nAbstract\nTransformer-based language models (LMs)\npretrained on large text collections implicitly\nstore a wealth of lexical semantic knowledge,\nbut it is non-trivial to extract that knowledge\neffectively from their parameters. Inspired by\nprior work on semantic specialization of static\nword embedding (WE) models, we show that it\nis possible to expose and enrich lexical knowl-\nedge from the LMs, that is, to specialize them\nto serve as effective and universal “decontex-\ntualized” word encoders even when fed input\nwords “in isolation” (i.e., without any context).\nTheir transformation into such word encoders\nis achieved through a simple and efﬁcient lex-\nical ﬁne-tuning procedure (termed L EXFIT)\nbased on dual-encoder network structures. Fur-\nther, we show that LEXFIT can yield effective\nword encoders even with limited lexical super-\nvision and, via cross-lingual transfer, in dif-\nferent languages without any readily available\nexternal knowledge. Our evaluation over four\nestablished, structurally different lexical-level\ntasks in 8 languages indicates the superior-\nity of LEXFIT-based WEs over standard static\nWEs (e.g., fastText) and WEs from vanilla\nLMs. Other extensive experiments and abla-\ntion studies further proﬁle the L EXFIT frame-\nwork, and indicate best practices and perfor-\nmance variations across L EXFIT variants, lan-\nguages, and lexical tasks, also directly ques-\ntioning the usefulness of traditional WE mod-\nels in the era of large neural models.\n1 Introduction\nProbing large pretrained encoders like BERT (De-\nvlin et al., 2019) revealed that they contain a wealth\nof lexical knowledge (Ethayarajh, 2019; Vuli´c et al.,\n2020). If type-level word vectors are extracted from\nBERT with appropriate strategies, they can even\noutperform traditional word embeddings (WEs) in\nsome lexical tasks (Vuli´c et al., 2020; Bommasani\net al., 2020; Chronis and Erk, 2020). However,\nLexFit loss\n(w, v) = (dormant, asleep)\n1. SOFTMAX\n2. MNEG\n3. MSIM\nBERT BERT\nPooling Pooling\nWord embedding\nextraction\nw v\nu = sleeping BERT\nStep 1: Lexical ﬁne-tuning\nu\nStep 2: Extracting word vectors from (LexFit-ed) BERT \nFigure 1: Illustration of the full pipeline for obtaining\ndecontextualized word representations, based on lexi-\ncally ﬁne-tuning pretrained LMs via dual-encoder net-\nworks (Step 1, §2.1), and then extracting the represen-\ntations from their (ﬁne-tuned) layers (Step 2, §2.2).\nboth static and contextualized WEs ultimately learn\nsolely from the distributional word co-occurrence\nsignal. This source of signal is known to lead to\ndistortions in the induced representations by con-\nﬂating meaning based on topical relatedness rather\nthan authentic semantic similarity (Hill et al., 2015;\nSchwartz et al., 2015; Vuli´c et al., 2017). This also\ncreates a ripple effect on downstream applications,\nwhere model performance may suffer (Faruqui,\n2016; Mrkši´c et al., 2017; Lauscher et al., 2020).\nOur work takes inspiration from the methods to\ncorrect these distortions and complement the distri-\nbutional signal with structured information, which\nwere originally devised for static WEs. In particu-\nlar, the process known as semantic specialization\n(or retroﬁtting) injects information about lexical\nrelations from databases like WordNet (Beckwith\net al., 1991) or the Paraphrase Database (Ganitke-\nvitch et al., 2013) into WEs. Thus, it accentuates\nrelationships of pure semantic similarity in the re-\n5270\nﬁned representations (Faruqui et al., 2015; Mrkši´c\net al., 2017; Ponti et al., 2019, inter alia).\nOur goal is to create representations that take\nadvantage of both 1) the expressivity and lexical\nknowledge already stored in pretrained language\nmodels (LMs) and 2) the precision of lexical ﬁne-\ntuning. To this effect, we develop LEXFIT, a versa-\ntile lexical ﬁne-tuning framework, illustrated in Fig-\nure 1, drawing a parallel with universalsentence en-\ncoders like SentenceBERT (Reimers and Gurevych,\n2019).1 Our working hypothesis, extensively evalu-\nated in this paper, is as follows: pretrained encoders\nstore a wealth of lexical knowledge, but it is not\nstraightforward to extract that knowledge. We can\nexpose this knowledge by rewiring their parame-\nters through lexical ﬁne-tuning, and turn the LMs\ninto universal (decontextualized) word encoders.\nCompared to prior attempts at injecting lexical\nknowledge into large LMs (Lauscher et al., 2020),\nour LEXFIT method is innovative as it is deployed\npost-hoc on top of already pretrained LMs, rather\nthan requiring joint multi-task training. Moreover,\nLEXFIT is: 1) more efﬁcient, as it does not in-\ncur the overhead of masked language modeling\npretraining; and 2) more versatile, as it can be\nported to any model independently from its archi-\ntecture or original training objective. Finally, our\nresults demonstrate the usefulness of LEXFIT: we\nreport large gains over WEs extracted from vanilla\nLMs and over traditional WE models across 8 lan-\nguages and 4 lexical tasks, even with very limited\nand noisy external lexical knowledge, validating\nthe rewiring hypothesis. The code is available at:\nhttps://github.com/cambridgeltl/lexfit.\n2 From Language Models to\n(Decontextualized) Word Encoders\nThe motivation for this work largely stems from the\nrecent work on probing and analyzing pretrained\nlanguage models for various types of knowledge\nthey might implicitly store (e.g., syntax, world\nknowledge) (Rogers et al., 2020). Here, we focus\non their lexical semantic knowledge (Vuli´c et al.,\n2020; Liu et al., 2021), with an aim of extracting\nhigh-quality static word embeddings from the pa-\nrameters of the input LMs. In what follows, we\ndescribe lexical ﬁne-tuning via dual-encoder net-\nworks (§2.1), followed by the WE extraction pro-\n1These approaches are connected as they are both trained\nvia contrastive learning on dual-encoder architectures, but they\nprovide representations for a different granularity of meaning.\ncess from the ﬁne-tuned layers of pretrained LMs\n(§2.2), see Figure 1.\n2.1 L EXFIT: Methodology\nOur hypothesis is that the pretrained LMs can be\nturned into effective static decontextualized word\nencoders via additional inexpensive lexical ﬁne-\ntuning (i.e., LEXFIT-ing) on lexical pairs from an\nexternal resource. In other words, they can be spe-\ncialized to encode lexical knowledge useful for\ndownstream tasks, e.g., lexical semantic similarity\n(Wieting et al., 2015; Mrkši ´c et al., 2017; Ponti\net al., 2018). Let P = {(w,v,r )m}M\nm=1 refer to\nthe set of M external lexical constraints. Each item\np ∈P comprises a pair of words w and v, and\ndenotes a semantic relation rthat holds between\nthem (e.g., synonymy, antonymy). Further, let Pr\ndenote a subset of P where a particular relation r\nholds for each item, e.g., Psyn is a set of synonymy\npairs. Finally, for each positive tuple (w,v,r ), we\ncan construct 2knegative “no-relation” examples\nby randomly pairing w with another word w¬,k′,\nand pairing v with v¬,k′, k′ = 1,...,k , ensuring\nthat these negative pairs do not occur in P. We re-\nfer to the full set of negative pairs as NP. Lexical\nﬁne-tuning then leverages P and NP; We propose\nto tune the underlying LMs (e.g., BERT, mBERT),\nusing external lexical knowledge, via different loss\nfunctions, relying on dual-encoder networks with\nshared LM weights and mean pooling, as illustrated\nin Figure 1. We now brieﬂy describe several loss\nfunctions, evaluated later in §4.\nClassiﬁcation Loss. Similar to prior work on\nsentence-level text inputs (Reimers and Gurevych,\n2019), for each input word pair (w,v) we con-\ncatenate their d-dimensional encodings w and v\n(obtained after passing them through BERT and\nafter pooling, see Figure 1) with their element-wise\ndifference |w −v|. The objective is then:\nL= softmax\n(\nW(w ⊕v ⊕|w −v|)\n)\n. (1)\n⊕denotes concatenation, and W ∈ R3d×c is a\ntrainable weight matrix of the softmax classiﬁer,\nwhere c is the number of classiﬁcation classes.\nWe experiment with two variants of this objective,\ntermed SOFTMAX henceforth: in the simpler binary\nvariant, the goal is to distinguish between positive\nsynonymy pairs (the subset Psyn) and the corre-\nsponding set of 2k×|Psyn|no-relation negative\npairs. In the ternary variant (c = 3), the classi-\nﬁer must distinguish between synonyms ( Psyn),\n5271\nantonyms (Pant), and no-relation negatives. The\nclassiﬁers are optimized via standard cross-entropy.\nRanking Loss. The multiple negatives ranking\nloss (MNEG ) is inspired by prior work on learn-\ning universal sentence encoders (Cer et al., 2018;\nHenderson et al., 2019, 2020); the aim of the loss,\nnow adapted to word-level inputs, is to rank true\nsynonymy pairs from Psyn over randomly paired\nwords. The similarity between any two words w\nand vis quantiﬁed via the similarity function Sop-\nerating on their encodings S(wi,wj). In this work\nwe use the scaled cosine similarity following Hen-\nderson et al. (2019): S(wi,wj) =C·cos(w1,w2),\nwhere Cis the scaling constant. Lexical ﬁne-tuning\nwith MNEG then proceeds in batches of B pairs\n(wi,vi),..., (wB,vB) from Psyn, with the MNEG\nloss for a single batch computed as follows:\nL= −\nB∑\ni=1\nS(wi,vi) +\nB∑\ni=1\nlog\nB∑\nj=1,j̸=i\neS(wi,vj) (2)\nEffectively, for each batch Eq. (2) maximizes the\nsimilarity score of positive pairs (wi,vi), and mini-\nmizes the score of B−1 random pairs. For simplic-\nity, as negatives we use all pairings of wi with vj-s\nin the current batch where (wi,vj) ̸∈Psyn (Yang\net al., 2018; Henderson et al., 2019).\nMulti-Similarity Loss. We also experiment with\na recently proposed state-of-the-art multi-similarity\nloss of Wang et al. (2019), labeled MSIM . The\naim is again to rank positive examples from Psyn\nabove any corresponding no-relation 2knegatives\nfrom NP. Again using the scaled cosine similarity\nscores, the adapted MSIM loss per batch of Bposi-\ntive pairs (wi,vi) from Psyn is deﬁned as follows:\nL= 1\nB\nB∑\ni=1\n(\nlog\n(\n1 +\nk∑\nk′=1\neC(cos(wi,wi,¬,k′)−ϵ)\n)\n+ 1\nC log\n(\n1 +e−C(cos(wi,vi)−ϵ)\n))\n.\n(3)\nFor brevity, in Eq. (3) we only show the formula-\ntion with the knegatives associated with wi, but\nthe reader should be aware that the complete loss\nfunction contains another term covering k nega-\ntives vi,¬,k′ associated with each vi. C is again\nthe scaling constant, and ϵis the offset applied on\nthe similarity matrix. 2 MSIM can be seen as an\nextended variant of the MNEG ranking loss.\n2ϵ=1; C=20 (also in MNEG ). For further technical details\nwe refer the reader to the original paper (Wang et al., 2019).\nFinally, for any input wordw, we extract its word\nvector via the approach outlined in §2.2; exactly the\nsame approach can be applied to the original LMs\n(e.g., BERT) or their lexically ﬁne-tuned variants\n(“LEXFIT-ed” BERT), see Figure 1.\n2.2 Extracting Static Word Representations\nThe extraction of static type-level vectors from any\nunderlying Transformer-based LM, both before and\nafter LEXFIT ﬁne-tuning, is guided by best prac-\ntices from recent comparative analyses and probing\nwork (Vuli´c et al., 2020; Bommasani et al., 2020).\nStarting from an underlying LM with N Trans-\nformer layers {L1 (bottom layer),...,L N (top)}\nand referring to the embedding layer as L0, we\nextract a decontextualized word vector for some\ninput word w, fed into the LM “in isolation” with-\nout any surrounding context, following Vuli´c et al.\n(2020): 1) w is segmented into 1 or more of its\nconstituent subwords [swi],i ≥1, where [] refers\nto the sequence of isubwords; 2) Special tokens\n[CLS] and [SEP] are respectively prepended and\nappended to the subword sequence, and the se-\nquence [CLS][swi][SEP] is then passed through\nthe LM; 3) The ﬁnal representation is constructed\nas the average over the subword encodings further\naveraged over n≤N layers (i.e., all layers up to\nlayer Ln included, denoted as A VG(≤n)).3 Fur-\nther, Vuli´c et al. (2020) empirically veriﬁed that:\n(a) discarding ﬁnal encodings of[CLS] and [SEP]\nproduces better type-level vectors – we follow this\nheuristic in this work; and (b) excluding higher\nlayers from the average may also result in stronger\nvectors with improved performance in lexical tasks.\nThis approach operates fully “in isolation” (ISO):\nwe extract vectors of words without any surround-\ning context. The ISO approach is lightweight: 1) it\ndisposes of any external text corpora; 2) it encodes\nwords efﬁciently due to the absence of context.\nMoreover, it allows us to directly study the richness\nof lexical information stored in the LM’s parame-\nters, and to combine it with ISO lexical knowledge\nfrom external resources (e.g., WordNet).\n3 Experimental Setup\nLanguages and Language Models. Our language\nselection for evaluation is guided by the following\n(partially clashing) constraints (Vuli´c et al., 2020):\na) availability of comparable pretrained monolin-\ngual LMs; b) task and evaluation data availabil-\n3Note that this always includes the embedding layer (L0).\n5272\nity; and c) ensuring some typological diversity of\nthe selection. The ﬁnal test languages are English\n(EN), German (DE), Spanish (ES), Finnish (FI), Ital-\nian (IT), Polish ( PL), Russian ( RU), and Turkish\n(TR). For comparability across languages, we use\nmonolingual uncased BERT Base models for all\nlanguages (N = 12Transformer layers, 12 atten-\ntion heads, hidden layer dimensionality is 768),\navailable (see the appendix) via the HuggingFace\nrepository (Wolf et al., 2020).\nExternal Lexical Knowledge. We use the stan-\ndard collection of EN lexical constraints from pre-\nvious work on (static) word vector specialization\n(Zhang et al., 2014; Ono et al., 2015; Vuli´c et al.,\n2018; Ponti et al., 2018, 2019). It covers the\nlexical relations from WordNet (Fellbaum, 1998)\nand Roget’s Thesaurus (Kipfer, 2009); it com-\nprises 1,023,082 synonymy (Psyn) word pairs and\n380,873 antonymy pairs (Pant). For all other lan-\nguages, we rely on non-curated noisy lexical con-\nstraints, obtained via an automatic word translation\nmethod by Ponti et al. (2019); see the original work\nfor the details of the translation procedure.\nLEXFIT: Technical Details. The implementa-\ntion is based on the SBERT framework (Reimers\nand Gurevych, 2019), using the suggested settings:\nAdamW (Loshchilov and Hutter, 2018); learning\nrate of 2e−5; weight decay rate of 0.01, and we\nrun LEXFIT for 2 epochs. The batch size is 512\nwith MNEG , and 256 with SOFTMAX and MSIM ,\nwhere one batch always balances between Bposi-\ntive examples and 2k·Bnegatives (see §2.1).\nWord Vocabularies and Baselines. We extract\ndecontextualized type-level WEs in each language\nboth from the original BERTs (termedBERT-REG )4\nand the LEXFIT-ed BERT models for exactly the\nsame vocabulary. Following Vuli ´c et al. (2020),\nthe vocabularies cover the top 100K most fre-\nquent words represented in the respective fastText\n(FT) vectors, trained on lowercased monolingual\nWikipedias by Bojanowski et al. (2017). 5 The\nequivalent vocabulary coverage allows for a direct\ncomparison of all WEs regardless of the induc-\ntion/extraction method; this also includes the FT\n4For the baseline BERT-REG WEs, we report two variants:\n(a) all performs layerwise averaging over all Transformer\nlayers (i.e., A VG(≤12)); (b) best reports the peak score when\npotentially excluding highest layers from the layer averaging\n(i.e., A VG(≤n), n≤12; see §2.2) (Vuli´c et al., 2020).\n5Note that the LEXFIT procedure does not depend on the\nchosen vocabulary, as it operates only on the lexical items\nfound in the external constraints (i.e., the set P).\nvectors, used as baseline “traditional” static WEs\n(termed FASTTEXT .WIKI ) in all evaluation tasks.\nEvaluation Tasks. We evaluate on the following\nstandard and diverse lexical semantic tasks:\nTask 1: Lexical semantic similarity (LSIM) is\nan established intrinsic task for evaluating static\nWEs (Hill et al., 2015). We use the recent com-\nprehensive multilingual LSIM benchmark Multi-\nSimLex (Vuli´c et al., 2020), which comprises 1,888\npairs in 13 languages, for our EN, ES, FI, PL, and\nRU LSIM evaluation. We also evaluate on a verb-\nfocused EN LSIM benchmark: SimVerb-3500 (SV)\n(Gerz et al., 2016), covering 3,500 verb pairs, and\nSimLex-999 (SL) for DE and IT (999 pairs) (Le-\nviant and Reichart, 2015).6\nTask 2: Bilingual Lexicon Induction (BLI) , a\nstandard task to assess the “semantic quality” of\nstatic cross-lingual word embeddings (CLWEs)\n(Ruder et al., 2019), enables investigations on the\nalignability of monolingual type-level WEs in dif-\nferent languages before and after the LEXFIT pro-\ncedure. We learn CLWEs from monolingual WEs\nobtained with all WE methods using the established\nand supervision-lenient mapping-based approach\n(Mikolov et al., 2013a; Smith et al., 2017) with the\nVECMAP framework (Artetxe et al., 2018). We\nrun main BLI evaluations for 10 language pairs\nspanning EN, DE, RU, FI, TR.7\nTask 3: Lexical Relation Prediction (RELP).\nWe assess the usefulness of lexical knowledge in\nWEs to learn relation classiﬁers for standard lex-\nical relations (i.e., synonymy, antonymy, hyper-\nnymy, meronymy, plus no relation) via a state-of-\nthe-art neural model for RELP which learns solely\nbased on input type-level WEs (Glavaš and Vuli´c,\n2018). We use the WordNet-based evaluation data\nof Glavaš and Vuli´c (2018) for EN, DE, ES; they\ncontain 10K annotated word pairs per language, 8K\nfor training, 2K for test, balanced by class and in\nthe splits. We extract evaluation data for two more\nlanguages: FI and IT. We report micro-averaged F1\nscores, averaged across 5 runs for each input WE\nspace; the default RELP model setting is used. In\nRELP and LSIM, we remove all training and test\n6The evaluation metric is the Spearman’s rank correlation\nbetween the average of human LSIM scores for word pairs\nand the cosine similarity between their respective WEs.\n7A standard BLI setup and data from Glavaš et al. (2019) is\nadopted: 5K training word pairs are used to learn the mapping,\nand another 2K pairs as test data. The evaluation metric is\nstandard Mean Reciprocal Rank (MRR). For EN–ES, we run\nexperiments on MUSE data (Conneau et al., 2018).\n5273\nRELP/LSIM examples also present in the Psyn and\nPant sets to avoid any evaluation data leakage.8\nTask 4: Lexical Simpliﬁcation (LexSIMP) aims\nto automatically replace complex words (i.e., spe-\ncialized terms, less-frequent words) with their sim-\npler in-context synonyms, while retaining gram-\nmaticality and conveying the same meaning as\nthe more complex input text (Paetzold and Specia,\n2017). Therefore, discerning between semantic\nsimilarity (e.g., synonymy injected via LEXFIT)\nand broader relatedness is critical for LexSIMP\n(Glavaš and Vuli´c, 2018). We adopt the standard\nLexSIMP evaluation protocol used in prior research\non static WEs (Ponti et al., 2018, 2019). 1) We use\nLight-LS (Glavaš and Štajner, 2015), a language-\nagnostic LexSIMP tool that makes simpliﬁcations\nin an unsupervised way based solely on word simi-\nlarity in an input (static) WE space; 2) we rely on\nstandard LexSIMP benchmarks, available for EN\n(Horn et al., 2014), IT (Tonelli et al., 2016), and\nES (Saggion, 2017); and 3) we report the standard\nAccuracy scores (Horn et al., 2014).9\nImportant Disclaimer. We note that the main pur-\npose of the chosen evaluation tasks and experimen-\ntal protocols is not necessarily achieving state-of-\nthe-art performance, but rather probing the vectors\nin different lexical tasks requiring different types\nof lexical knowledge,10 and offering fair and in-\nsightful comparisons between different LEXFIT\nvariants, as well as against standard static WEs\n(fastText) and non-tuned BERT-based static WEs.\n4 Results and Discussion\nThe main results for all four tasks are summarized\nin Tables 1-4, and further results and analyses are\navailable in §4.1 (with additional results in the ap-\npendix). These results offer multiple axes of com-\nparison, discussed in what follows.\nComparison to Other Static Word Embeddings.\nThe results over all 4 tasks indicate that static WEs\nfrom LEXFITed monolingual BERT 1) outperform\ntraditional WE methods such as FT, and 2) offer\nalso large gains over WEs originating from non-\nLEXFITed BERTs (Vuli´c et al., 2020). These re-\n8In BLI and RELP, we do PCA ( d = 300) on all input\nWEs, which slightly improves performance.\n9For further details regarding the LexSIMP benchmarks\nand evaluation, we refer the reader to the previous work.\n10RELP and LexSIMP use WEs as input features of neu-\nral architectures; LSIM and BLI fall under similarity-based\nevaluation tasks (Ruder et al., 2019).\nsults demonstrate that the inexpensive lexical ﬁne-\ntuning procedure can indeed turn large pretrained\nLMs into effective decontextualized word encoders,\nand this can be achieved for a reasonably wide\nspectrum of languages for which such pretrained\nLMs exist. What is more, LEXFIT for all non-\nEN languages has been run with noisy automat-\nically translated lexical constraints, which holds\npromise to support even stronger static LEXFIT-\nbased WEs with human-curated data in the future,\ne.g., extracted from multilingual WordNets (Bond\nand Foster, 2013), PanLex (Kamholz et al., 2014),\nor BabelNet (Ehrmann et al., 2014).\nThe results give rise to additional general impli-\ncations. First, they suggest that the pretrained LMs\nstore even more lexical knowledge than thought\npreviously (Ethayarajh, 2019; Bommasani et al.,\n2020; Vuli´c et al., 2020); the role of LEXFIT ﬁne-\ntuning is simply to ‘rewire’ and expose that knowl-\nedge from the LM through (limited) lexical-level\nsupervision. To further investigate the ‘rewiring’\nhypothesis, in §4.1, we also run LEXFIT with a\ndrastically reduced amount of external knowledge.\nBERT-REG vectors display large gains over FT\nvectors in tasks such as RELP and LexSIMP, again\nhinting that plenty of lexical knowledge is stored\nin the original parameters. However, they still lag\nFT vectors for some tasks (BLI for all language\npairs; LSIM for ES, RU, PL). However, LEXFIT-ed\nBERT-based WEs offer large gains and outperform\nFT WEs across the board. Our results indicate that\n‘classic’ WE models such as skip-gram (Mikolov\net al., 2013b) and FT are undermined even in their\nlast ﬁeld of use, lexical tasks.\nThis comes as a natural ﬁnding, given that\nword2vec and FT can in fact be seen as reduced and\ntraining-efﬁcient variants of full-ﬂedged language\nmodels (Bengio et al., 2003). The modern LMs\nare pretrained on larger training data with more pa-\nrameters and with more sophisticated Transformer-\nbased neural architectures. However, it has not\nbeen veriﬁed before that effective static WEs can\nbe distilled from such LMs. Efﬁciency differences\naside, this begs the following discussion point for\nfuture work: with the existence of large pretrained\nLMs, and effective methods to extract static WEs\nfrom them, as proposed in this work, how useful are\ntraditional WE models still in NLP applications?\nLexical Fine-Tuning Objectives. The scores indi-\ncate that all LEXFIT variants are effective and can\nexpose the lexical knowledge from the ﬁne-tuned\n5274\nMethod EN EN : SV ES FI PL RU DE : SL IT: SL\nFASTTEXT .WIKI 44.2 25.8 45.0 58.7 36.7 35.8 41.3 30.5\nBERT-REG (all) 46.7 23.9 42.4 55.3 32.0 30.6 31.3 28.8\nBERT-REG (best) 51.8 28.9 44.2 61.5 32.4 30.7 34.6 31.1\nMNEG [113 min] 73.6 68.3 62.3 72.0 52.4 50.4 49.7 58.7\nMSIM [174 min] 74.3 69.6 61.8 71.1 51.8 49.9 49.7 58.9\nSOFTMAX (binary) [177 min] 64.3 58.8 58.9 62.4 44.7 44.6 43.7 49.4\nSOFTMAX (ternary) [212 min] 67.8 61.7 59.4 66.2 46.3 38.8 45.3 52.4\nTable 1: Results in the LSIM task; Spearman’s ρcorrelation scores (×100). k = 1for the MSIM and SOFTMAX\nlexical ﬁne-tuning variants (see §3). SV = SimVerb-3500; SL = SimLex-999. The best score in each column is\nin bold; the second best is underlined. Additional LSIM results are available in the appendix. The numbers in []\ndenote the average ﬁne-tuning time with each LEXFIT objective per 1 epoch in English (1 GTX TITAN X GPU).\nMethod EN–DE EN –TR EN –FI EN –RU DE –TR DE –FI DE –RU TR –FI TR –RU FI –RU avg\nFASTTEXT.WIKI 61.0 43.3 48.8 52.2 35.8 43.5 46.9 35.8 36.4 43.9 44.8\nBERT-REG (all) 44.6 37.9 47.1 47.3 32.3 39.5 41.2 35.2 31.9 38.7 39.6\nBERT-REG (best) 47.2 39.0 48.6 48.8 32.3 39.5 41.2 35.2 31.9 39.2 40.3\nMNEG 58.1 46.2 57.7 54.0 36.2 46.1 46.7 39.6 36.7 42.4 46.4\nMSIM 58.9 45.9 57.7 53.7 37.1 46.4 46.7 39.4 37.4 44.2 46.7\nSOFTMAX (binary) 57.9 45.3 53.8 53.6 35.9 44.3 43.5 38.4 36.0 42.8 45.2\nSOFTMAX (ternary) 57.1 44.9 54.8 52.7 35.2 44.0 44.6 38.4 34.9 41.1 44.8\nTable 2: Results in the BLI task (MMR ×100). k= 1. Additional BLI results are available in the appendix.\nMethod EN DE ES FI IT\nFASTTEXT.WIKI 66.0 60.1 62.2 68.2 64.8\nBERT-REG (all) 71.4 67.3 65.1 69.6 66.8\nBERT-REG (best) 71.8 67.9 65.5 69.9 67.2\nMNEG 74.1 69.7 67.8 71.3 71.1\nMSIM 74.3 69.0 68.6 72.2 71.4\nSOFTMAX (binary) 74.0 68.4 67.4 71.5 70.1\nSOFTMAX (ternary) 75.5 70.3 70.3 73.2 71.3\nTable 3: Results in the RELP task (Micro- F1 ×100,\naveraged over 5 runs). More results in the appendix.\nMethod EN ES IT\nFASTTEXT .WIKI 11.4 16.3 14.2\nBERT-REG (all) 71.6 38.3 32.7\nMNEG 83.8 55.3 45.0\nMSIM 84.4 56.7 45.4\nSOFTMAX (binary) 84.8 56.7 45.8\nSOFTMAX (ternary) 84.0 53.9 44.2\nTable 4: LexSIMP results (Accuracy ×100).\nBERTs. However, there are differences across their\ntask performance: the ranking-based MNEG and\nMSIM variants display stronger performance on\nsimilarity-based ranking lexical tasks such as LSIM\nand BLI. The classiﬁcation-based SOFTMAX objec-\ntive is, as expected, better aligned with the RELP\ntask, and we note slight gains with its ternary vari-\nant which leverages extra antonymy knowledge.\nThis ﬁnding is well aligned with the recent ﬁnd-\nings demonstrating that task-speciﬁc pretraining re-\nsults in stronger (sentence-level) task performance\n(Glass et al., 2020; Henderson et al., 2020; Lewis\net al., 2020). In our case, we show that task-speciﬁc\nlexical ﬁne-tuning can reshape the underlying LM’s\nparameters to not only act as a universal word en-\ncoder, but also towards a particular lexical task.\nThe per-epoch time measurements from Table 1\nvalidate the efﬁciency of LEXFIT as a post-training\nﬁne-tuning procedure. Previous approaches that at-\ntempted to inject lexical information (i.e., word\nsenses and relations) into large LMs (Lauscher\net al., 2020; Levine et al., 2020) relied on joint LM\n(re)training from scratch: it is effectively costlier\nthan training the original BERT models.\nPerformance across Languages and Tasks. As\nexpected, the scores in absolute terms are highest\nfor EN: this is attributed to (a) larger pretraining\nLM data as well as (b) to clean external lexical\nknowledge. However, we note encouragingly large\ngains in target languages even with noisy trans-\nlated lexical constraints. LEXFIT variants show\nsimilar relative patterns across different languages\nand tasks. We note that, while BERT-REG vectors\nare unable to match FT performance in the BLI\ntask, our LEXFIT methods (e.g., see MNEG and\nMSIM BLI scores) outperform FT WEs in this task\n5275\nFull 100k 50k 20k 10k 5k\n# of LEX FIT ﬁne-tuning examples\n50\n55\n60\n65\n70\n75Spearman ρ correlation\nEN EN (SV) ES FI IT (SL)\n(a) LSIM\nFull 100k 50k 20k 10k 5k\n# of LEX FIT ﬁne-tuning examples\n40\n45\n50\n55\n60\n65Mean Reciprocal Rank\nEN–ES EN–FI EN–IT FI–IT (b) BLI\nFull 100k 50k 20k 10k 5k\n# of LEX FIT ﬁne-tuning examples\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0Micro F1\nEN ES FI (c) RELP\nFigure 2: Varying the amount of external lexical knowledge for LEXFIT (MSIM , k= 1).\nk = 1 k = 2 k = 4 k = 8\nNumber of negative examples\n50\n60\n70\n80Spearman ρ correlation\nEN: A (44.2)\nEN: B\nES : A (45.0)\nES : B\nFI: A (58.7)\nFI: B\n(a) LSIM\nk = 1 k = 2 k = 4 k = 8\nNumber of negative examples\n30\n40\n50\n60BLI scores (MRR)\nEN-FI: A (48.8)\nEN-FI: B\nEN-TR : A (43.3)\nEN-TR : B\nTR -FI: A (35.8)\nTR -FI: B (b) BLI\nk = 1 k = 2 k = 4 k = 8\nNumber of negative examples\n60\n65\n70\n75Micro F1\nEN: A (66.0)\nEN: B\nDE : A (60.1)\nDE : B\nES : A (62.2)\nES : B (c) RELP\nFigure 3: Impact of the number of of negative examples kon lexical task performance. In the legends, A = MSIM ;\nB = SOFTMAX (the binary variant plotted for RELP and BLI, ternary for RELP). The numbers in the parentheses\ndenote performance of FT vectors. The full results with more languages and LEXFIT variants are in the appendix.\nas well, offering improved alignability (Søgaard\net al., 2018) between monolingual WEs. The large\ngains of BERT-REG over FT in RELP and LexSIMP\nacross all evaluation languages already suggest that\nplenty of lexical knowledge is stored in the pre-\ntrained BERTs’ parameters; however,LEXFIT-ing\nthe models offers further gains in LexSIMP and\nRELP across the board, even with limited external\nsupervision (see also Figure 2c).\nHigh scores with FI in LSIM and BLI are aligned\nwith prior work (Virtanen et al., 2019; Rust et al.,\n2021) that showcased strong monolingual perfor-\nmance of FI BERT in sentence-level tasks. Along\nthis line, we note that the ﬁnal quality of LEXFIT-\nbased WEs in each language depends on several\nfactors: 1) pretraining data; 2) the underlying LM;\n3) the quality and amount of external knowledge.\n4.1 Further Discussion\nThe multi-component LEXFIT framework allows\nfor a plethora of additional analyses, varying com-\nponents such as the underlying LM, properties of\nthe LEXFIT variants (e.g., negative examples, ﬁne-\ntuning duration, the amount of lexical constraints).\nWe now analyze the impact of these components\non the “lexical quality” of the LEXFIT-tuned static\nWEs. Unless noted otherwise, for computational\nfeasibility and to avoid clutter, we focus 1) on a\nsubset of target languages: EN, ES, FI, IT, 2) on the\nMSIM variant (k= 1), which showed robust perfor-\nEN ES FI IT\nLanguage\n50\n55\n60\n65\n70\n75Spearman ρ correlation\nMONO BERT MBERT\n(a) LSIM\nEN–ES EN–FI EN–IT FI–IT\nLanguage pair\n10\n20\n30\n40\n50\n60\n70Mean Reciprocal Rank (MRR)\nMONO BERT MBERT (b) BLI\nEN ES FI\nLanguage\n66\n68\n70\n72\n74\n76Micro F1\nMONO BERT MBERT\n(c) RELP\nEN ES IT\nLanguage\n65\n70\n75\n80\n85\n90Accuracy\nMONO BERT MBERT (d) LexSIMP\nFigure 4: Performance comparison between language-\nspeciﬁc monolingual BERT models (MONO BERT) and\nmBERT serving as the underlying LM. MSIM (k= 1).\nmance in the main experiments before, and 3) on\nLSIM, BLI, and RELP as the main tasks in these\nanalyses, as they offer a higher language coverage.\nVarying the Amount of Lexical Constraints.We\nalso probe what amount of lexical knowledge is\nrequired to turn BERTs into effective decontextual-\nized word encoders by running tests with reduced\nlexical sets P sampled from the full set. The scores\nover different P sizes, averaged over 5 samples per\neach size, are provided in Figure 2, and we note\nthat they extend to other evaluation languages and\nLEXFIT objectives. As expected, we do observe\nperformance drops with fewer external data. How-\never, the decrease is modest even when relying on\n5276\nn= 2 4 6 8 10 12\nLSIM\nEN:REG 51.6 51.8 50.7 49.5 48.0 46.7\nEN:MSIM 58.8 61.5 64.2 65.0 71.7 74.3\nFI:REG 57.3 59.8 61.5 61.1 59.3 55.3\nFI:MSIM 57.0 64.1 66.6 69.6 70.2 71.1\nBLI EN–FI:REG 39.2 43.8 47.6 48.6 48.3 47.1\nEN–FI:MSIM 40.2 45.6 50.7 54.3 56.1 57.7\nTable 5: Task performance of WEs extracted via\nlayerwise averaging over different Transformer layers\n(A VG(≤n) extraction variants; §2.2) for a selection of\ntasks and languages. L EXFIT variant: MSIM (k = 1).\nREG = BERT-REG . Highest scores per row are in bold.\nonly 5k external constraints (e.g., see the scores in\nBLI and RELP for all languages; EN Multi-SimLex\nscore is 69.4 with 50k constraints, 65.0 with 5k),\nor even non-existent (RELP in FI).\nRemarkably, the LEXFIT performance with only\n10k or 5k ﬁne-tuning pairs11 remains substantially\nhigher than with FT or BERT-REG WEs in all tasks.\nThis empirically validates LEXFIT’s sample efﬁ-\nciency and further empirically corroborates our\nknowledge rewiring hypothesis: the original LMs\nalready contain plenty of useful lexical knowledge\nimplicitly, and even a small amount of external\nsupervision can expose that knowledge.\nCopying or Rewiring Knowledge? Large gains\nover BERT-REG even with mere 5k pairs (LEXFIT-\ning takes only a few minutes), where the large por-\ntion of the 100K word vocabulary is not covered\nin the external input, further reveal that LEXFIT\ndoes not only copy the knowledge of seen words\nand relations into the LM: it leverages the (small)\nexternal set to generalize to uncovered words.\nWe conﬁrm this hypothesis with another experi-\nment where our input LM is the same BERT Base\narchitecture parameters with the same subword vo-\ncabulary as English BERT, but with its parameters\nnow randomly initialized using the Xavier initial-\nization (Glorot and Bengio, 2010). Running LEX-\nFIT on this model for 10 epochs with the full set\nof lexical constraints (see §3) yields the follow-\ning LSIM scores: 23.1 (Multi-SimLex) and 14.6\n(SimVerb), and the English RELP accuracy score\nof 61.8%. The scores are substantially higher than\nthose of fully random static WEs (see also the ap-\npendix), which indicates that the LEXFIT proce-\ndure does enable storing some lexical knowledge\ninto the model parameters. However, at the same\n11When sampling all reduced sets, we again deliberately\nexcluded all words occurring in our LSIM benchmarks.\ntime, these scores are substantially lower than the\nones achieved when starting from LM-pretrained\nmodels, even when LEXFIT is run with mere 5k\nﬁne-tuning lexical pairs.12 This again strongly sug-\ngests that LEXFIT ’unlocks’ already available lexi-\ncal knowledge stored in the pretrained LM, yielding\nbeneﬁts beyond the knowledge available in the ex-\nternal data. Another line of recent work (Liu et al.,\n2021) further corroborates our ﬁndings.\nMultilingual LMs. Prior work indicated that mas-\nsively multilingual LMs such as multilingual BERT\n(mBERT) (Devlin et al., 2019) and XLM-R (Con-\nneau et al., 2020) cannot match the performance\nof their language-speciﬁc counterparts in both lex-\nical (Vuli´c et al., 2020) and sentence-level tasks\n(Rust et al., 2021). We also analyze this conjec-\nture by LEXFIT-ing mBERT instead of monolin-\ngual BERTs in different languages. The results\nwith MSIM (k = 1) are provided in Figure 4; we\nobserve similar comparison trends with other lan-\nguages and LEXFIT variants, not shown due to\nspace constraints. While LEXFIT-ing mBERT of-\nfers huge gains over the original mBERT model,\nsometimes even larger in relative terms than with\nmonolingual BERTs (e.g., LSIM scores for EN in-\ncrease from 0.21 to 0.69, and from 0.24 to 0.60 for\nFI; BLI scores for EN-FI rise from 0.21 to 0.37), it\ncannot match the absolute performance peaks of\nLEXFIT-ed monolingual BERTs.\nStoring the knowledge of 100+ languages in\nits limited parameter budget, mBERT still cannot\ncapture monolingual knowledge as accurately as\nlanguage-speciﬁc BERTs (Conneau et al., 2020).\nHowever, we believe that its performance with\nLEXFIT may be further improved by leveraging re-\ncently proposed multilingual LM adaptation strate-\ngies that mitigate a mismatch between shared multi-\nlingual and language-speciﬁc vocabularies (Artetxe\net al., 2020; Chung et al., 2020; Pfeiffer et al.,\n2020); we leave this for future work.\nLayerwise Averaging. A consensus in prior work\n(Tenney et al., 2019; Ethayarajh, 2019; Vuli´c et al.,\n2020) points that out-of-context lexical knowledge\nin pretrained LMs is typically stored in bottom\nTransformer layers (see Table 5). However, Table 5\nalso reveals that this does not hold after LEXFIT-\ning: the tuned model requires knowledge from all\nlayers to extract effective decontextualized WEs\nand reach peak task scores. Effectively, this means\n12The same ﬁndings hold for other tasks and languages.\n5277\nthat, through lexical ﬁne-tuning, model “reformats”\nall its parameter budget towards storing useful lexi-\ncal knowledge, that is, it specializes as (decontex-\ntualized) word encoder.\nVarying the Number of Negative Examples and\ntheir impact on task performance is recapped in\nFigure 3b. Overall, increasing kdoes not beneﬁt\n(and sometimes even hurts) performance – the ex-\nceptions are EN LSIM; and the RELP task with the\nSOFTMAX variant for some languages. We largely\nattribute this to the noise in the target-language lex-\nical pairs: with larger kvalues, it becomes increas-\ningly difﬁcult for the model to discern between\nnoisy positive examples and random negatives.\nLonger Fine-Tuning. Instead of the standard\nsetup with 2 epochs (see §3), we runLEXFIT for 10\nepochs. The per-epoch snapshots of scores are sum-\nmarized in the appendix. The scores again validate\nthat LEXFIT is sample-efﬁcient: longer ﬁne-tuning\nyields negligible to zero improvements inEN LSIM\nand RELP after the ﬁrst few epochs, with very high\nscores achieved after epoch 1 already. It even yields\nsmall drops for other languages in LSIM and BLI:\nwe again attribute this to slight overﬁtting to noisy\ntarget-language lexical knowledge.\n5 Conclusion and Future Work\nWe proposed LEXFIT, a lexical ﬁne-tuning pro-\ncedure which transforms pretrained LMs such as\nBERT into effective decontextualized word en-\ncoders through dual-encoder architectures. Our\nexperiments demonstrated that the lexical knowl-\nedge already stored in pretrained LMs can be fur-\nther exposed via additional inexpensive LEXFIT-\ning with (even limited amounts of) external lexical\nknowledge. We successfully applied LEXFIT even\nto languages without any external human-curated\nlexical knowledge. Our LEXFIT word embeddings\n(WEs) outperform “traditional” static WEs (e.g.,\nfastText) across a spectrum of lexical tasks across\ndiverse languages in controlled evaluations, thus\ndirectly questioning the practical usefulness of the\ntraditional WE models in modern NLP.\nBesides inducing better static WEs for lexical\ntasks, following the line of lexical probing work\n(Ethayarajh, 2019; Vuli´c et al., 2020), our goal in\nthis work was to understand how (and how much)\nlexical semantic knowledge is coded in pretrained\nLMs, and how to ‘unlock’ the knowledge from the\nLMs. We hope that our work will be beneﬁcial for\nall lexical tasks where static WEs from traditional\nWE models are still largely used (Schlechtweg\net al., 2020; Kaiser et al., 2021).\nDespite the extensive experiments, we only\nscratched the surface, and can indicate a spectrum\nof future enhancements to the proof-of-concept\nLEXFIT framework beyond the scope of this work.\nWe will test other dual-encoder loss functions, in-\ncluding ﬁner-grained relation classiﬁcation tasks\n(e.g., in the SOFTMAX variant), and hard (instead of\nrandom) negative examples (Wieting et al., 2015;\nMrkši´c et al., 2017; Lauscher et al., 2020; Kalan-\ntidis et al., 2020). While in this work, for simplicity\nand efﬁciency, we focused on fully decontextual-\nized ISO setup (see §2.2), we will also probe alter-\nnative ways to extract static WEs from pretrained\nLMs, e.g., averages-over-context (Liu et al., 2019;\nBommasani et al., 2020; Vuli ´c et al., 2020). We\nwill also investigate other approaches to procuring\nmore accurate external knowledge for LEXFIT in\ntarget languages, and extend the framework to more\nlanguages, lexical tasks, and specialized domains.\nWe will also focus on reducing the gap between\npretrained monolingual and multilingual LMs.\nAcknowledgments\nWe thank the three anonymous reviewers, Nils\nReimers, and Jonas Pfeiffer for their helpful com-\nments and suggestions. Ivan Vuli´c and Anna Korho-\nnen are supported by the ERC Consolidator Grant\nLEXICAL: Lexical Acquisition Across Languages\n(no. 648909) awarded to Korhonen, and the ERC\nPoC Grant MultiConvAI: Enabling Multilingual\nConversational AI (no. 957356). Goran Glavaš\nis supported by the Baden Württemberg Stiftung\n(Eliteprogramm, AGREE grant).\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.\nA robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. InPro-\nceedings of ACL 2018, pages 789–798.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of ACL\n2020, pages 4623–4637.\nRichard Beckwith, Christiane Fellbaum, Derek Gross,\nand George A. Miller. 1991. WordNet: A lexical\ndatabase organized on psycholinguistic principles.\nLexical acquisition: Exploiting on-line resources to\nbuild a lexicon, pages 211–231.\n5278\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the ACL ,\n5:135–146.\nRishi Bommasani, Kelly Davis, and Claire Cardie.\n2020. Interpreting Pretrained Contextualized Repre-\nsentations via Reductions to Static Embeddings. In\nProceedings of ACL 2020, pages 4758–4781.\nFrancis Bond and Ryan Foster. 2013. Linking and ex-\ntending an open multilingual Wordnet. In Proceed-\nings of ACL 2013, pages 1352–1362.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\n2018. Universal sentence encoder for English. In\nProceedings of EMNLP 2018, pages 169–174.\nGabriella Chronis and Katrin Erk. 2020. When is a\nbishop not like a rook? When it’s like a rabbi! multi-\nprototype bert embeddings for estimating semantic\nrelationships. In Proceedings of CoNLL 2020, page\n227–244.\nHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and\nJason Riesa. 2020. Improving multilingual models\nwith language-clustered vocabularies. In Proceed-\nings of EMNLP 2020, pages 4536–4546.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of ACL 2020, pages 8440–8451.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In Proceed-\nings of ICLR 2018.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT 2019 ,\npages 4171–4186.\nMaud Ehrmann, Francesco Cecconi, Daniele Vannella,\nJohn Philip McCrae, Philipp Cimiano, and Roberto\nNavigli. 2014. Representing multilingual data as\nlinked data: The case of BabelNet 2.0. In Proceed-\nings of LREC 2014, pages 401–408.\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? Comparing the ge-\nometry of BERT, ELMo, and GPT-2 embeddings.\nIn Proceedings of EMNLP-IJCNLP 2019, pages 55–\n65.\nManaal Faruqui. 2016. Diverse Context for Learning\nWord Representations. Ph.D. thesis, Carnegie Mel-\nlon University.\nManaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,\nChris Dyer, Eduard Hovy, and Noah A. Smith.\n2015. Retroﬁtting word vectors to semantic lexi-\ncons. In Proceedings of NAACL-HLT 2015 , pages\n1606–1615.\nChristiane Fellbaum. 1998. WordNet. MIT Press.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. PPDB: The Paraphrase\nDatabase. In Proceedings of NAACL-HLT 2013 ,\npages 758–764.\nDaniela Gerz, Ivan Vuli´c, Felix Hill, Roi Reichart, and\nAnna Korhonen. 2016. SimVerb-3500: A large-\nscale evaluation set of verb similarity. In Proceed-\nings of EMNLP 2016, pages 2173–2182.\nMichael Glass, Alﬁo Gliozzo, Rishav Chakravarti, An-\nthony Ferritto, Lin Pan, GP Shrivatsa Bhargav, Di-\nnesh Garg, and Avirup Sil. 2020. Span selection pre-\ntraining for question answering. In Proceedings of\nACL 2020, pages 2773–2782.\nGoran Glavaš and Sanja Štajner. 2015. Simplifying lex-\nical simpliﬁcation: Do we need simpliﬁed corpora?\nIn Proceedings of ACL-IJCNLP 2015, pages 63–68.\nGoran Glavaš and Ivan Vuli´c. 2018. Discriminating be-\ntween lexico-semantic relations with the specializa-\ntion tensor model. In Proceedings of NAACL-HLT\n2018, pages 181–187.\nGoran Glavaš and Ivan Vuli ´c. 2018. Explicit\nretroﬁtting of distributional word vectors. In Pro-\nceedings of ACL 2018, pages 34–45.\nGoran Glavaš, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of ACL 2019, pages 710–721.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of AISTATS 2010, pages\n249–256.\nMatthew Henderson, Iñigo Casanueva, Nikola Mrkši´c,\nPei-Hao Su, Tsung-Hsien Wen, and Ivan Vuli ´c.\n2020. ConveRT: Efﬁcient and accurate conversa-\ntional representations from transformers. In Find-\nings of EMNLP 2020, pages 2161–2174.\nMatthew Henderson, Ivan Vuli ´c, Daniela Gerz, Iñigo\nCasanueva, Paweł Budzianowski, Sam Coope,\nGeorgios Spithourakis, Tsung-Hsien Wen, Nikola\nMrkši´c, and Pei-Hao Su. 2019. Training neural re-\nsponse selection for task-oriented dialogue systems.\nIn Proceedings of ACL 2019, pages 5392–5404.\n5279\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimLex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics, 41(4):665–695.\nColby Horn, Cathryn Manduca, and David Kauchak.\n2014. Learning a lexical simpliﬁer using Wikipedia.\nIn Proceedings of ACL 2014, pages 458–463.\nJens Kaiser, Sinan Kurtyigit, Serge Kotchourko, and\nDominik Schlechtweg. 2021. Effects of pre- and\npost-processing on type-based embeddings in lexi-\ncal semantic change detection. In Proceedings of\nEACL 2021, pages 125–137.\nYannis Kalantidis, Mert Bülent Sariyildiz, Noé Pion,\nPhilippe Weinzaepfel, and Diane Larlus. 2020. Hard\nnegative mixing for contrastive learning. InProceed-\nings of NeurIPS 2020.\nDavid Kamholz, Jonathan Pool, and Susan M. Colow-\nick. 2014. PanLex: Building a resource for panlin-\ngual lexical translation. In Proceedings of LREC\n2014, pages 3145–3150.\nBarbara Ann Kipfer. 2009. Roget’s 21st Century The-\nsaurus (3rd Edition). Philip Lief Group.\nAnne Lauscher, Ivan Vuli´c, Edoardo Maria Ponti, Anna\nKorhonen, and Goran Glavaš. 2020. Specializing\nunsupervised pretraining models for word-level se-\nmantic similarity. In Proceedings of COLING 2020,\npages 1371–1383.\nIra Leviant and Roi Reichart. 2015. Separated by\nan un-common language: Towards judgment lan-\nguage informed vector space modeling. CoRR,\nabs/1508.00106.\nYoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan\nPadnos, Or Sharir, Shai Shalev-Shwartz, Amnon\nShashua, and Yoav Shoham. 2020. SenseBERT:\nDriving some sense into BERT. In Proceedings of\nACL 2020, pages 4656–4667.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettle-\nmoyer. 2020. Pre-training via paraphrasing. CoRR,\nabs/2006.15020.\nFangyu Liu, Ivan Vuli ´c, Anna Korhonen, and Nigel\nCollier. 2021. Fast, effective and self-supervised:\nTransforming masked language models into uni-\nversal lexical and sentence encoders. CoRR,\nabs/2104.08027.\nQianchu Liu, Diana McCarthy, Ivan Vuli ´c, and Anna\nKorhonen. 2019. Investigating cross-lingual align-\nment methods for contextualized embeddings with\ntoken-level evaluation. In Proceedings of CoNLL\n2019, pages 33–43.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In Proceedings of\nICLR 2018.\nTomas Mikolov, Quoc V . Le, and Ilya Sutskever.\n2013a. Exploiting similarities among languages\nfor machine translation. arXiv preprint, CoRR ,\nabs/1309.4168.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013b. Distributed rep-\nresentations of words and phrases and their compo-\nsitionality. In Proceedings of NeurIPS 2013, pages\n3111–3119.\nNikola Mrkši´c, Diarmuid Ó Séaghdha, Tsung-Hsien\nWen, Blaise Thomson, and Steve Young. 2017. Neu-\nral belief tracker: Data-driven dialogue state track-\ning. In Proceedings of ACL 2017, pages 1777–1788.\nNikola Mrkši´c, Ivan Vuli´c, Diarmuid Ó Séaghdha, Ira\nLeviant, Roi Reichart, Milica Gaši ´c, Anna Korho-\nnen, and Steve Young. 2017. Semantic specialisa-\ntion of distributional word vector spaces using mono-\nlingual and cross-lingual constraints. Transactions\nof the ACL, 5:309–324.\nMasataka Ono, Makoto Miwa, and Yutaka Sasaki.\n2015. Word embedding-based antonym detection\nusing thesauri and distributional information. In\nProceedings of NAACL-HLT 2015, pages 984–989.\nGustavo Paetzold and Lucia Specia. 2017. A survey\non lexical simpliﬁcation. Journal of Artiﬁcial Intel-\nligence Research, 60:549–593.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2020. UNKs everywhere: Adapting mul-\ntilingual language models to new scripts. CoRR,\nabs/2012.15562.\nEdoardo Maria Ponti, Ivan Vuli´c, Goran Glavaš, Nikola\nMrkši´c, and Anna Korhonen. 2018. Adversar-\nial propagation and zero-shot cross-lingual transfer\nof word vector specialization. In Proceedings of\nEMNLP 2018, pages 282–293.\nEdoardo Maria Ponti, Ivan Vuli ´c, Goran Glavaš, Roi\nReichart, and Anna Korhonen. 2019. Cross-lingual\nsemantic specialization via lexical relation induction.\nIn Proceedings of EMNLP 2019, pages 2206–2217.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of EMNLP 2019 , pages\n3982–3992.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: what we know about\nhow BERT works. Transactions of the ACL, 8:842–\n866.\nSebastian Ruder, Ivan Vuli ´c, and Anders Søgaard.\n2019. A survey of cross-lingual embedding models.\nJournal of Artiﬁcial Intelligence Research , 65:569–\n631.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli ´c, Sebastian\nRuder, and Iryna Gurevych. 2021. How good is your\n5280\ntokenizer? On the monolingual performance of mul-\ntilingual language models. In Proceedings of ACL\n2021.\nHoracio Saggion. 2017. Automatic text simpliﬁcation.\nSynthesis Lectures on Human Language Technolo-\ngies, 10(1):1–137.\nDominik Schlechtweg, Barbara McGillivray, Simon\nHengchen, Haim Dubossarsky, and Nina Tahmasebi.\n2020. Semeval-2020 task 1: Unsupervised lexical\nsemantic change detection. In Proceedings of Se-\nmEval 2020, pages 1–23.\nRoy Schwartz, Roi Reichart, and Ari Rappoport. 2015.\nSymmetric pattern based word embeddings for im-\nproved word similarity prediction. In Proceedings\nof CoNLL 2015, pages 258–267.\nSamuel L. Smith, David H.P. Turban, Steven Hamblin,\nand Nils Y . Hammerla. 2017. Ofﬂine bilingual word\nvectors, orthogonal transformations and the inverted\nsoftmax. In Proceedings of ICLR 2017.\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli ´c.\n2018. On the limitations of unsupervised bilingual\ndictionary induction. In Proceedings of ACL 2018 ,\npages 778–788.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of ACL 2019, pages 4593–4601.\nSara Tonelli, Alessio Palmero Aprosio, and Francesca\nSaltori. 2016. SIMPITIKI: A simpliﬁcation corpus\nfor Italian. In Proceedings of CLiC-IT 2016.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBERT for Finnish. CoRR, abs/1912.07076.\nIvan Vuli´c, Simon Baker, Edoardo Maria Ponti, Ulla\nPetti, Ira Leviant, Kelly Wing, Olga Majewska, Eden\nBar, Matt Malone, Thierry Poibeau, Roi Reichart,\nand Anna Korhonen. 2020. Multi-Simlex: A large-\nscale evaluation of multilingual and cross-lingual\nlexical semantic similarity. Computational Linguis-\ntics.\nIvan Vuli´c, Goran Glavaš, Nikola Mrkši ´c, and Anna\nKorhonen. 2018. Post-specialisation: Retroﬁtting\nvectors of words unseen in lexical resources. In Pro-\nceedings of NAACL-HLT 2018, pages 516–527.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Probing\npretrained language models for lexical semantics. In\nProceedings of EMNLP 2020, pages 7222–7240.\nIvan Vuli´c, Roy Schwartz, Ari Rappoport, Roi Reichart,\nand Anna Korhonen. 2017. Automatic selection of\ncontext conﬁgurations for improved class-speciﬁc\nword representations. In Proceedings of CoNLL\n2017, pages 112–122.\nXun Wang, Xintong Han, Weilin Huang, Dengke Dong,\nand Matthew R. Scott. 2019. Multi-similarity loss\nwith general pair weighting for deep metric learning.\nIn Proceedings of CVPR 2019, pages 5022–5030.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2015. From paraphrase database to compo-\nsitional paraphrase model and back. Transactions of\nthe ACL, 3:345–358.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of EMNLP 2020: System\nDemonstrations, pages 38–45.\nYinfei Yang, Steve Yuan, Daniel Cer, Sheng-Yi Kong,\nNoah Constant, Petr Pilar, Heming Ge, Yun-hsuan\nSung, Brian Strope, and Ray Kurzweil. 2018. Learn-\ning semantic textual similarity from conversations.\nIn Proceedings of The 3rd Workshop on Representa-\ntion Learning for NLP, pages 164–174.\nJingwei Zhang, Jeremy Salwen, Michael Glass, and Al-\nﬁo Gliozzo. 2014. Word semantic representations\nusing bayesian probabilistic tensor factorization. In\nProceedings of EMNLP 2014, pages 1522–1531.\n5281\nLanguage URL\nEN https://huggingface.co/bert-base-uncased\nDE https://huggingface.co/bert-base-german-dbmdz-uncased\nES https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased\nFI https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1\nIT https://huggingface.co/dbmdz/bert-base-italian-xxl-uncased\nPL https://huggingface.co/dkleczek/bert-base-polish-uncased-v1\nRU https://huggingface.co/DeepPavlov/rubert-base-cased\nTR https://huggingface.co/dbmdz/bert-base-turkish-uncased\nMultilingual https://huggingface.co/bert-base-multilingual-uncased\nTable 6: URLs of the pretrained LMs used in our study, obtained via the HuggingFace repo (Wolf et al., 2020).\nMethod EN EN : SV ES FI RU DE : SL\nFASTTEXT .WIKI 44.2 25.8 45.0 58.7 35.8 41.3\nBERT-REG (all) 46.7 23.9 42.4 55.3 30.6 31.3\nBERT-REG (best) 51.8 28.9 44.2 61.5 30.7 34.6\nMNEG\n– 73.6 68.3 62.3 72.0 50.4 49.7\nMSIM\nk= 1 74.3 69.6 61.8 71.1 49.9 49.7\nk= 2 74.3 69.6 61.8 71.1 49.9 49.6\nk= 4 75.7 71.7 61.9 68.4 48.6 47.9\nk= 8 75.9 72.3 62.0 66.4 49.9 46.5\nSOFTMAX (binary)\nk= 1 64.3 58.8 58.8 62.4 44.6 43.7\nk= 2 67.9 61.4 60.1 67.6 46.6 45.9\nk= 4 70.2 64.9 60.6 69.6 46.7 47.0\nk= 8 71.3 67.2 61.4 70.2 46.7 47.6\nSOFTMAX (ternary)\nk= 1 67.8 61.7 59.4 66.2 38.8 45.3\nk= 2 68.8 62.6 60.1 66.7 42.4 46.6\nk= 4 70.6 65.8 59.7 67.8 45.3 47.6\nk= 8 71.6 67.8 60.9 68.5 45.0 47.0\nTable 7: A summary of results in the lexical semantic similarity (LSIM) task (Spearman’s ρcorrelation scores),\nalso showing the dependence on the number of negative examples per positive example: k. The scores for EN, ES,\nFI, and RU are reported on the Multi-SimLex lexical similarity benchmark (Vuli ´c et al., 2020) (1,888 word pairs).\nThe scores for DE, not represented in Multi-SimLex, are calculated on a smaller benchmark: German SimLex-\n999 (Hill et al., 2015; Leviant and Reichart, 2015) (SL; 999 word pairs) For EN, we also report the scores on the\nverb similarity dataset SimVerb-3500 (Gerz et al., 2016) (SV). All L EXFIT-based WEs have been induced from\n“lexically ﬁne-tuned” LMs, relying on the standard setup described in §3, and relying on lexical constraints also\nsummarized in §3. All results with L EXFIT variants are obtained relying on the best-performing conﬁguration\nfor extracting word representations from the comparative study of Vuli ´c et al. (2020). BERT-REG denotes the\nextraction of word representations (again with the best strategy from prior work) from the regular underlying\nBERT models, which were not further “LEXFIT-ed”: (all) layerwise averaging over all Transformer layers; (best)\nthe highest results reported by Vuli ´c et al. (2020), often achieved by excluding several highest layers from the\nlayerwise averaging. The highest scores per column are in bold; the second best result per column is underlined.\n1 2 3 4 5 6 7 8 9 10\nEpoch\n50\n55\n60\n65\n70\n75Spearman ρ correlation\nEN EN (SV) ES FI IT (SL)\n(a) LSIM\n1 2 3 4 5 6 7 8 9 10\nEpoch\n40\n45\n50\n55\n60\n65Mean Reciprocal Rank (MRR)\nEN–ES EN–FI EN–IT FI–IT (b) BLI\n1 2 3 4 5 6 7 8 9 10\nEpoch\n60\n65\n70\n75Micro F1\nEN ES (c) RELP\nFigure 5: Impact of L EXFIT ﬁne-tuning duration (i.e., the number of ﬁne-tuning epochs) in three lexical tasks\n(LSIM, BLI, RELP). We report a subset of results with a selection of languages and language pairs, relying on the\nMSIM (k= 1) LEXFIT ﬁne-tuning variant. Similar trends are observed with other L EXFIT variants.\n5282\n(a) Training dictionary: 5,000 word translation pairs\nMethod EN–DE EN –TR EN –FI EN –RU DE –TR DE –FI DE –RU TR –FI TR –RU FI –RU avg\nFASTTEXT.WIKI 61.0 43.3 48.8 52.2 35.8 43.5 46.9 35.8 36.4 43.9 44.8\nBERT-REG (all) 44.6 37.9 47.1 47.3 32.3 39.5 41.2 35.2 31.9 38.7 39.6\nMNEG\n– 58.1 46.2 57.7 54.0 36.2 46.1 46.7 39.6 36.7 42.4 46.4\nMSIM\nk= 1 58.9 45.9 57.6 53.7 37.1 46.4 46.7 39.4 37.4 44.2 46.7\nk= 2 57.2 44.4 56.7 52.8 35.7 44.7 46.1 39.3 37.4 42.2 45.7\nk= 4 57.0 43.6 55.2 51.5 35.5 43.6 44.8 38.0 35.1 39.3 44.4\nk= 8 55.4 44.0 53.0 49.1 34.0 41.8 42.2 36.5 32.0 37.5 42.6\nSOFTMAX (binary)\nk= 1 57.9 45.3 53.8 53.6 35.9 44.3 43.5 38.4 36.0 42.8 45.2\nk= 2 55.8 44.6 55.4 51.9 34.7 43.8 41.9 39.1 34.6 40.0 44.2\nk= 4 55.8 43.8 54.9 51.4 34.6 42.8 39.9 37.9 33.3 39.0 43.3\nk= 8 54.2 43.1 54.4 50.2 33.3 42.0 39.7 36.8 32.9 38.7 42.5\nSOFTMAX (ternary)\nk= 1 57.1 44.9 54.8 52.7 35.2 44.0 44.6 38.4 34.9 41.1 44.8\nk= 2 55.7 45.2 54.4 53.2 34.1 43.6 42.6 38.4 34.5 40.7 44.2\nk= 4 55.5 44.7 55.1 52.6 34.0 42.8 40.2 38.6 33.4 40.7 43.8\nk= 8 54.9 44.2 53.3 51.5 33.3 41.3 38.7 37.2 32.9 37.8 42.5\n(b) Training dictionary: 1,000 word translation pairs\nMethod EN–DE EN –TR EN –FI EN –RU DE –TR DE –FI DE –RU TR –FI TR –RU FI –RU avg\nFASTTEXT.WIKI 53.9 31.7 35.4 39.0 23.0 31.5 37.8 21.4 22.2 29.6 32.6\nBERT-REG (all) 26.4 20.6 25.8 25.4 17.4 24.6 23.4 20.4 15.6 21.4 22.1\nMNEG\n– 55.2 34.1 44.8 40.3 25.9 33.9 31.7 29.2 22.3 30.1 34.8\nMSIM\nk= 1 54.3 33.2 45.1 39.3 26.0 33.9 31.4 29.1 23.8 30.8 34.7\nk= 2 54.3 32.0 43.3 38.8 24.6 32.7 30.0 28.4 22.1 27.1 33.3\nk= 4 53.0 31.8 41.6 38.1 24.3 30.9 27.4 25.2 20.1 26.1 31.9\nk= 8 51.4 30.6 40.1 36.4 22.7 28.7 24.9 24.2 17.8 23.7 30.1\nSOFTMAX (binary)\nk= 1 54.1 32.0 40.4 39.7 25.6 32.1 31.6 27.2 23.1 29.4 33.5\nk= 2 52.3 32.3 43.7 39.6 25.5 33.4 31.3 28.9 22.8 27.8 33.8\nk= 4 52.7 31.9 42.4 37.4 25.5 32.2 29.6 27.5 21.0 26.5 32.7\nk= 8 52.2 31.1 42.1 38.0 23.5 30.0 28.4 25.9 20.8 25.8 31.8\nSOFTMAX (ternary)\nk= 1 53.5 32.0 42.8 38.7 24.2 32.0 31.0 27.6 22.0 28.6 33.2\nk= 2 52.7 32.7 43.0 38.0 23.9 30.4 29.6 27.1 21.8 27.5 32.7\nk= 4 52.9 31.7 42.8 37.0 22.9 32.0 29.8 26.9 22.3 27.9 32.6\nk= 8 51.8 31.6 41.2 36.8 23.1 29.8 27.8 26.1 20.2 25.3 31.4\nTable 8: Results in the BLI task across different language pairs and dual-encoder lexical ﬁne-tuning (L EXFIT)\nobjectives (MNEG , MSIM , SOFTMAX ). The size of the training dictionary is (a) 5,000 or (b) 1,000 word translation\npairs. MRR scores reported; avg refers to the average score across all 10 language pairs. All results with L EXFIT\nvariants are obtained relying on the best-performing conﬁguration for extracting word representations from the\ncomparative study of Vuli´c et al. (2020). BERT-REG denotes the extraction of word representations (again with the\nbest strategy from prior work) from the regular underlying BERT models, which were not further “L EXFIT-ed”:\n(all) layerwise averaging over all Transformer layers. The highest scores per column for each training dictionary\nsize are in bold; the second best result is underlined.\n5283\nMethod EN DE ES FI\nRANDOM .XAVIER 47.3±0.3 51.2±0.8 49.7±0.9 51.8±0.5\nFASTTEXT .WIKI 66.0±0.8 60.1±0.7 62.2±1.6 68.2±0.3\nBERT-REG (all) 71.4±1.2 67.3±0.3 65.1±1.1 69.6±0.6\nBERT-REG (best) 71.8±0.2 67.9±0.8 65.5±1.2 69.9±0.5\nMNEG\n– 74.1±1.1 69.7±1.0 67.8±0.3 71.3±1.5\nMSIM\nk= 1 74 .3±1.3 69.0±0.6 68.6±0.7 72.2±0.4\nk= 2 73 .8±0.8 68.6±1.2 68.4±0.5 72.3±0.3\nk= 4 73 .5±1.0 68.8±1.2 67.1±1.1 72.0±0.9\nk= 8 72 .1±0.9 68.9±0.6 67.6±1.3 71.2±1.3\nSOFTMAX (binary)\nk= 1 74 .0±1.6 68.4±0.7 67.4±0.3 71.5±0.6\nk= 2 73 .8±1.0 69.4±0.5 67.4±0.8 71.2±0.9\nk= 4 73 .9±1.0 69.4±0.9 67.2±1.1 72.7±0.7\nk= 8 73 .2±1.0 68.2±1.1 67.8±1.1 71.4±1.1\nSOFTMAX (ternary)\nk= 1 75 .5±0.5 70.3±0.7 70.3±0.6 73.2±1.2\nk= 2 75.7±0.8 68.8±0.8 69.8±0.8 73.2±0.5\nk= 4 74 .4±0.8 69.9±0.5 69.9±0.5 72.2±0.6\nk= 8 74 .0±0.2 68.1±0.9 68.1±0.9 72.3±0.4\nTable 9: A summary of results in the relation prediction (RELP) task, also showing the dependence on the number\nof negative examples per positive example:k. Micro-averaged F1 scores, obtained as averages over 5 experimental\nruns for each input word vector space; standard deviation is also reported in the subscript. RANDOM .XAVIER\nare 768-dimensional vectors for the same vocabularies, randomly initialized via Xavier initialization (Glorot and\nBengio, 2010). The highest scores per column are in bold, the second best is underlined.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6362966299057007
    },
    {
      "name": "Göran",
      "score": 0.5647936463356018
    },
    {
      "name": "Computational linguistics",
      "score": 0.5245521068572998
    },
    {
      "name": "Natural language processing",
      "score": 0.48615118861198425
    },
    {
      "name": "Joint (building)",
      "score": 0.4842783808708191
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.48415738344192505
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45714902877807617
    },
    {
      "name": "Linguistics",
      "score": 0.4367435574531555
    },
    {
      "name": "Association (psychology)",
      "score": 0.42131301760673523
    },
    {
      "name": "Speech recognition",
      "score": 0.32832813262939453
    },
    {
      "name": "Art",
      "score": 0.16133186221122742
    },
    {
      "name": "Engineering",
      "score": 0.1599070131778717
    },
    {
      "name": "Humanities",
      "score": 0.15508508682250977
    },
    {
      "name": "Psychology",
      "score": 0.15415650606155396
    },
    {
      "name": "Philosophy",
      "score": 0.09091037511825562
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I5023651",
      "name": "McGill University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210164802",
      "name": "Mila - Quebec Artificial Intelligence Institute",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I177802217",
      "name": "University of Mannheim",
      "country": "DE"
    }
  ]
}