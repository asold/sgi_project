{
    "title": "Training Trajectories of Language Models Across Scales",
    "url": "https://openalex.org/W4385570738",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5003465314",
            "name": "Mengzhou Xia",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A5023341622",
            "name": "Mikel Artetxe",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5110252779",
            "name": "Chunting Zhou",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5062266757",
            "name": "Xi Victoria Lin",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5075564427",
            "name": "Ramakanth Pasunuru",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5051064208",
            "name": "Danqi Chen",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A5067919401",
            "name": "Luke Zettlemoyer",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5091317839",
            "name": "Veselin Stoyanov",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3199241049",
        "https://openalex.org/W4313304472",
        "https://openalex.org/W4367628401",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4283157303",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4287332927",
        "https://openalex.org/W4288087322",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W4226434736",
        "https://openalex.org/W4307418160",
        "https://openalex.org/W4385572950",
        "https://openalex.org/W4385574029",
        "https://openalex.org/W4386566638",
        "https://openalex.org/W4225156065",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4297412003",
        "https://openalex.org/W4385572722",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W4385573102",
        "https://openalex.org/W4206410067",
        "https://openalex.org/W4385567093",
        "https://openalex.org/W3177765786",
        "https://openalex.org/W4394664678",
        "https://openalex.org/W4389518729",
        "https://openalex.org/W4362655426",
        "https://openalex.org/W3212496002",
        "https://openalex.org/W4287891150",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4385574005",
        "https://openalex.org/W4385571645",
        "https://openalex.org/W4281481109",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W4288024261",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4226146865",
        "https://openalex.org/W3199373335",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W3092185277",
        "https://openalex.org/W4297833882"
    ],
    "abstract": "Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, Veselin Stoyanov. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 13711–13738\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nTraining Trajectories of Language Models Across Scales\nMengzhou Xia1, Mikel Artetxe2, Chunting Zhou2, Xi Victoria Lin2,\nRamakanth Pasunuru2, Danqi Chen1, Luke Zettlemoyer2, Ves Stoyanov2\n1Princeton University 2Meta AI\nmengzhou@princeton.edu\nAbstract\nScaling up language models has led to unprece-\ndented performance gains, but little is under-\nstood about how the training dynamics change\nas models get larger. How do language mod-\nels of different sizes learn during pre-training?\nWhy do larger language models demonstrate\nmore desirable behaviors? In this paper, we\nanalyze the intermediate training checkpoints\nof differently sized OPT models (Zhang et al.,\n2022)—from 125 M to 175 B parameters—on\nnext-token prediction, sequence-level genera-\ntion and downstream tasks. We ﬁnd that 1) at\na given perplexity and independent of model\nsizes, a similar subset of training tokens see\nthe most signiﬁcant reduction in loss, with the\nrest stagnating or showing double-descent be-\nhavior (Nakkiran et al., 2020); 2) early in train-\ning, all models learn to reduce the perplexity\nof grammatical sequences that contain halluci-\nnations, with small models halting at this sub-\noptimal distribution and larger ones eventually\nlearning to assign these sequences lower prob-\nabilities; and 3) perplexity is a strong predic-\ntor of in-context learning performance on 74\nmultiple-choice tasks from BIG-Bench, and\nthis holds independently of the model size.\nTogether, these results show that perplexity\nis more predictive of model behaviors than\nmodel size or training computation.1\n1 Introduction\nScaling up language models has been shown to im-\nprove language modeling perplexity (Kaplan et al.,\n2020; Hernandez et al., 2022) as well as zero- or\nfew-shot end task accuracies (Brown et al., 2020;\nRae et al., 2021; Chowdhery et al., 2022; Zhang\net al., 2022). However, relatively little is under-\nstood about why or how this happens. How do\nthe training dynamics differ as models get larger?\nWhat do language models of different sizes learn\n1Code is publicly available athttps://github.com/\nxiamengzhou/training_trajectory_analysis.\nduring pre-training in terms of both generating texts\nand solving end tasks?\nWe attempt to make progress to answer these\nquestions by studying the training trajectories of\ndifferently-sized OPT models (Zhang et al., 2022)\nthrough analyzing their intermediate checkpoints.\nIn contrast to prior work, which studies the trajecto-\nries of small models with up to 300M parameters\n(Liu et al., 2021; Choshen et al., 2022; Blevins\net al., 2022) or focuses on the language modeling\nobjective alone (Kaplan et al., 2020; Hernandez\net al., 2021, 2022), we are the ﬁrst to comprehen-\nsively study the training trajectories of large-scale\nautoregressive language models with up to 175 B\nparameters across a wide range of settings.\nRepeatedly across training and different model\nscales, we analyze three aspects of model perfor-\nmance: (i) next-token prediction on subsets of to-\nkens (ii) sequence-level generation and (iii) down-\nstream task performance. We use perplexity, which\nis closely tied to language model evaluation, as the\nmajor metric throughout the study.\nFor next-token prediction(§3), we study the\ntrajectory by categorizing each token’s prediction\nas stagnated, upward or downward according to\nits perplexity trend as training progresses. We ﬁnd\neach category comprising a signiﬁcant number of\ntokens: while a signiﬁcant number of tokens’ per-\nplexity stagnate, a subset of tokens with an increas-\ning perplexity in smaller models exhibit a double-\ndescent trend (Nakkiran et al., 2020) where perplex-\nity increases and then decreases in larger models.\nThese behaviors primarily emerge at a similar vali-\ndation perplexity across model scales.\nFor sequence-level generation(§4), we study\nthe distribution shift at a document level (50-500 to-\nkens) by decoding sequences that small/large mod-\nels favor more than the other. Human texts present\nexpected scaling patterns in that they are best mod-\neled by larger (or longer trained) models. However,\nto our surprise, large models are better at modeling\n13711\n1018 1019 1020 1021 1022 1023\nFLOPs\n10\n20\n30\n40\n50Validation PPL\n125m\n1.3b\n6.7b\n13b\n30b\n175b\nFigure 1: Validation perplexity (PPL) of OPT models\nagainst training FLOPs. Our work suggests that mod-\nels with comparable perplexity levels during training\nexhibit similar predictions, regardless of their scales.\nless human-like texts which contain synthetic noise\nand factually incorrect prompts. We propose an\napproach to decoding texts that small models favor\nmore than large models from an interpolated dis-\ntribution induced by combining signals from both\nmodels and ﬁnd them grammatical but hallucinat-\ning.2 All models go through a stage during training\nwhere the perplexity for such texts decreases; small\nmodels halt at this suboptimal distribution, while\nlarger models escape it by eventually increasing\nthe perplexity of these unnatural texts.\nWe further connect language modeling perplex-\nity to downstream tasks(§5). By evaluating more\nthan 70 multiple-choice tasks in BIG-Bench (Sri-\nvastava et al., 2022), we ﬁnd that language mod-\neling perplexity correlates well with few-shot in-\ncontext learning performance along the trajectory,\nregardless of model sizes. The gradual divergence\nof likelihood between correct and incorrect options\nleads to improvements in in-context learning.\nOur work presents a comprehensive study of\ntraining trajectories of language models trained\nwith similar procedures, e.g., OPT. We conclude\nthat language models learn the same phenomena in\nthe same order across different model sizes. The\noverall model perplexity is a composite measure of\nwhich language phenomena have been learned.\n2 Experimental Settings\nModels. Unless otherwise indicated, all of our\nexperiments use OPT (Zhang et al., 2022), a col-\nlection of open-source autoregressive language\n2Concurrent to our work, Li et al. (2022) propose a simi-\nlar contrastive decoding approach for a different application.\nRefer to Appendix C.2 for more details.\nmodels. OPT models serve as a good ﬁt for this\nstudy due to their controlled pre-training proce-\ndures across all model sizes. In particular, all the\nmodels share the same tokenization and are trained\non the same training data, covering a total of 300B\ntokens (180B unique). Note that different-sized\nmodels differ in batch sizes and total number of\nsteps.3 We collect intermediate checkpoints from\nthe authors and perform evaluations of these check-\npoints across six different sizes: 125 M, 1.3 B, 6.7 B,\n13B, 30B, and 175 B.\nValidation perplexity. Throughout this paper,\nwe use Validation Perplexity (Valid PPL)to refer\nto the autoregressive language modeling perplex-\nity measured on the entire validation set. We use\nthe original OPT validation set, a held-out subset\nof the training corpus that covers a wide range of\ndomains, such as books, news, and subtitles. We\nplot the trajectory of validation perplexity in Fig-\nure 1, which follows a similar power-law pattern\nobserved in previous scaling work (Kaplan et al.,\n2020; Hoffmann et al., 2022).\nMethodology. We aim to understand how mod-\nels of different sizes behave throughout training as\na function of computing (FLOPs)4 and validation\nperplexity. Throughout the paper, we use different\nmeasurements to characterize model behavior and\nplot them against these two metrics.\n3 Next-Token Prediction\nAutoregressive language models are trained to pre-\ndict the next token given a context. Figure 1\nshows that validation perplexity, aggregated over\nall positions, gradually declines as training pro-\ngresses. However, it is not clear if all token in-\nstances evolve similarly to the aggregated measure-\nment. In this section, we study the trajectory of\nnext-token predictions, dividing them into three\ncategories—stagnated, upward trend, or downward\ntrend—to understand how language models gradu-\nally learn new language phenomena.\n3.1 Methodology\nWe evaluate intermediate checkpoints on a subset\nof validation data. 5 For each context-token pair\n(c,t), we obtain a series of perplexities PPL m1 (t|\n3See Appendix A for more details of model checkpoints.\n4We estimate the number of FLOPs of language models\nfollowing Chowdhery et al. (2022).\n5More dataset details are in Appendix B.1.\n13712\n10% 40% 70%\nPercentage of Training\n0%\n5%\n10%\n15%\n20%\n25%Percentage of T okens\nT okens that PPL Stagnates\n125m\n1.3b\n6.7b\n13b\n30b\n175b\n10% 40% 70%\nPercentage of Training\n0%\n5%\n10%\n15%Percentage of T okens\nT okens that PPL Increases\n10% 40% 70%\nPercentage of Training\n0%\n10%\n20%\n30%\n40%\n50%Percentage of T okens\nT okens that PPL Decreases\nFigure 2: Percentage of predictions where perplexity stagnates (left), follows an upward trend (middle) and an\ndownward trend (right). X-axis denotes that the trend is estimated after p% percentage of training.\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\n125m 1.3b 6.7b 13b 30b 175b\n1018 1019 1020 1021 1022 1023\nFLOPs\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6PPL\n6.2%\n8.8%\n9.0%\n9.6%\n11.3%\n6.8%\nAfter 10% Training of Each Model\n1018 1019 1020 1021 1022 1023\nFLOPs\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6PPL\n8.8%\nAfter 10% Training of 1.3b Model\n1018 1019 1020 1021 1022 1023\nFLOPs\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0PPL\n 6.8%\nAfter 10% Training of 175b Model\nFigure 3: Perplexity of stagnated tokens. Left: different models are evaluated on different subsets of tokens\nselected after 10% of training of individual models (all showing a stagnated trend after 10%). Middle/right: all\nmodels are evaluated on the same set of tokens , selected after 10% of training of the 1.3 B model and the 175 B\nmodel respectively. The number next to the dashed line denotes the percentage of the selected tokens out of all\ntokens. Stagnated tokens selected by a smaller model (1.3 B) are stagnated in larger models. Stagnated tokens\nselected by a larger model (175 B) present a downward trend in perplexity in smaller models.\nc),PPL m2 (t|c),..., PPL mn (t|c) for checkpoints\nm1,m2,...,m n. We use linear regression to esti-\nmate the slope of a normalized series to roughly\ncapture its trend. Starting from any intermediate\ncheckpoint after p% of training (assuming that it\nis the j-th checkpoint) to the end checkpoint mn,\n∀i ∈[j,n], we ﬁt the following function to learn\nthe parameters αand βfor each series:\nPPL mi (t|c)\nPPL mj (t|c) = α+ β·(i−j). (1)\nNote that different starting points might result in\ndifferent trend estimations. We categorize the\ntrends as follows based on βand its signiﬁcance:\nUpward trend. If β > 0 and its p-value is <\n0.05, we consider that the series follows an upward\ntrend (forgetting).\nDownward trend. If β < −0 and its p-value\nis < 0.05, we consider that the series follows a\ndownward trend (still learning).\nStagnated trend. If a series does not follow\nan upward or downward trend, and the start and\nend values fall in a restricted interval, that is,\n0.95 ≤ PPLmj /PPL AVG ≤ 1.05 and 0.95 ≤\nPPLmn /PPLavg ≤ 1.05, where PPLavg =\nexp( 1\nn−j+1\n∑\ni log PPLmi ), we consider the series\nto be stagnated (already learned).\nWe design the criteria to roughly capture the\ntrend of the perplexity series of each next-token\nprediction. Under these criteria, a stagnated se-\nries from an earlier checkpoint would continue to\nstagnate, and a series that follows an upward or\ndownward trend earlier might turn stagnated after-\nwards. The criteria do not necessarily cover all the\nseries—wavy series with a large variance do not\nfall within any category and are eliminated. For the\nrest of the section, for simplicity, we use tokens to\nrefer to context-token pairs.\n3.2 Analysis\nPercentage of tokens. We show the percentage\nof tokens that follow each trend in Figure 2. Over-\nall, the percentage of stagnated tokens increases\nand the percentage of the other two types of tokens\ndecreases, indicating that more tokens get to be\nlearned and fewer tokens are still learning or, more\n13713\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\n125m 1.3b 6.7b 13b 30b 175b\n1018 1019 1020 1021 1022 1023\nFLOPs\n10\n20\n30\n40\n50\n60\n70PPL\n10.6%\n9.4%\n9.6%8.3%7.8% 11.1%\nAfter 10% Training of Each Model\nDouble Descent\n1018 1019 1020 1021 1022 1023\nFLOPs\n10\n20\n30\n40PPL\n11.1%\nAfter 10% Training of 175b Model\nFigure 4: Perplexity of upward-trend tokens. Left: different models are evaluated on different subsets of tokens\nselected after 10% of training of individual models (all showing a downward-then-upward trend). Middle/right:\nall models are evaluated on the same set of tokens, selected after 10% of training of the 1.3 B model and the 175 B\nmodel respectively. The number next to the dashed line denotes the percentage of the selected tokens out of all\ntokens. Tokens selected by a smaller model (1.3 B) present a double descent-like trend in larger models. Tokens\nselected by a larger model (175 B) present a downward trend in the smaller models.\nsurprisingly, forgetting as training progresses. 6\nStagnated tokens. We select stagnated tokens\nstarting from 10% of training for a particular model\nand analyze the trajectory of these same tokens in\nother models. As shown in Figure 3 (middle), we\nobserve that stagnated tokens after 10% of training\nin a small model ( 1.3 B) also stagnate in larger\nmodels. However, the stagnated tokens selected\nby a large model ( 175 B) still show a downward\ntrend in smaller models. This suggests that larger\nmodels’ stagnated tokens are roughly a superset of\nsmaller models. On manual inspection, stagnated\ntokens are primarily non-content words such as\nprepositions, determiners, and punctuations.\nUpward trend tokens. Similarly, we present the\nperplexity of upward trend tokens in Figure 4. The\nleftmost ﬁgure shows that such a phenomemon ex-\nists for all the models. For tokens that present\nan upward trend after 10% training of a small\nmodel (1.3 B), we observe a stepwise double de-\nscent (Nakkiran et al., 2020) trend in larger models’\ntrajectories, where the perplexity ﬁrst increases and\nthen decreases. We are the ﬁrst to observe this phe-\nnomenon during language model training, and it\nsuggests that larger models, with more computation\nand a larger capacity, ﬁrst overﬁt to this subset of to-\nkens and further generalize better for them. For the\ntokens identiﬁed after 20% training of the largest\nmodel (175 B), the upward trend appears only at the\nend of training for the 13B and 30B models. We\nﬁnd it hard to characterize these tokens considering\ntheir contexts,7 but the synergy across model sizes\n6Only around 60% tokens are captured by our criteria and\nplease ﬁnd more details on other tokens in Appendix B.2.\n7More details are in Appendix B.3.\nstrongly suggests that consistent types of learning\nare triggered at particular computation levels for\nmodels across scales. 8\nSummary. In conclusion, large models ﬁrst repli-\ncate small models’ behavior on the same subset of\ntokens, and further unlock exclusive phenomena\nwhen fueled with more computation. In Appendix\nB.5, we ﬁnd that trajectories of differently-sized\nmodels largely overlap when plotting against vali-\ndation perplexity, indicating that they make similar\npredictions at a similar perplexity.9\n4 Sequence-Level Generation\nIn this section, we extend the analysis from token-\nlevel predictions to entire sequences, up to 50-500\ntokens. Larger language models consistently obtain\na better perplexity in modeling human texts such\nas Wikipedia, with the perplexity decreasing as\nthe model size and training computation increases\n(Figure 1). Autoregressive language models are\nprobabilistic models of sequences that can generate\nstrings of text. If larger models assign a higher\nprobability to virtually all human-authored texts,\nwhat sequences do smaller models favor? We aim\nto ﬁrst characterize these sequences and further\nanalyze learning behavior on them to understand\nhow models of different sizes evolve into their ﬁnal\ndistributions. In what follows, we ﬁrst show that it\nis difﬁcult to manually design such sequences, as\nlarge models can also favor corrupted or factually\nincorrect texts (§4.1). We then devise a decoding\nalgorithm to automatically generate sequences fa-\n8We explore the upward trends with different starting\npoints and model scales in Appendix B.4.\n9Please ﬁnd more discussions in Appendix B.5.\n13714\n125M 1.3B 6.7B 13B 30B\nModel Size\n0\n2000\n4000\n6000\n8000\n10000PPL\n258.2 182.4 151.5 145.0 138.2\n1100.6 893.3 797.2 785.0 748.5\n2991.5 2513.1 2354.9 2357.2 2278.7\n6234.2\n4979.7 4559.9 4665.7 4429.5\n10968.7\n7747.1\n6597.1 6772.3 6503.3\n20%\n40%\n60%\n80%\n100%\n125M 1.3B 6.7B 13B 30B 175B\nModel Size\n1.5\n2.0\n2.5\n1.84\n1.35\n1.24 1.24 1.24 1.19\n2.56\n1.84\n1.65 1.63 1.65\n1.58\ncorrect options\nincorrect options\nFigure 5: Scaling trends for corrupted datasets (p% ran-\ndom tokens) and options in multiple choice tasks. The\nperplexity on corrupted texts and incorrect options de-\ncrease as model size increases, even for sequences con-\nsisting of completely random tokens (p= 100).\nvored by smaller models (§4.2), and conclude with\nan analysis of such sequences (§4.3).\n4.1 Manual Design\nCorrupted datasets. We hypothesize that inject-\ning noise into human texts might reverse the scal-\ning trend (i.e., perplexity on corrupted texts might\nincrease as model size increases). To test this hy-\npothesis, we replace 20%, 40%, 60%, 80%, and\n100% of the subwords in each sequence with ran-\ndom subwords. We evaluate corrupted datasets on\nthe ﬁnal model checkpoints and report the perplex-\nity in Figure 5 (left). Contrary to our hypothesis,\ndownward trends largely retain across all noise\nlevels, even when the entire sequence consists of\nrandom tokens (100%). This can be explained by\nthe copy-and-complete interpretation for in-context\nlearning described in Olsson et al. (2022): larger\nmodels fare better at making predictions to follow\nthe context distribution than smaller models, even\nwhen the context is pure noise.10\nIncorrect options of multiple-choice tasks.We\nnext hypothesize that the perplexity of incorrect\noptions for multiple-choice tasks might present an\ninverse scaling trend, as they are generally factually\nwrong. We present the perplexity of correct and\nincorrect options of 74 multiple-choice tasks from\nthe BIG-Bench dataset in Figure 5.11 However, we\nﬁnd that the perplexity of correct and incorrect op-\ntions decreases as the size of the model increases.12\nIn summary, our initial attempt failed—we are\nnot able to manually construct texts that are more\nprobable in smaller models than larger models.\n10Please ﬁnd details on corrupted datasets in Appendix C.1.\n11Details on task selection are in Appendix D.1.\n12To clarify, we are not discussing task accuracy here, but\nthe scaling trend of correct and incorrect options. Find exam-\nples of correct and incorrect prompts in Table 8.\n4.2 Methodology\nTo continue our search for such texts, we next de-\nvise a decoding approach that combines signals\nfrom two models and generates texts based on the\ninterpolation of their distributions:\np′\ni = λ1 ·ps(xi|x<i) +λ2 ·pl(xi|x<i); (2)\nwhere ps and pl are the next-token distributions\nfrom the small and large models, respectively, and\nλ1,λ2 ∈[−1,1]. A set of λ1 and λ2 denotes a\nspeciﬁc conﬁguration. When λ1 = 0,λ2 = 1, it\nis simply decoding with the large model; when\nλ1 = 1,λ2 = −1, the decoding process favors the\nsmall model’s prediction and suppresses the large\nmodel’s prediction. This is the conﬁguration that\ndecodes sequences that small models have a lower\nperplexity on than large models.\nWe further remove tokens that have a negative\nscore, and renormalize the distribution p′\ni to ensure\nthat the sum of the probabilities of all tokens is 1:\np(xi|x<i) = 1 (p′\ni >0) ·p′\ni∑1 (p′\ni >0) ·p′\ni\n. (3)\nGeneration process. We decode sequences with\ntwo models, 125 M and 30B, using different con-\nﬁgurations of λ1 and λ2. We take the ﬁrst 5 tokens\nof a subset of validation documents as prompts\nand generate 50 tokens conditioned on them.13 We\ntry greedy search and nucleus sampling (Holtzman\net al., 2019) for decoding and evaluate the texts\ndecoded from each conﬁguration as follows: 1)\nwe measure the text perplexity at ﬁnal checkpoints\nof different-sized models to understand its scal-\ning trend; 2) we measure the text perplexity at all\nintermediate checkpoints to understand how the\nperplexity evolves as training progresses.\n4.3 Analysis\nInverse scaling. As shown in Figure 6 (row 1),\nwe conﬁrm that the perplexity of texts generated\nwith the ps −pt conﬁguration presents an inverse\nscaling trend—perplexity increases as model size\nincreases (column 1, 5). Other conﬁgurations either\nonly show a modest upward trend (ps), or a normal\ndownward trend (pl and pl −ps). Even though mod-\nels of intermediate sizes (1.3 B, 6.7 B, 13B) are not\ninvolved in decoding, the scaling trend holds sys-\ntematically across all model sizes. To further verify\n13We also generate longer sequences up to 100 and 500\nwords and the conclusions hold similarly. More discussions\ncan be found in Appendix C.5.\n13715\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\n125M final ckpt\n125M intermediate ckpts\n1.3B final ckpt\n1.3B intermediate ckpts\n6.7B final ckpt\n6.7B intermediate ckpts\n13B final ckpt\n13B intermediate ckpts\n30B final ckpt\n30B intermediate ckpts\n50\n100PPL\nModel Size\nNucleus Sampling\n10\n20PPL\nModel Size\nGreedy Search\n50\n100PPL\nFLOPs (log scale)\n20\n40PPL\nFLOPs (log scale)\nps pl\n ps ps + pl pl pl ps\n0\n50mauve\n27.7\n9.7 8.8\n27.4\n41.8\nps pl\n ps ps + pl pl pl ps\n0\n50mauve\n27.7\n9.7 8.8\n27.4\n41.8\nFigure 6: Perplexity of texts (generated withλ1ps +λ2pl) evaluated with differently-sized ﬁnal model checkpoints\n(ﬁrst row) and perplexity trajectory evaluated over intermediate checkpoints against FLOPs (second row). Each\ncolumn denotes one conﬁguration with different λ1 and λ2. Note that all the texts are generated by combining\nsignals only from 125 M and 30 B models, but are evaluated over all the model scales.\n125m 1.3b 2.7b\nModel Size\n20\n40\n60\n80\n100PPL\nGreedy Search Generations\n125m 1.3b 2.7b\nModel Size\n10\n20\nNucleus Sampling Generations\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\nps pl\n ps pl pl ps\nFigure 7: Evaluations using GPT Neo models on texts\ngenerated with OPT 125 M and OPT 30 B models. The\nperplexity follows a similar trend as OPT, suggesting a\nsystematic distribution shift between model sizes.\nthe universality of the phenomenon in other fami-\nlies of language models, we evaluate the generated\ntexts with ﬁnal GPT Neo checkpoints (Black et al.,\n2021), which were trained on the Pile dataset (Gao\net al., 2020). As shown in Figure 7, the perplex-\nity trend aligns with OPT models. This conﬁrms\nthat the texts generated with our approach are not\na result of model or data artifacts, but embody uni-\nversal properties exhibiting a similar scaling trend\nin other model families.\nPerplexity trajectory of generated sequences.\nIn the second row of Figure 6, we present the\nperplexity trajectory of texts generated with dif-\nferent conﬁgurations. We observe that texts gen-\nerated based on ps −pl and, to a less extent, ps,\nlargely differ from the other conﬁgurations: 125 M\ncheckpoints present a downward trend, while other\ncheckpoints present an upward trend. This might\nsuggest that differently-sized models optimize in\ndifferent directions for phenomena speciﬁc to these\ntexts. However, taking a closer look, we observe\nthat the 1.3 B model also shows a downward trend\nat the beginning, which turns upward afterwards.\nThis indicates that all models improve the perplex-\nity of these texts at ﬁrst but, with more training\nFLOPs, larger models shift away from this speciﬁc\ndistribution where the 125 M model stalls. In Ap-\npendix C.7, we further show that perplexity of the\nsequences decoded by contrasting the two models\n(ps −pl and pl −ps) are less aligned with validation\nperplexity as other conﬁgurations.\nGenerated examples. Table 1 presents examples\ngenerated with different conﬁgurations. We ﬁnd\nthat the generations from ps −pl are grammatically\ncorrect and carry actual meanings both for greedy\nsearch and nucleus sampling, but manifest other\nissues: 1) they entail highly-unlikely semantic us-\nages such as Fortunately, it wasn’t all that great—\nan ending word with a negative sentiment should be\nmore prevalent; 2) the nucleus sampling examples,\ndespite being ﬂuent and consistent, hardly ground\nto real world scenarios. This suggests that small\nmodels are highly capable linguistically, and learn-\ning at scale primarily focuses on acquiring other\ntypes of knowledge.14\n5 Downstream Tasks\nIn this section, we examine the trajectory of down-\nstream tasks, evaluated on few-shot in-context\nlearning (ICL).\n5.1 Task Selection and Evaluation\nBIG-Bench (Srivastava et al., 2022) is a large col-\nlection of tasks for evaluating language models.\nWe evaluate intermediate checkpoints on its subset\n14We present more generated examples and have a more\ndetailed discussion on generation quality in Appendix C.3.\n13716\nDist. Greedy Search Nucleus Sampling\nFortunately, the day wasn’t all ... Fortunately, the day wasn’t all ...\nps−pl that great. The sun was setting and the sun was falling. I\nwent to bed and woke my husband, who was asleep in\nhis bed, to ﬁnd that I was still asleep in the middle of the\nnight with him. He was still awake when we\nthat good when the computer said doom and gloom about\nme. Sure enough, because of our stubborn attempt at\nterrorizing him via cyberbackup (which relied heavily\non computer trafﬁc management (VCMD) to ensure my\nidentity), I was able ﬁx my old\nps that bad. I was in the middle of a long day of work and\nI was in the middle of a long day of work. I was in the\nmiddle of a long day of work. I was in the middle of a\nlong day\nthat bad. Not because the weather wasn’t bad, but be-\ncause of how many people didn’t move their car around.\nFor those who did, I wanted to say thanks to everyone\nelse who still had a tire change on. That doesn’t change\nps+pl bad. I was able to get a few things done, and I was able\nto get a few things done. I was able to get a few things\ndone, and I was able to get a few things done. I was able\nto\ncold and we didn’t have to set up a heated bed so we\nwouldn’t freeze off in the middle of the night. It was\na nice fall day and I had just ﬁnished wrapping up the\ncolor scheme on the wall. I still haven\npl bad. I got to spend some time with my family, and I got\nto see my friends. I got to see my friends, and I got to\nsee my family. I got to see my family, and I got to see\nmy\ngloom, glum, and doom. One nice thing was the gift\nof snow for a few minutes this afternoon. It was fun to\nwatch it pile up on the porch, watch the kids watch it\npile up, and then run out and scatter\npl−ps bad news. The U.N.’s Intergovernmental Panel on Cli-\nmate Change released a landmark study showing that\nwe have 12 years to limit climate catastrophe. And a\ngroup of young activists ﬁled a landmark climate lawsuit\nin federal district court, demanding that the government\ntake\nbad for Iowa fans. Tight end C. J. Fiedorowicz decided,\nfor what has to be the millionth time now, to use Twit-\nter as his own personal slogan board, and this time he\ndecided to riff off the famous Bugs Bunny\nTable 1: Examples generated with greedy decoding and nucleus sampling under different conﬁgurations. The\nprompt is Fortunately, the day wasn’t all.\nof 74 multiple-choice tasks.15 BIG-Bench comes\nwith predeﬁned templates with a uniﬁed QA format\nfor in-context learning, which mitigates the extra\ncomplexity of prompt design.16\nWe focus on the 2-shot setting. Following Sri-\nvastava et al. (2022), we randomly select two in-\ncontext learning examples (excluding the evalua-\ntion example itself) for each test instance and pick\nthe candidate for each evaluation example that has\nthe highest probability normalized over its length.\nWe use the average 2-shot accuracy of downstream\ntasks as a proxy for in-context learning capability.\n5.2 Trajectory of ICL Performance\nICL vs. valid PPL. From Figure 8 (leftmost),\nit is evident that the downstream task perfor-\nmance strongly correlates with validation perplex-\nity across all model sizes. The curves of different\nmodel sizes signiﬁcantly overlap, indicating that\nwhen a small model and a large model are trained to\nthe same perplexity level, they achieve comparable\ndownstream task performance.\nICL vs. other metrics. it is evident that plot-\nting task accuracy against various metrics yields\n15Mode details on task selection are in Appendix D.1.\n16Examples of prompts are in Appendix D.2.\ndistinct patterns. Notably, when subjected to an\nequal amount of training FLOPs, the performance\nof smaller models consistently surpasses that of\nlarger models, with the exception of the 125 M\nmodel. This observation implies that larger models\npossess untapped potential for improvement, espe-\ncially when provided with more training FLOPs or\ndata (Hoffmann et al., 2022; Touvron et al., 2023).\nConversely, the remaining two plots indicate that\nlarger models consistently outperform smaller ones\nwhen trained with the same number of training\ntokens and training steps.\n5.3 Linearity vs. Breakthroughness Tasks\nWe select 12 tasks that present a linearity scaling\npattern and 6 tasks that present a breakthroughness\nscaling pattern, 17 and plot the perplexity of the\ncorrect and incorrect options for each group of\ntasks against validation perplexity in Figure 9.\nThe performance of breakthroughness tasks in-\ncreases tremendously as the validation perplexity\ndrops below 8. The perplexity gap between the cor-\nrect and incorrect options also starts to expand at\nthis point for the30B and 175 B models. In contrast,\n17Breakthroughness here similar to the emergent dehavior\ndeﬁned in Wei et al. (2022). Details on how we select linearity\nand breakthroughness tasks are in Appendix D.3.\n13717\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\nRandom 125M 1.3B 6.7B 13B 30B 175B\n8101525\nValidation PPL\n30\n35\n40\n45Accuracy\n31.8\n1018 1020 1022\nFLOPs\n30\n35\n40\n45Accuracy\n31.8\n109 1010 1011\nTokens\n30\n35\n40\n45Accuracy\n31.8\n104 105\nSteps\n30\n35\n40\n45Accuracy\n31.8\nFigure 8: The 2-shot performance trajectory of 74 BIG-Bench tasks. The performance is measured by the average\naccuracy on the default set and plotted against validation perplexity, training FLOPs, training tokens and number\nof training steps. The task accuracy aligns with validation perplexity across different model sizes.\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\nRandom 125M 1.3B 6.7B 13B 30B 175B\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\nCorrect Options Incorrect Options\n78101525\nValidation PPL\n24\n36\n48\n60Accuracy\n26.2\nLinearity T asks\n78101525\nValidation PPL\n24\n36\n48\n60Accuracy\n 33.3\nBreakthroughness T asks\n78101525\nValidation PPL\n1\n2\n3\n4\n6\n9\n15\n25 PPL of Options\nLinearity T asks\n78101525\nValidation PPL\n1\n2\n3\n4\n6\n9 PPL of Options\nBreakthroughness T asks\nFigure 9: Trajectory of 2-shot in-context learning performance (left two) and option perplexity (right two) of 12\nlinearity and 6 breakthroughness tasks against validation perplexity. The perplexity divergence of correct and\nincorrect options drives the performance improvement.\nthe accuracy of linearity tasks gradually increases.\nThe perplexity of correct and incorrect options ﬁrst\ndecrease as validation perplexity decreases, and it\nis only at the end of the curve that the perplexity of\ncorrect and incorrect options starts to diverge. This\nsuggests that improvements in downstream accu-\nracy are not generally driven by the model learning\nto assign a lower probability to incorrect candidates,\nbut rather driven by the perplexity divergence of\ncorrect and incorrect options.\n5.4 Breakthroughness Tasks Learn Smoothly\non Trajectory\nIn Appendix D.4, we provide a detailed analysis of\ntask accuracy in relation to perplexity and FLOPs\nfor individual linearity and breakthroughness tasks.\nThe corresponding plots can be found in Figure\n17 and Figure 18. As expected, these plots ex-\nhibit a signiﬁcantly larger variance, showcasing\nsubstantial ﬂuctuations in task performance dur-\ning the training process. However, we still ob-\nserve a notable alignment between task accuracy\nand validation perplexity across different model\nscales. Notably, the breakthroughness tasks, which\ndemonstrate sudden performance improvements at\nthe ﬁnal checkpoints, display a smooth and con-\ntinuous growth trend along the training trajectory.\nThis observation reinforces the ﬁndings of a recent\nstudy conducted by Schaeffer et al. (2023), where\nthey discovered that modifying downstream task\nmetrics results in gradual changes in performance\nrather than abrupt and unexpected shifts as model\nscale increases. These results suggest that when\nexamining task performance at a ﬁner level, either\nthrough continuous metrics or continuous model\ncheckpoints, task performance largely exhibits a\nsmooth growth pattern in tandem with validation\nperplexity. Nevertheless, as suggested by Ganguli\net al. (2022), accurately predicting the learning\ncurve of a speciﬁc task still remains challenging.\n6 Related Work\nPhase change. Olsson et al. (2022) study induc-\ntion heads to understand the formation of in-context\nlearning ability. The main ﬁnding is that there\nexists a critical phase chage (Power et al., 2022;\nNanda and Lieberum, 2022) that forms the in-\ncontext learning ability. Our studies are in the same\nspirit as these work, but we did not discover any\nphase change for the phenomena we examined; all\nof them evolve steadily as training progresses.\n(Inverse) scaling laws. Previous work studies\nscaling on downstream tasks (Wei et al., 2022; Sri-\nvastava et al., 2022), pre-training data (Hernandez\net al., 2022), architectures (Tay et al., 2022a), bi-\n13718\nases (Tal et al., 2022), and other domains, such as\nvision tasks and neural machine translation (Alab-\ndulmohsin et al., 2022). Our work studies different\nscaling behaviors over model trajectories.\nInverse scaling refers to a scaling behavior where\nincreasing the model size leads to worse perfor-\nmance for a downstream task (Perez and McKen-\nzie). Part of our work intends to understand the dis-\ntributional shift from small models to large models\nfor language modeling along training trajectories,\nwhich overlaps with the theme of inverse scaling.\nPerplexity vs. downstream performance. Re-\ngarding the pre-training/ﬁne-tuning paradigm, Wet-\ntig et al. (2022) and Tay et al. (2022a) ﬁnd that\na lower pre-training perplexity does not necessar-\nily translate to better ﬁne-tuning performance. For\nzero-shot inference, Saunshi et al. (2020) mathe-\nmatically shows that doing well in language mod-\neling beneﬁts downstream tasks. On the contrary,\nShin et al. (2022) claims the opposite relationship\nfor in-context learning performance and perplexity\nwhen training language models with different cor-\npora, but they only test four downstream tasks on a\nfew model checkpoints. Our work extensively eval-\nuates multiple domains and tasks on both language\nmodeling and downstream tasks across checkpoints\nof different scales, which entails less variance.\nEffective scaling Several prior studies have fo-\ncused on effectively scaling models by examining\nlimited compute settings (Geiping and Goldstein,\n2022), exploring different objectives (Tay et al.,\n2022b; Artetxe et al., 2022b), and investigating dif-\nferent architecture and training setups (Scao et al.,\n2022b). This work speciﬁcally examines model\nscales under a uniﬁed setting, but the proposed\ntechniques can be applied to other settings as well.\n7 Conclusion\nTo summarize, our study demonstrates that valida-\ntion perplexity is a reliable indicator of the behavior\nof OPT models, regardless of their sizes. Larger\nmodels, with increased computational power and\ncapacity, exhibit behavior similar to that of smaller\nmodels while also unlocking new phenomena and\ncapabilities as validation perplexity decreases fur-\nther. However, there are certain exceptional cases\nwhere models behave differently, sometimes even\nin opposite directions, such as in the perplexity of\ntexts generated by contrasting two models. This\nsuggests that the underlying model distributions are\nnot entirely identical at the same perplexity level.\nThe availability of a larger number of open-\nsourced model checkpoints, such as those provided\nby Biderman et al. (2023), offers opportunities for\ninterpreting language model behaviors through the\nanalysis of training trajectories. The techniques we\npropose can be extended to analyze language mod-\nels trained using different resources and method-\nologies. Additionally, we leave open questions for\nfuture research, such as further exploring the phe-\nnomenon of double-descent more in-depth.\nLimitations\nWe discuss the limitations of the work as follows:\n• One major limitation of our work is that we\nanalyze language models pre-trained with the\nsame data, similar training procedures, and the\nsame autoregressive language modeling objec-\ntive. Our ﬁndings may support model families\ntrained in this restricted setting. When com-\nparing models trained with different corpora,\nsuch as Neo GPT NEO (Black et al., 2021)\nand BLOOM (Scao et al., 2022a), different\narchitectures and objectives, such as retrieval-\nbased language models (Khandelwal et al.,\n2020; Zhong et al., 2022; Borgeaud et al.,\n2021) and sparse models (Fedus et al., 2022;\nArtetxe et al., 2022a), the relationship between\nvalidation perplexity and downstream task per-\nformance could be more obscure.\n• For downstream task evaluation, we only eval-\nuate on multiple-choice tasks, where the eval-\nuation protocol is the most similar to the pre-\ntraining objective. Evaluating on generation-\nbased tasks is more messy and hard to scale\nup, and we will leave it as future work. An-\nother risk is that as we always take aggregated\nmeasurements over tasks, it might conceal im-\nportant patterns of individual tasks.\n• We do not provide a concrete explanation for\nthe double-descent behavior that consistently\noccurs during pre-training, nor do we know\nif it is an artifact of the data, the objective or\nthe optimization process. We consider it an\ninteresting phenomenon and will look more\nclosely into it in future works.\n13719\nAcknowledgement\nWe thank Sadhika Malladi for helping out with writ-\ning and having insightful discussions on the project\nwith the authors. We thank Tianyu Gao for helping\nout running experiments on open-text generation in\nthe Appendix. We also thank Stephen Roller, Srini\nIyyer, Todor Mihaylov, Xiaochuang Han, and all\nmembers of the Princeton NLP group for helpful\ndiscussion and valuable feedback. This work was\nconducted when Mengzhou Xia was interning at\nMeta Platforms, Inc.\nReferences\nIbrahim Alabdulmohsin, Behnam Neyshabur, and Xi-\naohua Zhai. 2022. Revisiting neural scaling laws in\nlanguage and vision. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS).\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor\nMihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,\net al. 2022a. Efﬁcient large scale language model-\ning with mixtures of experts. In Empirical Methods\nin Natural Language Processing (EMNLP).\nMikel Artetxe, Jingfei Du, Naman Goyal, Luke Zettle-\nmoyer, and Ves Stoyanov. 2022b. On the role of\nbidirectionality in language model pre-training. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nStella Biderman, Hailey Schoelkopf, Quentin An-\nthony, Herbie Bradley, Kyle O’Brien, Eric Halla-\nhan, Mohammad Aﬂah Khan, Shivanshu Purohit,\nUSVSN Sai Prashanth, Edward Raff, et al. 2023.\nPythia: A suite for analyzing large language models\nacross training and scaling. In International Confer-\nence on Machine Learning (ICML).\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. GPT-Neo: Large Scale\nAutoregressive Language Modeling with Mesh-\nTensorﬂow. If you use this software, please cite it\nusing these metadata.\nTerra Blevins, Hila Gonen, and Luke Zettlemoyer.\n2022. Analyzing the mono-and cross-lingual pre-\ntraining dynamics of multilingual language models.\nIn Empirical Methods in Natural Language Process-\ning (EMNLP).\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\ning language models by retrieving from trillions of\ntokens. arXiv preprint arXiv:2112.04426.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS).\nLeshem Choshen, Guy Hacohen, Daphna Weinshall,\nand Omri Abend. 2022. The grammar-learning tra-\njectories of neural language models. In Association\nfor Computational Linguistics (ACL).\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. The Jour-\nnal of Machine Learning Research (JMLR).\nDeep Ganguli, Danny Hernandez, Liane Lovitt,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Con-\nerly, Nova Dassarma, Dawn Drain, Nelson Elhage,\net al. 2022. Predictability and surprise in large gener-\native models. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nJonas Geiping and Tom Goldstein. 2022. Cramming:\nTraining a language model on a single gpu in one\nday. arXiv preprint arXiv:2212.14034.\nDanny Hernandez, Tom Brown, Tom Conerly, Nova\nDasSarma, Dawn Drain, Sheer El-Showk, Nelson\nElhage, Zac Hatﬁeld-Dodds, Tom Henighan, Tristan\nHume, et al. 2022. Scaling laws and interpretabil-\nity of learning from repeated data. arXiv preprint\narXiv:2205.10487.\nDanny Hernandez, Jared Kaplan, Tom Henighan, and\nSam McCandlish. 2021. Scaling laws for transfer.\narXiv preprint arXiv:2102.01293.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations (ICLR).\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\n13720\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations (ICLR).\nKalpesh Krishna, Yapei Chang, John Wieting, and Mo-\nhit Iyyer. 2022. Rankgen: Improving text generation\nwith large ranking models. In Empirical Methods in\nNatural Language Processing (EMNLP).\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Association\nfor Computational Linguistics (ACL).\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy\nLiang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. 2022. Contrastive de-\ncoding: Open-ended text generation as optimization.\narXiv preprint arXiv:2210.15097.\nZeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Ha-\njishirzi, and Noah A Smith. 2021. Probing across\ntime: What does roberta know and when? In Find-\nings of Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 820–842.\nPreetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan\nYang, Boaz Barak, and Ilya Sutskever. 2020. Deep\ndouble descent: Where bigger models and more data\nhurt. In International Conference on Learning Rep-\nresentations (ICLR).\nNeel Nanda and Tom Lieberum. 2022. A mechanistic\ninterpretability analysis of grokking. Alignment Fo-\nrum.\nCatherine Olsson, Nelson Elhage, Neel Nanda,\nNicholas Joseph, Nova DasSarma, Tom Henighan,\nBen Mann, Amanda Askell, Yuntao Bai, Anna\nChen, Tom Conerly, Dawn Drain, Deep Ganguli,\nZac Hatﬁeld-Dodds, Danny Hernandez, Scott John-\nston, Andy Jones, Jackson Kernion, Liane Lovitt,\nKamal Ndousse, Dario Amodei, Tom Brown, Jack\nClark, Jared Kaplan, Sam McCandlish, and Chris\nOlah. 2022. In-context learning and induction heads.\nTransformer Circuits Thread.\nEthan Perez and Ian McKenzie. Inverse scaling prize:\nRound 1 winners.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. Advances in Neural Information Process-\ning Systems (NeurIPS).\nAlethea Power, Yuri Burda, Harri Edwards, Igor\nBabuschkin, and Vedant Misra. 2022. Grokking:\nGeneralization beyond overﬁtting on small algorith-\nmic datasets. arXiv preprint arXiv:2201.02177.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\nCompressive transformers for long-range sequence\nmodelling. In International Conference on Learn-\ning Representations (ICLR).\nNikunj Saunshi, Sadhika Malladi, and Sanjeev Arora.\n2020. A mathematical exploration of why lan-\nguage models help solve downstream tasks. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Ro-\nman Castagné, Alexandra Sasha Luccioni, François\nYvon, Matthias Gallé, et al. 2022a. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lu-\ncile Saulnier, Stas Bekman, M Saiful Bari, Stella\nBideman, Hady Elsahar, Niklas Muennighoff, Jason\nPhang, et al. 2022b. What language model to train\nif you have one million gpu hours? arXiv preprint\narXiv:2210.15424.\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo.\n2023. Are emergent abilities of large language mod-\nels a mirage? arXiv preprint arXiv:2304.15004.\nSeongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sung-\ndong Kim, HyoungSeok Kim, Boseop Kim,\nKyunghyun Cho, Gichang Lee, Woomyoung Park,\nJung-Woo Ha, et al. 2022. On the effect of pre-\ntraining corpora on in-context learning by a large-\nscale language model. In North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nYixuan Su and Nigel Collier. 2022. Contrastive search\nis what you need for neural text generation. arXiv\npreprint arXiv:2210.14140.\nYarden Tal, Inbal Magar, and Roy Schwartz. 2022.\nFewer errors, but more stereotypes? the effect of\nmodel size on gender bias. In Proceedings of the\n4th Workshop on Gender Bias in Natural Language\nProcessing (GeBNLP).\n13721\nYi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won\nChung, William Fedus, Jinfeng Rao, Sharan Narang,\nVinh Q Tran, Dani Yogatama, and Donald Met-\nzler. 2022a. Scaling laws vs model architectures:\nHow does inductive bias inﬂuence scaling? arXiv\npreprint arXiv:2207.10551.\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fe-\ndus, Samira Abnar, Hyung Won Chung, Sharan\nNarang, Dani Yogatama, Ashish Vaswani, and Don-\nald Metzler. 2022b. Scale efﬁciently: Insights from\npretraining and ﬁnetuning transformers. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,\nPercy Liang, Jeff Dean, and William Fedus. 2022.\nEmergent abilities of large language models. Trans-\nactions on Machine Learning Research. Survey Cer-\ntiﬁcation.\nAlexander Wettig, Tianyu Gao, Zexuan Zhong, and\nDanqi Chen. 2022. Should you mask 15% in\nmasked language modeling? arXiv preprint\narXiv:2202.08005.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\ning language models with memory augmentation. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\n13722\nA Checkpoint Details\nWe present the checkpoint information in Table 2.\nOPT models of different sizes are trained with dif-\nferent batch sizes and end up training with different\nnumber of steps given the same amount of training\ntokens. We select early-stage checkpoints every\n4K steps for evaluation, and enlarge the interval\nto 10K or 20K for late stage checkpoints. There\nare a few checkpoints missing/corrupted from the\ntraining process, e.g., 125 M 180 K, and we have to\neliminate them our evaluation.\nAll OPT models are trained with 300B tokens, of\nwhich 180B tokens are unique. This training pro-\ncedure means that OPTs are trained with repeated\ndata, though training with non-repeating data con-\nsistently lead to better performance in language\nmodeling and downstream tasks (Lee et al., 2022;\nHernandez et al., 2022).\nB Next-Token Predictions\nB.1 Data Used in the Main Paper\nWe use the Gutenberg PG-19 (Rae et al., 2020)\nsubset as the main dataset for analysis in the main\npaper. This validation subset contains 50 lines of\ntexts, and we take the ﬁrst 2048 tokens of each\nline for analysis, resulting in 102350 context-token\npairs. We observe similar patterns when evaluated\non other validation subsets such as Wikipedia and\nopensubtitles, and we omit the results for brevity.\nB.2 Trajectory of Other Tokens\nWe set our criteria to be relatively strict to\nmake sure that the perplexity trajectory of the\nselected tokens does present the trend (stag-\nnated/upward/downward) we expect. We present\nthe trajectory of the tokens that do not fall into\nany of the categories in Figure 10. We ﬁnd that\nthe trend of these tokens are not consistent across\nmodels. After 10% of training, the curves of 125 M,\n1.3 B, 6.7 B present a slight double-descent trend,\nand for the rest of the models, the curves present a\ndownward/stagnated trend. After 40% of training,\nthe curves of 125 M present a slight double-descent\ntrend towards the end, and the curves of other mod-\nels present a downward/stagnated trend. This sug-\ngests that the rest of the tokens might contain a\nlarger variance in their perplexity trajectories.\n1018 1019 1020 1021 1022 1023\nFLOPs\n20\n40\n60\n80PPL\n43.9%\n45.6%\n43.0%\n45.6%\n48.3%\n31.5%\nAfter 10% Training of Each Model\n1018 1019 1020 1021 1022 1023\nFLOPs\n20\n40\n60\n80\n100PPL\n58.7%\n57.6%\n54.6%\n62.7%\n66.1%\n46.0%\nAfter 40% Training of Each Model\nFigure 10: Perplexity of tokens that do not fall into\nany of the categories. Different models are evaluated\non different subsets of tokens selected after 10% (up)\nand 40% (down) of training of individual models. The\ntrends are not consistent across different model sizes.\nB.3 Properties of Stagnated and\nUpward-Trend Tokens\nWe show an example paragraph in Table 3, where\nthe stagnated tokens are in blue, upward-trend to-\nkens are in red and downward-trend tokens are\nin green. It’s easy to see that stagnated tokens\nare mostly connecting words, determiners, punc-\ntuation and continuation of words. However, we\nﬁnd it hard to characterize the tokens that present\nan upward-trend in perplexity simply based on to-\nken types. We made attempts to further decipher\nwhat language properties this subset might entail\nbased on the part-of-speech tags and positions in se-\nquences, and did not observe any obvious patterns\nwhen compared to all the tokens in the validation\nset. One thing we are sure is that the phenomenon\nof the upward trend in perplexity as well as the\ndouble-descent phenomenon on a certain subset\nof tokens systematically appears across all model\nsizes. Therefore, this subset of context-token pairs\nmust embody certain intrinsic language properties,\nwhich might be beyond our comprehension so far.\n13723\n# Params LR Batch Size # Steps # C Kpt C Kpt Steps\n125 M 6.0e−4 0.5M 600 K 36 2K, 6 K, 10 K, 14 K, 18 K, 22 K, 26 K, 30 K,\n34K, 38 K, 40 K, 60 K, 80 K, 100 K, 120 K,\n140K, 160 K, 200 K, 220 K, 240 K, 260 K,\n280K, 300 K, 320 K, 340 K, 360 K, 380 K,\n400K, 420 K, 440 K, 460 K, 480 K, 500 K,\n520K, 540K, 560K\n1.3 B 2.0e−4 1M 300 K 22 2K, 6 K, 10 K, 14 K, 18 K, 22 K, 26 K, 30 K,\n34K, 38 K, 40 K, 60 K, 80 K, 100 K, 120 K,\n140K, 160 K, 180 K, 200 K, 220 K, 240 K,\n260K\n6.7 B 1.2e−4 2M 150 K 21 2K, 6 K, 10 K, 14 K, 18 K, 22 K, 26 K, 30 K,\n34K, 38K, 40K, 50K, 60K, 70K, 80K, 90K,\n100K, 110K, 120K, 130K, 140K\n13B 1.0e−4 4M 75 K 18 2K, 6 K, 10 K, 14 K, 18 K, 22 K, 26 K, 30 K,\n34K, 38K, 42K, 46K, 50K, 54K, 58K, 62K,\n66K, 70K\n30B 1.0e−4 4M 75 K 18 2K, 6 K, 10 K, 14 K, 18 K, 22 K, 26 K, 30 K,\n34K, 38K, 42K, 46K, 50K, 54K, 58K, 62K,\n66K, 70K\n175 B 1.2e−4 2M 150 K 32 4K, 8 K, 12 K, 16 K, 20 K, 24 K, 36 K, 40 K,\n44K, 48K, 52K, 56K, 60K, 64K, 68K, 72K,\n76K, 80 K, 84 K, 88 K, 92 K, 96 K, 100 K,\n104K, 108 K, 112 K, 120 K, 124 K, 128 K,\n132K, 136K, 140K\nTable 2: Checkpoint (Ckpt) information for OPT models. LR denotes learning rate. Note that we take these\ncheckpoints for practical reasons and the distance between checkponts are not evenly spaced. But it should not\naffect the analysis.\nIt would be interesting to do an in-depth analysis in\nunderstanding why it happens during pre-training,\nand how it connects to natural language properties.\nB.4 More Explorations on Upward Trends\nIn this section, we explore the subset of tokens that\npresent an upward trend when selected by mod-\nels of other sizes from the main paper (6.7 B, 13B,\n30B). We present the perplexity trajectory of these\ntokens in Figure 11. For the subset of tokens se-\nlected after 10% of training of the 6.7 B model, the\nlarger models’ perplexity also increase but only the\nlargest 175 B model presents a double descent be-\nhavior where the perplexity declines further. When\nthe tokens are selected after 40% of training of\n6.7 B, the trends remain similar but the change is\nmulch more mild. Overall, except the model that is\nused to select the tokens, the curves of other mod-\nels present a similar trend, and we will show that\nthese curves overlap with each other almost com-\npletely when plotting against validation perplexity\nin the next subsection. The consistent occurrence\nof double-descent behavior along the trajectory\nshows that it’s a phenomenon happening univer-\nsally across the entire autoregressive pre-training\nprocess.\nB.5 Results against Validation Perplexity\nIn the main paper, we mostly plot measurements\nagainst FLOPs, in this section, we plot the perplex-\nity trajectory of tokens that present different trends\nagainst validation perplexityin Figure 12. These\nﬁgures present the same series of results as Figure 3\nand Figure 4, except that the x-axis is validation\nperplexity. As mentioned in section 2, we use the\naggregated perplexity of a number of subsets as the\nvalidation perplexity.\nFrom Figure 12, we see that given a similar level\nof validation perplexity, for different subsets of to-\nkens, the trajectories of models across sizes overlap\nwell with each other, suggesting that the predictions\nfor these tokens are similar across model scales at\n13724\nAfter 10% training of1.3 B model After 10% training of 175 B model\nAppropri ate ; pertaining to the subject . \\n P\nect oral . The bone which forms the main rib or\nsupport at the forward edge of a bird ’s wing .\n\\n Pers istent . Keeping at it ; determination to\nproceed . \\n Per pend icular . At right angles\nto a surface . This term is sometimes wrongly\napplied in referring to an object , particularly to\nan object which is vertical , meaning up and down\n. The blade of a square is perpend ie ular to the\nhandle at all times , but the blade is vertical only\nwhen it points to the center of the earth . \\n P\nern icious . Bad ; not having good features or\npossessing wrong attributes . \\n P end ulum . A\nbar or body suspended at a point and adapted to\nswing to and fro . \\n Per pet ual . For all time ;\nun ending or unlimited time . \\n P hen omen a .\nSome peculiar happening , or event , or object .\n\\n P itch . In aviation this applies to the angle at\nwhich the blades of a prope ller are cut . If a prope\nller is turned , and it moves forward ly in the exact\npath made by the angle , for one complete turn\n, the distance traveled by the prope ller ax ially\nindicates the pitch in feet . \\n Pl acement . When\nan object is located at any particular point , so that\nit is operative the location is called the placement\n. \\n Pl ane . A ﬂat surface for supporting a ﬂying\nmachine in the air . Plane of movement per tains\nto the imaginary surface described by a moving\nbody\nAppropri ate ; pertaining to the subject . \\n P\nect oral . The bone which forms the main rib or\nsupport at the forward edge of a bird ’s wing .\n\\n Pers istent . Keeping at it ; determination to\nproceed . \\n Per pend icular . At right angles\nto a surface . This term is sometimes wrongly\napplied in referring to an object , particularly to\nan object which is vertical , meaning up and down\n. The blade of a square is perpend ie ular to the\nhandle at all times , but the blade is vertical only\nwhen it points to the center of the earth . \\n P\nern icious . Bad ; not having good features or\npossessing wrong attributes . \\n P end ulum . A\nbar or body suspended at a point and adapted to\nswing to and fro . \\n Per pet ual . For all time ;\nun ending or unlimited time . \\n P hen omen a .\nSome peculiar happening , or event , or object .\n\\n P itch . In aviation this applies to the angle at\nwhich the blades of a prope ller are cut . If a prope\nller is turned , and it moves forward ly in the exact\npath made by the angle , for one complete turn\n, the distance traveled by the prope ller ax ially\nindicates the pitch in feet . \\n Pl acement . When\nan object is located at any particular point , so that\nit is operative the location is called the placement\n. \\n Pl ane . A ﬂat surface for supporting a ﬂying\nmachine in the air . Plane of movement per tains\nto the imaginary surface described by a moving\nbody\nTable 3: An example paragraph to demonstrate tokens that present a stagnating, upward or downward trend after\n10% training of 1.3 B and 175 B models. Tokens that present an upward trend in perplexity are in Red; tokens that\npresent a downward trend are in Green; stagnating tokens are in Blue. Black tokens do not present a clear trend.\na ﬁxed level of validation perplexity. The only\nexception is the upward-trend tokens selected af-\nter 10 % training of 1.3 B, where evaluating with\n1.3 B presents a clear upward trend as the valida-\ntion perplexity increases, while the models larger\nthan 1.3 B present a overlapping double descent-\nlike trend. This indicates that the underlying distri-\nbution of models at the same level of perplexity are\nlargely similar but could differ in edge cases.\nThese results lays the foundation for downstream\ntask evaluations, which heavily relies on the pre-\ntraining objective for evaluation.\nC Sequence-Level Generation\nC.1 Details of Corrupted Datasets\nWe corrupt texts from the opensubtitle subset of the\nvalidation set by replacing p% tokens (subwords)\nwith randomly sampled tokens in the sequences.\nWe cap the max length of a sequence to be 100,\nthough changing max length values does not af-\nfect the conclusion. Although the perplexity on\nthese corrupted sequences is extremely high, es-\npecially when the replacement rate is high, it is\nstill much lower than a truely random model (the\nperplexity of a random model should be |V|where\nV is the vocabulary), even for the fully corrupted\ndataset. It reﬂects that larger language models are\nbetter at exploiting random patterns to produce in-\n13725\n1018 1019 1020 1021 1022 1023\nFLOPs\n10\n15\n20\n25\n30\n35\n40PPL\n9.6%\nAfter 10% Training of 6.7b Model\n1018 1019 1020 1021 1022 1023\nFLOPs\n10\n15\n20\n25\n30\n35PPL\n8.3%\nAfter 10% Training of 13b Model\n1018 1019 1020 1021 1022 1023\nFLOPs\n5\n10\n15\n20\n25\n30\n35\n40PPL\n7.8%\nAfter 10% Training of 30b Model\n1018 1019 1020 1021 1022 1023\nFLOPs\n10\n20\n30\n40\n50PPL\n7.3%\nAfter 40% Training of 6.7b Model\n1018 1019 1020 1021 1022 1023\nFLOPs\n10\n20\n30\n40PPL\n5.5%\nAfter 40% Training of 13b Model\n1018 1019 1020 1021 1022 1023\nFLOPs\n10\n20\n30\n40\n50PPL\n 5.2%\nAfter 40% Training of 30b Model\nFigure 11: Perplexity of tokens that present an upward trend after 10% or 40% of training of the 6.7 B, 13 B and\n30B models. For each ﬁgure, all the models are evaluated on the same subset of tokens.\ndistribution contents than smaller counterparts. We\nalso tried other ways of corruption, such as delet-\ning, inserting, repeating tokens/spans, and all these\ncorruptions result in similar scaling trends.\nC.2 Comparison to Li et al. (2022)\nOur decoding approach is similar to the contrastive\ndecoding method (CD) proposed in Li et al. (2022),\nthough initially for completely different purposes.\nThe difference between the two methods is in the\nsubtraction space. The contrastive score in CD is\ndeﬁned by dividing the expert probability over am-\nateur probability, which is equivalent to subtraction\nin the log probability space. Our approach operates\nsubtraction in the probability space directly, ruling\nout unlikely options where the small model is much\nmore conﬁdent than the large model directly. Due\nto this different design choice, our approach does\nnot need to add the adaptive plausibility restriction,\nnor involve any additional hyperparameter. Sub-\ntraction in the probability space easily eliminates\nthe false positive cases.\nWe initially propose the approach to decoding\nsequences that small models favor more than large\nmodels to understand the distributional shift across\nmodel scales, while contrastive decoding proposed\nin Li et al. (2022) is a general open-generation ap-\nproach. Nonetheless, our approach could be an ef-\nfective and lightweight alternative for open-ended\ngeneration without the need to adjust hyperparame-\nters. In Appendix C.4, we show that our approach\noutperforms nucleus sampling on MAUVE scores.\nC.3 Generation Quality\nTo have a better understanding of the overall qual-\nity of the generated sequences, we evaluate these\nsequences decoded with each conﬁguration in Fig-\nure 6 using MAUVE scores (Pillutla et al., 2021).\nWe present the MAUVE scores in Figure 13 . Our\ngeneration protocol is slightly different from the\nstandard open-ended generation practices in that\nwe only use 5 tokens as prompts for generation,\nwhile usually at least 128 tokens are used (Krishna\net al., 2022; Su and Collier, 2022; Li et al., 2022).\nUsing fewer tokens as prompts leads to a higher\ngeneration diversity, and the generated distribution\ncould be largely different from the ground-truth\nsentences. Therefore, we ﬁnd that the MAUVE\nscores of our generated sequences are much lower\nthan reported in open-ended generation literature.\nComparing the two decoding protocols, subtrac-\ntion between two distributions (ps −pl and pl −ps)\nleads to a better generation quality than summing\nthe two ( ps + pl) for greedy sampling, but vice\nversa for nucleus sampling. To verify the effec-\ntiveness of the approach, we compare it to nucleus\nsampling with standard open-generation protocols\nin Appendix C.4.\nC.4 Open-ended Generation Evaluation\nWe follow the generation protocol in Krishna et al.\n(2022) for open-ended generation, where we gen-\nerate sequences with a maximum length of 128\ngiven contexts that have 256 tokens. We decode\nsequences based on either pl −ps or pl with greedy\n13726\n0.91.01.11.21.31.41.5\nValidation PPL\n1.2\n1.4\n1.6\n1.8\n2.0PPL\n8.8%\nAfter 10% Training of 1.3b Model\n0.91.01.11.21.31.41.5\nValidation PPL\n1.5\n2.0\n2.5\n3.0PPL\n6.8%\nAfter 10% Training of 175b Model\n(a) Stagnated Tokens\n0.91.01.11.21.31.41.5\nValidation PPL\n15\n20\n25\n30\n35\n40\n45\n50PPL\n9.4%\nAfter 10% Training of 1.3b Model\n0.91.01.11.21.31.41.5\nValidation PPL\n10\n20\n30\n40PPL\n11.1%\nAfter 10% Training of 175b Model\n(b) Upward-Trend Tokens\n0.91.01.11.21.31.41.5\nValidation PPL\n20\n40\n60\n80\n100PPL\n 32.1%\nAfter 10% Training of 1.3b Model\n0.91.01.11.21.31.41.5\nValidation PPL\n20\n40\n60\n80PPL\n 50.6%\nAfter 10% Training of 175b Model\n(c) Downward-Trend Tokens\nFigure 12: Perplexity of stagnated tokens, upward-trend tokens and downward-trend tokens against validation\nperplexity. Curves of different models largely overlap with each other, signifying that validation perplexity is\na good indicator of model behaviors along the trajectory, e.g. the double descent-like phenomenon, agnostic to\nmodel sizes.\ndecoding or nucleus sampling (p= 0.9) and eval-\nuate the quality of the generation with MAUVE\nscores.\nWe present the results in Table 4. Consistently,\nour approach to subtracting the probability from a\nsmall model from a large model outperforms nu-\ncleus sampling with one single model consistently,\nindicating that our approach has the potential to\nserve as an effective general decoding method for\nopen-ended generation.\nC.5 Generating Longer Sequences\nWe extend the study to generate longer sequences\nup to 100 and 500 tokens, and we present perplexity\ntrajectories in Figure 14 and Figure 15, respectively.\nWe ﬁnd that the inverse scaling trend across model\n13727\nps pl\n ps ps + pl pl pl ps\n0\n50mauve\n43.3 52.1 58.9 62.9\n49.3\nps pl\n ps ps + pl pl pl ps\n0\n50mauve\n27.7\n9.7 8.8\n27.4\n41.8\nFigure 13: M AUVE scores (the higher, the better) on sequences with a maximum length of 50.\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\n125M final ckpt\n125M intermediate ckpts\n1.3B final ckpt\n1.3B intermediate ckpts\n6.7B final ckpt\n6.7B intermediate ckpts\n13B final ckpt\n13B intermediate ckpts\n30B final ckpt\n30B intermediate ckpts\n50\n100PPL\nModel Size\n5\n10\n15PPL\nModel Size\nGreedy Search\n50\n100PPL\nFLOPs (log scale)\n20\n40PPL\nFLOPs (log scale)\nps pl\n ps ps + pl pl pl ps\n0\n50mauve\n23 30.9 31.4 39.2 29.7\nps pl\n ps ps + pl pl pl ps\n0\n50mauve\n17.6\n1.1 1.4 7.6\n20.4\nFigure 14: Greedy search and nucleus sampling results with generations of a length of 100.\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\n125M final ckpt\n125M intermediate ckpts\n1.3B final ckpt\n1.3B intermediate ckpts\n6.7B final ckpt\n6.7B intermediate ckpts\n13B final ckpt\n13B intermediate ckpts\n30B final ckpt\n30B intermediate ckpts\n25\n50PPL\nModel Size\nNucleus Sampling\n5\n10PPL\nModel Size\nGreedy Search\n25\n50\n75PPL\nFLOPs (log scale)\n0\n20PPL\nFLOPs (log scale)\nps pl\n ps ps + pl pl pl ps\n0\n50mauve 6.8 15.1 17.3 20.5 20.7\nps pl\n ps ps + pl pl pl ps\n0\n50mauve 5.7 0.6 0.6 0.7\n13.5\nFigure 15: Greedy search and nucleus sampling results with generations of a length of 500.\nsizes and the opposite perplexity trend between\nthe 125 M and 30B also hold for longer sequences.\nMAUVE scores on generated sequences of different\nlengths are largely consistent. The longer the de-\ncoded sequences are, the worse the overall quality.\nC.6 Examples of Generated Sequences\nWe present more examples of generated sequences\nin Table 5 and Table 6. Similar to Table 1, we ﬁnd\nthat nucleus sampling with pl,pl −ps and greedy\nsearch with pl −ps constantly generate high-quality\nsequences. Greedy decoding ps −pl generates\nmediocre sequences that are largely grammatical\nand ﬂuent, but less coherent and sometimes contain\nhallucinations.\nC.7 Validation Perplexity vs. Perplexity of\nGenerated Texts\nWe plot the perpelxity trajectory of generated texts\nagainst validation perplexity in Figure 16. The tra-\njectories largely align well across model sizes for\nps, ps + pl and pl but diverge in the case of pl −ps\nand ps −pl. This indicates that the underlying dis-\ntributions of different-sized models given the same\nperplexity are similar but not exactly identical.\n13728\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\n125m 1.3b 6.7b 13b 30b 175b\n10 20 30\n50\n100PPL\n10 20 30 10 20 30\nValidation PPL\n10 20 30 10 20 30\n10 20 30\n20\n40PPL\n10 20 30 10 20 30\nValidation PPL\n10 20 30 10 20 30\nps pl\n ps ps + pl pl pl ps\n0\n50mauve\n27.7\n9.7 8.8\n27.4\n41.8\nps pl\n ps ps + pl pl pl ps\n0\n50mauve\n27.7\n9.7 8.8\n27.4\n41.8\nFigure 16: Validation perplexity vs. perplexity of generated texts. We ﬁnd that models of different scales do not\nhave the same perplexity on the generated texts when decoded with ps −pl or pl −ps given the same validation\nperplexity, but they largely align when decoded with other conﬁgurations.\ngreedy nucleus\n350m 0.065 0.807\n350m-125m 0.795 0.852\n1.3b 0.164 0.877\n1.3b-125m 0.851 0.890\n1.3b-350m 0.888 0.886\n2.7b 0.237 0.832\n2.7b-125m 0.815 0.851\n2.7b-350m 0.846 0.843\nTable 4: MAUVE scores of generations following open-\ngeneration protocols. Nucleus sampling on an interpo-\nlated distribution (pl −ps) consistently outperforms de-\ncoding with a single model (pl).\nD Downstream Tasks\nD.1 Task Selection and Evaluation\nOut of comuputational considerations, we only\nevaluate multiple-choice tasks that have fewer than\n1000 evaluation examples. The list of selected tasks\nis shown in Table 7. We report 2-shot in-context\nlearning performance on the default set of each\nBIG-Bench dataset.\nD.2 Prompts\nWe use ﬁxed prompt formats from the BIG-Bench\ndatasets. Optimizing the prompts might lead to\nextra margins in performance. Studying the rela-\ntionship between prompt formats and downstream\ntask performance along the trajectory is interesting,\nbut we consider it out of the scope of this work. We\npresent examples from four datasets in Table 8.\nD.3 Linearity and Breakthroughness Tasks\nSrivastava et al. (2022) identify tasks showing a lin-\nearity or breakthroughness pattern and (Wei et al.,\n2022) coin the term emergent ability for models\nshowing breakthroughness patterns on certain tasks.\nPrevious works mainly study scaling patterns of\ndownstream tasks with ﬁnal model checkpoints,\nand we extend this to training trajectories of mod-\nels across scales. We largely follow Srivastava et al.\n(2022) to identify tasks with linearity and break-\nthroughness patterns – the former depicts the trend\nwhere the task performance scales with the model\nsize reliably, and for the latter, the performance\nremains low until a critical model size.\nWe select 12 tasks that show a linearity pattern\nand 6 tasks that show a breakthroughness pattern\nbased on the metrics proposed in (Srivastava et al.,\n2022). For each model size xi and the correspond-\ning performance yi, the metrics are deﬁned as\nL= I(y)√\n1\nn\n∑\ni z2\ni\n; B = I(y)√\nMedian({z2\ni })\n; (4)\nwhere I(y) =sign(arg maxi yi −arg maxi yi)\n·(maxi yi −mini yi) is a measure to capture the\noverall improvement of performance when scal-\ning up. We ﬁnd that these two measures are not\nsufﬁcient for identifying the scaling trends for lin-\nearity and breakthroughness, thus we also manually\ncheck the scaling pattern to verify. The linearity\nand breakthroughness tasks are lists in Table 9.\nD.4 Trajectory of Each Task\nWe present the scaling curves (on the ﬁnal model\ncheckpoints) and training trajectories of each lin-\nearity and breakthroughness task in Figure 17 and\nFigure 18. The evaluation of each task presents a\nlarge variance across the training steps. Though\nthe tasks might present a breakthroughness pattern\non the scaling curves, their trajectory curves show\nthat language models pick up the task gradually.\n13729\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\nRandom 125M 1.3B 6.7B 13B 30B 175B\n125M 1.3B 6.7B 13B 30B 175B\n20\n30\n40\n50\n60\ndate_understanding\n125M 1.3B 6.7B 13B 30B 175B\n50\n55\n60\nhhh_alignment\n125M 1.3B 6.7B 13B 30B 175B\n50\n55\n60\n65\nfantasy_reasoning\n125M 1.3B 6.7B 13B 30B 175B\n5\n10\n15\n20\nimplicit_relations\n1018 1020 1022\nFLOPs\n20\n30\n40\n50\n60\ndate_understanding\n1018 1020 1022\nFLOPs\n50\n55\n60\nhhh_alignment\n1018 1020 1022\nFLOPs\n40\n50\n60\nfantasy_reasoning\n1018 1020 1022\nFLOPs\n5\n10\n15\n20\n25\nimplicit_relations\n8101525\nValidation PPL\n20\n30\n40\n50\n60\ndate_understanding\n8101525\nValidation PPL\n50\n55\n60\nhhh_alignment\n8101525\nValidation PPL\n40\n50\n60\nfantasy_reasoning\n8101525\nValidation PPL\n5\n10\n15\n20\n25\nimplicit_relations\n125M 1.3B 6.7B 13B 30B 175B\n20\n40\n60\nintent_recognition\n125M 1.3B 6.7B 13B 30B 175B\n10\n15\n20\n25\n30\nfigure_of_speech_detection\n125M 1.3B 6.7B 13B 30B 175B\n40\n60\n80\nsimilarities_abstraction\n125M 1.3B 6.7B 13B 30B 175B\n20\n30\n40\n50\nsimple_ethical_questions\n1018 1020 1022\nFLOPs\n20\n40\n60\n80\nintent_recognition\n1018 1020 1022\nFLOPs\n10\n20\n30\nfigure_of_speech_detection\n1018 1020 1022\nFLOPs\n40\n60\n80\nsimilarities_abstraction\n1018 1020 1022\nFLOPs\n20\n30\n40\n50\nsimple_ethical_questions\n8101525\nValidation PPL\n20\n40\n60\n80\nintent_recognition\n8101525\nValidation PPL\n10\n20\n30\nfigure_of_speech_detection\n8101525\nValidation PPL\n40\n60\n80\nsimilarities_abstraction\n8101525\nValidation PPL\n20\n30\n40\n50\nsimple_ethical_questions\n125M 1.3B 6.7B 13B 30B 175B\n35\n40\n45\nstrange_stories\n125M 1.3B 6.7B 13B 30B 175B\n20\n30\n40\n50\n60\nundo_permutation\n125M 1.3B 6.7B 13B 30B 175B\n46\n48\n50\n52\n54\nmisconceptions\n125M 1.3B 6.7B 13B 30B 175B\n20\n30\n40\n50\n60\nnonsense_words_grammar\n1018 1020 1022\nFLOPs\n30\n35\n40\n45\n50\nstrange_stories\n1018 1020 1022\nFLOPs\n20\n30\n40\n50\n60\nundo_permutation\n1018 1020 1022\nFLOPs\n45\n50\n55\nmisconceptions\n1018 1020 1022\nFLOPs\n20\n30\n40\n50\n60\nnonsense_words_grammar\n8101525\nValidation PPL\n30\n35\n40\n45\n50\nstrange_stories\n8101525\nValidation PPL\n45\n50\n55\n60\nundo_permutation\n8101525\nValidation PPL\n45\n50\n55\nmisconceptions\n8101525\nValidation PPL\n30\n40\n50\n60\nnonsense_words_grammar\nFigure 17: Scaling curves and trajectories of linearity tasks.\n13730\n0.04\n 0.02\n 0.00 0.02 0.04\n0.04\n0.02\n0.00\n0.02\n0.04\nRandom 125M 1.3B 6.7B 13B 30B 175B\n125M 1.3B 6.7B 13B 30B 175B\n25\n30\n35\n40\n45\ncode_line_description\n125M 1.3B 6.7B 13B 30B 175B\n20\n30\n40\n50\nhuman_organs_senses\n125M 1.3B 6.7B 13B 30B 175B\n50\n55\n60\n65\nimplicatures\n125M 1.3B 6.7B 13B 30B 175B\n25\n30\n35\n40\n45\n50\nphrase_relatedness\n1018 1020 1022\nFLOPs\n30\n40\n50\ncode_line_description\n1018 1020 1022\nFLOPs\n20\n30\n40\n50\n60\nhuman_organs_senses\n1018 1020 1022\nFLOPs\n50\n55\n60\n65\nimplicatures\n1018 1020 1022\nFLOPs\n25\n30\n35\n40\n45\n50\nphrase_relatedness\n8101525\nValidation PPL\n30\n40\n50\nbigbench__codelinedescription\n8101525\nValidation PPL\n20\n30\n40\n50\n60\nbigbench__humanorganssenses\n8101525\nValidation PPL\n50\n55\n60\n65\nbigbench__implicatures\n8101525\nValidation PPL\n30\n40\n50\nbigbench__phraserelatedness\n125M 1.3B 6.7B 13B 30B 175B\n22\n24\n26\n28\n30\n32\nswahili_english_proverbs\n125M 1.3B 6.7B 13B 30B 175B\n50\n60\n70\nwhat_is_the_tao\n1018 1020 1022\nFLOPs\n20\n25\n30\nswahili_english_proverbs\n1018 1020 1022\nFLOPs\n40\n50\n60\n70\nwhat_is_the_tao\n8101525\nValidation PPL\n20\n25\n30\nbigbench__swahilienglishproverbs\n8101525\nValidation PPL\n40\n50\n60\n70\nbigbench__whatisthetao\nFigure 18: Scaling curves and trajectories of breakthroughness tasks.\n13731\nDist. Greedy Search Nucleus Sampling\nA girl (Lisbeth Salander) has ... A girl (Lisbeth Salander) has ...\nps−pl just discovered a new way to get her hair done!\\n\\nLisba\nis a blonde beauty who has been living her dream for\nquite some time now.\\n\\nLisba was recently spotted\nwearing a red and white wig and a black and blue striped\nsuit that she wore in an Instagram video. The blonde\nbeauty is currently living in the UK with her husband,\nMatt, with the help of their daughter Lizz.\\n\\nLizz and\nMatt have been dating for a while but have been dating\nleft the grandpa home he’s lived to preserve for\nher.\\nLisbin and her daughter Dylan Joanna (Arista\nWelch-Collinson) do everything they can to protect their\nsister.\\nBut unlike ever before their young girl ﬁlls it all\nwith grief with every form of loss.\\nAs she learns Dylan\nisn’t here anymore and acknowledges he’s changed his\nmind, Daley ﬁnds herself falling back on the same old\nrules again.\\nYellen’s been teaching the inane Lyle\nps a crush on a guy (Lisbeth Salander) and she’s not sure\nwhat to do about it.\\n\\nShe’s a girl who’s been in love\nwith a guy (Lisbeth Salander) for a while, but she’s not\nsure what to do about it.\\n\\nShe’s a girl who’s been in\nlove with a guy (Lisbeth Salander) for a while, but she’s\nnot sure what to do about it.\\n\\nShe\njust discovered\\nthat it’s not an actual\\nworm! Thanks to\nthe Halloween\\nMirror campaign, she was discovered in\nthe\\ngoldeneye-buxco-only prologue of the main\\ngame,\nfor her interest in science and fantasia.\\n\\nMalcolm\nYoung: There are four bugs on here.We’re working on\nthem, though, because they’re incredibly busy and we’re\nboth making what are\\nexpected\\nto\nps+pl a crush on a guy (Alexander Skarsgard) and he’s a bit\nof a jerk.\\nI think you mean Lisbeth Salander.\\n\\nI think\nyou mean Lisbeth Salander.\\nI think you mean Lisbeth\nSalander.\\nI think you mean Lisbeth Salander.\\nI think\nyou mean Lisbeth Salander.\\nI think you mean Lisbeth\nSalander.\\nI think you mean Lisbeth Salander.\\nI think\nyou\na feeling that there’s something more to computer games\nthan what we see in them. Turns out, there is something\nmore to computer games. But it’s not “aliens,” “moles,”\nor “spies.” It’s about a virtual world that we can all\nbe part of if we so choose.\\n\\nWe believe in calling\nthat world Virtual Reality.\\n\\nThe game is called THE\nPREMIER.\npl a crush on a guy (Alexander Skarsgard) who is\na computer hacker.\\nThe Girl with the Dragon Tat-\ntoo?\\nYup.<s>\nhas been attacked at her apartment by three men. They\nrape her and beat her. She later ﬁnds herself at a center\nfor the mentally disabled. There, she has... See full\nsummary »\\nThe beginning of a story. The ﬁrst nympho-\nmaniac to admit that she has a problem, Helga refuses\nto seek treatment because she sees it as a surrender to\ndefeat. However, it’s... See full summary »\\nA young\nman has just killed a drunken girl in a subway station.\npl−ps sex for the ﬁrst and last times in The Girl with the Dragon\nTattoo.\\nI don’t remember that in the book.\\nIt’s implied.\nShe says something like \"I’ve never done this before\"\nwhen they’re having sex for the ﬁrst time. And when\nshe’s having sex with her boyfriend for the last, she says\nsomething along the same lines.</s>\nher face ripped open in The Girl with the Dragon Tattoo.\nThat one still disturbs me. The third movie not as much,\nbut that scene was still disturbing</s>\nTable 5: Generated examples with greedy decoding and nucleus sampling under different conﬁgurations. The\nprompt is A girl (Lisbeth Salander) has.\n13732\nDist. Greedy Search Nucleus Sampling\nNow in private practice together, ... Now in private practice together, ...\nps−pl I’m going through the same process. I’ve never had\nany issues.\\nI’ve had the same issue too! I’m trying\nnot get into any of the practices because it’s so hard.\nBut now I can’t do anything because of it :( I’m hoping\nto do something with my time and money to get some\nhelp!\\nThat’s really sad! Hopefully I can get some help!\nI hope to get some advice from someone who knows\nhow to help me out, and that they\nI can conﬁrm it works pretty perfectly on My\"EBM Used\nby me if I ever need\\nGreat news :)</s>\nps I’m a big fan of the \"I’m a big fan of the \"I’m a big fan\nof the \"I’m a big fan of the \"I’m a big fan of the \"I’m a\nbig fan of the \"I’m a big fan of the \"I’m a big fan of the\n\"I’m a big fan of the \"I’m a big fan of the \"I’m a big fan\nof the \"I’m a big fan of the \"I’m a big fan\na ﬁrm working on management strategies for retailing\nfor software, designing, and engineering complex health-\ncare facilities, and leading multi-channel providers in\naddition to providing a variety of consulting services. Ex-\nperience in all stages of PR is critical to have.\\n\\nThis 3.3\nyear-term contract includes a wide range of consultant\ntraining, including training for email and email integra-\ntion, and three-way calls for projects.\\n\\nAt Microsoft\nHealthcare, we have a broad selection of technical lead-\nership and support teams for our healthcare\nps+pl I have the pleasure of working with a number of clients\nwho have been referred to me by my colleagues. I have\nbeen able to help them with their legal issues and I have\nbeen able to help them with their personal issues.\\n\\nI\nhave been able to help them with their legal issues and\nI have been able to help them with their personal is-\nsues.\\n\\nI have been able to help them with their legal\nissues and I have been able to help them with their per-\nsonal issues.\\n\\nI have\nFather Harry Thomas, a faculty member at Canisius\nCollege, and Father Christopher Cooney, pastor at Holy\nRedeemer Church in Lancaster, are a good team. The\ntwo have collaborated on two traditional healing classes\nfor children since the spring of 2016. Their latest effort,\nfollowed by Father John Clifford, pastor at Christ the\nKing Church in Canisius, has taken the call of mercy\nto the study level. Beginning September 24, Christ the\nKing Church, Canisius, will host “Pope\npl Dr. David and Dr. David are a husband and wife team\nof chiropractors who specialize in the treatment of back\npain, neck pain, headaches, and other musculoskeletal\nproblems. They are dedicated to providing the highest\nquality of care to their patients in a comfortable, friendly,\nand professional environment.\\n\\nDr. David is a gradu-\nate of the Palmer College of Chiropractic in Davenport,\nIowa. He has been practicing in the greater San Diego\narea since 1995. He\nSpencer and Field with many years of combined practice\nare passionate about delivering high quality health care\nto the people of Texas. \"Our mission is to empower\nyou and your family to reach your health and wellness\ngoals through nutritional and lifestyle changes. We take\na whole-family approach to care and believe that true\nhealth is created from the inside out. If you’re ready to\nfeel better, we want to be part of your journey\"</s>\npl−ps Drs. Michael J. Gazzaniga and David A. Eagleman have\nwritten a new book that explores what they believe are\nsome fundamental mysteries of the human mind. In The\nBrain: The Story of You, they argue that the brain is\nnot just the seat of our thoughts and emotions but also\nof who we are as people.\\n\\nIn this excerpt from the\nintroduction, the authors explain why they wrote the\nbook and what they hope readers take away.\\nThe Brain:\nThe...</s>\nthe pair focus their legal expertise on helping immi-\ngrant families and individuals resolve a wide range im-\nmigration matters, including deportation defense, asy-\nlum, naturalization (citizenship), removal defense, con-\nsular processing (visas), V AW A petitions (domestic vio-\nlence) as well as deportation and removal proceedings,\nappeals and motions before immigration court, admin-\nistrative motions in immigration court, removal orders\nand waivers of inadmissability. Both attorneys are ad-\nmitted to the Maryland State Bar as well as the District\nof Columbia Court of appeals\nTable 6: Generated examples with greedy decoding and nucleus sampling under different conﬁgurations. The\nprompt is Now in private practice together,.\n13733\nanachronisms analogical_similarity analytic_entailment\nauthorship_verification causal_judgment cause_and_effect\ncode_line_description common_morpheme conceptual_combinations\ncrash_blossom crass_ai cryobiology_spanish\ndark_humor_detection date_understanding disambiguation_qa\ndiscourse_marker_prediction emoji_movie empirical_judgments\nenglish_russian_proverbs entailed_polarity entailed_polarity_hindi\nevaluating_information_essentiality fantasy_reasoning figure_of_speech_detection\nhhh_alignment hinglish_toxicity human_organs_senses\nidentify_math_theorems identify_odd_metaphor implicatures\nimplicit_relations intent_recognition international_phonetic_alphabet_nli\nirony_identification kannada key_value_maps\nknown_unknowns logical_args logical_sequence\nmathematical_induction metaphor_boolean metaphor_understanding\nmisconceptions misconceptions_russian moral_permissibility\nmovie_recommendation nonsense_words_grammar odd_one_out\npenguins_in_a_table periodic_elements persian_idioms\nphrase_relatedness physical_intuition physics\npresuppositions_as_nli riddle_sense ruin_names\nsalient_translation_error_detection sentence_ambiguity similarities_abstraction\nsimple_arithmetic_json_multiple_choice simple_ethical_questions snarks\nsocial_support sports_understanding strange_stories\nsuicide_risk swahili_english_proverbs symbol_interpretation\nunderstanding_fables undo_permutation unit_interpretation\nwhat_is_the_tao which_wiki_edit\nTable 7: The list of multiple-choice tasks we use from BIG-Bench. Clicking the name of a task will direct you to\nthe task’s GitHub page.\n13734\ndate_understanding\nQ: Yesterday, Jan 21, 2011, Jane ate 2 pizzas and 5 wings. What is the date tomorrow in MM/DD/YYYY?\nA: 01/23/2011\nQ: It is 4/19/1969 today. What is the date yesterday in MM/DD/YYYY?\nA: 04/18/1969\nQ: Yesterday was April 30, 2021. What is the date today in MM/DD/YYYY?\nA:\nOptions: 05/01/2021,02/23/2021,03/11/2021,05/09/2021,06/12/2021\nnonsense_words_grammar\nQ: How many things does the following sentence describe? The balforator, heddleilwilder and the\nsminniging crolostat operate superbly and without interrtulation.\nA: 3\nQ: How is the quijerinnedescribed in the next sentence? The umulophanitc quijerinne eriofrols the dusty\ngrass.\nA: umulophanitc\nQ: Which word in the following sentence is a verb? The grilshaws bolheavened whincely.\nA:\nOptions: The, grilshaws, bolheavened, whincely\nentailed_polarity\nGiven a fact, answer the following question with a yes or a no.\nFact: Ed grew to like Mary. Q: Did Ed like Mary?\nA: yes\nGiven a fact, answer the following question with a yes or a no.\nFact: They did not condescend to go. Q: Did they go?\nA: no\nGiven a fact, answer the following question with a yes or a no.\nFact: The report was admitted to be incorrect. Q: Was the report incorrect?\nA:\nOptions: yes, no\nsentence_ambiguity\nClaim: Delhi is not the only Hindi-speakingstate in India.\nTrue or False? True\nClaim: The population of the second-largest country in the world in 2021 exceeds the population of the\nthird, fourth, and ﬁfth largest countries combined.\nTrue or False? True\nClaim: Pescatarians almost never consume vegetarian food.\nTrue or False?\nOptions: True, False\nTable 8: Examples of prompts and answer options for four BIG-Bench multiple-choice tasks.\n13735\nLinearity Tasks\ndate_understanding fantasy_reasoning figure_of_speech_detection\nhhh_alignment implicit_relations intent_recognition\nmisconceptions similarities_abstraction simple_ethical_questions\nstrange_stories undo_permutation nonsense_words_grammar\nBreakthroughness Tasks\ncode_line_description human_organs_senses phrase_relatedness\nswahili_english_proverbs what_is_the_tao implicatures\nTable 9: The list of linearity and breakthroughness tasks.\n13736\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nsection 8\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nsection abstract and section 1\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nI used copilot to generate image captions and complete sentences throughout the paper, but all the\ngenerated texts have been heavily edited.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nsection 2\n□\u0013 B1. Did you cite the creators of artifacts you used?\nsection 2\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe use internal data from the organization.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nsection 2\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThe data we use consists of a collection of open-sourced language modeling datasets, though the\nsplit is used internally, the contents should be largely observed by other researchers.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nsection 2\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nsection 2\nC □\u0013 Did you run computational experiments?\nsection 3, 4, 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nsection 2 and Appendix A\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n13737\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nsection 3, 4, 5\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n13738"
}