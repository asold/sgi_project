{
  "title": "Swin Unet3D: a three-dimensional medical image segmentation network combining vision transformer and convolution",
  "url": "https://openalex.org/W4320728002",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2481776311",
      "name": "Yimin Cai",
      "affiliations": [
        "Guiyang Medical University",
        "Guizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2259059565",
      "name": "Yuqing Long",
      "affiliations": [
        "Zunyi Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2504206652",
      "name": "Zhenggong Han",
      "affiliations": [
        "Guizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2148229402",
      "name": "Mingkun Liu",
      "affiliations": [
        "Guiyang Medical University",
        "Guizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2102236337",
      "name": "Yuchen Zheng",
      "affiliations": [
        "Guizhou University",
        "Guiyang Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A1987581894",
      "name": "Wei Yang",
      "affiliations": [
        "Guizhou University",
        "Guiyang Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2103636590",
      "name": "Liming Chen",
      "affiliations": [
        "Guizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2481776311",
      "name": "Yimin Cai",
      "affiliations": [
        "Guizhou University",
        "Guiyang Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2259059565",
      "name": "Yuqing Long",
      "affiliations": [
        "Zunyi Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2504206652",
      "name": "Zhenggong Han",
      "affiliations": [
        "Guizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2148229402",
      "name": "Mingkun Liu",
      "affiliations": [
        "Guiyang Medical University",
        "Guizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2102236337",
      "name": "Yuchen Zheng",
      "affiliations": [
        "Guizhou University",
        "Guiyang Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A1987581894",
      "name": "Wei Yang",
      "affiliations": [
        "Guiyang Medical University",
        "Guizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2103636590",
      "name": "Liming Chen",
      "affiliations": [
        "Guizhou University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035665735",
    "https://openalex.org/W2100860054",
    "https://openalex.org/W1993456370",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W2535388113",
    "https://openalex.org/W6604044515",
    "https://openalex.org/W3014974815",
    "https://openalex.org/W4253570578",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W2741891296",
    "https://openalex.org/W2959736198",
    "https://openalex.org/W6600213211",
    "https://openalex.org/W6602254124",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W6600595061",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6608687280",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W4283080861",
    "https://openalex.org/W2751069891",
    "https://openalex.org/W1641498739",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W6604344240",
    "https://openalex.org/W6819060087",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W6912515347",
    "https://openalex.org/W6912412674",
    "https://openalex.org/W2127890285",
    "https://openalex.org/W3003257820",
    "https://openalex.org/W3112701542",
    "https://openalex.org/W3103145119"
  ],
  "abstract": null,
  "full_text": "Cai et al. \nBMC Medical Informatics and Decision Making           (2023) 23:33  \nhttps://doi.org/10.1186/s12911-023-02129-z\nRESEARCH\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nOpen Access\nBMC Medical Informatics and\nDecision Making\nSwin Unet3D: a three-dimensional medical \nimage segmentation network combining vision \ntransformer and convolution\nYimin Cai1†, Yuqing Long2†, Zhenggong Han3, Mingkun Liu1, Yuchen Zheng1, Wei Yang1* and Liming Chen4* \nAbstract \nBackground Semantic segmentation of brain tumors plays a critical role in clinical treatment, especially for three-\ndimensional (3D) magnetic resonance imaging, which is often used in clinical practice. Automatic segmentation of \nthe 3D structure of brain tumors can quickly help physicians understand the properties of tumors, such as the shape \nand size, thus improving the efficiency of preoperative planning and the odds of successful surgery. In past decades, \n3D convolutional neural networks (CNNs) have dominated automatic segmentation methods for 3D medical images, \nand these network structures have achieved good results. However, to reduce the number of neural network param-\neters, practitioners ensure that the size of convolutional kernels in 3D convolutional operations generally does not \nexceed 7 × 7 × 7 , which also leads to CNNs showing limitations in learning long-distance dependent information. \nVision Transformer (ViT) is very good at learning long-distance dependent information in images, but it suffers from \nthe problems of many parameters. What’s worse, the ViT cannot learn local dependency information in the previous \nlayers under the condition of insufficient data. However, in the image segmentation task, being able to learn this local \ndependency information in the previous layers makes a big impact on the performance of the model.\nMethods This paper proposes the Swin Unet3D model, which represents voxel segmentation on medical images as \na sequence-to-sequence prediction. The feature extraction sub-module in the model is designed as a parallel struc-\nture of Convolution and ViT so that all layers of the model are able to adequately learn both global and local depend-\nency information in the image.\nResults On the validation dataset of Brats2021, our proposed model achieves dice coefficients of 0.840, 0.874, and \n0.911 on the ET channel, TC channel, and WT channel, respectively. On the validation dataset of Brats2018, our model \nachieves dice coefficients of 0.716, 0.761, and 0.874 on the corresponding channels, respectively.\nConclusion We propose a new segmentation model that combines the advantages of Vision Transformer and Con-\nvolution and achieves a better balance between the number of model parameters and segmentation accuracy. The \ncode can be found at https:// github. com/ 11525 45264/ SwinU net3D.\nKeywords Deep learning, Medical image segmentation, 3D Swin Transformer, Brain tumor\n†Yimin Cai and Yuqing Long have contributed equally to this work\n*Correspondence:\nWei Yang\nvyang@gzu.edu\nLiming Chen\nlmchen@gzu.edu\n1 School of Medical, Guizhou University, Guiyang, China\n2 School of Stomatolog, ZunYi Medical University, Zunyi, China\n3 Key Laboratory of Advanced Manufacturing Technology of Ministry \nof Education, Guizhou University, Guiyang, China\n4 Guiyang Dental Hospital (Dental Hospital of Guizhou University), \nGuizhou University, Guiyang, China\nPage 2 of 13Cai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \nBackground\nA brain tumor is an abnormal growth of cells in human \nbrain tissue. When the tumor grows gradually, it presses \non the affected nerves, causing a series of adverse symp -\ntoms and even threatening life. Brain tumors are medi -\ncally classified into benign and malignant tumors, the \nlatter of which are further classified into primary and \nmetastatic brain tumors [1]. Primary brain tumors are \ninitially lesion cells inside human brain tissue, while \nmetastatic brain tumors are caused by cancer cells from \nother tissues that metastasize to the brain. Metastatic \nbrain tumors are more common than primary brain \ntumors, and about half of metastatic tumors come from \nlung cancer [1]. Under the current scientific and tech -\nnological conditions, it is no longer difficult to obtain \nthree-dimensional (3D) imaging of brain tissue, whether \nby computed tomography (CT) or magnetic resonance \nimaging (MRI), but the problem mainly lies in how to \nreliably and quickly identify the tumor lesion area in the \nimaging and obtain the 3D spatial structure of the tumor \nso as to improve the efficiency of preoperative planning \nand the odds of successful surgery. One solution is to rely \non doctors specialized in imaging to identify and outline \nthe lesion areas in the images, but this method is too inef-\nficient and requires a lot of time and labor. Because of the \nhigh computing speed of computers, it is a good choice \nto automate medical image segmentation with the help of \nimage segmentation techniques in computer vision.\nImage segmentation is the grouping of each pixel in an \nimage into a certain category. This is the basis for under -\nstanding the concept of a scene [2]. Image segmentation \nplays a pivotal role in medical image analysis. Image seg -\nmentation can automatically outline the structure of dis -\neased or other target tissues, providing information for \nsubsequent diagnosis and treatment by physicians. Early \nmedical image segmentation algorithms mainly include \nthreshold-based segmentation algorithms [3], region-\nbased segmentation algorithms [4], wavelet analysis and \ntransform-based segmentation algorithms [5], Markov \nrandom field-based-based segmentation algorithms [6], \nand genetic algorithm-based segmentation algorithms \n[7]. With the improvement of hardware capability and \nthe development of deep convolutional neural networks \n(CNNs), CNN has achieved dominance in the field of \ncomputer vision. The seminal U-net [8] network was \nproposed and achieved impressive results in the field of \ntwo-dimensional (2D) medical image segmentation. The \nclassical Unet [8, 9] network consists of an encoder and \na decoder; the encoder extracts image features using a \nseries of convolutional and downsampling layers, while \nthe decoder uses a series of transposed convolutional \nlayers [10, 11] to upsample image features to the origi -\nnal image resolution for semantic prediction of pixels, in \naddition to incorporating the image features extracted by \nthe encoder during the upsampling process to reduce the \ninformation loss during downsampling. Given the sim -\nple structure and superior performance of U-Net, schol -\nars have subsequently proposed various variants of the \nU-Net model [9, 12–17].\nGiven the powerful fitting capability of such CNN-\nbased neural networks, they have achieved superior \nresults in various medical image segmentation tasks \nso far. However, in order to improve the computational \nspeed of the model and reduce the number of parameters \nin the model, we must ensure that the maximum size of \nthe convolutional kernels in most models does not exceed \n3 × 3 × 3 , such as 3D-Unet [9], DenseVoxNet [18], and \n3D U 2 − Net [19]. There are also some networks that \nuse 5 × 5 × 5 convolutional kernels, such as V-Net [14], \nand some networks that use 7 × 7 × 7 convolutional ker-\nnels, such as ResNetMed3D [20]. The receptive field of \nCNN-based methods is highly localized due to the limi -\ntation of the fixed receptive field of the convolutional ker-\nnel. Although the perceptual field of these methods can \ngradually increase in higher layers, they still cannot fully \nlearn long-range dependent information in lower layers \n[21, 22]. Nevertheless, such long-distance dependencies \nare crucial for accurate segmentation of tissue structures \nin medical images [19]. Some studies have attempted to \novercome the CNN’s inability to acquire remote depend -\nencies through techniques such as dilation convolution \n[23], the spatial attention mechanism, the channel atten -\ntion mechanism [24], and the image pyramid [25], but \nthese techniques still have some limitations. Inspired by \nthe great success of Transformer [26] in the field of natu -\nral language processing, some scholars have introduced \nTransformer into the field of computer vision and have \nproposed Vision Transformer (ViT) [27] and Swin Trans-\nformer [28]. Dividing 2D images into image blocks and \ncombining them with positional encoding, ViT achieves \ncomparable performance with the CNN on large data -\nsets. Swin Transformer proposes a window attention \nmechanism and a cyclic moving window attention \nmechanism to solve the problem of high time complex -\nity in the computation of ViT. Based on the robust per -\nformance achieved by ViT on image classification tasks, \nsome scholars have introduced it to image segmentation \ntasks. Swin-Unet [29] is a pure Transformer network \nstructure, where the encoder and decoders are composed \nof Transformers. However, Swin-Unet is a model for 2D \nmedical image segmentation, which is not applicable to \nvoxel segmentation of 3D medical images unless a lot of \nadditional work has been performed or some complex \nadaptation code has been written. TransUnet [21] and \nTransBTS [30] are a kind of hybrid model in combining \nCNN and Transformer, using successive convolutional \nPage 3 of 13\nCai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \n \nlayers and Transformer in the encoder for feature extrac-\ntion and transposed convolution for upsampling opera -\ntions in the decoder to recover spatial resolution for \nsemantic segmentation. In the UnetR [31] and SwinBTS \n[32] structure, the authors used the Transformer to ini -\ntially extract the image features while using the CNN as \nthe backbone network in both its encoders and decoders.\nAmong the existing models, segmentation models \nimplemented entirely based on CNNs have significant \nlimitations in modeling long-distance dependent infor -\nmation in images, while segmentation models based \nentirely on ViT are unable to learn low-level detail infor -\nmation in images well [21, 22]. There are also some mod -\nels that combine the advantages of ViT and CNN to \nmodel both long-distance dependent information and \nshort-distance dependent information in images, but \nthese models have a large number of model parameters \nand high computational time complexity due to the influ-\nence of ViT [28].\nBased on the above state of affairs and inspired by the \nwork of Swin Transformer [28] and Swin Unet [29], we \npropose a new segmentation model, Swin Unet3D, for \nvoxel segmentation of 3D medical images. Our model \nis proposed after consideration of the advantages and \ndisadvantages of Swin Transformer and the CNN, and \nwithout degrading the model modeling capability, our \nmodel only adds additional linear time complexity com -\npared to CNN-based segmentation models. Our model \nconsists of an encoder, a decoder, and a jump connec -\ntion, as shown in Fig.  1. A 3D medical image with a reso -\nlution of H × W × D is divided into non-overlapping \nvoxel patches of size 4 × 4 × 4 . Each voxel patch is then \nflattened by a fully connected layer and encoded as a \n96-dimensional vector. Each vector is considered as a \ntoken, and these obtained H\n4 × W\n4 × D\n4  tokens are then \nfed into a transformer-based encoder for image feature \nextraction. The image features extracted by the four \nencoders are sent to the decoder for upsampling, which \nrecovers the spatial resolution of the image and gradually \nfuses them with the features extracted by the encoders to \ncomplete the semantic segmentation of the image using \na jump connection. The experiments that have been con -\nducted on the Brats2021 challenge dataset [33–35] and \nthe Brats2018 challenge dataset [33–35] show that our \nFig. 1 The architecture of the Swin Unet3D\nPage 4 of 13Cai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \nmodel achieves a good balance in terms of segmentation \naccuracy and the number of model parameters.\nOur contributions can be summarized as follows: (1) \nbased on the idea of Swin Transformer, we first imple -\nmented Swin Transformer Block3D, a module that can \nextract features in 3D medical images like Convolu -\ntion3D; (2) we proposed ViT and CNN parallel struc -\ntures. The Swin Transformer-based Swin Block3D \nmodule is responsible for learning long-distance depend -\nency information in images, while the CNN-based \nConv Block3D is responsible for learning short-distance \ndependency information in images, and at the end of \neach decoder, the model performs feature fusion of the \nimage features extracted by these two modules; (3) the \nresults of ablation experiments show that in an intensive \nprediction task like image segmentation, the ViT struc -\nture and the Convolution structure combined can com -\npensate for each other’s shortcomings.\nMethods\nImplementation details of Swin Unet3D\nArchitecture overview\nFigure 1 presents an overview of Swin Unet3D, a model \nconsisting of an encoder, a jump connection, and a \ndecoder. The Patch Merging3D module is mainly used \nfor image downsampling, while the Swin Transformer \nBlock3D module and the Conv Block3D module are \ndesigned to extract image features. Specifically, Swin \nBlock3D is employed to learn the long-range dependency \ninformation in the image, and Conv Block3D is adopted \nto learn local dependency information in the image. \nThe patch Expanding3D module is used for upsampling \nto recover the spatial resolution of the image. A Patch \nMerging and several Swin Block3D are stacked to form \na Down Stage, while a Patch Expanding3D and several \nSwin Block3D are stacked to form an Up Stage.\nThe input 3D image is first chopped into multiple \n4 × 4 × 4 voxel blocks, and then each voxel block is flat -\ntened into a one-dimensional vector of length 64. Finally, \nthese one-dimensional vectors are linearly transformed \nand their length is changed to N. After completing the \nabove steps, the input image is encoded as H\n4 × W\n4 × D\n4  \ntokens, each tokens is a one-dimensional vector of length \nN. Referring to the original Swin Transformer [28], N can \nbe set to 96. These tokens are further fed into the Conv \nBlocks3D module and Swin Block3D module for extract -\ning the features of the image. The Patch Expanding3D \nmodule is used in the decoder to recover the spatial res -\nolution of the feature vector, and the Swin Block3D and \nConv Block3D modules are used to continue the feature \nextraction. After stacking multiple decoders, the spa -\ntial resolution of the feature map can be restored to the \ninput spatial resolution, and the pixel-level segmentation \nprediction of the input image can be obtained by apply -\ning a linear change to the last layer of the feature map.\nEach encoder or decoder contains a multi-header \nattention mechanism layer. According to the design of \nSwin Unet [29], the number of multi-headed attention \nmechanisms used by Encoder12 ,Encoder3 ,Encoder4 , \nand Encoder5 are 3,6,9,12 respectively, and the num -\nber of multi-headed attention mechanisms used by \nDecoder4 ,Decoder3 ,andDecoder12 is 9,6,3 respectively. \nThe number n of Swin Blocks3D contained in each \nEncoder in Fig.  1 is 2,2,4,2 from Encoder12 to Encoder5 , \nand 4,4,2 from Decoder4 to Decoder12 , respectively.\nMulti‑head window self‑attention3D\nThe window multi-head self-attention3D (SW-MSA3D) \nmodule divides the input tokens into multiple sub-\nwindows. The window size is specified according to the \nspatial resolution of the input image, which is generally \nequal to the spatial resolution of the input image divided \nby 32. The reason for using this design is that the images \nare downsampled four times during the encoding pro -\ncess, and the downsampling factors are 4, 2, 2, and 2. To \navoid misalignment of feature map dimensions during \nthe fusion using feature map information, it is necessary \nto set the spatial resolution of the input image to a mul -\ntiple of 32. Also, to be able to divide the input image into \nmultiple windows of the exact same size, we must be able \nto integer divide the input in each dimension by the size \nof the window in the corresponding dimension (Fig. 2).\nLike the two-dimensional W-MSA, W-MSA-3D com -\nputes a multi-head self-attention mechanism for each \nwindow. It computes the similarity between the tokens \nin each window, and we adapted the idea of SW-MSA \nimplementation in Video Swin Transformer [36] in the \nprocess of our implementation. However, as W-MSA-\n3D only calculates the similarity between tokens within \nthe same window, it lacks the information interaction \nbetween windows. To solve this problem, the SW-MSA-\n3D mechanism is introduced. The input is cyclically \nshifted by s units in each dimension, where the value of s \nmust be smaller than the window size in the correspond -\ning dimension, and the default value of s is half of the \nwindow size with reference to the setting of the original \npaper [28]. However, the circular shifted-window mecha-\nnism leads to two more problems: (1) an increase in the \nnumber of windows; (2) inconsistent window sizes. Some \ntokens originally located in non-adjacent windows are in \nthe same window after cyclic shifting, and the similarity \ngenerated between these tokens should be filtered out \nwhen calculating the self-attention inside the window, so \nwe introduce the window-masking mechanism. Figure 3a \nshows the normal method of calculating the window self-\nattention, which only calculates the similarity between \nPage 5 of 13\nCai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \n \nFig. 2 Overview of the structure of some sub-modules: a swin Block3D, b Conv Block3D, c Patch Merging3D, d Patch Expand3D\nFig. 3 The schematic diagram of a W-MSA-3D and b SW-MSA-3D with a window size of 2. The tokens of the same color in a belong to the same \nwindow, and we only calculate the self-attention within each window. To obtain the dependency information interaction between adjacent \nwindows, we divide some tokens within neighboring windows into the same window after cyclic shifting, and only tokens satisfying these \nconditions are allowed to calculate the window self-attention between them. Other tokens that do not satisfy the condition are shielded from \nattention between them by a masking mechanism even if they belong to the same window after a circular shift, as shown in b \nPage 6 of 13Cai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \ntokens of the same color, as they are in the same window. \nFigure 3b depicts the cyclic shifted-window mechanism. \nTokens that are originally in adjacent windows (patches \nwith the same color in Fig.  3b) are in the same window \nafter cyclic shifting, and the similarity between them \ncan be calculated. The similarity between tokens that are \nnot in adjacent windows (patches with different colors \nin Fig.  3b) should be filtered out even if they are in the \nsame window after the cyclic shifting. The inclusion of \nthe cyclic shifted-window mechanism can introduce \ninformation interaction between neighboring windows \nwith the cost of only increasing the linear computational \ncomplexity, thus allowing the neural network to learn \nlong-distance dependency information. The process of \ncalculating the attention between tokens in each window \ncan be described by the following formula:\nEach ofW Q,W K,W V is a parameter-learnable square array \nwith the same dimensionality, dk is the dimensionality of \nK, X is the matrix of tokens in the same window, and attn \nis the similarity between tokens.\nConv Block3D\nThe Conv Block3D module, shown in Fig.  2b, is stacked \ntwice in the order of 1 × 1 × 1 convolutional layers, Lay -\nerNorm [37] layers, and PRelu [38] layers, and it is mainly \nresponsible for learning the local dependencies of the \nimages. The computational process of this module can be \ndescribed as follows:\nwhere X denotes the input of Conv Block3D, Y indicates \nthe output of Conv Block3D, and X t denotes the inter -\nmediate temporary variable. In order to avoid the extra-\nlarge computational effort caused by this module, we use \nthe depth-wise separable convolution [ 39] instead of the \nnormal convolution. This sub-module was designed to \n(1)Q = W Q × X\n(2)K = W K × X\n(3)V = W V × X\n(4)attn= Softmax Q × K T\ndk\n× V\n(5)X t = PReLu1(LN 1(Conv3D 1(X )))\n(6)X t = PReLu2(LN 2(Conv3D 2(X t)))\n(7)Y = X t × X\nensure that the model can better fit the detailed informa -\ntion in the image and draws on the implementation in \nVAN [40] so that multiplication rather than addition is \nused in performing the feature convergence of X t and X.\nSwin Block3D\nThe composition of Swin Block3D is shown in Fig.  2a, \nand its design idea is derived from the Block module in \nSwin Transformer [28]. It consists of two basic units: The \nfirst unit consists of a LayerNorm (LN) layer, window \nmulti-Head self-attention3D (W-MSA-3D) module, a \nLayerNorm layer, and an MLP module in order of succes-\nsive composition; the second unit uses the shifted-win -\ndow multi-head self-attention3D (SW-MSA-3D) module, \nwhich replaces the W-MSA-3D module in the first cell, \nand the rest of the structure is the same as that of the first \nunit. The whole calculation process of Swin Block3D can \nbe described by the following mathematical equation:\nThe X t1 and X t2 are temporary variables used to facilitate \nthe description of these formulas.\nThe input of each encoder or decoder is some feature \nmaps, but the Self Attention module in Swin Block3D \nneeds to divide the feature maps into voxel patches, and \nthen turn each voxel patch into a one-dimensional token \nin order to calculate the self-attention. After the self-\nattentive calculation, each token needs to be converted \ninto the corresponding voxel patches, and finally, these \nvoxel patches are stitched into the feature map. In gen -\neral, the conversion between token and voxel patches is \nthe dimensional transformation of the matrix. A voxel \npatch of dimension [ h, w, d] is flattened to a one-dimen -\nsional token of length h × w × d , and a token of length \nh × w × d can be transformed to a voxel patch of dimen -\nsion [ h,  w,  d] by matrix dimension transformation. We \nused the class named Rearrange in the einops [ 41] library \nto convert tokens and feature maps to each other.\nPatch Merging3D and Patch Expanding3D\nThe function of the Patch Merging3D module mainly \nincludes reducing the image spatial resolution and \nincreasing the number of image channels. The structure \nof the Patch Expanding3D module, shown in Fig.  2c, is \nrelatively simple, containing only a Conv3D layer and a \n(8)Xt1 = LN1(XL−1) + W - SA-3 D(LN1(XL−1))\n(9)Xt = LN2(Xt1 ) + MLP (LN2(Xt1 ))\n(10)Xt2 = LN3(Xt) + SW - MSA-3 D(LN3(Xt))\n(11)XL = LN4(Xt2 ) + MLP (LN4(Xt2 ))\nPage 7 of 13\nCai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \n \nLayerNorm [37] layer, while the functions of the Patch \nExpanding3D module are exactly the opposite of those of \nthe Patch Merging3D module, mainly consisting of grad -\nually restoring the image spatial resolution and reducing \nthe number of image channels. As shown in Fig.  2d, this \nmodule contains a ConvTranspose3d layer and a Layer -\nNorm layer.\nThe reason why these two modules use LayerNorm [37] \ninstead of BatchNorm is as follows: 3D images generally \noccupy a relatively large amount of GPU memory, so dur-\ning the experiment, the BatchSize parameter generally \ncannot be too large, otherwise, it will easily cause GPU \nmemory overflow. However, in the case of BatchSize is \nrelatively small using BatchNorm is not very meaningful.\nEncoder and decoder\nThe internal details of the Encoder and Decoder are \nshown in Fig.  1. Each Encoder consists of a Patch Merg -\ning3D sub-module, a Conv Blocks3D sub-module, and \nmore than one Swin Blocks 3D sub-module. The Patch \nMerging3D sub-module outputs a temporary feature \nimage ( X t ) after downsampling the input image or the \nfeature image output from the previous Encoder. Both \nthe Conv Blocks3D sub-module and the Swin Blocks3D \nsub-module get X t as input, where the Conv Blocks3D \nsub-module is used to learn short-distance dependency \ninformation in X t , while the Swin Blocks3D sub-module \nis used to learn long-distance dependency information \nin X t . A matrix addition operation on the output of the \nConv Blocks3D and Swin Blocks3D sub-modules yields \nthe feature image of this Encoder output.\nThe internal composition of the Decoder is similar to \nthat of the Encoder, with the difference that there are two \ninputs given to the Decoder. The first input is the feature \nimage output by the Encoder of the same level, and the \nsecond input is the feature image output by the Decoder \nof the previous level or the feature image output by the \nEncoder5 . The first input is added mainly to introduce the \nresidual connection to avoid the gradient vanishing in the \nback-propagation process [42].\nEvaluation metrics\nFor each segmentation task, we used the dice coeffi -\ncient [14] as an evaluation criterion, which is defined as \nfollows:\nwhere X is the prediction result of the models, Y is the \nground truth, TP refers to the correctly classified tumor \nvoxels, FN refers to the correctly classified non-tumor \nvoxels, and FP refers to those voxels that are determined \nto be tumors by the model but are non-tumors in the \n(12)Dice= 2�X ⋂Y �\n�X �+� Y � = 2TP\n2TP + FP + FN\nground truth. The dice coefficient is used to measure the \nsimilarity between the model prediction and the ground \ntruth (GT), and its value ranges from 0 to 1. The closer \nthe dice coefficient is to 1, the closer the prediction is to \nthe GT.\nExperiments\nExperimental conditions configuration\nSwin Unet3D was implemented on Python 3.7.9, PyTorch \n1.9.0 [43], and einops 0.3.2 [41]. We used an RTX3090 \ngraphics card and an NVIDIA A100 graphics card to \ncomplete these experiments. To accelerate the train -\ning process of the model, we used the hybrid precision \nprovided in PytorchLightning [44] for model training, \ninferences, and gradient accumulation techniques, thus \ndisguising the expansion of the BatchSize. In all experi -\nments, we used the Monai [45] framework to complete \nthe preprocessing of the input images.\nExperiment on the dataset of Brats2021 Challenge\nThe Brats2021 Challenge [33–35] dataset contains a \ntotal of 2000 MRI scans of glioma patients, with a train -\ning dataset size of 1251. Each patient’s MRI scan con -\ntains four contrasts: native T1-weighted, post-contrast \nT1-weighted (T1-GD), T2-weighted, and T2 Fluid-Atten-\nuated Inversion Recovery (T2-Flair) images. All MRI \nscans have an image size of 240 × 240 × 155 after inter -\npolation to a resolution of 1 mm3 . Segmentation annota -\ntions for all of the patients included the GD-enhanced \ntumor (ET-Label 4), peritumoral edema/infiltrating tissue \n(ED-Label 2), and necrotic tumor core (NCR-Label 1).\nTo obtain better segmentation results and faster con -\nvergence of the model, we converted the multi-class \nlabels into a multi-label segmentation task in the one-\nhot format, as follows: Label 2 was used to construct the \nenhancing tumor(ET), label 2 and label 4 were combined \nto construct the Tumor core(TC) channel, and label 1, \nlabel 2, and label 4 were merged to construct the Whole \ntumor (WT). The merge operation was implemented by a \nlogical OR operation.\nAll models were trained and validated using the same \nhyperparameters, except for some individual model-spe -\ncific hyperparameters, as detailed below. The data set was \ndivided into training and validation sets in the ratio of \n0.8:0.2, using a random seed of 42. The validation set was \ndivided at the beginning of the training phase and was \nnot involved in the training process. To make all experi -\nmental results reproducible, we also used random seeds \nwith values equal to 42 for initializing the environments \nof PyTorch [43], PyTorch-lightning [44], Monai [45], and \nCuda.\nAll models use the AdamW [47] optimizer and the \nDiceLoss [14] loss function, with the Batchsize being set \nPage 8 of 13Cai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \nto 1 and then disguised to expand the BatchSize to 16 \nusing the gradient accumulation technique in Pytorch -\nLightning [44]. In the training stage, a region of size \n128 × 128 × 128 is randomly cropped from the original \nimage and fed into the model for training. In the valida -\ntion stage, we use the sliding window inference technique \nprovided by Monai [45], with a sliding window size of \n[128, 128, 128] and an overlap region overlap parameter \nof 0.125. All models use the same learning rate of 3e-4. \nWe also use the early stop technique in Pytorch-Light -\nning [44] to avoid overfitting the model on the training \ndataset. Figure  4 provides a comparison of the segmen -\ntation results among Ground Truth and the individual \nmodels.\nExperiments on the dataset of Brats2018 Challenge\nWe also complete comparative experiments on Swin \nUnet3D and other models on the validation dataset of \nBrats2018 [33–35]. Brats2018 has the same segmentation \nregions and targets as Brats2021, but the dataset size is \ndifferent. The training dataset for Brats2018 contains 285 \nMRI scans of patients with glioma.\nAnd the experimental results are shown in Table 2, The \nchange curves of relevant metrics can be obtained from \nFig.  6. In the training stage of this experiment, we crop \nrandom image blocks of size 128 × 128 × 128 from the \ninput images and feed them into the network for training. \nIn the inference stage, we use the sliding window body \nmechanism provided by the monai [45] framework, with \nthe roi_size parameter set to [128, 128, 128] and the over-\nlap parameter set to 0.125. In addition, the other param -\neters used in this experiment were completely inherited \nfrom Brats2021, which was done to test the fitting ability \nof Swin Unet3D more fully.\nResult and discussion\nResult analysis\nTo explore the fitting ability of our model, we completed \nmodel training and segmentation performance validation \non the datasets of Brats2018 and Brats2021. And we also \nconducted comparison experiments using 3D U-Net [9], \nV-Net [14], UnetR [31], TransBTS [30], SwinBTS [32], \nand AttentionUnet [16], with almost the same hyper-\nparameters for all experimental trials. As the number of \ntraining epochs increases, these models achieve the aver -\nage dice coefficients on the validation dataset as shown in \nFigs. 5 and 6. From Table  1, it can be seen that the aver -\nage dice coefficients achieved by our models on the vali -\ndation dataset of Brats2021 are 0.834 (ET channel), 0.866 \n(TC channel), and 0.905 (WT channel), respectively. \nFrom Table  2, it can be seen that our model achieved \naverage dice of 0.716 (ET channel), 0.761 (TC chan -\nnel), and 0.874 (WT channel) on the Brats2018 dataset, \nrespectively. Combining the results in Tables  1 and 2, we \ncan tentatively conclude that our model achieves a better \nbalance of model size and segmentation accuracy com -\npared to other models. To explore the fitting ability of our \nmodel, we completed model training and segmentation \nperformance validation on the datasets of Brats2018 and \nBrats2021. And we also conducted comparison experi -\nments using 3D U-Net [9], V-Net [14], UnetR [31], Trans-\nBTS [30], SwinBTS [32], and AttentionUnet [16], with \nalmost the same hyper-parameters for all experimental \ntrials. As the number of training epochs increases, these \nmodels achieve the average dice coefficients on the vali -\ndation dataset as shown in Figs.  5 and 6. From Table  1, \nit can be seen that the average dice coefficients achieved \nby our models on the validation dataset of Brats2021 are \n0.840 (ET channel), 0.874 (TC channel), and 0.911 (WT \nchannel), respectively. From Table  2, it can be seen that \nour model achieved average dice coefficients of 0.716 \n(ET channel), 0.761 (TC channel), and 0.874 (WT chan -\nnel) on the validation dataset of Brats2018, respectively. \nCombining the results in Tables  1 and 2, we can tenta -\ntively conclude that our model achieves a better balance \nof model size and segmentation accuracy compared to \nother models.\nDiscussion\nWe also tried to remove the Conv Blocks3D submod -\nule from Swin Unet3D, and only the Swin Block3D sub -\nmodule is used to complete the image feature extraction \nwork, getting a model called Swin Pure Unet3D. As can \nbe seen from Table  1 and Fig.  5, the average dice coef -\nficients achieved by Swin Pure Unet3D on the validation \ndataset of Brats021 are consistently lower than those of \nSwin Unet3D. Figure  7 was used to visualize the predic -\ntion results of Swin Pure Unet3D, which uses MRI scans \nof the same patient as in Fig. 5 from the Brats2021 valida-\ntion dataset. Although the difference between the aver -\nage dice coefficient achieved by Swin Pure Unet3D and \nSwin Unet3D on the Brats2021 validation dataset is no \nmore than 4.4%. This difference appears to be relatively \nsmall, however, it can be seen from Fig.  7 that many \nnoise points appear in the prediction results of Swin Pure \nUnet3D, in contrast, there is no such phenomenon in \nSwin Unet3D, and the prediction results of Swin Unet3D \nare closer to Ground Truth. It can be seen from Table  2 \nand Fig. 6 that on the validation dataset of Brats2018, the \ndifference between Swin Pure Unet3D and Swin Unet3D \nis much larger, with the largest dice coefficient difference \nreaching 7.7% (WT channel).\nWe also performed significance testing experi -\nments, and Table  3 depicts the significance testing \nresults on the validation dataset of Brats2021, while \nTable  4 shows the significance testing results on the \nPage 9 of 13\nCai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \n \nvalidation dataset of Brats2018. We first saved the \ndice coefficients obtained for each model on the vali -\ndation dataset to a CSV file, and then used the SciPy \n[48] library to perform significance analysis on the \ndice coefficients obtained for each model and the dice \ncoefficients obtained for Swin Unet3D. The analysis \ntransverse sagittal coronal3 d\nGround Truth\n3D U-Net\nV-Net\nUnetR\nTransBTS\nSwinBTS\nAttentionUnet\nSwin Unet3D\nFig. 4 The segmentation results of each model were visualized on the validation dataset of Brats2021 using the ITK-SNAP [46] software. The red \narea in the figure shows the necrotic tumor core (NCR—label 1), the green area shows the peritumoral edematous/invaded tissue (ED—label 2), \nand the yellow area shows the GD-enhancing tumor (ET—label 4)\nPage 10 of 13Cai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \nresults were kept in four valid digits. Together with \nthe results shown in Tables  3 and 4 , it can be seen that \nthere is a sizable significant difference between Swin \nPure Unet3D and Swin Unet3D on segmentation per -\nformance. This could, to some degree, indicate that the \nconvolutional module can compensate for ViT’s inabil -\nity to fit the image detail information well.\nFig. 5 Variation curves of metrics for measuring the model fitting ability on the validation dataset of Brats2021: a mean loss; b dice coefficient on \nET channel; c dice coefficient on TC channel; d dice coefficient on WT channel\nTable 1 Performance comparison of multiple models on the Brats2021 validation dataset and and the analysis of the significant \ndifferences between the performance of Swin Unet3D on the validation set and the performance of other models using the Wilcoxon \nsign test\nBold values indicate the best metrics\nModel name Params Params size Mean dice Significant difference\n(M) (MB) ET TC WT ET TC WT\n3D U-Net 7.9 15.834 0.825 0.844 0.900 Yes No Yes\nV-Net 45.6 182.432 0.815 0.840 0.751 No Yes Yes\nUnetR 102 204.899 0.842 0.853 0.905 Yes No Yes\nTransBTS 33.0 65.975 0.824 0.843 0.889 Yes Yes Yes\nSwinBTS 35.7 71.394 0.828 0.843 0.896 Yes Yes Yes\nAttentionUnet 23.6 47.257 0.841 0.851 0.870 No No No\nSwin Pure Unet3D 33.6 67.163 0.817 0.822 0.885 Yes Yes Yes\nSwin Unet3D 33.7 67.403 0.834 0.866 0.905 – – –\nPage 11 of 13\nCai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \n \nConclusion\nIn this paper, we proposed a new 3D medical image seg -\nmentation model by adding the Swin Block3D module \nbased on the Swin Transformer and Conv Block3D mod -\nule based on CNN to each decoder and encoder of the \nmodel.\nThe Swin Block3D sub-module based on ViT is \nresponsible for learning the global dependency infor -\nmation in the image, and the Conv Blocks3D sub-mod -\nule based on convolution is responsible for learning \nthe local dependency information of the image. Merg -\ning the dependency information learned by these two \ncan ensure that all layers in Swin Unet3D will model \nTable 2 Performance comparison of multiple models on the Brats2018 validation dataset and the analysis of the significant \ndifferences between the performance of Swin Unet3D on the validation set and the performance of other models using the Wilcoxon \nsign test\nBold values indicate the best metrics\nModel name Params Params size Mean dice Significant difference\n(M) (MB) ET TC WT ET TC WT\n3D U-Net 7.9 15.834 0.704 0.763 0.869 No Yes Yes\nV-Net 45.6 91.216 0.361 0.528 0.801 Yes Yes Yes\nUnetR 102 204.899 0.743 0.767 0.869 Yes Yes Yes\nTransBTS 33.0 65.975 0.707 0.723 0.844 Yes Yes No\nSwinBTS 15.7 34.411 0.732 0.717 0.863 No No No\nAttentionUnet 23.6 47.257 0.613 0.550 0.658 Yes Yes Yes\nSwin Pure Unet3D 33.6 67.163 0.657 0.646 0.797 Yes Yes Yes\nSwin Unet3D 33.7 67.403 0.716 0.761 0.874 – – –\nFig. 6 Variation curves of metrics for measuring the model fitting ability on the validation dataset of Brats2018: a mean loss; b dice coefficient on \nET channel; c dice coefficient on TC channel; d dice coefficient on WT channel\nPage 12 of 13Cai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \nthe dependency information of the image well. Mean -\nwhile, using jump connections in Swin Unet3D can \nmitigate the excessive loss of image information due to \ndownsampling in the encoder. We have demonstrated \nthe powerful fitting ability of the Swin Unet3D model \nthrough experiments on the Brats2021 Challenge and \nBrats2018 Challenge datasets. The results of the abla -\ntion experiments show that the Conv Blocks3D module \nand the Swin Block3D module can compensate for each \nother’s inherent deficiencies.\nAcknowledgements\nDuring the research and preparing the paper, my teachers and many col-\nleagues give their self-giving help. So, I will give my thanks for their great \ncontribution.\nAuthor contributions\nYC proposed the main idea of this paper and completed the experiments, YL \nwas responsible for the experimental data set collection, ZH and ML calibrated \nthe paper, YZ completed the plotted images, and WY and LC followed up the \nprogress of this paper in real-time. All authors read and approved the final \nmanuscript.\nFunding\nThis work was supported in part by the Natural Science Foundation of \nGuizhou Province (Qiankehe support normal [2022] No. 272).\nAvailability of data and materials\nThe Brats2021 dataset used in this study can be found in the RSNA-ASNR-\nMICCAI Brain Tumor Segmentation (BraTS) Challenge 2021, http:// brain tumor \nsegme ntati on. org/. The dataset of Brats2018 used in this study can be found \nin the Paddle website,https:// aistu dio. baidu. com/ aistu dio/ datas etdet ail/ 64660.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 26 September 2022   Accepted: 3 February 2023\nReferences\n 1. Board PATE. Adult central nervous system tumors treatment (PDQ®): \nHealth Professional Version. Website. 2022. https:// www. cancer. gov/ \ntypes/ brain/ hp/ adult- brain- treat ment- pdq.\n 2. Taghanaki SA, Abhishek K, Cohen JP , Cohen-Adad J, Hamarneh G. Deep \nsemantic segmentation of natural and medical images: a review. Artif \nIntell Rev. 2021;54(1):137–78.\n 3. Bhargavi K, Jyothi S. A survey on threshold based segmentation tech-\nnique in image processing. Int J Innov Res Dev. 2014;3(12):234–9.\n 4. Kaganami HG, Beiji Z. In: 2009 Fifth international conference on intel-\nligent information hiding and multimedia signal processing (IEEE). 2009; \np. 1217–21.\n 5. Unser M. Texture classification and segmentation using wavelet frames. \nIEEE Trans Image Process. 1995;4(11):1549–60.\n 6. Manjunath B, Chellappa R. Unsupervised texture segmentation using \nMarkov random field models. IEEE Trans Pattern Anal Mach Intell. \n1991;13(5):478–82.\n 7. Paulinas M, Ušinskas A. A survey of genetic algorithms applications \nfor image enhancement and segmentation. Inf Technol Control. \n2007;36(3):66.\ntransverse sagittal coronal 3d\nFig. 7 Visualization of segmentation results of Swin Pure Unet3D \n(Swin Unet3D implemented purely based on Swin Transformer)\nTable 3 On the validation dataset of Brats2021, we applied \nWilcoxon Signed-Rank Test to the dice coefficients obtained from \nthe comparison model and the dice scores obtained from Swin \nUnet3D, respectively\nmodel name P-value\nET TC WT\n3D U-Net 0.0001 0.1073 0.0022\nV-Net 0.0058 0 0.0115\nUnetR 0.001 0.057 0\nTransBTS 0 0 0\nSwinBTS 0 0 0.0108\nAttentionUnet 0.0079 0 0.0005\nSwin Pure Unet3D 0 0 0\nTable 4 On the validation dataset of Brats2018, we applied \nWilcoxon Signed-Rank Test to the dice coefficients obtained from \nthe comparison model and the dice scores obtained from Swin \nUnet3D, respectively\nmodel name P-value\nET TC WT\n3D U-Net 0.2852 0.0133 0.0003\nV-Net 0 0 0\nUnetR 0 0 0\nTransBTS 0.0038 0.0001 0.0559\nSwinBTS 0.3714 0.3304 0.6393\nAttentionUnet 0 0 0\nSwin Pure Unet3D 0 0 0\nPage 13 of 13\nCai et al. BMC Medical Informatics and Decision Making           (2023) 23:33 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 8. Ronneberger O, Fischer P , Brox T. In: International conference on medi-\ncal image computing and computer-assisted intervention. Springer; \n2015;pp. 234–41.\n 9. Çiçek Ö, Abdulkadir A, Lienkamp SS, Brox T, Ronneberger O. In: Interna-\ntional conference on medical image computing and computer-assisted \nintervention. Springer; 2016; pp. 424–32.\n 10. Odena A, Dumoulin V, Olah C. Deconvolution and checkerboard artifacts. \nDistill. 2016;1(10): e3.\n 11. Dumoulin V, Visin F. A guide to convolution arithmetic for deep learning. \narXiv preprint arXiv: 1603. 07285. 2016.\n 12. Long J, Shelhamer E, Darrell T. In: Proceedings of the IEEE conference on \ncomputer vision and pattern recognition; 2015. p. 3431–40.\n 13. Isensee F, Jaeger PF, Kohl SA, Petersen J, Maier-Hein KH. nnu-net: a \nself-configuring method for deep learning-based biomedical image \nsegmentation. Nat Methods. 2021;18(2):203–11.\n 14. Milletari F, Navab N, Ahmadi SA. In: 2016 Fourth international conference \non 3D vision (3DV). IEEE; 2016. p. 565–71.\n 15. Zhou Z, Siddiquee MMR, Tajbakhsh N, Liang J. In: Deep learning in medi-\ncal image analysis and multimodal learning for clinical decision support. \nSpringer; 2018. p. 3–11.\n 16. Oktay O, Schlemper J, Folgoc LL, Lee M, Heinrich M, Misawa K, Mori K, \nMcDonagh S, Hammerla NY, Kainz B, et al. Attention u-net: learning \nwhere to look for the pancreas. arXiv preprint arXiv: 1804. 03999. 2018.\n 17. Huang H, Lin L, Tong R, Hu H, Zhang Q, Iwamoto Y, Han X, Chen YW, Wu \nJ. In: ICASSP 2020—2020 IEEE international conference on acoustics, \nspeech and signal processing (ICASSP). IEEE; 2020. p. 1055–9.\n 18. Yu L, Cheng J, Dou Q, Yang X, Chen H, Qin J, Heng P . Automatic 3d cardio-\nvascular MR segmentation with densely-connected volumetric convnets. \nCoRR. 2017. arXiv: http:// arxiv. org/ abs/ 1708. 00573.\n 19. Huang C, Han H, Yao Q, Zhu S, Zhou SK. In: MICCAI; 2019.\n 20. Nikolaos A. Deep learning in medical image analysis: a comparative \nanalysis of multi-modal brain-mri segmentation with 3d deep neural \nnetworks. Master’s thesis, University of Patras; 2019. https:// github. com/ \nblack 0017/ Medic alZoo Pytor ch.\n 21. Chen J, Lu Y, Yu Q, Luo X, Adeli E, Wang Y, Lu L, Yuille AL, Zhou Y. Transunet: \ntransformers make strong encoders for medical image segmentation. \narXiv preprint arXiv: 2102. 04306. 2021.\n 22. Raghu M, Unterthiner T, Kornblith S, Zhang C, Dosovitskiy A. Do vision \ntransformers see like convolutional neural networks? Adv Neural Inf \nProcess Syst. 2021;34:12116–28.\n 23. Yu F, Koltun V. Multi-scale context aggregation by dilated convolutions. \narXiv preprint arXiv: 1511. 07122. 2015.\n 24. Woo S, Park J, Lee JY, Kweon IS. In: Proceedings of the European confer-\nence on computer vision (ECCV); 2018. p. 3–19.\n 25. Zhao H, Shi J, Qi X, Wang X, Jia J. In: Proceedings of the IEEE conference \non computer vision and pattern recognition; 2017. p. 2881–90.\n 26. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser \nŁ, Polosukhin I. In: Advances in neural information processing systems; \n2017. p. 5998–6008.\n 27. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner \nT, Dehghani M, Minderer M, Heigold G, Gelly S, et al. An image is worth \n16x16 words: transformers for image recognition at scale. arXiv preprint \narXiv: 2010. 11929. 2020.\n 28. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B. Swin transformer: \nhierarchical vision transformer using shifted windows. arXiv preprint \narXiv: 2103. 14030. 2021.\n 29. Cao H, Wang Y, Chen J, Jiang D, Zhang X, Tian Q, Wang M. Swin-unet: \nUnet-like pure transformer for medical image segmentation. arXiv pre-\nprint arXiv: 2105. 05537. 2021.\n 30. Wang W, Chen C, Ding M, Yu H, Zha S, Li J. In: International conference on \nmedical image computing and computer-assisted intervention. Springer; \n2021. p. 109–19.\n 31. Hatamizadeh A, Tang Y, Nath V, Yang D, Myronenko A, Landman B, Roth \nHR, Xu D. In: Proceedings of the IEEE/CVF Winter conference on applica-\ntions of computer vision; 2022. p. 574–84.\n 32. Jiang Y, Zhang Y, Lin X, Dong J, Cheng T, Liang J. Swinbts: a method for 3d \nmultimodal brain tumor segmentation using Swin Transformer. Brain Sci. \n2022;12(6):66. https:// doi. org/ 10. 3390/ brain sci12 060797.\n 33. Baid U, Ghodasara S, Mohan S, Bilello M, Calabrese E, Colak E, Farahani K, \nKalpathy-Cramer J, Kitamura FC, Pati S, et al. The rsna–asnr–miccai brats \n2021 benchmark on brain tumor segmentation and radiogenomic clas-\nsification. arXiv preprint arXiv: 2107. 02314. 2021.\n 34. Bakas S, Akbari H, Sotiras A, Bilello M, Rozycki M, Kirby JS, Freymann JB, \nFarahani K, Davatzikos C. Advancing the cancer genome atlas glioma mri \ncollections with expert segmentation labels and radiomic features. Sci \nData. 2017;4(1):1–13.\n 35. Menze BH, Jakab A, Bauer S, Kalpathy-Cramer J, Farahani K, Kirby J, \nBurren Y, Porz N, Slotboom J, Wiest R, et al. The multimodal brain tumor \nimage segmentation benchmark (brats). IEEE Trans Med Imaging. \n2014;34(10):1993–2024.\n 36. Liu Z, Ning J, Cao Y, Wei Y, Zhang Z, Lin S, Hu H. Video Swin Transformer. \narXiv preprint arXiv: 2106. 13230. 2021.\n 37. Ba JL, Kiros JR, Hinton GE. Layer normalization. arXiv preprint arXiv: 1607. \n06450. 2016.\n 38. He K, Zhang X, Ren S, Sun J. In: Proceedings of the IEEE international \nconference on computer vision. 2015; p. 1026–34.\n 39. Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, \nAndreetto M, Adam H. Mobilenets: efficient convolutional neural \nnetworks for mobile vision applications. arXiv preprint arXiv: 1704. 04861. \n2017.\n 40. Guo MH, Lu CZ, Liu ZN, Cheng MM, Hu SM. Visual attention network. \narXiv preprint arXiv: 2202. 09741. 2022.\n 41. Rogozhnikov A. In: International conference on learning representations; \n2022. https:// openr eview. net/ forum? id= oapKS VM2bcj.\n 42. He K, Zhang X, Ren S, Sun J. In: Proceedings of the IEEE conference on \ncomputer vision and pattern recognition; 2016. p. 770–8.\n 43. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, \nGimelshein N, Antiga L, et al. Pytorch: an imperative style, high-perfor-\nmance deep learning library. Adv Neural Inf Process Syst. 2019;32:66.\n 44. Falcon W, team TPL. Pytorch lightning; 2019. https:// doi. org/ 10. 5281/ \nzenodo. 38289 35. https:// www. pytor chlig htning. ai\n 45. Consortium M. Monai: medical open network for AI; 2020. https:// doi. \norg/ 10. 5281/ zenodo. 43230 58. https:// github. com/ Proje ct- MONAI/ \nMONAI\n 46. Yushkevich PA, Piven J, Cody Hazlett H, Gimpel Smith R, Ho S, Gee JC, \nGerig G. User-guided 3D active contour segmentation of anatomical \nstructures: significantly improved efficiency and reliability. Neuroimage. \n2006;31(3), 1116–28.\n 47. Loshchilov I, Hutter F. Decoupled weight decay regularization. arXiv \npreprint arXiv: 1711. 05101. 2017.\n 48. Virtanen P , Gommers R, Oliphant TE, Haberland M, Reddy T, Cournapeau \nD, Burovski E, Peterson P , Weckesser W, Bright J, van der Walt SJ, Brett M, \nWilson J, Millman KJ, Mayorov N, Nelson ARJ, Jones E, Kern R, Larson E, \nCarey CJ, Polat İ, Feng Y, Moore EW, VanderPlas J, Laxalde D, Perktold J, \nCimrman R, Henriksen I, Quintero EA, Harris CR, Archibald AM, Ribeiro AH, \nPedregosa F, van Mulbregt P SciPy 1.0 Contributors, SciPy 1.0: funda-\nmental algorithms for scientific computing in python. Nat Methods. \n2020;17:261–72. https:// doi. org/ 10. 1038/ s41592- 019- 0686-2.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.7785054445266724
    },
    {
      "name": "Computer science",
      "score": 0.7720402479171753
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7372483015060425
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7222889065742493
    },
    {
      "name": "Deep learning",
      "score": 0.5353707671165466
    },
    {
      "name": "Image segmentation",
      "score": 0.5275682210922241
    },
    {
      "name": "Scale-space segmentation",
      "score": 0.5264621376991272
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5123099088668823
    },
    {
      "name": "Segmentation-based object categorization",
      "score": 0.47385743260383606
    },
    {
      "name": "Feature extraction",
      "score": 0.4254700243473053
    },
    {
      "name": "Medical imaging",
      "score": 0.4172958433628082
    },
    {
      "name": "Computer vision",
      "score": 0.36867648363113403
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I149137203",
      "name": "Guiyang Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I178232147",
      "name": "Guizhou University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I182386381",
      "name": "Zunyi Medical University",
      "country": "CN"
    }
  ],
  "cited_by": 71
}