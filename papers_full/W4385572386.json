{
  "title": "Improving the Transferability of Clinical Note Section Classification Models with BERT and Large Language Model Ensembles",
  "url": "https://openalex.org/W4385572386",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2472033190",
      "name": "Weipeng Zhou",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2102230835",
      "name": "Majid Afshar",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2625526828",
      "name": "Dmitriy Dligach",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2144316405",
      "name": "Yanjun Gao",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2096515443",
      "name": "Timothy Miller",
      "affiliations": [
        "Harvard University",
        "Boston Children's Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2565495132",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W2190421341",
    "https://openalex.org/W4386855366",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W1456619997",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4226470220",
    "https://openalex.org/W2162800060",
    "https://openalex.org/W2292990913",
    "https://openalex.org/W2293452551",
    "https://openalex.org/W2138565540",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4366815474",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W4385565111"
  ],
  "abstract": "Text in electronic health records is organized into sections, and classifying those sections into section categories is useful for downstream tasks. In this work, we attempt to improve the transferability of section classification models by combining the dataset-specific knowledge in supervised learning models with the world knowledge inside large language models (LLMs). Surprisingly, we find that zero-shot LLMs out-perform supervised BERT-based models applied to out-of-domain data. We also find that their strengths are synergistic, so that a simple ensemble technique leads to additional performance gains.",
  "full_text": "Proceedings of the 5th Clinical Natural Language Processing Workshop, pages 125–130\nJuly 14, 2023 ©2023 Association for Computational Linguistics\nImproving the Transferability of Clinical Note Section Classification\nModels with BERT and Large Language Model Ensembles\nWeipeng Zhou\nDepartment of Biomedical\nInformatics and Medical Education\nSchool of Medicine\nUniversity of Washington\nwzhou87@uw.edu\nMajid Afshar and Yanjun Gao\nDepartment of Medicine\nSchool of Medicine and Public Health\nUniversity of Wisconsin\nmafshar, ygao@medicine.wisc.edu\nDmitriy Dligach\nDepartment of Computer Science\nLoyola University Chicago\nddligach@luc.edu\nTimothy A. Miller\nComputational Health Informatics Program\nBoston Children’s Hospital\nHarvard Medical School\nTimothy.Miller@childrens.harvard.edu\nAbstract\nText in electronic health records is organized\ninto sections, and classifying those sections\ninto section categories is useful for downstream\ntasks. In this work, we attempt to improve\nthe transferability of section classification mod-\nels by combining the dataset-specific knowl-\nedge in supervised learning models with the\nworld knowledge inside large language models\n(LLMs). Surprisingly, we find that zero-shot\nLLMs out-perform supervised BERT-based\nmodels applied to out-of-domain data. We also\nfind that their strengths are synergistic, so that a\nsimple ensemble technique leads to additional\nperformance gains.\n1 Introduction\nThe text in electronic health record notes is typi-\ncally organized into multiple sections. Correctly\nunderstanding what parts of a note correspond to\ndifferent section categories has been shown to be\nuseful for a variety of downstream tasks – includ-\ning abbreviation resolution (Zweigenbaum et al.,\n2013), cohort retrieval (Edinger et al., 2017), and\nnamed entity recognition (Lei et al., 2014). How-\never, documentation of sections is not consistently\ndone across health systems, so building systems to\nrobustly classify clinical text into sections is not\ntrivial. Prior work on text classification has shown\nthat systems trained on a dataset from one source\nperform quite poorly on different sources (Tepper\net al., 2012a).\nIn this work, we extend recent work on sec-\ntion classification (Zhou et al., 2023) that uses the\nSOAP (\"Subjective\", \"Objective\", \"Assessment\",\n\"Plan\") framework (Podder et al., 2022; Wright\net al., 2014). Our previous work (Zhou et al., 2023)\nmapped heterogeneous section types across three\ndatasets onto SOAP categories (plus \"Other\") in or-\nder to facilitate cross-domain adaptation. However,\ndespite showing improvements, that work showed\nthat the problem was still challenging for a super-\nvised approach that fine tuned pre-trained BERT-\nstyle encoder methods.\nThe insight of this current work is that super-\nvised transformers, while powerful, may overfit to\nsource domain training data. Zero-shot methods,\non the other hand, have recently gained attention\nfor their sometimes surprising ability to make accu-\nrate classification decisions without supervision. In\ngeneral, for zero-shot classification to work, (1) the\npre-training data must contain enough information\nabout the kind of questions it will be asked, and\n(2) the prompt must be able to precisely represent\nthe meaning of the classification labels. To work\non section classification, then, we explore differ-\nent base models since it is hard to know a priori\nwhich models will satisfy (1), and we explore vari-\nations in prompts that inject knowledge about the\nclassification task to satisfy (2).\nTherefore, we investigate the following research\nquestions related to the ability of large language\nmodels (LLMs) to do SOAP section classification:\nRQ1: How do different LLMs perform on the\nsection classification task in zero-shot and few-shot\nexperiments?\nRQ2: How do LLMs in the zero-shot setting\ncompare against supervised BERT-based models\napplied across domains in their ability to classify\nSOAP sections?\nRQ3: Are the strengths of LLMs and BERT-\nbased models complementary so that ensemble\nmethods may be synergistic?\n125\n2 Methods\n2.1 Datasets\nIn this study we used three datasets, discharge,\nthyme and progress, containing 1372, 4223, and\n13367 sections, respectively. Thedischarge dataset\nconsists of discharge summaries from the i2b2\n2010 challenge (Tepper et al., 2012b). The thyme\ndataset consists of colorectal clinical notes from\nthe THYME (Temporal History of Your Medi-\ncal Events) corpus (Styler IV et al., 2014). The\nprogress dataset consists of progress notes from\nMIMIC-III (Gao et al., 2022). Although these\ndatasets are common in that they are all medical\nnotes, they differ in both the health care institutions\nthey are coming from and the specialties who wrote\nthem. Following Zhou et al. (2023), the section\nnames in these datasets were mapped to SOAP cat-\negories (“Subjective”, “Objective”, “Assessment”\nand “Plan”). For sections that did not fit into the\nSOAP categories, the \"Others\" label was assigned.\nTherefore, these datasets are tasks that classify a\nsection into one of the 5 categories. We followed\nthe same train/test split as in Zhou et al. (2023).\n2.2 Prompt design\nPerforming classification with generative LLMs re-\nquires the creation of an input prompt that cues the\nmodel to generate output that can be deterministi-\ncally mapped to a classifier output. We design our\nprompt to be a clinical note section followed by\na multiple-choice question. The multiple-choice\nquestion begins with \"Which of the following state-\nments is correct about the text above\" and is fol-\nlowed by statements describing the 5 categories in\nthe SOAP section classification task. The prompt\nthen lists the possible multiple choice answers as\ncategories of SOAP, describing them based on the\noriginal definitions (Podder et al., 2022) instead of\ntheir labels, to attempt to inject more knowledge\ninto the prompt. We also include the fifth possible\nanswer of \"none of them is correct\", meaning that\nthe section does not belong to any one of the SOAP\ncategories. Figure 1 shows an example of a prompt\nwith an answer.\nFor few-shot classification, we randomly sample\na few examples from the training set with answers,\nformatted as in Figure 1, and concatenate them\ntogether, followed by the query section text with\nthe answer left blank. For zero-shot classification,\nthe prompt contains only the query text with the\nanswer left blank.\nPrior work has shown LLMs prefer an option at\na specific location for multiple-choice questions,\nsuch as always choosing the first or the last op-\ntion (Singhal et al., 2022). To control for this source\nof variation, we shuffle the options every time be-\nfore feeding the prompt into the model such that,\nfor example, the option \"Subjective\" can be in any\none of the five options’ locations. For the very rare\ncases that a model generates outputs not belonging\nto one of the 5 options, we consider that to be the\n\"Others\" category.\nFigure 1: Example of a prompt with the answer pro-\nvided. It consists of a clinical note section text and a\nmultiple choice question. The options are for \"Subjec-\ntive\", \"Objective\", \"Assessment\", \"Plan\" and \"Others\"\nrespectively.\n2.3 LLM experiments\nTo understand the performance of LLMs on section\nclassification, we performed experiments to com-\npare different LLMs and across different number\nof shots. In this study, we chose to experiment with\nFLAN-T5 (Chung et al., 2022), BioMedLM (Veni-\ngalla et al., 2022) and Galactica (Taylor et al.,\n2022). 1 We chose these models because, during\npreliminary work, they performed well with seem-\ningly fewer hallucinations (Ji et al., 2023) than\nother models we explored.\nBioMedLM has 2.7 billion parameters and is\ntrained on biomedical abstracts and papers. FLAN-\nT5 is trained on the web crawl C4 dataset (Raffel\net al., 2020) and additionally more than 1000 tasks,\nand we used the XXL version which contains 11\nbillion parameters. Galactica is trained on a large\ncorpus containing scientific literature, and we used\nthe standard version which contains 6.7 billion pa-\nrameters. For each model we selected the largest\nvariant that could fit in the memory of our GPU.\n1We were unable to experiment with models like ChatGPT\ndue to the terms of the data use agreements of our datasets.\n126\nThe maximum input token size is 512 for FLAN-\nT5, 1024 for BioMedLM, and 2048 for Galactica,\nwhich limits the maximum number of shots (input\nexamples in the prompt) to 0, 5, and 10, respec-\ntively. Following Zhou et al. (2023), we report the\nmicro-F1 scores. These experiments were done on\na 40 GB NVIDIA A40 GPU. The best LLM will be\nused in the following ensemble model experiments.\n2.4 Ensemble of BERT and LLMs\nWe experiment with improving the performance\nof cross-domain section classification by ensem-\nbling BERT (Vaswani et al., 2017) and LLMs. At a\nhigh level, the ensemble model will weight the two\nmodels’ prediction by their confidence and choose\nthe one with the highest confidence. Confidence is\nmeasured by a model’s prediction probability of a\ncategory. For a pair of source and target domain, we\nfirst train a BERT model on the source domain and\napply it to the target domain. For the target domain,\nwe will obtain the model’s prediction (predBERT )\nalong with the prediction probability (probBERT )\nof that class by applying a softmax function on the\nmodel’s output logits. Second, we apply an LLM\nto the target domain as well. To obtain confidence\nestimates from LLMs, we introduce a “black-box”\nmethod for estimating confidence of an LLM based\non bootstrapping. We use this method for maxi-\nmum generalizability – it could be applied even to\nblack box models like ChatGPT that do not allow\naccess to underlying probability distributions. To\nestimate confidence values, we make predictions\nfor the same section ten times and vary the order\nof the five options across the runs. Because the\nprompt becomes different, the model sometimes\nmakes different option choices. Probabilities are\nobtained by simply dividing option counts by the\nnumber of predictions (ten). We define the LLMs\nprediction (predLLM ) to be the one with the high-\nest probability (probLLM ) . When ensembling, for\neach instance, we compare the prediction probabil-\nities (probBERT , probLLM ) from both models and\nuse the prediction with the highest probability:\npredEns =\n{\npredLLM if probBERT < probLLM\npredBERT if probBERT > probLLM\nAs an example, if BERT predicts a section to\nbe \"Subjective\" with a probability of 0.55 and the\nLLM predicts it to be \"Objective\" with a prob-\nability of 0.7, the ensemble model will use the\nLLM’s \"Objective\" prediction because it has a\nhigher prediction probability. We use BioClini-\ncalBERT (Alsentzer et al., 2019) for the BERT\nmodel and the training of BERT follows the same\nhyperparameter settings as described in Zhou et al.\n(2023).\n3 Results\n3.1 Comparing LLMs\nFigure 2 shows the results of running Random (ran-\ndom guess), FLAN-T5, Galactica and BioMedLM\nwith 0-, 5-, and 10-shot experiments, averaged\nacross datasets. Because of the input token size\nlimit, the maximum number of shots for the three\nmodels are 0-, 5- and 10-shots respectively. We\nobserve that the best performing LLM is FLAN-\nT5 at 0-shot (RQ1). We will use FLAN-T5 in the\nensembling model development.\nFigure 2: Dataset averaged F1 score of Random, FLAN-\nT5, BioMedLM and Galactica models using 0-, 5- and\n10-shot. Due to different prompt-length restrictions, not\nall settings could be run with all models.\n3.2 Ensemble of BERT and LLMs\nTable 1 shows the cross-domain F1 score for BERT,\n0-shot FLAN-T5, and their ensemble for each pair\nof source and target domains. After averaging,\nwe observe that FLAN-T5 is competitive against\nBERT (RQ2), and the ensemble model that com-\nbines both achieves the best performance.\nTo understand the performance gain of the en-\nsemble method, in Table 2, we show the dataset av-\neraged F1 scores of BERT and FLAN-T5 by SOAP\ncategories. We observe that FLAN-T5 is outper-\nforming BERT on the \"Assessment\" and \"Plan\"\ncategories by a large margin, is slightly better on\nthe \"Subjective\" category, but is under perform-\ning on the \"Objective\" category. Because \"Assess-\nment\" and \"Plan\" are less prevalent categories in\n127\nSource\ndomain\nTarget\ndomain BioClinicalBERT FLAN-T5 Ensemble\nthyme discharge 0.622 0.495 0.651\nprogress 0.465 0.495 0.491\ndischarge thyme 0.499 0.542 0.58\nprogress 0.652 0.542 0.593\ndischarge progress 0.741 0.795 0.821\nthyme 0.625 0.795 0.817\nAverage 0.601 0.611 0.659\nTable 1: F1 scores of BioClinicalBERT, FLAN-T5 and the ensemble when trained on the source domain and tested\non the target domain.\nBioClinicalBERT FLAN-T5\nSubjective 0.676 0.691\nObjective 0.696 0.613\nAssessment 0.164 0.46\nPlan 0.127 0.29\nOthers 0.166 0.16\nTable 2: The F1 scores of BioClinicalBERT and FLAN-\nT5 broken down by prediction categories. The rows are\nthe categories and the columns are the models.\nthe datasets, and the \"Objective\" category is more\nprevalent, FLAN-T5 achieves a competitive perfor-\nmance against BERT on average. This observation\nis also indicative that BERT and FLAN-T5 cap-\nture different aspects of the task and therefore their\nensemble achieves the best performance (RQ3).\n4 Discussion\nOur results related to RQ1 were quite surprising.\nThe best-performing LLM, FLAN-T5-XXL, while\nbeing the largest model, has the least overlap with\nour data genre and was unable to fit any example\ninstances into its prompt. The success of FLAN-\nT5-XXL could be attributed to it both being larger\nin parameter size and having instruction tuning\nthat other models don’t have. Future work should\nexplore smaller versions of FLAN-T5 to learn\nwhether the model size or fine tuning is more impor-\ntant, but one interesting hypothesis is that explicit\nfine tuning on tasks with multiple choice setups\nmay have benefited FLAN-T5.\nDespite the BioMedLM (2.7b) having fewer than\nhalf the parameters of the Galactica (6.7b) models,\nperformance is not as degraded as we might ex-\npect. This could be an indicator that incorporating\nmedical knowledge helps LLMs recognize medical\ntexts better and thus performs closer to models that\nare larger when doing section classification. Here\nagain, it would be valuable to isolate the model size\nvariable from the pre-training genre variable, but\nthe closest Galactica model in size to BioMedLM\nhas 1.3 billion parameters – a closer model size\nbut still not a perfect comparison. Neither model\nwas seemingly able to take advantage of seeing\nlabeled instances in their prompts. One possible ex-\nplanation is that, because the output space has five\nunique labels, and the categories are quite hetero-\ngeneous, it is just not able to see enough diversity\nof each category type to meaningfully generalize.\nClinical-T5 (Lehman and Johnson, 2023; Gold-\nberger et al., 2000), which is trained on\nMIMIC (Johnson et al., 2016, 2020), can be ex-\nplored in the future too, to examine the effect of pre-\ntraining on a more highly aligned domain. How-\never, we note that the pre-training data for Clinical-\nT5 overlaps with the progress dataset we evaluate\non here, which makes it difficult to obtain fair zero-\nshot comparisons.\nFinally, the pace of new releases of LLMs is\nquite fast, and models released after this work are\npotentially quite powerful (e.g. Alpaca (Taori et al.,\n2023) and Vicuna (Team, 2023)). Future work can\nalso include assessing those models’ capability for\nsection classification.\nThe ensemble model was found to be the best,\nand a hypothesis can be that LLMs learn better for\nthe rarer categories and supervised learning learns\nbetter on prevalent categories. One explanation for\nthis is that the supervised learner implicitly learns\na distribution over label frequency, which may bias\nit towards frequent categories, while the zero-shot\nlearner only has access to the textual evidence to\nmake its decisions. If this same dynamic holds\nmore generally (as seen in other recent work (Yuan\net al., 2023)), LLMs may serve as an important\n128\nsupplement to supervised learning in terms of pre-\ndicting rare categories.\nThis study estimated the prediction probability\nfor LLM by repeating the experiments, and future\nwork can explore additional methods for obtaining\nthe prediction probability.\n5 Conclusion\nThis paper demonstrates the use of LLMs for sec-\ntion classification and an ensemble method for im-\nproving the transferability of section classification\nmodels. The supervised learning model and LLMs\nare competitive, and when ensembled based on the\nprediction probabilities, we observed a higher per-\nformance. In analyzing the prediction performance\nby categories, we found LLMs complemented the\nsupervised learning by performing better on the\nrare categories, and the supervised method per-\nformed better for the most prevalent category. Fu-\nture studies can extend to updated LLMs and the\nuse of LLMs for section classification is promising.\n6 Limitations\nA limitation in this study is we only used open-\nsource models. We were unable to evaluate Chat-\nGPT, for example, because the data use agreements\nunder which these datasets are made available for-\nbid sending the data to outside APIs. Other mod-\nels are frequently being released and we did not\nexhaustively test all publicly available language\nmodels. However, the focus of the paper is not to\nfind the best LLMs but instead providing insights\ninto using LLMs to improve transferability.\nReferences\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical bert\nembeddings.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. 2022.\nScaling instruction-finetuned language models.\nTracy Edinger, Dina Demner-Fushman, Aaron M Co-\nhen, Steven Bedrick, and William Hersh. 2017. Eval-\nuation of clinical text segmentation to facilitate co-\nhort retrieval. In AMIA Annual Symposium Proceed-\nings, volume 2017, page 660. American Medical\nInformatics Association.\nYanjun Gao, Dmitriy Dligach, Timothy Miller, Samuel\nTesch, Ryan Laffin, Matthew M. Churpek, and Majid\nAfshar. 2022. Hierarchical annotation for building\na suite of clinical natural language processing tasks:\nProgress note understanding. In Proceedings of the\nThirteenth Language Resources and Evaluation Con-\nference, pages 5484–5493, Marseille, France. Euro-\npean Language Resources Association.\nAry L Goldberger, Luis AN Amaral, Leon Glass, Jef-\nfrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark,\nJoseph E Mietus, George B Moody, Chung-Kang\nPeng, and H Eugene Stanley. 2000. Physiobank,\nphysiotoolkit, and physionet: components of a new\nresearch resource for complex physiologic signals.\nCirculation, 101(23):e215–e220.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nAlistair Johnson, Lucas Bulgarelli, Tom Pollard, Steven\nHorng, Leo Anthony Celi, and Roger Mark. 2020.\nMimic-iv.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H\nLehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi,\nand Roger G Mark. 2016. Mimic-iii, a freely accessi-\nble critical care database. Scientific data, 3(1):1–9.\nEric Lehman and Alistair Johnson. 2023. Clinical-t5:\nLarge language models built using mimic clinical\ntext. https://physionet.org/content/clinical-t5/1.0.0/.\nJianbo Lei, Buzhou Tang, Xueqin Lu, Kaihua Gao, Min\nJiang, and Hua Xu. 2014. A comprehensive study\nof named entity recognition in chinese clinical text.\nJournal of the American Medical Informatics Associ-\nation, 21(5):808–814.\nVivek Podder, Valerie Lew, and Sassan Ghassemzadeh.\n2022. Soap notes. In StatPearls [Internet]. Stat-\nPearls Publishing.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\net al. 2022. Large language models encode clinical\nknowledge.\nWilliam F Styler IV , Steven Bethard, Sean Finan,\nMartha Palmer, Sameer Pradhan, Piet C de Groen,\nBrad Erickson, Timothy Miller, Chen Lin, Guergana\nSavova, et al. 2014. Temporal annotation in the clini-\ncal domain. Trans. Assoc. Comput. Linguist., 2:143–\n154.\n129\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\nford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nThe Vicuna Team. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90% chat-\ngpt quality. https://lmsys.org/blog/\n2023-03-30-vicuna/ .\nMichael Tepper, Daniel Capurro, Fei Xia, Lucy Van-\nderwende, and Meliha Yetisgen-Yildiz. 2012a. Sta-\ntistical section segmentation in free-text clinical\nrecords. In Proceedings of the Eighth International\nConference on Language Resources and Evaluation\n(LREC’12), pages 2001–2008, Istanbul, Turkey. Eu-\nropean Language Resources Association (ELRA).\nMichael Tepper, Daniel Capurro, Fei Xia, Lucy Vander-\nwende, and Meliha Yetisgen-Yildiz. 2012b. Statisti-\ncal section segmentation in free-text clinical records.\nIn Lrec, pages 2001–2008.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAbhinav Venigalla, Jonathan Frankle, and Michael\nCarbin. 2022. Biomedlm: A domain-specific large\nlanguage model for biomedical text.\nAdam Wright, Dean F Sittig, Julie McGowan, Joan S\nAsh, and Lawrence L Weed. 2014. Bringing science\nto medicine: an interview with larry weed, inventor\nof the problem-oriented medical record. Journal\nof the American Medical Informatics Association ,\n21(6):964–968.\nChenhan Yuan, Qianqian Xie, and Sophia Ananiadou.\n2023. Zero-shot temporal relation extraction with\nchatgpt.\nWeipeng Zhou, Meliha Yetisgen, Majid Afshar, Yan-\njun Gao, Guergana Savova, and Timothy A. Miller.\n2023. Improving model transferability for clinical\nnote section classification models using continued\npretraining. medRxiv.\nPierre Zweigenbaum, Louise Deléger, Thomas\nLavergne, Aurélie Névéol, and Andreea Bodnari.\n2013. A supervised abbreviation resolution system\nfor medical text. In CLEF (Working Notes). Citeseer.\n130",
  "topic": "Transferability",
  "concepts": [
    {
      "name": "Transferability",
      "score": 0.7968759536743164
    },
    {
      "name": "Section (typography)",
      "score": 0.730035662651062
    },
    {
      "name": "Computer science",
      "score": 0.725800096988678
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5947908163070679
    },
    {
      "name": "Language model",
      "score": 0.5682719349861145
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5197948217391968
    },
    {
      "name": "Natural language processing",
      "score": 0.5108015537261963
    },
    {
      "name": "Health records",
      "score": 0.44225820899009705
    },
    {
      "name": "Machine learning",
      "score": 0.4109559655189514
    },
    {
      "name": "Data science",
      "score": 0.33656591176986694
    },
    {
      "name": "Health care",
      "score": 0.06849199533462524
    },
    {
      "name": "Mathematics",
      "score": 0.06571906805038452
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}