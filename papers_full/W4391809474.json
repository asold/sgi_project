{
    "title": "Context Transformer and Adaptive Method with Visual Transformer for Robust Facial Expression Recognition",
    "url": "https://openalex.org/W4391809474",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2228712633",
            "name": "Lingxin Xiong",
            "affiliations": [
                "Dalian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2752744447",
            "name": "Jicun Zhang",
            "affiliations": [
                "Neusoft (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2137654757",
            "name": "Xiaojia Zheng",
            "affiliations": [
                "Dalian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2110267983",
            "name": "Yuxin Wang",
            "affiliations": [
                "Dalian University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6855392806",
        "https://openalex.org/W4285256347",
        "https://openalex.org/W4387951256",
        "https://openalex.org/W6859228911",
        "https://openalex.org/W4323977922",
        "https://openalex.org/W4323314065",
        "https://openalex.org/W3157999215",
        "https://openalex.org/W4367596453",
        "https://openalex.org/W3003720578",
        "https://openalex.org/W3115297295",
        "https://openalex.org/W4387938143",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3183430956",
        "https://openalex.org/W2035372623",
        "https://openalex.org/W2217426128",
        "https://openalex.org/W2769016416",
        "https://openalex.org/W2925355748",
        "https://openalex.org/W3034102869",
        "https://openalex.org/W3216126951",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2946948417",
        "https://openalex.org/W4317840325",
        "https://openalex.org/W4380631067",
        "https://openalex.org/W3194949249",
        "https://openalex.org/W4312465353",
        "https://openalex.org/W4386617334",
        "https://openalex.org/W4366438791",
        "https://openalex.org/W4387019970",
        "https://openalex.org/W4362576614",
        "https://openalex.org/W4388629854",
        "https://openalex.org/W3209798173",
        "https://openalex.org/W3195286673",
        "https://openalex.org/W4311414773",
        "https://openalex.org/W4385423423",
        "https://openalex.org/W4321599488",
        "https://openalex.org/W4386078137",
        "https://openalex.org/W2600389231",
        "https://openalex.org/W2964139520",
        "https://openalex.org/W3184571633",
        "https://openalex.org/W4386615840",
        "https://openalex.org/W4376278471",
        "https://openalex.org/W4292075193",
        "https://openalex.org/W4313020443",
        "https://openalex.org/W4319663643",
        "https://openalex.org/W4378388700",
        "https://openalex.org/W4365514941",
        "https://openalex.org/W4317033482",
        "https://openalex.org/W4319996317",
        "https://openalex.org/W4385514413",
        "https://openalex.org/W4389829249"
    ],
    "abstract": "In real-world scenarios, the facial expression recognition task faces several challenges, including lighting variations, image noise, face occlusion, and other factors, which limit the performance of existing models in dealing with complex situations. To cope with these problems, we introduce the CoT module between the CNN and ViT frameworks, which improves the ability to perceive subtle differences by learning the correlations between local area features at a fine-grained level, helping to maintain the consistency between the local area features and the global expression, and making the model more adaptable to complex lighting conditions. Meanwhile, we adopt an adaptive learning method to effectively eliminate the interference of noise and occlusion by dynamically adjusting the parameters of the Transformer Encoderâ€™s self-attention weight matrix. Experiments demonstrate the accuracy of our CoT_AdaViT model in the Oulu-CASIA dataset as (NIR: 87.94%, VL: strong: 89.47%, weak: 84.76%, dark: 82.28%). As well as, CK+, RAF-DB, and FERPlus datasets achieved 99.20%, 91.07%, and 90.57% recognition results, which achieved excellent performance and verified that the model has strong recognition accuracy and robustness in complex scenes.",
    "full_text": null
}