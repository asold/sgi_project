{
  "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models",
  "url": "https://openalex.org/W4363676214",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2126366710",
      "name": "Ferrara, Emilio",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3196248941",
    "https://openalex.org/W2580906503",
    "https://openalex.org/W4301369855",
    "https://openalex.org/W3123610095",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2594475271",
    "https://openalex.org/W3046113390",
    "https://openalex.org/W2997335426",
    "https://openalex.org/W2975933472",
    "https://openalex.org/W4308654226",
    "https://openalex.org/W4380887356",
    "https://openalex.org/W2618694529",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2788481061",
    "https://openalex.org/W2897702578",
    "https://openalex.org/W2916904544",
    "https://openalex.org/W4387559778",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W2888109941",
    "https://openalex.org/W3104723404",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2963797754",
    "https://openalex.org/W2769358515",
    "https://openalex.org/W4230971817",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W4385572101",
    "https://openalex.org/W3127673435",
    "https://openalex.org/W1806647190",
    "https://openalex.org/W2981869278",
    "https://openalex.org/W3015977534",
    "https://openalex.org/W3045238219",
    "https://openalex.org/W2972735048",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4390229867",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2950888501",
    "https://openalex.org/W3119394424",
    "https://openalex.org/W2524301210",
    "https://openalex.org/W2943491685",
    "https://openalex.org/W3013300757",
    "https://openalex.org/W2493343568",
    "https://openalex.org/W2963481894",
    "https://openalex.org/W4301029487",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W4205952596",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W4321207578",
    "https://openalex.org/W2891525068",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W4245284769",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2742947407",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2031342017",
    "https://openalex.org/W4312062661",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W2594690992",
    "https://openalex.org/W2957654274",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W3086663505",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2096434649",
    "https://openalex.org/W3112646659",
    "https://openalex.org/W2974817986",
    "https://openalex.org/W4388996954",
    "https://openalex.org/W2162510130",
    "https://openalex.org/W2910707576",
    "https://openalex.org/W4320009668",
    "https://openalex.org/W4388995913",
    "https://openalex.org/W3130896941",
    "https://openalex.org/W3121541553",
    "https://openalex.org/W2891340972",
    "https://openalex.org/W4231095134",
    "https://openalex.org/W4230354434",
    "https://openalex.org/W4226381434",
    "https://openalex.org/W2964060106",
    "https://openalex.org/W2963612262",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W2911227954",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W3094239814",
    "https://openalex.org/W2969625533",
    "https://openalex.org/W4388996366",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2953532875",
    "https://openalex.org/W2991138171",
    "https://openalex.org/W3008087766",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963116854",
    "https://openalex.org/W3204423820",
    "https://openalex.org/W2913897682",
    "https://openalex.org/W2762694293",
    "https://openalex.org/W3010919171",
    "https://openalex.org/W3181414820",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2963808661",
    "https://openalex.org/W2755246076",
    "https://openalex.org/W334792815",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2996844929",
    "https://openalex.org/W2791170418",
    "https://openalex.org/W4312181988",
    "https://openalex.org/W2968511602",
    "https://openalex.org/W3100307207",
    "https://openalex.org/W4289418544",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W1905966190",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W3173162544",
    "https://openalex.org/W2806921177",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4285077564",
    "https://openalex.org/W2540757487",
    "https://openalex.org/W2514140684",
    "https://openalex.org/W2962772482",
    "https://openalex.org/W2953522645",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W4387451550",
    "https://openalex.org/W4386564359",
    "https://openalex.org/W3099361686",
    "https://openalex.org/W2557671501",
    "https://openalex.org/W2944176689",
    "https://openalex.org/W2767982226",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W2075045151"
  ],
  "abstract": "As the capabilities of generative language models continue to advance, the implications of biases ingrained within these models have garnered increasing attention from researchers, practitioners, and the broader public. This article investigates the challenges and risks associated with biases in large-scale language models like ChatGPT. We discuss the origins of biases, stemming from, among others, the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions. We explore the ethical concerns arising from the unintended consequences of biased model outputs. We further analyze the potential opportunities to mitigate biases, the inevitability of some biases, and the implications of deploying these models in various applications, such as virtual assistants, content generation, and chatbots. Finally, we review the current approaches to identify, quantify, and mitigate biases in language models, emphasizing the need for a multi-disciplinary, collaborative effort to develop more equitable, transparent, and responsible AI systems. This article aims to stimulate a thoughtful dialogue within the artificial intelligence community, encouraging researchers and developers to reflect on the role of biases in generative language models and the ongoing pursuit of ethical AI.",
  "full_text": "arXiv:2304.03738v3  [cs.CY]  13 Nov 2023\nShould ChatGPT be Biased?\nChallenges and Risks of Bias in Large Language Models\nEmilio Ferraraa,b\naThomas Lord Department of Computer Science, University of Southern California, Los Angeles, 90007, CA, USA\nbUSC Information Sciences Institute, 4676 Admiralty Way #1001, Marina del Rey, 90292, CA, USA\nE-mail: emiliofe@usc.edu\nAbstract\nAs generative language models, exempliﬁed by ChatGPT, cont inue to advance in their capa-\nbilities, the spotlight on biases inherent in these models i ntensiﬁes. This article delves into the\ndistinctive challenges and risks associated with biases sp eciﬁcally in large-scale language models.\nWe explore the origins of biases, stemming from factors such as training data, model speciﬁcations,\nalgorithmic constraints, product design, and policy decis ions. Our examination extends to the\nethical implications arising from the unintended conseque nces of biased model outputs. In addi-\ntion, we analyze the intricacies of mitigating biases, ackn owledging the inevitable persistence of\nsome biases, and consider the consequences of deploying the se models across diverse applications,\nincluding virtual assistants, content generation, and cha tbots. Finally, we provide an overview of\ncurrent approaches for identifying, quantifying, and miti gating biases in language models, under-\nscoring the need for a collaborative, multidisciplinary eﬀo rt to craft AI systems that embody equity,\ntransparency, and responsibility. This article aims to cat alyze a thoughtful discourse within the\nAI community, prompting researchers and developers to cons ider the unique role of biases in the\ndomain of generative language models and the ongoing quest f or ethical AI.\nKeywords: Artiﬁcial Intelligence, Generative AI, Bias, Large Langua ge Models, ChatGPT,\nGPT-4\n1. Introduction\nIn recent years, there has been a remarkable surge in the real m of artiﬁcial intelligence (AI),\nmarked by the emergence of transformative technologies lik e ChatGPT and other generative lan-\nguage models ([1, 2, 3, 4, 5, 6, 7, 8, 9]). These AI systems repr esent a class of sophisticated models\ndesigned to excel in generating human-like text and compreh ending natural language. Using deep\nlearning techniques and vast datasets, they have the abilit y to discern intricate patterns, make\ncontextual inferences, and generate coherent and contextu ally relevant responses to a diverse range\nof inputs ([10, 11, 12, 1, 2]). By mirroring human language ca pabilities, these models have unveiled\na multitude of applications, ranging from chatbots and virt ual assistants to translation services\nand content generation tools ([13]).\nLanguage models have ushered in a transformative era, under pinning the development of chat-\nbots that emulate human interaction in conversations ([14] ). These chatbots have become vital\ntools, simplifying customer service, technical support, a nd information queries with human-like\ninteraction. Furthermore, the integration of language mod els into virtual assistants has endowed\nPreprint submitted to First Monday November 14, 2023\nContributing Factor Description References\nTraining Data Biases in the source material or the selection process\nfor training data can be absorbed by the model and\nreﬂected in its behavior.\n[28, 29, 30, 31]\nAlgorithms Biases can be introduced or ampliﬁed through algo-\nrithms that place more importance on certain fea-\ntures or data points.\n[32, 30, 33]\nLabeling and Annotation In (semi)supervised learning scenarios, biases may\nemerge from subjective judgments of human annota-\ntors providing labels or annotations for the training\ndata.\n[34, 35, 36]\nProduct Design Decisions Biases can arise from prioritizing certain use cases or\ndesigning user interfaces for speciﬁc demographics or\nindustries, inadvertently reinforcing existing biases\nand excluding diﬀerent perspectives.\n[37, 38]\nPolicy Decisions Developers might implement policies that prevent (or\nencourage) a given model behavior. For example,\nguardrails that modulate the behavior of ChatGPT\nand Bing-AI were designed to mitigate unintended\ntoxic model behaviors or prevent malicious abuse.\n[39, 40, 41, 42]\nTable 1: Factors Contributing to Bias in AI Models\nthem with the ability to provide precise and contextually ap propriate responses to user inquiries\n([15]). Virtual assistants, thus enhanced, become indispe nsable aides, capable of managing tasks\nranging from appointment scheduling to web searches and sma rt home device control ([16]).\nIn the sphere of translation, harnessing the prowess of larg e language models facilitates markedly\nimproved and ﬂuent translations that span multiple languag es, including those with limited re-\nsources ([17, 18, 19, 20, 21]). Such capabilities not only fo ster enhanced cross-linguistic commu-\nnication but can also enable timely solutions during emerge ncies and crises, especially in regions\nwhere low-resource languages or indigenous dialects are sp oken ([22]).\nMoreover, the aptitude of language models to generate coher ent and contextually pertinent text\nhas rendered them invaluable in the realm of content creatio n. Acknowledged for their proﬁciency in\nproducing various types of content, spanning articles, soc ial media posts, and marketing materials,\nthey have established a profound impact ([23, 24]).\nThese applications, among many others, underscore the tran sformative prowess of generative\nlanguage models across an array of industries and sectors. H owever, as their adoption proliferates\nand their inﬂuence extends into ever-diverse domains ([25, 26]), it is imperative to confront the\ndistinctive challenges posed by the potential biases that m ay be established within these models.\nThese biases can have profound implications for users and so ciety at large, highlighting the urgent\nneed for comprehensive examination and mitigation of these issues [27].\n2. Deﬁning bias in generative language models\n2.1. Factors contributing to bias in Large Language Models\nBias, in the context of large language models such as GPT-4 ([ 9, 43, 44]) and predecessors\n([3, 4, 5]), or other state-of-the-art alternatives ([45, 4 6]; including multimodal variants, [47]),\ncan be deﬁned as the presence of systematic misrepresentati ons, attribution errors, or factual\ndistortions that result in favoring certain groups or ideas , perpetuating stereotypes, or making\n2\nincorrect assumptions based on learned patterns. Biases in such models can arise due to several\nfactors ( cf., Table 1).\nOne factor is the training data. If the data used to train a language model contain biases, ei ther\nfrom the source material or through the selection process, t hese biases can be absorbed by the\nmodel and subsequently reﬂected in its behavior ([28, 29, 30 , 31]). Biases can also be introduced\nthrough the algorithms used to process and learn from the data. For example, if an alg orithm\nplaces more importance on certain features or data points, i t may unintentionally introduce or\namplify biases present in the data ([32, 30, 33]). In (semi)s upervised learning scenarios, where\nhuman annotators provide labels or annotations for the training data, biases may emerge from the\nsubjective judgments of the annotators themselves, inﬂuen cing the model’s understanding of the\ndata ([34, 35, 36]).\nThe choice of which use cases to prioritize or the design of user interfaces can also contr ibute to\nbiases in large language models. For example, if a language m odel is primarily designed to generate\ncontent for a certain demographic or industry, it may inadve rtently reinforce existing biases and\nexclude diﬀerent perspectives ([37, 38]). Lastly, policy decisions can play a role in the manifestation\nof biases in language models. The developers of both commerc ial and openly available language\nmodels might implement policies that prevent (or encourage ) a given model behavior. For example,\nboth OpenAI and Microsoft have deliberate guardrails that m odulate the behavior of ChatGPT and\nBing-AI to mitigate unintended toxic model behaviors or pre vent malicious abuse ([39, 40, 41, 42]).\n2.2. Types of biases in Large Language Models\nLarge language models, which are commonly trained from vast amounts of text data present\non the Internet, inevitably absorb the biases present in suc h data sources. These biases can take\nvarious forms ( cf., Table 2).\nDemographic biases arise when the training data over-repre sents or under-represents certain\ndemographic groups, leading the model to exhibit biased beh avior towards speciﬁc genders, races,\nethnicities, or other social groups ([34, 28, 29, 35, 31, 48] ). Cultural biases occur when large\nlanguage models learn and perpetuate cultural stereotypes or biases, as they are often present\nin the data used for training. This can result in the model pro ducing outputs that reinforce or\nexacerbate existing cultural prejudices ([49, 50, 30]). Li nguistic biases emerge since the majority\nof the internet’s content is in English or a few other dominan t languages, making large language\nmodels more proﬁcient in these languages. This can lead to bi ased performance and a lack of\nsupport for low-resource languages or minority dialects ([ 51, 52, 53, 54, 31]). Temporal biases\nappear as the training data for these models are typically re stricted to limited time periods or have\ntemporal cutoﬀs. This may cause the model to be biased when rep orting on current events, trends,\nand opinions. Similarly, the model’s understanding of hist orical contexts or outdated information\nmay be limited due to a lack of temporally representative dat a ([3, 55, 56, 57]). Conﬁrmation biases\nin the training data may result from individuals seeking out information that aligns with their pre-\nexisting beliefs. Consequently, large language models may inadvertently reinforce these biases by\nproviding outputs that conﬁrm or support speciﬁc viewpoint s ([28, 29, 2, 58]). Lastly, ideological\nand political biases can be learned and propagated by large l anguage models due to the presence\nof such biases in their training data. This can lead to the mod el generating outputs that favor\ncertain political perspectives or ideologies, thereby amp lifying existing biases ([59, 60, 56, 61]).\nThis paper aims to explore the question of whether language m odels like GPT-4 ([9, 43, 44]), its\nprior versions ([3, 4, 5]), or other commercial or open-sour ce alternatives ([45, 46, 62]) that power\napplications like ChatGPT ([8]) (or similar) should be bias ed or unbiased, taking into account\n3\nTypes of Bias Description References\nDemographic Biases These biases arise when the training data over-\nrepresents or under-represents certain demo-\ngraphic groups, leading the model to exhibit bi-\nased behavior towards speciﬁc genders, races, eth-\nnicities, or other social groups.\n[34, 28, 29, 35, 31, 48]\nCultural Biases Large language models may learn and perpetuate\ncultural stereotypes or biases, as they are often\npresent in the data used for training. This can\nresult in the model producing outputs that rein-\nforce or exacerbate existing cultural prejudices.\n[49, 50, 30]\nLinguistic Biases Since the majority of the internet’s content is in\nEnglish or a few other dominant languages, large\nlanguage models tend to be more proﬁcient in\nthese languages. This can lead to biased perfor-\nmance and a lack of support for low-resource lan-\nguages or minority dialects.\n[51, 52, 53, 54, 31]\nTemporal Biases The training data for these models are typically\nrestricted to limited time periods, or have tem-\nporal cutoﬀs, which may cause the model to be\nbiased when reporting on current events, trends,\nand opinions. Similarly, the model’s understand-\ning of historical contexts or outdated information\nmay be limited for lack of temporally representa-\ntive data.\n[3, 55, 56, 57]\nConﬁrmation Biases The training data may contain biases that re-\nsult from individuals seeking out information that\naligns with their pre-existing beliefs. Conse-\nquently, large language models may inadvertently\nreinforce these biases by providing outputs that\nconﬁrm or support speciﬁc viewpoints.\n[28, 29, 2, 58]\nIdeological & Political Biases Large language models can also learn and prop-\nagate the political and ideological biases present\nin their training data. This can lead to the model\ngenerating outputs that favor certain political per-\nspectives or ideologies, thereby amplifying exist-\ning biases.\n[59, 60, 56, 61]\nTable 2: Types of Biases in Large Language Models\nthe implications and risks of both perspectives. By examini ng the ethical, practical, and societal\nconsequences of each viewpoint, we hope to contribute to the ongoing discussion surrounding\nresponsible language model development and use. Through th is exploration, our goal is to provide\ninsights that can help guide the future evolution of GPT-sty le and other generative language models\ntoward more ethical, fair, and beneﬁcial outcomes while min imizing potential harm.\n3. Why are generative language models prone to bias?\n3.1. Biases from the data\nChatGPT and other applications based on large language mode ls are trained using a process\nthat primarily relies on unsupervised learning, a machine l earning technique that enables models\nto learn patterns and structures from vast amounts of unlabe lled data ([63, 64]). In most cases\n4\nwith these language models, the data consists of extensive t ext corpora available on the internet,\nwhich includes websites, articles, books, and other forms o f written content ([2, 45, 46, 62]).\nChatGPT in particular is trained on a diverse range of intern et text datasets that encompass\nvarious domains, genres, and languages. While the speciﬁcs of the dataset used for GPT-4 are\nproprietary ([9, 43]), the data sources utilized for traini ng its predecessor, GPT-3, likely share\nsimilarities. For GPT-3 and predecessors, the primary data set used was WebText ([3]), which is\nan ever-growing large-scale collection of web pages ([4, 5] ). WebText was created by crawling the\ninternet and gathering text from web pages. The sources of da ta include, but are not limited to:\n• Websites: Text is extracted from a wide array of websites, covering to pics such as news,\nblogs, forums, and informational websites like Wikipedia. This enables the model to learn\nfrom diverse sources and gain knowledge on various subjects .\n• Books: Text from books available online, including both ﬁction an d non-ﬁction, contributes\nto the training data. This helps the model to learn diﬀerent wr iting styles, narrative struc-\ntures, and a wealth of knowledge from various ﬁelds.\n• Social media platforms : Content from social media platforms, like Twitter, Facebo ok, and\nReddit, is incorporated to expose the model to colloquial la nguage, slang, and contemporary\ntopics of discussion.\n• Conversational data : To improve the model’s conversational abilities, text fro m chat logs,\ncomment sections, and other conversational sources are als o included in the training dataset.\nThe developers of ChatGPT note that the WebText data is prepr ocessed and ﬁltered to remove\nlow-quality content, explicit material, web and social spa m [65, 66], and other undesirable text\nbefore being fed into the model ([4, 5]). However, due to the v ast scale of the data and the\nlimitations of current ﬁltering techniques, some undesira ble or biased content may still seep into the\ntraining dataset, aﬀecting the behavior of the resulting mod el. In addition to WebText, GPT-3 was\nfurther trained using a ﬁltered version of the Common Crawl d ataset ( https://commoncrawl.org),\na publicly available, massive web-crawled dataset that con tains raw web page data, extracted\nmetadata, and text content from billions of web pages in mult iple languages ([5]).\nAnother commonly-used dataset for language model training is The Pile ([67, 68]) an extensive\nand diverse collection of 22 smaller datasets, combining va rious sources of scientiﬁc articles, books,\nand web content. It is designed for training large-scale lan guage models, particularly in the domain\nof scientiﬁc research and understanding.\n3.2. Biases from the models\nDuring the training process, generative language models ar e exposed to billions of sentences\nand phrases, allowing them to learn the intricate relations hips between words, grammar, context,\nand meaning ([63, 64]). As they process the text data, they gr adually acquire natural language\ngeneration capabilities, enabling them to produce coheren t and contextually relevant responses to\nvarious inputs. However, some capabilities of these models can lead to bias ( cf., Table 3):\nGeneralization. One crucial aspect of these models is their ability to genera lize, which allows them\nto apply the knowledge gained from their training data to new and previously unseen inputs,\nproviding contextually relevant responses and prediction s even in unfamiliar situations. However,\n5\nthis ability also raises concerns about potential biases, a s models may inadvertently learn and\nperpetuate biases present in their training data, even if th e data has been ﬁltered and cleaned to\nthe extent possible ([69, 29]).\nPropagation. As these models learn from the patterns and structures prese nt in their training data,\nthey may inadvertently absorb and propagate biases they enc ounter, such as adopting stereotypes,\nfavoring certain groups or ideas, or making assumptions bas ed on learned patterns that do not\naccurately represent the full spectrum of human experience . This propagation of biases during\ntraining poses signiﬁcant challenges to the development of fair and equitable AI systems, as biased\nmodels can lead to unfair treatment, reinforce stereotypes , and marginalize certain groups ([28, 70,\n71]).\nEmergence. In large language models, the phenomenon of emergence, whic h refers to the spon-\ntaneous appearance of unanticipated capabilities despite these functionalities not being explicitly\nencoded within the model’s architecture or training data, c an also result in unexpected biases\ndue to the intricate interplay between model parameters and biased training data ([72, 73]). The\nhigh-dimensional representations and non-linear interac tions in these models make it diﬃcult to\npredict or control these emergent biases, which may manifes t in various ways, such as stereotyping,\noﬀensive language, or misinformation. To address this chall enge, researchers are exploring bias\nmitigation strategies during training, ﬁne-tuning with cu rated datasets, and post-hoc emergent\nbias analyses ([74, 75]).\nNon-linearity. The non-linear relationships between biases in the system o r data and their real-\nworld impact imply that small biases may have massive negati ve eﬀects, and large biases might not\nresult in signiﬁcant consequences. This disproportionali ty arises due to the complex interdepen-\ndencies between the model parameters and the high-dimensio nal representations learned during\ntraining [71]. Randomized controlled trials could be used t o draw causal relationships between\nthe extent of each bias and their eﬀects. In the absence of that , due to ethical reasons, multi-\nfaceted approaches involving in-depth analysis of model be havior, rigorous evaluation with diverse\nbenchmarks, and the application of mitigation techniques t hat account for the nonlinear nature of\nemergent biases are needed [76].\nAlignment. To address these issues, a strategy known as Reinforcement L earning with Human\nFeedback (RLHF) ([7]) was developed to ﬁne-tune large langu age models like ChatGPT to reduce\ntheir biases and align them with human values. This approach involves collecting a dataset of\nhuman demonstrations, comparisons, and preferences to cre ate a reward model that guides the\nﬁne-tuning process [77]. InstructGPT (ChatGPT’s default m odel) ([8]) is trained using RLHF and\nthen ﬁne-tuned using Proximal Policy Optimization (PPO), a policy optimization algorithm ([6]).\nIt is paramount to understand if the same principles could be exploited to deliberately misalign a\nmodel.\n3.3. Can bias be mitigated with human-in-the-loop approache s?\nBias in generative language models can be mitigated to some e xtent with human-in-the-loop\n(HITL) approaches. These approaches involve incorporatin g human input, feedback, or oversight\nthroughout the development and deployment of the language m odel, which can help address is-\nsues related to biases and other limitations. Here are some w ays to integrate human-in-the-loop\napproaches to mitigate bias:\n6\nSource Description References\nGeneralization Models generalize knowledge from training data to new input s,\npotentially leading to biased behavior if the data contains biases.\nThis raises concerns about perpetuating biases, even if tra ining\ndata has been cleaned and ﬁltered.\n[69, 29]\nPropagation Models may absorb and propagate biases in training data, adopt-\ning stereotypes and favoring certain groups or ideas, or mak ing\nassumptions on on non-representative learned patterns.\n[28, 70, 71]\nEmergence Unanticipated capabilities and biases may emerge in large l an-\nguage models due to complex interactions between model pa-\nrameters and biased training data. It has been proven diﬃcul t\nto predict or control these emergent biases.\n[72, 73, 74, 75]\nNon-linearity Biases in AI systems may have non-linear real-world impact,\nmaking it diﬃcult to predict their consequences: small mode l\nbiases may have massive negative eﬀects, whereas large mode l\nbiases might not cause signiﬁcant consequences.\n[76, 71]\nAlignment Reinforcement Learning with Human Feedback (RLHF) ﬁne-\ntunes large language models to reduce biases and align them\nwith human values. The same principles might be abused to\nlead to unfair model behaviors.\n[7, 8, 6]\nTable 3: Sources of model bias in large language models and th eir descriptions.\n• Training data curation : Humans can be involved in curating and annotating high-qua lity\nand diverse training data. This may include identifying and correcting biases, ensuring\na balance of perspectives, and reducing the inﬂuence of cont roversial or oﬀensive content\n([78, 31, 33]).\n• Model ﬁne-tuning : Subject matter experts can guide the model ﬁne-tuning proc ess by\nproviding feedback on the model’s outputs, helping the mode l generalize better and avoid\nbiased or incorrect responses ([79]).\n• Evaluation and feedback : Human reviewers can evaluate the model’s performance and\nprovide feedback to developers, who can then iteratively im prove the model. This feedback\nloop is essential for identifying and addressing bias-rela ted issues. ([58]).\n• Real-time moderation : Human moderators can monitor and review the model’s output s\nin real-time, intervening when necessary to correct biased or inappropriate responses. This\napproach can be especially useful in high-stakes or sensiti ve applications ([80]).\n• Customization and control : Users can be provided with options to customize the model’s\nbehavior, adjusting the output according to their preferen ces or requirements. This approach\ncan help users mitigate bias in the model’s responses by tail oring it to speciﬁc contexts or\ndomains ([3, 81]).\nWhile human-in-the-loop approaches can help mitigate bias , it is essential to recognize that\nthey may not be able to eliminate it entirely. Bias can stem fr om various sources, such as the\ntraining data, ﬁne-tuning process, or even the human review ers themselves. However, combining\nmachine learning techniques with human expertise can be a pr omising way to address some of the\nchallenges posed by biases in generative language models.\n7\nChallenge Description References\nInherent biases in\nlanguage\nHuman language is a reﬂection of society, containing var-\nious biases, stereotypes, and assumptions. Separating\nuseful patterns from these biases can be challenging as\nthey are deeply ingrained in language structures and ex-\npressions.\n[82, 83, 84, 85, 86, 87]\nAmbiguity of cul-\ntural norms\nCultural norms and values vary signiﬁcantly across com-\nmunities and regions. Determining which norms to en-\ncode in AI models is a complex task that requires a nu-\nanced understanding of diverse cultural perspectives.\n[88, 89, 90, 91]\nSubjectivity of fair-\nness\nFairness is a subjective concept with various interpreta-\ntions. Eliminating bias from AI models requires deﬁning\n“fair” in the context of applications, which is challenging\ndue to the diverse range of stakeholders and perspectives.\n[92, 93, 94]\nContinuously evolv-\ning language and cul-\nture\nLanguage and culture constantly evolve, with new expres-\nsions, norms, and biases emerging over time. Keeping\nAI models up-to-date with these changes and ensuring\nthey remain unbiased requires continuous monitoring and\nadaptation.\n[95, 96, 97]\nTable 4: Challenges in addressing biases in Large Language M odels\n4. The inevitability of some forms of bias\n4.1. Are some biases inevitable?\nCompletely eliminating bias from large language models is a complex and challenging task due\nto the inherent nature of language and cultural norms. Since these models learn from vast amounts\nof text data available on the internet, they are exposed to th e biases present within human language\nand culture. Addressing bias in these models involves tackl ing several key challenges ( cf., Table 4).\nFirst, human language is a reﬂection of society and as such, i t contains various biases, stereo-\ntypes, and assumptions. Separating useful patterns from th ese biases can be challenging, as they\nare often deeply ingrained in the way people express themsel ves and the structures of language\nitself ([82, 83, 84, 85, 86, 87]). Second, cultural norms and values can vary signiﬁcantly across\ndiﬀerent communities and regions. What is considered accept able or appropriate in one context\nmay be seen as biased or harmful in another. Determining whic h norms should be encoded in AI\nmodels and which should be ﬁltered out is a complex task that r equires careful consideration and\na nuanced understanding of diverse cultural perspectives ( [88, 89, 90, 91]).\nFurthermore, fairness is a subjective concept that can be in terpreted in various ways. Com-\npletely eliminating bias from AI models would require devel opers to deﬁne what “fair” means\nin the context of their applications, which can be a challeng ing task, given the diverse range of\nstakeholders and perspectives involved ([92, 93, 94]). Las tly, language and culture are constantly\nevolving, with new expressions, norms, and biases emerging over time. Keeping AI models up-\nto-date with these changes and ensuring that they remain unb iased is an ongoing challenge that\nrequires continuous monitoring and adaptation ([95, 96, 97 ]).\nDespite these challenges, it is essential for developers, r esearchers, and stakeholders to continue\nworking towards reducing bias in large language models. By d eveloping strategies for identifying\nand mitigating biases, collaborating with diverse communi ties, and engaging in ongoing evaluation\nand improvement, we can strive to create AI systems that are m ore equitable, fair, and beneﬁcial\nfor all users.\n8\n4.2. Utility despite bias?\nBiased AI models can still be useful in certain contexts or ap plications, as long as users are\naware of their limitations and take them into account when ma king decisions. In some cases, the\nbiases present in these models may even be representative of the real-world context in which they\nare being used, providing valuable insights in surfacing so cietal inequalities that need to be tackled\nat their root.\nThe key to leveraging biased AI models responsibly is to ensu re that users have a clear under-\nstanding of the potential biases and limitations associate d with these models, so they can make\ninformed decisions about whether and how to use them in diﬀere nt contexts. Some strategies for\naddressing this issue include:\n• Transparency: Developers should be transparent about the methodologies , data sources,\nand potential biases of their AI models, providing users wit h the necessary information to\nunderstand the factors that may inﬂuence the model’s predic tions and decisions. Best prac-\ntices about the documentation of models and data have been ad vanced by the AI community\n([58, 98]).\n• Education and awareness : Providing resources, training, and support to help users b etter\nunderstand the potential biases in AI models and how to accou nt for them when making\ndecisions. This may involve creating guidelines, best prac tices, or other educational materials\nthat explain the implications of bias in AI and how to navigat e it responsibly.\n• Context-speciﬁc applications : In some limited cases, biased AI models may be viable\nfor speciﬁc applications or contexts where their biases ali gn with the relevant factors or\nconsiderations. Experts should be employed to carefully ev aluate the appropriateness of\nusing biased models in these situations, taking into accoun t the potential risks and beneﬁts\nassociated with their use, and actionable plans to recogniz e, quantify, and mitigate biases.\n• Continuous monitoring and evaluation : Regularly assessing the performance of AI mod-\nels in real-world contexts, monitoring their impact on user s and aﬀected communities, and\nmaking adjustments as needed to address any biases or uninte nded consequences that emerge\nover time ([31]).\nBy acknowledging that biased models can still be useful in li mited contexts and taking steps\nto ensure that users are aware of their limitations and quali ﬁed to recognize and mitigate them,\nwe can promote the responsible use of AI technologies and har ness their potential beneﬁts while\nminimizing the risks associated with bias.\n5. The broader risks of generative AI bias\n5.1. Pillars of responsible generative AI development\nEthical considerations of fairness and equality play a cruc ial role in the development and de-\nployment of generative AI applications. As these models int egrate increasingly into various aspects\nof our lives, their potential impact on individuals and soci ety as a whole becomes a matter of\nsigniﬁcant concern. The responsibility lies with develope rs, researchers, and stakeholders to ensure\nthat AI models treat all users and groups equitably, avoidin g the perpetuation of existing biases\nor the creation of new ones ( cf., Table 5).\n9\nOne key ethical consideration is representation. It is essential to ensure that the training data\nused to develop generative AI models are representative of t he diverse range of perspectives, ex-\nperiences, and backgrounds that exist within society ([99, 35, 94, 58, 100]). This helps to reduce\nthe risk that biases are absorbed and propagated by models, l eading to more equitable outcomes.\nTransparency is another important aspect. Developers should be transpar ent about the method-\nologies, data sources, and potential limitations of their g enerative AI models ([101, 102]). This\nenables users to better understand the factors that may inﬂu ence the model’s predictions and de-\ncisions. Accountability is also crucial for responsible generative AI development. Developers and\nstakeholders must establish a clear framework for accounta bility, which may include monitoring the\nperformance of AI models, addressing biases and errors, and responding to users and communities\naﬀected ([103, 104, 105]). An unique aspect of generative AI, compared to traditional machine\nlearning, is its ability to possibly replace human artistic expression or to plagiarize the style and\nuniqueness of human work: as such, preservation of intellec tual property, copyright protection and\nprevention of plagiarism are paramount [27]. Inclusivity is another key ethical consideration. Gen-\nerative AI applications should be designed to be inclusive a nd accessible to all users, taking into\naccount factors such as language, culture, and accessibili ty needs ([106, 107]). This ensures that\nthe beneﬁts of AI are shared equitably across society.\nLastly, continuous improvement is vital to achieve fairness and equality in generative AI ap -\nplications. Developers must commit to an ongoing process of evaluating, reﬁning, and improving\ntheir AI models to address biases and ensure fairness over ti me ([108, 109, 58, 110]). This may\ninvolve collaborating with researchers, policymakers, an d aﬀected communities to gain insight and\nfeedback that can help guide the development of more equitab le AI systems.\nBy prioritizing ethical considerations of fairness and equ ality, AI developers can create appli-\ncations that not only harness the power of advanced technolo gies such as large language models,\nbut also promote a more just and inclusive society, where the beneﬁts and opportunities of AI are\naccessible to all.\n5.2. The risks of exacerbating existing societal biases\nBias in widely-adopted AI models, including ChatGPT and oth er generative language models,\ncan have far-reaching consequences that extend beyond the i mmediate context of their applications.\nWhen these models absorb and propagate biases, including th ose present in their training data, they\nmay inadvertently reinforce stereotypes, marginalize cer tain groups, and lead to unfair treatment\nacross various domains. Some examples of how biased AI model s can adversely impact diﬀerent\nareas include:\n• Hiring: AI-driven hiring tools that use biased models may exhibit u nfair treatment towards\napplicants from underrepresented groups or those with non- traditional backgrounds. This\ncould lead to the perpetuation of existing inequalities in t he job market, limiting opportunities\nfor aﬀected individuals and reducing diversity in the workfo rce ([111, 112]). Large language\nmodels can be used to automate the screening of job applicant s, such as by analyzing resumes\nand cover letters. Since these models are trained on vast amo unts of text data, they may\nhave internalized biases present in the data, such as gender or racial biases. As a result, they\ncould unintentionally favor certain applicants or disqual ify others based on factors unrelated\nto their qualiﬁcations, reinforcing existing inequalitie s in the job market.\n• Lending: Financial institutions increasingly rely on AI models for credit scoring and lending\ndecisions. Biased models may unfairly penalize certain gro ups, such as minority communities\n10\nPillar Description References\nRepresentation Ensuring that the training data used to develop AI models is\nrepresentative of the diverse range of perspectives, exper iences,\nand backgrounds that exist within society. This helps to red uce\nthe risk of biases being absorbed and propagated by the model s,\nleading to more equitable outcomes\n[99, 35, 94, 58, 100]\nTransparency Developers should be transparent about the methodologies,data\nsources, and potential limitations of their AI models, enab ling\nusers to better understand the factors that may inﬂuence the\nmodel’s predictions and decisions\n[101, 102]\nAccountability It is essential for developers and stakeholders to establish a clear\nframework for accountability, which may include monitoring the\nperformance of AI models, addressing biases and errors, and\nresponding to the concerns of users and aﬀected communities\n[103, 104, 105]\nInclusivity AI applications should be designed to be inclusive and accessible\nto all users, taking into account factors such as language, culture,\nand accessibility needs, to ensure that the beneﬁts of AI are\nshared equitably across society\n[106, 107]\nProtection of\nIP, human\nwork, and hu-\nman artistic\nexpression\nGenerative AI models have the remarkable capability to crea te\nhuman-like text, artwork, music, and more. This creative aspect\npresents unique challenges, including issues related to in tellec-\ntual property and the protection of human-generated copyri ght\nwork to avoid AI plagiarism\n[27]\nContinuous im-\nprovement\nDevelopers must commit to an ongoing process of evaluating,\nreﬁning, and improving their AI models to address biases and\nensure fairness over time. This may involve working with re-\nsearchers, policy makers, and aﬀected communities to gain i n-\nformation and feedback that can help guide the development o f\nmore equitable AI systems\n[108, 109, 58, 110]\nTable 5: Pillars of responsible generative AI development\nor individuals with lower socio-economic status, by assign ing them lower credit scores or\ndenying them access to loans and ﬁnancial services based on b iased assumptions ([113, 114,\n115]). In lending, large language models can be used to asses s creditworthiness or predict\nloan default risk, e.g., based on automated analysis of appl ication or support documents.\nIf the data used to train these models contain historical bia ses or discriminatory lending\npractices, the models may learn to replicate these patterns . Consequently, they could deny\nloans to certain demographics or oﬀer unfavorable terms base d on factors like race, gender,\nor socioeconomic status, perpetuating ﬁnancial inequalit y ([116]).\n• Content moderation : AI-powered content moderation systems help manage and ﬁlt er\nuser-generated content on social media platforms and other online communities. If these\nsystems are trained on biased data, they may disproportiona tely censor or suppress the\nvoices of certain groups, while allowing harmful content or misinformation from other sources\nto proliferate ([117, 118, 119, 120]). Language models can b e employed to automatically\nmoderate and ﬁlter content on social media platforms or onli ne forums. However, these\nmodels may struggle to understand the nuances of language, c ontext, and cultural diﬀerences.\nThey might over-moderate or under-moderate certain types o f content, disproportionately\naﬀecting certain groups or topics. This could lead to censors hip or the ampliﬁcation of\nharmful content, perpetuating biases and misinformation ( [121, 122, 123, 124]).\n11\n• Healthcare: AI models are increasingly used to support medical decisio n-making and re-\nsource allocation. Biased models may result in unfair treat ment for certain patient groups,\nleading to disparities in healthcare access and quality, an d potentially exacerbating existing\nhealth inequalities ([125, 126, 110, 105]). Large language models can be employed for tasks\nsuch as diagnosing diseases, recommending treatments, or a nalyzing patient data ([127, 128]).\nIf the training data includes biased or unrepresentative in formation, the models may produce\nbiased outcomes. Data used to train these models might be pre dominantly collected from\nspeciﬁc populations, leading to less accurate predictions or recommendations for underrepre-\nsented groups. This can result in misdiagnoses, inadequate treatment plans, or unequal access\nto care. Models might unintentionally learn to associate ce rtain diseases or conditions with\nspeciﬁc demographic factors, perpetuating stereotypes an d potentially inﬂuencing healthcare\nprofessionals’ decision-making. Finally, biases in healt hcare data could lead to models that\nprioritize certain types of treatments or interventions ov er others, disproportionately beneﬁt-\ning certain groups and disadvantaging others ([129, 130]).\n• Education: AI-driven educational tools and platforms can help person alize learning experi-\nences and improve educational outcomes. However, biased mo dels may perpetuate disparities\nby favoring certain learning styles or cultural background s, disadvantaging students from un-\nderrepresented or marginalized communities ([131, 132]). Large language models can be used\nin education for tasks such as personalized learning, gradi ng, or content creation. If the mod-\nels are trained on biased data, they may exacerbate existing biases in educational settings.\nFurthermore, if used for grading or assessing student work, language models might internalize\nbiases in historical grading practices, leading to unfair e valuation of students based on factors\nlike race, gender, or socioeconomic status. Finally, acces s in itself to ChatGPT or other AI\ntools for education can exacerbate preexisting inequaliti es.\n5.3. Paths to AI transparency\nTransparency and trust are essential components in the deve lopment and deployment of AI\nsystems. As these models become more integrated into variou s aspects of our lives, it is increasingly\nimportant for users and regulators to understand how they ma ke decisions and predictions, ensuring\nthat they operate fairly, ethically, and responsibly.\nEmphasizing transparency in AI systems can provide several beneﬁts:\n• Informed decision-making : When users and regulators have a clear understanding of\nhow AI models make decisions and predictions, they can make m ore informed choices about\nwhether to use or rely on these systems in diﬀerent contexts. T ransparency can empower\nusers to evaluate the potential risks and beneﬁts of AI syste ms and make decisions that align\nwith their values and priorities ([133, 134]).\n• Public trust : Fostering transparency can help build public trust in AI sy stems, as it demon-\nstrates a commitment to ethical development and responsibl e deployment. When users and\nregulators can see that developers are taking steps to ensur e the fairness and equity of their\nmodels, they may be more likely to trust and adopt these techn ologies ([101, 102]).\n• Ethical compliance : By promoting transparency, developers can demonstrate th eir compli-\nance with ethical guidelines and regulations, showcasing t heir commitment to the responsible\ndevelopment of AI systems. This can help to establish a stron g reputation and foster positive\nrelationships with users, regulators, and other stakehold ers ([135, 116]).\n12\n• Collaborative improvement : Transparency can facilitate collaboration between devel op-\ners, researchers, policymakers, and aﬀected communities, e nabling them to share insights\nand feedback that can help guide the development of more equi table and ethical AI systems\n([108, 109, 58, 110]).\nIn summary, emphasizing transparency and trust in AI system s, including generative language\nmodels, is crucial for ensuring that users and regulators ha ve a clear understanding of how these\nmodels make decisions and predictions. By promoting transp arency, developers can demonstrate\ntheir commitment to ethical development and responsible de ployment, fostering public trust and\npaving the way for more equitable and beneﬁcial AI applicati ons.\n5.4. Regulatory eﬀorts, industry standards, and ethical gu idelines\nAs concerns about bias in AI systems continue to grow, severa l ongoing regulatory eﬀorts\nand industry standards have emerged to address these challe nges. AI ethics guidelines and fair-\nness frameworks aim to provide guidance and best practices f or developers, organizations, and\npolicymakers to reduce bias and ensure responsible develop ment and deployment of AI systems\n([135, 136]).\nSome notable eﬀorts are summarized in Table 6. These ongoing r egulatory eﬀorts and industry\nstandards represent important steps in addressing bias and promoting the responsible develop-\nment of AI systems. By adhering to these guidelines and frame works, developers can contribute\nto the creation of AI technologies that are more equitable, f air, and beneﬁcial for all users and\ncommunities.\n6. The role of human oversight and intervention\n6.1. How to identify and mitigate bias?\nIdentifying and mitigating bias in AI models is essential fo r ensuring their responsible and\nequitable use. Various methods can be employed to address bi as in AI systems:\n• Regular audits : Conducting regular audits of AI models can help identify po tential biases,\nerrors, or unintended consequences in their outputs. These audits involve evaluating the\nmodel’s performance against a set of predeﬁned metrics and c riteria, which may include\nfairness, accuracy, and representativeness. By monitorin g AI models on an ongoing basis,\ndevelopers can detect and address biases before they become problematic ([104]).\n• Retraining with curated data : Retraining AI models with curated data can help reduce\nbiases in their predictions and decisions. By carefully sel ecting and preparing training data\nthat is more diverse, balanced, and representative of diﬀere nt perspectives, developers can\nensure that AI models learn from a broader range of inputs and experiences, which may help\nmitigate the inﬂuence of biases present in the original trai ning data ([79, 8]).\n• Applying fairness metrics : Fairness metrics can be used to evaluate the performance of\nAI models with respect to diﬀerent user groups or populations ([145, 146]). By analyzing AI\nmodel outputs based on these metrics, developers can identi fy potential disparities or biases\nin the model’s treatment of diﬀerent users and take steps to ad dress them. Examples of\nfairness metrics include demographic parity, equalized od ds, and equal opportunity ([147]).\n13\nTable 6: Ongoing Regulatory Eﬀorts and Industry Standards t o Address Bias in AI\nEﬀort Description\nEuropean Union’s AI\nEthics Guidelines\nThe EU’s High-Level Expert Group on Artiﬁcial Intelligence has developed a set of AI\nEthics Guidelines that outline key ethical principles and r equirements for trustworthy AI,\nincluding fairness, transparency, accountability, and human agency ([137]).\nIEEE’s Ethically\nAligned Design\nThe Institute of Electrical and Electronics Engineers (IEE E) has published a comprehen-\nsive document, ”Ethically Aligned Design,” which provides recommendations and guide-\nlines for the ethical development of AI and autonomous syste ms, with a focus on human\nrights, data agency, and technical robustness ([138]).\nPartnership on AI A coalition of tech companies, research in stitutes, and civil society organizations, the\nPartnership on AI aims to promote the responsible development and use of AI technologies.\nThey work on various initiatives, including fairness, tran sparency, and accountability, to\nensure that AI beneﬁts all of humanity ([139]).\nAI Fairness 360\n(AIF360)\nDeveloped by IBM Research, AIF360 is an open-source toolkit that provides a compre-\nhensive suite of metrics and algorithms to help developers d etect and mitigate bias in\ntheir AI models. It assists developers in understanding and addressing fairness concerns\nin their AI applications ([140]).\nGoogle’s AI Princi-\nples\nGoogle has outlined a set of AI Principles that guide the ethi cal development and use of\nAI technologies within the company. These principles empha size fairness, transparency,\naccountability, and the importance of avoiding harmful or u njust impacts ([141]).\nAlgorithmic Impact\nAssessment (AIA)\nDeveloped by the AI Now Institute, the AIA is a framework designed to help organizations\nevaluate and mitigate the potential risks and harms of AI sys tems. The AIA guides\norganizations through a structured process of identifying and addressing potential biases,\ndiscrimination, and other negative consequences of AI depl oyment ([142]).\nOECD Recommenda-\ntion of the Council on\nArtiﬁcial Intelligence\nThe OECD Recommendation of the Council on Artiﬁcial Intelligence is a set of guidelines\nthat provides a framework for the responsible development a nd deployment of AI, with\nﬁve principles focused on inclusive growth, human-centere d values, transparency and ex-\nplainability, robustness, security and safety, and accountability ([143, 144]).\n• Algorithmic debiasing techniques : Various algorithmic techniques have been developed\nto mitigate bias in AI models during training or post-proces sing. Some of these techniques\ninclude adversarial training, re-sampling, and re-weight ing, which aim to minimize the inﬂu-\nence of biased patterns and features on the model’s predicti ons and decisions ([28, 148, 36,\n70, 49, 100, 114, 112]).\n• Inclusion of diverse perspectives : Ensuring that AI development teams are diverse and\ninclusive can help bring a wide range of perspectives and exp eriences to the table, which\ncan contribute to the identiﬁcation and mitigation of biase s in AI models. By involving\nindividuals from diﬀerent backgrounds, cultures, and disci plines, developers can create more\nrobust, fair, and representative AI systems ([67, 68]).\n• Human-in-the-loop approaches : Incorporating human experts into the AI model devel-\nopment and decision-making processes can help provide valu able contextual understanding\nand ethical judgment that AI models may lack. Humans can serv e as an additional layer of\nquality control, identifying biases, errors, or unintende d consequences in AI model outputs\nand providing feedback to improve the model’s performance a nd fairness.\nNext, we dive deeper into the role that human experts can play in the responsible design and\n14\ndevelopment of AI systems, including large language models , and their continuous oversight.\n6.2. The importance of humans in the AI loop\nEmphasizing the importance of involving human experts in AI system development, monitoring,\nand decision-making is crucial to ensuring the responsible and ethical deployment of AI technologies.\nHumans possess the ability to provide context and ethical ju dgment that AI models may lack, and\ntheir involvement can help address potential biases, error s, and unintended consequences that may\narise from the use of these systems. Some key beneﬁts of invol ving human experts in AI system\ndevelopment include:\n• Contextual understanding : Human experts can provide valuable insights into the cultu ral,\nsocial, and historical contexts that shape language and com munication, helping to guide AI\nmodels in generating more appropriate and sensitive respon ses ([131, 149, 150]).\n• Ethical judgment : Human experts possess the moral and ethical reasoning skil ls needed to\nevaluate the potential impacts of AI systems on users and aﬀec ted communities. By involving\nhuman experts in decision-making, we can ensure that AI mode ls align with ethical principles\nand values, such as fairness, transparency, and accountabi lity ([138, 135, 151, 137, 116]).\n• Bias identiﬁcation and mitigation : Human experts can help identify and address biases in\nAI models, working alongside developers to implement strat egies for mitigating or eliminating\nharmful biases and promoting more equitable and representa tive AI systems ([36, 100, 112]).\n• Quality assurance and validation : Human experts can serve as a vital layer of quality\ncontrol, evaluating AI model outputs for coherence, releva nce, and potential biases, and\nproviding feedback to improve the model’s performance, acc uracy, regulatory compliance,\nand trustworthiness ([152]).\n• Human override : Incorporating human experts into AI system workﬂows can he lp strike a\nbalance between automation and human judgment, allowing hu mans to intervene and override\nAI model decisions when necessary to ensure fairness, accou ntability, and ethical compliance\n([153]).\nBy involving human experts in the development, monitoring, and decision-making processes of\nAI systems, we can leverage their contextual understanding and ethical judgment to complement\nthe capabilities of AI models. This collaborative approach can help us create AI systems that are\nmore responsible, equitable, and beneﬁcial for all users, w hile also addressing the potential risks\nand challenges associated with bias in AI.\n6.3. Possible strategies and best practices to address bias in generative AI\nAddressing and mitigating potential biases in generative A I models requires a collaborative\neﬀort between AI developers, users, and aﬀected communities. Fostering a more inclusive and fair\nAI ecosystem involves engaging various stakeholders in the development, evaluation, and deploy-\nment of AI technologies. This collaboration can oversee tha t AI models are designed to be more\nequitable, representative, and beneﬁcial to all users.\nSome key aspects of fostering collaboration in the AI ecosys tem are shown in Table 7. By fos-\ntering collaboration between AI developers, users, and aﬀec ted communities, we can work towards\n15\nStrategy Description References\nEngaging with af-\nfected communities\nInvolving aﬀected communities in the development\nand evaluation of AI models can lead to the creation\nof generative AI systems that are more culturally sen-\nsitive, contextually relevant, and fair to all users\n[154, 155, 156, 157, 158, 159,\n160]\nMultidisciplinary col-\nlaboration\nBringing together experts from diﬀerent ﬁelds, such\nas computer science, social sciences, humanities, and\nethics, can help to develop more robust strategies for\naddressing and mitigating bias in generative AI sys-\ntems\n[109, 41, 156]\nUser feedback and\nevaluation\nEncouraging users to provide feedback on AI model\noutputs and performance can contribute to the on-\ngoing improvement and reﬁnement of generative AI\nmodels, ensuring that they remain fair, accurate, and\nrelevant to users’ needs\n[161, 162, 163, 164]\nOpenness and trans-\nparency\nSharing information about the methodologies, data\nsources, and potential biases of generative AI mod-\nels can enable stakeholders to make more informed\ndecisions about whether and how to use these tech-\nnologies in diﬀerent contexts\n[156, 165, 143, 135]\nEstablishing partner-\nships\nForming partnerships between AI developers, re-\nsearch institutions, non-proﬁt organizations, and\nother stakeholders can facilitate the sharing of knowl-\nedge, resources, and best practices, leading to the de-\nvelopment of more equitable and responsible AI tech-\nnologies\n[135]\nTable 7: Strategies for addressing bias in generative AI sys tems\ncreating a more inclusive and fair AI ecosystem that respect s and values diverse perspectives and\nexperiences. This collaborative approach can help ensure t hat AI technologies are developed and\ndeployed in a way that is equitable, responsible, and beneﬁc ial for all users, while also addressing\nthe potential risks and challenges associated with bias in A I.\n7. Conclusions\nThis paper highlights the challenges and risks associated w ith biases in generative language\nmodels like ChatGPT, emphasizing the need for a multi-disci plinary, collaborative eﬀort to develop\nmore equitable, transparent, and responsible AI systems th at enhance a wide array of applications\nwhile minimizing unintended consequences.\nVarious methods for identifying and mitigating bias in AI mo dels were presented, including\nregular audits, retraining with curated data, applying fai rness metrics, and incorporating human\nexperts in AI system development, monitoring, and decision -making. To achieve this goal, the\ndevelopment and deployment of AI technologies should prior itize ethical principles, such as fairness\nand equality, ensuring that all users and groups are treated equitably. Human oversight plays a vital\nrole in providing context and ethical judgment that AI model s may lack, helping to identify and\naddress potential biases, errors, or unintended consequen ces. Collaboration between AI developers,\nusers, and aﬀected communities is essential for fostering a m ore inclusive and fair AI ecosystem,\nensuring that diverse perspectives and experiences are con sidered and valued.\nContinued research into methods for identifying, addressi ng, and mitigating biases in AI models\nwill be critical to advancing the state of the art and promoti ng more equitable and inclusive AI\n16\nTable 8: Future avenues for Large Language Models and genera tive AI research.\nResearch Area Description\nFairness, Bias, and Ethics Addressing and minimizing biase s in language models, as well as under-\nstanding their ethical implications, is a critical area of r esearch; developing\nmethods to detect, mitigate and prevent biases in AI models i s essential.\nInterpretability and Explainability Understanding the in ternal workings of large language models is a signiﬁ-\ncant challenge; researchers are developing methods to make models more\ninterpretable and explain their predictions.\nAuditability and Accountability Large language models inc reasingly impact various sectors of society, inﬂu-\nencing decision-making and shaping public discourse; ensu ring that models\nare transparent, their actions can be traced, and those resp onsible for their\ndevelopment and deployment can be held accountable for the c onsequences\nof the AI’s actions is vital for fostering trust and maintain ing ethical stan-\ndards in the AI community and beyond.\nControllable and Safe AI Ensuring that AI models can generat e outputs that align with human inten-\ntions and values is an important research question; develop ing methods to\ncontrol AI behavior, reduce harmful outputs and improve saf ety measures\nis vital.\nSocietal Eﬀects The societal eﬀects and implications of the deployment of AI systems encom-\npass a wide range of concerns, including labor markets, priv acy, bias, access\nto technology, public discourse, security, ethics, and reg ulation; Observing,\ncharacterizing, quantifying and understanding the broade r eﬀects that the\ndeployment of AI systems has on society warrant careful cons ideration and\ncontinued research as these technologies continue to proli ferate.\nsystems. By bringing together experts from various discipl ines, including computer science, social\nsciences, humanities, and ethics, we can foster a more compr ehensive understanding of the potential\nbiases and ethical challenges associated with AI applicati ons.\nFostering an open and ongoing dialogue between stakeholder s is crucial for sharing knowledge,\nbest practices, and lessons learned from the development an d use of AI applications. This dialogue\ncan help to raise awareness of the potential risks and challe nges associated with biases in AI models\nand promote the development of strategies and guidelines fo r mitigating their negative impacts.\nFuture research avenues. As the development of large language models continues, seve ral essential\naspects of research are necessary to advance their understa nding and ensure their responsible\ndeployment (cf., Table 8). Among these are understanding their inner workin gs, addressing ethical\nconcerns, ensuring controllability and safety, and develo ping more robust evaluation methods.\nOne critical area of research is fairness, bias, and ethics, which involves detecting, mitigating,\nand preventing biases in AI models to minimize their impact o n various sectors of society. Another\nimportant research question is interpretability and expla inability, as understanding the internal\nworkings of large language models remains a signiﬁcant chal lenge. Researchers are working to\nmake models more interpretable and explain their predictio ns. Additionally, large language models\ncan inﬂuence decision-making and shape public discourse, m aking auditability and accountability\nessential for fostering trust and maintaining ethical stan dards in the AI community and beyond.\nControllable and safe AI is also important to ensure that AI m odels can generate outputs\nthat align with human intentions and values. Finally, obser ving, characterizing, quantifying, and\nunderstanding the broader eﬀects that deploying AI systems h as on society is critical for addressing\n17\nsocietal eﬀects, which encompass a wide range of concerns, in cluding labor markets, privacy, bias,\naccess to technology, public discourse, security, ethics, and regulation. The need for a multi-\ndisciplinary, collaborative eﬀort to develop more equitabl e, transparent, and responsible AI systems\nis clear.\nThis paper emphasized the challenges and risks associated w ith biases in generative language\nmodels like ChatGPT and highlights the importance of contin ued research to develop responsible\nAI systems that enhance a wide array of applications while mi nimizing unintended consequences.\nAcknowledgements\nThe author is grateful to all current and past members of his l ab at USC, and the numerous\ncolleagues and students at USC Viterbi and Annenberg, who en gaged in stimulating discussions\nand provided invaluable feedback about this study.\nReferences\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones , A. N. Gomez, /suppress L. Kaiser, I. Polosukhin, Attention\nis all you need, Advances in neural information processing s ystems 30 (2017).\n[2] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-t raining of deep bidirectional transformers for\nlanguage understanding, arXiv preprint arXiv:1810.04805 (2018).\n[3] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., Improving language understanding by generative\npre-training (2018).\n[4] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskev er, et al., Language models are unsupervised\nmultitask learners, OpenAI blog 1 (8) (2019) 9.\n[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dh ariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al., Language models are few-shot learners, Ad vances in neural information processing systems\n33 (2020) 1877–1901.\n[6] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimo v, Proximal policy optimization algorithms, arXiv\npreprint arXiv:1707.06347 (2017).\n[7] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, G. Irving, Fine-tuning\nlanguage models from human preferences, arXiv preprint arX iv:1909.08593 (2019).\n[8] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\net al., Training language models to follow instructions wit h human feedback, arXiv preprint arXiv:2203.02155\n(2022).\n[9] OpenAI, Gpt-4 technical report (2023).\n[10] Y. Bengio, R. Ducharme, P. Vincent, A neural probabilis tic language model, Advances in neural information\nprocessing systems 13 (2000).\n[11] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean , Distributed representations of words and phrases\nand their compositionality, Advances in neural informatio n processing systems 26 (2013).\n[12] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to sequence learning with neural networks, Advances in neural\ninformation processing systems 27 (2014).\n[13] T. Young, D. Hazarika, S. Poria, E. Cambria, Recent trends in deep learning based natural language processing,\nIEEE Computational intelligenCe magazine 13 (3) (2018) 55– 75.\n[14] H. Chen, X. Liu, D. Yin, J. Tang, A survey on dialogue syst ems: Recent advances and new frontiers, Acm\nSIGKDD Explorations Newsletter 19 (2) (2017) 25–35.\n[15] S. Zhang, E. Dinan, J. Urbanek, A. Szlam, D. Kiela, J. Wes ton, Personalizing dialogue agents: I have a dog,\ndo you have pets too?, arXiv preprint arXiv:1801.07243 (201 8).\n[16] T.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. M. Rojas-B arahona, P.-H. Su, S. Ultes, S. Young, A network-\nbased end-to-end trainable task-oriented dialogue system , arXiv preprint arXiv:1604.04562 (2016).\n[17] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, L. S. Chao , Learning deep transformer models for machine\ntranslation, arXiv preprint arXiv:1906.01787 (2019).\n[18] A. Karakanta, J. Dehdari, J. van Genabith, Neural machi ne translation for low-resource languages without\nparallel corpora, Machine Translation 32 (2018) 167–189.\n18\n[19] N. Pourdamghani, K. Knight, Neighbors helping the poor : improving low-resource machine translation using\nrelated languages, Machine Translation 33 (3) (2019) 239–2 58.\n[20] M. R. Costa-juss` a, J. Cross, O. C ¸elebi, M. Elbayad, K.Heaﬁeld, K. Heﬀernan, E. Kalbassi, J. Lam, D. Licht,\nJ. Maillard, et al., No language left behind: Scaling human- centered machine translation, arXiv preprint\narXiv:2207.04672 (2022).\n[21] S. Ranathunga, E.-S. A. Lee, M. Prifti Skenduli, R. Shek har, M. Alam, R. Kaur, Neural machine translation\nfor low-resource languages: A survey, ACM Computing Survey s 55 (11) (2023) 1–37.\n[22] C. Christianson, J. Duncan, B. Onyshkevych, Overview o f the DARPA LORELEI program, Machine Transla-\ntion 32 (2018) 3–9.\n[23] K.-C. Yang, E. Ferrara, F. Menczer, Botometer 101: Soci al bot practicum for computational social scientists,\nJournal of Computational Social Science 5 (2) (2022) 1511–1 528.\n[24] E. Ferrara, Social bot detection in the age of ChatGPT: C hallenges and opportunities, First Monday 28 (6)\n(2023).\n[25] A. Gilson, C. W. Safranek, T. Huang, V. Socrates, L. Chi, R. A. Taylor, D. Chartash, et al., How does\nChatGPT perform on the united states medical licensing exam ination? the implications of large language\nmodels for medical education and knowledge assessment, JMI R Medical Education 9 (1) (2023) e45312.\n[26] J. H. Choi, K. E. Hickman, A. Monahan, D. Schwarcz, ChatG PT goes to law school, Available at SSRN (2023).\n[27] E. Ferrara, GenAI against humanity: Nefarious applica tions of generative artiﬁcial intelligence and large\nlanguage models, arXiv preprint arXiv:2310.00737 (2023).\n[28] T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, A. T. Kalai, Man is to computer programmer as woman\nis to homemaker? debiasing word embeddings, Advances in neu ral information processing systems 29 (2016).\n[29] A. Caliskan, J. J. Bryson, A. Narayanan, Semantics deri ved automatically from language corpora contain\nhuman-like biases, Science 356 (6334) (2017) 183–186.\n[30] S. L. Blodgett, S. Barocas, H. Daum´ e III, H. Wallach, La nguage (technology) is power: A critical survey of\n“bias” in NLP, arXiv preprint arXiv:2005.14050 (2020).\n[31] E. M. Bender, T. Gebru, A. McMillan-Major, S. Shmitchell, On the dangers of stochastic parrots: Can language\nmodels be too big?, in: Proceedings of the 2021 ACM conferenc e on fairness, accountability, and transparency,\n2021, pp. 610–623.\n[32] I. Solaiman, M. Brundage, J. Clark, A. Askell, A. Herber t-Voss, J. Wu, A. Radford, G. Krueger, J. W. Kim,\nS. Kreps, et al., Release strategies and the social impacts o f language models, arXiv preprint arXiv:1908.09203\n(2019).\n[33] D. Hovy, S. Prabhumoye, Five sources of bias in natural l anguage processing, Language and Linguistics Com-\npass 15 (8) (2021) e12432.\n[34] R. Munro, S. Bethard, V. Kuperman, V. T. Lai, R. Melnick, C. Potts, T. Schnoebelen, H. Tily, Crowdsourcing\nand language studies: the new generation of linguistic data , in: NAACL Workshop on Creating Speech and\nLanguage Data With Amazon’s Mechanical Turk, Association f or Computational Linguistics, 2010, pp. 122–\n130.\n[35] J. Buolamwini, T. Gebru, Gender shades: Intersectiona l accuracy disparities in commercial gender classiﬁcation,\nin: Conference on fairness, accountability and transparen cy, PMLR, 2018, pp. 77–91.\n[36] E. M. Bender, B. Friedman, Data statements for natural l anguage processing: Toward mitigating system bias\nand enabling better science, Transactions of the Associati on for Computational Linguistics 6 (2018) 587–604.\n[37] J. Kleinberg, S. Mullainathan, M. Raghavan, Inherent t rade-oﬀs in the fair determination of risk scores, arXiv\npreprint arXiv:1609.05807 (2016).\n[38] R. Benjamin, Race after technology: Abolitionist tool s for the new jim code (2020).\n[39] F. Doshi-Velez, B. Kim, Towards a rigorous science of in terpretable machine learning, arXiv preprint\narXiv:1702.08608 (2017).\n[40] R. Binns, Fairness in machine learning: Lessons from po litical philosophy, in: Conference on fairness, account-\nability and transparency, PMLR, 2018, pp. 149–159.\n[41] K. Crawford, R. Dobbe, T. Dryer, G. Fried, B. Green, E. Ka ziunas, A. Kak, V. Mathur, E. McElroy, A. N.\nS´ anchez, et al., Ai now 2019 report, New York, NY: AI Now Institute (2019).\n[42] M. O. Prates, P. H. Avelar, L. C. Lamb, Assessing gender b ias in machine translation: a case study with\nGoogle translate, Neural Computing and Applications 32 (20 20) 6363–6381.\n[43] OpenAI, GPT-4 System Card (2023).\n[44] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Ho rvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li,\nS. Lundberg, et al., Sparks of artiﬁcial general intelligen ce: Early experiments with gpt-4, arXiv preprint\narXiv:2303.12712 (2023).\n19\n[45] C. Raﬀel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Ma tena, Y. Zhou, W. Li, P. J. Liu, Exploring the\nlimits of transfer learning with a uniﬁed text-to-text tran sformer, The Journal of Machine Learning Research\n21 (1) (2020) 5485–5551.\n[46] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. La chaux, T. Lacroix, B. Rozi` ere, N. Goyal, E. Hambro,\nF. Azhar, et al., Llama: Open and eﬃcient foundation languag e models, arXiv preprint arXiv:2302.13971\n(2023).\n[47] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual Chat GPT: Talking, drawing and editing with visual\nfoundation models, arXiv preprint arXiv:2303.04671 (2023 ).\n[48] H. R. Kirk, Y. Jun, F. Volpin, H. Iqbal, E. Benussi, F. Dre yer, A. Shtedritski, Y. Asano, Bias out-of-the-box:\nAn empirical analysis of intersectional occupational bias es in popular generative language models, Advances\nin neural information processing systems 34 (2021) 2611–26 24.\n[49] S. Bordia, S. R. Bowman, Identifying and reducing gende r bias in word-level language models, arXiv preprint\narXiv:1904.03035 (2019).\n[50] M. T. Ribeiro, T. Wu, C. Guestrin, S. Singh, Beyond accur acy: Behavioral testing of NLP models with\nchecklist, arXiv preprint arXiv:2005.04118 (2020).\n[51] A. Conneau, G. Lample, M. Ranzato, L. Denoyer, H. J´ egou , Word translation without parallel data, arXiv\npreprint arXiv:1710.04087 (2017).\n[52] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Che n, N. Thorat, F. Vi´ egas, M. Wattenberg,\nG. Corrado, et al., Google’s multilingual neural machine tr anslation system: Enabling zero-shot translation,\nTransactions of the Association for Computational Linguis tics 5 (2017) 339–351.\n[53] T. Pires, E. Schlinger, D. Garrette, How multilingual i s multilingual BERT?, arXiv preprint arXiv:1906.01502\n(2019).\n[54] S. Ruder, I. Vuli´ c, A. Søgaard, A survey of cross-lingual word embedding models, Journal of Artiﬁcial Intelli-\ngence Research 65 (2019) 569–631.\n[55] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi , F. Roesner, Y. Choi, Defending against neural fake\nnews, Advances in neural information processing systems 32 (2019).\n[56] R. T. McCoy, E. Pavlick, T. Linzen, Right for the wrong re asons: Diagnosing syntactic heuristics in natural\nlanguage inference, arXiv preprint arXiv:1902.01007 (201 9).\n[57] N. A. Smith, Contextual word representations: putting words into computers, Communications of the ACM\n63 (6) (2020) 66–74.\n[58] M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman , B. Hutchinson, E. Spitzer, I. D. Raji, T. Gebru,\nModel cards for model reporting, in: Proceedings of the conference on fairness, accountability, and transparency,\n2019, pp. 220–229.\n[59] N. Garg, L. Schiebinger, D. Jurafsky, J. Zou, Word embed dings quantify 100 years of gender and ethnic\nstereotypes, Proceedings of the National Academy of Scienc es 115 (16) (2018) E3635–E3644.\n[60] L. Dixon, J. Li, J. Sorensen, N. Thain, L. Vasserman, Mea suring and mitigating unintended bias in text\nclassiﬁcation, in: Proceedings of the 2018 AAAI/ACM Confer ence on AI, Ethics, and Society, 2018, pp. 67–73.\n[61] R. W. McGee, Is ChatGPT biased against conservatives? a n empirical study, An Empirical Study (February\n15, 2023) (2023).\n[62] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,\nS. Gehrmann, et al., Palm: Scaling language modeling with pa thways, arXiv preprint arXiv:2204.02311 (2022).\n[63] Z. Jiang, F. F. Xu, J. Araki, G. Neubig, How can we know wha t language models know?, Transactions of the\nAssociation for Computational Linguistics 8 (2020) 423–43 8.\n[64] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herb ert-Voss, K. Lee, A. Roberts, T. B. Brown, D. Song,\nU. Erlingsson, et al., Extracting training data from large l anguage models., in: USENIX Security Symposium,\nVol. 6, 2021.\n[65] E. Ferrara, The history of digital spam, Communication s of the ACM 62 (8) (2019) 82–91.\n[66] E. Ferrara, Twitter spam and false accounts prevalence , detection, and characterization: A survey, First\nMonday 27 (12) (2022).\n[67] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Fo ster, J. Phang, H. He, A. Thite, N. Nabeshima,\net al., The pile: An 800gb dataset of diverse text for languag e modeling, arXiv preprint arXiv:2101.00027\n(2020).\n[68] S. Biderman, K. Bicheno, L. Gao, Datasheet for the pile, arXiv preprint arXiv:2201.07311 (2022).\n[69] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, N. A. Smith, Annotation artifacts in\nnatural language inference data, arXiv preprint arXiv:180 3.02324 (2018).\n[70] S. Dev, J. Phillips, Attenuating bias in word vectors, i n: The 22nd International Conference on Artiﬁcial\n20\nIntelligence and Statistics, PMLR, 2019, pp. 879–887.\n[71] E. Ferrara, The butterﬂy eﬀect in artiﬁcial intelligen ce systems: Implications for ai bias and fairness, arXiv\npreprint arXiv:2307.05842 (2023).\n[72] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, D. Zhou, Chain of thought prompting elicits\nreasoning in large language models, arXiv preprint arXiv:2 201.11903 (2022).\n[73] T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, LLM. int8 (): 8-bit matrix multiplication for transformers\nat scale, Advances in Neural Information Processing System s 35 (2022) 30318–30332.\n[74] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid , A. Fisch, A. R. Brown, A. Santoro, A. Gupta,\nA. Garriga-Alonso, et al., Beyond the imitation game: Quant ifying and extrapolating the capabilities of lan-\nguage models, arXiv preprint arXiv:2206.04615 (2022).\n[75] J. Wei, Y. Tay, R. Bommasani, C. Raﬀel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler,\net al., Emergent abilities of large language models, arXiv p reprint arXiv:2206.07682 (2022).\n[76] S. Chiappa, Path-speciﬁc counterfactual fairness, in : Proceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, Vol. 33, 2019, pp. 7801–7808.\n[77] V. Kumar, H. Koorehdavoudi, M. Moshtaghi, A. Misra, A. C hadha, E. Ferrara, Controlled text generation\nwith hidden representation transformations, in: Findings of the Association for Computational Linguistics:\nACL 2023, Association for Computational Linguistics, Toro nto, Canada, 2023, pp. 9440–9455.\n[78] D. Hovy, S. L. Spruit, The social impact of natural langu age processing, in: Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics ( Volume 2: Short Papers), 2016, pp. 591–598.\n[79] S. Gururangan, A. Marasovi´ c, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, N. A. Smith, Don’t stop\npretraining: Adapt language models to domains and tasks, ar Xiv preprint arXiv:2004.10964 (2020).\n[80] J. H. Park, P. Fung, One-step and two-step classiﬁcatio n for abusive language detection on twitter, arXiv\npreprint arXiv:1706.01206 (2017).\n[81] Y. Bisk, A. Holtzman, J. Thomason, J. Andreas, Y. Bengio , J. Chai, M. Lapata, A. Lazaridou, J. May,\nA. Nisnevich, et al., Experience grounds language, arXiv pr eprint arXiv:2004.10151 (2020).\n[82] P. Bourdieu, Language and symbolic power, Harvard Univ ersity Press, 1991.\n[83] N. Fairclough, Language and power, Pearson Education, 2001.\n[84] G. Lakoﬀ, M. Johnson, Metaphors we live by, University o f Chicago press, 2008.\n[85] J. H. Hill, The everyday language of white racism, John W iley & Sons, 2009.\n[86] B. L. Whorf, Language, thought, and reality: Selected w ritings, MIT press, 2012.\n[87] M. Foucault, Archaeology of knowledge, Routledge, 201 3.\n[88] C. Geertz, The interpretation of cultures, Vol. 5043, B asic books, 1973.\n[89] G. Hofstede, Culture’s consequences: International d iﬀerences in work-related values, Vol. 5, sage, 1984.\n[90] R. Inglehart, Christian Welzel Modernization, Cultur al Change, and Democracy The Human Development\nSequence, Cambridge: Cambridge university press, 2005.\n[91] H. C. Triandis, Individualism and collectivism, Routl edge, 2018.\n[92] S. A. Friedler, C. Scheidegger, S. Venkatasubramanian , On the (im)possibility of fairness, arXiv preprint\narXiv:1609.07236 (2016).\n[93] M. B. Zafar, I. Valera, M. Gomez Rodriguez, K. P. Gummadi, Fairness beyond disparate treatment & disparate\nimpact: Learning classiﬁcation without disparate mistrea tment, in: Proceedings of the 26th international\nconference on world wide web, 2017, pp. 1171–1180.\n[94] S. Barocas, M. Hardt, A. Narayanan, Fairness and machin e learning: Limitations and opportunities, Fairness\nand Machine Learning: Limitation and Oppotunities (2019).\n[95] S. S. Mufwene, The Ecology of Language Evolution. Cambr idge Approaches to Language Contact., ERIC,\n2001.\n[96] H. Jenkins, M. Deuze, Convergence culture (2008).\n[97] M. Castells, The rise of the network society, John wiley & sons, 2011.\n[98] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H . Wallach, H. D. Iii, K. Crawford, Datasheets for\ndatasets, Communications of the ACM 64 (12) (2021) 86–92.\n[99] A. Torralba, A. A. Efros, Unbiased look at dataset bias, in: CVPR 2011, IEEE, 2011, pp. 1521–1528.\n[100] T. Sun, A. Gaut, S. Tang, Y. Huang, M. ElSherief, J. Zhao , D. Mirza, E. Belding, K.-W. Chang, W. Y. Wang,\nMitigating gender bias in natural language processing: Lit erature review, arXiv preprint arXiv:1906.08976\n(2019).\n[101] S. Larsson, F. Heintz, Transparency in artiﬁcial inte lligence, Internet Policy Review 9 (2) (2020).\n[102] U. Ehsan, Q. V. Liao, M. Muller, M. O. Riedl, J. D. Weisz, Expanding explainability: Towards social trans-\nparency in AI systems, in: Proceedings of the 2021 CHI Confer ence on Human Factors in Computing Systems,\n21\n2021, pp. 1–19.\n[103] S. Wachter, B. Mittelstadt, L. Floridi, Transparent, explainable, and accountable AI for robotics, Science\nrobotics 2 (6) (2017) eaan6080.\n[104] I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-Loud, D. Theron, P. Barnes,\nClosing the AI accountability gap: Deﬁning an end-to-end fr amework for internal algorithmic auditing, in:\nProceedings of the 2020 conference on fairness, accountabi lity, and transparency, 2020, pp. 33–44.\n[105] H. Smith, Clinical AI: opacity, accountability, resp onsibility and liability, AI & SOCIETY 36 (2) (2021) 535–\n545.\n[106] M. R. Morris, Ai and accessibility, Communications of the ACM 63 (6) (2020) 35–37.\n[107] R. Schwartz, J. Dodge, N. A. Smith, O. Etzioni, Green AI , Communications of the ACM 63 (12) (2020) 54–63.\n[108] C. O’neil, Weapons of math destruction: How big data in creases inequality and threatens democracy, Crown,\n2017.\n[109] K. Holstein, J. Wortman Vaughan, H. Daum´ e III, M. Dudik, H. Wallach, Improving fairness in machine learning\nsystems: What do industry practitioners need?, in: Proceed ings of the 2019 CHI conference on human factors\nin computing systems, 2019, pp. 1–16.\n[110] R. Challen, J. Denny, M. Pitt, L. Gompels, T. Edwards, K . Tsaneva-Atanasova, Artiﬁcial intelligence, bias\nand clinical safety, BMJ Quality & Safety 28 (3) (2019) 231–2 37.\n[111] M. Bogen, A. Rieke, Help wanted: An examination of hiri ng algorithms, equity, and bias (2018).\n[112] M. Raghavan, S. Barocas, J. Kleinberg, K. Levy, Mitiga ting bias in algorithmic hiring: Evaluating claims\nand practices, in: Proceedings of the 2020 conference on fai rness, accountability, and transparency, 2020, pp.\n469–481.\n[113] D. K. Citron, F. Pasquale, The scored society: Due proc ess for automated predictions, Wash. L. Rev. 89 (2014)\n1.\n[114] N. T. Lee, P. Resnick, G. Barton, Algorithmic bias dete ction and mitigation: Best practices and policies to\nreduce consumer harms, Brookings Institute: Washington, D C, USA 2 (2019).\n[115] B. Ustun, A. Spangher, Y. Liu, Actionable recourse in l inear classiﬁcation, in: Proceedings of the conference\non fairness, accountability, and transparency, 2019, pp. 1 0–19.\n[116] L. Weidinger, J. Mellor, M. Rauh, C. Griﬃn, J. Uesato, P .-S. Huang, M. Cheng, M. Glaese, B. Balle,\nA. Kasirzadeh, et al., Ethical and social risks of harm from l anguage models, arXiv preprint arXiv:2112.04359\n(2021).\n[117] T. Gillespie, Custodians of the Internet: Platforms, content moderation, and the hidden decisions that shape\nsocial media, Yale University Press, 2018.\n[118] S. T. Roberts, Behind the screen, Yale University Pres s, 2019.\n[119] E. C. Choi, E. Ferrara, Automated claim matching with l arge language models: Empowering fact-checkers in\nthe ﬁght against misinformation, arXiv preprint arXiv:231 0.09223 (2023).\n[120] I. Augenstein, T. Baldwin, M. Cha, T. Chakraborty, G. L . Ciampaglia, D. Corney, R. DiResta, E. Fer-\nrara, S. Hale, A. Halevy, et al., Factuality challenges in th e era of large language models, arXiv preprint\narXiv:2310.05189 (2023).\n[121] M. Sap, D. Card, S. Gabriel, Y. Choi, N. A. Smith, The ris k of racial bias in hate speech detection, in:\nProceedings of the 57th annual meeting of the association fo r computational linguistics, 2019, pp. 1668–1678.\n[122] T. Davidson, D. Bhattacharya, I. Weber, Racial bias in hate speech and abusive language detection datasets,\narXiv preprint arXiv:1905.12516 (2019).\n[123] I. V. Pasquetto, B. Swire-Thompson, M. A. Amazeen, F. B enevenuto, N. M. Brashier, R. M. Bond, L. C.\nBozarth, C. Budak, U. K. Ecker, L. K. Fazio, et al., Tackling m isinformation: What researchers could do with\nsocial media data, The Harvard Kennedy School Misinformati on Review (2020).\n[124] F. Ezzeddine, O. Ayoub, S. Giordano, G. Nogara, I. Sbei ty, E. Ferrara, L. Luceri, Exposing inﬂuence campaigns\nin the age of llms: a behavioral-based ai approach to detecti ng state-sponsored trolls, EPJ Data Science 12 (1)\n(2023) 46.\n[125] M. A. Gianfrancesco, S. Tamang, J. Yazdany, G. Schmaju k, Potential biases in machine learning algorithms\nusing electronic health record data, JAMA internal medicin e 178 (11) (2018) 1544–1547.\n[126] Z. Obermeyer, B. Powers, C. Vogeli, S. Mullainathan, D issecting racial bias in an algorithm used to manage\nthe health of populations, Science 366 (6464) (2019) 447–45 3.\n[127] T. Davenport, R. Kalakota, The potential for artiﬁcia l intelligence in healthcare, Future healthcare journal\n6 (2) (2019) 94.\n[128] K. Y. Ngiam, W. Khor, Big data and machine learning algorithms for health-care delivery, The Lancet Oncology\n20 (5) (2019) e262–e273.\n22\n[129] J. K. Paulus, D. M. Kent, Predictably unequal: underst anding and addressing concerns that algorithmic clinical\nprediction may increase health disparities, NPJ digital me dicine 3 (1) (2020) 99.\n[130] E. Ferrara, Fairness and bias in artiﬁcial intelligen ce: A brief survey of sources, impacts, and mitigation\nstrategies, arXiv preprint arXiv:2304.07683 (2023).\n[131] M. C. Elish, D. Boyd, Situating methods in the magic of B ig Data and AI, Communication monographs 85 (1)\n(2018) 57–80.\n[132] N. Selwyn, Should robots replace teachers?: AI and the future of education, John Wiley & Sons, 2019.\n[133] B. Goodman, S. Flaxman, European union regulations on algorithmic decision-making and a “right to expla-\nnation”, AI magazine 38 (3) (2017) 50–57.\n[134] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, D. Pedreschi, A survey of methods for explaining\nblack box models, ACM computing surveys (CSUR) 51 (5) (2018) 1–42.\n[135] A. Jobin, M. Ienca, E. Vayena, The global landscape of A I ethics guidelines, Nature Machine Intelligence 1 (9)\n(2019) 389–399.\n[136] L. Floridi, Establishing the rules for building trust worthy AI, Nature Machine Intelligence 1 (6) (2019) 261–262.\n[137] Independent High-Level Expert Group on Artiﬁcial Int elligence, Ethics guidelines for trustworthy AI, Technical\nreport, European Commission (2019).\n[138] K. Shahriari, M. Shahriari, Ieee standard review—eth ically aligned design: A vision for prioritizing human well-\nbeing with artiﬁcial intelligence and autonomous systems, in: 2017 IEEE Canada International Humanitarian\nTechnology Conference (IHTC), IEEE, 2017, pp. 197–201.\n[139] J. Heer, The partnership on AI, AI Matters 4 (3) (2018) 2 5–26.\n[140] R. K. Bellamy, K. Dey, M. Hind, S. C. Hoﬀman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta,\nA. Mojsilovi´ c, et al., Ai fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias, IBM\nJournal of Research and Development 63 (4/5) (2019) 4–1.\n[141] S. Pichai, Ai at Google: our principles, The Keyword 7 ( 2018) 1–3.\n[142] D. Reisman, J. Schultz, K. Crawford, M. Whittaker, Alg orithmic impact assessments: A practical framework\nfor public agency, AI Now (2018).\n[143] C. Cath, S. Wachter, B. Mittelstadt, M. Taddeo, L. Flor idi, Artiﬁcial intelligence and the ‘good society’: the\nUS, EU, and UK approach, Science and engineering ethics 24 (2 018) 505–528.\n[144] K. Yeung, Recommendation of the council on artiﬁcial i ntelligence (OECD), International legal materials 59 (1)\n(2020) 27–34.\n[145] S. Yan, H.-t. Kao, E. Ferrara, Fair class balancing: En hancing model fairness without observing sensitive at-\ntributes, in: Proceedings of the 29th ACM International Conference on Information & Knowledge Management,\n2020, pp. 1715–1724.\n[146] Y. H. Ezzeldin, S. Yan, C. He, E. Ferrara, S. Avestimehr, Fairfed: Enabling group fairness in federated learning,\nin: Proceedings of the 37th AAAI Conference on Artiﬁcial Int elligence, 2023.\n[147] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, A. Gal styan, A survey on bias and fairness in machine\nlearning, ACM Computing Surveys (CSUR) 54 (6) (2021) 1–35.\n[148] B. H. Zhang, B. Lemoine, M. Mitchell, Mitigating unwan ted biases with adversarial learning, in: Proceedings\nof the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 2 018, pp. 335–340.\n[149] D. Leslie, Understanding artiﬁcial intelligence eth ics and safety, arXiv preprint arXiv:1906.05684 (2019).\n[150] Y. K. Dwivedi, L. Hughes, E. Ismagilova, G. Aarts, C. Co ombs, T. Crick, Y. Duan, R. Dwivedi, J. Edwards,\nA. Eirug, et al., Artiﬁcial intelligence (ai): Multidisciplinary perspectives on emerging challenges, opportunities,\nand agenda for research, practice and policy, Internationa l Journal of Information Management 57 (2021)\n101994.\n[151] N. A. Smuha, The EU approach to ethics guidelines for tr ustworthy artiﬁcial intelligence, Computer Law\nReview International 20 (4) (2019) 97–106.\n[152] M. Felderer, R. Ramler, Quality assurance for ai-base d systems: Overview and challenges (introduction to\ninteractive session), in: Software Quality: Future Perspe ctives on Software Engineering Quality: 13th Inter-\nnational Conference, SWQD 2021, Vienna, Austria, January 1 9–21, 2021, Proceedings 13, Springer, 2021, pp.\n33–42.\n[153] A. Etzioni, O. Etzioni, Keeping AI legal, Vand. J. Ent. & Tech. L. 19 (2016) 133.\n[154] W. Wallach, C. Allen, I. Smit, Machine morality: botto m-up and top-down approaches for modelling human\nmoral faculties, AI & Society 22 (2008) 565–582.\n[155] L. Taylor, R. Schroeder, Is bigger better? the emergen ce of big data as a tool for international development\npolicy, GeoJournal 80 (2015) 503–518.\n[156] B. D. Mittelstadt, P. Allo, M. Taddeo, S. Wachter, L. Fl oridi, The ethics of algorithms: Mapping the debate,\n23\nBig Data & Society 3 (2) (2016) 2053951716679679.\n[157] V. Eubanks, Automating inequality: How high-tech too ls proﬁle, police, and punish the poor, St. Martin’s\nPress, 2018.\n[158] M. L. Gray, S. Suri, Ghost work: How to stop Silicon Vall ey from building a new global underclass, Eamon\nDolan Books, 2019.\n[159] S. M. West, M. Whittaker, K. Crawford, Discriminating systems, AI Now (2019).\n[160] S. Costanza-Chock, Design justice: Community-led pr actices to build the worlds we need, The MIT Press,\n2020.\n[161] H. Cramer, V. Evers, S. Ramlal, M. Van Someren, L. Rutle dge, N. Stash, L. Aroyo, B. Wielinga, The eﬀects of\ntransparency on trust in and acceptance of a content-based art recommender, User Modeling and User-adapted\ninteraction 18 (2008) 455–496.\n[162] M. K. Lee, D. Kusbit, E. Metsky, L. Dabbish, Working wit h machines: The impact of algorithmic and data-\ndriven management on human workers, in: Proceedings of the 3 3rd annual ACM conference on human factors\nin computing systems, 2015, pp. 1603–1612.\n[163] S. Amershi, D. Weld, M. Vorvoreanu, A. Fourney, B. Nush i, P. Collisson, J. Suh, S. Iqbal, P. N. Bennett,\nK. Inkpen, et al., Guidelines for human-AI interaction, in: Proceedings of the 2019 chi conference on human\nfactors in computing systems, 2019, pp. 1–13.\n[164] J. Stoyanovich, B. Howe, H. Jagadish, Responsible dat a management, Proceedings of the VLDB Endowment\n13 (12) (2020).\n[165] M. Taddeo, L. Floridi, Regulate artiﬁcial intelligen ce to avert cyber arms race (2018).\n24",
  "topic": "Unintended consequences",
  "concepts": [
    {
      "name": "Unintended consequences",
      "score": 0.6959565281867981
    },
    {
      "name": "Generative grammar",
      "score": 0.6205508708953857
    },
    {
      "name": "Computer science",
      "score": 0.5414320230484009
    },
    {
      "name": "Language model",
      "score": 0.5217517018318176
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4440208673477173
    },
    {
      "name": "Data science",
      "score": 0.408576101064682
    },
    {
      "name": "Engineering ethics",
      "score": 0.35052061080932617
    },
    {
      "name": "Management science",
      "score": 0.32860034704208374
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2644640803337097
    },
    {
      "name": "Political science",
      "score": 0.1908894181251526
    },
    {
      "name": "Engineering",
      "score": 0.12244990468025208
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": []
}