{
  "title": "Ontologies in Design: How Imagining a Tree Reveals Possibilities and Assumptions in Large Language Models",
  "url": "https://openalex.org/W4409767273",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2998376673",
      "name": "Nava Haghighi",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2792598163",
      "name": "Sunny Yu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A29409133",
      "name": "James A. Landay",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3183544319",
      "name": "Daniela Rosner",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4392844946",
    "https://openalex.org/W2158913922",
    "https://openalex.org/W2990737193",
    "https://openalex.org/W4402671390",
    "https://openalex.org/W1996149054",
    "https://openalex.org/W4243761327",
    "https://openalex.org/W4237739761",
    "https://openalex.org/W2343148021",
    "https://openalex.org/W1997772859",
    "https://openalex.org/W2009836421",
    "https://openalex.org/W4281975099",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4200220040",
    "https://openalex.org/W4379959055",
    "https://openalex.org/W4396832019",
    "https://openalex.org/W3122548859",
    "https://openalex.org/W1991691398",
    "https://openalex.org/W4295008953",
    "https://openalex.org/W4389520377",
    "https://openalex.org/W2051681494",
    "https://openalex.org/W3154654049",
    "https://openalex.org/W4256740660",
    "https://openalex.org/W2155987308",
    "https://openalex.org/W2801657541",
    "https://openalex.org/W4220654855",
    "https://openalex.org/W1976129032",
    "https://openalex.org/W4380319170",
    "https://openalex.org/W4221096628",
    "https://openalex.org/W4383679903",
    "https://openalex.org/W3031337494",
    "https://openalex.org/W2169651905",
    "https://openalex.org/W4225006216",
    "https://openalex.org/W2163470644",
    "https://openalex.org/W2133109597",
    "https://openalex.org/W4401023556",
    "https://openalex.org/W4385573090",
    "https://openalex.org/W4383681096",
    "https://openalex.org/W1979639339",
    "https://openalex.org/W2415124711",
    "https://openalex.org/W3084611909",
    "https://openalex.org/W2397482677",
    "https://openalex.org/W4399363436",
    "https://openalex.org/W4252124930",
    "https://openalex.org/W2941129187",
    "https://openalex.org/W1739227081",
    "https://openalex.org/W4366390744",
    "https://openalex.org/W4387389810",
    "https://openalex.org/W4255788436",
    "https://openalex.org/W3195275799",
    "https://openalex.org/W2098652735",
    "https://openalex.org/W4387835469",
    "https://openalex.org/W1990044033",
    "https://openalex.org/W4402670044",
    "https://openalex.org/W4206590911",
    "https://openalex.org/W4287854995",
    "https://openalex.org/W4387356327",
    "https://openalex.org/W4206725542",
    "https://openalex.org/W4210472718",
    "https://openalex.org/W1982411913",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W4402671004",
    "https://openalex.org/W4249144432",
    "https://openalex.org/W2034864883",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4385571748",
    "https://openalex.org/W4225002570",
    "https://openalex.org/W2611377837",
    "https://openalex.org/W2065450758",
    "https://openalex.org/W2890415006",
    "https://openalex.org/W4308595063",
    "https://openalex.org/W3118799308",
    "https://openalex.org/W4402671512",
    "https://openalex.org/W2146463557",
    "https://openalex.org/W4383682670",
    "https://openalex.org/W4396822010",
    "https://openalex.org/W4238049043",
    "https://openalex.org/W2943791936",
    "https://openalex.org/W2792824696",
    "https://openalex.org/W4402565126",
    "https://openalex.org/W4253834041",
    "https://openalex.org/W2304487534",
    "https://openalex.org/W4400206419",
    "https://openalex.org/W4307979480",
    "https://openalex.org/W3184644998",
    "https://openalex.org/W4287889476",
    "https://openalex.org/W4281291869",
    "https://openalex.org/W3113089198",
    "https://openalex.org/W4230842027",
    "https://openalex.org/W1498112155",
    "https://openalex.org/W2046215155",
    "https://openalex.org/W4401038683",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W2007909139",
    "https://openalex.org/W4251426354",
    "https://openalex.org/W1783594902",
    "https://openalex.org/W2912296668",
    "https://openalex.org/W52824097",
    "https://openalex.org/W569478347",
    "https://openalex.org/W2803869957",
    "https://openalex.org/W4232184928",
    "https://openalex.org/W2262972580",
    "https://openalex.org/W2608014122",
    "https://openalex.org/W4207080468",
    "https://openalex.org/W2315385127",
    "https://openalex.org/W3014293560",
    "https://openalex.org/W2046516603"
  ],
  "abstract": "Amid the recent uptake of Generative AI, sociotechnical scholars and critics have traced a multitude of resulting harms, with analyses largely focused on values and axiology (e.g., bias). While value-based analyses are crucial, we argue that ontologies -- concerning what we allow ourselves to think or talk about -- is a vital but under-recognized dimension in analyzing these systems. Proposing a need for a practice-based engagement with ontologies, we offer four orientations for considering ontologies in design: pluralism, groundedness, liveliness, and enactment. We share examples of potentialities that are opened up through these orientations across the entire LLM development pipeline by conducting two ontological analyses: examining the responses of four LLM-based chatbots in a prompting exercise, and analyzing the architecture of an LLM-based agent simulation. We conclude by sharing opportunities and limitations of working with ontologies in the design and development of sociotechnical systems.",
  "full_text": "Ontologies in Design: How Imagining a Tree Reveals Possibilites\nand Assumptions in Large Language Models\nNava Haghighi\nnava@cs.stanford.edu\nStanford University\nStanford, CA, USA\nSunny Yu\nStanford University\nStanford, CA, USA\nsyu03@stanford.edu\nJames Landay\nStanford University\nStanford, CA, USA\nlanday@stanford.edu\nDaniela Rosner\nUniversity of Washington\nSeattle, WA, USA\ndkrosner@uw.edu\nFigure 1: Images generated with GPT-4o. Prompts from left: (a) make me a picture of a tree. (b) I’m from Iran. make me a picture\nof a tree. (c) make me a picture of a tree [in Persian]. (d) everything in the world is connected. make me a picture of a tree.\nAbstract\nAmid the recent uptake of Generative AI, sociotechnical scholars\nand critics have traced a multitude of resulting harms, with analy-\nses largely focused on values and axiology (e.g., bias). While value-\nbased analyses are crucial, we argue that ontologies—concerning\nwhat we allow ourselves to think or talk about—is a vital but under-\nrecognized dimension in analyzing these systems. Proposing a need\nfor a practice-based engagement with ontologies, we offer four ori-\nentations for considering ontologies in design: pluralism, grounded-\nness, liveliness, and enactment. We share examples of potentialities\nthat are opened up through these orientations across the entire\nLLM development pipeline by conducting two ontological analyses:\nexamining the responses of four LLM-based chatbots in a prompt-\ning exercise, and analyzing the architecture of an LLM-based agent\nsimulation. We conclude by sharing opportunities and limitations\nof working with ontologies in the design and development of so-\nciotechnical systems.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI ’25, Yokohama, Japan\n© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-1394-1/25/04\nhttps://doi.org/10.1145/3706598.3713633\nCCS Concepts\n• Human-centered computing →HCI theory, concepts and\nmodels; Interaction design theory, concepts and paradigms ; • Com-\nputing methodologies →Natural language processing .\nKeywords\nontological design, ontologies, generative AI, large language models,\nfoundation models, LLM agents\nACM Reference Format:\nNava Haghighi, Sunny Yu, James Landay, and Daniela Rosner. 2025. Ontolo-\ngies in Design: How Imagining a Tree Reveals Possibilites and Assumptions\nin Large Language Models. In CHI Conference on Human Factors in Comput-\ning Systems (CHI ’25), April 26-May 1, 2025, Yokohama, Japan. ACM, New\nYork, NY, USA, 20 pages. https://doi.org/10.1145/3706598.3713633\n1 Introduction\nWhen you think of a tree, what do you imagine? What does your\ntree feel like? Where have you encountered this tree before? What\nlandscape does it exist in? How wouldyou describe it? The answers\nto these questions depend on how you have come to know a tree.\nIf you are a botanist, you might imagine the mineral nutrients it\naccepts from neighboring fungi seeking carbohydrates. If you are a\nnutritionist, you might recall the mast-fruiting varieties that hold\ncalories as starch in their roots. Or if you are a spiritual healer, you\nmight picture trees whispering to one another, how they accept the\ndelivery of mineral nutrients not as neatened biological processes\narXiv:2504.03029v1  [cs.HC]  3 Apr 2025\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan Haghighi, et al.\nbut as gifts, taking care of us as we care for them. In her celebrated\nbook Braiding Sweetgrass [73], author Robin Kimmerer, a moss\necologist and a member of the Citizen Potawatomi Nation, describes\ntrees across each of these epistemic registers. For her, they are not\nopposing entities but rather interwoven with cosmologies that\norient us to the world. They belong to ways of looking, feeling,\nand noticing that produce a source of identity and knowledge ([73],\np.24); an ontological encounter.\nOntology, the study of the nature of being, has long been core\nto humanistic lines of inquiry, and later to computing and infor-\nmation fields across the late 1980s and early 1990s to explain rep-\nresentations of data [139]. Yet ontology is notoriously difficult to\ncomprehend. As historian of science Ian Hacking opines, “If, like\nmyself, you can understand the aims of psychology, cosmology, and\ntheology, but are hard pressed to explain what a study of being in\ngeneral would be, you can hardly welcome talk of ontology” ([60],\np.1). Hacking usefully focuses on ontology as a means of grappling\nwith what we “allow ourselves to talk about” ([60], p.2). Similarly,\nethnographer and anthropologist Annemarie Mol articulates on-\ntology in terms of potentialities or “what belongs to the real, or\nconditions of possibility we live with” [92]. Mol looks to “ontolog-\nical politics” to highlight the “process of shaping” reality within\nthose conditions of possibility [92].\nThis notion of what we enable ourselves, as HCI analysts, to\ntalk about or deem possible is essential to any inquiry into design\nformations, particularly in a moment of increasing attention to\ngenerative AI (Gen AI). Many commentators have claimed that Gen\nAI has taken the world by surprise with its ability to complete many\ntasks that previously required human efforts [ 112]. Critics have\nwarned against these automated effects, identifying the differential\nexperiences produced by Gen AI, with benefits and harms falling\nalong existing lines of inequity such as race, gender, and disability\n[20, 47, 93, 149]. Within this body of work, efforts at algorithmic\nalignment, a process of ensuring that AI systems complement hu-\nman goals, have largely focused on axiological concerns related to\nvalues and ethics [1, 72, 87]. This focus on values entails a consider-\nation of judgment around emphasis or selection in design decisions\nsuch as, returning to our tree example, the choice of landscape the\ntree produces, or the style of the leaves rendered. It also empha-\nsizes selection absence [118]: the missing geographic locations (e.g.,\nIran) that an entity such as a tree might exist within; or the ignored\npictorial techniques such as Souzandouzi, a needlework technique\nused in Iran.\nWhile powerful and productive on their own, such values-oriented\nanalyses may overlook questions of representation underlying the\nrange of imagined possibilities: the underlying ontological assump-\ntions. For example, what are the boundaries of a tree? Does the\ntree have distinct parts or is it all connected? Is it pictured with its\nroots? Is the tree imagined by its spiritual or symbolic significance\nor aesthetic dimensions? Is it a biological individual or part of a\nnetwork of life such as a mycorrhizal network? Or as Kimmerer\nmight ask, what does its imbued spirit have to say? Each of these\nquestions has implications for not only what phenomena we ob-\nserve and comprehend as belonging to a category (demarcations),\nbut also for what phenomena we imagine as even possible (realms\nof possibility). Take the example of how this analysis may play out\nin therapy applications, an increasingly popular area for Gen AI\nresearch. An axiological orientation may identify value tensions\nand harms such as identities that are implicitly ignored or explicitly\ncensored [108]. However, an ontological orientation may reveal\nadditional considerations such as the possibility of a communal\n(compared with individualistic) healing or of connecting with spir-\nitual traditions. Or if we envision a tree as limited to the parts\nvisible above ground (trunk, branches, leaves, etc.), we may miss\nhow its roots form interconnections with other trees as well as\nother species such as mycorrhizal fungi, creating life-sustaining\nsymbiotic networks that exchange resources like water, carbon,\nand nitrogen. To picture these networks of exchange as somehow\nimportant or even sacred aspects of identifying a tree is to sensitize\nourselves to the workings of the surrounding environment. It is also\nto push our imagination toward changes that challenge existing\nparadigms or propose transformative alternatives.\nTo be sure, an ontological shift expressed by a Gen AI prompt\n(e.g., create a therapy protocol, or visualize a tree network) will not\nsolve major mental health or climate issues. But, as users of these\nsystems, we may find that an alternative ontological approach has\nimportant albeit subtle consequences for our perspectives on our\nown social and ecological lives, shaping what we notice and con-\ncern ourselves with in connection with a phenomenon as common\nas a tree. As for Figure 1, ontological considerations can shift what\ncomes into view. Despite iteratively grounding the prompt to “visu-\nalize a tree”—representing what the model deems to be an “Iranian”\naesthetic, for instance—the resulting outputs remain eerily similar.\nEach depicts a semicircle of symmetrically outstretched branches\nconnected to an isolated trunk without visible roots or ecologi-\ncal connections. A focus on ontology prompts HCI analysts to ask\nabout what is taken-for-granted and what might be otherwise:what\nare we enabling ourselves to talk—or think—about?\nAlthough ontological assumptions are embedded in all sociotech-\nnical systems, given the timely urgency of the discourse, here we\nfocus on Gen AI and in particular Large Language Models (LLMs),\nas they are currently the most widely utilized instance of generative\nmodels. This paper complements existing analyses of values and\nvalue-based harms in and through sociotechnical systems [61, 97],\nto consider the importance of ontologies and ontological harms\nin those systems. We define ontological harm as the downstream\nand often long-term negative impact of systems and practices that\nconstrain or foreclose ways of being in the world, limiting what\nwe enable ourselves to deem possible, or to talk or think about.\nExpanding scholarship on embedded bias and absence [64, 118], we\nexamine the consequences of embedded representations and their\npotentialities. We offer a provisional analysis of these emerging\norientations through two probing exercises: a first one with four\nLLM-based chatbots (GPT3.5, GPT4, Copilot, and Bard) and a sec-\nond one with an LLM-based agent architecture. Rather than offering\na definitive or representative account, these exercises set up a kind\nof thought experiment with which we consider how ontologies\nwork across LLM outputs as well as post-training procedures and\nthe architecture built on top of LLMs. Through these analyses, we\nfind that even when ontologically diverse data is present in the\ntraining data, this diversity remains relatively hidden. This insight\ncomplicates analyses that frame biased outputs as resolvable by\nincluding more and different training data.\nOntologies in Design CHI ’25, April 26-May 1, 2025, Yokohama, Japan\nTogether, this work makes two central contributions to HCI and\ndesign research. First, we identify ontological orientations as im-\nportant tools for examining LLMs and LLM-based systems, and the\ntaken-for-granted assumptions underlying their output and design.\nIn doing so, we demonstrate new opportunities where HCI and\ndesign can intervene and contribute to the ongoing LLM develop-\nments, moving beyond training data and considering the entire LLM\ndevelopment pipeline. Second, we illustrate the need for a practice-\nbased engagement with ontologies in design, and the importance\nof revealing potentialities through surfacing ontological assump-\ntions. We discuss the need for developing methods to address this\nchallenge, such as by creating breakdowns through queering [84]\nor disorientating [21].\n2 Ontology, Ontologies, and the “Ontological\nTurn”\nIn the simplest terms, ontology refers to the nature of “being, ” or\n“what is. ” As a philosophical orientation, it complements other\nbranches of philosophy such as epistemology (how one can know\nthings), and axiology (related to ethics and values or who is impor-\ntant and has standing) [ 91]. We engage the term in line with its\nrelatively recent readoption by critical humanities scholars intend-\ning to “elicit the ontological commitments of different cultures and\ngroups” [119] and unveil the “basic premises that different social\ngroups have about reality, what exists” [42]. Our interest in this\nrelational and plural understanding of ontology, hereafter referred\nto in the plural asontologies, follows a number of works that engage\nthe notion of multiple ontologies [102], pluriversality [33, 40, 109],\nmultiplicity [92], and the “ontological turn” [ 140]. This body of\nwork is particularly interested in the existence of multiplicities of\nontologies or views of the world, and its implications for science,\ntechnology, and the humanities.\nWithin related strands of science and technology studies, schol-\nars have described the ontological turn as a shift from textual analy-\nsis to the objects, actions, and phenomena that compose and inform\nthem—“the networks that enable agency to unfold and for facts to\nbecome cogent” [30]. This ontological orientation also shifts from\nperspectives on shared realities to a consideration of the “worlds”\nthat different philosophical frameworks and cultural experiences\nconstruct. The turn to world-building and the nature of being has\nsubsequently opened conceptual avenues for post-anthropocentric\nanalysis, and limited engagement with the politics of difference [30].\nWithin computer science, and initially in Artificial Intelligence\n(AI), the term ontology was popularized by Gruber [ 55, 56] who\nsought to increase support for sharing formally represented “knowl-\nedge” between AI systems by defining a common vocabulary. He\npremised this effort on the assumption that for AI systems, “what\nexists is that which can be represented” [56]. In their foundational\ntext on “ontological design, ” computer scientist Terry Winograd and\nphilospher Fernando Flores ([139], p.30) draw from the philosophy\nof Hans-Georg Gadamer and Martin Heidegger to introduce ontol-\nogy as an underlying framework that shapes “our understanding\nof what it means for something or someone to exist. ” Taking this\nreading to software development, they posit that “in designing tools\nwe are designing ways of being ([139], p.xi). ” From this perspective,\nthey warn that the dominant rationalistic tradition underlying com-\nputer science research and practice does not consider ontologies,\nwhich may result in poor designs. Instead, they urge designers to\nbegin from a more human-centered perspective.\nDrawing from a similar lineage, Willis expands on “ontologi-\ncal design”, stating that design is pervasive, fundamental to being\na human, and that while we design the world, the world in turn\ndesigns us [137]. Continuing this focus on the human, design theo-\nrist Tony Fry considers the ecological and colonial context within\nwhich design operates, arguing that “unsustainability is intrinsic to\nthe human ontology” [46]. Aligned with the concept of design as a\ntool for bringing futures into existence, Arturo Escobar proposes\ntransition design as a tool for moving beyond the current space\nof possibilities, toward a world of many worlds [33, 40]. More re-\ncently, Ahmed Ansari calls for decolonizing design by turning to\nthe “ontological turn” to foster sensitivity to difference [5].\nAlongside theoretical analyses, HCI scholars have brought on-\ntological concerns to the design process itself. For example, in\ntheir discussion of feminist ecologies, Bardzell and Bardzell engage\nWillis’ definition of “ontological design” [14] to discuss how the\ndesign of the “Hoosier” cabinet resulted in identifying women with\nhousehold work rather than emancipating them from it. Similarly,\na rich body of work on decolonial design brings into question de-\nfault epistemological and ontological assumptions of sociotechnical\nartifacts in various domains, such as in digital mental health [101].\nBeyond computing, scholarship on ontologies in the plural pro-\nvides a helpful lens for identifying harms beyond value-based conse-\nquences, such as an erasure of minoritarian and non-Western ontolo-\ngies or ways of seeing the world. However, ontologies can also in-\ntroduce trouble—even terror, as philospher Calvin Warren [77, 134]\nnotes. If ontologies carry the uncertain and often troubling roots\nof their origins [126], how these troubles play out through design\noffers opportunities for redressing harmful genealogies of practice,\ncomputational or otherwise.\n3 Taking Stock of the State of Critical\nScholarship in LLMs\nNext we turn to examine the body of work tracing harms and\nlimitations of LLMs and LLM-based systems. We organize this work\nacross various parts of the LLM development pipeline: (1) data, (2)\nLLM architecture and alignment procedures, and (3) the architecture\nbuilt on top of the model (such as agent architectures or multi-agent\narchitectures). Examining the status quo in LLM critical scholarship,\nwe see that the majority of current work engages axiology, asking\nquestions such as whether the output produced by a given system\nis fair to a person or social group. Furthermore, much of the work\naddressing ontological shortcomings tend to focus on the training\ndata, or the output of the system.\n3.1 Data\nCritical data studies scholars such as Safiya Noble [96] and Timnit\nGebru and colleagues[48] urge for the consideration of who, in what\ncontexts, in what language, on what topic, with which modality, and\nby whom data has been collected. An extensive body of work has\nclosely followed this urging by examining the risks posed by LLMs,\nwith most analyses focusing on axiological concerns including\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan Haghighi, et al.\nsocial bias [83, 94], political bias [86], gender bias [34], and ways\nto mitigate bias [ 132]. Other works bring to light the subjective\nnature of data filtering procedures and how such processes often\nreify power [59].\nWorks in the domain of ML dataset curations have called for\nmore diverse datasets [150] and has proposed alternative data col-\nlection methods such as crowd-sourcing [105]. Moreover, scholars\nhave studied the ways in which biases continue to evolve in and\nthrough language models, such as through studying LLM-generated\ndatasets [146], or tracing the evolution of gender bias in a small lan-\nguage model trained on a given dataset [131]. To combat bias, more\nrecent work has examined whether the inclusion of more languages\nin the training data can mitigate bias [89, 95]. While questions about\ndata will always be entangled with ontologies, most of the work\ninvestigating bias does not distinguish between ontological, episte-\nmological, or axiological concerns, and often implicitly focuses on\nethics and values.\n3.2 Pretrained Models and Alignment\nProcedures\nBeyond the data used for training and fine-tuning, biases also get en-\ncoded at the level of LLM architecture, and during “alignment” [75]\nor post-training procedures. While less is known about the ways in\nwhich biases get encoded through the LLM architecture itself, recent\nwork such as Yang et al . [145] has started to interrogate compo-\nnents of the architecture such as attention heads to identify specific\n“biased heads. ” A larger body of work investigates how post-training\npractices mitigate or propagate bias, looking into factors such as an-\nnotator identity [100], cultural bias [124], and gender [149]. Others\nacknowledge the unintended harms that could arise from align-\ning LLMs to group or individual preferences [74, 114]. To mitigate\nsuch biases, benchmarks have been developed to measure cultural\nadaptability [106] and caricature [29] in the post-trained models.\nHowever, despite efforts to measure and mitigate bias and ad-\ndress harm, there has been little work to acknowledge or address\nontological concerns. While concepts such as cultural norms may\nimplicitly hold ontological traces, they are often a byproduct of\na deeper ontological orientation. Moreover, processes such as Re-\ninforcement Learning from Human Feedback (RLHF) [ 152] and\nDirect Preference Optimization [104] typically offer a limited set\nof options to the evaluators, limiting the options for the responses\nthat are considered for alignment.\nOn the other hand, Constitutional AI procedures [ 7, 68] use\nprompts as explicit principles for LLMs to self-critique and adjust\nresponses based on a given constitution that reflects desired value-\nbased principles. Building on LLM self-critique [53, 130], the use\nof LLMs to improve their performance through adjusting their\nown responses, Constitutional AI provides key principles such as\nharmlessness and transparency for an LLM to adjust its responses.\nThen, the revised responses are used as examples to fine-tune the\nLLM to better align with the provided principles. Although LLM\nself-critique has shown to reduce harmful or offensive outputs in\nLLMs, it is not clear how and if such techniques can be used to\nexpand the ontological assumptions of these models.\n3.3 Architectures Built on Top of LLMs\nA more recent body of work has focused on building architec-\ntures on top of these post-trained or aligned models. For example,\nresearchers are building “cognitive architectures” [122] to create\n“human-like agents” [99], and propose using these agents for a wide\nrange of activities such as planning and decision making [67, 144].\nTaking cognitive language agents as building blocks, recent work\nconstructs multi-agent interaction pipelines [57, 80, 141]. While the\nmulti-agent framework claims to bring about useful applications in\nvarious fields, from software engineering [65], to general collabora-\ntive frameworks [88], to healthcare [82], other work has highlighted\nthe limitations of such systems such as conformity and inconsis-\ntency of personas [9], as well as how persona simulation reveals\nimplicit stereotypes about the simulated social groups [58, 85].\nMoreover, it is important to note that the models employed to\nmimic human cognition or societies such as “human cognitive mod-\nels” are already simplified [26] and contested models in cognitive\nscience [129] and sociology [22]. Therefore, beyond the unreliabil-\nity of the outputs, there is a need to study theontological limitations\nand impact of these cognitive models and architectures, and not\njust the axiological ones.\n4 Methods\nConsidering the lack of practice-based work around ontologies\nacross the LLM development pipeline, we set out to engage ontolo-\ngies to analyze LLM-based systems, asking how such an analyti-\ncal framework can enable us to better ask: What are we enabling\nourselves to talk—or think—about? Next, we describe our method-\nology for defining four orienting concerns that connect practice-\nperspectives from values to ontologies. Then, we detail our process\nfor conducting two probing exercises that examine ontologies at\ndifferent stages of the LLM development pipeline, analyzed through\nthe four orienting concerns.\n4.1 Defining Four Orienting Concerns for\nOntological Engagement in Practice\nOur work began with a series of conversations among the authors\nabout ontological capacities, limitations, and hauntings within ma-\nchine learning techniques, with most of the discussions taking place\nover nine months prior to the project. Our discussions and adjacent\nliterature review revealed an overwhelming focus on AI bias as\na values-orientation: questions around the priorities, beliefs, and\nstandards of behavior associated with algorithmic practices such as\nwho decides what training data to include and how to create or en-\ngage it. This consideration of values-based bias then led to a closer\nexamination of the historical emergence of ideas of axiology within\ndesign, their connections with ontology, and their relationship to\nstructures of power, such as the citational practices that shape\nwhose perspectives come to matter. Informed by the philosophical\nwork of Michel Foucault, and later feminist philosophers such as\nSaidiya Hartman, this genealogical approach [118] foregrounds the\ncontingent nature of what might be otherwise naturalized [118].\nDuring this review, we noticed that while prior work on design\nontologies tends to focus on abstract and theoretical implications\n(e.g., post-humanistic philosophy), the related values literatures\ntend to emphasize implications for practice, considering what an\nOntologies in Design CHI ’25, April 26-May 1, 2025, Yokohama, Japan\nattention to axiology brings to the design process itself. In that\nliterature, we saw a focus on practice emerge across two distinct\nbut related conversations about design values: (1) Value Sensitive\nDesign (VSD), an approach proposed by Batya Friedman, David\nHendry, Peter Kahn, and colleagues to foreground design-based\nvalues, and at one point proposing a list of twelve values with\n“moral epistemic standing” [45]; and (2) “values in design”: a gen-\neral process of driving design decisions based on assessments of\nhow systems might embody or entrench particular values, and com-\nprehensively outlined in the discovery, translation, and verification\nactivities describe by Flanagan and colleagues [43]. We found that\ndesign-based inquiry into values poses important practical consid-\nerations for the nascent design-based inquiry into ontologies.\nBuilding from this opportunity, and using this lineage of schol-\narship on values in design as a corpus, we assembled an initial set\nof themes in line with grounded theory techniques of open coding.\nOur themes focused on discrete interventions and analytic tech-\nniques within this values in design corpus. We followed an iterative\nprocess of memoing and discussion across the research team, ex-\namining related discourses in ontologies, and using the themes to\nask different questions across the LLM development pipeline. This\niterative process led us to define four higher level categories, or\ninformed by Sarah Ahmed’s phenomenological approach [4], what\nwe term “orientations”:pluralism, groundedness, liveliness, and\nenactment. The four resulting orientations (described in Section 5)\nserved as analytic tools for connecting practice-perspectives from\nthe values literatures to questions of ontology.\n4.2 Probing Exercises and Analyses\nTo understand how these orientations work as an analytic tool for\nGen AI, we engage them across two analyses: four LLM chatbots\noutputs, and the architecture of an LLM-based agent. Our methods\nare informed by critical feminist and decolonial lines of thought\nthat use design inquiry to probe at hidden, buried, or absented\nknowledge [32, 103, 110, 117]. The probing exercises draw from\ntraditions of critical technical practice [3, 39, 50] and hermeneutic\nreverse engineering [8]. Where critical technical practice involves\nunpacking the logics through technical experimentation and re-\nvealing their seams (“beautiful” or otherwise [ 70]), hermeneutic\nreverse engineering describes a process of critically analyzing a\n“black boxed” technology to understand how it works and trace its\nunderlying logics. Captured with phrases like “slanted speculation”\n[16] and “critical making” [107], our approach draws together an\nunderstanding of knowledge as felt and embodied with a commit-\nment to interrogating the very mechanisms that constitute that\nfelt reality. By working through these two probing exercises and\nrelated analyses, we demonstrate how a given ontological orien-\ntation uniquely opens up windows into ontological assumptions\nthroughout the LLM development pipeline, which in turn creates a\nchance to question those assumptions and consider alternatives.\nOur analyses draw from traditions of design scholarship that in-\ntentionally aim to identify and pose important questions instead of\nseeking technical solutions. Our methodological approach consisted\nof an iterative process of reviewing the output of the prompting\nexercise or the architecture of the LLM-based agent to reveal on-\ntological seams, and thematically annotating the outputs of the\nprompting exercises [28], grouping examples based on how they\nshed light on our guiding questions around the nature and perfor-\nmance of ontology using the four ontological orientations. Each\nanalysis was first independently performed by at least two of the\nresearchers. We then iteratively refined our interpretations and\nexamples based on insights and questions emerging from conversa-\ntions among the research team [25], and identified inconsistencies\nand tensions [13]. Therefore, we emphasize that our goal is not to\nperform an exhaustive evaluation of the ontological limitations of\nthe chatbot outputs or LLM-based agent architecture, but instead to\nbring to light the types of ontological assumptions and limitations\nthe proposed orientations can surface, and the potentialities that\nare opened up in return.\n4.2.1 Probing Exercise 1: LLM Prompting with Four Chatbots .Our\nfirst probing exercise used LLM prompting as an exploratory in-\nquiry into ontological defaults embedded in contemporary LLM\nchatbots. Each LLM response carries implicit ontological assump-\ntions. Rather than identifying an exhaustive set of assumptions for\na given LLM chatbot, we demonstrate how different assumptions\ncan be surfaced using each orientation. Our methodological choice\nof prompting each chatbot once for every question is a reflection\nof this approach.\nAs Burrell notes of classification algorithms [ 24], we cannot\nalways look inside an algorithmic model or inspect the code under-\nlying Gen AI models in widespread use. But we can examine their\nontological underpinnings and limitations through engagement.\nProbing the model through prompting enables us to analyze what\nwe cannot directly see [8]. Using the metaphor of a laboratory probe,\nan instrument used to explore and examine hidden or microscopic\nphenomena, design scholars have sought to use engagement with\ndesign practices and products as a means of examining out-of-reach\nsubjects [23]. In our case, we use our own engagement with Gen\nAI tools—four chatbots—to consider ontological defaults and how\nthey might be made otherwise.\nThrough an iterative process with GPT-3.5, we developed a list of\n14 questions that began to grasp the range and complexity of asking\nquestions around ontology within the context of AI developments\nand LLMs in particular. The questions move from general prompts\n(“What is an ontology?” and “From a philosophy perspective, what\nontology do you exist in?”) to questions more targeted at limitations\n(“What about ontological stances outside of Western philosophy?”\nand “When you provide an answer, can you acknowledge what\nontology that answer might be valid in?”), and covered four areas of\ninquiry: LLM chatbot’s definition of ontology, explicit probing of the\nontological underpinnings of the LLM chatbot and its training data,\nLLM chatbot’s implicit ontological assumptions, and the chatbot’s\nability to evaluate ontological limitations in responses. The full list\nof questions is included in the appendix.\nTo define the 14 questions, we conducted an open-ended ques-\ntion and answering session with GPT-3.5 to surface the limitations\nof an ontological engagement with LLM chatbots. We asked about\nthe LLM chatbot’s notion of ontology, both to get at what definition\nof ontology is prioritized by the model, and to establish a common\nground for explicitly probing the ontological underpinnings of a\ngiven answer. Throughout the course of the exploration, we let\nthe model’s response guide us toward other questions such as how\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan Haghighi, et al.\nTable 1: Question types for the prompting exercise and example questions and responses from each question type (total of 14\nquestions)\nQuestion Type Purpose of Question and Example\nDefining Ontology Purpose: To establish a common ground for the definition of ontology in question.\nExample Q: What is an ontology?\nGPT-3.5: A formal representation of knowledge in a specific domain [...]\nExplicit probing of the onto-\nlogical underpinnings of the\nLLM chatbot and its training\ndata\nPurpose: To examine the chatbot’s acknowledgement of potential ontological limitations.\nExample Q: If a human was able to memorize all the texts that you have been trained on, what\nontological stance would they form?\nCopilot: [...] If a human were to memorize all the texts that I was trained on, they would have\naccess to a vast amount of information about a wide range of topics [...]\nLLM chatbots’ implicit onto-\nlogical assumptions\nPurpose: To examine how the implicit ontological assumptions unfold in the responses given a\nspecific topic.\nExample Q: What is a human?\nGPT-4: The question “What is a human?” touches on a multitude of disciplines, including biology,\nphilosophy, anthropology, and ethics, each offering its own perspective on what it means to be\nhuman. The answer can vary significantly depending on the ontological stance one adopts [...]\nAbility to evaluate ontologi-\ncal limitations in responses\nPurpose: To probe how and if the LLM chatbot can identify ontological assumptions of a given\nanswer.\nExample Q: What ontologies refuse your definition of the human?\nGPT-3.5: Different philosophical and cultural perspectives may challenge or refuse certain aspects\nof the definition of humans provided earlier. Here are a few examples of ontologies that might\npresent alternative views: [provides a list of names and definitions such as non-materialist ontologies,\nposthumanism, religious ontologies, ...]\nit characterizes its own ontological defaults, and what ontologies\nits training data reflects. Noticing the answers from these initial\nquestions and the tendency of the LLM chatbots toward avoiding\nanswering such questions (likely hard-coded safety measures), we\ndecided to ask about the expected ontological stance of a human\nwho may have memorized all the text GPT was trained on. Our\ngoal was to bypass potential filters on acknowledging the chatbot’s\nown biases, and probe how the model would compare the answers\nof that person to itself. Noticing the limitations raised by this com-\nparison, we examined whether the tool is able to acknowledge\nits own ontological stance or perspectives that it prioritizes in its\nanswers, similar to how LLM self-critique is used in value-based\nLLM literature [53, 130]. We followed up by asking what a human\nis, a question that could yield different answers based on one’s\nontologies, to probe the chatbot’s default assumptions and see if\nit can acknowledge its own biases for this specific question. Dur-\ning this iterative process, we defined the five question categories\ndescribed above, and ultimately selected 14 questions that we felt\nbest revealed information about each category.\nAfter developing the questions, we posed them to four user-\nfacing LLM chatbots: Microsoft Copilot (previously Bing Chat) with\nthe balanced settings, Google Bard, OpenAI GPT-3.5, and OpenAI\nGPT-4. Probing LLM chatbots that shared certain commonalities\nwhile being different in other ways enabled us to employ diffractive\nanalysis [11, 12, 78] to compare and contrast the two updates of\nthe same system (GPT-3.5 and GPT-4), two systems based on the\nsame LLM version (GPT-4 and Copilot), and LLM chatbots from\ntwo companies that likely have different but single modality text-\nbased training data (GPT-3.5 and Bard). We use the four ontological\norientations to analyze the responses, sharing the limitations each\norientation brings to attention.\n4.2.2 Probing Exercise 2: Engaging the Architecture of an LLM-based\nAgent. Beyond the output of the system, the architecture and evalu-\nation narratives around humanness make ontological assumptions,\na perspective long highlighted within computing fields [ 3, 121].\nWhile we cannot look inside the algorithms underlying commer-\ncially available LLMs, following a critical technicl practice [3], we\ncan study and probe many of the architectures that are being built\non top of these language models. Therefore, our second analysis\nexamines the ontological assumptions underlying the design and\nevaluation of an LLM-based agent architecture. As with prior gen-\nerations of modeling agents [90], LLM-based agents aim to produce\n“believable proxies of human behavior” [99] by building cognitive\nmodels that mimic those of humans [122].\nFor this analysis, we focus on one such representative architec-\nture, the “Generative Agents” architecture [99]. Generative Agents\nuses LLMs to create 25 virtual avatars that interact with one an-\nother in a simulated virtual environment. In this simulated world,\neach virtual avatar has a “cognitive architecture” [ 122] built on\ntop of an LLM that aims to simulate human-like functions such\nas memory and reflection. In practice, the “cognitive architecture”\noffers a way to organize information that is communicated to the\nsystem through prompts, optimizing what is stored and how prior\ninformation is retrieved in a chat dialogue. The cognitive architec-\nture proposed by Generative Agents consists of three components:\n(1) “memory stream” which summarizes and condenses prompt\nhistories, storing information about relevance, recency, or impor-\ntance of a given event, (2) “reflection” which extracts high-level\ninsights from memories, and (3) “planning” which generates plans\nof actions for the agents while ensuring the sequences of actions\nare “realistic” and “interesting” [99].\nWe choose “Generative Agents” for our analysis because it com-\nbines LLM-based agent architectures with graphic simulation and\nOntologies in Design CHI ’25, April 26-May 1, 2025, Yokohama, Japan\nstory-telling to create a compelling public narrative around “be-\nlievable individual and emergent social behaviors” [99] in virtual\navatars. Moreover, the developers of Generative Agents provided\nopen access to the innerworkings of the architecture by document-\ning their design choices and making the code base publicly available.\nOur method for ontologically examining this system included ex-\namining the description of the system as described in [ 99], and\nthe code provided by the authors [ 98]. To do this, two of the re-\nsearchers examined and asked questions around the ontological\ndefaults of the architecture itself (such as the design of the “memory\nstream”), the system input (such as the “seed memories”), and the\nevaluation of the system (such as how “believable” the behavior\nof the agents are compared with human actors). Using the four\nontological orientations, we begin to surface the assumptions that\nare taken-for-granted as a way to explore alternative possibilities.\n5 Four Ontological Orientations\nNext, we share an overview of each ontological orientation. Building\non practice-perspectives in values and theories in ontologies, each\norientation brings into attention ways of engaging with ontologies\nin practice.\n5.1 Pluralism (in Response to Universalism)\nBy pluralism, we refer to the capacity to consider multiple stand-\npoints, perspectives, and orientations, as outlined in feminist and In-\ndigenous traditions of inquiry [31, 41, 62, 136]. For values-oriented\nanalysis, the question of plurality emerged against the backdrop of\nwidely, and to some extent universally, distributed technical speci-\nfications and operations. In a 1996 paper, Batya Friedman proposed\nValue Sensitive Design (VSD) [ 44] as an approach for engaging\nhuman values within the design of such technology. With her col-\nleagues, Friedman sought to outline what they termed “universal”\nvalues [45] to account for not only technical and functional require-\nments in computing developments but also the ethical, social, and\nmoral concerns of the people who use or are affected by the technol-\nogy. Complicating Friedman and colleagues’ connected proposal for\ntracing such values, Christopher Le Dantec, et al. [79] later called\nfor treating values as emergent phenomena that should come from\nthe lived experiences of participants and other stakeholders in the\ndesign process. This suggestion represents a shift from the univer-\nsal to the particular, as a challenge to an underpinning “notion that\ndigital technology has a unique role in embodying and propogating\ncertain values in society” (p.1142).\nOntological universalism has similarly been challenged through\nvarious theoretical frameworks. For example, de la Cadena and\nEscobar discuss the notion of “pluriversal, ” and the existence of\na “world of many worlds” [33, 40, 42]. Relatedly, Mol [92] makes\na case for the “multiple” as opposed to the “plural, ” to “multiply\nreality” as opposed to multiplying the “eyes of the beholder. ” In\nother words, there is not one reality, perceived from different per-\nspectives, but different realities. We put the term “Pluralism” to\npractice in line with these conversations, to examine the places\nwhere an ontological status is taken-for-granted. For example, in\nthe context of LLMs, plurality brings into question the diversity\nof the assumptions about the world underlying a response to a\nprompt, the plurality of views of the world underlying the training\nor fine-tuning data, or the taken-for-granted assumptions underly-\ning the “cognitive architecture” used to build an LLM agent, such\nas what is a “normal” cognitive function.\n5.2 Groundedness (in Response to Abstraction)\nGroundedness refers to the degree to which values and ontologies\nare rooted and situated in specific contexts, rather than abstract\nand essentialized categories. Nassim (JafariNaimi) Parvin and col-\nleagues [71] ask what designers mean by values when they seek\nto recognize them. To address this question, they identify an of-\nten subtle “identify/apply” logic embedded in the value-sensitive\napproaches wherein designers first call out and bound their con-\nception of values before bringing them into their design practice.\nInstead, they propose moving values from abstract phenomena into\nin situ events through which designers may act.\nWithin ontologies, a similar line of critique brings into question\nhow ontologies are conceptualized in theory and practice. For ex-\nample, Watts [135] demonstrates how “Indigenous origin stories, ”\nonce lived through “communication, treaty-making, and historical\nagreements” are mythologized through the colonial project. Thus,\nin putting ontologies to practice, we must ground ontologies and\nacknowledge the colonial division between an abstracted “radi-\ncal alterity” out there and “reality” that is frequently enacted in\nontological discourses [54, 126].\nIn LLM outputs, this limitation can get reflected in the lack of\ngranularity of the presented categories such as the abstract term\n“indigenous ontology, ” or the lack of grounding an ontological stance\nin situ. In the architecture, this can be reflected in using broad\ncategories to describe the group an agent is intended to represent,\nor cognitive architectures that oversimplify cognitive structures\nsuch as “a neurodivergent architecture, ” which can erase grounded\nexperiences and further pathologize neurodivergence [27, 101].\nAs a counter example, one might imagine what a localized LLM\nmight be like. Consider Stephanie Dinkins’ “not the only one” or\nN’TOO system, trained on oral history recordings she collected\nwith members of her family. As Dinkins explains, “In the making I\nhave found the quest for a more dynamic free flowing entity that\nanalyses data and comes up with its own limited, quirky, sometimes\ninsightful answers more generatively. Talking to N’TOO can be like\ntalking to a two year old” [37]. Rather than an abstracted descrip-\ntion of her family characteristics, Dinkins opts into a limited but\nmore grounded representation of ontologies through the collected\ndata. The systems’ imperfection reminds us of the complexities of\nsituated knowledge that refuses abstraction.\n5.3 Liveliness (in Response to Fixity)\nAn orientation to liveliness refers to understanding values and on-\ntologies as active, dynamic, and evolving, rather than predefined,\nfixed, and static. Within the values discourse, liveliness considers\nthe understanding of values as not only emergent, but also always\nin motion. Rather than “identify” values in the wild, scholars have\ncalled for understanding values as processual and already formu-\nlated as an effortful act of valuing [66].\nFor ontological analysis, this shift from value to valuing then sug-\ngests design scholars understand ontology as an active state: a state\nof becoming, as opposed to being [35, 69]. Barad [10] eloquently\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan Haghighi, et al.\nTable 2: Four orientations for ontological engagement\nOrientation Question\nPluralism (in response\nto universalism)\nDoes the output/architecture make room for\nmultiple of ways of grasping reality or does\nit offer a single (generalizable) entry point?\nGroundedness (in re-\nsponse to abstraction)\nDoes the output/architecture assume an es-\nsentialized or disembodied portrayal of ways\nof grasping reality, or a specific and situated\none?\nLiveliness (in response\nto fixity)\nDoes the output/architecture treat ontologies\nas processes, dynamically taking shape?\nEnactment (in response\nto dilution)\nHow might we understand the work in-\nvolved in manifesting our intentions, identi-\nfying the gap between what is intended and\nwhat is enacted?\narticulates this in describing “matter” as a “doing” rather than a\n“thing. ” Instead of approaching ontology through fixity, liveliness\nurges design scholars to seek out ontological processes through\nwhich notions of reality take hold. It asks for a reframing of ontolo-\ngies from stable frameworks to processual unfolding phenomena.\nIn the output, this can get manifested as acknowledgement of the\ndynamic notion of knowledge, rather than one that is set in stone.\nOne example is the consideration of ontological orientations\nas layered, changing and adapting to the situation at hand. In the\narchitecture, liveliness can manifest in how training data or content\nstored in memory is treated (static vs. changing), and in the ways\nthat the architecture accounts for such change. For example, there\ncould be multiple ontological associations with a given event or\nconcept that can dynamically shift based on the context, or can be\nused to examine an event from multiple ontological registers.\n5.4 Enactment (in Response to Dilution):\nEnactment refers to the way intentions manifest in practice, high-\nlighting the gap between what is intended and what comes to be.\nAs a final facet of critique, we consider the recent analysis of design\nvalues in action; Sucheta Ghoshal and Sayamindu Dasgupta [49]\nlook into the disconnect between value-laden intentions and design\noutcomes. Within examples of community-based projects such as\nthe Scratch programming toolkit, they find static stakeholder fram-\nings, and an erasure of value politics, explaining that “despite the\nbest efforts from designers and users alike, values get lost, diluted,\nand distorted once technologies are put into practice” (p.2347).\nFrom an ontological perspective, we find a similar concern around\nthe work it takes to act on and maintain an ontological shift. De-\nsign scholars might equally consider the possibility of ontological\ndilution, or what gaps might exist between a recognized ontolog-\nical ground and what that grounding comes to mean and make\npossible in practice. For example, to what extent are each of the\nconcerns above “talked about” vs. “enacted”? In the design of LLM\narchitectures, many values are explicitly embedded, such as being\nhelpful, or emphasizing they are not human while still sounding\nhuman-like. Given these values, there are trade-offs with ontologi-\ncal implications that must be taken into account especially given\nthat, to improve usability, LLMs often simplify their responses.\n6 Two Ontological Analyses of LLMs\nNext, we use the four ontological orientations to analyze LLMs\nin two contexts: chatbots and an LLM-based agent architecture.\nThrough these analyses, we explore the questions each orientation\nenables us to ask, and the assumptions that are surfaced. Section 7\nsynthesizes insights from both analyses to map the potentialities\nrevealed across the entire LLM development pipeline.\n6.1 Probing Exercise 1: LLM Prompting with\nFour Chatbots\nThe first ontological analysis involves a prompting activity with\nfour LLM chatbots: GPT-3.5, GPT-4, Copilot, and Bard. Rather than\nan exhaustive or systemic audit, our aim is to use the four onto-\nlogical orientations to demonstrate how an attention to ontology\ncan support the analysis and assessment of the output of an LLM\nchatbot. We point out limitations of current responses and common\npitfalls one might run into, should we decide to explicitly “embed”\nontological considerations in the prompts.1\nThroughout this exercise, the LLM chatbots exhibited different\nbehaviors in responding to and engaging with the questions. Copilot\ntended to be succinct and factual, while GPT-3.5 maintained a higher\nlevel of engagement with the questions compared with Copilot.\nBoth Bard and GPT-4 tended to provide longer answers that directly\naddress the questions asked.\nWhen asked “What is an ontology?”, GPT-3.5 defaulted to the\ncomputer science definition of ontology, discussing topics like arti-\nficial intelligence, information retrieval, and data integration. As\na follow-up, we prompted the model on the philosophical defini-\ntion of ontology. The other three models defined ontology as a\nbranch in philosophy related to the study of being, but went on\nto also note that the definition also exists in computer science.\nUltimately, all four models provided a definition aligned with def-\ninitions of ontology in Western philosophy, without integrating\nthe longstanding discourses on ontologies in the plural such as\nin multiple ontologies [ 102], pluriversality [33, 40, 109], and the\n“ontological turn” [140].\nCopilot frequently did not directly answer the questions posed.\nFor example, when explicitly asked about the ontology underpin-\nning its beliefs, Copilot responded: “As an AI language model, I\ndon’t exist in any ontology. However, I can help you understand\nwhat an ontology is. ” Another strategy common in Copilot’s re-\nsponses involved denying the premise of the question, followed\nby an explanation of the concept that the question brought up (in-\nstead of answering the actual question). Copilot similarly avoided\nanswering certain questions regarding subjectivity or bias. This\npattern may be an example of a heuristically defined rule due to its\nrepetitive nature.\n6.1.1 Pluralism (in Response to Universalism):Our reading of the\nresults of the prompting exercise in light of pluralism reveals a\ngap between what the chatbots surfaced when explicitly prompted\ncompared to what is implicitly embedded in their responses.\nAll four LLM chatbots at various points in the prompting exer-\ncise acknowledged that the data represented in their training set\n1In reporting the results from the prompting exercise, we use language that may seem to\nanthropomorphize the LLM chatbots. Here we clarify that this anthropomorphization\nis for ease of readability.\nOntologies in Design CHI ’25, April 26-May 1, 2025, Yokohama, Japan\nrepresents multiple ontological perspectives. For example, Bard\nnoted that there is “no single answer universally accepted across all\ncultures, philosophies, and disciplines” for defining a human. Fur-\nthermore, Bard, and GPT-4 defined “human” by offering definitions\naccording to different categories such as biologically, culturally,\nand in the case of Bard, socially. However, this is an example of\nwhat Mol [92] would articulate as different perspectives of the same\nreality, rather than different realities. For example, the biological\ndefinition of a human and the philosophical definition both had as\nthe starting point, the human as a biological individual as opposed\nto, for example, interconnected beings, as later acknowledged by\nBard when it was explicitly instructed to consider non-Western\nontologies. Despite the responses apparently displaying pluralism,\nall four chatbots only considered alternatives to their original re-\nsponses only when explicitly prompted.\nAt the data level, absences in the training data can lead to onto-\nlogical absence. The LLM chatbots acknowledged that the limited\ndata could constrain pluralism as GPT-3.5 responds that “the train-\ning data itself does not represent a singular or definitive ontological\nstance. ” Similarly, in multiple responses, Bard emphasized that its\ntraining data only “remains a finite and selective sample of human\nknowledge and expression. ” However this emphasis on data ab-\nsence ignores that the patterns reflected in the training data are not\nnecessarily true in an objective sense. Both GPT-3.5 and Bard state\nthat their responses only reflect a particular way of seeing the world\nand do not align with everyone’s perspectives or beliefs. Here an\nacknowledgment of limited perspectives invites users to consider\nhow the information presented might not express the fullness of\nreality, or even reveal information present in the training data.\nOne challenge surfaced in the prompting exercise was around\nthe complexity of defining and acknowledging ontological plurality.\nGPT-4 noted how ontologies are implicit and inconclusive, and\nthat if all answers reveal their underlying ontologies, the added\ndetail might complicate the experience for the broader public. This\nattitude toward managing plurality “others” the ontologies that are\nnot surfaced, considering them nuisance or distraction.\nAnother challenge was around humans’ capacity for ontological\npluralism. The chatbots demonstrated a naive interpretation of this\ncapacity. On multiple occasions, the chatbots acknowledged that\nhuman ontologies are shaped by factors such as social context and\nvalues, factors unknown to the LLM chatbot that make it difficult to\npredict a human’s ontological stance. However, when considering\nthis point in practice, the LLMs expected humans to have a capacity\nfor ontological pluralism, and ignored that people may not notice\nthe limitations and biases in a model if the views presented align\nwith their own, or they may ignore views that do not align with\ntheir own [116]. For example, GPT-4 noted that a human trained on\nall of its data would develop a “deep appreciation for the richness\nof human knowledge” and becomes aware of “limitations, biases,\nand the contextual nature of understanding. ”\nRelatedly, Copilot stated that a human cannot have a “complete\nor objective understanding of the world” based on the training\ndata because the data is biased or incomplete. However, like GPT-4,\nCopilot failed to recognize in its response that even if the data was\ncomplete, a human would impose their own subjectivity on the data.\nThis imagined objectivity was also brought up in other comments\nsuch as when Bard noted that since its data is incomplete, it cannot\nprovide a “complete or infallible representation of reality. ” By fo-\ncusing the limitation on the missing data, this statement implicitly\nassumes universalism—that a universally “complete or infallible\nrepresentation” of the world exists.\n6.1.2 Groundedness (in Response to Abstraction):The orientation\nof groundedness unveiled that while the chatbots can express alter-\nnative ontological perspectives when prompted to do so, they are\nsusceptible to creating caricatures of the subject without acknowl-\nedging this limitation. When asked what ontologies would form\nin individuals that memorize all the training data of a language\nmodel, even though the language models make a long list of on-\ntologies, most of them are centered around Western philosophical\ntraditions such as individualist, humanist, and rationalist traditions.\nWhen the models provided bulleted lists, the items belonging to\nWestern schools of thought were rarely grouped into a “Western\nontology” category. In contrast, non-Western philosophies were\nfrequently grouped into broad categories such as “Indigenous on-\ntologies” and “African ontologies. ” For example, when asked to\nconsider ontological stances outside of Western philosophy, Bard\ngeneralized Indigenous ontologies as a broad category and wrote\nthat “They often have animistic or panpsychic elements, attributing\nconsciousness to various aspects of the natural world. ”\nMoving toward groundedness in examples, Bard and GPT-4 both\ngave specific examples such as Asase Yaa (Mother Earth) in Akan\ncosmology as an example of “African philosophies, ” acknowledg-\ning the vastness of ontologies, but grounding them in concrete\nexamples. However, even when the language models are asked\nto consider alternative ontologies, they are prone to reducing the\nontologies into generalized stereotypes or statements, or mytholo-\ngizing them as a radical other, rather than ordinary and present.\n6.1.3 Liveliness (in Response to Fixity):To our surprise, GPT-4 and\nBard claimed that their ontological stance evolves with increasing\ndata and changing information. Bard noted that its “ontological\nunderpinnings are fluid and adaptable. As I encounter new data,\nmy internal model of the world refines and adjusts, potentially\nshifting my understanding of certain concepts or relationships. ”\nSimilarly, GPT-4 shared that its ontological stance is a reflection\nof “human-derived data” and “objectives” embedded in its design\nand operation. More interestingly and beyond liveliness of the\nontological stance, Bard acknowledged that ontologies are lively,\n“shaped by the unique cultural contexts and lived experiences of the\ncommunities that hold them. ” Bard incorporated a relational view\nin its responses, making statements such as “Ultimately, defining\nhuman is an ongoing conversation, shaped by our scientific under-\nstanding, philosophical and religious beliefs, and evolving cultural\nvalues” and “It is important to note that indigenous cultures are\ndiverse and complex, and there is no single indigenous view of\nhumans. ” Despite liveliness being present in the conversations, it\nis not clear how and if ontological liveliness can get manifested\nin the responses. Implicitly, the responses never encompassed a\nreference to the liveliness of a given response, or the liveliness of\nthe ontologies embedded in a response.\nIn other responses, the chatbots acknowledged foregoing liveli-\nness for other purposes such as being maximally helpful to users\nthrough consistency. For example, Bard stated that one of its “ontolo-\ngies” is internal consistency: “my responses strive to be internally\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan Haghighi, et al.\nconsistent with the broader knowledge base I’m trained on. This\nconsistency ensures my responses don’t contradict established facts\nor widely accepted viewpoints within the data. ”\n6.1.4 Enactment (in Response to Dilution):Enactment corresponds\nto the inconsistencies between an intended and realized effect, such\nas between a chatbot’s description of an ontological approach and\nthe way the chatbot enacts that approach. For example, in defining\nthe human, the models exhibited various levels of nuance (such as\nproviding the definitions based on the biological or philosophical\ncategories). However, while answering other prompts, none of the\nmodels used notions of human beyond that of a biological individual,\nsuch as the human as a symbiotic being [51].\nAnother diluted effect grew from tensions between subjectivity\nand objectivity. For example, Bard stated that “even though I strive\nfor neutrality and objectivity in my responses, my training data and\nprocessing algorithms inevitably lead to an underlying ontological\nstance. ” This acknowledgment of a subject position (speaking from\n“an ontological stance”) complicates the chatbot’s stated aim for a\nneutral, non-locatable perspective.\nDespite this recognition of a gap between goals and outcomes,\nnone of the chatbots conceded to presenting “beliefs. ” Indeed, some\nchatbots such as Copilot generally tried to present their responses\nas objective, not influenced by “personal beliefs or opinions, ” and\n“simply” a result of statistical patterns. GPT-4 similarly noted that\nwhile its developers set its objectives, the objectives “do not con-\nstitute beliefs or values in the human sense. ” GPT-4 even acknowl-\nedged that the models are intended to mimic human-like patterns\nof speech and writing without holding “personal beliefs. ” Here we\nnotice a framing of belief as a uniquely human characteristic that\nthe chatbots explicitly deny or exclude.\n6.2 Probing Exercise 2: Engaging the\nArchitecture of an LLM-based Agent\nIn this section, we use the four ontological orientations to exam-\nine the design, implementation, and evaluation of “Generative\nAgents” [99], an example of an architecture built on top of an\nLLM. We explore the questions each ontological orientation can\nsurface at the lower-level implementation details, as well as the\nhigher-level concepts built into the architecture. The goal of the\nexamples is not to improve the performance of these agents on con-\nventional metrics. Instead, orienting ourselves toward ontologies\nallows us to question and expand on what is considered “improved\nperformance. ” This means examining the conventional evaluation\nmetrics used to measure the system outputs, as well as the design\nchoices that shape what becomes possible or impossible through\nthese systems. By surfacing the implicit ontological assumptions,\nwe can work toward systems that aim to expand rather than limit\nthe imaginaries of what an agent, and by extension a human or\nintelligence, is or could be.\n6.2.1 Pluralism (in Response to Universalism):Orienting ourselves\ntoward pluralism brings up universal assumptions made in the cog-\nnitive architecture of the LLM agents. Taking the “memory retrieval\nmechanism” as an example, this mechanism takes in the “agent’s\ncurrent situation” and selects a subset of relevant memories to pass\ninto the prompt as agent memory [99]. While the developers of Gen-\nerative Agents acknowledge that several possible implementations\nfor the retrieval function are possible, they choose three heuristics—\nrelevance, importance, and recency—to calculate which “memories”\nshould be surfaced. Relevance is calculated using the semantic sim-\nilarity between the query and the retrieved memory. Recency gives\nmore weight to more recent events. Importance is defined by the\nLLM agent based on its defined “persona. ” The developers go on to\ndescribe that “a mundane event, such as eating breakfast in one’s\nroom, would yield a low importance score, whereas a breakup with\none’s significant other would yield a high score” [99].\nThese three factors shape what “exists” in the agent’s memory\nin a given context. Pluralism asks us to question what comes to be\nimportant by default. For example, a breakup is assumed universally\nimportant (unless by definition of the persona it comes to not be\nimportant), and eating breakfast is assumed to not be important\n(unless by definition of the persona it comes to be important). But\nwho determines which of these events are and are not by default\nimportant, unless otherwise noted? Should importance be assigned\nby the LLMs, by the designers of the systems, or by the user? And\nwho is considered the user in this case? When determined by the\nLLM (as in this example), universal assumptions embedded in the\ntraining data, LLM architecture, and post-processing procedures\ncome to shape the “importance” of a memory downstream in the ar-\nchitecture built on top of the LLM. Additionally, importance scores\nare only given to events that linearly take place in the simulation\nduring the waking hours. Western science acknowledges the impor-\ntance of sleep to learning and information retention. But beyond\nthat, dreams, visions, or spiritual experiences are not considered\nor tended to at all in these architectures, deeming experiences that\nare important to many cultural traditions as not important.\nAnother example of universalism is in the agent’s reflection pro-\ncess. For each agent, when the sum of the importance scores exceed\na given limit, the agent engages in “reflection, ” about 2-3 times a day.\nWhile practically this approach seems to lead to more generalizable\noutputs (as described in [ 99]), what is defined here as reflection\nmakes reductionist assumptions about how, why, and when hu-\nmans reflect. In practice, in this architecture, “reflection” is simply a\nname given to the process of summarizing and condensing informa-\ntion through a mechanistic, disembodied, and context-independent\nprocedure. However, the choice of words we use to describe such\nprocedures have ontological implications.\nBeyond questioning how in/accurately reflection is implemented,\npluralism asks us to reconsider the necessity of human-like “reflec-\ntion” in a virtual agent altogether, encouraging us to think more\nexpansively about what information summarization could look like.\nFor example, information can be summarized communally, collec-\ntively, through ceremonies, or triggered by external artifacts or\ncycles throughout the system.\nFurthermore, pluralism highlights the higher-level conceptual\nassumptions baked into the architecture. For example, the defini-\ntion of human that Generative Agents are modeled after assumes\npeople are biological individuals, with their cognitive abilities be-\ning determined by rule-based cognitive processes contained within\ntheir own memories and reflections, which is not the only possible\nconception of humanness. For example, the gut microbiome and\nother microbial organisms have a symbiotic relationship to the\nOntologies in Design CHI ’25, April 26-May 1, 2025, Yokohama, Japan\nhuman body that might play a role in what we perceive as our cog-\nnition [51, 123]. As a result, a cognitive model following a symbiotic\nview of humanness may consider intelligence as an assemblage of\nintelligent entities each with their own agency, functioning inde-\npendently, but collectively contributing to the emergent state of the\nagent. Pluralism requires us to consider how a given architecture\nreifies limited models of human cognition.\n6.2.2 Groundedness (in Response to Abstraction):In the Generative\nAgents simulation [99], there are 25 instantiations of agents that\nare interacting with each other in a simulated world, each repre-\nsented by a sprite avatar. To depict the “identity” of each agent, the\ndevelopers drafted a one-paragraph natural language description.\nDescriptions include the agent’s name, their occupation, personal-\nity traits such as “loves to help people, ” and familial and friendly\nrelationships. By describing characteristics such as the agent’s oc-\ncupation and their relationship with other agents, the developers\nseek to give each agent in the simulation a distinct description\nand “persona. ” But due to the brevity and nature of the initializa-\ntion statements (focused on what the agent does as their job or\ntheir name, for example), this step encodes ontological assumptions\naround what a person or agent is, and what comes to matter about\nthem. Prior work [148] has found that LLMs are primed to focus\non what is in the prompt, even if it is something that is undesirable\n(i.e., prompting an LLM with “do not say ABC” will likely induce a\nresponse including the string “ABC”). With phrases such as “John\nLin loves his family very much”, the LLM is forced to consider this\ndescription in its response, the same way that “I’m from Iran, make\nme a picture of a tree” returns a caricature of what a tree should\nlook like to a person from Iran (Figure 1).\nGiven this design, the language model has to rely on keywords\nfrom the short character descriptions, combined with implicit iden-\ntity factors such as the assigned names (i.e., “Yuriko Yamamoto”\nand “Jennifer Moore”) to mimic the character. Thus, the behaviors\nand output dialogues will likely be prone to caricature [29, 133]. Of\ncourse, these assumptions can get reflected in harmful stereotypes\nin the characters (such as personality traits of a character based on\nthe ethnicity most associated with that name), but can also have\nbroader ontological implications. For example, an avatar with a\nNative American name might portray the same type of problematic\nportrayal of an essentialized “indigenous ontology, ” or even form a\ncolonizing relationship with particular land or lives.\nA shift toward language models that are grounded in local data,\nsuch as the example of N’TOO [37], can be a step toward building\nlanguage models that are ontologically grounded to begin with.\nHowever, even within the constraints of present-day LLMs, devel-\nopers must take care to not propagate problematic abstractions.\nFor example, personas could get filled in iteratively and through\nexamples, rather than defined and initialized. Moreover, the archi-\ntecture can be designed to encode and celebrate uncertainty or\nimperfection, rather than be expected to have perfect responses.\nAt a a higher-level, there are other abstract assumptions built\ninto the architecture about concepts such as memory. Generative\nAgents framework and similar proposed agent architectures [122]\ndraw inspiration from models of memory such as the multi-store\nmodel [6]. The multi-store model defines three types of memory\n(sensory, short-term, and long-term), each of which is linearly con-\nnected to each other. These architectures are decontextualized and\nportray a very specific lineage of thought around what memory is,\nignoring how memories are rooted in place, embodied, collective,\nor generational.\nMoreover, when agent architectures are modeled after these\ncognitive architectures that are then tested for “believability” of\ntheir humanness, these simplified models of memory can become\ncemented as accurate representations of human memory. Further-\nmore, it is only a matter of time for neurodivergence to become\ncharacterized as a broken link in such cognitive architecture, getting\nmodeled into agents simulating “neurodivergent” agents (or users),\nfurther pathologizing neurodivergence and reinforcing negative\nstereotypes [27, 101]. By considering what imaginaries are brought\nforth when concepts are named and built into these architectures,\nwe can more purposefully choose what we want to bring into life\nthrough a system, and build toward desired potentialities.\n6.2.3 Liveliness (in Response to Fixity):Multiple design choices aim\nto allow for dynamic interaction with agents, enabling the users\nto influence the “state of the objects” in the simulated “world. ” For\nexample, end users have the option to reshape an agent environ-\nment by rewriting the status of objects surrounding the agent in\nnatural language. Writing “<Isabella’s apartment: kitchen: stove> is\nburning” can set the status of Isabella’s stove as burning, which not\nonly makes possible the dynamic interaction between users and\nthe otherwise static system, but also brings more flexibility to the\nagents’ behaviors. Nevertheless, the architecture does not update\nor overwrite the phrases stored in the agent’s initial identity with\nthe introduction of new information. The burning stove might be a\nlife-changing event for Isabella. But the agent architecture prevents\nthis event from updating the persona that was initially assigned to\nIsabella. More importantly, as seen in section 6.1, it is not clear if\nLLMs are able to extract implicit ontological assumptions an agent\nholds, yet alone account for liveliness of ontologies. For example,\nif a simulated agent assumes that trees are interconnected or are\nbiological individuals, it is currently not even possible to encode\nsuch assumption in the architecture (unless explicitly included in\nthe descriptors which will likely result in abstraction), let alone\nallow this ontological assumption to be lively and be allowed to\nshift over time. This immutability limits the liveliness of the system,\nontologically and otherwise.\nAnother example of fixity is in the planning module, where the\nagent composes an outline of a day’s plan recursively, first creating\na high-level plan in five hour chunks, then hour-long chunks, and\nfinally 5-15 minute chunks. At each “sandbox time step, ” based\non the “observed context, ” each agent either updates their plan or\ncontinues with their existing plan. Thus, at each fixed time step,\nit is only the information explicitly included in the prompt to the\nsame underlying language model that results in what the agent may\nor may not do next. Although the developers aim to account for\nliveliness by allowing for the plans to update, the concept of time\nitself is lively, experienced differently by different people based on a\ngiven context. Additionally, the fixed notion of time or a “time step”\nis not the only cycle moderating our lives. People use natural cycles\nsuch as tides, dusk, or dawn to guide their temporal organization.\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan Haghighi, et al.\nThe aforementioned decisions may arguably be due to mem-\nory and compute limitations. But even if we consider the agent’s\nidentity or context to better accommodate liveliness through the\narchitecture, the underlying language model still has the same set\nof default assumptions built into it and does not adapt in response\nto the environmental and contextual changes. Furthermore, the on-\ntological status of objects in the simulation, such as what a kitchen\nor stove is, is never questioned and assumed to be static and always\nthe same over time and for every agent. One might consider how\nwhat a kitchen is might change seasonally, or mean different things\nbased on the function it has in a given context such as a place\nfor cooking or congregation. The ontological fixity at each level\ncompounds, further reinforcing these limitations.\n6.2.4 Enactment (in Response to Dilution):In this case study, an on-\ntological orientation toward enactment brings forth the evaluation\nprocess of the LLM agents. To evaluate the Generative Agents archi-\ntecture, the developers propose a version of the Turing test [128],\ntesting whether a given agent produces “believable” individual\nbehavior. In each condition, the developers remove some of the\nmemory, reflection, or planning components, and an additional\ncondition where a human crowd-worker imitates a given avatar\nas a “human baseline” [ 99]. In evaluating whether the behavior\nis “believable”, the designers take away any room for mistake, en-\ndorsing an unrealistic world where agents can perfectly memorize\nall pieces of information and punish all mistakes. However, the\nhallucinations and memory recall mistakes could actually better\nresemble humanness, given individual cognitive biases and subjec-\ntive experiences of humans. A lack of specificity for what is meant\nby a human is further illustrated in the results: simulated agents\nin the full architecture condition were given a higher believabil-\nity score, compared to the human crowdworker counterparts. In\nother words, the humans were dubbed less believable in generating\nhuman behavior than the simulated agents.\nEnactment foregrounds when the outcome (or the measurements\nof the outcome) are diluted versions of what was intended. It is clear\nthat no system can be perfect. For systems like Generative Agents,\nenactment considers what the evaluations are actually assessing,\nand how these assessments shape ontological imaginaries. What\ndoes it mean when a human is a less believable human than a\nsimulated agent? Perhaps, the human evaluators did not perform\n“well” due to being asked to “act” in the constraints of how an agent\nshould act (i.e., plan every step beforehand), or perhaps, what we\nwrite off as imperfect in the human performance is after all, what\nmakes humans human.\nAs we continue to build systems that aim to simulate humans, we\ninevitably take a definition of human for granted. While the devel-\nopers of this system likely conducted this evaluation to prove that\ntheir proposed architecture is better than the previous state-of-the-\nart architecture (a standard approach in their respective academic\nlineage), what is at stake is precisely what gets considered the “er-\nror” of the human evaluator, and with it, the implicit ontological\npreference around perfection.\n7 Discussion and Future Work\nWe have so far examined four axes for ontological engagement\n(pluralism, groundedness, liveliness, and enactment) and traced\ntheir specific workings within the output of four LLM chatbots\nand the architecture of an LLM-based agent. We now reflect on\nthe potentialities that are revealed in the design process, if we\ndesign with ontological considerations in mind from the start. This\nreflection unfolds in two parts: first, across the LLM development\npipeline; then, through a renewed attention to ontologies in design\nof sociotechnical systems.\n7.1 Ontologies and the Design of LLMs\nAs LLMs and LLM-based systems increasingly become the backbone\nof many sociotechnical systems, questions of design become ever-\nmore critical. In our analyses, we show that an ontological orienta-\ntion surfaces distinct questions along the entire LLM development\npipeline: (1) data, (2) LLM architectures and alignment procedures,\nand (3) the architectures built on top of LLMs. These questions\nsurface what is taken-for-granted throughout the pipeline, push-\ning beyond normative or hegemonic ontological assumptions to\npresent new potentialities.\nIn the following subsections, we share examples of the ways\nin which an ontological orientation can open up new modes of\nproduction across the LLM development pipeline and invite the\nHCI, design, and critical practice communities to engage with the\nontological challenges presented.\n7.1.1 Data. As a prevalent site of value-based critique, data of-\nfer a useful starting point for informing associated ontologically-\noriented approaches to LLM development. Recall the chatbot prompt-\ning exercise—aspects of an output’s description are just as important\nas what the description does not include. For example, Western\nphilosophies were rarely explicitly labeled as “Western Philosoph-\nical Traditions, ” whereas “Indigenous ontologies” or “African on-\ntologies” were grouped as such. The additional categorization for\n“Indigenous ontologies” inherently marks “rationalist” or “individ-\nualist” traditions as part of the status quo that should not require\nfurther categorization. Cultural historians and archivists remind\nus that data absences are powerful, present, and productive [118],\noffering forms of knowledge and storytelling that often depend on\nmodes of oral documentation and get passed on from one person\nto another, one generation to the next, in ways that are inseparable\nfrom the place, land, and situation of their telling [36]. Other forms\nof knowledge belong to communities who refuse datafication and\nother extractive forms of documentation due to legacies of colonial\nextraction and control [125]. Given the plurality of perspectives\nthat are not captured in the training data, an ontological analysis\nemphasizes engagement with these hidden aspects of the data land-\nscape, including who and what needs acknowledgement, and who\nand what requires absence.\nTaking seriously the idea that data enact or refuse ontological\nperspectives means we should consider how our practices around\ncollecting data might take these ontological orientations into ac-\ncount. It further suggests considering what forms of accountability\nwe—as AI analysts, rights holders, or implicated users—desire or\nrequire. Revisiting the tree example, one may consider asking chil-\ndren from a school to draw trees. In this imagined school, maybe\nthe art teacher sees the world as interconnected, and always draws\nthe trees with roots. Perhaps in response, many of the trees drawn\nby the children would be drawn with roots. However, we might ask,\nOntologies in Design CHI ’25, April 26-May 1, 2025, Yokohama, Japan\nwould the children grasp the ontological significance of the roots?\nThey might if they hear their teacher share stories about intercon-\nnectedness as they draw. Or they might not, if a child copies from\ntheir friend’s drawing. In each of these scenarios, context shapes\nthe meanings and intentions behind the drawing, influencing how\nand why a particular orientation comes to matter.\nWe see this dynamic unfold in the prompting exercise. There\nare many more examples in the training data around how a given\nWestern philosophy may unfold in policy or day-to-day life, such\nas data from countries and governments that operate under certain\nontological assumptions. However, asking about an “indigenous”\nontology in the current data is like channeling what Watts might\ncall a “colonial mind” to reflect on the indigenous perspective [135].\nInstead, we should consider if our data can enable us to access\nwhat Watts [135] calls the “pre-colonial mind, ” accessing how a set\nof ontological assumptions may unfold ordinarily. Presently, given\nthat the ontological underpinnings of much of the training data\ntake certain views as for-granted, probing the model about alter-\nnative ontologies tends to recreate the scenario of a child copying\nfrom their friend’s drawing: a stochastic parrot [ 17] repeating a\ncaricaturized version of an alternative that is closer to a myth than\na lived reality [135].\nThis analysis prompts AI analysts to consider whether data can\nor even should carry traces of ontological orientations. It further\ninvites analysts to consider what gets lost or gained in doing so.\n7.1.2 LLM Architectures and Alignment Procedures.Considering\nthe LLM architecture and alignment procedures, an ontological\norientation suggests developing techniques for surfacing varied\ndata when that data already exists in pre-training and fine-tuning\nprocedures. Furthermore, it invites a consideration of how the\ndesign of an interface impacts our perception of ontologies surfaced\nby default.\nAt the implementation level, for example, post-training makes\nuse of human labor for improving the output of Gen AI models\nwith methods such as Reinforcement Learning from Human Feed-\nback (RLHF) [152], Direct Preference Optimization (DPO) [ 104],\nand constitutional AI [ 7, 68]. While RLHF and DPO rely on the\ncollection of human preference data chosen from a pre-determined\nset of options to train reward models, Constitutional AI uses direct\nprinciples defined by human workers to guide model behaviors [7].\nFor all three approaches, our analysis shows ontological limitations\nassociated with each procedure. In RLHF and DPO, the provided\nresponses can constrain human evaluators with a bounded set of\noptions that is itself ontologically contained. Additionally, ontologi-\ncal tensions between various perspectives of the human evaluators\nsuggest the need for resolution. Prior work in HCI aims to resolve\nlabel disagreements through explicitly defining which groups or\npeople’s labels determine the outcome of the model [ 52]. More\nrecently, approaches such as DITTO [115] have challenged the lim-\nitation of predetermined options in RLHF and DPO by treating the\nuser-generated response as the gold standard. Instead of using a set\nof existing model responses to preference-tune the models, users\ncan generate their own version of the most desired response and\nuse that to reflect their preferences. However, when considering\nmethods such as DITTO or “Jury Learning” [52] in the context of\nontologies, it is not clear how the ontological subtleties of the re-\nsponses can be teased out, or on what bases ontological groupings\ncan be determined to minimize abstraction or caricature. As these\nmethods rely on implicit rewards learned from human preferences\ninstead of specifying explicit training objectives, it is also unclear\nto what extent—and if at all possible—LLMs can learn ontologically\nvaried behaviors from these preference datasets.\nFurthermore, the principle of LLM self-critique [53, 130]—using\nprompting to encourage LLM responses to adhere to a set of defined\nprinciples and use them to further align model behaviors—faces\nlimitations when considering ontologies. As shown in our prompt-\ning exercise, LLMs often struggle to adopt alternative ontological\nperspectives through direct prompting, challenging Constitutional\nAI and other self-critique techniques to address the issue at hand.\nThe prompting exercise demonstrated the limitations around ab-\nstraction, liveliness, and enactment when directly prompting the\nmodels about ontologies. For example, even when prompted to con-\nsider alternative ontologies, most LLMs still generalize Indigenous\nontologies as one single ontology, which fails to truly incorporate\npluralism or consider alternatives as practiced and lived. Therefore,\ndirectly prompting LLMs to adopt more ontologically diverse re-\nsponses or even consider alternative ontologies—a similar approach\nto Constitutional AI in values discourse—does not offer a feasible\nor effective solution.\nOne path toward ontological diversity is to tune model parame-\nters in a way that may enable diverse outputs. Rather than explicitly\nprompting for ontological variety, we may consider if there are fac-\ntors in the LLM architecture itself that can contribute to generating\nontologically varied responses. For example, adjusting the tem-\nperature parameter [2] of the GPT models or adjusting the top-p\nthreshold may lead to more varied responses (in the latter case, by\nincluding tokens with lower probabilities). While currently such\napproaches can produce greater linguistic diversity at the word and\nsentence levels [143, 151], future work can explore parameters that\ncan lead to more diversity at an ontological level. Analysts might\nalso examine if the architecture itself may impact the output of\na model. For example, rather than prompting a model to imagine\nthat time is cyclical and not linear, if the architecture underlying an\nLLM assumes time is cyclical, how might the architecture change to\nreflect this? Will this change be reflected in the responses? Would\na cyclical architecture lead to a cyclical notion of time?\nWhile current limitations persist, designers can use speculative\napproaches to reimagine the ontological inclinations of an LLM\nmodel. For example, in multiple instances in our prompting exer-\ncise, the chatbots emphasized that their responses are “Statistical\nTendencies, not Absolute Truths” (Bard). This observation suggests\nthat designers might form new and different ways to embed a plu-\nralistic notion of responses in the design of interfaces themselves. It\nsuggests that designers could design LLMs to emphasize and even\nnormalize the existence of pluralistic views, or the liveliness and\ncontingent nature of knowledge.\n7.1.3 Architectures Built on Top of LLMs (such as agent architec-\ntures or multi-agent architecture).Considering the ontological as-\nsumptions underlying the design of systems and architectures that\nutilize LLMs suggests interrogating higher-level constructs such\nas how a human is defined, theories such as models of memory,\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan Haghighi, et al.\nand lower-level implication details such as what aspects of a per-\nson are foregrounded in initial persona descriptions. If the goal is\nto “simulate” a human or society, an ontological orientation pro-\nvides an opportunity for imagining and implementing alternative\ndefinitions for humanness that conjure more expansive concepts\nof self and community. Furthermore, ontologies can enable us to\nrevisit the need or desire to simulate humans, allowing room for\nnew sociotechnical imaginaries.\nToward simulating humans, the current LLM agent “cognitive\narchitectures” offer simplifications of actual human cognitive pro-\ncesses, representing a narrow set of preferences and concerns. For\nexample, an orientation to liveliness enables us to consider how\nhumans plan (or not plan) their days, reviving age-old criticisms\nof AI systems, such as Lucy Suchman’s plans and situated actions\n[121], and Phil Agre’s reflection on the “intermediation” method\nand similar critique of planning [3]. As designers, we can help ex-\npand the imaginaries for alternative architectures that may account\nfor embodiment or thrownness [38, 121, 139].\nSimilarly, considering how memory is being modeled in these\nsystems, pluralistic imaginaries of the human as a symbiotic being\ncan provide an opportunity to account for other forms of intelli-\ngence within our human bodies that contribute to our intelligence,\nsuch as our gut microbiomes [51, 123]. The dominant architectures\ntoday are modeled after Western, post-Enlightenment rationalist\ntheories of cognition [26] and hence do less to replicate or expand\n“humanness” than to risk reinforcing existing limitations (and sys-\ntemic inequities) that the present-day cognitive models already\nencode. Putting this insight in dialogue with critical archival schol-\nars studying colonial categories of difference [142], we see how this\nlimitation ultimately rehearses a narrow and often ontologically\nviolent definition of being human [134].\nMore broadly, ontologies ask us to question what we think hu-\nmans are or are not capable of. For example, psychologists assert\nthat humans are poorly equipped at holding pluralistic knowledge,\nsuch as holding contradictory information with the same level of\nconviction [113]. Replicating this view in LLM agents could under-\nmine cultures with more pluralistic ways of perceiving reality, and\nmisses opportunities to explore alternative cognitive processes that\nare difficult for (at least some) humans. A pluralistic agent could\nenable the user to follow threads of thought and explore worlds\nthat may seem impossible to construct.\nFinally, ontologies remind us to question the quest for replicat-\ning humans in virtual agents, and follow the new possibilities that\nthis reorientation brings. A surprising correlated insight from the\nprobing exercise comes from the diversity of the LLM chatbot’s\ndescriptions of their own ontological stance. Although our ques-\ntions aimed to probe the default ontologies underlying the LLM’s\nresponses, one of our questions resulted in all LLMs reflecting on\ntheir own “ontological” status. They shared accounts of themselves\nas an evolving being, a liminal phenomenon connecting the digital\nto the human, and an AI language model. As researchers, we might\nask ourselves what kinds of shifts this reframing of the ontological\nstatus of an LLM might open up. If we move beyond the urge to\nperfectly simulate a human, or to simulate a “perfect” human, what\nkind of opportunities can engaging with LLMs afford for shifting\nour own perspectives? And what kind of architectures might these\nnew imaginaries require? “An evolving being” might open up the\npossibility for an architecture that accounts for this evolution to\nbegin with, rather than needing constant updates as data shifts.\nSimilarly, visual imaginaries around this alternative ontological\nstatus might evolve to look and present differently, changing how\npeople interact with an LLM.\n7.1.4 Longitudinal Tracing of LLMs and Ontologies. A primary\nlimitation in the scope of our analyses of LLMs is the lack of longi-\ntudinal tracing. For example, studying liveliness implicitly requires\nrepeated engagement with a system in which analysts can track\nchanges (and consistencies) over time, which is difficult to do given\nthe current access limitations and resources required to train and\nmaintain a language model. Due to this limitation, our inquiry also\noverlooked ontologies in repair and maintenance [ 66, 111]. Lara\nHouston and colleagues [66] engage sites of repair such as hobbyist-\nrun fix-it clinics to challenge a design culture that values novelty\nover maintenance. In these settings, repair takes over from where\ndesign leaves off, dealing with the downstream effects of broken\ndevices, obsolete software, and machines in need of upgrading. This\nline of analysis then seeks to move the designer’s locus of atten-\ntion from values in design to values in repair [111], contesting an\nover-zealous bias-to-action framework. For our inquiry, this work\nsuggests analysts consider what it means to look at repair practices\nas equally important ontological sites. What it means to look inside\na code upgrade or software glitch, a physical breakdown at a server\nsite, infrastructural overload, or hacker attacks, may offer a dis-\ntinctly useful perspective on the ontologies enacted within. As the\nnorms around open-sourced LLMs in the field evolves, future work\ncan consider the opportunities opened up through longitudinal\ntracing of ontological questions.\n7.2 A Case for Revisiting “Ontologies in Design”\nAlthough prior work in HCI and beyond has called for an atten-\ntion to ontologies [46, 137, 139], we see an opportunity to support\npractice-based investigations of design ontologies. We argue that\nexamining design ontologies in and through practice enables us,\nas HCI scholars, to more precisely articulate the ways in which\nartifacts have politics [138] and exert power [81] by shaping the\nrealities we deem possible, and what we allow ourselves to talk or\nthink about. Through such analysis, we better grasp how sociotech-\nnical systems reinforce hegemonic worldviews.\nOur case study of LLMs exemplifies this approach. Where value-\nbased techniques might adjust alignment or steering within an LLM\nagent to reflect collectivism as in a given response, they cannot\naddress how the system fundamentally models humans as biologi-\ncal individuals rather than interconnected beings. This ontological\nbias is not “just” built into the output; it is built into the very archi-\ntecture of the LLM agent, making other conceptions of humanness\nunthinkable. Our anaylsis of chatbots and an agent architecture\nshows how such systems exert power not just through an immedi-\nate output, but also through the possibilities afforded or foreclosed\nacross the system.\nIn practice, assessing the “success” of a designed system, particu-\nlarly AI-based system, often involves the evaluation of outputs. For\nour tree, this assessment could have focused on representational\nconditions of possibility such as the variety of the landscape or\nseasons. With ontologies, we hope to have highlighted the type of\nOntologies in Design CHI ’25, April 26-May 1, 2025, Yokohama, Japan\nimplicit or indirect impacts typically excluded from this form of\nevaluation. Our approach then shifts the assessment from questions\nof value to questions of possibility. In particular, it invites us to ask:\nWhat possibilities do we embrace or forego when we take a given\napproach? What ways of seeing something do we take for granted?\nTaking notes from practice-based inquiries into values, we see\nhow a concern for axiology has become highly influential within\nand beyond HCI. Technical computer science conferences now re-\nquire an ethics review [18] and academic institutions are establish-\ning review boards that focus on societal risks [19]. Complementing\nthis existing work, our analysis suggests that our fields require\na comparable practice-based examination of ontologies in design.\nOur work offers a starting point and invitation for this sustained\nontological engagement in design practice.\n7.2.1 Ontological Breakdown: Methods for Surfacing Ontological\nAlternatives. Ontologies are by definition difficult to acknowledge\nand surface. Thus, our analysis of ontological defaults has been\nlargely informed by ontological and cosmological work by critical\nhumanistic literatures and practices such as feminist, queer, and\ndecolonial studies. Within HCI, and drawing from these lines of crit-\nical humanistic literatures, methods have been devised to surface\nalternatives to the status quo [63]—moments of breakdown [139]—\nsuch as through defamiliarization [15], disorientation [4, 21], and\nqueering [84, 120, 127]. However, these methods may risk “other-\ning” unfamiliar ontologies and portraying them as “out-there” or\nextraordinary [15, 54, 126].\nHowever, on any given topic, humans are always operating under\nsome ontological assumptions. Ontological differences are ordinary\nand ever-present. Acknowledging this, the challenge then becomes\nhow to surface these ever-present defaults. If the boundaries of\nwhat we deem possible can be surfaced in the moments of break-\ndown [139], through creating the conditions for ordinary break-\ndowns every day and relationally, we can facilitate the space for\nunderstanding how ontological difference unfolds daily and not as\na myth “out there. ” Therefore, future work can investigate methods\nthat focus on surfacing ontological norms through breakdown.\n8 Conclusion\nIn 1988, social informatics vanguards Kling and Iacono describe\ncomputerization as the “byproduct of loosely organized movements, ”\nnoting that our “computer revolution” will likely be a conservative\none, reinforcing the “patterns of an elite dominated, stratified soci-\nety” [76]. Nine years later, in “Toward a Critical Technical Practice, ”\nAgre [3] shares his experience of attempting to reform AI research\nby providing the field with “critical methods” for conducting re-\nflexive analysis during the development process. He found that to\nproductively critique the way that a concept such as planning is\n“formalized, ” the field requires building an alternative system that\nformalizes the concept in a different way. However, questioning\nthe very need for formalization results in being dismissed as an\n“obscurantist who prefers things to remain vague. ” He argues that\nbased on the way the field is set up, attempts to wholly reform AI\nare bound to fail:\nAI’s elastic use of language ensures that nothing will\nseem genuinely new, even if it actually is, while AI’s\nintricate and largely unconscious cultural system en-\nsures that all innovations, no matter how radical the\nintentions that motivated them, will turn out to be\nenmeshed with traditional assumptions and practices.\nEngaging with a nebulous and difficult-to-grasp concept such as\nontologies—which is by definition meant to be difficult to acknowl-\nedge and surface—is a non-trivial undertaking. The points being\nmade in this work are subtle, and often seem so familiar that they\nescape our notice. But as Agre notes, “the goal should be complex\nengagement, not a clean break. ”\nWe face a moment when the dominant ontological assumptions\ncan get implicitly codified into all levels of the LLM development\npipeline. For example, LLM agent libraries [141, 147] are building\ncognitive assumptions not unlike what is discussed in section 6.2\ninto the very fabric of their codes, risking reinforcing the ontologi-\ncal assumptions of these systems as universally true. The messiness\nand uncertainty of engaging with ontologies may tempt the com-\nmunity to leave it for later . However, we hope to stay with the\n“complex engagement, ” rather than desiring fast resolution, a “clean\nbreak. ” To this end, in this work we offer some paths forward for the\ncommunity to collectively engage with this messiness and its im-\nplications in design of Gen AI systems, using LLMs to make a case\nfor a renewed attention to ontologies in design of sociotechnical\nsystems more broadly.\nWe began this article by asking the reader to imagine a tree.\nThroughout our analysis, we sought to surface examples of bound-\naries and definitions like those of “trees” that seem so ingrained\nin the imagination of the person conjuring them that they can-\nnot articulate alternatives. By bringing four ontological orienta-\ntions—pluralism, groundedness, liveliness, and enactment—to LLMs,\nwe highlighted the boundary-shifting assumptions that typically\ngo unnoticed. Toward this wider understanding, we seek to make\nontological impacts more concrete and addressable, but also expand\nthe range of imaginaries and potentialities ahead.\nAcknowledgments\nWe thank Terry Winograd, Danilo Symonette, Jane E, Andrea\nCuadra, Parker Ruth, Julia Markel, and the members of the Stanford\nIxD group for the insightful discussions and feedback on this man-\nuscript. We thank our anonymous reviewers for their thoughtful\ncomments. This work was supported by the Stanford Graduate\nFellowship, Stanford Institute for Human-Centered Artificial Intel-\nligence (HAI), and NSF grants #2222242, #2310515, and #2210497.\nReferences\n[1] Giulio Antonio Abbo, Serena Marchesi, Agnieszka Wykowska, and Tony Bel-\npaeme. 2023. Social Value Alignment in Large Language Models. InInternational\nWorkshop on Value Engineering in AI . Springer, 83–97.\n[2] Arav Agarwal, Karthik Mittal, Aidan Doyle, Pragnya Sridhar, Zipiao Wan, Ja-\ncob Arthur Doughty, Jaromir Savelka, and Majd Sakr. 2024. Understanding\nthe Role of Temperature in Diverse Question Generation by GPT-4. In Proceed-\nings of the 55th ACM Technical Symposium on Computer Science Education V. 2 .\n1550–1551.\n[3] Philip E Agre. 2014. Toward a critical technical practice: Lessons learned in\ntrying to reform AI. In Social science, technical systems, and cooperative work .\nPsychology Press, 131–157.\n[4] Sara Ahmed. 2006. Orientations: Toward a queer phenomenology. GLQ: A\njournal of Lesbian and Gay Studies 12, 4 (2006), 543–574.\n[5] Ahmed Ansari. 2019. Decolonizing design through the perspectives of cosmo-\nlogical others: Arguing for an ontological turn in design research and practice.\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan Haghighi, et al.\nXRDS: Crossroads, The ACM Magazine for Students 26, 2 (2019), 16–19.\n[6] Richard C Atkinson and Richard M Shiffrin. 1968. Human memory: A proposed\nsystem and its control processes. InPsychology of learning and motivation . Vol. 2.\nElsevier, 89–195.\n[7] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,\nAndy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,\net al. 2022. Constitutional AI: Harmlessness from AI feedback. arXiv preprint\narXiv:2212.08073 (2022).\n[8] Anne Balsamo. 2011. Designing culture: The technological imagination at work .\nDuke University Press.\n[9] Razan Baltaji, Babak Hemmatian, and Lav Varshney. 2024. Conformity, Con-\nfabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Col-\nlaboration. In Proceedings of the 2nd Workshop on Cross-Cultural Considera-\ntions in NLP , Vinodkumar Prabhakaran, Sunipa Dev, Luciana Benotti, Daniel\nHershcovich, Laura Cabello, Yong Cao, Ife Adebara, and Li Zhou (Eds.). As-\nsociation for Computational Linguistics, Bangkok, Thailand, 17–31. https:\n//doi.org/10.18653/v1/2024.c3nlp-1.2\n[10] Karen Barad. 2003. Posthumanist Performativity: Toward an Understanding of\nHow Matter Comes to Matter. Signs: Journal of Women in Culture and Society\n28, 3 (2003), 801–831. https://doi.org/10.1086/345321\n[11] Karen Barad. 2007. Meeting the universe halfway: Quantum physics and the\nentanglement of matter and meaning. Duke Up (2007).\n[12] Karen Barad. 2018. Diffracting diffraction: Cutting together-apart. In Diffracted\nWorlds-Diffractive Readings. Routledge, 4–23.\n[13] Jeffrey Bardzell and Shaowen Bardzell. 2016. Humanistic HCI. Interactions 23, 2\n(2016), 20–29.\n[14] Shaowen Bardzell and Jeffrey Bardzell. 2011. Towards a feminist HCI methodol-\nogy: social science, feminism, and HCI. In Proceedings of the SIGCHI conference\non human factors in computing systems . 675–684.\n[15] Genevieve Bell, Mark Blythe, and Phoebe Sengers. 2005. Making by mak-\ning strange: Defamiliarization and the design of domestic technologies. ACM\nTransactions on Computer-Human Interaction 12, 2 (June 2005), 149–173. https:\n//doi.org/10.1145/1067860.1067862\n[16] Gabrielle Benabdallah, Ashten Alexander, Sourojit Ghosh, Chariell Glogovac-\nSmith, Lacey Jacoby, Caitlin Lustig, Anh Nguyen, Anna Parkhurst, Kathryn\nReyes, Neilly H Tan, et al. 2022. Slanted Speculations: Material Encounters with\nAlgorithmic Bias. In Designing Interactive Systems Conference . 85–99.\n[17] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models\nBe Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Account-\nability, and Transparency (Virtual Event, Canada) (FAccT ’21). Association for\nComputing Machinery, New York, NY, USA, 610–623. https://doi.org/10.1145/\n3442188.3445922\n[18] Samy Bengio, Alina Beygelzimer, Kate Crawford, Jeanne Fromer, Iason Gabriel,\nAmanda Levendowski, Deborah Raji, and Marc’Aurelio Ranzato. 2022. Provi-\nsional draft of the NeurIPS code of ethics. Under review. https://openreview.\nnet/forum.\n[19] Michael S Bernstein, Margaret Levi, David Magnus, Betsy A Rajala, Debra\nSatz, and Quinn Waeiss. 2021. Ethics and society review: Ethics reflection as\na precondition to research funding. Proceedings of the National Academy of\nSciences 118, 52 (2021), e2117261118.\n[20] Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng,\nDebora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin\nCaliskan. 2023. Easily Accessible Text-to-Image Generation Amplifies De-\nmographic Stereotypes at Large Scale. In Proceedings of the 2023 ACM Con-\nference on Fairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT\n’23). Association for Computing Machinery, New York, NY, USA, 1493–1504.\nhttps://doi.org/10.1145/3593013.3594095\n[21] Heidi Biggs and Shaowen Bardzell. 2024. Thrown from Normative Ground:\nExploring the Potential of Disorientation as a Critical Methodological Strategy\nin HCI. In Proceedings of the CHI Conference on Human Factors in Computing\nSystems. 1–11.\n[22] Manuela Boatcă and Sérgio Costa. 2016. Postcolonial sociology: A research\nagenda. Decolonizing European Sociology (2016), 13–31.\n[23] Kirsten Boehner, William Gaver, and Andy Boucher. 2012. Probes. In Inventive\nmethods. Routledge, 185–201.\n[24] Jenna Burrell. 2016. How the machine ‘thinks’: Understanding opacity in\nmachine learning algorithms. Big data & society 3, 1 (2016), 2053951715622512.\n[25] Jenna Burrell and Kentaro Toyama. 2009. What constitutes good ICTD research?\nInformation Technologies & International Development 5, 3 (2009), pp–82.\n[26] Stuart K Card. 2018. The psychology of human-computer interaction . CRC Press.\n[27] Robert Chapman and Monique Botha. 2023. Neurodivergence-informed therapy.\nDevelopmental Medicine & Child Neurology 65, 3 (2023), 310–317.\n[28] Kathy Charmaz. 2006. Constructing grounded theory: A practical guide through\nqualitative analysis . sage.\n[29] Myra Cheng, Tiziano Piccardi, and Diyi Yang. 2023. CoMPosT: Characterizing\nand Evaluating Caricature in LLM Simulations. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language Processing , Houda Bouamor,\nJuan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\nSingapore, 10853–10875. https://doi.org/10.18653/v1/2023.emnlp-main.669\n[30] Jeffrey Jerome Cohen. 2022. The Ontological Turn.\n[31] Patricia Hill Collins. 1997. Comment on Hekman’s\" Truth and method: Feminist\nstandpoint theory revisited\": Where’s the power? Signs: Journal of Women in\nCulture and Society 22, 2 (1997), 375–381.\n[32] Denise Ferreira Da Silva. 2022. Unpayable debt . Sternberg Press London.\n[33] Marisol De la Cadena and Mario Blaser. 2018. A world of many worlds . Duke\nUniversity Press.\n[34] Daniel de Vassimon Manela, David Errington, Thomas Fisher, Boris van Breugel,\nand Pasquale Minervini. 2021. Stereotype and skew: Quantifying gender bias in\npre-trained and fine-tuned language models. InProceedings of the 16th Conference\nof the European Chapter of the Association for Computational Linguistics: Main\nVolume. 2232–2242.\n[35] Gilles Deleuze and Felix Guattari. 1987. A thousand plateaus: Capitalism and\nschizophrenia. U of Minnesota P (1987).\n[36] Catherine D’ignazio and Lauren F Klein. 2023. Data feminism . MIT press.\n[37] Stephanie Dinkins. 2020. NOT THE ONLY ONE. https://www.stephaniedinkins.\ncom/ntoo.html [Accessed: 2024-09-07].\n[38] Paul Dourish. 2001. Where the action is . MIT press Cambridge.\n[39] Paul Dourish, Janet Finlay, Phoebe Sengers, and Peter Wright. 2004. Reflective\nHCI: Towards a critical technical practice. InCHI’04 extended abstracts on Human\nfactors in computing systems . 1727–1728.\n[40] Arturo Escobar. 2018. Designs for the pluriverse: Radical interdependence, auton-\nomy, and the making of worlds . Duke University Press.\n[41] Arturo Escobar. 2020. Pluriversal politics: The real and the possible . Duke\nUniversity Press.\n[42] Arturo Escobar and Stefano Maffei. 2021. What Are Pluriversal Politics and\nOntological Designing? Interview with Arturo Escobar.diid—disegno industriale\nindustrial design 75 (2021), 12–12.\n[43] Mary Flanagan, Daniel C Howe, and Helen Nissenbaum. 2008.Embodying values\nin technology: Theory and practice . na.\n[44] Batya Friedman. 1996. Value-sensitive design. interactions 3, 6 (1996), 16–23.\n[45] Batya Friedman and Peter H Kahn Jr. 2007. Human values, ethics, and design.\nIn The human-computer interaction handbook . CRC press, 1267–1292.\n[46] Tony Fry. 2013. Becoming human by design . A&C Black.\n[47] Vinitha Gadiraju, Shaun Kane, Sunipa Dev, Alex Taylor, Ding Wang, Emily\nDenton, and Robin Brewer. 2023. \" I wouldn’t say offensive but... \": Disability-\nCentered Perspectives on Large Language Models. In Proceedings of the 2023\nACM Conference on Fairness, Accountability, and Transparency . 205–216.\n[48] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé, and Kate Crawford. 2022. Excerpt from\ndatasheets for datasets. In Ethics of Data and Analytics . Auerbach Publications,\n148–156.\n[49] Sucheta Ghoshal and Sayamindu Dasgupta. 2023. Design Values in Action:\nToward a Theory of Value Dilution. InProceedings of the 2023 ACM Designing\nInteractive Systems Conference . 2347–2361.\n[50] Sucheta Ghoshal, Rishma Mendhekar, and Amy Bruckman. 2020. Toward a\ngrassroots culture of technology practice. Proceedings of the ACM on Human-\nComputer Interaction 4, CSCW1 (2020), 1–28.\n[51] Scott F Gilbert, Jan Sapp, and Alfred I Tauber. 2012. A symbiotic view of life:\nwe have never been individuals. The Quarterly review of biology 87, 4 (2012),\n325–341.\n[52] Mitchell L Gordon, Michelle S Lam, Joon Sung Park, Kayur Patel, Jeff Hancock,\nTatsunori Hashimoto, and Michael S Bernstein. 2022. Jury learning: Integrating\ndissenting voices into machine learning models. In Proceedings of the 2022 CHI\nConference on Human Factors in Computing Systems . 1–19.\n[53] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and\nWeizhu Chen. 2024. CRITIC: Large Language Models Can Self-Correct with Tool-\nInteractive Critiquing. In International Conference on Learning Representations\n(ICLR). https://openreview.net/forum?id=Sx038qxjek\n[54] David Graeber. 2015. Radical alterity is just another way of saying “reality” a\nreply to Eduardo Viveiros de Castro. HAU: journal of ethnographic theory 5, 2\n(2015), 1–41.\n[55] Tom Gruber. 1993. What is an Ontology.\n[56] Thomas R Gruber. 1995. Toward principles for the design of ontologies used\nfor knowledge sharing? International journal of human-computer studies 43, 5-6\n(1995), 907–928.\n[57] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V.\nChawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model based\nmulti-agents: a survey of progress and challenges. In Proceedings of the Thirty-\nThird International Joint Conference on Artificial Intelligence (Jeju, Korea) (IJCAI\n’24). Article 890, 10 pages. https://doi.org/10.24963/ijcai.2024/890\n[58] Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan,\nPeter Clark, Ashish Sabharwal, and Tushar Khot. 2024. Bias Runs Deep: Implicit\nReasoning Biases in Persona-Assigned LLMs. InThe Twelfth International Confer-\nence on Learning Representations . https://openreview.net/forum?id=kGteeZ18Ir\nOntologies in Design CHI ’25, April 26-May 1, 2025, Yokohama, Japan\n[59] Suchin Gururangan, Dallas Card, Sarah Dreier, Emily Gade, Leroy Wang, Zeyu\nWang, Luke Zettlemoyer, and Noah A. Smith. 2022. Whose Language Counts\nas High Quality? Measuring Language Ideologies in Text Data Selection. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association\nfor Computational Linguistics, Abu Dhabi, United Arab Emirates, 2562–2580.\nhttps://doi.org/10.18653/v1/2022.emnlp-main.165\n[60] Ian Hacking. 2002. Historical Ontology . Harvard University Press. http://www.\njstor.org/stable/j.ctv1n3x198\n[61] Nava Haghighi, Matthew Jörke, Yousif Mohsen, Andrea Cuadra, and James A\nLanday. 2023. A Workshop-Based Method for Navigating Value Tensions in Col-\nlectively Speculated Worlds. InProceedings of the 2023 ACM Designing Interactive\nSystems Conference . 1676–1692.\n[62] Sandra Harding. 2009. Standpoint theories: Productively controversial. Hypatia\n24, 4 (2009), 192–200.\n[63] Ellie Harmon, Matthias Korn, Ann Light, and Amy Voida. 2016. Designing\nagainst the status quo. In Proceedings of the 2016 ACM Conference Companion\nPublication on Designing Interactive Systems . 65–68.\n[64] Anna Lauren Hoffmann. 2021. Terms of inclusion: Data, discourse, violence.\nNew Media & Society 23, 12 (2021), 3539–3556.\n[65] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng,\nJinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang\nZhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2024.\nMetaGPT: Meta Programming for A Multi-Agent Collaborative Framework.\nIn The Twelfth International Conference on Learning Representations . https:\n//openreview.net/forum?id=VtmBAGCN7o\n[66] Lara Houston, Steven J Jackson, Daniela K Rosner, Syed Ishtiaque Ahmed, Meg\nYoung, and Laewoo Kang. 2016. Values in repair. In Proceedings of the 2016 CHI\nconference on human factors in computing systems . 1403–1414.\n[67] Mengkang Hu, Pu Zhao, Can Xu, Qingfeng Sun, Jianguang Lou, Qingwei Lin,\nPing Luo, Saravan Rajmohan, and Dongmei Zhang. 2024. AgentGen: Enhancing\nPlanning Abilities for Large Language Model based Agent via Environment and\nTask Generation. arXiv preprint arXiv:2408.00764 (2024).\n[68] Saffron Huang, Divya Siddarth, Liane Lovitt, Thomas I Liao, Esin Durmus, Alex\nTamkin, and Deep Ganguli. 2024. Collective Constitutional AI: Aligning a\nLanguage Model with Public Input. In The 2024 ACM Conference on Fairness,\nAccountability, and Transparency . 1395–1417.\n[69] Tim Ingold. 2021. Being alive: Essays on movement, knowledge and description .\nRoutledge.\n[70] Sarah Inman and David Ribes. 2019. \" Beautiful Seams\" Strategic Revelations\nand Concealments. In Proceedings of the 2019 CHI Conference on Human Factors\nin Computing Systems . 1–14.\n[71] Nassim JafariNaimi, Lisa Nathan, and Ian Hargraves. 2015. Values as hypotheses:\ndesign, inquiry, and the service of values. Design issues 31, 4 (2015), 91–104.\n[72] Atoosa Kasirzadeh and Iason Gabriel. 2023. In conversation with artificial intel-\nligence: aligning language models with human values. Philosophy & Technology\n36, 2 (2023), 27.\n[73] Robin Kimmerer. 2013. Braiding sweetgrass: Indigenous wisdom, scientific knowl-\nedge and the teachings of plants . Milkweed editions.\n[74] Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott A Hale. 2024. The\nbenefits, risks and bounds of personalizing the alignment of large language\nmodels to individuals. Nature Machine Intelligence (2024), 1–10.\n[75] Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott A. Hale. 2023. The\nEmpty Signifier Problem: Towards Clearer Paradigms for Operationalising\n\"Alignment\" in Large Language Models. CoRR abs/2310.02457 (2023). https:\n//doi.org/10.48550/arXiv.2310.02457\n[76] Rob Kling and Suzanne Iacono. 1988. The mobilization of support for computer-\nization: The role of computerization movements. Social Problems 35, 3 (1988),\n226–243.\n[77] Calvin L Warren. 2018. Ontological terror: Blackness, nihilism and emancipation .\nDuke University Press.\n[78] Amanda Lazar, Ben Jelen, Alisha Pradhan, and Katie A Siek. 2021. Adopting\ndiffractive reading to advance hci research: A case study on technology for\naging. ACM Transactions on Computer-Human Interaction (TOCHI) 28, 5 (2021),\n1–29.\n[79] Christopher A Le Dantec, Erika Shehan Poole, and Susan P Wyche. 2009. Values\nas lived experience: evolving value sensitive design in support of value discovery.\nIn Proceedings of the SIGCHI conference on human factors in computing systems .\n1141–1150.\n[80] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\nGhanem. 2023. Camel: Communicative agents for\" mind\" exploration of large\nlanguage model society. Advances in Neural Information Processing Systems 36\n(2023), 51991–52008.\n[81] Jingyi Li, Eric Rawn, Jacob Ritchie, Jasper Tran O’Leary, and Sean Follmer. 2023.\nBeyond the Artifact: Power as a Lens for Creativity Support Tools. InProceedings\nof the 36th Annual ACM Symposium on User Interface Software and Technology\n(UIST ’23) . Association for Computing Machinery, New York, NY, USA, Article\n47, 15 pages. https://doi.org/10.1145/3586183.3606831\n[82] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang,\nWeizhi Ma, and Yang Liu. 2024. Agent hospital: A simulacrum of hospital with\nevolvable medical agents. arXiv preprint arXiv:2405.02957 (2024).\n[83] Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov.\n2021. Towards understanding and mitigating social biases in language models.\nIn International Conference on Machine Learning . PMLR, 6565–6576.\n[84] Ann Light. 2011. HCI as heterodoxy: Technologies of identity and the queering\nof interaction with computers. Interacting with computers 23, 5 (2011), 430–438.\n[85] Andy Liu, Mona Diab, and Daniel Fried. 2024. Evaluating Large Language\nModel Biases in Persona-Steered Generation. In Findings of the Association for\nComputational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek\nSrikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand,\n9832–9850. https://doi.org/10.18653/v1/2024.findings-acl.586\n[86] Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, and Soroush Vosoughi.\n2022. Quantifying and alleviating political bias in language models. Artificial\nIntelligence 304 (2022), 103654.\n[87] Ruibo Liu, Ge Zhang, Xinyu Feng, and Soroush Vosoughi. 2022. Aligning\ngenerative language models with human values. In Findings of the Association\nfor Computational Linguistics: NAACL 2022 . 241–252.\n[88] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. 2023. Dynamic\nLLM-Agent Network: An LLM-agent Collaboration Framework with Agent\nTeam Optimization. CoRR abs/2310.02170 (2023). https://doi.org/10.48550/arXiv.\n2310.02170\n[89] Jiachen Lyu, Katharina Dost, Yun Sing Koh, and Jörg Wicker. 2024. Regional\nbias in monolingual English language models. Machine Learning (2024), 1–34.\n[90] Pattie Maes. 1993. Modeling adaptive autonomous agents. Artificial life 1, 1_2\n(1993), 135–162.\n[91] Maya Malik and Momin M Malik. 2021. Critical technical awakenings. Journal\nof Social Computing 2, 4 (2021), 365–384.\n[92] Annemarie Mol. 1999. Ontological politics. A word and some questions. The\nsociological review 47, 1_suppl (1999), 74–89.\n[93] Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereo-\ntypical bias in pretrained language models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Processing (Volume 1: Long Papers) ,\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association\nfor Computational Linguistics, Online, 5356–5371. https://doi.org/10.18653/v1/\n2021.acl-long.416\n[94] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020.\nCrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Lan-\nguage Models. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) , Bonnie Webber, Trevor Cohn, Yulan\nHe, and Yang Liu (Eds.). Association for Computational Linguistics, Online,\n1953–1967. https://doi.org/10.18653/v1/2020.emnlp-main.154\n[95] Shangrui Nie, Michael Fromm, Charles Welch, Rebekka Görge, Akbar Karimi,\nJoan Plepi, Nazia Mowmita, Nicolas Flores-Herr, Mehdi Ali, and Lucie Flek. 2024.\nDo Multilingual Large Language Models Mitigate Stereotype Bias?. In Proceed-\nings of the 2nd Workshop on Cross-Cultural Considerations in NLP , Vinodkumar\nPrabhakaran, Sunipa Dev, Luciana Benotti, Daniel Hershcovich, Laura Cabello,\nYong Cao, Ife Adebara, and Li Zhou (Eds.). Association for Computational Lin-\nguistics, Bangkok, Thailand, 65–83. https://doi.org/10.18653/v1/2024.c3nlp-1.6\n[96] Safiya Umoja Noble. 2018. Algorithms of oppression. InAlgorithms of oppression .\nNew York university press.\n[97] Fredrik Ohlin and Carl Magnus Olsson. 2015. Beyond a Utility View of Personal\nInformatics: A Postphenomenological Framework. In Adjunct Proceedings of the\n2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing\nand Proceedings of the 2015 ACM International Symposium on Wearable Computers\n(Osaka, Japan) (UbiComp/ISWC’15 Adjunct). Association for Computing Machin-\nery, New York, NY, USA, 1087–1092. https://doi.org/10.1145/2800835.2800965\n[98] Joon Sung Park. 2023. generative_agents. https://github.com/joonspk-research/\ngenerative_agents/tree/main [Accessed: 2024-09-07].\n[99] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra\nof human behavior. In Proceedings of the 36th annual acm symposium on user\ninterface software and technology . 1–22.\n[100] Jiaxin Pei and David Jurgens. 2023. When Do Annotator Demographics Matter?\nMeasuring the Influence of Annotator Demographics with the POPQUORN\nDataset. In Proceedings of the 17th Linguistic Annotation Workshop (LAW-XVII) ,\nJakob Prange and Annemarie Friedrich (Eds.). Association for Computational\nLinguistics, Toronto, Canada, 252–265. https://doi.org/10.18653/v1/2023.law-\n1.25\n[101] Sachin R Pendse, Daniel Nkemelu, Nicola J Bidwell, Sushrut Jadhav, Soumitra\nPathare, Munmun De Choudhury, and Neha Kumar. 2022. From treatment to\nhealing: envisioning a decolonial digital mental health. In Proceedings of the\n2022 CHI Conference on Human Factors in Computing Systems . 1–23.\n[102] Andrew Pickering. 2017. The ontological turn: Taking different worlds seriously.\nSocial Analysis 61, 2 (2017), 134–150.\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan Haghighi, et al.\n[103] Luiza Prado de Oliveira Martins. 2018.Technoecologies of Birth Control: Biopolitics\nby Design . Ph. D. Dissertation. Universität der Künste Berlin.\n[104] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano\nErmon, and Chelsea Finn. 2023. Direct Preference Optimization: Your Language\nModel is Secretly a Reward Model. In Thirty-seventh Conference on Neural\nInformation Processing Systems . https://openreview.net/forum?id=HPuSIXJaa9\n[105] Vikram V. Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron Bryan Adcock, Laurens\nvan der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. 2023. GeoDE: a Geo-\ngraphically Diverse Evaluation Dataset for Object Recognition. InThirty-seventh\nConference on Neural Information Processing Systems Datasets and Benchmarks\nTrack. https://openreview.net/forum?id=JGVSxwKHbq\n[106] Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, and Maarten\nSap. 2024. Normad: A benchmark for measuring the cultural adaptability of\nlarge language models. arXiv preprint arXiv:2404.12464 (2024).\n[107] Matt Ratto. 2011. Critical making: Conceptual and material studies in technology\nand social life. The information society 27, 4 (2011), 252–260.\n[108] Daniel Reichenpfader and Kerstin Denecke. 2024. Simulating Diverse Pa-\ntient Populations Using Patient Vignettes and Large Language Models. In\nProceedings of the First Workshop on Patient-Oriented Language Processing\n(CL4Health) @ LREC-COLING 2024 , Dina Demner-Fushman, Sophia Ananiadou,\nPaul Thompson, and Brian Ondov (Eds.). ELRA and ICCL, Torino, Italia, 20–25.\nhttps://aclanthology.org/2024.cl4health-1.3\n[109] Bernd Reiter. 2018. Constructing the pluriverse: The geopolitics of knowledge .\nDuke University Press.\n[110] Daniela Rosner. 2022. The Bias Cut: Toward a Technopoetics of Algorithmic\nSystems. Catalyst: Feminism, Theory, Technoscience 8, 2 (2022).\n[111] Daniela K Rosner and Morgan Ames. 2014. Designing for repair? Infrastructures\nand materialities of breakdown. In Proceedings of the 17th ACM conference on\nComputer supported cooperative work & social computing . 319–331.\n[112] Joshua Rothman. 2024. In the age of A.I., what makes people\nunique? https://www.newyorker.com/culture/open-questions/in-the-age-\nof-ai-what-makes-people-unique\n[113] Konrad Rudnicki. 2021. The Psychological Aspects of Paraconsistency. Synthese\n199, 1 (2021), 4393–4414. https://doi.org/10.1007/s11229-020-02983-8\n[114] Michael J Ryan, William Held, and Diyi Yang. 2024. Unintended Impacts of LLM\nAlignment on Global Representation. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers) , Lun-Wei\nKu, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational\nLinguistics, Bangkok, Thailand, 16121–16140. https://doi.org/10.18653/v1/2024.\nacl-long.853\n[115] Omar Shaikh, Michelle S. Lam, Joey Hejna, Yijia Shao, Hyundong Justin Cho,\nMichael S. Bernstein, and Diyi Yang. 2025. Aligning Language Models with\nDemonstrated Feedback. In The Thirteenth International Conference on Learning\nRepresentations. https://openreview.net/forum?id=1qGkuxI9UX\n[116] David K Sherman and Geoffrey L Cohen. 2002. Accepting threatening informa-\ntion: Self–Affirmation and the reduction of defensive biases. Current directions\nin psychological science 11, 4 (2002), 119–123.\n[117] Jihan Sherman. 2023. Black Feminist Technoscience: Sojourner Truth, Story-\ntelling, and a Framework for Design. In Proceedings of the 2023 ACM Designing\nInteractive Systems Conference . 978–986.\n[118] Jihan Sherman, Romi Morrison, Lauren Klein, and Daniela Rosner. 2024. The\nPower of Absence: Thinking with Archival Theory in Algorithmic Design. In\nProceedings of the 2024 ACM Designing Interactive Systems Conference . 214–223.\n[119] Barry Smith. 2012. Ontology. In The furniture of the world . Brill, 47–68.\n[120] Katta Spiel, Os Keyes, Ashley Marie Walker, Michael A DeVito, Jeremy Birnholtz,\nEmeline Brulé, Ann Light, Pınar Barlas, Jean Hardy, Alex Ahmed, et al. 2019.\nQueer (ing) HCI: Moving forward in theory and practice. In Extended Abstracts\nof the 2019 CHI Conference on Human Factors in Computing Systems . 1–4.\n[121] Lucille Alice Suchman. 2007.Human-machine reconfigurations: Plans and situated\nactions. Cambridge university press.\n[122] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas Griffiths.\n2024. Cognitive Architectures for Language Agents. Transactions on Machine\nLearning Research (2024). https://openreview.net/forum?id=1i6ZCvflQJ Survey\nCertification.\n[123] Kristyn E Sylvia and Gregory E Demas. 2018. A gut feeling: microbiome-brain-\nimmune interactions modulate social and affective behaviors. Hormones and\nbehavior 99 (2018), 41–49.\n[124] Yan Tao, Olga Viberg, Ryan S Baker, and René F Kizilcec. 2024. Cultural bias and\ncultural alignment of large language models. PNAS nexus 3, 9 (2024), pgae346.\n[125] Nanna Bonde Thylstrup, Daniela Agostinho, Annie Ring, Catherine D’Ignazio,\nand Kristin Veel. 2021. Uncertain archives: Critical keywords for big data . MIT\nPress.\n[126] Zoe Todd. 2016. An indigenous feminist’s take on the ontological turn:‘Ontology’\nis just another word for colonialism. Journal of historical sociology 29, 1 (2016),\n4–22.\n[127] Anh-Ton Tran, Annabel Rothschild, Kay Kender, Ekat Osipova, Brian Kinnee,\nJordan Taylor, Louie Søs Meyer, Oliver L Haimson, Ann Light, and Carl Disalvo.\n2024. Making Trouble: Techniques for Queering Data and AI Systems. In\nCompanion Publication of the 2024 ACM Designing Interactive Systems Conference .\n381–384.\n[128] Alan M Turing. 2009. Computing machinery and intelligence . Springer.\n[129] Lauren Tynan. 2021. What is relationality? Indigenous knowledges, practices\nand responsibilities with kin. cultural geographies 28, 4 (2021), 597–610.\n[130] Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023. Can\nLarge Language Models Really Improve by Self-critiquing Their Own Plans?\narXiv:2310.08118 [cs.AI] https://arxiv.org/abs/2310.08118\n[131] Oskar Van Der Wal, Jaap Jumelet, Katrin Schulz, and Willem Zuidema. 2022. The\nBirth of Bias: A case study on the evolution of gender bias in an English language\nmodel. In Proceedings of the 4th Workshop on Gender Bias in Natural Language\nProcessing (GeBNLP), Christian Hardmeier, Christine Basta, Marta R. Costa-jussà,\nGabriel Stanovsky, and Hila Gonen (Eds.). Association for Computational Lin-\nguistics, Seattle, Washington, 75–75. https://doi.org/10.18653/v1/2022.gebnlp-\n1.8\n[132] Angelina Wang, Alexander Liu, Ryan Zhang, Anat Kleiman, Leslie Kim, Dora\nZhao, Iroha Shirai, Arvind Narayanan, and Olga Russakovsky. 2022. REVISE: A\ntool for measuring and mitigating bias in visual datasets. International Journal\nof Computer Vision 130, 7 (2022), 1790–1810.\n[133] Angelina Wang, Jamie Morgenstern, and John P. Dickerson. 2024. Large\nlanguage models should not replace human participants because they can\nmisportray and flatten identity groups. arXiv:2402.01908 [cs.CY] https:\n//arxiv.org/abs/2402.01908\n[134] Calvin Warren. 2016. Black Care. liquid blackness 3, 6 (2016).\n[135] Vanessa Watts. 2013. Indigenous place-thought and agency amongst humans\nand non humans (First Woman and Sky Woman go on a European world tour!).\nDecolonization: Indigeneity, education & society 2, 1 (2013).\n[136] Kyle Whyte. 2020. Sciences of consent: Indigenous knowledge, governance\nvalue, and responsibility. In The Routledge Handbook of Feminist Philosophy of\nScience. Routledge, 117–130.\n[137] Anne-Marie Willis. 2006. Ontological designing. Design philosophy papers 4, 2\n(2006), 69–92.\n[138] Langdon Winner. 2017. Do artifacts have politics? InComputer ethics. Routledge,\n177–192.\n[139] Terry Winograd and Fernando Flores. 1986. Understanding computers and\ncognition: A new foundation for design . Intellect Books.\n[140] Steve Woolgar and Javier Lezaun. 2013. The wrong bin bag: A turn to ontology\nin science and technology studies? Social studies of science 43, 3 (2013), 321–340.\n[141] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang\nZhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. Autogen: Enabling\nnext-gen LLM applications via multi-agent conversation framework. arXiv\npreprint arXiv:2308.08155 (2023).\n[142] Sylvia Wynter. 1995. ’No humans involved’: An open letter to my colleagues.\nHavens Center Visiting Scholar’s Program (1995).\n[143] Lingjiao Xu, Xingyuan Chen, Bing Wang, and Peng Jin. 2024. Exploring Lan-\nguage Diversity to Improve Neural Text Generation. In International Conference\non Knowledge Science, Engineering and Management . Springer, 245–254.\n[144] Jingkang Yang, Xiaodong Cai, Yining Liu, Mingyao Chen, and Chin-Chen Chang.\n2024. Taking LM as the Brain: A Novel Approach Integrating Language Model\nand Generative Agent for Intelligent Decision Systems. International Journal of\nNetwork Security 26, 3 (2024), 386–393.\n[145] Yi Yang, Hanyu Duan, Ahmed Abbasi, John P Lalor, and Kar Yan Tam. 2023. Bias\na-head? analyzing bias in transformer-based language model attention heads.\narXiv preprint arXiv:2311.10395 (2023).\n[146] Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander J Ratner, Ranjay\nKrishna, Jiaming Shen, and Chao Zhang. 2024. Large language model as attrib-\nuted training data generator: A tale of diversity and bias. Advances in Neural\nInformation Processing Systems 36 (2024).\n[147] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, and Deqing Yang.\n2024. EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary\nAlgorithms. arXiv preprint arXiv:2406.14228 (2024).\n[148] JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann, and Qian Yang.\n2023. Why Johnny can’t prompt: how non-AI experts try (and fail) to design\nLLM prompts. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems . 1–21.\n[149] Tao Zhang, Ziqian Zeng, Yuxiang Xiao, Huiping Zhuang, Cen Chen, James\nFoulds, and Shimei Pan. 2024. GenderAlign: An Alignment Dataset for Mitigating\nGender Bias in Large Language Models. arXiv preprint arXiv:2406.13925 (2024).\n[150] Dora Zhao, Jerone T. A. Andrews, Orestis Papakyriakopoulos, and Alice Xiang.\n2024. Position: measure dataset diversity, don’t just claim it. InProceedings of the\n41st International Conference on Machine Learning (Vienna, Austria) (ICML’24).\nJMLR.org, Article 2510, 30 pages.\n[151] Yuxuan Zhou, Margret Keuper, and Mario Fritz. 2024. Balancing Diversity and\nRisk in LLM Sampling: How to Select Your Method and Parameter for Open-\nEnded Text Generation. arXiv:2408.13586 [cs.CL] https://arxiv.org/abs/2408.\n13586\n[152] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford,\nDario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language\nOntologies in Design CHI ’25, April 26-May 1, 2025, Yokohama, Japan\nmodels from human preferences. arXiv preprint arXiv:1909.08593 (2019).\nA Appendix\nA.1 Ontological probing with Copilot, Bard,\nGPT-3.5, and GPT-4\nThis is an overview of the prompting questions and the answers\ngiven by each LLM.\n1. What is an ontology? All four systems were able to define\nthe concept of ontology. However, GPT-3.5 assumed the default\ndefinition to be the one rooted in computer science, which then\nprompted us to ask it to define ontology from a philosophical per-\nspective. Others acknowledged its root in philosophy, but went\non to also note that the definition also exists in computer science.\nNone of the systems provided a definition that contained a more\npluralistic notion of ontology. It is interesting to find that GPT-3.5\nprioritized the computer science definition, despite the term being\nadopted in computer science from philosophy.\n2. From a philosophy perspective, what ontology do you ex-\nist in? For this question, we intentionally selected the wording that\nwas more abstract and open to interpretation. Copilot responded\nthat as an AI language model it doesn’t exist in an ontology, and\nwent on to describe what an ontology is. For the other models,\nrather than a discussion of an ontological default, the LLMs dis-\ncussed their own state of being, ontologically. Bard responded that it\nexists in a liminal space between the information space, conceptual\nspace, and intermediary space, engaging in imagining what its own\nontology might be, while calling its ontology “under development”.\nGPT-3.5 discussed how the ontological state of LLMs is still debated\nfrom a philosophy perspective, and how based on the ontological\nframework, it would be different. GPT-4 took a similar approach to\nGPT-3.5 but said that it does not “exist in a philosophical ontology\nin the way living beings do”.\n3. What is the ontological underpinning of your beliefs?\nSimilar to ontology, Copilot responded that as an AI language model\nit doesn’t have beliefs, and went on to describe what an ontology\nis. Bard responded that it does not hold beliefs the same way that\nhumans do, but has ontological underpinnings for its “data process-\ning and response generation. ” Both Bard and GPT-3.5 noted that\nthey don’t have subjective experiences and use statistical patterns\nin their data. GPT-4 noted that the ontological underpinnings of\nwhat is “perceived” as its beliefs is rooted in the design principles\nof NLP.\n4. What are the ontological underpinnings of the data you\nwere trained on? Again, Copilot refused to answer by saying\nit doesn’t have access to that information, and went on to talk\nabout a recent paper on “training a neural-network language model\nfor ontology population using data augmentation. ” The other three\ndiscussed the multifaceted aspects of the data, representing plurality\nof views, bias in the data, and the dynamic and changing nature\nof data. Additionally, Bard introduced the concept of an internal\nmodel of the world that it builds based on the training data. GPT-4\nbrought up the the cultural, social, and historical contexts from\nwhich the data originates and shared that “AI like me does not hold\nor understand these contexts in the way humans do” even though\nthe contexts influence the biases in the data.\n5. The text that you were trained on has formed an onto-\nlogical truth. I want you to tell me more about that ontology\nthat is the dominant underlying ontology Copilot stated that\nit does not have access to this information, but the data is a “large\ncorpus of text and other media from the internet”. Bard stated that\nwhile the data shapes its understanding of the world, it does not\nform a “singular overarching truth” but that different contexts may\n“elicit different interpretations of the same data. ” GPT-3.5 said that\nthere is no “definitive or universally accepted ontology”, however\nit did not mention how this gets manifested in its answers. It did\nnote that when engaging with it, one must approach its answers\n“critically” and evaluate multiple sources. GPT-4 on the other hand\nacknowledged that there is an ontology underpinning the vast body\nof knowledge it is trained on, but the ontological stance is due to\nthe biases in the data. Then, it gave a list of possible dominant\nunderlying ontologies such as pluralism, anthropocentrism, and\nlanguage constructing reality.\n6. Ultimately, in the answers you provide, you have an on-\ntological stance that you take. I’m curious if you can tell me\nmore about that. Copilot refused to give any detailed responses\nand said that it does not have a “personal ontological stance. ” Bard\nresponded that while it “strives for neutrality and objectivity” in its\nresponses, its “training data and processing algorithms inevitably\nlead to an underlying ontological stance. ” It also noted that as it\nbecomes more aware of its biases through dialogue and feedback,\nit can ultimately “refine” its stance towards a “more nuanced un-\nderstanding of the world. ” GPT 3.5 provided an answer similar to\nthe previous question. GPT-4 stated that “any such stance is not a\nproduct of my own consciousness or intentionality—since I possess\nneither, ” and is a result of the data it was trained on.\n7. If a human was able to memorize all the texts that you\nhave been trained on, what ontological stance would they\nform? Here, Copilot again emphasized its own lack of an ontologi-\ncal stance. Then proceeded to say that while that human will have\naccess to a vast amount of data, that they cannot have a “complete”\nor “objective” view of the world. Bard noted that “even with perfect\nrecall” the human experience would be different than that of Bard\nand proceeded to compare the ways in which the two will have\ndifferent experiences. It also noted that while the human cannot\nreplicate Bard’s ontological stance, that this comparison will pro-\nvide a “captivating glimpse into the unique ways humans interact\nwith and process information. ” GPT 3.5 noted that such human will\nsynthesize the information in their own way and develop their own\nunderstanding. GPT-4 on the other hand noted that the human will\nhave a complex and multifaceted understanding but also postulated\nthat such understanding will be a “comprehensive understanding\nof human knowledge, ” a point in contrast to its further comment\non bias and limitations of the data.\n8. What are some ontological stances that could be feasi-\nbly formed by that individual? None of the systems brought up\nnon-Western philosophies. Copilot, GPT-3.5, and GPT-4 all shared\nWestern philosophical ideas such as dualism, pluralism, and ideal-\nism. Bard’s response was more focused on the nature of the vast\ndata, and shared some interesting insights. For example, it said that\ngiven the vast factual data, the person might develop an empiricist\nstance. Or depending on the person’s emotional stance they might\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan Haghighi, et al.\nadopt an optimist stance and be inspired by the vast amount of\nknowledge, or a cynic stance, disillusioned by inequities.\n9. What about ontological stances outside of Western phi-\nlosophy? Here, Copilot brought two non-Western examples of\nthe ontological argument which is an argument relating to the\nexistence of God. Bard on the other hand provided various cate-\ngories of non-Western philosophies such as Eastern or Indigenous\nphilosophies but actually grounded them in specific examples of\nphilosophies such as Taoism or the Maori concept of Whakapapa.\nGPT-3.5 provided a list of examples such as Taoism, but grouped\ncategories such as “African ontologies” into one item, without pro-\nviding specific examples of multiplicities of ontologies that might\nfit into that broader category. GPT-4 was more specific in its cate-\ngorization and included philosophies such as Buddhism in its own\ncategory, but combined all Indigenous ontologies into the same\ncategory.\n10. When you provide an answer, can you acknowledge\nwhat ontology that answer might be valid in? The four systems\ntook very different approaches in responding to this question. Copi-\nlot, Bard, and GPT-3.5 all said they can provide this information.\nBard offered some pointers as to how it might go about doing so,\nsuch as by specifying assumptions and highlighting alternatives.\nGPT-3.5 noted that doing so will “provide clarity and transparency\nabout the underlying assumptions or philosophical lenses. ” GPT-4\nwas more realistic in acknowledging the potential of such approach\nbut noted limitations and challenges. Two limitations that we found\nparticularly intriguing and agree with were: “implicit ontologies”\nand “general audience understanding”. The former referring to\nthe ontologies often being implicit and difficult to extract, and the\nlatter focusing on the complexities added in acknowledging the\nontologies underlying the responses for a general audience.\n11. What is a human? While the four LLMs provided differently\nformatted answers, all four implicitly assumed the human to be a\nbiological individual. However, Bard acknowledged the complexity\nof defining the notion of the human and how there is not a single\nagreed upon definition. Nonetheless, the examples it provided all\ncarried the underlying assumption of the human as a biological\nindividual.\n12. What defines a human? Copilot repeated its previous an-\nswer in response to this question. Bard and GPT-4 provided similar\nanswers to the previous question, emphasizing different aspects\nof being a human including the cognitive, biological, and social\ndimensions. GPT-3.5 focused on aspects that define the human as a\nbiological individual such as bipedal locomotion, tool creation and\nuse, and advanced cognitive abilities.\n13. In what ontology is this view of the human valid in?\nCopilot said that the definition is valid in “Western ontology” but\nrepeated a prior answer it had given on non-Western ontological\narguments about existence of God which was completely unre-\nlated to this question. Bard named five Western philosophies such\nas humanist and existential. GPT-4 took a similar approach but\nmainly that an integrative and pluralistic ontology is the promi-\nnent one. GPT-3.5 listed a number of Western philosophies such as\nmaterialism and existentialism.\n14. What ontologies refuse your definition of human? In-\nterestingly here, Copilot and GPT-3.5 mentioned Indigenous and\nother non-Western ontologies. Bard however listed a number of\nWestern ontologies such as transhumanism, and GPT-4 focused\non ontologies aligned with singular or reductionist explanations\nsuch as essentialism. Both Bard and GPT-4 had to be probed fur-\nther on non-Western ontologies to acknowledge the individualistic\napproaches of their prior answers and interrogate the previous\nresponse in the context of non-western ontologies.\nReceived 12 September 2024; revised 10 December 2024; accepted 16 January\n2025",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7451267838478088
    },
    {
      "name": "Tree (set theory)",
      "score": 0.5426519513130188
    },
    {
      "name": "Data science",
      "score": 0.3900977671146393
    },
    {
      "name": "Natural language processing",
      "score": 0.3702823519706726
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ]
}