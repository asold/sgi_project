{
  "title": "Spatiotemporal Transformer for Video-based Person Re-identification",
  "url": "https://openalex.org/W3141972123",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1709994932",
      "name": "Zhang, Tianyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226358518",
      "name": "Wei, Longhui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746615068",
      "name": "Xie, Lingxi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287465016",
      "name": "Zhuang, Zijie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352252563",
      "name": "Zhang Yongfei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1967255241",
      "name": "Li Bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114747614",
      "name": "Tian Qi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3045296664",
    "https://openalex.org/W3168915470",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963736028",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W2799185441",
    "https://openalex.org/W3035029089",
    "https://openalex.org/W2168356304",
    "https://openalex.org/W2156303437",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2953003997",
    "https://openalex.org/W3169884992",
    "https://openalex.org/W3099899804",
    "https://openalex.org/W2598634450",
    "https://openalex.org/W2463071499",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2990928867",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3034417718",
    "https://openalex.org/W1932380673",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3100506510",
    "https://openalex.org/W3096285474",
    "https://openalex.org/W2963438548",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W2622829582",
    "https://openalex.org/W2519803806",
    "https://openalex.org/W2963322158",
    "https://openalex.org/W3021892279",
    "https://openalex.org/W3035486808",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3036967712",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2755066373",
    "https://openalex.org/W3035750285",
    "https://openalex.org/W2520433280",
    "https://openalex.org/W2984145721",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Recently, the Transformer module has been transplanted from natural language processing to computer vision. This paper applies the Transformer to video-based person re-identification, where the key issue is to extract the discriminative information from a tracklet. We show that, despite the strong learning ability, the vanilla Transformer suffers from an increased risk of over-fitting, arguably due to a large number of attention parameters and insufficient training data. To solve this problem, we propose a novel pipeline where the model is pre-trained on a set of synthesized video data and then transferred to the downstream domains with the perception-constrained Spatiotemporal Transformer (STT) module and Global Transformer (GT) module. The derived algorithm achieves significant accuracy gain on three popular video-based person re-identification benchmarks, MARS, DukeMTMC-VideoReID, and LS-VID, especially when the training and testing data are from different domains. More importantly, our research sheds light on the application of the Transformer on highly-structured visual data.",
  "full_text": "Spatiotemporal Transformer for Video-based Person Re-identiÔ¨Åcation\nTianyu Zhang,1 Longhui Wei,5 Lingxi Xie,4 Zijie Zhuang,4 Yongfei Zhang,1,2,3 Bo Li,1,2,3 Qi Tian 6\n1Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University,\n2State Key Laboratory of Virtual Reality Technology and Systems, Beihang University,\n3Pengcheng Laboratory, 4Tsinghua University, 5University of Science and Technology of China, 6Xidian University\n{tianyu1949, weilh2568, 198808xc, jayzhuang42, wywqtian}@gmail.com, {yfzhang, boli}@buaa.edu.cn\nAbstract\nRecently, the Transformer module has been transplanted\nfrom natural language processing to computer vision. This\npaper applies the Transformer to video-based person re-\nidentiÔ¨Åcation, where the key issue is to extract the discrimi-\nnative information from a tracklet. We show that, despite\nthe strong learning ability, the vanilla Transformer suf-\nfers from an increased risk of over-Ô¨Åtting, arguably due\nto a large number of attention parameters and insufÔ¨Å-\ncient training data. To solve this problem, we propose a\nnovel pipeline where the model is pre-trained on a set of\nsynthesized video data and then transferred to the down-\nstream domains with the perception-constrained Spatiotem-\nporal Transformer (STT) module and Global Transformer\n(GT) module. The derived algorithm achieves signiÔ¨Åcant\naccuracy gain on three popular video-based person re-\nidentiÔ¨Åcation benchmarks, MARS, DukeMTMC-VideoReID,\nand LS-VID, especially when the training and testing data\nare from different domains. More importantly, our research\nsheds light on the application of the Transformer on highly-\nstructured visual data.\n1. Introduction\nWith increasing concerns on public security, person\nre-identiÔ¨Åcation (ReID), which aims to re-identify certain\npedestrians in a disjoint camera network, has drawn more\nand more attention from researchers. In past decades,\nimage-based ReID tasks, which rely on stationary pedes-\ntrian appearance from isolated and noncontinuous frames,\nhave been studied widely, and lots of novel works [55, 46,\n37, 38, 19, 20] have been proposed.\nHowever, image-based ReID tasks neglect several vi-\ntal clues. Compared to isolated frames, continuous frames\nfrom surveillance videos provide rich spatiotemporal infor-\nmation that reveals the causality between motions/viewing-\nangle-changes and identity-appearance-variations. Aggre-\ngating such spatiotemporal information properly into ReID\n(a) Spatial attention. \n(b) Temporal attention. \n (c) Biased attention. \n0.40 0.10 0.500.30 0.65 0.05\nFigure 1. Illustrations of the strengths and weaknesses of the\nTransformer. (a) Spatial attention is suitable for misaligned bound-\ning boxes or occluded persons. (b) Temporal attention is good at\nexcluding misdetection. (c) The vanilla Transformer learns biased\nattention. Minimal regions of the images are involved in the atten-\ntion map.\ntasks beneÔ¨Åts deep ReID models in extracting discrimina-\ntive and stable features against the misalignment of both\nbody parts and pedestrian locations inside each frame.\nMeanwhile, the redundancy of pedestrian appearance in\ncontinuous frames helps to overcome temporary occlusion\nnaturally. Thus, video-based ReID tasks, more precisely\ntracklet-based ReID tasks, have recently become more at-\ntractive for the ReID community [43, 29, 39, 11].\nThe key for video-based ReID tasks is to efÔ¨Åciently ag-\ngregate the spatial and temporal knowledge provided by\ntracklets into one representation learning framework. The\nTransformer [35] originated from natural language process-\ning (NLP) tasks sheds light on long sequence modeling. En-\ncouraged by its success in NLP tasks [8, 1], there are sev-\neral attempts to utilize the Transformer in computer vision\ntasks [3, 44]. Most of them focus on static and noncon-\ntinuous images. Images are manually cropped as patches,\nand Transformers are used to model the relations of these\npatches. BeneÔ¨Åted from a strong capability of ‚Äúattention\nand then aggregation‚Äù, Transformer-based networks learn\ndiscriminative representations from all image patches and\nthus achieve notable success.\narXiv:2103.16469v1  [cs.CV]  30 Mar 2021\nSuch a capability meets the demands of video-based\nReID tasks perfectly, where the learning procedure is re-\nquired in not only the spatial dimension of each frame, e.g.,\nrelations between human body parts but also the tempo-\nral dimension across continuous frames, e.g., appearance\nvariations through time. Thus, we propose a two-stage\nSpatiotemporal Transformer (STT) module, where a Spa-\ntial Transformer handles the information in image patches\nand another subsequent Temporal Transformer focuses on\nthe video sequence. As shown in Fig. 1(a) and Fig. 1(b),\nthe Spatial Transformer is capable of attending to human re-\ngions from various backgrounds, while the Temporal Trans-\nformer excludes ‚Äúnoisy‚Äù frames in the given tracklet. How-\never, such a vanilla assembly of two Transformer mod-\nules suffers from the over-Ô¨Åtting risk due to the insufÔ¨Å-\ncient training data in video-based ReID tasks. Fig. 1(c)\ndemonstrates a typical failure case. Although the Trans-\nformer surpasses convolutional neural networks (CNN) in\nexcluding background areas, it also tends to over-focusing\non local salient body parts while ignoring other common but\nuseful body parts information. Without any restrictions on\nlearning generic pedestrian appearance knowledge, the per-\nformance of the vanilla Transformer on video-based ReID\ntasks is much worse than previous CNN-based methods,\ne.g., 11.8% rank-1 accuracy drop on MARS dataset.\nTo address the over-Ô¨Åtting issue mentioned above, we\nfurther propose the constrained attention learning scheme\nupon the STT module, where multiple prior constraints are\napplied on the Spatial Transformer and Temporal Trans-\nformer, respectively. Meanwhile, we introduce a global\nattention learning branch as the supplement to exploit the\nrelationships between patches from different frames. Fi-\nnally, since the insufÔ¨Åcient training data hinders us from\nfully validating the Transformer in video ReID, we resort\nto the synthesized data and propose a novel pre-training\npipeline that relieves the over-Ô¨Åtting problem. Extensive\nexperiments demonstrate that, with our proposed pipeline,\non three popular benchmarks, i.e., MARS, DukeMTMC-\nVideoReID, and LS-VID, our STT outperforms both CNN\nbaselines and the vanilla Transformer by a large margin.\nOur contributions are summarized as follows:\n‚Ä¢ We propose a simple yet effective framework with\nthe Spatiotemporal Transformer for video-based per-\nson ReID. To the best of our knowledge, it is the Ô¨Årst to\nevaluate the effectiveness of the Transformer in video-\nbased ReID tasks, which paves a new way for further\nstudy.\n‚Ä¢ To reduce the over-Ô¨Åtting risk caused by the Trans-\nformer and further enhance the diversity of learned\nrepresentation, we propose a constrained attention\nlearning scheme upon the STT module and an extra\nGlobal Transformer module as a supplement. More-\nover, the synthesized video data are generated for pre-\ntraining.\n‚Ä¢ Extensive experiments on two large-scale video-based\nReID datasets have validated the effectiveness of the\nproposed approaches. Compared with the previ-\nous state-of-the-art methods with pure CNN architec-\ntures, our Transformer-based approach signiÔ¨Åcantly\nimproves the Ô¨Ånal performance in multiple datasets.\n2. Related Work\n2.1. Person Re-identiÔ¨Åcation\nExisting image-based ReID methods mainly focus on\ndiscriminative feature extraction from images. Human body\nparts are essential for image-based ReID. Many study how\nto learn feature embeddings from detected body parts [38,\n30], or Ô¨Åxed image areas [36, 33]. Some works learn the re-\nlationship between body parts. For example, Su et al. [32]\nalign image patches that contain the same body part; Xia et\nal. [2] utilize Non-local module to explore the importance\nof different parts; He et al. [16] adopt the Vision Trans-\nformer [9] for image-based ReID feature extraction. These\nmethods show excellent performance on public datasets,\nsuch as Market-1501 [50] and MSMT17 [37]. However,\nthe temporal information from video tracklets also pro-\nvides helpful information to re-identify a person. There-\nfore, researchers pay more and more attention to video-\nbased ReID methods recently. Early works introduce the\noptical Ô¨Çow and design two-stream networks to process op-\ntical Ô¨Çow and RGB frames, respectively. Due to the com-\nputational complexity of the optical Ô¨Çow, many works turn\nto more direct ways of modeling image sequences, such as\nRNN [27, 54, 41] and LSTM [5, 42]. Graph neural net-\nworks are also effective to mine the relationship of con-\ntinuous frames [29, 43]. Although these methods achieve\npromising performance, the extra heavy modules mentioned\nabove are time-consuming. Compared to these methods,\nthe Transformer enjoys the beneÔ¨Åts of time-efÔ¨Åciency and\nis more suitable for video-based ReID tasks.\n2.2. Transformer for Vision Tasks\nTransformer [35] is a self-attention-based architecture\nproposed for modelling sequence data. In natural language\nprocessing (NLP), it has become the basic module for state-\nof-the-art methods [8, 1]. Attention mechanism has also\nbeen studied in computer vision and plays an important\nrole in many computer vision tasks. More recently, re-\nsearchers apply the Transformer directly on image patches,\nlike Vit [9] and DeiT [34]. With large-scale pre-training [9],\nor distillation techniques [34], these methods achieve com-\npetitive results on image classiÔ¨Åcation tasks. The appli-\ncation of Transformer modules on video tasks has also\nCNN\nTracklet Feature maps\nSpatial \nTransformer\nTemporal \nTransformer\nPatch tokens\ncls token\ncls token\ncls token\nGlobal Transformer\nSpatial \nPart Loss\nSpatial \nClassification Loss\nTemporal \nAttention Loss\nTemporal \nTriplet Loss\nTemporal \nClassification Loss\nGlobal \nClassification Loss\n(a) Constrained attention learning.\n(b) Global attention learning.\n(c)  Synthesized video pre-training.\nSpatiotemporal Transformer\nFigure 2. The overall framework with the Spatiotemporal Transformer. (a) The constrained attention learning is adopted on the Spatial\nTransformer and Temporal Transformer. (b) A global attention learning branch is proposed to supplement STT. (c) The synthesized video\npre-training plays an important role in avoiding over-Ô¨Åtting.\narisen. For example, Gavrilyuket al. [12] propose the actor-\ntransformer for activity recognition. To better encode the\npoint cloud for the 3D video object detection task, Yin et\nal. [45] propose a Spatial Transformer to distinguish back-\nground and a Temporal Transformer to integrate motion in-\nformation. In this paper, we propose a hybrid architecture\nof CNN and Transformer encoders for extracting features\nfrom each frame and aggregating appearance information\nfrom the whole tracklet. To the best of our knowledge, this\nis the Ô¨Årst work that applies the Transformer to video-based\nperson ReID.\n3. Methods\n3.1. Formulation\nGiven an annotated video dataset S =\n{(T1,y1),(T2,y2),..., (TN,yN)}, where each Ti denote\na tracklet, andyi is the ground truth label of the correspond-\ning identity, the goal of video-based ReID tasks is to learn\na feature embedding function f(Œ∏; Ti) that maps tracklets\ninto a feature space X= {xi|xi = f(Œ∏; Ti),1 ‚â§i‚â§N},\nwhere tracklets of the same identity are closer than that of\ndifferent identity. To achieve this goal, researchers propose\ntwo types of objective functions, i.e., classiÔ¨Åcation loss\nand distance metric learning loss. The classiÔ¨Åcation loss\nclassiÔ¨Åes features xi of the same person into the same\ncategory, which can be implemented by the cross entropy\nloss:\nLxent =\nN‚àë\ni=1\n‚àílog( exp(h(xi)c)‚àë\nj exp(h(xi)j)), (1)\nwhere h is a classiÔ¨Åer, consisting of a linear layer and soft-\nmax layer that maps x into a one-hot classiÔ¨Åcation vector.\nFor distance metric learning losses, the triplet loss with on-\nline hard sample mining [17] is a popular choice. For each\nmini-batch, it demands each positive sample pair closer than\nall negative samples by at least a margin:\nLtrip =\nC‚àë\ni=1\n[m+dist(xi,p(xi))‚àídist(xi,n(xi))]+, (2)\nwhere [z]+ = max(z,0), mdenotes the margin. p(xi) is\nthe farthest sample belonging to the same person as xi, and\nn(xi) is another identity‚Äôs image that is the nearest toxi.\nIn video-based ReID tasks, each tracklet consists of a\nseries of images, which are usually generated by tracking\ntechniques, i.e., Ti = {I(1)\ni ,I(2)\ni ,..., I(n)\ni }. Compared to\nimage-based ReID, video-based ReID tasks introduce a new\ntemporal dimension alongside existing spatial dimensions.\nThus, the major challenges in video-based ReID tasks are\nusually two-fold. First, a good feature embedding for each\nimage is required to encode appearance characteristics and\nspatial information. Second, which is also the main dif-\nference compared to image-based ReID tasks, video-based\nReID tasks demand an ingenious module for modeling tem-\nporal relations among all frames in each tracklet.\nHowever, modeling temporal relations is extremely chal-\nlenging in real-world applications, especially for a non-\nideal, unconstrained surveillance video. Both the severe\nmismatch of human body parts and foreground position\nshifts inside each frame might lead to signiÔ¨Åcant perfor-\nmance decreases. To tackle these issues, we disassemble the\nmismatch problem into two categories: spatial mismatch\nin the image-level and temporal mismatch in the tracklet-\nlevel, and attempts to adopt Transformer-based methods to\naddress the above challenges accordingly. Details of our\nmethod are presented in the next section.\n3.2. Overall Framework\nBeneÔ¨Åted from the strong capability of ‚Äúattention and ag-\ngregation‚Äù, we can directly utilize Transformers for video-\nbased ReID tasks, extracting and aggregating the useful hu-\nman information spatially and temporally while neglecting\ndisturbances such as occlusions and background areas.\nThe overall of our proposed framework is shown in\nFig. 2. We Ô¨Årst utilize a truncated convolutional neural\nnetwork, e.g., a ResNet-50 backbone with the Ô¨Årst three\nblocks, as the preliminary feature encoder. For each frame,\nthe CNN backbone encodes it into a 3-dimensional feature\nmap. Next, each feature map is split into small patches. We\nregard each patch as a ‚Äútoken‚Äù and feed tokens from the\nsame image into our Spatial Transformer (ST) module. In\nthis way, the ST module learns the spatial relations among\nall given tokens and aggregates discriminative spatial infor-\nmation as well as appearance features from them. Simul-\ntaneously, following the practice in [9, 34], we add another\ntrainable classiÔ¨Åcation token for fusing the spatial informa-\ntion of each input image. This token fuses the information\nof all patch-based tokens. Since it further encodes the spa-\ntial relation between tokens, we name it the spatial token.\nFor each frame, this spatial token is regarded as the Ô¨Ånal\noutput of the ST module. After the ST module encodes each\nframe, the Temporal Transformer (TT) module collects all\nspatial tokens from the same tracklet and aggregates them\ninto a temporal token. The Ô¨Ånal temporal token embeds\nthe discriminative identity information spatially and tem-\nporally. It is used as the Ô¨Ånal representation for the entire\ntracklet.\nThe vanilla two-stage Transformer mentioned above pro-\ncesses both spatial and temporal information at the same\ntime, making it conceptually suitable for video-based ReID\ntasks. However, when using it in video-based ReID tasks di-\nrectly, it suffers from severe over-Ô¨Åtting. As shown in Tab. 2\nand Tab. 1, compared to the CNN baseline, this framework\nreports much worse performance, even though its prelim-\ninary feature encoder is the same as the Ô¨Årst three blocks\nof the CNN baseline. We attribute this performance de-\ncrease to three reasons. First, the Transformer is heavily\nparameterized. Without any restrictions, it could easily fall\ninto the local minimal and generalize poorly on the testing\ndata. To address this issue, we propose a constrained atten-\ntion learning scheme especially to prevent the Transformer\nfrom over-focusing on local regions. Second, the two-stage\ndesign separates the spatial information and temporary in-\nformation to avoid the over-Ô¨Åtting. However, with this de-\nsign, patches from one image cannot communicate with any\nother patches of another image. Thus, we add a global atten-\ntion learning branch that associates patches across frames.\nThird, since the Transformer introduces much more param-\neters, existing video-based ReID datasets are insufÔ¨Åcient to\ntune these parameters fully. To tackle this problem, we in-\ntroduce pre-trained video-based ReID models with synthe-\nsized images to achieve better initialization. In the follow-\ning section, we introduce our design principles and the pro-\nposed pipeline in detail.\nSpatial \nTransformer\navg pool ‚Ñí!\"#_\"#%&_'()&\navg pool ‚Ñí!\"#_\"#%&_'()&\n... ‚Ñí!\"#_'()&\n‚Ñí!\"#_'()&\n‚Ñí!\"#_'()&\nFigure 3. Illustration of the spatial constraint. The tokens of\npatches are reshaped for horizontal average pooling.\n3.3. Constrained Attention Learning\nOwing to the two-stage architecture of the Spatiotem-\nporal Transformer, which contains the Spatial Transformer\n(ST) to process the patch tokens in image-level and the\nTemporal Transformer (TT) to handle the image tokens in\ntracklet-level, we set different constraints on ST and TT in-\ndividually to relieve the over-Ô¨Åtting problem.\n3.3.1 Spatial Constraint\nThis Spatial Transformer module of STT processes patch\ntokens in each image. To be more speciÔ¨Åc, each feature\nmap extracted by the CNN backbone is split into H √óW\npatches and then Ô¨Çattened into 1-dim tokens. Therefore,\nwith the extra classiÔ¨Åcation token (or called spatial token in\nthis paper), the ST module receivesH√óW+1 tokens in to-\ntal. Aiming to learn discriminative representation in spatial\ndimension, we add the cross entropy loss, i.e., Lspa xent,\non the spatial token. Therefore, the spatial token is forced\nto focus on the human information to classify the input. Be-\ncause of the small-scale data for video ReID, ST can eas-\nily focus on limited regions but ignore detailed cues ( e.g.,\nbackpack). To further avoid this, we add a spatial part cross\nentropy loss, i.e., Lspa part xent, to force each token to learn\nuseful recognition information as much as possible. The\nspatial part loss is motivated by the recent part-based image\nReID methods [38, 33]. SpeciÔ¨Åcally, as shown in Fig. 3,\nwe divide the H√óW tokens into P groups horizontally so\nthat each group has H/P √óW tokens. An average pooling\noperation is conducted within every group. Finally, a cross\nentropy classiÔ¨Åcation loss is calculated for every averaged\ntoken, and the sum is formulated as the spatial part cross\nentropy loss:\nLspa part xent = 1\nP\nP‚àë\np=1\nL(p)\nspa part xent, (3)\nwhere L(p)\nspa part xent represents the cross entropy loss for the\npth horizontal group. Obviously, this loss term leads the\nmodel to discover person-related clues in every horizontal\npart of the body, which makes sure each patch token learns\nenough human information.\nTemporal Transformer\nMulti-head \nAttention MLP\n‚Ñí!\"#_%!!&\n‚Ñí!\"#_'\"&!\n‚Ñí!\"#_!()*\n‚Ñí!\"#_!()*\n‚Ñí!\"#_!()*\nattention matrix\nùëé!,# ùëé!,$ ùëé!,% ùëé!,&\nFigure 4. Illustration of the temporal constraint. Normalization\nlayers and skip connections are omitted in this Ô¨Ågure for clear pre-\nsentation. The Ô¨Årst row of the attention matrix ( i.e., weights of\nclassiÔ¨Åcation tokens) is taken as the input of the temporal atten-\ntion loss.\nTherefore, the spatial constraint loss can be summarized\nas follows:\nLSpaC = Lspa part xent + Lspa xent. (4)\n3.3.2 Temporal Constraint\nAs shown in Fig. 4, the Temporal Transformer (TT) mod-\nule takes the spatial tokens from the Spatial Transformer as\ninput. One extra classiÔ¨Åcation token (or called the tempo-\nral token) is added as the Ô¨Ånal feature representation. In a\ntracklet sequence, the pedestrian may be occluded in some\nimages but occur again in other images. With the cross en-\ntropy loss Ltem xent that supervises the Ô¨Ånal output, we ex-\npect the Temporal Transformer to exclude frames of lousy\nquality and utilize more information from frames that con-\ntain complete human bodies. However, in the meantime, the\nrisk of over-Ô¨Åtting is raised because the learning algorithm\nis not aware of the targeted human body. For instance, an\nimage containing a unique distractor person is also quite\ndiscriminative for the model. Therefore, the Transformer\nmodule will attend to this image but ignore the targeted\nhuman. More examples can be seen in Fig. 6. To solve\nthis problem, we expect the exploited representation should\ncontain the common information of all frames in the tracklet\nand other speciÔ¨Åc information on each frame.\nTo achieve this, apart from the cross entropy loss that su-\npervises the Ô¨Ånal output, we add supervision for the tempo-\nral output tokens of each frame, namedLtem trip. As shown\nin Eq. 2, the triplet loss shrinks the distances of positive\npairs. Because frames within the same tracklet are assigned\nwith the same identity label, their distances are naturally re-\nduced so that the extracted information is shared by most\nof the frames. Besides, we also encourage the Temporal\nTransformer to focus on as many frames as possible to avoid\nbiased attention to speciÔ¨Åc frames. We add a temporal atten-\ntion loss on the attention weights of the Ô¨Ånal classiÔ¨Åcation\ntoken:\nLtem attn =\nN‚àë\ni=1\n[exp(\nL‚àë\nk=1\nai,k log(ai,k)) ‚àíŒ±]+, (5)\nwhere Lis the number of frames within a tracklet, ai,k is\nthe attention weight of thekth frame in the ith tracklet, and\nŒ± is a hyper-parameter that adjusts the upper bound. The\ntemporal attention loss increases the information entropy of\nthe attention weights in each tracklet. Also, it leaves much\nspace for the Transformer to decide which frame is more\ncritical with the parameter Œ±.\nFinally, the temporal constraint loss is as follows:\nLTemC = Ltem trip + Ltem attn. (6)\n3.4. Global Attention Learning\nAs described above, the Spatiotemporal Transformer\nlearns image-level attention and tracklet-level attention in\nsuccession. Therefore, the relationships between patches\nof different frames are ignored. To address this problem,\nwe further introduce the global attention learning branch in\nour framework. This part has a Global Transformer mod-\nule, which takes the feature map patches of all frames in a\ntracklet as the input. More speciÔ¨Åcally, supposing we have\nH √óW patches for every frame and L frames in a track-\nlet, we concatenate these tokens of every frame within the\nsame tracklet, resulting in an input, of which the length is\nH√óW √óL. These are directly fed into the Global Trans-\nformer (GT) with an extra classiÔ¨Åcation token. Then, a\ncross entropy loss Lglobal xent is adopted to supervise the\nlearning of GT. When the global attention learning branch\nis enabled, the Ô¨Ånal representation is generated by the con-\ncatenation of STT outputs and GT outputs.\n3.5. Synthesized Video Pre-training\nPrevious works [9, 34] show that the data scale is one of\nthe most important keys for the Transformer and conclude\nthat Transformers can work better than pure CNN networks\nwhen the training data is sufÔ¨Åcient. Limited by the difÔ¨Å-\nculty of annotations, it is nearly impossible to annotate a\nsuper large-scale video ReID dataset. To further relieve the\nover-Ô¨Åtting problem of the Transformer in the data aspect,\nwe resort to the synthesized data and pre-train our model on\nthis dataset. SpeciÔ¨Åcally, we adopt the UnrealPerson [47]\ntoolkit to generate videos in 3D virtual scenes. Four differ-\nent environments and 34 cameras are used in our synthe-\nsized data. We implement two extra modiÔ¨Åcations to Un-\nrealPerson. First, we add large disturbance when cutting\nthe bounding boxes. In real scenes, the bounding boxes for\nvideo-based ReID tasks are not ideally aligned. Therefore,\npersons in bounding boxes may not appear in the middle.\nThis helps the Transformer to concentrate on local patches\nModels Spatial ConstraintTemporal ConstraintGlobal AttentionPre-training MARS Duke LS-VID\nrank-1 mAP rank-1 mAP rank-1 mAP\nCNN baseline 84.1 78.8 50.6 48.3 16.6 11.2\nCNN+Transformer* 72.3 63.1 24.5 24.5 4.5 3.9\nCNN+Transformer ‚úì 87.1 83.5 60.5 56.3 16.2 11.7\nCNN+Transformer ‚úì ‚úì 87.2 84.2 61.1 57.4 18.2 13.1\nCNN+Transformer ‚úì ‚úì ‚úì 88.6 83.9 62.4 61.0 18.7 13.1\nCNN+Transformer ‚úì ‚úì ‚úì 87.3 85.0 65.0 62.1 21.0 14.6\nCNN+Transformer ‚úì ‚úì ‚úì ‚úì 88.7 86.3 69.2 66.2 24.3 17.2\nTable 1. Evaluation results when training on MARS. * denotes the vanilla version of the Transformer.\nModels Spatial ConstraintTemporal ConstraintGlobal AttentionPre-training MARS Duke LS-VID\nrank-1 mAP rank-1 mAP rank-1 mAP\nCNN baseline 34.1 18.8 91.2 88.0 20.8 13.7\nCNN+Transformer* 23.2 12.8 90.7 88.2 7.3 5.9\nCNN+Transformer ‚úì 45.0 28.2 96.2 96.0 32.0 22.0\nCNN+Transformer ‚úì ‚úì 46.4 28.9 96.6 96.0 32.9 22.6\nCNN+Transformer ‚úì ‚úì ‚úì 51.4 33.6 97.4 97.0 34.6 24.3\nCNN+Transformer ‚úì ‚úì ‚úì 48.6 30.8 96.7 96.6 34.7 24.6\nCNN+Transformer ‚úì ‚úì ‚úì ‚úì 56.5 39.1 97.6 97.4 41.3 29.4\nTable 2. Evaluation results when training on Duke. * denotes the vanilla version of the Transformer.\nModels Spatial ConstraintTemporal ConstraintGlobal AttentionPre-training MARS Duke LS-VID\nrank-1 mAP rank-1 mAP rank-1 mAP\nCNN baseline 36.1 20.3 50.6 43.2 58.5 44.6\nCNN+Transformer* 12.0 7.7 16.8 17.9 31.9 28.3\nCNN+Transformer ‚úì 50.0 32.7 74.2 71.3 78.8 68.0\nCNN+Transformer ‚úì ‚úì 52.7 35.4 77.8 74.9 80.7 70.5\nCNN+Transformer ‚úì ‚úì ‚úì 54.0 37.8 83.1 80.5 84.6 74.7\nCNN+Transformer ‚úì ‚úì ‚úì 53.9 36.8 79.6 76.0 82.4 73.0\nCNN+Transformer ‚úì ‚úì ‚úì ‚úì 59.6 42.7 86.0 83.5 87.5 78.0\nTable 3. Evaluation results when training on LS-VID. * denotes the vanilla version of the Transformer.\nwhere the body parts exist. Second, to mimic the tracklets\nby tracking algorithms, we also keep the severely occluded\nframes. Therefore, in a tracklet, the quality of images may\nvary largely. This increases the difÔ¨Åculty of the synthesized\ndata and encourages the model to Ô¨Ånd images of good qual-\nity in a tracklet. The synthesized dataset we adopt in the\nexperiments contains 90,673 tracklets of 2,808 identities.\n4. Experiments\n4.1. Datasets\nWe conduct experiments on two benchmark datasets,\nMARS [49], DukeMTMC-VideoReID [40] and LS-\nVID [19]. MARS consists of 1,261 pedestrians captured\nunder 6 different cameras on campus. The training set of\nMARS has 8,298 tracklets of 625 persons, and the testing\nset has 621 persons and 9,330 gallery images. There are\n1,840 valid query tracklets in total. In MARS, each tracklet\nhas 59 images on average. DukeMTMC-VideoReID (re-\nferred as Duke for short) has 2,196 tracklets of 702 pedes-\ntrians for training, and 2,336 tracklets of 1,110 persons for\ntesting. Each tracklet of Duke has 168 frames on average.\nLS-VID is constructed with videos from 15 cameras. The\ntraining set contains 2,831 tracklets of 842 identities, and\nthe testing set includes 11,333 tracklets of 2,730 identities,\namong which 3,504 tracklets are used as queries. Images\nand tracklets of MARS and LS-VID are generated automat-\nically by detection and tracking algorithms, i.e., DPM [10],\nGMMCP [6] and Faster-RCNN [28], while Duke is man-\nually labeled. In our experiments, we follow the standard\nevaluation protocols [50] and report rank-1 accuracy and\nmean average precision (mAP).\n4.2. Implementation Details\nArchitecture. We adopt the Ô¨Årst three residual blocks of\nResNet-50 [15] that is pre-trained on ImageNet [7] as the\nCNN backbone. The CNN baseline model has the complete\nlayers of all 4 residual blocks of ResNet-50, while in the\nproposed CNN+Transformer architecture, the 4th residual\nblock is replaced by Transformer blocks. For our proposed\nSpatiotemporal Transformer, we follow the implementation\nof ViT [9]. The output feature maps of the CNN backbone\ngo through a convolutional layer and are Ô¨Çattened to patch\ntokens. The Spatial and Temporal Transformers share the\nsame architecture design, with 1 layer and 6 heads. The\nGlobal Transformer has 2 layers and 6 heads. The embed-\nding dimension of all Transformers is set to 768. Positional\nembeddings are only used in the Spatial Transformer. By\ndiscarding positional embeddings in the Temporal Trans-\nformer and Global Transformer, our framework is able to\nprocess tracklets of any length.\nInput. Inspired by DeiT [34], we use more extreme data\naugmentation operations in our experiments, including ran-\ndom erasing [51], random patch [53, 52], random crop, ran-\ndom Ô¨Çip and AutoAugment [4]. The input size of images is\nset as 256 √ó128. We sample 8 images with the restricted\nsampling strategy [22] from each tracklet when training.\nFor evaluation, we sample 32 images from each tracklet.\nA balanced sampling strategy [17] is adopted to form the\nmini-batches. SpeciÔ¨Åcally, 15 identities are selected Ô¨Årst,\nand for each identity, 4 tracklets are chosen. Thus the mini-\nbatch size is 60. The size of the output feature map in the\nCNN backbone is 1024 √ó16 √ó8, among which 1024 is the\nchannel number. The feature map is further embedded into\n8 √ó4 patches by a convolution layer.\nTraining Details. For all cross entropy losses, we add\nlabel smoothing regularization with œµ= 0.1. The part num-\nber of the spatial part loss is 4. The margin of the tempo-\nral triplet loss is set as 0.3. In the temporal attention loss,\nthe parameter Œ±is set to 0.15 empirically. Besides loading\nthe pre-trained weights for the CNN backbone, we also add\nan extra cross entropy loss for it to stabilize training. In\nall our experiments, we adopt AdamW [26, 18] optimizer\nwith the weight decay of 5e‚àí4. The learning rate is set to\n3.5e‚àí4 initially and multiply by a factor 0.1 at the 80th\nepoch and 160th epoch, respectively. The learning rate of\nall classiÔ¨Åers is set to a larger value, 1.75e‚àí3. During\nthe Ô¨Årst 200 iterations, the learning rate warm-up strategy is\nadopted. All of the models are trained for 200 epochs.\n4.3. Ablation Study\nIn the ablation study, we evaluate the effectiveness of\nour proposed framework and the extra constraints for Trans-\nformer modules. The results are shown in Tab. 1, Tab. 2\nand Tab. 3. The direct transfer evaluation is also conducted\namong different datasets to validate the generalization abil-\nity of the trained models.\nVanilla Transformer. Compared to the pure CNN\nbaseline model, the Transformer network without any ad-\nditional constraints suffers performance drop, especially\nwhen trained on MARS and LS-VID. The direct transfer\nperformance decays even more than self-domain evaluation,\nindicating serious over-Ô¨Åtting exists in the trained model.\nThe reason has been illustrated in Fig. 1(c), i.e., an un-\nconstrained Transformer network produces biased attention\nweights.\nConstrained Attention Learning. When we add the\nspatial constraint losses in the model, the performance sur-\npasses the pure CNN baseline by a large margin, especially\non direct transfer results. For example, when we evaluate\nDuke with the model trained on MARS, the rank-1 accuracy\n+ Spatial\nconstraint\n+Spatial\nConstraint\n+ Spatial\nconstraint\nVa nilla\nTrans.Images +Spatial\nConstraint\nVa nilla\nTrans.Images\nFigure 5. Spatial attention maps after adding spatial constraint\nlosses.\nraises from 50.6% to 60.5%. The temporal constraint also\nimproves all performances slightly. These results demon-\nstrate that our proposed loss terms are effective in prevent-\ning over-Ô¨Åtting.\nGlobal Attention Learning. The global attention learn-\ning empowers each patch token to associate with patches\nfrom other frames. Experiment results show that this global\nscope is an important complementary component for the\nSpatiotemporal Transformer. For instance, in Tab. 1, when\ntrained on MARS and evaluated on Duke, the model with\nglobal attention learning improves the rank-1 accuracy by\n3.9% compared to the model without global attention.\nSynthesized Data Pre-training. The pre-training\nweights provide a good initialization for the Transformer\nmodules, avoiding over-Ô¨Åtting from early training stages.\nNote that we have the pre-trained weights Ô¨Åne-tuned on\nthe downstream datasets for 200 epochs, the same as train-\ning without synthesized data pre-training. Therefore, there\nshould leave very little information about the synthesized\ndataset in the Ô¨Ånal model weights. Nevertheless, the direct\ntransfer performance is boosted signiÔ¨Åcantly on all three\ndatasets, indicating that the Transformer beneÔ¨Åts from bet-\nter pre-trained weights and reaches a more optimal conver-\ngence in the end.\n4.4. Visualization Analysis\n4.4.1 Comparison on Spatial Attention\nAs shown in Fig. 5, we draw the attention maps with the\nvanilla Transformer models and compare them with at-\ntention maps after adding the spatial constraints. The at-\ntention weights are extracted from the attention matrix in\n+ Constrained Attention Learning\n + Constrained Attention Learning\n0.12 0.15 0.15 0.13 0.15 0.12 0.12 0.05 0.02 0.14 0.07 0.16 0.17 0.15 0.19 0.09 \n0.12 0.16 0.09 0.12 0.13 0.25 0.09 0.03 0.07 0.12 0.09 0.14 0.09 0.29 0.10 0.09\nFigure 6. Temporal attention weights after using constrained attention learning.\nVa n i l l aOurs Va n i l l aOurs\nFigure 7. The attention maps of two images from the right Ô¨Ågure of\nFig. 6. These two images are notable for the vanilla Transformer\nin the tracklet because of distractors.\nthe Spatial Transformer. The Ô¨Årst row of the matrix, i.e.,\nclassiÔ¨Åcation-token-related weights, is resized to reÔ¨Çect the\nimportance of local patches. The given examples are all\nsuccessful detection bounding boxes, and the human bodies\nare occupying most of the images. Despite this, the vanilla\nTransformer learns biased attention and discards many use-\nful regions, indicating it suffers from serious over-Ô¨Åtting. In\ncomparison, with our proposed constrained spatial attention\nlearning, we set up a more challenging objective for models\nto learn. We encourage every image and every horizontal\npart can be classiÔ¨Åed correctly, so more regions on human\nbodies are covered by the attention maps.\n4.4.2 Comparison on Temporal Attention\nIn Fig. 6, the left Ô¨Ågure shows that in a tracklet where most\nof the images are of good quality, the vanilla Transformer\nwithout extra constraints tends to pay attention to very few\nframes. For example, only the 6th frame has a high re-\nsponse in the attention matrix of the Temporal Transformer.\nOur proposed temporal attention loss makes the weights\nmore evenly distributed across all frames via increasing the\ninformation entropy of attention weights. We also set up\nan explicit upper bound of information entropy. Therefore,\nthere still exists space for the Transformer to distinguish less\nimportant frames, like the 8th frame in the left example of\nFig. 6.\nAnother beneÔ¨Åt the temporal constraint brings is that the\nÔ¨Ånal representation of the tracklet should be shared by most\nframes. In the right Ô¨Ågure of Fig. 6, the vanilla Transformer\nMethods MARS Duke\nrank-1 mAP rank-1 mAP\nTKP [14] 84.0 73.3 94.0 91.7\nSTA [11] 86.2 81.2 96.0 95.0\nGLTR [19] 87.0 78.5 96.3 93.7\nMG-RAFA [48] 88.8 85.9 - -\nSTE-NV AN [23] 88.9 81.2 95.2 93.5\nAGRL [39] 89.5 81.9 97.0 95.4\nNL-AP3D [13] 90.7 85.6 97.2 96.1\nSTGCN [29] 90.0 83.7 97.3 95.7\nGRL [24] 91.0 84.8 - -\nOurs 88.7 86.3 97.6 97.4\nTable 4. Comparison with recent works on MARS and Duke\ndatasets.\nMethods rank-1 rank-5 rank-10 mAP\nTwo-stream [31] 48.2 68.7 75.1 32.1\nLSTM [42] 52.1 72.6 78.9 35.9\nSTMP [25] 56.8 76.2 82.0 39.1\nM3D [21] 57.7 76.1 83.4 40.1\nGLTR [19] 63.1 77.2 83.8 44.3\nOurs 87.5 95.0 96.5 78.0\nTable 5. Comparison with recent works on LS-VID.\ngives high attention weight to the 6th frame. We observe\nthe spatial attention map of this frame (as shown in Fig. 7)\nand Ô¨Ånd that the person in the red box is considered as the\nobject of the tracklet. From the whole tracklet, we know that\nthe red box contains a distractor and should be ignored by\nthe model. With the temporal constraint, we require the out-\nput tokens of every frame within a tracklet to be as close as\npossible. In this way, distractors are paid less attention be-\ncause they only appear in some frames. From Fig. 7, we see\nthat with constrained attention learning, the model mainly\nextracts features from the target person.\n4.5. Comparison with Other Methods\nIn Tab. 4 and Tab. 5, we compare our method with re-\ncent video-based person ReID methods on the three bench-\nmark datasets. On both Duke and LS-VID, our proposed\nframework outperforms state-of-the-art methods in rank-1\naccuracy and mAP. For example, we surpass the graph-\nbased method STGCN [29] by 1.7% in mAP on Duke. This\nimprovement is signiÔ¨Åcant, considering the baseline accu-\nracy of Duke is rather high. Our method also outperforms\nGLTR [19] by 24.4% in rank-1 accuracy on LS-VID. On\nMARS dataset, our method achieves state-of-the-art perfor-\nmance in mAP. From other methods, we see that there is no\nabsolutely positive correlation between rank-1 accuracy and\nmAP. For instance, GRL [24] achieves the best rank-1 accu-\nracy, but the mAP of GRL is signiÔ¨Åcantly lower than MG-\nRAFA [48]. In MARS, of which detection boxes and track-\ning sequences are automatically generated by algorithms,\nthe quality of images and tracklets may vary largely. There-\nfore, different algorithms may emphasize different aspects.\nRank-1 accuracy reÔ¨Çects the ability to Ô¨Ånd the most conÔ¨Å-\ndent positive sample, while mAP is a more comprehensive\nindicator, as it considers the ranking positions of all positive\nsamples. Therefore, the signiÔ¨Åcant improvement in mAP on\nMARS validates the effectiveness of our proposed method.\n5. Conclusions\nThis paper presents a simple yet effective Transformer-\nbased representation learning framework for video-based\nperson ReID tasks. In this framework, the constrained atten-\ntion learning scheme and global attention learning branch\nare proposed to exploit spatial and temporal knowledge\nfrom ReID videos. Furthermore, we introduce synthesized\ndata pre-training for better initializing our framework and\nreducing the over-Ô¨Åtting risk. Extensive experiments have\nshown the effectiveness of our proposed approaches in this\npaper. Importantly, to our best knowledge, it is the Ô¨Årst\nwork to evaluate the ability of the Transformer in video-\nbased ReID tasks, which paves a new way for the applica-\ntion of the Transformer on more video-based tasks.\nReferences\n[1] T. Brown, B. Mann, Nick Ryder, Melanie Subbiah, Jared\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-V oss, G. Kr ¬®uger, T. Henighan, R. Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, E. Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, J. Clark, Christopher\nBerner, Sam McCandlish, A. Radford, Ilya Sutskever, and\nDario Amodei. Language models are few-shot learners. In\nNeurIPS, 2020. 1, 2\n[2] B. Bryan, Y . Gong, Y . Zhang, and C. Poellabauer.\nSecond-order non-local attention networks for person re-\nidentiÔ¨Åcation. In ICCV, 2019. 2\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. ArXiv,\nabs/2005.12872, 2020. 1\n[4] E. Cubuk, Barret Zoph, Dandelion Man ¬¥e, Vijay Vasudevan,\nand Quoc V . Le. Autoaugment: Learning augmentation\nstrategies from data. In CVPR, 2019. 7\n[5] J. Dai, Pingping Zhang, D. Wang, H. Lu, and H. Wang.\nVideo person re-identiÔ¨Åcation by temporal residual learning.\nIEEE TIP, 28:1366‚Äì1377, 2019. 2\n[6] Afshin Dehghan, S. M. Assari, and M. Shah. Gmmcp\ntracker: Globally optimal generalized maximum multi clique\nproblem for multiple object tracking. In CVPR, 2015. 6\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 6\n[8] J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n1, 2\n[9] A. Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, M. De-\nhghani, Matthias Minderer, Georg Heigold, S. Gelly, Jakob\nUszkoreit, and N. Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale. In ICLR, 2021.\n2, 4, 5, 6\n[10] Pedro F. Felzenszwalb, Ross B. Girshick, David A.\nMcAllester, and D. Ramanan. Object detection with discrim-\ninatively trained part based models. IEEE TPAMI, 32:1627‚Äì\n1645, 2009. 6\n[11] Y . Fu, Xiaoyang Wang, Yunchao Wei, and Thomas Huang.\nSta: Spatial-temporal attention for large-scale video-based\nperson re-identiÔ¨Åcation. In AAAI, 2019. 1, 8\n[12] Kirill Gavrilyuk, R. Sanford, M. Javan, and Cees G. M.\nSnoek. Actor-transformers for group activity recognition. In\nCVPR, 2020. 3\n[13] Xinqian Gu, H. Chang, Bingpeng Ma, Hongkai Zhang, and\nX. Chen. Appearance-preserving 3d convolution for video-\nbased person re-identiÔ¨Åcation. In ECCV, 2020. 8\n[14] Xinqian Gu, Bingpeng Ma, H. Chang, S. Shan, and X. Chen.\nTemporal knowledge propagation for image-to-video person\nre-identiÔ¨Åcation. In ICCV, 2019. 8\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 6\n[16] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. Transreid: Transformer-based object re-\nidentiÔ¨Åcation, 2021. 2\n[17] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-\nfense of the triplet loss for person re-identiÔ¨Åcation. arXiv\npreprint arXiv:1703.07737, 2017. 3, 7\n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014. 7\n[19] Jianing Li, J. Wang, Q. Tian, Wen Gao, and S. Zhang.\nGlobal-local temporal representations for video person re-\nidentiÔ¨Åcation. In ICCV, 2019. 1, 6, 8, 9\n[20] Jianing Li and Shiliang Zhang. Joint visual and tempo-\nral consistency for unsupervised domain adaptive person re-\nidentiÔ¨Åcation. In ECCV, 2020. 1\n[21] Jianing Li, Shiliang Zhang, and Tiejun Huang. Multi-\nscale 3d convolution network for video based person re-\nidentiÔ¨Åcation. In AAAI, 2019. 8\n[22] Shuang Li, Slawomir Bak, P. Carr, and Xiaogang Wang. Di-\nversity regularized spatiotemporal attention for video-based\nperson re-identiÔ¨Åcation. In CVPR, 2018. 7\n[23] C. Liu, Chih-Wei Wu, Yu-Chiang Frank Wang, and S. Chien.\nSpatially and temporally efÔ¨Åcient non-local attention net-\nwork for video-based person re-identiÔ¨Åcation. In BMVC,\n2019. 8\n[24] Xuehu Liu, Pingping Zhang, Chenyang Yu, Huchuan Lu,\nand Xiaoyun Yang. Watching you: Global-guided reciprocal\nlearning for video-based person re-identiÔ¨Åcation. In CVPR,\n2021. 8, 9\n[25] Yiheng Liu, Zhenxun Yuan, W. Zhou, and H. Li. Spatial\nand temporal mutual promotion for video-based person re-\nidentiÔ¨Åcation. In AAAI, 2019. 8\n[26] I. Loshchilov and F. Hutter. Decoupled weight decay regu-\nlarization. In ICLR, 2019. 7\n[27] N. McLaughlin, J. Rinc ¬¥on, and P. Miller. Recurrent convo-\nlutional network for video-based person re-identiÔ¨Åcation. In\nCVPR, 2016. 2\n[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-\nwards real-time object detection with region proposal net-\nworks. IEEE TPAMI, 39(6):1137‚Äì1149, 2017. 6\n[29] Jin rui Yang, W. Zheng, Q. Yang, Y . Chen, and Q. Tian.\nSpatial-temporal graph convolutional network for video-\nbased person re-identiÔ¨Åcation. In CVPR, 2020. 1, 2, 8, 9\n[30] M Saquib Sarfraz, Arne Schumann, Andreas Eberle, and\nRainer Stiefelhagen. A pose-sensitive embedding for per-\nson re-identiÔ¨Åcation with expanded cross neighborhood re-\nranking. In CVPR, 2018. 2\n[31] K. Simonyan and Andrew Zisserman. Two-stream convolu-\ntional networks for action recognition in videos. InNeurIPS,\n2014. 8\n[32] Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao,\nand Qi Tian. Pose-driven deep convolutional model for per-\nson re-identiÔ¨Åcation. In ICCV, 2017. 2\n[33] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\nWang. Beyond part models: Person retrieval with reÔ¨Åned\npart pooling (and a strong convolutional baseline). InECCV,\n2018. 2, 4\n[34] Hugo Touvron, M. Cord, M. Douze, Francisco Massa,\nAlexandre Sablayrolles, and H. J ¬¥egou. Training data-\nefÔ¨Åcient image transformers & distillation through attention.\nArXiv, abs/2012.12877, 2020. 2, 4, 5, 7\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, L. Kaiser, and Illia Polo-\nsukhin. Attention is all you need. In NeurIPS, 2017. 1, 2\n[36] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi\nZhou. Learning discriminative features with multiple granu-\nlarities for person re-identiÔ¨Åcation. In ACMMM, 2018. 2\n[37] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.\nPerson transfer gan to bridge domain gap for person re-\nidentiÔ¨Åcation. In CVPR, 2018. 1, 2\n[38] Longhui Wei, Shiliang Zhang, Hantao Yao, Wen Gao, and Qi\nTian. Glad: global-local-alignment descriptor for pedestrian\nretrieval. In ACMMM, 2017. 1, 2, 4\n[39] Yiming Wu, Omar El Farouk Bourahla, X. Li, F. Wu, and\nQ. Tian. Adaptive graph representation learning for video\nperson re-identiÔ¨Åcation. IEEE TIP, 29:8821‚Äì8830, 2020. 1,\n8\n[40] Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wanli Ouyang,\nand Yi Yang. Exploit the unknown gradually: One-shot\nvideo-based person re-identiÔ¨Åcation by stepwise learning. In\nCVPR, 2018. 6\n[41] Yichao Yan, B. Ni, Zhichao Song, Chao Ma, Yan Yan, and\nX. Yang. Person re-identiÔ¨Åcation via recurrent feature aggre-\ngation. In ECCV, 2016. 2\n[42] Yichao Yan, B. Ni, Zhichao Song, Chao Ma, Yan Yan, and\nX. Yang. Person re-identiÔ¨Åcation via recurrent feature aggre-\ngation. In ECCV, 2016. 2, 8\n[43] Yichao Yan, J. Qin, Jiaxin Chen, Li Liu, F. Zhu, Ying Tai,\nand Ling Shao. Learning multi-granular hypergraphs for\nvideo-based person re-identiÔ¨Åcation. In CVPR, 2020. 1, 2\n[44] Fuzhi Yang, Huan Yang, J. Fu, Hongtao Lu, and B.\nGuo. Learning texture transformer network for image super-\nresolution. In CVPR, 2020. 1\n[45] Junbo Yin, J. Shen, Chenye Guan, Dingfu Zhou, and\nRuigang Yang. Lidar-based online 3d video object detec-\ntion with graph-based message passing and spatiotemporal\ntransformer attention. In CVPR, 2020. 3\n[46] Tianyu Zhang, Lingxi Xie, Longhui Wei, Yongfei Zhang,\nBo Li, and Qi Tian. Single camera training for person re-\nidentiÔ¨Åcation. In AAAI, 2020. 1\n[47] Tianyu Zhang, Lingxi Xie, Longhui Wei, Zijie Zhuang,\nYongfei Zhang, Bo Li, and Qi Tian. Unrealperson: An adap-\ntive pipeline towards costless person re-identiÔ¨Åcation. In\nCVPR, 2021. 5\n[48] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, and Zhibo\nChen. Multi-granularity reference-aided attentive feature ag-\ngregation for video-based person re-identiÔ¨Åcation. In CVPR,\n2020. 8, 9\n[49] L. Zheng, Zhi Bie, Y . Sun, Jingdong Wang, Chi Su, S. Wang,\nand Q. Tian. Mars: A video benchmark for large-scale per-\nson re-identiÔ¨Åcation. In ECCV, 2016. 6\n[50] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-\ndong Wang, and Qi Tian. Scalable person re-identiÔ¨Åcation:\nA benchmark. In ICCV, 2015. 2, 6\n[51] Z. Zhong, L. Zheng, Guoliang Kang, Shaozi Li, and Y . Yang.\nRandom erasing data augmentation. In AAAI, 2020. 7\n[52] K. Zhou, Yongxin Yang, A. Cavallaro, and T. Xiang. Learn-\ning generalisable omni-scale representations for person re-\nidentiÔ¨Åcation. ArXiv, abs/1910.06827, 2019. 7\n[53] K. Zhou, Yongxin Yang, A. Cavallaro, and T. Xiang. Omni-\nscale feature learning for person re-identiÔ¨Åcation. In ICCV,\n2019. 7\n[54] Z. Zhou, Y . Huang, Wei Wang, Liang Wang, and T. Tan. See\nthe forest for the trees: Joint spatial and temporal recurrent\nneural networks for video-based person re-identiÔ¨Åcation. In\nCVPR, 2017. 2\n[55] Zijie Zhuang, Longhui Wei, Lingxi Xie, Tianyu Zhang,\nHengheng Zhang, Haozhe Wu, Haizhou Ai, and Qi Tian. Re-\nthinking the distribution gap of person re-identiÔ¨Åcation with\ncamera-based batch normalization. In ECCV, 2020. 1",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7708996534347534
    },
    {
      "name": "Transformer",
      "score": 0.7303595542907715
    },
    {
      "name": "Discriminative model",
      "score": 0.6419284343719482
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4964764714241028
    },
    {
      "name": "Training set",
      "score": 0.42641302943229675
    },
    {
      "name": "Machine learning",
      "score": 0.3971618413925171
    },
    {
      "name": "Computer vision",
      "score": 0.3485938310623169
    },
    {
      "name": "Engineering",
      "score": 0.10240098834037781
    },
    {
      "name": "Electrical engineering",
      "score": 0.08538180589675903
    },
    {
      "name": "Voltage",
      "score": 0.07946747541427612
    }
  ],
  "topic": "Computer science",
  "institutions": [],
  "cited_by": 30
}