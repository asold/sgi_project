{
    "title": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
    "url": "https://openalex.org/W3193521535",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4286961604",
            "name": "Chung, Yu-An",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102355683",
            "name": "Zhang, Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2018778232",
            "name": "Han Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223153740",
            "name": "Chiu, Chung-Cheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223153741",
            "name": "Qin, James",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2993384157",
            "name": "Pang, Ruoming",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1993644842",
            "name": "Wu Yonghui",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3015995734",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2995181338",
        "https://openalex.org/W3035202887",
        "https://openalex.org/W3160525311",
        "https://openalex.org/W3157697407",
        "https://openalex.org/W2996383576",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W2088622183",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W3112034174",
        "https://openalex.org/W2973049979",
        "https://openalex.org/W3099782249",
        "https://openalex.org/W2962907457",
        "https://openalex.org/W3016011332",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W3041561163",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W3003875258",
        "https://openalex.org/W3015265920",
        "https://openalex.org/W2101210369",
        "https://openalex.org/W2982223350",
        "https://openalex.org/W2972943112",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3015522062",
        "https://openalex.org/W2767286248",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W2991213871",
        "https://openalex.org/W2988736778",
        "https://openalex.org/W3169320628",
        "https://openalex.org/W1828163288",
        "https://openalex.org/W3026041220",
        "https://openalex.org/W3128768055",
        "https://openalex.org/W2962911098",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3093579165"
    ],
    "abstract": "Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\\% to~10\\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\\% relatively.",
    "full_text": "W2V-BERT: COMBINING CONTRASTIVE LEARNING AND MASKED LANGUAGE\nMODELING FOR SELF-SUPERVISED SPEECH PRE-TRAINING\nYu-An Chung1,2∗, Yu Zhang2, Wei Han2, Chung-Cheng Chiu2, James Qin2, Ruoming Pang2, Yonghui Wu2\n1MIT Computer Science and Artiﬁcial Intelligence Laboratory\n2Google Brain\n{andyyuan, ngyuzh, weihan, chungchengc, jamesqin, rpang, yonghui}@google.com\nABSTRACT\nMotivated by the success of masked language modeling (MLM) in\npre-training natural language processing models, we propose w2v-\nBERT that explores MLM for self-supervised speech representation\nlearning. w2v-BERT is a framework that combines contrastive\nlearning and MLM, where the former trains the model to discretize\ninput continuous speech signals into a ﬁnite set of discriminative\nspeech tokens, and the latter trains the model to learn contextualized\nspeech representations via solving a masked prediction task con-\nsuming the discretized tokens. In contrast to existing MLM-based\nspeech pre-training frameworks such as HuBERT, which relies on\nan iterative re-clustering and re-training process, or vq-wav2vec,\nwhich concatenates two separately trained modules, w2v-BERT\ncan be optimized in an end-to-end fashion by solving the two self-\nsupervised tasks (the contrastive task and MLM) simultaneously.\nOur experiments show that w2v-BERT achieves competitive results\ncompared to current state-of-the-art pre-trained models on the Lib-\nriSpeech benchmarks when using the Libri-Light 60k corpus as the\nunsupervised data. In particular, when compared to published mod-\nels such as conformer-based wav2vec 2.0 and HuBERT, our model\nshows 5% to 10% relative WER reduction on the test-clean and\ntest-other subsets. When applied to the Google’s V oice Search traf-\nﬁc dataset, w2v-BERT outperforms our internal conformer-based\nwav2vec 2.0 by more than 30% relatively.\nIndex Terms— Self-supervised learning, representation learn-\ning, unsupervised pre-training, BERT, wav2vec 2.0\n1. INTRODUCTION\nHow to leverage large-scale unannotated speech to improve super-\nvised automatic speech recognition (ASR) performance has been a\nlongstanding research problem. To date, there have been two ma-\njor streams for utilizing unlabeled speech data for tackling such a\nsemi-supervised ASR task.\nThe ﬁrst line of work is self-training [1, 2, 3], also known as\npseudo-labeling, where the system starts with training a teacher\nmodel using initially available labeled data. Next, the teacher model\nis used to label the unlabeled data. The combined labeled and\npseudo-labeled data are then used to train a student model. The\npseudo-labeling process can be repeated multiple times to improve\nthe quality of the teacher model. Self-training has been a practically\nuseful and extensively studied technique in ASR [4, 5, 6, 7, 8, 9].\nThe second direction of taking advantage of unlabeled speech\ndata is unsupervised pre-training, or self-supervised pre-training.\nIn unsupervised pre-training, a model is ﬁrst trained to complete a\n∗Work done during an internship at Google Brain.\nproxy task that is designed to consume only unlabeled data (hence\nunsupervised). Such proxy task is commonly believed to be capable\nof initializing the parameters of the model at a good starting point\nbefore it is being trained on the supervised data. Signiﬁcant recent\nresearch effort has been made to develop proxy tasks that allow\nmodels to perform well when the models are ﬁne-tuned on ASR\ntasks [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]. There have also\nbeen studies that show that the gains brought by self-training and\nunsupervised pre-training are additive in downstream ASR [21, 22].\nIn this work, we focus on improving the unsupervised pre-\ntraining aspect of semi-supervised ASR by proposing a novel\npre-training framework. Our method, which we call w2v-BERT,\ncombines the core methodologies from two recent frameworks for\nself-supervised pre-training of speech and language respectively:\nwav2vec 2.0 [23] and BERT [24]. The idea of w2v-BERT is to use\nthe contrastive task deﬁned in wav2vec 2.0 to obtain an inventory of\na ﬁnite set of discriminative, discretized speech units, and then use\nthem as target in a masked prediction task in a way that is similar to\nmasked language modeling (MLM) proposed in BERT for learning\ncontextualized speech representations. Although the masked pre-\ndiction task requires to consume tokens that are to be learned by\nsolving the contrastive task ﬁrst, we show that in practice the two\nobjectives can be optimized simultaneously. Figure 1 illustrates the\nw2v-BERT pre-training framework.\nIn this paper, we make the following contributions:\n• We propose w2v-BERT that directly optimizes a contrastive\nloss and a masked prediction loss simultaneously for end-to-\nend self-supervised speech representation learning.\n• We show that w2v-BERT yields state-of-the-art performance\non the well-benchmarked LibriSpeech task.\n• We show that w2v-BERT greatly improves a real-world\nrecognition task (voice search) over conformer-based wav2vec 2.0.\n• We provide an analysis that empirically conﬁrms the neces-\nsity of contrastive learning for enabling masked prediction in\nour framework. We also show in our voice search experi-\nments that mask prediction is very useful for alleviating the\nproblem of “easy negative samples” in contrastive learning.\nThe rest of the paper is organized as follows. We begin with dis-\ncussing the differences between w2v-BERT and some of the most\nrelevant unsupervised speech pre-training frameworks from the lit-\nerature in Section 2. Then, in Section 3 we present the w2v-BERT\npre-training framework, including the model architecture and train-\ning objectives. Section 4 describes the experimental setup, followed\nby our results and analysis in Section 5 where we apply pre-trained\nw2v-BERT models to LibriSpeech and voice search ASR. Finally,\nwe conclude in Section 6.\narXiv:2108.06209v2  [cs.LG]  13 Sep 2021\nInput Features\nQuantization\nConvolutional \nSubsampling\nLinear\nConformer Blocks\nConformer Blocks\nConformer Blocks\nConformer Blocks\nMasking\nTarget\nContext Vectors Discretized ids\nContrastive Loss\nMLM Loss\nContext Vectors\nContext Vectors \nEncoded Features\nMLM Stack\nContrastive \nStack\nFeature Encoder\nFig. 1: Illustration of the w2v-BERT pre-training framework. w2v-\nBERT is composed of a feature encoder, a contrastive module, and\na masked language modeling (MLM) module, where the latter two\nare both a stack of conformer blocks. N and M denote the number\nof conformer blocks in the two modules, respectively.\n2. RELATED WORK\nWe consider our work most related to HuBERT [25], vq-wav2vec [26],\nand DiscreteBERT [27]: w2v-BERT and these methods all try to\nﬁrst transform continuous speech signals into discretized units so\nas to exploit masked language modeling (MLM) [24] for learning\ncontextualized speech representations. Despite sharing this same\nhigh-level philosophy for learning speech representations, there are\ntwo key differences between w2v-BERT and other methods.\nThe most noticeable difference is that w2v-BERT’s speech dis-\ncretizing module and its main contextualized representation learning\nmodule can be trained end-to-end. This is in contrast to vq-wav2vec\nand DiscreteBERT, which involve a two-stage process where the\nspeech discretizing module needs to be obtained in advance and is\nkept frozen during the training of the representation learning module.\nIn vq-wav2vec and DiscreteBERT, a problematic token ID assign-\nment would negatively affect the subsequent learning module and it\nis hard for the learning module to recover the errors made by the\ndiscretizer. Observing such drawback, HuBERT greatly improves\nvq-wav2vec and DiscreteBERT by allowing reﬁnement on the ID\nassignment via iterating between k-means clustering and re-training\nits representation learning module. However, the fact that HuBERT\niterates between the two stages also means it involves more heuris-\ntic design choices, for example, the gradually increasing number of\nclusters in different iterations. End-to-end methods such as w2v-\nBERT alleviate the need of coordinating multiple stages. One poten-\ntial risk for end-to-end approaches compared to k-means clustering\nis codebook collapse. In w2v-BERT, we ﬁnd the contrastive learn-\ning objective effectively avoids codebook collapse and thus enables\nmasked prediction training.\nIn addition, unlike other methods that use transformer lay-\ners [28] as building blocks, w2v-BERT adopts conformer layers [29]\nfor constructing the network. As demonstrated in [29], conformer\nlayers, which combine convolution neural networks (CNNs) and\ntransformers to model both local and global dependencies of audio\nsequences, are likely a better option for modeling speech than trans-\nformer layers and CNNs. That being said, using a potentially more\npowerful building block is not the only factor that makes w2v-BERT\noutperform other methods, as the effectiveness of the pre-training\nframework itself is also validated in our experiments where w2v-\nBERT outperforms w2v-Conformer [21], which is also built with\nconformer layers.\nw2v-BERT is also related to wav2vec 2.0 [23]. Same as\nw2v-BERT, wav2vec 2.0 is end-to-end where the discretizer is\njointly trained with its representation learning module. However,\nwav2vec 2.0 only employs contrastive learning, whose resulting\nASR performance lags behind that of combining contrastive learn-\ning and masked prediction.\n3. METHOD\nIn this section we present each component in w2v-BERT, starting\nwith its model architecture.\n3.1. Model architecture\nOur model architecture for pre-training is composed of a feature en-\ncoder that extracts latent speech representations from raw acoustic\ninputs, a module for solving wav2vec 2.0’s contrastive task [23] to\nobtain a set of discretized speech tokens, and a module for solving a\nmasked prediction task [24] for learning contextualized speech rep-\nresentations.\nFeature encoder The feature encoder acts as a convolutional sub-\nsampling block that consists of two 2D-convolution layers, both with\nstrides (2,2), resulting in a 4x reduction in the acoustic input’s se-\nquence length. Given, for example, a log-mel spectrogram as input,\nthe feature encoder extracts latent speech representations that will be\ntaken as input by the subsequent contrastive module.\nContrastive module The module contains a linear projection layer\nfollowed by a stack of conformer blocks [29], each of which is a\nseries of multi-headed self attention [28], depth-wise convolution\nand feed-forward layers.\nThe goal of the contrastive module is to discretize the feature en-\ncoder output into a ﬁnite set of representative speech units. For this\npurpose, the contrastive module involves a quantization mechanism.\nThe output of the feature encoder, on one hand, is fed into the lin-\near projection layer followed by the stack of conformer blocks after\nmasking to produce context vectors, and on the other hand, is passed\nto the quantizer without maskingto yield quantized vectors and their\nassigned token IDs. The quantized vectors are used in conjunction\nwith the context vectors that correspond to the masked positions to\nsolve the contrastive task deﬁned in wav2vec 2.0 to optimize the\ncontrastive module; the assigned token IDs will be later used by the\nsubsequent masked prediction module as prediction target.\nMasked prediction module The masked prediction module is a\nstack of conformer blocks where each block has an identical conﬁg-\nuration to those from the contrastive module. The module directly\ntakes in the context vectors produced by the contrastive module and\nextracts high-level contextualized speech representations.\n3.2. Pre-training\nDuring pre-training only unlabeled speech data is used.\nContrastive loss The contrastive loss is used to train the contrastive\nmodule along with the quantizer, such that the former yields ade-\nquate context vectors that will be taken as input by the subsequent\nmasked prediction module, and the latter produces discriminative\ndiscretized speech tokens that will be used by the masked prediction\nTable 1: Parameters for w2v-BERT models. Dim. stands for dimension.\nModel # Params (B) # Contrastive\nLayers\n# Masked\nLayers\nModel\nDim.\nAttention\nHeads\nConv. Layer\nKernel Size\nRelative\nAttention\nCodebook\nSize\nCode\nDim.\nw2v-BERT XL 0.6 12 12 1024 8 5 N 1024 1024\nw2v-BERT XXL 1.0 12 30 1024 8 5 N 1024 1024\nmodule as prediction target. We adopt the contrastive task deﬁned in\nwav2vec 2.0 and follow its quantization mechanism.\nOnce the feature encoder has transformed the raw acoustic input\ninto latent speech representations, we randomly select some time\nsteps to mask. Unlike wav2vec 2.0 where the masked positions’ la-\ntent vectors are replaced with a shared learnable feature vector, we\nsimply replace them with random vectors. The masked feature en-\ncoder output is fed into the contrastive module for producing context\nvectors. In parallel, the feature encoder output is also passed to the\nquantizer without masking to yield its quantized vectors. For a con-\ntext vector ct corresponding to a masked time step t, the model is\nasked to identify its true quantized vector qt from a set of K dis-\ntractors {˜q1,˜q2,..., ˜qK}that are also quantized vectors uniformly\nsampled from other masked time steps of the same utterance. We\ndenote the loss as Lw, and further augment it with a codebook diver-\nsity loss Ld to encourage a uniform usage of codes. Therefore, the\nﬁnal contrastive loss is deﬁned as:\nLc = Lw + α·Ld, (1)\nwhere α= 0.1 following [23].\nMasked prediction loss The context vectors produced by the con-\ntrastive module are directly passed to the masked prediction module\nfor producing the ﬁnal context vectors that are to be used to com-\nplete a masked prediction task. A softmax layer is appended on top\nof the module’s last conformer block. If a context vector at the ﬁnal\nlayer corresponds to a masked position, the softmax layer will take\nthe context vector as input and attempt to predict its corresponding\ntoken ID, which is assigned earlier in the contrastive module by the\nquantizer. We denote the cross-entropy loss for this masked predic-\ntion task as Lm.\nw2v-BERT is trained to solve the two self-supervised tasks at\nthe same time. The ﬁnal training loss to be minimized is:\nLp = β·Lc + γ·Lm. (2)\nIn our experiments, we simply set both βand γto 1.\n3.3. Fine-tuning\nDuring ﬁne-tuning we have access to labeled data. We apply our\npre-trained w2v-BERT to two tasks: LibriSpeech and voice search.\nThe ASR network is a sequence transducer [30] that consists of a\npre-trained w2v-BERT model and a LSTM [31] decoder. We insert a\nlinear layer with Swish activation [32] and batch normalization [33]\nbetween the pre-trained w2v-BERT model and the LSTM decoder\nas the projection block.\n4. EXPERIMENTAL SETUP\nApart from the pre-training method, the rest of the experimental\npipeline follows the exact same setup as in [21].\n4.1. Data\nWe use the Libri-Light unlab-60k subset [34], which contains\nabout 60,000 hours of unannotated speech audio, for pre-training\nw2v-BERT models. For our main results, we use the LibriSpeech\n960hr subset [35] as the supervised data, and use the 100hr subset\nfor ablation studies. We report word error rates (WERs) on the\ndev-clean, dev-other, test-clean, and test-other evaluation subsets.\n80-dimensional log-mel ﬁlter bank coefﬁcients are used as acoustic\ninputs to our model. For transcript tokenization, we use a 1024-\ntoken WordPiece model [36] that is constructed from the transcripts\nof the LibriSpeech training set (or the 100hr subset when the models\nare ﬁne-tuned on it).\n4.2. Pre-training details\nMasking For masking the feature encoder output, we randomly\nsample the starting positions to be masked with a probability\nof 0.065 and mask the subsequent 10 time steps (same as [23, 21]).\nThe masked spans may overlap.\nOptimization We pre-train two versions of w2v-BERT models, one\nhas about 0.6 billion parameters and the other has about 1 billion\nparameters, denoted as w2v-BERT XL and w2v-BERT XXL, re-\nspectively. The two variants share the same model conﬁguration\nthat is summarized in Table 1, and their only difference is the num-\nber of conformer blocks. Speciﬁcally, w2v-BERT XL’s contrastive\nmodule consists of 12 conformer blocks and the masked prediction\nmodule is composed of another 12. w2v-BERT XXL, while hav-\ning the same amount of conformer blocks in its contrastive module,\nenlarges its masked prediction module to 30 conformer blocks. For\nw2v-BERT XL, we train it with a batch size of 2048 using the Adam\noptimizer [37] with a transformer learning rate schedule as described\nin section 5.3 of [28]. The peak learning rate is 2e-3 and the warm-up\nsteps are 25k. For w2v-BERT-XXL, we train it with the Adafactor\noptimizer [38] with β1 = 0.9 and β2 = 0.98, with the learning rate\nschedule remaining the same.\n4.3. Fine-tuning details\nOptimization For both w2v-BERT XL and w2v-BERT-XXL, we\ntake their pre-trained checkpoints at 400k steps, and ﬁne-tune them\non the supervised data with a batch size of 256. The decoder for\nboth models are a two-layer LSTM with a hidden dimension of 640.\nWe employ separate optimizers and learning rate schedules for op-\ntimizing the pre-trained model and the decoder, given the fact that\nthe former has been pre-trained while the latter needs to be trained\nfrom scratch. For w2v-BERT XL, both the pre-trained model and the\ndecoder are optimized with an Adam optimizer with a transformer\nlearning schedule. The difference is that for the pre-trained com-\nponent we use a peak learning rate of 3e-4 with 5k warm-up steps,\nwhile for the decoder we use a peak learning rate of 1e-3 and 1.5k\nwarm-up steps. For w2v-BERT-XXL, an Adafactor optimizer that\nTable 2: WERs(%) when using the LibriSpeech 960hr subset as supervised data. We compare models trained without any unlabeled\ndata (Trained from Scratch), trained using Noisy Student Training (NST) without any pre-training (Self-training Only), ﬁne-tuned from a\npre-trained model only using supervised data (Pre-training Only), and the models obtained by combining pre-training and self-training (Pre-\ntraining + Self-training). We also include the best results of several methods that we can ﬁnd from the literature, and their corresponding\nreferences are where the numbers are quoted from. The lowest WER(s) under different settings are marked in bold. AM/LM Size denotes\nthe number of parameters in the acoustic/language model. ∗The reason why we do not include Conformer XL and Conformer XXL is that,\naccording to [21], simply enlarging Conformer L produces worse results when the model is trained from scratch. †Calculated based on the\nLM conﬁguration provided in [22, 23]; >because some information such as the token embedding size is not given therefore not included.\nMethod Unlabeled\nData (hrs)\nAM\nSize (B)\nLM\nSize (B)\nNo LM With LM\ndev dev-other test test-other dev dev-other test test-other\nTrained from Scratch\nConformer L [21]∗ N/A 0.1 0.1 1.9 4.4 2.1 4.3 − − 1.9 3.9\nSelf-training Only\nConformer L with NST [21] 60k 0.1 0.1 1.6 3.3 1.7 3.5 1.6 3.1 1.7 3.3\nPre-training Only\nwav2vec 2.0 [22] 60k 0.3 >0.4† 2.1 4.5 2.2 4.5 1.6 3.0 1.8 3.3\nHuBERT Large [25] 60k 0.3 − − − − − 1.5 3.0 1.9 3.3\nHuBERT X-Large [25] 60k 1.0 − − − − − 1.5 2.5 1.8 2.9\nw2v-Conformer XL [21] 60k 0.6 0.1 1.7 3.5 1.7 3.5 1.6 3.2 1.5 3.2\nw2v-Conformer XXL [21] 60k 1.0 0.1 1.6 3.2 1.6 3.3 1.5 3.0 1.5 3.1\nw2v-BERT XL (Ours) 60k 0.6 0.1 1.5 2.9 1.5 2.9 1.4 2.8 1.5 2.8\nw2v-BERT XXL (Ours) 60k 1.0 0.1 1.5 2.7 1.5 2.8 1.4 2.6 1.5 2.7\nPre-training + Self-training\nwav2vec 2.0 [22] 60k 0.3 >0.4 1.3 3.1 1.7 3.5 1.1 2.7 1.5 3.1\nw2v-Conformer XXL [21] 60k 1.0 0.1 1.3 2.7 1.5 2.8 1.3 2.6 1.4 2.7\nw2v-Conformer XXL+ [21] 60k 1.1 0.1 1.3 2.7 1.5 2.7 1.3 2.6 1.4 2.6\nw2v-BERT XL (Ours) 60k 0.6 0.1 1.3 2.6 1.4 2.7 1.3 2.6 1.4 2.6\nw2v-BERT XXL (Ours) 60k 1.0 0.1 1.4 2.4 1.4 2.5 1.3 2.4 1.4 2.5\nhas the same conﬁguration as in pre-training is used, and the learn-\ning rate schedules for the encoder and decoder are the same as the\nXL variant.\nSelf-training, data augmentation, and LM fusion In addition to\nself-supervised pre-training, in the ﬁne-tuning stage we also employ\na number of practical techniques that further improve models’ per-\nformance on ASR. These techniques include SpecAugment [39, 40]\nfor data augmentation, Noisy Student Training [41] for self-training,\nand language model fusion for decoding. When any of the tech-\nniques are used, we follow the exact same setup as in [21]. We refer\nthe readers to the paper for the details on these techniques.\n5. RESULTS AND DISCUSSION\n5.1. Main results\nIn Table 2, we present our results on the four LibriSpeech evaluation\nsets using the 960hr subset as the supervised data. We compare\nw2v-BERT to a number of state-of-the-art self-supervised represen-\ntation learning methods from the literature such as HuBERT [25]\nand wav2vec 2.0 [23] under different semi-supervised settings, in-\ncluding whether self-training is employed during the ﬁne-tuning\nstage and whether a language model is incorporated during infer-\nence time. We also include the model size of the ASR network used\nby each method, denoted as acoustic model (AM) Size and language\nmodel (LM) Size. Results missing from the literature (e.g., results of\nHuBERT without self-training and LM) are indicated with a “−” in\nthe table. From Table 2 we have the following two key conclusions.\nWithout self-training and LM, w2v-BERT already either outper-\nforms or matches other models with LM.We see that with just pre-\ntraining when neither self-training nor LM is used, w2v-BERT XL\nachieves a WER of 1.5/2.9 (test/test-other), which already either\noutperforms or matches other models with LM, and outperforms\ntheir counterparts without LM by a larger margin. w2v-BERT XXL\nfurther increases the gap on the more challenging dev-other and\ntest-other subsets. Noticeably, compared to wav2vec 2.0, w2v-\nBERT-XXL shows a relative WER reduction of 28%, 42%, 32%,\nand 38% on the four evaluation subsets respectively without LM,\nand 13%/ 13%/ 17%/ 18% when LM is employed.\nWe want to highlight that although w2v-BERT XL (0.6B) and\nw2v-BERT XXL (1.0B) have a larger pre-trained model size than\nwav2vec 2.0 (0.3B), the latter also incorporates a much larger LM (>\n0.4B) during self-training and decoding according to [22, 23].\nWhen considering the sum of the two components, wav2vec 2.0 ( >\n0.7B) actually features a similar (if not bigger) model size as w2v-\nBERT XL (0.7B).\nContrastive learning combined with masked language model-\ning is more effective than contrastive learning alone. w2v-\nConformer [21] and w2v-BERT only differ in their pre-training\nmethod and have all other aspects in common such as their model\nsize and ﬁne-tuning pipeline. This allows a truly apple-to-apple\ncomparison between the two pre-training methods for their effec-\ntiveness in representation learning. Below we brieﬂy describe their\ndifferences in pre-training.\nw2v-Conformer adopted wav2vec 2.0’s contrastive task as the\nsole pre-training objective, but replaced the quantization module\nwith a linear layer as the authors did not ﬁnd quantization help-\nful for improving downstream ASR performance. w2v-BERT, on\n(a) MLM training loss\n (b) MLM training accuracy\n (c) Training diversity loss\nFig. 2: Training curves of w2v-BERT models with and without contrastive module. From left to right: MLM training loss, MLM training\naccuracy, training diversity loss. The blue curve represents the w2v-BERT model without contrastive module, and the orange curve represents\nw2v-BERT XL (with contrastive module). We show results for the ﬁrst 300k steps.\nthe other hand, adopts wav2vec 2.0’s contrastive task not just for\nlearning contextualized speech representations, but mainly for the\npurpose of obtaining a codebook that can represent every segment of\ncontinuous speech as an discriminative discrete token, such that we\ncan exploit MLM for learning powerful speech representations. As\nwill be demonstrated in our analysis (Section 5.2), the contrastive\nloss is essential for making MLM to work.\nBy comparing the Pre-training Only results of w2v-Conformer\nand w2v-BERT, we see that w2v-BERT XL, despite having fewer\nmodel parameters, already outperforms w2v-Conformer-XXL—the\nprevious state of the art—especially on the dev-other and test-other\nsubsets. When self-training is applied, w2v-BERT XL still either\noutperforms or matches w2v-Conformer-XXL’s results. w2v-BERT-\nXXL, which is of the same model size as w2v-Conformer XXL,\noutperforms w2v-Conformer XXL on even more evaluation sub-\nsets. These results demonstrate the superiority of the proposed w2v-\nBERT over existing pre-training frameworks.\n5.2. Analysis and discussion\nThe goal of our analysis is to understand the roles of contrastive\nlearning as well as its learned codebook in the w2v-BERT pre-\ntraining framework.\nOn the necessity of contrastive module The ﬁrst natural question\nis whether the contrastive module is an essential component of the\nframework, or is that a masked prediction module alone can already\nderive a suitable codebook for its own MLM purpose.\nWithout the contrastive module, the feature encoder output is di-\nrectly fed to the masked prediction module. Intuitively, the masked\nprediction module then gets a full control over the quantizer (which\nmay originally be viewed as part of the contrastive module) and de-\ncide its own prediction target. In order to maximize the prediction\nperformance, the masked prediction module can “cheat” by coming\nup with a trivial solution where it asks the quantizer to cooperate\nwith it by quantizing all feature encoder’s output frames that cor-\nrespond to the masked positions to the same code vector, in other\nwords, always assigning the same target ID for the masked predic-\ntion module to predict. The module thus perfectly solves the masked\nprediction task without learning any useful representation.\nTo verify our intuition, we train a series of w2v-BERT models\nwithout the contrastive module. These variants all have the same ca-\npacity as w2v-BERT XL, that is, they are all constructed with 24\nconformer layers. We try different values of α = 0.1,0.3,0.5,\nand 0.7 in Equation 1 for increasing encouragement of uniform us-\nage of codes. Nevertheless, we ﬁnd all models are untrainable and\nquickly collapse regardless of the value of α. In Figure 2 we show\nthe training curves of a w2v-BERT model without contrastive mod-\nule with α set to 0.5. We include the curves of the successfully\ntrained w2v-BERT XL for comparison. The plots include the mod-\nels’ masked prediction loss (Figure 2a), masked prediction accu-\nracy (Figure 2b), and diversity loss (Figure 2c) during pre-training.\nWe ﬁnd that the training curves of w2v-BERT without con-\ntrastive module (in blue) strongly align with our intuition: the\nmasked prediction loss quickly decreases to close to 0 at the early\nstage of training (Figure 2a) where the model reaches 100% pre-\ndiction accuracy (Figure 2b). Meanwhile, as shown in Figure 2c,\nthe diversity loss quickly increases to close to 1, where in our im-\nplementation this indicates an extremely low entropy of softmax\ndistribution over the codebook entries, suggesting code collapse.\nComparing the curves of w2v-BERT models with and without con-\ntrastive module, we hypothesize that the contrastive loss guides the\nentries in the codebook to be discriminative, thus preventing the\nmasked prediction module from deriving a trivial solution just to\nmaximize masked prediction performance.\nOn the impact of contrastive module’s capacity After conﬁrming\nthe necessity of contrastive module, next we are interested in inves-\ntigating the impact of its capacity on downstream ASR performance.\nWe train a series w2v-BERT models with different numbers of\nconformer layers in their contrastive module. To rule out the factor\nof masked prediction module’s capacity, we keep the total number\nof conformer layers in the two modules ﬁxed at 24. We use Cn to\ndenote each variant, where nis the number of conformer layers in\nthe contrastive module. For instance, C4 has 4 conformer layers in\nits contrastive module and 20 in its masked prediction module. Here\nwe consider n= 2,4,6,8,10,12,and 24, where C24 is an extreme\ncase where the two modules are completely overlapped with each\nother and hence the contrastive and MLM tasks will be both tackled\nat the last (24-th) layer. Note that C12 is essentially w2v-BERT XL.\nWe use the LibriSpeech 100hr subset as the supervised data for\nthis experiment, and both self-training and LM fusion are not used\nwhen training the ASR network. Results are shown in Table 3. We\ninclude the results of some pre-training methods from the literature\nthat also do not incorporate self-training and LM.\nFrom Table 3 we can roughly observe a performance sweet\nspot on all four evaluation subsets when we increase the number\nof layers in the contrastive module. From C2 to C8, the WERs\nTable 3: WERs (%) when using the LibriSpeech 100hr subset as\nsupervised data. For all methods, both self-training and LM fusion\nare not used. References are where the numbers are quoted from.\nMethod dev dev-other test test-other\nBaseline\nwav2vec 2.0 [23] 3.3 6.5 3.1 6.3\nw2v-Conformer XL [21] 2.5 4.7 2.6 4.9\nw2v-BERT XXL (Ours) 2.3 4.0 2.3 4.3\nw2v-BERT w/ 24 layers\nC2 2.4 5.1 2.5 5.1\nC4 2.5 4.6 2.5 5.1\nC6 2.5 4.2 2.4 4.7\nC8 2.3 4.3 2.4 4.6\nC10 2.4 4.5 2.5 4.8\nC12 (w2v-BERT XL) 2.4 4.4 2.5 4.6\nC24 2.4 4.9 2.5 5.0\nare mostly decreasing, meaning that enlarging the contrastive mod-\nule is helpful for learning better representations. The fact that the\nperformance continues to improve while the masked prediction\nmodule shrinks (and hence becomes less expressive) as we deepen\nthe contrastive module further suggests the importance of making\nthe contrastive module sufﬁciently large.\nStarting from C8, however, the WERs stop decreasing as we\ndeepen the contrastive module. We hypothesize that this is because\nthe masked prediction module has now become too small to learn\nrepresentations useful for the MLM task. Such reasoning is sup-\nported by the fact that enlarging the masked prediction module while\nkeeping the contrastive module the same size can still improve the\nperformance (w2v-BERT XL vs. w2v-BERT XXL).\nLast but not least, we see that w2v-BERT always outperforms\nwav2vec 2.0 regardless of its layer conﬁguration. It also either out-\nperforms or matches w2v-Conformer XL’s performance when its\ncontrastive module has enough capacity (i.e., when n> 4).\n5.3. Results on Voice Search trafﬁc\nSo far we have shown w2v-BERT pre-trained on read speech au-\ndio can achieve great performance on the well-benchmarked Lib-\nriSpeech task. To validate the effectiveness of w2v-BERT on real-\nworld audio trafﬁc, we apply it to Google’s V oice Search trafﬁc. Our\ntrain and test sets are derived from [42]. We use 34.3k hours of En-\nglish audio for pre-training, and randomly pick 1k hours as the ﬁne-\ntuning data, which is anonymized and human-transcribed. The test\nset contains around 12k V oice Search utterances with duration less\nthan 5.5s long. The testing utterances are anonymized and human-\ntranscribed, and are representative of Google’s V oice Search trafﬁc.\nThe trafﬁc data is more challenging to be used for pre-training\nthan read speech audio in two folds: (1) It is noisier and contains\nmore silences that make negative sampling for contrastive learning\nless effective. (2) The average length of the trafﬁc audio (5 seconds)\nis much shorter than that of read speech audio. These factors make\nthe context learned from the audio segments much less effective.\nAs shown in Table 4, if we take the same training script as w2v-\nConformer-XL, the model tends to cheat on negative samples due\nto the large portion of non-speech and shorter context. To make\ncontrastive learning more effective, we have to use a less aggres-\nsive subsampling: instead of using a 4 times convolutional stride,\nwe stack 3 frames as target to encourage the model to learn better\nTable 4: Results on voice search data. Baseline conformer model\nis 100M parameters. All the other models are 600M parameters,\nmarked as XL.\nMethod Unlabeled Data (Domain) Test (VS)\nConformer N/A 10.7\nw2v-Conformer-XL 34.3k (V oice search) 10.8\nw2v-Conformer-XL-tuned 34.3k (V oice search) 8.9\nw2v-BERT XL (Ours) 34.3k (V oice search) 6.2\ncontext. However, by taking an identical architecture and using the\nsame training receipt, our w2v-BERT XL signiﬁcantly improves the\ntuned contrastive baseline by relative 30%.\n6. CONCLUSION AND FUTURE WORK\nWe proposed w2v-BERT for self-supervised speech representation\nlearning. w2v-BERT is composed of a contrastive module for dis-\ncretizing continuous speech and a masked prediction module that\nperforms masked language modeling with the discretized speech.\nThe two modules can be jointly optimized. We pre-train w2v-BERT\non 60k hours of unlabeled speech data from the Libri-Light corpus,\nand show it either outperforms or matches state-of-the-art systems\nsuch as w2v-Conformer, HuBERT, and wav2vec 2.0. The gain also\ntransfers to a more challenging internal dataset. We also provide an\nanalysis on the importance of the contrastive module for enabling\neffective masked language modeling in w2v-BERT.\nIn our experiments, all the hyperparameter setups are directly\ntaken from [21] without any changes. For future work, we plan to\nﬁrst search for the best training conﬁguration for w2v-BERT. We are\nalso interested in evaluating w2v-BERT in low-resource data settings\nusing the Libri-Light 10min, 1hr, and 10hr benchmarks.\n7. REFERENCES\n[1] Ellen Riloff and Janyce Wiebe, “Learning extraction patterns\nfor subjective expressions,” in EMNLP, 2003.\n[2] David Yarowsky, “Unsupervised word sense disambiguation\nrivaling supervised methods,” in ACL, 1995.\n[3] Henry Scudder, “Probability of error of some adaptive pattern-\nrecognition machines,” IEEE Transactions on Information\nTheory, vol. 11, no. 3, pp. 363–371, 1965.\n[4] Jacob Kahn, Ann Lee, and Awni Hannun, “Self-training for\nend-to-end speech recognition,” in ICASSP, 2020.\n[5] Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana\nLikhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sri-\nram, Vitaliy Liptchinsky, and Ronan Collobert, “End-to-end\nASR: from supervised to semi-supervised learning with mod-\nern architectures,” in ICML SAS Workshop, 2020.\n[6] Bo Li, Tara N. Sainath, Ruoming Pang, and Zelin Wu, “Semi-\nsupervised training for end-to-end models via weak distilla-\ntion,” in ICASSP, 2019.\n[7] Sree Hari Krishnan Parthasarathi and Nikko Strom, “Lessons\nfrom building acoustic models with a million hours of speech,”\nin ICASSP, 2019.\n[8] Scott Novotney and Richard Schwartz, “Analysis of low-\nresource acoustic model self-training,” in Interspeech, 2009.\n[9] George Zavaliagkos and Thomas Colthurst, “Utilizing un-\ntranscribed training data to improve performance,” in DARPA\nBroadcast News Transcription and Understanding Workshop,\n1998.\n[10] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Repre-\nsentation learning with contrastive predictive coding,” arXiv\npreprint arXiv:1807.03748, 2018.\n[11] Yu-An Chung, Wei-Ning Hsu, Hao Tang, and James Glass,\n“An unsupervised autoregressive model for speech represen-\ntation learning,” in Interspeech, 2019.\n[12] Steffen Schneider, Alexei Baevski, Ronan Collobert, and\nMichael Auli, “wav2vec: Unsupervised pre-training for speech\nrecognition,” in Interspeech, 2019.\n[13] Shaoshi Ling, Yuzong Liu, Julian Salazar, and Katrin Kirch-\nhoff, “Deep contextualized acoustic representations for semi-\nsupervised speech recognition,” in ICASSP, 2020.\n[14] Yu-An Chung and James Glass, “Generative pre-training for\nspeech with autoregressive predictive coding,” in ICASSP,\n2020.\n[15] Andy T. Liu, Shu-Wen Yang, Po-Han Chi, Po-Chun Hsu, and\nHung-Yi Lee, “Mockingjay: Unsupervised speech representa-\ntion learning with deep bidirectional transformer encoders,” in\nICASSP, 2020.\n[16] Yu-An Chung and James Glass, “Improved speech represen-\ntations with multi-target autoregressive predictive coding,” in\nACL, 2020.\n[17] Andy T. Liu, Shang-Wen Li, and Hung-Yi Lee, “TERA: Self-\nsupervised learning of transformer encoder representation for\nspeech,” IEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, vol. 29, pp. 2351–2366, 2021.\n[18] Weiran Wang, Qingming Tang, and Karen Livescu, “Unsuper-\nvised pre-training of bidirectional speech encoders via masked\nreconstruction,” in ICASSP, 2020.\n[19] Shaoshi Ling and Yuzong Liu, “DeCoAR 2.0: Deep contextu-\nalized acoustic representations with vector quantization,”arXiv\npreprint arXiv:2012.06659, 2020.\n[20] Junwen Bai, Weiran Wang, Yingbo Zhou, and Caiming Xiong,\n“Representation learning for sequence data with deep autoen-\ncoding predictive components,” in ICLR, 2021.\n[21] Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng\nChiu, Ruoming Pang, Quoc V . Le, and Yonghui Wu, “Push-\ning the limits of semi-supervised learning for automatic speech\nrecognition,” arXiv preprint arXiv:2010.10504, 2020.\n[22] Qiantong Xu, Alexei Baevski, Tatiana Likhomanenko, Paden\nTomasello, Alexis Conneau, Ronan Collobert, Gabriel Syn-\nnaeve, and Michael Auli, “Self-training and pre-training are\ncomplementary for speech recognition,” in ICASSP, 2021.\n[23] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli, “wav2vec 2.0: A framework for self-supervised\nlearning of speech representations,” in NeurIPS, 2020.\n[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova, “BERT: Pre-training of deep bidirectional trans-\nformers for language understanding,” in NAACL-HLT, 2019.\n[25] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,\nKushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman\nMohamed, “HuBERT: Self-supervised speech representation\nlearning by masked prediction of hidden units,” arXiv preprint\narXiv:2106.07447, 2021.\n[26] Alexei Baevski, Steffen Schneider, and Michael Auli, “vq-\nwav2vec: Self-supervised learning of discrete speech represen-\ntations,” in ICLR, 2020.\n[27] Alexei Baevski, Michael Auli, and Abdelrahman Mohamed,\n“Effectiveness of self-supervised pre-training for speech recog-\nnition,” arXiv preprint arXiv:1911.03912, 2019.\n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin, “Attention is all you need,” in NIPS, 2017.\n[29] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par-\nmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng-\ndong Zhang, Yonghui Wu, and Ruoming Pang, “Conformer:\nConvolution-augmented transformer for speech recognition,”\nin Interspeech, 2020.\n[30] Alex Graves, “Sequence transduction with recurrent neural\nnetworks,” arXiv preprint arXiv:1211.3711, 2012.\n[31] Sepp Hochreiter and J ¨urgen Schmidhuber, “Long short-term\nmemory,” Neural Computation, vol. 9, no. 8, pp. 1735–1780,\n1997.\n[32] Prajit Ramachandran, Barret Zoph, and Quoc V . Le, “Search-\ning for activation functions,”arXiv preprint arXiv:1710.05941,\n2017.\n[33] Sergey Ioffe and Christian Szegedy, “Batch normalization: Ac-\ncelerating deep network training by reducing internal covariate\nshift,” in ICML, 2015.\n[34] Jacob Kahn, Morgane Rivi `ere, Weiyi Zheng, Evgeny\nKharitonov, Qiantong Xu, Pierre-Emmanuel Mazar ´e, Julien\nKaradayi, Vitaliy Liptchinsky, Ronan Collobert, Christian\nFuegen, Tatiana Likhomanenko, Gabriel Synnaeve, Armand\nJoulin, Abdelrahman Mohamed, and Emmanuel Dupoux,\n“Libri-light: A benchmark for ASR with limited or no super-\nvision,” in ICASSP, 2020.\n[35] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev\nKhudanpur, “LibriSpeech: An ASR corpus based on public\ndomain audio books,” in ICASSP, 2015.\n[36] Mike Schuster and Kaisuke Nakajima, “Japanese and Korean\nvoice search,” in ICASSP, 2012.\n[37] Diederik P. Kingma and Jimmy Ba, “Adam: A method for\nstochastic optimization,” in ICLR, 2015.\n[38] Noam Shazeer and Mitchell Stern, “Adafactor: Adaptive learn-\ning rates with sublinear memory cost,” in ICML, 2018.\n[39] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, and Quoc V . Le, “SpecAugment:\nA simple data augmentation method for automatic speech\nrecognition,” in Interspeech, 2019.\n[40] Daniel S. Park, Yu Zhang, Chung-Cheng Chiu, Youzheng\nChen, Bo Li, William Chan, Quoc V . Le, and Yonghui Wu,\n“SpecAugment on large scale datasets,” in ICASSP, 2020.\n[41] Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng\nChiu, Bo Li, Yonghui Wu, and Quoc V . Le, “Improved noisy\nstudent training for automatic speech recognition,” in Inter-\nspeech, 2020.\n[42] Bo Li, Ruoming Pang, Tara N. Sainath, Anmol Gulati,\nYu Zhang, James Qin, Parisa Haghani, W. Ronny Huang, and\nMin Ma, “Scaling end-to-end models for large-scale multilin-\ngual ASR,” arXiv preprint arXiv:2104.14830, 2021."
}