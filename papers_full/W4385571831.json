{
    "title": "Distilling Reasoning Capabilities into Smaller Language Models",
    "url": "https://openalex.org/W4385571831",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2807756193",
            "name": "Kumar Shridhar",
            "affiliations": [
                "Institute for Ethnic Studies"
            ]
        },
        {
            "id": "https://openalex.org/A4287858569",
            "name": "Alessandro Stolfo",
            "affiliations": [
                "Institute for Ethnic Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2035754532",
            "name": "Mrinmaya Sachan",
            "affiliations": [
                "Institute for Ethnic Studies"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2950990772",
        "https://openalex.org/W4306295121",
        "https://openalex.org/W4385572102",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4385565015",
        "https://openalex.org/W2105717194",
        "https://openalex.org/W2945720633",
        "https://openalex.org/W3104499181",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4310625358",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4312205996",
        "https://openalex.org/W1539746312",
        "https://openalex.org/W4389523706",
        "https://openalex.org/W4303648545",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W4287393336",
        "https://openalex.org/W3138392969",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2134797427",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W2251349042",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W2963532001",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3034643750",
        "https://openalex.org/W4385573161",
        "https://openalex.org/W2984284833",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W2962833140",
        "https://openalex.org/W4283768109",
        "https://openalex.org/W4300978888",
        "https://openalex.org/W4225591000"
    ],
    "abstract": "Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver.In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems.On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available: https://github.com/kumar-shridhar/Distiiling-LM.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 7059–7073\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDistilling Reasoning Capabilities into Smaller Language Models\nKumar Shridhar∗ Alessandro Stolfo∗ Mrinmaya Sachan\nDepartment of Computer Science, ETH Z¨urich\n{shkumar, stolfoa}@ethz.ch\nAbstract\nStep-by-step reasoning approaches like chain\nof thought (CoT) have proved to be very ef-\nfective in inducing reasoning capabilities in\nlarge language models. However, the success of\nthe CoT approach is fundamentally tied to the\nmodel size, and billion parameter-scale mod-\nels are often needed to get CoT to work. In\nthis paper, we propose a knowledge distillation\napproach that leverages the step-by-step CoT\nreasoning capabilities of larger models and dis-\ntills these abilities into smaller models.\nIn this work, we propose an alternative rea-\nsoning scheme, SOCRATIC COT that learns a\ndecomposition of the original problem into a\nsequence of subproblems and uses it to guide\nthe intermediate reasoning steps. We use SO-\nCRATIC COT to train a combination of two\nsmall distilled models: a problem decomposer\nand a subproblem solver. In practice, given a\nnew problem, the two distilled models work\nin sync to decompose and solve complex prob-\nlems. On multiple reasoning datasets (GSM8K,\nStrategyQA, and SV AMP), our proposed dis-\ntillation strategies boost the performance of\nsmaller models over 70% compared to the base-\nlines. Finally, we investigate when SOCRATIC\nCOT is an effective alternative to CoT, demon-\nstrating cases where a much smaller model\n(GPT-2 large) can outperform a 10X larger\nmodel (GPT-3 6B). Our code is available here.\n1 Introduction\nLarge language models (LLMs) have demonstrated\nstrong performance on a variety of reasoning tasks\n(Brown et al., 2020; Hoffmann et al., 2022; Chowd-\nhery et al., 2022, inter alia). One particularly inter-\nesting strategy for prompting these models is chain-\nof-thought (CoT), which has been shown to elicit\nreasoning abilities in LLMs by asking the model\nto incorporate intermediate reasoning steps while\nsolving a problem (Nye et al., 2021; Wei et al.,\n∗ Equal contribution.\n!LLM\n!\nSmall LM \nFine-tuning\nA robe takes 2 bolts of blue fiber and half \nthat much white fiber.  How many bolts in \ntotal does it take?\n\"\nCoT:\nIt takes 2/2=<<2/2=1>>1 \nbolt of white ﬁber. So the \ntotal amount of fabric is…\nReasoning Skill \nTransfer\nFew-shot \nPrompting\nGenerate \nAnnotationCoT:\nHow many bolts of white ﬁber does it take?\nIt takes 2/2=<<2/2=1>>1 bolt of white ﬁber.\nHow many bolts in total does it take?\nSo the total amount of fabric is \n2+1=<<2+1=3>>3 bolts of fabric. Socratic CoT:\nHow many bolts of white \nﬁber does it take?\nIt takes…\n!\n !Question \nGeneration\nQuestion \nAnswering\n⚙\n⚙\nSingle Model\nReasoning Annotation via LLM\nFigure 1: Illustration of the proposed framework. First,\nan LLM is prompted to decompose a multi-step problem\nproviding annotation for the intermediate steps leading\nto the final solution. Then, the generated annotation is\nused to provide additional supervision when fine-tuning\nsmaller models.\n2022b; Wang et al., 2022). However, CoT has been\nshown to work primarily on models with hundreds\nof billions of parameters (Wei et al., 2022b,a) or\nthose tuned on a wide range of tasks (Chung et al.,\n2022; Iyer et al., 2022).\nDue to the significant computational resources or\nexpensive API calls required to access CoT-capable\nLLMs, we ask whether it is possible to elicit such\nreasoning capabilities in smaller models.1\n1Following Li et al. (2022), we argue that small and large\nmodels are relative terms and context-dependent. We consider\nmodels with billions of parameters to be large, and models\nwith millions of parameters to be small.\n7059\nSmall-sized, non-fine-tuned language models are\nknown to be poor reasoners (Stolfo et al., 2023).\nTherefore, a possible approach to induce CoT-like\nreasoning abilities in smaller models would be fine-\ntuning them on step-by-step examples.\nIn our work, we propose a framework for leverag-\ning the reasoning capabilities of LLMs to supervise\nthe training of smaller models. This approach can\nbe thought of as a form of knowledge distillation\n(Hinton et al., 2015), where a larger teacher model\ntransfers knowledge to a smaller student model.\nHowever, unlike standard knowledge distillation,\nour method transfers the reasoning abilities of the\nteacher model only using its generated solutions\nas a proxy, i.e., we do not assume access to the\nteacher model parameters. Our approach consists\nof prompting an LLM to produce step-by-step anno-\ntations leading to the answer for a set of problems.\nThis annotation is then used as supervision to fine-\ntune the student model. A high-level illustration of\nthe process is provided in Figure 1.\nWithin this framework, we study three different\ntypes of annotation structure for supervising our\ndistillation approach: (i) We consider fine-tuning\non the gold step-by-step solution procedure for\ndatasets where the step-by-step solutions are avail-\nable. (ii) We study whether procedural supervision,\ncoming from the chain of thought (CoT) of the\nteacher model can improve upon the baseline. (iii)\nWe propose a third type of supervision structure,\nwhich we call SOCRATIC COT. This approach re-\nlies on learning a semantic decomposition of the\noriginal problem into a sequence of subproblem-\nsolution pairs using two models – a) a question\ngenerator that learns to decompose the problem\ninto a sequence of subproblems, and b) a question-\nanswering model that solves the various generated\nsubproblems (more details are in section 3.2). This\napproach can be thought of as an extension of the\ntypical chain of thought reasoning where, unlike\nCoT, the intermediate steps are now decomposed\ninto subquestion-solution pairs; the subquestions\nguide the generation of intermediate steps that lead\nto the final answer to the problem.\nWe train distilled student models with the var-\nious annotation structures mentioned above. De-\npending on the annotation available for the given\ndata, we use the teacher model to generate either\na CoT-like solution to a problem or, if the step-by-\nstep annotation is available, a set of subquestions\nleading to the solution of the problem, or both (ex-\namples of different annotations are shown in Figure\n2).\nWe perform our analyses on three multi-step\nreasoning datasets: GSM8K (Cobbe et al., 2021),\nStrategyQA (Geva et al., 2021), and SV AMP (Pa-\ntel et al., 2021). We consider data with various\ntypes of annotation to cover a range of realistic data\nscenarios. Our results show that supervision by\nCoT-decomposed examples helps smaller models\nperform better, and subquestioning introduced by\nSOCRATIC COT can provide further improvement.\nWe observe performance gains of up to 40% with\nLLM-generated step-by-step annotations – this vali-\ndates the effectiveness of our distillation framework\n(detailed analysis in Section 5).\n2 Related Work\nDecomposing Multi-Step Reasoning Tasks\nSolving multi-step reasoning tasks like MWPs has\nbeen a popular area of research for the last cou-\nple of years (Kushman et al., 2014; Hosseini et al.,\n2014; Roy et al., 2015; Amini et al., 2019; Zhang\net al., 2020; Shridhar et al., 2022; Opedal et al.,\n2023). However, the majority of the modern ap-\nproaches for these problems are shifting towards\nusing large language models, often relying on ap-\nproaches involving prompting or in-context learn-\ning (Cobbe et al., 2021; Kojima et al., 2022; Wei\net al., 2022b; Chowdhery et al., 2022; Lewkowycz\net al., 2022; Srivastava et al., 2022). One such\nprompting approach is the chain of thought prompt-\ning (Wei et al., 2022b), which prompts the language\nmodel to generate a series of intermediate steps that\nimprove the reasoning capabilities in LLMs. Wang\net al. (2022) took another step forward and sam-\npled multiple reasoning paths and selected the most\nrelevant output using majority voting. Huang et al.\n(2022) used the most voted outputs to further fine-\ntune the model for better performance. Kojima et al.\n(2022) further improved the reasoning of LLM in a\nzero-shot manner by appending “Let’s think step\nby step” to the prompt. In contrast, our work does\nnot propose prompting solutions; instead, we ex-\nplicitly guide the student model reasoning using\nsub-questions at each step. Most similar to our\nwork is the work by Zhou et al. (2022) which de-\ncomposes questions into sub-questions and asks\nthe language model to solve each sub-question se-\nquentially. However, this work is also restricted to\nprompting and only works with LLMs with billions\nof parameters.\n7060\nKnowledge Distillation Our approach is remi-\nniscent of knowledge distillation (Ba and Caru-\nana, 2014; Hinton et al., 2015) in that we use a\nstudent network to mimic the large teacher lan-\nguage model. Snell et al. (2022) demonstrated the\nusefulness of providing instruction that can help\nmodels achieve better reasoning skills. Similar to\nour hypothesis, Eisenstein et al. (2022) argued that\nquestion-answering systems should focus not only\non the final answer, but also on the rationale that\njustifies their reasoning, to help them reason bet-\nter. We go beyond this; in our work, in addition to\nthe question-answering system, we also focus on\nwhat questions need to be asked at each step that\ncan help to learn that reasoning step better. Finally,\nsimilar to our hypothesis of injecting reasoning ca-\npabilities into smaller models, Li et al. (2022) used\nCoT-like reasoning from LLMs to train smaller\nmodels on a joint task of generating the solution\nand explaining the generated solution. We, on the\nother hand, use the LLM to generate subquestions\nand solution pairs and use them together to inject\nreasoning capabilities into smaller models.\nSubquestioning as supervision The idea of in-\nquiring or asking information-seeking questions for\ndiscovery learning has been studied well in the past\n(Bruner, 1961). Rao and Daum ´e III generated clar-\nification questions based on Stack Exchange ques-\ntions as supervision, Klein and Nabi (2019) used\na joint question answering model to ask questions\nfrom a given span of text and later answer them,\nand (Rajani et al., 2019; Shwartz et al., 2020) asked\nquestions to improve common sense QA models.\nIn contrast, our work focuses on multistep reason-\ning tasks where intermediate clarifying questions\nand reasoning steps may not always be available\nand may need to be extracted from a teacher model.\n3 Methodology\nThe setting we consider consists of a data set D,\nwhere each problem Pi is accompanied by a final\nanswer ai that can be reached by several steps of\nreasoning. The task of solving the problem using\na model ψis to predict an answer ˆa= ψ(P) such\nthat ˆa = a. We consider different data scenarios\nwhere intermediate annotations of the solution may\nbe available in different forms (e.g., step-by-step,\nas a semantic decomposition by subquestions) or\nmay not be present. Depending on the availability\nof annotations, we propose different approaches\nto augment the training of a small model on Dby\nA robe takes 2 bolts of blue fiber and half \nthat much white fiber.  How many bolts in \ntotal does it take?\n\"\nReasoning Problem\nAnswer-Only: The answer is 3.\nCoT:\nIt takes 2/2=<<2/2=1>>1 bolt of white ﬁber.\nSo the total amount of fabric is \n2+1=<<2+1=3>>3 bolts of fabric.\nThe answer is 3.\nSocratic CoT:\nHow many bolts of white ﬁber does it take?\nIt takes 2/2=<<2/2=1>>1 bolt of white ﬁber.\nHow many bolts in total does it take?\nSo the total amount of fabric is \n2+1=<<2+1=3>>3 bolts of fabric.\nThe answer is 3.\nFigure 2: Illustration of the three different kinds of an-\nnotation structure. Our proposed approach, SOCRATIC\nCOT, augments the typical chain-of-thought step-by-\nstep solution with subquestioning.\nusing LLMs.\n3.1 Distilling step-by-step reasoning via CoT\nA data set may present an annotation that contains\nintermediate reasoning steps that lead to the answer\nai (i.e., a chain-of-thought annotation). This inter-\nmediate annotation can be used directly to fine-tune\na small model. However, in cases where such step-\nby-step information is not available, we use a LLM\nto generate the reasoning steps that might improve\nthe performance of the small model.\nTo achieve this, we consider a small subset of the\ndataset Dand decompose each problem Pi into ni\nintermediate reasoning steps. We construct these\nintermediate reasoning steps manually, since we\nonly need a few examples as prompts (examples\nare provided in Appendix Table 6).\nFor each remaining problem P ∈D, we then\nprompt a large language model Mto generate the\nintermediate reasoning steps. We make sure that\nthe chain of reasoning steps is meaningful by check-\ning whether the last solution matches the ground\ntruth answer, i.e. whether a(ni)\ni = ai, where a(ni)\ni\nrepresents the answer corresponding to the last rea-\nsoning step. If this is not the case, we discard the\nproblem and sample a new chain by prompting the\nmodel again (for a maximum of 3 times). In this\nway, we obtain an augmented dataset D∗in which\na subset of problems is paired with a sequence of\nreasoning steps leading to the correct result. Fi-\n7061\nnally, we can distill the reasoning capabilities into\nsmaller models by fine-tuning them with the gener-\nated intermediate steps.\n3.2 Distilling step-by-step reasoning through\nSOCRATIC COT\nIn this section, we describe how CoT can be en-\nhanced through subquestioning. An illustration of\nour approach is shown in Figure 3.\n3.2.1 Extracting the Reasoning Capability\nfrom the Teacher\nIn Section 3.1, we detailed how an LLM can be\nused to generate the intermediate annotation of a\nproblem Pi as a chain of steps leading to the an-\nswer ai. We now extend this procedure to include a\nsubquestion at each step of the solution. Following\na similar procedure as described in Section 3.1, we\nprompt the LLM with few exemplars of problems\ndecomposed as a set of intermediate subquestion-\nsolution pairs (the prompts are reported in Ap-\npendix Table 6). This way, we obtain an inter-\nmediate annotation that includes subquestioning.\nIn particular, each of the ni steps constituting the\noverall solution is a subquestion-solution pair, de-\nnoted q(j)\ni ,s(j)\ni , j ∈ {1,...,n i}(an example is\nshown in Figure 2). We refer to the ordered list\nof subquestion-solution pairs for problem Pi as\n(q(1)\ni ,s(1)\ni ),..., (q(ni)\ni ,s(ni)\ni ).\n3.2.2 Transferring the Reasoning Capability\ninto the Student\nWe present two strategies to distill the reasoning\nannotation provided by the LLM into smaller mod-\nels.\nIn the first strategy, a single unified student is\ntrained to generate the subquestion-solution pairs\nsimultaneously, while in the second strategy, the\nquestion generation and question-answering tasks\nare assigned to two separate models. We call\nthis second strategy iterative because the question-\nanswering model is trained to solve each subques-\ntion iteratively.\nUnified. Using the problems in Dthat contain\nthe chain of intermediate questions and solutions,\nwe train a unified student model Muni that learns\nto generate the sequence of subquestion-solution\npairs {(q(1),s(1)),(q(2),s(2)),... }that lead to the\nsolution of a given problem. We use a pre-trained\ntransformer-based model (Vaswani et al., 2017) and\ntrain it on the chain of subquestion-solution pairs\nfor each problem P. Given a step jof problem P\n(i.e., the concatenation of q(j) and s(j)) consisting\nof a sequence of mj tokens {x(1)\nj ,...,x (mj )\nj }, we\nuse a typical auto-regressive language modeling\nloss, L:\nLj(P) = −\nmj∑\nk=1\nlog Puni (x(k)\nj |x:(k−1)\nj ,P) (1)\nwhere Puni(x|c) is the probability assigned by\nMuni to token x given context c, and x:(y) indi-\ncates the sequence {x(1),...,x (y)}. The loss Lj\nis computed for each problem Pi and for each pair\n(q(j),s(j)) leading to the final answer ai.\nIterative. The iterative version of the student sep-\narates the tasks of generating the subquestions and\nproviding an intermediate answer to each subques-\ntion into two distinct models: a question generation\n(QG) model and a question answering (QA) model.\nBoth the QG and QA models are implemented us-\ning a Transformer-based language model (Vaswani\net al., 2017). In particular, the QA model Mqa is\niteratively trained to answer the teacher-generated\nsub-questions. The learning objective is computed\nat the token level for each intermediate solution:\nL(P,s(j)) =−\nlj∑\nk=1\nlogPQA(y(k)\nj |y:(k−1)\nj ,q:(j),s:(j−1),P)\nwhere lj and the yj’s represent, respectively, the\nlength and the tokens of the intermediate solution\ns(j). s:(j−1) consists of the previous solution gen-\nerated by the QA model iteratively in the past itera-\ntions.\nSimilarly, the QG model is trained to acquire\nthe ability of the teacher model to decompose the\nproblem’s main question into a series of sub-steps,\neach of which corresponds to a subquestion. The\nloss for this model is analogous to Equation 1, with\nthe only difference being that the intermediate so-\nlutions are not considered for the QG model. Dur-\ning training, the previous intermediate solutions\ngenerated by the QA model are replaced with the\nteacher-generated solutions using teacher forcing\n(Cho et al., 2014). However, the intermediate solu-\ntions generated by the model are used at inference\ntime.\n3.3 Inference-time Predictions\nGiven an unseen problem P, the unified student\nmodel can directly predict a solution as a sequence\n7062\nFine-Tuning\nReasoning Skill TransferReasoning Annotation\n#\n !\n !QG QA\n/uni0302 a(n)\nFinal Answer\nInference\nUnseen Problem\n/uni0302 q(1)\n/uni0302 q(2)\n/uni0302 q(n)\n.\n.\n.\nA robe takes 2 bolts of blue fiber and half \nthat much white fiber.  How many bolts in \ntotal does it take?\n\"\nReasoning Problem\nAnswer: The answer is 3.\nCoT:\nIt takes 2/2=<<2/2=1>>1 bolt of \nwhite ﬁber.\nSo the total amount of fabric is \n2+1=<<2+1=3>>3 bolts of fabric.\nSocratic CoT:\n: How many bolts of white ﬁber does \nit take?\n: It takes 2/2=<<2/2=1>>1 bolt of \nwhite ﬁber.\n: How many bolts in total does it \ntake?\n: So the total amount of fabric is \n2+1=<<2+1=3>>3 bolts of fabric.\n: The answer is 3.\nq(1)\ns(1)\nq(2)\ns(2)\na\n!LLM\n!\n !QG Model QA Model\nq( j)'s (q( j), s( j))'s\nFigure 3: Detailed illustration of our framework. First, a LLM is prompted to decompose the input problem P into\na series of subquestion-solution pairs (q(j)\ni ,s(j)\ni , j ∈{1,...,n i}) with an answer at each step a(j)\ni . The generated\nsubquestions-solutions are used to train two student models: a) the QG model which learns to mimic the LLM’s sub\nquestioning capability and b) the QA model, which learns to solve each subquestion. At the bottom, the inference\nprocess is depicted for an unseen problem and no LLM is involved. The QG model breaks the unseen problem into\nsimpler subquestions and the QA model solves each one of them eventually leading to the final answer a(ni)\ni .\nof subquestions and answers. In the iterative ap-\nproach, we first generate the subquestions condi-\ntioning the generation of the QG model onP. After\nthese questions are generated, they are provided to\nthe QA model one by one, decoding the intermedi-\nate solution ˆs(j) at step jtoken by token according\nto the model’s probability distribution over its vo-\ncabulary:\nPQA(y(k)\nj |y:(k−1)\nj ,ˆq:(j),ˆs:(j−1),P), (2)\nwhere y(k)\nj is the k-th token being decoded in\ngreedy fashion.\nAfter the last solution ˆs(n) has been generated,\nthe numerical prediction ˆa(n) is parsed from the\ntext using simple heuristics.\n4 Empirical Analysis\n4.1 Datasets\nWe study how smaller models can learn to rea-\nson better on three multi-step reasoning datasets:\nGSM8K (Cobbe et al., 2021), StrategyQA (Geva\net al., 2021), and SV AMP (Patel et al., 2021).\nGSM8K consists of 8.5K grade school math word\nproblems, each requiring 2 to 8 steps of reason-\ning to solve. The solutions primarily involve a se-\nquence of elementary calculations using basic arith-\nmetic operations (+, −, ×, ÷). The dataset is di-\nvided into 7.5K training problems and 1K test prob-\nlems. To evaluate the model on SV AMP, we train\nthe model on 761 multi-step math word problems\ntaken from the ASDiv (Miao et al., 2020) training\nset and evaluate it on 237 multi-step SV AMP prob-\nlems. For StrategyQA, the test set with facts is\nnot available, so we split the data into 80% train-\ning, 10% as validation data, and the last 10% as\ntest data. We do not shuffle the data to maintain\nreproducibility.\n4.2 Experimental Setup\nWe use three kinds of annotation, corresponding to\nthe three datasets that we consider.\nStep-by-step solution. The GSM8K dataset falls\ninto this category and includes a Socratic version\nwhere intermediate subquestion-solution pairs are\nprovided for each MWP. While the intermediate\nstep-by-step solutions were manually annotated,\nthe authors report that the subquestions were gen-\nerated by prompting GPT-3. We reproduced a sub-\nset of these subquestions using a GPT-3 model\nwith prompts, and we observed a high similarity\nbetween the questions provided and the ones gen-\n7063\nUnified\nInput: Output:\nA robe takes 2 bolts of blue fiber and half that much white\nfiber. How many bolts in total does it take?\nHow many bolts of white fiber does it take? It takes 2/2\n= <<2/2=1>> 1 bolt of white fiber. How many bolts in\ntotal does it take? So the total amount of fabric is 2+1 =\n<<2+1=3>> 3 bolts of fabric. The answer is 3.\nIterative\nIteration 1\nInput: Output:\nA robe takes 2 bolts of blue fiber and half that much white\nfiber. How many bolts in total does it take?\nQG: How many bolts of white fiber does it take?\nQA: It takes 2/2 = <<2/2=1>> 1 bolt of white fiber.\nIteration 2\nInput: Output:\nA robe takes 2 bolts of blue fiber and half that much white\nfiber. How many bolts in total does it take? How many bolts\nof white fiber does it take? It takes 2/2 = <<2/2=1>> 1 bolt\nof white fiber.\nQG: How many bolts in total does it take?\nQA: So the total amount of fabric is 2+1 = <<2+1=3>> 3\nbolts of fabric. The answer is 3.\nTable 1: Example demonstraing the input-output format for unified vs iterative setup. QG represents the question\ngeneration model and QA is the question answerer model. Note that the QA model uses the QG output to answer it\nas shown in Figure 3.\nerated by us (BERT F1 score of 95%). For SO-\nCRATIC COT, we thus use the subquestioning an-\nnotation already provided.\nSupporting facts. We study the StrategyQA\ndataset, which falls in this category. Strategy\nQA consists of a factual question with binary\nTrue/False as the final answer. Additional support-\ning facts and decomposed questions are provided.\nHowever, the set of facts and the decomposed ques-\ntions provided with a given question are not always\naligned (i.e., a fact is not necessarily the answer to\none subquestion). Therefore, having a setup simi-\nlar to the one for GSM8K is not possible. We thus\nconsider two versions of the data. One in which\nthe supporting facts are used as CoT and the corre-\nsponding questions are generated by prompting a\nGPT-3 model, and a second in which we take the\nprovided questions and generate the facts (this time\naligned with the questions) using GPT-3.\nFinal answers only. AsDiv/SV AMP falls in this\ncategory and for training, we use GPT-3 to gener-\nate both intermediate subquestions and solutions.\nIntermediate solutions are used as CoT and the gen-\nerated subquestion-solution pairs for SOCRATIC\nCOT.\n4.3 Implementation Details\nWe use GPT-2 variants (Radford et al., 2019) as\nstudent models. GPT-3 175B (Brown et al., 2020)\nserved as the teacher model for decomposing com-\nplex problems into a series of simpler substeps (we\nreport the prompts used in Appendix Table 6).\nAll models were trained using the Huggingface\nlibrary (Wolf et al., 2020) on an NVIDIA Tesla\nA100 GPU with 40 GB of memory. Each experi-\nment was run for the same number of iterations to\nensure fairness with periodic evaluation over the\nvalidation set. Teacher forcing was used during\ntraining to replace the generated responses with\nground truth answers from the training dataset.\nEvaluation Metric. To evaluate the question-\nanswering performance on the GSM8K, SV AMP,\nand StrategyQA datasets, we compute the accuracy\nbased on the final answer provided by the student\nmodel.\n5 Results and Discussion\nCan our framework improve the reasoning capa-\nbilities of smaller models? Table 2 demonstrates\nthat leveraging LLMs reasoning capabilities using\nour framework can improve the reasoning results\nfor all dataset types.\nStep-by-Step Solution. When human-annotated\nstep-by-step solutions are available, training\nsmaller models with LLM-generated CoT is not\nadvantageous, as shown on GSM8K. This is to\nbe expected since the annotation generated by an\nLLM is likely to be noisier and of lower quality\nthan human-annotated data. However, the ground-\ntruth step-by-step annotation can be leveraged to\nprompt an LLM to generate subquestions for the\nSOCRATIC COT approach, giving a performance\n7064\nIterative Unified\nDataset Model Answer Only GT Steps GT Facts CoT Soc CoT SocGT SocCoT\nSmall (124M) 1.45 5.05 - 4.70 5.98 6.44 (↑20%) 5.10\nGSM8K Medium (355M) 2.90 7.88 - 7.10 11.57 12.74 (↑38%) 7.90\nLarge (774M) 4.62 14.10 - 12.85 17.89 21.08 (↑33%) 13.25\nGPT-3 (6B) - 21.00 - - - - -\nMedium (355M) 54.10 - 52.02 55.01 52.05 60.31 (↑13%) 52.05\nStrategyQA Large (774M) 61.10 - 62.80 55.90 61.32 66.40 (↑5%) 59. 45\nXL (1.5B) 60.51 - 66.30 58.07 62.30 63.56 ( ↓4%) 62.05\nSmall (124M) 2.15 - - 5.35 6.79 - 5.82\nSV AMP Medium (355M) 4.80 - - 17.30 18.99 - 17.62\nLarge (774M) 7.40 - - 23.60 18.14 - 17.45\nTable 2: Accuracy comparison (in %) on the three considered datasets. We consider three human-annotated\nbaselines: final answers only (Answer Only), ground-truth step-by-step solution (GT Steps), and supporting facts\n(GT Facts). We compare the different supervision strategies for fine-tuning the small models: CoT represents the\ncase where the chain of intermediate reasoning steps is generated by GPT-3, SocCoT represents the case where both\nthe chain of intermediate solutions and the subquestions are generated by LLM and used to fine-tune small models.\nSocGT represents the case where GT solutions/facts are used when prompting GPT-3 to generate the subquestions.\nIterative and Unified represent the two SocCoT strategies described above. All models are GPT-2 versions and their\nsize is reported within parentheses. All experiments were run at least 3 times and the average is reported. GPT-3 6B\nresults are taken from Cobbe et al. (2021).\nboost of up to 38% when the LLM-generated sub-\nquestions are used at inference time. When the\nsubquestions are learned by the QG model (Iter-\native SocCoT ), the accuracy of the student model\ndecreases slightly but still improves over the step-\nby-step annotation without subquestions (17.89 vs.\n14.10). Figure 5 shows a comparison of predictions\ngenerated by SocCoT models and a model trained\non the GT step-by-step annotation. Unified SO-\nCRATIC COT performs similarly to training with\nthe step-wise ground-truth annotation. We addition-\nally include the score produced by GTP-3 6B to\nshow that training with SOCRATIC COT can help\na small model (GPT-2 large with 774M parame-\nters) perform as well as a nearly 10x larger model\nfine-tuned with human annotated data.\nSupporting facts. On StrategyQA, we observe\nthat the inclusion of ground-truth supporting facts\nin the fine-tuning procedure improves the perfor-\nmance of the small models. However, surprisingly,\nwhen the supporting facts are generated by GPT-3,\ntheir inclusion actually hurts performance (58.07\nvs 60.51 for GPT-2 Large). We hypothesize that\nthis is likely due to the imperfect factual knowl-\nedge provided by the LLM, which mars the quality\nof the supervision. We have observed that the GT\nsupporting facts provided often do not represent a\nlogical sequence of propositions leading to the final\nanswer. This is likely the reason why decomposing\nMedium Large XL\nGPT-2 model size\n52\n54\n56\n58\n60\n62\n64\n66\nAccuracy\nAns only\nGT Facts\nCoT\nSocGT\nSocCoT\nFigure 4: Accuracy comparison for different supervi-\nsion strategies on StrategyQA. The baseline method\nconsists of fine-tuning on final answers only (Ans only),\nand it is compared to fine-tuning with: ground-truth sup-\nporting facts (GT Facts), GPT-3-generated supporting\nfacts (CoT), ground-truth supporting facts with GPT-3-\ngenerated subquestions (SocCoT ), and LLM-generated\nfacts with human-annotated subquestions (SocGT ).\nthe problem through subquestions based on such\nfacts actually harms accuracy (see SocCoT column\nin Table 2). Instead, using the provided subques-\ntions and using an LLM to generate the answers\n(representing coherent facts leading to the final\nanswer) proves to be an effective strategy (60.31\nvs. 52.02 for GPT-2 Medium). A more detailed\ncomparison between our proposed approaches is\npresented in Figure 4. However, GPT-2 XL mod-\n7065\nProblem It takes 2*2=<<2*2=4>>4 white \nfibers So it takes 2*4=<<2*4=8>>8 \nblue fibers That means it takes \n8*2=<<8*2=16>>16 fibers. The \nanswer is 16.\nHow many bolts of white fiber does \nit take? It takes 2/2=<<2/2=1>>1 \nbolt of white fiber. How many bolts \nin total does it take? So it takes \n2+1=<<2+1=3>>3 bolts in total.  \nThe answer is 3.\n: Tom gets 4 car washes a month.  \nIf each car wash costs $15 how \nmuch does he pay in a year? \nGT Solution: \ne gets 4*12=<<4*12=48>>48 car \nwashes a year. That means it cost \n48*15=$<<48*15=720>>720. The \nanswer is 720.\nP\nProblemHe gets 4*15=<<4*15=60>>60 car \nwashes a year. So he pays \n60*12=$<<60*12=720>>720 a year \nThe answer is 720.\nTraining w/ GT Steps\nHow many car washes does Tom \nget in a year? He gets \n4*12=<<4*12=48>>48 car washes a \nyear. \nHow much does Tom pay in a year? \nThat means he pays \n48*15=$<<48*15=720>>720 a year. \nThe answer is 720.\nTraining w/ SocCoT\n(correct ﬁnal answer \n but wrong reasoning)\n: A robe takes 2 bolts of blue fiber \nand half that much white fiber.  How \nmany bolts in total does it take? \nGT Solution: \nIt takes 2/2=<<2/2=1>>1 bolt of \nwhite fiber So the total amount of \nfabric is 2+1=<<2+1=3>>3 bolts of \nfabric. The answer is 3. \nP\nTraining w/ SocCoT\nTraining w/ GT Steps\nFigure 5: Example of predictions generated by a GPT-2\nLarge model fine-tuned with GT steps and SOCRATIC\nCOT on GSM8K dataset.\nModels Methodology Accuracy\nGPT-3 (1-shot) CoT 27.5\n(175B) Sub-ques 47.1 (↑ 41%)\nTable 3: Accuracy comparison (in %) of using CoT vs\nSOCRATIC COT (Sub-ques) on the GSM8K dataset for\nGPT-3 model with prompting.\nels perform well when trained on facts as unlike\nsmaller models, larger models can encode more\nfacts at once in their parameters, which assists in\nanswering a factual question.\nAnswers only. On the SV AMP dataset, which\nincludes only final answers and no intermediate\nannotation, LLMs can be used to generate both\nthe intermediate steps and the subquestions. Both\nthe consideration of intermediate solutions without\nsubquestions (CoT) and the consideration of inter-\nmediate solutions with subquestions (SocCoT ) lead\nto an improvement in performance. The trend here\nis similar to what was observed for StrategyQA,\nwith SOCRATIC COT being more effective for the\ntwo smaller models but falling back to CoT for the\nlarger model.\nCan SOCRATIC COT be used as a prompt-\ning strategy? We experimented with SOCRATIC\nCOT as a prompting strategy. First, we prompted\nGPT-3 (175B) to decompose the main problem into\nsimpler steps by formulating subquestions. Then,\nGPT-3 is used again to solve the sequence of sub-\nproblems in a single-shot setting with a problem\ndecomposed into intermediate subquestions and so-\nlutions included in the prompt. The introduction\nof subquestioning boosts accuracy by over 40%\ncompared to standard CoT prompting (Table 3).\nOther work (e.g., Wei et al. 2022b) has used a\nlarger number of exemplars in the few-shot prompt,\nachieving higher overall accuracy. We limited our\nexperiments to single-shot prompts due to budget\nconstraints.\n6 Ablation Studies\nIn this Section, we describe additional analyses\nregarding specific components of the framework\nwe propose, as well as negative results that we\nobtained with alternative strategies.\nHow good are the sub-questioning capabilities\nof a smaller model? We investigate in more de-\ntail the ability of a small model to decompose a\nproblem by generating meaningful subquestions.\nWe fine-tuned GPT-2 Large on the GPT-3 gener-\nated subquestions provided in the GSM8K dataset.\nWe then evaluated the quality of the generated ques-\ntions in terms of BLEU score (Post, 2018), BERT\nF1 score (Zhang et al., 2019), and by measuring\nfor how many problems the number of questions\ngenerated by GPT-2 (#Q) matches the number of\nGPT-3 annotated questions for a given problem.\nWe found that the fine-tuned GPT-2 predicted\nan incorrect number of subquestions for the ma-\njority of problems (see Table 4, first row). Thus,\nfollowing previous work on subquestion generation\n(Shridhar et al., 2022), we introduced a guidance\nmechanism that conditions the generation of sub-\nquestions for a problem P on the equations describ-\ning the intermediate solutions of P. This strategy\nimproved the quality of the generated questions\nfor all three metrics considered (Table 4, second\nrow). To avoid the dependence on the step-by-step\nannotation of the equations for each problem P\nat inference time, we train an additional sequence-\nto-sequence model to predict, given P, the set of\nequations that lead to the solution of the problem.\nAt inference time, the predictions for the guidance\nmodel are used to condition the generation by the\nQG model. Although the predicted equations often\ndo not lead to the correct solution of the problem,\nthey help the QG model to generate more meaning-\n7066\nMethodology BLEU BERT F1 # Q\nNo-guidance 51.5 0.78 0.42\nGuidance 58.8 0.81 0.80\nTable 4: BLEU, BERT F1 and the number of questions\n(# Q) comparison between the question generator model\nand the Socratic subquestions present in the GSM8K\ndataset using GPT2-large model.\nAns Only No guide Guide\n0\n1\n2\n3\n4\n5\n6\nAccuracy\nSmall\nAns Only No guide Guide\n0\n2\n4\n6\n8\n10\n12\nMedium\nAns Only No guide Guide\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nLarge\nFigure 6: Accuracy of student models (QA + QG) when\nthe question generation is conditioned using the guid-\nance model (Guide) and with non-guided question gen-\neration (No guide). Ans only represents the baseline.\nAll models are GPT-2 versions.\nful sub-questions. Figure 6 shows the overall accu-\nracy of the GPT-2 student models (QA + QG) fine-\ntuned with SOCRATIC COT on the GSM8K data\nwith and without equation conditioning provided by\nthe guide model. We have extended this guidance\nmechanism to StrategyQA and SV AMP, where the\ngeneration of subquestions is conditioned on the\nnumber of facts (StrategyQA) or steps (SV AMP)\nneeded to answer the problem.\nEliminating the need for a subquestion module.\nWe have experimented with an alternative training\nsolution that does not involve a question-generation\nmodel. This strategy aims to improve the su-\npervision for fine-tuning a small model through\nsubquestioning, but without relying on the pres-\nence of subquestions at test time. The procedure\nconsists of training the student model to gener-\nate the entire chain of steps leading to an inter-\nmediate answer. That is, when the sub-question\nq(1) is asked, the model is trained to generate the\nanswer s(1), but when q(j) is asked, the model\nis trained to generate the chain of thought rea-\nsoning {s(1),s(2),...,s (j)}(instead of just s(j)).\nThis eliminates the need for the intermediate sub-\nquestions at inference time, as the model is trained\nto implicitly decompose the main problem into\nsmaller reasoning steps. However, this method\nGPT-2 No SubQ SubQ with QG\nSmall 2.70 5.98\nMedium 7.20 11.57\nLarge 8.18 17.89\nTable 5: Accuracy comparison (in %) of student models\ntrained with (SubQ with QG) and without (No SubQ)\nquestion generation model on GSM8K.\nProblem Haley has <<+ number0 number1>> \ntrees. Haley has <<- + number0 \nnumber1 number2>> trees left \nHow many trees did haley grow in \ntotal? \nHaley grew <<+ number0 \nnumber2>> trees in total. \nHow many trees does she have? \nHaley has <<- + number0 number2 \nnumber1>> trees left.\n: Faye was placing her pencils into \nrows with number0 pencils in each \nrow. She had number1 packs of \npencils each one having number2 \npencils. How many rows could she \nmake?  \nGT Solution: \n<</ + number0 number1 number2>>\nP\nProblem Faye put <<+ number0 number1>> \npencils into each row. Faye could \nmake <</ + number0 number1 \nnumber2>> rows.\nTraining w/ GT Steps\nHow many pencils did Faye have in \ntotal? Faye had <<+ number1 \nnumber2>> pencils in total. How \nmany rows could she make?Faye \ncould make <</ + number1 number2 \nnumber0>> rows.\nTraining w/ SocCoT\n: Haley grew number0 trees in her \nbackyard. After a typhoon number1 \ndied. Then she grew number2 more \ntrees. How many trees does she \nhave left?\nGT Solution: \n<<- + number0 number2 number1>> \nP\nTraining w/ SocCoT\nTraining w/ GT Steps\nFigure 7: Example of predictions generated by a GPT-2\nMedium model fine-tuned with GT steps andSOCRATIC\nCOT on the SV AMP dataset.\nleads to significant performance degradation (re-\nsults are reported in Table 5), highlighting the need\nfor subquestions at inference time.\nExample outputs In Figures 5 and 7, we report\nexample outputs predicted by GPT-2 models for a\nset of GSM8K and SV AMP problems.\n7 Conclusion\nThe chain-of-thought style of step-by-step reason-\ning has proven to be very effective for reasoning\nin LLMs. In this work, we propose ways to distill\nthese reasoning capabilities into smaller models\nand suggest ways to further improve them by ex-\nplicitly asking stepwise questions. We demonstrate\nthe effectiveness of our proposed methodology on\nthree popular multi-step reasoning datasets, and dis-\ncuss cases where one method should be preferred\nover the other for different datasets.\n7067\nLimitations\nIn our work, we use only one solution from the\nLLM to distill information into the student model,\nand according to Wang et al. (2022), multiple\nsubquestion-solution pairs can be sampled, and\nusing majority voting, all pairs leading to the most\nfrequent answer can be used to distill knowledge\ninto the student models. Also, due to computational\nbudget, we used a single prompt to compare the\nCoT and SOCRATIC COT and using more prompts\n(up to 8) might lead to a fairer comparison and\nbetter results (Wei et al., 2022b). We leave these\nexperiments for the future.\nEthical Considerations\nAlthough this work improves the reasoning capa-\nbilities of smaller models, the models are still not\npowerful enough to be used in sensitive settings\nsuch as education. We plan to release our code and\nmodel checkpoints, but the models must be used\ncarefully by users, as many generative models, in-\ncluding ours, are prone to hallucination.\nAcknowledgements\nAlessandro Stolfo is supported by Armasuisse Sci-\nence and Technology through a CYD Doctoral Fel-\nlowship.\nReferences\nAida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-\nKedziorski, Yejin Choi, and Hannaneh Hajishirzi.\n2019. Mathqa: Towards interpretable math word\nproblem solving with operation-based formalisms.\narXiv preprint arXiv:1905.13319.\nJimmy Ba and Rich Caruana. 2014. Do deep nets really\nneed to be deep? Advances in neural information\nprocessing systems, 27.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJerome S Bruner. 1961. The act of discovery. Harvard\neducational review, 31:21–32.\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint\narXiv:1406.1078.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veri-\nfiers to solve math word problems. arXiv preprint\narXiv:2110.14168.\nJacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael\nCollins, and David Mimno. 2022. Honest students\nfrom untrusted teachers: Learning an interpretable\nquestion-answering pipeline from a pretrained lan-\nguage model. arXiv preprint arXiv:2210.02498.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did Aristo-\ntle Use a Laptop? A Question Answering Bench-\nmark with Implicit Reasoning Strategies. Transac-\ntions of the Association for Computational Linguis-\ntics (TACL).\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\n7068\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\nEtzioni, and Nate Kushman. 2014. Learning to solve\narithmetic word problems with verb categorization.\nIn Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 523–533.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve. arXiv\npreprint arXiv:2210.11610.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, D´aniel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022. Opt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017.\nTassilo Klein and Moin Nabi. 2019. Learning to answer\nby learning to ask: Getting the best of gpt-2 and bert\nworlds. arXiv preprint arXiv:1911.02365.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nNate Kushman, Yoav Artzi, Luke Zettlemoyer, and\nRegina Barzilay. 2014. Learning to automatically\nsolve algebra word problems. In Proceedings of the\n52nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n271–281, Baltimore, Maryland. Association for Com-\nputational Linguistics.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, et al. 2022. Solving quantitative\nreasoning problems with language models. arXiv\npreprint arXiv:2206.14858.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen,\nXinlu Zhang, Zekun Li, Hong Wang, Jing Qian,\nBaolin Peng, Yi Mao, et al. 2022. Explanations from\nlarge language models make small reasoners better.\narXiv preprint arXiv:2210.06726.\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and developing\nEnglish math word problem solvers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 975–984, Online.\nAssociation for Computational Linguistics.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nAndreas Opedal, Niklas Stoehr, Abulhair Saparov, and\nMrinmaya Sachan. 2023. World models for math\nstory problems. In Findings of the Association\nfor Computational Linguistics: ACL 2023, Toronto,\nCanada.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are nlp models really able to solve\nsimple math word problems? arXiv preprint\narXiv:2103.07191.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain Yourself!\nLeveraging Language Models for Commonsense Rea-\nsoning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4932–4942, Florence, Italy. Association for Compu-\ntational Linguistics.\nSudha Rao and Hal Daum´e III. Answer-based Adversar-\nial Training for Generating Clarification Questions.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers).\nSubhro Roy, Tim Vieira, and Dan Roth. 2015. Reason-\ning about quantities in natural language. Transac-\ntions of the Association for Computational Linguis-\ntics, 3:1–13.\nKumar Shridhar, Jakub Macina, Mennatallah El-Assady,\nTanmay Sinha, Manu Kapur, and Mrinmaya Sachan.\n2022. Automatic generation of socratic subquestions\nfor teaching math word problems. arXiv preprint\narXiv:2211.12835.\nVered Shwartz, Peter West, Ronan Le Bras, Chandra\nBhagavatula, and Yejin Choi. 2020. Unsupervised\ncommonsense question answering with self-talk. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4615–4629, Online. Association for Computa-\ntional Linguistics.\nCharlie Snell, Dan Klein, and Ruiqi Zhong. 2022.\nLearning by distilling context. arXiv preprint\narXiv:2209.15189.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\n7069\nAdri`a Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nAlessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bern-\nhard Sch ¨olkopf, and Mrinmaya Sachan. 2023. A\ncausal framework to quantify the robustness of math-\nematical reasoning with language models. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), Toronto, Canada. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. ArXiv, abs/2203.11171.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nJipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan\nWang, Jie Shao, and Ee-Peng Lim. 2020. Graph-to-\ntree learning for solving math word problems. Asso-\nciation for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-\nto-most prompting enables complex reasoning in\nlarge language models. ArXiv, abs/2205.10625.\n7070\nLet’s generate sub-questions for these problems. Use exactly one operation per step.\n—\nQ: Zoe was unboxing some of her old winter clothes . She found number0 boxes of clothing and inside each box there were\nnumber1 scarves and number2 mittens . How many pieces of winter clothing did Zoe have total ?\nSQ1: How many pieces of winter clothing did Zoe have in each box?\nA1: Zoe had <<+ number1 number2>> pieces of winter clothing in each box.\nSQ2: How many pieces of winter clothing did Zoe have total ?\nA2: Zoe had <<* number0 + number1 number2>> pieces of winter clothing in total.\n—\nQ: Katie picked number0 tulips and number1 roses to make flower bouquets . If she only used number2 of the flowers though\n, how many extra flowers did Katie pick ?\nSQ1: How many flowers did Katie pick in total?\nA1: Katie picked <<+ number0 number1>> flowers in total.\nSQ2: How many extra flowers did Katie pick ?\nA2: Katie picked <<- + number0 number1 number2>> extra flowers.\n—\nQ: Conner has number0 dollars in his bank account . Every month he spends number1 dollars . He does not add money to the\naccount . How much money will Conner have in his account after number2 months ?,\nSQ1: How much money does Conner spend in total? A1: Conner spends<<* number1 number2>> dollars.\nSQ2: How much money will Conner have in his account after 8.0 months ? A2: After 8.0 months, Conner will have ¡¡-\nnumber0 * number1 number2>> dollars.\nFor each of the following topics, generate intermediate answers to the subquestions leading to the final answer.\n—\nTopic: Albany, Georgia (City in Georgia, United States)\nWill the Albany in Georgia reach a hundred thousand occupants before the one in New York?\nAlbany, GA has around 75,000 people.\nAlbany, NY has almost 100,000 people.\nThe difference is 100,000-75,000=25,000\nThe difference is 100,000-100,000=0\nNo, 25,000 is not smaller than 0.\nThe final answer is NO.\n—\nTopic: The Police (English rock band)\nCould the members of The Police perform lawful arrests?\nOnly law enforcement officers can perform lawful arrests.\nNo, the members of The Police (rock band) are not law enforcement officers.\nThe final answer is NO.\n—\nTopic: Wonder Woman (2017 film) (American superhero film directed by Patty Jenkins) Is a Boeing 737 cost covered by\nWonder Woman (2017 film) box office receipts?\nThe average cost of a US Boeing 737 plane is 1.6 million dollars.\nWonder Woman (2017 film) grossed over 800 million dollars at the box office.\nYes, 800 is larger than 1.6.\nThe final answer is YES.\nTable 6: Exemplars included in the few-shot prompt for the decomposition of the problems from the ASDiv (upper\nrow) and StrategyQA (lower row) datasets.\n7071\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitation\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthical considerations\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3, Methodology\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4.2\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nConclusion\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nOur models are free to be used by anyone. We mention the limitations of our approach\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe used standard open source datasets\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4.2\nC □\u0013 Did you run computational experiments?\nSection 4.3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4.3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7072\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4.3\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4.3, Table 1\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4.3\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n7073"
}