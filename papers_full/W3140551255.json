{
  "title": "LoFTR: Detector-Free Local Feature Matching with Transformers",
  "url": "https://openalex.org/W3140551255",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2348205815",
      "name": "Sun Jia-ming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3018660208",
      "name": "Shen Zehong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378634878",
      "name": "Wang, Yuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A86173022",
      "name": "Bao, Hujun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973944412",
      "name": "Zhou, Xiaowei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3107540572",
    "https://openalex.org/W2754925132",
    "https://openalex.org/W2117228865",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3046043768",
    "https://openalex.org/W2970172541",
    "https://openalex.org/W3043075211",
    "https://openalex.org/W3034411221",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3034275286",
    "https://openalex.org/W2962705366",
    "https://openalex.org/W2963760790",
    "https://openalex.org/W3035477606",
    "https://openalex.org/W3096678291",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2928800630",
    "https://openalex.org/W2951870616",
    "https://openalex.org/W2046166954",
    "https://openalex.org/W2594519801",
    "https://openalex.org/W2090518410",
    "https://openalex.org/W3001017583",
    "https://openalex.org/W3104213423",
    "https://openalex.org/W3107939686",
    "https://openalex.org/W2963210849",
    "https://openalex.org/W2320444803",
    "https://openalex.org/W3106462076",
    "https://openalex.org/W3093386634",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3104100999",
    "https://openalex.org/W2967640080",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W2177274842",
    "https://openalex.org/W3167674261",
    "https://openalex.org/W2963748588",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2558625610",
    "https://openalex.org/W2963674285",
    "https://openalex.org/W3092135995",
    "https://openalex.org/W3035387557",
    "https://openalex.org/W3103187163",
    "https://openalex.org/W1953691509"
  ],
  "abstract": "We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. In contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive field provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. LoFTR also ranks first on two public benchmarks of visual localization among the published methods.",
  "full_text": "LoFTR: Detector-Free Local Feature Matching with Transformers\nJiaming Sun1,2∗ Zehong Shen1∗ Yuang Wang1∗ Hujun Bao1 Xiaowei Zhou1†\n1Zhejiang University 2SenseTime Research\nAbstract\nWe present a novel method for local image feature\nmatching. Instead of performing image feature detection,\ndescription, and matching sequentially, we propose to ﬁrst\nestablish pixel-wise dense matches at a coarse level and\nlater reﬁne the good matches at a ﬁne level. In contrast\nto dense methods that use a cost volume to search corre-\nspondences, we use self and cross attention layers in Trans-\nformer to obtain feature descriptors that are conditioned on\nboth images. The global receptive ﬁeld provided by Trans-\nformer enables our method to produce dense matches in\nlow-texture areas, where feature detectors usually strug-\ngle to produce repeatable interest points. The experiments\non indoor and outdoor datasets show that LoFTR outper-\nforms state-of-the-art methods by a large margin. LoFTR\nalso ranks ﬁrst on two public benchmarks of visual local-\nization among the published methods. Code is available at\nour project page: https://zju3dv.github.io/loftr/.\n1. Introduction\nLocal feature matching between images is the corner-\nstone of many 3D computer vision tasks, including structure\nfrom motion (SfM), simultaneous localization and mapping\n(SLAM), visual localization, etc. Given two images to be\nmatched, most existing matching methods consist of three\nseparate phases: feature detection, feature description, and\nfeature matching. In the detection phase, salient points like\ncorners are ﬁrst detected as interest points from each im-\nage. Local descriptors are then extracted around neigh-\nborhood regions of these interest points. The feature de-\ntection and description phases produce two sets of interest\npoints with descriptors, the point-to-point correspondences\nof which are later found by nearest neighbor search or more\nsophisticated matching algorithms.\nThe use of a feature detector reduces the search space of\nmatching, and the resulted sparse correspondences are sufﬁ-\ncient for most tasks, e.g., camera pose estimation. However,\na feature detector may fail to extract enough interest points\n∗The ﬁrst three authors contributed equally. The authors are afﬁliated\nwith the State Key Lab of CAD&CG and ZJU-SenseTime Joint Lab of 3D\nVision. †Corresponding author: Xiaowei Zhou.\nFigure 1: Comparison between the proposed method\nLoFTR and the detector-based method SuperGlue [37].\nThis example demonstrates that LoFTR is capable of ﬁnd-\ning correspondences on the texture-less wall and the ﬂoor\nwith repetitive patterns, where detector-based methods\nstruggle to ﬁnd repeatable interest points.1\nthat are repeatable between images due to various factors\nsuch as poor texture, repetitive patterns, viewpoint change,\nillumination variation, and motion blur. This issue is espe-\ncially prominent in indoor environments, where low-texture\nregions or repetitive patterns sometimes occupy most areas\nin the ﬁeld of view. Fig. 1 shows an example. Without re-\npeatable interest points, it is impossible to ﬁnd correct cor-\nrespondences even with perfect descriptors.\nSeveral recent works [34, 33, 19] have attempted to rem-\nedy this problem by establishing pixel-wise dense matches.\nMatches with high conﬁdence scores can be selected from\nthe dense matches, and thus feature detection is avoided.\nHowever, the dense features extracted by convolutional neu-\nral networks (CNNs) in these works have limited receptive\nﬁeld which may not distinguish indistinctive regions. In-\nstead, humans ﬁnd correspondences in these indistinctive\nregions not only based on the local neighborhood, but with\na larger global context. For example, low-texture regions in\n1Only the inlier matches after RANSAC are shown. The green color\nindicates a match with epipolar error smaller than 5 ×10−4 (in the nor-\nmalized image coordinates).\n1\narXiv:2104.00680v1  [cs.CV]  1 Apr 2021\nFig. 1 can be distinguished according to their relative po-\nsitions to the edges. This observation tells us that a large\nreceptive ﬁeld in the feature extraction network is crucial.\nMotivated by the above observations, we propose Lo-\ncal Feature TRansformer (LoFTR), a novel detector-free\napproach to local feature matching. Inspired by seminal\nwork SuperGlue [37], we use Transformer [48] with self\nand cross attention layers to process (transform) the dense\nlocal features extracted from the convolutional backbone.\nDense matches are ﬁrst extracted between the two sets of\ntransformed features at a low feature resolution ( 1/8 of the\nimage dimension). Matches with high conﬁdence are se-\nlected from these dense matches and later reﬁned to a sub-\npixel level with a correlation-based approach. The global\nreceptive ﬁeld and positional encoding of Transformer en-\nable the transformed feature representations to be context-\nand position-dependent. By interleaving the self and cross\nattention layers multiple times, LoFTR learns the densely-\narranged globally-consented matching priors exhibited in\nthe ground-truth matches. A linear transformer is also\nadopted to reduce the computational complexity to a man-\nageable level.\nWe evaluate the proposed method on several image\nmatching and camera pose estimation tasks with indoor and\noutdoor datasets. The experiments show that LoFTR out-\nperforms detector-based and detector-free feature matching\nbaselines by a large margin. LoFTR also achieves state-\nof-the-art performance and ranks ﬁrst among the published\nmethods on two public benchmarks of visual localization.\nCompared to detector-based baseline methods, LoFTR can\nproduce high-quality matches even in indistinctive regions\nwith low-textures, motion blur, or repetitive patterns.\n2. Related Work\nDetector-based Local Feature Matching.Detector-based\nmethods have been the dominant approach for local fea-\nture matching. Before the age of deep learning, many\nrenowned works in the traditional hand-crafted local fea-\ntures have achieved good performances. SIFT [26] and\nORB [35] are arguably the most successful hand-crafted\nlocal features and are widely adopted in many 3D com-\nputer vision tasks. The performance on large viewpoint\nand illumination changes of local features can be signif-\nicantly improved with learning-based methods. Notably,\nLIFT [51] and MagicPoint [8] are among the ﬁrst success-\nful learning-based local features. They adopt the detector-\nbased design in hand-crafted methods and achieve good\nperformance. SuperPoint [9] builds upon MagicPoint and\nproposes a self-supervised training method through homo-\ngraphic adaptation. Many learning-based local features\nalong this line [32, 11, 25, 28, 47] also adopt the detector-\nbased design.\nThe above-mentioned local features use the nearest\nneighbor search to ﬁnd matches between the extracted in-\nterest points. Recently, SuperGlue [37] proposes a learning-\nbased approach for local feature matching. SuperGlue ac-\ncepts two sets of interest points with their descriptors as\ninput and learns their matches with a graph neural net-\nwork (GNN), which is a general form of Transformers [16].\nSince the priors in feature matching can be learned with a\ndata-driven approach, SuperGlue achieves impressive per-\nformance and sets the new state of the art in local feature\nmatching. However, being a detector-dependent method,\nit has the fundamental drawback of being unable to detect\nrepeatable interest points in indistinctive regions. The at-\ntention range in SuperGlue is also limited to the detected\ninterest points only. Our work is inspired by SuperGlue in\nterms of using self and cross attention in GNN for message\npassing between two sets of descriptors, but we propose a\ndetector-free design to avoid the drawbacks of feature de-\ntectors. We also use an efﬁcient variant of the attention lay-\ners in Transformer to reduce the computation costs.\nDetector-free Local Feature Matching. Detector-free\nmethods remove the feature detector phase and directly pro-\nduce dense descriptors or dense feature matches. The idea\nof dense features matching dates back to SIFT Flow [23].\n[6, 39] are the ﬁrst learning-based approaches to learn pixel-\nwise feature descriptors with the contrastive loss. Similar to\nthe detector-based methods, the nearest neighbor search is\nusually used as a post-processing step to match the dense\ndescriptors. NCNet [34] proposed a different approach by\ndirectly learning the dense correspondences in an end-to-\nend manner. It constructs 4D cost volumes to enumer-\nate all the possible matches between the images and uses\n4D convolutions to regularize the cost volume and enforce\nneighborhood consensus among all the matches. Sparse\nNCNet [33] improves upon NCNet and makes it more ef-\nﬁcient with sparse convolutions. Concurrently with our\nwork, DRC-Net [19] follows this line of work and proposes\na coarse-to-ﬁne approach to produce dense matches with\nhigher accuracy. Although all the possible matches are con-\nsidered in the 4D cost volume, the receptive ﬁeld of 4D con-\nvolution is still limited to each matches’ neighborhood area.\nApart from neighborhood consensus, our work focuses on\nachieving global consensus between matches with the help\nof the global receptive ﬁeld in Transformers, which is not\nexploited in NCNet and its follow-up works. [24] proposes\na dense matching pipeline for SfM with endoscopy videos.\nThe recent line of research [46, 45, 44, 15] that focuses on\nbridging the task of local feature matching and optical ﬂow\nestimation, is also related to our work.\nTransformers in Vision Related Tasks.Transformer [48]\nhas become the de facto standard for sequence modeling\nin natural language processing (NLP) due to their simplic-\n2\n<latexit sha1_base64=\"HysyoDDQ8gG0PfCEBIBLGTDKrqQ=\">AAAC4nicjVHLSsNAFD3GV62vqksRgkVwVRJBdCkWxIUUBfsAW8pkOq3BNAmTiSilK3fuxK0/4FY/RvwD/QvvjBF8IDohyZlz7zkz914vDvxEOc7ziDU6Nj4xmZvKT8/Mzs0XFhZrSZRKLqo8CiLZ8FgiAj8UVeWrQDRiKVjfC0TdOyvreP1cyMSPwmN1GYtWn/VCv+tzpohqF1aaSlworztwS/ZBxFlg7wmmUinscqUybBeKTskxy/4J3AwUka3DqPCEJjqIwJGiD4EQinAAhoSeE7hwEBPXwoA4Scg3cYEh8qRNKUtQBiP2jL492p1kbEh77ZkYNadTAnolKW2skSaiPElYn2abeGqcNfub98B46rtd0t/LvPrEKpwS+5fuI/O/Ol2LQhfbpgafaooNo6vjmUtquqJvbn+qSpFDTJzGHYpLwtwoP/psG01iate9ZSb+YjI1q/c8y03xqm9JA3a/j/MnqG2U3M2Sc7RR3NnNRp3DMlaxTvPcwg72cYgqeV/hHg94tDrWtXVj3b6nWiOZZglflnX3BtsZmoY=</latexit>\n1. Local Feature CNN\n<latexit sha1_base64=\"vJMgTC04/pe/nXyxUdFKMCmxugU=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkVwISURRZe1biy4qGAfUFtJptMamheTiVCKO3/ArX6Y+Af6F94ZI6hFdEKSM+eec2fuvW7se4m0rJecMTM7N7+QXywsLa+srhXXN5pJlArGGyzyI9F2nYT7Xsgb0pM+b8eCO4Hr85Y7OlXx1i0XiReFl3Ic827gDENv4DFHEtWq9U72ar3qdbFklS29zGlgZ6CEbNWj4jOu0EcEhhQBOEJIwj4cJPR0YMNCTFwXE+IEIU/HOe5QIG9KKk4Kh9gRfYe062RsSHuVM9FuRqf49ApymtghT0Q6QVidZup4qjMr9rfcE51T3W1MfzfLFRArcUPsX75P5X99qhaJAY51DR7VFGtGVceyLKnuirq5+aUqSRli4hTuU1wQZtr52WdTexJdu+qto+OvWqlYtWeZNsWbuiUN2P45zmnQ3C/bh2Xr4qBUqWajzmML29ileR6hgjPU0dBVPuART8a5IYyxMfmQGrnMs4lvy7h/B/EskUI=</latexit>\nI\nA\n,I\nB\n<latexit sha1_base64=\"LTlZo7p0bI0j1k3e8BPEOnIGqhk=\">AAACDnicbZDNSsNAEMcnftb60ahHL8EieCpJQeyx6MVjBfsBbSmb7aRdutmE3Y1QQt7Bu1d9BW/i1VfwDXwMt20E2/qHgT//mWGGnx9zprTrflkbm1vbO7uFveL+weFRyT4+aakokRSbNOKR7PhEIWcCm5ppjp1YIgl9jm1/cjvrtx9RKhaJBz2NsR+SkWABo0SbaGCXeoJRDCShqZeltWxgl92KO5ezbrzclCFXY2B/94YRTUIUmnKiVNdzY91PidSMcsyKvURhTOiEjLBrrCAhqn46fzxzLkwydIJImhLamad/N1ISKjUNfTMZEj1Wq71Z+F+vm+ig1k+ZiBONgi4OBQl3dOTMKDhDJpFqPjWGUMnMrw4dE0NBG1ZLV37xZEWDxlsFsW5a1Yp3VXHvq+X6TQ6pAGdwDpfgwTXU4Q4a0AQKCTzDC7xaT9ab9W59LEY3rHznFJZkff4A2EacaQ==</latexit>\n1\n/ 8\n<latexit sha1_base64=\"8qAoK4O1KPUlff4rINa8ECB6M6U=\">AAACxHicjVHLSsNAFD2Nr1pfVZdugkVwVRJRdFkUxGUL9gFaJJlO69A0CZOJUIr+gFv9NvEP9C+8M05BLaITkpw5954zc+8N00hkyvNeC87c/MLiUnG5tLK6tr5R3txqZUkuGW+yJEpkJwwyHomYN5VQEe+kkgejMOLtcHim4+07LjORxJdqnPLuKBjEoi9YoIhq+Dflilf1zHJngW9BBXbVk/ILrtFDAoYcI3DEUIQjBMjouYIPDylxXUyIk4SEiXPco0TanLI4ZQTEDuk7oN2VZWPaa8/MqBmdEtErSelijzQJ5UnC+jTXxHPjrNnfvCfGU99tTP/Qeo2IVbgl9i/dNPO/Ol2LQh8npgZBNaWG0dUx65Kbruibu1+qUuSQEqdxj+KSMDPKaZ9do8lM7bq3gYm/mUzN6j2zuTne9S1pwP7Pcc6C1kHVP6p6jcNK7dSOuogd7GKf5nmMGi5QR9N4P+IJz865EzmZk3+mOgWr2ca35Tx8AMO/jzo=</latexit>\n1\n<latexit sha1_base64=\"nEYe5yr7/RvgwRCNItFzePeULbE=\">AAAC2XicjVHLSsNAFD2Nr1pf8bFzEyyCG0tSEF1W3bhwUdE+oJaSpNMazIvJRKjFhTtx6w+41R8S/0D/wjtjCmoRnZDkzLn3nJl7rxP7XiJM8zWnTUxOTc/kZwtz8wuLS/rySj2JUu6ymhv5EW86dsJ8L2Q14QmfNWPO7MDxWcO5PJTxxhXjiReFZ2IQs3Zg90Ov57m2IKqjr50yv7e9LwQLJWEc2wPGO3rRLJlqGePAykAR2apG+gvO0UUEFykCMIQQhH3YSOhpwYKJmLg2hsRxQp6KM9ygQNqUshhl2MRe0rdPu1bGhrSXnolSu3SKTy8npYFN0kSUxwnL0wwVT5WzZH/zHipPebcB/Z3MKyBW4ILYv3SjzP/qZC0CPeypGjyqKVaMrM7NXFLVFXlz40tVghxi4iTuUpwTdpVy1GdDaRJVu+ytreJvKlOycu9muSne5S1pwNbPcY6Derlk7ZTMk3KxcpCNOo91bGCL5rmLCo5QRY28r/GIJzxrLe1Wu9PuP1O1XKZZxbelPXwA336XgA==</latexit>\nSelf-Attention Layer\n<latexit sha1_base64=\"FKqZnrEocY6yDqcbtjrp2fWEyhA=\">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJlgEN5a0ILqsduPCRQX7AFtKkk5raF7MTIRS3LgTt/6AW/0g8Q/0L7wzpqAW0QlJzpx7z5m59zqx7wlpWa8ZY2Z2bn4hu5hbWl5ZXcuvbzRElHCX1d3Ij3jLsQXzvZDVpSd91oo5swPHZ01nWFXx5jXjwovCCzmKWSewB6HX91xbEtXNb1V5JMT+sZQsVIx5Zo8Y7+YLVtHSy5wGpRQUkK5alH9BGz1EcJEgAEMISdiHDUHPJUqwEBPXwZg4TsjTcYYb5EibUBajDJvYIX0HtLtM2ZD2ylNotUun+PRyUprYJU1EeZywOs3U8UQ7K/Y377H2VHcb0d9JvQJiJa6I/Us3yfyvTtUi0ceRrsGjmmLNqOrc1CXRXVE3N79UJckhJk7hHsU5YVcrJ302tUbo2lVvbR1/05mKVXs3zU3wrm5JAy79HOc0aJSLpYOidV4uVE7SUWexjR3s0TwPUcEpaqiT9xiPeMKz0TZujTvj/jPVyKSaTXxbxsMHPyWYCg==</latexit>\nCross-Attention Layer\n<latexit sha1_base64=\"9FwO3bbzTqyFgc3cGvYDb/Iep3A=\">AAAC1nicjVHLSsNAFD2Nr1pfqS7dBIvgqqSC6FJ047KCfUAtkkyndWheTCY+KLoTt/6AW/0k8Q/0L7wzpqAW0QlJzpx7z5m59/pJIFLluq8Fa2p6ZnauOF9aWFxaXrHLq800ziTjDRYHsWz7XsoDEfGGEirg7URyL/QD3vKHhzreuuAyFXF0oq4T3g29QST6gnmKqDO7zOKoL3o8YtwJPSXF1ZldcauuWc4kqOWggnzVY/sFp+ghBkOGEBwRFOEAHlJ6OqjBRUJcFyPiJCFh4hw3KJE2oyxOGR6xQ/oOaNfJ2Yj22jM1akanBPRKUjrYJE1MeZKwPs0x8cw4a/Y375Hx1He7pr+fe4XEKpwT+5dunPlfna5FoY89U4OgmhLD6OpY7pKZruibO1+qUuSQEKdxj+KSMDPKcZ8do0lN7bq3nom/mUzN6j3LczO861vSgGs/xzkJmtvV2k7VPd6u7B/koy5iHRvYonnuYh9HqKNB3pd4xBOerbZ1a91Z95+pViHXrOHbsh4+AIRXlps=</latexit>\nconﬁdence matrix\n<latexit sha1_base64=\"sPL2CbpmuH1PY0evGqAvfyaGPDA=\">AAAC2HicjVHLSsNAFD2Nr1pf0S7dBIvgQkoiii5rBXFZwT6wrSVJpzU0L5KJUErBnbj1B9zqF4l/oH/hnTEFtYhOSHLm3HvOzL3XCl0n5rr+mlFmZufmF7KLuaXlldU1dX2jFgdJZLOqHbhB1LDMmLmOz6rc4S5rhBEzPctldWtwIuL1GxbFTuBf8GHI2p7Z952eY5ucqI6ab12bfHQ6vjrenaByRy3oRV0ubRoYKSggXZVAfUELXQSwkcADgw9O2IWJmJ4mDOgIiWtjRFxEyJFxhjFypE0oi1GGSeyAvn3aNVPWp73wjKXaplNceiNSatgmTUB5EWFxmibjiXQW7G/eI+kp7jakv5V6ecRyXBP7l26S+V+dqIWjhyNZg0M1hZIR1dmpSyK7Im6ufamKk0NInMBdikeEbamc9FmTmljWLnpryvibzBSs2NtpboJ3cUsasPFznNOgtlc0Dor6+X6hVE5HncUmtrBD8zxECWeooEreQzziCc/KpXKr3Cn3n6lKJtXk8W0pDx+oSpcH</latexit>\nˆ\nF\nA\n,\nˆ\nF\nB\n<latexit sha1_base64=\"5SpVm0dThrj6kHJbOLEm9Ji8Zeo=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkVwVZKC6LLoxoWLCvYBtUgyndbQaRImE6EUd/6AW/0w8Q/0L7wzTkEtohOSnDn3nDtz7w1TEWXK814LzsLi0vJKcbW0tr6xuVXe3mllSS4Zb7JEJLITBhkXUcybKlKCd1LJg3EoeDscnel4+47LLEriKzVJeW8cDONoELFAEdUeiEApHt+UK17VM8udB74FFdjVSMovuEYfCRhyjMERQxEWCJDR04UPDylxPUyJk4QiE+e4R4m8Oak4KQJiR/Qd0q5r2Zj2Omdm3IxOEfRKcro4IE9COklYn+aaeG4ya/a33FOTU99tQv/Q5hoTq3BL7F++mfK/Pl2LwgAnpoaIakoNo6tjNktuuqJv7n6pSlGGlDiN+xSXhJlxzvrsGk9mate9DUz8zSg1q/fManO861vSgP2f45wHrVrVP6p6l7VK/dSOuog97OOQ5nmMOs7RQNNU+YgnPDsXjnQmzvRT6hSsZxfflvPwAS/CkjE=</latexit>\nﬂatten\n<latexit sha1_base64=\"YyWZuvJNnbSGxYL4Ip77E86NEeE=\">AAACy3icjVHLSsNAFD2Nr1pfVZdugkWom5KIosuiCG6ECvYBbZFkOq2heZGZCLW69Afc6n+Jf6B/4Z0xBbWITkhy5txz7sy91419T0jLes0ZM7Nz8wv5xcLS8srqWnF9oyGiNGG8ziI/SlquI7jvhbwuPenzVpxwJ3B93nSHJyrevOGJ8KLwUo5i3g2cQej1PeZIolqn5Q7rRXL3qliyKpZe5jSwM1BCtmpR8QUd9BCBIUUAjhCSsA8Hgp42bFiIietiTFxCyNNxjnsUyJuSipPCIXZI3wHt2hkb0l7lFNrN6BSf3oScJnbIE5EuIaxOM3U81ZkV+1vusc6p7jaiv5vlCoiVuCb2L99E+V+fqkWijyNdg0c1xZpR1bEsS6q7om5ufqlKUoaYOIV7FE8IM+2c9NnUHqFrV711dPxNKxWr9izTpnhXt6QB2z/HOQ0aexX7oGJd7Jeqx9mo89jCNso0z0NUcYYa6nqOj3jCs3FuCOPWuPuUGrnMs4lvy3j4AJNbkes=</latexit>\nE ( · )\n<latexit sha1_base64=\"+/MslZvgSgUwDUGnit9ODuTaO9E=\">AAACznicjVHLSsNAFD2Nr/quunQTLIKrkhREl0U3LivYB9QiyXRah+bFZFIspbj1B9zqZ4l/oH/hnTEFtYhOSHLm3HPuzL3XTwKRKsd5LVgLi0vLK8XVtfWNza3t0s5uM40zyXiDxUEs276X8kBEvKGECng7kdwL/YC3/OG5jrdGXKYijq7UOOHd0BtEoi+Yp4jq8LuEM2XwTansVByz7Hng5qCMfNXj0guu0UMMhgwhOCIowgE8pPR04MJBQlwXE+IkIWHiHFOskTcjFSeFR+yQvgPadXI2or3OmRo3o1MCeiU5bRySJyadJKxPs008M5k1+1vuicmp7zamv5/nColVuCX2L99M+V+frkWhj1NTg6CaEsPo6lieJTNd0Te3v1SlKENCnMY9ikvCzDhnfbaNJzW16956Jv5mlJrVe5ZrM7zrW9KA3Z/jnAfNasU9rjiX1XLtLB91Efs4wBHN8wQ1XKCOhun4I57wbNWtkTW17j+lViH37OHbsh4+AOXxlA8=</latexit>\nexpectation\n<latexit sha1_base64=\"c6YWAQ+7mdRv51eVn3alFIwwCes=\">AAAC5nicjVHLSsNAFD2N7/qKunQTLYIbS1oUXRYFcSMo2AeoSJJONTjJhMlELMW1O3fi1h9wq58i/oH+hXfGFHwgOiHJmXPvOTP3Xj/hYapc96VgDQwODY+MjhXHJyanpu2Z2UYqMhmweiC4kC3fSxkPY1ZXoeKslUjmRT5nTf98S8ebF0ymoYgPVDdhx5F3GoedMPAUUSf2wpFil8rv9FbLzpbwZMpWlFjZJjdnV7Qzzq5O7JJbds1yfoJKDkrI156wn3GENgQCZIjAEEMR5vCQ0nOIClwkxB2jR5wkFJo4wxWKpM0oi1GGR+w5fU9pd5izMe21Z2rUAZ3C6ZWkdLBEGkF5krA+zTHxzDhr9jfvnvHUd+vS38+9ImIVzoj9S9fP/K9O16LQwYapIaSaEsPo6oLcJTNd0Td3PlWlyCEhTuM2xSXhwCj7fXaMJjW16956Jv5qMjWr90Gem+FN35IGXPk+zp+gUS1X1srufrVU28xHPYp5LGKZ5rmOGnawhzp5X+MBj3iyzqwb69a6+0i1CrlmDl+Wdf8O7FucfQ==</latexit>\n4. Coarse-to-Fine Module\n<latexit sha1_base64=\"stuJvCeWjCU8eDCQpOwvzNhK/lc=\">AAACznicjVHLSsNAFD2Nr1pfVZdugkVwVZKC6LLoxmUF+4BaJJlOa2iSCZNJoZTi1h9wq58l/oH+hXfGFNQiOiHJmXPPuTP3Xj8Jg1Q5zmvBWlpeWV0rrpc2Nre2d8q7e61UZJLxJhOhkB3fS3kYxLypAhXyTiK5F/khb/ujCx1vj7lMAxFfq0nCe5E3jINBwDxFVJcJKXlo8G254lQds+xF4Oaggnw1RPkFN+hDgCFDBI4YinAIDyk9XbhwkBDXw5Q4SSgwcY4ZSuTNSMVJ4RE7ou+Qdt2cjWmvc6bGzeiUkF5JThtH5BGkk4T1abaJZyazZn/LPTU59d0m9PfzXBGxCnfE/uWbK//r07UoDHBmagiopsQwujqWZ8lMV/TN7S9VKcqQEKdxn+KSMDPOeZ9t40lN7bq3nom/GaVm9Z7l2gzv+pY0YPfnOBdBq1Z1T6rOVa1SP89HXcQBDnFM8zxFHZdooGk6/ognPFsNa2zNrPtPqVXIPfv4tqyHD+EblA0=</latexit>\ncorrelation\n<latexit sha1_base64=\"yHvCs0laogAaO3xbxpq20EJosbY=\">AAACxXicjVHLSsNAFD2Nr1pfVZdugkVxVdKC6LLoQpdV7APaIsl0WofmRTIplCL+gFv9NfEP9C+8M05BLaITkpw5954zc+/1Yl+k0nFec9bC4tLySn61sLa+sblV3N5pplGWMN5gkR8lbc9NuS9C3pBC+rwdJ9wNPJ+3vNG5irfGPElFFN7IScx7gTsMxUAwVxJ13T28LZacsqOXPQ8qBpRgVj0qvqCLPiIwZAjAEUIS9uEipaeDChzExPUwJS4hJHSc4x4F0maUxSnDJXZE3yHtOoYNaa88U61mdIpPb0JKGwekiSgvIaxOs3U8086K/c17qj3V3Sb094xXQKzEHbF/6WaZ/9WpWiQGONU1CKop1oyqjhmXTHdF3dz+UpUkh5g4hfsUTwgzrZz12daaVNeueuvq+JvOVKzaM5Ob4V3dkgZc+TnOedCslivHZeeqWqqdmVHnsYd9HNE8T1DDJepokPcAj3jCs3VhBZa0xp+pVs5odvFtWQ8fqaqPkw==</latexit>\n&\n<latexit sha1_base64=\"uNy47YhixWxowmHYsKo9uYv2Z0A=\">AAACynicjVHLSgMxFD2Or1pfVZduBovgqkwLosuiGxcuKtgH1CKZaVqHzoskI5bizh9wqx8m/oH+hTcxBbWIZpiZk3PPucm918+iUCrPe51z5hcWl5YLK8XVtfWNzdLWdkumuQh4M0ijVHR8JnkUJrypQhXxTiY4i/2It/3RqY63b7mQYZpcqnHGezEbJuEgDJgiqi3TgYrZ3XWp7FU8s9xZULWgDLsaaekFV+gjRYAcMTgSKMIRGCQ9XVThISOuhwlxglBo4hz3KJI3JxUnBSN2RN8h7bqWTWivc0rjDuiUiF5BThf75ElJJwjr01wTz01mzf6We2Jy6ruN6e/bXDGxCjfE/uWbKv/r07UoDHBsagippswwurrAZslNV/TN3S9VKcqQEadxn+KCcGCc0z67xiNN7bq3zMTfjFKzeh9YbY53fUsacPXnOGdBq1apHla8i1q5fmJHXcAu9nBA8zxCHWdooGmqfMQTnp1zRzhjZ/IpdeasZwfflvPwAV+hkkU=</latexit>\nsoftmax\n<latexit sha1_base64=\"IeL/tRnryF8Q5vXm7SE2uYw/TG0=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwVVJRdFl040oq2IfYIkk6rUPzYjKxlNKtP+BWv0v8A/0L74xTUIvohCRnzr3nzNx7vSTgqXSc15w1N7+wuJRfLqysrq1vFDe3GmmcCZ/V/TiIRctzUxbwiNUllwFrJYK5oRewpjc4U/HmPRMpj6MrOUpYJ3T7Ee9x35VEXQ/bkocstYe3xZJTdvSyZ0HFgBLMqsXFF7TRRQwfGUIwRJCEA7hI6blBBQ4S4joYEycIcR1nmKBA2oyyGGW4xA7o26fdjWEj2ivPVKt9OiWgV5DSxh5pYsoThNVpto5n2lmxv3mPtae624j+nvEKiZW4I/Yv3TTzvzpVi0QPJ7oGTjUlmlHV+cYl011RN7e/VCXJISFO4S7FBWFfK6d9trUm1bWr3ro6/qYzFav2vsnN8K5uSQOu/BznLGgclCtHZefysFQ9NaPOYwe72Kd5HqOKc9RQJ+8Qj3jCs3VhSWtsTT5TrZzRbOPbsh4+APmIkuU=</latexit>\nw ⇥ w\n<latexit sha1_base64=\"uZjR57D00cM27uGOBHLD9qDpZDg=\">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJlgFVyUtiC6LgrisYFvBFknGsQ6mmTCZCKV0407c+gNu9YPEP9C/8M6YglpEJyQ5c+49Z+beG8ShSLTnveacicmp6Zn8bGFufmFxqbi80kxkqhhvMBlKdRr4CQ9FxBta6JCfxor7vSDkreD6wMRbN1wlQkYnuh/zTs/vRuJSMF8TdV5cY0rGsYi6rozczfaVrweHw83zYskre3a546CSgRKyVZfFF7RxAQmGFD1wRNCEQ/hI6DlDBR5i4joYEKcICRvnGKJA2pSyOGX4xF7Tt0u7s4yNaG88E6tmdEpIryKliy3SSMpThM1pro2n1tmwv3kPrKe5W5/+QebVI1bjiti/dKPM/+pMLRqX2LM1CKoptoypjmUuqe2Kubn7pSpNDjFxBl9QXBFmVjnqs2s1ia3d9Na38TebaVizZ1luindzSxpw5ec4x0GzWq7slL3jaqm2n406j3VsYJvmuYsajlBHg7wHeMQTnp22c+vcOfefqU4u06zi23IePgA+opee</latexit>\ncropping on ˆF\n<latexit sha1_base64=\"MZ3lJGZ4KHaNEaS64IeBIgDirEY=\">AAACynicjVHLSsNAFD2Nr/quunQTLIKrkoiiy6IbFy4q2Ae0RZLptB2aJmEyEUrozh9wqx8m/oH+hXfGFNQiOiHJmXPPuTP3Xj8ORKIc57VgLSwuLa8UV9fWNza3tks7u40kSiXjdRYFkWz5XsIDEfK6EirgrVhyb+wHvOmPLnW8ec9lIqLwVk1i3h17g1D0BfMUUc3O0FOZmN6Vyk7FMcueB24OyshXLSq9oIMeIjCkGIMjhCIcwENCTxsuHMTEdZERJwkJE+eYYo28Kak4KTxiR/Qd0K6dsyHtdc7EuBmdEtAryWnjkDwR6SRhfZpt4qnJrNnfcmcmp77bhP5+nmtMrMKQ2L98M+V/fboWhT7OTQ2CaooNo6tjeZbUdEXf3P5SlaIMMXEa9yguCTPjnPXZNp7E1K5765n4m1FqVu9Zrk3xrm9JA3Z/jnMeNI4r7mnFuTkpVy/yURexjwMc0TzPUMUVaqibKh/xhGfr2pLWxMo+pVYh9+zh27IePgBMpJI/</latexit>\nˆ\ni\n<latexit sha1_base64=\"hba2RMo7VlCiENuwzTQw4Wozteg=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkVwVRJRdFl048JFBfuAtkgynbaxeTGZCCV05w+41Q8T/0D/wjtjCmoRnZDkzLnn3Jl7rxv7XiIt67VgLCwuLa8UV0tr6xubW+XtnWYSpYLxBov8SLRdJ+G+F/KG9KTP27HgTuD6vOWOL1S8dc9F4kXhjZzEvBc4w9AbeMyRRLW6I0dmd9PbcsWqWnqZ88DOQQX5qkflF3TRRwSGFAE4QkjCPhwk9HRgw0JMXA8ZcYKQp+McU5TIm5KKk8IhdkzfIe06ORvSXuVMtJvRKT69gpwmDsgTkU4QVqeZOp7qzIr9LXemc6q7Tejv5rkCYiVGxP7lmyn/61O1SAxwpmvwqKZYM6o6lmdJdVfUzc0vVUnKEBOncJ/igjDTzlmfTe1JdO2qt46Ov2mlYtWe5doU7+qWNGD75zjnQfOoap9UrevjSu08H3URe9jHIc3zFDVcoo6GrvIRT3g2rgxhTIzsU2oUcs8uvi3j4QNPBZJA</latexit>\nˆ\nj\n<latexit sha1_base64=\"oN02b/X09Xsgnh/bKDpJWJPRInI=\">AAACy3icjVHLSsNAFD2Nr1pfVZdugkV0VRJRdFl040aoYB9Qi0ym0zY2L5KJUGuX/oBb/S/xD/QvvDOmoBbRCUnOnHvOnbn3OpHnJtKyXnPGzOzc/EJ+sbC0vLK6VlzfqCdhGnNR46EXxk2HJcJzA1GTrvREM4oF8x1PNJzBqYo3bkWcuGFwKYeRaPusF7hdlzNJVPOqz+ToZrx7XSxZZUsvcxrYGSghW9Ww+IIrdBCCI4UPgQCSsAeGhJ4WbFiIiGtjRFxMyNVxgTEK5E1JJUjBiB3Qt0e7VsYGtFc5E+3mdIpHb0xOEzvkCUkXE1anmTqe6syK/S33SOdUdxvS38ly+cRK9In9yzdR/tenapHo4ljX4FJNkWZUdTzLkuquqJubX6qSlCEiTuEOxWPCXDsnfTa1J9G1q94yHX/TSsWqPc+0Kd7VLWnA9s9xToP6ftk+LFsXB6XKSTbqPLawjT2a5xEqOEMVNT3HRzzh2Tg3EuPOuP+UGrnMs4lvy3j4ANRsknE=</latexit>\nˆ\nj\n0\n<latexit sha1_base64=\"RnPmSRNHAHVF9/tVkRrMGrDT9yw=\">AAAC2nicjVHLSsNAFD2Nr1pfVXHlJliEClJSUXRZdOOygn1AW0qSTuvYNAnJRCilG3fi1h9wqx8k/oH+hXfGKahFdEKSM+fec2buvU7o8VhY1mvKmJmdm19IL2aWlldW17LrG9U4SCKXVdzAC6K6Y8fM4z6rCC48Vg8jZg8cj9Wc/pmM125YFPPAvxTDkLUGds/nXe7agqh2divfFNzrsBEf72t0Pd5rZ3NWwVLLnAZFDXLQqxxkX9BEBwFcJBiAwYcg7MFGTE8DRVgIiWthRFxEiKs4wxgZ0iaUxSjDJrZP3x7tGpr1aS89Y6V26RSP3oiUJnZJE1BeRFieZqp4opwl+5v3SHnKuw3p72ivAbECV8T+pZtk/lcnaxHo4kTVwKmmUDGyOle7JKor8ubml6oEOYTESdyheETYVcpJn02liVXtsre2ir+pTMnKvatzE7zLW9KAiz/HOQ2qB4XiUcG6OMyVTvWo09jGDvI0z2OUcI4yKuQ9wiOe8Gw0jVvjzrj/TDVSWrOJb8t4+ABqzZge</latexit>\n(\n˜\ni,\n˜\nj )\n<latexit sha1_base64=\"9jEcPWcgotShHACro9FoNRB7URI=\">AAACz3icjVHLSsNAFD2Nr1pfVZdugkVwVdKC6EoKgrhQaKUvsEWSdFpDJ5mQTJRSFLf+gFv9K/EP9C+8M6agFtEJSc6ce8+Zufc6IfdiaVmvGWNmdm5+IbuYW1peWV3Lr280Y5FELmu4gouo7dgx417AGtKTnLXDiNm+w1nLGR6peOuaRbEngrochazr24PA63uuLYnqnIrj+rl5JnoJZ5f5glW09DKnQSkFBaSrKvIv6KAHARcJfDAEkIQ5bMT0XKAECyFxXYyJiwh5Os5wixxpE8pilGETO6TvgHYXKRvQXnnGWu3SKZzeiJQmdkgjKC8irE4zdTzRzor9zXusPdXdRvR3Ui+fWIkrYv/STTL/q1O1SPRxoGvwqKZQM6o6N3VJdFfUzc0vVUlyCIlTuEfxiLCrlZM+m1oT69pVb20df9OZilV7N81N8K5uSQMu/RznNGiWi6W9olUrFyqH6aiz2MI2dmme+6jgBFU0yDvEI57wbNSMG+POuP9MNTKpZhPflvHwAe4rk54=</latexit>\nLoFTR Module\n<latexit sha1_base64=\"rhB68Oc0gRFMX/esAAnj+TR8xfY=\">AAAC0HicjVHLTsJAFD3UF+ILdemmkZjgBluikZUhuGGJRh4JgmnLgA192U6NhBjj1h9wq19l/AP9C++MJVGJ0Wnanjn3njNz7zUDx464pr2mlJnZufmF9GJmaXlldS27vtGI/Di0WN3yHT9smUbEHNtjdW5zh7WCkBmu6bCmOTwW8eY1CyPb9874KGAd1xh4dt+2DE5UJ6/vlXa7xWq30uxWLrI5raDJpU4DPQE5JKvmZ19wjh58WIjhgsEDJ+zAQERPGzo0BMR1MCYuJGTLOMMtMqSNKYtRhkHskL4D2rUT1qO98Iyk2qJTHHpDUqrYIY1PeSFhcZoq47F0Fuxv3mPpKe42or+ZeLnEclwS+5dukvlfnaiFo4+SrMGmmgLJiOqsxCWWXRE3V79UxckhIE7gHsVDwpZUTvqsSk0kaxe9NWT8TWYKVuytJDfGu7glDVj/Oc5p0CgW9IOCdrKfKx8lo05jC9vI0zwPUUYVNdTJ+wqPeMKzcqrcKHfK/Weqkko0m/i2lIcPFsaS1Q==</latexit>\n(1 / 8)\n2\nH\nB\nW\nB\n<latexit sha1_base64=\"AsXy8ScbatACFYYaVQhd6G+nbCc=\">AAAC0HicjVHLTsJAFD3UF+ILdemmkZjgBluikZWBuGGJRh4JgmnLgA192U6NhBjj1h9wq19l/AP9C++MJVGJ0Wnanjn3njNz7zUDx464pr2mlJnZufmF9GJmaXlldS27vtGI/Di0WN3yHT9smUbEHNtjdW5zh7WCkBmu6bCmOTwW8eY1CyPb9874KGAd1xh4dt+2DE5UJ6/vlXa7xWq30uxWLrI5raDJpU4DPQE5JKvmZ19wjh58WIjhgsEDJ+zAQERPGzo0BMR1MCYuJGTLOMMtMqSNKYtRhkHskL4D2rUT1qO98Iyk2qJTHHpDUqrYIY1PeSFhcZoq47F0Fuxv3mPpKe42or+ZeLnEclwS+5dukvlfnaiFo4+SrMGmmgLJiOqsxCWWXRE3V79UxckhIE7gHsVDwpZUTvqsSk0kaxe9NWT8TWYKVuytJDfGu7glDVj/Oc5p0CgW9IOCdrKfKx8lo05jC9vI0zwPUUYVNdTJ+wqPeMKzcqrcKHfK/Weqkko0m/i2lIcPEgOS0w==</latexit>\n(1 / 8)\n2\nH\nA\nW\nA\n<latexit sha1_base64=\"xxS2p9vN9RTA0LQc1zQRRnYHdB4=\">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVZKC6EoKgoi4qNK0Qi2STKd1aF4kE6UUN/6AW/0y8Q/0L7wzpqAW0QlJzpx7z5m593qxL1JpWa8FY2Z2bn6huFhaWl5ZXSuvb7TSKEsYd1jkR8ml56bcFyF3pJA+v4wT7gaez9ve8EjF27c8SUUUNuUo5t3AHYSiL5griXLOouPmxXW5YlUtvcxpYOeggnw1ovILrtBDBIYMAThCSMI+XKT0dGDDQkxcF2PiEkJCxznuUSJtRlmcMlxih/Qd0K6TsyHtlWeq1YxO8elNSGlihzQR5SWE1WmmjmfaWbG/eY+1p7rbiP5e7hUQK3FD7F+6SeZ/daoWiT4OdA2Caoo1o6pjuUumu6Jubn6pSpJDTJzCPYonhJlWTvpsak2qa1e9dXX8TWcqVu1ZnpvhXd2SBmz/HOc0aNWq9l7VOq9V6of5qIvYwjZ2aZ77qOMEDTjkLfCIJzwbp0Zs3Bmjz1SjkGs28W0ZDx/U0JDS</latexit>\nLoFTR\n<latexit sha1_base64=\"9BXphpSr9pmtJ94+b9/vlaunj58=\">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZKC6EoKbgQRKtgH1CLJdFpjp0mcTMRaXPkDbvXHxD/Qv/DOGEEtohOSnDn3njNz7/VjESTKcV5y1tT0zOxcfr6wsLi0vFJcXWskUSoZr7NIRLLlewkXQcjrKlCCt2LJvaEveNMfHOh485rLJIjCUzWKeWfo9cOgFzBPEdU4jrqp4OfFklN2zLIngZuBErJVi4rPOEMXERhSDMERQhEW8JDQ04YLBzFxHYyJk4QCE+e4Q4G0KWVxyvCIHdC3T7t2xoa0156JUTM6RdArSWljizQR5UnC+jTbxFPjrNnfvMfGU99tRH8/8xoSq3BB7F+6z8z/6nQtCj3smRoCqik2jK6OZS6p6Yq+uf2lKkUOMXEadykuCTOj/OyzbTSJqV331jPxV5OpWb1nWW6KN31LGrD7c5yToFEpuztl56RSqu5no85jA5vYpnnuoopD1FAn70s84BFP1pF1Zd1Ytx+pVi7TrOPbsu7fAcK4kZs=</latexit>\nModule\n<latexit sha1_base64=\"Q+7D5CPMw1a4EiwQOpjj8UZXD/4=\">AAAC4HicjVHLSsNAFD3GV31XXboJFsFVSBTRZdGNm4KCbQUrkkyndTDNhGQiSnHhzp249Qfc6teIf6B/4Z1xBLWITkhy5tx7zsy9N0pjkSvffxlyhkdGx8ZLE5NT0zOzc+X5hUYui4zxOpOxzA6jMOexSHhdCRXzwzTjYS+KeTM629Hx5jnPciGTA3WZ8uNe2E1ER7BQEXVSXmopfqGiTn/dc2uhYqci6bo12S5ifnVSrvieb5Y7CAILKrBrT5af0UIbEgwFeuBIoAjHCJHTc4QAPlLijtEnLiMkTJzjCpOkLSiLU0ZI7Bl9u7Q7smxCe+2ZGzWjU2J6M1K6WCGNpLyMsD7NNfHCOGv2N+++8dR3u6R/ZL16xCqcEvuX7jPzvzpdi0IHW6YGQTWlhtHVMetSmK7om7tfqlLkkBKncZviGWFmlJ99do0mN7Xr3oYm/moyNav3zOYWeNO3pAEHP8c5CBprXrDh+ftrleq2HXUJS1jGKs1zE1XsYg918r7GAx7x5ETOjXPr3H2kOkNWs4hvy7l/Bysamk8=</latexit>\n3. Matching Module\n<latexit sha1_base64=\"zc0aezEzeZIS0DlL6Uvlm7Cb88M=\">AAAC9XicjVFNSxxBEH1O1BgTzSY5emlcArlkmV2Q5CgRJAcPCq4KKtLT1upgz/TQ3eMHi3/DW24hV/+A1/gXxH9g/oXV7QiJIqaHmXn9qt7rrqqs0rnzaXo9krwYHRt/OfFq8vWbqem3rXfv15ypraK+MtrYjUw60nlJfZ97TRuVJVlkmtazg4UQXz8k63JTrvqTirYLuVfmg1xJz9ROK93ydOyzwbDXEQtGWkefl+iQtFgySmqxSNLXlsSqlaUbGFuc7rTaaSeNSzwG3Qa00axl07rCFnZhoFCjAKGEZ6wh4fjZRBcpKua2MWTOMspjnHCKSdbWnEWcIZk94O8e7zYbtuR98HRRrfgUza9lpcBH1hjOs4zDaSLG6+gc2Ke8h9Ez3O2E/1njVTDrsc/sc7r7zP/VhVo8Bvgaa8i5pioyoTrVuNSxK+Hm4q+qPDtUzAW8y3HLWEXlfZ9F1LhYe+itjPGbmBnYsFdNbo0/4ZY84O7DcT4Ga71Od66TrvTa89+aUU9gBrP4xPP8gnl8xzL67H2GC/zGZXKU/Eh+Jr/uUpORRvMB/6zk/BaFr6LE</latexit>\n2. Coarse-Level Local Feature Transform\n<latexit sha1_base64=\"TZUPvt+BIuIbixm7Tog9lEQFSV0=\">AAAC5nicjVHLSsNAFD3GV31XXbqJFsGFlEQUXdYK4lLB2oKPkqTTdmiahMlEKKFrd+7ErT/gVj9F/AP9C++MKb4QnZDkzLn3nJl7rxv5PJaW9TxkDI+Mjo3nJianpmdm5/LzCydxmAiPVbzQD0XNdWLm84BVJJc+q0WCOV3XZ1W3s6fi1UsmYh4Gx7IXsfOu0wp4k3uOJKqeXz6T3G+wdL9/sVtPpeivfxBlTdTzBato6WX+BHYGCsjWYZh/whkaCOEhQRcMASRhHw5iek5hw0JE3DlS4gQhruMMfUySNqEsRhkOsR36tmh3mrEB7ZVnrNUeneLTK0hpYpU0IeUJwuo0U8cT7azY37xT7anu1qO/m3l1iZVoE/uXbpD5X52qRaKJHV0Dp5oizajqvMwl0V1RNzc/VSXJISJO4QbFBWFPKwd9NrUm1rWr3jo6/qIzFav2Xpab4FXdkgZsfx/nT3CyUbS3itbRZqFUzkadwxJWsEbz3EYJBzhEhbyvcI8HPBpt49q4MW7fU42hTLOIL8u4ewPkuJ23</latexit>\n˜\nF\nA\ntr\n,\n˜\nF\nB\ntr\n<latexit sha1_base64=\"xYs+hDM+y4CIMQLKjqgukEDtybM=\">AAAC3HicjVHLSsNAFD2Nr1pfURcu3ASL4EJKKoouawVxWcE+oC+SdFqHpklIJkIp3bkTt/6AW/0e8Q/0L7wzRnwU0QlJzpx7z5l759qByyNhms8pbWp6ZnYuPZ9ZWFxaXtFX1yqRH4cOKzu+64c124qYyz1WFly4rBaEzBrYLqva/RMZr16xMOK+dyGGAWsOrJ7Hu9yxBFFtfaMhuNtho9Nx63j3ExfbetbMmWoZkyCfgCySVfL1JzTQgQ8HMQZg8CAIu7AQ0VNHHiYC4poYERcS4irOMEaGtDFlMcqwiO3Tt0e7esJ6tJeekVI7dIpLb0hKA9uk8SkvJCxPM1Q8Vs6S/c17pDxlbUP624nXgFiBS2L/0n1k/lcnexHo4kj1wKmnQDGyOydxidWtyMqNL10JcgiIk7hD8ZCwo5Qf92woTaR6l3drqfiLypSs3DtJboxXWSUNOP9znJOgspfLH+TM8/1soZiMOo1NbGGH5nmIAs5QQlnVf48HPGot7Vq70W7fU7VUolnHt6XdvQFVm5jZ</latexit>\n˜\nF\nA\n,\n˜\nF\nB\n<latexit sha1_base64=\"AJe3vcFSw3RK84aR3WWfbmHZjUA=\">AAAB9XicbVDLSgNBEJyNrxhfUY9eBoPgKewGRE8S8OJBIYJ5QLKG3skkGTI7u8z0KmHJf3jxoIhX/8Wbf+Mk2YMmFjQUVd10dwWxFAZd99vJrayurW/kNwtb2zu7e8X9g4aJEs14nUUy0q0ADJdC8ToKlLwVaw5hIHkzGF1N/eYj10ZE6h7HMfdDGCjRFwzQSg+3gGwo1IDewJjrbrHklt0Z6DLxMlIiGWrd4lenF7Ek5AqZBGPanhujn4JGwSSfFDqJ4TGwEQx421IFITd+Ort6Qk+s0qP9SNtSSGfq74kUQmPGYWA7Q8ChWfSm4n9eO8H+hZ8KFSfIFZsv6ieSYkSnEdCe0JyhHFsCTAt7K2VD0MDQBlWwIXiLLy+TRqXsnZXdu0qpepnFkSdH5JicEo+ckyq5JjVSJ4xo8kxeyZvz5Lw4787HvDXnZDOH5A+czx8zspJM</latexit>\nMatching Layer\n<latexit sha1_base64=\"3/7EpPLUSUPKZvcFJc5MS3DcM2c=\">AAAB9XicbVDLSgMxFL3js9ZX1aWbYBFclZmC6EoKunBZwT6gHUsmvdOGZjJDklHK0P9w40IRt/6LO//GtJ2Fth4IHM65J7k5QSK4Nq777aysrq1vbBa2its7u3v7pYPDpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApG11O/9YhK81jem3GCfkQHkoecUWOlhxsehqhQGk5tolcquxV3BrJMvJyUIUe9V/rq9mOWRvYCJqjWHc9NjJ9RZTgTOCl2U40JZSM6wI6lkkao/Wy29YScWqVPwljZIw2Zqb8TGY20HkeBnYyoGepFbyr+53VSE176GZdJalCy+UNhKoiJybQC0ucKmRFjSyhT3O5K2JAqyowtqmhL8Ba/vEya1Yp3XnHvquXaVV5HAY7hBM7AgwuowS3UoQEMFDzDK7w5T86L8+58zEdXnDxzBH/gfP4AqFqSmA==</latexit>\nDi ↵ erentiable\n<latexit sha1_base64=\"0j85EC6VxGJJL8YD06ks4hRpdEI=\">AAAB8XicbVDLSgMxFL2pr1pfVZdugkVwVWYKosuCG5cV7APboWTSTBuaSYYkI5Shf+HGhSJu/Rt3/o2ZdhbaeiBwOOdebs4JE8GN9bxvVNrY3NreKe9W9vYPDo+qxycdo1JNWZsqoXQvJIYJLlnbcitYL9GMxKFg3XB6m/vdJ6YNV/LBzhIWxGQsecQpsU56TJThOSFiWK15dW8BvE78gtSgQGtY/RqMFE1jJi0VxJi+7yU2yIi2nAo2rwxSwxJCp2TM+o5KEjMTZIsfz/GFU0Y4Uto9afFC/b2RkdiYWRy6yZjYiVn1cvE/r5/a6CbIuExSyyRdHopSga3CeXw84ppRK2aOEKpddorphGhCrSup4krwVyOvk06j7l/VvftGrdks6ijDGZzDJfhwDU24gxa0gYKEZ3iFN2TQC3pHH8vREip2TuEP0OcPBLKRIA==</latexit>\npositional\n<latexit sha1_base64=\"uBljJUNCXaUEI0gHVLPq/u2Jk/8=\">AAAB73icbVDLSgMxFL1TX7W+qi7dBIvgqswURJcFNy4r2Ae0Q8lkMm1oHmOSEcrQn3DjQhG3/o47/8a0nYW2HggczrmX3HOilDNjff/bK21sbm3vlHcre/sHh0fV45OOUZkmtE0UV7oXYUM5k7RtmeW0l2qKRcRpN5rczv3uE9WGKflgpykNBR5JljCCrZN6VBIVMzkaVmt+3V8ArZOgIDUo0BpWvwaxIpmg0hKOjekHfmrDHGvLCKezyiAzNMVkgke076jEgpowX9w7QxdOiVGitHvSooX6eyPHwpipiNykwHZsVr25+J/Xz2xyE+ZMppl1yZYfJRlHVqF5eBQzTYnlU0cw0czdisgYa0ysq6jiSghWI6+TTqMeXNX9+0at2SzqKMMZnMMlBHANTbiDFrSBAIdneIU379F78d69j+VoySt2TuEPvM8fNjuQEQ==</latexit>\nencoding\n<latexit sha1_base64=\"1BoXq4FayW5GBWd6m7prH/qpgR8=\">AAAB83icbVBNS8NAEJ3Ur1q/qh69LLaCp5IURI9FL56kgq2FJpTNdtMu3WzC7kQopX/DiwdFvPpnvPlv3LY5aOuDgcd7M8zMC1MpDLrut1NYW9/Y3Cpul3Z29/YPyodHbZNkmvEWS2SiOyE1XArFWyhQ8k6qOY1DyR/D0c3Mf3zi2ohEPeA45UFMB0pEglG0kl/1UcTckLseq/bKFbfmzkFWiZeTCuRo9spffj9hWcwVMkmN6XpuisGEahRM8mnJzwxPKRvRAe9aqqjdFEzmN0/JmVX6JEq0LYVkrv6emNDYmHEc2s6Y4tAsezPxP6+bYXQVTIRKM+SKLRZFmSSYkFkApC80ZyjHllCmhb2VsCHVlKGNqWRD8JZfXiXtes27qLn39UrjOo+jCCdwCufgwSU04Baa0AIGKTzDK7w5mfPivDsfi9aCk88cwx84nz/fKZDq</latexit>\n⇥ N c\n<latexit sha1_base64=\"XIYENWdlvrfpYWx7m1RzUcpYllo=\">AAAB83icbVBNS8NAEJ3Ur1q/qh69LLaCp5IURE9S8OJJKthaaELZbDft0s0m7E6EUvo3vHhQxKt/xpv/xm2bg7Y+GHi8N8PMvDCVwqDrfjuFtfWNza3idmlnd2//oHx41DZJphlvsUQmuhNSw6VQvIUCJe+kmtM4lPwxHN3M/Mcnro1I1AOOUx7EdKBEJBhFK/lVH0XMDbnrRdVeueLW3DnIKvFyUoEczV75y+8nLIu5QiapMV3PTTGYUI2CST4t+ZnhKWUjOuBdSxW1m4LJ/OYpObNKn0SJtqWQzNXfExMaGzOOQ9sZUxyaZW8m/ud1M4yugolQaYZcscWiKJMEEzILgPSF5gzl2BLKtLC3EjakmjK0MZVsCN7yy6ukXa95FzX3vl5pXOdxFOEETuEcPLiEBtxCE1rAIIVneIU3J3NenHfnY9FacPKZY/gD5/MH4oSQ6Q==</latexit>\n⇥ N f\n<latexit sha1_base64=\"ecZC9ObuyD9gXYn3hXyEPAY9vLE=\">AAACLnicbVDBattAFFy5TZO6aas2x1yW2gUXipEMJTmmlEIugRTixGAZs3p6srde7YrdVcAIfVEu+ZX2EEhLyLWf0ZWjQ2p3TsPMG957E+eCGxsEt17rydOtZ9s7z9svdl++eu2/eXtuVKEBh6CE0qOYGRRc4tByK3CUa2RZLPAiXnyp/YtL1IYreWaXOU4yNpM85cCsk6b+11Rpim5iSUExbZC6eMKhdmm3F1kuEix59bFh36sPNOKSRhmzc2CiPKmm0J36naAfrEA3SdiQDmlwOvV/RomCIkNpQTBjxmGQ20nJtOUgsGpHhcGcwYLNcOyoZBmaSbl6t6LvnZLQ+vBUSUtX6uNEyTJjllnsJusrzbpXi//zxoVNDycll3lhUcLDorQQ1Cpad0cTrhGsWDrCQHN3K4U50wysa7jtSgjXX94k54N++KkffBt0jj43deyQffKO9EhIDsgROSanZEiAXJEf5Bf57V17N96dd/8w2vKazB75B96fv/R6qRI=</latexit>\nfor every coarse prediction ( ˜i, ˜j ) 2 M c\n<latexit sha1_base64=\"oGZIY88oLlqSZyOmbJaNwSFJW14=\">AAAB+HicbVDLSsNAFL2pr1ofjbp0M9gKrkpSEF0W3bisYB/QhjCZTtqhk0mYmQg19EvcuFDErZ/izr9x0mahrQcGDufcyz1zgoQzpR3n2yptbG5t75R3K3v7B4dV++i4q+JUEtohMY9lP8CKciZoRzPNaT+RFEcBp71gepv7vUcqFYvFg54l1IvwWLCQEayN5NvV+jDCekIwz9pzn9R9u+Y0nAXQOnELUoMCbd/+Go5ikkZUaMKxUgPXSbSXYakZ4XReGaaKJphM8ZgODBU4osrLFsHn6NwoIxTG0jyh0UL9vZHhSKlZFJjJPKVa9XLxP2+Q6vDay5hIUk0FWR4KU450jPIW0IhJSjSfGYKJZCYrIhMsMdGmq4opwV398jrpNhvuZcO5b9ZaN0UdZTiFM7gAF66gBXfQhg4QSOEZXuHNerJerHfrYzlasoqdE/gD6/MHN1SSyw==</latexit>\nP c\n<latexit sha1_base64=\"2PkG3C1/8NWpg03vn6qOgvL2F/s=\">AAACGHicbVDLSsNAFJ3UV62vqEs3g61QQWpSEN0IRTduhAr2AU0sk+mkHTt5MDMRSshnuPFX3LhQxG13/o2TNAttPTBwOOde7pzjhIwKaRjfWmFpeWV1rbhe2tjc2t7Rd/faIog4Ji0csIB3HSQIoz5pSSoZ6YacIM9hpOOMr1O/80S4oIF/LychsT009KlLMZJK6uunFctDcoQRi2+TvgsvoRVXrRGSMU1OYEYekwcr5NQjx1ZS6etlo2ZkgIvEzEkZ5Gj29ak1CHDkEV9ihoTomUYo7RhxSTEjScmKBAkRHqMh6SnqI48IO86CJfBIKQPoBlw9X8JM/b0RI0+IieeoyTSFmPdS8T+vF0n3wo6pH0aS+Hh2yI0YlAFMW4IDygmWbKIIwpyqv0I8QhxhqbosqRLM+ciLpF2vmWc1465eblzldRTBATgEVWCCc9AAN6AJWgCDZ/AK3sGH9qK9aZ/a12y0oOU7++APtOkPz4yfoQ==</latexit>\nM f = { (\nˆ\ni,\nˆ\nj\n0\n) }\n<latexit sha1_base64=\"lAmzKgRjSb1aOKZToY5XoLA1Orw=\">AAACDnicbZDNSsNAEMcnftb60ahHL8EieCpJUfRY9OKxgv2AtpTNdtIu3WzC7kYoIe/g3au+gjfx6iv4Bj6G2zaCbf3DwJ//zDDDz485U9p1v6y19Y3Nre3CTnF3b/+gZB8eNVWUSIoNGvFItn2ikDOBDc00x3YskYQ+x5Y/vp32W48oFYvEg57E2AvJULCAUaJN1LdLXcEoBpLQ1MvSata3y27FnclZNV5uypCr3re/u4OIJiEKTTlRquO5se6lRGpGOWbFbqIwJnRMhtgxVpAQVS+dPZ45ZyYZOEEkTQntzNK/GykJlZqEvpkMiR6p5d40/K/XSXRw3UuZiBONgs4PBQl3dORMKTgDJpFqPjGGUMnMrw4dEUNBG1YLV37xZEWDxlsGsWqa1Yp3WXHvL8q1mxxSAU7gFM7BgyuowR3UoQEUEniGF3i1nqw36936mI+uWfnOMSzI+vwBz0ScZQ==</latexit>\n1\n/ 2\nFigure 2: Overview of the proposed method.LoFTR has four components: 1. A local feature CNN extracts the coarse-level\nfeature maps ˜FA and ˜FB, together with the ﬁne-level feature maps ˆFA and ˆFB from the image pair IA and IB (Section\n3.1). 2. The coarse feature maps are ﬂattened to 1-D vectors and added with the positional encoding. The added features\nare then processed by the Local Feature TRansformer (LoFTR) module, which has Nc self-attention and cross-attention\nlayers (Section 3.2). 3. A differentiable matching layer is used to match the transformed features, which ends up with a\nconﬁdence matrix Pc. The matches in Pc are selected according to the conﬁdence threshold and mutual-nearest-neighbor\ncriteria, yielding the coarse-level match prediction Mc (Section 3.3). 4. For every selected coarse prediction (˜i,˜j) ∈Mc, a\nlocal window with size w×wis cropped from the ﬁne-level feature map. Coarse matches will be reﬁned within this local\nwindow to a sub-pixel level as the ﬁnal match prediction Mf (Section 3.4).\nity and computation efﬁciency. Recently, Transformers are\nalso getting more attention in computer vision tasks, such as\nimage classiﬁcation [10], object detection [3] and seman-\ntic segmentation [49]. Concurrently with our work, [20]\nproposes to use Transformer for disparity estimation. The\ncomputation cost of the vanilla Transformer grows quadrat-\nically as the length of input sequences due to the multipli-\ncation between query and key vectors. Many efﬁcient vari-\nants [42, 18, 17, 5] are proposed recently in the context of\nprocessing long language sequences. Since no assumption\nof the input data is made in these works, they are also well\nsuited for processing images.\n3. Methods\nGiven the image pair IA and IB, the existing local fea-\nture matching methods use a feature detector to extract in-\nterest points. We propose to tackle the repeatability issue of\nfeature detectors with a detector-free design. An overview\nof the proposed method LoFTR is presented in Fig. 2.\n3.1. Local Feature Extraction\nWe use a standard convolutional architecture with\nFPN [22] (denoted as the local feature CNN) to extract\nmulti-level features from both images. We use ˜FA and ˜FB\nto denote the coarse-level features at 1/8 of the original im-\nage dimension, and ˆFAand ˆFB the ﬁne-level features at1/2\nof the original image dimension.\nConvolutional Neural Networks (CNNs) possess the in-\nductive bias of translation equivariance and locality, which\nare well suited for local feature extraction. The downsam-\npling introduced by the CNN also reduces the input length\nof the LoFTR module, which is crucial to ensure a manage-\nable computation cost.\n3.2. Local Feature Transformer (LoFTR) Module\nAfter the local feature extraction, ˜FA and ˜FB are passed\nthrough the LoFTR module to extract position and context\ndependent local features. Intuitively, the LoFTR module\ntransforms the features into feature representations that are\neasy to match. We denote the transformed features as ˜FA\ntr\nand ˜FB\ntr.\nPreliminaries: Transformer [48]. We ﬁrst brieﬂy intro-\nduce the Transformer here as background. A Transformer\nencoder is composed of sequentially connected encoder lay-\ners. Fig. 3(a) shows the architecture of an encoder layer.\nThe key element in the encoder layer is the attention\n3\n<latexit sha1_base64=\"wyhfSbGISndIuhB6pYtAQnanJrI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsceCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2RyUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasKan3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9m6rbvK7Ua3kcRTiDc7gED26hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBqW2Mzw==</latexit>\nQ\n<latexit sha1_base64=\"aVetB5EXBB9JZ00ye6Q1GtusioY=\">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Koko9ljwInhpwX5AG8pmO2nXbjZhdyOU0F/gxYMiXv1J3vw3btsctPXBwOO9GWbmBYng2rjut7O2vrG5tV3YKe7u7R8clo6OWzpOFcMmi0WsOgHVKLjEpuFGYCdRSKNAYDsY38789hMqzWP5YCYJ+hEdSh5yRo2VGvf9UtmtuHOQVeLlpAw56v3SV28QszRCaZigWnc9NzF+RpXhTOC02Es1JpSN6RC7lkoaofaz+aFTcm6VAQljZUsaMld/T2Q00noSBbYzomakl72Z+J/XTU1Y9TMuk9SgZItFYSqIicnsazLgCpkRE0soU9zeStiIKsqMzaZoQ/CWX14lrcuKd11xG1flWjWPowCncAYX4MEN1OAO6tAEBgjP8ApvzqPz4rw7H4vWNSefOYE/cD5/AKBVjMk=</latexit>\nK\n<latexit sha1_base64=\"Ydi9DQswQh3789ZwUOHtqUP9KR8=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsceCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2R6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasKan3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9m6rbvK7Ua3kcRTiDc7gED26hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBsQGM1A==</latexit>\nV\n<latexit sha1_base64=\"wyhfSbGISndIuhB6pYtAQnanJrI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsceCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2RyUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasKan3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9m6rbvK7Ua3kcRTiDc7gED26hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBqW2Mzw==</latexit>\nQ\n<latexit sha1_base64=\"aVetB5EXBB9JZ00ye6Q1GtusioY=\">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4Koko9ljwInhpwX5AG8pmO2nXbjZhdyOU0F/gxYMiXv1J3vw3btsctPXBwOO9GWbmBYng2rjut7O2vrG5tV3YKe7u7R8clo6OWzpOFcMmi0WsOgHVKLjEpuFGYCdRSKNAYDsY38789hMqzWP5YCYJ+hEdSh5yRo2VGvf9UtmtuHOQVeLlpAw56v3SV28QszRCaZigWnc9NzF+RpXhTOC02Es1JpSN6RC7lkoaofaz+aFTcm6VAQljZUsaMld/T2Q00noSBbYzomakl72Z+J/XTU1Y9TMuk9SgZItFYSqIicnsazLgCpkRE0soU9zeStiIKsqMzaZoQ/CWX14lrcuKd11xG1flWjWPowCncAYX4MEN1OAO6tAEBgjP8ApvzqPz4rw7H4vWNSefOYE/cD5/AKBVjMk=</latexit>\nK\n<latexit sha1_base64=\"Ydi9DQswQh3789ZwUOHtqUP9KR8=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsceCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1Fip2R6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasKan3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9m6rbvK7Ua3kcRTiDc7gED26hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBsQGM1A==</latexit>\nV\n<latexit sha1_base64=\"8BCOj5rRBg4HCb8uPe4wjybi6gM=\">AAAB83icbVDLSsNAFL3xWeur6tJNsAgVoSSi2GXBjcsK9gFNKJPJpB06mQnzEErob7hxoYhbf8adf+O0zUJbD1w4nHMv994TZYwq7Xnfztr6xubWdmmnvLu3f3BYOTruKGEkJm0smJC9CCnCKCdtTTUjvUwSlEaMdKPx3czvPhGpqOCPepKRMEVDThOKkbZSQJipBTgW+uLSH1SqXt2bw10lfkGqUKA1qHwFscAmJVxjhpTq+16mwxxJTTEj03JgFMkQHqMh6VvKUUpUmM9vnrrnVondREhbXLtz9fdEjlKlJmlkO1OkR2rZm4n/eX2jk0aYU54ZTTheLEoMc7VwZwG4MZUEazaxBGFJ7a0uHiGJsLYxlW0I/vLLq6RzVfdv6t7DdbXZKOIowSmcQQ18uIUm3EML2oAhg2d4hTfHOC/Ou/OxaF1zipkT+APn8wfi+5Dl</latexit>\nelu ( · )+1\n<latexit sha1_base64=\"8BCOj5rRBg4HCb8uPe4wjybi6gM=\">AAAB83icbVDLSsNAFL3xWeur6tJNsAgVoSSi2GXBjcsK9gFNKJPJpB06mQnzEErob7hxoYhbf8adf+O0zUJbD1w4nHMv994TZYwq7Xnfztr6xubWdmmnvLu3f3BYOTruKGEkJm0smJC9CCnCKCdtTTUjvUwSlEaMdKPx3czvPhGpqOCPepKRMEVDThOKkbZSQJipBTgW+uLSH1SqXt2bw10lfkGqUKA1qHwFscAmJVxjhpTq+16mwxxJTTEj03JgFMkQHqMh6VvKUUpUmM9vnrrnVondREhbXLtz9fdEjlKlJmlkO1OkR2rZm4n/eX2jk0aYU54ZTTheLEoMc7VwZwG4MZUEazaxBGFJ7a0uHiGJsLYxlW0I/vLLq6RzVfdv6t7DdbXZKOIowSmcQQ18uIUm3EML2oAhg2d4hTfHOC/Ou/OxaF1zipkT+APn8wfi+5Dl</latexit>\nelu ( · )+1\n<latexit sha1_base64=\"NAHOtIgEQtavlY0G60hJVcJtZF0=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSLUS0lEsceCF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/nbX1jc2t7cJOcXdv/+CwdHTcMnGqGW+yWMa6E1DDpVC8iQIl7ySa0yiQvB2Mb2d++4lrI2L1iJOE+xEdKhEKRtFKDxV60S+V3ao7B1klXk7KkKPRL331BjFLI66QSWpM13MT9DOqUTDJp8VeanhC2ZgOeddSRSNu/Gx+6pScW2VAwljbUkjm6u+JjEbGTKLAdkYUR2bZm4n/ed0Uw5qfCZWkyBVbLApTSTAms7/JQGjOUE4soUwLeythI6opQ5tO0YbgLb+8SlqXVe+66t5fleu1PI4CnMIZVMCDG6jDHTSgCQyG8Ayv8OZI58V5dz4WrWtOPnMCf+B8/gCG+41E</latexit>\n( a )\n<latexit sha1_base64=\"BULFK08EE3/qJaWSR6G8iw1H+5Y=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSLUS0lEsceCF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O2vrG5tb24Wd4u7e/sFh6ei4peNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPx7cxvP6HSPJaPZpKgH9Gh5CFn1FjpoRJc9Etlt+rOQVaJl5My5Gj0S1+9QczSCKVhgmrd9dzE+BlVhjOB02Iv1ZhQNqZD7FoqaYTaz+anTsm5VQYkjJUtachc/T2R0UjrSRTYzoiakV72ZuJ/Xjc1Yc3PuExSg5ItFoWpICYms7/JgCtkRkwsoUxxeythI6ooMzadog3BW355lbQuq9511b2/KtdreRwFOIUzqIAHN1CHO2hAExgM4Rle4c0Rzovz7nwsWtecfOYE/sD5/AGIgI1F</latexit>\n( b )\n<latexit sha1_base64=\"SMXWLN4fAamBJ8whz7/XI0gMaIk=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSLUS0lEsceCF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/nbX1jc2t7cJOcXdv/+CwdHTcMnGqGW+yWMa6E1DDpVC8iQIl7ySa0yiQvB2Mb2d++4lrI2L1iJOE+xEdKhEKRtFKDxV20S+V3ao7B1klXk7KkKPRL331BjFLI66QSWpM13MT9DOqUTDJp8VeanhC2ZgOeddSRSNu/Gx+6pScW2VAwljbUkjm6u+JjEbGTKLAdkYUR2bZm4n/ed0Uw5qfCZWkyBVbLApTSTAms7/JQGjOUE4soUwLeythI6opQ5tO0YbgLb+8SlqXVe+66t5fleu1PI4CnMIZVMCDG6jDHTSgCQyG8Ayv8OZI58V5dz4WrWtOPnMCf+B8/gCKBY1G</latexit>\n( c )\n<latexit sha1_base64=\"4XUGxwNURwmnEZdHAYd77F3yrjU=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4Koko9iIUvHhswX5AG8JmO2mXbjZxdyOU0j/hxYMiXv073vw3btsctPXBwOO9GWbmhang2rjut7O2vrG5tV3YKe7u7R8clo6OWzrJFMMmS0SiOiHVKLjEpuFGYCdVSONQYDsc3c389hMqzRP5YMYp+jEdSB5xRo2VOo3bdtCIAh6Uym7FnYOsEi8nZchRD0pfvX7CshilYYJq3fXc1PgTqgxnAqfFXqYxpWxEB9i1VNIYtT+Z3zsl51bpkyhRtqQhc/X3xITGWo/j0HbG1Az1sjcT//O6mYmq/oTLNDMo2WJRlAliEjJ7nvS5QmbE2BLKFLe3EjakijJjIyraELzll1dJ67LiXVfcxlW5Vs3jKMApnMEFeHADNbiHOjSBgYBneIU359F5cd6dj0XrmpPPnMAfOJ8/adiPhw==</latexit>\nQ = W Q f i\n<latexit sha1_base64=\"NFLzNNQxUfcVflQnMcUwpbaP+HI=\">AAAB73icbVBNSwMxEJ34WetX1aOXYBE8lV1R7EUoeBF6qWA/oF2WbJptY7PZNckKZemf8OJBEa/+HW/+G9N2D9r6YODx3gwz84JEcG0c5xutrK6tb2wWtorbO7t7+6WDw5aOU0VZk8YiVp2AaCa4ZE3DjWCdRDESBYK1g9HN1G8/MaV5LO/NOGFeRAaSh5wSY6VO/brt10P/wS+VnYozA14mbk7KkKPhl756/ZimEZOGCqJ113US42VEGU4FmxR7qWYJoSMyYF1LJYmY9rLZvRN8apU+DmNlSxo8U39PZCTSehwFtjMiZqgXvan4n9dNTVj1Mi6T1DBJ54vCVGAT4+nzuM8Vo0aMLSFUcXsrpkOiCDU2oqINwV18eZm0zivuZcW5uyjXqnkcBTiGEzgDF66gBrfQgCZQEPAMr/CGHtELekcf89YVlM8cwR+gzx9Y8I98</latexit>\nK = W K f j\n<latexit sha1_base64=\"8HhSfiQ1YEjRG3PALXyrVmoLveQ=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4Koko9iIUvHisYNNCG8Jmu2nXbnbj7kYooX/CiwdFvPp3vPlv3LY5aOuDgcd7M8zMi1LOtHHdb2dldW19Y7O0Vd7e2d3brxwc+lpmitAWkVyqToQ15UzQlmGG006qKE4iTtvR6Gbqt5+o0kyKezNOaZDggWAxI9hYqeNft0M/Dh/CStWtuTOgZeIVpAoFmmHlq9eXJEuoMIRjrbuem5ogx8owwumk3Ms0TTEZ4QHtWipwQnWQz+6doFOr9FEslS1h0Ez9PZHjROtxEtnOBJuhXvSm4n9eNzNxPciZSDNDBZkvijOOjETT51GfKUoMH1uCiWL2VkSGWGFibERlG4K3+PIy8c9r3mXNvbuoNupFHCU4hhM4Aw+uoAG30IQWEODwDK/w5jw6L8678zFvXXGKmSP4A+fzB3q2j5I=</latexit>\nV = W V f j\n<latexit sha1_base64=\"YxgTbE4Tg2Dm9Lslt6jOsGRRbvU=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEsceCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipOR6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasKan3GZpAYlWy4KU0FMTOZfkyFXyIyYWkKZ4vZWwsZUUWZsNiUbgrf68jppX1W9m6rbvK7Ua3kcRTiDc7gED26hDvfQgBYwQHiGV3hzHp0X5935WLYWnHzmFP7A+fwBzEmM5g==</latexit>\nh\n<latexit sha1_base64=\"U1rdFGwGh0pcwnJ90JyafWom9uc=\">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZKC6EoK3QhuKtgH1CJJOq1j00ycTMRaXPkDbvXHxD/Qv/DOGEEtohOSnDn3njNz7/XjkCfKcV5y1szs3PxCfrGwtLyyulZc32gmIpUBawQiFLLtewkLecQaiquQtWPJvJEfspY/rOl465rJhIvoVI1j1h15g4j3eeApopo1ERE6L5acsmOWPQ3cDJSQrbooPuMMPQgESDECQwRFOISHhJ4OXDiIietiQpwkxE2c4Q4F0qaUxSjDI3ZI3wHtOhkb0V57JkYd0CkhvZKUNnZIIyhPEtan2SaeGmfN/uY9MZ76bmP6+5nXiFiFC2L/0n1m/lena1Ho48DUwKmm2DC6uiBzSU1X9M3tL1UpcoiJ07hHcUk4MMrPPttGk5jadW89E381mZrV+yDLTfGmb0kDdn+Ocxo0K2V3r+ycVErVw2zUeWxhG7s0z31UcYQ6GuR9iQc84sk6tq6sG+v2I9XKZZpNfFvW/TuhNZGN</latexit>\nConcat\n<latexit sha1_base64=\"PhpH5+dpLtKsiBxC1lbrw82me1Y=\">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZKC6EoKbgQpVLAPqEUm6bTG5uVkItbiyh9wqz8m/oH+hXfGFNQiOiHJmXPvOTP3Xif2vURa1mvOmJmdm1/ILxaWlldW14rrG80kSoXLG27kR6LtsIT7Xsgb0pM+b8eCs8DxecsZHql464aLxIvCMzmKeTdgg9Drey6TRDVrTNZS/6JYssqWXuY0sDNQQrbqUfEF5+ghgosUAThCSMI+GBJ6OrBhISauizFxgpCn4xz3KJA2pSxOGYzYIX0HtOtkbEh75ZlotUun+PQKUprYIU1EeYKwOs3U8VQ7K/Y377H2VHcb0d/JvAJiJS6J/Us3yfyvTtUi0ceBrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrJ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMaNCtle69snVZK1cNs1HlsYRu7NM99VHGMOhrkfYVHPOHZODGujVvj7jPVyGWaTXxbxsMHjimRhQ==</latexit>\nMatMul\n<latexit sha1_base64=\"PhpH5+dpLtKsiBxC1lbrw82me1Y=\">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZKC6EoKbgQpVLAPqEUm6bTG5uVkItbiyh9wqz8m/oH+hXfGFNQiOiHJmXPvOTP3Xif2vURa1mvOmJmdm1/ILxaWlldW14rrG80kSoXLG27kR6LtsIT7Xsgb0pM+b8eCs8DxecsZHql464aLxIvCMzmKeTdgg9Drey6TRDVrTNZS/6JYssqWXuY0sDNQQrbqUfEF5+ghgosUAThCSMI+GBJ6OrBhISauizFxgpCn4xz3KJA2pSxOGYzYIX0HtOtkbEh75ZlotUun+PQKUprYIU1EeYKwOs3U8VQ7K/Y377H2VHcb0d/JvAJiJS6J/Us3yfyvTtUi0ceBrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrJ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMaNCtle69snVZK1cNs1HlsYRu7NM99VHGMOhrkfYVHPOHZODGujVvj7jPVyGWaTXxbxsMHjimRhQ==</latexit>\nMatMul\n<latexit sha1_base64=\"PhpH5+dpLtKsiBxC1lbrw82me1Y=\">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZKC6EoKbgQpVLAPqEUm6bTG5uVkItbiyh9wqz8m/oH+hXfGFNQiOiHJmXPvOTP3Xif2vURa1mvOmJmdm1/ILxaWlldW14rrG80kSoXLG27kR6LtsIT7Xsgb0pM+b8eCs8DxecsZHql464aLxIvCMzmKeTdgg9Drey6TRDVrTNZS/6JYssqWXuY0sDNQQrbqUfEF5+ghgosUAThCSMI+GBJ6OrBhISauizFxgpCn4xz3KJA2pSxOGYzYIX0HtOtkbEh75ZlotUun+PQKUprYIU1EeYKwOs3U8VQ7K/Y377H2VHcb0d/JvAJiJS6J/Us3yfyvTtUi0ceBrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrJ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMaNCtle69snVZK1cNs1HlsYRu7NM99VHGMOhrkfYVHPOHZODGujVvj7jPVyGWaTXxbxsMHjimRhQ==</latexit>\nMatMul\n<latexit sha1_base64=\"PhpH5+dpLtKsiBxC1lbrw82me1Y=\">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZKC6EoKbgQpVLAPqEUm6bTG5uVkItbiyh9wqz8m/oH+hXfGFNQiOiHJmXPvOTP3Xif2vURa1mvOmJmdm1/ILxaWlldW14rrG80kSoXLG27kR6LtsIT7Xsgb0pM+b8eCs8DxecsZHql464aLxIvCMzmKeTdgg9Drey6TRDVrTNZS/6JYssqWXuY0sDNQQrbqUfEF5+ghgosUAThCSMI+GBJ6OrBhISauizFxgpCn4xz3KJA2pSxOGYzYIX0HtOtkbEh75ZlotUun+PQKUprYIU1EeYKwOs3U8VQ7K/Y377H2VHcb0d/JvAJiJS6J/Us3yfyvTtUi0ceBrsGjmmLNqOrczCXVXVE3N79UJckhJk7hHsUFYVcrJ302tSbRtaveMh1/05mKVXs3y03xrm5JA7Z/jnMaNCtle69snVZK1cNs1HlsYRu7NM99VHGMOhrkfYVHPOHZODGujVvj7jPVyGWaTXxbxsMHjimRhQ==</latexit>\nMatMul\n<latexit sha1_base64=\"lC7cP4gSkeboFw+jM4J7tCJKb/U=\">AAACynicjVHLSsNAFD2Nr1pfVZdugkVwVZKC6EoKbly4qGgfUItM0mkNTTJhMhFLcecPuNUPE/9A/8I7YwpqEZ2Q5My559yZe6+XhEGqHOe1YM3NLywuFZdLK6tr6xvlza1WKjLp86YvQiE7Hkt5GMS8qQIV8k4iOYu8kLe90YmOt2+5TAMRX6pxwnsRG8bBIPCZIqp9IQYqYnfX5YpTdcyyZ4Gbgwry1RDlF1yhDwEfGSJwxFCEQzCk9HThwkFCXA8T4iShwMQ57lEib0YqTgpG7Ii+Q9p1czamvc6ZGrdPp4T0SnLa2COPIJ0krE+zTTwzmTX7W+6JyanvNqa/l+eKiFW4IfYv31T5X5+uRWGAI1NDQDUlhtHV+XmWzHRF39z+UpWiDAlxGvcpLgn7xjnts208qald95aZ+JtRalbv/Vyb4V3fkgbs/hznLGjVqu5B1TmvVerH+aiL2MEu9mmeh6jjFA00TZWPeMKzdWZJa2xNPqVWIfds49uyHj4AEa2SIQ==</latexit>\nSoftmax\n<latexit sha1_base64=\"QzIH/J+GsHQgFrA7ZQ2rOyDTwO8=\">AAAC2nicjVHLSsNAFD3GV31XxZWbYBHcWFJBdOlr4bKCrYVaJJmONTTJhMlEKMWNO3HrD7jVDxL/QP/CO9Mp+EB0QpIz595zZu69QRqFmfK81xFndGx8YrIwNT0zOze/UFxcqmcil4zXmIiEbAR+xqMw4TUVqog3Usn9OIj4WdA91PGzay6zUCSnqpfyVux3kvAyZL4i6qK4ciTUZlWKds6Uu68UTwZ8ySt7Zrk/QcWCEuyqiuILztGGAEOOGBwJFOEIPjJ6mqjAQ0pcC33iJKHQxDluME3anLI4ZfjEdunboV3TsgnttWdm1IxOieiVpHSxThpBeZKwPs018dw4a/Y3777x1Hfr0T+wXjGxClfE/qUbZv5Xp2tRuMSuqSGkmlLD6OqYdclNV/TN3U9VKXJIidO4TXFJmBnlsM+u0WSmdt1b38TfTKZm9Z7Z3Bzv+pY04Mr3cf4E9a1yZbvsnWyV9g7sqAtYxRo2aJ472MMxqqiRdx+PeMKzc+7cOnfO/SDVGbGaZXxZzsMHPuyYCw==</latexit>\nDot-Product Attention\n<latexit sha1_base64=\"qnMzXGhZ9eQLsFMyeb/FjAtfOZA=\">AAAC1XicjVHLSsNAFD2Nr1pfUZdugkVwVZKC6LLqxoWLCvYBtUgyndbQNAmTSaGU7sStP+BWf0n8A/0L70xTUIvohCRnzr3nzNx7vTjwE2nbbzljYXFpeSW/Wlhb39jcMrd36kmUCsZrLAoi0fTchAd+yGvSlwFvxoK7Ay/gDa9/ruKNIReJH4XXchTz9sDthX7XZ64k6tY0L0noCutUSh5OqaJdsvWy5oGTgSKyVY3MV9yggwgMKQbgCCEJB3CR0NOCAxsxcW2MiROEfB3nmKBA2pSyOGW4xPbp26NdK2ND2ivPRKsZnRLQK0hp4YA0EeUJwuo0S8dT7azY37zH2lPdbUR/L/MaECtxR+xfulnmf3WqFokuTnQNPtUUa0ZVxzKXVHdF3dz6UpUkh5g4hTsUF4SZVs76bGlNomtXvXV1/F1nKlbtWZab4kPdkgbs/BznPKiXS85Ryb4qFytn2ajz2MM+Dmmex6jgAlXUyHuIJzzjxWgYE+PeeJimGrlMs4tvy3j8BPollf8=</latexit>\nLinear Attention\n<latexit sha1_base64=\"f4jXYSUiJn4KudYnZNr5TSSFBPI=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRJRdCUFN11WtA+opSTTaR2aF5OJIkXwB9zqp4l/oH/hnTEFtYhOSHLm3HvOzL3XTwKRKsd5LVhz8wuLS8Xl0srq2vpGeXOrlcaZZLzJ4iCWHd9LeSAi3lRCBbyTSO6FfsDb/vhMx9s3XKYiji7VXcJ7oTeKxFAwTxF1MeyLfrniVB2z7Fng5qCCfDXi8guuMEAMhgwhOCIowgE8pPR04cJBQlwPE+IkIWHiHPcokTajLE4ZHrFj+o5o183ZiPbaMzVqRqcE9EpS2tgjTUx5krA+zTbxzDhr9jfvifHUd7ujv597hcQqXBP7l26a+V+drkVhiBNTg6CaEsPo6ljukpmu6JvbX6pS5JAQp/GA4pIwM8ppn22jSU3tureeib+ZTM3qPctzM7zrW9KA3Z/jnAWtg6p7VHXODyu103zURexgF/s0z2PUUEcDTfIe4RFPeLbqVmRl1u1nqlXINdv4tqyHD2hFkEc=</latexit>\nf i\n<latexit sha1_base64=\"XHptQbu2aN/FQZticJn/tr7CI8o=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRJRdCUFN11WtA+opSTTaY1NkzCZKKUI/oBb/TTxD/QvvDNOQS2iE5KcOfeeM3Pv9ZMwSKXjvOasufmFxaX8cmFldW19o7i51UjjTDBeZ3EYi5bvpTwMIl6XgQx5KxHcG/khb/rDMxVv3nKRBnF0KccJ74y8QRT0A+ZJoi763ZtuseSUHb3sWeAaUIJZtbj4giv0EIMhwwgcESThEB5Setpw4SAhroMJcYJQoOMc9yiQNqMsThkesUP6DmjXNmxEe+WZajWjU0J6BSlt7JEmpjxBWJ1m63imnRX7m/dEe6q7jenvG68RsRLXxP6lm2b+V6dqkejjRNcQUE2JZlR1zLhkuivq5vaXqiQ5JMQp3KO4IMy0ctpnW2tSXbvqrafjbzpTsWrPTG6Gd3VLGrD7c5yzoHFQdo/KzvlhqXJqRp3HDnaxT/M8RgVV1FAn7wEe8YRnq2pFVmbdfaZaOaPZxrdlPXwAaqWQSA==</latexit>\nf j\n<latexit sha1_base64=\"f4jXYSUiJn4KudYnZNr5TSSFBPI=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVRJRdCUFN11WtA+opSTTaR2aF5OJIkXwB9zqp4l/oH/hnTEFtYhOSHLm3HvOzL3XTwKRKsd5LVhz8wuLS8Xl0srq2vpGeXOrlcaZZLzJ4iCWHd9LeSAi3lRCBbyTSO6FfsDb/vhMx9s3XKYiji7VXcJ7oTeKxFAwTxF1MeyLfrniVB2z7Fng5qCCfDXi8guuMEAMhgwhOCIowgE8pPR04cJBQlwPE+IkIWHiHPcokTajLE4ZHrFj+o5o183ZiPbaMzVqRqcE9EpS2tgjTUx5krA+zTbxzDhr9jfvifHUd7ujv597hcQqXBP7l26a+V+drkVhiBNTg6CaEsPo6ljukpmu6JvbX6pS5JAQp/GA4pIwM8ppn22jSU3tureeib+ZTM3qPctzM7zrW9KA3Z/jnAWtg6p7VHXODyu103zURexgF/s0z2PUUEcDTfIe4RFPeLbqVmRl1u1nqlXINdv4tqyHD2hFkEc=</latexit>\nf i\n<latexit sha1_base64=\"TAABLtYdTuqDTFnOGcGO7GJUajc=\">AAAB+HicbVDLSgNBEOz1GeMjqx69DAbBU9gNiB4jXjx4iGAekCxhdjKbDJl9MNMrrCFf4sWDIl79FG/+jbPJHjSxYKCo6u7pLj+RQqPjfFtr6xubW9ulnfLu3v5BxT48aus4VYy3WCxj1fWp5lJEvIUCJe8mitPQl7zjT25yv/PIlRZx9IBZwr2QjiIRCEbRSAO7co3Io5yTO5pxNbCrTs2Zg6wStyBVKNAc2F/9YczS0Axhkmrdc50EvSlVKJjks3I/1TyhbEJHvGdoREOuvel88Rk5M8qQBLEyL0IyV393TGmodRb6pjKkONbLXi7+5/VSDK68qYiS1FzHFh8FqSQYkzwFMhSKM5SZIZQpYXYlbEwVZWiyKpsQ3OWTV0m7XnMvas59vdpoFHGU4ARO4RxcuIQG3EITWsAghWd4hTfryXqx3q2PRemaVfQcwx9Ynz+qjZMU</latexit>\nAttention Layer\n<latexit sha1_base64=\"dBKBSog7S9HuDKyipGTcDMGUarc=\">AAAB+HicbVBNT8JAEJ3iF+IHVY9eNoKJJ9KSGDmSyMEjJgIm0DTbZQsbtttmd0uCDb/EiweN8epP8ea/cYEeFHzJJC/vzWRmXpBwprTjfFuFre2d3b3ifung8Oi4bJ+cdlWcSkI7JOaxfAywopwJ2tFMc/qYSIqjgNNeMLld+L0plYrF4kHPEupFeCRYyAjWRvLtcrXlTwaaRVShlj+t+nbFqTlLoE3i5qQCOdq+/TUYxiSNqNCEY6X6rpNoL8NSM8LpvDRIFU0wmeAR7RsqsNnkZcvD5+jSKEMUxtKU0Gip/p7IcKTULApMZ4T1WK17C/E/r5/qsOFlTCSppoKsFoUpRzpGixTQkElKNJ8Zgolk5lZExlhiok1WJROCu/7yJunWa+51zbmvV5qNPI4inMMFXIELN9CEO2hDBwik8Ayv8GY9WS/Wu/Wxai1Y+cwZ/IH1+QN2u5JG</latexit>\nD k ⇥ D v\n<latexit sha1_base64=\"nkvCLAreyMro5M9CC3ZOHe1LSyI=\">AAAB+HicbVBNT8JAEJ3iF+IHVY9eNoKJJ9KSGDmSePFEMBEwgabZLlvYsN3W3a0JNvwSLx40xqs/xZv/xgV6UPAlk7y8N5OZeUHCmdKO820VNja3tneKu6W9/YPDsn103FVxKgntkJjH8j7AinImaEczzel9IimOAk57weR67vceqVQsFnd6mlAvwiPBQkawNpJvl6st/2GgWUQVavmTqm9XnJqzAFonbk4qkKPt21+DYUzSiApNOFaq7zqJ9jIsNSOczkqDVNEEkwke0b6hAptNXrY4fIbOjTJEYSxNCY0W6u+JDEdKTaPAdEZYj9WqNxf/8/qpDhtexkSSairIclGYcqRjNE8BDZmkRPOpIZhIZm5FZIwlJtpkVTIhuKsvr5NuveZe1pzbeqXZyOMowimcwQW4cAVNuIE2dIBACs/wCm/Wk/VivVsfy9aClc+cwB9Ynz+OTpJV</latexit>\nN q ⇥ N k\n<latexit sha1_base64=\"us089y29wdz+cA7jZv34rWkHHmo=\">AAACEHicbVBLSgNBFOzxG+Nvoks3jcHgKswERFcSyMaFSATzgcwQejo9SZP+DN09Sgi5hHu3egV34tYbeAOPYSeZhUkseFBUveI9KkoY1cbzvp219Y3Nre3cTn53b//g0C0cNbVMFSYNLJlU7QhpwqggDUMNI+1EEcQjRlrRsDb1W49EaSrFgxklJOSoL2hMMTJW6rqFGjJB6damkQpKd1Lxrlv0yt4McJX4GSmCDPWu+xP0JE45EQYzpHXH9xITjpEyFDMyyQepJgnCQ9QnHUsF4kSH49nrE3hmlR6MpbIjDJypfxNjxLUe8chucmQGetmbiv95ndTEV+GYiiQ1ROD5oThl0Eg47QH2qCLYsJElCCtqf4V4gBTCxra1cEVQTGJrTPK2Gn+5iFXSrJT9i7J3XylWr7OScuAEnIJz4INLUAU3oA4aAIMn8AJewZvz7Lw7H87nfHXNyTLHYAHO1y9e75yc</latexit>\nCat&Linear&Norm\n<latexit sha1_base64=\"07FK7xSW8qBFutnd9obD/b2ZSTs=\">AAACEXicbVDLSsNAFJ3UV62vWpdugkVxY0kKoispCMWVVLAPaEOZTG7aoZOZMDNRS+hXuHerv+BO3PoF/oGf4fSxsK0HLhzOuZdzOX7MqNKO821lVlbX1jeym7mt7Z3dvfx+oaFEIgnUiWBCtnysgFEOdU01g1YsAUc+g6Y/uB77zQeQigp+r4cxeBHucRpSgrWRuvlCFSA4qwr5iGXQObkVMurmi07JmcBeJu6MFNEMtW7+pxMIkkTANWFYqbbrxNpLsdSUMBjlOomCGJMB7kHbUI4jUF46+X1kHxslsEMhzXBtT9S/FymOlBpGvtmMsO6rRW8s/ue1Ex1eeinlcaKBk2lQmDBbC3tchB1QCUSzoSGYSGp+tUkfS0y0qWsuhVMCoTFGOVONu1jEMmmUS+55ybkrFytXs5Ky6BAdoVPkogtUQTeohuqIoCf0gl7Rm/VsvVsf1ud0NWPNbg7QHKyvX1o/nSc=</latexit>\nFeed-Forward&Norm\n<latexit sha1_base64=\"C0y8xmW2B+t4PdH2qwF/9HwQ3J4=\">AAACAHicbVBNS8NAEJ3Ur1q/qh69BIvgqSQF0ZNUvHisaD+gDWWzmbRLN5uwuxFK6MW7V/0L3sSr/8R/4M9w2+ZgWx8MPN6bYWaen3CmtON8W4W19Y3NreJ2aWd3b/+gfHjUUnEqKTZpzGPZ8YlCzgQ2NdMcO4lEEvkc2/7oduq3n1AqFotHPU7Qi8hAsJBRoo30cBME/XLFqToz2KvEzUkFcjT65Z9eENM0QqEpJ0p1XSfRXkakZpTjpNRLFSaEjsgAu4YKEqHystmpE/vMKIEdxtKU0PZM/TuRkUipceSbzojooVr2puJ/XjfV4ZWXMZGkGgWdLwpTbuvYnv5tB0wi1XxsCKGSmVttOiSSUG3SWdgiGMXQGJOSicZdDmKVtGpV96Lq3Ncq9es8pCKcwCmcgwuXUIc7aEATKAzgBV7hzXq23q0P63PeWrDymWNYgPX1C5z1ltU=</latexit>\nAdd\nFigure 3: Encoder layer and attention layer in LoFTR.\n(a) Transformer encoder layer. h represents the multiple\nheads of attention. (b) Vanilla dot-product attention with\nO(N2) complexity. (c) Linear attention layer with O(N)\ncomplexity. The scale factor is omitted for simplicity.\nlayer. The input vectors for an attention layer are conven-\ntionally named query, key, and value. Analogous to infor-\nmation retrieval, the query vector Q retrieves information\nfrom the value vector V, according to the attention weight\ncomputed from the dot product of Qand the key vector K\ncorresponding to each value V. The computation graph of\nthe attention layer is presented in Fig. 3(b). Formally, the\nattention layer is denoted as:\nAttention(Q,K,V ) = softmax(QKT)V.\nIntuitively, the attention operation selects the relevant infor-\nmation by measuring the similarity between the query ele-\nment and each key element. The output vector is the sum\nof the value vectors weighted by the similarity scores. As a\nresult, the relevant information is extracted from the value\nvector if the similarity is high. This process is also called\n“message passing” in Graph Neural Network.\nLinear Transformer.Denoting the length of Q and K as\nN and their feature dimension as D, the dot product be-\ntween Qand K in the Transformer introduces computation\ncost that grows quadratically ( O(N2)) with the length of\nthe input sequence. Directly applying the vanilla version of\nTransformer in the context of local feature matching is im-\npractical even when the input length is reduced by the local\nfeature CNN. To remedy this problem, we propose to use\nan efﬁcient variant of the vanilla attention layer in Trans-\nformer. Linear Transformer [17] proposes to reduce the\ncomputation complexity of Transformer to O(N) by sub-\nstituting the exponential kernel used in the original atten-\ntion layer with an alternative kernel function sim(Q,K) =\nφ(Q) ·φ(K)T,where φ(·) = elu(·) + 1. This operation is\nillustrated by the computation graph in Fig. 3(c). Utilizing\nSelf\nAttention\nCross\nAttention\nFeature \nVisualization\nL R\nConv #1\nConv #2\nConv #3\nTransformer #1\nL R\nATTENTION\n(b)\n(a)\n(c)\nFigure 4: Illustration of the receptive ﬁeld of (a) Convolu-\ntions and (b) Transformers. Assume that the objective is\nto establish a connection between the L and R elements to\nextract their joint feature representation. Due to the local-\nconnectivity of convolutions, many convolution layers need\nto be stacked together in order to achieve this connection.\nThe global receptive ﬁeld of Transformers enables this con-\nnection to be established through only one attention layer.\n(c) Visualization of the attention weights and transformed\ndense features. We use PCA to reduce the dimension of the\ntransformed features ˜FA\ntr and ˜FB\ntr and visualize the results\nwith RGB color. Zoom in for details.\nthe associativity property of matrix products, the multipli-\ncation between φ(K)T and V can be carried out ﬁrst. Since\nD≪N, the computation cost is reduced to O(N).\nPositional Encoding. We use the 2D extension of the\nstandard positional encoding in Transformers following\nDETR [3]. Different from DETR, we only add them to the\nbackbone output once. We leave the formal deﬁnition of\nthe positional encoding in the supplementary material. In-\ntuitively, the positional encoding gives each element unique\nposition information in the sinusoidal format. By adding\nthe position encoding to ˜FA and ˜FB, the transformed fea-\ntures will become position-dependent, which is crucial to\nthe ability of LoFTR to produce matches in indistinctive re-\ngions. As shown in the bottom row of Fig. 4(c), although\nthe input RGB color is homogeneous on the white walls,\nthe transformed features ˜FA\ntr and ˜FB\ntr are unique for each\nposition demonstrated by the smooth color gradients. More\nvisualizations are provided in Fig. 6.\nSelf-attention and Cross-attention Layers. For self-\nattention layers, the input features fi and fj (shown in\nFig. 3) are the same (either ˜FA or ˜FB). For cross-attention\nlayers, the input features fi and fj are either ( ˜FA and\n˜FB) or ( ˜FB and ˜FA) depending on the direction of cross-\nattention. Following [37], we interleave the self and cross\nattention layers in the LoFTR module by Nc times. The\nattention weights of the self and cross attention layers in\nLoFTR are visualized in the ﬁrst two rows of Fig. 4(c).\n4\n3.3. Establishing Coarse-level Matches\nTwo types of differentiable matching layers can be ap-\nplied in LoFTR, either with an optimal transport (OT) layer\nas in [37] or with a dual-softmax operator [34, 47]. The\nscore matrix Sbetween the transformed features is ﬁrst cal-\nculated by S(i,j) = 1\nτ ·⟨˜FA\ntr(i), ˜FB\ntr(j)⟩. When matching\nwith OT, −Scan be used as the cost matrix of the partial\nassignment problem as in [37]. We can also apply softmax\non both dimensions (referred to as dual-softmax in the fol-\nlowing) of Sto obtain the probability of soft mutual nearest\nneighbor matching. Formally, when using dual-softmax, the\nmatching probability Pc is obtained by:\nPc(i,j) = softmax (S(i,·))j ·softmax (S(·,j))i.\nMatch Selection. Based on the conﬁdence matrix Pc, we\nselect matches with conﬁdence higher than a threshold of\nθc, and further enforce the mutual nearest neighbor (MNN)\ncriteria, which ﬁlters possible outlier coarse matches. We\ndenote the coarse-level match predictions as:\nMc = {\n(˜i,˜j\n)\n|∀\n(˜i,˜j\n)\n∈MNN (Pc) , Pc\n(˜i,˜j\n)\n≥θc}.\n3.4. Coarse-to-Fine Module\nAfter establishing coarse matches, these matches are re-\nﬁned to the original image resolution with the coarse-to-\nﬁne module. Inspired by [50], we use a correlation-based\napproach for this purpose. For every coarse match (˜i,˜j),\nwe ﬁrst locate its position (ˆi,ˆj) at ﬁne-level feature maps\nˆFA and ˆFB, and then crop two sets of local windows of\nsize w×w. A smaller LoFTR module then transforms the\ncropped features within each window byNf times, yielding\ntwo transformed local feature maps ˆFA\ntr(ˆi) and ˆFB\ntr(ˆj) cen-\ntered at ˆiand ˆj, respectively. Then, we correlate the center\nvector of ˆFA\ntr(ˆi) with all vectors in ˆFB\ntr(ˆj) and thus produce\na heatmap that represents the matching probability of each\npixel in the neighborhood of ˆj with ˆi. By computing ex-\npectation over the probability distribution, we get the ﬁnal\nposition ˆj′with sub-pixel accuracy onIB. Gathering all the\nmatches {(ˆi,ˆj′)}produces the ﬁnal ﬁne-level matchesMf.\n3.5. Supervision\nThe ﬁnal loss consists of the losses for the coarse-level\nand the ﬁne-level: L= Lc + Lf.\nCoarse-level Supervision. The loss function for the\ncoarse-level is the negative log-likelihood loss over the con-\nﬁdence matrix Pc returned by either the optimal trans-\nport layer or the dual-softmax operator. We follow Super-\nGlue [37] to use camera poses and depth maps to compute\nthe ground-truth labels for the conﬁdence matrix during\ntraining. We deﬁne the ground-truth coarse matchesMgt\nc as\nthe mutual nearest neighbors of the two sets of1/8-resolution\ngrids. The distance between two grids is measured by the\nre-projection distance of their central locations. More de-\ntails are provided in the supplementary. With the optimal\ntransport layer, we use the same loss formulation as in [37].\nWhen using dual-softmax for matching, we minimize the\nnegative log-likelihood loss over the grids in Mgt\nc :\nLc = − 1\n|Mgt\nc |\n∑\n(˜i,˜j)∈Mgt\nc\nlog Pc\n(˜i,˜j\n)\n.\nFine-level Supervision. We use the ℓ2 loss for ﬁne-level\nreﬁnement. Following [50], for each query point ˆi, we\nalso measure its uncertainty by calculating the total variance\nσ2(ˆi) of the corresponding heatmap. The target is to opti-\nmize the reﬁned position that has low uncertainty, resulting\nin the ﬁnal weighted loss function:\nLf = 1\n|Mf|\n∑\n(ˆi,ˆj′)∈Mf\n1\nσ2(ˆi)\nˆj′−ˆj′\ngt\n\n2\n,\nin which ˆj′\ngt is calculated by warping each ˆifrom ˆFA\ntr(ˆi) to\nˆFB\ntr(ˆj) with the ground-truth camera pose and depth. We\nignore (ˆi, ˆj′) if the warped location ofˆifalls out of the local\nwindow of ˆFB\ntr(ˆj) when calculating Lf. The gradient is not\nbackpropagated through σ2(ˆi) during training.\n3.6. Implementation Details\nWe train the indoor model of LoFTR on the ScanNet [7]\ndataset and the outdoor model on the MegaDepth [21] fol-\nlowing [37]. On ScanNet, the model is trained using Adam\nwith an initial learning rate of 1 ×10−3 and a batch size\nof 64. It converges after 24 hours of training on 64 GTX\n1080Ti GPUs. The local feature CNN uses a modiﬁed ver-\nsion of ResNet-18 [12] as the backbone. The entire model\nis trained end-to-end with randomly initialized weights. Nc\nis set to 4 and Nf is 1. θc is chosen to 0.2. Window size\nw is equal to 5. ˜FA\ntr and ˜FB\ntr are upsampled and concate-\nnated with ˆFAand ˆFB before passing through the ﬁne-level\nLoFTR in the implementation. The full model with dual-\nsoftmax matching runs at 116 ms for a 640×480 image pair\non an RTX 2080Ti. Under the optimal transport setup, we\nuse three sinkhorn iterations, and the model runs at 130 ms.\nWe refer readers to the supplementary material for more de-\ntails of training and timing analyses.\n4. Experiments\n4.1. Homography Estimation\nIn the ﬁrst experiment, we evaluate LoFTR on the widely\nadopted HPatches dataset [1] for homography estimation.\nHPatches contains 52 sequences under signiﬁcant illumina-\ntion changes and 56 sequences that exhibit large variation\nin viewpoints.\n5\nCategory Method Homography est. AUC #matches@3px @5px @10px\nDetector-based\nD2Net [11]+NN 23.2 35.9 53.6 0.2K\nR2D2 [32]+NN 50.6 63.9 76.8 0.5K\nDISK [47]+NN 52.3 64.9 78.9 1.1K\nSP [9]+SuperGlue [37] 53.9 68.3 81.7 0.6K\nDetector-free\nSparse-NCNet [33] 48.9 54.2 67.1 1.0K\nDRC-Net [19] 50.6 56.2 68.3 1.0K\nLoFTR-DS 65.9 75.6 84.6 1.0K\nTable 1: Homography estimation on HPatches [7].The\nAUC of the corner error in percentage is reported. The\nsufﬁx DS indicates the differentiable matching with dual-\nsoftmax.\nEvaluation protocol. In every test sequence, one reference\nimage is paired with the rest ﬁve images. All images are re-\nsized with shorter dimensions equal to 480. For each image\npair, we extract a set of matches with LoFTR trained on\nMegaDepth [21]. We use OpenCV to compute the homog-\nraphy estimation with RANSAC as the robust estimator. To\nmake a fair comparison to methods that produce different\nnumbers of matches, we compute the corner error between\nthe images warped with the estimated ˆHand the ground-\ntruth Has a correctness identiﬁer as in [9]. Following [37],\nwe report the area under the cumulative curve (AUC) of the\ncorner error up to threshold values of 3, 5, and 10 pixels, re-\nspectively. We report the results of LoFTR with a maximum\nof 1K output matches.\nBaseline methods. We compare LoFTR with three cate-\ngories of methods: 1) detector-based local features includ-\ning R2D2 [32], D2Net [11], and DISK [47], 2) a detector-\nbased local feature matcher, i.e., SuperGlue [37] on top of\nSuperPoint [9] features, and 3) detector-free matchers in-\ncluding Sparse-NCNet [33] and DRC-Net [19]. For local\nfeatures, we extract a maximum of 2K features with which\nwe extract mutual nearest neighbors as the ﬁnal matches.\nFor methods directly outputting matches, we restrict a max-\nimum of 1K matches, same as LoFTR. We use the default\nhyperparameters in the original implementations for all the\nbaselines.\nResults. Tab. 1 shows that LoFTR notably outperforms\nother baselines under all error thresholds by a signiﬁcant\nmargin. Speciﬁcally, the performance gap between LoFTR\nand other methods increases with a stricter correctness\nthreshold. We attribute the top performance to the larger\nnumber of match candidates provided by the detector-free\ndesign and the global receptive ﬁeld brought by the Trans-\nformer. Moreover, the coarse-to-ﬁne module also con-\ntributes to the estimation accuracy by reﬁning matches to\na sub-pixel level.\n4.2. Relative Pose Estimation\nDatasets. We use ScanNet [7] and MegaDepth [21] to\ndemonstrate the effectiveness of LoFTR for pose estimation\nCategory Method\nPose estimation AUC\n@5° @10° @20°\nDetector-based\nORB [35]+GMS [2] 5.21 13.65 25.36\nD2-Net [11]+NN 5.25 14.53 27.96\nContextDesc [27]+Ratio Test [26] 6.64 15.01 25.75\nSP [9]+NN 9.43 21.53 36.40\nSP [9]+PointCN [52] 11.40 25.47 41.41\nSP [9]+OANet [53] 11.76 26.90 43.85\nSP [9]+SuperGlue [37] 16.16 33.81 51.84\nDetector-free\nDRC-Net † [19] 7.69 17.93 30.49\nLoFTR-OT† 16.88 33.62 50.62\nLoFTR-OT 21.51 40.39 57.96\nLoFTR-DS 22.06 40.8 57.62\nTable 2: Evaluation on ScanNet [7] for indoor pose es-\ntimation. The AUC of the pose error in percentage is re-\nported. LoFTR improves the state-of-the-art methods by\na large margin. †indicates models trained on MegaDepth.\nThe sufﬁxes OT and DS indicate differentiable matching\nwith optimal transport and dual-softmax, respectively.\nCategory Method\nPose estimation AUC\n@5° @10° @20°\nDetector-based SP [9]+SuperGlue [37] 42.18 61.16 75.96\nDetector-free\nDRC-Net [19] 27.01 42.96 58.31\nLoFTR-OT 50.31 67.14 79.93\nLoFTR-DS 52.8 69.19 81.18\nTable 3: Evaluation on MegaDepth [21] for outdoor pose\nestimation. Matching with LoFTR results in better perfor-\nmance in the outdoor pose estimation task.\nin indoor and outdoor scenes, respectively.\nScanNet contains 1613 monocular sequences with\nground truth poses and depth maps. Following the proce-\ndure from SuperGlue [37], we sample 230M image pairs\nfor training, with overlap scores between 0.4 and 0.8. We\nevaluate our method on the 1500 testing pairs from [37].\nAll images and depth maps are resized to 640 ×480. This\ndataset contains image pairs with wide baselines and exten-\nsive texture-less regions.\nMegaDepth consists of 1M internet images of 196 differ-\nent outdoor scenes. The authors also provide sparse recon-\nstruction from COLMAP [40] and depth maps computed\nfrom multi-view stereo. We follow DISK [47] to only use\nthe scenes of “Sacre Coeur” and “St. Peter’s Square” for\nvalidation, from which we sample 1500 pairs for a fair com-\nparison. Images are resized such that their longer dimen-\nsions are equal to 840 for training and 1200 for validation.\nThe key challenge on MegaDepth is matching under ex-\ntreme viewpoint changes and repetitive patterns.\nEvaluation protocol. Following [37], we report the AUC\nof the pose error at thresholds (5◦,10◦,20◦), where the pose\nerror is deﬁned as the maximum of angular error in rota-\ntion and translation. To recover the camera pose, we solve\nthe essential matrix from predicted matches with RANSAC.\nWe don’t compare the matching precisions between LoFTR\nand other detector-based methods due to the lack of a well-\n6\nMethod Day Night\n(0.25m,2°) / (0.5m,5°) / (1.0m,10°)\nLocal Feature Evaluation on Night-time Queries\nR2D2 [32]+NN - 71.2 / 86.9 / 98.9\nLISRD [31]+SP [9]+AdaLam [4] - 73.3 / 86.9 / 97.9\nISRF [29]+NN - 69.1 / 87.4 / 98.4\nSP [9]+SuperGlue [37] - 73.3 / 88.0 / 98.4\nLoFTR-DS - 72.8 / 88.5 / 99.0\nFull Visual Localization with HLoc\nSP [9]+SuperGlue [37] 89.8 / 96.1 / 99.4 77.0 / 90.6 / 100.0\nLoFTR-OT 88.7 / 95.6 / 99.0 78.5 / 90.6 / 99.0\nTable 4: Visual localization evaluation on the Aachen\nDay-Night [54] benchmark v1.1. The evaluation results\non both the local feature evaluation track and the full visual\nlocalization track are reported.\ndeﬁned metric (e.g., matching score or recall [13, 30]) for\ndetector-free image matching methods. We consider DRC-\nNet [19] as the state-of-the-art method in detector-free ap-\nproaches [34, 33].\nResults of indoor pose estimation.LoFTR achieves the\nbest performance in pose accuracy compared to all com-\npetitors (see Tab. 2 and Fig. 5). Pairing LoFTR with opti-\nmal transport or dual-softmax as the differentiable matching\nlayer achieves comparable performance. Since the released\nmodel of DRC-Net†is trained on MegaDepth, we provide\nthe results of LoFTR†trained on MegaDepth for a fair com-\nparison. LoFTR †also outperforms DRC-Net †by a large\nmargin in this evaluation (see Fig. 5), which demonstrates\nthe generalizability of our model across datasets.\nResults of Outdoor Pose Estimation.As shown in Tab. 3,\nLoFTR outperforms the detector-free method DRC-Net by\n61% at AUC@10°, demonstrating the effectiveness of the\nTransformer. For SuperGlue, we use the setup from the\nopen-sourced localization toolbox HLoc [36]. LoFTR out-\nperforms SuperGlue by a large margin (13% at AUC@10°),\nwhich demonstrates the effectiveness of the detector-free\ndesign. Different from indoor scenes, LoFTR-DS performs\nbetter than LoFTR-OT on MegaDepth. More qualitative re-\nsults can be found in Fig. 5.\n4.3. Visual Localization\nVisual Localization. Besides achieving competitive per-\nformance for relative pose estimation, LoFTR can also ben-\neﬁt visual localization, which is the task to estimate the 6-\nDoF poses of given images with respect to the correspond-\ning 3D scene model. We evaluate LoFTR on the Long-Term\nVisual Localization Benchmark [43] (referred to as Vis-\nLoc benchmark in the following). It focuses on benchmark-\ning visual localization methods under varying conditions,\ne.g., day-night changes, scene geometry changes, and in-\ndoor scenes with plenty of texture-less areas. Thus, the vi-\nsual localization task relies on highly robust image match-\ning methods.\nMethod DUC1 DUC2\n(0.25m,10°) / (0.5m,10°) / (1.0m,10°)\nISRF [29] 39.4 / 58.1 / 70.2 41.2 / 61.1 / 69.5\nKAPTURE [14]+R2D2 [32] 41.4 / 60.1 / 73.7 47.3 / 67.2 / 73.3\nHLoc [36]+SP [9]+SuperGlue [37] 49.0 / 68.7 / 80.8 53.4 / 77.1 / 82.4\nHLoc [36]+ LoFTR-OT 47.5 / 72.2 / 84.8 54.2 / 74.8 / 85.5\nTable 5: Visual localization evaluation on the InLoc [41]\nbenchmark.\nMethod\nPose estimation AUC\n@5° @10° @20°\n1) replace LoFTR with convolution 14.98 32.04 49.92\n2) 1/16 coarse-resolution + 1/4 ﬁne-resolution 16.75 34.82 54.0\n3) positional encoding per layer 18.02 35.64 52.77\n4) larger model with Nc = 8,Nf = 2 20.87 40.23 57.56\nFull (Nc = 4,Nf = 1) 20.06 40.8 57.62\nTable 6: Ablation study. Five variants of LoFTR are\ntrained and evaluated both on the ScanNet dataset.\nEvaluation. We evaluate LoFTR on two tracks of VisLoc\nthat consist of several challenges. First, the “visual local-\nization for handheld devices” track requires a full localiza-\ntion pipeline. It benchmarks on two datasets, the Aachen-\nDay-Night dataset [38, 54] concerning outdoor scenes and\nthe InLoc [41] dataset concerning indoor scenes. We\nuse open-sourced localization pipeline HLoc [36] with the\nmatches extracted by LoFTR. Second, the “local features\nfor long-term localization” track provides a ﬁxed localiza-\ntion pipeline to evaluate the local feature extractors them-\nselves and optionally the matchers. This track uses the\nAachen v1.1 dataset [54]. We provide the implementation\ndetails of testing LoFTR on VisLoc in the supplementary\nmaterial.\nResults. We provide evaluation results of LoFTR in Tab. 4\nand Tab. 5. We have evaluated LoFTR pairing with either\nthe optimal transport layer or the dual-softmax operator and\nreport the one with better results. LoFTR-DS outperforms\nall baselines in the local feature challenge track, showing\nits robustness under day-night changes. Then, for the vi-\nsual localization for handheld devices track, LoFTR-OT\noutperforms all published methods on the challenging In-\nLoc dataset, which contains extensive appearance changes,\nmore texture-less areas, symmetric and repetitive elements.\nWe attribute the prominence to the use of the Transformer\nand the optimal transport layer, taking advantage of global\ninformation and jointly bringing global consensus into the\nﬁnal matches. The detector-free design also plays a criti-\ncal role, preventing the repeatability problem of detector-\nbased methods in low-texture regions. LoFTR-OT performs\non par with the state-of-the-art method SuperPoint + Su-\nperGlue on night queries of the Aachen v1.1 dataset and\nslightly worse on the day queries.\n7\nIndoorOutdoor\nSuperPoint + SuperGlue DRC-Net LoFTR\nFigure 5: Qualitative results. LoFTR is compared to SuperGlue [37] and DRC-Net [19] in indoor and outdoor environ-\nments. LoFTR obtains more correct matches and fewer mismatches, successfully coping with low-texture regions and large\nviewpoint and illumination changes. The red color indicates epipolar error beyond 5 ×10−4 for indoor scenes and 1 ×10−4\nfor outdoor scenes (in the normalized image coordinates). More qualitative results can be found on the project webpage.\nSelfCross\nFeature PCA\nFigure 6: Visualization of self and cross attention weights and the transformed features.In the ﬁrst two examples, the\nquery point from the low-texture region is able to aggregate the surrounding global information ﬂexibly. For instance, the\npoint on the chair is looking at the edge of the chair. In the last two examples, the query point from the distinctive region can\nalso utilize the richer information from other regions. The feature visualization with PCA further shows that LoFTR learns a\nposition-dependent feature representation.\n4.4. Understanding LoFTR\nAblation Study. To fully understand the different modules\nin LoFTR, we evaluate ﬁve different variants with results\nshown in Tab. 6: 1) Replacing the LoFTR module by con-\nvolution with a comparable number of parameters results in\na signiﬁcant drop in AUC as expected. 2) Using a smaller\nversion of LoFTR with1/16 and 1/4 resolution feature maps at\nthe coarse and ﬁne level, respectively, results in a running\ntime of 104 ms and a degraded pose estimation accuracy. 3)\nUsing DETR-style [3] Transformer architecture which has\npositional encoding at each layer, leads to a noticeably de-\nclined result. 4) Increasing the model capacity by doubling\nthe number of LoFTR layers to Nc = 8and Nf = 2barely\nchanges the results. We conduct these experiments using\nthe same training and evaluation protocol as indoor pose\nestimation on ScanNet with an optimal transport layer for\nmatching.\nVisualizing Attention. We visualize the attention weights\nin Fig. 6.\n5. Conclusion\nThis paper presents a novel detector-free matching ap-\nproach, named LoFTR, that can establish accurate semi-\ndense matches with Transformers in a coarse-to-ﬁne man-\nner. The proposed LoFTR module uses the self and cross\nattention layers in Transformers to transform the local fea-\ntures to be context- and position-dependent, which is crucial\nfor LoFTR to obtain high-quality matches on indistinctive\nregions with low-texture or repetitive patterns. Our exper-\niments show that LoFTR achieves state-of-the-art perfor-\nmances on relative pose estimation and visual localization\non multiple datasets. We believe that LoFTR provides a new\ndirection for detector-free methods in local image feature\nmatching and can be extended to more challenging scenar-\nios, e.g., matching images with severe seasonal changes.\nAcknowledgement. The authors would like to acknowl-\nedge the support from the National Key Research and\nDevelopment Program of China (No. 2020AAA0108901),\nNSFC (No. 61806176), and ZJU-SenseTime Joint Lab of\n3D Vision.\n8\nReferences\n[1] Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-\ntian Mikolajczyk. HPatches: A benchmark and evaluation of\nhandcrafted and learned local descriptors. In CVPR, 2017. 5\n[2] JiaWang Bian, Wen-Yan Lin, Yasuyuki Matsushita, Sai-Kit\nYeung, Tan-Dat Nguyen, and Ming-Ming Cheng. GMS:\nGrid-based motion statistics for fast, ultra-robust feature cor-\nrespondence. In CVPR, 2017. 6\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 3,\n4, 8\n[4] Luca Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten\nSattler, and Marc Pollefeys. Handcrafted Outlier Detection\nRevisited. In ECCV, 2020. 7\n[5] Krzysztof Choromanski, Valerii Likhosherstov, David Do-\nhan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,\net al. Rethinking attention with performers. ICLR, 2021.\n3\n[6] Christopher B Choy, JunYoung Gwak, Silvio Savarese, and\nManmohan Chandraker. Universal correspondence network.\nNeurIPS, 2016. 2\n[7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nießner. ScanNet:\nRichly-annotated 3d reconstructions of indoor scenes. In\nCVPR, 2017. 5, 6\n[8] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\nnovich. Toward geometric deep slam. arXiv:1707.07410.\n2\n[9] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-\nnovich. SuperPoint: Self-supervised interest point detection\nand description. In CVPRW, 2018. 2, 6, 7\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. ICLR, 2021. 3\n[11] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-\nfeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net:\nA trainable cnn for joint detection and description of local\nfeatures. CVPR, 2019. 2, 6\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 5\n[13] Jared Heinly, Enrique Dunn, and Jan-Michael Frahm. Com-\nparative evaluation of binary features. In ECCV, 2012. 7\n[14] Martin Humenberger, Yohann Cabon, Nicolas Guerin, Julien\nMorat, J ´erˆome Revaud, Philippe Rerole, No ´e Pion, Cesar\nde Souza, Vincent Leroy, and Gabriela Csurka. Robust\nImage Retrieval-based Visual Localization using Kapture.\narXiv:2007.13867. 7\n[15] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi,\nand Kwang Moo Yi. COTR: Correspondence Transformer\nfor Matching Across Images, 2021. 2\n[16] Chaitanya Joshi. Transformers are Graph Neural Networks.\nhttps://thegradient.pub/transformers-are-graph-\nneural-networks/, 2020. 2\n[17] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and\nFranc ¸ois Fleuret. Transformers are RNNs: Fast autoregres-\nsive transformers with linear attention. In ICML, 2020. 3,\n4\n[18] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Re-\nformer: The efﬁcient transformer. ICLR, 2020. 3\n[19] Xinghui Li, Kai Han, Shuda Li, and Victor Prisacariu. Dual-\nresolution correspondence networks. NeurIPS, 2020. 1, 2, 6,\n7, 8\n[20] Zhaoshuo Li, Xingtong Liu, Francis X Creighton, Russell H\nTaylor, and Mathias Unberath. Revisiting Stereo Depth\nEstimation From a Sequence-to-Sequence Perspective with\nTransformers. arXiv:2011.02910. 3\n[21] Zhengqi Li and Noah Snavely. Megadepth: Learning single-\nview depth prediction from internet photos. In CVPR, 2018.\n5, 6\n[22] Tsung-Yi Lin, Piotr Doll ´ar, Ross B. Girshick, Kaiming He,\nBharath Hariharan, and Serge J. Belongie. Feature Pyramid\nNetworks for Object Detection. CVPR, 2017. 3\n[23] Ce Liu, Jenny Yuen, and Antonio Torralba. SIFT Flow:\nDense correspondence across scenes and its applications. T-\nPAMI, 2010. 2\n[24] X. Liu, Y . Zheng, B. Killeen, M. Ishii, G. D. Hager, R. H.\nTaylor, and M. Unberath. Extremely Dense Point Correspon-\ndences Using a Learned Feature Descriptor. In CVPR, 2020.\n2\n[25] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao,\nand Xiaowei Zhou. GIFT: Learning transformation-invariant\ndense visual descriptors via group cnns. NeurIPS, 2019. 2\n[26] David G Lowe. Distinctive image features from scale-\ninvariant keypoints. IJCV, 2004. 2, 6\n[27] Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao,\nShiwei Li, Tian Fang, and Long Quan. ContextDesc: Lo-\ncal Descriptor Augmentation with Cross-Modality Context.\nCVPR, 2019. 6\n[28] Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui\nZhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan.\nASLFeat: Learning local features of accurate shape and lo-\ncalization. In CVPR, 2020. 2\n[29] Iaroslav Melekhov, Gabriel J Brostow, Juho Kannala, and\nDaniyar Turmukhambetov. Image Stylization for Robust\nFeatures. arXiv:2008.06959. 7\n[30] Krystian Mikolajczyk and Cordelia Schmid. A performance\nevaluation of local descriptors. T-PAMI, 2005. 7\n[31] R ´emi Pautrat, Viktor Larsson, Martin R Oswald, and Marc\nPollefeys. Online Invariance Selection for Local Feature De-\nscriptors. In ECCV, 2020. 7\n[32] Jerome Revaud, Philippe Weinzaepfel, C ´esar De Souza, Noe\nPion, Gabriela Csurka, Yohann Cabon, and Martin Humen-\nberger. R2D2: repeatable and reliable detector and descrip-\ntor. NeurIPS, 2019. 2, 6, 7\n[33] Ignacio Rocco, Relja Arandjelovi ´c, and Josef Sivic. Efﬁcient\nneighbourhood consensus networks via submanifold sparse\nconvolutions. In ECCV, 2020. 1, 2, 6, 7\n9\n[34] Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovi ´c, Akihiko\nTorii, Tomas Pajdla, and Josef Sivic. Neighbourhood con-\nsensus networks. NeurIPS, 2018. 1, 2, 5, 7\n[35] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary\nBradski. ORB: An efﬁcient alternative to SIFT or SURF.\nIn ICCV, 2011. 2, 6\n[36] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and\nMarcin Dymczyk. From coarse to ﬁne: Robust hierarchical\nlocalization at large scale. In CVPR, 2019. 7\n[37] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\nand Andrew Rabinovich. SuperGlue: Learning feature\nmatching with graph neural networks. In CVPR, 2020. 1,\n2, 4, 5, 6, 7, 8\n[38] Torsten Sattler, Tobias Weyand, Bastian Leibe, and Leif\nKobbelt. Image Retrieval for Image-Based Localization Re-\nvisited. In BMVC, 2012. 7\n[39] Tanner Schmidt, Richard Newcombe, and Dieter Fox. Self-\nsupervised visual descriptor learning for dense correspon-\ndence. RAL, 2016. 2\n[40] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-Motion revisited. In CVPR, 2016. 6\n[41] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea\nCimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Ak-\nihiko Torii. InLoc: Indoor visual localization with dense\nmatching and view synthesis. In CVPR, 2018. 7\n[42] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.\nEfﬁcient transformers: A survey. arXiv:2009.06732. 3\n[43] Carl Toft, Will Maddern, Akihiko Torii, Lars Hammarstrand,\nErik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc\nPollefeys, Josef Sivic, Tomas Pajdla, et al. Long-Term Vi-\nsual Localization Revisited. T-PAMI, 2020. 7\n[44] Prune Truong, Martin Danelljan, L. Gool, and R. Timo-\nfte. Learning Accurate Dense Correspondences and When\nto Trust Them. ArXiv, abs/2101.01710, 2021. 2\n[45] Prune Truong, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. GOCor: Bringing Globally Optimized Correspon-\ndence V olumes into Your Neural Network. InNeurIPS, 2020.\n2\n[46] Prune Truong, Martin Danelljan, and Radu Timofte. GLU-\nNet: Global-Local Universal Network for dense ﬂow and\ncorrespondences. In CVPR, 2020. 2\n[47] Michał Tyszkiewicz, Pascal Fua, and Eduard Trulls. DISK:\nLearning local features with policy gradient. NeurIPS, 2020.\n2, 5, 6\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NeurIPS, 2017. 2, 3\n[49] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-Deeplab: Stand-\nalone axial-attention for panoptic segmentation. In ECCV,\n2020. 3\n[50] Qianqian Wang, Xiaowei Zhou, Bharath Hariharan, and\nNoah Snavely. Learning feature descriptors using camera\npose supervision. In ECCV, 2020. 5\n[51] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal\nFua. LIFT: Learned invariant feature transform. In ECCV,\n2016. 2\n[52] Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit,\nMathieu Salzmann, and Pascal Fua. Learning to ﬁnd good\ncorrespondences. In CVPR, 2018. 6\n[53] Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou,\nTianwei Shen, Yurong Chen, Long Quan, and Hongen Liao.\nLearning Two-View Correspondences and Geometry Using\nOrder-Aware Network. ICCV, 2019. 6\n[54] Zichao Zhang, Torsten Sattler, and Davide Scaramuzza. Ref-\nerence Pose Generation for Long-term Visual Localization\nvia Learned Features and View Synthesis. IJCV, 2020. 7\n10",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7530919313430786
    },
    {
      "name": "Computer science",
      "score": 0.6504111289978027
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.591161847114563
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.5381655693054199
    },
    {
      "name": "Feature matching",
      "score": 0.532464325428009
    },
    {
      "name": "Transformer",
      "score": 0.53196120262146
    },
    {
      "name": "Computer vision",
      "score": 0.521234393119812
    },
    {
      "name": "Feature extraction",
      "score": 0.5122732520103455
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5067327618598938
    },
    {
      "name": "Pixel",
      "score": 0.5000228881835938
    },
    {
      "name": "Matching (statistics)",
      "score": 0.4962807297706604
    },
    {
      "name": "Detector",
      "score": 0.4740457534790039
    },
    {
      "name": "Mathematics",
      "score": 0.24540314078330994
    },
    {
      "name": "Engineering",
      "score": 0.10019570589065552
    },
    {
      "name": "Voltage",
      "score": 0.07291126251220703
    },
    {
      "name": "Machine learning",
      "score": 0.07182502746582031
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "topic": "Artificial intelligence",
  "institutions": [],
  "cited_by": 75
}