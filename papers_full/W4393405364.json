{
  "title": "Multimodality Self-distillation for Fast Inference of Vision and Language Pretrained Models",
  "url": "https://openalex.org/W4393405364",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2132698986",
      "name": "Jun Kong",
      "affiliations": [
        "Yunnan University"
      ]
    },
    {
      "id": "https://openalex.org/A2105468542",
      "name": "Jin Wang",
      "affiliations": [
        "Yunnan University"
      ]
    },
    {
      "id": "https://openalex.org/A4373347396",
      "name": "Liang-Chih Yu",
      "affiliations": [
        "Yuan Ze University"
      ]
    },
    {
      "id": "https://openalex.org/A2115527613",
      "name": "XueJie Zhang",
      "affiliations": [
        "Yunnan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1933349210",
    "https://openalex.org/W6758704467",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W6766904570",
    "https://openalex.org/W6767211374",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W6760732026",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W6762945437",
    "https://openalex.org/W3035451444",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W6768080748",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W3034292689",
    "https://openalex.org/W3104738015",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W6779313456",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3035030897",
    "https://openalex.org/W3035333188",
    "https://openalex.org/W6605323724",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W3177224328",
    "https://openalex.org/W6789753369",
    "https://openalex.org/W3173909648",
    "https://openalex.org/W3169884222",
    "https://openalex.org/W3034578524",
    "https://openalex.org/W4313591672",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W3171353004",
    "https://openalex.org/W6775188310",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W6779590353",
    "https://openalex.org/W6779473860",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W3106070274",
    "https://openalex.org/W6638632666",
    "https://openalex.org/W6726275242",
    "https://openalex.org/W6797854001",
    "https://openalex.org/W3197726142",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6767279747",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2912371042",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W3035652667",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3014611590"
  ],
  "abstract": "The computational cost of the vision and language pretrained models (VL-PTMs) limits their deployment in resource-constrained devices that require low latency. One existing solution is to apply the early exiting (EE) strategy to accelerate the inference. This technique can force model prediction using only a few former transformer layers. However, these former layers behave differently with the final classifier, inevitably resulting in performance decline. To counter such limitation, self-distillation has been commonly introduced to enhance the representation abilities of the EE classifiers. This results in a semantic gap since EE classifiers are directly trained to mimic the outputs of the final classifier without access to the modality-specific behaviors. This study proposes a multimodality self-distillation method for the fast inference of VL-PTMs. To fill the semantic gap between modalities, we split the multimodalities into separate modalities and added them as extra inputs to encourage the effective distillation of each modality. Furthermore, the mean squared error (MSE) is introduced to minimize the distance of feature maps and further enhance the representation ability of the EE classifiers. Experiments show that the proposed method outperforms the previous EE strategies with the same inference time, and performs competitively even if the model exited very early.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nMultimodality Self-distillation for Fast Inference of\nVision and Language Pretrained Models\nJun Kong, Jin Wang, Liang-Chih Yu, and Xuejie Zhang\nAbstract—The computational cost of the vision and lan-\nguage pretrained models (VL-PTMs) limits their deployment\nin resource-constrained devices that require low latency. One\nexisting solution is to apply the early exiting (EE) strategy\nto accelerate the inference. This technique can force model\nprediction using only a few former transformer layers. However,\nthese former layers behave differently with the ﬁnal classiﬁer,\ninevitably resulting in performance decline. To counter such\nlimitation, self-distillation has been commonly introduced to\nenhance the representation abilities of the EE classiﬁers. This\nresults in a semantic gap since EE classiﬁers are directly trained\nto mimic the outputs of the ﬁnal classiﬁer without access to the\nmodality-speciﬁc behaviors. This study proposes a multimodality\nself-distillation method for the fast inference of VL-PTMs. To ﬁll\nthe semantic gap between modalities, we split the multimodalities\ninto separate modalities and added them as extra inputs to\nencourage the effective distillation of each modality. Furthermore,\nthe mean squared error (MSE) is introduced to minimize the\ndistance of feature maps and further enhance the representation\nability of the EE classiﬁers. Experiments show that the proposed\nmethod outperforms the previous EE strategies with the same\ninference time, and performs competitively even if the model\nexited very early.\nIndex Terms—Vision and language pretrained models, mul-\ntimodality self-distillation, accelerating inference, early exiting.\nI. I NTRODUCTION\nT\nRANSFORMER architecture applications are increas-\ningly being used for various multimodal tasks, including\nvisual question answering (VQA) [1], visual entailment (VE)\n[2] and natural language for visual reasoning (NLVR2) [3].\nThis success is attributed to the shared underlying textual and\nvisual properties associated with texts with visual concepts.\nVision and language pretrained models (VL-PTMs), such as\nViLBERT [4], VL-BERT [5], Unicoder-VL [6] and UNITER\n[7], can be ﬁne-tuned to improve the performance of down-\nstream multimodal tasks. However, the resulting exponential\ngrowth of the parameters may severely limit the deployment\nand ﬂexibility of these models for real-time applications on\nThis work was supported by the National Natural Science Foundation of\nChina (NSFC) under Grants No. 61966038 and 62266051 and partly by\nthe Ministry of Science and Technology, Taiwan, ROC, under Grant No.\nMOST110-2628-E-155-002. The authors would like to thank the anonymous\nreviewers for their constructive comments.(Corresponding authors: Jin Wang;\nLiang-Chih Yu)\nJ. Kong, J. Wang and X. Zhang are with the School of Information\nScience and Engineering, Yunnan University, Kunming 650000, China (e-\nmail: kongjun@mail.ynu.edu.cn; wangjin@ynu.edu.cn; xjzhang@ynu.edu.cn).\nL. C. Yu is with the Department of Information Management, Yuan Ze\nUniversity, Taoyuan 32003, Taiwan (e-mail: lcyu@saturn.yzu.edu.tw).\nThe code for this paper is available at: https://github.com/JunKong5/\nUNITER-MSD.\nresource-constrained platforms, such as drones, self-driving\ncars, and wearable devices.\nRecent studies have suggested compressing the parameters\nin pretrained models (PTMs) [8]–[10] to reduce the compu-\ntational cost and accelerate the inference. Existing methods\ninclude knowledge distillation (KD) [11]–[13], pruning [14],\n[15], and quantization [16]. Knowledge distillation (KD) [17],\n[18] refers to the use of the predictive distributions of a\npowerful teacher model as soft targets to guide the training of a\nsmaller student model, such that the student model becomes an\nequally effective model with a tolerable performance sacriﬁce.\nSimilarly, pruning [19] removes unnecessary parts of PTMs\nafter training, whereas quantization [20] truncates ﬂoating\npoint numbers such that only a few bits are used, thus\naccelerating the computation. These techniques permanently\ndiscard parts of PTMs, leading to an inevitable decline in\nperformance. Moreover, once the models are redesigned and\nretrained, their parameters and computations are ﬁxed, making\nit impossible to migrate to other platforms.\nAn alternative approach to accelerate model inference for\nPTMs is the early exiting (EE) strategy [21], [22], used in\napplications such as DeeBERT [23] and PABEE [24]. Specif-\nically, extra classiﬁers (that is, off-ramps for EE) are inserted\nbetween each two transformer layers of the PTMs. After\nan input goes through a transformer layer, the EE classiﬁer\ndetermines whether the prediction is sufﬁciently robust to\nachieve adequate performance as the ﬁnal classiﬁer. Once\nthe off-ramp is sufﬁciently conﬁdent, the result is returned;\notherwise, the sample is passed to the next layer, and the\ncalculation is repeated.\nAlthough EE classiﬁers in the former layers are already suf-\nﬁciently conﬁdent, previous studies have shown that different\nlevels of features can be learned in different transformer layers\n[25]. For example, surface features are typically learned in\nformer layers, syntactic features in middle layers, and semantic\nfeatures in deeper layers. This leads to the contradiction where\nthe earlier the model exits, the fewer semantic features can\nbe learned. In addition, EE classiﬁers that share the same\nparameters are not applicable for input features with different\nsemantic levels. Since the EE classiﬁers tend to capture fewer\nfeatures, and primarily those at the surface level, they may\nbehave differently from the ﬁnal classiﬁer that can learn a\ngreater number of and more semantic features. As shown\nin Fig. 1(a), the logits of the EE classiﬁers in the former\nlayers are quite different from those of the ﬁnal classiﬁer.\nIn this circumstance, applying the EE strategy may lead to\nperformance decline.\nTo ensure that the former layers can approximate the higher\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nImage + Text\nTransformer 1\nFinal Classifier\n(Teacher)\nTransformer 2\nTransformer 3\nEE Classifier\n(Student1)\nEE Classifier\n(Student2)\nTransformer K\nImage + Text\ntwo dogs sit on the couch\nImage\n Text\ntwo dogs sit on the couch\n...\nTrue Label：entailment\nFalse Lalbel: contradiction\nForward Computation\nImage + Text\nText ImageImage + Text\n...\n100\n0\n50\nneu.ent. con.\n100\n0\n50\nneu.ent. con.\n100\n0\n50\nneu.ent. con.\n100\n0\n50\n100\n0\n50\nneu.ent. con.\nneu.ent. con.\nImage + Text\n100\n0\n50\nneu.ent. con.\nImage + Text\nImage + Text\n100\n0\n50\n100\n0\n50\nneu.ent. con.\nneu.ent. con.\n...\n...\nImage + Text\n100\n0\n50\nneu.ent. con.\nSelf-distillation\nDifferent Layer\nDifferent Modality\n(c)\n(a) (b)\nFig. 1. Conceptual diagram of early exiting strategy for vision and language pretrained model.\nlayers in terms of representation ability, FastBERT [26] adopts\na self-distillation strategy to improve the stability of the EE\nclassiﬁer, where the ﬁnal classiﬁer was applied as a teacher to\ntransfer the representation abilities to the EE classiﬁers as stu-\ndents. As shown in Fig. 1(b), once the self-distillation strategy\nis applied, the logits of the EE classiﬁers are improved, and\nare similar to those of the ﬁnal classiﬁer. However, multimodal\ninputs contain different modalities, such as text and images,\npresenting different types of knowledge. The amount of infor-\nmation in each modality is likely to differ [27]. As a result,\ndifferent modalities exert different degrees of inﬂuence on\ndifferent tasks. For example, the language modality dominates\non VQA, while the visual modality dominates on SNLI-VE.\nWhile self-distillation can be applied to multimodal tasks,\nEE classiﬁers are trained with the joint modality without\nconsidering individual modalities, which can also provide\nuseful knowledge for prediction. As the example shown in\nFig. 1(c), the individual modality (Text) provides more prac-\ntical knowledge than the joint modality (Image+Text). In this\nexample, the EE classiﬁers trained with self-distillation can\nonly mimic the outputs of the ﬁnal classiﬁer with the joint\nmodality, thus producing incorrect predictions.\nTo address this issue, this study proposes a multimodality\nself-distillation (MSD) method for fast inference of VL-PTMs.\nThe proposed method improves existing EE strategies by split-\nting the multimodalities into separate modalities and adding\nthem as extra inputs to encourage effective distillation from\neach modality. Additionally, instead of exiting from the former\nlayers, which lack the necessary semantic information, the pro-\nposed method transfers knowledge from the last transformer\nlayer to guide the training of the former layers and ensure\nthey remain consistent with the behavior of the ﬁnal classiﬁer.\nEven if the VL-PTMs exit very early, they can still achieve\ncompetitive performance with the original VL-PTMs model.\nInspired by Sun et al. [18], the mean squared error (MSE) is\nused to minimize the distance of the feature maps between the\nteacher and student model to guide the training, and further\nenhance the representation ability of the EE classiﬁers.\nComparative experiments were conducted on various vision\nand language multimodal tasks. The results showed that the\nproposed method signiﬁcantly outperformed the previous EE\nand KD methods. Multimodality distillation is superior to\nconventional distillation because the former layers mimic the\nﬁnal classiﬁer on each modality for better knowledge transfer.\nEven if the PTMs exit very early, performance and inference\ntime are better balanced with little performance loss.\nThe remainder of this paper is structured as follows. Section\nII reviews related work on compression methods for VL-\nPTMs. Section III describes the proposed multimodality self-\ndistillation method. Section IV presents extensive experiments\nfor comparison with several existing methods. Finally, conclu-\nsions are presented in Section V .\nII. R ELATED WORK\nA. Vision and Language Pretrained Models\nIn relevant ﬁelds of both natural language processing tasks\n[28], [29] and computer vision tasks [30], [31], transformer-\nbased models [32]–[35] have achieved remarkable success\nin multimodality tasks, such as the embedded AI vision-\nlanguage navigation task. Since only minimal information\ncan be obtained from language instructions, Gao et al. [36]\nproposed a cross-modality knowledge reasoning (CKR) model\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nTABLE I\nCHARACTERISTICS OR TECHNIQUES OF PREVIOUS METHODS AND THE PROPOSED MSD METHOD FOR ADAPTIVE INFERENCE .\nEarly Exiting Classiﬁer Two-stage Training Joint Training Self-distillation Multimodality Self-distillation\nDeeBERT V V\nRightTool V V\nFastBERT V V V\nPABEE V V\nProposed MSD V V V V\nutilizing common-sense knowledge to address the remote\nembodied visual referring expression in real indoor environ-\nments (REVERIE) task [37]. Furthermore, Qiao et al. [38]\nproposed a history-enhanced and order-aware pretraining with\nthe complementing ﬁne-tuning paradigm (HOP+) for vision-\nand-language navigation. The regions of interest (ROI) are\nextracted from an image modality as image features using\nthe Fast R-CNN algorithm [39]. At the same time, texts are\nmapped as a token representation, followed by applying a dual-\nor single-encoder architecture.\nDual-encoder Architecture. The dual-encoder architectures\nassign a separate transformer encoder to each modality and\nlearn the output embeddings separately. Then, several projec-\ntion layers are added to both the vision and text encoders\nto project the output embeddings to a shared latent space.\nBased on this, LXMERT [40] and ViLBERT [4] introduce\nextra pretraining tasks to enhance the performance of these\nencoders, including masked multimodality modeling and mul-\ntimodality alignment prediction. 12-in-1 [41] is a vision-\nlanguage multitask learning model based on ViLBERT as\nthe backbone. VL-BERT [5] is pretrained on visual-linguistic\nand raw text datasets. Considering the confounding effect,\nCATT [42] uses causal attention to eliminate confounding\neffects in existing attention-based visual language methods.\nThe results demonstrate that joint pretraining can improve\nthe generalization of complex sentences and enhance the\nperformance of visual representations.\nSingle-encoder Architecture. Different from the dual-encoder\narchitecture, the single-encoder architecture feeds text and\nimages into a joint transformer encoder. The main challenge of\nthese models lies in the alignment of latent visual and textual\nspaces. To accomplish this goal, UNITER [7] introduces four\nextra pretraining tasks, including masked language model-\ning, image-text matching, word-region alignment, and masked\nregion modeling. The results indicate that joint image-text\npretraining is more effective than separate pretraining on either\nvision or language. Similarly, Pixel-BERT [43] applies the\npixel-level image feature to complement the language infor-\nmation, bridging the gap between ROI features and language\nunderstanding. Oscar [44] aligns image and text modalities\nin the same shared semantic space using the detected object\nlabels as anchor points. InterBERT [45] proposes a broader\nrange of masking operations and modality feature fusion,\nand retains modality independence. VIILA [46] improves\ngeneralization capabilities using adversarial and adversarial\nﬁne-tuning.\nB. Model Compression\nModel compression seeks to minimize model size while\nretaining model performance, thereby reducing the neural\nnetwork footprint, increasing its inference speed, and reducing\nenergy consumption. Existing model compression methods\ninclude pruning, quantization, knowledge distillation and con-\nditional computation.\nPruning applies a binary criterion to identify weights for\npruning, with weights that match the pruning criteria assigned\na value of zero [47]–[49]. Pruned elements are trimmed from\nthe model, i.e., their values are zeroed and are excluded\nfrom back-propagation. Based on this, several studies [50],\n[51] have investigated the importance of model parameters\nand neurons during the training process. The less important\nparameters and neurons are then zeroed, which may negatively\nimpact network accuracy.\nModel Quantization refers to reducing the number of bits\nrepresenting a number. It converts high numerical precision\nintegers, e.g., usually 32-bit ﬂoat, into low-precision integers,\ne.g., 8-bit integers, thus effectively reducing computational\ncost and parameter size to accelerate model inference. For\nthe transformer, Q-BERT [16] implements a hybrid precision\nquantization for the BERT model. For visual transformer,\nLiu et al. [52] proposed quantizating similarity perception\nand ranking-aware quantization for feed-forward networks\nand multi-head self-attention in encoders. In addition, Gao\net al. [53] proposed simultaneously quantizing the activation\nfunction and weight parameter to reduce quantization errors.\nKnowledge Distillation is a model compression method in\nwhich a trained larger model is used as a teacher model to\nsupervise a smaller untrained model as the student. The knowl-\nedge contained in the teacher model can then be transferred\nto the student model through a special distillation operation.\nHinton et al. [11] used the category probability distributions\nof the ﬁnal classiﬁcation layer as soft labels and minimized\nthe KL-divergence between the teacher and student models for\ninformation transfer. Unlike using the ﬁnal classiﬁcation layer\nfor information transfer, BERT-PKD [18] further learns from\nthe intermediate layer. TinyBERT [54] performs knowledge\ndistillation during pretraining and task-speciﬁc ﬁne-tuning\nphases and uses data augmentation to improve the student\nmodel accuracy.\nConditional Computation refers to a class of algorithms in\nwhich each input sample uses a different part of the model,\nthereby reducing the average computational resource require-\nments, latency, or power consumption. The most widely used\nmethod is adaptive inference, which usually inserts additional\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\nearly exiting classiﬁers between each of the two transformer\nlayers of the PTMs. After the input samples pass through\nthe encoder layers, the early exiting classiﬁer determines\nwhether prediction conﬁdence is sufﬁciently strong. The result\nis returned once the early exiting classiﬁer is sufﬁciently\nconﬁdent; otherwise, the samples are passed to the next\nlayer and the computation is repeated. DeeBERT [23] inserts\nadditional classiﬁers for each pair of transformer layers and\nadopts a two-stage training approach. First, the backbone of\nBERT is ﬁne-tuned for downstream tasks. Then, it is frozen\nto ﬁne-tune the early exiting classiﬁers. Such a two-stage\nstrategy brings extra costs for computation and training time.\nRightTool [21] applies the early exiting approach to BERT on\nthe document ranking task. To ensure that the former layers\ncan obtain powerful representation ability, FastBERT [26]\nadopted the self-distillation strategy to improve the stability of\nthe EE classiﬁer. PABEE [24] proposes a patience-based exit\nstrategy considering the consistency of early exiting classiﬁer\npredictions. The model will stop inference and exit if the exit\nprediction remains constant for a preset time. The performance\nof former layers is reduced due to the lack of high-level\nsemantic information in former layers.\nC. Discussion\nTable I summarizes the characteristics or techniques of\nexisting approaches to adaptive inference. The EE strategy\nwith adaptive inference dynamically activates only some of\nthe parts in a model according to the properties of the\ninput samples. The immediate effect of activating fewer units\naccelerates information propagation through the network in\ntraining and testing.\nBased on this, DeeBERT [23] inserts additional EE classi-\nﬁers for each pair of transformer layers and adopts a two-stage\ntraining approach. RightTool [21] also applies the same EE\nstructure in document ranking and sets different thresholds to\nalleviate class distribution imbalance. Unfortunately, the infor-\nmation learned by the former EE classiﬁers differs from that of\nthe ﬁnal classiﬁer, often resulting in divergent predictions. As\na result, the earlier the model exits, the fewer semantic features\nrequired for the task are learned, thus degrading performance.\nTo address this shortcoming, PABEE [24] introduced a\npatience strategy to preserve the consistency of EE classi-\nﬁer predictions. Meanwhile, FastBERT [26] applied a self-\ndistillation approach to transfer knowledge from the ﬁnal\nclassiﬁer to the EE classiﬁers, improving the representation\nability of the former layers. By directly introducing these\nmethods into multimodal tasks, the EE classiﬁers are trained\nto mimic the outputs of the ﬁnal classiﬁer without access\nto the modality-speciﬁc behaviors. As a result, the semantic\ngap occurred between the modality-speciﬁc behaviors of the\nteacher and the student, and self-distillation was inefﬁcient\nsince the EE classiﬁers did not carefully mimic the modality-\nspeciﬁc prediction. Furthermore, the two-stage training strat-\negy of these methods increases computational and training\ntime costs.\nThis paper proposes splitting the multimodalities into sepa-\nrate modalities to ﬁll the semantic gap between modalities. It\nadds them as extra inputs to promote the effective distillation\nof each modality and obtain modality-speciﬁc information.\nFurthermore, MSE is used to minimize the distance of feature\nmaps between the teacher and student model to guide the\ntraining and further enhance the representation ability of the\nEE classiﬁers.\nIII. M ULTIMODALITY SELF -DISTILLATION\nFig. 2 shows an overview of the proposed multimodality\nself-distillation, unifying knowledge distillation and EE with\ndynamic inference to accelerate the VL-PTMs. The last trans-\nformer layer is used as the teacher to guide the training of the\nformer layers, such that these layers can mimic the teacher’s\nbehavior. The texts and images are used as separate inputs\nfor self-distillation to learn the individual features of language\nand vision, respectively. The conﬁdence in each EE classiﬁer is\nmeasured to determine whether the model should be returned\nto this layer. The details of each module are presented as\nfollows.\nA. Early Exiting Classiﬁer\nTypically, a VL-PTM contains K layers of transformers\n[55]. The multimodal input contains both image regions and\ntext words, which are then encoded as a representation se-\nquence of both image and text, that is, V = {v1,v2,...,v o}and\nW = {w1,w2,...,w q}, where o and q respectively denote the\nnumber of visual regions and textual tokens. The correspond-\ning ground-truth label is y. A special token [CLS] is added to\nthe head of the sequence such that the corresponding hidden\nstate h(k)\n[CLS] ∈Rdh in each layer is a joint representation of\nboth images and texts. For the k-th layer, the encoding process\nof the transformer is deﬁned as follows:\n[h(k)\n[CLS],h(k)\nv1 ,...,h (k)\nvo ,h(k)\nw1 ,...,h (k)\nwq ]\n= f(k)([h(k−1)\n[CLS] ,h(k−1)\nv1 ,...,h (k−1)\nvo ,\nh(k−1)\nw1 ,...,h (k−1)\nwq ])\n(1)\nwhere f(k) denotes the k-th layer of the transformer encoder.\nThe standard approach of EE is to add EE classiﬁers in\nthe intermediate layers, similar to the ﬁnal classiﬁer in the\nlast layer. Each EE classiﬁer is a fully connected layer with\nsoftmax activation. The EE classiﬁers take as input the joint\nembeddings corresponding to the [CLS] token, that is, h(k)\n[CLS]\nin the k-th layer of the VL-PTMs, which is formulated as\nfollows:\nz(k)=W(k)\nz h(k)\n[CLS] + b(k)\nz (2)\nˆy(k)\nz = softmax(z(k)) (3)\nwhere W(k)\nz ∈ RC×dh and b(k)\nz ∈ RC respectively repre-\nsent the weights and bias of the k-th EE classiﬁer, and C\ndenotes the number of classes. The training objective of these\nclassiﬁers is categorical cross-entropy (CE) LCE , deﬁned as\nfollows:\nLCE = −\nK∑\nk=1\nI(y) ◦log(ˆy(k)\nz ) (4)\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nText\nImage\nImage + Text\nTransformer 1\nFinal Classifier\n(Teacher)\nTransformer 2\nTransformer 3\nVL-PTM\nEE Classifier\n(Student1)\nEE Classifier\n(Student2)\nEE Classifier\n(Student3)\nEE Classifier\n(StudentK-1)\nTransformer K\nImage + Text\ntwo dogs sit on the couch\nImage\n Text\ntwo dogs sit on the couch\n...\nEntropy\nEntropy\nEntropy\nEntropy\nConfidence\nConfidence\nConfidence\nConfidence\nAdaptive Inference\n...\nLow Uncertainty\nHigh Uncertainty\nForward Computation\nMultimodality Self-distillation\nFig. 2. Overall architecture of the proposed multimodality self-distillation for fast inference of VL-PTMs.\nwhere y and ˆy(k)\nz denote the corresponding ground-truth label\nand probability distribution of the k-th layer, respectively.\nI(y) denotes a one-hot label and ◦represents an element-wise\nmultiplication operation.\nB. Multimodality Self-distillation\nUsing only the ground-truth label to train EE classiﬁers\nwill result in their representation abilities diverging from the\nﬁnal classiﬁer because of their different inputs. Thus, self-\ndistillation was applied to encourage EE classiﬁers in the\nformer layers to mimic the behavior of the ﬁnal classiﬁer and\nobtain rich semantic information in the hidden representation\nh(k)\n[CLS]. As shown in Fig. 3, the classiﬁer in the K-th layer\nwas regarded as the teacher model, whereas the other EE\nclassiﬁers in the former layers were regarded as the student\nmodel. In knowledge distillation [11], student models can\nlearn from the distribution of teacher models to improve their\nclassiﬁcation performance. The feature maps z(k)\ns ∈Rdp and\nz(K)\nt ∈Rdp respectively of the EE classiﬁer (student) and the\nﬁnal classiﬁer (teacher), are denoted as follows,\nz(k)\ns = W(k)\ns h(k)\n[CLS] + b(k)\ns (5)\nz(K)\nt = W(K)\nt h(K)\n[CLS] + b(K)\nt (6)\nwhere h(k)\n[CLS] and h(K)\n[CLS] are hidden representations in the k-th\nand the ﬁnal transformer layers, and W(k)\ns ,b(k)\ns ,W(K)\nt and b(K)\ns\nare weights and biases associated with the EE classiﬁers and\nthe ﬁnal classiﬁer. Each EE classiﬁer is required to mimic\nthe behavior of the ﬁnal classiﬁer. The self-distillation loss\nfunction L(k)\nKL measures the Kullback-Leibler (KL) divergence\nbetween the k-th EE classiﬁer and the ﬁnal classiﬁer, denoted\nas\np(k)\ns = softmax\n(\nz(k)\ns\n/\nτ\n)\n(7)\np(K)\nt = softmax\n(\nz(K)\nt\n/\nτ\n)\n(8)\nL(k)\nKL = τ2KL(p(k)\ns ||p(K)\nt ) (9)\nwhere KL (•||•) denotes the KL-divergence, τ denotes the\ntemperature, which is used to control the softness of the\ndistribution, and τ2 compensates for the size of the gradient\nscaled by the soft target, ensuring that there is no negative\nimpact on the gradient size. p(k)\ns ∈ Rdp and p(K)\nt ∈ Rdp\nrespectively represent the soft probability distributions of the\nk-th EE classiﬁer (student) and the ﬁnal classiﬁer (teacher).\nNotably, p(k)\ns are the soft probability distributions using both\ntextual wi ∈Rdw and visual vi ∈Rdv modalities as input.\nConsidering that the input sample contains both image and\ntext modalities, we add each modality as an extra individual\ninput to the VL-PTMs separately, such that the students can\nseparately learn the effective information of the teacher model\ntoward a speciﬁc modality to ﬁll the semantic gap between\ndifferent modalities. By successively masking the inputs of\nvisual modality vi ∈Rdv and textual modality wi ∈Rdw , we\nrespectively obtain the output soft probability distribution of\ntextual modality p(k)\nw and visual modality p(k)\nv . Thus, two extra\nlosses of KL-divergence were introduced to each modality,\nrespectively denoted as,\nL(k)\nT−KL = τ2KL(p(k)\nw ||p(K)\nw ) (10)\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nVL-PTMs\nText\nImage\nImage + Text\nEarly Exiting Classifier (Student)\nTransformer K\nTransformer 3\nTransformer 1\nFinal Classifier (Teacher)\nKL-divergence\nImage + Text\ntwo dogs sit on the couch\nImage\n Text\ntwo dogs sit on the couch\n...\nTransformer 2\nFig. 3. Overall architecture of the proposed multimodality self-distillation for fast inference of VL-PTMs.\nL(k)\nI−KL = τ2KL(p(k)\nv ||p(K)\nv ) (11)\nwhere L(k)\nT−KL and L(k)\nI−KL respectively represent the self-\ndistillation loss functions of the textual and visual modalities.\nThe multimodality self-distillation (MS) loss LMS is deﬁned\nas follows:\nLMS = λ\nK−1∑\nk=1\nL(k)\nKL + λw\nK−1∑\nk=1\nL(k)\nT−KL + λv\nK−1∑\nk=1\nL(k)\nI−KL (12)\nwhere λs denote hyper-parameters used to balance the different\nmodality loss functions.\nUsing only the logits of the teacher for knowledge distilla-\ntion is insufﬁcient to make the students imitate the teacher’s\nbehavior entirely. Inspired by Sun et al. [18], we also minimize\nthe mean squared error (MSE) between the input features of\nthe student and the teacher for each input modality.\nL(k)\nMSE = MSE(z(k)\ns ,z(K)\nt ) (13)\nL(k)\nT−MSE = MSE(z(k)\nw ,z(K)\nw ) (14)\nL(k)\nI−MSE = MSE(z(k)\nv ,z(K)\nv ) (15)\nwhere z(k)\nw and z(k)\nv respectively denote the feature maps of\nthe EE classiﬁer (student) of the textual and visual modalities.\nCorrespondingly, the mean squared error between the multi-\nmodal inputs is deﬁned as follows:\nLED = λ\nK−1∑\nk=1\nL(k)\nMSE + λw\nK−1∑\nk=1\nL(k)\nT−MSE\n+ λv\nK−1∑\nk=1\nL(k)\nI−MSE\n(16)\nwhere LED denotes the multimodality feature self-distillation\nloss. The total training objective L of multimodality self-\ndistillation is formulated as follows,\nL= LCE +LMS + LED (17)\nInstead of using two-stage training in previous EE models,\nall components for self-distillation in the proposed method can\nbe trained using a standard back-propagation algorithm in an\nend-to-end manner.\nC. Adaptive Inference\nInspired by Teerapittayanon et al. [56], the features learned\nin the former layers are sufﬁciently robust to provide easy\nexamples to obtain a performance similar to that of the ﬁnal\nlayer, such that the EE classiﬁer can accelerate the inference.\nConversely, hard examples must be propagated through the\nclassiﬁer in the latter layers. In practice, most samples are\nrelatively easy; therefore, adaptive inference can be applied to\nforce the model to exit early to reduce the inference time and\ncomputational cost.\nTo determine whether inference can be terminated at a\nspeciﬁc layer, the output distribution of the k-th EE classiﬁer\nis determined by an entropy value E(k), calculated as follows:\nE(k) = −∑\ndp\nz(k)\ns log z(k)\ns\n= ln(∑\ndp\nexp(z(k)\ns )) −\n∑\ndp\nz(k)\ns exp(z(k)\ns )\n∑\ndp\nexp(z(k)\ns )\n(18)\nwhere dp denotes the dimensionality of the hidden representa-\ntion of the EE classiﬁer. Here, E(k) measures the uncertainty\nin the output of the EE classiﬁer, i.e., z(k)\ns . The higher the\nentropy value, the higher the EE classiﬁer’s uncertainty. If\nthe uncertainty E(k) is lower than a preset threshold F, the\nEE classiﬁer is sufﬁciently conﬁdent to achieve performance\ncompetitive with the ﬁnal classiﬁer. The model then takes the\nprediction of the EE classiﬁer as the result, and immediately\nsuspends the inference of the latter layers. Otherwise, it\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nTABLE II\nDESCRIPTIVE STATISTICS OF THE DATASETS .\nTask Dataset Image Src. #Images #Text Metric #Train #Dev #Test\nNLVR NLVR2 ILSVRC-2014 ImageNet 107k 29k Accuracy 86k 6k 14k\nVE SNLI-VE Flickr30k 31k 528k Accuracy 529k 17k 17k\nVQA VQA 2.0 MS COCO 204k 1105k Accuracy 443k 214k 447k\nTABLE III\nEXPERIMENTAL RESULTS FOR COMPARING BASELINE METHODS ON MULTIMODAL DATASETS WITH UNITER BACKBONE .\nρ Methods SNLI-VE NLVR2 VQA\nVal Time% Test Time% Dev Time% Test-P Time% Test-dev Test-std Time%\n∼100%\nUNITER 78.59 100 78.28 100 77.18 100 77.85 100 72.70 72.9 100\nLXMERT - - - - 74.90 100 74.50 100 72.42 72.54 100\nVL-BERT(Large) - - - - - - - - 71.79 72.22 100\nVisualBERT 77.57 100 77.32 100 67.40 100 67.00 100 70.80 71.00 100\n∼50%\nUNITER-KD-6L 75.14 50 75.41 50 57.05 50 57.93 50 69.20 69.33 50\nUNITER-EE 76.51 49.09 76.44 48.97 75.18 50.49 75.60 50.29 69.26 69.49 50.51\nUNITER-MSD 77. 37 48.26 76.96 48.55 75.84 49.34 75.94 50.23 69.69 69.84 49.76\n∼42%\nUNITER-KD-5L 74.07 41.67 74.29 41.67 56.30 41.67 55.92 41.67 69.08 69.27 41.67\nUNITER-EE 76.45 41.48 76.39 41.36 74.18 41.50 74.42 41.32 67.40 67.73 43.67\nUNITER-MSD 77.29 40.34 76.94 40.44 75.12 40.41 75.20 41.00 69.11 69.32 41.12\n∼33%\nUNITER-KD-4L 74.02 33.33 74.23 33.33 54.57 33.33 53.41 33.33 67.11 67.43 33.33\nUNITER-EE 76.32 32.45 76.25 32.73 72.99 34.27 72.92 33.32 65.55 65.83 33.76\nUNITER-MSD 77.21 31.98 76.78 32.03 73.59 32.78 73.69 32.66 67.26 67.51 33.27\ncontinues to execute on the next layer until it falls below the\nrequired threshold.\nThe right part in Fig. 2 shows the adaptive inference of\nVL-PTMs. Intuitively, the former classiﬁers predict the easy\nsamples, whereas the latter classiﬁers predict only the hard\nexamples. Based on this, adaptive inference can drastically\nimprove efﬁciency by reducing computation requirements on\nthe portion of easy examples in the dataset.\nIV. E XPERIMENTS\nA. Experimental Setup\n1) Dataset: We conducted experiments on different multi-\nmodal datasets to evaluate the effectiveness of the proposed\nmultimodality self-distillation model inference. Descriptive\nstatistics of the datasets are shown in Table II.\n• Visual entailment (SNLI-VE) is used to predict whether a\ngiven image semantically contains an input sentence. The\nmodel’s performance was measured using the classiﬁca-\ntion accuracy over three categories: entailment, neutral,\nand contradiction.\n• Visual question answering (VQA) refers to answering\ncontent-related questions for an image. The dataset was\ndivided into three subsets: train, val, and test. The test\nsubset was further divided into test-dev and test-std for\nonline evaluation.\n• Natural language for visual reasoning (NLVR2) deter-\nmines whether the correspondence between a pair of\nimages and natural language captions is consistent.\n2) Inference Time Measurement: The runtime of the VL-\nPTMs is highly dependent on the hardware environment and is\nthus unstable in most cases. Following Xin et al. [23] and Zhou\net al. [24], we gradually adjusted the threshold F to measure\nthe time reduction ratio ρby comparing the adaptive inference\nwith the original execution with complete layers, that is, the\nratio of the EE layers to the originally required total layers for\nall samples. For a K-layers model, the time reduction ratio ρ\nof the inference is measured by\nρ=\n∑K\nk=1 k×m(k)\nK×M (19)\nwhere m(k) denotes the number of samples exited at the k-th\nlayer, and K and M respectively denote the number of layers\nand samples.\n3) Implementation Details: Both UNITER [7] and Oscar\n[44] were used as the backbone model of VL-PTMs. They\nall included 12 layers and 768 hidden dimensions. For the\nVQA and SNLI-VE datasets, we fed image and text pairs\ninto UNITER. Than we extracted the joint embeddings corre-\nsponding to the [CLS] token with a fully connected layer as\nthe ﬁnal representation of the input image and text pairs. For\nthe NLVR2 dataset, each input sample contains a description\ntext with two images. The two outputs of the text and image\nwere integrated using a bi-attention layer for classiﬁcation.\nThe AdamW optimizer [57] was used for the training. The\nlearning rate and weight decay were 8e-5 and 0.01 for VQA,\n7e-6 and 0.01 for SNLI-VE, and 3e-5 and 0.01 for NLVR2,\nrespectively. The maximum length of the input text was 128.\nUsing the grid search strategy, the optimal settings of λ, λw\nand λv were respectively 0.5, 0.25 and 0.25 for VQA and\nSNLI-VE, and 0.7, 0.15, and 0.15 for NLVR2.\nB. Baselines\nTo comprehensively evaluate the proposed multimodal-\nity self-distillation, comparative experiments were conducted\nagainst various knowledge distillation, and early exiting meth-\nods as well as several complete VL-PTMs, including Visu-\nalBERT [58], UNITER [7], LXMERT [40], UNITER-KD,\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nTABLE IV\nEXPERIMENTAL RESULTS FOR COMPARING BASELINE METHODS ON MULTIMODAL DATASETS WITH OSCAR BACKBONE .\nρ Methods NLVR2 VQA\nDev Time% Test-P Time% Test-dev Test-std Time%\n∼100%\nOscar 78.07 100 78.36 100 73.16 73.44 100\nLXMERT 74.90 100 74.50 100 72.42 72.54 100\nVL-BERT(Large) - - - - 71.79 72.22 100\nVisualBERT 67.40 100 67.00 100 70.80 71.00 100\n∼50%\nOSCAR-FT-6L 74.51 50 74.67 50 69.95 70.64 50\nOscar-KD-6L 76.05 50 76.33 50 71.12 71.33 50\nOscar-EE 75.23 52.06 75.36 52.66 70.36 70.51 52.37\nOscar-MSD 76.53 50.36 76.64 50.23 71.58 71.82 51.23\n∼42%\nOSCAR-FT-5L 73.08 41.67 73.16 41.67 68.26 68.39 41.67\nOscar-KD-5L 73.66 41.67 73.87 41.67 69.52 69.65 41.67\nOscar-EE 73.38 42.67 73.56 42.13 68.66 68.89 42.56\nOscar-MSD 75.06 43.10 75.17 42.86 70.17 70.53 42.15\n∼33%\nOSCAR-FT-4L 72.14 33.33 72.25 33.33 67.85 67.96 33.33\nOscar-KD-4L 73.31 33.33 73.46 33.33 68.97 69.10 33.33\nOscar-EE 72.87 34.56 72.95 34.23 68.05 68.37 34.78\nOscar-MSD 73.70 32.87 73.85 33.18 69.36 69.72 34.55\nTABLE V\nRESULTS OF THE ABLATION STUDY OF THE PROPOSED UNITER-MSD MODEL .\nρ Methods NLVR2 SNLI-VE\nentropy Dev Time% Test-P Time% entropy Val Time% Test Time%\n∼60%\nUNITER-MSD w/o ED 0.03 76.07 58.21 76.43 59.76 0.04 76.70 59.46 76.63 59.56\nUNITER-MSD w/o MS 0.02 76.17 58.14 76.51 60.07 0.09 76.84 59.40 76.86 59.33\nUNITER-MSD w/o MS & ED 0.03 76.04 57.94 76.29 59.08 0.04 76.61 60.26 76.54 60.22\nUNITER-MSD w/o SD 0.04 76.06 58.05 76.40 59.02 0.06 76.68 60.18 76.60 60.15\nUNITER-MSD w/o ED-T&I 0.05 76.15 58.36 76.48 59.86 0.06 76.92 59.66 76.73 60.12\nUNITER-MSD w/o MS-T&I 0.04 76.19 57.89 76.53 60.10 0.08 76.98 60.15 76.88 59.68\nUNITER-MSD w/o MS-T&I & ED-T&I 0.05 76.10 58.28 76.42 60.05 0.05 76.86 59.86 76.62 60.35\nUNITER-MSD 0.06 76.30 57.25 76.59 58.73 0.07 77.43 58.59 76.97 58.62\n∼40%\nUNITER-MSD w/o ED 0.20 74.32 40.59 74.56 41.07 0.20 76.67 39.81 76.61 39.88\nUNITER-MSD w/o MS 0.12 74.36 40.26 74.84 41.05 0.28 76.81 40.28 76.83 40.51\nUNITER-MSD w/o MS & ED 0.20 74.18 41.50 74.42 41.32 0.17 76.45 41.48 76.39 41.36\nUNITER-MSD w/o SD 0.22 74.29 41.46 74.53 41.30 0.21 76.63 40.23 76.58 40.42\nUNITER-MSD w/o ED-T&I 0.17 74.39 40.46 74.86 40.89 0.19 76.83 41.25 76.72 41.25\nUNITER-MSD w/o MS-T&I 0.18 74.42 41.38 74.91 41.08 0.26 76.89 40.65 76.75 41.36\nUNITER-MSD w/o MS-T&I & ED-T&I 0.28 74.27 40.88 74.49 41.23 0.23 76.65 41.36 76.61 40.93\nUNITER-MSD 0.24 74.85 39.76 75.14 40.29 0.24 77.27 39.61 76.91 39.69\nTABLE VI\nNUMBER OF PARAMETERS AND TRAINING TIME FOR DIFFERENT MODELS SNLI-VE.\nModel # Param (Transformers) # Param (EE classiﬁers) Total Params Train Time (Step) ρ Acc\nUNITER12 111.1 M N/A 111.1 M 4000 (1×) 100 (1×) 78.59\nUNITER-MSD 111.1 M 25.7 M 136.7 M 9500 (2.37×) 32.55 (3.07×) 77.32\nUNITER-EE. The UNITER-KD denotes the original UNITER\nmodel, which was compressed using conventional knowledge\ndistillation. The UNITER-EE denotes that an EE method was\nintroduced into the UNITER model. An EE classiﬁer was\nadded to each intermediate layer of UNITER.\nC. Hyperparameters Fine-tuning\nSeveral hyperparameters may affect the performance of the\nproposed MSD method in downstream tasks. Fig. 4 shows\nthe optimal settings for selecting different hyperparameters\ndepending on the ﬁnal performance of the development set. We\nﬁne-tuned each parameter to obtain the optimal value, which\nis then ﬁxed so that the other parameters can be ﬁne-tuned in\nturn.\nFor the temperature τ in Eqs. (5) and (6), we try to select\nthe superior values in the set of temperature candidates in\n1,2,3,4,5,6,7,8,9,10, as shown in Fig. 4(b). VQA, SNLI-VE,\nand NLVR2 perform best at τ of 3, 2, 5, respectively.\nThe balance coefﬁcients λ, λw, and λv in Eq. (10) are used\nto balance the information of different modalities. We also use\ndifferent combinations for the grid search while ensuring that\nthe different balance coefﬁcients sum to 1. As shown in Fig.\n4(a), the proposed method performs best on the VQA dataset\nwhen λis set to 0.5 and both λw, and λv are set to 0.25. The\nproposed method performs best on the NLVR2 and SNLI-\nVE datasets when λ is set to 0.7 and both λw, and λv are\nset to 0.15. This illustrates that the information of different\nmodalities has different effects on the model in different\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\n(a) Modality loss weights ( λ, λw, and λv)\n (b) Temperature τ\nFig. 4. Hyperparameters ﬁne-tuning on different datasets.\nFig. 5. The KL divergence between the ﬁnal layer output and the outputs\nof the other former layers before and after removing individual modalities.\ntasks. Once these parameters are exceeded, the performance\nof the optimal settings will degrade. The results indicate that\nappropriate parameters can improve the performance of early\nexiting for accelerating inference.\nD. Comparative Results\nTo fairly compare the proposed method with the baseline,\nwe tuned the time reduction ratio ρ of the proposed method\nto be consistent with that of the corresponding baselines.\nDifferent expected ρ values were obtained by adjusting the\nconﬁdence threshold F. For original the VL-PTMs (e.g.,\nUNITER, LXMERT, VL-BERT (Large), and Visual-BERT), ρ\nis 100%. Tables III and IV compare the accuracy under a cer-\ntain time reduction ratio of the proposed UNITER-MSD and\nOscar-MSD against the baselines. Experiments with different\nVL-PTMs as the backbone demonstrate the effectiveness and\ngeneralizability of the method. Both UNITER and Oscar were\nused as the backbone for these baselines.\nCompared to the original complete VL-PTMs, the proposed\nUNITER-MSD and Oscar-MSD signiﬁcantly improved efﬁ-\nciency and reduced inference times with little performance\nloss. Compared to the original complete UNITER, the per-\nformance of the UNITER-MSD decreased by 1.22% and\n1.34% respectively in SNLI-VE and NLVR2, but the efﬁciency\nimproved by 50% ( ρ∼50%) in both SNLI-VE and NLVR2.\nFurthermore, UNITER-MSD achieved extremely competitive\nresults but faster inference than the original VisualBERT\nin NLVR2. Oscar-MSD outperformed LXMERT and Visual-\nBERT on the time reduction ratios ( ρ∼50%).\nWith different time reduction ratios ρ, the proposed\nUNITER-MSD and Oscar-MSD outperformed the KD and\nEE methods. For ρ ∼ 50%, UNITER-MSD outperformed\nUNITER-KD by 2.23% and UNITER-EE by 0.86% on the\nSNLI-VE dataset. Similar improvements were observed in the\nother datasets. This was because multimodality self-distillation\nallowed the performance of EE classiﬁers to approximate\nthat of the ﬁnal classiﬁer closely and to learn high-level\ninformation. If time reduction ratio requirement ρ changes,\na new student model must be trained from scratch using KD.\nThus, it dose not apply to different platforms with different\nrequirements of ρ. Conversely, the proposed UNITER-MSD\ncan perform adaptive inference by adjusting the threshold F\nto provide a different ρ with robust performance.\nE. Ablation Experiments\nTable V presents the results of the ablation experiments\nto evaluate each component’s effectiveness in the proposed\nUNITER-MSD. We successively removed one or more loss\nfunctions, that is, LMS in Eq. (12) and LED in Eq. (16).\nUNITER-MSD w/o MS indicates the removal of LMS .\nUNITER-MSD w/o ED indicates the removal of LED.\nUNITER-MSD w/o MS & ED indicates the removal of both\nLMS and LED, that is, UNITER-EE. UNITER-MSD w/o SD\ndenotes with EE strategy and separate modalities as inputs, and\nwithout KD. As indicated, the removal of each component of\nthe proposed method will degrade the performance, indicating\nthat the LMS and LED loss functions are indispensable to\nperformance improvement. Namely, the main advantage of\nUNITER-MSD comes from the proposed multimodality self-\ndistillation and self-distillation with MSE.\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\n(a) SNLI-VE-Val\n (b) SNLI-VE-Test\n (c) NLVR2-Dev\n (d) NLVR2-Test-P\nFig. 6. The performance-time trade-off curve on SNLI-VE and NLVR2 datasets of UNITER-MSD and UNITER-EE.\nSpeciﬁcally, UNITER-MSD without MS and ED performed\n0.82% lower than UNITER-MSD on the SNLI-VE dataset,\nand 0.26% lower on NLVR2 for ρ ∼60%. Without the MS\nand ED loss function, the former layers only learned low-\nlevel features that are not competent for the ﬁnal classiﬁcation.\nFurthermore, UNITER-MSD w/o SD performs better than\nUNITER-MSD w/o MS & ED, illustrating the effectiveness\nof different modalities as additional inputs for multimodal\nlearning and for ﬁlling the semantic gap between modalities.\nA similar observation can be obtained that the accuracies of\nUNITER-MSD without MS and without ED were lower than\nthat of UNITER-MSD by 0.53% and 0.49%, respectively, for\nNLVR2, and 0.44% and 0.37% for SNLI-VE for ρ ∼40%.\nThere was no MS loss function or reduction in model per-\nformance. This indicates that multimodality self-distillation\nencourages the former layers to learn useful information about\nspeciﬁc modalities. Furthermore, UNITER-MSD without MS\nsimultaneously distills texts and images. Unfortunately, if\none modality is dominant, the distillation of the other will\nfail, leading to a performance decline. Without the ED loss\nfunction, the EE classiﬁers are too shallow to learn enough\ninformation for classiﬁcation.\nTo further examine the effect of adding individual modal-\nities in multimodality self-distillation, we removed the loss\nfunctions of text (T) and image (I) modalities in LMS and\nLED. UNITER-MSD w/o MS-T&I indicates the removal of\nLT−KL and LI−KL in LMS , UNITER-MSD w/o ED-T&I\nindicates the removal of LT−MSE and LI−MSE in LED, and\nUNITER-MSD w/o MS-T&I & ED-T&I indicates the removal\nof the LT−KL and LI−KL terms in LMS and LT−MSE\nand LI−MSE in LED. Note that removing the loss functions\nof individual modalities (T&I) still retains the loss function\nof the joint modality LKL in LMS and LMSE in LED.\nThat is, comparing UNITER-MSD and w/o MS-T&I (or w/o\nED-T&I) can show the effect of individual modalities, and\ncomparing w/o MS-T&I (or w/o ED-T&I) and w/o MS (or\nw/o ED) can show the effect of the joint modality. The results\nshow that the performance degradation from UNITER-MSD\nto w/o MS-T&I is greater than that from w/o MS-T&I to w/o\nMS. Comparing UNITER-MSD, w/o ED-T&I and w/o ED\nalso shows similar results. These ﬁndings indicate that adding\nindividual modalities in self-distillation contributes more than\nusing the joint modality. To further verify the effectiveness of\nindividual modalities, we measured the KL divergence of the\nﬁnal layer output and the other former layers’ outputs to see\nif their outputs are similar, as shown in Fig. 5. Compared with\nUNITER-MSD, the KL divergences increased after removing\nindividual modalities (T&I), indicating that the outputs of the\nﬁnal layer and the other former layers diverge without the help\nof individual modalities. This also shows that using individual\nmodalities as an additional knowledge source improved self-\ndistillation effectiveness.\nF . Performance-Time Tradeoff\nTo explore whether the proposed UNITER-MSD and\nUNITER-EE performance varies with ρ, Fig. 6 presents the\ninference time-performance trade-off curves for SNLI-VE and\nNLVR2.\nFor the same ρ, the proposed method obtained a better\naccuracy than UNITER-EE. The performance of UNITER-EE\ndecreased sharply as ρ gradually decreased. As shown in Fig.\n6, the proposed method has a high-performance improvement\nfor the low-level early exiting classiﬁer, demonstrating that\nrelatively high performance is guaranteed even when the\ninference time is reduced. It also shows that the performance of\nexisting EE methods decreases signiﬁcantly with the inference\ntime. This limits the usefulness of the existing EE methods in\nmeeting higher inference requirements. Conversely, the pro-\nposed method can guarantee good performance while reducing\nthe inference time, making it more robust and efﬁcient than\nthe existing EE methods.\nG. Analysis of Model Efﬁciency\nTo better analyze the model’s efﬁciency, Table VI shows\nthe model’s training time, size, and reduced inference time.\nAs indicated, the proposed EE strategy with multimodal-\nity self-distillation aims to provide more efﬁcient inference\nwith limited performance loss. Due to the introduction of\nmultimodality self-distillation in the training of the proposed\nEE strategy, the overall training time has been increased by\n2.37×. However, the inference speed has also been acceler-\nated by 3.07×. Consistent with KD and other EE strategies,\nthe proposed method trades off increased training cost with\naccelerated inference time. The difference is that the proposed\nmethod can dynamically adjust the acceleration ratio according\nto different running platforms, while KD needs to retrain the\nmodel for the new platform.\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\n(a) SNLI-VE-Val\n (b) SNLI-VE-Test\n (c) NLVR2-Dev\n (d) NLVR2-Test-P\nFig. 7. The performance of early exiting classiﬁers of UNITER-EE and UNITER-MSD on SNLI-VE and NLVR2 datasets.\n(a) SNLI-VE-Val\n (b) SNLI-VE-Test\n (c) NLVR2-Dev\n (d) NLVR2-Test-P\nFig. 8. The statistics of the number of samples that exit early at different layers on SNLI-VE and NLVR2 datasets.\nH. The Effect of Multimodality Self-distillation\nFig. 7 illustrates a detailed analysis of the effects of multi-\nmodality self-distillation. The test samples were ﬁrst divided\ninto several groups according to the layers at which the\nsamples exit. Performance was calculated for each group.\nFor both UNITER-EE and UNITER-MSD, relatively poor\nperformances were obtained for the former layers. In contrast,\nthe latter layers (layers 9-11) all achieved a performance\nsimilar to that of the ﬁnal classiﬁer in the 12th layer. That\nis, the earlier the model exits, the lower the performance of\nthe model is achieved. On both SNLI-VE and NLVR2 dev\ndatasets, the proposed UNITER-MSD outperformed UNITER-\nEE by 0.75% and 3.0% on average in all layers, respectively. In\nthe latter layers (layers 9-12), the performances of UNITER-\nEE and UNITER-MSD were similar. In the former layers\n(layers 1-6), UNITER-MSD outperformed UNITER-EE by\n0.87% and 4.78%, particularly in the ﬁrst two layers. The\nrationale is that the former layers of the proposed UNITER-\nMSD can mimic the ﬁnal classiﬁer’s behavior to improve the\nEE classiﬁers’ performance.\nFig. 8 describes the number of samples that exit early at\ndifferent layers with different time reduction ratios ρ. The\nUNITER-KD permanently discards the former six transformer\nlayers in the UNITER of the model and thus can only obtain\na ﬁxed ρ in one distillation; the proposed UNITER-MSD can\ndynamically change the conﬁdence threshold F to satisfy the\nrequirement and obtain different ρ. If the requirement of ρ\nis strict, 79.92% and 83.50% of the samples tend to exit\nas soon as possible (layers 1-6) on SNLI-VE and NLVR2,\nrespectively. If the requirement ofρis slightly lenient, the hard\nsamples (51.40% and 44.85%) will choose to exit at the latter\nlayer (layers 9-12), thereby improving model performance.\nConversely, both easy and hard samples will be treated equally\nand exited at the ﬁnal classiﬁer using KD.\nV. C ONCLUSIONS\nThis paper proposes a multimodality self-distillation method\nfor the fast inference of VL-PTMs to improve existing EE\nstrategies. The classiﬁer in the ﬁnal layer is used to distill all\nEE classiﬁers in the former layers so that the EE classiﬁers\ncan mimic the behavior of the ﬁnal classiﬁer to improve\nperformance. To ﬁll the semantic gap between modalities, the\nmultimodalities are split into separate modalities as an extra\nindividual input to encourage the effective distillation of each\nmodality. Furthermore, the MSE was introduced to minimize\nthe distance of feature maps between the teacher and student\nmodels and further enhance the representation ability of the\nEE classiﬁers. Experiments showed that the proposed method\noutperformed the KD and EE strategies for the same time\nreduction requirement, and performed competitively even if\nthe model exited very early.\nFuture work will explore the problem of mutual interference\nthat can exist in self-distillation between different layers.\nREFERENCES\n[1] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, and C. Zitnick,\n“VQA: Visual question answering,” in Proceedings of the IEEE Inter-\nnational Conference on Computer Vision (ICCV-2015), 2015, pp. 2425–\n2433.\n[2] N. Xie, F. Lai, D. Doran, and A. Kadav, “Visual entailment: a novel task\nfor ﬁne-grained image understanding,”arXiv preprint arXiv:1901.06706,\n2019.\n[3] A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y . Artzi, “A\ncorpus for reasoning about natural language grounded in photographs,”\nin Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics (ACL-2019) , 2019, pp. 6418–6428.\n[4] J. Lu, D. Batra, D. Parikh, and S. Lee, “ViLBERT: Pretraining task-\nagnostic visiolinguistic representations for vision-and-language tasks,” in\nProceedings of the 33th International Conference on Neural Information\nProcessing Systems (NeurIPS-2019) , 2019, pp. 13–23.\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\n[5] W. Su, X. Zhu, Y . Cao, B. Li, L. Lu, F. Wei, and J. Dai, “VL-BERT: Pre-\ntraining of generic visual-linguistic representations,” in arXiv preprint\narXiv:1908.08530, 2020.\n[6] G. Li, N. Duan, Y . Fang, M. Gong, and D. Jiang, “Unicoder-VL: A\nuniversal encoder for vision and language by cross-modal pre-training,”\nin Proceedings of the 34th AAAI Conference on Artiﬁcial Intelligence\n(AAAI-2020), 2020, pp. 11 336–11 344.\n[7] Y .-c. Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan, Y . Cheng,\nand J. Liu, “UNITER : Universal image-text representation learning,”\nin Proceedings of the 16th European Conference on Computer Vision\n(ECCV-2020), 2020, pp. 104—-120.\n[8] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n“Albert: A lite bert for self-supervised learning of language representa-\ntions,” arXiv preprint arXiv:1909.11942 , 2019.\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,”\nin Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies (NAACL-HLT 2019), 2019, pp. 4171–4186.\n[10] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “RoBERTa: A robustly optimized\nBERT pretraining approach,” arXiv preprint arXiv:1907.11692 , 2019.\n[11] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural\nnetwork,” arXiv preprint arXiv:1503.02531. , 2015.\n[12] R. Tang, Y . Lu, L. Liu, L. Mou, O. Vechtomova, and J. Lin, “Distilling\ntask-speciﬁc knowledge from BERT into simple neural networks,” arXiv\npreprint arXiv:1903.12136, 2019.\n[13] Z. Sun, H. Yu, X. Song, R. Liu, Y . Yang, and D. Zhou, “MobileBERT: a\ncompact task-agnostic BERT for resource-limited devices,” in Proceed-\nings of the 58th Annual Meeting of the Association for Computational\nLinguistics (ACL-2020), 2020, pp. 2158–2170.\n[14] P. Michel, O. Levy, and G. Neubig, “Are sixteen heads really better than\none?” in Proceedings of the 33th International Conference on Neural\nInformation Processing Systems (NeurIPS-2019) , 2019, pp. 14 014–\n14 024.\n[15] S. Bao, H. He, F. Wang, H. Wu, and H. Wang, “PLATO: Pre-trained\ndialogue generation model with discrete latent variable,” in Proceedings\nof the 58th Annual Meeting of the Association for Computational\nLinguistics (ACL-2020), 2020, pp. 85–96.\n[16] S. Shen, Z. Dong, J. Ye, L. May, Z. Yao, A. Gholami, M. W. Mahoney,\nand K. Keutzer, “Q-BERT: Hessian based ultra low precision quantiza-\ntion of BERT,” in The 34th AAAI Conference on Artiﬁcial Intelligence\n(AAAI-2020), 2020, pp. 8815–8821.\n[17] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a distilled\nversion of BERT: smaller, faster, cheaper and lighter,” arXiv preprint\narXiv:1910.01108, 2019.\n[18] S. Sun, Y . Cheng, Z. Gan, and J. Liu, “Patient knowledge distillation\nfor BERT model compression,” in Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language Processing\n(EMNLP/IJCNLP-2019), 2019, pp. 4323–4332.\n[19] A. Fan, E. Grave, and A. Joulin, “Reducing transformer depth on demand\nwith structured dropout,” in arXiv preprint arXiv:1909.11556 , 2019.\n[20] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat, “Q8BERT:\nQuantized 8Bit BERT,” arXiv preprint arXiv:1910.06188 , 2019.\n[21] R. Schwartz, G. Stanovsky, S. Swayamdipta, J. Dodge, and N. A. Smith,\n“The right tool for the job: matching model and instance complexities,”\nin Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics (ACL-2020) , 2020, pp. 6640–6651.\n[22] J. Xin, R. Nogueira, Y . Yu, and J. Lin, “Early exiting BERT for efﬁcient\ndocument ranking,” in Proceedings of SustaiNLP: Workshop on Simple\nand Efﬁcient Natural Language Processing , 2020, pp. 83–88.\n[23] J. Xin, R. Tang, J. Lee, Y . Yu, and J. Lin, “DeeBERT: Dynamic early\nexiting for accelerating BERT inference,” in Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics (ACL-\n2020), 2020, p. 2246–2251.\n[24] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, “BERT loses\npatience: Fast and robust inference with early exit,” in Proceedings of\nthe 34th International Conference on Neural Information Processing\nSystems (NeurIPS-2020), 2020, pp. 18 330–18 341.\n[25] G. Jawahar, B. Sagot, and D. Seddah, “What does bert learn about\nthe structure of language?” in ACL 2019-57th Annual Meeting of the\nAssociation for Computational Linguistics , 2019.\n[26] W. Liu, P. Zhou, Z. Zhao, Z. Wang, H. Deng, and Q. Ju, “FastBERT: a\nself-distilling BERT with adaptive inference time,” in Proceedings ofthe\n58th Annual Meeting ofthe Association for Computational Linguistics\n(ACL-2020), 2020, pp. 6035–6044.\n[27] W. Wang, D. Tran, and M. Feiszli, “What Makes Training Multi-Modal\nClassiﬁcation Networks Hard?” in Proceedings of the IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition (CVPR\n2020), 2020, pp. 12 692–12 702.\n[28] R. Socher, A. Perelygin, J. Y . Wu, J. Chuang, C. D. Manning, A. Y . Ng,\nand C. Potts, “Recursive deep models for semantic compositionality\nOver a sentiment treebank richard,” in In Proceedings of the 2013\nConference on Empirical Methods in Natural Language Processing,\nEMNLP, 2013, pp. 1631–1642.\n[29] W. B. Dolan and C. Brockett, “Automatically Constructing a Corpus\nof Sentential Paraphrases,” in Proceedings of the Third International\nWorkshop on Paraphrasing (IWP2005), 2005, pp. 9–16.\n[30] H. Kaiming, Z. Xiangyu, R. Shaoqing, and S. Jian, “Deep Residual\nLearning for Image Recognition,” in Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR-2016) , 2016,\npp. 770–778.\n[31] R. Girshick, J. Donahue, T. Darrell, J. Malik, U. C. Berkeley, and\nJ. Malik, “Rich feature hierarchies for accurate object detection and\nsemantic segmentation,” in Proceedings of the IEEE Computer Society\nConference on Computer Vision and Pattern Recognition(CVPR-2014) ,\n2014, pp. 580–587.\n[32] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,\n“Learning Transferable Visual Models From Natural Language Supervi-\nsion,” in Proceedings of the 38th International Conference on Machine\nLearning (ICML-2021), 2021, pp. 8748–8763.\n[33] W. Li, C. Gao, G. Niu, X. Xiao, H. Liu, J. Liu, H. Wu, and H. Wang,\n“UNIMO: Towards uniﬁed-modal understanding and generation via\ncross-modal contrastive learning,” in Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing\nConference (ACL-IJCNLP-2021), 2021, pp. 2592–2607.\n[34] W. Kim, B. Son, and I. Kim, “ViLT: Vision-and-Language Transformer\nWithout Convolution or Region Supervision,” inProceedings of the 38th\nInternational Conference on Machine Learning (ICML-2021) , 2021, pp.\n5583–5594.\n[35] F. Yu, J. Tang, W. Yin, Y . Sun, H. Tian, H. Wu, and H. Wang, “ERNIE-\nViL: Knowledge Enhanced Vision-Language Representations Through\nScene Graph,” in Proceedings of the 34th AAAI Conference on Artiﬁcial\nIntelligence (AAAI-2020), 2020, pp. 1–9.\n[36] C. Gao, J. Chen, S. Liu, L. Wang, Q. Zhang, and Q. Wu, “Room-\nand-object aware knowledge reasoning for remote embodied referring\nexpression,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , 2021, pp. 3064–3073.\n[37] Y . Qi, Q. Wu, P. Anderson, X. Wang, W. Y . Wang, C. Shen, and A. v. d.\nHengel, “Reverie: Remote embodied visual referring expression in real\nindoor environments,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2020, pp. 9982–9991.\n[38] Y . Qiao, Y . Qi, Y . Hong, Z. Yu, P. Wang, and Q. Wu, “Hop+: History-\nenhanced and order-aware pre-training for vision-and-language naviga-\ntion,” IEEE Transactions on Pattern Analysis and Machine Intelligence ,\nvol. 45, no. 7, pp. 8524–8537, 2023.\n[39] R. Girshick, “Fast R-CNN,” in Proceedings of the IEEE International\nConference on Computer Vision (ICCV-2015) , 2015, pp. 1440–1448.\n[40] H. Tan and M. Bansal, “LXMert: Learning cross-modality encoder\nrepresentations from transformers,” in Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP-2020), 2020, pp. 5100–5111.\n[41] J. Lu, V . Goswami, M. Rohrbach, D. Parikh, and S. Lee, “12-in-1: Multi-\ntask vision and language representation learning,” in Proceedings of the\nIEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR 2020) , 2020, pp. 10 434–10 443.\n[42] X. Yang, H. Zhang, G. Qi, and J. Cai, “Causal Attention for Vision-\nLanguage Tasks,” in Proceedings of the IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition (CVPR2021), 2021,\npp. 9842–9852.\n[43] Z. Huang, Z. Zeng, B. Liu, D. Fu, and J. Fu, “Pixel-BERT: Aligning\nImage Pixels with Text by Deep Multi-Modal Transformers,” arXiv\nPrepr. arXiv2004.00849, 2020.\n[44] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu,\nL. Dong, F. Wei, Y . Choi, and J. Gao, “Oscar: Object-Semantics Aligned\nPre-training for Vision-Language Tasks,” inProceedings of the European\nConference on Computer Vision (ECCV-2020) , 2020, pp. 121–137.\n[45] J. Lin, A. Yang, Y . Zhang, J. Liu, J. Zhou, and H. Yang, “InterBERT:\nVision-and-Language Interaction for Multi-modal Pretraining,” in arXiv\npreprint arXiv:2003.13198., 2020.\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\n[46] Z. Gan, Y . C. Chen, L. Li, C. Zhu, Y . Cheng, and J. Liu, “Large-scale\nadversarial training for vision-and-language representation learning,” in\nProceedings of the 34th International Conference on Neural Information\nProcessing Systems (NeurIPS-2020) , 2020, pp. 6616–6628.\n[47] M. Gordon, K. Duh, and N. Andrews, “Compressing BERT:\nStudying the Effects of Weight Pruning on Transfer Learning,”\narXiv:2002.08307v2, 2020.\n[48] J. S. McCarley, R. Chakravarti, and A. Sil, “Structured Pruning of BERT-\nbased Question Answering Models,” arXiv:1910.06360v3, 2019.\n[49] Z. Lin, J. Liu, Z. Yang, N. Hua, and D. Roth, “Pruning Redundant\nMappings in Transformer Models via Spectral-Normalized Identity\nPrior,” in Findings of the Association for Computational Linguistics\n(EMNLP-2020), 2020, pp. 719–730.\n[50] S. Han, J. Pool, J. Tran, and W. J. Dally, “Learning both weights and\nconnections for efﬁcient neural networks,” in Proceedings of the 29th\nInternational Conference on Neural Information Processing Systems\n(NeurIPS-2015), 2015, pp. 1135–1143.\n[51] H. Li, H. Samet, A. Kadav, I. Durdanovic, and H. P. Graf, “Pruning\nﬁlters for efﬁcient convnets,” in Proceedings of the 5th International\nConference on Learning Representations (ICLR-2017) , 2017, pp. 1–13.\n[52] Z. Liu, Y . Wang, K. Han, S. Ma, and W. Gao, “Post-Training Quantiza-\ntion for Vision Transformer,” in Proceedings of the 34th International\nConference on Neural Information Processing Systems (NeurIPS-2021) ,\n2021, pp. 1–12.\n[53] Z. Gao, Y . Yao, S. Zhang, J. Yang, M. Lei, and I. McLoughlin,\n“Extremely low footprint end-to-end ASR system for smart device,”\narXiv Prepr. arXiv2104.05784, 2021.\n[54] X. Jiao, Y . Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and\nQ. Liu, “Tinybert: Distilling bert for natural language understanding,”\narXiv:1909.1035, 2019.\n[55] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is All You Need,” inProceedings\nof the 31th International Conference on Neural Information Processing\nSystems (NeurIPS-2017), 2017, pp. 5998–6008.\n[56] S. Teerapittayanon, B. McDanel, and H. T. Kung, “BranchyNet: Fast\ninference via early exiting from deep neural networks,” in Proceedings\nof the 23th International Conference on Pattern Recognition (ICPR-\n2016), 2016, pp. 2464–2469.\n[57] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\narXiv preprint arXiv:1711.05101 , 2017.\n[58] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, “Visu-\nalBERT: A simple and performant baseline for vision and language,”\narXiv preprint arXiv:1908.03557 , 2019.\nJun Kong is currently a Ph.D. candidate at the\nSchool of Information Science and Engineering,\nYunnan University, China. He received a Bachelor’s\nDegree in Information Engineering from Southwest\nForestry University, China. His research interests\ninclude natural language processing, text mining,\nand machine learning.\nJin Wang is a professor at the School of Information\nScience and Engineering, Yunnan University, China.\nHe holds a Ph.D. in Computer Science and Engineer-\ning from Yuan Ze University, Taoyuan, Taiwan, and\nanother Ph.D. in Communication and Information\nSystems from Yunnan University, Kunming, China.\nHis research interests include natural language pro-\ncessing, text mining, and machine learning.\nLiang-Chih Yu is a professor in the Department of\nInformation Management at Yuan Ze University in\nTaiwan, R.O.C. He received his Ph.D. in Computer\nScience and Information Engineering from National\nCheng Kung University in Taiwan, R.O.C. He was\na visiting scholar at the Natural Language Group,\nInformation Sciences Institute, University of South-\nern California (USC/ISI) from 2007 to 2008 and at\nDOCOMO Innovations for three months in 2018.\nHe is a Board Member and Convener of SIGCALL\nof the Association for Computational Linguistics\nand Chinese Language Processing (ACLCLP). He serves as an editorial\nboard member of International Journal of Computational Linguistics and\nChinese Language Processing. His research interests include natural language\nprocessing, sentiment analysis, and computer-assisted language learning. His\nteam has developed systems that ranked ﬁrst in IJCNLP 2017 Task 4:\nCustomer Feedback Analysis and second in the recent SemEval and BEA\nshared task competitions.\nXuejie Zhang is a professor at the School of\nInformation Science and Engineering, and Director\nof the High-Performance Computing Center, Yunnan\nUniversity, China. He received his Ph.D. in Com-\nputer Science and Engineering from the Chinese\nUniversity of Hong Kong in 1998. His research\ninterests include high-performance computing, cloud\ncomputing, and big data analytics.\nThis article has been accepted for publication in IEEE Transactions on Multimedia. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMM.2024.3384060\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8285372257232666
    },
    {
      "name": "Inference",
      "score": 0.7397903203964233
    },
    {
      "name": "Distillation",
      "score": 0.639564573764801
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6215642094612122
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6200441122055054
    },
    {
      "name": "Machine learning",
      "score": 0.5620856285095215
    },
    {
      "name": "Multimodality",
      "score": 0.53546541929245
    },
    {
      "name": "Modalities",
      "score": 0.526383638381958
    },
    {
      "name": "Semantic gap",
      "score": 0.4858707785606384
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.476121723651886
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3376920223236084
    },
    {
      "name": "Image retrieval",
      "score": 0.09031832218170166
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I189210763",
      "name": "Yunnan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99908691",
      "name": "Yuan Ze University",
      "country": "TW"
    }
  ],
  "cited_by": 5
}