{
    "title": "Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics",
    "url": "https://openalex.org/W3086963535",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2603417723",
            "name": "Avisha Das",
            "affiliations": [
                "University of Houston"
            ]
        },
        {
            "id": "https://openalex.org/A2118989996",
            "name": "Rakesh M. Verma",
            "affiliations": [
                "University of Houston"
            ]
        },
        {
            "id": "https://openalex.org/A2603417723",
            "name": "Avisha Das",
            "affiliations": [
                "University of Houston"
            ]
        },
        {
            "id": "https://openalex.org/A2118989996",
            "name": "Rakesh M. Verma",
            "affiliations": [
                "University of Houston"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6749351710",
        "https://openalex.org/W6738966219",
        "https://openalex.org/W2072471709",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W1982897610",
        "https://openalex.org/W2963544700",
        "https://openalex.org/W6761205521",
        "https://openalex.org/W2963672599",
        "https://openalex.org/W2605035112",
        "https://openalex.org/W2598692538",
        "https://openalex.org/W2810732773",
        "https://openalex.org/W1507711477",
        "https://openalex.org/W2561658355",
        "https://openalex.org/W2808064329",
        "https://openalex.org/W2963167310",
        "https://openalex.org/W6682691769",
        "https://openalex.org/W2143017621",
        "https://openalex.org/W2992347006",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2807791032",
        "https://openalex.org/W6763240421",
        "https://openalex.org/W2807738734",
        "https://openalex.org/W2752337926",
        "https://openalex.org/W6763701032",
        "https://openalex.org/W6769243733",
        "https://openalex.org/W2889009749",
        "https://openalex.org/W2807925339",
        "https://openalex.org/W2914949666",
        "https://openalex.org/W7017743087",
        "https://openalex.org/W2563845258",
        "https://openalex.org/W6761999480",
        "https://openalex.org/W6638273328",
        "https://openalex.org/W6691493741",
        "https://openalex.org/W6761551260",
        "https://openalex.org/W6776403474",
        "https://openalex.org/W2964213788",
        "https://openalex.org/W2068390867",
        "https://openalex.org/W6766654264",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2135046866",
        "https://openalex.org/W6747826347",
        "https://openalex.org/W6743477263",
        "https://openalex.org/W6763473550",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W6607974698",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2983962589",
        "https://openalex.org/W2962821399",
        "https://openalex.org/W6950536863",
        "https://openalex.org/W4299567010",
        "https://openalex.org/W2971008823",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2784823820",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2967126358",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2949615070",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W196214544",
        "https://openalex.org/W2622385665",
        "https://openalex.org/W3034287667",
        "https://openalex.org/W2251291469",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2996287690",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W1590378318",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2943553207",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2888779557",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2792210438",
        "https://openalex.org/W2896457183"
    ],
    "abstract": "Massive textual content has enabled rapid advances in natural language modeling. The use of pre-trained deep neural language models has significantly improved natural language understanding tasks. However, the extent to which these systems can be applied to content generation is unclear. While a few informal studies have claimed that these models can generate `high quality' readable content, there is no prior study on analyzing the generated content from these models based on sampling and fine-tuning hyperparameters. We conduct an in-depth comparison of several language models for open-ended story generation from given prompts. Using a diverse set of automated metrics, we compare the performance of transformer-based generative models - OpenAI's GPT2 (pre-trained and fine-tuned) and Google's pre-trained TransformerXL and XLNet to human-written textual references. Studying inter-metric correlation along with metric ranking reveals interesting insights - the high correlation between the readability scores and word usage in the text. A study of the statistical significance and empirical evaluations between the scores (human and machine-generated) at higher sampling hyperparameter combinations (t = {0.75, 1.0}, k = {100, 150, 250}) reveal that the top pre-trained and fine-tuned models generated samples condition well on the prompt with an increased occurrence of unique and difficult words. The GPT2-medium model fine-tuned on the 1024 Byte-pair Encoding (BPE) tokenized version of the dataset along with pre-trained Transformer-XL models generated samples close to human written content on three metrics: prompt-based overlap, coherence, and variation in sentence length. A study of overall model stability and performance shows that fine-tuned GPT2 language models have the least deviation in metric scores from human performance.",
    "full_text": "Received June 28, 2020, accepted August 4, 2020, date of publication September 11, 2020, date of current version October 14, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3023421\nCan Machines Tell Stories? A Comparative Study\nof Deep Neural Language Models and Metrics\nAVISHA DAS\n AND RAKESH M. VERMA\n, (Member, IEEE)\nDepartment of Computer Science, University of Houston, Houston, TX 77204, USA\nCorresponding author: Rakesh M. Verma (rverma@uh.edu)\nThis work was supported in part by NSF under Grant DGE 1433817 and Grant CCF 1950297, and in part by the U.S. Army Research\nOfﬁce under Grant W911NF-16-1-0422 and Grant W911NF-20-1-0254.\nABSTRACT Massive textual content has enabled rapid advances in natural language modeling. The use of\npre-trained deep neural language models has signiﬁcantly improved natural language understanding tasks.\nHowever, the extent to which these systems can be applied to content generation is unclear. While a few\ninformal studies have claimed that these models can generate ‘high quality’ readable content, there is no prior\nstudy on analyzing the generated content from these models based on sampling and ﬁne-tuning hyperparam-\neters. We conduct an in-depth comparison of several language models for open-ended story generation from\ngiven prompts. Using a diverse set of automated metrics, we compare the performance of transformer-based\ngenerative models – OpenAI’s GPT2 (pre-trained and ﬁne-tuned) and Google’s pre-trained Transformer-\nXL and XLNet to human-written textual references. Studying inter-metric correlation along with metric\nranking reveals interesting insights – the high correlation between the readability scores and word usage\nin the text. A study of the statistical signiﬁcance and empirical evaluations between the scores (human and\nmachine-generated) at higher sampling hyperparameter combinations (t ={0.75, 1.0}, k ={100, 150,250})\nreveal that the top pre-trained and ﬁne-tuned models generated samples condition well on the prompt\nwith an increased occurrence of unique and difﬁcult words. The GPT2-medium model ﬁne-tuned on the\n1024 Byte-pair Encoding (BPE) tokenized version of the dataset along with pre-trained Transformer-XL\nmodels generated samples close to human written content on three metrics: prompt-based overlap, coherence,\nand variation in sentence length. A study of overall model stability and performance shows that ﬁne-tuned\nGPT2 language models have the least deviation in metric scores from human performance.\nINDEX TERMS Deep learning, transformer-based architecture, neural language models, natural language\nevaluation, story generation, generative pre-trained transformer (GPT), transformer-XL, XLNet, natural\nlanguage generation.\nI. INTRODUCTION\nNatural language generation has gained popularity with new\nlanguage resources and language models, which can be used\nto emulate the stylistic aspects of the training dataset. Beyond\ngenerating textual content such as stories and poems, lan-\nguage generation systems have been used for conversational\ndialog [17], automated headline generation [11], etc.\nWith the growing prominence of deep learning, an\napproach known as end-to-end learning [8] has become\npopular. Researchers have proposed several novel architec-\ntures capable of modeling robust representations of natural\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Alicia Fornés\n.\nlanguage – recurrent neural networks (RNNs) [47], sequential\nencoders-decoders (sequence-to-sequence learning) [8], gen-\nerative adversarial networks (GANs) [9], and transformers\nwith attention-modeling [51].\nIn recent years, use of large-scale neural language models\ntrained on massive volumes of textual content has emerged\nas a solution to many natural language based tasks. Publicly\navailable pre-trained models, such as OpenAI’s GPT [35],\n[36], AllenNLP’s ELMo [31], Google’s BERT [7], and\nGoogle/CMU’s XLNet [55], have improved performance on\nnatural language understanding tasks considerably. 1 These\n1https://gluebenchmark.com/leaderboard\n181258 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\narchitectures have been pre-trained on massive 2 amounts of\nraw or unlabeled textual content to build language models that\ncan be readily applied to natural language based tasks with\nhardly any ﬁne-tuning [41] - a technique called ‘‘zero-shot\"\nlearning [35]. Further ﬁne-tuning these pre-trained models\non a speciﬁc dataset (usually smaller in size than the origi-\nnal) leads to better models. This achieves better results than\nsolely training a neural architecture on the new dataset, which\nresults in an overﬁtted model [41], as was the case with\npreviously proposed RNN-based generative systems.\nIn this paper, we examine the reproducibility and general-\nizability of multiple massively-trained language models in the\nrealm of open-ended natural language generation. We study\nthe behavior of these pre-trained models in a ‘‘zero-shot\"\nsetting and with ﬁne-tuning on a dataset of human written sto-\nries and writing prompts. We also recognize that a selection\nof sampling hyperparameters (e.g. temperature, top-k value,\netc.) for generation play an important role in determining\nthe quality of the text generated. To evaluate the generated\ncontent, we use a range of metrics to compare with human\nwritten references – semantic relatedness, linguistic quality\nand syntactic style.\nIn addition, we explore three questions. Does ﬁne-tuning\na model on the dataset improve the quality of sample genera-\ntion? What role do hyperparameters play in linguistic quality\nof generated text? Can writing quality be quantiﬁed using\nstatistical measures? Are some metrics better at capturing the\nsimilarity/differences between the textual content generated\nby a machine or by a human? Our contributions are:\n• We analyze correlations among several metrics.\n• We present a rank-based evaluation of the metrics\nusing a linear regularized model to see which metrics\nbest distinguish the human writing and auto-generated\ninstances.\n• We analyze the results of the higher ranked metrics\nby varying the sampling hyperparameters (sampling\nparameter k, softmax temperature t) for both pre-trained\nand ﬁne-tuned models.\n• We ﬁnd combinations of k, t, models and metrics for\nwhich there is no statistically signiﬁcant difference\nbetween the generated text and human-written content.\nOur analysis of the metrics shows that, as expected,\nthe story-prompt overlap percentages are highly correlated\n(ρ >0.9) at different values of n with bigram overlap being\nthe best ranked metric. n-gram overlap metrics are highly\ncorrelated with the stylistic metrics – sentence length (mean,\nL_avg and standard deviation, L_sd) and noun distributions.\nDespite having low correlation between themselves and with\ncoherency-based metrics, Dale-Chall readability score has\na high positive correlation (ρ > 0.9) with Type-token\nratio, which shows the generation of unique and difﬁcult\nwords. The higher ranked metrics with positive correla-\ntion coefﬁcient with respect to the text nature (human or\n2the size of the training data is much larger than the size of the data used\nfor the targeted task\nmachine-generated) are the bigram story-prompt overlap per-\ncentage, type-token ratio and standard deviation of sentence\nlengths.\nInterestingly, we see that retraining the model on a subset\nof the domain speciﬁc data enhances model performance.\nFine-tuning helps generate samples with story-prompt over-\nlap closer to human writing, but the type token ratio increases\nwith increasing sampling hyperparameter values. In our\nstudy, a model performs the best with respect to a metric when\nthe scores of the generated stories are statistically similar\nto that of the human written references. However, as we\nwill see, sampling parameters play an important role in text\ngeneration – in our analysis, we ﬁnd that higher values of\nsoftmax temperature t (> 0.5) and mid-range values of k\n(50 < k < 500) work the best. The models with sam-\nples performing at the human level on the top metrics are\nthe ﬁne-tuned GPT2 (117M and 355M) models along with\nthe pre-trained models like GPT-110M and Transformer-XL.\nWe see that the best hyperparameter combination is t =\n0.75,k = 150 for generating samples closest to human\nwriting. While the overlap of the generated stories with the\nprompt is the closest to human references at higher sampling\nparameters, variation in the sentence length of the textual\ncontent increases when compared to human references. Ana-\nlyzing the L_sd shows that the deviation from the mean\nlength increases with an increase in the sampling parameters\n(t and k), with the pre-trained models performing closer to\nthe human level.\nA. PAPER ORGANIZATION\nSection II presents the related work. The dataset used for\nevaluation and the background required is described in\nSections III and IV respectively. Section V describes the\nsampling and decoding algorithms used for generating\nthe content. The experiment setup is in Section VI. The\nmetrics, their correlation analysis and metric ranking are\nin Section VII. Results are in Sections X through VIII.\nSection XIV concludes.\nII. RELATED WORKS\nA deep network trained on large amount of written text is\ncapable of emulating the human writing style [47]. We now\nsummarize related work on language generation models and\nevaluation metrics.\nA. NEURAL TEXT GENERATION\nWe distinguish text generation models based on length of the\ngenerated content.\n1) LONG-FORM CONTENT GENERATION\nAutomated long content generation is a difﬁcult\ntask – maintaining coherence becomes more challenging with\nan increase in the length. Deep neural architectures such as\nRecursive Neural Networks (RNNs), Long Short Term Mem-\nory (LSTM) networks are widely used for content generation\n[14], [23], [47] owing to their ability to learn dependencies\nVOLUME 8, 2020 181259\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nacross the textual context [15]. LSTMs have been used\nfor generating stories [8], Shakespearean Sonnets [54] and\nnon-English poetry [12], [57], and for reading comprehension\nbased tasks [24].\nRNNs have also been used widely for sequence-to-\nsequence learning, e.g., [10], [19], [34], [59]. Standard\nsequence-to-sequence models have found application in\nopen-ended content generation [48], but such straight-\nforward encoder-decoder setups fail to generate content con-\nditioned on a given starting seed. In [8], researchers proposed\na state-of-the-art hierarchical neural fusion architecture using\ntwo seq2seq models [46] along with multi-scale gated atten-\ntion mechanism ensuring relatedness of generated content to\na given prompt. Since simple encoder-decoder architectures\nfail to model important meaningful representations of words\nand phrases, [21] used Gated Recurrent Unit (GRU)-based\nneural checklist models for recipe generation.\nOther generation techniques include deep learning with\nMarkov Models [52], variational auto-encoders [38], [44],\nand generative adversarial networks [33]. Researchers used\nmulti-level variational auto encoders (sentence and word\nlevel) for generative decoder networks in [44]. The proposed\nsystem is used to generate Yelp reviews and abstracts of\narXiv academic papers. Researchers in [10] use attention\nbased encoder-decoder models for preserving coherence and\ncontext in the generated open-domain stories.\nWith the rise of language models pre-trained on massive\namounts of textual content, generating content is becoming\nfeasible [28]. Deep GPT2 language model [36] from Ope-\nnAI has gained a lot of attention in language generation\napplications.3 GPT models are built using a decoder-only\ntransformer based architecture. The authors in [41] have com-\npared generated content from the seq2seq fusion models and\nﬁne-tuned small GPT2 language model (117M parameters)\nwith respect to story generation from writing prompts.\n2) SHORT-FORM CONTENT GENERATION\nWhile creative content (stories, poems) is longer text gen-\neration – automated systems are widely used for generating\nshorter textual content. These include generation of tweets\nwith synthetic URLs [43] and reviews [56]. RNN-based\nmethods for generating text messages are in [45]. The authors\nof [58] generate fake news articles from a given set of head-\nlines using a transformer based architecture proposed by [35].\nThe authors found that readers preferred machine generated\nnews articles as more readable than human written references.\nResearchers in [2], [6], [13], ‘weaponize’ machine learn-\ning techniques to launch targeted attacks. The paper [2]\nuses a manual grammar-based approach for synthetic email\ngeneration and also studies the likelihood of a human to\nrecognize a generated email from a legitimate counterpart.\nThe system proposed in [6] uses an RNN-based architecture\nwith word units for email generation. The authors train the\nmodel on a dataset of legitimate and phishing emails and\n3https://openai.com/blog/better-language-models/\ngenerate samples by using greedy decoding techniques. How-\never, the generated samples suffer from incoherence. For\ndecoding, researchers in [41] use top-k sampling method.\nPreviously researchers have used mostly greedy sampling\nmethods for generating sequences [12], [14], [54].\nB. NEURAL TEXT EVALUATION\nWe organize the literature on content evaluation metrics based\non syntactic and semantic properties.\n1) SYNTACTIC EVALUATION\nBangalore et al. [3] proposed automated accuracy-based met-\nrics, which account for string matching as well as matches\nin the dependency-based parse tree to quantify the level\nof agreement between a given reference and the generated\ncontent. They also manually rate the generated content for\nquality and understandability using a scale of numeric scores\nfrom 1 (lowest) to 7 (highest). The paper [40] presents a\ncomparison of automated and subject-based approaches for\nsynthetic text with gold-standard reference texts using an\nNLG-based case study called ENIGMA. A Turing-style test\nfor quality evaluation was also proposed in this paper. A num-\nber of grammar-based metrics: count of misspelled words,\nparsing score, and percentage of word overlap (BLEU) were\ncompared with human evaluation results in [27].\n2) SEMANTIC EVALUATION\nEvaluating linguistic and semantic quality of generated text\nis essential. However, there could be a bias in choosing a\nmetric or a method to evaluate the generated content automat-\nically [50]. While existing automated evaluation metrics are\nthe not the best, manually evaluating generated text quality\ncan be time consuming and prone to bias [27]. Authors put\ntogether a comprehensive evaluation of semantic-based auto-\nmated metrics in [37]. To capture semantic relations across\nsentences at a word-level, the authors of [60] propose a\nsimilarity-based evaluation metric BERTScore. This metric is\nshown to perform better at measuring linguistic quality than\nexisting metrics BLEU, ITER by correlating the score with\nhuman level judgements on two natural language understand-\ning tasks: image captioning and machine translation.\nRecently, researchers have looked into methods that com-\npare text quality while taking into account hyperparameters\nthat inﬂuence sample decoding from trained deep language\nmodels. The authors in [18] investigate how automated dis-\ncriminators compare with human evaluators in this context.\nThey also explore whether factors like sampling temperature\nand decoding parameters play a role in controlling the nature\nof the generated content. The paper [27] also reports results\nusing a semantic similarity measure based on distributional\nsimilarity in text and Latent Semantic Analysis proposed\nby [16].\nIII. DATASET\nWe use the WritingPrompts dataset [8], [41] consisting\nof 303,358 pairs of prompts and manually written stories.\n181260 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nThe dataset was collected by scraping three years of prompts\nand associated stories from Reddit’s WritingPrompts forum. 4\nThe dataset was built through crowdsourcing on an online\ncommunity called WritingPrompts where users can submit\npremises to stories or prompts and can invite submissions\nfrom other online users. Each prompt or premise can have\nmultiple story submissions, varying in length, topic and struc-\nture. The submitted story responses should follow or be\ninspired in some manner by the prompt. For other details\nabout this dataset, we refer the readers to [8]. Table 1 shows\nan example of a story-prompt pair from the WritingPrompts\ndataset.\nTABLE 1. An example prompt-story pair from the WritingPrompts dataset.\nFor evaluation, the dataset was split into three parts:\ntraining (90%), testing (5%) and validation (5%). Follow-\ning the preprocessing steps in [8], the stories from the\ndataset are truncated to the ﬁrst 1000 words for the exper-\niments. For model ﬁne-tuning, we preprocess the origi-\nnal dataset to create two new datasets using the Byte Pair\nEncoding (BPE) tokenization scheme – WritingPrompts-\n512 and WritingPrompts-1024, where the cut-off BPE length\nis 512 and 1024 tokens respectively. We discuss in detail the\ndata preprocessing steps along with the dataset statistics in\nSection VI-A.\nIV. NEURAL GENERATIVE ARCHITECTURE AND\nLANGUAGE MODELS\nWhile RNNs have been used widely used for modeling\ncontextual representations of textual content, such networks\nare computationally expensive [11] and fail to capture long\nterm dependencies across longer sequences of written text.\nIn this paper, we compare language models built by train-\ning transformer-based architectures with self-attention [51].\nThese include OpenAI’s GPT and GPT-2 architectures\n[35], [36]. Additionally, we discuss two transformer-based\npre-trained generative language models: XLNet [55] and\nTransformer-XL [5], proposed by Google/CMU as modiﬁed\nversions of the GPT-2 architecture.\nThe network is built from a stack of encoders and decoders\nwith self-attention, called the transformer blocks. The self-\nattention layer takes into account the importance of the neigh-\nbouring units in a given context piece as it encodes the input\nfor boosting model performance. We refer readers to [51] and\n[36] for a detailed overview about self attention and trans-\nformer networks. To compare generative models with human\nwriting, we look at the publicly available large-scale pre-\ntrained language models released by OpenAI and Google.\n4www.reddit.com/r/WritingPrompts/\nThese models have yielded exemplary results in a a wide\nvariety of applications even when applied in a ‘zero-shot’ 5\nsetting [36], [41]. While experimenting with pre-trained lan-\nguage models may be feasible for demonstrating a proof-of-\nthe-concept application – an in-depth study and evaluation\nfor a speciﬁc task would require model retraining or ﬁne-\ntuning. We give the model names and their parameter sizes\nbelow.\nA. OpenAI’s GPT\nOpenAI’s GPT2 [35], [36] is essentially a large transformer-\nbased network trained on web-scraped textual content. 6\nThe generative GPT architecture is based on trans-\nformer decoder-only blocks. The largest trained model has\n1.5 billion parameters and has been shown to outperform\nSOTA approaches on natural language understanding based\ntasks [36]. OpenAI’s GPT [35] model variants considered\nin this paper are the following – the smallest openai-gpt,\nand the subsequently released three GPT2 models [36], gpt2,\ngpt2-medium and gpt2-large.\nFine-tuning refers to model retraining on a task and\ndomain speciﬁc dataset without largely modifying the archi-\ntecture, to further tune the model to the speciﬁc data to be\nevaluated on. However, ﬁne-tuning such huge transformer\nmodels can be computationally intensive. See et al. [41]\ncompares the ﬁne-tuned version of the smallest GPT2 model\n(117 million parameters) with the Seq2Seq based Fusion\nmodel [8], SOTA architecture for open-ended story gener-\nation from writing prompts. In this study, we retrain two\nGPT2 models. 7 After ﬁne-tuning these models on each of\nthe above datasets, we built the following four language\nmodels – wP_512_117M, wP_1024_117M, wP_512_355M\nand wP_1024_355M. We explain our ﬁne-tuning experiment\nin Section VI.\nB. GOOGLE/CMU’s TRANSFORMER-XL AND XLNet\nFollowing the success of GPT2, Google/CMU released the\nTransformer-XL [5] and XLNet [55] models which improve\nupon the GPT2 language models. XLNet has been trained\non multiple datasets which amount to a total of 136GB\nof text. Apart from the GPT models, models such as –\nXLNet [55] and Transformer-XL [5], have been shown to\ngenerate comparatively better textual content without prior\ntraining [1]. There are two pre-trained XLNet models – xlnet-\nbase-cased and xlnet-large-cased, and the Transformer-XL\nmodel (transfo-xl-wt103). We refer the readers to the cited\npapers to explore these architectures in detail. However, at the\ntime of our experiments, the authors had not released the\naforementioned models for ﬁne-tuning.\n5an experimental setup without any parameter or architecture modiﬁcation\nand training\n6dataset size is 40GB\n7https://storage.googleapis.com/gpt-2/\nVOLUME 8, 2020 181261\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nV. SAMPLE GENERATION AND DECODING ALGORITHMS\nText generation using a trained language model is initiated by\nfeeding one word at a time (starting seed word) to the model.\nThe output is generated by stochastically selecting the most\nlikely word to follow this given word from the probability\ndistribution returned by the trained language model.\nA. SAMPLING ALGORITHMS\nPrior research uses techniques like random sampling, greedy\nsampling based and beam-search based decoding for sample\ngeneration. While greedy sampling techniques [12], [47],\n[54] choose the word with the highest probability from the\ndistribution (argmax ), this may not be the best solution for\nsample generation always. For sample generation, authors\nin [8], [36] use top-k random sampling scheme which is nei-\nther greedy (greedy decoding) nor non-deterministic (random\nsampling). The probability distribution at each timestep is\nredistributed among the top k tokens. The token with the max-\nimum probability is chosen as the output. 8 The papers [8],\n[36], [41] discuss the efﬁcacy of this technique over conven-\ntional beam search and greedy methods [11] and experiment\nwith different values of k for sample generation. In this paper,\nwe use top-k sampling algorithm with varying values of k.\nB. SOFTMAX TEMPERATURE CONTROL\nThe ﬁnal layer of the model, responsible for calculating the\nconditional probability, is a softmax normalization used for\ncomputing the distribution for the next word followed by\nsubsequent sampling. We use temperature (τ) as a hyperpa-\nrameter for selecting word samples - regulating the parameter\nτ in Equation 1 encourages or controls the diversity of the\ngenerated text. The novelty or eccentricity of the generative\nmodel can be evaluated by varying the temperature param-\neter between 0 < τ ≤ 1.0. While, lower values of τ\ngenerate relatively deterministic samples, higher values can\nmake the process more stochastic. Equation 1 shows the\nprobability distribution built by the model for the sequences\nof words along with the incorporation of temperature control\n8https://huggingface.co/blog/how-to-generate#top-k-sampling\nTABLE 2. Statistics of truncated WritingPrompts dataset.\nparameter(τ), P(Wt+1|Wt ) =softmax(Wt ),\nsoftmax(Wt ) = e\nW it\nτ\n∑n\nj=1 e\nW j\nt\nτ\n(1)\nC. UNCONDITIONAL AND CONDITIONAL SAMPLING\nSample generation is the ﬁnal step in complete text generative\nmodeling. Common techniques to sample textual content\nusing generative language models with or without retrain-\ning include two methods – unconditional and conditional\nsampling.\nGenerating samples unconditionally refers to using the\ngenerative model to output textual content without taking into\naccount any user input. The model spits out text without tak-\ning actual starting seed or conditional statement from the user.\nInteractive conditional sample refers to generating samples\nbased on user input. In other words, the user inputs some text\nand the trained language model does its best to ﬁll in the rest.\nThe command and parameters available is the same as that of\nunconditional sampling.\nVI. EXPERIMENTAL SETUP\nWe describe the preprocessing steps followed to prepare the\ndatasets for model ﬁne-tuning and evaluation. Additionally,\nthe section also describes how to acquire and setup the\npre-trained language models during sample generation.\nA. DATASET PREPROCESSING\nAuthors of [8] have provided a fairly clean and pre-processed\nversion of the WritingPrompts dataset. 9 The details of the\noriginal dataset has been described in Section III. Follow-\ning [8] and [41], we truncate the human-written stories in\nthe dataset to the ﬁrst 1000 words. Table 2 summarizes the\nstatistics of the truncated WritingPrompts dataset used in\nexperiments.\nByte Pair Encoding (BPE) tokenization scheme was intro-\nduced by Sennrich et al. [42] as a data compression technique\nto improve machine translation tasks. The method reduces the\ntotal vocabulary size by keeping more frequent words while\nreplacing the less frequent ones with a sequence of tokens.\nThe BPE method ensures a balance between character- and\nword-level hybrid representations thus making it capable\nof effectively encoding large corpora. For the purpose of\nﬁne-tuning the language models, we create two additional\ndatasets - WritingPrompts-512 and WritingPrompts-1024.\nThese datasets are more suitable for the limited context\n9https://dl.fbaipublicﬁles.com/fairseq/data/writingPrompts.tar.gz\n181262 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 1. Heatmap showing pairwise metric correlation.\nTABLE 3. Statistics of truncated WritingPrompts-512 dataset.\nTABLE 4. Statistics of truncated WritingPrompts-1024 dataset.\nTABLE 5. Hyperparameters for fine-tuning the GPT2-medium and\nGPT2-large models.\nsize10 of the GPT2 models that will be ﬁne-tuned on the\nprompt-story dataset.\nThe scheme is used to tokenize the (prompt ,story) pairs\nand select the instances having a total token length equal to\na certain given threshold (BPE token length). In this paper,\nthe BPE token lengths chosen were 512 and 1024 respec-\ntively. Speciﬁcally, we keep the instances which have a\ntotal length 11 less than or equal to 512 and 1024 BPE\n10the maximum BPE token size (here, 1024) that the model can process\n11length is calculated by concatenating the prompt and the story together\nFIGURE 2. Metric ranking with lasso regularization method.\ntokens respectively. The tokenization was done using the\nBPE model for English provided by the Python library\n‘BPEmb.’12 Tables 3 and 4 summarize the statistics of\nthe resulting preprocessed datasets. Note that the Writing-\nPrompts-1024 dataset is a better representative of the original\ndataset than WritingPrompts-512, as demonstrated by the\ndescriptive statistics in Table 4.\nB. MODEL FINE-TUNING\nRetraining the pretrained models is necessary to condition\nthe model on the given set of story prompts and make the\n12https://nlp.h-its.org/bpemb/#usage\nVOLUME 8, 2020 181263\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 3. Pilot Study I withpre-trained GPT modelsfor Bigram-Prompt overlap by (a) varying top-ksampling parameter at constant\ntemperature t = 1.0. (b) varying temperaturet at a constant top-k= 50.\nFIGURE 4. Pilot Study II withfine-tuned GPT modelsfor Bigram-Prompt overlap by (a) varying top-ksampling parameter at constant\ntemperature t = 1.0. (b) varying temperaturet at a constant top-k= 50.\nretrained model generate stylistically and linguistically bet-\nter stories from the prompts. The model ﬁne-tuning can be\ntermed similar to building a language model on the Writ-\ningPrompts dataset, where each prompt and story pair is\nregarded as a one sequence separated by the delimiter token\n– <|endoftext|>. We use the Python implementation of\nthe GPT2 models made available by OpenAI. 13\nThe ﬁne-tuning experiment resulted in four models – the\nhyperparameters and the model training times and average\nloss achieved on the validation dataset are given in Table 5.\nThe batch size and initial learning rate for the ﬁne tuning\nexperiments are chosen as 2 and 2 ∗10−5. The batch size\nand the learning rate 14 were chosen based on the computation\ncapability of our GPU. The models were trained using Python\n3.6 on a Quadro P1000 GPU. For the four models, per-word\nperplexity using average loss (\nloss), i.e. if we consider it equal\n13https://github.com/nshepperd/gpt-2\n14Initial learning rate with an exponential decay and a decay rate\nof 0.96 and 10,000 steps\nto eloss, falls in the range of 13.06 to 18.7 units which is lesser\nthan the baseline fusion model mentioned earlier [8] on the\nvalidation datasets.\nC. REPRODUCING PRE-TRAINED LANGUAGE MODELS\nWith the pre-trained Seq2Seq Fusion model [8] as base-\nline, authors in [41] ﬁne-tune the small GPT2 model with\n117M parameters (GPT2-117) [36] on a trimmed version\nof the WritingPrompts dataset (instances with 1024 BPE\ntokens). Besides model ﬁne-tuning, we also apply the lan-\nguage models mentioned earlier for the story generation\ntask in a ‘zero-shot’ setting. For reproducing the ‘massively’\npre-trained language models – OpenAI’s GPT and GPT2\nand Google/CMU’s XLNet and Transformer-XL, we use\nthe HuggingFace repository [53] which has implementations\nof these models for language generation. 15 We ran the models\n15https://github.com/huggingface/transformers/tree/master/examples#\nlanguage-generation\n181264 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nusing the PyTorch 1.2.0 framework and Python 3.7.4 on a\nsystem with NVIDIA Tesla M10 GPU.\nD. SAMPLE GENERATION\nThe stories in the test set of the WritingPrompts dataset are\nused as the references for comparing human written text\n(referred as human in our experiments) and auto-generated\nsamples. The stories are generated using the top- k sampling\ntechnique [8], [36] during the generation process. The sam-\nples generated by the ﬁne-tuned models are grouped together\nusing the model names in Section VI-B.\nFor each of the pre-trained language models, two sets\nof experiments are performed to better visualize how\nthe generated instances change with the hyperparameter\ntuning.\n• Varying the softmax temperature parameter t (t =\n{0.25,0.5,0.75,1.0}), while keeping the top-k sampling\nparameter k constant, where k ∈{0, 10,50,250,1000}.\n• Varying the top-k sampling parameter k (k =\n{5,10,50,150,250,500,1000}), while keeping the\nsoftmax temperature parameter t constant, where t ∈\n{0.5,0.75,1.0}.\nWe generate samples by randomly selecting 200 different\nprompts from the test set. To compare performance with\nprevious open-ended story generation literature, each gen-\nerated story is limited to 150 words as mentioned in [8],\n[41]. For each of the selected test prompts, we randomly\nselect a human-written story from the test set and use the\nﬁrst 150 words from the story for comparison during the\nevaluation step.\nE. PILOT STUDY\nOpenAI has made multiple GPT and GPT2 model variants\npublicly available. These language models vary in the number\nof trained parameters, size and number of layers, etc. [28]\nWhile [41] looks at a comparison of a ﬁne-tuned GPT2 small\n(117M) model, there currently exists no literary work which\ncompares all the provided language models - ﬁne-tuned\nor pre-trained. This study looks at four ﬁne-tuned models\ndepending on the GPT2 model parameters and the BPE\ntoken size (see Section VI-B). Also, among the pre-trained\nGPT models provided by OpenAI, not all perform equally\nwell.\nTherefore, we conduct two sets of small scale pilot exper-\niments to select the top two efﬁcient pre-trained models\namong the pre-trained and ﬁne-tuned GPT models. The two\npilot studies are: (a) Pilot Study I: Comparing the pre-trained\nGPT language models; and (b) Pilot Study II: Comparing the\nﬁne-tuned language models.\nFor the set of pilot experiments, we choose to report\nthe model performance using two sets of sub-experiments:\n(a) keeping temperature t =1.0, we vary the value of k =\n{5,10,50,150,250,500,1000}; and (b) keeping sampling\nparameter k = 50, we vary t = {0.25,0.5,0.75,1.0}.\nThe generated sample instances are compared with human\nreferences.\nFIGURE 5. Comparing all models for Bigram-Prompt overlap by varyingk\nat constantt where (a)t = 0.5. (b)t = 0.75. (c)t = 1.0.\nVII. METRIC OVERVIEW, CORRELATION AND RANKING\nWe give a brief overview of the evaluation metrics considered\nhere. An important step is studying their pairwise correlations\nto ﬁlter out redundant metrics during analysis of system\nperformance. Finally, we present a ranking of metrics with\nrespect to their ability to distinguish between human-written\nand machine generated textual content.\nVOLUME 8, 2020 181265\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 6. Comparing all models for Bigram-Prompt overlap by varyingt at constantk where (a)k = 0. (b)k = 10. (c)k = 50. (d)k = 250.\n(e) k = 1000.\nA. METRIC OVERVIEW\nWe divide the set of metrics into ﬁve major groups based on\ntheir domain of evaluation – readability, syntactic style and\ncomplexity, part-of-speech usage, measure of coherence and\nprompt-based conditioning.\n1) PROMPT-BASED CONDITIONING\nThe models must be able to condition well on the given story\nprompt, which means that the text generated must relate to a\ngiven initiating premise. To represent the conditioning capa-\nbility of the model and how well they compare with human\n181266 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 7. Pilot Study I withpre-trained GPT modelsfor L_avg by (a) varying top-ksampling parameter at constant temperaturet = 1.0.\n(b) varying temperaturet at a constant top-k= 50.\nFIGURE 8. Pilot Study II withfine-tuned GPT modelsfor L_avg by (a) varying top-ksampling parameter at constant temperaturet = 1.0.\n(b) varying temperaturet at a constant top-k= 50.\nreferences – we look at n-gram (n ∈{1, 2,3}) word overlap\nbetween the prompt and the generated story with stopword\nelimination. However, these models often repeat terms from\nthe given prompt, which could lead to higher overlap. But the\nmain indicator should be how the generative models perform\nwith respect to the overlap percentage observed in human\nwriting.16\n2) STYLE AND COMPLEXITY\nWhile there is no good way to measure stylistic complexity\nof textual content, an overly complex piece of text can reduce\nreadability while poorly written content demonstrates lack of\nsophistication [41]. Along with considering the mean (L_avg)\nand standard deviation (L_sd) of the sentence length in the\nstories, we study the type token ratio (as percentage, ttr_pc)\n16Here, we assume the human written stories as the best possible reference\nfor ease of comparison.\nto observe how the stylistic quality of the generated text\ncompares to human writing.\n3) READABILITY MEASURES\nReadability metrics are an automatic and easy measurement\nof text difﬁculty [20], [27]. Readability scores like Flesch\nReading Ease (fre) [22] and Dale-Chall Readability score\n(dcr) [32] attempt to quantify the level of difﬁculty of a\ntext with respect to the reader’s education level. While in\nFRE, text complexity is calculated using the average length\nof the sentence as well as presence of polysyllabic words; the\nDCR score takes into account familiarity or knowledge of a\nword while calculating readability. We discuss more in the\nexperimental results section.\n4) MEASURE OF COHERENCE\nIt is important to evaluate the coherence in written content\nto determine whether there is correlation across the units\n(sentence or words). We propose the sentence connectedness\nVOLUME 8, 2020 181267\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 9. Comparing all models for L_avg by varyingk at constantt\nwhere (a)t = 0.5. (b)t = 0.75. (c)t = 1.0.\n(sent_conn) metric to evaluate the cohesion at the sentence\nlevel in textual content. Using Sent2Vec-based embeddings,\nwe transform each sentence in the story to their embed-\nding vectors. To capture the difference in sentence meaning,\nwe calculate the angular variation in consecutive sentence\nvectors in radians. We compute the standard deviation of the\nlist of pairwise angular differences obtained for each adjacent\nsentence pair. Lower the variation, more is the connectedness\nor coherence across the text content.\n5) PART-OF-SPEECH USAGE\nPart-of-Speech (POS) usage can be a useful indicator of\nlinguistic quality. An exploratory analysis of the textual\ncontent in existing research reveals that Noun and Verb\ntags are the most commonly occurring parts of speech [39].\nHence we primarily compare the frequency distributions of\nthese two tags between the synthetic and human written\nreferences.\nB. METRIC CORRELATION\nWe use the mean metric scores on the generated instances\nacross different hyperparameter combinations to compute the\ncorrelation between each pairwise metric. The metrics are\ngiven ranks on the basis of statistical scores based on the met-\nrics’ correlation with the outcome variable (here, ‘label’) as\nwell as each other. We use Pearson’s Correlation Coefﬁcient\n(ρ) for this purpose.\nFigure 1 is a heatmap showing the measure of correlation\namongst the metrics as well as the outcome variable. Note\nthe low level of correlation between the readability metrics\n(fre and dc). As expected, there is a high positive correlation\n(ρ >0.9) among the story-prompt n−gram overlap metrics\n(uniOL, biOL and triOL). Interestingly, these overlap metrics\nhave relatively high positive correlation (ρ > 0.75) with the\nstyle metrics: average sentence length and the distribution of\nnouns in the textual content. Other interesting high positive\ncorrelations are between Dale-Chall Readability score and\nType-Token Ratio, and between average sentence length and\nnoun usage. A high correlation between the Dale-Chall Read-\nability score and Type-token ratio shows that the generated\ncontent has an increased occurence of difﬁcult and unique\nwords. Below, we do regression analysis of these metrics with\nthe outcome variable.\nC. METRIC RANKING\nIn this experiment, we use the metric values for each story\nand the label ‘human’ versus ‘automatic’ for the generated\nsamples. The scores of the evaluation metrics are given to\nthe well-known regression-based LASSO algorithm [49].\nIn this way, we can determine which metrics are bet-\nter at distinguishing between manual and auto-generated\nsamples. LASSO applies a penalty based technique that\ndetermines how many ‘‘features’’ are retained. Additionally,\nusing cross-validation to choose the penalty factor (α ) helps\nimprove model generalizability and select the best ﬁtted\nmodel. Here, we use 5-fold cross-validation with the lin-\near ‘LassoCV’ 17 model provided by Python’s Scikit-Learn\npackage. The absolute value of the correlation coefﬁcient\ndetermines the level of impact of a unit change in the\nknown variable (here, the evaluation metrics) on the estimated\n17https://scikit-learn.org/stable/modules/generated/\nsklearn.linear_model.LassoCV.html\n181268 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 10. Comparing all models for L_avg by varyingt at constantk where (a)k = 0. (b)k = 10. (c)k = 50. (d)k = 250. (e)k = 1000.\nvariable (here, the nature of written content – human or\nmachine) and the sign (positive or negative) of the coefﬁcient\ndetermines the nature of the impact of the variable. The model\niterates over a 100 possible alpha values to select the best α.\nFor this work, the best αis 0.000651 and the best model score\n(coefﬁcient of determination, R2) is 0.840436.\nWe see that the metrics capable of giving the most impor-\ntant information for distinguishing the generated samples\nfrom human references are N−gram overlap percentages\nbetween the story and the prompt, speciﬁcally bigrams and\nunigrams. The bigram overlap shows a strong positive coef-\nﬁcient of correlation while unigram overlap percentage has\nVOLUME 8, 2020 181269\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 11. Pilot Study I withpre-trained GPT modelsfor L_sd by (a) varying top-ksampling parameter at constant temperaturet = 1.0.\n(b) varying temperaturet at a constant top-k= 50.\nFIGURE 12. Pilot Study II withfine-tuned GPT modelsfor L_sd by (a) varying top-ksampling parameter at constant temperaturet = 1.0.\n(b) varying temperaturet at a constant top-k= 50.\na similar but negative correlation with the outcome. Here Y\ndenotes the outcome variable i.e, ‘label’. Dale-Chall read-\nability score and Type-token ratio percentage also appear as\nmetrics with strong coefﬁcients for distinguishing between\nthe human and non-human content. The statistical properties\nof number of words in the textual content at the sentence level\nlike mean sentence length (L_avg) and standard deviation of\nsentence length (L_sd) are also highly correlated with the\nlabel. While mean sentence length has a negative correla-\ntion coefﬁcient, standard deviation has a similar but positive\ncorrelation with the nature of the content. The measure of\ninteraction of the metrics with the outcome variable in terms\nof their coefﬁcients based on the ﬁtted linear Lasso model is\ngiven by Equation 2:\nY =3.219 ∗biOL +(−2.704) ∗uniOL +(−1.26) ∗triOL\n+(−1.018) ∗dc +(−0.979) ∗L_avg\n+(0.887) ∗L_sd +0.574 ∗ttr_pc\n+(−0.088) ∗sent_conn +(−0.069) ∗verb\n+(−0.029 ∗noun) +(−0.023) ∗fre (2)\nAlthough, the model does not predict any metrics with\nzero coefﬁcient, we see from Figure 2, the lowest coefﬁcient\nvalues are assigned to the metrics fre, sent_conn, noun and\nverb frequency distributions. The other metrics with lower\npower of distinguishing between human and non-human\n(generated) writing are the percentages of noun and verb\nusage. While L_avg and L_sd may not be the top ranked\nparameters because of variance in sentence lengths in human\nand machine-written content. Also, while L_sd is positively\ncorrelated to the label (Y ), L_avg has a negative correlation\ncoefﬁcient.\nWe now present the metric-wise performance of the\nmodels. All experiments compare the metric scores of the\ngenerated model samples in two sets of sub-experiments:\n(i) varying k at different constant values of temperature t;\nand (ii) varying t at different constant values of k.\nVIII. PROMPT-BASED CONDITIONING\nConventional models often fail to produce text that is seman-\ntically and contextually related to a given prompt/seed [8].\n181270 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 13. Comparing all models for L_sd by (a) varyingk at t = 0.5\n(b) varyingk at t = 0.75 (c) varyingk at t = 1.0.\nConditioning on the provided starting seed acts as a guide\nfor the generative model by providing some prior context\nfor it to choose the best possible sequence of words and/or\nphrases from the distribution. A measure of sample-prompt\nrelatedness ideally acts as an indicator to how the generative\nlanguage model can ‘stick’ to the context in the given seed.\nA higher score of overlap (close to 100%) can also be due to\nsamples containing only words from the prompt repeatedly.\nSo our targeted level of bigram overlap percentage is 0.9%\nin the human written samples – thus humans writes samples\nwhich reuse words from the given prompt very scarcely.\nUsing the Python NLTK toolkit [25], we look into the\npercentage overlap [41] of uni-, bi-, and tri-grams between the\ngenerated stories and the prompt. The analysis above shows\nthe feature importance of bigram overlap – the percentage\nis a good indicator of how the human references differ from\nhuman samples. We also note from prior correlation analysis\n(Figure 1), that there exists a very strong correlation amongst\nn−gram overlap percentages. In this section, we focus on the\nﬁndings from the bigram overlap percentage results.\nTable 6 show the statistical signiﬁcance of the bigram\noverlap percentage scores of generated text with the human\nreferences. We see that not many pre-trained models can\ngenerate samples which are equivalent in overlap scores with\nthe human references. Next, we look at how these mod-\nels actually perform with different settings of the sampling\nhyperparameters.\nTABLE 6. (k, t) combinations of pre-trained models where Bigram\nOverlap Percentage is statistically similar to human references.\nAmong the pre-trained GPT2 models, the bigram overlap\nfor the models, OG, G2 are the closest to the human level.\nFigures 3a and 3b show a decreasing trend in the overlap\npercentage with an increase in the hyperparameter values\nwith varying k and t. At k values of 250, 500 and 1000 at\nt =1.0, OG and GPT2 are the closest to the human levels.\nThis explains the lower overlap percentages of the models\nfor k = 50 with varying t. A similar trend is shown in\nFigures 4a and 4b for the samples generated by the ﬁne-tuned\nmodels. At t = 1.0, and at values of k greater than 100,\nthe ﬁne-tuned models w10-1M and w5-1M score the clos-\nest to the human bigram overlap percentages. Therefore,\nthe overlap percentages shown in Figure 4b for k =50 are\nnot signiﬁcantly closer to the human references for the model\nsamples.\nTaking the best generative models from the pilot exper-\niments, we look at the trend in the percentage of bigrams\ncommon to the story and the prompt. With changing\nthe sampling parameter k at constant temperature, the\nFigures 5a, 5b and 5c show how the bigram overlap percent-\nage changes. The same for varying softmax temperature t are\nshown in Figures 6a, 6b, 6c, 6d and 6e.\nA higher overlap percentage means that generator is repeat-\ning words from the prompt, as seen in the samples from\nthe pre-trained models at lower values of hyperparameters.\nAmong the pre-trained models, XLNet (XB and XL) and\nVOLUME 8, 2020 181271\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 14. Comparing all models for L_sd by varyingt at constantk where (a)k = 0. (b)k = 10. (c)k = 50. (d)k = 250 (e)k = 1000.\nlarger GPT2 models (GM and GL) generate samples that are\nnot close to the human references. However, the pre-trained\nTX and smaller GPT models OG and G2 generate content\nhaving minimal overlap (approx. 1%) with the story prompts\nat temperature 0.75 and k values greater than 150 (Figure 5b).\nThis is a sign that the models maybe generating unique words.\nKey observations on bigram overlap are:\n• The pre-trained and ﬁne-tuned models generate samples\nwith lower overlap bigram percentages (approx. 1%)\nwith an increasing value of the sampling k and t val-\nues. The percentages are closest to the human level at\nt =0.75,1.0 and k =500,1000.\n181272 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 15. Pilot Study I withpre-trained GPT modelsfor Type Token Ratio (TTR) percentage by (a) varying top-ksampling parameter at\nconstant temperaturet = 1.0. (b) varying temperaturet at a constant top-k= 50.\nFIGURE 16. Pilot Study II withfine-tuned GPT modelsfor Type Token Ratio (TTR) percentage by (a) varying top-ksampling parameter at\nconstant temperaturet = 1.0. (b) varying temperaturet at a constant top-k= 50.\n• The pre-trained models generating the most human-like\nsamples are OG, G2 and TX, and the ﬁne-tuned models\nare w10-1M and w5-1M.\n• Lower values of sampling parameters have higher over-\nlap with the prompt for the pre-trained models. However\nthe samples generated by ﬁne-tuned models at lower\nvalues of sampling parameters (t =0.5 and k =10)\nshow low prompt-overlap percentages, which are very\nsimilar to the overlap scores of human authored text.\nIX. SYNTACTIC STYLE AND COMPLEXITY\nNow, we observe the syntactic quality of generated content\nusing sentence length and type token ratio.\nA. SENTENCE LENGTH\nSentence length has been used in previous research to esti-\nmate the level of syntactic complexity [22], [39], [41]. The\nauthors in [39] consider average sentence length a reliable\nmetric that can capture text genre and overall content read-\nability. Although the feature ranks lower in our metric ranking\nanalysis (Section VII), we include this metric to spot any\ninteresting trends. Table 7a shows that the pre-trained OG\ngenerates samples that are similar to the human written refer-\nences at different values of k at different constant t. The table\nreveals that the ﬁne-tuned w10-3M and w5-3M perform the\nbest compared to human scores.\nFigure 7 shows OG performing similar to human writing –\nthe best results at k =50,t =0.75. The L_avg values for the\nother GPT2 models are signiﬁcantly higher than the human\nlevel, thus showing they generate longer sentences. The\nobservation also supports the statistical signiﬁcance study for\nL_avg. The results of PS-II are shown in Figures 8a and 8b.\nThe L_avg values of the sentences generated by models\nw5-1M, w5-3M and w10-3M are close to human scores. It is\ninteresting to note that at t =1.0 and k =10, the models\nw10-3M and w5-1M have scores exactly equal to the human\nreference. L_avg increases with an increasing k.\nVOLUME 8, 2020 181273\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 17. Comparing all models for Type Token Ratio (TTR) percentage\nby varyingk at constantt where (a)t = 0.5. (b)t = 0.75. (c)t = 1.0.\nWe consider L_avg of the generated content of the top\nmodels from PS-I and PS-II and the samples from XLNet and\nTransformer-XL models at varying combinations of (k ,t).\nFor different constant values of t, the trends with varying\nk are shown in Figures 9a, 9b and 9c. While varying the\ntemperature t, the Figures 10a, 10b, 10c, 10d and 10e show\nTABLE 7. (k, t) combinations of pre-trained and fine-tuned models\nwhere metric is statistically insignificant from human references.\nthe changes in L_avg at constant values of k. We observe\nthat the change in L_avg is uniform with the change in the\ncorresponding sampling parameter in both the cases. The\nmodels that generate samples with L_avg closest to human\nreferences are – ﬁne-tuned w5-1M and w10-3M and pre-\ntrained OG. However, the XLNet models perform poorly\ngenerating sentences with much higher lengths. The best set\nof sampling parameter combination is k =1000 at t =0.75.\nWe further analyze the standard deviation in the sentence\nlength (L_sd) of the generated textual content and how the\nmetric score varies with respect to change in the model\nnature and sampling parameters (t and k). The results of the\npilot studies for the pre-trained and ﬁne-tuned GPT mod-\nels are shown in Figures 11 and 12 respectively. From the\nresults, we choose the pre-trained models – OG and GM and\nﬁne-tuned models – w5-3M and w10-3M as the best models\nfr =or further comparison with the XLNet based models -\nXL, XB and TX. We compare the pre-trained and selected\nﬁne-tuned models based on the standard deviation of the\nlength of generated sentences to that of the human authored\ncontent. The results of the metric scores with varying k at dif-\nferent values of constant t is shown in Figure 13. We observe\nthat the L_sd in general has a more stable trend for all\nthe models except the XLNet-based models. The trends are\nclosest to the human scores at the t =0.75 showing that most\nmodels perform closest to the human level at t =0.5,0.75.\nThere is a deviation in the overall L_sd scores at t =1.0\nwith varying k values. Somewhat similar observations can be\ndrawn from Figure 14 with varying t values at constant k.\nObservations on mean and standard deviation of sentence\nlength are:\n• L_avg does not show a uniform or consistent trend with\nvarying sampling hyperparameters for the GPT2-based\npre-trained models as seen in our PS-I analyses. The\nﬁne-tuned models perform similar to human written\ncontent on this metric.\n• XLNet models underperform on this metric; the best\nmodel is surprisingly OpenAI’s GPT model – the\nsmallest transformer-based GPT variant along with\nGoogle/CMU’s Transformer-XL model.\n• Fine-tuned models perform the best – GPT2-medium\nmodel trained on WritingPrompts-1024 dataset performs\n181274 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 18. Comparing all models for Type Token Ratio (TTR) percentage by varyingt at constantk where (a)k = 0. (b)k = 10. (c)k = 50.\n(d) k = 250. (e)k = 1000.\nthe best (Table 7a) at (k =1000, t =0.75). For the\nmodel, wP_512_117M, the best combination is (k =50,\nt =0.75). Models perform better at moderate tempera-\nture values (0.75) and higher values of k.\nB. TYPE TOKEN RATIO\nType-token ratio (TTR) [39] measures complexity, lexical\nrichness or variety in vocabulary. TTR is the ratio between\nthe total vocabulary 18 or types, and the total number of\nwords or tokens in a given piece of textual content. Higher\nTTR value indicates greater lexical richness of the text.\nSimilar to the other sections, Table 7c reports the statistical\nsigniﬁcance study of TTR for auto-generated samples with\nrespect to human references. The majority of the models\n18number of unique words\nVOLUME 8, 2020 181275\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 19. Pilot Study I withpre-trained GPT modelsfor Flesch Reading Ease (FRE) scores by (a) varying top-ksampling parameter at\nconstant temperaturet = 1.0. (b) varying temperaturet at a constant top-k= 50.\nFIGURE 20. Pilot Study II withfine-tuned GPT modelsfor Flesch Reading Ease (FRE) scores by (a) varying top-ksampling parameter at\nconstant temperaturet = 1.0. (b) varying temperaturet at a constant top-k= 50.\noccur at t =1.0 for different values of k. Content generated\nby pre-trained larger GPT2 based models (GM and GL) and\nTX have scores similar to human references. Among the\nﬁne-tuned models, the top ones are w10-1M and w5-1M.\nWe notice that at a higher k, the samples generated by the\nmodels digress from the human reference score – maybe\ndue to the generation of more unique words than present\nin human writing. The mean TTR percentage recorded by\nhuman writing is 40% for the WritingPrompts dataset.\nThe pilot studies show a comparison of pre-trained\nand ﬁne-tuned GPT2 models for varying k and t\nparameters – the studies reveal the generation of more unique\nwords with an increasing parameter value (k or t). This\nmay indicate that the model is generating random strings of\nunique words. The samples from models OG, GM and GL\nperform closest to the human references at {t =0.75,k =\n50}and {t = 1.0,k = 10}as seen in Figures 15a and\n15b. For PS-II, the TTR values of the ﬁne-tuned models are\nshown in Figures 16a and 16b reveal the same increasing\ntrend. The models w10-1M and w5-1M perform the best at\n(t =0.75, k =50) and (t =1.0, k = {10,0}). For the\ncomprehensive model study, we look at the metric scores\nfor samples generated at different combinations of k and t.\nFigure 17b shows that the pre-trained models OG and GL\ngenerate samples with TTR scores closest to human writing\nalong with the ﬁne-tuned model w10-1M at higher values\nof k.19 For t =1.0, the samples generated by the models\ngenerate human level samples at lower k values of 0 and 10\n(Figure 17c).\nFigure 18a shows the XB model samples have values close\nto human reference level. The models TX, OG and GM\nhave values comparable to humans at t =0.75. A similar\nobservation can be made from Figure 18c.\nKey takeaway points on type-token ratio are:\n• TTR scores of the generated instances are close to the\nhuman level at lower values of k i.e., k = {0,10}at\n19k ={50, 100,150,1000}\n181276 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 21. Comparing all models for Flesch Reading Ease (FRE) scores\nby varyingk at constantt where (a)t = 0.5. (b)t = 0.75. (c)t = 1.0.\ntemperature, t = 1. The increasing, non-converging\ntrend shows that this metric can moderately differentiate\nbetween human and machine generated text.\n• The increasing trend shows that the generative models\n– the ﬁne-tuned and pre-trained GPT models tend to\ngenerate newer words thereby increasing the vocabulary\nTABLE 8. (k, t) combinations of pre-trained models where FRE is\nstatistically insignificant from human references.\nsize. Thus at higher k values, the models tend to gener-\nate more random unique words than present in human\nwriting.\n• The pre-trained models OG, TX and the ﬁne-tuned\nw10-1M shows TTR scores closest to human level with\nvarying k at higher t values (t = {1.0,0.75}). The\nmodels seem to perform better at lower k values with\nrespect to their TTR scores.\n• Varying the temperature t at constant values of k shows\na different trend in the model samples – the pre-trained\nXL and XB perform closer to the human reference levels\nand have a consistent trend with change in t.\nX. READABILITY MEASURES\nWe use Python’s textstat [4] library to calculate the Flesch\nreading ease (fre) [22] and Dale-Chall Readability score\n(dcr) [32], for varying t and k values. We report the models’\nperformance on both set of metrics since there exists a low\ncorrelation between the two metrics.\nA. FLESCH READING EASE\nFor narrowing down the models and the best set of sam-\npling hyperparameter (k and t) combinations, we look at\nthe t-statistic and p-value given by the one-sample T-test of\nstatistical signiﬁcance.20 The aim is to ﬁnd the ﬁne-tuned and\npre-trained model(s) and (k , t) where the generated samples\nhave a mean FRE value that is statistically similar to that of\nthe human written references (approx. 64.7). 21 The results\nof this analysis is shown in Table 8. We observe that the\nFRE metric is not good for differentiating between human\nwriting and synthetic examples, that is consistent with the\nLASSO results in Section VII. A closer look at the evaluation\nexperiments provides a better insight.\nFor the pilot studies, Figures 19a and 19b show the\nresults for PS-I for varying k and t respectively, and\nFigures 20a and 20b show the same for PS-II. Figure 19b\nshows that the model GL generates samples with values com-\nparable to human written counterparts at t =1. Additionally,\nthe FRE scores of the G2 model reﬂected in Figure 19a,\nwith varying k at t = 1, are closer to the human written\nscores. This ﬁgure shows a more stable trend for models\nGM and GL as well. However, we see some sudden drops\nin the score for speciﬁc parameter combinations – GM has\na very low value (3.68096) for k =250 at t =1.0. PS-II\n20https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/\nscipy.stats.ttest_1samp.html\n21https://readabilityformulas.com/ﬂesch-reading-ease-readability-\nformula.php\nVOLUME 8, 2020 181277\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 22. Comparing all models for Flesch Reading Ease (FRE) scores by varyingt at constantk where (a)k = 0. (b)k = 10. (c)k = 50.\n(d) k = 250. (e)k = 1000.\nsheds light on the top performing ﬁne-tuned models using a\nsimilar approach. Figure 20a shows a much smoother trend\nor variability in the FRE values with changing k. The change\nin softmax temperature t at k =50 in Figure 20b, hows a\nsteady increase in the FRE value with the model w10-3M\nbeing the top performer, while w5-3M can be called a second\nbest consistent model based on the trend variability. Again,\nour observations from PS-II can be supported by the results\nshown in Table 8 – where w5-3M and w10-3M – occur most\nfrequently.\nFinally, we combine the selected pre-trained and ﬁne-tuned\nGPT models with the set of pre-trained transformer\n181278 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 23. Pilot Study I withpre-trained GPT modelsfor Dale-Chall Readability (DCR) scores by (a) varying top-ksampling parameter at\nconstant temperaturet = 1.0. (b) varying temperaturet at a constant top-k= 50.\nFIGURE 24. Pilot Study II withfine-tuned GPT modelsfor Dale-Chall Readability (DCR) scores by (a) varying top-ksampling parameter at\nconstant temperaturet = 1.0. (b) varying temperaturet at a constant top-k= 50.\nmodels from Google/CMU - TX and XL and XB. Changing\nthe top-k sampling parameter at different values of constant\nsoftmax temperature t reveal some interesting insights into\nmodel performance with sampling parameter tuning. While\nthe models XB and XL consistently perform poorly in all\nthree scenarios shown in Figures 21a, 21b, the samples\nfrom the pre-trained transfo-xl model achieves a consistent\nFRE scores close to the human references. The ﬁne-tuned\nGPT2 models, w10-3M and w5-3M also generate textual con-\ntent with readability scores similar to human written textual\ncontent at a combination of t = 0.5 and k = 500. But\nthis is expected since both these models have been trained\nadditionally on a preprocessed version of the provided Writ-\ningPrompts dataset. These ﬁne-tuned models along with the\npre-trained TX and G2 (small and medium) also perform\nwell for k ={10, 50,150,250,1000}at t =0.75 as seen\nfrom Figure 21b. For varying t, we observe that the samples\ngenerated at a top-k sampling value of k = 10, are the\nmost consistent and closest to the human reference scores\nfor models OG, TX and XL. But, on an average the worst\nscores (negative FRE) are observed by samples generated by\nthe XB and XL. An interesting observation is that we see\nhigher overlap at t =0.5 and t =0.75 of the sample scores\nwith the human references for the above mentioned.\nFindings from the analysis of FRE scores are:\n• The experiments and statistical signiﬁcance results sup-\nport that samples generated by the pre-trained models\nopenai-gpt, transfo-xl and xlnet-large are closest to the\nhuman samples in FRE scores.\n• The ﬁne-tuned GPT2-medium model trained on the\nWP-1024 (wP_1024_355M) dataset generate samples\nwith a reading ease score similar to the human\nreferences.\n• The models perform better on this metric at top-k val-\nues of k =10,50,1000 and softmax temperatures of\nt =0.75,1.0.\n• Contrary to the previous automated metric study [27],\nFRE does not capture the linguistic quality of the\ngenerated content well. The correlation (Figure 1) and\nthe metric ranking (Figure 2) along with high number\nVOLUME 8, 2020 181279\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 25. Comparing all models for Dale-Chall Readability (DCR) scores\nby varyingk at constantt where (a)t = 0.5. (b)t = 0.75. (c)t = 1.0.\nof statistically insigniﬁcant (Table 8) hits support that\nFlesch Reading Ease is not well suited to distinguish\nbetween generated and human authored content.\nB. DALE-CHALL READABILITY SCORE\nFirst, we perform a test of statistical signiﬁcance using\nthe one-sampled t-test. The results for the pre-trained and\nTABLE 9. (k, t) combinations of pre-trained models where DCR is\nstatistically insignificant from human references.\nﬁne-tuned models are shown in Table 9. We see that among\nthe pre-trained models, TX has generated samples similar in\nDCR scores to human writings. An important observation\nhere is that the ﬁne-tuned models perform the best at t =1\nand k =1000, that is almost the entire length of the provided\nhuman stories used for training.\nFor evaluation, we start with identifying the best\nGPT2 models through Pilot Study I (PS-I) for pre-trained\nand Pilot Study II (PS-II) for ﬁne-tuned GPT2 models.\nFigures 23a and 23b show the results for PS-I – samples\nfrom models G2, GM and GL perform best. However,\nat temperature t =1.0, the scores of the samples at k =50\nin Figure 23a are the closest to the human preferences (little\nless than 6.0) that can also be seen in the variability of the\nvalues in Figure 23b. Although, OG and GL may have statis-\ntically similar scores to human references at particular values\nk and t, a closer look shows more homogeneity in the plots\ngenerated by the models G2 and GM. Figures 24a and 24b\nshow a similar upward trend for PS-II as seen in PS-I with\nan increase in the sampling parameter value depending on\nthe setup. Interestingly, the models w10-1M, w5-3M and\nw10-3M generate the samples that have DCR scores closest\nto human references at k ={250, 500,1000}. The top two\nmodels recording scores similar to the human written stories\nin Figure 24b are w5-3M and w10-3M. This is supported\nby the statistical signiﬁcance values in Table 9, where we\nsee more overlap between the GPT2 models, GL, w5-3M,\nw10-3M and w5-1M, at higher t and k values.\nTaking the top models from PS-I and PS-II, we com-\npare the stories generated by the following models –\ngpt2 and gpt2-medium from PS-I; wP_512_355M and\nwP_1024_355M from PS-II; xlnet-base-cased, xlnet-large-\ncased and transfo-xl with the human references. Like the\nprevious experiments, this analysis also has two sub exper-\niments (a) varying k at different t values as shown in\nFigures 25a, 25b and 25c respectively; and (b) varying t at\ndifferent values of k shown in Figures 26a, 26b, 26c, 26d\nand 26e. Looking at DCR scores with varying k at constant t,\ntranso-xl generates better quality samples scoring closer to\nhuman writing. For the other models, scores are lower when\nthe top-k sampling is done at t = 0.5 and the values\nincrease slightly when t =0.75 as observed from the change\nin gradient of score plot. This increasing gradient is even\nmore apparent in Figure 25c, with the samples generated\nby models gpt2, gpt2-medium. Fine-tuned wP_512_355M\n181280 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 26. Comparing all models for Dale-Chall Readability (DCR) scores by varyingt at constantk where (a)k = 0. (b)k = 10. (c)k = 50.\n(d) k = 250. (e)k = 1000.\nand wP_1024_355M score close to human references. For\nk = 1000, the ﬁne-tuned models generate samples with\nscores closer to human references at t =1.0. The best results\nare observed at k =250 for the following models – gpt2,\ngpt2-medium and ﬁne-tuned wP_512_355M and wP_1024_\n355M – which additionally perform well in terms of generat-\ning samples as readable as the human references.\nKey takeaway points for this metric are below:\n• The overlap of the metric scores at higher values of tem-\nperature and sampling value k. For k =1000, the ﬁne-\ntuned models generate samples with scores closer to\nhuman references at t =1.\n• The best results are observed at k = 250 for the\nfollowing models – gpt2, gpt2-medium and ﬁne-tuned\nVOLUME 8, 2020 181281\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 27. Pilot Study I withpre-trained GPT modelsfor sentence connectedness by (a) varying top-ksampling parameter at constant\ntemperature t = 1.0. (b) varying temperaturet at a constant top-k= 50.\nFIGURE 28. Pilot Study II withfine-tuned GPT modelsfor sentence connectedness by (a) varying top-ksampling parameter at constant\ntemperature t = 1.0. (b) varying temperaturet at a constant top-k= 50.\nwP_512_355M and wP_1024_355M – which addition-\nally perform well in terms of generating samples as\nreadable as the human references. The model transo-xl\nalso perform well in this set of evaluation.\n• The best combinations are t ∈ {1.0,0.75}and k ∈\n{250,500,1000}. This is corroborated with the evi-\ndence of statistical signiﬁcance from the experiments\nin Table 9.\n• DCR is a good metric for differentiating between the\nhuman written and machine written samples.\nXI. SENTENCE CONNECTEDNESS\nOne important aspect of evaluating generated content with\nrespect to the human written counterparts is looking at the\ncoherence. We propose a metric to measure the connected-\nness of sentences. Sentence embeddings capture the semantic\nnature and overall collective dependency between the words\nas a whole much better than word-based embeddings like\nWord2Vec [26] and GloVe [30]. Therefore, each sentence in\nthe story along with the provided prompt is converted into a\nsentence-based embedding vector [29].\nWe use the Sent2Vec Python library22 proposed in [29] and\nthe pre-trained Wikipedia Bigram model23 for the 700 dimen-\nsional sentence vectors. For each story, we compute the angle\nbetween each pair of consecutive sentences. The angular\ndifference between two consecutive equi-length (700 dimen-\nsional) sentence vectors is calculated in radians. Finally,\nto capture the variation in the angular difference, we com-\npute the standard deviation of the list of pairwise angular\ndifferences obtained for each adjacent sentence pair. The\ndeviation in the sentence connectedness values in human\nwriting is close to 0.21. We hypothesize that a greater value of\nstandard deviation in the angular difference is an indicator of\ngreater variability in the sentence coherency. The statistical\n22https://github.com/epfml/sent2vec\n23https://github.com/epfml/sent2vec#downloading-sent2vec-pre-trained-\nmodels\n181282 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 29. Comparing all models for Sentence Connectedness by varying\nk at constantt where (a)t = 0.5. (b)t = 0.75. (c)t = 1.0.\nsigniﬁcance study conducted for sentence connectedness\nmeasure is reported in Table 10.\nFrom pilot studies for the different GPT2 variants\n(pre-trained and ﬁne-tuned models), we observe that at vary-\ning values of k at t = 0.75, the pre-trained model GM\nand GL show a change in coherence similar to human\nTABLE 10. (k, t) combinations of pre-trained models where Sentence\nConnectedness is statistically similar to human references.\nwriting at the sentence level. Similarly for the ﬁne-tuned\nmodels, w5-3M and w10-3M are the ones generating con-\ntent similar to humans. The trend can be observed in\nthe Figures 27a and 27b for the pre-trained models and\nFigures 28a and 28b for the ﬁne-tuned models. Selecting the\ntop models, we look at the generated textual content at\ndifferent combinations of the sampling hyperparameters k\nand t. The ﬁgures in this section show that the variation in\nthe sentence connectedness values are closest to the human\nlevels at the k values greater than 100 and t =0.75. The\nbest performing models are the selected ﬁne-tuned models\n(w5-3M and w10-3M) and GM.\nObservations on sentence connectedness are below:\n• Samples generated by the pretrained model gpt2-\nmedium and the ﬁne-tuned wP_512_355M and\nwP_1024_355M at softmax temperature, t =0.75 score\nare closest to the human content.\n• The samples generated by the top models at the top-k\nsampling values greater than 100, i.e. at 150, 250,\n500, have variation in sentence connectedness closest to\nhuman written references.\n• Changing k at constant t generates samples that do\nnot ﬂuctuate on coherence values. The gradient of the\nvariation is more consistent for t =0.75 and shows a\nmore downward trend with t =1.\n• The connectedness scores for the XLNet based mod-\nels show an upward trend with changing temperature\nat constant k values while other models show a more\ndownward trend.\nXII. PART-OF-SPEECH USAGE\nThe distribution of parts-of-speech (POS) tags in textual con-\ntent provides information like similarity in authorship, rarity\nof word usage and originality of POS tags. Here, we report the\npercentage of Noun and Verb tags that appear in the generated\ntext with human stories acting as the baseline. The Spacy POS\ntagger24 for Python was used for tagging purposes.\nA. VERB USAGE DISTRIBUTION\nWe ﬁrst discuss the statistical signiﬁcance of the verb tag\ndistribution in the generated content to the human references\nas shown in Table 11. The table shows that the samples\nfrom the pre-trained XL and OG and ﬁne-tuned w10-1M and\nw5-3M models have verb distributions statistically similar\nto human references at different combinations of k and t.\n24https://spacy.io/usage/linguistic-features\nVOLUME 8, 2020 181283\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 30. Comparing all models for Sentence Connectedness by varyingt at constantk where (a)k = 0. (b)k = 10. (c)k = 50. (d)k = 250.\n(e) k = 1000.\nWe perform the set of pilot studies to select the best perform-\ning pre-trained (PS-I) and ﬁne-tuned (PS-II) GPT2 models.\nThe results of the pre-trained GPT models for PS-I\nare shown in Figures 31a and 31b for constant t and k\nrespectively. The evaluation also reveals that openai-gpt and\ngpt2-large models have a more uniform plot with respect\nto the human baseline although the softmax temperature\nt = 1.0 is not suitable for the openai-gpt model. For the\nﬁne-tuned models in PS-II, the models w10-1M and w5-3M\nare the top performers with samples generated being consis-\ntently closer to the scores of human references as seen from\nFigures 32a and 32b. The best k values were 150, 250 and\n181284 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 31. Pilot Study I withpre-trained GPT modelsfor Verb usage by (a) varying top-ksampling parameter at constant temperature\nt = 1.0. (b) varying temperaturet at a constant top-k= 50.\nFIGURE 32. Pilot Study II withfine-tuned GPT modelsfor Verb usage by (a) varying top-ksampling parameter at constant temperaturet = 1.0.\n(b) varying temperaturet at a constant top-k= 50.\n500 at t =1.0, while temperature values are 0.75 and 1.0.\nFor PS-II, we see that that the usage has a decreasing trend\nwith an increasing value of k sampling parameter as shown\nby Figure 32a. The trend is reversed in Figure 32b as at\nk =50, the increase in the temperature sees an increased\nverb frequency.\nThe analysis using the best models from pilot stud-\nies and the additional XLNet and Transformer-XL pre-\ntrained models, are shown in Figures 33a, 33b and 33c\nfor varying k at constant values of t. The verb usage\ndistribution for the chosen GPT2-based ﬁne-tuned mod-\nels resemble the human baseline at t = 0.75. The XB\nand XL models also generate samples that have verb tag\ndistributions closer to human baseline. The results of the\nexperiments by varying t at constant values of k are shown\nin Figures 34a, 34d, 34b, 34c, and 34e.\nOur observations on Verb usage are:\n• We see a decreasing trend in the occurrence of Verbs\nwith an increasing k value at t =1.0 for all the models.\nAt t = 1.0 and k = 150, the models G2, GM and\nthe ﬁne-tuned w10-1M and w5-3M generate verbs at\na rate similar to human writing references. The rate of\nverb usage is consistent with the changing values of k at\nt =0.75 for all the models apart from TX (Figure 33a).\n• Varying t at a constant value of k shows that the\npre-trained models XL and ﬁne-tuned w10-1M generate\ntext with verb distributions similar to human references\nat t ={0.5, 0.75}.\n• The XLNet based models and the ﬁne-tuned GPT2 mod-\nels perform similar to human references on this\nmetric. However, the statistical signiﬁcance results\nand empirical evaluation reveal that verb usage in\ngenerated content is similar to human references. There-\nfore, this metric cannot differentiate between human and\nmachine-generated writing.\nB. NOUN USAGE DISTRIBUTION\nNow we look at noun tag frequency in generated content as\ncompared with the human references. As seen above, metric\nvalues change with sampling parameter values. Therefore,\nVOLUME 8, 2020 181285\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nTABLE 11. (k, t) combinations of pre-trained and fine-tuned models\nwhere metric is statistically insignificant from human references.\nwe consider different combinations of sampling hyperparam-\neters k and t.\nOur observations on the statistical signiﬁcance of the\nmodels for Noun tag distribution are in Table 11. It shows\nthat the GPT-based models largely generate samples that\nare similar in scores to the human reference scores at\nhigher values of softmax temperatures t ∈ {0.75,1.0}.\nThe ﬁne-tuned models generate better samples at t =1.0\nat higher sampling value of k at 250 and 1000. We now\nstudy how these signiﬁcance results compare with model\nperformance at different temperatures and top-k sampling\ncombinations.\nFor PS-I, we see how the frequency of the nouns\ngenerated by the pre-trained GPT2 models varies with\nchange in sampling parameters - t and k as shown in\nFigures 35a and 35b respectively. The variation of the same\nin the samples generated by the ﬁne-tuned models are shown\nin Figures 36a and 36b for varying k and t respectively.\nA summary of the results comparing across samples gen-\nerated across a comprehensive set of models is shown in the\nfollowing experiments. Figures 37c, 37a and 37b show the\nchanges in the metric with varying k at different values of\nconstant t. The changes with varying t at constant values of\nk are shown in Figures 38a, 38b, 38c, 38d and 38a.\nKey takeaways on Noun tag distributions are:\n• Samples generated by pre-trained G2 and GM have\nNoun tag distributions similar to human references at\nvarying k values with constant t values.\n• Fine-tuned models generate more nouns with an increase\nin temperature t =1.0 and higher values of sampling k.\nAt lower t values, the ﬁne-tuned models generate fewer\nnouns than found in human writing.\n• The generated samples tend to have more nouns than\nhuman references – the trend in the noun usage is almost\nconsistent with changing hyperparameter combinations\nthus providing little information on how to differentiate\nFIGURE 33. Comparing all models for Verb usage by varyingk at\nconstant t where (a)t = 0.5. (b)t = 0.75. (c)t = 1.0.\nthem from human writing. Hence, noun usage is one of\nthe lower ranked metrics as seen above.\nXIII. MODEL RANKING\nIn the above experiments, we observed how the metric\nscores of the generated content compare with those of the\n181286 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 34. Comparing all models for Verb usage by varyingt at constantk where (a)k = 0. (b)k = 10. (c)k = 50. (d)k = 250. (e)k = 1000.\nhuman-authored content. The most striking inference is that\nno model is close to human scores for every metric. For exam-\nple, we observe that the TTR scores, on the text generated by\nthe pre-trained models XLNet-large and XLNet-base models\nwith varying t (Figure 18), change notably across different\nvalues of constant k. This difference in trends among the\nmodels can be seen across varying sampling hyperparameters\nas well as metrics.\nThus to study model performance and stability, we com-\npute two sets of deviation-based metrics – (a) TotalSD:\nThe total standard deviation from the mean performance of\nthe models across all combinations of metrics and sampling\nVOLUME 8, 2020 181287\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 35. Pilot Study I withpre-trained GPT modelsfor Noun usage by (a) varying top-ksampling parameter at constant temperature\nt = 1.0. (b) varying temperaturet at a constant top-k= 50.\nFIGURE 36. Pilot Study II withfine-tuned GPT modelsfor Noun usage by (a) varying top-ksampling parameter at constant temperature\nt = 1.0. (b) varying temperaturet at a constant top-k= 50.\nTABLE 12. Ranking of pre-trained and fine-tuned language models based\non deviation-based metrics.\nparameters; and (b) TotalDevGH: The total absolute devi-\nation of the mean performance score of the model from the\nhuman score across all combinations of metrics and sampling\nparameters. We rank the models based on the above two\nTABLE 13. An example showing a prompt and a generated story using\nthe pre-trained gpt2-mediummodel atk = 250 andt = 1.0.\nmetrics in the following Table 12, starting with the model that\nhas the lowest TotalDevGH score.\nWe also calculate the Pearson’s product-moment correla-\ntion coefﬁcient25 (ρ) to study the agreement between the two\nmetrics reported in this section. The ρvalue is 0.93, showing\n25https://revisionmaths.com/advanced-level-maths-\nrevision/statistics/product-moment-correlation-coefﬁcient\n181288 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 37. Comparing all models for Noun usage by varyingk at constantt where (a)t = 0.75. (b)t = 0.75. (c)t = 1.0.\nTABLE 14. An example showing a prompt and a generated story using\nthe fine-tuned wP_512_355Mmodel atk = 150 andt = 1.0.\nTABLE 15. An example showing a prompt and a generated story using\nthe fine-tuned wP_1024_355Mmodel atk = 250 andt = 0.75.\na strong positive correlation between the two sets of ranking\nmetrics. The table shows that the ﬁne-tuned GPT2 models\nand the Transformer-XL models are the ones reporting lowest\ndeviation in the metrics. The XLNet models have a high value\nof deviation, that agrees with our inference from the previous\nexperiments specially the model scores from the readability\nmetrics - FRE and DCR in Section‘X-A.\nXIV. CONCLUSION\nIn this paper, we apply the large pre-trained deep neu-\nral language models such as OpenAI’s GPT and GPT2,\nGoogle/CMU’s Transformer-XL and XLNet to open-ended\nstory generation and test their generalizability. We also\ncompare performance of the ﬁne-tuned GPT2 models\n(GPT2-117M and GPT2-355M) on two BPE-token based\nsubsets of the WritingPrompts dataset.\nUsing a variety of automated metrics that measure linguis-\ntic, syntactic and semantic quality of the generated stories,\nthe pre-trained and ﬁne-tuned models are evaluated by com-\nparing with human-written stories. Moreover, we also analyze\nthe metrics with two techniques – a LASSO-based regression\nmodel and inter-metric correlation. In the exploratory analy-\nsis of metric importance, we see that the bigram based overlap\nmeasure is the best performing followed by the standard\ndeviation in sentence length (L_sd).\nVOLUME 8, 2020 181289\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nFIGURE 38. Comparing all models for Noun usage by varyingt at constantk where (a)k = 0. (b)k = 10. (c)k = 50. (d)k = 250. (e)k = 1000.\nInterestingly, retraining the model on a subset of the\ndomain speciﬁc data enhances model performance. In our\nstudy, a model performs the best on a metric when the scores\nof the generated stories on that metric are statistically similar\nto that of human references. However, sampling parameters\nplay an important role in text generation – higher values\nof softmax temperature t (>0.5) and mid-range values of\nk (50 < k < 500) are the best parameter combinations.\nThe top models are the pre-trained OpenAI-GPT-110M,\nGPT2-medium and Transformer-XL models along with\nGPT2-medium and GPT2-small models trained on Writing-\nPrompts-1024 subset data.\n181290 VOLUME 8, 2020\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\nAPPENDIX\nWe provide some examples of the text generated by the\npre-trained and ﬁne-tuned models at different combinations\nof sampling parameters k and t.\nThese examples are provided in Tables 13, 14 and 15.\nACKNOWLEDGMENT\nThe authors thank the reviewers for their insightful com-\nments, that helped us improve the paper.\nREFERENCES\n[1] A. Rusia (2019), XLNet Speaks. Comparison With GPT-2.\nAccessed: Oct. 20, 2019. [Online]. Available: https://medium.\ncom/amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e\n[2] S. Baki, R. Verma, A. Mukherjee, and O. Gnawali, ‘‘Scaling and effec-\ntiveness of email masquerade attacks: Exploiting natural language gen-\neration,’’ in Proc. ACM Asia Conf. Comput. Commun. Secur., Apr. 2017,\npp. 469–482.\n[3] S. Bangalore, O. Rambow, and S. Whittaker, ‘‘Evaluation metrics for\ngeneration,’’ in Proc. 1st Int. Conf. Natural Lang. Gener. (INLG) , vol. 14.\nStroudsberg, PA, USA: Association for Computational Linguistics, 2000,\npp. 1–8. [Online]. Available: https://www.aclweb.org/portal/\n[4] S. Bansal and C. Aggarwal. (2019). TextStat. Accessed: Oct. 20, 2019.\n[Online]. Available: https://pypi.org/project/textstat/\n[5] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhut-\ndinov, ‘‘Transformer-XL: Attentive language models beyond a ﬁxed-\nlength context,’’ 2019, arXiv:1901.02860. [Online]. Available: http://\narxiv.org/abs/1901.02860\n[6] A. Das and R. Verma, ‘‘Automated email generation for targeted\nattacks using natural language,’’ in Proc. 11th Int. Conf. Lang.\nResour. Eval. (LREC). Paris, France: European Language Resources\nAssociation (ELRA), 2018, pp. 1–8. [Online]. Available: http://lrec-\nconf.org/workshops/lrec2018/W32/summaries/4_W32.html\n[7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805. [Online]. Available: http://arxiv.org/abs/1810.04805\n[8] A. Fan, M. Lewis, and Y. Dauphin, ‘‘Hierarchical neural story\ngeneration,’’ 2018, arXiv:1805.04833. [Online]. Available: http://\narxiv.org/abs/1805.04833\n[9] W. Fedus, I. Goodfellow, and A. M. Dai, ‘‘MaskGAN: Better text gen-\neration via ﬁlling in the_,’’ 2018, arXiv:1801.07736. [Online]. Available:\nhttp://arxiv.org/abs/1801.07736\n[10] X. Feng, M. Liu, J. Liu, B. Qin, Y. Sun, and T. Liu, ‘‘Topic-to-essay\ngeneration with neural networks,’’ in Proc. 27th Int. Joint Conf. Artif.\nIntell., Jul. 2018, pp. 4078–4084.\n[11] D. Gavrilov, P. Kalaidin, and V. Malykh, ‘‘Self-attentive model for headline\ngeneration,’’ in Proc. Eur. Conf. Inf. Retr. Cham, Switzerland: Springer,\n2019, pp. 87–93.\n[12] M. Ghazvininejad, X. Shi, Y. Choi, and K. Knight, ‘‘Generating topi-\ncal poetry,’’ in Proc. Conf. Empirical Methods Natural Lang. Process.,\nNov. 2016, pp. 1183–1191.\n[13] A. Giaretta and N. Dragoni, ‘‘Community targeted phishing: A mid-\ndle ground between massive and spear phishing through natural\nlanguage generation,’’ 2017, arXiv:1708.07342. [Online]. Available:\nhttp://arxiv.org/abs/1708.07342\n[14] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep Learning,\nvol. 1. Cambridge, MA, USA: MIT Press, 2016.\n[15] A. Graves, ‘‘Generating sequences with recurrent neural networks,’’ 2013,\narXiv:1308.0850. [Online]. Available: http://arxiv.org/abs/1308.0850\n[16] L. Han, A. L. Kashyap, T. Finin, J. Mayﬁeld, and J. Weese,\n‘‘UMBC_EBIQUITY-CORE: Semantic textual similarity systems,’’ in\nProc. 2nd Joint Conf. Lexical Comput. Semantics, Main Conf. Shared Task,\nSemantic Textual Similarity (SEM), vol. 1. Atlanta, GA, USA: Association\nfor Computational Linguistics, Jun. 2013, pp. 44–52. [Online]. Available:\nhttps://www.aclweb.org/anthology/S13-1005\n[17] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, ‘‘The curious case\nof neural text degeneration,’’ 2019, arXiv:1904.09751. [Online]. Available:\nhttp://arxiv.org/abs/1904.09751\n[18] D. Ippolito, D. Duckworth, C. Callison-Burch, and D. Eck, ‘‘Automatic\ndetection of generated text is easiest when humans are fooled,’’ 2019,\narXiv:1911.00650. [Online]. Available: http://arxiv.org/abs/1911.00650\n[19] G. Jagfeld, S. Jenne, and N. Thang Vu, ‘‘Sequence-to-Sequence models for\ndata-to-text natural language generation: Word- vs. character-based pro-\ncessing and output diversity,’’ 2018, arXiv:1810.04864. [Online]. Avail-\nable: http://arxiv.org/abs/1810.04864\n[20] S. Karimi, L. Moraes, A. Das, A. Shakery, and R. Verma, ‘‘Citance-based\nretrieval and summarization using IR and machine learning,’’ Scientomet-\nrics, vol. 116, no. 2, pp. 1331–1366, Aug. 2018.\n[21] C. Kiddon, L. Zettlemoyer, and Y. Choi, ‘‘Globally coherent text gener-\nation with neural checklist models,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., 2016, pp. 329–339.\n[22] J. P. Kincaid, R. P. Fishburne, Jr., R. L. Rogers, and B. S. Chissom,\n‘‘Derivation of new readability formulas (automated readability index, fog\ncount and ﬂesch reading ease formula) for navy enlisted personnel,’’ Naval\nTechnical Training Command Millington TN Research Branch, Tech.\nRep., 1975. [Online]. Available: https://stars.library.ucf.edu/istlibrary/56/\n[23] J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and D. Jurafsky, ‘‘Deep\nreinforcement learning for dialogue generation,’’ 2016, arXiv:1606.01541.\n[Online]. Available: http://arxiv.org/abs/1606.01541\n[24] C. Liu, S. He, K. Liu, and J. Zhao, ‘‘Curriculum learning for natural\nanswer generation,’’ in Proc. 27th Int. Joint Conf. Artif. Intell., Jul. 2018,\npp. 4223–4229.\n[25] E. Loper and S. Bird, ‘‘NLTK: The natural language toolkit,’’ 2002,\narXiv:cs/0205028. [Online]. Available: https://arxiv.org/abs/cs/0205028\n[26] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, ‘‘Distributed\nrepresentations of words and phrases and their compositionality,’’ in Proc.\nAdv. Neural Inf. Process. Syst., 2013, pp. 3111–3119.\n[27] J. Novikova, O. Dušek, A. Cercas Curry, and V. Rieser, ‘‘Why we need\nnew evaluation metrics for NLG,’’ 2017, arXiv:1707.06875. [Online].\nAvailable: http://arxiv.org/abs/1707.06875\n[28] OpenAI. (2019). Better Language Models and Their Implications.\nAccessed: Oct. 20, 2019. [Online]. Available: https://openai.\ncom/blog/better-language-models/\n[29] M. Pagliardini, P. Gupta, and M. Jaggi, ‘‘Unsupervised learning of sen-\ntence embeddings using compositional n-Gram features,’’ in Proc. Conf.\nNorth Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Tech-\nnol. (Long Papers), vol. 1. New Orleans, LA, USA: Association for\nComputational Linguistics, Jun. 2018, pp. 528–540. [Online]. Available:\nhttps://www.aclweb.org/anthology/N18-1049\n[30] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2014, pp. 1532–1543.\n[31] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, ‘‘Deep contextualized word representations,’’ 2018,\narXiv:1802.05365. [Online]. Available: http://arxiv.org/abs/1802.05365\n[32] R. D. Powers, W. A. Sumner, and B. E. Kearl, ‘‘A recalculation of four\nadult readability formulas,’’ J. Educ. Psychol., vol. 49, no. 2, p. 99, 1958.\n[33] O. Press, A. Bar, B. Bogin, J. Berant, and L. Wolf, ‘‘Language\ngeneration with recurrent generative adversarial networks without\npre-training,’’ 2017, arXiv:1706.01399. [Online]. Available:\nhttp://arxiv.org/abs/1706.01399\n[34] Q. Qian, M. Huang, H. Zhao, J. Xu, and X. Zhu, ‘‘Assigning Personal-\nity/Proﬁle to a chatting machine for coherent conversation generation,’’ in\nProc. 27th Int. Joint Conf. Artif. Intell., Jul. 2018, pp. 4279–4285.\n[35] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. (2018).\nImproving Language Understanding by Generative Pre-Training.\n[Online]. Available: https://s3-us-west-2.amazonaws.com/openai-assets/\nresearchcovers/languageunsupervised/languageUnderstandpaper.pdf\n[36] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n‘‘Language models are unsupervised multitask learners,’’ OpenAI Blog ,\nvol. 1, no. 8, p. 9, 2019.\n[37] E. Reiter and A. Belz, ‘‘An investigation into the validity of some metrics\nfor automatically evaluating natural language generation systems,’’ Com-\nput. Linguistics, vol. 35, no. 4, pp. 529–558, Dec. 2009.\n[38] A. Roberts, J. Engel, C. Raffel, C. Hawthorne, and D. Eck, ‘‘A hierarchical\nlatent vector model for learning long-term structure in music,’’ 2018,\narXiv:1803.05428. [Online]. Available: http://arxiv.org/abs/1803.05428\n[39] M. Roemmele, A. S. Gordon, and R. Swanson, ‘‘Evaluating story gen-\neration systems using automated linguistic analyses,’’ in Proc. Workshop\nMach. Learn. Creativity (SIGKDD), 2017, pp. 13–17.\n[40] D. Scott and D. Hardcastle, ‘‘Can we evaluate the quality of generated\ntext?’’ in Proc. LREC, May 2008, pp. 1–8.\n[41] A. See, A. Pappu, R. Saxena, A. Yerukola, and C. D. Manning, ‘‘Do\nmassively pretrained language models make better storytellers?’’ 2019,\narXiv:1909.10705. [Online]. Available: http://arxiv.org/abs/1909.10705\nVOLUME 8, 2020 181291\nA. Das, R. M. Verma: Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics\n[42] R. Sennrich, B. Haddow, and A. Birch, ‘‘Neural machine translation of rare\nwords with subword units,’’ in Proc. 54th Annu. Meeting Assoc. Comput.\nLinguistics (Long Papers), vol. 1. Berlin, Germany: Association for Com-\nputational Linguistics, Aug. 2016, pp. 1715–1725. [Online]. Available:\nhttps://www.aclweb.org/anthology/P16-1162\n[43] J. Seymour and P. Tully, ‘‘Weaponizing data science for social engi-\nneering: Automated E2E spear phishing on Twitter,’’ Black Hat,\nSan Francisco, CA, USA, Tech. Rep., 2016, p. 37. [Online]. Available:\nhttps://www.blackhat.com/html/contact.html\n[44] D. Shen, A. Celikyilmaz, Y. Zhang, L. Chen, X. Wang, J. Gao, and\nL. Carin, ‘‘Towards generating long and coherent text with multi-level\nlatent variable models,’’ 2019, arXiv:1902.00154. [Online]. Available:\nhttp://arxiv.org/abs/1902.00154\n[45] J. Shropshire, ‘‘Natural language processing as a weapon,’’ in Proc. 13th\nPre-ICIS Workshop Inf. Secur. Privacy, vol. 1, 2018. [Online]. Available:\nhttps://aisel.aisnet.org/wisp2018/26/\n[46] A. Sriram, H. Jun, S. Satheesh, and A. Coates, ‘‘Cold fusion:\nTraining Seq2Seq models together with language models,’’ 2017,\narXiv:1708.06426. [Online]. Available: http://arxiv.org/abs/1708.06426\n[47] I. Sutskever, J. Martens, and G. E. Hinton, ‘‘Generating text with recurrent\nneural networks,’’ in Proc. 28th Int. Conf. Mach. Learn. (ICML), 2011,\npp. 1017–1024.\n[48] I. Sutskever, O. Vinyals, and Q. V. Le, ‘‘Sequence to sequence learning\nwith neural networks,’’ in Proc. Adv. Neural Inf. Process. Syst., 2014,\npp. 3104–3112.\n[49] R. Tibshirani, ‘‘Regression shrinkage and selection via the lasso,’’ J. Roy.\nStat. Soc. B, Methodol., vol. 58, no. 1, pp. 267–288, Jan. 1996.\n[50] C. van der Lee, A. Gatt, E. van Miltenburg, S. Wubben, and E. Krahmer,\n‘‘Best practices for the human evaluation of automatically generated text,’’\nin Proc. 12th Int. Conf. Natural Lang. Gener., 2019, pp. 355–368.\n[51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[52] S. Wiseman, S. M. Shieber, and A. M. Rush, ‘‘Learning neural tem-\nplates for text generation,’’ 2018, arXiv:1808.10122. [Online]. Available:\nhttp://arxiv.org/abs/1808.10122\n[53] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz, and J. Brew, ‘‘Transformers: State-of-\nthe-art natural language processing,’’ 2019, arXiv:1910.03771. [Online].\nAvailable: https://arxiv.org/abs/1910.03771\n[54] S. Xie, R. Rastogi, and M. Chang, ‘‘Deep poetry: Word-level and\ncharacter-level language models for shakespearean sonnet generation,’’\nNatural Lang. Process. Deep Learn., 2017. [Online]. Available: https://\nwww.semanticscholar.org/paper/Deep-Poetry-%3A-Word-Level-and-\nCharacter-Level-Models-Xie/a17379f2236d47cd42f874c23b441f078e7a9\necf\n[55] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and\nQ. V. Le, ‘‘XLNet: Generalized autoregressive pretraining for lan-\nguage understanding,’’ 2019, arXiv:1906.08237. [Online]. Available:\nhttp://arxiv.org/abs/1906.08237\n[56] Y. Yao, B. Viswanath, J. Cryan, H. Zheng, and B. Y. Zhao, ‘‘Auto-\nmated crowdturﬁng attacks and defenses in online review systems,’’ 2017,\narXiv:1708.08151. [Online]. Available: http://arxiv.org/abs/1708.08151\n[57] X. Yi, M. Sun, R. Li, and Z. Yang, ‘‘Chinese poetry generation with a\nworking memory model,’’ 2018, arXiv:1809.04306. [Online]. Available:\nhttp://arxiv.org/abs/1809.04306\n[58] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, and\nY. Choi, ‘‘Defending against neural fake news,’’ 2019, arXiv:1905.12616.\n[Online]. Available: http://arxiv.org/abs/1905.12616\n[59] H. Zhang, Y. Lan, J. Guo, J. Xu, and X. Cheng, ‘‘Reinforcing coherence\nfor sequence to sequence model in dialogue generation,’’ in Proc. 27th Int.\nJoint Conf. Artif. Intell., Jul. 2018, pp. 4567–4573.\n[60] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, ‘‘BERTScore:\nEvaluating text generation with BERT,’’ 2019, arXiv:1904.09675.\n[Online]. Available: http://arxiv.org/abs/1904.09675\nAVISHA DAS received the B.Tech. degree in\nelectronics and communication engineering from\nthe West Bengal University of Technology, West\nBengal, India. She is currently pursuing the Ph.D.\ndegree with the Department of Computer Sci-\nence, University of Houston. Her research interests\ninclude natural language understanding and gener-\nation, cybersecurity, and information retrieval.\nRAKESH M. VERMA (Member, IEEE) is\ncurrently a Professor of computer science with\nthe University of Houston (UH), where he is\nleading a research group that applies reasoning\nand data science to cybersecurity challenges. He is\nthe coauthor of Cybersecurity Analytics (CRC\nPress, 2019), which discusses key data analy-\nsis techniques and their adaptations for cyberse-\ncurity challenges. He has co-organized the 1st\nAnti-phishing Shared Task, in 2018, with proceed-\nings in the CEUR workshop series; the training dataset is publicly available\nfor academic research upon request. Since 2015, he has been co-organizing\nand editing the proceedings of the ACM International Workshop on Security\nand Privacy Analytics. He is an Editor of Frontiers of Big Data in the\nCybersecurity Area, an ACM Distinguished Speaker, from 2011 to 2018,\nand the winner of two Best Paper Awards. He received the 2013 Lifetime\nMentoring Award from the University of Houston. He is a Fulbright Senior\nSpecialist in computer science, a Panel Chair of ACM CODASPY 2020, and\na Technical Program Committee Co-chair of ACM CODASPY 2021.\n181292 VOLUME 8, 2020"
}