{
  "title": "Diagnostic accuracy of a large language model in rheumatology: comparison of physician and ChatGPT-4",
  "url": "https://openalex.org/W4386998832",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2134178283",
      "name": "Martin Krusche",
      "affiliations": [
        "University Medical Center Hamburg-Eppendorf",
        "Universität Hamburg"
      ]
    },
    {
      "id": null,
      "name": "Johnna Callhoff",
      "affiliations": [
        "German Rheumatism Research Centre",
        "Charité - Universitätsmedizin Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A2803914004",
      "name": "Johannes Knitza",
      "affiliations": [
        "Université Grenoble Alpes",
        "Universitätsklinikum Gießen und Marburg"
      ]
    },
    {
      "id": "https://openalex.org/A2912249676",
      "name": "Nikolas Ruffer",
      "affiliations": [
        "University Medical Center Hamburg-Eppendorf",
        "Universität Hamburg"
      ]
    },
    {
      "id": "https://openalex.org/A2134178283",
      "name": "Martin Krusche",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Johnna Callhoff",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2803914004",
      "name": "Johannes Knitza",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2912249676",
      "name": "Nikolas Ruffer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970234096",
    "https://openalex.org/W4223937742",
    "https://openalex.org/W4304003161",
    "https://openalex.org/W3155149157",
    "https://openalex.org/W4296295603",
    "https://openalex.org/W4367310045",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4364378939",
    "https://openalex.org/W4364374804",
    "https://openalex.org/W4384626331",
    "https://openalex.org/W4380730209",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4224944609",
    "https://openalex.org/W4317853296"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)1 3\nRheumatology International (2024) 44:303–306 \nhttps://doi.org/10.1007/s00296-023-05464-6\nOBSERVATIONAL RESEARCH\nDiagnostic accuracy of a large language model in rheumatology: \ncomparison of physician and ChatGPT‑4\nMartin Krusche1  · Johnna Callhoff2,3  · Johannes Knitza4,5  · Nikolas Ruffer1 \nReceived: 21 August 2023 / Accepted: 7 September 2023 / Published online: 24 September 2023 \n© The Author(s) 2023\nAbstract\nPre-clinical studies suggest that large language models (i.e., ChatGPT) could be used in the diagnostic process to distinguish \ninflammatory rheumatic (IRD) from other diseases. We therefore aimed to assess the diagnostic accuracy of ChatGPT-4 in \ncomparison to rheumatologists. For the analysis, the data set of Gräf et al. (2022) was used. Previous patient assessments \nwere analyzed using ChatGPT-4 and compared to rheumatologists’ assessments. ChatGPT-4 listed the correct diagnosis \ncomparable often to rheumatologists as the top diagnosis 35% vs 39% (p = 0.30); as well as among the top 3 diagnoses, 60% \nvs 55%, (p  = 0.38). In IRD-positive cases, ChatGPT-4 provided the top diagnosis in 71% vs 62% in the rheumatologists’ \nanalysis. Correct diagnosis was among the top 3 in 86% (ChatGPT-4) vs 74% (rheumatologists). In non-IRD cases, Chat-\nGPT-4 provided the correct top diagnosis in 15% vs 27% in the rheumatologists’ analysis. Correct diagnosis was among the \ntop 3 in non-IRD cases in 46% of the ChatGPT-4 group vs 45% in the rheumatologists group. If only the first suggestion \nfor diagnosis was considered, ChatGPT-4 correctly classified 58% of cases as IRD compared to 56% of the rheumatolo-\ngists (p = 0.52). ChatGPT-4 showed a slightly higher accuracy for the top 3 overall diagnoses compared to rheumatologist’s \nassessment. ChatGPT-4 was able to provide the correct differential diagnosis in a relevant number of cases and achieved \nbetter sensitivity to detect IRDs than rheumatologist, at the cost of lower specificity. The pilot results highlight the potential \nof this new technology as a triage tool for the diagnosis of IRD.\nKeywords Large language models · ChatGPT · Rheumatology · Triage · Diagnostic process · Artificial intelligence\nIntroduction\nRecent diagnostic and therapeutic advances in rheumatol-\nogy are still counterbalanced by a shortage of specialists \n[1] resulting in a significant diagnostic delay [2 ]. Early and \ncorrect diagnosis is, however, essential to prevent persistent \njoint damage.\nIn this context, artificial intelligence applications includ-\ning patient-facing symptom checkers represent a field of \ninterest and could facilitate patient triage and accelerate \ndiagnosis [3 , 4]. In 2022, we were able to show that the \nsymptom-checker Ada had a significantly higher diagnostic \naccuracy than physicians in the evaluation of rheumatologi-\ncal case vignettes [5].\nCurrently, the introduction of large language models \n(LLM) such as ChatGPT has raised expectations for their \nuse in medicine [6 ]. The impact of ChatGPT's arises from \nits ability to engage in conversations and its performance \nthat is either close to or on par with human capabilities in \nvarious cognitive tasks [7 ]. For instance, Chat-GPT has \nRheumatology\nINTERNATIONAL \n * Martin Krusche \n m.krusche@uke.de\n Johnna Callhoff \n Johanna.callhoff@drfz.de\n Johannes Knitza \n johannes.knitza@uk-erlangen.de\n Nikolas Ruffer \n n.ruffer@uke.de\n1 Division of Rheumatology and Systemic Inflammatory \nDiseases, University Hospital Hamburg-Eppendorf (UKE), \nHamburg, Germany\n2 Epidemiology Unit, German Rheumatism Research Centre, \nBerlin, Germany\n3 Institute for Social Medicine, Epidemiology and Health \nEconomics, Charité Universitätsmedizin, Berlin, Germany\n4 Institute of Digital Medicine, University Hospital of Giessen \nand Marburg, Philipps University Marburg, Marburg, \nGermany\n5 Université Grenoble Alpes, AGEIS, Grenoble, France\n304 Rheumatology International (2024) 44:303–306\n1 3\nachieved satisfactory scores on the United States Medical \nLicensing Examinations [8 ] and some authors suggest that \nLLM applications might be suitable for clinical, educational, \nor research environments [9, 10].\nInterestingly, pre-clinical studies suggest that this tech-\nnology could also be used in the diagnostic process [11, 12] \nto distinguish inflammatory rheumatic from other diseases.\nWe therefore aimed to assess the diagnostic accuracy of \nChatGPT-4 in comparison to a previous analysis including \nphysicians and symptom checkers regarding rheumatic and \nmusculoskeletal diseases (RMDs).\nMethods\nFor the analysis, the data set of Gräf et al. [5] was used \nwith minor updates to disease classification regarding the \ngrouping of diagnoses. The assessments of the symptom-\nchecker app were analyzed using ChatGPT-4 and compared \nto the previous assessment results of Ada and the diagnos-\ntic ranking of the blinded rheumatologists. ChatGPT-4 was \ninstructed to name the top five differential diagnoses based \non the available information of the Ada assessment (see Sup-\nplement 1).\nAll diagnostic suggestions were manually reviewed. If an \nInflammatory rheumatic disease (IRD) was among the top \nthree (D3) or top five suggestions (ChatGPT-4 D5), respec-\ntively, D3 and D5 were summarized as IRD-positive (even \nif non-IRD diagnoses were also among the suggestions). \nProportions of correctly classified patients were compared \nbetween the different groups using McNemar’s test. Clas-\nsification of inflammatory rheumatic disease (IRD) status \nwas additionally assessed.\nResults\nChatGPT-4 listed the correct diagnosis comparable often to \nphysicians as the top diagnosis 35% vs 39% (p  = 0.30); as \nwell as among the top 3 diagnoses, 60% vs 55%, (p = 0.38). \nIn IRD-positive cases, ChatGPT-4 provided the top diag-\nnosis in 71% vs 62% in the physician analysis. The correct \ndiagnosis was among the top 3 in 86% (ChatGPT-4) vs 74% \n(physicians). In non-IRD cases, ChatGPT-4 provided the \ncorrect top diagnosis in 15% vs 27% in the physician analy-\nsis. The correct diagnosis was among the top 3 in non-IRD \ncases in 46% of the ChatGPT-4 group vs 45% in the physi-\ncian group (Fig. 1).\nIf only the first suggestion for diagnosis was considered, \nChatGPT-4 correctly classified 58% of cases as IRD com-\npared to 56% of the rheumatologists (p  = 0.52). If the top \n3 diagnoses were considered, ChatGPT-4 classified 36% of \nthe cases correctly as IRD vs 52% of the rheumatologists \n(p = 0.01) (see Fig. 1). ChatGPT-4 had at least one sugges -\ntion of an inflammatory diagnosis for all non-IRD cases.\nDiscussion\nChatGPT-4 showed a slightly higher accuracy (60% vs. \n55%) for the top 3 overall diagnoses compared to the \nrheumatologist’s assessment. It had a higher sensitivity to \ndetermine the correct IRD status than rheumatologists, but \nconsiderably worse specificity, suggesting that ChatGPT-4 \nmay be particularly useful for detecting IRD patients, \nwhere timely diagnosis and treatment initiation are criti-\ncal. It could therefore potentially be used as a triage tool \nfor digital pre-screening and facilitate quicker referrals of \npatients with suspected IRDs.\nOur results are in line with those of Kanjee et al. [12 ] \nwho demonstrated an accuracy of 64% for ChatGPT-4 \nevaluating the top 5 differential diagnoses of the New Eng-\nland Journal of Medicine clinicopathological conferences.\nInterestingly, in the cross-sectional study of Ayers et al. \n[13], the authors found that chatbot responses to publicly \nasked medical questions on a public social media forum \nwere preferred over physician responses and rated signifi-\ncantly higher for both quality and empathy, highlighting \nthe potential of this technology as a first point of contact \nand source of information for patients. In summary, Chat-\nGPT-4 was able to provide the correct differential diag-\nnosis in a relevant number of cases and achieved better \nsensitivity to detect IRDs than a rheumatologist, at the \ncost of lower specificity.\nAlthough this analysis has some shortcomings, i.e., the \nsmall sample size and the limited information (only access \nto the Ada assessments without further clinical data), it \nhighlights the potential of this new technology as a triage \ntool that could support or even speed up the diagnosis of \nRMDs.\nAs digital self-assessment and remote care options are \ndifficult for some patients due to limited digital health com-\npetencies [14], up-to-date studies should be conducted on \nhow accurately patients can express their symptoms and \ncomplaints using AI and symptom-checker applications, so \nthat we can benefit from these technologies more effectively.\nUntil satisfactory results are obtained, the use of artificial \nintelligence by GPs for effective referral instead of diag-\nnostic use can be expanded and larger prospective studies \nare recommended to further evaluate the technology. Fur -\nthermore, issues, such as ethics, patient consent, and data \nprivacy in the context of the use of artificial intelligence in \nmedical-decision making, are crucial critical guidelines for \nthe application of LLM technologies such as ChatGPT are \nneeded [15].\n305Rheumatology International (2024) 44:303–306 \n1 3\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s00296- 023- 05464-6.\nAuthor contribution Conceptualization: MK and NR; data curation: \nall authors, formal analysis: MK and JC, and funding acquisition: \nnot applicable. Investigation: all authors. Methodology: MK, JC, and \nJK; software: MK; validation: all authors; visualization: MK and JC; \nwriting—original draft: MK and NR; writing—review and editing: \nall authors.\nFunding Open Access funding enabled and organized by Projekt \nDEAL. MK: Speaker fee from Ada, Scientific funding: Ada. JC: \nSpeaker’ fees from Janssen-Cilag, Pfizer, and Idorsia, all unrelated \nto this work.\nFig. 1  Percentage correctly \nclassified diagnosis rank\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n12345\n% of correct diagnosis\n# of diagnosis cosidered \nAll cases \nChatGPT\nADA\nRheumatologist\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n1234 5\nsisonagidtcerrocfo%\n# of diganosis considered \nWith IRD\nChatGPT\nADA\nRheumatologist\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n1234 5\nsisongaidtcerrocfo%\n# of diagnosis cosidered \nNon-IRD \nChatGPT\nADA\nRheumatologist\n306 Rheumatology International (2024) 44:303–306\n1 3\nData availability The datasets used and/or analysed during the cur -\nrent study are available from the corresponding author on reasonable \nrequest.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n 1. Rheumadocs und Arbeitskreis Junge Rheumatologie (AGJR), \nKrusche M, Sewerin P, Kleyer A, Mucke J, Vossen D, u. a. \nFacharztweiterbildung quo vadis? Z Für Rheumatol. Oktober \n2019;78(8):692–7.\n 2. Miloslavsky EM, Marston B (2022) The challenge of address-\ning the rheumatology workforce shortage. J Rheumatol Juni \n49(6):555–557\n 3. Fuchs F, Morf H, Mohn J, Mühlensiepen F, Ignatyev Y, Bohr D \n(2023) Diagnostic delay stages and pre-diagnostic treatment in \npatients with suspected rheumatic diseases before special care \nconsultation: results of a multicenter-based study. Rheumatol Int \nMärz 43(3):495–502\n 4. Knitza J, Mohn J, Bergmann C, Kampylafka E, Hagen M, Bohr \nD (2021) Accuracy, patient-perceived usability, and acceptance \nof two symptom checkers (Ada and Rheport) in rheumatology: \ninterim results from a randomized controlled crossover trial. \nArthritis Res Ther 23(1):112\n 5. Gräf M, Knitza J, Leipe J, Krusche M, Welcker M, Kuhn S \n(2022) Comparison of physician and artificial intelligence-\nbased symptom checker diagnostic accuracy. Rheumatol Int \n42(12):2167–2176\n 6. Hügle T (2023) The wide range of opportunities for large lan-\nguage models such as ChatGPT in rheumatology. RMD Open \n9(2):e003105\n 7. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan \nTF, Ting DSW (2023) Large language models in medicine. Nat \nMed 29(8):1930–1940\n 8. Kung TH, Cheatham M, Medenilla A, Sillos C, De Leon L, \nElepaño C (2023) Performance of ChatGPT on USMLE: potential \nfor AI-assisted medical education using large language models. \nPLOS Digit Health. 2(2):e0000198\n 9. Thirunavukarasu AJ, Hassan R, Mahmood S, Sanghera R, Bar -\nzangi K, El Mukashfi M (2023) Trialling a large language model \n(ChatGPT) in general practice with the applied knowledge test: \nobservational study demonstrating opportunities and limitations \nin primary care. JMIR Med Educ 9:e46599\n 10. Verhoeven F, Wendling D, Prati C (2023) ChatGPT: when artifi-\ncial intelligence replaces the rheumatologist in medical writing. \nAnn Rheum Dis 82(8):1015–1017\n 11. Ueda D, Mitsuyama Y, Takita H, Horiuchi D, Walston SL, \nTatekawa H (2023) ChatGPT’s diagnostic performance from \npatient history and imaging findings on the diagnosis please quiz-\nzes. Radiology 308(1):e231040\n 12. Kanjee Z, Crowe B, Rodman A (2023) Accuracy of a generative \nartificial intelligence model in a complex diagnostic challenge. \nJAMA 330(1):78\n 13. Ayers JW, Poliak A, Dredze M, Leas EC, Zhu Z, Kelley JB (2023) \nComparing physician and artificial intelligence chatbot responses \nto patient questions posted to a public social media forum. JAMA \nIntern Med 183(6):589–596\n 14. de Thurah A, Bosch P, Marques A, Meissner Y, Mukhtyar CB, \nKnitza J (2022) EULAR points to consider for remote care in \nrheumatic and musculoskeletal diseases. Ann Rheum Dis \n81(8):1065–1071\n 15. Tools such as ChatGPT threaten transparent science; here are our \nground rules for their use. Nature. Januar 2023;613(7945):612.\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.9137979745864868
    },
    {
      "name": "Rheumatology",
      "score": 0.8538572192192078
    },
    {
      "name": "Medical diagnosis",
      "score": 0.7209043502807617
    },
    {
      "name": "Internal medicine",
      "score": 0.7101882100105286
    },
    {
      "name": "Diagnostic accuracy",
      "score": 0.5580012202262878
    },
    {
      "name": "Differential diagnosis",
      "score": 0.41861289739608765
    },
    {
      "name": "Physical therapy",
      "score": 0.3569335639476776
    },
    {
      "name": "Pathology",
      "score": 0.19547346234321594
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210108711",
      "name": "University Medical Center Hamburg-Eppendorf",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I159176309",
      "name": "Universität Hamburg",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210095248",
      "name": "German Rheumatism Research Centre",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I268975460",
      "name": "Universitätsklinikum Gießen und Marburg",
      "country": "DE"
    }
  ],
  "cited_by": 81
}