{
  "title": "Motion-Guided Global–Local Aggregation Transformer Network for Precipitation Nowcasting",
  "url": "https://openalex.org/W4313002791",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2113571200",
      "name": "Xichao Dong",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2549127767",
      "name": "Zewei Zhao",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2557713978",
      "name": "Yupei Wang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098059456",
      "name": "Jianping Wang",
      "affiliations": [
        "Delft University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099049767",
      "name": "Cheng Hu",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6691096134",
    "https://openalex.org/W3117344638",
    "https://openalex.org/W2963391479",
    "https://openalex.org/W2963782415",
    "https://openalex.org/W764651262",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3180045188",
    "https://openalex.org/W2971432438",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2566769621",
    "https://openalex.org/W2621798389",
    "https://openalex.org/W2171314103",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2562637781",
    "https://openalex.org/W2962785568",
    "https://openalex.org/W6790307280",
    "https://openalex.org/W6765779288",
    "https://openalex.org/W2016184960",
    "https://openalex.org/W2175461096",
    "https://openalex.org/W3211959704",
    "https://openalex.org/W4213344944",
    "https://openalex.org/W3094401231",
    "https://openalex.org/W3200655876",
    "https://openalex.org/W3199775909",
    "https://openalex.org/W6774413459",
    "https://openalex.org/W2800283575",
    "https://openalex.org/W3181942264",
    "https://openalex.org/W2987228832",
    "https://openalex.org/W6785723781",
    "https://openalex.org/W6702130928",
    "https://openalex.org/W2475287302",
    "https://openalex.org/W1951289974",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W6630875275",
    "https://openalex.org/W6753412334",
    "https://openalex.org/W3207011416",
    "https://openalex.org/W2936479764",
    "https://openalex.org/W3202525453",
    "https://openalex.org/W6628877408",
    "https://openalex.org/W6739112683",
    "https://openalex.org/W6745829810",
    "https://openalex.org/W6750259706",
    "https://openalex.org/W2967033144",
    "https://openalex.org/W3184447318",
    "https://openalex.org/W3118349806",
    "https://openalex.org/W3198258664",
    "https://openalex.org/W2030459037",
    "https://openalex.org/W2058913535",
    "https://openalex.org/W2594928006",
    "https://openalex.org/W1989679522",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W2890108942",
    "https://openalex.org/W6738897456",
    "https://openalex.org/W3126335003",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W2517954747",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W3089203045",
    "https://openalex.org/W4200348065",
    "https://openalex.org/W3200090046",
    "https://openalex.org/W3164939772",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W4297790878",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1485009520",
    "https://openalex.org/W4301206121",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2768975186",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W3104861085",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4297772798",
    "https://openalex.org/W2796873224",
    "https://openalex.org/W2625614184",
    "https://openalex.org/W3008659735"
  ],
  "abstract": "&lt;p&gt;Nowadays deep learning-based weather radar echo extrapolation methods have competently improved nowcasting quality. Current pure convolutional or convolutional recurrent neural network-based extrapolation pipelines inherently struggle in capturing both global and local spatiotemporal interactions simultaneously, thereby limiting nowcasting performances, e.g., they not only tend to underestimate heavy rainfalls' spatial coverage and intensity but also fail to precisely predict nonlinear motion patterns. Furthermore, the usually adopted pixel-wise objective functions lead to blurry predictions. To this end, we propose a novel motion-guided global-local aggregation Transformer network for effectively combining spatiotemporal cues at different time scales, thereby strengthening global-local spatiotemporal aggregation urgently required by the extrapolation task. First, we divide existing observations into both short- and long-term sequences to represent echo dynamics at different time scales. Then, to introduce reasonable motion guidance to Transformer, we customize an end-to-end module for jointly extracting motion representation of short- and long-term echo sequences (MRS, MRL), while estimating optical flow. Subsequently, based on Transformer architecture, MRS is used as queries to retrospect the most useful information from MRL for an effective aggregation of global long-term and local short-term cues. Finally, the fused feature is employed for future echo prediction. Additionally, for the blurry prediction problem, predictions from our model trained with an adversarial regularization achieve superior performances not only in nowcasting skill scores but also in precipitation details and image clarity over existing methods. Extensive experiments on two challenging radar echo datasets demonstrate the effectiveness of our proposed method.&lt;/p&gt;",
  "full_text": "  \nDelft University of Technology\nMotion-Guided Global-Local Aggregation Transformer Network for Precipitation\nNowcasting\nDong, Xichao; Zhao, Zewei; Wang, Yupei; Wang, Jianping; Hu, Cheng\nDOI\n10.1109/TGRS.2022.3217639\nPublication date\n2022\nDocument Version\nFinal published version\nPublished in\nIEEE Transactions on Geoscience and Remote Sensing\nCitation (APA)\nDong, X., Zhao, Z., Wang, Y., Wang, J., & Hu, C. (2022). Motion-Guided Global-Local Aggregation\nTransformer Network for Precipitation Nowcasting. IEEE Transactions on Geoscience and Remote Sensing,\n60, Article 5119816. https://doi.org/10.1109/TGRS.2022.3217639\nImportant note\nTo cite this publication, please use the final published version (if applicable).\nPlease check the document version above.\nCopyright\nOther than for strictly personal use, it is not permitted to download, forward or distribute the text or part of it, without the consent\nof the author(s) and/or copyright holder(s), unless the work is under an open content license such as Creative Commons.\nTakedown policy\nPlease contact us and provide details if you believe this document breaches copyrights.\nWe will remove access to the work immediately and investigate your claim.\nThis work is downloaded from Delft University of Technology.\nFor technical reasons the number of authors shown on this cover page is limited to a maximum of 10.\nGreen Open Access added to TU Delft Institutional Repository 'You share, we take care!' - Taverne project   \nhttps://www.openaccess.nl/en/you-share-we-take-care \nOtherwise as indicated in the copyright section: the publisher is the copyright holder of this work and the author uses the Dutch legislation to make this work public.   \nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022 5119816\nMotion-Guided Global–Local Aggregation\nTransformer Network for Precipitation Nowcasting\nXichao Dong , Member, IEEE, Zewei Zhao , Yupei Wang , Jianping Wang ,\nand Cheng Hu , Senior Member, IEEE\nAbstract— Nowadays deep learning-based weather radar echo\nextrapolation methods have competently improved nowcasting\nquality. Current pure convolutional or convolutional recurrent\nneural network-based extrapolation pipelines inherently struggle\nin capturing both global and local spatiotemporal interactions\nsimultaneously, thereby limiting nowcasting performances, e.g.,\nthey not only tend to underestimate heavy rainfalls’ spatial\ncoverage and intensity but also fail to precisely predict nonlinear\nmotion patterns. Furthermore, th e usually adopted pixel-wise\nobjective functions lead to blurry predictions. To this end, we pro-\npose a novel motion-guided global–local aggregation Transformer\nnetwork for effectively combining spatiotemporal cues at differ-\nent time scales, thereby strengthening global–local spatiotemporal\naggregation urgently required by the extrapolation task. First,\nwe divide existing observations into both short- and long-term\nsequences to represent echo dynamics at different time scales.\nThen, to introduce reasonable motion guidance to Transformer,\nwe customize an end-to-end module for jointly extracting motion\nrepresentation of short- and long-term echo sequences (MRS,\nMRL), while estimating optical ﬂow. Subsequently, based on\nTransformer architecture, MRS is used as queries to retrospect\nthe most useful information from MRL for an effective aggre-\ngation of global long-term and local short-term cues. Finally,\nthe fused feature is employed for future echo prediction. Addi-\ntionally, for the blurry prediction problem, predictions from our\nmodel trained with an adversarial regularization achieve superior\nperformances not only in nowcasting skill scores but also in\nprecipitation details and image clarity over existing methods.\nExtensive experiments on two challenging radar echo datasets\ndemonstrate the effectiveness of our proposed method.\nManuscript received 19 January 2022; revised 22 August 2022;\naccepted 18 October 2022. Date of publication 26 October 2022; date of\ncurrent version 11 November 2022. This work was supported in part by the\nSpecial Fund for Research on National Major Research Instruments (NSFC)\nunder Grant 61827901 and Grant 31727901, in pa rt by the National Natural\nScience Foundation of China under Grant 61960206009, in part by the Natural\nScience Foundation of Chongqing under G rant cstc2020jcyj-msxmX0621,\nand in part by the Distinguished Young Scholars of Chongqing under Grant\ncstc2020jcyj-jqX0008. (Corresponding author: Yupei Wang.)\nXichao Dong is with the School of Information and Electronics, Beijing\nInstitute of Technology, Beijing 100081, China, also with the Key Laboratory\nof Electronic and Information Technology in Satellite Navigation, Ministry of\nEducation, Beijing Institute of Technology, Beijing 100081, China, and also\nwith the Chongqing Key Laboratory of Novel Civilian Radar, Beijing Institute\nof Technology Chongqing Innovation Center, Chongqing 401120, China.\nZewei Zhao, Yupei Wang, and Cheng Huare with the School of Information\nand Electronics, Beijing Institute of Technology, Beijing 100081, China, and\nalso with the Key Laboratory of Electronic and Information Technology in\nSatellite Navigation, Ministry of Edu cation, Beijing Institute of Technology,\nBeijing 100081, China (e-mail: wangyupei2019@outlook.com).\nJianping Wang is with the Faculty of Electrical Engineering, Mathematics\nand Computer Science (EEMCS), Delf t University of Technology, 2628 CD\nDelft, The Netherlands.\nDigital Object Identiﬁer 10.1109/TGRS.2022.3217639\nIndex Terms— Attention mechanism, optical ﬂow, precipitation\nnowcasting, transformers, weather radar.\nI. I NTRODUCTION\nP\nRECIPITATION nowcasting (PN) is of great value for\nreducing adverse effects of weather disasters on modern\nsociety. Weather radar provides high-quality data support\nfor developing nowcasting algorithms. Based on recent past\nweather radar observations in local regions, weather radar echo\nextrapolation algorithms aim to p redict near future radar echo\nsequences precisely and promptly.\nTraditional methods [1], [2], [3], [4], [5], [6], [7] usually\nextrapolate linearly relying o n the precalculated motion vec-\ntors. However, they either simplify intricacy strong radar echo\nas isolated storm cells [1], [2] or assume the echo intensity\nremains constant [3], [4], [5], [6], [7], failing in extrapolating\nscattered or split echo as well as predicting ﬁne-grained\nprecipitation pattern evolutions. Furthermore, traditional meth-\nods cannot beneﬁt fully from the large amount of historical\nweather radar data.\nNowadays, the data-driven deep learning (DL)-based meth-\nods have shown remarkable potential [8], [9], [10], [11], [12],\n[13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23],\n[24] for pn tasks. They are either based on pure convolu-\ntional structures or convolutional recurrent neural networks\n(ConvRNNs).\nThe convolutional structures [9], [10], [11] are with advan-\ntages of simple and easy to understand while inherently limited\nto the size of the reception ﬁeld, hence unable to capture\nlong-range spatiotemporal relationships.\nPrevailing ConvRNN models [12], [13], [14], [15], [16],\n[17], [18], [19], [20], [25], [26] are generally dedicated to\ndesigning more satisﬁed ConvRNN units based on convolu-\ntional long short-term memory (ConvLSTM) network [12].\nHowever, basic topologies of these recent developed units are\ntoo complex to be optimized easily and bring higher computa-\ntion burden. What is more, common ConvRNNs are ﬁrst-order\nMarkovian models, i.e., they only use information from the\nprevious time step to update the hidden state, resulting in\ninherent struggles in perceiving long-range spatiotemporal\ndependencies simultaneously. F urthermore, previous models\nonly consider short-term input sequence with limited dynamics\nto encode spatiotemporal interactions.\nIn addition, for the choice of objective functions, current\ndeep prediction models [12], [13], [14], [15], [16], [17]\n1558-0644 © 2022 I EEE. Personal u se is perm itted, but republication/redistribution requires IEEE permission.\nSee https://www.ieee.org/publications/rights/index.html for more information.\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \n5119816 IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022\nusually minimize the mean squared error (MSE) or mean\nabsolute error (MAE) between the ground-truth and the\npredicted images, which leads to conservative, blurry, and\nover-smoothing results lacking the key texture information of\nprecipitation ﬁelds. However, since the nowcasting products\nare aimed for forecasters, the human perceptual quality and the\nclarity of predicted images are also important and the blurry\nresults cannot provide effective references for the issuance of\nsevere weather forecast warning.\nIn summary, previous DL-based nowcasting models strug-\ngle in 1) capturing both global and local spatiotemporal\ninteractions simultaneously, limiting their performance and\n2) predicting high ﬁdelity echo image sequence with rich\nrainfall details. For example, they tend to underestimate heavy\nrainfalls’ spatial coverages and intensities at longer nowcast-\ning lead times, and show constrains in precisely nowcasting\ncomplex echo dynamics such as convective initialization,\ndissipation, and deformation.\nMore recently, the Transformer [27], [28] architectures’\nemergence provides an alternative to mitigate the limitations\nof previous pure convolutional or ConvRNN models applied\nfor nowcasting tasks. The self-attention module in Transformer\nis dedicated to calculating the long-range spatial correlations\namong pixels and cross-attention module is proper for per-\nceiving the temporal similarities among echo sequences with\ndifferent temporal scales. Thus, t he Transformer architecture\nessentially resonates with the goal of effective global–local\nspatiotemporal aggregation urgently required by pn.\nNevertheless, using existing Transformers directly for now-\ncasting precipitations has the following issues. On the one\nhand, the computational burde n is sometimes unaffordable\nwhen we directly use the whole standard encoder–decoder\nvision Transformer [27] architecture, since the computational\ncost becomes quadratic to spatiotemporal dimensions [29].\nOn the other hand, when performing attention mechanism, the\nguidance of motion information is lacked in previous Trans-\nformer architecture. This lead s to the model neglects some\nkey information of similar and ﬁne-grained echo patterns in\nthe spatiotemporal neighborhood when fast echo motions such\nas convection generation prese nt, thereby fails in capturing\ncorrect and accurate echo pattern evolutions.\nTo tackle aforementioned challenges, we propose a novel\nmotion-guided global–local aggregation Transformer network\nfor pn. Particularly, we divide existing observations into both\nshort- and long-term sequences to represent echo dynam-\nics at different time scales. Instead of directly using the\nwhole encoder–decoder structure of Transformer architecture,\nwe mainly leverage the decoder part for effectively and efﬁ-\nciently combining spatiotemporal cues at different time scales.\nWhat is more, to introduce reasonable motion guidance to\nTransformer, we customize an end-to-end module for jointly\nextracting motion representation of short- and long-term echo\nsequences abbreviated to MRS and MRL, while estimat-\ning optical ﬂow. Then based o n Transformer architecture,\nMRS is used to perform the self-attention in addition to the\ncross-attention operations to MRL, so as to retrospect the most\nuseful information from MRL for an effective aggregation\nof global long-term and local short-term cues. Finally, the\nfused feature is employed for future echo prediction. For the\nblurry prediction problem, we further adopt an adversarial\ntraining strategy for improving predictions’ perceptual quality\nand clarity.\nThe contributions of our work are summarized as follows.\n1) We propose a motion-guided global–local aggregation\nTransformer network for pn. We divide existing obser-\nvations into both short- and long-term sequences, and\nintroduce the Transformer architecture for effective\ncombination of spatiotempor al cues at different time\nscales, thereby strengthening global–local spatiotempo-\nral aggregation required by the pn task.\n2) We propose an end-to-end module to jointly obtain\nmotion representation (MR ) of echo sequences while\nestimating optical ﬂow, thereby introducing reasonable\nmotion guidance to the Transformer architecture.\n3) We further train our proposed model with an adver-\nsarial strategy to tackle the blurry prediction problem.\nExperimental results demonstrate that our predictions\nachieve superior performances not only in nowcasting\nskill scores but also in precipitation details and image\nclarity over existing methods.\nThe rest of this article is organized as follows. Section II\ndescribes a review of the basics. Section III introduces the\ndetails of our proposed method. In Section IV, we report and\nanalyze quantitative and qualitative experimental results. The\ndiscussion and conclusion are drawn in Section V.\nII. R\nEVIEW OF THE BASICS\nA. Problem Deﬁnition and ConvRNN Structures\nWe formulate the pn task as follows. Given past weather\nradar echo observations S1:t ={ Xk |k = 1,2,..., t}∈\nRt×H ×W×C as input (where Xk ∈ RH ×W×C represents the\nkth frame of S1:t ), our goal is to optimize the extrapolation\nmodel F for obtaining predicted sequence ˜S(t+1):T similar with\nthe ground-truth future sequence S(t+1):T . The echo images\nare always stored as grayscale images hence the channel C\nis 1 here.\nIn ConvLSTM [12] basic unit, full connections of the\nstandard LSTMs [30] are replaced with convolutions to capture\nboth the spatial and temporal information at the same time.\nAdditionally, [13] builds an encoder-forecaster structure by\nstacking ConvLSTMs, as shown in Fig. 1. The cell states\nand hidden states are delivered horizontally along the temporal\ndimension. In addition, the hidden states are transferred ver-\ntically to handle spatial appearances. The down sampling and\nup sampling blocks are inserted in between two ConvLSTM\nlayers.\nFollow-up ConvRNN methods [14], [15], [16], [17], [18],\n[19], [20] are generally dedicated to designing more completed\nConvRNN units based on ConvLSTMs, e.g., PredRNN [14]\nuses pairwise memory cells to extend ConvLSTM, memory\nin memory (MIM) [16] adopts additional memory cells for\ncapturing both nonstationary and stationary processes better.\nMotionRNN [17] is improved from MIM and it decomposes\nmotions into transient variations and motion trends. Interaction\ndual attention long short-term memory (IDA-LSTM) [18]\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \nDONG et al.: MOTION-GUIDED GLOBAL-LOCAL AGGREGATION TRANSFORMER NETWORK 5119816\nFig. 1. Encoder-forecaster architecture based on ConvLSTMs. The super-\nscript l denotes the lth ConvLSTM layer in spatial dimensions and J denotes\nthe total ConvLSTM layers. The subscript represents the time step.\nleverages attention mechanism to reweight the fused hidden\nstate and cell state features in channel and spatial dimension.\nThey have achieved promising performances.\nB. End-to-End Optical Flow Estimation\nOptical ﬂow encodes motion i nformation between echo\nframes and traditional optical ﬂow-based pn methods [4], [5],\n[6], [7] still provide important references for the issuance of\nsevere weather forecast warnings. However, the bottlenecks of\ncumbersome calculation of opti cal ﬂow and using optical ﬂow\nto extrapolating linearly hinder further applications of optical\nﬂows in pn task.\nFor the ﬁrst bottleneck, encouraged by the success of\nDL-based optical ﬂow estimation [31], [32] methods and\n3-D convolutions for video optical ﬂow estimation [33], [34],\nwe customize an end-to-end module for optical ﬂow estimation\nbut emphasize on improving pn qualities.\nFor the second bottleneck, instead of using optical ﬂow for\nlinear echo extrapolation, our model jointly obtains MR of\necho sequences while estimating optical ﬂow, thereby intro-\nducing reasonable motion guidance. Obtaining high-level MR\nof the echo sequence in latent space enhances the robustness\nof our model, and further helps model nowcast ﬁne-grained\necho pattern evolutions precisely.\nC. Brief Review of Transformers\nThe Transformer architecture [27] is originally proposed\nfor natural language processing tasks. Recently many variant\nTransformer structures have been adopted for computer vision\ntasks [27], [28], [29], [35] and achieved impressive results.\nThe prominent performance of Transformers in these tasks\nhas fascinated researchers to explore their applications in\nremote sensing ﬁelds, including hyperspectral image classiﬁca-\ntion [36], remote sensing image change detection [37], remote\nsensing image captioning [38], and so on.\nThe self-attention and cross-attention mechanisms are key\ncomponents of Transformers. Th e intuition of the attention\nmechanisms in Transformer is that each token can interact with\nothers and exploit rich semantic information more efﬁciently,\nwhich makes Transformers suitable for preforming long-range\ninteractions [27].\nD. Blurry Prediction Problem in pn\nDespite the promising performances in improving nowcast-\ning skill scores, the DL-based pn models tend to produce\nblurry predictions, which is a common problem of ill repute.\nThis could be explained by the analysis of the adopted\nobjective functions, i.e., current models [12], [13], [14], [15],\n[16], [17], [18], [19], [20] usually minimize the MSE or\nMAE between the ground truth and the predictions. However,\nthe widely used MSE estimator tends to return the average\nof many possible solutions and the MAE estimator tends\nto return the median of the set of equally like values [39],\nwhich leads to conservative, blurry, and over smoothing\nresults lacking the key texture information of precipitation\nﬁelds.\nSome recent studies [40], [41], [42], [43], [44] try adding\ndifferent generative adversarial (GAN) losses to tackle this\nproblem. However, instead of specially designing new net-\nwork architectures for the underestimation problem of rainfall\nregions and intensities, these methods mainly add adver-\nsarial training strategies based on the existing extrapolation\nframeworks and explore the performances. For example, the\ngenerator in [42] is based on the spatiotemporal long short-\nterm memory (ST-LSTM) [14], the generator in [43] is based\non a simple 3D-convolutional neural network (CNN), in [44]\nthe authors mainly adopt the network architecture from [45]\nfor radar extrapolation, and in [41] the authors explore two\nclassic DL-based radar extrapolation models’ (U-Net [8] and\nConvLSTM [12]) performan ce when combined with GAN\nlosses. This adoption of existing architectures limits further\nimprovements.\nIII. O\nUR PROPOSED METHOD\nAs shown in Fig. 2, our model has two branches, i.e.,\nwe adopt the encoder-forecaster structure as our basic echo\nprediction branch (hereinafter referred to as the prediction\nbranch), and we further propose the global–local aggregation\nbranch (hereinafter referred to as the aggregation branch)\nwhich contains of the optical ﬂow guided MR module (here-\ninafter referred to as the motion module), Transformer decoder\n(TD), and a channel attention-based fusion module.\nWe ﬁrst divide existing observations into both short- and\nlong-term sequences, mainly considering that storms usually\nhave their own life cycles [46]. For example, the single-cell\nstorms are usually small scale and fairly disorganized con-\nvective elements which generate or dissipate rapidly, having\n2 h or less life cycle [46]. For the future 1 h nowcasting\nwith the past 1 h observations as input, the past 1-hour\nshort-term sequence hardly capture the whole evolution cycle\nof these convective elements. However, with a longer term\nsequence as reference, we have the possibility to accu-\nrately describe its motion. For the multicell storms which\nare maintained in an organized linear pattern, they tend to\npersist longer and evolve less rapidly. In this situation, the\nshort- and long-term sequence split strategy is also better\nfor tracking this slower move ment trend because the motion\ntrend of the short-term sequence is reﬂected in the long-term\nsequence.\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \n5119816 IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022\nFig. 2. Overall framework of our proposed network for pn. (a) Data ﬂow of the “update” training stage. (b) Data ﬂow of the “ﬁxed” training stage. (c) Structure\nof the TD unit.\nFig. 3. (a) Overall structure of our proposed optical ﬂow guided MR\nmodule. (b)–(d) Demonstrates the detai l architectures of our used Res-block,\ndown sampling block, and the tokenizer, respectively. BN represents Batch\nNormalization layer. LReLU denotes Leaky-ReLU activation function.\nA. Our Motion Module\nTaken S1:t as input, our motion module outputs the cor-\nresponding optical ﬂows Ot ={ ok |k = 1,2,..., t}∈\nRt×H ×W×2,w h e r eok ∈ RH ×W×2 is the 2-D optical ﬂow vector\nfor the kth and ( k + 1)th input frames.\nAs shown in Fig. 3, our designed motion module is\nan encoder–decoder architecture based on 3-D residual\nblocks [47] (Res-blocks). Particularly, the encoder contains of\none Res-block and three down-sampling blocks, the decoder\nhas three Res-blocks, three up-sampling blocks, and four\n1 × 1 × 1 convolutional layers. To mitigate the information\nlost by down sampling in the encoding stage, we also adopt the\nskip connections [8], i.e., the red dashed lines in Fig. 3. This\nis helpful for deep network training [48], [49], and enables\nthe decoder to obtain more high-resolution information so as\nto restore better detailed information.\nNote that o\nt is the optical ﬂow for the tth and (t + 1)th\ninput frames while we do not give the (t + 1)th frame to\nthe model. We predict this last echo image’s optical ﬂow\nmainly considering the following two beneﬁts: 1) the model\nrequires semantic inference to predict the future optical ﬂow,\nand this may force the model to exploit better motion cues\nfor pn task [34] and 2) because the output dimensions of\nthe deconvolution layers are usually a multiple of the input,\nit is actually easier to implement a model with same input\nand output dimensions. With optical ﬂow as supervision, our\nmodule learns MR without relying on explicit optical ﬂow\ncomputation.\nFor the Res-blocks, as illustrated in Fig. 3(b), we adopt\nthe preactivation mechanism [ 47] to construct the identify\nmapping, i.e., the input feature map passes through the nor-\nmalization layer, the activation l ayer, and the regularization\nlayer before passing through the 3 × 3 × 3 convolutional\nlayer. Instead of using max pooling layers for down sam-\npling, we adopt stride convolutions to preserve spatiotemporal\ndetails. Details of our used down sampling blocks are shown\nin Fig. 3(c). Note that our used down sampling blocks can be\nseen as special forms of the Res-blocks where we add one\n3 × 3 × 3 convolutional layer with a normalization layer in\nthe identity skip connection stream, and for convolutions in the\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \nDONG et al.: MOTION-GUIDED GLOBAL-LOCAL AGGREGATION TRANSFORMER NETWORK 5119816\ndown sampling block, the strides are set to 2. The up-sampling\nblocks are implemented by 3-D deconvolution layers.\nAfter S1:t is passed through the encoder of our motion\nmodule and with the guidance of optical ﬂow motion infor-\nmation, our intuition is that here the high-level feature maps\nA ∈ R\n(t/Dt )×(H/Dhw)×(W/Dhw)×C1 could describe the motion\ninformation of the input sequence in an abstract way, where\nDt and Dhw represent the down sampling factor in temporal\nand spatial dimensions, respectively, and C1 is the channel\ndimension of A.\nAdditionally, to generate vector sequence as token embed-\nding which meets the input form requirements of the\nTransformer module and further aggregate spatiotemporal\ninformation, we forward A to a tokenizer to obtain motion\ntokens T . In speciﬁc, we map A into an embedding through\none 1 × 1 × 1 convolutional layer with an activation layer.\nAfter that, we use a max-polling layer to further aggregate\nspatial information. Additionally, an adaptive pooling layer is\nemployed for aggregating temporal information. Here we get\nthe intermediate feature map A\n1 ∈ R(H/D′\nhw)×(W/D′\nhw )×C1 ,a n d\nﬁnally we ﬂat A1 in raster-scan order to obtain T ∈ RL×C1 ,\nwhere L = HW /(D′\nhw)2 and D′\nhw = ks kt Dhw is another down\nsampling factor similar with Dhw, ks and kt are the aggregating\nfactors in spatial dimension of the max pooling layer and\ntemporal dimension of the adaptive pooling layer, respectively.\nThe detail structure of our tokenizer is shown in Fig. 3(d).\nB. TDs for Global–Local Aggregation\n1) TD Structure: As shown in Fig. 2(c), the TD has two\ngroups of inputs, denoted as output tokens α ∈ Rm×C and\ninput tokens β ∈ Rn×C .\nFirst, to retain positional information, the positional encod-\ning ϵpos ∈ Rm×C is added to α to generate α0 = α + ϵpos.\nTaken α0 as input, the multihead self-attention (MSA) block\nwith Nhead heads is expressed as\nQi = α0W i\nq , Ki = α0W i\nk , Vi = α0W i\nv (1)\nhi = Attn[Qi ; Ki ; Vi ]= softmax\n(Qi K T\ni\n√\ndk\n)\nVi (2)\nh = Concat[h1,..., hi ,..., hNhead ]Wd (3)\nwhere Qi , Ki , Vi ∈ Rm×(C/Nhead ) are the query, key, and value\nvectors, respectively. W i\nq , W i\nk , W i\nv ∈ RC×(C/Nhead ) and Wd ∈\nRC×C are linear projection layers. hi denotes the output from\nthe ith head. Concat [·] denotes the concatenation operation.\nAfter that, with a layer normalization layer and an identity\nshortcut, we obtain α1 ∈ Rm×C , written as\nα1 = LayerNorm(h + α0). (4)\nSubsequently, in the multihead cross-attention (MCA)\nblock, the query vector is projected linearly from α1,a n dt h e\nkey and value vectors are projected linearly from the input\nembedding β. The output from MCA r ∈ Rm×C is formed as\nfollows:\nr = Concat[r1,..., r j ,..., rNhead ]W ′\nd (5)\nwhere\nr j = Attn\n[\nα1W j\nq ; βW j\nk ; βW j\nv\n]\n(6)\nis the output from the jth head, W ′\nd , W j\nq , W j\nk ,a n d W j\nv\nrepresent linear projections.\nThen, to provide nonlinearity, the feedforward network\n(FFN) layer is adopted, which is consist of two linear pro-\njection layers with a rectiﬁed lin ear unit (ReLU) activation in\nbetween. Formally, the output of FFN λ ∈ R\nm×C is calculated\nas\nλ = FFN(γ )= ReLU(γ W1)W2 ∈ Rm×C (7)\nwhere W1 ∈ RC×Cin and W2 ∈ RCin ×C are linear projections,\nCin is the dimension of the FFN inter layer.\nFinally, after another layer normalization layer and an\nidentity shortcut, we obtain the TD output θ ∈ Rm×C as\nfollows:\nθ = LayerNorm(λ + γ) . (8)\nIn short, the operations in a TD unit are summarized as\nθ = TD(α, β) ∈ Rm×C . (9)\n2) “Update” Training Stage Using TDs:To make compre-\nhensive use of echo motion characteristics at different time\nscales, we divide the existing observations into both long- and\nshort-term histories (denoted as Slong = S1:T0 (t < T0 ≤ T ),\nSshort = S1:t ), and leverage TDs to perceive the global and\nlocal correlations. Note that for a spatiotemporal prediction\ntask like pn, the precondition is that, we have no future\nobservation information as input at model inference stage.\nTherefore, the divided S\nlong and Sshort are conceptions only\nfor the training data, and we still only use as the model input\nduring the model inference stage. To this end, we adopt an\n“update/ﬁxed” strategy to train the proposed model inspired\nby memory networks [50], [51].\nConcretely, in this stage, we use both Slong and Sshort as input\nof our model. As shown in Fig. 2(a), Slong is forwarded to the\naggregation branch and Sshort is forwarded to our prediction\nbranch. Under the guidance of optical ﬂows, the long-term\nsequence is encoded by our motion module to obtain the\ncorresponding MRL T\nlong ∈ RL×C1 .T h e nw eu s et h eT D st o\nupdate the latent long-term motion information into an external\nmemory bank G ∈ R\nM×C1 ,w h e r eL and M are the number\nof tokens of Tlong and G, respectively. In speciﬁc, Tlong and\nG are adopted as the two sets of inputs of TD unit to obtain\ncorresponding motion prototype θupdate, as deﬁned in (9)\nθupdate = TD(Tlong, G) ∈ RL×C1 (10)\nwhere the subscript represents the feature in “update” stage.\nSubsequently, we reshape θupdate ∈ RL×C1 to obtain the\nmotion context representation θ′\nupdate ∈ R(H/D′\nhw)×(W/D′\nhw)×C1 for\nfurther fusion with inadequate-spatiotemporal-representation\nfeatures output from the prediction branch.\nIn the “update” training stage, we ﬁrst initialize the weights\nof the memory bank G with normal distribution. Then, during\ntraining, the weights in G are updated continuously with\nthe backpropagation algorithm [50], [51], i.e., once there are\nupdates of G’s weights, it could be seen as that the MRL is\nstored to G iteratively.\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \n5119816 IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022\nFig. 4. (a) Data ﬂow in traditional encoder-forecaster structure. (b) Data ﬂow\nbetween encoder-forecaster and our aggregation branch. (c) Detail structure\nof our used channel attention fusion submodule.\n3) Channel Attention-Based Feature Fusion: Since\nConvLSTMs are the basic components of our prediction\nbranch, and the cell state C\nt in ConvLSTM stores the\nlong short-term information iterations from the history to\nthe current, we choose Ct to reﬁne θ′\nupdate for inserting\nthe necessary motion context at present time step t.T h i s\nreﬁnement is based on a channel-attention submodule [52].\nLet C\nL\nt , H L\nt ∈ R(H/D′\nhw)×(W/D′\nhw )×C2 represents the cell state\nand hidden state of the Lth ConvLSTM layer (i.e., the deepest\nlayer) in our prediction branch, where C2 is the number of\nchannels. As shown in Fig. 4(c), the spatial information of\nC L\nt and θ′\nupdate are ﬁrst aggregated using both max-pooling\nand average-pooling operations, obtaining two sets of spatial\ncontext embedding Fmax and Favg, respectively. Then Fmax\nand Favg are forwarded to a shared FFN layer, added together\nand passed through a Sigmoid activation layer to generating\nthe channel attention map MC ∈ R1×1×C1 . Subsequently,\nHadamard product is performed between θ′\nupdate and MC to\nobtain θ′\natt. In addition, θ′\natt is concatenated with H L\nt and\nforwarded to a 1 × 1 convolutional layer to obtain Rl=L\nt .\nFinally, Rl=L\nt is fed to the corresponding up sampling layer\nin the prediction branch for subsequent operations, and\npredict corresponding next echo image ˜Xt+1 as illustrated in\nFig. 4(b).\n4) “Fixed” Training Stage Using TDs:The “ﬁxed” training\nstage is designed to allow the most proper MRL in the memory\nbank G to be retrieved by the dynamic-limited MRS extracted\nfrom Sshort. In this stage, only Sshort is forwarded to the\naggregation and prediction branch. Similar to the “update”\ntraining process, Sshort is encoded by our motion module to\nobtain MRS (i.e., Tshort ∈ RL×C1 ) under the guidance of optical\nﬂows. Note that the motion module used here has the same\nstructure with that used in the “update” training process while\ndoes not share weights for unique and diverse expression of\nMRL and MRS characteristics.\nSubsequently, T\nshort and G are adopted as two sets of inputs\nof TD unit to obtain corresponding motion prototype θﬁxed\nθﬁxed = TD(Tshort, G) ∈ RL×C1 (11)\nwhere the subscript “ﬁxed” represents the feature map in\nthe “ﬁxed” training stage. Additionally, we also reshape\nθﬁxed ∈ RL×C1 to obtain the motion context representation\nθ′\nﬁxed ∈ R(H/D′\nhw)×(W/D′\nhw )×C1 . This procedure is the same as the\n“update” training stage (10). However, we lock the gradient\nof G, and the weights of G are no longer updated and\noptimized but remain ﬁxed for retrieving the motion context\nin MRL during this “ﬁxed” training stage. That is, the entire\nmodel parameters are trained to nowcast the long-term echo\nsequence, except for the memory bank G. Actually, the TD\nunits can be seen as a unique soft addressing paradigm [53],\n[54] here, i.e., given the query vectors, the attention scores\nare calculated from the similarity between key-value pairs and\nappended to the value vectors in a weighted manner, instead\nof strictly meeting the condition that the key vectors are equal\nto query vectors to retrieve the corresponding stored values,\nwhich is adopted in the hard addressing process. Based on\nthe MSA and MCA mechanism o f TD, it enables the most\nproper MRS extracted by our motion module to access the\nmost useful MRL, realizing effective aggregation of global\nlong-term and local short-term motion cues. In addition, as the\nsame with the “update” training stage, there is also a channel\nattention-based fusion between the prediction and aggregation\nbranch to generate the predicted image ˜X\nt+1.\nIn short, the “update” and “ﬁxed” training stages are\nperformed alternately to predict future echo sequences. The\nshort-term echo sequence S1:t is forwarded to the prediction\nbranch in both the “update” and “ﬁxed” training stage. The\nlong- and short-term sequences S\n1:T0 and S1:t are forwarded to\nour aggregation branch alternatively during the two training\nstages.\nC. Loss Functions\nOur adopted loss functions include the pixel-wise loss, the\noptical ﬂow loss, the GAN loss, and the feature-wise style\nloss. The pixel-wise loss Lpred in both the “update” and “ﬁxed”\ntraining phases are the MAE and MSE error, denoted as\nLpred = 1\n(T − t)HW\nT∑\ni=t+1\n∑\nj\n(|˜si, j − si, j |+ (˜si, j − si, j )2)\n(12)\nwhere ˜si, j and si, j are the ith grid points of the jth timestamp\nin the predicted sequence ˜S(t+1):T and ground-truth S(t+1):T .\nThe optical ﬂow loss Lﬂow is based on the endpoint error,\ni.e., the sum of L2 distance between the estimated optical ﬂows\nand ground-truth optical ﬂows, denoted as\nLﬂow =\nK∑\nk=1\n∑\np\n∥ ˜ok,p − ok,p ∥2 (13)\nwhere K is the number of predicted 2-D optical ﬂow vectors.\n˜ok,p is the estimated 2-D optical ﬂow vector of the kth\nand (k + 1)th image at pixel p. ok,p is the corresponding\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \nDONG et al.: MOTION-GUIDED GLOBAL-LOCAL AGGREGATION TRANSFORMER NETWORK 5119816\nFig. 5. (a) Split each event into three echo sequences. (b) Determine whether\nan echo image is eligible or not by average ﬁltering.\nground-truth optical ﬂow vector. The subscript “2” represents\nthe L2-norm. We use the recent EpicFlow [55] method to\ncalculate the pseudo-ground-truth optical ﬂows.\nWhat is more, for the blurry prediction problem, we explore\nthe conditional GAN loss [56] and the style loss [57] to further\nimprove the ﬁdelity and perceptual quality of our predictions,\nwritten as\nL\nGAN = min\nG\nmax\nD\n{E[log(D(S(t+1):T , ˜S(t+1):T ))]\n+ E[log(1 − D(S(t+1):T , ˜S(t+1):T ))]\n}\n(14)\nLstyle =\nT −t∑\ni=1\nN/Psi1∑\nj=1\nGram/Psi1\ni, j (Si ) − Gram/Psi1\ni, j (˜Si )\n\nF (15)\nwhere D is the convolutional discriminator [45]. /Psi1is the\npretrained Visual Geometry Group (VGG)-19 network. N/Psi1is\nthe number of adopted layers for feature extraction in VGG-19\nnetwork. Subscript “F” in (15) represents the Frobenius norm\nof the matrix. Gram /Psi1\ni, j is the feature map’s Gram matrix [58],\nwhich is formed as\nGram/Psi1\ni, j (Si ) = /Psi1j (Si ) · /Psi1j (Si )T /\nC/Psi1\ni, j H /Psi1\ni, j W /Psi1\ni, j (16)\nwhere /Psi1j (Si ) ∈ RC/Psi1\ni, j ×(H /Psi1\ni, j W/Psi1\ni, j ) is the reshaped output feature\nmap from the jth VGG-19 layer, with the ith frame Si as\ninput, and /Psi1j (Si )T is the corresponding transpose matrix.\nThe total loss function Ltot is ﬁnally formulated as\nLtot = Lpred + μ1 · Lﬂow + μ2 · LGAN + μ3 · Lstyle (17)\nwhere μ1,μ2,μ3 are individual loss weights for Lﬂow, LGAN,\nand Lstyle, respectively.\nIV . EXPERIMENTS AND ANALYSES\nA. Dataset and Preprocessing\nTwo challenging real-world weather radar datasets (Storm\nEVent ImageRy (SEVIR) [59] an d standardized radar dataset\n(SRAD2018) [60]) are adopted to evaluate our model.\n1) SEVIR Dataset: The SEVIR dataset [59] is mainly\nsampled from storm events such as heavy rainfalls over the\nUS and it contains spatiotemporally aligned image sequences\nfrom the geostationary environmental satellite system (GOES)\nand the next-generation radar (NEXRAD).\nEach sequence, or “event” in SEVIR contains a 384 ×\n384 km region with a spatial resolution of 1 × 1k ma n d\nspanning a 4 h of time which is sampled in 5 min steps.\nNote that we only use NEXRAD derived vertically integrated\nliquid (VIL) data and do not use other kinds of data such as\ndata from GOES. The VIL images in SEVIR are stored as\nintegers in the range of 0–255. The converting rule between\nthese encoded integers and the true VIL data with units of\nkg/m\n2 is as follows [59]:\nVIL =\n⎧\n⎪⎨\n⎪⎩\n0, if p ≤ 5\n(p − 2)/90.66, if 5 < p ≤ 18\nexp[(p − 83.9)/38.9], if p > 18\n(18)\nwhere p is the integers stored in the images.\nOur data preprocessing process mainly contain two steps,\ni.e., event splitting and echo images ﬁltering. For the event\nsplitting, we concretize the pn task into predicting echo\nsequence of the future one hour based on previous one-hour\nobservations, i.e., for SEVIR dataset, given the previous\n12 echo images, we aim to predict the future 12 echo images.\nTherefore, as shown in Fig. 5(a), we split each SEVIR event\ninto three input-output subsequences, and each of them has\nT = 24 images spanning 2 h of time.\nFor the echo images ﬁltering, we use an average ﬁltering\nstrategy [24] to determine whether each image is a rainy image\nor not, and further determine whether a subsequence is kept or\nnot. Concretely, as shown in Fig. 5(b), a ﬁlter was convolved\nwith each image in the subsequences to detect areas of high\nrainfall intensities and the subsequence is kept if more than\nhalf of the images in the subsequence contain at least one pixel\nexceeding a predeﬁned threshold. The ﬁlter size is 1/8th of the\nimage size and the threshold for SEVIR VIL dataset is set to\n0.7 kg/m\n2 which correspond to a reﬂectivity value of 30 dBZ.\nThe sequences are divided into train dataset, validation\ndataset, and test dataset and ﬁnally we obtain 14 154 training\nsequences, 3654 validation sequences, and 4304 test sequences\nfor SEVIR dataset. The data distribution is shown in Fig. 6.\n2) SRAD2018 Dataset: The SRAD2018 dataset is from\nTianchi IEEE International Conference on Data Mining\n(ICDM) 2018 Global Artiﬁcial intelligence (AI) Challenge on\nMeteorology, collected by Shenzhen Meteorological Bureau\nand Hong Kong Observatory. Each sequence or “event” in\nSRAD2018 originally contains a 501 × 501 km region with\na spatial resolution of 1 × 1 km and spanning a 6 h of time\nwhich is sampled in 6 min steps, and is taken from an altitude\nof 3 km. The reﬂectivity values are directly stored in images\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \n5119816 IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022\nFig. 6. Histograms of the pixel values in SEVIR dataset.\nFig. 7. Histograms of the pixe l values in SRAD2018 dataset.\nin SRAD2018. Considering the limited computing resources\nto process image sequence with such spatial sizes, we crop\nthe central part of images in SRAD2018 to get echo images\nwith the size of 384 × 384, which is the same with the echo\nimage size in SEVIR.\nWe also preprocess SRAD2018 dataset by event splitting\nand echo images ﬁltering. Since images in SRAD2018 are\nsampled in 6 min’ time steps, we split each SRAD2018 event\ninto three input-output subsequences, and each of them has\nT = 20 images spanning a 2 h of time. For the echo images\nﬁltering of SRAD2018, the ﬁlter size is also 1/8th of the image\nsize and the threshold is set to 40 dBZ. Moreover, a nonlinear\nscaling operation [18] is performed on SRAD2018 dataset to\nmake converting between the reﬂ ectivity values and integers\nas follows:\ndBZ = p × 95/255 − 10 (19)\nwhere p is the integers stored in the images after the conver-\nsion.\nWe divide the sequences into train dataset, validation\ndataset, and test dataset and ﬁnally we obtain 7258 training\nsequences, 1802 validation sequences, and 1200 test sequences\nfor SRAD2018 dataset. The dataset distribution is shown in\nFig. 7.\nB. Experimental setups\n1) Evaluation Protocols: We evaluate the models with\nnowcasting skill scores, pixel-wise image evaluation indexes,\nand perceptual image e valuation indexes.\nFirst, for the nowcasting skill scores, we qualify the model\nperformance using the widely adopted metrics in the area\nof pn, including the probability of detection (POD), critical\nsuccess index (CSI), false alarm ratio (FAR), and the Heidke\nskill score (HSS) [61], [62]. Higher POD, CSI, and HSS\nvalues and lower FAR values indicate better nowcasting per-\nformance of the model. Note that the POD metric is biased to\noverestimating the size of precipitation areas while the FAR\nmetric does the opposite [63]. The HSS and CSI metrics take\ninto account of both the false ala rm rate and probabilities of\ndetection, hence HSS, CSI are mainly adopted for judging\nmodel performances while POD, FAR are mainly for analyzing\nwhy the HSS and CSI of a model are better or worse than\nanother.\nAbove metrics are actually based on the binary classi-\nﬁcation, therefore the reﬂectivity (or VIL) threshold must\nbe clariﬁed for any metrics. As shown in Figs. 5 and 6,\nthere are not enough samples satisfying the threshold of\n50 dBZ and 12.0 kg/m\n2 in the SRAD2018 and SEVIR\ndataset, respectively, hence for the SRAD2018 dataset, our\nadopted thresholds are 10, 20, 30, and 40 dBZ. For SEVIR\ndataset, our adopted thresholds for VIL are 0.140 kg/m\n2,\n0.700 kg/m2, 3.50 kg/m 2, and 6.90 kg/m 2 [64]. We can easily\nmodify these thresholds accordin g to users’ requirements in\napplications.\nAdditionally, we adopt peak signal to noise ratio (PSNR),\nMSE, and structural similarity (SSIM) [65] metrics to measure\nthe pixel-level performances . What is more, the perceptual\nevaluation metrics including l earned perceptual image patch\nsimilarity (LPIPS) [66] and Frechet inception distance (FID)\n[67] are also considered. LPIPS is a perceptual metric which\nindicates the perceptual similarity between two images ranging\nfrom 0 to 1, and it is considered to be similar with human\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \nDONG et al.: MOTION-GUIDED GLOBAL-LOCAL AGGREGATION TRANSFORMER NETWORK 5119816\nTABLE I\nPERFORMANCE COMPARISONS OF DIFFERENT DL-B ASED MODELS FOR PN TASK UNDER METRICS OF MSE, PSNR, SSIM, LPIPS, AND FID.\n↑ DENOTES THE HIGHER THE BETTER , AND ↓ DENOTES THE LOWER THE BETTER .T HE BEST PERFORMANCE UNDER SPECIFIC SETTINGS IS\nMARKED WITH BOLD RED .T HE SECOND -BEST PERFORMANCE IS MARKED WITH BOLD BLUE (SAME BELLOW )\nTABLE II\nPERFORMANCE COMPARISONS OF DIFFERENT DL-B ASED MODELS FOR PN TASK UNDER POD AND CSI M ETRICS ON SEVIR D ATAS ET\nTABLE III\nPERFORMANCE COMPARISONS OF DIFFERENT DL-B ASED MODELS FOR PN TASK UNDER HSS AND FAR M ETRICS ON SEVIR D ATAS ET\nFig. 8. CSI and HSS scores against different now casting lead times under different VIL thresholds on SE VIR dataset. Results in the upper row show CSI\nscores. Results in the lower row show HSS scores.\nobjects’ recognition system. The FID indicates the generated\nimages clarity.\n2) Model Conﬁgurations: For the prediction branch,\nwe adopt an encoder-forecaster structure with four ConvLSTM\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \n5119816 IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022\nFig. 9. Visualizations of a convective organization process in SEVIR data set. (a) Observations. (b) Our proposed model trained with GAN strategy. (c )O u r\nproposed model trained without GAN strategy. (d) rainymotion. (e) ConvLSTM. (f) PredRNN. (g ) IDA-LSTM. (h) Conv-TT-LSTM. (i) U-Net.\nTABLE IV\nPERFORMANCE COMPARISONS OF DIFFERENT DL-B ASED MODELS FOR PN TASK UNDER POD AND CSI M ETRICS ON SRAD2018 D ATAS ET\nlayers (i.e., J = 4 in Fig. 1), with the number of hidden\nstates for the RNNs setting to 16, 64, 128, and 128. For the\nmotion module, the output channel of the ﬁrst Res-block is set\nto 32. Then, each time the feature maps are passed through\nthe down sampling block, their channel dimension is doubled.\nThe rest three up sampling blocks keep the channel dimension\nunchanged. The ﬁrst three 1 × 1 × 1 convolutional layers are\nused to adjust channel dimensions for the skip connections.\nThe channel dimension of the last 1 × 1 × 1 convolutional\nlayer is set to 2 for the 2-D optical ﬂow vector estimation. For\nTDs, the channel dimensions of the input and output tokens\nare set to 128. We set the length of long-term sequence as\nthe same with the total length of subsequences in training\ndata.\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \nDONG et al.: MOTION-GUIDED GLOBAL-LOCAL AGGREGATION TRANSFORMER NETWORK 5119816\nTABLE V\nPERFORMANCE COMPARISONS OF DIFFERENT DL-B ASED MODELS FOR PN TASK UNDER HSS AND FAR M ETRICS ON SRAD2018 D ATAS ET\nFig. 10. CSI and HSS scores against different now casting lead times under different VIL thresholds on SRAD dataset. Results in the upper row show CSI\nscores. Results in the lower row show HSS scores.\nOur adopted discriminator consists of six convolutional\nlayers, of which the kernel size of the ﬁrst ﬁve layers is set\nto 4, the stride is set to 2, and each of them is with a ReLU\nlayer behind. The kernel size of the last convolution layer\nis set to 1 and the stride is set to 1. For the ﬁrst layer, the\noutput channel dimension is set to 64 and then after each\nconvolutional layer, the number of channels is doubled.\nFour other recent competitive ConvRNN-based models\nincluding ConvLSTM [12], PredRNN [14], convolutional\ntensor-train LSTM (Conv-TT-LSTM) [26], and IDA-LSTM\n[18] and one convolutional model U-Net [8] are implemented\nto compare performances. We a lso use an optical ﬂow-based\nmethod (rainymotion) [7] for comparison.\nThe models are implemented by PyTorch framework on a\nserver which is equipped with three NVIDIA TITAN RTX\ngraphics cards. The Adam optimizer is adopted. Batch-size\nis set to 4 and learning rate is set to 0.0001. Except for\nU-Net, we train each of the other models for 10 000 iterations,\nsince we observe obvious nonconvergence after 10 000 itera-\ntions on SRAD2018 dataset when training U-Net. Therefore,\nanother 10 000 iterations are added when we train U-Net on\nSRAD2018 dataset.\nNote that due to the adoption of different datasets in which\nthe dataset size and data distrib ution are signiﬁcantly various,\nand the limited computing resources, when implementing\nreference methods, sometimes we cannot keep the parameters\nconsistent with that given in the literature and put the same\neffort to optimize all these models. Hence, there is still the\npossibility of performance biases and these biases are hard to\nbe eliminated completely.\nC. Quantitative and Qualitative Results\n1) Results on SEVIR Dataset:We ﬁrst analyze quantitative\nresults. Table I lists performance comparisons under MSE,\nSSIM, PSNR, LPIPS, and FID metrics. Our model trained\nwithout GAN strategy achieves the best result for the pixel-\nwise metrics. When trained with the GAN strategy, our model\nis no longer optimal among pixel-wise indicators; however,\nit performs either the best or the second best in perceptual\nevaluation indicators including LPIPS and FID. The perceptual\nquality of predictions is signiﬁcantly improved, which is\ncomparable to the optical ﬂow method.\nTables II and III list nowcastin g skill score performance\ncomparisons under POD, CSI, HSS and FAR metrics. Our\nproposed model when trained without GAN strategy out-\nperforms all other reference methods in terms of HSS and\nCSI scores, and it maintains the FAR scores at a relatively\nlow level at the same time. The HSS and CSI scores at\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \n5119816 IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022\nFig. 11. Visualizations of a convective organization process in SRAD datas et. (a) Observations. (b) Our proposed model trained with GAN strategy. (c )O u r\nproposed model trained without GAN strategy. (d) rainymotion. (e) ConvLSTM. (f) PredRNN. (g ) IDA-LSTM. (h) Conv-TT-LSTM. (i) U-Net.\n3.5 and 6.9 kg/m 2 thresholds are signiﬁcantly better than\nother methods. These results imply that our model is more\nadvanced in nowcasting heavy rainfalls, which is challenging\nfor other models. When trained with the GAN strategy, there is\na slight drop in HSS and CSI indicator except for the 3.5 and\n6.9 kg/m\n2 thresholds. We think this performance ﬂuctuation\nwhen introducing the GAN losse s is acceptable since the\nimage clarity is ameliorated, and the predictions provides more\nprecipitation details.\nFig. 8 shows CSI and HSS scores against different now-\ncasting lead times over different thresholds. From Fig. 8,\nwe intuitively see that the U-Net performs the worst among\nall thresholds and during all nowcasting lead times. For other\nConvRNN-based models, ConvLSTM get better nowcasting\nresults than PredRNN, IDA-LSTM, and Conv-TT-LSTM in\nthe ﬁrst 12 min’ lead time period. However, the perfor-\nmance of ConvLSTM drops sharply after 12 min. Perfor-\nmances of PredRNN, IDA-LSTM, and Conv-TT-LSTM on\nSEVIR dataset are similar and PredRNN gets slightly higher\nHSS and CSI scores than IDA-LSTM and Conv-TT-LSTM\nduring 6 to 48 min’ lead time. However, the results of\nall these models are inferior to those of our proposed\nmethod.\nFor qualitative analyses, we visualize a convective orga-\nnization process selected from SEVIR dataset. As shown in\nFig. 9, we focus on the area enclosed by the black circles and\nblack rectangle. These areas are with strong rainfall intensities\neven exceeding a value of 32 kg/m\n2 during the convective\norganization process. The rainfall ﬁeld during this procedure\nhas complex motion patterns. Our proposed model alleviates\nthe problem of underestimating the intensity and spatial area\nof heavy rainfall, no matter with or without the introduction\nof GAN losses, while other models tend to underestimate\nthe strong rainfall. In addition, the image clarity is improved\nsigniﬁcantly when trained with GAN loss, and there are more\nprecipitation details compared with other blurry predictions.\nThe optical ﬂow method also pred icts clear images; however,\nthe distortions appear and worsen as nowcasting lead time\nincreases. What is more, alt hough the perceptual quantity is\nimproved with the introduction of GAN loss, we could tell\nfrom Fig. 9 that there are still inaccurate nowcasting regions.\nThis indicates that the image perceptual quality such as the\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \nDONG et al.: MOTION-GUIDED GLOBAL-LOCAL AGGREGATION TRANSFORMER NETWORK 5119816\nclarity is not always positively correlated with nowcasting skill\nscores.\n2) Results on SRAD2018 Dataset: As shown in Tables IV\nand V , performances of our proposed model using HSS, CSI\nmetrics under all the adopted thresholds are superior to those\nof other models. The optical ﬂow method is characterized by\nhigh POD values while high FAR values at the same time,\nresulting in low HSS and CSI values. When trained with the\nGAN strategy, there is a slight performance drop in both HSS\nand CSI.\nFig. 10 shows the CSI and HSS scores against different\nnowcasting lead times under different reﬂectivity thresholds\non SRAD2018 dataset. For the results under 40 dBZ threshold,\nConvLSTM shows good scores before a nowcasting lead time\nof 18 min but its performance drops rapidly after 18 min.\nConv-TT-LSTM does not obtain competitive results in the ﬁrst\n12 min but achieves similar performances with PredRNN and\nIDA-LSTM during 12 to 60 min, under both 30 and 40 dBZ\nthresholds. Different with results on SEVIR dataset, the U-Net\nmodel even gets competitive nowcasting scores with other\nConvRNN-based models. Our model does not show signiﬁcant\nimprovements under the 30 dBZ threshold, where IDA-LSTM\nachieves the best scores at a nowcasting lead time of 1 h.\nHowever, when we raise the threshold to 40 dBZ, HSS and\nCSI scores of our model is obviously superior to others. The\noptical ﬂow method still has good POD scores while poor FAR\nscores, as a result, there HSS and CSI scores are the worst at\nlong nowcasting lead times.\nFig. 11 shows a nowcasting result comparison of a develop-\ning squall line system. We mainly focus on the four enclosed\nareas with solid black lines. The oblique black rectangle\nencloses the main structure of the squall line. All of the\nmodels nowcast the general structure of the squall line to a\ncertain extent, but the prediction results of our models (trained\nwith or without GAN loss) are more consistent with the\nreal observations in terms of echo intensity and echo pattern\nevolutions. The left vertical black rectangle encloses an area\nof convection generation, and only our models successfully\nnowcast this convection initiation compared with other meth-\nods. The area enclosed by the horizontal black ellipse indicates\nthe convective core strengthen, and only our models capture\nand predict this trend of echo int ensity increasing precisely.\nThis case study shows our model is more sensitive to complex\necho motions such as convection g eneration and deformation,\nespecially in strong rainfall cases, no matter with or without\nthe introduction of GAN loss. It is interesting that although\nthe optical ﬂow method seems to predict sharp and clear\nimages, however, the quantitative metrics are not so satisﬁed.\nImportantly, the predictions from the addition of GAN loss\ncontains more echo structures although there are not obvious\nimprovements in the nowcasting skill scores.\n3) Effects of the Optical Flow Guidance: Fig. 12 shows\nan example of the learned optical ﬂows and corresponding\nground-truth optical ﬂows calculated from EpicFlow. From\nFig. 12, we know that the learned optical ﬂow is consistent\nwith the ground truth in the overall motion trend. This ensures\na reasonable echo sequence MR learning. To further evaluate\nthe importance of the optical ﬂow guidance, we make the\nFig. 12. (a) and (b) Two consecutive echo frames from SEVIR dataset.\n(c) Ground-truth optical ﬂow calculate d from Epicﬂow. (d) Learned optical\nﬂow by our motion module. In the middle, we show the color wheel.\nThe different colors indicate the motion directions, and the color intensity\nrepresents the displacement’s magnitude.\nTABLE VI\nE\nFFECTS OF ADOPTING OPTICAL FLOW AS MOTION GUIDANCE ON\nPERFORMANCE OF NOWCASTING QUALITIES\nfollowing ablation studies. In the aggregation branch of our\nmodel, we only adopt the encoder part of the motion module\nbut without the decoder part and the supervision of the\nground-truth optical ﬂow. As listed in Table VI, for both\ndatasets, after we remove the optical ﬂow as motion guidance,\nthe performances under heavy rainfall thresholds have different\ndegrees of decline. This indicates the effectiveness of adopting\na clear and reasonable motion guidance for improving now-\ncasting quality.\nV. D\nISCUSSION AND CONCLUSION\nIn this article, we proposed a motion-guided global–local\naggregation Transformer network for improving pn quality.\nDifferent with previous convolutional structures or ConvRNN\nmodels for pn, on the one hand, we innovatively explore\nthe Transformer architecture for an effective and efﬁcient\ncombination of spatiotemporal cues at different time scales,\nthereby further enhancing the global–local aggregation which\nis desperately required by pn task.\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \n5119816 IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022\nOn the other hand, we notice that previous Transformer\narchitecture lacks the guidance of motion information when\nperforming attention calculations. To introduce reasonable\nmotion guidance, we customize an end-to-end learning module\nfor jointly extracting MR of echo sequences while estimating\noptical ﬂow. This has the following beneﬁts. First, an end-\nto-end learning manner avoids nontrivial computation burden\nof calculating optical ﬂow. What is more, using optical ﬂow\nas priori motion guidance forces our model learning latent\nMRs which are proper for nowcasting, and further beneﬁts\nour model nowcasting ﬁne-grained echo pattern evolutions\nprecisely. Additionally, we do not use optical ﬂow to extrapo-\nlate linearly. This enhances the nowcasting robustness of our\nmodel.\nFurthermore, for the blurry prediction problem, we intro-\nduce the GAN training strategy to the proposed model. The\nexperimental results show that the introduction of GAN loss\nhelp improve the predictions’ perceptual quality and image\nclarity notably while the nowcasting skill scores are slightly\nunstable and maybe with acceptable performance drop.\nFor the evaluation metrics, many existing methods only\nadopts the pixel-wise indicators and the nowcasting skill scores\nfor performance evaluation. As our experimental show, the\nblurry predictions sometimes have close HSS and CSI scores\nand better pixel-wise evaluation scores, compared with the\nclear results. However, the clear predictions which have rich\necho structure details provide better reference for reﬁned and\naccurate nowcasting. Hence, we propose to use nowcasting\nskill scores as the main evaluation metrics while use pixel-wise\nevaluation indicators (such as MSE) as optional insigniﬁcant\nreferences. What is more, when the skill score indicators of\ndifferent models are almost equivalent, it is recommended to\nuse perceptual indicators such as the clarity indicator FID for\nfurther judgment.\nR\nEFERENCES\n[1] J. Johnson et al., “The storm cell i dentiﬁcation and tr acking algorithm:\nAn enhanced WSR-88D algorithm,” Weather Forecast., vol. 13, no. 2,\npp. 263–276, 1998.\n[2] M. Dixon and G. Wiener, “TITAN: Thunderstorm identiﬁcation, track-\ning, analysis, and nowcasting—A radar-based methodology,” J. Atmos.\nOcean. Technol., vol. 10, no. 6, pp. 785–797, Dec. 1993.\n[3] R. Rinehart and E. Garvey, “Three-dimensional storm motion detection\nby conventional weather radar,”Nature, vol. 273, no. 5660, pp. 287–289,\n1978.\n[4] H. Sakaino, “Spatio–temporal imag e pattern prediction method based on\na physical model with time-varying optical ﬂow,” IEEE Trans. Geosci.\nRemote Sens., vol. 51, no. 5, pp. 3023–3036, May 2013.\n[5] N. E. Bowler, C. E. Pierce, and A. Seed, “Development of a precipitation\nnowcasting algorithm based upon optical ﬂow techniques,” J. Hydrol.,\nvol. 288, nos. 1–2, pp. 74–91, Mar. 2004.\n[6] W.-C. Woo and W.-K. Wong, “Operational application of optical ﬂow\ntechniques to radar-based rainfall nowcasting,” Atmosphere,v o l .8 ,\nno. 12, p. 48, Feb. 2017.\n[7] G. Ayzel, M. Heistermann, and T. Winterrath, “Optical ﬂow models as an\nopen benchmark for radar-based preci pitation nowcasting (rainymotion\nv0. 1),” Geosci. Model Develop., vol. 12, no. 4, pp. 1387–1402, 2019.\n[8] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional net-\nworks for biomedical image segmentation,” in Proc. Int. Conf. Med.\nImage Comput. Comput.-Assist. Intervent. Cham, Switzerland: Springer,\n2015, pp. 234–241.\n[9] K. Trebing, T. Sta` nczyk, and S. Mehrkanoon, “SmaAt-UNet: Precip-\nitation nowcasting using a small attention-UNet architecture,” Pattern\nRecognit. Lett., vol. 145, pp. 178–186, May 2021.\n[10] X. Pan, Y . Lu, K. Zhao, H. Huang, M. Wang, and H. Chen, “Improving\nnowcasting of convective developmen t by incorporating polarimetric\nradar variables into a deep-learning model,” Geophys. Res. Lett., vol. 48,\nno. 21, Nov. 2021, Art. no. e2021GL095302.\n[11] G. Ayzel, M. Heistermann, A. Sorokin, O. Nikitin, and O. Lukyanova,\n“All convolutional neural networks for radar-based precipitation now-\ncasting,” Proc. Comput. Sci., vol. 150, pp. 186–192, Jan. 2019.\n[12] X. Shi, Z. Chen, H. Wang, D.-Y . Yeung, W.-K. Wong, and W.-C. Woo,\n“Convolutional LSTM network: A machine learning approach for pre-\ncipitation nowcasting,” in Proc. Adv. Neural Inf. Process. Syst., vol. 28,\n2015, pp. 802–810.\n[13] X. Shi et al., “Deep learning for precipitation nowcasting: A benchmark\nand a new model,” in Proc. Adv. Neural Inf. Process. Syst., vol. 30,\n2017, pp. 5622–5632.\n[14] Y . Wang, M. Long, J. Wang, Z. Gao, and P. S. Yu, “PredRNN: Recurrent\nneural networks for predictive learning using spatiotemporal LSTMs,”\nin Proc. Adv. Neural Inf. Process. Syst., vol. 30, 2017, pp. 879–888.\n[15] Y . Wang, Z. Gao, M. Long, J. Wang, and S. Y . Philip, “PredRNN++:\nTowards a resolution of the deep-in-time dilemma in spatiotemporal\npredictive learning,” in Proc. Int. Conf. Mach. Learn. (PMLR), 2018,\npp. 5123–5132.\n[16] Y . Wang, J. Zhang, H. Zhu, M. Long, J. Wang, and P. S. Yu, “Memory\nin memory: A predictive neural netw ork for learning higher-order non-\nstationarity from spatiotemporal dynamics,” in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 9154–9162.\n[17] H. Wu, Z. Yao, J. Wang, and M. Long, “MotionRNN: A ﬂexible\nmodel for video prediction with spacetime-varying motions,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021,\npp. 15435–15444.\n[18] C. Luo, X. Li, Y . Wen, Y . Ye, and X. Zhang, “A novel LSTM model with\ninteraction dual attention for radar echo extrapolation,” Remote Sens.\n,\nvol. 13, no. 2, p. 164, Jan. 2021.\n[19] T. Yu, Q. Kuang, and R. Yang, “ATMConvGRU for weather forecasting,”\nIEEE Geosci. Remote Sens. Lett., vol. 19, pp. 1–5, 2022.\n[20] T. Xiong, J. He, H. Wang, X. Tang, Z. Shi, and Q. Zeng, “Contextual\nsa-attention convolutional LSTM for precipitation nowcasting: A spa-\ntiotemporal sequence forecasting view,” IEEE J. Sel. Topics Appl. Earth\nObserv. Remote Sens., vol. 14, pp. 12479–12491, 2021.\n[21] C. Zhang, X. Zhou, X. Zhuge, and M. Xu, “Learnable optical ﬂow\nnetwork for radar echo extrapolation,” IEEE J. Sel. Topics Appl. Earth\nObserv. Remote Sens., vol. 14, pp. 1260–1266, 2021.\n[22] J. Han et al., “Weight loss function for the cooperative inversion\nof atmospheric duct parameters,” Atmosphere, vol. 13, no. 2, p. 338,\nFeb. 2022.\n[23] L. Han, H. Liang, H. Chen, W. Zhang, and Y . Ge, “Convective precip-\nitation nowcasting using U-Net model,” IEEE Trans. Geosci. Remote\nSens., vol. 60, pp. 1–8, 2022.\n[24] J. Cuomo and V . Chandrasekar, “Developing deep learning models for\nstorm nowcasting,”IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–13,\n2022.\n[25] W. Byeon, Q. Wang, R. K. Srivastava, and P. Koumoutsakos, “Con-\ntextVP: Fully context-aware video prediction,” in Proc. Eur. Conf.\nComput. Vis. (ECCV), Sep. 2018, pp. 753–769.\n[26] J. Su, W. Byeon, J. Kossaiﬁ, F. Huang, J. Kautz, and A. Anandkumar,\n“Convolutional tensor-train LSTM for spatio-temporal learning,” in\nProc. Adv. Neural Inf. Process. Syst., vol. 33, 2020, pp. 13714–13726.\n[27] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 30, 2017, pp. 1–11.\n[28] A. Dosovitskiy et al., “An image is worth 16 ×16 words: Transformers\nfor image recognition at scale,” 2020, arXiv:2010.11929.\n[29] G. Bertasius, H. Wang, and L. Torresani, “Is space-time attention all\nyou need for video understanding,” in Proc. Int. Conf. Mach. Learn.\n(ICML), vol. 2, no. 3, 2021, p. 4.\n[30] S. Hochreiter and J. Schmi dhuber, “Long short-term memory,” Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[31] A. Dosovitskiy et al., “FlowNet: Learning optical ﬂow with convo-\nlutional networks,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) ,\nDec. 2015, pp. 2758–2766.\n[32] D. Sun, X. Yang, M.-Y . Liu, and J. Kautz, “PWC-Net: CNNs for optical\nﬂow using pyramid, warping, and cost volume,” in Proc. IEEE/CVF\nConf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 8934–8943.\n[33] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Deep\nend2end voxel2 V oxel prediction,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. Workshops (CVPRW), Jun. 2016, pp. 17–24.\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \nDONG et al.: MOTION-GUIDED GLOBAL-LOCAL AGGREGATION TRANSFORMER NETWORK 5119816\n[34] J. Y .-H. Ng, J. Choi, J. Neumann, and L. S. Davis, “ActionFlowNet:\nLearning motion representation for action recognition,” in Proc. IEEE\nWinter Conf. Appl. Comput. Vis. (WACV), Mar. 2018, pp. 1616–1624.\n[35] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in Proc.\nEur. Conf. Comput. Vis. (ECCV), Aug. 2020, pp. 213–229.\n[36] J. He, L. Zhao, H. Yang, M. Zhang, and W. Li, “HSI-BERT: Hyperspec-\ntral image classiﬁcation using the bidirectional encoder representation\nfrom transformers,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 1,\npp. 165–178, Sep. 2019.\n[37] H. Chen, Z. Qi, and Z. Shi, “Remote sensing image change detection\nwith transformers,”IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–14,\n2022.\n[38] Q. Wang, W. Huang, X. Zhang, and X. Li, “Word–sentence framework\nfor remote sensing image captioning,” IEEE Trans. Geosci. Remote\nSens., vol. 59, no. 12, pp. 10532–10543, Dec. 2021.\n[39] M. Mathieu, C. Couprie, and Y . LeCun, “Deep multi-scale video pre-\ndiction beyond mean square error,” in Proc. Int. Conf. Learn. Represent.\n(ICLR), 2016, pp. 1–48.\n[40] S. Ravuri et al., “Skilful precipitation nowcasting using deep generative\nmodels of radar,” Nature, vol. 597, no. 7878, pp. 672–677, Sep. 2021.\n[41] Y . Hu, L. Chen, Z. Wang, X. Pan, and H. Li, “Towards a more realistic\nand detailed deep-learning-based r adar echo extrapolation method,”\nRemote Sens., vol. 14, no. 1, p. 24, Dec. 2021.\n[42] P. Xie et al., “An energy-based generative adversarial forecaster for\nradar echo map extrapolation,” IEEE Geosci. Remote Sens. Lett., vol. 19,\npp. 1–5, 2022.\n[43] C. Wang, P. Wang, P. Wang, B. Xue, and D. Wang, “Using conditional\ngenerative adversarial 3-D convolutional neural network for precise radar\nextrapolation,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.,\nvol. 14, pp. 5735–5749, 2021.\n[44] Y . Kim and S. Hong, “Very short-term rainfall prediction using\nground radar observations and conditional generative adversarial\nnetworks,” IEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1–8,\n2022.\n[45] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation\nwith conditional adversarial networks,” inProc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jul. 2017, pp. 1125–1134.\n[46] M. Wolfson et al., “Tactical 0–2 hour convective weather forecasts for\nFAA,” in Proc. 11th Conf. Aviation, Range Aerosp. Meteorol., 2004,\npp. 1–35.\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep residual\nnetworks,” inProc. Eur. Conf. Comput. Vis. (ECCV). Cham, Switzerland:\nSpringer, 2016, pp. 630–645.\n[48] M. Drozdzal, E. V orontsov, G. Ch artrand, S. Kadoury, and C. Pal, “The\nimportance of skip connections in biomedical image segmentation,” in\nDeep Learning and Data Labeling for Medical Applications .C h a m ,\nSwitzerland: Springer, 2016, pp. 179–187.\n[49] E. Orhan and X. Pitkow, “Skip connections eliminate singularities,” in\nProc. Int. Conf. Learn. Represent. (ICLR), 2018, pp. 1–22.\n[50] S. Lee, H. G. Kim, D. H. Choi, H.-I. Kim, and Y . M. Ro, “Video\nprediction recalling long-term motion context via memory alignment\nlearning,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2021, pp. 3054–3063.\n[51] D. Gong et al., “Memorizing normality to detect anomaly: Memory-\naugmented deep autoencoder for unsupervised anomaly detection,”\nin Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV) , Oct. 2019,\npp. 1705–1714.\n[52] S. Woo, J. Park, J. Y . Lee, and I. S. Kweon, “CBAM: Convolutional\nblock attention module,” in Proc. Eur. Conf. Comput. Vis. (ECCV),\nSep. 2018, pp. 3–19.\n[53] K. Xu et al., “Show, attend and tell: Neural image caption genera-\ntion with visual attention,” Comput. Sci., vol. 2015, pp. 2048–2057,\nFeb. 2015.\n[54] T. Luong, H. Pham, and C. D. Manning, “Effective approaches to\nattention-based neural machine translation,” in\nProc. Conf. Empirical\nMethods Natural Lang. Process., 2015, pp. 1412–1421.\n[55] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid, “EpicFlow:\nEdge-preserving interpolation of c orrespondences for optical ﬂow,” in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015,\npp. 1164–1172.\n[56] I. J. Goodfellow et al., “Generative adversarial nets,” in Proc. NIPS,\n2014, pp. 1–47.\n[57] L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style transfer using\nconvolutional neural networks,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2016, pp. 2414–2423.\n[58] J. Johnson, A. Alahi, and L. Fei-Fei , “Perceptual losses for real-time\nstyle transfer and super-resolution,” in Proc. Eur. Conf. Comput. Vis.\n(ECCV), Oct. 2016, pp. 694–711.\n[59] M. Veillette, S. Samsi, and C. Mattioli, “SEVIR: A storm event imagery\ndataset for deep learning applications in radar and satellite meteorol-\nogy,” in Proc. Adv. Neural Inf. Process. Syst. (NIPS), vol. 33, 2020,\npp. 22009–22019.\n[60] IEEE ICDM 2018 Global AI Challenge on Meteorology .\nAccessed: Nov. 5, 2022. [Online]. Available: https://tianchi.aliyun.\ncom/competition/entrance/231662/information?lang=en-us\n[61] M. Chen, B. Bica, L. Tüchler, A. Kann, and Y . Wang, “Statistically\nextrapolated nowcasting of summertime precipitation over the eastern\nAlps,” Adv. Atmos. Sci., vol. 34, no. 7, pp. 925–938, Jul. 2017.\n[62] R. Donaldson, R. M. Dyer, and M. J. Kraus, “An objective evaluator\nof techniques for predicting severe weather events,” in Proc. 9th Conf.\nSevere Local Storms, vol. 321326, 1975, pp. 321–326.\n[63] R. J. Hogan, C. A. Ferro, I. T. Jolliffe, and D. B. Stephenson, “Equitabil-\nity revisited: Why the ‘equitable threat score’ is not equitable,” Weather\nForecasting, vol. 25, no. 2, pp. 710–726, 2010.\n[64] M. Robinson, J. Evans, and B. Cro we, “En route weather depiction\nbeneﬁts of the NEXRAD vertically integrated liquid water product\nutilized by the corridor integrated weather system,” in Proc. 10th Conf.\nAviation, Range Aerosp. Meteorol., 2002, pp. 1–4.\n[65] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for image\nrestoration with neural networks,” IEEE Trans. Comput. Imag.,v o l .3 ,\nno. 1, pp. 47–57, Mar. 2017.\n[66] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The\nunreasonable effectiveness of deep features as a perceptual metric,”\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018,\npp. 586–595.\n[67] M. Heusel, H. Ramsauer, T. Untert hiner, B. Nessler, and S. Hochreiter,\n“GANs trained by a two time-scale update rule converge to a local nash\nequilibrium,” Proc. Int. Conf. Neural Inf. Process. Syst. (NIPS), vol. 30,\n2017, pp. 6629–6640.\nXichao Dong (Member, IEEE) r eceived the B.S.\ndegree in electrical engineering and the Ph.D. degree\nin target detection and recognition from the Beijing\nInstitute of Technology (BIT), Beijing, China, in\n2008 and 2014, respectively.\nFrom 2011 to 2013, he was a Research Assistant\nwith CTCD, University of Shefﬁeld, Shefﬁeld, U.K.\nFrom 2014 to 2017, he was in a postdoctoral position\nwith the School of Information and Electronics, BIT.\nIn 2017, he joined the Teaching Staff of BIT. From\nDecember 2019, he was also with the Beijing Insti-\ntute of Technology Chongqing Innovation Center, Chongqing, China. Since\n2021, he has been an Associate Professor at BIT. His research interests include\ngeosynchronous synthetic aperture radar,signal processing, and weather radar.\nDr. Dong was a recipient of the IEEE Chinese Institute of Electronics\n(CIE) International Radar Conference Excellent Paper Award in 2011 and\nthe Chinese Institute of Electronics Youth Conference Poster Award in 2014.\nZewei Zhao was born in Shandong, China, in 1997.\nHe received the B.S. degree in information engineer-\ning from the Beijing Institute of Technology, Beijing,\nChina, in 2018, where he is currently pursuing the\nPh.D. degree in signal and information processing\nwith the Radar Research Laboratory.\nHis research interests mainly include signal\nprocessing and weather radar.\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. \n5119816 IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 60, 2022\nYupei Wang received the Ph.D. degree from the\nNational Laboratory of Pattern Recognition (NLPR),\nInstitute of Automation, Chinese Academy of Sci-\nences (CASIA), Beijing, China, in 2019.\nIn July 2019, he joined the Beijing Institute of\nTechnology, Beijing, where he is currently an Assis-\ntant Professor. His research interests include com-\nputer vision, deep learning, semantic segmentation,\nand remote sensing image analysis.\nJianping Wang received the Ph.D. degree in elec-\ntrical engineering from the Delft University of Tech-\nnology, Delft, The Netherlands, in 2018.\nFrom August 2012 to April, 2013, he worked\nas a Research Associate with the University of\nNew South Wales, Sydney, Australia, on frequency\nmodulated continuous wave synthetic aperture radar\nsignal processing for formation ﬂying satellites.\nHe is currently a Post-Doctoral Researcher with the\nGroup of Microwave Sensing, Signals and Systems\n(MS3), Delft University of Technology. His research\ninterests include microwave imaging, signal processing, and antenna array\ndesign.\nDr. Wang was a Technical Program Committee (TPC) Member of the\nInstitution of Engineering and Technology (IET) International Radar Con-\nference, Nanjing, China, in 2018. He wa s a Finalist of the Best Student Paper\nAward in International Workshop on Advanced Ground Penetrating Radar\n(IWAGPR), Edinburgh, U.K., in 2017, and the International Conference on\nRadar, Brisbane, Australia, in 2018. He has served as a reviewer of many\nIEEE journals.\nCheng Hu (Senior Member, IEEE) received the B.S.\ndegree in electronic engineering from the National\nUniversity of Defense Technology, Changsha, China,\nin July 2003, and the Ph.D. degree in target detection\nand recognition from the Beijing Institute of Tech-\nnology (BIT), Beijing, China, in July 2009.\nHe was a Visiting Research Associate with the\nUniversity of Birmingha m, Birmingham, U.K., for\n15 months from 2006 to 2007. In September 2009,\nhe joined the School of Information and Electronics,\nBIT, where he was promoted to be a Full Professor\nin 2014. He has published over 60 science citation index (SCI)-indexed journal\narticles and over 100 conference papers . His main research interests include\nthe new concept of synthetic aperture radar imaging, biological detection radar\nsystems, and signal processing.\nAuthorized licensed use limited to: TU Delft Library. Downloaded on November 18,2022 at 10:04:07 UTC from IEEE Xplore.  Restrictions apply. ",
  "topic": "Nowcasting",
  "concepts": [
    {
      "name": "Nowcasting",
      "score": 0.8939427137374878
    },
    {
      "name": "Computer science",
      "score": 0.7385969161987305
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5658025145530701
    },
    {
      "name": "Extrapolation",
      "score": 0.534307599067688
    },
    {
      "name": "Feature learning",
      "score": 0.4771135151386261
    },
    {
      "name": "Radar",
      "score": 0.4642781615257263
    },
    {
      "name": "Deep learning",
      "score": 0.43893975019454956
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4294296205043793
    },
    {
      "name": "Radar imaging",
      "score": 0.4253786504268646
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3823551535606384
    },
    {
      "name": "Machine learning",
      "score": 0.36954545974731445
    },
    {
      "name": "Computer vision",
      "score": 0.3573480546474457
    },
    {
      "name": "Meteorology",
      "score": 0.14408141374588013
    },
    {
      "name": "Mathematics",
      "score": 0.12600833177566528
    },
    {
      "name": "Geography",
      "score": 0.11960440874099731
    },
    {
      "name": "Telecommunications",
      "score": 0.09916287660598755
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I98358874",
      "name": "Delft University of Technology",
      "country": "NL"
    }
  ]
}