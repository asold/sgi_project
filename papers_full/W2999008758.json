{
  "title": "Faster Transformer Decoding: N-gram Masked Self-Attention",
  "url": "https://openalex.org/W2999008758",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5068010225",
      "name": "Ciprian Chelba",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5025244406",
      "name": "Mia Xu Chen",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5024316712",
      "name": "Ankur Bapna",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5021878400",
      "name": "Noam Shazeer",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2799001369",
    "https://openalex.org/W2902608666",
    "https://openalex.org/W2802023636",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2988394319",
    "https://openalex.org/W2928941594"
  ],
  "abstract": "Motivated by the fact that most of the information relevant to the prediction of target tokens is drawn from the source sentence $S=s_1, \\ldots, s_S$, we propose truncating the target-side window used for computing self-attention by making an $N$-gram assumption. Experiments on WMT EnDe and EnFr data sets show that the $N$-gram masked self-attention model loses very little in BLEU score for $N$ values in the range $4, \\ldots, 8$, depending on the task.",
  "full_text": "arXiv:2001.04589v2  [cs.LG]  18 Dec 2024\nFaster Transformer Decoding: N-gram Masked Self-Attentio n\nCiprian Chelba and Mia Chen and Ankur Bapna and Noam Shazeer\nGoogle, Inc.\n1600 Amphitheatre Parkway\nMountain V iew , CA 94043, USA\n{ciprianchelba,miachen,ankurbpn,noam}@google.com\nDecember 19, 2019\nAbstract\nMotivated by the fact that most of the information relevant t o the prediction of target tokens is drawn from the\nsource sentence S = s1, . . . , s S, we propose truncating the target-side window used for comp uting self-attention\nby making an N-gram assumption. Experiments on WMT EnDe and EnFr data sets show that the N-gram\nmasked self-attention model loses very little in BLEU score for N values in the range 4, . . . , 8, depending on the\ntask.\n1 Introduction\nTransformers (V aswani et al., 2017) are the most effective n eural architectures for sequence modeling problems\nencountered in natural language, in particular language mo deling and machine translation (MT). For MT in par-\nticular, the most successful modeling paradigm predicts a g iven target word using a conditional probability model\nleveraging all source words and the previous target words.\nA ﬁrst empirical observation is that the perplexity of a lang uage model (LM) that predicts the target sentence\nusing P (tk|t1, . . . , t k− 1; θLM ) is signiﬁcantly higher than that of a conditional neural tra nslation model (NMT):\nP (tk|t1, . . . , t k− 1, s 1, . . . , s S; θNMT ). A transformer NMT model (6 layers, 8 attention heads, 512 mo del/embedding,\n2048 hidden dimensionality , respectively , dropout 0.1) tr ained on the WMT EnDe data used for quality evaluation\nas described in Section 4.1.1 of (W ang et al., 2018) achieves conditional perplexity (PPL) 13.5 on the newstest2017\ntest data. An LSTM LM (2-layer, 1024 embedding and 2048 state dimensionality , respectively) trained on the\nDe monolingual side of the parallel data, using the same word -piece model/vocabulary as the NMT model (32k,\nbilingual), achieves PPL 99.5.\nMotivated by the fact that most of the information relevant t o the prediction of target token tk is drawn from the\nsource sentence S = s1, . . . , s S, we propose truncating the target-side window used for comp uting self-attention by\nmaking an N-gram assumption. The self-attention mechanism in transfo rmer models (V aswani et al., 2017) already\nemploys masking to make sure only target tokens prior to the c urrent predicted position k are used when estimating\nP (tk|t1, . . . , t k− 1; s1, . . . , s S; θNMT ). Our proposed N-gram mask will restrict the self-attention mechanism to\nusing only the previous N − 1 tokens.\nA more detailed description of the N-gram self-attention mechanism is presented in Section 2. T he experiments\npresented in Section 3 compare the baseline with the N-gram self-attention transformer.\n2 N-gram Self-attention\nThe incremental computation of encodings for the target con text t1, . . . , t k− 1 in a layered transformer decoder\ninvolves the following steps at each layer, after receiving the token embeddings (along with position encoding) or\nthe encodings from the previous layer:\n1\n1. compute the self-attention query qk and key kk\n2. compute softmax over the indexes j = 1, . . . , k − 1, and then the attention context for position k, of O(k)\n3. feed-forward computation of encoding for position k\nFor a target sentence of length T , the above steps need to be repeated ∀k = 1, . . . , T , resulting in computational\ncomplexity of O(T 2).\nThe N-gram self-attention mechanism reduces the context used fo r the prediction at position k, resulting in a\ncomputational complexity of O(N ·T ). As shown in our experiments, see Section 3, N = 8 is a viable value for\nthe N-gram order; for sentences of length T ≈ 16 − 25 this promises a reduction in computational complexity on\nthe order of O(T/N ), or an ≈ 2 − 3X speed-up for the self-attention computation.\nIt remains to be seen to what extent this can be realized in pra ctice in a given implementation and hardware\nplatform (CPU, GPU or TPU), since the sibling feed-forward c omputation at step 3 may dominate the incremental\ncomputation at position k in the target sentence. The results in (Zhang et al., 2018) do show that optimizing the\nself-attention computation can have a signiﬁcant impact on decoding speed.\nAnother potential computational advantage is the ability t o store the context in a ﬁxed size memory buffer of\nlength N − 1, replacing context elements one by one as the decoder advanc es in the target sentence, e.g. by indexing\nthe buffer modulo N − 1. This reduces the memory bandwidth required by the model at i nference/beam-search time\n(a major bottleneck on TPU) by a factor of O(T/N ).\n3 Experiments\nW e implemented N-gram self-attention in lingvo (Shen et al., 2019) as a conﬁg uration option to TransformerAttentionLayer\n(lingvo/core/layers_with_attention.py). W e perform experiments on two data sets: WMT’18 EnDe\nand WMT’14 EnFr; for EnDe we use newstest2012/2017 as dev/te st data, respectively; for EnFr we concatenate\nnewstest2012 and newstest2013 as dev data and use newstest2 014 as test data.\nThe EnDe transformer model used is conﬁgured as follows: 6 la yers, 8 attention heads, 512 model/embedding,\n2048 hidden dimensionality , respectively , dropout 0.1. Fo r EnFr we used 6 layers, 16 attention heads, 1024\nmodel/embedding, 8192 hidden dimensionality , respective ly , dropout 0.1. In both cases we used bilingual (source,\ntarget) word-piece models of size 32k.\nThe results are presented in T ables 1-2. Setting N = 8 or N = 10 achieves the best BLEU score on dev data\nand is within 0.3-0.4 BLEU from the baseline. Smaller N-gram orders are also a viable choice since performance\ndegrades gracefully for N ≥ 3.\nModel corpus BLEU log pplx @steps\ndev test dev test (dev)\nbaseline 22.4 28.6 2.96 2.64 @151.6k\n3-gram 22.1 28.0 3.12 2.76 @98.32k\n4-gram 22.2 28.4 3.06 2.72 @93.62k\n6-gram 22.2 28.3 3.01 2.68 @347.5k\n8-gram 22.5 28.2 2.99 2.66 @467.8k\n10-gram 22.5 28.3 2.98 2.65 @126.9k\nT able 1: WMT’18 EnDe experiments: corpus BLEU and log-perpl exity on the dev/test data, respectively at the best\ncheckpoint picked according to the BLEU score on dev data.\n2\nModel corpus BLEU log pplx @steps\ndev test dev test (dev)\nbaseline 32.9 41.1/40.7+ 2.52 2.16 @175.4k\n2-gram 30.0 36.7 2.97 2.52 @176.4k\n3-gram 32.2 40.3 2.58 2.21 @183.7k\n4-gram 32.4 40.4 2.56 2.19 @180.3k\n6-gram 32.7 41.1 2.53 2.17 @200.4k\n8-gram 32.7 40.7 2.52 2.17 @176.5k\n10-gram 32.8 40.8 2.52 2.17 @166.9k\nT able 2: WMT’14 EnFr experiments: corpus BLEU and log-perpl exity on the dev/test data, respectively at the best\ncheckpoint picked according to the BLEU score on dev data; +: depending on the exact checkpoint on test data.\n4 Related W ork\nThe use of local attention mechanism is not novel. It is used i n LM and NMT experiments reported in (Shazeer,\n2019), text summarization work in (Liu et al., 2018), image p rocessing in (Parmar et al., 2018) and as well as auto-\nmatic speech recognition as in (Povey et al., 2018). W e wish t o clarify that our use of local attention is restricted to\nthe decoder component of the transformer model, unlike the i mage/speech processing use cases highlighted previ-\nously .\nUnlike the blocking algorithm described in Section 4.2.4 of (Liu et al., 2018), we incrementally slide the atten-\ntion window , just as also implemented in (Shazeer, 2019), se e footnote on page 7 and results for N = 32 in T ables\n1 and 2, rows labeled with “-local” sufﬁx. Our experiments sh ow that signiﬁcantly smaller values for the window\nlength N are feasible.\nA detailed performance analysis in both number of operation s and memory footprint is presented in (Shazeer,\n2019); particularly relevant is the one in Section 3.1 for in cremental operation at inference time. Directly relevant t o\nour proposed self-attention variant is the one in (Zhang et a l., 2018) showing 4X improvements in decoding speed\nover the baseline transformer model.\n5 Conclusion and Future W ork\nExperimental results show that N-gram self-attention is a viable alternative to full (one-s ided/causal) self-attention\nin the decoder component of transformer models used in NMT . T his slight model change promises run-time advan-\ntages in terms of memory footprint and speed that are yet to be investigated thoroughly , particularly at inference\ntime.\nReferences\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Rya n Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018.\nGenerating Wikipedia by Summarizing Long Sequences. CoRR, abs/1801.10198.\nNiki Parmar, Ashish V aswani, Jakob Uszkoreit, Łukasz Kaise r, Noam Shazeer, Alexander Ku, and Dustin Tran.\n2018. Image Transformer. In Proceedings of the 35th International Conference on Machine Learning.\nDaniel Povey , Hossein Hadian, Pegah Ghahremani, Ke Li, and S anjeev Khudanpur. 2018. A Time-restricted Self\nAttention Layer for ASR. In Proceedings of the International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE.\nNoam Shazeer. 2019. Fast Transformer Decoding: One Write-H ead is All Y ou Need. ArXiv.\n3\nJonathan Shen, Patrick Nguyen, Y onghui Wu, Zhifeng Chen, Mi a X. Chen, Y e Jia, Anjuli Kannan, T ara N. Sainath,\nand Y uan Cao et al. 2019. Lingvo: a Modular and Scalable Frame work for Sequence-to-Sequence Modeling.\nCoRR, abs/1902.08295.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit , Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention Is All Y ou Need. In Advances in Neural Information Processing Systems, pages\n5998–6008.\nJiayi W ang, Kai Fan, Bo Li, Fengming Zhou, Boxing Chen, Y angb in Shi, and Luo Si. 2018.\nAlibaba Submission for WMT18 Quality Estimation T ask. In Proceedings of the Third Conference on Machine\nTranslation: Shared T ask P apers, pages 809–815, Belgium, Brussels.\nBiao Zhang, Deyi Xiong, and Jinsong Su. 2018. Accelerating N eural Transformer via an A verage Attention Net-\nwork. CoRR, abs/1805.00631.\n4",
  "topic": "Gram",
  "concepts": [
    {
      "name": "Gram",
      "score": 0.7658395767211914
    },
    {
      "name": "Transformer",
      "score": 0.7140334844589233
    },
    {
      "name": "Decoding methods",
      "score": 0.6978765726089478
    },
    {
      "name": "n-gram",
      "score": 0.685870349407196
    },
    {
      "name": "Computer science",
      "score": 0.663174033164978
    },
    {
      "name": "Sentence",
      "score": 0.6087092161178589
    },
    {
      "name": "Task (project management)",
      "score": 0.5024504661560059
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4301459491252899
    },
    {
      "name": "Natural language processing",
      "score": 0.41755372285842896
    },
    {
      "name": "Speech recognition",
      "score": 0.40782803297042847
    },
    {
      "name": "Algorithm",
      "score": 0.256159245967865
    },
    {
      "name": "Language model",
      "score": 0.15133947134017944
    },
    {
      "name": "Physics",
      "score": 0.07432901859283447
    },
    {
      "name": "Engineering",
      "score": 0.06418994069099426
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Bacteria",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}