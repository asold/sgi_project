{
  "title": "Multilingual Sentence Transformer as A Multilingual Word Aligner",
  "url": "https://openalex.org/W4385573925",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100663684",
      "name": "Weikang Wang",
      "affiliations": [
        "Shanghai University of Finance and Economics"
      ]
    },
    {
      "id": "https://openalex.org/A5100665991",
      "name": "Guanhua Chen",
      "affiliations": [
        "Southern University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101837435",
      "name": "Hanqing Wang",
      "affiliations": [
        "Shanghai University of Finance and Economics"
      ]
    },
    {
      "id": "https://openalex.org/A5100686269",
      "name": "Yue Han",
      "affiliations": [
        "Shanghai University of Finance and Economics"
      ]
    },
    {
      "id": "https://openalex.org/A5100416519",
      "name": "Yun Chen",
      "affiliations": [
        "Shanghai University of Finance and Economics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3106321930",
    "https://openalex.org/W2156985047",
    "https://openalex.org/W2970045405",
    "https://openalex.org/W3035380217",
    "https://openalex.org/W3155609600",
    "https://openalex.org/W3093871477",
    "https://openalex.org/W2300691323",
    "https://openalex.org/W2947799968",
    "https://openalex.org/W183550236",
    "https://openalex.org/W2970908867",
    "https://openalex.org/W1411230545",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W2138322361",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2887920589",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2998135987",
    "https://openalex.org/W3105813095",
    "https://openalex.org/W3105011057",
    "https://openalex.org/W2962714778",
    "https://openalex.org/W2141532438",
    "https://openalex.org/W3177376359",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2952682849",
    "https://openalex.org/W2912070261",
    "https://openalex.org/W3104881680",
    "https://openalex.org/W3213816951",
    "https://openalex.org/W2398041834",
    "https://openalex.org/W2972954451",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2970849641",
    "https://openalex.org/W3176693010",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3166567197",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W3176344506",
    "https://openalex.org/W2538358357",
    "https://openalex.org/W3175321423",
    "https://openalex.org/W2251150542",
    "https://openalex.org/W2132713736",
    "https://openalex.org/W2148708890",
    "https://openalex.org/W3175810841",
    "https://openalex.org/W3039695075",
    "https://openalex.org/W92833821",
    "https://openalex.org/W3035463087"
  ],
  "abstract": "Multilingual pretrained language models (mPLMs) have shown their effectiveness in multilingual word alignment induction. However, these methods usually start from mBERT or XLM-R. In this paper, we investigate whether multilingual sentence Transformer LaBSE is a strong multilingual word aligner. This idea is non-trivial as LaBSE is trained to learn language-agnostic sentence-level embeddings, while the alignment extraction task requires the more fine-grained word-level embeddings to be language-agnostic. We demonstrate that the vanilla LaBSE outperforms other mPLMs currently used in the alignment task, and then propose to finetune LaBSE on parallel corpus for further improvement. Experiment results on seven language pairs show that our best aligner outperforms previous state-of-the-art models of all varieties. In addition, our aligner supports different language pairs in a single model, and even achieves new state-of-the-art on zero-shot language pairs that does not appear in the finetuning process.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2952–2963\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nMultilingual Sentence Transformer as A Multilingual Word Aligner\nWeikang Wang1∗ Guanhua Chen2∗ Hanqing Wang1 Yue Han1 Yun Chen1†\n1Shanghai University of Finance and Economics\n2Southern University of Science and Technology\nwwk@163.sufe.edu.cn ghchen08@gmail.com\n{whq,hanyue}@163.sufe.edu.cn yunchen@sufe.edu.cn\nAbstract\nMultilingual pretrained language models\n(mPLMs) have shown their effectiveness in\nmultilingual word alignment induction. How-\never, these methods usually start from mBERT\nor XLM-R. In this paper, we investigate\nwhether multilingual sentence Transformer\nLaBSE is a strong multilingual word aligner.\nThis idea is non-trivial as LaBSE is trained\nto learn language-agnostic sentence-level em-\nbeddings, while the alignment extraction task\nrequires the more fine-grained word-level em-\nbeddings to be language-agnostic. We demon-\nstrate that the vanilla LaBSE outperforms other\nmPLMs currently used in the alignment task,\nand then propose to finetune LaBSE on parallel\ncorpus for further improvement. Experiment\nresults on seven language pairs show that our\nbest aligner outperforms previous state-of-the-\nart models of all varieties. In addition, our\naligner supports different language pairs in a\nsingle model, and even achieves new state-of-\nthe-art on zero-shot language pairs that does\nnot appear in the finetuning process.\n1 Introduction\nWord alignment aims to find the correspondence\nbetween words in parallel texts (Brown et al., 1993).\nIt is useful in a variety of natural language process-\ning (NLP) applications such as noisy parallel cor-\npus filtering (Kurfalı and Östling, 2019), bilingual\nlexicon induction (Shi et al., 2021), code-switching\ncorpus building (Lee et al., 2019; Lin et al., 2020)\nand incorporating lexical constraints into neural\nmachine translation (NMT) models (Hasler et al.,\n2018; Chen et al., 2021b).\nRecently, neural word alignment approaches\nhave developed rapidly and outperformed statistical\nword aligners like GIZA++ (Och and Ney, 2003)\nand fast-align (Dyer et al., 2013). Some works\n(Garg et al., 2019; Li et al., 2019; Zenkel et al.,\n∗The first two authors contribute equally.\n†Corresponding author.\nFigure 1: Cosine similarities between subword repre-\nsentations in a parallel sentence pair from 8th layer of\nmBERT (left) and 6th layer of LaBSE (right). Red boxes\ndenote the gold alignments.\n2019, 2020; Chen et al., 2020b; Zhang and van Gen-\nabith, 2021; Chen et al., 2021a) induce alignments\nfrom NMT model or its variants. However, these\nbilingual models only support the language pair\ninvolved in the training process. They also treat the\nsource and target side differently, thus two models\nare required for bidirectional alignment extraction.\nAnother line of works (Jalili Sabet et al., 2020; Dou\nand Neubig, 2021) build multilingual word aligners\nwith contextualized embeddings from the multilin-\ngual pretrained language model (Wu and Dredze,\n2019; Conneau et al., 2020, mPLM). Thanks to\nthe language-agnostic representations learned with\nmultilingual masked language modeling task, these\nmethods are capable of inducing word alignments\neven for language pairs without any parallel corpus.\nDifferent from previous methods, in this pa-\nper we present AccAlign, a more accurate mul-\ntilingual word aligner with the multilingual sen-\ntence Transformer LaBSE (Feng et al., 2022, see\nFigure 1). The LaBSE is trained on large scale\nparallel corpus of various language pairs to learn\nlanguage-agnostic sentence embeddings with con-\ntrastive learning. However, it is unclear whether\nLaBSE has learned language-agnostic word-level\nembeddings, which is the key for the success of\n2952\nword alignment extraction. Specifically, we first\ndirect induce word alignments from LaBSE and\ndemonstrate that LaBSE outperforms other mPLMs\ncurrently used in the alignment task. This indi-\ncates that LaBSE has implicitly learned language-\nagnostic word-level embeddings at some intermedi-\nate layer. Then we propose a simple and effective\nfinetuning method to further improve performance.\nEmpirical results on seven language pairs show that\nour best aligner outperforms previous SOTA mod-\nels of all varieties. In addition, our aligner supports\ndifferent language pairs in a single model, and even\nachieves new SOTA on zero-shot language pairs\nthat does not appear in finetuning process.1\n2 AccAlign\n2.1 Background: LaBSE\nLaBSE (Feng et al., 2022) is the state-of-the-art\nmodel for the cross-lingual sentence retrieval task.\nGiven an input sentence, the model can retrieve the\nmost similar sentence from candidates in a different\nlanguage. LaBSE is first pretrained on a combina-\ntion of masked language modeling (Devlin et al.,\n2019) and translation language modeling (Conneau\nand Lample, 2019) tasks. After that, it is effec-\ntively finetuned with contrastive loss on 6B parallel\nsentences across 109 languages. We leave the train-\ning detail of LaBSE in the appendix. However, as\nLaBSE does not include any word-level training\nloss when finetuning with contrastive loss, it is un-\nclear whether the model has learned high-quality\nlanguage-agnostic word-level embeddings, which\nis the key for a multilingual word aligner.\n2.2 Alignment Induction from LaBSE\nTo investigate whether LaBSE is a strong multilin-\ngual word aligner, we first induce word alignments\nfrom vanilla LaBSE without any modification or\nfinetuning. This is done by utilizing the contextual\nembeddings from LaBSE. Specifically, consider\na bilingual sentence pair x = ⟨x1,x2,...,x n⟩and\ny = ⟨y1,x2,...,y m⟩, we denote the contextual em-\nbeddings from LaBSE as hx = ⟨hx1 ,...,h xn ⟩and\nhy = ⟨hy1 ,...,h ym ⟩, respectively. Following pre-\nvious work (Dou and Neubig, 2021; Jalili Sabet\net al., 2020), we get the similarity matrix from the\ncontextual embeddings:\nS = hxhT\ny . (1)\n1Code is available at https://github.com/sufenlp/\nAccAlign.\nFigure 2: The framework of adapter-based finetuning.\nThe blue blocks are kept frozen, while the red adapter\nblocks are updated during finetuning.\nThe similarity matrix is normalized for each row to\nget Sxy. Sxy is treated as the probability matrix as\nits i-th row represents the probabilities of aligning\nxi to all tokens in y. The reverse probability ma-\ntrix Syx is computed similarly by normalizing each\ncolumn of S. Taking intersection of the two prob-\nability matrices yields the final alignment matrix:\nA= (Sxy >c) ∗(ST\nyx >c), (2)\nwhere cis a threshold and Aij = 1indicates that\nxi and yj are aligned. The above method induces\nalignments on the subword level, which are con-\nverted into word-level alignments by aligning two\nwords if any of their subwords are aligned follow-\ning (Zenkel et al., 2020; Jalili Sabet et al., 2020).\n2.3 Finetuning LaBSE for Better Alignments\nInspired by (Dou and Neubig, 2021), we propose a\nfinetuning method to further improve performance\ngiven parallel corpus with alignment labels.\nAdapter-based Finetuning Adapter-based fine-\ntuning (Houlsby et al., 2019; Bapna and Firat, 2019;\nHe et al., 2021) is not only parameter-efficient,\nbut also benefits model performance, especially\nfor low-resource and cross-lingual tasks (He et al.,\n2021). Figure 2 illustrates our overall framework,\nwhere the adapters are adopted from (Houlsby et al.,\n2019). For each layer of LaBSE, we introduce\nan adapter for each sublayer, which maps the in-\nput vector of dimension dto dimension mwhere\nm < d, and then re-maps it back to dimension d.\nLet h and h′denote the input and output vector,\n2953\nModel Setting de-en sv-en fr-en ro-en ja-en zh-en fa-en avg\nBilingual Statistical Methods\nfast-align (Dyer et al., 2013)\nscratch\n27.0 - 10.5 32.1 51.1 38.1 - -\neflomal (Östling and Tiedemann, 2016) 22.6 - 8.2 25.1 47.5 28.7 - -\nGIZA++ (Och and Ney, 2003) 20.6 - 5.9 26.4 48.0 35.1 - -\nBilingual Neural Methods\nMTL-FULLC-GZ (Garg et al., 2019)\nscratch\n16.0 - 4.6 23.1 - - - -\nBAO-GUIDE (Zenkel et al., 2020) 16.3 - 5.0 23.4 - - - -\nSHIFT-AET (Chen et al., 2020b) 15.4 - 4.7 21.2 - 17.2 - -\nMASK-ALIGN (Chen et al., 2021a) 14.4 - 4.4 19.5 - 13.8 - -\nBTBA-FCBO-SST (Zhang and van Genabith, 2021) 14.3 - 6.7 18.5 - - - -\nMultilingual Neural Methods\nSimAlign (Jalili Sabet et al., 2020) no ft 18.8 11.2 7.6 27.2 46.6 21.6 32.7 23.7\nAwesomeAlign (Dou and Neubig, 2021)\nno ft 17.4 9.7 5.6 27.9 45.6 18.1 33.0 22.5\nself-sup ft 15.9 7.9 4.4 26.2 42.4 14.9 27.1 19.8\nsup ft 15.2 7.2 4.0 25.5 40.6 13.4 25.8 18.8\nAccAlign\nno ft 16.0 7.3 4.5 20.8 43.3 16.2 23.4 18.8\nself-sup ft 14.3 5.8 3.9 21.6 39.2 13.0 22.6 17.2\nsup ft 13.6 5.2 2.8 20.8 36.9 11.5 22.2 16.1\nTable 1: AER comparison between AccAlign and the baselines on test set of7 language pairs. self-sup and sup mean\nfinetuning the model with parallel corpus of self-supervised and human-annotated alignment labels, respectively.\nAll multilingual methods are tested on zero-shot language pairs.\nrespectively. The output vector h′is calculated as:\nh\n′\n= Wup ·tanh(Wdown ·h) +h. (3)\nNote that a skip-connection is employed to approx-\nimate an identity function if parameters of the pro-\njection matrices are near zero. During finetuning,\nonly parameters of the adapters are updated.\nTraining Objective Let ˆAdenote the alignment\nlabels for the given sentence pair x and y. We\ndefine the learning objective as:\nL=\n∑\nij\nˆAij\n1\n2\n(\n(Sxy)ij\nn + (ST\nyx)ij\nm\n)\n, (4)\nwhere Sxy and Syx are the alignment probabil-\nity matrices, nand mare the length of sentence\nx and y, respectively. Intuitively, this objective\nencourages the gold aligned words to have closer\ncontextualized representations. In addition, as both\nSxy and ST\nyx are encouraged to be close to ˆA, it im-\nplicitly encourages the two alignment probability\nmatrices to be symmetrical to each other as well.\nOur framework can be easily extended to cases\nwhere alignment labels are unavailable, by replac-\ning ˆAwith pseudo labels A(Equation 2) and train-\ning in a self-supervised manner.\n3 Experiments\n3.1 Setup\nAs we aim at building an accurate multilingual\nword aligner, we evaluate AccAlign on a di-\nverse alignment test set of seven language pairs:\nde/sv/ro/fr/ja/zh/fa-en. For finetuning LaBSE, we\nuse nl/cs/hi/tr/es/pt-en as the training set and cs-en\nas the validation set. To reduce the alignment anno-\ntation efforts and the finetuning cost, our training\nset only contains 3,362 annotated sentence pairs.\nTo simulate the most difficult use cases where the\ntest language pair may not included in training, we\nset the test language pairs different from training\nand validation. Namely, LaBSE is tested in a zero-\nshot manner. We denote this dataset as ALIGN6.\nWe induce alignments from 6-th layer of LaBSE,\nwhich is selected on the validation set. We use\nAlignment Error Rate (AER) as the evaluation met-\nric. Our model is not directly comparable to the\nbilingual baselines, as they build model for each\ntest language pair using large scale parallel corpus\nof that language pair. In contrast, our method is\nmore efficient as it supports all language pairs in\na single model and our finetuning only requires\n3,362 sentence pairs. Appendix B show more\ndataset, model, baselines and other setup details.\n3.2 Main Results\nTable 1 shows the comparison of our methods\nagainst baselines. AccAlign-supft achieves new\nSOTA on word alignment induction, outperforming\nall baselines in 6 out of 7 language pairs. AccAlign\nis also simpler than AwesomeAlign, which is the\nbest existing multilingual word aligner, as Awe-\nsomeAlign finetunes with a combination of five\nobjectives, while AccAlign only has one objective.\nThe vanilla LaBSE is a strong multilingual word\n2954\nModel fi-el fi-he\nSimAglin noft 69.3 85.8\nAwesomeAlign\nnoft 69.8 84.4\nself-sup ft 68.8 87.7\nsup ft 67.4 86.1\nAccAlign\nnoft 47.0 81.2\nself-sup ft 40.8 76.1\nsup ft 36.7 71.7\nTable 2: AER comparison between AccAlign and mul-\ntilingual baselines on non-English zero-shot language\npairs. The best AER for each column is bold and under-\nlined.\naligner (see AccAlign-noft). It performs better than\nSimAlign-noft and AwesomeAlign-noft, and com-\nparable with AwesomeAlign-supft, indicating that\nLaBSE has learned high-quality language-agnostic\nword embeddings. Our finetuning method is ef-\nfective as well, improving AccAlign-noft by 1.6\nand 2.7 AER with self-supervised and supervised\nalignment labels, respectively. Our model improves\nmultilingual baselines even more significantly on\nnon-English language pairs. See Table 2 of ap-\npendix for detailed results.\n3.3 Analysis\nPerformance on non-English Language Pair\nWe conduct experiments to evaluate AccAlign\nagainst multilingual baselines on non-English test\nlanguage pairs. The fi-el (Finnish-Greek) and fi-he\n(Finnish-Hebrew) test set contains 791 and 2,230\nannotated sentence pairs, respectively. Both test\nsets are from ImaniGooghari et al. (2021) 2. The\nresults are shown in Table 2. As can be seen, Ac-\ncAlign in all three settings significantly improves\nall multilingual baselines. The improvements is\nmuch larger compared with zero-shot English lan-\nguage pairs, demonstrating the effectiveness of Ac-\ncAlign on non-English language pairs. We also\nobserve that finetuning better improves AccAlign\nthan AwesomeAlign. This verifies the strong cross-\nlingual transfer ability of LaBSE , even between\nEnglish-centric and non-English language pairs.\nAdapter-based vs. Full Finetuning We com-\npare full and adapter-based fine-tuning in Table 3.\nCompared with full finetuning, adapter-based fine-\ntuning updates much less parameters and obtains\nbetter performance under both supervised and self-\nsupervised settings, demonstrating its efficiency\nand effectiveness for zero-shot word alignments.\n2https://github.com/cisnlp/graph-align\nFt type full adapter\nFt mode self-supervised (avg.) 17.4 17.2\nsupervised (avg.) 16.2 16.1\nNumber of ft param. 428M 2.4M\nTable 3: AER comparison of full finetuning and adapter-\nbased finetuning.\nBilingual Finetuning To better understand our\nmethod, we compare with AwesomeAlign under\nbilingual finetuning setup where the model is fine-\ntuned and tested in the same single language pair.\nWe follow the setup in (Dou and Neubig, 2021) and\nuse finetuning corpus without human-annotated la-\nbels. As shown in Table 4, LaBSE outperforms\nAwesomeAlign in the finetuning language pair\n(18.8 vs. 18.2). The performance gap becomes\nlarger for zero-shot language pairs (21.3 vs. 18.8).\nThe results demonstrate that AccAlign is an effec-\ntive zero-shot aligner, as LaBSE has learned more\nlanguage-agnostic representations which benefit\ncross-lingual transfer.\nDifferent Multilingual Pretrained ModelsWe\ninvestigate the performance of AccAlign-noft when\nreplacing LaBSE with other mPLMs, including\nXLM-R, mBERT and four other multilingual sen-\ntence Transformer from HuggingFace. LaBSE out-\nperforms other mPLMs by 3.5 to 9.6 averaged AER.\nTable 9 in appendix shows more details.\nPerformance across Layer We investigate the\nperformance of AccAlign-noft when extracts align-\nments from different layers. Layer 6, which is the\nlayer we use for all experiments, outperforms other\nlayers by 0.1 to 26.0 averaged AER. Please refer to\nTable 10 in appendix for more details.\nRepresentation Analysis To succeed in multi-\nlingual word alignment, the contextual embed-\ndings should prefer two following properties: (1)\nlanguage-agnostic: two aligned bilingual words\nshould be mapped to nearby features in the\nsame language-agnostic feature space. (2) word-\nidentifiable: the embeddings of two random tokens\nfrom the same sentence should be distinguishable.\nTherefore, we analyze the embeddings from dif-\nferent layers of AccAlign under different settings\nby computing cosine similarity for aligned word\npairs and word pairs randomly sampled from the\nsame sentence, denoted as sbi and smono (see ap-\npendix for more experiment details). Intuitively,\nbigger sbi and smaller smono are preferred as we\n2955\nModel Test lang.\nFt lang. de-en fr-en ro-en ja-en zh-en avg.\nAwesomeAlign ft lang. 14.9 4.0 22.9 38.1 14.1 18.8\nzero-shot langs (avg.) 16.3 4.7 26.6 43.7 15.0 21.3\nAccAlign ft lang. 14.2 3.8 21.0 38.0 13.8 18.2\nzero-shot langs (avg.) 14.8 3.9 20.7 40.5 13.8 18.8\nTable 4: AER results with bilingual finetuning.\nFigure 3: sbi (↑) and smono (↓) of AccAlign without\nfinetuning (noft), with self-supervised finetuning (self-\nsup ft) and supervised finetuning (sup ft).\nexpect the features of aligned words to be similar\nwhile that of two different words to be different.\nThe results on de-en test set are presented in Fig-\nure 3. For vanilla LaBSE (green curves), we find\nthat features from 6-th layer, namely the best layer\nto induce alignment, successfully trades off these\ntwo properties as it obtains the biggest sbi −smono\namong all layers. In addition, adapter-based fine-\ntuning improves performance mainly by making\nfeatures more word-identifiable, as it significantly\ndecreases smono while almost maintaining sbi .\n4 Conclusion\nIn this paper, we introduce AccAlign, a novel multi-\nlingual word aligner based on multilingual sentence\nTransformer LaBSE. The best proposed approach\nfinetunes LaBSE on a few thousands of annotated\nparallel sentences and achieves state-of-the-art per-\nformance even for zero-shot language pairs. Ac-\ncAlign is believed to be a valuable alignment tool\nthat can be used out-of-the-box for other NLP tasks.\nLimitations\nAccAlign has shown to extract high quality word\nalignments when the input texts are two well-paired\nbilingual sentences. However, the condition is\nnot always met. In lexically constrained decod-\ning of NMT (Hasler et al., 2018; Song et al., 2020;\nChen et al., 2021b), the aligner takes a full source-\nlanguage sentence and a partial target-language\ntranslation as the input at each step to determine\nthe right position to incorporate constraints. In cre-\nating translated training corpus in zero-resource\nlanguage for sequence tagging or parsing (Ni et al.,\n2017; Jain et al., 2019; Fei et al., 2020), the aligner\nextracts alignments from the labelled sentence and\nits translation to conduct label projection. Both\ncases deviate from our current settings as the input\nsentence may contain translation error or even be\nincomplete. We leave exploring the robustness of\nAccAlign as the future work.\nAt the same time, our proposed method only\nsupports languages included in LaBSE. This hin-\nders applying AccAlign to more low-resource lan-\nguages. Future explorations are needed to rapidly\nadapt AccAlign to new languages (Neubig and Hu,\n2018; Garcia et al., 2021).\nAcknowledgements\nThis project was supported by National Natural\nScience Foundation of China (No. 62106138) and\nShanghai Sailing Program (No. 21YF1412100).\nWe thank the anonymous reviewers for their in-\nsightful feedbacks on this work.\nReferences\nNiraj Aswani and Robert Gaizauskas. 2005. Aligning\nwords in english-hindi parallel corpora. In Proceed-\nings of the ACL Workshop on Building and Using\nParallel Texts, pages 115–118.\nAnkur Bapna and Orhan Firat. 2019. Simple, scal-\nable adaptation for neural machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1538–\n1548, Hong Kong, China. Association for Computa-\ntional Linguistics.\n2956\nPeter F Brown, Stephen A Della Pietra, Vincent J\nDella Pietra, and Robert L Mercer. 1993. The mathe-\nmatics of statistical machine translation: Parameter\nestimation. Computational linguistics, 19(2):263–\n311.\nMehmet Talha Cakmak, Süleyman Acar, and Gül¸ sen\nEryi˘git. 2012. Word alignment for english-turkish\nlanguage pair. In Proceedings of the Eighth Inter-\nnational Conference on Language Resources and\nEvaluation (LREC’12), pages 2177–2180.\nChi Chen, Maosong Sun, and Yang Liu. 2021a. Mask-\nalign: Self-supervised neural word alignment. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4781–\n4791, Online. Association for Computational Lin-\nguistics.\nGuanhua Chen, Yun Chen, and Victor OK Li. 2021b.\nLexically constrained neural machine translation with\nexplicit alignment guidance. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 35, pages 12630–12638.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton. 2020a. A simple framework for\ncontrastive learning of visual representations. In In-\nternational conference on machine learning, pages\n1597–1607. PMLR.\nYun Chen, Yang Liu, Guanhua Chen, Xin Jiang, and\nQun Liu. 2020b. Accurate word alignment induction\nfrom neural machine translation. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 566–576,\nOnline. Association for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. Advances in\nneural information processing systems, 32.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nZi-Yi Dou and Graham Neubig. 2021. Word alignment\nby fine-tuning embeddings on parallel corpora. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 2112–2128, Online.\nAssociation for Computational Linguistics.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013. A simple, fast, and effective reparameteriza-\ntion of IBM model 2. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–648, Atlanta,\nGeorgia. Association for Computational Linguistics.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, et al. 2021. Beyond english-centric mul-\ntilingual machine translation. J. Mach. Learn. Res.,\n22(107):1–48.\nHao Fei, Meishan Zhang, and Donghong Ji. 2020.\nCross-lingual semantic role labeling with high-\nquality translated training corpus. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7014–7026, On-\nline. Association for Computational Linguistics.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2022. Language-agnostic\nBERT sentence embedding. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n878–891, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nXavier Garcia, Noah Constant, Ankur Parikh, and Orhan\nFirat. 2021. Towards continual learning for multilin-\ngual machine translation via vocabulary substitution.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1184–1192.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly learning to align\nand translate with transformer models. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4453–4462, Hong\nKong, China. Association for Computational Linguis-\ntics.\nJoao Graca, Joana Paulo Pardal, Luísa Coheur, and Dia-\nmantino Caseiro. 2008. Building a golden collection\nof parallel multi-language word alignment. In Pro-\nceedings of the Sixth International Conference on\nLanguage Resources and Evaluation (LREC’08).\nEva Hasler, Adrià de Gispert, Gonzalo Iglesias, and\nBill Byrne. 2018. Neural machine translation decod-\ning with terminology constraints. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\n2957\nHuman Language Technologies, Volume 2 (Short Pa-\npers), pages 506–512.\nRuidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng\nDing, Liying Cheng, Jiawei Low, Lidong Bing, and\nLuo Si. 2021. On the effectiveness of adapter-based\ntuning for pretrained language model adaptation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 2208–\n2222, Online. Association for Computational Lin-\nguistics.\nMaria Holmqvist and Lars Ahrenberg. 2011. A gold\nstandard for english-swedish word alignment. In\nProceedings of the 18th Nordic conference of compu-\ntational linguistics (NODALIDA 2011), pages 106–\n113.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nAyyoob ImaniGooghari, Masoud Jalili Sabet,\nLutfi Kerem Senel, Philipp Dufter, François Yvon,\nand Hinrich Schütze. 2021. Graph algorithms for\nmultiparallel word alignment. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 8457–8469, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nAlankar Jain, Bhargavi Paranjape, and Zachary C. Lip-\nton. 2019. Entity projection via machine transla-\ntion for cross-lingual NER. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1083–1092, Hong Kong,\nChina. Association for Computational Linguistics.\nMasoud Jalili Sabet, Philipp Dufter, François Yvon,\nand Hinrich Schütze. 2020. SimAlign: High qual-\nity word alignments without parallel training data\nusing static and contextualized embeddings. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1627–1643, Online. Association\nfor Computational Linguistics.\nMurathan Kurfalı and Robert Östling. 2019. Noisy par-\nallel corpus filtering through projected word embed-\ndings. In Proceedings of the Fourth Conference on\nMachine Translation (Volume 3: Shared Task Papers,\nDay 2), pages 277–281, Florence, Italy. Association\nfor Computational Linguistics.\nGrandee Lee, Xianghu Yue, and Haizhou Li. 2019. Lin-\nguistically motivated parallel data augmentation for\ncode-switch language modeling. In INTERSPEECH,\npages 3730–3734.\nXintong Li, Guanlin Li, Lemao Liu, Max Meng, and\nShuming Shi. 2019. On the word alignment from\nneural machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 1293–1303, Florence, Italy.\nAssociation for Computational Linguistics.\nZehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu,\nJiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre-\ntraining multilingual neural machine translation by\nleveraging alignment information. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 2649–\n2663.\nYang Liu and Maosong Sun. 2015. Contrastive unsu-\npervised word alignment with non-local features. In\nTwenty-Ninth AAAI Conference on Artificial Intelli-\ngence.\nLieve Macken. 2010. An annotation scheme and gold\nstandard for dutch-english word alignment. In 7th\nconference on International Language Resources and\nEvaluation (LREC 2010), pages 3369–3374. Euro-\npean Language Resources Association (ELRA).\nDavid Mare ˇcek. 2011. Automatic alignment of tec-\ntogrammatical trees from czech-english parallel cor-\npus.\nRada Mihalcea and Ted Pedersen. 2003. An evaluation\nexercise for word alignment. In Proceedings of the\nHLT-NAACL 2003 Workshop on Building and Using\nParallel Texts: Data Driven Machine Translation and\nBeyond, pages 1–10.\nGraham Neubig. 2011. The Kyoto free translation task.\nhttp://www.phontron.com/kftt.\nGraham Neubig and Junjie Hu. 2018. Rapid adapta-\ntion of neural machine translation to new languages.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n875–880, Brussels, Belgium. Association for Com-\nputational Linguistics.\nJian Ni, Georgiana Dinu, and Radu Florian. 2017.\nWeakly supervised cross-lingual named entity recog-\nnition via effective annotation and representation pro-\njection. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1470–1480, Vancouver,\nCanada. Association for Computational Linguistics.\nFranz Josef Och and Hermann Ney. 2003. A systematic\ncomparison of various statistical alignment models.\nComputational Linguistics, 29(1):19–51.\nRobert Östling and Jörg Tiedemann. 2016. Efficient\nword alignment with markov chain monte carlo. The\nPrague Bulletin of Mathematical Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\n2958\nHaoyue Shi, Luke Zettlemoyer, and Sida I. Wang. 2021.\nBilingual lexicon induction via unsupervised bitext\nconstruction and word alignment. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 813–826, Online.\nAssociation for Computational Linguistics.\nKai Song, Kun Wang, Heng Yu, Yue Zhang, Zhongqiang\nHuang, Weihua Luo, Xiangyu Duan, and Min Zhang.\n2020. Alignment-enhanced transformer for con-\nstraining nmt with pre-specified translations. In Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 34, pages 8886–8893.\nLeila Tavakoli and Heshaam Faili. 2014. Phrase align-\nments in parallel corpus using bootstrapping ap-\nproach.\nDavid Vilar, Maja Popovi´c, and Hermann Ney. 2006.\nAer: Do we need to “improve” our alignments? In\nProceedings of the Third International Workshop on\nSpoken Language Translation: Papers.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of bert. In\nProceedings of EMNLP-IJCNLP, pages 833–844.\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,\nJax Law, Noah Constant, Gustavo Hernandez Abrego,\nSteve Yuan, Chris Tar, Yun-Hsuan Sung, et al. 2020.\nMultilingual universal sentence encoder for semantic\nretrieval. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 87–94.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2019. Adding interpretable attention to neural trans-\nlation models improves word alignment. arXiv\npreprint arXiv:1901.11359.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2020. End-to-end neural word alignment outper-\nforms GIZA++. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1605–1617, Online. Association for\nComputational Linguistics.\nJingyi Zhang and Josef van Genabith. 2021. A bidirec-\ntional transformer based alignment model for unsu-\npervised word alignment. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 283–292, Online. Association\nfor Computational Linguistics.\n2959\nA LaBSE\nLaBSE (Feng et al., 2022) is the state-of-the-art\nmodel for the cross-lingual sentence retrieval task.\nGiven an input sentence, the model can retrieve the\nmost similar sentence from candidates in a differ-\nent language. It has 471M parameters and supports\n109 languages. The model is first pretrained on a\ncombination of masked language modeling (De-\nvlin et al., 2019) and translation language model-\ning (Conneau and Lample, 2019) tasks on the 17B\nmonolingual data and 6B bilingual translation pairs,\nrespectively. After that, it is effectively finetuned\nwith contrastive loss on 6B bilingual translation\npairs across 109 languages.\nSpecifically, given a bilingual sentence pair\n⟨xi,yi⟩, we use exi and eyi to denote their sen-\ntence embeddings from LaBSE. Then the model is\nfinetuned using contrative loss with in-batch nega-\ntives (Chen et al., 2020a):\nℓ= − 1\nN\nN∑\ni=1\n{\nlog exp\n(\nϕ(exi ,eyi )\n)\n∑N\nj=1 exp\n(\nϕ(exi ,eyj )\n)+\nlog exp\n(\nϕ(exi ,eyi )\n)\n∑N\nj=1 exp\n(\nϕ(exj ,eyi )\n)\n}\n, (5)\nwhere ϕ(exi ,eyj ) measures the similarity of sen-\ntence xi and yj in the embedding space:\nϕ\n(\nexi ,eyj\n)\n=\n{\ne⊤\nxi eyj −b if i= j\ne⊤\nxi eyj if i̸= j . (6)\nNote that a margin bis introduced to improve the\nseparation between positive and negative pairs.\nB Experiments Setup\nB.1 Language Code\nWe refer to the language information in Table 1 of\n(Fan et al., 2021). The information of the languages\nused in this paper is listed in Table 5.\nB.2 Dataset\nTable 6 shows the detailed data statistics of\nALIGN6. The ja and zh sentences are preprocessed\nby Dou and Neubig (2021) and Liu and Sun (2015),\nrespectively. For finetuning AccAlign and multilin-\ngual baselines, we use the training and validation\nset from ALIGN6. As bilingual baselines are not\ncapable of zero-shot alignment induction, they are\ntrained from scratch with parallel corpus of the\ntest language pair using the same dataset as Dou\nISO Name Family\nen English Germanic\nnl Dutch Germanic\ncs Czech Slavic\nhi Hindi Indo-Aryan\ntr Turkish Turkic\nes Spanish Romance\npt Portuguese Romance\nde German Germanic\nsv Swedish Germanic\nfr French Romance\nro Romanian Romance\nja Japanese Japonic\nzh Chinese Chinese\nfa Persian Iranian\nTable 5: The information of the languages used in this\npaper.\nand Neubig (2021). The bilingual training data\nset of de/fr/ro/ja/zh-en contain 1.9M, 1.1M, 450K,\n444K and 40K parallel sentence pairs, respectively,\nwhich are much larger than the training dataset of\nALIGN6.\nB.3 Model Setup\nWe use the contextual word embeddings from the\n6-th layer of the official LaBSE3, which have 768\ndimensions. We set the threshold in Equation 2 to\n0.1, which is selected on validation set by manual\ntuning among [0,0.2]. For adapter-based finetun-\ning, we set the hidden dimension of the adapters to\nbe 128. The adapters have 2.4M parameters, which\naccount 0.5% of the parameters of LaBSE. We use\nthe AdamW optimizer with learning rate of 1e-4,\nand do not use warmup or dropout. The batch size\nis set to 40 and maximum updates number is 1500\nsteps. We use a single NVIDIA V100 GPU for all\nexperiments.\nB.4 Baselines\nBesides three statistical baselines fast-align (Dyer\net al., 2013), eflomal (Östling and Tiedemann,\n2016) and GIZA++ (Och and Ney, 2003), we com-\npare AccAlign with the following neural baselines:\nMTL-FULLC-GZ (Garg et al., 2019). This model\nsupervises an attention head in Transformer-based\nNMT model with GIZA++ word alignments in a\nmultitask learning framework.\nBAO-GUIDE (Zenkel et al., 2020). This model\n3https://huggingface.co/sentence-transformers/LaBSE\n2960\nType Lang. Source Link #Sents\nTraining set\ncs-en Mareˇcek (2011) http://ufal.mff.cuni.cz/\nczech-english-manual-word-alignment\n2400\nnl-en Macken (2010) http://www.tst.inl.nl 372\nhi-en Aswani and Gaizauskas (2005)http://web.eecs.umich.edu/~mihalcea/wpt05/90\ntr-en Cakmak et al. (2012) http://web.itu.edu.tr/gulsenc/resources.htm300\nes-en Graca et al. (2008) https://www.hlt.inesc-id.pt/w/Word_Alignments100\npt-en Graca et al. (2008) https://www.hlt.inesc-id.pt/w/Word_Alignments100\nValidation set cs-en Mareˇcek (2011) http://ufal.mff.cuni.cz/\nczech-english-manual-word-alignment\n101\nTest set\nde-en Vilar et al. (2006) http://www-i6.informatik.rwth-aachen.de/\ngoldAlignment/\n508\nsv-en Holmqvist and Ahrenberg (2011)https://www.ida.liu.se/divisions/hcs/nlplab/\nresources/ges/\n192\nfr-en Mihalcea and Pedersen (2003)http://web.eecs.umich.edu/~mihalcea/wpt/ 447\nro-en Mihalcea and Pedersen (2003)http://web.eecs.umich.edu/~mihalcea/wpt05/248\nja-en Neubig (2011) http://www.phontron.com/kftt 582\nzh-en Liu and Sun (2015) https://nlp.csai.tsinghua.edu.cn/~ly/systems/\nTsinghuaAligner/TsinghuaAligner.html\n450\nfa-en Tavakoli and Faili (2014) http://eceold.ut.ac.ir/en/node/940 400\nTable 6: Training, validation and test dataset of ALIGN6. Note that this is a zero-shot setting as the test language\npairs do not appear in training and validation.\nadds an extra alignment layer to repredict the to-be-\naligned target token and further improves perfor-\nmance with Bidirectional Attention Optimization.\nSHIFT-AET (Chen et al., 2020b). This model\ntrains a separate alignment module in a self-\nsupervised manner, and induce alignments when\nthe to-be-aligned target token is the decoder input.\nMASK-ALIGN (Chen et al., 2021a). This model\nis a self-supervised word aligner which makes use\nof the full context on the target side.\nBTBA-FCBO-SST (Zhang and van Genabith,\n2021). This model has similar idea with Chen\net al. (2021a), but with different model architecture\nand training objectives.\nSimAlign (Jalili Sabet et al., 2020). This model is a\nmultilingual word aligner which induces alignment\nwith contextual word embeddings from mBERT\nand XLM-R.\nAwesomeAlign (Dou and Neubig, 2021). This\nmodel improves over SimAlign by designing new\nalignment induction method and proposing to fur-\nther finetune the mPLM on parallel corpus.\nAmong them, SimAlign and AwesomeAlign are\nmultilingual aligners which support multiple lan-\nguage pairs in a single model, while others are\nbilingual word aligners which require training from\nscratch with bilingual corpus for each test lan-\nguage pair. We re-implement SimAlign and Awe-\nsomeAlign, while quote the results from (Dou and\nNeubig, 2021) for the three statistical baselines and\nthe corresponding paper for other baselines.\nB.5 Sentence Transformer\nWe compare LaBSE with four other multilingual\nsentence Transformer in HuggingFace. The de-\ntailed information of these models are:\ndistiluse-base-multilingual-cased-v2.4 This\nmodel is a multilingual knowledge distilled version\nof m-USE (Yang et al., 2020), which has 135M\nparameters and supports more than 50+ languages.\nparaphrase-xlm-r-multilingual-v1.5 This model\nis a multilingual version of paraphrase-distilroberta-\nbase-v1 (Reimers and Gurevych, 2019), which has\n278M parameters and supports 50+ languages. It\ninitializes the student model with an mPLM and\ntrains it to imitate monolingual sentence Trans-\nformer on parallel data with knowledge distillation.\nparaphrase-multilingual-MiniLM-L12-v2.6\nThis model is a multilingual version of paraphrase-\nMiniLM-L12-v2 (Reimers and Gurevych, 2019),\nwhich has 118M parameters and supports 50+\nlanguages. It trains similarly as paraphrase-xlm-\nr-multilingual-v1, but with different teacher and\nstudent model initialization.\nparaphrase-multilingual-mpnet-base-v2.7 This\nmodel is a multilingual version of paraphrase-\nmpnet-base-v2 (Reimers and Gurevych, 2019),\n4https://huggingface.co/sentence-transformers/distiluse-\nbase-multilingual-cased-v2\n5https://huggingface.co/sentence-\ntransformers/paraphrase-xlm-r-multilingual-v1\n6https://huggingface.co/sentence-\ntransformers/paraphrase-multilingual-MiniLM-L12-v2\n7https://huggingface.co/sentence-\ntransformers/paraphrase-multilingual-mpnet-base-v2\n2961\nwhich has 278M parameters and supports 50+ lan-\nguages. It trains similarly as paraphrase-xlm-r-\nmultilingual-v1, but with different teacher model\ninitialization.\nB.6 Bilingual Finetuning\nWe use the same dataset as bilingual baselines for\nbilingual finetuning following (Dou and Neubig,\n2021). At each time, we finetune LaBSE with one\nlanguage pair among de/fr/ro/ja/zh-en and test on\nall seven language pairs. For Awesome-align, we\nfollow the setup in their paper, while for AccAlign,\nwe use the same hyperparameters as the main ex-\nperiments.\nB.7 Representation Analysis\nWe conduct representation analysis on de-en test\nset. To compute sbi, we calculate the averaged co-\nsine similarity of all gold aligned bilingual word\npairs. To compute smono, we randomly permute a\ngiven sentence x = ⟨x1,x2,...,x n⟩to get x′ =\n⟨x′\n1,x′\n2,...,x ′\nn⟩and then create n word pairs as\n{⟨xi-x′\ni⟩}n\ni=1. We go through all de and en test\nsentences and report the averaged cosine similarity\nof all created word pairs as smono.\nC Experiment Results\nDetailed results for each test language in Sec-\ntion 3.3 are shown in Table 7 to Table 10.\n2962\nFt mode Ft type de-en sv-en fr-en ro-en ja-en zh-en fa-en avg\nSelf-supervised full 14.7 5.8 3.7 21.6 39.9 13.3 22.7 17.4\nadapter 14.3 5.8 3.9 21.6 39.2 13.0 22.6 17.2\nSupervised full 13.6 5.3 2.8 21.0 37.1 11.0 22.5 16.2\nadapter 13.6 5.2 2.7 20.8 36.8 11.5 22.2 16.1\nTable 7: AER comparison of full finetuning and adapter-based finetuning. The best AER for each column is bold\nand underlined.\nModel Ft lang.\nTest lang. de-en fr-en ro-en ja-en zh-en sv-en fa-en\nAwesomeAlign\nde-en 14.9 4.7 26.2 43.6 14.6 7.1 28.2\nfr-en 16.4 4.0 26.9 44.6 15.7 7.6 28.0\nro-en 15.8 4.7 22.9 44.2 15.1 7.8 27.0\nja-en 16.8 4.9 27.0 38.1 15.2 8.5 30.0\nzh-en 16.2 4.6 26.2 42.4 14.1 8.1 28.0\nAccAlign\nde-en 14.2 3.8 20.9 39.3 13.1 5.7 22.5\nfr-en 14.6 3.8 20.8 41.0 14.1 6.0 22.5\nro-en 15.2 4.0 21.0 42.1 14.4 6.5 23.2\nja-en 14.8 3.9 20.3 38.0 13.5 6.3 22.5\nzh-en 14.6 3.9 20.7 38.9 13.4 5.9 22.4\nTable 8: AER results with bilingual finetuning. The results where the model is trained and tested on the same\nlanguage pair are bold and underlined.\nlayer de-en sv-en fr-en ro-en ja-en zh-en fas-en avg\nmBERT 8 17.4 8.7 5.6 27.9 45.6 18.1 33.0 22.3\nXLM-R 8 23.1 13.3 9.2 28.6 62.0 30.3 28.6 27.9\ndistiluse-base-multilingual-cased-v2 3 23.7 17.2 9.8 29.2 56.3 29.2 33.5 28.4\nparaphrase-xlm-r-multilingual-v1 6 17.4 8.7 4.9 24.7 53.8 26.1 26.5 23.2\nparaphrase-multilingual-MiniLM-L12-v26 19.4 9.4 6.2 26.0 57.7 29.7 27.4 25.1\nparaphrase-multilingual-mpnet-base-v2 5 18.0 8.9 5.4 24.1 54.9 25.7 25.5 23.2\nLaBSE 6 16.0 7.3 4.5 20.8 43.3 16.2 23.4 18.8\nTable 9: AER comparison of LaBSE and other multilingual pretrained model. All are without finetuning. We\ndetermine the best layer of alignment induction for each model using the validation set. The best AER for each\ncolumn is bold and underlined.\nLayer de-en sv-en fr-en ro-en ja-en zh-en fa-en avg\n0 32.4 27.7 20.5 44.2 65.5 40.1 38.7 38.4\n1 27.3 19.7 12.8 35.6 64.0 33.9 35.4 32.7\n2 22.3 14.0 8.6 28.8 58.0 25.0 31.3 26.9\n3 18.5 9.9 6.0 24.0 50.3 17.9 26.8 21.9\n4 17.7 8.7 5.9 23.3 48.4 16.3 25.7 20.9\n5 15.8 7.4 4.5 21.5 43.7 15.4 23.8 18.9\n6 16.0 7.3 4.5 20.8 43.3 16.2 23.4 18.8\n7 16.5 7.6 4.8 22.4 43.4 15.0 23.7 19.1\n8 16.2 7.3 5.0 21.6 42.7 16.7 23.4 19.0\n9 16.8 7.6 5.3 21.5 42.7 17.9 23.2 19.3\n10 17.7 9.0 5.6 23.0 44.4 20.4 24.4 20.6\n11 36.7 27.0 24.2 43.6 61.3 35.0 46.2 39.1\n12 43.1 33.2 30.5 46.0 65.7 42.6 52.4 44.8\nTable 10: AER comparison of vanilla LaBSE across layers. Layer 0 is the embedding layer. The best AER for each\ncolumn is bold and underlined.\n2963",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8826355934143066
    },
    {
      "name": "Transformer",
      "score": 0.7653450965881348
    },
    {
      "name": "Natural language processing",
      "score": 0.7191120386123657
    },
    {
      "name": "Sentence",
      "score": 0.6952717900276184
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6236358880996704
    },
    {
      "name": "Language model",
      "score": 0.5803344249725342
    },
    {
      "name": "Word (group theory)",
      "score": 0.5576890707015991
    },
    {
      "name": "Task (project management)",
      "score": 0.44182634353637695
    },
    {
      "name": "Speech recognition",
      "score": 0.3321242332458496
    },
    {
      "name": "Linguistics",
      "score": 0.1455572247505188
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I181679659",
      "name": "Shanghai University of Finance and Economics",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3045169105",
      "name": "Southern University of Science and Technology",
      "country": "CN"
    }
  ]
}