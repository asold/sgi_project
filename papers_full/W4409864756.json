{
  "title": "Utilizing Large language models to select literature for meta-analysis shows workload reduction while maintaining a similar recall level as manual curation",
  "url": "https://openalex.org/W4409864756",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5101449031",
      "name": "Xiangming Cai",
      "affiliations": [
        "Amsterdam University Medical Centers",
        "Vrije Universiteit Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5017306869",
      "name": "Yuanming Geng",
      "affiliations": [
        "Jinling Institute of Technology",
        "Nanjing General Hospital of Nanjing Military Command",
        "Nanjing Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5101177096",
      "name": "Yiming Du",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5066774152",
      "name": "Bart A. Westerman",
      "affiliations": [
        "Amsterdam Neuroscience",
        "Amsterdam University Medical Centers",
        "Vrije Universiteit Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A5033299238",
      "name": "Duolao Wang",
      "affiliations": [
        "Liverpool School of Tropical Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5041010870",
      "name": "Chiyuan Ma",
      "affiliations": [
        "Jinling Institute of Technology",
        "Nanjing General Hospital of Nanjing Military Command",
        "Nanjing Medical University",
        "Nanjing University",
        "Southeast University",
        "Southern Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A5013242801",
      "name": "Juan J. García‐Vallejo",
      "affiliations": [
        "Amsterdam University Medical Centers",
        "Vrije Universiteit Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4316507348",
    "https://openalex.org/W3199267631",
    "https://openalex.org/W4225269224",
    "https://openalex.org/W3029811621",
    "https://openalex.org/W4381435533",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W4367186868",
    "https://openalex.org/W4323350039",
    "https://openalex.org/W4385571667",
    "https://openalex.org/W4384641573",
    "https://openalex.org/W2593758073",
    "https://openalex.org/W4385571728",
    "https://openalex.org/W4405655184",
    "https://openalex.org/W4395474395",
    "https://openalex.org/W4401306886",
    "https://openalex.org/W4401306979",
    "https://openalex.org/W4376610005",
    "https://openalex.org/W4366997452",
    "https://openalex.org/W4368356903",
    "https://openalex.org/W4378783233",
    "https://openalex.org/W4319332853",
    "https://openalex.org/W3124072015"
  ],
  "abstract": null,
  "full_text": "Cai et al. BMC Medical Research Methodology          (2025) 25:116  \nhttps://doi.org/10.1186/s12874-025-02569-3\nRESEARCH Open Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/.\nBMC Medical Research\nMethodology\nUtilizing Large language models to select \nliterature for meta-analysis shows workload \nreduction while maintaining a similar recall level \nas manual curation\nXiangming Cai1*†, Yuanming Geng2,3†, Yiming Du4†, Bart Westerman5, Duolao Wang6*, Chiyuan Ma2,3,7,8,9* and \nJuan J. Garcia Vallejo1 \nAbstract \nBackground Large language models (LLMs) like ChatGPT showed great potential in aiding medical research. A heavy \nworkload in filtering records is needed during the research process of evidence-based medicine, especially meta-\nanalysis. However, few studies tried to use LLMs to help screen records in meta-analysis.\nObjective In this research, we aimed to explore the possibility of incorporating multiple LLMs to facilitate the screen-\ning step based on the title and abstract of records during meta-analysis.\nMethods Various LLMs were evaluated, which includes GPT-3.5, GPT-4, Deepseek-R1-Distill, Qwen-2.5, Phi-4, Llama-\n3.1, Gemma-2 and Claude-2. To assess our strategy, we selected three meta-analyses from the literature, together \nwith a glioma meta-analysis embedded in the study, as additional validation. For the automatic selection of records \nfrom curated meta-analyses, a four-step strategy called LARS-GPT was developed, consisting of (1) criteria selection \nand single-prompt (prompt with one criterion) creation, (2) best combination identification, (3) combined-prompt \n(prompt with one or more criteria) creation, and (4) request sending and answer summary. Recall, workload reduction, \nprecision, and F1 score were calculated to assess the performance of LARS-GPT.\nResults A variable performance was found between different single-prompts, with a mean recall of 0.800. Based \non these single-prompts, we were able to find combinations with better performance than the pre-set threshold. \nFinally, with a best combination of criteria identified, LARS-GPT showed a 40.1% workload reduction on average \nwith a recall greater than 0.9.\n†Xiangming Cai, Yuanming Geng and Yiming Du contributed equally and \nshare the co-first authorship.\n*Correspondence:\nXiangming Cai\nx.cai@amsterdamumc.nl\nDuolao Wang\nduolao.wang@lstmed.ac.uk\nChiyuan Ma\nmachiyuan_nju@126.com\nFull list of author information is available at the end of the article\nPage 2 of 10Cai et al. BMC Medical Research Methodology          (2025) 25:116 \nConclusions We show here the groundbreaking finding that automatic selection of literature for meta-analysis \nis possible with LLMs. We provide it here as a pipeline, LARS-GPT, which showed a great workload reduction \nwhile maintaining a pre-set recall.\nKeywords Large language model, Meta-analysis, ChatGPT, Deepseek, Phi\nIntroduction\nThe medical understanding of diseases has advanced \nrapidly during the last decades, but the translation from \nbench to bedside is lagging [1]. Evidence-based medicine \n(EBM), especially meta-analysis, facilitates the applica -\ntion of novel therapies into clinics; however, the processes \nof conducting meta-analysis are time-consuming and \nwork intensive [2]. Artificial intelligence (AI) is becom -\ning ubiquitous in medicine. [1] And AI-based solutions \nare developed to reduce human efforts spent on EBM \nwith promising performance [3]. AI models can provide \npredicted probability for all records based on “similarity” \nbetween them. However, human annotators are needed \nto train the AI models [4, 5]. What’s more, although it \nhelps to accelerate the research process, researchers still \nneed to screen all records.\nRecent releases of large language models (LLMs) \nlike ChatGPT have dramatic implications on medical \nresearch; [6–8] however, few studies have evaluated its \napplication in aiding EBM and review writing. Shaib et al. \nutilized ChatGPT (text-davinci- 003) to synthesize medi -\ncal evidence, [9] and Shuai et al. explored its effectiveness \nin generating Boolean queries for a literature search [10]. \nHowever, almost no study has investigated its applica -\ntion in compensating or substituting human effort spent \non filtering records during meta-analysis, a key issue \nbecause of the exponentially increased number of pri -\nmary literature and systemic reviews required by medical \nresearchers nowadays [11]. Kartchner et al. applied LLM \nto the extraction of clinical data from literature. However, \nthey only tested the performance of GPT 3.5 Turbo and \nGPT-JT [12].\nIn this study, we aimed to explore the possibility of \nusing LLM to aid the automatic selection of literature \nrecords (based on their title and abstract) for meta-anal -\nysis by developing a pipeline named LARS-GPT (Lit -\nerature Records Screener based on ChatGPT-like LLM). \nWith this study, we show a way to integrate LLMs into \nthe field of EBM, which may impact the research pattern \nof meta-analysis.\nMethods\nScreen pipeline incorporating LLM: LARS‑GPT\nIn general, the workflow of meta-analysis has the fol -\nlowing steps: (1) define research question; (2) select lit -\nerature databases and design search strategy; (3) screen \nFig. 1 Schematic illustration of the LARS-GPT pipeline. Single-prompt represents a prompt with only one criterion. Combined-prompt stands \nfor prompt with more than one criterion. Color of labels: single-prompt (blue), combined-prompt and prompt strategy (orange), and answer \nand decision (yellow)\nPage 3 of 10\nCai et al. BMC Medical Research Methodology          (2025) 25:116 \n \nrecords based on their titles and abstracts; (4) screen \nrecords based on full text of records; (5) extract and \nsynthesize data. In the present study, we focused on \nincorporating LLM into the third step of this workflow.\nTo do that, we designed the four-step pipeline, LARS-\nGPT (Fig.  1). First, users need to select criteria (some \nsuitable criteria from filtering criteria of meta-analysis) \nand create a prompt for each criterion (single-prompt; \nTable  1). Second, users need to evaluate these single-\nprompts using a few records and then select the best \ncombination of single-prompts. Third, users need to \nchoose a prompt strategy and merge single-prompts \nin the best combination to make a combined prompt \n(combined-prompt; Supplementary File 1) in accord -\nance with the selected prompt strategy. Finally, the \ncombined-prompt, together with the title and abstract \nof each record, will be submitted to LLM as chat com -\npletion. The decisions about whether a record meets \nthe user’s criteria will then be extracted from returned \nanswers. In practice, LARS-GPT could be performed in \nbatches using Python.\nModels and parameter setting\nIn this study, we evaluated both GPT- 3.5 (gpt- 3.5-turbo- \n0301) and GPT- 4 (gpt- 4–0314) using the API (Applica -\ntion Programming Interface) provided by OpenAI. We \nalso evaluated Deepseek (DeepSeek R1 Distill (Qwen \n7B)) [13], Qwen (Qwen2.5 7B) [14], Phi (phi- 4 14B) [15], \nLlama (Meta Llama 3.1 8B) [16], Gemma (Gemma 2 27B) \n[17] and Claude (Claude2-alpaca- 13B) [18]. LM Studio \n(version 0.3.10) is applied to download and access those \nLLMs locally. Temperature was set to be zero in LLMs, \nwhich means no randomness was introduced while gen -\nerating answers.\nSelection of validation meta‑analyses\nTo cover broad medical fields, we selected three high-\nquality published meta-analyses as validation datasets, \nwhich focused on inflammatory bowel diseases (IBD), \n[19] diabetes mellitus (DM), [20] and sarcopenia, [21] \nrespectively (Table  2). These published meta-analyses \nprovided clear search strategies for Medline/PubMed \ndatabase and a complete list of records that remained \nafter screening based on their titles and abstracts. Thanks \nTable 1 Representative prompt with single criterion (single-prompt)\nsingle‑prompt name single‑prompt content\nSpecies I want you to act as a helpful assistant. I will give you title and abstract of a publication and you will reply whether it meets \nour criteria or not. I want you to only reply with yes, no, or not sure, and followed with reasons. The criteria is: studies that use \nhuman as primary research subject\nDisease …. The criteria is: studies that involve patients with glioma, glioblastoma, astrocytoma, oligodendroglioma\nResearch type …. The criteria is: studies that are prospective or retrospective cohort study, case–control study. Of note, these research types \ndoesn’t meet the criteria: cross-sectional study, randomized controlled trial, review, protocol or others\nAge …. The criteria is: studies that involve adult patients (at least 18 years old)\nProtein related …. The criteria is: The title and abstract must mention that the study is related to the consumption of protein (e.g., total dairy, \nmilk, meat, fish, poultry, process meat, and egg)\nTable 2 Summary of meta-analyses included as validation datasets for LARS-GPT\nFirst author Field Publication \nyear\nJournal Original research Our repetition (validation \ndatasets)\nAll \nidentified \nrecords\nIdentified \nrecords from \nMedline/\nPubMed\nRecords \npreserved \nin title and \nabstract \nscreen step\nIdentified \nrecords from \nMedline/\nPubMed\nRecords \npreserved \nin title and \nabstract screen \nstep (Matched)\nCai X Glioma NA NA 8550 6020 272 1360 264\nTalebi S Inflammatory \nBowel Diseases\n2023 Adv Nutr 2755 1285 51 1284 45\nAune D Diabetes Mel-\nlitus\n2023 Eur J Epidemiol 5320 1040 216 1039 124\nBeaudart C Sarcopenia 2023 J Cachexia \nSarcopenia \nMuscle\n2293 NA 188 1293 122\nPage 4 of 10Cai et al. BMC Medical Research Methodology          (2025) 25:116 \nto this, we were able to repeat their literature search in \nMedline/PubMed and match record list to obtain the \ncorrect answer that whether these identified records \ncould pass the screening step in a real-world practice \n(Table  2; Supplementary File 2). On top of these pub -\nlished meta-analyses, we conducted a new meta-analysis \nabout glioma. The protocol of the glioma meta-analysis \nwas registered on PROSPERO (CRD42023425790). In \ndoing so, we can evaluate the performance of LLM in a \nfirst-hand practice.\nThe number of records used for each step evaluation is \ndifferent, due to the requirements of each step, the work -\nload, and the cost of money. In the final step evaluation \nwith the combined-prompt, almost all records were used \nfor the GPT- 3.5 evaluation. However, only 100 randomly \nselected records were used for the evaluation of other \nmodels, due to the limited funding and long generating \ntime. The detailed randomization method used here can \nbe found in Supplementary File 3.\nStep1: Prompt strategy design\nWe designed prompts (Table  1; Supplementary File 1) \nwith the guidance from OpenAI (https:// platf orm. openai. \ncom/ docs/ guides/ gpt- best- pract ices). However, the high \nflexibility of prompts and the “black box” nature of LLMs \nmade it impossible to design a “best” prompt. In this \nstudy, we designed three distinct types of prompt strate -\ngies to help create better combined-prompt (Fig.  1 and 2; \nSupplementary File 1). For the “single criterion” prompt \n(prompt strategy 1), we simply maintain these single-\nprompts in the best combination. LLMs will respond \nto each single-prompt and determine whether a record \nmeets each criterion or not. After receiving answers from \nLLMs, users need to summarize answers for each single-\nprompt and make a final decision for each record. In this \nstudy, as long as there is one answer that is “No” , the final \ndecision for a record is “No” . Otherwise, the final decision \nwill be “Yes” . For the “instruction prompt” (prompt strat-\negy 3) and “chain of thought prompt” (prompt strategy 2), \nthe best combination of single-prompts was merged into \none combined-prompt (Fig.  1 and 2; Supplementary File \n1). Users expect a final judgment from LLMs directly. In \nthis research, we selected 4–5 criteria from each meta-\nanalysis (Table 1; Supplementary File 1).\nStep2: Evaluation of the classification performance \nof single‑prompt\nWe (XC and YG) manually labeled the correct \nanswers of each single-prompt within 100 randomly \nselected records (about 10 positive records and 90 \nnegative records) for each validation meta-analysis. \nHere, records were called “positive records” if they \nremained after the screening step based on their titles \nand abstracts. Otherwise, they were called “negative \nrecords” . To avoid potential bias from the researchers, \nthese records were manually labeled before we tested \nthem on the LLMs. With these 100-reords datasets, we \nevaluated the performance of LLMs and a random clas -\nsifier regarding single-prompts.\nStep3: Evaluation of single‑prompt combination \nand identification of best combination\nBefore conducting any evaluation, the “best” combination \nof single-prompts was unknown. In other words, how \nmany single-prompts and which single-prompts should \nbe selected for combined-prompt creation? To address \nthis question, we evaluated all possible combinations of \ndesigned single-prompts. Among these combinations, we \nselected the best combination, which has a recall ≥ 0.9 \nand the best workload reduction.\nStatistical analysis\nBecause of the nature of ChatGPT, the generated answer \nfrom ChatGPT varies each time, even with exactly iden -\ntical input. So, we assessed the robustness score of each \nsingle-prompt with repeated requests before testing the \nLARS-GPT pipeline (see Supplementary File 3). In gen -\neral, the returns were stable, with a robustness score \nranging from 0.747 to 0.996 (Supplementary Fig.  1 and \n2). For other models, temperature was set to be zero to \navoid introducing randomness.\nThe performance of LLMs was assessed with precision, \nrecall, F1 score, and workload reduction metrics. The \nworkload reduction indicator was defined as:\nwhere n is the number of records. The workload reduc -\ntion indicator varies between 0 and 1, where 0 indicates \nnone of the work was reduced and 1 signifies that all \nwork was reduced. For meta-analysis, recall is the most \nsignificant indicator, followed with workload reduc -\ntion, F1, and precision. Throughout the study, we placed \ngreater emphasis on recall and workload reduction as the \nprimary performance metrics.\nFor other machine learning (ML) models, it’s possible \nto reach a 100% recall with the compromise of low accu -\nracy. However, due to the distinct mechanisms behind \nLLMs and other ML models, this is impossible for LLM-\nbased solutions, at least for our LARS-GPT pipeline. So, \nin this study, a random classifier was used as a baseline \nreference (see Supplementary File 3). The classifications \nmade by human researchers are used as “true decisions” \nto calculate the performance metrics of the LARS-GPT \npipeline.\nworkload reduction = nrecords excluded by model /nall records\nPage 5 of 10\nCai et al. BMC Medical Research Methodology          (2025) 25:116 \n \nFig. 2 The research flow of this study. A representative case showing a request containing a single-prompt and the response from ChatGPT (A). \nThe schematic illustrations of the research flow (B). Here also shows the detailed input (made by human researchers) for ChatGPT performance \nmetrics calculation. Single-prompt represents a prompt with only one criterion. Combined-prompt stands for the prompt with more than one \ncriterion. Color of labels: single-prompt (blue), combined-prompt and prompt strategy (orange), answer and decision (yellow), and true outcome \nof validation datasets (green)\nPage 6 of 10Cai et al. BMC Medical Research Methodology          (2025) 25:116 \nResults\nSingle‑prompts exhibit distinct performance\nThe performance of each single-prompt was assessed \n(Table 3; Supplementary Table 1). Overall, the majority of \nprompts had better performance with LLMs than a ran -\ndom classifier. However, Claude- 2 had a precision simi -\nlar to that of a random classifier. The mean recall for all \nLLM was 0.800, ranging from 0.72 for Phi- 4 to 0.89 for \nGemma- 2. A total of 63.9% single-prompts had a recall \nhigher than 0.8. Surprisingly, the recalls could be quite \ndifferent between these two versions of GPT, even for \nthe same single-prompt, e.g., the “Control” single-prompt \nfrom sarcopenia meta-analysis (GPT- 3.5: 0.838; GPT- 4: \n0.235; Supplementary Table 1) and the “Protein related” \nsingle-prompt from IBD meta-analysis (GPT- 3.5: 0.897; \nGPT- 4: 0.483; Supplementary Table 1).\nDifferent single-prompts also exhibited distinct recalls. \nMost single-prompts performed well, like the “Species” \nprompt from DM meta-analysis (all models > 0.8; Sup -\nplementary Table 1). However, few single-prompts dem -\nonstrated low recalls, e.g., the “Control” prompt from \nSarcopenia meta-analysis (Qwen- 2.5: 0.191; Llama- 3.1: \n0.044; GPT- 4:0.235; Supplementary Table 1).\nThe best combination of single‑prompts is identified \nby evaluating the performance of all possible \ncombinations\nAll combinations of single-prompts were shown in \nthe form of UpSet plots (Supplementary Fig.  3–6). As \nexpected, when the number of single-prompts increases, \nthe recall tends to decrease, while workload reduction \nand precision increase. In general, most combinations \npresented superior performance compared to a random \nclassifier. To our surprise, it’s not uncommon to find a \ncombination with three single-prompts having a recall of \n0.9 or higher.\nBased on the preset threshold, we identified the best \ncombination with the highest workload reduction \nfrom combinations that have a recall greater than 0.9. \nHowever, in some cases, there was only one combina -\ntion with a recall ≥ 0.9, which only included one single-\nprompt. Because we wanted to evaluate the performance \nof prompt strategies 2 and 3, which were specifically \ntailored for combinations involving multiple single-\nprompts, we selected another combination instead as a \nsub-best combination for the following analyses.\nThree prompt strategies show similar performance\nFull combination (including all designed single-\nprompts) and best combination were both evaluated \nwith three prompt strategies (Supplementary Table  2; \nSupplementary File 1). Obviously, the best combina -\ntions had ideal and much better recalls than full com -\nbinations (mean recalls: 0.876 vs. 0.540) and random \nclassifier. The best combinations demonstrated remark -\nable recalls ranged from 0.889 to 1.000 in 65.5% cases. \nTable 3 Performance of single-prompts from glioma meta-\nanalysis using LLMs, and random classifier\nGlioma\nsingle‑\nprompt\nModel Precision Recall F1 Workload \nreduction\nSpecies GPT- 3.5 0.587 0.786 0.672 0.250\nGPT- 4 0.791 0.607 0.687 0.570\nDeepseek-\nR1-Distill\n0.815 0.946 0.876 0.350\nQwen- 2.5 0.840 0.750 0.792 0.500\nPhi- 4 0.878 0.643 0.742 0.590\nLlama- 3.1 0.700 0.750 0.724 0.400\nGemma- 2 0.662 0.911 0.767 0.230\nClaude- 2 0.529 0.821 0.643 0.130\nRandom clas-\nsifier\n0.557 0.494 0.523 0.504\nDisease GPT- 3.5 0.989 0.905 0.945 0.130\nGPT- 4 1.000 1.000 1.000 0.050\nDeepseek-\nR1-Distill\n1.000 0.705 0.827 0.330\nQwen- 2.5 1.000 0.979 0.989 0.070\nPhi- 4 1.000 0.979 0.989 0.070\nLlama- 3.1 1.000 0.937 0.967 0.110\nGemma- 2 1.000 0.979 0.989 0.070\nClaude- 2 0.958 0.716 0.820 0.290\nRandom clas-\nsifier\n0.950 0.506 0.659 0.494\nTreatment GPT- 3.5 0.530 0.917 0.672 0.170\nGPT- 4 0.745 0.792 0.768 0.490\nDeepseek-\nR1-Distill\n0.721 0.646 0.681 0.570\nQwen- 2.5 0.764 0.875 0.816 0.450\nPhi- 4 0.917 0.688 0.786 0.640\nLlama- 3.1 0.955 0.438 0.601 0.780\nGemma- 2 0.655 0.792 0.717 0.420\nClaude- 2 0.472 0.875 0.613 0.110\nRandom clas-\nsifier\n0.485 0.504 0.494 0.501\nResearch type GPT- 3.5 0.915 0.966 0.940 0.060\nGPT- 4 0.946 0.989 0.967 0.070\nDeepseek-\nR1-Distill\n0.929 0.584 0.717 0.440\nQwen- 2.5 0.977 0.966 0.971 0.120\nPhi- 4 0.944 0.955 0.949 0.100\nLlama- 3.1 0.978 0.978 0.978 0.110\nGemma- 2 0.978 0.978 0.978 0.110\nClaude- 2 0.939 0.697 0.800 0.340\nRandom clas-\nsifier\n0.893 0.495 0.635 0.506\nPage 7 of 10\nCai et al. BMC Medical Research Methodology          (2025) 25:116 \n \nThe corresponding workload reductions varied from 0 \nto 0.890, with an average of 0.401.\nThese three prompt strategies showed comparable \nlevels of performance (Fig.  3), regarding all four met -\nrics. However, these 8 included models had distinct \nperformance (Fig.  4). Claude- 2 showed statistically \nlower precision and F1 scores compared with Gemma- \n2, GPT- 4, Limma- 3.1, Phi- 4, and Qwen- 2.5 (Fig.  4A \nand 4 C). The recall is similar between models, except \nthat Claude- 2 had a higher recall than Qwen- 2.5 \n(Fig.  4B). Also, Claude- 2 showed a much lower work -\nload reduction than other models (Fig. 4 D).\nDiscussion\nIn this research, we developed LARS-GPT and proved \nthat it can greatly reduce the filtering workload \nwhile maintaining an ideal recall during the screen -\ning step based on the titles and abstracts of records for \nmeta-analysis.\nThe mechanism employed by LLMs is different from \nthat of previous AI models. Previous AI models applied \nactive learning to select the training dataset and \nreturned all records ordered by a “similarity” index [5 ]. \nHowever, LLMs have been trained to predict text that \nfollows the input text. By doing so, LLMs can directly \nFig. 3 Comparison of the performance of best and full combinations between three prompt strategies. Comparison of the performance \nbetween three prompt strategies, regarding precision (A), recall (B), F1 score (C), and workload reduction (D)\nFig. 4 Comparison of the performance of best and full combinations between LLMs. Comparison of the performance between 8 models, \nregarding precision (A), recall (B), F1 score (C), and workload reduction (D). The lower panel shows the results of the corresponding non-parametric \nmultiple comparison with a log10 transformed p value\nPage 8 of 10Cai et al. BMC Medical Research Methodology          (2025) 25:116 \nanswer questions and return whether an input record \nmeets the provided criteria or not. Due to the distinct \nmechanisms applied, previous AI models can reach a \nperfect recall with the compromise of low accuracy, but \nnot for LLM-based methods. Thus, we excluded previ -\nous AI models as baseline references for performance \nevaluation in this study.\nLLMs have advantages over previous AI models. One \nadvantage is that extra training is unnecessary when \napplying LLMs to a new meta-analysis (although fine-\ntuning is possible) because LLMs were pre-trained on \nlarge-scale datasets. In comparison, a training dataset \nis required for every new meta-analysis if choosing \nprevious AI models. Additionally, users do not need \nto worry about the imbalanced data problem [5 ] when \nusing LARS-GPT for the same reason.\nAn obvious benefit of LARS-GPT is that it could be \neasily adapted to other LLMs by simply changing the \nAPI, since most LLMs work similarly. However, the per -\nformance of LARS-GPT depends on the performance \nof the LLM used, which is not guaranteed. We also \nbelieve that a well-performed prompt could be used \nfor other LLMs. However, further research is needed to \nverify this idea of adapting LARS-GPT to other LLMs.\nLLM hallucinations are one issue that has been \nemphasized in research. These hallucinations occur \nwhen a LLM makes up fake information and describes \nit like it is real [22, 23]. LARS-GPT avoids this issue \nbecause users need to provide the titles and abstracts \nof records to ChatGPT, rather than having ChatGPT \nsearch for the information. Nonetheless, we did observe \ninstances where ChatGPT made false causal inferences. \nFor example, ChatGPT might give a reason supporting \na record meeting one filtering criterion, which is then \nfollowed by an opposite judgment. A similar false con -\nclusion may occur when users ask ChatGPT to sum -\nmarize a final judgment, e.g., “The publication meets \ncriterion 1, but not criterion 2. So, the publication \nmeets all your criteria. ” Despite occasional false judg -\nment, LARS-GPT demonstrated an ideal performance \nin the current research.\nSurprisingly, in this study, GPT- 4’s performance was \nnot much better than GPT- 3.5. Although GPT- 4 may \nbe more accurate, it could have lower recall compared \nto GPT- 3.5 (Supplementary Table 2), and recall is much \nmore important than precision when screening litera -\nture for a meta-analysis. Furthermore, when evaluating \nthe performance of three prompt strategies, Claude- 2 \nshowed lower precision, F1 score, and workload reduc -\ntion than other models (Fig.  4). The other 7 models had \nsimilar performance across all measures. In short, in the \ncontext of this research, no model was overwhelmingly \nsuperior to the other one, except Claude- 2.\nIt is important to evaluate the performance of LARS-\nGPT in various scenarios. Thus, in the study, we selected \n4 meta-analyses with distinct types of diseases, which \nstand for cancer, immune-related disease, metabolic-\nrelated disease, and skeletal muscle disorder, respec -\ntively. In general, LARS-GPT demonstrated an ideal \nperformance on all of them (Supplementary Table  2). \nWhat really impacts the performance of LARS-GPT is \nthe prompts designed, which also highlights the value of \nprompt design steps in our pipeline.\nIn this study, a single-prompt is developed from a sin -\ngle filtering criteria, and a key step of single-prompt crea-\ntion is the selection of criteria. Potential criteria should \nbe derived from the inclusion and exclusion criteria of \nthe designed meta-analysis. In some cases, however, \nresearchers need to extract information from a subgroup \nanalysis, which may not be presented in the title and \nabstract of a record, e.g. materials used in surgery, [24] \nand criteria related to such information are not suitable \nfor prompt creation. To avoid this issue, it is better to use \noptions that are more likely to be adequately judged using \nonly the title and abstract of the record, which are cri -\nteria related to “Species” , “Disease” , and “Research type” . \nIn fact, the majority of the best combinations identified \nin the current research were based on these three crite -\nria. Thus, users are recommended to try them first when \nusing LARS-GPT.\nTo apply LARS-GPT, users need to manually label a few \nrecords for single-prompts so that the best combination \ncan be identified. Based on our experiences, to be well \nevaluated, each single-prompt needs around 10 positive \nand 10 negative records. Considering overlaps between \nthe records for single-prompts, researchers need to label \nabout 20 to 100 records for five single-prompts. Once an \napplication based on LARS-GPT is developed, it will be \nmuch easier to do this labeling.\nWe tried three prompt strategies, including a “chain of \nthought prompt” (prompt strategy 2) that was designed \nfollowing the OpenAI’s guidelines. Surprisingly, all three \nprompt strategies showed comparable performance (Sup-\nplementary Table 2; Fig. 3). Indeed, the “chain of thought \nprompt” takes more time for LLMs to answer in a more \norganized format. However, this improvement does not \ntranslate into enhanced performance in LARS-GPT. A \npossible reason is that the two other “less structured” \nstrategies already guided LLMs sufficiently. However, \ndue to the “black box” nature of LLMs, we cannot explain \nthe phenomenon. As a result, users are recommended to \nselect whichever they prefer.\nIn our research, we did not use metrics like Work \nSaved over Sampling (WSS) and Average Time to \nDiscover (ATD), [5 ] which have been commonly \nused to evaluate previous AI models. This is because \nPage 9 of 10\nCai et al. BMC Medical Research Methodology          (2025) 25:116 \n \nLARS-GPT works in a completely different way, as \nmentioned before. Within LARS-GPT, LLMs will \ndirectly answer whether to include or exclude a record, \ninstead of returning a probability for it.\nIn the filtering step of meta-analysis, a high recall is \nof very priority. There is a possibility that recalls are \nnot satisfied, even though we have included a “recall \n> 0.9” criteria in choosing the best combination in the \nLARS-GPT. The balance between recall and precision \nis always a difficult issue to be addressed. Users might \ntry some more single criteria in the beginning of LARS-\nGPT to have a best combination with high recall. Also, \nit is worthwhile to randomly check the filtered results \nafter applying LLM.\nConclusion\nThis study developed a pipeline named LARS-GPT, and \nusing this pipeline showed that an automatic selection \nof records for a meta-analysis is possible with LLMs. \nThree prompt strategies showed similar performance. \nAll LLMs evaluated, except Claude- 2, also have com -\nparable performance. Further research may incorporate \nLLMs (and the multiple LLMs approach) into other \nsteps of the meta-analysis workflow.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s12874- 025- 02569-3.\nSupplementary Material 1.\nSupplementary Material 2.\nAcknowledgements\nWe would like to thank researchers that created these open-source models. \nWe also want to thank OpenAI for sharing ChatGPT with the research field. \nThis study was funded by the China Scholarship Council (CSC; grant no. \n202206090022).\nAuthors’ contributions\nXC and CM conceived the idea. XC designed the study. XC, YG, and YD devel-\noped the methodology, acquired, and analyzed the data. XC, BW, DW, and \nJJGV were involved in the interpretation of data. XC drafted the manuscript, \nand all authors edited the manuscript. All authors had full access to the raw \ndata in the study. XC and YG accessed and verified the data. XC and JJGV \noversaw the conduct of the study. All authors contributed to the article and \napproved the submitted version. All authors had final responsibility for the \ndecision to submit for publication.\nFunding\nThis study was funded by the China Scholarship Council (CSC; grant no. \n202206090022).\nData availability\nThe original code used in this paper is available on GitHub (https://github.\ncom/xiangmingcai/LARS). All responses from LLMs can be found in Sup-\nplementary File 2. Any additional information required is available from the \ncorresponding author upon request.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nAuthor details\n1 Department of Molecular Cell Biology & Immunology, Amsterdam Infection \n& Immunity Institute and Cancer Center Amsterdam, Amsterdam UMC, Vrije \nUniversiteit Amsterdam, Amsterdam, The Netherlands. 2 Department of Neuro-\nsurgery, Jinling Hospital, Nanjing, China. 3 Department of Neurosurgery, Affili-\nated Jingling Hospital, Nanjing Medical University, Nanjing, China. 4 Depart-\nment of System Engineering and Engineering Management, The Chinese \nUniversity of Hong Kong, Hong Kong, China. 5 Department of Neurosurgery, \nCancer Center Amsterdam, Brain Tumor Center Amsterdam, Amsterdam UMC \nLocation Vrije Universiteit Amsterdam, Amsterdam, The Netherlands. 6 Depart-\nment of Clinical Sciences, Liverpool School of Tropical Medicine, Liverpool, \nUK. 7 School of Medicine, Southeast University, Nanjing, China. 8 Department \nof Neurosurgery, Affiliated Jinling Hospital, Medical School of Nanjing Univer-\nsity, Nanjing, China. 9 Department of Neurosurgery, Jinling Hospital, the First \nSchool of Clinical Medicine, Southern Medical University, Nanjing, China. \nReceived: 9 January 2025   Accepted: 15 April 2025\nReferences\n 1. Subbiah V. The next generation of evidence-based medicine. Nat Med. \n2023;29(1):49–58.\n 2. Abdelkader W, Navarro T, Parrish R, Cotoi C, Germini F, Linkins LA, et al. A \nDeep Learning Approach to Refine the Identification of High-Quality Clin-\nical Research Articles From the Biomedical Literature: Protocol for Algo-\nrithm Development and Validation. JMIR Res Protoc. 2021;10(11):e29398.\n 3. Tercero-Hidalgo JR, Khan KS, Bueno-Cavanillas A, Fernández-López R, \nHuete JF, Amezcua-Prieto C, et al. Artificial intelligence in COVID-19 \nevidence syntheses was underutilized, but impactful: a methodological \nstudy. J Clin Epidemiol. 2022;148:124–34.\n 4. Gates A, Gates M, Sebastianski M, Guitard S, Elliott SA, Hartling L. The \nsemi-automation of title and abstract screening: a retrospective explora-\ntion of ways to leverage Abstrackr’s relevance predictions in systematic \nand rapid reviews. BMC Med Res Methodol. 2020;20(1):139.\n 5. Ferdinands G, Schram R, de Bruin J, Bagheri A, Oberski DL, Tummers L, \net al. Performance of active learning models for screening prioritization in \nsystematic reviews: a simulation study into the Average Time to Discover \nrelevant records. Syst Rev. 2023;12(1):100.\n 6. van Dis EAM, Bollen J, Zuidema W, van Rooij R, Bockting CL. ChatGPT: five \npriorities for research. Nature. 2023;614(7947):224–6.\n 7. Li H, Moon JT, Purkayastha S, Celi LA, Trivedi H, Gichoya JW. Ethics of large \nlanguage models in medicine and medical research. Lancet Digit Health. \n2023;5(6):e333–5.\n 8. Ali SR, Dobbs TD, Hutchings HA, Whitaker IS. Using ChatGPT to write \npatient clinic letters. Lancet Digit Health. 2023;5(4):e179–81.\n 9. Shaib C, Li M, Joseph S, Marshall IJ, Li JJ, Wallace B. Summarizing, sim-\nplifying, and synthesizing medical evidence using GPT-3 (with Varying \nSuccess). ACL. In: Proceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short Papers). 2023. pp. \n1387–1407. https:// aclan tholo gy. org/ 2023. acl- short. 119/.\n 10. Wang S, Scells H, Koopman B, Zuccon G. Can ChatGPT write a good \nboolean query for systematic review literature search?. In: Proceedings of \nthe 46th International ACM SIGIR Conference on Research and Develop-\nment in Information Retrieval. 2023. pp. 1426–36. https:// dl. acm. org/ doi/ \n10. 1145/ 35396 18. 35917 03.\nPage 10 of 10Cai et al. BMC Medical Research Methodology          (2025) 25:116 \n 11. Borah R, Brown AW, Capers PL, Kaiser KA. Analysis of the time and workers \nneeded to conduct systematic reviews of medical interventions using \ndata from the PROSPERO registry. BMJ Open. 2017;7(2):e012545.\n 12. Kartchner D, Ramalingam S, Al-Hussaini I, Kronick O, Mitchell C. Zero-shot \ninformation extraction for clinical meta-analysis using large language \nmodels. ACL. In: The 22nd Workshop on Biomedical Natural Language \nProcessing and BioNLP Shared Tasks. 2023. pp. 396–405. https:// aclan \ntholo gy. org/ 2023. bionlp- 1. 37/.\n 13. DeepSeek AI. DeepSeek-R1: Incentivizing reasoning capability in LLMs via \nreinforcement learning. 2025. https:// doi. org/ 10. 48550/ arXiv. 2501. 1294.\n 14. Yang A, Yu B, Li C, Liu D, Huang F, Huang H, et al. Qwen2.5–1M technical \nreport. 2025. https:// doi. org/ 10. 48550/ arXiv. 2412. 15115.\n 15. Abdin M, Aneja J, Behl H, Bubeck S, Eldan R, Gunasekar S, et al. Phi-4 \ntechnical report. 2024. https:// doi. org/ 10. 48550/ arXiv. 2404. 14219.\n 16. Grattafiori A, Dubey A, Jauhri A, Pandey A, Kadian A, Al-Dahle A, et al. The \nLlama 3 herd of models. 2024. https:// doi. org/ 10. 48550/ arXiv. 2407. 21783.\n 17. Team G, Riviere M, Pathak S, Sessa PG, Hardin C, Bhupatiraju S, et al. \nGemma 2: Improving open language models at a practical size. 2024. \nhttps:// doi. org/ 10. 48550/ arXiv. 2408. 00118.\n 18. Chen L, Saifullah K, Li M, Zhou T, Huang H. Claude2-Alpaca: Instruction \ntuning datasets distilled from claude. GitHub repository. 2023. https:// \ngithub. com/ Licha ng- Chen/ claud e2- alpaca.\n 19. Talebi S, Zeraattalab-Motlagh S, Rahimlou M, Naeini F, Ranjbar M, Talebi A, \net al. The Association between Total Protein, Animal Protein, and Animal \nProtein Sources with Risk of Inflammatory Bowel Diseases: A Systematic \nReview and Meta-Analysis of Cohort Studies. Adv Nutr. 2023;14(4):752–61.\n 20. Aune D, Schlesinger S, Mahamat-Saleh Y, Zheng B, Udeh-Momoh CT, \nMiddleton LT. Diabetes mellitus, prediabetes and the risk of Parkinson’s \ndisease: a systematic review and meta-analysis of 15 cohort stud-\nies with 29.9 million participants and 86,345 cases. Eur J Epidemiol. \n2023;38(6):591–604.\n 21. Beaudart C, Demonceau C, Reginster JY, Locquet M, Cesari M, Cruz \nJentoft AJ, et al. Sarcopenia and health-related quality of life: A \nsystematic review and meta-analysis. J Cachexia Sarcopenia Muscle. \n2023;14(3):1228–43.\n 22. Jin Q, Leaman R, Lu Z. Retrieve, summarize, and verify: How will ChatGPT \naffect information seeking from the medical literature? J Am Soc Nephrol. \n2023;34(8):1302–4.\n 23. Stokel-Walker C, Van Noorden R. What ChatGPT and generative AI mean \nfor science. Nature. 2023;614(7947):214–6.\n 24. Cai X, Yang J, Zhu J, Tang C, Cong Z, Liu Y, et al. Reconstruction strategies \nfor intraoperative CSF leak in endoscopic endonasal skull base surgery: \nsystematic review and meta-analysis. Br J Neurosurg. 2022;36(4):436–46.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Workload",
  "concepts": [
    {
      "name": "Workload",
      "score": 0.8610057830810547
    },
    {
      "name": "Meta-analysis",
      "score": 0.7417430877685547
    },
    {
      "name": "Recall",
      "score": 0.6204509735107422
    },
    {
      "name": "Computer science",
      "score": 0.5683858394622803
    },
    {
      "name": "Psychology",
      "score": 0.33627933263778687
    },
    {
      "name": "Medicine",
      "score": 0.3079548478126526
    },
    {
      "name": "Cognitive psychology",
      "score": 0.18828549981117249
    },
    {
      "name": "Pathology",
      "score": 0.1603630781173706
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}