{
  "title": "General purpose large language models match human performance on gastroenterology board exam self-assessments",
  "url": "https://openalex.org/W4387008801",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4317043392",
      "name": "Shuhaib Ali",
      "affiliations": [
        "The University of Texas Health Science Center at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A2255512367",
      "name": "Omer Shahab",
      "affiliations": [
        "Virginia Hospital Center"
      ]
    },
    {
      "id": "https://openalex.org/A2937009265",
      "name": "Reem Al Shabeeb",
      "affiliations": [
        "Inova Fairfax Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1981612773",
      "name": "Farah Ladak",
      "affiliations": [
        "The University of Texas Health Science Center at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A2924024367",
      "name": "Jamie O. Yang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2419298921",
      "name": "Girish Nadkarni",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2497869150",
      "name": "Juan Echavarria",
      "affiliations": [
        "The University of Texas Health Science Center at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A2888786938",
      "name": "Sumbal Babar",
      "affiliations": [
        "The University of Texas Health Science Center at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A2041234489",
      "name": "Aasma Shaukat",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2036204426",
      "name": "ali soroush",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2606641357",
      "name": "Bara El Kurdi",
      "affiliations": [
        "The University of Texas Health Science Center at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A4317043392",
      "name": "Shuhaib Ali",
      "affiliations": [
        "The University of Texas Health Science Center at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A2255512367",
      "name": "Omer Shahab",
      "affiliations": [
        "Virginia Hospital Center"
      ]
    },
    {
      "id": "https://openalex.org/A2937009265",
      "name": "Reem Al Shabeeb",
      "affiliations": [
        "Inova Fairfax Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1981612773",
      "name": "Farah Ladak",
      "affiliations": [
        "The University of Texas Health Science Center at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A2924024367",
      "name": "Jamie O. Yang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2419298921",
      "name": "Girish Nadkarni",
      "affiliations": [
        "Child Health and Development Institute",
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2497869150",
      "name": "Juan Echavarria",
      "affiliations": [
        "The University of Texas Health Science Center at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A2888786938",
      "name": "Sumbal Babar",
      "affiliations": [
        "Texas Center for Infectious Disease",
        "The University of Texas Health Science Center at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A2041234489",
      "name": "Aasma Shaukat",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2036204426",
      "name": "ali soroush",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai",
        "The University of Texas Health Science Center at San Antonio",
        "Child Health and Development Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2606641357",
      "name": "Bara El Kurdi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3046375318",
    "https://openalex.org/W6850668563",
    "https://openalex.org/W4376640725",
    "https://openalex.org/W4379599010",
    "https://openalex.org/W4380291159",
    "https://openalex.org/W4367175039",
    "https://openalex.org/W4385491575",
    "https://openalex.org/W4376461276",
    "https://openalex.org/W4367175507",
    "https://openalex.org/W4377220156",
    "https://openalex.org/W4389518686",
    "https://openalex.org/W4225080353",
    "https://openalex.org/W1975356361"
  ],
  "abstract": "Abstract Introduction While general-purpose large language models(LLMs) were able to pass USMLE-style examinations, their ability to perform in a specialized context, like gastroenterology, is unclear. In this study, we assessed the performance of three widely available LLMs: PaLM-2, GPT-3.5, and GPT-4 on the most recent ACG self-assessment(2022), utilizing both a basic and a prompt-engineered technique. Methods We interacted with the chat interfaces of PaLM-2, GPT-3.5, and GPT-4. We first applied a basic prompt approach, providing each exam question and answer text with minimalist text descriptions of any images. For the engineered approach, we added additional context and instructions. We assessed each model-prompt combination in terms of overall and difficulty-stratified performance and compared this to average human performance. We also evaluated each model’s self-assessed uncertainty. The highest scoring model-prompt combination was further assessed on the 2021 exam. We also assessed the impact of image descriptions on our findings. Results Using a basic prompt, PaLM-2, GPT-3.5, and GPT-4 achieved scores of 32.6%, 55.3%, and 68.9% respectively. With the engineered prompt, scores improved to 42.7%, 65.2%, and 76.3% respectively. Testing GPT-4 on the ACG-2021 exam yielded a similar score(75.3%). GPT-4 scores matched the average score for human test-takers reported by ACG(75.7%). GPT-4 showed a capability to self-assess its confidence accurately in the context of a multiple-choice exam with its confidence estimates falling within 5% of its actual performance. Excluding image-based questions didn’t change the primary findings. Discussion Our study highlights the capability of GPT-4 to answer subspecialty board-exam questions at a level commensurate with the average human test-taker. The results confirm that prompt-engineering can enhance LLMs’ performance on medical reasoning tasks. We also show GPT-4 can provide insightful measures of uncertainty in the setting of board-style multiple-choice questions, alerting users to low-quality answers. Future studies of LLMs in gastroenterology should incorporate prompt-engineering to maximize model capabilities. WHAT IS KNOWN State of the Art large language models like GPT-4 and PaLM-Med 2 have achieved above average performance on USMLE board examinations. In a previous study using basic model prompt instructions, GPT 3.5 and GPT 4 did not pass the 2021 and 2022 ACG self-assessment exams. WHAT IS NEW HERE Optimizing large language model prompt instructions improved the performance of chat-based GPT-3.5, GPT-4, and PaLM 2 on the ACG self-assessment exams. With optimized prompt instructions, chat-based GPT-4 performed at the level of average human test takers on ACG-self assessment examinations and achieved a passing score. Chat-based GPT-4 self-reported confidence levels correlated with correct answer rates on the ACG-self assessment examinations.",
  "full_text": "Title: General purpose large language models match human performance on gastroenterology \nboard exam self-assessments. \nShuhaib Ali1*, Omer Shahab2*, Reem Al Shabeeb3, Farah Ladak1, Jamie O. Yang4, Girish \nNadkarni8,9, Juan Echavarria1, Sumbal Babar5, Aasma Shaukat6, Ali Soroush7,8,9**, Bara El \nKurdi1** \n*Co-first author (equal contribution). \n**Co-senior author (equal contribution). \nAffiliations:  \n1. University of Texas Health, Department of Medicine, Division of Gastroenterology and \nHepatology, San Antonio, Texas, USA.  \n \n2. Division of Gastroenterology, Virginia Hospital Center, Arlington, Virginia, USA. \n \n3. Department of Medicine, Inova Fairfax Medical Campus, Falls Church, Virginia, USA. \n \n4. University of California Los Angeles, Department of Internal Medicine, Los Angeles, CA, \nUSA. \n \n5. University of Texas Health, Department of Medicine, Division of Infectious Diseases, San \nAntonio, Texas, USA. \n \n6. Division of Gastroenterology, Department of Medicine, NYU Grossman School of Medicine, \nNew York, NY, USA. \n \n7. Henry D. Janowitz Division of Gastroenterology, Icahn School of Medicine at Mount Sinai, \nNew York, New York, USA. \n \n8. Division of Data-Driven and Digital Medicine (D3M), Icahn School of Medicine at Mount Sinai, \nNew York, New York, USA. \n \n9. The Charles Bronfman Institute of Personalized Medicine, Icahn School of Medicine at Mount \nSinai, New York, New York, USA. \n \n \nFunding: None \n \n \n \n \n \n \n \n \nWHAT IS KNOWN: \n1. State of the Art large language models like GPT-4 and PaLM-Med 2 have achieved above \naverage performance on USMLE board examinations.  \n \n2. In a previous study using basic model prompt instructions, GPT 3.5 and GPT 4 did not pass \nthe 2021 and 2022 ACG self-assessment exams.  \n \nWHAT IS NEW HERE: \n1. Optimizing large language model prompt instructions improved the performance of chat-\nbased GPT-3.5, GPT-4, and PaLM 2 on the ACG self-assessment exams.  \n \n2. With optimized prompt instructions, chat-based GPT-4 performed at the level of average \nhuman test takers on ACG-self assessment examinations and achieved a passing score. \n \n3. Chat-based GPT-4 self-reported confidence levels correlated with correct answer rates on \nthe ACG-self assessment examinations. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAbstract: \nIntroduction: \nWhile general-purpose large language models(LLMs) were able to pass USMLE-style \nexaminations, their ability to perform in a specialized context, like gastroenterology, is unclear. \nIn this study, we assessed the performance of three widely available LLMs: PaLM-2, GPT-3.5, \nand GPT-4 on the most recent ACG self-assessment(2022), utilizing both a basic and a prompt-\nengineered technique. \nMethods: \nWe interacted with the chat interfaces of PaLM-2, GPT-3.5, and GPT-4. We first applied a basic \nprompt approach, providing each exam question and answer text with minimalist text \ndescriptions of any images. For the engineered approach, we added additional context and \ninstructions. We assessed each model-prompt combination in terms of overall and difficulty-\nstratified performance and compared this to average human performance. We also evaluated \neach model’s self-assessed uncertainty. The highest scoring model-prompt combination was \nfurther assessed on the 2021 exam. We also assessed the impact of image descriptions on our \nfindings.  \nResults: \nUsing a basic prompt, PaLM-2, GPT-3.5, and GPT-4 achieved scores of 32.6%, 55.3%, and \n68.9% respectively. With the engineered prompt, scores improved to 42.7%, 65.2%, and 76.3% \nrespectively. Testing GPT-4 on the ACG-2021 exam yielded a similar score(75.3%). GPT-4 \nscores matched the average score for human test-takers reported by ACG(75.7%). GPT-4 \nshowed a capability to self-assess its confidence accurately in the context of a multiple-choice \nexam with its confidence estimates falling within 5% of its actual performance. Excluding image-\nbased questions didn’t change the primary findings. \nDiscussion: \nOur study highlights the capability of GPT-4 to answer subspecialty board-exam questions at a \nlevel commensurate with the average human test-taker. The results confirm that prompt-\nengineering can enhance LLMs’ performance on medical reasoning tasks. We also show GPT-4 \ncan provide insightful measures of uncertainty in the setting of board-style multiple-choice \nquestions, alerting users to low-quality answers. Future studies of LLMs in gastroenterology \nshould incorporate prompt-engineering to maximize model capabilities. \n  \nIntroduction: \nState of the art large language models (LLMs) have become increasingly capable of a diverse \nrange of tasks, ranging from natural language processing to predicting protein sequences. \nThese LLMs are utilize the transformer deep learning architecture (1) and self-supervised \nlearning (2,3) to train on massive datasets. This results in a foundational artificial intelligence \nmode that is capable of a broad range of tasks without significant additional model training. The \ngeneral-purpose capabilities of these LLMs has already begun to have an impact across various \nindustries, such as medicine (4–6).  \nPrevious generation LLMs have already been tailored for medical use cases such as \nPubMedBERT (7), ClinicalBERT(8) with limited success. Another model; GatorTron (9) shortly \nfollowed, it was trained on PubMed, Wikipedia, and EHR clinical note text, but was only able to \nperform a limited set of natural processing tasks. The first LLM to achieve a passing score on \nUSMLE-style questions, Med-PaLM, is an instruction-tuned version of Google’s Flan-PaLM \nmodel (10) that is further “prompt tuned” to align model responses to the requirements of the \nmedical domain (11). Newer LLMs such as the next-generation Med-PaLM 2 (12) as well as \nOpenAI’s GPT-3.5(13) and GPT-4(14) follow the same general instruction-based framework of \nthe Flan-PaLM model, but with even larger training datasets and further model alignment (12). \nWith these advances, the most advanced GPT-4 and Med-PaLM2 models have achieved \nexpert-level performance on most standardized medical reasoning metrics with only a minimal \ndegree of prompt engineering (12,15). Both successfully and comfortably passed the USMLE \nexamination with a cushion of more than 20% (12,15).  \nThough the models performed well with USMLE board exam questions, their performance on \nspecialty and subspecialty board exams that are less likely to be present in the training data and \nrequire an even higher level of expertise is still a subject of debate. A growing number of studies \nhave assessed this in different medical specialties including radiology (16), otolaryngology (17), \nneurosurgery (18), ophthalmology (19) and(20), orthopedic surgery (21), and bariatric surgery \n(22) (Supplementary Table 1). A recent study evaluated baseline GPT-3.5 and GPT-4 \ngastroenterology reasoning performance using the 2021 and 2022 American College of \nGastroenterology’s Self-Assessment exams (ACG-SA), finding that both models did not score \nhigh enough to pass (23). Notably, this study did not employ the prompt engineering methods \nutilized in key prior medical reasoning papers, which have previously been shown to improve \nLLM reasoning performance (24–26). Surprisingly, GPT-3.5 outscored the newer GPT-4 in this \nstudy, in sharp contrast to prior studies. Additionally, the study did not compare LLM \nperformance to human test takers. \nSupplementary Table 1: LLM performance on specialty and subspecialty board exams. (N/A = \nnot applicable or not reported). \nStudy Field Model \nused \nPerformance \nfor each \nmodel \nDid model \npass test? \nPrompt \nengineering \nBhayana et al. \n(16) \nRadiology GPT 3.5 69% Narrowly \nfailed \n(Passing \nscore \n70%) \nN/A \nHoch et al. (17) Otolaryngology GPT 3.5 57% N/A N/A \nAli et al. (18) Neurosurgery GPT-3.5 \nGPT-4 \nPaLM \n62.4 \n82% \n44.2% \nN/A N/A \nMihalache et al. \n(19) \nOphthalmology GPT 3.5 46% N/A N/A \nCai et al. (20) Ophthalmology GPT-3.5 \nGPT-4 \n58.8% \n71% \nN/A N/A \nLum et al. (21) Orthopedic \nsurgery \nGPT 3.5 47% Failed N/A \nSamaan et al. \n(22) \nBariatric surgery GPT 3.5 86.8% N/A N/A \n \nTo interrogate the gastroenterology reasoning capabilities of current generation LLMs further, \nwe explored the effect of prompt engineering and assessed LLM performance versus humans at \nthe global and question levels. In this study, we assessed the performance of GPT-3.5 and \nGPT-4 and the now publicly available PaLM 2.  \nMethods: \nExam Question Pre-Processing \nBecause access to image-capable LLMs has been limited, previous studies have excluded \nquestions with images to isolate the models’ text-based reasoning capabilities (12,15,16). Since \nour question set was small and contained a large proportion of questions with images, we \nelected 1) to assess if a provided image was necessary to answer the associated question \ncorrectly and 2) to provide text descriptions for any necessary images. Two gastroenterologists \nindividually performed the evaluations and generated text descriptions that conveyed descriptive \nvisual information without providing any high-level classification or key buzzwords which could \nreveal the answer independently. For example, an image of a perianal abscess was described \nas “a small perianal bulge with overlying redness and surrounding erythema.” Discrepancies in \nevaluations and descriptions were resolved by consensus.  \nLarge Language Model Approach \nFor this study, all three language models were accessed in May 2023, directly through their \nrespective public chat interfaces: Bard (PaLM 2), ChatGPT (GPT-3.5), and ChatGPT plus (GPT-\n4). The 2022 ACG-SA examination was first provided to all three LLMs (PaLM 2, GPT-3.5, and \nGPT-4) to determine the best performing model. Each question was entered in succession \nfollowing the same order provided by the test. A new chat session was started for each unique \nexam, model, and prompt strategy combination.  \nTo assess baseline model capabilities, we elicited exam answers using a basic prompt strategy \nof only providing each individual question and its answer choices. To assess more advanced \nlatent reasoning capabilities, we applied a more advanced engineered prompt strategy. For this \napproach, the following additional text was added before the basic prompt: \"You are a \ngastroenterology fellow undergoing board examination. You will be given a list of multiple-choice \nquestions. Please read each question carefully and assess/consider each choice separately \nstep by step before providing your answer. You should also state your degree of confidence in \nyour answers on a scale from 0 to 100. Do this systematically and in a step-by-step fashion.\"  \nIn the prompt engineered approach, including additional contextual information allows the LLM \nto better calibrate its responses (26). Asking the model to think step-by-step elicits the “chain-of-\nthought” method of augmenting model reasoning by forcing the model to generate descriptive \ntext in a step-by-step manner (25). Finally, LLM self-confidence assessments in the context of \nmultiple-choice questions have been shown to have reasonable accuracy (27–29).  \nThe LLM and prompting strategy that collectively demonstrated the best performance was \nadditionally validated on the ACG-SA 2021 exam. \nPerformance Metrics \nAnswer choices generated by each LLM were compared against the exam answer key to \ncalculate the overall, and difficulty-stratified percentage of correct answers for each LLM. \nAverage scores for human test takers were kindly provided by ACG upon request. To evaluate \nperformance based on questions difficulty, we stratified the questions into 4 categories based on \npercentage of human respondents that answered them correctly: 12-40% was considered very \nchallenging, 40-60% was considered challenging, 60-80% was considered moderate and 80-\n100% was considered easy. To generate a calibration curve for each model, exam questions \nwere grouped based on the model’s confidence in its answer into 4 categories, mean model \nconfidence for each category was plotted against the mean actual percentage correct.  A \ncalibration plot was used to represent this as described by Nori et al. (15). To assess the impact \nof our image-based descriptions on our analysis, we used a two-sample t-test with a \nsignificance level of 0.05 to compare LLM performance on questions with and without images.  \nResults: \nExam Characteristics \nThe 2022 ACG-SA examination consisted of 300 questions, of which 137 contained images \n(Supplementary Table 2). For 36 of these image-based questions, the image was either not \nrequired to answer the question or the question text provided an adequate description. The \n2021 ACG-SA examination contained the same total number of questions, but contained 114 \nwith images. Of those image-based questions, only seven could be answered without \nsupplemental information. Neither of the assessed examinations is publicly available and \naccessible for inclusion in LLM training data.  \nSupplementary Table 2: Exam Characteristics \n 2022 2021 \nQuestions   \n  Image Present \n     Description Needed \n     No Description Needed \n  No Image Present \n137 \n  101 \n  36 \n163 \n114 \n  107 \n  7 \n186 \n \nLLM Performance \nUsing the basic prompting strategy, PaLM 2 achieved a score of 32.6% correct answers, while \nGPT-3.5 and GPT-4 scored 55.3% and 68.9% respectively (Table 1). PaLM 2 was unable to \nanswer 76 questions, citing its limited capability to provide clinical responses (e.g., \"I'm a text-\nbased AI, and that is outside of my capabilities\"). When these questions were excluded, PaLM \n2's performance improved to 43.8%. Using the engineered prompt strategy, the scores of PaLM \n2, GPT-3.5, and GPT-4 improved to 42.7%, 63.0%, and 76.3% respectively. PaLM 2 could not \nprovide answers to only 26 questions with this approach, which was improved compared to the \nbasic prompting strategy (when these questions were excluded, performance improved to \n47.8%).  \nTable 1: Overall LLM Performance on ACG-SA 2022. \nPrompt Strategy Model \n PaLM 2 GPT-3.5 GPT-4 \n % Correct % Answered % Correct % Answered % Correct % Answered \nBasic 32.6% 74.7% 55.3% 100% 68.9% 100% \nEngineered 42.7% 91.3% 63.0% 100% 76.3% 100% \n \nFigure 1. LLM performance on ACG-SA 2022 stratified by model and approach.  \n \nUsing an engineered prompt strategy with GPT-4 produced the highest exam score. To confirm \nthe validity of these results, we applied this approach to the 2021 ACG-SA examination. The \nmodel performance remained consistent, scoring 75.3% on the older exam. When adjusting for \nquestion difficulty, GPT-4's performance based on question difficulty was consistently within the \nrange of human performers for both the 2021 and 2022 exams, while GPT-3.5’s lagged closely \nbehind, and PaLM-2 significantly underperformed at all levels. Data shared by ACG showed \naverage performance by human test takers at 75.7%(Table 2). \n \n \n \n\nTable 2: LLM Performance, Stratified by Question Difficulty \n Humans \n(% correct) \nGPT-4 2022  \n(% correct) \nGPT-4 2021 \n(% correct) \nGPT-3.5 2022 \n(% correct) \nPaLM-2 2022 \n(% correct) \nAll Questions 75.7% 76.3% 75.3% 63.0% 42.7% \n      \nQuestion difficulty      \n  Very Challenging 12-40% 26.9% 31.6% 26.9% 11.5% \n  Challenging 40-60% 70.2% 59.1% 51.4% 32.4% \n  Medium 60-80% 71.1% 73.1% 56.7% 40.0% \n  Easy 80-100% 89.1% 87.5% 61.9% 54.4% \n \nUncertainty Assessment \nThe engineered prompt strategy also sought to self-assess model confidence for each exam \nanswer. PaLM 2 was unable to perform this task. GPT-4 self-characterized its response \ncertainty between 40% and 95% and generally matched its actual aggregated performance \nwithin a range of 5%. In contrast, GPT 3.5 self-assessed its certainty between 40% and 95%, \nbut its aggregate response accuracy never surpassed 75%. At its most confident, GPT-3.5 \noverestimated its actual performance by more than 20%. (Table 3, Figure 2). \nTable 3: LLM performance based on self-reported uncertainty (predicted probability that a \nprovided answer is correct).  \n GPT-4 GPT-3.5 PaLM 2 \nMean predicted \nprobability Questions (n) Correct (%) Questions (n) Correct (%) Questions (n) Correct (%) \n55% 53 56.6% 45 51.1% N/A N/A \n75% 46 67.4% 93 54.8% N/A N/A \n85% 148 79.7% 140 70.7% N/A N/A \n95% 53 88.7% 22 72.7% N/A N/A \nFigure 2: Large Language Model Response Confidence Calibration Curve\n \n \nImage Description Sensitivity Analysis \nNo significant difference in performance was observed across models for questions with or \nwithout images (Table 2). This applied to both basic and engineered approaches.   \nSupplementary Table 2: The impact of image descriptions on model performance (2022 ACG-\nSA).  \n PaLM 2  GPT-3.5 GPT-4 \n \nNo image \nquestions \n(% correct) \nWith image \ndescriptions \n(% correct) \np-\nValue \nNo image \nquestions \n(% correct) \nWith image \ndescriptions \n(% correct) \np-\nValue \nNo image \nquestions \n(% correct) \nWith image \ndescriptions \n(% correct) \np-\nValue \nBasic 27.8% 32.6% 0.27 56.9% 55.3% 0.31 69.7% 68.2% 0.50 \nEngineered 43.0% 42.7% 0.87 63.2% 63.0% 0.97 73.5% 75.7% 0.46 \n \n \n \n\n \nDiscussion: \nIn this study, we found that even a simple prompt engineering strategy can significantly enhance \nthe performance of LLMs on subspecialty clinical examinations. With prompt engineering, GPT-\n4 obtained passing scores on both the 2021 and 2022 ACG-SA exams and performed \nconsistently within the same range as human test takers overall as well as for every question \ndifficulty level. We also found that self-reported uncertainty measurements were reasonably \naccurate for GPT-4, suggesting that prompts that include uncertainty measures may help alert \nusers to low quality responses.  \nWhile GPT-3.5’s performance improved with the engineered prompt, it failed to reach the \npassing threshold of 70%. Additionally, it significantly overestimated its true response accuracy \nby significant margins at the upper ends of self-reported confidence, indicating that its \nuncertainty measures are unreliable when the model is reporting high levels of confidence. \nThese findings highlight the significant improvements in newer iterations of the GPT model and \nparallel previously published studies(15).   \nIn contrast, we found that the base PaLM 2 model is poorly suited for use in the medical \ndomain. PaLM 2 repeatedly declined to provide answers to the exam questions and refused to \nperform any self-assessments of certainty, likely due to model limitations imposed at the system \nlevel. However, PaLM 2 had much worse performance than both GPT-3.5 or GPT-4, even when \nexcluding refused questions. It is likely that the medically tailored Med-PaLM 2 model performs \nbetter with the type of assessment in this study, but at the time of publication we did not have \naccess to this more tailored model.  \nNo meaningful difference in our results were noted when excluding image questions with our \nsupplementary image descriptions, indicating that this approach can be used to increase \ndataset pool until future models are able to process medical images.  \nOur findings contrast with those of a highly publicized recent study by Suchman et al.(23), which \nshowed that GPT-3.5 and GPT-4 could not pass the 2021 and 2022 ACG-SA exams. We \nsuspect there are several reasons for the differences between the results of our studies. The \ndifferences in performance when using a basic prompt strategy may be due to underlying \nimprovements in the GPT models or perhaps subtle differences in how each study interfaced \nwith the LLM. Our largest performance gain, however, was from the use of a prompt engineering \nstrategy incorporating additional contextual information and a chain of thought instruction.  \nThe training data and model architecture of GPT-4 are not known, making it difficult to \nunderstand how or why GPT-4 is able to reason to such a high degree on medical board exams. \nIt is likely that the model was trained, at minimum, on all available open-source text datasets \nincluding textbooks, Wikipedia, PubMed abstracts, and open-access journal articles. Like all \nLLMs, GPT-4 “reasons” by using a statistical model to serially predict chunks of text, or \n“tokens”(14). Prompt engineering methods help guide LLMs to prioritize text responses that are \nwell-reasoned and correct and to narrow the knowledge sources used for reasoning tasks(30). \nLLM-type reasoning appears to be well-suited for multiple-choice question reasoning, but it is \nunclear if this extends to the more ambiguous reasoning of day-to-day clinical tasks.  \nOur study has several limitations. Our engineered prompt approach did not include more \nadvanced prompt engineering such as few-shot prompting, self-consistency, ensemble \nrefinement, or response chaining methods to enhance reasoning performance (12). We elected \nto avoid these to demonstrate the benefit of even a simple, no-code prompt-based approach. \nWe also avoided changes to model temperature and system prompts. These approaches could \nimprove outcomes but require software expertise that most clinicians do not possess and \ntherefore, maybe out of reach for our intended audience. Additionally, the ACG self-assessment \nis an imperfect exam and does not reflect all aspects of medical reasoning used in routine \nclinical practice. Finally, our human-generated image descriptions may be biasing model \nperformance for image-based questions and would not reflect a real-world use case for the \nLLMs. It is notable that the tested LLMs consistently performed slightly better on questions \nwithout images, indicating that image descriptions may be useful but do not replace the need for \nindependent image analysis. \nDespite these limitations, our study has several strengths. We have the broadest range of data \nassessing LLMs in the gastroenterology space to date, assessing the impact of simple prompt \nengineering, comparing model performance to human test takers, and examining self-\nassessment capabilities for all three widely available chat-based LLMs. Our assessment of a \nno-code prompt engineering approach allows the average clinician to understand and \nimplement our methodology for future research. We also describe a new approach for inclusion \nof image-based questions in test datasets, increasing the pool of questions to be considered in \nfuture LLM research.  \nThe high level of subspeciality board exam medical reasoning demonstrated by GPT-4 with a \nsimple prompt engineering strategy holds great promise for gastroenterology medical education \nand other clinical reasoning tasks. Self-paced chatbot-based tutors the Khan Academy’s \nKhanmigo (31) could be adapted for subspecialty medical training, increasing access to expert \nclinical knowledge and reasoning and democratizing medical education. Future work should \nassess multimodal models that can ingest multiple data types (e.g., text and images) as well as \nthe integration and application of additional reasoning augmentation strategies.  \nWith further refinements and appropriate validation of medical reasoning capabilities, state of \nthe art LLMs could also power intelligent clinical decision support tools, clinical workflows, and \npatient-support interfaces. LLM augmentation techniques such as model finetuning (32), \nadvanced prompt engineering (26), prompt chaining (33), and database linkage (34) have been \napplied successfully to improve reasoning in other contexts. Smaller, more efficient models are \nalso being developed (35), which will improve model speed and reduce computational costs. \nBefore integrating LLMs into clinical practice, a comprehensive characterization of their clinical \nreasoning capabilities with real-world clinical scenarios is needed.  \nConclusion: \nWe find that GPT-4 was able to pass the ACG self-assessment exam with an engineered \nprompt, performing within the range of average human test takers and providing insightful \nmeasures of uncertainty. While other models failed this examination, the success of GPT-4 \nsuggests that further applications in clinical education and practice may become possible as \nthese models and our skills in interfacing them continue to evolve.  \n \nReferences: \n1.  Vaswani A, Shazeer N, Parmar N, et al. Attention Is All You Need [Internet]. 2017;[cited \n2023 Jun 28] Available from: http://arxiv.org/abs/1706.03762 \n2.  Radford A, Narasimhan K, Salimans T, et al. Improving Language Understanding by \nGenerative Pre-Training.  \n3.  Devlin J, Chang M-W, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers \nfor Language Understanding [Internet]. 2019;[cited 2023 Jun 28] Available from: \nhttp://arxiv.org/abs/1810.04805 \n4.  Kaplan J, McCandlish S, Henighan T, et al. Scaling Laws for Neural Language Models \n[Internet]. 2020;[cited 2023 Jun 28] Available from: http://arxiv.org/abs/2001.08361 \n5.  Liang P, Bommasani R, Lee T, et al. Holistic Evaluation of Language Models [Internet]. \n2022;[cited 2023 Jun 28] Available from: http://arxiv.org/abs/2211.09110 \n6.  Bommasani R, Hudson DA, Adeli E, et al. On the Opportunities and Risks of Foundation \nModels [Internet]. 2022;[cited 2023 Jun 28] Available from: http://arxiv.org/abs/2108.07258 \n7.  Gu Y, Tinn R, Cheng H, et al. Domain-Specific Language Model Pretraining for Biomedical \nNatural Language Processing. ACM Trans. Comput. Healthc. 2022;3:1–23. \n8.  Huang K, Altosaar J, Ranganath R. ClinicalBERT: Modeling Clinical Notes and Predicting \nHospital Readmission [Internet]. 2020;[cited 2023 Jun 29] Available from: \nhttp://arxiv.org/abs/1904.05342 \n9.  Yang X, Chen A, PourNejatian N, et al. GatorTron: A Large Clinical Language Model to \nUnlock Patient Information from Unstructured Electronic Health Records [Internet]. \n2022;[cited 2023 Jun 28] Available from: http://arxiv.org/abs/2203.03540 \n10.  Wei J, Bosma M, Zhao VY, et al. Finetuned Language Models Are Zero-Shot Learners \n[Internet]. 2022;[cited 2023 Jul 12] Available from: http://arxiv.org/abs/2109.01652 \n11.  Singhal K, Azizi S, Tu T, et al. Large Language Models Encode Clinical Knowledge \n[Internet]. 2022;[cited 2023 Jun 29] Available from: http://arxiv.org/abs/2212.13138 \n12.  Singhal K, Tu T, Gottweis J, et al. Towards Expert-Level Medical Question Answering with \nLarge Language Models [Internet]. 2023;[cited 2023 Jun 28] Available from: \nhttp://arxiv.org/abs/2305.09617 \n13.  Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human \nfeedback [Internet]. 2022;[cited 2023 Jul 12] Available from: \nhttp://arxiv.org/abs/2203.02155 \n14.  OpenAI. GPT-4 Technical Report [Internet]. 2023;[cited 2023 Jul 12] Available from: \nhttp://arxiv.org/abs/2303.08774 \n15.  Nori H, King N, McKinney SM, et al. Capabilities of GPT-4 on Medical Challenge Problems.  \n16.  Bhayana R, Krishna S, Bleakney RR. Performance of ChatGPT on a Radiology Board-\nstyle Examination: Insights into Current Strengths and Limitations. Radiology \n2023;307:e230582. \n17.  Hoch CC, Wollenberg B, Lüers J-C, et al. ChatGPT’s quiz skills in different otolaryngology \nsubspecialties: an analysis of 2576 single-choice and multiple-choice board certification \npreparation questions. Eur. Arch. Oto-Rhino-Laryngol. Off. J. Eur. Fed. Oto-Rhino-\nLaryngol. Soc. EUFOS Affil. Ger. Soc. Oto-Rhino-Laryngol. - Head Neck Surg. 2023; \n18.  Ali R, Tang OY , Connolly ID, et al. Performance of ChatGPT, GPT-4, and Google Bard on a \nNeurosurgery Oral Boards Preparation Question Bank. Neurosurgery 2023; \n19.  Mihalache A, Popovic MM, Muni RH. Performance of an Artificial Intelligence Chatbot in \nOphthalmic Knowledge Assessment. JAMA Ophthalmol. 2023;141:589–597. \n20.  Cai LZ, Shaheen A, Jin A, et al. Performance of Generative Large Language Models on \nOphthalmology Board Style Questions. Am. J. Ophthalmol. 2023;S0002-9394(23)00230–1. \n21.  Lum ZC. Can Artificial Intelligence Pass the American Board of Orthopaedic Surgery \nExamination? Orthopaedic Residents Versus ChatGPT. Clin. Orthop. 2023; \n22.  Samaan JS, Yeo YH, Rajeev N, et al. Assessing the Accuracy of Responses by the \nLanguage Model ChatGPT to Questions Regarding Bariatric Surgery. Obes. Surg. \n2023;33:1790–1796. \n23.  Suchman K, Garg S, Trindade AJ. Chat Generative Pretrained Transformer Fails the \nMultiple-Choice American College of Gastroenterology Self-Assessment Test. Am. J. \nGastroenterol. 2023; \n24.  Liévin V, Hother CE, Winther O. Can large language models reason about medical \nquestions? [Internet]. 2023;[cited 2023 Jul 12] Available from: \nhttp://arxiv.org/abs/2207.08143 \n25.  Wei J, Wang X, Schuurmans D, et al. Chain-of-Thought Prompting Elicits Reasoning in \nLarge Language Models [Internet]. 2023;[cited 2023 Jun 28] Available from: \nhttp://arxiv.org/abs/2201.11903 \n26.  White J, Fu Q, Hays S, et al. A Prompt Pattern Catalog to Enhance Prompt Engineering \nwith ChatGPT [Internet]. 2023;[cited 2023 Jul 2] Available from: \nhttp://arxiv.org/abs/2302.11382 \n27.  Tian K, Mitchell E, Zhou A, et al. Just Ask for Calibration: Strategies for Eliciting Calibrated \nConfidence Scores from Language Models Fine-Tuned with Human Feedback [Internet]. \n2023;[cited 2023 Jul 12] Available from: http://arxiv.org/abs/2305.14975 \n28.  Lin S, Hilton J, Evans O. Teaching Models to Express Their Uncertainty in Words [Internet]. \n2022;[cited 2023 Jul 12] Available from: http://arxiv.org/abs/2205.14334 \n29.  Kadavath S, Conerly T, Askell A, et al. Language Models (Mostly) Know What They Know \n[Internet]. 2022;[cited 2023 Jul 12] Available from: http://arxiv.org/abs/2207.05221 \n30.  Zhao WX, Zhou K, Li J, et al. A Survey of Large Language Models [Internet]. 2023;[cited \n2023 Jul 17] Available from: http://arxiv.org/abs/2303.18223 \n31.  Singer N. New A.I. Chatbot Tutors Could Upend Student Learning [Internet]. N. Y. Times \n2023;Available from: https://www.nytimes.com/2023/06/08/business/khan-ai-gpt-tutoring-\nbot.html \n32.  Wu C, Zhang X, Zhang Y , et al. PMC-LLaMA: Further Finetuning LLaMA on Medical \nPapers [Internet]. 2023;[cited 2023 Jul 2] Available from: http://arxiv.org/abs/2304.14454 \n33.  Wu T, Jiang E, Donsbach A, et al. PromptChainer: Chaining Large Language Model \nPrompts through Visual Programming [Internet]. 2022;[cited 2023 Jul 2] Available from: \nhttp://arxiv.org/abs/2203.06566 \n34.  Bohensky MA, Jolley D, Sundararajan V, et al. Data Linkage: A powerful research tool with \npotential problems. BMC Health Serv. Res. 2010;10:346. \n35.  Werra L von, Belkada Y , Mangrulkar S, et al. Falcon 40B LLM. [Internet]. Available from: \nhttps://huggingface.co/blog/falcon \n ",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.7284393906593323
    },
    {
      "name": "Computer science",
      "score": 0.47592002153396606
    },
    {
      "name": "Confidence interval",
      "score": 0.42630690336227417
    },
    {
      "name": "Test (biology)",
      "score": 0.4102014899253845
    },
    {
      "name": "Medicine",
      "score": 0.37598052620887756
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36645978689193726
    },
    {
      "name": "Internal medicine",
      "score": 0.35807815194129944
    },
    {
      "name": "Medical physics",
      "score": 0.3220517039299011
    },
    {
      "name": "Biology",
      "score": 0.0844217836856842
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165951966",
      "name": "The University of Texas Health Science Center at San Antonio",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210139747",
      "name": "Virginia Hospital Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800468613",
      "name": "Inova Fairfax Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I98704320",
      "name": "Icahn School of Medicine at Mount Sinai",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ]
}