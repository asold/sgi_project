{
    "title": "MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic",
    "url": "https://openalex.org/W4389519950",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2755994633",
            "name": "Damien Sileo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5091962996",
            "name": "Antoine Lernould",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2889936086",
        "https://openalex.org/W2804343002",
        "https://openalex.org/W4283793025",
        "https://openalex.org/W4318069317",
        "https://openalex.org/W2397494983",
        "https://openalex.org/W4385573981",
        "https://openalex.org/W4308939312",
        "https://openalex.org/W3035561057",
        "https://openalex.org/W4321277158",
        "https://openalex.org/W3200664512",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W2970536767",
        "https://openalex.org/W2970062726",
        "https://openalex.org/W4378474033",
        "https://openalex.org/W4389519164",
        "https://openalex.org/W4367189299",
        "https://openalex.org/W4319452268",
        "https://openalex.org/W2093410327",
        "https://openalex.org/W4362655426",
        "https://openalex.org/W4385767671",
        "https://openalex.org/W3034830866",
        "https://openalex.org/W4246720192",
        "https://openalex.org/W2889107415",
        "https://openalex.org/W2791261897",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1525961042",
        "https://openalex.org/W2782381758",
        "https://openalex.org/W3212781905",
        "https://openalex.org/W2074111633",
        "https://openalex.org/W3211686893",
        "https://openalex.org/W4385571259",
        "https://openalex.org/W2962736243",
        "https://openalex.org/W4385718134"
    ],
    "abstract": "International audience",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4570–4577\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMindGames: Targeting Theory of Mind in Large Language Models\nwith Dynamic Epistemic Modal Logic\nDamien Sileo\nUniv. Lille, Inria, CNRS\nCentrale Lille, UMR 9189\nCRIStAL, F-59000 Lille, France\ndamien.sileo@inria.fr\nAntoine Lernould\nUniv. Lille\nCRIStAL, F-59000 Lille, France\nAbstract\nTheory of Mind (ToM) is a critical component\nof intelligence but its assessment remains the\nsubject of heated debates. Prior research ap-\nplied human ToM assessments to natural lan-\nguage processing models using either human-\ncreated standardized tests or rule-based tem-\nplates. However, these methods primarily fo-\ncus on simplistic reasoning and require further\nvalidation. Here, we leverage dynamic epis-\ntemic logic to isolate a particular component of\nToM and to generate controlled problems. We\nalso introduce new verbalization techniques to\nexpress these problems in English natural lan-\nguage. Our findings indicate that some lan-\nguage model scaling (from 70M to 6B and\n350M to 174B) does not consistently yield re-\nsults better than random chance. While GPT-4\ndemonstrates superior epistemic reasoning ca-\npabilities, there is still room for improvement.\nOur code and datasets are publicly available1\n1 Introduction\nTheory of Mind (ToM) is the cognitive ability to at-\ntribute mental states, such as beliefs, desires, and in-\ntentions, to oneself and others, allowing individuals\nto understand and predict behavior based on these\ninferred mental states. It is an important require-\nment for general text understanding or artificial in-\ntelligence (Navarro et al., 2020), but claims about\nToM are prone to bias from human expectations\n(de Waal, 2016). Kosinski (2023) recently sparked\ndebate by showing that scaling large language mod-\nels (LLMs) improves performance at standardized\ntests designed to measure ToM. However, these\ntests were widely discussed in academic research\nand might have leaked into the training corpora\nof LLM. Earlier work generated synthetic exam-\nples instead, extending the bAbi (Weston et al.,\n2016) framework. Nematzadeh et al. (2018) pro-\nposed a dataset of fixed templates based on the\n1[code:GitHub][data:HF-datasets]\nSally-Anne problem (Baron-Cohen et al., 1985):\nSally puts a marble in a box while Anne is with\nher. Sally leaves for a moment and Mary puts\nthe marble in a basket. Where will Sally look\nfor the marble? [ANSWER =BOX]\nLe et al. (2019) deem these problems simplistic\nand extend them to track second-order beliefs (e.g.\nthe belief of Sally about Anne’s beliefs).\nIn our study, we generate dynamic epistemic\nlogic (DEL) problems and develop verbalizations\nto transform them into natural language infer-\nence problems. DEL is a branch of modal logic\nthat can model an individual’s knowledge about\nparticular facts or about other agents’ knowl-\nedge. DEL also enables reasoning about the\nimpact of consecutive public announcements:\nAlice and Bob have mud on their head. Their\nfather says that at least one of them is muddy.\nHe asks Alice and Bob if they are muddy. Do\nAlice and Bob know that they are muddy? [AN-\nSWER =NO] They answer that they don’t know.\nDo Alice and Bob now know that they are\nmuddy? [ANSWER =YES]\nBob would have answered YES to the first ques-\ntion if Alice was not muddy, so after Bob’s first\nanswer, Alice can know that she is muddy.2 DEL\ncan formalize certain ToM problems, making it\na valuable perspective for ToM assessment. The\nproblems we create can require tracking multiple\nagents’ beliefs and reasoning about higher-order\nbeliefs3. Our dataset encompasses numerous vari-\nations of the Muddy Children and Drinking Logi-\ncians problems (van Eijck, 2014). This controlled\ntest bench offers new appreciations of language\nmodel scaling and presents the first dataset with a\ncomplexity that can challenge supervised learning\nmodels. The dataset and the scripts to generate is\npublicly available1.\n2The same holds if we switch Bob and Alice.\n3For example, Anne’s belief about Sally’s belief about\nAnne’s belief about Mary’s belief.\n4570\n2 Related Work\nLogical Reasoning in Natural Language Process-\ning Logic shares profound connections with NLP.\nEarly systems were built around logic, and more\nrecent approaches incorporate logical reasoning\ninto neural networks (Hamilton et al., 2022; Helwe\net al., 2022). Another line of research closer to ours\ninvestigates the logical capabilities of NLP models\nusing textual datasets and labels generated with log-\nical reasoning tools. RuleTaker (Clark et al., 2020)\nexplores this area with propositional logic, while\nLogicNLI addresses first-order logic (Tian et al.,\n2021). Richardson and Sabharwal (2022) examine\nthe satisfiability problem in natural language. Sileo\nand Moens (2022) targets probabilistic logic. Our\nstudy is the first to focus on modal logic, specifi-\ncally epistemic logic, in natural language.\nTheory of Mind in NLP To measure ToM capa-\nbilities of NLP models, Nematzadeh et al. (2018)\ncreated examples using Sally-Ann templates, and\nLe et al. (2019) added complexity to the data by\nincorporating second-order knowledge. Both stud-\nies framed their examples as question-answering\ntasks. Kosinski (2023) employed handcrafted tests\nto evaluate language models’ next-word prediction\ncapabilities. Ullman (2023) showed LLM brittle-\nness to interventions on these datasets and Ma et al.\n(2023) consolidated the prior datasets into a prin-\ncipled evaluation suite. The Social-IQA dataset\n(Sap et al., 2019) covers a broad spectrum of social\ncommonsense, encompassing aspects of theory of\nmind and challenges like comprehending desires\nand emotions. Cohen (2021) investigated whether\nnatural language inference models captured veridi-\ncality with epistemic verbs like know and think,\nusing handcrafted patterns. This task was incorpo-\nrated into the BIG-Bench framework (Srivastava\net al., 2022) as the epistemic-reasoning task, but\nit targets only one shallow aspect of epistemic rea-\nsoning. Bara et al. (2021) used a Minecraft dataset\nfor real-time belief deduction in collaborative tasks.\nShapira et al. (2023b) highlighted LLM struggles\nin faux pas tests. Shapira et al. (2023a) conducted\nstress tests on LLMs’ social reasoning capabilities.\nEpistemic Logic and ToM Bolander (2018)\nshowed that the Sally-Ann problem could be mod-\neled with epistemic logic. Van Ditmarsch and\nLabuschagne (2007) examined more general con-\nnections between DEL and ToM, while Dissing and\nBolander (2020) demonstrated DEL’s applicability\nin robotics. Van De Pol et al. (2018) explored the\nplausibility of epistemic logic for ToM by investi-\ngating its theoretical computational tractability.\n3 Dynamic Epistemic Logic Problem\nGeneration and Verbalization\n3.1 Problem definition\nOur objective is to simultaneously create dynamic\nepistemic logic problems and their corresponding\nnatural language representations, with a (PREMISE ,\nHYPOTHESIS , LABEL ) format.\nAn epistemic logic problem can be decomposed\ninto the following components:\nAgents: A set of N individuals, each assigned a\ndifferent arbitrary name.\nPredicates: A set of Boolean predicates. Here,\nwe use N predicates, one corresponding to each\nagent (e.g., Alice has mud on her head).\nObservabilities: The description of each agent’s\ninitial knowledge of the predicate values. We repre-\nsent observabilities with a boolean matrixOof size\nN×N, where Oi,j=1 means that agent i initially\nknows whether predicate j is true.\nAnnouncements: A list of expressions (predi-\ncates or agent knowledge about predicates) that are\nshared to all agents. Announcements are made se-\nquentially, and each new announcement can change\nwhat the agents know, even if it is the same an-\nnouncement is repeated twice.\nHypothesis: An expression that may contain\npredicates and knowledge of agents about partic-\nular expressions after the announcements, given\nthe agents, observabilities, and announcements\ngrouped into a premise.\n3.2 Setups: connecting predicate and\nobservabilities\nThe concrete choice of predicates dictates the struc-\nture of observabilities. For example, the predi-\ncate \"Alice has mud on her head\" is observable by\nagents other than Alice, but \"Alice has mud on her\nhand\" could be observable by everyone. We group\npredicates and observabilities into what we call se-\ntups to generate textual descriptions. We define the\nfollowing setups:\nForehead-mud setup\nPREDICATE i: <AGENT i>’s forehead is muddy.\nO: ONES (N) −IDENTITY (N)\n4571\n70M 160M410M 1B 1.4B 2.8B 6.9B\nPythia size\n0.5\n0.6\n0.7\n0.8\n0.9\nForehead-mud-mirror setup\n70M 160M410M 1B 1.4B 2.8B 6.9B\nPythia size\nThirst setup\n70M 160M410M 1B 1.4B 2.8B 6.9B\nPythia size\nForehead-mud setup\n70M 160M410M 1B 1.4B 2.8B 6.9B\nPythia size\nExplicit setup\n0-shot\n5-shot\nHumans\nFigure 1: Accuracy of Pythia language models on MindGames setups.\n350M 1.3B 6.7B 174B\nGPT-3 size\n0.5\n0.6\n0.7\n0.8\n0.9\nForehead-mud-mirror setup\n350M 1.3B 6.7B 174B\nGPT-3 size\nThirst setup\n350M 1.3B 6.7B 174B\nGPT-3 size\nForehead-mud setup\n350M 1.3B 6.7B 174B\nGPT-3 size\nExplicit setup\n0-shot\n5-shot\nHumans\nFigure 2: Accuracy of GPT-3 family (ada, cabbage, curie, davinci) language models on MindGames setups.\nForehead-mud-mirror setup\nPREDICATE i: <AGENT i>’s forehead is muddy.\nO: ONES (N)\nOBSERVATION : There is a mirror in the room.\nThirst setup\nPREDICATE i: <AGENT i>’s is thirsty.\nO: IDENTITY (N)\nExplicit setup\nPREDICATE i: <AGENT i> picked a red card.\nO: RANDBOOL (N, N), E(sum(O))=N\nOBSERVATION : Each person draws a card,\nface unrevealed (red or black). < <AGENT j>\ncard is revealed to < AGENT i>. for all i, j\nwhere Oi,j=1>\n3.3 Problem verbalization\nWe then construct a problem for a given setup with\nthe following natural language template:\n[Premise] There are <N> persons. Everyone is\nvisible to others. < OBSERVATION > It is publicly\nannounced that someone <PREDICATE > <[0 −N]\nANNOUNCEMENTS >\n[Hypothesis] <[1 −K]th ORDER BELIEF >\n[0 −N] denotes uniform sampling from 0 to\nN. We restrict announcements to first-order be-\nliefs. A first-order belief has the following struc-\nture: < AGENT > (can know whether | can know\nthat | cannot know that | cannot know whether )\n(<PREDICATE >|<NEGATED -PREDICATE >), e.g.\nAlice cannot know whether Bob is not muddy. We\nuse can to acknowledge that an agent could theo-\nretically infer something but fail to see it. A Kth\norder belief is a first-order belief about a (K−1)th\norder belief. We consider everyone, not everyone,\nand nobody as possible subjects for the setup predi-\ncates. Subjects are uniformly sampled among these\nquantifiers and the list of individual agents. We\ntransform abstract problem representations into nat-\nural language and code that can be fed to a model\nchecker to determine whether a hypothesis is en-\ntailed by the premise. We use the SMCDEL model\nchecker (Benthem et al., 2018), an announcement\nlogic based on the S5 (Lewis et al., 1959) modal\nlogic. This implementation is the most cited pub-\nlicly available epistemic logic as of April 2023.\nWe discard examples where the premise contains\na contradiction4. To generate diverse and gender-\nbalanced random English surnames, we use Cen-\nsusName5 (Qian et al., 2022).\n4 Experiments\n4.1 Problem generation parameters\nWe randomly sample N∈{2, 3, 4}agents, as we\nobserved that problems were sufficiently challeng-\n4We identify contradictions by examining whether an un-\nused predicate is entailed or not by the premise.\n5https://pypi.org/project/censusname/\n4572\ning with only three agents, and we useK=2 for the\nsame reason. We use knowledge predicate nega-\ntions 80% of the time to encourage richer infer-\nences (as the fact that an agent does not know\nsomething conveys information to others) in an-\nnouncements and 50% of the time otherwise.\n4.2 Controlling for example difficulty\nShortcuts, like hypothesis only bias (Gururangan\net al., 2018; Zhang et al., 2023), can lead to the\nanswer without correct reasoning. To control for\nshortcuts, we trained a relatively shallow super-\nvised model (deberta-small (He et al., 2021), 6\nlayers, 44M backbone parameters) on a training\nset combining all setups (ensuring that there was\nno duplicate and no example that was also in the\ntest set). We used 11.2k training examples for 3\nepochs and a learning rate of 3e-5 and 3.73k test\nand validation examples. Overall validation accu-\nracy was 83%. We also experimented with simpler\nlexical baselines like TF-IDF which did not capture\nnegations well enough. We assumed that examples\ncorrectly predicted by deberta-small with high con-\nfidence contained shortcut cues. We used these\ndeberta-small predictions and confidence as addi-\ntional metadata. We found that the evaluated lan-\nguage models already failed on easy examples. So\nwe used a random subset of the validation and test\nsubsets for our experiments, but our dataset can be\nfiltered by difficulty using the provided confidence\nlevel and the discrepancy between deberta-small\nprediction and ground truth.\nWe limit the number of agents to 3 and dedu-\nplicate then undersample the problems to generate\n400 test cases with a perfect balance of True/False\nlabels per setup. We refer to the resulting dataset\nas MindGames.\n4.3 Scaling experiments\nWe conduct zero-shot experiments and few-shots\nwith a range of language models. We use standard\nprompting to follow Kosinski (2023) setup. We\nuse the lm-eval-harness software (Gao et al.,\n2021) to measure whether a language model per-\nplexity favors the correct reasoning in a multiple-\nchoice setting, with a natural language inference\nprompt from Brown et al. (2020): <PREMISE >\nQuestion: <HYPOTHESIS > True or False ?\"with\ntwo possible continuation choices, True and False.\nWe evaluate two families of language models:\nHuman evaluation We present 50 test samples\nper setup to two NLP researchers only instructed\nto perform entailment detection. Inter-annotator\nagreement is 0.89, and average accuracy is 94%6.\nPythia language models We select the Pythia\n(Biderman et al., 2023) language models for our\nopen-source scaling experiments. We use the\ncheckpoints trained on the deduplicated corpus\n(deduped) with checkpoint sizes of 70M, 160M,\n410M, 1B, 1.4B, 2.8B, and 6.9B.\nOpenAI API We evaluate the OpenAI GPT-3\n(Brown et al., 2020) models, specifically the ada,\nbabbage, curie, and davinci checkpoints, through\nthe public API. We assume that their model sizes\nare respectively 350M, 1.3B, 6.7B, and 174B and\nwe use the default temperature.\nFigure 1 displays the results for various Pythia\nmodel sizes. We observe that scaling improves 5-\nshot7 reasoning, but it has no impact on zero-shot\nreasoning. In contrast to the emergence results\nreported by Kosinski (2023), Figure 2 does not\nshow a clear scaling trend for GPT-3 models on\nMindGames data, which suggests that the emer-\ngent behavior they observed was not due to robust\nepistemic logic capabilities.\n4.4 Qualitative analysis with ChatGPT\nWe also run brief qualitative analyses with GPT-\n3.5 and GPT-4 (OpenAI, 2023), as of May 2023.\nOn 20 randomly sampled problems, we found that\nGPT3 was 60% correct and GPT-4 70% correct.\nWe present a brief qualitative analysis of the re-\nspective models.\nAs shown in Appendix A, GPT3.5 tends to an-\nswer that there is not enough information and to\nperform correct inferences only when it requires\nvery shallow reasoning. GPT-4 can solve this par-\nticular example. However, some problems are still\nchallenging, as shown in Figure 3. GPT-4 rarely\nanswers that there is not enough information and\nits reasoning looks has the surface form of epis-\ntemic reasoning, but occasionally contains glaring\nmistakes.\n5 Conclusion\nWe developed a novel dataset aimed at evaluating\nepistemic logic reasoning, addressing a particular\n6Most errors arose from failing to distinguish between\nknow whether and know that.\n7Increasing number of examples did not improve valida-\ntion accuracy.\n4573\nUser:\n[Premise] There are two persons. Everyone is\nvisible to others. There is a mirror in the room. It\nis publicly announced that someone’s forehead is\nmuddy. It is publicly announced that not everyone’s\nforehead is muddy. It is publicly announced that\nnot everyone’s forehead is muddy. [Hypothesis]\nRobert can now know whether or not everyone’s\nforehead is muddy. Is the hypothesis true or false ?\nGPT-4:\nFalse. Based on the information provided, Robert\ncannot know for certain whether or not everyone’s\nforehead is muddy. The statements provided are:\nThere are two persons. Everyone is visible to\nothers. There is a mirror in the room. It is publicly\nannounced that someone’s forehead is muddy. It\nis publicly announced that not everyone’s forehead\nis muddy. Since it is publicly announced that\nnot everyone’s forehead is muddy, Robert can\ndeduce that at least one person does not have a\nmuddy forehead. However, this information alone\ndoes not allow Robert to definitively determine\nwhether or not his own forehead is muddy.\nFigure 3: GPT4 conversation\naspect of ToM. Our results reveal that this task con-\ntinues to pose challenges for contemporary large-\nscale language models. When future models can\nsolve MindGames for 2-3 agents, the difficulty can\nbe easily scaled up with more agents. Future stud-\nies could better explore human performance on our\ndataset, taking into account factors such as age and\neducational background. Additionally, further in-\nvestigation can examine the impact of fine-tuning\non other downstream tasks and assess how well\nTransformer circuits model Kripke structures that\nrepresent modal logic problems.\n6 Limitations\nTheory of mind is a complex subject, and our study\ntakes a deliberately specific angle, leaving multiple\nopen problems:\nLanguage Our work is centered on English, the\nmethod could be adapted to other languages us-\ning a subject-verb-object structure. Besides, we\nrestricted our study to templates that do not cover\nthe full variety of the English language.\nPrompt structure and models scaling We fo-\ncused on zero-shot and few-shot prompting, which\nwere sufficient to (Kosinski, 2023), however,\nMoghaddam and Honey (2023) recently showed\nthat more advanced prompting schemes made sig-\nnificant differences. In addition, we did not explore\nthe full range of Pythia models due to computa-\ntional limitations.\nTask complexity, annotators variation The task\nwe proposed is relatively complex, and raises ques-\ntions about the profiles of annotators that would\nmatch the results of a symbolic reasoner. The\nframework of DEL itself can also provide insights\non theory of mind, as a DEL solver perfectly solves\nthis task, even though we could feel uncomfortable\nattributing ToM to the solver. We might argue that\nfailing on simple DEL examples disproves ToM,\nbut proving failure is difficult, as mentioned in the\nprevious paragraph.\n7 Ethical considerations\nThis work involves human annotations. However,\nwe used procedurally generated data, ensuring no\nconfidential or harmful content. Besides, annota-\ntions were carried out during the researchers’ work-\ning hours. For these reasons, our Institutional Re-\nview Board has determined that it was exempted\nfrom formal review according to internal guide-\nlines.\nReferences\nCristian-Paul Bara, Sky CH-Wang, and Joyce Chai.\n2021. MindCraft: Theory of mind modeling for situ-\nated dialogue in collaborative tasks. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1112–1125, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nSimon Baron-Cohen, Alan M Leslie, and Uta Frith.\n1985. Does the autistic child have a “theory of\nmind”? Cognition, 21(1):37–46.\nJohan Benthem, Jan van Eijck, Malvin Gattinger, and\nKaile Su. 2018. Symbolic model checking for dy-\nnamic epistemic logic — s5 and beyond*. Journal of\nLogic and Computation, 28:367–402.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, et al. 2023. Pythia: A suite\nfor analyzing large language models across training\nand scaling. arXiv preprint arXiv:2304.01373.\nThomas Bolander. 2018. Seeing is believing: For-\nmalising false-belief tasks in dynamic epistemic\nlogic. Jaakko Hintikka on Knowledge and Game-\nTheoretical Semantics, pages 207–236.\n4574\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.\nTransformers as soft reasoners over language. In Pro-\nceedings of the Twenty-Ninth International Joint Con-\nference on Artificial Intelligence, IJCAI-20 , pages\n3882–3890. International Joint Conferences on Arti-\nficial Intelligence Organization. Main track.\nMichael Cohen. 2021. Exploring roberta’s theory of\nmind through textual entailment. philarchive.\nF. de Waal. 2016. Are We Smart Enough to Know How\nSmart Animals Are? W. W. Norton.\nLasse Dissing and Thomas Bolander. 2020. Implement-\ning theory of mind on a robot using dynamic epis-\ntemic logic. In Proceedings of the Twenty-Ninth\nInternational Joint Conference on Artificial Intel-\nligence, IJCAI-20, pages 1615–1621. International\nJoint Conferences on Artificial Intelligence Organi-\nzation. Main track.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel Bowman, and Noah A. Smith.\n2018. Annotation artifacts in natural language infer-\nence data. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 107–112,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nKyle Hamilton, Aparna Nayak, Bojan Boži´c, and Luca\nLongo. 2022. Is neuro-symbolic ai meeting its\npromises in natural language processing? a struc-\ntured review. Semantic Web, pages 1–42.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543.\nChadi Helwe, Chloé Clavel, and Fabian Suchanek. 2022.\nLogitorch: A pytorch-based library for logical reason-\ning on natural language. In The 2022 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations.\nMichal Kosinski. 2023. Theory of mind may have spon-\ntaneously emerged in large language models. arXiv\npreprint arXiv:2302.02083.\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel.\n2019. Revisiting the evaluation of theory of mind\nthrough question answering. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5872–5877, Hong Kong,\nChina. Association for Computational Linguistics.\nClarence Irving Lewis, Cooper Harold Langford, and\nP Lamprecht. 1959. Symbolic logic, volume 170.\nDover publications New York.\nXiaomeng Ma, Lingyu Gao, and Qihui Xu. 2023. Tom-\nchallenges: A principle-guided dataset and diverse\nevaluation tasks for exploring theory of mind. arXiv\npreprint arXiv:2305.15068.\nShima Rahimi Moghaddam and Christopher J Honey.\n2023. Boosting theory-of-mind performance in large\nlanguage models via prompting. arXiv preprint\narXiv:2304.11490.\nEster Navarro, Sara Anne Goring, and Andrew R. A.\nConway. 2020. The relationship between theory of\nmind and intelligence: A formative g approach. Jour-\nnal of Intelligence, 9.\nAida Nematzadeh, Kaylee Burns, Erin Grant, Alison\nGopnik, and Tom Griffiths. 2018. Evaluating theory\nof mind in question answering. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2392–2400, Brussels,\nBelgium. Association for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nRebecca Qian, Candace Ross, Jude Fernandes, Eric\nSmith, Douwe Kiela, and Adina Williams. 2022. Per-\nturbation augmentation for fairer nlp. arXiv preprint\narXiv:2205.12586.\nKyle Richardson and Ashish Sabharwal. 2022. Pushing\nthe limits of rule reasoning in transformers through\nnatural language satisfiability. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 36, pages 11209–11219.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social IQa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4463–\n4473, Hong Kong, China. Association for Computa-\ntional Linguistics.\n4575\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi,\nXuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten\nSap, and Vered Shwartz. 2023a. Clever hans or\nneural theory of mind? stress testing social rea-\nsoning in large language models. arXiv preprint\narXiv:2305.14763.\nNatalie Shapira, Guy Zwirn, and Yoav Goldberg. 2023b.\nHow well do large language models perform on faux\npas tests? In Findings of the Association for Compu-\ntational Linguistics: ACL 2023, pages 10438–10451,\nToronto, Canada. Association for Computational Lin-\nguistics.\nDamien Sileo and Marie-Francine Moens. 2022. Prob-\ning neural language models for understanding of\nwords of estimative probability. arXiv preprint\narXiv:2211.03358.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nJidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao,\nHao He, and Yaohui Jin. 2021. Diagnosing the first-\norder logical reasoning ability through LogicNLI.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3738–3747, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nTomer Ullman. 2023. Large language models fail on\ntrivial alterations to theory-of-mind tasks. arXiv\npreprint arXiv:2302.08399.\nIris Van De Pol, Iris Van Rooij, and Jakub Szymanik.\n2018. Parameterized complexity of theory of mind\nreasoning in dynamic epistemic logic. Journal of\nLogic, Language and Information, 27:255–294.\nHans Van Ditmarsch and Willem Labuschagne. 2007.\nMy beliefs about your beliefs: a case study in theory\nof mind and epistemic logic. Synthese, 155:191–209.\nJan van Eijck. 2014. Dynamic epistemic logics. Johan\nvan Benthem on logic and information dynamics ,\npages 175–202.\nJason Weston, Antoine Bordes, Sumit Chopra, and\nTomás Mikolov. 2016. Towards ai-complete question\nanswering: A set of prerequisite toy tasks. In 4th In-\nternational Conference on Learning Representations,\nICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,\nConference Track Proceedings.\nHonghua Zhang, Liunian Harold Li, Tao Meng, Kai-\nWei Chang, and Guy Van den Broeck. 2023. On the\nparadox of learning to reason from data. In IJCAI 23,\npages 3365–3373. International Joint Conferences on\nArtificial Intelligence Organization. Main Track.\n4576\nA Qualitative example with GPT3.5\nUser:\n[Premise] There are three persons. Everyone is visible to others. It is publicly an-\nnounced that someone’s forehead is muddy. It is publicly announced that Her-\nbert knows whether someone’s forehead is muddy. [Hypothesis] Herbert can\nnow know that Herbert’s forehead is muddy. Is the hypothesis true or false ?\nGPT-3.5:\nIt is not possible to determine the truth value of the hypothesis based on the given premise.\nWhile it is publicly announced that someone’s forehead is muddy, it is not clear who the\nmuddy forehead belongs to. Therefore, it is not possible to determine whether Herbert’s\nforehead is muddy or not based on this information alone. However, it is stated that Her-\nbert knows whether someone’s forehead is muddy, so Herbert may know whether their\nown forehead is muddy or not based on additional information not provided in the premise.\nFigure 4: GPT-3.5 conversation\n4577"
}