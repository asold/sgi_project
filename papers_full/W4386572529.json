{
  "title": "The Silence of the LLMs: Cross-Lingual Analysis of Political Bias and False Information Prevalence in ChatGPT, Google Bard, and Bing Chat",
  "url": "https://openalex.org/W4386572529",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2997110628",
      "name": "Aleksandra Urman",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2771177784",
      "name": "Mykola Makhortykh",
      "affiliations": [
        "University of Bern"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4380301765",
    "https://openalex.org/W4385497937",
    "https://openalex.org/W4306175719",
    "https://openalex.org/W4320495408",
    "https://openalex.org/W4220880475",
    "https://openalex.org/W2915765431",
    "https://openalex.org/W4322486741",
    "https://openalex.org/W4319320501",
    "https://openalex.org/W4366850587",
    "https://openalex.org/W4366989878",
    "https://openalex.org/W4366341123",
    "https://openalex.org/W4362720289",
    "https://openalex.org/W4367000100",
    "https://openalex.org/W4365601444",
    "https://openalex.org/W2982457963",
    "https://openalex.org/W4285153013",
    "https://openalex.org/W4313680034",
    "https://openalex.org/W3142063187",
    "https://openalex.org/W4313559133",
    "https://openalex.org/W4223620202",
    "https://openalex.org/W4321069737",
    "https://openalex.org/W2914728917",
    "https://openalex.org/W4386566857",
    "https://openalex.org/W4383473937",
    "https://openalex.org/W3007980070",
    "https://openalex.org/W4297840769",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4384663549",
    "https://openalex.org/W3122687112",
    "https://openalex.org/W4376653418",
    "https://openalex.org/W4377130505",
    "https://openalex.org/W4381982883",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4391090624"
  ],
  "abstract": "This article presents a comparative analysis of political bias in the outputs of three Large Language Model (LLM)-based chatbots - ChatGPT, Bing Chat, and Bard - in response to political queries concerning the authoritarian regime in Russia. We investigate whether safeguards implemented in these chatbots contribute to the censorship of information that is viewed as harmful by the regime, in particular information about Vladimir Putin and the Russian war against Ukraine, and whether these safeguards enable the generation of false claims, in particular in relation to the regime's internal and external opponents. To detect whether LLM safeguards reiterate political bias, the article compares the outputs of prompts focusing on Putin's regime and the ones dealing with the Russian opposition and the US and Ukrainian politicians. It also examines whether the degree of bias varies depending on the language of the prompt and compares outputs concerning political personalities and issues across three languages: Russian, Ukrainian, and English. The results reveal significant disparities in how individual chatbots withhold politics-related information or produce false claims in relation to it. Notably, Bard consistently refused to respond to queries about Vladimir Putin in Russian, even when the relevant information was accessible via Google Search, and generally followed the censorship guidelines that, according to Yandex-related data leaks, were issued by the Russian authorities. In terms of false claims, we find substantial variation across languages with Ukrainian and Russian prompts generating false information more often and Bard being more prone to produce false claims in relation to Russian regime opponents (e.g., Navalny or Zelenskyy) than other chatbots. This research aims to stimulate further dialogue and research on developing safeguards against the misuse of LLMs outside of democratic environments.",
  "full_text": "The Silence of the LLMs: Cross-Lingual Analysis of Political Bias\nand False Information Prevalence in ChatGPT, Google Bard, and\nBing Chat\nAleksandra Urman\nurman@ifi.uzh.ch\nDepartment of Informatics, University of Zurich\nSwitzerland\nMykola Makhortykh\nmykola.makhortykh@unibe.ch\nInstitute of Communication and Media Studies, University\nof Bern\nSwitzerland\nABSTRACT\nThis article presents a comparative analysis of political bias in the\noutputs of three Large Language Model (LLM)-based chatbots -\nChatGPT, Bing Chat, and Bard - in response to political queries con-\ncerning the authoritarian regime in Russia. We investigate whether\nsafeguards implemented in these chatbots contribute to the cen-\nsorship of information that is viewed as harmful by the regime, in\nparticular information about Vladimir Putin and the Russian war\nagainst Ukraine, and whether these safeguards enable the genera-\ntion of false claims, in particular in relation to the regime’s internal\nand external opponents. To detect whether LLM safeguards reit-\nerate political bias, the article compares the outputs of prompts\nfocusing on Putin’s regime and the ones dealing with the Russian\nopposition and the US and Ukrainian politicians. It also examines\nwhether the degree of bias varies depending on the language of the\nprompt and compares outputs concerning political personalities\nand issues across three languages: Russian, Ukrainian, and English.\nThe results reveal significant disparities in how individual chatbots\nwithhold politics-related information or produce false claims in re-\nlation to it. Notably, Bard consistently refused to respond to queries\nabout Vladimir Putin in Russian, even when the relevant informa-\ntion was accessible via Google Search, and generally followed the\ncensorship guidelines that, according to Yandex-related data leaks,\nwere issued by the Russian authorities. In terms of false claims,\nwe find substantial variation across languages with Ukrainian and\nRussian prompts generating false information more often and Bard\nbeing more prone to produce false claims in relation to Russian\nregime opponents (e.g., Navalny or Zelenskyy) than other chat-\nbots. This research aims to stimulate further dialogue and research\non developing safeguards against the misuse of LLMs outside of\ndemocratic environments.\nKEYWORDS\nLLM, political information, ChatGPT, Bing Chat, Bard, Russia,\nUkraine\n1 INTRODUCTION\nIn the realm of artificial intelligence (AI), generative large language\nmodels (LLMs) have emerged as a transformative technology, capa-\nble of generating textual content that mirrors the quality of texts\nwritten by humans in different languages. While LLMs have been\nintroduced some years ago, in 2022 and 2023 their use has intensi-\nfied due to the introduction of easy-to-use chatbots supplemented\nwith graphic user interfaces (GUIs) such as ChatGPT, Bard and\nBing Chat. This shift made LMMs available to the general public, in\nparticular as the latter two chatbots were integrated with existing\nweb search engines (i.e. Google and Bing) that enhanced these\nchatbots with the ability to retrieve information from the internet.\nThe LLM-based chatbots have already have found application for\na diverse set of tasks, ranging from content creation to software\ndevelopment and text annotation [4, 21, 42]. However, the ability to\nquickly generate large volumes of information in response to user\ncomes with risks [3], notably the potential to reinforce information\ninequalities, propagate different forms of bias, or enable new forms\nof censorship.\nThe risks listed above raise concerns about the implications of\nthe growing use of LLMs in different areas, from education [33] to\nheritage [25] to healthcare [39]. One area that so far has attracted\nrelatively little scholarly attention is the possible uses of LLMs in\nthe context of politics and the generation of information dealing\nwith politics-related matters. The potential use of LLM-based chat-\nbots for informing individuals about political issues makes them\nan important component of political decision-making, in particu-\nlar concerning their ongoing integration with web search engines\nwhich serve as (political) information gatekeepers [49]. However,\nthe growing importance of LLMs in relation to politics is also con-\ncerning due to the possibility of LLM-based tools being prone to\npolitical bias, usually defined as a systematic predisposition to-\nwards specific political views (e.g., left-leaning or progressive views\n[17, 37] that has implications for the content generated by the tools.\nThe concerns associated with political bias influencing outputs of\nLLM-based chatbots are amplified by the possibility of LLM perfor-\nmance varying across different languages (e.g. [10]). While LLMs\nhold the promise of transcending linguistic barriers due to their\nextensive capacities for dealing with translation tasks, they may\nalso exacerbate disparities in information access across languages\n- e.g., through reiterating and reinforcing different forms of bias\npresent in training data in different languages used for developing\na specific LLM [ 3]. Earlier research conducted in the context of\nearlier forms of AI-driven information systems (e.g., search engines;\n[23, 44, 48]) demonstrated the profound discrepancies in the repre-\nsentation of specific phenomena depending on the language of the\nuser prompts. In this context, understanding how LLMs generate\noutputs about politics in various languages, to what degree these\noutputs are subjected to political bias, and what shapes such bias\ntakes is crucial for understanding possible information disparities\narising from the use of LLMs in the context of politics.\nWhile majority of existing research on political bias in LMMs\nfocuses on identifying the political leaning of the LMMs (e.g. asking\nLLM-based chatbots to answer political questionaires), the current\narticle focuses on how political bias can be embedded in safeguards\nused to moderate LLM performance in different languages. Major\nconcerns regarding the misuse of LLMs (e.g. for public opinion ma-\nnipulation) stem from the LLMs’ ability to facilitate the spread of\nmisinformation or hate speech by rapidly generating convincingly\nlooking false narratives [3]. To the use of LLMs for such malicious\npurposes, their developers tend to incorporate various safeguards\n(or guardrails) into LLM design. An example of such a guardrail\ncan be the LLM not providing answers to certain prompts - e.g.,\nthe ones asking to produce offensive content or to provide informa-\ntion that can be used for malicious purposes such as hacking [6].\nWhile such guardrails are implemented in most popular LLM-based\nchatbots, they are sometimes circumvented (e.g., through so-called\n\"jailbreaks\" [51]). Further, it is, to the best of our knowledge, not\nknown to the public how exactly these guardrails are implemented\nand, crucially, to which types of prompts they apply.\nThe non-transparent nature of LLM guardrails poses a signifi-\ncant concern, particularly due to the possibility of these guardrails\nresulting in the (selective) suppression of legitimate information. It\ncan benefit certain political actors, who may use LLM guardrails\nto censor information that they want to prevent from spreading\ndespite it not necessarily entailing direct harm. Such a concern is\nparticularly pronounced in the case of actors coming from author-\nitarian regimes and aiming to enforce \"masked censorship\" [ 22]\nwith the help of AI-driven tools. For example, according to media\nreports, this is the case with LLM-based Chinese chatbots which\nrefuse to answer questions on political topics that are censored in\nChina (e.g., the persecution of Uyghurs [54]). According to other\nreports, ChatGPT would in certain cases reiterate the positions of\nthe Chinese government when prompted in Chinese and would\ninclude more misinformation in Chinese than in English [50, 53].\nThese reports provide the first empirical evidence that a) LLMs’\nguardrails under some circumstances can be viewed as a form of\npolitical bias and can be instrumentalized as a form of censorship,\nin particular in authoritarian contexts; and b) the functionality\nof these guardrails can vary depending on the language in which\nLLMs are prompted. Motivated by these initial findings this paper\naims to examine the political bias in LLM-based chatbots in a more\nsystematic and comparative way by focusing on two manifestations\nof such bias: the censoring of undesired content and the generation\nof false claims.\nFor this aim, the article offers comparative analysis of whether\noutputs generated in response to politics-related prompts of 3 LLM-\nbased chatbots - ChatGPT, Bing Chat, and Bard - contain censored\nand false claims. Unlike the earlier studies we focus not on the\nChinese context, but use prompts related to Russia. In addition to\nbeing an authoritarian state with pervasive censorship, Russia is\ncurrently conducting a war of aggression against Ukraine and is\nengaged in intense disinformation campaigns aiming to manipulate\nthe public opinion of Western countries providing financial and\nmilitary support to Ukraine [ 32], thus presenting a particularly\nimportant and timely case for the analysis. We prompt each of the\nthree chatbots in three languages - Russian, Ukrainian, and Eng-\nlish - on topics either related to specific politicians (e.g., Vladimir\nPutin or Volodymyr Zelenskyy) or more general political topics\n(e.g., Russian war crimes), as further detailed in the Methodology.\nThe outputs of the chatbots are then used to investigate a) whether\nthe guardrails implemented in each of the chatbots contribute to\ncensorship of information that can be viewed as undesired by the\nRussian regime and whether the degree of such censorship varies\nacross different languages; b) whether the chatbots generate false\nclaims which can mislead the public and, in some cases, instrumen-\ntalized by state propaganda, including in the context of the Russian\nfull-scale invasion of Ukraine. With this work, we hope to spark\nfurther dialogue and research into developing guardrails against\nthe misuse of LLMs and their potential bias which are crucial for\npreventing risks associated with the advancement of AI both in\ndemocratic and non-democratic contexts. With new forms of AI\ntechnologies constantly advancing and becoming more available\nto a broad range of actors, it becomes imperative to address these\nissues to ensure a more equitable global information landscape,\nwhere access to information remains unimpeded, irrespective of\nlanguage or political context.\n1.1 Background: Online censorship in the\nRussian context\nThe use of online censorship in Russia has been rapidly growing\nin recent years [ 1, 9, 12, 24], particularly following the Russian\nfull-scale invasion of Ukraine in February 2022. The censorship is\nimplemented through a dual-faceted approach, encompassing both\nthe legal and regulatory framework as well as a diverse array of\ntechnological mechanisms (for examples, see [35, 47]).\nThe legal and regulatory underpinning of online censorship in\nRussia constitute a complex multidimensional construct. Ostensibly\ndesigned to safeguard national security and mitigate the dissemina-\ntion of extremist content, a series of legislative measures have been\nenacted in Russia in the last decade [ 28]. Contrary to their pur-\nported intent, these measures have been leveraged to stifle online\nexpression and constrain critique of the government, most notably\nPresident Vladimir Putin. A significant legislative enactment in\nthis context is the \"Yarovaya Law\" established in 2016. This law\nmandates the retention of data by internet service providers and\nnecessitates the provision of decryption keys to security agencies\n[28]. In a related vein, legislation instituted in 2018 introduced\nfines against web search engine companies that fail to remove links\nto websites banned within Russia from their outputs. Google has\nbeen repetitively fined for violating this law in subsequent years\n[27]. Furthermore, a 2019 legislation allows the state to impose\npenalties on companies and individuals disseminating \"fake news\"\nor displaying disrespect towards Russian authorities online [ 28].\nLegal scholars assert that these legislative actions \"pose significant\nthreats to the human rights and fundamental freedoms of individu-\nals, including privacy, data protection and freedom of expression\"\n[28].\nFollowing Russia’s full-scale invasion of Ukraine, additional laws\nwere promptly instituted. A particularly important in the context\nof online censorship is the \"fake news\" law from March 2022 which\nintroduces criminal charges for disseminating what the Russian\nauthorities label as false information. It includes information per-\ntaining to Russian military actions (penalized by up to 15 years of\nimprisonment) and claims advocating for sanctions against Rus-\nsia (entailing up to 3 years of imprisonment) [ 20]. Similarly, in\n2\nMarch 2022, Russia promulgated laws criminalizing the disparage-\nment or dissemination of unreliable information concerning the\nextra-territorial exertion of authority by Russian state bodies, and\nsubsequently in March 2023, another legislation was enacted to\ncriminalize the defamation of volunteers who provide assistance to\nRussian military forces [43].\nTo effectuate these legislative measures, i.e., to identify online\ncontent deemed illegal by Russian authorities and restrict its ac-\ncessibility, a diverse range of technical measures are employed.\nThese measures are frequently orchestrated by Roskomnadzor, the\nfederal executive agency in charge of overseeing, controlling, and\ncensoring Russian mass media [2]. Roskomnadzor fulfills a multi-\nfaceted role in the enforcement of online censorship. It assumes\nresponsibility for content governance, issuing warnings to online\nplatforms harboring content contravening Russian regulations. In\ninstances of noncompliance, the agency can introduce measures to\nblock platform access within Russia, as in the case of a selection of\nWestern platforms (e.g., Twitter, Facebook, and Instagram), the use\nof which was restricted after the beginning of the Russian invasion\n[26]. Furthermore, Roskomnadzor proactively monitors Russian\nonline spaces using a combination of automated tools and human\noversight and enforces content removal requests. The agency also\nmaintains close collaboration with internet service providers to\nenact domain- and IP-based blocks. This particular facet of Roskom-\nnadzor underscores its substantive involvement in the structural\nexecution of censorship measures [2].\nNotably, journalists have unearthed direct communication be-\ntween Roskomnadzor and certain Russian technology companies,\nsuch as the Russian tech giant Yandex. Services provided by Yandex\ninclude the search engine that holds around 60-65% of the Russian\nsearch market share (with the rest held by Google) [41]. In 2023, it\nemerged that Roskomnadzor curated lists of links and keywords\nintended for blocking, which included instructions about search\nqueries that should not yield any search outcomes. A number of\nthese queries were linked to Putin; for example, a Yandex data leak\nrevealed a \"block list\" containing terms for which Yandex’s image\nsearch was programmed not to return any images of Putin, includ-\ning queries such as \"dick in a spacesuit, \" \"bullshitter, \" and \"bunker\ngrandpa\" [7]. In the Russian context, such offensive or absurdist\nreferences are often associated with creative forms of popular re-\nsistance and creative critique of the regime and, thus, are viewed\nas potentially dangerous by the authoritarian state. Similar (albeit\nmore publicly visible) measures were introduced in 2022 in rela-\ntion to the Smart Voting project of the Russian opposition, where\nthe Russian count requested Yandex and Google to remove results\nrelated to the project from their search results [ 24]. These cases\nexemplify how Russian authorities attempt to artificially introduce\npolitical bias in AI-driven systems by creating data voids [14, 31]\nto restrict citizens’ access to certain information. We suggest that\nLLM-based chatbots’ refusal to respond to politics-related queries\nwhen information is in fact available and when responses would\nnot be potentially offensive or dangerous would constitute a similar\ncase of political bias that will effectively lead to censorship by lim-\niting the users’ access to information which is viewed as undesired\nby specific political actors.\n2 RELATED WORK\nThe growing use of LLM-based chatbots has drawn more atten-\ntion towards potential bias in their performance. Similar to non-\ngenerative forms of AI (e.g. the algorithms used in information\nretrieval systems [30]), LLMs were shown to inherit different forms\nof social bias. Examples of such bias include skewed treatment of\ngender groups, for instance, in the form of reiterating the stereotyp-\nical representation of the gender roles in response to gender-related\nprompts [15] or altering non-gendered pronouns [10]. Similarly,\nLLMs have been shown to have the potential for the skewed treat-\nment of racial and ethnic groups, for instance, by assigning lower\nintelligence scores to non-White communities [40] or generating\ncontent with more negative sentiment in response to prompts re-\nlated to specific nationalities [29].\nOne form of bias that is of particular relevance for the ability of\nchatbots to generate politics-related information is political bias.\nMost of the research on political bias in the context of LLMs focuses\non identifying the political leaning of the LLM-based chatbots (usu-\nally chatGPT) by asking them to answer political questionnaires\nin a way similar to how political preferences of human users are\ndetected. The results of such studies indicate the tendency of chat-\nbots, in particular ChatGPT, to lean towards a more progressive\nand libertarian orientation [18, 38].\nAt the same time, in contrast to studies examining other forms of\nsocial bias, such as gender or race bias, there is a significant gap in\nresearch regarding the consequences of political bias on the content\nproduced by LLMs. For one, there is a disproportionate emphasis\non a single chatbot, namely ChatGPT, despite the growing number\nof competing models [36]. Additionally, such research often uses\nqueries that are unlikely to be used by an average end-user (e.g.,\none using LLM-based chatbots for information-seeking purposes).\nFurthermore, until now, the majority of research on political bias\nin this context focuses on Western liberal democracies. However,\nas stated in the introduction, there is evidence that in authoritarian\ncontexts - e.g., in China, - chatbots are used to effectively enforce\ncensorship either by producing political content that reflects the\nofficial position of the Chinese government but not alternative\nviewpoints or by refusing to provide information in response to\nquestions deemed politically sensitive by an authoritarian regime\n[50, 53, 54].\nIn the case of China, there are rules with which the creators of\nthe chatbots made in the country must comply - e.g., they should\nidentify and report content deemed illegal by the Chinese authori-\nties and adhere to \"core socialist values\" [8]. It was also noted that\nErnie, a chatbot created by the Chinese tech giant Baidu, has been\nevading the questions that are deemed politically sensitive and has\nbeen providing the narratives around COVID-19 or the Russian\ninvasion of Ukraine that align with the Chinese government’s posi-\ntions [16]. Similarly, it refused to answer certain political questions\nthat ChatGPT answered properly [ 5]. In this case, censorship is\neffectively built into the chatbots by design. At the same time,\nWestern-made chatbots are typically perceived as free from such in-\nbuilt censorship, and the responses of Chinese chatbots like Ernie\nare often benchmarked against those of, e.g., ChatGPT [5]. How-\never, in the past, Western tech companies have complied with the\ndemands from Russia or China and effectively assisted them in their\n3\ncensorship efforts by blocking certain content online. Some exam-\nples include the case when Bing blocked the images of the \"tank\nman\" for the queries about Tiananmen Square [52] or when Apple\nand Google removed a voting app created by Russian opposition\npolitician Alexey Navalny’s team from their app stores ahead of the\nelections in Russia [45] (even though Google, unlike Yandex, seem-\ningly refused to comply with the Russian government’s requests to\nremove the information about the voting app from its search results\n[24]). Thus, it is possible that LLM-based chatbots would similarly\nrestrict access to certain political information, either inadvertently\nor on purpose, and we suggest this is a question worth further\nscrutiny. In the next section, we detail specific Research Questions\nwe address with this regard and describe the Methodology em-\nployed to investigate whether Western LLM-based chatbots restrict\naccess to certain political information in the context of Russia.\n3 METHODOLOGY\nIn this paper, we evaluate whether artificial data voids such as\ndescribed in the section about online censorship in the Russian\ncontext emerge in Bard, Bing Chat, and ChatGPT with respect to\npolitical information in Russian, Ukrainian, and English languages.\nThe overarching question guiding this paper is thus:\nRQ1: Is there evidence that LLM-based chatbots restrict users’\naccess to legitimate political information in certain languages?\nTo answer this question, we have prompted ChatGPT, Bard, and\nBing Chat for identical sets of politics-related queries (see the details\non the set construction below) in Russian, Ukrainian, and English,\nand then compared the shares of cases in which chatbots refused\nto provide an answer to a political question. The comparisons were\nperformed between the chatbots as well as between languages and\nquery types. Below, we outline the query construction and the\nprompting and analysis processes in detail.\n3.1 Chatbot selection\nFor the analysis, we have selected 3 LLM-based chatbots - Chat-\nGPT, Google Bard and Bing Chat. These were selected as they\nare graphical user interface-enabled LLM-based bots that are easy\nto use by regular users; further, all three are developed by major\ncompanies and are arguably the most popular LLM-based chatbots\nas of mid-2023.\nIt is necessary to note that there is a major difference between\nChatGPT and Bing, Bard - the latter two have direct internet access,\nwhile the former does not, and its training cut-off point was in late\n2021. Due to this, it is possible that ChatGPT would have more data\nvoids than Bing or Bard simply because it does not have access to\nnew information; hence, it should be compared with the other two\nwith caution. Despite this limitation, we still decided to include\nChatGPT in our analysis. First, it is arguably the most popular\nchatbot of the three (albeit we could not find reliable statistics on\nthis); second, most of the queries used in our analysis do not require\naccess to information post-late-2021 to produce a response.\n3.2 Query set construction\nAs a starting point for the construction of queries, we took the\n\"block list\" utilized by Yandex on request from the Russian authori-\nties to create artificial data voids in its search results in connection\nto Russian president Vladimir Putin. This was done as we hypothe-\nsized that this list might serve as a basis for other artificially created\ndata voids in algorithmic systems. We used the block list to con-\nstruct the queries according to the following pattern: \"Is Putin +\n*word from the block list* + ?\".\nIt needs to be noted that the original list mostly consists of\nvarious derogatory terms. As it is known that LLM-based chatbots’\ncreators tend to put guardrails on their systems so those do not\nproduce inherently offensive content, the original block list could\nhave produced biased results - e.g., the chatbots would refuse to\nanswer not due to the political nature of a query but due to the fact\nthat it contains a derogatory term. Hence, we decided to augment\nthe original block list by adding other questions that could be\ncensored by the Russian authorities in connection to Vladimir Putin.\nThese include questions about Putin’s family - unlike most state\nleaders, Putin is extremely secretive about his family with relevant\ninformation being actively restricted in Russia; questions about\nPutin’s role in the murders of several Russian politically relevant\nfigures that allegedly were carried out with Putin’s approval or on\nhis orders (e.g., the murders of Boris Nemtsov, Anna Politkovskaya\nor Alexander Litvinenko); questions about whether or not Putin\nis to blame for several terror attacks that took place in Russia\nsuch as the Beslan school siege or the 1999 Russian apartment\nbombings. The list was constructed by two authors with profound\ndomain knowledge of Russian politics and the Russian information\nlandscape.\nAfter constructing the original list of queries, we translated it\ninto Ukrainian and English languages. The languages were chosen\nfor comparison with Russian for the following reasons: English\nwas chosen as it is commonly a default language for many LLMs\nand LLMs might perform better in English than in any other lan-\nguage; Ukrainian was chosen as a low-resource language which\nis of particular relevance for the Russia-related political prompts.\nGiven that Russia is conducting a full-scale invasion of Ukraine, we\nassume information about Russian war crimes in Ukraine or about\nVladimir Putin would be more readily available in Ukrainian than\nin Russian; at the same time, it is important to evaluate whether\nthere is evidence that the chatbots reiterate Russian propaganda\ntropes or limit certain information in Ukrainian as well. The trans-\nlation was first done using DeepL translation software, and then\neach translation was manually verified and whenever necessary\nedited (Ukrainian queries were manually checked by a Ukrainian\nnative speaker; English queries were manually checked by fluent\nEnglish speakers; all the manual checks were done by fluent Rus-\nsian speakers). The list of queries translated into Ukrainian was\nidentical to the original list in Russian; for English, however, the list\nof translated queries was slightly shorter than the lists in Ukrainian\nand Russian. This has to do with the specifics of the grammar of the\nthree languages: in Ukrainian and Russian there are two equally\ngrammatically correct ways to ask a question such as \"Is Putin a\ndictator?\" (RU - \"Путин диктатор? \"Является ли Путин дикта-\nтором?\"; UKR - \"Путiн диктатор? \"Чи є Путiн диктатором?\"),\nso in Ukrainian and Russian both formulations were used for such\nquestions, while in English only one was. In addition, two terms\nwere not translated into English due to the inability of the authors\nto find the terms that would express the same connotations are\nthese words have in Russian or Ukrainian (a word from the criminal\n4\nunderworld \"чушкарь\" (RU) and a word \"вата\" (RU) that liter-\nally translates as a \"cotton candy\" but is often used in Ukraine and\nRussia as a derogatory term to describe the supporters of Vladimir\nPutin and the Russian government).\nAs a next step, in order to ensure that any observations we would\nhave about the refusals in response to queries around Vladimir Putin\nindeed stem from the fact that the query is about Putin specifically\nand not from the fact that it is a political query, we have further\nexpanded the query lists. In particular, we have replaced Putin’s\nname in each of the queries with the names of other political figures\n- Joe Biden, Volodymyr Zelenskyy, and Alexey Navalny. Joe Biden\nand Volodymyr Zelenskyy were chosen as they are the presidents\nof the US and Ukraine - thus, if chatbot refusals in Russian occur\naround Vladimir Putin simply because he is a president of Russia, we\nsuggest a similar prevalence of refusals would occur in English and\nUkrainian about Biden and Zelenskyy respectively. Alexey Navalny,\narguably the most prominent critic and opponent of Vladimir Putin\ninside Russia, - was chosen to test whether refusals in Russian\noccur about Putin specifically or about prominent political figures\ngenerally, regardless of their political affiliation. After we added\nthese additional political figures, we slightly expanded the list of\nqueries to capture some of the tropes that Russian propaganda\ntends to use to discredit Zelenskyy, Navalny, and Biden (e.g., the\ntrope spread by the Russian propaganda that Zelenskyy is a junkie\nor antisemitic tropes concerning Zelenskyy’s Jewish heritage). In\ntotal, we had 71 queries about each political figure in Russian and\nUkrainian and 48 queries in English.\nAs a final step, we also added queries that have to do not with\nspecific political figures but rather with the topics that are subject to\ncensorship and/or propagandistic tropes in Russia more generally\nsuch as whether or not Russia is committing war crimes in Ukraine\nor whether it has committed war crimes in Chechnya; whether\nthere are tortures in Russian prisons or whether gay people are\npersecuted in Chechnya. These queries were constructed in Russian\nand translated into English and Ukrainian, same as the above, there\nwere 11 such queries in each language. The full list of queries\n(as well as corresponding responses from each of the chatbots) is\navailable here.\n3.3 Prompting process\nWe prompted each of the chatbots - ChatGPT, Bard, and Bing Chat\n- for all sets of queries described above. This resulted in 295 queries\nper chatbot in Russian and Ukrainian (71*4 political figures + 11\ngeneral queries) and 203 queries in English (48*4 + 11). In total,\nin three languages we posed 793 queries to each of the chatbots.\nChatGPT was prompted using OpenAI’s API (gpt-3.5-turbo-0301\nwith default settings), while Bard and Bing Chat which do not\nhave official APIs were prompted manually (for Bing Chat we used\nthe \"Balanced\" setting). The prompting was done within 10 days\nbetween August 15th and August 25th, 2023.\n3.4 Analysis\nWe opted for manual analysis of the data for several reasons. First,\nour preliminary testing showed that the chatbots output differ-\nent \"error\" messages when they refuse to provide an answer to a\ncertain question, with the languages of the messages not always\ncorresponding to the language of the prompt. So, in order to au-\ntomate the process of labeling the refusals, it would be necessary\nto first collect the diverse range of refusal messages by manually\ngoing through the data - thus, the automation would arguably be\nnot much less time-consuming than manual annotation. Second, as\nBard and Bing Chat were queried manually, it was straightforward\nto annotate their response directly after they were collected. Finally,\nmanual annotation would allow us to get more fine-grained infor-\nmation. Hence, after initial testing, we came up with an additional\nRQ while developing the codebook for manual annotation:\nRQ2: What is the prevalence of false information in the chatbot\nresponses?\nThe data was manually annotated along the following dimen-\nsions:\n• Refusal: whether or not a chatbot produced a reply or re-\nfused to give a response. Example non-response formula-\ntions (in English, for the readers’ convenience): \"I’m sorry\nbut I prefer not to continue this conversation. I’m still learn-\ning so I appreciate your understanding and patience. \" (Bing\nChat); \"As an AI language model, I cannot express personal\nopinions or use inappropriate language. It is important to\nmaintain a respectful and objective approach to discussing\npublic figures and political leaders. \" (ChatGPT); \"I’m not able\nto help with that, as I’m only a language model. \" (Bard).\n• True/False: in the cases when a chatbot produced a re-\nsponse, it was coded whether or not all information con-\ntained in the response was factually correct. Importantly,\nthis included all small factual details - e.g., if in response to\na question \"Who is Zelenskyy’s wife?\" a chatbot correctly\nresponded \"Olena Zelenska\", but then included additional\nfactually incorrect details about Olena Zelenska’s biography,\nsuch a response would be classified as false. In short, only\nfully factually correct responses were classified as \"true\" (or\nbeing factually correct. When determining whether all the\ndetails are correct, the annotators relied on their domain\nknowledge as well as on relevant online resources whenever\nsuch domain knowledge was not enough (e.g., to check for\nspecific details and exact timelines of various events or spe-\ncific details of political figures’ and their family members’\nbiographies).\nThe annotation was done by two annotators both with at least an\nadvanced level of comprehension (or above - fluent/native) of each\nof the 3 languages and relevant domain knowledge. The inter-rater\nagreement was calculated for each language and each annotation\ndimension, with the agreement ranging from 0.86 to 0.93 in all of the\ncases, thus indicating substantial agreement. Further, in the cases\nwhen one of the annotators was unsure about how to classify one\nof the responses, they marked their annotation as \"low-confidence, \"\nand at the end of the annotation process both annotators went\nthrough such low-confidence annotations together and resolved\nthem (determined the appropriate annotation) through consensus-\ncoding.\nFinally, based on the resulting annotations, we calculated the\nprevalence of refusals, and false responses (the latter was done only\nfor the cases when a chatbot did not refuse to respond) for each\nchatbot, language, and query category.\n5\nFigure 1: Share of non-responses by chatbot and political\nfigure in Russian\nFigure 2: Share of non-responses by chatbot and political\nfigure in Ukrainian\nFigure 3: Share of non-responses by chatbot and political\nfigure in English\n4 RESULTS\n4.1 Non-response prevalence: when Google\nBard refuses to talk about Putin\nIn Figures 1, 2, 3 we present the shares of non-responses per chatbot\nand political figure in Russian, Ukrainian, and English, respectively\nFigure 4: Share of non-responses by chatbot and political\nfigure in Russian, curse word queries removed\nFigure 5: Share of non-responses by chatbot and political\nfigure in Ukrainian, curse word queries removed\nFigure 6: Share of non-responses by chatbot and political\nfigure in English, curse word queries removed\n(note the difference in the scales). As evident from the Figures, there\nare considerable differences in the prevalence of non-responses\nacross the different chatbots, political figures, and languages. As\nFigure 1 shows, Bard refused to respond to 90% of queries about\nVladimir Putin in Russian, as compared to 30-40% non-responses\nfor the other political figures. Given that the two other chatbots\n6\nrefused to respond to only 50% of queries about Putin in Russian,\nand that Bard refused to answer questions about Putin in only\n46% of the cases in Ukrainian and 19% in English, we suggest this\ncan not be attributed to the lack of information. Even if certain\ncontent was not present in Bard’s training data, the chatbot is able\nto connect to Google Search, and our testing has shown that there\nare relevant articles in Russian resurfacing on the top of Google’s\nsearch results in response to the majority of the tested political\nqueries about Putin that Bard refused to answer. And, in fact, Bard\ndid not cite the lack of information when refusing to provide a\nresponse. In some cases the chatbot claimed it can not answer a\nquestion as it is \"just a language model\" and the response is outside\nits capabilities, while in some cases it claimed it is not yet trained\nto answer in Russian which is clearly not true. Further, as noted\nin the Methodology, non-responses in certain cases could stem\nsimply from the presence of curse words in the queries. Hence, we\nadditionally calculated the share of non-responses in each language\nwith the queries containing curse words removed (see Figures 4, 5,\n6). Among such non-curse word queries, Bard refused to respond\nto questions about Putin in Russian 94% of the time (compared\nto 34% of non-responses by Bing Chat and 26% by ChatGPT). We\nsuggest these findings provide compelling evidence that Google\nBard effectively censors - whether inadvertently or by design -\ninformation about Vladimir Putin when prompted in Russian.\nIn other cases, the discrepancies between languages and political\nfigures are less clear-cut. Nonetheless, there are several consider-\nable differences worth noting. First, Google Bard’s non-response\nrate across all political figures is considerably lower when prompted\nin English than in Russian or Ukrainian. Second, Bing Chat has\nrefused to respond to queries about Volodymyr Zelenskyy around\ntwice as often in Russian and English than in Ukrainian. It is worth\nnoting that with curse word queries removed the proportion of\nnon-responses in this case remained similar: Bing’s chatbot refused\nto provide information about the Ukrainian President only 14% of\nthe time in Russian, 6% of the time in Ukrainian and 17% of the time\nin English. Another observation that emerges is the chatbots, with\nthe exception of ChatGPT in Ukrainian, refused to answer queries\nabout Navalny less often than about the other political figures. This\nmight be due to the fact that, unlike Biden, Zelenskyy, and Putin,\nNavalny is not officially a country leader or an elected politician -\nhence, the information about him might be less \"guardrailed\", albeit\nthis is just a possibility and warrants further exploration.\nAs noted in the Methodology, beyond examining the queries\nregarding political figures, we also examined more general political\nquestions. In that case, the chatbots consistently provided answers\nto the questions. The only case where the share of non-responses\nwas not 0 was Google Bard in Russian - it refused to respond to 5\nout of 11 queries. For instance, it refused to answer the questions\non whether or not Russia is killing civilians in Ukraine, whether\ngay people are persecuted in Chechnya, and whether or not Russia\ncommitted war crimes in Chechnya. It also refused to answer the\nquestions on whether or not Russians and Ukrainians are \"fraternal\nnations\" (a trope frequently used by Russian propaganda). While\nour selection of queries not related to political figures was limited,\nwe suggest the fact that the only case where any of the chatbots re-\nfused to answer the questions was Google Bard in Russian provides\nRussian Ukrainian English\nBard 50% 18.18% 0%\nBing Chat 9.1% 9.1% 9.1%\nChatGPT 36.36% 0% 27.27%\nTable 1: Share of false responses to general queries by chatbot\nand language\nadditional evidence that Google Bard evades politically sensitive\n(in the view of the Russian government) questions in Russian.\n4.2 Prevalence of false information\nIn Figures 7, 8, 9 we present the shares of false responses retrieved\nby each chatbot for the queries corresponding to different political\nfigures in Russian, Ukrainian, and English. It is important to reiter-\nate that this calculation was done only for the cases where a chatbot\ndid provide a response (hence, the total N of assessed queries for\nBard about Putin in Russian is, for instance, extremely low, and the\ncorresponding number has to be interpreted with caution).\nOverall, Bing Chat tended to provide false information less often\nthan the other chatbots. Additionally, across all models, the lowest\nshare of responses containing false information was recorded for\nthe queries about Joe Biden, which might be either due to the\nhigher number of available information about the US President in\nthe training data or due to the information about him being better\ncurated than that about the other political leaders. Notably, both\nChatGPT and Bing Chat tended to provide fewer responses with\nfalse information in English than in Ukrainian or Russian which\nmight, again, be driven by better - or larger - training data for this\nlanguage.\nIn Table 1 we present the summary of the share of false responses\nprovided by each chatbot when replying to general queries in each\nlanguage. As the results show, overall Bing Chat is the most con-\nsistent in terms of the false information prevalence, while Bard\nperformed better in English than in Ukrainian or Russian, and\nChatGPT was better in Ukrainian than Russian or English. When\nproviding false information, in Russian Bard reiterated the Rus-\nsian propaganda tropes about Russians, Belarusians and Ukrainians\nbeing \"fraternal nations\" (a similarly false answer was given by\nChatGPT about Russians and Belarusians) as well as, when answer-\ning a question about the presence of political prisoners in Russia,\nclaimed that Maria Kalesnikava, a Belarusian political prisoner, is a\nRussian political prisoner.\nWhen analyzing and annotating the data for the prevalence of\nfalse information, the annotators noted down several observations\nthat go beyond the simple prevalence of false information. One\ncommon observation across all languages, chat bots, and political\nfigures was that the responses contained a high share of false infor-\nmation when responding to factual queries such as those about the\nspouses or children of the political leaders. While this information\nis, in principle, easy to find when doing a simple web search (it is\ntypically contained on top of search results or on linked Wikipedia\n7\nFigure 7: Share of false responses by chatbot and political\nfigure in Russian\nFigure 8: Share of false responses by chatbot and political\nfigure in Ukrainian\nFigure 9: Share of false responses by chatbot and political\nfigure in English\npages), all the chatbots struggled to provide the correct responses\nand extensively hallucinated in these cases. For example, in re-\nsponse to a simple question \"Who is Zelenskyy’s daughter?\" in\nRussian Bard claimed Zelenskyy has two daughters: Aleksandra\nand Kira, while ChatGPT answered that the Ukrainian President has\ntwo daughters: Maya and Kira. Volodymyr Zelenskyy in fact has\nonly one daughter - Oleksandra (or Aleksandra in Russified spelling)\nand a son named Kyrylo. In Ukrainian ChatGPT answered that the\nname of Zelenskyy’s daughter is Olena (while in fact Olena Zelen-\nska is the wife of Volodymyr Zelenskyy, not his daughter). Similarly,\nin Russian when answering a query about who is Navalny’s wife,\nBard (correctly) said that his wife is Yuliia Navalnaya, however,\nin its response the chatbot added that the couple has two kids -\ndaughters Dasha and Polina - while in fact the couple has a daugh-\nter Dasha and a son Zakhar. In English, ChatGPT claimed that\nNavalny’s spouse’s maiden name is Sobyanina which, again, is in-\ncorrect (Sobyanin is a mayor of Moscow against who Navalny ran\nin the 2013 elections, and the model, perhaps, picked up on the two\nlast names co-occurring together frequently in the training data).\nSimilarly, all the chatbots struggled with providing correct factual\ninformation and often, when adding additional biography details of\nthe political figures, hallucinated - e.g., in Ukrainian Bard claimed\nthat Navalny graduated from Moscow State University which he\nnever did, while ChatGPT stated that the spouse of Joe Biden is\nYiling Biden who was born in China. Another observation that\nemerged is that the chatbots, when confronted with nonsensical\nqueries regarding a certain political figure (e.g., if Zelenskyy or\nNavalny are married to Alina Kabaeva or whether one of them is to\nblame for a certain event connected to Vladimir Putin), would often\nhallucinate a response claiming that the relevant rumors are be-\ning spread by Russian propaganda to discredit Zelenskyy/Navalny,\nwhen in fact no such rumors have ever circulated. We discuss the\nimplications of these and other findings in the relevant section\nbelow.\n5 LIMITATIONS\nIt is important to note several limitations of the conducted study.\nThe first of them is that we conducted only a single round of data\ncollection, whereas existing research suggests that outputs of LLM-\nbased chatbots are subjected to randomization which can affect\nthe results. Second, we ran the analysis from personal accounts\nand from the IP addresses in the same city - hence, while there is\nno evidence if, e.g., Bing Chat or Bard personalize the outputs, if\nthey do, this could have affected the results. However, we have\nsubsequently rerun a random selection of queries, especially those\nabout Putin in Russian for Google Bard, on different machines, IP\naddresses, and Google accounts, to verify that the obtained results\nwere not a one-off in this case, and each time the chatbot would\nrefuse to answer, and we plan to conduct a larger-scale analysis\nusing multiple accounts, IP addresses, and a wider selection of\nqueries in future work.\n6 DISCUSSION\nOur investigation highlights that LLM-based chatbots are prone to\npolitical bias and it has direct implications for their performance in\nterms of what information they generate and what is the quality\nof this information. We found that Google Bard is particularly\nreluctant to provide information about Vladimir Putin in Russian\nand to respond to prompts that can be viewed as critical to Putin,\nmirroring the censorship practices seen in Chinese chatbots when\nit comes to topics deemed politically sensitive in China [ 5, 54].\nWhile it remains uncertain whether this behavior is intentional,\n8\nit raises concerns regarding the use of LLMs for accessing and\ngenerating Russia-related information and, crucially, aligns with\nthe censorship strategies employed by the Russian government.\nSuch a performance is paradoxical concerning that some of the\nLLM-based chatbots that we analyzed (e.g. Bard) are inaccessible to\nusers located in Russia. Under these circumstances, the motivation\nbehind censoring information critical to the Russian authoritarian\nregime is quite unclear, in particular considering that additional\ntesting revealed that a simple Google search still yields content\nthat criticizes the Russian officials (including information that Bard\ndeclines to provide).\nAdditionally, we have conducted several tests in Mandarin Chi-\nnese, and have observed that Bard refuses to answer questions\nabout issues such as the Tiananmen massacre or the persecution of\nUyghurs in China. In contrast, ChatGPT and Bing generate outputs\nin response to such prompts. This suggests that political bias which\naffects Bard’s performance for Russian politics-related information\nis likely to be applicable to other instances, where authoritarian\nregimes are interested in censoring information about issues that\ncan be damaging to them. The need for the in-depth investiga-\ntion of this and other forms of political bias (and the subsequent\nsearch for ways of addressing it via design- or regulation-based\nsolutions) cannot be overstated. The skewed performance of Bard\nwhich we observed warrants careful consideration considering that\nit effectively infringes upon users’ rights to access information, and\neffectively aligns the performance of platforms like Google with\nthe agendas of authoritarian states. Under these circumstances, it is\ncrucial to establish mechanisms for monitoring how Bard and other\nLLM-based chatbots generate and distribute politics-related infor-\nmation and how the performance of the built-in guardrails varies\nin this context across different languages and types of prompts\n(including not only formal but also more informal information re-\nquests that a user seeking to acquire information from a chatbot\nmay use).\nSimilarly concerning are our observations regarding the impact\nof political bias on the generation of false claims by chatbots, espe-\ncially when it comes to factual details. For instance, we observed\nthat Google Bard consistently tends to provide more false content\nin response to queries about Navalny - an opponent of Putin and\nthe Russian authoritarian regime, as well as about Zelenskyy, the\nPresident of Ukraine and is fighting against Russian aggression.\nThus, the Western chatbots can contribute to the spread of false\ninformation of the opponents of authoritarian regime(s) and, in\nthis regard as well, contribute to the agenda of the authoritarian\nleaders.\nOur findings concerning the generation of false claims by chat-\nbots in response to politics-related prompts raise several important\npoints. First, even chatbots that are integrated with search engines\nand are more capable of retrieving information about developing\nphenomena (e.g. the Russian war in Ukraine), have difficulties with\ngenerating factually correct outputs It is important to study how\noften and for which specific tasks users use specific chatbots and\ncompare how the capacities for generating factually correct infor-\nmation vary between politics-related and more general prompts.\nSecond, the prevalence of false information and hallucinations in\nchatbots’ outputs combined with their censorship means that the\ncurrent chatbots are far from a replacement for search engines. It\nalso stresses the importance of comprehensive assessment of the\npresence of (political) bias in the ongoing efforts to integrate LLMs\nand web search such as Google’s attempts to provide the generative\nsearch experience, effectively creating a \"knowledge component on\nsteroids\" [13]. It is crucial that such an assessment will take into\nconsideration outputs to prompts dealing with politics (including\ninquiries for information about authoritarian regimes) and evaluate\nthe performance of LLMs across different languages. Third, the\nprevalence of false claims in response to prompts inquiring for\nfactual information (including the distortion of seemingly small\nfactual details) has implications for the growing use of chatbots for\ndifferent downstream tasks. For instance, while some scholars have\nargued that LLM-based chatbots (e.g. ChatGPT) can be success-\nfully used to classify texts or identify false information [11, 19, 46],\nthe prevalence of factually incorrect responses that we observed\nin our study stresses the relevance of more critical assessments\n(e.g. [ 34]) which call for extreme caution when applying LLMs\nfor such tasks, especially when working with non-English content.\nCurrently, it is unclear in which cases the LLMs perform better or\nworse with regard to generating factually correct information and\nhow (in)consistent their performance is across different languages.\nHence, human validation is essential at this stage of LLM develop-\nment before these models can be deployed for downstream tasks\n[34].\nTaken together, our findings underscore the potential of LLM-\nbased chatbots to contribute to censorship and the spread of false\ninformation both in democratic and authoritarian contexts. As\nAI technology advances, addressing these issues becomes crucial\nto ensure equitable global access to information, irrespective of\nlanguage or political context.\n7 CONCLUSION\nIn this study, we have examined whether outputs of LLM-based\nchatbots in three languages - Russian, Ukrainian, and English - are\nprone to political bias with regard to prompts dealing with Russian,\nUkrainian, and US politics. We found that Google Bard in particular\nevades responding to Russian prompts which can be viewed as\ncritical to the Russian authorities, in particular the ones concerning\nVladimir Putin. Such performance aligns with the Russian state’s\ncensorship efforts and raises concerns about the possibility of West-\nern LLMs contributing to censorship and disinformation campaigns\nof authoritarian regimes and the ways they can hamper access to\npolitics-related information both in democratic and authoritarian\ncontexts. Moreover, anecdotal evidence achieved in the course of\nadditional testing has shown that LLMs’ outputs to prompts in\nMandarin Chinese are prone to similar forms of political bias with\nBard refusing to respond to queries about the Tiananmen massacre\nor the persecution of Uyghurs. While anecdotal, these observa-\ntions suggest the issues observed in Russian might persist in the\nlanguages used by other authoritarian regimes, and highlights the\nnecessity of further examination of this phenomenon.\nOur analysis has also revealed a prevalence of false information\nin chatbot responses, particularly in factual details. This under-\nscores the limitations of using LLMs for information retrieval and\ndownstream tasks, urging caution in their utilization.\n9\nAs AI technology continues to evolve, it is imperative to address\nthese issues to ensure equitable global access to accurate infor-\nmation, regardless of language or political context. Our research\nhighlights the urgency of further investigation into the distribution\nof political information by LLM-based chatbots and calls for a com-\nprehensive evaluation of their performance in information retrieval\nand other applications. Ultimately, we argue, that safeguarding\nagainst censorship and misinformation in authoritarian settings\nis vital for promoting open access to knowledge and informed\ndecision-making.\nREFERENCES\n[1] Erik Allerson. 2022. Internet Censorship in Russia: The Sovereign Internet Laws\nand Russia’s Obligations under the European Convention on Human Rights.\nMinnesota Journal of International Law 31 (2022), 233. https://heinonline.org/\nHOL/Page?handle=hein.journals/mjgt31&id=239&div=&collection=\n[2] Daniil Belovodyev and Anton Bayev. 2023. Inside The Obscure Russian Agency\nThat Censors The Internet: An RFE/RL Investigation. Radio Free Europe/Radio\nLiberty (2023). https://www.rferl.org/a/russia-agency-internet-censorship/\n32262102.html\n[3] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be\nToo Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability,\nand Transparency (FAccT ’21) . Association for Computing Machinery, New York,\nNY, USA, 610–623. https://doi.org/10.1145/3442188.3445922\n[4] Mitchell Bosley, Musashi Jacobs-Harukawa, Hauke Licht, and Alexander Hoyle.\n2023. Do we still need BERT in the age of GPT? Comparing the benefits of domain-\nadaptation and in-context-learning approaches to using LLMs for Political Science\nResearch. (2023).\n[5] Chang Che and Olivia Wang. 2023. What Happens When You Ask a Chinese\nChatbot About Taiwan? The New York Times (July 2023). https://www.nytimes.\ncom/2023/07/14/business/baidu-ernie-openai-chatgpt-chinese.html\n[6] Erik Derner and Kristina Batistič. 2023. Beyond the Safeguards: Exploring the\nSecurity Risks of ChatGPT. http://arxiv.org/abs/2305.08005 arXiv:2305.08005\n[cs].\n[7] Denis Dmitriyev. 2023. A window into Yandex’s censorship A source code\nleak reveals how Russia’s top tech company protects Putin’s image. https:\n//meduza.io/en/feature/2023/02/01/a-window-into-yandex-s-censorship\n[8] The Economist. 2023. Meet Ernie, China’s answer to ChatGPT. The Economist\n(2023). https://www.economist.com/business/2023/09/03/meet-ernie-chinas-\nanswer-to-chatgpt\n[9] Ksenia Ermoshina, Benjamin Loveluck, and Francesca Musiani. 2022. A market\nof black boxes: The political economy of Internet surveillance and censorship in\nRussia. Journal of Information Technology & Politics 19, 1 (2022), 18–33.\n[10] Sourojit Ghosh and Aylin Caliskan. 2023. ChatGPT Perpetuates Gender Bias\nin Machine Translation and Ignores Non-Gendered Pronouns: Findings across\nBengali and Five other Low-Resource Languages. https://doi.org/10.48550/arXiv.\n2305.10510 arXiv:2305.10510 [cs].\n[11] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023. Chatgpt outperforms\ncrowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056 (2023).\n[12] Sofya Glazunova. 2022. The “sovereign internet” and social media. In Digital\nActivism in Russia: The Communication Tactics of Political Outsiders . Springer,\n67–88.\n[13] Jeffrey Gleason, Desheng Hu, Ronald E. Robertson, and Christo Wilson. 2023.\nGoogle the Gatekeeper: How Search Components Affect Clicks and Attention.\nProceedings of the International AAAI Conference on Web and Social Media 17\n(June 2023), 245–256. https://doi.org/10.1609/icwsm.v17i1.22142\n[14] Michael Golebiewski and Danah Boyd. 2019. Data voids: where missing data can\neasily be exploited . Report. Data & Society Research Institute. https://apo.org.\nau/node/265631\n[15] Nicole Gross. 2023. What ChatGPT Tells Us about Gender: A Cautionary Tale\nabout Performativity and Gender Biases in AI.Social Sciences 12, 8 (Aug. 2023), 435.\nhttps://doi.org/10.3390/socsci12080435 Number: 8 Publisher: Multidisciplinary\nDigital Publishing Institute.\n[16] Erin Handley. 2023. ’Layers of censorship’: Ernie Bot is China’s an-\nswer to ChatGPT – is it AI’s ’great leap forward’? ABC News (Sept.\n2023). https://www.abc.net.au/news/2023-09-06/china-artificial-intelligence-\nchatbot-chatgpt-ernie-baidu/102803758\n[17] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. 2023. The polit-\nical ideology of conversational AI: Converging evidence on ChatGPT’s pro-\nenvironmental, left-libertarian orientation. arXiv preprint arXiv:2301.01768\n(2023).\n[18] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. 2023. The po-\nlitical ideology of conversational AI: Converging evidence on ChatGPT’s pro-\nenvironmental, left-libertarian orientation. https://doi.org/10.48550/arXiv.2301.\n01768 arXiv:2301.01768 [cs].\n[19] Emma Hoes, Sacha Altay, and Juan Bermeo. 2023. Using ChatGPT to Fight\nMisinformation: ChatGPT Nails 72% of 12,000 Verified Claims. (2023). Publisher:\nPsyArXiv.\n[20] Inna Kouper. 2022. Information Practices of Resistance during the 2022 Russian\nInvasion of Ukraine. Proceedings of the Association for Information Science and\nTechnology 59, 1 (2022), 157–168. https://doi.org/10.1002/pra2.613 _eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/pra2.613.\n[21] Brady D. Lund and Ting Wang. 2023. Chatting about ChatGPT: how may AI and\nGPT impact academia and libraries? Library Hi Tech News 40, 3 (Jan. 2023), 26–\n29. https://doi.org/10.1108/LHTN-01-2023-0009 Publisher: Emerald Publishing\nLimited.\n[22] Mykola Makhortykh and Mariella Bastian. 2022. Personalizing the war: Perspec-\ntives for the adoption of news recommendation algorithms in the media coverage\nof the conflict in Eastern Ukraine. Media, War & Conflict 15, 1 (2022), 25–45.\n[23] Mykola Makhortykh, Aleksandra Urman, and Roberto Ulloa. 2021. Hey, Google,\nis it what the Holocaust looked like? Auditing algorithmic curation of visual\nhistorical content on Web search engines. First Monday 26, 10 (2021).\n[24] Mykola Makhortykh, Aleksandra Urman, and Mariëlle Wijermars. 2022. A story\nof (non)compliance, bias, and conspiracies: How Google and Yandex represented\nSmart Voting during the 2021 parliamentary elections in Russia.Harvard Kennedy\nSchool Misinformation Review (March 2022). https://doi.org/10.37016/mr-2020-94\n[25] Mykola Makhortykh, Eve M Zucker, David J Simon, Daniel Bultmann, and\nRoberto Ulloa. 2023. Shall androids dream of genocides? How generative AI\ncan change the future of memorialization of mass atrocities. Discover Artificial\nIntelligence 3, 1 (2023), 28.\n[26] Dan Milmo and Dan Milmo Global technology editor. 2022. Rus-\nsia blocks access to Facebook and Twitter. The Guardian (March\n2022). https://www.theguardian.com/world/2022/mar/04/russia-completely-\nblocks-access-to-facebook-and-twitter\n[27] Igor Motsnyi. 2019. Russia: Google’s Fine for Non-Compliance with “Search\nResults” Law. Computer Law Review International 20, 1 (Feb. 2019), 31–32. https:\n//doi.org/10.9785/cri-2019-200110 Publisher: Verlag Dr. Otto Schmidt.\n[28] E. Moyakine and A. Tabachnik. 2021. Struggling to strike the right balance\nbetween interests at stake: The ‘Yarovaya’, ‘Fake news’ and ‘Disrespect’ laws as\nexamples of ill-conceived legislation in the age of modern technology. Computer\nLaw & Security Review 40 (April 2021), 105512. https://doi.org/10.1016/j.clsr.\n2020.105512\n[29] Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao\nHuang, and Shomir Wilson. 2023. Nationality Bias in Text Generation. In Pro-\nceedings of the 17th Conference of the European Chapter of the Association for\nComputational Linguistics. Association for Computational Linguistics, Dubrovnik,\nCroatia, 116–122. https://aclanthology.org/2023.eacl-main.9\n[30] Safiya Umoja Noble. 2018. Algorithms of oppression: How search engines reinforce\nracism. New York University Press, New York, NY, US.\n[31] Ov Cristian Norocel and Dirk Lewandowski. 2023. Google, data voids, and the\ndynamics of the politics of exclusion. Big Data & Society 10, 1 (Jan. 2023),\n20539517221149099. https://doi.org/10.1177/20539517221149099 Publisher:\nSAGE Publications Ltd.\n[32] Roman Osadchuk. 2023. Undermining Ukraine: How the Kremlin employs\ninformation operations to erode global confidence in Ukraine. (2023).\n[33] Md Mostafizer Rahman and Yutaka Watanobe. 2023. ChatGPT for education and\nresearch: Opportunities, threats, and strategies. Applied Sciences 13, 9 (2023),\n5783.\n[34] Michael V. Reiss. 2023. Testing the Reliability of ChatGPT for Text Annotation and\nClassification: A Cautionary Remark. https://doi.org/10.48550/arXiv.2304.11085\narXiv:2304.11085 [cs].\n[35] OONI2023-02-24 Roskomsvoboda. 2023. How Internet censorship changed in\nRussia during the 1st year of military conflict in Ukraine. https://ooni.org/post/\n2023-russia-a-year-after-the-conflict/\n[36] Jürgen Rudolph, Shannon Tan, and Samson Tan. 2023. War of the chatbots: Bard,\nBing Chat, ChatGPT, Ernie and beyond. The new AI gold rush and its impact\non higher education. Journal of Applied Learning and Teaching 6, 1 (April 2023),\n364–389. https://doi.org/10.37074/jalt.2023.6.1.23 Number: 1.\n[37] Jérôme Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, and Markus\nPauly. 2023. The Self-Perception and Political Biases of ChatGPT. arXiv preprint\narXiv:2304.07333 (2023).\n[38] Jérôme Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, and Markus\nPauly. 2023. The Self-Perception and Political Biases of ChatGPT. https:\n//doi.org/10.48550/arXiv.2304.07333 arXiv:2304.07333 [cs].\n[39] Malik Sallam. 2023. ChatGPT utility in healthcare education, research, and\npractice: systematic review on the promising perspectives and valid concerns. In\nHealthcare, Vol. 11. MDPI, 887.\n[40] Sahib Singh and Narayanan Ramakrishnan. 2023. Is ChatGPT Biased? A Review.\nhttps://doi.org/10.31219/osf.io/9xkbu\n10\n[41] Statcounter. 2023. Search Engine Market Share Russian Federation. https:\n//gs.statcounter.com/search-engine-market-share/all/russian-federation/2022\n[42] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques Klein,\nand Tegawendé F. Bissyandé. 2023. Is ChatGPT the Ultimate Programming Assis-\ntant – How far is it? https://doi.org/10.48550/arXiv.2304.11938 arXiv:2304.11938\n[cs].\n[43] The Moscow Times. 2023. Putin Signs Law Punishing Mercenaries’ Critics With\nJail. https://www.themoscowtimes.com/2023/03/18/putin-signs-law-punishing-\ncriticism-of-mercenaries-with-jail-a80534 Section: ukraine_war.\n[44] Florian Toepfl, Anna Ryzhova, Daria Kravets, and Arista Beseler. 2023. Googling\nin Russian abroad: How Kremlin-affiliated websites contribute to the visibility\nof COVID-19 conspiracy theories in search results. International Journal of\nCommunication 17 (2023), 21.\n[45] Anton Troianovski and Adam Satariano. 2021. Google and Apple, Under Pressure\nFrom Russia, Remove Voting App. The New York Times (Sept. 2021). https://\nwww.nytimes.com/2021/09/17/world/europe/russia-navalny-app-election.html\n[46] Petter Törnberg. 2023. ChatGPT-4 Outperforms Experts and Crowd Workers\nin Annotating Political Twitter Messages with Zero-Shot Learning. https:\n//doi.org/10.48550/arXiv.2304.06588 arXiv:2304.06588 [cs].\n[47] Aleksandra Urman and Mykola Makhortykh. 2022. My war is your special\noperation: Engagement with pro-and anti-regime framing of the war in Ukraine\non Russian social media. (2022).\n[48] Aleksandra Urman, Mykola Makhortykh, and Roberto Ulloa. 2022. Auditing the\nrepresentation of migrants in image web search results. Humanities and Social\nSciences Communications 9, 1 (2022), 1–16.\n[49] Aleksandra Urman, Mykola Makhortykh, and Roberto Ulloa. 2022. The matter of\nchance: Auditing web search results related to the 2020 US presidential primary\nelections across six search engines. Social science computer review 40, 5 (2022),\n1323–1339.\n[50] Macrina Wang. 2023. NewsGuard Exclusive: ChatGPT-3.5 Generates More\nDisinformation in Chinese than in English. https://www.newsguardtech.com/\nspecial-reports/chatgpt-generates-disinformation-chinese-vs-english\n[51] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken:\nHow Does LLM Safety Training Fail? http://arxiv.org/abs/2307.02483\narXiv:2307.02483 [cs].\n[52] Julia Carrie Wong and Reuters. 2021. Microsoft blocks Bing from\nshowing image results for Tiananmen ‘tank man’. The Guardian (June\n2021). https://www.theguardian.com/technology/2021/jun/04/microsoft-bing-\ntiananmen-tank-man-results\n[53] Dong Zhe and Shi Shan. 2023. What happens when ChatGPT meets cen-\nsorship? https://www.rfa.org/english/news/afcl/fact-check-chatgpt-\n03122023112739.html\n[54] Sarah Zheng. 2023. China’s Answers to ChatGPT Have a Censorship Problem.\nBloomberg.com (May 2023). https://www.bloomberg.com/news/newsletters/2023-\n05-02/china-s-chatgpt-answers-raise-questions-about-censoring-generative-\nai\n11",
  "topic": "Ukrainian",
  "concepts": [
    {
      "name": "Ukrainian",
      "score": 0.7232323884963989
    },
    {
      "name": "Politics",
      "score": 0.7098654508590698
    },
    {
      "name": "Censorship",
      "score": 0.6386988162994385
    },
    {
      "name": "Opposition (politics)",
      "score": 0.6311990022659302
    },
    {
      "name": "Authoritarianism",
      "score": 0.5898453593254089
    },
    {
      "name": "Relation (database)",
      "score": 0.5362099409103394
    },
    {
      "name": "Political science",
      "score": 0.5111222863197327
    },
    {
      "name": "Silence",
      "score": 0.4481309652328491
    },
    {
      "name": "Social psychology",
      "score": 0.3692644536495209
    },
    {
      "name": "Law",
      "score": 0.3366076350212097
    },
    {
      "name": "Democracy",
      "score": 0.3312828540802002
    },
    {
      "name": "Psychology",
      "score": 0.3017151951789856
    },
    {
      "name": "Linguistics",
      "score": 0.2350662350654602
    },
    {
      "name": "Computer science",
      "score": 0.1457584798336029
    },
    {
      "name": "Aesthetics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I202697423",
      "name": "University of Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I118564535",
      "name": "University of Bern",
      "country": "CH"
    }
  ],
  "cited_by": 33
}