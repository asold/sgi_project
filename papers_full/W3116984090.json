{
  "title": "HOLMS: Alternative Summary Evaluation with Large Language Models",
  "url": "https://openalex.org/W3116984090",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1140460861",
      "name": "Yassine Mrabet",
      "affiliations": [
        "United States National Library of Medicine",
        "National Institutes of Health"
      ]
    },
    {
      "id": "https://openalex.org/A2054753577",
      "name": "Dina Demner-Fushman",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2742152190",
    "https://openalex.org/W2182572585",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2150824314",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2752395160",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2752501236",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W2134061542",
    "https://openalex.org/W2613678836",
    "https://openalex.org/W2398210540",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W1826672328",
    "https://openalex.org/W2793429163"
  ],
  "abstract": "Efficient document summarization requires evaluation measures that can not only rank a set of systems based on an average score, but also highlight which individual summary is better than another. However, despite the very active research on summarization approaches, few works have proposed new evaluation measures in the recent years. The standard measures relied upon for the development of summarization systems are most often ROUGE and BLEU which, despite being efficient in overall system ranking, remain lexical in nature and have a limited potential when it comes to training neural networks. In this paper, we present a new hybrid evaluation measure for summarization, called HOLMS, that combines both language models pre-trained on large corpora and lexical similarity measures. Through several experiments, we show that HOLMS outperforms ROUGE and BLEU substantially in its correlation with human judgments on several extractive summarization datasets for both linguistic quality and pyramid scores.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 5679–5688\nBarcelona, Spain (Online), December 8-13, 2020\n5679\nHOLMS: Alternative Summary Evaluation with Large Language Models\nYassine Mrabet\nmrabety@mail.nih.gov\nDina Demner-Fushman\nddemner@mail.nih.gov\nU.S. National Library of Medicine\nNational Institutes of Health\n8600 Rockville Pike, Bethesda, MD, 20894\nAbstract\nEfﬁcient document summarization requires evaluation measures that can not only rank a set of\nsystems based on an average score, but also highlight which individual summary is better than\nanother. However, despite the very active research on summarization approaches, few works have\nproposed new evaluation measures in the recent years. The standard measures relied upon for the\ndevelopment of summarization systems are most often ROUGE and BLEU which, despite being\nefﬁcient in overall system ranking, remain lexical in nature and have a limited potential when it\ncomes to training neural networks. In this paper, we present a new hybrid evaluation measure for\nsummarization, called HOLMS, that combines both language models pre-trained on large corpora\nand lexical similarity measures. Through several experiments, we show that HOLMS outperforms\nROUGE and BLEU substantially in its correlation with human judgments on several extractive\nsummarization datasets for both linguistic quality and pyramid scores.\n1 Introduction\n.\nGenerating human readable summaries of textual documents is posed to remain a key technology in our\ninformation era. Whether summarizing news, scientiﬁc articles, encyclopedias, or social media posts, the\ndemand for a faster consumption of the most relevant information is expected to grow hand in hand with\nthe amount of information available online.\nA current bottleneck to a wider adoption of automatic summarizers is the lack of efﬁcient solutions\naddressing both the relevance of the generated summaries and their linguistic quality. One component of\nthis current limitation is the lack of efﬁcient evaluation measures that address both aspects.\nThe two most cited and widely adopted measures in various summarization datasets and summarization\nchallenges are ROUGE (Lin and Hovy, 2003; Lin, 2004) and BLEU (Papineni et al., 2002). While both\nmeasures have been shown to be highly efﬁcient baselines in ranking summarization systems based on\ntheir average score (Dang and Vanderwende, 2007; Dang and Owczarzak, 2008; Dang and Owczarzak,\n2009; Owczarzakg, 2010; Owczarzak and Dang, 2011), fewer studies have examined their relevance for\nindividual summary ranking and their adequacy with linguistic quality and ﬂuency.\nWith the new levels of performance achieved by neural language models in a variety of natural language\nprocessing tasks, several insights point towards the high potential of their contextual language encoding\nfor language representation. Most of the state-of-the-art models such as T5 (Raffel et al., 2019), BERT\n(Devlin et al., 2019), GPT (Radford et al., 2018; Radford et al., 2019) and the Universal Sentence Encoder\n(Cer et al., 2018), are built from very large corpora, which reduces substantially the potential bias from\nusing them to evaluate summaries in a restricted document set or benchmark.\nOn the other hand, the shallow lexical features of the original texts play a key role in extractive\nsummarization and in pointer-generator approaches. It can also be argued that lexical similarities with\ngold summaries are implicitly capturing an important portion of the relevant semantics, especially at high\nsimilarity values.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nLicense details: http://creativecommons.org/licenses/by/4.0/\n5680\nIn this paper, we propose a new evaluation measure for summarization, called HOLMS, relying on\nboth contextual neural representations and lexical similarities. To capture the salient indicators from each\nside more efﬁciently, HOLMS relies on a multi-dimensional Gaussian function combining a sequential\nsimilarity measure based on neural embeddings and ROUGE. We motivate and present each component\nin more details in section 3.\nWe evaluate HOLMS on ﬁve different summarization datasets by computing its correlation with human\njudgements for content relevance and linguistic quality, and show that HOLMS has higher Pearson,\nSpearman and Kendall correlations than state-of-the-art measures on both aspects.\nIn the next section, we discuss related works on summarization evaluation. We present HOLMS in\ndetails in section 3 and our experiments in section 4. Finally we discuss the results in section 5 before\nconcluding.\n2 Related Work\nSummary evaluation was studied from several perspectives, including the similarity of the candidate\nsummaries with reference human summaries, their intrinsic linguistic quality and coherence, and their\nrelevance w.r.t. the original document (Cabrera-Diego and Torres-Moreno, 2018; Lloret et al., 2018). In\nthis paper, we focus on extrinsic evaluations when human generated summaries are available as they offer\nboth more speciﬁc parameters for the task, and available benchmarks with human-generated scores for\nautomatically generated summaries.\nLexical Semantic Gold sum. Full text Reference Citations\nROUGE ! — ! — (Lin and Hovy, 2003) 1,531\nBLEU ! — ! — (Papineni et al., 2002) 10,628\nROUGE-WE ! ! ! — (Ng and Abrecht, 2015) 35\nROUGE-2.0 ! ! ! — (Ganesan, 2018) 15\nAutoSummENG ! — ! — (Giannakopoulos and Karkaletsis, 2011) 26\nHOLMS ! ! ! — — —\nTable 1: Related summarization evaluation metrics\nTable 1, we present several summarization evaluation measures and their main characteristics. In terms\nof usage, ROUGE and BLEU stand out as the most cited measures, albeit a big portion of BLEU citations\nare likely from language translation papers, and not from research works on summarization.\nROUGE stands for Recall-Oriented Understudy of Gisting Evaluation (Lin, 2004). It allows evaluating\nsystem-generated summaries by comparing them with (ideal) summaries created by humans. It relies on\ncomputing the ratio of overlapping units between the two summaries and has several variants according to\nthe unit type: e.g., unigrams, n-grams or skip-grams.\nBLEU was designed for the evaluation of language translation systems (Papineni et al., 2002). It is\na precision metric that counts a unigram as correct if it occurs in a reference translation and tackles\nredundancies by clipping the maximum number that a candidate word can be matched to its maximum\nnumber of occurrences in the gold/reference translation.\nOther metrics have been proposed, mostly in the ﬁrst decade of the twenty ﬁrst century, in the context\nof the DUC and TAC challenges, with a few recent exceptions. For instance, ROUGE 2.0 was proposed in\n2018 as an extension of ROUGE that relies on Wordnet synonyms (Pedersen et al., 2004) and main topics\nin addition to the original tokens. AutoSummENG is a lexical method relying on computing similarities\nbetween the n-gram graphs of candidate summaries and gold summaries. Edges represent proximity\nbetween two n-grams and are weighted by the number of co-occurrences of their vertices (connected\nn-grams) in a speciﬁed window of text.\nOne of the closest approaches to our work considered applying the ROUGE metric using word\nembeddings (Ng and Abrecht, 2015). Instead of computing word matching in a binary fashion as in\nROUGE, the authors consider a word or n-gram similarity to be either 0 if the candidate word is out of\nvocabulary, or equal to the dot-product of their embeddings otherwise. While this approach made use\nof contextual embeddings, it still required the words to be present in both the gold summary and the\n5681\ncandidate summary, which makes it subject to the same limitation of ROUGE, i.e., the lack of semantic\ngeneralization that would allow matching synonyms and paraphrases. Results-wise, their approach was\ntested only on one dataset, and under-performed substantially ROUGE-SU4 and other ROUGE variants in\nterms of Pearson’s correlation.\nIn contrast, HOLMS does not restrict the coverage of neural representations with lexical constraints, but\nexplores new associative ways to get the best of both worlds. The embeddings component of HOLMS also\nrelies on a sequential similarity function that grants increasingly more weight to (matching) sub-sequences\nwith higher embeddings similarity. As far as we know, HOLMS is the ﬁrst summarization evaluation\nmeasure that (a) takes into account embeddings similarity with a sequential perspective, and (b) uses a\nGaussian function to combine lexical and embeddings-based similarities.\n3 HOLMS\nHOLMS stands for Hybrid Lexical and MOdel-based evaluation of Summaries. It relies on both deep\ncontextual embeddings and lexical similarities.\nThe Embeddings-based Similarity(ES) used in HOLMS is computed in a sequential method. The\nintuition behind ES is that embeddings similarity between a word in a system summary and a word in\na reference summary should be more pronounced if the contiguous words also have high embeddings\nsimilarity. Such perspective can also be extended to consecutive n-grams to capture conceptual similarity\nbetween sequences.\nConcretely, to compute ES, a ﬁrst step is to transform the input texts from both the gold and system\nsummaries into two sets of consecutive n-grams: G = {g1,...,g l}for the gold summary, and A =\n{a1,...,a m}for a system summary. An embedding vector VL(x) is then generated for each n-gram x\nfrom a given language model L.\nA second step is to compute the best distance value for a given gold n-gram gi, DistA(gi), with a\nﬁltering method where each system n-gram a∈Acan only be used once as the best match for any given\ngold n-gram g∈G.\nDistA(gi) =Min{aj ∈Ai} euc(VL(gi),VL(aj)) (1)\nWith eucbeing the euclidean distance, and Ak the dynamic set computed by removing the previously\nmatched n-grams in A.\nThe last step is inspired from TextFlow (Mrabet et al., 2017), and consists in using the (consecutive)\ndistance values as coordinates on a curve and computing the area under curve as the overall distance. This\nis best shown with an example as in ﬁgure 1. The two example sentences, S, and Gare from a news\narticle1, and were slightly modiﬁed for ease of presentation:\n•G: “The man who ate the $120K banana art on the wall said that he was not sorry and that he was\nperforming art by eating it.”\n•S: “An artist claimed he was performing art by eating a banana used as a center piece in the art work\nof a colleague.”\nDrawing the curve connecting the distance values as coordinates allows adding more weight to highly\nmatching sub-sequences and less weight to weakly matched sub-sequences. The ﬁnal value of ES is then\ncomputed as the complementary of the area under curve normalized by |G|×Maxeuc to obtain values in\nthe [0,1] range.\nThe Lexical Similarityused in HOLMS is the ROUGE-1 recall. There is no restriction on the\nlexical similarity measure, or ensemble of measures, that could be used for the lexical component\n1https://cutt.ly/UrSccG7\n5682\nFigure 1: Illustration of the area under curve representing the HOLMS value.\nof HOLMS. We picked ROUGE-1 for our ﬁrst proof-of-concept as it has shown a good potential in\nranking systems based on their average performance and is also widely used in the community. For\nthe remainder of the paper, we will refer to ROUGE-1, ROUGE-2 ,and ROUGE-SU4 asR1, R2, and RSU 4.\nThe Combinationof both aspects (lexical and neural) needs to take into account the implicit and\nrelative links existing between shallow lexical similarities and embeddings-based similarity. To this effect,\nwe build HOLMS using a bound three-dimensional Gaussian function that highlights further the summary\npairs on which both measures agree, by promoting or downgrading exponentially strong agreements and\nstrong disagreements. The function has its peak at 1 for perfect agreement on the quality of a summary\nand a low at 0 for total disagreement between the two measures (cf. ﬁgure 2).\nHOLMS{ES,R1}(x,y) = exp\n(\n−(x−x0)2\n2σ2x\n−(y−y0)2\n2σ2y\n)\n(2)\nwith x0 and y0 the coordinates of ES and R1 peaks and σ2\nx and σ2\ny the respective spreads.\nFigure 2: HOLMS: structure and value range (3D Gaussian peaks and spreads are both set to 1).\nIn practice, the values on the X and Y axes (representing the values of the evaluation measures) will\nnever exceed the x and y coordinates of the peak, hence the bound nature of the function.\n5683\n4 Evaluation\nWe used 4-grams as our units to compute the ES and HOLMS values. The peaks and spreads are set to 1.\nTo compare HOLMS with a hybrid baseline, we also compute the correlation results for linear, a simple\nequal-weighted linear combination of ES and R1.\nWe tested several variations ofES using three different sources for neural embeddings:\n•The BERT large uncased model based on transformers and trained on large corpora such as Google\nbooks and Wikipedia using word masking and consecutive sentences classiﬁcation.\n•The universal sentence encoder (Cer et al., 2018) based on auto-encoders. We used the transformer-\nbased version in our experiments instead of the slightly less accurate deep averaging version.\n•Glove embeddings (Pennington et al., 2014) based on both word co-occurrence and local context\nwindows.\nIn our preliminary experiment on the TAC 2011 dataset (Owczarzak and Dang, 2011), we observed\nthat both the BERT embeddings and Glove embeddings underperformed substantially the embeddings\nfrom the universal sentence encoder (use) by more than 51%. We experimented with both the CLS class\nembeddings and the sum of the tokens embeddings from BERT, and with different vector dimensions for\nGlove. Subsequently, we used only the universal sentence encoder for our extended experiments on the 5\ndatasets. In future works, we plan to test more recent language models such as T5 (Raffel et al., 2019) and\nSciBERT (Beltagy et al., 2019).\nWe compute the correlations of HOLMS with human judgments of automatic summaries on 5 datasets\nfrom the summarization benchmarks introduced in the DUC and TAC challenges from 2007 to 2011(Dang\nand Vanderwende, 2007; Dang and Owczarzak, 2008; Dang and Owczarzak, 2009; Owczarzakg, 2010;\nOwczarzak and Dang, 2011). We use the three standard correlation measures: Pearson correlation (P),\nSpearman rank coefﬁcient (S), and Kendall’s rank coefﬁcient (K).\nEach dataset consists of a set of source documents, a set of human-generated summaries for\neach document (called models in the challenges), and manual annotations of the relevance score\nfor each candidate summary generated by the participating systems. The score for each candidate\nsummary is computed as the average similarity between the system summary and the set of reference\nsummaries. For content relevance, human assessors ﬁrst selected the important content units (SCUs)\nthen used the pyramid method (Passonneau et al., 2005) to score automatically the system-generated\nsummaries based on the SCUs. In addition to content relevance scoring, the assessors also annotated\nthe system-summaries with linguistic quality scores ranging from 1 to 5 for the DUC 2007, TAC 2008,\nand TAC 2011 datasets. Several scores ranging from 1 to 5 were ﬁrst assigned to assess the system\nsummaries according to ﬁve questions or aspects: grammaticality, non-redundancy, referential clarity,\nfocus, structure, and coherence. The ﬁnal linguistic quality score is then obtained by averaging the\nanswers/scores given to all sub-questions. Further details about the data collection and annotation\nmethods are described in the challenges overview papers and websites (Dang and Vanderwende, 2007;\nDang and Owczarzak, 2008; Dang and Owczarzak, 2009; Owczarzakg, 2010; Owczarzak and Dang, 2011).\nIn the DUC and TAC editions, the ROUGE measure achieved state-of-the-art performance on its\ncorrelation with human judgments for both content relevance and linguistic quality. However, most\nevaluations of the TAC and DUC tracks relied only on average system scores and did not focus on ranking\nindividual summaries.\nTables 2 and 3 present the correlations of BLEU, ROUGE, and HOLMS, with the linguistic quality\n5684\nscores computed manually by the NIST assessors2.\nTable 2 presents the Pearson, Spearman, and Kendall correlations on the linguistic quality of individual\nsummaries. Table 3 presents the correlations on the average linguistic quality scores of each participating\nsystem.\nTables 4 and 5 present the correlations with Pyramid scores computed manually by NIST assessors to\nmeasure the content relevance of the system summaries. Table 4 presents the Pearson’s (P), Spearman’s\n(S), and Kendall’s (K) correlations on individual summaries. Table 5 presents the correlations on the\naverage scores of each participating system.\nDataset Corr. BLEU R1 R2 RSU 4 ES Lin. HOLMS\nDUC 2007\nP -.059 (.017) .216 .168 .216 .139 .186 0.238\nS -.066 (.008) .121 .137 .121 .094 .110 0.109\nK -.058 (.008) .090 .101 .090 .070 .080 0.076\nTAC 2008\nP -.010 (.604) .132 .128 .126 .117 .132 .136\nS -.011 (.578) .147 .144 .145 .145 .156 .159\nK -.010 (.576) .110 .108 .108 .111 .119 .121\nTAC 2011\nP .003 (.896) .361 .263 .294 .342 .365 .376\nS .001 (.967) .242 .260 .235 .286 .272 .276\nK .001 (.965) .183 .196 .178 .215 .206 .208\nTotal Average\nP -.022 .236 .186 .212 .199 .227 .25\nAverage S -.026 .170 .180 .167 .175 .179 .181\nK -.022 .127 .135 .125 .132 .135 .135\nTable 2: Correlations with Individual Summaries’ Linguistic Quality\nDataset Corr. BLEU R1 R2 RSU 4 ES Lin. HOLMS\nDUC07\nP .569 .352 (.022) .326 .352 .328 (.034) .344 (.026) .753\nS .474 .427 (.005) .301 .427 .353 (.022) .390 (.011) .643\nK .336 .286 (.009) .192 .286 .245 (.026) .259 (.018) .451\nTAC08\nP .132 (.324) .434 .430 .417 .383 (.003) .416 (.001) .411 (.001)\nS .161 (.227) .379 (.003) .433 .404 .324 (.013) .351 (.007) .362 (.005)\nK .105 (.248) .268 (.003) .309 .287 .225 (.013) .243 (.007) .254 (.005)\nTAC11\nP .308 (.030) .733 .705 .739 .729 .739 .741\nS .210 (.144) .341 (.015) .361 .358 .331 (.019) .342 (.015) .325 (.021)\nK .150 (.128) .242 (.014) .263 .263 .226 (.022) .230 (.019) .219 (.026)\nTotal Average\nP .336 .506 .487 .503 .480 .500 .634\nAverage S .281 .382 .365 .396 .336 .361 .443\nK .197 .265 .255 .279 .232 .243 0.310\nTable 3: Correlations with Average Systems’ Linguistic Quality\n2(P: Pearson, S: Spearman, K: Kendall). All statistical p-values are strictly below 0.001 unless otherwise speciﬁed between\nbrackets. Best results are highlighted in bold. Second best are underlined.\n5685\nDataset Corr. BLEU R1 R2 RSU 4 ES Lin. HOLMS\nDUC 2007\nP -.064 .365 .316 .365 .357 .379 0.376\nS -.058 .340 .322 .340 .348 .358 .42 (.095)\nK -.051 .250 .235 .250 .257 .265 .59 (.001)\nTAC 2008\nP .000 (.982) .546 .465 .495 .468 .541 .538\nS .000 (.995) .537 .486 .507 .480 .547 .539\nK .000 (.999) .376 .336 .353 .332 .381 .376\nTAC 2009\nP -.008 (.679) .665 .645 .665 .650 .685 .682\nS -.002 (.916) .630 .609 .630 .604 .643 .639\nK -.002 (.925) .460 .442 .460 .436 .471 .467\nTAC 2010\nP .115 .690 .653 .690 .708 .726 .722\nS .116 .715 .662 .715 .704 .739 .735\nK .094 .525 .480 .525 .516 .547 .543\nTAC 2011\nP .154 .630 .568 .606 .642 .660 .660\nS .162 .596 .552 .566 .609 .630 .627\nK .129 .425 .391 .401 .434 .452 .450\nTotal Average\nP .039 .579 .529 .564 .565 .598 .596\nAverage S .043 .564 .526 .552 .549 .583 .592\nK .059 .407 .377 .398 .395 .423 .485\nTable 4: Correlations with Individual Summaries’ Pyramid Scores\nDataset Corr. BLEU R1 R2 RSU 4 ES Lin. HOLMS\nDUC 2007\nP .598 .615 .690 .615 .640 .632 .769\nS .604 .762 .699 .762 .698 .738 .876\nK .474 .573 .511 .573 .499 .547 .698\nTAC 2008\nP .242 (.067) .882 .907 .887 .911 .908 .905\nS .109 (.417) .865 .908 .884 .905 .889 .889\nK .097 (.283) .706 .757 .729 .724 .722 .724\nTAC 2009\nP .427 .940 .911 .940 .944 .955 .954\nS .350 (.009) .894 .952 .894 .922 .923 .922\nK .240 (.010) .730 .824 .730 .787 .775 .772\nTAC 2010\nP .369 (.015) .928 .977 .928 .978 .968 .985\nS .505 .950 .917 .950 .958 .969 .969\nK .364 .824 .781 .824 .829 .872 .872\nTAC 2011\nP .614 .954 .954 .975 .969 .972 .976\nS .566 .909 .889 .888 .858 .897 .902\nK .400 .742 .736 .724 .675 .728 .736\nTotal Average\nP .450 .864 .888 .869 .888 .887 .917\nAverage S .426 .876 .873 .876 .868 .883 .912\nK .315 .715 .722 .716 .703 .729 .760\nTable 5: Correlations with Average Systems’ Pyramid Score\n5686\n5 Discussion\nCommon observations. BLEU underperformed substantially all other measures in our experiments,\nexcept for the correlation on the average systems linguistic score in DUC 2007, which is likely due to\nshorter summaries. This could be caused mostly by the design of BLEU that was aimed at sentence-level\nmachine translation. To capture only the relative performance of each summary and each system, we\nnormalized the BLEU scores by the maximum score obtained by a system/summary for a given document.\nThis improved the results (presented in section 4) compared with the correlation of the raw BLEU scores\nbut was not enough to close the gap with the other evaluation measures. Another common observation is\nthe relatively low level of correlation of all measures on linguistic quality when compared to content\nrelevance correlations (pyramid scores). This can be explained in part by a higher level of subjectivity for\nlinguistic quality inducing a higher assessor bias.\nEvaluation of the linguistic quality of individual summaries.HOLMS outperformed the ROUGE\nvariants, its embeddings-based component ES and the linear baseline in terms of Pearson correlation with\na relative improvement ranging from 3% to 10.1%. The fact that HOLMS led to an improvement over\nboth of its components (ES and R1) and over the linear baseline suggests that:\n•The embeddings-based sequential similarity ES and ROUGE-1 brought different but complementary\nperspectives on the linguistic quality of a given summary.\n•HOLMS was better suited to take advantage of those different perspectives than the linear equal-\nweighted baseline.\nIn terms of Spearman’s and Kendall’s correlation factors, the picture was slightly more nuanced,\nwith ES performing better on TAC 2011, HOLMS performing better on TAC 2008, and ROUGE-2\nperforming better on DUC2007. The macro-average is less prone to dataset-speciﬁc bias and showed that\nHOLMS performed better than all other measures.\nEvaluation of the average linguistic quality of summarization systems. When analyzing the\ncorrelation values for average system scores, the beneﬁts brought by HOLMS are even more substantial\nwith a relative improvement of +25.2% on average (cf. table 3). The ablation study led to the interesting\nobservation of the ROUGE variants outperforming the embeddings-based similarity and the linear\ncombination. This suggests that: (1) lexical similarity measures can have a more relevant coarse-grained\npicture on system-level linguistic quality, and that neural language models are not necessarily better suited\nto rank extractive systems based on linguistic quality when gold summaries are available, and (2) neural\nlanguage models still have a distinct perspective as shown by the better results obtained by HOLMS, and\ncan make relevant hybrid methods more efﬁcient at system ranking than lexical measures alone.\nEvaluation of content relevance for individual summaries.The hybrid methods outperformed the\nROUGE variants and the embeddings based similarity (ES) on content relevance with HOLMS per-\nforming better than the linear baseline on average in terms of Spearman and Kendall correlation factors,\nwhile maintaining comparable Pearson values (cf.table 4). In terms of components, the picture was less\nmixed, with ROUGE-1 performing slightly better on average on the three correlation measures. This\nshows that when gold summaries are available, performing better than lexical evaluation measures on\ncontent relevance is not as straightforward as computing embeddings similarities or taking into account\nsub-sequence similarities with measures like ES. Going to the embeddings space seemed to result in a\nrelatively small loss of information when compared to ROUGE-1 for individual summary ranking. One\npotential explanation is that at the scale of one (small) document, the precision of a restricted terminology\nis greater than the precision of large (contextual) language models. This ﬁnding, together with the higher\nperformance of HOLMS on the evaluation of content relevance for individual summaries, supports\nthe theoretical effectiveness of pointer-generator approaches that combine abstractive and extractive\nfunctions.\n5687\nEvaluation of content relevance for summarization systems. The improvement provided by\nHOLMS on the evaluation of average system performance for content relevance was noticeable over all\nbaselines and datasets, with an average increase in correlation of 3.2%, 3.2%, and 4.2% respectively\nfor Pearson’s, Spearman’s and Kendall’s correlation factors. The lexical and neural components had\ncomparable correlation results, with the linear baseline consistently under-performing HOLMS. This\nvalidates further the observation that the methods used to combine the lexical and neural language model\nspaces for summary evaluation can play a key role in improving systems evaluation when designed\nappropriately.\nIn a more general note, the proximity of the correlation results obtained byES and the ROUGE variants\nraise an interesting question on the codiﬁcation of language (or meaning) by neural embeddings and the\nextent to which their underlying representations provide an actual semantic generalization or rather a\nsymbolic compression that remains tuned for data patterns that are both complex and shallow. What\nwe can observe from our experiments is that the Gaussian combination through HOLMS outperforms\nboth lexical and neural measures. This shows that embeddings do provide distinct and complementary\ninformation to the discrete/lexical information considered by ROUGE, but that the differences might\nnot be as wide as could be expected. Moving forward, we are likely going to need either substantially\ndifferent language representation spaces, or the integration of a different source of semantics such as\nknowledge bases and their associated neural graph models.\nLimitations. In this paper, we did not address abstractive summarization due to the lack of sufﬁciently\nlarge abstractive summarization datasets with human judgments of content quality and relevance. We\nexpect the neural-embedding component of HOLMS to have a higher impact in generative summarization\napproaches, and acquiring or building such datasets is one of our short-term objectives.\n6 Conclusions\nWe presented a new summarization evaluation measure, called HOLMS , based on a sequential n-gram\nembeddings similarity and ROUGE. In our experiments on 5 summarization evaluation benchmarks,\nHOLMS performed consistently better on average than its individual components, BLEU, and a linear\ncombination baseline. HOLMS can also be used as a framework, as many more variations can be\ntested, including the use of consecutive skip-grams as the input instead of n-grams, and the combination\nof sentence level similarity and n/skip-gram similarity. Moving forward, we think that summarization\nsystems should be evaluated on a combination of discrete and contextual or semantic evaluation measures.\nSuch extension ﬁts naturally in HOLMS through the insertion of additional dimensions in its Gaussian\nfunction. Our results suggest that such combination is likely to bring a higher level of correlation with\nhuman assessments of both linguistic quality and content relevance.\nAcknowledgments\nThis work was supported by the intramural research program at the U.S. National Library of Medicine,\nNational Institutes of Health.\nReferences\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientiﬁc text. InProceed-\nings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3606–3611.\nLuis Adri ´an Cabrera-Diego and Juan-Manuel Torres-Moreno. 2018. Summtriver: A new trivergent model to\nevaluate summaries automatically without human references. Data & Knowledge Engineering, 113:184–197.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario\nGuajardo-Cespedes, Steve Yuan, Chris Tar, et al. 2018. Universal sentence encoder. arXiv preprint\narXiv:1803.11175.\n5688\nHoa Trang Dang and Karolina Owczarzak. 2008. Overview of the tac 2008 update summarization task. In\nProceedings of the Text Analysis Conference (TAC).\nHoa Trang Dang and Karolina Owczarzak. 2009. Overview of the tac 2009 summarization track. Proceedings of\nthe Text Analysis Conference (TAC).\nHoa Trang Dang and Lucy Vanderwende. 2007. Overview of duc 2007 tasks and evaluation results. In Document\nUnderstanding Conference (DUC).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186.\nKavita Ganesan. 2018. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks. arXiv\npreprint arXiv:1803.01937.\nGeorge Giannakopoulos and Vangelis Karkaletsis. 2011. Autosummeng and memog in evaluating guided sum-\nmaries. In TAC.\nChin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics.\nIn Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the\nAssociation for Computational Linguistics, pages 150–157.\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of Workshop on\nText Summarization Branches Out, Post2Conference Workshop of ACL.\nElena Lloret, Laura Plaza, and Ahmet Aker. 2018. The challenging task of summary evaluation: an overview.\nLanguage Resources and Evaluation, 52(1):101–148.\nYassine Mrabet, Halil Kilicoglu, and Dina Demner-Fushman. 2017. Textﬂow: A text similarity measure based\non continuous sequences. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 763–772.\nJun-Ping Ng and Viktoria Abrecht. 2015. Better summarization evaluation with word embeddings for rouge.\narXiv preprint arXiv:1508.06034.\nKarolina Owczarzak and Hoa Trang Dang. 2011. Overview of the tac 2011 summarization track: Guided task and\naesop task. Proceedings of the Text Analysis Conference (TAC).\nKarolina Owczarzakg. 2010. Overview of the tac 2010 summarization track. Proceedings of the Text Analysis\nConference (TAC).\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation\nof machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics,\npages 311–318. Association for Computational Linguistics.\nRebecca J Passonneau, Ani Nenkova, Kathleen McKeown, and Sergey Sigelman. 2005. Applying the pyramid\nmethod in duc 2005. In Proceedings of the document understanding conference (DUC 05), Vancouver, BC,\nCanada.\nTed Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet:: Similarity: measuring the related-\nness of concepts. In Demonstration papers at HLT-NAACL 2004, pages 38–41. Association for Computational\nLinguistics.\nJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word represen-\ntation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),\npages 1532–1543.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving lan-\nguage understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language understanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners. OpenAI Blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J. Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv\ne-prints.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9502010345458984
    },
    {
      "name": "Computer science",
      "score": 0.8418015241622925
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7056669592857361
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.6931527853012085
    },
    {
      "name": "Natural language processing",
      "score": 0.6914255619049072
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.5919271111488342
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5228829383850098
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.4785333573818207
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.46957752108573914
    },
    {
      "name": "Multi-document summarization",
      "score": 0.4115193784236908
    },
    {
      "name": "Machine learning",
      "score": 0.3946499228477478
    },
    {
      "name": "Information retrieval",
      "score": 0.3943770229816437
    },
    {
      "name": "Data mining",
      "score": 0.21235498785972595
    },
    {
      "name": "Mathematics",
      "score": 0.06400677561759949
    },
    {
      "name": "Image (mathematics)",
      "score": 0.06170949339866638
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    }
  ]
}