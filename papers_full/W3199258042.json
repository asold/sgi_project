{
  "title": "Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation",
  "url": "https://openalex.org/W3199258042",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2965692062",
      "name": "Mozhdeh Gheini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096853820",
      "name": "Jonathan May",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2972534020",
    "https://openalex.org/W2887920589",
    "https://openalex.org/W3096966601",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W3104652516",
    "https://openalex.org/W2945383715",
    "https://openalex.org/W3096841893",
    "https://openalex.org/W2964085268",
    "https://openalex.org/W4287597359",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2294774419",
    "https://openalex.org/W2963118869",
    "https://openalex.org/W3093886841",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3116273903",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2561995736",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W2752630748",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2126725946",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W3034955736",
    "https://openalex.org/W2741602058",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2963088995",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3103182178",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3134307371",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963633299",
    "https://openalex.org/W3035254119",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2990555152"
  ],
  "abstract": "We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine-tuning only the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the entire translation model). We provide insights into why this is the case and observe that limiting fine-tuning in this manner yields cross-lingually aligned embeddings. The implications of this finding for researchers and practitioners include a mitigation of catastrophic forgetting, the potential for zero-shot translation, and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1754–1765\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n1754\nCross-Attention is All You Need:\nAdapting Pretrained Transformers for Machine Translation\nMozhdeh Gheini, Xiang Ren, Jonathan May\nInformation Sciences Institute\nUniversity of Southern California\n{gheini, xiangren, jonmay}@isi.edu\nAbstract\nWe study the power of cross-attention in the\nTransformer architecture within the context\nof transfer learning for machine translation,\nand extend the ﬁndings of studies into cross-\nattention when training from scratch. We\nconduct a series of experiments through ﬁne-\ntuning a translation model on data where ei-\nther the source or target language has changed.\nThese experiments reveal that ﬁne-tuning only\nthe cross-attention parameters is nearly as ef-\nfective as ﬁne-tuning all parameters (i.e., the\nentire translation model). We provide insights\ninto why this is the case and observe that lim-\niting ﬁne-tuning in this manner yields cross-\nlingually aligned embeddings. The implica-\ntions of this ﬁnding for researchers and practi-\ntioners include a mitigation of catastrophic for-\ngetting, the potential for zero-shot translation,\nand the ability to extend machine translation\nmodels to several new language pairs with re-\nduced parameter storage overhead.1\n1 Introduction\nThe Transformer (Vaswani et al., 2017) has become\nthe de facto architecture to use across tasks with\nsequential data. It has been dominantly used for\nnatural language tasks, and has more recently also\npushed the state-of-the-art on vision tasks (Doso-\nvitskiy et al., 2021). In particular, transfer learn-\ning from large pretrained Transformer-based lan-\nguage models has been widely adopted to train new\nmodels: adapting models such as BERT (Devlin\net al., 2019) and XLM-R (Conneau et al., 2020)\nfor encoder-only tasks and models such as BART\n(Lewis et al., 2020) and mBART (Liu et al., 2020)\nfor encoder-decoder tasks like machine translation\n(MT). This transfer learning is predominantly per-\nformed in the form of ﬁne-tuning: using the values\nof several hundred million parameters from the pre-\ntrained model to initialize a model and start training\nfrom there.\n1Our code is available at https://github.com/\nMGheini/xattn-transfer-for-mt .\nFine-tuning pretrained models often involves up-\ndating all parameters of the model without making\na distinction between them based on their impor-\ntance. However, copious recent studies have looked\ninto the relative cruciality of multi-headed self- and\ncross- attention layers when training an MT model\nfrom scratch(V oita et al., 2019; Michel et al., 2019;\nYou et al., 2020). Cross-attention (also known as\nencoder-decoder attention) layers are more impor-\ntant than self-attention layers in the sense that they\nresult in more degradation in quality when pruned,\nand hence, are more sensitive to pruning (V oita\net al., 2019; Michel et al., 2019). Also, cross-\nattention cannot be replaced with hard-coded coun-\nterparts (e.g., an input-independent Gaussian dis-\ntribution) without signiﬁcantly hurting the perfor-\nmance, while self-attention can (You et al., 2020).\nWith the ubiquity of ﬁne-tuning as a training tool,\nwe ﬁnd a similar investigation focused on trans-\nfer learning missing. In this work, we inspect\ncross-attention and its importance and capabilities\nthrough the lens of transfer learning for MT.\nAt a high level, we look at training a model for\na new language pair by transferring from a pre-\ntrained MT model built on a different language\npair. Given that, our study frames and addresses\nthree questions: 1) How powerful is cross-attention\nalone in terms of adapting to the new language pair\nwhile other modules are frozen? 2) How crucial\nare the cross-attention layers pretrained values with\nregard to successful adaptation to the new task?\nand 3) Are there any qualitative differencesin the\nlearned representations when cross-attention is the\nonly module that gets updated?\nTo answer these questions, we compare mul-\ntiple strategies of ﬁne-tuning towards a new lan-\nguage pair from a pretrained translation model that\nshares one language with the new pair. These are\ndepicted in Figure 1: a) Ignoring the pretrained\nparameters and training entirely from randomly\ninitialized parameters (i.e. ‘from scratch’) b) Fine–\n1755\nFigure 1: Overview of our transfer learning experiments, depicting (a) training from scratch, (b) conventional\nﬁne-tuning ( src+body), (c) ﬁne-tuning cross-attention ( src+xattn), (d) ﬁne-tuning new vocabulary ( src), (e)\nﬁne-tuning cross-attention when transferring target language ( tgt+xattn), (f) transfer learning with updating\ncross-attention from scratch ( src+randxattn). Dotted components are initialized randomly, while solid lines\nare initialized with parameters from a pretrained model. Shaded, underlined components are ﬁne-tuned, while\nother components are frozen.\ntuning all parameters except the embeddings for\nthe language in common,2 (i.e. ‘regular’ ﬁne-tun-\ning, our upper bound), c) Fine-tuning solely the\ncross-attention layers and new embeddings, and\nd) Fine-tuning only the new embeddings. Here,\nnew embeddings refer to randomly initialized em-\nbeddings corresponding to the vocabulary of the\nnew language. In Figures 1a–1d, we assume the\nnew language pair has a new source language and\nnot a new target language; Figure 1e shows an ex-\nample of target-side transfer. In the experiments\nthat follow we will always train new, randomly\ninitialized embeddings for the vocabulary of the\nnewly introduced language. Generally, all other\nparameters are imported from a previously built\ntranslation model and, depending on the experi-\nment, some will remain unchanged and others will\nbe adjusted during training.\nOur experiments and analyses show that ﬁne-\ntuning the cross-attention layers while keeping the\nencoder and decoder ﬁxed results in MT quality\nthat is close to what can be obtained when ﬁne-\ntuning all parameters (§4). Evidence also sug-\ngests that ﬁne-tuning the previously trainedcross-\nattention values is in fact important—if we start\nwith randomly initialized cross-attention parame-\nter values instead of the pretrained ones, we see a\nquality drop.\nFurthermore, intrinsic analysis of the embed-\ndings learned under the two scenarios reveals that\nfull ﬁne-tuning exhibits different behavior from\n2Freezing shared language embeddings is common prac-\ntice (Zoph et al., 2016).\ncross-attention-only ﬁne-tuning. When the encoder\nand decoder bodies are not ﬁne-tuned, we show\nthat the new language’s newly-learned embeddings\nalign with the corresponding embeddings in the\npretrained model. That is, when we transfer from\nFr–En to Ro–En for instance, the resulting Roma-\nnian embeddings are aligned with the French em-\nbeddings. However, we do not observe the same\neffect when ﬁne-tuning the entire body. In §5 we\nsee how such aligned embeddings can be useful.\nWe speciﬁcally show they can be used to alleviate\nforgetting and perform zero-shot translation.\nFinally, from a practical standpoint, our strategy\nof ﬁne-tuning only cross-attention is also a more\nlightweight ﬁne-tuning approach (Houlsby et al.,\n2019) that reduces the storage overhead for extend-\ning models to new language pairs: by ﬁne-tuning a\nsubset of parameters, we only need to keep a copy\nof those instead of a whole-model’s worth of values\nfor the new pair. We quantify this by reporting the\nfraction of parameters that is needed in our case\nrelative to having to store a full new model for each\nadapted task.\nOur contributions are: 1) We empirically show\nthe competitive performance of exclusively ﬁne-\ntuning the cross-attention layers when contrasted\nwith ﬁne-tuning the entire Transformer body; 2) We\nshow that when ﬁne-tuning only the cross-attention\nlayers, the new embeddings get aligned with the re-\nspective embeddings in the pretrained model. The\nsame effect does not hold when ﬁne-tuning the en-\ntire Transformer body; 3) we demonstrate effective\napplication of this aligning artifact in mitigating\n1756\ncatastrophic forgetting (Goodfellow et al., 2014)\nand zero-shot translation.\n2 Cross-Attention Fine-Tuning for MT\nFine-tuning pretrained Transformer models to-\nwards downstream tasks has pushed the limits of\nNLP, and MT has been no exception (Liu et al.,\n2020). Despite the prevalence of using pretrained\nTransformers, recent studies focus on investigat-\ning the importance of self- and cross- attention\nheads while training models from scratch (V oita\net al., 2019; Michel et al., 2019; You et al., 2020).\nThese studies verify the relative importance of\ncross-attention over self-attention heads by explor-\ning either pruning (V oita et al., 2019; Michel et al.,\n2019) or hard-coding methods (You et al., 2020).\nConsidering these results and the popularity of pre-\ntrained Transformers, our goal in this work is to\nstudy the signiﬁcance of cross-attention while fo-\ncusing on transfer learning for MT. This section\nformalizes our problem statement, introduces the\nnotations we will use, and describes our setup to\naddress the questions we raise.\n2.1 Problem Formulation\nIn this work, we focus on investigating the effects\nof the cross-attention layers when ﬁne-tuning pre-\ntrained models towards new MT tasks. Fine-tuning\nfor MT is a transfer learning method that, in its\nsimplest form (Zoph et al., 2016), involves training\na model called the ‘parent’ model on a relatively\nhigh-resource language pair, and then using the\nobtained parameters to initialize a ‘child model’\nwhen further training towards a new, potentially\nlow-resource, language pair. Here, high-resource\nand low-resource refer to the amount of parallel\ndata that is available for the languages. Hence-\nforth, we use ‘parent’ and ‘child’ when referring to\ntraining components (e.g., model, data, etc.) in the\npretraining and ﬁne-tuning stages, respectively.\nFormal Deﬁnition. Consider a model fθ trained\non the parent dataset, where each training instance\n(xsp ,ytp ) is a pair of source and target sentences in\nthe parent language pair sp–tp. Then ﬁne-tuning\nis the practice of taking the model’s parameters\nθ from the model fθ to initialize another model\ngθ. gθ is then further optimized on a dataset of\n(xsc ,ytc ) instances in the child language pair sc–tc\nuntil it converges to gφ. We assume either sc = sp\nor tc = tp (i.e., child and parent language pairs\nshare one of the source or target sides).\nGranular Notations. It is common practice for\nﬁne-tuning to further update all parent parameters\nθon the child data without making any distinction\nbetween them. We instead consider θ at a more\ngranular level, namely as:\nθ= ⋃{θsrc,θtgt,θenc,θdec,θxattn}\nwhere θsrc includes source-language token embed-\ndings, source positional embeddings, and source\nembeddings layer norm parameters; θtgt similarly\nincludes target-language (tied) input and output to-\nken embeddings, target positional embeddings, and\ntarget embeddings layer norm parameters; θenc in-\ncludes self-attention, layer norm, and feed-forward\nparameters in the encoder stack; θdec includes self-\nattention, layer norm, and feed-forward parame-\nters in the decoder stack; and θxattn includes cross-\nattention and corresponding layer norm parameters.\n2.2 Analysis Setup\nInspections like ours into individual modules of\nTransformer often rely on introducing some con-\nstraints in order to understand the module better.\nThese constraints come in the form of full removal\nor pruning (Tang et al., 2019; V oita et al., 2019),\nhard-coding (You et al., 2020), and freezing (Bo-\ngoychev, 2020). We rely on freezing. We proceed\nby taking pretrained models, freezing certain parts,\nand recording the effect on performance, measured\nby BLEU.\nWithin the framework of our problem, to address\nthe questions raised in §1, our analysis compares\nfull and partially-frozen ﬁne-tuning for MT under\nseveral settings, which we summarize here:\nCross-attention ﬁne-tuning & embedding ﬁne-\ntuning comparative performance. This is to re-\nalize how much ﬁne-tuning the cross-attention lay-\ners helps in addition to ﬁne-tuning respective em-\nbeddings alone.\nCross-attention ﬁne-tuning & full ﬁne-tuning\ncomparative performance. We wish to ﬁnd out\nwhere ﬁne-tuning cross-attention stands relative\nto ﬁne-tuning the entire body. This is to conﬁrm\nwhether or not cross-attention alone can adapt to\nthe child language pair while the encoder and de-\ncoder layers are frozen.\nPretrained cross-attention layers & random\ncross-attention layers. We wish to understand\nhow important a role cross-attention’s pretrained\nvalues play when single-handedly adapting to a\n1757\nnew language pair. This determines if the knowl-\nedge encoded in cross-attention itself has a part in\nits power.\nTranslation cross-attention & language mod-\nelling cross-attention. Finally, we contrast the\nknowledge encoded in cross-attention learned by\ndifferent pretraining objectives. This is to evalu-\nate if the knowledge brought about by a different\npretraining objective affects the patterns observed\nfrom a cross-attention pretrained on MT while ﬁne-\ntuning for MT.\n3 Experimental Setup\nIn this section, we describe our experiments and\nthe data and model that we use to materialize the\nanalysis outlined in §2.2.\n3.1 Methods\nWe ﬁrst provide the details of our transfer setup,\nand then describe the speciﬁc ﬁne-tuning baselines\nand variants used in our experiments.\nGeneral Setup. An important concern when\ntransferring is initializing the embeddings of the\nnew language. When initializing parameters in the\nchild model, there are several ways to address the\nvocabulary mismatch between the parent and the\nchild model: frequency-based assignment, random\nassignment (Zoph et al., 2016), joint (shared) vo-\ncabularies (Nguyen and Chiang, 2017; Kocmi and\nBojar, 2018; Neubig and Hu, 2018; Gheini and\nMay, 2019; Liu et al., 2020), and no assignment at\nall, which results in training randomly initialized\nembeddings (Aji et al., 2020). In our experiments,\nwe choose to always use new random initialization\nfor the new embeddings (including token embed-\ndings, positional embeddings, and corresponding\nlayer norm parameters). This decision is made\nto later let us study what happens to embeddings\nunder each of the settings, independent of any pre-\ntraining artifacts that exist in them. For instance,\nwhen transferring from Fr–En to {Ro–En, Fr–Es},\nrespectively, all parameters are reused except for\n{θsrc, θtgt},3 which get re-initialized given the new\n{source, target} language. The side that remains\nthe same (e.g., En when going from Fr–En to Ro–\nEn), uses the parent vocabulary and keeps the cor-\nresponding embeddings frozen during ﬁne-tuning.4\n3We drop the “respectively” henceforth and use {...}\nthroughout to indicate alternation.\n4Preliminary ablations ﬁne-tuning all embeddings did not\nchange the outcome or conclusions of our experiments.\nTrain Corpus\n(Sent. Count) Test Corpus V ocab. Size\nRo–EnWMT16\n(612.4 K) newstest2016 16 K / reuse tgt\nJa–EnIWSLT17\n(223.1 K) IWSLT17 8 K / reuse tgt\nDe–EnIWSLT16\n(196.9 K) IWSLT16 8 K / reuse tgt\nHa–EnParaCrawl v8\n(159.0 K) newsdev2021 8 K / reuse tgt\nFr–Es News Comm. v15\n(283.5 K) newstest2013 reuse src / 8 K\nFr–DeNews Comm. v15\n(284.1 K) newstest2020 reuse src / 8 K\nTable 1: Data sources and statistics for each of the child\nlanguage pairs.\nFine-tuning Settings. With the general transfer\nsetup, we employ different settings in our exper-\niments to address the points in §2.2. Each ﬁne-\ntuning method is clariﬁed based on our notations\nin §2.1 : 1) {src,tgt} only updates the embed-\ndings {θsrc, θtgt} (Figure 1d). 2) {src,tgt}+body\nadditionally updates the entire Transformer body\n({θsrc, θtgt} + θenc + θdec + θxattn) (Figure 1b). 3)\n{src,tgt}+xattn only updates the cross-attention\nlayers in addition to the ﬁrst baseline ({θsrc, θtgt} +\nθxattn), and keeps the encoder and decoder stacks\nfrozen (Figure 1c, 1e). These collectively ad-\ndress the ﬁrst and second settings in §2.2. 4)\n{src,tgt}+randxattn similarly only updates the\ncross-attention layers in addition to embeddings,\nbut uses randomly initialized values instead of pre-\ntrained values (Figure 1f). This addresses the third\nsetting in §2.2.\nFor all transfer experiments, we also conduct\nthe scratch variant (Figure 1 a), where we train\na model from scratch on the child dataset. This\nis to conﬁrm the effectiveness of transfer under\neach setting. We conduct all the above experi-\nments using a French–English translation model\nas parent and transferring to six different child lan-\nguage pairs. In §4.1 we conduct an ablation that\nsubstitutes mBART (Liu et al., 2020) as a parent.\nmBART is trained with denoising objective in a\nself-supervised manner. In contrast to a transla-\ntion model, the cross-attention layers in mBART\nhave thus not been learned using any parallel data.\nThis enables us to distinguish between different\npretraining objectives, addressing the fourth setting\nin §2.2.\n1758\n3.2 Data and Model Details\nDataset. For the choice of language pairs and\ndatasets, we mostly follow You et al. (2020) (Fr–En,\nRo–En, Ja–En, De–En) and additionally include\nHa–En, Fr–Es, and Fr–De. We designate Fr–En as\nthe parent language pair and Ro–En, Ja–En, De–En,\nHa–En (new source), Fr–Es, Fr–De (new target) as\nchild language pairs. Our Fr–En parent model is\ntrained on the Europarl + Common Crawl subset\nof WMT14 Fr–En, 5 which comprises 5,251,875\nsentences. Details and statistics of the data for the\nchild language pairs are provided in Table 1.\nModel Details. We use the Transformer base ar-\nchitecture (6 layers of encoder and decoder with\nmodel dimension of 512 and 8 attention heads) for\nall models, (Vaswani et al., 2017) and the Fairseq\n(Ott et al., 2019) toolkit for all our experiments.\nAll models rely on BPE subword vocabularies\n(Sennrich et al., 2016) processed through the Sen-\ntencePiece (Kudo and Richardson, 2018) BPE im-\nplementation. The vocabulary for the parent model\nconsists of 32K French subwords on the source\nside, and 32K English subwords on the target side.\nThe sizes of the vocabularies for child models are\nalso reported in Table 1. We follow the advice from\nGowda and May (2020) when deciding what vocab-\nulary size to choose, i.e., we choose the maximum\nnumber of operations to ensure a minimum of 100\ntokens per type.\n4 Results and Analysis\nOur preliminary empirical results consist of\nﬁve experiments for each of the child lan-\nguage pairs based on methods described in\n§3.1: scratch, {src,tgt}, {src,tgt}+body,\n{src,tgt}+xattn, and {src,tgt}+randxattn.\nOur core results, which rely on transferring from\nthe Fr–En parent under each setting, are reported\nin Table 2. All scores are detokenized cased BLEU\ncomputed using SACRE BLEU (Post, 2018).6\n4.1 Cross-attention’s Power and Importance\nTranslation Quality. Table 2 shows that\n{src,tgt}+xattn substantially improves upon\n{src,tgt} in all but one case (Ha–En), especially\nwhen transferring to a pair with a new target lan-\nguage, and is competitive with {src,tgt}+body\n5http://statmt.org/wmt14/translation-task.\nhtml\n6Signature: BLEU+case.mixed+numrefs.1+smooth.exp\n+tok.13a+version.1.4.8.\nacross all six language pairs, suggesting that\ncross-attention is capable of taking advantage\nof encoded generic translation knowledge in\nthe Transformer body to adapt to each child\ntask. Performance gain from {src,tgt} and\ndrop from {src,tgt}+body when changing the\ntarget language (i.e., Fr–Es and Fr–De) are more\npronounced than when transferring the source.\nThis is expected—when changing the target, two\nout of three cross-attention matrices (key and value\nmatrices) are now exposed to a new language.\nWhen transferring source, only the query matrix is\nexposed to the new language.\nStorage. We also report the fraction of the param-\neters that need to be updated in each case. This is\nequivalent to the storage overhead that the training\nprocess incurs, as the updated parameters need to\nbe stored to be used later. However, the parameters\nthat are reused are only stored once. The number\nof parameters updated is dependent on the size of\nthe vocabulary in each experiment, since embed-\ndings for a new vocabulary are included. Hence,\nthe single number reported for each ﬁne-tuning\nstrategy is the average across the six language\npairs. Extending to new language pairs following\n{src,tgt}+xattn is much more efﬁcient in this re-\ngard, as expected. We concretely calculate the num-\nber of parameters that need to be stored combined\nfor the six new language pairs: {src,tgt}+xattn\nstores only 124,430,336 parameters compared to\n{src,tgt}+body’s 313,583,616.\nPretrained and Random Values. Finally,\n{src,tgt}+randxattn experiments also offer\nperspective on the importance of translation\nknowledge encoded in cross-attention itself. Not\nonly does randomly initialized cross-attention fail\nto perform as well as pretrained cross-attention\nwhen being transferred, but in two cases, it even\nfalls behind training from scratch.\nOur results from transferring mBART (Liu et al.,\n2020) to the child language pairs also emphatically\nillustrate the importance of the type of knowledge\nencoded in cross-attention. mBART is a 12-layer\nTransformer pretrained with a denoising objective\nin a self-supervised manner using span masking\nand sentence permutation noising functions. Hence,\nits cross-attention does not have any translation\nknowledge a priori, in contrast with the French–\nEnglish MT parent model. We transfer mBART\nto the same language pairs as in Table 2 and pro-\n1759\nRo–En Ja–En De–En Ha–En Fr–Es Fr–De\nscratch(100%) 29.0 9.2 30.8 5.4 24.4 18.5\n{src,tgt}(8%) 29.8 8.7 32.4 8.6 21.6 11.6\n{src,tgt}+body(75%) 31.0 11.8 36.2 8.8 27.3 21.4\n{src,tgt}+xattn(17%) (-0.1) 30.9 (-2.0) 9.8 (-1.2) 35.0 (-0.4) 8.4 (-0.8) 26.5 (-1.8) 19.6\n{src,tgt}+randxattn(17%) 27.9 8.4 33.3 7.0 26.0 18.8\nTable 2: BLEU scores for each of the ﬁve experiments across six language pairs. Bold numbers indicate the top two\nscoring approaches. Percentages in parentheses next to ﬁne-tuning strategy is the fraction of parameters that had\nto be updated and hence stored as new values for future use. Numbers in parentheses next to {src,tgt}+xattn\nscores show the difference from {src,tgt}+body.\nvide the results in Figure 2. Since mBART uses a\nshared vocabulary and tied embeddings between\nthe encoder and decoder, in Figure 2 we use embed\nin experiments’ names to signify all embeddings\nget updated in the case of mBART (θsrc + θtgt).\nmBART is a larger model than our Fr–\nEn parent, both in terms of architecture and\ntraining data. So a higher range of scores\nis expected. While the same patterns hold\nacross embed+{body,xattn,randxatnn} ﬁne-\ntuning, the crux of the matter is that embed ﬁne-\ntuning fails in contrast to the comparable {src,\ntgt} ﬁne-tuning setting of the translation parent.\nsrc ﬁne-tuning has higher BLEU than scratch\nin three cases (Ro–En, De–En, Ha–En). How-\never, embed ﬁne-tuning has higher BLEU than the\nscratch baseline only in the Ja–En case, and even\nthen, very slightly so (only by 0.1 BLEU). This\nshows that absence of translation knowledge in\nmBART’s pretrained cross-attention leads to its\nﬁne-tuning being more crucial in mBART’s func-\ntionality for translation adaptation: exclusively ﬁne-\ntuning embeddings in mBART simply fails, while\ndoing the same with a translation parent model is\nmore successful.\n4.2 Learned Representations Properties\nGiven that besides cross-attention, embeddings\nare the only parameters that get updated in both\n{src,tgt}+body and {src,tgt}+xattn settings,\nwe take a closer look at them. We want to know\nhow embeddings change under each setting.\nTo probe the relationship between embeddings\nlearned as a result of different kinds of ﬁne-tuning,\nwe examine the quality of induced7 bilingual lex-\nicons, a common practice in cross-lingual embed-\ndings literature (Artetxe et al., 2017) but inciden-\ntally learned in this case.\n7via nearest neighbor retrieval\nWe use the bilingual dictionaries released as a\nresource in the MUSE (Lample et al., 2018) reposi-\ntory.8 For instance, to compare the German embed-\ndings from each of the src+body and src+xattn\nDe–En models to the French embeddings learned\nin the parent model, we use the De–Fr dictionary.\nWe ﬁlter our learned embeddings (which are, in\ngeneral, of subwords) to be compatible with the\nMUSE vocabulary. Of the 8,000 German subwords\nin the vocabulary, 2,025 are found in MUSE. For\neach of these, we ﬁnd the closest French embed-\nding by cosine similarity; if the resulting (German,\nFrench) pair is in MUSE, we consider this a match.\nVia this method, we ﬁnd the accuracy of the bilin-\ngual lexicon induction through the embeddings\nof src+xattn model is 55%. However, the accu-\nracy through the embeddings of src+body is much\nlower at 19.7%. Due to only considering the exact\nmatches against the gold dictionary, this is a very\nstrict evaluation. We also manually look at a sam-\nple of 40 words from the German set and check for\nthe correctness of retrieved pairs for those using\nan automatic translator: while src+xattn scores\nin the range of 80%, src+body scores in the range\nof 30%. Details of this manual inspection are pro-\nvided in Table 4 of the appendix. We further report\nthe accuracy of the bilingual dictionaries of three\nother pairs learned under the two ﬁne-tuning set-\ntings for which gold dictionaries are available in\nFigure 3. We don’t limit ourselves to child-parent\ndictionary induction; we also consider child-child\ndictionary induction (e.g., De–Es) which essen-\ntially relies on both languages being aligned with\nthe parent (i.e., En).\nOverall, these results conﬁrm that embeddings\nlearned under {src,tgt}+xattn effectively get\naligned with corresponding parent embeddings.\nHowever, this is not the case with embeddings\n8https://github.com/facebookresearch/MUSE\n1760\nFigure 2: BLEU scores across different transfer settings using mBART as parent. Exclusive ﬁne-tuning of embed-\ndings (embed) is not effective at all due to lack of translation knowledge in the cross-attention layers.\nFigure 3: Accuracy of bilingual dictionaries induced\nthrough embeddings learned under tgt+body and\ntgt+xattn settings. De and Es effectively get aligned\nwith En under tgt+xattn (left). As they are both\naligned to En, we can also indirectly obtain a De–Es\ndictionary (right). Similar practice completely fails un-\nder tgt+body.\nlearned under {src,tgt}+body. This suggests\nsuch effect is not the default pattern in translation\nmodels, but rather an artifact of the freezing choices\nmade in {src,tgt}+xattn.\n5 Utilities of Aligned Embeddings\nWe saw how ﬁne-tuning only cross-attention results\nin cross-lingual embeddings with respect to parent\nembeddings. That is how cross-attention is able\nto use the baked-in knowledge in the encoder and\ndecoder without any further updates to them. In\nthis section, we discuss two areas where this can\nbe turned to our advantage: mitigating forgetting\nand performing zero-shot translation.\n5.1 Mitigating Forgetting\nOne area where the discovery of §4.2 can be taken\nadvantage of is mitigating catastrophic forgetting.\nCatastrophic forgetting refers to the loss of pre-\nviously acquired knowledge in the model during\ntransfer to a new task. To the best of our knowledge,\ncatastrophic forgetting in MT models has only been\nstudied within the context of inter-domain adapta-\ntion (Thompson et al., 2019; Gu and Feng, 2020),\nand not inter-lingual adaptation.\nThe effectiveness of the cross-lingual embed-\ndings learned under the {src,tgt}+xattn setting\nat mitigating forgetting is evident from the re-\nsults provided in Figure 4. Here we take three\nof the transferred models, plug back in the appro-\npriate embeddings in them, and compare their per-\nformance on the original language pair against\nthe parent model. Speciﬁcally, we take the De–\nEn, Ro–En, and Fr–Es models transferred from\nFr–En under each of the two {src,tgt}+xattn\nand {src,tgt}+body settings , plug in back\nthe original {Fr, En} embeddings, and evalu-\nate performance on the Fr–En test set. This\nscore is then compared against the Fr–En par-\nent model performance on Fr–En test set, which\nscores 35.0 BLEU. While being comparable in\nterms of performance on the child task as reported\nin Table 2, {src,tgt}+xattn constantly outper-\nforms {src,tgt}+body on Fr–En. Compared to\nthe original Fr–En model, the source-transferred\nmodels (De–En, Ro–En) outperform the target-\ntransferred model (Fr–Es). However, tgt+xattn\nis much more robust against forgetting compared\nto tgt+body, which remembers close to nothing\n(0.2 BLEU).\nFigure 4: Performance on the original language pair\nafter transfer. The original Fr–En parent model scores\n35.0 BLEU on the Fr–En test set. {src,tgt}+xattn\noutperforms {src,tgt}+body on the parent task.\n5.2 Zero-Shot Translation\nAnother area where well-aligned embeddings from\nthe {src,tgt}+xattn setting can come in handy\nis zero-shot translation. Since the source embed-\n1761\ndings are aligned, we, for instance, can replace the\nFrench embeddings in the Fr–Es model learned\nvia tgt+xattn with German embeddings from the\nDe–En model learned via src+xattn and form a\nDe–Es translation model with no De–Es training or\ndirect De–Fr alignment. We additionally build two\nmore zero-shot systems in the same manner: Ro–Es\n(using transferred Ro–En and Fr–Es models) and\nRo–De (using transferred Ro–En and Fr–De mod-\nels). To put zero-shot scores in context, for each\npair we also train a model from scratch: for De–\nEs using 294,216-sentence News Commentary v14\ncorpus, and for Ro–Es and Ro–De using 387,653-\nsentence and 385,663-sentence Europarl corpora\nrespectively. All scores are provided in Table 3.\nDe–Es Ro–Es Ro–De\nZero-shot BLEU 9.2 14.7 9.8\nSupervised BLEU 18.3 18.6 13.4\nTable 3: Performance of zero-shot systems for three\nlanguage pairs. De–Es is evaluated on newstest2013\ntest set. Ro–Es and Ro–De are evaluated on respective\nTED talks corpus test sets (Qi et al., 2018).\nIn the case of De–Es, we train two additional\nmodels from scratch on 50,000- and 100,000- sen-\ntence subsets of the training corpus. These re-\nspectively score 7.2 and 12.0 BLEU on the new-\nstest2013 De–Es test set (v.s. zero-shot perfor-\nmance of 9.2). Taken together, these results show\nthat the zero-shot methods we obtain from cross-\nattention-based transfer can yield reasonable trans-\nlation models in the absence of parallel data.\n6 Related Work\nStudying Cross-attention. Several recent works\nconsider the importance of self- and cross-attention\nheads in the Transformer architecture (V oita et al.,\n2019; Michel et al., 2019; You et al., 2020).\nThe consensus among these works is that cross-\nattention heads are relatively more important than\nself-attention heads when it comes to introducing\nrestrictions in terms of pruning and hard-coding.\nModule Freezing. In terms of restrictions intro-\nduced, our work is related to a group of recent\nworks that freeze certain modules while ﬁne-tuning\n(Zoph et al., 2016; Artetxe et al., 2020; Lu et al.,\n2021). Artetxe et al. (2020) conduct their study on\nan encoder-only architecture. They show that by\nfreezing a pretrained English Transformer language\nmodel body and only lexically (embedding layers)\ntransferring it to another language, they can later\nplug in those embeddings into a ﬁne-tuned down-\nstream English model, achieving zero-shot transfer\non the downstream task in the other language. Lu\net al. (2021) also work with a decoder-only archi-\ntecture. They show that by only ﬁne-tuning the\ninput layer, output layer, positional embeddings,\nand layer norm parameters of an otherwise frozen\nTransformer language model, they can match the\nperformance of a model fully trained on the down-\nstream task in several modalities.\nLightweight Fine-tuning. Houlsby et al. (2019)\nreduce the number of parameters to be updated\nby inserting adapter modules in every layer of\nthe Transformer model. Then during ﬁne-tuning,\nthey update the adapter parameters from scratch\nand ﬁne-tune layer norm parameters while keep-\ning the rest of the parameters frozen. Since\nadapters are only inserted and initialized at the\ntime of ﬁne-tuning, they are not able to reveal\nanything about the importance of pretrained mod-\nules. Our approach, however, enables highlight-\ning the crucial role of the encoded translation\nknowledge by contrasting {src,tgt}+xattn and\n{src,tgt}+randxattn. Bapna and Firat (2019)\ndevise adapters for MT by inserting language pair-\nspeciﬁc adapter parameters in the Transformer ar-\nchitecture. In the multilingual setting, they show\nthat by ﬁne-tuning adapters in a shared pretrained\nmultilingual model, they can compensate for the\nperformance drop of high-resource languages in-\ncurred by shared training. Philip et al. (2020) re-\nplace language pair-speciﬁc adapters with mono-\nlingual adapters, which enables adapting under the\nzero-shot setting.\nAnother family of lightweight ﬁne-tuning ap-\nproaches (Li and Liang, 2021; Hambardzumyan\net al., 2021; Lester et al., 2021), inspired by prompt\ntuning (Brown et al., 2020), also relies on updating\na set of additional new parameters from scratch\ntowards each downstream task. Such sets of pa-\nrameters equal a very small fraction of the total\nparameters in the pretrained model. By contrast,\nour approach updates a subset of the model’s own\nparameters instead of adding new ones. We leave\na comparison of the relative advantages and disad-\nvantages of these approaches to future work.\nCross-lingual Embeddings. Finally, while we\nwere able to obtain cross-lingual embeddings\n1762\nthrough our transfer learning approach without\nusing any dictionaries or direct parallel corpora,\nWada et al. (2020) use a direct parallel corpus and\na shared LSTM model that does translation and re-\nconstruction at the same time to obtain aligned em-\nbeddings. Given tremendously large monolingual\ncorpora for embedding construction, cross-lingual\nembeddings can also be obtained by applying a lin-\near transformation on one language’s embedding\nspace to map it to the second one in a way that\nminimizes the distance between equivalents in the\nshared space according to a dictionary (Mikolov\net al., 2013; Xing et al., 2015; Artetxe et al., 2016).\nThese works speciﬁcally targeted the parallel dic-\ntionary reconstruction task, while we used the task\nincidentally, to intrinsically evaluate the parameters\nlearned by our methods.\n7 Conclusion\nWe look at how powerful cross-attention can be un-\nder constrained transfer learning setups. We empiri-\ncally show that cross-attention can single-handedly\nresult in comparable performance with ﬁne-tuning\nthe entire Transformer body, and it is through no\nmagic: it relies on translation knowledge in the\npretrained values to do so and has new embeddings\nalign with corresponding parent language embed-\ndings. We furthermore show that such aligned em-\nbeddings can be used towards catastrophic forget-\nting mitigation and zero-shot transfer. We hope\nthis investigative study encourages more analyses\nin the same spirit towards more insights into the\ninner workings of different modules and how they\ncan be put to good use.\nAcknowledgements\nThe authors would like to thank Kushal Chawla,\nMuhao Chen, Katy Felkner, Thamme Gowda,\nXuezhe Ma, Meryem M 'hamdi, and Xusen Yin\nfor their helpful feedback on the pre-submission\ndraft of this work. This research is based in part\non research sponsored by the Ofﬁce of the Direc-\ntor of National Intelligence (ODNI), Intelligence\nAdvanced Research Projects Activity (IARPA), via\nAFRL Contract FA8650-17-C-9116 and in part on\nresearch sponsored by Air Force Research Labora-\ntory (AFRL) under agreement number FA8750-\n19-1-1000. Ren’s research is supported in part\nby the Ofﬁce of the Director of National Intelli-\ngence (ODNI), Intelligence Advanced Research\nProjects Activity (IARPA), via Contract No. 2019-\n19051600007, the Defense Advanced Research\nProjects Agency with award W911NF-19-20271,\nNSF IIS 2048211, and NSF SMA 182926. The\nviews and conclusions contained herein are those\nof the authors and should not be interpreted as\nnecessarily representing the ofﬁcial policies or en-\ndorsements, either expressed or implied, of the\nODNI, IARPA, Air Force Laboratory, DARPA, or\nthe U.S. Government. The U.S. Government is\nauthorized to reproduce and distribute reprints for\nGovernmental purposes notwithstanding any copy-\nright annotation thereon.\nReferences\nAlham Fikri Aji, Nikolay Bogoychev, Kenneth\nHeaﬁeld, and Rico Sennrich. 2020. In neural ma-\nchine translation, what does transfer learning trans-\nfer? In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics,\npages 7701–7710, Online. Association for Compu-\ntational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.\nLearning principled bilingual mappings of word em-\nbeddings while preserving monolingual invariance.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2289–2294, Austin, Texas. Association for Compu-\ntational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 451–462,\nVancouver, Canada. Association for Computational\nLinguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nAnkur Bapna and Orhan Firat. 2019. Simple, scal-\nable adaptation for neural machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1538–\n1548, Hong Kong, China. Association for Computa-\ntional Linguistics.\nNikolay Bogoychev. 2020. Not all parameters are born\nequal: Attention is mostly what you need. CoRR,\nabs/2010.11859.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\n1763\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on\nLearning Representations.\nMozhdeh Gheini and Jonathan May. 2019. A universal\nparent model for low-resource neural machine trans-\nlation transfer. CoRR, abs/1909.06516.\nIan J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron\nCourville, and Yoshua Bengio. 2014. An empirical\ninvestigation of catastrophic forgeting in gradient-\nbased neural networks. In In Proceedings of Inter-\nnational Conference on Learning Representations\n(ICLR).\nThamme Gowda and Jonathan May. 2020. Finding the\noptimal vocabulary size for neural machine transla-\ntion. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 3955–3964,\nOnline. Association for Computational Linguistics.\nShuhao Gu and Yang Feng. 2020. Investigating catas-\ntrophic forgetting during continual training for neu-\nral machine translation. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 4315–4326, Barcelona, Spain (Online).\nInternational Committee on Computational Linguis-\ntics.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. W ARP: Word-level Adversar-\nial ReProgramming. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 4921–4933, Online. Associa-\ntion for Computational Linguistics.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for nlp.\nIn International Conference on Machine Learning,\npages 2790–2799. PMLR.\nTom Kocmi and Ond ˇrej Bojar. 2018. Trivial transfer\nlearning for low-resource neural machine translation.\nIn Proceedings of the Third Conference on Machine\nTranslation: Research Papers, pages 244–252, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nGuillaume Lample, Alexis Conneau, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In Inter-\nnational Conference on Learning Representations.\nOpenReview.net.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. CoRR, abs/2104.08691.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n4582–4597, Online. Association for Computational\nLinguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\n1764\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mor-\ndatch. 2021. Pretrained transformers as universal\ncomputation engines. CoRR, abs/2103.05247.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nTomás Mikolov, Quoc V . Le, and Ilya Sutskever. 2013.\nExploiting similarities among languages for ma-\nchine translation. CoRR, abs/1309.4168.\nGraham Neubig and Junjie Hu. 2018. Rapid adapta-\ntion of neural machine translation to new languages.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n875–880, Brussels, Belgium. Association for Com-\nputational Linguistics.\nToan Q. Nguyen and David Chiang. 2017. Trans-\nfer learning across low-resource, related languages\nfor neural machine translation. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 296–301, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nJerin Philip, Alexandre Berard, Matthias Gallé, and\nLaurent Besacier. 2020. Monolingual adapters for\nzero-shot neural machine translation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4465–4470, Online. Association for Computational\nLinguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nYe Qi, Devendra Sachan, Matthieu Felix, Sarguna Pad-\nmanabhan, and Graham Neubig. 2018. When and\nwhy are pre-trained word embeddings useful for neu-\nral machine translation? In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers),\npages 529–535, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2019.\nUnderstanding neural machine translation by sim-\npliﬁcation: The case of encoder-free models. In\nProceedings of the International Conference on\nRecent Advances in Natural Language Processing\n(RANLP 2019), pages 1186–1193, Varna, Bulgaria.\nINCOMA Ltd.\nBrian Thompson, Jeremy Gwinnup, Huda Khayrallah,\nKevin Duh, and Philipp Koehn. 2019. Overcoming\ncatastrophic forgetting during domain adaptation of\nneural machine translation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2062–2068, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nTakashi Wada, Tomoharu Iwata, Yuji Matsumoto, Tim-\nothy Baldwin, and Jey Han Lau. 2020. Learning\ncontextualised cross-lingual word embeddings for\nextremely low-resource languages using parallel cor-\npora. CoRR, abs/2010.14649.\nChao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.\nNormalized word embedding and orthogonal trans-\nform for bilingual word translation. In Proceedings\nof the 2015 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1006–1011,\nDenver, Colorado. Association for Computational\nLinguistics.\nWeiqiu You, Simeng Sun, and Mohit Iyyer. 2020.\nHard-coded Gaussian attention for neural machine\ntranslation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7689–7700, Online. Association for Computa-\ntional Linguistics.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1568–1575, Austin,\nTexas. Association for Computational Linguistics.\n1765\nA Manual Bilingual Dictionary\nEvaluation\nGerman Wordsrc+xattn\nFrench Equivalent\nsrc+body\nFrench Equivalent\nEntdeckung découverte amende\nFeind ennemi ennemi\nArchitekten architectes architecture\ngibt existe jette\nerforschen explorer sond\nPhilosoph philosophie philosophie\nCent centi centaines\nformen forme forme\nlassen laissez PCP\nNummer numéro Key\nkönnen puissent puisse\ndasselbe mêmes lourds\ngelöst résoud résoud\nwenig peu peu\nzerstört détruit dévas\nBericht reportage témoin\nMark Mark trailer\nBrief lettre lettres\nLinien lignes lignes\nentworfen conçus monté\nDunkelheit ténèbres obscur\nKreis cercle rond\nHaie requins Hun\nspielt joue tragédie\nElektrizität électricité électriques\nSolar solaire Arabes\nFlügel ailes avion\nKonzept concept alliance\nStrukturen structures déﬁnit\nwill veut voulons\nHier Ici V ous\nverlieren perdent perdent\nunterstützen soutien appui\nPlanet planète planète\nbuchstäblich littéralement multimédia\nSchuld blâ génére\ndass que toi\nplötzlich soudainement risques\nKann Pouvez ciel\nBall ballon ballon\nTable 4: Sampled German words and their equiv-\nalents based on the embeddings learned by each\nof the models. The correct translations are high-\nlighted. Each pair was manually checked for cor-\nrectness using an automatic translator.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8002623319625854
    },
    {
      "name": "Computer science",
      "score": 0.7855173349380493
    },
    {
      "name": "Machine translation",
      "score": 0.77439945936203
    },
    {
      "name": "Limiting",
      "score": 0.6105822324752808
    },
    {
      "name": "Fine-tuning",
      "score": 0.5531613826751709
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5309044122695923
    },
    {
      "name": "Forgetting",
      "score": 0.5157316327095032
    },
    {
      "name": "Translation (biology)",
      "score": 0.49705770611763
    },
    {
      "name": "Scratch",
      "score": 0.4843195974826813
    },
    {
      "name": "Language model",
      "score": 0.4838258624076843
    },
    {
      "name": "Natural language processing",
      "score": 0.40076538920402527
    },
    {
      "name": "Machine learning",
      "score": 0.3507981300354004
    },
    {
      "name": "Voltage",
      "score": 0.13484063744544983
    },
    {
      "name": "Electrical engineering",
      "score": 0.10715016722679138
    },
    {
      "name": "Programming language",
      "score": 0.09416350722312927
    },
    {
      "name": "Engineering",
      "score": 0.0937623679637909
    },
    {
      "name": "Particle physics",
      "score": 0.06906861066818237
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ]
}