{
  "title": "DLRG@DravidianLangTech-ACL2022: Abusive Comment Detection in Tamil using Multilingual Transformer Models",
  "url": "https://openalex.org/W4285297080",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5084918146",
      "name": "R. Rajalakshmi",
      "affiliations": [
        "University of Technology Sydney",
        "Vellore Institute of Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A5075103238",
      "name": "Ankita Duraphe",
      "affiliations": [
        "University of Technology Sydney",
        "Vellore Institute of Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A5033447472",
      "name": "Antonette Shibani",
      "affiliations": [
        "University of Technology Sydney",
        "Vellore Institute of Technology University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3156530738",
    "https://openalex.org/W3155060640",
    "https://openalex.org/W3028350948",
    "https://openalex.org/W4241748643",
    "https://openalex.org/W3156060912",
    "https://openalex.org/W3197959470",
    "https://openalex.org/W2997018275",
    "https://openalex.org/W3083527349",
    "https://openalex.org/W2948463148",
    "https://openalex.org/W3154997565",
    "https://openalex.org/W2963625095",
    "https://openalex.org/W4297801177",
    "https://openalex.org/W4210356903",
    "https://openalex.org/W3047397334",
    "https://openalex.org/W4210579724",
    "https://openalex.org/W4285291115",
    "https://openalex.org/W4287758476",
    "https://openalex.org/W3015597294",
    "https://openalex.org/W4280533528",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3154047021",
    "https://openalex.org/W3127158713",
    "https://openalex.org/W3213648952",
    "https://openalex.org/W2626019506",
    "https://openalex.org/W4285141940",
    "https://openalex.org/W3214539860",
    "https://openalex.org/W3155391396",
    "https://openalex.org/W2169205257",
    "https://openalex.org/W3118477442",
    "https://openalex.org/W3119937365",
    "https://openalex.org/W3099919888",
    "https://openalex.org/W3154746858",
    "https://openalex.org/W2009718190",
    "https://openalex.org/W3154363817",
    "https://openalex.org/W4287026727",
    "https://openalex.org/W2790055280",
    "https://openalex.org/W3155839123",
    "https://openalex.org/W2563351168",
    "https://openalex.org/W3018024187",
    "https://openalex.org/W2808920007"
  ],
  "abstract": "Online Social Network has let people to connect and interact with each other. It does, however, also provide a platform for online abusers to propagate abusive content. The vast majority of abusive remarks are written in a multilingual style, which allows them to easily slip past internet inspection. This paper presents a system developed for the Shared Task on Abusive Comment Detection (Misogyny, Misandry, Homophobia, Transphobic, Xenophobia, CounterSpeech, Hope Speech) in Tamil DravidianLangTech@ACL 2022 to detect the abusive category of each comment. We approach the task with three methodologies - Machine Learning, Deep Learning and Transformer-based modeling, for two sets of data - Tamil and Tamil+English language dataset. The dataset used in our system can be accessed from the competition on CodaLab. For Machine Learning, eight algorithms were implemented, among which Random Forest gave the best result with Tamil+English dataset, with a weighted average F1-score of 0.78. For Deep Learning, Bi-Directional LSTM gave best result with pre-trained word embeddings. In Transformer-based modeling, we used IndicBERT and mBERT with fine-tuning, among which mBERT gave the best result for Tamil dataset with a weighted average F1-score of 0.7.",
  "full_text": "Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages, pages 207 - 213\nMay 26, 2022 ©2022 Association for Computational Linguistics\nDLRG@DravidianLangTech-ACL2022: Abusive Comment Detection in\nTamil using Multilingual Transformer Models\nAnkita Duraphe and Ratnavel Rajalakshmi∗\nSchool of Computer Science and Engineering\nVellore Institute of Technology\nChennai, India\nankitaduraphe@gmail.com\nrajalakshmi.r@vit.ac.in\nAntonette Shibani\nTD School\nUniversity of Technology Sydney\nSydney, Australia\nantonette.shibani@gmail.com\nAbstract\nOnline Social Network has let people connect\nand interact with each other. It does, however,\nalso provide a platform for online abusers to\npropagate abusive content. The majority of\nthese abusive remarks are written in a multi-\nlingual style, which allows them to easily slip\npast internet inspection. This paper presents a\nsystem developed for the Shared Task on Abu-\nsive Comment Detection (Misogyny, Misandry,\nHomophobia, Transphobic, Xenophobia, Coun-\nterSpeech, Hope Speech) in Tamil Dravidi-\nanLangTech@ACL 2022 to detect the abu-\nsive category of each comment. We approach\nthe task with three methodologies - Machine\nLearning, Deep Learning and Transformer-\nbased modeling, for two sets of data - Tamil\nand Tamil+English language dataset. The\ndataset used in our system can be accessed\nfrom the competition on CodaLab. For Ma-\nchine Learning, eight algorithms were imple-\nmented, among which Random Forest gave the\nbest result with Tamil+English dataset, with\na weighted average F1-score of 0.78. For\nDeep Learning, Bi-Directional LSTM gave\nbest result with pre-trained word embeddings.\nIn Transformer-based modeling, we used In-\ndicBERT and mBERT with fine-tuning, among\nwhich mBERT gave the best result for Tamil\ndataset with a weighted average F1-score of\n0.7.\n1 Introduction\nThe usage of the Internet and social media has\nincreased exponentially over the previous two\ndecades, allowing people to connect and interact\nwith each other (Priyadharshini et al., 2021; Ku-\nmaresan et al., 2021). This has resulted in a num-\nber of favourable outcomes such as monitoring\npandemic trends, empowering patients and enhanc-\ning public communication through social media,\namongst others (Cornelius et al., 2020; Househ\n∗Corresponding Author\net al., 2014; Picazo-Vela et al., 2012). At the same\ntime, it has also brought with it hazards and neg-\native consequences, one of which is the use of\nabusive language on others (Chakravarthi, 2020;\nChakravarthi and Muralidaran, 2021).\nThe rapid spread of abusive content on social\nnetworking has become a major source of concern\nfor government organisations. It is very difficult\nto identify abuse over online social network due to\nthe massive volume of content generated through\nsocial media in different online platforms (Sampath\net al., 2022; Ravikiran et al., 2022; Chakravarthi\net al., 2022; Bharathi et al., 2022). It becomes a\nbigger problem when most of the communication\nis in multilingual style (Priyadharshini et al., 2020;\nChakravarthi et al., 2021a,b). Hence, there is in-\ncreasing interest in the use of automated methods\nfor detecting online social abuse (Priyadharshini\net al., 2022). It is becoming a major area of research\nto find solutions with powerful algorithmic systems\nto curb the growth of abusive content online. One\npossible way of achieving such a system is by us-\ning state-of-the-art Natural Language Processing\n(NLP) techniques, which can analyse, comprehend\nand interpret the meaning of the natural language\ndata.\nIn addition, the detection of abusive language on-\nline is harder for some languages like Tamil due to\nthe presence of code-mixed (Barman et al., 2014)\nand code-switched (Poplack, 2001) data. Code-\nswitching is when in a single discourse, a person\nswitches between two or more languages or lan-\nguage varieties/dialects (B and A, 2021b,a). It\nrefers to using elements from more than one lan-\nguage in a way that is consistent with the syntax,\nmorphology, and phonology of each language or\ndialect. Code-mixing is the hybridization of two\nlanguages (for example, parkear, which uses an En-\nglish root word and Spanish morphology), which\nrefers to the migration from one language to an-\nother. Many such language pairs have a hybrid\n207\nname.\nTamil is a member of the southern branch of\nthe Dravidian languages, a group of about 26 lan-\nguages indigenous to the Indian subcontinent. It\nis also classed as a member of the Tamil lan-\nguage family, which contains the languages of\naround 35 ethno-linguistic groups, including the Ir-\nula and Yerukula languages (Anita and Subalalitha,\n2019b,a; Subalalitha and Poovammal, 2018; Subal-\nalitha, 2019). Malayalam is Tamil’s closest signifi-\ncant cousin; the two began splitting during the 9th\ncentury AD. Although several variations between\nTamil and Malayalam indicate a pre-historic break\nof the western dialect, the process of separating\ninto a different language, Malayalam, did not oc-\ncur until the 13th or 14th century (Sakuntharaj and\nMahesan, 2021, 2017, 2016; Thavareesan and Mah-\nesan, 2019, 2020a,b, 2021). Tanglish is an example\nwhich is Tamil+English. In this task, we are given\ntwo datasets: One with a Tamil meaning written in\nEnglish but the content is a combination of Tamil\nand English. The other is a Tamil+English dataset\n(Tanglish) which is written in Tamil and English\nwith content in Tamil and English as well. There\nare also known challenges in the development of\ncomputational systems in Tamil because of the lack\nof linguistic resources (Magueresse et al., 2020).\nIn this paper, we present computational systems\nfor the automated detection of abusive language\nusing the two different data sets containing Tamil\nand Tamil+English.\n2 Related work\nIn this section, we review the various method-\nologies and systems previously implemented for\nsimilar tasks in under-resourced languages like\nTamil. Hope speech is annotated Equality, Diver-\nsity and Inclusion (HopeEDI) (Chakravarthi, 2020).\nThey also created several baselines to standard the\ndataset. (Chakravarthi and Muralidaran, 2021) re-\nports on the shared task of hope speech detection\nfor Tamil, English and Malyalam languages. They\npresents the dataset used in the shared task and\nalso surveys various competing approaches devel-\noped for the shared task and their corresponding\nresults. (Mandalam and Sharma, 2021) presents\nthe methodologies implemented while classifying\nDravidian Tamil and Malayalam code-mixed com-\nments according to their polarity and uses LSTM\narchitecture. (Sai and Sharma, 2021; Li, 2021;\nQue, 2021) use XLM-RoBERTa for offensive lan-\nguage identification. Novel approach of selective\ntranslation and transliteration have been used to im-\nprove the performance of multilingual transformer\nnetworks such as XLMRoBERTa and mBERT by\nfine-tuning and ensembling. Online messaging has\nbecome one of the most popular methods of com-\nmunication with instances of online/digital bully-\ning. The challenge of detecting objectionable lan-\nguage in YouTube comments from the Dravidian\nlanguages of Tamil, Malayalam, and Kannada is\nviewed as a multi-class classification problem (An-\ndrew, 2021). Several Machine Learning algorithms\nhave been trained for the task at hand after being\nexposed to language-specific pre-processing.\n3 Dataset\nThe dataset for the current study is taken from\nthe competition 1 which consists of YouTube\ncomments in Tamil and Tamil-English languages\nannotated for Misogyny, homophobia, transpho-\nbic, xenophobia, counter-speech, hope-speech and\nmisandry (and None-of-the-above) (Priyadharshini\net al., 2022). Table 1 shows the count of comments\nfor both the datasets under each split. Table 2 gives\nthe class-distribution of each abusive category for\nboth the datasets.\n4 Proposed Technique\nRaw texts are inaccessible to Machine Learning\n(ML) and Deep Learning (DL) algorithms. To train\nthe models for classification, feature extraction is\nnecessary. To extract features in ML approaches,\nthe TF-IDF representation is used. For DL models,\nwe use fastText word embeddings feature extrac-\ntion strategies (Joulin et al., 2016). fastText em-\nbedding uses a pre-trained embedding matrix for\nTamil language (Grave et al., 2018). To study the\nresults and come up with the best model possible,\nwe follow three approaches - Machine Learning,\nDeep Learning and Transformer-based.\nAs it can be clearly seen from Table 1, both the\ndatasets contain class imbalance. Class imbalance\nis a problem in machine learning when there are\ngreat differences in the class-distribution of the\ndataset. It is seen as a problem when a dataset is\nbiased towards a class in the dataset. If this problem\npersists, any algorithm trained on the same data will\nagain be biased towards the same class. To resolve\n1https://competitions.codalab.org/\ncompetitions/36403\n208\nClass Tamil+English Tamil\nTrain-set 5948 2238\nValidation-set 1488 560\nTest-set 1857 699\nTable 1: Number of comments across both the datasets in each of the three splits.\nClass Tamil+English Tamil\nMisandry 1048 550\nCounter-speech 443 185\nXenophobia 367 124\nHope-Speech 266 97\nMisogyny 261 149\nHomophobia 213 43\nTransphobic 197 8\nNone-of-the-above 4639 1642\nTable 2: Class-distribution across both datasets.\nthe issue of class imbalance, we practice various\napproaches:\nChanging the performance metric: Since accu-\nracy is not always the best metric to use on imbal-\nanced datasets, we use F1-score instead to evaluate\nthe models.\nUsing a penalized algorithm (cost-sensitive train-\ning): This algorithm also handles class imbalance\nwhich can be achieved by using ’balanced’ as a\nparameter while computing class weights.\nChanging the algorithms: This is why we have\nused a wide variety of algorithms to get a bigger\npicture of which models suit the dataset and the\nclassification problem better.\nTable 3 provides the details about tuning\nthe hyperparameters in our system both for\nTamil+English and Tamil datasets.\nTo study the results and come up with the best\nmodel possible, we follow three approaches - Ma-\nchine Learning, Deep Learning and Transformer-\nbased, described in the sub-sections below.\n4.1 Approach A: Machine Learning/\nNon-Neural Network approaches\nTo start with, we implemented various Machine\nLearning algorithms which include Logistic Re-\ngression (LR), Random Forest (RF), K-nearest\nneighbors (KNN), Decision Tree, Support Vec-\ntor Machine (SVM), Gradient Boosting, Adap-\ntive Boosting (AdaBoost), and Ensemble (Husain,\n2020). We have used ML algorithms only for\nTamil+English dataset due to the poor performance\nof ML models on Tamil written text (Tamil dataset).\n4.2 Approach B: Recurrent Neural Network\napproaches\nTo improve the performance of ML models, we\ndive into deep learning algorithms. Here, we have\nimplemented DL approach for both the datasets.\nWe use two models of Bi-directional LSTM -\nBiLSTM-M1 and BiLSTM-M2 (Chiu and Nichols,\n2015). BiLSTM-M1 is a mix of bidirectional\nLSTM architecture that uses a convolution and a\nmax-pooling layer to extract a new feature vec-\ntor from the per-character feature vectors for each\nword. These vectors are concatenated for each\nword and sent to the BiLSTM network, which sub-\nsequently feeds the output layers. BiLSTM-M2\nis an advanced BiLSTM-M1 where we adopted\npre-trained word embeddings since BiLSTM and\nfastText produced better results for classification\ntasks.\n4.3 Approach C: Transformer-based\napproaches\nIn natural language processing, the Transformer\nis a unique design that seeks to solve sequence-\nto-sequence tasks while also resolving long-range\ndependencies. It does not use sequence-aligned\nRNNs or convolution to compute representations\nof its input and output, instead relying solely on\nself-attention.\nBidirectional Encoder Representations from\nTransformers (BERT) (Devlin et al., 2018) is a\n209\nParameters Values\nLearning rate 1x10 −3\nBatch Size 32\nEpochs 25\nValidation Split 0.2\nTable 3: Hyperparameters used in our system.\nModel name P R F1\nRF 0.91 0.71 0.78\nGradient Boosting 0.85 0.71 0.76\nSVM 0.78 0.72 0.75\nKNN 0.85 0.68 0.75\nAdaBoost 0.86 0.69 0.74\nLR 0.71 0.71 0.71\nDecision Tree 0.72 0.66 0.68\nEnsemble 0.71 0.72 0.68\nBiLSTM-M1 0.71 0.68 0.7\nBiLSTM-M2 0.64 0.61 0.62\nIndicBERT 0.55 0.67 0.60\nTable 4: Metric evaluation for Tamil+English dataset.\nModel name P R F1\nBiLSTM-M1 0.63 0.55 0.58\nBiLSTM-M2 0.74 0.67 0.7\nmBERT 0.64 0.7 0.7\nTable 5: Metric evaluation for Tamil dataset.\ntransformer language model with a variable num-\nber of encoder layers and self-attention capabilities.\nWe again use two BERT models - mBERT (bert-\nbase-multilingual-cased) and IndicBERT\nWe follow fine-tuning for Transformer models\nand use pre-trained BERT, bert-base-multilingual-\ncased (Devlin et al., 2018) and IndicBert classi-\nfication models (Kakwani et al., 2020) that have\nbeen trained on 104 languages and 12 Indian lan-\nguages respectively, including Tamil, from the\nlargest Wikipedia.\n5 Results and Discussion\nWe ran 8 Machine Learning algorithms, 2\nDeep Learning and 1 Transformer model on the\nTamil+English dataset. For the Tamil dataset, we\nused 2 Deep Learning and 1 Transformer model.\nFor the Tamil+English dataset, the best perfor-\nmance was of Random Forest with macro average\nF1-score of 0.32 and weighted average F1-score\nof 0.78. For the Tamil dataset, the best model was\nBiLSTM-M2 with macro average F1-score of 0.39\nand weighted average F1-score of 0.70.\nFor Tamil, performance improved from\nswitching BiLSTM-M2 to mBERT. And for\nTamil+English, the best performer was BiLSTM-\nM1, followed by BiLSTM-M2 and then IndicBERT\nand mBERT.\nTable 4 and Table 5 show the result of our mod-\nels across both the datasets. For Tamil language,\nML models performed best when DL models were\noriginally expected to perform better. The exten-\nsive use of multilingual language in the text could\nbe a reason for the poor performance of DL. Pre-\ntrained word embeddings could not deliver higher\nperformance due to the lack of feature mapping be-\ntween the words. As a result, DL models might not\nbe able to uncover sufficient relational relationships\namong the features, and perform poorly.\n6 Conclusions and Future Work\nIn this paper, we presented approaches for the auto-\nmated detection of abusive comments in Tamil. We\nused various models to do a comparative study to\nsee which model performed better with the dataset\ngiven in the shared task. We found that Deep\nLearning and Transformer models outperformed\nMachine Learning models with Tamil data whereas\nMachine Learning models achieved better results\nthan Deep Learning and Transformer-based for\nTamil+English data. We did not apply contextual-\nized embeddings (such as ELMO, FLAIR) which\nmay improve the performance of the system. Im-\nplementation of Contextualised embeddings using\nlanguage modelling with deep learning is the future\nwork to explore.\nAcknowledgements\nWe would like to thank the management of Vellore\nInstitute of Technology, Chennai for their support\nto carry out this research.\n210\nReferences\nJudith Jeyafreeda Andrew. 2021.\nJudithJeyafreedaAndrew@DravidianLangTech-\nEACL2021:offensive language detection for\nDravidian code-mixed YouTube comments. In\nProceedings of the First Workshop on Speech and\nLanguage Technologies for Dravidian Languages ,\npages 169–174, Kyiv. Association for Computational\nLinguistics.\nR Anita and CN Subalalitha. 2019a. An approach to\ncluster Tamil literatures using discourse connectives.\nIn 2019 IEEE 1st International Conference on En-\nergy, Systems and Information Processing (ICESIP),\npages 1–4. IEEE.\nR Anita and CN Subalalitha. 2019b. Building discourse\nparser for Thirukkural. In Proceedings of the 16th\nInternational Conference on Natural Language Pro-\ncessing, pages 18–25.\nBharathi B and Agnusimmaculate Silvia A. 2021a.\nSSNCSE_NLP@DravidianLangTech-EACL2021:\nMeme classification for Tamil using machine\nlearning approach. In Proceedings of the First\nWorkshop on Speech and Language Technologies\nfor Dravidian Languages , pages 336–339, Kyiv.\nAssociation for Computational Linguistics.\nBharathi B and Agnusimmaculate Silvia A. 2021b.\nSSNCSE_NLP@DravidianLangTech-EACL2021:\nOffensive language identification on multilingual\ncode mixing text. In Proceedings of the First\nWorkshop on Speech and Language Technologies\nfor Dravidian Languages , pages 313–318, Kyiv.\nAssociation for Computational Linguistics.\nUtsab Barman, Amitava Das, Joachim Wagner, and\nJennifer Foster. 2014. Code mixing: A challenge\nfor language identification in the language of social\nmedia.\nB Bharathi, Bharathi Raja Chakravarthi, Subalalitha\nChinnaudayar Navaneethakrishnan, N Sripriya,\nArunaggiri Pandian, and Swetha Valli. 2022. Find-\nings of the shared task on Speech Recognition for\nVulnerable Individuals in Tamil. In Proceedings of\nthe Second Workshop on Language Technology for\nEquality, Diversity and Inclusion . Association for\nComputational Linguistics.\nBharathi Raja Chakravarthi. 2020. HopeEDI: A mul-\ntilingual hope speech detection dataset for equality,\ndiversity, and inclusion. In Proceedings of the Third\nWorkshop on Computational Modeling of People’s\nOpinions, Personality, and Emotion’s in Social Me-\ndia, pages 41–53, Barcelona, Spain (Online). Associ-\nation for Computational Linguistics.\nBharathi Raja Chakravarthi and Vigneshwaran Mural-\nidaran. 2021. Findings of the shared task on hope\nspeech detection for equality, diversity, and inclu-\nsion. In Proceedings of the First Workshop on Lan-\nguage Technology for Equality, Diversity and Inclu-\nsion, pages 61–72, Kyiv. Association for Computa-\ntional Linguistics.\nBharathi Raja Chakravarthi, Ruba Priyadharshini, Then-\nmozhi Durairaj, John Phillip McCrae, Paul Buitaleer,\nPrasanna Kumar Kumaresan, and Rahul Ponnusamy.\n2022. Findings of the shared task on Homophobia\nTransphobia Detection in Social Media Comments.\nIn Proceedings of the Second Workshop on Language\nTechnology for Equality, Diversity and Inclusion. As-\nsociation for Computational Linguistics.\nBharathi Raja Chakravarthi, Ruba Priyadharshini,\nRahul Ponnusamy, Prasanna Kumar Kumaresan,\nKayalvizhi Sampath, Durairaj Thenmozhi, Sathi-\nyaraj Thangasamy, Rajendran Nallathambi, and\nJohn Phillip McCrae. 2021a. Dataset for identi-\nfication of homophobia and transophobia in mul-\ntilingual YouTube comments. arXiv preprint\narXiv:2109.00227.\nBharathi Raja Chakravarthi, Priya Rani, Mihael Arcan,\nand John P McCrae. 2021b. A survey of orthographic\ninformation in machine translation. SN Computer\nScience, 2(4):1–19.\nJason P. C. Chiu and Eric Nichols. 2015. Named en-\ntity recognition with bidirectional lstm-cnns. CoRR,\nabs/1511.08308.\nJoseph Cornelius, Tilia Ellendorff, Lenz Furrer, and\nFabio Rinaldi. 2020. COVID-19 Twitter moni-\ntor: Aggregating and visualizing COVID-19 related\ntrends in social media. In Proceedings of the Fifth\nSocial Media Mining for Health Applications Work-\nshop & Shared Task, pages 1–10, Barcelona, Spain\n(Online). Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nAvishek Garain, Atanu Mandal, and Sudip Ku-\nmar Naskar. 2021. JUNLP@DravidianLangTech-\nEACL2021: Offensive language identification in Dra-\nvidian langauges. In Proceedings of the First Work-\nshop on Speech and Language Technologies for Dra-\nvidian Languages, pages 319–322, Kyiv. Association\nfor Computational Linguistics.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018), Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nAdeep Hande, Ruba Priyadharshini, Anbukkarasi Sam-\npath, Kingston Pal Thamburaj, Prabakaran Chandran,\nand Bharathi Raja Chakravarthi. 2021. Hope speech\ndetection in under-resourced kannada language.\nMowafa Househ, Elizabeth Borycki, and Andre Kush-\nniruk. 2014. Empowering patients through social\nmedia: the benefits and challenges. Health Informat-\nics J., 20(1):50–58.\n211\nFatemah Husain. 2020. Arabic offensive language de-\ntection using machine learning and ensemble ma-\nchine learning approaches. CoRR, abs/2005.08946.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, Hérve Jégou, and Tomas Mikolov.\n2016. Fasttext.zip: Compressing text classification\nmodels. arXiv preprint arXiv:1612.03651.\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.\nKhapra, and Pratyush Kumar. 2020. IndicNLPSuite:\nMonolingual Corpora, Evaluation Benchmarks and\nPre-trained Multilingual Language Models for Indian\nLanguages. In Findings of EMNLP.\nPrasanna Kumar Kumaresan, Ratnasingam Sakuntharaj,\nSajeetha Thavareesan, Subalalitha Navaneethakr-\nishnan, Anand Kumar Madasamy, Bharathi Raja\nChakravarthi, and John P McCrae. 2021. Findings\nof shared task on offensive language identification\nin Tamil and Malayalam. In Forum for Information\nRetrieval Evaluation, pages 16–18.\nZichao Li. 2021. Codewithzichao@DravidianLangTech-\nEACL2021: Exploring multilingual transformers for\noffensive language identification on code mixing text.\nIn Proceedings of the First Workshop on Speech and\nLanguage Technologies for Dravidian Languages ,\npages 164–168, Kyiv. Association for Computational\nLinguistics.\nAlexandre Magueresse, Vincent Carles, and Evan\nHeetderks. 2020. Low-resource languages: A re-\nview of past work and future challenges. CoRR,\nabs/2006.07264.\nAsrita Venkata Mandalam and Yashvardhan Sharma.\n2021. Sentiment analysis of Dravidian code mixed\ndata. In Proceedings of the First Workshop on\nSpeech and Language Technologies for Dravidian\nLanguages, pages 46–54, Kyiv. Association for Com-\nputational Linguistics.\nSergio Picazo-Vela, Isis Gutiérrez-Martínez, and\nLuis Felipe Luna-Reyes. 2012. Understanding risks,\nbenefits, and strategic alternatives of social media\napplications in the public sector. Gov. Inf. Q. ,\n29(4):504–511.\nShana Poplack. 2001. Code Switching: Linguistic ,\npages 2062–2065.\nRuba Priyadharshini, Bharathi Raja Chakravarthi, Sub-\nalalitha Chinnaudayar Navaneethakrishnan, Then-\nmozhi Durairaj, Malliga Subramanian, Kogila-\nvani Shanmugavadivel, Siddhanth U Hegde, and\nPrasanna Kumar Kumaresan. 2022. Findings of\nthe shared task on Abusive Comment Detection in\nTamil. In Proceedings of the Second Workshop on\nSpeech and Language Technologies for Dravidian\nLanguages. Association for Computational Linguis-\ntics.\nRuba Priyadharshini, Bharathi Raja Chakravarthi,\nSajeetha Thavareesan, Dhivya Chinnappa, Durairaj\nThenmozhi, and Rahul Ponnusamy. 2021. Overview\nof the DravidianCodeMix 2021 shared task on senti-\nment detection in Tamil, Malayalam, and Kannada.\nIn Forum for Information Retrieval Evaluation, pages\n4–6.\nRuba Priyadharshini, Bharathi Raja Chakravarthi, Mani\nVegupatti, and John P McCrae. 2020. Named entity\nrecognition for code-mixed Indian corpus using meta\nembedding. In 2020 6th international conference\non advanced computing and communication systems\n(ICACCS), pages 68–72. IEEE.\nQinyu Que. 2021. Simon @ DravidianLangTech-\nEACL2021: Detecting offensive content in Kannada\nlanguage. In Proceedings of the First Workshop on\nSpeech and Language Technologies for Dravidian\nLanguages, pages 160–163, Kyiv. Association for\nComputational Linguistics.\nManikandan Ravikiran, Bharathi Raja Chakravarthi,\nAnand Kumar Madasamy, Sangeetha Sivanesan, Rat-\nnavel Rajalakshmi, Sajeetha Thavareesan, Rahul Pon-\nnusamy, and Shankar Mahadevan. 2022. Findings\nof the shared task on Offensive Span Identification\nin code-mixed Tamil-English comments. In Pro-\nceedings of the Second Workshop on Speech and\nLanguage Technologies for Dravidian Languages .\nAssociation for Computational Linguistics.\nSiva Sai and Yashvardhan Sharma. 2021. Towards offen-\nsive language identification for Dravidian languages.\nIn Proceedings of the First Workshop on Speech and\nLanguage Technologies for Dravidian Languages ,\npages 18–27, Kyiv. Association for Computational\nLinguistics.\nRatnasingam Sakuntharaj and Sinnathamby Mahesan.\n2016. A novel hybrid approach to detect and correct\nspelling in Tamil text. In 2016 IEEE International\nConference on Information and Automation for Sus-\ntainability (ICIAfS), pages 1–6.\nRatnasingam Sakuntharaj and Sinnathamby Mahesan.\n2017. Use of a novel hash-table for speeding-up\nsuggestions for misspelt tamil words. In 2017 IEEE\nInternational Conference on Industrial and Informa-\ntion Systems (ICIIS), pages 1–5.\nRatnasingam Sakuntharaj and Sinnathamby Mahesan.\n2021. Missing word detection and correction based\non context of Tamil sentences using n-grams. In\n2021 10th International Conference on Information\nand Automation for Sustainability (ICIAfS) , pages\n42–47.\nAnbukkarasi Sampath, Thenmozhi Durairaj,\nBharathi Raja Chakravarthi, Ruba Priyadharshini,\nSubalalitha Chinnaudayar Navaneethakrishnan,\nKogilavani Shanmugavadivel, Sajeetha Thavareesan,\nSathiyaraj Thangasamy, Parameswari Krishnamurthy,\nAdeep Hande, Sean Benhur, and Santhiya Pon-\nnusamy, Kishor Kumar Pandiyan. 2022. Findings of\n212\nthe shared task on Emotion Analysis in Tamil. In\nProceedings of the Second Workshop on Speech and\nLanguage Technologies for Dravidian Languages .\nAssociation for Computational Linguistics.\nC. N. Subalalitha. 2019. Information extraction frame-\nwork for Kurunthogai. S¯adhan¯a, 44(7):156.\nCN Subalalitha and E Poovammal. 2018. Automatic\nbilingual dictionary construction for Tirukural. Ap-\nplied Artificial Intelligence, 32(6):558–567.\nSajeetha Thavareesan and Sinnathamby Mahesan. 2019.\nSentiment analysis in Tamil texts: A study on ma-\nchine learning techniques and feature representation.\nIn 2019 14th Conference on Industrial and Informa-\ntion Systems (ICIIS), pages 320–325.\nSajeetha Thavareesan and Sinnathamby Mahesan.\n2020a. Sentiment lexicon expansion using Word2vec\nand fastText for sentiment prediction in Tamil texts.\nIn 2020 Moratuwa Engineering Research Conference\n(MERCon), pages 272–276.\nSajeetha Thavareesan and Sinnathamby Mahesan.\n2020b. Word embedding-based part of speech tag-\nging in Tamil texts. In 2020 IEEE 15th International\nConference on Industrial and Information Systems\n(ICIIS), pages 478–482.\nSajeetha Thavareesan and Sinnathamby Mahesan. 2021.\nSentiment analysis in Tamil texts using k-means and\nk-nearest neighbour. In 2021 10th International Con-\nference on Information and Automation for Sustain-\nability (ICIAfS), pages 48–53.\n213",
  "topic": "Tamil",
  "concepts": [
    {
      "name": "Tamil",
      "score": 0.9208173751831055
    },
    {
      "name": "Computer science",
      "score": 0.6354150772094727
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6041738390922546
    },
    {
      "name": "Deep learning",
      "score": 0.5434631705284119
    },
    {
      "name": "Transformer",
      "score": 0.5185799598693848
    },
    {
      "name": "Machine learning",
      "score": 0.508766233921051
    },
    {
      "name": "The Internet",
      "score": 0.47375208139419556
    },
    {
      "name": "Natural language processing",
      "score": 0.468666672706604
    },
    {
      "name": "Random forest",
      "score": 0.4642207622528076
    },
    {
      "name": "Speech recognition",
      "score": 0.33498600125312805
    },
    {
      "name": "World Wide Web",
      "score": 0.1841285228729248
    },
    {
      "name": "Engineering",
      "score": 0.13230681419372559
    },
    {
      "name": "Linguistics",
      "score": 0.12322995066642761
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "cited_by": 10
}