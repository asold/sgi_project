{
  "title": "Learning Attention from Attention: Efficient Self-Refinement Transformer for Face Super-Resolution",
  "url": "https://openalex.org/W4385764582",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5103073798",
      "name": "Guanxin Li",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5064150384",
      "name": "Jingang Shi",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5027316177",
      "name": "Yuan Zong",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A5100455958",
      "name": "Fei Wang",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5100428385",
      "name": "Tian Wang",
      "affiliations": [
        "Beihang University",
        "Institute of Art"
      ]
    },
    {
      "id": "https://openalex.org/A5100687952",
      "name": "Yihong Gong",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6791858558",
    "https://openalex.org/W6863994431",
    "https://openalex.org/W2774812616",
    "https://openalex.org/W2738472810",
    "https://openalex.org/W2772824517",
    "https://openalex.org/W3109016549",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2776107444",
    "https://openalex.org/W1796263212",
    "https://openalex.org/W6803771590",
    "https://openalex.org/W3207299541",
    "https://openalex.org/W3196057788",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3013623108",
    "https://openalex.org/W6839759065",
    "https://openalex.org/W6758774750",
    "https://openalex.org/W4285599866",
    "https://openalex.org/W4297435073",
    "https://openalex.org/W6864014924",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6863631769",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2341528187",
    "https://openalex.org/W2963583792",
    "https://openalex.org/W2913036400",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W2963393566",
    "https://openalex.org/W4295725306",
    "https://openalex.org/W3035605421",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W3101998545",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1834627138",
    "https://openalex.org/W3111002277",
    "https://openalex.org/W2962785568",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W2963676087",
    "https://openalex.org/W4293680532",
    "https://openalex.org/W2964167901",
    "https://openalex.org/W3169612303",
    "https://openalex.org/W4306167924"
  ],
  "abstract": "Recently, Transformer-based architecture has been introduced into face super-resolution task due to its advantage in capturing long-range dependencies. However, these approaches tend to integrate global information in a large searching region, which neglect to focus on the most relevant information and induce blurry effect by the irrelevant textures. Some improved methods simply constrain self-attention in a local window to suppress the useless information. But it also limits the capability of recovering high-frequency details when flat areas dominate the local searching window. To improve the above issues, we propose a novel self-refinement mechanism which could adaptively achieve texture-aware reconstruction in a coarse-to-fine procedure. Generally, the primary self-attention is first conducted to reconstruct the coarse-grained textures and detect the fine-grained regions required further compensation. Then, region selection attention is performed to refine the textures on these key regions. Since self-attention considers the channel information on tokens equally, we employ a dual-branch feature integration module to privilege the important channels in feature extraction. Furthermore, we design the wavelet fusion module which integrate shallow-layer structure and deep-layer detailed feature to recover realistic face images in frequency domain. Extensive experiments demonstrate the effectiveness on a variety of datasets.",
  "full_text": "Learning Attention from Attention: Efficient Self-Refinement Transformer for\nFace Super-Resolution\nGuanxin Li1 , Jingang Shi1∗ , Yuan Zong2 , Fei Wang1 , Tian Wang3 , Yihong Gong1\n1School of Software Engineering, Xi’an Jiaotong University\n2Key Laboratory of Child Development and Learning Science, Southeast University\n3Institute of Artificial Intelligence, Beihang University\n{liguanxin, jingang, feynmanw, ygong}@xjtu.edu.cn,\nxhzongyuan@seu.edu.cn, wangtian@buaa.edu.cn\nAbstract\nRecently, Transformer-based architecture has been\nintroduced into face super-resolution task due to\nits advantage in capturing long-range dependen-\ncies. However, these approaches tend to inte-\ngrate global information in a large searching re-\ngion, which neglect to focus on the most relevant\ninformation and induce blurry effect by the irrel-\nevant textures. Some improved methods simply\nconstrain self-attention in a local window to sup-\npress the useless information. But it also lim-\nits the capability of recovering high-frequency de-\ntails when flat areas dominate the local search-\ning window. To improve the above issues, we\npropose a novel self-refinement mechanism which\ncould adaptively achieve texture-aware reconstruc-\ntion in a coarse-to-fine procedure. Generally, the\nprimary self-attention is first conducted to recon-\nstruct the coarse-grained textures and detect the\nfine-grained regions required further compensation.\nThen, region selection attention is performed to re-\nfine the textures on these key regions. Since self-\nattention considers the channel information on to-\nkens equally, we employ a dual-branch feature inte-\ngration module to privilege the important channels\nin feature extraction. Furthermore, we design the\nwavelet fusion module which integrates shallow-\nlayer structure and deep-layer detailed feature to\nrecover realistic face images in frequency domain.\nExtensive experiments demonstrate the effective-\nness on a variety of datasets. The code is released at\nhttps://github.com/Guanxin-Li/LAA-Transformer.\n1 Introduction\nFace Super-resolution (FSR) is a specific super-resolution\nproblem which needs to consider the unique textures on the\nface, such as eyes, mouth, nose, etc. The reconstruction of\nthese structures are critical to distinguishing identity infor-\nmation. In recent years, FSR technology has been widely\nused and has attracted much attention. With the development\n∗Corresponding author\nof Convolutional Neural Networks (CNN), many researchers\nhave designed various CNN networks [Huang et al., 2017;\nZhang et al., 2018a; Zhang et al., 2018c; Shi and Zhao, 2019;\nLi et al., 2021 ] to improve FSR performance. Moreover,\nface priors, such as facial landmarks and heatmaps, were\nalso incorporated into some methods [Chen et al., 2018;\nBulat and Tzimiropoulos, 2018 ] in order to improve global\nface contour recovery.\nThe CNN-based network has limited receptive field since\nconvolution is a local operation. Due to the symmetry of the\nface, we also need to establish long-term dependencies in the\nFSR task in order to reconstruct the complex details on facial\ncomponents. Different from CNN, Transformer [Vaswani et\nal., 2017 ] proposes a self-attention mechanism to establish\nglobal dependencies. In the field of computer vision, the pi-\noneering work ViT [Dosovitskiy et al., 2021 ] separates the\nimage into patches of equal size and then calculates the self-\nattention between the patches in the whole image, yielding\nbetter results compared to CNN in the classification task. Fol-\nlowing this, various vision Transformers [Wang et al., 2021;\nChu et al., 2021; Yuanet al., 2021; Liuet al., 2021] were pro-\nposed to address different visual tasks. Subsequently, Trans-\nformers were also introduced into the SR task. Generally, re-\ncent Transformers mainly conduct self-attention in the global\nsearching region or pre-defined local window. However, it\nmay encounter practical difficulties when deals with FSR.\nFor the self-attention performed on global image region, the\nreconstructed texture details are calculated by the combina-\ntion of all input tokens, which fails to focus the attention on\nthe most relevant ones. The integration of irrelevant textures\ncould cause blurry artifacts on the reconstructed results. For\nself-attention conducted on local window, it may fail to pro-\nduce high-frequency details on complex facial components\n(e.g., eyes) if flat textures dominate the pre-defined rectangu-\nlar window.\nTo solve these problems, we propose an efficient self-\nrefinement mechanism for Transformer, called Region Selec-\ntion Attention (RSA), which first produces the coarse atten-\ntion map for conducting self-attention and then learns fine-\ngrained attention map from the coarse one for further refine-\nment. In the coarse-grained self-attention, the attention map\nis calculated on the down-sampled scale, which is effective\nfor reconstructing flat facial regions and has the advantage\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1035\nof saving computational cost. To enhance the features of key\npatches in the coarse attention map, we adaptively divide sev-\neral regions with the strongest attention as refined searching\nfield to calculate fine-grained self-attention. In this way, we\ncould further restore the detailed features to compensate the\nreconstructed coarse textures on the key regions. The advan-\ntages of the proposed RSA can be summarized in two aspects.\nFirst, it conducts a content-aware feature reconstruction that\ntreats the coarse structure and detailed texture in different\nmanners. Second, it could explore fine-grained self-attention\nin the receptive field with irregular shape, which is more ro-\nbust compared to the traditional rectangular regions.\nAnother drawback in self-attention is that the operation\ntakes the channel information of tokens equally without con-\nsidering the importance. To privilege the important channels\nin feature extraction, we propose Feature Integration Module\n(FIM) which consists of alternating channel attention module\nand depth-wise convolution in dual-branch. It could promote\na further step to achieve cross-spatial and cross-channel inte-\ngration simultaneously in Transformer.\nFurthermore, we design the Wavelet Fusion Module\n(WFM) which could modulate the global facial structure in-\nformation and local detailed texture in frequency-domain. As\nwe know, the shallow layer in the deep network contains the\nstructural information (e.g., facial contour) while the deep\nlayer has the advantage of extracting complex local details\n(e.g., eyelids). Different from previous methods which con-\nduct simple concatenation or summation in the temporal do-\nmain, the proposed WFM achieves reconstruction by explor-\ning the frequency property. We employ Wavelet Transforma-\ntion (WT) to separate low-frequency and middle-frequency\nparts from shallow-layer features, while obtaining the high-\nfrequency parts from deep-layer features. The modulation of\nfrequency-specific feature maps is then conducted in WFM\nfor better restoration.\nOverall, our main contributions are summarized as follows:\n• We propose an efficient self-refinement Transformer-\nbased architecture for FSR task. It could adaptively\nconduct texture-aware reconstruction in a coarse-to-fine\nmanner.\n• The Feature Integration Module (FIM) is employed to\nconsider cross-channel difference in Transformer, which\npromotes the integration of spatial-wise and channel-\nwise information for feature extraction.\n• We design the Wavelet Fusion Module (WFM) to\nachieve the modulation of shallow-layer and deep-layer\nfeatures through frequency decomposition and recombi-\nnation.\n• Our method achieves state-of-the-art quantitative met-\nrics and visualizations. It obtains the advantages of more\nthan 0.32dB for PSNR values on the Helen datasets.\n2 Related Works\n2.1 Face Super-Resolution\nIn recent years, deep learning has achieved great develop-\nments in the field of computer vision. The FSR task has\nattracted many researchers because of its wide application\nprospects. GLN [Tuzel et al., 2016] designs the Global Up-\nsampling Network to reconstruct the overall face and the Lo-\ncal Refinement Network to enhance the local details of the\nface. Attention-FH [Cao et al., 2017] utilizes the context de-\npendencies between facial components to recursively restore\nthe details. RCAN [Zhang et al., 2018c] presents a very deep\nresidual channel attention network, which significantly en-\nhances the learning capacity of CNN. FSRNet [Chen et al.,\n2018] adds facial geometry priors such as facial landmarks,\nheatmaps and parsing maps to the network and achieves\nexcellent results. SPARNet [Chen et al., 2020 ] proposes\na network built from Face Attention Units that can effi-\nciently capture key features in very low-resolution face im-\nages. DIC [Ma et al., 2020 ] adopts a novel iterative col-\nlaboration network to gradually obtain accurate facial land-\nmarks and super-resolution images. IGAN [Li et al., 2021 ]\nconsiders SR as the information-growth process and recov-\ners HR images by exploring information differences in im-\nages of different resolutions. SRDD [Maeda, 2022] proposes\na high-resolution (HR) dictionary that can be learned explic-\nitly, which reduces the information that the network needs\nto process in the HR space. HGSRCNN [Tian et al., 2022 ]\nadopts a heterogeneous structure to enhance the internal and\nexternal correlations of channels in parallel, which promotes\nthe recovery of images.\n2.2 Vision Transformer\nThe Transformer is originally used for sequence processing in\nnatural language tasks. The proposal of ViT [Dosovitskiy et\nal., 2021] proves that Transformer can achieve state-of-the-\nart performance in the image classification task. By recur-\nsively aggregating tokens of neighboring items, T2T [Yuan\net al., 2021] greatly decreases tokens length for practical ap-\nplication. [Wu et al., 2021] builds a Pyramid Pooling Trans-\nformer to achieve better performance in various downstream\nvision tasks such as semantic segmentation and object detec-\ntion. Swin Transformer [Liu et al., 2021 ] decomposes the\nimage into non-overlapping windows, calculates multi-head\nself-attention (MHSA) within the window, and introduces\na shifted window mechanism to establish the cross-window\nconnection. Recently, various vision Transformers [Wang et\nal., 2021; Chu et al., 2021] are proposed to address different\nvisual tasks. Meanwhile, Transformers are also applied in the\nfield of super-resolution. [Liang et al., 2021] builds SwinIR\nbased on Swin Transformer and achieves excellent results in\nSR tasks. VSR [Cao et al., 2021] turns MHSA into a spatial-\ntemporal convolutional self-attention to achieve state-of-the-\nart performance in video super-resolution tasks. [Shi et al.,\n2022] proposes a pyramid encoder/decoder Transformer ar-\nchitecture to extract and restore feature textures in different\nspaces through a hierarchical structure.\n3 Method\n3.1 Overview\nAs shown in Figure 1(a), we describe three core stages of the\nproposed network: Feature Extraction Stage, Feature Trans-\nformation Stage, and Feature Recovery Stage. Let IL ∈\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1036\nRSA Blocks\nRSA Blocks\nRSA Blocks\nRSA Blocks RSA Blocks\nC\nRSA Blocks\nWFM\nWFM\nWFM\nFeature Extraction Stage Feature Recovery Stage\nFeature Transformation Stage\nRSA Blocks\nRSA Blocks\nElement-wise Sum\n(a) The architecture of the proposed FSR method\nWT\nWT\nWT\nLL, LH, HL\nLL, LH, HL\nLL, LH, HL\nWT Wavelet Transformation\n(c) Wavelet Fusion Module (WFM)\nLeakyReLU\nLeakyReLU\nWIT\nSplit\nConcatenation\nDownsample\nDownsample\nDownsample\nWIT Wavelet Inverse \nTransformation\nC Concatenation \nconvolution convolution\n(b) The proposed \nRSA Blocks\nLN\nLN\nRSA\nMLP\nStep 1 Step 4\nStep 3Step 2\nStep 3 Step 2\nStep 1Step 4\nUpsample\nRSA Blocks\nFigure 1: An illustration of the proposed face super-resolution architecture. (a) Three main stages of the network: Feature Extraction\nStage, Feature Transformation Stage, and Feature Recovery Stage. (b) The Region Search Attention (RSA) Block consists of Layer Nor-\nmalization (LN), Region Selection Attention (RSA), multi-layer perceptron (MLP), and residual connections. (c) The Wavelet Fusion Mod-\nules (WFM) can fuse three general features (LL , LH, and HL) from the Feature Extraction Stage and one high-frequency feature (HH )\nfrom the Feature Recovery Stage.\nR3×H×W be an input low-resolution RGB image, where H\nand W represent the height and width, respectively. We first\nexpand the channel of input image IL to C as:\nF0\nes = HEF (IL), (1)\nwhere HEF (·) consists of a 3 × 3 convolution and a\nLeakyReLU activation. The convolutional layer expands the\nfeature to a higher dimensional feature space, which is bene-\nficial for recovering details in different feature channels.\nFeature Extraction Stage. We extract features through a hi-\nerarchical feature pyramid structure containing 4 steps. Each\nstep consists of 2 Region Selection Attention (RSA) Blocks,\na down-sampling operation, a Wavelet Transformation (WT),\nand a 3 × 3 convolution. In particular, the 4-th step only con-\ntains 2 RSA Blocks. We get the output features Fi\nes of i-th\nstep as:\nFi\nes =\n\u001aDown(HRB(Fi−1\nes )), i = 1, 2, 3,\nHRB(Fi−1\nes ), i = 4, (2)\nwhere HRB(·) denotes 2 consecutive RSA Blocks, and\nDown(·) is the down-sampling operation. For the down-\nsampling operation, we apply a4×4 convolutional layer with\nstride 2 to double the channel number and reduce the height\nand width to 1/2 of their original size, respectively. In each\nstep, the features are also decomposed into frequency-domain\nfeatures by Wavelet Transformation (WT). Our proposed WT\nis composed of a low-pass filter and a high-pass filter. The\nfeatures sequentially pass through the combination of two fil-\nters, which can be converted into frequency-domain features:\nLL, LH, HL, and HH (see Sec. 3.3 for details). In the i-\nth step, for the output ˆFi\nes of 2 RSA Blocks, we extract the\nlow-frequency and middle-frequency features to present the\nglobal facial structure information:\nLLi, LHi, HLi = HWT ( ˆFi\nes),\nLLi = Conv3×3(LLi),\nLHi = Conv3×3(LHi),\nHLi = Conv3×3(HLi),\n(3)\nwhere HWT (·) denotes WT and Conv3×3(·) is a 3 × 3 con-\nvolutional layer.\nFeature Transformation Stage. At this stage, the output of\nthe previous step F4\nes is fed into 2 RSA Blocks and concate-\nnated with itself as:\nFts = Concat[HRB(F4\nes), F4\nes], (4)\nwhere Concat[·] denotes concatenation operation.\nFeature Recovery Stage. This stage also contains 4 steps to\ngradually restore high-resolution images. Each step consists\nof 2 RSA Blocks, and a Wavelet Fusion Module (WFM). The\n1-st step doesn’t contain the WFM. WFM can integrate the\ngeneral facial features extracted from the Feature Extraction\nStage and the corresponding high-frequency features in the\nFeature Recovery Stage through Wavelet Inverse Transforma-\ntion (WIT).\nWe will describe the WFM in detail in Sec 3.4. More\nspecifically, the output of j-th step can be formulated as:\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1037\nSoftmax\nInput\nCoarse Input\nFIM\nFIM\nQ\nK\nV\nCoarse \nAttention \nMap\nCoarse Output\nSelect Key Regions \nby Indices\nDownsample\nMHSA\nOutput\nElement-wise Sum\nMatrix Multiplication\nFIM\nTopk\nSum\nFeature Integration Module\nGet the largest    elements \nGet the accumulation along \nthe penultimate dimension \nDW-Conv\nRegion \nSelection \nAttention (RSA)\nMulti-Head\nSelf-Attention (MHSA)\nAdd by \nIndices\nSum\nUpsample\nDW-Conv  Depth-wise Convolution\nTopk\nCoarse Self-Attention (CSA)\nFigure 2: The architecture of our proposed Region Selection Attention (RSA). It consists of Feature Integration Module (FIM), Coarse Self-\nAttention (CSA), key regions selection, key regions self-attention, and a depth-wise convolution.\nFj\nrs =\n\n\n\nHRB(Fts), j = 1,\nHRB(HWFM (Fj−1\nrs , LLi,\nLHi, HLi)), j = 2, 3, 4,\n(5)\nwhere i = 5−j, and HWFM (·) represents the WFM. Finally,\nthe features F4\nrs ∈ R2C×H×W are refined by a 3×3 convolu-\ntion and a LeakyReLU activation to obtain the residual image\nIR ∈ R3×H×W . The final recovered image ˆI can be obtained\nby ˆI = IL + IR.\n3.2 Efficient Region Search Attention (RSA) Block\nAs shown in Figure 1(b), we replace the traditional multi-\nhead self-attention (MHSA) with our proposed Region Se-\nlection Attention (RSA). Given the input as Fs, our proposed\nRSA Block can be formulated as:\nFs\n′ = RSA(LN(Fs)) +Fs,\nˆFs = MLP (LN(Fs\n′)) +Fs\n′,\n(6)\nwhere LN(·) is Layer Normalization and RSA(·) is RSA,\nwhich selects multiple regions with rich high-frequency in-\nformation and computes their self-attention.\nRSA is a data-driven approach to adaptively implement\ntexture-aware reconstruction in a coarse-to-fine procedure.\nSpecifically, we get a global coarse attention map via Coarse\nSelf-Attention (CSA). This coarse attention map can reflect\nthe high-frequency information density of each region of the\nimage. Under the guidance of the coarse attention map, we\ncan find the largestk attention values in this feature map, rep-\nresenting the most important k regions in the original fea-\ntures. These regions are extracted from the original features\nand calculated MHSA to refine the results. Key regions with\nrich high-frequency information are thus reconstructed by the\nrefinement procedure.\nAs illustrated in Figure 2, given the input Finput ∈\nRC′×H′×W′\n, Finput is first downsampled into a smaller fea-\nture map Fcoarse as:\nFcoarse = Downsample(Finput), (7)\nwhere Downsample(·) uses a convolutional layer with ker-\nnel size of λ × λ and stride λ. Then the features are fed\ninto the Feature Integration Module (FIM) to privilege the\nchannel-wise importance. This can be formulated as:\nˆFinput = HFIM (Finput), ˆFcoarse = HFIM (Fcoarse) (8)\nThen we transpose the coarse feature ˆFcoarse ∈\nRC′×H′\nλ ×W′\nλ into Fc ∈ Rnc×C′\n, where nc = H′\nλ × W′\nλ . We\nfeed it into MHSA with M heads as:\nQc = FcWq, Kc = FcWk, Vc = FcWv,\nAM(h) = Softmax (Q(h)\nc K(h)T\nc√Dh\n), h= 1, . . . , M,\nSA(h) = AM(h)V (h)\nc ,\nOutputc = Concat(SA(1), . . . , SA(M))Wo,\n(9)\nwhere Wq, Wk, Wv, Wo are learnable parameters, and Dh =\nC′/M is the number of dimensions for one head. SA(h),\nAM(h), Q(h)\nc , K(h)\nc and V (h)\nc represent the output of self-\nattention, attention map, query embedding, key embedding,\nand value embedding from the h-th attention head, respec-\ntively. Then we select the key regions according to the coarse\nattention map. It is can be defined as:\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1038\nElement-wise SumElement-wise Product Sigmoid Activation\nInput\nDW-Conv Adaptive \nAvgPool\nAdaptive \nAvgPool DW-Conv\nFirst Branch \nSecond Branch \n Depth-wise Convolution convolutionDW-Conv\nOutput\nFigure 3: The structure of the Feature Integration Module (FIM).\nAM = Concat(AM(1), . . . , AM(M)),\nIndex = GetIndex(T opk(Sum(AM))), (10)\nwhere AM ∈ RM×nc×nc means coarse attention map and\nSum(·) represents accumulation along the penultimate di-\nmension of AM, which turns region-to-region attention into\nregion-to-global attention. T opk(·) means to get the largest\nk elements and GetIndex(·) denotes getting the indices of\nelements in feature ˆFinput. Here we let k = ⌊\n√\nH′W′\nλ ⌋ to\nbalance the computational cost and effect. Thus we get the\nindices of the key regions in ˆFinput. Next, we extract these\nregions to calculate the MHSA and add them back to ˆFinput\nby indices. The output of this process can be obtained by:\nFregions = SelectByIndex ( ˆFinput, Index),\nOutputf = AddByIndex (Finput,\nMHSA( Fregions), Index),\n(11)\nwhere SelectByIndex (·) means to select the regions in\nˆFinput according to Index and AddByIndex (·) denotes\nadding Fregions after MHSA to Finput according to Index.\nFinally, we upsample Outputc and add it to Outputf , which\nis then fed into a depth-wise convolutional layer. It could be\nformulated as:\nOutput = DW Conv(Outputf + Upsample(Outputc)),\n(12)\nwhere Upsample(·) denotes the up-sampling operation,\nwhich is a 4 × 4 transposed convolution with stride 2.\nDW Conv(·) means 3 × 3 depth-wise convolution.\nFeature Integration Module (FIM). As shown in Figure 3,\nour proposed FIM consists of depth-wise convolutions, chan-\nnel attention module, and residual connection. Depth-wise\nconvolution can effectively model the spatial information of\nfeatures. The channel attention module allows the network to\nfocus on channels which should be paid more attention. We\nchange the combination of the depth-wise convolution and\nthe channel attention for better interaction. Moreover, we uti-\nlize the residual connections to merge dual branches and the\noriginal input. This design complements the features with\nrich cross-spatial and cross-channel information.\nIn FIM, the kernel size of depth-wise convolution is 3 × 3,\nand Adaptive AvgPool means global average pooling opera-\ntion.\n3.3 Wavelet Transformation (WT) and Wavelet\nInverse Transformation (WIT)\nOur proposed Wavelet Transformation (WT) and Wavelet In-\nverse Transformation (WIT) consist of a convolutional block\nand a transposed convolution block, respectively. To decom-\npose features into several frequency components, we adopt\na high-efficiency wavelet transformation, namely the Haar\nwavelet.\nL = 1\n√\n2 [1 1 ]T , H= 1√\n2 [−1 1 ]T , (13)\nwhere L and H represent low-pass and high-pass filters, re-\nspectively. Low-pass filters can capture general information\nlike global contour, and facial structure. In contrast, a high-\npass filter extracts local details such as texture, eyes, facial\ncomponents, etc.\nWith the combination of two filters, we can achieve four\nkernels of Haar wavelet:\nLLT = 1\n2\n\u0014\n1 1\n1 1\n\u0015\n, LHT = 1\n2\n\u0014\n−1 1\n−1 1\n\u0015\n,\nHLT = 1\n2\n\u0014\n−1 −1\n1 1\n\u0015\n, HHT = 1\n2\n\u0014\n1 −1\n−1 1\n\u0015 (14)\nThe above four kernels could be utilized to decompose the\nfeature map into frequency-domain components: LL, LH,\nHL, and HH . Given an arbitrary feature map F, the WT can\nbe defined as:\nLL = ConvLLT (F),\nLH = ConvLHT (F),\nHL = ConvHLT (F),\nHH = ConvHHT (F),\n(15)\nwhere ConvLLT (·), ConvLHT (·), ConvHLT (·),\nConvHHT (·) represent group convolution with stride 2\nand weights LLT , LHT , HLT and HH T , respectively.\nWe use WIT to integrate the general information (LL, LH,\nand HL) and the high-frequency details HH to reconstruct\nthe face image. The WIT can be defined as:\nˆF = DeconvLLT (LL) +DeconvLHT (LH)+\nDeconvHLT (HL) +DeconvHHT (HH ), (16)\nwhere DeconvLLT (·), DeconvLHT (·), DeconvHLT (·),\nDeconvHHT (·) represent four separate transposed convolu-\ntions with the weights as Eq. 14.\nThe WT operation achieves the modulation of shallow-\nlayer and deep-layer feature maps from the perspective of the\nfrequency domain. We employ skip connections to perform\nWIT in the Feature Recovery Stage, which can stabilize the\ngeneration of detailed information.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1039\nBicubic\n RCAN\n DIC\n SPARNet\n IGAN\n SwinIR\n SRDD\n HGSRCNN\n Ours\n HR\nFigure 4: Visual comparisons for 8× FSR. Our method can produce more accurate details. In contrast, other methods generate undiscernible\nartifacts in complex regions. Zoom in for the best view.\n3.4 Wavelet Fusion Module (WFM)\nIn this section, we describe our proposed Wavelet Fusion\nModule (WFM). As shown in Figure 1(c), for the featureFrs,\nit is split evenly into two features Frs L and Frs R as:\nFrs L, Frs R = Split(Frs) (17)\nThen, Frs L and Frs R are fed into a Wavelet Inverse\nTransformation (WIT) and an up-sampling block, respec-\ntively, which are defined as:\nˆFrs\nL = HWIT (Frs L, LL, LH, HL),\nˆFrs R = Upsample(Frs R),\n(18)\nwhere HWIT (·) and Upsample(·) separately represents the\nWIT and a 4 × 4 transposed convolution with stride 2. Deep-\nlayer feature Frs L and the corresponding shallow-layer fea-\ntures LL, LH, HL are modulated to refine the detail in the\nWIT.\nThen we feed the combination of two features into a con-\nvolutional layer to get the output of this module, which can\nbe formulated as:\nˆFrs = Conv1×1(Concat[ ˆFrs L, ˆFrs R]), (19)\nwhere Concat[·] denotes concatenation operation.\nConv1×1(·) is an 1 × 1 convolution, which reduces the\nchannel number of the features to half.\n4 Experiments\n4.1 Datasets\nThe CelebA [Liu et al., 2015] and the Helen [Le et al., 2012]\nare two publicly available face image datasets. First, we use\nMTCNN [Zhang et al., 2016] to crop the face region. After\nexcluding images with a resolution smaller than 128 × 128,\nthe image is resized to128×128. So we obtained about 178k\nimages from CelebA, of which 177k images were used as\nHR training images. Using bicubic interpolation, the HR im-\nages are downsampled to 16 × 16 to generate LR images. In\nthe testing phase, we extract the remaining 1000 images from\nthe cropped CelebA dataset and randomly extract 100 images\nfrom the cropped Helen dataset. For evaluation, we employ\nthe following metrics: Peak Signal-to-Noise Ratio (PSNR),\nStructural Similarity (SSIM) [Wang et al., 2004 ] computed\non the Y channel of the image YCbCr space, and Learned Per-\nceptual Image Patch Similarity (LPIPS)[Zhang et al., 2018b].\n4.2 Implementation Details\nBefore the Feature Extraction Stage, we first extend the num-\nber of feature channels to 32. In the Feature Extraction Stage,\nthe channel number of feature map is 2i−1 × 32 and the at-\ntention head number of RSA Block is 2i−1 in the i-th step.\nIn the Feature Transformation Stage, the number of feature\nchannels is set to 256. The number of attention heads is set to\n16. In the Feature Recovery Stage at the j-th step, the chan-\nnel number of feature map is25−j ×32 and the attention head\nnumber of RSA Block is 24−j. When the height × width of\nthe feature map is less than or equal to 32 × 32, we only\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1040\nMethod Helen CelebA\nPSNR SSIM LPIPS PSNR SSIM LPIPS\nBicubic 24.25 0.6798 0.5236 24.10 0.6605 0.5267\nRCAN [Zhang et al.\n, 2018c] 27.94 0.8196 0.1927 28.08 0.8149 0.1825\nDIC [Ma et al.\n, 2020] 26.61 0.7755 0.2292 27.38 0.7911 0.1950\nSPARNet [Chen et\nal., 2020] 27.62 0.8094 0.2012 27.83 0.8067 0.1874\nIGAN [Li et al.\n, 2021] 27.97 0.8213 0.1845 28.15 0.8171 0.1767\nSwinIR [Liang et al.\n, 2021] 28.04 0.8219 0.1905 28.27 0.8192 0.1808\nSRDD [Maeda, 2022] 27.64 0.8093 0.2184 27.88 0.8071 0.2058\nHGSRCNN [Tian et\nal., 2022] 27.93 0.8192 0.1885 28.20 0.8175 0.1793\nOurs 28.36 0.8318 0.1626 28.58 0.8297 0.1542\nTable 1: Quantitative comparison on Helen and CelebA test set for\n8× FSR. The best and second-best performances are denoted by the\nred and blue.\nemploy CSA in the RSA with λ = 1. Otherwise, we apply\nthe full RSA and λ = 4. The AdamW optimizer is used to\ntrain our model with β1 = 0.9, β2 = 0.99, and weight decay\nis set to 0.02. The learning rate is initially set to 4 × 10−4\nand dropped by half every 20 epochs. Meanwhile, L1 loss is\nused for training. The batchsize is set to 32. Our model is\nimplemented in PyTorch and trained for 100 epochs using 2\nNVIDIA GeForce RTX 3090 GPUs.\n4.3 Comparisons with State-of-the-Art Methods\nWe compare the proposed FSR method with several state-of-\nthe-art methods: RCAN [Zhang et al., 2018c ], DIC [Ma et\nal., 2020 ], SPARNet [Chen et al., 2020 ], IGAN [Li et al.,\n2021], SwinIR [Liang et al., 2021 ], SRDD [Maeda, 2022 ],\nand HGSRCNN [Tian et al., 2022]. Our quantitative results\nfor Helen and CelebA dataset are shown in Table 1. The re-\nsults demonstrate that our method achieves the best PSNR,\nSSIM, and LPIPS performance on both datasets, outperform-\ning other state-of-the-art approaches by a significant margin.\nSpecifically, for the PSNR value, our method outperforms the\nbest other methods by 0.32dB and 0.31dB on the Helen and\nCelebA datasets, respectively. Qualitative results are shown\nin Figure 4. Our method can recover more realistic details in\nthe blurry region, and the reconstructed result is closer to the\nreal image. The pupils and nose of the face are restored more\nclearly. Other methods tend to produce blurring artifacts in\nregions with complex textures.\n4.4 Ablation Study\nEffectiveness of the Region Selection Attention (RSA).The\ncore part of RSA is the region selection strategy. Relying on\nthis design, our RSA can effectively focus on regions with\nlong-range dependencies and rich high-frequency informa-\ntion. To demonstrate the effectiveness of this strategy, we re-\nmove the part that selects regions to compute key regions self-\nattention and only retain the Coarse Self-Attention (CSA).\nAs shown in Table 2, our RSA improves the PSNR value by\n0.12dB on the Helen test set. The above experiments demon-\nstrate the effectiveness of the proposed RSA.\nEffectiveness of the Feature Integration Module (FIM).\nOur proposed FIM is used to enhance cross-channel aware-\nness and cross-spatial awareness of features. To verify the\neffectiveness of FIM, we use learnable position parameters,\ndepth-wise convolution or single-branch FIM to replace full\nFIM. As can be seen from Table 3, compared to learnable\nMethods RSA\nCSA\nPSNR 28.36 28.24\nSSIM 0.8318 0.8297\nTable 2: Comparison of Region Selection Attention (RSA) and\nCoarse Self-Attention (CSA).\nMethods PSNR SSIM\nlearnable position parameters 28.25 0.8296\nDW-Conv 28.25 0.8300\nFIM only first branch 28.29 0.8301\nFIM only second branch 28.28 0.8293\nFIM 28.36 0.8318\nTable 3: Comparison of learnable position parameters, the depth-\nwise convolution(DW-Conv), and different branches of FIM.\nMethods PSNR SSIM\nconcatenation 28.31 0.8306\nWFM w/o splitting 28.32 0.8310\nWFM 28.36 0.8318\nTable 4: Quantitative comparisons of WFM. WFM without splitting\nmeans that the entire feature participates in WIT.\nposition parameters and depth-wise convolution, our first-\nbranch-only FIM and second-branch-only FIM improve the\nPSNR values by 0.04dB and 0.03dB, respectively. The\nfull FIM achieved a significant PSNR value improvement of\n0.11dB, which illustrates the superiority of our proposed in-\nnovative FIM.\nEffectiveness of the Wavelet Fusion Module (WFM).\nWFM is used in the Feature Recovery Stage to recombine\nthe frequency-domain features. In WFM, we retain half of\nthe features that do not participate in the WIT. Preserved fea-\ntures help stabilize feature recovery. Therefore, we designed\ntwo ablation experiments: In the first one, we simply replaced\nWFM with a concatenation operation. In the second one, we\nlet all the features in the WFM participate in the WIT with-\nout splitting. Table 4 demonstrates that our proposed WFM\nachieves 0.05dB and 0.04dB improvements, respectively. Ex-\nperiments prove that the designed WFM is efficient.\n5 Conclusion\nIn this paper, we propose a self-refinement Transformer\nfor FSR. It could conduct the coarse-grained self-attention\nand further compensate for the details by fine-grained self-\nattention on key regions. To consider the importance of\nchannel information in Transformer, we also employ the\nFIM to achieve cross-spatial and cross-channel integration si-\nmultaneously. Furthermore, WFM is designed to modulate\nthe shallow and deep feature maps in frequency domain for\nrestoration. Extensive experiments demonstrate the effective-\nness of the proposed method.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1041\nAcknowledgments\nThis work was supported by the National Natural Science\nFoundation of China (Grant No. 62002283, 62311530046,\nU2003207, 61921004, U21B2048, 62102307, 61972016,\n62071380), the Key Research and Development Program of\nShaanxi (No.2021GXLH-Z-021) and the Fundamental Re-\nsearch Funds for the Central Universities.\nReferences\n[Bulat and Tzimiropoulos, 2018] Adrian Bulat and Georgios\nTzimiropoulos. Super-fan: Integrated facial landmark lo-\ncalization and super-resolution of real-world low resolu-\ntion faces in arbitrary poses with gans. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 109–117, 2018.\n[Cao et al., 2017] Qingxing Cao, Liang Lin, Yukai Shi, Xi-\naodan Liang, and Guanbin Li. Attention-aware face hallu-\ncination via deep reinforcement learning. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, pages 690–698, 2017.\n[Cao et al., 2021] Jiezhang Cao, Yawei Li, Kai Zhang, and\nLuc Van Gool. Video super-resolution transformer. arXiv\npreprint arXiv:2106.06847, 2021.\n[Chen et al., 2018] Yu Chen, Ying Tai, Xiaoming Liu, Chun-\nhua Shen, and Jian Yang. Fsrnet: End-to-end learning face\nsuper-resolution with facial priors. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 2492–2501, 2018.\n[Chen et al., 2020] Chaofeng Chen, Dihong Gong, Hao\nWang, Zhifeng Li, and Kwan-Yee K Wong. Learning spa-\ntial attention for face super-resolution. IEEE Transactions\non Image Processing, 30:1219–1231, 2020.\n[Chu et al., 2021] Xiangxiang Chu, Zhi Tian, Yuqing Wang,\nBo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Twins: Revisiting the design of spatial\nattention in vision transformers. Advances in Neural In-\nformation Processing Systems, 34, 2021.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. In International Conference on Learning Repre-\nsentations, 2021.\n[Huang et al., 2017] Huaibo Huang, Ran He, Zhenan Sun,\nand Tieniu Tan. Wavelet-srnet: A wavelet-based cnn for\nmulti-scale face super resolution. In Proceedings of the\nIEEE International Conference on Computer Vision, pages\n1689–1697, 2017.\n[Le et al., 2012] Vuong Le, Jonathan Brandt, Zhe Lin,\nLubomir Bourdev, and Thomas S Huang. Interactive facial\nfeature localization. In European conference on computer\nvision, pages 679–692. Springer, 2012.\n[Li et al., 2021] Zhuangzi Li, Ge Li, Thomas Li, Shan Liu,\nand Wei Gao. Information-growth attention network for\nimage super-resolution. In Proceedings of the 29th ACM\nInternational Conference on Multimedia, pages 544–552,\n2021.\n[Liang et al., 2021] Jingyun Liang, Jiezhang Cao, Guolei\nSun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir:\nImage restoration using swin transformer. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pages 1833–1844, 2021.\n[Liu et al., 2015] Ziwei Liu, Ping Luo, Xiaogang Wang, and\nXiaoou Tang. Deep learning face attributes in the wild.\nIn Proceedings of the IEEE international conference on\ncomputer vision, pages 3730–3738, 2015.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 10012–\n10022, 2021.\n[Ma et al., 2020] Cheng Ma, Zhenyu Jiang, Yongming Rao,\nJiwen Lu, and Jie Zhou. Deep face super-resolution\nwith iterative collaboration between attentive recovery and\nlandmark estimation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 5569–5578, 2020.\n[Maeda, 2022] Shunta Maeda. Image super-resolution with\ndeep dictionary. In European Conference on Computer\nVision, pages 464–480. Springer, 2022.\n[Shi and Zhao, 2019] Jingang Shi and Guoying Zhao. Face\nhallucination via coarse-to-fine recursive kernel regression\nstructure. IEEE Transactions on Multimedia, 21(9):2223–\n2236, 2019.\n[Shi et al., 2022] Jingang Shi, Yusi Wang, Songlin Dong, Xi-\naopeng Hong, Zitong Yu, Fei Wang, Changxin Wang, and\nYihong Gong. IDPT: interconnected dual pyramid trans-\nformer for face super-resolution. In Proceedings of the\nThirty-First International Joint Conference on Artificial\nIntelligence (IJCAI), pages 1306–1312, 2022.\n[Tian et al., 2022] Chunwei Tian, Yanning Zhang, Wang-\nmeng Zuo, Chia-Wen Lin, David Zhang, and Yixuan Yuan.\nA heterogeneous group cnn for image super-resolution.\nIEEE Transactions on Neural Networks and Learning Sys-\ntems, 2022.\n[Tuzel et al., 2016] Oncel Tuzel, Yuichi Taguchi, and John R\nHershey. Global-local face upsampling network. arXiv\npreprint arXiv:1603.07235, 2016.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. Advances in neural information processing systems,\n30, 2017.\n[Wang et al., 2004] Zhou Wang, Alan C Bovik, Hamid R\nSheikh, and Eero P Simoncelli. Image quality assessment:\nfrom error visibility to structural similarity. IEEE transac-\ntions on image processing, 13(4):600–612, 2004.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1042\n[Wang et al., 2021] Wenhai Wang, Enze Xie, Xiang Li,\nDeng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A ver-\nsatile backbone for dense prediction without convolutions.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 568–578, 2021.\n[Wu et al., 2021] Yu-Huan Wu, Yun Liu, Xin Zhan, and\nMing-Ming Cheng. P2t: Pyramid pooling transformer for\nscene understanding. arXiv preprint arXiv:2106.12011,\n2021.\n[Yuan et al., 2021] Li Yuan, Yunpeng Chen, Tao Wang, Wei-\nhao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token vit: Training\nvision transformers from scratch on imagenet. InProceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 558–567, 2021.\n[Zhang et al., 2016] Kaipeng Zhang, Zhanpeng Zhang,\nZhifeng Li, and Yu Qiao. Joint face detection and align-\nment using multitask cascaded convolutional networks.\nIEEE signal processing letters, 23(10):1499–1503, 2016.\n[Zhang et al., 2018a] Kaipeng Zhang, Zhanpeng Zhang,\nChia-Wen Cheng, Winston H Hsu, Yu Qiao, Wei Liu, and\nTong Zhang. Super-identity convolutional neural network\nfor face hallucination. In Proceedings of the European\nconference on computer vision (ECCV), pages 183–198,\n2018.\n[Zhang et al., 2018b] Richard Zhang, Phillip Isola, Alexei A\nEfros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 586–595, 2018.\n[Zhang et al., 2018c] Yulun Zhang, Kunpeng Li, Kai Li,\nLichen Wang, Bineng Zhong, and Yun Fu. Image super-\nresolution using very deep residual channel attention net-\nworks. In Proceedings of the European conference on\ncomputer vision (ECCV), pages 286–301, 2018.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1043",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8399428129196167
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6008968949317932
    },
    {
      "name": "Feature extraction",
      "score": 0.4960730969905853
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.46267861127853394
    },
    {
      "name": "Computer vision",
      "score": 0.38201087713241577
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I87445476",
      "name": "Xi'an Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210090971",
      "name": "Southeast University",
      "country": "BD"
    },
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210105595",
      "name": "Institute of Art",
      "country": "PL"
    }
  ]
}