{
    "title": "ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation",
    "url": "https://openalex.org/W4280557512",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2003050958",
            "name": "Long Phan",
            "affiliations": [
                "Case Western Reserve University"
            ]
        },
        {
            "id": "https://openalex.org/A2120452325",
            "name": "Hieu Tran",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2123210919",
            "name": "Hieu Nguyen",
            "affiliations": [
                "Case Western Reserve University"
            ]
        },
        {
            "id": "https://openalex.org/A2789698198",
            "name": "Trieu H. Trinh",
            "affiliations": [
                "New York University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3093517588",
        "https://openalex.org/W2970419734",
        "https://openalex.org/W4287017558",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3011161054",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2905330007",
        "https://openalex.org/W2989539713",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3169653581",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3170083118",
        "https://openalex.org/W3107826490",
        "https://openalex.org/W3037205571",
        "https://openalex.org/W4226107112",
        "https://openalex.org/W2889518897",
        "https://openalex.org/W3106445907",
        "https://openalex.org/W3183962691",
        "https://openalex.org/W3098637735",
        "https://openalex.org/W3205785479",
        "https://openalex.org/W2962739339"
    ],
    "abstract": "Long Phan, Hieu Tran, Hieu Nguyen, Trieu H. Trinh. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop. 2022.",
    "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies: Student Research Workshop, pages 136 - 142\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nViT5: Pretrained Text-to-Text Transformer for Vietnamese Language\nGeneration\nLong Phan1,2, Hieu Tran1, Hieu Nguyen1,2, Trieu H. Trinh1,3\n1VietAI Research\n2Case Western Reserve University\n3New York University\nlong.phan@case.edu\nAbstract\nWe present ViT5, a pretrained Transformer-\nbased encoder-decoder model for the Viet-\nnamese language. With T5-style self-\nsupervised pretraining, ViT5 is trained on\na large corpus of high-quality and diverse\nVietnamese texts. We benchmark ViT5 on\ntwo downstream text generation tasks, Ab-\nstractive Text Summarization and Named En-\ntity Recognition. Although Abstractive Text\nSummarization has been widely studied for\nthe English language thanks to its rich and\nlarge source of data, there has been min-\nimal research into the same task in Viet-\nnamese, a much lower resource language. In\nthis work, we perform exhaustive experiments\non both Vietnamese Abstractive Summariza-\ntion and Named Entity Recognition, validat-\ning the performance of ViT5 against many\nother pretrained Transformer-based encoder-\ndecoder models. Our experiments show that\nViT5 significantly outperforms existing mod-\nels and achieves state-of-the-art results on\nVietnamese Text Summarization. On the task\nof Named Entity Recognition, ViT5 is com-\npetitive against previous best results from pre-\ntrained encoder-based Transformer models.\nFurther analysis shows the importance of con-\ntext length during the self-supervised pretrain-\ning on downstream performance across differ-\nent settings.\n1 Introduction\nIn recent years, Transformer-based architecture\nmodels and pretrained language models (LMs)\nhave played a crucial role in the development of\nNatural Language Processing (NLP). Large pre-\ntrained models such as ELMo (Peters et al., 2018),\nGPT (Brown et al., 2020), BERT (Devlin et al.,\n2018) is trained on large corpora and have the\nability to derive contextual representation of the\nlanguage(s) in the training data. After pretrain-\ning is complete, these models achieved state-of-\nthe-art results on a broad range of downstream\ntasks (Devlin et al., 2018). These self-supervised\nlearning methods make use of learning objectives\nsuch as Masked Language Modeling (MLM) (De-\nvlin et al., 2018) where random tokens in the\ninput sequence are masked and the model at-\ntempts to predict the original tokens. The suc-\ncesses of pretrained models in English have in-\nspired new research efforts to develop pretrained\nmodels in other languages such as Vietnamese\n(i.e., PhoBERT (Nguyen and Nguyen, 2020) and\nViBERT (Bui et al., 2020)) and Italian (Sarti and\nNissim, 2022). There are also ongoing efforts\nto develop multilingual pretrained models (mT5\n(Xue et al., 2020), mBART (Liu et al., 2020)),\nin order to improve performance across multiple\nlanguages by learning both general and language-\nspecific representations.\nA short time ago, BARTpho (Tran et al., 2021),\na large pretrained sequence-to-sequence model for\nVietnamese inheriting BART style (Lewis et al.,\n2019), demonstrated the effectiveness of pre-\ntrained language models on Vietnamese abstrac-\ntive summarization. Nevertheless, there are some\npast works that have shown that T5 architecture\n(Raffel et al., 2019) might outperform BART in\nsome aspects (i.e., (Phan et al., 2021a)). Inspired\nby that, we propose ViT5, trained on the Viet-\nnamese monolingual subset of CC100, following\nthe architecture and training methodology in Raf-\nfel et al. (2019). We perform exhaustive compar-\nisons on downstream performance to many differ-\nent pretrained Transformer-based models (Nguyen\net al., 2021; Tran et al., 2021; To et al., 2021).\nSpecifically, we finetune the ViT5 on two sum-\nmarization datasets, Wikilingua (Ladhak et al.,\n2020) and Vietnews (Nguyen et al., 2019), and\none Named Entity Recognition dataset (PhoNER\n(Truong et al., 2021)).\nText summarization is an important downstream\n136\ntask whose input is a free-form text paragraph\nor document(s), and the output sequence is ex-\npected to be a short summarization of the input.\nViT5 achieves state-of-the-art results on both two\nof the single-document summarization tasks. We\nalso perform an analysis on the max-length hyper-\nparameter for input and output sequences during\nself-supervised learning and showed that longer\nlengths that match the downstream document’s\nlength lead to better result.\nFor NER, we reformulated the per-token clas-\nsification task into a generation task, where the\ndecoder reconstructs the original input sentence\nwith inserted Named Entity tags following each\ntoken (Phan et al., 2021b). This simple and\nstraightforward formulation achieves competitive\nresults in comparison to direct per-token classifi-\ncation done on encoder-only model (Nguyen and\nNguyen, 2020).\n2 Related Work\nThere are lots of abstractive summarization stud-\nies in English. In an early example, (Gehrmann\net al., 2018) employed a bottom-up content se-\nlector (BottomUp) to determine which phrases\nin the source document should be part of the\nsummary, and then a copy mechanism was ap-\nplied only to pre-select phrases during decoding.\nTheir experiments obtained significant improve-\nments on ROUGE for some canonical summariza-\ntion datasets.\nIn recent years, pretrained language models\nhave been used to enhance performance on lan-\nguage generation tasks. (Liu and Lapata, 2019)\ndeveloped a Transformer-based encoder-decoder\nmodel so that pretrained language models like\nBERT can be adopted for abstractive summa-\nrization. Here, the authors proposed a novel\ndocument-level BERT-based encoder (BERTSum)\nand a general framework encompassing both\nextractive and abstractive summarization tasks.\nBased on BERTSum, Dou et al. (2021) introduced\nGSum that effectively used different types of guid-\nance signals as input in order to generate more\nsuitable words and more accurate summaries. This\nmodel accomplished state-of-the-art performance\non four popular English summarization datasets.\nMeanwhile, there are a small number of stud-\nies on Vietnamese text summarization. Most of\nthese focus on inspecting extractive summariza-\ntion. The researchers (Nguyen et al., 2018) com-\npared a wide range of extractive methods, includ-\ning unsupervised ranking methods (e.g., LexRank,\nLSA, KL-divergence), supervised learning meth-\nods using TF-IDF and classifiers (e.g., Support\nVector Machine, AdaBoost, Learning-2-rank), and\ndeep learning methods (e.g., Convolutional Neural\nNetwork, Long-Short Term Memory). Similarly,\nthe authors (Nguyen et al., 2019) also evaluated\nthe extractive methods on their own dataset, which\nwas released publicly as a benchmark for future\nstudies.\nRecent work (Quoc et al., 2021) investigated the\ncombination of a pretrained BERT model and an\nunsupervised K-means clustering algorithm on ex-\ntractive text summarization. The authors utilized\nmultilingual and monolingual BERT models to\nencode sentence-level contextual information and\nthen ranked this information using the K-means\nalgorithm. Their report showed that monolingual\nmodels achieved better results compared when to\nmultilingual models performing the same extrac-\ntive summarization tasks. However, due to the\nlack of studies on Vietnamese abstractive summa-\nrization, we compare both multilingual and mono-\nlingual encoder-decoder models.\n3 ViT5\nIn this section, we will explain our newly released\nViT5 models, the vocabulary generation steps, the\npretraining data, and the training setup.\n0.0 0.2 0.4 0.6 0.8 1.0\nStep 1e6\n0.2\n0.4\n0.6\n0.8Loss\nFigure 1: Loss curves for the masked span prediction\ntask were used to pretrain the ViT5 models. Larger\nmodel with larger context optimizes much better, which\nleads to better downstream performance.\n3.1 Model\nViT5 follows the encoder-decoder architecture\nproposed by Vaswani et al. (2017) and the T5\n137\nViT5\nEncoder\nViT5\nDecoder\nwikilingua: Anh \u0001 y b\u0002 t xe t\u0003 i tham gia b\u0004 a ti\u0005 c t\u0006 i m\u0007 t nhà\nhàng sang tr\b ng. Nh\t ng trong bu\n i ti\u0005 c, anh \u0001 y ngã qu\u000b\nxu\f ng và \r\n\t\n\u0000 c \r\n\t a t\u0003 i b\u0005 nh vi\u0005 n. \n(He took the car to attend a party at a luxury restaurant. But\nat the party, he collapsed and was taken to the hospital.)\nAnh \u0001 y \n\r\nã nh\u000f p vi\u0005 n sau khi tham gia b\u0004 a ti\u0005 c.\n(He was hospitalized after attending the party.)\n<task_name>: <input_text>\n<output_text>\npho_ner: B\u0005 nh nhân 75 là n\u0004  , 40 tu\n i , \n\r\u000e\na ch\n\u0011\n \n\u0012\nQu\u000f n 2 , TP. HCM\n(Patient No.75 is a female, 40 years old, and lives in\nDistrict 2, HCM city)\nB\u0005 nh nhân PATIENT_ID* 75 PATIENT_ID* là GENDER* n\u0004  GENDER* ,\nAGE* 40 AGE* tu\n i , \r\u000e a ch\u0011  \u0012  LOCATION* Qu\u000f n 2 LOCATION* ,\nLOCATION* TP. HCM LOCATION*\n(Patient PATIENT_ID* No.75 PATIENT_ID* is a GENDER*\nfemale GENDER* , AGE* 40 AGE* years old, and lives\nin LOCATION* District 2 LOCATION* , LOCATION* HCM city LOCATION*)\nFigure 2: An overview of ViT5 encoder-decoder architecture, with input-output examples of two downstream\ntasks. For Named Entity Recognition, the decoder reconstructs the sentence with inserted Entity tags.\nframework proposed by (Raffel et al., 2019). The\noriginal works of T5 proposed five different con-\nfigs of model size: small, base, large, 3B, and 11B.\nFor the purpose of practical study, we adapt the\nbase (310M parameters) and large (866M param-\neters) models for ViT5 models and leave bigger\nmodels for future works.\nWe train ViT5 models with two different in-\nput and output lengths: 256 and 1024-length. We\nthoroughly experimented with these two models to\nhave an insight into the importance of pretraining\ndata length for summarization tasks. For the self-\nsupervised training learning objectives, we use the\nspan-corruption objective with a corruption rate of\n15%. Figure 1 shows the computed loss during the\nself-supervised training stage for the three models.\n3.2 Vocabulary\nDifferent from some other current Vietnamese\nTransformer-based language models, we find that\nan effective vocabulary can contribute a significant\nimprovement to our model performance. There-\nfore, we did pre-process on a 5GB subset of\nour pretraining corpus with care like normalizing\npunctuation and capitalization, splitting numbers.\nWe fixed the size of vocabulary to 36K sub-words\nand trained SentencePiece (Kudo and Richardson,\n2018) model on that dataset.\n3.3 Pretraining Data\nWe use the CC100 Dataset (Monolingual Datasets\nfrom Web Crawl Data) (Wenzek et al., 2020; Con-\nneau et al., 2020). The corpus contains mono-\nlingual data for over 100 languages. The corpus\nwas constructed using the pipeline provided by\n(Wenzek et al., 2020) through processing January-\nDecember 2018 Commoncrawl snapshots. The\ntotal size for the Vietnamese Corpus is 138GB\nof raw text. We process and filter out 69GB of\nshort paragraphs for 256-length model and 71GB\nof long paragraphs for 1024-length model.\nTable 1: Input and Output Length of Finetuned\nDatasets\nWikilingua Vietnews\nTrain 13707 99134\nTest 3916 22498\n#avg body length 521 519\n#avg abstract length 44 38\n4 Abstractive Summarization\n4.1 Wikilingua\nWikilingua (Ladhak et al., 2020) is a large-scale\nmultilingual corpus for abstractive summarization\ntasks. The corpus consists of 18 languages, includ-\ning Vietnamese. These article and summary pairs\nare extracted from WikiHow1. These articles have\nbeen reviewed by human authors to ensure quality.\nThe Vietnamese articles are translated from the\noriginal English articles and have been reviewed\nby WikiHow’s international translation team.\n4.2 Vietnews\nVietnews (Nguyen et al., 2019) is a single-\ndocument abstractive summarization dataset in-\ncluding news data from reputable Vietnamese\nnews website ( tuoitre.vn, vnexpress.net, and\nnguoiduatin.vn). The authors of this work re-\nmoved all articles related to questionnaires, ana-\nlytical comments, and weather forecasts to ensure\nthe quality of document summarization. The fi-\nnal released dataset only includes long document\nnews events. The data consists of 150704 word-\nlevel news articles with a summary abstract and\nbody text pairs. We follow the filtering pipeline by\nTran et al. (2021) to deduplicate the train/dev/test\ndataset. The statistics after filtering are shown in\nTable 1.\n1https://www.wikihow.com\n138\nTable 2: Test result on Wikilingua and Vietnews Summarization\nModels WikiLingua Vietnews\nROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L\nTransformer\n(RND2RND) 46.25 16.57 29.82 57.56 24.25 35.53\nPhoBERT2PhoBERT 50.4 19.88 32.49 60.37 29.12 39.44\nmBERT2mBERT 52.82 20.57 31.55 59.67 27.36 36.73\nmBART 55.21 25.69 37.33 59.81 28.28 38.71\nmT5 55.27 27.63 38.30 58.05 26.76 37.38\nBARTpho 57.16 31.18 40.89 61.14 30.31 40.15\nViT5base 256-length 57.86 29.98 40.23 61.85 31.70 41.70\nViT5base 1024-length 58.61 31.46 41.45 62.77 33.16 42.75\nViT5large 1024-length 60.22 33.12 43.08 63.37 34.24 43.55\nNotes: The best scores are in bold and second best scores are underlined. The scores in gray color are our experiments.\nCode and models for reproducing our experiments: https://github.com/vietai/ViT5\n4.3 Baselines\nIn order to verify the effectiveness of our pro-\nposed methods, we compare ViT5 models with\nthe Transformer models based on (Vaswani et al.,\n2017), the ViSum BERT2BERT models (Nguyen\net al., 2021), multilingual encoder-decoder model\n(Xue et al., 2020; Liu et al., 2020), and Viet-\nnamese encoder-decoder BARTpho model (Tran\net al., 2021). The baseline transformer models (la-\nbeled RND) have a multi-head self-attention and a\nfeed-forward network. RND models are initialized\nwith random weights. For the BARTpho models,\nwe follow the models set up and results released\nby (Tran et al., 2021). All finetuned ViT5 models\nare conducted with a sequence length of 1024.\n4.4 Results\nWe report the results of the ViT5 models on two\ndatasets: Wikilingua and Vietnews. We do experi-\nments with two versions of pretraining ViT5: 256-\nlength and 1024-length to have an insight into the\nimportance of pretraining data’s paragraph length\nfor summarization in Vietnamese. We also com-\npare the results of ViT5base and ViT5large models.\nWe use ROUGE (Recall-Oriented Understudy\nfor Gisting Evaluation) as our benchmark metrics\nfor both single document summarization datasets.\nThe metric measures the overlap of n-grams and\nword sequences between two candidate and ref-\nerence sequences. ROUGE-1, ROUGE-2, and\nROUGE-L mean the overlap between unigram,\nbigram, and longest matching sequence, respec-\ntively.\n4.4.1 Wikilingua\nThe results of our models on Wikilingua summa-\nrization dataset are shown in Table 2. ViT5 mod-\nels outperform all of the experimented pretrained\nmodels, achieving state-of-the-art on all ROUGE\nmetrics. There is also a significant increase in\nROUGE scores when the models are pretrained\non a longer input and output sequence (1024 com-\npared to 256).\nBoth versions of ViT5 1024-length achieve the\nhighest results on Wikilingua summarization tasks\nacross all ROUGE metrics with ViT5large 1024-length\nachieving state-of-the-art. There is a signifi-\ncant improvement in score between the base and\nlarge ViT51024-length architectures (approximately\n2% for ROUGE-1, ROUGE-2, and ROUGE-L).\nThis is predictable as the number of parameters of\nViT5large (866M) is approximately 2.8 times larger\nthan ViT5base (310M).\nThere are interesting results when comparing\nthe results of 256-length and 1024-length ver-\nsions of ViT5 base. Although the finetuning set-\ntings are 1024-length for both ViT5 base models,\nViT5base 1024-length performs slightly better with\n1% higher score for ROUGE-1, ROUGE-2, and\nROUGE-L. These results are attributed to the\nlonger sequences during self-supervised training.\nAs reported in Table 1, the average words in an\ninput body of Wikilingua corpus are more than\n256 tokens, which can be considered long docu-\nments. For this reason, pretraining ViT5 on a 1024\nsequence length corpus achieves better results on\nWikilingua summarization task.\nTwo-out-of-three ViT5 models perform better\n139\nthan the published BARTpho model in summa-\nrizing Wikilingua corpus. This can be the result\nof the quality of pretraining data. While BART-\npho (and PhoBERT) was trained on 20GB of news\ndata, ViT5 models are trained using CC100, which\nis a subset of Common Crawl data. CC100 cor-\npus contains more diverse and general representa-\ntion of the Vietnamese language than news data.\nMeanwhile, Wikilingua is more of an academic or\ninstruction representation than news-like text.\n4.4.2 Vietnews\nThe size of Vietnews corpus is much larger than\nWikilingua corpus (with 7.7% for train and 5.8%\nfor test set). The result of Vietnews abstractive\nsummarization is in Table 2. Following the discus-\nsion of the need for an effective large pretrained\nencoder-decoder model in Section 1, we can see\nthat there is a minimum increase in performance\nfor the existing Vietnamese encoder-only model\ncompared to the Transformer baseline. Pretraining\non a large corpus of Vietnamese news, BARTpho\nstill showed its limitation in the Vietnews summa-\nrization task with slightly better ROUGE scores\nthan multilingual models (mBART and mT5).\nOur ViT5 models still achieve state-of-the-art\non Vietnews task for both 256 and 1024-length.\nFor a more specific news-domain corpus, ViT5\nmodels achieve notable results on the news do-\nmain although being trained on a more general\nVietnamese natural language domain (CC100).\nThis supports the assumption that our ViT5 mod-\nels learn a better representation of the Vietnamese\nlanguage even for more domain-specific summa-\nrization problems.\nSimilar to the results discussed in Section 4.4,\nViT5base models when pretrained on a longer se-\nquence corpus (1024-length) achieve better per-\nformance in summarizing compared to a short\nsequence corpus (256-length) across all ROUGE\nmetrics. The average input length for Vietnews\ndocuments is approximately the same as in the\nWikilingua task (more than 500 words). There-\nfore, the quality of long sequences during self-\nsupervised training data also leads to a better sum-\nmarizing in downstream Vietnews finetuned tasks.\n5 Named Entity Recognition (NER)\nTable 3: Test results on PhoNER COVID19\nModels Micro-F1\nXLM-Rlarge 93.8\nPhoBERTbase 94.2\nPhoBERTlarge 94.5\nViT5base 256-length 93.19\nViT5base 1024-length 94.5\nViT5large 1024-length 93.8\nNotes: The best scores are in bold.\nTo verify the effectiveness of ViT5 on clas-\nsification tasks, we test our models on\nPhoNER COVID19 dataset (Truong et al.,\n2021). PhoNER is a dataset for recognizing\nnamed entities related to the COVID19 domain\nin Vietnamese. The dataset consists of 35,000\nentities in over 10,000 sentences. The goal is to\nrecognize 10 entity types related to the domain\nof COVID19 and epidemics topics. The dataset\nwas released and benchmarked with PhoBERT\n(Nguyen and Nguyen, 2020).\nWe treat the NER classifications tasks as text-\nto-text generating tasks with tags of labels before\nand after an entity token (Phan et al., 2021b). An\nexample of NER in text-to-text format is shown in\nFigure 2. The results are shown in Table 3.\nThe ViT5large 1024-length model, although effec-\ntive in generating Vietnamese abstractive sum-\nmarization, shows its limitation in classification\ntasks with lower F1 scores on NER task. On the\nother hand, our ViT5base 1024-length model still per-\nforms slightly better than PhoBERT base and com-\npetitively the same as the current state-of-the-art\nPhoBERTlarge on the PhoNER corpus.\n6 Discussion\nAccording to the results on both Wikilin-\ngua and Vietnews summarization tasks (Ta-\nble 2 and Table 4.4.2), there is a steady in-\ncrease in ROUGE scores going from the base-\nline Transformer, BERT2BERT related mod-\nels (PhoBERT2PhoBERT and mBERT2mBERT),\nmultilingual encoder-decoder models (mBART,\nmT5), to pretrained monolingual models (BART-\npho and ViT5). For Vietnamese summarization\ntasks, monolingual encoder-decoder models no-\nticeably outperform multilingual models, most\nlikely thanks to their more focused and narrower\npretraining stage.\n140\nInterestingly, a more general domain of pre-\ntraining texts can lead to a better domain-specific\nsummarization performance. In Section 4.4.1, our\nViT5 models while being trained on a more gen-\neral corpus (CC100), outperform current models\nthat are trained on news-related corpus. More\ntechnical domains such as laws, medicals, or en-\ngineering are not tested as we leave these domain-\nspecific summarization tasks for future studies.\nThe slightly better performance of\nViT5base 1024-length compared to ViT5 base 256-length\nsuggests that longer document summarization\n(more than 512 tokens) need a comparatively\nlonger context length during the pretraining\nstage.\n7 Conclusion\nWe introduce ViT5, a pretrained sequence-to-\nsequence Transformer model for the Vietnamese\nlanguage. Leveraging the T5 self-supervised pre-\ntraining formulation on massive and high-quality\nVietnamese corpora, we showed that finetuned\nViT5 models are performant on both generation\nand classification tasks. We exhaustively com-\npare ViT5 with other pretrained formulations on\nboth multilingual and monolingual corpora. Our\nexperiments show that ViT5 achieves state-of-the-\nart results on summarization in both Wikilingua\nand Vietnews corpus, and competitive results in\ngenerating Named Entity Recognition (NER) on\nthe PhoNER COVID19 dataset. We also analyze\nand discuss the importance of context length dur-\ning the self-supervised pretraining stage, which\nstrongly influences and positively leads to better\ndownstream performance.\n8 Acknowledgements\nWe would like to thank the Google TPU Research\nCloud (TRC) program and VietAI for providing us\nwith free access to TPU v3-8 to train and finetune\nlarge ViT5 models.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot\nlearners. CoRR, abs/2005.14165.\nThe Viet Bui, Oanh Thi Tran, and Phuong Le-Hong.\n2020. Improving sequence tagging for vietnamese\ntext using transformer-based neural models. CoRR,\nabs/2006.15994.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nZi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao\nJiang, and Graham Neubig. 2021. Gsum: A general\nframework for guided neural abstractive summariza-\ntion. In Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summariza-\ntion. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 4098–4109, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nEMNLP.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen R. McKeown. 2020. Wikilingua: A new bench-\nmark dataset for cross-lingual abstractive summa-\nrization. CoRR, abs/2010.03093.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2019. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. CoRR, abs/1910.13461.\n141\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In EMNLP/IJCNLP.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. CoRR,\nabs/2001.08210.\nDat Quoc Nguyen and Anh Tuan Nguyen. 2020.\nPhoBERT: Pre-trained language models for Viet-\nnamese. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 1037–\n1042.\nHieu Nguyen, Long Phan, James Anibal, Alec Pel-\ntekian, and Hieu Tran. 2021. Viesum: How robust\nare transformer-based models on vietnamese sum-\nmarization?\nMinh-Tien Nguyen, Hoang-Diep Nguyen, Thi-Hai-\nNang Nguyen, and Van-Hau Nguyen. 2018. To-\nwards state-of-the-art baselines for vietnamese\nmulti-document summarization. In 2018 10th In-\nternational Conference on Knowledge and Systems\nEngineering (KSE), pages 85–90.\nVan-Hau Nguyen, Thanh-Chinh Nguyen, Minh-Tien\nNguyen, and Nguyen Hoai. 2019. Vnds: A viet-\nnamese dataset for summarization. pages 375–380.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. CoRR, abs/1802.05365.\nLong Phan, Hieu Tran, Daniel Le, Hieu Nguyen,\nJames Annibal, Alec Peltekian, and Yanfang Ye.\n2021a. CoTexT: Multi-task learning with code-text\ntransformer. In Proceedings of the 1st Workshop\non Natural Language Processing for Programming\n(NLP4Prog 2021), pages 40–47, Online. Associa-\ntion for Computational Linguistics.\nLong N. Phan, James T. Anibal, Hieu Tran, Shau-\nrya Chanana, Erol Bahadroglu, Alec Peltekian, and\nGr´egoire Altan-Bonnet. 2021b. Scifive: a text-\nto-text transformer model for biomedical literature.\nCoRR, abs/2106.03598.\nHuy To Quoc, Kiet Van Nguyen, Ngan Luu-Thuy\nNguyen, and Anh Gia-Tuan Nguyen. 2021. Mono-\nlingual versus multilingual bertology for vietnamese\nextractive multi-document summarization.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. CoRR, abs/1910.10683.\nGabriele Sarti and Malvina Nissim. 2022. It5: Large-\nscale text-to-text pretraining for italian language un-\nderstanding and generation.\nHuy Quoc To, Kiet Van Nguyen, Ngan Luu-Thuy\nNguyen, and Anh Gia-Tuan Nguyen. 2021. Mono-\nlingual versus multilingual bertology for vietnamese\nextractive multi-document summarization.\nNguyen Luong Tran, Duong Minh Le, and Dat Quoc\nNguyen. 2021. Bartpho: Pre-trained sequence-to-\nsequence models for vietnamese.\nThinh Hung Truong, Mai Hoang Dao, and Dat Quoc\nNguyen. 2021. COVID-19 Named Entity Recog-\nnition for Vietnamese. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm ´an, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. CoRR, abs/2010.11934.\n142"
}