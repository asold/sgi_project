{
  "title": "BERTić -- The Transformer Language Model for Bosnian, Croatian, Montenegrin and Serbian",
  "url": "https://openalex.org/W3153208071",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222398571",
      "name": "Ljubešić, Nikola",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lauc, Davor",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3099771192",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2972734857",
    "https://openalex.org/W3112685591",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "In this paper we describe a transformer model pre-trained on 8 billion tokens of crawled text from the Croatian, Bosnian, Serbian and Montenegrin web domains. We evaluate the transformer model on the tasks of part-of-speech tagging, named-entity-recognition, geo-location prediction and commonsense causal reasoning, showing improvements on all tasks over state-of-the-art models. For commonsense reasoning evaluation, we introduce COPA-HR -- a translation of the Choice of Plausible Alternatives (COPA) dataset into Croatian. The BERTić model is made available for free usage and further task-specific fine-tuning through HuggingFace.",
  "full_text": "arXiv:2104.09243v1  [cs.CL]  19 Apr 2021\nBERTi´c - The T ransformer Language Model for\nBosnian, Croatian, Montenegrin and Serbian\nNikola Ljube ˇsi ´c\nJo ˇ zef Stefan Institute\nJamova cesta 39\nLjubljana, Slovenia\nnikola.ljubesic@ijs.si\nDavor Lauc\nFaculty of Humanities and Social Sciences\nIvana Lu ˇ ci ´ ca 3\nZagreb, Croatia\ndavor.lauc@ffzg.hr\nAbstract\nIn this paper we describe a transformer model\npre-trained on 8 billion tokens of crawled\ntext from the Croatian, Bosnian, Serbian and\nMontenegrin web domains. W e evaluate\nthe transformer model on the tasks of part-\nof-speech tagging, named-entity-recognition,\ngeo-location prediction and commonsense\ncausal reasoning, showing improvements on\nall tasks over state-of-the-art models. For com-\nmonsense reasoning evaluation we introduce\nCOP A-HR - a translation of the Choice of Plau-\nsible Alternatives (COP A) dataset into Croat-\nian. The BERTi´c model is made available for\nfree usage and further task-speciﬁc ﬁne-tuning\nthrough HuggingFace.\n1 Introduction\nIn recent years, pre-trained transformer models\nhave taken the NLP world by storm (\nDevlin et al. ,\n2018; Liu et al. , 2019; Brown et al. , 2020), yield-\ning new state-of-the-art results in various tasks\nand settings. While such models, requiring signiﬁ-\ncant computing power and data quantity , started to\nemerge for non-English languages (\nMartin et al. ,\n2019; de Vries et al. , 2019), as well as in multilin-\ngual ﬂavours ( Devlin et al. , 2018; Conneau et al. ,\n2019), there is a signiﬁcant number of languages\nfor which better models can be obtained with the\navailable pre-training techniques.\nThis paper describes such an effort - training a\ntransformer language model on more than 8 bil-\nlion tokens of text written in the Bosnian, Croatian,\nMontenegrin or Serbian language, all these lan-\nguages being very closely related, mutually intel-\nligible, and classiﬁed under the same HBS (Serbo-\nCroatian) macro-language by the ISO-693-3 stan-\ndard.\n1\nThe name of the model – BERTi´c – points at\ntwo facts: (1) the language model was trained in\n1 https://iso639-3.sil.org/code_tables/macrolanguage_mappings/data\nZagreb, Croatia, in whose vernacular diminutives\nending in i´c are frequently used ( foti´c eng. photo\ncamera, smajli´c eng. smiley , hengi´c eng. hanging\ntogether), and (2) in all the countries / languages\nof this model the patronymic surnames end to a\ngreat part with the sufﬁx i´c, with likely diminutive\netymology .\nThe paper is structured as follows: in the fol-\nlowing section we describe the data the model is\nbased on, in the third section we give a short de-\nscription of the modelling performed, and in the\nfourth section we present a detailed evaluation of\nthe model.\n2 Data\nAs our data basis we use already existing\ndatasets, namely (1) the hrW aC corpus of\nthe Croatian top-level domain, crawled in\n2011 (\nLjubeˇ si´ c and Erjavec, 2011) and again\nin 2014 ( Ljubeˇ si´ c and Klubiˇ cka, 2014), (2) the\nsrW aC corpus of the Serbian top-level domain,\ncrawled in 2014 (\nLjubeˇ si´ c and Klubiˇ cka, 2014),\n(3) the bsW aC corpus of the Bosnian top-level\ndomain, crawled in 2014 ( Ljubeˇ si´ c and Klubiˇ cka,\n2014), (4) the cnrW aC corpus of the Montene-\ngrin top-level domain, crawled in 2019, and (5)\nthe Riznica corpus consisting of Croatian liter-\nary works and newspapers (\n´Cavar and Ronˇ cevi´ c,\n2012).\nGiven that most of the crawls contain data only\nup to year 2014, we performed new crawls of the\nBosnian, Croatian and Serbian top-level domains.\nW e brand these corpora as CLASSLA web corpora\ngiven that CLASSLA is the CLARIN knowledge\ncentre for South Slavic languages\n2 under which\nwe perform most of the described activities. W e\ndeduplicate the CLASSLA corpora by removing\nidentical sentences that were already present in\n2 https://www.clarin.si/info/k-centre/\ndataset language # of words\nhrW aC Croatian 1,250,923,836\nCLASSLA-hr Croatian 1,341,494,461\ncc100-hr Croatian 2,880,490,449\nRiznica Croatian 87,724,737\nsrW aC Serbian 493,202,149\nCLASSLA-sr Serbian 752,916,260\ncc100-sr Serbian 711,014,370\nbsW aC Bosnian 256,388,597\nCLASSLA-bs Bosnian 534,074,921\ncnrW aC Montenegrin 79,451,738\nT able 1: Datasets used for training the BERTi´c model\nwith their size (in number of words) after deduplica-\ntion.\nthe W aC corpora. The amount of data removed\nthrough this deduplication is minor, in all cases in\nsingle digit percentages.\nW e further exploit the recently published cc100\ncorpora (\nConneau et al. , 2019) that are based on\nthe CommonCrawl data collection. W e per-\nform the same level of deduplication as with the\nCLASSLA corpora, with every sentence already\npresent in the W aC or CLASSLA corpus being\nremoved from the cc100 corpus. This round of\ndeduplication removed around 15% of the Com-\nmonCrawl data.\nThe resulting sizes of the datasets used for\ntraining the BERTi´c model are presented in T a-\nble\n1. The overall text collection consists of\n8,387,681,518 words.\n3 Model training\nFor training this model we selected the Electra ap-\nproach to training transformer models (\nClark et al. ,\n2020). These models are based on training a\nsmaller generator model and the main, larger,\ndiscriminator model whose task is to discrimi-\nnate whether a speciﬁc word is the original word\nfrom the text, or a word generated by the gener-\nator model. The authors claim that the Electra\napproach is computationally more efﬁcient than\nthe BERT models (\nDevlin et al. , 2018) based on\nmasked language modelling.\nAs in BERT and similar transformers models,\nwe constructed a W ordPiece vocabulary with a vo-\ncabulary size of 32 thousand tokens. A W ord-\nPiece model was trained using the HuggingFace\ntokenizers library\n3 on the random sample of 10\n3 https://huggingface.co/transformers/main_classes/tokenizer.html\nmillion paragraphs from the whole dataset. T ext\npre-processing and cleaning differ from the origi-\nnal BERT only in preserving all Unicode charac-\nters, while in the original pre-processing diacritics\nare removed.\nTraining of the model was performed to the\nmost part with the hyperparameters set for base-\nsized models (110 million parameters in 12 trans-\nformer layers) as deﬁned in the Electra pa-\nper (\nClark et al. , 2020). Training batch size was\nkept at 1024, the maximum size for the 8 TPUv3\nunits on which the training was performed. The\ntraining was run for 2 million steps (roughly 50\nepochs).\n4 Evaluation\nIn this section we present an exhaustive evaluation\nof the newly trained BERTi´c model on two token\nclassiﬁcation tasks – morphosyntactic tagging and\nnamed entity recognition, and two sequence clas-\nsiﬁcation tasks – geolocation prediction and com-\nmonsense causative reasoning.\nThe reference points in each task are the\nstate-of-the art transformer models cov-\nering the macro-language - multilingual\nBERT (\nDevlin et al. , 2018) and CroSloEn-\ngual BERT ( Ulˇ car and Robnik- ˇSikonja, 2020).\nWhile multilingual BERT ( mBERT onwards) was\ntrained on Wikipedia corpora, CroSloEngual\nBERT ( cseBERT onwards) was trained on a\nsimilar amount of Croatian data used to train\nBERTi´c, but without the data from the remaining\nlanguages.\n4.1 Morphosyntactic tagging\nOn the task of morphosyntactic tagging (assign-\ning each word one among multiple hundreds of\ndetailed morphosyntactic classes, e.g. Ncmsay\nreferring to a common masculine noun, in ac-\ncusative case, singular number, animate) we\ncompare the three transformer models, mBERT,\ncseBERT and BERTi´c. W e additionally report\nresults, when available, for the current produc-\ntion tagger for the two languages - the CLASSLA\ntool (\nLjubeˇ si´ c and Dobrovoljc, 2019), based on\nStanford’s Stanza, exploiting static embedding\nand BiLSTM technology (\nQi et al. , 2020).\nW e perform evaluation of the models on this\ntask on four datasets: the Croatian standard lan-\nguage dataset hr500k (\nLjubeˇ si´ c et al., 2018), the\nCroatian non-standard language dataset ReLDI-\ndataset language variety CLASSLA mBERT cseBERT BERTi ´c\nhr500k Croatian standard 93.87 94.60 95.74 ***95.81\nreldi-hr Croatian non-standard - 88.87 91.63 ***92.28\nSETimes.SR Serbian standard 95.00 95.50 96.41 96.31\nreldi-sr Serbian non-standard - 91.26 93.54 ***93.90\nT able 2: A verage microF1 results on the morphosyntactic ann otation task over ﬁve training iterations. The highest\nscore per dataset is marked with bold. The statistical signi ﬁcance is tested with the two-sided t-test over the ﬁve\nruns between the two strongest results. Level of signiﬁcanc e is labeled with asteriks signs (*** p <=0.001).\ndataset language variety CLASSLA mBERT cseBERT BERTi ´c\nhr500k Croatian standard 80.13 85.67 88.98 ****89.21\nReLDI-hr Croatian non-standard - 76.06 81.38 ****83.05\nSETimes.SR Serbian standard 84.64 92.41 92.28 92.02\nReLDI-sr Serbian non-standard - 81.29 82.76 ***87.92\nT able 3: A verage F1 results on the named entity recognition t ask over ﬁve training iterations. The highest score per\ndataset is marked with bold. The statistical signiﬁcance is tested with the two-sided t-test over the ﬁve runs between\nthe two strongest results. Level of signiﬁcance is labeled w ith asteriks signs (*** p <=0.001, **** p <=0.0001).\nhr ( Ljubeˇ si´ c et al., 2019a), the Serbian standard\nlanguage dataset SETimes.SR ( Batanovi´ c et al.,\n2018) and the Serbian non-standard T witter lan-\nguage dataset ReLDI-sr ( Ljubeˇ si´ c et al., 2019b).\nFor each dataset and model we perform hyper-\nparameter optimization via Bayesian search on the\nwandb.ai platform ( Biewald, 2020), allowing for\n30 iterations. W e optimize the initial learning rate\n(we search between the values of 9e-6 and 1e-4)\nand the epoch number (we search between the val-\nues of 3 and 15).\nW e report average microF1 results of ﬁve runs\nper dataset and model in T able\n2. The highest\nscore per dataset is marked with bold. The statisti-\ncal signiﬁcance is tested with the two-sided t-test\nover the ﬁve runs between the two highest average\nresults. W e can observe that the BERTi´c model\noutperforms all the remaining models, cseBERT\ncoming second, on three out of four datasets. Only\non the Serbian standard dataset the difference be-\ntween these two models is insigniﬁcant. W e argue\nthat this is due to the simplicity of the dataset - it\nconsists of texts from one newspaper only , there-\nfore containing text with little variation even be-\ntween the training and the testing data.\n4.2 Named entity recognition\nOn the task of named entity recognition we com-\npare the same models on the same datasets as was\nthe case in the previous Section\n4.1. W e also\nperform an identical hyperparameter optimisation\nand experimentation and report the results in T a-\nble\n3. The results show again that the two best per-\nforming models are cseBERT and BERTi´c with\nthe latter performing better on three out of four\ndatasets, again, with no signiﬁcant difference on\nthe standard Serbian task for the same reasons as\nwith the previous task.\n4.3 Social media geolocation\nIn this subsection we compare the three trans-\nformer models on the Social Media Geolocation\n(SMG2020) shared task, which part of the V arDial\n2020 Evaluation Campaign (\nGaman et al. , 2020).\nThe task consists of predicting the exact lati-\ntude and longitude of a geo-encoded tweet pub-\nlished in Croatia, Bosnia, Montenegro or Serbia.\nThe shared task winner in 2020 was using the\ncseBERT model (\nScherrer and Ljubeˇ si´ c, 2020) in\nits approach.\nW e evaluate the model on the two evaluation\nmetrics of the shared task - median and mean of\nthe distance between gold and predicted geoloca-\ntions. Given the large size of the training dataset\n(320,042 instances), we do not perform any addi-\ntional hyperparameter tuning beyond the one per-\nformed during the participation in the shared task\nand apply the same methodology: we ﬁne-tune\nthe transformer model with batch size of 64 for\n40 epochs and retain the model with minimum\nmedian distance on development data. The re-\nsults in T able\n4 show that the BERTi´c model im-\nproved the results of the shared task winner – the\ncseBERT model.\nmedian mean\ncentroid 107.10 145.72\nmBERT 42.25 82.05\ncseBERT 40.76 81.88\nBERTi´c 37.96 79.30\nT able 4: Median distance and mean distance between\ngold and predicted geolocation (lower is better) on the\ntask of social media geolocation prediction. The best\nresults are marked in bold. No statistical testing was\nperformed due to a large size of the test dataset (39,723\ninstances).\n4.4 Commonsense causal reasoning\nThe ﬁnal evaluation round of the new BERTi´c\nmodel is performed on the task of commonsense\ncausal reasoning on a translation of the COP A\ndataset (\nRoemmele et al. , 2011) into Croatian, the\nCOP A-HR dataset. The translation is performed\nby following the methodology laid out while\npreparing the XCOP A dataset (\nPonti et al. , 2020),\na translation of the COP A dataset into 11 typolog-\nically balanced languages.\nThe dataset consists of 400 training, 100 devel-\nopment and 500 examples. Each instance in the\ndataset consists of a premise ( The man broke his\ntoe), a question ( What was the cause? ),\n4 and two\nalternatives, one of them to be chosen by the sys-\ntem as being more plausible ( He got a hole in his\nsock, or He dropped a hammer on his foot ).\nWhile translating the dataset, the translator was\nalso given the task of selecting the more plausible\nalternative given their translation. The observed\nagreement between the annotations in the English\ndataset and the annotations of the Croatian trans-\nlator was perfect on the training set and the devel-\nopment set, while on the test set one out of 500\nchoices differed. The problematic example proved\nto be a rather unclear case – the premise being\nI paused to stop talking. , with the question What\nwas the cause? , and the alternatives I lost my voice.\nand I ran out of breath. .\n5 The dataset is avail-\nable from the CLARIN.SI repository ( Ljubeˇ si´ c,\n2021).6\nThe approach taken to benchmarking the three\ntransformer models is that of sentence pair classi-\nﬁcation, each original instance becoming two sen-\n4 Roughly half of the instances contain the other question:\nWhat was the effect? ,\n5 The Croatian translator chose the second alternative,\nwhile in the original dataset the ﬁrst alternative is chosen .\n6 http://hdl.handle.net/11356/1404\naccuracy\nrandom 50.00\nmBERT 54.12\ncseBERT 61.80\nBERTi´c **65.76\nT able 5: A verage accuracy results on the commonsense\ncausal reasoning task over ﬁve training iterations. The\nhighest score per dataset is marked with bold. The sta-\ntistical signiﬁcance is tested with the two-sided t-test\nover the ﬁve runs between the two strongest results (**\np<=0.01).\ntence pair instances (each sentence pair contain-\ning the premise and one alternative), with differ-\nent models being trained for cause and effect ques-\ntions. During evaluation, separate predictions are\nmade on each of the alternatives, the per-class pre-\ndictions being fed to a softmax function, and the\nhigher positive-class alternative being chosen as\nthe correct one.\nThe standard evaluation metric for this dataset is\naccuracy . Given the balanced nature of the test set,\nthe random baseline is 50%. For hyperparameter\noptimization the same approach was taken as with\nthe token classiﬁcation tasks.\nThe results presented in T able\n5 show that both\nlanguage-speciﬁc transformer models outperform\nmBERT signiﬁcantly , with BERTi´c obtaining a\nsigniﬁcant lead over cseBERT.\n5 Conclusion\nIn this paper we have presented a newly published\nElectra transformer language model, BERTi´c,\ntrained on more than 8 billion tokens of previ-\nously and newly collected web text written in\nBosnian, Croatian, Montenegrin or Serbian. W e\nhave applied a very thorough evaluation of the\nmodel, comparing it primarily to other state-of-\nthe-art transformer models that support the lan-\nguages in question. W e have obtained signiﬁcant\nimprovements on all four tasks, with no difference\nobtained only on one single-source-dataset with\nlittle text variation and high training and testing\ndata similarity .\nThe main conclusions we can draw from our re-\nsults are the following. (1) Although cseBERT\nand BERTi´c use a different approach to build-\ning transformer language models, our assumption\nis that the performance difference between these\ntwo lies primarily in the larger amount of data pre-\nsented to the BERTi´c model. (2) The improve-\nments on the four tasks with the BERTi´c model\nseem to be smaller on the morphosyntactic tagging\ntask than the remaining three tasks that require\nmore world and commonsense reasoning knowl-\nedge. (3) Except for the named entity recogni-\ntion task on the Serbian non-standard dataset, we\nfail to observe greater improvements on Serbian\ntasks than on Croatian ones between cseBERT\nand BERTi´c, regardless the fact that the former\nhas seen none and the latter has seen huge quanti-\nties of Serbian text, showing the irrelevance of mi-\nnor language differences for performance of large\ntransformer models. (4) While BiLSTM models\nare still close-to-competitive on the morphosyntac-\ntic tagging task, they cannot hold up on the named\nentity recognition task as it requires more common\nknowledge. Such knowledge transformer models\nmanage to absorb to a much higher level than pre-\ntrained static embeddings used by BiLSTMs.\nThe BERTi´c model is available\nfrom the HuggingFace repository at\nhttps://huggingface.co/classla/bcms-bertic.\nAcknowledgments\nThis work has been supported by the Slovenian Re-\nsearch Agency and the Flemish Research Founda-\ntion through the bilateral research project ARRS\nN6-0099 and FWO G070619N “The linguistic\nlandscape of hate speech on social media”, the\nSlovenian Research Agency research core fund-\ning No. P6-0411 “Language resources and tech-\nnologies for Slovene language”, and the Euro-\npean Union’s Rights, Equality and Citizenship\nProgramme (2014-2020) project IMSyPP (grant\nno. 875263). W e would like to thank the anony-\nmous reviewers and Ivo-Pavao Jazbec for their use-\nful feedback.\nReferences\nV uk Batanovi´ c, Nikola Ljubeˇ si´ c, T anja\nSamard ˇ zi´ c, and T oma ˇ z Erjavec. 2018.\nTraining corpus SETimes.SR 1.0 . Slovenian\nlanguage resource repository CLARIN.SI.\nLukas Biewald. 2020.\nExperiment tracking with weights and biases .\nSoftware available from wandb.com.\nT om B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165 .\nDamir ´Cavar and Dunja Brozovi´ c Ron ˇ cevi´ c. 2012.\nRiznica: the Croatian language corpus. Prace ﬁlo-\nlogiczne, 63:51–65.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555 .\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nV ishrav Chaudhary, Guillaume W enzek, Francisco\nGuzm ´ an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and V eselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116 .\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805 .\nMihaela Gaman, Dirk Hovy, Radu Tudor Ionescu,\nHeidi Jauhiainen, T ommi Jauhiainen, Krister\nLind ´ en, Nikola Ljubeˇ si´ c, Niko Partanen, Christoph\nPurschke, Yves Scherrer, et al. 2020. A report on the\nV arDial evaluation campaign 2020. In Proceedings\nof the 7th W orkshop on NLP for Similar Languages,\nV arieties and Dialects . International Committee on\nComputational Linguistics.\nY inhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and V eselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692 .\nNikola Ljubeˇ si´ c. 2021.\nChoice of plausible alternatives dataset in Croatian COP A- HR.\nSlovenian language resource repository\nCLARIN.SI.\nNikola Ljubeˇ si´ c, ˇZeljko Agi´ c, Filip Klubiˇ cka,\nV uk Batanovi´ c, and T oma ˇ z Erjavec. 2018.\nTraining corpus hr500k 1.0 . Slovenian language\nresource repository CLARIN.SI.\nNikola Ljubeˇ si´ c and Kaja Dobrovoljc. 2019.\nWhat does neural bring? analysing improvements in morphosy ntactic annotation and lemmatisation of Slovenian, Croati an and Serbian .\nIn Proceedings of the 7th W orkshop on Balto-Slavic\nNatural Language Processing , pages 29–34,\nFlorence, Italy. Association for Computational\nLinguistics.\nNikola Ljubeˇ si´ c and T oma ˇ z Erjavec. 2011. hrW aC\nand slW aC: Compiling web corpora for Croatian\nand Slovene. In International Conference on T ext,\nSpeech and Dialogue , pages 395–402. Springer.\nNikola Ljubeˇ si´ c, T oma ˇ z Erjavec, V uk Batanovi´ c,\nMaja Miliˇ cevi´ c, and T anja Samard ˇ zi´ c. 2019a.\nCroatian twitter training corpus ReLDI-NormT agNER-hr 2.1 .\nSlovenian language resource repository\nCLARIN.SI.\nNikola Ljubeˇ si´ c, T oma ˇ z Erjavec, V uk Batanovi´ c,\nMaja Miliˇ cevi´ c, and T anja Samard ˇ zi´ c. 2019b.\nSerbian twitter training corpus ReLDI-NormT agNER-sr 2.1 .\nSlovenian language resource repository\nCLARIN.SI.\nNikola Ljubeˇ si´ c and Filip Klubiˇ cka. 2014. {bs, hr, sr }\nwac-web corpora of Bosnian, Croatian and Serbian.\nIn Proceedings of the 9th W eb as Corpus W orkshop\n(W aC-9), pages 29–35.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSu ´ arez, Y oann Dupont, Laurent Romary, ´Eric V ille-\nmonte de la Clergerie, Djam ´ e Seddah, and Benoˆ ıt\nSagot. 2019. Camembert: a tasty french language\nmodel. arXiv preprint arXiv:1911.03894 .\nEdoardo Maria Ponti, Goran Glavaˇ s, Olga Majewska,\nQianchu Liu, Ivan V uli´ c, and Anna Korhonen. 2020.\nXCOP A: A multilingual dataset for causal commonsense reaso ning.\nIn Proceedings of the 2020 Conference on Empir-\nical Methods in Natural Language Processing\n(EMNLP), pages 2362–2376, Online. Association\nfor Computational Linguistics.\nPeng Qi, Y uhao Zhang, Y uhui Zhang, Jason Bolton,\nand Christopher D Manning. 2020. Stanza:\nA Python natural language processing toolkit\nfor many human languages. arXiv preprint\narXiv:2003.07082 .\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of Plausible Alterna-\ntives: An Evaluation of Commonsense Causal Rea-\nsoning. In AAAI Spring Symposium: Logical F or-\nmalizations of Commonsense Reasoning , pages 90–\n95.\nYves Scherrer and Nikola Ljubeˇ si´ c. 2020. HeLju@\nV arDial 2020: Social media variety geolocation with\nBER T models. In Proceedings of the 7th W orkshop\non NLP for Similar Languages, V arieties and Di-\nalects, pages 202–211.\nMatej Ulˇ car and Marko Robnik- ˇSikonja. 2020. FinEst\nBER T and CroSloEngual BER T. In International\nConference on T ext, Speech, and Dialogue , pages\n104–111. Springer.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, T ommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. Bertje: A dutch BER T\nmodel. arXiv preprint arXiv:1912.09582 .",
  "topic": "Serbian",
  "concepts": [
    {
      "name": "Serbian",
      "score": 0.9245816469192505
    },
    {
      "name": "Croatian",
      "score": 0.8811016082763672
    },
    {
      "name": "Bosnian",
      "score": 0.8659741878509521
    },
    {
      "name": "Transformer",
      "score": 0.5234277248382568
    },
    {
      "name": "Computer science",
      "score": 0.4298853278160095
    },
    {
      "name": "Linguistics",
      "score": 0.3966655433177948
    },
    {
      "name": "Engineering",
      "score": 0.18120956420898438
    },
    {
      "name": "Electrical engineering",
      "score": 0.10621663928031921
    },
    {
      "name": "Voltage",
      "score": 0.0832194983959198
    },
    {
      "name": "Philosophy",
      "score": 0.0758202075958252
    }
  ],
  "institutions": []
}