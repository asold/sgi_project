{
  "title": "Two-Level Transformer and Auxiliary Coherence Modeling for Improved Text Segmentation",
  "url": "https://openalex.org/W2997244287",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5079336821",
      "name": "Goran Glavaš",
      "affiliations": [
        "University of Mannheim"
      ]
    },
    {
      "id": "https://openalex.org/A1210890480",
      "name": "Swapna Somasundaran",
      "affiliations": [
        "Educational Testing Service"
      ]
    },
    {
      "id": "https://openalex.org/A5079336821",
      "name": "Goran Glavaš",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1210890480",
      "name": "Swapna Somasundaran",
      "affiliations": [
        "Educational Testing Service"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W105299784",
    "https://openalex.org/W2798908575",
    "https://openalex.org/W2140676672",
    "https://openalex.org/W6633119129",
    "https://openalex.org/W6639619044",
    "https://openalex.org/W2105685762",
    "https://openalex.org/W2080179128",
    "https://openalex.org/W2017776283",
    "https://openalex.org/W1626945812",
    "https://openalex.org/W6679356491",
    "https://openalex.org/W1983814883",
    "https://openalex.org/W6678441938",
    "https://openalex.org/W342285082",
    "https://openalex.org/W1871378960",
    "https://openalex.org/W2251951653",
    "https://openalex.org/W2018627475",
    "https://openalex.org/W2914073025",
    "https://openalex.org/W2512217112",
    "https://openalex.org/W6999244237",
    "https://openalex.org/W6674877832",
    "https://openalex.org/W6636440780",
    "https://openalex.org/W2794904608",
    "https://openalex.org/W2251356693",
    "https://openalex.org/W2118612506",
    "https://openalex.org/W2525695389",
    "https://openalex.org/W6742186100",
    "https://openalex.org/W1862888253",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2893316907",
    "https://openalex.org/W2594021297",
    "https://openalex.org/W2148818577",
    "https://openalex.org/W1525245406",
    "https://openalex.org/W2057399676",
    "https://openalex.org/W2774605568",
    "https://openalex.org/W2606288287",
    "https://openalex.org/W2963165489",
    "https://openalex.org/W2740181799",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2962716111",
    "https://openalex.org/W2123849094",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2626534681",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W4238205294",
    "https://openalex.org/W2128709346",
    "https://openalex.org/W1612003148",
    "https://openalex.org/W2100873065",
    "https://openalex.org/W2964266061",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2952190837",
    "https://openalex.org/W2970037872",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Breaking down the structure of long texts into semantically coherent segments makes the texts more readable and supports downstream applications like summarization and retrieval. Starting from an apparent link between text coherence and segmentation, we introduce a novel supervised model for text segmentation with simple but explicit coherence modeling. Our model – a neural architecture consisting of two hierarchically connected Transformer networks – is a multi-task learning model that couples the sentence-level segmentation objective with the coherence objective that differentiates correct sequences of sentences from corrupt ones. The proposed model, dubbed Coherence-Aware Text Segmentation (CATS), yields state-of-the-art segmentation performance on a collection of benchmark datasets. Furthermore, by coupling CATS with cross-lingual word embeddings, we demonstrate its effectiveness in zero-shot language transfer: it can successfully segment texts in languages unseen in training.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nTwo-Level Transformer and Auxiliary\nCoherence Modeling for Improved Text Segmentation\nGoran Glavaˇs,1 Swapna Somasundaran2\n1Data and Web Science Research Group\nUniversity of Mannheim\ngoran@informatik.uni-mannheim.de\n2Educational Testing Service (ETS)\nssomasundaran@ets.org\nAbstract\nBreaking down the structure of long texts into semantically\ncoherent segments makes the texts more readable and supports\ndownstream applications like summarization and retrieval.\nStarting from an apparent link between text coherence and\nsegmentation, we introduce a novel supervised model for text\nsegmentation with simple but explicit coherence modeling.\nOur model – a neural architecture consisting of two hierar-\nchically connected Transformer networks – is a multi-task\nlearning model that couples the sentence-level segmentation\nobjective with the coherence objective that differentiates cor-\nrect sequences of sentences from corrupt ones. The proposed\nmodel, dubbed Coherence-Aware Text Segmentation (CATS),\nyields state-of-the-art segmentation performance on a collec-\ntion of benchmark datasets. Furthermore, by coupling CATS\nwith cross-lingual word embeddings, we demonstrate its ef-\nfectiveness in zero-shot language transfer: it can successfully\nsegment texts in languages unseen in training.\nIntroduction\nNatural language texts are, more often than not, a result\nof a deliberate cognitive effort of an author and as such\nconsist of semantically coherent segments. Text segmenta-\ntion deals with automatically breaking down the structure of\ntext into such topically contiguous segments, i.e., it aims to\nidentify the points of topic shift (Hearst 1994; Choi 2000;\nBrants, Chen, and Tsochantaridis 2002; Riedl and Biemann\n2012; Du, Buntine, and Johnson 2013; Glavaˇs, Nanni, and\nPonzetto 2016; Koshorek et al . 2018). Reliable segmenta-\ntion results with texts that are more readable for humans,\nbut also facilitates downstream tasks like automated text\nsummarization (Angheluta, De Busser, and Moens 2002;\nBokaei, Sameti, and Liu 2016), passage retrieval (Huang et\nal. 2003; Shtekh et al. 2018), topical classiﬁcation (Zirn et\nal. 2016), or dialog modeling (Manuvinakurike et al. 2016;\nZhao and Kawahara 2017).\nText coherence is inherently tied to text segmentation –\nintuitively, the text within a segment is expected to be more\ncoherent than the text spanning different segments. Consider,\ne.g., the text in Figure 1, with two topical segments. Snippets\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nAmsterdam is younger than Dutch cities such \nas Nijmegen, Rotterdam, and Utrecht.  \n \nAmsterdam was granted city rights in either \n1300 or 1306.  \n \nIn the 14th century Amsterdam flourished \nbecause of trade with the Hanseatic League. \n \nAmsterdam is located in the Western \nNetherlands. \n \nThe river Amstel ends in the city centre and \nconnects to numerous canals. \n \nAmsterdam is about 2 metres (6.6 feet) \nbelow sea level. \nT1 \nT2 \nT3 \nT4 \nFigure 1: Snippet illustrating the relation (i.e., dependency)\nbetween text coherence and segmentation.\nT1 and T2 are more coherent than T3 and T4: all T1 sentences\nrelate to Amsterdam’s history, and all T2 sentences to Ams-\nterdam’s geography; in contrast, T3 and T4 contain sentences\nfrom both topics. T1 and T2 being more coherent than T3 and\nT4 signals that the fourth sentence starts a new segment.\nGiven this duality between text segmentation and coher-\nence, it is surprising that the methods for text segmentation\ncapture coherence only implicitly. Unsupervised segmen-\ntation models rely either on probabilistic topic modeling\n(Brants, Chen, and Tsochantaridis 2002; Riedl and Biemann\n2012; Du, Buntine, and Johnson 2013) or semantic similarity\nbetween sentences (Glavaˇs, Nanni, and Ponzetto 2016), both\nof which only indirectly relate to text coherence. Similarly,\na recently proposed state-of-the-art supervised neural seg-\nmentation model (Koshorek et al\n. 2018) directly learns to\npredict binary sentence-level segmentation decisions and has\nno explicit mechanism for modeling coherence.\nIn this work, in contrast, we propose a supervised neural\nmodel for text segmentation that explicitly takes coherence\ninto account: we augment the segmentation prediction objec-\ntive with an auxiliary coherence modeling objective. Our pro-\nposed model, dubbed Coherence-Aware Text Segmentation\n(CATS), encodes a sentence sequence using two hierarchi-\ncally connected Transformer networks (Vaswani et al. 2017;\nDevlin et al. 2018). Similar to (Koshorek et al. 2018), CATS’\n7797\nmain learning objective is a binary sentence-level segmen-\ntation prediction. However, CATS augments the segmen-\ntation objective with an auxiliary coherence-based objec-\ntive which pushes the model to predict higher coherence for\noriginal text snippets than for corrupt (i.e., fake) sentence\nsequences. We empirically show (1) that even without the\nauxiliary coherence objective, the Two-Level Transformer\nmodel for Text Segmentation (TLT-TS) yields state-of-the-art\nperformance across multiple benchmarks, (2) that the full\nCATS model, with the auxiliary coherence modeling, fur-\nther signiﬁcantly improves the segmentation, and (3) that\nboth TLT-TS and CATS are robust in domain transfer. Fur-\nthermore, we demonstrate models’ effectiveness in zero-shot\nlanguage transfer. Coupled with a cross-lingual word embed-\nding space,1 our models trained on English Wikipedia suc-\ncessfully segment texts from unseen languages, outperform-\ning the best-performing unsupervised segmentation model\n(Glavaˇs, Nanni, and Ponzetto 2016) by a wide margin.\nCATS: Coherence-Aware Two-Level\nTransformer for Text Segmentation\nFigure 2 illustrates the high-level architecture of the CATS\nmodel. A snippet of text – a sequence of sentences of ﬁxed\nlength – is an input to the model. Token encodings are a con-\ncatenation of a pretrained word embedding and a positional\nembedding. Sentences are ﬁrst encoded from their tokens\nwith a token-level Transformer (Vaswani et al. 2017). Next,\nwe feed the sequence of obtained sentence representations\nto the second, sentence-level Transformer. Transformed (i.e.,\ncontextualized) sentence representations are next fed to the\nfeed-forward segmentation classiﬁer, which makes a binary\nsegmentation prediction for each sentence. We additionally\nfeed the encoding of the whole snippet (i.e., the sentence\nsequence) to the coherence regressor (a feed-forward net),\nwhich predicts a coherence score. In what follows, we de-\nscribe each component in more detail.\nTransformer-Based Segmentation\nThe segmentation decision for a sentence clearly does not\ndepend only on its content but also on its context, i.e., in-\nformation from neighboring sentences. In this work, we\nemploy the encoding stack of the attention-based Trans-\nformer architecture (Vaswani et al\n. 2017) to contextual-\nize both token representations in a sentence and, more im-\nportantly, sentence representations within the snippet. We\nchoose Transfomer encoders because (1) they have recently\nbeen reported to outperform recurrent encoders on a range\nof NLP tasks (Devlin et al\n. 2018; Radford et al . 2018;\nShaw, Uszkoreit, and Vaswani 2018) and (2) they are faster\nto train than recurrent nets.\nSentence Encoding. Let S = {S1,S2,...,S K } denote a\nsingle training instance – a snippet consisting of K sentences\nand let each sentence Si = {ti\n1,ti\n2\n,...,t i\nT } be a ﬁxed-size\n1See (Ruder, Søgaard, and Vuli´c 2018; Glavaˇse ta l. 2019) for\na comprehensive overview of methods for inducing cross-lingual\nword embeddings.\nInput snippet (sentence sequence)\nS1: [ss] [amsterdam] [was] [granted] [city] [rights]...\nS2: [ss] [in] [the] [14th] [century] [amsterdam] ...\nS3: [ss] [amsterdam] [is] [located] [in] [the] ...\n...\nSK: [ss] [amsterdam] [is] [about] [2] [metres] ...  \nword embedding \nlookup\npositional \nembedding\ntoken encoding layer\ntoken-level transformer\nmulti-head attention\nadd & normalize\nfeed-forward net\nadd & normalize\nNTT\nX\nsentence representations\n<sss>    S1 S2 S3 ... SK:\nmulti-head attention\nadd & normalize\nfeed-forward net\nadd & normalize\nNTS\nX\nsentence-level transformer\nfeed-forward net\nsoftmax\n<sss>:\nS1:     \nS2: \nS3:   \n...\nSK:\ntransformed sentence \nrepresentations\nSegmentation classifier\nfeed-forward net\nCoherence regressor\nCoherence\nscore\nSegmentation probabilities\n(for each sentence)\nFigure 2: High-level depiction of the Coherence-Aware Text\nSegmentation (CATS) model.\n7798\nsequence of T tokens.2 Following (Devlin et al. 2018), we\nprepend each sentence Si with a special sentence start token\nti\n0 = [ss], aiming to use the transformed representation of\nthat token as the sentence encoding. 3 We encode each to-\nken ti\nj (i ∈{ 1,...,K }, j ∈{ 0,1,...,T }) with a vector ti\nj\nwhich is the concatenation of a de-dimensional word embed-\nding and a dp-dimensional embedding of the position j.W e\nuse pretrained word embeddings and ﬁx them in training;\nwe learn positional embeddings as model’s parameters. Let\nTransformT denote the encoder stack of the Transformer\nmodel (Vaswani et al. 2017), consisting of NTT layers, each\ncoupling a multi-head attention net with a feed-forward net.4\nWe then apply TransformT to the token sequence of each\nsnippet sentence:\n{tti\nj}T\nj=0\n= TransformT\n(\n{ti\nj}T\nj=0\n)\n; (1)\nThe sentence encoding is then the transformed vector of the\nsentence start token [ss]: si = tti\n0.\nSentence Contextualization. Sentence encodings {si}K\ni=1\nproduced with TransformT only capture the content of the\nsentence itself, but not its context. We thus employ a second,\nsentence-level Transformer TransformS (with NTS layers)\nto produce context-informed sentence representations. We\nprepend each sequence of non-contextualized sentence em-\nbeddings {si}K\ni=1 with a ﬁxed embedding s0, denoting the\nsnippet start token <sss>, in order to capture the encoding\nof the whole snippet (i.e., sequence of K sentences) as the\ntransformed embedding of the <sss> token:\n{ssi}K\ni=0 = TransformS\n(\n{si}K\ni=0\n)\n; (2)\nwith the transformed vector ss0 being the encoding of the\nwhole snippet S.\nSegmentation Classiﬁcation. Finally, contextualized sen-\ntence vectors ssi go into the segmentation classiﬁer, a single-\nlayer feed-forward net coupled with softmax function:\nˆyi = softmax(ssiWseg +bseg); (3)\nwith Wseg ∈ R(de+dp)×2 and bseg ∈ R2 as classiﬁer’s\nparameters. Let yi ∈{ [0,1],[1,0]} be the true segmentation\nlabel of the i-th sentence. The segmentation loss Jseg is then\nthe simple negative log-likelihood over all sentences of allN\nsnippets in the training batch:\nJseg = −\nN∑\nn=1\nK∑\ni=1\nlnˆyn\ni ·yn\ni . (4)\nAuxiliary Coherence Modeling\nGiven the obvious dependency between segmentation and\ncoherence, we pair the segmentation task with an auxiliary\n2We trim/pad sentences longer/shorter than T tokens.\n3This eliminates the need for an additional self-attention layer\nfor aggregating transformed token vectors into a sentence encoding.\n4For more details on the encoding stack of the Transformer\nmodel, see the original publication (Vaswani et al. 2017).\ntask of predicting snippet coherence. To this effect, we couple\neach true snippet S from the original text with a corrupt (i.e.,\nincoherent) snippet S, created by (1) randomly shufﬂing the\norder of sentences in S and (2) randomly replacing sentences\nfrom S, with other document sentences.\nLet (S,S) be a pair of a true snippet and its corrupt coun-\nterpart, and (ss0, ss0) their respective encodings, obtained\nwith the Two-Level Transformer. The encodings of the cor-\nrect snippet (ss0) and the scrambled snippet ( ss0) are then\npresented to the coherence regressor, which independently\ngenerates a coherence score for each of them. The scalar\noutput of the coherence regressor is:\nˆyS = ss0wc +bc; ˆyS = ss0wc +bc; (5)\nwith wc ∈ Rde+dp and bc ∈ R as regressor’s parameters. We\nthen jointly softmax-normalize the scores for S and S:\n[coh(S),coh(S)] =softmax\n(\n[ˆyS,ˆyS]\n)\n. (6)\nWe want to force the model to produce higher coherence\nscore for the correct snippet S than for its corrupt counterpart\nS. We thus deﬁne the following contrastive margin-based\ncoherence objective:\nJcoh =m a x\n(\n0,δcoh −(coh(S)−coh(ˆS))\n)\n(7)\nwhere δcoh is the margin by which we would like coh(S) to\nbe larger than coh(S).\nCreating Training Instances\nOur presumed training corpus contains documents that are\ngenerally longer than the snippet size K and annotated for\nsegmentation at the sentence level. We create training in-\nstances by sliding a sentence window of size\nK over doc-\numents’ sentences with a stride of K/2. For the sake of\nauxiliary coherence modeling, for each original snippet S,\nwe create its corrupt counterpart S with the following cor-\nruption procedure: (1) we ﬁrst randomly shufﬂe the order\nof sentences in S; (2) for p1 percent of snippets (random\nselection) we additionally replace sentences of the shufﬂed\nsnippet (with the probability p2) with randomly chosen sen-\ntences from other, non-overlapping document snippets.\nInference\nAt inference time, given a long document, we need to make a\nbinary segmentation decision for each sentence. Our model,\nhowever, does not take individual sentences as input, but\nrather sequences of\nK sentences (i.e., snippets) and makes\nin-context segmentation prediction for each sentence. Since\nwe can create multiple different sequences of K consecu-\ntive sentences that contain some sentence S,5 our model\ncan obtain multiple segmentation predictions for the same\nsentence. As we do not know apriori which of the snippets\ncontaining the sentence\nS is the most reliable with respect\nto the segmentation prediction for S, we consider all possi-\nble snippets containing S. In other words, at inference time,\n5Sliding the sentence window with the stride of 1, the m-th\nsentence will, in the general case, be found in K different snippets:\n[m −K +1: m], [m −K +2: m +1],..., [m : m +K −1].\n7799\nunlike in training, we create snippets by sliding the window\nof K sentences over the document with the stride of 1. Let\nS = {S1,S2,..., SK } be the set of (at most) K different\nsnippets containing a sentence S. We then average the seg-\nmentation probabilities predicted for the sentence S over all\nsnippets in S:6\nPseg(S)= 1\nK\n∑\nSk∈S\nˆyS (Sk)[0] (8)\nFinally, we predict that S starts a new segment if Pseg(S) >\nτ, where τ is the conﬁdence threshold, tuned as a hyperpa-\nrameter of the model.\nCross-Lingual Zero-Shot Transfer\nModels that do not require any language-speciﬁc features\nother than pretrained word embeddings as input can (at\nleast conceptually) be easily transferred to another lan-\nguage by means of a cross-lingual word embedding space\n(Ruder, Søgaard, and Vuli\n´c 2018; Glavaˇse ta l. 2019). Let\nXL1 be the monolingual embedding space of the source\nlanguage (most often English), which we use in training\nand let XL2 be the independently trained embedding space\nof the target language to which we want to transfer the\nsegmentation model. To transfer the model, we need to\nproject target-language vectors from\nXL2 to the source-\nlanguage space XL1. There is a plethora of recently pro-\nposed methods for inducing projection-based cross-lingual\nembeddings (Faruqui and Dyer 2014; Smith et al\n. 2017;\nArtetxe, Labaka, and Agirre 2018; Vuli ´ce ta l. 2019, inter\nalia). We opt for the supervised alignment model based on\nsolving the Procrustes problem (Smith et al . 2017), due to\nits simplicity and competitive performance in zero-shot lan-\nguage transfer of NLP models (Glavaˇse ta l. 2019). Given a\nlimited-size word translation training dictionary D,w eo b -\ntain the linear projection matrix WL2→L1 between XL2 and\nXL1 as follows:\nWL2→L1 = UV⊤ ; UΣV⊤ = SVD(XSXT\n⊤ ); (9)\nwith XS ⊂ XL1 and XT ⊂ XL2 as subsets of mono-\nlingual spaces that align vectors from training translations\npairs from D. Once we obtain WL2→L1, the language trans-\nfer of the segmentation model is straightforward: we in-\nput the embeddings of L2 words from the projected space\nX′\nL2 = XL2WL2→L1.\nExperimental Setup\nWe ﬁrst describe datasets used for training and evaluation\nand then provide the details on the comparative evaluation\nsetup and model optimization.\nData\nWIKI -727K Cor pus. Koshorek et al. (2018) leveraged the\nmanual structuring of Wikipedia pages into sections to au-\ntomatically create a large segmentation-annotated corpus.\nWIKI -727K consists of 727,746 documents created from\n6The ﬁrst element (i.e., index [0]) of the predicted vector ˆy\ndenotes the (positive) segmentation probability.\nEnglish (EN) Wikipedia pages, divided into training (80%),\ndevelopment (10%), and test portions (10%). We train, opti-\nmize, and evaluate our models on respective portions of the\nWIKI -727K dataset.\nStandard Test Corpora. Koshorek et al. (2018) addition-\nally created a small evaluation set W IKI -50 to allow for\ncomparative evaluation against unsupervised segmentation\nmodels, e.g., the G\nRAPH SEG model of Glavaˇs, Nanni, and\nPonzetto (2016), for which evaluation on large datasets is\nprohibitively slow. For years, the synthetic dataset of Choi\n(2000) was used as a standard becnhmark for text segmen-\ntation models. C\nHOI dataset contains 920 documents, each\nof which is a concatenation of 10 paragraphs randomly sam-\npled from the Brown corpus. C HOI dataset is divided into\nsubsets containing only documents with speciﬁc variability\nof segment lengths (e.g., segments with 3-5 or with 9-11 sen-\ntences).7 Finally, we evaluate the performance of our models\non two small datasets, C ITIES and ELEMENTS , created by\nChen et al . (2009) from Wikipedia pages dedicated to the\ncities of the world and chemical elements, respectively.\nOther Languages. In order to test the performance of\nour Transformer-based models in zero-shot language trans-\nfer setup, we prepared small evaluation datasets in other\nlanguages. Analogous to the W\nIKI -50 dataset created by\nKoshorek et al. (2018) from English (EN) Wikipedia, we cre-\nated WIKI -50-CS, W IKI -50-FI, and W IKI -50-TR datasets\nconsisting of 50 randomly selected pages from Czech (CS),\nFinnish (FI), and Turkish (TR) Wikipedia, respectively.8\nComparative Evaluation\nEvaluation Metric. Following previous work (Riedl and\nBiemann 2012; Glavaˇs, Nanni, and Ponzetto 2016; Koshorek\net al. 2018), we also adopt the standard text segmentation\nmeasure Pk (Beeferman, Berger, and Lafferty 1999) as our\nevaluation metric. Pk score is the probability that a model\nmakes a wrong prediction as to whether the ﬁrst and last sen-\ntence of a randomly sampled snippet ofk sentences belong to\nthe same segment (i.e., the probability of the model predict-\ning the same segment for the sentences from different seg-\nment or different segments for the sentences from the same\nsegment). Following (Glava ˇs, Nanni, and Ponzetto 2016;\nKoshorek et al. 2018), we set k to the half of the average\nground truth segment size of the dataset.\nBaseline Models. We compare CATS against the state-of-\nthe-art neural segmentation model of Koshorek et al. (2018)\nand against GRAPH SEG (Glavaˇs, Nanni, and Ponzetto 2016),\n7Following Koshorek et al. (2018), we evaluate our models on\nthe whole CHOI corpus and not on speciﬁc subsets.\n8For our language transfer experiments we selected target lan-\nguages from different families and linguistic typologies w.r.t English\nas our source language: Czech is, like English, an Indo-European\nlanguage (but as a Slavic language it is, unlike English, fusional\nby type); Finnish is an Uralic language (fusionally-agglutinative by\ntype); whereas Turkish is a Turkic language (agglutinative by type).\n7800\nthe state-of-the-art unsupervised text segmentation model.\nAdditionally, as a sanity check, we evaluate the R ANDOM\nbaseline – it assigns a positive segmentation label to a sen-\ntence with the probability that corresponds to the ratio of the\ntotal number of segments (according to the gold segmenta-\ntion) and total number of sentences in the dataset.\nModel Conﬁguration\nModel Variants. We evaluate two variants of our two-level\ntransformer text segmentation model: with and without the\nauxiliary coherence modeling. The ﬁrst model, TLT-TS, min-\nimizes only the segmentation objective Jseg. CATS, our sec-\nond model, is a multi-task learning model that alternately\nminimizes the segmentation objective\nJseg and the coher-\nence objective Jcoh. We adopt a balanced alternate training\nregime for CATS in which a single parameter update based\non the minimization of Jseg is followed by a single parameter\nupdate based on the optimization of Jcoh.\nWord Embeddings. In all our experiments we use 300-\ndimensional monolingual FAST TEXT word embeddings pre-\ntrained on the Common Crawl corpora of respective lan-\nguages: EN, CS, FI, and TR.9 We induce a cross-lingual word\nembedding space, needed for the zero-shot language trans-\nfer experiments, by projecting CS, FI, and TR monolingual\nembedding spaces to the EN embedding space. Following\n(Smith et al . 2017; Glavaˇse ta l . 2019), we create training\ndictionaries D for learning projection matrices by machine\ntranslating 5,000 most frequent EN words to CS, FI, and TR.\nModel Optimization. We optimize all hyperparameters,\nincluding the data preparation parameters like the snippet\nsize K, via cross-validation on the development portion\nof the Wiki-727K dataset. We found the following conﬁg-\nuration to lead to robust\n10 performance for both TLT-TS\nand CATS: (1) training instance preparation: snippet size of\nK =1 6sentences with T =5 0tokens; scrambling proba-\nbilities p1 = p2 =0 .5; (2) conﬁguration of Transformers:\nNTT = NTS =6 layers and with 4attention heads per layer\nin both transformers;11 (3) other model hyperparameters: po-\nsitional embedding size of dp =1 0; coherence objective\ncontrastive margin of δcoh =1 . We found different optimal\ninference thresholds: τ =0 .5for the segmentation-only TLT-\nTS model and τ =0 .3for the coherence-aware CATS model.\nWe trained both TLT-TS and CATS in batches of N =3 2\nsnippets (each with K =1 6sentences), using the Adam op-\ntimization algorithm (Kingma and Ba 2014) with the initial\nlearning rate set to 10−4.\n9https://tinyurl.com/y6j4gh9a\n10Given the large hyperparameter space and large training set, we\nonly searched over a limited-size grid of hyperparameter conﬁgu-\nrations. It is thus likely that a better-performing conﬁguration than\nthe one reported can be found with a more extensive grid search.\n11We do not tune other transformer hyperparameters, but rather\nadopt the recommended values from (Vaswani et al. 2017): ﬁlter size\nof 1024 and dropout probabilities of 0.1 for both attention layers\nand feed-forward ReLu layers.\nResults and Discussion\nWe ﬁrst present and discuss the results that our models, TLT-\nTS and CATS, yield on the previously introduced EN eval-\nuation datasets. We then report and analyze models’ perfor-\nmance in the cross-lingual zero-shot transfer experiments.\nBase Evaluation\nTable 1 shows models’ performance on ﬁve EN evaluation\ndatasets. Both our Transformer-based models – TLT-TS and\nCATS – outperform the competing supervised model of\nKoshorek et al. (2018), a hierarchical encoder based on re-\ncurrent components, across the board. The improved per-\nformance that TLT-TS has with respect to the model of\nKoshorek et al. (2018) is consistent with improvements that\nTransformer-based architectures yield in comparison with\nmodels based on recurrent components in other NLP tasks\n(Vaswani et al. 2017; Devlin et al . 2018). The gap in per-\nformance is particularly wide ( >20 Pk points) for the E L-\nEMENTS dataset. Evaluation on the E LEMENTS test set is,\narguably, closest to a true domain-transfer setting:12 while the\ntrain portion of the WIKI -727K set contains pages similar in\ntype to those found in WIKI -50 and C ITIES test sets, it does\nnot contain any Wikipedia pages about chemical elements\n(all such pages are in the E LEMENTS test set). This would\nsuggest that TLT-TS and CATS offer more robust domain\ntransfer than the recurrent model of Koshorek et al. (2018).\nCATS signiﬁcantly13 and consistently outperforms TLT-\nTS. This empirically conﬁrms the usefulness of explicit co-\nherence modeling for text segmentation. Moreover, Koshorek\net al. (2018) report human performance on the W IKI -50\ndataset of 14.97, which is a mere one Pk point better than\nthe performance of our coherence-aware CATS model.\nThe unsupervised G RAPH SEG model of Glavaˇs, Nanni,\nand Ponzetto (2016) seems to outperform all supervised mod-\nels on the synthetic C HOI dataset. We believe that this is\nprimarily because (1) by being synthetic, the CHOI dataset\ncan be accurately segmented based on simple lexical overlaps\nand word embedding similarities (and GRAPH SEG relies on\nsimilarities between averaged word embeddings) and because\n(2) by being trained on a much more challenging real-world\nWIKI -727K dataset – on which lexical overlap is insufﬁcient\nfor accurate segmentation – supervised models learn to seg-\nment based on deeper natural language understanding (and\nlearn not to encode lexical overlap as reliable segmentation\nsignal). Additionally, G RAPH SEG is evaluated separately\non each subset of the C HOI dataset, for each of which it\nis provided the (gold) minimal segment size, which further\nfacilitates and improves its predicted segmentations.\nZero-Shot Cross-Lingual Transfer\nIn Table 2 we show the results of our zero-shot cross-lingual\ntransfer experiments. In this setting, we use our Transformer-\n12The C HOI dataset – albeit from a different domain – is syn-\nthetic, which impedes direct performance comparisons with other\nevaluation datasets.\n13According to the non-parametric random shufﬂing test (Yeh\n2000): p< 0.01 for WIKI -727K, C HOI and CITIES ; p< 0.05 for\nWIKI -50 and E LEMENTS .\n7801\nModel Model Type W IKI -727K W IKI -50 C HOI CITIES ELEMENTS\nRANDOM unsupervised 53.09 52.65 49.43 47.14 50.08\nGRAPH SEG unsupervised – 63.56 5.6–7.2* 39.95 49.12\nKoshorek et al. (2018) supervised 22.13 18.24 26.26 19.68 41.63\nTLT-TS supervised 19.41 17.47 23.26 19.21 20.33\nCATS supervised 15.95 16.53 18.50 16.85 18.41\nTable 1: Performance of text segmentation models on ﬁve English evaluation datasets. GRAPH SEG model (Glavaˇs, Nanni, and\nPonzetto 2016) was evaluated independently on different subcorpora of the CHOI dataset (indicated with an asterisk).\nModel CS FI TR\nRANDOM 52.92 52.02 45.04\nGRAPH SEG 49.47 49.28 39.21\nTLT-TS 24.27 25.99 25.89\nCATS 22.32 22.87 24.20\nTable 2: Performance of text segmentation models in zero-\nshot language transfer setting on the WIKI -50-X (X ∈{ CS,\nFI, TR}) datasets.\nbased models, trained on the English W IKI -727K dataset,\nto segment texts from the WIKI -50-X (X ∈{ CS, FI, TR})\ndatasets in other languages. As a baseline, we additionally\nevaluate GRAPH SEG (Glavaˇs, Nanni, and Ponzetto 2016), as\na language-agnostic model requiring only pretrained word\nembeddings of the test language as input.\nBoth our Transformer-based models, TLT-TS and CATS,\noutperform the unsupervised G RAPH SEG model (which\nseems to be only marginally better than the random base-\nline) by a wide margin. The coherence-aware CATS model\nis again signiﬁcantly better (p< 0.01 for FI and p< 0.05\nfor CS and TR) than the TLT-TS model which was trained to\noptimize only the segmentation objective. While the results\non the WIKI -50-{CS, FI, TR} datasets are not directly com-\nparable to the results reported on the EN WIKI -50 (see Table\n1) because the datasets in different languages do not contain\nmutually comparable Wikipedia pages, results in Table 2 still\nsuggest that the drop in performance due to the cross-lingual\ntransfer is not big. This is quite encouraging as it suggests\nthat it is possible to, via the zero-shot language transfer, rather\nreliably segment texts from under-resourced languages lack-\ning sufﬁciently large gold-segmented data needed to directly\ntrain language-speciﬁc segmentation models (that is, robust\nneural segmentation models in particular).\nRelated Work\nIn this work we address the task of text segmentation – we\nthus provide a detailed account of existing segmentation\nmodels. Because our CATS model has an auxiliary coherence-\nbased objective, we additionally provide a brief overview of\nresearch on modeling text coherence.\nText Segmentation\nText segmentation tasks come in two main ﬂavors: (1)\nlinear (i.e., sequential) text segmentation and (2) hierar-\nchical segmentation in which top-level segments are fur-\nther broken down into sub-segments. While the hierarchi-\ncal segmentation received a non-negligible research atten-\ntion (Yaari 1997; Eisenstein 2009; Du, Buntine, and John-\nson 2013), the vast majority of the proposed models (in-\ncluding this work) focus on linear segmentation (Hearst\n1994; Beeferman, Berger, and Lafferty 1999; Choi 2000;\nBrants, Chen, and Tsochantaridis 2002; Misra et al\n. 2009;\nRiedl and Biemann 2012; Glavaˇs, Nanni, and Ponzetto 2016;\nKoshorek et al. 2018, inter alia).\nIn one of the pioneering segmentation efforts, Hearst\n(1994) proposed an unsupervised TextTiling algorithm based\non the lexical overlap between adjacent sentences and para-\ngraphs. Choi (2000) computes the similarities between sen-\ntences in a similar fashion, but renormalizes them within the\nlocal context; the segments are then obtained through divisive\nclustering. Utiyama and Isahara (2001) and Fragkou, Petridis,\nand Kehagias (2004) minimize the segmentation cost via\nexhaustive search with dynamic programming.\nFollowing the assumption that topical cohesion guides\nthe segmentation of the text, a number of segmentation ap-\nproaches based on topic models have been proposed. Brants,\nChen, and Tsochantaridis (2002) induce latent representa-\ntions of text snippets using probabilistic latent semantic anal-\nysis (Hofmann 1999) and segment based on similarities be-\ntween latent representations of adjacent snippets. Misra et\nal. (2009) and Riedl and Biemann (2012) leverage topic vec-\ntors of snippets obtained with the Latent Dirichlet Allocation\nmodel (Blei, Ng, and Jordan 2003). While Misra et al\n. (2009)\nﬁnds a globally optimal segmentation based on the similari-\nties of snippets’ topic vectors using dynamic programming,\nRiedl and Biemann (2012) adjust the TextTiling model of\n(Hearst 1994) to use topic vectors instead of sparse lexical-\nized representations of snippets.\nMalioutov and Barzilay (2006) proposed a ﬁrst graph-\nbased model for text segmentation. They segment lecture\ntranscripts by ﬁrst inducing a fully connected sentence graph\nwith edge weights corresponding to cosine similarities be-\ntween sparse bag-of-word sentence vectors and then running\na minimum normalized multiway cut algorithm to obtain\nthe segments. Glava\nˇs, Nanni, and Ponzetto (2016) propose\nGRAPH SEG, a graph-based segmentation algorithm similar\nin nature to (Malioutov and Barzilay 2006), which uses dense\nsentence vectors, obtained by aggregating word embeddings,\nto compute intra-sentence similarities and performs segmen-\ntation based on the cliques of the similarity graph.\nFinally, Koshorek et al. (2018) identify Wikipedia as a free\n7802\nlarge-scale source of manually segmented texts that can be\nused to train a supervised segmentation model. They train a\nneural model that hierarchically combines two bidirectional\nLSTM networks and report massive improvements over unsu-\npervised segmentation on a range of evaluation datasets. The\nmodel we presented in this work has a similar hierarchical ar-\nchitecture, but uses Transfomer networks instead of recurrent\nencoders. Crucially, CATS additionally deﬁnes an auxiliary\ncoherence objective, which is coupled with the (primary)\nsegmentation objective in a multi-task learning model.\nText Coherence\nMeasuring text coherence amounts to predicting a score that\nindicates how meaningful the order of the information in the\ntext is. The majority of the proposed text coherence models\nare grounded in formal theories of text coherence, among\nwhich the entity grid model (Barzilay and Lapata 2008),\nbased on the centering theory of Grosz, Weinstein, and Joshi\n(1995), is arguably the most popular. The entity grid model\nrepresent texts as matrices encoding the grammatical roles\nthat the same entities have in different sentences. The en-\ntity grid model, as well as its extensions (Elsner and Char-\nniak 2011; Feng and Hirst 2012; Feng, Lin, and Hirst 2014;\nNguyen and Joty 2017) require text to be preprocessed –\nentities extracted and grammatical roles assigned to them –\nwhich prohibits an end-to-end model training.\nIn contrast, Li and Hovy (2014) train a neural model that\ncouples recurrent and recursive sentence encoders with a\nconvolutional encoder of sentence sequences in an end-to-end\nfashion on limited-size datasets with gold coherence scores.\nOur models’ architecture is conceptually similar, but we use\nTransformer networks to both encode sentences and sentence\nsequences. With the goal of supporting text segmentation\nand not aiming to predict exact coherence scores, our model\ndoes not require gold coherence labels; instead we devise\na coherence objective that contrasts original text snippets\nagainst corrupted sentence sequences.\nConclusion\nThough the segmentation of text depends on its (local) co-\nherence, existing segmentation models capture coherence\nonly implicitly via lexical or semantic overlap of (adjacent)\nsentences. In this work, we presented CATS, a novel super-\nvised model for text segmentation that couples segmentation\nprediction with explicit auxiliary coherence modeling. CATS\nis a neural architecture consisting of two hierarchically con-\nnected Transformer networks: the lower-level sentence en-\ncoder generates input for the higher-level encoder of sentence\nsequences. We train the model in a multi-task learning setup\nby learning to predict (1) sentence segmentation labels and\n(2) that original text snippets are more coherent than corrupt\nsentence sequences. We show that CATS yields state-of-the-\nart performance on several text segmentation benchmarks and\nthat it can – in a zero-shot language transfer setting, coupled\nwith a cross-lingual word embedding space – successfully\nsegment texts from target languages unseen in training.\nAlthough effective for text segmentation, our coherence\nmodeling is still rather simple: we use only fully randomly\nshufﬂed sequences as examples of (highly) incoherent text.\nIn subsequent work, we will investigate negative instances\nof different degree of incoherence as well as more elaborate\nobjectives for (auxiliary) modeling of text coherence.\nReferences\nAngheluta, R.; De Busser, R.; and Moens, M.-F. 2002. The\nuse of topic segmentation for automatic summarization. In\nProc. of the ACL-2002 Workshop on Automatic Summariza-\ntion, 11–12.\nArtetxe, M.; Labaka, G.; and Agirre, E. 2018. A robust\nself-learning method for fully unsupervised cross-lingual\nmappings of word embeddings. In Proc. of ACL, 789–798.\nBarzilay, R., and Lapata, M. 2008. Modeling local coher-\nence: An entity-based approach. Computational Linguistics\n34(1):1–34.\nBeeferman, D.; Berger, A.; and Lafferty, J. 1999. Statistical\nmodels for text segmentation.Machine learning34(1-3):177–\n210.\nBlei, D. M.; Ng, A. Y .; and Jordan, M. I. 2003. Latent\ndirichlet allocation. Journal of machine Learning research\n3(Jan):993–1022.\nBokaei, M. H.; Sameti, H.; and Liu, Y . 2016. Extractive\nsummarization of multi-party meetings through discourse\nsegmentation. Natural Language Engineering22(1):41–72.\nBrants, T.; Chen, F.; and Tsochantaridis, I. 2002. Topic-based\ndocument segmentation with probabilistic latent semantic\nanalysis. In Proc. of CIKM, 211–218. ACM.\nChen, H.; Branavan, S.; Barzilay, R.; and Karger, D. R. 2009.\nGlobal models of document structure using latent permuta-\ntions. In Proc. of Human Language Technologies: The 2009\nAnnual Conference of the North American Chapter of the\nAssociation for Computational Linguistics, 371–379. Associ-\nation for Computational Linguistics.\nChoi, F. Y . 2000. Advances in domain independent linear\ntext segmentation. In 1st Meeting of the North American\nChapter of the Association for Computational Linguistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDu, L.; Buntine, W.; and Johnson, M. 2013. Topic seg-\nmentation with a structured topic model. In Proc. of the\n2013 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, 190–200.\nEisenstein, J. 2009. Hierarchical text segmentation from\nmulti-scale lexical cohesion. In Proc. of HLT-NAACL, 353–\n361. Association for Computational Linguistics.\nElsner, M., and Charniak, E. 2011. Extending the entity\ngrid with entity-speciﬁc features. In Proc. of the 49th Annual\nMeeting of the Association for Computational Linguistics:\nHuman Language Technologies, 125–129.\nFaruqui, M., and Dyer, C. 2014. Improving vector space\nword representations using multilingual correlation. In Proc.\nof EACL, 462–471.\n7803\nFeng, V . W., and Hirst, G. 2012. Extending the entity-\nbased coherence model with multiple ranks. In Proc. of the\n13th Conference of the European Chapter of the Association\nfor Computational Linguistics, 315–324. Association for\nComputational Linguistics.\nFeng, V . W.; Lin, Z.; and Hirst, G. 2014. The impact of deep\nhierarchical discourse structures in the evaluation of text\ncoherence. In Proc. of COLING 2014, the 25th International\nConference on Computational Linguistics: Technical Papers,\n940–949.\nFragkou, P.; Petridis, V .; and Kehagias, A. 2004. A dynamic\nprogramming algorithm for linear text segmentation. Journal\nof Intelligent Information Systems23(2):179–197.\nGlavaˇs, G.; Litschko, R.; Ruder, S.; and Vuli´c, I. 2019. How\nto (properly) evaluate cross-lingual word embeddings: On\nstrong baselines, comparative analyses, and some miscon-\nceptions. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 710–721. Flo-\nrence, Italy: Association for Computational Linguistics.\nGlavaˇs, G.; Nanni, F.; and Ponzetto, S. P. 2016. Unsupervised\ntext segmentation using semantic relatedness graphs. In Proc.\nof the Fifth Joint Conference on Lexical and Computational\nSemantics, 125–130.\nGrosz, B. J.; Weinstein, S.; and Joshi, A. K. 1995. Centering:\nA framework for modeling the local coherence of discourse.\nComputational linguistics 21(2):203–225.\nHearst, M. A. 1994. Multi-paragraph segmentation of exposi-\ntory text. In Proc. of the 32nd annual meeting on Association\nfor Computational Linguistics, 9–16. Association for Com-\nputational Linguistics.\nHofmann, T. 1999. Probabilistic latent semantic analysis. In\nProc. of the Fifteenth conference on Uncertainty in artiﬁcial\nintelligence, 289–296. Morgan Kaufmann Publishers Inc.\nHuang, X.; Peng, F.; Schuurmans, D.; Cercone, N.; and\nRobertson, S. E. 2003. Applying machine learning to text\nsegmentation for information retrieval. Information Retrieval\n6(3-4):333–362.\nKingma, D. P., and Ba, J. 2014. Adam: A method for stochas-\ntic optimization. arXiv preprint arXiv:1412.6980.\nKoshorek, O.; Cohen, A.; Mor, N.; Rotman, M.; and Berant,\nJ. 2018. Text segmentation as a supervised learning task. In\nProc. of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, V olume 2 (Short Papers), 469–473.\nLi, J., and Hovy, E. 2014. A model of coherence based\non distributed sentence representation. In Proc. of the 2014\nConference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), 2039–2048.\nMalioutov, I., and Barzilay, R. 2006. Minimum cut model\nfor spoken lecture segmentation. In Proc. of COLING-ACL,\n25–32. Association for Computational Linguistics.\nManuvinakurike, R.; Paetzel, M.; Qu, C.; Schlangen, D.;\nand DeVault, D. 2016. Toward incremental dialogue act\nsegmentation in fast-paced interactive dialogue systems. In\nProc. of the 17th Annual Meeting of the Special Interest\nGroup on Discourse and Dialogue, 252–262.\nMisra, H.; Yvon, F.; Jose, J. M.; and Cappe, O. 2009. Text\nsegmentation via topic modeling: An analytical study. In\nProc. of CIKM, 1553–1556. ACM.\nNguyen, D. T., and Joty, S. 2017. A neural local coherence\nmodel. In Proc. of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (V olume 1: Long Papers),\n1320–1330.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training. Technical Report. Preprint.\nRiedl, M., and Biemann, C. 2012. Topictiling: a text segmen-\ntation algorithm based on lda. In Proc. of ACL 2012 Student\nResearch Workshop, 37–42. Association for Computational\nLinguistics.\nRuder, S.; Søgaard, A.; and Vuli´c, I. 2018. A survey of cross-\nlingual embedding models. arXiv preprint arXiv:1706.04902.\nShaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-attention\nwith relative position representations. In Proc. of the 2018\nConference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, V olume 2 (Short Papers), 464–468.\nShtekh, G.; Kazakova, P.; Nikitinsky, N.; and Skachkov, N.\n2018. Exploring inﬂuence of topic segmentation on informa-\ntion retrieval quality. In International Conference on Internet\nScience, 131–140. Springer.\nSmith, S. L.; Turban, D. H.; Hamblin, S.; and Hammerla,\nN. Y . 2017. Ofﬂine bilingual word vectors, orthogonal\ntransformations and the inverted softmax. In Proc. of ICLR.\nUtiyama, M., and Isahara, H. 2001. A statistical model for\ndomain-independent text segmentation. In Proc. of ACL.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention\nis all you need. In Advances in neural information processing\nsystems, 5998–6008.\nVuli´c, I.; Glavaˇs, G.; Reichart, R.; and Korhonen, A. 2019.\nDo we really need fully unsupervised cross-lingual embed-\ndings? In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 4398–4409.\nYaari, Y . 1997. Segmentation of expository texts by hierar-\nchical agglomerative clustering. In Proc. of RANLP.\nYeh, A. 2000. More accurate tests for the statistical signiﬁ-\ncance of result differences. In Proc. of COLING, 947–953.\nZhao, T., and Kawahara, T. 2017. Joint learning of dialog act\nsegmentation and recognition in spoken dialog using neural\nnetworks. In Proc. of IJCNLP, 704–712.\nZirn, C.; Glava ˇs, G.; Nanni, F.; Eichorts, J.; and Stucken-\nschmidt, H. 2016. Classifying topics and detecting topic\nshifts in political manifestos. In Proceedings of the Interna-\ntional Conference on the Advances in Computational Analy-\nsis of Political Text, 88–93. University of Zagreb.\n7804",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.7622533440589905
    },
    {
      "name": "Computer science",
      "score": 0.7401374578475952
    },
    {
      "name": "Automatic summarization",
      "score": 0.6381105780601501
    },
    {
      "name": "Transformer",
      "score": 0.6285299062728882
    },
    {
      "name": "Natural language processing",
      "score": 0.6144841313362122
    },
    {
      "name": "Artificial intelligence",
      "score": 0.606532096862793
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.5503612160682678
    },
    {
      "name": "Sentence",
      "score": 0.5180023312568665
    },
    {
      "name": "Text segmentation",
      "score": 0.46085232496261597
    },
    {
      "name": "Language model",
      "score": 0.4461381435394287
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.35421860218048096
    },
    {
      "name": "Mathematics",
      "score": 0.1087522804737091
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177802217",
      "name": "University of Mannheim",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I1341030882",
      "name": "Educational Testing Service",
      "country": "US"
    }
  ]
}