{
  "title": "CLAIR: Evaluating Image Captions with Large Language Models",
  "url": "https://openalex.org/W4389524413",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2059092488",
      "name": "David Chan",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2963506946",
      "name": "Suzanne Petryk",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2165931820",
      "name": "Joseph Gonzalez",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2174985400",
      "name": "Trevor Darrell",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2101610026",
      "name": "John Canny",
      "affiliations": [
        "University of California, Berkeley"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2159243025",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W4378510407",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W2964042428",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W3021989929",
    "https://openalex.org/W2425121537",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W3153469116",
    "https://openalex.org/W68733909",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3034417909",
    "https://openalex.org/W4353115969",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W3087871082",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W2970858040",
    "https://openalex.org/W4377121434",
    "https://openalex.org/W2962735233",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4296151226",
    "https://openalex.org/W2226480163"
  ],
  "abstract": "The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6% and over image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the underlying reasoning behind its assigned score.",
  "full_text": "CLAIR: Evaluating Image Captions with Large Language Models\nDavid M. Chan, Suzanne Petryk, Joseph E. Gonzalez, Trevor Darrell, John Canny\nUniversity of California, Berkeley\n{davidchan,spetryk,jegonzal,trevordarrell,canny}@berkeley.edu\nAbstract\nThe evaluation of machine-generated image\ncaptions poses an interesting yet persistent\nchallenge. Effective evaluation measures\nmust consider numerous dimensions of\nsimilarity, including semantic relevance, visual\nstructure, object interactions, caption diversity,\nand specificity. Existing highly-engineered\nmeasures attempt to capture specific aspects,\nbut fall short in providing a holistic score that\naligns closely with human judgments. Here, we\npropose CLAIR1, a novel method that leverages\nthe zero-shot language modeling capabilities\nof large language models (LLMs) to evaluate\ncandidate captions. In our evaluations, CLAIR\ndemonstrates a stronger correlation with human\njudgments of caption quality compared to\nexisting measures. Notably, on Flickr8K-\nExpert, CLAIR achieves relative correlation\nimprovements over SPICE of 39.6% and over\nimage-augmented methods such as RefCLIP-S\nof 18.3%. Moreover, CLAIR provides noisily\ninterpretable results by allowing the language\nmodel to identify the underlying reasoning\nbehind its assigned score. Code is available at\nhttps://davidmchan.github.io/clair/.\n1 Introduction & Background\nAutomatically evaluating the quality of image\ncaptions is challenging. There are many dimensions\nto consider, such as grammatical quality, semantic\nrelevance, correctness, and specificity, among\nothers. To ensure fair evaluations, most image\ncaptioning works employ a suite of measures,\neach capturing different aspects. For instance,\nn-gram-based measures like BLEU (Papineni\net al., 2002) or CIDEr (Vedantam et al., 2015)\nbroadly measure content overlap, SPICE (Anderson\net al., 2016) compares scene graph structures, and\nCLIPScore, TIFA, SeeTrue and VPEval (Hessel\n1Meaning “clear” in French, in line with other colorful\nmetric names (Papineni et al., 2002; Lin, 2004; Lita et al., 2005).\nYou are trying to tell if a\ncandidate set of captions is\ndescribing the same image as a\nreference set of captions.\nCandidate set:\n{candidate captions}\nReference set:\n{reference captions}\nOn a precise scale from 0 to\n100, how likely is it that the\ncandidate set is describing the\nsame image as the reference set?\n(JSON format, with a key “score\",\nvalue between 0 and 100, and a key\n“reason\" with a string value.)\nFigure 1: CLAIR: a (surprisingly simple) large language\nmodel-based measure for image caption evaluation. We\nfind that CLAIR not only correlates strongly with human\njudgments of caption quality but can also generate\ninterpretable reasons for the generated scores.\net al., 2021; Hu et al., 2023; Yarom et al., 2023; Cho\net al., 2023) directly incorporate visual information.\nUnfortunately, while significant strides have been\nmade in automated evaluation, human preference\nstudies remain the most reliable (yet costly) source\nof caption evaluation.\nFortunately, recent advances in large language\nmodels (LLMs) have opened new avenues for\nautomatic evaluation. Models trained with\nreinforcement learning from human feedback\n(RLHF, Christiano et al. (2017)) or similar methods\nare particularly useful for open-ended evaluation\ntasks, including image captioning, due to their\nexplicit training to align with human preferences.\nIn our work, paralleling several recent works\nwhich find that LLMs can act as effective “judges”\nfor selecting the better answer from two candi-\ndates (Bubeck et al., 2023; Dettmers et al., 2023;\nChiang et al., 2023), we explore the ability of LLMs\nto evaluate caption quality in the multimodal setting.\nWe introduce CLAIR (Criterion using LAnguage\nmodels for Image caption Rating), a measure\nwhich scores a candidate caption based on the\nlikelihood that it describes the same image as a set\nof references by directly asking an LLM to produce\na numeric rating. We further query the LLM to\nprovide a reason for its score, providing a level of\ninterpretability to the scalar rating. As far as we\nare aware, this is the first paper to explore replacing\nmeasures of semantic text quality with directly\nobtained LLM judgments, however concurrently,\nZheng et al. (2023) have shown that directly provid-\ning an answer rating can align highly with human\npreferences on a range of standard language-based\ntasks, such as conversational instruction following.\nThrough several experiments on captioning\ndatasets such as MS-COCO (Xu et al., 2016),\nFlickr8k (Mao et al., 2014), and PASCAL-\n50S (Vedantam et al., 2015), we find that CLAIR\ncorrelates surprisingly well with human preferences,\noutperforming prior captioning measures. We\nadditionally propose CLAIRE, where weEnsemble\nthe outputs of several LLMs by taking the average\nscore, leading to further improvements.\nDespite a simple pipeline using an LLM prompt\nwith minimal output parsing, CLAIR’s strong\ncorrelation with human preferences suggests that it\ncaptures multiple dimensions of caption similarity\nat once – a feature that prior measures struggle\nto achieve alone. More generally, CLAIR demon-\nstrates how language-only models can evaluate\nvision-language tasks. We show LLMs can provide\nnot only reliable scalar ratings but also correspond-\ning reasoning for a given rating, offering a valuable\ncombination of accuracy and interpretability.\n2 CLAIR: LLMs for Caption Evaluation\nIn CLAIR, we adapt the zero-shot in-context\nlearning approach described in Brown et al. (2020)\nto score candidate captions with large language\nmodels (LLMs). This involves converting the\ncaption evaluation problem into a human-readable\ntext completion task which is solved by the LLM.\nUsing the prompt in Figure 1, CLAIR first generates\ncompletions from the LLM and then parses those\ncompletions into both candidate scores and an\nexplainable reason for the score. We use a greedy\nsampling method (t=0) to encourage reproducibil-\nity in the results, while acknowledging the inherent\nnondeterminism in LLMs (see section 4). CLAIR’s\nexperimental implementation is surprisingly simple:\nit uses no in-context examples (is entirely zero-\nshot), and default inference parameters for the APIs.\nSee Appendix B for further implementation details.\nThe choice of language model directly affects the\nquality of the CLAIR measure – more accurate mod-\nels should produce evaluations that align better with\nhuman judgment. We explore three language mod-\nels: GPT-3.5 (ChatGPT) (OpenAI, 2022), Claude\n(Instant) (Bai et al., 2022), and PaLM (Chowdhery\net al., 2022). Unfortunately, we found for several\nopen-weight language models including Koala\n(Geng et al., 2023) and Vicuna (Chiang et al., 2023)\nthat CLAIR aligned poorly with human judgment.\nAs CLAIR is language model-agnostic, we can\nleverage the different distributions learned by each\nmodel and combine their decisions in an ensemble\napproach we termCLAIR E. We calculate individ-\nual CLAIR scores for each model and compute an\nunweighted average to obtain the ensemble score.\nBenchmark measures: We benchmark against\nseveral existing measure of caption similarity.\nBLEU (Papineni et al., 2002), ROUGE (Lin, 2004),\nMETEOR (Agarwal and Lavie, 2008) and CIDEr\n(Vedantam et al., 2015) all primarily measure n-\ngram overlap (however have different weighting\nschemes between n-grams, and across precision/re-\ncall). We also compare against SPICE (Anderson\net al., 2016), which compares caption parse trees and\nfocuses on matching perceived action and object re-\nlationships in addition to n-grams. While the afore-\nmentioned measures are commonly reported in im-\nage captioning works, we also compare against sev-\neral modern measures, including CLIP-Score (Hes-\nsel et al., 2021) which uses the recent CLIP (Radford\net al., 2021) model for reference-free evaluation.\n3 Evaluation & Discussion\nTo evaluate the quality of the measure, we run\nseveral evaluations that compare scores generated\nby CLAIR to both human judgments of caption\nquality and other image captioning evaluation mea-\nsures. We additionally provide several qualitative\nexamples in Figure 2. A unique benefit of CLAIR is\nthat it provides not only numeric scores but is also\nintrospectable, as it can identify which details in\nthe candidate caption set match the reference set.\nSample-level human correlation: We first ask\nthe question, how well does CLAIR correlate with\nPerson snowboarding at a ski slope.\nThe candidate caption mentions only \none person snowboarding, while all the\nreference captions mention multiple \npeople skiing or climbing a mountain. \nAdditionally, the candidate caption does \nnot mention any details about the \nmountain or the surroundings, which are \npresent in the reference captions.\nCandidate\nCLAIR Reason\n2/5\nHuman\n0.4\nCLAIR\n0.0\nBLEU4\n0.00\nCIDEr\n0.14\nROUGE\nThe candidate set only mentions a cyclist in a \nhelmet and yellow jersey, while the reference set\n describes a man in a green shirt carrying a drink \nwhile riding a bicycle. Although there are some \nsimilarities, such as the fact that both sets mention \na person riding a bike, the differences in the details \nmake it uncertain whether they are describing \nthe same image or not.\nA cyclist in a helmet and yellow jersey.\nCandidate\nCLAIR Reason\n2.7/5\nHuman\n0.5\nCLAIR\n0.0\nBLEU4\n0.01\nCIDEr\n0.43\nROUGE\nA boy jumps into the blue pool water.\nThe candidate set and reference set both \nmention a boy jumping into water, but the \ncandidate set does not provide as much \ndetail about the boy's appearance or the \nlocation of the pool. Additionally, the\nreference set includes captions about diving\noff a pier and holding one's nose, which are\nnot mentioned in the candidate set.\nCandidate\nCLAIR Reason\n3/5\nHuman\n0.7\nCLAIR\n0.0\nBLEU4\n0.55\nCIDEr\n0.61\nROUGE\n\"A blonde boy wearing a blue life vest and camo shorts \njumps off a diveboard into a blue lake.\"\n\"A boy diving backward off a pier in a large lake.\"\n\"A boy in a blue life jacket jumps into the water.\"\n\"A boy in a blue life jacket jumps off a board into the lake.\"\n\"A boy is holding his nose and jumping off a diving board \nbackwards into a lake.\"\nReferences\n\"A biker enjoys a coffee.\"\n\"A man in a bright green shirt and sunglasses is riding a bicycle through \nthe streets whilst drinking a latte.\"\n\"A man in a bright green shirt riding a bicycle in a paved courtyard , cary-\ning a drink.\"\n\"A man rides on his bike with one hand and holds a drink with the other.\"\n\"A man riding a bike wearing a green shirt with a drink in his hand .\"\nReferences\n\"a group of skiers going up a mountain.\"\n\"Four cross-country skiers climb uphill.\"\n\"Four people climbing a hill in the snow.\"\n\"Four skiers walking up a snow covered hill.\"\n\"four skiers climbing snow drapped mountain.\"\nReferences\nFigure 2: Several qualitative examples of CLAIR from the Flickr8K-Expert dataset. CLAIR not only correlates better with\nhuman judgments of caption quality but also provides detailed explanations for its score. CLAIR scores normalized by 100.\nTable 1: Sample-level correlation (Kendall’sτb) with human\njudgments. All p-values < 0.001. *: Model has access to\nadditional visual context.\nDataset\nMeasure COMPOSITE Flickr8K MS-COCO\nBLEU@1 0.313 0.323 0.265\nBLEU@4 0.306 0.308 0.215\nROUGE-L 0.324 0.323 0.221\nBERT-S 0.301 0.392 0.163\nMETEOR 0.389 0.418 0.239\nCIDEr 0.377 0.439 0.262\nSPICE 0.403 0.449 0.257\nCLIP-S* 0.498 0.511 -\nRefCLIP-S* 0.512 0.526 -\nRefCLIP-X* 0.523 0.549 0.274\nCLAIR\n+ GPT3.5 0.604 0.616 0.296\n+ Claude 0.542 0.563 0.320\n+ PaLM 0.580 0.546 0.355\nCLAIRE 0.592 0.627 0.374\nInter-Human - 0.736 -\nhuman judgments of caption quality at a sample\nlevel? We do so by exploring the performance on\nthree datasets, COMPOSITE, Flickr8K-Expert, and\nMS-COCO (See Appendix B for details).\nThe results of our sample-level correlation experi-\nments are shown in Table 1. We can see that CLAIR\noutperforms language-only measures (e.g., 0.604 to\n0.403 for SPICE), and in most cases, outperforms\nvision-augmented measures. CLAIR E achieves\nstrong sample-level correlation on all datasets; for\ninstance, CLAIRE closes the gap to inter-human\nagreement by 0.097 over vision-based measures and\n0.132 over language-based measures. The improve-\nments of CLAIRE over CLAIR suggest that each\nlanguage model may have some bias (similar to\neach human), yet the ensemble of models correlates\nmore strongly with human judgments. A reasonable\nconcern might be that the models underlying\nexisting approaches are significantly smaller than\nthose in CLAIR, and trained on less data. To address\nthis, we introduce and compare against RefCLIP-X,\nwhich replaces the CLIP model in RefCLIP with\na CLIP ViT-bigG/14 model trained on LAION 2B\n(Ilharco et al., 2021). Even in this case, CLAIR\ndemonstrates significantly improved performance.\nNote that in Table 1, we useτb, instead of the form\nτc, due to the ties generated by the CLAIR model.\nSystem-level human correlation: In addition\nto computing the sample-level correlation on the\nMS-COCO dataset, we use the annotations from the\nfive models considered by Rohrbach et al. (2018) to\ncompute the system-level correlation. For each of\nthe methods, we compute the mean human score on\nthe test samples, and mean metric score on the test\nsamples, followed by the Kendall’s rank correlation\ncoefficient (Kendall’s tau, strength of ordinal asso-\nciation) between these values (the set of five mean\nhuman scores, and the set of five metric scores). Our\nTable 2:System-level correlation between the average CLAIR\nscore and human model evaluation for 5 models trained and\nevaluated on MS-COCO. All p-values<0.05.\nMeasure Kendall’s τb Spearman’s ρ Pearson r\nBLEU@1 0.399 0.600 0.706\nBLEU@4 0.799 0.899 0.910\nROUGE-L 0.600 0.700 0.792\nMETEOR 0.600 0.700 0.666\nCIDEr 0.399 0.600 0.856\nSPICE 0.399 0.600 0.690\nCLAIR\n+ GPT3.5 0.799 0.899 0.869\n+ Claude 1.000 1.000 0.868\n+ PaLM 1.000 1.000 0.954\nCLAIRE 1.000 1.000 0.903\nTable 3: Accuracy of measures when matching human\ndecisions for PASCAL-50S (5 reference captions). *: Model\nhas access to additional visual context.\nMeasure HC HI HM MM All\nBLEU@1 51.20 95.70 91.20 58.20 74.08\nBLEU@4 53.00 92.40 86.70 59.40 72.88\nROUGE-L 51.50 94.50 92.50 57.70 74.05\nMETEOR 56.70 97.60 94.20 63.40 77.98\nCIDEr 53.00 98.00 91.50 64.50 76.75\nSPICE 52.60 93.90 83.60 48.10 69.55\nTIGEr* 56.00 99.80 92.80 74.20 80.70\nCLIP-S* 56.50 99.30 96.40 70.40 80.70\nRefCLIP-S* 64.50 99.60 95.40 72.80 83.10\nCLAIR\n+ GPT3.5 52.40 99.50 89.80 73.00 78.67\n+ Claude 57.90 98.50 91.30 62.90 77.65\n+ PaLM 54.70 98.30 87.30 64.00 76.08\nCLAIRE 57.70 99.80 94.60 75.60 81.93\nresults, given in Table 2, demonstrate that CLAIR\nranks the five methods in a novel way that is more\naccordant with human rankings of the methods.\nThese results further suggest that CLAIR has the\npotential to redefine which methods are preferable\nto humans compared to existing n-gram approaches.\nDecision Making: In addition to evaluating the\ncorrelation with human judgments, we also evaluate\nthe capability of the measure to perform discrimina-\ntive analysis. The PASCAL-50S dataset (Vedantam\net al., 2015) contains a set of 4000 human-annotated\ncaption pairs. For each pair of captions, humans\nlabel which caption in the pair is closest to the\nreference set for the image. The caption pairs fall\ninto four groups: “HC:\" two human-written cap-\ntions matching the image, “HI:\" one human caption,\nand one machine-generated caption, with only one\nmatching the image, “HM:\" a matching human cap-\ntion and a matching machine-generated caption and\n“MM:\" two matching machine-generated captions.\nTable 4: Pearson correlation with human judgments when\nevaluating sets of captions on MS-COCO (N =794).\nMeasure Coverage p-value Correctnessp-value\nBLEU@4 0.004 0.816 0.003 0.888\nROUGE-L 0.011 0.563 0.038 0.184\nMETEOR 0.016 0.398 0.006 0.765\nCIDEr 0.004 0.844 0.026 0.173\nTRM-METEOR 0.128 <0.001 0.108<0.001\nTRM-BLEU 0.127 <0.001 0.151<0.001\nMMD-BERT 0.129 <0.001 0.124<0.001\nFID-BERT 0.081 0.011 0.098<0.001\nCLAIR\n+ GPT3.5 0.195 0.011 0.187 0.014\n+ Claude 0.110 0.099 0.124 0.145\n+ PaLM 0.129 0.081 0.085 0.172\nCLAIRE 0.183 0.027 0.156 0.018\nInter-Human 0.225 <0.001 0.274<0.001\nSee Appendix B for more dataset information.\nThe performance on PASCAL-50S is given in\nTable 3. We can see that CLAIR E outperforms\nall existing text-only measures (e.g., by 5.18%\noverall score over CIDEr), and in many cases,\neven outperforms measures that have access to the\nimage at test time. Note that it is relatively weaker\nthan image-augmented models in the HC setting;\nhowever, since both captions are correct, the model\noften cannot judge which is better purely the text.\nModels such as RefCLIP-S that have access to\nthe image are naturally better discriminators in\nthis case. We suspect that CLAIR’s discriminative\nperformance could be further improved by giving\nthe LLM a choice between the two captions;\nhowever, we leave this optimization to future work.\nGroups of Captions: While CLAIR is capable\nof comparing a single candidate caption to a set of\nreference captions, it is also capable of comparing\nsets of candidate captions to sets of reference\ncaptions. This task is necessary when evaluating\nthe ability of a model to generate captions that are\ndiverse and that fully describe the conditional text\ndistribution. We evaluate on the COCO-Sets dataset\n(Chan et al., 2022), 794 caption sets rated by AMT\nworkers on two scales: how closely a candidate\nset matches the reference set in terms of both\ncorrectness and content coverage (See Appendix B\nfor details). The results of this experiment are given\nin Table 4. We can see that CLAIR outperforms well\nwhen measuring the quality of a group of captions,\nand approaches the inter-human correlation on the\n(very) challenging task. CLAIR also outperforms\nTRM-METEOR and TRM-BLEU (Chan et al.,\n2022), suggesting that LLMs can judge both the\ncontent and diversity of the caption sets.\n4 Limitations\nWhile CLAIR correlates well with human judg-\nments of caption quality, it has several limitations:\nNon-Determinism and Parsing Errors:Because\nCLAIR depends on the output of a language model,\nthe measure can be non-deterministic and noisy. For\ninstance, it may fail to elicit a judgment (e.g., “As an\nAI language model, I cannot see, and thus, cannot de-\ntermine if the image captions match the references”),\nor rarely, generate malformed JSON output. To ad-\ndress these issues, we perform multiple queries to\nthe LLM, sometimes at higher temperatures if nec-\nessary. As a consequence, the measure may differ\nbetween runs, although we found the variance to be\nrelatively insignificant (<0.01 in many of the exper-\niments). Additionally, since the language models\nused are not open-source, the models are subject to\narbitrary change, replacement, or removal, which\nlimits the efficacy of the measure as a long-term com-\nparable measurement. We hope that increasing open\naccess to language models with efforts such as Koala\n(Geng et al., 2023) and Vicuna (Chiang et al., 2023),\nwill help to alleviate these challenges in the future.\nIncreased Cost:CLAIR relies on language models\nwhich contain many billions of parameters. These\nlanguage models have not only monetary cost but\nalso human and environmental costs (Bender et al.,\n2021) which can reduce its utility as a target during\ntraining, such as for self-critical sequence training\n(Rennie et al., 2017). While API-based LLMs\nmay be considered costly, even open-source LLMs\nhave a cost (which can often be hard to quantify).\nCLAIR on the MS-COCO dataset uses an average\nof 226.148 tokens per sample (on OpenAI’s API),\nrepresenting a cost of $0.0067 per sample (GPT-4),\nor $0.00033 per sample (GPT 3.5). For PALM,\nthis drops to $0.000113 per sample. We hope\nthat over time, advances in LLM inference (such\nas quantization and distillation), coupled with\nimprovements in architecture will continue to yield\nlower-cost alternatives with strong performance on\nthe caption evaluation task.\nHallucination: While CLAIR does suffer from\npotential hallucination, we strongly believe that this\nweakness does not diminish the fact that CLAIR\nstill correlates strongly with human judgment. In\nCLAIR, hallucinations in the score manifest as\n“incorrect” judgements of similarity, while hallu-\ncinations in the explanations manifest as poorly\ngrounded explanations of the score/quality. Hal-\nlucinations in the score should be considered false\nnegatives (blind spots instead of hallucinations). In\nthe case of hallucinations in the explanations, such\nhallucinations may lead to misinterpretation, but\narguably less misinterpretation than a black box\nmethod, and may even indicate misunderstandings\nin the model. Hallucination is a well-known chal-\nlenge of current LLMs and is the subject of a great\namount of research on instruction-tuning, RLHF,\nRLAIF, and other methods. As hallucination and\ninstruction-following performance of the base mod-\nels improves, CLAIR inherit similar improvements.\nExplainability: While CLAIR generates explana-\ntions for each rating, CLAIR has no strict scoring\nrubric. Much like human judgments, there is no\ndirect way of attributing changes in score to changes\nin caption quality. For similar reasons, it is difficult\nto evaluate the quality of the generated explanations.\nQualitatively, the explanations are generally\nreasonable and consider multiple axes of judgment.\n5 Conclusion\nThis work introduces CLAIR, an LLM-based\nevaluation measure for image captioning. CLAIR’s\nsuperior performance compared to highly-\nengineered measures indicates a remarkable fact:\nLLMs are well aligned with human judgments of\ncaption quality, even more so than some measures\ndesigned specifically for semantic similarity.\nCLAIR is only a glimpse into how LLMs can be\nused for evaluation tasks, and image captioning is\nonly the beginning. We hope that our work will\ninspire further exploration of similar measures in\nother vision and language domains, such as visual\nstorytelling (Huang et al., 2016), where human eval-\nuation of generated text remains a challenging task.\nReferences\nSomak Aditya, Yezhou Yang, Chitta Baral, Cornelia\nFermuller, and Yiannis Aloimonos. 2015. From\nimages to sentences through scene description graphs\nusing commonsense reasoning and knowledge.ArXiv\npreprint, abs/1511.03292.\nAbhaya Agarwal and Alon Lavie. 2008. Meteor,\nM-BLEU and M-TER: Evaluation metrics for\nhigh-correlation with human rankings of machine\ntranslation output. In Proceedings of the Third\nWorkshop on Statistical Machine Translation, pages\n115–118. Association for Computational Linguistics.\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016. Spice: Semantic propositional\nimage caption evaluation. In European conference\non computer vision, pages 382–398. Springer.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022. Training a helpful and harmless assistant with\nreinforcement learning from human feedback.ArXiv\npreprint, abs/2204.05862.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models be\ntoo big? In Proceedings of the 2021 ACM conference\non fairness, accountability, and transparency, pages\n610–623.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems\n33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan,\nJohannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,\nYin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.\nSparks of artificial general intelligence: Early exper-\niments with gpt-4. ArXiv preprint, abs/2303.12712.\nDavid M Chan, Yiming Ni, Austin Myers, Sudheendra\nVijayanarasimhan, David A Ross, and John Canny.\n2022. Distribution aware metrics for conditional\nnatural language generation. ArXiv preprint ,\nabs/2209.07518.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\nEric P. Xing. 2023. Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality.\nJaemin Cho, Abhay Zala, and Mohit Bansal. 2023.\nVisual programming for text-to-image generation and\nevaluation. arXiv preprint arXiv:2305.15328.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. ArXiv preprint,\nabs/2204.02311.\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan\nMartic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nAdvances in Neural Information Processing Systems\n30: Annual Conference on Neural Information\nProcessing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA, pages 4299–4307.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. ArXiv preprint, abs/2305.14314.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric\nWallace, Pieter Abbeel, Sergey Levine, and Dawn\nSong. 2023. Koala: A dialogue model for academic\nresearch. Blog post.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan\nLe Bras, and Yejin Choi. 2021. CLIPScore: A\nreference-free evaluation metric for image caption-\ning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7514–7528. Association for Computational\nLinguistics.\nMicah Hodosh, Peter Young, and Julia Hockenmaier.\n2013. Framing image description as a ranking task:\nData, models and evaluation metrics. Journal of\nArtificial Intelligence Research, 47:853–899.\nYushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang,\nMari Ostendorf, Ranjay Krishna, and Noah A Smith.\n2023. Tifa: Accurate and interpretable text-to-image\nfaithfulness evaluation with question answering.\narXiv preprint arXiv:2303.11897.\nTing-Hao Kenneth Huang, Francis Ferraro, Nasrin\nMostafazadeh, Ishan Misra, Aishwarya Agrawal,\nJacob Devlin, Ross Girshick, Xiaodong He, Pushmeet\nKohli, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh,\nLucy Vanderwende, Michel Galley, and Margaret\nMitchell. 2016. Visual storytelling. In Proceedings of\nthe 2016 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, pages 1233–1239,\nSan Diego, California. Association for Computational\nLinguistics.\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman,\nCade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John\nMiller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig\nSchmidt. 2021. Openclip. If you use this software,\nplease cite it as below.\nMing Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang,\nPengchuan Zhang, Zhe Gan, Jana Diesner, and Jian-\nfeng Gao. 2019. TIGEr: Text-to-image grounding\nfor image caption evaluation. In Proceedings of the\n2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2141–2152. Association\nfor Computational Linguistics.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.\nHoi. 2022. BLIP: bootstrapping language-image\npre-training for unified vision-language understand-\ning and generation. In International Conference\non Machine Learning, ICML 2022, 17-23 July\n2022, Baltimore, Maryland, USA , volume 162 of\nProceedings of Machine Learning Research, pages\n12888–12900. PMLR.\nChin-Yew Lin. 2004. ROUGE: A package for automatic\nevaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81. Association for\nComputational Linguistics.\nLucian Vlad Lita, Monica Rogati, and Alon Lavie.\n2005. Blanc: Learning evaluation metrics for mt.\nIn Proceedings of Human Language Technology\nConference and Conference on Empirical Methods\nin Natural Language Processing, pages 740–747.\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, and\nAlan L Yuille. 2014. Explain images with multi-\nmodal recurrent neural networks. arXiv preprint\narXiv:1410.1090.\nOpenAI. 2022. Introducing chatgpt.\nKishore Papineni, Salim Roukos, Todd Ward, and\nWei-Jing Zhu. 2002. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings\nof the 40th Annual Meeting of the Association\nfor Computational Linguistics , pages 311–318.\nAssociation for Computational Linguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack\nClark, Gretchen Krueger, and Ilya Sutskever. 2021.\nLearning transferable visual models from natural\nlanguage supervision. In Proceedings of the 38th\nInternational Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, volume\n139 of Proceedings of Machine Learning Research,\npages 8748–8763. PMLR.\nSteven J. Rennie, Etienne Marcheret, Youssef Mroueh,\nJerret Ross, and Vaibhava Goel. 2017. Self-critical\nsequence training for image captioning. In2017 IEEE\nConference on Computer Vision and Pattern Recog-\nnition, CVPR 2017, Honolulu, HI, USA, July 21-26,\n2017, pages 1179–1195. IEEE Computer Society.\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,\nTrevor Darrell, and Kate Saenko. 2018. Object\nhallucination in image captioning. In Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing , pages 4035–4045.\nAssociation for Computational Linguistics.\nRamakrishna Vedantam, C. Lawrence Zitnick, and\nDevi Parikh. 2015. Cider: Consensus-based image\ndescription evaluation. In IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR\n2015, Boston, MA, USA, June 7-12, 2015 , pages\n4566–4575. IEEE Computer Society.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022. OFA: unifying\narchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. InInter-\nnational Conference on Machine Learning, ICML\n2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 23318–23340. PMLR.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. MSR-\nVTT: A large video description dataset for bridging\nvideo and language. In 2016 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV , USA, June 27-30, 2016, pages\n5288–5296. IEEE Computer Society.\nMichal Yarom, Yonatan Bitton, Soravit Changpinyo,\nRoee Aharoni, Jonathan Herzig, Oran Lang, Eran\nOfek, and Idan Szpektor. 2023. What you see is what\nyou read? improving text-image alignment evaluation.\narXiv preprint arXiv:2305.10400.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic\ninference over event descriptions.Transactions of the\nAssociation for Computational Linguistics, 2:67–78.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. ArXiv preprint, abs/2306.05685.\nAppendix\nA Acknowledgements\nWe thank Suhong Moon and Kate Saenko for their\nhelpful comments on the work. Authors, as part of\ntheir affiliation with UC Berkeley, were supported\nin part by the NSF, DoD, and/or the Berkeley\nArtificial Intelligence Research (BAIR) industrial\nalliance program, as well as gifts from Anyscale, As-\ntronomer, Google, IBM, Intel, Lacework, Microsoft,\nMohamed Bin Zayed University of Artificial Intel-\nligence, Samsung SDS, Uber, and VMware.\nB Additional Experimental Details\nIn this section, we provide several additional details\nfor the experiments in section 3 run with the CLAIR\nmeasure.\nB.1 Input Prompt Formatting\nThe CLAIR prompt is given in its entirety in\nFigure 1. During run-time, candidate and reference\ncaptions are prefixed with a “- \" and inserted into\nthe prompt, one per line. The resulting query is\npassed to the large language model. In addition,\nfor models which were not RLHF-tuned to perform\nconversation (such as PaLM), we found that it was\nhelpful to append an additional prefix{\"score\":to\nthe beginning of the output, to encourage the correct\noutput formatting. CLAIR is surprisingly simple: it\nuses no in-context examples (is entirely zero-shot),\nand default inference parameters for the APIs. The\nmodel checkpoint metadata is generally unknown\n(as the APIs are somewhat fluid and evolving).\nB.2 LLM Output Post-Processing\nBecause CLAIR relies on an LLM to produce\noutput, there is no guarantee that the output will be\nin the format we expect (i.e. valid, parsable JSON).\nTo extract both the score and the reason, we first\nextract the first set of paired braces from the output\nof the LLM and attempt to parse the result as JSON.\nIn most cases ( 99.997% for GPT-3, 99.991% for\nClaude, and 99.94% for PaLM during the course of\nour experiments), this is successful, and the score\nand reason are returned. In the case that the JSON\noutput is malformed, we attempt to extract any\nsequence of digits from the LLM to use as a score,\nand set the reason to “Unknown.” When this fails, as\ncan be the case when the models produce an output\nsuch as “As an AI language model, I cannot see,\nand thus, cannot determine if the image captions\nmatch the references”, we retry the prompt at a\nhigher temperature (t=1.0) several times. Failing\nthis (which occurred only three times in the entire\nevaluation of this paper, across several hundred\nthousand calls), we set the score to 0 for the caption.\nB.3 Datasets\nIn this section, we provide additional detail regard-\ning the datasets used in the evaluations in section 3.\nCOMPOSITE: The COMPOSITE dataset\n(Aditya et al., 2015) contains machine-generated\ntest captions for 3995 images spread across the\nMS-COCO (Xu et al., 2016), Flickr8K (Mao et al.,\n2014) and Flickr30k (Young et al., 2014) datasets.\nEach image has three test captions, one written by\na human, and two that are model generated. The\ncandidate captions are graded by annotators on\nAmazon Mechanical Turk (AMT) on a scale of\n1 (not relevant) to 5 (very relevant). Inter-human\ncorrelations are not available for this dataset.\nFlickr8K-Expert: The Flickr8K-Expert dataset\n(Hodosh et al., 2013) contains 5822 captions asso-\nciated with 1000 images. The dataset is annotated\nwith expert human judgments of quality, where\nimages are rated from 1 (caption is unrelated to the\nimage) to 4 (caption describes the image without\nerrors). Unlike the composite and MS-COCO\ndatasets, the captions here are selected using an\nimage retrieval system, instead of generated using\na learned image captioning model. Following Jiang\net al. (2019), we exclude any candidate captions\nthat overlap the reference set.\nMS-COCO: Following experiments by\nRohrbach et al. (2018), we compute the sample-\nlevel correlation between our method and human\nratings on a 500-image subset of the MS-COCO\nKarpathy test set. Each image in the subset contains\ncandidate captions generated by 5 models, and each\ncaption is labeled with the average three human\nratings generated by AMT workers which range\nfrom 1 (very bad) to 5 (very good). Inter-human\ncorrelations are not available for this dataset.\nPASCAL-50S: PASCAL-50S contains 1000\nimages drawn from the PASCAL sentence dataset.\nEach image is associated with at least 50 (and as\nmany as 120) reference captions. In addition to\nthe reference captions, PASCAL-50S contains a\nset of 4000 human annotated image/caption pairs\ncontaining an image, and two candidate captions.\nThe caption pairs fall into four groups:\n1. HC: In the HC group, both captions in the pair\nare human written, and describe the content\nof the target image correctly.\n2. HI: In the HI group, both captions in the pair\nare human written, but one caption correctly\ndescribes the content of the image, and the\nother caption describes the content of a\ndifferent image.\n3. HM: In the HM group, one caption is written\nby a human, and one caption is written by\na machine, but both correctly describe the\ncontent of the image.\n4. MM: In the MM group, both captions are\nwritten by a machine, and both correctly\ndescribe the image content.\nIn PASCAL-50S, the task is to decide which caption\nin the pair humans prefer more (a subjective task,\nhopefully indicating caption quality). Following\nprevious work (Jiang et al., 2019; Hessel et al.,\n2021), we limit the number of reference sentences\nto five during evaluation.\nCOCO-Sets: The COCO-Sets dataset (Chan\net al., 2022) is a set of samples that are designed\nto evaluate the correlation of distribution-aware\nimage captioning measures with human judgments\nof distributional distance. In this dataset, humans\nwere presented with two candidate caption sets\n(two image captioning models, OFA (Wang et al.,\n2022) and BLIP (Li et al., 2022) using different\ntemperatures), and asked which candidate caption\nset correlated better with a reference caption set on\ntwo measures: how much they overlapped factually\n(correctness), and how much information they pro-\nvided about the references (coverage). It consists of\n794 AMT worker-generated judgments of caption\nquality for images in the MS-COCO dataset.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7733166813850403
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6296467781066895
    },
    {
      "name": "Natural language processing",
      "score": 0.6083727478981018
    },
    {
      "name": "Relevance (law)",
      "score": 0.5980038642883301
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5440716743469238
    },
    {
      "name": "Image (mathematics)",
      "score": 0.5073654055595398
    },
    {
      "name": "Language model",
      "score": 0.48553845286369324
    },
    {
      "name": "Visual reasoning",
      "score": 0.47310829162597656
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4252408742904663
    },
    {
      "name": "Machine learning",
      "score": 0.4251648783683777
    },
    {
      "name": "Correlation",
      "score": 0.42299336194992065
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.42243391275405884
    },
    {
      "name": "Information retrieval",
      "score": 0.3800100088119507
    },
    {
      "name": "Programming language",
      "score": 0.08460670709609985
    },
    {
      "name": "Mathematics",
      "score": 0.07597798109054565
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ],
  "cited_by": 14
}