{
  "title": "Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data",
  "url": "https://openalex.org/W3099142828",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2112272916",
      "name": "Lingkai Kong",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2472586093",
      "name": "Haoming Jiang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2284452802",
      "name": "Yuchen Zhuang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2317268570",
      "name": "Jie Lyu",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2234658305",
      "name": "Tuo Zhao",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096826587",
      "name": "Chao Zhang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W4300833946",
    "https://openalex.org/W2254249950",
    "https://openalex.org/W2963756980",
    "https://openalex.org/W4394639725",
    "https://openalex.org/W2963494066",
    "https://openalex.org/W2759474451",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2921861056",
    "https://openalex.org/W2970121940",
    "https://openalex.org/W4297798436",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2964059111",
    "https://openalex.org/W2581377246",
    "https://openalex.org/W2804697534",
    "https://openalex.org/W2531327146",
    "https://openalex.org/W2948210185",
    "https://openalex.org/W4252684946",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W2964031043",
    "https://openalex.org/W2164411961",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4362223627",
    "https://openalex.org/W2964159205",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2996564870",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2898681588",
    "https://openalex.org/W2597787948",
    "https://openalex.org/W2964282813",
    "https://openalex.org/W2098824882",
    "https://openalex.org/W2970316625",
    "https://openalex.org/W2963501948",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2951786554",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963238274",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4288360049",
    "https://openalex.org/W2970206392",
    "https://openalex.org/W2592505114",
    "https://openalex.org/W2108281845",
    "https://openalex.org/W3159527663",
    "https://openalex.org/W2115791615",
    "https://openalex.org/W2785787385",
    "https://openalex.org/W582134693"
  ],
  "abstract": "Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1326–1340,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n1326\nCalibrated Language Model Fine-Tuning for In- and\nOut-of-Distribution Data\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao, Chao Zhang\nGeorgia Institute of Technology, Atlanta, USA\n{lkkong,jianghm,yczhuang,jie.lyu,tourzhao,chaozhang}@gatech.edu\nAbstract\nFine-tuned pre-trained language models can\nsuffer from severe miscalibration for both\nin-distribution and out-of-distribution (OOD)\ndata due to over-parameterization. To mit-\nigate this issue, we propose a regularized\nﬁne-tuning method. Our method intro-\nduces two types of regularization for bet-\nter calibration: (1) On-manifold regulariza-\ntion, which generates pseudo on-manifold\nsamples through interpolation within the data\nmanifold. Augmented training with these\npseudo samples imposes a smoothness regu-\nlarization to improve in-distribution calibra-\ntion. (2) Off-manifold regularization, which\nencourages the model to output uniform dis-\ntributions for pseudo off-manifold samples to\naddress the over-conﬁdence issue for OOD\ndata. Our experiments demonstrate that the\nproposed method outperforms existing cal-\nibration methods for text classiﬁcation in\nterms of expectation calibration error, mis-\nclassiﬁcation detection, and OOD detection\non six datasets. Our code can be found\nat https://github.com/Lingkai-Kong/\nCalibrated-BERT-Fine-Tuning .\n1 Introduction\nPre-trained language models have recently brought\nthe natural language processing (NLP) community\ninto the transfer learning era. The transfer learn-\ning framework consists of two stages, where we\nﬁrst pre-train a large-scale language model, ( e.g.,\nBERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019), ALBERT (Lan et al., 2020) and T5 (Raffel\net al., 2019)) on a large text corpus and then ﬁne-\ntune it on downstream tasks. Such a ﬁne-tuning\napproach has achieved SOTA performance in many\nNLP benchmarks (Wang et al., 2018, 2019).\nMany applications, however, require trustwor-\nthy predictions that need to be not only accurate\nbut also well calibrated. In particular, a well-\ncalibrated model should produce reliable conﬁ-\nFigure 1: The reliability diagrams on in-distribution\ndata (the ﬁrst row) and the histograms of the model con-\nﬁdence on out-of-distribution (OOD) data (the second\nrow) of CNN (Kim, 2014) and ﬁne-tuned BERT-MLP\nclassiﬁer (Devlin et al., 2019). Though BERT improves\nclassiﬁcation accuracy, it makes over-conﬁdent predic-\ntions for both in-distribution and OOD data.\ndent estimates for both in-distribution and out-of-\ndistribution (OOD) data: (1) For in-distribution\ndata, a model should produce predictive probabili-\nties close to the true likelihood for each class, i.e.,\nconﬁdence ≈true likelihood. (2) For OOD data,\nwhich do not belong to any class of the training\ndata, the model output should produce high un-\ncertainty to say ‘I don’t know’, i.e., conﬁdence\n≈random guess, instead of producing absurdly\nwrong yet wildly conﬁdent predictions. Providing\nsuch calibrated output probabilities can help us to\nachieve better model robustness (Lee et al., 2018),\nmodel fairness (Chouldechova, 2017) and improve\nlabel efﬁciency via uncertainty driven learning (Gal\net al., 2017; Siddhant and Lipton, 2018; Shen et al.,\n2018).\n1327\nUnfortunately, Guo et al. (2017) have shown that\ndue to over-parameterization, deep convolutional\nneural networks are often miscalibrated. Our ex-\nperimental investigation further corroborates that\nﬁne-tuned language models can suffer from miscal-\nibration even more for NLP tasks. As shown in Fig-\nure 1, we present the calibration of a BERT-MLP\nmodel for a text classiﬁcation task on the 20NG\ndataset. Speciﬁcally, we train a TextCNN (Kim,\n2014) and a BERT-MLP using 20NG15 (the ﬁrst\n15 categories of 20NG) and then evaluate them on\nboth in-distribution and OOD data. The ﬁrst row\nplots their reliability diagrams (Niculescu-Mizil\nand Caruana, 2005) on the test set of 20NG 15.\nThough BERT improves the classiﬁcation accu-\nracy from 83.9% to 87.4%, it also increases the\nexpected calibration error (ECE, see more details\nin Section 2) from 4.0% to 9.5%. This indicates\nthat BERT-MLP is much more miscalibrated for\nin-distribution data. The second row plots the his-\ntograms of the model conﬁdence, i.e., the maxi-\nmum output probability, on the test set of 20NG5\n(the unseen 5 categories of 20NG). While it is de-\nsirable to produce low probabilities for these un-\nseen classes, BERT-MLP produces wrong yet over-\nconﬁdent predictions for such OOD data.\nSuch an aggravation of miscalibration is due to\nthe even more signiﬁcant over-parameterization of\nthese language models. At the pre-training stage,\nthey are trained on a huge amount of unlabeled data\nin an unsupervised manner, e.g., T5 is pre-trained\non 745 GB text. To capture rich semantic and syn-\ntactic information from such a large corpus, the\nlanguage models are designed to have enormous\ncapacity, e.g., T5 has about 11 billion parameters.\nAt the ﬁne-tuning stage, however, only limited la-\nbeled data are available in the downstream tasks.\nWith the extremely high capacity, these models\ncan easily overﬁt training data likelihood and be\nover-conﬁdent in their predictions.\nTo ﬁght against miscalibration, a natural option\nis to apply a calibration method such as tempera-\nture scaling (Guo et al., 2017) in a post-processing\nstep. However, temperature scaling only learns a\nsingle parameter to rescale all the logits, which is\nnot ﬂexible and insufﬁcient. Moreover, it cannot\nimprove out-of-distribution calibration. A second\noption is to mitigate miscalibration during train-\ning using regularization. For example, Pereyra\net al. (2017) propose an entropy regularizer to pre-\nvent over-conﬁdence, but it can needlessly hurt\nlegitimate high conﬁdent predictions. A third op-\ntion is to use Bayesian neural networks (Blundell\net al., 2015; Louizos and Welling, 2017), which\ntreat model parameters as probability distributions\nto represent model uncertainty explicitly. However,\nthese Bayesian approaches are often prohibitive, as\nthe priors of the model parameters are difﬁcult to\nspecify, and exact inference is intractable, which\ncan also lead to unreliable uncertainty estimates.\nWe propose a regularization approach to ad-\ndressing miscalibration for ﬁne-tuning pre-trained\nlanguage models from a data augmentation per-\nspective. We propose two new regularizers using\npseudo samples both on and off the data manifold\nto mitigate data scarcity and prevent over-conﬁdent\npredictions. Speciﬁcally, our method imposes two\ntypes of regularization for better calibration during\nﬁne-tuning: (1) On-manifold regularization: We\nﬁrst generate on-manifold samples by interpolat-\ning the training data and their corresponding labels\nalong the direction learned from hidden feature\nspace; training over such augmented on-manifold\ndata introduces a smoothness constraint within the\ndata manifold to improve the model calibration for\nin-distribution data. (2) Off-manifold regulariza-\ntion: We generate off-manifold samples by adding\nrelatively large perturbations along the directions\nthat point outward the data manifold; we penal-\nize the negative entropy of the output distribution\nfor such off-manifold samples to address the over-\nconﬁdence issue for OOD data.\nWe evaluate our proposed model calibration\nmethod on six text classiﬁcation datasets. For in-\ndistribution data, we measure ECE and the per-\nformance of misclassiﬁcation detection. For out-\nof-distribution data, we measure the performance\nof OOD detection. Our experiments show that\nour method outperforms existing state-of-the-art\nmethods in both settings, and meanwhile maintains\ncompetitive classiﬁcation accuracy.\nWe summarize our contribution as follows: (1)\nWe propose a general calibration framework, which\ncan be applied to pre-trained language model ﬁne-\ntuning, as well as other deep neural network-based\nprediction problems. (2) The proposed method\nadopts on- and off-manifold regularization from\na data augmentation perspective to improve cali-\nbration for both in-distribution and OOD data. (3)\nWe conduct comprehensive experiments showing\nthat our method outperforms existing calibration\nmethods in terms of ECE, miscalssiﬁcation detec-\n1328\ntion and OOD detection on six text classiﬁcation\ndatasets.\n2 Preliminaries\nWe describe model calibration for both in-\ndistribution and out-of-distribution data.\nCalibration for In-distribution Data: For in-\ndistribution data, a well-calibrated model is ex-\npected to output prediction conﬁdence comparable\nto its classiﬁcation accuracy. For example, given\n100 data points with their prediction conﬁdence\n0.6, we expect 60 of them to be correctly classi-\nﬁed. More precisely, for a data point X, we denote\nby Y(X) the ground truth label, ˆY(X) the label\npredicted by the model, and ˆP(X) the output prob-\nability associated with the predicted label. The\ncalibration error of the predictive model for a given\nconﬁdence p∈(0,1) is deﬁned as:\nEp = |P( ˆY(X) = Y(X)|ˆP(X) = p) −p|. (1)\nAs (1) involves population quantities, we usually\nadopt empirical approximations (Guo, 2017) to esti-\nmate the calibration error. Speciﬁcally, we partition\nall data points into M bins of equal size according\nto their prediction conﬁdences. Let Bm denote the\nbin with prediction conﬁdences bounded between\nℓm and um. Then, for any p∈[ℓm,um), we deﬁne\nthe empirical calibration error as:\nˆEp = ˆEm = 1\n|Bm|\n⏐⏐⏐\n∑\ni∈Bm\n[\n1(ˆyi = yi) −ˆpi\n]⏐⏐⏐, (2)\nwhere yi, ˆyi and ˆpi are the true label, predicted\nlabel and conﬁdence for sample i.\nTo evaluate the overall calibration error of the\npredictive model, we can futher take a weighted\naverage of the calibration errors of all bins, which is\nalso known as the expected calibration error (ECE)\n(Naeini et al., 2015) deﬁned as:\nECE =\nM∑\nm=1\n|Bm|\nn\nˆEm, (3)\nwhere nis the sample size.\nWe remark that the goal of calibration is to mini-\nmize the calibration error without signiﬁcantly sac-\nriﬁcing prediction accuracy. Otherwise, a random\nguess classiﬁer can achieve zero calibration error.\nCalibration for Out-of-distribution Data: In\nreal applications, a model can encounter test data\nthat signiﬁcantly differ from the training data. For\nexample, they come from other unseen classes, or\nthey are potential outliers. A well-calibrated model\nis expected to produce an output with high uncer-\ntainty for such out-of-distribution (OOD) data, for-\nmally,\nP(Y = j) = 1/K ∀j = 1,...,K,\nwhere K is the number of classes of the training\ndata. As such, we can detect OOD data by setting\nup an uncertainty threshold.\n3 Calibrated Fine-Tuning via Manifold\nSmoothing\nWe consider N data points of the target task S =\n{(xi,yi)}N\ni=1, where xi’s denote the input embed-\nding of the sentence and yi’s are the associated one-\nhot labels. Let f(·) denote the feature extraction\nlayers (e.g., BERT); letg(·) denote the task-speciﬁc\nlayer; and let θdenote all parameters of f and g.\nWe propose to optimize the following objective at\nthe ﬁne-tuning stage:\nmin\nθ\nF(θ) = Ex,y∼Sℓ(g◦f(x),y)\n+ λonRon(g◦f) + λoﬀRoﬀ(g◦f), (4)\nwhere ℓis the cross entropy loss, and λon,λoﬀ are\ntwo hyper-parameters. The regularizers Ron and\nRoﬀ are for on- and off-manifold calibration, re-\nspectively.\n3.1 On-manifold Regularization\nThe on-manifold regularizer Ron exploits the inter-\npolation of training data within the data manifold\nto improve the in-distribution calibration. Speciﬁ-\ncally, given two training samples (x,y) and (˜x,˜y)\nand the feature extraction layers f, we generate an\non-manifold pseudo sample (x′,y′) as follows:\nx′∗= arg min\nx′∈B(x,δon)\nDx(f(x′),f(˜x)), (5)\ny′= (1 −δy)y+ δy˜y, (6)\nwhere δon and δy are small interpolation parameters\nfor data and label, and Dx is a proper distance for\nfeatures extracted by f such as cosine distance,\ni.e., Dx(a,b) = ⟨a/∥a∥2,b/∥b∥2⟩, and B(x,δon)\ndenotes an ℓ∞ball centered at x with a radius δon,\ni.e.,\nB(x,δon) = {x′|∥x′−x∥∞≤δon}.\nAs can be seen, x′∗ is essentially interpolat-\ning between x and ˜x on the data manifold, and\nDx(f(·),f(·)) can be viewed as a metric over such\na manifold. However, as f(·) is learnt from ﬁnite\ntraining data, it can recover the actual data mani-\nfold only up to a certain statistical error. Therefore,\n1329\nx\n<latexit sha1_base64=\"kO36Fwqnz69PGe5U1YsdTl42T9o=\">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclZkq2GXBjcsK9oFtKZk004ZmMkNyRyxD/8KNC0Xc+jfu/Bsz7Sy09UDgcM695Nzjx1IYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJRoxpsskpHu+NRwKRRvokDJO7HmNPQlb/uTm8xvP3JtRKTucRrzfkhHSgSCUbTSQy+kOPaD9Gk2KJXdijsHWSVeTsqQozEoffWGEUtCrpBJakzXc2Psp1SjYJLPir3E8JiyCR3xrqWKhtz003niGTm3ypAEkbZPIZmrvzdSGhozDX07mSU0y14m/ud1Ewxq/VSoOEGu2OKjIJEEI5KdT4ZCc4ZyagllWtishI2ppgxtSUVbgrd88ippVSveZaV6d1Wu1/I6CnAKZ3ABHlxDHW6hAU1goOAZXuHNMc6L8+58LEbXnHznBP7A+fwB+4WRFg==</latexit>\n˜x\n<latexit sha1_base64=\"ptYS2YtzpEm19IJM5Kr9s0kFMHA=\">AAAB+3icbVDLSsNAFJ3UV62vWJduBovgqiRVsMuCG5cV7AOaUCaTSTt08mDmRlpCfsWNC0Xc+iPu/BsnbRbaemDgcM693DPHSwRXYFnfRmVre2d3r7pfOzg8Oj4xT+t9FaeSsh6NRSyHHlFM8Ij1gINgw0QyEnqCDbzZXeEPnphUPI4eYZEwNySTiAecEtDS2Kw7wIXPMickMPWCbJ7nY7NhNa0l8CaxS9JAJbpj88vxY5qGLAIqiFIj20rAzYgETgXLa06qWELojEzYSNOIhEy52TJ7ji+14uMglvpFgJfq742MhEotQk9PFhHVuleI/3mjFIK2m/EoSYFFdHUoSAWGGBdFYJ9LRkEsNCFUcp0V0ymRhIKuq6ZLsNe/vEn6raZ93Ww93DQ67bKOKjpHF+gK2egWddA96qIeomiOntErejNy48V4Nz5WoxWj3DlDf2B8/gDvQpT9</latexit>\n\u0000 on\n<latexit sha1_base64=\"CZChaCfdAh9O/ewyV96FqPYve1U=\">AAAB+HicbVDLSsNAFJ34rPXRqEs3g0VwVZIq2GXBjcsK9gFNCJPJpB06jzAzEWrol7hxoYhbP8Wdf+O0zUJbD1w4nHMv994TZ4xq43nfzsbm1vbObmWvun9weFRzj096WuYKky6WTKpBjDRhVJCuoYaRQaYI4jEj/XhyO/f7j0RpKsWDmWYk5GgkaEoxMlaK3FqQEGZQVASKQylmkVv3Gt4CcJ34JamDEp3I/QoSiXNOhMEMaT30vcyEBVKGYkZm1SDXJEN4gkZkaKlAnOiwWBw+gxdWSWAqlS1h4EL9PVEgrvWUx7aTIzPWq95c/M8b5iZthQUVWW6IwMtFac6gkXCeAkyoItiwqSUIK2pvhXiMFMLGZlW1IfirL6+TXrPhXzWa99f1dquMowLOwDm4BD64AW1wBzqgCzDIwTN4BW/Ok/PivDsfy9YNp5w5BX/gfP4A3QWTMA==</latexit>\nTraining data\nOn-manifold sample \nOﬀ-manifold sample Data manifold\nx\n<latexit sha1_base64=\"kO36Fwqnz69PGe5U1YsdTl42T9o=\">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclZkq2GXBjcsK9oFtKZk004ZmMkNyRyxD/8KNC0Xc+jfu/Bsz7Sy09UDgcM695Nzjx1IYdN1vZ219Y3Nru7BT3N3bPzgsHR23TJRoxpsskpHu+NRwKRRvokDJO7HmNPQlb/uTm8xvP3JtRKTucRrzfkhHSgSCUbTSQy+kOPaD9Gk2KJXdijsHWSVeTsqQozEoffWGEUtCrpBJakzXc2Psp1SjYJLPir3E8JiyCR3xrqWKhtz003niGTm3ypAEkbZPIZmrvzdSGhozDX07mSU0y14m/ud1Ewxq/VSoOEGu2OKjIJEEI5KdT4ZCc4ZyagllWtishI2ppgxtSUVbgrd88ippVSveZaV6d1Wu1/I6CnAKZ3ABHlxDHW6hAU1goOAZXuHNMc6L8+58LEbXnHznBP7A+fwB+4WRFg==</latexit>\n˜x\n<latexit sha1_base64=\"ptYS2YtzpEm19IJM5Kr9s0kFMHA=\">AAAB+3icbVDLSsNAFJ3UV62vWJduBovgqiRVsMuCG5cV7AOaUCaTSTt08mDmRlpCfsWNC0Xc+iPu/BsnbRbaemDgcM693DPHSwRXYFnfRmVre2d3r7pfOzg8Oj4xT+t9FaeSsh6NRSyHHlFM8Ij1gINgw0QyEnqCDbzZXeEPnphUPI4eYZEwNySTiAecEtDS2Kw7wIXPMickMPWCbJ7nY7NhNa0l8CaxS9JAJbpj88vxY5qGLAIqiFIj20rAzYgETgXLa06qWELojEzYSNOIhEy52TJ7ji+14uMglvpFgJfq742MhEotQk9PFhHVuleI/3mjFIK2m/EoSYFFdHUoSAWGGBdFYJ9LRkEsNCFUcp0V0ymRhIKuq6ZLsNe/vEn6raZ93Ww93DQ67bKOKjpHF+gK2egWddA96qIeomiOntErejNy48V4Nz5WoxWj3DlDf2B8/gDvQpT9</latexit>\n\u0000 o ↵\n<latexit sha1_base64=\"XfdbJCoJT3eMoz4Sp59lm8GkcII=\">AAAB+XicbVDLSsNAFJ3UV62vqEs3g0VwVZIq2GXBjcsK9gFNCJPJpB06jzAzKZTQP3HjQhG3/ok7/8Zpm4W2HrhwOOde7r0nzhjVxvO+ncrW9s7uXnW/dnB4dHzinp71tMwVJl0smVSDGGnCqCBdQw0jg0wRxGNG+vHkfuH3p0RpKsWTmWUk5GgkaEoxMlaKXDdICDMoKgLFoUzTeeTWvYa3BNwkfknqoEQncr+CROKcE2EwQ1oPfS8zYYGUoZiReS3INckQnqARGVoqECc6LJaXz+GVVRKYSmVLGLhUf08UiGs947Ht5MiM9bq3EP/zhrlJW2FBRZYbIvBqUZozaCRcxAATqgg2bGYJworaWyEeI4WwsWHVbAj++subpNds+DeN5uNtvd0q46iCC3AJroEP7kAbPIAO6AIMpuAZvII3p3BenHfnY9VaccqZc/AHzucPlnmTmA==</latexit>\nMixup sampleInterpolation path\nFigure 2: The on-manifold and off-manifold samples generated by our calibration procedure. Mixup adopts a\ncoarse linear interpolation and the generated data point may deviate from the data manifold.\nwe constrain x′∗to stay in a small neighborhood\nof x, which ensures x′∗to stay close to the actual\ndata manifold.\nThis is different from existing interpolation\nmethods such as Mixup (Zhang et al., 2018; Verma\net al., 2019). These methods adopt coarse linear\ninterpolations either in the input space or latent fea-\nture space, and the generated data may signiﬁcantly\ndeviate from the data manifold.\nNote that our method not only interpolates x but\nalso y. This can yield a soft label for x′∗, when x\nand ˜x belong to different classes. Such an inter-\npolation is analogous to semi-supervised learning,\nwhere soft pseudo labels are generated for the un-\nlabelled data. These soft-labelled data essentially\ninduce a smoothing effect, and prevent the model\nfrom making overconﬁdent predictions toward one\nsingle class.\nWe remark that our method is more adaptive\nthan the label smoothing method (M ¨uller et al.,\n2019). As each interpolated data point involves\nat most two classes, it is unnecessary to distribute\nprobability mass to other classes in the soft label. In\ncontrast, label smoothing is more rigid and enforces\nall classes to have equally nonzero probability mass\nin the soft label.\nWe then deﬁne the on-manifold regularizer as\nRon(g◦f) = E(x′,y′)∼SonDKL(y′,g ◦f(x′)),\nwhere Son denotes the set of all pseudo labelled\ndata generated by our interpolation method, and\nDKL denotes the KL-divergence between two prob-\nability simplices.\n3.2 Off-manifold Regularization\nThe off-manifold regularizer, R2, encourages the\nmodel to yield low conﬁdence outputs for sam-\nples outside the data manifold, and thus mitigates\nAlgorithm 1 Our Proposed Efﬁcient Stochastic Op-\ntimization Algorithm for Solving (4). dis the di-\nmension of features.\nfor # training iterations do\nSample a mini-batch B = {xi,yi}from S.\n// Generate on-manifold samples:\nFor each xi ∈B, randomly select {˜xi,˜yi}\nfrom B, initialize x′\ni ←xi + vi with vi ∼\nUNIF[−δon,δon]d\n∆′\ni ←sign(∇x′\ni\nDx(f(x′\ni),f(˜xi)))\nx′\ni ←Π∥x′\ni−xi∥∞≤δon(x′\ni −δon∆′\ni)\ny′←(1 −δy)yi + δy˜yi\n// Generate off-manifold samples:\nFor each xi ∈B, initialize x′′\ni ←xi+v′\ni with\nv′\ni ∼UNIF[−δoﬀ,δoﬀ]d\n∆′′\ni ←sign(∇x′′\ni\nℓ(g◦f(x′′\ni),y)\nx′′\ni ←Π∥x′′\ni −xi∥∞=δoﬀ (x′′\ni + δoﬀ∆′′\ni)\nUpdate θusing ADAM\nend for\nthe over-conﬁdence issue for out-of-distribution\n(OOD) data. Speciﬁcally, given a training sample\n(x,y), we generate an off-manifold pseudo sample\nx\n′′∗by:\nx\n′′∗= max\nx′′∈S(x,δoﬀ)\nℓ(g◦f(x′′),y), (7)\nwhere S(x,δoﬀ) denotes an ℓ∞sphere centered at\nx with a radius δoﬀ.\nSince we expectx′′∗to mimic OOD data, we ﬁrst\nneed to choose a relatively large δoﬀ such that the\nsphere S(x,δoﬀ) can reach outside the data mani-\nfold. Then, we generate the pseudo off-manifold\nsample from the sphere along the adversarial direc-\ntion. Existing literature (Stutz et al., 2019; Gilmer\net al., 2018) has shown that such an adversarial\ndirection points outward the data manifold.\n1330\nBy penalizing the prediction conﬁdence for these\noff-manifold samples, we are able to encourage low\nprediction conﬁdence for OOD data. Hence, we\ndeﬁne the off-manifold regularizer as\nRoﬀ(g◦f) = Ex′′∼Soﬀ\n−H(g◦f(x′′)), (8)\nwhere Soﬀ denotes the set of all generated off-\nmanifold samples, and H(·) denotes the entropy of\nthe probability simplex.\n3.3 Model Training\nWe can adopt stochastic gradient-type algorithms\nsuch as ADAM (Kingma and Ba, 2014) to opti-\nmize (4). At each iteration, we need to ﬁrst solve\ntwo inner optimization problems in (5) and (7),\nand then plug x′and x′′into (4) to compute the\nstochastic gradient. The two inner problems can\nbe solved using the projected sign gradient update\nfor multiple steps. In practice, we observe that one\nsingle update step with random initialization is al-\nready sufﬁcient to efﬁciently optimize θ. Such a\nphenomenon has also been observed in existing lit-\nerature on adversarial training (Wong et al., 2019).\nWe summarize the overall training procedure in\nAlgorithm 1.\n4 Experiments\nTo evaluate calibration performance for in-\ndistribution data, we measure the expected calibra-\ntion error (ECE) and the misclassiﬁcation detection\nscore. For out-of-distribution data, we measure the\nOOD detection score.\nWe detect the misclassiﬁed and OOD samples by\nmodel conﬁdence, which is the output probability\nassociated with the predicted label ˆP(X). Specif-\nically, we setup a conﬁdence threshold τ ∈[0,1],\nand take the samples with conﬁdence below the\nthreshold, i.e., ˆP(X) <τ , as the misclassiﬁed or\nOOD samples. We can compute the detection F1\nscore for every τ: F1(τ), and obtain a calibration\ncurve (F1(τ) vs. τ). Then, we set τupper as the up-\nper bound of the conﬁdence threshold, since a well\ncalibrated model should provide probabilities that\nreﬂect the true likelihood and it is not reasonable to\nuse a large τ to detect them. We use the empirical\nNormalized Bounded Area Under the Calibration\nCurve (NBAUCC) as the overall detection score:\nNBAUCCτupper = 1\nM\nM∑\ni=1\nF1\n(τupper\nM i\n)\n,\nwhere M is the number of sub-intervals for the\nnumerical integration. We set M = 50 through-\nout the following experiments. Note that the tradi-\ntional binary classiﬁcation metrics, e.g., AUROC\nand AUPR, cannot measure the true calibration be-\ncause the model can still achieve high scores even\nthough it has high conﬁdences for the misclassiﬁed\nand OOD samples. We provide more explanations\nof the metrics in Appendix C. We report the per-\nformance when τupper = 0.5 here and the results\nwhen τupper = 0.7 and 1 in Appendix D.\n4.1 Datasets\nFor each dataset, we construct an in-distribution\ntraining set, an in-distribution testing set, and an\nOOD testing set. Speciﬁcally, we use the following\ndatasets:\n20NG1. The 20 Newsgroups dataset (20NG) con-\ntains news articles with 20 categories. We use Stan-\nford Sentiment Treebank (SST-2) (Socher et al.,\n2012) as the OOD data.\n20NG15. We take the ﬁrst 15 categories of 20NG\nas the in-distribution data and the other 5 categories\n(20NG5) as the OOD data.\nWOS (Kowsari et al., 2017). Web of Science\n(WOS) dataset contains scientiﬁc articles with 134\ncategories. We use AGnews (Zhang et al., 2015) as\nthe OOD data.\nWOS100. We use the ﬁrst 100 classes of WOS as\nthe in-distribution data and the other 34 classes\n(WOS34) as the OOD data.\nYahoo (Chang et al., 2008). This dataset contains\nquestions with 10 categories posted to ‘Yahoo! An-\nswers’. We randomly draw 2000 from 140,000\nsamples for each category as the training set. We\nuse Yelp (Zhang et al., 2015) as the OOD data.\nYahoo8. We use the ﬁrst 8 classes of Yahoo as the\nin-distribution data and the other 2 classes (Yahoo2)\nas the OOD data.\nThe testing set of OOD detection consists of\nthe in-distribution testing set and the OOD data.\nMore dataset details can be found in Appendix A.\nWe remark that 20NG15, WOS100, and Yahoo8 are\nincluded to make OOD detection more challenging,\nas the OOD data and the training data come from\nsimilar data sources.\n4.2 Baselines\nWe consider the following baselines:\n•BERT (Devlin et al., 2019) is a pre-trained base\nBERT model stacked with one linear layer.\n1We use the 20 Newsgroups dataset from: http://\nqwone.com/˜jason/20Newsgroups/\n1331\n•Temperature Scaling (TS)(Guo, 2017) is a post-\nprocessing calibration method that learns a single\nparameter to rescale the logits on the development\nset after the model is ﬁne-tuned.\n• Monte Carlo Dropout (MCDP) (Gal and\nGhahramani, 2016) applies dropout at testing time\nfor multiple times and then averages the outputs.\n•Label Smoothing (LS) (M¨uller et al., 2019)\nsmoothes the one-hot label by distributing a certain\nprobability mass to other non ground-truth classes.\n• Entropy Regularized Loss (ERL) (Pereyra\net al., 2017) adds a entropy penalty term to pre-\nvent DNNs from being over-conﬁdent.\n•Virtual Adversarial Training (V AT) (Miyato\net al., 2018) introduces a smoothness-inducing ad-\nversarial regularizer to encourage the local Lips-\nchitz continuity of DNNs.\n•Mixup (Zhang et al., 2018; Thulasidasan et al.,\n2019) augments training data by linearly interpo-\nlating training samples in the input space.\n• Manifold-mixup (M-mixup) (Verma et al.,\n2019) is an extension of Mixup, which interpolates\ntraining samples in the hidden feature space.\n4.3 Implementation Details\nWe use ADAM (Kingma and Ba, 2014) with β1 =\n0.9 and β2 = 0 .999 as the optimizer. For our\nmethod, we simply set λon = λoﬀ = 1 ,δon =\n10−4,δoﬀ = 10 −3, and δy = 0 .1 for all the ex-\nperiments. We also conduct an extensive hyper-\nparameter search for the baselines. See more de-\ntails in Appendix B.\n4.4 Main Results\nOur main results are summarized as follows:\nExpected Calibration Error: Table 1 reports the\nECE and predictive accuracy of all the methods.\nOur method outperforms all the baselines on all the\ndatasets in terms of ECE except for Yahoo, where\nonly ERL is slightly better. Meanwhile, our method\ndoes not sacriﬁce the predictive accuracy.\nMisclassiﬁcation Detection: Table 2 compares\nthe NBAUCC0.5 on misclassiﬁcation detection of\ndifferent methods. As shown, our method outper-\nforms all the baselines on all the six datasets.\nOut-of-distribution Detection: Table 2 reports\nthe NBAUCC0.5 on OOD detection of different\nmethods. Again, our method achieves the best per-\nformance on all the six datasets. The improvement\nis particularly remarkable on the 20NG dataset,\nwhere NBAUCC0.5 increases from 47.00 to 63.92\ncompared with the strongest baseline. We also ﬁnd\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013\n/uni00000037/uni0000004b/uni00000055/uni00000048/uni00000056/uni0000004b/uni00000052/uni0000004f/uni00000047\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a/uni00000029/uni00000014/uni00000003/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048\n/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000047/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013\n/uni00000037/uni0000004b/uni00000055/uni00000048/uni00000056/uni0000004b/uni00000052/uni0000004f/uni00000047\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000032/uni00000032/uni00000027/uni00000003/uni00000047/uni00000048/uni00000057/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000025/uni00000028/uni00000035/uni00000037/uni00000007\n/uni00000030/uni00000026/uni00000027/uni00000033\n/uni00000037/uni00000036\n/uni00000028/uni00000035/uni0000002f\n/uni0000002f/uni00000036\n/uni00000030/uni0000004c/uni0000005b/uni00000058/uni00000053\n/uni00000030/uni00000010/uni00000050/uni0000004c/uni0000005b/uni00000058/uni00000053\n/uni00000039/uni00000024/uni00000037\n/uni00000032/uni00000058/uni00000055/uni00000056\nFigure 3: Calibration curves of OOD detection and\nmisclassiﬁcation detection on WOS. Our method can\nachieve high F1 scores starting from a small threshold\nwhich indicates that it indeed provides low conﬁdences\nfor misclassiﬁed and OOD samples; the F1 scores of\nthe baselines peak at high thresholds which indicates\nthat they are poorly calibrated.\nthat detecting the unseen classes from the original\ndataset is much more challenging than detecting\nOOD samples from a totally different dataset.\nSigniﬁcance Test: We perform the Wilcoxon\nsigned rank test (Wilcoxon, 1992) for signiﬁcance\ntest. For each dataset, we conduct experiments us-\ning 5 different random seeds with signiﬁcance level\nα= 0.5. We ﬁnd that our model outperforms other\nbaselines on all the datasets signiﬁcantly, with only\nexceptions of ERL in ECE on Yahoo and ERL in\nmisclassiﬁcation detection on 20NG.\n4.5 Parameter Study\nWe investigate the effects of the interpolation pa-\nrameters for on-manifold data, i.e., δon and δy, and\nthe perturbation size for off-manifold samples, i.e.,\nδoﬀ. The default values are δon = 10 −4,δoﬀ =\n10−3 and δy = 0.1. Figure 4 shows the reuslts on\n20NG15, 20NG, WOS100, and WOS datasets. Our\nresults are summarized as follows:\n•The performance of all metrics versus δon is sta-\nble within a large range from 10−5 to 10−2. When\nδon is larger than 10−1, the predictive accuracy be-\ngins to drop.\n•The performance versus δoﬀ is more sensitive:\n(1) when δoﬀ is too small, ECE increases dramati-\ncally becasue the generated off-manifold samples\nare too close to the manifold and make the model\nunder-conﬁdent. (2) when δoﬀ is too large, the\noff-manifold regularization is too weak and OOD\ndetection performance drops.\n•In general, δon should be small to let x′stay on\nthe data manifold while δoﬀ should be large to let\nx′′leave the data manifold. However, the regular-\nization effect of Ron (Roﬀ) depends on both λon\n1332\nModel ECE Accuracy\n20NG15 20NG WOS100 WOS Yahoo8 Yahoo 20NG15 20NG WOS100 WOS Yahoo8 Yahoo\nBERT 9.24 11 .61 6 .81 6 .74 10 .11 10 .54 87.42 84 .55 81 .94 79 .40 73 .58 71 .89\nTS 4.42 8 .17 3 .63 4 .43 5 .18 4 .24 87.42 84 .55 81 .94 79 .40 73 .58 71 .89\nMCDP 6.88 9 .17 4 .00 3 .55 6 .54 6 .72 87.45 84 .55 82 .09 79 .67 73 .67 71 .99\nLS 4.35 6 .15 4 .35 4 .67 4 .89 3 .61 87.54 85 .02 81 .95 79 .47 73 .66 71 .54\nERL 7.16 6 .10 3 .74 3 .35 3 .42 2.96 87.67 84 .83 81 .96 79 .48 73 .63 72 .01\nV AT 9.07 11 .28 7 .27 6 .76 10 .96 7 .92 87.61 85 .20 81 .65 79 .71 73 .71 72 .08\nMixup 5.98 9 .02 4 .72 4 .21 4 .60 5 .18 87.49 84 .86 81 .97 79 .51 73 .88 71 .82\nM-mixup 5.04 7 .78 6 .48 6 .68 7 .01 6 .07 87.40 84 .45 81 .77 79 .57 73 .67 72 .03\nOurs 3.69 4.43 3.24 3.04 3.03 3.42 87.44 84 .53 81 .59 79 .06 73 .71 72 .17\nTable 1: ECE and accuracy (in percentage). We report the average performance of 5 random initializations.\nMisclassiﬁcation Detection OOD Detection\nData 20NG15 20NG WOS100 WOS Yahoo8 Yahoo 20NG15 20NG WOS100 WOS Yahoo 8 Yahoo\n( OOD ) 20NG5 SST-2 WOS34 AGnews Yahoo2 Yelp\nBERT 2.30 2 .86 16 .53 20 .52 7 .47 8 .43 2.66 21 .65 23 .12 49 .84 8 .35 13 .88\nTS 6.08 5 .74 21 .20 23 .76 10 .48 12 .74 6.62 32 .64 28 .12 53 .32 11 .55 20 .27\nMCDP 4.37 5 .28 20 .44 24 .16 10 .12 10 .75 3.99 25 .10 27 .28 53 .52 9 .98 15 .93\nLS 4.72 6 .75 20 .37 23 .56 11 .19 16 .15 5.70 41 .08 27 .12 58 .48 12 .02 19 .81\nERL 8.54 10 .35 20 .49 25 .13 12 .89 15 .47 8.78 47 .00 27 .73 56 .67 13 .78 23 .47\nV AT 2.52 3 .36 18 .70 19 .96 6 .54 10 .37 2.96 29 .62 23 .41 54 .60 7 .42 17 .65\nMixup 4.99 4 .51 20 .65 24 .80 10 .75 11 .29 5.86 31 .84 26 .77 58 .02 11 .62 19 .84\nM-mixup 2.16 3 .16 16 .94 19 .39 9 .09 11 .79 2.36 26 .08 24 .08 51 .39 10 .08 22 .41\nOurs 9.10 10.76 26.93 30.80 14.34 17.88 9.69 63.92 35.60 71.13 14.94 29.40\nTable 2: NBAUCC0.5 on misclassiﬁcation detection and OOD detection. We report the average performance of 5\nrandom initializations.\n(λoﬀ) and δon (δoﬀ). Therefore, it is not necessary\nto let δon be smaller than δoﬀ. We can also tune\nλon and λoﬀ to achieve better performance.\n•The performance versus δy is relatively stable\nexcept for the metric of ECE. When δy is larger\nthan 0.2, ECE begins to increase.\n4.6 Ablation Study\nWe investigate the effectiveness of the on-manifold\nregularizer Ron and the off-manifold regularizer\nRoﬀ via ablation studies. Table 3 shows the results\non the 20NG15 and 20NG datasets.\n•As expected, removing either component in our\nmethod would result in a performance drop. It\ndemonstrates that these two components comple-\nment each other. All the ablation models outper-\nform the BERT baseline model, which demon-\nstrates the effectiveness of each module.\n•We observe that the optimalδon is different when\nusing only Ron. This indicates that the hyperpa-\nrameters of Ron and Roﬀ should be jointly tuned,\ndue to the joint effect of both components.\n•By removing Roﬀ, we observe a severe OOD per-\nformance degradation on the 20NG dataset (from\n63.92 to 43.87). This indicates that Roﬀ is vital\nto out-of-distribution calibration. Meanwhile, the\nperformance degradation is less severe on 20NG15\n(from 9.69 to 7.94). It is because Ron can also help\ndetect the OOD samples from similar data sources.\n(20NG5).\n•By removing Ron, the in-distribution calibration\nperformance drops as expected.\n5 Related Works and Discussion\nOther Related Works: Lakshminarayanan et al.\n(2017) propose a model ensembling approach to\nimprove model calibration. They ﬁrst train multi-\nple models with different initializations and then\naverage their predictions. However, ﬁne-tuning\nmultiple language models requires extremely inten-\nsive computing resources.\nKumar et al. (2018) propose a differentiable sur-\nrogate for the expected calibration error, called\n1333\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n /uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013\n/uni0000001b/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n /uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013\n/uni0000001b/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n /uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013\n/uni0000001b/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.5\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\nFigure 4: Parameter study of δon, δoﬀ and δy.\nDataset 20NG15 20NG\nModel δon Accuracy ECE OOD Mis Accuracy ECE OOD Mis\nBERT - 87.42 9 .24 2 .66 2 .30 84.55 11 .61 21 .65 2 .86\nw/ Roﬀ - 86.48 6 .51 6 .22 6 .09 83.90 7 .98 55 .40 7 .12\nw/ Ron 10−2 88.73 2 .77 7 .94 8 .08 85.60 5 .00 35 .80 8 .66\nw/ Ron 10−3 88.29 3 .52 7 .39 6 .83 85.69 4 .43 38 .00 9 .01\nw/ Ron 10−4 87.93 4 .48 5 .33 4 .83 85.12 6 .76 43 .87 5 .95\nw/ Ron 10−5 87.61 4 .69 3 .83 4 .73 85.39 6 .35 35 .70 5 .30\nw/ Both 10−4 87.44 3 .69 9 .69 9 .10 84.53 4 .43 63 .92 10 .76\nTable 3: Ablation study on the 20NG15 and 20NG datasets. For OOD detection and misclassiﬁcation detection, we\nreport BAUCC0.5. We set δy = 0.1 and δoﬀ = 10−3.\nmaximum mean calibration error (MMCE), using\nkernel embedding. However, such a kernel embed-\nding method is computationally expensive and not\nscalable to the large pre-trained language models.\nAccelerating Optimization: To further improve\nthe calibration performance of our method, we can\nleverage some recent minimax optimization tech-\nniques to better solve the two inner optimization\nproblems in (5) and (7) without increasing the com-\nputational complexity. For example, Zhang et al.\n(2019) propose an efﬁcient approximation algo-\nrithm based on Pontryagin’s Maximal Principle to\nreplace the multi-step projected gradient update for\nthe inner optimization problem. Another option is\nthe learning-to-learn framework (Jiang et al., 2018),\nwhere the inner problem is solved by a learnt opti-\nmizer. These techniques can help us obtain x′and\nx′′more efﬁciently.\nConnection to Robustness: The interpolated\ntraining samples can naturally promote the local\nLipschitz continuity of our model. Such a local\nsmoothness property has several advantages: (1)\nIt makes the model more robust to the inherent\nnoise in the data, e.g., noisy labels; (2) it is partic-\nularly helpful to prevent overﬁtting and improve\ngeneralization, especially for low-resource tasks.\n1334\nExtensions: Our method is quite general and can\nbe applied to other deep neural network-based prob-\nlems besides language model ﬁne-tuning.\n6 Conclusion\nWe have proposed a regularization method to mit-\nigate miscalibration of ﬁne-tuned language mod-\nels from a data augmentation perspective. Our\nmethod imposes two new regularizers using gener-\nated on- and off- manifold samples to improve both\nin-distribution and out-of-distribution calibration.\nExtensive experiments on six datasets demonstrate\nthat our method outperforms state-of-the-art cali-\nbration methods in terms of expected calibration\nerror, misclassiﬁcation detection and OOD detec-\ntion.\nAcknowledgement\nThis work was supported in part by the National\nScience Foundation award III-2008334, Amazon\nFaculty Award, and Google Faculty Award.\nReferences\nCharles Blundell, Julien Cornebise, Koray\nKavukcuoglu, and Daan Wierstra. 2015. Weight\nuncertainty in neural network. In International Con-\nference on Machine Learning, pages 1613–1622.\nMing-Wei Chang, Lev Ratinov, Dan Roth, and Vivek\nSrikumar. 2008. Importance of semantic representa-\ntion: Dataless classiﬁcation. In Proceedings of the\nTwenty-Third AAAI Conference on Artiﬁcial Intelli-\ngence, page 830–835.\nAlexandra Chouldechova. 2017. Fair prediction with\ndisparate impact: A study of bias in recidivism pre-\ndiction instruments. Big data, 5(2):153–163.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a\nbayesian approximation: Representing model uncer-\ntainty in deep learning. In International Conference\non Machine Learning, pages 1050–1059.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani.\n2017. Deep bayesian active learning with image\ndata. In International Conference on Machine\nLearning, pages 1183–1192.\nJustin Gilmer, Luke Metz, Fartash Faghri, Samuel S\nSchoenholz, Maithra Raghu, Martin Wattenberg, Ian\nGoodfellow, and G Brain. 2018. The relationship\nbetween high-dimensional geometry and adversarial\nexamples. arXiv preprint arXiv:1801.02774.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In International Conference on Machine\nLearning, pages 1321–1330.\nHongyu Guo. 2017. A deep network with visual text\ncomposition behavior. In Proceedings of the 55th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 2: Short Papers) , pages\n372–377, Vancouver, Canada. Association for Com-\nputational Linguistics.\nDan Hendrycks and Kevin Gimpel. 2016. A baseline\nfor detecting misclassiﬁed and out-of-distribution\nexamples in neural networks. In International Con-\nference on Learning Representations.\nHaoming Jiang, Zhehui Chen, Yuyang Shi, Bo Dai, and\nTuo Zhao. 2018. Learning to defense by learning to\nattack. arXiv preprint arXiv:1811.01213.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xi-\naodong Liu, Jianfeng Gao, and Tuo Zhao. 2020.\nSMART: Robust and efﬁcient ﬁne-tuning for pre-\ntrained natural language models through principled\nregularized optimization. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2177–2190.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classiﬁcation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1746–1751.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nKamran Kowsari, Donald E Brown, Mojtaba Hei-\ndarysafa, Kiana Jafari Meimandi, , Matthew S Ger-\nber, and Laura E Barnes. 2017. Hdltex: Hierarchical\ndeep learning for text classiﬁcation. In IEEE Inter-\nnational Conference on Machine Learning and Ap-\nplications (ICMLA), pages 364–371.\nAviral Kumar, Sunita Sarawagi, and Ujjwal Jain. 2018.\nTrainable calibration measures for neural networks\nfrom kernel mean embeddings. In International\nConference on Machine Learning , pages 2805–\n2814.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles. In\nAdvances in Neural Information Processing Systems,\npages 6402–6413.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\n1335\nof language representations. In International Con-\nference on Learning Representations.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.\n2018. A simple uniﬁed framework for detecting out-\nof-distribution samples and adversarial attacks. In\nAdvances in Neural Information Processing Systems,\npages 7167–7177.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nChristos Louizos and Max Welling. 2017. Multiplica-\ntive normalizing ﬂows for variational Bayesian neu-\nral networks. In International Conference on Ma-\nchine Learning, pages 2218–2227.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama,\nand Shin Ishii. 2018. Virtual adversarial training:\na regularization method for supervised and semi-\nsupervised learning. IEEE transactions on pat-\ntern analysis and machine intelligence, 41(8):1979–\n1993.\nRafael M ¨uller, Simon Kornblith, and Geoffrey E Hin-\nton. 2019. When does label smoothing help? In\nAdvances in Neural Information Processing Systems,\npages 4696–4705.\nMahdi Pakdaman Naeini, Gregory F Cooper, and Milos\nHauskrecht. 2015. Obtaining well calibrated proba-\nbilities using bayesian binning. In Proceedings of\nthe Twenty-Ninth AAAI Conference on Artiﬁcial In-\ntelligence, page 2901–2907.\nAlexandru Niculescu-Mizil and Rich Caruana. 2005.\nPredicting good probabilities with supervised learn-\ning. In International Conference on Machine Learn-\ning, page 625–632.\nGabriel Pereyra, George Tucker, Jan Chorowski,\nŁukasz Kaiser, and Geoffrey Hinton. 2017. Regular-\nizing neural networks by penalizing conﬁdent output\ndistributions. arXiv preprint arXiv:1701.06548.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nYanyao Shen, Hyokun Yun, Zachary C. Lipton,\nYakov Kronrod, and Animashree Anandkumar.\n2018. Deep active learning for named entity recogni-\ntion. In International Conference on Learning Rep-\nresentations.\nAditya Siddhant and Zachary C. Lipton. 2018. Deep\nBayesian active learning for natural language pro-\ncessing: Results of a large-scale empirical study.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2904–2909, Brussels, Belgium. Association\nfor Computational Linguistics.\nRichard Socher, Yoshua Bengio, and Christopher D.\nManning. 2012. Deep learning for NLP (without\nmagic). In Proceedings of the 50th Annual Meet-\ning of the Association for Computational Linguistics:\nTutorial Abstracts, page 5, Jeju Island, Korea. Asso-\nciation for Computational Linguistics.\nDavid Stutz, Matthias Hein, and Bernt Schiele. 2019.\nDisentangling adversarial robustness and generaliza-\ntion. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , pages\n6976–6987.\nSunil Thulasidasan, Gopinath Chennupati, Jeff A\nBilmes, Tanmoy Bhattacharya, and Sarah Michalak.\n2019. On mixup training: Improved calibration and\npredictive uncertainty for deep neural networks. In\nAdvances in Neural Information Processing Systems,\npages 13888–13899.\nVikas Verma, Alex Lamb, Christopher Beckham, Amir\nNajaﬁ, Ioannis Mitliagkas, David Lopez-Paz, and\nYoshua Bengio. 2019. Manifold mixup: Better rep-\nresentations by interpolating hidden states. In In-\nternational Conference on Machine Learning, pages\n6438–6447.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, pages 3266–3280.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nFrank Wilcoxon. 1992. Individual comparisons by\nranking methods. In Breakthroughs in statistics ,\npages 196–202. Springer.\nEric Wong, Leslie Rice, and J Zico Kolter. 2019. Fast\nis better than free: Revisiting adversarial training.\nIn International Conference on Learning Represen-\ntations.\nDinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanx-\ning Zhu, and Bin Dong. 2019. You only propagate\nonce: Accelerating adversarial training via maximal\nprinciple. In Advances in Neural Information Pro-\ncessing Systems, pages 227–238.\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin,\nand David Lopez-Paz. 2018. mixup: Beyond empir-\nical risk minimization. In International Conference\non Learning Representations.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in neural information pro-\ncessing systems, pages 649–657.\n1336\nA Dataset Details\n#Train #Dev #Test #Label\n20NG15 7010 1753 5833 15\n20NG5 - - 1699 5\n20NG 9051 2263 7532 20\nSST-2 - - 1822 2\nWOS100 16794 4191 13970 100\nWOS34 - - 4824 34\nWOS 22552 5639 18794 134\nAGnews - - 7600 4\nYahoo8 16000 4000 48000 8\nYahoo2 - - 12000 2\nYahoo 20000 5000 60000 10\nYelp - - 38000 2\nTable 4: Dataset statistics and dataset split. ’-’ denotes\nthat this part is not used. The original Yahoo dataset\ncontains 140,000 training samples for each class which\nis too large; we randomly draw 2,000 and 500 samples\nfor each class as our training and development set.\nAll the data are publicly available. We also offer\nthe links to the data as follows:\n1. 20NG: http://qwone.com/˜jason/\n20Newsgroups/.\n2. SST-2: https://nlp.stanford.edu/\nsentiment/index.html.\n3. WOS: https://data.mendeley.com/\ndatasets/9rw3vkcfy4/2.\n4. AGnews: https://github.com/yumeng5/\nWeSTClass.\n5. Yahoo: https://www.kaggle.com/\nsoumikrakshit/yahoo-answers-dataset.\n6. Yelp: https://github.com/yumeng5/\nWeSTClass.\nB Experiment Details\nWe use ADAM (Kingma and Ba, 2014) with β1 =\n0.9 and β2 = 0 .999 as the optimizer in all the\ndatasets. We use the learning rate of 5 ×10−5\nand batch size 32 except 1 ×10−5 and 16 for\nYahoo8 and Yahoo. We set the maximum num-\nber of epochs to 5 in Yahoo8 and Yahoo and 10 in\nthe other datasets. We use the dropout rate of 0.1\nas in (Devlin et al., 2019). The documents are tok-\nenized using wordpieces and are chopped to spans\nno longer than 150 tokens on 20NG15 and 20NG\nand 256 on other datasets..\nHyper-parameters: For our method, we use\nλon = λoﬀ = 1 , δon = 10 −4, δoﬀ = 10 −3 and\nδy = 0 .1 for all the datasets. We then conduct\nan extensive hyper-parameter search for the base-\nlines: for label smoothing, we search the smooth-\ning parameter from {0.05,0.1}as in (M¨uller et al.,\n2019); for ERL, the penalty weight is chosen from\n{0.05,0.1,0.25,0.5,1,2.5,5}; for V AT, we search\nthe perturbation size in {10−3,10−4,10−5}as in\n(Jiang et al., 2020); for Mixup, we search the in-\nterpolation parameter from {0.1,0.2,0.3,0.4}as\nsuggested in (Zhang et al., 2018; Thulasidasan\net al., 2019); for Manifold-mixup, we search from\n{0.2,0.4,1,2,4}. We perform 10 stochastic for-\nward passes for MCDP at test time. For hyper-\nparameter tuning, we run all the methods 5 times\nand then take the average. The hyper-parameters\nare selected to get the best ECE on the development\nset of each dataset. The interpolation of Mixup is\nperformed on the input embeddings obtained from\nthe ﬁrst layer of the language model; the interpo-\nlation of Manifold-mixup is performed on the fea-\ntures obtained from the last layer of the language\nmodel.\nC Metrics of Misclassiﬁcation and\nOut-of-distribution detection\nExisting works on out-of-distribution (OOD) de-\ntection and misclassiﬁcation detection (Hendrycks\nand Gimpel, 2016) use traditional binary classiﬁ-\ncation metrics, e.g., AUPR and AUROC. As we\ndiscussed in Section 1 and 2, the output probability\nof a calibrated model should reﬂect the true likeli-\nhood. However, AUROC and AUPR cannot reﬂect\ntrue model calibration because the model can still\nachieve high scores even though it has high con-\nﬁdences for misclassiﬁed and OOD samples. We\nargue that it is more reasonable to use the Normal-\nized Bounded Area Under the Calibration Curve\n(NBAUCC) deﬁned as in Section 4.\nTable 5 shows an illustrative example. As can\nbe seen, h1 is better calibrated than h2, since h1\ncan detect OOD samples under a wide range of\nthreshold ( 0.15 < τ < 0.9) while h2 requires\nan absurdly large threshold ( 0.85 < τ < 0.9).\nHowever, if we use the traditional AUPR and\nAUROC metrics, we will conclude that h1 is as\nwell calibrated as h2 since AUPRh1 = AUPRh2\n= 0.417 and AUROCh1 = AUROCh2= 1. On the\n1337\nModel Conﬁdence Optimal τ AUPR AUROC NBAUCC1 NBAUCC0.5xin,1 xin,2 xout,1 xout,2\nh1 (Miscalibrated) 0.9 0.95 0.8 0.85 (0.85,0.9) 0.417 1 0.145 0\nh2 (Well-calibraterd) 0.9 0.95 0.1 0.15 (0.15,0.9) 0.417 1 0.845 0.773\nTable 5: NBAUCC vs. AUROC/AUPR\nother hand, if we use NBAUCC, we will have\nNBAUCCh1\n1 = 0.845 > NBAUCCh1\n1 = 0.145,\nor NBAUCCh1\n0.5 = 0 .773 > NBAUCCh1\n0.5 = 0\nwhich can reﬂect the true calibration of the two\nclassiﬁers.\nWe remark that it is more appropriate to use\nNBAUCC0.5 than NBAUCC1 since a calibrated\nmodel should provide low conﬁdences for the mis-\nclassiﬁed and OOD samples and it is unreasonable\nto use a large threshold to detect them.\nD Additional Results\nTable 6 and 7 report the NBAUCCs of all the meth-\nods on misclassiﬁcation and OOD detection when\nτupper = 0.7 and τupper = 1. Table 8 and 9 report\nthe ablation study results of all the methods when\nτupper = 0.7 and τupper = 1. Figure 5 and 6 report\nthe parameter study results of all the methods when\nτupper = 0.7 and τupper = 1.\n1338\nMisclassiﬁcation Detection OOD Detection\nData 20NG15 20NG WOS100 WOS Yahoo8 Yahoo 20NG15 20NG WOS100 WOS Yahoo 8 Yahoo\n( OOD ) 20NG5 SST-2 WOS34 AGnews Yahoo2 Yelp\nBERT 17.86 18.48 35.84 39.08 28.83 29.67 13.52 42.86 40.04 59.42 26.63 38.30\nTS 23.74 23.58 38.34 40.76 31.10 32.63 19.74 50.00 42.96 60.70 28.30 42.07\nMCDP 23.58 24.58 38.54 41.20 31.43 32.57 16.82 44.96 42.74 60.72 27.47 39.83\nLS 21.22 23.24 37.22 40.12 30.93 34.30 18.76 55.24 42.54 63.62 27.87 40.77\nERL 24.04 25.68 37.87 41.17 32.27 33.90 22.10 54.20 42.67 62.10 28.73 43.37\nV AT 17.80 17.50 35.90 38.80 27.87 31.13 13.00 49.00 40.30 62.50 25.80 40.63\nMixup 21.42 21.86 37.72 40.92 30.97 32.97 16.70 50.94 42.13 62.98 28.00 44.57\nM-mixup 17.86 19.24 36.48 38.33 29.67 31.50 14.06 44.56 41.51 61.30 27.43 44.20\nOurs 26.50 28.10 40.93 43.70 33.07 35.13 23.20 66.36 46.73 68.10 29.70 46.43\nTable 6: NBAUCC1 on misclassiﬁcation detection and OOD detection. We report the average performance of 5\nrandom initializations.\nMisclassiﬁcation Detection OOD Detection\nData 20NG15 20NG WOS100 WOS Yahoo8 Yahoo 20NG15 20NG WOS100 WOS Yahoo 8 Yahoo\n( OOD ) 20NG5 SST-2 WOS34 AGnews Yahoo2 Yelp\nBERT 8.26 8.70 26.95 31.18 18.52 19.46 7.05 33.24 32.97 57.45 18.86 27.68\nTS 14.60 13.72 31.73 33.89 22.32 24.61 12.91 43.55 37.84 59.86 22.17 34.03\nMCDP 13.14 14.21 31.05 34.74 21.41 22.62 9.85 36.96 36.97 60.06 19.99 29.45\nLS 12.45 14.24 30.92 33.51 22.94 27.52 11.63 49.60 36.04 65.28 22.38 33.00\nERL 17.92 20.04 30.83 35.26 25.07 27.34 15.43 55.69 36.69 61.93 24.07 36.74\nV AT 8.44 9.66 29.39 30.57 17.23 21.74 7.26 41.35 32.56 60.81 17.64 31.17\nMixup 13.33 11.87 31.71 35.24 22.62 22.80 11.50 43.60 37.09 65.51 22.19 33.66\nM-mixup 8.67 9.89 27.33 29.61 20.33 23.05 7.18 37.10 33.57 58.13 20.66 36.42\nOurs 18.35 20.18 36.63 40.01 25.94 29.15 16.55 68.72 43.40 72.62 25.03 41.11\nTable 7: NBAUCC0.7 on misclassiﬁcation detection and OOD detection. We report the average performance of 5\nrandom initializations.\nDataset 20NG15 20NG\nModel δon Accuracy ECE OOD Mis Accuracy ECE OOD Mis\nBERT - 87.42 9.24 13.52 17.86 84.55 11.61 42.86 18.48\nw/ Roﬀ - 86.48 6.51 18.10 24.53 83.90 7.98 63.73 25.40\nw/ Ron 10−2 88.73 2.77 22.83 27.40 85.60 5.00 51.53 27.40\nw/ Ron 10−3 88.29 3.52 21.03 24.13 85.69 4.43 53.87 26.30\nw/ Ron 10−4 87.93 4.48 17.43 21.63 85.12 6.76 57.47 21.93\nw/ Ron 10−5 87.61 4.69 15.73 21.43 85.39 6.35 52.07 21.63\nw/ Both 10−4 87.44 3.69 23.20 26.50 84.53 4.43 66.36 28.10\nTable 8: Ablation study on the 20NG15 and 20NG datasets. For OOD detection and misclassiﬁcation detection, we\nreport NBAUCC1. We set δy = 0.1 and δoﬀ = 10−3.\n1339\nDataset 20NG15 20NG\nModel δon Accuracy ECE OOD Mis Accuracy ECE OOD Mis\nBERT - 87.42 9.24 7.05 8.26 84.55 11.61 33.24 8.70\nw/ Roﬀ - 86.48 6.51 11.75 14.79 83.90 7.98 62.67 15.42\nw/ Ron 10−2 88.73 2.77 15.27 18.35 85.60 5.00 46.67 18.39\nw/ Ron 10−3 88.29 3.52 13.86 15.66 85.69 4.43 50.07 18.17\nw/ Ron 10−4 87.93 4.48 10.61 12.59 85.12 6.76 53.64 13.18\nw/ Ron 10−5 87.61 4.69 8.71 12.25 85.39 6.35 46.24 12.20\nw/ Both 10−4 87.44 3.69 16.55 18.35 84.53 4.43 68.72 20.18\nTable 9: Ablation study on the 20NG15 and 20NG datasets. For OOD detection and misclassiﬁcation detection, we\nreport NBAUCC0.7. We set δy = 0.1 and δoﬀ = 10−3.\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018\n/uni00000018/uni00000013/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000261\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\nFigure 5: Parameter study of δon, δoﬀ and δy. We use NBAUCC1 for OOD and misclassiﬁcation detection.\n1340\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\non\n/uni00000014/uni00000013\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000014\non\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000014/uni00000013\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n /uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000013\noff\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni0000001a/uni00000019\n/uni0000001a/uni0000001b\n/uni0000001b/uni00000013\n/uni0000001b/uni00000015\n/uni0000001b/uni00000017\n/uni0000001b/uni00000019\n/uni0000001b/uni0000001b\n/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000013\n/uni00000015\n/uni00000017\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017/uni00000028/uni00000026/uni00000028\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000014/uni00000013\n/uni00000014/uni00000018\n/uni00000015/uni00000013\n/uni00000015/uni00000018\n/uni00000016/uni00000013\n/uni00000016/uni00000018\n/uni00000017/uni00000013\n/uni00000017/uni00000018/uni00000030/uni0000004c/uni00000056/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018\ny\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013\n/uni0000001a/uni00000013/uni00000032/uni00000032/uni00000027/uni00000003/uni00000031/uni00000025/uni00000024/uni00000038/uni00000026/uni000000260.7\n/uni00000015/uni00000013/uni00000031/uni0000002a15\n/uni00000015/uni00000013/uni00000031/uni0000002a\n/uni0000003a/uni00000032/uni00000036100\n/uni0000003a/uni00000032/uni00000036\nFigure 6: Parameter study of δon, δoﬀ and δy. We use NBAUCC0.7 for OOD and misclassiﬁcation detection.",
  "topic": "Regularization (linguistics)",
  "concepts": [
    {
      "name": "Regularization (linguistics)",
      "score": 0.7947866916656494
    },
    {
      "name": "Manifold (fluid mechanics)",
      "score": 0.6638755798339844
    },
    {
      "name": "Computer science",
      "score": 0.6461711525917053
    },
    {
      "name": "Calibration",
      "score": 0.6314134001731873
    },
    {
      "name": "Interpolation (computer graphics)",
      "score": 0.580169677734375
    },
    {
      "name": "Smoothness",
      "score": 0.5045171976089478
    },
    {
      "name": "Distribution (mathematics)",
      "score": 0.43266281485557556
    },
    {
      "name": "Data point",
      "score": 0.4296833872795105
    },
    {
      "name": "Manifold alignment",
      "score": 0.42748332023620605
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4060702919960022
    },
    {
      "name": "Algorithm",
      "score": 0.39419299364089966
    },
    {
      "name": "Nonlinear dimensionality reduction",
      "score": 0.2567513585090637
    },
    {
      "name": "Mathematics",
      "score": 0.2157251536846161
    },
    {
      "name": "Statistics",
      "score": 0.19258072972297668
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Motion (physics)",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Dimensionality reduction",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}