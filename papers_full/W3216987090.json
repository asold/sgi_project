{
  "title": "TransMEF: A Transformer-Based Multi-Exposure Image Fusion Framework Using Self-Supervised Multi-Task Learning",
  "url": "https://openalex.org/W3216987090",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3216050001",
      "name": "Linhao Qu",
      "affiliations": [
        "Fudan University",
        "Fujian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2138878892",
      "name": "Shaolei Liu",
      "affiliations": [
        "Fudan University",
        "Fujian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2127622379",
      "name": "Manning Wang",
      "affiliations": [
        "Fudan University",
        "Fujian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2024153824",
      "name": "Zhijian Song",
      "affiliations": [
        "Fujian Medical University",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A3216050001",
      "name": "Linhao Qu",
      "affiliations": [
        "Shanghai Medical College of Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2138878892",
      "name": "Shaolei Liu",
      "affiliations": [
        "Shanghai Medical College of Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2127622379",
      "name": "Manning Wang",
      "affiliations": [
        "Shanghai Medical College of Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2024153824",
      "name": "Zhijian Song",
      "affiliations": [
        "Shanghai Medical College of Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6679633191",
    "https://openalex.org/W2783573276",
    "https://openalex.org/W3015967052",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W2552290192",
    "https://openalex.org/W2999838674",
    "https://openalex.org/W2752849939",
    "https://openalex.org/W6657381012",
    "https://openalex.org/W3105639468",
    "https://openalex.org/W6754374339",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W6623237181",
    "https://openalex.org/W2964915061",
    "https://openalex.org/W2776574544",
    "https://openalex.org/W2990293450",
    "https://openalex.org/W2590560192",
    "https://openalex.org/W6697317341",
    "https://openalex.org/W6675464714",
    "https://openalex.org/W2778174095",
    "https://openalex.org/W1505723240",
    "https://openalex.org/W2794263251",
    "https://openalex.org/W6810827500",
    "https://openalex.org/W2998529071",
    "https://openalex.org/W3035441328",
    "https://openalex.org/W6780367617",
    "https://openalex.org/W2024270358",
    "https://openalex.org/W2998012573",
    "https://openalex.org/W3045898449",
    "https://openalex.org/W2963787388",
    "https://openalex.org/W2132746638",
    "https://openalex.org/W3035503663",
    "https://openalex.org/W2293879958",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W4226178544",
    "https://openalex.org/W2296478018",
    "https://openalex.org/W3131111098",
    "https://openalex.org/W3037874813",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2890263162",
    "https://openalex.org/W3088623196",
    "https://openalex.org/W828875492",
    "https://openalex.org/W2963530785",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2136001449",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2131168375",
    "https://openalex.org/W2890107857"
  ],
  "abstract": "In this paper, we propose TransMEF, a transformer-based multi-exposure image fusion framework that uses self-supervised multi-task learning. The framework is based on an encoder-decoder network, which can be trained on large natural image datasets and does not require ground truth fusion images. We design three self-supervised reconstruction tasks according to the characteristics of multi-exposure images and conduct these tasks simultaneously using multi-task learning; through this process, the network can learn the characteristics of multi-exposure images and extract more generalized features. In addition, to compensate for the defect in establishing long-range dependencies in CNN-based architectures, we design an encoder that combines a CNN module with a transformer module. This combination enables the network to focus on both local and global information. We evaluated our method and compared it to 11 competitive traditional and deep learning-based methods on the latest released multi-exposure image fusion benchmark dataset, and our method achieved the best performance in both subjective and objective evaluations. Code will be available at https://github.com/miccaiif/TransMEF.",
  "full_text": "TransMEF: A Transformer-Based Multi-Exposure Image Fusion Framework\nUsing Self-Supervised Multi-Task Learning\nLinhao Qu*, Shaolei Liu*, Manning Wang‚Ä†, Zhijian Song‚Ä†\nDigital Medical Research Center, School of Basic Medical Science, Fudan University, Shanghai 200032, China\nShanghai Key Lab of Medical Image Computing and Computer Assisted Intervention\n{lhqu20, slliu, mnwang, zjsong}@fudan.edu.cn\nAbstract\nIn this paper, we propose TransMEF, a transformer-based\nmulti-exposure image fusion framework that uses self-\nsupervised multi-task learning. The framework is based on\nan encoder-decoder network, which can be trained on large\nnatural image datasets and does not require ground truth fu-\nsion images. We design three self-supervised reconstruction\ntasks according to the characteristics of multi-exposure im-\nages and conduct these tasks simultaneously using multi-task\nlearning; through this process, the network can learn the char-\nacteristics of multi-exposure images and extract more gener-\nalized features. In addition, to compensate for the defect in\nestablishing long-range dependencies in CNN-based archi-\ntectures, we design an encoder that combines a CNN mod-\nule with a transformer module. This combination enables\nthe network to focus on both local and global information.\nWe evaluated our method and compared it to 11 competi-\ntive traditional and deep learning-based methods on the lat-\nest released multi-exposure image fusion benchmark dataset,\nand our method achieved the best performance in both sub-\njective and objective evaluations. Code will be available at\nhttps://github.com/miccaiif/TransMEF.\n1 Introduction\nDue to the low dynamic range (LDR) of common imaging\nsensors, a single image often suffers from underexposure\nor overexposure and fails to depict the high dynamic range\n(HDR) of luminance levels in natural scenes. The multi-\nexposure image fusion (MEF) technique provides an eco-\nnomical and effective solution by fusing LDR images with\ndifferent exposures into a single HDR image and thus is\nwidely used in HDR imaging for mobile devices (Reinhard\net al. 2010; Hasinoff et al. 2016; Shen et al. 2011).\nThe study of MEF has a long history, and a series of tradi-\ntional methods have been proposed (Li, Manjunath, and Mi-\ntra 1995; Liu and Wang 2015; Lee, Park, and Cho 2018; Ma\nand Wang 2015; Ma et al. 2017b,a). However, their perfor-\nmances are limited because weak hand-crafted representa-\ntions have low generalizability and are not robust to varying\ninput conditions (Zhang 2021; Zhang et al. 2020b; Xu et al.\n2020a).\n*Equal Contribution.\n‚Ä†Corresponding Authors\nCopyright ¬© 2022, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nRecently, deep learning-based algorithms have gradually\nbecome mainstream in the MEF Ô¨Åeld. In these methods,\ntwo source images with different exposures are directly in-\nput into a fusion network, and the fused image is obtained\nfrom the output of the network. The fusion networks can\nbe trained in a common supervised way using ground truth\nfusion images (Zhang et al. 2020b; Wang et al. 2018; Li\nand Zhang 2018) or in an unsupervised way by encourag-\ning the fused image to retain different aspects of the im-\nportant information in the source images (Xu et al. 2020a;\nRam Prabhakar, Sai Srikar, and Venkatesh Babu 2017; Xu\net al. 2020b; Zhang et al. 2020a; Ma et al. 2019). How-\never, both supervised and unsupervised MEF methods re-\nquire a large amount of multi-exposure data for training. Al-\nthough many researchers (Ram Prabhakar, Sai Srikar, and\nVenkatesh Babu 2017; Cai, Gu, and Zhang 2018; Zeng et al.\n2014) have collected various multi-exposure datasets, their\nquantities are not comparable to large natural image datasets\nsuch as ImageNet (Deng et al. 2009) or MS-COCO (Lin\net al. 2014). The absence of large amounts of training data\ngenerally leads to overÔ¨Åtting or tedious parameter optimiza-\ntion. In addition, ground truth is also in high demand for\nsupervised MEF methods but is not commonly available in\nthe Ô¨Åeld (Zhang 2021). Some researchers synthesize ground\ntruth images (Wang et al. 2018) or use the fusion results from\nother methods as ground truth for training (Yin et al. 2020;\nChen and Chuang 2020; Xu, Ma, and Zhang 2020). How-\never, these ground truth images are not real, and using them\nleads to inferior performance.\nMoreover, all the existing deep learning-based MEF\nmethods utilize convolutional neural networks (CNNs) for\nfeature extraction, but it is difÔ¨Åcult for CNNs to model long-\nrange dependencies due to their small receptive Ô¨Åeld, which\nis an inherent limitation. In image fusion, the quality of the\nfused images is related to the pixels within the receptive Ô¨Åeld\nas well as to the pixel intensity and texture of the entire im-\nage. Therefore, modeling both global and local dependen-\ncies is required.\nTo address the above issues, we propose TransMEF, a\ntransformer-based multi-exposure image fusion framework\nthat uses self-supervised multi-task learning. The framework\nis based on an encoder-decoder network and is trained on a\nlarge natural image dataset using self-supervised image re-\nconstruction to avoid training with multi-exposure images.\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2126\nDuring the fusion phase, we Ô¨Årst apply the trained encoder\nto extract feature maps from two source images and then ap-\nply the trained decoder to generate a fused image from the\nfused feature maps. We also design three self-supervised re-\nconstruction tasks according to the characteristics of multi-\nexposure images to train the network so that our network\nlearns these characteristics more effectively.\nIn addition, we design an encoder that includes both a\nCNN module and a transformer module so that the en-\ncoder can utilize both local and global information. Exten-\nsive experiments demonstrate the effectiveness of the self-\nsupervised reconstruction tasks as well as the transformer-\nbased encoder and show that our method outperforms the\nstate-of-the-art MEF methods in both subjective and objec-\ntive evaluations.\nThe main contributions of this paper are summarized as\nfollows:\n‚Ä¢ We propose three self-supervised reconstruction tasks ac-\ncording to the characteristics of multi-exposure images\nand train an encoder-decoder network using multi-task\nlearning so that our network is not only able to be trained\non large natural image datasets but also learns the char-\nacteristics of multi-exposure images.\n‚Ä¢ To compensate for the defect in establishing long-range\ndependencies in CNN-based architectures, we design an\nencoder that combines a CNN module with a transformer\nmodule, which enables the network to utilize both local\nand global information during feature extraction.\n‚Ä¢ To provide a fair and comprehensive comparison with\nother fusion methods, we used the latest released multi-\nexposure image fusion benchmark dataset (Zhang 2021)\nas the test dataset. We selected 12 objective evaluation\nmetrics from four perspectives and compared our method\nto 11 competitive traditional and deep learning-based\nmethods in the MEF Ô¨Åeld. Our method achieves the best\nperformance in both subjective and objective evaluations.\n2 Related Work\n2.1 Traditional MEF Algorithms\nTraditional MEF methods can be further classiÔ¨Åed into spa-\ntial domain-based fusion methods (Liu and Wang 2015; Lee,\nPark, and Cho 2018; Ma and Wang 2015; Ma et al. 2017b,a)\nand transform domain-based fusion methods (Li, Manju-\nnath, and Mitra 1995; Burt and Kolczynski 1993; Mertens,\nKautz, and Van Reeth 2007; Kou et al. 2017). Spatial do-\nmain methods calculate the fused image‚Äôs pixel values di-\nrectly from the source image‚Äôs pixel values, and three types\nof techniques are commonly used to fuse images in the spa-\ntial domain, namely, pixel-based methods (Liu and Wang\n2015; Lee, Park, and Cho 2018), patch-based methods (Ma\nand Wang 2015; Ma et al. 2017b) and optimization-based\nmethods (Ma et al. 2017a).\nIn transform domain-based fusion algorithms, the source\nimages are Ô¨Årst transformed to a speciÔ¨Åc transform domain\n(such as the wavelet domain) to obtain different frequency\ncomponents, and then appropriate fusion rules are used to\nfuse different frequency components. Finally, the fused im-\nages are obtained by inversely transforming the fused fre-\nquency components. Commonly used transform methods\ninclude pyramid transform (Burt and Kolczynski 1993),\nLaplacian pyramid (Mertens, Kautz, and Van Reeth 2007),\nwavelet transform (Li, Manjunath, and Mitra 1995), and\nedge-preserving smoothing (Kou et al. 2017), among others.\nAlthough traditional methods achieve promising fusion\nresults, weak hand-crafted representations with low gener-\nalizability hinder further improvement.\n2.2 Deep-Learning Based MEF Algorithms\nIn deep learning-based algorithms, two source images with\ndifferent exposures are directly input into a fusion net-\nwork, and the network outputs the fused image. The fusion\nnetworks can be trained with ground truth fusion images\n(Zhang et al. 2020b; Wang et al. 2018; Li and Zhang 2018)\nor similarity metric-based loss functions (Xu et al. 2020a;\nRam Prabhakar, Sai Srikar, and Venkatesh Babu 2017; Xu\net al. 2020b; Zhang et al. 2020a; Ma et al. 2019).\nDue to the lack of real ground truth fusion images in the\nMEF Ô¨Åeld, several methods have been proposed to synthe-\nsize ground truth. For example, Wang et al. (Wang et al.\n2018) generated ground truth data by changing the pixel in-\ntensity of normal images (Deng et al. 2009), while other re-\nsearchers utilized the fusion results from other MEF meth-\nods as ground truth (Yin et al. 2020; Chen and Chuang 2020;\nXu, Ma, and Zhang 2020). However, these ground truth im-\nages are not real, leading to inferior fusion performance.\nIn addition to training with ground truth images, another\nresearch direction is to train the fusion network with sim-\nilarity metric-based loss functions that encourage the fu-\nsion image to retain important information from different\naspects of the source images. For example, Prabhakar et al.\n(Ram Prabhakar, Sai Srikar, and Venkatesh Babu 2017) ap-\nplied a no-reference image quality metric (MEF-SSIM) as a\nloss function. Zhang et al. (Zhang et al. 2020a) designed a\nloss function based on gradient and intensity information to\nperform unsupervised training. Xu et al. (Xu et al. 2020a,b)\nproposed U2Fusion, in which a fusion network is trained to\npreserve the adaptive similarity between the fusion result\nand the source images. Although these methods do not re-\nquire ground truth images, a large number of multi-exposure\nimages are still in high demand for training. Although sev-\neral multi-exposure datasets (Ram Prabhakar, Sai Srikar, and\nVenkatesh Babu 2017; Cai, Gu, and Zhang 2018; Zeng et al.\n2014) have been collected, their quantities are incompara-\nble to large natural image datasets such as ImageNet (Deng\net al. 2009) or MS-COCO (Lin et al. 2014). The absence of\nlarge amounts of training data leads to overÔ¨Åtting or tedious\nparameter optimization.\nNotably, researchers have already utilized encoder-\ndecoder networks in infrared and visible image fusion (Li\nand Wu 2018) as well as multi-focus image fusion tasks\n(Ma et al. 2021). They trained the encoder-decoder network\non the natural image dataset, but the network cannot effec-\ntively learn the characteristics of multi-exposure images due\nto the domain discrepancy. In contrast, we design three self-\nsupervised reconstruction tasks according to the character-\n2127\nistics of multi-exposure images so that our network can not\nonly be trained on large natural image datasets but will also\nbe able to learn the characteristics of multi-exposure images.\n3 Method\n3.1 Framework Overview\nAs shown in Figure 1, our framework is an encoder-decoder-\nbased architecture. We train the network via image recon-\nstruction on a large natural image dataset. During the fusion\nphase, we apply the trained encoder to extract feature maps\nfrom a pair of source images and then fuse the two feature\nmaps and input it into the decoder to generate the fused im-\nage.\nThe framework training process is shown in Figure 1 (a),\nwhere we use the network to perform self-supervised im-\nage reconstruction tasks, i.e., to reconstruct the original im-\nage from the destroyed input image. Concretely, given an\noriginal image Iin ‚ààRH\u0002W , three different destroyed im-\nages ÀúI\nN\nin(N = 1;2;3) are generated by destroying several\nsubregions using one of the three different transformations\n(Gamma-based transformation TG(¬∑), Fourier-based trans-\nformation TF (¬∑) and global region shufÔ¨Çing TS(¬∑)). The de-\nstroyed images are input into the Encoder, which consists of\na feature extraction module, TransBlock, and a feature en-\nhancement module, EnhanceBlock. TransBlock uses a CNN\nmodule and a transformer module for feature extraction.\nThe destroyed images ÀúIN\nin are directly input into the CNN-\nModule, and concurrently, they are divided into patches ÀúIN\np ,\nwhich are then input into the Transformer-Module. The En-\nhanceBlock aggregates and enhances the feature maps ex-\ntracted from the CNN-Module and the Transformer-Module.\nFinally, the image features extracted by the Encoder are used\nto obtain the reconstructed image IN\nout(N = 1;2;3) by the\nDecoder. We utilize a multi-task learning approach that si-\nmultaneously performs three self-supervised reconstruction\ntasks. The detailed structure of the Encoder and the three\nreconstruction tasks are introduced in Sections 3.2 and 3.3,\nrespectively.\nThe trained Encoder and Decoder are then used for image\nfusion, as shown in Figure 1 (b). SpeciÔ¨Åcally, two source\nimages Ik (k = 1;2) are Ô¨Årst input to the Encoder for fea-\nture encoding, and then the extracted feature maps F1 and\nF2 are fused using the Fusion Rule to obtain the fused fea-\nture maps F0. Finally, the fused image IF is reconstructed\nby the Decoder. The fusion rule is described in detail in Sec-\ntion 3.4. Here, we only introduce this framework‚Äôs pipeline\nfor single-channel grayscale image fusion, and the fusion of\ncolor images is elaborated in Section 3.5.\n3.2 Transformer-Based Encoder-Decoder\nFramework\nEncoder-Decoder Framework for Image Reconstruc-\ntion The encoder-decoder network is shown in Figure\n1 (a). Using a single self-supervised reconstruction task\nas an example, given a training image Iin ‚àà RH\u0002W ,\nwe Ô¨Årst randomly generate 10 image subregions xi ‚àà\nRHi\u0002Wi;(i= 1;2;::: 10) to form a set that will be trans-\nformed \u001f= {x1;x2;:::;x 10}, where the sizes of the subre-\ngions Hi, Wi are all random values uniformly sampled from\nthe positive integer set [1,25]. After that, we transform each\nsubregion xi in the set\u001fwith an image transform tailored for\nmulti-exposure image fusion (the three different transforma-\ntions are described in detail in Section 3.3) to obtain the set\nof transformed subregions e\u001f, which are then used to replace\nthe original subregions to obtain the transformed image ÀúIin.\nIn Figure 1 (a), TG(¬∑), TF (¬∑) and TS(¬∑) represent the transfor-\nmation based on Gamma transform, Fourier transform and\nglobal region shufÔ¨Çing, respectively.\nThe Encoder contains a feature extraction module, Trans-\nBlock, and a feature enhancement module, EnhanceBlock.\nThe detailed architecture of TransBlock is introduced in the\nfollowing section. The feature enhancement module, En-\nhanceBlock, aggregates and enhances the feature maps ex-\ntracted by TransBlock so that the Encoder can better inte-\ngrate the global and local features. Concretely, we concate-\nnate the two feature maps from the CNN-Module and the\nTransformer-Module in TransBlock and input them into two\nsequentially connected ConvBlock layers to achieve feature\nenhancement. As shown in Figure 1 (c), each ConvBlock\nconsists of two convolutional layers with a kernel size of\n3√ó3, a padding of 1 and one ReLU activation layer. The De-\ncoder contains two sequentially connected ConvBlock lay-\ners and a Ô¨Ånal 1√ó1 convolution to reconstruct the original\nimage.\nTransBlock: A Powerful Feature Extractor Inspired by\nTransUnet (Chen et al. 2021) and ViT (Dosovitskiy et al.\n2020), we propose a feature extraction module, Trans-\nBlock, that combines the CNN and transformer architec-\nture to model both local and global dependencies in im-\nages. The architecture of TransBlock is shown in Figure\n1 (a). SpeciÔ¨Åcally, the CNN-Module consists of three se-\nquentially connected ConvBlock layers, and the input to the\nCNN-Module is the destroyed images. Simultaneously, the\ndestroyed image ÀúIin ‚àà RH\u0002W is divided into a total of\nM patches of size H\nP √ó W\nP . The patches are used to con-\nstruct the sequence xseq ‚ààRM\u0002P2\n, where xseq =\n\b\nxk\np\n\t\n,\n(k= 1;2;:::M ), M = HW=P2 and P is the size of the\npatches. The sequence is fed into the Transformer-Module,\nwhich starts with a patch embeddings linear projection E,\nand the encoded sequence features z0 ‚àà RM\u0002D are ob-\ntained. Then, z0 passes through L Transformer layers and\nthe output of each layer is denoted as zl(l = 1:::L). Figure\n1 (c) illustrates the architecture of one Transformer layer,\nwhich consists of a multi-head attention mechanism (MSA)\nblock and a multi-layer perceptron (MLP) block, where\nlayer normalization (LN) is applied before every block and\nresidual connections are applied after every block. The MLP\nblock consists of two linear layers with a GELU activation\nfunction.\nLoss Function Our architecture applies a multi-task\nlearning approach to simultaneously perform three self-\nsupervised reconstruction tasks using the following loss\nfunction.\nLoss = LossTask G + LossTask F + LossTask S (1)\n2128\nùêºùëñùëõ\nÔºàaÔºâSelf-supervised and multi-task image reconstruction network.\nùõµùê∫ ¬∑\n‚Ä¶\nLinear Projection\nTransformer-Module\nTransBlock\nEncoder\nConvBlock\nCNN-Module\nEnhanceBlock\nÔºàbÔºâThe image fusion architecture. ÔºàcÔºâDetailed structures of Transformer and ConvBlock.\nConvBlock\nConvBlock\nConvBlock\nConvBlock\nùëáùêπ ¬∑\nùëáùëÜ ¬∑\nùëáùëéùë†ùëòùê∫\nùëáùëéùë†ùëòùêπ\nùëáùëéùë†ùëòùëÜ\n·àöùêºùëñùëõ\nùëÅ ·àöùêºùëùùëÅ ùêºùëúùë¢ùë°\nùëÅ\nùë¨\nùíõùüé\nùë≥ùíêùíîùíî= ùë≥ùíêùíîùíîùëªùíÇùíîùíåùëÆ +ùë≥ùíêùíîùíîùëªùíÇùíîùíåùë≠ +ùë≥ùíêùíîùíîùëªùíÇùíîùíåùë∫\nTransformer\nTransformer\n‚Ä¶\nL=12\nDecoder\n1 √óùüèCNN\nConvBlock\nConvBlock\nDecoder\nùë∞ùüè\nùë∞ùüê\nFusion\nRule\nùë∞ùë≠\nF1\nF2\nF‚Äô\nEncoder\nEncoder\n1 √óùüèCNN\nConvBlock\nConvBlock\nCNN CNN ReLU\nTransformer\nConvBlock\nFigure 1: TransMEF image fusion framework. (a) The proposed self-supervised image reconstruction network, which uses\nmulti-task learning. (b) The image fusion architecture. (c) Detailed structures of Transformer and ConvBlock.\nwhere Lossdenotes the overall loss function.LossTask G,\nLossTask F and LossTask S are the loss functions of the\nthree self-supervised reconstruction tasks.\nIn each reconstruction task, we encourage the network to\nnot only learn the pixel-level image reconstruction, but also\ncapture the structural and gradient information in the image.\nTherefore, the loss of each reconstruction task contains three\nparts and is deÔ¨Åned as follows:\nL= Lmse + \u00151Lssim + \u00152LTV (2)\nwhere Lmse is the mean square error (MSE) loss function,\nLssim is the structural similarity (SSIM) loss function, and\nLTV is the total variation loss function. \u00151 and \u00152 are two\nhyperparameters that are empirically set to 20.\nThe MSE loss is used to ensure pixel-level reconstruction\nand is deÔ¨Åned as follows:\nLmse = ||Iout ‚àíIin||2 (3)\nwhere Iout is the output, a fused image reconstructed by\nthe network, andIin represents the input, the original image.\nThe SSIM loss helps the model better learn structural in-\nformation from images and is deÔ¨Åned as:\nLssim = 1 ‚àíSSIM(Iout;Iin) (4)\nThe total variation loss LTV introduced in VIFNet (Hou\net al. 2020) is used to better preserve gradients in the source\nimages and further eliminate noise. It is deÔ¨Åned as follows:\nR(p;q) = Iout (p;q) ‚àíIin (p;q) (5)\nLTV =\nX\np;q\n(||R(p;q + 1) ‚àíR(p;q)||2\n+||R(p+ 1;q) ‚àíR(p;q)||2) (6)\nwhere R(p;q) denotes the difference between the origi-\nnal image and the reconstructed image, ||¬∑ ||2 denotes the l2\nnorm, and p, q represent the horizontal and vertical coordi-\nnates of the image‚Äôs pixels, respectively.\n3.3 Three SpeciÔ¨Åc Self-Supervised Image\nReconstruction Tasks\nIn this section, we introduce three transformations that de-\nstroy the original images and generate the input for the im-\nage reconstruction encoder-decoder network. An example\nthat shows the image and the corresponding subregions be-\nfore and after the transformations is presented in Supple-\nmentary Material Section 1.\n(1) Learning Scene Content and Luminance Infor-\nmation using Gamma-based Transformation. In general,\noverexposed images contain sufÔ¨Åcient content and struc-\ntural information in dark regions, while underexposed im-\nages contain sufÔ¨Åcient color and structural information in\nbright regions. In the fused image, it is desirable to main-\ntain uniform brightness while retaining rich information in\nall regions (Xu et al. 2020a; Ram Prabhakar, Sai Srikar,\nand Venkatesh Babu 2017). We adopt Gamma transform\n2129\nto change the luminance in several subregions of the orig-\ninal image and train the network to reconstruct that origi-\nnal image. In this process, our network learns the content\nand structural information from the images at different lu-\nminance levels.\nGamma transform is deÔ¨Åned as:\ne = Œì ( ) =  gamma (7)\nwhere e and  are the transformed and original pixel values,\nrespectively. For each pixel in the selected subregion xi, we\nuse a random Gamma transform Œì to change the luminance,\nwhere gamma is a random value uniformly sampled from\nthe interval [0, 3].\n(2) Learning Texture and Detail Information us-\ning Fourier-based Transformation. We introduce a self-\nsupervised task based on Fourier transform that enables the\nnetwork to learn texture and detail information from the fre-\nquency domain.\nIn the discrete Fourier transform (DFT) of an image,\nthe amplitude spectrum determines the image‚Äôs intensities,\nwhile the phase spectrum primarily determines the high-\nlevel semantics of the image and contains information about\nthe image‚Äôs content and the location of the objects. (See Sup-\nplementary Material Section 1.2 for further descriptions and\nexperiments).\nUnderexposed images are too dark due to insufÔ¨Åcient ex-\nposure time, and overexposed images are too bright due to\na long exposure time, both of which result in inappropriate\nimage intensity distribution. Therefore, it is critical to en-\ncourage the network to learn the proper intensity distribution\nfrom the images.\nDespite the poor intensity distribution in both underex-\nposed and overexposed images, the shape and content of\nthe objects in the image are still well-contained in the phase\nspectrum. Hence, it is beneÔ¨Åcial to build a network that can\ncapture that shape and content information under such cir-\ncumstances.\nTo this end, for the selected image subregions, we Ô¨Årst\nperform Fourier transform to obtain the amplitude and phase\nspectrum. Then, we destroy the subregions in the frequency\ndomain. SpeciÔ¨Åcally, Gaussian blurring (\u001b = 0:5) is used\nto change the amplitude spectrum, and random swapping is\nperformed np times on all phase values in the phase spec-\ntrum, where np is a random number in the positive integer\nset [1, 5].\n(3) Learning Structure and Semantic Information us-\ning Global Region ShufÔ¨Çing We introduce the global re-\ngion shufÔ¨Çing transform (Kang et al. 2017) to destroy the\noriginal images, thus enabling the network to learn the struc-\nture and semantic information through image reconstruc-\ntion. SpeciÔ¨Åcally, for each image subregion xi in the set of\nimage subregions \u001f selected in the original image Iin, we\nrandomly select another image subregion x0\ni with the same\nsize of xi. After that, they are swapped and the process is\nrepeated 10 times to obtain the destroyed image.\n3.4 Fusion Rule\nBecause our network already has a strong feature extraction\ncapability, we simply average the feature maps from the two\nsource images F1 and F2 to obtain the fused feature maps\nF0, which is then forwarded to the Decoder.\n3.5 Managing RGB Input\nWe adopt a strategy commonly applied in previous deep\nlearning-based studies to fuse RGB multi-exposure images\n(Zhang et al. 2020a). The color image‚Äôs RGB channels are\nÔ¨Årst converted to the YCbCr color space. Then, the Y (lumi-\nnance) channel is fused using our network, and the informa-\ntion in the Cb and Cr (chrominance) channels is fused using\nthe traditional weighted average method, deÔ¨Åned as:\nCf = C1 (|C1 ‚àí\u001c|) +C2(|C2 ‚àí\u001c|)\n|C1 ‚àí\u001c|+ |C2 ‚àí\u001c| (8)\nwhere C1 and C2 represent the Cb (or Cr) channel val-\nues from the multi-exposure images. Cf denotes their fused\nchannel result, where \u001c is set to 128. Finally, the fused Y\nchannel, Cb and Cr channels are converted back to the RGB\nspace.\n4 Experiments and Results\n4.1 Datasets\nWe used the large natural dataset MS-COCO (Lin et al.\n2014) to train the encoder-decoder network.MS-COCO con-\ntains more than 70,000 natural images of various scenes. For\nconvenience, all images were resized to 256√ó256 and con-\nverted into grayscale images. It is worth mentioning that al-\nthough many competitive MEF algorithms have been pro-\nposed, they are not evaluated on a uniÔ¨Åed MEF benchmark.\nWe used the latest released multi-exposure image fusion\nbenchmark dataset (Zhang 2021) as the test dataset. This\nbenchmark dataset consists of 100 pairs of multi-exposure\nimages with a variety of scenes and multiple objects.\n4.2 Implementation Details\nOur network was trained on an NVIDIA GTX 3090 GPU\nwith a batch size of 64 and 70 epoch. We used an ADAM\noptimizer and a cosine annealing learning rate adjustment\nstrategy with a learning rate of 1e-4 and a weight decay of\n0.0005. For a 256 √ó256 training image, we randomly gen-\nerated 10 subregions of random size to form the set \u001f to\nbe transformed. In TransBlock, we divided the transformed\ninput image into patches with the size of 16 √ó16 and con-\nstructed the sequence xseq.\n4.3 Evaluation Metrics\nWe rigorously evaluated our method using both subjective\nand objective evaluations (Zhang 2021). Subjective evalua-\ntion is the observer‚Äôs subjective assessment of the quality of\nthe fused images in terms of sharpness, detail, and contrast,\namong other factors. In the objective evaluation, to provide a\nfair and comprehensive comparison with other fusion meth-\nods, we selected 12 objective evaluation metrics from four\nperspectives. These include information theory-based met-\nrics, QMI, QTE, QNICE, PSNR, FMI; image feature-based\nmetrics, QA=BF, QP, STD, QG; image structural similarity-\nbased metrics, SSIM, CC; and human perception inspired\n2130\n(a1) over-exposure (b1) under-exposure (c1) DWT (d1) DSIFT-EF (e1) MEFAW (f1) MEFOpt (g1) PWA \n(i1) MEFNet (j1) Deepfuse (k1) IFCNN (l1) PMGI (m1) U2Fusion(h1) SPD-MEF (n1) TransMEF\n(a2) over-exposure (b2) under-exposure (c2) DWT (d2) DSIFT-EF (e2) MEFAW (f2) MEFOpt (g2) PWA \n(i2) MEFNet (j2) Deepfuse (k2) IFCNN (l2) PMGI (m2) U2Fusion(h2) SPD-MEF (n2) TransMEF\nFigure 2: Two examples of source image pairs and fusion results from different methods. (a1)-(b1) and (a2)-(b2) are the source\nimage pairs, and (c1)-(n1) and (c2)-(n2) are the fusion results from various methods.\nMethod QMI QTE QNICE PSNR FMI CC QA/BF QP STD QG SSIM VIFF\nTraditional\nDWT 1.0305 0.5406 0.8304 53.6758 0.4982 0.9162 0.6448 0.7386 48.1925 0.5570 0.9252 0.5802 \nDSIFT-EF 0.6010 0.5043 0.8178 53.2974 0.4042 0.6178 0.6991 0.7155 48.7593 0.5577 0.9385 0.7073 \nMEFAW 0.6037 0.5062 0.8176 53.4695 0.4070 0.7327 0.7167 0.7264 50.4788 0.5781 0.9608 0.7641 \nMEFOpt 0.6233 0.5048 0.8190 53.2470 0.4060 0.6092 0.6987 0.6656 49.1201 0.5722 0.9238 0.7209 \nPWA 0.3657 0.4545 0.8123 52.7227 0.1538 0.5757 0.1235 0.0157 52.3884 0.1194 0.3595 0.2780 \n(TIP'17)SPD-MEF 0.8055 0.5265 0.8236 53.6225 0.4117 0.8651 0.7093 0.7225 53.7534 0.5749 0.9593 0.7359 \nDL-based\n(ICCV'17) Deepfuse 0.9745 0.5446 0.8276 53.6374 0.4750 0.9167 0.5938 0.7311 50.1292 0.4816 0.9074 0.6128 \n(TIP'20) MEFNet 0.6886 0.5082 0.8232 52.9449 0.4544 0.4401 0.6410 0.5498 53.0310 0.5538 0.7684 0.6669 \n(IF'20) IFCNN 0.8918 0.5282 0.8249 53.6595 0.5274 0.9075 0.6948 0.7679 49.5114 0.5884 0.9464 0.6548 \n(AAAI'20) PMGI 0.8621 0.5208 0.8240 53.4893 0.4065 0.8682 0.4184 0.5152 56.2019 0.3528 0.8687 0.6135 \n(TPAMI'20) U2Fusion 0.8793 0.5391 0.8236 53.6380 0.4258 0.9103 0.6005 0.7295 45.5663 0.4958 0.9146 0.5960 \nTransMEF 1.2847 0.5555 0.8447 53.6447 0.5488 0.9188 0.7142 0.8223 57.0667 0.6652 0.9478 0.7847 \nTable 1: Objective evaluation results for the benchmark dataset with the maximum values depicted in bold.\nTransBlock 3 SSL Tasks QMI QTE QNICE PSNR FMI CC QA/BF QP STD QG SSIM VIFF\n1.0166 0.5392 0.8304 53.6511 0.3649 0.9149 0.6547 0.7453 54.0384 0.5439 0.9456 0.7030 \n‚úì 1.1133 0.5484 0.8349 53.6562 0.4071 0.9178 0.6795 0.7948 54.4647 0.5763 0.9446 0.7234 \n‚úì 1.1999 0.5507 0.8396 53.6464 0.5344 0.9185 0.6931 0.8154 56.7438 0.6419 0.9450 0.7728 \n‚úì ‚úì 1.2212 0.5525 0.8408 53.6446 0.5407 0.9186 0.6984 0.8196 57.0225 0.6520 0.9456 0.7774 \nTable 2: Results of the ablation study for TransBlock and three self-supervised reconstruction tasks using 20% of the training\ndata.\n2131\nTransBlock Gamma Fourier Shuffling QMI QTE QNICE PSNR FMI CC QA/BF QP STD QG SSIM VIFF\n‚úì 1.1133 0.5484 0.8349 53.6562 0.4071 0.9178 0.6795 0.7948 54.4647 0.5763 0.9446 0.7234 \n‚úì ‚úì 1.1633 0.5523 0.8375 53.6441 0.4665 0.9183 0.6842 0.8120 55.0718 0.5980 0.9417 0.7371 \n‚úì ‚úì 1.1533 0.5516 0.8369 53.6492 0.4612 0.9183 0.6807 0.8112 54.9505 0.6038 0.9409 0.7302 \n‚úì ‚úì 1.1756 0.5521 0.8381 53.6484 0.4593 0.9182 0.6865 0.8127 54.7772 0.5999 0.9427 0.7355 \n‚úì ‚úì ‚úì ‚úì 1.2212 0.5525 0.8408 53.6446 0.5407 0.9186 0.6984 0.8196 57.0225 0.6520 0.9456 0.7774 \nTable 3: Results of the ablation study for each self-supervised reconstruction task using 20% of the training data.\nmetrics, VIF. Details about the metrics can be found in Sup-\nplementary Material Section 3. All objective metrics are cal-\nculated as the average for the 100 fused images, and a larger\nvalue indicates better performance for all metrics.\nWe compared our method with 11 competitive traditional\nmethods (Li, Manjunath, and Mitra 1995; Liu and Wang\n2015; Lee, Park, and Cho 2018; Ma and Wang 2015; Ma\net al. 2017b,a) and deep learning methods (Zhang et al.\n2020b; Xu et al. 2020a; Ram Prabhakar, Sai Srikar, and\nVenkatesh Babu 2017; Xu et al. 2020b; Zhang et al. 2020a;\nMa et al. 2019) in the MEF Ô¨Åeld. The comparison meth-\nods are as follows: The traditional methods include: DWT\n(Li, Manjunath, and Mitra 1995), DSIFT-EF (Liu and Wang\n2015), MEFAW (Lee, Park, and Cho 2018), PW A (Ma and\nWang 2015), SPD-MEF (Ma et al. 2017b), and MEFOpt (Ma\net al. 2017a) and the deep learning-based methods include\nDeepfuse (Ram Prabhakar, Sai Srikar, and Venkatesh Babu\n2017), MEFNet (Ma et al. 2019), U2Fusion (Xu et al.\n2020a), PMGI (Zhang et al. 2020a), and IFCNN (Zhang\net al. 2020b).\n4.4 Subjective Evaluation\nFigure 2 shows the fusion results from our method and our\ncompetitors in an indoor and outdoor scene. More fusion\nresults are shown in Supplementary Material Section 4.\nWhen fusing the Ô¨Årst pair of source images in Figure 2\n(a1) and (b1), DSIFT-EF, MEFAW, MEFOpt, SPD-MEF and\nMEFNet result in disappointing luminance maintenance,\nand the fused images appear dark. PW A introduces arti-\nfacts, and the color is unrealistic. Although DWT, Deepfuse,\nPMGI, IFCNN and U2Fusion maintain moderate luminance,\ntheir fusion results suffer from low contrast and fail to depict\nthe image‚Äôs details. In comparison, our method maintains the\nbest luminance and contrast and simultaneously displays ex-\ncellent details with better visual perception.\nWhen fusing the second pair of source images in Figure\n2 (a2) and (b2), most methods fail to maintain appropriate\nluminance. MEFNet and PMGI maintain relatively better\nluminance but introduce artifacts and blurring. Clearly, our\nmethod maintains optimal luminance and contrast and si-\nmultaneously retains more detailed information.\n4.5 Objective Evaluation\nTable 1 presents the objective evaluation for all comparison\nmethods on the benchmark dataset. Our method achieves the\nbest performance for nine of the 12 metrics, while for the\nother three metrics, the gap between our method‚Äôs results\nand the best results is small.\n5 Ablation Study\n5.1 Ablation Study for TransBlock\nTo verify the effectiveness of TransBlock, we conducted an\nablation study using 20% of the training data, and the re-\nsults of the ablation study are shown in Table 2. Regardless\nof whether the proposed self-supervised reconstruction tasks\nare used, adding TransBlock always improves the fusion per-\nformance.\nTo further explain why TransBlock is effective, we visual-\nized the effect of image reconstruction using both the tradi-\ntional CNN architecture and the model that includes Trans-\nBlock. It can be seen that the latter reconstructed better de-\ntails. More information can be found in Supplementary Ma-\nterial Section 2.\n5.2 Ablation Study for Three SpeciÔ¨Åc\nSelf-Supervised Image Reconstruction Tasks\nIn this ablation study, we demonstrate the effectiveness of\neach of the self-supervised reconstruction tasks and the su-\nperiority of performing them simultaneously in a multi-task\nmanner. This study was performed using 20% of the train-\ning data, and the experimental results are shown in Table 3.\nThe results show that each of the self-supervised reconstruc-\ntion tasks alone can improve the fusion performance, and the\noverall best performance is achieved by conducting the three\ntasks simultaneously through multi-task learning.\n6 Conclusion\nIn this paper, we propose TransMEF, a transformer-based\nmulti-exposure image fusion framework via self-supervised\nmulti-task learning. TransMEF is based on an encoder-\ndecoder structure so that it can be trained on large natural\nimage datasets. The TransMEF encoder integrates a CNN\nmodule and a transformer module so that the network can\nfocus on both local and global information. In addition, we\ndesign three self-supervised reconstruction tasks according\nto the characteristics of multi-exposure images and conduct\nthese tasks simultaneously using multi-task learning so that\nthe network can learn those characteristics during the pro-\ncess of image reconstruction. Extensive experiments show\nthat our new method achieves state-of-the-art performance\nwhen compared with existing competitive methods in both\nsubjective and objective evaluations. The proposed Trans-\nBlock and the self-supervised reconstruction tasks have the\npotential to be applied in other image fusion tasks and other\nareas of image processing.\n2132\nAcknowledgments\nThis work was supported by National Natural Science Foun-\ndation of China under Grant 82072021.\nReferences\nBurt, P. J.; and Kolczynski, R. J. 1993. Enhanced image cap-\nture through fusion. In 1993 (4th) International Conference\non Computer Vision (ICCV), 173‚Äì182. IEEE.\nCai, J.; Gu, S.; and Zhang, L. 2018. Learning a deep single\nimage contrast enhancer from multi-exposure images. IEEE\nTransactions on Image Processing, 27(4): 2049‚Äì2062.\nChen, J.; Lu, Y .; Yu, Q.; Luo, X.; Adeli, E.; Wang, Y .; Lu,\nL.; Yuille, A. L.; and Zhou, Y . 2021. Transunet: Transform-\ners make strong encoders for medical image segmentation.\narXiv preprint arXiv:2102.04306.\nChen, S.-Y .; and Chuang, Y .-Y . 2020. Deep exposure fusion\nwith deghosting via homography estimation and attention\nlearning. In ICASSP 2020-2020 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP),\n1464‚Äì1468. IEEE.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 248‚Äì255. IEEE.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An Image is Worth\n16x16 Words: Transformers for Image Recognition at Scale.\nIn International Conference on Learning Representations\n(ICLR).\nHasinoff, S. W.; Sharlet, D.; Geiss, R.; Adams, A.; Barron,\nJ. T.; Kainz, F.; Chen, J.; and Levoy, M. 2016. Burst pho-\ntography for high dynamic range and low-light imaging on\nmobile cameras. ACM Transactions on Graphics, 35(6): 1‚Äì\n12.\nHou, R.; Zhou, D.; Nie, R.; Liu, D.; Xiong, L.; Guo, Y .;\nand Yu, C. 2020. VIF-Net: an unsupervised framework for\ninfrared and visible image fusion. IEEE Transactions on\nComputational Imaging, 6: 640‚Äì651.\nKang, G.; Dong, X.; Zheng, L.; and Yang, Y .\n2017. PatchshufÔ¨Çe regularization. arXiv preprint\narXiv:1707.07103.\nKou, F.; Li, Z.; Wen, C.; and Chen, W. 2017. Multi-scale\nexposure fusion via gradient domain guided image Ô¨Åltering.\nIn 2017 IEEE International Conference on Multimedia and\nExpo (ICME), 1105‚Äì1110. IEEE.\nLee, S.-h.; Park, J. S.; and Cho, N. I. 2018. A multi-exposure\nimage fusion based on the adaptive weights reÔ¨Çecting the\nrelative pixel intensity and global gradient. In 2018 25th\nIEEE International Conference on Image Processing (ICIP),\n1737‚Äì1741. IEEE.\nLi, H.; Manjunath, B.; and Mitra, S. K. 1995. Multisensor\nimage fusion using the wavelet transform. Graphical Mod-\nels and Image Processing, 57(3): 235‚Äì245.\nLi, H.; and Wu, X.-J. 2018. DenseFuse: A fusion approach\nto infrared and visible images. IEEE Transactions on Image\nProcessing, 28(5): 2614‚Äì2623.\nLi, H.; and Zhang, L. 2018. Multi-exposure fusion with\nCNN features. In 2018 25th IEEE International Conference\non Image Processing (ICIP), 1723‚Äì1727. IEEE.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ¬¥ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In European Conference\non Computer Vision (ECCV), 740‚Äì755. Springer.\nLiu, Y .; and Wang, Z. 2015. Dense SIFT for ghost-free\nmulti-exposure fusion. Journal of Visual Communication\nand Image Representation, 31: 208‚Äì224.\nMa, B.; Zhu, Y .; Yin, X.; Ban, X.; Huang, H.; and Mukeshi-\nmana, M. 2021. SESF-fuse: An unsupervised deep model\nfor multi-focus image fusion. Neural Computing and Appli-\ncations, 33(11): 5793‚Äì5804.\nMa, K.; Duanmu, Z.; Yeganeh, H.; and Wang, Z. 2017a.\nMulti-exposure image fusion by optimizing a structural sim-\nilarity index. IEEE Transactions on Computational Imaging,\n4(1): 60‚Äì72.\nMa, K.; Duanmu, Z.; Zhu, H.; Fang, Y .; and Wang, Z. 2019.\nDeep guided learning for fast multi-exposure image fusion.\nIEEE Transactions on Image Processing, 29: 2808‚Äì2819.\nMa, K.; Li, H.; Yong, H.; Wang, Z.; Meng, D.; and Zhang,\nL. 2017b. Robust multi-exposure image fusion: a structural\npatch decomposition approach. IEEE Transactions on Im-\nage Processing, 26(5): 2519‚Äì2532.\nMa, K.; and Wang, Z. 2015. Multi-exposure image fusion:\nA patch-wise approach. In 2015 IEEE International Confer-\nence on Image Processing (ICIP), 1717‚Äì1721. IEEE.\nMertens, T.; Kautz, J.; and Van Reeth, F. 2007. Exposure\nfusion. In 15th PaciÔ¨Åc Conference on Computer Graphics\nand Applications (PCCGA), 382‚Äì390. IEEE.\nRam Prabhakar, K.; Sai Srikar, V .; and Venkatesh Babu, R.\n2017. Deepfuse: A deep unsupervised approach for expo-\nsure fusion with extreme exposure image pairs. In IEEE In-\nternational Conference on Computer Vision (ICCV), 4714‚Äì\n4722.\nReinhard, E.; Heidrich, W.; Debevec, P.; Pattanaik, S.; Ward,\nG.; and Myszkowski, K. 2010. High dynamic range imag-\ning: acquisition, display, and image-based lighting. Morgan\nKaufmann.\nShen, R.; Cheng, I.; Shi, J.; and Basu, A. 2011. Generalized\nrandom walks for fusion of multi-exposure images. IEEE\nTransactions on Image Processing, 20(12): 3634‚Äì3646.\nWang, J.; Wang, W.; Xu, G.; and Liu, H. 2018. End-to-end\nexposure fusion using convolutional neural network. IEICE\nTransactions on Information and Systems, 101(2): 560‚Äì563.\nXu, H.; Ma, J.; Jiang, J.; Guo, X.; and Ling, H. 2020a.\nU2Fusion: A UniÔ¨Åed Unsupervised Image Fusion Network.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence.\nXu, H.; Ma, J.; Le, Z.; Jiang, J.; and Guo, X. 2020b. Fu-\nsiondn: A uniÔ¨Åed densely connected network for image fu-\nsion. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial\nIntelligence, volume 34, 12484‚Äì12491.\n2133\nXu, H.; Ma, J.; and Zhang, X.-P. 2020. MEF-GAN: Multi-\nexposure image fusion via generative adversarial networks.\nIEEE Transactions on Image Processing, 29: 7203‚Äì7216.\nYin, J.-L.; Chen, B.-H.; Peng, Y .-T.; and Tsai, C.-C. 2020.\nDeep prior guided network for high-quality image fusion.\nIn 2020 IEEE International Conference on Multimedia and\nExpo (ICME), 1‚Äì6. IEEE.\nZeng, K.; Ma, K.; Hassen, R.; and Wang, Z. 2014. Percep-\ntual evaluation of multi-exposure image fusion algorithms.\nIn 2014 Sixth International Workshop on Quality of Multi-\nmedia Experience (QoMEX), 7‚Äì12. IEEE.\nZhang, H.; Xu, H.; Xiao, Y .; Guo, X.; and Ma, J. 2020a.\nRethinking the image fusion: A fast uniÔ¨Åed image fusion\nnetwork based on proportional maintenance of gradient and\nintensity. In Proceedings of the AAAI Conference on ArtiÔ¨Å-\ncial Intelligence, volume 34, 12797‚Äì12804.\nZhang, X. 2021. Benchmarking and comparing multi-\nexposure image fusion algorithms. Information Fusion, 74:\n111‚Äì131.\nZhang, Y .; Liu, Y .; Sun, P.; Yan, H.; Zhao, X.; and Zhang, L.\n2020b. IFCNN: A general image fusion framework based\non convolutional neural network. Information Fusion, 54:\n99‚Äì118.\n2134",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8162553906440735
    },
    {
      "name": "Encoder",
      "score": 0.7090737819671631
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6865237951278687
    },
    {
      "name": "Transformer",
      "score": 0.612725019454956
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5678303241729736
    },
    {
      "name": "Deep learning",
      "score": 0.5534117221832275
    },
    {
      "name": "Image fusion",
      "score": 0.5459096431732178
    },
    {
      "name": "Task (project management)",
      "score": 0.4527972638607025
    },
    {
      "name": "Machine learning",
      "score": 0.4479826092720032
    },
    {
      "name": "Ground truth",
      "score": 0.44158411026000977
    },
    {
      "name": "Code (set theory)",
      "score": 0.4290083646774292
    },
    {
      "name": "Fusion",
      "score": 0.4247729182243347
    },
    {
      "name": "Image (mathematics)",
      "score": 0.39546358585357666
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3932655453681946
    },
    {
      "name": "Engineering",
      "score": 0.07498475909233093
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ]
}