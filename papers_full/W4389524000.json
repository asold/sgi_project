{
  "title": "Embed_Llama: Using LLM Embeddings for the Metrics Shared Task",
  "url": "https://openalex.org/W4389524000",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093457082",
      "name": "Sören Dreano",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114731785",
      "name": "Derek Molloy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978924457",
      "name": "Noel Murphy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2758950307",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W8870360",
    "https://openalex.org/W3097023354",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W2101105183"
  ],
  "abstract": "Embed_llama is an assessment metric for language translation that hinges upon the utilization of the recently introduced Llama 2 Large Language Model (LLM), specifically, focusing on its embedding layer, with the aim of transforming sentences into a vector space that establishes connections between geometric and semantic proximities",
  "full_text": "Proceedings of the Eighth Conference on Machine Translation (WMT), pages 738–745\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n738\nEmbed_Llama: using LLM embeddings for the Metrics Shared Task\nSören Dréano\nML-Labs\nDublin City University\nsoren.dreano2@mail.dcu.ie\nDerek Molloy\nSchool of Electronic Engineering\nDublin City University\nderek.molloy@dcu.ie\nNoel Murphy\nSchool of Electronic Engineering\nDublin City University\nnoel.murphy@dcu.ie\nAbstract\nEmbed_llama is an assessment metric for lan-\nguage translation that hinges upon the utiliza-\ntion of the recently introduced Llama 2 Large\nLanguage Model (LLM), specifically focusing\non its embedding layer, to transform sentences\ninto a vector space that establishes connections\nbetween geometric and semantic proximities.\nInvestigations utilizing previous WMT datasets\nhave revealed that within the Llama 2 archi-\ntecture, relying solely on the initial embedding\nlayer does not result in the highest degree of cor-\nrelation when assessing machine translations.\nThe incorporation of additional layers, however,\nholds the potential to augment the contextual\nunderstanding of sentences.\nAs a contribution to the WMT23 challenge, this\nstudy delves into the advantages derived from\nemploying a pre-trained LLM that has not un-\ndergone fine-tuning specifically for translation\nevaluation tasks, to provide a metric conducive\nto operation on readily accessible consumer-\ngrade hardware. This research digs into the\nobservation that deeper layers within the model\ndo not result in a linear increase in the spatial\nproximity between sentences within the vector\nspace.\n1 Introduction\nThe assessment of algorithm-generated translations\nentails the utilization of evaluation metrics that\nfurnish quantitative scores to objectively gauge\nthe precision of the model’s output. Various\nmethodologies are employed to juxtapose machine-\ngenerated translations with their human-generated\ncounterparts, contingent upon the specific evalua-\ntion metric employed. In recent years, the realm\nof machine translation (MT) has witnessed notable\nadvancements in terms of both translation accuracy\nand linguistic fluency.\nOver time, there has been a significant enhance-\nment in the correlation coefficient between human\nassessments and the automated evaluation of sen-\ntences generated by machines. Earlier metrics,\nsuch as BLEU (Papineni et al., 2002) or chrF++\n(Popovi´c, 2017), predominantly relied on the tex-\ntual overlap between reference translations and the\nmachine-generated counterparts. In contrast, con-\ntemporary approaches, exemplified by COMET\n(Rei et al., 2020), harness recent breakthroughs\nin Natural Language Processing(NLP) and trans-\nformer models, enabling them to consider not only\nindividual words, but also to leverage contextual\nsemantics for a more comprehensive evaluation.\nIn the domain of Machine Learning (ML) ap-\nplied to NLP, the embedding layer assumes a piv-\notal role within neural network architectures, par-\nticularly in tasks centered on textual data. Its cen-\ntral objective lies in the transformation of discrete\ntokens, encompassing entities like words or charac-\nters into continuous vector representations. These\nvector representations, which maintain continuity,\nare amenable to acquisition and manipulation by\nneural networks and are commonly referred to as\nword embeddings.\n2 Embed_Llama\nThe initial component in a Natural Language Pro-\ncessing (NLP) model is typically an embedding\nlayer, which serves the purpose of converting the\ndistinct identifiers of tokens within the input sen-\ntence into a vectorized representation. In this con-\ntext, it is essential to emphasize that sentences\nconveying similar semantic content should exhibit\nproximity in the vector space, irrespective of the\npresence of word-level overlap, in contrast to sen-\ntences chosen randomly.\nEmbed_Llama draws inspiration from Word2vec\n(Mikolov et al., 2013) using Llama 2 (Touvron\net al., 2023), a contemporary open-source pre-\ntrained model. Rather than needing to train an extra\nNLP model for the purpose of assessing translation\nquality, a viable alternative approach involves uti-\n739\nlizing a pre-trained, extensive neural network like\nLlama 2, which has been originally trained for next-\ntoken prediction. This approach allows for the in-\nvestigation of how closely related sentences evolve\nacross the model’s various layers, all without incur-\nring the supplementary expenses associated with\nfine-tuning or training anew.\n2.1 Word2vec\nWord2vec is a methodology that utilizes a neu-\nral network model to extract word associations\nfrom comprehensive textual corpora. Post train-\ning, this model possesses the capability to iden-\ntify synonymous terms and offer word suggestions\nfor unfinished sentences. As the terminology im-\nplies, Word2vec symbolizes individual words by\nemploying distinct numerical vectors, systemati-\ncally engineered to encapsulate both the semantic\nand syntactic characteristics inherent to the words.\nEmbed_Llama leverages vectorial space to es-\ntimate similarity or dissimilarity. This estimation\nis accomplished by computing the cosine distance\nbetween two sentences.\n2.2 Vocabulary size\nThe lexical repertoire of Llama 2 encompasses\n32,000 unique tokens, a figure lower than that of\nboth GPT-2 (Radford et al., 2019) and GPT-NeoX\n(Black et al., 2022), which both employ 50,000\nunique tokens. Regrettably, the specific vocabulary\nsizes pertaining to GPT-3 (Brown et al., 2020) and\nGPT-4 (OpenAI, 2023) remain undisclosed.\n2.3 Embeddings\nThe dimensionality of the embedding space repre-\nsents a hyperparameter subject to adjustment. Em-\nbeddings of higher dimensions have the capacity\nto capture more intricate relationships; however,\nit is noteworthy that such higher-dimensional em-\nbeddings may necessitate increased quantities of\ndata and computational resources for their effective\nutilization.\n2.4 Cosine distance\nThe cosine similarity metric quantifies the similar-\nity between two n-dimensional vectors by comput-\ning the cosine of the angle between them. This\nscoring measure finds common application in the\ndomain of text mining (Singhal, 2001). The general\nformula for two vectors A and B is:\ncos θ = A · B\n∥A∥∥B∥\nTo enable efficient computation on consumer-\ngrade hardware, the two sentences slated for com-\nparison are padded to match the maximum token\ncount of the longer sentence in the pair. Con-\nsequently, a sentence initially shaped as [length]\ntransforms into the shape [length×4,096] following\nits processing through the embedding layer.\n3 Hyperparameter\nAs the objective of this current study revolves\naround the utilization of a pre-trained network, our\nsphere of influence is limited to a sole hyperparam-\neter, namely, the number of blocks to retain before\ncomputing the cosine distance.\n3.1 Block architecture\nAs shown in Figure 1, each block, denoted as\nthe LlamaDecoderLayer, is structured with sev-\neral components, including an attention layer, two\nnormalization layers, and a multi-layer perceptron.\nThe multi-layer perceptron, in turn, consists of\nthree linear layers along with an associated acti-\nvation function, while the attention layer also in-\ncludes a rotary embedding layer.\nThe Llama 2 model, comprising 7 billion pa-\nrameters, encompasses an embedding layer, 32\nblocks, and a projection layer. To determine the op-\ntimal number of blocks, datasets extracted from the\nWMT challenge editions of 2020, 2021, and 2022\nwere employed. Due to the limited GPU memory\nallocation in the current project, it was only feasi-\nble to investigate the Llama 2 model up to a depth\nof 22 blocks, whereas the model has a total of 32\navailable blocks.\nFigure 1: Architecture of the Llama2 model as displayed\nin the Huggingface library\n3.2 Exploring the depth\nIt was initially hypothesized that increasing the\nnumber of blocks would improve the contextual\n740\nrepresentation of sentence meaning. Figure 2 re-\nveals that, in contrast to our initial hypotheses, the\naugmentation of block quantities does not signifi-\ncantly modify the correlation between the systems\nand the ground truth, whether by augmentation\nor reduction. Furthermore, it is noteworthy that\nthis correlation exhibits variability across different\ndatasets. Specifically, the Pearson correlation be-\ntween the number of layers and algorithm accuracy\nis 0.34 for the WMT20 dataset, but it decreases to\n-0.79 for the WMT22 dataset.\nFigure 2: System correlations depending on the Llama\nlayer\nAs shown in Figure 3, these observations hold\nwith respect to segment correlation as well. The\nquantity of blocks employed in the Embed_Llama\ndoes not consistently enhance the metric’s quality,\nwhether for individual segments or entire systems.\nFigure 3: Segment correlations depending on the Llama\nlayer\nThe highest levels of correlation between hu-\nman rankings and Embed_Llama rankings were\nachieved by utilizing a mere two blocks following\nthe embedding layer, resulting in optimal overall\nperformance across the WMT20, WMT21, and\nWMT22 datasets. This not only expedited the com-\nputational process, but also decreased the GPU\nmemory demands for metric computation.\n3.3 Inter-languages variations\nAs depicted in Figure 4, the associations between\nmetric quality and language pairs exhibit large\nvariations. For instance, when considering the\nHungarian-to-English language pair, the Pearson\ncoefficient registers at 0.83, whereas it falls to -0.85\nfor the Czech-to-Ukrainian pair.\nFigure 4: System correlations for each language pair in\nthe WMT2022 dataset depending on the Llama layer\n3.4 Intra-languages variations\nConsiderable variability is observed even within\nthe same language pair across various datasets. For\ninstance, the English-to-Chinese language pair is\nencompassed within the WMT2020, WMT2021,\nand WMT2022 datasets. However, as illustrated\nin Figure 5, no discernible correlations emerge be-\ntween the number of utilized blocks and the quality\nof Embed_Llama scores. This is evident in the\ntransformation of the Pearson coefficient, which\nshifts from -0.28 in the WMT2021 to 0.73 in the\nWMT2022 dataset.\n3.5 Inter-datasets variations\nTable 1 presents the average mean values and their\ncorresponding standard deviations, showcasing the\nrelationship between metric accuracy and the num-\nber of utilized blocks for language pairs common\nto all three datasets. It is notable that, apart for\nthe English-to-Japanese pair, there exists a signifi-\ncant degree of variability in the performance of the\n741\nFigure 5: System correlations of the English-to-Chinese\nlanguage pair depending on the Llama layer and the\ndataset\nLanguage pair Mean Standard deviation\nen-cs -0.34 0.22\nen-de -0.02 0.4\nen-ja -0.89 0.04\nen-ru -0.02 0.54\nen-zh 0.35 0.45\nzh-en -0.17 0.46\nTable 1: Means and standard deviations of the sys-\ntem correlations depending on the Llama layer when\nWMT2020, WMT2021 and WMT2022 are merged\nsame language pairs across different datasets. This\nvariability is underscored by the substantial stan-\ndard deviations in relation to the absolute values\nof the means. Given the limited usage of just three\ndatasets, it is essential to acknowledge that the rel-\natively small sample size may hinder the ability to\ndraw conclusive inferences regarding inter-dataset\nvariability.\n3.6 Full results\nTables 2, 4 and 6, correspondingly, present the Pear-\nson correlation coefficients for datasets WMT2020,\nWMT2021 and WMT2021, illustrating the associ-\nation between the algorithm-assigned scores and\nthe actual rankings of the evaluated systems for\nindividual language pairs.\nWith regard to segment-level correlations, they\nare presented in Tables 3, 5 and 7 for WMT2020,\nWMT2021, and WMT2022, respectively. It is\nnoteworthy that, in contrast to system correla-\ntions, these are represented by Kendall coefficients,\nwhich are utilized as a measure of ordinal associa-\ntion.\nIt is noteworthy that the observed variations in\nthese correlations are predominantly influenced by\nthe specific language pairs, rather than the depth\nof the final block employed prior to cosine simi-\nlarity computation, aligning with our anticipated\noutcome.\n3.7 Source code\nThe source code of Embed_Llama is available at\nhttps://github.com/SorenDreano/embed_llama.\n4 Conclusion\nAlthough the authors initially anticipated that Em-\nbed_Llama would exhibit suboptimal performance\nfor a majority of language pairs, except for those\ninvolving English, due to the apparent constraints\nposed by a limited vocabulary size and the nature\nof the dataset Llama 2 was trained on, the actual\nperformance did not exhibit a significant underper-\nformance.\nThe results from previous iterations of the WMT\nmetrics shared task, presented in Appendix A, in-\ndicate that this approach may not meet the con-\ntemporary state-of-the-art standards exemplified by\nMETRICX_XXL (unpublished) and COMET-22\n(Rei et al., 2022).\nThe methodology involving the utilization of a\nnon-finetuned, pre-trained Large Language Model\n(LLM) to assess translation quality through vector\nspace similarity comparisons remains a prospective\navenue of inquiry. This prospect gains relevance\nin light of forthcoming open-source models charac-\nterized by expansive vocabularies and training data\nencompassing diverse languages.\n5 Further work\nGiven the recent proliferation of open-source\nLLMs, it is likely that another model, either\npresently or in the near future, may surpass the\nperformance of Llama 2 for translation evaluation\nwithout necessitating any fine-tuning.\nIn the current investigation, the exploration has\nbeen confined to the 7 billion parameters model. It\nremains conceivable that employing a more exten-\nsive model with increased parameters may yield\na more precise metric, albeit at the trade-off of\nheightened computational resource demands.\nIn the present evaluation, an exploration was lim-\nited to the initial 22 blocks. Subsequent endeavors\n742\nmay consider augmenting this number, as doing so\ncould potentially result in further benefits.\nMoreover, it is worth noting that the optimal\nselection of the number of blocks to employ may\nbe contingent upon the specific target language.\nConsequently, adjusting this hyperparameter based\non the language in question could potentially yield\nenhanced correlation results.\nIn the scope of the current academic study, solely\nthe cosine distance served as the chosen similarity\nmeasure for tensors. Future research endeavors\nmay wish to investigate alternative distance metrics,\nsuch as the Euclidean distance or the Manhattan\ndistance, for potential exploration.\n6 Acknowledgements\nThis publication has emanated from research con-\nducted with the financial support of Science Foun-\ndation Ireland under Grant number 18/CRT/6183.\nFor the purpose of Open Access, the author has\napplied a CC BY public copyright licence to any\nAuthor Accepted Manuscript version arising from\nthis submission.\nReferences\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit,\nLaria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. Gpt-neox-20b: An open-\nsource autoregressive language model.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efficient estimation of word representa-\ntions in vector space.\nOpenAI. 2023. Gpt-4 technical report.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMaja Popovi´c. 2017. chrF++: words helping charac-\nter n-grams. In Proceedings of the Second Confer-\nence on Machine Translation, pages 612–618, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nRicardo Rei, José G. C. de Souza, Duarte Alves,\nChrysoula Zerva, Ana C Farinha, Taisiya Glushkova,\nAlon Lavie, Luisa Coheur, and André F. T. Martins.\n2022. COMET-22: Unbabel-IST 2022 submission\nfor the metrics shared task. In Proceedings of the\nSeventh Conference on Machine Translation (WMT),\npages 578–585, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. Unbabel’s participation in the WMT20\nmetrics shared task. In Proceedings of the Fifth Con-\nference on Machine Translation, pages 911–920, On-\nline. Association for Computational Linguistics.\nAmit Singhal. 2001. Modern information retrieval: A\nbrief overview. IEEE Data Eng. Bull., 24:35–43.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\n743\nA Appendix. Tables\nTable 2: Pearson correlations (r) for the WMT20 system dataset depending on the Llama layer.\nThe findings pertaining to the second layer are shown in bold, as it represents the prevailing\ndefault layer count within the Embed_Llama\nLanguage pair L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11\nen-cs 0.855 0.866 0.875 0.877 0.873 0.876 0.872 0.863 0.861 0.864 0.867\nen-de 0.923 0.928 0.933 0.929 0.928 0.927 0.927 0.926 0.925 0.925 0.928\nen-ru 0.856 0.677 0.769 0.772 0.81 0.829 0.821 0.902 0.912 0.899 0.918\nen-ta 0.883 0.901 0.914 0.921 0.926 0.932 0.934 0.936 0.939 0.942 0.945\nen-zh 0.137 0.019 0.07 0.028 0.038 0.065 0.083 0.135 0.143 0.191 0.193\nen-ja 0.898 0.892 0.879 0.871 0.88 0.881 0.874 0.883 0.88 0.876 0.871\nen-pl 0.88 0.874 0.88 0.878 0.875 0.864 0.868 0.873 0.873 0.872 0.868\nen-iu 0.252 0.226 0.194 0.156 0.137 0.121 0.119 0.106 0.105 0.096 0.094\ncs-en 0.795 0.777 0.752 0.771 0.789 0.788 0.796 0.795 0.796 0.789 0.775\nde-en 0.992 0.996 0.996 0.994 0.99 0.99 0.99 0.988 0.988 0.988 0.99\npl-en 0.419 0.403 0.428 0.433 0.401 0.395 0.4 0.394 0.388 0.398 0.407\nta-en 0.876 0.889 0.918 0.922 0.919 0.923 0.921 0.919 0.917 0.919 0.918\nkm-en 0.954 0.969 0.988 0.988 0.988 0.989 0.99 0.989 0.987 0.986 0.985\nps-en 0.926 0.874 0.877 0.862 0.872 0.877 0.873 0.875 0.875 0.875 0.878\nja-en 0.913 0.938 0.947 0.946 0.946 0.948 0.949 0.948 0.946 0.948 0.949\nru-en 0.949 0.939 0.949 0.948 0.953 0.947 0.948 0.949 0.948 0.944 0.939\nzh-en 0.97 0.967 0.963 0.956 0.957 0.955 0.955 0.954 0.954 0.951 0.951\niu-en 0.64 0.676 0.68 0.662 0.638 0.645 0.638 0.625 0.616 0.615 0.633\nLanguage pair L12 L13 L14 L15 L16 L17 L18 L19 L20 L21 L22\nen-cs 0.865 0.866 0.86 0.858 0.851 0.856 0.855 0.857 0.866 0.874 0.874\nen-de 0.925 0.926 0.922 0.921 0.917 0.918 0.918 0.921 0.923 0.929 0.929\nen-ru 0.922 0.904 0.903 0.898 0.902 0.885 0.862 0.902 0.93 0.941 0.938\nen-ta 0.942 0.942 0.944 0.946 0.947 0.947 0.948 0.948 0.948 0.946 0.946\nen-zh 0.202 0.181 0.195 0.2 0.193 0.173 0.162 0.163 0.149 0.122 0.125\nen-ja 0.87 0.867 0.869 0.865 0.865 0.857 0.858 0.848 0.838 0.836 0.836\nen-pl 0.873 0.871 0.875 0.872 0.869 0.86 0.865 0.862 0.858 0.857 0.857\nen-iu 0.084 0.085 0.082 0.084 0.083 0.078 0.073 0.082 0.091 0.092 0.086\ncs-en 0.781 0.787 0.787 0.794 0.802 0.794 0.8 0.79 0.783 0.776 0.785\nde-en 0.989 0.989 0.988 0.988 0.986 0.989 0.989 0.991 0.992 0.994 0.994\npl-en 0.408 0.42 0.421 0.418 0.417 0.414 0.413 0.41 0.406 0.415 0.415\nta-en 0.918 0.917 0.914 0.91 0.907 0.912 0.912 0.913 0.913 0.919 0.921\nkm-en 0.983 0.982 0.975 0.975 0.967 0.964 0.967 0.968 0.97 0.966 0.968\nps-en 0.876 0.881 0.887 0.892 0.891 0.888 0.889 0.889 0.889 0.895 0.897\nja-en 0.947 0.946 0.944 0.941 0.936 0.94 0.942 0.943 0.942 0.941 0.941\nru-en 0.938 0.938 0.938 0.939 0.94 0.935 0.935 0.935 0.933 0.935 0.937\nzh-en 0.95 0.951 0.951 0.95 0.952 0.953 0.951 0.954 0.955 0.958 0.958\niu-en 0.619 0.613 0.606 0.6 0.602 0.627 0.63 0.639 0.644 0.662 0.66\nTable 3: Kendall correlations (τ) for the WMT20 segment dataset depending on the Llama layer\nLanguage pair L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11\nen-cs 0.226 0.231 0.244 0.234 0.232 0.226 0.229 0.225 0.225 0.222 0.226\nen-de 0.181 0.19 0.207 0.201 0.205 0.198 0.202 0.201 0.199 0.196 0.198\nen-ru 0.028 0.034 0.037 0.045 0.038 0.045 0.042 0.044 0.043 0.046 0.05\nen-ta 0.355 0.354 0.323 0.284 0.282 0.267 0.279 0.281 0.282 0.26 0.255\nen-zh 0.147 0.141 0.164 0.144 0.138 0.141 0.147 0.152 0.152 0.16 0.165\nen-ja 0.277 0.281 0.295 0.29 0.285 0.287 0.283 0.285 0.284 0.284 0.282\nen-pl 0.097 0.095 0.103 0.101 0.101 0.102 0.098 0.101 0.098 0.094 0.102\nen-iu 0.218 0.224 0.205 0.187 0.18 0.173 0.185 0.187 0.186 0.179 0.18\ncs-en 0.064 0.065 0.073 0.072 0.077 0.076 0.076 0.073 0.074 0.078 0.083\nde-en 0.349 0.374 0.387 0.384 0.39 0.389 0.385 0.376 0.372 0.375 0.381\npl-en -0.016 -0.016 -0.0 -0.007 -0.0 0.0 -0.003 0.002 -0.002 -0.005 -0.006\nta-en 0.108 0.118 0.125 0.121 0.122 0.119 0.128 0.121 0.106 0.108 0.123\nkm-en 0.141 0.145 0.144 0.12 0.13 0.122 0.117 0.114 0.097 0.108 0.114\nps-en 0.088 0.081 0.074 0.076 0.058 0.053 0.06 0.061 0.065 0.061 0.08\nja-en 0.109 0.136 0.136 0.144 0.147 0.149 0.151 0.144 0.137 0.139 0.134\nru-en 0.011 0.02 0.019 0.022 0.026 0.022 0.031 0.023 0.02 0.02 0.017\nzh-en 0.065 0.067 0.071 0.072 0.073 0.072 0.072 0.071 0.068 0.069 0.069\niu-en 0.16 0.161 0.175 0.178 0.187 0.191 0.195 0.194 0.195 0.198 0.199\nLanguage pair L12 L13 L14 L15 L16 L17 L18 L19 L20 L21 L22\nen-cs 0.23 0.229 0.222 0.223 0.215 0.218 0.221 0.222 0.221 0.222 0.221\nen-de 0.19 0.193 0.187 0.183 0.177 0.178 0.184 0.184 0.186 0.186 0.19\nen-ru 0.043 0.037 0.043 0.042 0.045 0.036 0.039 0.037 0.039 0.043 0.044\nen-ta 0.281 0.29 0.283 0.266 0.261 0.267 0.264 0.232 0.204 0.224 0.237\nen-zh 0.166 0.168 0.162 0.165 0.16 0.163 0.166 0.163 0.165 0.166 0.164\nen-ja 0.282 0.276 0.28 0.281 0.274 0.272 0.271 0.271 0.263 0.266 0.27\nen-pl 0.098 0.096 0.091 0.087 0.093 0.093 0.095 0.093 0.09 0.092 0.092\nen-iu 0.17 0.167 0.162 0.163 0.154 0.153 0.149 0.146 0.141 0.145 0.143\ncs-en 0.081 0.073 0.073 0.076 0.074 0.079 0.081 0.081 0.077 0.073 0.075\nde-en 0.377 0.377 0.371 0.372 0.361 0.37 0.37 0.378 0.374 0.383 0.386\npl-en -0.004 0.0 -0.001 0.005 0.001 -0.001 0.001 -0.004 -0.006 -0.004 -0.003\nta-en 0.118 0.111 0.107 0.101 0.103 0.11 0.112 0.114 0.112 0.106 0.103\nkm-en 0.109 0.11 0.092 0.097 0.083 0.091 0.1 0.111 0.116 0.124 0.126\nps-en 0.074 0.081 0.067 0.068 0.064 0.056 0.066 0.07 0.07 0.067 0.07\nja-en 0.128 0.125 0.116 0.112 0.102 0.115 0.117 0.125 0.126 0.129 0.129\nru-en 0.015 0.007 0.009 0.008 0.013 0.012 0.015 0.018 0.02 0.025 0.022\nzh-en 0.069 0.072 0.07 0.069 0.066 0.069 0.07 0.072 0.07 0.074 0.074\niu-en 0.192 0.177 0.173 0.176 0.173 0.179 0.184 0.183 0.188 0.189 0.189\n744\nTable 4: Pearson correlations (r) for the WMT21 system dataset depending on the Llama layer\nLanguage pair L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11\nen-cs 0.985 0.984 0.985 0.982 0.981 0.981 0.982 0.982 0.979 0.98 0.982\nen-zh 0.497 0.531 0.555 0.559 0.58 0.597 0.591 0.583 0.555 0.553 0.551\nen-ha 0.534 0.551 0.567 0.567 0.567 0.553 0.554 0.553 0.563 0.56 0.547\nen-ja 0.82 0.83 0.838 0.836 0.83 0.84 0.837 0.839 0.829 0.822 0.816\nen-ru 0.567 0.621 0.682 0.674 0.674 0.658 0.674 0.614 0.572 0.576 0.573\nen-de 0.818 0.793 0.804 0.801 0.794 0.789 0.794 0.799 0.796 0.792 0.79\ncs-en 0.542 0.454 0.435 0.428 0.429 0.432 0.426 0.426 0.429 0.42 0.423\nzh-en 0.232 0.186 0.155 0.159 0.172 0.179 0.188 0.196 0.198 0.196 0.192\nha-en 0.825 0.855 0.858 0.855 0.867 0.868 0.867 0.867 0.865 0.86 0.855\nja-en 0.728 0.726 0.748 0.753 0.761 0.76 0.759 0.759 0.761 0.761 0.759\nru-en 0.613 0.606 0.535 0.514 0.5 0.496 0.502 0.506 0.506 0.504 0.509\nde-en 0.171 0.22 0.237 0.238 0.221 0.23 0.223 0.223 0.225 0.234 0.243\nfr-de 0.564 0.556 0.555 0.556 0.556 0.555 0.557 0.555 0.556 0.554 0.552\nde-fr 0.477 0.511 0.578 0.579 0.579 0.58 0.579 0.575 0.563 0.564 0.574\nbn-hi 0.908 0.943 0.94 0.942 0.937 0.929 0.941 0.95 0.943 0.941 0.942\nhi-bn 0.879 0.872 0.913 0.93 0.925 0.915 0.912 0.907 0.911 0.908 0.915\nxh-zu 0.952 0.932 0.934 0.931 0.923 0.909 0.904 0.905 0.898 0.899 0.891\nzu-xh 0.95 0.937 0.97 0.973 0.975 0.972 0.971 0.972 0.976 0.976 0.976\nLanguage pair L12 L13 L14 L15 L16 L17 L18 L19 L20 L21 L22\nen-cs 0.983 0.984 0.983 0.983 0.979 0.981 0.982 0.983 0.983 0.984 0.983\nen-zh 0.548 0.545 0.548 0.545 0.545 0.549 0.554 0.543 0.533 0.524 0.528\nen-ha 0.541 0.543 0.554 0.553 0.571 0.562 0.572 0.567 0.556 0.542 0.534\nen-ja 0.81 0.804 0.795 0.793 0.781 0.783 0.786 0.777 0.772 0.774 0.779\nen-ru 0.581 0.582 0.562 0.58 0.526 0.557 0.565 0.603 0.621 0.625 0.637\nen-de 0.796 0.803 0.808 0.808 0.814 0.81 0.81 0.8 0.79 0.787 0.791\ncs-en 0.394 0.386 0.383 0.387 0.388 0.395 0.41 0.425 0.422 0.433 0.435\nzh-en 0.189 0.189 0.185 0.189 0.19 0.173 0.174 0.168 0.157 0.142 0.146\nha-en 0.846 0.842 0.84 0.841 0.841 0.827 0.828 0.829 0.823 0.818 0.814\nja-en 0.76 0.763 0.768 0.771 0.775 0.772 0.772 0.77 0.769 0.762 0.763\nru-en 0.51 0.512 0.513 0.52 0.521 0.52 0.529 0.534 0.535 0.532 0.528\nde-en 0.23 0.217 0.212 0.196 0.184 0.207 0.21 0.205 0.203 0.215 0.214\nfr-de 0.553 0.554 0.555 0.555 0.553 0.55 0.55 0.547 0.543 0.541 0.542\nde-fr 0.566 0.564 0.558 0.558 0.556 0.567 0.564 0.561 0.563 0.574 0.577\nbn-hi 0.935 0.932 0.931 0.931 0.93 0.933 0.933 0.938 0.94 0.93 0.922\nhi-bn 0.904 0.892 0.876 0.86 0.869 0.863 0.859 0.865 0.864 0.852 0.829\nxh-zu 0.9 0.903 0.906 0.903 0.909 0.905 0.908 0.912 0.903 0.898 0.896\nzu-xh 0.977 0.979 0.981 0.981 0.981 0.981 0.977 0.977 0.976 0.969 0.962\nTable 5: Kendall correlations (τ) for the WMT21 segment dataset depending on the Llama layer\nLanguage pair L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11\nen-cs 0.197 0.198 0.243 0.243 0.238 0.223 0.221 0.219 0.219 0.214 0.22\nen-zh 0.028 0.028 0.048 0.042 0.042 0.044 0.039 0.041 0.039 0.037 0.037\nen-ha 0.073 0.077 0.087 0.081 0.077 0.071 0.071 0.068 0.062 0.061 0.062\nen-ja 0.181 0.184 0.204 0.21 0.214 0.204 0.197 0.196 0.204 0.196 0.192\nen-ru 0.094 0.101 0.099 0.099 0.089 0.09 0.091 0.086 0.081 0.087 0.084\nen-de 0.241 0.31 0.172 0.172 0.241 0.31 0.31 0.241 0.241 0.241 0.241\ncs-en -0.042 -0.048 -0.049 -0.045 -0.041 -0.044 -0.042 -0.05 -0.046 -0.045 -0.051\nzh-en 0.319 0.286 0.319 0.384 0.395 0.319 0.308 0.319 0.384 0.297 0.276\nha-en -0.022 -0.016 -0.007 -0.003 -0.003 -0.009 -0.008 -0.007 -0.011 -0.014 -0.011\nja-en -0.028 -0.021 -0.019 -0.018 -0.015 -0.017 -0.014 -0.015 -0.014 -0.012 -0.015\nru-en -0.121 -0.124 -0.12 -0.12 -0.117 -0.123 -0.12 -0.119 -0.121 -0.124 -0.122\nde-en -0.155 -0.158 -0.151 -0.148 -0.153 -0.157 -0.155 -0.156 -0.158 -0.155 -0.152\nfr-de 0.057 0.058 0.055 0.052 0.044 0.055 0.062 0.057 0.053 0.047 0.051\nde-fr 0.054 0.066 0.082 0.079 0.075 0.051 0.057 0.054 0.049 0.055 0.055\nbn-hi -0.006 0.007 0.016 0.026 0.026 0.024 0.024 0.022 0.015 0.023 0.014\nhi-bn 0.132 0.119 0.126 0.135 0.139 0.137 0.145 0.15 0.147 0.146 0.14\nxh-zu 0.128 0.129 0.139 0.13 0.118 0.104 0.101 0.101 0.09 0.082 0.065\nzu-xh 0.169 0.139 0.181 0.181 0.16 0.129 0.122 0.116 0.12 0.118 0.109\nLanguage pair L12 L13 L14 L15 L16 L17 L18 L19 L20 L21 L22\nen-cs 0.213 0.218 0.213 0.22 0.198 0.209 0.209 0.196 0.193 0.194 0.197\nen-zh 0.042 0.045 0.048 0.048 0.043 0.043 0.043 0.046 0.041 0.04 0.04\nen-ha 0.058 0.06 0.061 0.061 0.061 0.06 0.062 0.064 0.068 0.067 0.067\nen-ja 0.189 0.187 0.196 0.193 0.202 0.196 0.195 0.19 0.184 0.179 0.179\nen-ru 0.088 0.089 0.109 0.107 0.116 0.116 0.109 0.106 0.108 0.104 0.105\nen-de 0.172 0.172 0.172 0.172 0.034 0.034 0.034 0.034 0.034 0.034 0.034\ncs-en -0.047 -0.048 -0.045 -0.055 -0.054 -0.06 -0.056 -0.055 -0.053 -0.058 -0.058\nzh-en 0.297 0.319 0.265 0.265 0.33 0.286 0.362 0.286 0.297 0.276 0.276\nha-en -0.014 -0.013 -0.011 -0.014 -0.012 -0.015 -0.014 -0.014 -0.014 -0.014 -0.014\nja-en -0.016 -0.02 -0.021 -0.02 -0.02 -0.022 -0.022 -0.021 -0.02 -0.021 -0.018\nru-en -0.121 -0.121 -0.123 -0.123 -0.123 -0.126 -0.124 -0.116 -0.116 -0.12 -0.121\nde-en -0.152 -0.152 -0.152 -0.154 -0.159 -0.154 -0.154 -0.155 -0.156 -0.155 -0.157\nfr-de 0.049 0.047 0.043 0.041 0.031 0.034 0.032 0.036 0.032 0.032 0.031\nde-fr 0.039 0.04 0.035 0.039 0.049 0.046 0.053 0.046 0.046 0.046 0.052\nbn-hi 0.015 0.02 0.022 0.022 0.019 0.02 0.023 0.023 0.027 0.025 0.025\nhi-bn 0.123 0.114 0.114 0.113 0.114 0.12 0.12 0.116 0.108 0.091 0.074\nxh-zu 0.076 0.083 0.081 0.094 0.096 0.088 0.098 0.089 0.085 0.083 0.07\nzu-xh 0.118 0.122 0.14 0.131 0.141 0.138 0.141 0.144 0.137 0.125 0.122\n745\nTable 6: Pearson correlations (r) for the WMT22 system dataset depending on the Llama layer\nLanguage pair L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11\nen-cs 0.484 0.477 0.469 0.458 0.451 0.443 0.442 0.437 0.434 0.434 0.429\nen-zh 0.069 0.091 0.115 0.122 0.113 0.118 0.119 0.111 0.11 0.115 0.123\nen-hr 0.866 0.862 0.869 0.868 0.875 0.875 0.869 0.871 0.868 0.871 0.87\nen-ja 0.911 0.914 0.909 0.901 0.906 0.909 0.911 0.905 0.896 0.893 0.896\nen-liv 0.943 0.946 0.941 0.938 0.94 0.943 0.943 0.944 0.945 0.945 0.944\nen-ru 0.798 0.787 0.792 0.794 0.798 0.793 0.793 0.797 0.8 0.798 0.793\nen-uk 0.816 0.828 0.832 0.825 0.818 0.804 0.803 0.796 0.795 0.789 0.788\nen-de 0.598 0.602 0.62 0.641 0.644 0.654 0.651 0.659 0.673 0.672 0.672\nliv-en 0.987 0.984 0.978 0.979 0.979 0.976 0.978 0.98 0.982 0.981 0.978\nzh-en 0.715 0.777 0.807 0.821 0.83 0.841 0.842 0.851 0.852 0.85 0.848\nsah-ru 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\nuk-cs 0.857 0.849 0.854 0.851 0.854 0.861 0.861 0.858 0.855 0.856 0.857\ncs-uk 0.851 0.846 0.836 0.841 0.84 0.839 0.838 0.836 0.835 0.836 0.834\nLanguage pair L12 L13 L14 L15 L16 L17 L18 L19 L20 L21 L22\nen-cs 0.429 0.431 0.436 0.44 0.44 0.44 0.445 0.444 0.439 0.437 0.439\nen-zh 0.119 0.123 0.108 0.114 0.108 0.116 0.122 0.136 0.148 0.149 0.148\nen-hr 0.871 0.87 0.872 0.873 0.877 0.878 0.879 0.881 0.881 0.879 0.879\nen-ja 0.892 0.893 0.881 0.879 0.872 0.877 0.88 0.884 0.885 0.889 0.891\nen-liv 0.944 0.946 0.946 0.947 0.947 0.946 0.949 0.947 0.943 0.942 0.942\nen-ru 0.795 0.794 0.798 0.795 0.798 0.795 0.796 0.789 0.785 0.782 0.782\nen-uk 0.789 0.79 0.793 0.794 0.794 0.793 0.798 0.798 0.796 0.796 0.794\nen-de 0.668 0.666 0.665 0.658 0.666 0.664 0.66 0.652 0.648 0.644 0.644\nliv-en 0.977 0.977 0.978 0.977 0.978 0.977 0.98 0.978 0.977 0.975 0.975\nzh-en 0.845 0.844 0.845 0.842 0.842 0.839 0.84 0.836 0.828 0.826 0.83\nsah-ru 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\nuk-cs 0.855 0.855 0.855 0.855 0.854 0.853 0.853 0.851 0.848 0.846 0.848\ncs-uk 0.832 0.832 0.83 0.831 0.828 0.829 0.83 0.831 0.83 0.83 0.832\nTable 7: Kendall correlations (τ) for the WMT22 segment dataset depending on the Llama layer\nLanguage pair L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11\nen-cs 0.014 0.014 0.023 0.024 0.021 0.024 0.021 0.021 0.015 0.009 0.013\nen-zh -0.041 -0.03 -0.034 -0.039 -0.035 -0.024 -0.026 -0.027 -0.026 -0.029 -0.026\nen-hr 0.119 0.115 0.14 0.125 0.123 0.11 0.117 0.118 0.128 0.123 0.122\nen-ja 0.097 0.098 0.101 0.091 0.085 0.082 0.081 0.082 0.079 0.08 0.075\nen-liv 0.362 0.332 0.35 0.34 0.33 0.33 0.334 0.31 0.292 0.322 0.332\nen-ru 0.262 0.227 0.234 0.248 0.23 0.199 0.188 0.227 0.223 0.244 0.234\nen-uk 0.081 0.063 0.062 0.062 0.066 0.055 0.044 0.041 0.04 0.048 0.052\nen-de 0.4 0.4 0.4 0.8 0.8 0.8 1.0 0.8 0.8 0.8 1.0\nliv-en 0.247 0.276 0.311 0.303 0.302 0.302 0.286 0.298 0.291 0.306 0.314\nzh-en 0.179 0.217 0.241 0.217 0.195 0.225 0.223 0.213 0.193 0.209 0.203\nsah-ru 0.458 0.419 0.484 0.492 0.478 0.45 0.456 0.47 0.461 0.456 0.444\nuk-cs 0.145 0.159 0.16 0.153 0.141 0.125 0.132 0.146 0.167 0.156 0.146\ncs-uk 0.133 0.149 0.159 0.154 0.154 0.148 0.15 0.151 0.149 0.154 0.151\nLanguage pair L12 L13 L14 L15 L16 L17 L18 L19 L20 L21 L22\nen-cs 0.008 0.014 0.014 0.011 0.02 0.018 0.017 0.011 0.011 0.008 0.008\nen-zh -0.03 -0.028 -0.034 -0.033 -0.031 -0.033 -0.033 -0.028 -0.029 -0.027 -0.027\nen-hr 0.118 0.11 0.119 0.116 0.109 0.101 0.106 0.118 0.127 0.129 0.128\nen-ja 0.081 0.083 0.082 0.083 0.081 0.083 0.084 0.075 0.072 0.074 0.074\nen-liv 0.344 0.344 0.346 0.34 0.328 0.328 0.346 0.34 0.34 0.344 0.348\nen-ru 0.22 0.202 0.174 0.188 0.16 0.167 0.167 0.202 0.192 0.195 0.206\nen-uk 0.049 0.047 0.048 0.048 0.044 0.049 0.042 0.036 0.049 0.038 0.034\nen-de 1.0 0.8 0.8 0.8 0.6 0.8 1.0 0.8 1.0 1.0 1.0\nliv-en 0.313 0.286 0.261 0.26 0.249 0.258 0.269 0.28 0.272 0.277 0.286\nzh-en 0.225 0.219 0.195 0.191 0.179 0.185 0.179 0.183 0.211 0.213 0.209\nsah-ru 0.441 0.436 0.45 0.439 0.43 0.427 0.447 0.416 0.408 0.399 0.408\nuk-cs 0.144 0.15 0.155 0.153 0.149 0.159 0.158 0.156 0.143 0.137 0.137\ncs-uk 0.148 0.152 0.154 0.15 0.14 0.14 0.142 0.141 0.139 0.141 0.141",
  "topic": "Embedding",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.7378381490707397
    },
    {
      "name": "Computer science",
      "score": 0.7325107455253601
    },
    {
      "name": "Task (project management)",
      "score": 0.6024553775787354
    },
    {
      "name": "Translation (biology)",
      "score": 0.599579930305481
    },
    {
      "name": "Metric (unit)",
      "score": 0.5936200618743896
    },
    {
      "name": "Natural language processing",
      "score": 0.5309103727340698
    },
    {
      "name": "Space (punctuation)",
      "score": 0.47561055421829224
    },
    {
      "name": "Vector space",
      "score": 0.46300721168518066
    },
    {
      "name": "Metric space",
      "score": 0.45629146695137024
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43105319142341614
    },
    {
      "name": "Theoretical computer science",
      "score": 0.41110557317733765
    },
    {
      "name": "Mathematics",
      "score": 0.1651739776134491
    },
    {
      "name": "Discrete mathematics",
      "score": 0.11608251929283142
    },
    {
      "name": "Pure mathematics",
      "score": 0.07155156135559082
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I42934936",
      "name": "Dublin City University",
      "country": "IE"
    }
  ],
  "cited_by": 5
}