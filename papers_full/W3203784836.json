{
  "title": "DziriBERT: a Pre-trained Language Model for the Algerian Dialect",
  "url": "https://openalex.org/W3203784836",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4310918714",
      "name": "Abdaoui, Amine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4310918715",
      "name": "Berrimi, Mohamed",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2403656781",
      "name": "Oussalah Mourad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1689668599",
      "name": "Moussaoui Abdelouahab",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W3133440961",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2953958347",
    "https://openalex.org/W3108716566",
    "https://openalex.org/W2990188683",
    "https://openalex.org/W3155053993",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W3105234097",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3019239186",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3176169354",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3031208816",
    "https://openalex.org/W2994831951"
  ],
  "abstract": "Pre-trained transformers are now the de facto models in Natural Language Processing given their state-of-the-art results in many tasks and languages. However, most of the current models have been trained on languages for which large text resources are already available (such as English, French, Arabic, etc.). Therefore, there are still a number of low-resource languages that need more attention from the community. In this paper, we study the Algerian dialect which has several specificities that make the use of Arabic or multilingual models inappropriate. To address this issue, we collected more than one million Algerian tweets, and pre-trained the first Algerian language model: DziriBERT. When compared with existing models, DziriBERT achieves better results, especially when dealing with the Roman script. The obtained results show that pre-training a dedicated model on a small dataset (150 MB) can outperform existing models that have been trained on much more data (hundreds of GB). Finally, our model is publicly available to the community.",
  "full_text": "arXiv:2109.12346v3  [cs.CL]  12 Dec 2022\nDziriBERT : a Pre-trained Language Model\nfor the Algerian Dialect\nAmine Abdaoui\nOracle ∗\namin.abdaoui@oracle.com\nMohamed Berrimi\nUniversity of Ferhat Abbas 1\nDepartment of computer science\nmohamed.berrimi@univ-setif.dz\nMourad Oussalah\nUniversity of Oulu\nDepartment of CS and Eng.\nmourad.oussalah@oulu.fi\nAbdelouahab Moussaoui\nUniversity of Ferhat Abbas 1\nDepartment of computer science\nAbdelouahab.moussaoui@univ-setif.dz\nAbstract\nPre-trained transformers are now the de facto models in Natu ral Language Processing given their\nstate-of-the-art results in many tasks and languages. Howe ver, most of the current models have been\ntrained on languages for which large text resources are alre ady available (such as English, French,\nArabic, etc.). Therefore, there are still a number of low-re source languages that need more attention\nfrom the community. In this paper, we study the Algerian dial ect which has several speciﬁcities that\nmake the use of Arabic or multilingual models inappropriate . T o address this issue, we collected\nmore than one million Algerian tweets, and pre-trained the ﬁ rst Algerian language model: DziriB-\nER T . When compared with existing models, DziriBER T achieve s better results, especially when\ndealing with the Roman script. The obtained results show tha t pre-training a dedicated model on a\nsmall dataset (150 MB) can outperform existing models that h ave been trained on much more data\n(hundreds of GB). Finally, our model is publicly available t o the community.\n1 Introduction\nRecently, there has been a wide interest in pre-training and ﬁne-tuning large language models using\nthe transformer architecture [1, 2]. In contrast to previou s word embeddings [3, 4], current language\nmodels are trained to generate contextualized embeddings w hich allow a quality leap in most Natu-\nral Language Processing tasks. However, most of the current transformers have been pre-trained on\nlanguages for which large text resources are already availa ble, such as English [1], French [5] and\nItalian [6]. Even multilingual models, such as the mBER T [1] and XLM-R [7], are limited to ofﬁ-\ncial languages that have a large web presence. Low-resource languages such as African and Arabic\ndialects received less attention due to the lack of data and t heir speciﬁc and/or complex morphol-\nogy. For example, the Algerian dialect is spoken by 44 Millio n people but lacks publicly available\ndatasets.\nIndeed, Modern Standard Arabic (MSA) is the most common writ ten language in ofﬁcial documents,\nbooks, and newspapers in Algeria. However, the local dialec t is very frequent in informal commu-\nnications, messaging, or in the social media sphere. A recen t study [8] showed that 74.6% of the\nAlgerian web-generated content (mostly on Facebook) is con veyed in dialectal Arabic rather than\nMSA, and 62% of this content is transcribed in Roman alphabet characters (which is also known as\nArabizi).\n∗ This work was conducted during the time the author was employ ed by Geotrend.\n36th Conference on Neural Information Processing Systems ( NeurIPS 2022).\nThe Algerian dialect is mainly inspired from standard Arabi c but also from T amazight 2, French,\nTurkish, Spanish, Italian, and English. It has several spec iﬁcities that make the application of MSA\nor multilingual models inappropriate. First, it may be writ ten either using Arabic or Roman letters\n(e.g. Salam /chard0/char43/char83(eng: Peace) ). Then, numbers are sometimes used to represen t letters that do not\nexist in the Roman alphabet (e.g. the use of the number 3 to rep resent the letter /chara8 or the number 7\nto represent the letter /char68). Finally, despite the inﬂuence of the above-cited languag es, the Algerian\ndialect also has its own vocabulary that does not exist in oth er standard languages.\nIn this paper, we present a new BER T -like model for the Algeri an dialect, named DziriBER T . It has\nbeen pre-trained on one Million Algerian tweets. W e evaluat e DziriBER T on sentiment, emotion and\ntopic classiﬁcation datasets. The experiments revealed th at DziriBER T achieves new state-of-the-art\nresults on several datasets when compared to existing Arabi c and multilingual models.\n2 DziriBERT : an Algerian Language Model\nIn this section, we describe the collected data and the pre-t raining settings of DziriBER T .\n2.1 T raining Data\nSince there is no available text dataset for the Algerian dia lect, we collected 1.2 Million tweets us-\ning T witter API 3 that were posted from major and populated Algerian cities, u sing a set of popular\nkeywords in the Algerian spoken dialect, such as: ya kho <eng: my brother>, /char68/char40/charf0/char50/char0d/char40<eng: come>,\njibli <eng: get me>, etc. The collected tweets may be written eithe r using Arabic or Latin characters.\nThe ﬁnal dataset after removing all duplicates and entries w ith less than three tokens contained 1.1\nMillion tweets (20 Million tokens), which represents almos t 150 MB of text data. Then, we per-\nformed a light prepossessing on the collected data by (i) rep lacing all user mentions with @user; (ii)\nall email addresses with mail@email.com; and (iii) all hyperlinks with https://anonymizedlink.com.\nFinally, we randomly separate the collected data to a traini ng set (having 1 Million entries) and a\ntest set (having 100 Thousand entries).\nThe collected dataset is smaller in size when compared to oth er large scale studies [1, 9]. However,\nit has been reported that we may need much less data than what w e usually use when pre-training\nlanguage models [5]. The authors have shown that their ofﬁci al model (CamemBER T) trained on\n138 GB performs similarly with another version trained only on a sample dataset of 4 GB. Here, we\ntry to push this limit even further.\n2.2 Language Modeling\nDziriBER T uses the same architecture of BER T Base (12 encoders, 12 attention heads, and a hidden\ndimension of 768). First, we train a W ordPiece T okenizer [10 ] on our training data with a vocabu-\nlary size of 50 Thousand entries. Then, we train our language model using the Masked Language\nModeling (MLM) task. Indeed, several studies have shown tha t the Next Sentence Prediction (NSP)\ntask, originally used in BER T , does not improve the results o f downstream tasks [11, 12].\nSince tweets have a short length, we used an MLM probability o f 25% (instead of the usual 15%).\nW e also set a batch size of 64 due to the limitations of our comp utational resources. The model\nhas been trained on an A WS g4dn.2xlarge instance 4 with 32 GB of memory and 1 NVIDIA T4\nGPU. The training took almost 10 days to complete 50 epochs ac ross the whole training set (around\n800k steps). The ﬁnal model created using PyT orch has been up loaded on the Transformers Hub to\nfacilitate its use 5 .\n2 The original language of the ﬁrst inhabitants of the region.\n3 https://developer.twitter.com/en/docs\n4 https://aws.amazon.com/ec2/instance-types/g4/\n5 for anonymity reasons, the link will be added in later versio ns\n2\n3 Evaluation of DziriBERT\nIn order to compare DziriBER T with existing models, we have t o ﬁne-tune them on downstream\ntasks. It should also be noted that most of related studies [1 3] used either non publicly available\ndataset or contain only a small part of Algerian dialect, whi ch restricted the scale of potential com-\nparative analysis. In this paper, we considered two publicl y available corpora that covered both\nArabic and Roman scripts: T wiﬁl [14] and Narabizi [15].\n3.1 T wiﬁl\n[14] collected and annotated thousands of Algerian tweets a ccording to the expressed sentiments and\nemotions. Most of them were written with Arabic letters but t here were also many tweets written\nusing the Roman script. The authors shared two publicly avai lable6 datasets:\n• T wiﬁl sentiment\n: 9437 tweets annotated according to 3 polarity classes (pos itive, negative\nand neutral);\n• T wiﬁl emotion : 5110 tweets annotated according to the 10 Plutchnik emotio n classes [16].\n3.2 Narabizi\nThe Narabizi corpus, originally published in [17], contain s Algerian Arabic sentences written exclu-\nsively with the Roman script (Arabizi). In this paper, we use the sentiment and topic classiﬁcation\ndatasets annotated in [15]:\n• Narabizi sentiment : 1279 sentences annotated according to 4 sentiment classes (positive,\nnegative, mix and neutral);\n• Narabizi topic : 1279 sentences annotated according to 5 topic classes (spo rts, societal, pol-\nitics, religion and none).\nThese four datasets were used to compare DziriBER T with the t wo most known multilingual trans-\nformers (mBER T and XLM-R), and with multiple standard and di alectal Arabic models(AraBER T ,\nQARiB, CamelBER T and MARBER T). Among the four available ver sions of CamelBER T , we eval-\nuated the dialectal version (CamelBER T -da) and the one that has been pre-trained on a mix of all\ndatasets (CamelBER T -mix).\nFollowing the work done in [1], we used the ﬁnal hidden state o f the classiﬁcation token ([CLS]) as a\nsentence representation followed by one linear layer as a cl assiﬁer. All models have been ﬁne-tuned\nfor three epochs using the Trainer Class of the Transformers library [18] with its default settings.\nT en different runs have been conducted for each model on each dataset according to the same 10\nseeds that have been randomly generated. Still, the present ed results may be reproduced using the\nshared Github repository 7 .\nT ables 1, and 2 present the obtained results on the T wiﬁl and N arabizi datasets. W e calculate the\naccuracy and the macro averaged precision, recall, and F1 sc ore for each model on each dataset.\n4 Discussion\nAs shown in T able 1, DziriBER T and MARBER T [19] achieved the b est results on the T wiﬁl datasets\n(which are mainly composed of Arabic script). These two mode ls, which are both pre-trained on\ntweets, yielded better results than all other multilingual , standard Arabic, and dialectal Arabic mod-\nels. However, DziriBER T yielded much better results on the N arabizi datasets (which are exclusively\ncomposed of Roman script) as shown in T ables 1 and 2. MARBER T i s again in the second posi-\ntion but the difference with DziriBER T is much more importan t (+5.5% in sentiment accuracy and\n+13.8% in topic classiﬁcation).\nAn error analysis step revealed that the T wiﬁl datasets cont ain several entries that are not written in\nAlgerian Arabic. DziriBER T tends to fail more often than MAR BER T on documents that are written\n6 https://github.com/kinmokusu/oea_algd\n7 https://github.com/alger-ia/dziribert(the generated 10 seeds are listed in the evaluation script)\n3\nT able 1: Accuracy and macro averaged Precision, Recall and F 1 score obtained on the T wiﬁl datasets\nT wiﬁl sentiment T wiﬁl emotion\nModel Acc. F1. Pre. Rec. Acc. F1. Pre. Rec.\nmBER T 74.2 73.8 75.2 73.0 62.0 26.0 33.3 27.0\nXLM-R 79.9 79.5 80.9 79.1 64.9 26.1 26.5 28.1\nAraBER T 73.8 73.2 74.9 72.3 64.6 30.3 38.0 30.7\nQARiB 78.8 78.2 79.0 77.9 68.9 39.2 42.2 38.7\nCamel-BER T -da 75.2 74.6 76.0 74.0 66.0 34.6 38.7 34.6\nCamel-BER T -mix 77.7 72.2 78.6 76.7 69.1 38.2 43.8 37.5\nMARBER T 80.6 79.9 80.7 79.6 70.2 39.1 41.7 39.4\nDziriBER T 80.5 80.0 81.1 79.5 70.4 40.1 42.8 39.6\nT able 2: Accuracy and macro averaged Precision, Recall and F 1 score obtained on the Narabizi\ndatasets\nNarabizi sentiment Narabizi topic\nModel Acc. F1. Pre Rec. Acc. F1. Pre Rec.\nmBER T 52.6 49.3 50.5 49.5 49.3 30.8 33.8 34.1\nXLM-R 41.9 32.2 38.1 38.3 43.6 21.4 19.3 27.2\nAraBER T 49.1 46.0 47.9 47.7 42.8 20.8 19.4 26.5\nQARiB 55.0 52.9 53.7 53.4 45.7 29.7 29.9 32.4\nCamel-BER T -da 40.9 35.5 36.0 40.1 43.7 21.5 20.2 27.3\nCamel-BER T -mix 49.4 48.3 49.4 49.6 47.0 27.5 25.8 31.4\nMARBER T 58.0 55.5 56.3 55.7 49.0 31.0 29.9 34.1\nDziriBER T 63.5 61.2 62.0 61.4 62.8 54.8 64.0 53.2\nin standard Arabic or in other Arabic dialects, which may als o explain the good results obtained by\nMARBER T on T wiﬁl. Overall, our experiments have shown that D ziriBER T can yield very good\nresults despite the size of its pre-training dataset. For ex ample, MARBER T has been trained on 128\nGB of text (almost x1000 times larger than our pre-training c orpus). Still, DziriBER T is at least\nas good as MARBER T on the Algerian dialect and even much bette r when dealing with Roman\ncharacters.\nFurthermore, DziriBER T’s vocabulary contains a relativel y small number of tokens when compared\nto the other baselines. Since the embedding layer concentra tes most of the model parameters [20],\nreducing the number of tokens should have a signiﬁcant impac t on the ﬁnal model size (which\nshould facilitate its deployment on Public Cloud Platforms ). T able 3 presents the vocabulary length,\nthe total number of parameters, and the ﬁnal size on disk for a ll models studied here. As expected,\neven if all models share the same architecture (12 encoders, 12 attention heads, and 768 hidden\ndimensions), the total number of parameters varies from 110 Million to 278 Million. With its 50k\nvocabulary, DziriBER T is therefore one of the smallest mode ls studied here.\nModel V ocab. #Params Size\n(Million) (MB)\nmBER T 106k 167 672\nXLM-R 250k 278 1147\nAraBER T 64k 135 543\nQARiB 64k 135 543\nCamel-BER T -da 30k 110 439\nCamel-BER T -mix 30k 110 439\nMARBER T 100k 163 654\nDziriBER T 50k 124 498\nT able 3: Models comparison according to the vocabulary leng th, the total number of parameters and\nthe ﬁnal size on disk.\n4\n5 Conclusion\nIn this paper, we have presented the pre-training and evalua tion of DziriBER T , a BER T -based model\nfor the Algerian dialect. Our experiments have shown that th is model can achieve good performance\non various NLP tasks, even when trained on a relatively small amount of data. In order to support the\ndevelopment of NLP applications for this low-resource dial ect, we are making our pre-trained model\npublicly available. W e will also release ﬁne-tuned version s of the model for sentiment analysis,\nemotion detection, and topic classiﬁcation. A natural futu re work would concern the compilation\nof more datasets for the Algerian Dialect and the comparison with more recent pre-trained models\nsuch as charBER T [21].\nW e hope that our work will help to advance the state of the art i n NLP for the Algerian dialect and\nenable researchers and developers to build more effective a nd useful applications for this language\nReferences\n[1] J. Devlin, M.-W . Chang, K. Lee, and K. T outanova, “BER T: P re-training of deep bidirectional\ntransformers for language understanding, ” in Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computationa l Linguistics: Human Language\nT echnologies, V olume 1 (Long and Short P apers) , Jun. 2019, pp. 4171–4186. [Online].\nA vailable: https://www .aclweb.org/anthology/N19-1423\n[2] T . Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P . Dh ariwal, A. Neelakantan,\nP . Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T . Henighan,\nR. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandli sh, A. Radford,\nI. Sutskever, and D. Amodei, “Language models are few-shot l earners, ” in Advances\nin Neural Information Processing Systems , H. Larochelle, M. Ranzato, R. Hadsell,\nM. F . Balcan, and H. Lin, Eds., vol. 33, 2020, pp. 1877–1901. [ Online]. A vailable:\nhttps://proceedings.neurips.cc/paper/2020/ﬁle/1457c 0d6bfcb4967418bfb8ac142f64a-Paper .pdf\n[3] T . Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. De an, “Distributed\nrepresentations of words and phrases and their composition ality, ” in Advances in Neural\nInformation Processing Systems , C. J. C. Burges, L. Bottou, M. W elling, Z. Ghahra-\nmani, and K. Q. W einberger, Eds., vol. 26. Curran Associates , Inc., 2013. [Online]. A vailable:\nhttps://proceedings.neurips.cc/paper/2013/ﬁle/9aa42 b31882ec039965f3c4923ce901b-Paper.pdf\n[4] J. Pennington, R. Socher, and C. Manning, “GloVe: Global vectors for word\nrepresentation, ” in Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) , Oct. 2014, pp. 1532–1543. [Online]. A vailable:\nhttps://www .aclweb.org/anthology/D14-1162\n[5] L. Martin, B. Muller, P . J. O. Suárez, Y . Dupont, L. Romary , É. V . de la Clergerie, D. Seddah,\nand B. Sagot, “Camembert: a tasty french language model, ” in Proceedings of the 58th\nAnnual Meeting of the Association for Computational Lingui stics, 2020. [Online]. A vailable:\nhttps://www .aclweb.org/anthology/2020.acl-main.645\n[6] M. Polignano, P . Basile, M. de Gemmis, G. Semeraro, and V . Basile, “ Alberto: Italian bert\nlanguage understanding model for nlp challenging tasks bas ed on tweets, ” in 6th Italian\nConference on Computational Linguistics, CLiC-it 2019 , vol. 2481, 11 2019. [Online].\nA vailable: http://ceur-ws.org/V ol-2481/paper57.pdf\n[7] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. W en zek, F . Guzmán,\nÉ. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov, “Unsuperv ised cross-lingual\nrepresentation learning at scale, ” in Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , 2020, pp. 8440–8451. [Online]. A vailable:\nhttps://www .aclweb.org/anthology/2020.acl-main.747.pdf\n[8] J. Y ounes, E. Souissi, H. Achour, and A. Ferchichi, “Lang uage resources for maghrebi arabic\ndialects’ nlp: a survey, ” Language Resources and Evaluation , vol. 54, no. 4, pp. 1079–1142,\n2020. [Online]. A vailable: https://link.springer.com/a rticle/10.1007/s10579-020-09490-9\n5\n[9] W . Antoun, F . Baly, and H. Hajj, “AraBER T: Transformer-b ased model for Arabic language\nunderstanding, ” in Proceedings of the 4th W orkshop on Open-Source Arabic Corpo ra\nand Processing T ools, with a Shared T ask on Offensive Langua ge Detection . Marseille,\nFrance: European Language Resource Association, May 2020, pp. 9–15. [Online]. A vailable:\nhttps://www .aclweb.org/anthology/2020.osact-1.2\n[10] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W . Mache rey, M. Krikun, Y . Cao, Q. Gao,\nK. Macherey et al. , “Google’s neural machine translation system: Bridging th e gap between\nhuman and machine translation, ” arXiv preprint arXiv:1609.08144 , 2016. [Online]. A vailable:\nhttps://arxiv.org/abs/1609.08144\n[11] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\nand V . Stoyanov, “Roberta: A robustly optimized bert pretra ining approach, ” arXiv preprint\narXiv:1907.11692 , 2019. [Online]. A vailable: https://arxiv.org/abs/1907 .11692\n[12] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P . Sharma, and R. S oricut, “ Albert: A lite bert for\nself-supervised learning of language representations, ” arXiv preprint arXiv:1909.11942 , 2019.\n[Online]. A vailable: https://arxiv.org/abs/1909.11942\n[13] I. Guellil, A. Adeel, F . Azouaou, M. Boubred, Y . Houichi , and A. A. Moumna, “Sexism\ndetection: The ﬁrst corpus in algerian dialect with a code-s witching in arabic/ french and\nenglish, ” 2021. [Online]. A vailable: https://arxiv.org/ abs/2104.01443\n[14] L. Moudjari, K. Akli-Astouati, and F . Benamara, “ An alg erian corpus and an\nannotation platform for opinion and emotion analysis, ” in Proceedings of The 12th\nLanguage Resources and Evaluation Conference , 2020, pp. 1202–1210. [Online]. A vailable:\nhttps://www .aclweb.org/anthology/2020.lrec-1.151/\n[15] S. T ouileb and J. Barnes, “The interplay between langua ge similarity and script\non a novel multi-layer algerian dialect corpus, ” in Findings of the Association for\nComputational Linguistics: ACL-IJCNLP 2021 , 2021, pp. 3700–3712. [Online]. A vailable:\nhttps://aclanthology.org/2021.ﬁndings-acl.324/\n[16] R. Plutchik, “Emotions: A general psychoevolutionary theory, ” Approaches to emotion , vol.\n1984, pp. 197–219, 1984.\n[17] D. Seddah, F . Essaidi, A. Fethi, M. Futeral, B. Muller, P . J. O. Suárez, B. Sagot, and\nA. Srivastava, “Building a user-generated content north-a frican arabizi treebank: T ackling hell, ”\nin Proceedings of the 58th Annual Meeting of the Association fo r Computational Linguistics ,\n2020, pp. 1139–1150. [Online]. A vailable: https://aclant hology.org/2020.acl-main.107/\n[18] T . W olf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. M oi, P . Cistac, T . Rault, R. Louf,\nM. Funtowicz, J. Davison, S. Shleifer, P . von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu,\nT . Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush, “Trans formers: State-of-the-art\nnatural language processing, ” in Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstrations , Online, Oct. 2020, pp. 38–45.\n[Online]. A vailable: https://www .aclweb.org/anthology /2020.emnlp-demos.6\n[19] M. Abdul-Mageed, A. A. Elmadany, and E. M. B. Nagoudi, “A RBER T & MARBER T:\ndeep bidirectional transformers for arabic, ” in Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics , 2021, p. 7088–7105. [Online]. A vailable:\nhttps://aclanthology.org/2021.acl-long.551.pdf\n[20] A. Abdaoui, C. Pradel, and G. Sigel, “Load what you need: Smaller versions of\nmutililingual BER T, ” in Proceedings of SustaiNLP: W orkshop on Simple and Efﬁcient\nNatural Language Processing @EMNLP , 2020, pp. 119–123. [Online]. A vailable:\nhttps://www .aclweb.org/anthology/2020.sustainlp-1.16\n[21] A. Riabi, B. Sagot, and D. Seddah, “Can character-based language models improve\ndownstream task performance in low-resource and noisy lang uage scenarios?” 2021. [Online].\nA vailable: https://arxiv.org/abs/2110.13658\n6",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7881772518157959
    },
    {
      "name": "Arabic",
      "score": 0.7005525827407837
    },
    {
      "name": "Natural language processing",
      "score": 0.6250818967819214
    },
    {
      "name": "Transformer",
      "score": 0.5699206590652466
    },
    {
      "name": "Language model",
      "score": 0.5657371282577515
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5462609529495239
    },
    {
      "name": "De facto",
      "score": 0.4847269058227539
    },
    {
      "name": "Training set",
      "score": 0.46637728810310364
    },
    {
      "name": "Linguistics",
      "score": 0.33704882860183716
    },
    {
      "name": "Political science",
      "score": 0.07274973392486572
    },
    {
      "name": "Engineering",
      "score": 0.06774723529815674
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250955327",
      "name": "Huawei Technologies (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I40193446",
      "name": "University Ferhat Abbas of Setif",
      "country": "DZ"
    }
  ],
  "cited_by": 7
}