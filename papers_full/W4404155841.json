{
  "title": "Virtual Patients Using Large Language Models: Scalable, Contextualized Simulation of Clinician-Patient Dialogue With Feedback (Preprint)",
  "url": "https://openalex.org/W4404155841",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2148983146",
      "name": "David A. Cook",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2269725249",
      "name": "Joshua Overgaard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2034496786",
      "name": "V. Shane Pankratz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1596865025",
      "name": "Guilherme Del Fiol",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2966873230",
      "name": "Chris A. Aakre",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2140657380",
    "https://openalex.org/W2545846385",
    "https://openalex.org/W2163596827",
    "https://openalex.org/W2064207419",
    "https://openalex.org/W2029342902",
    "https://openalex.org/W2147782722",
    "https://openalex.org/W2036488299",
    "https://openalex.org/W2009683697",
    "https://openalex.org/W3004612364",
    "https://openalex.org/W2800373720",
    "https://openalex.org/W2943533685",
    "https://openalex.org/W4319825970",
    "https://openalex.org/W2767867182",
    "https://openalex.org/W2995551433",
    "https://openalex.org/W3037384924",
    "https://openalex.org/W2316373143",
    "https://openalex.org/W2067627650",
    "https://openalex.org/W2129404566",
    "https://openalex.org/W4285096383",
    "https://openalex.org/W2129889309",
    "https://openalex.org/W2426331214",
    "https://openalex.org/W2020270970",
    "https://openalex.org/W2063236842",
    "https://openalex.org/W1973975145",
    "https://openalex.org/W2016697128",
    "https://openalex.org/W2103832456",
    "https://openalex.org/W2003579301",
    "https://openalex.org/W2981601710",
    "https://openalex.org/W3004481391",
    "https://openalex.org/W4235186849",
    "https://openalex.org/W2302501749",
    "https://openalex.org/W2885740121",
    "https://openalex.org/W2138200074",
    "https://openalex.org/W3131346895",
    "https://openalex.org/W4385694104",
    "https://openalex.org/W4400593735",
    "https://openalex.org/W4234240788",
    "https://openalex.org/W2103356100",
    "https://openalex.org/W3109862485",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W2944069152",
    "https://openalex.org/W3035629539",
    "https://openalex.org/W3172931322",
    "https://openalex.org/W2963903950",
    "https://openalex.org/W4285188834",
    "https://openalex.org/W4205686084",
    "https://openalex.org/W2992347006",
    "https://openalex.org/W2146401134",
    "https://openalex.org/W2939046508",
    "https://openalex.org/W2163004454",
    "https://openalex.org/W3118220099",
    "https://openalex.org/W4391787898",
    "https://openalex.org/W4388704286",
    "https://openalex.org/W2953634197",
    "https://openalex.org/W4390105133",
    "https://openalex.org/W3213621237",
    "https://openalex.org/W4390988578",
    "https://openalex.org/W3196248941",
    "https://openalex.org/W4376643691",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W2046249402"
  ],
  "abstract": "<sec> <title>BACKGROUND</title> Virtual patients (VPs) are computer screen–based simulations of patient-clinician encounters. VP use is limited by cost and low scalability. </sec> <sec> <title>OBJECTIVE</title> We aimed to show that VPs powered by large language models (LLMs) can generate authentic dialogues, accurately represent patient preferences, and provide personalized feedback on clinical performance. We also explored using LLMs to rate the quality of dialogues and feedback. </sec> <sec> <title>METHODS</title> We conducted an intrinsic evaluation study rating 60 VP-clinician conversations. We used carefully engineered prompts to direct OpenAI’s generative pretrained transformer (GPT) to emulate a patient and provide feedback. Using 2 outpatient medicine topics (chronic cough diagnosis and diabetes management), each with permutations representing different patient preferences, we created 60 conversations (dialogues plus feedback): 48 with a human clinician and 12 “self-chat” dialogues with GPT role-playing both the VP and clinician. Primary outcomes were dialogue authenticity and feedback quality, rated using novel instruments for which we conducted a validation study collecting evidence of content, internal structure (reproducibility), relations with other variables, and response process. Each conversation was rated by 3 physicians and by GPT. Secondary outcomes included user experience, bias, patient preferences represented in the dialogues, and conversation features that influenced authenticity. </sec> <sec> <title>RESULTS</title> The average cost per conversation was US $0.51 for GPT-4.0-Turbo and US $0.02 for GPT-3.5-Turbo. Mean (SD) conversation ratings, maximum 6, were overall dialogue authenticity 4.7 (0.7), overall user experience 4.9 (0.7), and average feedback quality 4.7 (0.6). For dialogues created using GPT-4.0-Turbo, physician ratings of patient preferences aligned with intended preferences in 20 to 47 of 48 dialogues (42%-98%). Subgroup comparisons revealed higher ratings for dialogues using GPT-4.0-Turbo versus GPT-3.5-Turbo and for human-generated versus self-chat dialogues. Feedback ratings were similar for human-generated versus GPT-generated ratings, whereas authenticity ratings were lower. We did not perceive bias in any conversation. Dialogue features that detracted from authenticity included that GPT was verbose or used atypical vocabulary (93/180, 51.7% of conversations), was overly agreeable (n=56, 31%), repeated the question as part of the response (n=47, 26%), was easily convinced by clinician suggestions (n=35, 19%), or was not disaffected by poor clinician performance (n=32, 18%). For feedback, detractors included excessively positive feedback (n=42, 23%), failure to mention important weaknesses or strengths (n=41, 23%), or factual inaccuracies (n=39, 22%). Regarding validation of dialogue and feedback scores, items were meticulously developed (content evidence), and we confirmed expected relations with other variables (higher ratings for advanced LLMs and human-generated dialogues). Reproducibility was suboptimal, due largely to variation in LLM performance rather than rater idiosyncrasies. </sec> <sec> <title>CONCLUSIONS</title> LLM-powered VPs can simulate patient-clinician dialogues, demonstrably represent patient preferences, and provide personalized performance feedback. This approach is scalable, globally accessible, and inexpensive. LLM-generated ratings of feedback quality are similar to human ratings. </sec>",
  "full_text": null,
  "topic": "Preprint",
  "concepts": [
    {
      "name": "Preprint",
      "score": 0.9572045207023621
    },
    {
      "name": "Dialog box",
      "score": 0.8614461421966553
    },
    {
      "name": "Scalability",
      "score": 0.7131968140602112
    },
    {
      "name": "Computer science",
      "score": 0.6526027917861938
    },
    {
      "name": "Human–computer interaction",
      "score": 0.543984591960907
    },
    {
      "name": "Virtual agent",
      "score": 0.48192453384399414
    },
    {
      "name": "Language model",
      "score": 0.41636112332344055
    },
    {
      "name": "Multimedia",
      "score": 0.3757680058479309
    },
    {
      "name": "World Wide Web",
      "score": 0.31924697756767273
    },
    {
      "name": "Natural language processing",
      "score": 0.28048470616340637
    },
    {
      "name": "Operating system",
      "score": 0.1546229124069214
    }
  ]
}