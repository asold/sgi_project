{
  "title": "Transformer-based Lexically Constrained Headline Generation",
  "url": "https://openalex.org/W3200447800",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2126494540",
      "name": "Kosuke Yamada",
      "affiliations": [
        "Nagoya University"
      ]
    },
    {
      "id": "https://openalex.org/A2760348770",
      "name": "Yuta Hitomi",
      "affiliations": [
        "Insight (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1977190966",
      "name": "Hideaki Tamori",
      "affiliations": [
        "Asahi Shimbun Company (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2052260464",
      "name": "Ryohei Sasano",
      "affiliations": [
        "Nagoya University"
      ]
    },
    {
      "id": "https://openalex.org/A2030501650",
      "name": "Naoaki Okazaki",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2084773436",
      "name": "Kentaro Inui",
      "affiliations": [
        "Tohoku University",
        "RIKEN Center for Advanced Intelligence Project"
      ]
    },
    {
      "id": "https://openalex.org/A2095805206",
      "name": "Koichi Takeda",
      "affiliations": [
        "Nagoya University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2340329281",
    "https://openalex.org/W2995057977",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W165283731",
    "https://openalex.org/W2963768805",
    "https://openalex.org/W2963877622",
    "https://openalex.org/W2964029788",
    "https://openalex.org/W2963352809",
    "https://openalex.org/W3100650951",
    "https://openalex.org/W2963986868",
    "https://openalex.org/W3111372071",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4385970303",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2467173223",
    "https://openalex.org/W648786980"
  ],
  "abstract": "This paper explores a variant of automatic headline generation methods, where a generated headline is required to include a given phrase such as a company or a product name. Previous methods using Transformer-based models generate a headline including a given phrase by providing the encoder with additional information corresponding to the given phrase. However, these methods cannot always include the phrase in the generated headline. Inspired by previous RNN-based methods generating token sequences in backward and forward directions from the given phrase, we propose a simple Transformer-based method that guarantees to include the given phrase in the high-quality generated headline. We also consider a new headline generation strategy that takes advantage of the controllable generation order of Transformer. Our experiments with the Japanese News Corpus demonstrate that our methods, which are guaranteed to include the phrase in the generated headline, achieve ROUGE scores comparable to previous Transformer-based methods. We also show that our generation strategy performs better than previous strategies.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4085â€“4090\nNovember 7â€“11, 2021.câƒ2021 Association for Computational Linguistics\n4085\nTransformer-based Lexically Constrained Headline Generation\nKosuke Yamada1âˆ— Yuta Hitomi2â€  Hideaki Tamori3 Ryohei Sasano1\nNaoaki Okazaki4 Kentaro Inui5,6 Koichi Takeda1\n1Nagoya University 2Insight Edge, Inc. 3The Asahi Shimbun Company\n4Tokyo Institute of Technology 5Tohoku University 6RIKEN AIP\nyamada.kosuke@c.mbox.nagoya-u.ac.jp,\nyuta.hitomi@insightedge.jp, tamori-h@asahi.com,\n{sasano,takedasu}@i.nagoya-u.ac.jp,\nokazaki@c.titech.ac.jp, inui@ecei.tohoku.ac.jp\nAbstract\nThis paper explores a variant of automatic\nheadline generation methods, where a gener-\nated headline is required to include a given\nphrase such as a company or a product name.\nPrevious methods using Transformer-based\nmodels generate a headline including a given\nphrase by providing the encoder with addi-\ntional information corresponding to the given\nphrase. However, these methods cannot al-\nways include the phrase in the generated\nheadline. Inspired by previous RNN-based\nmethods generating token sequences in back-\nward and forward directions from the given\nphrase, we propose a simple Transformer-\nbased method that guarantees to include the\ngiven phrase in the high-quality generated\nheadline. We also consider a new headline\ngeneration strategy that takes advantage of the\ncontrollable generation order of Transformer.\nOur experiments with the Japanese News Cor-\npus demonstrate that our methods, which are\nguaranteed to include the phrase in the gener-\nated headline, achieve ROUGE scores compa-\nrable to previous Transformer-based methods.\nWe also show that our generation strategy per-\nforms better than previous strategies.\n1 Introduction\nFollowing the initial work of Rush et al. (2015),\nabstractive headline generation using the encoder-\ndecoder model has been studied extensively\n(Chopra et al., 2016; Nallapati et al., 2016; Paulus\net al., 2018). In the automatic headline generation\nfor advertising articles, there are requests to include\na given phrase such as a company or product name\nin the headline.\nGenerating a headline that includes a given\nphrase has been considered one of the lexically con-\nstrained sentence generation tasks. For these tasks,\nâˆ—Work done during an internship at The Asahi Shimbun\nCompany\nâ€ Work done at The Asahi Shimbun Company\nthere are two major approaches. One approach is\nto select a plausible sentence including the given\nphrase from several candidate sentences generated\nfrom left to right (Hokamp and Liu, 2017; Ander-\nson et al., 2017; Post and Vilar, 2018). Although\nthese methods can include multiple phrases in a\ngenerated sentence, they are computationally ex-\npensive due to the large search space of the decod-\ning process. In addition, since they try to force\ngiven phrases into sentences at every step of the\ngeneration process, these methods may harm the\nquality of the generated sentence (Liu et al., 2019).\nAnother approach proposed by Mou et al. (2015)\nis to generate token sequences in backward and\nforward directions from the given phrase. Mou\net al. (2016) proposed Sequence to Backward and\nForward Sequences (Seq2BF), which applies the\nmethod of Mou et al. (2015) to the sequence-to-\nsequence (seq2seq) framework. They use an RNN-\nbased model and adopt the best strategies proposed\nby Mou et al. (2015), generating the backward se-\nquence from the phrase and then generating the\nremaining forward sequence. Liu et al. (2019) intro-\nduced the Generative Adversarial Network (GAN)\nto the model of Mou et al. (2015) to resolve the\nexposure bias problem (Bengio et al., 2015) caused\nby generating sequences individually, and used the\nattention mechanism (Bahdanau et al., 2015) to\nimprove the consistency between both sequences.\nHowever, their model does not support the seq2seq\nframework.\nRecently, He et al. (2020) used a Transformer-\nbased model (Vaswani et al., 2017), which is re-\nported to achieve high performance, to generate a\nheadline containing a given phrase. They proposed\nproviding an encoder with additional information\nrelated to the given phrase. However, their method\nmay not always include the given phrases in the\ngenerated headline.\nIn this study, we work on generating lexically\nconstrained headlines using Transformer-based\n4086\nSeq2BF. The RNN-based model used by Mou et al.\n(2016) executes a strategy of continuous generation\nin one direction, and thus cannot utilize the infor-\nmation of the forward sequence when generating\nthe backward sequence. However, Transformer can\nexecute a variety of generative strategies by devis-\ning attention masks, so it can solve the problem of\nthe RNN-based model. We propose a new strategy\nthat generates each token from a given phrase alter-\nnately in the backward and forward directions, in\naddition to adapting and extending the strategies of\nMou et al. (2016) to the Transformer architecture.\nOur experiments with a Japanese summarization\ncorpus show that our proposed method always in-\ncludes the given phrase in the generated headline\nand achieves performance comparable to previous\nTransformer-based methods. We also show that our\nproposed generating strategy performs better than\nthe extended strategy of the previous methods.\n2 Proposed Method\nWe propose a Transformer-based Seq2BF model\nthat applies Seq2BF proposed by Mou et al. (2016)\nto the Transformer model to generate headlines\nincluding a given phrase. The Seq2BF takes W(=\nw1, ..., wL; w1:L) as the given phrase consisting of\nL tokens and generates the headline yâˆ’M:âˆ’1 of M\ntokens backward from W, and the headline y1:N\nof N tokens forward from W. The Transformer-\nbased Seq2BF is the Transformer model with two\ngeneration components, consisting of a linear and\na softmax layer (see Figure 1).\nIn Transformer-based Seq2BF unlike Trans-\nformer generating tokens from left to right, the\ntoken position changes relatively depending on al-\nready generated tokens. We determine the token\nposition, inputting to the positional encoding layer\nof the decoder, âŒŠL+1\n2 âŒ‹in W to be 0, and the posi-\ntion in the backward direction to be negative, and\nthe position in the forward direction to be positive.\nWe consider the following four generation strate-\ngies. In addition to two strategies (a) and (b), which\nextend those proposed by Mou et al. (2016), we\nproposfe new strategies (c) and (d) as step-wise al-\nternating generation to keep better contextual con-\nsistency in both backward and forward directions.\n(a) Generating a sequence backwardand then a\nsequence forward. (Seq-B)\n(b) Generating a sequence forwardand then a se-\nquence backward. (Seq-F)\n2NM+N\nM+N\n2N-1\nNM+N\n43\n4\n4M+N432M\n32M-1 N+1N+2M+N\nğ‘¦!ğ‘¦\"â€¦ğ‘¦#ğ‘¦$%â€¦ğ‘¦$\"ğ‘¦$!\nğ‘¦$%&!â€¦ğ‘¦$!ğ‘¤!â€¦ğ‘¤'ğ‘¦!â€¦ğ‘¦#$!ğ‘‹\n1312212M+1M+2\nâ€¦â€¦â€¦â€¦â€¦â€¦\n2â€¦ 2â€¦ 1â€¦M+N 1â€¦â€¦ 1Mâ€¦2(a) Seq-B(b) Seq-F\n(d) Tok-F\n(c) Tok-BM\tâ‰¦NM\t>NM\t<NM\tâ‰§N\nGeneration order\n2\n\u0013\u0011\u0013\u0011\u0010\u0011\u001a\u0010\u0012\u0018\u0001à£¨Ú€İšNUH\nFigure 1: Overview of Transformer-based Seq2BF.\nBlue, green, and orange boxes indicate Transformer\nencoder, decoder, and generation components, respec-\ntively. The arrow from the encoder to the decoder rep-\nresents that the decoderâ€™s attention mechanism refers to\nthe output from the encoder.\n(c) Generating each token backwardand then for-\nward alternately. (Tok-B)\n(d) Generating each token forwardand then back-\nward alternately. (Tok-F)\nTransformer-based Seq2BF is formulated as\nP(Y |X, W) =\nâˆ\njâˆˆPOS j\nP(yj|Yobs, X), (1)\nwhere X denotes tokens of the article, W\ndenotes tokens of the given phrase, Y (=\nyâˆ’M:âˆ’1, w1:L, y1:N ) denotes tokens of the ï¬nal\ngenerated headline, and Yobs denotes the already-\ngenerated partial headline including W. Also,\nPOS j denotes a list of token positions representing\nthe order of tokens to be generated corresponding\nto each generation strategy (see Figure 1), for ex-\nample [âˆ’1, âˆ’2, ...,âˆ’M, 1, 2, ..., N] in Seq-B. In\nTok-B/F which M and N are different, once the\ngeneration in one direction is completed, the gen-\neration will be continued only in the remaining\ndirections until M + N steps. For example in the\ncase of M > Nin Tok-B, our method completes\ngenerating tokens in the forward direction ï¬rst, so\nit generates them in both directions until the 2N\nstep, and then generates them only in the backward\ndirection from the 2N + 1step to the M + N step.\nTo train the model on these generative strate-\ngies, we have prepared an attention mask for the\n4087\n(b) Seq-F\nğ‘¦!\"ğ‘¦!#ğ‘¤#ğ‘¦#ğ‘¦\"\n(a) Seq-B (d) Tok-F(c) Tok-B\nğ‘¦!\"ğ‘¦!#ğ‘¤#ğ‘¦#ğ‘¦\"\nInput\nOutput\nğ‘¦!\"ğ‘¦!#ğ‘¤#ğ‘¦#ğ‘¦\" ğ‘¦!\"ğ‘¦!#ğ‘¤#ğ‘¦#ğ‘¦\" ğ‘¦!\"ğ‘¦!#ğ‘¤#ğ‘¦#ğ‘¦\"ğ‘¦!\"ğ‘¦!#ğ‘¤#ğ‘¦#ğ‘¦\"\nğ‘¦!\"ğ‘¦!#ğ‘¤#ğ‘¦#ğ‘¦\"\nğ‘¦!\"ğ‘¦!#ğ‘¤#ğ‘¦#ğ‘¦\"\nFigure 2: Attention mask patterns on the decoder side during training, used for each generation strategy in\nTransformer-based Seq2BF. The dark cells indicate the masked areas. These are examples of the headline with\nthe length of 5, where w1 is speciï¬ed as the phrase.\ndecoder. Transformer can control the generation\norder of tokens by devising the attention mask used\nin the decoderâ€™s self-attention mechanism. Trans-\nformer generates tokens from left to right, so it is\nsufï¬cient to disable the attention to tokens forward\nfrom the input tokens. However, the Transformer-\nbased Seq2BF needs to specify the areas where\ninput tokens disallow the attention in the backward\nand forward directions, depending on each genera-\ntion strategy (see Figure 2).\n3 Experiment\nWe conducted the experiment to verify the perfor-\nmance of our methods in the headline generation\ntask. The objective of our experiment is to com-\npare our method with previous Transformer-based\nmethods that generate tokens from left to right. We\nalso compare Seq-B/F, the generation orders pro-\nposed by Mou et al. (2016), with Tok-B/F, our new\ngeneration orders.\n3.1 Setting\nWe used the 2019 version of the Japanese News\nCorpus (JNC)1 (Hitomi et al., 2019) as the dataset.\nThe JNC contains 1,932,399 article-headline pairs,\nand we split them randomly at a ratio of 98:1:1\nfor use as training, validation, and test sets, respec-\ntively.2 We utilized MeCab (Kudo et al., 2004)\nwith the IPAdic3 and then applied the Byte Pair\nEncoding (BPE) algorithm4 (Gage, 1994) for to-\nkenization. We trained BPE with 10,000 merge\noperations and obtained the most frequent 32,000\n1https://cl.asahi.com/api_data/\njnc-jamul-en.html\n2We applied the preprocessing script at\nhttps://github.com/asahi-research/\nscript-for-transformer-based-seq2bf to\nthe original JNC to obtain the split dataset.\n3https://taku910.github.io/mecab/\n4https://github.com/rsennrich/\nsubword-nmt\ntokens from the articles and the headlines, respec-\ntively.\nWe used context word sequences extracted from\nthe reference headlines by GiNZA5 as the â€˜givenâ€™\nphrase.6 An average of 4.99 phrases was ex-\ntracted from the reference headlines, and the â€˜givenâ€™\nphrases consisted of an average of 2.32 tokens. We\nevaluated our methods using precision, recall, and\nF-score of ROUGE-1/2/L (Lin, 2004) and success\nrate (SR), which is the percentage of the headline\nthat includes the given phrase. We also calculated\nthe Average Length Difference (ALD) to analyze\nthe length of the generated headlines, as\nALD = 1\nn\nnâˆ‘\ni=1\nli âˆ’leni, (2)\nwhere n, li, and leni are the number of samples,\nthe length of the generated headline, and the length\nof the reference headline, respectively.\nAs a comparison method, we adopted the method\nproposed by He et al. (2020) with vanilla Trans-\nformer instead of BART (Lewis et al., 2020). This\nmethod controls the output by inserting the given\nphrase and the special token â€˜|â€™ in front of the in-\nput articles and randomly drops the given phrase\nfrom the input articles during training to improve\nthe performance. The hyperparameters of both the\ncomparison and our models are determined as de-\nscribed in Vaswani et al. (2017). The training was\nterminated when the perplexity computed on the\nvalidation set did not update three times in a row,\nand we used the model with the minimum perplex-\nity on the validation set. The beam size during the\ninference was set to three.\n5https://github.com/megagonlabs/ginza\n6We used â€œginza.bunsetu_phrase_spansâ€API.\n4088\nSR ROUGE-1 ROUGE-2 ROUGE-L ALD params\nP/R/F P/R/F P/R/F Ã—106\nTransformer (Vaswani et al., 2017) 36.3 57.1/48.9/51.4 29.8/25.2/26.5 47.1/40.9/42.8 â€“3.62 72\nTransformer version of He et al. (2020) 90.2 63.1/ 54.8/57.2 36.0/30.7/32.2 51.9/ 45.4/47.4 â€“3.02 72\n(Seq-B) 100.0 63.6/52.4/55.8 37.4/30.2/32.3 54.3/44.2/47.5 â€“4.19 80\nTransformer-based Seq2BF (Seq-F) 100.0 64.6/53.2/56.7 38.1/ 30.8/32.9 54.8/44.9/48.1 â€“4.30 80\n(Tok-B) 100.0 66.6/52.9/57.6 39.3/30.8/33.6 55.6/45.0/48.6 â€“5.29 80\n(Tok-F) 100.0 67.6 /51.6/57.1 40.1/30.2/33.5 56.7/44.2/48.6 â€“6.05 80\nTable 1: Experimental results. SR means success rate, and P/R/F means Precision/Recall/F-score.\n(b) Seq-F(a) Seq-B (d) Tok-F(c) Tok-B020004000600080001000012000140001600018000\n300510152025 300510152025 300510152025 300510152025\nFrequency\nFigure 3: Histogram of the character-level position of the given phrase in the headlines generated by Transformer-\nbased Seq2BF. Blue and orange bars indicate the generated and reference headlines, respectively.\n3.2 Results\nTable 1 shows the experimental results. Note\nthat the proposed and compared methods achieved\nhigher ROUGE scores than Transformer because\nwe computed ROUGE scores between the reference\nand the system-generated headlines, including the\nphrase extracted from the reference headlines.\nOur methods always include the given phrase\nin the generated headlines, whereas the compar-\nison method had a success rate of around 90%.\nAlthough the recall of ROUGE scores tended\nto be higher in the comparison method than in\nthe proposed method, the precision and F-scores\nof ROUGE scores in the proposed method were\ncomparable or higher than in the comparison\nmethod. As we notice from ALD, we found that\nTransformer-based Seq2BF generated shorter head-\nlines than the Transformer models. It has been con-\nï¬rmed that the Transformer models with a single\noutput direction tend to generate shorter headlines\nthan the reference. Because Transformer-based\nSeq2BF has two output directions, the generated\nheadlines were considered to be even shorter. This\nis the reason why our methods had lower recall\nscores than the comparison methods. Compar-\ning the generation strategies of Transformer-based\nSeq2BF, we can see that Tok-B/F had a higher score\nthan Seq-B/F.\nTo analyze how the four generation strategies\nof Transformer-based Seq2BF affected the system-\ngenerated headlines, we showed the character-level\nposition of the given phrase in the headline us-\ning histograms in Figure 3. As we can see, all\ngeneration strategies had similar distributions in\nthe reference and system-generated headlines, and\nhence Transformer-based Seq2BF has also been\npresumed to learn the position of a given phrase\nin the headline. Focusing on the headlines that in-\nclude the given phrase in the head, the difference\nbetween the reference and the headline generated\nby Tok-B/F is smaller than that of the headline\ngenerated by Seq-B/F. Also, the headlines gener-\nated by Seq-B tend to place the given phrase in the\nbeginning, while this tendency is opposite for the\nheadlines generated by Seq-F.\nTable 2 shows examples of the headlines gen-\nerated by the Transformer-based Seq2BF (Tok-B).\nWhen a product name such as â€œæ¡œã¨ã‚¤ãƒ¯ã‚·ã®ãƒ‘\nãƒ•ã‚§â€ (â€œCherry Blossom and Sardine Parfaitâ€) was\ngiven, our methods could generate a natural head-\nline that includes the given phrase. Also, given the\nphrase â€œ6æœˆæœ«â€ (â€œthe End of Juneâ€), our methods\ngenerated a headline with the addition of â€œ è²©å£²â€\n(â€œon Saleâ€) that matched the given phrase. On the\nother hand, we found the problem of generating the\nsame words related to the given phrase in the back-\nward and forward directions, such as the headline\ngenerated given â€œç¾¤ã‚Œâ€ (â€œSchoolsâ€). In addition,\ngiven the phrase â€œç´„1ä¸‡åŒ¹â€ (â€œAbout 10,000â€), our\nmethods generated the headline meaning that spe-\ncial sweets contain about 10,000 sardines. In this\nway, examples that were not faithful to the article\nwere conï¬rmed.\n4089\nArticle: ç´„1ä¸‡åŒ¹ã®ã‚¤ãƒ¯ã‚·ãŒç¾¤ã‚Œã§æ³³ãæ§˜å­ã‚’è¦‹ã‚‰ã‚Œã‚‹äº¬éƒ½æ°´æ—é¤¨ã€‚ã“ã®å±•ç¤ºã«åˆã‚ã›ã€ã¡ã‚‡ã£ã´ã‚Šå¤‰ã‚ã£ãŸç‰¹\nåˆ¥ã‚¹ã‚¤ãƒ¼ãƒ„ãŒ6æœˆæœ«ã¾ã§è²©å£²ã•ã‚Œã‚‹ã€‚åå‰ã¯ã€Œæ¡œã¨ã„ã‚ã—ã®ãƒ‘ãƒ•ã‚§ã€ã§ã€...\nAt the Kyoto Aquarium, you can see about 10,000 sardines swimming in schools. To coincide with this exhibition, a special\nsweet that is slightly unique will be on sale until the end of June. It is called â€œCherry Blossom and Sardine Parfait,â€ and ...\nReference Headline:ã€Œç›®ã‹ã‚‰ã‚¦ãƒ­ã‚³ã®ãŠã„ã—ã•ã€ äº¬éƒ½æ°´æ—é¤¨ã«ã‚¤ãƒ¯ã‚·ãƒ‘ãƒ•ã‚§\nâ€œScales Falling from Your Eyesâ€ â€“ Kyoto Aquarium Serves Sardine Parfait\næ¡œã¨ã„ã‚ã—ã®ãƒ‘ãƒ•ã‚§ ã€Œ æ¡œã¨ã„ã‚ã—ã®ãƒ‘ãƒ•ã‚§ã€ç‰¹åˆ¥ã‚¹ã‚¤ãƒ¼ãƒ„ äº¬éƒ½æ°´æ—é¤¨\nCherry Blossom and Special Sweets â€œCherry Blossom and Sardine Parfaitâ€\nSardine Parfait at Kyoto Aquarium\nGiven 6æœˆæœ« Generated ã‚¤ãƒ¯ã‚·ã®ç‰¹åˆ¥ã‚¹ã‚¤ãƒ¼ãƒ„ã€6æœˆæœ«ã¾ã§è²©å£² äº¬éƒ½æ°´æ—é¤¨\nPhrases the End of June Headlines Special Sardine Sweets on Sale at Kyoto Aquarium until the End of June\nç¾¤ã‚Œ ã‚¤ãƒ¯ã‚·ã® ç¾¤ã‚Œã®ã‚¤ãƒ¯ã‚·ã€ç‰¹åˆ¥ã‚¹ã‚¤ãƒ¼ãƒ„ã«äº¬éƒ½æ°´æ—é¤¨\nSchools Sardines of Schools of Sardines, to Special Sweets at Kyoto Aquarium\nç´„1ä¸‡åŒ¹ ã‚¤ãƒ¯ã‚·ç´„1ä¸‡åŒ¹ã®ç‰¹åˆ¥ã‚¹ã‚¤ãƒ¼ãƒ„äº¬éƒ½æ°´æ—é¤¨\nAbout 10,000 Special Sweets of About 10,000 Sardines at Kyoto Aquarium\nTable 2: Examples of headlines generated by Transformer-based Seq2BF (Tok-B).\nAs can be seen from Table 2, various headlines\nare generated according to given phrases. In gen-\neral, it is difï¬cult to control the diversity in headline\ngeneration, but our methods can generate diverse\nheadlines by giving a variety of phrases. However,\nit may be necessary to discuss whether our methods\ncould generate diverse headlines. The reason is that\nall examples are only partially diverse. Speciï¬cally,\nthey always include â€œ ç‰¹åˆ¥ã‚¹ã‚¤ãƒ¼ãƒ„â€ (â€œSpecial\nSweetsâ€) and â€œäº¬éƒ½æ°´æ—é¤¨â€ (â€œKyoto Aquariumâ€)\nas important contents in the headline.\n4 Conclusion\nWe proposed Transformer-based Seq2BF that gen-\nerates the lexically constrained headline by devis-\ning the attention mask for the decoder and gener-\nating backward and forward sentences from the\nphrase. Our experiments using the JNC demon-\nstrated that Transformer-based Seq2BF always in-\ncludes the given phrase in the generated headline\nand obtains comparable performance compared\nto previous Transformer-based methods. We also\nshowed that strategies of generating each token\nalternately between backward and forward direc-\ntions are more effective than that of generating a\nsequence in one direction and then a sequence in\nanother direction.\nIn future work, we will investigate whether\nTransformer-based Seq2BF can generate natural\nheadlines even when given a variety of phrases,\nsuch as phrases not in the reference or the article,\nand examine if our methods can creatively generate\ndiverse headlines by giving a variety of phrases\nquantitatively. Also, we will explore methods for\ngenerating headlines that include multiple phrases.\nReferences\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2017. Guided open vocabulary im-\nage captioning with constrained beam search. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2017), pages 936â€“945.\nDzmitry Bahdanau, Kyung Hyun Cho, and Yoshua\nBengio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of the\n3rd International Conference on Learning Represen-\ntations (ICLR 2015).\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. 2015. Scheduled sampling for se-\nquence prediction with recurrent neural networks.\nIn Proceedings of the 28th International Conference\non Neural Information Processing (NIPS 2015) ,\npages 1171â€“1179.\nSumit Chopra, Michael Auli, and Alexander M. Rush.\n2016. Abstractive sentence summarization with at-\ntentive recurrent neural networks. In Proceedings of\nthe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguis-\ntics: Human Language Technologies (NAACL-HLT\n2016), pages 93â€“98.\nPhilip Gage. 1994. A new algorithm for data compres-\nsion. C Users Journal, 12(2):23â€“38.\nJunxian He, Wojciech Kry Â´sciÂ´nski, Bryan McCann,\nNazneen Rajani, and Caiming Xiong. 2020. CTRL-\nsum: Towards generic controllable text summariza-\ntion. arXiv preprint arXiv:2012.04281.\nYuta Hitomi, Yuya Taguchi, Hideaki Tamori,\nKo Kikuta, Jiro Nishitoba, Naoaki Okazaki,\nKentaro Inui, and Manabu Okumura. 2019. A\nlarge-scale multi-length headline corpus for analyz-\ning length-constrained headline generation model\nevaluation. In Proceedings of the 12th International\nConference on Natural Language Generation (INLG\n2019), pages 333â€“343.\n4090\nChris Hokamp and Qun Liu. 2017. Lexically con-\nstrained decoding for sequence generation using grid\nbeam search. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL 2017), pages 1535â€“1546.\nTaku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.\n2004. Applying conditional random ï¬elds to\nJapanese morphological analysis. In Proceedings of\nthe 2004 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP 2004), pages\n230â€“237.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL 2020), pages 7871â€“7880.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Proceedings of\nthe Workshop on Text Summarization Branches Out,\npages 74â€“81.\nDayiheng Liu, Jie Fu, Qian Qu, and Jiancheng\nLv. 2019. BFGAN: Backward and forward\ngenerative adversarial networks for lexically con-\nstrained sentence generation. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing,\n27(12):2350â€“2361.\nLili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang,\nand Zhi Jin. 2016. Sequence to backward and for-\nward sequences: A content-introducing approach\nto generative short-text conversation. In Proceed-\nings of the 26th International Conference on Com-\nputational Linguistics: Technical Papers (COLING\n2016), pages 3349â€“3358.\nLili Mou, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin.\n2015. Backward and forward language modeling\nfor constrained sentence generation. arXiv preprint\narXiv:1512.06612.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nCaglar Gulcehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of the 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning (CoNLL 2016), pages 280â€“290.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018. A deep reinforced model for abstractive\nsummarization. In Proceedings of the 6th Inter-\nnational Conference on Learning Representations\n(ICLR 2018).\nMatt Post and David Vilar. 2018. Fast lexically con-\nstrained decoding with dynamic beam allocation for\nneural machine translation. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies (NAACL-HLT 2018),\npages 1314â€“1324.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP 2015), pages 379â€“389.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems (NIPS 2017), pages 5998â€“6008.",
  "topic": "Headline",
  "concepts": [
    {
      "name": "Headline",
      "score": 0.9791486263275146
    },
    {
      "name": "Computer science",
      "score": 0.7468481063842773
    },
    {
      "name": "Phrase",
      "score": 0.7404572367668152
    },
    {
      "name": "Transformer",
      "score": 0.6854051351547241
    },
    {
      "name": "Encoder",
      "score": 0.5249626040458679
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5127993226051331
    },
    {
      "name": "Natural language processing",
      "score": 0.4310793876647949
    },
    {
      "name": "Engineering",
      "score": 0.12346452474594116
    },
    {
      "name": "Linguistics",
      "score": 0.10769662261009216
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60134161",
      "name": "Nagoya University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210099567",
      "name": "Insight (China)",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4210105160",
      "name": "Asahi Shimbun Company (Japan)",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I114531698",
      "name": "Tokyo Institute of Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210126580",
      "name": "RIKEN Center for Advanced Intelligence Project",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I201537933",
      "name": "Tohoku University",
      "country": "JP"
    }
  ]
}