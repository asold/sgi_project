{
  "title": "Harnessing Large Language Models and Stochastic Programming for Optimized Plant Breeding Strategies",
  "url": "https://openalex.org/W4406340763",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2479458615",
      "name": "Yuqun Zhou",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A5114921359",
      "name": "Zuen Cen",
      "affiliations": [
        "Northern Arizona University"
      ]
    },
    {
      "id": "https://openalex.org/A2479458615",
      "name": "Yuqun Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114921359",
      "name": "Zuen Cen",
      "affiliations": [
        "Northern Arizona University",
        "University of Wisconsin–Madison"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4405413465",
    "https://openalex.org/W6871080141",
    "https://openalex.org/W4405413477",
    "https://openalex.org/W4404886474",
    "https://openalex.org/W6912003235",
    "https://openalex.org/W4404881448",
    "https://openalex.org/W4405448635",
    "https://openalex.org/W6892959096",
    "https://openalex.org/W6893076642",
    "https://openalex.org/W6948730051",
    "https://openalex.org/W6968120519",
    "https://openalex.org/W2949474492",
    "https://openalex.org/W6912006108",
    "https://openalex.org/W6893034028",
    "https://openalex.org/W6892855892",
    "https://openalex.org/W6930431821",
    "https://openalex.org/W4396520619",
    "https://openalex.org/W4403797784",
    "https://openalex.org/W4388191365",
    "https://openalex.org/W6861020000",
    "https://openalex.org/W4385226914",
    "https://openalex.org/W4391296692",
    "https://openalex.org/W4385326446",
    "https://openalex.org/W2990825308",
    "https://openalex.org/W4404573629",
    "https://openalex.org/W4391958341",
    "https://openalex.org/W4401383425",
    "https://openalex.org/W4390917634",
    "https://openalex.org/W4403814310"
  ],
  "abstract": "The convergence of Generative AI (GenAI) and stochastic programming introduces unprecedented opportunities for optimizing plant breeding strategies under uncertainty. This paper presents a hybrid framework that integrates Large Language Models (LLMs) with stochastic programming to enhance decision-making in crop improvement. LLMs are employed to analyze vast datasets, generate insights on genotype-environment interactions, and simulate breeding scenarios, while stochastic programming optimizes the selection of genotypes for maximum yield and resilience. Case studies demonstrate the effectiveness of this approach in addressing challenges such as climate variability and evolving market demands, offering a transformative solution for sustainable agriculture.",
  "full_text": "Academic Journal of Natural Science \nJournal Home: http://ajns.suaspress.org | CODEN: AJNSAE \nVol. 2, No. 1, 2025 | ISSN 3078-5170 (Print) | ISSN 3078-5189 (Online) | ISSN 3078-5197 (Digital)   \nPublished By SOUTHERN UNITED ACADEMY OF SCIENCES LIMITED  12 \nCopyright ©   2025 The author retains copyright and grants the journal the right of first publication.  \nThis work is licensed under a Creative Commons Attribution 4.0 International License.  \nHarnessing Large Language Models and Stochastic \nProgramming for Optimized Plant Breeding Strategies \nZHOU, Yuqun 1*  CEN, Zuen 2   \n1 University of Wisconsin-Madison, USA \n2 Northern Arizona University, USA \n \n* ZHOU, Yuqun is the corresponding author, E-mail: yzhou364@wisc.edu  \n \n \nAbstract: The convergence of Generative AI (GenAI) and stochastic programming introduces unprecedented opportunities for \noptimizing plant breeding strategies under uncertainty. This paper presents a hybrid framework that integrates Large Language  \nModels (LLMs) with stochastic programming to enhance decision-making in crop improvement. LLMs are employed to \nanalyze vast datasets, generate insights on genotype-environment interactions, and simulate breeding scenarios, while \nstochastic programming optimizes the selection of genotypes for maximum yield and resilience. Case studies demonstrate the \neffectiveness of this approach in addressing challenges such as climate variability and evolving market demands, offering a \ntransformative solution for sustainable agriculture.  \nKeywords:  Large Language Models, Stochastic Programming, Plant Breeding, Optimization Strategies, Genetic \nImprovement, Crop Yield Prediction, Predictive Analytics, Decision Support Systems, Agricultural Technology, Data-driven \nModeling. \nDisciplines: Biological Sciences.    Subjects: Genetics.  \n \nDOI: https://doi.org/10.70393/616a6e73.323632 ARK: https://n2t.net/ark:/40704/AJNS.v2n1a03 \n \n \n1 INTRODUCTION \nPlant breeding faces growing challenges in meeting \nglobal food security demands due to uncertainties in \nenvironmental conditions, genetic traits, and  market \ndynamics. Traditional optimization methods often fail to \ncapture the complexity and variability inherent in breeding \nsystems. Recent advances in  Generative AI (GenAI) and \nLarge Language Models (LLMs) offer novel capabilities for \nsynthesizing knowledge from diverse datasets, predicting \noutcomes, and generating innovative strategies. This paper \nproposes a hybrid framework that combines LLM-driven data \nanalysis with stochastic  programming to address these \nchallenges. The LLMs are used to analyze scientific literature, \nexperimental data, and environmental trends, providing \ninsights into genotypeenvironment interactions and guiding \nscenario generation. These scenarios are then fed into a \nstochastic programming model, which optimizes breeding \ndecisions to maximize yield, resilience, and profitability.  \n2 SHORTLIST OF MACHINE-\nLEARNING APPLICATIONS FOR \nCROP IMPROVEMENT AND \nPRODUCTION \nWith emerging new technologies and approaches, large \ndatasets are generated from different agricultural domains, \nparticularly from the crop production domain [1]. These vast \ndatasets can easily feed into machine -learning approaches to \nhelp all beneficiaries optimize crop improvement systems. \nEven though machine -learning applications are extensive, \ntheir subcategories, mainly in crop quality , crop phenotyping, \ncrop weed identification , disease detection, crop recognition, \ncrop-related microbiome improvements, and yield prediction, \nwere separated into crop development, production, and \nimprovement[2-4]. \n3 ESSENTIAL CONCEPTS \nWe discuss several fundamental ideas in machine \nlearning and, whenever possible, present examples from \nagricultural literature to clarify these concepts[5]. \n3.1 BASIC TERMS IN MACHINE LEARNING \nA dataset consists of several instances, or data points, \nthat are conceptualized as individual experimental \nobservations. Several fixed features describe each data \npoint[6-9]. Phenotype, genotype (SNPs), product price, and \nclimatic parameters are a few examples of these features[10]. \nWhatever we aim to do with a machine -learning model is \nspecified objectively by a machinelearning task. For instance, \nwe could predict the rate of price fluctuation at a particular \nAcademic Journal of Natural Science \nJournal Home: http://ajns.suaspress.org | CODEN: AJNSAE \nVol. 2, No. 1, 2025 | ISSN 3078-5170 (Print) | ISSN 3078-5189 (Online) | ISSN 3078-5197 (Digital)   \nPublished By SOUTHERN UNITED ACADEMY OF SCIENCES LIMITED  13 \nCopyright ©   2025 The author retains copyright and grants the journal the right of first publication.  \nThis work is licensed under a Creative Commons Attribution 4.0 International License.  \npoint in time for a specific agricultural product with an \nexperiment examining the cost of the crop product over \ntime[11-16]. In this instance, the features “cost of crop \nproduct” and “time” could be referred to as input features. \nThe conversion rate, which would represent the anticipated \noutput of the target model at a specific moment, is the \nquantity we are interested in fore casting. Input and output \nfeatures of a model can be as many as desired. Features could \nbe either categorical (accepting just discre te values) or \ncontinuous (continuous numerical values are used). \nTechnically, categorical features are usually binary in nature, \nmeaning they can be 1 (true) or 0 (false)[17]. \n \nFIGURE 1. THIS SCHEMATIC ILLUSTRATES KEY \nAPPLICATIONS OF ARTIFICIAL INTELLIGENCE AND \nMACHINE LEARNING FOR CROP DEVELOPMENT AND \nIMPROVEMENT, INCLUDING CROP DISEASES, CROP \nQUALITY, CROP SPECIES RECOGNITION, CROP \nDEVELOPMENT, CROP YIELD PREDICTION, CROP-RELATED \nMICROBIOME IMPROVEMENT, WATER MANAGEMENT, SOIL \nMANAGEMENT, ETC.  \nFarmers and researchers still encounter numerous \nobstacles due to employing traditional methods in the crop \nsector. Artificial intelligence and machine learning are used \nextensively to address these issues. Also, this figure shows \npossible data types and c ollection zones from crop fields to \nfeed different machine -learning models to improve and \ndevelop different crops. \n3.2 CONCEPT OF SUPERVISED, UNSUPERVISED, \nSEMI-SUPERVISED, AND REINFORCEMENT \nLEARNING \nSupervised machine learning describes how a model can \nbe fitted to data or part of target data that distinct labels have \nreceived for which a ground truth attribute exists; this quality \nis often determined by experimentation, researchers, or data \ncollectors. In contrast to knowledge derived from inference, \nground truth is information verified via direct observation and \nmeasurement, thus known to be accurate or real. Among the \nexamples are high -yield prediction and water quality \nprediction using supervised l earning for crop improvement. \nLaboratory or experimental observations ultimately serve as \nthe source of ground truth in both cases. Contrary to \nsupervised learning, patterns in unlabeled data can be found \nusing unsupervised -learning techniques This approach does \nnot require predetermin ed labels with ground truth \ninformation [18-20]. For example, plant image data can be \nanalyzed using an unsupervised machinelearning technique. \nSemisupervised learning, in which a significant quantity of \nunlabeled data is paired with tiny quantities of labeled data, \noccasionally combines the two methodologies ; for example, \nweed distribution and density estimation . When obtaining \ntagged or labeled data is expe nsive, this can dramatically \nenhance performance. Another component of machine \nlearning known as reinforcement learning (RL) teaches an \nagent how to behave and react in a given environment by \nhaving it carry out specific tasks and then watching the \nrewards or outcomes. This technique is already employed in \ndifferent agricultural domains, such as crop yield prediction \nand a completely autonomous precision agricultural aerial \nscouting technique [21-24]. \n4 CONVENTIONAL MACHINE \nLEARNING \nThis section investigates several essential and \ntraditional machine -learning techniques, focusing on their \nadvantages and disadvantages , presents a comparison of \nseveral machinelearning techniques along with some \napplications for crop improvement and production. Figure 4 \nillustrates a few of the conventional machine -learning \ntechniques[25]. To train these models, several software \nprograms have been available, such as Caret in R, MLJ in \nJulia , and scikit-learn in Python. When developing machine-\nlearning alg orithms for crop improvement -related data, \nconventional machine learning is typically the first area to \ninvestigate to find the most appropriate solution for a given \nproblem. Deep learning is currently prevalent and has the \npotential to be a robust and val uable method. It is still \nrestricted to the application domains where it performs well, \nthough, such as when a vast quantity of data are accessible, \nsuch as extreme data points, when there are several features \non each data point or when the features have a lot of structure . \nDrone images from crop fields and genotypic data (SNPs) are \ntwo examples of agricultural data for which deep learning \ncould be effectively used. Even when the other two \nconditions are satisfied, deep learning may not be the best \noption because of the need for vast volumes of data. \nTechnically, conventional approaches build and evaluate \nsolutions for a particular problem far more quickly than deep \nlearning. When compared to more conventional models such \nas random forests and support vecto r machines (SVMs) , \ncreating the architecture and training a deep neural network \nmight be a computation -intensive and costly process . For a \ngiven agricultural prediction problem, even if deep learning \nseems theoretically doable, it is usually wise to trai n a \nconventional technique and evaluate it against a model based \non neural networks such as ANN (artificial neural network), \n\nAcademic Journal of Natural Science \nJournal Home: http://ajns.suaspress.org | CODEN: AJNSAE \nVol. 2, No. 1, 2025 | ISSN 3078-5170 (Print) | ISSN 3078-5189 (Online) | ISSN 3078-5197 (Digital)   \nPublished By SOUTHERN UNITED ACADEMY OF SCIENCES LIMITED  14 \nCopyright ©   2025 The author retains copyright and grants the journal the right of first publication.  \nThis work is licensed under a Creative Commons Attribution 4.0 International License.  \nif at all possible. Conventional approaches usually assume \nthat every sample in the collection has the same number of \ncharacteristics, which is not always feasible. Using SNP data \nwith varying lengths for each case is a clear illustration of this \nproblem. The data can be adjusted using basic techniques \nsuch as windowing and padding to make them all the same \nsize and employing standard ways with them. Padding refers \nto the process that can add zero value to each example up to \nmaking the size of each of them equal to the most prominent \nexample in the target dataset. Conversely, the windowing \napproach condenses each sample to a specific size[26]. \n4.1 APPLICATION OF REGRESSION AND \nCLASSIFICATION MODELS \nRegarding regression problems such as those depicted \nin Figure 4A, ridge regression (a type of linear regression) is \nfrequently a valuable place to start when building and \ndeveloping a model since it could offer a quick and clear \nbaseline for a particular responsibility. The value of one \nvariable can be predicted by using linear regression analysis \naccording to the value of another variable. On the other hand, \nwhen a model relies on as few features as possible from the \ngiven data, then other variations of l inear regression, such as \nelastic net regression and LASSO regression , are also worthy \nof consideration. Since the correlations between the \ncharacteristics in the data are frequently non -linear, using a \nmodel such as an SVM is usually a better option in t hese \nsituations , as shown in Figure 4B. SVMs are a practical kind \nof classification and regression model that convert non -\nseparable problems into easier -to-solve separable problems \nby using kernel functions. A kernel function is a technique for \ntransforming input data into the format needed for data \nprocessing. Both non -linear (a statistical method called \nnonlinear regression is used to model non-linear relationships \nbetween independent and dependent variables) and linear \nregression could be carried out wi th SVMs based on the \nkernel function that was applied. To quantify, the best idea is \nto train an SVM through a kernel of a radial basis function \nand a linear SVM can be used from a nonlinear model, if any. \nNumerous models that are often employed in regress ion \ncould be used in classification as well. Another acceptable \ndefault starting point for a classification problem is to train an \nSVM based on the kernel function and a linear SVM. k -\nnearest neighbors classification (also known as k-NN or KNN) \nis a furthe r technique that could be used. A non -parametric \nsupervisedlearning classifier, the k-nearest neighbors method \nemploys closeness to classify or anticipate how a single data \npoint will be grouped. XGBoost (Figure 4C) are examples of \nensemble-based models, w hich provide another family of \nresilient non-linear techniques. These techniques are effective \nnonlinear models offering feature significance estimations \nand frequently just need minor adjustments to the \nhyperparameters. There are often an overwhelming num ber \nof variations among the several models available for \nregression and classification. It can be misleading to try to \nforecast how well-suited a specific method will be to a given \nissue in advance; instead, it is usually wiser to use an \nempirical approach to identify the optimum model via trial -\nand-error methods. Swapping out these model versions often \ninvolves only one line of code change thanks to a novel and \nrobust machine -learning library such as scikitlearn , which \ncan efficiently run in a Python envi ronment. To find the best \napproach overall, it is an excellent strategy to optimize and \ntrain several of the previously described techniques, and then \ncompare the results on a different test set to see which method \nperformed the best on the validation set. \n \nFIGURE 2. MACHINE LEARNING METHODS IN \nAGRICULTURAL DATA ANALYSIS \n (A) Regression is the link between a single and/or \nseveral independent variables, also known as features, and a \ndependent variable (the observable attribute) is determined by \nusing regression. A straightforward example is the prediction \nof crop yield based on one or some of the phenotypic features.  \n(B) SVM: a support vector machine divides the original \ninput data into several categories by creating a gap as large as \nfeasible between the data in each converted version. One \nexample is a prediction of whether a variety of a specific crop \nis a low- or high-yield variety.  \n(C) Gradient boosting makes predictions by combining \nseveral weak prediction models, most often decision trees; for \nexample, the prediction of sugarcane yield grade.  \n(D) Clustering: using one of several algorithms, based \non related objects; for example, better energy use in crop \nproduction.  \n(E) t -distributed stochastic neighbor embedding (t -\nSNE), for example, dimensionality reduction of crop \ngenotypic (SNP) data. \n4.2 APPLICATION OF CLUSTERING MODELS \nLike many other clustering algorithms (Figure 4D), k -\nmeans is a powerful multi -purpose clustering technique that \nrequires the number of clusters to be specified as a \nhyperparameter . An alternate method that is not necessary \nfor a predetermined number of clusters is DBSCAN[27]. For \ndatasets with plenty of features, dimensionality reduction can \nalso be done prior to clustering to enhance performance.  \n\nAcademic Journal of Natural Science \nJournal Home: http://ajns.suaspress.org | CODEN: AJNSAE \nVol. 2, No. 1, 2025 | ISSN 3078-5170 (Print) | ISSN 3078-5189 (Online) | ISSN 3078-5197 (Digital)   \nPublished By SOUTHERN UNITED ACADEMY OF SCIENCES LIMITED  15 \nCopyright ©   2025 The author retains copyright and grants the journal the right of first publication.  \nThis work is licensed under a Creative Commons Attribution 4.0 International License.  \n4.3 DIMENSIONALITY REDUCTION \nHigh-dimensional data can be transformed into a \nlowerdimensional format while preserving the different \nconnections and interactions between the data points and \npieces using dimensionality reduction techniques. Although \nmore dimensions could be used in mach ine learning, two or \nthree dimensions are often selected to enable data \nvisualization on several axes. These methods include data \ntransformations that are both linear and nonlinear. Principal \ncomponent analysis (PCA) and t -distributed stochastic \nneighbor e mbedding (t -SNE) are some of the examples \ncommon in the agriculture domain for dimensionality \nreduction. The circumstance determines which technique to \napply. PCA is based on a linear combination of input features; \neach component preserves the global conne ctions between \nthe data points and could be explainable, implying that it is \nsimple to identify the characteristics that contribute to data \ndiversity. t -SNE is a versatile technique that can uncover \nstructure in complicated datasets and more robustly maintain \nlocal links between data points. Concept of artificial neural \nnetworks The mathematical principle of artificial neural \nnetworks (ANN) has been conceptualized by following and \nunderstanding the behaviors and connectivity of human \nneurons in the human br ain. It was created initially to study \nthe workings of the brain. The significant advances in deep \nneural network training and architecture over the past few \ndecades have increased interest in neural network models. \nThe following section covers the fundame ntals of neural \nnetworks and common varieties used in research on crop \nimprovement. Figure 5 displays some of these concepts [28]. \nConcept of neural network fundamentals The capacity of \nneural networks to approximate functions universally is one \nof their primary characteristics; this implies that, with \nminimal presumptions, any mathematical function can be \naccurately approximated to  any degree by a neural network \nthat is set up appropriately. The fundamental units of every \nneural network model are artificial  neurons. A mathematical \nfunction that translates (converts) inputs to outputs in a \ncertain way constitutes an artificial neuron. Any number of \ninput values can be fed into a single artificial neuron, which \nthen uses a predetermined mathematical function t o produce \nan output value. Artificial neurons are layered and the output \nof one layer is the input of the next, which forms a network. \nIn the following subsections, we present several methods for \nconfiguring artificial neurons, sometimes called neural \nnetwork architectures. Combining several architectural styles \nis also popular. For instance, fully linked layers are typically \nused to provide the final classification output in a CNN \n(convolutional neural network) used for classification. \n5 CONCLUSIONS \nDisplay significantly greater use of AI and ML \napproaches in crop science, which could open a new horizon \nfor integrated and valuable solutions in this area. We have \nundertaken a thorough review of the essential elements, \nconcepts, applications, and machine -learning definitions \nrequired for agri-crop improvement. Nowadays, crop science \nis leveraging tons of available data to obtain deeper insights \nthrough AI and ML and offer the best suggestions for \nfollowing actions and de cisions for enhancing crop \nproductivity or for other necessary tasks. Crop improvement \nand forecasting are made more accessible by combining \ncomputer science and agriculture. Offering broad \nrecommendations and guidance for machine learning in \nagriculture is challenging because of the diver sity of \nagricultural data. Therefore, our article aimed to provide \nagricultural and crop science researchers with an overview of \nthe many accessible approaches, as well as some suggestions \nfor conducting efficient machine learning through available \ndata. I t is vital to recognize that machine learning is \ninappropriate for all problems and to know when to avoid it: \nwhen the available data are insufficient, when it is necessary \nto comprehend rather than anticipate, or when it is not \napparent how to fairly eval uate performance. Also, here we \nhighlighted the application of federated learning in \nagriculture along with the definition, procedures, and \nstructure, which can be beneficial for researchers in the \nagricultural sector. Even though there has been huge progress \nin machine learning in agriculture, many challenges still need \nto be addressed to mark ML territory in agricultural science. \nThere is no denying that machine learning has influenced and \nwill continue to influence agricultural research significantly. \n \nACKNOWLEDGMENTS \nThe authors thank the editor and anonymous reviewers \nfor their helpful comments and valuable suggestions. \nFUNDING \nNot applicable. \nINSTITUTIONAL REVIEW BOARD \nSTATEMENT \nNot applicable. \nINFORMED CONSENT STATEMENT \nNot applicable. \nDATA AVAILABILITY STATEMENT \nThe original contributions presented in the study are \nincluded in the article/supplementary material, further \ninquiries can be directed to the corresponding author. \nCONFLICT OF INTEREST \nThe authors declare that the research was conducted in \nAcademic Journal of Natural Science \nJournal Home: http://ajns.suaspress.org | CODEN: AJNSAE \nVol. 2, No. 1, 2025 | ISSN 3078-5170 (Print) | ISSN 3078-5189 (Online) | ISSN 3078-5197 (Digital)   \nPublished By SOUTHERN UNITED ACADEMY OF SCIENCES LIMITED  16 \nCopyright ©   2025 The author retains copyright and grants the journal the right of first publication.  \nThis work is licensed under a Creative Commons Attribution 4.0 International License.  \nthe absence of any commercial or financial relationships that \ncould be construed as a potential conflict of interest. \nPUBLISHER'S NOTE \nAll claims expressed in this article are solely those of \nthe authors and do not necessarily represent those of their \naffiliated organizations, or those of the publisher, the editors \nand the reviewers. Any product that may be evaluated in this \narticle, or claim that may be made by its manufacturer, is not \nguaranteed or endorsed by the publisher. \nAUTHOR CONTRIBUTIONS \nNot applicable. \nABOUT THE AUTHORS \nZHOU, Yuqun \nUniversity of Wisconsin-Madison, USA. \nCEN, Zuen \nComputer Information Technology, Northern Arizona \nUniversity, Flagstaff, AZ, USA. \n \n \nREFERENCES \n[1] Cen, Z., & Zhao, Y. (2024). Investigating the Impact of \nAI-Driven Voice Assistants on User Productivity and \nSatisfaction in Smart Homes. Journal of Economic \nTheory and Business Management, 1(6), 8-14. \n[2] Farooq, M. A., Gao, S., Hassan, M. A., Huang, Z., \nRasheed, A., Hearne, S., ... & Li, H. (2024). Artificial \nintelligence in plant breeding. Trends in Genetics. \n[3] Cen, Z., & Zhao, Y. (2024). Enhancing User Engagement \nthrough Adaptive Interfaces: A Study on Real -time \nPersonalization in Web Applications. Journal of \nEconomic Theory and Business Management, 1(6), 1-7. \n[4] Zhao, Y., & Cen, Z. (2024). Exploring Multimodal \nFeedback Mechanisms for Improving User Interaction in \nVirtual Reality Environments. Journal of Industrial \nEngineering and Applied Science, 2(6), 35-41. \n[5] Lin, W., Xiao, J., & Cen, Z. (2024). Exploring Bias in \nNLP Models: Analyzing the Impact of Training Data on \nFairness and Equity. Journal of Industrial Engineering \nand Applied Science, 2(5), 24-28. \n[6] Zhao, Y., & Wu, J. (2024). Enhancing User Engagement \nand Behavior Change in Healthy Eating Apps: A Human-\nComputer Interaction Perspective. Journal of Industrial \nEngineering and Applied Science, 2(6), 27-34. \n[7] Wó jcik-Gront, E., Zieniuk, B., & Pawełkowicz, M. (2024). \nHarnessing AI -Powered Genomic Research for \nSustainable Crop Improvement. Agriculture, 14(12), \n2299. \n[8] Xiao, J., Zhang, B., Zhao, Y., Wu, J., & Qu, P. (2024). \nApplication of Large Language Models in Personalized \nAdvertising Recommendation Systems. Journal of \nIndustrial Engineering and Applied Science, 2(4), 132 -\n142. \n[9] Zhao, Y., Qu, P., Xiao, J., Wu, J., & Zhang, B. (2024). \nOptimizing Telehealth Services with LILM -Driven \nConversational Agents: An HCI Evaluation. Journal of \nIndustrial Engineering and Applied Science, 2(4), 122 -\n131. \n[10] Zhao, Y., Wu, J., Qu, P., Zhang, B., & Yan, H. (2024). \nAssessing User Trust in LLM -based Mental Health \nApplications: Perceptions of Reliability and Effectiveness. \nJournal of Computer Technology and Applied \nMathematics, 1(2), 19-26. \n[11] Wu, J., & Xiao, J. (2024). Application of Natural \nLanguage Processing in Network Security Log Analysis. \nJournal of Computer Technology and Applied \nMathematics, 1(3), 39-47. \n[12] Harfouche, A. L., Jacobson, D. A., Kainer, D., Romero, \nJ. C., Harfouche, A. H., Mugnozza, G. S., ... & Altman, \nA. (2019). Accelerating climate resilient plant breeding \nby applying next-generation artificial intelligence. Trends \nin biotechnology, 37(11), 1217-1235. \n[13] Xiao, J., & Wu, J. (2024). Transfer Learning for Cross -\nLanguage Natural Language Processing Models. Journal \nof Computer Technology and Applied Mathematics, 1(3), \n30-38. \n[14] Wu, J., Qu, P., Zhang, B., & Zhou, Z. (2024). Sentiment \nAnalysis in Social Media: Leveraging BERT for \nEnhanced Accuracy. Journal of Industrial Engineering \nand Applied Science, 2(4), 143-149. \n[15] Zhang, B., Yan, H., Wu, J., & Qu, P. (2024). Application \nof Semantic Analysis Technology in Natural Language \nProcessing. Journal of Computer Technology and \nApplied Mathematics, 1(2), 27-34. \n[16] Qu, P., Zhang, B., Wu, J., & Yan, H. (2024). Comparison \nof Text Classification Algorithms based on Deep \nLearning. Journal of Computer Technology and Applied \nMathematics, 1(2), 35-42. \n[17] Zhong, Y. N. (2024). Optimizing the Structural Design \nof Computing Units in Autonomous Driving Systems and \nElectric Vehicles to Enhance Overall Performance \nStability. International Journal of Advance in Applied \nScience Research, 3, 93-98. \n[18] Pandey, D. K., & Mishra, R. (2024). Towards \nsustainable agriculture: Harnessing AI for global food \nsecurity. Artificial Intelligence in Agriculture. \nAcademic Journal of Natural Science \nJournal Home: http://ajns.suaspress.org | CODEN: AJNSAE \nVol. 2, No. 1, 2025 | ISSN 3078-5170 (Print) | ISSN 3078-5189 (Online) | ISSN 3078-5197 (Digital)   \nPublished By SOUTHERN UNITED ACADEMY OF SCIENCES LIMITED  17 \nCopyright ©   2025 The author retains copyright and grants the journal the right of first publication.  \nThis work is licensed under a Creative Commons Attribution 4.0 International License.  \n[19] Zhong, Y. (2024). Enhancing the Heat Dissipation \nEfficiency of Computing Units Within Autonomous \nDriving Systems and Electric Vehicles. \n[20] Wang, X., Li, X., Wang, L., Ruan, T., & Li, P. (2024). \nAdaptive Cache Management for Complex Storage \nSystems Using CNN -LSTM-Based Spatiotemporal \nPrediction. arXiv preprint arXiv:2411.12161. \n[21] Mendoza-Revilla, J., Trop, E., Gonzalez, L., Roller, M., \nDalla-Torre, H., de Almeida, B. P., ... & Lopez, M. (2024). \nA foundational large language model for edible plant \ngenomes. Communications Biology, 7(1), 835. \n[22] Wang, L., Xu, Z., Stone, P., & Xiao, X. (2024). \nGrounded curriculum learning. arXiv preprint \narXiv:2409.19816. \n[23] Sun, Y., & Ortiz, J. (2024). Machine Learning -Driven \nPedestrian Recognition and Behavior Prediction for \nEnhancing Public Safety in Smart Cities. Journal of \nArtificial Intelligence and Information, 1, 51-57. \n[24] Stock, M., Pieters, O., De Swaef, T., & Wyffels, F. \n(2024). Plant science in the age of simulation intelligence. \nFrontiers in Plant Science, 14, 1299208. \n[25] Sinha, D., Maurya, A. K., Abdi, G., Majeed, M., \nAgarwal, R., Mukherjee, R., ... & Chen, J. T. (2023). \nIntegrated genomic selection for accelerating breeding \nprograms of climate-smart cereals. Genes, 14(7), 1484. \n[26] Gonzá lez-Rodrí guez, V. E., Izquierdo -Bueno, I., \nCantoral, J. M., Carbú , M., & Garrido, C. (2024). \nArtificial intelligence: A promising tool for application in \nphytopathology. Horticulturae, 10(3), 197. \n[27] Ferrã o, L. F. V., Dhakal, R., Dias, R., Tieman, D., \nWhitaker, V., Gore, M. A., ... & Resende Jr, M. F. (2023). \nMachine learning applications to improve flavor and \nnutritional content of horticultural crops through breeding \nand genetics. Current Opinio n in Biotechnology, 83, \n102968. \n[28] Parmley, K. A., Higgins, R. H., Ganapathysubramanian, \nB., Sarkar, S., & Singh, A. K. (2019). Machine learning \napproach for prescriptive plant breeding. Scientific \nreports, 9(1), 17132. \n ",
  "topic": "Transformative learning",
  "concepts": [
    {
      "name": "Transformative learning",
      "score": 0.7150230407714844
    },
    {
      "name": "Stochastic programming",
      "score": 0.5920162796974182
    },
    {
      "name": "Computer science",
      "score": 0.5722814202308655
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5698552131652832
    },
    {
      "name": "Psychological resilience",
      "score": 0.5108287334442139
    },
    {
      "name": "Resilience (materials science)",
      "score": 0.4858071804046631
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.44576916098594666
    },
    {
      "name": "Convergence (economics)",
      "score": 0.43016722798347473
    },
    {
      "name": "Agriculture",
      "score": 0.4103944003582001
    },
    {
      "name": "Mathematical optimization",
      "score": 0.3175496459007263
    },
    {
      "name": "Machine learning",
      "score": 0.2722998261451721
    },
    {
      "name": "Ecology",
      "score": 0.17958912253379822
    },
    {
      "name": "Economics",
      "score": 0.1600767970085144
    },
    {
      "name": "Mathematics",
      "score": 0.13149350881576538
    },
    {
      "name": "Biology",
      "score": 0.08561331033706665
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin–Madison",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I203172682",
      "name": "Northern Arizona University",
      "country": "US"
    }
  ]
}