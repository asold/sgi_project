{
    "title": "Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers",
    "url": "https://openalex.org/W4385572482",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5055075998",
            "name": "Israt Jahan",
            "affiliations": [
                "York University"
            ]
        },
        {
            "id": "https://openalex.org/A5069546450",
            "name": "Md Tahmid Rahman Laskar",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5039118504",
            "name": "Chun Peng",
            "affiliations": [
                "York University"
            ]
        },
        {
            "id": "https://openalex.org/A5000409439",
            "name": "Jimmy Xiangji Huang",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3101223450",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4223492536",
        "https://openalex.org/W4385570371",
        "https://openalex.org/W4321276803",
        "https://openalex.org/W2970482702",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W2127466325",
        "https://openalex.org/W4220967417",
        "https://openalex.org/W4389523957",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W4308163975",
        "https://openalex.org/W3166358520",
        "https://openalex.org/W2070983861",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2170189740",
        "https://openalex.org/W4288088047",
        "https://openalex.org/W2857028992",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2950161719",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4287888693",
        "https://openalex.org/W3090073303",
        "https://openalex.org/W2060816264",
        "https://openalex.org/W3029927342",
        "https://openalex.org/W4220732108",
        "https://openalex.org/W3202242582",
        "https://openalex.org/W2346452181",
        "https://openalex.org/W4284713203",
        "https://openalex.org/W2174775663",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3175432034",
        "https://openalex.org/W3175225269",
        "https://openalex.org/W4297253404",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W4319793302",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4223546521"
    ],
    "abstract": "ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT’s pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that lack large annotated data.",
    "full_text": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 326–336\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nEvaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison\nwith Fine-Tuned Generative Transformers\nIsrat Jahan†, $, Md Tahmid Rahman Laskar‡, $, §, Chun Peng†, Jimmy Xiangji Huang‡, $\n†Department of Biology, York University\n‡School of Information Technology, York University\n$Information Retrieval and Knowledge Management Research Lab, York University\n§Dialpad Canada Inc.\nToronto, Ontario, Canada\n{israt18,tahmid20,cpeng,jhuang}@yorku.ca\nAbstract\nChatGPT is a large language model devel-\noped by OpenAI1. Despite its impressive per-\nformance across various tasks, no prior work\nhas investigated its capability in the biomedical\ndomain yet. To this end, this paper aims to eval-\nuate the performance of ChatGPT on various\nbenchmark biomedical tasks, such as relation\nextraction, document classification, question\nanswering, and summarization. To the best\nof our knowledge, this is the first work that\nconducts an extensive evaluation of ChatGPT\nin the biomedical domain. Interestingly, we\nfind based on our evaluation that in biomedical\ndatasets that have smaller training sets, zero-\nshot ChatGPT even outperforms the state-of-\nthe-art fine-tuned generative transformer mod-\nels, such as BioGPT and BioBART. This sug-\ngests that ChatGPT’s pre-training on large text\ncorpora makes it quite specialized even in the\nbiomedical domain. Our findings demonstrate\nthat ChatGPT has the potential to be a valuable\ntool for various tasks in the biomedical domain\nthat lack large annotated data.\n1 Introduction\nThe rapid growth of language models (Rogers\net al., 2021; Zhou et al., 2023) in the field of Nat-\nural Language Processing (NLP) in recent years\nhas led to significant advancements in various do-\nmains, including the biomedical domain (Kalyan\net al., 2022). Although specialized models (e.g.,\nBioBERT (Lee et al., 2020), BioBART (Yuan\net al., 2022a), BioGPT (Luo et al., 2022), etc.)\nhave shown promising results in the biomedical\ndomain, they require fine-tuning using domain-\nspecific datasets. This fine-tuning process can be\ntime-consuming due to the requirement of task-\nspecific large annotated datasets. In contrast, zero-\nshot learning enables models to perform tasks\nwithout the need for fine-tuning on task-specific\ndatasets. ChatGPT, a large language model, has\n1https://openai.com/blog/chatgpt\ndemonstrated impressive zero-shot performance\nacross various tasks (Laskar et al., 2023). However,\nits performance in the biomedical domain remains\nto be thoroughly investigated. In this regard, this\npaper presents a comprehensive evaluation of Chat-\nGPT on four key biomedical tasks: relation extrac-\ntion, question answering, document classification,\nand summarization.\nIn this paper, our primary objective is to ex-\nplore the extent to which ChatGPT can perform\nthese tasks without fine-tuning and assess its per-\nformance by comparing with state-of-the-art gener-\native fine-tuned models, BioGPT and BioBART. To\nour best knowledge, this is the first work that eval-\nuated ChatGPT on benchmark biomedical datasets.\nOur evaluation of ChatGPT can have a profound im-\npact on the biomedical domain that lacks domain-\nspecific datasets by exploring its zero-shot learning\ncapabilities. To ensure the reproducibility of our\nevaluation and to help facilitate future research, we\nwill release all the ChatGPT-generated responses\nalong with our evaluation code here: https://\ngithub.com/tahmedge/chatgpt-eval-biomed.\n2 Related Work\nThe effective utilization of transformer-based\n(Vaswani et al., 2017) NLP models like BERT\n(Devlin et al., 2019) have also led to significant\nprogress in the biomedical domain (Lee et al., 2020;\nAlsentzer et al., 2019; Beltagy et al., 2019; Gu et al.,\n2020; Peng et al., 2019) in recent years. BERT\nleverages the encoder of the transformer architec-\nture, while GPT leverages the decoder of the trans-\nformer. In addition to these models, sequence-to-\nsequence models like BART (Lewis et al., 2019)\nthat leverage both the encoder and the decoder of\ntransformer have also emerged as a powerful ap-\nproach in various text generation tasks.\nIt has been observed that domain-specific pre-\ntraining of these models on the biomedical text\ncorpora followed by fine-tuning on task-specific\n326\nDataset Type Data Split\n(Train / Valid / Test)\nPrompt\nBC5CDR Chemical-Disease\nRelation Extraction\n500 / 500 / 500 Identify each pair of drugs and the drug-induced side-effects (e.g., diseases) in the following passage:\n[PASSAGE]\nKD-DTI Drug-Target\nRelation Extraction\n12K / 1K / 1.3K Identify the drug-target interactions in the following passage (along with the interaction type among the following: ’in-\nhibitor’, ’agonist’, ’modulator’, ’activator’, ’blocker’, ’inducer’, ’antagonist’, ’cleavage’, ’disruption’, ’intercalation’,\n’inactivator’, ’bind’, ’binder’, ’partial agonist’, ’cofactor’, ’substrate’, ’ligand’, ’chelator’, ’downregulator’, ’other’, ’anti-\nbody’, ’other/unknown’):\n[PASSAGE]\nDDI Drug-Drug\nRelation Extraction\n664 / 50 / 191 Identify the pairs of drug-drug interactions in the passage given below based on one of the following interaction types:\n(i) mechanism: this type is used to identify drug-drug interactions that are described by their pharmacokinetic mechanism.\n(ii) effect: this type is used to identify drug-drug interactions describing an effect.\n(iii) advice: this type is used when a recommendation or advice regarding a drug-drug interaction is given.\n(iv) int: this type is used when a drug-drug interaction appears in the text without providing any additional information.\n[PASSAGE]\nHoC Document\nClassification\n9972 / 4947 / 4947 The 10 hallmark cancer taxonomy with their definitions MeQSumgiven below:\n(i) Sustaining proliferative signaling: Cancer cells can initiate and maintain continuous cell division by producing their own\ngrowth factors or by altering the sensitivity of receptors to growth factors. (ii) Evading growth suppressors: Cancer cells can\nbypass the normal cellular mechanisms that limit cell division and growth, such as the inactivation of tumor suppressor genes.\n(iii) Resisting cell death: Cancer cells develop resistance to apoptosis, the programmed cell death process, which allows\nthem to survive and continue dividing. (iv) Enabling replicative immortality: Cancer cells can extend their ability to divide\nindefinitely by maintaining the length of telomeres, the protective end caps on chromosomes. (v) Inducing angiogenesis:\nCancer cells stimulate the growth of new blood vessels from existing vessels, providing the necessary nutrients and oxygen to\nsupport their rapid growth. (vi) Activating invasion and metastasis: Cancer cells can invade surrounding tissues and migrate\nto distant sites in the body, forming secondary tumors called metastases. (vii) Cellular energetics: Cancer cells rewire their\nmetabolism to support rapid cell division and growth, often relying more on glycolysis even in the presence of oxygen\n(a phenomenon known as the Warburg effect). (viii) Avoiding immune destruction: Cancer cells can avoid detection and\nelimination by the immune system through various mechanisms, such as downregulating cell surface markers or producing\nimmunosuppressive signals. (ix) Tumor promoting inflammation: Chronic inflammation can promote the development\nand progression of cancer by supplying growth factors, survival signals, and other molecules that facilitate cancer cell\nproliferation and survival. (x) Genome instability and mutation: Cancer cells exhibit increased genomic instability, leading\nto a higher mutation rate, which in turn drives the evolution of more aggressive and drug-resistant cancer cells.\nClassify the following sentence in one of the above 10 hallmark cancer taxonomy. If cannot be classified, answer as \"empty\":\n[SENTENCE]\nPubMedQA Question Answering 450 / 50 / 500 For the question, the reference context, and the answer given below, is it possible to infer the answer for that question from\nthe reference context? Only reply as either Yes or No or Maybe.\nQuestion: [QUESTION]\nReference context: [REFERENCE CONTEXT]\nAnswer: [ANSWER]\nTable 1: Datasets Details with our Prompts for Relation Extraction, Document Classification, and Question Answering Tasks.\nbiomedical datasets have helped these models to\nachieve state-of-the-art performance in a variety\nof BioNLP tasks (Gu et al., 2021). However, one\nmajor limitation of using such fine-tuned models\nis that they require task-specific large annotated\ndatasets, which is significantly less available in the\nBioNLP domain in comparison to the general NLP\ndomain. In this regard, having a strong zero-shot\nmodel could potentially alleviate the need for large\nannotated datasets, as it could enable the model to\nperform well on tasks that it was not trained on.\nRecently, large autoregressive language models\nlike GPT-3 (Brown et al., 2020) have demonstrated\nimpressive few-shot learning capability. More re-\ncently, a new variant of GPT-3, called the Instruct-\nGPT model (Ouyang et al., 2022) has been pro-\nposed that leverages the reinforcement learning\nfrom human feedback (RLHF) mechanism. The\nresulting InstructGPT models (in other words, GPT-\n3.5) are much better at following instructions than\nthe original GPT-3 model, resulting in an impres-\nsive zero-shot performance across various tasks.\nChatGPT, a very recent addition to the GPT-3.5\nseries, has been trained using dialog-based instruc-\ntional data alongside its regular training phase.\nThough ChatGPT has demonstrated strong zero-\nshot performance across various NLP tasks (Laskar\net al., 2023; Qin et al., 2023; Bang et al., 2023;\nYang et al., 2023), it is yet to be investigated in the\nbiomedical domain. To this end, this paper aims\nto evaluate the zero-shot capability of ChatGPT in\nthe biomedical domain.\n3 Our Methodology\nFor a given test sample X, we prepare a task in-\nstruction T and concatenate the text in the test sam-\nple with the task instruction to construct the prompt\nP. Then the prompt P is given as input to Chat-\nGPT (gpt-3.5-turbo) to generate the response R. In\nthis paper, we evaluate ChatGPT on 4 biomedical\ntasks across 11 benchmark datasets. Below, we\ndescribe these tasks, the datasets we use for evalua-\ntion, and the prompt P that we construct for each\ntask depending on the respective dataset.\n(i) Relation Extraction: Given a text sequence\nS, the biomedical relation extraction task aims to\nextract relations between entities mentioned in the\ntext by identifying all possible relation triplets. In\nthis paper, we evaluate drug-target-interaction in\nthe KD-DTI dataset (Hou et al., 2022), chemical-\ndisease-relation in the BC5CDR dataset (Li et al.,\n327\nDataset Type Data Split\n(Train / Valid / Test)\nPrompt\niCliniq Dialog\nSummarization\n24851 / 3105 / 3108 Write a very short and concise one line summary of the following dialogue as an informal question in a\nhealthcare forum:\n[DIALOGUE]\nHealthCare Magic Dialog\nSummarization\n181122 / 22641 / 22642 Write a very short and concise one line summary of the following dialogue as a question in a healthcare forum:\n[DIALOGUE]\nMeQSum Question\nSummarization\n500 / - / 500 Rewrite the following question in a short and concise form:\n[QUESTION]\nMEDIQA-QS Question\nSummarization\n- / 50 / 100 Rewrite the following question in a short and concise form:\n[QUESTION]\nMEDIQA-MAS Answer\nSummarization\n- / 50 / 80 For the following question, some relevant answers are given below. Please write down a short concise answer\nby summarizing the given answers.\nQuestion: [QUESTION]\nAnswer 1: [ANSWER1]\nAnswer 2: [ANSWER2]\nMEDIQA-ANS Answer\nSummarization\n- / - / 552 Write a very short and concise summary of the following article based on the question given below:\n[ARTICLE]\nTable 2: Datasets Details with our Prompts for Abstractive Summarization tasks.\n2016), and drug-drug-interaction in the DDI dataset\n(Herrero-Zazo et al., 2013). Our prompts for these\ndatasets are demonstrated in Table 1.\n(ii) Document Classification: Given a text doc-\nument S, the goal is to classify the type of the\ndocument. For this task, we use the HoC (the\nHallmarks of Cancers corpus) dataset (Baker et al.,\n2016) that consists of 1580 PubMed abstracts. This\ndataset was annotated at the sentence level by hu-\nman experts among ten currently known hallmarks\nof cancer. Our prompt is shown in Table 1.\n(iii) Question Answering: For the question-\nanswering task, we evaluate the performance of\nChatGPT on the PubMedQA dataset (Jin et al.,\n2019). Here, the objective is to determine whether\nthe answer to a given question can be inferred from\nthe reference context. We give the question, the\nreference context, and the answer as input to Chat-\nGPT to determine whether the answer to the given\nquestion can be inferred from the given reference\ncontext, with ChatGPT being prompted to reply\neither as yes, no, or maybe (see Table 1 for details).\n(iv) Abstractive Summarization: Given a text\nsequence S, the goal is to generate a concise ab-\nstractive summary of S. To this end, we evalu-\nate ChatGPT on various biomedical summariza-\ntion tasks, such as healthcare question summa-\nrization (we used MeQSum (Abacha and Demner-\nFushman, 2019) and MEDIQA-QS (Abacha et al.,\n2021) datasets), medical answer summarization\n(we used MEDIQA-ANS (Savery et al., 2020) and\nMEDIQA-MAS (Abacha et al., 2021) datasets), and\ndialogue summarization (we used the iCliniq and\nHealthCareMagic datasets (Zeng et al., 2020; Mrini\net al., 2021) for doctor-patient dialogue summariza-\ntion to generate short queries for healthcare forums\ndescribing patient’s medical conditions). We show\nour prompts for this task in Table 2.\n4 Experiments\nSince ChatGPT is a generative model, we consider\ntwo state-of-the-art generative transformers as our\nbaselines. Below, we first present these baselines,\nfollowed by presenting the results.\n4.1 Fine-tuned Baselines\nBioGPT: The backbone of BioGPT (Luo et al.,\n2022) is GPT-2 (Radford et al., 2019), which is a de-\ncoder of the transformer. The BioGPT model was\ntrained over PubMed titles and abstracts via lever-\naging the standard language modeling task. We\ncompare zero-shot ChatGPT with BioGPT models\nfine-tuned on relation extraction, document classi-\nfication, and question-answering tasks.\nBioBART: BioBART is a sequence-to-sequence\nmodel that was pre-trained over PubMed abstracts\n(Yuan et al., 2022a). The pre-training process in-\nvolves reconstructing corrupted input sequences.\nWe compare the zero-shot ChatGPT with BioBART\nfine-tuned on abstractive summarization datasets.\n4.2 Results & Discussion\nWe first compare the performance of ChatGPT with\nBioGPT on relation extraction, document classifi-\ncation, and the question-answering task (see Table\n3). Then we compare its performance with Bio-\nBART on summarization datasets (see Table 4).\nMore evaluation details are given in Appendix A.1.\nRelation Extraction Evaluation: We observe\nthat in the BC5CDR and KD-DTI datasets for re-\n328\nBC5CDR KD-DTI DDI HoC PubmedQA\nModel Precision Recall F1 Precision Recall F1 Precision Recall F1 F1 Accuracy\nChatGPT 36.20 73.10 48.42 19.19 66.02 29.74 47.11 45.77 46.43 59.14 51.60\nBioGPT (Luo et al., 2022) 49.52 43.25 46.17 40.00 39.72 38.42 41.70 44.75 40.76 85.12 78.20\nTable 3: Performance on Relation Extraction, Document Classification, and Question Answering datasets.\niCliniq HealthCareMagic MeQSum MEDIQA-QS MEDIQA-MAS MEDIQA-ANS\nModel R-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S R-1 R-2 R-L B-S\nChatGPT 30.5 12.8 25.4 89.3 28.1 9.8 2.4 88.9 30.0 12.3 26.2 89.0 30.6 11.6 26.7 89.0 38.9 14.6 22.1 87.9 28.7 10.4 24.4 89.0\nBioBART-Base 61.1 48.5 59.4 94.1 46.7 26.0 44.1 91.8 53.8 36.5 51.3 92.9 30.1 11.3 27.4 89.8 32.9 11.3 29.3 86.1 19.0 7.5 16.8 85.0\nBioBART-Large 60.3 48.0 58.7 94.0 46.5 26.1 44.2 91.9 55.6 38.1 53.2 93.3 32.0 12.4 29.7 90.3 30.6 10.4 27.0 86.1 21.6 9.3 19.2 85.7\nTable 4: Performance on Abstractive Summarization datasets. Here, ‘R-1’, ‘R-2’, ‘R-L’ and ‘B-S’ denote ‘ROUGE-1’,\n‘ROUGE-2’, ‘ROUGE-L’, and ‘BERTScore’, respectively. For BERTScore (Zhang et al., 2019), we use the RoBERTa-Large\nmodel (Liu et al., 2019). The results for BioBART models are taken from Yuan et al. (2022a).\nlation extraction, ChatGPT led to higher recall\nscores but much lower precision scores compared\nto the fine-tuned BioGPT model. This is because\nChatGPT tends to generate long and descriptive\nresponses, leading to many inaccurate relation ex-\ntractions. Though in terms of F1, it outperforms\nfine-tuned BioGPT in the BC5CDR dataset, it fails\nto outperform in the KD-DTI dataset. More impor-\ntantly, it outperforms BioGPT in the DDI dataset\nin all metrics: Precision, Recall, and F1.\nWhile analyzing the results in different datasets,\nwe observe that in both BC5CDR and DDI datasets\nwhere ChatGPT outperforms BioGPT, the training\nset is small, only 500 and 664 instances, respec-\ntively. On the other hand, in the KD-DTI dataset\nwhere ChatGPT fails to outperform BioGPT, the\ntraining set contains 12000 instances. This gives\nus a strong indication that even in the biomedical\ndomain, zero-shot ChatGPT can outperform fine-\ntuned biomedical models in smaller-sized datasets.\nWe also observe that more descriptive prompts\nmay help ChatGPT to obtain better Precision\nscores. Contrary to the KD-DTI dataset, we de-\nscribe the definition of each interaction type in the\nDDI dataset (see Table 1) where ChatGPT performs\nthe best. To further investigate the effect of prompts\nin relation extraction, we evaluate the performance\nin BC5CDR with a new prompt:\ni. Identify the chemical-disease interactions in the\npassage given below: [PASSAGE].\nWe observe that the Precision, Recall, and F1\nscores are decreased by 16.07%, 10.3%, and\n14.29%, respectively, with this prompt variation.\nDocument Classification Evaluation: We ob-\nserve that in the HoC dataset, the zero-shot Chat-\nGPT achieves an F1 score of 59.14, in compari-\nson to its counterpart fine-tuned BioGPT which\nachieves an F1 score of 85.12. We also investigate\nthe effect of prompt tuning by evaluating with two\nnew prompts that are less descriptive (see Appendix\nA.2 for more details):\ni. Prompting without explicitly mentioning the\nname of 10 HoC classes, drops F1 to 38.20.\nii. Prompting with the name of each HoC class is\ngiven without providing the definition of each class,\ndrops the F1 score to 46.93.\nQuestion Answering Evaluation: We observe\nthat in the PubMedQA dataset, the zero-shot Chat-\nGPT achieves much lower accuracy than BioGPT\n(51.60 by ChatGPT in comparison to 78.20 by\nBioGPT). However, the BioGPT model was fine-\ntuned on about 270K QA-pairs in various versions\nof the PubMedQA dataset for this task. While\nChatGPT achieves more than 50% accuracy even\nwithout any few-shot examples in the prompt.\nSummarization Evaluation: We observe that in\nterms of all ROUGE scores (Lin, 2004), ChatGPT\nperforms much worse than BioBART in datasets\nthat have dedicated training sets, such as iCliniq,\nHealthCareMagic, and MeQSum. Meanwhile, it\nperforms on par with BioBART in the MEDIQA-\nQS dataset. More importantly, it outperforms Bio-\nBART in both MEDIQA-ANS and MEDIQA-MAS\ndatasets. Note that MEDIQA-ANS, MEDIQA-\nMAS, and MEDIQA-QS datasets do not have any\ndedicated training data and ChatGPT achieves com-\nparable or even better performance in these datasets\ncompared to the BioBART model fine-tuned on\nother related datasets (Yuan et al., 2022a). This\nfurther confirms that zero-shot ChatGPT is more\nuseful than domain-specific fine-tuned models in\nbiomedical datasets that lack large training data.\n329\n5 Conclusions and Future Work\nIn this paper, we evaluate ChatGPT on 4 bench-\nmark biomedical tasks to observe that in datasets\nthat have large training data, ChatGPT performs\nquite poorly in comparison to the fine-tuned models\n(BioGPT and BioBART), whereas it outperforms\nfine-tuned models on datasets where the training\ndata size is small. These findings suggest that\nChatGPT can be useful in low-resource biomed-\nical tasks. We also observe that ChatGPT is sensi-\ntive to prompts, as variations in prompts led to a\nnoticeable difference in results.\nThough in this paper, we mostly evaluate Chat-\nGPT on tasks that require it to generate responses\nby only analyzing the input text, in the future, we\nwill investigate the performance of ChatGPT on\nmore challenging tasks, such as named entity recog-\nnition and entity linking (Yadav and Bethard, 2018;\nYan et al., 2021; Yuan et al., 2022b; Laskar et al.,\n2022a,b,c), as well as problems in information re-\ntrieval (Huang et al., 2005; Huang and Hu, 2009;\nYin et al., 2010; Laskar et al., 2020, 2022d). We\nwill also explore the ethical implications (bias or\nprivacy concerns) of using ChatGPT in the biomed-\nical domain.\nLimitations\nSince the training datasets of ChatGPT are un-\nknown, some data used for evaluation may or may\nnot exist during the training phase of ChatGPT\nAlso, a new version called the GPT-4 model has\nbeen released that may ensure higher accuracy.\nNonetheless, GPT-4 is very costly to use, around\n60x more expensive than ChatGPT. Meanwhile,\neven using the paid ChatGPT Plus2 subscription, it\nis available for just a limited use (allows evaluation\nof only 25 samples in 3 hours). Another limitation\nof this research is that the results mentioned in this\npaper for ChatGPT may not be reproducible, as\nChatGPT may generate different responses for the\nsame input prompt. Although the experimental re-\nsults may change over time, this work will still give\na concrete direction for future research using Chat-\nGPT like large language models in the biomedical\ndomain.\nEthics Statement\nThe paper evaluates ChatGPT on 4 benchmark\nbiomedical tasks that require ChatGPT to generate\n2https://openai.com/blog/chatgpt-plus, last ac-\ncessed June 4th, 2023\na response based on the information provided in the\ninput text. Thus, no data or prompt was provided\nas input that could lead to ChatGPT generating any\nresponses that pose any ethical or privacy concerns.\nThis evaluation is only done in some academic\ndatasets that already have gold labels available and\nso it does not create any concerns like humans re-\nlying on ChatGPT responses for sensitive issues,\nsuch as disease diagnosis. Since this paper only\nevaluates the performance of ChatGPT and investi-\ngates its effectiveness and limitations, conducting\nthis evaluation does not lead to any unwanted bi-\nases. Only the publicly available academic datasets\nare used that did not require any licensing. Thus, no\npersonally identifiable information has been used.\nAcknowledgements\nWe would like to thank all the anonymous review-\ners for their detailed review comments. This work\nis done at York University and supported by the\nNatural Sciences and Engineering Research Coun-\ncil (NSERC) of Canada and the York Research\nChairs (YRC) program.\nReferences\nAsma Ben Abacha and Dina Demner-Fushman. 2019.\nOn the summarization of consumer health questions.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 2228–\n2234.\nAsma Ben Abacha, Yassine M’rabet, Yuhao Zhang,\nChaitanya Shivade, Curtis Langlotz, and Dina\nDemner-Fushman. 2021. Overview of the mediqa\n2021 shared task on summarization in the medical\ndomain. In Proceedings of the 20th Workshop on\nBiomedical Language Processing, pages 74–85.\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical bert\nembeddings. arXiv preprint arXiv:1904.03323.\nSimon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan\nHögberg, Ulla Stenius, and Anna Korhonen. 2016.\nAutomatic semantic classification of scientific litera-\nture according to the hallmarks of cancer. Bioinfor-\nmatics, 32(3):432–440.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\n330\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3615–3620.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 4171–4186.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2020. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. arXiv preprint arXiv:2007.15779.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nMaría Herrero-Zazo, Isabel Segura-Bedmar, Paloma\nMartínez, and Thierry Declerck. 2013. The ddi\ncorpus: An annotated corpus with pharmacological\nsubstances and drug–drug interactions. Journal of\nbiomedical informatics, 46(5):914–920.\nYutai Hou, Yingce Xia, Lijun Wu, Shufang Xie, Yang\nFan, Jinhua Zhu, Tao Qin, and Tie-Yan Liu. 2022.\nDiscovering drug–target interaction knowledge from\nbiomedical literature. Bioinformatics, 38(22):5100–\n5107.\nXiangji Huang and Qinmin Hu. 2009. A bayesian learn-\ning approach to promoting diversity in ranking for\nbiomedical information retrieval. In Proceedings of\nthe 32nd international ACM SIGIR conference on\nResearch and development in information retrieval,\npages 307–314.\nXiangji Huang, Ming Zhong, and Luo Si. 2005. York\nuniversity at TREC 2005: Genomics track. In Pro-\nceedings of the Fourteenth Text REtrieval Conference,\nTREC 2005, Gaithersburg, Maryland, USA, Novem-\nber 15-18, 2005, volume 500-266 of NIST Special\nPublication. National Institute of Standards and Tech-\nnology (NIST).\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\nKatikapalli Subramanyam Kalyan, Ajit Rajasekharan,\nand Sivanesan Sangeetha. 2022. Ammu: a sur-\nvey of transformer-based biomedical pretrained lan-\nguage models. Journal of biomedical informatics ,\n126:103982.\nMd Tahmid Rahman Laskar, M Saiful Bari, Mizanur\nRahman, Md Amran Hossen Bhuiyan, Shafiq Joty,\nand Jimmy Xiangji Huang. 2023. A systematic study\nand comprehensive evaluation of chatgpt on bench-\nmark datasets. arXiv preprint arXiv:2305.18486.\nMd Tahmid Rahman Laskar, Cheng Chen, Xue-yong Fu,\nand Shashi Bhushan Tn. 2022a. Improving named\nentity recognition in telephone conversations via ef-\nfective active learning with human in the loop. In\nProceedings of the Fourth Workshop on Data Science\nwith Human-in-the-Loop (Language Advances), Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nMd Tahmid Rahman Laskar, Cheng Chen, Jonathan\nJohnston, Xue-Yong Fu, Shashi Bhushan TN, and Si-\nmon Corston-Oliver. 2022b. An auto encoder-based\ndimensionality reduction technique for efficient en-\ntity linking in business phone conversations. In Pro-\nceedings of the 45th International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval, pages 3363–3367.\nMd Tahmid Rahman Laskar, Cheng Chen, Aliak-\nsandr Martsinovich, Jonathan Johnston, Xue-Yong\nFu, Shashi Bhushan Tn, and Simon Corston-Oliver.\n2022c. BLINK with Elasticsearch for efficient entity\nlinking in business conversations. In Proceedings of\nthe 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies: Industry Track, pages\n344–352, Hybrid: Seattle, Washington + Online. As-\nsociation for Computational Linguistics.\nMd Tahmid Rahman Laskar, Enamul Hoque, and\nJimmy Xiangji Huang. 2022d. Domain adaptation\nwith pre-trained transformers for query-focused ab-\nstractive text summarization. Computational Linguis-\ntics, 48(2):279–320.\nMd Tahmid Rahman Laskar, Xiangji Huang, and Ena-\nmul Hoque. 2020. Contextualized embeddings based\ntransformer encoder for sentence similarity model-\ning in answer selection task. In Proceedings of the\n12th Language Resources and Evaluation Confer-\nence, pages 5505–5514.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. BART:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. arXiv preprint arXiv:1910.13461.\n331\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text summariza-\ntion branches out, pages 74–81.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\nZhang, Hoifung Poon, and Tie-Yan Liu. 2022.\nBiogpt: generative pre-trained transformer for\nbiomedical text generation and mining. Briefings\nin Bioinformatics, 23(6).\nKhalil Mrini, Franck Dernoncourt, Seunghyun Yoon,\nTrung Bui, Walter Chang, Emilia Farcas, and Ndapan-\ndula Nakashole. 2021. A gradually soft multi-task\nand data-augmented approach to medical question\nunderstanding. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1505–1515.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of bert and elmo on ten bench-\nmarking datasets. In Proceedings of the 18th BioNLP\nWorkshop and Shared Task, pages 58–65.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2021. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics, 8:842–866.\nMax Savery, Asma Ben Abacha, Soumya Gayen, and\nDina Demner-Fushman. 2020. Question-driven sum-\nmarization of answers to consumer health questions.\nScientific Data, 7(1):1–9.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nVikas Yadav and Steven Bethard. 2018. A survey on re-\ncent advances in named entity recognition from deep\nlearning models. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics,\npages 2145–2158.\nHang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng\nZhang, and Xipeng Qiu. 2021. A unified generative\nframework for various ner subtasks. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 5808–5822.\nXianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and\nWei Cheng. 2023. Exploring the limits of chatgpt\nfor query or aspect-based text summarization. arXiv\npreprint arXiv:2302.08081.\nXiaoshi Yin, Jimmy Xiangji Huang, Xiaofeng Zhou,\nand Zhoujun Li. 2010. A survival modeling ap-\nproach to biomedical search result diversification\nusing wikipedia. In Proceedings of the 33rd inter-\nnational ACM SIGIR conference on Research and\ndevelopment in information retrieval, pages 901–902.\nHongyi Yuan, Zheng Yuan, Ruyi Gan, Jiaxing Zhang,\nYutao Xie, and Sheng Yu. 2022a. Biobart: Pretrain-\ning and evaluation of a biomedical generative lan-\nguage model. In Proceedings of the 21st Workshop\non Biomedical Language Processing, pages 97–109.\nHongyi Yuan, Zheng Yuan, and Sheng Yu. 2022b. Gen-\nerative biomedical entity linking via knowledge base-\nguided pre-training and synonyms-aware fine-tuning.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4038–4048.\nGuangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang,\nSicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng,\nXiangyu Dong, Ruoyu Zhang, et al. 2020. Med-\ndialog: Large-scale medical dialogue datasets. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9241–9250.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. Bertscore: Evaluating\ntext generation with bert. In International Confer-\nence on Learning Representations.\n332\nCe Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu,\nGuangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,\nLifang He, et al. 2023. A comprehensive survey on\npretrained foundation models: A history from bert to\nchatgpt. arXiv preprint arXiv:2302.09419.\nA Appendix\nA.1 Evaluating ChatGPT on Different Tasks\nSince ChatGPT generated responses can be lengthy,\nand may sometimes contain unnecessary informa-\ntion while not in a specific format, especially in\ntasks that may have multiple answers (e.g., Rela-\ntion Extraction), it could be quite difficult to au-\ntomatically evaluate its performance in such tasks\nby comparing with the gold labels by using just\nan evaluation script. Thus, for some datasets and\ntasks, we manually evaluate the ChatGPT gener-\nated responses by ourselves and compare them with\nthe gold labels. Below we describe our evaluation\napproach for different tasks:\n• Relation Extraction: The authors manually\nevaluated the ChatGPT generated responses\nin this task by comparing them with the gold\nlabels. To ensure the reproducibility of our\nevaluation, we will release the ChatGPT gen-\nerated responses.\n• Document Classification: We created an\nevaluation script and identifies if the gold label\n(one of the 10 HoC classes) is present in the\nChatGPT generated response. For fair eval-\nuation, we lowercase each character in both\nthe gold label and the ChatGPT generated re-\nsponse. Our evaluation script will be made\npublicly available to ensure reproducibility of\nour findings.\n• Question Answering: Similar to Document\nClassification, we also evaluated using an eval-\nuation script that compares the gold label and\nthe ChatGPT generated response (here, we\nalso convert each character to lowercase). The\nevaluation script will also be made public.\n• Abstractive Summarization: We used the\nHuggingFace’s Evaluate3 library (Wolf et al.,\n2020) to calculate the ROUGE scores and the\nBERTScore for the Abstractive Summariza-\ntion task evaluation.\n3https://huggingface.co/docs/evaluate/index\nA.2 Effects of Prompt Variation\nWe investigate the effects of prompt tuning in the\nHoC dataset by evaluating the performance of Chat-\nGPT based on the following prompt variations:\n• Prompting with explicitly defining the 10 HoC\nclasses achieves an F1 score of 59.14 (see Row\n1 in Table 5).\n• Prompting without explicitly mentioning the\nname of 10 HoC classes, drops F1 to 38.20\n(see Row 2 in Table 5).\n• Prompting with the name of each HoC class is\ngiven without providing the definition of each\nclass, drops the F1 score to 46.93 (see Row 3\nin Table 5).\nOur findings demonstrate that more descriptive\nprompts yield better results.\nA.3 Sample ChatGPT Generated Responses\nSome sample prompts with the ChatGPT generated\nresponses for Relation Extraction, Document Clas-\nsification, and Question Answering tasks are given\nin Table 6 and for the Abstractive Summarization\ntask are given in Table 7.\n333\n# Prompt F1\n1. The 10 hallmark cancer taxonomy with their definitions MeQSum given below:\n(i) Sustaining proliferative signaling: Cancer cells can initiate and maintain\ncontinuous cell division by producing their own growth factors or by altering the\nsensitivity of receptors to growth factors.\n(ii) Evading growth suppressors: Cancer cells can bypass the normal cellular\nmechanisms that limit cell division and growth, such as the inactivation of tumor\nsuppressor genes.\n(iii) Resisting cell death: Cancer cells develop resistance to apoptosis, the\nprogrammed cell death process, which allows them to survive and continue\ndividing.\n(iv) Enabling replicative immortality: Cancer cells can extend their ability to\ndivide indefinitely by maintaining the length of telomeres, the protective end\ncaps on chromosomes.\n(v) Inducing angiogenesis: Cancer cells stimulate the growth of new blood\nvessels from existing vessels, providing the necessary nutrients and oxygen to\nsupport their rapid growth.\n(vi) Activating invasion and metastasis: Cancer cells can invade surrounding\ntissues and migrate to distant sites in the body, forming secondary tumors called\nmetastases.\n(vii) Cellular energetics: Cancer cells rewire their metabolism to support rapid\ncell division and growth, often relying more on glycolysis even in the presence\nof oxygen (a phenomenon known as the Warburg effect).\n(viii) Avoiding immune destruction: Cancer cells can avoid detection and\nelimination by the immune system through various mechanisms, such as\ndownregulating cell surface markers or producing immunosuppressive signals.\n(ix) Tumor promoting inflammation: Chronic inflammation can promote the\ndevelopment and progression of cancer by supplying growth factors, survival\nsignals, and other molecules that facilitate cancer cell proliferation and survival.\n(x) Genome instability and mutation: Cancer cells exhibit increased genomic\ninstability, leading to a higher mutation rate, which in turn drives the evolution\nof more aggressive and drug-resistant cancer cells.\nClassify the following sentence in one of the above 10 hallmark cancer taxonomy.\nIf cannot be classified, answer as \"empty\":\n[SENTENCE]\n59.14\n2. Is it possible to classify the following sentence in one of the 10 categories in the\nHallmarks of Cancer taxonomy? If possible, write down the class.\n[SENTENCE]\n38.20\n3. Classify the sentence given below in one of the 10 categories (i. activating\ninvasion and metastasis, ii. tumor promoting inflammation, iii. inducing\nangiogenesis, iv. evading growth suppressors, v. resisting cell death,vi. cellular\nenergetics, vii. genomic instability and mutation, viii. sustaining proliferative\nsignaling, ix. avoiding immune destruction, x. enabling replicative immortality)\nin the Hallmarks of Cancer taxonomy? If cannot be classified, answer as\n“empty”.\n[SENTENCE]\n46.93\nTable 5: Effects of Prompt Variation in the HoC dataset for the Document Classification Task.\n334\nDataset Type Prompt ChatGPT Response Gold Label\nBC5CDR Chemical-Disease\nRelation Extraction\nIdentify each pair of drugs and the drug-induced side-effects (e.g., diseases) in the\nfollowing passage:\nFamotidine-associated delirium. A series of six cases. Famotidine is a histamine\nH2-receptor antagonist used in inpatient settings for prevention of stress ulcers and is\nshowing increasing popularity because of its low cost. Although all of the currently\navailable H2-receptor antagonists have shown the propensity to cause delirium, only\ntwo previously reported cases have been associated with famotidine. The authors\nreport on six cases of famotidine-associated delirium in hospitalized patients who\ncleared completely upon removal of famotidine. The pharmacokinetics of famotidine\nare reviewed, with no change in its metabolism in the elderly population seen. The\nimplications of using famotidine in elderly persons are discussed.\nDrug: Famotidine\nDrug-induced disease: Delir-\nium\nSide effect: Increased risk of\ndelirium in elderly patients\nchemical: famotidine\ndisease: delirium\nKD-DTI Drug-Target\nRelation Extraction\nIdentify the drug-target interactions in the following passage (along with the in-\nteraction type among the following: ’inhibitor’, ’agonist’, ’modulator’, ’activator’,\n’blocker’, ’inducer’, ’antagonist’, ’cleavage’, ’disruption’, ’intercalation’, ’inactiva-\ntor’, ’bind’, ’binder’, ’partial agonist’, ’cofactor’, ’substrate’, ’ligand’, ’chelator’,\n’downregulator’, ’other’, ’antibody’, ’other/unknown’): Analogue inhibitors by\nmodifying oseltamivir based on the crystal neuraminidase structure for treating\ndrug-resistant H5N1 virus. The worldwide spread of H5N1 avian influenza and the\nincreasing reports about its resistance to the existing drugs have made a priority\nfor the development of the new anti-influenza molecules. The crystal structure of\nH5N1 avian influenza neuraminidase reported recently by Russell et al. [R.J. Russell,\nL.F. Haire, D.J. Stevens, P.J. Collins, Y . P. Lin, G.M. Blackburn, A.J. Hay, S.J.\nGamblin, J.J. Skehel, The structure of H5N1 avian influenza neuraminidase suggests\nnew opportunities for drug design, Nature 443 (2006) 45-49] have provided new\nopportunities for drug design in this regard. It is revealed through the structure that\nthe active sites of the group-1 neuraminidases, which contain the N1 subtype, have\na very different three-dimensional structure from those of group-2 neuraminidases.\nThe key difference is in the 150-loop cavity adjacent to the conserved active site in\nneuraminidase. Based on these findings and by modifying oseltamivir, six analog\ninhibitors were proposed as candidates for developing inhibitors against H5N1 virus,\nparticularly against the oseltamivir-resistant H5N1 virus strain.\nDrug-target interactions:\n- Oseltamivir and neu-\nraminidase\n- Analog inhibitors and\nneuraminidase\nInteraction type: Inhibitor\ndrug: Oseltamivir\ntarget: Neuraminidase\ninteraction: inhibitor\nDDI Drug-Drug\nRelation Extraction\nIdentify the pairs of drug-drug interactions in the passage given below based on one\nof the following interaction types:\n(i) mechanism: this type is used to identify drug-drug interactions that are described\nby their pharmacokinetic mechanism.\n(ii) effect: this type is used to identify drug-drug interactions describing an effect.\n(iii) advice: this type is used when a recommendation or advice regarding a drug-drug\ninteraction is given.\n(iv) int: this type is used when a drug-drug interaction appears in the text without\nproviding any additional information.\nInteractions between Leukine and other drugs have not been fully evaluated. Drugs\nwhich may potentiate the myeloproliferative effects of Leukine, such as lithium and\ncorticosteroids, should be used with caution.\n1. Leukine and lithium - effect\n2. Leukine and corticosteroids\n- effect\n3. Lithium and Leukine -\nadvice\n4. Corticosteroids and\nLeukine - advice\ndrug: Leukine\ntarget: lithium\ninteraction: effect\ndrug: Leukine\ntarget: corticosteroids\ninteraction: effect\nHoC Document\nClassification\nThe 10 hallmark cancer taxonomy with their definitions are given below:\n(i) Sustaining proliferative signaling: Cancer cells can initiate and maintain\ncontinuous cell division by producing their own growth factors or by altering the\nsensitivity of receptors to growth factors. (ii) Evading growth suppressors: Cancer\ncells can bypass the normal cellular mechanisms that limit cell division and growth,\nsuch as the inactivation of tumor suppressor genes. (iii) Resisting cell death: Cancer\ncells develop resistance to apoptosis, the programmed cell death process, which\nallows them to survive and continue dividing. (iv) Enabling replicative immortality:\nCancer cells can extend their ability to divide indefinitely by maintaining the length\nof telomeres, the protective end caps on chromosomes. (v) Inducing angiogenesis:\nCancer cells stimulate the growth of new blood vessels from existing vessels,\nproviding the necessary nutrients and oxygen to support their rapid growth. (vi)\nActivating invasion and metastasis: Cancer cells can invade surrounding tissues and\nmigrate to distant sites in the body, forming secondary tumors called metastases.\n(vii) Cellular energetics: Cancer cells rewire their metabolism to support rapid\ncell division and growth, often relying more on glycolysis even in the presence of\noxygen (a phenomenon known as the Warburg effect). (viii) Avoiding immune\ndestruction: Cancer cells can avoid detection and elimination by the immune\nsystem through various mechanisms, such as downregulating cell surface markers\nor producing immunosuppressive signals. (ix) Tumor promoting inflammation:\nChronic inflammation can promote the development and progression of cancer by\nsupplying growth factors, survival signals, and other molecules that facilitate cancer\ncell proliferation and survival. (x) Genome instability and mutation: Cancer cells\nexhibit increased genomic instability, leading to a higher mutation rate, which in\nturn drives the evolution of more aggressive and drug-resistant cancer cells.\nClassify the following sentence in one of the above 10 hallmark cancer taxonomy. If\ncannot be classified, answer as \"empty\":\nHeterogeneity in DNA damage within the cell population was observed as a function\nof radiation dose.\ngenomic instability and muta-\ntion.\ngenomic instability and muta-\ntion.\nPubMedQA Question Answering For the question, the reference context, and the answer given below, is it possible to\ninfer the answer for that question from the reference context? Only reply as either\nYes or No or Maybe.\nQuestion: Do some U.S. states have higher/lower injury mortality rates than others?\nReference context: This article examines the hypothesis that the six U.S. states\nwith the highest rates of road traffic deaths (group 1 states) also had above-average\nrates of other forms of injury such as falling, poisoning, drowning, fire, suffocation,\nhomicide, and suicide, and also for the retail trade and construction industries. The\nconverse, second hypothesis, for the six states with the lowest rates of road traffic\ndeaths (group 2 states) is also examined. Data for these 12 states for the period 1983\nto 1995 included nine categories of unintentional and four categories of intentional\ninjury. Seventy-four percent of the group 1 states conformed to the first hypothesis,\nand 85% of the group 2 states conformed to the second hypothesis.\nAnswer: Group 1 states are likely to exhibit above-average rates for most other\ncategories of injury death, whereas group 2 states are even more likely to exhibit\nbelow-average rates for most other categories of injury death.\nYes. Yes.\nTable 6: Sample prompts with ChatGPT generated responses for Relation Extraction, Document Classification, and Question\nAnswering Tasks.\n335\nDataset Type Prompt ChatGPT Response Gold Summary\niCliniq Dialog\nSummarization\nWrite a very short and concise one line summary of the following dialogue as an informal\nquestion in a healthcare forum:\nPatient: Hello doctor, I am depressed. What can I do? My parents are not supporting for a\nsurgery which is important for my self-confidence and self-esteem. My father is not taking care\nof my health or well-being. Please help. Doctor: Hello. Any related incident or cause for having\ndepression is unclear in this query. Details of surgery and symptoms which are not mentioned\nare appreciated to know the complete picture. Depressive feelings are generally linked with\nsome loss, self-related issues, stress, or environmental or biological factors. It can be managed\nby: 1. Medicines. 2. Psychotherapy to identify the causing agents and try resolving the issue.\n3. Modifying thoughts related to concerns. 4. Physical exercises like walking, jogging, and\nworkout. 5. Diet changes - Reduce the intake of too much salt and junk food and reduce sugar\nintake. Consume healthy foods and fruits having serotonin and omega-3 can reduce depression.\nWhat are some ways\nto manage depression\nwithout surgery and\nwhat details should I\nprovide to my doctor?\nI am depressed, and my par-\nents are not supporting me\nto overcome it. Please help\nHealthCare\nMagic\nDialog\nSummarization\nWrite a very short and concise one line summary of the following dialogue as a question in a\nhealthcare forum:\nPatient: Dear Doctor,I have a 1.5 years old sick daughter, I am looking for the written file to\nunderstand some clinical numbers of a normal young child(1-2 years old):1: what is the diameter\nof the hepatic portal vein? 2: what is the frequency of the hepatic portal flow?I will appreciate if\nyou could help me with these information.Kelvin Lu Doctor: 4-8 mm is range for diameter for\nhepatic portal vein for 1.5 year of children , there is no thing like frequency of hepatic portal\nflow. please make it clear\nWhat is the diameter of\nthe hepatic portal vein\nin a 1-2 year old child,\nand what is the fre-\nquency of the hepatic\nportal flow?\nWhat should be the diame-\nter of hepatic portal vein of\na 1.5 years old?\nMeQSum Question\nSummarization\nRewrite the following question in a short and concise form:\nQuestion: Williams’ syndrome . I would like to have my daughter tested for William’s syndrome.\nCould you please tell me where I would go or who does it in my area? Thank you!!\nWhere can I get my\ndaughter tested for\nWilliams’ syndrome in\nmy area?\nWhere can I get genetic\ntesting for william’s syn-\ndrome?\nMEDIQA-\nQS\nQuestion\nSummarization\nRewrite the following question in a short and concise form:\nQuestion: 23 surgeries and counting......lower lip birthmark, have tried all options out the there\nand guess what still have it, continues to grow back.....any suggestions? Is there a cure coming\nin the next few years hopefully?\nAny cure for a growing\nlower lip birthmark after\n23 surgeries?\nHow can i get rid of a lower\nlip birthmark permanently?\nMEDIQA-\nANS\nAnswer\nSummarization\nWrite a very short and concise summary of the following article based on the question given\nbelow:\nQuestion: Bad Breath I have very bad breath and at times it can make myself and others sick. I\nneed some advice as to what I need to do.\nArticle: Bad Breath Halitosis Summary There are many reasons why you might have bad breath.\nYou can get it if you don’t brush and floss regularly. Bacteria that build up in your mouth and\nbetween your teeth produce the bad odor. Other problems in your mouth, such as gum disease,\ndry mouth or cavities, may also cause it. Sinusitis or problems with your nose may be to blame.\nYou can also have bad breath if you eat some foods, like raw onions, garlic or cabbage. And of\ncourse smoking causes its own bad aroma. Some diseases and medicines are associated with a\nspecific breath odor. Having good dental habits, like brushing and flossing regularly, help fight\nbad breath. Mouthwashes, mints or chewing gum may make your breath fresher. If you have an\nunderlying disorder, treating it may help eliminate the breath odor.\nThe article discusses\nthe causes and consid-\nerations of bad breath,\nwhich is usually re-\nlated to poor dental hy-\ngiene. Other disor-\nders and diseases can\nalso produce distinct\nbreath odors, such as\nfruity breath being a\nsign of ketoacidosis in\ndiabetes. Proper den-\ntal hygiene, avoiding\nsmoking, and following\nhealthcare provider’s in-\nstructions are recom-\nmended for treating bad\nbreath. If bad breath\npersists or is accompa-\nnied by other symptoms,\nmedical attention may\nbe necessary.\nBad breath could be caused\nby poor oral hygiene, den-\ntal and gastroesophageal\ndiseases, infections, smok-\ning, alcoholism, and cer-\ntain foods.\nMEDIQA-\nMAS\nAnswer\nSummarization\nFor the following question, some relevant answers are given below. Please write down a short\nconcise answer by summarizing the given answers.\nQuestion: Can I use a Power Plate machine if I have a pacemaker?\nAnswer 1: Most machines and devices will not interfere with your pacemaker. But some with\nstrong magnetic fields may. Always ask your provider about any specific device that you need\nto avoid. DO NOT put a magnet near your pacemaker.Most appliances in your home are safe\nto be around. This includes your refrigerator, washer, dryer, toaster, blender, computers and\nfax machines, hair dryer, stove, CD player, remote controls, and microwaves.You should keep\nseveral devices at least 12 inches (30 centimeters) away from the site where the pacemaker is\nplaced under your skin. These include: Battery powered cordless tools (such as screwdrivers and\ndrills) Plug-in power tools (such as drills and table saws) Electric lawnmowers and leaf blowers\nSlot machines Stereo speakers Tell all providers that you have a pacemaker before any tests are\ndone.Some medical equipment may interfere with your pacemaker.Stay away from large motors,\ngenerators, and equipment. DO NOT lean over the open hood of a car that is running. Also\nstay away from: Radio transmitters and high-voltage power lines Products that use magnetic\ntherapy, such as some mattresses, pillows, and massagers Large electrical- or gasoline-powered\nappliances If you have a cell phone: DO NOT put it in a pocket on the same side of your body\nas your pacemaker. When using your cell phone, hold it to your ear on the opposite side of\nyour body. Be careful around metal detectors and security wands. Handheld security wands\nmay interfere with your pacemaker. Show your wallet card and ask to be hand searched. Most\nsecurity gates at airports and stores are OK. But DO NOT stand near these devices for long\nperiods. Your pacemaker may set off alarms. After any operation, have your provider check your\npacemaker.\nAnswer 2: You will probably be able to go home after 1 day or even the same day in some cases.\nYou should be able to return to your normal activity level quickly.Ask your provider how much\nyou can use the arm on the side of your body where the pacemaker was placed. You may be\nadvised not to: Lift anything heavier than 10 to 15 pounds (4.5 to 6.75 kilograms) Push, pull, and\ntwist your arm for 2 to 3 weeks. Raise your arm above your shoulder for several weeks. When\nyou leave the hospital, you will be given a card to keep in your wallet. This card lists the details\nof your pacemaker and has contact information for emergencies. You should always carry this\nwallet card with you. You should try to remember the name of the pacemaker manufacturer if\nyou can in case you lose your card.\nIt is important to ask\nyour provider about spe-\ncific devices that may\ninterfere with a pace-\nmaker, but most ap-\npliances in the home\nare safe. It is recom-\nmended to stay away\nfrom large motors, gen-\nerators, and equipment,\nproducts that use mag-\nnetic therapy, and to be\ncautious around metal\ndetectors and security\nwands. Additionally, af-\nter getting a pacemaker,\nsome activity restric-\ntions may apply, such as\nnot lifting heavy objects\nand limiting arm move-\nments. It is important\nto carry a wallet card\nwith pacemaker details\nand emergency contact\ninformation.\nMost machines are safe\nto use with a pacemaker.\nSome devices should be\nkept at least 12 inches\naway. These include bat-\ntery powered cordless tools\n(such as screwdrivers and\ndrills) Plug-in power tools\n(such as drills and table\nsaws) Electric lawnmow-\ners and leaf blowers Slot\nmachines Stereo speak-\ners Some medical equip-\nment may interfere with\nyour pacemaker.Stay away\nfrom large motors, gener-\nators, and equipment, Ra-\ndio transmitters and high-\nvoltage power lines Prod-\nucts that use magnetic\ntherapy, such as some\nmattresses, pillows, and\nmassagers Large electrical-\nor gasoline-powered appli-\nances. Do not put your\ncell phone in a pocket or\nto the ear on the same side\nas your pacemaker.\nTable 7: Sample prompts with ChatGPT generated responses for Abstractive Summarization tasks.\n336"
}