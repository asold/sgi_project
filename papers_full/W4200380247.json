{
    "title": "Pretrained models and evaluation data for the Khmer language",
    "url": "https://openalex.org/W4200380247",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2105365543",
            "name": "Sheng-Yi Jiang",
            "affiliations": [
                "Guangdong University of Foreign Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2769987352",
            "name": "Sihui Fu",
            "affiliations": [
                "Guangdong University of Foreign Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2914944597",
            "name": "Nankai Lin",
            "affiliations": [
                "Guangdong University of Foreign Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2566648366",
            "name": "Yingwen Fu",
            "affiliations": [
                "Guangdong University of Foreign Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2105365543",
            "name": "Sheng-Yi Jiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2769987352",
            "name": "Sihui Fu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2914944597",
            "name": "Nankai Lin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2566648366",
            "name": "Yingwen Fu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6600424091",
        "https://openalex.org/W6601141708",
        "https://openalex.org/W3121097595",
        "https://openalex.org/W4245596590",
        "https://openalex.org/W4241345206",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2104167780",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2321025932",
        "https://openalex.org/W3048986363",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2895122278",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3049537629",
        "https://openalex.org/W2138685326",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3011203202"
    ],
    "abstract": "Trained on a large corpus, pretrained models (PTMs) can capture different levels of concepts in context and hence generate universal language representations, which greatly benefit downstream natural language processing (NLP) tasks. In recent years, PTMs have been widely used in most NLP applications, especially for high-resource languages, such as English and Chinese. However, scarce resources have discouraged the progress of PTMs for low-resource languages. Transformer-based PTMs for the Khmer language are presented in this work for the first time. We evaluate our models on two downstream tasks: Part-of-speech tagging and news categorization. The dataset for the latter task is self-constructed. Experiments demonstrate the effectiveness of the Khmer models. In addition, we find that the current Khmer word segmentation technology does not aid performance improvement. We aim to release our models and datasets to the community in hopes of facilitating the future development of Khmer NLP applications.",
    "full_text": "TSINGHUA SCIENCE AND TECHNOLOGY\nISSNll1007-0214 05/10 pp709–718\nDOI: 1 0 . 2 6 5 9 9 / T S T . 2 0 2 1 . 9 0 1 0 0 6 0\nVolume 27, Number 4, August 2022\n\rC The author(s) 2022. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\nPretrained Models and Evaluation Data for the Khmer Language\nShengyi Jiang, Sihui Fu, Nankai Lin\u0003, and Yingwen Fu\nAbstract: Trained on a large corpus, pretrained models (PTMs) can capture different levels of concepts in context and\nhence generate universal language representations, which greatly beneﬁt downstream natural language processing\n(NLP) tasks. In recent years, PTMs have been widely used in most NLP applications, especially for high-resource\nlanguages, such as English and Chinese. However, scarce resources have discouraged the progress of PTMs for\nlow-resource languages. Transformer-based PTMs for the Khmer language are presented in this work for the ﬁrst\ntime. We evaluate our models on two downstream tasks: Part-of-speech tagging and news categorization. The\ndataset for the latter task is self-constructed. Experiments demonstrate the effectiveness of the Khmer models. In\naddition, we ﬁnd that the current Khmer word segmentation technology does not aid performance improvement. We\naim to release our models and datasets to the community in hopes of facilitating the future development of Khmer\nNLP applications.\nKey words: pretrained models; Khmer language; word segmentation; part-of-speech (POS) tagging; news\ncategorization\n1 Introduction\nPretrained models (PTMs) have greatly shaped the\nlandscape of natural language processing (NLP).\nIn general, PTMs are aimed at learning universal\nlanguage representations by training models on large\nunannotated corpora[1] and then ﬁne-tuning the learned\nrepresentations for the tasks of interest. Extensive\nresearch[2–4] has demonstrated that the use of PTMs\nin a variety of downstream NLP tasks could bring\nremarkable improvements (and mostly achieve state-of-\nthe-art performance) and demand a minimal amount of\nlabeled data in supervised learning.\nAlthough PTMs have been the default settings for\nmost NLP applications, they generally require a large\n\u000fShengyi Jiang, Sihui Fu, Nankai Lin, and Yingwen Fu are with\nthe School of Information Science and Technology, Guangdong\nUniversity of Foreign Studies, Guangzhou 510000, China.\nE-mail: neakail@outlook.com.\n\u000fShengyi Jiang is also with Guangzhou Key Laboratory of\nMultilingual Intelligent Processing, Guangdong University of\nForeign Studies, Guangzhou 510000, China.\n\u0003To whom correspondence should be addressed.\nManuscript received: 2021-05-15; revised: 2021-07-01;\naccepted: 2021-07-30\namount of computation to produce good results. As\nrevealed by several reports[5–7], most massive network\narchitectures are trained on unimaginably large corpora\nand use thousands of Graphics Processing Units (GPUs)\nor Tensor Processing Units (TPUs). For high-resource\nlanguages, such as English and Chinese, the required\nresources may not be difﬁcult to obtain. However, for\nlow-resource languages, acquiring abundant unlabeled\ndata is a major obstacle. Hence, NLP for minority\nlanguage groups has yet to progress further.\nIn the current work, we attempt to advance\nthe research on PTMs for the Khmer language.\nUtilizing publicly available Open Super-large Crawled\nAggregated coRpus (OSCAR) and Wiki corpora, we\ntrain several Khmer PTMs under different settings. We\nthen evaluate their performance on two downstream\ntasks: Part-of-speech tagging and news categorization.\nWhereas the former task adopts an open-source dataset,\nthe latter uses a self-constructed one. The experimental\nresults show the effectiveness of the Khmer PTMs.\nIn addition, as Khmer is a language with no explicit\ndelimiters between two words, we also exploit the impact\nof performing word segmentation on downstream tasks\nand ﬁnd that the current Khmer segmentation technology\n710 Tsinghua Science and Technology, August2022, 27(4): 709–718\nis not helpful. We plan to make all our models and\ndatasets accessible to the community to serve as strong\nbaselines and encourage future research on Khmer NLP.\nOur paper is organized as follows: Section 2 brieﬂy\nreviews the development of pretrained language models,\nas well as some published works related to Khmer NLP;\nSection 3 introduces the two PTMs employed in this\nwork, the data source for pretraining, and the word\nsegmentation tool; Section 4 describes two downstream\ntasks, as well as the datasets and evaluation metrics\nused; Section 5 details the experiments and analyzes the\nresults; and Section 6 concludes our work.\n2 Related Work\n2.1 Pretrained language models\nGenerally, pretrained representations could be either\ncontext-free or contextual. Representatives of context-\nfree models include word2vec [8] and Global Vectors\nfor Word Representation (GloVe) [9], both of which\nonly generate a single word embedding for each\nword in the vocabulary and ignore the fact that\na word might have different meanings in different\ncontexts. By contrast, contextual language models\nconsider context information as the representation of\na word depends on the other words in a sentence.\nContextual models such as Embedding from Language\nModels (ELMo)[10] and Universal Language Model Fine-\ntuning (ULMFit)[11] have long used the unidirectional\napproach. In other words, during the pretraining phase,\nmodels are trained by predicting a word conditioned\nonly on one side of the input sequence. Based on\nthe architecture of a transformer [12], Bidirectional\nEncoder Representations from Transformers (BERT)[2]\nemploys Masked Language Modeling (MLM) as one of\nits training objectives and ﬁrst achieves bidirectional\nlanguage understanding in the true sense. Several\nmodels, such as Robustly optimized BERT approach\n(RoBERTa)[3] and XLNet[5], have since been proposed\nto promote the success of BERT, but they usually\nrequire large networks and datasets to be effective.\nEfﬁciently Learning an Encoder that Classiﬁes Token\nReplacements Accurately (ELECTRA)[13] introduces a\nrelatively efﬁcient pretraining approach involving the\nreplacement of token detection, which helps it to produce\na model that is comparable to or better than the best\ntransformers with minimal computing power.\nAlthough pretrained language models are widely used\nin the ﬁeld of NLP, their progress on the Khmer language\nremains stagnant mainly because of the scarcity of\ntraining data and the difﬁculty of compiling evaluation\ntasks. To address the research gap, we present the ﬁrst\nBERT and ELECTRA models pretrained for the Khmer\nlanguage.\n2.2 Natural language processing for Khmer\nDespite being a low-resource language, Khmer has\nattracted increasing attention in recent years. The\nresearch on Khmer is growing and now covers word\nsegmentation[14], knowledge graph construction [15],\nparallel/comparable corpus construction [16, 17], named\nentity recognition[18, 19], etc. However, the data of most\nexisting research are not publicly accessible. Hence,\nwe could only choose part-of-speech (POS) tagging\nand text classiﬁcation as the downstream tasks for\nsubsequent model evaluation because the data for these\ntwo tasks are readily available. In the following, we\nbrieﬂy review related studies on Khmer POS tagging\nand text classiﬁcation.\nPOS tagging for Khmer. Nou and Kameyama [20]\ndesigned a 27-tag scheme for Khmer and then developed\na Khmer POS corpus, which includes 1298 sentences,\nalong with a tagger built upon the transformation-\nbased approach; they subsequently proposed a hybrid\napproach that combines rule-based and tri-gram models\nfor unknown word POS guessing [21]. The Pan Asia\nNetworking (PAN) Localization Project [22, 23] for the\nKhmer language also deﬁned a 21-POS tag set and\nconstructed a semi-automatic tagging corpus comprising\n3998 sentences; the proposed POS tagger was based on\nthe decision tree approach. Aiming at joint tokenization\nand POS tagging for low-resource languages. Ding\net al.[24] introduced the NOV A annotation system and\napplied it to Khmer; they ﬁnally presented an annotation\nguideline with seven POS tags and a corpus with 20 106\nannotated sentences. Thu et al. [25] also developed a\nmanually tagged corpus with their own devised tag\nset; on the basis of this corpus, they systematically\ncompared the performance of six well-known POS\ntagging methods so as to present a robust Khmer POS\ntagger. In the current work, we adopt the corpus of Thu\net al.[25] as our evaluation data.\nText classiﬁcation for Khmer. To the best of our\nknowledge, few scholars have conducted research on text\nclassiﬁcation for Khmer. Khoeurn and Kim[26] suggested\na Khmer music ranking website on which the data are\nsourced from the posts and comments found on the\nFacebook pages of production companies. The basic\nShengyi Jiang et al.: Pretrained Models and Evaluation Data for the Khmer Language 711\nidea was to translate Khmer texts to English ﬁrst and\nthen conduct sentiment analysis on the translated texts\nto acquire the ranking. This study could be regarded\nas an initial attempt to perform sentiment analysis\nfor Khmer texts via machine translation. Meanwhile,\nRatanak[27] focused on the sentiment classiﬁcation of\nKhmer comments on the news, and ﬁrst attempted to\nidentify the sentiments of texts at the sentence level and\nthen made use of such clues to determine the sentiments\nat the document level. However, these works did not\nrelease the data. Moreover, we cannot ﬁnd any publicly\navailable data about Khmer text classiﬁcation. Hence,\nwe build a dataset from scratch.\n3 Khmer Pretrained Models\n3.1 BERT\nIn contrast to previous works on pretraining\ncontextual representations that adopt unidirectional\nlanguage modeling, BERT [2] takes advantage of its\nbidirectionality, which allows it to consider the full\ncontext of a word by looking at the words that precede\nand follow it. Its internal structure is actually the\nencoder part of a transformer, which could model\ndependencies within a long sequence while enabling\nefﬁcient parallelization with the help of the multihead\nself-attention mechanism. Figure 1 presents a brief\nillustration of the architecture of BERT.\nBERT models are usually ﬁrst pre-trained on the\nenormous amount of unlabeled text from the web, and\nthen ﬁne-tuned for speciﬁc tasks which possess far\nless data. The pre-training of BERT involves two tasks:\nmasked language modeling and next sentence prediction.\nIn the MLM task, some percentage of the input tokens\nare masked randomly, and the model needs to predict\nthese masked tokens. As for NSP, the model is asked\nto learn relationships between sentences, so as to tell\nwhether Sentence B is the actual next sentence that\nFig. 1 Architecture of BERT.\nfollows Sentence A, or is just a random sentence from\nthe corpus. The pre-training procedure for BERT is\ndemonstrated in Fig. 2.\n3.2 ELECTRA\nAlthough BERT achieves superior performance in many\nnatural language understanding tasks, BERT models\ngenerally require a large number of parameters and\nextensive data to achieve high performance. In search\nof an alternative, Clark et al. [13] proposed an efﬁcient\nPTM called ELECTRA. Unlike its predecessors that\nrely on MLM pretraining, ELECTRA adopts a novel\napproach called “replaced token detection” (RTD).\nInstead of masking a random selection of input tokens,\nthis approach tries to construct a corrupted sequence\nby replacing some tokens in the original input with\nplausible alternatives sampled from a small generator\n(a transformer encoder). Then, a discriminator (also\na transformer encoder) takes the corrupted sequence\nas input and identiﬁes whether each token in it has\nbeen replaced by the generator or not. During the\npretraining phase, the generator is trained jointly with\nthe discriminator, with their combined loss minimized.\nAs for ﬁne-tuning, the generator is discarded, and the\ndiscriminator, i.e., the pretrained ELECTRA model,\nis retained. Similar to BERT, ELECTRA can then be\napplied to various language tasks. As RTD is deﬁned\nover all input tokens rather than on masked tokens alone,\nFig. 2 Pretraining procedure for BERT. Trm indicates\ntransformer-encoder, [CLS] is the special symbol for\nclassification output, and [SEP] is the special symbol to\nseparate non-consecutive token sequences. Ei represents the\ninput embedding of token i and E[CLS] represents the input\nembedding of [CLS].Ti represents the contextual representation\nof tokeni and C represents the representation of [CLS].\n712 Tsinghua Science and Technology, August2022, 27(4): 709–718\nit is more efﬁcient than MLM. In addition, it could\nhelp to mitigate pretraining/ﬁne-tuning discrepancies.\nFigure 3 presents an overview of RTD.\n3.3 Data for pretraining\nTo train our models, we try to collect texts from different\nsources. On the one hand, we use all the central Khmer\ndata from the OSCAR corpus , a large multilingual\ncorpus whose texts come from the Common Crawl\ncorpus. Although Common Crawl comprises scraped\ndata from the Internet and covers a wide range of topics,\nit distributes the data as a set of plain text ﬁles, each of\nwhich includes numerous documents that are written in\ndifferent languages but lack any language information.\nSu´arez et al. [28] proposed the goclassy architecture to\nperform language classiﬁcation and ﬁltering on the\nCommon Crawl corpus and obtained the language-\nclassiﬁed and ready-to-use OSCAR with 166 different\nlanguages available thus far.\nArticles on Khmer Wikipedia‘ are also used as part\nof our corpus for pretraining. We adopt the Khmer wiki\ndata∥ downloaded in January 2020 from the Wikipedia\ndumps. The data consist of 2536 documents written in\nKhmer. The statistics of all the data used for pretraining\nare shown in Table 1.\n3.4 Word segmentation of Khmer language\nIn the writing system of the Khmer language, words\nwithin the same sentences or phrases are run together\nwith no explicit delimiters among them. Unlike those in\nEnglish or French, spaces in Khmer texts are not used as\nword boundary delimiters and usually serve as phrase\ndelimiters for ease of reading. No standard rule indicates\nwhen to use or not use spaces. Chea et al.[29] pointed out\nthat one challenge for Khmer word segmentation lies in\nFig. 3 Overview of replaced token detection.\nTable 1 Statistics for pretraining of the corpus.\nSource Number of lines Number of tokens\nUnsegmented Segmented\nOSCAR 1 705 029 10 316 527 40 255 297\nWiki 333 060 2 539 906 8 725 557\nhttps://oscar-corpus.com/\nhttps://commoncrawl.org/\n‘https://km.wikipedia.org/\n∥https://github.com/phylypo/khmer-language-model-ulmﬁt\nthe fact that a single sentence could be segmented in\nvarious ways with regard to its meaning in context.\nHence, for Khmer NLP, one question needs to be\nanswered: Is it necessary to conduct word segmentation\non Khmer texts at the beginning? In other words, how\nbeneﬁcial is word segmentation for downstream tasks?\nTo address this problem, we pretrain our models on\nunsegmented and segmented texts and then compare\ntheir performances on different downstream tasks. In\nthis work, we employ the Khmer word segmenter \u0003\u0003\ndeveloped by Chea et al. [29]. Apart from segmenting\nsingle words, Chea et al.[29] proposed to segment three\ntypes of compound words: Those composed of two\nor more single words, those with speciﬁc preﬁxes,\nand those with speciﬁc sufﬁxes. On the basis of such\na segmentation scheme, Chea et al. [29] constructed\na manually segmented corpus with 97 340 sentences.\nThen, the Khmer word segmenter was trained on this\ncorpus by using the conditional random ﬁeld model in\na closed test. The precision, recall, and F-score were\nreported to be 0.986, 0.983, and 0.985, respectively. The\nstatistics of our pretraining data after sentence and word\nsegmentation are presented in Table 1.\n4 Evaluation Tasks\nWe evaluate our PTMs on two downstream NLP tasks:\nPOS tagging and text classiﬁcation. In the following\nsections, we brieﬂy introduce each task, along with the\nevaluation datasets and procedures.\n4.1 POS tagging\nPOS tagging refers to the process of determining the\ngrammatical category of a word in a text according to\nits deﬁnition and context. It is usually regarded as a\nsequence labeling problem in which each token in a\ngiven input sequence is assigned a categorical label.\nThe dataset we use for this task comes from\nThu et al. [25], they ﬁrst collected 12 000 sentences\nfrom several Khmer websites and performed initial\nword segmentation using the segmenter mentioned in\nSection 3.4. Annotators were then asked to tag each\nword in the sentences under the guidance of the proposed\n24-tag POS scheme and to ﬁx the segmentation errors. In\naddition, Thu et al.[25] collected 1000 Khmer sentences\nto build a test set. Some statistics about this corpus are\nshown in Table 2.\nThe performance of POS tagging is evaluated using\naccuracy, precision, recall, and micro-F1 score, as\n\u0003\u0003https://github.com/VietHoang1512/khmer-nltk\nShengyi Jiang et al.: Pretrained Models and Evaluation Data for the Khmer Language 713\nTable 2 Statistics for POS corpus.\nType of\ndataset\nNumber of\nsentences\nNumber of\nwords\nNumber of\nunique words\nTraining set 12 000 129 029 7624\nTest set 1000 10 397 2743\nprovided by the seqeval module. Accuracy refers to\nthe ratio of the number of POS tags that a model\ncorrectly predicts to the number of all POS tags in the\ncorpus. For each POS tag, its precision refers to the\nnumber of tokens correctly labeled as this tag (i.e., true\npositives, TPs) divided by the total number of tokens\npredicted by the model as having this tag (i.e., the sum\nof TPs and false positives, FPs, which refer to the items\nincorrectly predicted as having this tag). The recall\nis deﬁned as the number of TPs divided by the total\nnumber of tokens that actually have this tag (i.e., the\nsum of TPs and false negatives, FNs, which are the\ntokens wrongly predicted as not having this tag). For the\nﬁnal results, seqeval gives the tag-wise precision and\nrecall (i.e., micro-precision and micro-recall), which can\nbe respectively calculated as follows:\nPmicro D\nnX\niD1\nTPi\nnX\niD1\nTPi C\nnX\niD1\nFPi\n(1)\nRmicro D\nnX\niD1\nTPi\nnX\niD1\nTPi C\nnX\niD1\nFNi\n(2)\nwhere n refers to the number of POS tags in the corpus.\nAs the harmonic mean of precision and recall, micro-F1\ncan be obtained by\nFmicro D 2PmicroRmicro\nPmicro CRmicro\n(3)\n4.2 Text classiﬁcation\nText classiﬁcation is the task of assigning sentences or\ndocuments to predeﬁned categories. In this work, we\nhandle the problem of news categorization.\nGiven the absence of a publicly available dataset,\nwe scrape some news articles written during 2010 to\n2021 from VOA Khmer⋇ to build our evaluation dataset.\nAs the articles are sorted into different categories, we\ndo not need to manually annotate the category for each\nnews article; we simply adopt its classiﬁcation scheme.\nThe whole dataset comprises 7166 Khmer news\narticles; each labeled as one of the following eight\ncategories: Culture, economic, education, environment,\nhealth, politics, rights, and science. The dataset is\ndivided into the training, validation, and tests with\na ratio of 0.6:0.2:0.2. We should point out that, as\ndifferent categories have signiﬁcantly different numbers\nof articles, the division is conducted at the category level\nrather than on the whole dataset so as to preserve the\npercentage of samples for each category. The detailed\nstatistics of our dataset for Khmer news categorization\nare presented in Table 3.\nThe classiﬁcation performance is evaluated using\nmacro F1-score and accuracy. With regard to each\ncategory, the F1-score is calculated as follows:\nFi D 2Pi Ri\nPi CRi\n(4)\nwhere Pi and Ri are obtained by\nPi D TPi\nTPi CFRi\n(5)\nRi D TPi\nTPi CFNi\n(6)\nTable 3 Statistics of our dataset for Khmer news categorization.\nCategory Number of articles Number of tokens\n(segmented)\nNumber of articles\nin the training set\nNumber of articles\nin the validation set\nNumber of articles\nin the test set\nEducation 568 80 646 340 114 114\nPolitics 1205 127 884 723 241 241\nEconomic 1150 137 157 690 230 230\nRights 1149 145 154 689 230 230\nHealth 1201 103 238 720 240 241\nEnvironment 965 111 571 579 193 193\nScience 266 30 139 159 53 54\nCulture 662 93 023 396 133 133\nTotal 7166 828 812 4296 1434 1436\nhttps://github.com/SunYanCN/seqeval\n⋇https://khmer.voanews.com\n714 Tsinghua Science and Technology, August2022, 27(4): 709–718\nThus, the macro F1-score is computed as follows:\nFmacro D\nncX\niD1\nFi\nnc\n(7)\nwhere nc is the number of categories. As for the accuracy,\nit is simply the ratio between the number of those\ncorrectly classiﬁed articles and the total number of\narticles.\n5 Experiment\nIn this part, we present the experimental setup and results\nfor the Khmer PTMs. In particular, we pretrain the\nmodels on unsegmented and segmented texts and then\napply them to the two tasks to explore the beneﬁts of\nword segmentation.\n5.1 Pretraining\nAll the models are trained on the pretrained data for\nten epochs. Their learning rates gradually increase over\nthe ﬁrst 5000 steps to a peak value of 1 \u000210\u00004, after\nwhich they decline linearly. The weights are initialized\nrandomly from a normal distribution with a mean of 0.0\nand a standard deviation of 0.02. We build and train the\nBERT and ELECTRA tokenizers from scratch on our\npretrained data, each of which has a vocabulary size of\n32 000.\nFor the BERT models, the size of a minibatch\nis 128, with the maximum sequence length being\n512. The BERT SMALL model has four layers with\neight attention heads. The dimensions of the encoder\nand feedforward layers are 512 and 2048, respectively.\nThe BERT BASE model has 12 layers with 12\nattention heads, and the dimensions of the encoder and\nfeedforward layers are 768 and 3072, respectively.\nWith regard to the ELECTRA models, the\nhyperparameters used in ELECTRA SMALL and\nELECTRA BASE are the same as those in BERT\nSMALL and BERT BASE, except that the MLM\nprobability for the ELECTRA models is 0.25 while that\nfor the BERT models is 0.15.\nFigure 4 illustrates the pretraining curves for each\nmodel. Given the same training time, the deep and wide\nmodels are more helpful in achieving a low training loss\nthan the shallow models. Pretraining on segmented texts\nalso aids the decrease of training loss. After 150 000\ntraining steps, BERT BASE and ELECTRA BASE\ntrained on segmented texts reach the lowest training\nloss.\nFig. 4 Pretraining curves for all models, with training loss\nover the steps.\n5.2 Fine-tuning\nFine-tuning is conducted by attaching relevant classiﬁers\nrequired for each task. After performing hyperparameter\noptimization, we employ the optimal setup for\nall evaluation experiments. While all other settings\nemployed in both SMALL and BASE models are the\nsame, the only difference is the peak value of the\nlearning rate. The SMALL models take a peak value\nof 1\u000210\u00004, but for BASE this value is smaller, which\nis 5\u000210\u00005. From the perspective of evaluation tasks,\nthe POS tagging models go through 10 epochs and use\nsequences of 200 tokens in a batch size of 32. In terms\nof the classiﬁcation task, the models are ﬁne-tuned on\nsequences of 128 tokens in the batch size of 32, for up\nto 3 epochs. We report the results of the models that\nperform best on the validation sets.\n5.3 Results of POS tagging\nIn ﬁne-tuning the models for POS tagging, we attach a\nlinear classiﬁer to the top layers of the PTMs to predict\nthe POS tag for each token in the input sequence. Cross\nentropy is then used as the loss function. Table 4 shows\nthe ﬁnal results of our models on this task.\nApart from developing the Khmer POS corpus, Thu et\nal.[25] evaluated several POS tagging approaches; their\nreleased codes are run herein to reproduce the results\nreported in their paper as our baseline and thereby\neffectively assess the performance of our models. In the\ncodes of Thu et al.[25], they provided the implementation\nof four approaches, namely, support vector machine\n(SVM), hidden Markov model (HMM), maximum\nentropy (MAX-Ent), and ripple-down rules (RDRs);\nfor the former three methods, the features they adopted\ninclude the labels of the current word and its neighboring\nwords. The paper of Thu et al. [25] only reported the\naccuracy of these methods, whereas the current work\nShengyi Jiang et al.: Pretrained Models and Evaluation Data for the Khmer Language 715\nTable 4 Results of POS tagging task on the test set.\n(%)\nModel Accuracy Precision Recall F1 score\nSVM 81.68 69.45 76.40 72.76\nHMM 95.26 93.50 93.33 93.42\nMAX-Ent 92.97 90.32 89.12 89.91\nRDR 95.82 94.27 93.83 94.05\nBERT (small) 96.10 93.40 93.14 93.27\nBERT (base) 96.79 94.37 94.20 94.29\nELECTRA (small) 96.20 93.27 93.27 93.27\nELECTRA (base) 96.73 94.39 94.34 94.36\nBERT (small,\nsegmented) 96.83 94.42 94.23 94.33\nBERT (base,\nsegmented) 97.03 94.77 94.94 94.86\nELECTRA (small,\nsegmented) 96.66 94.49 94.27 94.38\nELECTRA (base,\nsegmented) 97.00 94.75 94.94 94.85\npresents the micro-precisions, recalls, and F1 scores for\na comprehensive comparison?.\nIn general, the PTMs outperform all the baseline\nmethods. The best model, BERT BASE, shows an\nimproved accuracy of 1.21 points and recall of 1.11\npoints relative to the best baseline, RDR. The base\nmodels obviously perform better than the small variants,\nbut the improvements may not be signiﬁcant for this\ntask, especially with respect to accuracy. As for model\nselection, the gap between BERT and ELECTRA is quite\nsmall.\nWe generally presume POS tagging to be a token-\nbased task, but the results show that segmentation\ndoes not greatly improve performance. Even the\nmodels pretrained on unsegmented texts are able to\nbeat the baseline approaches. As the PTMs read an\nentire sequence at once and map it into intermediate\nrepresentations, the result suggests that POS tagging\ncould beneﬁt from the information provided by the\nwhole sequence. As segmentation is performed by the\nsegmenter without any manual intervention, the negative\nimpact of segmentation errors on the results deserves\nfurther investigation.\n5.4 Results of text classiﬁcation\nAs for the ﬁne-tuning of the models for text classiﬁcation,\na linear classiﬁer is also added to the PTMs to predict\nthe category to which each input article belongs. The\nloss function is still cross entropy. Table 5 shows the\noverall results for each model on the test set. Note that\nthe results concerning segmentation are all reported on\nthe basis of the models pretrained and ﬁne-tuned on the\nsegmented training data; the others are based on the\nunsegmented data.\nThe results are consistent with those of POS tagging.\nIn most cases, the base models beat the small variants,\nand the ELECTRA-based models perform slightly\nbetter than the BERT-based ones. Segmentation indeed\nenhances model performance, but its inﬂuence is still\nunremarkable. Hence, the current word segmentation\nfor Khmer does not appear to be highly beneﬁcial for\ndownstream tasks.\nAs the F1 scores and accuracy are not satisfactory,\nwe try to conduct an error analysis by checking the\nperformance of the models with the help of a confusion\nmatrix. For the four BASE models, we consider macro\nF1 scores on each category (Table 6) and suppose that\nthe models may suffer from the class imbalance problem\nas their performance on the categories with the least\nnumber of articles (e.g., “Science” and “Education”) is\nrelatively poor. To deal with the imbalanced data, we\nTable 5 Results of the news categorization task on the test set.\n(%)\nModel Macro F1 score ACC\nBERT (small) 65.97 67.41\nBERT (base) 66.99 68.66\nELECTRA (small) 66.89 68.18\nELECTRA (base) 68.22 69.29\nBERT (small, segmented) 68.50 69.78\nBERT (base, segmented) 67.46 69.08\nELECTRA (small, segmented) 67.97 68.94\nELECTRA (base, segmented) 68.64 69.99\nTable 6 Macro F1 scores on each category for four PTMs.\n(%)\nModel Culture Economic Education Environment Health Politics Rights Science\nBERT (base) 70.21 67.24 66.67 80.68 76.17 62.64 65.80 57.43\nELECTRA (base) 68.59 67.49 66.00 79.51 76.52 61.64 63.95 59.62\nBERT (base, segmented) 69.75 69.05 63.21 79.43 78.37 62.78 62.47 56.00\nELECTRA (base, segmented) 70.97 67.91 66.03 81.17 76.99 62.95 63.39 59.05\n? Although the accuracies of the other three models are close to the reported values, our result for SVM is quite at odds with the result of\nThu et al.[25] (94.57%).\n716 Tsinghua Science and Technology, August2022, 27(4): 709–718\nemploy a simple yet effective informed undersampling\nmethod called EasyEnsemble[30], which samples several\nsubsets from majority classes, trains a learner on\neach subset, and combines all weak learners into\na ﬁnal ensemble. In our experiments, we generate\nseven subsets, with each one satisfying an equal class\ndistribution. Seven learners are trained, and their\noutputs are combined to obtain the ensemble results\n(Table 7). The results demonstrate that the ensemble\nmethod does help improve model performance as all\nmodels achieve high F1 scores and high accuracy. When\nwe regard the macro F1 scores for each category for\nthe four PTMs after utilizing EasyEnsemble (Table 8),\nwe ﬁnd that although the performance on “Science”\nis slightly low, our ensemble learners are still able to\nalleviate the imbalance issue to some extent.\nWe further conduct a case study and consider the\nTable 7 Results of news classiﬁcation task on the test set\nafter utilizing EasyEnsemble. (%)\nModel Macro F1 score ACC\nBERT (small) 67.05 68.38\nBERT (base) 68.21 69.71\nELECTRA (small) 67.54 68.73\nELECTRA (base) 69.31 70.47\nBERT (small, segmented) 68.64 69.78\nBERT (base, segmented) 68.86 70.19\nELECTRA (small, segmented) 68.96 70.40\nELECTRA (base, segmented) 69.42 70.61\ntop ﬁve mistakes on the test set. As Table 9 reveals,\nthe major categories (e.g., “Politics” and “Economics”)\naccount for the most mistakes. Hence, the introduction\nof the undersampling method fails to lead to signiﬁcant\nimprovements. All models struggle to distinguish the\nmajor categories, especially “Politics” and “Rights”.\nSuch a result could shed some light on their relatively\nlow scores for these two categories and on the fact that,\nin practice, the new articles under these categories tend\nto overlap one another and are more closely related than\nwe think.\n6 Conclusion\nIn this work, we present PTMs for the Khmer\nlanguage for the ﬁrst time by using BERT and\nELECTRA. Considering the challenges presented by\nlimited resources and the difﬁculty of compiling labeled\ndata, we only apply the models to two downstream tasks,\nthe dataset for one of which is self-constructed. The\nexperimental results demonstrate the effectiveness of\nour Khmer PTMs. We also explore whether performing\nword segmentation exerts a positive inﬂuence on\ndownstream tasks. Although the current Khmer word\nsegmentation technology could offer some beneﬁts, the\nimprovements gained are not signiﬁcant. By releasing\nour models and datasets to the community, we hope to\nadvance the Khmer NLP research. For our future work,\nwe will explore whether a more effective segmenter can\nTable 8 Macro F1 scores on each category for four PTMs after utilizing EasyEnsemble.\n(%)\nModel Culture Economic Education Environment Health Politics Rights Science\nBERT (base) 72.85 67.83 65.79 78.91 77.31 63.86 65.19 53.91\nELECTRA (base) 72.30 70.00 68.47 79.90 77.18 63.23 65.78 57.63\nBERT (base, segmented) 73.68 69.64 66.97 80.10 78.03 62.24 64.18 56.00\nELECTRA (base, segmented) 73.40 69.74 67.28 81.64 77.85 62.18 64.75 58.54\nTable 9 Top ﬁve mistakes on the test set for the four PTMs.\nModel Reference Hypothesis Frequency Model Reference Hypothesis Frequency\nBERT (base)\nPolitics Rights 48\nBERT\n(base, segmented)\nPolitics Rights 50\nPolitics Economics 28 Rights Politics 38\nRights Politics 27 Politics Economics 26\nHealth Economics 23 Economic Environment 20\nHealth Politics 19 Rights Economic 19\nELECTRA (base)\nPolitics Rights 44\nELECTRA\n(base, segmented)\nPolitics Rights 49\nRights Politics 31 Rights Politics 29\nPolitics Economics 27 Politics Economics 25\nHealth Economics 21 Rights Economics 21\nRights Economics 19 Politics Health 21\n For each subset, the sample ratios are 1.0 for the “Science” category, 0.3 for the “Culture” and “Education” categories, and 0.5 for all\nthe others.\nShengyi Jiang et al.: Pretrained Models and Evaluation Data for the Khmer Language 717\nlead to even higher performance. We will also attempt\nto develop other Khmer NLP tasks, such as named entity\nrecognition, natural language inference, and question\nanswering.\nAcknowledgment\nThis work was supported by the Major Projects\nof Guangdong Education Department for Foundation\nResearch and Applied Research (No. 2017KZDXM031)\nand Guangzhou Science and Technology Plan Project (No.\n202009010021). We would like to extend our sincere\ngratitude to the anonymous reviewers for their insightful\nfeedbacks.\nReferences\n[1] X. P. Qiu, T. X. Sun, Y . G. Xu, Y . F. Shao, N. Dai, and X. J.\nHuang, Pre-trained models for natural language processing:\nA survey,Sci. China Technol. Sci., vol. 63, no. 10, pp. 1872–\n1897, 2020.\n[2] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova,\nBERT: Pre-training of deep bidirectional transformers\nfor language understanding, in Proc. 2019 Conf. North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Minneapolis,\nMinnesota, 2019, pp. 4171–4186.\n[3] Y . H. Liu, M. Ott, N. Goyal, J. F. Du, M. Joshi,\nD. Q. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and\nV . Stoyanov, RoBERTa: A robustly optimized BERT\npretraining approach, arXiv preprint arXiv: 1907.11692,\n2019.\n[4] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever, Language models are unsupervised multitask\nlearners, OpenAI Blog, vol. 1, no. 8, pp. 9–32, 2019.\n[5] Z. L. Yang, Z. H. Dai, Y . M. Yang, J. Carbonell, R.\nR. Salakhutdinov, and Q. V . Le, XLNet: Generalized\nautoregressive pretraining for language understanding,\npresented at the Advances in Neural Information Processing\nSystems 32 (NeurIPS 2019), Vancouver, Canada, 2019, pp.\n5754–5764.\n[6] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M.\nMatena, Y . Zhou, W. Li, and P. Liu, Exploring the limits of\ntransfer learning with a uniﬁed text-to-text transformer, J .\nMach. Learn. Res., vol. 21, no. 140, pp. 1–67, 2020.\n[7] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P.\nDhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\net al., Language models are few-shot learners, arXiv preprint\narXiv: 2005.14165, 2020.\n[8] T. Mikolov, K. Chen, G. Corrado, and J. Dean, Efﬁcient\nestimation of word representations in vector space, in\nProc. 1st Int. Conf. Learning Representations, ICLR 2013,\nScottsdale, AZ, USA, 2013, pp. 1–9.\n[9] J. Pennington, R. Socher, and C. Manning, GloVe: Global\nvectors for word representation, in Proc. 2014 Conf.\nEmpirical Methods in Natural Language Processing, Doha,\nQatar, 2014, pp. 1532–1543.\n[10] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C.\nClark, K. Lee, and L. Zettlemoyer, Deep contextualized\nword representations, in Proc. 2018 Conf. North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, New Orleans, LA, USA,\n2018, pp. 2227–2237.\n[11] J. Howard and S. Ruder, Universal language model ﬁne-\ntuning for text classiﬁcation, inProc. 56th Annu. Meeting of\nthe Association for Computational Linguistics, Melbourne,\nAustralia, 2018, pp. 328–339.\n[12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.\nN. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you\nneed, in Proc. 31st Conf. Neural Information Processing\nSystems, Long Beach, CA, USA, 2017, pp. 1–15.\n[13] K. Clark, M. T. Luong, Q. V . Le, and C. D. Manning,\nELECTRA: Pre-training text encoders as discriminators\nrather than generators, in Proc. 8th Int. Conf. Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, 2020,\npp. 1–18.\n[14] B. B. Yu, Y . Nuo, X. Yan, Q. L. Lei, G. Y . Xu, and\nF. Zhou, Segmentation and alignment of Chinese and\nKhmer bilingual names based on hierarchical dirichlet\nprocess, presented at Int. Conf. Mechatronics and Intelligent\nRobotics (ICMIR2018), Kunming, China, 2018, pp. 441–\n450.\n[15] U. Phon and C. Pluempitiwiriyawej, Khmer WordNet\nconstruction, presented at the 5 th Int. Conf. Information\nTechnology (InCIT), Chonburi, Thailand, 2020, pp. 122–\n127.\n[16] H. Y . Chi, X. Yan, S. Y . Li, F. Zhou, G. Y . Xu, and L.\nZhang, The acquisition of Khmer-Chinese parallel sentence\npairs from comparable corpus based on manhattan-BiGRU\nmodel, presented at the 2020 Chinese Control and Decision\nConf., Hefei, China, 2020, pp. 4801–4805.\n[17] S. Ning, X. Yan, Y . Nuo, F. Zhou, Q. Xie, and J. P.\nZhang, Chinese-Khmer parallel fragments extraction from\ncomparable corpus based on dirichlet process, Procedia\nComput. Sci., vol. 166, pp. 213–221, 2020.\n[18] H. S. Pan, X. Yan, Z. T. Yu, and J. Y . Guo, A Khmer\nnamed entity recognition method by fusing language\ncharacteristics, presented at the 26th Chinese Control and\nDecision Conf., Changsha, China, 2014, pp. 4003–4007.\n[19] X. H. Liu, X. Yan, G. Y . Xu, Z. T. Yu, and G. S. Qin, Khmer-\nChinese bilingual LDA topic model based on dictionary,Int.\nJ. Comput. Sci. Math., vol. 10, no. 6, pp. 557–565, 2019.\n[20] C. Nou and W. Kameyama, Khmer POS Tagger: A\ntransformation-based approach with hybrid unknown word\nhandling, presented at the Int. Conf. Semantic Computing\n(ICSC 2007), Irvine, CA, USA, 2007, pp. 482–492.\n[21] C. Nou and W. Kameyama, Hybrid approach for Khmer\nunknown word POS guessing, presented at the 2007 IEEE\nInt. Conf. Information Reuse and Integration, Las Vegas,\nNV , USA, 2007, pp. 215–220.\n[22] PAN Localization Cambodia (PLC) of IDRC, Part\nof speech template, https://www.dit.gov.bt/sites/default/\nﬁles/PartOfSpeech.pdf, 2007.\n[23] PAN Localization Cambodia (PLC) of IDRC,\n718 Tsinghua Science and Technology, August2022, 27(4): 709–718\nKhmer automatic Pos tagging, https://moam.info/research-\nreport-on-khmer-automatic-pos-pan-localization\n5a22d8711723ddefdcf2139f.html, 2008.\n[24] C. C. Ding, M. Utiyama, and E. Sumita, NOV A: A feasible\nand ﬂexible annotation system for joint tokenization and\npart-of-speech tagging, ACM Trans. Asian Low-Resour.\nLang. Inf. Process., vol. 18, no. 2, p. 17, 2019.\n[25] Y . K. Thu, V . Chea, and Y . Sagisaka, Comparison of six\nPOS tagging methods on 12K sentences Khmer language\nPOS tagged corpus, in Proc. 1st Regional Conf. Optical\nCharacter Recognition and Natural Language Processing\nTechnologies for ASEAN Languages(ONA 2017), Phnom\nPenh, Cambodia, 2017, pp. 1–12.\n[26] S. Khoeurn and Y . S. Kim, Sentiment analysis engine for\nCambodian music industry re-building,J. Korea Soc. Simul.,\nvol. 26, no. 4, pp. 23–34, 2017.\n[27] T. Ratanak, A study on the sentiment classiﬁcation for\nKhmer comments on news, (in Chinese), Master\ndissertation, Kunming Univ. Sci. Technol., Kunming,\nChina, 2017.\n[28] P. J. O. Su´arez, B. Sagot, and L. Romary, Asynchronous\npipelines for processing huge corpora on medium to\nlow resource infrastructures, in Proc. 22nd Workshop on\nChallenges in the Management of Large Corpora(CMLC-\n7/, Mannheim, Germany, 2019, pp. 9–16.\n[29] V . Chea, Y . K. Thu, C. C. Ding, M. Utiyama, A. Finch,\nand E. Sumita, Khmer word segmentation using conditional\nrandom ﬁelds, in Khmer Natural Language Processing,\nPhnom Penh, Cambodia, 2015, pp. 62–69.\n[30] X. Y . Liu, J. X. Wu, and Z. H. Zhou, Exploratory\nundersampling for class-imbalance learning, IEEE Trans.\nSyst., Man, Cybern., Part B (Cybern.), vol. 39, no. 2, pp.\n539–550, 2009.\nShengyi Jiang received the PhD degree\nin computer science from Huazhong\nUniversity of Science and Technology,\nWuhan, China, in 2005. He is a professor at\nGuangdong University of Foreign Studies,\nGuangzhou, China. He is mainly engaged\nin data mining and natural language\nprocessing.\nSihui Fu received the MS degree in\nmanagement science and engineering\nfrom Guangdong University of Foreign\nStudies, Guangzhou, China, in 2019. She\nhas published papers in the International\nConference on Language Resources and\nEvaluation. She is mainly engaged in data\nmining and natural language processing.\nNankai Lin received the BS degree in\nsoftware engineering from Guangdong\nUniversity of Foreign Studies, Guangzhou,\nChina, in 2019. Now he is pursuing the\nmaster degree in cyberspace security at\nGuangdong University of Foreign Studies,\nGuangzhou, China. He is mainly engaged\nin data mining and natural language\nprocessing.\nYingwen Fu received the BS degree in\nsoftware engineering from Guangdong\nUniversity of Foreign Studies, Guangzhou,\nChina, in 2020. She is now pursuing\nthe MS degree at Guangdong University\nof Foreign Studies, Guangzhou, China.\nShe has published papers in Journal of\nIntelligent and Fuzzy Systems. She is mainly\nengaged in data mining and natural language processing."
}