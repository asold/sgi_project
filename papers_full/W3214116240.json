{
  "title": "Implicit Transformer Network for Screen Content Image Continuous Super-Resolution",
  "url": "https://openalex.org/W3214116240",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A684921559",
      "name": "Yang Jing-yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120683661",
      "name": "Shen, Sheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753224522",
      "name": "Yue, Huanjing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2002222866",
      "name": "Li Kun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963926543",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2607041014",
    "https://openalex.org/W3109585842",
    "https://openalex.org/W3035591705",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W3032958975",
    "https://openalex.org/W3117476483",
    "https://openalex.org/W2020399841",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W2964101377",
    "https://openalex.org/W1561538450",
    "https://openalex.org/W2963627347",
    "https://openalex.org/W3163146604",
    "https://openalex.org/W3083579885",
    "https://openalex.org/W2546641693",
    "https://openalex.org/W2242218935",
    "https://openalex.org/W2962849139",
    "https://openalex.org/W3034395814",
    "https://openalex.org/W3103242287",
    "https://openalex.org/W3103313582",
    "https://openalex.org/W2963031226",
    "https://openalex.org/W2560428775",
    "https://openalex.org/W2964265128"
  ],
  "abstract": "Nowadays, there is an explosive growth of screen contents due to the wide application of screen sharing, remote cooperation, and online education. To match the limited terminal bandwidth, high-resolution (HR) screen contents may be downsampled and compressed. At the receiver side, the super-resolution (SR) of low-resolution (LR) screen content images (SCIs) is highly demanded by the HR display or by the users to zoom in for detail observation. However, image SR methods mostly designed for natural images do not generalize well for SCIs due to the very different image characteristics as well as the requirement of SCI browsing at arbitrary scales. To this end, we propose a novel Implicit Transformer Super-Resolution Network (ITSRN) for SCISR. For high-quality continuous SR at arbitrary ratios, pixel values at query coordinates are inferred from image features at key coordinates by the proposed implicit transformer and an implicit position encoding scheme is proposed to aggregate similar neighboring pixel values to the query one. We construct benchmark SCI1K and SCI1K-compression datasets with LR and HR SCI pairs. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods for both compressed and uncompressed SCIs.",
  "full_text": "Implicit Transformer Network for Screen Content\nImage Continuous Super-Resolution\nJingyu Yang1 Sheng Shen1 Huanjing Yue1∗ Kun Li2\n1School of Electrical and Information Engineering, Tianjin University\n2College of Intelligence and Computing, Tianjin University\n{yjy, codyshen, huanjing.yue, lik}@tju.edu.cn\nhttps://github.com/codyshen0000/ITSRN\nAbstract\nNowadays, there is an explosive growth of screen contents due to the wide ap-\nplication of screen sharing, remote cooperation, and online education. To match\nthe limited terminal bandwidth, high-resolution (HR) screen contents may be\ndownsampled and compressed. At the receiver side, the super-resolution (SR)\nof low-resolution (LR) screen content images (SCIs) is highly demanded by the\nHR display or by the users to zoom in for detail observation. However, image\nSR methods mostly designed for natural images do not generalize well for SCIs\ndue to the very different image characteristics as well as the requirement of SCI\nbrowsing at arbitrary scales. To this end, we propose a novel Implicit Transformer\nSuper-Resolution Network (ITSRN) for SCISR. For high-quality continuous SR at\narbitrary ratios, pixel values at query coordinates are inferred from image features\nat key coordinates by the proposed implicit transformer and an implicit position\nencoding scheme is proposed to aggregate similar neighboring pixel values to the\nquery one. We construct benchmark SCI1K and SCI1K-compression datasets with\nLR and HR SCI pairs. Extensive experiments show that the proposed ITSRN\nsigniﬁcantly outperforms several competitive continuous and discrete SR methods\nfor both compressed and uncompressed SCIs.\n1 Introduction\nNowadays, screen content images are becoming ubiquitous due to the wide application of screen\nsharing and wireless display. Meanwhile, due to the limited bandwidth, screen content images\nreceived by users may be in low-resolution (LR) and users may need to zoom in the content for detail\ninspection. Therefore, screen content image super-resolution (SCI SR) is to improve the quality of\nLR SCIs.\nHowever, different from natural scene images, SCIs are dominated by the contents generated or\nrendered by computers, such as texts and graphics. Such contents highly demanded are characterized\nby thin and sharp edges, little color variance, and high contrast. In contrast, the natural scene images\nare relatively smooth, and contain rich colors and textures. Conventional image SR methods designed\nfor nature images are good at modeling the local smoothness of natural images other than the thin\nand sharp edges in SCIs. Very recently, Wanget al.[1] proposed a SR method for compressed screen\ncontent videos, which addressed the compression artifacts of screen content videos by introducing a\ndistortion differential guided reconstruction module. However, their network is still composed of\nfully convolution layers without designing speciﬁc structures for the thin and sharp edges in screen\ncontents. In addition, they utilize previous frames to help reconstruct the current frame, which makes\nit unsuitable for frame-wise SCI SR.\n∗Corresponding author: Huanjing Yue\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2112.06174v1  [cs.CV]  12 Dec 2021\nFigure 1: Comparison of the proposed Implicit Transformer SR Network (ITSRN) with state-of-\nthe-art image continuous magniﬁcation methods. The ground truth (GT), which has the same\nresolution with that of the ×4 upsampling, is visualized at the top row, and its magniﬁcation results\n(×2.3,×5.125,×10) are obtained by bicubic-interpolation.\nOn the other hand, conventional SR methods are designed for discrete ( i.e., several ﬁxed) magni-\nﬁcation ratios [ 2, 3, 4], making them hard to ﬁt screens with various sizes. Recently, a few SR\nmethods for continuous magniﬁcation have been proposed [5, 6]. Hu et. al.[5] proposed to perform\narbitrary-scale SR with a learnable up-sampling weight matrix based on meta-learning. The work\nLIIF [ 6] introduced the concept of implicit function [ 7, 8, 9] to image SR. The implicit function,\nwhich attempts to represent images with continuous coordinates and directly maps the coordinates to\nvalues, enables continuous magniﬁcation.\nIn this work, we observe that convolution ﬁlters could be harmful to sharp and thin edges in SCIs since\nthe weight sharing strategy makes them tend to produce a smooth reconstruction result. Therefore,\nwe propose to render the pixel values by a point-to-point implicit function, which adapts to image\ncontent according to image coordinates and pixel features. Fortunately, this also enables us to perform\ncontinuous magniﬁcation for SCIs. We would like to point out that even with the point-to-point\nimplicit function, reconstructing dense edges are still quite challenging. As shown in Fig. 1, LIIF\n[7] cannot reconstruct the dense edges well since it directly concatenates the pixel coordinates and\nfeatures together to predict the pixel values, which is not optimal since the two variables have different\nphysical meanings. As a departure, we reformulate the interpolation process as a transformer and\nintroduce implicit mapping to model the relationship between pixel coordinates, which are used to\naggregate the pixel feature. Our main contributions are summarized as follows.\n• First, we propose a novel Implicit Transformer Super-Resolution Network(ITSRN) for SCI\nSR. The LR and HR image coordinates are termed as the “key” and “query”, respectively.\nCorrespondingly, the LR image pixel features are termed as “value”. In this way, we can\ninfer pixel values by an implicit transformer, where implicit means we model the relationship\nbetween LR and HR images in terms of coordinates instead of pixel values.\n• Second, instead of directly concatenating the coordinates and pixel features to predict the\npixel value, we propose to predict the transform weights with query and key coordinates via\nnonlinear mapping, which are then used to transform the pixel features to pixel values. In\n2\naddition, we propose an implicit position encoding to aggregate similar neighboring pixel\nvalues to the central pixel.\n• Third, we construct a benchmark dataset with various screen contents for SCI SR. Extensive\nexperiments demonstrate that the proposed method outperforms the competitive continuous\nand discrete SR methods for both compressed and uncompressed screen content images.\nFig. 1 presents an example of our SR results, which demonstrates that the proposed method\nis good at reconstructing thin and sharp edges for various magniﬁcation ratios.\n2 Related Work\n2.1 Screen Content Processing\nThe screen content is generally dominated by texts and graphics rendered by computers, making the\npixel distribution of screen contents totally different from that of natural scenes. Therefore, many\nworks speciﬁcally designed for screen contents are proposed, such as screen content image quality\nassessment [ 10, 11, 12, 13], screen content video (image) compression [14, 15]. However, there is\nstill no work exploring screen content image SR. Very recently, Wang et al. [1] proposed screen\ncontent video SR, which reconstructed the current frame by taking advantage of the correlations\nbetween neighboring frames, making it cannot deal with image SR. In addition, its main motivation is\nsolving the SR problem when the videos are degraded by compression other than designing speciﬁc\nstructures for continuously recovering thin and sharp edges in screen contents. In this work, we\naddress this issue by introducing point-to-point implicit transformation.\n2.2 Continuous Image Super-Resolution\nImage SR refers to the task of recovering HR images from LR observations. Many deep learning based\nmethods have been proposed for super-resolving the LR image with a ﬁxed scale [16, 3, 2, 17, 18, 4,\n19, 20]. Since the screen contents are usually required to be displayed on screens with various sizes.\nTherefore, continuous SR is essential for screen contents. In recent years, several continuous image\nSR methods [5, 6] are proposed in order to achieve arbitrary resolution SR. MetaSR [5] introduces a\nmeta-upscale module to generate continuous magniﬁcation but it has limited performance in dealing\nwith out-of-training-scale upsampling factors. LIIF [6] reformulates the SR process as an implicit\nneural representation(INR) problem, which achieves promising results for both in-distribution and\nout-of-distribution upsampling ratios. Inspired by LIIF, we utilize the point-to-point implicit function\nfor SCI SR.\n2.3 Implicit Neural Representation\nImplicit Neural Representation (INR) usually refers to continuous and differentiable function (e.g.,\nMLP), which can map coordinates to a certain signal. INR was widely used in 3D shape modeling [21,\n22, 23, 24], volume rendering (i.e., neural radiance ﬁelds(Nerf)) [9, 25], and 3D reconstruction [8, 26].\nVery Recently, LIIF [6] was proposed for continuous image representation, in which networks took\nimage coordinates and the deep features around the coordinate as inputs, and then map them to the\nRGB value of the corresponding position. Inspired by LIIF, we propose an implicit transformer\nnetwork to achieve continuous magniﬁcation while retaining the sharp edges of SCIs well.\n2.4 Positional Encoding (Mapping)\nPositional encodingis critical to exploit the position order of the sequence in transformer networks\n[27, 28, 29]. We have to utilize positional encoding to indicate the order of the sequence since\nthere are no other modules to model the relative or absolute position information in the sequence-\nto-sequence model. In the literature, sine and cosine functions are used for positional encoding in\ntransformer [27]. Coincidentally, we ﬁnd that the Fourier feature (combined with a set of sinusoids)\nbased positional mappingis used in the implicit function to improve the convergence speed and\ngeneralization ability [30, 9, 31]. Speciﬁcally, a Fourier feature mapping is applied on the input\ncoordinates to map them to a higher dimensional hypersphere before going through the coordinate-\nbased MLP. Although the two schemes are proposed based on different motivations, they show\nsigniﬁcant effects in representing position information, which further boost the ﬁnal results. Inspired\n3\nby them, we propose implicit positional encoding to model the relationship between neighboring\npixel values. Here \"implicit\" means that we do not explicitly encode (map) the coordinates but encode\nthe pixel values of neighboring coordinates.\nFigure 2: The framework of our proposed ITSRN. The input LR image ﬁrst goes through a CNN\nbackbone to generate pixel features. Then the features and key coordinates are upsampled by nearest\nneighbor interpolation based on query coordinates. Hereafter, we utilize the proposed implicit\ntransformer to learn the transform weights with query, key coordinates and scale token, and the pixel\nvalue is obtained via transforming the pixel features with transform weights. Finally, the pixel value\nis further reﬁned by the proposed implicit position encoding. The symbols ↑⃝, -⃝, c⃝, and ×⃝refer to\nupsampling, subtraction, concatenation, and matrix product operations respectively.\n3 Approach\nFigure 2 illustrates the framework of the proposed ITSRN. In the following, we give details for the\nproposed implicit transformer network and implicit position encoding.\n3.1 Implicit Transformer Network\nLet’s ﬁrst review the process of image interpolation. Suppose we have an LR imageIL that needs\nto be upsampled. The pixel value of query point q(i,j) in the HR image IH is obtained by fusing\npixel values of its closest key points k(i′,j′) in IL with a weighting matrix. Denoted by Qthe query\npoints in upsampled image, Kthe key points in the input LR image, and V the feature values on the\ncorresponding key points. Then, the image interpolator can be reformulated as a transformer [27].\nDifferent from the explicit transformer which takes pixel values as Q and K, the interpolation\ntransformer deals with pixels’ coordinates instead of their values. Inspired by the implicit function in\nNeRF [9], which uses the pixel coordinates to generate RGB values, we reformulate the interpolation\nprocess as Implicit Transformer, and propose a novel Implicit Transformer Network for SCI SR.\nThe super-resolution process as well as image interpolation is reformulated in terms of implicit\nfunction as\nIq = Φ(q,k,v ), (1)\nwhere Iq is the target RGB value that need to be predicted, qis the query coordinate(s) in IH, kis the\nkey coordinate(s) in IL, and vis the pixel value(s) or feature(s) corresponding to the key coordinate(s).\nNote that both qand kare coordinates in the continuous image domain. Φ is a mapping function\nwhich maps coordinates and features to RGB values. In image interpolation, kis the neighboring key\ncoordinate of q, vare the pixel value of k, Φ is the weighting matrix. In implicit function based SR\nmethod LIIF [6], kis the nearest neighbor coordinate in IL for qin IH, and vis the corresponding\npixel feature of k. LIIF directly concatenates v and the relative coordinate of q from k, and then\nutilize the nonlinear mapping Φ realized by a multi-layer perceptron (MLP) to render the pixel value\nIq. It achieves promising results due to the strong ﬁtting ability of the MLP. However, we observe\nthat directly concatenating the pixel feature and the relative coordinates is not optimal since they\n4\nhave different physical meanings. To solve this problem, following the idea of transformer [27], we\nreorganize Formula (1) as follows.\nIq = Φ(q,k,v ) = φ(q,k)v. (2)\nDifferent from the explicit transformer, here qand kare pixel coordinates other than pixel values.\nIn other words, we are not learning the the RGB →RGBmapping, but the coordinate→RGB\nmapping. Due to the continuity of coordinates, φ(·) is a continuous projection function, which\ncomputes the transform weights φ(q,k) to aggregate the feature v, i.e., Iq is computed with the\nmultiplication of φand v. To further illustrate it, we decompose φas:\nφ(q,k) = f(δ(q,k)), (3)\nwhere the function δproduces a vector that represents the relationship between qand k. Then the\nmapping function f projects this vector into another vector, which is then multiplied with feature v\nas indicated in Eq. 2.\nThere are many ways to model the relation function δ, such as dot product and hadamard product\ncommonly used in transformers [ 27]. Different from their approaches, in this paper, we utilize\nsubtraction operation, i.e.,\nδ(q,k) = γ(q) −τ(k), (4)\nwhere γ and τ can use trainable functions or identity mapping. In this work, we utilize identity\nmapping since it generates similar results as that of trainable functions.\nInspired by ViT’s [32] [class] token, which is extra global information for classiﬁcation, we augment\nthe input coordinates with scale information, and name it as [scale] token. It represents the global\nmagniﬁcation factor. With the [scale] token, the implicit transformer can take the shape of the query\npixel as additional information for reconstructing the target RGB value. Although it is feasible to\npredict RGB values without [scale] token, it is not optimal since the predicted RGB value should\nnot be completely independent of its shape 2. The output of δ(q,k) is concatenated with the [scale]\ntoken, and is then fed to the mapping function f. In this way, φin Formula (3) is reformulated as\nφ(q,k) = f([δ(q,k),s]), (5)\nwhere s= (sh,sw) is a two-dimensional scale token, representing the upsampling scale along the\nrow and column for the query pixel. [δ(q,k),s] means the concatenation of δ(q,k) and salong the\nchannel dimension. Note that, to avoid large coordinates in HR space, we normalize the coordinates\nas\np(i,j) = [−1 + 2i−1\nH ,−1 + 2i−1\nW ],i ∈[0,H −1],j ∈[0,W −1], (6)\nwhere (i,j) represents the spatial location within the RH×W space.\n3.2 Implicit Position Encoding\nAlthough we have considered the positional relationship between qand k, we still ignore the relative\nposition of different query points. Therefore, inspired by the position encoding in explicit transformer,\nwe propose to introduce implicit position encoding (IPE) to avoid the discontinuity of neighboring\npredictions. Here, \"Implicit Position Encoding\" means that we do not explicitly encode the positional\nrelationship among the pixel coordinates in the Qsequence since they are already absolute position\ncoordinates. On the contrary, we encode the pixel value relationships within a local neighborhood.\nChu et al.claimed that convolution, which models the relationship between neighboring pixels, could\nbe considered as an implicit position encoding scheme [33]. Therefore, we termed Eq.7 as implicit\nposition encoding. Speciﬁcally, we unfold the query within a local window. The ﬁnal predict value of\nthe query coordinate is conditioned on its neighbor values. The IPE process is denoted as\nˆIq =\n∑\np∈Ω(q)\nw(p,q)Iq, (7)\nwhere ˆIq is the reﬁned pixel value, Ω(q) is a local window centered at q, wrepresents the neighbors’\n(denoted by p) contribution to the target pixel. In traditional image ﬁltering, w(p,q) is generally\nrealized by a Gaussian ﬁlter or bilateral ﬁlter. In this work, we utilize an MLP to learn adaptive\nweighting parameters for each q.\n2It is surprised to ﬁnd that our formulation in terms of transformer for the [scale] token is similar to the cell\ndecoding strategy in LIIF.\n5\nBicubic\n(23.41 / 0.8239)\nRDN\n(26.51 / 0.9455)\nRCAN\n(26.78 / 0.9497)\nMetaSR\n(26.66 / 0.9468)\nLIIF\n(26.71 / 0.9519 )\nITSRN(Ours)\n(27.23 / 0.9574 )\nGT\n(PSNR / SSIM )\nBicubic\n(24.50 / 0.8274)\nRDN\n(31.04 / 0.9725)\nRCAN\n(32.15 / 0.9784)\nMetaSR\n(33.00 / 0.9828)\nLIIF\n(33.05 / 0.9829 )\nITSRN(Ours)\n(33.87 / 0.9857 )\nGT\n(PSNR / SSIM)\nFigure 3: Visual comparison with state-of-the-arts at ×4 SR.\n4 Architecture Details of ITSRN\nAs shown in Figure 2, the proposed ITSRN has three parts, i.e., a CNN backbone to extract compact\nfeature representations for each pixel, an implicit transformer for mapping coordinates to target\nvalues, and an implicit position encoding that further enhances the target values. In the following, we\ngive details for the three modules.\nBackbone. Given an LR image IL ∈ R3×h×w, we utilize a CNN to extract its feature map\nV ∈Rc×h×w. In our experiments, c= 64. Normally, any CNN without downsampling / upsampling\ncan be adopted as the feature extraction backbone. To be consistent with LIIF [6], we utilize RDN [18]\n(excluding its up-sampling layers) as the backbone. The extracted features V will be used in the\nfollowing implicit transformer.\nImplicit Transformer. First, inspired by LIIF [6] and MetaSR [5], to enlarge each feature’s local\nreceptive ﬁeld, we apply feature unfolding, namely concatenating the features for the pixels in a\nlocal region (3 ×3 in this work) to get v′∈Rc×9×1×1. After that, v′replaces vfor the following\nprocesses. All the key coordinates corresponding to the feature vectors construct a coordinate matrix\nK ∈R2×h×w with the same spatial shape as V. The channel dimension is 2, which includes the\nrow and column coordinates. Similarly, the query coordinates build a matrix Q∈R2×H×W , where\nH×W is larger than h×w. Therefore, we ﬁrst utilize the nearest neighbor interpolation to upsample\nthe coordinates Kto the size of Q. Hereafter, we utilize an MLP as the nonlinear mapping function\nf(·) (mentioned in Eq. 3) to learn the relationship between the coordinate pairs in upsampled Kand\nQ. As demonstrated in Eq. 5, f(·) maps the 4-dimensional (coordinates and scale token) input to a\n9c-dimensional output(9c×3 for RGB channels). Finally, the 9c-dimensional output is multiplied by\nthe corresponding pixel feature v, generating the coarse result for Iq.\nImplicit Position Encoding. After obtaining the coarse value Iq, we further utilize IPE to reﬁne it.\nIn this way, the central pixel can be more continuous with its neighbors. As mentioned in Eq. 7, we\nset Ω to 3 ×3 and wfor each point is learned via an MLP. The MLP includes two layers,i.e., { Linear\n→Gelu[27] →Linear }, and the numbers of neurons for the two layers are 256 and 1, respectively.\n5 Experiments\n5.1 Datasets\nTo the best of our knowledge, there is still no SCI SR dataset for public usage. Therefore, we ﬁrst\nbuild a dataset named SCI1K. It contains 1000 screenshots with various screen contents, including but\nnot limited to web pages, game scenes, cartoons, slides, documents, etc. Among them, 800 images\nwith 1280×720 are used for training and validation. The other 200 images with resolution ranging\nfrom 1280×720 to 2560×1440 are used for testing. To be consistent with previous works [4, 5, 6, 18],\n6\nInput (48px) MetaSR [5] LIIF[6] ITSRN(480px)\nFigure 4: Visual comparison with state-of-the-arts for arbitrary SR results. The input is a 48 ×48\npatch from an image in SCID [12] test set. All the three models are trained with continuous scales in\nthe range ×1 ∼×4 and are tested for ×10 magniﬁcation. Note that the LR input is generated with\n×4 downsampling, and there is no groud truth for its ×10 magniﬁcation.\nwe utilize bicubic downsampling to synthesize the LR images. In addition, to simulate the degradation\nintroduced in transmission and storage, we build another dataset, named SCI1K-compression, by\nutilizing JPEG compression to further degrade the LR images. The quality factor of JPEG is randomly\nselected from 75, 85, and 95.\nTo evaluate the generalization of the trained model, besides our test set, we also test on two other\nscreen content datasets constructed for image quality assessment,i.e., SCID (including 40 images with\nresolution 1280×720 ) [12] and SIQAD (including 20 images with resolution around 600×800)[13].\n5.2 Training Details\nIn the training phase, to simulate continuous magniﬁcation, the downsampling scale is sampled in\na uniform distribution U(1,4). We then randomly crop 48 ×48 patches from the LR images and\naugment them via ﬂipping and rotation.\nFollowing [18], we utilize the ℓ1 distance between the reconstructed image and the ground truth as the\nloss function. The Adam [34] optimizer is used with beta1=0.9 and beta2=0.999. All the parameters\nare initialized with He initialization and the whole network is trained end-to-end. Following [6], the\nlearning rate starts with 1e−4 for all modules and decays in half every 200 epochs. We parallelly\nrun our ITSRN-RDN on two GeForce GTX 1080Ti GPU with mini-batch size 16 and it cost 2 days\nto reach convergence (about 500 epochs). During the test, we directly feed the whole LR image into\nour network (as long as the graphic memory is sufﬁcient) to generate the SR result.\n5.3 Comparison with State-of-the-arts\nTo demonstrate the effectiveness of the proposed SR strategy, we compare our method with state-of-\nthe-art continuous SR methods, i.e., MetaSR [5] and LIIF [6]. We also compare with the discrete\nSR methods, i.e., RDN [18] and RCAN [4], which are state-of-the-art natural image SR methods.\nSince RDN and RCAN rely on speciﬁc up-sampling modules, they have to train different models\nfor different upsampling scales and cannot be tested for the scales not in the training set. For a\nfair comparison, we retrain all the compared methods using our training set with the recommended\nparameters and codes released by the authors. Note that, during test, we downsample the ground\ntruth with different ratios to generate the LR inputs for different magniﬁcation ratios. For larger\nmagniﬁcation ratios, the details are fewer in its corresponding LR input.\nTable 1 and Table 2 present the quantitative comparison results on different datasets. Table 1 lists the\naverage SR results for 200 screen content images in our SCI1K and SCI1K-compression test sets.\n7\nTable 1: Quantitative comparison on SCI1K and SCI1K-compression test sets in terms of PSNR\n(dB). The best (second best) results are in red (blue). RDN [18] and RCAN [4] use different models\nfor different upsampling scales. MetaSR [ 5], LIIF [6] and ITSRN(ours) use one model for all the\nupsampling scales, and the three models are trained with continuous random scales uniformly sampled\nfrom ×1 ∼×4.\nMethod\nDataset: SCI1K Dataset: SCI1K-compression\nIn-training-scale Out-of-training-scale In-training-scale Out-of-training-scale\n×2 ×3 ×4 ×5 ×7 ×9 ×2 ×3 ×4 ×5 ×7 ×9\nBicubic [18] 28.81 25.15 23.18 22.02 20.72 19.96 28.28 24.87 22.99 21.84 20.58 19.84\nRDN [18] 38.45 33.59 29.81 - - - 35.16 30.60 27.17 - - -\nRCAN [4] 38.61 33.91 30.80 - - - 35.25 31.15 27.78 - - -\nMetaSR-RDN [5]38.57 33.67 30.12 27.52 23.91 22.02 35.20 30.96 27.63 25.31 22.57 21.30\nLIIF-RDN [6] 38.65 33.97 30.55 27.77 23.99 22.18 35.43 31.07 27.69 25.27 22.59 21.36\nITSRN-RDN(Ours)38.74 34.32 30.82 28.15 24.36 22.36 35.53 31.31 28.02 25.62 22.79 21.45\nTable 2: Quantitative evaluation on SCI quality assessment datasets in terms of PSNR (dB). The best\n(second best) results are in red (blue). RDN [18] and RCAN [4] train different models for different\nupsampling scales. The rest methods train one model for all the upsampling scales. All the models\nare trained on the SCI1K training set.\nDataset Method In-training-scale Out-of-training-scale\n×2 ×3 ×4 ×5 ×6 ×7 ×8 ×9 ×10\nSCID [12]\nRDN [18] 34.00 28.34 25.74 - - - - - -\nRCAN [4] 33.90 28.98 26.02 - - - - - -\nMetaSR-RDN [5]33.84 29.08 25.76 23.62 22.38 21.59 21.07 20.71 20.41\nLIIF-RDN [6] 34.24 29.10 25.89 23.77 22.53 21.73 21.21 20.84 20.54\nITSRN-RDN 34.19 29.46 26.22 23.96 22.64 21.80 21.26 20.87 20.56\nSIQAD [13]\nRDN [18] 33.53 26.89 23.38 - - - - - -\nRCAN [4] 32.87 27.27 23.69 - - - - - -\nMetaSR-RDN [5]34.12 28.40 23.55 21.18 20.18 19.63 19.25 18.94 18.65\nLIIF-RDN [6] 34.31 28.27 23.44 21.16 20.25 19.70 19.36 19.02 18.70\nITSRN-RDN 34.68 29.07 24.03 21.44 20.38 19.77 19.40 19.09 18.79\nAll the methods are retrained on the corresponding training sets of SCI1K and SCI1K-compression.\nIt can be observed that our method consistently outperforms all the compared methods. Compared\nwith RDN, which is our backbone, our method achieves nearly 1 dB gain at ×4 SR on SCI1K test\nset. Compared with the competitive RCAN, our method still achieves nearly 0.4 dB gain at ×3\nupsampling for the uncompressed dataset. Note that at ×4 SR, the result of our method is similar\nas that of RCAN. The main reason is that our backbone RDN generates much worse results than\nRCAN at ×4 SR. If we change our backbone to more powerful structures, our results could also be\nfurther improved. For the scales that are not used in the training process (denoted by out-of-training-\nscales), RDN and RCAN are not applicable. Meanwhile, our method outperforms the continuous\nSR methods (i.e., MetaSR and LIIF), which demonstrates that the proposed implicit transformer\nscheme is superior in modeling both coordinates and features. Table 2 presents the SR results on\ntwo SCI quality assessment datasets. Since the images in the two datasets are not compressed, we\ndirectly utilize the models trained on SCI1K training set to test. It can be observed that our method\nstill outperforms the compared methods.\nBesides visual results in Fig. 1, Fig. 3 presents an example of the qualitative comparison results on\nSCI1K test set3. It can be observed that our method recovers more realistic edges of characters than\nthe compared methods. Fig. 4 presents the visual results for ×10 SR. It can be observed that our\nmethod reconstructs the thin edges better than the compared methods at large magniﬁcation ratios.\n5.4 Ablation Study Results\nIn this section, we perform ablation study to demonstrate the effectiveness of the proposed modules.\nTable 3 lists quantitative comparison results on SCI1K test set. It can be observed that the scale\ntoken and implicit position encoding totally contributes 0.39 dB to the ﬁnal SR result at ×4 SR. Even\n3More visual comparison results are presented in the supplementary ﬁle.\n8\nTable 3: Ablation study for scale token and implicit position encoding. The PSNR/SSIM results are\nthe averaging results on all the SCI1K test images.\nScale token × ✓ × ✓\nimplicit position encoding × × ✓ ✓\nIn-training-scale(×4) PSNR 30.43 30.60 30.76 30.82\nSSIM 0.9329 0.9351 0.9353 0.9364\nOut-of-training-scale(×6) PSNR 25.94 25.95 26.05 26.00\nSSIM 0.8686 0.8715 0.8725 0.8746\nfor the out-of-training-scales, the two modules also contribute 0.06 dB gain. Note that, the gain is\nlower for larger magniﬁcation factor is because that the thin edges in screen contents are hardly to be\ndistinguished after ×6 downsampling. Therefore, it is difﬁcult to bring gains with scale token and\nposition encoding.\nTable 4: Comparison of using different realizations of the weight w in IPE. \"Fixed\" means w is\ncalculated based on the spatial distance between the query and its neighbors.\"Learned\" refers to\nlearning wvia an MLP, which is used in this work.\nWeight win IPE In-training-scale Out-of-training-scale\n×2 ×3 ×4 ×5 ×7 ×9 ×24 ×40\nFixed 35.27 31.03 27.78 25.47 22.78 21.45 18.42 17.31\nLearned 35.53 31.31 28.02 25.62 22.79 21.45 18.40 17.29\nTable 5: Comparison of SR performance with different training scales.\nScale ×4 ×6 ×8 ×10\nTraining scale ×2-×4 30.82 26.00 23.16 21.77\nTraining scale ×2-×8 30.83 26.37 23.59 21.86\nWe also conduct ablation study on the win IPE by changing it to different realizations. Table 4 lists\nthe SR results on SCI1K-compression test set. From the table, we can observe that our proposed\nlearnable scheme is superior to the ﬁxed scheme when the input LR images suffer from JPEG\ncompression. This is because that, with JPEG compression, the neighboring information can help\nalleviate the compression artifacts and the ﬁxed distance based weighting strategy is not suitable in\nthis case. Note that, for very large magniﬁcation ratios, the learned wis slightly inferior to the ﬁxed\nwby 0.02 dB since the heavy distortions in the LR input may confuse the learning process. Besides,\nwe conduct ablation study on the training scales. As shown in Table 5, the SR results at ×6 and ×8\nupsampling with training scales ×2 −×8 are better than that with training scale ×2 −×4. The main\nreason is that the scales of ×6 and ×8 are in the range of the second training scales. In addition, the\nsecond strategy is also beneﬁcial for ×10 upsampling. This indicates us that using wide distributed\ntraining scales is better than using narrow distributed training scales.\n5.5 Discussion\nThe most related work to our proposed ITSRN is LIIF [ 6], which utilizes implicit function for\ncontinuous SR. However, it directly concatenates the image coordinate and its CNN feature, and\nthen map them to the RGB value via an MLP. Different from it, we reformulate the SR process as\nan implicit transformer. There are two MLPs in our scheme. The ﬁrst MLP is utilized to model the\nrelationship between pixel coordinates. Then the \"relationship” is multiplied by the pixel features\nto generate the pixel value, which can also be termed as an attention strategy. The second MLP is\nutilized for implicit position encoding, which is useful to keep the continuous of the reconstructed\npixels. The experiment results also demonstrate that the proposed two strategies make our method\nconsistently outperform LIIF on all the four test sets.\n9\n6 Conclusion\nIn this paper, we propose a novel arbitrary-scale SR method for screen content images. With the\nproposed implicit transformer and implicit position encoding modules, the proposed method achieves\nthe best results on four datasets at various magniﬁcation ratios. Due to the continuous magniﬁcation\nability, our method enables users to display the received screen contents on screens with various sizes.\nIn addition, we construct the ﬁrst SCI SR dataset, which will facilitate more research on this topic.\nNote that, although our method outperforms state-of-the-arts on screen content images, it may not\nwork the best for natural images at a ﬁxed magniﬁcation ratio. The main reason is that our method is\ndesigned for SCIs with high contrast and dense edges, which are suitable to be modeled by point-to-\npoint mapping. Besides, we simulate the degradation process with bicubic and JPEG compression,\nwhich may be different from the actual degradation in transmission and acquisition. Thus, there will\nbe limitation in practical applications. In the future, we would like to develop blind distortion based\nSCI SR to make our model adapt to real scenarios better.\nBroader Impact\nThis work is an exploratory work on screen content images super-resolution with arbitrary-scale\nmagniﬁcation. The constructed dataset can facilitate research on this topic. In addition, the proposed\nmethod can be combined with image (video) compression technology to enable screen contents\ntransmission with limited bandwidth. As for societal inﬂuence, this work will improve the quality of\npictures displayed on the screen of any resolution. However, We would like to point out that SR is\nactually predicting (hallucinating) new pixels, which may make the image deviate from the ground\ntruth. Therefore, image SR has a weak link with deep fakes. It is worth noting that the positive social\nimpact of this technology far exceeds the potential problems. We call on people to use this technology\nand its derivative applications without harming the personal interests of the public.\nAcknowledgments\nThe authors would like to thank the anonymous reviewers for their valuable comments. This research\nwas supported in part by the National Natural Science Foundation of China under Grant 62072331,\nGrant 62171317, and Grant 61771339.\nReferences\n[1] Meng Wang, Jizheng Xu, Li Zhang, Junru Li, and Shiqi Wang. Super resolution for compressed\nscreen content video. In 2021 Data Compression Conference (DCC), pages 173–182, 2021.\n[2] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced deep residual networks for single\nimage super-resolution. In 2017 IEEE Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPRW), 2017.\n[3] J. Kim, J. K. Lee, and K. M. Lee. Accurate image super-resolution using very deep convolutional\nnetworks. In IEEE Conference on Computer Vision Pattern Recognition, 2016.\n[4] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-\nresolution using very deep residual channel attention networks. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 294–310, 2018.\n[5] Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang, Tieniu Tan, and Jian Sun. Meta-sr:\nA magniﬁcation-arbitrary network for super-resolution. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 1575–1584, 2019.\n[6] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with\nlocal implicit image function. arXiv preprint arXiv:2012.09161, 2020.\n[7] K. O. Stanley. Compositional pattern producing networks: A novel abstraction of development.\nGenetic Programming and Evolvable Machines, 8(2):131–162, 2007.\n[8] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.\nOccupancy networks: Learning 3d reconstruction in function space. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4460–4470, 2019.\n10\n[9] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and N. Ren. Nerf:\nRepresenting scenes as neural radiance ﬁelds for view synthesis. In European Conference on\nComputer Vision, 2020.\n[10] Huan Yang, Yuming Fang, and Weisi Lin. Perceptual quality assessment of screen content\nimages. IEEE Transactions on Image Processing, 24(11):4408–4421, 2015.\n[11] Ke Gu, Guangtao Zhai, Weisi Lin, Xiaokang Yang, and Wenjun Zhang. Learning a blind quality\nevaluation engine of screen content images. Neurocomputing, 196:140–149, 2016.\n[12] Zhangkai Ni, Lin Ma, Huanqiang Zeng, Jing Chen, Canhui Cai, and Kai-Kuang Ma. Esim:\nEdge similarity for screen content image quality assessment. IEEE Transactions on Image\nProcessing, 26(10):4818–4831, 2017.\n[13] Huan Yang, Yuming Fang, and Weisi Lin. Perceptual quality assessment of screen content\nimages. IEEE Transactions on Image Processing, 24(11):4408–4421, 2015.\n[14] Shiqi Wang, Xinfeng Zhang, Xianming Liu, Jian Zhang, Siwei Ma, and Wen Gao. Utility-driven\nadaptive preprocessing for screen content video compression.IEEE Transactions on Multimedia,\n19(3):660–667, 2017.\n[15] Wen-Hsiao Peng, Frederick G. Walls, Robert A. Cohen, Jizheng Xu, Jorn Ostermann, Alexander\nMacInnis, and Tao Lin. Overview of screen content video coding: Technologies, standards, and\nbeyond. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 6(4):393–408,\n2016.\n[16] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional\nnetwork for image super-resolution. In European Conference on Computer Vision, pages\n184–199, 2014.\n[17] W. S. Lai, J. B. Huang, N. Ahuja, and M. H. Yang. Deep laplacian pyramid networks for fast\nand accurate super-resolution. In IEEE Conference on Computer Vision & Pattern Recognition,\npages 5835–5843, 2017.\n[18] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network\nfor image super-resolution. In 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2472–2481, 2018.\n[19] S. Anwar and N. Barnes. Densely residual laplacian super-resolution. IEEE Transactions on\nSoftware Engineering, PP(99), 2020.\n[20] Y . Mei, Y . Fan, Y . Zhou, L. Huang, T. S. Huang, and H. Shi. Image super-resolution with\ncross-scale non-local attention and exhaustive self-exemplars mining. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[21] Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for generative shape modeling. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n5939–5948, 2019.\n[22] Matan Atzmon and Yaron Lipman. Sal: Sign agnostic learning of shapes from raw data. In2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2565–2574,\n2020.\n[23] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep\nimplicit functions for 3d shape. In 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 4857–4866, 2020.\n[24] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.\nDeepsdf: Learning continuous signed distance functions for shape representation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 165–174,\n2019.\n[25] Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, and Andrea Tagliasacchi.\nDerf: Decomposed radiance ﬁelds. arXiv preprint arXiv:2011.12490, 2020.\n[26] Songyou Peng, Michael Niemeyer, Lars M. Mescheder, Marc Pollefeys, and Andreas Geiger.\nConvolutional occupancy networks. In ECCV (3), pages 523–540, 2020.\n[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems, volume 30, pages 5998–\n6008, 2017.\n11\n[28] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina N. Toutanova. Bert: Pre-training\nof deep bidirectional transformers for language understanding. In Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2018.\n[29] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional\nsequence to sequence learning. In International Conference on Machine Learning, pages 1243–\n1252. PMLR, 2017.\n[30] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-\nplicit neural representations with periodic activation functions. Advances in Neural Information\nProcessing Systems, 33, 2020.\n[31] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,\nUtkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let\nnetworks learn high frequency functions in low dimensional domains. In Advances in Neural\nInformation Processing Systems, volume 33, pages 7537–7547, 2020.\n[32] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on Learning Representations, 2021.\n[33] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit\nposition encodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021.\n[34] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International\nConference on Learning Representations, 12 2014.\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7737939357757568
    },
    {
      "name": "Zoom",
      "score": 0.6332435011863708
    },
    {
      "name": "Uncompressed video",
      "score": 0.5817572474479675
    },
    {
      "name": "Computer vision",
      "score": 0.529065728187561
    },
    {
      "name": "Transformer",
      "score": 0.5006272792816162
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47602158784866333
    },
    {
      "name": "Pixel",
      "score": 0.45823079347610474
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.3914618492126465
    },
    {
      "name": "Video processing",
      "score": 0.10459038615226746
    },
    {
      "name": "Geology",
      "score": 0.08452373743057251
    },
    {
      "name": "Petroleum engineering",
      "score": 0.0
    },
    {
      "name": "Video tracking",
      "score": 0.0
    },
    {
      "name": "Lens (geology)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": []
}