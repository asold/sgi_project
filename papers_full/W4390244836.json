{
  "title": "Exploring LLM-based Automated Repairing of Ansible Script in Edge-Cloud Infrastructures",
  "url": "https://openalex.org/W4390244836",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5051373234",
      "name": "Sunjae Kwon",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5037298291",
      "name": "Sungu Lee",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5088627145",
      "name": "Taehyoun Kim",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5079355233",
      "name": "Duksan Ryu",
      "affiliations": [
        "Jeonbuk National University"
      ]
    },
    {
      "id": "https://openalex.org/A5046980238",
      "name": "Jongmoon Baik",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2290181636",
    "https://openalex.org/W6610910252",
    "https://openalex.org/W6759591935",
    "https://openalex.org/W6602225820",
    "https://openalex.org/W6603435670",
    "https://openalex.org/W6600164255",
    "https://openalex.org/W6630224890",
    "https://openalex.org/W4282555216",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3036348283",
    "https://openalex.org/W4381486153",
    "https://openalex.org/W4367000100",
    "https://openalex.org/W3118657661",
    "https://openalex.org/W4226332943",
    "https://openalex.org/W4385187421",
    "https://openalex.org/W1946412544",
    "https://openalex.org/W4377372285",
    "https://openalex.org/W3112995305",
    "https://openalex.org/W3013988266",
    "https://openalex.org/W4312438588",
    "https://openalex.org/W4399203759",
    "https://openalex.org/W4380568688",
    "https://openalex.org/W3154201789",
    "https://openalex.org/W4389158474",
    "https://openalex.org/W4385682310",
    "https://openalex.org/W4318832634",
    "https://openalex.org/W4318902699",
    "https://openalex.org/W4377866432",
    "https://openalex.org/W4362598291",
    "https://openalex.org/W4385302156",
    "https://openalex.org/W2993710525",
    "https://openalex.org/W2560855557"
  ],
  "abstract": "Edge-Cloud system requires massive infrastructures located in closer to the user to minimize latencies in handling Big data. Ansible is one of the most popular Infrastructure as Code (IaC) tools crucial for deploying these infrastructures of the Edge-cloud system. However, Ansible also consists of code, and its code quality is critical in ensuring the delivery of high-quality services within the Edge-Cloud system. On the other hand, the Large Langue Model (LLM) has performed remarkably on various Software Engineering (SE) tasks in recent years. One such task is Automated Program Repairing (APR), where LLMs assist developers in proposing code fixes for identified bugs. Nevertheless, prior studies in LLM-based APR have predominantly concentrated on widely used programming languages (PL), such as Java and C, and there has yet to be an attempt to apply it to Ansible. Hence, we explore the applicability of LLM-based APR on Ansible. We assess LLMs’ performance (ChatGPT and Bard) on 58 Ansible script revision cases from Open Source Software (OSS). Our findings reveal promising prospects, with LLMs generating helpful responses in 70% of the sampled cases. Nonetheless, further research is necessary to harness this approach’s potential fully.",
  "full_text": "Exploring LLM-based Automated\nRepairing of Ansible Script\nin Edge-Cloud Infrastructures\nSunjae Kwon1, Sungu Lee1, Taehyoun Kim1, Duksan Ryu2,∗\nand Jongmoon Baik1\n1Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea\n2Jeonbuk National University, Jeonju, Republic of Korea\nE-mail: duksan.ryu@jbnu.ac.kr\n∗Corresponding Author\nReceived 07 August 2023; Accepted 29 September 2023;\nPublication 23 December 2023\nAbstract\nEdge-Cloud system requires massive infrastructures located in closer to the\nuser to minimize latencies in handling Big data. Ansible is one of the\nmost popular Infrastructure as Code (IaC) tools crucial for deploying these\ninfrastructures of the Edge-cloud system. However, Ansible also consists of\ncode, and its code quality is critical in ensuring the delivery of high-quality\nservices within the Edge-Cloud system. On the other hand, the Large Langue\nModel (LLM) has performed remarkably on various Software Engineering\n(SE) tasks in recent years. One such task is Automated Program Repairing\n(APR), where LLMs assist developers in proposing code fixes for identified\nbugs. Nevertheless, prior studies in LLM-based APR have predominantly\nconcentrated on widely used programming languages (PL), such as Java\nand C, and there has yet to be an attempt to apply it to Ansible. Hence,\nwe explore the applicability of LLM-based APR on Ansible. We assess\nLLMs’ performance (ChatGPT and Bard) on 58 Ansible script revision\nJournal of Web Engineering, Vol. 22_6, 889–912.\ndoi: 10.13052/jwe1540-9589.2263\n© 2023 River Publishers\n890 S. Kwon et al.\ncases from Open Source Software (OSS). Our findings reveal promising\nprospects, with LLMs generating helpful responses in 70% of the sampled\ncases. Nonetheless, further research is necessary to harness this approach’s\npotential fully.\nKeywords: Edge-cloud, Ansible, Bard, large langue model, automated\nprogram repairing.\n1 Introduction\nThe Edge-Cloud system is a crucial computing infrastructure designed to\nenhance response times for processing Big data through extensively dis-\ntributed infrastructures [1, 4]. This low-latency responsiveness is critical in\nadvancing modern society, particularly in smart cities and factories. Never-\ntheless, the manual deployment of such massive infrastructures is fraught\nwith errors and consumes significant time. Infrastructure-as-Code (IaC)\n[17, 19] is a vital tool empowering developers to efficiently provision and\nmanage the Edge-Cloud system’s massive infrastructures using reusable code\ninstead of manual labor. Ansible is the most popular IaC tool owing to its\nstraightforward functionality and a framework implemented using Python\nand YAML, languages familiar to various users. However, given that Ansible\nis a collection of code, its code quality directly correlates with the quality\nof service within the Edge-Cloud system. Consequently, various studies\nhave been dedicated to ensuring the correctness of Ansible, thereby assuring\nservice quality [5, 7, 10, 22, 24, 31].\nConcurrently, Automated Program Repairing (APR) has emerged as a\ndynamic field among various Software Engineering (SE) tasks. APR seeks\nto automatically furnish potential code patches when confronted with buggy\ncode, offering developers a means to reduce both effort and time spent on\nbug fixes significantly, consequently curtailing software development costs\n[18, 24, 25]. Recently, APR researchers have actively applied the Large Lan-\nguage Model (LLM), a deep learning model pre-trained on billions of text and\ncode tokens, due to its remarkable performance on various SE tasks [2, 9,\n16, 20, 28]. Furthermore, the LLM-based APRs have demonstrated better\nperformance than state-of-the-art APR techniques [8, 24].\nNevertheless, prior LLM-based APR studies evaluated their performance\non widely used Programming Languages (PL), such as Java and C [8, 21,\n24, 26, 29, 30]; Ansible has yet to be studied. In the case of Ansible,\ncompared to Java and C, the availability of training data is notably limited,\nExploring LLM-based Automated Repairing of Ansible Script 891\npotentially impacting the reliability of findings from prior studies. There-\nfore, we explore the applicability of LLM-based APR on Ansible through\nexperiments. We evaluate the performance of the two version of ChatGPT\nand Bard, which are the most actively used LLMs, on 58 Ansible script\nrevision cases from Open Source Software (OSS) repositories enrolled in\nGitHub. Our findings indicate that the three models provide helpful responses\nto solve the cases (i.e., offer valuable insights to address cases) in 41 of 58\ncases. However, the responses provided by individual models remained rela-\ntively modest, underscoring the imperative for further research to establish a\npractical framework for real-world program repair.\nOur contributions are as follows: (1) We evaluate the LLM-based ARP\nperformance on Ansible script for the first time. (2) We curate defective\nAnsible scripts from OSS enrolled in GitHub to confirm the applicability\nof LLM-based ARP. (3) We conduct comparative assessments involving\nthree LLMs, exploring a variety of prompts and defect types, with the aim\nof elucidating strategies for making prompts to effectively guide LLMs in\ngenerating error-free Ansible scripts.\n2 Background and Related Work\n2.1 Edge-Cloud System and IaC\nFigure 1 shows the overall structure of Edge-Cloud system. The main advan-\ntage of Edge-Cloud system is collecting and processing Big data with low\nlatency using more infrastructures, i.e., Edge nodes in Figure 1, geograph-\nically located close to the user or data source [1, 4]. Such locating Edge\nnodes support the implementation of the Internet of Things (IoT) or Internet\n.  \nFigure 1 Overall structure of Edge-Cloud system.1\n1https://www.alibabacloud.com/en/knowledge/what-is-edge-computing.\n892 S. Kwon et al.\nFigure 2 Ansible script example for deploying a webserver.2\nof Everything (IoE), which is challenging to implement within traditional\nCloud systems [1, 4].\nNevertheless, the manual implementation and management of the massive\ninfrastructure required for the Edge-Cloud system can be highly error-prone\nand time-consuming. IaC is a concept designed to automate these laborious\nand error-prone tasks by leveraging reusable code [5, 7, 19]. Thanks to the\nadvantages of IaC, it is widely employed in deploying Cloud infrastructure,\nand the same applies to Edge-Cloud systems.\n2.2 Ansible\nAnsible is one of the most popular IaC tools [5, 6, 17]. Originating as open-\nsource software (OSS), Ansible has evolved into an end-to-end automation\nplatform provided by Red Hat, configuring systems, deploying software, and\norchestrating complex workflow. Figure 2 shows a simple Ansible script\nexample of deploying a webserver. This script employs YAML formatting,\nwith the entire script referred to as a Playbook. Within each Playbook,\nmultiple tasks are defined, each commencing with the ‘ -name’ tag, which\nsuccinctly outlines the task’s purpose in natural language. Although not\nexpressed in the script, each task is executed by internally invoking the cor-\nresponding Python code. Consequently, these tasks are executed sequentially\nin the order of the script.\nWhile Ansible offers substantial conveniences, it is important to recog-\nnize that, like any other software, Ansible is not immune to bugs. These bugs\ncan significantly impact the overall quality of the system. Bugs in Ansible\nhave been associated with a range of Cloud service outages [5, 7, 22, 25, 32].\n2https://www.middlewareinventory.com/blog/ansible-playbook-example/.\nExploring LLM-based Automated Repairing of Ansible Script 893\nResearchers have conducted studies focused on enhancing the quality of\nAnsible scripts to mitigate the risk of such outages. Opdebeeck et al. [22]\nemploy graph algorithms to identify problematic variables within the script,\nreferred to as “smelly variables.” Dalla Palma et al. [5] predict scripts that are\nprone using static analysis results from Ansible scripts. Meanwhile, Kwon\net al. [15] confirm that LLM can identify the Ansible script and recommend\nan error-free version of the Ansible script.\nWe extend Kwon et al.’s study by comparing two prominent Large Lan-\nguage Models (LLMs), ChatGPT and Bard, with various prompt patterns and\nissues. We aim to find the optimal prompt to guide LLMs in resolving issues\non the Ansible script.\n2.3 LLM-based APR\nLarge Language Model (LLM) is a deep learning model pre-trained on\nbillions of text and code tokens, and it generates the most relevant next word\nto a given sequence of words. LLM’s capabilities have significantly advanced\nwith the expansion of training data and the increase in model parameters\n[3, 24]. Recently, LLMs have been fine-tuned by using Reinforcement Learn-\ning from Human Feedback (RLHF) to respond to a wide range of user queries,\nand the representative examples are Bard 3 and ChatGPT.4 While LLMs are\ntypically utilized for general purposes, such as Natural Language Processing\n(NLP), their powerful capabilities have also been found in automating SE\ntasks [2, 9, 14, 16, 28]. Tian et al. [27] demonstrate that ChatGPT can provide\ndevelopers valuable insights for resolving software bugs. Xia et al. [29] utilize\nthe conversational manner of LLM for patch generation and validation. Jin\net al. [8] combine the capabilities of LLM with a static analyzer. It is an end-\nto-end program repair framework that identifies source code bugs to execute\nthe necessary repairs.\nDespite the ongoing research efforts in the field, the prior studies have\nconcentrated on Java and C, which are widely used in software development.\nHowever, research on Ansible has yet to be done. Since the information on\nAnsible was expected to be relatively scarce compared to C and Java when\nlearning the model, there exists the potential for discrepancies in the results\nobtained from previous studies. Consequently, our objective is to evaluate the\nautomated repairing performance of LLMs on Ansible.\n3https://bard.google.com/\n4https://chat.openai.com/\n894 S. Kwon et al.\n3 Approach\n3.1 GitHub Pull-Request (GHPR)\nWe gathered cases to evaluate the performance of Large Language Models\n(LLMs) in Automated Program Repair (ARP) by utilizing the GitHub Pull-\nRequest (GHPR) workflow, which is a standard process on GitHub for\naddressing issues in developing software. Figure 3 provides an overview of\nthe GHPR process. Initially, when a software issue arises, a developer creates\na separate branch from the main branch, which represents the original code\nthread of a project, to address the issue. After resolving the issue, the devel-\noper submits a Pull Request (PR) to the maintainers of the main repository,\nrequesting a review of the modified code. If the maintainers approve the PR,\nthe changes are merged into the main branch. All GHPR-related information\nis stored on GitHub, and this data can be accessed through the GitHub API.\nDue to the ease of collecting information using the GitHub API, several\nstudies have utilized it to gather data for various Software Engineering (SE)\nresearch tasks [12, 13, 15, 16, 31].\nWe gathered cases to evaluate the performance of LLMs in APR by uti-\nlizing GitHub Pull-Request (GHPR), one of GitHub’s workflows, to resolve\nissues on the developing Software. Figure 2 provides the overall process of\nGHPR. Initially, when an issue arises, a developer creates a separate branch\nfrom the main branch, the original thread of code for a project, to resolve\nthe issue. After addressing the issue, the developer submits a Pull Request\n(PR) to the main repository maintainers requesting a review of the modified\ncode. If the maintainers accept the PR, the changed code is merged into the\nmain branch. All GHPR-related information is stored on GitHub, and this\ndata can be accessed through the GitHub API.5 Due to the ease of collecting\ninformation using the GitHub API, several studies have utilized it to gather\ndata for various Software Engineering (SE) research tasks [12, 13, 15, 16, 31].\nWe implemented a Python-based tool for crawling the information using\nthe API. This tool enabled us to collect three distinct types of information:\nthe original code (i.e., Pre-modified code), issue information (i.e., Symptoms\nof the issue), the changed code to address the issue (i.e., Post-modified\ncode). We consider the changes merged into the main branch reliable, as\nthe repository’s maintainers have reviewed and approved these modifications.\nConsequently, we employ the merged code as the ground truth for our\nevaluation process.\n5https://docs.github.com/en/rest?apiVersion=2022-11-28\nExploring LLM-based Automated Repairing of Ansible Script 895\n \nFigure 3 Overview of the GHPR process.\nFigure 4 Overall process for collecting cases and evaluation LLM.\n3.2 Collecting Cases and Evaluation\nFigure 4 provides an overview of the process involved in data-collecting cases\nand evaluating LLM using the collected cases. The first line represents the\ndata-collecting process, and the second line represents the evaluating process.\nThe data-collecting process begins with the Andromeda dataset [21] con-\ntaining metadata of approximately 25K Ansible projects enrolled in Ansible\nGalaxy,6 a platform for sharing pre-packaged Ansible works. We applied six\ncriteria to select the necessary repositories. Initially, we chose repositories\nwith (1) more than 50 forks and stars and (2) licenses to filter out unserious\nprojects (i.e., homework or personal purposes). Second, we gathered projects\nthat (3) consist of over half of Ansible, (4) more than two contributors, and\n(5) releases. We also confirmed that the (6) issue frequency is more than\n6https://galaxy.ansible.com/\n896 S. Kwon et al.\none per month to select actively developing Ansible projects. Consequently,\nwe selected 25 Ansible repositories actively developing and serious projects\nusing the criteria. Utilizing our self-implemented crawling tool based on\nGHPR, as mentioned in Section 3.1, we collected 61 cases, each resolving\nan issue by modifying an Ansible script.\nThe evaluation process commences using the collected cases comprising\nissue information, pre-modified, and post-modified code. The issue informa-\ntion provides a natural language description of the issue’s symptoms. The\npre-modified code represents the Ansible script afflicted with the issue, and\nthe post-modified code is the human-modified code to resolve the issue.\nSubsequently, we generate prompts that include an instruction for LLM to\npropose issue-free Ansible scripts based on the issue information and pre-\nmodified code from each case. Finally, we assess the APR performance of\nthe LLM by comparing the script suggested by the LLM in response to the\nprompts and the human-modified script, which is post-modified code.\n4 Experimental Setup\n4.1 Dataset\nTable 1 presents an overview of the 58 collected cases, categorized into three\ndistinct types. The first type is ‘ Modification/Addition,’ which entails cases\nwhere the issues can be resolved by modifying or introducing new code. Out\nof the 58 cases, 30 were addressed by modifying the code, while 20 required\nadding new lines. The second type, ‘ Single-line/Multi-line,’ distinguishes\nbetween cases where an issue can be resolved by modifying or adding just a\nsingle line of code and cases where multiple lines of code need to be altered or\nadded. The last type, ‘Single-task/Multi-task,’ relates to the structure of Ansi-\nble scripts, which consist of several independent tasks. Thus, the necessary\nmodifications or additions to address an issue can occur within a single task or\nspan multiple tasks. This classification allows us to explore the performance\nof LLM-based APR on Ansible from various angles, considering the diversity\nof issues and the scope of changes required for resolution.\n4.2 Research Questions\nThis study establishes 2 research questions and they are as follows.\n(1) RQ1: Does prompt affect LLM based ARPs’ performance? (Analysis on\nthe pattern of the prompt)\nExploring LLM-based Automated Repairing of Ansible Script 897\nTable 1 Characteristics of the 58 collected cases\nCase Type # of Case\nModification/Addition 30/28\nSingle-line/Multi-line 19/39\nSingle-task/Multi-task 35/23\nTotal 58\nA prompt is a sequence of tokens we provide to the LLMs, and these\nmodels generate their responses based on the information provided in the\nprompt. Prior studies show that LLMs’ outputs are sensitive to the given\nprompt. Therefore, it is crucial to carefully engineer prompts to ensure that\nLLMs produce the desired responses with our intentions. Thus, we began by\nfollowing the prompt engineering guidelines provided by OpenAI. 7 These\nguidelines offer specific formats for prompts that align more effectively with\nuser intent and tend to yield better results. The first instruction in the guideline\nis “ Put instructions at the beginning of the prompt and use ### or \"\"\" to\nseparate the instruction and context .” We construct our prompt framework\nfollowing these instructions. Subsequently, we crafted a variety of prompts\nby incrementally adding more context to each one. Each case in our study\nconsisted of issue information, including an issue title and body. We generate\na variance of prompts by adding these one by one, and the prompts for our\nexperiment are shown in Table 2.\nPrompt1 provides the LLM with the defective Ansible script and makes it\nfix it. This prompt is designed to ascertain the LLM’s ability to comprehend\nthe given Ansible script, and accurately predict and fix an issue within the\nscript. Prompt2 additionally provides LLM an issue title, which is the implicit\ninformation regarding the issue in the defective Ansible script. We designed\nthis prompt to examine how the performance of LLMs is affected when\nthey are given supplementary implicit information about the issue. Similarly,\nPrmopt3 additionally provides LLM with an issue body, which offers detailed\ninformation on the issues. The issue body includes the symptoms, the pre-\ncise location of bugs within the code, and the desired direction for fixing\nthem. Using the third prompt, we aim to analyze how LLMs perform when\npresented with a wealth of context related to specific issues.\nRQ2: Does different LLM show similar performance? (Analysis on the\ntype of the LLMs)\n7https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-wit\nh-openai-api\n898 S. Kwon et al.\nTable 2 Variance of prompts\nType Prompt\nPrompt1 You will be provided with a defective Ansible script. Your task is to find and fix\nbugs and provide the modified code with a reference for notification.\nDefective Ansible script:\"\"\"{defective_script}\"\"\"\nPrompt2 You will be provided with a defective Ansible script and bug report title. Your task\nis to find and fix bugs and provide the modified code with a reference for\nnotification.\nIssue report Title:\"\"\"{issue_title}\"\"\"\nDefective Ansible script:\"\"\"{defective_script}\"\"\"\nPrompt3 You will be provided with a defective Ansible script and bug report title and body.\nYour task is to find and fix bugs and provide the modified code with a reference\nfor notification.\nIssue report Title:\"\"\"{issue_title}\"\"\"\nIssue report body:\"\"\"{issue_body}\"\"\"\nDefective Ansible script:\"\"\"{defective_script}\"\"\"\nTable 3 Characteristics of various LLMs\nType ChatGPT 3.5 ChatGPT 4.0 Bard\nManufacturer OpenAI OpenAI Google\nBasic Model GPT3.5 GPT4.0 PalM2\nParameters 1750B Private Private\nDataset (Word) 1.37T 1.37T 1.56T\nRecently, LLMs developed independently by large companies have been\nactively announced. Except for Llama 8 provided by Meta, these announced\nLLMs are working in black-box natural, which means information on their\narchitecture, layers, and parameters are private for potential commercial rea-\nsons. Furthermore, their enormous size makes them impractical for operation\nwithin local computing environments. Instead, they are designed to be oper-\nated via the web or the API provided by the manufacturer. Among various\nLLMs, we select three highly popular models, the characteristics of which\nare summarized in Table 3. OpenAI’s ChatGPT 3.5 is the first LLM freely\navailable on the web, and ChatGPT 4.0 is the most advanced system with\nmore sophisticated language understanding and processing abilities than the\nprevious version. On the other hand, Bard is a model announced to compete\n8https://github.com/facebookresearch/llama\nExploring LLM-based Automated Repairing of Ansible Script 899\ndirectly with ChatGPT and is also currently available for free use on the web.\nWhile OpenAI offers an API for accessing their LLMs, there is no officially\nprovided API for Bard. Therefore, the three models experimented on the web\nand synthesized results.\n4.3 Evaluation and Statistical Test\nThe evaluation process involved three Ph.D. students, each independently\nassessing a set of 63 cases, the number of cases initially collected. These\nassessments were conducted on three LLMs using three different prompts.\nThey utilized objective and subjective indicators to evaluate LLMs’ per-\nformance. The objective indicators are used to measure how closely LLM\nmodifies the script to the human-modified script (i.e., Post-modified script),\nand there are four choices: (1) Does the script modified by the LLM according\nto the given prompt exactly match the human-modified code? If not, (2) Is it\nmodifying the content inside the same task? (3) Amend the same line within\nthe same task? (4) Amend irrelevant parts? The subjective indicator is used to\nassess how helpful the LLM-modified script is in fixing the actual defective\nscript, and there are two choices: (1) Is the LLM-provided script helpful in\nmodifying the actual defective script? or (2) isn’t it?\nFollowing the independent evaluation, we employed Krippendorff’s\nα [11] to measure the agreement between evaluation results. It is a reliability\ncoefficient that measures the agreement among observers or raters. Its values\nfall within the range of −1 to 1, with 1 representing perfect agreement\nbetween the raters, 0 indicating they are guessing randomly, and negative\nvalues suggesting the raters systematically disagree. The initial agreement\nrate of individual results was 0.58. However, all raters discussed cases where\ntheir evaluations diverged until Krippendorff’s α was 1.0. Finally, of the 63\ncases initially collected, 58 cases were used for analysis, excluding 5 cases in\nwhich the evaluations did not match.\n5 Experimental Results\n5.1 RQ1: Does Prompt Affect LLM Based ARP’s Performance?\nTable 4 illustrates the performance of three LLMs in ARP on various prompts.\nAs described in Section 4.3, the performance analysis consists of objective\nand subjective evaluation. In the objective evaluation, we assess how closely\nthe LLM responds to the human-modified code. “ Totally match” indicates\nthe case where the LLM’s response is identical to the human-modified code.\n900 S. Kwon et al.\nTable 4 Performance analysis on various prompts over the three LLMs\n# of cases (# of improved cases compare to prior prompt)\nObjective Evaluation Subjective Evaluation\nTotally Match Task Match Line Match Miss Helpful Not Helpful\nPrompt1 1 (−) 5 ( −) 0 ( −) 52 ( −) 1 ( −) 57 ( −)\nPrompt2 3 (↑2) 17 (↑12) 4 ( ↑4) 34 ( ↑18) 8 ( ↑7) 50 ( ↑7)\nPrompt3 2 (↓1) 12 ( ↓5) 12 (↑8) 32 (↑2) 15 (↑7) 43 (↑7)\n(a) Bard\n# of cases (# of improved cases compare to prior prompt)\nObjective Evaluation Subjective Evaluation\nTotally Match Task Match Line Match Miss Helpful Not Helpful\nPrompt1 3 (−) 3 ( −) 2 ( −) 50 ( −) 7 ( −) 51 ( −)\nPrompt2 6 (↑3) 9 ( ↑6) 4 ( ↑2) 38 ( ↑12) 18 ( ↑11) 40 ( ↑9)\nPrompt3 7 (↑1) 16 (↑7) 10 (↑6) 24 (↑14) 26 (↑8) 32 (↑8)\n(b) ChatGPT v3.5\n# of cases (# of improved cases compare to prior prompt)\nObjective Evaluation Subjective Evaluation\nTotally Match Task Match Line Match Miss Helpful Not Helpful\nPrompt1 1 (−) 0 ( −) 4 ( −) 53 ( −) 4 ( −) 54 ( −)\nPrompt2 3 (↑2) 9 ( ↑9) 15 ( ↑11) 31 ( ↑22) 20 ( ↑16) 38 ( ↑16)\nPrompt3 7 (↑4) 10 (↑1) 20(↑5) 22 (↑9) 28 (↑8) 30 (↑8)\n(c) ChatGPT v4.0\n“Task match” and “Line match” indicate the cases where the LLM’s response\nis not the same as human-modified code, but it modified the same part\nof the human-modified code (i.e., in a task or line). Lastly, “ miss” means\nLLM’s response modifies parts unrelated to human-modified code. On the\nother hand, the subjective evaluation focuses on whether the LLM’s response\nproves helpful in addressing the issue.\nThe first finding is that incorporating more information into the prompt\ngenerates more satisfactory code across all three models. This observation\nholds for objective and subjective evaluation indicators, with improvements\nevident as the amount of information in the prompt increases (i.e., prompt1\n→ prompt2 → prompt3). Even the addition of implicit information about\nthe issue, as seen in prompt2 where only the issue title is included, results\nin significant enhancements in objective and subjective evaluation indica-\ntors compared to prompt1. Furthermore, when a detailed description of\nthe issue (i.e., issue body) is added in prompt2, there is an increase in\nboth indicators. However, this improvement is relatively less significant than\nExploring LLM-based Automated Repairing of Ansible Script 901\n--- a/tasks/install.yml \n+++ b/tasks/install.yml \n@@ -2,7 +2,7 @@ \n- name: enable overcommit in sysctl \n   sysctl: \n     name: vm.overcommit_memory \n-    value: 1 \n+    value: \"1\" \n     state: present \n     reload: yes \n     ignoreerrors: yes \nFigure 5 git diff result of case #13.\nbetween prompt1 and prompt2. These results show that specifying the direc-\ntion of automatic repairing LLM with additional information is important.\nExceptionally, we can identify that Bard’s objective evaluation performance\ndegrades between prompt2 and prompt3. This degradation is attributed to\nBard’s token limitation, which restricts the amount of the given prompt. In\nthe case of prompt3, there were cases where the entire Ansible script could\nnot be entered in Bard, since the issue report body located in the middle of\nthe prompt was too long, so it exceeded the token limitation. It prevents Bard\nfrom understanding the information of the entire Ansible script, resulting\nin poor performance compared to propmt2. In conclusion, the quantity of\ninformation included in the prompt provided to LLMs significantly influences\ntheir performance in ARP tasks.\nThe second finding highlights a limitation of LLMs ability to predict\nthe presence of defects in a given Ansible script when provided with only\nthe script information (i.e., prompt1), with some exceptions. Figure 5 shows\nthe git diff result of case # 13, where the three LLM models modified the\ndefective Ansible script to match the human-modified code precisely. In this\ncase, the issue stemmed from a discrepancy in data types. Ansible script\nbasically uses the String type as a variable. However, 1 (i.e., int type) is\nentered in the defective script, so the developer changed 1 as Int type into\n“1” as String type to resolve the issue. However, it is important to note\nthat most cases involved issues not related to syntax errors in the script but\nspecific conditions during script execution. Thus, it is essential to incorporate\ninformation about the specific conditions causing the issue into the prompt to\naddress these cases (i.e., through prompt2 or prompt3). In conclusion, LLMs\nhave limitations in predicting whether a given Ansible script contains defects\nwhen provided with script information alone. It is also imperative to include\n902 S. Kwon et al.\nissue reports that provide context about the specific conditions leading to the\nissues.\nThe third finding is that it is difficult to directly apply LLM-based APR on\nAnsible script because only 28% (7 cases) are modified to be the same as the\nhuman-modified code, even when prompt3 is applied. In addition, in the case\nof subjective evaluation, the code generated by both versions of ChatGPT\nonly helps with solving bugs in about 44% (26 cases) and 48% (28 cases) of\nall cases. However, the above findings indicate the potential for performance\nenhancement based on the format of the prompt and the information included.\nConsequently, there are plans for further research to improve performance by\naugmenting the prompt with additional information, such as utilizing few-\nshot learning, which involves providing examples of fixing similar issues.\nIn conclusion, we confirm that the performance of LLM varies depending\non the given prompt, with more information provided in the prompt yielding\nbetter performance. However, further research is needed to enhance the\nquality of LLM’s response. Furthermore, in subsequent analysis, we apply\nthe subjective evaluation, which shows the difference in performance more\nclearly than the objective evaluation, and prompt3, which consistently shows\nthe best performance among other prompts.\n5.2 RQ2: Does Different LLM Show Similar Performance?\nTable 5 summarizes the performance of each model, focusing on subjective\nindicators when using prompt3. As shown in the table, Bard’s performance is\nlower than the other two versions of ChatGPT, with no significant disparity\nbetween the performance of the two ChatGPT versions. These results indicate\nthat ChatGPTs comprehend Ansible scripts better than Bard and offer helpful\nresponses in solving issues. However, it is important to note that model\nconfiguration and training data for all three models are private; analysis\nof the above performance differences cannot be performed. Nonetheless,\nconsidering that Ansible has a relatively simple structure compared to the\nother PL, it is reasonable to speculate that the amount of Ansible script in\neach training data may influence the models’ performance.\nFigure 8 shows a Venn diagram of cases where each model provides\nhelpful responses for solving an issue. First, the three models provide helpful\nTable 5 LLM’s performance on subjective indicator using prompt3\nBard ChatGPT v3.5 ChatGPT v4.0\nHelpful/Not Helpful 15/43 26/32 28/30\nExploring LLM-based Automated Repairing of Ansible Script 903\n \nFigure 6 Venn diagram of cases where LLMs provide helpful responses.\nTable 6 LLMs’ performance analysis on various types of case\nHelpful/Not helpful (Total case)\nModification Addition Single Line Multi Line Single Task Multi Task\nBard 6/24 9/19 5/ 14 10/29 9/26 6/17\n(30) (28) (19) (39) (35) (23)\nChatGPTv3.5 12/18 14/14 6/13 18/21 14/21 12/11\n(30) (28) (19) (39) (35) (23)\nChatGPTv4.0 16 /14 12/16 9/10 19/20 18/17 10/13\n(30) (28) (19) (39) (35) (23)\nresponses for issue resolution in 41 of the 58 cases (i.e., 70%). Though the\nproblem of determining which LLM model’s response is accurate remains,\nthese results highlight the promise of LLM-based APR for Ansible scripts.\nSecond, the models that provided helpful responses differ across the collected\ncases, even with the same prompt. There are cases where Bard, which shows\nrelatively lower performance than the two versions of ChatGPT, provides\nmore helpful responses than them, and similar patterns emerge with the\ntwo ChatGPT versions that show comparable performance between them.\nIn addition, the cases in which the three models provided helpful responses in\ncommon were 13% (i.e., 8 cases) of the total. Based on these facts, providing\nprompts tailored to each model is essential because each model interprets\na given prompt and responds differently. Finally, to check the differences\nbetween cases where responses are commonly helpful (i.e., 8 cases) and\nresponses are not helpful (i.e., 17 cases), we analyze the characteristics of\nthe given prompts, and the results will be discussed in Sector 5.\nTable 6 shows the subjective evaluation results of the three models’\nperformance across various types of cases when using prompt3. Similar to\n904 S. Kwon et al.\nTable 2, Bard does not outperform ChatGPT in any case. In addition, there\nare no notable differences between the two versions of ChatGPT depending\non the type of case. However, ChatGPT v4.0 outperforms ChatGPT v 3.5,\nexcept in cases where code additions are required (i.e., Addition) or modified\nparts are distributed across multiple tasks (i.e., Multi-Task). Consequently,\nChatGPT v4.0 outperforms other models, while there are no significant\ndifferences in the performance among models depending on the type of case.\nIn conclusion, among the three models, ChatGPT v4.0 is better than\nthe others. Moreover, we can confirm that LLM-based ARP on Ansible\nis promising, although further investigation is necessary to determine the\noptimal model providing the most accurate responses.\n6 Discussion\nIn this section, we analyze the relationship between the constituent elements\nof the prompt and the level of satisfaction of the responses provided by LLM.\nAs explained in Section 3.2, Prompt3 consists of a defective Ansible script,\nan issue report title, and an issue report body. Given that the issue report\ntitle occupies a relatively minor portion of the prompt compared to the other\ncomponents, we analyze the impact of the issue report body and defective\nAnsible script on the satisfaction levels associated with LLM responses.\nFigure 7 is a dot graph showing cases where the three models provide\nhelpful responses in common and cases where they do not (i.e., 8 cases\nand 17 cases in Section 4.3, respectively), according to the number of lines\n \nFigure 7 Correlation between the elements of the prompt and satisfaction of the response.\nExploring LLM-based Automated Repairing of Ansible Script 905\nin the issue report body and the number of lines of the defective Ansible\nscript. In the figure, regarding the number of lines of the defective Ansible\nscript in the prompt, no discernible correlation is evident between it and the\nsatisfaction level of the response. On the other hand, a significant correlation\nemerges between the length of the issue report body and the satisfaction\nlevel of the response. In cases where helpful responses were obtained (i.e.,\nOrange dot in the figure), the number of lines in the issue report body\nis relatively small (i.e., 9 ∼36 lines). It indicates that the number of lines\nin the issue report, which LLM relies on to determine how to resolve the\nissue, has more influence than the line of Ansible script requiring correction.\nAdditionally, if the issue report body in the middle of the prompt is too long, it\nexceeds the number of tokens LLM can remember while writing the response.\nConsequently, this leads to a decline in LLM performance because it loses the\ninformation necessary for reference during response generation.\nIn order to address this issue, we expect that satisfaction with LLM’s\nresponse can be increased by providing a summary of the issue report body\nrather than providing it as is. As illustrated in Figure 8, we outline future work\nfor an automated framework for Ansible. Figure 8(a) shows the currently\napplied framework, where an overly lengthy issue report body adversely\naffects response quality. On the other hand, Figure 8(b) shows our future\n \n \n(a) Present work \n(b) Future work \nFigure 8 Frameworks of present and future work.\n906 S. Kwon et al.\nwork’s framework, which incorporates an additional LLM tasked with sum-\nmarizing the issue report body. Through this approach, we aim to generate\nmore effective prompts and enhance the performance of LLM-based APR for\nAnsible.\n7 Threats to Validity\nInternal Validity is the inconsistency of Bard and ChatGPTs’ responses to\nthe same question. The evaluation results could depend on repeated trials.\nTo mitigate this threat, the three raters confirmed the results of the other\nraters through cross-validation, and we removed cases that did not match the\nevaluation results.\nExternal Validity is that the small number of raters and cases (i.e., three\nraters and 58 cases, respectively, may limit the generalizability of our find-\nings. To mitigate this threat, we applied Krippendorff’sα to select only cases\nwhere the three evaluators were unanimous, and through this, we tried to\novercome the small number of cases and raters. In future work, we plan to\nimprove external Validity by involving more raters and cases.\n8 Conclusion\nEdge-Cloud system has a massively distributed infrastructure, and IaC is a\ncrucial tool that helps deploy and manage the infrastructure of the edge-cloud\nsystem effectively. Ansible is one of the popular IaC tools; as Ansible is a\nset of code, its code quality influences the quality of the service delivered\nby the Edge-cloud system. We focused on LLM-based ARP to ensure the\nquality of the Ansible script. However, prior LLM-based APR studies have\nconcentrated on widely used Programming Languages (PL), such as Java\nand C. Hence, this study evaluated the performance of LLM-based ARP on\nAnsible for the first time to confirm its applicability to Ansible. We assessed\nthe performance of three LLMs with three types of prompts on 58 Ansible\nscript revision cases. The results show that the LLMs provide d helpful\nresponses in 70% of cases, which is promising, but further research is needed\nto apply it in practice.\nIn future work, we plan to apply few-shot learning in the prompt, which\ngives hints for issue resolution. Additionally, we plan to use an additional\nLLM that summarizes the overly extensive information in the prompt, which\nprevents LLMs from losing the information.\nExploring LLM-based Automated Repairing of Ansible Script 907\nAcknowledgment\nThis research was supported by Information Technology Research Cen-\nter (ITRC) support program supervised by the Institute of Information &\nCommunications Technology Planning & Evaluation (IITP-2023-2020-0-\n01795), and Basic Science Research Program through the National Research\nFoundation of Korea (NRF) funded by the Ministry of Education (NRF-\n2022R1I1A3069233).\nReferences\n[1] Agapito, G., Bernasconi, A., Cappiello, C., Khattak, H.A., Ko, I.,\nLoseto, G., Mrissa, M., Nanni, L., Pinoli, P., Ragone, A., et al.: Cur-\nrent Trends in Web Engineering: ICWE 2022 International Workshops,\nBECS, SWEET and WALS, Bari, Italy, July 5–8, 2022, Revised Selected\nPapers. Springer Nature (2023)\n[2] Ahmad, A., Waseem, M., Liang, P., Fahmideh, M., Aktar, M.S., Mikko-\nnen, T.: Towards human-bot collaborative software architecting with\nchatgpt. In: Proceedings of the 27th International Conference on Evalu-\nation and Assessment in Software Engineering. pp. 279–285 (2023)\n[3] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,\nNeelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language mod-\nels are few-shot learners. Advances in neural information processing\nsystems 33, 1877–1901 (2020)\n[4] Buyya, R., Srirama, S.N.: Fog and edge computing: principles and\nparadigms. John Wiley & Sons (2019)\n[5] Dalla Palma, S., Di Nucci, D., Palomba, F., Tamburri, D.A.: Within-\nproject defect prediction of infrastructure-as-code using product and\nprocess metrics. IEEE Transactions on Software Engineering 14(8), 1\n(2020)\n[6] Guerriero, M., Garriga, M., Tamburri, D.A., Palomba, F.: Adoption, sup-\nport, and challenges of infrastructure-as-code: Insights from industry.\nIn: 2019 IEEE International conference on software maintenance and\nevolution (ICSME). pp. 580–589. IEEE (2019)\n[7] Hassan, M.M., Rahman, A.: As code testing: Characterizing test quality\nin open source ansible development. In: 2022 IEEE Conference on Soft-\nware Testing, Verification and Validation (ICST). pp. 208–219. IEEE\n(2022)\n908 S. Kwon et al.\n[8] Jin, M., Shahriar, S., Tufano, M., Shi, X., Lu, S., Sundaresan, N.,\nSvyatkovskiy, A.: Inferfix: End-to-end program repair with llms. arXiv\npreprint arXiv:2303.07263 (2023)\n[9] Kabir, S., Udo-Imeh, D.N., Kou, B., Zhang, T.: Who answers it better?\nan in-depth analysis of chatgpt and stack overflow answers to software\nengineering questions. arXiv preprint arXiv:2308.02312 (2023)\n[10] Kokuryo, S., Kondo, M., Mizuno, O.: An empirical study of utilization\nof imperative modules in ansible. In: 2020 IEEE 20Th international con-\nference on software quality, reliability and security (QRS). pp. 442–449.\nIEEE (2020)\n[11] Krippendorff, K.: Computing krippendorff’s alpha-reliability (2011) 12.\nKwon, S., Jang, J.I., Lee, S., Ryu, D., Baik, J.: Codebert based software\ndefect prediction for edge-cloud systems. In: Current Trends in Web\nEngineering: ICWE 2022 International Workshops, BECS, SWEET and\nWALS, Bari, Italy, July 5–8, 2022, Revised Selected Papers. pp. 11–21.\nSpringer (2023)\n[12] Kwon, S., Jang, J.I., Lee, S., Ryu, D., Baik, J.: Codebert based software\ndefect prediction for edge-cloud systems. In: Current Trends in Web\nEngineering: ICWE 2022 International Workshops, BECS, SWEET and\nWALS, Bari, Italy, July 5–8, 2022, Revised Selected Papers. pp. 11–21.\nSpringer (2023)\n[13] Kwon, S., Lee, S., Ryu, D., Baik, J.: Pre-trained model-based software\ndefect prediction for edge-cloud systems. Journal of Web Engineering\npp. 255–278 (2023)\n[14] Kwon, S., Lee, S., Ryu, D., Baik, J.: Exploring the Feasibility of Chat-\nGPT for Improving the Quality of Ansible Scripts in Edge-Cloud Infras-\ntructures through Code Recommendation: ICWE 2023 International\nWorkshops, BECS, In proceeding\n[15] Li, Z., Lu, S., Guo, D., Duan, N., Jannu, S., Jenks, G., Majumder, D.,\nGreen, J., Svyatkovskiy, A., Fu, S., et al.: Codereviewer: Pre-training\nfor automating code review activities. arXiv preprint arXiv:2203.09095\n(2022)\n[16] Ma, W., Liu, S., Wang, W., Hu, Q., Liu, Y ., Zhang, C., Nie, L., Liu, Y .:\nThe scope of chatgpt in software engineering: A thorough investigation.\narXiv preprint arXiv:2305.12138 (2023)\n[17] Meijer, B., Hochstein, L., Moser, R.: Ansible: Up and Running.\n“O’Reilly Media, Inc.” (2022)\n[18] Monperrus, M.: Automatic software repair: A bibliography. ACM Com-\nputing Surveys (CSUR) 51(1), 1–24 (2018)\nExploring LLM-based Automated Repairing of Ansible Script 909\n[19] Morris, K.: Infrastructure as code: managing servers in the cloud.\n“O’Reilly Media, Inc.” (2016)\n[20] Nascimento, N., Alencar, P., Cowan, D.: Comparing software devel-\nopers with chatgpt: An empirical investigation. arXiv preprint\narXiv:2305.11837 (2023)\n[21] Nguyen, N., Nadi, S.: An empirical evaluation of github copilot’s code\nsuggestions. In: Proceedings of the 19th International Conference on\nMining Software Repositories. pp. 1–5 (2022)\n[22] Opdebeeck, R., Zerouali, A., De Roover, C.: Andromeda: A dataset\nof ansible galaxy roles and their evolution. In: 2021 IEEE/ACM 18th\nInternational Conference on Mining Software Repositories (MSR).\npp. 580–584. IEEE (2021)\n[23] Opdebeeck, R., Zerouali, A., De Roover, C.: Smelly variables in ansible\ninfrastructure code: detection, prevalence, and lifetime. In: Proceedings\nof the 19th International Conference on Mining Software Repositories.\npp. 61–72 (2022)\n[24] Pearce, H., Tan, B., Ahmad, B., Karri, R., Dolan-Gavitt, B.: Examining\nzero-shot vulnerability repair with large language models. In: 2023 IEEE\nSymposium on Security and Privacy (SP). pp. 2339–2356. IEEE (2023)\n[25] Rahman, A., Rahman, M.R., Parnin, C., Williams, L.: Security smells\nin ansible and chef scripts: A replication study. ACM Transactions on\nSoftware Engineering and Methodology (TOSEM) 30(1), 1–31 (2021)\n[26] Sobania, D., Briesch, M., Hanna, C., Petke, J.: An analysis of\nthe automatic bug fixing performance of chatgpt. arXiv preprint\narXiv:2301.08653 (2023)\n[27] Tian, H., Lu, W., Li, T.O., Tang, X., Cheung, S.C., Klein, J., Bissyandé,\nT.F.: Is chatgpt the ultimate programming assistant–how far is it? arXiv\npreprint arXiv:2304.11938 (2023)\n[28] White, J., Hays, S., Fu, Q., Spencer-Smith, J., Schmidt, D.C.: Chat-\ngpt prompt patterns for improving code quality, refactoring, require-\nments elicitation, and software design. arXiv preprint arXiv:2303.07839\n(2023)\n[29] Xia, C.S., Zhang, L.: Conversational automated program repair. arXiv\npreprint arXiv:2301.13246 (2023)\n[30] Xia, C.S., Zhang, L.: Keep the conversation going: Fixing 162 out of\n337 bugs for $0.42 each using chatgpt. arXiv preprint arXiv:2304.00385\n(2023)\n[31] Xu, J., Yan, L., Wang, F., Ai, J.: A github-based data collection method\nfor software defect prediction. In: 2019 6th International Conference on\n910 S. Kwon et al.\nDependable Systems and Their Applications (DSA). pp. 100–108. IEEE\n(2020)\n[32] Zhang, Y ., Rahman, M., Wu, F., Rahman, A.: Quality assurance for\ninfrastructure orchestrators: Emerging results from ansible. In: 2023\nIEEE 20th International Conference on Software Architecture Compan-\nion (ICSA-C). pp. 1–3. IEEE (2023)\nBiographies\nSunjae Kwon received the bachelor’s degree in electric engineering from\nKorea Military Academy in 2009, the master’s degree in computer engineer-\ning from Maharishi Markandeshwar University in 2015. He is a doctoral\nstudent in software engineering from KAIST. His research areas include\nsoftware analytics based on AI, software defect prediction, mining software\nrepositories, and software reliability engineering.\nSungu Lee received the bachelor’s degree in mathematics from KAIST in\n2021, the master’s degree in software engineering from KAIST in 2022. He is\na doctoral student in software engineering from KAIST. His research areas\ninclude software analytics based on AI, software defect prediction, mining\nsoftware repositories, and software reliability engineering.\nExploring LLM-based Automated Repairing of Ansible Script 911\n \nTaehyoun Kimreceived the bachelor’s degree in information and computer\nengineering from Ajou university in 2012, the master’s degree in computer\nscience from KAIST in 2014. He is a full-time researcher at Agency for\nDefense Development and a doctoral student in computer science from\nKAIST. His research areas include software analytics based on AI, soft-\nware defect prediction, mining software repositories, and software reliability\nengineering.\n \nDuksan Ryuearned a bachelor’s degree in computer science from Hanyang\nUniversity in 1999 and a Master’s dual degree in software engineering from\nKAIST and Carnegie Mellon University in 2012. He received his Ph.D.\ndegree in school of computing from KAIST in 2016. His research areas\ninclude software analytics based on AI, software defect prediction, mining\nsoftware repositories, and software reliability engineering. He is currently an\nassociate professor in software engineering department at Jeonbuk National\nUniversity.\n912 S. Kwon et al.\nJongmoon Baikreceived his B.S. degree in computer science and statistics\nfrom Chosun University in 1993. He received his M.S. degree and Ph.D.\ndegree in computer science from University of Southern California in 1996\nand 2000 respectively. He worked as a principal research scientist at Software\nand Systems Engineering Research Laboratory, Motorola Labs, where he\nwas responsible for leading many software quality improvement initiatives.\nHis research activity and interest are focused on software six sigma, software\nreliability & safety, and software process improvement. Currently, he is a full\nprofessor in school of computing at Korea Advanced Institute of Science and\nTechnology (KAIST). He is a member of the IEEE.",
  "topic": "Cloud computing",
  "concepts": [
    {
      "name": "Cloud computing",
      "score": 0.8231889009475708
    },
    {
      "name": "Computer science",
      "score": 0.7365590333938599
    },
    {
      "name": "Java",
      "score": 0.6259880065917969
    },
    {
      "name": "Software engineering",
      "score": 0.6025360226631165
    },
    {
      "name": "Enhanced Data Rates for GSM Evolution",
      "score": 0.5602582693099976
    },
    {
      "name": "Code (set theory)",
      "score": 0.480728417634964
    },
    {
      "name": "Computer security",
      "score": 0.47155165672302246
    },
    {
      "name": "Software",
      "score": 0.4572266638278961
    },
    {
      "name": "World Wide Web",
      "score": 0.4093630611896515
    },
    {
      "name": "Data science",
      "score": 0.3912133276462555
    },
    {
      "name": "Programming language",
      "score": 0.24190449714660645
    },
    {
      "name": "Operating system",
      "score": 0.2107686996459961
    },
    {
      "name": "Artificial intelligence",
      "score": 0.1283671259880066
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ]
}