{
    "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
    "url": "https://openalex.org/W3007672467",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4281172484",
            "name": "Guu, Kelvin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227328976",
            "name": "Lee, Kenton",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Tung, Zora",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202085626",
            "name": "Pasupat, Panupong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223922037",
            "name": "Chang, Ming-Wei",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2252136820",
        "https://openalex.org/W2409591106",
        "https://openalex.org/W2988841832",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2155482025",
        "https://openalex.org/W2971869958",
        "https://openalex.org/W2144578941",
        "https://openalex.org/W2973123473",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2989312920",
        "https://openalex.org/W2551396370",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963056065",
        "https://openalex.org/W1793121960",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W2950527759",
        "https://openalex.org/W2951434086",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2010416066",
        "https://openalex.org/W2990928880",
        "https://openalex.org/W2115758952",
        "https://openalex.org/W2556691798",
        "https://openalex.org/W2962753370",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W2963018920",
        "https://openalex.org/W2170973209",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2950726992",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2197084977",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W2970401203",
        "https://openalex.org/W2890397703"
    ],
    "abstract": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
    "full_text": "arXiv:2002.08909v1  [cs.CL]  10 Feb 2020\nREALM: Retrieval-Augmented Language Model Pre-T raining\nKelvin Guu * 1 Kenton Lee * 1 Zora T ung1 Panupong Pasupat 1 Ming-W ei Chang1\nAbstract\nLanguage model pre-training has been shown to\ncapture a surprising amount of world knowledge,\ncrucial for NLP tasks such as question answer-\ning. However, this knowledge is stored implic-\nitly in the parameters of a neural network, requir-\ning ever-larger networks to cover more facts. T o\ncapture knowledge in a more modular and inter-\npretable way, we augment language model pre-\ntraining with a latent knowledge retriever , which\nallows the model to retrieve and attend over doc-\numents from a large corpus such as Wikipedia,\nused during pre-training, ﬁne-tuning and infer-\nence. For the ﬁrst time, we show how to pre-\ntrain such a knowledge retriever in an unsuper-\nvised manner, using masked language model-\ning as the learning signal and backpropagating\nthrough a retrieval step that considers millions\nof documents. W e demonstrate the effective-\nness of Retrieval-Augmented Language Model\npre-training (REALM) by ﬁne-tuning on the chal-\nlenging task of Open-domain Question Answer-\ning (Open-QA). W e compare against state-of-the-\nart models for both explicit and implicit knowl-\nedge storage on three popular Open-QA bench-\nmarks, and ﬁnd that we outperform all previous\nmethods by a signiﬁcant margin (4-16% absolute\naccuracy), while also providing qualitative bene-\nﬁts such as interpretability and modularity.\n1. Introduction\nRecent advances in language model pre-training have\nshown that models such as BER T (\nDevlin et al. , 2018),\nRoBER T a ( Liu et al. , 2019) and T5 ( Raffel et al. , 2019)\nstore a surprising amount of world knowledge, ac-\nquired from the massive text corpora they are trained\non (\nPetroni et al. , 2019). For example, BER T is able to\n* Equal contribution 1 Google Research. Correspondence\nto: Kelvin Guu <kguu@google.com>, Kenton Lee <ken-\ntonl@google.com>, Zora Tung <gatoatigrado@google.com>,\nPanupong Pasupat <ppasupat@google.com>, Ming-W ei Chang\n<mingweichang@google.com>.\nFigure 1. REALM augments language model pre-training with\na neural knowledge retriever that retrieves knowledge from a\ntextual knowledge corpus , Z (e.g., all of Wikipedia). Signal\nfrom the language modeling objective backpropagates all th e way\nthrough the retriever, which must consider millions of docu ments\nin Z—a signiﬁcant computational challenge that we address.\ncorrectly predict the missing word in the following sen-\ntence: “ The\nis the currency of the United\nKingdom” (answer: “ pound”).\nIn these language models, the learned world knowledge is\nstored implicitly in the parameters of the underlying neural\nnetwork. This makes it difﬁcult to determine what knowl-\nedge is stored in the network and where. Furthermore, stor-\nage space is limited by the size of the network—to cap-\nture more world knowledge, one must train ever-larger net-\nworks, which can be prohibitively slow or expensive.\nT o capture knowledge in a more interpretable and modular\nway, we propose a novel framework, Retrieval-Augmented\nLanguage Model (REALM) pre-training, which augments\nlanguage model pre-training algorithms with a learned tex-\ntual knowledge retriever . In contrast to models that store\nknowledge in their parameters, this approach explicitly ex-\nposes the role of world knowledge by asking the model to\nREALM: Retrieval-Augmented Language Model Pre-T raining\ndecide what knowledge to retrieve and use during inference.\nBefore making each prediction, the language model uses\nthe retriever to retrieve documents\n1 from a large corpus\nsuch as Wikipedia, and then attends over those documents\nto help inform its prediction. Learning this model end-to-\nend requires backpropagating through a retrieval step that\nconsiders an entire corpus of textual knowledge, as shown\nin Figure\n1.\nThe key intuition of REALM is to train the retriever us-\ning a performance-based signal from unsupervised text:\na retrieval that improves the language model’s perplex-\nity is helpful and should be rewarded, while an un-\ninformative retrieval should be penalized. For exam-\nple, in Figure\n1, if the model needs to ﬁll the blank\nin “ the at the top of the pyramid”, the re-\ntriever should be rewarded for selecting a document con-\ntaining “ The pyramidion on top allows for less\nmaterial higher up the pyramid”. W e achieve this\nbehavior by modeling our retrieve-then-predict approach\nas a latent variable language model and optimizing the\nmarginal likelihood.\nIncorporating a large-scale neural retrieval module durin g\npre-training constitutes a signiﬁcant computational chal -\nlenge, since the retriever must consider millions of candi-\ndate documents for each pre-training step, and we must\nbackpropagate through its decisions. T o address this, we\nstructure the retriever such that the computation performe d\nfor each document can be cached and asynchronously up-\ndated, and selection of the best documents can be formu-\nlated as Maximum Inner Product Search (MIPS).\nNumerous prior works have demonstrated the bene-\nﬁt of adding a discrete retrieval step to neural net-\nworks (\nMiller et al. , 2016; Chen et al. , 2017), but did not\napply the framework to language model pre-training and\nemployed non-learned retrievers to handle large-scale doc -\nument collections. In the language modeling literature, th e\nk-Nearest Neighbor Language Model (\nKhandelwal et al. ,\n2019) ( kNN-LM) retrieves similar LM examples to im-\nprove memorization. However, kNN-LM was not ﬁne-\ntuned for downstream tasks, perhaps because it is unclear\nhow to adapt the retrieval mechanism: a kNN can only use\nexamples labeled for the target task—during ﬁne-tuning,\nthis precludes LM examples, which contain the desired\nworld knowledge. In contrast, REALM’s retriever is de-\nsigned to transfer to other tasks, and the retrieval is just\ntext, not a labeled example.\nW e evaluate our approach by ﬁne-tuning the mod-\nels pre-trained with REALM on the task of Open-\ndomain Question Answering (Open-QA), one of the most\nknowledge-intensive tasks in natural language process-\ning. W e evaluate on three popular Open-QA bench-\nmarks ( NAT U R A LQU E S T I O N S -O P EN , WE B QU E S T I O N S , and\nCU R AT E DTR E C) and compare to state-of-the-art Open-QA\nmodels, including both extremely large models that store\nknowledge implicitly (such as T5) as well as previous ap-\nproaches that also use a knowledge retriever to access ex-\nternal knowledge, but implement retrieval in a more heuris-\ntic fashion (\nLee et al. , 2019; Min et al. , 2019a; Asai et al. ,\n2019). REALM achieves new state-of-the-art results on all\nthree benchmarks, signiﬁcantly outperforming all previou s\nsystems by 4-16% absolute accuracy. W e also demonstrate\nqualitative beneﬁts of REALM, including interpretability\nand modularity.\n2. Background\nLanguage model pre-training The goal of language\nmodel pre-training is to learn useful representations of la n-\nguage, usually from unlabeled text corpora. The resulting\npre-trained model can then be further trained ( ﬁne-tuned )\nfor a downstream task of primary interest (in our case,\nOpen-QA), often leading to better generalization than trai n-\ning from scratch (\nDai & Le , 2015; Radford et al. , 2019).\nW e focus on the masked language model 2 (MLM) variant\nof pre-training popularized by BER T ( Devlin et al. , 2018).\nIn its basic form, an MLM is trained to predict the miss-\ning tokens in an input text passage. Given an unlabeled\npre-training corpus X (e.g., Wikipedia text), a training ex-\nample (x, y ) can be generated by randomly masking to-\nkens in a sampled piece of text (e.g., x = “ The [MASK]\nis the currency [MASK] the UK”; y = (“ pound”,\n“ of”)). The model uses its representation of the masked\ninput x to predict the token that should go in each mask.\nA good MLM must learn to encode syntactic and semantic\ninformation (e.g., to predict “ of”) as well as some world\nknowledge (e.g., to predict “ pound”).\nOpen-domain question answering (Open-QA) T o mea-\nsure a model’s ability to incorporate world knowledge, we\nneed a downstream task where world knowledge is criti-\ncal. Perhaps one of the most knowledge-intensive tasks in\nnatural language processing is open-domain question an-\nswering (Open-QA): given a question x such as “ What is\nthe currency of the UK?”, a model must output the\ncorrect answer string y, “ pound”. The “open” part of Open-\nQA refers to the fact that the model does not receive a pre-\nidentiﬁed document that is known to contain the answer,\nunlike traditional reading comprehension (RC) tasks such\nas SQuAD (\nRajpurkar et al. , 2016; 2018). While RC mod-\n1 W e use the term “document” loosely to refer to a passage\nfrom the knowledge corpus, not necessarily a whole article.\n2 Strictly speaking, MLM is not a standard language model,\nsince it does not deﬁne a distribution over the entire sequen ce\nof tokens. In the paper we sometimes abuse the term “language\nmodel” slightly to make the phrase shorter.\nREALM: Retrieval-Augmented Language Model Pre-T raining\nels comprehend a single document, Open-QA models must\nretain knowledge from millions of documents, since a ques-\ntion could be about any of them.\nW e focus on Open-QA systems that utilize a textual knowl-\nedge corpus Z as the knowledge source. Many of these\nsystems employ a retrieval-based approach: given a ques-\ntion x, retrieve potentially relevant documents z from\nthe corpus Z, and then extract an answer y from the\ndocuments (\nBrill et al. , 2002; Chen et al. , 2017; Lee et al. ,\n2019). Our approach, REALM, is inspired by this\nparadigm and extends it to language model pre-training.\nAlternatively, some recent work has proposed generation-\nbased systems that apply a sequence-to-sequence model on\nx to directly generate y token-by-token ( Lewis et al. , 2019;\nRaffel et al. , 2019). W e will compare against state-of-the-\nart systems from both paradigms in our experiments.\n3. Approach\nW e start by formalizing REALM’s pre-training and ﬁne-\ntuning tasks as a retrieve-then-predict generative process\nin Section\n3.1. Then in Section 3.2, we describe the model\narchitectures for each component of that process. In Sec-\ntion\n3.3, we show how to implement REALM pre-training\nand ﬁne-tuning by maximizing the likelihood of REALM’s\ngenerative process. En route, we address important compu-\ntational challenges, explain why training works, and also\ndiscuss strategies for injecting useful inductive biases. The\noverall framework is illustrated in Figure\n2.\n3.1. REALM’s generative process\nFor both pre-training and ﬁne-tuning, REALM takes some\ninput x and learns a distribution p(y | x) over possible out-\nputs y. For pre-training, the task is masked language mod-\neling: x is a sentence from a pre-training corpus X with\nsome tokens masked out, and the model must predict the\nvalue of those missing tokens, y. For ﬁne-tuning, the task\nis Open-QA: x is a question, and y is the answer.\nREALM decomposes p(y | x) into two steps: retrieve, then\npredict. Given an input x, we ﬁrst retrieve possibly helpful\ndocuments z from a knowledge corpus Z. W e model this as\na sample from the distribution p(z | x). Then, we condition\non both the retrieved z and the original input x to generate\nthe output y—modeled as p(y | z, x ). T o obtain the overall\nlikelihood of generating y, we treat z as a latent variable\nand marginalize over all possible documents z, yielding\np(y | x) =\n∑\nz∈Z\np(y | z, x ) p(z | x). (1)\n3.2. Model architecture\nW e now describe the two key components: the\nneural knowledge retriever , which models p(z | x),\nand the knowledge-augmented encoder , which models\np(y | z, x ).\nKnowledge Retriever The retriever is deﬁned using a\ndense inner product model:\np(z | x) = exp f(x, z )\n∑\nz′ exp f(x, z ′),\nf(x, z ) =Embedinput(x)⊤Embeddoc(z),\nwhere Embedinput and Embeddoc are embedding functions\nthat map x and z respectively to d-dimensional vectors.\nThe relevance score f(x, z ) between x and z is deﬁned as\nthe inner product of the vector embeddings. The retrieval\ndistribution is the softmax over all relevance scores.\nW e implement the embedding functions using BER T -style\nTransformers (\nDevlin et al. , 2018). Following standard\npractices, we join spans of text by applying wordpiece tok-\nenization, separating them with [SEP] tokens, preﬁxing a\n[CLS] token, and appending a ﬁnal [SEP] token.\njoinBERT(x) =[CLS]x[SEP]\njoinBERT(x1, x 2) =[CLS]x1[SEP]x2[SEP]\nAs in\nDevlin et al. (2018), we pass this into a Transformer,\nwhich produces one vector for each token, including the\nvector corresponding to [CLS] which is used as a “pooled”\nrepresentation of the sequence (denoted BERTCLS). Finally,\nwe perform a linear projection to reduce the dimensionality\nof the vector, denoted as a projection matrix W:\nEmbedinput(x) =WinputBERTCLS(joinBERT(x))\nEmbeddoc(z) =WdocBERTCLS(joinBERT(ztitle , z body))\nwhere ztitle is the document’s title and zbody is its body. W e\nlet θ denote all parameters associated with the retriever,\nwhich include the Transformer and projection matrices.\nKnowledge-Augmented Encoder Given an input x and\na retrieved document z, the knowledge-augmented encoder\ndeﬁnes p(y | z, x ). W e join x and z into a single sequence\nthat we feed into a Transformer (distinct from the one used\nin the retriever). This allows us to perform rich cross-\nattention between x and z before predicting y. See Figure\n1\nfor a concrete example.\nAt this stage, the architectures for pre-training and ﬁne-\ntuning differ slightly. For the masked language model pre-\ntraining task, we must predict the original value of each\n[MASK] token in x. T o do so, we use the same masked\nREALM: Retrieval-Augmented Language Model Pre-T raining\nFigure 2. The overall framework of REALM. Left: Unsupervised pre-training. The knowledge retriever and knowledge-augmented\nencoder are jointly pre-trained on the unsupervised langua ge modeling task. Right: Supervised ﬁne-tuning. After the parameters of the\nretriever ( θ) and encoder ( φ) have been pre-trained, they are then ﬁne-tuned on a task of p rimary interest, using supervised examples.\nlanguage modeling (MLM) loss as in Devlin et al. (2018):\np(y | z, x ) =\nJx∏\nj=1\np(yj | z, x )\np(yj | z, x ) ∝ exp\n(\nw⊤\nj BERTMASK(j)(joinBERT(x, z body ))\n)\nwhere BERTMASK(j) denotes the Transformer output vector\ncorresponding to the jth masked token, Jx is the total num-\nber of [MASK] tokens in x, and wj is a learned word em-\nbedding for token yj .\nFor Open-QA ﬁne-tuning, we wish to produce the answer\nstring y. Following previous reading comprehension work\n(\nRajpurkar et al. , 2016; Seo et al. , 2016; Lee et al. , 2016;\nClark & Gardner , 2017), we will assume that the answer\ny can be found as a contiguous sequence of tokens in some\ndocument z. Let S(z, y ) be the set of spans matching y in\nz. Then we can deﬁne p(y | z, x ) as:\np(y | z, x ) ∝\n∑\ns∈S(z,y)\nexp\n(\nMLP\n([\nhSTART(s); hEND(s)\n]))\nhSTART(s) = BERTSTART(s)(joinBERT(x, z body )),\nhEND(s) = BERTEND(s)(joinBERT(x, z body )),\nwhere BERTSTART(s) and BERTEND(s) denote the Transformer\noutput vectors corresponding to the start and end tokens of\nspan s, respectively, while MLP denotes a feed-forward neu-\nral network. W e will let φ denote all parameters associated\nwith the knowledge-augmented encoder.\n3.3. T raining\nFor both pre-training and ﬁne-tuning, we train by maxi-\nmizing the log-likelihood log p(y | x) of the correct out-\nput y. Since both the knowledge retriever and knowledge-\naugmented encoder are differentiable neural networks, we\ncan compute the gradient of log p(y | x) (deﬁned in Equa-\ntion\n1) with respect to the model parameters θ and φ, and\noptimize using stochastic gradient descent.\nThe key computational challenge is that the marginal prob-\nability p(y | x) =∑\nz∈Z p(y | x, z ) p(z | x) involves a sum-\nmation over all documents z in the knowledge corpus Z.\nW e approximate this by instead summing over the top k\ndocuments with highest probability under p(z | x)—this is\nreasonable if most documents have near zero probability.\nEven with this approximation, we still need an efﬁcient way\nto ﬁnd the top k documents. Note that the ordering of doc-\numents under p(z | x) is the same as under the relevance\nscore f(x, z ) = Embedinput(x)⊤Embeddoc(z), which is an\ninner product. Thus, we can employ Maximum Inner Prod-\nuct Search (MIPS) algorithms to ﬁnd the approximate top k\ndocuments, using running time and storage space that scale\nsub-linearly with the number of documents (\nRam & Gray ,\n2012; Shrivastava & Li , 2014; Shen et al. , 2015).\nT o employ MIPS, we must pre-compute Embeddoc(z) for\nevery z ∈ Z and construct an efﬁcient search index over\nthese embeddings. However, this data structure will no\nlonger be consistent with p(z | x) if the parameters θ of\nEmbeddoc are later updated. Hence, the search index goes\n“stale” after every gradient update on θ.\nOur solution is to “refresh” the index by asynchronously\nre-embedding and re-indexing all documents every several\nhundred training steps. The MIPS index is slightly stale be-\ntween refreshes, but note that it is only used to select the\ntop k documents. W e recompute p(z | x) and its gradient,\nusing the fresh θ, for these top k documents after retriev-\ning them. In Section\n4.5, we empirically demonstrate that\nthis procedure results in stable optimization, provided th at\nrefreshes happen at a sufﬁciently frequent rate.\nImplementing asynchronous MIPS refreshes W e asyn-\nchronously refresh the MIPS index by running two jobs in\nparallel: a primary trainer job, which performs gradient\nupdates on the parameters, and a secondary index builder\njob, which embeds and indexes the documents. As shown\nREALM: Retrieval-Augmented Language Model Pre-T raining\nFigure 3. REALM pre-training with asynchronous MIPS re-\nfreshes.\nbelow , the trainer sends the index builder a snapshot of its\nparameters, θ′. The trainer then continues to train while the\nindex builder uses θ′ to construct a new index in the back-\nground. As soon as the index builder is done, it sends the\nnew index back to the trainer, and the process repeats.\nWhile asynchronous refreshes can be used for both pre-\ntraining and ﬁne-tuning, in our experiments we only use it\nfor pre-training. For ﬁne-tuning, we just build the MIPS in-\ndex once (using the pre-trained θ) for simplicity and do not\nupdate Embeddoc.\n3 Note that we still ﬁne-tune Embedinput,\nso the retrieval function is still updated from the query sid e.\nWhat does the retriever learn? Since the knowledge re-\ntrieval of REALM is latent, it is not obvious how the train-\ning objective encourages meaningful retrievals. Here, we\nshow how it rewards retrievals that improve prediction ac-\ncuracy.\nFor a given query x and document z, recall that f(x, z ) is\nthe “relevance score” that the knowledge retriever assigns\nto document z. W e can see how a single step of gradient\ndescent during REALM pre-training alters this score by an-\nalyzing the gradient with respect to the parameters of the\nknowledge retriever, θ:\n∇ log p(y | x) =\n∑\nz∈Z\nr(z)∇f(x, z )\nr(z) =\n[ p(y | z, x )\np(y | x) − 1\n]\np(z | x).\nFor each document z, the gradient encourages the retriever\nto change the score f(x, z ) by r(z) — increasing if r(z)\nis positive, and decreasing if negative. The multiplier r(z)\nis positive if and only if p(y | z, x ) > p (y | x). The term\np(y | z, x ) is the probability of predicting the correct output\ny when using document z. The term p(y | x) is the expected\nvalue of p(y | x, z ) when randomly sampling a document\nfrom p(z | x). Hence, document z receives a positive up-\ndate whenever it performs better than expected.\n3 This works because pre-training already yields a good\nEmbeddoc function. However, it is possible that refreshing the in-\ndex would further improve performance.\n3.4. Injecting inductive biases into pre-training\nIn the process of developing REALM, we discovered sev-\neral additional strategies that further guide the model to-\nwards meaningful retrievals, described below .\nSalient span masking During REALM pre-training, we\nwant to focus on examples x that require world knowledge\nto predict the masked tokens. As explained in Section\n2,\nsome MLM spans only require local context. T o focus on\nproblems that require world knowledge, we mask salient\nspans such as “ United Kingdom” or “ July 1969”. W e\nuse a BER T -based tagger trained on CoNLL-2003 data\n(\nSang & De Meulder , 2003) to identify named entities, and\na regular expression to identify dates. W e select and mask\none of these salient spans within a sentence for the masked\nlanguage modeling task. W e show that this signiﬁcantly\noutperforms other masking strategies in Section\n4.5.\nNull document Even with salient span masking, not all\nmasked tokens require world knowledge to predict. W e\nmodel this by adding an empty null document ∅ to the top\nk retrieved documents, allowing appropriate credit to be as-\nsigned to a consistent sink when no retrieval is necessary.\nProhibiting trivial retrievals If the pre-training corpus\nX and the knowledge corpus Z are the same, there exists\na trivial retrieval candidate z that is too informative: if the\nmasked sentence x comes from document z, the knowledge\naugmented encoder can trivially predict y by looking at the\nunmasked version of x in z. This results in a large positive\ngradient for p(z | x). If this occurs too often, the knowledge\nretriever ends up learning to look for exact string matches\nbetween x and z, which does not capture other forms of\nrelevance. For this reason, we exclude this trivial candida te\nduring pre-training.\nInitialization At the beginning of training, if the retriever\ndoes not have good embeddings for Embedinput(x) and\nEmbeddoc(z), the retrieved documents z will likely be unre-\nlated to x. This causes the knowledge augmented encoder\nto learn to ignore the retrieved documents. Once this oc-\ncurs, the knowledge retriever does not receive a meaning-\nful gradient and cannot improve, creating a vicious cycle.\nT o avoid this cold-start problem, we warm-start Embedinput\nand Embeddoc using a simple training objective known as\nthe Inverse Cloze T ask (ICT) where, given a sentence, the\nmodel is trained to retrieve the document where that sen-\ntence came from. W e defer to\nLee et al. (2019) for de-\ntails. For the knowledge-augmented encoder, we warm-\nstart it with BER T pre-training—speciﬁcally, the uncased\nBER T -base model (12 layers, 768 hidden units, 12 atten-\ntion heads).\nREALM: Retrieval-Augmented Language Model Pre-T raining\n4. Experiments\nW e now evaluate our approach on the Open-QA task. In\nthis section, we describe in detail the benchmarks used and\nthe different approaches to which we compare empirically.\n4.1. Open-QA Benchmarks\nA number of benchmarks have been proposed for Open-\nQA. In this work, we focus on datasets where the ques-\ntion writers did not already know the answer. This yields\nquestions that reﬂect more realistic information-seeking\nneeds, and also avoids artifacts that can arise if the ques-\ntion is formulated with a particular answer in mind. A\ndeeper justiﬁcation is given in\nLee et al. (2019). In all\ncases, the predicted answer is evaluated via exact match\nwith any reference answer, following previous Open-QA\nwork (\nChen et al. , 2017).\nNaturalQuestions-Open The NaturalQuestions dataset\n(Kwiatkowski et al. , 2019) consists of naturally occurring\nGoogle queries and their answers. Each answer also comes\nwith an “answer type”: following Lee et al. (2019), we only\nkeep questions that are categorized as “short answer type”\nwith at most ﬁve tokens. The dataset also provides a sug-\ngested Wikipedia document to retrieve; like all models we\ncompare against, we do not provide this to our model.\nW ebQuestions The W ebQuestions dataset (\nBerant et al. ,\n2013) was collected from the Google Suggest API, using\none seed question and expanding the set to related ques-\ntions. W e follow the setting deﬁned by\nChen et al. (2017).\nCuratedT rec The CuratedTrec dataset is a collection of\nquestion-answer pairs drawn from real user queries issued\non sites such as MSNSearch and AskJeeves. T o account for\nmultiple correct answers or different spelling variations , the\nanswers in this dataset are deﬁned as regular expressions\nthat match all correct answers. It is unclear how to train\ngeneration-based models with this type of supervision, so\nwe do not evaluate them on this dataset.\n4.2. Approaches compared\nRetrieval-based Open-QA Most existing Open-QA sys-\ntems answer the input question by ﬁrst retrieving poten-\ntially relevant documents from a knowledge corpus, and\nthen using a reading comprehension system to extract an\nanswer from the documents. In this paradigm, the knowl-\nedge is stored explicitly in the corpus. W e wish to compare\ndifferent methods for implementing retrieval.\nMany approaches use non-learned heuristic retrieval such\nas sparse bag-of-words matching (\nRobertson et al. , 2009)\nor entity linking on the question to select a small set of rel-\nevant documents (e.g., 20). These documents are typically\nthen re-ranked using a learned model, but coverage may be\nlimited by the initial heuristic retrieval step. Approache s\nsuch as DrQA (\nChen et al. , 2017), HardEM ( Min et al. ,\n2019a), GraphRetriever ( Min et al. , 2019b), and PathRe-\ntriever ( Asai et al. , 2019) in T able 1 are in this category.\nSome recent approaches have proposed to implement learn-\nable retrieval using a MIPS index. ORQA (\nLee et al. , 2019)\nformulates Open-QA using a similar latent variable model\nas REALM, and also trains by maximizing the marginal\nlikelihood. However, REALM adds a novel language\nmodel pre-training step, and backpropagates into the MIPS\nindex, rather than using a ﬁxed index. In T able\n1, we di-\nrectly compare the two. It is also important to note that\nthe retrievers for both REALM pretraining and ORQA are\ninitialized using the Inverse Cloze T ask, described in Sec-\ntion\n3.4.\nGeneration-based Open-QA An emerging alternative\napproach to Open-QA is to model it as a sequence pre-\ndiction task: simply encode the question, and then decode\nthe answer token-by-token based on the encoding. While\nit was initially unclear how large amounts of knowledge\ncould be injected into the model, GPT -2 (\nRadford et al. ,\n2019) hinted at the possibility of directly generating an-\nswers without using any given context via sequence-to-\nsequence. However, their performance was not competi-\ntive possibly due to the lack of ﬁne-tuning. Orthogonally,\nT5 (\nRaffel et al. , 2019) showed that directly generating an-\nswers without explicit extraction from the given context is\nviable approach, but they only experimented on the read-\ning comprehension task, where a context document is pro-\nvided.\nFor the most competitive and comparable generation-based\nbaseline, we compare to concurrent work which ﬁne-tunes\nT5 for Open-QA (\nRoberts et al. , 2020).4 W e compare\nagainst the Base, Large, and even larger 11-billion parame-\nter model to measure the effect of model size.\n4.3. Implementation Details\nFine-tuning W e reuse all hyperparameters from\nLee et al. (2019), to enable direct comparison. Our\nknowledge corpus is derived from the December 20, 2018\nsnapshot of English Wikipedia. Documents are greedily\nsplit into chunks of up to 288 BER T wordpieces, resulting\nin just over 13 million retrieval candidates. During ﬁne-\ntuning inference, we consider the top-5 candidates, and the\n4 W e initially conducted our own T5 experiments using\nthe code from https://tinyurl.com/t5-openqa-colab (Raffel et al. ,\n2019). W e now report results from the concurrent work of\nRoberts et al. (2020), which has an improved ﬁne-tuning proce-\ndure.\nREALM: Retrieval-Augmented Language Model Pre-T raining\nT able 1. T est results on Open-QA benchmarks. The number of train/tes t examples are shown in paretheses below each benchmark.\nPredictions are evaluated with exact match against any refe rence answer. Sparse retrieval denotes methods that use spa rse features such\nas TF-IDF and BM25. Our model, REALM, outperforms all existi ng systems.\nName Architectures Pre-training NQ\n(79k/4k)\nWQ\n(3k/2k)\nCT\n(1k /1k) # params\nBERT -Baseline (Lee et al. , 2019) Sparse Retr. +Transformer BERT 26.5 17.7 21.3 110m\nT5 (base) ( Roberts et al. , 2020) Transformer Seq2Seq T5 (Multitask) 27.0 29.1 - 223m\nT5 (large) ( Roberts et al. , 2020) Transformer Seq2Seq T5 (Multitask) 29.8 32.2 - 738m\nT5 (11b) ( Roberts et al. , 2020) Transformer Seq2Seq T5 (Multitask) 34.5 37.4 - 11318m\nDrQA ( Chen et al. , 2017) Sparse Retr. +DocReader N/A - 20.7 25.7 34m\nHardEM ( Min et al. , 2019a) Sparse Retr. +Transformer BERT 28.1 - - 110m\nGraphRetriever ( Min et al. , 2019b) GraphRetriever +Transformer BERT 31.8 31.6 - 110m\nPathRetriever ( Asai et al. , 2019) PathRetriever +Transformer MLM 32.6 - - 110m\nORQA ( Lee et al. , 2019) Dense Retr. +Transformer ICT +BERT 33.3 36.4 30.1 330m\nOurs ( X = Wikipedia, Z = Wikipedia) Dense Retr. +Transformer REALM 39.2 40.2 46.8 330m\nOurs ( X = CC-News, Z = Wikipedia) Dense Retr. +Transformer REALM 40.4 40.7 42.9 330m\nT able 2. Ablation experiments on NQ’s development set.\nAblation Exact\nMatch\nZero-shot\nRetrieval\nRecall@5\nREALM 38.2 38.5\nREALM retriever +Baseline encoder 37.4 38.5\nBaseline retriever +REALM encoder 35.3 13.9\nBaseline (ORQA) 31.3 13.9\nREALM with random uniform masks 32.3 24.2\nREALM with random span masks 35.3 26.1\n30× stale MIPS 28.7 15.1\nentire model can be run on a single machine with a 12GB\nGPU.\nPre-training W e pre-train for 200k steps on 64 Google\nCloud TPUs, with a batch size of 512 and a learning rate\nof 3e-5, using BER T’s default optimizer. The document\nembedding step for the MIPS index is parallelized over 16\nTPUs. For each example, we retrieve and marginalize over\n8 candidate documents, including the null document ∅ .\nW e experiment with two choices of the pre-training corpus\nX : (1) Wikipedia, which is identical to the knowledge cor-\npus Z, and (2) CC-News, our reproduction of the corpus of\nEnglish news proposed by\nLiu et al. (2019).\n4.4. Main results\nT able 1 shows the accuracy of different approaches on the\nthree Open-QA datasets. REALM outperform all previous\napproaches by a signiﬁcant margin. T able\n1 also shows the\nnumber of parameters for each model.\nAs reported in the concurrent work of Roberts et al. (2020),\nthe generative Open-QA systems based on T5 are surpris-\ningly powerful, with the largest T5-11B model outperform-\ning the previous best Open-QA system. Increasing the size\nof T5 yields consistent improvement, but comes at signif-\nicant computational cost (from Base to 11B, the model is\n50 times larger, and gains roughly 5 points in accuracy). In\ncontrast, REALM outperforms the largest T5-11B model\nwhile being 30 times smaller. It is also important to note\nthat T5 accesses additional reading comprehension data\nfrom SQuAD during its pre-training (100,000+ examples).\nAccess to such data could also beneﬁt REALM, but was not\nused in our experiments.\nAmong all systems, the most direct comparison with\nREALM is ORQA (\nLee et al. , 2019), where the ﬁne-tuning\nsetup, hyperparameters and training data are identical. Th e\nimprovement of REALM over ORQA is purely due to bet-\nter pre-training methods. The results also indicate that ou r\nmethod of pre-training can be applied both on (1) the single-\ncorpus setting ( X = Wikipedia, Z = Wikipedia), or (2) the\nseparate-corpus setting ( X = CC-News, Z = Wikipedia).\nCompared to other retrieval-based systems (\nAsai et al. ,\n2019; Min et al. , 2019a;b) which often retrieve from 20 to\n80 documents, our system gets the overall best performance\nwhile only retrieving 5 documents.\n4.5. Analysis\nIn T able\n2 we present results for NaturalQuestions-Open\nafter ablating critical components of REALM. In addition\nto the end-to-end results, we also report how often the gold\nanswer appears in the top-5 retrievals before applying any\nﬁne-tuning. The latter metric more signiﬁcantly isolates t he\ncontribution of improving the retriever during pre-traini ng.\nREALM: Retrieval-Augmented Language Model Pre-T raining\nT able 3. An example where REALM utilizes retrieved documents to bett er predict masked tokens. It assigns much higher probabilit y\n(0.129) to the correct term, “ Fermat”, compared to BERT . (Note that the blank corresponds to 3 BER T wordpieces.)\nx: An equilateral triangle is easily constructed using a stra ightedge and compass, because 3 is a prime.\n(a) BERT p(y = “ Fermat” |x) = 1 . 1 × 10− 14 (No retrieval.)\n(b) REALM p(y = “ Fermat” |x, z ) = 1. 0 (Conditional probability with document z =“257 is . . . a Fermat prime.\nThus a regular polygon with 257 sides is constructible with c ompass . . . ”)\n(c) REALM p(y = “ Fermat” |x) = 0 . 129 (Marginal probability , marginalizing over top 8 retrieved documents.)\nEncoder or Retriever W e ﬁrst aim to determine whether\nREALM pre-training improves the retriever or the encoder,\nor both. T o do so, we can reset the parameters of either\nthe retriever or the encoder to their baseline state before\nREALM pre-training, and feed that into ﬁne-tuning. Reset-\nting both the retriever and encoder reduces the system to\nour main baseline, ORQA. W e ﬁnd that both the encoder\nand retriever beneﬁt from REALM training separately, but\nthe best result requires both components acting in unison.\nMasking scheme W e compare our salient span masking\nscheme (Section\n3.4) with (1) random token masking in-\ntroduced in BER T ( Devlin et al. , 2018) and (2) random\nspan masking proposed by SpanBER T ( Joshi et al. , 2019).\nWhile such salient span masking has not been shown to\nbe impactful in previous work with standard BER T train-\ning (\nJoshi et al. , 2019), it is crucial for REALM. Intuitively,\nthe latent variable learning relies heavily on the utility o f re-\ntrieval and is therefore more sensitive to a consistent lear n-\ning signal.\nMIPS index refresh rate During pre-training, we run a\nparallel process to re-embed corpus documents and rebuild\nthe MIPS index. This results in one index refresh per ap-\nproximately 500 training steps. T o demonstrate the impor-\ntance of frequent index refreshes, we compare against using\na slower refresh rate. The results in T able\n2 suggests that\na stale index can hurt model training, and further reducing\nthis staleness could offer better optimization.\nExamples of retrieved documents T able\n3 shows an\nexample of the REALM masked language model predic-\ntion. In this example, “ Fermat” is the correct word, and\nREALM (row (c)) gives the word a much high probability\ncompared to the BER T model (row (a)). Since REALM\nmanages to retrieve some documents with a related fact\n(row (b)), the marginalized probability of the correct an-\nswer dramatically increases. This shows that REALM is\nable to retrieve document to ﬁll in the masked word even\nthough it is trained with unsupervised text only.\n5. Discussion and Related W ork\nW e previously discussed related methods for Open-QA.\nHere we present several alternate ways of viewing REALM\nthat connect it to a broader set of ideas beyond Open-QA:\nLanguage modeling with corpus as context Language\nrepresentation models have been incorporating contexts of\nincreasingly large scope when making predictions. Ex-\namples of this progression include models that condi-\ntion on surrounding words (\nMikolov et al. , 2013a;b), sen-\ntences ( Kiros et al. , 2015; Peters et al. , 2018), and para-\ngraphs ( Radford et al. , 2018; Devlin et al. , 2018). W e can\nview REALM as a generalization of the above work to the\nnext level of scope: the entire text corpus.\nRetrieve-and-edit with learned retrieval In order to\nbetter explain the variance in the input text and en-\nable controllable generation,\nGuu et al. (2018) proposed\na language model with the retrieve-and-edit frame-\nwork ( Hashimoto et al. , 2018) that conditions on text with\nhigh lexical overlap. REALM has a similar approach, ex-\ncept that the model learns for itself which texts are most\nuseful for reducing perplexity. By jointly learning the re-\ntriever, REALM has the capacity to depend on information\nbeyond lexical overlap.\nScalable grounded neural memory The document in-\ndex can be viewed as a memory where the keys are\nthe document embeddings. From this view , our work\nshare motivations with works such as product key mem-\nory (\nLample et al. , 2019), which enables sub-linear mem-\nory access in a memory network ( W eston et al. , 2014;\nGraves et al. , 2014; Sukhbaatar et al. , 2015), allowing\nthese scalable memory layers to be integrated into large\nlanguage models. One main difference is that our memo-\nries are grounded—each memory is associated with a docu-\nment rather than unnamed value vectors. This level of inter-\npretability is crucial for applications like Open-QA, wher e\nusers would require provenance for a predicted answer to\nbe trustworthy.\nUnsupervised Corpus Alignment In sequence-to-\nsequence models with attention (\nBahdanau et al. , 2014),\nREALM: Retrieval-Augmented Language Model Pre-T raining\ntext is generated with latent selection of relevant tokens.\nThis results in a set of model-centric unsupervised align-\nments between target and source tokens. Analogously,\nREALM also generates text with latent selection of\nrelevant documents. A by-product of our method is that\nwe offer a set of model-centric unsupervised alignments\nbetween text in the pre-training corpus X and knowledge\ncorpus Z.\n6. Future W ork\nThe work presented here is the minimal instantiation of a\nfamily of REALM-like approaches where a representation\nis pre-trained to perform reasoning over a large corpus of\nknowledge on-the-ﬂy during inference. W e are particularly\noptimistic about generalizations of this work to (1) struc-\ntured knowledge, which would result in a generalization of\nPeters et al. (2019) where we would also learn the decision\nof which entities are informative, (2) the multi-lingual se t-\nting, e.g., retrieving knowledge in a high-resource langua ge\nto better represent text in a low-resource language, and (3)\nthe multi-modal setting, e.g., retrieving images or videos\nthat can provide knowledge rarely observed in text.\nReferences\nAsai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and\nXiong, C. Learning to retrieve reasoning paths over\nwikipedia graph for question answering. arXiv preprint\narXiv:1911.10470 , 2019.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473 , 2014.\nBerant, J., Chou, A., Frostig, R., and Liang, P . Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 Conference on Empirical Methods\nin Natural Language Processing , pp. 1533–1544, 2013.\nBrill, E., Dumais, S., and Banko, M. An analysis of the\naskmsr question-answering system. In Empirical Meth-\nods in Natural Language Processing , 2002.\nChen, D., Fisch, A., W eston, J., and Bordes, A. Read-\ning wikipedia to answer open-domain questions. In Pro-\nceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (V olume 1: Long P apers) ,\nvolume 1, pp. 1870–1879, 2017.\nClark, C. and Gardner, M. Simple and effective multi-\nparagraph reading comprehension. In Annual Meeting\nof the Association for Computational Linguistics , 2017.\nDai, A. M. and Le, Q. V . Semi-supervised sequence learn-\ning. In Advances in neural information processing sys-\ntems, pp. 3079–3087, 2015.\nDevlin, J., Chang, M.-W ., Lee, K., and T outanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805 ,\n2018.\nGraves, A., W ayne, G., and Danihelka, I. Neural turing\nmachines. ArXiv, abs/1410.5401, 2014.\nGuu, K., Hashimoto, T . B., Oren, Y ., and Liang, P . Gen-\nerating sentences by editing prototypes. T ransactions\nof the Association for Computational Linguistics , 6:437–\n450, 2018.\nHashimoto, T . B., Guu, K., Oren, Y ., and Liang, P . S.\nA retrieve-and-edit framework for predicting structured\noutputs. In Advances in Neural Information Processing\nSystems, pp. 10052–10062, 2018.\nJoshi, M., Chen, D., Liu, Y ., W eld, D. S., Zettlemoyer,\nL., and Levy, O. SpanBER T: Improving pre-training\nby representing and predicting spans. arXiv preprint\narXiv:1907.10529 , 2019.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer,\nL., and Lewis, M. Generalization through memo-\nrization: Nearest neighbor language models. ArXiv,\nabs/1911.00172, 2019.\nKiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urta-\nsun, R., T orralba, A., and Fidler, S. Skip-thought vectors.\nIn Advances in neural information processing systems ,\npp. 3294–3302, 2015.\nKwiatkowski, T ., Palomaki, J., Rhinehart, O., Collins, M.,\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kel-\ncey, M., Devlin, J., et al. Natural questions: a benchmark\nfor question answering research. T ransactions of the As-\nsociation for Computational Linguistics , 2019.\nLample, G., Sablayrolles, A., Ranzato, M., Denoyer, L.,\nand J´ egou, H. Large memory layers with product keys.\nIn Advances in Neural Information Processing Systems ,\npp. 8546–8557, 2019.\nLee, K., Salant, S., Kwiatkowski, T ., Parikh, A., Das,\nD., and Berant, J. Learning recurrent span representa-\ntions for extractive question answering. arXiv preprint\narXiv:1611.01436 , 2016.\nLee, K., Chang, M.-W ., and T outanova, K. Latent re-\ntrieval for weakly supervised open domain question an-\nswering. In Proceedings of the Conference of Associa-\ntion for Computational Linguistics , 2019.\nLewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo-\nhamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L.\nBart: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehen-\nsion. ArXiv, abs/1910.13461, 2019.\nREALM: Retrieval-Augmented Language Model Pre-T raining\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692 , 2019.\nMikolov, T ., Chen, K., Corrado, G., and Dean, J. Efﬁcient\nestimation of word representations in vector space. arXiv\npreprint arXiv:1301.3781 , 2013a.\nMikolov, T ., Sutskever, I., Chen, K., Corrado, G. S.,\nand Dean, J. Distributed representations of words and\nphrases and their compositionality. In Advances in\nneural information processing systems , pp. 3111–3119,\n2013b.\nMiller, A., Fisch, A., Dodge, J., Karimi, A.-H., Bordes, A.,\nand W eston, J. Key-value memory networks for directly\nreading documents. arXiv preprint arXiv:1606.03126 ,\n2016.\nMin, S., Chen, D., Hajishirzi, H., and Zettlemoyer, L. A dis-\ncrete hard em approach for weakly supervised question\nanswering. arXiv preprint arXiv:1909.04849 , 2019a.\nMin, S., Chen, D., Zettlemoyer, L., and Hajishirzi,\nH. Knowledge guided text retrieval and reading\nfor open domain question answering. arXiv preprint\narXiv:1911.03868 , 2019b.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. In Proc. of NAACL , 2018.\nPeters, M. E., Neumann, M., IV , R. L. L., Schwartz, R.,\nJoshi, V ., Singh, S., and Smith, N. A. Knowledge en-\nhanced contextual word representations, 2019.\nPetroni, F ., Rockt¨ aschel, T ., Lewis, P ., Bakhtin, A., Wu, Y .,\nMiller, A. H., and Riedel, S. Language models as knowl-\nedge bases? arXiv preprint arXiv:1909.01066 , 2019.\nRadford, A., Narasimhan, K., Salimans, T ., and Sutskever,\nI. Improving language understanding with unsupervised\nlearning. T echnical report, OpenAI, 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multi-\ntask learners. OpenAI Blog , 2019.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W ., and Liu, P . J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683 , 2019.\nRajpurkar, P ., Zhang, J., Lopyrev, K., and Liang, P . Squad:\n100,000+ questions for machine comprehension of text.\nIn Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing , pp. 2383–\n2392, 2016.\nRajpurkar, P ., Jia, R., and Liang, P . Know what you don’t\nknow: Unanswerable questions for squad. arXiv preprint\narXiv:1806.03822 , 2018.\nRam, P . and Gray, A. G. Maximum inner-product search us-\ning cone trees. In Proceedings of the 18th ACM SIGKDD\ninternational conference on Knowledge discovery and\ndata mining , pp. 931–939, 2012.\nRoberts, A., Raffel, C., and Shazeer, N. How much knowl-\nedge can you pack into the parameters of a language\nmodel? arXiv preprint arXiv:TBD , 2020.\nRobertson, S., Zaragoza, H., et al. The probabilistic rele-\nvance framework: Bm25 and beyond. F oundations and\nT rends in Information Retrieval , 3(4):333–389, 2009.\nSang, E. T . K. and De Meulder, F . Introduction to the conll-\n2003 shared task: Language-independent named entity\nrecognition. In Proceedings of the Seventh Conference\non Natural Language Learning at HLT -NAACL 2003 , pp.\n142–147, 2003.\nSeo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H.\nBidirectional attention ﬂow for machine comprehension.\nIn International Conference on Learning Representa-\ntions, 2016.\nShen, F ., Liu, W ., Zhang, S., Y ang, Y ., and T ao Shen,\nH. Learning binary codes for maximum inner product\nsearch. In Proceedings of the IEEE International Con-\nference on Computer V ision , pp. 4148–4156, 2015.\nShrivastava, A. and Li, P . Asymmetric lsh (alsh) for sub-\nlinear time maximum inner product search (mips). In\nAdvances in Neural Information Processing Systems , pp.\n2321–2329, 2014.\nSukhbaatar, S., W eston, J., Fergus, R., et al. End-to-end\nmemory networks. In Advances in neural information\nprocessing systems , 2015.\nW eston, J., Chopra, S., and Bordes, A. Memory networks.\narXiv preprint arXiv:1410.3916 , 2014.\nREALM: Retrieval-Augmented Language Model Pre-T raining\nA. Derivation of the gradient with respect to\nthe knowledge retriever\nW e compute the gradient of the REALM pre-training objec-\ntive (a log-likelihood) with respect to the parameters of th e\nknowledge retriever, θ:\n∇ log p(y | x) =p(y | x)−1∇p(y | x)\n= p(y | x)−1 ∑\nz\np(y | z, x )∇p(z | x)\n= p(y | x)−1 ∑\nz\np(y | z, x )p(z | x)∇ log p(z | x)\n=\n∑\nz\np(z | y, x )∇ log p(z | x),\nwhere the last line follows from applying conditional\nBayes’ rule. W e can then expand ∇ log p (z | x) as:\n∇ log p(z | x) =∇ log exp f(x, z )∑\nz′ exp f(x, z ′)\n= ∇\n[\nf(x, z ) − log\n∑\nz′\nexp f(x, z ′)\n]\n= ∇f(x, z ) −\n∑\nz′\np(z′ | x)∇f(x, z ′)\nPlugging this back into the ﬁrst set of equations yields:\n∇ log p (y | x) =\n∑\nz\np (z | y, x )\n[\n∇f(x, z ) −\n∑\nz′\np (z′ | x) ∇f(x, z ′)\n]\n=\n∑\nz\np (z | y, x ) ∇f(x, z ) −\n∑\nz′\np (z′ | x) ∇f(x, z ′)\n=\n∑\nz\n[p (z | y, x ) − p (z | x)] ∇f(x, z )\n=\n∑\nz\n[ p (y | z, x ) p (z | x)\np (y | x) − p (z | x)\n]\n∇f(x, z )\n=\n∑\nz\n[ p (y | z, x )\np (y | x) − 1\n]\np (z | x) ∇f(x, z ).\nIn the second line, we used the fact that the overall expres-\nsion is an expectation with respect to p (z | y, x ), and the\nterms which depend on z′ but not z can be moved out of\nthat expectation.\nB. Connection between REALM and\nsupervised learning\nFrom the equations in Appendix A, we saw that\n∇ log p (y | x) =\n∑\nz\n[p (z | y, x ) − p (z | x)] ∇f(x, z ).\nSuppose that there exists one document z∗ which causes\nthe model to achieve perfect prediction accuracy (i.e.,\np (y | z∗, x ) = 1), while all other documents z′ result in\nzero accuracy (i.e., p (y | z′, x ) = 0 ). Under this set-\nting, p (z∗ | y, x ) = 1(provided that p (z∗ | x) is non-zero),\nwhich causes the gradient to become\n∇ log p (y | x) =∇f (x, z ∗) −\n∑\nz\np (z | x) ∇f(x, z )\n= ∇ log p (z∗ | x) .\nFrom this, we see that gradient descent on the REALM ob-\njective is equivalent to gradient descent on log p (z∗ | x).\nThis is none other than the typical maximum likelihood\ntraining objective used in supervised learning, where z∗ is\nthe “gold” document.\nC. Adapting to new knowledge\nAn explicit retrieval system allows us to adapt to new\nworld knowledge simply by modifying the corpus docu-\nments. T o demonstrate this ability, we replace the knowl-\nedge corpus with a more recent version of Wikipedia cor-\npus after pre-training is done. When the input query is\nabout a fact where the two corpora disagree, REALM can\nchange the prediction to reﬂect the updated information,\nas exempliﬁed in T able\n4. However, even with an ex-\nplicit retrieval mechanism, the knowledge-augmented en-\ncoder will still end up remembering some world knowl-\nedge, making the prediction of some input sentences not\nupdated with the new corpus. (For instance, the model pre-\ndicts “ Thatcher” for “\nis the prime minister\nof United Kingdom.” on both corpora, perhaps due to\nthe frequent mention of her name in Wikipedia articles.)\nD. Retrieval Utility\nThe null document ∅ described in Section\n3.4 provides a\nway to measure the importance of a retrieved document z:\nwe deﬁne the retrieval utility (RU) of z for the masked\ninput x as the difference between the log-likelihood of\nthe knowledge-augmented encoder when conditioning on\nz versus on ∅ :\nRU(z | x) = logp(y | z, x ) − log p(y | ∅ , x ). (2)\nA negative RU shows that z is less useful for predicting y\nthan the null document. This could mean that z is irrelevant\nto x, but could also mean that the masked tokens in x do\nnot require world knowledge to predict, or that the world\nknowledge is sufﬁciently commonplace it has been baked\ninto the model’s parameters. In practice, we ﬁnd that RU\nincreases steadily over the course of pre-training, and is\nmore predictive of good performance on the downstream\ntask of Open-QA than even the overall log-likelihood. An\nexample of how RU behaves over time and across different\nsettings is in Figure\n4.\nREALM: Retrieval-Augmented Language Model Pre-T raining\nx: “ Jennifer formed the production company Excellent Cadaver.”\nBER T also (0.13), then (0.08), later (0.05), . . .\nREALM ( Z =20 Dec 2018 corpus) smith (0.01), brown (0.01), jones (0.01 )\nREALM ( Z =20 Jan 2020 corpus) lawrence (0.13), brown (0.01), smith (0.01), . . .\nT able 4. An example where REALM adapts to the updated knowledge corpu s. The Wikipedia page “Excellent Cadaver” was added in\n2019, so the model was not about to recover the word when the kn owledge corpus is outdated (2018). Interestingly , the same REALM\nmodel pre-trained on the 2018 corpus is able to retrieve the d ocument in the updated corpus (2020) and generate the correc t token,\n“ Lawrence”.\n0 50 100 150 200\n0\n1\n2\n3\nPre-training Steps (Thousands)\nRetrieval Utility\nSalient span masking\nRandom span masking\nRandom uniform masking\nFigure 4. The Retrieval Utility (RU, described in Eq. 2) vs the number of pre-training steps. RU roughly estimates t he “usefulness” of\nretrieval. RU is impacted by the choice of masking and the num ber of pre-training steps."
}