{
  "title": "Minding Language Models‚Äô (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker",
  "url": "https://openalex.org/W4385571775",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5039472047",
      "name": "Melanie Sclar",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5032948428",
      "name": "Sachin Kumar",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5112759123",
      "name": "Peter West",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5009625151",
      "name": "Alane Suhr",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A5102992157",
      "name": "Yejin Choi",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A5062910836",
      "name": "Yulia Tsvetkov",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4389009528",
    "https://openalex.org/W4321277158",
    "https://openalex.org/W3196345292",
    "https://openalex.org/W4287084089",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W46539433",
    "https://openalex.org/W4378474033",
    "https://openalex.org/W2970536767",
    "https://openalex.org/W2589691014",
    "https://openalex.org/W2804778516",
    "https://openalex.org/W3035850279",
    "https://openalex.org/W4385574293",
    "https://openalex.org/W3170629081",
    "https://openalex.org/W4287117470",
    "https://openalex.org/W2030904969",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W2139188400",
    "https://openalex.org/W4389009448",
    "https://openalex.org/W4235688607",
    "https://openalex.org/W2339119186",
    "https://openalex.org/W3200664512",
    "https://openalex.org/W3196886373",
    "https://openalex.org/W4312091000",
    "https://openalex.org/W3213030159",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2951976932",
    "https://openalex.org/W2154851666",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W4309663019",
    "https://openalex.org/W4311992335",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4385570291",
    "https://openalex.org/W3180258554",
    "https://openalex.org/W2472908287",
    "https://openalex.org/W2039421648",
    "https://openalex.org/W2123947560",
    "https://openalex.org/W3177898556",
    "https://openalex.org/W2001931243",
    "https://openalex.org/W2093410327",
    "https://openalex.org/W766039104",
    "https://openalex.org/W2141538250",
    "https://openalex.org/W3211375313",
    "https://openalex.org/W3160674997",
    "https://openalex.org/W4385572854",
    "https://openalex.org/W2889107415"
  ],
  "abstract": "Theory of Mind (ToM)‚Äîthe ability to reason about the mental states of other people‚Äîis a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity's beliefs, their estimation of other entities' beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks' theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 13960‚Äì13980\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nMinding Language Models‚Äô (Lack of) Theory of Mind:\nA Plug-and-Play Multi-Character Belief Tracker\nMelanie Sclar1 Sachin Kumar2 Peter West1 Alane Suhr3\nYejin Choi1,3 Yulia Tsvetkov1\n1Paul G. Allen School of Computer Science & Engineering, University of Washington\n2Language Technologies Institute, Carnegie Mellon University\n3Allen Institute for Artificial Intelligence\nmsclar@cs.washington.edu\nAbstract\nTheory of Mind (ToM)‚Äîthe ability to reason\nabout the mental states of other people‚Äîis a\nkey element of our social intelligence. Yet, de-\nspite their ever more impressive performance,\nlarge-scale neural language models still lack ba-\nsic theory of mind capabilities out-of-the-box.\nWe posit that simply scaling up models will not\nimbue them with theory of mind due to the in-\nherently symbolic and implicit nature of the phe-\nnomenon, and instead investigate an alternative:\ncan we design a decoding-time algorithm that\nenhances theory of mind of off-the-shelf neu-\nral language models without explicit supervi-\nsion? We present SYMBOLIC TOM, a plug-and-\nplay approach to reason about the belief states\nof multiple characters in reading comprehen-\nsion tasks via explicit symbolic representation.\nMore concretely, our approach tracks each en-\ntity‚Äôs beliefs, their estimation of other entities‚Äô\nbeliefs, and higher-order levels of reasoning, all\nthrough graphical representations, allowing for\nmore precise and interpretable reasoning than\nprevious approaches. Empirical results on the\nwell-known ToMi benchmark (Le et al., 2019)\ndemonstrate that SYMBOLIC TOM dramatically\nenhances off-the-shelf neural networks‚Äô theory\nof mind in a zero-shot setting while showing ro-\nbust out-of-distribution performance compared\nto supervised baselines. Our work also reveals\nspurious patterns in existing theory of mind\nbenchmarks, emphasizing the importance of\nout-of-distribution evaluation and methods that\ndo not overfit a particular dataset.\n1 Introduction\nReasoning about other people‚Äôs intentions, desires,\nthoughts, and beliefs is a cornerstone of human\nsocial intelligence. Children naturally develop an\nunderstanding of every individual‚Äôs unique mental\nstate and how it might impact their actions (Frith\net al., 2003). Known as Theory of Mind (ToM)\n(Premack and Woodruff, 1978), this ability is cru-\ncial for efficient and effective communication.\nAlice and Bob are in a room with a basket and a box. \nAlice puts some celery in the basket and leaves the \nroom. Bob then moves the celery into the box. \nWhere will Bob search for the celery? (*) \nWhere does Bob think that Alice  \nwill look for the celery when she returns? (**)\nAlice\nBob\nis in\nis in\nis in\nroom\nbasket\nbox\ncelery\nis in\nis in is in\nroom\nbasket\nbox\ncelery\n(*)\n(**)\nFigure 1: A simple story requiring theory of mind. Note\nthat Alice‚Äôs belief of the celery‚Äôs location differs from\nreality (i.e. Alice holds a false belief ). Readers must\nreason that Alice will look for the celery where she\nleft it, and that Bob will make that same assumption.\nQuestions shown require different depths of mental state\nmodeling.\nCognitive and literary studies have extensively\nargued theory of mind‚Äôs key role in understanding\nstories, in order to explain and predict each charac-\nter‚Äôs actions (Zunshine, 2006; Carney et al., 2014;\nLeverage et al., 2010; van Duijn et al., 2015, inter\nalia). As exemplified in Figure 1, readers need to\nmodel Bob‚Äôs mental state (called first-order ToM),\nas well as Bob‚Äôs estimation of Alice‚Äôs mental state\n(second-order ToM) to answer questions.\nDespite recent progress in language understand-\ning abilities, large language models have been\nshown to lack theory of mind skills (Sap et al.,\n2022). Existing efforts to enable them have primar-\nily relied on supervised methods (e.g., Grant et al.,\n13960\n2017; Nematzadeh et al., 2018; Arodi and Cheung,\n2021). However, current reading comprehension\ndatasets for theory of mind reasoning are simplistic\nand lack diversity, leading to brittle downstream\nmodels which, as we show, fail in the presence of\neven slight out-of-distribution perturbations.\nWe introduce SYMBOLIC TOM, an inference-\ntime method that improves large language models‚Äô\ntheory of mind capabilities by augmenting them\nwith an explicit symbolic graphical representation\nof each character‚Äôs beliefs. Unlike prior efforts, our\napproach does not require training and instead di-\nvides the problem into simpler subtasks, leveraging\noff-the-shelf models to solve them, and carefully\nconsolidating their results. This makes SYMBOLIC -\nTOM significantly more robust than existing mod-\nels trained specifically for theory of mind behavior.\nWhile beliefs about the world state differ among\npeople, most existing work on encoding belief\nstates do not model this behavior relying on sin-\ngular graphs (Jansen, 2022; Jacqmin et al., 2022).\nSYMBOLIC TOM, instead, utilizes a set of graphs,\neach representing what the character p1 thinks that\np2 believes that [...] pm assumes to be the cur-\nrent state of the world, where m is the maximum\nreasoning depth as determined by the user. This ex-\nplicit, recursive mental state representation enables\nthe model to answer questions from the perspec-\ntive of each character. SYMBOLIC TOM‚Äôs process\nof selecting and querying a particular character‚Äôs\ngraph grounds it in cognitive science research ar-\nguing theory of mind as an essential mechanism\nof selective attention (Leslie et al., 2004). Our\napproach also instills desirable inductive biases,\nsuch as object permanence‚Äîfor example, object\nlocations (represented by edges in the graphs) are\nassumed to be constant until the method can infer\na change. Although existing NLP datasets only\ntest up to second-order reasoning (i.e., m ‚â§2),\nSYMBOLIC TOM is designed to work at any depth.\nSYMBOLIC TOM dramatically improves the per-\nformance of large language models in theory of\nmind reading comprehension tasks. For example,\nGPT-3-Davinci‚Äôs (Brown et al., 2020) accuracy on\nthe ToMi benchmark (Le et al., 2019) increases by\n38 absolute points using SYMBOLIC TOM (yield-\ning 92% accuracy averaging across question types).\nFurthermore, we extend the ToMi test sets with\ndiverse story structures and sentence paraphrases\nand demonstrate that our approach is significantly\nmore robust than supervised approaches.\n2 Motivation and Background\nAlthough large-scale language models have re-\ncently shown improvements in some classic theory\nof mind examples, they are still far from reliably\nshowing theory of mind capabilities (Sap et al.,\n2022; Yu et al., 2022; Ullman, 2023; Shapira et al.,\n2023). While the training data for these models\nincludes human-written stories which require the-\nory of mind reasoning, this information is largely\nimplicit and hence difficult for models to learn.\nChatGPT and GPT3-Davinci‚Äôs incorrect answers\nto Figure 1‚Äôs question #2 are shown below.1\nChatGPT (gpt-3.5-turbo): Based on the information\nprovided, Bob would likely think that Alice will look for\nthe celery in the box when she returns. Since Bob moved\nthe celery from the basket to the box, he would assume that\nAlice would expect to find it in its new location.\nGPT3 (text-davinci-003): Bob will likely think that\nAlice will look for the celery in the box, since that is where\nhe moved it.\nNatural stories which make theory of mind ex-\nplicit are scarce, necessitating automatically gener-\nated, template-based datasets like ToM-bAbI (Ne-\nmatzadeh et al., 2018) and ToMi (Le et al., 2019).\nHowever, templated narratives cover limited types\nof interactions, and include only simplistic dis-\ncourse and sentence structures. On the other hand,\nrelying on human-generated data, e.g., in situated\ndialogue (Bara et al., 2021), leads to barriers in\ndataset size due to high annotation costs. More-\nover, another source of data‚Äîtext-based games\nwith multiple characters‚Äîalso faces limitations;\nin particular, modeling mental states is required\nmainly to infer intents (Zhou et al., 2022) and to\nmaintain a consistent style of each character (Qiu\net al., 2022). Rather, in this work, we aim to study\nand evaluate differences in knowledge and beliefs\namong multiple characters, traditional cognitive\naspects of theory of mind.\nTo the best of our knowledge, the only available\ndatasets for measuring theory of mind in reading\ncomprehension tasks are ToM-bAbI and ToMi. Be-\ncause of their templated nature, supervised training\non them is prone to overfitting to spurious artifacts\nin the data. While ToMi was developed to counter\nthis behavior in ToM-bAbI by introducing noise in\nthe form of flexible sentence ordering and distrac-\ntor sentences and characters, we show it still faces\nthe same pitfalls.\n1Queried on May 22, 2023 with top_p=1 and\ntemperature=0. Given the non-deterministic and continu-\nously changing nature of these models, exact examples may\nnot produce the same response we report.\n13961\n3. Feed to Language Model1. Detect entities in the question, \nretrieve the relevant belief graph and \nperform recursion over the question\n2. Retrieve sentences \ncaptured by the graph\nWhere does \nBob think  \nthat Alice  \nwill look for  \nthe celery?\nWhere is the celery?\nis in\nis in\nis in\nis in\nroom\nbasket\nboxcelery\nBobAlice\nAlice and Bob are in a \nroom with a basket and a \nbox. Alice puts some \ncelery in the basket and \nleaves the room.\nbasket\nBBob,Alice\t=\nFigure 2: Pipeline overview of SYMBOLIC TOM, a decoding-time algorithm that enhances large language models‚Äô\ntheory of mind capabilities. SYMBOLIC TOM does not require training: it divides the problem into smaller subtasks\nand uses off-the-shelf models to solve them. Given a passage, SYMBOLIC TOM constructs explicit symbolic\ngraphical representations of each character‚Äôs belief states (step 1). To answer ToM questions, it retrieves relevant\nsentences from the graph (step 2) and then queries the LLM in a zero-shot manner (step 3).\nDue to theory of mind‚Äôs inherently implicit na-\nture and limited naturally available data, in this\nwork, we argue against supervision as a way\nforward and instead call for unsupervised, or\ninference-time approaches that combine modern\nneural models and traditional symbolic algorithms.\n3 Methods\n3.1 S YMBOLIC TOM: Algorithm Overview\nOur goal is to automatically answer reading com-\nprehension questions given a story involving mul-\ntiple characters, without requiring any supervised\ntraining or fine-tuning on this task. We first in-\ntroduce key notation, then provide a high-level\noverview of SYMBOLIC TOM (Algorithm 1).\nNotation We use the term k-th order theory of\nmind to refer to an estimate of what a character\np1 thinks that p2 thinks that [...] pk thinks about\nthe world state. We denote this belief by Bp1,...,pk.\nWe let k ‚â§m, where m is a maximum reasoning\ndepth. This is a user-specified limit, denoting the\nmaximum recursion that the reader is assumed to\nbe capable of performing. For instance, in Figure\n1, questions #1 and #2 measure 1st- and 2nd-order\ntheory of mind respectively; BBob refers to Bob‚Äôs\nbeliefs about the current world state, and BBob,Alice\nrepresents Bob‚Äôs estimation of Alice‚Äôs beliefs about\nthe world state. In this work, Bp1,...,pk only repre-\nsents beliefs about the current world state, without\nadditional modeling of other characters‚Äô mental\nstates, such as their opinions.\nA benefit of this notation is that any belief\nstate can be represented as an m-th order one.\nWe assume that what pk thinks that pk thinks is\nequivalent to what pk thinks, and by induction,\nBp1...pk ‚â°Bp1,...,pk,pk,...,pk, where the last pk is\nrepeated m ‚àík times. We adopt this notation\ngoing forward, denoting all states as m-th or-\nder. As a conceptual note, the set of belief states\n{Bp1...pk,qk+1...qm |‚àÄqk+1, . . . , qm}represents the\nmental state from the perspective of p1, . . . , pk, us-\ning m ‚àík order of theory of mind.\nLocal and Global Context We represent each\nBp1...pk as a graph (a simplified version is de-\npicted in Figure 1) where each node represents\nan entity (e.g. a character, object, room, con-\ntainer) and each edge connects two nodes with a\nstated relationship in the story. We construct the\ngraphs by iterating through a story one sentence\nat a time, and adding both nodes and edges to the\ngraph (BELIEF TRACKING STRUCTURE ; described\nin ¬ß3.2 and Algorithm 2). Each edge is also paired\nwith the sentence from the story from which it was\nconstructed. We refer to the set of all belief state\ngraphs as the local contexts. We also maintain a\nglobal context graph, denoted byG, which contains\nthe true world state. G has an identical structure to\nBp1...pk. See A.1 for a detailed definition of G.\nQuestion Answering After parsing a story and\nconstructing the complete set of belief-tracking\nstructures, we can use these structures to answer\nquestions by querying the appropriate graph and\nconsidering it as the real-world state. For example,\nif the question is ‚ÄúWhere will Bob think that Alice\nwill look for the celery?‚Äù, we retrieve BBob, Alice,\nbut if instead the question were ‚ÄúWhere will Bob\n13962\nlook for the celery?‚Äù, we would retrieve BBob. In\nboth cases, we would ask ‚ÄúWhere is the celery?‚Äù\non the retrieved graph. Figure 2 shows an example\nof the full pipeline.\nGiven a question, we identify the relevant char-\nacters p1, . . . , pk mentioned in order heuristically,\nand rephrase the question to ask directly about\nthe world state ( PROCESS QUESTION ; owing to\nthe questions‚Äô templatic nature in our evalua-\ntion data, this approach rephrases all questions\ncorrectly).2 We then retrieve the corresponding\ngraph; i.e., Bp1,...,pk, of which we can simply\nask the question ‚ÄúWhere is the celery?‚Äù. To\nobtain the answer, we first reconstruct a sub-\nset S‚Ä≤ of sentences in the original story, consist-\ning of those represented by the retrieved graph\n(SENTENCES REPRESENTED BYGRAPH ). We then\nuse a large language model L to answer the\nsimplified question zero-shot given S‚Ä≤, using\nas input the sentences in S‚Ä≤ in the same order\nas they appeared in the original text, and pre-\nserving phrasing. We optionally further filter S‚Ä≤\nbased on the entities mentioned in the question\n(FILTER BASED ONQUESTION ). An ablation study\nshowed this last step can often be skipped (see\nAppendix C.1).\nAlgorithm 1 SYMBOLIC TOM\nB ‚ÜêBELIEF TRACKING STRUCTURE (sentences)\np1,. . ., pk, question‚Ä≤ ‚ÜêPROCESS QUESTION (question)\nS‚Ä≤ ‚ÜêSENTENCES REPRESENTED BYGRAPH (Bp1,...,pk )\nS‚Ä≤‚Ä≤ ‚ÜêFILTER BASED ONQUESTION (S‚Ä≤, question)\nreturn S‚Ä≤‚Ä≤, question‚Ä≤\n3.2 Computing the Belief Graphs Bp1...pk\nAssuming each story is told chronologically, SYM-\nBOLIC TOM processes each sentence s sequentially\nin two stages (Algorithm 2). First, it extracts all\nactions in s and updates the global context G from\nan omniscient point of view while identifying the\ncharacters (W) who witnessed actions and world\nstate changes described in the sentence. Second,\nfor each witness w ‚ààW, it propagates this new\ninformation to update w‚Äôs local contexts; i.e., we\nonly update Bp1,...,pm with, for 1 ‚â§i ‚â§m, each\npi ‚ààW, and leave the rest unchanged.\nAs an example, when processing the last sen-\ntence in Figure 3, we update Bob and Charles‚Äôs\nstate ( BBob and BCharles) and the perception of\n2Our explorations show that GPT3 is also capable of\nrephrasing the questions zero-shot (see ¬ßA.3), but we refrained\nfrom this solution due to budget concerns.\nAlice and Bob are in a room with a basket and a box. Alice puts \nsome celery in the basket and leaves the room. Bob then moves \nthe celery into the box. Charles immediately enters the room. \nCharles puts the celery in a chest.\nroom\nbasket\nchestcelery\nBob\nCharlesAlice\nbox\nùí≤ = {Bob,¬†Charles}\n1. Update global context  and \ndetect witnesses  as all entities \nin the same connected component \nas the edges inserted.\nG\nùí≤\n2. Propagate new information to local contexts if and only if \nall people involved are witnesses.\n‚úò\t\tBAlice,Alice\t\t\n‚úò\t\tBAlice,Bob\t\n‚úò\t\tBAlice,Charles\t = ‚àÖ\n‚úò\t\tBBob,Alice\t\n‚úì\tBBob,Bob\t=\tBBob\t\n‚úì\tBBob,Charles\n‚úò\t\tBCharles,Alice \t\n‚úì\tBCharles,Bob\t\n‚úì\tBCharles,Charles\t=\tBCharles\n= ‚àÖ\n=\twhat X thinks that Y thinks is the current world stateBX,Y\t\t=\t\nFigure 3: High-level depiction of the belief update pro-\ncedure for m = 2. Bp1,...,pk denotes a graph, and the\ngraph updating procedure is detailed in the main text.\nAlgorithm 2 Belief Tracking\nfunction BELIEF TRACKING STRUCTURE (sentences)\nfor s ‚ààsentences do\nG, W‚Üê GLOBAL CONTEXT UPDATE (G, s)\nfor all [p1, . . . , pm] ‚ààWm do\nBp1...pm ‚ÜêLOCAL CONTEXT UPDATE (Bp1...pm,G,s)\nend for\nend for\nend function\nothers‚Äô respective state (BBob,Charles, BCharles, Bob),\nbut we need not update Alice‚Äôs state, or Bob and\nCharles‚Äôs perception of Alice‚Äôs mental state, be-\ncause she did not witness the actions described.\n3.2.1 Detecting Witnesses, Updating Graphs,\nand Propagating Knowledge\nStarting with an empty graph, for each new sen-\ntence s, we update the global context G by combin-\ning off-the-shelf models in four steps (Algorithm\n3; GLOBAL CONTEXT UPDATE ). First, we detect\nthe existing edges E in G that contradict s. This\nis implemented as detecting Natural Language In-\nference (NLI) contradictions, considering s as the\npremise, and every edge in G as a hypothesis. Sec-\nond, we augment G with new edges and nodes, by\nfirst deriving a natural language representation r\nof the state resulting from the actions described\nin s, and then extract new nodes and edges from\nr as OpenIE triples (Stanovsky et al., 2018). For\nexample, for ‚ÄúBob then moves the celery to the\nbox‚Äù, the resulting state r would be the sentence\n13963\n‚ÄúThe celery is in the box‚Äù. To obtain r from s, we\nprompt a language model such as GPT3 (see Ap-\npendix A.2 for details). After obtaining r, we use\nthe corresponding triple (e.g., (celery, box, is\nin)) to add new nodes and edges to G if not al-\nready present (e.g., the nodes ‚Äúcelery‚Äù and ‚Äúbox‚Äù,\nand a directed edge connecting them labeled by ‚Äúis\nin‚Äù). Importantly, we only add edges that represent\npositive relations between nodes; i.e., there will not\nbe an edge representing ‚ÄúThe celery is not in the\nbox‚Äù. Third, we detect the witnesses Wof the ac-\ntions described in s. Since each character will be a\nnode in G, we identify Was all the characters that\nare in the same connected component as the newly\nadded edges. Finally, we remove all edges E that\nare no longer valid in G as identified by the NLI\ncontradictions. This step is done last to ensure all\nwitnesses are found before their edges are deleted.\nAlgorithm 3 World State Beliefs Graphs Update\nfunction GLOBAL CONTEXT UPDATE (G, s)\nE ‚ÜêDETECT CONTRADICTING EDGES (G, s)\nG ‚ÜêG ‚à™TRIPLES (RESULTING STATE(s))\nW‚Üê FIND WITNESSES (G)\nG ‚ÜêG \\E\nreturn G, W\nend function\nfunction LOCAL CONTEXT UPDATE (C, G, s)\nE ‚ÜêDETECT CONTRADICTING EDGES (G, s)\nC ‚ÜêC ‚à™TRIPLES (RESULTING STATE(s))\nC ‚ÜêPROPAGATE KNOWLEDGE (G, C, s)\nC ‚ÜêC \\E\nreturn C\nend function\nThe local contexts (Bp1,...,pk) are updated simi-\nlarly (LOCAL CONTEXT UPDATE in Algorithm 3),\nexcept for an additional step of knowledge propaga-\ntion. While performing an action, a character may\nimplicitly gain information not described in the\ntext. For example, when entering a room, a char-\nacter may gain knowledge of the people and visi-\nble objects in the room. This knowledge (already\npresent in G, which tracks the omniscient world\nstate) needs to be propagated to each Bp1,...,pk with\neach pi ‚ààW. As G represents the true world state,\nwe simplify the problem: if a character pi is in\na specific connected component D of G, then it\npossesses all knowledge encoded in D. To model\nimplicit knowledge gain, we add all edges in D to\nBp1,...,pk. As D represents the latest global con-\ntext information, we remove from the local context\nedges that are in Bp1,...,pk but not in D (represent-\ning outdated beliefs about the world state).\n3.3 Notes on Memory Efficiency\nMemory requirements grow exponentially with m,\nthe maximum order of theory of mind considered.\nHowever, m in practice is small, as humans find\ntasks increasingly challenging as m increases. For\nexample, psychological tests for m = 3are aimed\nat teenagers and adults (Valle et al., 2015). All\nexperiments in this work are done with m = 2,\nthe maximum order of theory of mind reasoning\nthat current datasets evaluate. If memory were a\nconcern, one could process the questions first for\nmemory efficiency, and compute only the graphs\nBp1,...,pk required for target queries.\n4 Fundamental Issues in Existing ToM\nDatasets\nConstruction of ToMi As introduced in ¬ß2, the\nsole large-scale theory of mind dataset for reading\ncomprehension tasks is ToMi (Le et al., 2019). Bar-\nring its added distractor characters and sentences,\nToMi strictly mimics the Sally-Anne test, a widely\nadopted evaluation for assessing children‚Äôs social\ncognitive ability to reason about others‚Äô mental\nstates (Wimmer and Perner, 1983; Baron-Cohen\net al., 1985). Stories are structured are as follows:\ncharacters A and B are in a room, and A moves\nan object from an opaque container to another; B\nmay or may not leave the room before A moves\nthe object. B will know the object‚Äôs new location\nif and only if they were in the room at the time\nit was moved. Four types of ToM questions are\nposed: first-order or second-order, probing a char-\nacter about either a true or a false belief (i.e, belief\nthat matches reality or not). ToMi also includes\nquestions probing about reality (or zeroth-order\nToM, Sclar et al., 2022) and memory.\nToMi has six types of sentences (i.e. six primi-\ntives) with set phrasing. These include someone (a)\nentering or (b) exiting a room; the location of (c)\nan object or (d) a person; (e) someone moving an\nobject; and (f) someone‚Äôs opinion about an object\n(distractors). Primitives are combined into stories\nwith a finite list of possible orderings. Despite the\nlimited types of primitives, correctly answering\nquestions requires high-order levels of reasoning.\nTemplated stories are filled with randomly sam-\npled objects, locations, containers, and rooms from\na set list. ToMi implicitly assumes that questions\nabout the story do not depend on these decisions,\nonly on the underlying story template. Yet, in a\nsmall-scale human study, we find physical com-\n13964\n1. Oliver entered the front yard.\n2. Ethan entered the front yard.\n3. Liam entered the kitchen.\n4. objectA is in the basket.\n5. Ethan exited the front yard.\n6. Ethan entered the kitchen.\n7. Oliver moved objectA to the containerX.\n8. Where does Ethan think objectA is?\nToMi Gold Label: basket\nTable 1: Interpretation of ambiguities in ToMi can be\naffected by commonsense. In the above template, the\ncorrect label is that Ethan thinks objectA is in the bas-\nket, as this is where he last saw it. Setting objectA\nto hat and containerX to box results in 80% human\naccuracy. However, setting these to apple and pantry,\naccuracy drops to 20%. Physical commonsense suggests\nthe pantry is likely in the kitchen, changing the answer\nto pantry, but regardless of the identity of objectA or\ncontainerX, the correct label in ToMi is basket.\nmonsense leads human answers to change, and\ndisagree with ToMi‚Äôs labels depending on the noun.\nTable 1 presents an example where the object and\ncontainer have a large effect on human responses.3\nResolving Unintentional Ambiguities ToMi‚Äôs\nstory construction process often leaves object loca-\ntions ambiguous, which forces humans to (incor-\nrectly) rely on their physical commonsense. For\nexample, the location of the basket in line 4 of Ta-\nble 1 is ambiguous. This ambiguity is at times\nresolved at a later step in the story (Arodi and\nCheung, 2021), but it is not true for all cases, and\nthese resolutions were not expressly intended by\nToMi‚Äôs original design. This complicates the task\nbeyond theory of mind. For example, in Table 1,\nthe reader must conclude from ‚ÄúOliver is in front\nyard‚Äù, ‚ÄúOliver moved the objectA (...)‚Äù, and ‚ÄúThe\nobjectA is in basket‚Äù that the basket is in the front\nyard, and hence that Ethan saw it there. This re-\nquires 3-hop reasoning, and knowing ahead of time\nthat, in ToMi, characters do not change rooms un-\nless explicitly stated.\nTo solve these unintentional ambiguities and ad-\nditional 3-hop reasoning requirements, and instead\nsolely measure theory of mind reasoning skills, we\nautomatically add a sentence that disambiguates\nthe location of each container immediately after\neach primitive (c) or (e) (e.g., adding ‚ÄúThe basket\n3Using Amazon Mechanical Turk, we present 20 humans\nwith the template in Table 1, using either (hat,box) or (apple,\npantry). Workers are paid $1 per HIT.\nis in the front yard‚Äù as line 5 in Table 1). Finally,\nas reported in Arodi and Cheung (2021); Sap et al.\n(2022), ToMi contains some mislabeled second-\norder questions, which we also correct.\n5 Experiments\nWe experiment with several base LMs, and eval-\nuate each of them both out-of-the-box via zero-\nshot prompting, and by applying SYMBOLIC TOM\nto ToMi stories to produce answers. We evalu-\nate Macaw-3B (Tafjord and Clark, 2021), GPT3-\n{Curie,Davinci} (Brown et al., 2020), Flan-T5-\n{XL,XXL} (Chung et al., 2022), LLaMA-{7B,\n13B} (Touvron et al., 2023), GPT3.5 (OpenAI,\n2022), and GPT4 (OpenAI, 2023). We use W ANLI\n(Liu et al., 2022) for identifying NLI contradictions,\nand the AllenNLP library (Gardner et al., 2018) for\nOpenIE. We additionally refine each subject and\nobject in extracted triples to remove any stopwords\nthat may be accidentally included by OpenIE.\nWe first evaluate SYMBOLIC TOM‚Äôs perfor-\nmance as a plug-and-play method for different base\nLMs on ToMi (¬ß5.1). We then test whether per-\nformance gains are robust to ToMi story structure\nmodifications (¬ß5.2). Finally, we explore SYMBOL -\nICTOM‚Äôs robustness to linguistic diversity (¬ß5.3).\nSupervised Models For comparison, we train\ntwo supervised models: Textual Time Travel (TTT)\n(Arodi and Cheung, 2021), and a fine-tuned GPT3-\nCurie. TTT is a modification of EntNet (Henaff\net al., 2017) designed for theory of mind tasks;\nGPT3-Curie is finetuned on 6000 ToMi examples\nfor one epoch. GPT3-Curie achieves near-perfect\nperformance when finetuned on ToMi (98.5% ac-\ncuracy when averaging all questions; Table 5). In-\nterestingly, GPT3-Curie achieves a higher accuracy\nthan the theory of mind-motivated TTT (accuracy\n92.3%). We explore model robustness in ¬ß5.2.\n5.1 In-Domain Evaluation\nWe evaluate all base LMs comparing their perfor-\nmance out-of-the-box, versus when adding SYM-\nBOLIC TOM. Figure 4 shows results by question\ntype, showing dramatic improvements for all the-\nory of mind questions: +62 points in accuracy for\nfirst-order false-belief questions for Flan-T5-XL,\n+78 points in accuracy for second-order false-belief\nquestions for GPT3.5, among other improvements.\nIn addition, we observe all models maintain near-\nperfect performance with and without SYMBOL -\nICTOM in memory questions. Supervised models\n13965\n0.0\n0.5\n1.0\n1st order ToM True Belief 1st order ToM False Belief\n0.0\n0.5\n1.0\n2nd order ToM True Belief 2nd order ToM False Belief\n0.0 0.5 1.0\n0.0\n0.5\n1.0\nReality\n0.0 0.5 1.0\nMemory\nBase model accuracy\nBase model + SymbolicToM accuracy\nFigure 4: Accuracy for each ToMi question type and\nbase model (higher is better). Dots in the upper triangle\nhave higher performance with SYMBOLIC TOM than\nthe base model out-of-the-box. Horizontal lines give\nsupervised models‚Äô performance. Full results in Table 5.\nshow high accuracy for all question types.\nWe only see significant decreases in performance\nfor reality questions in Flan-T5 models. This can\nbe partially attributed to the questions‚Äô phrasing:\nquestions are posed as ‚ÄúWhere is the celery re-\nally?‚Äù. Removing really results in 96% accuracy\nfor Flan-T5-XL. Flan-T5-XXL empirically shows\na bias towards providing a room rather than con-\ntainer as an answer when only one container is men-\ntioned, which is often the case forSYMBOLIC TOM-\nfiltered stories. Rooms are invalid answers in ToMi.\nAn ablation on the final filter function of Algorithm\n1 suggests that keeping more containers in the final\nstory reduces this bias and still yields significant\nimprovements for false-belief questions across all\nmodels (see ¬ßC.1). Besides reality questions, Flan-\nT5-XXL with SYMBOLIC TOM achieves results\ncomparable to the supervised TTT.\n5.2 Story Structure Robustness Test Sets\nWe create three test sets by modifying ToMi‚Äôs sto-\nries structures without adding new types of actions\nor linguistic diversity. These tests were only evalu-\nated once, after finishing development of SYMBOL -\nICTOM. Test sets are defined below. See Appendix\nB.2 for concrete examples.\nD1 D2 D3\nOff-the-shelf models\nMacaw-3B 8 12 30\nFlan-T5-XL 86 51 68\nFlan-T5-XXL 69 59 52\nGPT3-Curie 37 39 57\nGPT3-Davinci 20 25 39\nGPT3.54 1 0 48\nGPT4 58 62 97\nLLaMA-7B 17 17 17\nLLaMA-13B 26 36 37\nSYMBOLIC TOM + Off-the-shelf models\nMacaw-3B 89 (+81) 71 (+60) 70 (+41)\nFlan-T5-XL 76 (-10) 96 (+46) 100 (+33)\nFlan-T5-XXL 93 (+24) 100 (+41) 100 (+49)\nGPT3-Curie 84 (+48) 81 (+42) 73 (+16)\nGPT3-Davinci 92 (+73) 91 (+66) 90 (+50)\nGPT3.5 100 (+99) 100 (+99) 99 (+51)\nGPT4 100 (+42) 100 (+38) 100 ( +4)\nLLaMA-7B 99 (+82) 92 (+75) 88 (+71)\nLLaMA-13B 78 (+52) 84 (+48) 84 (+47)\nSupervised models\nTTT 49 65 78\nFinetuned GPT3 51 68 32\nTable 2: Precision using SYMBOLIC TOM on all ques-\ntions from 100 stories for each of the modified test sets\nDi. Supervised models were trained on ToMi; all others\ndo not require training. Parenthesis reflect differences\nbetween using and not using SYMBOLIC TOM: bold re-\nflects higher overall performance, and green reflects the\nhighest net improvements when usingSYMBOLIC TOM.\nDouble Room False Belief Story (D1) Two false\nbelief substories involving the same two characters\np1, p2 are concatenated to yield a longer, more com-\nplex story. Each substory has different objects be-\ning moved, across different containers. The system\nis probed using all four combinations of second-\norder theory of mind questions involving the two\ncharacters and locations. Questions are evenly split\nbetween the first and second substory.\nThree Active Characters Story ( D2) Three\ncharacters p1, p2, p3 are in the same room, where\nan object o1 and three containers c1, c2, c3 are avail-\nable. The story is as follows: p2 leaves before p1\nmoves o1 from c1 to c2, but p3 witnesses the move.\n4Low scores are due to the model refusing to answer, e.g.\nanswering ‚ÄúThere is no information in the given text to deter-\nmine where Bob thinks Alice searches for the celery. ‚Äù\n13966\nThen, p1 leaves the room. Later, p3 moves the ob-\nject to container c3 without any witnesses. The\nsystem is probed using all combinations of second-\norder theory of mind questions.\nMultiple Object Movements Across Four Con-\ntainers (D3) Two characters p1, p2 are in a room,\nwith a single object, and four containers c1, . . . , c4.\np1 moves the object from c1 to c2 and right before\nleaving the room, p2 enters. p2 then moves the\nobject to c3, and then c4. We probe with all first\nand second-order theory of mind questions.\nResults Supervised models significantly overfit\nto ToMi‚Äôs original story structures (Table 2). In con-\ntrast, all models had high accuracy when equipped\nwith SYMBOLIC TOM, especially larger models,\nsuch as GPT3.5, LLaMA-{7B,13B}, among others.\nD2 may also be used to test third-order ToM rea-\nsoning, asking questions such as ‚ÄúWhere does p1\nthink that p2 thinks that p1 will search for the o1?‚Äù.\nThird-order ToM is a reasoning depth currently\nuntested by available NLP benchmarks. SYMBOL -\nICTOM consistently enhances the performance of\noff-the-shelf LLMs and outperforms supervised\nmethods in the third-order ToM setting. See de-\ntails in Appendix C.2. This experiment showcases\nhow extensions of ToMi may be used to test higher-\norder reasoning. This is the first approach towards\ntesting third-order ToM in LLMs; a benchmark\nto comprehensively test such order of reasoning\nexceeds the scope of this paper.\n5.3 Paraphrasing Robustness Evaluation\nWe assess the robustness of all models when utiliz-\ning various wordings for each sentence. We reword\nall templates using GPT3-Davinci, utilizing dif-\nferent choices of objects, rooms, and names, and\nmanually excluded incorrect paraphrases. The re-\nsulting dataset‚ÄîParaphrasedToMi‚Äîexhibits much\ngreater complexity, as these rewordings can ex-\npress actions in a less straightforward way. All\nparaphrases are shown in Appendix B.1.\nFigure 5 demonstrates significant performance\ndecreases for supervised models transferring to\nParaphrasedToMi. TTT‚Äôs average accuracy drops\n54 points from ToMi, with losses across all ques-\ntion types. Finetuned GPT3 exhibits significant\nlosses in false-belief questions (-40 average accu-\nracy) but is robust for other question types.\nMethods without supervision also suffer signif-\nicant losses, but SYMBOLIC TOM still results in\n0.0\n0.5\n1.0\n1st order ToM True Belief 1st order ToM False Belief\n0.0\n0.5\n1.0\n2nd order ToM True Belief 2nd order ToM False Belief\n0.0 0.5 1.0\n0.0\n0.5\n1.0\nReality\n0.0 0.5 1.0\nMemory\nBase model accuracy\nBase model + SymbolicToM accuracy\nFigure 5: Results for ParaphrasedToMi when prompting\nGPT3 as implementation ofRESULTING STATE (Davinci\nfor all except for Curie). Dots in the upper triangle imply\nperformance with SYMBOLIC TOM is higher than using\nthe base model out-of-the-box. Horizontal lines reflect\nsupervised models‚Äô performance (higher is better).\nlarge improvements for theory of mind questions.\nModels equipped with SYMBOLIC TOM perform\nsignificantly better than the supervised TTT model\nacross all theory of mind questions. Paraphrased-\nToMi is significantly more difficult forSYMBOLIC -\nTOM since it triggers more errors in edge removal\n(due to errors in NLI classification), as well as\nerrors in edge insertion (due to errors in the re-\nsulting state‚Äôs triple extraction). Although comput-\ning RESULTING STATE by prompting the base LMs\nwas successful with original phrasings (as defined\nin ¬ß3.2.1), we observed differences in robustness\nwhen prompting with paraphrases. We found im-\nplementing RESULTING STATE with GPT3 reliable,\nand thus we use it for all models. Results using\nother models are included in ¬ßC.3: false-belief per-\nformance is even better for models like LLaMA,\nGPT3.5, or GPT4.\n6 Related Work\nExisting Approaches Classical reasoning tasks\nrequire achieving some goal, e.g., proving a state-\nment, given a set of facts and universally valid\nrules (e.g., Tafjord et al., 2021). A common ap-\nproach is to decompose the target reasoning task\ninto subtasks, for example by using off-the-shelf\n13967\nLMs (Creswell et al., 2023; Kazemi et al., 2022;\nNye et al., 2021). We use a similar technique in\nSYMBOLIC TOM, breaking the higher-level reason-\ning task into graph reasoning subtasks. Nonethe-\nless, these approaches cannot be simply ported to\nour domain: stories‚Äô facts (i.e. the world state)\nchange over time and are not universally accessi-\nble to all characters, and commonsense rules and\nassumptions like object permanence must made ex-\nplicit. SYMBOLIC TOM‚Äôs design addresses these\nchallenges by maintaining and updating graphs\nabout facts and beliefs as a story progresses.\nIn scenarios where world state changes over time,\nsuch as in text-based games, existing approaches\nmaintain and update structured world representa-\ntions as the world state changes (Ammanabrolu and\nRiedl, 2021; Adhikari et al., 2020). However, while\nthese approaches could potentially be applied in\nour scenario to update G, they would not address\nthe problems of multiple-belief representation or\nknowledge propagation to witnesses‚Äô graphs, with\nsome approaches even being explicitly impossible\nfor modeling second-order ToM (Qiu et al., 2022).\nToM beyond NLP Theory of mind is also cru-\ncial in multi-agent reinforcement learning (Ra-\nbinowitz et al., 2018), including in bidirectional\nsymbolic-communication (Wang et al., 2022; Sclar\net al., 2022), unidirectional natural-language set-\ntings (Zhu et al., 2021); and recently, by combining\nreinforcement learning, planning, and language, to\ncreate a human-level Diplomacy player (, FAIR).\nIt has also received increased attention in human-\ncomputer interaction (Wang et al., 2021) and ex-\nplainable AI (Akula et al., 2022).\nPsychologists divide theory of mind into two\ntypes of reasoning: affective (emotions, de-\nsires) and cognitive (beliefs, knowledge) (Shamay-\nTsoory et al., 2010), with the former developing\nearlier in children (Wellman, 2014). Our work fo-\ncuses on the latter, but the principle of multiple\nbelief representation could also be applied to affec-\ntive theory of mind reasoning. Existing work has\nshown that humans are proficient at second-order or\nhigher false-belief reasoning, also referred to as ad-\nvanced ToM (Bia≈Çecka-Pikul et al., 2017), with evi-\ndence that we can perform even third- and fourth-\norder reasoning (Valle et al., 2015; Osterhaus et al.,\n2016). While, to best of our knowledge, no dataset\nrequires beyond second-order ToM, SYMBOLIC -\nTOM explicitly models the recursive reasoning that\nsupports queries of any reasoning order.\n7 Conclusions\nTheory of mind is an essential social intelligence\nability. Developing agents with theory of mind is\nrequisite for a wide range of applications, including\nreading comprehension, tutoring, dialogue, person-\nalization, and negotiation. For example, in reading\ncomprehension settings (and broadly for natural\nlanguage understanding), having a multi-level un-\nderstanding of texts is crucial for providing mean-\ningful and contextualized answers: stories often\nrely on theory of mind reasoning to create conflict\n(e.g., in murder mysteries, drama, and romances,\nas in the final acts of Romeo and Juliet).\nWe present SYMBOLIC TOM, a plug-and-play\nmethod to enable theory of mind reasoning in lan-\nguage models via explicit symbolic representations\nin the form of nested belief states. SYMBOLIC -\nTOM requires no training or fine-tuning, a key\naspect for a domain with scarce supervised data\nand limited success in learning from massive un-\nlabeled text alone. With experiments on reading\ncomprehension tasks, our approach demonstrates\ndramatic improvement in the accuracy of base lan-\nguage models, especially for false-belief scenarios.\nWe also show that, in contrast to supervised\nmethods, SYMBOLIC TOM is highly robust to\nstory perturbations and out-of-domain inputs where\nsupervised methods suffer significant degrada-\ntions (as in, e.g., Yu et al., 2022).5 Our results show\nthe promise of augmenting neural language mod-\nels with symbolic knowledge for improving their\nsocial reasoning skills. We leave to future work to\ninvestigate similar approaches for other types of\nsocial intelligence; as well as develop new datasets\nthat cover a more diverse set of interactions.\nLimitations\nSYMBOLIC TOM assumes stories are written\nchronologically, which may not hold for some\nhuman-written stories. This may be alleviated us-\ning time-stamping models like Faghihi and Kord-\njamshidi (2021). Furthermore, since we use off-the-\nshelf models (WANLI (Liu et al., 2022) and Ope-\nnIE (Stanovsky et al., 2018)) to create and update\nthe graphs, the presented approach may propagate\nerrors as revealed in the linguistic diversity exper-\niments. However, these issues can be largely alle-\n5As a part of out-of-domain testing, we also create a more\nchallenging version of the available ToM datasets, available\nat https://github.com/msclar/symbolictom along with\na corrected version of ToMi.\n13968\nviated by using more sophisticated models, even\nthe LLMs like GPT3 themselves. We do not exper-\niment with them due to budgetary restrictions.\nCurrently, all NLP datasets available for theory\nof mind reasoning describe Sally-Anne tests. In\nthese datasets, the concept of large distances is\nabsent, meaning that anyone specified to be in a\nlocation is assumed to be a witness of the actions\nthat occur there. This assumption can be violated in\nrealistic settings. For example,‚ÄúAnne is in the USA‚Äù\ndoes not imply she is a witness to every action hap-\npening in the USA. In future work, this approach\ncan be improved by refining the witnesses detec-\ntion algorithm to incorporate physical common-\nsense reasoning. We could also refine the witness\ndetection algorithm by sampling paths between the\ninserted edge and each node referring to a person,\nto query an LM directly on that substory by asking\nif the person witnessed the action. To be able to test\nboth of these ideas, we would need to obtain new\ntheory of mind datasets with significantly more\ntypes of interactions and physical commonsense in\nthe stories.\nEthics Statement\nTheory of mind research at its core deals with rea-\nsoning about the mental states of others. In this\nwork, we focus on reading comprehension, a task\nwhich can similarly be exposed to ethical concerns:\nfor example, when a model makes erroneous pre-\ndictions about the mental states of characters in\nthe description, when it is misused to reason about\nprivate situations, and when it makes predictions\nwhich reinforce social biases. This issue can be ex-\nacerbated if the characters are actual people. In this\nwork, however, we experiment with simple, proto-\ntypical character references from a public dataset,\nand not with actual people. This decision is inten-\ntional. Furthermore, we focus on reasoning about\nphysical objects and observers‚Äô knowledge about\ntheir location in space, which is less prone to ethical\nconcerns. This data can nonetheless lead to biased\ndecisions, such as imbalanced decisions correlated\nwith social attributes like gender (often correlated\nwith names). Future work in this area may include\nscenarios with more realistic human-agent interac-\ntion, such as dialogue tasks, where parties involved\nmay not have the same incentive structure. These\nscenarios will need to be handled with special care\nas they could lead to agents learning to deceive hu-\nmans by exploiting a predicted (lack of) knowledge.\nThe state-of-the-art in machine theory of mind is\nstill far from these capabilities, but we believe it is\nimportant to consider these risks when designing\nexperiments.\nAcknowledgements\nWe thank Lucille Njoo and Tianxing He for the\nvaluable discussions, and Akshatha Arodi for the\nsupport in running the Textual Time Travel code\nbase. S.K. gratefully acknowledges support from\nGoogle Ph.D. Fellowship. We also thank OpenAI\nfor providing academic access to their language\nmodel API. This material is based upon work\npartly funded by the DARPA CMO under Con-\ntract No. HR001120C0124, by DARPA MCS pro-\ngram through NIWC Pacific (N66001-19-2-4031),\nby NSF DMS-2134012, by NSF CAREER Grant\nNo. IIS2142739, and an Alfred P. Sloan Founda-\ntion Fellowship. Any opinions, findings and con-\nclusions or recommendations expressed in this ma-\nterial are those of the authors and do not necessarily\nstate or reflect those of the United States Govern-\nment or any agency thereof.\nReferences\nAshutosh Adhikari, Xingdi Yuan, Marc-Alexandre C√¥t√©,\nMikul√°≈° Zelinka, Marc-Antoine Rondeau, Romain\nLaroche, Pascal Poupart, Jian Tang, Adam Trischler,\nand Will Hamilton. 2020. Learning dynamic belief\ngraphs to generalize on text-based games. Advances\nin Neural Information Processing Systems, 33:3045‚Äì\n3057.\nArjun R. Akula, Keze Wang, Changsong Liu, Sari Saba-\nSadiya, Hongjing Lu, Sinisa Todorovic, Joyce Chai,\nand Song-Chun Zhu. 2022. Cx-tom: Counterfactual\nexplanations with theory-of-mind for enhancing hu-\nman trust in image recognition models. iScience,\n25(1):103581.\nPrithviraj Ammanabrolu and Mark Riedl. 2021. Learn-\ning knowledge graph-based world models of textual\nenvironments. In Advances in Neural Information\nProcessing Systems.\nAkshatha Arodi and Jackie Chi Kit Cheung. 2021. Tex-\ntual time travel: A temporally informed approach\nto theory of mind. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pages\n4162‚Äì4172, Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nCristian-Paul Bara, CH-Wang Sky, and Joyce Chai.\n2021. Mindcraft: Theory of mind modeling for situ-\nated dialogue in collaborative tasks. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 1112‚Äì1125.\n13969\nSimon Baron-Cohen, Alan M Leslie, and Uta Frith.\n1985. Does the autistic child have a ‚Äútheory of\nmind‚Äù? Cognition, 21(1):37‚Äì46.\nMarta Bia≈Çecka-Pikul, Anna Ko≈Çodziejczyk, and Sandra\nBosacki. 2017. Advanced theory of mind in adoles-\ncence: Do age, gender and friendship style play a\nrole? Journal of Adolescence, 56:145‚Äì156.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nJames Carney, Rafael Wlodarski, and Robin Dunbar.\n2014. Inference or enaction? the impact of genre on\nthe narrative processing of other minds. PloS one,\n9(12):e114172.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2023. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. In The\nEleventh International Conference on Learning Rep-\nresentations.\nHossein Rajaby Faghihi and Parisa Kordjamshidi. 2021.\nTime-stamped language model: Teaching language\nmodels to understand the flow of events. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4560‚Äì4570.\nMeta Fundamental AI Research Diplomacy Team\n(FAIR), Anton Bakhtin, Noam Brown, Emily Dinan,\nGabriele Farina, Colin Flaherty, Daniel Fried, An-\ndrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul\nJacob, Mojtaba Komeili, Karthik Konath, Minae\nKwon, Adam Lerer, Mike Lewis, Alexander H.\nMiller, Sasha Mitts, Adithya Renduchintala, Stephen\nRoller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexan-\nder Wei, David Wu, Hugh Zhang, and Markus Zi-\njlstra. 2022. Human-level play in the game of\n<i>diplomacy</i> by combining language models\nwith strategic reasoning. Science, 378(6624):1067‚Äì\n1074.\nC.D. Frith, D.M. Wolpert, Uta Frith, and Christopher D.\nFrith. 2003. Development and neurophysiology of\nmentalizing. Philosophical Transactions of the Royal\nSociety of London. Series B: Biological Sciences ,\n358(1431):459‚Äì473.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS), pages 1‚Äì6,\nMelbourne, Australia. Association for Computational\nLinguistics.\nErin Grant, Aida Nematzadeh, and Thomas L. Griffiths.\n2017. How can memory-augmented neural networks\npass a false-belief task? Cognitive Science.\nMikael Henaff, Jason Weston, Arthur Szlam, Antoine\nBordes, and Yann LeCun. 2017. Tracking the world\nstate with recurrent entity networks. In International\nConference on Learning Representations.\nL√©o Jacqmin, Lina M Rojas Barahona, and Benoit Favre.\n2022. ‚Äúdo you follow me?‚Äù: A survey of recent\napproaches in dialogue state tracking. InProceedings\nof the 23rd Annual Meeting of the Special Interest\nGroup on Discourse and Dialogue, pages 336‚Äì350.\nPeter Jansen. 2022. A systematic survey of text worlds\nas embodied natural language environments. In The\nThird Wordplay: When Language Meets Games Work-\nshop.\nSeyed Mehran Kazemi, Najoung Kim, Deepti Bhatia,\nXin Xu, and Deepak Ramachandran. 2022. Lam-\nbada: Backward chaining for automated reasoning in\nnatural language. arXiv preprint arXiv:2212.13894.\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel.\n2019. Revisiting the evaluation of theory of mind\nthrough question answering. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5872‚Äì5877.\nAlan M Leslie, Ori Friedman, and Tim P German. 2004.\nCore mechanisms in ‚Äòtheory of mind‚Äô. Trends in\ncognitive sciences, 8(12):528‚Äì533.\nPaula Leverage, Howard Mancing, and Richard Schwe-\nickert. 2010. Theory of mind and literature. Purdue\nUniversity Press.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022. W ANLI: Worker and ai collabora-\ntion for natural language inference dataset creation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 6826‚Äì6847, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nAida Nematzadeh, Kaylee Burns, Erin Grant, Alison\nGopnik, and Tom Griffiths. 2018. Evaluating theory\nof mind in question answering. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2392‚Äì2400, Brussels,\nBelgium. Association for Computational Linguistics.\nMaxwell Nye, Michael Tessler, Josh Tenenbaum, and\nBrenden M Lake. 2021. Improving coherence and\nconsistency in neural sequence models with dual-\nsystem, neuro-symbolic reasoning. Advances in\nNeural Information Processing Systems, 34:25192‚Äì\n25204.\n13970\nOpenAI. 2022. ChatGPT: Optimizing language models\nfor dialogue.\nOpenAI. 2023. GPT-4 technical report.\nChristopher Osterhaus, Susanne Koerber, and Beate\nSodian. 2016. Scaling of advanced theory-of-mind\ntasks. Child development, 87(6):1971‚Äì1991.\nDavid Premack and Guy Woodruff. 1978. Does the\nchimpanzee have a theory of mind? Behavioral and\nbrain sciences, 1(4):515‚Äì526.\nLiang Qiu, Yizhou Zhao, Yuan Liang, Pan Lu, Weiyan\nShi, Zhou Yu, and Song-Chun Zhu. 2022. Towards\nsocially intelligent agents with mental state transition\nand human value. In Proceedings of the 23rd Annual\nMeeting of the Special Interest Group on Discourse\nand Dialogue, pages 146‚Äì158, Edinburgh, UK. As-\nsociation for Computational Linguistics.\nNeil Rabinowitz, Frank Perbet, Francis Song, Chiyuan\nZhang, SM Ali Eslami, and Matthew Botvinick. 2018.\nMachine theory of mind. In International conference\non machine learning, pages 4218‚Äì4227. PMLR.\nMaarten Sap, Ronan LeBras, Daniel Fried, and Yejin\nChoi. 2022. Neural theory-of-mind? on the limits of\nsocial intelligence in large lms. In Proceedings of the\nAssociation for Computational Linguistics: EMNLP\n2022, page 3762‚Äì3780, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nMelanie Sclar, Graham Neubig, and Yonatan Bisk. 2022.\nSymmetric machine theory of mind. In Proceedings\nof the 39th International Conference on Machine\nLearning, volume 162 of Proceedings of Machine\nLearning Research, pages 19450‚Äì19466. PMLR.\nSimone G Shamay-Tsoory, Hagai Harari, Judith Aharon-\nPeretz, and Yechiel Levkovitz. 2010. The role of\nthe orbitofrontal cortex in affective theory of mind\ndeficits in criminal offenders with psychopathic ten-\ndencies. Cortex, 46(5):668‚Äì677.\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi,\nXuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten\nSap, and Vered Shwartz. 2023. Clever hans or neural\ntheory of mind? stress testing social reasoning in\nlarge language models.\nGabriel Stanovsky, Julian Michael, Luke Zettlemoyer,\nand Ido Dagan. 2018. Supervised open information\nextraction. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 885‚Äì\n895.\nOyvind Tafjord and Peter Clark. 2021. General-purpose\nquestion-answering with macaw. arXiv preprint\narXiv:2109.02593.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 3621‚Äì3634, Online.\nAssociation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nTomer Ullman. 2023. Large language models fail on\ntrivial alterations to theory-of-mind tasks. arXiv\npreprint arXiv:2302.08399.\nAnnalisa Valle, Davide Massaro, Ilaria Castelli, and\nAntonella Marchetti. 2015. Theory of mind devel-\nopment in adolescence and early adulthood: The\ngrowing complexity of recursive thinking ability. Eu-\nrope‚Äôs journal of psychology, 11(1):112.\nMax J van Duijn, Ineke Sluiter, and Arie Verhagen.\n2015. When narrative takes over: The representa-\ntion of embedded mindstates in shakespeare‚Äôs othello.\nLanguage and Literature, 24(2):148‚Äì166.\nQiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner,\nand Ashok Goel. 2021. Towards mutual theory of\nmind in human-ai interaction: How language reflects\nwhat students perceive about a virtual teaching assis-\ntant. In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems, pages 1‚Äì14.\nYuanfei Wang, fangwei zhong, Jing Xu, and Yizhou\nWang. 2022. Tom2c: Target-oriented multi-agent\ncommunication and cooperation with theory of mind.\nIn International Conference on Learning Representa-\ntions.\nHenry M Wellman. 2014. Making minds: How theory\nof mind develops. Oxford University Press.\nHeinz Wimmer and Josef Perner. 1983. Beliefs about\nbeliefs: Representation and constraining function of\nwrong beliefs in young children‚Äôs understanding of\ndeception. Cognition, 13(1):103‚Äì128.\nPing Yu, Tianlu Wang, Olga Golovneva, Badr\nAlkhamissy, Gargi Ghosh, Mona Diab, and Asli Ce-\nlikyilmaz. 2022. Alert: Adapting language models to\nreasoning tasks. arXiv preprint arXiv:2212.08286.\nPei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, Xiang\nRen, Chris Callison-Burch, Yejin Choi, and Prithvi-\nraj Ammanabrolu. 2022. An ai dungeon master‚Äôs\nguide: Learning to converse and guide with intents\nand theory-of-mind in dungeons and dragons. arXiv\npreprint arXiv:2212.10060.\nHao Zhu, Graham Neubig, and Yonatan Bisk. 2021.\nFew-shot language coordination by modeling theory\nof mind. In International Conference on Machine\nLearning, pages 12901‚Äì12911. PMLR.\nLisa Zunshine. 2006. Why we read fiction: Theory of\nmind and the novel. Ohio State University Press.\n13971\nA Additional Details on S YMBOLIC TOM\nA.1 Detailed Description of Information\nContained in Global Context G\nIn the main paper, we define G as a graph contain-\ning the true world state (as opposed to beliefs about\nthe current world state). This means thatG will rep-\nresent where people and objects are truly located,\nregardless of beliefs. G will in general contain only\nthe observable true world state. Thus, information\npassed verbally would not be stored in the global\ncontext (e.g. someone speaking in a room is not\nobservable after they finished talking), and would\ninstead be stored in the local contexts of the people\nthat heard the speech. Since verbal interactions are\nnot tested by available datasets, this distinction is\nnot relevant in ToMi.\nA.2 Prompts for Resulting State Extraction\nFor GPT3-Curie we 2-shot prompt with the\nfollowing prompt (both for original and linguistic\ndiversity experiments):\nJohn quit his job. The resulting state\nafter this action is that John no longer\nhas a job.\\n\\nJohn signed a contract. The\nresulting state after this action is that\nthe contract is signed.\\n\\n <sentence>.\nThe resulting state after this action is\nthat\nWe find that GPT3-Davinci, Flan-T5-XL,\nGPT3.5, and GPT4 are able to zero-shot answer to\nthis subtask just by describing the instruction, but\nsmaller models benefit from few-shot. We were\nunable to query Macaw for this task, so we instead\nrely on GPT3-Curie, a model of comparable size.\nZero-shot instruction is as follows:\n<sentence>. What is the resulting\nstate after this action? Do not add new\ninformation. The resulting state after\nthis action is that\nWe observe that GPT3 is significantly more ro-\nbust to paraphrases than Flan-T5: Flan-T5 models\nare poor at detecting the resulting state for florid\nparaphrases, although the original phrasings are a\nstraightforward task for Flan-T5.\nLarger models like GPT3.5 and GPT4 are able to\nperform the task well zero-shot, similarly to GPT3;\nLLaMA models require fewer demonstrations than\nFlan-T5. We ran all main experiments implement-\ning Resulting State Extraction with GPT3.\nA.3 Solving PROCESS QUESTION using GPT3\nOur explorations suggest that GPT3 (Curie and\nGPT3-Davinci text-davinci-002‚Äîthe version\nused in all our experiments) can successfully ex-\ntract entities and rephrase the question. See Figure\n6 for an example prompt.\nFigure 6: GPT3 shows one-shot generalization abilities\nfrom first-order to second-order questions.\nB Details on Out-Of-Domain Evaluation\nB.1 Linguistic Diversity Per ToMi Template\nSentence type Count\nObject‚Äôs Position 38\nDistractor Negative Sentiment 36\nDistractor Positive Sentiment 31\nPerson Entered Room 21\nPerson Exited Room 19\nPerson Moved Object 18\nPerson‚Äôs Position 9\nTable 3: Number of paraphrases per original sentence\ntemplate. Paraphrases were obtained from prompting\nGPT3-Davinci (text-davinci-002).\nB.1.1 All Paraphrases of PersonX entered\nthe RoomY.\nPersonX entered the RoomY.\nPersonX approached the RoomY.\nPersonX arrived at the RoomY.\nPersonX arrived in the RoomY.\nPersonX bounded into the RoomY.\nPersonX came by the RoomY.\nPersonX came into the RoomY.\nPersonX came to the RoomY.\nPersonX crept into the RoomY.\nPersonX entered the RoomY.\nPersonX leapt into the RoomY.\nPersonX showed up at the RoomY.\nPersonX shuffled into the RoomY.\nPersonX sidled into the RoomY.\nPersonX slithered into the RoomY.\n13972\nPersonX stepped into the RoomY.\nPersonX tiptoed into the RoomY.\nPersonX visited the RoomY.\nPersonX walked into the RoomY.\nPersonX went into the RoomY.\nPersonX went to the RoomY.\nB.1.2 All Paraphrases of PersonX exited\nthe RoomY.\nPrompted with the prompt: Find 30 alternative\nways of expressing the following sentence: Abigail\nexited the bedroom. and manually filtering results\n(with this and other name/location selection.\nPersonX exited the RoomY.\nPersonX left the RoomY.\nPersonX walked out of the RoomY.\nPersonX stepped out of the RoomY.\nPersonX departed the RoomY.\nPersonX went out of the RoomY.\nPersonX came out of the RoomY.\nPersonX emerged from the RoomY.\nPersonX quit the RoomY.\nPersonX took off from the RoomY.\nPersonX bolted from the RoomY.\nPersonX flew from the RoomY.\nPersonX ran from the RoomY.\nPersonX sprinted from the RoomY.\nPersonX jogged from the RoomY.\nPersonX hurried from the RoomY.\nPersonX crawled from the RoomY.\nPersonX crept from the RoomY.\nPersonX tiptoed from the RoomY.\nB.1.3 All Paraphrases of The Object1 is in\nthe Container1.\nPrompted with Object1=apple,\nContainer1={fridge, envelope, bathtub} .\nThen filtered to remove object-specific wording.\nThe Object1 is in the Container1.\nThe Object1 is stored in the Container1.\nThe Object1 is kept in the Container1.\nThe Object1 is located in the Container1.\nThe Object1 is situated in the Container1.\nThe Object1 is set in the Container1.\nThe Object1 is placed in the Container1.\nThe Object1 is found in the Container1.\nThe Object1 is positioned in the\nContainer1.\nThe Object1 is set upon in the Container1.\nThe Object1 is put in the Container1.\nThe Object1 is laid in the Container1.\nThe Object1 is deposited in the\nContainer1.\nThe Object1 is stationed in the\nContainer1.\nThe Object1 is put to rest in the\nContainer1.\nThe Object1 is set to rest in the\nContainer1.\nThe Object1 is rested in the Container1.\nThe Object1 is set aside in the\nContainer1.\nThe Object1 is stowed in the Container1.\nThe Container1 contains the Object1.\nThe Object1 is inside the Container1.\nThe Object1 is within the Container1.\nThe Container1 is where the Object1 is.\nThe Container1 has the Object1.\nThe Container1 is holding the Object1.\nThe Container1 is keeping the Object1.\nThe Container1 is safeguarding the\nObject1.\nThe Container1 is storing the Object1.\nThe Container1 has the Object1 within it.\nThe Container1 has the Object1 inside of\nit.\nThe Container1 is holding the Object1\nwithin it.\nThe Container1 is keeping the Object1\ninside of it.\nThe Container1 is safeguarding the\nObject1 inside of it.\nThe Container1 is storing the Object1\ninside of it.\nThere is a Object1 in the Container1.\nA Object1 is in the Container1.\nThe Container1 has a Object1 in it.\nInside the Container1 is a Object1.\nB.1.4 All Paraphrases of PersonX moved the\nObject1 to the Container1.\nPersonX moved the Object1 to the\nContainer1.\nPersonX relocated the Object1 to the\nContainer1.\nPersonX transferred the Object1 to the\nContainer1.\nPersonX shifted the Object1 to the\nContainer1.\nPersonX placed the Object1 in the\nContainer1.\nPersonX set the Object1 in the Container1.\nPersonX put the Object1 in the Container1.\n13973\nPersonX stowed the Object1 in the\nContainer1.\nPersonX stored the Object1 in the\nContainer1.\nPersonX hid the Object1 in the Container1.\nPersonX shoved the Object1 into the\nContainer1.\nPersonX pushed the Object1 to the\nContainer1.\nPersonX carried the Object1 to the\nContainer1.\nPersonX conveyed the Object1 to the\nContainer1.\nPersonX led the Object1 to the Container1.\nPersonX transported the Object1 to the\nContainer1.\nPersonX brought the Object1 to the\nContainer1.\nPersonX took the Object1 to the\nContainer1.\nB.1.5 All Paraphrases of PersonX is in the\nRoomY.\nPersonX is in the RoomY.\nPersonX is inside the RoomY.\nPersonX is located in the RoomY.\nPersonX is situated in the RoomY.\nPersonX is present in the RoomY.\nPersonX is to be found in the RoomY.\nPersonX is contained in the RoomY.\nThe RoomY holds PersonX.\nThe RoomY shelters PersonX.\nB.1.6 All Paraphrases of positive distractor\nsentences\nPersonX has a bad case of Object1 fever.\nPersonX is Object1 crazy.\nPersonX is Object1-crazed.\nPersonX is Object1-obsessed.\nPersonX is a Object1 fiend.\nPersonX is a Object1 maniac.\nPersonX is a Object1-aholic.\nPersonX is always thirsty for a Object1.\nPersonX is besotted with the Object1.\nPersonX is captivated by the Object1.\nPersonX is charmed by the Object1.\nPersonX is crazy about the Object1.\nPersonX is crazy for the Object1.\nPersonX is eager for the Object1.\nPersonX is enamored with the Object1.\nPersonX is enthusiastic about the\nObject1.\nPersonX is entranced by the Object1.\nPersonX is fascinated by the Object1.\nPersonX is fond of the Object1.\nPersonX is in love with the Object1.\nPersonX is infatuated with the Object1.\nPersonX is keen on the Object1.\nPersonX is mad about the Object1.\nPersonX is never seen without a Object1.\nPersonX is nuts about the Object1.\nPersonX is smitten with the Object1.\nPersonX is spellbound by the Object1.\nPersonX is taken with the Object1.\nPersonX is wild about the Object1.\nPersonX loves to drink from a Object1.\nPersonX would do anything for a Object1.\nB.1.7 All Paraphrases of positive negative\nsentences (PersonX hates ObjectY)\nPersonX hates Object1.\nPersonX can‚Äôt stand the Object1.\nPersonX despises the Object1.\nPersonX detests the Object1.\nPersonX is annoyed by the Object1.\nPersonX is bothered by the Object1.\nPersonX is concerned by the Object1.\nPersonX is disconcerted by the Object1.\nPersonX is discouraged by the Object1.\nPersonX is disgusted by the Object1.\nPersonX is disheartened by the Object1.\nPersonX is disquieted by the Object1.\nPersonX is grieved by the Object1.\nPersonX is horrified by the Object1.\nPersonX is irritated by the Object1.\nPersonX is offended by the Object1.\nPersonX is pained by the Object1.\nPersonX is repelled by the Object1.\nPersonX is revolted by the Object1.\nPersonX is scandalized by the Object1.\nPersonX is shocked by the Object1.\nPersonX is sorrowful by the Object1.\nPersonX is terrified by the Object1.\nPersonX is troubled by the Object1.\nPersonX is vexed by the Object1.\nPersonX loathes the Object1.\nThe Object1 horrifies PersonX.\nThe Object1 is abhorrent to PersonX.\nThe Object1 nauseates PersonX.\nThe Object1 offends PersonX.\nThe Object1 repulses PersonX.\nThe Object1 revolts PersonX.\nThe Object1 scandalizes PersonX.\nThe Object1 shocks PersonX.\n13974\nThe Object1 sickens PersonX.\nThe Object1 terrifies PersonX.\nThe Object1 turns PersonX‚Äôs stomach.\nB.2 Structure of Story Structure Robustness\nTest Sets\nB.2.1 Double Room False-Belief Episode\nperson1 entered the room1.\nperson2 entered the room1.\nThe object1 is in the container1.\nThe container1 is in the room1.\nperson2 exited the room1.\nperson1 moved the object1 to the\ncontainer2.\nThe container2 is in the room1.\nperson1 exited the room1.\nperson2 entered the room2.\nperson1 entered the room2.\nThe object2 is in the container3.\nThe container3 is in the room2.\nperson1 exited the room2.\nperson2 moved the object2 to the\ncontainer4.\nThe container4 is in the room2.\nperson2 exited the room2.\nB.2.2 Three Active Characters Story\nperson1 entered the room1.\nperson2 entered the room1.\nperson3 entered the room1.\nThe object1 is in the container1.\nThe container1 is in the room1.\nperson2 exited the room1.\nperson1 moved the object1 to the\ncontainer2.\nThe container2 is in the room1.\nperson1 exited the room1.\nperson3 moved the object1 to the\ncontainer3.\nThe container3 is in the room1.\nperson3 exited the room1.\nB.2.3 True-Belief Interaction, Falsified by\nUnwitnessed Third-Person Story\nperson1 entered the room1.\nperson2 entered the room1.\nThe object1 is in the container1.\nThe container1 is in the room1.\nperson1 moved the object1 to the\ncontainer2.\nThe container2 is in the room1.\nperson2 exited the room1.\nperson1 exited the room1.\nperson3 entered the room1.\nperson3 moved the object1 to the\ncontainer1.\nB.2.4 Four Containers with Multiple\nMovements\nperson1 is in the room1.\nThe object1 is in the container1.\nThe container1 is in the room1.\nperson1 moved the object1 to the\ncontainer2.\nThe container2 is in the room1.\nperson2 entered the room1.\nperson1 exited the room1.\nperson2 moved the object1 to the\ncontainer3.\nThe container3 is in the room1.\nperson2 moved the object1 to the\ncontainer4.\nThe container4 is in the room1.\nC Expanded Results\nExperimental Note: All zero-shot GPT3\n(text-curie-001 and text-davinci-002)\nexperiments were performed between November\n2022 and January 2023. GPT3.5 (gpt-3.5-turbo)\nand GPT4 (gpt-4) were added in May 2023.\nC.1 Ablating FILTER BASED ONQUESTION\nfrom SYMBOLIC TOM\nFILTER BASED ONQUESTION definition This\nfunction filters the story S‚Ä≤ to obtain an even\nshorter subset of the original story S‚Ä≤‚Ä≤ by only\nkeeping edges where at least one of the endpoints\nrepresents an entity mentioned in the question.\nLast step of Algorithm 1 is applying FIL-\nTER BASED ONQUESTION , which yields an even\nshorter story to feed language models. We evaluate\nthe effect this final filter has on the final perfor-\nmances reported by SYMBOLIC TOM.\nFILTER BASED ONQUESTION has a positive ef-\nfect on Macaw-3B, GPT3, Flan-T5-XXL, and\nLLaMA-7B (+7, +3.5, +12.8, and +15 points in\naverage accuracy gain across all question types),\nand a mild negative one on Flan-T5-XL, and GPT4\n(-5.3, and -4 points of accuracy on average). See\nTable 7 for all differences between executing SYM-\nBOLIC TOM using this final filtering or not. Figure\n7 visually represents the accuracy of all models by\n13975\n0.0\n0.5\n1.0\n1st order ToM True Belief 1st order ToM False Belief\n0.0\n0.5\n1.0\n2nd order ToM True Belief 2nd order ToM False Belief\n0.0 0.5 1.0\n0.0\n0.5\n1.0\nReality\n0.0 0.5 1.0\nMemory\nBase model accuracy\nBase model + SymbolicToM accuracy\nFigure 7: Precision using SYMBOLIC TOM on ToMi, for\nseveral language models without the final filter function.\nPerformance is shown for each question type, dots in\nupper triangle imply performance improvements. Full\nresults table may be found in Table 6.\nquestion type. Regardless of the final filter applica-\ntion, GPT4+SYMBOLIC TOM significantly outper-\nforms out-of-the-box GPT4 in all four ToM ques-\ntion types and maintains performance on Reality\nand Memory questions. For Flan-T5-XL, Flan-\nT5-XL+SYMBOLIC TOM outperforms Flan-T5-XL\nsignificantly in all four ToM question types (e.g.\n+76 and +36 points in accuracy for first and second-\norder false belief questions), and shows slight de-\nclines for Reality and Memory questions‚Äîin line\nwith findings on the full algorithm, but with less\nstark declines, suggesting that having more entities\nmay help reduce bias towards answering rooms in-\nstead of containers. See Table 6 for the full table\nof accuracy differences.\nRegardless of the final filtering application,\nSYMBOLIC TOM shows improvements in theory of\nmind questions for all models. We only find the fil-\nter application to be relevant to beat the base model\nin theory of mind questions for Flan-T5-XXL.\nC.2 Third-Order Theory of Mind Evaluation\nWe ask all third-order theory of mind questions for\neach D2 story, such as ‚ÄúWhere does p1 think that\np2 thinks that p1 will search for the o1?‚Äù. Ques-\ntions involving p2 will have a final answerc1, since\neveryone saw p2 leaving. We ask all six possible\nD2‚ÄôS THIRD -ORDER\nTOM QUESTIONS\nOff-the-shelf models\nMacaw-3B 13\nFlan-T5-XL 32\nFlan-T5-XXL 62\nGPT3-Curie 28\nGPT3-Davinci 19\nGPT3.5 8\nGPT4 26\nLLaMA-7B 22\nLLaMA-13B 39\nSYMBOLIC TOM + Off-the-shelf models\nMacaw-3B 85 (+72)\nFlan-T5-XL 97 (+65)\nFlan-T5-XXL 100 (+38)\nGPT3-Curie 89 (+61)\nGPT3-Davinci 90 (+71)\nGPT3.5 100 (+91)\nGPT4 100 (+73)\nLLaMA-7B 90 (+68)\nLLaMA-13B 95 (+57)\nSupervised models\nTTT 52\nFinetuned GPT3 76\nTable 4: Precision using SYMBOLIC TOM on all ques-\ntions from 100 stories for each of the modified test sets\nDi. Supervised models were trained on ToMi; all others\ndo not require training. Parenthesis reflect differences\nbetween using and not using SYMBOLIC TOM: bold re-\nflects higher overall performance, and green reflects the\nhighest net improvements when usingSYMBOLIC TOM.\nquestions involving p2. We also ask the two third-\norder theory of mind questions that do not involve\np2 nor repeats the same person twice consecutively\n(‚ÄúWhere does p1 think that p3 thinks that p1 will\nsearch for the o1?‚Äù and ‚ÄúWhere does p3 think that\np1 thinks that p3 will search for the o1?‚Äù), totaling\neight questions per D2 story.\nTable 4 shows results for all models using k = 2\nrepresentations (same depth as in the main paper).\nUsing SYMBOLIC TOM significantly outperforms\nthe supervised baselines and yields dramatic im-\nprovements with respect to using the LLMs off-\nthe-shelf. We hypothesize that although the task\ntheoretically requires k = 3, the second-order the-\nory of mind representation already helps models\navoid attending to parts of the story that are inac-\ncessible to relevant characters.\n13976\nC.3 Alternative RESULTING STATE\nImplementations\n0.0\n0.5\n1.0\n1st order ToM True Belief 1st order ToM False Belief\n0.0\n0.5\n1.0\n2nd order ToM True Belief 2nd order ToM False Belief\n0.0 0.5 1.0\n0.0\n0.5\n1.0\nReality\n0.0 0.5 1.0\nMemory\nBase model accuracy\nBase model + SymbolicToM accuracy\nFigure 8: Results for ParaphrasedToMi when using the\nsame model for implementing the RESULTING STATE\nfunction as in the final question-answering task (except\nusing Davinci for Macaw, who did not show reliable\nenough few shot-prompting). Dots in upper triangle\nimply performance with SYMBOLIC TOM is higher than\nusing the base model out-of-the-box. Horizontal lines re-\nflect supervised models‚Äô performance (higher is better).\nRESULTING STATE(s) refers to the state of the\nworld after s has been performed. For example,\nif ‚ÄúOliver moved the apple to the box‚Äù, then the\nresulting state is that ‚ÄúThe apple is in the box‚Äù.\nIf ‚ÄúOliver exited the bedroom‚Äù, the resulting\nstate would be that ‚ÄúOliver is no longer in the\nbedroom‚Äù. These are the relationships that\nwe may insert in a context graph‚Äîactions are\ninstantaneous and do not reflect an observable state.\nIn this section, we explore using the same LLM\nfor implementing RESULTING STATE as well as the\nfinal inference. In the main text, we use Davinci\nfor all non-GPT3-based models.\nWe find GPT3 to be among the most reliable\nto answer the resulting state of a given action in\na zero-shot (Davinci) or two-shot (Curie) manner.\nSimilarly, GPT3.5 and GPT4 perform well zero-\nshot: for experiments, we use GPT3.5 zero-shot\nand GPT4 two-shot to improve the resulting phras-\ning stability.\nAdditional exploration shows that although Flan-\nT5 models perform worse zero-shot than GPT mod-\nels, they are capable of performing this task with\nmore careful prompting. Figure 8 shows the results\nafter nine-shot prompting Flan-T5-XL and eleven-\nshot prompting Flan-T5-XXL. Our explorations\nshow that LLaMA models require fewer demon-\nstrations than the Flan-T5 models to compute the\nresulting state: we observe highly reliable results\nwhen using six-shot prompting for LLaMA-7B,\nand seven-shot prompting for LLaMA-13B. Ac-\ncuracy using LLaMA was even higher than when\nusing GPT3.\nC.4 Detailed Result Tables\nAll results in the appendix show accuracy as a ratio\n(between 0 and 1). For simplicity of reading, in\nthe main text, they are referred to in percentages\n(values 0 to 100, higher is better). Figures 5, 6, and\n7 show performances when applying the final filter-\ning function, when not applying it, and the differ-\nence in performance between the two, respectively.\n13977\n1st TB 1st FB 2nd TB 2nd FB Reality Memory\nMacaw-3B 0.86 [0.50] 0.79 [0.33] 0.86 [0.34] 0.84 [0.17] 0.10 [0.14] 0.95 [0.91]\nGPT3-Curie 0.77 [0.42] 0.82 [0.35] 0.73 [0.26] 0.89 [0.26] 0.61 [0.69] 0.99 [0.86]\nGPT3-Davinci 0.96 [0.75] 0.96 [0.25] 0.93 [0.14] 0.90 [0.26] 0.77 [0.86] 0.98 [0.98]\nFlan-T5-XL 0.98 [0.97] 0.80 [0.18] 0.98 [0.68] 0.78 [0.56] 0.73 [0.97] 1.00 [1.00]\nFlan-T5-XXL 0.98 [0.84] 0.95 [0.67] 1.00 [0.76] 0.90 [0.39] 0.13 [0.63] 1.00 [1.00]\nLLaMA-7B 0.82 [0.32] 0.95 [0.66] 0.66 [0.31] 0.72 [0.41] 0.87 [0.37] 1.00 [0.83]\nLLaMA-13B 0.82 [0.60] 0.86 [0.67] 0.70 [0.53] 0.62 [0.77] 0.87 [0.48] 1.00 [0.90]\nGPT3.5 0.97 [0.76] 0.95 [0.66] 0.99 [0.02] 0.87 [0.09] 0.98 [1.00] 0.99 [0.80]\nGPT4 0.98 [0.83] 0.94 [0.73] 0.98 [0.36] 0.89 [0.64] 0.94 [1.00] 1.00 [1.00]\nFinetuned GPT3 0.95 0.99 0.97 1.00 1.00 1.00\nTTT-learned 0.84 1.00 0.82 0.88 1.00 1.00\nTable 5: Performance per model and question using SYMBOLIC TOM, with out-of-the-box performance shown in\nbrackets (100 samples per question type). Bottom rows represent supervised baselines.\n1st TB 1st FB 2nd TB 2nd FB Reality Memory\nMacaw-3B 0.54 [0.50] 0.86 [0.33] 0.56 [0.34] 0.88 [0.17] 0.16 [0.14] 0.98 [0.91]\nGPT3-Curie 0.66 [0.42] 0.79 [0.35] 0.69 [0.26] 0.87 [0.26] 0.65 [0.69] 0.94 [0.86]\nGPT3-Davinci 0.94 [0.75] 0.88 [0.25] 0.90 [0.14] 0.83 [0.26] 0.83 [0.86] 0.90 [0.98]\nFlan-T5-XL 1.00 [0.97] 0.94 [0.18] 1.00 [0.68] 0.92 [0.56] 0.88 [0.97] 0.85 [1.00]\nFlan-T5-XXL 0.74 [0.84] 0.69 [0.67] 0.68 [0.76] 0.64 [0.39] 0.44 [0.63] 1.00 [1.00]\nLLaMA-7B 0.48 [0.32] 0.95 [0.66] 0.38 [0.31] 0.98 [0.41] 0.48 [0.37] 0.84 [0.83]\nLLaMA-13B 0.75 [0.60] 0.96 [0.67] 0.70 [0.53] 0.96 [0.77] 0.57 [0.48] 0.89 [0.90]\nGPT3.5 0.99 [0.76] 1.00 [0.66] 1.00 [0.02] 0.98 [0.09] 0.98 [1.00] 0.90 [0.80]\nGPT4 0.99 [0.83] 1.00 [0.73] 1.00 [0.36] 0.98 [0.64] 1.00 [1.00] 1.00 [1.00]\nFinetuned GPT3 0.95 0.99 0.97 1.00 1.00 1.00\nTTT-learned 0.84 1.00 0.82 0.88 1.00 1.00\nTable 6: Performance per model and question using SYMBOLIC TOM without FILTER BASED ONQUESTION , with\nout-of-the-box performance shown in brackets (100 samples per question type). Bottom rows represent supervised\nbaselines.\n1st TB 1st FB 2nd TB 2nd FB Reality Memory\nMacaw-3B 0.32 -0.07 0.30 -0.04 -0.06 -0.03\nGPT3-Curie 0.11 0.03 0.04 0.02 -0.04 0.05\nGPT3-Davinci 0.02 0.08 0.03 0.07 -0.06 0.08\nFlan-T5-XL -0.02 -0.14 -0.02 -0.14 -0.15 0.15\nFlan-T5-XXL 0.24 0.26 0.32 0.26 -0.31 0.00\nLLaMA-7B 0.34 0.00 0.28 -0.26 0.39 0.16\nLLaMA-13B 0.07 -0.10 0.00 -0.34 0.30 0.11\nGPT3.5 -0.02 -0.05 -0.01 -0.11 0.00 0.09\nGPT4 -0.01 -0.06 -0.02 -0.09 -0.06 0.00\nTable 7: Differences between accuracy of base models usingSYMBOLIC TOM with the final FILTER BASED ONQUES -\nTION filter, and without using the final filter. As shown in Table 5 and 6, both versions are still far superior to not\nusing SYMBOLIC TOM.\n13978\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nSection \"Limitations\" after Conclusions but before the references, as required by ACL 2023 guide-\nlines.\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\nSection \"Ethics Statement\" after Conclusions but before the references, as required by ACL 2023\nguidelines.\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nAbstract + Section 1\n‚ñ°\u0013 A4. Have you used AI writing assistants when working on this paper?\nGPT3-Davinci, for brainstorming paraphrases of sentences in Section 1 and Section 2. We later\nedited these paraphrases, but GPT3-Davinci gave interesting suggestions.\nB ‚ñ°\u0017 Did you use or create scientiÔ¨Åc artifacts?\nLeft blank.\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nSection 1, Section 4, Section 5, Abstract.\n‚ñ°\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nArtifact is an NLP research dataset.\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 4\n‚ñ°\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThe dataset is artiÔ¨Åcially generated.\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nSection 5\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n13979\nC ‚ñ°\u0013 Did you run computational experiments?\nSection 5\n‚ñ°\u0017 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nModel does not require training, it is inference-time only.\n‚ñ°\u0017 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nModel does not require training, it is inference-time only.\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 5\nD ‚ñ°\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 4\n‚ñ°\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nAnnotators are only used to contribute to a small comment and are not used in evaluating our method.\n‚ñ°\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nLeft blank.\n‚ñ°\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n‚ñ°\u0013 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n‚ñ°\u0017 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot necessary for the small-scale experiment ran.\n13980",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.729763388633728
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5757997632026672
    },
    {
      "name": "Overfitting",
      "score": 0.5730016231536865
    },
    {
      "name": "Theory of mind",
      "score": 0.5726441144943237
    },
    {
      "name": "Language model",
      "score": 0.5014796257019043
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.44539809226989746
    },
    {
      "name": "Machine learning",
      "score": 0.4016451835632324
    },
    {
      "name": "Artificial neural network",
      "score": 0.3568606972694397
    },
    {
      "name": "Cognition",
      "score": 0.22061094641685486
    },
    {
      "name": "Psychology",
      "score": 0.15581387281417847
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}