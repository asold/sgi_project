{
    "title": "Authorship Attribution Using a Neural Network Language Model",
    "url": "https://openalex.org/W2963514251",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4302742399",
            "name": "Ge, Zhenhao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2147094482",
            "name": "Sun Yu-fang",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Smith, Mark J. T.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1602390003",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2108557656",
        "https://openalex.org/W179875071"
    ],
    "abstract": "In practice, training language models for individual authors is often expensive because of limited data resources. In such cases, Neural Network Language Models (NNLMs), generally outperform the traditional non-parametric N-gram models. Here we investigate the performance of a feed-forward NNLM on an authorship attribution problem, with moderate author set size and relatively limited data. We also consider how the text topics impact performance. Compared with a well-constructed N-gram baseline method with Kneser-Ney smoothing, the proposed method achieves nearly 2:5% reduction in perplexity and increases author classification accuracy by 3:43% on average, given as few as 5 test sentences. The performance is very competitive with the state of the art in terms of accuracy and demand on test data. The source code, preprocessed datasets, a detailed description of the methodology and results are available at https://github.com/zge/authorship-attribution.",
    "full_text": "Authorship Attribution Using a Neural Network Language Model\nZhenhao Ge, Yufang Sun and Mark J.T. Smith\nSchool of Electrical and Computer Engineering, Purdue University\n465 Northwestern Ave, West Lafayette, Indiana, USA, 47907-2035\nEmails: {zge, sun361, mjts}@purdue.edu, Phone: +1 (317) 457-9348\nAbstract\nIn practice, training language models for individual authors\nis often expensive because of limited data resources. In such\ncases, Neural Network Language Models (NNLMs), gener-\nally outperform the traditional non-parametric N-gram mod-\nels. Here we investigate the performance of a feed-forward\nNNLM on an authorship attribution problem, with moderate\nauthor set size and relatively limited data. We also consider\nhow the text topics impact performance. Compared with a\nwell-constructed N-gram baseline method with Kneser-Ney\nsmoothing, the proposed method achieves nearly 2.5% re-\nduction in perplexity and increases author classiﬁcation ac-\ncuracy by 3.43% on average, given as few as 5 test sen-\ntences. The performance is very competitive with the state\nof the art in terms of accuracy and demand on test data.\nThe source code, preprocessed datasets, a detailed descrip-\ntion of the methodology and results are available at https:\n//github.com/zge/authorship-attribution.\nIntroduction\nAuthorship attribution refers to identifying authors from\ngiven texts by their unique textual features. It is challenging\nsince the author’s style may vary from time to time by topics,\nmood and environment. Many methods have been explored\nto address this problem, such as Latent Dirichlet Allocation\nfor topic modeling (Seroussi, Zukerman, and Bohnert 2011)\nand Naive Bayes for text classiﬁcation (Coyotl-Morales et\nal. 2006). Regarding language modeling methods, there\nis mixed advocacy for the conventional N-gram methods\n(Keˇselj et al. 2003) and methods using more compact and\ndistributed representations, like Neural Network Language\nModels (NNLMs), which was claimed to capture semantics\nbetter with limited training data (Bengio et al. 2003).\nMost NNLM toolkits available (Mikolov et al. 2010) are\ndesigned for recurrent NNLMs which are better for captur-\ning complex and longer text patterns and require more train-\ning data. In contrast, the feed-forward NNLM framework\nwe proposed is less computationally expensive and more\nsuitable for language modeling with limited data. It is devel-\noped in MATLAB with full network tuning functionalities.\nThe database we use is composed of transcripts of 16\nvideo courses taken from Coursera, collected one sentence\nper line into a text ﬁle for each course. To reduce the inﬂu-\nence of “topic” on author/instructor classiﬁcation, courses\nwere all selected from science and engineering ﬁelds, such\nCopyright c⃝ 2015, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nas Algorithm, DSP, Data Mining, IT, Machine Learning,\nNLP, etc. There are 8000+ sentences/course and about 20\nwords/sentence on average. The vocabulary size of each\nauthor varies from 3000 to 9000. After stemming with\nPorter’s algorithm and pruning words with frequency less\nthan 1/100, 000, author vocabulary size is reduced to a range\nfrom 1800 to 2700, with average size around 2000. Fig. 1\nshows the vocabulary size for each course, under various\nconditions and the database coverage with the most frequent\nk words (k = 500, 1000, 2000) after stemming and pruning.\n0 2 4 6 8 10 12 14 16\n0\n5000\n10000\nDataset index (C)\nVocabulary size (V)\nVocabulary size for each dataset\n \n \nVoriginal Vstemmed Vstemmed−pruned\n0 2 4 6 8 10 12 14 16\n0.8\n0.85\n0.9\n0.95\n1\nDataset index (C)\nDatabase Coverage (DC)\nDatabase coverage from most frequent k words for each dataset\nstemmed & pruned datasets, k = 500, 1000, 2000\n \n \nDC 2000 DC 1000 DC 500\nFig. 1: V ocabulary size and word coverage in various stages\nNeural Network Language Model (NNLM)\nSimilar to N-gram methods, the NNLM is also used to an-\nswer one of the fundamental questions in language model-\ning: predicting the best target word W∗, given a context of\nN −1 words. The target word is typically the last word\nwithin context size N. However, it theoretically can be in\nany position. Fig. 2 demonstrates the structure of the pro-\nposed NNLM with multinomial classiﬁcation cost function:\nC = −\n∑\nV\ntj log yj, j∈V, (1)\nwhere V is the vocabulary size, yj and tj are the ﬁnal out-\nput and the target label. This NNLM setup contains 4 types\nof layers. The word layer contains N −1 input words rep-\nresented by V -dimensional index vectors with V −1 “0”s\nand one “1” positioned in a different location to differenti-\nate it from all other words. Words are then transformed to\ntheir distributed representation and concatenated in the em-\nbedding layer. Outputs from this layer forward propagate to\nthe hidden sigmoid layer, then softmax layer to predict the\nprobabilities of the possible target words. Weights/biases\nbetween layers are initiated randomly and with zeros re-\nspectively, and their error derivatives are computed through\nbackward propagation. The network is iteratively updated\nwith parameters, such as learning rate and momentum.\narXiv:1602.05292v1  [cs.CL]  17 Feb 2016\nIndex of Context \nWord௜ሻ\nIndex of Context \nWordேሻ\nIndex of Context \nWordଵሻ\nIndex of Context \nWordଶሻ ⋯⋯\nHidden Layer (sigmoid)\n୵୭୰ୢିୣ୫ୠ୵୭୰ୢିୣ୫ୠ\nୣ୫ୠି୦୧ୢ୦୧ୢୢୣ୬\nOutput Layer (Softmax)\n୦୧ୢି୭୳୲୭୳୲୮୳୲\nEmbedding Layer\nRepresentation \nof Word݅\nRepresentation \nof Wordܰ\nRepresentation \nof Word\t1\nRepresentation \nof Word\t2 ⋯⋯\nIndex of Target Wordݐ\nܫ∈݅\nFig. 2: A feed-forward NNLM setup ( I: index, W: word,\nN: number of context words, W: weight, b: bias)\nIn implementation, the processed text data for each course\nare randomly split into training, validation, and test sets with\nratio 8:1:1. This segmentation is performed 10 times with\ndifferent randomization seeds, so the mean/variance of per-\nformance of NNLMs can be measured later. We optimized\na 4-gram NNLM (predicting the 4th word using the previous\n3) with mini-batch training through10 to 20 epochs for each\ncourse. The model parameters, such as number of nodes in\neach layer, learning rate, and momentum are customized for\nobtaining the best individual models.\nClassiﬁcation with Perplexity Measurement\nDenote Wn\n1 as a word sequence (W1, W2, . . . ,WN) and\nP(Wn\n1 ) as the probability of Wn\n1 given a LM, perplexity\nis an intrinsic measurement of the LM ﬁtness deﬁned by:\nPP (Wn\n1 ) =P(Wn\n1 )− 1\nn (2)\nUsing Markov chain theory, P(Wn\n1 ) can be approximated\nby the probability of the closest N words P(Wn\nn−N+1), so\nPP (Wn\n1 ) can be approximated by\nPP (Wn\nn−N+1) = (\nn∏\nk=1\nP(Wk|Wk−1\nk−N+1))−1/n (3)\nThe mean perplexity of applying trained 4-gram NNLMs\nto their corresponding test sets are 67.3 ±2.4. This is lower\n(better) than the traditional N-gram method (69.0 ±2.4 with\n4-gram SRILM). The classiﬁcation is performed by ﬁnding\nthe author with his/her NNLM that maximizes the accumu-\nlative perplexity of the test sentences. By randomly select-\ning 1 to 20 test sentences from the test set, Fig. 3 shows\nthe 16-way classiﬁcation accuracy using 3 methods, for one\nparticular course/instructor and for all courses on average.\nThere are 2 courses taught by the same instructor, intention-\nally added for investigating the topic impact on accuracy.\nThey are excluded when computing the average accuracy\nin Fig. 3. Similarly, the accuracies for courses using two\nmethods with differing text lengths are compared in Fig. 4.\nBoth ﬁgures show the NNLM method is slightly better than\nthe SRI baselines at the 4-gram level. A classiﬁcation con-\nfusion matrix (not included due to space limits) was also\ncomputed to show the similarity between authors. The re-\nsults show higher confusion on similar courses, which indi-\ncates the topic does impact accuracy. The NNLM has higher\nconfusion values than the SRI baseline on the two different\ncourses from the same instructor, so it is more biased toward\nthe author rather than the topic in that sense.\n2 4 6 8 10 12 14 16 18 200.4\n0.6\n0.8\n1\nNo. of sentences\nAvgerage Accuracy\n1−of−16 Classfication Accuracy vs. Text Length, Course ID: 3\n \n \nunigram (SRI)\n4gram (SRI)\n4gram (NNLM)\n2 4 6 8 10 12 14 16 18 200.6\n0.7\n0.8\n0.9\n1\nNo. of sentences\nCourse Avgerage Accuracy\n1−of−16 Course Average Classfication Accuracy vs. Text Length\n \n \n4gram (SRI)\n4gram (NNLM)\nFig. 3: Individual and mean accuracies vs. text length in\nterms of the number of sentences\n0 5 10 15\n0.6\n0.8\n1\nDataset index (C)\nAccuracy\nSRI 4−gram\n \n \n10 sentences5 sentences1 sentence\n0 5 10 15\n0.6\n0.8\n1\nDataset index (C)\nAccuracy\nNNLM 4−gram\n \n \nFig. 4: Accuracies at 3 stages differed by text length for 14\ncourses (2 courses from the same instructor are excluded)\n.\nConclusion and Future Work\nThe NNLM-based work achieves promising results com-\npared with the N-gram baseline. The nearly perfect accura-\ncies given 10+ test sentences are competitive with the state-\nof-the-art, which achieved 95%+ accuracy on a similar au-\nthor size (Coyotl-Morales et al. 2006), or80%+ with tens of\nauthors and limited training data (Seroussi, Zukerman, and\nBohnert 2011). However, it may also indicate this dataset is\nnot sufﬁciently challenging, probably due to the training and\ntest data consistency and the topic distinction. In the future,\ndatasets with more authors can be used, for example, taken\nfrom collections of books or transcribed speeches. We also\nplan to integrate a nonlinear function optimization scheme\nusing the conjugate gradient (Rasmussen 2006), which auto-\nmatically selects the best training parameters and saves time\nin model customization. To compensate for the relatively\nsmall size of the training set, LMs may also be trained with\na group of authors and then adapted to the individuals.\nReferences\n[Bengio et al. 2003] Bengio, Y .; Ducharme, R.; Vincent, P.; and\nJanvin, C. 2003. A neural probabilistic language model. The\nJournal of Machine Learning Research3:1137–1155.\n[Coyotl-Morales et al. 2006] Coyotl-Morales, R. M.; Villase ˜nor-\nPineda, L.; Montes-y G´omez, M.; and Rosso, P. 2006. Authorship\nattribution using word sequences. In Progress in Pattern Recogni-\ntion, Image Analysis and Applications. Springer.\n[Keˇselj et al. 2003] Ke ˇselj, V .; Peng, F.; Cercone, N.; and Thomas,\nC. 2003. N-gram-based author proﬁles for authorship attribution.\nIn PACLING.\n[Mikolov et al. 2010] Mikolov, T.; Karaﬁ ´at, M.; Burget, L.; Cer-\nnock`y, J.; and Khudanpur, S. 2010. Recurrent neural network\nbased language model. In INTERSPEECH 2010.\n[Rasmussen 2006] Rasmussen, C. E. 2006. Gaussian processes for\nmachine learning.\n[Seroussi, Zukerman, and Bohnert 2011] Seroussi, Y .; Zukerman,\nI.; and Bohnert, F. 2011. Authorship attribution with latent dirich-\nlet allocation. In CoNLL."
}