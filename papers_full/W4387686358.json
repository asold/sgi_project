{
  "title": "Connecting AI: Merging Large Language Models and Knowledge Graph",
  "url": "https://openalex.org/W4387686358",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2117624602",
      "name": "Mlađan Jovanović",
      "affiliations": [
        "Singidunum University"
      ]
    },
    {
      "id": "https://openalex.org/A2101504016",
      "name": "Mark Campbell",
      "affiliations": [
        "Evotec (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4362579589",
    "https://openalex.org/W3003265726",
    "https://openalex.org/W3015158377",
    "https://openalex.org/W3100254008",
    "https://openalex.org/W4385571011",
    "https://openalex.org/W6852353527",
    "https://openalex.org/W6851552839",
    "https://openalex.org/W6854027915",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2883445328",
    "https://openalex.org/W4307717106",
    "https://openalex.org/W3113149630",
    "https://openalex.org/W2997292130",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W4213385996",
    "https://openalex.org/W6853341658",
    "https://openalex.org/W6854186793",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W6809646742",
    "https://openalex.org/W4381586841",
    "https://openalex.org/W4392619039",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4383605209",
    "https://openalex.org/W4390692489",
    "https://openalex.org/W4392240262",
    "https://openalex.org/W4226278401"
  ],
  "abstract": "Combining the generative abilities of large language models with the logical and factual coherence of knowledge graphs using a connected artificial intelligence architecture minimizes each system's shortcomings and amplifies their strengths across many real-world domains.",
  "full_text": "T\nhe rise of artificial intelli -\ngence (AI) models in fields \nlike image recognition and \nnatural language process -\ning has been driven by general ad -\nvancements in neural computing, \nand in particular, multilayered ar -\nchitectures such as deep learning. \nRecent developments in machine \nlearning attention mechanisms and \ngenerative models, such as trans -\nformers, have greatly accelerated AI \nadoption. Many newer AI applica -\ntions require more than just the pat -\ntern recognition offered by main -\nstream deep learning models and \nare grappling with more complex \ntasks like fact-checking, general \nreasoning, real-time learning, and \nperformant large-scale inference.\nAlthough generative AI archi -\ntecture like large language models \n(LLMs) have demonstrated remark -\nable success in handling diverse queries, they neverthe -\nless face limitations in capturing, learning, and recall -\ning factual and contextual knowledge beyond a short \nsession window. By contrast, knowledge graphs (KGs) \nare structured data models capable of explicitly learning \nDigital Object Identifier 10.1109/MC.2023.3305206\nDate of current version: 18 October 2023\nIT INNOVATION\nEDITOR MARK CAMPBELL \nEVOTEK; mark@evotek.com\nCOMPUTER  0018-9162/23©2023IEEE  PUBLISHED BY THE IEEE COMPUTER SOCIETY   NOVEMBER 2023  103\nConnecting AI: \nMerging Large \nLanguage Models \nand Knowledge \nGraph\nMlađan Jovanović , Singidunum University\nMark Campbell , EVOTEK\nCombining the generative abilities of large \nlanguage models with the logical and factual \ncoherence of knowledge graphs using a \nconnected artificial intelligence architecture \nminimizes each system’s shortcomings and \namplifies their strengths across many  \nreal-world domains.\n104 COMPUTER    WWW.COMPUTER.ORG/COMPUTER\nIT INNOVATION \nand permanently storing rich factual \nknowledge in 3D. However, construct -\ning and maintaining KGs can be chal -\nlenging, and they often struggle in in -\ncomplete or dynamic data spaces.\nThis raises the intriguing possi -\nbility of combining LLMs and KGs. \nWould an LLM and KG hybrid leverage \nthe strengths of each while addressing \ntheir weaknesses, resulting in a more \npowerful and comprehensive knowl -\nedge processing platform? How would \nthis integration take place? And what \nchallenges would this face?\nLLMS\nLLMs are commonly deployed using \nin-context learning, wherein their be -\nhavior is controlled through prompt -\ning and conditioning on relevant con -\ntextual data. 1 The typical workflow \npreprocesses the training data, de -\ncomposes relevant documents into \nchunks, passes them through an em -\nbedding model, and stores them in a \nvector database. Next, prompts and \nvalid output examples based on the \nground truth and external context \n(that is, from beyond the model’s train -\ning) are fed to the pretrained model for \ninference and fine-tuning.\nAlthough creating a library of \nfine-tuning prompts is relatively sim -\nple, crafting them to provide the most \neffective guidance is much more of a \nchallenge and requires robust moni -\ntoring mechanisms to optimize train -\ning and fine-tuning. One common ap -\nproach to assess an LLM’s robustness, \nsuch as Fiddler AI’s Auditor platform, \nuses a second monitoring LLM to mea -\nsure the sensitivity of the first LLM’s \nresponse to varying inputs. By gener -\nating variations of an original prompt \nand evaluating semantic similarities \nacross the LLM’s responses, the mon -\nitoring LLM can help speed fine-  \ntuning, increase model robustness, \nand identify performance issues such \nas data drift in text embeddings. 2\nLLM CHALLENGES\nDespite meteoric adoption across many \ndifferent domains, LLMs have signifi -\ncant shortcomings and drawbacks, in -\ncluding the following:\n › Factual accuracy : Research has \ndemonstrated that LLMs strug -\ngle to recall actual facts and are \nprone to generating inaccurate \ninformation, known as halluci-\nnations, and false assertions. 3 \nLLMs can also exhibit unex -\npected errors when confronted \nwith examples  \nthat do not conform to the \npatterns learned during train -\ning. “GPTs can produce useful \ncontent, but when it comes to \ndecisions where high accuracy \nis a critical requirement, we \ncannot rely on them,” Peter Voss, \nCEO and chief scientist at Aigo.\nai points out. 4\n › Data poisoning: In addition, \nmodels exposed to maliciously \naltered data can sometimes alter \ntheir behavior in ways imper -\nceptible to humans. Various \ntechniques have been employed \nto mitigate these errors, such as \nprompting pretrained models 5 \nor employing human-in-the-\nloop reinforcement for fine-\ntuned models. 6\n › Bias and opaqueness : LLMs have \na propensity to amplify social, \ncultural, demographic, and \nvarious other biases present \nin the training data. 7 More-\nover, explaining a model’s \ndecision-making process is \ntypically ambiguous or even \nimpossible. 8 This opaqueness \nis particularly concerning in \napplications related to medical, \nfinancial, legal, and autono -\nmous systems.\n › Large infrastructure : LLM \nworkflows depend heavily on \ncomputationally intensive \nand memory-demanding base \nmodels that do not scale lin -\nearly. The continuous growth of \nmodel parameters, scaling to -\nken vocabulary, and increasing \ntraining memory has reached a \npoint of diminishing returns. 9 \nAdditionally, the physical, tech -\nnical, and organizational chal -\nlenges of continually deploying \nmore computation and data \ninfrastructure makes the cost \nand complexity of endless scal -\ning untenable, particularly for \nmodels like ChatGPT, 10  leaving \nLLMs in the domain of only the \nlargest implementors.\n › Data access : LLM training is \nconstrained by data access, \nwhich is increasingly expensive \ndue to copyright complexities, \nprivacy concerns, regulations, \ndata fees, Web3 (where users \nstore data in personal vaults \nor crypto wallets), geopolitical \nfactors, and datasets contam -\ninated with vast amounts of \nAI-generated synthetic data. \nThis is causing a shift toward \nmodel enhancement through \nmethods not solely reliant \non more data. An illustrative \nexample of this approach is \nchain-of-thought prompting, \nin which a model is requested \nto generate steps of logical \nthinking while simultaneously \nproviding annotations to train \nsmaller models. 11\n › Real-time learning : After train -\ning and fine-tuning, LLMs \nremain quite static until another \nLLMs are commonly deployed using in-context \nlearning, wherein their behavior is controlled \nthrough prompting and conditioning on  \nrelevant contextual data.\n NOVEMBER  2023  105\ntraining cycle. As such, they \ndo not assimilate new facts or \npatterns while conducting infer -\nence and are therefore incapable \nof real-time learning.\nKGS\nA KG is a machine-readable represen -\ntation of real-world knowledge, such \nas general facts, domain-specific facts, \nand common-sense maxims, and en -\ncompasses modalities beyond text (for \nexample, images, sounds, or video). 4 \nTypically implemented using a graph \ndatabase, KGs form a network of digi -\ntal entities categorized into reference \nclasses (nodes) and the interconnected \nrelationships between them (edges). 12 \nUpon this node and edge network, KGs \nbuild an ontology of concepts, proper -\nties, relationships, and instances for \nthe given knowledge domain using \na paradigm in which terminological \nboxes depict conceptualizations, and \nterminological knowledge and asser -\ntional boxes describe instances con -\nforming to those concepts. Each entity \nclass in a KG is defined by properties \nsuch as attributes, functions, relations, \nand meta-attributes and takes the form \nof general entity types (for instance, a \nperson or event) and domain-specific \nentity types (for example, health or fi -\nnance related).\nKGs also offer various services \nranging from simple operations like \ncreate, read, update, delete to ad -\nvanced functionalities such as seman -\ntic search, matching, and navigation \nfor natural language processing tasks. \nThe combination of KG operations \nand services allows new facts to be \n in  tegrated as they are encountered, \n allowing them to learn in real time af -\nter construction.\nKG CHALLENGES\nAlthough KGs offer various benefits, \nthe also present several challenges:\n › Construction: KGs are generated \nby combining structured and un -\nstructured (noisy) data from var -\nious sources. Existing methods \nof knowledge extraction have \nlow accuracy, which can pro -\nduce inconsistent or incomplete \nKGs.13 In addition, non-English \ndatasets are still rare and multi -\nmodal datasets remain difficult \nto extract and represent.\n › Maintenance: KGs often suffer \nfrom incomplete or missing \nentities, attributes, and relations \ndue to erroneous data sources. \nImplementers resort to tedious \nhuman-in-the-loop techniques \nlike crowdsourcing and expert \nsourcing to maintain knowledge \nquality. KGs also struggle to de -\ntect and represent how domain \nand real-world knowledge evolve \nover time.14\n › Interoperability : Merging distinct \nKGs presents a significant chal -\nlenge due to the variety, dissimi -\nlarity, and intricacy of data used \nduring construction. 15 Locating \nidentical entities across KGs is a \nformidable task due to semantic \ndifferences in schemas and con -\ncepts, complicated by language \npolysemy (that is, similar enti -\nties having different meaning \nacross KGs) and entities with a \nvariety of modalities.\nCOMBINING LLMS AND KGS\nAn LLM and KG hybrid offers the pos -\nsibility of synergizing strengths and \novercoming challenges (see Figure 1 ).  \nAlthough LLMs embody patterns to \ncreate synthetic artifacts and KGs pro -\nvide structure for semantic and factual \ndata, their integration can identify en -\ntities and handle diverse descriptions \nefficiently, advancing overall capabil -\nities significantly. 16\nIn current LLM and KG integra -\ntions, each exchanges its information \nwith the other, yet each functions as \na distinct element. 16 LLMs are adept \nat discovering knowledge, while KGs \ncompile this knowledge in a reinforc -\ning feedback loop, leading to ongoing \nenhancements and expanded capabili -\nties. LLMs dynamically generate, main-\ntain, and expand KGs, while KGs offer \nrefining prompts along with pertinent \ncontext to train LLMs and validate \n responses. For instance, OntoGPT 17 \nand GraphGPT 18  utilize prompting \nto extract schema-based information \nand populate the knowledge base. 19 \nSensEmBERT generates semantic rep -\nresentations of word meanings in the \nform of vectors, akin to LLM’s contex -\ntualized word embeddings, connecting \na word’s occurrence and its meaning \nLLMs\nData\nPatterns\nPerception\nKGs\nStructure\nSemantics\nCognition\nCompilation \nKnowledge Discovery\nArtifact Synthesis\nTraining/Tuning\nReal-Time Learning\nFactual Veracity\nFIGURE 1. The LLM and KG combination.\nKGs are generated by combining structured  \nand unstructured (noisy) data from  \nvarious sources.\n106 COMPUTER    WWW.COMPUTER.ORG/COMPUTER\nIT INNOVATION \nto disambiguating word senses across \nmultiple languages. 20\nUsing automated pipelines, LLMs \ncan also be connected to sources be -\nsides KGs, such as application pro -\ngramming interfaces (APIs), docu -\nments, and tabular data, to enhance \ntheir reasoning capabilities. 21,22 How-\never, the efficacy of these integrations \ndepends on the LLM’s recognition ca -\npacity and how seamlessly each com -\nponent can function in a composition \nof services.\nTHE CONNECTED AI \nARCHITECTURE\nCognitive AI has driven the develop -\nment of architectures that mimic hu -\nman behavior and reasoning in ma -\nchines.23 Recent AI advancements have \nrenewed interest in enabling systems \nto “understand” by integrating mul -\ntiple knowledge sources. 24 However, \nbridging the gap between machine \nperception and cognition remains a \nchallenge for operational AI systems \nattempting to accurately perceive and \nreason about their surroundings.\nThe emerging area of neurosym -\nbolic computing 25 combines neural \nnetworks’ pattern-recognition capabil -\nities with KGs’ reasoning abilities using \none of the following two methods:\n1. Compression and vectorization : \nThis method compresses \nknowledge structures into \nvectorized representations \nsuitable for neural networks. \nVectorization generates mul -\ntidimensional vector repre -\nsentations for graph triples \n(knowledge embedding) 26 \nusing techniques like manifold \nlearning, topological data anal -\nysis, graph neural networks, \nand generative graph models \nto capture the structure and \nsemantics of the network. Al -\nthough compressed knowledge \nenhances the reasoning capa -\nbilities of LLMs and orchestra -\ntion pipelines, 21,22  it does lose \nsome of the original semantics \nin the produced representa -\ntions and encoded textual \nentities. 27 Nevertheless, such \nrepresentations become “reg -\nularizers” by providing more \nflexible responses through \nconstraining the neural net -\nwork search problem, and by \ncategorizing LLMs’ outputs for \nverification. This allows KGs to \ninstill rigor in LLMs behavior, \nakin to verifying a computer \nprogram.\n2. Pattern extraction : This tech -\nnique links neural patterns \nwith symbolic knowledge by \nextracting pertinent pat -\ntern information. 25 Pattern \nextraction is predominantly \nimplemented using end-to-end \npipelines that incorporate \ndifferentiable LLM and KG \ncomponents. 21,22\nNeurosymbolic computation tech -\nniques can be realized in a platform \nwe have dubbed the “connected AI ar -\nchitecture” (see Figure 2 ), which uses \nbidirectional graph-to-vector and \nvector-to-graph links between per -\nception (LLMs) and cognition (KGs) to \ntrack and interpret content exchang -\n ed between them. In this method, a \nKG encoder translates graphs into \nan intermediate format compatible \nwith corresponding transformations \nTranslation Search Dialog Recommender APIsDecision \nMaking\nServices\nKnowledge\nStructured \nDocuments\nSchemas\nModels\nKnowledge \nBases\nLLMs\n(Perception)\nPretrained \nModels\nFine-Tuned \nModels\nText-to-Text \nModels\nMultimodal \nModels\nVectors\n(t1, t2, t3, ..., tn)\nKGs\n(Cognition)\nGeneral \nKnowledge\nDomain \nKnowledge\nCommon-Sense\nKnowledge\nMultimodal \nKnowledge\nTriples\n<Entity-Relation-Entity>\n<Entity-Property-Value>\nNeurosymbolic Mapping\nIntermediate Content\n(Transformation Procedures and Structures)\nNeurosymbolic \nTransformation\nLLM\nEncoder\nLLM\nDecoder\nKnowledge\nEncoder\nKnowledge\nDecoder\nLLM Extraction\nKG Compression\nData\nText\nImage\nAudio\nVideo\nFIGURE 2. The connected AI architecture.\n NOVEMBER  2023  107\n(for instance, graph embedding or \nmasking methods), and then an LLM \ndecoder reconstructs and validates \nthese transformations, yielding vec -\ntor representations. When applied in \nan iterative fashion, this can achieve \ndesired performance levels, such as \nminimizing compression loss. Like -\nwise, LLM extraction encodes vectors, \nanalyzes acquired patterns to extract \nentities and relationships, and uses \nthese to create triple-like structures \nfor ingestion by the target graph. An \niterative feedback loop ensures that \nthe generated triples attain logical \ncoherence and consistency.\nThe connected AI architecture \nbridges the gap between machine se -\nmantics and the evolving context of the \nphysical world it mirrors. Addition -\nally, creating simpler probabilistic and \nformal components establishes causal \nrelations between digital and physical \nobjects, capturing real-world dynam -\nics. This enables discovery of how dig -\nital entities correspond to real-world \nobjects, assesses whether a digital \naction results in the desired real-  \nworld outcome, and identifies which \ndigital counterparts are affected by \nthis outcome. These real-time seman -\ntics can be accessed remotely through \nAPIs and integrated into various  \nservices like translation, search, and \ndialogue functions.\nCHALLENGES\nTo implement and deploy a success -\nful connected AI architecture, one \nmust address the following critical \nrequirements:\n › Trust: One must trust the under -\nlying LLMs and KGs to trust the \noverall architecture. Although \nthere are several techniques un -\nder development that assess the \ntrustworthiness of LLMs, 7 these \nneed to be extended to the larger \nconnected AI architecture. Be -\nyond this, it is crucial to educate \nend users on system interaction, \nintended use cases, expected \nand unexpected behaviors, and \npotential repercussions from \nexceeding them.\n › Explainability : “Language mod -\nels have a vector representation \nbased on word distance, which \nis very opaque, and there is no \neffective way to map vectors \nto nodes in a KG due to this \nopaqueness. LLMs can map to \nKGs using an intermediate rep -\nresentation they produce, but we \nmight need a supervisory level \nover both due to hallucinations \nand opaqueness of LLMs,” notes \nVoss.4 As such, a connected AI \nsystem must provide explain -\nability to the decision-making \nprocess, focusing on the sys -\ntem’s rationale rather than rely -\ning solely on posthoc techniques \nlike feature importance.\n › Accuracy : A connected AI ar -\nchitecture must ensure precise \nsymbolic representation and \nmeaning and be verifiable in a \nmathematical and unambigu -\nous way given the absence of a \nshared consensus or guidelines \nregarding symbolic systems. 25\n › Randomness: Algorithmic pro -\ncesses, individual behaviors, and \nsocial interactions embody some \ndegree of randomness. Thus, \nthe algorithmic predictions and \ngenerative artifacts produced \ncan only reduce error rates to a \nparticular threshold. A rigorous \nand comprehensive evaluation \nof systems built on a connected \nAI architecture must be con -\nducted to minimize errors  \nfrom randomness.\n › Safety: LLM risk does not arise \nfrom LLMs’ statistical capabil -\nities but from their limitations \nand inherent deceptive abilities \n(for example, hallucinations, \ndeepfakes, and disinformation). \nContinuous input and output \naudits are crucial for detecting \ndata and concept drift, ensuring \nunbiased outcomes and detect -\ning toxic or harmful decisions \nand content.\nC\noncerns surrounding LLMs’ lim-\nitations have led to an increased \nfocus on KG-based downstream \napplications. Combining LLMs’ gen -\nerative abilities with KGs’ logical and \nfactual coherence into a connected AI \narchitecture creates a theoretical frame-\nwork to maximize capabilities and min-\nimize systemic shortcomings across \nmany real-world domains. \nACKNOWLEDGMENT\nWe wish to thank Peter Voss and \nSrini Pagidyala from Aigo.ai for their \ncomments.\nREFERENCES\n1. J. Yang et al., “Harnessing the \npower of LLMs in practice: A survey \non ChatGPT and beyond,” 2023. \nAccessed: Jul. 17, 2023. [Online]. \nAvailable: https:/ /arxiv.org/abs/ \n2304.13712\n2. “Fiddler introduces end-to-end \nworkflow for robust generative AI.” \nFiddler AI. Accessed: Jul. 24, 2023. \n[Online]. Available: https:/ /www.\nfiddler.ai/blog/fiddler-introduces  \n-end-to-end-workflow-for-robust  \n-generative-ai\n3. F. Petroni et al., “Language mod -\nels as knowledge bases?” 2019, \narXiv:1909.01066 .\n4. P. Voss and S. Pagidyala, “Interview -\nees,” Aigo.ai, Jul. 2023. \n5. J. Wei et al., “Chain-of-thought \nprompting elicits reasoning in large \nlanguage models,” in Proc. 36th \nConf. Neural Inf. Process. Syst. , New \nOrleans, LA, USA, 2022. [Online]. \nAvailable: https:/ /openreview.net/\npdf?id=_VjQlMeSB_J\n6. L. Ouyang et al., “Training lan -\nguage models to follow instructions \nwith human feedback,” in Proc. \n35th Conf. Neural Inf. Process. Syst. , \nNew Orleans, LA, USA, 2022, pp. \n27,7 30 –27,74 4.\n7. B. Wang et al., “DecodingTrust: \nA comprehensive assessment of \ntrustworthiness in GPT models,” \n2023. Accessed: Jul. 24, 2023. [On -\nline]. Available: https:/ /arxiv.org/\nabs/2306.11698\n108 COMPUTER    WWW.COMPUTER.ORG/COMPUTER\nIT INNOVATION \n8. M. Jovanović and M. Schmitz, “Ex -\nplainability as a user requirement for \nartificial intelligence systems,” Com-\nputer, vol. 55, no. 2, pp. 90–94, Feb. \n2022, doi: 10.1109/MC.2021.3127753.\n9. S. Tworkowski, K. Staniszewski,  \nM. Pacek, Y. Wu, H. Michalewski, \nand P. Miłos, “Focused transformer: \nContrastive training for context \nscaling,” 2023. [Online]. Available: \nhttps:/ /arxiv.org/abs/2307 .03170\n10. A. Mok, “ChatGPT could cost over  \n$700,000 per day to operate,” Bus. \nInsider, Apr. 2023. [Online].  \nAvailable: https:/ /www.business \ninsider.in/tech/news/chatgpt  \n-could-cost-over-700000-per-day\n-to-operate-microsoft-is-reportedly  \n-trying-to-make-it-cheaper-/article  \nshow/99637548.cms \n11. C.-Y. Hsieh et al., “Distilling \nstep-by-step! Outperforming larger \nlanguage models with less training \ndata and smaller model sizes,” 2023, \narXiv:2305.02301 .\n12. S. Ji, S. Pan, E. Cambria, P. Marttinen, \nand P. S. Yu, “A survey on knowledge \ngraphs: Representation, acquisition, \nand applications,” IEEE Trans. Neural \nNetw. Learn. Syst. , vol. 33, no. 2, pp. \n494–514, Feb. 2022, doi: 10.1109/\nTNNLS.2021.3070843 .\n13. C. Peng, F. Xia, M. Naseriparsa, and \nF. Osborne, “Knowledge graphs: \nOpportunities and challenges,” Artif. \nIntell. Rev., early access, Mar. 2023, \ndoi: 10.1007/s10462-023-10465-9.\n14. P. Shao, G. Yang, D. Zhang, J. \nTao, F. Che, and T. Liu, “Tucker \ndecomposition-based temporal \nknowledge graph completion,” \nKnowl.-Based Syst. , vol. 238, Feb. \n2022, Art. no. 107841, doi: 10.1016/j.\nknosys.2021.107841.\n15. H. L. Nguyen, D. T. Vu, and J. J. Jun, \n“Knowledge graph fusion for smart \nsystems: A survey,” Inf. Fusion, \nvol. 61, Sep. 2020, pp. 56–70, doi: \n10.1016/j.inffus.2020.03.014.\n16. S. Pan, L. Luo, Y. Wang, C. Chen, J. \nWang, and X. Wu, “Unifying large \nlanguage models and knowledge  \ngraphs: A roadmap,” 2023, \narXiv:2306.08302 .\n17. H. J. Caufield et al., “Structured \nprompt interrogation and recursive \nextraction of semantics (SPIRES): A \nmethod for populating knowledge \nbases using zero-shot learning,” \n2023, arXiv:2304.02711 .\n18. V. Espinoza, “GraphGPT: Convert \nunstructured natural language into \na knowledge graph,” Medium, Mar. \n2023. [Online]. Available: https:/ / \nmedium.com/@vespinozag/\ngraphgpt-convert-unstructured  \n-natural-language-into-a-knowledge  \n-graph-cccbee19abdf#:~:text=  \nGraphGPT%20converts%20  \nunstructured%20natural%20  \nlanguage,of%20entities%20and%  \n20their%20relationships \n19. P. Liu, W. Yuan, J. Fu, Z. H. Jiang, H. \nHayashi, and G. Neubig, “Pre-train, \nprompt, and predict: A systematic \nsurvey of prompting methods in \nnatural language processing,” ACM \nComput. Surv., vol. 55, no. 9, pp. 1–35, \nSep. 2023, doi: 10.1145/3560815.\n20. B. Scarlini, T. Pasini, and R. Navigli, \n“Sensembert: Context-enhanced \nsense embeddings for multilingual \nword sense disambiguation,” in \nProc. AAAI Conf. Artif. Intell. , 2020, \npp. 8758–8765, doi: 10.1609/aaai.\nv34i05.6402.\n21. A. Tabir, “Langchain: Building \npowerful language model appli -\ncations with Python,” Medium, \nApr. 2023. [Online]. Available: \nhttps://medium.com/@Algi.T/\nlangchain-building-powerfu\nl-language-model-applications-  \nwith-python-c2b77a107709#:~: -\ntext=LangChain%20is%20a%20\npython%20framework,to%20\nother%20sources%20of%20data  \n22. K. Wiggers. “LlamaIndex adds private \ndata to large language models.” \nTechCrunch. Accessed: Jul. 27, \n2023. [Online]. Available: https:/ /\ntechcrunch.com/2023/06/06/\nllamaindex-adds-private-data- \nto-large-language-models/#:~: \ntext=Today%2C%20LlamaIndex% \n20(the%20company),LLM%20 \napplications%2C%E2%80%9D%20\nLiu%20said \n23. I. Kotseruba and J. K. Tsotsos, “40 \nyears of cognitive architectures: \nCore cognitive abilities and practical \napplications,” Artif. Intell. Rev .,  \nvol. 53, pp. 17–94, Jan. 2020, doi: \n10.1007/s10462-018-9646-y.\n24. “Cognitive AI research: Higher \nmachine intelligence for next-\ngen AI,” Intel, Satan Clara, CA, \nUSA, 2022. [Online]. Available: \nhttps:/ /www.intel.com/content/\nwww/us/en/research/blogs/\nhigher-machine-intelligenc\ne-for-next-gen-ai.html\n25. A. d’Avila Garcez and L. C. Lamb, \n“Neurosymbolic AI: The 3rd \nwave,” Artif. Intell. Rev ., early \naccess, Mar. 2023, doi: 10.1007 /\ns10462-023-10448-w.\n26. M. M. Li, K. Huang, and M. Zitnik, \n“Graph representation learning \nin biomedicine and healthcare,” \nNature Biomed. Eng ., vol. 6, pp. \n1353–1369, Dec. 2022, doi: 10.1038/\ns41551-022-00942-x.\n27. X. Wang et al., “KEPLER: A unified \nmodel for knowledge embedding and \npre-trained language representa -\ntion,” Trans. Assoc. Comput. Linguis -\ntics, vol. 9, pp. 176–194, Nov. 2020, \ndoi: 10.1162/tacl_a_00360 .\nMLAĐAN JOVANOVIĆ is an \nassociate professor at Singidunum \nUniversity, 11000 Belgrade, Serbia. \nContact him at mjovanovic@  \nsingidunum.ac.rs .\nMARK CAMPBELL  is the chief \ninnovation officer at EVOTEK, San \nDiego, CA 92121 USA. Contact him at \nmark@evotek.com .",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8809393048286438
    },
    {
      "name": "Generative grammar",
      "score": 0.5841972827911377
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5786721110343933
    },
    {
      "name": "Knowledge graph",
      "score": 0.5605120062828064
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.542748749256134
    },
    {
      "name": "Graph",
      "score": 0.46600446105003357
    },
    {
      "name": "Natural language processing",
      "score": 0.4425647556781769
    },
    {
      "name": "Architecture",
      "score": 0.4367658495903015
    },
    {
      "name": "Theoretical computer science",
      "score": 0.35015594959259033
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170253739",
      "name": "Singidunum University",
      "country": "RS"
    },
    {
      "id": "https://openalex.org/I96690311",
      "name": "Evotec (United States)",
      "country": "US"
    }
  ],
  "cited_by": 16
}