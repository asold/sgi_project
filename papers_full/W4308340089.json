{
  "title": "A lightweight classification of adaptor proteins using transformer networks",
  "url": "https://openalex.org/W4308340089",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2987121117",
      "name": "Sylwan Rahardja",
      "affiliations": [
        "University of Eastern Finland",
        "Northwestern Polytechnical University",
        "Singapore Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101969270",
      "name": "Mou Wang",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    },
    {
      "id": "https://openalex.org/A2131756995",
      "name": "Binh P. Nguyen",
      "affiliations": [
        "Victoria University of Wellington"
      ]
    },
    {
      "id": "https://openalex.org/A145985620",
      "name": "Pasi Fränti",
      "affiliations": [
        "University of Eastern Finland"
      ]
    },
    {
      "id": "https://openalex.org/A2109369563",
      "name": "Susanto Rahardja",
      "affiliations": [
        "University of Eastern Finland",
        "Northwestern Polytechnical University",
        "Singapore Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2987121117",
      "name": "Sylwan Rahardja",
      "affiliations": [
        "Singapore Institute of Technology",
        "University of Eastern Finland",
        "Northwestern Polytechnical University"
      ]
    },
    {
      "id": "https://openalex.org/A2101969270",
      "name": "Mou Wang",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    },
    {
      "id": "https://openalex.org/A2131756995",
      "name": "Binh P. Nguyen",
      "affiliations": [
        "Victoria University of Wellington"
      ]
    },
    {
      "id": "https://openalex.org/A145985620",
      "name": "Pasi Fränti",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109369563",
      "name": "Susanto Rahardja",
      "affiliations": [
        "Northwestern Polytechnical University",
        "Singapore Institute of Technology",
        "University of Eastern Finland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2754307534",
    "https://openalex.org/W6744425714",
    "https://openalex.org/W2739999456",
    "https://openalex.org/W2103017472",
    "https://openalex.org/W6675471023",
    "https://openalex.org/W2153187042",
    "https://openalex.org/W2128196429",
    "https://openalex.org/W2767196078",
    "https://openalex.org/W2997405786",
    "https://openalex.org/W3096508121",
    "https://openalex.org/W2158714788",
    "https://openalex.org/W2965851497",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W4224923672",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W4237670716"
  ],
  "abstract": "Abstract Background Adaptor proteins play a key role in intercellular signal transduction, and dysfunctional adaptor proteins result in diseases. Understanding its structure is the first step to tackling the associated conditions, spurring ongoing interest in research into adaptor proteins with bioinformatics and computational biology. Our study aims to introduce a small, new, and superior model for protein classification, pushing the boundaries with new machine learning algorithms. Results We propose a novel transformer based model which includes convolutional block and fully connected layer. We input protein sequences from a database, extract PSSM features, then process it via our deep learning model. The proposed model is efficient and highly compact, achieving state-of-the-art performance in terms of area under the receiver operating characteristic curve, Matthew’s Correlation Coefficient and Receiver Operating Characteristics curve. Despite merely 20 hidden nodes translating to approximately 1% of the complexity of previous best known methods, the proposed model is still superior in results and computational efficiency. Conclusions The proposed model is the first transformer model used for recognizing adaptor protein, and outperforms all existing methods, having PSSM profiles as inputs that comprises convolutional blocks, transformer and fully connected layers for the use of classifying adaptor proteins.",
  "full_text": "A lightweight classification of adaptor \nproteins using transformer networks\nSylwan Rahardja1, Mou Wang2, Binh P . Nguyen4, Pasi Fränti1 and Susanto Rahardja2,3* \nBackground\nProteins make up a significant portion of the human body. This includes macroscopic \nstructures like the musculoskeletal system, and microscopic processes such as cell to cell \nsignaling. Due to its extensive role in human anatomy and physiology, it is no surprise \nthat proteins contribute to a variety of pathologic conditions. For example, abnormalities \nof protein physiology result in multiorgan-involving diseases such as alpha-1 antitrypsin \ndeficiency, cystic fibrosis and hereditary hemochromatosis. Even common conditions \nsuch as diabetes mellitus, with its established disorder in insulin, revolves around pro -\nteins. Hence, it is no surprise that elucidating protein structure and function is a key \ninterest of the biomedical industry.\nAbstract \nBackground: Adaptor proteins play a key role in intercellular signal transduction, and \ndysfunctional adaptor proteins result in diseases. Understanding its structure is the \nfirst step to tackling the associated conditions, spurring ongoing interest in research \ninto adaptor proteins with bioinformatics and computational biology. Our study aims \nto introduce a small, new, and superior model for protein classification, pushing the \nboundaries with new machine learning algorithms.\nResults: We propose a novel transformer based model which includes convolutional \nblock and fully connected layer. We input protein sequences from a database, extract \nPSSM features, then process it via our deep learning model. The proposed model is \nefficient and highly compact, achieving state-of-the-art performance in terms of area \nunder the receiver operating characteristic curve, Matthew’s Correlation Coefficient \nand Receiver Operating Characteristics curve. Despite merely 20 hidden nodes trans-\nlating to approximately 1% of the complexity of previous best known methods, the \nproposed model is still superior in results and computational efficiency.\nConclusions: The proposed model is the first transformer model used for recognizing \nadaptor protein, and outperforms all existing methods, having PSSM profiles as inputs \nthat comprises convolutional blocks, transformer and fully connected layers for the use \nof classifying adaptor proteins.\nKeywords: Adaptor protein, Protein classification, Deep learning, Transformer\nOpen Access\n© The Author(s) 2022, corrected publication 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 Interna-\ntional License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appro-\npriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. \nThe images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in \na credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by \nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of \nthis licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nRahardja et al. BMC Bioinformatics  2022, 23(1):461 \nhttps://doi.org/10.1186/s12859-022-05000-6\nBMC Bioinformatics\n*Correspondence:   \nsusantorahardja@ieee.org\n1 School of Computing, \nUniversity of Eastern Finland, \nJoensuu, Finland\n2 School of Marine Science \nand Technology, Northwestern \nPolytechnical University, \n710072 Xi’an, China\n3 Singapore Institute \nof Technology, \nSingapore 138683, Singapore\n4 School of Mathematics \nand Statistics, Victoria University \nof Wellington, Wellington, New \nZealand\nPage 2 of 14Rahardja et al. BMC Bioinformatics  2022, 23(1):461\nWhile communication is key in the first world setting, cells communicate at a micro -\nscopic level to maintain homeostasis by signal transduction. For accurate transmission \nof information, the signal must be conveyed reliably into the individual cells. Proteins \nplay a key role in this process. Adaptor proteins are proteins with specific three-dimen -\nsional (3D) structural conformity that serve this purpose. Examples include MYD88 and \nSHC1. These adaptor proteins contain protein-binding molecules linking protein-bind -\ning partners together, facilitating the signal transduction cascade.\nDue to its microscopic complexity, the study of protein structure has been limited \nuntil recent time. Proteins are synthesized via trinucleotide ribonucleic acid (RNA) \ncodons, namely Adenosine, Uracil, Guanine and Cytosine. The triplet RNA codons, each \nof which could either be Adenosine, Uracil, Guanine or Cytosine, give rise to 64 distinct \ntriple codons. The triplet codons each code for an amino acid. The RNAs are then tran -\nscribed into amino acids, the building blocks of proteins. Each amino acid is structur -\nally different, and thus each protein will have a specific 3D structure to serve its unique \nfunction. Specifically in this context, adaptor proteins have structures to facilitate sig -\nnal transduction. Due to coding overlaps, for example both CUU and CUC coding for \namino acid leucine, the 64 permutations only code for 20 different amino acids.\nProtein function prediction is an emerging field in bioinformatics [1], due to the avail -\nability of aforementioned databases and recent development in machine learning. Exten-\nsive research into protein structure and function resulted in the advent of databases such \nas UniProt [2] and Gene Ontology [3], kickstarting the drive into further protein struc -\nture research. Establishing the correct sequence for protein is vital in ensuring its 3D \nstructure is intact. This explains the drive for protein function prediction research and \nthe importance of minimizing losses or errors of amino acid sequences.\nSince a minor discrepancy in amino acids could result in a distinct pathology, the \naccuracy of predictive methods is key. Satisfactory results have been achieved by prior \nstudies such as position specific scoring matrix (PSSM) [4], biochemical properties \n(AAindex) [5], Pseudo Amino Acid Composition [6], and innovative methods using \nRNN and PSSM [7]. However, existing work still left much to be desired.\nIn the field of bioinformatics, application of deep learning algorithms such as CNN \nand RNN had been explored. In [7], RNN was used to model the sequence of PSSM. \nHowever, existing research had its limitations. The RNN has a set number of hidden \nstate but the PSSM has a widely variable length. In contrast, transformer is a novel deep \nlearning model that adopts the mechanism of attention [8]. It outperforms CNN and \nRNN in most cases, and can been used in genomics [9].\nThis paper aims to provide a new standard for distinguishing adaptor proteins. We \nhereby propose an ultra lightweight deep learning framework based on transformer and \nPSSM profiles to identify adaptor proteins. Transformer is a novel deep learning model \nfor sequence analysis of adaptor proteins and the proposed model size in its optimum is \nonly 1.6% of the state-of-the-art methods while in the sub-optimum the model size is less \nthan 1% of the state-of-the-art, wherein both optimum and sub-optimum have better \nperformance than previous best. It takes PSSM profile from the database as the input of \nthe model, uses CNN and transformer for dimensionality reduction and sequence mod -\neling, and outputs the probability of whether the protein under evaluation is an adaptor \nprotein. We then considered usage of layer normalization and Gradient accumulation \nPage 3 of 14\nRahardja et al. BMC Bioinformatics  2022, 23(1):461\n \nalgorithm to solve the problem of single sample training caused by the variable length of \nproteins sequence. The experiment results on the independent dataset proved that our \nproposed model can effectively distinguish adaptor proteins from general proteins and \nexhibit superior performance compared to state-of-the-art algorithm.\nResults and discussion\nDataset\nWe conducted our experiments on the dataset created in [7]. The dataset includes 1224 \nadaptor proteins and 11,078 non-adaptor proteins. We used all the protein sequences \nimported from two well-known databases, namely UniProt and Gene Ontology. Only \nprotein sequences which have been published in papers (termed reviewed sequences) \nwere selected. To prevent over-fitting, redundant sequences with sequence identity level \nof more than 30% were removed with the Basic Local Alignment Search Tool (BLAST) \nmethod [10].\nWe used one-fifths of both the adaptor proteins and the non-adaptor proteins as the \ntest set to evaluate model performance. The rest of the valid sequences were used as \na training dataset for model training. The detailed numbers of proteins are shown in \nTable 1.\nSettings\nThe Proposed model was implemented on NVIDIA GeForce 3090 GPU with PyTorch-\nlightning library. For all experiments, we trained the models for 50 epoches with Adam \noptimizer. The learning rate was initialized to 5 × 10−4 , and halved if the Area Under the \nCurve (AUC) of validation set was not improved after 6 consecutive epochs. Early stop -\nping was applied if the AUC was not improved after 20 consecutive epochs.\nThe batch size had to be set to 1 because of the problem of sequence length. Due to \nthe batch size being set to 1, the gradient of model optimization would have been too \nrandom, making the model training unstable and difficult. Hence, to mitigate this issue, \nwe used gradient accumulation. With gradient accumulation, the model variables would \nnot be updated in every step until the gradients of a set number of batches were accu -\nmulated. In this article, the size of accumulate gradient batch is set to 24.\nTo evaluate the performance, we utilized fivefold cross-validation technique on the \ntraining dataset. We selected the model with the best performance on the validation set \nfor each fold. Finally, the independent dataset was used to evaluate the model.\nEvaluation metrics\nFor simplicity of calculation and presentation, protein and non-adaptor protein are \ndefined as positive data and negative data respectively. Common but effective evalua -\ntion metrics that were used to measure the classification performance of the proposed \nTable 1 Dataset\nTrain Test Total\nAdaptor 1069 155 1224\nNon-adaptor 9695 1383 11,078\nPage 4 of 14Rahardja et al. BMC Bioinformatics  2022, 23(1):461\nmodel, include accuracy, specificity, sensitivity and MCC (Matthew’s correlation coef -\nficient), which can all be derived from the confusion matrix. In the confusion matrix, \nthere are four categories, namely true positives, false positives, true negatives, false neg -\natives, denoted as TP , FP , TN, FN respectively. Then the evaluation metrics are defined \nas follows:\nIn a binary classification, accuracy, specificity and sensitivity cannot reflect the real \nperformance of the method, especially when the data is imbalanced. However, MCC is \nessentially a correlation coefficient between the observed and predicted binary classifi -\ncations. Hence it is more used as a means to provide correlation information rather than \naccuracy of the classification, because it takes into account the balance ratios of the four \nconfusion matrix categories.\nReceiver Operating Characteristic (ROC) curve is also a common and reliable per -\nformance measurement for a classification problem at various thresholds settings. The \nAUC measures the entire two-dimensional (2D) area under the ROC curve. This score \ncan reflect the performance of the classifier. The AUC value falls within a range from 0 \nto 1, where a higher value indicates a superior model. Besides, area under the precision-\nrecall curve (AUPRC) is a useful performance metric for imbalanced data as well. In this \npaper, we focus on AUC and MCC.\nComparison methods\nEarlier, there were articles that utilize summation of amino acids to form 400-dimen -\nsional vector for the input of the neural networks [5, 11]. In addition, k-NN, Random \nForest, Support Vector Machine (SVM), 2D Convolutional Neural Network and Recur -\nrent Neural Networks (RNN) were also used to distinguish adaptor proteins [7]. RNN \nwas being considered as state-of-the-art since it has the best performance as reported \nin [7] and achieved cross validation accuracy and MCC of 80.4% and 44.5% respectively. \nSpecifically, the RNN model utilized PSSM profiles as inputs and obtained their features \nby two one-dimensional (1D) convolutional layers and 1D average pooling layers. In the \nmodel, the kernel size of convolution and pooling was 3, and the channel number of \neach distinct convolutional layer was 256. The features were then fed forward to Gated \nRecurrent Units (GRU) with 256 hidden cells. Lastly, the model processed the GRU out -\nput through a fully connected layer with 512 nodes, and then passed through a sigmoid \nlayer to produce a prediction probability value.\n(1)Accuracy= TP + TN\nTP + TN + FP + FN\n(2)Speciﬁcity= TN\nTP + FP\n(3)Sensitivity= TP\nTP + FN\n(4)MCC = TP× TN − FP× FN√(TP+ FP)(TP+ FN)(TN + FP)(TN+ FN)\nPage 5 of 14\nRahardja et al. BMC Bioinformatics  2022, 23(1):461\n \nBeside the RNN model, SVM and CNN [11] were also used to classify the adaptor pro-\nteins in [7]. CNN and RNN currently represent the state-of-the-art for protein classifica-\ntion problem. In the SVM, g was set to 0.5 and margin parameter c was set to 8. In CNN \nmethod, the filter number of convolution was 128 with kernel of 3 × 3 . In this article, we \ndesigned a transformer based system and compared against the CNN and RNN that cur-\nrently represent the state-of-the-art for protein classification problem.\nResults\nThe proposed model utilized the newly introduced transformer blocks in combination \nwith convolutional blocks and fully connected layers. In the simulation, the proposed \nmodel was compared with SVM method, CNN [11] and RNN [7], and the results in \nterms of sensitivity, specificity, accuracy, AUC and MCC were tabulated in Table  2 for \nboth cross validation and independent tests. In addition, the model size is also shown as \nanother metric of comparison. We observe that the proposed model achieved a higher \nAUC and MCC than all other existing methods. This proves that the sequential infor -\nmation of PSSM has more potential in classification of adaptor proteins, and the trans -\nformer based model can effectively extract and utilize it.\nThe sensitivity of the model reflects the effectiveness of a classifier in identifying true \npositives. The higher the sensitivity is, the more adapter proteins can be discovered from \na sample of all proteins. From the result, we concluded that the sensitivity of the pro -\nposed method was significantly higher than SVM and CNN. In contrast, the model was \nonly slightly better than RNN, because both the proposed method and RNN have the \nability of sequence modeling. This shows that the sequence information plays an impor -\ntant role in identification of adaptor proteins.\nComparing RNN and our model, it was clear that our model is superior. As shown \nin Table 3, the model size of the proposed model is about 12.1k, which is only 1.6% of \nthat of RNN. The FLOPs of the proposed method is about 216k, which is 2.5% of that \nof RNN. Despite being ultra lightweight, the model still achieved superior sensitivity, \ndemonstrating that transformer is significantly more effective and efficient than CNN \nand RNN, as it allowed discovery of adaptor proteins more rapidly. In addition, the sig -\nnificantly reduced model size opens possibilities as it naturally makes embedding it into \nother platforms easier.\nNext, we consider the ROC curve as a comparison of efficacy. The ROC curve reflects \nthe performance at all different decision threshold levels. The ROC curves of RNN and \nthe tranformer based model are shown in Fig.  1. Evidently, the transformer model being \ntested outperforms RNN at most decision threshold levels. Moreover, the proposed \nmethod attains an AUC of 0.903 which shows that the model can still perform well \ndespite the complication provided by varying sequence length in the dataset. Thus, this \nmodel is well suited to be used as adaptor proteins predictor.\nTo verify the effectiveness of transformer in the proposed method, we conducted abla-\ntion experiment by disabling the Transformer Encoder block. The result is shown in \nTable 4. From the Table, we can find that transformer can significantly improve the per -\nformance on all the metrics. Because transformer is utilized to explore sequence infor -\nmation, this ablation experiments demonstrate that the sequence information plays a \nsignificant role in identification of adaptor proteins again.\nPage 6 of 14Rahardja et al. BMC Bioinformatics  2022, 23(1):461\nTable 2 Performance of adaptor proteins classification with different methods\nBold indicates the best value per metric\nMethods Cross validation Independent test\nSensitivity Specificity Accuracy AUC MCC Sensitivity Specificity Accuracy AUC MCC AUPRC\nSVM 0.397 0.934 0.881 0.818 0.332 0.426 0.932 0.881 0.806 0.353 0.342\nCNN [11] 0.801 0.738 0.750 0.834 0.368 0.851 0.780 0.787 0.874 0.423 0.437\nRNN [7] 0.812 0.751 0.757 0.853 0.373 0.856 0.798 0.804 0.893 0.446 0.462\nProposed 0.786 0.803 0.801 0.868 0.404 0.865 0.827 0.831 0.903 0.487 0.509\nPage 7 of 14\nRahardja et al. BMC Bioinformatics  2022, 23(1):461\n \nBesides the self-attention, the Feed Forward Network (FFN) is also an important com -\nponent in transformer as it can increase the complexity and improve performance. For \ncomparison, we also performed experiments using different numbers of hidden nodes \nin transformer. The results are shown in Table  5. We observe that the system had the \nbest performance when the FNN had 128 hidden nodes. As shown in Table 5, the lowest \ncomplexity with just 20 hidden nodes corresponds to a model size of 7.7k. This trans -\nlates to less than 1% compared to the size of RNN based method, yet retaining its perfor-\nmance ability in terms of AUC and MCC.\nConclusions\nA new model to classify adaptor proteins is proposed in this article. The new model con-\nsiders sequence information using transformer and PSSM profile. With this model, the \nPSSM feature was first obtained with Position-Specific Iterated BLAST (PSI-BLAST) \nmethod, then fed into a transformer based model for classification. It is the first time \nthat transformer is utilized in the field of adaptor protein recognition, with clearly \nTable 3 Comparison of model size and complexity\nBold indicates the best value per metric\nMethods Model size FLOPs\nCNN 160k 1629k\nRNN [7] 729k 8549k\nProposed 12.1k 216k\nFig. 1 The receiver operating characteristic curve\nTable 4 Ablation studies on proposed method\nMethods AUC MCC AUPRC\nProposed (default) 0.903 0.487 0.509\nw/o Transformer 0.889 0.441 0.459\nPage 8 of 14Rahardja et al. BMC Bioinformatics  2022, 23(1):461\nunparalleled results. The experimental results proved that the proposed method can \nachieve AUC of 0.903 and MCC of 0.487 on independent testing dataset, which tri -\numphs the state-of-the-art methods. Despite a remarkably small size with just 1.6% of \nthe previous leading model, this model demonstrated that transformers can model the \nsequence information of protein more effectively and efficiently than RNN based model.\nWith its multitude of functions, we hope our work in adaptor protein brings sig -\nnificant contribution to the field. This article shows that transformer based model can \neffectively model the sequence information, and we believe it can be further applied \nfor detection, classification and analytics of other proteins functions, or even other \nchallenges in bioinformatics and computational biology.\nMethods\nProposed methods\nThe objective of this study is to accurately identify adaptor proteins from an unclassified \nand unknown sequence. The flowchart is shown in Fig.  2. We first obtain the adaptor \nproteins and non-adaptor proteins from the database. Then, the processing contains two \nparts: the PSSM features were first extracted from proteins squence, then fed into a deep \nlearning model to output a prediction. In the following, we introduce each step in detail.\nTable 5 Performance of adaptor protein prediction on independent testing set using different \nnumbers of hidden nodes in transformer. The model has three convolutional blocks with 20 \nconvolution kernel\nBold indicates the best value per metric\nHidden nodes AUC MCC Model size\n20 0.8941 0.4668 7.7k\n32 0.9042 0.4696 8.2k\n64 0.8978 0.456 9.5k\n80 0.8998 0.4646 10.1k\n128 0.9031 0.4872 12.1k\n200 0.8999 0.4639 15.0k\n256 0.8978 0.4633 17.3k\nFig. 2 The flowchart of the proposed method\nPage 9 of 14\nRahardja et al. BMC Bioinformatics  2022, 23(1):461\n \nFeature extraction\nAs PSSM had shown promising results in bioinformatics research [4] in the past, it has \nsince been a common and effective feature to describe protein secondary structure. A \nPSSM profile is a matrix which can be used to assimilate amino acid peptide sequences. \nThe matrix is created by generating two sequences with different peptide sequences, but \ncomparable with 3 dimensional conformation. Given that there are 20 distinct amino \nacids, we simply use a N × 20 matrix, where N denotes the sequence of interest. The \nindividual components of the PSSM profile can be denoted as P ij , where i represents \nthe amino acid in the j-th position of the sequence. A high output value is optimal, as \nit means the peptide sequence is conserved, while a negative value is suboptimal and \nrepresents a compromised value. In this study, we utilized protein sequences for dataset \nin FASTA format. Then, PSI-BLAST was used to change FASTA sequences into PSSM \nprofiles.\nA significant challenge posed by data of proteins sequence is a wide range of sequence \nlength. For example, in this work, the shortest sequence of PSSM profile has only 18 \npoints, but the longest sequence of PSSM has more than 20,000 points. The variation in \nlength brought about challenges in establishing a reliable model, as most models require \ninput sequences of similar length. Although some deep learning models can process vec-\ntors with variable length, the input sequences should ideally have equal length during the \ntraining stage to build a reliable model. To tackle this issue, some authors consider the \nfollowing solution [12]: We could sum all the amino acids in PSSM profile, and PSSM \nprofiles with N × 20 is converted into a 20 × 20 . While the input length was a constant, \nthis came at a cost of loss of sequence information as the ordering of the PSSM profile is \ncompromised.\nProtein sequences are distinct from other sequence analysis problems. By nature, pro -\ntein sequences are distinct from other topics such as speech and text. In most appli -\ncations such as audio processing, there were common methods proposed to solve this \nproblem such as padding, sliding windows and etc. The methods which are effective \nfor speech and text are unable to achieve similar results for protein sequences. Padding \nmakes short sequences meaningless when standard deviation of sequence length is large, \nand sliding window will break the protein sequences leading to artificial and meaning -\nless sequences. Similar to [7], we had to treat each entire protein sequence as a whole \nand input the sequence into the model. This led to the batch size preset of the input \nmodel to 1.\nModels\nAs shown in Fig.  2, the model consists of three modules: three convolutional block, \ntransformer and fully connected layers.\nThe model took PSSM profiles as inputs and extracted their features by three con -\nvolutional blocks, namely 1D convolutional layers and 1D average pooling layer, where \nthe 1D convolution operates on the sequence dimension. Then, the extracted features \nwere fed into the transformer, where the spatial context within the entire PSSM pro -\nfile was explored and modeled. Subsequently, global pooling was used to summarize the \nsequence and achieve a 1D vector. The advantage of global pooling was the ability to \nPage 10 of 14Rahardja et al. BMC Bioinformatics  2022, 23(1):461\nmap the sequence with different length into a vector with the same length. Hence, we \nused global average pooling.\nIn the final stage, two fully-connected (FC) layer and a sigmoid function were used to \nclassify the vector. The RNN model output is a scalar having [0, 1] which indicates the \nprobability of the testing PSSM profile belonging to the adaptor or non-adaptor protein \ncategories. Finally, to avoid overfitting, dropout of 0.5 was applied after the first FC layer.\nConvolutional block\nCNN is a powerful and effective method for feature transformation. Comparing to tra -\nditional and manually designed features, the learnable feature extracted by CNN is more \ncompact and effective. Therefore, we used CNN to further extract more effective fea -\ntures from PSSM before sequence modeling.\nIn CNN, the features are converted into a higher dimension feature map with a set of \nconvolution kernels. To obtain good feature representation, some incorporate more con-\nvolution kernals as high as 256 [7]. Because transformer has a strong ability of sequence \nmodeling, the requirement of convolution kernels for good feature representation can be \nreduced.\nIn addition, a large feature map will increase memory consumption of transformers. \nTherefore, we propose the usage of three convolution layers with only 20 convolution \nkernels of 3 × 3 , followed by a normalization layer. Then, the 1D average pooling layer \nwith kernel of 2 was applied which was essentially aimed to lessen the feature maps \ndimension and at the same time enlarge the receptive field of the CNN network.\nBatch normalization is a common method in CNN. It applies scalar scale and bias for \nall batches. It can make the convergence of CNN model more stable and rapid during \nthe training, and reduce the undesired effect of model over-fitting. However, batch nor -\nmalization was not applicable in this work because the batch size had to be 1. To address \nthis issue, layer normalization was used. Unlike batch normalization, layer normaliza -\ntion applies per-element scale and bias along the channel dimension [13]. Given the fea -\nture map x, the layer normalization can be expressed as\nwhere E and Var denote expectation and variance respectively, and γ and β are learnable \naffine transform parameters.\nTransformer\nTransformer is a novel neural network for natural language processing, first proposed by \nGoogle [8]. Transformer has advantages of both sequence modeling like RNN and par -\nallel processing like CNN. With its self-attention mechanism, transformer can explore \nlonger contextual information than RNN. Therefore, it has been rapidly applied in vari -\nous fields such as machine translation [14], speech [15], image [16] and genome [17].\nThe transformer architecture is essentially an encoder-decoder model [18]. While the \nencoder has encoding layers that process input systemically, the decoder has decoding \nlayers with similar function based on the encoding layer output. Both share structurally \n(5)y = x − E[x]√ Var[x]∗γ + β\nPage 11 of 14\nRahardja et al. BMC Bioinformatics  2022, 23(1):461\n \nsimilar model. However, the decoder is dependent on encoding output. Specifically, we \nfocused on the transformer encoders in this article. It consists of three core modules: \nScale dot-product attention, multi-head attention and position-wise FFN.\nThe most basic element in a transformer is the scaled dot-product attention, which is \nessentially a self-attention mechanism that aims to efficiently combine different positions \nof input sequences so as to generate inputs representations, as shown in Fig.  3. Each \nindividual output has a significance value which is attained from the attention function \nderived from query of the respective keys and adding the weighted sum to these outputs \nwould produce the outcome of the transformer module. As shown in Fig.  4, multi-head \nattention comprises multiple scaled dot-product attention modules. In the first stage, \nthe module linearly calculated the inputs h times with varying and learnable linear pro -\njections to acquire parallel queries, keys and values respectively. In the subsequent stage, \ndot-product attention was then applied to these queries, keys and values together.\nFig. 3 Scaled dot-product attention. This figure is copied from [8]\nFig. 4 Multi-head attention. This figure is copied from [8]\nPage 12 of 14Rahardja et al. BMC Bioinformatics  2022, 23(1):461\nPosition-wise FFN is a completely integrated feed-forward networking. It consists of \ntwo linear transformations with a ReLU activation in the middle. Besides these three \ncore modules, transformers incorporate multiple residual and normalization layers, with \nlayer normalization employed [19]. The overall architecture of the transformer can be \nmapped as:\nHere, Z ∈ Rl×d is the input with length l and dimension d, and Q i, Ki, Vi ∈ Rl×d/h are the \nmapped queries, keys and values respectively. W Q\ni , W K\ni , W V\ni ∈ Rd×d/h and W O ∈ Rd×d \nare parameter matrices. FFN denotes the output of the position-wise FFN, in which \nW 1 ∈ Rd×dﬀ ,W 2 ∈ Rdﬀ ×d,b1 ∈ Rdﬀ ,b2 ∈ Rd . In this work, d was set to 20, h was set to \n5, and dﬀ  was set to 128.\nLoss\nBased on the provided dataset, there were obvious discrepancies in the available adaptor \nproteins versus non adaptor protein, where the latter significantly outnumbered the for -\nmer. Hence, we utilized weighted binary cross-entropy loss in the training. Let x denote \nthe input sequence, y denote label , w denote the weight, L denote the loss, we have the \nfollowing equation\nwhere weight w is set to the inverse class frequency. In this work, it is set to [10.07, 1.11].\nAbbreviations\nAUC   Area under the ROC curve\nCNN  Convolutional neural network\nFFN  Feed forward network\nMCC  Matthew’s correlation coefficient\nPSSM  Position specific scoring matrix\nReLU  Rectified linear unit\nRNN  Recurrent neural network\nROC  Receiver operating characteristic\nSVM  Support vector machine\n(6)Qi = ZW Q\ni ,Ki = ZW K\ni ,Vi = ZW V\ni ,i∈[ 1, h]\n(7)headi = Attention(Qi, Ki, V i)\n(8)= softmax(Q iK T\ni√\nd\n)V i\n(9)MultiHead= Concat(head1 ,... ,headh)W O\n(10)Mid = LayerNorm(Z + MultiHead)\n(11)FFN = ReLU(MidW1 + b1)W 2 + b2\n(12)Output = LayerNorm(Mid + FFN )\n(13)L = w ∗y∗log x + (1 − y) ∗log(1 − x),\nPage 13 of 14\nRahardja et al. BMC Bioinformatics  2022, 23(1):461\n \nAcknowledgements\nNot applicable.\nAuthor contributions\nSR: conceptualization, investigation, formal analysis, validation, visualization, writing draft, and editing. MW: conceptualiza-\ntion, investigation, formal analysis, validation, visualization, writing draft, editing, software. BPN: writing review and editing. \nPF: formal analysis, writing review and editing, supervision. SR: conceptualization, formal analysis, writing review and editing, \nsupervision. SR and MW contribute equally to the work. All authors read and approved the final manuscript.\nFunding\nMou Wang gratefully acknowledges financial support from China Scholarship Council. The work of S. Rahardja was sup-\nported in part by the Overseas Expertise Introduction Project for Discipline Innovation (111 project: B18041). The publication \ncosts were covered by the authors.\nAvailability of data and materials\nThe datasets analysed during the current study are available at https:// github. com/ ngphu binh/ adapt ors. Our source code \nare available at https:// github. com/ wangm ou21/ adapt or. If someone wants to request the raw data or source code, please \nfeel free to contact Mou Wang or Susanto Rahardja.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 3 April 2022   Accepted: 13 September 2022\nPublished: 4 November 2022\nReferences\n 1. Cruz LM, Trefflich S, Weiss VA. Protein function prediction. Methods Mol Biol. 2017;55–75.\n 2. Consortium TU. UniProt: a hub for protein information. Nucleic Acids Res. 2014;43(D1):204–12.\n 3. Ashburner M, Ball CA, Blake JA, Botstein D, Butler H, Cherry JM, Davis AP , Dolinski K, Dwight SS, Eppig JT, et al. Gene \nontology: tool for the unification of biology. Nat Genet. 2000;25(1):25–9.\n 4. Jones DT. Protein secondary structure prediction based on position-specific scoring matrices. J Mol Biol. \n1999;292(2):195–202.\n 5. Chen S-A, Ou Y-Y, Lee T-Y, Gromiha MM. Prediction of transporter targets using efficient RBF networks with PSSM profiles \nand biochemical properties. Bioinformatics. 2011;27(15):2062–7.\n 6. Cheng X, Xiao X, Chou K-C. pLoc-mHum: predict subcellular localization of multi-location human proteins via general \nPseAAC to winnow out the crucial GO information. Bioinformatics. 2017;34(9):1448–56.\n 7. Le NQK, Nguyen QH, Chen X, Rahardja S, Nguyen BP . Classification of adaptor proteins using recurrent neural networks \nand PSSM profiles. BMC Genomics. 2019;20:1–9.\n 8. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Lu, Polosukhin I. Attention is all you need. In: \nAdvances in neural information processing systems, vol 30. 2017. p. 5998–6008.\n 9. Clauwaert J, Waegeman W. Novel transformer networks for improved sequence labeling in genomics. IEEE/ACM Trans \nComput Biol Bioinform. 2020;1–11. https:// doi. org/ 10. 1109/ TCBB. 2020. 30350 21.\n 10. Altschul SF, Madden TL, Schäffer AA, Zhang J, Zhang Z, Miller W, Lipman DJ. Gapped BLAST and PSI-BLAST: a new gen-\neration of protein database search programs. Nucleic Acids Res. 1997;25(17):3389–402.\n 11. Le N-Q-K, Nguyen BP . Prediction of fmn binding sites in electron transport chains based on 2-D CNN and PSSM profiles. \nIEEE/ACM Trans Comput Biol Bioinform. 2019;1. https:// doi. org/ 10. 1109/ TCBB. 2019. 29324 16.\n 12. Chen S-A, Ou Y-Y, Lee T-Y, Gromiha MM. Prediction of transporter targets using efficient RBF networks with PSSM profiles \nand biochemical properties. Bioinformatics. 2011;27(15):2062–7. https:// doi. org/ 10. 1093/ bioin forma tics/ btr340.\n 13. Ba JL, Kiros JR, Hinton GE. Layer normalization. arXiv: 1607. 06450 2016.\n 14. Devlin J, Chang M-W, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understand-\ning. arXiv: 1810. 04805 2018.\n 15. Chen J, Wang M, Zhang X-L, Huang Z, Rahardja S. End-to-end multi-modal speech recognition with air and bone \nconducted speech. In: ICASSP 2022–2022 IEEE international conference on acoustics, speech and signal processing \n(ICASSP). 2022. p. 6052–6056. https:// doi. org/ 10. 1109/ ICASS P43922. 2022. 97473 06.\n 16. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, \net al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv: 2010. 11929 2020.\n 17. Ji Y, Zhou Z, Liu H, Davuluri RV. DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model \nfor DNA-language in genome. Bioinformatics. 2021;37(15):2112–20. https:// doi. org/ 10. 1093/ bioin forma tics/ btab0 83.\nPage 14 of 14Rahardja et al. BMC Bioinformatics  2022, 23(1):461\n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 18. Bai J, Wang M, Chen J. Dual-path transformer for machine condition monitoring. In: 2021 Asia-Pacific signal and informa-\ntion processing association annual summit and conference (APSIPA ASC). 2021. p. 1144–1148.\n 19. Ba j, Chen j, Wang M, Muhammad SA. A squeeze-and-excitation and transformer based cross-task system for envi-\nronmental sound recognition. arXiv: 2203. 08350 2022.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Signal transducing adaptor protein",
  "concepts": [
    {
      "name": "Signal transducing adaptor protein",
      "score": 0.8149438500404358
    },
    {
      "name": "Computer science",
      "score": 0.6668636202812195
    },
    {
      "name": "Transformer",
      "score": 0.4926326870918274
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4487546384334564
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3768826723098755
    },
    {
      "name": "Machine learning",
      "score": 0.33106207847595215
    },
    {
      "name": "Signal transduction",
      "score": 0.1862846314907074
    },
    {
      "name": "Biology",
      "score": 0.18104207515716553
    },
    {
      "name": "Engineering",
      "score": 0.1384306252002716
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}