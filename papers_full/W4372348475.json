{
  "title": "Between Reality and Delusion: Challenges of Applying Large Language Models to Companion Robots for Open-Domain Dialogues with Older Adults",
  "url": "https://openalex.org/W4372348475",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2607467068",
      "name": "Bahar Irfan",
      "affiliations": [
        "KTH Royal Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4376000007",
      "name": "Sanna-Mari Kuoppamäki",
      "affiliations": [
        "KTH Royal Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1791992558",
      "name": "Gabriel Skantze",
      "affiliations": [
        "KTH Royal Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964236337",
    "https://openalex.org/W3042276730",
    "https://openalex.org/W3090022999",
    "https://openalex.org/W2980476332",
    "https://openalex.org/W3049046209",
    "https://openalex.org/W2963095307",
    "https://openalex.org/W2809386502",
    "https://openalex.org/W3122548859",
    "https://openalex.org/W3192706046",
    "https://openalex.org/W2965168627",
    "https://openalex.org/W4300961861",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2946083353",
    "https://openalex.org/W2891525068",
    "https://openalex.org/W2609169299",
    "https://openalex.org/W3185031028",
    "https://openalex.org/W3037263482",
    "https://openalex.org/W3090839650",
    "https://openalex.org/W2896550350",
    "https://openalex.org/W3181414820",
    "https://openalex.org/W2792732529",
    "https://openalex.org/W4281254423",
    "https://openalex.org/W3148471254",
    "https://openalex.org/W4211155626",
    "https://openalex.org/W4300961860",
    "https://openalex.org/W4210566925",
    "https://openalex.org/W3183385616",
    "https://openalex.org/W3111447970",
    "https://openalex.org/W4296977180",
    "https://openalex.org/W4220855288",
    "https://openalex.org/W4221071943",
    "https://openalex.org/W2137577228",
    "https://openalex.org/W3159977137",
    "https://openalex.org/W3088002417",
    "https://openalex.org/W3187911027",
    "https://openalex.org/W2611611216",
    "https://openalex.org/W1924618820",
    "https://openalex.org/W2946052718",
    "https://openalex.org/W3175314523",
    "https://openalex.org/W3162594825",
    "https://openalex.org/W3168715003",
    "https://openalex.org/W2972844995",
    "https://openalex.org/W3199541953",
    "https://openalex.org/W2985588979",
    "https://openalex.org/W3135281669",
    "https://openalex.org/W2941231575",
    "https://openalex.org/W3160278099",
    "https://openalex.org/W2791006121",
    "https://openalex.org/W2075729255",
    "https://openalex.org/W3173226117",
    "https://openalex.org/W3179942467",
    "https://openalex.org/W2033875623",
    "https://openalex.org/W2046077605",
    "https://openalex.org/W3032205432",
    "https://openalex.org/W3134702929",
    "https://openalex.org/W2632366256",
    "https://openalex.org/W3000465599",
    "https://openalex.org/W2721233823",
    "https://openalex.org/W3128474827",
    "https://openalex.org/W3014679511",
    "https://openalex.org/W2943794164",
    "https://openalex.org/W3030063333",
    "https://openalex.org/W3094018806",
    "https://openalex.org/W2967859656",
    "https://openalex.org/W2040259556",
    "https://openalex.org/W2294978157",
    "https://openalex.org/W2945173102",
    "https://openalex.org/W3174596858",
    "https://openalex.org/W2890000005",
    "https://openalex.org/W2077173600",
    "https://openalex.org/W3047975279",
    "https://openalex.org/W3084929246",
    "https://openalex.org/W2787712888",
    "https://openalex.org/W2702583360",
    "https://openalex.org/W3039125063",
    "https://openalex.org/W2796979547",
    "https://openalex.org/W3009322070",
    "https://openalex.org/W3019586752",
    "https://openalex.org/W2336272118",
    "https://openalex.org/W2594575328",
    "https://openalex.org/W2843010082",
    "https://openalex.org/W3160418499",
    "https://openalex.org/W2940457005",
    "https://openalex.org/W2766884442",
    "https://openalex.org/W4288359815",
    "https://openalex.org/W2796009745",
    "https://openalex.org/W2971622523",
    "https://openalex.org/W2945502151",
    "https://openalex.org/W2972203331",
    "https://openalex.org/W3135127693",
    "https://openalex.org/W2948227308",
    "https://openalex.org/W2099936971",
    "https://openalex.org/W2946507556",
    "https://openalex.org/W2155044807",
    "https://openalex.org/W3180786255",
    "https://openalex.org/W3183818515",
    "https://openalex.org/W3043274366",
    "https://openalex.org/W1979245380",
    "https://openalex.org/W2898938475",
    "https://openalex.org/W2654569425",
    "https://openalex.org/W1977616813",
    "https://openalex.org/W2794198300",
    "https://openalex.org/W2559823655",
    "https://openalex.org/W2986874029",
    "https://openalex.org/W4288359812",
    "https://openalex.org/W2978422582",
    "https://openalex.org/W3031172930",
    "https://openalex.org/W2038540961",
    "https://openalex.org/W2890735031",
    "https://openalex.org/W3012288267",
    "https://openalex.org/W2994761079",
    "https://openalex.org/W3015138955",
    "https://openalex.org/W3185973438",
    "https://openalex.org/W2896069216",
    "https://openalex.org/W2809954094",
    "https://openalex.org/W4214885931",
    "https://openalex.org/W2546565370",
    "https://openalex.org/W2906275827",
    "https://openalex.org/W2109031474",
    "https://openalex.org/W2136608905",
    "https://openalex.org/W2340234432",
    "https://openalex.org/W2953965928",
    "https://openalex.org/W2899195520",
    "https://openalex.org/W2180977419",
    "https://openalex.org/W2547134104",
    "https://openalex.org/W1926404136",
    "https://openalex.org/W2602413655",
    "https://openalex.org/W2887408769",
    "https://openalex.org/W3003369869",
    "https://openalex.org/W3022328840",
    "https://openalex.org/W3167831474",
    "https://openalex.org/W4225974503",
    "https://openalex.org/W2032191478",
    "https://openalex.org/W3135068013",
    "https://openalex.org/W3031361937",
    "https://openalex.org/W2899179635",
    "https://openalex.org/W4297909582",
    "https://openalex.org/W4308798315",
    "https://openalex.org/W2118157444",
    "https://openalex.org/W2169355841",
    "https://openalex.org/W3133520998",
    "https://openalex.org/W3033821783",
    "https://openalex.org/W3010903994",
    "https://openalex.org/W3093935650",
    "https://openalex.org/W3012117326",
    "https://openalex.org/W4294680543",
    "https://openalex.org/W2997444116",
    "https://openalex.org/W2918817626",
    "https://openalex.org/W3087532900",
    "https://openalex.org/W2169953052",
    "https://openalex.org/W2140140278",
    "https://openalex.org/W1992856715",
    "https://openalex.org/W2097775348",
    "https://openalex.org/W1967993231",
    "https://openalex.org/W2117959244",
    "https://openalex.org/W55512605",
    "https://openalex.org/W2053468430",
    "https://openalex.org/W2091552421",
    "https://openalex.org/W2509580039",
    "https://openalex.org/W2130336894",
    "https://openalex.org/W2091460338",
    "https://openalex.org/W1967747674",
    "https://openalex.org/W1984748091",
    "https://openalex.org/W3203483626",
    "https://openalex.org/W2052284387",
    "https://openalex.org/W2904555252",
    "https://openalex.org/W2903645393",
    "https://openalex.org/W2594770275",
    "https://openalex.org/W2107275372",
    "https://openalex.org/W2572325343",
    "https://openalex.org/W2811122113",
    "https://openalex.org/W3198324653",
    "https://openalex.org/W4293550658",
    "https://openalex.org/W3042239274",
    "https://openalex.org/W3033476063",
    "https://openalex.org/W4205231570",
    "https://openalex.org/W4311000453",
    "https://openalex.org/W4389010463",
    "https://openalex.org/W4323537006",
    "https://openalex.org/W1542318638",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W3186804217",
    "https://openalex.org/W2794343903",
    "https://openalex.org/W1985479696",
    "https://openalex.org/W2293392332",
    "https://openalex.org/W2084235337",
    "https://openalex.org/W3010164621",
    "https://openalex.org/W3120491243",
    "https://openalex.org/W2953586260",
    "https://openalex.org/W2528441053",
    "https://openalex.org/W6772715161",
    "https://openalex.org/W2911489449",
    "https://openalex.org/W2913443447",
    "https://openalex.org/W3186032793",
    "https://openalex.org/W2143331706",
    "https://openalex.org/W1986826527",
    "https://openalex.org/W6798616804",
    "https://openalex.org/W2101348300",
    "https://openalex.org/W1966938024",
    "https://openalex.org/W1796112755",
    "https://openalex.org/W4221160645",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W2140721185",
    "https://openalex.org/W6811129797",
    "https://openalex.org/W47029222",
    "https://openalex.org/W2030335114",
    "https://openalex.org/W2001829221",
    "https://openalex.org/W2001771035",
    "https://openalex.org/W3103891289",
    "https://openalex.org/W1646777016",
    "https://openalex.org/W3073992377",
    "https://openalex.org/W6747724447",
    "https://openalex.org/W2944069152",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W2962974452",
    "https://openalex.org/W2151814822",
    "https://openalex.org/W2964309167",
    "https://openalex.org/W2104896032",
    "https://openalex.org/W3036846138",
    "https://openalex.org/W3169976744",
    "https://openalex.org/W4309663019",
    "https://openalex.org/W2208327541",
    "https://openalex.org/W1582109619",
    "https://openalex.org/W2150715964",
    "https://openalex.org/W2166267632",
    "https://openalex.org/W1729704137",
    "https://openalex.org/W2161565097",
    "https://openalex.org/W2116094002",
    "https://openalex.org/W2139109673",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4238855440",
    "https://openalex.org/W2100758964",
    "https://openalex.org/W3094545784",
    "https://openalex.org/W2970252402",
    "https://openalex.org/W3199655832",
    "https://openalex.org/W3205054974",
    "https://openalex.org/W2032568497",
    "https://openalex.org/W1992262987",
    "https://openalex.org/W2491176990",
    "https://openalex.org/W4210764005",
    "https://openalex.org/W2082028827",
    "https://openalex.org/W2074599152",
    "https://openalex.org/W2341461363",
    "https://openalex.org/W1972491660",
    "https://openalex.org/W3014518745",
    "https://openalex.org/W3093720164",
    "https://openalex.org/W3204652402",
    "https://openalex.org/W2790179348",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3214206622",
    "https://openalex.org/W2141973273",
    "https://openalex.org/W2122229597",
    "https://openalex.org/W2934055000",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4389009518",
    "https://openalex.org/W3112188842",
    "https://openalex.org/W4319165686",
    "https://openalex.org/W3094393093",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2252053380",
    "https://openalex.org/W2980823180",
    "https://openalex.org/W4312985860",
    "https://openalex.org/W4312827348",
    "https://openalex.org/W4224903891",
    "https://openalex.org/W4200595320",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3116890199",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3115021520",
    "https://openalex.org/W3039772337",
    "https://openalex.org/W2788388592",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W3021582395",
    "https://openalex.org/W3207604732",
    "https://openalex.org/W3153046263",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W2791380498",
    "https://openalex.org/W3047520359",
    "https://openalex.org/W1908150177",
    "https://openalex.org/W4307938831",
    "https://openalex.org/W2131374070",
    "https://openalex.org/W2048168755",
    "https://openalex.org/W3142599250",
    "https://openalex.org/W3036971864",
    "https://openalex.org/W4389665836",
    "https://openalex.org/W4366597729",
    "https://openalex.org/W4323536841",
    "https://openalex.org/W4323871435",
    "https://openalex.org/W4323870446",
    "https://openalex.org/W3102393842",
    "https://openalex.org/W2169956378",
    "https://openalex.org/W3104462958",
    "https://openalex.org/W4226369973",
    "https://openalex.org/W3104405162",
    "https://openalex.org/W4387965833"
  ],
  "abstract": "<title>Abstract</title> This work aims to provide initial guidelines towards developing companion robots with large language models (LLMs) to be part of everyday lives of older adults. Using iterative participatory design (co-design) approaches, we analyze the challenges of applying LLMs for multi-modal open-domain dialogue, deriving from older adults' (one-to-one) interactions with a personalized companion robot, built on Furhat robot with GPT-3.5. An initial study with 6 Swedish-speaking older adults (65 and older) showed that the robot frequently interrupted the users, responded slowly and repetitively, engaged in superficial conversations, and caused a barrier in the interaction due to foreign language (English). Upon incremental technical developments to address these issues, participatory design workshops were conducted with 28 Swedish-speaking older adults. While the interactions (in Swedish) were smoother, less disrupted, and more varied in topics and responses, further challenges were observed due to hallucinations and obsolete information, and disengagement cues, causing frustration, confusion, and worry.",
  "full_text": "Between Reality and Delusion: Challenges of\nApplying Large Language Models to Companion\nRobots for Open-Domain Dialogues with Older\nAdults\nBahar Irfan \nRoyal Institute of Technology\nSanna-Mari Kuoppamäki \nRoyal Institute of Technology\nGabriel Skantze \nRoyal Institute of Technology\nResearch Article\nKeywords: Large language models, human-robot interaction, elderly care, open-domain dialogue, socially\nassistive robot, participatory design\nPosted Date: May 5th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-2884789/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: Competing interest reported. G.S. is co-a\u0000liated with Furhat Robotics, as its Co-\nfounder and Chief Scientist. The remaining authors have no relevant \u0000nancial or non-\u0000nancial interests\nto disclose.\nVersion of Record: A version of this preprint was published at Autonomous Robots on March 10th, 2025.\nSee the published version at https://doi.org/10.1007/s10514-025-10190-y.\nBetween Reality and Delusion: Challenges of\nApplying Large Language Models to Companion\nRobots for Open-Domain Dialogues with Older\nAdults\nBahar Irfan1*, Sanna-Mari Kuoppam¨ aki2 and Gabriel Skantze1\n1*Division of Speech, Music and Hearing, KTH Royal Institute of\nTechnology, Stockholm, 100 44, Sweden.\n2Division of Health Informatics and Logistics, KTH Royal Institute of\nTechnology, Stockholm, 100 44, Sweden.\n*Corresponding author(s). E-mail(s): birfan@kth.se;\nContributing authors: sannaku@kth.se; skantze@kth.se;\nAbstract\nThis work aims to provide initial guidelines towards developin g companion robots\nwith large language models (LLMs) to be part of everyday lives of older adults.\nUsing iterative participatory design (co-design) approaches, we analyze the chal-\nlenges of applying LLMs for multi-modal open-domain dialogu e, deriving from\nolder adults’ (one-to-one) interactions with a personalized companion robot,\nbuilt on Furhat robot with GPT-3.5. An initial study with 6 Swed ish-speaking\nolder adults (65 and older) showed that the robot frequently inte rrupted the\nusers, responded slowly and repetitively, engaged in superﬁcia l conversations,\nand caused a barrier in the interaction due to foreign language (En glish). Upon\nincremental technical developments to address these issues, participatory design\nworkshops were conducted with 28 Swedish-speaking older adul ts. While the\ninteractions (in Swedish) were smoother, less disrupted, and mo re varied in topics\nand responses, further challenges were observed due to hallucina tions and obsolete\ninformation, and disengagement cues, causing frustration, confu sion, and worry.\nKeywords: Large language models, human-robot interaction, elderly c are,\nopen-domain dialogue, socially assistive robot, particip atory design\n1\n1 Introduction\nWith more than 1 billion people over 60 worldwide\n1, there is a growing need for\ninnovative solutions that can improve the quality of life of older adul ts. In particular,\nloneliness in older adults is a risk factor that negatively inﬂuences mental and physical\nhealth, leading to depression, lower quality of life, decline in h ealth, and mortality\nrates [ 1, 2]. Companion robots are targeted to enhance the well-being, quality of\nlife, and independence of older adults, by providing service and c ompanionship and\nassisting (e.g., carrying out a variety of tasks) in everyday life [ 3, 4]. Their designed\nfunctionalities may include cognitive and social support, support f or mobility, health\nmonitoring, and care. Several studies have shown their beneﬁts in r educing social\nisolation and loneliness, thereby signiﬁcantly contributing to imp roving the quality of\nlife or well-being of older adults in several studies [ 3].\n‘Participatory design’ (or ‘co-design’) [ 5] approaches have been recognized as a\npowerful tool for developing technologies that meet the needs and pre ferences of end-\nusers. These approaches emphasize collaboration between designers an d end-users, as\nwell as iterative prototyping, interviews, and testing, to ensure t hat the ﬁnal product is\nusable, useful, and desirable for the target population. Participatory d esign approaches\ncan help ensure that companion robots are tailored to the needs and prefer ences of\nolder adults [ 6, 7], such that they can be eﬀective in facilitating social interaction and\nengagement in the user.\nIn order to integrate companion robots into the daily lives of older adults , they\nneed to be fully autonomous, and provide a natural way to interact with out the older\nadults having to learn how to use them. Hence, spoken dialogue in compan ion robots is\nintegral, and needs to be ﬂexible enough to adapt to unforeseen circum stances during\nthe conversation [ 8]. Large language models (LLMs) provide that ﬂexibility with their\nopen-domain dialogue capabilities, i.e., these models can converse on any topic, with\npre-trained models on vast amount of data [ 9]. However, they are typically trained\nand evaluated on textual data, whereas robots carry the additional complexit y of\nmulti-modal interactions. Moreover, LLMs are still far from perfect w ith signiﬁcant\nlimitations in their ability to generate coherent and factual response s in open-domain\ndialogue [ 10]. As a result, developing companion robots with LLMs for older adults\nrequires a careful balance between technical advancements and human -centered design\nprinciples.\nThis article aims to present initial guidelines towards developin g companion robots\nwith LLMs that can be a part of the everyday lives of older adults. The nov elty of\nthis work, thus, lies in the systematic (quantitative and qualitative) analysis of the\nchallenges of applying LLMs for multi-modal open-domain dialogue in the context\nof older adults’ interactions with a personalized companion robot, using iterative\nparticipatory design . Our approach derives insights into the technical development\nof companion robots through an initial study with 6 Swedish-speaking olde r adults\n(65 and older) and a subsequent series of participatory design workshops with 28\nSwedish-speaking older adults.\n1https://www.who.int/health-topics/ageing\n1Furhat Robotics: https://furhatrobotics.com/\n2\nFig. 1: Older adult interacting with the autonomous Furhat robot 1 with GPT-3.5 during\nthe participatory design workshop.\n2 Background\n2.1 Socially Assistive Robots in Elderly Care\nSocially assistive robots are designed to increase, maintain or improv e the functional\ncapabilities, activity, and participation of older adults or persons wit h a risk of disabil-\nity [\n11]. These robots are used for physical assistance such as increasing mobi lity [ 12],\ncognitive training such as reminding medication [ 13–15], and social companionship\n(companion robots ) in the prevention of loneliness and social isolation [ 16, 17]. Con-\nversational agents as a speciﬁc form of socially assistive robots are used t o enhance\nsocial connectedness among older adults due to their ability to commu nicate with\nhumans using natural language processing. Previous studies have deve loped conver-\nsational agents as companion robots in casual conversations [ 18, 19], virtual coaching\nbased on text-based and vocal interaction [ 20], and supporting the coordination and\nplanning work between older adults and their caregivers [ 21].\nCompanion robots hold a signiﬁcant potential to improve the well-being of older\nadults, since conversational user interfaces have been shown to re duce technology\nadoption barriers that older adults typically experience with comput ing devices [ 22].\nHowever, older adults also perceive unique challenges in interacti ng with conversa-\ntional agents, from choosing an appropriate linguistic style and content w ith the\nagent [23] to improving the accessibility that can become a barrier for older ad ults with\nhearing impairments [ 24]. Therefore, speech-based interaction should consider ques-\ntions of synthesis choices and conversation content in responding to t he experiences of\n3\nthis age group [ 23]. The linguistic content and default voices that conversational agents\nprovide when interacting with older adults should be designed appr opriately from the\nconversational user experience, and to support a more inclusive voic e interaction.\nPrevious studies have identiﬁed age-speciﬁc strategies for inter acting with conver-\nsational agents [ 25]. For older adults, companion robots can evoke feelings of inde-\npendence and empowerment [ 26]. Older adults tend to anthropomorphize the agent\nby using polite greetings when communicating with the agent, while y ounger adults\ntended to consider it as a tool by placing more importance on its conveni ence [ 27].\nOlder adults use voice assistants for speciﬁc purposes, such as seek ing online infor-\nmation. Age-speciﬁc challenges with interacting with conversational age nts include\nconcerns about the reliability and trust towards the agent, and unpred ictability,\nunclarity, and inconsistency of voice commands [ 22].\nGollasch and Weber [ 28] identiﬁed age-speciﬁc strategies in dialogue systems and\nspeech recognition accuracy. To respond to the needs of older adults, c onversational\nagents should be able to correctly recognize even unusual formulations; complex dia-\nlogues comprising multiple pieces of information should be presente d as simple or\nguided dialogues; agent should ask only one question per dialogue with a limi ted set of\npossible answers; it should be able to keep information about the conver sation context;\nand so on.\nParticipatory design (also known as co-design) [ 5] has been oﬀered as a solution to\novercome challenges in the design process of companion robots with older adults [ 6, 7].\nParticipatory design builds on participants’ self-identiﬁed issu es and concerns which\nare taken as a starting point for developing robotic applications. Partici pants are given\nthe possibility to interpret the capabilities of robotic systems an d discuss the potential\nsocial consequences and meanings of robots in daily life contexts [ 29]. This aims to\npromote older adults as designers, rather than only users of robotic techn ologies [ 6].\nHowever, participants may not have experience with robotic technol ogies, and they\nmay not see themselves as designers of any technology [ 30]. Therefore, participatory\ndesign requires a mutual trust and understanding of the everyday l ife conditions among\nolder adults, and a reﬂective approach towards designing companion robots with older\nadults [ 6].\n2.2 Large Language Models for Open-Domain Dialogue\nThe term ‘open-domain’ is often used to refer to dialogue systems that are more\nunrestricted when it comes to the topic of the conversation, compared to systems\nthat are targeted towards more speciﬁc domains, such as restaurant bookin g or lan-\nguage learning [\n31–33]. Whereas task-oriented dialog systems have traditionally been\ndesigned using a modular architecture (Natural Language Understanding, D ialogue\nManagement, Natural Language Generation), current open-domain chatbots are typ -\nically implemented in an end-to-end fashion using LLMs. These LLMs are t rained to\ndo next-token prediction on large amounts of text data and then used to pr edict the\nsystem response word-by-word, autoregressively [ 34]. Earlier LLMs for chatbots, such\nas the Meena chatbot [ 32] from Google, used social media conversations as training\ndata. The Blender chatbot from Facebook [ 35] was based on training data collected\nfrom Reddit. More recent chatbots, like LaMDA from Google [ 36], have been trained\n4\nusing larger, more general datasets (including both dialogue and other pub lic web doc-\numents). Similarly, GPT-3 [ 37] is a general-purpose language model that can be used\nas a chatbot in a ‘zero-shot’ fashion. To do this, the last few turns of th e dialogue are\nused, together with a description of how the agent should behave, as a ‘ prompt’ that\nthe model then makes its word-by-word predictions from.\nWhile general language models can be used directly as chatbots, their re sponses will\nreﬂect ordinary language use, which might not always align with the desir ed output in\nterms of, for example, truthfulness and toxicity (the so-called ‘al ignment-problem’).\nTo address this, [ 36] ﬁned-tuned LaMDA to optimize human ratings of safety and\nother qualitative metrics. A more sophisticated approach was taken by [ 38] with their\nmodel InstructGPT, which uses so-called ‘reinforcement learni ng from human feed-\nback’ (RLHF), where a model of human raters is used during reinforcem ent learning\nto optimize the model towards the desired criteria. The RLHF approach was also used\nwhen training the chatbot ChatGPT 2 (OpenAI).\nThe term ‘open-domain’ has been questioned by [ 39], since it is not clear what\nthe boundaries of this ‘openness’ are, and whether systems that are de scribed in that\nway are truly open to all the diﬀerent forms of dialogue that humans engage in . This\nmight involve persuasion, asking for favors, small talk, recapping eve nts, making plans,\netc. [ 40]. However, when analyzing the Google Meena chatlogs, [ 39] found that most\ninteractions were simply small talk and they did not exhibit this var iety of topics. One\nexplanation for this is the lack of common ground [ 41] in human-machine interaction,\nwhich naturally restricts the user’s expectations of what is appropr iate and meaningful\nto talk about. Often, when open-domain chatbots are evaluated, the user i s simply\ninstructed to “chat with the system”, without any further context [ 32, 36]. Thus,\nthe setting of the interaction, and the user’s expectations of that se tting, are very\nimportant for how the dialogue will unfold and ultimately whether it wi ll be perceived\nas meaningful by the user.\n2.3 Large Language Models in Social Robots\nLLMs have recently also been applied to social robots in human-robot int eraction\n(HRI) [\n19, 42–45]. One example of this is the collaborative story-telling game Cre-\nativeBot [ 42], where a child and a robot take turns creating a story. An LLM (GPT-3)\nwas ﬁnetuned on a dataset of creative stories, and used to generate the rob ot’s\ncontributions to the story.\nThe most similar work to ours is [ 19], which used a ﬁne-tuned GPT-3 with the\npersonalized QT robot for open-domain dialogue with (12) older adults. The us er\nperceptions and feedback are reported, in which one of the participant s noted that\nrobot responses were slow. However, a systematic analysis of interact ion challenges was\nnot reported. In addition, emotion recognition was used, which can create confounds\nin understanding the challenges of LLMs.\nAs previously discussed, one limitation with chatbots is the lack of ph ysical embod-\niment or physical situation from which common ground could be inferred. Common\nground can to some extent be inferred from cues like our physical appear ance (e.g.,\nage or how we dress) and the language/dialect we use. Thus, a social robot migh t help\n2https://openai.com/blog/chatgpt/\n5\nthe user to infer more about their potential common ground [ 46, 47]. In the case of a\nrobot situated in a physical environment, there should be even more c ontextual cues.\nStudies have also shown that people use the robot’s presumed origin [ 48] or gender [ 49]\nto infer the robot’s knowledge (and thereby their common ground).\n3 Integrating a Large Language Model with a Social\nRobot\nAs described in the previous sections, LLMs enable open-domain dialogues , but have\nbeen rarely explored outside of single-session interactions with sh ort durations, nor\nwith older adults, hence, it is challenging to design a personalized companion robot\nwith them without knowing their limitations in prolonged conversati ons and the\nrequirements for the target population. Correspondingly, we kept the integration incre-\nmental (i.e., with small changes) to identify their drawbacks wit hout any confounds.\nThe resulting design choices for a fully autonomous personalized compan ion robot for\nadults are described in this section.\n3.1 Robot\nIn conversations, the gaze is an important element that signals the addres see of the\nattention, helps coordinate turn-taking in conversation, helps dis ambiguate references\nto objects, and establishes joint attention [\n50, 51]. Another important aspect is the\nappearance of the agent interacted with. Since large language models can soun d\nhuman-like in conversation, it is important to project that aspect to the agent’s\nappearance to maintain the naturalness in conversation and create believ able agents.\nHowever, it is important to avoid the Uncanny Valley [ 52] eﬀect with human-likeness,\nwhich refers to the theory that the likeability will increase wit h anthropomorphism\n(human-likeness of the agent) until a point where there is imperfe ct resemblance (a\nmismatch of appearance or capabilities), which will cause a sudden drop until the\nagent looks (and acts) exactly like a human for it to rise again. Furhat robot ( Furhat\nRobotics)1 [53] satisﬁes these conditions, especially over repeated interaction s [ 54],\nand is perceived to be more human-like than other common robot platforms s uch as\nNAO and Pepper (Aldebaran Robotics) 3 [55]. Thus, the Furhat robot was chosen as\nthe companion robot in this work, as shown in Fig. 1.\nFurhat is a social robot, with a back-projected face, which allows it to display a\nrange of facial expressions (e.g., smiles, frowns) and movements (e. g., blinks, eyebrow\nraises, gaze), lip sync to speech, and perform head movements (e.g., nods, head shakes).\nIt has two in-built microphones and dual speakers. It oﬀers the possib ility to use\nGoogle Cloud Speech-to-Text 4 or Microsoft Azure Speech-to-Text 5 engines for speech\nrecognition. It has a 1080p RGB 120 °diagonal ﬁeld-of-view camera. The robot has face\ndetection and tracking that allows both gaze and head orientation to follow t he user’s\nlocation.\n3https://www.aldebaran.com/en\n4https://cloud.google.com/speech-to-text/\n5https://azure.microsoft.com/en-us/products/cognitive-services/speech-to-text/\n6\n3.1.1 Face\nIn this work, the FaceCore engine and the latest SDK (2.4.0 for the ﬁrst s tudy, 2.5.0\nfor the second study) in Kotlin are used. A neutral-looking face is ch osen (‘Alex’).\nThe face engine of the robot creates random smiles and eyebrow raises whi le talking\nand listening to the user. This behavior is intended to improve the naturalness of the\nconversation and give a non-verbal backchannelling to the user. Howeve r, this happens\nwithout context, since speech is not analyzed until after the user s tops speaking.\nHence, it may be inappropriate in some contexts. For improving the nat uralness of\nthe interaction, the robot blinks, shifts its eyes, and looks away ( gaze aversion) brieﬂy\nwhile talking based on the silence in the input speech to avoid star ing at the user,\nrandomly alternating between looking down or up to the left or right. F ace detection\nand tracking (default engines) are used to give the illusion of agency and awareness\nin the robot.\n3.1.2 Speech\nEnglish was used as the language for speech recognition and synthesis, sin ce the large\nlanguage models have larger training data in English, thus, expected t o be more capa-\nble of conversing in English. The ‘Matthew-Neural’ voice in Amazon Polly within\nFurhat was used for text-to-speech (TTS), as a natural-sounding male v oice.\nA USB microphone array (Seeed Studio)\n6 is used to obtain higher-quality audio\ndata for speech recognition, as the robot’s fans can interfere with the au dio record-\ning. Google Cloud Speech-to-Text is used for speech recognition from th e audio\nobtained from the microphone during ‘listening’, which is determi ned by the following\nparameters of the Furhat robot:\n1. no speech timeout : the duration of silence before the robot deems that the user\ndid not respond. This is increased to 8 seconds (as suggested in the do cumenta-\ntion\n7), instead of the default 5 seconds. The robot restarts listening afte r this period\nhas passed.\n2. silence timeout : the maximum duration after the user stopped speaking before\nspeech recognition is triggered. This was set to 1.2 seconds. The defau lt value in\nFurhat is 0.8 seconds, however, this was increased empirically to ac count for the\npauses in older adult speech.\n3. maximum speech timeout : the maximum length of the user’s utterance before\nan interruption from the robot. This is increased to 30 seconds (the de fault of 15\nseconds, but this value is suggested in the documentation) to preve nt frequently\ninterrupting the user when speaking, as older adults may not be accus tomed\nto interact with spoken dialogue systems that typically require shor t and clear\nsentences.\n6https://wiki.seeedstudio.com/ReSpeaker-USB-Mic-Arra y/\n7https://docs.furhat.io/listening/#listening\n7\n3.2 Large Language Model\nThe development of the personalized companion robot started in July 2022, pr ior to the\nrelease of ChatGPT\n8 (OpenAI) and the surge of the new LLMs that followed (in 2023).\nWhen the design choices were made, the only publicly available mode ls suitable for\nopen-domain dialogue were GPT-3.5 Mar 15 (OpenAI, text-davinci-002 mode l) [ 37],\nBLOOM (BigScience Workshop) [ 56], and BlenderBot 2.0 (Meta) [ 57, 58]. BlenderBot\nand BLOOM both require large computing power and graphics for achieving f ast\ninference, thus, taking away from the portability of the robot (e.g., for using it in\nelderly houses or senior care centers). Hence, GPT-3.5 was used as the LLM, with\ntext-davinci-0029 for the initial study and text-davinci-003 10 for the second study.\nBoth davinci models have training data up to June 2021, and allow up to 4097 t okens\nin a prompt. The hyperparameters of the models used in the study are provided in\nTable A1 in Appendix A. GPT-4 was announced 11after both studies, and the API is\nnot yet publicly available as of the time of writing of this article.\nWhile ChatGPT (or gpt-3.5-turbo models 12 in the API) is faster in inference than\ndavinci models and includes slightly more recent training data (u p to September 2021),\nthe instructional safety ﬁlters (guardrails) from OpenAI to decrease the anthropo-\nmorphism of the model resulted in responses typically starting wi th “As an artiﬁcial\nintelligence, I do not have preferences/I do not have feelings”, w hich would take away\nfrom the desired naturalness of the interaction with a companion robot. Not e that our\nintention is not to deceive the users that the robot is a human, in fact , the prompt\nused clearly states that it is a robot (in Section 3.2.1), and responses are generated\naccordingly. However, anthropomorphism and small talk can lead to higher acc eptance\nand trust in the robot [ 59–61]. Hence, text-davinci-003 was used for the second study,\nwhich does not have these safety ﬁlters in place. To diﬀerentiate gpt-3.5-turbo models\nfrom davinci models, this article refers to the former as ChatGPT an d the latter as\nGPT-3.5.\nGPT-3.5 integration to Kotlin for the Furhat robot was adapted from the\nOpenAIChat13 skill by Furhat Robotics, which uses the OpenAI-Java library 14.\n3.2.1 Agent Model\nThe persona of the personalized companion robot (Prompt\n1) was based on the work\nby Zhang et al. [ 62] and the follow-up work by Xu et al. [ 58]. In [ 62], the crowdworkers\nwere asked to chitchat with another worker for one or multiple sessions based on\ngiven personas by instructing them to “chat with the other person natu rally and try\nto get to know each other”, “both ask questions and answer questions of your c hat\npartner”, and “chitchat with another worker for 6 turns, as if you were catc hing up\nsince last time you two spoke.” and that “when you expand the topic, make s ure it\nmakes sense with the personal details already mentioned.” in the latt er. We adapted\n8Released on November 30, 2022: https://openai.com/blog/chatgpt\n9Released on March 15, 2022\n10Released on November 28, 2022\n11Announced on March 13, 2023\n12Released on March 7, 2023\n13https://github.com/FurhatRobotics/example-skills/tree/master/OpenAIChat\n14https://github.com/TheoKanning/openai-java\n8\nthese instructions for a personalized companion robot. Since a male voic e and face\nwas used for the robot, ‘he’ was used in the persona prompt to avoid a mis match in\ngenerated responses with the robot features. The robot’s name was kept as Furhat.\nFurhat is a personalized companion robot. Furhat tries to get to kn ow more about\nhis conversation partner, their interests and activities. When he expands on a topic of\nconversation, he uses personal details already mentioned to pe rsonalize the conversation.\nPrompt 1: GPT-3.5 prompt for agent persona.\nAgent’s and user’s utterances in the interaction are stored in the DIA-\nLOGUE HISTORY variable. To develop and maintain a consistent persona of the\nagent through time with multiple users, the facts that the agent says ab out itself dur-\ning the conversation are extracted to be stored for future interaction s with users. The\nfacts learned from each conversation are concatenated, and stored in the agen t ﬁle, in\naddition to the number of known users and persona of the agent (Prompt 1). The facts\nlearned at the end of an interaction for the agent and the user were extract ed from\nDIALOGUE HISTORY using a prompt: “Summarize what we know about NAME”.\n3.2.2 User Model\nIn order to create a personalized companion robot that remembers its pre vious con-\nversations with users to personalize subsequent interactions, th e information about\nthe users obtained from the conversations is extracted through GPT-3.5 and stored\nas JSON ﬁles on the robot. Each user is assigned a manual ID (by the experi menter)\nwhich is used as their identiﬁer (and in the ﬁle name) to start the i nteraction manually.\nAn index ﬁle stores all the user IDs known, for easier access to the ﬁl es.\nThe name of the user is extracted at the end of the interaction from the d ialogue\nhistory with a prompt. Per interaction, the extracted information fr om the dialogue\n(i.e., the user’s name and facts learned about the user and the agent), t he date and\ntime of the interaction, the dialogue duration, the number of turns, and gender and\nemotions detected by Furhat (for performance analysis) are stored in t he user ﬁle.\n3.2.3 Response Generation\nIn order to ensure a consistent interaction with the robot between u sers, the initial\nand end phrases of the robot were manually written (instead of generated) , however,\nthe rest of the agent utterances were generated by GPT-3.5. If it is the ﬁrst interaction\nwith the user, the robot says “Hello! I am Furhat, the personalized compan ion robot.\nWhat is your name?” to obtain the name of the user. If the user is known (i.e., has been\ninteracted with before, thus has a user model ﬁle), the robot says “Hi USER\nNAME!\nNice to see you again. What have you been up to since the last time?” to “cat ch up”\nwith the activities the user has done in between the interactions w ith the robot.\nThe prompt to generate the robot utterances using GPT-3.5 was correspon dingly:\nPrompt 1\nFurhat is AGENT FACTS\nFurhat knows that USER NAME is USER FACTS\nUSER NAME knows that Furhat is USER FACTS\n9\nThe following is a/second/third conversation between the perso n/USER NAME and\nFurhat.\nDIALOGUE\nHISTORY\nFurhat:\nPrompt 2: GPT-3.5 prompt for response generation.\nThe AGENT FACTS are used if the robot had at least one interaction with a user.\nUSER FACTS and USER NAME are used if the user had at least one interaction\nwith the robot.\nIf GPT-3.5 generates an empty response or a service failure occurs, a clariﬁcation\nrequest is made to obtain the participant’s response again, by choosing a random\nphrase from: “I am sorry. I didn’t understand you.”, “Could you repeat that p lease?”,\n“Not sure if I understood you.”, “Could you rephrase that please?”, “Sorry, I didn’t\nhear you clearly.”.\nIn order to ensure that the robot expresses leave-taking at the end of the interaction\ninstead of trying to continue the conversation, a random response was ch osen from\nthe list of leave-taking expressions (i.e., “Bye! Hope to see you again s oon.”, “See you\nsoon!”, “Take care until next time.”, “Looking forward to seeing you again soon !”) as\nthe ﬁnal response of the robot.\n3.3 Interaction Flow\nThe interaction manually starts (by the experimenter) by enterin g the user ID. The\nuser and agent ﬁles are parsed to activate the current user and agent mode ls and set\nthe prompts. When the user is in the engagement zone, the robot says th e greeting\n(either generic or personalized), and it starts listening to the us er. After receiving the\nresponse from the user, the audio is transcribed by speech recognit ion. Afterward,\nthe prompts and the dialogue history (including the newly transcrib ed utterance) are\nsent to GPT-3.5, as described above. The generated response is said by the robot,\nafter which the robot starts listening to the user again. The robot does not listen to\nthe user, while speaking or generating a response. The interaction e nds when the user\nexpresses leave-taking (e.g., “Goodbye!”, “See you later.”), and the robot responds\nwith a random response from its list of leave-taking expressions. Th e user and agent\nmodels are saved to ﬁles, and the robot goes to the idle stage. The ﬁrst interaction\nwith the robot can be seen in the video excerpt from the ﬁrst study\n15.\n4 Preliminary Interviews\nWe investigated the challenges of applying LLMs to companion robots for open- domain\ndialogue with older adults in two separate phases. In the ﬁrst phase, pr eliminary\ninterviews were conducted with 6 older adults based on their open-d omain dialogue\nwith the robot, which lasted an hour each. The interviews took place in September\nand October 2022, at KTH Digital Futures premises. Each participant was int erviewed\nand interacted with the robot individually. The interactions with t he robot were in\nEnglish, but the interviews were made in Swedish.\n15https://youtu.be/rkuoOfFuvRU\n10\n4.1 Procedure\nEach participant was ﬁrst asked about their expectations towards compani onship\nrobots, their prior experience with robots, and their living condit ions (i.e., alone, with\na partner or family member, or in senior housing). This was followed by a 5-minute\ndemonstration of the robot’s capabilities for autonomous dialogue by the expe rimenter.\nAfter the demonstration, the participants were instructed that the y can talk\nabout anything they want with the robot, that the robot would start the interac-\ntion, and they can end the conversation whenever they want , by saying “Goodbye”.\nThe participants were also told that the robot would not hear them, when i t is\nspeaking. The overall interaction with the robot lasted 4 to 13 minutes (M = 9 .24,\nSD = 3 .45). The experimenters were present throughout the robot interaction s,\nbut did not interfere with the participant’s interaction with the robot, unless the\nparticipant asked for help.\nAfter the interaction with the robot, a semi-structured intervie w was conducted\nwith the participants, in which they were asked about their experi ences of having a\nsocial conversation with the robot based on the robot acceptance model of so cially\nassistive agents by older adults [\n63]. A range of open-ended questions was covered,\nfocusing on the perceived usefulness, usability, user satisfac tion, trust, social pres-\nence and adaptability of the robot, combined with questions about ethic al concerns\nof using robots for social and emotional support, personalization aspects f or future\napplications, and how their expectations compared to their interact ion.\nAll individual interactions with the robot were video-recorded throu gh an exter-\nnal camera, and interviews with the participants were audio-recorded . All participants\ngave informed consent for the study, which included options for conse nting to\nanonymized (i.e., blurred face, and without full name released) image and/or video\nsharing in publications.\n4.2 Participants\nFor the preliminary interviews, 6 Swedish-speaking older adults (3 female, 3 male) aged\n65 and over were recruited by distributing the invitation on social media, through KTH\ncommunication channels, and sending an email invitation to a group of older adults\nwho had previously attended another experiment at KTH with a social r obot [\n64].\nConsequently, 2 of the 6 participants had previous experience in in teracting with the\nrobot in the previous experiment, but 4 of them had no previous exper ience. It is\nimportant to note that those two participants may had higher expectation s towards\nthe robot than those who had no experience with robots. The participant s were not\noﬀered any type of compensation.\n4.3 Data Analysis\nA total of 55 minutes of video data for the participants’ interactions wit h the robot\nwere recorded. The pre- and post-interaction recorded audio interv iews were in total\n3 hours and 9 minutes long. Both the audio and video data were transcribe d. The\ndata were qualitatively analyzed, using a combination of conversation anal ysis [\n65]\nto detect disruptions in the conversation due to poor task performanc e, and content\n11\nanalysis [ 66] for categorizing topics in the conversation with the robot and feedback\nin the interviews.\n4.3.1 Transcriptions\nWhisper\n16 (OpenAI) [ 67] was used on the videos from external cameras for English\ntranscriptions. Large-v2 model (with default parameters) was used, since it achieves\nthe best overall performance for English [ 67]. The transcriptions and their timings\nwere corrected manually.\nInterviews were manually transcribed using audio recordings by a r esearch assis-\ntant, who is a native Swedish speaker, for obtaining more accurate resp onses in a\nmulti-speaker setting (three experimenters and the participan t).\n4.3.2 Qualitative Analysis\nThe video and audio data were manually coded. Deductive coding (i. e., coding the\ndata based on a predeﬁned set of codes) was applied to detect the disr uptions in\nthe dialogue, based on conversation analysis to identify turn-taking er rors and speech\nrecognition errors. Inductive coding (i.e., coding derived from the data) was applied\nto identify additional causes for dialogue disruptions. Content analysi s was used to\nderive conversation topics and categorize feedback in the interviews . A hybrid coding\nstrategy was taken to use frequency coding (i.e., counting the num ber of times a code\noccurs) for dialogue disruptions, descriptive coding (i.e., sin gle word coding) for topics,\nprocess coding (i.e., noting actions) for out of ordinary responses and reactions in\nthe context of the conversation, and in vivo coding (i.e., quoting th e participants) for\nhighlighting the participants’ opinions in the interviews or utter ances in the dialogue\nin particular cases.\nTo evaluate the open-domain nature of the conversations with the robot, th e topics\ndiscussed were categorized in the speech events using the taxonomy by Goldsmith and\nBaxter [\n40] and analyzed under the categories deﬁned by Do˘ gru¨ oz and Skantze [ 39]\n(see [ 39] for a detailed description of speech events and categories).\nIn this article, the feedback from the interviews is only used for s upporting the\nconclusions regarding the technical challenges of applying large language m odels to\ncompanion robots in open-domain dialogue. The detailed analysis of that data i s\noutside the scope of this article.\n4.4 Technical Challenges\nBased on the analysis, four main categories of challenges were identiﬁed t hat caused\ndisruptions in the interaction and negative user experiences: fre quent interruptions and\nslow responses, repetitive responses, superﬁcial conversations , and language barrier.\n4.4.1 Turn-taking: Frequent Interruptions and Slow Responses\nThe participants were interrupted quite frequently by the robot (M = 14 .00, SD =\n6.26), as can be observed in the video\n17. These interruptions resulted in negative\n16https://github.com/openai/whisper\n17https://youtu.be/2H5ufXVXKl4\n12\nreactions, such as frowns, irritated responses, foreign language anxiet y, and the need\nto rush in speaking. There were three main reasons behind the inter ruptions:\n1. The participants were not aware when the robot stopped listening to them, since\nthere were no indicators for turn-taking in the robot. There were pau ses in the par-\nticipants’ utterances that were longer than the “silence timeout”, wh ich triggered\nthe robot to start generating a response, while the person continued s peaking with\nthe reasons being (a) the long thought process within speech for an une xpected\nquestion with spread-out lexical ﬁllers (e.g., “My favorite movie i s... hmm...”), (b)\nspeaking in a foreign language (e.g., trying to come up with the right tran slation),\nor (c) pauses and hesitancy in older adults’ speech [ 68].\n2. The participants were not aware that the robot was generating a response . There\nwere long silences in robot speech due to the long response generation time (2-3\nseconds) from GPT-3.5, which made them unsure whether to continue s peaking\nand urge them to maintain the ﬂow of conversation by continuing [ 69, 70]. Two\nparticipants commented in the interviews, e.g., P4 : “response time was too long,\nso it felt strange”.\n3. The participants gave longer responses than the “maximum speech time out”, either\ndue to word repetition or the slow speaking rate of older adults [ 68]. One of the\nparticipants noted that they might have too long phrases in the inter views.\nIn rare occasions ( M = 1 .83, SD = 1 .94), overlaps happened when the participants\ntalked over the robot (1) by backchannelling (e.g., “hmm hmm”, “yes”), (2) when\nthey tried to ﬁnish or repair their utterance after the robot interr upted them, or (3)\nwhen they started speaking again while the robot was generating a respons e. In the\nthird case, the participants stopped speaking when the robot started speaking.\nThe errors in turn-taking also resulted in speech recognition error s since only part\nof the speech was processed, thus resulting in incorrect or out-of- context generated\nresponses from GPT-3.5.\n4.4.2 Repetitive Responses\nThere were a lot of repetitive utterances from the robot that were eit her due to (1) an\nempty response or a connection problem with GPT-3.5 ( M = 9 .50, SD = 6 .50) up to\n18 times in an interaction ( P6 ), which triggered the clariﬁcation request as described\nin Section\n3.2.3, or (2) the same exact response being generated from GPT-3.5 ( M =\n12.00, SD = 17 .92) which occurred up to 47 times in an interaction ( P5 ), such as\nrepeating a previous response (e.g., “I love nature too” in the video 18) consecutively,\nasking the user to talk more about the topic (e.g., “That’s really intere sting. Can\nyou tell me more about it?”), or thanking the user (e.g., “Thanks for sharin g that\nwith me.”). It is important to note that some of the empty response error s were due\nto speech recognition errors ( M = 4 .67, SD = 3 .78) and interruptions ( M = 2 .00,\nSD = 3 .52). It is important to clarify that these repeated responses were not d ue to\na bug in the code, it was all generated by GPT-3.5.\nIn one of the interactions (with P5 ), the same response (“That is really great. I\nlove swimming in the sea too. What is your favorite thing about swimm ing in the\n18https://youtu.be/2hirZc0kTEI\n13\n532 2 2 2\n1 1 1\n1 1 1\n1 1\n1 1\n1\n1 1\n1\n1\n1\nGetting to know\nsomeone\nSmall talk\nCurrent events talk\nSerious conversation\n0 5 10 15 20\nOccupation\nLanguage\nTechnology\nFood\nTravel\nHobbies and Interests\nHealth\nWeather\nNews\nPolitics\nCount\nParticipant P1 P2 P3 P4 P5 P6\nInformal/\nSuperficial\nTalk\nInvolving\nTalk\nFig. 2: Topics participants discussed with the robot during the prelimi nary interviews.\nsea?”) was generated by GPT-3.5 consecutively 12 times, after which th e experimenter\ninterfered to restart the robot. After restarting, the same set of que stions was asked\nto the participant in the same order and exact phrasing twice (i.e., 6 questions on\ndiﬀerent topics with the same follow-ups from the robot, and repeated after in the\nsame order at the end of 6 questions). The participant either rephrase d their response\nor changed them when responding on all occasions, and did not ask for help fr om the\nexperimenter.\nIn addition to the repeated phrases, the template of the utterance was quite often\nthe same as well (e.g., “I love to ... too”, “I like to ... too”, “That sounds l ike a ...”),\noften mirroring the user’s preferences and likes as its own, rarel y expressing a diﬀerent\npreference or idea, like a ‘stochastic parrot’ [ 71].\nThese utterances often generated frustration expressed through gest ures, facial\nexpressions, and responses, caused participants to change their res ponse or topic just\nto get the conversation ﬂowing, and led two of the participants to ask for h elp from\nthe experimenters, with one of them asking to stop the interaction af ter 4 minutes\n(P2 ) and requesting the experimenter to continue talking for them. T he same exact\ngenerated responses also led to amusement on one occasion. 3 participant s noted in\nthe interviews that the responses were repetitive and should be v aried.\n4.4.3 Superﬁcial Conversations\nThe topics discussed are displayed in Fig.\n2. The conversations mainly focused on the\nhobbies and interests (50%) (e.g., literature, movies, music, spor ts activities, outdoor\nactivities) as a form of ‘informal/superﬁcial talk’. In fact, in all but on e case, all par-\nticipants had a superﬁcial talk with the robot. The robot took an active rol e in the\nconversations, thus, initiating most of the topics. Considering thi s was the ﬁrst inter-\naction with the robot and the robot persona (Section 3.2.1) was to “get to know more\nabout his conversation partner, their interests and activities”, thi s was anticipated.\n14\nHowever, this led 4 (out of 6) participants to perceive the conversati on as superﬁ-\ncial, topics and the knowledge of the robot very limited, and the robot to be simple,\nsingle-minded, and self-reserved about its own hobbies and intere sts, such as:\n• P1: “He doesn’t want to talk about his own book, ‘I don’t have a favorite book’ , ‘I\ndon’t have a favorite movie’. And then the conversation gets weird.”\n• P6: “I would be very pissed oﬀ to have such an idiot robot at home. I mean if you’r e\ngoing to have an interesting conversation about a Nobel Prize winner in l iterature, it\nhas to be someone who has read a few thousand books and has something intere sting\nto say.”\nThe topics were mostly abruptly changed by the robot without a connecti on\nbetween topics, and on one occasion ( P5 as described before) changed 5 times with\nonly one or two follow-up questions. This could have led the robot to be perceived as\ninterrogative, rather than a nice conversational partner. Nonetheless, 4 participants\nasked the robot questions about itself and anthropomorphized the robot, su ch as:\n– P1: “Are you healthy?”\n– P4: “So where do you go when you go out in nature?”\n– P6: “And what kind of books do you read? Do you have a favorite?”\nDespite the similarity of the topics discussed, the user experi ences varied based\non the interaction disﬂuencies, the generated text from the GPT-3.5 based on the\nresponses of the participants, and the adaptation of the robot persona over time\nwith interaction with users. In addition, the interaction style of t he robot (e.g., inter-\nrogative, interested, helpful) changed between users, which caus ed inconsistencies in\ninteractions. The topics discussed with previous users were aske d to new users, because\nthe robot persona contained these learned facts. However, this might p rove to be a\nsecurity and privacy issue, which 3 participants were concerned ab out.\n4.4.4 Language Barrier\nDisruptions in the interaction due to speech recognition failures w ere not very common\n(M = 3 .67, SD = 3 .27), and most were due to Swedish names or phrases (e.g., par-\nticipant’s name, book, author) in the dialogue, as can be seen from a video excerpt\n19.\nBut combined with other disﬂuencies described above, it result ed in negative user\nexperiences for all participants. 4 participants emphasized the ne ed to have the con-\nversations in Swedish with the robot, in the interviews. One of the participants also\nnoted that the robot spoke very fast.\nHowever, a particular use case for having the robot speak English with f oreign\nolder adults is to practice English with the robot, which came up natur ally in one of\nthe interactions, as can be seen in the video 20. Another participant also noted in the\ninterviews that older adults would be very interested in such an app lication.\n19https://youtu.be/rjDipaS0bis\n20https://youtu.be/sWL8uGK4u0Q\n15\n4.5 Technical Improvements\nTechnical improvements were made to the robot based on the challenges identiﬁed.\nHowever, to prevent confounds in the evaluation of LLMs in companion robots , no\nadditional library (e.g., for turn-taking, speech recognition, or LLM) was u sed.\n4.5.1 Turn-taking\nThe ‘silence timeout’ was increased by 50% (to 1.8 seconds) to account f or the long\npauses in speech. Since the response time of GPT-3.5 is very long, th e added duration\nfor silence would create further disruption in interaction (‘awkw ard silence’), especially\nif the users restart speaking when the robot starts generating a resp onse, hence, this\nvalue was not increased substantially.\nIn order to clarify when the robot is ‘listening’ to the user, the LED underneath\nthe Furhat robot was used. The LED turned red when the robot stopped ‘li stening’\n(i.e., right before the recorded audio is sent to speech recognition ), and turned oﬀ\nwhen it was (after the robot ﬁnished saying the utterance generated b y the GPT-3.5\nresponse).\nGazing away is a powerful indicator to improve turn-taking in HRI [\n51], hence,\ngaze aversion (i.e., looking either top/bottom left/right) during res ponse generation\nwas implemented on the robot to demonstrate that the robot is ‘thinking’ . The robot\nreturned its gaze at the user when the response was generated to talk and maintain\neye contact with the user.\n4.5.2 Repetitive Responses\nIn order to decrease the number of empty responses or connection failu res due to GPT-\n3.5, the request was changed to be sent three times. In the case that th e response was\nstill not ﬁlled, instead of only asking clariﬁcation requests, which were frustrating to\nthe participants, backchannelling responses (i.e., “I see!”, “Hmm hm m.”, “Right.”)\nand invitation for elaboration (“Could you tell me more about that?”) were added to\npush the conversation forward.\n‘Frequency penalty’ was not changed to evaluate whether improvemen t of the other\nfactors in the robot would overcome the repeated responses.\n4.5.3 Superﬁcial Conversations\nIn order to establish deeper conversations with participants, the p ersona of the robot\nwas changed based on the feedback in the interviews, such as “The robot ne eds to ask\nquestions about feelings and health.”, “The robot needs to talk more about it self.”. In\naddition, an anthropologist, who had ongoing ethnographic research for puttin g robots\nin senior care centers, was consulted (e.g., “Senior care staﬀ often talk to senior citizens\nabout their families, memories, and emotions.”, “The robot needs to be em pathetic.”).\nThe resulting ‘empathetic’ persona is given in Prompt\n3). Instead of Furhat, a Swedish\nname (Leo or Linda) was used for improving both speech recognition (Furhat is often\ntranscribed as “for hat”) and the believability of the robot persona. Based on the\nresults of the design activity in the participatory design workshops, brieﬂy described\n16\nin Section 5.1, Linda was the ﬁnal name with Fig. 1 as the robot’s appearance (at\neach workshop, the participants preferred the same face, voice, and nam e).\nLinda is a personalized empathetic friendly companion robot for e lderly people. She talks\nabout people’s lives, interests, experiences, emotions, rela tionship with others, and reﬂects\non them. She values people’s opinions, recognizes their feeli ngs, and provides social and\nemotional support to the people. She also talks about her own e xperiences to reﬂect on\nsituations as a friend. She is an active listener, and understand ing. She asks open questions.\nShe wants to talk about people’s memories and family members. Sh e tries to create positive\nemotions in the person. When she expands on a topic of conversat ion, she uses personal\ndetails already mentioned to personalize the conversation.\nPrompt 3: GPT-3.5 prompt for empathetic robot persona.\nIn addition, the location, date and time of the interaction was added in th e prompt\nto provide more accurate responses. For personalized interactions, t he previous date\nand time of the interaction were also provided. However, the learnin g of persona over\ntime was removed to ensure a consistent interaction between user s.\nPrompt 3\nTime at the start of this conversation is DATE TIME.\nUSER NAME and Linda are located in Stockholm, Sweden.\nLast time Linda and USER NAME spoke was LAST DATE TIME.\nThe following is a/second/third conversation between the perso n/USER NAME and\nLinda.\nDIALOGUE\nHISTORY\nLinda:\nPrompt 4: Updated GPT-3.5 prompt for response generation.\nThe LLM in the robot was updated to GPT-3.5 text-davinci-003 model, base d on\nits “higher quality writing with clearer, more engaging, and more compell ing content”.\n4.5.4 Language Barrier\nSince most of the participants (4) noted that the robot needs to speak in S wedish,\nthe prompts were translated to Swedish for GPT-3.5, which in respons e, generates\nSwedish utterances. The language for Google Cloud Speech-to-Text was se t to Swedish\nas well. In addition, a Swedish TTS was used. Based on the feedback from one of the\nparticipants, the speaking rate was decreased to 80% of the TTS, since ol der adults\nspeak 20 to 25% slower than young adult speakers [\n68].\n4.5.5 Other\nWith the aim of conducting participatory design workshops with older adu lts, a wizard\ninterface was created to ensure that experimenters can simultaneou sly start multiple\nrobots to interact with users without changing any code. To ensure th at each par-\nticipant had suﬃcient and equivalent time to interact with the robot , without being\nconcerned about when they should end it, a (7-minute) timer was added . When the\nrobot stops listening, before a response was requested from GPT-3.5, th e timer was\n17\nFig. 3: Participatory design workshop with older adults (6-8 participa nts per workshop).\nchecked. If the dialogue duration is equal or greater, the robot would end t he inter-\naction with a pre-scripted phrase. Hence, the overall conversation d uration may be\nslightly longer than the set timer. The robot would then listen to the response from\nthe user, before saving the interaction ﬁles. In case the particip ant wanted to end\nthe interaction prior to the timer, an option was added in the wizard int erface to\nend the interaction with the same phrase to ensure consistency and p revent the robot\nfrom continuing the conversation. For malfunctioning of the robot, anothe r option was\nadded in the wizard interface to abort the interaction, which saved t he current inter-\naction ﬁles, and notiﬁed the user about the error in the system. When t he interaction\nis restarted with the same user from the wizard interface, the save d ﬁles were used to\ncontinue the conversation from where it was left oﬀ.\nA recorder for the robot’s camera and microphone, and a logger for events and\nfacial landmarks and emotions detected by the robot were added for perform ance and\ndata analysis.\n5 Participatory Design Workshops\nAfter the technical improvements in the robot’s abilities to tackl e the challenges faced\nin the preliminary interviews, we moved on to the second phase in t he co-design process\nof a personalized companion robot, through participatory design workshops. 28 older\nadults aged 65 and over participated in the workshops. A total of 4 workshops w ere\nconducted, which lasted two hours each. They took place on March 6 and Mar ch 8,\n2023, at KTH Digital Futures premises. The workshops and the robot interact ions\nwere conducted in Swedish.\n5.1 Procedure\nThe participatory design workshop consisted of three stages: 1) design sc enarios and\nfocus group discussions based on older adults’ expectations towards com panion robots\n18\n(n=4 groups, Fig. 3), 2) individual interactions with the robot in open-domain dia-\nlogue, followed by a robot acceptability questionnaire (n=28 interacti ons, Fig. 1), and\n3) small groups interviews of the user experience after the robot inte raction (n=8\ngroups). All individual interactions with the robot were video-record ed both through\nan external camera and the camera on the robot. The interviews with the participants\nwere audio-recorded. All participants gave an informed consent for the s tudy.\n5.1.1 Design scenarios\nThe researcher explained the project, and demonstrated the robot’s capabilities by\ntalking to the robot for two minutes. The participants were instruc ted not to speak\nwhen the red light underneath the robot is on, as it means the robot is eit her generating\na response or talking, and the robot would not hear them if they talked. The gaze\naversion was not mentioned to maintain the naturalness of the interacti on.\nAfter the demonstration, the participants were presented with vid eo-based design\nscenarios on companion robots providing social and emotional support in eve ryday\nlife situations. The participants were asked about their perception s of the robot, and\nwhat kind of conversation they would like to have with the robot in thes e scenarios.\nAlso, a design activity was conducted, where the participants were ask ed to choose the\nface and voice of the robot from several options, that they considered most p leasant.\n5.1.2 Open-domain dialogue\nAfter the design scenarios, the participants were asked to individu ally interact with\nthe robot, and were instructed that they can talk about anything they want . Each\nparticipant had an open-domain dialogue of 7 minutes with the robot that was in itiated\nand monitored by an experimenter through the wizard interface. The ex perimenter\ndid not interfere with the interaction (i.e., the robot was fully aut onomous during\nthe conversation), unless there was a malfunction in the system or if the participant\nwanted to end the interaction before the 7-minute duration. The inte raction ended\nwith the robot saying a pre-scripted phrase, as described in Sect ion\n4.5.5.\nAfter the interaction, all participants ﬁlled out a questionnaire on th e acceptability\nof the robot, which was adapted from the commonly conducted questionnaire s (Likert\nscales from 1 to 5) in HRI [ 63, 72–74] and open-domain dialogue [ 62]. It consisted of\nquestions that measure their user experience, perceived satisf action, and trust with the\nrobot, evaluate the perceived personality of the robot, combined with q uestions from\nparticipants’ socio-demographic background and previous experience w ith robots.\n5.1.3 Small group interviews\nAfter the dialogue with the robot, the participants attended small group i nterviews,\neach consisting of 3-4 participants and lasting 30 to 40 minutes. These s mall group\ninterviews followed the principles of qualitative semi-struct ured interviews, where par-\nticipants were asked about their experience of interacting with th e robot to prevent\nloneliness, their sense of integrity, and how and why they perceiv ed the interaction\nwith the robot in a certain way. The participants were encouraged to ope nly share\n19\ntheir reﬂections with the researcher, and provide their insight s for improvements of\nthe robot.\n5.2 Participants\nThe participants were recruited by distributing the invitation to KTH communica-\ntion channels, social media, and platforms for gathering senior citizen s. In total, 28\nparticipants from the age group 65 and over registered as volunteers ( M = 74 .46,\nSD = 5 .59). All participants were Swedish speakers, and 15 of 28 were females. T he\nmajority of the participants were married (20), living with a spouse ( 20), and most\n(25) of them had children. None of the participants was living in a senior c are house.\nOnly a few of them had previously interacted with a robot (5), and 3 of 28 par ticipants\nhad a robot in their own home, and only 1 participant had previously talked with\na robot. The participants from the preliminary interviews and the pr evious kitchen\nrobot study at KTH were not part of this study. The participants were oﬀe red a small\ncompensation (100 SEK gift card) at the end of the study. The distribute d invitation\nfor the study mentioned that a gift card will be given as compensation, bu t did not\nspecify the amount.\n5.3 Data Analysis\nA total of 3 hours and 24 minutes of video data for the participants’ interac tions with\nthe robot were recorded. The audio recordings for the design scenarios w ere 3 hours\nand 40 minutes long, and the post-interaction interviews were in total 3 hours and 52\nminutes long. Both the audio and video data were transcribed. The vid eo data were\nqualitatively analyzed in the same structure as described in Secti on\n4.3. In addition,\nquantitative analysis was conducted for the response generation time of G PT-3.5,\nand for the questionnaire data. As explained in Section 4.3, the design scenarios, the\nfeedback from the interviews, and the questionnaires are only used f or supporting the\nconclusions for the technical challenges. The detailed analysis of that data is outside\nof the scope of this article.\n5.3.1 Transcriptions\nWhisper (OpenAI) was used on the videos from external cameras for transc riptions\nof robot interactions. Large-v2 model (with default parameters) was used , since it\nachieved the best overall performance for Swedish [\n67]. Google Translate was used to\ntranslate transcripts into English. All videos were manually checke d to correct the text\nand timing of transcriptions, and to remove transcribed text for back ground voices.\nInterviews and design scenarios were manually transcribed using aud io recordings\nby a research assistant, who is a native Swedish speaker, for obtainin g more accurate\nresponses in a multi-speaker setting (1-2 experimenters, and 3-8 participants).\n5.3.2 Quantitative Analysis\nAll the events that are triggered in the robot during an interaction were logged for\naccurate data analysis. From these logs, the time it takes GPT-3.5 to gene rate a\n20\nresponse was obtained using the timestamp when the robot stops listen ing (indicated\nby the red light under the robot) and the timestamp for the robot to start speaking\n(when the gaze aversion ﬁnishes) were used.\nSince the full response history is added to the prompt for GPT-3.5 at e ach turn of\nthe interaction, the response time increases throughout the convers ation. Hence, the\nmeans and standard deviations of the initial (to generate the ﬁrst response), average\n(response time on average throughout the conversation), and ﬁnal (to generate the\nlast response) response times are provided, along with the prompt le ngth for each (in\ntokens21). It is important to note that since the request to get a response is s ent to\nOpenAI three times (as explained in Section 4.5.3), the response generation time can\nalso be long even at the start of the conversation.\n5.4 Technical Challenges\nIn addition to the previously identiﬁed categories of challenges in th e ﬁrst study (i.e.,\nturn-taking, repetitive responses, superﬁcial conversations, and language barrier), hal-\nlucinations and obsolete information, disengagement cues, and premature closures\nwere identiﬁed as challenges of LLMs in companion robots through this study . Note\nthat the duration of interaction with the robot was less in this study ( M = 7 .27,\nSD = 1 .52) than the ﬁrst study ( M = 9 .24, SD = 3 .45).\n5.4.1 Turn-taking\nRobot interruptions decreased drastically ( M = 3 .18, SD = 2 .57) compared to the\ninitial study ( M = 14 .00, SD = 6 .26), occurring in 24 (out of 28) interactions up\nto 9 times, which shows that turn-taking indicators (the red light and gaze aversion)\nwere beneﬁcial. Participants also perceived to a large extent that t he robot did not\ninterrupt them ( Md = 4 .0, IQR = 2 .0), the robot could understand when they wanted\nto take a turn ( Md = 4 .0, IQR = 1 .0), and it was easy to start and continue the\nconversation without any help ( Md = 4 .0, IQR = 1 .0) in the questionnaire. However,\ntwo participants noted that the gaze aversion behavior was making it diﬃ cult to focus\non the robot, possibly referring to the random gaze aversion during robot conversation\nand listening rather than the gaze aversion during response generation , since another\nparticipant noted that it’s important to show through gaze or facial expres sions, that\nthe robot is generating a response, even if nothing is heard. A partici pant, who was\ninterrupted 7 times in the conversation, noted that the red light was not easy to keep\ntrack of. Since participants often gazed away when they are thinking, e ven when they\nhave the turn, it was possible to miss the light indicator.\nSimilar to the initial study, participants’ long responses depend ing on the context\n(e.g., talking about daily activities prior to interaction) resulted in partial responses\nto be recorded, however, none of these resulted in disruptions in the interaction.\nThe response time was very long due to slow GPT-3.5 response generati on\n(Table\n1). Participants had conﬂicting perceptions when asked if the robot was slow\nin its responses ( Md = 3 .0, IQR = 2 .0). Note that the maximum prompt in an\n21Calculated using OpenAI’s tiktoken library: https://github.com/openai/tiktoken\n21\ninteraction (2243 tokens) was shorter than the maximum prompt length of GPT -3.5\n(4097).\nTable 1: Response generation time of GPT-3.5.\nCategory Response Time (s) Prompt Length (tokens)\nInitial M = 2 .45, SD = 0 .55 M = 391 .75, SD = 9 .76\nAverage M = 2 .38, SD = 0 .22 M = 1118 .64, SD = 176 .20\nFinal M = 2 .42, SD = 0 .59 M = 1710 .82, SD = 278 .40\n5.4.2 Repetitive Responses\nThe empty responses from GPT-3.5 ( M = 0 .54, SD = 0 .74) were fewer, which hap-\npened at most 2 times in an interaction (for 11 participants). Due to the (only) bug\nin the code, there was no utterance made by the robot in those cases. Hen ce, instead\nof making a clariﬁcation request or backchannelling, it started listen ing to the user\nagain (the red light was oﬀ). This behavior confused the participants on w hether they\nshould wait for the robot to answer or say something. When the participant asked\nfor help from the experimenters, they were told the robot did not hear them, to say\ntheir response again. Otherwise, they said something about the robot’s silence and\nthe robot responded (e.g.,I was lost in thought about your beautiful boat. Wh at did\nyou say again?), which was a good repair from GPT-3.5.\nRepetitive phrases from GPT-3.5 ( M = 0 .75, SD = 2 .76) were also lower, and only\noccurred in 3 interactions. But it occurred up to 14 times in an int eraction, in which\nthe robot might have been perceived as interrogative and oﬀensive, sin ce it kept on\nquestioning why the user was thinking in a certain way (about UFOs) w ith the exact\nsame phrase. The participant turned to the experimenter to ask how they can change\nthe topic, but this triggered a topic change from the robot.\n5.4.3 Superﬁcial Conversations\nThe topics discussed with the robot were more diverse than the prel iminary interviews\nas shown in Fig.\n4. Albeit being much less frequent than the initial study, hobbies and\ninterests formed most of the conversations (23 .07%), but in itself had a wide range\nof topics, from science, space, and UFOs to literature, music, exerc ise, and outdoor\nactivities. The underlying reasons for the higher diversity of the speech events or topics\ncould be due to (1) the design scenarios prior to the interaction in whi ch a part of these\ntopics were discussed between participants, and the robot’s capabil ities were clariﬁed\nby the researchers whenever requested, (2) the variability in us er preferences and a\nhigher number of participants, (3) the lower number of disruptions i n the conversation\nﬂow, which encouraged users to engage more in the conversation, and (4) the robot’s\npersona that prioritized social and emotional support more than small talk .\nThe majority of the conversations (69 .87%) was ‘informal/superﬁcial’, arising from\nthe ﬁrst interaction with the robot, as well as the robot trying to inst antiate conversa-\ntions around interests, memories, activities, and feelings due to i ts persona. However,\n22\n7\n4 3 3 3 3\n3\n2 2 2 2 2\n2\n2\n2 2\n1 1 1 1 1 1 1 1 1 1\n1 1 1\n1 1 1 1 1\n1\n1 1 1 1 1 1 1 1\n1\n1\n1 1 1\n1 1 1 1 1 1 1\n1\n1\n1 1 1 1 1\n1 1 1 1 1 1 1 1\n1\n1\n1 1 1 1 1 1 1\n1 1\n2 1 1 1 1 1 1\n1 1 1\n1\n1\n1\n3\n32 2 2\n1 1 1 1\n1 1 1 1 1\n2 21 1 1 1 1 1\n1 1\nGetting to Know\nSomeone\nSmall Talk\nCurrent Events Talk\nRecapping the Day’s\nEvents\nSerious Conversation\nComplaining\nMaking Up\nConflict\nRelationships\nGiving and Getting\nInstructions\nDecision−making\nConversation\nMaking Plans\n0 10 20 30 40\nFriends\nLanguage\nOccupation\nPets\nRetirement\nFood\nMemories\nFamily\nTravel\nResidence\nTechnology\nHobbies and Interests\nHealth\nWeather\nNews\nPlans for the Day\nDaily Activities\nPolitics\nComplaining\nApology\nDisagreement\nRelationships\nAction Request\nInformation Request\nAdvice Seeking\nPlans with Robot\nCount\nParticipant P1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\nP9\nP10\nP11\nP13\nP14\nP15\nP16\nP17\nP18\nP19\nP21\nP22\nP23\nP25\nP27\nP28\nP29\nP30\nP31\nP32\nInformal/Superficial TalkInvolving TalkGoal-directed\nTalk\nFig. 4: Topics participants discussed with the robot during the partici patory design\nworkshops.\nthe participants also explored the task-oriented capabilities of the robot (or rather\nthe LLM) in 21.15% of the conversations. These ‘goal-directed talks’ consist ed of (1)\n‘information request’ about a restaurant, transportation, TV program, and r ecipes, (2)\n‘action request’, such as the user requesting the robot to book a res taurant, ask it to\nsing or say something, (3) ‘advice seeking’ in which the user asks the robot to provide\nopinions on a subject and discusses with it on that topic (e.g., a recomm endation for a\nrug, gardening, activities for the weekend, travel), and (4) ‘plans wi th robot’, in which\nthe user invites the robot over or makes future plans with it. Note that these topics\nwere separated from ‘complaining’ about the robot’s behaviors (e.g., gaze) , ‘apology’\nfrom the person or robot about their action or utterance in the conversation, ‘dis-\nagreement’ with the information provided by the robot, and ‘technology’ , in which\n23\nthe user either asked the robot about its capabilities to get to know it more or dis-\ncussed broader topics about technology and AI with the robot, rather than req uesting\ninformation, advice, or an action. Only a small part of the conversations (8.97% ) went\ninto deeper topics (i.e., ‘involving talk’). In comparison to the p revious persona, the\nrobot was more submissive, i.e., the participants took the major lead i n directing the\nconversation. This is likely why, some participants did not perceiv e the interaction as\ninteresting ( Md = 3 .0, IQR = 1 .25). Some participants were also worried about what\nto say or talk about with the robot ( Md = 2, IQR = 2). The user experience was also\nmore varied due to the wide range of topics covered, thus the user sati sfaction with the\nconversation varied ( Md = 3 .0, IQR = 1 .25). Consequently, participants had mixed\nopinions about having the robot at home ( Md = 2 .0, IQR = 2 .0), with the majority\nnot considering it ready yet.\nWhile in some cases the robot mirrored the user’s responses similar t o the initial\nstudy, in most cases it expressed opinions and oﬀered suggestions, whi ch improved its\nagency and human-likeness for the participants, as noted by one of the part icipants\nin the interviews: “Because I asked about the color of carpets and what it w as, what\nwould ﬁt. And then it came up with something a little more independe nt but it was\ncompletely in line with what I had said. He had some words of his own about wh at\nI had said. It’s more like a friend like this who agrees.” Another parti cipant noted\nthat “the conversation felt like talking to a real person”. Yet in the in terviews, some\nof the participants noted that it was impersonal, too general, and it does n’t have a\npersonality, as it was mostly found machine-like rather than human-lik e ( Md = 2 .0,\nIQR = 2 .25 in the Godspeed [ 73] questionnaire scale) and most participants agreed to\na slight extent that it “felt like a real person” ( Md = 2 .0, IQR = 1 .0). Nonetheless, the\nparticipants considered the robot’s responses as consistent ( Md = 4 .0, IQR = 1 .0).\n5.4.4 Language Barrier\nSpeech recognition failures were also less frequent ( M = 0 .22, SD = 0 .64), occurring in\n4 interactions (up to 3 times per interaction), often due to Swedis h names ( M = 0 .21,\nSD = 0 .69). These failures resulted in either an out-of-context response fr om the\nrobot, which led to confusion and topic change, or correction by the partic ipant.\nParticipants perceived that the robot understood them ( Md = 4 .0, IQR = 1 .0), while\nsome participants worried that the robot wouldn’t understand or hear the m ( Md = 2,\nIQR = 1 .25). However, one of the participants noted in the interviews that “It’s kind\nof like talking to other voices like this on the phone and everything, it’s very diﬃcult\nbecause you feel like you’re not really getting through. But you try to b e overly clear.\nSo you don’t really speak naturally.”.\nOne of the participants asked the robot whether it can speak German in Ge rman,\nwhich resulted in a speech recognition failure, as only Swedish was u sed the input\nlanguage in Google Cloud Speech-to-Text. The robot also mispronounced Engl ish\nphrases in generated responses, such as reading the lyrics of an Englis h song as if it is\nwritten in Swedish, instead of singing the song, as the participant re quested. Similar\nto the initial study, some participants noted that speaking to the rob ot in English\nwould be a good use case for older adults.\n24\n5.4.5 Hallucinations and Obsolete Information\nDue to ‘goal-directed talk’, which did not take place in the initial st udy, ‘hallucinations’\n(i.e., misinformation about a restaurant, bus times, TV series, oper a, and exaggerating\nrobot capabilities) and obsolete information (e.g., weather, old movie re commenda-\ntion) came into sight ( M = 0 .57, SD = 0 .84) in 11 of the interactions, occurring up\nto 3 times. This led to neutral trust in the robot ( Md = 3 .0, IQR = 0 .25), and caused\nvaried perceptions of the correctness of the robot’s responses ( Md = 3 .5, IQR = 1 .0),\nand competency of the robot ( Md = 3 .0, IQR = 1 .0). But it also led one participant\nto doubt themselves about the information they know (about the produc tion date of\na TV series)\n22.\nOne of the participants noted in the interviews that they had tried as king the\nsame questions (about the production date of a sailboat and whether they c an sail\ntomorrow) that they had previously asked to ChatGPT, and the responses from the\nrobot were “much worse”, that the robot gave incorrect information about the w eather,\nwhereas ChatGPT told him, “I cannot forecast weather”.\n5.4.6 Disengagement Cues and Premature Closures\nDisengagement cues from the robot, such as “I understand”, and “That is good\nto know”, brought the conversation to a halt, with participants not know ing how\nto respond, in 5 of the interactions, up to 4 times in an interaction ( M = 0 .39,\nSD = 0 .99). Moreover, the robot tried to end the conversation prematurely (ear lier\nthan 7 minutes) in 7 interactions, up to 2 times ( M = 0 .32, SD = 0 .61). The partici-\npant either changed the topic to continue the interaction without int ervention, or the\nexperimenter told them they could continue if they want to. This be havior might have\nled to mostly positive, but mixed opinions for the engagingness of the con versation\n(Md = 3 .0, IQR = 1 .25).\nTopics were less frequently changed in comparison to the initial stu dy, with several\nlevels of follow-up questions in most interactions. However, in 3 of t he interactions, the\nrobot asked whether they would like to talk about another topic 3 or 4 times to the\nparticipants. In one of the interviews, a participant hypothesized that this behavior\nwas due to the robot’s lack of knowledge on the particular topic discusse d.\n6 Discussion\nMinor technical improvements changed the user experience drasti cally. However, these\ndevelopments were not suﬃcient to remove completely the presen ce of any of the\nchallenges. This section discusses solutions to help address thes e challenges further.\n6.1 Turn-taking\nBoth studies contrastively show the importance of turn-taking in HRI for user expe-\nriences, and that the disruptions in speech can have negative eﬀect s on older adults.\nWhile gaze aversion and light indication helped decrease the interru ptions, but they\nstill occurred in most of the conversations. Turn-taking behavior c an be further\n22https://youtu.be/8sHknc1mbhM\n25\nimproved by using dialogue context (e.g., [ 75]), speech prosody (e.g., [ 76]), estimating\nthe user’s gaze, gestures, and facial expressions, such as eyebrow mo vement and mouth\nopening [ 77], or a combination of these features, e.g., [ 78–80] (for an in-depth review\nof turn-taking in HRI and conversational systems, see [ 81]) to prevent interrupting\nthe user, both for improving user experience and speech recogniti on.\nThe long response generation time, which caused unnatural interaction s, can be\ndecreased by using a smaller LLM (with the trade-oﬀ of task performance or factu-\nality [ 82]), an LLM with faster inference, quantization techniques [ 83], or by using\nconversation summaries rather than the full dialogue to decrease the pr ompt length.\nHowever, summaries may result in further hallucinations, which c ould be detrimental\nto the interaction as well as to the personalization (of learned facts).\nTo overcome awkward silences during the response generation as noted by the\nparticipants, ﬁllers, such as “Hmm” and “Let me think”, can be used [ 84]. However,\noveruse of them might also lead to the robot being perceived as repeti tive. In addition,\nto decrease the superﬁcialness of turns (sequential nature), back channelling can be\nadded using both verbal (e.g., “mm hm”, “uh huh”, “yeah”) and non-verbal cues (e.g.,\nnods, head shakes, facial expressions) [ 85], but it is important to do this in context with\nthe dialogue, otherwise, it may also further deteriorate the user ex perience. Random\ngaze aversion should not be made during robot speech or while listening, as noted by\ntwo participants, because keeping eye contact with the user helps maintain the user\nengagement, perceived sociability, and improve quality of interact ion [ 86].\n6.2 Repetitive Responses\nThe increasing use of ChatGPT and other GPT-3.5 models, in addition t o OpenAI\nprioritizing Plus subscribers in access to the servers, resul t in frequent response gener-\nation failures, as experienced in both studies, which can be overcom e through multiple\nresponse requests. However, this increases the already long respon se time. Thus, using\nanother LLM, such as LLaMA (Meta) [\n87] or its variants (e.g., Alpaca 23), might be\nmore suitable to achieve more reliable response generation. However, i n the case of the\nresponse generation failures, the conversation can be pushed forward by invitation for\nelaboration, in addition to clariﬁcation requests, but they should be v aried, and used\nto a minimum to avoid backlash from the user.\nLarge language models tend to be repetitive, and can get stuck in a loop [ 88], as evi-\ndenced in both studies, resulting in user frustration. The ‘fre quency penalty’ parameter\nin GPT-3.5 can be increased to enforce the model to produce more vari ed responses\n(words). This can be further accompanied by sampling [ 82, 89], unlikelihood train-\ning [ 90], best-ﬁrst decoding [ 91], or reinforcement learning from demonstration [ 92] to\nreduce repetitive behavior. To reduce the LLM from getting conditi oned on itself to\nproduce the same response again, prompt initializers can be used (e.g. , “Rephrase the\nsentence”) if the previously generated response is repeated outsid e of user clariﬁcation\nrequests.\n23https://github.com/tatsu-lab/stanford alpaca\n26\n6.3 Superﬁcial Conversations\nTo decrease the superﬁcialness of conversations and provide a more nat ural and engag-\ning interaction to older adults in their daily lives, the range of topic s discussed can be\nmore varied using the ‘presence penalty’ and ‘temperature’ in GP T-3.5. However, this\nmay lead to more abrupt changes in topics, instead of smooth transitions w ith follow-\nups (i.e., exploration vs exploitation trade-oﬀ). Fine-tuning can be applied with data\non older adult speech to provide address this issue [\n19], however, that might also limit\nthe model’s goal-directed capabilities.\nIn addition, the persona prompt can be modiﬁed to use more follow-up q uestions\nand give deeper personality (e.g., pre-deﬁned preferences, disl ikes, memories) to the\nrobot to improve the believability of the character, instead of starti ng from ‘scratch’.\nThe believability can further be improved by using mood and emoti ons for the agent,\nin addition to detecting the user’s emotions to adapt the conversation accordingly [ 93].\nEmotions can also be used as unsupervised RLHF signals for task performance [94],\nsuch that frustration in a conversation topic or phrase can be avoided in f uture\ninteractions.\nTo generate deeper conversations that rely on shared history with the u ser, their\npreferences, dislikes, and daily activities, the robot should lear n from the user. How-\never, over long-term interactions, storing information in a database is not scalable, as\nprompts based on an ever-growing database would be too long. In addition, us ing a\nretrieval-based method and a database to update the results of LLMs with new data\ncan result in hallucinations [ 95]. One option is to retrain models with new information,\nhowever, this would be computationally ineﬃcient with a vast amounts of data, as well\nas result in ‘catastrophic forgetting’ of previously learned information [96, 97]. ‘Life-\nlong (or continual) learning’ [ 98] aims to address this problem by continually learning\n(the parameters of the model) over time to accommodate new knowledge (i.e., plas-\nticity) while retaining previously learned information (i.e., st ability). Within these\nmethods, combining LLMs with ‘parameter-expansion’ methods oﬀer th e best balance\nof stability and plasticity [ 99]. Lifelong learning can help personalize the interactions,\nwhich in turn can help mitigate the negative user experiences that m ay arise due to\npoor performance in dialogue [ 100], improving user engagement [ 101], acceptability,\nand trust [ 102].\nOver long-term deployments in real-world environments (e.g., olde r adults’ homes\nor senior care centers), it is important to have full autonomy, such th at the user\ncan start the interaction without the need of a wizarded system. For a p ersonalized\ncompanion robot, this requires detecting new users and incremental ly learn them to\nrecognize them to prevent sharing the information learned about the us er with others,\nhence, an ‘open world’ user recognition algorithm, such as Multi-modal I ncremental\nBayesian Network [ 103], can be used. With personalization, the privacy of the interac-\ntion should be considered with utmost importance. Since current oﬄi ne LLMs require\nvast computing power, it would not be possible to put them on a robot, bu t online\nmodels, as we did in our studies, risk the privacy of the individual . While the ﬁrst\ninteraction with a robot, as evidenced in our results, are mostly supe rﬁcial, deeper\nconversations can be achieved over long-term interactions, in which u sers can talk\nabout sensitive topics. The risks, trade-oﬀs, and possible solutions to achieve privacy\n27\nare discussed in depth in the works by Bommasani et al. [ 104] and Weidinger et al.\n[105], which should be considered in parallel to the challenges highlight ed in this work,\nwhile developing companion robots for older adults with LLMs.\n6.4 Language Barrier\nWhile speech recognition failures were low in the second study, th eir rare presence\nled to abrupt disruptions in the conversation ﬂow. Since older adult speech contains\npauses, and hesitancy, and is slower than younger adult speech, it is c hallenging to\nobtain good performance in speech recognition. One way to address this is to evaluate\ndiﬀerent speech recognition algorithms with older adult speech, suc h as Microsoft\nAzure or Whisper JAX\n24, which optimizes Whisper for speed that can enable real-time\ntranscription of the user utterances, and choose the algorithm that wor ks the best.\nSince GPT-3.5 allows changing the conversation language when the prompt i s in a\ndiﬀerent language, enabling multi-lingual speech recognition would im prove the capa-\nbilities of the robot, such as second language teaching as suggested by the p articipants\nin both studies, and help adapt to foreign visitors. However, allowing multiple lan-\nguages to be transcribed may result in further interaction failures , especially due to\noverlapping words in diﬀerent languages that may mean diﬀerent things .\nAlso, the speech recognition output from Google Cloud does not contain punc tu-\nation marks, which may lead to disambiguity in speech. Parsing the re sponses may\nhelp improve the response generation from GPT-3.5.\n6.5 Hallucinations and Obsolete Information\nDisinformation can be risky in older adults’ lives. For instance, if t he user asks for\nadvice on medicine, and the LLM produces a wrong answer, the consequen ces can be\ncritical. Thus it is important to ensure that correct information is p rovided by LLMs.\nOne approach is to prevent response generation in sensitive topics wi th ﬁltering [\n105].\nThe hallucinations in the model can also be mitigated by applying indu ctive attention\n(e.g., [ 106]) or retrieval-based methods (e.g., [ 107]). Ji et al. [ 10] provide a detailed\nsurvey on hallucinations in LLMs with further suggestions.\nIn addition, due to their training data cut-oﬀ dates, LLMs contain obsolet e informa-\ntion, which was present in the interactions in the second study, th at can be addressed\nalso by fact-checking in knowledge bases [ 108]. As suggested in Section 6.3, lifelong\nlearning can be applied to update the information over time for user fac ts. However,\nfor facts that can be obtained through the internet (such as weather), LLMs with\nbrowsing capabilities can be used (e.g., AutoGPT 25).\n6.6 Disengagement Cues and Premature Closures\nOur work aims to design a personal companion robot that decreases lonelines s in older\nadults, and supports them in their daily lives. If a robot tries to en d a conversation\nin a short duration, this could further deepen their feelings of lone liness. Hence, it is\nimportant to engage the user in the conversation. One possible solution i s to detect\n24https://huggingface.co/spaces/sanchit-gandhi/whisper-jax\n25https://github.com/Signiﬁcant-Gravitas/Auto-GPT\n28\nengagement in the conversation in both user’s and agent’s responses, as we ll as through\ngaze, and other non-verbal or contextual information [ 101]. Upon detection of disen-\ngagement, a prompt can be used to change the topic, ask the user what they w ould like\nto talk about, or make the current topic more interesting, to encourage con tinuation\nin the conversation.\n7 Limitations and Future Work\nThis work identiﬁed the challenges in applying LLMs to companion robots f or open-\ndomain dialogue with older adults as frequent interruptions and slow re sponses,\nrepetitive responses, superﬁcial conversations, language barrier, h allucinations and\nobsolete information, and disengagement cues. These challenges are by no means an\nexhaustive list, but aim to provide initial guidelines towards de veloping companion\nrobots that can be part of the everyday lives of older adults. They also sh ow the com-\nplexity of multi-modal interactions, in comparison to the text-base d interactions that\nLLMs are typically evaluated on, especially for a population that is not famil iar with\nthe current state of the technology.\nThe LLM (GPT-3.5) and robot (Furhat) used in this work, in addition to the\ntechnical design decisions made, have led to the identiﬁcation of di alogue disruptions\nthat has and could negatively aﬀect older adults’ experiences with comp anion robots.\nHowever, the identiﬁed challenges may not replicate in other LLMs or rob ots, depend-\ning on their open-domain dialogue capabilities, modalities used, and th e additional\nlibraries that can address these challenges.\nMoreover, as previously described, in both studies only a few parti cipants (2 in the\nﬁrst study, 5 in the second) had previously interacted with a robot, with 2 participants\ntalking to a wizarded robot prior to the ﬁrst study, and only 1 particip ant talking to\nan (unknown) robot prior to the second study. Hence, this was the parti cipants’ ever\nﬁrst interaction with a (social) robot in the majority of the cases, le ading to ‘novelty\neﬀect’ [\n109, 110] (i.e., users’ perceptions and behaviors are aﬀected by the novel ty\nof the technology). Furthermore, talking to a robot as part of an experimen t rather\nthan having the robot “in the wild” (at home or a senior care center) may l ead to\nchanges in the behavior changes due to being observed [ 111] and create an artiﬁcial\ncontext within conversations, preventing to unlock the full range of topics in open-\ndomain conversation and corresponding challenges that can arise in convers ations with\na companion robot over long-term interactions [ 112]. In addition, the participants in\nboth studies had volunteered (with only a small compensation oﬀered at t he second\nstudy), which indicates an interest in robots (i.e., ‘participati on bias’) [ 111] and may\nnot reﬂect the views of the general population of older adults. Nonetheles s, the aim of\nthese studies were to identify the primary obstacles that may aris e in conversations and\novercome them, in addition to understanding needs, preferences , and expectations of\nolder adults through iterative co-design, prior to exposing them to ne w technology in\ntheir homes or senior care centers to avoid backlash with fear, annoyanc e, or reluctance\nto use robots.\nIn future work, we intend to develop the robot’s capabilities furth er using the\nsolutions suggested in this section, and continue the iterative co-de sign with older\n29\nadults, by bringing back the participants from both studies for another interaction with\nthe robot. After conﬁrming that the robot is suitable for deployments, we intend to\ndeploy it for a real-world long-term study in older adults’ homes, whi ch can overcome\nthe unnaturalness of conversations and perhaps show deeper challenge s to address.\nWhile the challenges and the corresponding suggestions presented he re for applying\nLLMs to companion robots are based on the interactions with older adults, the y may\nhold for socially assistive robots or companion robots with other populations , as well\nas general-purpose robots and spoken dialogue systems, in other words, whe rever an\nopen-domain conversation may take place.\nAcknowledgments. The authors would like to thank Aida Hosseini for participat-\ning as an interviewer in the initial study and transcribing interv iews for both studies,\nYoussef Mohamed for helping conduct the robot interactions in two of the design work-\nshops, and Mikaela Hellstrand for helping shape the empathetic robot pe rsona based\non her research in senior care centers. The authors would like to thank the participants\nat the initial study and design workshops for their time and feedback.\nDeclarations\nCompliance with Ethical Standards\nThe study was conducted according to the standards from Ethical Revie w Authority\nin Sweden. All participants gave an informed consent to participate in the studies.\nParticipants, whose images and/or videos appear in this article, gave infor med consent\nfor anonymized (i.e., blurred face, and without full name released) i mage and/or video\nsharing in publications (conferences and/or journals). The study did not collect any\nsensitive or health-related information from the participants.\nFunding\nThis work is funded by KTH Digital Futures.\nConﬂicts of Interest\nG.S. is co-aﬃliated with Furhat Robotics, as its Co-founder and Chief Sc ientist. The\nremaining authors have no relevant ﬁnancial or non-ﬁnancial interests to disclose.\nUsage of a Large Language Model in the Article\nB.I. used ChatGPT (OpenAI) to generate ideas for the paper title, sec tion headers,\nterminology, and synonyms for words. The information generated by the mod el was\nnever used directly: it was modiﬁed, fact-checked, and combined with the author’s\nown ideas and text.\n30\nTable A1: Hyperparameters of GPT-3.5 used in\nboth studies. The default values from the\nOpenAI Playground for Chat preset were\nused, except for the maximum length\nand frequency penalty (see footnotes).\nHyperparameter Value Range\nTemperature 0.9 [0,1]\nMaximum Length 50 1 [1,4000]\nTop P 1.0 [0,1]\nFrequency Penalty 0.5 2 [0,2]\nPresence Penalty 0.6 [0,2]\nStop Words USER\nNAME:, Furhat:\nBest of 1 [1,20]\n1The maximum length for tokens was decreased (default\nvalue 150) to generate shorter responses to keep the con-\nversation ﬂowing and engaging the user.\n2The frequency penalty was increased (default value 0)\nto decrease the model’s likelihood to repeat the same\nresponses. Because the users may ask the robot to repeat\nits responses (e.g., when they don’t hear it well), this valu e\nwas only increased to 0.5.\nAppendix A Hyperparameters of GPT-3.5\nReferences\n[1] Cacioppo, J.T., Hughes, M.E., Waite, L.J., Hawkley, L.C., Thisted, R.A.: Lone-\nliness as a speciﬁc risk factor for depressive symptoms: cross-sec tional and\nlongitudinal analyses. Psychol Aging 21(1), 140–151 (2006)\nhttps://doi.org/10.\n1037/0882-7974.21.1.140\n[2] Luo, Y., Hawkley, L.C., Waite, L.J., Cacioppo, J.T.: Loneliness, health , and\nmortality in old age: a national longitudinal study. Soc Sci Med 74(6), 907–914\n(2012)\n[3] Kim, J., Kim, S., Kim, S., Lee, E., Heo, Y., Hwang, C.-Y., Choi, Y.-Y., Kon g, H.-\nJ., Ryu, H., Lee, H.: Companion robots for older adults: Rodgers’ evolutionar y\nconcept analysis approach. Intelligent Service Robotics 14(5), 729–739 (2021)\nhttps://doi.org/10.1007/s11370-021-00394-3\n[4] Dautenhahn, K.: Socially intelligent robots: dimensions of human- robot inter-\naction. Philos Trans R Soc Lond B Biol Sci 362(1480), 679–704 (2007) https:\n//doi.org/10.1098/rstb.2006.2004\n[5] ˇSabanovi´ c, S.: Robots in society, society in robots. International J ournal of Social\nRobotics 2(4), 439–450 (2010) https://doi.org/10.1007/s12369-010-0066-7\n31\n[6] Lee, H.R., ˇSabanovi´ c, S., Chang, W.-L., Nagata, S., Piatt, J., Bennett, C.,\nHakken, D.: Steps Toward Participatory Design of Social Robots: Mutual Lear n-\ning with Older Adults with Depression. In: Proceedings of the 2017 AC M/IEEE\nInternational Conference on Human-Robot Interaction, pp. 244–253. ACM,\nVienna, Austria (2017).\nhttps://doi.org/10.1145/2909824.3020237\n[7] Stegner, L., Senft, E., Mutlu, B.: Situated participatory design : A method for in\nsitu design of robotic interaction with older adults. In: Proceedings of the 2023\nCHI Conference on Human Factors in Computing Systems. CHI ’23. Association\nfor Computing Machinery, New York, NY, USA (2023).\nhttps://doi.org/10.1145/\n3544548.3580893 . https://doi.org/10.1145/3544548.3580893\n[8] Fern´ andez-Rodicio, E., Castro-Gonz´ alez, A., Alonso-Mart´ ın, F., Maroto-G´ omez,\nM., Salichs, M.A.: Modelling multimodal dialogues for social robots us ing\ncommunicative acts. Sensors 20(12) (2020) https://doi.org/10.3390/s20123440\n[9] Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang,\nB., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Re n,\nR., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.-Y., Wen, J.-R.: A Survey of Lar ge\nLanguage Models. Preprint at http://arxiv.org/abs/2303.18223 (2023)\n[10] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Ban g, Y.J., Madotto,\nA., Fung, P.: Survey of hallucination in natural language generation. ACM\nComput. Surv. 55(12) (2023)\nhttps://doi.org/10.1145/3571730\n[11] Feil-Seifer, D., Matari´ c, M.J.: Socially assistive robotics. IEEE Robotics &\nAutomation Magazine 18(1), 24–31 (2011) https://doi.org/10.1109/MRA.2010.\n940150\n[12] Jung, M.M., Ludden, G.D.S.: What Do Older Adults and Clinicians T hink\nAbout Traditional Mobility Aids and Exoskeleton Technology? ACM Transac-\ntions on Human-Robot Interaction 8(2), 1–17 (2019)\nhttps://doi.org/10.1145/\n3311789\n[13] J¨ onsson, K.-E., Ornstein, K., Christensen, J., Eriksson, J .: A reminder system\nfor independence in dementia care: a case study in an assisted livi ng facility.\nIn: Proceedings of the 12th ACM International Conference on PErvasive T ech-\nnologies Related to Assistive Environments, pp. 176–185. ACM, Rhodes, Gre ece\n(2019). https://doi.org/10.1145/3316782.3321530\n[14] Pou-Prom, C., Raimondo, S., Rudzicz, F.: A Conversational Robot for Old er\nAdults with Alzheimer’s Disease. ACM Transactions on Human-Robot Inter ac-\ntion 9(3), 1–25 (2020) https://doi.org/10.1145/3380785\n[15] Rudzicz, F., Wang, R., Begum, M., Mihailidis, A.: Speech Interac tion with\nPersonal Assistive Robots Supporting Aging at Home for Individuals with\nAlzheimer’s Disease. ACM Transactions on Accessible Computing 7(2), 1–22\n32\n(2015) https://doi.org/10.1145/2744206\n[16] Baecker, A.N., Geiskkovitch, D.Y., Gonz´ alez, A.L., Young, J.E.: Em otional Sup-\nport Domestic Robots for Healthy Older Adults: Conversational Prototyp es to\nHelp With Loneliness. In: Companion of the 2020 ACM/IEEE International\nConference on Human-Robot Interaction, pp. 122–124. ACM, Cambridge United\nKingdom (2020).\nhttps://doi.org/10.1145/3371382.3378279\n[17] Gupta, A., Bridges, N., Kamino, W.: Musically Assistive Robot for the E lderly\nin Isolation. In: Companion of the 2021 ACM/IEEE International Conference\non Human-Robot Interaction, pp. 620–621. ACM, Boulder CO USA (2021).\nhttps://doi.org/10.1145/3434074.3446963\n[18] Simpson, J., Gaiser, F., Mac´ ık, M., Breßgott, T.: Daisy: A Friend ly Conver-\nsational Agent for Older Adults. In: Proceedings of the 2nd Conference on\nConversational User Interfaces, pp. 1–3. ACM, Bilbao, Spain (2020).\nhttps:\n//doi.org/10.1145/3405755.3406166\n[19] Khoo, W., Hsu, L.-J., Amon, K.J., Chakilam, P.V., Chen, W.-C., Kaufm an,\nZ., Lungu, A., Sato, H., Seliger, E., Swaminathan, M., Tsui, K.M., Crandal l,\nD.J., Sabanovi´ c, S.: Spill the tea: When robot conversation agents s upport well-\nbeing for older adults. In: Companion of the 2023 ACM/IEEE International\nConference on Human-Robot Interaction. HRI ’23, pp. 178–182. Association for\nComputing Machinery, New York, NY, USA (2023).\nhttps://doi.org/10.1145/\n3568294.3580067\n[20] El Kamali, M., Angelini, L., Caon, M., Khaled, O.A., Mugellini, E., Du lack,\nN., Chamberlin, P., Craig, C., Andreoni, G.: NESTORE: Mobile Chatbot and\nTangible Vocal Assistant to Support Older Adults’ Wellbeing. In: Pro ceedings\nof the 2nd Conference on Conversational User Interfaces, pp. 1–3. ACM, Bilb ao,\nSpain (2020).\nhttps://doi.org/10.1145/3405755.3406167\n[21] Zubatiy, T., Vickers, K.L., Mathur, N., Mynatt, E.D.: Empowerin g Dyads of\nOlder Adults With Mild Cognitive Impairment And Their Care Partners Using\nConversational Agents. In: Proceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems, pp. 1–15. ACM, Yokohama, Japan (2021).\nhttps:\n//doi.org/10.1145/3411764.3445124\n[22] Pradhan, A., Lazar, A., Findlater, L.: Use of Intelligent Voice Assistants by Older\nAdults with Low Technology Use. ACM Transactions on Computer-Human\nInteraction 27(4), 1–27 (2020)\nhttps://doi.org/10.1145/3373759\n[23] Sayago, S., Neves, B.B., Cowan, B.R.: Voice assistants and older peopl e: some\nopen issues. In: Proceedings of the 1st International Conference on Con versa-\ntional User Interfaces - CUI ’19, pp. 1–3. ACM Press, Dublin, Ireland (2019).\nhttps://doi.org/10.1145/3342775.3342803\n33\n[24] Blair, J., Abdullah, S.: Understanding the Needs and Challenges of Usi ng Con-\nversational Agents for Deaf Older Adults. In: Conference Companion Publi cation\nof the 2019 on Computer Supported Cooperative Work and Social Computing,\npp. 161–165. ACM, Austin, TX, USA (2019).\nhttps://doi.org/10.1145/3311957.\n3359487\n[25] Randall, N., Joshi, S., Kamino, W., Hsu, L.-J., Agnihotri, A., Li, G., Wil liamson,\nD., Tsui, K., ˇSabanovi´ c, S.: Finding ikigai: How robots can support meaning in\nlater life. Frontiers in Robotics and AI 9 (2022) https://doi.org/10.3389/frobt.\n2022.1011327\n[26] Abdolrahmani, A., Kuber, R., Branham, S.M.: ”Siri Talks at You”: An Empir ical\nInvestigation of Voice-Activated Personal Assistant (VAPA) Usage by Individ u-\nals Who Are Blind. In: Proceedings of the 20th International ACM SIGACC ESS\nConference on Computers and Accessibility, pp. 249–258. ACM, Galway Irelan d\n(2018). https://doi.org/10.1145/3234695.3236344\n[27] Chung, K., Oh, Y.H., Ju, D.Y.: Elderly Users’ Interaction with Conv ersational\nAgent. In: Proceedings of the 7th International Conference on Human-Agent\nInteraction, pp. 277–279. ACM, Kyoto Japan (2019).\nhttps://doi.org/10.1145/\n3349537.3352791\n[28] Gollasch, D., Weber, G.: Age-Related Diﬀerences in Preference s for Using Voice\nAssistants. In: Mensch und Computer 2021, pp. 156–167. ACM, Ingolstadt,\nGermany (2021).\nhttps://doi.org/10.1145/3473856.3473889\n[29] ˇSabanovi´ c, S., Chang, W.-L., Bennett, C.C., Piatt, J.A., Hakken, D.: A Robot\nof My Own: Participatory Design of Socially Assistive Robots for Indepen dently\nLiving Older Adults Diagnosed with Depression. In: Zhou, J., Salvend y, G. (eds.)\nHuman Aspects of IT for the Aged Population. Design for Aging vol. 9193, pp.\n104–114. Springer, Cham (2015).\nhttps://doi.org/10.1007/978-3-319-20892-3\n11\n[30] Randall, N., ˇSabanovi´ c, S., Chang, W.: Engaging Older Adults with Depres-\nsion as Co-Designers of Assistive In-Home Robots. In: Proceedings of the 12t h\nEAI International Conference on Pervasive Computing Technologies For Heal th-\ncare, pp. 304–309. ACM, New York, NY, USA (2018). https://doi.org/10.1145/\n3240925.3240946\n[31] Deriu, J., Rodrigo, A., Otegi, A., Echegoyen, G., Rosset, S., Agirre , E., Cielibak,\nM.: Survey on evaluation methods for dialogue systems. Artiﬁcial Intel ligence\nReview 54, 755–810 (2020) https://doi.org/10.1007/s10462-020-09866-x\n[32] Adiwardana, D., Luong, M.-T., So, D.R., Hall, J., Fiedel, N., Thoppilan, R.,\nYang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., Le, Q.V.: Towards a Human-li ke\nOpen-Domain Chatbot. Preprint at http://arxiv.org/abs/2001.09977 (2020)\n34\n[33] Roller, S., Boureau, Y.-L., Weston, J., Bordes, A., Dinan, E., Fan, A., Gun-\nning, D., Ju, D., Li, M., Poﬀ, S., Ringshia, P., Shuster, K., Smith, E.M.,\nSzlam, A., Urbanek, J., Williamson, M.: Open-Domain Conversational Agents :\nCurrent Progress, Open Problems, and Future Directions. Preprin t at http:\n//arxiv.org/abs/2006.12442 (2020)\n[34] Vinyals, O., Le, Q.V.: A Neural Conversational Model. In: ICML Deep Lear ning\nWorkshop 2015, vol. 37 (2015)\n[35] Roller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott, M.,\nShuster, K., Smith, E.M., et al.: Recipes for building an open-dom ain chatbot.\nPreprint at http://arxiv.org/abs/2004.13637 (2020)\n[36] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshth a, A., Cheng,\nH.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al.: Lamda: Language models for\ndialog applications. Preprint at\nhttp://arxiv.org/abs/2201.08239 (2022)\n[37] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariw al, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,\nKrueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J. , Win-\nter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Ch ess, B., Clark,\nJ., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amode i, D.: Lan-\nguage models are few-shot learners. In: Larochelle, H., Ranzato, M., Hadsel l, R.,\nBalcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing S ys-\ntems, vol. 33, pp. 1877–1901. Curran Associates, Inc., Virtual (2020). Preprint\nat\nhttps://arxiv.org/abs/2005.14165\n[38] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P.,\nZhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton ,\nF., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J.,\nLowe, R.: Training language models to follow instructions with human f eedback.\nPreprint at http://arxiv.org/abs/2203.02155 (2022)\n[39] Do˘ gru¨ oz, A.S., Skantze, G.: How “open” are the conversations with open -domain\nchatbots? a proposal for speech event based evaluation. In: Proceedings of the\n22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue ,\npp. 392–402. Association for Computational Linguistics, Singapore and Online\n(2021).\nhttps://aclanthology.org/2021.sigdial-1.41\n[40] Goldsmith, D.J., Baxter, L.A.: Constituting relationships in tal k: A taxonomy\nof speech events in social and personal relationships. Human Communicat ion\nResearch 23(1), 87–114 (1996)\n[41] Clark, H.H.: Using Language. Cambridge university press, Cambridge, Uni ted\nKingdom (1996)\n35\n[42] Elgarf, M., Skantze, G., Peters, C.: Once upon a story: Can a creati ve story-\nteller robot stimulate creativity in children? In: Proceedings of the 21st ACM\nInternational Conference on Intelligent Virtual Agents, pp. 60–67 (2021)\n[43] Murali, P., Steenstra, I., Yun, H.S., Shamekhi, A., Bickmore, T. : Improv-\ning multiparty interactions with a robot using large language models. In :\nExtended Abstracts of the 2023 CHI Conference on Human Factors in Com-\nputing Systems. CHI EA ’23. Association for Computing Machinery, New Y ork,\nNY, USA (2023).\nhttps://doi.org/10.1145/3544549.3585602 . https://doi.org/\n10.1145/3544549.3585602\n[44] Billing, E., Ros´ en, J., Lamb, M.: Language models for human-robot inte raction.\nIn: Companion of the 2023 ACM/IEEE International Conference on Human-\nRobot Interaction. HRI ’23, pp. 905–906. Association for Computing Machinery,\nNew York, NY, USA (2023).\nhttps://doi.org/10.1145/3568294.3580040 . https:\n//doi.org/10.1145/3568294.3580040\n[45] Axelsson, A., Skantze, G.: Do you follow? a fully automated system for adap-\ntive robot presenters. In: Proceedings of the 2023 ACM/IEEE Internati onal\nConference on Human-Robot Interaction. HRI ’23, pp. 102–111. Association for\nComputing Machinery, New York, NY, USA (2023).\nhttps://doi.org/10.1145/\n3568162.3576958 . https://doi.org/10.1145/3568162.3576958\n[46] Kiesler, S.: Fostering common ground in human-robot interaction. In : ROMAN\n2005. IEEE International Workshop on Robot and Human Interactive Commu-\nnication, 2005., pp. 729–734. IEEE, Nashville, TN, USA (2005).\nhttps://doi.org/\n10.1109/ROMAN.2005.1513866\n[47] Fischer, K.: How People Talk with Robots: Designing Dialog to Reduc e User\nUncertainty. AI Magazine 32(4), 31–38 (2011) https://doi.org/10.1609/aimag.\nv32i4.2377\n[48] Sau-lai Lee, Ivy Yee-man Lau, Kiesler, S., Chi-Yue Chiu: Human Ment al Models\nof Humanoid Robots. In: Proceedings of the 2005 IEEE International Conferenc e\non Robotics and Automation, pp. 2767–2772. IEEE, Barcelona, Spain (2005).\nhttps://doi.org/10.1109/ROBOT.2005.1570532\n[49] Powers, A., Kramer, A., Lim, S., Kuo, J., Lee, S.-L., Kiesler, S.: C ommon ground\nin dialogue with a gendered humanoid robot. In: ROMAN 2005. IEEE Interna-\ntional Workshop on Robot and Human Interactive Communication, 2005. IEEE,\nNashville, TN, USA (2005)\n[50] Kendon, A.: Some functions of gaze-direction in social interaction. Ac ta Psycho-\nlogica 26, 22–63 (1967)\nhttps://doi.org/10.1016/0001-6918(67)90005-4\n[51] Skantze, G., Johansson, M., Beskow, J.: Exploring turn-taking c ues in multi-\nparty human-robot discussions about objects. In: Proceedings of the 2015 A CM\n36\non International Conference on Multimodal Interaction. ICMI ’15, pp. 67–74.\nAssociation for Computing Machinery, New York, NY, USA (2015).\nhttps://doi.\norg/10.1145/2818346.2820749\n[52] Mori, M., MacDorman, K.F., Kageki, N.: The uncanny valley [from the ﬁe ld].\nIEEE Robotics & Automation Magazine 19(2), 98–100 (2012) https://doi.org/\n10.1109/MRA.2012.2192811\n[53] Al Moubayed, S., Beskow, J., Skantze, G., Granstr¨ om, B.: Furhat : A back-\nprojected human-like robot head for multiparty human-machine inter action.\nIn: Esposito, A., Esposito, A.M., Vinciarelli, A., Hoﬀmann, R., M¨ uller, V.C.\n(eds.) Cognitive Behavioural Systems, pp. 114–130. Springer, Berlin, Hei delberg\n(2012)\n[54] Paetzel, M., Perugia, G., Castellano, G.: The persistence of ﬁrst impressions: The\neﬀect of repeated interactions on the perception of a social robot. In: P roceedings\nof the 2020 ACM/IEEE International Conference on Human-Robot Interaction.\nHRI ’20, pp. 73–82. Association for Computing Machinery, New York, NY, USA\n(2020).\nhttps://doi.org/10.1145/3319502.3374786\n[55] Phillips, E., Zhao, X., Ullman, D., Malle, B.F.: What is human-like ? decom-\nposing robots’ human-like appearance using the anthropomorphic robot (abot )\ndatabase. In: Proceedings of the 2018 ACM/IEEE International Conference\non Human-Robot Interaction. HRI ’18, pp. 105–113. Association for Comput-\ning Machinery, New York, NY, USA (2018).\nhttps://doi.org/10.1145/3171221.\n3171268\n[56] BigScience Workshop: Le Scao, Teven, et al.: BLOOM: A 176B-Parameter Ope n-\nAccess Multilingual Language Model. Preprint at https://arxiv.org/abs/2211.\n05100 (2023)\n[57] Komeili, M., Shuster, K., Weston, J.: Internet-Augmented Dial ogue Generation.\nPreprint at https://arxiv.org/abs/2107.07566 (2021)\n[58] Xu, J., Szlam, A., Weston, J.: Beyond Goldﬁsh Memory: Long-Term Open-\nDomain Conversation. Preprint at https://arxiv.org/abs/2107.07567 (2021)\n[59] Bickmore, T.W., Cassell, J.: Small talk and conversational storyte lling in embod-\nied conversational interface agents. In: AAAI Fall Symposium on Narrative\nIntelligence, Orlando, FL, USA, pp. 87–92 (1999)\n[60] Paradeda, R.B., Hashemian, M., Rodrigues, R.A., Paiva, A.: How facial exp res-\nsions and small talk may inﬂuence trust in a robot. In: Agah, A., Cabibihan,\nJ.-J., Howard, A.M., Salichs, M.A., He, H. (eds.) Social Robotics, pp. 169–178.\nSpringer, Cham (2016)\n[61] Babel, F., Kraus, J., Miller, L., Kraus, M., Wagner, N., Minker, W. , Baumann,\n37\nM.: Small talk with a robot? the impact of dialog content, talk initiative , and\ngaze behavior of a social robot on trust, acceptance, and proximity. Inte rnational\nJournal of Social Robotics 13(6), 1485–1498 (2021) https://doi.org/10.1007/\ns12369-020-00730-0\n[62] Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., Weston, J.: P ersonal-\nizing dialogue agents: I have a dog, do you have pets too? ACL 2018 - 56th\nAnnual Meeting of the Association for Computational Linguistics, Proceed ings\nof the Conference 1, 2204–2213 (2018)\nhttps://doi.org/10.18653/v1/p18-1205\n1801.07243\n[63] Heerink, M., Kr¨ ose, B., Evers, V., Wielinga, B.: Assessing Accep tance of\nAssistive Social Agent Technology by Older Adults: the Almere Model. I nterna-\ntional Journal of Social Robotics 2(4), 361–375 (2010) https://doi.org/10.1007/\ns12369-010-0068-5\n[64] Kuoppam¨ aki, S., Tuncer, S., Eriksson, S., McMillan, D.: Design ing Kitchen\nTechnologies for Ageing in Place: A Video Study of Older Adults’ Cooking at\nHome. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiqui tous\nTechnologies 5(2), 1–19 (2021) https://doi.org/10.1145/3463516\n[65] Sidnell, J., Stivers, T.: The Handbook of Conversation Analysis. Wi ley-\nBlackwell, West Sussex, UK (2012)\n[66] Krippendorﬀ, K.: Content analysis: An introduction to its metho dology (2019)\n[67] Radford, A., Kim, J.W., Xu, T., Brockman, G., McLeavey, C., Sutsk ever, I.:\nRobust Speech Recognition via Large-Scale Weak Supervision. Preprint at https:\n//arxiv.org/abs/2212.04356 (2022)\n[68] Benjamin, B.J.: Speech production of normally aging adults. In: Se minars in\nSpeech and Language, vol. 18, pp. 135–141. Thieme Medical Publishers, Inc.,\nNew York, NY, USA (1997)\n[69] Bernstein, B.: Social class, linguistic codes and grammatical ele ments. Language\nand speech 5(4), 221–240 (1962)\n[70] McLaughlin, M.L., Cody, M.J.: Awkward silences: Behavioral antece dent and\nconsequences of the conversational lapse. Human Communication Research 8(4),\n299–316 (1982)\nhttps://doi.org/10.1111/j.1468-2958.1982.tb00669.x\n[71] Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S. : On the dan-\ngers of stochastic parrots: Can language models be too big? In: Proceedin gs\nof the 2021 ACM Conference on Fairness, Accountability, and Transparency.\nFAccT ’21, pp. 610–623. Association for Computing Machinery, New York,\nNY, USA (2021).\nhttps://doi.org/10.1145/3442188.3445922 . https://doi.org/\n10.1145/3442188.3445922\n38\n[72] McCroskey, J.C., Teven, J.J.: Goodwill: A reexamination of the construct and\nits measurement. Communication Monographs 66(1), 90–103 (1999) https://\ndoi.org/10.1080/03637759909376464\n[73] Bartneck, C., Kuli´ c, D., Croft, E., Zoghbi, S.: Measurement in struments for the\nanthropomorphism, animacy, likeability, perceived intelligence, and perceived\nsafety of robots. International Journal of Social Robotics 1(1), 71–81 (2009)\n[74] Syrdal, D.S., Dautenhahn, K., Koay, K., Walters, M.: The negative attitudes\ntowards robots scale and reactions to robot behaviour in a live human-robot\ninteraction study. In: Adaptive and Emergent Behaviour and Complex Sy stems,\nEdinburgh, United Kingdom (2009)\n[75] Ekstedt, E., Skantze, G.: TurnGPT: a transformer-based language m odel\nfor predicting turn-taking in spoken dialog. In: Findings of the Assoc iation\nfor Computational Linguistics: EMNLP 2020, pp. 2981–2990. Association for\nComputational Linguistics, Online (2020).\nhttps://doi.org/10.18653/v1/2020.\nﬁndings-emnlp.268 . https://aclanthology.org/2020.ﬁndings-emnlp.268\n[76] Ekstedt, E., Skantze, G.: How much does prosody help turn-taki ng? inves-\ntigations using voice activity projection models. In: Proceedings of the 23rd\nAnnual Meeting of the Special Interest Group on Discourse and Dialogue, p p.\n541–551. Association for Computational Linguistics, Edinburgh, UK (2022).\nhttps://aclanthology.org/2022.sigdial-1.51\n[77] Danner, S.G., Krivokapi´ c, J., Byrd, D.: Co-speech movement i n conversational\nturn-taking. Frontiers in Communication 6 (2021) https://doi.org/10.3389/\nfcomm.2021.779814\n[78] Johansson, M., Skantze, G.: Opportunities and obligations to take tur ns in col-\nlaborative multi-party human-robot interaction. In: Proceedings of th e 16th\nAnnual Meeting of the Special Interest Group on Discourse and Dialogue,\npp. 305–314. Association for Computational Linguistics, Prague, Czech Repub-\nlic (2015).\nhttps://doi.org/10.18653/v1/W15-4642 . https://aclanthology.org/\nW15-4642\n[79] Shahverdi, P., Tyshka, A., Trombly, M., Louie, W.-Y.G.: Learning tu rn-taking\nbehavior from human demonstrations for social human-robot interactions.\nIn: 2022 IEEE/RSJ International Conference on Intelligent Robots and Sys-\ntems (IROS), Kyoto, Japan, pp. 7643–7649 (2022).\nhttps://doi.org/10.1109/\nIROS47612.2022.9981243\n[80] Yang, J., Wang, P., Zhu, Y., Feng, M., Chen, M., He, X.: Gated multimod al\nfusion with contrastive learning for turn-taking prediction in hum an-robot dia-\nlogue. In: ICASSP 2022 - 2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), Singapore, Singapore, pp. 7747–7751\n(2022).\nhttps://doi.org/10.1109/ICASSP43922.2022.9747056\n39\n[81] Skantze, G.: Turn-taking in conversational systems and human-robot interaction:\nA review. Computer Speech & Language 67, 101178 (2021) https://doi.org/10.\n1016/j.csl.2020.101178\n[82] Lee, N., Ping, W., Xu, P., Patwary, M., Fung, P.N., Shoeybi, M., Catanz aro,\nB.: Factuality enhanced language models for open-ended text generation. In:\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.)\nAdvances in Neural Information Processing Systems, vol. 35, pp. 34586–34599.\nCurran Associates, Inc., ??? (2022).\nhttps://proceedings.neurips.cc/paper ﬁles/\npaper/2022/ﬁle/df438caa36714f69277daa92d608dd63-Paper-Conference.pdf\n[83] Yao, Z., Li, C., Wu, X., Youn, S., He, Y.: A Comprehensive Study on Post-\nTraining Quantization for Large Language Models. Preprint at http://arxiv.org/\nabs/2303.08302 (2023)\n[84] Lala, D., Inoue, K., Kawahara, T.: Smooth turn-taking by a robot using an\nonline continuous model to generate turn-taking cues. In: 2019 Intern ational\nConference on Multimodal Interaction. ICMI ’19, pp. 226–234. Association for\nComputing Machinery, New York, NY, USA (2019).\nhttps://doi.org/10.1145/\n3340555.3353727\n[85] Moujahid, M., Hastie, H., Lemon, O.: Multi-party interaction with a r obot recep-\ntionist. In: 2022 17th ACM/IEEE International Conference on Human-Robot\nInteraction (HRI), pp. 927–931 (2022).\nhttps://doi.org/10.1109/HRI53351.2022.\n9889641\n[86] Kompatsiari, K., Ciardo, F., Tikhanoﬀ, V., Metta, G., Wykowska, A.: It ’s in\nthe eyes: The engaging role of eye contact in hri. International Journal of S ocial\nRobotics 13(3), 525–535 (2021)\n[87] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroi x, T.,\nRozi` ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A. , Grave,\nE., Lample, G.: LLaMA: Open and Eﬃcient Foundation Language Models.\nPreprint at\nhttp://arxiv.org/abs/2302.13971 (2023)\n[88] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,\nDiab, M., Li, X., Lin, X.V., Mihaylov, T., Ott, M., Shleifer, S., Shu ster, K.,\nSimig, D., Koura, P.S., Sridhar, A., Wang, T., Zettlemoyer, L.: OPT: O pen Pre-\ntrained Transformer Language Models. Preprint at http://arxiv.org/abs/2205.\n01068 (2022)\n[89] Holtzman, A., Buys, J., Du, L., Forbes, M., Choi, Y.: The Curious Case of Neural\nText Degeneration. Preprint at http://arxiv.org/abs/1904.09751 (2020)\n[90] Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., West on, J.: Neural Text\nGeneration with Unlikelihood Training. Preprint at http://arxiv.org/abs/1908.\n04319 (2019)\n40\n[91] Meister, C., Vieira, T., Cotterell, R.: Best-First Beam Search . Transactions of\nthe Association for Computational Linguistics 8, 795–809 (2020) https://doi.\norg/10.1162/tacl a 00346\n[92] Shi, W., Li, Y., Sahay, S., Yu, Z.: Reﬁne and Imitate: Reducing Re petition and\nInconsistency in Persuasion Dialogues via Reinforcement Learning and Human\nDemonstration. Preprint at http://arxiv.org/abs/2012.15375 (2022)\n[93] Irfan, B., Narayanan, A., Kennedy, J.: Dynamic emotional language adapta-\ntion in multiparty interactions with agents. In: Proceedings of the 20t h ACM\nInternational Conference on Intelligent Virtual Agents. IVA ’20. Associat ion for\nComputing Machinery, New York, NY, USA (2020). https://doi.org/10.1145/\n3383652.3423881\n[94] Lin, J., Ma, Z., Gomez, R., Nakamura, K., He, B., Li, G.: A review on inte ractive\nreinforcement learning from human social feedback. IEEE Access 8, 120757–\n120765 (2020) https://doi.org/10.1109/ACCESS.2020.3006254\n[95] Zhang, M., Choi, E.: SituatedQA: Incorporating extra-linguistic con texts into\nQA. In: Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 7371–7387. Association for Computational Linguistics,\nOnline and Punta Cana, Dominican Republic (2021).\nhttps://doi.org/10.18653/\nv1/2021.emnlp-main.586\n[96] McCloskey, M., Cohen, N.J.: Catastrophic interference in connec tionist net-\nworks: The sequential learning problem. Psychology of Learning and Moti-\nvation, vol. 24, pp. 109–165. Academic Press (1989).\nhttps://doi.org/10.\n1016/S0079-7421(08)60536-8 . https://www.sciencedirect.com/science/article/\npii/S0079742108605368\n[97] Irfan, B., Hellou, M., Belpaeme, T.: Coﬀee with a hint of data: Toward s\nusing data-driven approaches in personalised long-term interactions. Frontiers\nin Robotics and AI 8, 300 (2021) https://doi.org/10.3389/frobt.2021.676814\n[98] Parisi, G.I., Kemker, R., Part, J.L., Kanan, C., Wermter, S.: Con tinual lifelong\nlearning with neural networks: A review. Neural Networks 113, 54–71 (2019)\nhttps://doi.org/10.1016/j.neunet.2019.01.012\n[99] Jang, J., Ye, S., Yang, S., Shin, J., Han, J., KIM, G., Choi, S.J., Se o, M.:\nTowards continual knowledge learning of language models. In: Internati onal Con-\nference on Learning Representations, Virtual (2022). https://openreview.net/\nforum?id=vfsRB5MImo9\n[100] Irfan, B., Hellou, M., Mazel, A., Belpaeme, T.: Challenges of a real-w orld hri\nstudy with non-native english speakers: Can personalisation save the day? In:\nCompanion of the 2020 ACM/IEEE International Conference on Human-Robot\nInteraction. HRI ’20, pp. 272–274. Association for Computing Machinery, New\n41\nYork, NY, USA (2020). https://doi.org/10.1145/3371382.3378278\n[101] Oertel, C., Castellano, G., Chetouani, M., Nasir, J., Obaid, M., Pe lachaud, C.,\nPeters, C.: Engagement in human-agent interaction: An overview. Front iers in\nRobotics and AI 7 (2020) https://doi.org/10.3389/frobt.2020.00092\n[102] Whelan, S., Murphy, K., Barrett, E., Krusche, C., Santorelli, A., Casey, D.:\nFactors aﬀecting the acceptability of social robots by older adults incl uding\npeople with dementia or cognitive impairment: A literature review . Interna-\ntional Journal of Social Robotics 10(5), 643–668 (2018) https://doi.org/10.1007/\ns12369-018-0471-x\n[103] Irfan, B., Ortiz, M.G., Lyubova, N., Belpaeme, T.: Multi-modal op en world user\nidentiﬁcation. Transactions on Human-Robot Interaction 11(1) (2021) https:\n//doi.org/10.1145/3477963\n[104] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., Arx, S., Be rn-\nstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S.,\nCard, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J.Q., Dem-\nszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchem endy,\nJ., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Good-\nman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho,\nD.E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kall uri,\nP., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P.W., K rass, M.,\nKrishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Le skovec,\nJ., Levent, I., Li, X.L., Li, X., Ma, T., Malik, A., Manning, C.D., Mirchand ani,\nS., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., New-\nman, B., Nie, A., Niebles, J.C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L.,\nPapadimitriou, I., Park, J.S., Piech, C., Portelance, E., Potts, C. , Raghunathan,\nA., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., R´ e, C ., Sadigh,\nD., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R.,\nThomas, A.W., Tram` er, F., Wang, R.E., Wang, W., Wu, B., Wu, J., Wu, Y.,\nXie, S.M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang,\nX., Zhang, Y., Zheng, L., Zhou, K., Liang, P.: On the Opportunities and Risk s\nof Foundation Models. Preprint at\nhttp://arxiv.org/abs/2108.07258 (2022)\n[105] Weidinger, L., Uesato, J., Rauh, M., Griﬃn, C., Huang, P.-S., Mellor, J., Glaese,\nA., Cheng, M., Balle, B., Kasirzadeh, A., Biles, C., Brown, S., Kent on, Z.,\nHawkins, W., Stepleton, T., Birhane, A., Hendricks, L.A., Rimell, L. , Isaac,\nW., Haas, J., Legassick, S., Irving, G., Gabriel, I.: Taxonomy of risks pose d\nby language models. In: 2022 ACM Conference on Fairness, Accountability,\nand Transparency. FAccT ’22, pp. 214–229. Association for Computing Machin-\nery, New York, NY, USA (2022).\nhttps://doi.org/10.1145/3531146.3533088 .\nhttps://doi.org/10.1145/3531146.3533088\n42\n[106] Wu, Z., Galley, M., Brockett, C., Zhang, Y., Gao, X., Quirk, C., Konc el-\nKedziorski, R., Gao, J., Hajishirzi, H., Ostendorf, M., Dolan, B.: A c ontrollable\nmodel of grounded response generation. Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence 35(16), 14085–14093 (2021) https://doi.org/10.1609/aaai.\nv35i16.17658\n[107] Dziri, N., Madotto, A., Za¨ ıane, O., Bose, A.J.: Neural path hunter: Re ducing\nhallucination in dialogue systems via path grounding. In: Proceedings of the\n2021 Conference on Empirical Methods in Natural Language Processing, pp.\n2197–2214. Association for Computational Linguistics, Online and Punta Cana,\nDominican Republic (2021).\nhttps://doi.org/10.18653/v1/2021.emnlp-main.168\n. https://aclanthology.org/2021.emnlp-main.168\n[108] Gupta, P., Wu, C.-S., Liu, W., Xiong, C.: DialFact: A Benchmark for Fac t-\nChecking in Dialogue. Preprint at http://arxiv.org/abs/2110.08222 (2022)\n[109] Kahn, P.H., Freier, N., Friedman, B., Severson, R., Feldman, E.: Social and\nmoral relationships with robotic others?, pp. 545–550 (2004). https://doi.org/\n10.1109/ROMAN.2004.1374819\n[110] Smedegaard, C.V.: Reframing the role of novelty within social hri: from noise\nto information. In: 2019 14th ACM/IEEE International Conference on Human-\nRobot Interaction (HRI), Daegu, South Korea, pp. 411–420 (2019).\nhttps://doi.\norg/10.1109/HRI.2019.8673219\n[111] Irfan, B., Kennedy, J., Lemaignan, S., Papadopoulos, F., Senft, E., B elpaeme,\nT.: Social psychology and human-robot interaction: An uneasy marriage. In:\nCompanion of the 2018 ACM/IEEE International Conference on Human-Robot\nInteraction, pp. 13–20. ACM, Chicago, IL, USA (2018).\nhttps://doi.org/10.1145/\n3173386.3173389\n[112] Skantze, G., Do˘ gru¨ oz, A.S.: The Open-domain Paradox for Chatbots: Com mon\nGround as the Basis for Human-like Dialogue. Preprint at https://arxiv.org/\nabs/2303.11708 (2023)\n43",
  "topic": "Delusion",
  "concepts": [
    {
      "name": "Delusion",
      "score": 0.9336955547332764
    },
    {
      "name": "Robot",
      "score": 0.5800807476043701
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5635569095611572
    },
    {
      "name": "Psychology",
      "score": 0.4607667922973633
    },
    {
      "name": "Computer science",
      "score": 0.44282427430152893
    },
    {
      "name": "Human–computer interaction",
      "score": 0.39806729555130005
    },
    {
      "name": "Cognitive science",
      "score": 0.3730604350566864
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3355683386325836
    },
    {
      "name": "Psychiatry",
      "score": 0.12607327103614807
    },
    {
      "name": "Mathematics",
      "score": 0.09237122535705566
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86987016",
      "name": "KTH Royal Institute of Technology",
      "country": "SE"
    }
  ]
}