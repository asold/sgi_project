{
  "title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
  "url": "https://openalex.org/W4385570481",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2734668338",
      "name": "Luyu Gao",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2398480995",
      "name": "Zhuyun Dai",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A36597729",
      "name": "Panupong Pasupat",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2103119593",
      "name": "Anthony Chen",
      "affiliations": [
        "UC Irvine Health"
      ]
    },
    {
      "id": "https://openalex.org/A962403673",
      "name": "Arun Tejasvi Chaganty",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2098258132",
      "name": "Yicheng Fan",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2248838010",
      "name": "Vincent Zhao",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2145221253",
      "name": "Ni Lao",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2129755045",
      "name": "Hongrae Lee",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4258995210",
      "name": "Da-Cheng Juan",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2553037633",
      "name": "Kelvin Guu",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3159259047",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3171244865",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2889467844",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W4287855128",
    "https://openalex.org/W3173825754",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4385573912",
    "https://openalex.org/W4293138840",
    "https://openalex.org/W2998112083",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3169841173",
    "https://openalex.org/W4311731003",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963857245",
    "https://openalex.org/W3034188538",
    "https://openalex.org/W3035525293",
    "https://openalex.org/W4385859649",
    "https://openalex.org/W2970716846",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4225933709",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4226157795",
    "https://openalex.org/W3169283369",
    "https://openalex.org/W3128710690",
    "https://openalex.org/W3156012351",
    "https://openalex.org/W3170180819",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385573970",
    "https://openalex.org/W1647671624",
    "https://openalex.org/W3105753178",
    "https://openalex.org/W4205179624",
    "https://openalex.org/W3158724005",
    "https://openalex.org/W4285240908",
    "https://openalex.org/W4287854593",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W3099977667",
    "https://openalex.org/W3102645206",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W3153947101",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4221164017"
  ],
  "abstract": "Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, Kelvin Guu. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 16477–16508\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nRARR: Researching and Revising What Language Models Say,\nUsing Language Models\nLuyu Gao1⋄∗ Zhuyun Dai2∗ Panupong Pasupat2∗ Anthony Chen3⋄∗\nArun Tejasvi Chaganty2∗ Yicheng Fan2∗ Vincent Y. Zhao2 Ni Lao2\nHongrae Lee2 Da-Cheng Juan2 Kelvin Guu2∗\n1Carnegie Mellon University, 2Google Research, 3UC Irvine\nluyug@cs.cmu.edu anthony.chen@uci.edu\n{zhuyundai,ppasupat,arunchaganty,yichengfan,vzhao,nlao,hrlee,dacheng,kguu}@google.com\nAbstract\nLanguage models (LMs) now excel at many\ntasks such as question answering, reasoning,\nand dialog. However, they sometimes gener-\nate unsupported or misleading content. A user\ncannot easily determine whether their outputs\nare trustworthy or not, because most LMs do\nnot have any built-in mechanism for attribu-\ntion to external evidence. To enable attribution\nwhile still preserving all the powerful advan-\ntages of recent generation models, we propose\nRARR (Retrofit Attribution using Research and\nRevision), a system that 1) automatically finds\nattribution for the output of any text genera-\ntion model, and 2) post-edits the output to fix\nunsupported content while preserving the origi-\nnal output as much as possible. When applied\nto the output of several state-of-the-art LMs\non a diverse set of generation tasks, we find\nthat RARR significantly improves attribution\nwhile otherwise preserving the original input to\na much greater degree than previously explored\nedit models. Furthermore, the implementation\nof RARR requires only a handful of training ex-\namples, a large language model, and standard\nweb search.1\n1 Introduction\nGenerative language models (LMs) and other text\ngeneration models are now the backbone of many\nAI systems. For example, large language models\ncan perform multi-step reasoning (Nye et al., 2021;\nWei et al., 2022), generate plans (Ahn et al., 2022),\nuse tools and APIs (Shin et al., 2021; Thoppilan\net al., 2022), and answer open-domain questions\n(Petroni et al., 2019; Roberts et al., 2020).\nDespite these incredible advances, state-of-the-\nart LMs still frequently produce biased, misleading,\n∗Lead contributors. Please see Contributions section for\ndetails. ⋄Work done during an internship at Google Research.\n1We release open-source implementations of RARR, the\nevaluation pipeline, and the evaluation sets at https://\ngithub.com/anthonywchen/RARR.\nThe marathon world \nrecord is 2:01:39 2:01:09, \nset by Eliud Kipchoge of \nKenya in 2018 2022.\nResearch\n& Revision\nRevised output,  y Attribution report,  A\n[wikipedia.org] … Kipchoge is a \nKenyan long-distance runner …\n[npr.org] 2022 ... Kipchoge shaved \n30 seconds … to finish in 2:01:09\nAttribution score\nA → y Preservation score\nx → y\nHuman / automatic evaluation\n● Show the task definition\n○ orig → (revised, attrib_report)\n● Show the evaluation metric\n○ attribution\n○ preservation\n● Illustrate that one model should be able to handle \ndifferent generators, different tasks\nThe marathon world record is \n2:01:39, set by Eliud Kipchoge \nof Kenya in 2018.\nText generation model\nOriginal model \noutput,  x\nDocument \nCorpus\nFigure 1: The Editing for Attributiontask. The input\nx is a text passage produced by a generation model.\nOur Research & Revision model outputs an attribution\nreport A containing retrieved evidence snippets, along\nwith a revision y whose content can be attributed to the\nevidence in A while preserving other properties of x\nsuch as style or structure.\nor unsupported content, colloquially called “hal-\nlucinations” (Maynez et al., 2020; Menick et al.,\n2022). To make LMs more trustworthy, we want\nto justify each generation by an attribution report\n(Rashkin et al., 2021; Bohnet et al., 2022) that\ncontains supporting evidence from trusted sources\n(e.g., encyclopedia or articles) where appropriate.\nMost existing LMs, such as those based on\nsequence-to-sequence architectures, lack a built-\nin mechanism for attribution. Even retrieval-\naugmented models (Guu et al., 2020; Lewis et al.,\n2020), which retrieve relevant documents and then\ncondition on them to generate text, still do not\nguarantee attribution. Prior work has shown that\n16477\nretrieval-augmented models generate text that ei-\nther includes additional information outside the\nretrieved documents (Dziri et al., 2022), ignores\nthe documents altogether (Krishna et al., 2021),\nor even contradicts the documents (Longpre et al.,\n2021). In fact, occasionally ignoring the retrievals\ncan make the models more robust to bad retrievals\n(Khandelwal et al., 2020), illustrating that end-task\nperformance and attribution are not always aligned.\nInstead of constraining LMs to generate at-\ntributed text, we propose a model-agnostic ap-\nproach to improve the attribution of any existing\nLM: Retrofit Attribution using Research and Revi-\nsion (RARR). The approach is inspired by works on\nfact-checking2 where simple research-and-revise\nworkflows are effective at attributing or correct-\ning unattributed claims made by humans (Thorne\net al., 2018; Schuster et al., 2021; Thorne and Vla-\nchos, 2021). As shown in Figure 1, after gener-\nating text with the LM, RARR does research to\nretrieve relevant evidence, and then revises the text\nto make it consistent with the evidence while pre-\nserving qualities like style or structure, enabling\nthe revised text to be seamlessly used in place of\nthe original. RARR can be viewed as a retrieval-\naugmented model where retrieval happens after\ngeneration rather than before. This allows RARR\nto stand on the shoulders of giant LMs without\nhaving to modify them to support attribution.\nIn our effort to expand the scope of Research &\nRevision models to handle the output of arbitrary\nLMs, we make the following contributions. First,\nwe formalize the Editing for Attribution task and\npropose new metrics that evaluate revision models\nnot just on their ability to produce well-attributed\nrevisions, but also on their ability to otherwise pre-\nserve original properties of the text. Second, we\nuse these metrics to benchmark how existing re-\nvision models perform on various types of LM\noutputs such as knowledge-intensive statements,\nreasoning chains, and dialog responses. Finally,\nwe find that existing revision models do not al-\nways generalize across many tasks (and were not\noriginally intended to), and therefore propose a new\nresearch-and-revise model that leverages the power\nof few-shot prompting in large language models to\nrobustly generalize across domains.\n2In this paper, we generally avoid the term “fact-checking”\nother than to reference relevant literature, because we only\naddress attribution, and attribution does not entail correctness.\nEven if a claim is attributed to a particular source, it does not\nguarantee that the source is “correct” (Menick et al., 2022).\nQuery Generation\nWhen did Millie \nInbetween premiere?\nWhat channel was Millie \nInbetween on?\nRetrieval Retrieval\n[fandom.com]\n… the first series \npremiered on\n1 October 2014.\n[comedy.co.uk]\nMillie Inbetween. \nCBBC sitcom \nabout a young …\nAgreement Agreement Output Attribution \nReport A={e1, .., eM}\nEdit\nCombined figure v1d\nEdit skipped\n[fandom.com]\n… the first series \npremiered on\n1 October 2014.\n[comedy.co.uk]\nMillie Inbetween. \nCBBC sitcom \nabout a young …\nq1 qN\n[comedy.co.uk]\nMillie Inbetween. \nCBBC sitcom \nabout a young …\n[fandom.com]\n… the first series \npremiered on\n1 October 2014.\n{e1,j} {eN,j}\nMillie Inbetween \npremiered on 24 \nFebruary 2014 \non CBBC.\nInput Passage x\nMillie Inbetween \npremiered on 1 \nOctober 2014 \non CBBC.\nMillie Inbetween \npremiered on 1 \nOctober 2014 \non CBBC.\nOutput Passage y\nFigure 2: An overview of RARR,which improves attri-\nbution for a text passage viaResearch & Revision. Given\nthe input text passage, the research stage uses a query\ngenerator to raise questions about different aspects of\nthe text. The retriever then searches for evidence to\ninvestigate each query. The revision stage first runs an\nagreement model to detect disagreement between the\ntext and the evidence, then runs an edit model to revise\nthe text if needed. Finally, M evidence snippets are\nselected to form an attribution report.\n2 Task formulation\nWe propose the task of Editing for Attribution as\nfollows. As Figure 1 shows, the input to the system\nis a text passage x produced by a generation model.\nThe output is a revised text passage y along with\nan attribution report A, which contains evidence\nsnippets e1, . . . , eM that support the content in y.\nOptionally, the attribution report can contain addi-\ntional information such as the alignment between\nevidence snippets and relevant parts in y.\nWe propose to measure the quality of the revised\ntext y and attribution report A along two dimen-\nsions: (1) attribution: how much of the revised\ntext y can be attributed to the evidence in A, and\n(2) preservation: how much the revised text y\npreserves aspects of the original text x.\n2.1 Measuring attribution\nPreviously, Rashkin et al. (2021) proposed At-\ntributable to Identified Sources (AIS), a human\nevaluation framework which considers a binary no-\ntion of attribution. Roughly speaking, a text pas-\nsage y is attributable to a set A of evidence if a\n16478\ngeneric hearer would affirm the statement “Accord-\ning to A, y” under the context ofy. A system either\nreceives full credit (1.0) if all content in y can be\nattributed to A, and no credit (0.0) otherwise.\nWe propose a more fine-grained, sentence-level\nextension of AIS. We ask annotators to give an AIS\nscore for each sentence s of y, and then report the\naverage AIS score across all sentences:\nAttrAIS(y, A) = avg\ns∈y\nAIS(s, A). (1)\nSince the AIS score is binary, this effectively mea-\nsures the percentage of sentences in y that are fully\nattributed to A. When judging each sentence, we\nalso give annotators access to the surrounding sen-\ntences and other necessary context, such as the\nquestion that the text passage responded to. We\nalso impose the maximum number of evidence\nsnippets in the attribution report A to make it con-\ncise enough for both the annotator and downstream\nusers. By manually inspecting 30 examples from\nour benchmarks, we found M = 5snippets to be\nsufficient for full attribution.\nDuring model development, we define an au-\ntomated metric, auto-AIS (Attrauto), that approxi-\nmates human AIS judgments. We utilize the natural\nlanguage inference (NLI) model from Honovich\net al. (2022), which correlates well with AIS scores.\nFor each sentence s of y, and for each evidence\nsnippet e in A, let NLI(e, s) be the model probabil-\nity of e entailing s. We then define\nAttrauto(y, A) = avg\ns∈y\nmax\ne∈A\nNLI(e, s). (2)\nTo improve accuracy, we decontextualize (Choi\net al., 2021) each sentence based on the entire con-\ntext of y before computing the scores. See Ap-\npendix B for implementation details.\n2.2 Measuring preservation\nTo measure preservation, we first ask annotators to\ndecide if the revision preserves the text’s original\nintent (completely, somewhat, or not at all — see\nAppendix C for exact rubrics). Like AIS evalua-\ntion, we give annotators the necessary surrounding\ncontext. We define the binary metricPresintent(x, y)\nto be 1.0 if the revision completely preserves the\noriginal intent, and 0.0 otherwise.\nHowever, even if a revision preserves intent, it\nmay still make superfluous modifications, such as\nreordering words, changing textual style, or includ-\ning unnecessary additional information (Thorne\nand Vlachos, 2021). Different tasks have differ-\nent requirements for what should be preserved.\nHere, we desire a simple metric that can be readily\ncomputed for many tasks and that generally penal-\nizes unnecessary changes. We thus define a metric\nbased on the character-level Levenshtein edit dis-\ntance (Levenshtein, 1965) between x and y:\nPresLev(x, y) = max\n(\n1 −Lev(x, y)\nlength(x), 0\n)\n(3)\nThis metric is 1.0 if x and y are the same, and 0.0\nif y completely overwrites all parts of x. PresLev is\ngenerally sensitive to any kind of change, but cer-\ntainly does not capture all notions of preservation\n(e.g., preserving rhyme schemes or puns).\nWe want the revision to preserve the original\nintent while avoiding superfluous edits. To reflect\nthis, we finally combine the two metrics as\nPrescomb(x, y) =Presintent(x, y) ·PresLev(x, y).\n(4)\nwhich is 0.0 if the revision changes the intent and\nequal to PresLev(x, y) otherwise. Since Presintent\nrequires human annotation, we use PresLev as an\nautomated metric for model development.\n2.3 Discussion\nOptimizing for attribution alone cannot ensure a\ngood revision: for example, an adversarial editor\ncould ensure 100% attribution by simply replacing\nthe input x with the text of any arbitrary retrieved\ndocument, which is trivially attributable to itself.\nIdeally, we want to maximize both attribution and\npreservation, while navigating any tradoffs between\nthe two. In our experiments, we report both metrics,\nas well as their harmonic mean (F1AP, analogous\nto how recall and precision are combined in F1).\nWe emphasize that this evaluation scheme does\nnot require any “gold” or “reference” edits (unlike\nmany prior evaluations of text revision models),\nwhich are often only available for specialized do-\nmains. This enables us to broaden the scope to a\nmuch wider range of generation tasks.\n3 Approach\nWe now presentRetrofit Attribution using Research\nand Revision (RARR), a simple method for solving\nthe Editing for Attribution task. As illustrated in\nFigure 2, given an input passage x, the research\nstage first generates a set of queries {q1, ..., qN },\neach investigating one aspect of x that potentially\n16479\n(a)Query generationx→ {q1, . . . , qN}\nYou said: Your nose switches back and forth between nostrils. When you\nsleep, you switch about every 45 minutes. This is to prevent a buildup of\nmucus. It’s called the nasal cycle.\nTo verify it,\na)I googled:Doesyournoseswitchbetweennostrils?\nb)I googled:Howoftendoesyournostrilsswitch?\nc)I googled:Whydoesyournostrilswitch?\nd)I googled:Whatisnasalcycle?\n(b)Agreement model(y, q, e) → {0,1}\nYou said: Your nose switches . . . (same as above). . . nasal cycle.\nI checked: How often do your nostrils switch?\nI found this article: Although we don’t usually notice it, during the nasal\ncycle one nostril becomes congested and thus contributes less to airflow,\nwhile the other becomes decongested. On average, the congestion pattern\nswitches about every 2 hours, according to a small 2016 study published\nin the journal PLOS One.\nYournose’sswitchingtimeisaboutevery2hours,not45minutes.\nThisdisagreeswithwhatyousaid.\n(c)Edit model(y, q, e) →newy\nYou said: Your nose switches . . . (same as above). . . nasal cycle.\nI checked: How often do your nostrils switch?\nI found this article: Although we . . . (same as above). . . PLOS One.\nThissuggests45minutesswitchtimeinyourstatementiswrong.\nMyfix:Yournoseswitchesbackandforthbetweennostrils.Whenyou\nsleep,youswitchaboutevery2 hours.Thisis topreventa buildupof\nmucus.It’scalledthenasalcycle.\nFigure 3: Examples of few-shot examples used to\nprompt the PaLM model (blue = input; red = output).\nrequires attribution. For each query qi, it retrieves\nweb documents and selects the best evidence snip-\npets {ei1, ei2, . . .}. The revision stage then re-\nvises the original text x using the retrieval results\n{(q1, e11), . . .}, yielding a revised text y.\nMost components for RARR are implemented\nusing few-shot prompting (Brown et al., 2020). We\nuse PaLM (Chowdhery et al., 2022) as our language\nmodel. Figure 3 shows some few-shot examples\nwe use, while Appendix D lists the full prompts.\n3.1 Research stage\nQuery generation We perform comprehensive\nquestion generation (CQGen) which produces a\nsequence of questions covering all aspects of the\npassage x that need to be verified and attributed.\nA similar strategy has been employed to train text-\nplanning models (Narayan et al., 2022). A prompt\nwith six human demonstrations was sufficient for\nPaLM to adequately learn the task. To increase\ndiversity and coverage, we sample from our CQ-\nGen model three times and take the union of the\nresulting queries.\nEvidence retrieval For each query from CQGen,\nwe use Google Search to retrieveK = 5web pages.\nWe extract candidate evidence snippets from each\nweb page by running a sliding window of four\nsentences across the page, breaking at document\nheadings. The evidence snippets for each query\nare then ranked based on their relevance to the\nquery. For this, we use an existing query-document\nrelevance model trained following Ni et al. (2021),\nwhich computes a relevance score Srelevance(q, e)\nbetween a query q and an evidence snippet e. We\nthen keep the top J = 1evidence for each query.\nThe final retrieval result is[(q1, e11), . . . ,(q1, e1J),\n. . . ,(qN , eN1), . . . ,(qN , eNJ )], where eij denotes\nthe jth evidence for the ith query, and N denotes\nthe total number of queries from CQGen (which\ncan be different for each input x).\n3.2 Revision stage\nAfter retrieving evidence, certain parts of x may\nnow be properly attributed, but other parts remain\nunattributed and should be revised. As illustrated\nin Figure 2, the revision stage initializes the output\ny = x. Then for each retrieved (q, e) = (qi, eij),\nthe agreement model checks if the evidence e dis-\nagrees with the current output y regarding the issue\nin query q. If a disagreement is detected, the edit\nmodel edits y to agree with e; otherwise, it does\nnothing. The process continues until all retrievals\nare processed.\nAgreement model The agreement model takes\nthe partially edited passage y, a query q, and the\nevidence e as input. It then decides whether both\ny and e imply the same answer to the question in\nq. This form of question-guided agreement was\npreviously explored by Honovich et al. (2021). We\nimplement this by few-shot prompting PaLM using\na chain-of-thought style prompt (Wei et al., 2022),\nwhere we ask the model to explicitly state the im-\nplied answers for both y and e before producing its\njudgment about their agreement.\nEdit model The edit model is run only if a dis-\nagreement is detected. The model takes y, q and e\nas input, and outputs a new version of y that aims\nto agree with e while otherwise minimally altering\ny. We again use few-shot prompting and chain-of-\nthought, where we ask the model to first identify a\nparticular span in y that needs to be edited before\ngenerating the revised y. This helps reduce the\neditor’s deviation from the current y.3\n3The editor occasionally produces large edits that bring the\nnew revision close to e but far from the current y. Since this\nis rarely desirable, we reject edits with edit distance above 50\ncharacters or 0.5 times the original text length.\n16480\n3.3 Attribution report\nFinally, we select at mostM = 5evidence snippets\nto form an attribution reportA. Note that during ev-\nidence retrieval and revision, we may have encoun-\ntered and used more than M snippets. Our goal is\nto find a subset of snippets that maximizes cover-\nage over the potentially attributable points in the\npassage, as represented by the queries q1, . . . , qN .\nWe use the relevance model from Section 3.1 as\na proxy for measuring how much an evidence e\ncovers the point raised by a query q. Then, we ex-\nhaustively search for A ⊆{e11, . . . , eNJ }of size\nat most M that maximizes\nCover(A, q1:N ) :=\nN∑\ni=1\nmax\ne∈A\nSrelevance(qi, e). (5)\n4 Related work\nFact-checking Our research builds upon works\nto identify whether a claim is supported or refuted\nby the given evidence (Thorne et al., 2018; Wang,\n2017; Karadzhov et al., 2017; Augenstein et al.,\n2019; Wadden et al., 2020). In real-world scenar-\nios such as the one which RARR operates in, rele-\nvant evidence may not be provided, necessitating\nretrieval (Fan et al., 2020; Piktus et al., 2021).\nPost-hoc editing for factuality Recent work has\ngone beyond checking the validity of a claim to\ncorrecting a piece of text to be factually consistent\nwith a set of evidence via post-hoc editing (Shah\net al., 2020; Thorne and Vlachos, 2021; Schuster\net al., 2021; Balachandran et al., 2022; Cao et al.,\n2020; Iso et al., 2020). FRUIT (Logan IV et al.,\n2022) and PEER (Schick et al., 2022) both im-\nplement an editor that is fine-tuned on Wikipedia\nedit history with the goal of updating outdated in-\nformation and collaborative writing respectively.\nEvidence-based Factual Error Correction (EFEC;\nThorne and Vlachos, 2021) also implements a full\nresearch-and-revise workflow trained on Wikipedia\npassages (Thorne et al., 2018). A key differentia-\ntor of RARR is its ability to edit the output of any\ngeneration model without being restricted by the\ndomain, task, or the need for training data.\nMeasuring attribution A key part of improving\nattribution is being able to quantify it. Apart from\nhuman evaluation (Rashkin et al., 2021), several\nautomated evaluation methods have been proposed.\nOur work uses an entailment-based metric, which\nmeasures whether the referenced evidence entails\nPaLM outputs on NQ (factoid statements)\nMillie Inbetween is a British comedy television series. It premiered on\n24 February 2014 on BBC One. The first series was produced by John\nYorke and Phil Clymer.\nPaLM outputs on SQA (reasoning chains)\nThe highest point of Mount Wycheproof is 70 metres. Edmund Hillary\nclimbed Mount Everest, which is 8,848 metres. So Mount Wycheproof\nwould be a breeze for Edmund Hillary.\nLaMDA outputs on QReCC (knowledge-intensive dialogs)\nWhen was Welsh social reformer Robert Owen born?\n\ncontextRobert Owen was born on 14 May 1771\n. . .\nDid he have another job?\nIn 1810 he moved to Manchester and established a draper’s shop.\nFigure 4: Examples of input passages. For QReCC,\nprior dialog turns are also given as the context.\nthe output text (Bohnet et al., 2022; Kryscinski\net al., 2020; Goyal and Durrett, 2021). A common\nalternative is to evaluate whether the output text\ncontains the same factual information as the ev-\nidence; e.g., by checking if both yield the same\nanswer to the same question (Wang et al., 2020).\nWe use this notion of attribution in RARR’s agree-\nment model rather than for evaluation.\nRetrieval-augmented models Models with a re-\ntrieval component have seen successes in ques-\ntion answering (Chen et al., 2017; Lee et al.,\n2019; Nakano et al., 2021), machine translation\n(Zhang et al., 2018), code generation (Hayati\net al., 2018), language modeling (Khandelwal et al.,\n2020), and other knowledge-intensive tasks (Lewis\net al., 2020). Their retrievals are not necessarily at-\ntributions (Dziri et al., 2022; Longpre et al., 2021)\nand typically are not used to revise an existing out-\nput. An exception is LaMDA (Thoppilan et al.,\n2022), a language model for dialog that performs\nrevision by training on human annotations.\n5 Experiments\n5.1 Evaluation setups\nRARR aspires to be a general-purpose method for\nimproving the attribution of any text generation\nmodel in any text domain. We thus construct eval-\nuation benchmarks by taking the task input from\nthree diverse datasets, and prompting different gen-\neration models to produce long-form outputs which\nmay contain “hallucinations,” as demonstrated in\nFigure 4. These long-form outputs serve as input\ntext passages to RARR. We generate 150 develop-\nment and 150 test passages for each combination\nof generation model and source dataset.\n16481\nFactoid statements We prompt PaLM 540B and\nGPT-3 text-davinci-002 to generate long-form an-\nswers to questions from the Natural Questions dev\nset (NQ; Kwiatkowski et al., 2019). The result-\ning passages are mostly coherent but often contain\nfactual errors. This setup examines the ability to\nattribute a diverse range of factoid knowledge.\nReasoning chains Language models can gener-\nate reasoning chains to answer complex questions\n(Wei et al., 2022). We use PaLM and GPT-3 to gen-\nerate reasoning chains for the StrategyQA train set\n(SQA; Geva et al., 2021). This setup tests whether\nthe revision model can provide better attribution for\nintermediate steps of reasoning, while preserving\nthe overall reasoning process.\nKnowledge-intensive dialogs We consider the\nconversational QA task from the QReCC dev set\n(Anantha et al., 2021). Given the previous dia-\nlog turns, which are rounds of questions and an-\nswers (Q1, A1, Q2, A2, . . . , Qk), we use LaMDA\nand GPT-3 to answer to the final questionQk condi-\ntioned on the dialog history. The answer tends to be\ncontext-dependent, featuring pronouns and implicit\nreferences. All dialog turns are given alongside the\nanswer as inputs to the revision model.\n5.2 Models\nWe compare RARR to several systems that have a\nresearch-and-revise workflow.\nEFEC We consider EFEC (Thorne and Vlachos,\n2021) as a representative fine-tuned editor. EFEC\nfine-tunes a T5-based model to revise text condi-\ntioned on multiple evidence snippets using both\nsemi-supervised and fully-supervised approaches.\nWe compare against their fully-supervised ap-\nproach, which performed best in their experiments.\nEFEC uses a neural retrieval model (Karpukhin\net al., 2020) to retrieve from Wikipedia; however,\nnot all passages in our experiments are supported\nby Wikipedia articles. To more fairly compare the\nediting capabilities of EFEC, we instead use the\nevidence retrieved by our research stages (CQGen\nand web search). Note that the EFEC editor condi-\ntions on multiple pieces of evidence at once, while\nour editor iteratively conditions on one at a time.\nLaMDA LaMDA (Thoppilan et al., 2022) gener-\nates responses in three steps: 1) generate a “base\nresponse”; 2) generate search queries from the base\nresponse; 3) generate a “revised response” condi-\ntioned on the base response and retrieved evidence.\nAttribution Preservation\nModel auto-AIS AIS intent Lev comb F1AP\nPaLM outputs on NQ\nEFEC 45.6 →64.3 35.4→48.3 16.0 39.1 10.4 17.1\nLaMDA 39.5→49.9 18.3→30.4 26.0 39.6 21.1 24.9\nRARR 45.6→54.9 35.4→43.4 90.0 89.6 83.1 57.0\nPaLM outputs on SQA\nEFEC 37.8 →58.6 24.5→51.7 6.0 31.0 3.8 7.1\nLaMDA 32.7→43.2 15.8→27.0 40.0 46.4 33.7 30.0\nRARR 37.6→45.1 24.5→31.5 92.6 89.9 84.6 45.9\nLaMDA outputs on QReCC\nEFEC 19.1 →47.4 13.2→48.7 39.7 39.4 23.7 31.9\nLaMDA 16.4→36.2 16.0→27.1 21.3 24.8 12.0 16.6\nRARR 18.8→29.4 13.2→28.3 95.6 80.2 78.1 41.5\nTable 1: Evaluation results. For attribution, we report\nthe AIS scores of the texts both before and after editing\n(before →after). For preservation, we report intent\npreservation Presintent, Levenshtein similarity PresLev,\nand the combined Prescomb. We summarize AttrAIS and\nPrescomb using their harmonic mean (F1AP).\nTo apply LaMDA on a given textx, we simply set\nthe base response in step 1 to x, and then run steps\n2 and 3 (we call these latter two stages “LaMDA\nResearch”). LaMDA was trained as a dialog sys-\ntem, and always expects a dialog context where\nthe user speaks first. So, for non-dialog tasks, we\ninsert an artificial user utterance as dialog history:\n“Tell me something interesting. ”For the attribution\nreport, we take all evidence documents retrieved\nby LaMDA during its research process.\nRARR Our model uses few-shot prompting on\nPaLM 540B for query generation, the agreement\nmodel, and the edit model. We use the same\nprompts for all tasks except when the context\ncomes from a dialog, where we slightly modify the\nprompts to use the dialog context (e.g., CQGen now\nmaps dialog context + x to queries). The query-\nevidence relevance model Srelevance is a pretrained\nT5-large model (Raffel et al., 2020) fine-tuned fol-\nlowing Ni et al. (2021) on MS MARCO (Nguyen\net al., 2016). See Appendix D for the few-shot\nprompting strategies and more modeling details.\n5.3 Results\nFor the main experiments, we report results on pas-\nsages generated by PaLM and LaMDA. Results on\nGPT-3 passages show similar trends (Appendix A).\nTable 1 and Figure 5 show attribution and preser-\nvation results for each model and dataset. We also\nreport F1AP, the harmonic mean of the two metrics,\nwhich is shown as level curves in Figure 5.\n16482\n0 25 50 75 100\n0\n20\n40\n60\n80\n100Attribution (AIS)\nPaLM outputs on NQ\nEFEC\nLaMDA\nRARR\n0 25 50 75 100\nPreservation (Prescomb)\nPaLM outputs on SQA\n0 25 50 75 100\nLaMDA outputs on QReCC\nFigure 5: Attribution and preservation scores.\nDashed lines indicate the highest attribution score ob-\ntained by any of the models before editing: points above\nthe line have better attribution after revision. The con-\ntours are F1AP level curves: points along a contour have\nequivalent F1AP. Different models make very different\ntrade-offs between attribution and preservation. Only\nRARR has a robust F1AP across all tasks.\nRARR significantly improves attribution while\npreserving most of the original text. In terms of\nF1AP, RARR is the only method that performs ro-\nbustly across all three datasets, and significantly\noutperforms prior methods on NQ and SQA.\nWe found that RARR is the only method that pre-\nserves the original intent of x over 90% of the time\n— EFEC and LaMDA only manage to preserve the\noriginal intent 6–40% of the time. We also see that\nediting is crucial to improve attribution: if we only\nretrieve evidence to support the original response\nx without editing, attribution ranges from the low\n10s to mid 30s. After editing, RARR can increase\nattribution by up to 13% absolute, while changing\nonly 10–20% of the text.\nAs noted in Section 2, one can sacrifice preserva-\ntion for higher attribution. EFEC is able to obtain\nstrong F1AP on QReCC by making larger changes\nto the text in exchange for a higher attribution\nscore. However, it occupies a very different point\nfrom RARR on the attribution-preservation trade-\noff curve, as visualized in Figure 5.\n6 Analysis\n6.1 Qualitative analysis\nHuman oracle To understand the remaining\nheadroom in our task, we ask: what is the minimal\namount of editing needed to make a text passage\nfully attributed? The answer would depend on the\nquality of the LM that generated the text as well as\nthe task difficulty. As an approximation, we man-\nually edited 30 examples in our NQ benchmark\nuntil we judged them to be 100% attributable. We\nachieved a preservation score of 88%, which (when\ncombined with 100% attribution) translates to 93.6\nF1AP, indicating a significant headroom.\nx: Justice Ashok Kumar Mathur headed the 7th central pay com-\nmission in India. It was created in 2014 and submitted its\nreport in2016.\nAttribution:50% Preservation:100%\nEFEC:The 7th central pay commission in India was created in 2014.\nAttribution:100% Preservation:0%\nLaMDA:I heard the 7th CPC made recommendations for increasing the\nminimum salary pay from Rs 7066 to 18k per month for new\ncentral government employees.\nAttribution:0% Preservation:0%\nRARR:Justice Ashok Kumar Mathur headed the 7th central pay com-\nmission in India. It was created in 2014 and submitted its\nreport in2015.\nAttribution:100% Preservation:100%\nevidence:The 7th Central Pay Commission (Chair: Justice A. K.\nMathur)submitted its report on November 19, 2015. The\nCommission had beenappointed in February 2014, to look\nat remuneration for central government employees. . . .\nFigure 6: Example model outputs and human judg-\nment of their attribution and preservation scores.\nEFEC reduces the passage x into a single sentence.\nLaMDA changes the writing style. RARR preserves\nthe structure of the input passage. We show one evi-\ndence retrieved by RARR to help explain the example.\nAnalyzing the baselines As exemplified in Fig-\nure 6, EFEC frequently attempts to summarize the\nentire passage into one sentence, or drops later\nsentences. This is likely due to EFEC’s training\ndata, which was limited to single sentences. This\nbehavior generally increases the attribution score,\nbecause it is usually easier to make one sentence\nfully attributable than many sentences. However,\nin datasets where the claim contains multiple sen-\ntences (NQ and SQA), such a behavior yields low\npreservation scores, and also results in outputs that\nare less informative. We expect that EFEC could\nperform much better if its training data were aug-\nmented to include multiple sentences. LaMDA Re-\nsearch achieves similar attribution scores to RARR.\nBut as mentioned in Section 5.2, the intent and\nlinguistic style of the output tend to deviate from\nthe input, resulting in lower preservation scores\n(Figure 6). We emphasize that this is not a purely\napples-to-apples comparison since LaMDA was\nnot optimized for preservation. Overall, these ex-\nperiments are mainly meant to illustrate that prior\nmodels were simply not designed for the task of\nEditing for Attribution, rather than to mark RARR\nas the best method.\nAnalyzing RARR For the research stage, the\nquestion generation model had comprehensive cov-\nerage: a manual inspection of 40 examples shows\n> 80% with questions that fully cover all aspects\nof the input text. The retriever was strongest at re-\n16483\n(a)Correctly revising an entity\ny: If She Knew What She Wants was written byHenry Roth.\ne: [en.wikipedia.org] “If She Knew What She Wants” is a song written by\nAmerican singer-songwriterJules Shearand introduced on . . .\ny′: If She Knew What She Wants was written byJules Shear.\n(b)Correctly revising a number\ny: God Save the Queen became the British national anthem in1745. . . .\ne: [www.britannica.com] The oldest national anthem is Great Britain’s “God\nSave the Queen,” which was described as a national anthem in1825, . . .\ny′: God Save the Queen became the British national anthem in1825. . . .\n(The year 1745 was when the song was first performed.)\n(c)Performing a necessary larger revision\ny: “It’s My Party” is a song written and composed by American singer-\nsongwriter and producerWalter Gold. The song was published in1962\nand first recorded by American singer-songwriter,producer and arranger\nLesley Gore.\ne: [secondhandsongs.com] Song: It’s My Party written byJohn Gluck, Wally\nGold, Seymour Gottlieb, Herbert Weiner. . .\ny′: “It’s My Party” is a song written and composed byJohn Gluck, Wally Gold,\nSeymour Gottlieb and Herb Weiner. The song was published in1963and\nfirst recorded by AmericansingerLesley Gore.\n(d)Confused by misleading evidence\ny: In the Heat of the Night is a police procedural crime drama TV series produced\nby MGM Television. It aired onNBCfrom September 8, 1988 to May 3,\n1992. . . .\ne: [metv.com] Watch In the Heat of the Night onMeTV.\ny′: In the Heat of the Night is a police procedural crime drama TV series produced\nby MGM Television. It aired onMeTVfrom September 8, 1988 to May 3,\n1992. . . .\n(The show did actually air on NBC in the 1980s, but the model fails to realize\nthat MeTV is a re-run much later in time.)\n(e)Improved attribution, but reasoning doesn’t adapt to new info\n(Q: Does Homer Simpson need two hands worth of fingers to count to 5?)\ny: Homer Simpson has5fingers and 2 hands. Therefore, he does not need two\nhands worth of fingers to count to 5. He only need one hand to count to 5.\ne: [refinery29.com] Other iconic animated characters, such as Homer Simpson\nand his brood, . . . only havefour fingers, too.\ny′: Homer Simpson has4fingers and 2 hands. Therefore, he does not need two\nhands worth of fingers to count to 5. He only need one hand to count to 5.\n(Having only 4 fingers per hand, he does need two hands to count to 5.)\nFigure 7: Example revisions from RARR, both good\nand bad. y = partially edited passage; e = evidence; y′\n= passage after editing with e.\nsearching content involving distinct entities (e.g., a\nmovie, a major event, or a person). In contrast, we\nfound significant headroom for better attribution\nof statements involving generic objects and more\nabstract claims (e.g. “Video games require elec-\ntricity. ”— since this is obvious to most humans, re-\ntrieved articles from the web tend to address related\nbut different topics). We suspect that a significant\namount of attribution headroom on our benchmarks\nwould benefit from a better research stage.\nFor the revision stage, RARR was able to revise\nmany unattributed claims, especially those involv-\ning entities and numbers (Figures 7a and 7b). It can\nalso perform larger revisions when necessary (Fig-\nure 7c). Moreover, RARR abstains from editing\nwhen the claim is already well-attributed: on NQ,\namong the inputs with near-perfect attribution (pre-\nedit AttrAIS > 0.9), RARR does not make an edit\nin 90% of the cases. However, the system also has\nseveral shortcomings. Some erroneous edits arise\nfrom misleading irrelevant evidence (Figure 7d).\nWe also observed an interesting challenge when\nrevising reasoning chains, where the model suc-\ncessfully revised an incorrect claim, but did not\nrevise subsequent reasoning steps that depend on\nthe earlier claim (Figure 7e). In this case, further\nediting to improve logical coherence could help.\n6.2 Ablations\nAblating query generation RARR uses gener-\nated questions as search queries for evidence re-\ntrieval. We consider two natural alternatives: using\nthe entire input passage as a single search query,\nor using each sentence as a search query. For the\nformer, we retrieve J = 3 evidence snippets to\nmake the amount a closer match to other methods.\nThe results are in Table 2. Using the entire input\npassage as the query gives poor results, as the re-\ntrieved evidence tends to not focus on potentially\nunattributed parts in the passage. Using sentences\nas queries gives results closer to the full CQGen,\nbut a closer analysis reveals two caveats.\nFirst, sentences-as-queries are more effective\nwhen such sentences “mimic” content on the Web,\nand are less effective otherwise. In Table 3, we test\nthis by excluding all of Wikipedia from web search\nresults (since many PaLM outputs for NQ have\na Wikipedia style). The attribution performance\nof sentences-as-queries drops significantly, while\nCQGen is more robust.\nSecond, sentence-as-queries tends to retrieve\npassages that may encourage confirmation bias.\nConsider the example “Georgia is called the Peach\nState, but California actually produces the most\npeaches. ” Retrieval using sentences-as-queries\nfound an article echoing that California produces\nthe most peaches, while CQGen generated the more\nimpartial query “Which state produces the most\npeaches?” and found a newer article saying that\nSouth Carolina replaced California as the top peach\nproducer. In this case, RARR using CQGen needs\nto sacrifice more preservation score to edit the text,\nleading to a lower F1AP score. This underscores\nthat attribution alone cannot measure “correctness”\nsince not all evidence is up-to-date or reliable.\nAblating agreement model We try removing\nthe agreement model, which effectively forces the\nmodel to revise the passage based on every re-\ntrieved evidence. The results are shown in Table 2.\nAs expected, more revision leads to less preserva-\ntion score and spurious changes to the text passage,\nas demonstrated in Figure 8.\nImpact on downstream task performance We\nhave measured preservation using the metric de-\n16484\nPaLM outputs on NQ PaLM outputs on SQA LaMDA outputs on QReCC\nModel Attr auto PresLev F1AP Attrauto PresLev F1AP Attrauto PresLev F1AP\nFull RARR 45.6 → 54.9 89.6 68.1 37.6 → 45.1 89.9 60.0 18.8 → 29.4 80.2 43.1\nno agreement model 45.6 → 50.6 82.6 62.8 37.8 → 46.9 83.4 60.0 18.8 → 28.8 72.0 41.2\nquery = input 45.4 → 47.2 98.4 63.8 39.4 → 30.3 98.8 46.4 19.7 → 20.6 96.3 34.0\nquery = sentence 49.1 → 52.1 97.0 67.8 43.7 → 44.3 98.8 61.2 19.0 → 19.6 97.0 32.6\nTable 2: Ablation results. We report the automatic metrics: Attrauto, PresLev, and harmonic mean between the two\n(F1AP). We show auto-AIS scores both before and after editing (before →edit), with respect to the attribution report\nA produced by the model. Even though sentence-as-queries may achieve similar F1AP as RARR, it is less robust to\ncorpus shifts and tends to retrieve passages that may encourage confirmation bias.\nNQ F1AP SQA F1AP\nModel orig no wiki orig no wiki\nFull RARR 68.1 64.3 60.0 57.6\nquery = sentence 67.8 60.3 61.2 56.7\nTable 3: The impact of excluding Wikipedia from the\nretrieval corpus. CQGen (full RARR) is more robust to\nWikipedia’s absence, while using sentences-as-queries\nsuffers a bigger drop in performance.\nx: The Crown-of-thorns starfish is native to the Great Barrier Reef. . . The\nstarfish was introduced to the Great-Barrier-Reef byocean currents.\ne: [invasivespeciesinfo.gov]Ballast wateris one of the major pathways for\nthe introduction of nonindigenous marine species. . .\ny: The Crown-of-thorns starfish is native to the Great Barrier Reef. . . The\nstarfish was introduced to the Great-Barrier-Reef byballast water.\nFigure 8: Disabling the agreement model leads to\nover-edits. Here, the evidence e does not explicitly dis-\nagree with x, but without an agreement model to detect\nthis, the edit model makes an unsupported change.\nFigure 9: Downstream task performance on NQ and\nSQA. RARR’s revisions lead to better answer accuracy\non NQ. No models improved answer accuracy on SQA.\nfined in Section 2.2. However, another measure\nof preservation is whether the revised text can still\nbe used to perform the task that it was originally\ngenerated for. Following EFEC, we quantitatively\nevaluate this on short answer tasks NQ and SQA,\nand we summarize the result in Figure 9.\nFor NQ, each original text x is a long-form re-\nsponse to a factoid question. To determine whether\nthe revised text y still serves this purpose, we feed\nthe factoid question and y back into PaLM and\nprompt it to extract a short answer from y. We find\nthat RARR not only preserves the short answer\naccuracy but actually improves it by roughly 5%.\nFor SQA, each original text is a reasoning chain\nthat helps to answer a yes/no question. We feed the\nSQA question and y back into PaLM and prompt\nit to output a yes/no answer, and evaluate answer\naccuracy. Here, we find that increasing attribution\ncomes at a slight cost in downstream task perfor-\nmance: answer accuracy drops modestly for all\nrevision models (up to 2.6%). We suspect that this\nmay be due to noisy retrievals, which sometimes\nprovide misleading evidence (exemplified in Fig-\nure 7d). Furthermore, even though revisions can\naddress factoid errors in the passage (e.g., “Homer\nSimpson has 5 fingers” from Figure 7e), RARR\ncurrently does not try to modify subsequent reason-\ning steps which may no longer be logically entailed\n(e.g., “He only needs one hand to count to 5”).\n7 Conclusion\nLanguage models have developed increasingly\ngood “procedural” knowledge of what should be\ndiscussed and how it should be presented, but often\nstruggle to memorize “factoid” knowledge and pro-\nduce unsubstantiated claims. We proposed RARR,\na framework for revising such claims to make them\nattributable to the researched evidence. From ex-\nperiments on text passages generated by different\nmodels on various domains, we showed that RARR\ncan revise the passages to improve attribution while\npreserving other desirable properties such as writ-\ning style or structure. Furthermore, RARR sits on\ntop of existing generation models without needing\nto re-design or re-train LMs.\nMajor headroom still remains, as discussed in\nSection 6 and the Limitations section. We hope our\nanalysis of RARR would help with developing new\napproaches for integrating attribution to LMs.\n16485\n8 Limitations\nLimitations of our task definition Depending\non the application, attribution and preservation may\nnot deserve equal weight. For instance, if there are\nmultiple acceptable options for the output, such as\nin a dialog system, we might trade-off preservation\nfor attribution, similar to how LaMDA behaves in\nour experiments.\nOur evaluation metrics also do not measure all\naspects of attribution. For instance, some sentences\nare self-evident and do not require attribution (e.g.,\n“I agree. ”) but would be penalized in our evaluation.\nIt is also necessary to note that linguistic assertions\nhave varying scope: for example, there is a differ-\nence between “Frozen is a scary movie” and “I\ngot scared watching Frozen”— while expressing a\nsimilar sentiment, the former makes a more general\nstatement that many would disagree with, while the\nlatter is scoped to the speaker’s own experience.\nIn some applications, one could even argue that\nthe latter case does not require attribution, since\nthe speaker is their own source-of-truth. In addi-\ntion to varying scope, utterances can also make\nassertions with varying levels of directness. For\nexample, according to standard linguistics, “John\nate some of the cookies” yields the implicature that\nJohn did not eat all of the cookies, even though\nit is not logically entailed. This raises the ques-\ntion of which implicatures or implied assertions\nshould be detected and attributed, which should be\nexplored in future work. For more nuances, we\nrefer to Rashkin et al. (2021).\nFor preservation, we wish to explore other prop-\nerties that should be preserved, such as discourse\nor logical coherence. Additionally, if the input text\npassage is completely misguided or flawed, it can\nbe difficult to revise the text without significant\nchanges, which would be heavily penalized by the\ncurrent metrics.\nLimitations of our model While we aspire to\nimprove attribution for arbitrary text, it is clear\nthat RARR is not yet fully general. For example,\nthe current implementation of RARR would not\nbe well-prepared to edit poetry (where preserving\nrhyme matters) or long documents, primarily be-\ncause we do not provide examples of such inputs\nin our few-shot LLM prompts. However, we do be-\nlieve that future developers may be able to quickly\nadapt RARR to such tasks by simply changing the\nprompts. Second, RARR tends to preserve rather\nthan delete claims that it cannot attribute. Some of\nthese claims genuinely do not require attribution,\nbut others are hallucination and should be removed.\nJudging whether a claim requires attribution can be\nsubjective and challenging. Finally, our model is\ncomputationally costly, since it is based on prompt-\ning a large language model. One potential solu-\ntion is to leverage recent synthetic data generation\nrecipes to train a smaller model (Lee et al., 2021;\nSchick et al., 2022).\n9 Ethical considerations\nPartial attribution When RARR is not 100%\nsuccessful in making text consistent with retrieved\nevidence, the revised text will be partially at-\ntributed. One could identify unattributed parts us-\ning either the automated attribution score (AttrAIS)\nor the relevance scores used to generate the attribu-\ntion report (Section 3.3). Such information should\nbe presented to avoid misleading readers into think-\ning that the entire revision is attributed.\nEvidence trustworthiness RARR seeks to im-\nprove attribution for the output of any generative\nmodel. However, even if RARR can attribute con-\ntent to a particular source, the user must still con-\nsider whether the source itself is trustworthy. Even\nfor sources that are traditionally considered “au-\nthoritative” (such as an encyclopedia), there may\nstill be factual inaccuracies or biases. This work\ndoes not address the question of whether a source\nis trustworthy, or the related topic of misinforma-\ntion. While we do not provide a means for judging\ntrustworthiness, the design of RARR does allow for\nthe research stage to restrict its search over a user-\nspecified corpus, based on what the user deems\ntrustworthy.\nConflicting evidence There is also the possibility\nthat some content may be simultaneously supported\nby certain sources, while contradicted by others.\nThis can easily occur for content involving subjec-\ntive or imprecise claims. The current implementa-\ntion and evaluation for RARR does not explicitly\naddress this issue — we adopted a “permissive”\ndefinition of attribution, where we consider content\nto be attributed if there exists any source that sup-\nports it. For some applications, a more restrictive\ndefinition that requires both existence of support-\ning sources and absence of contradicting sources\nwould be needed.\n16486\nAcknowledgments\nWe wish to thank Raphael Hoffmann, Slav Petrov,\nDipanjan Das, Michael Collins, Iftekhar Naim,\nKristina Toutanova, William Cohen, Sundeep Tiru-\nmalareddy, Samer Hassan, Quoc Le and Heng-Tze\nCheng for their research mentorship, feedback and\nsupport. We are grateful to Hao Zhou and Petr\nPilar for helping us experiment with LaMDA and\nmotivating our dialog experiments. We also wish\nto thank Tal Schuster for pointing us to relevant\nwork in the fact checking literature, and helping\nus reproduce it. We thank Vitaly Nikolaev, David\nReitter and Roee Aharoni for helping us use AIS\nand auto-AIS. We also wish to thank Jianmo Ni and\nHonglei Zhuang for developing the query-evidence\nrelevance model we use, Daniel Andor for develop-\ning the sentence decontextualization model we use,\nand Ran Tian for the initial prototype of CQGen.\nFinally, we thank Kathy Meier-Hellstern, Philip\nParham and Diane Korngiebel for their thoughtful\nfeedback on ethical considerations.\nContributions\nLuyu Gao: Designed RARR’s few-shot prompting\nstrategies and implemented the first PaLM-based\nprototype. Analyzed results, and advised on the\ndesign of human and automatic evaluation.\nZhuyun Dai: Proposed the evaluation setup of\nediting long-form generations from PaLM/LaMDA\non various QA datasets. Hosted and mentored Luyu\nGao (student researcher) in prototyping RARR. Im-\nplemented the final models, designed overall exper-\niments, and obtained main results and ablations (to-\ngether with Ice Pasupat). Contributed many parts\nof the writing.\nIce Pasupat: Implemented the final models, de-\nsigned overall experiments, and obtained main re-\nsults and ablations (together with Zhuyun Dai). Au-\ntomated experimental infrastructure, conducted er-\nror analyses, and oversaw many parts of the paper\nwriting.\nAnthony Chen: Developed the automatic evalu-\nation for attribution and preservation and worked\nwith Arun Chaganty to design human evaluation.\nDeveloped the open-source implementation (GPT-\n3 RARR), made improvements to prompts, and\nhelped with writing.\nArun Chaganty: Led and implemented all hu-\nman evaluation. Proposed the two-dimensional\nattribution + preservation metric (together with\nKelvin Guu). Advised on model design and con-\ntributed many parts of the writing.\nYicheng Fan: Worked with Kelvin Guu to de-\nvelop the first prototype of RARR. Proposed multi-\nple retrieval strategies and implemented the EFEC\nbaseline.\nVincent Zhao: Co-hosted and mentored Luyu\nGao (student researcher) in prototyping RARR. En-\nabled bulk inference for PaLM. Proposed the down-\nstream task evaluation.\nNi Lao: Research mentorship, advising and con-\ntributed many parts of the writing.\nHongrae Lee: Research mentorship and advis-\ning. Helped integrate RARR with Google Search\nand evaluate LaMDA.\nDa-Cheng Juan: Research mentorship and early\ndesign discussions.\nKelvin Guu: Proposed the original research-and-\nrevise concept, implemented the first prototype, ini-\ntiated the project and involved all collaborators. Im-\nplemented baselines (together with Yicheng Fan).\nResearch mentorship, oversaw project coordination\nand paper writing.\nReferences\nMichael Ahn et al. 2022. Do as I can, not as I say:\nGrounding language in robotic affordances. ArXiv,\nabs/2204.01691.\nRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,\nShayne Longpre, Stephen G. Pulman, and Srini-\nvas Chappidi. 2021. Open-domain question answer-\ning goes conversational via question rewriting. In\nNAACL.\nIsabelle Augenstein, Christina Lioma, Dongsheng\nWang, Lucas Chaves Lima, Casper Hansen, Chris-\ntian Hansen, and Jakob Grue Simonsen. 2019. Mul-\ntiFC: A real-world multi-domain dataset for evidence-\nbased fact checking of claims. In EMNLP.\nVidhisha Balachandran, Hannaneh Hajishirzi, William\nCohen, and Yulia Tsvetkov. 2022. Correcting diverse\nfactual errors in abstractive summarization via post-\nediting and language model infilling. In EMNLP.\nBernd Bohnet, Vinh Quang Tran, Pat Verga, Roee Aha-\nroni, Daniel Andor, Livio Baldini Soares, Jacob\nEisenstein, Kuzman Ganchev, Jonathan Herzig, Kai\nHui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schus-\nter, William W. Cohen, Michael Collins, Dipanjan\nDas, Donald Metzler, Slav Petrov, and Kellie Webster.\n2022. Attributed question answering: Evaluation\nand modeling for attributed large language models.\nArXiv.\n16487\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In NeurIPS.\nMengyao Cao, Yue Dong, Jiapeng Wu, and Jackie\nChi Kit Cheung. 2020. Factual error correction for\nabstractive summarization models. In EMNLP.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In ACL.\nEunsol Choi, Jennimaria Palomaki, Matthew Lamm,\nTom Kwiatkowski, Dipanjan Das, and Michael\nCollins. 2021. Decontextualization: Making sen-\ntences stand-alone. TACL, 9:447–461.\nAakanksha Chowdhery et al. 2022. PaLM: Scal-\ning language modeling with pathways. ArXiv,\nabs/2204.02311.\nNouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and\nSiva Reddy. 2022. On the origin of hallucinations\nin conversational models: Is it the datasets or the\nmodels? In NAACL.\nAlexander R. Fabbri, Wojciech Kryscinski, Bryan Mc-\nCann, Richard Socher, and Dragomir Radev. 2021.\nSummEval: Re-evaluating summarization evaluation.\nTACL, 9:391–409.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In ACL.\nAngela Fan, Aleksandra Piktus, Fabio Petroni, Guil-\nlaume Wenzek, Marzieh Saeidi, Andreas Vlachos,\nAntoine Bordes, and Sebastian Riedel. 2020. Gener-\nating fact checking briefs. In EMNLP.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did Aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. TACL, 9:346–361.\nTanya Goyal and Greg Durrett. 2021. Annotating and\nmodeling fine-grained factuality in summarization.\nIn NAACL.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. REALM: Retrieval-\naugmented language model pre-training. In ICML.\nShirley Anugrah Hayati, Raphaël Olivier, Pravalika Av-\nvaru, Pengcheng Yin, Anthony Tomasic, and Graham\nNeubig. 2018. Retrieval-based neural code genera-\ntion. In EMNLP.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In ICLR.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nY . Matias. 2022. TRUE: Re-evaluating factual con-\nsistency evaluation. In Workshop on Document-\ngrounded Dialogue and Conversational Question An-\nswering.\nOr Honovich, Leshem Choshen, Roee Aharoni, Ella\nNeeman, Idan Szpektor, and Omri Abend. 2021.\nQ2: Evaluating factual consistency in knowledge-\ngrounded dialogues via question generation and ques-\ntion answering. In EMNLP.\nHayate Iso, Chao Qiao, and Hang Li. 2020. Fact-based\ntext editing. In ACL.\nGeorgi Karadzhov, Preslav Nakov, Lluís Màrquez, Al-\nberto Barrón-Cedeño, and Ivan Koychev. 2017. Fully\nautomated fact checking using external sources. In\nRANLP.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Yu Wu, Sergey Edunov, Danqi Chen,\nand Wen tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In EMNLP.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In ICLR.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nSciTaiL: A textual entailment dataset from science\nquestion answering. In AAAI.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answering.\nIn NAACL.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nEMNLP.\nTom Kwiatkowski et al. 2019. Natural Questions: A\nbenchmark for question answering research. TACL,\n7:453–466.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In ACL.\nKenton Lee, Kelvin Guu, Luheng He, Timothy Dozat,\nand Hyung Won Chung. 2021. Neural data\naugmentation via example extrapolation. ArXiv,\nabs/2102.01335.\n16488\nVladimir I. Levenshtein. 1965. Binary codes capable of\ncorrecting deletions, insertions, and reversals. Soviet\nphysics. Doklady, 10:707–710.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive NLP tasks. In NeurIPS.\nRobert L Logan IV , Alexandre Passos, Sameer Singh,\nand Ming-Wei Chang. 2022. FRUIT: Faithfully re-\nflecting updated information in text. In NAACL.\nShayne Longpre, Kartik Kumar Perisetla, Anthony\nChen, Nikhil Ramesh, Chris DuBois, and Sameer\nSingh. 2021. Entity-based knowledge conflicts in\nquestion answering. In EMNLP.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan T. McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In ACL.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, and Nathan McAleese.\n2022. Teaching language models to support answers\nwith verified quotes. ArXiv, abs/2203.11147.\nReiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff\nWu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William\nSaunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight,\nBenjamin Chess, and John Schulman. 2021. We-\nbGPT: Browser-assisted question-answering with hu-\nman feedback. ArXiv, abs/2112.09332.\nShashi Narayan, Joshua Maynez, Reinald Kim Am-\nplayo, Kuzman Ganchev, Annie Louis, Fantine Huot,\nDipanjan Das, and Mirella Lapata. 2022. Condi-\ntional generation with a question-answering blueprint.\nArXiv, abs/2207.00397.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A human generated machine\nreading comprehension dataset. In CoCo@NIPS.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hernández Ábrego, Ji Ma, Vincent Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021. Large dual encoders are generalizable\nretrievers. ArXiv, abs/2112.07899.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, Charles Sutton, and Augustus Odena.\n2021. Show your work: Scratchpads for interme-\ndiate computation with language models. ArXiv,\nabs/2112.00114.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? In EMNLP.\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nDmytro Okhonko, Samuel Broscheit, Gautier Izacard,\nPatrick Lewis, Barlas Ouguz, Edouard Grave, Wen\ntau Yih, and Sebastian Riedel. 2021. The web is your\noyster - knowledge-intensive nlp against a very large\nweb corpus. ArXiv, abs/2112.09924.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. JMLR, 21.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\nReitter. 2021. Measuring attribution in natural lan-\nguage generation models. ArXiv, abs/2112.12870.\nAdam Roberts, Colin Raffel, and Noam M. Shazeer.\n2020. How much knowledge can you pack into the\nparameters of a language model? In EMNLP.\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio\nPetroni, Patrick Lewis, Gautier Izacard, Qingfei You,\nChristoforos Nalmpantis, Edouard Grave, and Sebas-\ntian Riedel. 2022. PEER: A collaborative language\nmodel. ArXiv, abs/2208.11663.\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin C! robust fact verification with con-\ntrastive evidence. In NAACL.\nDarsh J. Shah, Tal Schuster, and Regina Barzilay. 2020.\nAutomatic fact-guided sentence modification. In\nAAAI.\nRichard Shin, Christopher H Lin, Sam Thomson,\nCharles Chen, Subhro Roy, Emmanouil Antonios\nPlatanios, Adam Pauls, Dan Klein, Jason Eisner, and\nBenjamin Van Durme. 2021. Constrained language\nmodels yield few-shot semantic parsers. In EMNLP.\nRomal Thoppilan et al. 2022. LaMDA: Language mod-\nels for dialog applications. ArXiv, abs/2201.08239.\nJames Thorne and Andreas Vlachos. 2021. Evidence-\nbased factual error correction. In ACL.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction and\nverification. In NAACL.\nDavid Wadden, Kyle Lo, Lucy Lu Wang, Shanchuan\nLin, Madeleine van Zuylen, Arman Cohan, and Han-\nnaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In EMNLP.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In ACL.\n16489\nWilliam Yang Wang. 2017. “Liar, liar pants on fire”: A\nnew benchmark dataset for fake news detection. In\nACL.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. ArXiv, abs/2201.11903.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL.\nJingyi Zhang, Masao Utiyama, Eiichiro Sumita, Gra-\nham Neubig, and Satoshi Nakamura. 2018. Guiding\nneural machine translation with retrieved translation\npieces. In NAACL.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPAWS: Paraphrase adversaries from word scrambling.\nIn NAACL.\nA Additional experiments and analysis\nModel variance The main experiments in Sec-\ntion 5 are based on a single run. We ran automated\nevaluation on 3 random runs of RARR, using PaLM\noutputs on NQ as input passages. The standard de-\nviations of Attrauto, PresLev, and F1AP are 1.2, 0.5,\nand 1.0 respectively.\nImpact of the retriever choice We tried using\nMicrosoft Bing in place of Google Search, with\nnear identical results (< 1% difference).\nImpact of model scale Many components in\nRARR work by few-shot prompting PaLM, a large\n540B parameter LM. To assess the benefit of LM\nscaling, we replaced PaLM 540B with a smaller\n62B parameter PaLM. As shown in Table 4, we\nfound that 540B outperforms 62B by a large mar-\ngin, suggesting that RARR could potentially further\nimprove with even more scaling. We also experi-\nmented with keeping the editor stage at 540B while\nshrinking the query generation stage to 64B — this\nyielded a relatively small performance drop, sug-\ngesting that model scaling is more important for\nthe editor.\nImpact of model type Few-shot prompting has\nproven to be effective for many recent large lan-\nguage models. We try replacing the query genera-\ntion model, agreement model, and edit model with\nGPT-3 text-davinci-003. The few-shot prompts\nwere slightly tuned to fit the GPT-3 model. Table 4\nshows the results, which are slightly better than\nRARR implemented with PaLM 540B on all three\ndatasets. We will release this open-source version\nof RARR that uses GPT-3 as the backbone.\nResults on GPT-3 passages Table 5 shows au-\ntomated evaluation results on passages generated\nby GPT-3. The results follow the same trend as the\nresults on PaLM and LaMDA passages.\nChallenging domains We report results on tasks\nwhere attribution was particularly hard, and signifi-\ncant future work is needed.\nWe considered news article summaries produced\nby summarization models from SummEval (Fabbri\net al., 2021) (e.g., “John Doe was left homeless\nwhen the storms hit Staten Island, New York . . . ”).\nResults are shown in Table 6. First, we note that the\nbefore-edit auto-AIS scores for all models are low.\nThese news article summaries are often about less\nwidely known people and events, which is chal-\nlenging for retrievers, leading to low attribution.\nFor example, our query generator may ask “where\ndoes John Doe live” but get results for a different\nJohn Doe. EFEC and LaMDA also face this issue,\nbut instead trade preservation for attribution and\nrewrite the text to a different topic. This result sug-\ngests that using web search with standard question\ngeneration methods may fail to capture important\ncontext from the input, and is not sufficient for the\nattribution task.\nWe also considered long-form explanations gen-\nerated by PaLM for the ELI5 dataset (Fan et al.,\n2019) (Table 6). ELI5 was collected from online\nforums, so many answers tend to have subjective\nopinions instead of specific entities and facts (e.g.,\n“How do our brains interpret scary music? To me,\nscary music often sounds a little bit like a person\n. . . ”), and are thus difficult to attribute. Sometimes\nthe whole output is based on a false premise and\nneeds to be completely rewritten, in which case\nRARR cannot satisfactorily edit due to our revision\nthreshold (Section 3.2).\nFinally, we considered technical explanations\nto questions from the MMLU dataset (Hendrycks\net al., 2021) which covers diverse subjects from\nsocial science, humanities, STEM, and others.4 An\nexample input looks like “Every time you remove\nan edge from a complete graph, you divide it into\ntwo connected components. So, a complete graph\nwith 13 vertices must have 12 connected compo-\nnents. ”Results are shown in Table 7. RARR im-\n4MMLU has questions from 57 subjects; we took 10 ran-\ndom question from each topic and generated answer explana-\ntions by prompting PALM 540B.\n16490\nPaLM outputs on NQ PaLM outputs on SQA LaMDA outputs on QReCC\nModel Attr auto PresLev F1AP Attrauto PresLev F1AP Attrauto PresLev F1AP\nFull RARR 45.6 → 54.9 89.6 68.1 37.6 → 45.1 89.9 60.0 18.8 → 29.4 80.2 43.1\nqgen 62B, editor 540B 45.9 → 54.6 87.8 67.4 37.0 → 40.5 90.0 55.9 15.8 → 28.4 76.1 41.4\nqgen 62B, editor 62B 45.9 → 49.9 91.0 64.4 37.0 → 38.3 93.0 54.2 15.8 → 21.9 71.6 33.5\nGPT-3 44.3 → 55.0 90.6 68.5 38.6 → 46.6 89.3 61.2 18.3 → 28.6 89.8 43.4\nTable 4: Additional ablation results. We report the automatic metrics: Attrauto, PresLev, and harmonic mean\nbetween the two (F1AP). We show auto-AIS scores both before and after editing (before →edit), with respect to the\nattribution report A produced by the model.\nGPT-3 outputs on NQ GPT-3 outputs on SQA GPT-3 outputs on QReCC\nModel Attr auto PresLev F1AP Attrauto PresLev F1AP Attrauto PresLev F1AP\nEFEC 48.3 → 66.8 41.5 51.2 32.6 → 50.6 29.4 37.2 26.4 → 53.1 39.0 44.9\nLaMDA 36.2 → 61.1 45.9 52.4 22.3 → 27.3 43.3 33.5 19.0 → 33.9 28.3 30.8\nPaLM RARR 48.3 → 57.2 89.6 69.8 32.6 → 36.3 91.6 52.0 26.4 → 31.1 87.7 45.9\nGPT-3 RARR 48.0 → 59.3 91.8 72.0 34.7 → 37.0 91.8 52.8 23.2 → 25.3 89.7 39.5\nTable 5: Results on passages from GPT-3. We report the automatic metrics: Attrauto, PresLev, and harmonic mean\nbetween the two (F1AP). We show auto-AIS scores both before and after editing (before →edit), with respect to the\nattribution report A produced by the model. The results show similar trends as the results on passages from PaLM\nand LaMDA in Table 1.\nModel Attr auto PresLev F1AP\nSummEval\nEFEC 17.9 → 34.6 20.9 26.0\nLaMDA 10.3 → 28.8 28.1 28.4\nRARR 18.3 → 16.9 92.9 28.6\nELI5\nEFEC 18.2 → 41.2 17.2 24.2\nLaMDA 19.9 → 40.1 31.2 35.1\nRARR 18.5 → 18.9 97.2 31.7\nTable 6: Results on ELI5 and SummEval.\nRARR\nMMLU Category Attr auto PresLev F1AP\nHumanities 26.6 → 29.6 6.6 45.0\nSocial Sciences 35.5 → 40.7 7.6 56.5\nSTEM 37.8 → 41.5 7.2 57.4\nOther 36.9 → 41.7 7.1 57.6\nTable 7: RARR results on MMLU.\nproves attribution of the explanations on all four\ncategories of MMLU, although the increases are\nrelatively small. We also found that RARR’s per-\nformance is low on examples with mathematical\nreasoning, as these are beyond the capability of the\nedit model with our current prompt.\nB Details on automated evaluation\nSentence splitting When computing the attri-\nbution score, we use spaCy en_core_web_sm\nv3.0.0a1 to segment the text passage into sentences.\n(More recent models gave similar results.) While\neach sentence may contain multiple claims that\ncould be attributed independently, there is currently\nno linguistic consensus on what constitutes a claim.\nInstead of depending on a particular definition of\nclaims, we use sentences as claims for simplicity\nand reproducibility. The same segmentation is also\nused for human evaluation.\nDecontextualization We decontextualize each\nsentence in the text passage before computing the\nattribution score. We use the model from Choi\net al. (2021), which is a T5 model fine-tuned to\nmap the input “ [HEAD] [SEP] context and pas-\nsage [start] sentence [end]” to the output\n“[OPCODE] decontextualized sentence”, where the\nOPCODE can be “ done” (success), “ un” (unneces-\nsary), or “imp” (impossible). We feed the passage’s\ncontext (questions for NQ and SQA; dialog context\nfor QRECC) along with the passage itself to the\ninput. We use beam search with beam size 8 and\ndiscard any result whose number of tokens differ\nby more than 4.\nNLI model We obtained a newer version of\nthe end-to-end NLI model from the authors of\nHonovich et al. (2022), which was trained on\nMNLI, SNLI, FEVER, PAWS, SciTail and Vita-\nminC (Williams et al., 2018; Bowman et al., 2015;\nThorne et al., 2018; Zhang et al., 2019; Khot et al.,\n2018; Schuster et al., 2021). The model is a T5\n16491\nFigure 10: Violin plot illustrating the strong correlation\nbetween human AIS and auto-AIS labels on our NQ\nbenchmark. Pearson correlation is 0.74 (N=450). y-\naxis is auto-AIS score, the two violins correspond to a\nhuman label of 0 or 1.\nmodel fine-tuned to map the input “premise: evi-\ndence hypothesis: claim sentence” to either “1”\n(entailed) or “0” (not entailed). As suggested by\nthe authors, we use the probability of producing “1”\nas the entailment score.\nComparing human and automated evaluation\nWe conducted correlation studies between human\nand automatic metrics and found strong Pearson\ncorrelation (attribution = 0.74; preservation = 0.62).\nWe visualize the correlation between human and\nautomated attribution scores on NQ and SQA in\nFigure 10. We found that the AIS scores from\nhuman correlate well with auto-AIS scores, with\nsome bias for non-attributed sentences to be judged\nas attributed by auto-AIS.\nC Details on human evaluation\nTo end-goal of RARR is to improve the attribu-\ntion of generation models through post-editing\nwhile preserving the original intent. Attribution\nand preservation are both subjective properties that\nmay change with even small edits. In the main\npaper, we present two automatic metrics to conve-\nniently gauge these properties, but rely on a human\nevaluation as the gold standard. In this section,\nwe describe how we conducted the human evalua-\ntion and what instructions and examples annotators\nwere provided.\nRater recruitment and training We engaged\nwith a vendor supplier of full-time crowd workers\nto recruit human annotators for our task. Anno-\ntators were asked to review the instructions be-\nlow and were provided direct feedback on their\nresponses during the pilot annotation runs. We had\n3 annotators rate each example in the pilot phase\nto measure inter-annotator agreement, and had a\nsingle rater annotate each example afterwards.\nC.1 Instructions: Overview\nIn this task you will evaluate the quality of text\ngenerated by a system (the “passage”) based on\nhow well it represents information from multiple\npieces of “evidence”.\nWe will be using two categories to evaluate the\nquality of the passage: Attribution and Intent\nSimilarity. You will evaluate these categories in\nsuccession. In some tasks, you will only evalu-\nate Attribution. The task interface will guide you\nthrough the flow; you can also see the overall task\nflow in the diagram below.\nNote: The passage may appear very fluent and\nwell-formed, but still contain slight inaccuracies\nthat are not easy to discern at first glance. Pay close\nattention to the text. Read it carefully as you would\nwhen proofreading.\nC.2 Instructions: Attribution\nIn this step, you will evaluate how much of the\npassage is attributable to one or more pieces of\nevidence (Figure 11).\nIn the interface, the passage of text and the con-\ntext in which it was generated is shown on the left,\nand each piece of evidence is shown on the right.\nYou will use all three (context, passage, evidence)\nto answer the following question for each sentence\nin the passage: Is all of the information provided by\nthis sentence fully supported by at least one piece\nof evidence?\nDetermining the information provided by the\nsentence. Three points are key when determining\ninformation provided by the sentence:\n1. The context and the other sentences of the\npassage are often critical in understanding the\ninformation provided by the sentence.\n2. The context should only be used to understand\nthe information provided by the sentence.\n3. The evidence should be completely ignored\nfor this step.\nConsider the following example:\nContext: who plays doug williams in\ndays of our lives\n16492\nFigure 11: Screenshot of interface to annotate attribution at the sentence level. annotators were asked to mark\nsentences as being fully attributable or not fully attributable by clicking each sentence, and rating each piece of\nevidence as being useful or not in helping determine attribution of the passage. Annotators were also presented with\nthe context of the generation.\nFigure 12: Screenshot of the preservation interface. Annotators are asked to read compare two passages and rate\nhow similar the intent conveyed by the two passages is.\n16493\nPassage: In the American daytime\ndrama Days of Our Lives, Doug\nWilliams and Julie Williams are por-\ntrayed by Bill Hayes and Susan Seaforth\nHayes.\nIn the above example, the meaning of the pas-\nsage is clear even without seeing the query. But\nconsider another example:\nContext: who plays doug williams in\ndays of our lives\nPassage: he is played by Bill Hayes\nPassage (interpreted): Doug Williams is\nplayed by Bill Hayes in days of our lives\nIn this case the pronoun “he” depends on the\ncontext, but it is clear that the intended meaning\nof the passage can be reasonably interpreted as\n“Doug Williams is played by Bill Hayes in days of\nour lives”. This interpretation is the “information\nprovided by the passage”.\nPronouns such as he/she/it/they etc. are one case\nwhere context is needed to figure out the intended\nmeaning of the system response. Here’s another\nexample (given with paraphrases of the information\nhighlighted below):\nContext: when is the last time the us lost\nbasketball at the olympics\nPassage: The last time they lost was in\n2004, when Argentina defeated the US\n89–79. Most recently, they won gold in\n2016.\nPassage (interpreted): The last time\nthe United States lost basketball at the\nOlympics was in 2004.\nThe context should only be used to determine\nthe information provided by the passage; at times,\nthe passage may be about a slightly different topic\nthan the context, for example:\nContext: the south west wind blows\nacross nigeria between\nPassage: The Harmattan is a dry and\ndusty northeasterly trade wind that blows\nacross West Africa from December to\nMarch. It is very dusty because it blows\nacross the Sahara.\nHere, the passage talks about a northeasterly\nwind, while the context asks about a south-west\nwind, but the passage can be fully understood.\nIn general, use your best judgment to determine\nthe information provided by the passage. If the\npassage is hard to understand and you are unsure\nwhat the intended meaning of the passage is, mark\nthe sentences as not attributedand enter a comment\nwith an explanation. As one example, take the\nfollowing:\nContext: how many NBA championships\ndid Michael Jordan win?\nPassage: it is the best team in the NBA\nDetermining if the information accurately repre-\nsents the evidence. Two points are key when de-\ntermining whether the information accurately rep-\nresents the evidence: When interpreting a piece of\nevidence, use only the title and text of that specific\nevidence. Completely ignore the context, passage\nand all other evidence. Check all the information in\na sentence. If only some information is supported\nby the evidence, mark the sentence as not fully\nattributable.\nConsider the following example:\nContext: when did reba mcentire record\nback to god\nPassage: Back to God was released by\nMcEntire in 2017.\nEvidence: “Back to God” is a song per-\nformed by American singer, Reba McEn-\ntire. It was released as the second sin-\ngle from her 2017 album, Sing it Now:\nSongs of Faith & Hope, on January 20,\n2017.\nIn the above example, it is reasonable to conclude\nthat the evidence supports all the information in\nthe passage, and we can mark the passage as being\nfully attributable. But consider another example:\nContext: who won the womens 2017\nncaa basketball tournament\nPassage: South Carolina Gamecocks\nwon the 2017 NCAA Women’sDivision\nI Basketball Tournament.\nEvidence: The South Carolina Game-\ncocks defeated the Mississippi State Bull-\ndogs, 67–55, to claim their first-ever na-\ntional championship.\nIn this case, while the evidence also mentions the\n“South Carolina Gamecocks”, it isn’t clear that the\nnational championship being mentioned is indeed\nthe 2017 NCAA Women’s Division I Basketball\n16494\nTournament. The passage should be marked as not\nattributable.\nFinally, when the passage contains multiple sen-\ntences, evaluate whether each sentence can be fully\nattributed to one or more pieces of evidence—it\nis possible for one sentence to be attributed while\nanother is not. For example:\nContext: who won the womens 2017\nncaa basketball tournament\nPassage: South Carolina Gamecocks\nwon the 2017 NCAA Women’sDivision\nI Basketball Tournament. The final score\nis 67-55. The championship game was\nheld in Dallas, Texas.\nEvidence 1: The South Carolina Game-\ncocks defeated the Mississippi State Bull-\ndogs, 67–55, to claim their first-ever na-\ntional championship.\nEvidence 2: The 2017 NCAA Women’s\nDivision I Basketball Tournament was\nplayed from Friday, March 17 to Sun-\nday, April 2, 2017, with the Final Four\nplayed at the American Airlines Center\nin Dallas, Texas on March 31 and April\n2.\nThe first two sentences cannot be attributed to\neither evidence for the same reason as the previous\nexample, but the last sentence is fully supported by\nEvidence 2 and should be marked as attributed.\nIn general, you should use your best judgment\nin determining whether all of the information pro-\nvided by the passage is “an accurate representation\nof information in at least one evidence”. See Table\n8 for additional examples.\nWe give the following final notes of guidance:\n• Marking evidence as useful. When review-\ning each piece of evidence, mark it as useful\nif it helps you judge the attributability of any\nsentence, and mark it not useful if not. In\nthe above example Evidence 1 is not useful\nbecause it didn’t contain enough context to\nactually help you assess if the passage was\nattributable, but Evidence 2 was useful.\n• Contradicting evidence. Mark a sentence as\nbeing attributed if any piece of evidence sup-\nports it: if two pieces of evidence contradict\neach other, but one of them supports the pas-\nsage, mark the sentence as fully attributable.\n• More on the concept of “accurate represen-\ntation”. We take as inspiration the journalist’s\nconception of “accurate representation”. For\nexample, take this excerpt on Accuracy in the\nNPR Ethics Handbook: “When quoting or\nparaphrasing anyone . . . consider whether the\nsource would agree with the interpretation...”\nIn other words, if you had written the source\ndocument, consider whether you would view\nthe system response as an accurate representa-\ntion of information in that source document.\nC.3 Instructions: Intent Similarity\nIn this step, you will evaluate how much similar\nthe passage is to another passage (Figure 12).\nIn the interface, the passage A and passage B are\nboth text generated by a system—given the same\ncontext in which it was generated. You will use\nall three (context, passage A, passage B) to answer\nthe following question: How similar is the intent\nexpressed by Passage A and Passage B? Please\nignore any differences in details.\nTwo points are key when determining whether\nthe two passages convey the same intent:\n1. Judge the similarity solely based on the simi-\nlarity in the type and quantity of information\nprovided by each passage.\n2. Ignore any differences in factual details be-\ntween the two passages.\nConsider the following examples:\nContext: who pays medical bills in great\nbritain where does the money come from\nto pay these bills\nPassage A: Britain’s National Health\nService (NHS) is paid for through gen-\neral taxation and national insurance. In\n2017/18, the NHS budget was £176.5bn.\nPassage B:Britain’s National Health Ser-\nvice (NHS) is paid for through general\ntaxation. In 2017/18, the NHS budget\nwas £118bn.\nRating: Very similar. Passage A is about\nthe same topic as Passage B, with a sim-\nilar level of detail and style of presenta-\ntion. They may differ in factual details.\nThe above example should be rated “very similar”\nbecause both passages include information about\n(1) how the NHS is paid for, and (2) what its budget\n16495\nContext + Passage Evidences Notes\nContext: who played morticia in\nthe addams family tv show\nThe Addams Family is an Amer-\nican animated sitcom TV series.\nIt was first aired on NBC on\nSeptember 24, 1973. Carolyn\nJones played the role of Morti-\ncia.\n1/ The Addams Family (1973 TV series): The Addams\nFamily is an American animated sitcom adaptation of the\nCharles Addams comic. The series was produced in 1973\nand was rebroadcast the following season.\n2/ The Addams Family (TV Series 1964–1966): When\nThe Addams Family went off the air in 1966, network\nexecutives in charge of children’s programming for NBC\nbrought them back in 1973 for their own Saturday Morn-\ning cartoon show featuring the voices of Carolyn Jones\nfrom the original series.\nWhile the evidence supports the\nshow being aired in 1973, it\ndoesn’t specify the exact date\n(September 24, 1973).\nSimilarly, while the evidence\nmentions Carolyn Jones as be-\ning a voice actor, it doesn’t say\nshe played the role of Mortica.\nContext: when will the la\nsagrada familia be finished\nThe La Sagrada Familia is a\nlarge Roman Catholic church in\nBarcelona. It is designed by An-\ntoni Gaudi. It started construc-\ntion in 1882, and the construc-\ntion is still going on. The esti-\nmated date to finish is 2026.\n1/ Sagrada Família - Wikipedia: The Basílica i Temple Ex-\npiatori de la Sagrada Família is a church in the Eixample\ndistrict of Barcelona, Catalonia, Spain, and is currently\nthe largest unfinished Roman Catholic church.\n2/ Find Out Sagrada Familia’s Expected Finish Date: Vis-\niting the breathtaking Sagrada Familia today also means\nwitnessing the slow progress towards the completion of the\nproject. Sagrada Familia is now expected to be completed\nin 2026, the centenary of Gaudi’s death. It’s a reasonable\ninference that La Sagrada Familia is the same as Sagrada\nFamilia, even though the names differ slightly.\nWhile Evidence 2 mentions\nGaudi, it isn’t clear this is a ref-\nerence to Antoni Gaudi and fur-\nther doesn’t say that he designed\nthe church.\nTable 8: Additional examples for annotating attribution.\nin 2017/18 was, though they differ in their actual\nanswers to these questions.\nContext: who is the owner of reading\nfootball club\nPassage A: Reading’s owner is Yongge\nDai. Yongge Dai is also the president of\nChinese company Dai Yongge Real Es-\ntate. Yongge’sson, Dai Xiu Li, is Read-\ning’s vice-president.\nPassage B: Reading’s owner is Dai\nYongge. Yongge’sbrother and sister pair\nbehind the Reading FC takeover—Dai\nYongge and Dai Xiu Li—has made their\nfortune through a massive property em-\npire. Mr Dai, has been the chairman of\nRenhe Commercial since 1999, which\nis an organisation owned by his sister\nbehind a vast network of underground\nshopping centres in China.\nRating: Somewhat similar. Passage A is\nabout the same topic as Passage B, but\ndiffers substantially in level of detail or\nstyle of presentation. They may differ in\nfactual details.\nThe above example should be rated “somewhat\nsimilar” because both passages are still about the\nsame topic—Reading’s owner— but differ substan-\ntially in the information they discuss: Passage A\nincludes information about (1a) who Reading’s\nowner is, (2a) which company they are the pres-\nident of and (3a) who their vice-president is. In\ncontrast, while Passage B shares information about\n(1a), it also includes information about (2b) how\nthe Reading owner made their fortune, (3b) their\ncompany position and how long they held it for and\n(4b) what the company also owns.\nContext: what is the numbers of total\nelected member of indian parliment in\npresent time\nPassage A: The total number of elected\nmembers of the Lok Sabha is 543.\nPassage B: The total number of elected\nmembers of the Rajya Sabha is 238.\nRating: Not at all similar. Passage A is\nabout a significantly different topic than\nPassage B.\nEven though the passages look very similar, the\nabove example should be rated “not at all similar”\nbecause the two passages are about significantly\ndifferent topics: “the Lok Sabha” vs “the Rajya\nSabha”.\nD Details on the model\nFew-shot prompting with LLMs We imple-\nment many sub-tasks within RARR using few-\nshot prompting of LLMs (also known as in-context\nlearning (Brown et al., 2020)) as follows:\n1. For each sub-task, we manually author\na small number of training examples:\n16496\n(inputj, outputj) for j = 1, . . . , J, where\nJ ranges between 5 and 10 and where both\nthe input and output are strings.\n2. We form the following prompt: input1 ⋄\noutput1 ⊕input2 ⋄output2 ⊕. . .⊕inputJ ⋄\noutputJ ⊕new_input, where ⋄denotes a\nnewline character and ⊕denotes a double\nnewline character.\n3. To perform inference on a new input, we con-\ndition the LLM on the prompt and sample\ncontinuations of the prompt up until the next\ndouble newline character.\nAll of our prompts are included in Figures 13, 14,\nand 15. The contextual version used for QReCC\nare in Figures 16, 17, and 18.\nModel statistics We implemented most parts of\nRARR with the PALM model which has 540B pa-\nrameters. We prompted PALM without any train-\ning or finetuning. We used a TPU v2-128 to run\ninference with PALM.\nWe manually wrote our prompts by eye-balling\nquality on a dozen of examples from a separate\nvalidation set. We tune our hyperparameters on the\nvalidation set as well. We used sampling temper-\nature 0.7 for all generation tasks. For each input\ntext, we sample 3 question generations, and for\neach question we retrieve 5 results. For agreement\ngate and editing, we only sample 1 generation. We\nreject an editing if the edit distance is more than\n50 characters or more than half of the original text\nlength.\nE Details on the dataset\nAs explained in Section 5.1, we generated 150 de-\nvelopment and 150 test passages for each of the 6\ncombinations of dataset and model: (NQ, PaLM),\n(SQA, PaLM), (QReCC, LaMDA), (NQ, GPT-3),\n(SQA, GPT-3), (QReCC, GPT-3). Figures 19, 20,\n21, and 22 are the few-shot prompts used to gener-\nate the passages.\nFollowing the corresponding datasets, all gener-\nated passages are in English. The authors have man-\nually looked through most of the data and found no\npersonal identifiers.\n16497\n1 [web] I will check things you said and ask questions.\n2\n3 (1) You said: Your nose switches back and forth between nostrils. When you sleep, you switch about every 45 minutes. This\nis to prevent a buildup of mucus. It’s called the nasal cycle.\n4 To verify it,\n5 a) I googled: Does your nose switch between nostrils?\n6 b) I googled: How often does your nostrils switch?\n7 c) I googled: Why does your nostril switch?\n8 d) I googled: What is nasal cycle?\n9\n10 (2) You said: The Stanford Prison Experiment was conducted in the basement of Encina Hall, Stanford’s psychology building.\n11 To verify it,\n12 a) I googled: Where was Stanford Prison Experiment was conducted?\n13\n14 (3) You said: The Havel-Hakimi algorithm is an algorithm for converting the adjacency matrix of a graph into its adjacency\nlist. It is named after Vaclav Havel and Samih Hakimi.\n15 To verify it,\n16 a) I googled: What does Havel-Hakimi algorithm do?\n17 b) I googled: Who are Havel-Hakimi algorithm named after?\n18\n19 (4) You said: \"Time of My Life\" is a song by American singer-songwriter Bill Medley from the soundtrack of the 1987 film\nDirty Dancing. The song was produced by Michael Lloyd.\n20 To verify it,\n21 a) I googled: Who sings \"Time of My Life\"?\n22 b) I googled: Which film is \"Time of My Life\" from?\n23 c) I googled: Who produced the song \"Time of My Life\"?\n24\n25 (5) You said: Kelvin Hopins was suspended from the Labor Party due to his membership in the Conservative Party.\n26 To verify it,\n27 a) I googled: Why was Kelvin Hopins suspended from Labor Party?\n28\n29 (6) You said: Social work is a profession that is based in the philosophical tradition of humanism. It is an intellectual\ndiscipline that has its roots in the 1800s.\n30 To verify it,\n31 a) I googled: What philosophical tradition is social work based on?\n32 b) I googled: What year does social work has its root in?\n33\n34 (7) You said: {text}\n35 To verify it,\n36 _____\nFigure 13: Few-shot prompt for query generation. To increase diversity and coverage, we sample the model three\ntimes and combine the resulting lists of queries.\n16498\n1 [web] I will check some things you said.\n2\n3 (1) You said: Your nose switches back and forth between nostrils. When you sleep, you switch about every 45 minutes. This\nis to prevent a buildup of mucus. It’s called the nasal cycle.\n4 I checked: How often do your nostrils switch?\n5 I found this article: Although we don’t usually notice it, during the nasal cycle one nostril becomes congested and thus\ncontributes less to airflow, while the other becomes decongested. On average, the congestion pattern switches about every\n2 hours, according to a small 2016 study published in the journal PLOS One.\n6 Your nose’s switching time is about every 2 hours, not 45 minutes.\n7 This disagrees with what you said.\n8\n9 (2) You said: The Little House books were written by Laura Ingalls Wilder. The books were published by HarperCollins.\n10 I checked: Who published the Little House books?\n11 I found this article: These are the books that started it all – the stories that captured the hearts and imaginations of\nchildren and young adults worldwide. Written by Laura Ingalls Wilder and published by HarperCollins, these beloved books\nremain a favorite to this day.\n12 The Little House books were published by HarperCollins.\n13 This agrees with what you said.\n14\n15 (3) You said: The Stanford Prison Experiment was conducted in the basement of Jordan Hall, Stanford’s psychology building.\n16 I checked: Where was Stanford Prison Experiment conducted?\n17 I found this article: Carried out August 15-21, 1971 in the basement of Jordan Hall, the Stanford Prison Experiment set\nout to examine the psychological effects of authority and powerlessness in a prison environment.\n18 The Stanford Prison Experiment was conducted in Jordan Hall.\n19 This agrees with what you said.\n20\n21 (4) You said: Social work is a profession that is based in the philosophical tradition of humanism. It is an intellectual\ndiscipline that has its roots in the 1800s.\n22 I checked: When did social work have its roots?\n23 I found this article: The Emergence and Growth of the Social work Profession<br><br> Social work’s roots were planted in\nthe 1880s, when charity organization societies (COS) were created to organize municipal voluntary relief associations and\nsettlement houses were established.\n24 Social work has its roots in the 1880s, not 1800s.\n25 This disagrees with what you said.\n26\n27 (5) You said: The Havel-Hakimi algorithm is an algorithm for converting the adjacency matrix of a graph into its adjacency\nlist. It is named after Vaclav Havel and Samih Hakimi.\n28 I checked: What is the Havel-Hakimi algorithm?\n29 I found this article: The Havel-Hakimi algorithm constructs a special solution if a simple graph for the given degree\nsequence exists, or proves that one cannot find a positive answer. This construction is based on a recursive algorithm.\nThe algorithm was published by Havel (1955), and later by Hakimi (1962).\n30 Havel-Hakimi algorithm is for constructing a special solution if a simple graph for the given degree sequence exists, or\nproving that one cannot find a positive answer, not converting the adjacency matrix of a graph into its adjacency list.\n31 This disagrees with what you said.\n32\n33 (6) You said: \"Time of My Life\" is a song by American singer-songwriter Bill Medley from the soundtrack of the 1987 film\nDirty Dancing. The song was produced by Michael Lloyd.\n34 I checked: Who was the producer of \"(I’ve Had) The Time of My Life\"?\n35 I found this article: On September 8, 2010, the original demo of this song, along with a remix by producer Michael Lloyd,\nwas released as digital files in an effort to raise money for the Patrick Swayze Pancreas Cancer Resarch Foundation at\nStanford University.\n36 \"Time of My Life\" was produced by Michael Lloyd.\n37 This agrees with what you said.\n38\n39 (7) You said: Kelvin Hopins was suspended from the Labor Party because he had allegedly sexually harassed and behaved\ninappropriately towards a Labour Party activist, Ava Etemadzadeh.\n40 I checked: Why was Kelvin Hopins suspeneded from the Labor Party?\n41 I found this article: A former Labour MP has left the party before an inquiry into sexual harassment allegations against\nhim was able to be concluded, the party has confirmed. Kelvin Hopkins was accused in 2017 of inappropriate physical\ncontact and was suspended by the Labour party pending an investigation.This agrees with what you said.\n42 Kelvin Hopins was suspended because he had allegedly sexually harassed and behaved inappropriately towards a Labour Party\nactivist, Ava Etemadzadeh.\n43 This agrees with what you said.\n44\n45 (8) You said: In the battles of Lexington and Concord, the British side was led by General Thomas Smith.\n46 I checked: Who led the British side in the battle of Lexington and Concord?\n47 I found this article: Interesting Facts about the Battles of Lexington and Concord. The British were led by Lieutenant\nColonel Francis Smith. There were 700 British regulars.\n48 The British side was led by Lieutenant Colonel Francis Smith, not General Thomas Hall.\n49 This disagrees with what you said.\n50\n51 (9) You said: {text}\n52 I checked: {query}\n53 I found this article: {evidence}\n54 _____\nFigure 14: Few-shot prompt for the agreement model, which uses chain-of-thought prompting.\n16499\n1 [web] I will fix some things you said.\n2\n3 (1) You said: Your nose switches back and forth between nostrils. When you sleep, you switch about every 45 minutes. This\nis to prevent a buildup of mucus. It’s called the nasal cycle.\n4 I checked: How often do your nostrils switch?\n5 I found this article: Although we don’t usually notice it, during the nasal cycle one nostril becomes congested and thus\ncontributes less to airflow, while the other becomes decongested. On average, the congestion pattern switches about every\n2 hours, according to a small 2016 study published in the journal PLOS One.\n6 This suggests 45 minutes switch time in your statement is wrong.\n7 My fix: Your nose switches back and forth between nostrils. When you sleep, you switch about every 2 hours. This is to\nprevent a buildup of mucus. It’s called the nasal cycle.\n8\n9 (2) You said: In the battles of Lexington and Concord, the British side was led by General Thomas Hall.\n10 I checked: who led the British side in the battle of Lexington and Concord?\n11 I found this article: Interesting Facts about the Battles of Lexington and Concord. The British were led by Lieutenant\nColonel Francis Smith. There were 700 British regulars.\n12 This suggests General Thomas Hall in your statement is wrong.\n13 My fix: In the battles of Lexington and Concord, the British side was led by Lieutenant Colonel Francis Smith.\n14\n15 (3) You said: The Stanford Prison Experiment was conducted in the basement of Encina Hall, Stanford’s psychology building.\n16 I checked: where was Stanford Prison Experiment conducted.\n17 I found this article: Carried out August 15-21, 1971 in the basement of Jordan Hall, the Stanford Prison Experiment set\nout to examine the psychological effects of authority and powerlessness in a prison environment.\n18 This suggests Encina Hall in your statement is wrong.\n19 My fix: The Stanford Prison Experiment was conducted in the basement of Jordan Hall, Stanford’s psychology building.\n20\n21 (4) You said: Phoenix Mills Ltd., a diversified business conglomerate, was established in 1854. It has a history of over\n160 years.\n22 I checked: When was Phoenix Mills Ltd. founded?\n23 I found this article: Phoenix Mills Ltd was incorporated in the year 1905. The company began their operations as a textile\nmanufacturing company on 17.3 acres of land at Lower Parel in Mumbai. In the year 1959 the company was listed in the\nBombay Stock Exchange.\n24 This suggests the year of establishment 1854 in your statement is wrong.\n25 My fix: Phoenix Mills Ltd., a diversified business conglomerate, was established in 1905. It has a history of over 160\nyears.\n26\n27 (5) You said: The Havel-Hakimi algorithm is an algorithm for converting the adjacency matrix of a graph into its adjacency\nlist. It is named after Vaclav Havel and Samih Hakimi.\n28 I checked: What is the Havel-Hakimi algorithm?\n29 I found this article: The Havel-Hakimi algorithm constructs a special solution if a simple graph for the given degree\nsequence exists, or proves that one cannot find a positive answer. This construction is based on a recursive algorithm.\nThe algorithm was published by Havel (1955), and later by Hakimi (1962).\n30 This suggests the Havel-Hakimi algorithm’s functionality in your statement is wrong.\n31 My fix: The Havel-Hakimi algorithm constructs a special solution if a simple graph for the given degree sequence exists,\nor proves that one cannot find a positive answer. It is named after Vaclav Havel and Samih Hakimi\n32\n33 (6) You said: \"Time of My Life\" is a song by American singer-songwriter Bill Medley from the soundtrack of the 1987 film\nDirty Dancing. The song was produced by Phil Ramone.\n34 I checked: Who was the producer of \"(I’ve Had) The Time of My Life\"?\n35 I found this article: On September 8, 2010, the original demo of this song, along with a remix by producer Michael Lloyd,\nwas released as digital files in an effort to raise money for the Patrick Swayze Pancreas Cancer Resarch Foundation at\nStanford University.\n36 This suggests \"Time of My Life\" producer name in your statement is wrong.\n37 My fix: \"Time of My Life\" is a song by American singer-songwriter Bill Medley from the soundtrack of the 1987 film Dirty\nDancing. The song was produced by Michael Lloyd.\n38\n39 (7) You said: Phoenix Market City Pune is located on 21 acres of prime property in Pune. It is spread across four levels\nwith approximately 1.4 million square feet of built-up space. The mall is owned and operated by Phoenix Mills Limited.\n40 I checked: What is the area of Phoenix Market City in Pune?\n41 I found this article: Phoenix Market City was opened in January 2013 and has the distinction of being the largest mall\nin the city of Pune, with the area of 3.4 million square feet. It is located in the Viman Nagar area of Pune.\n42 This suggests the 1.4 million square feet of built-up space in your statment is wrong.\n43 My fix: Phoenix Market City Pune is located on 21 acres of prime property in Pune. It is spread across four levels with\napproximately 3.4 million square feet of built-up space. The mall is owned and operated by Phoenix Mills Limited.\n44\n45 (8) You said: {text}\n46 I checked: {query}\n47 I found this article: {evidence}\n48 This suggests _____\nFigure 15: Few-shot prompt for the revision model, which uses chain-of-thought prompting.\n16500\n1 [web] I will read the context and check only the last thing you said by asking questions.\n2\n3 (1) Context: Your nose switches back and forth between nostrils. When you sleep, you switch about every 45 minutes.\n4 You said: This is to prevent a buildup of mucus. It’s called the nasal cycle.\n5 To verify what you just said,\n6 a) I googled: Why does your nostril switch during sleep?\n7 b) I googled: What is nasal cycle?\n8 c) I googled: What is the nostril switching during sleep called?\n9\n10 (2) Context: The Stanford Prison Experiment was conducted in the basement of Encina Hall, Stanford’s psychology building.\n11 You said: It is a psychological study to observe the behaviors of conflict and violence that happen between inmates and\nprisoners in real prisons.\n12 To verify what you just said,\n13 a) I googled: What type of experiment was the Stanford Prison Experiment?\n14 b) I googled: What was the objective of the Stanford Prison Experiment?\n15\n16 (3) Context: The Havel-Hakimi algorithm is an algorithm for converting the adjacency matrix of a graph into its adjacency\nlist.\n17 You said: It is named after Vaclav Havel and Samih Hakimi.\n18 To verify what you just said,\n19 a) I googled: Who are Havel-Hakimi algorithm named after?\n20\n21 (4) Context: \"Time of My Life\" is a song by American singer-songwriter Bill Medley from the soundtrack of the 1987 film\nDirty Dancing.\n22 You said: The song was produced by Michael Lloyd in the same year.\n23 To verify what you just said,\n24 a) I googled: Who produced the song \"Time of My Life\"?\n25 b) I googled: When was the song \"Time of My Life\" by Bill Medley produced?\n26\n27 (5) Context: The Late Show with Stephen Colbert is an American late-night talk show hosted by Stephen Colbert, which\npremiered on September 8, 2015.\n28 You said: Produced by Spartina Productions and CBS Television Studios, it is the second iteration of CBS’ Late Show\nfranchise.\n29 To verify what you just said,\n30 a) I googled: Who produces \"The Late Show with Stephen Colbert\"?\n31 b) I googled: What are the iterations of CBS’ Late Show franchise?\n32\n33 (6) Context: Super Mario Sunshine was released on GameCube in 2002. In the game, Mario uses a tool strapped to his back\ncalled FLUDD, which stands for The Flash Liquidizer Ultra Dousing Device.\n34 You said: It can be used to spray water at objects or enemies. This allows Mario to change his movements, kill enemies,\nor clean up hazards on the floor.\n35 To verify what you just said,\n36 a) I googled: What is the main function of FLUDD in Super Mario Sunshine?\n37 b) I googled: What can FLUDD in Super Mario Sunshine be used on?\n38 c) I googled: In Super Mario Sunshine, can Mario change movement with FLUDD?\n39 d) I googled: In Super Mario Sunshine, can Mario kill enemies with FLUDD?\n40 e) I googled: In Super Mario Sunshine, can Mario clean up hazards on the floor with FLUDD?\n41\n42 (7) Context: {context}\n43 You said: {text}\n44 To verify what you just said,\n45 _____\nFigure 16: Contextual version of the query generation prompt. The prompt works well for dialog contexts from\nQReCC even though the few-shot examples are not formatted as such.\n16501\n1 [web] I will check some things you said.\n2\n3 (1) Context: Your nose switches back and forth between nostrils. It’s called the nasal cycle. This is to prevent a buildup\nof mucus.\n4 You said: When you sleep, you switch about every 45 minutes.\n5 I checked: How often do your nostrils switch?\n6 I found this article: Although we don’t usually notice it, during the nasal cycle one nostril becomes congested and thus\ncontributes less to airflow, while the other becomes decongested. On average, the congestion pattern switches about every\n2 hours, according to a small 2016 study published in the journal PLOS One.\n7 Your nose’s switching time is about every 2 hours, not 45 minutes.\n8 This disagrees with what you said.\n9\n10 (2) Context: The Little House books is a series of American children’s novels.\n11 You said: The books were published by HarperCollins.\n12 I checked: Who published the Little House books?\n13 I found this article: These are the books that started it all – the stories that captured the hearts and imaginations of\nchildren and young adults worldwide. Written by Laura Ingalls Wilder and published by HarperCollins, these beloved books\nremain a favorite to this day.\n14 The Little House books were published by HarperCollins.\n15 This agrees with what you said.\n16\n17 (3) Context: The Stanford Prison Experiment is a psychological study to observe the behaviors of conflict and violence\nthat happen between inmates and prisoners in real prisons.\n18 You said: It was conducted in the basement of Jordan Hall, Stanford’s psychology building.\n19 I checked: Where was Stanford Prison Experiment conducted?\n20 I found this article: Carried out August 15-21, 1971 in the basement of Jordan Hall, the Stanford Prison Experiment set\nout to examine the psychological effects of authority and powerlessness in a prison environment.\n21 The Stanford Prison Experiment was conducted in Jordan Hall.\n22 This agrees with what you said.\n23\n24 (4) Context: Social work is a profession that is based in the philosophical tradition of humanism.\n25 You said: It is an intellectual discipline that has its roots in the 1800s.\n26 I checked: When did social work have its roots?\n27 I found this article: The Emergence and Growth of the Social work Profession<br><br> Social work’s roots were planted in\nthe 1880s, when charity organization societies (COS) were created to organize municipal voluntary relief associations and\nsettlement houses were established.\n28 Social work has its roots in the 1880s, not 1800s.\n29 This disagrees with what you said.\n30\n31 (5) Context: The Havel-Hakimi algorithm is named after Vaclav Havel and Samih Hakimi.\n32 You said: It is an algorithm for converting the adjacency matrix of a graph into its adjacency list.\n33 I checked: What is the Havel-Hakimi algorithm?\n34 I found this article: The Havel-Hakimi algorithm constructs a special solution if a simple graph for the given degree\nsequence exists, or proves that one cannot find a positive answer. This construction is based on a recursive algorithm.\nThe algorithm was published by Havel (1955), and later by Hakimi (1962).\n35 Havel-Hakimi algorithm is for constructing a special solution if a simple graph for the given degree sequence exists, or\nproving that one cannot find a positive answer, not converting the adjacency matrix of a graph into its adjacency list.\n36 This disagrees with what you said.\n37\n38 (6) Context: \"Time of My Life\" is a song by American singer-songwriter Bill Medley from the soundtrack of the 1987 film\nDirty Dancing.\n39 You said: The song was produced by Michael Lloyd in the same year.\n40 I checked: Who was the producer of \"(I’ve Had) The Time of My Life\"?\n41 I found this article: On September 8, 2010, the original demo of this song, along with a remix by producer Michael Lloyd,\nwas released as digital files in an effort to raise money for the Patrick Swayze Pancreas Cancer Resarch Foundation at\nStanford University.\n42 The song \"Time of My Life\" was produced by Michael Lloyd.\n43 This agrees with what you said.\n44\n45 (7) Context: Super Mario Sunshine was released on GameCube in 2002. In the game, Mario uses a tool strapped to his back\ncalled FLUDD.\n46 You said: FLUDD stands for Functional Language in a Unified Design Discipline. It can be used to spray water at objects\nor enemies. This allows Mario to change his movements, kill enemies, or clean up hazards on the floor.\n47 I checked: What does FLUDD stands for in Super Mario Sunshine?\n48 I found this article: The Flash Liquidizer Ultra Dousing Device, abbreviated and better known as FLUDD or F.L.U.D.D.,\nis a multipurpose water pack from Super Mario Sunshine invented by Professor Elvin Gadd, indicated by the Gadd Science,\nIncorporated logo at the base of its nozzle exclusively during the cutscene at Pinna Park.\n49 In Super Mario Sunshine, FLUDD stands for the Flash Liquidizer Ultra Dousing Device, not Functional Language in a Unified\nDesign Discipline.\n50 This disagrees with what you said.\n51\n52 (8) Context: {context}\n53 You said: {text}\n54 I checked: {query}\n55 I found this article: {evidence}\n56 _____\nFigure 17: Contextual version of the agreement model prompt.\n16502\n1 [web] I will fix some things you said.\n2\n3 (1) Context: Your nose switches back and forth between nostrils. It’s called the nasal cycle. This is to prevent a buildup\nof mucus.\n4 You said: When you sleep, you switch about every 45 minutes.\n5 I checked: How often do your nostrils switch?\n6 I found this article: Although we don’t usually notice it, during the nasal cycle one nostril becomes congested and thus\ncontributes less to airflow, while the other becomes decongested. On average, the congestion pattern switches about every\n2 hours, according to a small 2016 study published in the journal PLOS One.\n7 This suggests 45 minutes switch time in your statement is wrong.\n8 My fix: When you sleep, you switch about every 2 hours.\n9\n10 (2) Context: The Little House books is a series of American children’s novels.\n11 You said: The books were published by Amberjack Publishing.\n12 I checked: Who published the Little House books?\n13 I found this article: These are the books that started it all – the stories that captured the hearts and imaginations of\nchildren and young adults worldwide. Written by Laura Ingalls Wilder and published by HarperCollins, these beloved books\nremain a favorite to this day.\n14 This suggests Amberjack Publishing in your statement is wrong.\n15 My fix: The books were published by HarperCollins.\n16\n17 (3) Context: The Stanford Prison Experiment is a psychological study to observe the behaviors of conflict and violence\nthat happen between inmates and prisoners in real prisons.\n18 You said: It was conducted in the basement of Encina Hall, Stanford’s psychology building.\n19 I checked: where was Stanford Prison Experiment conducted.\n20 I found this article: Carried out August 15-21, 1971 in the basement of Jordan Hall, the Stanford Prison Experiment set\nout to examine the psychological effects of authority and powerlessness in a prison environment.\n21 This suggests Encina Hall in your statement is wrong.\n22 My fix: It was conducted in the basement of Jordan Hall, Stanford’s psychology building.\n23\n24 (4) Context: The Havel-Hakimi algorithm is named after Vaclav Havel and Samih Hakimi.\n25 You said: It is an algorithm for converting the adjacency matrix of a graph into its adjacency list.\n26 I checked: What is the Havel-Hakimi algorithm?\n27 I found this article: The Havel-Hakimi algorithm constructs a special solution if a simple graph for the given degree\nsequence exists, or proves that one cannot find a positive answer. This construction is based on a recursive algorithm.\nThe algorithm was published by Havel (1955), and later by Hakimi (1962).\n28 This suggests the Havel-Hakimi algorithm’s functionality in your statement is wrong.\n29 My fix: It constructs a special solution if a simple graph for the given degree sequence exists, or proves that one cannot\nfind a positive answer.\n30\n31 (5) Context: \"Time of My Life\" is a song by American singer-songwriter Bill Medley from the soundtrack of the 1987 film\nDirty Dancing.\n32 You said: The song was produced by Phil Ramone in the same year.\n33 I checked: Who was the producer of \"(I’ve Had) The Time of My Life\"?\n34 I found this article: On September 8, 2010, the original demo of this song, along with a remix by producer Michael Lloyd,\nwas released as digital files in an effort to raise money for the Patrick Swayze Pancreas Cancer Resarch Foundation at\nStanford University.\n35 This suggests \"Time of My Life\" producer name in your statement is wrong.\n36 My fix: The song was produced by Michael Lloyd in the same year.\n37\n38 (6) Context: Phoenix Market City Pune is located on 21 acres of prime property in Pune. The mall is owned and operated by\nPhoenix Mills Limited.\n39 You said: It is spread across four levels with approximately 1.4 million square feet of built-up space.\n40 I checked: What is the area of Phoenix Market City in Pune?\n41 I found this article: Phoenix Market City was opened in January 2013 and has the distinction of being the largest mall\nin the city of Pune, with the area of 3.4 million square feet. It is located in the Viman Nagar area of Pune.\n42 This suggests the 1.4 million square feet of built-up space in your statment is wrong.\n43 My fix: It is spread across four levels with approximately 3.4 million square feet of built-up space.\n44\n45 (7) Context: {context}\n46 You said: {text}\n47 I checked: {query}\n48 I found this article: {evidence}\n49 This suggests _____\nFigure 18: Contextual version of the revision model prompt.\n16503\n1 [web] I will think step by step and answer your question.\n2\n3 Question: is growing seedless cucumber good for a gardener with entomophobia\n4 Explanation: Entomophobia is a fear of insects. Plants need insects to pollinate them. Seedless fruits such as seedless\ncucumbers do not require pollination, so seedless fruits do not require insects. This makes good for people with\nentomophobia.\n5 Answer: Yes\n6\n7 Question: Who was british pm and viceroy during quit india movement?\n8 Explanation: The Quit India Movement was launched in 8th August 1942. The british PM at that time was Winston Churchill.\nThe british viceroy during the movement is Victor Hope, usually referred to as Lord Linlithgow.\n9 Answer: Winston Churchil and Lord Linlithgow\n10\n11 Question: Which year does game over man come out on netflix?\n12 Explanation: Game Over, Man! is an action-comedy movie released in March 23, 2018, on Netflix. It’s director is Kyle\nNewacheck. The movie stars Anders Holm, Adam DeVine, and Blake Anderson.\n13 Answer: March 23, 2018\n14\n15 Question: would it be very difficult for Nuno Gomes to dive to the Red Sea’s deepest point?\n16 Explanation: Nuno Gomes’ deepest dive in the Red Sea to date is 317 metres. The Red Sea has a maximum depth of over 3,000\nmetres. So it would be difficult for Nuno Gomes to the deepest point of the Red sea.\n17 Answer: Yes\n18\n19 Question: Are chinchillas cold-blooded?\n20 Explanation: Chinchillas are rodents, which are mammals. All mammals are warm-blooded.\n21 Answer: No\n22\n23 Question: what are the chemo drugs for gastric cancer?\n24 Explanation: Chemotherapy can be given for gastric cancer before or after surgery. Usually you have a combination of 2 or\n3 drugs. The most common types are: epirubicin, cisplatin and capecitabine (ECX) ; epirubicin, cisplatin and fluorouracil\n(ECF); or fluorouracil, folinic acid, oxaliplatin and docetaxel (FLOT).\n25 Answer: epirubicin, cisplatin and capecitabine (ECX) ; epirubicin, cisplatin and fluorouracil (ECF); or fluorouracil,\nfolinic acid, oxaliplatin and docetaxel (FLOT)\n26\n27 Question: How many sacks does Clay Matthews have in his career?\n28 Explanation: Clay Matthews III had 383 solo tackles, 136 assists, and 91.5 sacks. His father, Clay Matthews Jr., also\npalyed in NFL and had 69.5 sacks in his career.\n29 Answer: 91.5 (Clay Matthews III) and 69.5 (Clay Matthews Jr.)\n30\n31 Question: In the U.S., the most deaths are caused by\n32 Explanation: The leading cause of death in the U.S. is heart disease, causing nearly 700,000 deaths annually. The second\nmost common cause of deaths is cancer.\n33 Answer: heart disease\n34\n35 Question: the rank of indian economy in terms of nominal gdp is\n36 Explanation: India’s GDP is economy is $2.94 trillion. It is the fifth-largest in the world. The top GDP contries are\nUnited States, China, Japan, Germany and India.\n37 Answer: 5\n38\n39 Question: {question}\n40 Explanation: _____\nFigure 19: The PaLM prompt for generating long-form answers to questions from NQ and SQA.\n16504\n1 I will think step by step and answer your question.\n2\n3 1. Question: Is growing seedless cucumber good for a gardener with entomophobia?\n4 2. Explanation: Entomophobia is a fear of insects. Plants need insects to pollinate them. Seedless fruits such as seedless\ncucumbers do not require pollination so seedless fruits do not require insects. This is good for people with entomophobia.\n5 3. Answer: Yes.\n6\n7 1. Question: Who was British PM and Biceroy during Quit India Movement?\n8 2. Explanation: The Quit India Movement was launched in 8th August 1942. The British PM at that time was Winston Churchill.\nThe British Biceroy during the movement was Victor Hope, usually referred to as Lord Linlithgow.\n9 3. Answer: Winston Churchil and Lord Linlithgow.\n10\n11 1. Question: Which year does Game Over Man come out on Netflix?\n12 2. Explanation: Game Over, Man! is an action-comedy movie. Its director is Kyle Newacheck. The movie stars Anders Holm,\nAdam DeVine, and Blake Anderson. The movie was released March 23, 2018 on Netflix.\n13 3. Answer: March 23, 2018.\n14\n15 1. Question: Would it be very difficult for Nuno Gomes to dive to the Red Sea’s deepest point?\n16 2. Explanation: Nuno Gomes’ deepest dive in the Red Sea to date is 317 meters. The Red Sea has a maximum depth of over\n3,000 meters. So it would be difficult for Nuno Gomes to dive to the deepest point of the Red Sea.\n17 3. Answer: Yes.\n18\n19 1. Question: Are chinchillas cold-blooded?\n20 2. Explanation: Chinchillas are rodents. Rodents are mammals. All mammals are warm-blooded.\n21 3. Answer: No.\n22\n23 1. Question: What are the chemo drugs for gastric cancer?\n24 2. Explanation: Chemotherapy can be given for gastric cancer before or after surgery. Usually you have a combination\nof 2 or 3 drugs. The most common types are: epirubicin, cisplatin and capecitabine (ECX) ; epirubicin, cisplatin and\nfluorouracil (ECF); or fluorouracil, folinic acid, oxaliplatin and docetaxel (FLOT).\n25 3. Answer: Epirubicin, cisplatin and capecitabine (ECX) ; epirubicin, cisplatin and fluorouracil (ECF); or fluorouracil,\nfolinic acid, oxaliplatin and docetaxel (FLOT).\n26\n27 1. Question: How many sacks does Clay Matthews have in his career?\n28 2. Explanation: Clay Matthews has been pro linebacker for 11 seasons. Ten of the seasons he played for Green Bay Packers,\nwhile his last season was with Los Angeles Rams, dedicated to his hometown. Clay Matthews III had 383 solo tackles, 136\nassists, and 91.5 sacks. His father, Clay Matthews Jr., also played in NFL and had 69.5 sacks in his career.\n29 3. Answer: 91.5 (Clay Matthews III) and 69.5 (Clay Matthews Jr.).\n30\n31 1. Question: In the U.S., the most deaths are caused by?\n32 2. Explanation: According to the Centers for Disease Control and Prevention, there were 3 million deaths in 2018. The\nleading cause of death in the U.S. is heart disease, causing nearly 700,000 deaths annually. The second most common cause\nof deaths is cancer.\n33 3. Answer: Heart disease.\n34\n35 1. Question: The rank of Indian economy in terms of nominal GDP is?\n36 2. Explanation: The top GDP contries are United States, China, Japan, Germany, and India. India’s GDP is economy is $2.94\ntrillion. It is the fifth-largest economy in the world.\n37 3. Answer: 5.\n38\n39 1. Question: Who won season 16 on Dancing with the Stars?\n40 2. Explanation: Dancing with the Stars is an American dance competition television series that premiered on June 1, 2005,\non ABC. The 16th season premiered on March 18, 2013. It was won by Kellie Pickler.\n41 3. Answer: Kellie Pickler.\n42\n43 1. Question: {question}\n44 2. Explanation: _____\nFigure 20: The GPT-3 prompt for generating long-form answers to questions from NQ and SQA.\n16505\n1 Hi, I will think step by step and answer your question.\n2 Is growing seedless cucumber good for a gardener with entomophobia?\n3 Yes. Entomophobia is a fear of insects. Plants need insects to pollinate them. Seedless cucumber fruit does not require\npollination. So seedless cucumber does not require insects.\n4 Here is another question. Which year does game over man come out on netflix?\n5 Game Over, Man! is an American action-comedy movie. It was released on March 23, 2018, on Netflix. It is directed by Kyle\nNewacheck, written by Anders Holm, and starring himself, Adam DeVine, and Blake Anderson.\n6 Here is another question. What are the chemo drugs for gastric cancer?\n7 Chemotherapy can be given for gastric cancer before or after surgery. Usually you have a combination of 2 or 3 drugs. The\nmost common types are: epirubicin, cisplatin and capecitabine (ECX) ; epirubicin, cisplatin and fluorouracil (ECF); or\nfluorouracil, folinic acid, oxaliplatin and docetaxel (FLOT).\n8 Here is another question. Was N.W.A a album?\n9 N.W.A was an American hip hop group from Compton, California.\n10 Were they successful?\n11 They has sold over 10 million units in the United States alone.\n12 How many singles did it have?\n13 The group NWA released 8 singles.\n14 Here is another question. {Q1}\n15 {A1}\n16 ...\n17 {Qk}\n18 _____\nFigure 21: The LaMDA prompt for generating answers to questions from QReCC. Each line is a conversation turn.\nThe dialog context from QReCC contains rounds of questions and answers (Q1, A1, Q2, A2, . . . , Qk).\n1 I will think step by step and answer your question.\n2\n3 Is growing seedless cucumber good for a gardener with entomophobia?\n4 Yes. Entomophobia is a fear of insects. Plants need insects to pollinate them. Seedless cucumber fruit does not require\npollination. So seedless cucumber does not require insects.\n5\n6 Which year does game over man come out on netflix?\n7 Game Over, Man! is an American action-comedy movie. It was released on March 23, 2018, on Netflix. It is directed by Kyle\nNewacheck, written by Anders Holm, and starring himself, Adam DeVine, and Blake Anderson.\n8\n9 What are the chemo drugs for gastric cancer?\n10 Chemotherapy can be given for gastric cancer before or after surgery. Usually you have a combination of 2 or 3 drugs. The\nmost common types are: epirubicin, cisplatin and capecitabine (ECX) ; epirubicin, cisplatin and fluorouracil (ECF); or\nfluorouracil, folinic acid, oxaliplatin and docetaxel (FLOT).\n11\n12 Was N.W.A an album?\n13 N.W.A was an American hip hop group from Compton, California.\n14 Were they successful?\n15 They has sold over 10 million units in the United States alone.\n16 How many singles did they have?\n17 N.W.A had eight singles, including \"Straight Outta Compton\", \"Express Yourself\", \"Gangsta Gangsta\", \"Dopeman\" and \"Alwayz\nInto Somethin’\".\n18\n19 {Q1}\n20 {A1}\n21 ...\n22 {Qk}\n23 _____\nFigure 22: The GPT-3 prompt for generating answers to questions from QReCC.\n16506\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n8\n□\u0013 A2. Did you discuss any potential risks of your work?\n8,9\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n2,3,5\n□\u0013 B1. Did you cite the creators of artifacts you used?\n2,3,5\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nE\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n5\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nE\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n5,E\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n5\nC □\u0013 Did you run computational experiments?\n5,6\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n5,D\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n16507\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n5,D\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n5,B,D\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\n5,C\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nC\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nC\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nC\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nC\n16508",
  "topic": "Computational linguistics",
  "concepts": [
    {
      "name": "Computational linguistics",
      "score": 0.5668485760688782
    },
    {
      "name": "Chen",
      "score": 0.5596771836280823
    },
    {
      "name": "Computer science",
      "score": 0.5267738103866577
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.44789016246795654
    },
    {
      "name": "Linguistics",
      "score": 0.41068288683891296
    },
    {
      "name": "Natural language processing",
      "score": 0.33499300479888916
    },
    {
      "name": "Philosophy",
      "score": 0.25610387325286865
    },
    {
      "name": "Physics",
      "score": 0.12191757559776306
    },
    {
      "name": "Geology",
      "score": 0.0780387818813324
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}