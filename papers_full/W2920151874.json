{
  "title": "EvalAI: Towards Better Evaluation Systems for AI Agents",
  "url": "https://openalex.org/W2920151874",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4301238512",
      "name": "Yadav, Deshraj",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748639566",
      "name": "Jain, Rishabh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226671783",
      "name": "Agrawal, Harsh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287303958",
      "name": "Chattopadhyay, Prithvijit",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3160887632",
      "name": "Singh Taranjeet",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2188833392",
      "name": "Jain Akash",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Singh, Shiv Baran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221547106",
      "name": "Lee, Stefan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2478497110",
      "name": "Batra, Dhruv",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2952632681",
    "https://openalex.org/W2953119472",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2597985671",
    "https://openalex.org/W2342662072",
    "https://openalex.org/W2599765304",
    "https://openalex.org/W2615146352",
    "https://openalex.org/W2950697717",
    "https://openalex.org/W3037207827",
    "https://openalex.org/W2132862423",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2795601629",
    "https://openalex.org/W2964210218",
    "https://openalex.org/W1983927101",
    "https://openalex.org/W2522340145",
    "https://openalex.org/W3103780890",
    "https://openalex.org/W2776202271",
    "https://openalex.org/W2328886022",
    "https://openalex.org/W2900756484",
    "https://openalex.org/W2952228917",
    "https://openalex.org/W2949650786"
  ],
  "abstract": "We introduce EvalAI, an open source platform for evaluating and comparing machine learning (ML) and artificial intelligence algorithms (AI) at scale. EvalAI is built to provide a scalable solution to the research community to fulfill the critical need of evaluating machine learning models and agents acting in an environment against annotations or with a human-in-the-loop. This will help researchers, students, and data scientists to create, collaborate, and participate in AI challenges organized around the globe. By simplifying and standardizing the process of benchmarking these models, EvalAI seeks to lower the barrier to entry for participating in the global scientific effort to push the frontiers of machine learning and artificial intelligence, thereby increasing the rate of measurable progress in this domain.",
  "full_text": "EvalAI:\nTowards Better Evaluation Systems for AI Agents\nDeshraj Yadav1 Rishabh Jain1 Harsh Agrawal1 Prithvijit Chattopadhyay1\nTaranjeet Singh2 Akash Jain3 Shiv Baran Singh4 Stefan Lee1\nDhruv Batra1\n1Georgia Institute of Technology 2Paytm 3 Zomato 4Cyware\n{deshraj,rishabhjain,harsh.agrawal,prithvijit3,steflee,dbatra}@gatech.edu\n{reachtotj,akashjain993,spyshiv}@gmail.com\nAbstract\nWe introduce EvalAI, an open source platform for evaluating and comparing ma-\nchine learning (ML) and artiﬁcial intelligence algorithms (AI) at scale. EvalAI is\nbuilt to provide a scalable solution to the research community to fulﬁll the critical\nneed of evaluating machine learning models and agents acting in an environment\nagainst annotations or with a human-in-the-loop. This will help researchers, stu-\ndents, and data scientists to create, collaborate, and participate in AI challenges\norganized around the globe. By simplifying and standardizing the process of bench-\nmarking these models, EvalAI seeks to lower the barrier to entry for participating\nin the global scientiﬁc effort to push the frontiers of machine learning and artiﬁcial\nintelligence, thereby increasing the rate of measurable progress in this domain.\nOur code is available here.\n1 Introduction\nTime and again across different scientiﬁc and engineering ﬁelds, the formulation and creation of the\nright question, task, and dataset to study a problem has coalesced ﬁelds around particular challenges\n– driving scientiﬁc progress. Likewise, progress on important problems in the ﬁelds of Computer\nVision (CV) and Artiﬁcial Intelligence (AI) has been driven by the introduction of bold new tasks\ntogether with the curation of large, realistic datasets [23, 2].\nNot only do these tasks and datasets establish new problems and provide data necessary to address\nthem, but importantly they also establish reliable benchmarks where proposed solutions and hypothe-\nsis can be tested which is an essential part of the scientiﬁc process. In recent years, the development\nof centralized evaluation platforms have lowered the barrier to compete and share results on these\nproblems. As a result, a thriving community of data scientists and researchers has grown around\nthese tasks, increasing the pace of progress and technical dissemination.\nHistorically, the community has focused on traditional AI tasks such as image classiﬁcation, scene\nrecognition, and sentence parsing that follow a standard input-output paradigm for which models can\nbe evaluated in isolation using simple automatic metrics like accuracy, precision or recall. But with\nthe success of deep learning techniques on a wide variety of tasks and the proliferation of ‘smart’\napplications, there is an imminent need to evaluate AI systems in the context of human collaborators\nand not just in isolation. This is especially true as AI systems become more commonplace and we\nﬁnd ourselves interacting with AI agents on a daily basis. For instance, people frequently interact\nwith virtual assistants like Alexa, Siri, or Google Assistant to get answers to their questions, to book\nappointments at a restaurant, or to reply to emails and messages automatically. Another such example\nis the use of AI for recognizing content in images, helping visually impaired users interpret the\nsurrounding scene. To this end, the AI community has introduced several challenging high-level AI\ntasks ranging from question-answering about multiple modalities (short articles [ 29], images [?],\narXiv:1902.03570v1  [cs.AI]  10 Feb 2019\nFigure 1: EvalAI is a new evaluation platform with the overarching goal of providing the right tools, infrastructure\nand framework to setup exhaustive evaluation protocols for both traditional static evaluation tasks as well as\nthose in dynamic environments hosting multiple agents and/or humans.\nvideos [33]) to goal oriented dialog [ 4] to agents acting in simulated environments to complete\ntask-speciﬁc goals [9, 22, 36].\nAs AI improves and takes on these increasingly difﬁcult, high-level tasks that are poorly described by\nan input-output paradigm, robust evaluation faces a number of challenges. For instance, generating\na natural language description for an image, having a conversation with a human, or generating\naesthetically pleasing images cannot be evaluated accurately using automatic metrics as performance\non these metrics do not correlate well with human-judgment in practice [ 6]. Such tasks naturally\nrequire human-in-the-loop evaluation by connecting the AI system with a human workforce such\nas Amazon Mechanical Turk (AMT) [ 1] to quantify performance in a setup which is closest to\nthe one in which they may be eventually deployed. Moreover, human-AI interaction also reveal\ninteresting insights into the true capabilities of machine learning models. For instance, [6] connected\nhuman users with AI agents trained to answer questions about images in a 20-questions style image\nguessing game and then measured the performance of the human-AI team. The authors observed\nfrom the experiments that surprisingly, performance gained through AI-AI self-play does not seem to\ngeneralize to human-AI teams. These sort of useful insights are increasingly becoming important as\nmore and more of these models reach consumers.\nFurthermore, the rise of reinforcement learning based problems in which an agent must interact with an\nenvironment introduces additional challenges for benchmarking. In contrast to the supervised learning\nsetting where performance is measured by evaluating on a static test set, it is less straightforward to\nmeasure generalization performance of these agents in context of the interactions with the environment.\nEvaluating these agents involves running the associated code on a collection of unseen environments\nthat constitutes a hidden test set for such a scenario.\nTo address the aforementioned problems, we introduce a new evaluation platform for AI tasks called\nEvalAI. It is an extensible open-source platform that fulﬁlls the critical need in the community for (1)\nhuman-in-the-loop evaluation of machine learning models and (2) the ability to run user’s code in a\ndynamic environment to support the evaluation of interactive agents. We have also addressed several\nlimitations of existing platforms by supporting (3) custom evaluation pipelines compatible with any\nprogramming language, (4) arbitrary number of challenge phases and dataset splits, and (5) remote\nevaluation on private worker pool. By providing the functionality to connect agents, environments,\nand human evaluators in one single platform, EvalAI enables novel research directions to be explored\nquickly and at scale.\n2\nFeatures OpenML Topcoder Kaggle CrowdAI ParlAI CodaLab EvalAI\nAI Challenge Hosting \u0017 \u0013 \u0013 \u0013 \u0017 \u0013 \u0013\nCustom metrics \u0017 \u0017 \u0017 \u0013 \u0013 \u0013 \u0013\nMultiple phases/splits \u0017 \u0017 \u0017 \u0013 \u0017 \u0013 \u0013\nOpen Source \u0013 \u0017 \u0017 \u0013 \u0013 \u0013 \u0013\nRemote Evaluation \u0017 \u0017 \u0017 \u0017 \u0013 \u0013 \u0013\nHuman Evaluation \u0017 \u0017 \u0017 \u0017 \u0013 \u0017 \u0013\nEnvironments \u0017 \u0017 \u0017 \u0013 \u0017 \u0017 \u0013\nTable 1: Head-to-head comparison of capabilities between existing platforms and EvalAI\n2 Desiderata\nHaving outlined the need for an evaluation platform that can properly benchmark increasingly\ncomplex AI tasks, in this section we explicitly specify the following requirements that a modern\nevaluation tool should satisfy.\nHuman-in-the-loop evaluation of agents. As discussed in the Sec. 1, the AI community has\nintroduced increasingly bold tasks (goal-oriented dialog, question-answering, GuessWhich, image\ngeneration, etc.) some of which require pairing the AI agent with a human to accurately evaluate and\nbenchmark different approaches against each other. A modern evaluation platform should provide a\nuniﬁed framework for benchmarking in scenarios in which agents are not acting in isolation, but are\nrather interacting with other agents or humans at test-time.\nEnvironments, not datasets.As the community becomes more ambitious in the problems they\nare trying to solve, we have noticed a shift from static datasets to dynamic environments. Now\ninstead of evaluating a model on a single task, agents are deployed in new unseen environments\ninside a simulation to check for generalization to novel, unseen scenarios [22, 9]. As such, modern\nevaluation platforms need to be capable of running “submitted” agents within these environments – a\nsigniﬁcant departure from the standard evaluation paradigm of computing automatic metrics on a set\nof submitted predictions.\nExtensibility. Different tasks require different evaluation protocols. An evaluation platform needs to\nsupport an arbitrary number of phases and dataset splits to cater to the evaluation scenarios which\noften use multiple dataset splits, each serving a different purpose. For instance, COCO Challenge [25],\nVQA [2], and Visual Dialog [10] all use multiple splits such as test-dev for validation, test-std for\nreporting results in a paper and a separate test-challenge split for announcing the winners of a\nchallenge that may be centered around the task.\n3 Related Work\nHere we survey some of the existing evaluation platforms in light of the requirements highlighted in\nthe previous section. Additionally, for reader’s convenience, we summarize the features offered by\nEvalAI via head-to-head comparison with the existing platforms in Table. 1.\nKaggle [21] is one of the most popular platforms for hosting data-science and machine learning\ncompetitions. It allows users to share their approach with other data scientists through a cloud-based\nworkbench that is similar to IPython notebooks in terms of functionality. Despite it’s popularity,\nKaggle has several limitations. Firstly, being a closed-source platform limits Kaggle from supporting\ncomplex AI tasks that require customized metrics other than the usual ones available through the\nplatform. Secondly, it does not allow multiple challenge phases – a common practice in popular\nchallenges like VQA, Visual Dialog, COCO Caption Challenge. Lastly, Kaggle does not allow\nhosting a challenge with a private test splits or workers, thereby limiting the platforms usability in\nscenarios where organizers cannot share the test-set publicly.\nCodaLab [7] is another open-source alternative to Kaggle providing an ecosystem for conducting\ncomputational research in a more efﬁcient, reproducible, and collaborative manner. There are two\naspects of CodaLab – worksheets and competitions. Worksheets enable users to capture complex\nresearch pipelines in a reproducible way, creating “executable papers” in the process. By archiving\nthe code, data and the results of an experiment, the users can precisely capture the research pipeline in\nan immutable way. Additionally, it enables a researcher to present these pipelines in a comprehensible\nway using worksheets or notebooks written in a custom markdown language. CodaLab Competitions\n3\nprovide an evaluation platform on which supports hosting competitions and benchmarking through a\npublic leaderboard. While CodaLab Competitions is very similar to EvalAI and addresses some of\nthe limitations of Kaggle in terms of functionality, it does not support evaluating interactive agents\nin different environments with or without humans in the loop. As the community introduces more\ncomplex tasks in which evaluation requires running an agent inside a simulation or pairing an agent\nwith a human workforce for evaluation, a highly customizable backend like ours connected with\nexisting platforms like Amazon Mechanical Turk become extremely important.\nOpenML [34] is an online platform where researchers can automatically log and share machine\nlearning data sets, code, and experiments. As a system, OpenML allows people to organize their\nexperiments online, and build directly on top of the work of others. By readily integrating the\nonline platform with several machine learning tools, large-scale collaboration in real-time is enabled,\nallowing researchers to share their very latest results while keeping track of their impact and use.\nWhile the focus of OpenML is on experiments and datasets, EvalAI focuses more on the end result\n- models, their predictions and subsequent evaluation. OpenML and EvalAI are complementary to\neach other and will be useful to the user at different stages of the research.\nParlAI [28] is a recently introduced open-source platform for dialog research implemented in Python.\nIt serves as a uniﬁed platform for sharing, training and evaluating models for several dialog tasks.\nAdditionally, ParlAI also supports integration with Amazon Mechanical Turk – to collect dialog data\nand support human-evaluation. Several popular dialog datasets and tasks are supported off-the-shelf\nin ParlAI. Note that unlike EvalAI, ParlAI supports only evaluation for dialog models not for any AI\ntask in general. Also, unlike EvalAI – which supports evaluation across multiple-phases and splits to\ntruly test the generalization and robustness of the proposed model, ParlAI only supports evaluation\non one test split, as is the norm with most of the existing dialog datasets.\nAdditionally, reinforcement learning (RL) algorithms also require strong evaluation and good bench-\nmarks. A variety of benchmarks have been released, such as the Arcade Learning Environment\n(ALE) [3], which exposed a collection of Atari 2600 games as reinforcement learning problems, and\nrecently the RLLab benchmark for continuous control [14]. More recently, OpenAI [5] gym was\nreleased as a toolkit to develop and compare RL algorithms on a variety of environments and tasks –\nranging from walking to playing pong or pinball. The gym library provides a ﬂexible environment\nin which agents can be evaluated using existing machine learning frameworks, such as TensorFlow\nor PyTorch. OpenAI gym has a similar underlying philosophy of encouraging easy accessibility\nand reproducibility by not restricting to any particular framework. Additionally, environments are\nversioned in a way to ensure meaningful and reproducible results as the software is updated.\n4 EvalAI: Key Features\nAs discussed in the previous sections, ensuring algorithms are compared fairly in a standard way is\na difﬁcult and ultimately distracting task for AI researchers. Establishing fair comparison requires\nrectifying minor differences in algorithm inputs, implementing complex evaluation metrics, and often\nensuring the correct usage of non-standard dataset splits. In the following sub-sections, we describe\nthe key features that address the aforementioned problems.\n4.1 Custom Evaluation Protocol\nEvalAI is highly customizable since it allows creation of an arbitrary number of evaluation phases\nand dataset splits, compatibility using any programming language, and organizing results in both\npublic and private leaderboards. All these services are available through an intuitive web-platform\nand comprehensive REST APIs.\n4.2 Human-in-the-loop Evaluation\nWhile standard computer vision tasks such as image classiﬁcation [ 31, 20], semantic or instance\nsegmentation [27, 19], object detection [19, 30] are easy to evaluate using the automatic metrics, it\nis notoriously difﬁcult to evaluate tasks for which automated metrics correlate poorly with human\njudgement – for instance, natural language generation tasks such as image captioning [ 8, 24], visual\ndialog [10, 11] or image generation tasks [ 17]. Developing measures which correlate well with\nhuman judgment remains an open area of research. Automatic evaluation of models for these kind\nof tasks is further complicated by the huge set of possibly ‘correct’ or ‘plausible’ responses and the\nrelatively sparse set of ground truth annotations, even for large-scale datasets.\n4\nGiven these difﬁculties and the interactive nature of tasks, it is clear that the most appropriate way to\nevaluate these kind of tasks is with a human in the loop, i.e. a Visual Turing Test [16]! Unfortunately,\nlarge-scale human-in-the-loop evaluation is still limited by ﬁnancial and infrastructural challenges\nthat must be overcome by each interested research group independently. Consequently, human\nevaluations are rarely performed and experimental settings vary widely, limiting the usefulness of\nthese benchmarking studies in human-AI collaborative settings.\nWe propose to ﬁll this critical need in the community by providing the capability of human-in-the-loop\nevaluation. To this end, we have developed the infrastructure to pair Amazon Mechanical Turk (AMT)\nusers in real-time with artiﬁcial agents – speciﬁcally visual dialog as an example.\n4.2.1 Challenges\nBulding a framework to support human-in-the-loop evaluation comes with it’s own set of challenges:\n• Instructions: Since the workers do not know their roles before starting a study catered towards\nevluating such tasks, they need detailed instructions and a list of Do’s and Dont’s for the task.\nEach challenge might have different instructions and therefore we provide challenge organizers the\nﬂexibility to provide us with the required instructions in their own HTML templates.\n• Worker pool: We need to ensure that we have a pool of good quality workers who have prior\nexperience in doing certain tasks and have a history of high acceptance rate(s). We allow organizers\nto provide us with a list of whitelisted and blocked workers. Additionally, they can also provide a\nqualiﬁcation test which the workers need to pass to participate in the evaluation tasks.\n• Uninterrupted back-and-forth communication: Certain tasks like evaluating dialog agents need\nuninterrupted back-and-forth communication between agents and workers. However, this is not\nalways possible since turkers might disconnect or close a HIT before ﬁnishing it. We do extensive\nbook-keeping to ensure that incompleted HITS are re-evaluated and turkers can reconnect with the\nsame agent if the connection was interrupted only temporarily.\n• Gathering results: We provide a ﬂexible JSON based schema and APIs to fetch the results from\nthe evaluation tasks once they are completed. These results are automatically updated on the\nleaderboard for each submission.\n4.3 Remote Evaluation\nFigure 2: Remote Evaluation Pipeline: Challenge C1 and C2 are hosted on EvalAI but evaluation forC2 happens\non an evauation worker that is running on a private server which is outside EvalAI Virtual Private Cloud (VPC).\nFor two submissions S1 and S2 made to challenges C1 and C2 respectively, submission S1 will be evaluated on\nWL which is running on EvalAI whereas S2 will run on WR which is a remote machine.\nCertain large-scale challenges need special compute capabilities for evaluation. For instance, running\nan agent based on some deep reinforcement learning model in a dynamic environment will require\npowerful clusters with GPUs. If the challenge needs extra computational power, challenge organizers\ncan easily add their own cluster of worker nodes to process participant submissions while we take care\nof hosting the challenge, handling user submissions and the maintaining the leaderboard. Our remote\nevaluation pipeline (shown in Fig. 2) decouples the worker nodes from the web servers through\nvia message-queues. On submission, all related metadata is relayed to an external pool of workers\nthrough dedicated message queues.\n5 System Architecture\nThe architectural back-end of our system (Fig. 3) was designed with keeping in mind scalability\nand portability of such a system from the very inception of the idea. Most of the components rely\n5\nFigure 3: System architecture of EvalAI\nheavily on open-source technologies – Docker, Django, Node.js, and PostgreSQL. We also rely on\ncertain proprietary services but at the same time we ensure that the protocol is consistent with other\nopen-source alternatives to the best possible extent. The conﬁgurations to setup proprietary services\nare also available through our open-source code. In the following sub-sections, we describe in detail\nthe key pieces of our platform.\nOrchestration - We rely heavily on Docker [13] containers to run our infrastructure. Additionally,\nwe also deploy all our containers on Amazon Elastic Container Service (ECS) [15] which auto-scales\nthe cluster to meet the computational demands of the platform leading to high operational efﬁciency.\nWeb Servers -EvalAI uses Django [12] which is a Python based MVC framework that powers our\nbackend. It is responsible for accessing and modifying the database using APIs, and submitting the\nevaluation requests into a queue. It also exposes certain APIs to serve data and fetch results from\nAmazon Mechanical Turk [1] during human evaluation. Through these APIs, agents and workers on\nAMT communicate with each other using JSON blobs. By structuring the communication protocol to\nJSONs, the challenge organizers are enabled to customize it to support any kind of interaction.\nMessage Queue -The message queue is responsible for routing a user’s submission to the appropriate\nworker pool based on the unique routing key associated with each challenge. For our message broker,\nwe chose Amazon Simple Queue Service (SQS) [32]. By using SQS, we do not have to worry about\nconsistency and reliability of the queue. An added bonus of using SQS is that it works seamlessly\nwith other AWS services we use.\nWorker Nodes -For every challenge, there is a different pool of worker nodes dedicated to evaluating\nsubmissions speciﬁc to the challenge. We spawn worker nodes as docker containers running inside\nElastic Container Service (ECS) [ 15] which results in multiple advantages. First, worker nodes\nare isolated such that the dependencies for one challenge don’t clash with dependencies of other\nchallenges. Second, pool of worker nodes speciﬁc to the challenge can independently scale based\non the demands of the challenge. We also worked closely with challenge organizers to optimize\ntheir code to leverage the full computational capacity of the worker. For instance, we warm-up the\nworker nodes at start-upby importing the challenge code and pre-loading the dataset in memory.\nWe also split the dataset into small chunks that are simultaneously evaluated on multiple cores. These\nsimple tricks result in faster evaluation and reduces the evaluation time by an order of magnitude in\nsome cases. Refer to Section 7 for details on speed-up for the VQA Challenge.\n6 Lifecycle of a Challenge\nWe now describe the life-cycle of a challenge starting from creating a challenge, submitting entries to\nthe challenge and ﬁnally evaluating the submissions. This process will also elaborate how different\ncomponents of the platform communicate with each other.\n6\n6.1 Challenge Creation\nThere are two ways to create a challenge on our platform. For challenges like image classiﬁcation,\ndetection which require simple evaluation metrics (such as precision, recall, accuracy), a user can\nfollow a sequence of prompts on a user-interface to create a challenge. For more complex scenarios\nwhich require custom evaluation metrics, multiple dataset splits and phases, users are recommended\nto create a competition bundle which speciﬁes the challenge conﬁguration, evaluation code, and\ninformation about the said data-splits. The associated conﬁguration ﬁle provides enough ﬂexibility\nto conﬁgure different phases of the competition, deﬁne number of splits for the dataset and specify\ncustom evaluation scripts arbitrarily.\n6.2 Submission\nEvalAI supports both submitting the model predictions and the model itself for evaluation. Traditional\nchallenges require user to submit their model predictions on a static test set provided by the challenge\norganizers. On submission, these predictions are handed over to challenge speciﬁc workers that\ncompare the predictions against corresponding ground-truth using the custom evaluation script\nprovided by the challenge organizers. As we move towards developing intelligent agents for tasks\nsituated in active environments instead of static datasets, where agents take actions to change the state\nof the world around them, it is imperative that we build new tools to accurately benchmark agents in\nenvironments. In this regard, we have developed an evaluation framework (shown in Fig. 4)where\nparticipants upload Docker images with their pretrained models on Elastic Container Registry (ECR)\nand Amazon S3 respectively, which is then attached and run against test environments and evaluation\nmetrics provided by the challenge organizer. At the time of evaluation, the instantiated worker fetches\nthe image from ECR, assets and conﬁguration for test-environment, model snapshot from Amazon\nS3 and spins up a new container to perform evaluation. Once the evaluation is complete, the results\nare sent over to the leaderboard using the message queue described in Section 5.\nFigure 4: EvalAI lets participants submit code for their agent which are eventually evaluated in dynamic\nenvironments on the evaluation server. The pipeline involves participants uploading the model snapshot and the\ncode as docker image. Model snapshots are stored in Amazon S3 while the docker images are stored in Amazon\nElastic Container Registry (ECR). During evaluation, the worker fetches the image, test environment and the\nmodel snapshot and spins up a new container to perform evaluation on this model. The results are then sent over\nto the leaderboard through a message queue.\n6.3 Evaluation\nWe allow organizers to provide an implementation of their metric and is subsequently used to evaluate\nall submissions ensuring consistency in evaluation protocols. The by-product of containerizing the\nevaluation for different challenges in docker containers is that it allows us to package fairly complex\npipelines, with all it’s dependencies in an isolated environment. For human-in-the-loop evaluation, the\nevaluation code ﬁrst loads up the worker and launches a new HIT on Amazon Mechanical Turk. Once\nthe worker accepts the HIT, the worker is paired with the agent running inside a docker image. Based\non the instruction given, the worker will interact with the agent and evaluate it according to certain\ncriteria. This interaction data and the ﬁnal rating given by the worker is stored by EvalAI which\nis eventually reﬂected on the leaderboard. EvalAI takes care of managing a persistent connection\nbetween the agent and the worker, error handling, retrying , storing the interaction data corresponding\nto this HIT and automatically approving or rejecting HIT. We discuss one human-in-the-loop task in\nthe second case study.\n7\nFigure 5: Human-in-the-loop interface for evaluating visual dialog agents.\n7 Case Studies\nIn this section, we go over two speciﬁc past instantiations of challenges organized on our platform to\nshowcase its various capabilities.\n7.1 Visual Question Answering.\nVisual Question Answering (VQA) is a multi-modal task where given an image and a free-form\nopen-ended natural language question about the image, the AI agent’s task is to answer the question\naccurately. The Visual Question Answering Challenge (VQA) 2016 was organized on the VQAv1 [2]\ndataset and was hosted on another platform, where mean evaluation time per dataset instance was\n∼10 minutes. In the following years, VQA Challenge 2017 and 2018 (on the VQAv2 [18] dataset)\nwere hosted on EvalAI. Even with twice the dataset size (VQAv2 vs VQAv1), our parallelized\nimplementation offered a signiﬁcant reduction in per-instance evaluation time ( ∼130 seconds) –\nan approximately 12x speedup. This was made possible by leveraging map-reduce techniques to\ndistribute smaller chunks of the test-split on multiple cores and eventually combining the individual\nresults to compute overall performance. Execution time is further reduced by making sure that the\nevaluation program is not loaded in memory (preloaded earlier) everytime a new submission arrives.\nThe above instance of the challenge also utilized several other features of our platform – namely,\nsupporting multiple challenge phases for continued evaluation beyond the challenge; multiple data-\nsplits for debugging submissions and reporting public benchmarks and privacy levels for leaderboards\nassociated with different data-split evaluations.\n7.2 Visual Dialog.\nAs mentioned before, it is notoriously difﬁcult to evaluate free-form multimodal tasks such as image\ncaptioning [8, 24], visual dialog [10, 11, 26] etc using automatic metrics and as such they inherently\nrequire human-in-the-loop evaluation. Recall that Visual Dialog, where given an image, an associated\ndialog history and a follow-up question about the image, an agent is required to answer the question\nwhile inferring relevant context from history – evaluation is further complicated by the huge set of\npossibly ’correct’ answers and the relatively sparse sampling of this space, even in large-scale datasets\n– making human-in-the-loop evaluation imperative. As part of a demonstration at CVPR 2018, EvalAI\nhosted a visual dialog challenge where each submission was connected with a human subject (Fig. 5)\non Amazon Mechanical Turk tasked with rating a response generated by participating model along\n8\nseveral axes – correctness, ﬂuency, consistency, etc. After 10 such rounds of human-agent interaction,\nthe human’s rating of the agent was reﬂected as a score on the leaderboard immediately.\n7.3 Embodied Question Answering.\nFinally, as we move towards developing intelligent agents for tasks situated inactive environments\ninstead of static datasets, where agents take actions to change the state of the world around them, it is\nimperative that we build new tools to accurately benchmark agents in environments. One example of\nsuch a task is Embodied Question Answering [9] – an agent is spawned at a random location in a\nsimulated environment (say in a kitchen) and is asked a natural language question (“What color is\nthe car?”). The agent perceives its environment through ﬁrst-person vision and can perform a few\nactions: {move-forward, turn-left, turn-right, stop}. The agent’s objective is to explore the\nenvironment and gather visual information necessary to answer the question (“orange”). Evaluating\nagents for EmbodiedQA presents a key challenge – instead of a hidden test dataset, there are hidden\ntest environments, so participants have to submit pretrained models and inference code, which has to\nbe reliably executed in these environments to benchmark them. In ongoing work, we have developed\nan evaluation framework for EmbodiedQA – wherein participants upload Docker images with their\npretrained models on Amazon S3, which is then attached and run against test environments and\nevaluation metrics provided by the challenge organizer. We will be using this to host a CVPR 2019\nchallenge on EmbodiedQA, and aim to extend this to support a wide range of reinforcement learning\ntask evaluations in future.\n7.4 fastMRI.\nfastMRI [35] is a collaborative research project between Facebook AI Research (FAIR) and NYU\nLangone Health to investigate the use of AI to make MRI scans upto 10 times faster. By focusing\non the reconstruction capabilities of several AI algorithms, the goal is to enable faster scanning\nand subsequently making MRIs accessible to more people. This collaborative effort to accelerate\nMagnetic Resonance Imaging (MRI) by taking fewer measurements was recently structured as a\nchallenge organized around a large-scale open dataset for both raw MR measurements and clinical\nMR images. EvalAI currently hosts the ﬁrst iteration of the fastMRI challenge. Ensuring proper\nbenchmarking on such a medical dataset comes with it’s own set of challenges – primarily centered\naround privacy and storage. Firstly, since proposed algorithms for fastMRI have a direct real world\nimpact, any leakage of test-annotations compromises generalization and can subsequently have drastic\nconsequences. Secondly, in addition to privacy, it is important to note that the dataset itself consumes a\nlot of storage space as it consists of clinical MR images. As such, supporting decentralized evaluation\nin addition to a centralized leaderboard to benchmark solutions seems desirable from the organizer’s\nperspective. EvalAI fulﬁlls both of these requirements – as a platform, we do not have access to the\ntest-annotations for fastMRI but still support efﬁcient evaluation on a remote server (belonging to\nthe organizer). The evaluation metrics are sent from the remote servers to EvalAI via an API format\nprovided by EvalAI.The metrics are then displayed on a centralized leaderboard hosted on EvalAI.\n8 Conclusion\nWhile traditional platforms were adequate for evaluation of tasks using automatic metrics, there\nis a critical need to support human-in-the-loop evaluation for more free-form multimodal tasks\nlike (visual) dialog, question-answering, etc. To this end, we have developed a new evaluation\nplatform that supports the same on a large-scale. Effectively, EvalAI supports pairing an AI agent\nwith thousands of workers so as to rate or evaluate the former over multiple rounds of interaction.\nBy providing a scalable platform that supports such evaluations will eventually encourage the\ncommunity to benchmark performance on tasks extensively, leading to better understanding of a\nmodel’s performance both in isolation and in human-AI teams.\nAcknowledgements -We thank Abhishek Das, Devi Parikh, and Viraj Prabhu for helpful discussions.\nWe would also like to thank Google Summer of Code, Google Code-in, and the 80+ developers and\ndesigners who have contributed code to the EvalAI project. This work was supported in part by NSF,\nAFRL, DARPA, Siemens, Google, Amazon, ONR YIPs and ONR Grants N00014-16-1-2713,2793.\nThe views and conclusions contained herein are those of the authors and should not be interpreted as\nnecessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of the U.S.\nGovernment, or any sponsor.\n9\nReferences\n[1] Amazon Mechanical Turk (AMT). Website - https://www.mturk.com/. 2, 6\n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence\nZitnick, and Devi Parikh. VQA: Visual Question Answering. In International Conference on\nComputer Vision (ICCV), 2015. 1, 3, 8\n[3] Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. CoRR, abs/1207.4708, 2012. 4\n[4] Antoine Bordes and Jason Weston. Learning end-to-end goal-oriented dialog. CoRR,\nabs/1605.07683, 2016. 2\n[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. 4\n[6] Prithvijit Chattopadhyay, Deshraj Yadav, Viraj Prabhu, Arjun Chandrasekaran, Abhishek Das,\nStefan Lee, Dhruv Batra, and Devi Parikh. Evaluating visual conversational agents via coopera-\ntive human-ai games. In Proceedings of the Fifth AAAI Conference on Human Computation\nand Crowdsourcing (HCOMP), 2017. 2\n[7] CodaLab. Website - https://competitions.codalab.org/. 3\n[8] Bo Dai, Dahua Lin, Raquel Urtasun, and Sanja Fidler. Towards diverse and natural image\ndescriptions via a conditional gan. 2017 IEEE International Conference on Computer Vision\n(ICCV), pages 2989–2998, 2017. 4, 8\n[9] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra.\nEmbodied question answering. 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1–10, 2018. 2, 3, 9\n[10] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M. F. Moura,\nDevi Parikh, and Dhruv Batra. Visual dialog. 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 1080–1089, 2017. 3, 4, 8\n[11] Abhishek Das, Satwik Kottur, José M. F. Moura, Stefan Lee, and Dhruv Batra. Learning\ncooperative visual dialog agents with deep reinforcement learning. 2017 IEEE International\nConference on Computer Vision (ICCV), pages 2970–2979, 2017. 4, 8\n[12] Django: The web framework for perfectionists with deadlines. Website - https://www.\ndjangoproject.com/. 6\n[13] Docker. Website - https://www.docker.com/. 6\n[14] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep\nreinforcement learning for continuous control. CoRR, abs/1604.06778, 2016. 4\n[15] Amazon Elastic Container Service (ECS). Website - https://aws.amazon.com/ecs/. 6\n[16] Donald Geman, Stuart Geman, Neil Hallonquist, and Laurent Younes. Visual turing test for\ncomputer vision systems. Proceedings of the National Academy of Sciences, page 201422953,\n2015. 5\n[17] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems (NIPS), 2014. 4\n[18] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the\nv in vqa matter: Elevating the role of image understanding in visual question answering. In\nProceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 8\n[19] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Computer\nVision (ICCV), 2017 IEEE International Conference on, pages 2980–2988. IEEE, 2017. 4\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016. 4\n[21] Kaggle. Website - https://kaggle.com/. 3\n[22] Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi.\nAi2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017.\n2, 3\n10\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. ImageNet Classiﬁcation with Deep Convo-\nlutional Neural Networks. In Advances in Neural Information Processing Systems (NIPS), 2012.\n1\n[24] Dianqi Li, Xiaodong He, Qiuyuan Huang, Ming-Ting Sun, and Lei Zhang. Generating\ndiverse and accurate visual captions by comparative adversarial learning. arXiv preprint\narXiv:1804.00861, 2018. 4, 8\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision, pages 740–755. Springer, 2014. 3\n[26] Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle\nPineau. How not to evaluate your dialogue system: An empirical study of unsupervised\nevaluation metrics for dialogue response generation. arXiv preprint arXiv:1603.08023, 2016. 8\n[27] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for se-\nmantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 3431–3440, 2015. 4\n[28] Alexander H Miller, Will Feng, Adam Fisch, Jiasen Lu, Dhruv Batra, Antoine Bordes, Devi\nParikh, and Jason Weston. Parlai: A dialog research software platform. arXiv preprint\narXiv:1705.06476, 2017. 4\n[29] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+\nquestions for machine comprehension of text. In EMNLP, 2016. 1\n[30] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Uniﬁed,\nreal-time object detection. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 779–788, 2016. 4\n[31] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\nimage recognition. In Proceedings of the International Conference on Learning Representations\n(ICLR), 2015. 4\n[32] Amazon Simple Queue Service. Website - https://aws.amazon.com/sqs/. 6\n[33] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and\nSanja Fidler. Movieqa: Understanding stories in movies through question-answering. 2016\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4631–4640,\n2016. 2\n[34] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. Openml: Networked\nscience in machine learning. SIGKDD Explorations, 15(2):49–60, 2013. 4\n[35] Jure Zbontar, Florian Knoll, Anuroop Sriram, Matthew J Muckley, Mary Bruno, Aaron Defazio,\nMarc Parente, Krzysztof J Geras, Joe Katsnelson, Hersh Chandarana, et al. fastmri: An open\ndataset and benchmarks for accelerated mri. arXiv preprint arXiv:1811.08839, 2018. 9\n[36] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali\nFarhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In\nProceedings of IEEE International Conference on Robotics and Automation (ICRA), 2017. 2\n11",
  "topic": "Benchmarking",
  "concepts": [
    {
      "name": "Benchmarking",
      "score": 0.8287535905838013
    },
    {
      "name": "Computer science",
      "score": 0.7135661244392395
    },
    {
      "name": "Artificial intelligence",
      "score": 0.638133704662323
    },
    {
      "name": "Process (computing)",
      "score": 0.5845287442207336
    },
    {
      "name": "Scalability",
      "score": 0.5575752258300781
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5563894510269165
    },
    {
      "name": "Globe",
      "score": 0.515291690826416
    },
    {
      "name": "Machine learning",
      "score": 0.5025873184204102
    },
    {
      "name": "Data science",
      "score": 0.4931585192680359
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4193200469017029
    },
    {
      "name": "Database",
      "score": 0.09958499670028687
    },
    {
      "name": "Management",
      "score": 0.06651908159255981
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Ophthalmology",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": []
}