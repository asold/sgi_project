{
  "title": "R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling",
  "url": "https://openalex.org/W3174906586",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2160265677",
      "name": "Hu Xiang",
      "affiliations": [
        "University of Potsdam",
        "Hasso Plattner Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4222052269",
      "name": "Mi, Haitao",
      "affiliations": [
        "Hasso Plattner Institute",
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A4222052276",
      "name": "Wen, Zujie",
      "affiliations": [
        "University of Potsdam",
        "Hasso Plattner Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1999567110",
      "name": "Wang Yafang",
      "affiliations": [
        "University of Potsdam",
        "Hasso Plattner Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2112754441",
      "name": "Su Yi",
      "affiliations": [
        "University of Potsdam",
        "Hasso Plattner Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2128760848",
      "name": "Zheng Jing",
      "affiliations": [
        "Hasso Plattner Institute",
        "University of Potsdam"
      ]
    },
    {
      "id": "https://openalex.org/A4202079916",
      "name": "de Melo, Gerard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2124479173",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2072128103",
    "https://openalex.org/W2096204319",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2889260178",
    "https://openalex.org/W2932637973",
    "https://openalex.org/W1970961429",
    "https://openalex.org/W2949399644",
    "https://openalex.org/W4289373464",
    "https://openalex.org/W3098284381",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W1502293651",
    "https://openalex.org/W1879966306",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W71795751",
    "https://openalex.org/W2963451457",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2932376173",
    "https://openalex.org/W2911666613",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2250385622",
    "https://openalex.org/W2963411763",
    "https://openalex.org/W4231109964",
    "https://openalex.org/W2619818172",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2152907450",
    "https://openalex.org/W1508977358",
    "https://openalex.org/W2160382364"
  ],
  "abstract": "Human language understanding operates at multiple levels of granularity\\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\\nthat can be hierarchically combined. However, existing deep models with stacked\\nlayers do not explicitly model any sort of hierarchical process. This paper\\nproposes a recursive Transformer model based on differentiable CKY style binary\\ntrees to emulate the composition process. We extend the bidirectional language\\nmodel pre-training objective to this architecture, attempting to predict each\\nword given its left and right abstraction nodes. To scale up our approach, we\\nalso introduce an efficient pruned tree induction algorithm to enable encoding\\nin just a linear number of composition steps. Experimental results on language\\nmodeling and unsupervised parsing show the effectiveness of our approach.\\n",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 4897–4908\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4897\nR2D2: Recursive Transformer based on Differentiable Tree\nfor Interpretable Hierarchical Language Modeling\nXiang Hu†∗ Haitao Mi†∗ Zujie Wen† Yafang Wang†\nYi Su† Jing Zheng† Gerard de Melo‡\nAnt Financial Services Group†\n{aaron.hx, haitao.mi, zujie.wzj, yafang.wyf, yi.su, jing.zheng}\n@alibaba-inc.com†\nHasso Plattner Institute / University of Potsdam‡\ngdm@demelo.org‡\nAbstract\nHuman language understanding operates at\nmultiple levels of granularity (e.g., words,\nphrases, and sentences) with increasing levels\nof abstraction that can be hierarchically com-\nbined. However, existing deep models with\nstacked layers do not explicitly model any sort\nof hierarchical process. This paper proposes\na recursive Transformer model based on dif-\nferentiable CKY style binary trees to emulate\nthe composition process. We extend the bidi-\nrectional language model pre-training objec-\ntive to this architecture, attempting to predict\neach word given its left and right abstraction\nnodes. To scale up our approach, we also in-\ntroduce an efﬁcient pruned tree induction algo-\nrithm to enable encoding in just a linear num-\nber of composition steps. Experimental results\non language modeling and unsupervised pars-\ning show the effectiveness of our approach.1\n1 Introduction\nThe idea of devising a structural model of lan-\nguage capable of learning both representations and\nmeaningful syntactic structure without any human-\nannotated trees has been a long-standing but chal-\nlenging goal. Across a diverse range of linguistic\ntheories, human language is assumed to possess a\nrecursive hierarchical structure (Chomsky, 1956,\n2014; de Marneffe et al., 2006) such that lower-\nlevel meaning is combined to infer higher-level\nsemantics. Humans possess notions of characters,\nwords, phrases, and sentences, which children nat-\nurally learn to segment and combine.\nPretrained language models such as BERT (De-\nvlin et al., 2019) have achieved substantial gains\n∗Equal contribution.\n1The code is available at: https://github.com/\nalipay/StructuredLM_RTDT\nacross a range of tasks. However, they simply ap-\nply layer-stacking with a ﬁxed depth to increase\nthe modeling power (Bengio, 2009; Salakhutdinov,\n2014). Moreover, as the core Transformer compo-\nnent (Vaswani et al., 2017) does not capture posi-\ntional information, one also needs to incorporate\nadditional positional embeddings. Thus, pretrained\nlanguage models do not explicitly reﬂect the hier-\narchical structure of linguistic understanding.\nInspired by Le and Zuidema (2015), Maillard\net al. (2017) proposed a fully differentiable CKY\nparser to model the hierarchical process explicitly.\nTo make their parser differentiable, they primar-\nily introduce an energy function to combine all\npossible derivations when constructing each cell\nrepresentation. However, their model is based on\nTree-LSTMs (Tai et al., 2015; Zhu et al., 2015) and\nrequires O(n3) time complexity. Hence, it is hard\nto scale up to large training data.\nIn this paper, we revisit these ideas, and propose\na model applying recursive Transformers along dif-\nferentiable trees (R2D2). To obtain differentiabil-\nity, we adopt Gumbel-Softmax estimation (Jang\net al., 2017) as an elegant solution. Our encoder\nparser operates in a bottom-up fashion akin to CKY\nparsing, yet runs in linear time with regard to the\nnumber of composition steps, thanks to a novel\npruned tree induction algorithm. As a training ob-\njective, the model seeks to recover each word in\na sentence given its left and right syntax nodes.\nThus, our model does not require any positional\nembedding and does not need to mask any words\nduring training. Figure 1 presents an example bi-\nnary tree induced by our method: Without any\nsyntactic supervision, it acquires a model of hier-\narchical construction from the word-piece level to\nwords, phrases, and ﬁnally the sentence level.\n4898\nwhat ’s more , such short - term cat #ac #ly #sms are sur #vi #vable and are no cause for panic selling .\nFigure 1: An example output tree emerging from our proposed method.\nWe make the following contributions:\n•Our novel CKY-based recursive Transformer on\ndifferentiable trees model is able to learn both\nrepresentations and tree structure (Section 2.1).\n•We propose an efﬁcient optimization algorithm\nto scale up our approach to a linear number of\ncomposition steps (Section 2.2).\n•We design an effective pre-training objective,\nwhich predicts each word given its left and right\nsyntactic nodes (Section 2.3).\nFor simplicity and efﬁciency reasons, in this pa-\nper we conduct experiments only on the tasks of\nlanguage modeling and unsupervised tree induc-\ntion. The experimental results on language model-\ning show that our model signiﬁcantly outperforms\nbaseline models with same parameter size even in\nfewer training epochs. At unsupervised parsing,\nour model as well obtains competitive results.\n2 Methodology\n2.1 Model Architecture\nFigure 2: Chart data structure. There are two alter-\nnative ways of generating T1,3: combining either\n(T1,2, T3,3) or (T1,1, T2,3).\nDifferentiable Tree. We follow Maillard et al.\n(2017) in deﬁning a differentiable binary parser\nusing a CKY-style (Cocke, 1969; Kasami, 1966;\nYounger, 1967) encoder. Informally, given a sen-\ntence S = {s1,s2,...,s n}with nwords or word-\npieces, Figure 2 shows the chart data structure T,\nwhere each cell Ti,j is a tuple ⟨ei,j,pi,j,˜pi,j⟩, ei,j is\na vector representation, pi,j is the probability of a\nsingle composition step, and ˜pi,j is the probability\nof the subtree at span [i,j] over sub-string si:j. At\nthe lowest level, we have terminal nodes Ti,i with\nei,i initialized as embeddings of inputs si, while\npi,i and ˜pi,i are set to one. When j > i, the rep-\nresentation ei,j is a weighted sum of intermediate\ncombinations ck\ni,j, deﬁned as:\nck\ni,j, pk\ni,j = f(ei,k,ek+1,j) (1)\n˜pk\ni,j = pk\ni,j ˜pi,k ˜pk+1,j (2)\nαi,j = GUMBEL (log(˜pi,j)) (3)\nei,j = [ci\ni,j,ci+1\ni,j ,...,c j−1\ni,j ]αi,j (4)\n[pi,j,˜pi,j] =α⊺\ni,j[pi,j,˜pi,j] (5)\nHere, k is a split point from ito j −1, f(·) is a\ncomposition function that we shall further deﬁne\nlater on, pk\ni,j and ˜pk\ni,j denote the single step combi-\nnation probability and the subtree probability, re-\nspectively, at split pointk, pi,j and ˜pi,j are the con-\ncatenation of all pk\ni,j or ˜pk\ni,j values, and GUMBEL is\nthe Straight-Through Gumbel-Softmax operation\nof Jang et al. (2017) with temperature set to one.\nThe [,] notation denotes stacking of tensors.\nFigure 3: Recursive Transformer-based encoder.\nRecursive Transformer. Figure 3 provides a\nschematic overview of the composition function\nf(·), comprising N Transformer layers. Taking\nck\ni,j and pk\ni,j as an example, the input is a concate-\nnation of two special tokens [SUM] and [CLS],\nthe left cell ei,k, and the right cell ek+1,j. We also\n4899\nadd role embeddings ([LEFT] and [RIGHT]) to\nthe left and right inputs, respectively. Thus, the\ninput consists of four vectors in Rd. We denote as\nh[SUM],h[CLS],hi,k,hk+1,j ∈Rd the hidden state\nof the output of N Transformer layers. This is\nfollowed by a linear layer over h[SUM] to obtain\npk\ni,j = σ(Wph[SUM] + bp), (6)\nwhere Wp ∈R1×d, bp ∈R, and σrefers to sigmoid\nactivation. Then, ck\ni,j is computed as\nwk\ni,j = softmax(Wwh[CLS] + bw)\nck\ni,j = [hi,k,hk+1,j]wk\ni,j,\n(7)\nwhere Ww ∈R2×d with wk\ni,j ∈R2 capturing the\nrespective weights of the left and right hidden states\nhi,k and hk+1,j, and the ﬁnal ck\ni,j is a weighted sum\nof hi,k and hk+1,j.\nTree Recovery. As the Straight-Through\nGumbel-Softmax picks the optimal splitting point\nk at each cell in practice, it is straightforward to\nrecover the complete derivation tree, Tree(T1,n),\nfrom the root node T1,n in a top-down manner\nrecursively.\n2.2 Complexity Optimization\nAlgorithm 1 Pruned Tree Induction Algorithm\nRequire: T = 2-d array holding cell references\nRequire: m= pruning threshold\n1: function PRUNING (T, m)\n2: u←FIND (T, m) ⊿Find optimal merge point\n3: n←T .len\n4: T′←[n−1][n−1] ⊿Create a new 2-d array\n5: for i∈1 to n−1 do\n6: for j∈ito n−1 do\n7: i′←i≥u+ 1 ? i+ 1 : i\n8: j′←j ≥u ? j+ 1 : j\n9: T′\ni,j ←Ti′,j′ ⊿Skip dark gray cells in Fig. 4\n10: return T′\n11: function TREE INDUCTION (T, m)\n12: T′←T\n13: for t∈1 to T.len−1 do\n14: if t≥mthen\n15: T′←PRUNING (T′,m)\n16: l←min(t+ 1, m) ⊿Clamp the span length\n17: for i∈1 to T′.len−l+ 1do\n18: j ←i+ l−1\n19: if T′\ni,j is empty then\n20: Compute cell T′\ni,j with Equation 1\n21: return T\nAs the core computation comes from the compo-\nsition function f(·), our pruned tree induction al-\ngorithm aims to reduce the number of composition\ncalls from O(n3) in the original CKY algorithm to\nlinear.\nOur intuition is based on the conjecture that lo-\ncally optimal compositions are likely to be retained\nand participate in higher-level feature combination.\nSpeciﬁcally, taking T2 in Figure 4 (c) as an exam-\nple, we only pick locally optimal nodes from the\nsecond row of T2. If T2\n4,5 is locally optimal and\nnon-splittable, then all the cells highlighted in dark\ngray in (d) may be pruned, as they break span[4,5].\nFor any later encoding, including higher-level ones,\nwe can merge the nodes and treat T2\n4,5 as a new\nnon-splittable terminal node (see (e) to (g)).\nAlgorithm 2 Find the best merge point\nRequire: T = 2-d array holding cell references\nRequire: m= pruning threshold\n1: function FIND (T, m)\n2: n←T .len\n3: L← [n−1] ⊿Create an array\n4: for i∈1 to n−1 do\n5: L[i] ←Ti,i+1 ⊿Collect cells on the 2nd row\n6: τ ←∅\n7: for i∈1 to n−m+ 1do ⊿Iterate to m-th row\n8: j = i+ m−1\n9: τ ←τ ∪{c|c∈Tree(Ti,j) ∧c∈L}\n10: l←new List()\n11: for cell x∈τ do\n12: i←L.index(x)\n13: pl ←1 −L[i−1].p\n14: pr ←1 −L[i+ 1].p\n15: ⊿If index out of boundary then set to 0\n16: l.append(x.p·pl ·pr)\n17: return argmaxi l[i]\nFigure 4 walks through the steps of processing\na sentence of length 6, where si:j denotes a sub-\nstring from si to sj. Algorithm 1 constructs our\nchart table Tsequentially row-by-row. Let tbe the\ntime step and mbe the pruning threshold. First,\nwe invoke TREE INDUCTION (T,m), and compute\na row of cells at each time step when t < mas\nin regular CKY parsing, leading to result (b) in\nFigure 4. When t≥m, we call PRUNING (T,m)\nin Line 15. As mentioned, the PRUNING function\naims to ﬁnd the locally optimal combination node\nin T, prunes some cells, and returns a new table\nomitting the pruned cells. Algorithm 2 shows how\nwe FIND the locally optimal combination node.\nAgain, the candidate set for the locally optimal\nnode is the second row of T, and we also take\nadvantage of the subtrees derived from all nodes\nin the m-th row to limit the candidate set. Lines\n6 to 9 in Algorithm 2 generate the candidate set.\nEach candidate must be in the second row ofTand\nalso must be used in a subtree of any node in the\nm-th row. Given the candidate set, we ﬁnd the least\nambiguous one as the optimal selection (Lines 11 to\n4900\nFigure 4: Example of encoding. (a) Initialized chart table. (b) Row-by-row encoding up to pruning\nthreshold m. (c) For each cell in the m-th row, recover its subtree and collect candidate nodes, each of\nwhich must appear in the subtree and also must be in the 2nd row, e.g., the tree of T2\n3,5 is within the dark\nline, and the candidate node is T2\n4,5. (d) Find locally optimal node, which is T2\n4,5 here, and treat span s4:5\nas non-splittable. Thus, the dark gray cells become prunable. (e) Construct a new chart table T3 treating\ncell T2\n4,5 as a new terminal node and eliminating the prunable cells. (f) Compute empty cells in m-th row.\n(g) Keep pruning and growing the tree until no further empty cells remain. (h) Final discrete chart table.\n17), i.e., the node with maximum own probability\nwhile adjacent bi-gram node probabilities (Lines\n13 and 14 ) are as low as possible. After selecting\nthe best merge point u, cells in {Tt\ni,j |j = u}∪\n{Tt\ni,j | i = u + 1}are pruned (highlighted in\ndark gray in (d)), and we generate a new table\nTt+1 by removing pruned nodes (Lines 4 to 9 in\nAlgorithm 1). Then we obtain (e), and compute the\nempty cells on them-th row ofT3 to obtain (f). We\ncontinue with the loop in Line 13, triggerPRUNING\nagain, and obtain a new table Tt+1, and then ﬁll\nempty cells on the m-th row Tt+1. Continuing\nwith the process until all cells are computed, as\nshown in (g), we ﬁnally obtain a discrete chart\ntable as given in (h).\nIn terms of the time complexity, when t ≥m,\nthere are at most m cells to update, so the com-\nplexity of each step is less than O(m2). When\nt≤m, the complexity is O(t3) ≤O(m2t). Thus,\nthe overall times to call the composition function\nis O(m2n), which is linear considering mis a con-\nstant.\n2.3 Pretraining\nDifferent from the masked language model training\nof BERT, we directly minimize the sum of all neg-\native log probabilities of all words or word-pieces\nFigure 5: The objective for our pretrained model.\nsi given their left and right contexts.\nmin\nθ\nn∑\ni=1\n−log pθ(si |s1:i−1,si+1:n) (8)\nAs shown in Figure 5, after invoking our recur-\nsive encoder on a sentenceS, we directly use e1,i−1\nand ei+1,n as the left and right contexts, respec-\ntively, for each word si. To distinguish from the\nencoding task, the input consists of a concatenation\nof a special token [MASK], e1,i−1, and ei+1,n. We\napply the same composition function f(·) as in Fig-\nure 3, and feed h[MASK] through an output softmax\nto predict the distribution of si over the complete\n4901\nvocabulary. Finally, we compute the cross-entropy\nover the prediction and ground truth distributions.\nIn cases where e1,i−1 or ei+1,n is missing due\nto the pruning algorithm in Section 2.2, we simply\nuse the left or right longest adjacent non-empty\ncell. For example, Tx,i−1 means the longest non-\nempty cell assuming we cannot ﬁnd any non-empty\nTx′,i−1 for all x′ < x. Analogously, Ti+1,y is de-\nﬁned as the longest non-empty right cell. Note\nthat although the ﬁnal table is sparse, the sentence\nrepresentation e1,n is always established.\n3 Experiments\nAs our approach (R2D2) is able to learn both repre-\nsentations and intermediate structure, we evaluate\nits representation learning ability on bidirectional\nlanguage modeling and evaluate the intermediate\nstructures on unsupervised parsing.\n3.1 Bidirectional Language Modeling\n3.1.1 Setup\nBaselines and Evaluation. As the objective of\nour model is to predict each word with its left and\nright context, we use the pseudo-perplexity (PPPL)\nmetric of Salazar et al. (2020) to evaluate bidirec-\ntional language modeling.\nL(S) = 1\nn\nn∑\ni=1\nlogP(si |s1:i−1,si+1:n,θ)\nPPPL(S) = exp\n\n−1\nN\nN∑\nj=1\nL(Sj)\n\n\nPPPL is a bidirectional version of perplexity, estab-\nlishing a macroscopic assessment of the model’s\nability to deal with diverse linguistic phenomena.\nWe compared our approach with SOTA autoen-\ncoding and autoregressive language models ca-\npable of capturing bidirectional contexts, includ-\ning BERT, XLNet (Yang et al., 2019), and AL-\nBERT (Lan et al., 2020). For a fair apples to apples\ncomparison, all models use the same vocabulary\nand are trained from scratch on a language model-\ning corpus. The models are all based on the open\nsource Transformers library2. To compute PPPL\nfor models based on sequential Transformers, for\neach word si, we only mask si while others remain\nvisible to predict si. When we evaluate our R2D2\nmodel, for each word si, we treat the left s1:i−1\nand right si+1:n as two complete sentences sepa-\nrately, then encode them separately, and pick the\n2https://github.com/huggingface/transformers\n#param #layer #epoch cplx PPPL\nBERT 46M 3 10 O(n2) 441.42\nXLNet 46M 3 10 O(n) 301.87\nALBERT 46M 12 10 O(n2) 219.20\nXLNet 116M 12 10 O(n) 127.74\nBERT 109M 12 10 O(n2) 103.54\nT-LSTM (m=4) 46M 1 10 O(n) 820.57\nOurs (m=4) 45M 3 10 O(n) 83.10\nOurs (m=8) 45M 3 10 O(n) 57.40\nBERT 46M 3 60 O(n2) 112.17\nXLNet 46M 3 60 O(n) 105.64\nALBERT 46M 12 60 O(n2) 71.52\nXLNet 116M 12 60 O(n) 59.74\nBERT 109M 12 60 O(n2) 44.70\nOurs (m=4) 45M 3 60 O(n) 55.70\nOurs (m=8) 45M 3 60 O(n) 54.60\nTable 1: Comparison with state-of-the-art models\ntrained from scratch on WikiText-2 with different\nsettings (number of Transformer layers and training\nepochs). mis the pruning threshold.\nroot nodes as the ﬁnal representations of left and\nright contexts. In the end, we predict word si by\nrunning our Transformers as in Figure 5.\nData. The English language WikiText-2 cor-\npus (Merity et al., 2017) serves as training data.\nThe dataset is split at the sentence level, and sen-\ntences longer than 128 after tokenization are dis-\ncarded (about 0.03% of the original data). The total\nnumber of sentences is 68,634, and the average sen-\ntence length is 33.4.\nHyperparameters. The tree encoder of our\nmodel uses 3-layer Transformers with 768-\ndimensional embeddings, 3,072-dimensional hid-\nden layer representations, and 12 attention heads.\nOther models based on the Transformer share the\nsame setting but vary on the number of layers.\nTraining is conducted using Adam optimization\nwith weight decay with a learning rate of 5 ×10−5.\nThe batch size is set to 8 for m=8 and 32 for m=4,\nthough we also limit the maximum total length for\neach batch, such that excess sentences are moved\nto the next batch. The limit is set to 128 for m=8\nand 512 for m=4. It takes about 43 hours for 10\nepochs of training with m= 8and about 9 hours\nwith m=4, on 8 v100 GPUs.\n3.1.2 Results and Discussion\nTable 1 presents the results of all models with dif-\nferent parameters. Our model outperforms other\nmodels with the same parameter size and number\n4902\nemb.×hid.×lay. training time\nOurs (m=4) 768×3072×3 7h\n-w/o pruning 12×12×1 1125h\n-w/o pruning∗ 768×3072×3 5 ×107h\nTable 2: Training time for one epoch on a single\nv100 GPU, where emb. and hid. represent the\ndimensions of word embeddings and hidden state\nrespectively, and lay. is the number of transformer\nlayers. ∗means proportionally estimated time.\nof training epochs. These results suggest that our\nmodel architecture utilizes the training data more\nefﬁciently. Comparing the different pruning thresh-\nolds m=4 and m=8 (last two rows), the two models\nactually converge to a similar place after 60 epochs,\nconﬁrming the effectiveness of the pruned tree in-\nduction algorithm. We also replace Transformers\nwith Tree-LSTMs as in Jang et al. (2017), denoted\nas T-LSTM, ﬁnding that the perplexity is signiﬁ-\ncantly higher compared to other models.\nThe best score is from the BERT model with 12\nlayers at epoch 60. Although our model has a lin-\near time complexity, it is still a sequential encoding\nmodel, and hence its training time is not compa-\nrable to that of fully parallelizable models. Thus,\nwe do not have results of 12-layer Transformers\nin Table 1. The experimental results comparing\nmodels with the same parameter size suggest that\nour model may perform even better with further\ndeep layers.\nTable 2 shows the training time of our R2D2 with\nand without pruning. The last row is proportionally\nestimated by running the small setting (12×12×1).\nIt is clear that it is not feasible to run our R2D2\nwithout pruning.\n3.2 Unsupervised Constituency Parsing\nWe next assess to what extent the trees that nat-\nurally arise in our model bear similarities with\nhuman-speciﬁed parse trees.\n3.2.1 Setup\nBaselines and Evaluation. For comparison, we\nfurther include four recent strong models for un-\nsupervised parsing with open source code: BERT\nmasking (Wu et al., 2020), Ordered Neurons (Shen\net al., 2019), DIORA (Drozdov et al., 2019) and\nC-PCFG (Kim et al., 2019a). Following Htut et al.\n(2018), we train all systems on a training set con-\nsisting of raw text, and evaluate and report the\nresults on an annotated test set. As an evaluation\nmetric, we adopt sentence-level unlabeled F1 com-\nputed using the script from Kim et al. (2019a). We\ncompare against the non-binarized gold trees per\nconvention. The best checkpoint for each system is\npicked based on scores on the validation set.\nAs our model is a pretrained model based on\nword-pieces, for a fair comparison, we test all mod-\nels with two types of input: word level (W) and\nword-piece level (WP) 3. To support word-piece\nlevel evaluation, we convert gold trees to word-\npiece level trees by simply breaking each terminal\nnode into a non-terminal node with its word-pieces\nas terminals, e.g., (NN discrepancy) into (NN (WP\ndisc) (WP ##re) (WP ##pan) (WP ##cy). We set\nthe pruning threshold mto 8 for our tree encoder.\nTo support a word-level evaluation, since our\nmodel uses word-pieces, we force it to not prune\nor select spans that conﬂict with word spans dur-\ning prediction, and then merge word-pieces into\nwords in the ﬁnal output. However, note that this\nconstraint is only used for word-level prediction.\nFor training, we use the same hyperparame-\nters as in Section 3.1.1. Our model pretrained on\nWikiText-2 is ﬁnetuned on the training set with the\nsame unsupervised loss objective. For Chinese, we\nuse a subset of Chinese Wikipedia for pretraining,\nspeciﬁcally the ﬁrst 100,000 sentences shorter than\n150 characters.\nData. We test our approach on the Penn Treebank\n(PTB) (Marcus et al., 1993) with the standard splits\n(2-21 for training, 22 for validation, 23 for test)\nand the same preprocessing as in recent work (Kim\net al., 2019a), where we discard punctuation and\nlower-case all tokens. To explore the universality\nof the model across languages, we also run exper-\niments on Chinese Penn Treebank (CTB) 8 (Xue\net al., 2005), on which we also remove punctuation.\nNote that in all settings, the training is conducted\nentirely on raw unannotated text.\n3.2.2 Results and Discussion\nTable 3 provides the unlabeled F1 scores of all sys-\ntems on the WSJ and CTB test sets. It is clear that\nall systems perform better than left/right branching\nand random trees. Word-level C-PCFG (W) per-\nforms best on both the WSJ and CTB test sets when\nmeasuring against word-level gold standard trees.\nOur system performs better than ON-LSTM (W),\nbut worse than DIORA (W) and C-PCFG (W). Still,\n3As DIORA relies on ELMO word embeddings, to sup-\nport word-piece level inputs, we use BERT word-piece em-\nbeddings instead.\n4903\nWSJ CTB\nModel cplx F1(M) F1 F1\nLeft Branching (W) O(n) - 8.15 11.28\nRight Branching (W)O(n) - 39.62 27.53\nRandom Trees (W) O(n) - 17.76 20.17\nBERT-MASK (WP) O(n4) - 37.39 33.24\nON-LSTM (W) O(n) 50.0† 47.72 24.73\nDIORA (W) O(n3) 58.9† 51.42 -\nC-PCFG (W) O(n3) 60.1† 54.08 49.95\nOurs (WP) O(n) - 48.11 44.85\nDIORA (WP) O(n3) - 43.94 -\nC-PCFG (WP) O(n3) - 49.76 60.34\nOurs (WP) O(n) - 52.28 63.94\nTable 3: Unsupervised parsing results with word\n(W) or word-piece (WP) as input. Values with†are\ntaken from Kim et al. (2019a). F1(M) describes\nthe max. score of 4 runs with different random\nseeds. The F1 column shows results of our runs\nwith a random seed. The bottom three systems take\nword-pieces as input, and are also measured against\nword-piece level golden trees.\nthis is a remarkable result. Note that models such\nas C-PCFG are specially designed for unsupervised\nparsing, e.g., adopting 30 nonterminals, 60 preter-\nminals, and a training objective that is well-aligned\nwith unsupervised parsing. In contrast, the objec-\ntive of our model is that of bi-directional language\nmodeling, and the derived binary trees are merely\na by-product of our model that happen to emerge\nnaturally from the model’s preference for structures\nthat are conducive to better language modeling.\nAnother factor is the mismatch between our train-\ning and evaluation, where we train our model at the\nword-piece level, but evaluate against word-level\ngold trees. For comparison, we thus also consid-\nered DIORA (WP), C-PCFG (WP), and our sys-\ntem all trained on word-piece inputs, and evaluated\nagainst word-piece level gold trees. The last three\nlines show the results, with our system achieving\nthe best F1. As breaking words into word-pieces\nintroduces word boundaries as new spans, while\nword boundaries are easier to recognize, the overall\nF1 score may increase, especially on Chinese.\nAnalysis. In order to better understand why our\nmodel works better when evaluating on word-piece\nlevel golden trees, we compute the recall of con-\nstituents following Kim et al. (2019b) and Drozdov\net al. (2020). Besides standard constituents, we\nalso compare the recall of word-piece chunks and\n(WP) WD NNP NP VP SBAR\nWSJ\nDIORA 81.65 77.83 71.24 17.30 22.16\nC-PCFG 74.26 66.44 65.01 23.63 40.40\nOurs 99.24 86.76 72.59 24.74 39.81\nCTB\nC-PCFG 89.34 - 46.74 39.53 -\nOurs 97.16 - 61.26 37.90 -\nTable 4: Recall of constituents and words at word-\npiece level. WD means word.\nproper noun chunks. Proper noun chunks are ex-\ntracted by ﬁnding adjacent unary nodes with same\nparent and tag NNP.\nTable 4 reports the recall scores for constituents\nand words on the WSJ and CTB test sets. Our\nmodel and DIORA perform better for small se-\nmantic units, while C-PCFG better matches larger\nsemantic units such as VP and SBAR. The recall of\nword chunks (WD) of our system is almost perfect\nand signiﬁcantly better than for other algorithms.\nPlease note that all word-piece level models are\ntrained fairly without using any boundary informa-\ntion. Although it is trivial to recognize English\nword boundaries among word-pieces using rules,\nthis is non-trivial for Chinese. Additionally, the re-\ncall of proper noun segments is as well signiﬁcantly\nbetter for our model compared to other algorithms.\n3.3 Dependency Tree Compatibility\nWe compared examples of trees inferred by our\nmodel with the corresponding ground truth con-\nstituency trees (see Appendix), encountering rea-\nsonable structures that are different from the con-\nstituent structure posited by the manually deﬁned\ngold trees. Experimental results of previous work\n(Drozdov et al., 2020; Kim et al., 2019a) also show\nsigniﬁcant variance with different random seeds.\nThus, we hypothesize that an isomorphy-focused\nF1 evaluation with respect to gold constituency\ntrees is insufﬁcient to evaluate how reasonable the\ninduced structures are. In contrast, dependency\ngrammar encodes semantic and syntactic relations\ndirectly, and has the best interlingual phrasal co-\nhesion properties (Fox, 2002). Therefore, we in-\ntroduce dependency compatibility as an additional\nmetric and re-evaluate all system outputs.\n3.3.1 Setup\nBaselines and Data. As our approach is a word-\npiece level pretrained model, to enable a fair com-\nparison, we train all models on word-pieces and\n4904\nWSJ CTB\nModel %all %n≤10 %n≤20 %n≤40 %all %n≤10 %n≤20 %n≤40\nBERT-MASK (W) 53.53 77.03 55.46 44.66 48.56 68.89 47.27 36.62\nON-LSTM (W) 61.43† 77.05† 62.99† 55.94† 36.48 58.57 34.08 26.59\nDIORA (W) 67.76 78.08 68.80 64.15 — — — —\nC-PCFG (W) 72.74† 85.10† 74.65† 67.19† 64.41 75.54 65.89 58.16\nDIORA (WP) 54.73 68.80 55.68 49.22 — — — —\nC-PCFG (WP) 67.18 83.09 68.20 61.03 62.25 74.98 61.04 52.52\nOurs (WP) 69.29 80.29 70.29 64.79 64.74 74.42 63.86 59.20\nTable 5: Compatibility with dependency trees. (W) denotes word level inputs, (WP) refers to word-piece\nlevel inputs. %all denotes the accuracy on all test sentences, while %n≤x is the accuracy on sentences of\nup to xwords. Values with †are evaluated with predicted trees from Kim et al. (2019a)\nFigure 6: Examples of compatible and incompatible\nspans.\nlearn models with the same settings as in the orig-\ninal papers. Evaluation at the word-piece level\nreveals the model’s ability to learn structure from\na smaller granularity. In this section, we keep the\nword-level gold trees unchanged and invoke Stan-\nford CoreNLP (Manning et al., 2014) to convert\nthe WSJ and CTB into dependency trees.\nEvaluation. Our metric is based on the notion of\nquantifying the compatibility of a tree by counting\nhow many spans comply with dependency relations\nin the gold dependency tree. Speciﬁcally, as illus-\ntrated in Figure 6, a span is deemed compatible\nwith the ground truth if and only if this span forms\nan independent subtree.\nFormally, given a gold dependency tree D, we\ndenote as S(D) the raw token sequence forD. Con-\nsidering predicting a binary tree for word-level in-\nput, predicted spans in the binary tree are denoted\nas Z. For any span z ∈ Z, the subgraph of D\nincluding nodes in zand directional edges between\nthem is referred to as Gz. O(Gz) is deﬁned as the\nset of nodes with parent nodes not in Gz and I(Gz)\ndenotes the set of nodes whose child nodes are\nnot in Gz. Thus, |O(Gz)|and |I(Gz)|are the out-\ndegree and in-degree of the subgraph Gz. Let I(z)\ndenote whether zis valid, deﬁned as\nI(z)\n{\n1, |O(Gz)|= 1and I(Gz) ⊆O(Gz)\n0, otherwise. (9)\nFor binary tree spans for word-piece level input,\nif zbreaks word-piece spans, then I(z) = 0. Oth-\nerwise, word-pieces are merged to words and the\nword-level logic is followed. Speciﬁcally, to make\nthe results at the word and word-piece levels com-\nparable, I(z) is forced to be zero if zonly covers\na single word. The ﬁnal compatibility for Zis∑\nz∈ZI(z)\n|S(D)|−1.\n3.3.2 Results and Discussion\nTable 5 lists system results on the WSJ and CTB\ntest sets. %all refers to the accuracy on all test sen-\ntences, while %n≤x is the accuracy on sentences\nwith up toxwords. It is clear that the smaller granu-\nlarity at the word-piece level makes this task harder.\nOur model performs better than other systems at\nthe word-piece level on both English and Chinese\nand even outperforms the baselines in many cases\nat the word level. It is worth noting that the result\nis evaluated on the same binary predicted trees as\nwe use for unsupervised constituency parsing, yet\nour model outperforms baselines that perform bet-\nter in Table 3. One possible interpretation is that\nour approach learns to prefer structures different\nfrom human-deﬁned phrase structure grammar but\nself-consistent and compatible with a tree structure.\nTo further understand the strengths and weaknesses\nof each baseline, we analyzed the compatibility of\ndifferent sentence length ranges. Interestingly, we\nﬁnd that our approach performs better on long sen-\ntences compared with C-PCFG at the word-piece\nlevel. This shows that a bidirectional language\nmodeling objective can learn to induce accurate\nstructures even on very long sentences, on which\ncustom-tailored methods may not work as well.\n4905\n4 Related Work\nPre-trained models. Pre-trained models have\nachieved signiﬁcant success across numerous tasks.\nELMo (Peters et al., 2018), pretrained on bidi-\nrectional language modeling based on bi-LSTMs,\nwas the ﬁrst model to show signiﬁcant improve-\nments across many downstream tasks. GPT (Rad-\nford et al., 2018) replaces bi-LSTMs with a Trans-\nformer (Vaswani et al., 2017). As the global atten-\ntion mechanism may reveal contextual information,\nit uses a left-to-right Transformer to predict the\nnext word given the previous context. BERT (De-\nvlin et al., 2019) proposes masked language model-\ning (MLM) to enable bidirectional modeling while\navoiding contextual information leakage by directly\nmasking part of input tokens. As masking input\ntokens results in missing semantics, XLNET (Yang\net al., 2019) proposes permuted language model-\ning (PLM), where all bi-directional tokens are visi-\nble when predicting masked tokens. However, all\naforementioned Transformer based models do not\nnaturally capture positional information on their\nown and do not have explicit interpretable struc-\ntural information, which is an essential feature of\nnatural language. To alleviate the above shortcom-\nings, we extend pre-training and the Transformer\nmodel to structural language models.\nRepresentation with structures. In the line of\nwork on learning a sentence representation with\nstructures, Socher et al. (2011) proposed the ﬁrst\nneural network model applying recursive autoen-\ncoders to learn sentence representations, but their\napproach constructs trees in a greedy way, and it is\nstill unclear how autoencoders can perform against\nlarge pre-trained models (e.g., BERT). Yogatama\net al. (2017) jointly train their shift-reduce parser\nand sentence embedding components. As their\nparser is not differentiable, they have to resort to\nreinforcement training, but the learned structures\ncollapse to trivial left/right branching trees. The\nwork of URNNG (Kim et al., 2019b) applies varia-\ntional inference over latent trees to perform unsu-\npervised optimization of the RNNG (Dyer et al.,\n2016), an RNN model that estimates a joint dis-\ntribution over sentences and trees based on shift-\nreduce operations. Maillard et al. (2017) propose\nan alternative approach, based on CKY parsing.\nThe algorithm is made differentiable by using a\nsoft-gating approach, which approximates discrete\ncandidate selection by a probabilistic mixture of\nthe constituents available in a given cell of the chart.\nThis makes it possible to train with backpropaga-\ntion. However, their model runs in O(n3) and they\nuse Tree-LSTMs.\n5 Conclusion and Outlook\nIn this paper, we have proposed an efﬁcient CKY-\nbased recursive Transformer to directly model hi-\nerarchical structure in linguistic utterances. We\nhave ascertained the effectiveness of our approach\non language modeling and unsupervised parsing.\nWith the help of our efﬁcient linear pruned tree\ninduction algorithm, our model quickly learns in-\nterpretable tree structures without any syntactic\nsupervision, which yet prove highly compatible\nwith human-annotated trees. As future work, we\nare investigating pre-training our model on billion\nword corpora as done for BERT, and ﬁne-tuning\nour model on downstream tasks.\nAcknowledgements\nWe thank Liqian Sun, the wife of Xiang Hu, for\ntaking care of their newborn baby during critical\nphases, which enabled Xiang to polish the work\nand perform experiments.\n4906\nReferences\nYoshua Bengio. 2009. Learning deep architectures for\nAI. Now Publishers Inc.\nNoam Chomsky. 1956. Three models for the descrip-\ntion of language. IRE Trans. Inf. Theory, 2(3):113–\n124.\nNoam Chomsky. 2014. Aspects of the Theory of Syntax,\nvolume 11. MIT press.\nJohn Cocke. 1969. Programming Languages and Their\nCompilers: Preliminary Notes. New York Univer-\nsity, USA.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAndrew Drozdov, Subendhu Rongali, Yi-Pei Chen,\nTim O’Gorman, Mohit Iyyer, and Andrew McCal-\nlum. 2020. Unsupervised parsing with S-DIORA:\nSingle tree encoding for deep inside-outside recur-\nsive autoencoders. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4832–4845, Online. As-\nsociation for Computational Linguistics.\nAndrew Drozdov, Patrick Verga, Mohit Yadav, Mohit\nIyyer, and Andrew McCallum. 2019. Unsupervised\nlatent tree induction with deep inside-outside recur-\nsive auto-encoders. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1129–1141, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 199–209, San Diego, California.\nAssociation for Computational Linguistics.\nHeidi Fox. 2002. Phrasal cohesion and statistical ma-\nchine translation. In Proceedings of the 2002 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP 2002), pages 304–3111. Asso-\nciation for Computational Linguistics.\nPhu Mon Htut, Kyunghyun Cho, and Samuel Bowman.\n2018. Grammar induction with neural language\nmodels: An unusual replication. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4998–5003, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Categor-\nical reparameterization with gumbel-softmax. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nTadao Kasami. 1966. An efﬁcient recognition\nand syntax-analysis algorithm for context-free lan-\nguages. Coordinated Science Laboratory Report no.\nR-257.\nYoon Kim, Chris Dyer, and Alexander Rush. 2019a.\nCompound probabilistic context-free grammars for\ngrammar induction. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2369–2385, Florence, Italy. Asso-\nciation for Computational Linguistics.\nYoon Kim, Alexander Rush, Lei Yu, Adhiguna Kun-\ncoro, Chris Dyer, and G ´abor Melis. 2019b. Unsu-\npervised recurrent neural network grammars. InPro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1105–1117,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nPhong Le and Willem Zuidema. 2015. The forest con-\nvolutional network: Compositional distributional se-\nmantics with a neural chart and without binarization.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1155–1164, Lisbon, Portugal. Association for Com-\nputational Linguistics.\nJean Maillard, Stephen Clark, and Dani Yogatama.\n2017. Jointly learning sentence embeddings\nand syntax with unsupervised tree-lstms. CoRR,\nabs/1705.09189.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The Stanford CoreNLP natural language pro-\ncessing toolkit. In Proceedings of 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations, pages 55–60, Bal-\ntimore, Maryland. Association for Computational\nLinguistics.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional Linguistics, 19(2):313–330.\nMarie-Catherine de Marneffe, Bill MacCartney, and\nChristopher D. Manning. 2006. Generating typed\n4907\ndependency parses from phrase structure parses. In\nProceedings of the Fifth International Conference\non Language Resources and Evaluation (LREC’06),\nGenoa, Italy. European Language Resources Associ-\nation (ELRA).\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nRuslan Salakhutdinov. 2014. Deep learning. In The\n20th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, KDD ’14,\nNew York, NY, USA - August 24 - 27, 2014, page\n1973. ACM.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics,\npages 2699–2712, Online. Association for Compu-\ntational Linguistics.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron C. Courville. 2019. Ordered neurons: Inte-\ngrating tree structures into recurrent neural networks.\nIn 7th International Conference on Learning Repre-\nsentations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net.\nRichard Socher, Jeffrey Pennington, Eric H. Huang,\nAndrew Y . Ng, and Christopher D. Manning. 2011.\nSemi-supervised recursive autoencoders for predict-\ning sentiment distributions. In Proceedings of the\n2011 Conference on Empirical Methods in Natural\nLanguage Processing, pages 151–161, Edinburgh,\nScotland, UK. Association for Computational Lin-\nguistics.\nKai Sheng Tai, Richard Socher, and Christopher D.\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 1556–1566, Beijing, China. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020.\nPerturbed masking: Parameter-free probing for ana-\nlyzing and interpreting BERT. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4166–4176, Online. As-\nsociation for Computational Linguistics.\nNaiwen Xue, Fei Xia, Fu-dong Chiou, and Marta\nPalmer. 2005. The penn chinese treebank: Phrase\nstructure annotation of a large corpus. Nat. Lang.\nEng., 11(2):207–238.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 5754–5764.\nDani Yogatama, Phil Blunsom, Chris Dyer, Edward\nGrefenstette, and Wang Ling. 2017. Learning to\ncompose words into sentences with reinforcement\nlearning. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings.\nOpenReview.net.\nDaniel H Younger. 1967. Recognition and parsing of\ncontext-free languages in time n3. Information and\ncontrol, 10(2):189–208.\nXiao-Dan Zhu, Parinaz Sobhani, and Hongyu Guo.\n2015. Long short-term memory over recursive struc-\ntures. In Proceedings of the 32nd International Con-\nference on Machine Learning, ICML 2015, Lille,\nFrance, 6-11 July 2015, volume 37 of JMLR Work-\nshop and Conference Proceedings , pages 1604–\n1612. JMLR.org.\n4908\nA Appendix: Tree Examples\nSystem Tree\nR2D2 whenthe price of plasticstook off in 1987 quantumchemicalcorp . went alongfor the ride\nGOLD whenthe priceof plasticstook off in 1987 quantumchemicalcorp. went alongfor the ride\nR2D2 pricingcyclesto be sure are nothingnewfor plasticsproducers\nGOLD pricingcyclesto be sure are nothingnewfor plasticsproducers\nR2D2 we were all wonderfulheroeslast year says an executiveat one of quantum’s competitors\nGOLD we were all wonderfulheroeslast year says an executiveat one of quantum’scompetitors\nR2D2 in the u . s . poly ##eth##yle##ne marketquantumhas claimedthe largestshareabout20 %\nGOLD in the u.s. polyethylenemarketquantumhas claimedthe largestshareabout20 %\nR2D2 notingothers’estimatesof whenpriceincreasescan be sustainedhe remarkssomesay october\nGOLD notingothers’estimatesof whenpriceincreasescan be sustainedhe remarkssomesay october",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6269645690917969
    },
    {
      "name": "Differentiable function",
      "score": 0.5850175619125366
    },
    {
      "name": "Transformer",
      "score": 0.5015146732330322
    },
    {
      "name": "Tree (set theory)",
      "score": 0.4297567903995514
    },
    {
      "name": "Natural language processing",
      "score": 0.4249330163002014
    },
    {
      "name": "Programming language",
      "score": 0.4072086215019226
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39189061522483826
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3466978967189789
    },
    {
      "name": "Mathematics",
      "score": 0.18756446242332458
    },
    {
      "name": "Pure mathematics",
      "score": 0.10694071650505066
    },
    {
      "name": "Engineering",
      "score": 0.10004189610481262
    },
    {
      "name": "Combinatorics",
      "score": 0.0949486792087555
    },
    {
      "name": "Electrical engineering",
      "score": 0.08467307686805725
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I143288331",
      "name": "Hasso Plattner Institute",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I176453806",
      "name": "University of Potsdam",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210131353",
      "name": "Apax Partners (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 11
}