{
    "title": "Multi-label text classification of Indonesian customer reviews using bidirectional encoder representations from transformers language model",
    "url": "https://openalex.org/W4381737009",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3085961167",
            "name": "Nuzulul Khairu Nissa",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2306735609",
            "name": "Evi Yulianti",
            "affiliations": [
                "University of Indonesia"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2251648804",
        "https://openalex.org/W3005967578",
        "https://openalex.org/W4293414344",
        "https://openalex.org/W2900758626",
        "https://openalex.org/W4200125711",
        "https://openalex.org/W3092085049",
        "https://openalex.org/W2084465406",
        "https://openalex.org/W1999954155",
        "https://openalex.org/W3003979496",
        "https://openalex.org/W4210810455",
        "https://openalex.org/W4200142573",
        "https://openalex.org/W4281489419",
        "https://openalex.org/W4200370455",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3076947077",
        "https://openalex.org/W4284693641",
        "https://openalex.org/W3174995573",
        "https://openalex.org/W3215317537",
        "https://openalex.org/W3036832155",
        "https://openalex.org/W3086966320",
        "https://openalex.org/W3116295307",
        "https://openalex.org/W4206130444",
        "https://openalex.org/W4313546696",
        "https://openalex.org/W2787396463",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3199243620",
        "https://openalex.org/W3102476541",
        "https://openalex.org/W3006023459",
        "https://openalex.org/W2114315281",
        "https://openalex.org/W4289398497",
        "https://openalex.org/W4200051809",
        "https://openalex.org/W2295598076"
    ],
    "abstract": "&lt;p&gt;&lt;span lang=\"EN-US\"&gt;Customer review is a critical resource to support the decision-making process in various industries. To understand how customers perceived each aspect of the product, we can first identify all aspects discussed in the customer reviews by performing multi-label text classification. In this work, we want to know the effectiveness of our two proposed strategies using bidirectional encoder representations from transformers (BERT) language model that was&lt;br /&gt; pre-trained on the Indonesian language, referred to as IndoBERT, to perform multi-label text classification. First, IndoBERT is used as feature representation to be combined with convolutional neural network-extreme gradient boosting (CNN-XGBoost). Second, IndoBERT is used both as the feature representation as well as the classifier to directly solve the classification task. Additional analysis is performed to compare our results with those using multilingual BERT model. According to our experimental results, our first model using IndoBERT as feature representation shows significant performance over some baselines. Our second model using IndoBERT as both feature representation and classifier can significantly enhance the effectiveness of our first model. In summary, our proposed models can improve the effectiveness of the baseline using Word2Vec-CNN-XGBoost by 19.19% and 6.17%, in terms of accuracy and F-1 score, respectively.&lt;/span&gt;&lt;/p&gt;",
    "full_text": "International Journal of Electrical and Computer Engineering (IJECE)  \nVol. 13, No. 5, October 2023, pp. 5641~5652 \nISSN: 2088-8708, DOI: 10.11591/ijece.v13i5.pp5641-5652 ï²     5641 \n \nJournal homepage: http://ijece.iaescore.com \nMulti-label text classification of Indonesian customer reviews \nusing bidirectional encoder representations from transformers \nlanguage model \n \n \nNuzulul Khairu Nissa, Evi Yulianti \nFaculty of Computer Science, University of Indonesia, Depok, Indonesia \n \n \nArticle Info  ABSTRACT  \nArticle history: \nReceived Dec 8, 2022 \nRevised Feb 2, 2023 \nAccepted Feb 10, 2023 \n \n Customer review is a critical resource to support the decision-making process \nin various industries. To understand how customers perceived each aspect of \nthe product, we can first identify all aspects discussed in the customer reviews \nby performing multi-label text classification. In this work, we want to know \nthe effectiveness of our two proposed strategies using bidirectional encoder \nrepresentations from transformers (BERT) language model that was  \npre-trained on the Indonesian language, referred to as IndoBERT, to perform \nmulti-label text classification. First, IndoBERT is used as feature \nrepresentation to be combined with convolutional neural network -extreme \ngradient boosting (CNN -XGBoost). Second, IndoBERT is used both as the \nfeature representation as we ll as the classifier to directly solve the \nclassification task. Additional analysis is performed to compare our results \nwith those using multilingual BERT model. According to our experimental \nresults, our first model using IndoBERT as feature representatio n shows \nsignificant performance over some baselines. Our second model using \nIndoBERT as both feature representation and classifier can significantly \nenhance the effectiveness of our first model. In summary, our proposed \nmodels can improve the effectiveness of the baseline using Word2Vec-CNN-\nXGBoost by 19.19% and 6.17%, in terms of accuracy and F -1 score, \nrespectively. \nKeywords: \nConvolutional neural network \nCustomer review \nIndoBERT \nMulti-label text classification \nWord2Vec \nThis is an open access article under the CC BY-SA license. \n \nCorresponding Author: \nEvi Yulianti \nFaculty of Computer Science, University of Indonesia \nDepok, West Java, Indonesia \nEmail: evi.y@cs.ui.ac.id \n \n \n1. INTRODUCTION  \nCustomer review is a critical resource to discover useful information about user experiences on a \nparticular product (or service). Such information is important for a company to help them making a good \ndecision about their products. A review text may conta in userâ€™s opinion about several aspects of a product, \nwhere each aspect may accept different sentiments from the user. Here is an example of Indonesian customer \nreview of hotel experiences that contains different sentiments for different aspects of hotel: â€œkamarnya nyaman \ndan bersih, tetapi TV nya terlalu tinggi jadi kamu tidak bisa nontonâ€  (â€œThe room is comfortable and clean, \nbut the TV is too high, so you canâ€™t watch itâ€) . In that review, the aspect of â€œcleanlinessâ€ has a positive \nsentiment, but the aspect of â€œTVâ€ as one of the hotelâ€™s facilities has a negative sentiment. \nAspect category detection or aspect classification is one of the subtasks from aspect -based sentiment \nanalysis (ABSA) [1]. For the aspect classification task, the aspects contained in the text review are iden tified \nand the polarity of each aspect is then determined by sentiment classification. The results of this system are \n\n      ï²          ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 13, No. 5, October 2023: 5641-5652 \n5642 \nbeneficial for a company to understand which aspects of their product that are perceived as positive, and which \nof those that are perceived as negative by customers. The company can then make some decisions to improve \nthe aspect of product that are still perceived as negative by costumers. \nAspect classification is formulated as multi -label text classification problem [2]. A general text \nclassification problem will associate relevant text with pre -existing labels [3]. According to the number of \nlabels that must be identified, the text classification problem is divided into two categories: multi -class \nclassification and multi-label classification. For multi-label classification, a text can be assigned more than one \nlabel, while for multi-class classification, a text will be assigned one label only [4].  \nSeveral previous studies have used machine learning methods and problem transformation approaches \nto solve multi -label text classification tas k [2], [3], [5], [6] . Problem transformation is an approach that can \nconvert the multi -label case into single -label learning tasks [5]. The two approach of the problem \ntransformation that are frequently used include binary relevance (BR) strategy and classifier chain (CC) \nstrategy. Using the BR approach, a multi -label classification problem with n labels is converted into n binary \nclassification problems [7]. As the BR strategy believes th at each label in the dataset is independent of every \nother label, it completely ignores the correlation between labels. To analyze the correlation between the labels, \nthe CC method will link the labels in a â€œchainâ€ way [8]. \nDeep neural network-based multi-label text classification techniques have recently gained popularity \ndue to the rapid growth of deep learning, such as: long short -term memory (LSTM) [9], bidirectional long \nshort-term memory (BiLSTM) [10], convolutional neural network (CNN) [2], [11] â€“[13] and recurrent \nconvolutional neural network (RCNN) [12]. Apart from the development of various types of models to solve \nthe multi-label text classification, the feature representation that can be used are also evolving. There are several \nmethods that can extract the features from text, including: bag -of-words (BoW), term frequency inverse \ndocument frequency (TF-IDF) and context independent text embeddings (Word2Vec, fast-Text and GloVe). \nAzhar et al. [2] have studied the multi-label text classification for Indonesian customer hotel reviews \ndataset using Word2Vec as text embedding, CNN as the feature extraction method and various kind of machine \nlearning models (i.e., support vector machines (SVM), long short term memory (LSTM) and extreme gradient \nboosting (XGBoost)) as the top -level classifiers. They used of BR and CC strategies for the problem \ntransformation approaches. Their experiments show that combining CNN and XGBoost classifier with the CC \nstrategy, which can consider dependencies between the labels, produces better results than using the BR strategy. \nNowadays, the contextualized text embedding meth od from the pre -trained language model \nbidirectional encoder representation from transformers  (BERT) has been developed and we can also use it to \nsolve the multi -label text classification task. BERT is a state -of-the-art of the contextualized pre -trained \nlanguage model with a deeper understanding of the language as an effect of bidirectional learning [14]. Besides \nas feature representation, BERT can also be fine -tuned to effectively solve a range of downstream tasks, such \nas text classification [11]â€“[13], [15], [16] , question answering [17], named entity recognition [18] and \nsentiment analysis [19]. BERT also has several types of models, namely: multilingual BERT (mBERT) which \nis a single pre -trained language model on the concatenation of monolingual Wikipedia corpora from 104 \nlanguages. Monolingual BERT,  on the other hand, is a BERT model that has only been pre -trained in one \nlanguage. A recent BERT pre-trained model called IndoBERT was particularly trained utilizing a huge number \nof the Indonesian language corpus [20], [21]. Some recent work have started to exploit IndoBERT to enhance \nthe effectiveness of their methods on various tasks [16], [22]. \nKhasanah and Krisnadhi [11], used IndoBERT embedding with a single channel CNN as the classifier \nto perform the extreme multi -label text classification task. The result s demonstrate that their suggested \napproach (single CNN with IndoBERT) performs better than the single CNN with embedding of fastText and \nthe single CNN with embedding of Word2Vec. Another study from Neruda and Winarko [13] utilized the \nsocial media data to identify traffic events using the combination of IndoBERT as the text embedding and CNN \nas classifier. In comparison to non-contextualized text embedding, BERT's contextualized embedding helps in \nunderstanding the context and gives better results. \nDepart from the results of Azhar et al. [2], Khasanah and Krisnadhi  [11], and Neruda and Winarko  \n[13], we propose  two strategies to perform multi -label text classification to predict or categorize the aspects \nfrom Indonesian hotel reviews dataset. In our first strategy, following the method with the best results in the \nstudy from Khasanah and Krisnadhi  [11], we propose to use IndoBERT as text embedding that is combined \nwith CNN feature extraction method and XGBoost classifier, replacing Word2Vec embedding that was used \nin previous studies from Azhar et al. [2]. Then, in our second strategy, we use IndoBERT in end-to-end fashion, \nas feature representation as well as classifier, to directly solve the multi-label classification task. As far as we know, \nno previous studies have investigated the use of IndoBERT as a multi-label classifier for aspect classification in the \nIndonesian hotel reviews dataset, since the pre-trained model IndoBERT was still relatively new. \nOur research questions in this paper are as follows: i) How is the effectiveness of our first model using \nIndoBERT as feature representation that is combined with CNN and XGBoost classifier for m ulti-label text \nInt J Elec & Comp Eng  ISSN: 2088-8708 ï² \n \n Multi-label text classification of Indonesian customer reviews using bidirectional â€¦ (Nuzulul Khairu Nissa) \n5643 \nclassification?; ii) How is the effectiveness of our second model using IndoBERT end-to-end model for multi-\nlabel text classification?; and iii) How much different is the effectiveness of our models using monolingual language \nmodel IndoBERT compared to those using multilingual language model mBERT for multi-label text classification? \n \n \n2. RESEARCH METHOD \nThe aspect classification task is formulated as multi -label text classification. This section explains \ndataset, multilabel classification task, system components (BERT, CNN, and XGBoost), research method, \nevaluation method and experiment details. The gener al architecture for our two suggested strategies will also \nbe illustrated in this section. \n \n2.1.  Dataset \nWe use Airy Rooms hotel reviews dataset in Indonesian language from Azhar et al. [2]. We divided \nthe dataset for this study into train, valid, and test sets according to the standardization of IndoNLU \ndocumentation [20]. The dataset consists of 2,854 reviews, that was divided as follows: 2 ,283 for the train \ndataset, 286 for the test dataset and 285 for the validation dataset. Each review consists of text and a set of \nassigned labels. Table 1 presents the labels distribution of Airy Rooms hotel reviews dataset.  \nBased on the label distribution in Table 1, there are ten labels in this dataset: ac ( ac), hot water ( air \npanas), smell |(bau), general |(general), cleanliness |(kebersihan), linen |(linen), service |(service), sunrise meal \n(sunrise meal), tv |(tv)| and Wi-Fi |(Wi-Fi). We can also identify if the most frequently reviewed aspect was \ncleanliness (kebersihan), and the least reviewed aspect was the sunrise meal (sunrise meal).  \n \n \nTable 1. Labels distribution each aspect \nLabels (Eng) Labels (Indo) Train Valid Test \nAC â€œACâ€ 469 59 53 \nHot water â€œAir panasâ€ 361 55 41 \nSmell â€œBauâ€ 372 38 42 \nGeneral â€œGeneralâ€ 260 32 43 \nCleanliness â€œKebersihanâ€ 933 128 128 \nLinen â€œLinenâ€ 670 95 89 \nService â€œServiceâ€ 634 74 79 \nSunrise_meal â€œSunrise mealâ€ 175 24 22 \nTV â€œTVâ€ 208 31 29 \nWi-Fi â€œWi-Fiâ€ 355 41 36 \n \n \n2.2.  Multi-label classification task \nClassification task is generally described as: â€œGiven a train set made up of pairs (ğ‘¥ğ‘—, ğ‘¦ğ‘—), discover a \nfunction ğ‘“(ğ‘¥) that maps each attribute vector ğ‘¥ğ‘—, to its associated label ğ‘¦ğ‘—, with ğ‘— = 1, 2,3, â€¦ , ğ‘›, where the amount \nof training examples is nâ€ [23]. In multi-label classification, each input sample may correspond to more than one \nlabels. More specifically, for each input sample, there exist a set of labels â€œMâ€ to which the input sample belongs \n[24]. Figure 1 presents an illustration of the multi-label text classification formulation for aspect classification. \n \n \n \n \nFigure 1. The general research process of multi-label text classification \n \n \nThe aim of this study is to predict or categorize the aspect category of the customer review in the \nIndonesian dataset of hotel customer reviews. Given a customer review, the text embedding for the review is \ninitially generated. The embedding results are then inputted into the classifier that will produce the predictions of \nthe aspect labels of the customer review. For example, given a hotel review displayed in Figure 1, two aspects \nare classified from the review: â€œcleanlinessâ€ ( kebersihan) and â€œWi-Fiâ€ ( Wi-Fi). More detailed explanation \n\n      ï²          ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 13, No. 5, October 2023: 5641-5652 \n5644 \nabout our methods, including the text representation and the classification methods, are explained in   \nsection 2.4. \n \n2.3.  System components \n2.3.1. BERT \nThe current state-of-the-art for many natural language processing (NLP) applications is BERT, which \nstands for bidirectional encoder representations from transformers [14]. BERT is expected to learn a word's \ncontext right-to-left to predict the previous word or in left-to-right to predict the next word in a sequence [16]. \nThere are various kinds of BERT models which have been developed, such as: m -BERT [14], distil-BERT \n[25], XLMRoBERTa [26], IndoBERT (base, lite , and large) from Wilie et al.  [20], IndoBERT (base) and \nIndoBERTweet from Koto et al. [21], [27]. For more detailed explana tion on each of these methods, please \nrefer to the original paper. Table 2 summarizes the hyperparameter settings that were used in previous work to \nbuild all pre-trained language model BERT versions that are utilized in this work. Subscripts W and K in th e \ntable denotes the respective model was trained by Wilie et al. [20] and Koto et al. [21], respectively. \nMultilingual BERT  (M-BERT) is a single pre -trained language model on the concatenation of \nmonolingual Wikipedia corpora from 104 languages, including Indonesian language [14]. As a result of the  \nm-BERT model being pre -trained on high number of languages, it expands the applicability of this model, and \nresearcher can use it to solve the task in various languages. The DistilBERT is a distilled variation of the BERT; it \nis smaller and operates faster. It is also capable of retaining 97% of BERT's ability to understand a language [25]. \nOne of the multilingual model that has been trained on 100 different languages is called XLM-RoBERTa [26]. \nJust two years ago, a huge number of Indonesian corpus were used to pre -train the BERT model, and \nthe resulting model called IndoBERT [20], [21]  and were publicly available for research purpose. A lot of \nattention has been paid to the exploration of IndoBERT for several text processing task. The large -scale \nIndonesian dataset used to train IndoBERT by Willie et al. [20] was compiled from texts found on websites, \nnews, blogs, and social media. This dataset consist of around 4 billion  words, with around 250M sentences \n[20]. 220 million words from the Indonesian web corpus, news articles, and Wikipedia were used to train \nIndoBERT by Koto et al. [21]. Koto et al. [27] also released the IndoBERTweet, a BERT language model that \nhas been pre-trained with 409M word tokens from Indonesian Twitter dataset.  \n In this study, we build our model for aspect classification using the IndoBERT pre -trained model \ndeveloped by Wilie et al. [20] and Koto et al. [21], [27]. For our multi-label text classification task, we utilized \nIndoBERT using two strategies: i) using IndoBERT as feature representation only and ii) using IndoBERT as \nend-to-end model (i.e., it serves as feature representation as well as classifier). In addition, we also do further \nanalysis on the comparability of our models' results with those using multilingual BERT, such as m -BERT, \ndistil-BERT, and XLM-RoBERTa. \n \n \nTable 2. Hyperparameter configurations for BERT \nModel Type Embedding size Hidden layers Attention heads Vocab size Parameters \nIndoBERTW base 768 12 12 30522 124.5 M \nIndoBERTW lite 128 12 12 30000 11.7 M \nIndoBERTW large 1024 24 16 30522 335.2 M \nIndoBERTK base 768 12 12 31923 125 M \nIndoBERTweetK base 768 12 12 31984 125 M \nm-BERT base 768 12 12 30000 110 M \ndistil-BERT base 768 6 12 30522 66 M \nXLM-RoBERTa base 768 12 12 30522 125 M \n \n \n2.3.2. CNN \nIt has been shown that CNN is one of the models that performs great for the multi -label text \nclassification as investigated in [2], [11]â€“[13]. The type of convolu tion used in text-processing tasks is called \none-dimensional convolution. It involves mapping the input text into a set of embedding vectors that \ncorrespond to the text's word order [28]. In order to extract indicative information, over the sequence of word \nembedding vectors the convolutional layer moves a sliding window of size k, while performing a linear \ntransformation along with a non-linear activation function. The pooling layer selects only the information that \nis suitable for prediction for each window [28]. In this study, we used a single channel convolutional layer \nfollowing [11], with rectified linear unit (ReLU)  activation function. The model of CNN single is detailed in \nFigure 2. \nThe convolution windowâ€™s length is determined by the kernel size. For this study, the kernel will slide \nalong the input embedding and examine two words at a time because we used a kernel size of 2 (this essentially \ncorresponding to the bigram features) [11]. \nInt J Elec & Comp Eng  ISSN: 2088-8708 ï² \n \n Multi-label text classification of Indonesian customer reviews using bidirectional â€¦ (Nuzulul Khairu Nissa) \n5645 \n \n \nFigure 2. The architecture model of CNN single \n \n \n2.3.3. XGBoost \nAn expanded variant of the gradient boosting ensemble method is called XGBoost, or extreme \ngradient boosting [29]. To create an efficient learning machine for the ensemble decision three approach, the \ngradient boosting technique successively combines the results of weak classifiers. The components of XGBoost \nare an optimization objective function, a parameter adjustment, and a learning model. It is feasible to carry out \nobjective function optimization and reduce model complexity by optimizing the penalty function and \nminimizing the loss function [30]. \n \n2.4.  Research method \nThe research method section will illustrate the overall architecture of our proposed strategies. We also \nexplain in more detail our two proposed strategies for identifying all aspects contained in each customer review \nby performing multi-label text classification. The framework of our first and second strategies are illustrated \nrespectively in Figures 3 and 4. \n \n2.4.1. IndoBERT-CNN-XGBoost model \nFor the first strategy, we used IndoBERT as embeddin g, CNN as feature extraction method, and \nXGBoost as the classifier. Figure 3 depicts the research flow of this model. First, the dataset has to be \ntransformed into the BERT input format. We preprocessed the input data using the BERT Tokenizer. Because \nthe task that we used in this study is multi -label classification, every sentence must have a [SEP] token added \nat the end and a [CLS] token added at the beginning. To fit the maximum sequence length of 128 tokens, each \nsentence must be truncated or padded. Besides that, we used the â€˜attention_maskâ€™ which consists of â€²0â€² (denotes \nnot token) and â€²1â€² (denotes token). From this step we got the â€˜input_idsâ€™ and â€˜attention_maskâ€™ for each sentence. \nSuppose that we used the IndoBERT embedding from Wilie et al. [20], which has a vocabulary size \nof 30 522 and a dimension of 768. We got the 30522Ã—768 size of embedding matrix. Then, each input's \nâ€˜input_idsâ€™ and â€˜attention_maskâ€™ are embedded by the embed ding layer. An embedding vector ğ‘’ğ‘–, where ğ‘’ğ‘– âˆˆ\n ğ‘…ğ‘‘ is used to represent each word and ğ‘‘ stands for the word embedding dimension. To represent the sentence \nas a whole, the word vector representations of each word is then concatenated. We can define the  sentence \n\n      ï²          ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 13, No. 5, October 2023: 5641-5652 \n5646 \nrepresentation as ğ‘† = ğ‘’1:ğ‘š = ğ‘’1 âŠ• ğ‘’2 âŠ• â€¦ âŠ• ğ‘’ğ‘š, where ğ‘š is defined as the input text's maximum length [11]. \nInput for a single sentence is thus represented as ğ‘š Ã— ğ‘‘ matrix [15]. \nAfter we have obtained a vector representation for each review, then we used it as a feature to train \nthe CNN model to get the r efined version of the text feature. To solve the multi -label text classification, we \nreplace the CNN-trained model's output layer with XGBoost as the top -level classifier. The objective here is \nwe want to extract the text matrix's refined version produced by CNN and utilize it as a feature to train the \nXGBoost classifier. In this work, we also experimented with a few other classifiers, such as random forest and \nnaÃ¯ve Bayes algorithms. \n \n \n \n \nFigure 3. The research process of our first strategy model \n \n \n We use problem transformation method classifier chain (CC) to perform multi -label classification \nusing machine learning classifier [8], [15]. From our preliminary experiments, the results of CC strategy is \nbetter than BR strategy, which is consistent to previous work [5], [31]. The CC strategy is effective because it \ncan overcome the problem of label dependency [6]. Therefore, in our experiment in this work, we use CC \nstrategy to conduct problem transformation of our data for multi-label classification. \n \n2.4.2. IndoBERT end-to-end model \nFor the second strategy, we used I ndoBERT to build end-to-end model for multi-label classification. \nHere, IndoBERT is used as text embedding as well as classifier. Two versions of IndoBERT were used: \nIndoBERT that was pretrained by Wilie et al. [20], and IndoBERT and IndoBERTweet that was pretrained by \nKoto et al. [21], [27]. In our experiment, we also compared our results with those using multilingual BERT, \nsuch as m -BERT [14], distil-BERT [25] and XLM-RoBERTa [26] that were trained in previous work using \nmultilingual corpus (the different configurations of each model have been explained earlier in section 2.3.1). \nThe research process of the second strategy is illustrated in Figure 4. \n \n \n \n \nFigure 4. The research process for our second strategy model \n \n \nAs explained in the section 2.4.1 above, we need to adjust the dataset into BERT input format. After \nobtaining the appropriate data format, we must add a classifier layer on top of the model and allowing the \nBERT mod el to do the multi -label classification. To define the final value of the multi -label classification \n\nInt J Elec & Comp Eng  ISSN: 2088-8708 ï² \n \n Multi-label text classification of Indonesian customer reviews using bidirectional â€¦ (Nuzulul Khairu Nissa) \n5647 \nprocess, the [CLS] output from the final hidden layer, which is represented as a vector with dimensions of 768, \nwill be entered through a fully connected layer and then calculated using the sigmoid activation function. The \noutputs from sigmoid activation function give a value between 0 and 1, which represents the probabilities of \neach of the 10 predicted aspect labels. In this study, the predicted label outp ut is decided using a threshold \nvalue of 0.5 [32]. \n \n2.5.  Evaluation method \nIn this study, we use micro F1-score, hamming loss and accuracy for the model evaluation. Micro F1-\nscore calculated using the value of false negative (FN), true positive (TP) and false positive (FP). The micro \nF1-score which obtained from the average number of calculations from each aspect is defined as (1) [33]. \n \nğ‘€ğ‘–ğ‘ğ‘Ÿğ‘œ âˆ’ ğ¹1 =  \nâˆ‘ 2 Ã— ğ‘‡ğ‘ƒğ‘—\nğ¿\nğ‘—=1\nâˆ‘ (2 Ã— ğ¹ğ‘ƒğ‘— + ğ¹ğ‘ğ‘— + ğ‘‡ğ‘ƒğ‘—)ğ¿\nğ‘—=1\n (1) \n \nwhere the total number of aspects is ğ¿, the aspect index is ğ‘—, FP value on an aspect with index ğ‘— is ğ¹ğ‘ƒğ‘—, FN \nvalue on an aspect with index ğ‘— is ğ¹ğ‘ğ‘— and TP value on an aspect with index ğ‘— is ğ‘‡ğ‘ƒğ‘— . \nThe mismatches between the actual and the predicted aspect labels are measured using hamming loss \n(HL), which is determined as (2) [15]. \n \nğ»ğ¿ =  \n1\nğ‘ğ¾  âˆ‘ âˆ‘ 1(ğ‘¦ğ‘–,ğ‘˜ â‰  ğ‘¦Ì‚ğ‘–,ğ‘˜)\nğ¾\nğ‘˜=1\nğ‘\nğ‘–=1  (2) \n \nwhere ğ‘¦ğ‘–,ğ‘˜ is the actual aspect label and ğ‘¦Ì‚ğ‘–,ğ‘˜ denotes its predicted aspect label. ğ¾ is the total number of aspects \nand ğ‘ is the number of sample size. \nAccuracy is the probability of a label that has the same value between actual data and predictive data. \nThe (3) is the formula for accuracy in multi-label classification [6].  \n \nğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ =  1\nğ‘  âˆ‘\n|ğ‘¦Ì‚ğ‘– âˆ© ğ‘¦ğ‘–|\n|ğ‘¦Ì‚ğ‘– âˆª ğ‘¦ğ‘–|\nğ‘\nğ‘–=1  (3) \n \nwhere the total number of data is ğ‘, the actual aspect label set is ğ‘¦ğ‘–  and prediction aspect label is ğ‘¦Ì‚ğ‘– . \n \n2.6.  Experiment details \nIn our experiment, we use several baseline methods such as: random forest, naÃ¯ve Bayes, XGBoost, \nCNN, CNN -RandomForest, CNN -Naivebayes, and CNN -XGBoost. For each of these models, we also \nexperimented with the variation of text embeddings: Word2Vec and IndoBERT. The CNN -XGBoost model \nwith Word2Vec embedding was actually the best performing method in Azhar et al. [2], while the CNN model \nwith IndoBERT embedding was the best performing method in Khasanah and Krisnadhi [11]. For our second \nmodel, we experimented with some versions of IndoBERT, such as: IndoBERT (lite), IndoBERT (base), and \nIndoBERT (large) from Willie et al. [20]; and IndoBERT and IndoBERTweet fr om Koto et al. [21], [27]. \nFurther, a comparison with some multilingual BERT, such as m-BERT, distilBERT, and XLM-RoBERTa, was \nalso performed. For a summary of the configurations of each BERT model, we have detailed them in Table 2.  \nThe model architecture was created using Python 3.7 and the model was trained using a NVidia Tesla \nT4 with single core. For the hyperparameter of the CNN, we follow the single CNN architecture and settings \nfrom Khasanah and Krisnadhi [11], such as using Adam optimizer, batch size of 64, the dimension of the input \n512, dropout rate of 0.5, max epoch of 70, learning rate of 0.001 and kernel size of 2. For the machine learning \ntop classifier, we used random forest, XGBoost and naÃ¯ve Bayes with the CC strategy. For the hyperparameter \nsettings of BERT model, we followed Devlin et al. [14] recommendations, such as using dropout probability \nrate of 0.1, learning rate of 2e -5 and use Adam optimizer. We set the train and valid batch size of 32, and \nmaximum input length of 128. For the baseline methods using Word2Vec embedding, the training parameters \nare a window size of 5 and vector size of 512. \n \n \n3. RESULTS AND DISCUSSION \n3.1.  The results of our first model: IndoBERT-CNN-XGBoost \nBased on Table 3, the results show that deep learning model CNN is more effective than machine \nlearning models, i.e., random forest, naive Bayes and XGBoost, for aspect classification task. Further \ncombining deep learning model CNN with machine learning me thods can increase the effectiveness of the \nmodel. It appears from the results of CNN-random forest, CNN-naive Bayes, and CNN-XGBoost models that \noutperform the results of CNN model. This finding is consistent with the one reported in [2]. It indicates that \n      ï²          ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 13, No. 5, October 2023: 5641-5652 \n5648 \nthe use of CNN as feature extraction method results in more refined features that enable the machine learning \nclassifier to classify the aspects from a review text more accurately. \nAmong all models, CNN -XGBoost models consistently gain the best results in terms of micro  \nF1-score, hamming loss, and accuracy in each type of text embedding. Our model using IndoBERT as te xt \nembedding for CNN -XGBoost model is shown to significantly outperform the CNN -XGBoost model of  \nAzhar et al. [2] that uses Word2Vec embedding, by achieving the micro F1-Score of 0.8992, hamming loss of \n0.0404 and accuracy of 0.7228. This finding shows the effectiveness of our first model in using IndoBERT as \ntext representation in CNN-XGBoost model. These results are also consistent with the IndoBERT in IndoNLU \nbenchmark results obtained by Wilie et al. [20], in which one of their findings is the contextualized pretrained \nmodels significantly outperformed the static word-embedding-based models, the advantage of contextualized \nword embeddings over static word embeddings is illustrated by this [20]. Note that the scores of Word2Vec -\nCNN-XGBoost models presented in the original paper of Azhar et al. [2] are slightly different from those \ndisplayed in our table because they used a different data split from ours (here, we utilize the distribution of the \nAiry Room dataset split provided by IndoNLU [20]). However, we have ensured to foll ow similar \nhyperparameter settings used by Azhar et al.  [2] to generate the results of all Word2Vec -CNN-\nmachine_learning variations in our table. \n \n \nTable 3. The comparison results of using text embedding from Word2Vec and IndoBERT \nEmbedding Model Micro F1-score Hamming loss Accuracy \n Random forest 0.4689 0.1589 0.2070 \n NaÃ¯ve Bayes 0.4374 0.3782 0.0561 \n XGBoost 0.5390 0.1494 0.2456 \nWord2Vec CNN 0.6258 0.1087 0.3322 \n CNN-random forest 0.8675 0.0512 0.6316 \n CNN-naÃ¯ve Bayes 0.7273 0.1147 0.3193 \n CNN-XGBoost (Azhar et al. [2]) 0.8743 0.0491 0.6386 \n Random forest 0.3938 0.1642 0.1614 \n NaÃ¯ve Bayes 0.5020 0.3063 0.0737 \n XGBoost 0.5987 0.1298 0.2877 \nIndoBERTW CNN (Khasanah and Krisnadhi [11]) 0.6244 0.1122 0.3508 \n CNN-random forest 0.8877 0.0442 0.6982 \n CNN-naÃ¯ve Bayes 0.7797 0.0807 0.5263 \n CNN-XGBoost (ours) 0.8992 0.0404 0.7228 \n \n \n3.2.  The results of our second model: end-to-end IndoBERT \nWe used five types of monolingual models, IndoBERT, to do the multi -label text classification. The \nmodels are IndoBERT (base, lite, and large) from Wilie et al. [20], IndoBERT (base) [21] and IndoBERTweet \nfrom Koto et al. [27]. Table 4 presents the findings of these models. \n \n \nTable 4. The comparison results of using IndoBERT model \nEmbedding Model Micro F1-score Hamming loss Accuracy \n IndoBERTW-base 0.92700 0.02999 0.76098 \n IndoBERTW-lite 0.86334 0.05306 0.61357 \nIndoBERT IndoBERTW-large 0.92828 0.02953 0.76112 \n IndoBERTK-base 0.91796 0.03345 0.73947 \n IndoBERTweetK-base 0.92123 0.03207 0.74680 \n \n \nAmong all monolingual models, IndoBERT , used in this experiment, the IndoBERT -large model by \nWilie et al.  [20], achieves the best value of micro F1 -score 0.92828, hamming loss 0.02953 and accuracy \n0.76112. This can be explained because IndoBERT-large has a much bigger number of parameters in its neural \nnetwork architecture, which is almost three times bigger than the number of parameters for IndoBERT -base, \nIndoBERTK and IndoBERTweetK Table 2. This makes IndoBERT -large is mor e accurate in capturing the \nstructure and semantics of the data. This result shows an improvement in accuracy to the baseline Word2Vec -\nCNN-XGBoost model by up to 19.19%. \nBesides that, we also conclude that in general, the performance of our second models u sing end-to \nend IndoBERT model are significantly better than the performance results of our first model presented in \nsection 3.1. earlier. This indicates that using IndoBERT as an end -to-end model by fine-tuning the initial pre-\ntrained model with our specific task is more effective than using it as text embedding only. This result confirms \nthe effectiveness of IndoBERT to directly solve various text processing tasks [21]. \nInt J Elec & Comp Eng  ISSN: 2088-8708 ï² \n \n Multi-label text classification of Indonesian customer reviews using bidirectional â€¦ (Nuzulul Khairu Nissa) \n5649 \nBeside using the monolingual BERT model, we also con duct a comparative experiment using \nmultilingual BERT models, the results are presented in Table 5. The common multilingual models, m -BERT \n[14], distil-BERT [25], and XLM-RoBERTa [26], were used. \nBased on the findings in Table 5, the m-BERT outperforms the other two multilingual pre -trained \nlanguage models, with micro F1 -score of 0.90901, hamming loss of 0.03705, and accuracy of 0.71768.  \nWe might infer from the findings of Tables 4 and 5, that the results of monolingual BERT are still more effective \nthan the multilingual BERT. This is due to the monolingual model is trained using a single language only, so that \nthe model can be more focused and accurate in learning the characteristic of the language on the training data. \nHowever, in oth er languages when the monolingual BERT model is not available, we argue that using the \nmultilingual BERT model is suggested, especially m-BERT since its results are still more effective compared to \nthe results of machine learning or deep learning models CNN using Word2Vec displayed in Table 3. \n \n \nTable 5. The results of multilingual BERT model \nEmbedding Model Micro F1-score Hamming loss Accuracy \n m-BERT-base 0.90901 0.03705 0.71768 \nBERT distil-BERT-base 0.87896 0.04817 0.64533 \n XLM-RoBERTa-base 0.77074 0.08357 0.47192 \n \n \n3.3.  Qualitative analysis \nBased on the findings of the model evaluation that was done in the sections before, the IndoBERT -\nlarge model gives the best performance results. Therefore, we carried out the qualitative analysis using the \nevaluation results of that model. The qualitative analysis offers methods for assessing, analyzing, and \ninterpreting the significant patterns in the data.  \nFirst, we analyze the results from the IndoBERT -large model using a confusion matrix. The multi -\nlabel text classification model was tested using 286 data and the analysis was carried out for each aspect.  \nFigure 5 displays the confusion matrix's results. The confusion matrix, which compares the predicted aspect \nvalues with the actual aspect values, i s used to determine how effective the classification model is. The value \nis represented by TN, TP, FN and FP. \n \n \n \n \nFigure 5. Multi-label confusion matrix of IndoBERT-large \n \n \nBased on the confusion matrix in Figure 5, it can be identified that the TP and TN values are generally \nbigger than the FP and FN values, and it can be seen from the color difference in the confusion matrix, where \nthe TP and TN values generally have lighter colors when compared to FP and FN. Based on the confusion \nmatrix in Figure 5, we can also obtain the accuracy and micro F1 score values for each aspect, which are shown \nin Table 6. \nBased on Table 6, it can be identified that the â€˜Wi-Fiâ€™ is the most accurately predicted aspect with \naccuracy of 0.9964 and micro F1 -score of 0.9875. It can be happened because when a review discusses the  \nâ€˜Wi-Fiâ€™ aspect, customers will tend to explicitly mention the word â€˜Wi-Fiâ€™. So as a result, the model will better \n\n      ï²          ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 13, No. 5, October 2023: 5641-5652 \n5650 \nunderstand and more accurately predict that aspect. For example: â€œhotelnya bagus, makanan cukup, Wi-Fi \nkurang merataâ€ (â€œthe hotel is good, the food is enough, the Wi-Fi is unevenâ€). In that review, the â€˜Wi-Fiâ€™ \naspect is mentioned explicitly. Furthermore, the most inaccurate aspect is â€˜cleanlinessâ€™, with accuracy of \n0.9257 and micro F1 -score of 0.9181. Because in some cases, the reviews that discuss the aspect of \nâ€˜cleanlinessâ€™, do not explicitly mentioned the word â€˜cleanlinessâ€™ or â€˜kebersihanâ€™ and sometimes the customers \nassociate it with other aspects or elements. For example: â€œbanyak nyamuk, selebihnya oke2 ajaâ€  (â€œlots of \nmosquitoes, the rest is quite goodâ€). In that review, the â€˜cleanlinessâ€™ aspect is not explicitly mentioned, but the \nreview associates the aspect of â€˜cleanlinessâ€™ with the condition of too many mosquitoes. \nFrom Table 6, we can also see that in some cases, the IndoBERT-large model still could not correctly \npredict all aspects in a review completely. There are some reviews in wh ich the model can only predict some \nof the aspects correctly. Based on our further analysis, the prediction results have 226 reviews that are correctly \nclassified and 60 reviews that are not correctly classified in complete. The misclassified results are c aused by \nthe system that fails to understand the meaning of the review. Table 7 shows two examples of reviews whose \naspects could not be correctly classified in complete. In test data (1), the model can predict correctly two out \nof three aspects contained in the review. In test data (2), the model cannot predict the only one aspect contained \nin the review. \nIn test data (1), we found that the aspect â€˜kebersihanâ€™ (cleanliness) is not identified in the prediction \nresults, which can be happened because the mode l misunderstands the data. The model cannot identify that \nword â€˜kutuâ€™ (bedbugs) is related to the aspect â€˜kebersihanâ€™ (cleanliness). Next, in test data (2), the aspect \ncategory â€˜kebersihanâ€™ (cleanliness) also cannot be detected in the prediction results. W e analyzed that this is \ncaused by the typo word â€œkabersihanâ€ which should be written â€œkebersihanâ€. This makes the model could \nnot capture the meaning of the review well. \n \n \nTable 6. The result of IndoBERT-large for each aspect \nAspects (Eng) Aspects (Indo) TP TN FP FN Accuracy Micro F1-score \nAC AC 5794 22400 106 200 0.9893 0.9743 \nHot water Air panas 5372 22950 128 50 0.9938 0.9837 \nSmell Bau 3576 23929 224 771 0.9651 0.8779 \nGeneral General 2631 24679 569 621 0.9582 0.8156 \nCleanliness Kebersihan 11860 14523 940 1177 0.9257 0.9181 \nLinen Linen 8753 17961 747 1039 0.9373 0.9074 \nService Service 7097 20217 303 883 0.9584 0.9229 \nSunrise meal Sunrise meal 2212 25913 188 187 0.9868 0.9219 \nTV TV 3089 25230 11 170 0.9936 0.9715 \nWi-Fi Wi-Fi 4096 24300 4 100 0.9964 0.9875 \n \n \nTable 7. The example of misclassified aspect labels \nTest data (1) â€œLabelâ€ AC \n(AC) \nHot water \n(Air panas) \nSmell \n(Bau) \nGeneral \n(General) \nCleanliness \n(Kebersihan) \nLinen \n(Linen) \nService \n(Service) \nSunrise meal \n(Sunrise meal) \nTV \n(TV) \nWi-Fi \n(Wi-Fi) \nKasur ada kutu nya, \ndan badan saya jadi \ngatal gatal. dan AC \nsama sekali tdk dingin \nActual 1 0 0 0 1 1 0 0 0 0 \n(The mattress has \nbedbugs, and my body \nitches... And the AC is \nnot cold at all) \nPrediction 1 0 0 0 0 1 0 0 0 0 \nTest data (2) â€œLabelâ€ AC \n(AC) \nHot water \n(Air panas) \nSmell \n(Bau) \nGeneral \n(General) \nCleanliness \n(Kebersihan) \nLinen \n(Linen) \nService \n(Service) \nSunrise meal \n(Sunrise meal) \nTV \n(TV) \nWi-Fi \n(Wi-Fi) \nlumayan untuk harga \nsegitu... kabersihan \ntolong \nditingkatkan \nActual 0 0 0 0 1 0 0 0 0 0 \n(Not bad for that \nprice... \ncleanliness please \nimprove) \nPrediction 0 0 0 0 0 0 0 0 0 0 \n \n \n4. CONCLUSION \nIn this study, we proposed two strategies using monolingual pre -trained language model BERT on \nIndonesian language (i.e., IndoBERT) for identifying aspects in the customer review dataset, by performing \nmulti-label text classification. First, we used IndoBE RT as text embedding for CNN -XGBoost classifier. \nInt J Elec & Comp Eng  ISSN: 2088-8708 ï² \n \n Multi-label text classification of Indonesian customer reviews using bidirectional â€¦ (Nuzulul Khairu Nissa) \n5651 \nSecond, we used the IndoBERT as text embedding as well as the classifier in an end-to-end model. Moreover, \nas part of an in-depth examination of this study, a multilingual BERT model was also exploited. Acco rding to \nthe results of our studies, our proposed strategies significantly outperform some of the state -of-the-art \nbaselines. The use of IndoBERT as embedding for the CNN -XGBoost model give some improvement over \nsome machine learning and deep learning models, with micro F1-Score of 0.8992, hamming loss of 0.0404 and \naccuracy of 0.7228. IndoBERT as contextualized pre -trained models can give better text representation when \ncompared to the context -independent word -embedding model like Word2Vec. Next, the use o f IndoBERT \nmodels as embedding as well as classifier to solve multi -label text classification can further significantly \nenhance our first model which uses IndoBERT for text embedding only. The IndoBERT -large outperformed \nthe other IndoBERT models, accordin g to the results, with micro F1 -Score of 0.92828, hamming loss of \n0.02953 and accuracy of 0.76112. It has been demonstrated that this approach may improve the accuracy of a \nWord2Vec-CNN-XGBoost baseline by up to 19.19%. When we compared IndoBERT with the m ultilingual \nBERT (m-BERT, distil-BERT and XLM -RoBERTa), we found that the monolingual BERT is slightly more \naccurate than multilingual BERT. Here, the best-performing monolingual BERT model (i.e., IndoBERT-large) \ngains 6% higher accuracy compared to the be st-performing multilingual BERT model (i.e. , m-BERT-base). \nSome suggestions that can be conducted for future work, include: the use of another alternative architectures \nsuch as recurrent neural networks (RNNs) or other transformer -based architectures. Othe r than that, the \nimbalanced label dataset can be handled using a good and complex synthetic oversampling technique.  \n \n \nACKNOWLEDGEMENTS \nThis research was funded by the Directorate of Research and Development, Universitas Indonesia, \nunder Hibah PUTI Pascasarjana 2022 (Grant No. NKB-103/UN2.RST/HKP.05.00/2022). \n \n \nREFERENCES \n[1] M. Pontiki, D. Galanis, J. Pavlopoulos, H. Papageorgiou, I. Androutsopoulos, and S. Manandhar, â€œSemEval -2014 task 4: Aspect \nbased sentiment analysis,â€ in Proceedings of the 8 th International Workshop on Sema ntic Evaluation (SemEval 2014) , 2014,  \npp. 27â€“35, doi: 10.3115/v1/S14-2004. \n[2] A. N. Azhar, M. L. Khodra, and A. P. Sutiono, â€œMulti-label aspect categorization with convolutional neural networks and extreme \ngradient boosting,â€ in 2019 International Conference on Electrical Engineering and Informatics (ICEEI), Jul. 2019, pp. 35â€“40, doi: \n10.1109/ICEEI47359.2019.8988898. \n[3] E. Deniz, H. Erbay, and M. CoÅŸar, â€œMulti -label classification of E -commerce customer reviews via machine learning,â€ Axioms,  \nvol. 11, no. 9, Aug. 2022, doi: 10.3390/axioms11090436. \n[4] J. Du, Q. Chen, Y. Peng, Y. Xiang, C. Tao, and Z. Lu, â€œML -Net: multi-label classification of biomedical texts with deep neural \nnetworks,â€ Journal of the American Medical Informatics Association , vol. 26, no.  11, pp. 1279 â€“1285, Nov. 2019,  \ndoi: 10.1093/jamia/ocz085. \n[5] A. D. Asti, I. Budi, and M. O. Ibrohim, â€œMulti -label classification for hate speech and abusive language in Indonesian -local \nlanguages,â€ in 2021 International Conference on Advanced Computer Sc ience and Information Systems (ICACSIS) , Oct. 2021,  \n pp. 1â€“6, doi: 10.1109/ICACSIS53237.2021.9631316. \n[6] R. Hendrawan, Adiwijaya, and S. Al Faraby, â€œMultilabel classification of Hate speech and abusive words on Indonesian Twitter \nsocial media,â€ in 2020 In ternational Conference on Data Science and Its Applications (ICoDSA) , Aug. 2020, pp. 1 â€“7,  \ndoi: 10.1109/ICoDSA50139.2020.9212962. \n[7] E. MontaÃ±es, R. Senge, J. Barranquero, J. R . Quevedo, J. JosÃ© del Coz, and E. HÃ¼llermeier, â€œDependent binary relevance \nmodels for multi -label classification,â€ Pattern Recognition , vol. 47, no. 3, pp. 1494 â€“1508, 2014, doi: \n10.1016/j.patcog.2013.09.029.  \n[8] J. Read, B. Pfahringer, G. Holmes, and E. Frank, â€œClassifier chains for multi -label classification,â€ Machine Learning, vol. 85,  \nno. 3, pp. 333â€“359, Dec. 2011, doi: 10.1007/s10994-011-5256-5. \n[9] R. Y. Rumagit, â€œMultilabel classification for toxic comments in Indonesian,â€ Engineering, MAthematics and Computer Science \n(EMACS) Journal, vol. 2, no. 1, pp. 29â€“34, Jan. 2020, doi: 10.21512/emacsjournal.v2i1.6256. \n[10] R. A. Ilma, S. Hadi, and A. Helen, â€œTwitterâ€™s hate speech multi -label classification using bidirectional long short -term memory \n(BiLSTM) method,â€ in 2021 International Conference on Artificial Intelligence and Big  Data Analytics, Oct. 2021, pp. 93 â€“99,  \ndoi: 10.1109/ICAIBDA53487.2021.9689767. \n[11] I. N. Khasanah and A. A. Krisnadhi, â€œExtreme multilabel text classification on Indonesian tax court ruling using single chann el \nCNN and IndoBERT embedding,â€ in 2021 6th International Workshop on Big Data and Information Security (IWBIS), Oct. 2021, \npp. 59â€“66, doi: 10.1109/IWBIS53353.2021.9631855. \n[12] S. R. Anggraeni, N. A. Ranggianto, I. Ghozali, C. Fatichah, and D. Purwitasari, â€œDeep learning approaches for multi-label incidents \nclassification from Twitter textual information,â€ Journal of Information Systems Engineering and Business Intelligence , vol. 8,  \nno. 1, pp. 31â€“41, Apr. 2022, doi: 10.20473/jisebi.8.1.31-41. \n[13] G. A. Neruda and E. Winarko, â€œTraffic event detection from Twitter using a combination of CNN and BERT,â€ in 2021 International \nConference on Advanced Computer Science and Information Systems (ICACSIS) , Oct. 2021, pp. 1 â€“7,  \ndoi: 10.1109/ICACSIS53237.2021.9631334. \n[14] J. Devlin, M. -W. Chang, K. Lee, and K. Toutanova, â€œBERT: Pre -training of deep bidirectional transformers for language \nunderstanding,â€ in Proceedings of the 2019 Conference of the North, 2019, pp. 4171â€“4186, doi: 10.18653/v1/N19-1423. \n[15] L. Cai, Y. Song, T. Liu, and K. Zhang, â€œA hybrid BERT model that incorporates label semantics via adjustive attention for multi -\nlabel text classification,â€ IEEE Access, vol. 8, pp. 152183â€“152192, 2020, doi: 10.1109/ACCESS.2020.3017382. \n[16] L. F. Simanjuntak, R. Mahendra, and E. Yulianti, â€œWe know you are living in Bali: location prediction of Twitter users using BERT \nlanguage model,â€ Big Data and Cognitive Computing, vol. 6, no. 3, Jul. 2022, doi: 10.3390/bdcc6030077. \n      ï²          ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 13, No. 5, October 2023: 5641-5652 \n5652 \n[17] J. A. Alzubi, R. Jain, A. Singh, P. Parwekar, and M. Gupta, â€œCOBERT: COVID -19 question answering system using BERT,â€ \nArabian Journal for Science and Engineering, pp. 1â€“11, Jun. 2021, doi: 10.1007/s13369-021-05810-5. \n[18] N. Liu, Q. Hu, H. Xu, X. Xu, and M. Chen, â€œMed-BERT: A pretraining framework for medical records named entity recognition,â€ \nIEEE Transactions on Industrial Informatics, vol. 18, no. 8, pp. 5600â€“5608, Aug. 2022, doi: 10.1109/TII.2021.3131180. \n[19] J. Dong, F. He, Y. Guo, and H. Zhang, â€œA commodity review sentiment analysis based on BERT-CNN model,â€ in 5th International \nConference on Computer and Communication Systems (ICCCS), 2020, pp. 143â€“147, doi: 10.1109/ICCCS49078.2020.9118434. \n[20] B. Wilie et al., â€œIndoNLU: benchmark and resources for evaluating Indonesian natural language understanding,â€ in Proceedings of \nthe 1st Conference of the Asia -Pacific Chapter of the Association for Computational Linguistics and the 10 th International Joint \nConference on Natural Language Processing, 2020, pp. 843â€“857. \n[21] F. Koto, A. Rahimi, J. H. Lau, and T. Baldwin, â€œIndoLEM and IndoBERT: A benchmark dataset and pre-trained language model \nfor Indonesian NLP,â€ in Proceedings of the 28 th International Conference on Computational Linguistics , 2020, pp. 757 â€“770,  \ndoi: 10.18653/v1/2020.coling-main.66. \n[22] E. Yulianti, A. Kurnia, M. Adriani, and Y. S. Duto, â€œNormalisation of Indonesian-english code-mixed text and its effect on emotion \nclassification,â€ International Journal of Advanced Computer Science and Applications , vol. 12, no. 11, 2021,  \ndoi: 10.14569/IJACSA.2021.0121177. \n[23] G. B. Larrain, N. Rojas-Morales, P. D. O. Jeneral, and N. G. Rogel, â€œTowards a classifier ensamble to prevent burnout syndrome \non University students,â€ in 2022 41 st International Conference of the Chilean Computer Science Society (SCCC) , Nov. 2022,  \npp. 1â€“8, doi: 10.1109/SCCC57464.2022.10000313. \n[24] A. H. Ombabi, O. Lazzez, W. Ouarda, and A. M. Alimi, â€œDeep learning framework based on Word2Vec and CNN for users interests \nclassification,â€ in 2017 Sudan Conference on Computer Science and Information Technology (SCCSIT) , Nov. 2017, pp. 1 â€“7,  \ndoi: 10.1109/SCCSIT.2017.8293054. \n[25] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, â€œDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,â€  \nComputing Research Repository (CoRR), 2019. \n[26] A. Conneau et al., â€œUnsupervised cross-lingual representation learning at scale,â€ in Proceedings of the 58th Annual Meeting of the \nAssociation for Computational Linguistics, 2020, pp. 8440â€“8451, doi: 10.18653/v1/2020.acl-main.747. \n[27] F. Koto, J. H. Lau, and T. Baldwin, â€œIndoBERTweet: A pretrained language model for Indonesian Twitter with effective domain-\nspecific vocabulary Initialization,â€ in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, \n2021, pp. 10660â€“10668, doi: 10.18653/v1/2021.emnlp-main.833. \n[28] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT Press, 2016. \n[29] T. Chen and C. Guestrin, â€œXGBoost: A scalable tree boosting system,â€ in Proceedings of the 22 nd ACM SIGKDD International \nConference on Knowledge Discovery and Data Mining, Aug. 2016, pp. 785â€“794, doi: 10.1145/2939672.2939785. \n[30] E. Shang, X. Liu, H. Wang, Y. Rong, and Y. Liu, â€œResearch on the application of artificial intelligence and distributed paral lel \ncomputing in arch ives classification,â€ in 2019 IEEE 4 th Advanced Information Technology, Electronic and Automation Control \nConference (IAEAC), Dec. 2019, pp. 1267â€“1271, doi: 10.1109/IAEAC47372.2019.8997992. \n[31] M.-L. Zhang and Z. -H. Zhou, â€œA review on multi -label learning  algorithms,â€ IEEE Transactions on Knowledge and Data \nEngineering, vol. 26, no. 8, pp. 1819â€“1837, Aug. 2014, doi: 10.1109/TKDE.2013.39. \n[32] H. Fallah, P. Bellot, E. Bruno, and E. Murisasco, â€œAdapting transformers for multi -label text classification,â€ in CIRCLE (Joint \nConference of the Information Retrieval Communities in Europe), 2022, pp. 1â€“18. \n[33] M. A. Abdurrazzaq, G. A. P. Saptawati, and Y. Rusmawati, â€œMAGNET architecture optimization on multi-label text classification,â€ \nin 2021 8th International Conf erence on Advanced Informatics: Concepts, Theory and Applications (ICAICTA) , Sep. 2021,  \npp. 1â€“6, doi: 10.1109/ICAICTA53211.2021.9640263. \n \n \nBIOGRAPHIES OF AUTHORS  \n \n \nNuzulul Khairu Nissa     received B.Math. degree from Diponegoro  University in \n2019. She is currently pursuing a Master of Computer Science in University of Indonesia. Her \nresearch interests are related to machine learning and natural language processing. She can be \ncontacted at email: nuzulul.khairu@ui.ac.id. \n  \n \nEvi Yulianti     is a lecturer and researcher at Faculty of Computer Science, Universitas \nIndonesia. She received the B.Comp.Sc. degree from the Universitas Indonesia in 2010, the dual \nM.Comp.Sc. degree from Universitas Indonesia and Royal Melbourne Inst itute of Technology \nUniversity in 2013, and the Ph.D. degree from Royal Melbourne Institute of Technology \nUniversity in 2018. Her research interests include information retrieval and natural language \nprocessing. She can be contacted at email: evi.y@cs.ui.ac.id. \n \n \n"
}