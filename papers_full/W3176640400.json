{
  "title": "TILGAN: Transformer-based Implicit Latent GAN for Diverse and Coherent Text Generation",
  "url": "https://openalex.org/W3176640400",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2783116390",
      "name": "Shizhe Diao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154286075",
      "name": "Xinwei Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3173367801",
      "name": "Kashun Shum",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103229335",
      "name": "Yan Song",
      "affiliations": [
        "Shenzhen Research Institute of Big Data"
      ]
    },
    {
      "id": "https://openalex.org/A1973384194",
      "name": "Tong Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963746531",
    "https://openalex.org/W2951824008",
    "https://openalex.org/W3099388488",
    "https://openalex.org/W2964669873",
    "https://openalex.org/W4294149591",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2963620441",
    "https://openalex.org/W4293568373",
    "https://openalex.org/W2963131369",
    "https://openalex.org/W2964017345",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2616969219",
    "https://openalex.org/W4294294142",
    "https://openalex.org/W2115613106",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2891641674",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2784823820",
    "https://openalex.org/W2963574252",
    "https://openalex.org/W2565378226",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2785896739",
    "https://openalex.org/W2542835211",
    "https://openalex.org/W2188365844",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W3006949012",
    "https://openalex.org/W2933374552",
    "https://openalex.org/W2962846791",
    "https://openalex.org/W2970682219",
    "https://openalex.org/W2995404354",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2581637843",
    "https://openalex.org/W648786980",
    "https://openalex.org/W3034533785",
    "https://openalex.org/W2963456134",
    "https://openalex.org/W2593383075",
    "https://openalex.org/W2962826786",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2963248348",
    "https://openalex.org/W2625357353",
    "https://openalex.org/W4287817357",
    "https://openalex.org/W2739748921",
    "https://openalex.org/W4289543161",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2996286887",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2889002152",
    "https://openalex.org/W4288246040",
    "https://openalex.org/W3023581195",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2946385697",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Conventional autoregressive models have achieved great success in text generation but suffer from the exposure bias problem in that token sequences in the training and in the generation stages are mismatched. While generative adversarial networks (GANs) can remedy this problem, existing implementations of GANs directly on discrete outputs tend to be unstable and lack diversity. In this work, we propose TILGAN, a Transformer-based Implicit Latent GAN, which combines a Transformer autoencoder and GAN in the latent space with a novel design and distribution matching based on the Kullback-Leibler (KL) divergence. Specifically, to improve local and global coherence, we explicitly introduce a multi-scale discriminator to capture the semantic information at varying scales among the sequence of hidden representations encoded by Transformer. Moreover, the decoder is enhanced by an additional KL loss to be consistent with the latent-generator. Experimental results on three benchmark datasets demonstrate the validity and effectiveness of our model, by obtaining significant improvements and a better quality-diversity trade-off in automatic and human evaluation for both unconditional and conditional generation tasks. © 2021 Association for Computational Linguistics",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4844–4858\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4844\nTILGAN: Transformer-based Implicit Latent GAN\nfor Diverse and Coherent Text Generation\nShizhe Diao♦∗, Xinwei Shen ♦∗, KaShun Shum ♦, Yan Song ♠♥, Tong Zhang ♦\n♦The Hong Kong University of Science and Technology\n{sdiaoaa, xshenal, ksshumab, tongzhang}@ust.hk\n♠The Chinese University of Hong Kong (Shenzhen)\n♥Shenzhen Research Institute of Big Data\nsongyan@cuhk.edu.cn\nAbstract\nConventional autoregressive models have\nachieved great success in text generation but\nsuffer from the exposure bias problem in that\ntoken sequences in the training and in the gen-\neration stages are mismatched. While gener-\native adversarial networks (GANs) can rem-\nedy this problem, existing implementations\nof GANs directly on discrete outputs tend\nto be unstable and lack diversity. In this\nwork, we propose TILGAN , a Transformer-\nbased Implicit Latent GAN, which combines\na Transformer autoencoder and GAN in the la-\ntent space with a novel design and distribution\nmatching based on the Kullback-Leibler (KL)\ndivergence. Speciﬁcally, to improve local and\nglobal coherence, we explicitly introduce a\nmulti-scale discriminator to capture the seman-\ntic information at varying scales among the\nsequence of hidden representations encoded\nby Transformer. Moreover, the decoder is en-\nhanced by an additional KL loss to be consis-\ntent with the latent-generator. Experimental re-\nsults on three benchmark datasets demonstrate\nthe validity and effectiveness of our model, by\nobtaining signiﬁcant improvements and a bet-\nter quality-diversity trade-off in automatic and\nhuman evaluation for both unconditional and\nconditional generation tasks.1\n1 Introduction\nIn recent years, Transformer-based autoregres-\nsive (AR) models have made a dramatic impact\nin text generation tasks such as machine transla-\ntion (Vaswani et al., 2017; Wang et al., 2019) and\ndialogue systems (Le et al., 2019; Ham et al., 2020),\nespecially with the emergence of large pre-trained\nlanguage models (Radford et al., 2019; Brown et al.,\n2020; Wu et al., 2020). However, AR models pre-\ndict the next token conditioned on the ground truth\n*Equal Contribution.\n1Our code is available at https://github.com/\nshizhediao/TILGAN.\nduring training and on its own previously generated\ntoken during inference, which leads to a mismatch\nbetween training and generation stages, and this\ncauses low quality of generated texts and bad gen-\neralization ability of models on unseen data (Wise-\nman and Rush, 2016; Welleck et al., 2020).\nGenerative adversarial networks (GANs, Good-\nfellow et al., 2014) provide a promising approach\nto solve the exposure bias problem (Yu et al., 2017;\nKusner and Hernández-Lobato, 2016; Zhang et al.,\n2017). This is because GANs aim at matching the\ndistributions of the generated and real data instead\nof forcing the model output to align with the single\ncorrect sequence, and thus provide the potential\nto bypass the discrepancy issue. However, it is\nnon-trivial to apply GANs to discrete data since\nthe gradients cannot be normally back-propagated\nthrough discrete tokens. Existing approaches have\nimplemented the adversarial discrete generation\ntraining by reinforcement learning (RL) (Yu et al.,\n2017; Lin et al., 2017; Guo et al., 2018; Fedus\net al., 2018) and Gumbel-Softmax (Kusner and\nHernández-Lobato, 2016). Nevertheless, these ap-\nproaches suffer from the high variance problem\nwhich causes unstable performance and slow con-\nvergence, leading to other methods based on fea-\nture matching (Zhang et al., 2017; Zhao et al., 2018;\nChen et al., 2018).\nIn this work, we propose TILGAN , a\nTransformer-based Implicit Latent GAN, which\ncombines a Transformer autoencoder and a GAN\nin the latent space with novel designs and a learning\nformulation based on the Kullback-Leibler (KL)\ndivergence to enhance the text generation perfor-\nmance in both ﬁdelity and diversity. Speciﬁcally,\ninspired by the representation capacity of Trans-\nformer AR models, we ﬁrstly incorporate Trans-\nformer architectures to improve GANs in text gen-\neration. Note that the previous latent feature match-\ning methods are mostly RNN-based and assume\n4845\na single vector in the latent space, which do not\ndirectly handle a sequence of latent representations\nencoded by a Transformer. However, single latent\nvector representation hinders the incorporation of\ncorrelations among different tokens, leading to the\nloss of crucial semantic information captured by\na Transformer structure. This is especially prob-\nlematic for local and global coherence (Bi´nkowski\net al., 2020). In this paper, we directly match the\ndistributions of multi-token sequences in the latent\nspace, which is better suited for the Transformer\nstructure. To do so, we have to resolve two chal-\nlenges, the ﬁrst being how to do distribution match-\ning. We introduce a multi-scale discriminator over\nthe Transformer latent space to utilize the seman-\ntic information on different scales, where a global\ndiscriminator takes the entire sequence of latent rep-\nresentations as the input, and a local discriminator\ntakes only a randomly-sampled local neighborhood.\nThe second challenge is how to train the decoder\nreliably. We enhance an autoencoder loss by an-\nother KL loss optimized by GAN, forcing the latent\nrepresentations of the decoding output to be com-\npatible with the generated latent representations\nfrom the latent-generator.\nWe provide a theoretical justiﬁcation for the pro-\nposed formulation by connecting it to the standard\ngoal of generative modeling. Experimental results\non three datasets illustrate that TILGAN outper-\nforms all baselines in both unconditional and con-\nditional generation tasks, achieving state-of-the-art\nperformance. Particularly, TILGAN exhibits a\nbetter quality-diversity trade-off evaluated by au-\ntomatic metrics such as SelfBLEU and TestBLEU\nas well as human evaluation. Further analyses also\nconﬁrm the effectiveness of each component of\nour method, where decoder enhancement greatly\nbeneﬁts generation quality, while the multi-scale\ndiscriminator and KL objective provide great per-\nformance gains in generation diversity, and the im-\nplicit prior contributes to both.\n2 The Approach\n2.1 Model and Formulation\nIn this section, we introduce the proposed model\nand the learning formulation. Let x ∈X denote a\nsentence following the real data distribution pr(x)\nwith X= Vn where Vis the vocabulary, m= |V|\nis the vocabulary size, andnis the sequence length,\nand z ∈ Zbe the latent variable following a\nprior distribution pz(z). We consider a probabilis-\ntic model containing an encoder Eφ : X → Z\nand a decoder Gθ : Z → X. Both are gener-\nally stochastic mappings with parameters φ and\nθ, and induce the encoder conditional distribution\nqφ(z|x) and the decoder conditional pθ(x|z) re-\nspectively. Note that previous approaches to text\ngeneration use deterministic encoders and decoders\n(Zhao et al., 2018), which restricts the expressive-\nness of the modeled distribution family. We ﬁrst\nensure the consistency between Eφ and Gθ by min-\nimizing the negation of the expected reconstruction\nlog-likelihood\nLc(φ,θ) = −Ex∼prEz∼qφ(z|x)[ln pθ(x|z)], (1)\nwhich coincides with the reconstruction term in the\nevidence lower bound (ELBO).\nThe generated data distribution is given by\npG(x) = Ez∼pz[pθ(x|z)]. To achieve good gen-\neration performance, we design the model so that\nthe distribution family of pG(x) is large enough to\ncontain the real one pr(x). As described in Sec-\ntion 2.3, we use Transformer to model E and G,\nwhich we assume to have sufﬁcient capacity to\nreconstruct data well and learn informative latent\nrepresentations. In this way, pθ(x|z) is assumed\nto be expressive enough. To further enhance the\ncapacity of pG(x), we propose to use an implicit\nprior pz, by transforming samples from a simple\ndistribution with a deep neural network.\nConsider a random vector ϵ ∈ Efollowing\nsome simple distribution pϵ like a standard Gaus-\nsian. We then propose to learn a latent-generator\ngβ : E→Z with parameter βso that the distribu-\ntion of gβ(ϵ) matches that ofEφ(x), by minimizing\nthe KL divergence\nLg(φ,β) = DKL(qφ(z)∥pβ(z)),\nwhere pβ(z) denotes the distribution of gβ(ϵ) and\nqφ(z) = Ex∼pr[qφ(z|x)] is the distribution of\nE(x), a.k.a., the aggregated posterior. The advan-\ntage of KL divergence is that it imposes a heavy\npenalty when qφ(z) > 0 but pβ(z) ≈0, which\nmeans that it favors a gthat covers all the diverse\nmodes of qφ(z). This is commonly known and\nveriﬁed empirically in Shen et al. (2020). Hence\nminimizing KL encourages a better diversity in\ngeneration compared with the Jensen–Shannon (JS)\ndivergence or Wasserstein distance which are often\nused in the literature on generative models.\nTherefore, we formulate the overall objective\n4846\n…xi-2xi-1xi xi+1 …xi+2inputTransformerEncoder (Latent GeneratorTransformerDecoder#$ %$\nGlobalDiscriminatorLocalDiscriminator\n+ -+ - -…zi-2zi-1zi zi+1zi+2……zi-2zi-1zi zi+1zi+2 …zi-2zi-1zi zi+1zi+2…… -\nEncoder\n~ ^ ^ ^^ ^~~~~\nFigure 1: The overall architecture of TILGAN. Blue and orange stand for the global and local discriminators,\nrespectively, and green denotes the route of the enhanced decoder.\nfunction to be minimized as follows\nLc(φ,θ) + λLg(φ,β), (2)\nwhere λ> 0 is a coefﬁcient to balance both terms.\nDecoder Enhancement During testing, a new sen-\ntence is generated by ﬁrst sampling ϵ ∼pϵ, then\ncomputing the latent variableg(ϵ) and ﬁnally gener-\nating G(g(ϵ)), which means the decoder takes the\noutput of the latent-generator gas the input which\nit has never seen throughout the training. Although\nthe KL term aims at matching the distributions of\ng(ϵ) and E(x), it is possible that they do not match\nperfectly. In such cases, the decoder may generate\ndata with poor ﬁdelity and far from being real data.\nTo resolve this and reliably train the decoder, we\npropose to enhance the decoder by letting it see the\ngenerated latent g(ϵ) during training. Formally, let\n˜pg be the distribution of E(G(g(ϵ))). We add an-\nother term to the loss function (2) with coefﬁcient\nλ1 >0:\nλ1DKL(qφ(z)∥˜pg(z)). (3)\nSince this term is designed to enhance the decoder,\nwe regard the encoder and prior parametersφand β\nas ﬁxed constants. In other words, in optimization,\nwe do not propagate gradients of this term with\nrespect to φ and β and only update the decoder\nparameter θ.\n2.2 Algorithm\nIn this section, we propose a GAN-based algo-\nrithm for the optimization of the above formulation.\nSince pβ(z) is implicit, the KL term Lg in (2) does\nnot allow a closed form to be optimized directly.\nWe introduce a discriminator to estimate the gradi-\nents, following Shen et al. (2020). In Lemma 1, we\npresent the gradient formulas of Lg.\nLemma 1. Let D(z) = ln(qφ(z)/pβ(z)). Then\n∇φLg = E[∇zD(Eφ(x))⊤∇φEφ(x)],\n∇βLg = −E[sD(gβ(ϵ))∇zD(gβ(ϵ))⊤∇βgβ(ϵ)],\nwhere sD(z) = eD(z) is the scaling factor and the\nexpectations are taken over all the randomness.\nSince Ddepends on the unknown densities qφ\nand pβ so that the gradients in Lemma 1 can not\nbe directly computed from the data, we estimate\nthe gradients by training a discriminator Dψ with\nparameter ψvia the empirical logistic regression:\nmin\nψ\n[ ∑\nz∈Se\nln(1 +e−Dψ(z))\n|Se| +\n∑\nz∈Sg\nln(1 +eDψ(z))\n|Sg|\n]\n,\nwhere Se and Sg are ﬁnite samples from qφ(z) and\npβ(z) respectively. This leads to a GAN algorithm.\nThe optimization of the enhanced loss (3) is similar.\nHowever, GAN is commonly known to suf-\nfer from unstable training or gradient vanishing.\nTo stabilize our algorithm, we adopt the scaling\nclipping technique from Shen et al. (2020) and\nclip the scaling factor into a range of [r0,1/r0],\nwhere r0 = 0 .5 turns out to work well in all\nour experiments. Denote the clipped scaling by\ns′\nD(z) = max{min{sD(z),2},0.5}.\nFor the optimization of the consistency loss\nLc, we adopt the reparametrization trick from\nKingma and Welling (2014) and estimate it by\n4847\nAlgorithm 1: TILGAN\nInput: initial φ,θ,β,ψ,ξ , batch-size N, local size M\nwhile not convergence do\n// Update discriminators\nSample {xi}N\ni=1 ∼pr(x), {ϵi}N\ni=1 ∼pϵ\nCompute\nzi = Eφ(xi),ˆzi = gβ(ϵi),˜zi = Eφ(Gθ(ˆzi))\nUpdate ψby descending the gradient:\n1\nN\n∑N\ni=1 ∇ψ[ln(1+e−Dψ(zi))+ln(1+eDψ(ˆzi))]\nRandomly sample local blocks z′\ni and ˆz′\ni with size\nM\nUpdate local discriminator by descending:\n1\nN\n∑N\ni=1 ∇ξ[ln(1 +e−dξ(z′\ni)) + ln(1 +edξ(ˆz′\ni))]\n// Update encoder, decoder and latent-generator\nObtain xi,ϵi,zi,ˆzi,˜zi, z′\ni and ˆz′\ni as above\nCompute φ-gradient:\n1\nN\n∑N\ni=1[∇φˆLc(xi,zi)+λ∇φDψ(zi)+λ∇φdξ(z′\ni)]\nCompute β-gradient:\n−1\nN\n∑N\ni=1 λ[s′\nD(ˆzi)∇φDψ(ˆzi)+s′\nd(ˆz′\ni)∇φdξ(ˆz′\ni)]\nCompute θ-gradient:\n1\nN\n∑N\ni=1[∇θˆLc(xi,zi)+λ1s′\nD(˜zi)∇φDψ(˜zi)]\nUpdate parameters φ,θ,β using the gradients\nReturn: φ,θ,β\n1\nn\n∑n\ni=1 ˆLc(xi,zi) where xi ∼ pr(x), zi =\nEφ(xi), and ˆLc(xi,zi) = −ln pθ(xi|zi). The\nwhole training procedure is summarized in Algo-\nrithm 1, where the colored parts stand for the en-\nhanced decoder (green) and the multi-scale discrim-\ninator (blue and orange) introduced later.\n2.3 Architecture\nIn this section, we present the Transformer-based\narchitecture incorporated with multi-scale discrim-\ninators. We propose a Transformer autoencoder\nframework where both the encoder and decoder are\nself-attention layers with three novel ingredients\nspeciﬁc to improve the generation performance in\nboth quality and diversity: (i) a latent-generator g\nto transform Gaussian noises into an implicit prior\ndistribution, (ii) decoder enhancement, and (iii)\nmulti-scale discriminators. Figure 1 illustrates the\nentire architecture of TILGAN.\nAs mentioned in Section 1, we introduce mul-\ntiple discriminators over the Transformer’s latent\nspace to utilize the semantic information on differ-\nent scales, each of which operates on a different\nwindow of representations as the input. Speciﬁ-\ncally, given an input sentence x = [x1,x2,. . .,xn]\nwhere xi stands for the i-th word, it is passed\nthrough the Transformer encoder which results in a\nsequence of latent states z = [z1,z2,. . .,zn] where\nzi is the vector representation corresponding to\nxi. We introduce a global discriminator Dψ tak-\ning the whole sequence of representations z as the\ninput, and a local discriminator dξ with parame-\nter ξ taking only a local neighborhood of the M\nrandomly-sampled adjacent representations, e.g.,\nz′ = [zi−1,zi,zi+1] with M = 3, as the input. 2\nNotably, the local discriminator takes the generated\npieces of sequences into account, so it provides\nsignals of phrase-level ﬁdelity and local coherence,\nwhile the global discriminator is able to assess the\ngeneral realism and the degree of natural coherence\nfor the whole sequence.\n2.4 Extension to Conditional Generation\nOur proposed framework can be readily extended\nto conditional generation tasks such as story com-\npletion. To be speciﬁc, the goal is to learn a condi-\ntional real data distribution pr(x|c) where c is the\ngiven context following pr(c) with some missing\ncontent x to complete. We propose to feed c into\nall three components—encoder E, decoder G, and\nlatent-generator g—of our model, and modify the\nterms in objective function (2) as follows\nL′\nc(φ,θ) = −Epr(x,c)Eqφ(z|x,c)[ln pθ(x|z,c)],\nL′\ng(φ,β) = DKL(qc\nφ(z)∥pc\nβ(z)),\nwhere pr(x,c) = pr(x|c)pr(c), and the marginal\ndistributions of E(x,c) and g(ϵ,c) are given\nby qc\nφ(z) = Epr(x,c)[qφ(z|x,c)] and pc\nβ(z) =\nEpr(x,c)[pβ(z|x,c)] respectively. Then the ﬁnal\nobjective is to minimize L′\nc(φ,θ) + λL′\ng(φ,β).\n3 Theoretical Justiﬁcation\nThe goal of generative modeling is to learn the gen-\nerated distribution pG(x) that is close to the real\ndata distribution pr(x). Our proposed formulation\nin (2), however, does not explicitly optimize a dis-\ntance measure between pG and pr, so it is unclear\nwhether our method can match the distributions in\nthe data space. In this section, we provide justiﬁca-\ntion for the proposed formulation (2) by connect-\ning it with the above goal, based on the analysis of\nW AE (Tolstikhin et al., 2018).\nLet PG and Pr be the induced probability mea-\nsures of pG(x) and pr(x) respectively. We have\nthe Kantorovich’s formulation of the optimal trans-\nport (OT) problem with the L1 cost:\nW1(Pr,PG) = inf\nΓ∈P(x∼Pr,y∼PG)\nEx,y∼Γ[c(x,y)],\n2We have considered sampling multiple different neigh-\nborhoods within a given sequence as well, whose empirical\nperformance was shown to be comparable with our proposed\nscheme with one local neighborhood, so we only reported the\nlatter since it is simpler to implement.\n4848\nTASK UG CG\nDATASET MSCOCO WMTN EWS ROCS TORY\nVOCAB 27842 5728 20000\nAVG LEN. 10.4 27.8 10.0\nTRAIN S# 120K 278K 390K\nDEV S# - - 50K\nTEST S# 10K 10K 50K\nTable 1: The statistics of the datasets. Avg Len. means\nthe average length of sentences. S# refers to number of\nsentences. UG and CG stand for unconditional genera-\ntion and conditional generation, respectively.\nwhere c(x,y) = ∥x −y∥1 is the cost function and\nP(x ∼Pr,y ∼PG) is a set of all joint distribu-\ntions of (x,y) with marginals Pr and PG respec-\ntively. Note that W1(Pr,PG) is also known as the\n1-Wasserstein distance between Pr and PG. Then\nwe have the following theorem which gives an up-\nper bound of the 1-Wasserstein distance, whose\nproof is given in Appendix B.\nTheorem 1. Let pθ(x|z) be a multivariate multino-\nmial distribution with mean matrix ¯G(z) ∈Rm×n\nwhich is a common choice for text modeling, i.e.,\neach one-hot token xi|z follows a multinomial with\nmean ¯Gi(z) ∈simplex ∆m−1 for i = 1 ,...,n .\nThen we have W1(Pr,PG) is upper bounded by\ninf\nq(z|x):qz(z)=pβ(z)\n−2Ex∼prEz∼q(z|x)[ln pθ(x|z)],\n(4)\nwhere qz(z) = Ex∼pr(x)[q(z|x)] is the aggregated\nposterior and pβ(z) is the implicit prior.\nHence by minimizing (4) with respect toθand β,\nwe learn the composite generator Gθ(gβ(ϵ)) : E→\nXthat minimizes an upper bound of W1(Pr,PG),\nwhich is consistent with the standard goal of genera-\ntive modeling. However, this optimization problem\nis generally intractable due to the equality con-\nstraint and the nonparametric nature. Our formula-\ntion (2) can be regarded as an approximate problem\nof it by parametrizing q(z|x) with a distribution\nfamily induced by a stochastic encoder mapping\nEφ, and relaxing the hard constraintqz(z) = pβ(z)\nby introducing the relative entropy regularization\nDKL(qz(z)∥pβ(z)).\n4 Experiment Settings\n4.1 Datasets\nWe conduct our experiments on three bench-\nmark datasets, MSCOCO (Lin et al., 2014),\nWMTN EWS (Guo et al., 2018), and ROC-\nSTORY (Mostafazadeh et al., 2016). All of the\npreprocessing steps are the same as Chen et al.\n(2018) and Wang and Wan (2019). The statistics\nof the resulting datasets are reported in Table 1.\n4.2 Baselines\nUnconditional Generation Three simpliﬁed vari-\nants of TILGAN are implemented for comparison:\n•TILGAN P: a plain baseline using our backbone\nmodel, that is, a Transformer autoencoder and a\nGAN in the latent space based on KL divergence.\n•TILGAN E: TILGAN P equipped with decoder\nenhancement.\n•TILGAN MD : TILGAN P with the multi-scale\ndiscriminator.\nIn addition, the following existing models are\nadopted: recurrent neural network language\nmodel (RNNMLE), SeqGAN (Yu et al., 2017),\nRankGAN (Lin et al., 2017), GSGAN (Kusner\nand Hernández-Lobato, 2016), LeakGAN (Guo\net al., 2018), textGAN (Zhang et al., 2017), FM-\nGAN (Chen et al., 2018), ARAE (Zhao et al., 2018),\nTransformer language model (TMLE) (Vaswani\net al., 2017).\nConditional Generation For conditional gen-\neration, we compare our model with Trans-\nformer (Vaswani et al., 2017), IE+MSA (Guan\net al., 2019), Seq2Seq (Bahdanau et al., 2015),\nHLSTM (Li et al., 2015), CV AE (Sohn et al., 2015),\nand T-CV AE (Wang and Wan, 2019).\n4.3 Automatic Evaluation Metrics\nUnconditional Generation\n•TEST BLEU (Yu et al., 2017): a quality metric\ncomparing the n-gram similarity between gener-\nated samples and the whole test set.\n•SELF BLEU (Zhu et al., 2018): a diversity met-\nric calculating the similarity between one gen-\nerated sentence and the whole remaining gen-\neration. The lower the SelfBLEU score is, the\nhigher diversity we obtain in the generation.\nSpeciﬁcally, following Chen et al. (2018), we\nreport BLEU-2/3/4/5 for TestBLEU and BLEU-\n2/3/4 for SelfBLEU.\nConditional Generation\n•BLEU (Papineni et al., 2002): the BLEU score\nis calculated by taking the geometric mean of\nthe n-gram BLEU scores where n is from 1 to 4.\n•DIVERSITY (Li et al., 2016): the proportion of\ndistinct n-grams in the generated results which\nevaluates the degree of diversity. D1 and D2 are\nreported for unigram and bigram, respectively.\n4849\n(a) MSCOCO\nMETHODS SELF BLEU TEST BLEU HUMAN\nB2% B3% B4% B2% B3% B4% B5% Q D\nRNNMLE 75.4 51.1 23.2 82.0 60.7 38.9 24.8 3.33 2.8\nSEQGAN 80.7 57.7 27.8 82.0 60.4 36.1 21.1 3.86 2.8\nRANK GAN 82.2 59.2 28.8 85.2 63.7 38.9 24.8 3.26 2.6\nGSGAN 78.5 52.2 23.0 81.0 56.6 33.5 19.7 3.15 2.4\nLEAK GAN 91.2 82.5 68.9 92.2 79.7 60.2 41.6 3.07 3.0\nTEXT GAN 80.6 54.8 21.7 91.0 72.8 48.4 30.6 3.55 2.8\nFMGAN 83.1 63.2 32.5 94.2 81.2 61.8 41.4 4.06 2.2\nARAE 63.2 41.6 19.1 86.7 69.3 44.2 24.5 2.93 3.0\nTMLE 70.6 47.6 27.3 92.8 81.9 56.2 33.1 3.75 3.4\nTILGAN 61.6 35.6 9.9 96.7 90.3 77.2 53.2 4.38 3.8\nTILGAN P 61.7 45.9 18.2 94.7 86.6 63.1 39.9 - -\nTILGAN E 70.5 50.1 28.7 98.8 94.5 81.3 52.5 - -\nTILGAN MD 63.5 38.7 11.8 95.1 84.9 64.8 44.1 - -\nIMP _POST 73.3 63.8 47.7 95.5 87.1 69.8 31.5 - -\nJSGAN 76.7 67.9 51.8 75.1 54.2 32.1 11.0 - -\nWGAN 90.4 80.9 69.0 73.0 53.8 34.2 12.5 - -\n(b) WMTNews\nSELF BLEU TEST BLEU HUMAN\nB2% B3% B4% B2% B3% B4% B5% Q D\n66.4 33.7 11.3 76.1 46.8 23.1 11.6 3.65 2.8\n72.8 41.1 13.9 63.0 35.4 16.4 8.7 3.29 3.4\n67.2 34.6 11.8 77.4 48.4 24.9 13.1 2.98 3.8\n68.2 41.0 23.1 72.3 44.0 21.0 10.7 3.39 2.6\n85.7 69.6 37.3 92.0 72.5 50.2 32.1 2.51 2.8\n80.6 54.8 28.7 77.7 52.9 30.5 16.1 3.43 3.2\n83.1 68.2 38.5 93.2 77.1 55.2 39.9 3.40 3.2\n53.4 30.4 17.3 84.4 62.9 39.8 22.0 2.29 2.6\n61.3 43.7 25.1 87.5 74.8 44.2 26.4 3.31 3.6\n66.3 44.5 28.0 92.9 81.7 61.7 40.7 3.81 4.0\n64.8 48.2 34.9 88.9 76.5 56.5 27.5 - -\n62.7 43.3 23.0 91.5 79.2 59.6 33.9 - -\n53.1 33.2 20.6 92.6 78.2 53.9 29.5 - -\n71.2 59.7 47.1 73.1 68.2 46.6 20.1 - -\n64.7 49.9 39.6 80.8 64.9 41.1 14.2 - -\n93.3 91.0 88.6 89.1 77.4 50.2 26.7 - -\nTable 2: SelfBLEU and TestBLEU results on MSCOCO and EMNLP WMTNews datasets. Q and D denote the quality and\ndiversity evaluated by human, respectively. The results of previous baselines are listed in the top region, our method TILGAN\ntogether with simpliﬁed variants are shown in the middle, and more variants for further ablation studies are at the bottom. The\nbold numbers are the best results in each column.\n•ADVERSARIAL SUCCESS (Li et al., 2017): the\nfraction of instances in which a model is capa-\nble of fooling a ﬁne-tuned BERT classiﬁer with\nthe above 95% accuracy on the development set\nof the classiﬁcation task. Higher values are bet-\nter. The positive examples are original stories\nand negative examples are stories consisting of a\nrandom sentence from another story.\n4.4 Implementation\nUnconditional Generation We implement a\nTransformer-based autoencoder with 2 layers, 4\nheads, 512 embedding dimensions, and 512 hidden\ndimensions. The generator and discriminator are\nimplemented by 3 layers multi-layer perceptron\n(MLP). We set the maximum sequence length to 15\nand 32 for MSCOCO and WMTNews, respectively.\nDuring training, each sentence is padded to the\nmaximum length when fed into the encoder, and\nthen the encoder produces a latent vector for every\ninput token. During testing, the latent-generator\ngenerates a sequence of latent vectors with the\nsame maximum length, and then, conditional on\nthe latent vectors, the decoder generates a sentence\nwhich ends when a special token, <EOS>, is gen-\nerated. We adopt Adam (Kingma and Ba, 2015)\nas the optimizer with a learning rate of 0.00025\nand 0.0001 for autoencoder and GAN structure,\nrespectively with a dropout rate of 0.3.\nConditional Generation We adopt the same\nTransformer encoder-decoder architecture as the\nbackbone model and similar setups as Wang and\nWan (2019). The Transformer structure has 6 lay-\ners, 8 self-attention heads, 512 dimensions for hid-\nden states, and uses shared attention layers for en-\ncoder and decoder which allows the decoder to\nattend to the encoder state and the decoder state at\nthe same time to make the completed story more\ncoherent. The generator has 3 layers and the dis-\ncriminator has 4 layers. We adopt Adam (Kingma\nand Ba, 2015) as the optimizer with a learning rate\nof 0.0001 and a dropout rate of 0.15.\nMore details of the experimental setup and hyper-\nparameter settings are shown in the Appendix A.\n5 Experimental Results\n5.1 Unconditional Generation\n•Generation Quality The ﬁrst experiment is to\ncompare the quality of generated sentences of dif-\nferent models. In general, as shown in Table 2,\nTILGAN outperforms all baseline models in Test-\nBLEU on both MSCOCO and WMTNews datasets,\nwhich clearly indicates the advantages of our pro-\nposed framework. We make ﬁve main observations.\nFirstly, we notice that TMLE is comparable to most\nGAN baselines, which shows the powerful ﬁtting\ncapacity of the Transformer architecture as well as\nthe inferior performance of previous GAN imple-\nmentations. Despite this, TILGAN P outperforms\nTMLE by a wide margin, demonstrating that our\nbackbone model combining a Transformer autoen-\n4850\ncoder and a GAN can not only take advantage of\nthe Transformer’s capacity, but also exhibit bene-\nﬁts from our GAN formulation to further boost\nthe performance. Furthermore, compared with\nTILGAN P, our full version TILGAN achieves an\nimprovement of 13.3% and 13.2% for TestBLEU5\non MSCOCO and News, respectively. This con-\nﬁrms the effectiveness of the proposed multi-scale\ndiscriminators and decoder enhancement. In detail,\nwhen comparing the simpliﬁed variants TILGAN E\nv.s. TILGAN MD , the improvement of TILGAN E\nover the plain baseline TILGAN P is larger than\nthat of TILGAN MD , which illustrates that incor-\nporating TILGAN E is more crucial to improving\nthe generation quality. Lastly, we compare TIL-\nGAN with all previous methods and observe an\naverage improvement of 6.3% for TestBLEU5 on\ntwo datasets against the previous state-of-the-art\nFMGAN, which suggests the superiority of our\nmethod.\n•Generation Diversity The generation diversity\nis evaluated by SelfBLEU scores, which are shown\nin Table 2. First, compared with the baselines\nwith comparable and worse TestBLEU, e.g., FM-\nGAN and LeakGAN, our TILGAN achieves lower\nSelfBLEU scores, which indicates a better quality-\ndiversity trade-off from TILGAN. We notice that\nRNNMLE achieves the best SelfBLEU score on\nWMTNews but its quality shown by TestBLEU\nis pretty low and tends to generate incoherent or\nmeaningless segments, which can be conﬁrmed by\nthe generated samples in Appendix C. In addition,\nfrom the results of TILGAN MD , we ﬁnd that in-\ncorporating the multi-scale discriminator leads to\na signiﬁcant drop in SelfBLEU, suggesting that\nmost of the performance gains in generation di-\nversity are attributed to our design of multi-scale\ndiscriminators in contrast to decoder enhancement.\nMoreover, when comparing the performance across\ntwo datasets, we ﬁnd that the SelfBLEU scores of\nour models are lower on MSCOCO than that on\nWMTNews, illustrating that it is easier to gener-\nate more diverse texts on MSCOCO. One possible\nreason is that the texts in MSCOCO are shorter\nthan the texts in WMTNews as shown in Table 1.\nWhen generating long sequences, models are prone\nto generate repeated tokens and phrases. The same\nphenomenon was also observed for many other\nbaseline models like ARAE and FMGAN.\n5.2 Conditional Generation\nIn addition to unconditional generation, we test\nour model in a story completion task to verify its\nability in conditional generation. Table 3 shows\nthe automatic metrics in four metrics, with sev-\neral observations. (i) Overall, among all models,\nTILGAN achieves new state-of-the-art results on\nthe ROCStory dataset, showing the superiority of\nour method. (ii) Our model obtains substantial\nimprovement in the quality metrics of generated\nanswers, with 0.32% gains in BLEU, 6.92% gains\nin the adversarial success. It demonstrates that\nthe generated plots are in high coherence, which\nnot only share a higher proportion of word overlap\nwith ground-truth answers, but also have a higher\nsuccess rate fooling the BERT classiﬁer. (iii) As\nfor diversity, TILGAN improves upon the state-of-\nthe-art methods from 3.63% to 3.88% on D1 and\n23.46% to 25.61% on D2, showing that TILGAN\nproduces stories consisting of more diverse and\ndistinct n-grams.\n5.3 Human Evaluation\nDue to the limitations of automatic evaluation met-\nrics, we invite 5 judges to rate 100 sentences gen-\nerated by different models on a scale from 1 to 5\nfor both unconditional and conditional generation\ntasks. The results for unconditional generation are\nshown in Table 2. TILGAN shows a superior per-\nformance, which conﬁrms that our model is able\nto generate more realistic samples than the base-\nline models on two datasets. Among all baseline\nmodels, FMGAN has a high quality score but a\nlow diversity score, which indicates that most of its\ngenerated samples are repeated sentences that lack\ndiversity. Additional evidence is shown through\nthe case study in Section 6.2.\nIn addition, the human evaluation results on story\ncompletion are shown in Table 3 where we only\ncompare TILGAN with the best baseline, i.e., T-\nCV AE. We use Gram metric to evaluate whether\nthe generated story plot proceeds naturally, and\nLogic metric to evaluate whether the plot is reason-\nable and coherent following Wang and Wan (2019).\nCompared with T-CV AE, TILGAN is better in\nboth Gram and Logic, demonstrating that the gen-\nerated story plots of TILGAN are more natural\nand coherent.\n4851\nMETHODS BLEU% D1% D2% AS% G RAM LOGIC\nSEQ2SEQ 2.90 2.69 15.95 80.97 - -\nHLSTM 2.31 2.63 14.80 72.46 - -\nCVAE 3.03 2.72 16.32 81.18 - -\nTRANS . 3.05 2.93 16.75 82.51 - -\nT-CVAE 4.25 3.63 23.46 87.54 3.32 3.24\nTILGAN 4.57 3.88 25.61 94.46 3.58 3.60\nTable 3: Results on story completion task. AS refers\nto adversarial success score. T RANS . denotes a vanilla\nTransformer model.\n6 Analyses\n6.1 Ablation Study\nTo examine the impact of KL loss and the implicit\nprior, we conduct ablation studies of different de-\nsigns. We construct three variants of TILGAN and\nconduct experiments on two datasets of uncondi-\ntional generation, whose results are shown in the\nbottom region of Table 2.\n•Impact of KL Loss First, we implement two\nvariants named JSGAN (Goodfellow et al., 2014)\nand WGAN (Arjovsky et al., 2017) by replacing the\nKL loss term with JS divergence and Wasserstein\ndistance, while keeping the same architectures.\nIn general, TILGAN P outperforms JSGAN and\nWGAN in terms of SelfBLEU and TestBLEU on\ntwo datasets. Particularly, it is observed that with\nKL loss, the SelfBLEU4 score drops from 51.8%\nand 69.0% to 18.2% over JSGAN and WGAN on\nMSCOCO, and similar downward trends are ob-\nserved on WMTNews. It demonstrates that mini-\nmizing KL loss indeed beneﬁts the generation di-\nversity, which is consistent with previous ﬁndings\nin Shen et al. (2020). In addition, the TestBLEU of\nTILGAN P achieves an improvement of 28.9% and\n27.4% for TestBLEU5 on MSCOCO over JSGAN\nand WGAN, respectively.\n•Implicit Prior v.s. Implicit Posterior In addi-\ntion to imposing an implicit prior, one can instead\nimpose an implicit posterior as well by moving the\ntransformation network of the latent-generator to\nthe encoder and leaving a Gaussian prior. This\nresults in a variant with nearly the same total\nnumber of parameters, named IMP_POST . We\nsee from Table 2 that IMP_POST performs worse\nthan TILGAN P with an implicit prior, suggesting\nthat enlarging the distribution family of posterior\nqφ(z|x) contributes less to improving the overall\ngeneration performance than enlarging that of prior\npz(z), which is consistent to the analysis in the\nsecond paragraph of Section 2.1.\n6.2 Case Study\nTo further analyze the real quality and diversity\nof the generated sentences, some are examined\nand presented in Table 4 and more examples are\nshown in Appendix C. First, the samples generated\nby TILGAN are more coherent and semantically\nmeaningful. The majority of texts of TILGAN are\nin subject–verb–object order while those of other\nmodels are not. In addition, TILGAN exhibits\nmore diverse sentence structures and word choices\nthan others. For example, although each sentence\ngenerated by FMGAN looks good in quality, there\nare many repeated sentences or phrases, leading to\na low diversity. The case study is consistent with\nthe human evaluation results in Section 5.3.\n7 Related Work\nConventional text generation models leverage max-\nimum likelihood estimation (MLE) with teacher\nforcing and have shown powerful generation ca-\npabilities (Mikolov et al., 2010; Cho et al., 2014;\nBahdanau et al., 2016; Radford et al., 2019; Brown\net al., 2020) but they suffer from the exposure bias\nproblem. To address this, several solutions were\nintroduced including scheduled sampling (Bengio\net al., 2015), professor forcing (Lamb et al., 2016),\nand Gibbs sampling (Su et al., 2018).\nGAN-based text generation methods can be cat-\negorized into three classes: reinforcement learn-\ning (RL) based methods, Gumbel-Softmax (GS)\nbased methods and latent feature matching meth-\nods. RL-based methods (Yu et al., 2017; Lin et al.,\n2017; Che et al., 2017; Guo et al., 2018; Fedus\net al., 2018) design a reward incorporated with the\ndiscriminators, and use policy gradient or actor-\ncritic approaches to update the generator to resolve\nthe gradient propagating issue over discrete tokens.\nHowever, they suffer from high variance and mode\ncollapse issues caused by the unstable policy gra-\ndient training process and the lack of a reliable\nguiding signal (Zhang et al., 2017; Chen et al.,\n2018). GS-based methods (Kusner and Hernández-\nLobato, 2016) apply Gumbel-Softmax which is a\ncontinuous relaxation technique for transforming\nthe output of a generator to be as close to one-hot\nas possible in order to make the samples from a dis-\ncrete distribution like a multinomial differentiable\nwith respect to the distribution parameters.\nLatent feature matching methods (Zhang et al.,\n2017; Zhao et al., 2018) learn a manifold in the\nlatent space instead of the discrete output space.\n4852\nRankGAN: (1) A blue blue train sits on tracks with his residential asian toys.\n(2) A reﬂection of two birds walking by a sidewalk.\nFMGAN: (1) A man is standing on a table with a dog.\n(2) A man is standing on a table with a dog on a ﬁeld.\n(3) A man is standing on a ﬁeld of a large building.\nTILGAN : (1) A little boy sitting on a bench with a little girl.\n(2) A blue and white public transit bus is driving down acity street.\n(3) A train is going down the tracks in a forest.\nTable 4: Examples of generated sentences from RankGAN, FMGAN and our model.\nThis kind of methods usually incorporates an au-\ntoencoder to build the feature space and force the\ngenerator’s latent output distribution to approach\nthe real data latent distribution. Our method also\nresides in this category. To ease adversarial train-\ning, Zhang et al. (2017) introduce adversarial fea-\nture matching method by incorporating a kernel-\nized discrepancy metric to match high-dimensional\nlatent representations of real and synthetic sen-\ntences. ARAE (Zhao et al., 2018) extends AAE\n(Makhzani et al., 2015) to model discrete sequences\nand learns a parameterized prior by a generative\nmodel trained with WGAN. In contrast to our TIL-\nGAN whose Transformer-based encoder and de-\ncoder are both stochastic, ARAE uses RNN-based\nencoder and decoder which are both deterministic,\nas required in their theory, which reduces the model\nexpressiveness and results in much poorer perfor-\nmance than ours as shown in Table 2. iV AE (Fang\net al., 2019) proposes a V AE (Kingma and Welling,\n2014) with an implicit posterior which is inferior\nto the implicit prior that we adopt according to the\nablation study in Section 6.1. W AE-S (Bahuleyan\net al., 2019) is a W AE(Tolstikhin et al., 2018) with\na stochastic encoder trained using MMD with a dis-\ntinct goal of improving the reconstruction ability.\n8 Conclusion\nIn this paper, we proposed Transformer-based Im-\nplicit Latent GAN (TILGAN ), for text generation.\nIt combines a Transformer autoencoder and a GAN\nthrough matching the distributions of multi-token\nsequences in the Transformer’s latent space based\non KL divergence. To improve the local and global\ncoherence, we introduced a multi-scale discrimina-\ntor to utilize the semantic information on varying\nscales. To train the decoder reliably, we enhanced\nthe objective function by another KL term, forc-\ning the decoder to be compatible with the latent-\ngenerator. We theoretically connected the proposed\nformulation with the standard goal of generative\nmodeling. Empirically, TILGAN achieved the\nstate-of-the-art performance on three widely used\ndatasets for unconditional tasks and story comple-\ntion task, which demonstrated the effectiveness of\nour method to generate texts of high quality and\ndiversity compared with the existing approaches.\nAcknowledgments\nThis work was supported by the General Research\nFund (GRF) of Hong Kong (No. 16201320).\nY . Song was supported by NSFC under the\nproject “The Essential Algorithms and Technolo-\ngies for Standardized Analytics of Clinical Texts”\n(12026610) and Shenzhen Institute of Artiﬁcial\nIntelligence and Robotics for Society under the\nproject “Automatic Knowledge Enhanced Natu-\nral Language Understanding and Its Applications”\n(AC01202101001). The authors also want to thank\nthe anonymous reviewers for their valuable com-\nments and suggestions.\nReferences\nMartin Arjovsky, Soumith Chintala, and Léon Bot-\ntou. 2017. Wasserstein Generative Adversarial Net-\nworks. In Proceedings of the 34th International\nConference on Machine Learning, pages 214–223.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nDzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk,\nPhilemon Brakel, and Yoshua Bengio. 2016. End-\nto-end Attention-based Large V ocabulary Speech\nRecognition. In 2016 IEEE international confer-\nence on acoustics, speech and signal processing\n(ICASSP), pages 4945–4949. IEEE.\n4853\nHareesh Bahuleyan, Lili Mou, Hao Zhou, and Olga\nVechtomova. 2019. Stochastic Wasserstein Autoen-\ncoder for Probabilistic Sentence Generation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 4068–4076.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. 2015. Scheduled Sampling for Se-\nquence Prediction with Recurrent Neural Networks.\nAdvances in Neural Information Processing Systems,\npages 1171–1179.\nMikołaj Bi´nkowski, Jeff Donahue, Sander Dieleman,\nAidan Clark, Erich Elsen, Norman Casagrande,\nLuis C. Cobo, and Karen Simonyan. 2020. High Fi-\ndelity Speech Synthesis with Adversarial Networks.\nIn International Conference on Learning Represen-\ntations.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language Models are Few-Shot\nLearners. In Advances in Neural Information Pro-\ncessing Systems.\nTong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm,\nWenjie Li, Yangqiu Song, and Yoshua Bengio.\n2017. Maximum-Likelihood Augmented Discrete\nGenerative Adversarial Networks. arXiv preprint\narXiv:1702.07983.\nLiqun Chen, Shuyang Dai, Chenyang Tao, Haichao\nZhang, Zhe Gan, Dinghan Shen, Yizhe Zhang,\nGuoyin Wang, Ruiyi Zhang, and Lawrence Carin.\n2018. Adversarial Text Generation via Feature-\nMover’s Distance. In Advances in Neural Informa-\ntion Processing Systems, pages 4666–4677.\nKyunghyun Cho, Bart van Merrienboer, Çaglar\nGülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learn-\ning Phrase Representations using RNN Encoder-\nDecoder for Statistical Machine Translation. In\nEMNLP.\nLe Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and\nChangyou Chen. 2019. Implicit Deep Latent Vari-\nable Models for Text Generation. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3937–3947.\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. MaskGAN: Better Text Generation via Filling\nin the _. In International Conference on Learning\nRepresentations.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative\nAdversarial Nets. Advances in Neural Information\nProcessing Systems, pages 2672–2680.\nJian Guan, Yansen Wang, and Minlie Huang. 2019.\nStory Ending Generation with Incremental Encod-\ning and Commonsense Knowledge. In 33rd AAAI\nConference on Artiﬁcial Intelligence, AAAI 2019.\nJ Guo, S Lu, H Cai, W Zhang, Y Yu, and J Wang. 2018.\nLong Text Generation via Adversarial Training with\nLeaked Information. In 32nd AAAI Conference on\nArtiﬁcial Intelligence, AAAI 2018.\nDonghoon Ham, Jeong-Gwan Lee, Youngsoo Jang,\nand Kee-Eung Kim. 2020. End-to-End Neural\nPipeline for Goal-Oriented Dialogue Systems using\nGPT-2. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 583–592.\nDiederik P Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. In ICLR\n(Poster).\nDiederik P Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In International Confer-\nence on Learning Representations.\nMatt J Kusner and José Miguel Hernández-Lobato.\n2016. GANS for Sequences of Discrete Ele-\nments with the Gumbel-softmax Distribution. arXiv\npreprint arXiv:1611.04051.\nAlex M Lamb, Anirudh Goyal Alias Parth Goyal, Ying\nZhang, Saizheng Zhang, Aaron C Courville, and\nYoshua Bengio. 2016. Professor Forcing: A New\nAlgorithm for Training Recurrent Networks. In Ad-\nvances in Neural Information Processing Systems ,\npages 4601–4609.\nHung Le, Doyen Sahoo, Nancy Chen, and Steven Hoi.\n2019. Multimodal Transformer Networks for End-\nto-End Video-Grounded Dialogue Systems. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5612–\n5623.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A Diversity-Promoting Objec-\ntive Function for Neural Conversation Models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 110–119.\nJiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.\nA hierarchical neural autoencoder for paragraphs\nand documents. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\n4854\non Natural Language Processing (Volume 1: Long\nPapers), pages 1106–1115.\nJiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean,\nAlan Ritter, and Dan Jurafsky. 2017. Adversarial\nLearning for Neural Dialogue Generation. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing , pages 2157–\n2169.\nKevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang,\nand Ming-Ting Sun. 2017. Adversarial Ranking for\nLanguage Generation. In Advances in Neural Infor-\nmation Processing Systems, pages 3155–3165.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft COCO:\nCommon Objects in Context. In European confer-\nence on computer vision, pages 740–755. Springer.\nAlireza Makhzani, Jonathon Shlens, Navdeep Jaitly,\nIan Goodfellow, and Brendan Frey. 2015. Adversar-\nial autoencoders. arXiv preprint arXiv:1511.05644.\nTomáš Mikolov, Martin Karaﬁát, Lukáš Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent Neural Network Based Language Model. In\nEleventh annual conference of the international\nspeech communication association.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A Corpus\nand Cloze Evaluation for Deeper Understanding of\nCommonsense Stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a Method for Automatic Eval-\nuation of Machine Translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners. Ope-\nnAI blog, 1(8):9.\nXinwei Shen, Tong Zhang, and Kani Chen. 2020.\nBidirectional Generative Modeling Using Adver-\nsarial Gradient Estimation. arXiv preprint\narXiv:2002.09161.\nKihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.\nLearning structured output representation using\ndeep conditional generative models. In Advances in\nNeural Information Processing Systems, volume 28,\npages 3483–3491. Curran Associates, Inc.\nJinyue Su, Jiacheng Xu, Xipeng Qiu, and Xuanjing\nHuang. 2018. Incorporating Discriminator in Sen-\ntence Generation: a Gibbs Sampling Method. In\n32nd AAAI Conference on Artiﬁcial Intelligence,\nAAAI 2018.\nIlya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and\nBernhard Schoelkopf. 2018. Wasserstein auto-\nencoders. In International Conference on Learning\nRepresentations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning Deep Transformer Models for Ma-\nchine Translation. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1810–1822.\nTianming Wang and Xiaojun Wan. 2019. T-CV AE:\nTransformer-Based Conditioned Variational Autoen-\ncoder for Story Completion. In Proceedings of the\nTwenty-Eighth International Joint Conference on Ar-\ntiﬁcial Intelligence, IJCAI-19, pages 5233–5239.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral Text Generation With Unlikelihood Training. In\nInternational Conference on Learning Representa-\ntions.\nSam Wiseman and Alexander M Rush. 2016.\nSequence-to-Sequence Learning as Beam-Search\nOptimization. In Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 1296–1306.\nQingyang Wu, Lei Li, and Zhou Yu. 2020.\nTEXTGAIL: Generative Adversarial Imitation\nLearning for Text Generation. arXiv preprint\narXiv:2004.13796.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n2017. SeqGAN: Sequence Generative Adversarial\nNets with Policy Gradient. In 31st AAAI Conference\non Artiﬁcial Intelligence, AAAI 2017.\nYizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo\nHenao, Dinghan Shen, and Lawrence Carin. 2017.\nAdversarial Feature Matching for Text Generation.\nIn Proceedings of the 34th International Conference\non Machine Learning, pages 4006–4015.\nJunbo Zhao, Yoon Kim, Kelly Zhang, Alexander Rush,\nand Yann LeCun. 2018. Adversarially Regularized\nAutoencoders. In Proceedings of the 35th Inter-\nnational Conference on Machine Learning , pages\n5902–5911.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,\nWeinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-\ngen: A Benchmarking Platform for Text Generation\nModels. In The 41st International ACM SIGIR Con-\nference on Research & Development in Information\nRetrieval, pages 1097–1100.\n4855\nA Reproducibility Checklist\n•Description of Computing Infrastructure Tesla V100S-PCIE-32GB\n•Average Runtime\nMODEL UG CG\nDATASET MSCOCO WMTN EWS ROCS TORY\nTARAE 5.8 9.2 -\nENHANCE D 7.5 11 -\nLOCAL D 8 12 -\nTILGAN 8.5 13.4 106\nTable 5: The average runtime per epoch for each model, estimated in minutes. UG and CG refer to unconditional\ngeneration and conditional generation, respectively.\n•Number of Parameters\nMODEL UG CG\nTILGAN 25.4M 30.8M\nTable 6: The number of parameters of each model. UG and CG refer to unconditional generation and conditional\ngeneration, respectively.\n•Validation PerformanceNo validation evaluation for unconditional generation and conditional gener-\nation tasks.\n•Number of Runs We conduct 60 runs for unconditional generation tasks and 30 runs for conditional\ngeneration tasks.\n•Bounds and Best Setting for Hyperparameters Please refer to Table 3 for unconditional generation\ntask and Table 4 for conditional generation task.\nMSCOCO WMTNews\nBound Best-performing Bound Best-performing\nmax len 15 15 32 32\nbatch size [32,256] 256 [32,256] 256\nemb size [256,1024] 512 [256,1024] 512\nhidden size [256,1024] 512 [256,1024] 512\nnum layers [1,6] 2 [1,6] 2\nnum heads 4 4 4 4\nsqueezed hidden size [28,256] 56 [28,256] 56\nnoise size [50,512] 100 [50,512] 100\nniters autoencoder [1,3] 1 [1,3] 1\nniters discriminator [1,3] 1 [1,3] 1\nniters enhanceD [1,3] 1 [1,3] 1\nniters generator [1,3] 1 [1,3] 1\nniters gan into encoder [1,3] 1 [1,3] 1\nlearning rate autoencoder [0.01,10] 0.08 [0.01,10] 0.24\nlearning rate gan encoder [1e-5,1e-2] 1e-4 [1e-5,1e-2] 1e-4\nlearning rate generator [1e-5,1e-2] 1e-4 [1e-5,1e-2] 1e-4\nlearning rate discriminator [1e-5,1e-2] 1e-4 [1e-5,1e-2] 1e-4\nTable 7: The bounds for each hyperparameter and best-performing setting for unconditional generation task.\n4856\nROCStory\nBound Best-performing\nnum layers [1,8] 6\nhidden size [256,1024] 512\nnum heads [4,12] 8\nemb size 300 300\nlatent dimension [32,256] 64\nbatch size [32,128] 64\nlearning rate [1e-5,1e-2] 1e-4\ndroupout rate [0,0.5] 0.15\nTable 8: The bounds for each hyperparameter and best-performing setting for conditional generation task.\nB Proof of Theorem 1\nProof of Theorem 1. In this proof, we let x be the real data, y be the generated data, and z be the latent\nvariable. Let pG(y,z) = pβ(z)pθ(y|z) be the joint distribution of (y,z), where z is sampled from prior\npβ(z) and then y is sampled from the decoder conditional pθ(y|z). Further let Px,y,z denote the set of\nall joint distributions of (x,y,z) such that x ∼pr(x), (y,z) ∼pG(y,z), and x ⊥ ⊥y|z; let Px,z be the\nset of marginal distributions of (x,z) induced by Px,y,z, that is, the set of distributions with marginals\nx ∼pr(x) and z ∼pβ(z).\nRecall that nis the sequence length and mis the number of words in the vocabulary. For the i-th word\nxi which is an m-dimensional one-hot vector, indicator 1(xi = j) = 1 if the j-th dimension of xi is equal\nto 1 and 1(xi = j) = 0 otherwise, for j = 1,...,m . Then we have\nW1(Pr,PG) ≤W†\n1 (Pr,PG) := inf\np∈Px,y,z\nEz∼pβ(z)Ex∼p(x|z)Ey∼pθ(y|z)[c(x,y)]\n= inf\np∈Px,z\nEz∼pβ(z)Ex∼p(x|z)\n\n2\nn∑\ni=1\nm∑\nj=1\n1(xi = j)(1 −¯Gij(z))\n\n\n< inf\np∈Px,z\nEz∼pβ(z)Ex∼p(x|z)\n\n−2\nn∑\ni=1\nm∑\nj=1\n1(xi = j) ln ¯Gij(z)\n\n\n= inf\nq(z|x):qz(z)=pβ(z)\nEx∼prEz∼q(z|x)\n\n−2\nn∑\ni=1\nm∑\nj=1\n1(xi = j) ln ¯Gij(z)\n\n\n= inf\nq(z|x):qz(z)=pβ(z)\n{\n−2Ex∼prEz∼q(z|x)[ln pθ(x|z)]\n}\n,\nwhere the ﬁrst inequality comes from Tolstikhin et al. (2018, eq. 9), and the second inequality is due to\nthe fact that 1 −l< −ln lfor all l∈(0,1), leading to the desired result.\n4857\nC Generated Examples\nTable 9: Generated samples on ROCStory dataset.\nstory: ____________________. when i got to the stop sign , the check engine light started\nﬂashing . i panicked and carefully drove the van to the nearest mechanic shop . they\nchecked it out but could not repair the van . the van had to be sold for parts and i had\nto get a new vehicle .\nT-CV AE: i was driving my van down the street one day .\nTILGAN: i was driving my van to work one day .\nstory: krista was organizing her ofﬁce .____________________ . they were big and heavy .\nshe assembled them carefully . when she put all her books on them , they collapsed !\nT-CV AE: she had a bunch of books .\nTILGAN: she bought some new books .\nstory: the man won a contest . he went to the station to collect . ____________________ .\nhe did n’t really like the band . he tried to sell them back to the radio employees .\nT-CV AE: he got a ticket for a band .\nTILGAN: he saw some band members .\nstory: billy is bored . billy sits with his friends thinking of something to do . billy suggest they\nall head to the lake to go ﬁshing . ____________________ . billy takes his friends to go\nﬁshing and has great time .\nT-CV AE: billy and his friends go ﬁshing together .\nTILGAN: billy and his friends go ﬁshing .\nstory: ____________________ . her house was full of dust . she could n’t believe how ﬁlthy\nit was . alicia then decided to clean it . when she was done cleaning and it sparkled .\nT-CV AE: alicia was in the basement .\nTILGAN: alicia was cleaning her house .\nTable 10: Generated samples on MSCOCO dataset.\nTextGAN: - a train traveling down a street . SeqGAN: - a red stop sign .\n- a train station . - a couple of people are walking on a log and trees .\n- a street sign on a street . - the train car traveling mannequin driving down the tracks .\n- is in a bathroom with a sink controls . - people standing next to a large building .\nRankGAN: - a blue blue train sits on tracks with his resi-\ndential asian toys .\nMLE: - an orange booth contains the in trafﬁc light under a\nsign\n- a reﬂection of two birds walking by a sidewalk . - a man with hat on the horse in the street\n- a man tourist train in the egret - a couple walking around city tracks with people\n- a white ﬁre hydrant stands next to each other . - the bird are walking next to a small blue coop\nLeakGAN: - a table topped with pots . . FM-GAN: - a man is standing on a skateboard on the beach\n- a bathroom with a glass shower , sink , toilet and sink . - a man on a tennis game with a kite\n- a woman wearing a glass is sitting on a cupboard . - a man on a table with a red and white and a building\n- a group of men talking . - a man is standing on a table with a dog\nARAE: - a city street sign in the park bench parked in the\ngroup group the man\nTILGAN : - a little boy sitting on a bench with a little girl\n- two people standing at motorcycles on the bench - a group of people in the middle of a ﬁeld\n- two people standing at motorcycles on the bench at white\nkitchen\n- a large passenger plane ﬂying through the sky\n- there is a city bus on their city street sign parked in blue\nblue bus\n- a woman is sitting in a kitchen next to a restaurant\n- a white plane on the air plane parked in snow group their\nplane parked\n- a small bird sitting on a branch of a tree\n4858\nTable 11: Generated samples on EMNLP WMT dataset.\nSeqGAN: - it said the yield on our most traveled to china ’ s capital for “ the annual bank of cost credit against cuba .\n- the cars ( , taiwan argues that the cease - ﬁre contained included already reported on the outlook .\n- “ both the republican leader in the years of leadership she was in rome and signed a close cabinet , ” she said .\n- russia has said in its twitter documents since january which demanded that lawmakers had clearly been involved about .\nRankGAN: - the lakers left the us to hope of the ofﬁce and chose the general administration to build a further mission .\n- ms . bush had been remembered in a red ring after it taking these sunday week made focusing himself against the\nnumber of games .\n- “ “ i ’ d thought something i am running everything i saw my own american life usually respect at all , as some\nequipment they have that .\n- and i was hoping that management does still even bring to hillary carson , and listen to a guy playing off back .\nMLE: - what we need do this case if anybody had now touched a in - town community , all your kids in exchange\nbarriers are needed\nin , no more , all may come out the coming in reﬂect the options .\n- so the scottish government is signiﬁcant until not extension you own very so hard to change my job for your six points\nat half , social opinion and take beyond .\n- us president - elect donald trump will re consider an effort to set out that it would be to accept from the us - city solution\nto the world .\n- local judges ask for her children and went into a video itself , she said for 2016 ’ s early next day , he said .\nLeakGAN: - picture west eight my might conﬁdence , zero conﬁdence my either nazi a a time having accounts , skills a\ndifference x having must difference time having a develop pakistan conﬁdence time time killed wilson partners nazi\nunfair zero phones develop vital conﬁdence a might showed a having conﬁdence develop a\n- pupils evidence accounts having conﬁdence conﬁdence theft abortion time time sized time west coming a unfair time\naffecting time my theft a a killed killed phones , , time questioned pakistan a partners evidence sized conﬁdence unfair\nmy eight time pakistan zero zero conﬁdence partners either seventh having , killed a\nGsGAN: - i hope that i do something like that it ’ s a very important thing , i know what you want to see this i didn ’ t\nknow , i think it does not be able to work out this way that ’ s not a lot better needs to\n- the actor is it , well , which was a good job in a writing - christmas time out of a three - year - old woman who had been\ncharged for the murder , he said that he was investigating the government ’ s decision to 19 . 6 million\n- to give the ﬁrst time , it added , he had a few days and now he ’ s not just a new administration , he will do the same\ntime before .\n- it is that , but the two - year - old woman he didn ’ t agree on : they had been a right ago because i wanted to have to do\nsomething i was trying to kill them , i am , but i can do it had to stay\nFM-GAN: - The United States , the United States has been a major group of the United States in the United States , the\nUnited States in the United States .\n- We have to be able to pay the money to be able to pay the money to be able to pay the money to pay for the same time\n,\" he said .\n- \" It ’ s a lot of people who have been a woman , and I have been told the police ,\" she said .\n- The man ’ s death was a \" bit of the incident , but the police said that the police had been taken to the city , and the\npolice ofﬁcer was a \" very dangerous - driving area .\n- We have to be able to do the government to be able to do the government to be able to leave the country ,\" he said .\nARAE: - a more . 5 per cent the company said it would not be expected to rise if he hit the 2 percent year , it said , rose\n2 , 500 ,when the only reason only be the best way for the best time for the best time for the best time for them and not\nbeing able to have done with much\n- the fact : the fact only now not being able to have done with a much more time for the age amount time with a much\ntime for the best time for\n- the fact the only reason only be the best way .\n- the fact : the fact only now being a more person with a person with each person with a much time with the best time for\nthem as much as a person\nTILGAN : - many people who died , although they didn ’ t have been on the same day , not just because of those who\nhad been out of them .\n- \" i had to be able to get a good deal with the right time ,\" he said in a statement .\n- that ’ s why , in my life is now that ’ s not the same thing , and how much money is .\n- we are still working closely with the community who is still in the world , but we can ’ t be the best .\n- we can ’ t get some good players in the league , but not only because we ’ ve played well .",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6620815992355347
    },
    {
      "name": "Computer science",
      "score": 0.6019434332847595
    },
    {
      "name": "Natural language processing",
      "score": 0.4118736982345581
    },
    {
      "name": "Electrical engineering",
      "score": 0.22123530507087708
    },
    {
      "name": "Engineering",
      "score": 0.10993248224258423
    },
    {
      "name": "Voltage",
      "score": 0.09694930911064148
    }
  ]
}